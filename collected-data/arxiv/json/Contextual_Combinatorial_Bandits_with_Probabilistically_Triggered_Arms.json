{
    "title": "Contextual Combinatorial Bandits with Probabilistically Triggered Arms",
    "abstract": "We study contextual combinatorial bandits with probabilistically triggered arms (C2MAB-T) un- der a variety of smoothness conditions that cap- ture a wide range of applications, such as contex- tual cascading bandits and contextual influence maximization bandits. Under the triggering prob- ability modulated (TPM) condition, we devise the C2-UCB-T algorithm and propose a novel anal- ysis that achieves an ˜O(d √ KT) regret bound, removing a potentially exponentially large factor O(1/pmin), where d is the dimension of contexts, pmin is the minimum positive probability that any arm can be triggered, and batch-size K is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance- adaptive algorithm VAC2-UCB and derive a re- gret bound ˜O(d √ T), which is independent of the batch-size K. As a valuable by-product, our anal- ysis technique and variance-adaptive algorithm can be applied to the CMAB-T and C2MAB set- ting, improving existing results there as well. We also include experiments that demonstrate the im- proved performance of our algorithms compared with benchmark algorithms on synthetic and real- world datasets.",
    "body": "Contextual Combinatorial Bandits with Probabilistically Triggered Arms\nXutong Liu 1 Jinhang Zuo 2 3 Siwei Wang 4 John C.S. Lui 1 Mohammad Hajiesmaili 2 Adam Wierman 3\nWei Chen 4\nAbstract\nWe study contextual combinatorial bandits with\nprobabilistically triggered arms (C2MAB-T) un-\nder a variety of smoothness conditions that cap-\nture a wide range of applications, such as contex-\ntual cascading bandits and contextual influence\nmaximization bandits. Under the triggering prob-\nability modulated (TPM) condition, we devise the\nC2-UCB-T algorithm and propose a novel anal-\nysis that achieves an ˜O(d\n√\nKT) regret bound,\nremoving a potentially exponentially large factor\nO(1/pmin), where d is the dimension of contexts,\npmin is the minimum positive probability that any\narm can be triggered, and batch-size K is the\nmaximum number of arms that can be triggered\nper round. Under the variance modulated (VM)\nor triggering probability and variance modulated\n(TPVM) conditions, we propose a new variance-\nadaptive algorithm VAC2-UCB and derive a re-\ngret bound ˜O(d\n√\nT), which is independent of the\nbatch-size K. As a valuable by-product, our anal-\nysis technique and variance-adaptive algorithm\ncan be applied to the CMAB-T and C2MAB set-\nting, improving existing results there as well. We\nalso include experiments that demonstrate the im-\nproved performance of our algorithms compared\nwith benchmark algorithms on synthetic and real-\nworld datasets.\n1. Introduction\nThe stochastic multi-armed bandit (MAB) problem is a\nclassical sequential decision-making problem that has been\nwidely studied (Robbins, 1952; Auer et al., 2002; Bubeck\n1The Chinese University of Hong Kong, Hong Kong SAR,\nChina 2University of Massachusetts Amherst, MA, United States\n3California Institute of Technology, CA, United States 4Microsoft\nResearch, Beijing, China. Correspondence to: Xutong Liu <li-\nuxt@cse.cuhk.edu.hk>, Wei Chen <weic@microsoft.com>, John\nC.S. Lui <cslui@cse.cuhk.edu.hk>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\net al., 2012). As an extension of MAB, combinatorial multi-\narmed bandits (CMAB) have drawn attention due to fruitful\napplications in online advertising, network optimization,\nand healthcare systems (Gai et al., 2012; Kveton et al.,\n2015a; Chen et al., 2013; 2016a; Wang & Chen, 2017;\nMerlis & Mannor, 2019). CMAB is a sequential decision-\nmaking game between a learning agent and an environment.\nIn each round, the agent chooses a combinatorial action that\ntriggers a set of base arms (i.e., a super-arm) to be pulled\nsimultaneously, and the outcomes of these pulled base arms\nare observed as feedback (typically known as semi-bandit\nfeedback). The goal of the agent is to minimize the ex-\npected regret, which is the difference in expectation for the\noverall rewards between always playing the best action (i.e.,\nthe action with the highest expected reward) and playing\naccording to the agent’s own policy.\nMotivated by large-scale applications with a huge number\nof items (base arms), there exists a prominent line of work\nthat advances the CMAB model: the combinatorial con-\ntextual bandits (or C2MAB for short) (Qin et al., 2014; Li\net al., 2016; Takemura et al., 2021). Specifically, C2MAB in-\ncorporates contextual information and adds the simple yet\neffective linear structure assumption to allow scalability,\nwhich provides regret bounds that are independent of the\nnumber of base arms m. Despite C2MAB’s success in lever-\naging contextual information for better scalability, existing\nworks fail to formulate the general arm triggering process,\nwhich is essential to model a wider range of applications,\ne.g., cascading bandits (CB) and influence maximization\n(IM), and more importantly, they do not provide satisfying\nresults for settings with probabilistically triggered arms. For\nexample, Qin et al. (2014); Takemura et al. (2021) only con-\nsider the deterministic semi-bandit feedback for C2MAB.\nLi et al. (2016); Wen et al. (2017) implicitly consider the\narm triggering process for specific CB or IM applications\nbut only gives sub-optimal results with unsatisfying factors\n(e.g., 1/pmin, K that could be as large as the number of\nbase arms), owing to loose analysis, weak conditions, or\ninefficient algorithms that explore the unknown parameters\ntoo conservatively.\nTo handle the above issues, we enhance the C2MAB frame-\nwork by considering an arm triggering process. Specifically,\nwe propose the general framework of contextual combi-\n1\narXiv:2303.17110v3  [cs.LG]  18 Nov 2024\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nTable 1. Summary of the main results for C2MAB-T, and additional results for CMAB-T and C2MAB.\nC2MAB-T\nAlgorithm\nCondition\nCoefficient\nRegret Bound\nC3-UCB (Li et al., 2016)∗\n1-norm\nB1\nO(B1d\n√\nKT · log T/pmin)\n(Main Result 1)\nC2-UCB-T (Algorithm 1)\n1-norm TPM\nB1\nO(B1d\n√\nKT · log T)\n(Main Result 2)\nVAC2-UCB (Algorithm 2)\nVM\nBv†\nO(Bvd\n√\nT · log T/√pmin)\n(Main Result 3)\nVAC2-UCB (Algorithm 2)\nTPVM\nBv, λ ≥1‡\nO(Bvd\n√\nT · log T)\nCMAB-T\nAlgorithm\nCondition\nCoefficient\nRegret Bound\nBCUCB-T (Liu et al., 2022)\nTPVM\nBv, λ ≥1\nO(Bv\np\nm(log K)T · log T)\n(Additional Result 1)\nBCUCB-T (Our new analysis)\nTPVM\nBv, λ ≥1\nO(Bv\np\nm(log K)T · (log T)1/2)∗∗\nC2MAB\nAlgorithm\nCondition\nCoefficient\nRegret Bound\nC2UCB (Qin et al., 2014)\n2-norm\nB2§\nO(B2d\n√\nT log T)\nC2UCB (Takemura et al., 2021)\n1-norm\nB1\nO(B1d\n√\nKT log T)\n(Additional Result 2)\nVAC2-UCB (Algorithm 2)\nVM\nBv\nO(Bvd\n√\nT log T)\n∗This work is specified for contextual combinatorial cascading bandits, without formally defining the arm triggering process.\n† Generally, coefficient Bv = O(B1\n√\nK) and the existing regret bound is improved when Bv = o(B1\n√\nK)\n‡ λ is a coefficient in TPVM condition: when λ is larger, the condition is stronger with smaller regret but can include less applications.\n∗∗We also show improved distribution-dependent regret bound in Appendix C;\n§ Almost all applications satisfy B2 = Θ(B1\n√\nK).\nnatorial bandits with probabilistically triggered arms (or\nC2MAB-T for short). At the base arm level, C2MAB-T\nuses a time-varying feature map ϕt to model the contex-\ntual information at each round t, and the mean outcome of\neach arm i ∈[m] is a linear product of the feature vector\nϕt(i) ∈Rd and an unknown vector θ∗∈Rd (where d ≪m\nto handle large-scale applications). At the (combinatorial)\naction level, inspired by the non-contextual CMAB with\nprobabilistically triggered arms (or CMAB-T) works (Chen\net al., 2016b; Wang & Chen, 2017; Liu et al., 2022), we\nformally define an arm-triggering process to cover more\ngeneral feedback models such as semi-bandit, cascading,\nand probabilistic feedback. We also inherit smoothness con-\nditions for the non-linear reward function to cover different\napplication scenarios, such as CB, IM, and online probabilis-\ntic maximum coverage (PMC) problems (Chen et al., 2016a;\nWang & Chen, 2017). With this formulation, C2MAB-T re-\ntains C2MAB’s scalability while also enjoying CMAB-T’s\nrich reward functions and general feedback models.\nContributions. Our main results are shown in Table 1.\nFirst, we study C2MAB-T under the triggering probability\nmodulated (TPM) smoothness condition, a condition intro-\nduced by Wang & Chen (2017) to remove a factor of 1/pmin\nin the pioneer CMAB-T work (Chen et al., 2016a). This re-\nsult follows a similar vein by devising C2-UCB-T algorithm\nand proving a ˜O(d\n√\nKT) regret, which removes a 1/pmin\nfactor for prior contextual CB applications (Li et al., 2016)\n(Main Result 1 in Table 1). The key technical challenge\nis that the triggering group (TG) analysis (Wang & Chen,\n2017) for CMAB-T cannot handle the triggering probability\ndetermined by time-varying contexts. To tackle this issue,\nwe devise a new technique, called the triggering probability\nequivalence (TPE), which links the triggering probabilities\nwith the random triggering event under expectation. In this\nway, we no longer need to bound the regret caused by possi-\nbly triggered arms, but only need to bound the regret caused\nby actually triggered arms. As a result, we can then directly\napply the simple non-triggering C2MAB analysis to obtain\nthe regret bound for C2MAB-T. In addition, our TPE can\nreproduce the results for CMAB-T in a similar way.\nSecond, we study the C2MAB-T under the variance mod-\nulated (VM) smoothness condition (Liu et al., 2022), in\nlight of the recent variance-adaptive algorithms to remove\nthe batch size dependence O(\n√\nK) for CMAB-T (Merlis\n& Mannor, 2019; Liu et al., 2022; Vial et al., 2022). We\npropose a new variance-adaptive algorithm VAC2-UCB and\nprove a batch-size independent regret ˜O(d\np\nT/pmin) under\nVM condition (Main Result 2 in Table 1). The main techni-\ncal difficulty is to deal with the unknown variance. Inspired\nby Lattimore et al. (2015), we use the UCB/LCB value to\nconstruct an optimistic variance and on top of that, we prove\na new concentration bound to incorporate the triggered arms\nand optimistic variance to get the desirable results.\nThird, we investigate the stronger triggering probability and\nvariance modulated (TPVM) condition (Liu et al., 2022) in\norder to remove the additional 1/√pmin factor. The key\nchallenge is that we cannot directly use TPE to link the\ntrue triggering probability with the random trigger event as\nbefore, since the TPVM condition only yields a mismatched\ntriggering probability associated with the optimistic vari-\nance used in the algorithm. Our solution is to bound this\nadditional mismatch by lower-order terms based on mild\nconditions on the triggering probability, which achieves the\n˜O(d\n√\nT) regret bounds (Main Result 3 in Table 1).\nAs a valuable by-product, our TPE analysis and VAC2-UCB\nalgorithm can be applied to non-contextual CMAB-T and\nC2MAB, improving the existing results by a factor √log T\n(Additional Result 1 in Table 1) and\n√\nK (Additional Result\n2\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n2 in Table 1), respectively. Our empirical results on both\nsynthetic and real data demonstrate that the VAC2-UCB\nalgorithm outperforms the state-of-art variance-agnostic and\nvariance-aware bandit algorithms in the linear cascading\nbandit application that satisfies the TPVM condition.\nRelated Work. The stochastic CMAB model has received\nsignificant attention. The literature was initiated by (Gai\net al., 2012). Since, its regret has been improved by Kveton\net al. (2015b), Combes et al. (2015), Chen et al. (2016a).\nThere exist two prominent lines of work in the literature\nrelated to our study: contextual CMAB and CMAB with\nprobabilistically triggered arms (C2MAB and CMAB-T).\nFor C2MAB works, Qin et al. (2014) is the first study,\nwhich proposes C2UCB algorithm that considers reward\nfunctions under 2-norm B2 smoothness condition. Then\nTakemura et al. (2021) replaces the 2-norm smoothness con-\ndition with a new 1-norm B1 smoothness condition, and\nproves a O(B1d\n√\nKT log T) regret bound. In this work,\nwe extend the C2MAB with triggering arms to cover more\napplication scenarios (e.g., contextual CB and contextual\nIM). Moreover, we further consider the stronger VM con-\ndition and propose a new variance-adaptive algorithm that\ncan achieve a\n√\nK factor improvement in the regret upper\nbound for applications like PMC.\nFor CMAB-T works, Chen et al. (2016a) is the first work\nthat considers the arm triggering process to cover CB and IM\napplications. The authors propose the CUCB algorithm, and\ngive an O(B1\n√mKT log T/pmin) regret bound under 1-\nnorm B1 smoothness condition. Then Wang & Chen (2017)\nproposes the stronger 1-norm triggering probability modu-\nlated (TPM) B1 smoothness condition, and use the trigger-\ning group (TG) analysis to remove a 1/pmin factor in the\nprevious regret. Recently, Liu et al. (2022) incorporates the\nvariance information, and proposes the variance-adaptive\nalgorithm BCUCB-T, which also uses the TG analysis and\nfurther reduces the regret’s dependency on batch-size from\nO(K) to O(log K) under the new variance and triggering\nprobability modulated (TPVM) condition. The smoothness\nconditions considered in this work are mostly inspired by\nthe above works, but directly following their algorithm and\nTG analysis fail to obtain any meaningful result for our\nC2MAB-T setting. Conversely, our new TPE analysis can\nbe applied to CMAB-T, reproducing CMAB-T’s result un-\nder the 1-norm TPM condition, and improving a factor of\n(√log T) under the TPVM condition.\nThere are also many studies considering specific appli-\ncations under the C2MAB-T framework (by unifying\nC2MAB and CMAB-T), including contextual CB (Li et al.,\n2016; Vial et al., 2022), contextual IM (Wen et al., 2017), etc.\nOne can see that these applications fit into our framework\nby verifying that they satisfy the TPM, VM, or TPVM con-\nditions; thus achieving improved results regarding K, pmin\nfactors. We defer a detailed theoretical and empirical com-\nparison to Sections 3 to 5. Zuo et al. (2022) study the online\ncompetitive IM and also uses C2MAB-T to denote their\ncontextual setting. However, their meaning of “contexts” is\nthe action of the competitor, which acts at the action level\nand only affects the reward function (or regret) but not the\nbase arms’ estimation. This is very different from our set-\nting, where contexts act at the base arm level and hence one\ncannot directly apply their results.\n2. Problem Setting\nWe study contextual combinatorial bandits with probabilis-\ntically triggered arms (C2MAB-T). We use [n] to represent\nset {1, ..., n}. We use boldface lowercase letters and bold-\nface CAPITALIZED letters for column vectors and matrices,\nrespectively. ∥x∥p denotes the ℓp norm of vector x. For\nany symmetric positive semi-definite (PSD) matrix M (i.e.,\nx⊤Mx ≥0, ∀x), ∥x∥M =\n√\nx⊤Mx denotes the matrix\nnorm of x regarding matrix M.\nWe specify a C2MAB-T problem instance using a tuple\n([m], S, Φ, Θ, Dtrig, R), where [m] = {1, 2, ..., m} is the\nset of base arms (or arms); S is the set of eligible actions\nwhere S ∈S is an action;* Φ is the set of possible feature\nmaps where any feature map ϕ ∈Φ is a function [m] →Rd\nthat maps an arm to a d-dimensional feature vector (and\nw.l.o.g. we normalize ∥ϕ(i)∥2 ≤1); Θ ⊆Rd is the param-\neter space; Dtrig is the probabilistic triggering function to\ncharacterize the arm triggering process (and feedback), and\nR is the reward function.\nIn C2MAB-T, a learning game is played between a learn-\ning agent (or player) and the unknown environment in a\nsequential manner. Before the game starts, the environ-\nment chooses a parameter θ∗∈Θ unknown to the agent\n(and w.l.o.g. we also assume ∥θ∗∥2 ≤1). At the begin-\nning of round t, the environment reveals feature vectors\n(ϕt(1), ..., ϕt(m)) for each arm, where ϕt ∈Φ is the fea-\nture map known to the agent. Given ϕt, the agent selects\nan action St ∈S, and the environment draws Bernoulli\noutcomes Xt = (Xt,1, ...Xt,m) ∈{0, 1}m for base arms†,\nwith mean E[Xt,i|Ht] = ⟨θ∗, ϕt(i)⟩for each base arm\ni. Here Ht denotes the history before the agent chooses\nSt and will be specified shortly after. Note that the out-\ncome Xt is assumed to be conditional independent across\narms given history Ht, similar to previous works (Qin et al.,\n2014; Li et al., 2016; Vial et al., 2022). For convenience,\nwe use µt ≜(⟨θ∗, ϕt(i)⟩)i∈[m] to denote the mean vector\nand M ≜{⟨θ, ϕ(i)⟩i∈[m] : ϕ ∈Φ, θ ∈Θ} to denote all\npossible mean vectors generated by Φ and Θ.\n*S is a general action space. When S is a collection of subsets\nof [m], we often refer to S ∈S as a super arm.\n†We assume Xt,i are Bernoulli for the ease of exposition, yet\nour setting can handle any distribution with bounded Xt,i ∈[0, 1].\n3\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nAfter the action St is played on the outcome Xt, base arms\nin a random set τt ∼Dtrig(St, Xt) are triggered, meaning\nthat the outcomes of arms in τt, i.e. (Xt,i)i∈τt are revealed\nas the feedback to the agent, and are involved in determin-\ning the reward of action St. At the end of round t, the\nagent will receive a non-negative reward R(St, Xt, τt), de-\ntermined by St, Xt and τt, and similar to (Wang & Chen,\n2017), the expected reward is assumed to be r(St; µt) ≜\nE[R(St, Xt, τt)], a function of the unknown mean vector\nµt, where the expectation is taken over the randomness of\nXt and τt ∼Dtrig(St, Xt). To allow the algorithm to esti-\nmate the underlying parameter θ∗directly from samples, we\nassume the outcome does not depend on whether the arm i is\ntriggered, i.e., EX∼D,τ∼Dtrig(S,X)[Xi|i ∈τ] = EX∼D[Xi].\nTo this end, we can give the formal definition of the history\nHt = (ϕs, Ss, τs, (Xs,i)i∈τs)s<t\nS ϕt, which contains all\ninformation before round t, as well as the contextual infor-\nmation ϕt at round t.\nThe goal of CMAB-T is to accumulate as much reward as\npossible over T rounds by learning the underlying param-\neter θ∗. The performance of an online learning algorithm\nA is measured by its regret, defined as the difference of\nthe expected cumulative reward between always playing\nthe best action S∗\nt ≜argmaxS∈S r(S; µt) in each round t\nand playing actions chosen by algorithm A. For many re-\nward functions, it is NP-hard to compute the exact S∗\nt even\nwhen µt is known, so similar to (Wang & Chen, 2017), we\nassume that the algorithm A has access to an offline (α, β)-\napproximation oracle, which for mean vector µ outputs an\naction S such that Pr [r(S; µ) ≥α · r(S∗; µ)] ≥β. The\nT-round (α, β)-approximate regret is defined as\nReg(T) = E\nhPT\nt=1 (αβ · r(S∗\nt ; µt) −r(St; µt))\ni\n, (1)\nwhere the expectation is taken over the randomness of out-\ncomes X1, ..., XT , the triggered sets τ1, ..., τT , as well as\nthe randomness of algorithm A itself.\nRemark 1 (Difference with CMAB-T). C2MAB-T strictly\ngeneralizes CMAB-T by allowing a probably time-varying\nfeature map ϕt. Specifically, let θ∗= (µ1, ..., µm) and fix\nϕt(i) = ei where ei ∈Rm is the one-hot vector with 1 at\nthe i-th entry and 0 elsewhere, one can easily reproduce the\nCMAB-T setting in (Wang & Chen, 2017).\nRemark 2 (Difference with C2MAB). C2MAB-T en-\nhances the modeling power of prior C2MAB (Qin et al.,\n2014; Takemura et al., 2021) by capturing the probabilistic\nnature of the feedback (v.s. the deterministic semi-bandit\nfeedback). This enables a wider range of applications such\nas combinatorial CB, multi-layered network exploration,\nand online IM (Wang & Chen, 2017; Liu et al., 2022).\n2.1. Key Quantities and Conditions\nIn the C2MAB-T model, there are several quantities and\nassumptions that are crucial to the subsequent study. We\ndefine triggering probability pµ,Dtrig,S\ni\nas the probability that\nbase arm i is triggered when the action is S, the mean vec-\ntor is µ, and the probabilistic triggering function is Dtrig.\nSince Dtrig is always fixed in a given application context,\nwe ignore it in the notation for simplicity, and use pµ,S\ni\nhenceforth. Triggering probabilities pµ,S\ni\n’s are crucial for\nthe triggering probability modulated bounded smoothness\nconditions to be defined below. We define ˜S to be the set\nof arms that can be triggered by S, i.e.,{i ∈[m] : pµ,S\ni\n>\n0, for any µ ∈M}, the batch size K as the maximum num-\nber of arms that can be triggered, i.e., K = maxS∈S | ˜S|,\nand pmin = mini∈[m],µ∈M,S∈S,pµ,S\ni\n>0 pµ,S\ni\n.\nOwing to the nonlinearity and the combinatorial structure\nof the reward, it is essential to give some conditions for the\nreward function in order to achieve any meaningful regret\nbounds (Chen et al., 2013; 2016a; Wang & Chen, 2017;\nMerlis & Mannor, 2019; Liu et al., 2022). For C2MAB-T,\nwe consider the following conditions.\nCondition 1 (Monotonicity). We say that a C2MAB-T prob-\nlem instance satisfies monotonicity condition, if for any ac-\ntion S ∈S, any mean vectors µ, µ′ ∈[0, 1]m such that\nµi ≤µ′\ni for all i ∈[m], we have r(S; µ) ≤r(S; µ′).\nCondition 2 (1-norm TPM Bounded Smoothness, (Wang\n& Chen, 2017)). We say that a C2MAB-T problem instance\nsatisfies the triggering probability modulated (TPM) B1-\nbounded smoothness condition, if for any action S ∈S,\nany mean vectors µ, µ′ ∈[0, 1]m, we have |r(S; µ′) −\nr(S; µ)| ≤B1\nP\ni∈[m] pµ,S\ni\n|µi −µ′\ni|.\nCondition 3 (VM Bounded Smoothness, (Liu et al., 2022)).\nWe say that a C2MAB-T problem instance satisfies the vari-\nance modulated (VM) (Bv, B1)-bounded smoothness con-\ndition, if for any action S ∈S, mean vector µ, µ′ ∈\n(0, 1)m, for any ζ, η ∈[−1, 1]m s.t. µ′ = µ + ζ + η,\nwe have |r(S; µ′) −r(S; µ)| ≤Bv\nqP\ni∈˜S\nζ2\ni\n(1−µi)µi +\nB1\nP\ni∈˜S |ηi|.\nCondition 4 (TPVM Bounded Smoothness, (Liu et al.,\n2022)). We say that a C2MAB-T problem instance sat-\nisfies the triggering probability and variance modulated\n(TPVM) (Bv, B1, λ)-bounded smoothness condition, if for\nany action S ∈S, mean vector µ, µ′ ∈(0, 1)m, for\nany ζ, η ∈[−1, 1]m s.t.\nµ′ = µ + ζ + η, we have\n|r(S; µ′) −r(S; µ)| ≤Bv\nqP\ni∈[m](pµ,S\ni\n)λ\nζ2\ni\n(1−µi)µi +\nB1\nP\ni∈[m] pµ,S\ni\n|ηi|.\nCondition 1 indicates the reward is monotonically increasing\nwhen the parameter µ increases. Condition 2, 3 and 4 all\nbound the reward smoothness/sensitivity.\n4\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nFor Condition 2, the key feature is that the parameter change\nin each base arm i is modulated by the triggering probabil-\nity pµ,S\ni\n. Intuitively, for base arm i that is unlikely to be\ntriggered/observed (small pµ,S\ni\n), Condition 2 ensures that\na large change in µi (due to insufficient observation) only\ncauses a small change (multiplied by pµ,S\ni\n) in reward, which\nhelps to save a pmin factor for non-contextual CMAB-T.\nFor Condition 3, intuitively if we ignore the denominator\n(1 −µi)µi of the leading Bv term, the reward change would\nbe O(Bv\n√\nK∆) when the amount of parameter change\n|µ′\ni−µi| = ∆for each arm i. This introduces a O(\n√\nK) fac-\ntor reduction in the reward change and translates to a O(K)\nimprovement in the regret, compared with O(B1K∆) re-\nward change when applying the non-triggering version of\nCondition 2 (i.e., pµ,S\ni\n= 1 if i ∈˜S and pµ,S\ni\n= 0 other-\nwise). However, for real applications, B1 = Θ(B1\n√\nK)\nwhich cancels the O(\n√\nK) improvement. To reduce Bv\ncoefficient, the leading Bv term is modulated by the inverse\nof the variance Vi = (1−µi)µi, and thus allow applications\nto achieve a Bv coefficient independent of K (or at least\nBv = o(B1\n√\nK)), leading to significant savings in the re-\ngret bound for applications like PMC (Liu et al., 2022). The\nrelation between Condition 2 and 3 is generally not com-\nparable, but compared with Condition 2’s non-triggering\ncounterpart (i.e., 1-norm condition), Condition 3 is stronger.\nFinally, for Condition 4, it combines both the triggering-\nprobability modulation from Condition 2 and the variance\nmodulation from Condition 3. The exponent λ of pµ,S\ni\ngives\nadditional flexibility to trade-off between the strength of the\ncondition and the regret, i.e., with a larger λ, one can obtain\na smaller regret bound, while with a smaller λ, the condition\nis easier to satisfy to include more applications. In general,\nCondition 4 is stronger than Condition 2 and Condition 3,\nas the former degenerates to the other two conditions by\nsetting ζ = 0 and the fact that pµ,S\ni\n≤1 for i ∈˜S and\npµ,S\ni\n= 0 otherwise, respectively. Conversely, by applying\nthe Cauchy-Schwartz inequality, one can verify that if a re-\nward function is TPM B1-bounded smooth, then it is TPVM\n(B1\n√\nK/2, B1, λ)-bounded smooth for any λ ≤2 or simi-\nlarly VM (B1\n√\nK/2, B1)-bounded smooth, respectively.\nIn light of the above conditions that significantly advance the\nnon-contextual CMAB-T, the goal of subsequent sections\nis to design algorithms and conduct analysis to derive the\n(improved) results for the contextual setting. And later in\nSection 5, we demonstrate how these conditions are applied\nto applications, such as CB and online IM, to achieve both\ntheoretical and empirical improvements. Due to the space\nlimit, the detailed proofs are included in the Appendix.\n3. Algorithm and Regret Analysis for\nC2MAB-T under the TPM Condition\nAlgorithm 1 C2-UCB-T: Contextual Combinatorial Upper\nConfidence Bound Algorithm for C2MAB-T\n1: Input: Base arms [m], dimension d, regularizer γ, fail-\nure probability δ = 1/T, offline oracle ORACLE.\n2: Initialize: Gram matrix G1 = γI, vector b1 = 0.\n3: for t = 1, ..., T do\n4:\nˆθt = G−1\nt bt.\n5:\nfor i ∈[m] do\n6:\n¯µt,i = ⟨ϕt(i), ˆθt⟩+ ρ(δ) ∥ϕt(i)∥G−1\nt .\n7:\nend for\n8:\nSt = ORACLE(¯µt,1, ..., ¯µt,m).\n9:\nPlay St and observe triggering arm set τt and obser-\nvation set (Xt,i)i∈τt.\n10:\nGt+1 = Gt + P\ni∈τt ϕt(i)ϕt(i)⊤.\n11:\nbt+1 = bt + P\ni∈τt ϕt(i)Xt,i.\n12: end for\nOur proposed algorithm C2-UCB-T (Algorithm 1) is a gen-\neralization of the C3-UCB algorithm originally designed for\ncontextual combinatorial cascading bandits (Li et al., 2016).\nOur main contribution is to show an improved regret bound\nby a factor of 1/pmin under the 1-norm TPM condition.\nRecall that we define the data about the history as Ht =\n(ϕs, Ss, τs, (Xs,i)i∈τs)s<t\nS ϕt. Different from the CUCB\nalgorithm (Wang & Chen, 2017) that directly estimates the\nmean µt,i for each arm, Algorithm 1 estimates the under-\nlying parameter θ∗via a ridge regression problem over the\nhistory data Ht. More specifically, we estimate θ∗by solv-\ning the following ℓ2-regularized least-square problem with\nregularization parameter γ > 0:\nˆθt = argmin\nθ∈Θ\nX\ns<t\nX\ni∈τs\n(⟨θ, ϕs(i)⟩−Xs,i)2 + γ ∥θ∥2\n2 . (2)\nThe\nclosed\nform\nsolution\nis\nprecisely\nthe\nˆθt\ncalculated\nin\nline\n4,\nwhere\nthe\nGram\nmatrix\nGt\n=\nP\ns<t\nP\ni∈τs ϕs(i)ϕs(i)⊤\nand the b-vector\nbt = P\ns<t\nP\ni∈τs ϕs(i)Xs,i are computed in line 10 and\n11. We claim that ˆθt is a good estimator of θ∗by bounding\ntheir difference via the following lemma, which is also used\nin (Qin et al., 2014; Li et al., 2016).\nProposition 1 (Theorem 2, (Abbasi-Yadkori et al., 2011)).\nLet ρ(δ) =\nr\nlog\n\u0010\n(γ+KT/d)d\nγd·δ2\n\u0011\n+√γ, then with probability\nat least 1 −δ, for all t ∈[T],\n\r\r\rˆθt −θ∗\r\r\r\nGt ≤ρ(δ).\nBuilding on this, we construct an optimistic estimation of\neach arm’s mean ¯µt,i in line 6, where ρ(δ) is in Proposi-\ntion 1, ⟨ϕt(i), ˆθt⟩and ρ(δ) ∥ϕt(i)∥G−1\nt\nare the empirical\nmean and confidence interval towards the direction ϕt(i),\nrespectively. As a convention, we clip ¯µt,i into [0, 1] if\n¯µt,i > 1 or ¯µt,i < 0.\n5\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nThanks to Proposition 1, we have the following lemma for\nthe desired amount of the base arm level optimism,\nLemma 1. With probability at least 1 −δ, we have µt,i ≤\n¯µt,i ≤µt,i + 2ρ(δ) ∥ϕt(i)∥G−1\nt\nfor all i ∈[m], t ∈[T].\nProof. See Appendix A.2.\n■\nAfter computing the UCB values ¯µt, the agent selects action\nSt via the offline oracle with ¯µt as input. Then base arms\nin τt are triggered, and the agent receives observation set\n(Xt,i)i∈τt as feedback to improve future decisions.\nTheorem 1.\nFor a C2MAB-T instance that satis-\nfies monotonicity (Condition 1) and TPM smooth-\nness\n(Condition\n2)\nwith\ncoefficient\nB1,\nC2-UCB-\nT (Algorithm 1) with an (α, β)-approximation ora-\ncle achieves an (α, β)-approximate regret bounded by\nO\n\u0010\nB1(\np\nd log(KT/γ) + √γ)\np\nKTd log(KT/γ)\n\u0011\n.\nDiscussion.\nLooking at Theorem 1, we achieve an\nO(B1d\n√\nKT log T) regret bound when d ≪K ≤m ≪T,\nwhich is independent of the number of arms m and the\nminimum triggering probability pmin. Consider the combi-\nnatorial cascading bandits (Li et al., 2016) satisfying B1 = 1\n(see Section 5), our result improves the Li et al. (2016) by a\nfactor of 1/pmin. Consider the linear reward function (Take-\nmura et al., 2021) without triggering arms (i.e., pµ,S\ni\n= 1 for\ni ∈S, and 0 otherwise), one can easily verify B1 = 1 and\nour regret matches the lower bound Ω(d\n√\nKT) Takemura\net al. (2021) up to logarithmic factors.\nAnalysis. Here we explain how to prove a regret bound that\nremoves the 1/pmin factor under the 1-norm TPM condition.\nThe main challenge is that the mean vector µt and the trig-\ngering probability pµt,S\ni\nare dependent on time-varying con-\ntexts ϕt(i), so it is impossible to derive any meaningful con-\ncentration inequality or regret bound based on Tt,i, which\nis the number of times that arm i is triggered, and has been\nused by the triggering group (TG) technique (Wang & Chen,\n2017) to remove 1/pmin. To deal with this problem, we by-\npass the quantity Tt,i and use the triggering-probability\nequivalence (TPE) technique that equalizes pµt,S\ni\nwith\nEt[I{i ∈τt}], which in turn replaces the expected regret\nproduced by all possible arms with the expected regret pro-\nduced by i ∈τt to avoid pmin. To sketch our proof idea, we\nassume the oracle is deterministic with β = 1 (the random-\nness of the oracle and β < 1 are handled in Appendix A),\nand let filtration Ft−1 be the history data Ht (defined in\nSection 2). Denote Et[·] = E[· | Ft−1], the t-round regret\nEt[α · r(S∗\nt ; µt) −r(St; µt)]≤Et[r(St; ¯µt) −r(St; µt)],\nbased on Condition 1, Lemma 1 and definition of St. Then\nEt[r(St; ¯µt) −r(St; µt)]\n(a)\n≤Et\nhP\ni∈˜St B1pµt,St\ni\n(¯µt,i −µt,i)\ni\n(b)\n= E\n\u0002P\ni∈˜St B1Eτt[I{i ∈τt}](¯µt,i −µt,i) | Ft−1\n\u0003\n(c)\n= Et\n\u0002P\ni∈˜St I{i ∈τt}B1(¯µt,i −µt,i)\n\u0003\n(d)\n= Et\n\u0002P\ni∈τt B1(¯µt,i −µt,i)\n\u0003\n,\n(3)\nwhere (a) is by Condition 2, (b) is because ¯µt,i, µt,i, St\nare Ft−1 measurable so that the only randomness is from\ntriggering set τt and we can substitute pµt,St\ni\nwith event\nI{i ∈τt} under expectation, (c) is by absorbing the ex-\npectation over τt to Et, and (d) is a change of notation.\nAfter applying the TPE, we only need to bound the regret\nproduced by i ∈τt. Hence\nReg(T)≤E\nhP\nt∈[T ] Et\n\u0002P\ni∈τt B1(¯µt,i −µt,i)\n\u0003i\n(a)\n≤E\nhP\nt∈[T ] Et\nhP\ni∈τt 2B1ρ(δ) ∥ϕt(i)∥G−1\nt\nii\n(b)\n≤2B1ρ(δ)E\nhq\nKT P\nt∈[T ]\nP\ni∈τt ∥ϕt(i)∥2\nG−1\nt\ni\n(c)\n≤O(B1d\n√\nKT log T).\n(4)\nwhere (a) follows from Lemma 1, (b) is by Cauchy Schwarz\ninequality over both i and t, and (c) is by the ellipsoidal\npotential lemma (Lemma 5) in the Appendix.\nRemark 3. In addition to the general C2MAB-T setting,\nthe TPE technique can also replace the more involved TG\ntechnique (Wang & Chen, 2017) for CMAB-T. Such replace-\nment can save an unnecessary union bound over the group\nindex, which in turn reproduce Theorem 1 of Wang & Chen\n(2017) under Condition 2, and improve Theorem 1 of Liu\net al. (2022) under Condition 4 by a factor of O(√log T),\nsee Appendix C for details.\n4. Variance-Adaptive Algorithm and Analysis\nfor C2MAB-T under VM/TPVM Condition\nIn this section, we propose a new variance-adaptive al-\ngorithm VAC2-UCB (Algorithm 2) to further remove the\nO(\n√\nK) factor and achieve ˜O(Bvd\n√\nT) regret bound for\napplications satisfying stronger VM/TPVM conditions.\nDifferent from Algorithm 1, VAC2-UCB leverages the\nsecond-order statistics (i.e., variance) to speed up the learn-\ning process. To get some intuition, we first assume the\nvariance Vs,i = Var[Xs,i] for each base arm i at round s\nis known in advance. In this case, VAC2-UCB adopts the\nweighted ridge-regression to learn the parameter θ∗:\nˆθt = argmin\nθ∈Θ\nX\ns<t\nX\ni∈τs\n(⟨θ, ϕs(i)⟩−Xs,i)2/Vs,i + γ ∥θ∥2\n2 , (5)\nwhere the first term is “weighted” by the true variance\nVs,i.\nThe closed-form solution of the above estima-\ntor is ˆθt\n=\nG−1\nt bt where the Gram matrix Gt\n=\n6\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nAlgorithm 2 VAC2-UCB: Variance-Adaptive Contextual\nCombinatorial Upper Confidence Bound Algorithm\n1: Input: Base arms [m], dimension d, regularizer γ, fail-\nure probability δ = 1/T, offline oracle ORACLE.\n2: Initialize: Gram matrix G1 = γI, regressand b1 = 0.\n3: for t = 1, ..., T do\n4:\nˆθt = G−1\nt bt.\n5:\nfor i ∈[m] do\n6:\n¯µt,i = ⟨ϕt(i), ˆθt⟩+ 2ρ(δ) ∥ϕt(i)∥G−1\nt\n7:\n¯\nµt,i = ⟨ϕt(i), ˆθt⟩−2ρ(δ) ∥ϕt(i)∥G−1\nt\n8:\nSet the optimistic variance ¯Vt,i as Equation (6).\n9:\nend for\n10:\nSt = ORACLE(¯µt,1, ..., ¯µt,m).\n11:\nPlay St and observe triggering arm set τt and obser-\nvation set (Xt,i)i∈τt.\n12:\nGt+1 = Gt + P\ni∈τt ¯V −1\nt,i ϕt(i)ϕt(i)⊤.\n13:\nbt+1 = bt + P\ni∈τt ¯V −1\nt,i ϕt(i)Xt,i.\n14: end for\nP\ns<t\nP\ni∈τs V −1\ns,i ϕs(i)ϕs(i)⊤and the b-vector bt\n=\nP\ns<t\nP\ni∈τs V −1\ns,i ϕs(i)Xs,i, which enjoys the similar form\n(but uses different weights ¯Vs,i) of line 12 and line 13.\nThe intuition of using the inverse of Vs,i to re-weight the ob-\nservation is that: the smaller the variance, the more accurate\nthe observation (ϕt(i), Xt,i) is, and thus more important for\nthe agent to learn unknown θ∗. In fact, the above estimator\nˆθt is closely related to the best linear unbiased estimator\n(BLUE) (Henderson, 1975). Concretely, in the literature\nof linear regression, Equation (5) is the lowest variance es-\ntimator of θ∗among all unbiased linear estimators, when\nthe regularization term γ = 0, Vs,i are the true variance\nproxy of outcomes (Xs,i)s<t,i∈τs and the context sequence\n(ϕs(i))s<t,i∈τs follows the fixed design in Equation (5).\nFor our C2MAB-T setting, one new challenge arises since\nthe variance Vs,i = µs,i(1 −µs,i) is not known a pri-\nori.\nInspired by (Lattimore et al., 2015; Zhou et al.,\n2021), we construct an optimistic estimation ¯Vs,i to re-\nplace the true variance Vs,i in Equation (5). Indeed, we\nconstruct ¯Vt,i by solving the optimal value for the prob-\nlem maxµ∈[\n¯\nµt,i,¯µt,i] µ(1 −µ), whose closed form solution\nimmediately follows from the equation below,\n¯Vt,i =\n\n\n\n\n\n(1 −¯µt,i)¯µt,i,\nif ¯µt,i ≤1\n2\n(1 −\n¯\nµt,i)\n¯\nµt,i,\nif\n¯\nµt,i ≥1\n2\n1\n4,\notherwise\n(6)\nwhere ¯µt,i and\n¯\nµt,i are UCB and LCB values to be intro-\nduced later. Notice that with high probability the true µt,i\nlies within LCB and UCB values and as they are getting\nmore accurate, the optimistic variance Vt,i is also approach-\ning the true variance Vt,i.\nTo guarantee ˆθt is a good estimator, we prove a new lemma\n(similar to Proposition 1) to guarantee the concentration\nbound of θt but in face of the unknown variance. Note that\nthe sentinel work Lattimore et al. (2015) proves a similar\nconcentration bound, the difference is that we have multiple\narms triggered in each round instead of a single arm. To\naddress this, we replaced the original concentration bound\nwith the new one below that has an extra K4 factor in N,\nwhich finally results in log K factor in the confidence radius\nρ(δ).\nLemma 2. Let γ > 0, N = (4d2K4T 4)d so that ρ(δ) =\n\u0010\n1 + √γ + 4\nq\nlog\n\u0000 6T N\nδ\nlog\n\u0000 3T N\nδ\n\u0001\u0001\u0011\n. We have for all\nt ≤T, with probability at least 1−δ,\n\r\r\rˆθt −θ∗\r\r\r\n2\nGt ≤ρ(δ).\nProof. See Appendix B.1.\n■\nBuilding on this lemma, we construct ¯µt,i as an upper bound\nof µt,i in line 6, and\n¯\nµt,i as a lower bound of µt,i in line 7,\nbased on our variance-adaptive ˆθt, Gt. Note that the dou-\nbling of the radius 2ρ(δ) instead of using ρ(δ) in Lemma 2\nis purely for the correctness of our technical analysis. As a\nconvention, we clip ¯µt,i,\n¯\nµt,i into [0, 1] if they are above 1\nor below 0.\nLemma 3. With probability at least 1 −δ, we have µt,i ≤\n¯µt,i ≤µt,i + 3ρ(δ) ∥ϕt(i)∥G−1\nt , and µt,i ≥\n¯\nµt,i ≥µt,i −\n3ρ(δ) ∥ϕt(i)∥G−1\nt\nfor all i ∈[m].\nProof. This lemma follows from the similar derivation of\nLemma 1, where we have different definitions of ¯µt,i,\n¯\nµt,i\nand the concentration now relies on Lemma 2.\n■\nAfter the agent plays St, the base arms in τt are triggered,\nand the agent receives observation set (Xt,i)i∈τt as feed-\nback. These observations (reweighted by optimistic variance\n¯Vt,i) are then used to update Gt and bt for future rounds.\n4.1. Results and Analysis under VM condition\nWe first show a regret bound for VAC2-UCB that is inde-\npendent of batch size K when the VM condition holds.\nTheorem 2.\nFor a C2MAB-T instance that satis-\nfies\nmonotonicity\n(Condition\n1)\nand\nVM\nsmooth-\nness (Condition 3) with coefficient (Bv, B1), VAC2-\nUCB (Algorithm 2) with an (α, β)-approximation ora-\ncle achieves an (α, β)-approximate regret bounded by\nO\n\u0010\nBv\n√pmin (\np\nd log(KT/γ) + √γ)\np\nTd log(KT/γ)\n\u0011\n.\nDiscussion.\nLooking at Theorem 2, we achieve an\nO(Bvd\n√\nT log T/√pmin) regret bound when d ≪K ≤\nm ≪T. For combinatorial cascading bandits (Li et al.,\n2016) with Bv = 1, our regret is independent of m, K and\nimproves Li et al. (2016) by a factor O(\np\nK/pmin).\n7\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nTable 2. Summary of the coefficients, regret bounds and improvements for various applications.\nApplication\nCondition\n(Bv, B1, λ)\nRegret\nImprovement\nOnline Influence Maximization (Wen et al., 2017)\nTPM\n(−, |V |, −)\nO(d|V |\np\n|E|T log T)\n˜O(\np\n|E|)\nDisjunctive Combinatorial Cascading Bandits (Li et al., 2016)\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\n√\nK/pmin)\nConjunctive Combinatorial Cascading Bandits (Li et al., 2016)\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\n√\nK/rmax)\nLinear Cascading Bandits (Vial et al., 2022)∗\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\np\nK/d)\nMulti-layered Network Exploration (Liu et al., 2021b)\nTPVM\n(\np\n1.25|V |, 1, 2)\nO(d\np\n|V |T log T)\n˜O(√n/pmin)\nProbabilistic Maximum Coverage (Chen et al., 2013)∗∗\nVM\n(3\np\n2|V |, 1, −)\nO(d\np\n|V |T log T)\n˜O(\n√\nk)\n|V |, |E|, n, k, L denotes the number of target nodes, the number of edges that can be triggered by the set of seed nodes, the number of layers, the number of\nseed nodes and the length of the longest directed path, respectively;\nK is the length of the ordered list, rmax = α · maxt∈[T ],S∈S r(S; µt);\n∗A special case of disjunctive combinatorial cascading bandits.\n∗∗This row is for C2MAB application and the rest of rows are for C2MAB-T applications.\nIn addition to the general C2MAB-T setting, one can verify\nthat for non-triggering C2MAB, pmin = 1, and we obtain\nthe batch-size independent regret bound O(Bvd\n√\nT log T).\nRecall Bv = O(B1\n√\nK) for any C2MAB-T instances, so\nour regret bound reproduces O(B1d\n√\nKT log T), and thus\nmatches the similar lower bound (Takemura et al., 2021) for\nthe linear reward functions. For the more interesting non-\nlinear reward function with Bv = o(B1\n√\nK), our regret\nimproves non-variance-adaptive algorithm C2UCB, whose\nregret is O(B1d\n√\nKT log T) (Qin et al., 2014; Takemura\net al., 2021).\nAnalysis.\nAt a high level, the improvement of\n√\nK\ncomes from the VM condition and the optimistic vari-\nance, which together save the use of Cauchy-Schwarz\ninequality that generates a O(\n√\nK) factor in the step\n(b) of Equation (4).\nIn order to leverage the variance\ninformation, we decompose the regret into term (I) and (II),\nReg(T) ≤E\nhPT\nt=1 r(St; ¯µt) −r(St; µt)\ni\n≤E[PT\nt=1 |r(St; ¯µt) −r(St; ˜µt)|\n|\n{z\n}\n(I)\n+ |r(St; µt) −r(St; ˜µt)|\n|\n{z\n}\n(II)\n],\n(7)\nwhere\n˜µt\nis\nthe\nvector\nwhose\ni-th\nentry\nis\nthe\nmaximizer\nthat\nachieves\noptimistic\nvariance\n¯Vt,i,\ni.e.,\n˜µt,i\n=\nargmaxµ∈[\n¯\nµt,i,¯µt,i] µ(1 −µ).\nNow\nwe\nshow\na\nsketched\nproof\nto\nbound\nthe\nterm\n(I)\nand\none\ncan\nbound\nthe\nterm\n(II)\nsimilarly.\nE\nhP\nt∈[T ] (I)\ni (a)\n≤BvE\nhPT\nt=1\nqP\ni∈˜St (¯µt,i −˜µt,i)2/ ¯Vt,i\ni\n(b)\n≤\nBv\n√pmin · E\n\u0014PT\nt=1\nqP\ni∈˜St pµt,St\ni\n(¯µt,i −˜µt,i)2/ ¯Vt,i\n\u0015\n(c)\n≤\nBv\n√pmin ·\nr\nTE\nhPT\nt=1\nP\ni∈˜St pµt,St\ni\n(¯µt,i −˜µt,i)2/ ¯Vt,i\ni\n(d)\n≤\nBv\n√pmin ·\nr\nTE\nhPT\nt=1\nP\ni∈τt (6ρ(δ) ∥ϕt(i)∥G−1\nt )2/ ¯Vt,i\ni\n(e)\n≤O(Bvd√T log T/√pmin),\n(8)\nwhere (a) is by Condition 3, (b) is by the definition of\npµt,St\ni\n≥pmin for i ∈˜St, (c) is by Cauchy–Schwarz over t\nand the Jensen’s inequality, (d) follows from the TPE and\nLemma 3, (e) follows from Lemma 6.\n4.2. Results and Analysis under TPVM Condition\nNext, we show that VAC2-UCB can achieve regret bounds\nthat remove the O(\n√\nK) and O(1/√pmin) factor for appli-\ncations satisfying the stronger TPVM conditions.\nWe first introduce a mild condition over the triggering prob-\nability (which is similar to Condition 2) to give our regret\nbounds and analysis.\nCondition 5 (1-norm TPM Bounded Smoothness for\nTriggering Probability). We say that a C2MAB-T prob-\nlem instance satisfies the triggering probability modu-\nlated Bp-bounded smoothness condition over the trigger-\ning probability, if for any action S ∈S, any mean vec-\ntors µ, µ′ ∈[0, 1]m, and any arm i ∈[m], we have\n|pµ′,S\ni\n−pµ,S\ni\n| ≤Bp\nP\nj∈[m] pµ,S\nj\n|µj −µ′\nj|.\nNow we state our main theorem as follows.\nTheorem 3. For a C2MAB-T instance, when its reward\nfunction satisfies monotonicity (Condition 1) and TPVM\nsmoothness (Condition 4) with coefficient (Bv, B1, λ), and\nits triggering probability pµ,S\ni\nsatisfies 1-norm TPM smooth-\nness with coefficient Bp (Condition 5), if λ ≥2, then VAC2-\nUCB (Algorithm 2) with an (α, β)-approximation oracle\nachieves an (α, β)-approximate regret bounded by\nO\n\u0010\nBvd\n√\nT log T + BvBp\n√\nK(d log T)2/√pmin\n\u0011\n, (9)\nand if λ ≥1, then VAC2-UCB (Algorithm 2) achieves an\n(α, β)-approximate regret bounded by\nO\n\u0010\nBvd\n√\nT log T + Bv\np\nBp(KT)1/4(d log T)3/2/√pmin\n\u0011\n.\n(10)\nDiscussion.\nThe leading term of Theorem 3 is\nO(Bvd\n√\nT log T) when d ≪K ≤m ≪T, which re-\nmoves the 1/√pmin factor compared with Theorem 2. Also,\nnotice that Theorem 3 relies on an additional Bp-smoothness\ncondition over the triggering probability. However, we claim\nthat this condition is mild and almost always satisfies with\n8\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nBp = B1 for applications considered in this paper (see\nAppendix D).\nAnalysis. We use the regret decomposition of Equation (7)\nto the same term (I) and (II), and leverage on TPVM condi-\ntion (Condition 4) to obtain:\nE\n\nX\nt∈[T ]\n(I)\n\n\n(a)\n≤BvE\n\n\nT\nX\nt=1\nsX\ni∈˜St\n(p˜µt,St\ni\n)λ(¯µt,i −˜µt,i)2/ ¯Vt,i\n\n.\n(11)\nHowever, we cannot use TPE as Equation (8) because\np˜µt,St\ni\n̸= pµt,St\ni\nin general. To handle this mismatch, we\nuse the fact that triggering probability usually satisfies a\nsmoothness condition in Condition 5, and prove that the\nmismatch only affect the lower-order term as follows:\nBy Condition 5, (p˜µt,St\ni\n)λ is upper bounded by (pµt,St\ni\n+\nmin{1, P\nj∈˜St Bppµ,St\nj\n|µt,j −˜µt,j|})2 when λ\n≥\n2,\nand the regret is bounded by the terms as shown below:\nEq. (11) ≤E\n\u0014PT\nt=1 Bv\nqP\ni∈˜St 3pµt,St\ni\n(¯µt,i −˜µt,i)2/ ¯Vt,i\n\u0015\n|\n{z\n}\nleading term\n+ BvBp\n√\nK\n√pmin E\nhPT\nt=1\nP\ni∈˜St pµt,St\ni\n(¯µt,i −\n¯\nµt,i)2/ ¯Vt,i\ni\n|\n{z\n}\nlower-order term\n,\nwhere the leading term is of order O(Bv\n√\nT log T) by using\nthe same derivation of step (c)-(e) in Equation (8), and the\nlower order term is bounded by O(BvBp\np\nK/pmin log T)\nby TPE and the weighted ellipsoidal potential lemma\n(Lemma 6). For λ ≥1, the lower-order term becomes\nBv√\nBpK1/4\n√pmin\nE\n\"\nPT\nt=1\n\u0012P\ni∈˜St\npµt,St\ni\n(¯µt,i−\n¯\nµt,i)2\n¯Vt,i\n\u00133/4#\n,\nwhich results in a larger lower-order regret term.\nSee\nAppendix B.3 for details.\n5. Applications and Experiments\nWe now move to applications and experimental results.\nWe first show how our theoretical results improve various\nC2MAB and C2MAB-T applications under 1-norm TPM,\nTPVM and VM smoothness conditions with their corre-\nsponding B1, Bv, λ coefficients. Then, we provide an em-\npirical comparison in the context of the contextual cascading\nbandit application.\nThe instantiation of our theoretical results in the context of\na variety of specific C2MAB and C2MAB-T applications\nis shown in Table 2. The final column of the table details\nthe improvement in regret that our results yield in each\ncase. For detailed settings, proofs, and the discussion of the\napplication results, see Appendix D.\nOur experimental results are summarized in Figure 1, which\n(a) All genres\n(b) Particular genre\nFigure 1. Regret results for MovieLens data.\ndetails experiments on the MovieLens-1M dataset‡. Experi-\nments on other data are included in the Appendix. Figure 1\nillustrates that our VAC2-UCB algorithm outperforms C3-\nUCB (Li et al., 2016), the variance-agnostic cascading ban-\ndit algorithm, and CascadeWOFUL (Vial et al., 2022), the\nstate-of-the-art variance-aware cascading bandit algorithm,\neventually incurring 45% and 25% less regret. For detailed\nsettings, comparisons, and discussions, see Appendix E.\n6. Conclusion\nThis paper studies contextual combinatorial bandits with\nprobabilistically triggered arms (C2MAB-T) under a variety\nof smoothness conditions. Under the triggering probability\nmodulated (TPM) condition, we design the C2-UCB-T algo-\nrithm and propose a novel analysis to achieve an ˜O(d\n√\nKT)\nregret bound, removing a potentially exponentially large\nfactor O(1/pmin). Under the variance modulated condi-\ntions (VM or TPVM), we propose a new variance-adaptive\nalgorithm VAC2-UCB and derive a regret bound ˜O(d\n√\nT),\nwhich removes the batch-size K dependence. As valu-\nable by-product, we find our TPE analysis technique and\nvariance-adaptive algorithm can be applied to the CMAB-\nT and C2MAB setting, improving existing results as well.\nExperiments show that our algorithm can achieve at least\n13% and 25% improvement compared with benchmark al-\ngorithms on synthetic and real-world datasets, respectively.\nFor the future study, it would be interesting to extend our\napplication scenarios. One could also relax the perfectly\nlinear assumption by introducing model mis-specifications\nor corruptions.\nAcknowledgement\nThe work of John C.S. Lui was supported in part by RGC’s\nGRF 14215722. The work of Mohammad Hajiesmaili is\nsupported by NSF CAREER-2045641, CPS-2136199, CNS-\n2106299, and CNS-2102963. Wierman is supported by NSF\ngrants CNS-2146814, CPS-2136197, CNS-2106403, and\nNGSDI-2105648.\n‡grouplens.org/datasets/movielens/1m/\n9\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nReferences\nAbbasi-Yadkori, Y., P´al, D., and Szepesv´ari, C. Improved\nalgorithms for linear stochastic bandits. Advances in\nneural information processing systems, 24, 2011.\nAuer, P., Cesa-Bianchi, N., and Fischer, P.\nFinite-time\nanalysis of the multiarmed bandit problem. Machine\nlearning, 47(2-3):235–256, 2002.\nBernstein, S.\nThe Theory of Probabilities (Russian).\nMoscow, 1946.\nBubeck, S., Cesa-Bianchi, N., et al. Regret analysis of\nstochastic and nonstochastic multi-armed bandit prob-\nlems. Foundations and Trends® in Machine Learning, 5\n(1):1–122, 2012.\nChen, W., Wang, Y., and Yuan, Y. Combinatorial multi-\narmed bandit: General framework and applications. In\nInternational Conference on Machine Learning, pp. 151–\n159. PMLR, 2013.\nChen, W., Wang, Y., Yuan, Y., and Wang, Q. Combinatorial\nmulti-armed bandit and its extension to probabilistically\ntriggered arms. The Journal of Machine Learning Re-\nsearch, 17(1):1746–1778, 2016a.\nChen, X., Li, Y., Wang, P., and Lui, J. A general frame-\nwork for estimating graphlet statistics via random walk.\nProceedings of the VLDB Endowment, 10(3):253–264,\n2016b.\nCombes, R., Talebi Mazraeh Shahi, M. S., Proutiere, A.,\net al. Combinatorial bandits revisited. Advances in neural\ninformation processing systems, 28, 2015.\nFreedman, D. A. On tail probabilities for martingales. the\nAnnals of Probability, pp. 100–118, 1975.\nGai, Y., Krishnamachari, B., and Jain, R. Combinatorial net-\nwork optimization with unknown variables: Multi-armed\nbandits with linear rewards and individual observations.\nIEEE/ACM Transactions on Networking (TON), 20(5):\n1466–1478, 2012.\nHenderson, C. R. Best linear unbiased estimation and pre-\ndiction under a selection model. Biometrics, pp. 423–447,\n1975.\nKempe, D., Kleinberg, J., and Tardos, ´E. Maximizing the\nspread of influence through a social network. In Proceed-\nings of the ninth ACM SIGKDD international conference\non Knowledge discovery and data mining, pp. 137–146,\n2003.\nKveton, B., Wen, Z., Ashkan, A., and Szepesv´ari, C. Com-\nbinatorial cascading bandits. In Proceedings of the 28th\nInternational Conference on Neural Information Process-\ning Systems-Volume 1, pp. 1450–1458, 2015a.\nKveton, B., Wen, Z., Ashkan, A., and Szepesvari, C. Tight\nregret bounds for stochastic combinatorial semi-bandits.\nIn AISTATS, 2015b.\nLattimore, T. and Szepesv´ari, C. Bandit algorithms. Cam-\nbridge University Press, 2020.\nLattimore, T., Crammer, K., and Szepesv´ari, C. Linear multi-\nresource allocation with semi-bandit feedback. Advances\nin Neural Information Processing Systems, 28, 2015.\nLi, S., Wang, B., Zhang, S., and Chen, W. Contextual com-\nbinatorial cascading bandits. In International conference\non machine learning, pp. 1245–1253. PMLR, 2016.\nLi, S., Kong, F., Tang, K., Li, Q., and Chen, W. Online\ninfluence maximization under linear threshold model. Ad-\nvances in Neural Information Processing Systems, 33:\n1192–1204, 2020.\nLiu, L. T., Ruan, F., Mania, H., and Jordan, M. I. Bandit\nlearning in decentralized matching markets. Journal of\nMachine Learning Research, 22(211):1–34, 2021a.\nLiu, X., Zuo, J., Chen, X., Chen, W., and Lui, J. C.\nMulti-layered network exploration via random walks:\nFrom offline optimization to online learning. In Interna-\ntional Conference on Machine Learning, pp. 7057–7066.\nPMLR, 2021b.\nLiu, X., Zuo, J., Wang, S., Joe-Wong, C., Lui, J., and Chen,\nW. Batch-size independent regret bounds for combina-\ntorial semi-bandits with probabilistically triggered arms\nor independent arms. In Advances in Neural Information\nProcessing Systems, 2022.\nMerlis, N. and Mannor, S. Batch-size independent regret\nbounds for the combinatorial multi-armed bandit prob-\nlem. In Conference on Learning Theory, pp. 2465–2489.\nPMLR, 2019.\nQin, L., Chen, S., and Zhu, X. Contextual combinatorial\nbandit and its application on diversified online recommen-\ndation. In Proceedings of the 2014 SIAM International\nConference on Data Mining, pp. 461–469. SIAM, 2014.\nRobbins, H. Some aspects of the sequential design of exper-\niments. Bulletin of the American Mathematical Society,\n58(5):527–535, 1952.\nTakemura, K., Ito, S., Hatano, D., Sumita, H., Fukunaga, T.,\nKakimura, N., and Kawarabayashi, K.-i. Near-optimal\nregret bounds for contextual combinatorial semi-bandits\nwith linear payoff functions. In Proceedings of the AAAI\nConference on Artificial Intelligence, pp. 9791–9798,\n2021.\n10\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nVial, D., Shakkottai, S., and Srikant, R. Minimax regret for\ncascading bandits. In Advances in Neural Information\nProcessing Systems, 2022.\nWang, Q. and Chen, W. Improving regret bounds for com-\nbinatorial semi-bandits with probabilistically triggered\narms and its applications. In Advances in Neural Infor-\nmation Processing Systems, pp. 1161–1171, 2017.\nWen, Z., Kveton, B., Valko, M., and Vaswani, S. Online in-\nfluence maximization under independent cascade model\nwith semi-bandit feedback. Advances in neural informa-\ntion processing systems, 30, 2017.\nZhou, D., Gu, Q., and Szepesvari, C. Nearly minimax\noptimal reinforcement learning for linear mixture markov\ndecision processes. In Conference on Learning Theory,\npp. 4532–4576. PMLR, 2021.\nZong, S., Ni, H., Sung, K., Ke, N. R., Wen, Z., and Kveton,\nB. Cascading bandits for large-scale recommendation\nproblems. arXiv preprint arXiv:1603.05359, 2016.\nZuo, J., Liu, X., Joe-Wong, C., Lui, J. C., and Chen, W. On-\nline competitive influence maximization. In International\nConference on Artificial Intelligence and Statistics, pp.\n11472–11502. PMLR, 2022.\n11\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nAppendix\nThe Appendix is organized as follows. Appendix A gives the detailed proofs for theorems and lemmas in Section 3.\nAppendix B provides the detailed proofs for theorems and lemmas in Section 4. Appendix C shows how the triggering\nprobability equivalence technique can be applied to non-contextual CMAB-T to obtain improved results. Appendix D gives\nthe detailed settings, results and comparisons included in Table 2. Appendix E provides detailed experimental setups and\nadditional results. Appendix F summarizes the concentration bounds, facts, and technical lemmas used in this paper.\nA. Proofs for C2MAB-T under the TPM Condition (Section 3)\nA.1. Proof of Theorem 1\nWe first give/recall some definitions and events. Recall that in Algorithm 1, the gram matrix, the b-vector and the estimator\nare\nGt = γI +\nX\ns<t\nX\ni∈τs\nϕs(i)ϕs(i)⊤\n(12)\nbt =\nX\ns<t\nX\ni∈τs\nϕs(i)Xs,i\n(13)\nˆθt = G−1\nt bt.\n(14)\nLet us use Wt to denote the nice event when the oracle can output solution S with r(S; µ) ≥α · r(S∗; µ) where\nS∗= argmaxS∈S r(S; µ) for any µ at round t. We use Nt to denote the nice event when the\n\r\r\rˆθt −θ∗\r\r\r\nGt ≤ρ(δ) holds\nfor any t ∈[T]. Define the filtration to be Ft−1 = (S1, ϕ1, τ1, (X1,i)t∈τ1, ..., St−1, ϕt−1, τt−1, (Xt−1,i)t∈τt−1, St, ϕt)\nthat takes both history data Ht and action St to handle the randomness of the oracle, and let Et[·] = E[· | Ft−1]. Now we\nbound the regret under nice event Wt and Nt,\nReg(T)\n(a)\n= E\n\nX\nt∈[T ]\nEt[α · r(S∗\nt ; µt) −r(St; µt)]\n\n\n(b)\n≤E\n\nX\nt∈[T ]\nEt[α · r(S∗\nt ; ¯µt) −r(St; µt)]\n\n\n(c)\n≤E\n\nX\nt∈[T ]\nEt[r(St; ¯µt) −r(St; µt)]\n\n\n(d)\n≤E\n\nX\nt∈[T ]\nEt\n\nX\ni∈˜St\nB1pµt,St\ni\n(¯µt,i −µt,i)\n\n\n\n\n(e)\n= E\n\nX\nt∈[T ]\nEt\n\"X\ni∈τt\nB1(¯µt,i −µt,i)\n#\n\n(f)\n≤E\n\nX\nt∈[T ]\nEt\n\"X\ni∈τt\n2B1ρ(δ) ∥ϕt(i)∥G−1\nt\n#\n(g)\n= 2B1ρ(δ)E\n\nX\nt∈[T ]\nX\ni∈τt\n∥ϕt(i)∥G−1\nt\n\n\n(h)\n≤2B1ρ(δ)E\n\n\ns\nKT\nX\nt∈[T ]\nX\ni∈τt\n∥ϕt(i)∥2\nG−1\nt\n\n\n(i)\n≤O\n\u0010\nB1(\np\n2d log T + √γ)\np\n2KT log T\n\u0011\n≤O(B1d\n√\nKT log T).\n(15)\nwhere (a) follows from the regret definition and the tower rule, (b) is by Condition 1 and Lemma 1 saying that µt,i ≤¯µt,i,\n(c) is by nice event Wt and the definition of St, (d) is by Condition 2, (e) follows from by the TPE trick Lemma 4, (f)\nis by Lemma 1, (g) is by tower rule, (h) by Cauchy Schwarz inequality, and (i) is by the ellipsoidal potential lemma\n(Lemma 5). Similar to (Wang & Chen, 2017) The theorem is concluded by the definition of the (α, β)-approximate regret,\nand considering event ¬Wt or ¬Nt, which contributes to at most (1 −β)T∆max + δT∆max regret.\n12\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nA.2. Important Lemmas used for proving Theorem 1\nLemma 1. With probability at least 1 −δ, we have µt,i ≤¯µt,i ≤µt,i + 2ρ(δ) ∥ϕt(i)∥G−1\nt\nfor all i ∈[m], t ∈[T].\nProof. For any i ∈[m], t ∈[T], we have\n\f\f\f⟨ˆθt, ϕt(i)⟩−⟨θ∗, ϕt(i)⟩\n\f\f\f\n=\n\f\f\f⟨ˆθt −θ∗, ϕt(i)⟩\n\f\f\f\n(a)\n≤\n\r\r\rˆθt −θ∗\r\r\r\nGt · ∥ϕt(i)∥G−1\nt\n(b)\n≤ρ(δ) ∥ϕt(i)∥G−1\nt\n,\nwhere (a) by Cauchy-Schwartz, (b) by Proposition 1. Now use the definition of µt,i = ⟨θ∗, ϕt(i)⟩and ¯µt,i = ⟨ˆθt, ϕt(i)⟩+\nρ(δ) ∥ϕt(i)∥G−1\nt\nfinishes the proof.\n■\nLemma 4 (Triggering Probability Equivalence (TPE)). Et\nhP\ni∈˜St B1pµt,St\ni\n(¯µt,i −µt,i)\ni\n= Et\n\u0002P\ni∈τt B1(¯µt,i −µt,i)\n\u0003\n.\nProof. We have\nEt\n\nX\ni∈˜St\nB1pµt,St\ni\n(¯µt,i −µt,i)\n\n\n(a)\n= E\n\nX\ni∈˜St\nB1Eτt[I{i ∈τt}](¯µt,i −µt,i) | Ft−1\n\n\n(b)\n= Et\n\nX\ni∈˜St\nI{i ∈τt}B1(¯µt,i −µt,i)\n\n\n(c)\n= Et\n\"X\ni∈τt\nB1(¯µt,i −µt,i)\n#\n,\n(16)\n(a) is because ¯µt,i, µt,i, St are Ft−1-measurable so that the only randomness is from triggering set τt and we can substitute\npµt,St\ni\nwith event I{i ∈τt} under expectation, (b) is by absorbing the expectation over τt to Et, and (c) is a simple change\nof notation. Actually, TPE can be applied whenever the quantities (other than pD,S\ni\n) are Ft−1-measurable, which would be\nhelpful for later sections.\n■\nLemma 5 (Ellipsoidal Potential Lemma). PT\nt=1\nP\ni∈τt ∥ϕt(i)∥2\nG−1\nt\n≤2d log(1 + KT/(γd)) ≤2d log T when γ ≥K.\nProof.\ndet(Gt+1)\n(a)\n= det\n \nGt +\nX\ni∈τt\nϕt(i)ϕt(i)⊤\n!\n(b)\n= det(Gt) · det\n \nI +\nX\ni∈τt\nG−1/2\nt\nϕt(i)(G−1/2\nt\nϕt(i))⊤\n!\n(c)\n≥det(Gt) ·\n \n1 +\nX\ni∈τt\n∥ϕt(i)∥2\nG−1\nt\n!\n(d)\n≥det(γI)\ntY\ns=1\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n!\n,\n(17)\n13\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nwhere (a) follows from the definition, (b) follows from det(AB) = det(A) det(B) and A + B = A1/2(I +\nA−1/2BA−1/2)A1/2, (c) follows from Lemma 14, (d) follows from repeatedly applying (c).\nSince ∥ϕs(i)∥2\nG−1\ns\n≤∥ϕs(i)∥2\nλmin(Gs) ≤1/γ ≤1/K, we have P\ni∈τs ∥ϕs(i)∥2\nG−1\ns\n≤1. Using the fact that 2 log(1 + x) ≥x for\nany [0, 1], we have\nX\ns∈t\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n≤2\nt\nX\ns=1\nlog\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n!\n= 2 log\ntY\ns=1\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n!\n(a)\n≤2 log\n\u0012det(Gt+1)\ndet(γI)\n\u0013\n(b)\n≤2 log\n\u0012(γ + KT/d)d\nγd\n\u0013\n= 2d log(1 + KT/(γd)) ≤2d log(T),\nwhere the (a) follows from Equation (17), (b) follows from Lemma 15.\n■\nB. Proofs for C2MAB-T under the VM or TPVM Condition (Section 4)\nB.1. Proof of Lemma 2\nOur analysis is inspired by the derivation of Theorem 3 by (Lattimore et al., 2015) to bound the key ellipsoidal radius\n\r\r\rθ∗−ˆθt\n\r\r\r\nGt ≤ρ for the C2MAB-T setting, where multiple arms can be triggered in each round. Before we going into the\nmain proof, we first introduce some notations and events as follows.\nRecall that for t ≥1, Xt,i is a Bernoulli random variable with mean µt,i = ⟨θ∗, ϕt(i)⟩, suppose ∥θ∗∥2 ≤1, ∥ϕt(i)∥≤1,\nwe can represent Xt,i by Xt,i = µt,i + ηt,i, where noise ηt,i ∈[−1, 1], its mean E[ηt,i | Ft−1] = 0, and its variance\nVar[ηt,i | Ft−1] = µt,i(1 −µt,i). Also note that in Algorithm 2, the gram matrices, the b-vector and the weighted\nleast-square estimator are the following.\nGt = γ · I +\nt−1\nX\ns=1\nX\ni∈τs\n¯V −1\ns,i ϕs(i)ϕs(i)⊤,\n(18)\nbt =\nt−1\nX\ns=1\nX\ni∈τs\n¯V −1\ns,i ϕs(i)Xs,i,\n(19)\nˆθt = G−1\nt bt,\n(20)\nwhere we set G0 = γI, and optimistic variances ¯Vs,i are defined as in Equation (6) of Algorithm 2.\nLet us define Zt = P\ns<t\nP\ni∈τs ηs,iϕt(i)/ ¯Vs,i, and the key of this proof is to bound Zt (this quantity is often denoted as\nSt in the self-normalized bound (Abbasi-Yadkori et al., 2011), but St is occupied to denote actions at round t in this work).\nWe finally define failure events F0 ⊆F1 ⊆... ⊆FT be a sequence of events defined by\nFt = {∃s ≤t such that ∥Zs∥Gs + √γ ≥ρ}.\n(21)\nThese failure events are crucial in the sense that θ∗lies in the confidence ellipsoid\n\r\r\rθ∗−ˆθt\n\r\r\r\nGt ≤ρ (see Lemma 8 for its\nproof).\nNext, we can prove by induction that the probability of ∥Zt∥Gt +√γ ≥ρ given ¬Ft−1 is very small, for t = 1, ..., T (see its\nproof in Lemma 7). Based on this, we can have Pr[¬FT ] = 1−Pr[F0]−PT\nt=1 Pr\n\u0002\n∥Zt∥Gt + √γ ≥ρ and ¬Ft−1\n\u0003\n≥1−δ\n(as ¬F0 always holds), and thus by Equation (147), Lemma 2 is proved as desired.\n14\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nB.2. Proof of Theorem 2 under VM condition\nSimilar to Appendix A, we first give/recall some definitions and events. Recall that in Algorithm 2, the gram matrices,\nthe b-vector, and the weighted least-square estimator are defined in Equation (18) The optimistic variances ¯Vs,i are\ndefined as in Equation (6) of Algorithm 2. Let us use Wt to denote the nice event when the oracle can output solution\nS with r(S; µ) ≥α · r(S∗; µ) where S∗= argmaxS∈S r(S; µ) for any µ at round t. We use Nt to denote the nice\nevent when the\n\r\r\rˆθt −θ∗\r\r\r\nGt ≤ρ(δ) holds for any t ∈[T] (which can be implied by ¬FT ). Define the filtration to be\nFt−1 = (S1, ϕ1, τ1, (X1,i)t∈τ1, ..., St−1, ϕt−1, τt−1, (Xt−1,i)t∈τt−1, St, ϕt) that takes both history data Ht and action St\nto handle the randomness of the oracle, and let Et[·] = E[· | Ft−1].\nLet ˜µt be the vector whose i-th entry is the maximizer that achieves ¯Vt,i, i.e., ˜µt,i = argmaxµ∈[\n¯\nµt,i,¯µt,i] µ(1 −µ). Now we\nbound the regret under nice event Wt and N (where Nt can be implied from ¬FT by derivation in Lemma 8),\nReg(T)\n(a)\n= E\n\" T\nX\nt=1\nαr(S∗\nt ; µt) −r(St; µt)\n#\n(22)\n(b)\n≤E\n\" T\nX\nt=1\nαr(S∗\nt ; ¯µt) −r(St; µt)\n#\n(23)\n(c)\n≤E\n\" T\nX\nt=1\nr(St; ¯µt) −r(St; µt)\n#\n(24)\n(d)\n≤E\n\n\nT\nX\nt=1\n|r(St; ¯µt) −r(St; ˜µt)|\n|\n{z\n}\n(I)\n+ |r(St; µt) −r(St; ˜µt)|\n|\n{z\n}\n(II)\n\n,\n(25)\nwhere (a) is by definition, (b) follows from Condition 1 and Lemma 3, (c) from event W and the definition of St, (d) from\ntriangle inequality.\nNow we show how to bound term (I),\nE\n\nX\nt∈[T ]\n(I)\n\n\n(a)\n≤BvE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\n(¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(b)\n≤\nBv\n√pmin\n· E\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(c)\n≤\nBv\n√pmin\n· E\n\n\nv\nu\nu\ntT\nT\nX\nt=1\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(d)\n≤\nBv\n√pmin\n·\nv\nu\nu\nu\ntTE\n\n\nT\nX\nt=1\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(e)\n=\nBv\n√pmin\n·\nv\nu\nu\ntTE\n\" T\nX\nt=1\nX\ni∈τt\n(¯µt,i −˜µt,i)2\n¯Vt,i\n#\n(f)\n≤\nBv\n√pmin\n·\nv\nu\nu\ntTE\n\" T\nX\nt=1\nX\ni∈τt\n(6ρ(δ) ∥ϕt(i)∥G−1\nt )2\n¯Vt,i\n#\n(g)\n≤O(Bvd\n√\nT log(KT)/√pmin),\n(26)\nwhere (a) follows from Condition 3, (b) follows from the definition of pmin s.t. pµt,St\ni\n≥pmin for i ∈˜St, (c) follows from\n15\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nCauchy–Schwarz, (d) follows from Jensen’s inequality, (e) follows from the TPE trick, (f) follows from Lemma 3, (g)\nfollows from Lemma 6.\nNow for the term (II)≤O(Bvd\n√\nT log(KT)/√pmin) follows from the similar derivation of Equation (26) by replacing\n(¯µt,i −˜µt,i)2 with (µt,i −˜µt,i)2. And the theorem is concluded by considering ¬Wt and ¬Nt, similar to Appendix A.\nLemma 6 (Weighted Ellipsoidal Potential Lemma). PT\nt=1\nP\ni∈τt ∥ϕt(i)∥2\nG−1\nt\n/ ¯Vt,i ≤2d log(1 + KT/(γd)) ≤2d log T\nwhen ¬FT and γ ≥4K.\nProof.\ndet(Gt+1)\n(a)\n= det\n \nGt +\nX\ni∈τt\nϕt(i)ϕt(i)⊤/ ¯Vt,i\n!\n(b)\n= det(Gt) · det\n \nI +\nX\ni∈τt\nG−1/2\nt\nϕt(i)(G−1/2\nt\nϕt(i))⊤/ ¯Vt,i\n!\n(c)\n≥det(Gt) ·\n \n1 +\nX\ni∈τt\n∥ϕt(i)∥2\nG−1\nt\n/ ¯Vt,i\n!\n(d)\n≥det(γI)\ntY\ns=1\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n/ ¯Vt,i\n!\n,\n(27)\nwhere (a) follows from the definition, (b) follows from det(AB) = det(A) det(B) and A + B = A1/2(I +\nA−1/2BA−1/2)A1/2, (c) follows from Lemma 14, (d) follows from repeatedly applying (c).\nIf ¯Vs,i =\n1\n4, ∥ϕs(i)∥2\nG−1\ns\n/ ¯Vs,i ≤\n4∥ϕs(i)∥2\nλmin(Gs)\n≤4/γ ≤1/K, else if ¯Vs,i <\n1\n4, and since ¬FT , by Lemma 9,\n∥ϕs(i)∥2\nG−1\ns\n/ ¯Vs,i ≤\n1\nρ(δ)\n1\n√γ ≤\n1\nγ ≤1/(4K). Therefore, we have P\ni∈τs ∥ϕs(i)∥2\nG−1\ns\n≤1. Using the fact that\n2 log(1 + x) ≥x for any [0, 1], we have\nX\ns∈t\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n/ ¯Vs,i\n≤2\nt\nX\ns=1\nlog\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n/ ¯Vs,i\n!\n= 2 log\ntY\ns=1\n \n1 +\nX\ni∈τs\n∥ϕs(i)∥2\nG−1\ns\n/ ¯Vs,i\n!\n(a)\n≤2 log\n\u0012det(Gt+1)\ndet(γI)\n\u0013\n(b)\n≤2 log\n\u0012(γ + KT/d)d\nγd\n\u0013\n= 2d log(1 + 4dK2T 2/(γd)) ≤4d log(KT),\nwhere the (a) follows from Equation (17), (b) follows from Lemma 15 by setting L = ∥ϕs(i)∥2 / ¯Vs,i ≤4dKs (from\nLemma 11).\n■\nB.3. Proof of Theorem 3 Under TPVM Condition\nIn this section, we consider two cases when λ ≥2 and λ ≥1. Recall that to use the TPVM condition (Condition 4), we\nneed one additional condition over the triggering probability (Condition 5).\n16\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nB.3.1. WHEN λ ≥2:\nWe inherit the same notation and events as in Appendix A.1, and start to bound term (I) in Equation (26) differently,\nE\n\nX\nt∈[T ]\n(I)\n\n\n(a)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nt\nX\ni∈˜St\n(p˜µt,St\ni\n)2 (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(28)\n(b)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nu\nt\nX\ni∈˜St\n\npµt,St\ni\n+ min\n\n\n1,\nX\nj∈˜St\nBppµ,St\nj\n|µt,j −˜µt,j|\n\n\n\n\n\n2\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(29)\n(c)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nt\nX\ni∈˜St\n3pµt,St\ni\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n+ E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nu\nt\nX\ni∈˜St\n\nX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n\n2\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(30)\n(d)\n= O(Bvd\n√\nT log(KT)) + BvE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\nX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n\n(31)\n(e)\n≤O(Bvd\n√\nT log(KT)) + Bv\n1\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\nX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n\n(32)\n(f)\n≤O(Bvd\n√\nT log(KT)) + Bv\n1\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\ns\nK\nX\nj∈˜St\nB2ppµt,St\nj\n|µt,j −˜µt,j|2\n\n\n(33)\n(g)\n≤O(Bvd\n√\nT log(KT)) + BvBp\n√\nK\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\nsX\nj∈˜St\npµt,St\nj\n|µt,j −˜µt,j|2 / ¯Vt,j\n\n(34)\n(h)\n≤O(Bvd\n√\nT log(KT)) + BvBp\n√\nK\n√pmin\nE\n\n\nT\nX\nt=1\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −\n¯\nµt,i)2\n¯Vt,i\n\n\n(35)\n(i)\n≤O(Bvd\n√\nT log(KT) + BvBp\n√\nK\n√pmin\n(d log(KT))2) = O(Bvd\n√\nT log(KT)),\n(36)\nwhere (a) follows from Condition 4, (b) is by applying Condition 5 for triggering probability p¯µt, ˜St\ni\nand p¯µt, ˜St\ni\n, pµt, ˜St\ni\n≤1,\n(c) follows from\n√\na + b ≤√a +\n√\nb, (d) follows from same derivation from Equation (26), (e) follows from p˜µt,St\ni\n≥pi\nmin,\n(f) follows from Cauchy-Schwarz, (g) follows from ¯Vt,j ≤1/4, (h) follows from ˜µt,i, µt,i ∈[¯µt,i,\n¯\nµt,i] by event Nt, (i)\nfollows from the similar analysis of (d)-(g) in Equation (26) inside the square-root without considering the additional\nBv\n√\nT/√pmin.\nFor the term (II), one can easily verify it follows from the similar deviation of the term (I) with the difference in constant\nterms. And Theorem 3 is concluded by considering small probability ¬Wt and Nt events.\nB.3.2. WHEN λ ≥1:\nWe inherit the same notation and events as in Appendix A.1, and start to bound term (I) in Equation (28) as follows,\nE\n\nX\nt∈[T ]\n(I)\n\n\n(a)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nt\nX\ni∈˜St\n(p˜µt,St\ni\n)(¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(37)\n17\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(b)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nu\nt\nX\ni∈˜St\n\npµt,St\ni\n+ min\n\n\n1,\nX\nj∈˜St\nBppµ,St\nj\n|µt,j −˜µt,j|\n\n\n\n\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(38)\n(c)\n≤E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n+ E\n\n\nT\nX\nt=1\nBv\nv\nu\nu\nu\nt\nX\ni∈˜St\n\nX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n· (¯µt,i −˜µt,i)2\n¯Vt,i\n\n\n(39)\n(d)\n= O(Bvd\n√\nT log(KT)) + BvE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\nsX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n\n(40)\n(e)\n≤O(Bvd\n√\nT log(KT)) + Bv\n1\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\nsX\nj∈˜St\nBppµt,St\nj\n|µt,j −˜µt,j|\n\n\n(41)\n(f)\n≤O(Bvd\n√\nT log(KT)) + Bv\n1\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\n\nK\nX\nj∈˜St\nB2\nppµt,St\nj\n|µt,j −˜µt,j|2\n\n\n1/4\n\n(42)\n(g)\n≤O(Bvd\n√\nT log(KT)) + Bv\np\nBp\nK1/4\n√pmin\nE\n\n\nT\nX\nt=1\nv\nu\nu\nt\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −˜µt,i)2\n¯Vt,i\n·\n\nX\nj∈˜St\npµt,St\nj\n|µt,j −˜µt,j|2 / ¯Vt,j\n\n\n1/4\n\n(43)\n(h)\n≤O(Bvd\n√\nT log(KT)) + Bv\np\nBp\nK1/4\n√pmin\nE\n\n\nT\nX\nt=1\n\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −\n¯\nµt,i)2\n¯Vt,i\n\n\n3/4\n\n(44)\n(i)\n≤O(Bvd\n√\nT log(KT)) + Bv\np\nBp\n(KT)1/4\n√pmin\nE\n\n\n\n\nT\nX\nt=1\nX\ni∈˜St\npµt,St\ni\n(¯µt,i −\n¯\nµt,i)2\n¯Vt,i\n\n\n3/4\n\n(45)\n(j)\n≤O\n\u0012\nBvd\n√\nT log(KT) + Bv\np\nBp\n(KT)1/4\n√pmin\n(d log(KT))3/2\n\u0013\n= O(Bvd\n√\nT log(KT)),\n(46)\nwhere (a) follows from Condition 4, (b) is by applying Condition 5 for triggering probability p¯µt, ˜St\ni\nand p¯µt, ˜St\ni\n, pµt, ˜St\ni\n≤1,\n(c) follows from\n√\na + b ≤√a +\n√\nb, (d) follows from same derivation from Equation (26), (e) follows from p˜µt,St\ni\n≥pi\nmin,\n(f) follows from Cauchy-Schwarz, (g) follows from ¯Vt,j ≤1/4, (h) follows from ˜µt,i, µt,i ∈[¯µt,i,\n¯\nµt,i] by event Nt, (i)\nfollows from Holder’s inequality with p = 4, q = 4/3, (j) follows from the similar analysis of (d)-(g) in Equation (26) inside\nthe square-root without considering the additional Bv\n√\nT/√pmin.\nFor the term (II), one can easily verify it follows from the similar deviation of the term (I) with the difference in constant\nterms. And Theorem 3 is concluded by considering small probability ¬Wt and Nt events.\nC. Proofs for TPE Trick to Improve Non-Contextual CMAB-T\nWe first introduce some definitions that are used in (Wang & Chen, 2017) and (Liu et al., 2022). Recall that non-contextual\nCMAB-T is a degenerate case when ϕt(i) = ei and θ∗= µ, where µ ≜EXt∼D[Xt | Ht] is the mean of the true outcome\ndistribution D.\nDefinition 1 ((Approximation) Gap). Fix a distribution D ∈D and its mean vector µ, for each action S ∈S, we define the\n18\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(approximation) gap as ∆S = max{0, αr(S∗; µ) −r(S; µ)}. For each arm i, we define ∆min\ni\n= infS∈S:pD,S\ni\n>0, ∆S>0 ∆S,\n∆max\ni\n= supS∈S:pD,S\ni\n>0,∆S>0 ∆S. As a convention, if there is no action S ∈S such that pD,S\ni\n> 0 and ∆S > 0, then\n∆min\ni\n= +∞, ∆max\ni\n= 0. We define ∆min = mini∈[m] ∆min\ni\nand ∆max = maxi∈[m] ∆max\ni\n.\nDefinition 2 (Event-Filtered Regret). For any series of events (Et)t∈[T ] indexed by round number t, we define the\nRegA\nα,µ(T, (Et)t∈[T ]) as the regret filtered by events (Et)t∈[T ], or the regret is only counted in t if E happens in t. Formally,\nRegA\nα,µ(T, (Et)t∈[T ]) = E\n\nX\nt∈[T ]\nI(Et)(α · r(S∗; µ) −r(St; µ))\n\n.\n(47)\nFor simplicity, we will omit A, α, µ, t ∈[T] and rewrite RegA\nα,µ(T, (Et)t∈[T ]) as Reg(T, Et) when contexts are clear.\nC.1. Reproducing Theorem 1 of (Wang & Chen, 2017) under 1-norm TPM Condition\nTheorem 4. For a CMAB-T problem instance ([m], S, D, Dtrig, R) that satisfies monotonicity (Condition 1), and TPM\nbounded smoothness (Condition 2) with coefficient B1, if λ ≥1, CUCB (Wang & Chen, 2017) with an (α, β)-approximation\noracle achieves an (α, β)-approximate distribution-dependent regret bounded by\nReg(T) ≤\nX\ni∈[m]\n48B2\n1K log T\n∆min\ni\n+ 2B1m + π2\n3 · ∆max.\n(48)\nAnd the distribution-independent regret,\nReg(T) ≤14B1\np\nmKT log T + 2B1m + π2\n3 · ∆max.\n(49)\nThe main idea is to use TPE trick to replace ˜St (arms that could be probabilistically triggered by action St) with τt (arms\nthat are actually triggered by action St) under conditional expectation, so that we can use the simpler Appendix B.2 of Wang\n& Chen (2017) to avoid the much more involved Appendix B.3 of Wang & Chen (2017). Such replacement bypasses the\ntriggering group analysis (and its counter Nt,i,j) in Appendix B.3, which uses Nt,i,j to associate Tt,i with the counters for\n˜St. For our simplified analysis, we can directly associate the Tt,i with the arm triggering for the arms τt that are actually\ntriggered/observed and eventually reproduce the regret bounds of (Wang & Chen, 2017).\nWe follow exactly the same CUCB algorithm (Algorithm 1 (Wang & Chen, 2017)), conditions (Condition 1, 2 (Wang\n& Chen, 2017)).\nWe also inherit the event definitions of N s\nt (Definition 4 (Wang & Chen, 2017)) that for every\narm i ∈[m], |ˆµt−1,i −µi| < ρt,i =\nq\n3 log t\n2Tt−1,i , and the event Ft being {r(St; ¯µt) < α · opt(¯µt)}.\nLet us fur-\nther denote ∆St = αr(S∗; µ) −r(St; µ), τt be the arms actually triggered by St at time t. Let filtration Ft−1 be\n(ϕ1, S1, τ1, (X1,i)i∈τ1, ..., ϕt−1, St−1, τt−1, (Xt−1,i)i∈τt−1, ϕt, St), and let Et[·] = E[· | Ft−1] We also have that Ft−1,\nTt−1,i, ˆµt,i are measurable. Also note that we use pD,S\ni\nto denote the triggering probability pµ,S\ni\nfor any i ∈[m], S ∈S in\norder to match the notation of Wang & Chen (2017).\nProof. Under event N s\nt and ¬Ft, and given filtration Ft−1, we have\n∆St\n(a)\n≤B1\nX\ni∈[m]\npD,St\ni\n(¯µt,i −µi)\n(50)\n(b)\n≤−∆St + 2B1\nX\ni∈[m]\npD,St\ni\n(¯µt,i −µi)\n(51)\n= −\nP\ni∈[m] pD,St\ni\n∆St\nP\ni∈[m] pD,St\ni\n+ 2B1\nX\ni∈[m]\npD,St\ni\n(¯µt,i −µi)\n(52)\n(c)\n≤2B1\nX\ni∈[m]\npD,St\ni\n\u0012\n−∆min\ni\n2B1K + (¯µt,i −µi)\n\u0013\n(53)\n19\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(d)\n≤2B1\nX\ni∈[m]\npD,St\ni\n \n−∆min\ni\n2B1K + min\n(\n1,\ns\n6 log T\nTt−1,i\n)!\n,\n(54)\nwhere (a) follows from exactly the Equation (10) of Appendix B.3 in Wang & Chen (2017), (b) is by the reverse amortization\ntrick that multiplies two to both sides of (a) and rearranges the terms, (c) is by pD,St\ni\n≤1 and ∆min\ni\n≤∆St, (d) by event N s\nt\nso that (¯µt,i −µi) ≤min{1, 2ρt,i} =\nn\n1,\nq\n6 log T\nTt−1,i\no\n.\nLet\nκi,T (ℓ) =\n\n\n\n\n\n\n\n2B1,\nif ℓ= 0,\n2B1\nq\n6 log T\nℓ\n,\nif 1 ≤ℓ≤Li,T ,\n0,\nif ℓ> Li,T ,\n(55)\nwhere Li,T = 24B2\n1K2 log T\n(∆min\ni\n)2\n.\nIt follows that\n∆St = Et[∆St]\n(a)\n≤Et\n\n2B1\nX\ni∈[m]\npD,St\ni\n \n−∆min\ni\n2B1K + min\n(\n1,\ns\n6 log T\nTt−1,i\n)!\n\n(56)\n(b)\n= Et\n\n2B1\nX\ni∈[m]\nI{i ∈τt}\n \n−∆min\ni\n2B1K + min\n(\n1,\ns\n6 log T\nTt−1,i\n)!\n\n(57)\n= Et\n\"\n2B1\nX\ni∈τt\n \n−∆min\ni\n2B1K + min\n(\n1,\ns\n6 log T\nTt−1,i\n)!#\n(58)\n(c)\n≤Et\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n,\n(59)\nwhere (a) follows from Equation (54), (b) follows from the TPE trick to replace pD,St\ni\n= Et[I{i ∈τt}], (c) follows from that if\nTt−1,i ≤Li,T , we have min{\nq\n6 log T\nTt−1,i , 1} ≤\n1\n2B1 κi,T (Tt−1,i), and if Tt−1,i ≥Li,T + 1, then min\nn\n1,\nq\n6 log T\nTt−1,i\no\n≤∆min\ni\n2B1K ,\nso −∆min\ni\n2B1K + min\nn\n1,\nq\n6 log T\nTt−1,i\no\n≤0 = κi,T (Tt−1,i).\nNow we apply the definition of the event-filtered regret,\nReg(N s\nt , ¬Ft) = E\n\" T\nX\nt=1\n∆St\n#\n(60)\n(a)\n≤E\n\" T\nX\nt=1\nEt\n\"X\ni∈τt\nκi,T (Tt−1,i)\n##\n(61)\n(b)\n= E\n\" T\nX\nt=1\nX\ni∈τt\nκi,T (Tt−1,i)\n#\n(62)\n(c)\n= E\n\nX\ni∈[m]\nTT −1,i\nX\ns=0\nκi,T (s)\n\n\n(63)\n≤\nX\ni∈[m]\nLi,T\nX\ns=0\nκi,T (s)\n(64)\n= 2B1m +\nX\ni∈[m]\nLi,T\nX\ns=1\n2B1\nr\n6 log T\ns\n(65)\n20\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(d)\n≤2B1m +\nX\ni∈[m]\nZ Li,T\ns=0\n2B1\nr\n6 log T\ns\n· ds\n(66)\n≤2B1m +\nX\ni∈[m]\n48B2\n1K log T\n∆min\ni\n,\n(67)\nwhere (a) follows from Equation (59), (b) follows from the tower rule, (c) follows from that Tt−1,i is increased by 1 if\nand only if i ∈τt, (d) is by the sum & integral inequality\nR U\nL−1 f(x)dx ≥PU\ni=L f(i) ≥\nR U+1\nL\nf(x)dx for non-increasing\nfunction f.\nFollowing Wang & Chen (2017) to handle small probability events ¬N s\nt and Ft, we have\nReg(T) ≤\nX\ni∈[m]\n48B2\n1K log T\n∆min\ni\n+ 2B1m + π2\n3 · ∆max,\n(68)\nand the distribution-independent regret is\nReg(T) ≤14B1\np\nmKT log T + 2B1m + π2\n3 · ∆max.\n(69)\n■\nC.2. Improving Theorem 1 of (Liu et al., 2022) under TPVM Condition\nWe first show the regret bound of using our TPE technique in Theorem 5 and its prior result in Proposition 2.\nTheorem 5. For a CMAB-T problem instance ([m], S, D, Dtrig, R) that satisfies monotonicity (Condition 1), and TPVM\nbounded smoothness (Condition 4) with coefficient (Bv, B1, λ), if λ ≥1, BCUCB-T (Liu et al., 2022) with an (α, β)-\napproximation oracle achieves an (α, β)-approximate distribution-dependent regret bounded by\nO\n\nX\ni∈[m]\nB2\nv log K log T\n˜∆min\ni,λ\n+\nX\ni∈[m]\nB1 log\n\u0012 B1K\n∆min\ni\n\u0013\nlog T\n\n,\n(70)\nwhere ˜∆min\ni,λ = minS:pD,S\ni\n>0,∆S>0 ∆St/(pD,St\ni\n)λ−1. And the distribution-independent regret,\nReg(T) ≤O\n\u0010\nBv\np\nm(log K)T log T + B1m log(KT) log T\n\u0011\n.\n(71)\nProposition 2 (Theorem 1, Liu et al. (2022)). For a CMAB-T problem instance ([m], S, D, Dtrig, R) that satisfies mono-\ntonicity (Condition 1), and TPVM bounded smoothness (Condition 4) with coefficient (Bv, B1, λ), if λ ≥1, BCUCB-T with\nan (α, β)-approximation oracle achieves an (α, β)-approximate regret bounded by\nO\n\nX\ni∈[m]\nlog\n\u0012BvK\n∆min\ni\n\u0013 B2\nv log K log T\n∆min\ni\n+\nX\ni∈[m]\nB1 log2\n\u0012 B1K\n∆min\ni\n\u0013\nlog T\n\n.\n(72)\nAnd the distribution-independent regret,\nReg(T) ≤O\n\u0010\nBv\np\nm(log K)T log(KT) + B1m log2(KT) log T\n\u0011\n.\n(73)\nLooking at our regret bound (Theorem 5), there are two improvements compared with Proposition 2: (1) the min gap is\nimproved to ˜∆min\ni,λ ≥∆min\ni\n, (2) we remove a O(log( BvK\n∆min\ni\n)) for the leading term. For (2), it translates to a O(√log T)\nimprovement for the distribution-independent bound.\nProof. Similar to Appendix C.1, the main idea is to use TPE trick to replace ˜St (arms that could be probabilistically\ntriggered by action St) with τt (arms that are actually triggered by action St) under conditional expectation to avoid the\n21\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nusage of much more involved triggering group analysis (Wang & Chen, 2017). Such replacement bypasses the triggering\ngroup analysis (and its counter Nt,i,j) (Liu et al., 2022), which uses Nt,i,j to associate Tt,i with the counters for ˜St. By\ndoing so, we do not need a union bound over the group index j, which saves a log(BvK/∆min\ni\n(or log(B1K/∆min\ni\n) factor.\nWe follow exactly the same BCUCB-T algorithm (Algorithm 1 (Liu et al., 2022)), conditions (Condition 1, 2, 3 (Liu et al.,\n2022)). We also inherit the event definitions of N s\nt (Definition 6 (Liu et al., 2022)) that (1) for every base arm i ∈[m],\n|ˆµt−1,i −µi| ≤ρt,i, where ρt,i =\nr\n6 ˆVt−1,i log t\nTt−1,i\n+ 9 log t\nTt−1,i ; (2) for every base arm i ∈[m], ˆVt−1,i ≤2µi(1 −µi) + 3.5 log t\nTt−1,i .\nWe use the event Ft being {r(St; ¯µt) < α · opt(¯µt)}. Let us further denote ∆St = αr(S∗; µ) −r(St; µ), τt be the arms\nactually triggered by St at time t. Let filtration Ft−1 be (ϕ1, S1, τ1, (X1,i)i∈τ1, ..., ϕt−1, St−1, τt−1, (Xt−1,i)i∈τt−1, ϕt, St),\nand let Et[·] = E[· | Ft−1] We also have that Ft−1, Tt−1,i, ˆµt,i are measurable. Also note that we use pD,S\ni\nto denote the\ntriggering probability pµ,S\ni\nfor any i ∈[m], S ∈S in order to match the notation of Wang & Chen (2017); Liu et al. (2022).\nWe follow the same regret decomposition as in Lemma 9 of Liu et al. (2022), to decompose the event-filtered regret\nReg(T, N s\nt , ¬Ft) into two event-filtered regret Reg(T, Et,1) and Reg(T, Et,2) under events N s\nt , ¬Ft.\nReg(T) ≤Reg(T, Et,1) + Reg(T, Et,2),\n(74)\nwhere\nevent\nEt,1\n=\n{∆St\n≤\n2et,1(St)},\nevent\nEt,2\n=\n{∆St\n≤\n2et,2(St)},\net,1(St)\n=\n4\n√\n3Bv\nqP\ni∈˜St( log t\nTt−1,i ∧1\n28)(pD,St\ni\n)λ,et,2(St) = 28B1\nP\ni∈˜St( log t\nTt−1,i ∧1\n28)(pD,St\ni\n).\nC.2.1. BOUNDING THE Reg(T, Et,1) TERM\nWe bound the leading Reg(T, Et,1) term under two cases when λ ∈[1, 2) and λ ∈[2, ∞).\n(a) When λ ∈[1, 2),\nLet c1 = 4\n√\n3, ˜∆St = ∆St/(pD,S\ni\n)λ−1. Given filtration Ft−1 and event Et,1, we have\n∆St\n(a)\n≤\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(75)\n(b)\n= −∆St + 2\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(76)\n= −\nP\ni∈˜St pD,St\ni\n∆St/(pD,St\ni\n)λ−1\nP\ni∈[m](pD,St\ni\n)2−λ\n+ 2\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(77)\n(c)\n≤\nX\ni∈[m]\npD,St\ni\n \n8c2\n1B2\nv\nlog t\nTt−1,i\n∆St/(pD,St\ni\n)λ−1 −∆St/(pD,St\ni\n)λ−1\nK\n!\n(78)\n(d)\n=\nX\ni∈[m]\npD,St\ni\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St\n−\n˜∆St\nK\n!\n,\n(79)\nwhere (a) follows from event Et,1, (b) is by the reverse amortization trick that multiplies two to both sides of (a) and\nrearranges the terms, (c), (d) are by definition of K, ˜∆St.\nIt follows that\n∆St = Et[∆St]\n(a)\n≤Et\n\nX\ni∈[m]\npD,St\ni\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St\n−\n˜∆St\nK\n!\n\n(80)\n(b)\n= Et\n\"X\ni∈τt\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St\n−\n˜∆St\nK\n!#\n(81)\n22\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(c)\n≤Et\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n(82)\nwhere (a) follows from Equation (79), (b) follows from TPE trick to replace pD,St\ni\n= Et[I{i ∈τt}], (c) is because we define\na regret allocation function\nκi,T (ℓ) =\n\n\n\n\n\n\n\n\n\n\n\n\n\nc2\n1B2\nv\n˜∆min\ni\n,\nif ℓ= 0,\n2\nq\n4c2\n1B2v log T\nℓ\n,\nif 1 ≤ℓ≤Li,T,1,\n8c2\n1B2\nv log T\n˜∆min\ni\n1\nℓ,\nif Li,T,1 < ℓ≤Li,T,2,\n0,\nif ℓ> Li,T,2,\n(83)\nwhere Li,T,1 = 4c2\n1B2\nv log T\n( ˜∆min\ni\n)2 , Li,T,2 = 8c2\n1B2\nvK log T\n( ˜∆min\ni\n)2\n, ˜∆min\ni\n= minS:pD,S\ni\n>0,∆S>0 ∆St/(pD,St\ni\n)λ−1, and (c) holds due to\nLemma 16.\nReg(T, Et,1) = E\n\" T\nX\nt=1\n∆St\n#\n(84)\n(a)\n≤E\n\nX\nt∈[T ]\nEt\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n\n(85)\n(b)\n= E\n\nX\nt∈[T ]\nX\ni∈τt\nκi,T (Tt−1,i)\n\n\n(86)\n(c)\n= E\n\nX\ni∈[m]\nTT −1,i\nX\ns=0\nκi,T (s)\n\n\n(87)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni\n+\nX\ni∈[m]\nLi,T,1\nX\ns=1\n2\nr\n4c2\n1B2v log T\ns\n+\nX\ni∈[m]\nLi,T,2\nX\ns=Li,T,1+1\n8c2\n1B2\nv log T\n˜∆min\ni\n1\ns\n(88)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni\n+\nX\ni∈[m]\nZ Li,T,1\ns=0\n2\nr\n4c2\n1B2v log T\ns\n· ds +\nX\ni∈[m]\nZ Li,T,2\ns=Li,T,1\n8c2\n1B2\nv log T\n˜∆min\ni\n1\ns · ds\n(89)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni\n+\nX\ni∈[m]\n8c2\n1B2\nv log T\n˜∆min\ni\n(3 + log K),\n(90)\nwhere (a) follows from Equation (82), (b) follows from the tower rule, (c) follows from that Tt−1,i is increased by 1 if and\nonly if i ∈τt.\n(b) When λ ∈[2, ∞),\nLet ˜∆St,λ = ∆St/(pD,St\ni\n)λ−1, ˜∆St = ∆St/pD,St\ni\n. Note that ˜∆S,λ = ˜∆S when λ = 2, ˜∆S,λ ≥˜∆S when λ ≥2, and\n˜∆S,λ ≤˜∆S, when λ ≤2, for any i, S. Given filtration Ft−1 and under event Et,1, we have\n∆St ≤\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(91)\n= −∆St + 2\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(92)\n= −\nP\ni∈˜St pD,St\ni\n∆St/pD,St\ni\nK\n+ 2\nX\ni∈[m]\n4c2\n1B2\nv(pD,St\ni\n)λ log t\nTt−1,i\n∆St\n(93)\n23\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n≤\nX\ni∈[m]\npD,St\ni\n \n8c2\n1B2\nv\nlog t\nTt−1,i\n∆St/(pD,St\ni\n)λ−1 −∆St/pD,St\ni\nK\n!\n(94)\n=\nX\ni∈[m]\npD,St\ni\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St,λ\n−\n˜∆St\nK\n!\n.\n(95)\n∆St = Et[∆St] ≤Et\n\nX\ni∈[m]\npD,St\ni\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St,λ\n−\n˜∆St\nK\n!\n\n(96)\n= Et\n\"X\ni∈τt\n 8c2\n1B2\nv\nlog t\nTt−1,i\n˜∆St,λ\n−\n˜∆St\nK\n!#\n(97)\n≤Et\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n(98)\nwhere the last inequality is by Lemma 17 and we define a regret allocation function\nκi,T (ℓ) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc2\n1B2\nv\n˜∆min\ni,λ ,\nif ℓ= 0,\n2\nq\n4c2\n1B2v log T\nℓ\n,\nif 1 ≤ℓ≤Li,T,1,\n8c2\n1B2\nv log T\n˜∆min\ni,λ\n1\nℓ,\nif Li,T,1 < ℓ≤Li,T,2,\n0,\nif ℓ> Li,T,2,\n(99)\nwhere Li,T,1\n=\n4c2\n1B2\nv log T\n˜∆min\ni\n· ˜∆min\ni,λ ,\nLi,T,2\n=\n8c2\n1B2\nvK log T\n˜∆min\ni\n· ˜∆min\ni,λ\n,\n˜∆min\ni\n=\nminS:pD,S\ni\n>0,∆S>0 ∆St/pD,St\ni\n,\n˜∆min\ni,λ\n=\nminS:pD,S\ni\n>0,∆S>0 ∆St/(pD,St\ni\n)λ−1.\nReg(T, Et,1) = E\n\" T\nX\nt=1\n∆St\n#\n(100)\n(a)\n≤E\n\nX\nt∈[T ]\nEt\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n\n(101)\n(b)\n= E\n\nX\nt∈[T ]\nX\ni∈τt\nκi,T (Tt−1,i)\n\n\n(102)\n(c)\n= E\n\nX\ni∈[m]\nTT −1,i\nX\ns=0\nκi,T (s)\n\n\n(103)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni\n+\nX\ni∈[m]\nLi,T,1\nX\ns=1\n2\nr\n4c2\n1B2v log T\ns\n+\nX\ni∈[m]\nLi,T,2\nX\ns=Li,T,1+1\n8c2\n1B2\nv log T\n˜∆min\ni\n1\ns\n(104)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni\n+\nX\ni∈[m]\nZ Li,T,1\ns=0\n2\nr\n4c2\n1B2v log T\ns\n· ds +\nX\ni∈[m]\nZ Li,T,2\ns=Li,T,1\n8c2\n1B2\nv log T\n˜∆min\ni,λ\n1\ns · ds\n(105)\n≤\nX\ni∈[m]\nc2\n1B2\nv\n˜∆min\ni,λ\n+\nX\ni∈[m]\n8c2\n1B2\nv log T\n˜∆min\ni,λ\n(1 + log K) +\nX\ni∈[m]\n16c2\n1B2\nv log T\nq\n˜∆min\ni,λ · ˜∆min\ni\n,\n(106)\nwhere (a) follows from Equation (98), (b) follows from the tower rule, (c) follows from that Tt−1,i is increased by 1 if and\nonly if i ∈τt.\n24\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nC.2.2. BOUNDING THE Reg(T, Et,2) TERM\nLet c2 = 28. Given filtration Ft−1 and event Et,2, we have\n∆St\n(a)\n≤\nX\ni∈˜St\n2c2B1pD,St\ni\nmin\n\u001a\n1/28, log T\nTt−1,i\n\u001b\n(b)\n≤−∆St + 2\nX\ni∈˜St\n2c2B1pD,St\ni\nmin\n\u001a\n1/28, log T\nTt−1,i\n\u001b\n= −\nP\ni∈[m] pD,St\ni\n∆St\nP\ni∈[m] pD,St\ni\n+ 2\nX\ni∈[m]\n2c2B1pD,St\ni\nmin\n\u001a\n1/28, log T\nTt−1,i\n\u001b\n(107)\n(c)\n≤\nX\ni∈[m]\npD,St\ni\n\u0012\n−∆St\nK\n+ 4c2B1 min\n\u001a\n1/28, log T\nTt−1,i\n\u001b\u0013\n,\n(108)\nwhere (a) follows from event Et,2, (b) is by the reverse amortization trick that multiplies two to both sides of (a) and\nrearranges the terms, (c) follows from pD,St\ni\n≤1.\nIt follows that\n∆St = Et[∆St]\n(a)\n≤Et\n\nX\ni∈[m]\npD,St\ni\n\u0012\n−∆St\nK\n+ 4c2B1 min\n\u001a\n1/28, log T\nTt−1,i\n\u001b\u0013\n\n(b)\n= Et\n\"X\ni∈τt\n\u0012\n−∆St\nK\n+ 4c2B1 min\n\u001a\n1/28, log T\nTt−1,i\n\u001b\u0013#\n(c)\n≤Et\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n(109)\nregret allocation function\nκi,T (ℓ) =\n\n\n\n\n\n∆max\ni\n,\nif 0 ≤ℓ≤Li,T,1\n4c2B1 log T\nℓ\n,\nif Li,1 < ℓ≤Li,2\n0,\nif ℓ> Li,T,2 + 1,\n(110)\nwhere Li,T,1 = 4c2B1 log T\n∆max\ni\n, Li,T,2 = 4c2B1K log T\n∆min\ni\n. And (a) follows from Equation (108), (b) from the TPE, (c) follows\nfrom Lemma 18.\nReg(T, Et,2) = E\n\" T\nX\nt=1\n∆St\n#\n(111)\n(a)\n≤E\n\nX\nt∈[T ]\nEt\n\"X\ni∈τt\nκi,T (Tt−1,i)\n#\n\n(112)\n(b)\n= E\n\nX\nt∈[T ]\nX\ni∈τt\nκi,T (Tt−1,i)\n\n\n(113)\n(c)\n= E\n\nX\ni∈[m]\nTT −1,i\nX\ns=0\nκi,T (s)\n\n\n(114)\n≤m∆max +\nX\ni∈[m]\nLi,T,1\nX\nℓ=1\n∆max\ni\n+\nX\ni∈[m]\nLi,T,2\nX\nℓ=Li,T,1+1\n4c2B1 log T\nℓ\n(115)\n25\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n≤m∆max +\nX\ni∈[m]\n4c2B1 log T +\nX\ni∈[m]\n4c2B1 log(K∆max\ni\n∆min\ni\n) log T\n(116)\n= m∆max +\nX\ni∈[m]\n4c2B1\n\u0012\n1 + log(K∆max\ni\n∆min\ni\n)\n\u0013\nlog T\n(117)\n≤m∆max +\nX\ni∈[m]\n4c2B1\n\u0012\n1 + log(K∆max\ni\n∆min\ni\n)\n\u0013\nlog T,\n(118)\nwhere (a) follows from Equation (109), (b) follows from the tower rule, (c) follows from that Tt−1,i is increased by 1 if and\nonly if i ∈τt.\n■\nD. Applications\nFor convenience, we show our table again in Table 3.\nTable 3. Summary of the coefficients, regret bounds and improvements for various applications.\nApplication\nCondition\n(Bv, B1, λ)\nRegret\nImprovement\nOnline Influence Maximization (Wen et al., 2017)\nTPM\n(−, |V |, −) †\nO(d|V |\np\n|E|T log T)\n˜O(\np\n|E|)\nDisjunctive Combinatorial Cascading Bandits (Li et al., 2016)\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\n√\nK/pmin)‡\nConjunctive Combinatorial Cascading Bandits (Li et al., 2016)\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\n√\nK/rmax)\nLinear Cascading Bandits (Vial et al., 2022)∗\nTPVM\n(1, 1, 1)\nO(d\n√\nT log T)\n˜O(\np\nK/d)‡\nMulti-layered Network Exploration (Liu et al., 2021b)\nTPVM\n(\np\n1.25|V |, 1, 2) †\nO(d\np\n|V |T log T)\n˜O(√n/pmin)\nProbabilistic Maximum Coverage (Chen et al., 2013)∗∗\nVM\n(3\np\n2|V |, 1, −)\nO(d\np\n|V |T log T)\n˜O(\n√\nk)\n† |V |, |E|, n, k, L denotes the number of target nodes, the number of edges that can be triggered by the set of seed nodes, the number of layers, the number of seed\nnodes and the length of the longest directed path, respectively;\n‡ K is the length of the ordered list, rmax = α · maxt∈[T ],S∈S r(S; µt);\n∗A special case of disjunctive combinatorial cascading bandits.\n∗∗This row is for C2MAB application and the rest of rows are for C2MAB-T applications.\nD.1. Online Influence Maximization Bandit (Wang & Chen, 2017) and Its Contextual Generalization (Wen et al.,\n2017)\nFollowing the setting of (Wang & Chen, 2017, Section 2.1), we consider a weighted directed graph G(V, E, p), where V is\nthe set of vertices, E is the set of directed edges, and each edge (u, v) ∈E is associated with a probability p(u, v) ∈[0, 1].\nWhen the agent selects a set of seed nodes S ⊆V , the influence propagates as follows: At time 0, the seed nodes S are\nactivated; At time t > 1, a node u activated at time t −1 will have one chance to activate its inactive out-neighbor v with\nindependent probability p(u, v). The influence spread of S is denoted as σ(S) and is defined as the expected number of\nactivated nodes after the propagation process ends. The problem of Influence Maximization is to find seed nodes S with\n|S| ≤k so that the influence spread σ(S) is maximized.\nFor the problem of online influence maximization (OIM), we consider T rounds repeated influence maximization tasks and\nthe edge probabilities p(u, v) are assumed to be unknown initially. For each round t ∈[T], the agent selects k seed nodes as\nSt, the influence propagation of St is observed and the reward is the number of nodes activated in round t. The agent’s goal\nis to accumulate as much reward as possible in T rounds. The OIM fits into CMAB-T framework: the edges E are the set of\nbase arms [m], the (unknown) outcome distribution D is the joint of m independent Bernoulli random variables for the edge\nset E, the action S are any seed node sets with size k at most k. For the arm triggering, the triggered set τt is the set of edges\n(u, v) whose source node u is reachable from St. Let Xt be the outcomes of the edges E according to probability p(u, v)\nand the live-edge graph Glive\nt (V, Elive) be a induced graph with edges that are alive, i.e., e ∈Elive iff Xt,e = 1 for e ∈E.\nThe triggering probability distribution Dtrig(St, Xt) degenerates to a deterministic triggered set, i.e., τt is deterministically\ndecided given St and Xt. The reward R(St, Xt, τt) equals to the number activated nodes at the end of t, i.e., the nodes that\nare reachable from St in the live-edge graph Glive\nt . The offline oracle is a (1 −1/e −ε, 1/|V |)-approximation algorithm\ngiven by the greedy algorithm from (Kempe et al., 2003).\nNow consider OIM’s contextual generalization for large-scale OIM, we follow Wen et al. (2017), where each edge e = (u, v)\nis associated with a known feature vector xe ∈Rd and an unknown parameter θ∗∈Rd, and the edge probability is\np(u, v) = ⟨xe, θ∗⟩. By Lemma 2 of (Wang & Chen, 2017), B1 = ˜C ≤|V |, where ˜C is the largest number of nodes\nany node can reach and batch size K ≤|E|, so by Theorem 1, C2-UCB-T obtain a worst-case ˜O(d|V |\np\n|E|T) regret\n26\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nbound. Compared with IMLinUCB algorithm (Wen et al., 2017) that achieves ˜O(d(|V | −k)|E|\n√\nT), our regret achieves a\nimprovement up to a factor of ˜O(\np\n|E|).\nNow for the claim of the triggering probability satisfies Bp = B1, it follows from the Theorem 4 of Li et al. (2020) by\nidentifying f(S, w, v) = pw,S\ni\n.\nD.2. Contextual Combinatorial Cascading Bandits (Li et al., 2016)\nContextual Combinatorial cascading bandits have two categories: conjunctive cascading bandits and disjunctive cascading\nbandits (Li et al., 2016). We also compare with a special case of linear cascading bandits that also uses variance-adaptive\nalgorithms and achieve very competitive results.\nDisjunctive form. For the disjunctive form, we want to select an ordered list S of K items from total L items, so as to\nmaximize the probability that at least one of the outcomes of the selected items are 1. Each item is associated with a\nBernoulli random variable with mean µt,i ∈[0, 1] at round t, indicating whether the user will be satisfied with the item if\nhe scans the item. To leverage the contextual information, Li et al. (2016) assumes µt,i = ⟨xt,i, θ∗⟩, where xt,i ∈Rd is\nthe known context at round t for arm i, θ ∈Rd is the unknown parameter to be learned. This setting models the movie\nrecommendation system where the user sequentially scans a list of recommended items and the system is rewarded when the\nuser is satisfied with any recommended item. After the user is satisfied with any item or scans all K items but is not satisfied\nwith any of them, the user leaves the system. Due to this stopping rule, the agent can only observe the outcome of items\nuntil (including) the first item whose outcome is 1. If there are no satisfactory items, the outcomes must be all 0. In other\nwords, the triggered set is the prefix set of items until the stopping condition holds.\nWithout loss of generality, let the action be {1, ..., K}, then the reward function is r(S; µ) = 1 −QK\nj=1(1 −µj) and the\ntriggering probability is pµ,S\ni\n= Qi−1\nj=1(1 −µj). Let ¯µ = (¯µ1, ..., ¯µK) and µ = (µ1, ..., µK), where ¯µ = µ + ζ + η\nwith ¯µ, µ ∈(0, 1)K, ζ, η ∈[−1, 1]K. By Lemma 19 in Liu et al. (2021a), disjunctive CB satisfies Condition 4 with\n(Bv, B1, λ) = (1, 1, 1). Also, we can verify that disjunctive CB also satisfies Bp = B1 = 1 as follows:\n\f\f\fp¯µ,S\ni\n−pµ,S\ni\n\f\f\f\n=\n\f\f\f\f\f\f\niY\nj=1\n(1 −µj) −\niY\nj=1\n(1 −¯µj)\n\f\f\f\f\f\f\n(119)\n=\ni\nX\nj=1\n|¯µj −µj| (1 −µ1)...(1 −µj−1)(1 −¯µj+1)...(1 −¯µi)\n(120)\n≤\ni\nX\nj=1\n|¯µj −µj| (1 −µ1)...(1 −µj−1)\n(121)\n=\ni\nX\nj=1\n|¯µj −µj| pµ,S\nj\n.\n(122)\nNow by Theorem 3, VAC2-UCB obtains a regret bound of O(d\n√\nT log T). Compared with Corollary 4.5 in Li et al. (2016)\nthat yields a O(d\n√\nKT log T/pmin) regret, our results improves by a factor of O(\n√\nK/pmin).\nConjunctive form. For the conjunctive form, the learning agent wants to select K paths from total L paths (i.e., base arms)\nso as to maximize the probability that the outcomes of the selected paths are all 1. Each item is associated with a Bernoulli\nrandom variable with mean µt,i at round t, indicating whether the path will be live if the package will transmit via this path.\nSuch a setting models the network routing problem (Kveton et al., 2015a), where the items are routing paths and the package\nis delivered when all paths are alive. The learning agent will observe the outcome of the first few paths till the first one that\nis down, since the transmission will stop if any of the path is down. In other words, the triggered set is the prefix set of paths\nuntil the stopping condition holds.\nWithout loss of generality, let the action be {1, ..., K}, then the reward function is r(S; µ) = 1 −QK\nj=1(µj) and the\ntriggering probability is pµ,S\ni\n= Qi−1\nj=1(µj). Let ¯µ = (¯µ1, ..., ¯µK) and µ = (µ1, ..., µK), where ¯µ = µ + ζ + η\nwith ¯µ, µ ∈(0, 1)K, ζ, η ∈[−1, 1]K. By Lemma 20 in Liu et al. (2021a), conjunctive CB satisfies Condition 4 with\n27\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n(Bv, B1, λ) = (1, 1, 1). Also, we can verify that conjunctive CB also satisfies Bp = B1 = 1 as follows:\n\f\f\fp¯µ,S\ni\n−pµ,S\ni\n\f\f\f\n=\n\f\f\f\f\f\f\niY\nj=1\nµj −\niY\nj=1\n¯µj\n\f\f\f\f\f\f\n(123)\n=\ni\nX\nj=1\n|¯µj −µj| (µ1)...(µj−1)(¯µj+1)...(¯µi)\n(124)\n≤\ni\nX\nj=1\n|¯µj −µj| (µ1)...(µj−1)\n(125)\n=\ni\nX\nj=1\n|¯µj −µj| pµ,S\nj\n.\n(126)\nNow by Theorem 3, VAC2-UCB obtains a regret bound of O(d\n√\nT log T). Compared with Corollary 4.6 in Li et al. (2016)\nthat yields a O(d\n√\nKT log T/rmax) regret, our results improves by a factor of O(\n√\nK/rmax).\nLinear Cascading Bandit. Linear cascading bandit (Vial et al., 2022) is a special case of combinatorial cascading bandit (Li\net al., 2016). The former assumes that action space S is the collection of all permutations whose size equals to K (i.e., a\nuniform matroid). In this case, the items in the feasible solutions are exchangeable (a critical property for matroids), i.e.,\nS −{e1} + {e2} ∈S, for any S ∈S, e1, e2 ∈[m]. Based on this property, their analysis can get the correct results. For the\nlatter, however, S (i.e., Θ in [16]) consists of arbitrary feasible actions (perhaps with different sizes), e.g., S ∈S could refer\nto any path that connects the source and the destination in network routing applications.\nOther than the above difference, linear cascading bandits follow the same setting as disjunctive contextual combinatorial\nbandits. Following the similar argument of disjunctive contextual combinatorial bandits, the regret bound of VAC2-UCB is\nO(d\n√\nT log T). Compared with CascadeWOFUL that achieves ˜O(\np\nd(d + K)T) by Theorem 4 in Vial et al. (2022), our\nregret improves a factor of ˜O(\np\n1 + K/d). For the empirical comparison, see Section 5 for details.\nD.3. Multi-layered Network Exploration Problem (MuLaNE) (Liu et al., 2021b)\nWe consider the MuLaNE problem with random node weights. After we apply the bipartite coverage graph, the corresponding\ngraph is a tri-partite graph (n, V, R) (i.e., a 3-layered graph where the first layer and the second layer forms a bipartite graph,\nand the second and the third layer forms another bipartite graph), where the left nodes represent n random walkers; Middle\nnodes are |V | possible targets V to be explored; Right nodes R are V nodes, each of which has only one edge connecting\nthe middle edge. The MuLaNE task is to allocate B budgets into n layers to explore target nodes V and the base arms are\nA = {(i, u, b) : i ∈[n], u ∈V, b ∈[B]}.\nWith budget allocation k1, ..., kL, the (effective) base arms consist of two parts:\n(1) {(i, j) : i ∈[n], j ∈V }, each of which is associated with visiting probability xi,j ∈[0, 1] indicating whether node j\nwill be visited by explorer i given ki budgets. All these base arms corresponds to budget ki, i ∈[n] are triggered.\n(2) yj ∈[0, 1] for j ∈V represents the random node weight. The triggering probability pµ,S\nj\n= 1 −Q\ni∈[n] (1 −xi,j).\nFor its contextual generalization, we assume xi,j = ⟨ϕx(i, j), θ∗⟩, yj = ⟨ϕy(j), θ∗⟩, where ϕx(i, j), ϕy(j) are the known\nfeatures for visiting probability and the node weights for large-scale MuLaNE applications, respectively. Let effective\nbase arms µ = (x, y) ∈(0, 1)(n|V |+|V |), ¯µ = (¯x, ¯y) ∈(0, 1)(n|V |+|V |), where ¯x = ζx + ηx + x, ¯y = ζy + ηy + y, for\nζ, η ∈[−1, 1](n|V |+|V |). For the target node j ∈V , the per-target reward function rj(S; x, y) = yj(1 −Q\ni∈[n](1 −xi,j)).\nDenote ¯pµ,S\nj\n= 1 −Q\ni∈[n] (1 −¯xi,j). Based on Lemma 21 in Liu et al. (2022), contextual MuLaNE satisfies Condition 4\nwith (Bv, B1, λ) = (\np\n1.25|V |, 1, 2). To validate that this application satisfies Condition 5 with Bp = B1 = 1, we have\n\f\f\fp¯µ,S\nj\n−pµ,S\nj\n\f\f\f\n28\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\n=\n\f\f\f\f\f\f\nY\ni∈[n]\n(1 −xi,j) −\nY\ni∈[n]\n(1 −¯xi,j)\n\f\f\f\f\f\f\n(127)\n=\nX\ni∈[n]\n|¯xi,j −xi,j| (1 −x1,j)...(1 −xi−1,j)(1 −¯xi+1,j)...(1 −¯xi,j)\n(128)\n=\nX\ni∈[n]\n|¯xi,j −xi,j| .\n(129)\nBy Theorem 3, we obtain O(d\np\n|V |T log T), which improves the result O(d\np\nn|V |T log T/pmin) that follows the result\nof C3UCB algorithm (Li et al., 2016) by a factor of O(\np\nn/pmin).\nD.4. Probabilistic Maximum Coverage Bandit (Chen et al., 2016a; Merlis & Mannor, 2019)\nIn this section, we consider the probabilistic maximum coverage (PMC) problem. PMC is modeled by a weighted bipartite\ngraph G = (L, V, E), where L are the source nodes, V is the target nodes and each edge (u, v) ∈E is associated with a\nprobability p(u, v). The task of PMC is to select a set S ⊆L of size k so as to maximize the expected number of nodes\nactivated in V , where a node v ∈V can be activated by a node u ∈S with an independent probability p(u, v). PMC can\nnaturally model the advertisement placement application, where L are candidate web-pages, V are the set of users, and\np(u, v) is the probability that a user v will click on web-page u.\nPMC fits into the non-triggering CMAB framework: each edge (u, v) ∈E corresponds to a base arm, the action is the set of\nedges that are incident to the set S ⊆L, the unknown mean vectors µ ∈(0, 1)E with µu,v = p(u, v) and we assume they\nare independent across all base arms. In this context, the reward function r(S; µ) = P\nv∈V (1 −Q\nu∈S(1 −µu,v)).\nIn this paper, we consider a contextual generalization by assuming that p(u, v) = ⟨ϕ(u, v), θ∗⟩, where ϕ(u, v) ∈Rd is\nthe known context and θ∗∈Rd is the unknown parameter. By Lemma 24 in Liu et al. (2022), PMC satisfies Condition 3\nwith (Bv, B1) = (3\np\n2|V |, 1). Following Theorem 2, VAC2-UCB obtains O(d\np\n|V |T log T), which improves the C3UCB\nalgorithm’s bound O(d\np\nk|V |T log T) (Li et al., 2016) by a factor of O(\n√\nk).\nE. Experiments\nSynthetic data. We consider the same disjunctive linear cascading bandit setting as in (Vial et al., 2022), where the goal is\nto choose K ∈{2i}8\ni=2 out of m = 100 items to maximize the reward. Notice that the linear cascading bandit problem is a\nsimplified version of the contextual cascading bandit problem where the feature vectors of base arms are fixed in all rounds\n(see Appendix D.2 for details). For each K, we sample the click probability µi of item i uniformly in [ 2\n3K , 1\nK ] for i ≤K and\nin [0,\n1\n3K ] for i > K. We vary d ∈{2i}8\ni=2 to generate the same µ and compute unit-norm vectors θ∗and ϕ(i) satisfying\nµi = ⟨θ∗, ϕ(i)⟩. We compare VAC2-UCB to C3-UCB (Li et al., 2016) and CascadeWOFUL (Vial et al., 2022): C3-UCB\nis the variance-agnostic cascading bandit algorithm (essentially the same as CascadeLinUCB (Zong et al., 2016) in the\nlinear cascading setting by using the tunable parameter σ = 1) and CascadeWOFUL is the state-of-the-art variance-aware\ncascading bandit algorithm. As shown in Figure 2, the regret of our VAC2-UCB algorithm has superior dependence on\nK and d over that of C3-UCB. When d = K = 10, VAC2-UCB achieves sublinear regret; it incurs 75% and 13% less\nregret than C3-UCB and CascadeWOFUL after 100, 000 rounds. Notice that CascadeWOFUL is also variance-aware but\nspecifically designed for cascading bandits, while our algorithm can be applied to general C2MAB-T.\nReal data. We conduct experiments on the MovieLens-1M dataset which contains user ratings for m ≈4000 movies.\nFollowing the same experimental setup in (Vial et al., 2022), we set d = 20, K = 4, and the goal is to choose K out\nof m movies to maximize the reward of the cascading recommendation. We use their learned feature mapping ϕ from\nmovies to the probability that a uniformly random user rated the movie more than three stars. We point the reader to\nSection 6 of (Vial et al., 2022) for more details. In each round, we sample a random user Jt and define the potential\nclick result Xt,i = I{user Jt rated movie i more than 3 stars}. In other words, we observe the actual feedback of user Jt\ninstead of using the Bernoulli click model. Figure 1a shows that VAC2-UCB outperforms C3-UCB and CascadeWOFUL,\nincurring 45% and 25% less regret after 100, 000 rounds. To model platforms like Netflix that recommend movies in specific\ncategories, we also run experiments while restricting the candidate items to movies of a particular genre. Figure 1b shows\nthat VAC2-UCB is superior for all genres compared to C3-UCB and CascadeWOFUL.\n29\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nFigure 2. Results for synthetic data\nF. Concentration Bounds, Facts, and Technical Lemmas\nIn this section, we first give key concentration bounds and then provide lemmas that are useful for the analysis.\nF.1. Concentration Bounds\nWe mainly use the following concentration bounds, which is essentially a modification of the Freedman’s version of the\nBernstein’s inequality (Bernstein, 1946; Freedman, 1975).\nProposition 3 (Theorem 9, Lattimore et al. (2015)). Let δ ∈(0, 1) and X1, ..., Xn be a sequence of random variables\nadapted to filtration {Ft} with E[Xt | Ft−1] = 0. Let Z ⊆[n] be such that I{t ∈Z} is Ft−1-measurable and let Rt be\nFt−1 measurable such that |Xt| ≤Rt almost surely. Let V = P\nt∈Z Var[Xt | Ft−1] + P\nt/∈Z R2\nt /2, R = maxt∈Z Rt,\nand S = Pn\nt=1 Xt. Then Pr[S ≥f(R, V )] ≤δ, where f(r, v) =\n2(r+1)\n3\nlog\n2\nδr,v +\nq\n2(v + 1) log\n2\nδr,v , and δr,v =\nδ\n3(r+1)2(v+1).\nF.2. Facts\nFact 1. For any positive-definite matrices A, B > 0d and any vectors x, y ∈Rd. It holds that\n1. If A ≤B, then A−1 ≥B−1.\n2. If A ≤B, then ∥x∥A ≤∥x∥B.\n3. Suppose A has maximum eigenvalue λmax, then ∥Ax∥2 ≤λmax · ∥x∥2 and λmax ≤trace(A).\nF.3. Technical Lemmas\nRecall that event Ft is defined in Equation (21), gram matrix Gt is defined in Equation (18), optimistic variance ¯Vt,i is\ndefined in Equation (6), Rv is defined in Equation (131).\nLemma 7. Pr\n\u0002\n∥Zt∥Gt + √γ ≥ρ and ¬Ft−1\n\u0003\n≤δ/T, for t = 1, ..., T.\nProof of lemma 7. Let v ∈Rd and define\nVs,i,v =\n(\nVar[ηs,i | Fs−1]⟨ϕs(i), v⟩2/ ¯V 2\ns,i,\nif ¯Vs,i < 1\n4\n⟨ϕs(i), v⟩2/ ¯Vs,i,\notherwise.\n(130)\nRv =\nmax\ns<t,i∈τs{⟨ϕs(i), v⟩/ ¯Vs,i : ¯Vs,i < 1\n4}\n(131)\nBy applying the Proposition 3, with probability at least 1 −δ/T it holds that\n⟨Zt, v⟩=\nX\ns<t\nX\ni∈τs\nηs,i⟨ϕs(i), v⟩/ ¯Vs,i ≤2(Rv + 1)\n3\nlog 1\nδv\n+\ns\n2(1 +\nX\ns<t\nX\ni∈τs\nVs,i,v) log 1\nδv\n(132)\n30\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nwhere δv =\n3δ\nT (1+Rv)2(1+P\ns<t\nP\ni∈τs Vs,i,v)2 .\nSince v could be a random variable in later proofs, we use the covering argument trick (Chap.20, Lattimore & Szepesv´ari\n(2020)) to handle v. Specifically, we define the covering set Λ = {j · ε : j = −C\nε , −C\nε + 1, ..., C\nε −1, C\nε }d, with size\nN = |Λ| = (2C/ε)d and parameters C, ε will be determined shortly after. By applying union bound on Equation (132), we\nhave with probability at least 1 −δ that\n⟨Zt, v⟩≤2(Rv + 1)\n3\nlog N\nδv\n+\ns\n2(1 +\nX\ns<t\nX\ni∈τs\nVs,i,v) log N\nδv\nfor all v ∈Λ.\n(133)\nNow we can set v = G−1\nt Zt, and it follows from Lemma 12 that ∥v∥∞≤∥Zt∥1 ≤2dK2t2 = C. Based on our\nconstruction of the covering set Λ, there exists v′ ∈Λ with v′ ≤v, and ∥v′ −v∥∞≤ε, such that\n∥Zt∥2\nG−1\nt\n= ⟨Zt, v⟩≤∥Zt∥1 ε + ⟨Zt, v′⟩\n(134)\n≤∥Zt∥1 ε + 2(Rv + 1)\n3\nlog N\nδv\n+\ns\n2(1 +\nX\ns<t\nX\ni∈τs\nVs,i,v) log N\nδv\n(135)\n≤∥Zt∥1 ε + 2(Rv + 1)\n3\nlog N\nδv\n+\nr\n2(1 + ∥Zt∥2\nG−1\nt ) log N\nδv\n(136)\nwhere Equation (135) uses the fact that Rv′ ≤Rv, Vs,i,v′ ≤Vs,i,v,\n1\nδv′ ≤\n1\nδv for any v′ ≤v, Equation (136) follows from\nthe following derivation,\nX\ns<t\nX\ni∈τs\nVs,i,v ≤\nX\ns<t\nX\ni∈τs\n⟨ϕs(i), v⟩2/ ¯Vs,i\n(137)\n=\nX\ns<t\nX\ni∈τs\n(G−1\nt Zt)⊤ϕs(i)ϕs(i)⊤G−1\nt Zt/ ¯Vs,i\n(138)\n= (G−1\nt Zt)⊤\n X\ns<t\nX\ni∈τs\nϕs(i)ϕs(i)⊤/ ¯Vs,i\n!\nG−1\nt Zt\n(139)\n≤(G−1\nt Zt)⊤Gt(G−1\nt Zt)\n(140)\n= ∥Zt∥2\nG−1\nt\n,\n(141)\nwhere Equation (137) follows from ¬Fs−1 implies\n\r\r\rθ∗−ˆθs\n\r\r\r\nGs ≤ρ for s < t by Lemma 8 and thus ¯Vt,i ≥Var[ηs,i |\nFs−1], Equation (138) follows from definition of v, Equation (140) follows from P\ns<t\nP\ni∈τs ϕs(i)ϕs(i)⊤/ ¯Vs,i < Gt.\nNow we set ε = 1/C = 1/(2K2t2d), we have\n∥Zt∥2\nG−1\nt\n≤RHS of Equation (136)\n(142)\n≤Cε +\n2(2 ∥Zt∥G−1\nt\n/ρ + 1)\n3\nlog N\nδv\n+\nr\n2(1 + ∥Zt∥2\nG−1\nt ) log N\nδv\n(143)\n≤1 + 2 log N\nδv\n+\nr\n2(1 + ∥Zt∥2\nG−1\nt ) log N\nδv\n(144)\nwhere Equation (143) is to bound Rv by Lemma 10, Equation (144) is by the definition of ρ as an upper bound of ∥Zt∥G−1\nt .\nBy rearranging and simplifying Equation (144), we have\n∥Zt∥G−1\nt\n+ √γ ≤1 + √γ + 4\nr\nlog N\nδv\n(145)\n≤1 + √γ + 4\ns\nlog\n\u00126TN\nδ\n(1 + ∥Zt∥2\nG−1\nt )\n\u0013\n,\n(146)\n31\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nwhere the last inequality is because of δv ≥\n3δ\nT (1+∥Zt∥2\nG−1\nt\n) from the definition of δv, Lemma 10, and Equation (141).\nFinally, we solve the above equation and set ρ = 1 + √γ + 4\nq\nlog\n\u0000 6T N\nδ\nlog( 3T N\nδ )\n\u0001\n, which completes the reduction on t\nto show the probability Pr[∥Zt∥G−1\nt\n+ √γ ≥ρ] ≥1 −δ/T under event ¬Ft−1.\n■\nLemma 8. If ¬Ft holds, then it holds that,\n\r\r\rθ∗−ˆθt\n\r\r\r\nGt ≤ρ.\n(147)\nProof. We have\n\r\r\rθ∗−ˆθt\n\r\r\r\nGt\n=\n\r\r\r\r\rG−1\nt (\nX\ns<t\nX\ni∈τs\nϕs(i)Xs,i/ ¯Vs,i) −G−1\nt Gtθ∗\n\r\r\r\r\r\nGt\n(148)\n=\n\r\r\r\r\rG−1\nt Zt + G−1\nt (\nX\ns<t\nX\ni∈τs\nϕs(i)ϕs(i)⊤θ∗/ ¯Vs,i) −G−1\nt Gtθ∗\n\r\r\r\r\r\nGt\n(149)\n=\n\r\rG−1\nt Zt −γG−1\nt θ∗\r\r\nGt\n(150)\n≤∥Zt∥G−1\nt\n+ γ ∥θ∗∥G−1\nt\n(151)\n≤∥Zt∥G−1\nt\n+ √γ\n(152)\n≤ρ −√γ + √γ = ρ,\n(153)\nwhere Equation (148)-(150) follow from definition and math calculation, Equation (151) from Gt ≥G0 = γI and\n∥θ∥2 ≤1, Equation (152) from that if ¬Ft holds, then ∥Zt∥Gt + √γ ≤ρ.\n■\nLemma 9. For any s < t,\n∥ϕs(i)∥G−1\nt\n¯Vs,i\n≤\n∥ϕs(i)∥G−1\ns\n¯Vs,i\n, and if ¬Ft−1 holds and ¯Vs,i < 1\n4,\n∥ϕs(i)∥G−1\ns\n¯Vs,i\n≤2\nρ ≤1 for any\ni ∈[m].\nProof. The first inequality is by Gt ≥Gs and Fact 1. For the second inequality, when ¬Ft−1 holds,\n\r\r\rθ∗−ˆθs\n\r\r\r\nGs ≤ρ as\nin Equation (147), and since ¯Vs,i < 1\n4, it follows from the definition of ¯Vs,i Equation (6) that at least one of the following is\ntrue:\n¯Vs,i ≥1\n2(⟨ϕs(i), ˆθs + 2ρ ∥ϕs(i)∥G−1\ns ⟩) ≥ρ ∥ϕs(i)∥G−1\ns\n/2,\n(154)\n¯Vs,i ≥1\n2(1 −⟨ϕs(i), ˆθs + 2ρ ∥ϕs(i)∥G−1\ns ⟩) ≥ρ ∥ϕs(i)∥G−1\ns\n/2,\n(155)\nwhich concludes the second inequality.\n■\nLemma 10. Let v = G−1\nt bt, if ¬Ft−1, then Rv ≤\n2∥Zt∥G−1\nt\nρ\n.\nProof. For all s < t and i ∈[m], we have ⟨ϕs(i), v⟩/ ¯Vs,i ≤\n2⟨ϕs(i),v⟩\n∥ϕs(i)∥G−1\nt\n·ρ = 2⟨ϕs(i),G−1\nt\nZt⟩\n∥ϕs(i)∥G−1\nt\n·ρ\n≤\n2∥ϕs(i)∥G−1\nt\n·∥G−1\nt\nZt∥Gt\n∥ϕs(i)∥G−1\nt\n·ρ\n=\n2∥Zt∥G−1\nt\nρ\n, where the first inequality follows from Lemma 9, the last inequality follows from the Cauchy-Schwarz inequality.\n■\nLemma 11. If ¬Ft, then ∥ϕt(i)∥2\n2 / ¯Vt,i ≤4dKt.\nProof. If ¯Vt,i = 1\n4, the inequality trivially holds since ∥ϕt(i)∥≤1. Consider ¯Vt,i < 1\n4, and λmax be the maximum\neigenvalue of Gt. Then, it holds that ∥ϕt(i)∥2\n2 / ¯Vt,i ≤\n∥ϕt(i)∥2\n2\nρ∥ϕt(i)∥G−1\nt\n≤\n∥ϕt(i)∥2\n∥ϕt(i)∥G−1\nt\n=\n\r\r\rG1/2\nt\nG−1/2\nt\nϕt(i)\n\r\r\r\n2\n∥ϕt(i)∥G−1\nt\n≤√λmax, where\nthe first inequality follows from Lemma 9, the second inequality is by ρ ≥1, ∥ϕt(i)∥≤1, and the last is by Fact 1.3.\n32\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nNow Assume ∥ϕs(i)∥2\n2 / ¯Vs,i ≤4s for s < t, which always holds for t = 1. By reduction, we consider round t, it\nholds that ∥ϕt(i)∥2\n2 / ¯Vt,i ≤√λmax ≤\np\ntrace(Gt) =\nq\nγd + Pt−1\ns=1\nP\ni∈τs ∥ϕs(i)∥2\n2 / ¯Vs,i ≤\nq\nKd + Pt−1\ns=1 4dK2s ≤\np\nd(K + 2K2t(t −1)) ≤4dKt, where the first inequality follows from the analysis in the last paragraph, the third\ninequality follows from reduction over s < t, and the last inequality is by math calculation.\n■\nLemma 12. If ¬Ft, then ∥ϕt(i)∥1 / ¯Vt,i ≤4dKt.\nProof. Similar to the proof of Lemma 11, ∥ϕt(i)∥1 / ¯Vt,i ≤\n√\nd ∥ϕt(i)∥2 / ¯Vt,i ≤\n√\nd∥ϕt(i)∥2\nρ∥ϕt(i)∥G−1\nt\n≤\n∥ϕt(i)∥2\n∥ϕt(i)∥G−1\nt\n=\n\r\r\rG1/2\nt\nG−1/2\nt\nϕt(i)\n\r\r\r\n2\n∥ϕt(i)∥G−1\nt\n≤√λmax ≤4dKt, where the first inequality uses Cauchy-Schwarz, the second inequality uses\nρ ≥\n√\nd, and the rest follows from the proof of Lemma 11.\n■\nLemma 13. If ¬Ft−1, then ∥Zt∥1 ≤2dK2t2.\nProof. ∥Zt∥1 =\n\r\rP\ns<t\nP\ni∈τs ηs,iϕs(i)/ ¯Vs,i\n\r\r\n1 ≤P\ns<t\nP\ni∈τs\n\r\rϕs(i)/ ¯Vs,i\n\r\r\n1 ≤P\ns<t\nP\ni∈τs 4dKt ≤2dK2t2,\nwhere the first inequality follows from ηs,i ∈[−1, 1], the second inequality follows from Lemma 12.\n■\nLemma 14 (Lemma A.3, (Li et al., 2016)). Let xi ∈Rd, 1 ≤i ≤n. Then we have\ndet\n \nI +\nn\nX\ni=1\nxix⊤\ni\n!\n≥1 +\nn\nX\ni=1\n∥x∥2\n2 .\nLemma 15 (Lemma 11, (Abbasi-Yadkori et al., 2011)). Let xi ∈Rd with ∥xi∥2 ≤L, 1 ≤i ≤n and let Gt =\nγI + Pt−1\ni=1 xix⊤\ni , then\ndet(Gt+1) ≤(γ + tL2/d)d.\nLemma 16. Equation (82) holds.\nProof. When Tt−1,i > Li,T,2 = 8c2\n1B2\nvK log T\n( ˜∆min\ni\n)2\n,\nwe have (82, i) ≤8c2\n1B2\nv log T\nTt−1,i,· ˜∆St −\n˜∆St\nK\n< ( ˜∆min\ni\n)2\nK ˜∆St −\n˜∆St\nK\n≤0 = κi,T (Tt−1,i).\nWhen Li,T,1 < Tt−1,i ≤Li,T,2,\nWe have (82, i) ≤8c2\n1B2\nv log T\nTt−1,i,· ˜∆St −\n˜∆St\nK\n< 8c2\n1B2\nv log T\nTt−1,i,· ˜∆St ≤8c2\n1B2\nv log T\nTt−1,i· ˜∆i\nmin = κi,T (Tt−1,i,jSt\ni ).\nWhen Tt−1,i ≤Li,T,1,\nWe further consider two different cases Tt−1,i ≤4c2\n1B2\nv log T\n( ˜∆St)2\nor 4c2\n1B2\nv log T\n( ˜∆St)2\n< Tt−1,i ≤Li,T,1 = 4c2\n1B2\nv log T\n( ˜∆min\ni\n)2 .\nFor the former case, if there exists i ∈τt so that Tt−1,i ≤4c2\n1B2\nv log T\n( ˜∆St)2\n, then we know P\nq∈˜St κq,T (Tt−1,q) ≥κi,T (Tt−1,i) =\n2\nq\n4c2\n1B2v log T\nTt−1,i\n≥2 ˜∆St > ∆St, which makes eq. (82) holds no matter what. This means we do not need to consider this\ncase for good.\nFor\nthe\nlater\ncase,\nwhen\n4c2\n1B2\nv log T\n( ˜∆St)2\n<\nTt−1,i,\nwe\nknow\nthat\n(82, i)\n≤\n8c2\n1B2\nv log T\n˜∆St\n1\nTt−1,i\n=\n2\nr\n4c2\n1B2\nv log T\n( ˜∆St)2\n1\nTt−1,i\nq\n4c2\n1B2\nv log T\nTt−1,i\n≤2\nq\n4c2\n1B2\nv log T\nTt−1,i\n= κi,T (Tt−1,i).\nWhen ℓ= 0,\nWe have (82, i) ≤8c2\n1B2\nv\n˜∆St · 1\n28 −\n˜∆St\nK\n≤c2\n1B2\nv\n˜∆St ≤c2\n1B2\nv\n˜∆min\ni\n= κi,T (Tt−1,i).\nCombining all above cases, we have ∆St ≤E[P\ni∈τt κi,T (Tt−1,i)].\n■\n33\n\nContextual Combinatorial Bandits with Probabilistically Triggered Arms\nLemma 17. Equation (98) holds.\nProof. When Tt−1,i > Li,T,2 = 8c2\n1B2\nvK log T\n˜∆min\ni\n· ˜∆min\ni,λ\n,\nwe have (98, i) ≤\n8c2\n1B2\nv log T\nTt−1,i,· ˜∆St,λ −\n˜∆St\nK\n<\n˜∆min\ni\n· ˜∆min\ni,λ\nK ˜∆St,λ\n−\n˜∆St\nK\n≤0 = κi,T (Tt−1,i).\nWhen Li,T,1 < Tt−1,i ≤Li,T,2,\nWe have (98, i) ≤\n8c2\n1B2\nv log T\nTt−1,i· ˜∆St,λ −\n˜∆St\nK\n<\n8c2\n1B2\nv log T\nTt−1,i,· ˜∆St,λ ≤8c2\n1B2\nv log T\nTt−1,i· ˜∆i,λ\nmin = κi,T (Tt−1,i).\nWhen Tt−1,i ≤Li,T,1,\nWe further consider two different cases Tt−1,i ≤4c2\n1B2\nv log T\n˜∆St,λ· ˜∆St or 4c2\n1B2\nv log T\n˜∆St,λ· ˜∆St < Tt−1,i ≤Li,T,1 = 4c2\n1B2\nv log T\n˜∆min\ni,λ · ˜∆min\ni\n.\nFor the former case, if there exists i ∈τt so that Tt−1,i ≤4c2\n1B2\nv log T\n˜∆St,λ· ˜∆St , then we know P\nq∈˜St κq,T (Tt−1,q) ≥κi,T (Tt−1,i) =\n2\nq\n4c2\n1B2v log T\nTt−1,i\n≥2\nq\n˜∆St,λ · ˜∆St ≥∆St, which makes eq. (98) holds no matter what. This means we do not need to\nconsider this case for good.\nFor\nthe\nlater\ncase,\nwhen\n4c2\n1B2\nv log T\n˜∆St,λ· ˜∆St\n<\nTt−1,i,\nwe\nknow\nthat\n(98, i)\n≤\n8c2\n1B2\nv log T\n˜∆St,λ\n1\nTt−1,i\n=\n2\nr\n4c2\n1B2v log T\n( ˜∆St,λ)2\n1\nTt−1,i\nq\n4c2\n1B2v log T\nTt−1,i\n≤2\nr\n˜∆St\n˜∆St,λ\nq\n4c2\n1B2v log T\nTt−1,i\n≤2\nq\n4c2\n1B2v log T\nTt−1,i\n= κi,T (Tt−1,i).\nWhen ℓ= 0,\nWe have (98, i) ≤8c2\n1B2\nv\n˜∆St,λ · 1\n28 −\n˜∆St\nK\n≤\nc2\n1B2\nv\n˜∆St,λ ≤c2\n1B2\nv\n˜∆min\ni,λ = κi,T (Tt−1,i).\nCombining all above cases, we have ∆St ≤E[P\ni∈τt κi,T (Tt−1,i)].\n■\nLemma 18. Equation (109) holds.\nProof. When Tt−1,i > Li,T,2 = 4c2B1K log T\n∆min\ni\n,\nwe have (109, i) ≤4c2B1\nlog T\nTt−1,i −∆St\nK\n< ∆min\ni\nK\n−∆St\nK\n≤0 = κi,T (Tt−1,i).\nWhen Tt−1,i ≤Li,T,2,\nWe have (109, i) ≤4c2B1\nlog T\nTt−1,i −∆St\nK\n< 4c2B1 log T\nTt−1,i\n= κi,jSt\ni\n,T (Nt−1,i,jSt\ni ).\nWhen Tt−1,i ≤Li,T,1,\nIf there exists i ∈˜St so that Tt−1,i ≤Li,T,1, then we know P\nq∈˜St κi,T (Tt−1,q) ≥κi,T (Tt−1,i) = ∆max\ni\n≥∆St, which\nmakes eq. (109) holds no matter what. This means we do not need to consider this case for good.\nCombining all above cases, we have ∆St ≤Et[P\ni∈τt κi,T (Tt−1,i)].\n■\n34",
    "pdf_filename": "Contextual_Combinatorial_Bandits_with_Probabilistically_Triggered_Arms.pdf"
}