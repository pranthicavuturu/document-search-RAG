{
    "title": "RLtools A Fast, Portable Deep Reinforcement Learning Library for Continuous Control",
    "abstract": "Deep Reinforcement Learning (RL) can yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing libraries. To address these challenges, we present RLtools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Its novel architecture allows RLtools to be used on a wide variety of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simulation environments, RLtools can solve popular RL problems up to 76 times faster than other popular RL frameworks. We also benchmark the inference on a diverse set of microcontrollers and show that in most cases our optimized implementation is by far the fastest. Finally, RLtools enables the first-ever demonstration of training a deep RL algorithm directly on a microcontroller, giving rise to the field of Tiny Reinforcement Learning (TinyRL). The source code as well as documentation and live demos are available through our project page at https://rl.tools.",
    "body": "Journal of Machine Learning Research 25 (2024) 1-19\nSubmitted 2/24; Revised 8/24; Published 9/24\nRLtools: A Fast, Portable Deep Reinforcement Learning\nLibrary for Continuous Control\nJonas Eschmann1,2\nDario Albani2\nGiuseppe Loianno1\n1New York University\n2Technology Innovation Institute\n{jonas.eschmann,loiannog}@nyu.edu\ndario.albani@tii.ae\nEditor: Alexandre Gramfort\nDeployment\nOn-Device Inference\nHyperparameter Search\nResearch & Development\nOn-Device Training\npolicy\n3.4 kHz\nAbstract\nDeep Reinforcement Learning (RL) can yield capable agents and control policies in several\ndomains but is commonly plagued by prohibitively long training times. Additionally, in\nthe case of continuous control problems, the applicability of learned policies on real-world\nembedded devices is limited due to the lack of real-time guarantees and portability of\nexisting libraries. To address these challenges, we present RLtools, a dependency-free,\nheader-only, pure C++ library for deep supervised and reinforcement learning. Its novel\narchitecture allows RLtools to be used on a wide variety of platforms, from HPC clusters over\nworkstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically,\ndue to the tight integration of the RL algorithms with simulation environments, RLtools\ncan solve popular RL problems up to 76 times faster than other popular RL frameworks.\nWe also benchmark the inference on a diverse set of microcontrollers and show that in\nmost cases our optimized implementation is by far the fastest. Finally, RLtools enables\nthe first-ever demonstration of training a deep RL algorithm directly on a microcontroller,\ngiving rise to the field of Tiny Reinforcement Learning (TinyRL). The source code as well\nas documentation and live demos are available through our project page at https://rl.tools.\nKeywords:\nReinforcement Learning, Continuous Control, Deep Learning, TinyRL\n1 Introduction\nContinuous control is a ubiquitous and pervasive problem in a diverse set of domains such as\nrobotics, high-frequency decision-making in financial markets or the automation of chemical\nplants and smart grid infrastructure. Taking advantage of the recent progress in Deep\nLearning (DL) that is spilling over into decision-making in the form of RL, agents derived\nusing deep RL have already attained impressive performance in a range of decision-making\nproblems, like games and particularly continuous control. Despite these achievements, the\n©2024 Jonas Eschmann, Dario Albani, and Giuseppe Loianno.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v25/24-0248.html.\narXiv:2306.03530v4  [cs.LG]  19 Nov 2024\n\nEschmann, Albani, and Loianno\nreal-world adoption of RL for continuous control is hindered by prohibitively long training\ntimes as well as a lack of support for the deployment of trained policies on real-world\nembedded devices. Long training times obstruct rapid iteration in the problem space (reward\nfunction design, hyperparameter tuning, etc.) while deployment on computationally severely\nlimited embedded devices is necessary to control the bulk of physical systems such as: robots,\nautomotive components, medical devices, smart grid infrastructure, etc. In non-physical\nsystems, such as financial markets, the need for high-frequency decision-making leads to\nsimilar real-time requirements which cannot be fulfilled by current deep RL libraries. Hence,\nto address these challenges we present RLtools, a dependency-free, header-only pure C++\nlibrary for deep supervised and reinforcement learning combining the following contributions:\n• Novel Architecture: We describe the innovations in the software design of the library\nwhich allow for unprecedented training and inference speeds on a wide variety of devices\nfrom High-Performance Computing (HPC) clusters over workstations and laptops to\nsmartphones, smartwatches and microcontrollers.\n• Implementation: We contribute a modular, highly portable, and efficient implementation\nof the aforementioned architecture in the form of open-source code, documentation, and\ntest cases.\n• Fastest Training: We demonstrate large speedups in terms of wall-clock training time.\n• Fastest Inference: We demonstrate large speedups in terms of the inference time of\ntrained policies on a diverse set of common microcontrollers.\n• TinyRL: By utilizing RLtools, we successfully demonstrate, the first-ever training of a\ndeep RL algorithm for continuous control directly on a microcontroller.\n2 Related Work\nMultiple deep RL frameworks and libraries have been proposed, many of which cover\nalgorithmic research, with and without abstractions (Acme (Hoffman et al., 2020), skrl\n(Serrano-Munoz et al., 2023) and CleanRL (Huang et al., 2022) respectively). Other frame-\nworks and libraries focus on comprehensiveness in terms of the number of algorithms included\n(RLlib (Liang et al., 2018), ReinforcementLearning.jl (Tian, 2020), MushroomRL (D’Eramo\net al., 2021), Stable-Baselines3 (Raffin et al., 2021), ChainerRL (Fujita et al., 2021)), Tianshou\n(Weng et al., 2022), and TorchRL (Bou et al., 2024). In contrast to these aforementioned\nsolutions, RLtools aims at fast iteration in the problem space in the form of e.g., reward\nfunction design (Eschmann, 2021) and hyperparameter optimization. In the problem space,\nthe algorithmic intricacies and variety of the algorithms matter less than the robustness,\ntraining speed, and final performance as well as our understanding of how to train them\nreliably. From the formerly mentioned RL frameworks and libraries RLlib (Liang et al., 2018)\nis the most similar in terms of its mission statement being on quick iteration and deployment\n(cf. benchmark comparisons wrt. this goal in Section 4). By focusing on iteration in the\nspace of problems and subsequent deployment to real-time platforms, we also draw parallels\nbetween RLtools and the ACADOS software (Verschueren et al., 2022) for synthesizing\nModel Predictive Controls (MPCs) with RLtools aspiring to be its RL equivalent.\n2\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\n3 Approach\nTaking the last handful of years of progress in RL for continuous control, it can be observed\nthat the most prominent models used as function approximators are still relatively small,\nfully-connected neural networks. In Appendix A we analyze the architectures used in deep\nRL for continuous control and justify the focus of RLtools on (small) fully-connected neural\nnetworks. Based on these observations, we conclude that the great flexibility provided by\nautomatic differentiation frameworks like TensorFlow or PyTorch might not be necessary for\napplying RL to many continuous control problems. We believe that there is an advantage\nin trading-off the flexibility in the model architecture of the function approximators for the\noverall training speed. Reducing the training time and increasing the training efficiency saves\nenergy, simplifies reproducibility and democratizes access to state of the art RL methods.\nFurthermore, fast training facilitates principled hyperparameter search which in turn improves\ncomparability.\nArchitecture Our software architecture is guided by the previous observation and hence\nby maximizing the training time efficiency without sacrificing returns. Additionally, we\nwant the software to be able to run across many different accelerators and devices (CPUs,\nGPUs, microcontrollers, and other accelerators) so that trained policies can also directly be\ndeployed on microcontrollers and take advantage of device-specific instructions to run at\nhigh frequencies with hard realtime guarantees. This also entails that RLtools does not rely\non any dependencies because they might not be available on the target microcontrollers.\nTo attain maximum performance, we integrate the different components of our library\nas tightly as needed while maintaining as much flexibility and modularity as possible. To\nenable this goal, we heavily rely on the C++ templating system. Leveraging template\nmeta-programming, we can provide the compiler with a maximum amount of information\nabout the structure of the code, enabling it to be optimized heavily. We particularly make\nsure that the size of all loops is known at compile time such that the compiler can optimize\nthem via inlining and loop-unrolling (cf. Appendix B and F). Leveraging pure C++ without\nany dependencies, we implement the following major components: Deep Learning (MLP,\nbackpropagation, Adam, etc.), Reinforcement Learning (GAE, PPO, TD3, SAC), and\nSimulation (Pendulum, Acrobot, Quadrotor, Racing Car, MuJoCo interface). We implement\nRLtools in a modular way by using a novel static multiple-dispatch paradigm inspired\nby (dynamic) multiple-dispatch which was popularized by the Julia programming language\nBezanson et al. (2012). We highly recommend taking a look at the code example and\nexplanation in Appendix B as well as the ablation study in Appendix F measuring the impact\nof different components and optimizations.\n4 Results\nHorizontal Benchmark\nFigure 1 and 2 show the resulting mean training times from\nrunning the PPO and SAC algorithm across ten runs on an Intel-based laptop (details in\nTable 6). We find that RLtools outperforms existing libraries by a wide margin. Particularly\nin the case of PPO where RLtools only takes 0.54 s on average (2.59 s in case of SAC).\nVertical Benchmark\nIn Figure 3, we also present training results using RLtools on a\nwide variety of devices which are generally not compatible with the other RL libraries and\n3\n\nEschmann, Albani, and Loianno\n0\n25\n50\n75\n100\nTraining time [s] (smaller is better)\n1.0x = 0.54s\n1.0x\n47.9x\n67.8x\n76.3x\n100.1x\n147.7x\n156.0x\n297.8x\n312.9x\n313.1x\n456.5x\n579.6x\nFigure 1: PPO: Pendulum-v1 (300000 steps)\n1.0x\n6.2x\n9.8x\n12.5x\n24.1x25.3x25.9x\n29.5x29.8x\n38.2x\n41.4x\n142.2x\nFigure 2: SAC: Pendulum-v1 (10000 steps)\nPlatform\nRLtools: Generic\nDSP Library\nRLtools: Optimized\nCrazyflie\n743 us (1.3 kHz)\n478 us (2.1 kHz)\n293 us (3.4 kHz)\nPixhawk 6C\n133 us (7.5 kHz)\n93 us (10.8 kHz)\n53 us (18.8 kHz)\nTeensy 4.1\n64 us (15.5 kHz)\n45 us (22.3 kHz)\n41 us (24.3 kHz)\nESP32 (Xtensa)\n4282 us (234 Hz)\n279 us (3.6 kHz)\n333 us (3 kHz)\nESP32-C3 (RISC-V)\n8716 us (115 Hz)\n6950 us (144 Hz)\n6645 us (150 Hz)\nTable 1: Inference times on different platforms\nframeworks. Importantly, we also demonstrate the first training of a deep RL agent for\ncontinuous control on a microcontroller in form of the Teensy 4.1.\nFigure 3: SAC: Pendulum-v1 (10000 steps)\nInference on Microcontrollers Table\n1 shows the inference times on microcon-\ntrollers of different compute capabilities (e.g.\nCrazyflie is a 27 g quadrotor with very lim-\nited resources, cf. Appendix E). The generic\nimplementation already yields usable infer-\nence times but dispatching to the manufac-\nturers Digital Signal Processor (DSP) library\nimproves the performance. Finally, by opti-\nmizing the code further (e.g. through fusing\nthe activation operators) we achieve a signif-\nicant speedup even compared to the manufacturers DSP libraries.\n5 Conclusion\nWe believe RLtools fills a gap by allowing fast iteration in the problem space and subsequent\nreal-time deployment of policies. Furthermore, RLtools facilitates the first-ever deep RL\ntraining on a microcontroller. We acknowledge the steeper learning curve of C++ (over\ne.g. Python) but from our experience, the faster iteration made possible by shorter training\ntimes can outweigh the added time to get started. Currently RLtools is limited to dense\nobservations but we plan to add vision capabilities in the future. We believe that by relaxing\nthe compute requirements and, by being fully open-source, RLtools democratizes the training\nof state-of-the-art RL methods and accelerates progress in RL for continuous control.\n4\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nAcknowledgments and Disclosure of Funding\nThis work was supported by the Technology Innovation Institute, the NSF CAREER\nAward 2145277, and the DARPA YFA Grant D22AP00156-00. Giuseppe Loianno serves as a\nconsultant for the Technology Innovation Institute. This arrangement has been reviewed and\napproved by New York University in accordance with its policy on objectivity in research.\nAppendix A. Analysis of the Deep RL Landscape\nYear\nName\nHidden Dim\n#Params\nNon-Linearity\n2015\nTRPO Schulman et al. (2015)\n[50, 50]\n1.0x\ntanh\n2015\nGAE Schulman et al. (2016)\n[100, 50, 25]\n2.3x\ntanh\n2016\nDDPG Lillicrap et al. (2016)\n[400, 300]\n35.3x\nReLU\n2017\nPPO Schulman et al. (2017)\n[64, 64]\n1.5x\ntanh\n2018\nTD3 Fujimoto et al. (2018)\n[400, 300]\n35.3x\nReLU\n2018\nSAC Haarnoja et al. (2018)\n[256, 256]\n19.6x\nReLU\n2019\nSACv2 Haarnoja et al. (2019)\n[256, 256]\n19.6x\nReLU\n2020\nTQC Kuznetsov et al. (2020)\n[512, 512, 512]\n147.0x\nReLU\n2020\nD4PG&TD3Bach et al. (2020)\n[256, 256]\n19.6x\nReLU\n2021\nPPO&RMA Kumar et al. (2021)\n[128, 128, 128]\n9.8x\nReLU\nTable 2: Selection of works that introduced impactful algorithms and the respective neural\nnetwork dimensions used for their value function approximations. For the calculation\nof the number of parameters, an input size of 20 and an output size of 1 is assumed\nIn this section, we analyze the function approximator models used in the major deep RL for\ncontinuous control publications collected in Table 7. The most important observation is that\nover all the years the architecture (small, fully-connected neural networks) has not changed.\nThis can be attributed to the fact that in continuous control the observations are usually dense\nstates of the systems which do not contain any spatial or temporal regularities like images or\ntime series that would suggest the usage of less general, more tailored network structures\nlike Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs). This\nregularity, as stated in section 3, motivates our focus on optimizing and tightly integrating\nfully-connected neural networks as a first step. We also plan integrate recurrent and possibly\nconvolutional layers in the future.\nAppendix B. Programming Paradigm\nTo enable maximum performance, we are avoiding C++ Virtual Method Table (VMT)\nlookups by not using an object-oriented paradigm but a rather functional paradigm heavily\nbased on templating and method overloading resembling a static, compile-time defined\n5\n\nEschmann, Albani, and Loianno\n// file: implementation_generic.h\ntemplate <typename DEVICE, auto M, auto N, auto K>\nvoid multiply(DEVICE device, Matrix<M, K> a, Matrix<K, N> b, Matrix<M, N> result){\n// Generic code for matrix multiplication\n...\n}\n// file: implementation_microcontroller.h\ntemplate <auto M, auto N, auto K>\nvoid multiply(MICROCONTROLLER device, Matrix<M, K> a, Matrix<K, N> b, Matrix<M, N>\nresult){\n// Optimized code for matrix multiplication on a particular microcontroller (e.g\n. based on DSP extensions)\n...\n}\n// file: implementation_gpu.h\ntemplate <auto M, auto N, auto K>\nvoid multiply(GPU device, Matrix<M, K> a, Matrix<K, N> b, Matrix<M, N> result){\n// Optimized GPU code for matrix multiplication (e.g. CUDA kernel launch)\n...\n}\ntemplate<typename DEVICE, typename OBJECT_A, typename OBJECT_B, typename OBJECT_C>\nvoid algorithm(DEVICE device, OBJECT_A a, OBJECT_B b, OBJECT_C c){\n...\nmultiply(device, a, b, c);\n...\n}\n// usage\nGPU device;\nMatrix<10, 10> a, b, result;\n... // malloc and initialize randomly on the GPU device using tag dispatch as well\nalgorithm(device, a, b, result);\nFigure 4: Toy example for tag dispatch towards different implementations of elementary\nmatrix operations\ninterpretation of the multiple dispatch paradigm. Multiple dispatch has been popularized by\nthe Julia programming language Bezanson et al. (2012) and is based on advanced function\noverloading.\nLeveraging multiple dispatch, higher-level functions like the forward or backward pass of\na fully-connected neural network just specify the actions that should be taken on the different\nsub-components/layers and the actual implementation used is dependent on the type of the\narguments. In this way, it is simple to share code between GPU and CPU implementations by\njust implementing the lower-level primitives for the respective device types and then signaling\nthe implementations through the argument type (i.e. using the tag dispatch technique). A\ntoy example for this is displayed in Figure 4. In this case, some algorithm is using a matrix\nmultiplication operation on two objects. During the implementation of the algorithm, we do\n6\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nnot need to care about the type of the operands and just let them be specified by wildcard\ntemplate parameters. When this function is called by the user, the compiler infers the\ntemplate parameters and dispatches the call to the appropriate implementation. If the user\ndoes not have a GPU available he simply does not include the implementation_gpu.h and\nhence has no dependency on further dependencies that the GPU implementation would entail\n(e.g., the CUDA toolkit). In the case where there is no specialized implementation for a\nparticular hardware, the compiler will fall back to the generic implementation which in this\nexample could simply consist of a nested loop. The generic implementations are pure C++\nand are guaranteed to have no dependencies. We can also see that the compiler will check\nthe dimensions of the operands automatically at compile time such that the algorithm can\nnot be called with incompatible shapes. To create more complex dispatch behaviors and\noperand type checking C++ features like static_assert and enable_if can be leveraged\nthrough the Substitution Failure Is Not An Error (SFINAE) mechanism. In this way, we can\nmaintain composability while still providing all the structure to the compiler at compile-time.\nIn the case of Julia, this leads to unparalleled composability which manifests in a\nsmall number of people being able to implement and maintain e.g. a deep learning library\n(Flux Innes (2018)) that is competitive with PyTorch and TensorFlow which are backed\nby much more resources. In contrast to Julia, which reaches almost native performance\nwhile performing the multiple dispatch resolution at runtime, we make sure that all the\nfunction calls can be resolved at compile time. Additionally, Julia is not suited for our\npurposes because it does not fit to run on microcontrollers due to its runtime size and\nstochastic, non-realtime behavior due to the garbage collection-based memory management.\nNevertheless, in our benchmark presented later in this manuscript, we found that Julia is\none of the closest competitors when it comes to training performance. Furthermore, we find\nit important to emphasize that we focus on building a library not a framework.1 The main\nfeature of frameworks is that they restrict the freedoms of the user to make a small set of\ntasks easier to accomplish. In certain, repetitive problem settings this might be justified, but\nin many cases, the overhead coming with the steep learning curves and finding workarounds\nafter bumping into the tight restrictions of frameworks is not worth it. The major conceptual\ndifference is that frameworks provide a context from which they invoke the user’s code\nwhile in the case of libraries, the user is entirely in control and invokes the components\nhe needs. If not specifically made interoperable, the contexts provided by frameworks are\nusually incompatible while with libraries this is not generally the case.\nIn our implementation, this for example concretely manifests in the way function approx-\nimators are used in the RL algorithms. By using templating, any function approximator\ntype can be specified by the user at compile time. As long as he also provides the required\nforward and backward functions.\nAs demonstrated in Figure 4 we establish the convention of making a device-dependent\ncontext available in each function via tag dispatch to simplify the usage of different compute\ndevices like accelerators or microcontrollers.\n1. Write Libraries, Not Frameworks [link]\n7\n\nEschmann, Albani, and Loianno\nAppendix C. Benchmark Details\nParameter\nValue\nActor structure\n[64, 64]\nCritic structure\n[64, 64]\nActivation function\nRectified Linear Unit (ReLU)\nBatch size\n256\nNumber of environments\n4\nSteps per environment\n1024\nNumber of epochs\n2\nTotal number of (environment) steps\n300000\nDiscount factor γ\n0.9\nGeneralized Advantage Estimation (GAE) λ\n0.95\nϵ clip\n0.2\nEntropy coefficient β\n0\nAdvantage normalization\ntrue\nAdam α\n1 × 10−3\nAdam β1\n0.9\nAdam β2\n0.999\nAdam ϵ\n1 × 10−7\nTable 3: Pendulum-v1 PPO parameters (Figure 1)\n8\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nParameter\nValue\nActor structure\n[64, 64]\nCritic structure\n[64, 64]\nActivation function\nReLU\nBatch size\n100\nTotal number of (environment) steps\n10000\nReplay buffer size\n10000\nDiscount factor γ\n0.99\nEntropy bonus coefficient (learned, initial value) α\n0.5\nPolyak β\n0.99\nAdam α\n1 × 10−3\nAdam β1\n0.9\nAdam β2\n0.999\nAdam ϵ\n1 × 10−7\nTable 4: Pendulum-v1 SAC parameters (Figure 2)\nParameter\nValue\nInput dimensionality\n13\nPolicy structure\n[64, 64]\nOutput dimensionality\n4\nActivation function\nReLU\nTable 5: On-device inference parameters (Table 1)\nLabel\nDetails\nRLtools / Laptop (CPU) / Laptop (Web) / Baseline\nIntel i9-10885H\nLaptop (GPU)\nIntel i9-10885H + Nvidia T2000\nMacBook (CPU) / MacBook (Web)\nMacBook Pro (M3 Pro)\niPhone (Native) / iPhone (Web)\niPhone 14\nApple Watch (Native)\nApple Watch Series 4\nTable 6: Pendulum-v1 SAC devices (Figure 3)\n9\n\nEschmann, Albani, and Loianno\nAppendix D. Deep Reinforcement Learning Frameworks and Libraries\nName ↓\nPlatform\nStars / Citations\nAcme (Hoffman et al., 2020)\nJAX\n3316 / 219\nCleanRL (Huang et al., 2022)\nPyTorch\n4030 / 86\nMushroomRL (D’Eramo et al., 2021)\nTF/PyTorch\n749 / 61\nPFRL (Fujita et al., 2021)\nPyTorch\n1125 / 122\nReinforcementLearning.jl (JuliaRL) (Tian, 2020)\nFlux.jl (Julia)\n543 / n/a\nRLlib + ray (Liang et al., 2018)\nPyTorch\n29798 / 828\nStable Baselines3 (SB3) (Raffin et al., 2021)\nPyTorch\n7396 / 1149\nStable Baselines JAX (SBX) (Raffin et al., 2021)\nJAX\n223 / n/a\nTianshou (Weng et al., 2022)\nPyTorch\n7139 / 133\nTorchRL (Weng et al., 2022)\nPyTorch\n1691 / 5\nTable 7: Overview over different RL libraries/frameworks, the deep learning platform they\nbuild upon, and their popularity in terms of Github stars and publication citations\n(data as of 2024-02-07)\nAppendix E. Embedded Platforms\n1. Crazyflie: A small, open-source quadrotor which only weighs 27 g including the\nbattery. The Crazyflie’s main processor is a STM32F405 microcontroller using the\nARM Cortex-M4 architecture, featuring 192 KB of Random Access Memory (RAM)\nand running at 168 MHz.\n2. Pixhawk 6C: We use a Pixracer Pro, a Flight Controller Unit (FCU) that belongs\nto the family of Pixhawk FCUs and implements the Pixhawk 6C standard. Hence,\nthe PixRacer Pro supports the common PX4 firmware Meier et al. (2015) and can be\nused in many different vehicle types (aerial, ground, marine) but is predominantly used\nin multirotor vehicles of varying sizes. The main processor used in the Pixhawk 6C\nstandard is a STM32H743 using the ARM Cortex-M7 architecture. The PixRacer Pro\nruns at 460 MHz and comes with 1024 KB of RAM.\n3. Teensy 4.1: A general-purpose embedded device powered by an i.MX RT1060 ARM\nCortex-M7 microcontroller with 1024 KB on-chip and 16 MB off-chip RAM that is\nrunning at 600 MHz.\n4. ESP32: One of the most common microcontrollers for Internet of Things (IoT) and\nedge devices due to its built-in Wi-Fi and Bluetooth. Close to 1 billion devices built\naround this chip and its predecessor have been sold worldwide. Hence it is widely\navailable and relatively cheap (around $5 for a development kit). For our purposes,\n10\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nthe ESP32 is interesting because it deviates from the previous platforms in that its\nprocessor is based on the Xtensa LX7 architecture. In addition to the original version\nof the ESP32 based on the Xtensa architecture, we also evaluate the ESP32-C3 version\nbased on the RISC-V architecture.\nAppendix F. Ablation Study\nFigure 5: Ablation study. The “Baseline” con-\ntains all optimizations.\nWe conduct an ablation study to investigate\nthe contribution of different components and\noptimizations to the fast wall-clock training\ntime achieved by RLtools. Figure 5 shows\nthe resulting training times after removing\ndifferent components and optimizations from\nthe setup. The “Baseline” is exactly the same\nsetup used in the Pendulum-v1 (SAC) train-\ning in the other experiments in Section 4.\nWe simulate the slowness of the Python en-\nvironment by slowing down the C++ imple-\nmentation by the average time required for\na step in the Python implementation. We\ncan observe that the C++ implementation\nof the Pendulum-v1 dynamics has a mea-\nsurable, but not dominating impact on the\ntraining time. Additionally, we ablate the\ndifferent optimization levels -O0, -O1, -O2 and -O3 (used in the Baseline) of the C++ com-\npiler. We can observe that the compiler optimizations have a sizable impact on the training\ntime. When removing all optimizations (-O0) RLtools is roughly between ACME and\nCleanRL (cf. Figure 2). Furthermore, the “No Fast Math” configuration tests removing\nthe -ffast-math from the compiler options, and the “BLAS” Basic Linear Algebra Sub-\nprograms (BLAS) option removes the Intel oneMKL matrix multiplication kernels. In the\ncase of “AVX/AVX2” we disable the Advanced Vector Extensions (AVX) that are used for\nSingle Instruction, Multiple Data (SIMD) operations. We notice that due to the design of\nRLtools (refer to Appendix B) which allows the sizes of all loops and data structures to\nbe known at compile-time the compiler is able to better reason about the code and hence\nmake heavy use of vectorized/SIMD operations. We observe that 2276 + 1430 = 3706 (AVX\n+ Streaming SIMD Extensions (SSE), an older set of vectorized instructions) out of 11243\nmachine-code instructions in total refer to registers of the vector extensions. Unfortunately\n(for the sake of measurement), when turning off AVX, the compiler replaces the instructions\nwith SSE instructions (5406 out of 11243 in this case) which we could not turn off because\nof some dependency in libstdc++. Still, the number of SSE instructions demonstrates the\ncompiler-friendliness that RLtools’ architecture entails.\n11\n\nEschmann, Albani, and Loianno\nAppendix G. Convergence Study\nTo make sure the implementations of the supported RL algorithms (PPO, TD3, and SAC)\nare correct, we conduct a convergence study where we compare the learning curves across\ndifferent environments with learning curves of other implementations. We make sure that\nper environment the same hyperparameters are used across all implementations and run each\nsetup for multiple seeds (100 for Pendulum-v1 and 30 for Hopper-v1). For each of the seeds\nat every evaluation step, we perform 100 episodes with random initial states.\nBy comparing different sets of 100 seeds each, we found that, even for a large number of\nseeds, outliers have a significant impact on the mean final return. Hence, as also recommended\nby Agarwal et al. (2021), we report the Inter Quantile Mean (IQM) which discards the\nlowest and highest quantile to remove the impact of outliers on the statistics. We still aim at\ncapturing as much of the final return distribution by only discarding the lower upper 5 % for\nthe calculation of the IQM µ. We use the same inter-quantile set for the calculation of the\nstandard deviation σ. To make sure that the environments are identical in this convergence\nstudy, instead of re-implementing the environments in C++ using the RLtools interface, we\nbuilt a Python wrapper for RLtools such that we can use the original environments from\nthe Gymnasium (Towers et al.) suite. The Python wrapper makes RLtools easier to use but\nsacrifices in terms of performance if the environment/simulator is implemented in Python\n(as shown in Appendix F).\nFigure 6: PPO Pendulum-v1\n12\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nFigure 7: PPO Hopper-v4\nFigure 8: PPO Ant-v4\n13\n\nEschmann, Albani, and Loianno\nFigure 9: SAC Pendulum-v1\nFigure 10: SAC Hopper-v4\n14\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nFigure 11: SAC Ant-v4\nFigure 12: TD3 Pendulum-v1\n15\n\nEschmann, Albani, and Loianno\nFigure 13: TD3 Hopper-v4\nFigure 14: TD3 Ant-v4\n16\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nReferences\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc\nBellemare.\nDeep reinforcement learning at the edge of the statistical precipice.\nIn\nM. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 29304–29320.\nCurran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/\npaper/2021/file/f514cec81cb148559cf475e7426eed5e-Paper.pdf.\nNicolas Bach, Andrew Melnik, Malte Schilling, Timo Korthals, and Helge Ritter. Learn\nto Move Through a Combination of Policy Gradient Algorithms:\nDDPG, D4PG,\nand TD3.\nIn Giuseppe Nicosia, Varun Ojha, Emanuele La Malfa, Giorgio Jansen,\nVincenzo Sciacca, Panos Pardalos, Giovanni Giuffrida, and Renato Umeton, editors,\nMachine Learning, Optimization, and Data Science, volume 12566, pages 631–644.\nSpringer International Publishing, Cham, 2020. ISBN 978-3-030-64579-3 978-3-030-64580-\n9.\ndoi: 10.1007/978-3-030-64580-9_52.\nURL http://link.springer.com/10.1007/\n978-3-030-64580-9_52. Series Title: Lecture Notes in Computer Science.\nJeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman. Julia: A Fast Dynamic\nLanguage for Technical Computing, September 2012. URL http://arxiv.org/abs/1209.\n5145. arXiv:1209.5145 [cs].\nAlbert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng\nYang, Gianni De Fabritiis, and Vincent Moens. TorchRL: A data-driven decision-making\nlibrary for pytorch. In The Twelfth International Conference on Learning Representations,\n2024. URL https://openreview.net/forum?id=QxItoEAVMb.\nCarlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mush-\nroomrl: Simplifying reinforcement learning research. Journal of Machine Learning Research,\n22(131):1–5, 2021. URL http://jmlr.org/papers/v22/18-056.html.\nJonas Eschmann. Reward function design in reinforcement learning. Reinforcement Learning\nAlgorithms: Analysis and Applications, pages 25–33, 2021.\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation\nerror in actor-critic methods. In Jennifer Dy and Andreas Krause, editors, Proceedings\nof the 35th International Conference on Machine Learning, volume 80 of Proceedings\nof Machine Learning Research, pages 1587–1596. PMLR, 10–15 Jul 2018. URL https:\n//proceedings.mlr.press/v80/fujimoto18a.html.\nYasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, and Takahiro Ishikawa. Chainerrl: A\ndeep reinforcement learning library. Journal of Machine Learning Research, 22(77):1–14,\n2021. URL http://jmlr.org/papers/v22/20-376.html.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy\nand Andreas Krause, editors, Proceedings of the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine Learning Research, pages 1861–1870. PMLR,\n10–15 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.\n17\n\nEschmann, Albani, and Loianno\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,\nVikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft\nActor-Critic Algorithms and Applications, January 2019. URL http://arxiv.org/abs/\n1812.05905. arXiv:1812.05905 [cs, stat].\nMatthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola\nMomchev, Danila Sinopalnikov, Piotr Stańczyk, Sabela Ramos, Anton Raichuk, Damien\nVincent, Léonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis\nJacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin,\nOlivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer,\nFan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov,\nSergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan\nSrinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research\nframework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.\nURL https://arxiv.org/abs/2006.00979.\nShengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty,\nKinal Mehta, and Joao G.M. Araujo. Cleanrl: High-quality single-file implementations of\ndeep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):\n1–18, 2022. URL http://jmlr.org/papers/v23/21-1342.html.\nMike Innes. Flux: Elegant machine learning with Julia. Journal of Open Source Software, 3\n(25):602, May 2018. ISSN 2475-9066. doi: 10.21105/joss.00602. URL http://joss.theoj.\norg/papers/10.21105/joss.00602.\nAshish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik.\nRMA: Rapid Motor\nAdaptation for Legged Robots. In Proceedings of Robotics: Science and Systems, Virtual,\nJuly 2021. doi: 10.15607/RSS.2021.XVII.011.\nArsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov. Controlling\noverestimation bias with truncated mixture of continuous distributional quantile critics.\nIn Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Con-\nference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,\npages 5556–5566. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/\nkuznetsov20a.html.\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg,\nJoseph Gonzalez, Michael Jordan, and Ion Stoica. RLlib: Abstractions for distributed\nreinforcement learning.\nIn Jennifer Dy and Andreas Krause, editors, Proceedings of\nthe 35th International Conference on Machine Learning, volume 80 of Proceedings of\nMachine Learning Research, pages 3053–3062. PMLR, 10–15 Jul 2018.\nURL https:\n//proceedings.mlr.press/v80/liang18b.html.\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement\nlearning, 2016. URL http://arxiv.org/abs/1509.02971. arXiv:1509.02971 [cs, stat].\n18\n\nRLtools: A Fast, Portable Deep Reinforcement Learning Library\nLorenz Meier, Dominik Honegger, and Marc Pollefeys. Px4: A node-based multithreaded open\nsource robotics framework for deeply embedded platforms. In 2015 IEEE International\nConference on Robotics and Automation (ICRA), pages 6235–6240, 2015. doi: 10.1109/\nICRA.2015.7140074.\nAntonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah\nDormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal\nof Machine Learning Research, 22(268):1–8, 2021. URL http://jmlr.org/papers/v22/\n20-1364.html.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust\nregion policy optimization.\nIn Francis Bach and David Blei, editors, Proceedings of\nthe 32nd International Conference on Machine Learning, volume 37 of Proceedings of\nMachine Learning Research, pages 1889–1897, Lille, France, 07–09 Jul 2015. PMLR. URL\nhttps://proceedings.mlr.press/v37/schulman15.html.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. In Proceedings of\nthe International Conference on Learning Representations (ICLR), 2016.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\nPolicy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347.\narXiv:1707.06347 [cs].\nAntonio Serrano-Munoz, Dimitrios Chrysostomou, Simon Bøgh, and Nestor Arana-\nArexolaleiba. skrl: Modular and flexible library for reinforcement learning. Journal\nof Machine Learning Research, 24(254):1–9, 2023.\nJun Tian. Reinforcementlearning.jl: A reinforcement learning package for the julia pro-\ngramming language, 2020. URL https://github.com/JuliaReinforcementLearning/\nReinforcementLearning.jl.\nMark Towers, Jordan K Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan\nDeleu, Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-\nVicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Andrew Jin Shen Tan, and Omar G.\nYounis. Gymnasium. URL https://github.com/Farama-Foundation/Gymnasium.\nRobin Verschueren, Gianluca Frison, Dimitris Kouzoupis, Jonathan Frey, Niels Van Duijkeren,\nAndrea Zanelli, Branimir Novoselnik, Thivaharan Albin, Rien Quirynen, and Moritz\nDiehl. acados—a modular open-source framework for fast embedded optimal control.\nMathematical Programming Computation, 14(1):147–183, March 2022. ISSN 1867-2949,\n1867-2957.\ndoi: 10.1007/s12532-021-00208-8.\nURL https://link.springer.com/10.\n1007/s12532-021-00208-8.\nJiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang,\nYi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement\nlearning library. Journal of Machine Learning Research, 23(267):1–6, 2022. URL http:\n//jmlr.org/papers/v23/21-1127.html.\n19",
    "pdf_filename": "RLtools_A_Fast,_Portable_Deep_Reinforcement_Learning_Library_for_Continuous_Control.pdf"
}