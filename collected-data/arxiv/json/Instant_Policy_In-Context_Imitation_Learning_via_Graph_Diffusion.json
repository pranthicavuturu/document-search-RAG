{
    "title": "Instant Policy In-Context Imitation Learning via Graph Diffusion",
    "abstract": "Following the impressive capabilities of in-context learning with large transform- ers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy. Figure 1: Instant Policy acquires skills instantly after providing demos at test time. We model in- context imitation learning as a graph-based diffusion process, trained using pseudo-demonstrations. 1 INTRODUCTION Robot policies acquired through Imitation Learning (IL) have recently shown impressive capabili- ties, but today’s Behavioural Cloning (BC) methods still require hundreds or thousands of demon- strations per task (Zhao et al.). Meanwhile, language and vision communities have shown that when large transformers are trained on sufficiently large and diverse datasets, we see the emergence of In-Context Learning (ICL) (Brown, 2020). Here, trained models can use test-time examples of a novel task (the context), and instantly generalise to new instances of this task without updating the model’s weights. This now offers a promising opportunity of In-Context Imitation Learning (ICIL) in robotics. To this end, we present Instant Policy, which enables new tasks to be learned instantly: after providing just one or two demonstrations, new configurations of that task can be performed immediately, without any further training. This is far more time-efficient and convenient than BC methods, which require numerous demonstrations and hours of network training for each new task. ICL in language and vision benefits from huge and readily available datasets, which do not exist for robotics. As such, we are faced with two primary challenges. 1) Given the limited available data, we need appropriate inductive biases in observation and action representations for efficient learning in 1 arXiv:2411.12633v1  [cs.RO]  19 Nov 2024",
    "body": "INSTANT POLICY: IN-CONTEXT IMITATION\nLEARNING VIA GRAPH DIFFUSION\nVitalis Vosylius and Edward Johns\nThe Robot Learning Lab at Imperial College London\nvitalis.vosylius19@imperial.ac.uk\nABSTRACT\nFollowing the impressive capabilities of in-context learning with large transform-\ners, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics.\nWe introduce Instant Policy, which learns new tasks instantly (without further\ntraining) from just one or two demonstrations, achieving ICIL through two key\ncomponents. First, we introduce inductive biases through a graph representation\nand model ICIL as a graph generation problem with a learned diffusion process,\nenabling structured reasoning over demonstrations, observations, and actions.\nSecond, we show that such a model can be trained using pseudo-demonstrations\n– arbitrary trajectories generated in simulation – as a virtually infinite pool of\ntraining data. Simulated and real experiments show that Instant Policy enables\nrapid learning of various everyday robot tasks. We also show how it can serve as a\nfoundation for cross-embodiment and zero-shot transfer to language-defined tasks.\nCode and videos are available at https://www.robot-learning.uk/instant-policy.\nFigure 1: Instant Policy acquires skills instantly after providing demos at test time. We model in-\ncontext imitation learning as a graph-based diffusion process, trained using pseudo-demonstrations.\n1\nINTRODUCTION\nRobot policies acquired through Imitation Learning (IL) have recently shown impressive capabili-\nties, but today’s Behavioural Cloning (BC) methods still require hundreds or thousands of demon-\nstrations per task (Zhao et al.). Meanwhile, language and vision communities have shown that when\nlarge transformers are trained on sufficiently large and diverse datasets, we see the emergence of\nIn-Context Learning (ICL) (Brown, 2020). Here, trained models can use test-time examples of a\nnovel task (the context), and instantly generalise to new instances of this task without updating the\nmodel’s weights. This now offers a promising opportunity of In-Context Imitation Learning (ICIL)\nin robotics. To this end, we present Instant Policy, which enables new tasks to be learned instantly:\nafter providing just one or two demonstrations, new configurations of that task can be performed\nimmediately, without any further training. This is far more time-efficient and convenient than BC\nmethods, which require numerous demonstrations and hours of network training for each new task.\nICL in language and vision benefits from huge and readily available datasets, which do not exist for\nrobotics. As such, we are faced with two primary challenges. 1) Given the limited available data, we\nneed appropriate inductive biases in observation and action representations for efficient learning in\n1\narXiv:2411.12633v1  [cs.RO]  19 Nov 2024\n\n3D space; 2) Given the inefficiency and cost of manually collecting robotics data, we need a means\nto easily collect training data in a scalable way. In this work, we propose solutions to both these.\nWe address the first challenge by introducing a novel graph-based representation that integrates\ndemonstrations, current point cloud observations, and the robot’s actions within a unified graph\nspace. We then cast ICIL as a diffusion-based graph generation problem, enabling demonstrations\nand observations to be interpreted effectively in order to predict the robot’s actions. To address the\nsecond challenge, we observe that in traditional BC, the model’s weights directly encode policies\nfor a specific set of tasks, whereas in ICIL, the model’s weights should encode a more general, task-\nagnostic ability to interpret and act upon the given context. Due to this, we found that we were able\nto train the model using pseudo-demonstrations – sets of procedurally generated robot trajectories,\nbut where each set of demonstrations for a task is semantically consistent. This approach allows us\nto generate virtually infinite training data by scaling up the simulated data.\nOur experiments, with both simulated and real-world tasks, show that Instant Policy can learn vari-\nous everyday tasks, whilst achieving higher task success rates than state-of-the-art baselines trained\non the same data. As an emergent ability, we also observed generalisation capabilities to object\ngeometries unseen from the test-time demonstrations. Importantly, we found that performance im-\nproves as more data is generated and used for simultaneous training, offering scalable opportunities\nwith abundant simulated data. In our further experiments on downstream applications, Instant Pol-\nicy also achieves cross-embodiment transfer from human-hand demonstrations to robot policies, and\nzero-shot transfer to language-defined tasks without needing large language-annotated datasets.\nOur contributions are as follows: 1) We cast In-Context Imitation Learning as a diffusion-based\ngraph generation problem; 2) We show that this model can be trained using procedurally generated\npseudo-demonstrations; 3) We evaluate in simulation and the real world across various everyday\ntasks, showing strong performance, encouraging scaling trends, and promising downstream uses.\n2\nRELATED WORK\nIn-Context Learning (ICL). ICL is an emerging paradigm in machine learning which allows mod-\nels to adapt to new tasks using a small number of examples, without requiring explicit weight updates\nor retraining. Initially popularised in natural language processing with models like GPT-3 (Brown,\n2020), ICL has been applied to enable robots to rapidly adapt to new tasks by using foundation\nmodels (Di Palo & Johns, 2024), finding consistent object alignments (Vosylius & Johns, 2023a),\nidentifying invariant regions of the state space (Zhang & Boularias, 2024), and directly training\npolicies aimed at task generalisation (Duan et al., 2017; Fu et al., 2024) or cross-embodiment trans-\nfer (Jang et al., 2022; Jain et al., 2024; Vogt et al., 2017). Despite these advancements, challenges\nremain in achieving generalisation to tasks unseen during training and novel object geometries. In-\nstant Policy addresses this by leveraging simulated pseudo-demonstrations to generate abundant and\ndiverse data, while its structured graph representation ensures that this data is utilised efficiently.\nDiffusion Models. Diffusion models (Ho et al., 2020) have garnered significant attention across\nvarious domains, due to their ability to iteratively refine randomly sampled noise through a learned\ndenoising process, ultimately generating high-quality samples from the underlying distribution. Ini-\ntially popularised for image generation (Ramesh et al., 2021), diffusion models have recently been\napplied to robotics. They have been utilised for creating image augmentations (Yu et al., 2023; Man-\ndlekar et al., 2023) to help robots adapt to diverse environments, generating ‘imagined’ goals (Kape-\nlyukh et al., 2023) or subgoals (Black et al., 2023) for guiding robotic policies, and learning precise\ncontrol policies (Chi et al., 2023; Vosylius et al., 2024). In contrast, our work proposes a novel use\nof diffusion models for graph generation, enabling structured learning of complex distributions.\nGraph Neural Networks (GNNs). Graph Neural Networks (GNNs) allow learning on structured\ndata using message-passing or attention-based strategies.\nThese capabilities have been applied\nacross a wide range of domains, including molecular chemistry Jha et al. (2022), social network\nanalysis (Hu et al., 2021), and recommendation systems (Shi et al., 2018). In robotics, GNNs have\nbeen employed for obtaining reinforcement learning (RL) policies (Wang et al., 2018; Sferrazza\net al., 2024), managing object rearrangement tasks (Kapelyukh & Johns, 2022), and learning affor-\ndance models for skill transfer Vosylius & Johns (2023b). In our work, we build on these foundations\nby studying structured graph representations for ICIL, enabling learning of the relationships between\ndemonstrations, observations, and actions.\n2\n\n3\nINSTANT POLICY\n3.1\nOVERVIEW & PROBLEM FORMULATION\nOverview. We address the problem of In-Context Imitation Learning, where the goal is for the robot\nto complete a novel task immediately after the provided demonstrations. At test time, one or a few\ndemos of a novel task are provided to define the context, which our trained Instant Policy network\ninterprets together with the current point cloud observation, and infers robot actions suitable for\nclosed-loop reactive control (see Figure 1). This enables instantaneous policy acquisition, without\nextensive real-world data collection or training. We achieve this through a structured graph repre-\nsentation (Section 3.2), a learned diffusion process (Section 3.3), and an abundant source of diverse\nsimulated pseudo-demonstrations (Section 3.4).\nProblem Formulation. We express robot actions a as end-effector displacements TEA ∈SE(3)\n(which, when time-scaled, correspond to velocities), along with binary open-close commands for\nthe gripper, ag ∈{0, 1}. Such actions move the robot’s gripper from frame E to a new frame\nA and change its binary state accordingly.\nOur observations, ot at timestep t, consist of seg-\nmented point clouds P t, the current end-effector pose in the world frame W, T t\nW E ∈SE(3),\nand a binary gripper state st\ng ∈{0, 1}. Formally, our goal is to find a probability distribution,\np(at:t+T | ot, {(oij, aij)L\ni=1}N\nj=1), from which robot actions can be sampled and executed. Here,\nT denotes the action prediction horizon, while L and N represent the demonstration length and the\nnumber of demonstrations, respectively. For conciseness, from now onwards we refer to the demon-\nstrations, which define the task at test time and are not used during training, as context C, and the\naction predictions as a. Analytically defining such a distribution is infeasible, therefore we aim to\nlearn it from simulated pseudo-demonstrations using a novel graph-based diffusion process.\n3.2\nGRAPH REPRESENTATION\nTo learn the described conditional probability of actions, we first need to choose a suitable repre-\nsentation that would capture the key elements of the problem and introduce appropriate inductive\nbiases. We propose a heterogeneous graph that jointly expresses context, current observation, and\nfuture actions, capturing complex relationships between the robot and the environment and ensuring\nthat the relevant information is aggregated and propagated in a meaningful manner. This graph is\nconstructed using segmented point cloud observations, as shown in Figure 2.\nState Representation\nJoint Demo, Current State and Future Actions Representation\nCurrent State\nAction 1\nAction T\nDemo N \nDemo 1\nContext\nSequence of States\n1\n2\nL\nFigure 2: (Left) A local graph, Gl, representing the robot’s state (blue nodes) and local geometries\nof the objects (green nodes). (Right) A graph representing 2 demos (3 waypoints each), the current\nstate, and 2 future actions. Edge colours represent different edge types in a heterogeneous graph.\nLocal Representation. The core building block of our representation is the observation at time\nstep t, which we express as a local graph Gt\nl (P t, T t\nW E, sg) (Figure 2, left). First, we sample M\npoints from a dense point cloud P t using the Furthest Point Sampling Algorithm and encode lo-\ncal geometry around them with Set Abstraction (SA) layers (Qi et al., 2017), obtaining feature\nvectors F and positions p as {Fi, pi}M\ni=1 = ϕ(P t). The separately pre-trained ϕ, an implicit oc-\n3\n\ncupancy network (Mescheder et al., 2019), ensures these features describe local geometries (details\nin Appendix A). We then represent the gripper’s state in the same format, {Fi\ng, pi\ng}6\ni=1, by rigidly\ntransforming key points pkp on the end-effector, pg = TW E × pkp and assigning them embeddings\nthat encode node distinction and gripper state information Fi\ng = [f i\ng, ϕg(sg)]T . Finally, we link\nscene and gripper nodes with directional edges and assign edge attributes e representing relative\npositions in Cartesian space. To increase the precision and capture high-frequency changes in the\npositions of the described nodes, we represent edges as eij = (sin(20π(pj −pi)), cos(20π(pj −\npi)), ..., sin(2D−1π(pj −pi)), cos(2D−1π(pj −pi))), similar to Zhou et al. (2023).\nContext Representation. While Gt\nl captures the environment state, a sequence of such graphs,\ninterleaved with actions, defines a trajectory within context C (Figure 2, middle). We perform this\ninterleaving by linking gripper nodes across time to represent their relative movement (red edges)\nand connecting all demonstration gripper nodes to the current ones to propagate relevant information\n(grey edges). This enables the graph to efficiently handle any number of demos, regardless of length,\nwhilst ensuring that the number of edges grows linearly. The result, Gc(Gt\nl , {G1:L\nl\n}N\n1 ), enables a\nstructured flow of information between the context and the current observation.\nAction Representation.\nTo express future actions a = (TEA, ag) within the graph repre-\nsentation, we construct local graphs as if the actions were executed and the gripper moved:\nGa\nl (P t, T t\nW E × TEA, ag). This allows ‘imagining’ spatial implications of actions. Thus, the ac-\ntions are fully described within the positions and the features of the nodes of these local graphs.\nTo represent actions as relative movements from the current gripper pose, we then add edges\nbetween current and future gripper nodes with position-based embeddings that represent relative\nmovement between subsequent timesteps. These edges propagate the information from the cur-\nrent observation (and indirectly the context) to the nodes representing the actions. The final graph,\nG(Ga\nl (a), Gc(Gt\nl , {G1:L\nl\n}N\n1 )), aggregates relevant information from the context and the current ob-\nservation and propagates it to nodes representing the actions, enabling effective reasoning about the\nrobot actions by ensuring the observations and actions are expressed in the same graph space.\n3.3\nLEARNING ROBOT ACTION VIA GRAPH DIFFUSION\nTo utilise our graph representation effectively, we frame ICIL as a graph generation problem and\nlearn a distribution over previously described graphs pθ(G) using a diffusion model, depicted in\nFigure 3. This approach involves forward and backward Markov-chain processes, where the graph\nis altered and reconstructed in each phase. At test time, the model iteratively updates only the parts of\nthe graph representing robot actions, implicitly modelling the desired conditional action probability.\nFigure 3: (Left) High-level structure of the network used to train graph-based diffusion model.\n(Right) Position of gripper action nodes during the denoising process for one of the predicted actions.\nTraining. Training our diffusion model includes, firstly, the forward process, where noise is itera-\ntively added to the samples extracted from the underlying data distribution q(G). In this phase, we\nconstruct a noise-altered graph by adding noise to the robot actions according to Ho et al. (2020):\nq(Gk | Gk−1) = G(Ga\nl (N(ak;\np\n1 −βkak−1, βkI), Gc)),\nk = 1, . . . , K\n(1)\n4\n\nHere, N represents the normal distribution, βk the variance schedule, and K the total number of\ndiffusion steps. This process gradually transitions the ground truth graph representation into a graph\nconstructed using actions sampled from a Gaussian distribution.\nInversely, in the reverse diffusion process, the aim is to reconstruct the original data sample, in\nour case the graph, from its noise-altered state, utilising a parameterised model pθ(Gk−1 | Gk).\nIntuitively, such a model needs to learn how the gripper nodes representing the robot actions should\nbe adjusted, such that the whole graph moves closer to the true data distribution q(G). Formally, the\nparameterised model learns a denoising process of actions using our graph representation G(a) as:\nGk−1 = G(Ga\nl (α(ak −γεθ(Gk, k)) + N(0, σ2I)), Gc)\n(2)\nHere, εθ(.) can be interpreted as effectively predicting the gradient field, based on which a single\nnoisy gradient descent step is taken (Chi et al., 2023). As we represent actions as collections of\nnodes with their associated positions p and features, that depend on the binary gripper actions ag,\nsuch a gradient field has two components εθ = [∇p, ∇ag]T . As we will discuss later, ∇ag can\nbe used directly in the diffusion process, while a set of ∇p predictions is an over-parameterisation\nof a gradient direction on the SE(3) manifold, and additional steps need to be used to compute a\nprecise denoising update. However, this can result in a large translation dominating a small rotation,\nand vice versa, preventing precisely learning both components well. To address this, we represent\nthe denoising directions as a combination of centre-of-mass movement and rotation around it, ef-\nfectively decoupling the translation and rotation predictions while remaining in Cartesian space as\n[∇ˆpt, ∇ˆpr]T = [t0\nEA −tk\nEA, R0\nEA × pkp −Rk\nEA × pkp]T , with ∇ˆp = ∇ˆpt + ∇ˆpr representing flow\n(red arrows in Figure 3, left). Here, tEA ∈R3 and REA ∈SO(3) define the SE(3) transformation\nrepresenting actions TEA. Thus we learn εθ by making per-node predictions εk ∈R7 and optimis-\ning the variational lower bound of the data likelihood which has been shown (Ho et al., 2020) to be\nequivalent to minimising MSE(εk −εθ(Gk)). As our parameterised model, we use a heterogeneous\ngraph transformer, which updates features of each node in the graph, Fi, as (Shi et al., 2020):\nF′\ni = W1Fi+\nX\nj∈N (i)\natti,j (W2Fj + W5eij) ;\natti,j = softmax\n\u0012(W3Fi)T (W4Fj + W5eij)\n√\nd\n\u0013\n(3)\nHere, Wi represent learnable weights. Equipping our model with such a structured attention mech-\nanism allows for selective and informative information aggregation which is propagated through\nthe graph in a meaningful way, while ensuring that memory and computational complexity scales\nlinearly with increasing context length (both N and L). More details can be found in Appendix C.\nDeployment. During test time, we create the graph representation using actions sampled from the\nnormal distribution, together with the current observation and the demonstrations as the context. We\nthen make predictions describing how gripper nodes should be adjusted, and update the positions of\nthese nodes by taking a denoising step according to the DDIM (Song et al., 2020):\npk−1\ng\n= √αk−1ˆp0\ng +\nr\n1 −αk−1\n1 −αk\n\u0000pk\ng −√αk ˆp0\ng\n\u0001\n.\n(4)\nHere, ˆp0\ng = pk\ng + ∆pt + ∆pr. This leaves us with two sets of points pk−1\ng\nand pk\ng, that implicitly\nrepresent gripper poses at denoising time steps k −1 and k. As we know the ground truth corre-\nspondences between them, we can extract an SE(3) transformation that would align them using a\nSingular Value Decomposition (SVD) (Arun et al., 1987) as:\nTk−1,k =\narg min\nTk−1,k∈SE(3)\n||pk−1 −Tk−1,k × pk||2\n(5)\nFinally, the ak−1 is calculated by applying calculated transformation Tk−1,k to ak. Note that for\ngripper opening and closing actions utilising Equation 4 directly is sufficient. This process is re-\npeated K times until the graph that is in distribution is generated and, as a byproduct, final a0\nactions are extracted, allowing us to sample from the initially described distribution p(a | ot, C).\n5\n\n3.4\nAN INFINITE POOL OF DATA\nNow that we can learn the conditional distribution of actions, we need to answer the question of\nwhere a sufficiently large and diverse dataset will come from, to ensure that the learned model can\nbe used for a wide range of real-world tasks. With In-Context Learning, the model does not need to\nencode task-specific policies into its weights. Thus it is possible to simulate ‘arbitrary but consistent’\ntrajectories as training data. Here, consistent means that while the trajectories differ, they ‘perform’\nthe same type of pseudo-task at a semantic level. We call such trajectories pseudo-demonstrations.\nData Generation. Firstly, to ensure generalisation across object geometries, we populate a simu-\nlated environment using a diverse range of objects from the ShapeNet dataset (Chang et al., 2015).\nWe then create pseudo-tasks by randomly sampling object-centric waypoints near or on the objects,\nthat the robot needs to reach in sequence. Finally, by virtually moving the robot gripper between\nthem and occasionally mimicking rigid grasps by attaching objects to the gripper, we create pseudo-\ndemonstrations – trajectories that resemble various manipulation tasks. Furthermore, randomising\nthe poses of the objects and the gripper, allows us to create many pseudo-demonstrations performing\nthe same pseudo-task, resulting in the data that we use to train our In-Context model.\nPseudo-Task 2\nPseudo-Task 1\nFigure 4: Examples of the simulated trajectories - 3 pseudo-demonstrations for 2 pseudo-tasks.\nIn practice, to facilitate more efficient learning of common skills, we bias sampling towards way-\npoints resembling tasks like grasping or pick-and-place. Note that as the environment dynamics and\ntask specifications, such as feasible grasps, are defined as context at inference, we do not need to\nensure that these trajectories are dynamically or even kinematically feasible. In theory, with enough\nrandomisation, the convex hull of the generated trajectories would encapsulate all the possible test-\ntime tasks. More information about the data generation process can be found in Appendix D.\nData Usage. During training, we sample N pseudo-demonstrations for a given pseudo-task, using\nN −1 to define the context while the model learns to predict actions for the Nth. Although pseudo-\ndemonstrations are the primary training data, our approach can integrate additional data sources in\nthe same format, allowing the model to adapt to specific settings or handle noisier observations.\n4\nEXPERIMENTS\nWe conducted experiments in two distinct settings: 1) simulation with RLBench (James et al., 2020)\nand ground truth segmentations, and 2) real-world everyday tasks. Our experiments study perfor-\nmance relative to baselines, to understand the effect of different design choices, to reveal the scaling\ntrends, and to showcase applicability in cross-embodiment and modality transfer. Videos are avail-\nable on our anonymous webpage at https://www.robot-learning.uk/instant-policy.\nExperimental Setup. Here, we describe parameters used across all our experiments unless explic-\nitly stated otherwise. We use a single model to perform various manipulation tasks by providing N=2\ndemos, which we express as L=10 waypoints as context and predict T=8 future actions. We train\nthis model for 2.5M optimisation steps using pseudo-demonstrations that are being continuously\ngenerated. When we discuss integrating additional training data beyond pseudo-demonstrations, we\nrefer to models fine-tuned for an additional 100K optimisation steps using a 50/50 mix of pseudo-\ndemonstrations and new data. For more information, please refer to Appendix E.\nBaselines. We compare Instant Policy to 3 baselines which also enable In-Context Imitation Learn-\ning, namely: BC-Z (Jang et al., 2022), Vid2Robot (Jain et al., 2024), and a GPT2-style model (Rad-\nford et al., 2019). BC-Z combines latent embedding of the demonstrations with the current observa-\ntion and uses an MLP-based model to predict robot actions, Vid2Robot utilises a Perceiver Resam-\npler (Jaegle et al., 2021) and cross-attention to integrate information from the context and current\nobservation, and GPT2 uses causal self-attention to predict the next tokens in the sequence, which in\n6\n\nour case are robot actions. For a fair comparison, we implemented all baselines by adapting them to\nwork with point cloud observation using the same pre-trained encoder, and all have roughly the same\nnumber of trainable parameters. Additionally, all components that rely on language-annotated data,\nsuch as auxiliary losses, were removed because our generated pseudo-demonstrations do not have\nsuch information. To highlight this, we add an asterisk to these methods when discussing results.\n4.1\nSIMULATED EXPERIMENTS\nThe aim of our first set of experiments is two-fold: 1) to evaluate the effectiveness of Instant Pol-\nicy in performing various manipulation tasks by comparing it to state-of-the-art baselines, and 2) to\ninvestigate the role the training data plays in generalising to unseen scenarios. We use a standard\nRLBench setup using the Franka Emika Panda and test Instant Policy (IP) and the baselines on 24\ntasks, 100 rollouts each, randomising the poses of the objects in the environment each time. Addi-\ntionally, we test models trained using only pseudo-demonstrations (PD only) and a combination of\npseudo-demonstrations and 20 demonstrations for each of 12 out of the 24 RLBench tasks (PD++).\nResults & Discussion. The first notable observation from the results, presented in Table 1, is that all\nmethods achieve non-zero success rates when only pseudo-demonstrations are used and can perform\nwell on at least the simpler tasks. This indicates that these pseudo-demonstrations are a powerful\nsource of limitless data for In-Context Imitation Learning. Our second observation is that incorpo-\nrating additional demonstrations from the same domain can greatly boost the performance, helping\nwith generalisation to unseen tasks and novel object poses. Our third observation is that Instant\nPolicy achieves significantly higher success rates than the baselines, showing the importance of our\ngraph representation and its ability to interpret the context. We further demonstrate this by visual-\nising attention weights (Figure 5), which reveal the model’s ability to understand the task’s current\nstage and identify the relevant information in the context. We discuss failure cases in Appendix G.\nTasks\nInstant Policy\nBC-Z*\nVid2Robot*\nGPT2*\nTasks\nInstant Policy\nBC-Z*\nVid2Robot*\nGPT2*\nOpen box\n0.94 / 0.99\n0.22 / 0.98\n0.30 / 0.97\n0.25 / 0.95\nSlide buzzer\n0.35 / 0.94\n0.04 / 0.26\n0.05 / 0.19\n0.00 / 0.00\nClose jar\n0.58 / 0.93\n0.00 / 0.06\n0.00 / 0.11\n0.00 / 0.22\nPlate out\n0.81 / 0.97\n0.26 / 0.55\n0.11 / 0.52\n0.31 / 0.40\nToilet seat down\n0.85 / 0.93\n0.40 / 0.88\n0.54 / 0.85\n0.38 / 0.83\nClose laptop\n0.91 / 0.95\n0.64 / 0.65\n0.45 / 0.57\n0.53 / 0.72\nClose microwave\n1.00 / 1.00\n0.55 / 0.60\n0.72 / 0.87\n0.76 / 1.00\nClose box\n0.77 / 0.99\n0.81 / 1.00\n0.89 / 0.88\n0.42 / 0.41\nPhone on base\n0.98 / 1.00\n0.51 / 0.50\n0.48 / 0.51\n0.28 / 0.55\nOpen jar\n0.52 / 0.78\n0.12 / 0.28\n0.15 / 0.30\n0.22 / 0.51\nLift lid\n1.00 / 1.00\n0.82 / 0.82\n0.90 / 0.91\n0.88 / 0.94\nToilet seat up\n0.94 / 1.00\n0.62 / 0.63\n0.58 / 0.64\n0.31 / 0.34\nTake umbrella out\n0.88 / 0.91\n0.42 / 0.64\n0.90 / 0.90\n0.75 / 0.89\nMeat off grill\n0.77 / 0.9\n0.75 / 0.64\n0.76 / 0.33\n0.80 / 0.30\nSlide block\n0.75 / 1.00\n0.10 / 0.14\n0.12 / 0.16\n0.08 / 0.16\nOpen microwave\n0.23 / 0.56\n0.00 / 0.13\n0.00 / 0.02\n0.00 / 0.00\nPush button\n0.60 / 1.00\n0.75 / 0.81\n0.85 / 0.88\n0.80 / 0.91\nPaper roll off\n0.70 / 0.95\n0.32 / 0.53\n0.29 / 0.55\n0.26 / 0.48\nBasketball in hoop\n0.66 / 0.97\n0.02 / 0.09\n0.00 / 0.06\n0.03 / 0.07\nPut rubish in bin\n0.97 / 0.99\n0.11 / 0.11\n0.12 / 0.14\n0.18 / 0.17\nMeat on grill\n0.78 / 1.00\n0.64 / 0.88\n0.51 / 0.77\n0.53 / 0.81\nPut umbrella\n0.31 / 0.37\n0.35 / 0.34\n0.41 / 0.28\n0.28 / 0.39\nFlip switch\n0.40 / 0.94\n0.15 / 0.63\n0.05 / 0.16\n0.11 / 0.42\nLamp on\n0.42 / 0.41\n0.00 / 0.00\n0.00 / 0.00\n0.00 / 0.00\nAverage (PD++) Seen\n0.97\n0.59\n0.60\n0.65\nAverage (PD++) Unseen\n0.82\n0.43\n0.37\n0.31\nAverage (PD only) All\n0.71\n0.36\n0.38\n0.34\nTable 1: Success rates for Instant Policy and baselines on 24 tasks. 100 rollouts for each (trained\nusing only pseudo-demonstrations / with additional demos from the 12 RLBench tasks on the left).\nAttention Weight\n1.0\n0.0\nStart\nGrasping point\nPlacing point\nTask Snapshots\nt=20\nt=50\nDemonstration\nFigure 5: Attention weights visualised on sub-graph edges at two different timesteps in the phone-\non-base task, showing the model’s ability to track task progress and aggregate relevant information.\n4.2\nINSTANT POLICY DESIGN CHOICES & SCALING TRENDS\nOur next set of experiments investigates the impact of various hyperparameters on the performance\nof our method, focusing on design choices requiring model re-training, inference parameters that\n7\n\nalter model behaviour at test time, and scaling trends as model capacity and training time increase.\nFor the design choices and inference parameters, we calculate the average change in success rate on\n24 unseen RLBench tasks, with respect to the base model used in the previous set of experiments,\nwhile for the scaling trends, we report validation loss on a hold-out set of pseudo-demonstrations to\nsee how well it can capture the underlying data distribution.\nDesign Choices\nInference Parameters\nAction\nMode\n∆%\nDiffusion\nMode\n∆%\nPrediction\nHorizon (T)\n∆%\n# Diffusion\nSteps (K)\n∆%\nDemo\nLength (L)\n∆%\n# Demos\n(N)\n∆%\n∆p\n-15\nFlow\n0\n1\n-52\n1\n-16\n1\n-71\n1\n-12\n(∆pt, ∆pr)\n0\nSample\n-6\n4\n-13\n2\n-2\n5\n-26\n2\n0\n(∆t, ∆q)\n-37\nNoise\n-7\n8\n0\n4\n0\n10\n0\n3\n2\n(∆t, ∆θ)\n-21\nNo Diff\n-29\n16\n-4\n8\n0\n15\n1\n4\n-1\nTable 2: Performance change of ablation variants when compared to the base model.\nDesign Choices. We now examine the following: action mode, diffusion mode, and the prediction\nhorizon. For action modes, we compare our proposed parameterisation, which decouples translation\nand rotation, against an approach without such decoupling, and more conventional approaches like\ncombining translation with quaternion or angle-axis representations. For diffusion mode, we evalu-\nate predicting flow versus added noise, direct sample, and omitting diffusion, regressing the actions\ndirectly. Lastly, we assess the impact of predicting different numbers of actions. The results, shown\nin Table 2 (left), show that these choices greatly influence performance. Decoupling translation and\nrotation in Cartesian space allows for precise low-level action learning. The diffusion process is\nvital for capturing complex action distributions, with predicting flow showing the best results. Fi-\nnally, predicting multiple actions is helpful, but this also increases computational complexity. For a\ndetailed discussion of other design choices, including unsuccessful ones, please refer to Appendix H.\nInference Parameters. Using a diffusion with a flexible representation that handles arbitrary con-\ntext lengths allows us to adjust model performance at inference. We investigate the impact of the\nnumber of diffusion steps, the demonstration length, and the number of demonstrations in the con-\ntext, as shown in Table 2 (right). Results show that even with just two denoising steps, good perfor-\nmance can be achieved. Demonstration length is critical; it must be dense enough to convey how the\ntask should be solved, as this information is not encoded in the model weights. This is evident when\nonly the final goal is provided (demonstration length = 1), leading to poor performance. However,\nextending it beyond a certain point shows minimal improvement, as the RLBench tasks can often be\ndescribed by just a few waypoints. For more complex tasks, dense demonstrations would be crucial.\nFinally, performance improves with multiple demonstrations, though using more than two seems to\nbe unnecessary. This is because two demonstrations are sufficient to disambiguate the task when\ngeneralising only over object poses. However, as we will show in other experiments, this does not\nhold when the test objects differ in geometry from those in the demonstrations.\n100K\n200K\n400K\n800K\n1.6M\n2.5M\nOptimisation Steps\n0.005\n0.01\n0.02\n0.04\n6 × 10\n3\n3 × 10\n2\nValidation Loss\n69M\n117M\n178M\nFigure 6: Validation loss curves for three different\nmodel sizes.\nScaling Trends.\nThe ability to continuously\ngenerate training data in simulation allows our\nmodel’s performance to be limited only by\navailable compute (training time) and model ca-\npacity (number of trainable parameters). To as-\nsess how these factors influence our approach,\nwe trained three model variants with different\nnumbers of parameters and evaluated them af-\nter varying numbers of optimisation steps (Fig-\nure 6).\nThe results show that the model’s\nability to capture the data distribution (as re-\nflected by decreasing validation loss) scales\nwell with both training time and model com-\nplexity. This offers some promise that scaling\ncompute alone could enable the development\nof high-performing models for robot manipu-\nlation. Qualitatively, we observed a similar performance trend on unseen RLBench tasks. With\nincreased training, we see an increase in performance. However, it plateaus eventually. Similarly,\nby increasing the model’s capacity from 69M to 117M, the success rate reached before plateauing\n8\n\nincreases significantly. However, further increasing the number of trainable parameters to 178M\nresults in only minor, insignificant improvements in performance. This suggests the need for more\ndiverse and representative data. Such data could come from available robotics datasets or the gener-\nation of pseudo-demonstrations that more closely resemble real tasks.\n4.3\nREAL-WORLD EXPERIMENTS\nIn real-world experiments, we evaluate our method’s ability to learn everyday tasks and generalise\nto novel objects, unseen in both the training data and the context. We use a Sawyer robot with a\nRobotiq 2F-85 gripper and two external RealSense D415 depth cameras. We obtain segmentation by\nseeding the XMem++ (Bekuzarov et al., 2023) object tracker with initial results from SAM (Kirillov\net al., 2023), and we provide demonstrations using kinesthetic teaching. To help the model handle\nimperfect segmentations and noisy point clouds more effectively, we further co-fine-tuned the model\nused in our previous experiments using 5 demos from 5 tasks not included in the evaluation.\nInsert Paper Roll\nOpen Airfryer\nFlip Bottle\nStack Bowls\nKnock over Creeper\nKettle on Stove\nClose Coffee Machine\nHang Cable\nOpen Box\nTurn Tap Right\nTurn Tap Left\nTake Rose Out\nPush Cans Together\nPick up Kettle\nClose Box\nOpen Cash Register\nFigure 7: The 16 tasks used in our real-world evaluation.\nReal-World Tasks. To evaluate our model’s ability to tackle various tasks in the real world, we\ntested it and the baselines on 16 everyday tasks (Figure 7). We evaluated all methods using 10\nrollouts, randomising the poses of the objects in the environment each time. From the results (Ta-\nble 3), we can see that Instant Policy is able to complete various everyday tasks from just a couple\nof demonstrations with a high success rate, outperforming the baselines by a large margin.\nInsert\nPaper Roll\nOpen\nAirfryer\nFlip\nBottle\nStack\nBowls\nKnock over\nCreeper\nKettle on\nStove\nClose\nCoffee Machine\nHang\nCable\nInstant Policy\n9 / 10\n9 / 10\n7 / 10\n10 / 10\n8 / 10\n10 / 10\n10 / 10\n7 / 10\nBC-Z*\n1 / 10\n5 / 10\n0 / 10\n2 / 10\n5 / 10\n1 / 10\n1 / 10\n0 / 10\nVid2Robot*\n3 / 10\n6 / 10\n0 / 10\n1 / 10\n7 / 10\n3 / 10\n4 / 10\n1 / 10\nGPT2*\n1 / 10\n6 / 10\n0 / 10\n4 / 10\n5 / 10\n5 / 10\n5 / 10\n1 / 10\nOpen\nBox\nTurn Tap\nRight\nTurn Tap\nLeft\nTake\nRose Out\nPush Cans\nTogether\nPick up\nKettle\nClose\nBox\nOpen\nRegister\nAverage, %\nInstant Policy\n8 / 10\n10 / 10\n10 / 10\n9 / 10\n5 / 10\n10 / 10\n10 / 10\n10 / 10\n88.75\nBC-Z*\n8 / 10\n2 / 10\n3 / 10\n0 / 10\n2 / 10\n10 / 10\n7 / 10\n8 / 10\n34.38\nVid2Robot*\n9 / 10\n4 / 10\n3 / 10\n0 / 10\n1 / 10\n10 / 10\n7 / 10\n6 / 10\n40.63\nGPT2*\n0 / 10\n5 / 10\n5 / 10\n0 / 10\n0 / 10\n10 / 10\n5 / 10\n7 / 10\n36.88\nTable 3: Real-world success rates for Instant Policy and the baselines, with 10 rollouts each.\nDemo\nDemo\nDemo\nDemo\nTest\nTest\nTest\nTest\n1\n2\n3\n4\n1\n1\n1\n2\n2\n2\n3\n3\n3\n4\n4\n4\nPlace a mug on a plate\nUnplug Charger\nOpen Box\nFold in Half\nFigure 8:\nObjects used in the generalisation ex-\nperiment (# indicate demo order in the context).\nGeneralisation to Novel Objects. While all of\nour previous experiments focused on evaluat-\ning our method’s performance on the same ob-\njects used in the demonstrations, here we aim\nto test its ability to generalise to novel object\ngeometries at test time. We do so by provid-\ning demonstrations (i.e., defining the context)\nwith different sets of objects from the same se-\nmantic category, and testing on a different ob-\nject from that same category. For the evalua-\ntion, we use four different tasks (Figure 8), each\nwith six sets of objects (four for the demon-\nstrations/context and two for evaluation). We\nevaluate our method with a different number\n9\n\nof demonstrations in the context, randomising the poses of the test objects during each roll-\nout (5 rollouts for each unseen object set). The results, presented in Table 4, show that, with\nan increasing number of demonstrations across different objects, the performance on completely\nnovel object geometries increases. This indicates that Instant Policy is capable of selectively ag-\ngregating and interpolating the information present in the context to disambiguate the task and\nthe parts of the objects that are relevant to it.\nIt is important to note that this is an emer-\ngent behaviour, as we never trained our model with objects from different geometries across the\ncontext, and is enabled by the graph representation and structured cross-attention mechanism.\n4.3.1\nDOWNSTREAM APPLICATIONS\nN\nOpen\nBox\nFold\nin Half\nMug\non Plate\nUnplug\nCharger\nAverage,\n%\n1\n2 / 10\n7 / 10\n7 / 10\n0 / 10\n40\n2\n5 / 10\n8 / 10\n8 / 10\n0 / 10\n52.5\n3\n10 / 10\n10 / 10\n9 / 10\n5 / 10\n85\n4\n10 / 10\n10 / 10\n9 / 10\n7 / 10\n90\nTable 4: Success rates of Instant Policy with a\ndifferent number of demonstrations (N), enabling\ngeneralisation to novel object geometries.\nCross-embodiment transfer. Since our model\nuses segmented point clouds and defines the\nrobot state by the end-effector pose and grip-\nper state, different embodiments can be used to\ndefine the context and roll out the policy, pro-\nvided the mapping between them is known. We\ndemonstrate this by using human-hand demon-\nstrations with a handcrafted mapping to the\ngripper state, allowing us to transfer the policy\ndirectly to the robot. In qualitative experiments,\nwe achieve similar success rates on simple tasks, like pick-and-place, compared to kinesthetic teach-\ning. However, for more complex tasks, this approach is limited by the handcrafted mapping. See\nour webpage for video examples and refer to Appendix I for more information about this mapping.\nModality change. While obtaining a policy immediately after demonstrations is a powerful and\nefficient tool, it still requires human effort in providing those demonstrations. We can circumvent\nthis by exploiting the bottleneck of our trained model, which holds the information about the context\nand the current observation needed to predict actions. This information is aggregated in the gripper\nnodes of the current observations. If we approximate this bottleneck representation using different\nmodalities, such as language, we can bypass using demonstrations as context altogether. This can\nbe achieved with a smaller, language-annotated dataset and a contrastive objective. Using language-\nannotated trajectories from RLBench and rollout data from previous experiments, we qualitatively\ndemonstrate zero-shot task completion based solely on language commands. For more details, see\nAppendix J, and for videos, visit our webpage at https://www.robot-learning.uk/instant-policy.\n5\nDISCUSSION\nLimitations. While Instant Policy demonstrates strong performance, it has several limitations. First,\nlike many similar approaches, we assume the availability of segmented point clouds for sufficient\nobservability. Second, point cloud observations lack colour or other semantically rich information.\nThird, our method focuses on relatively short-horizon tasks where the Markovian assumption holds.\nFourth, Instant Policy is sensitive to the quality and downsampling of demonstrations at inference.\nFifth, it does not address collision avoidance or provide end-to-end control of the full configuration\nspace of the robot arm. Finally, it lacks the precision needed for tasks with extremely low tolerances\nor rich contact dynamics. However, we believe many of these limitations can be addressed primarily\nthrough improvements in generation of the pseudo-demonstrations, such as accounting for colli-\nsions, incorporating long-horizon tasks, and by improving the graph representation with additional\nfeatures from vision models, force information, or past observations.\nConclusions. In this work, we introduced Instant Policy, a novel framework for In-Context Imitation\nLearning that enables immediate robotic skill acquisition following one or two test-time demonstra-\ntions. This is a compelling alternative paradigm to today’s widespread behavioural cloning methods,\nwhich require hundreds or thousands of demonstrations. We showed that our novel graph structure\nenables data from demonstrations, current observations, and future actions, to be propagated ef-\nfectively via a novel graph diffusion process. Importantly, Instant Policy can be trained with only\npseudo-demonstrations generated in simulation, providing a virtually unlimited data source that is\nconstrained only by available computational resources. Experiments showed strong performance\nrelative to baselines, the ability to learn everyday real-world manipulation tasks, generalisation to\nnovel object geometries, and encouraging potential for further downstream applications.\n10\n\nREFERENCES\nK Somani Arun, Thomas S Huang, and Steven D Blostein. Least-squares fitting of two 3-d point\nsets. IEEE Transactions on pattern analysis and machine intelligence, (5):698–700, 1987.\nJimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\nMaksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, and Hao Li. Xmem++: Production-level\nvideo segmentation from few annotated frames. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 635–644, 2023.\nKevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and\nSergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models.\narXiv preprint arXiv:2310.10639, 2023.\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. arXiv preprint arXiv:1512.03012, 2015.\nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shu-\nran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint\narXiv:2303.04137, 2023.\nNorman Di Palo and Edward Johns. Keypoint action tokens enable in-context imitation learning in\nrobotics. arXiv preprint arXiv:2403.19578, 2024.\nYan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya\nSutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in\nneural information processing systems, 30, 2017.\nLetian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch,\nFangchen Liu, Hui Li, and Ken Goldberg. In-context imitation learning via next-token prediction.\narXiv preprint arXiv:2408.15980, 2024.\nDan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nWeihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc:\nA large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David\nDing, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A\ngeneral architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.\nVidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R\nSanketi, Pierre Sermanet, Stefan Welker, Christine Chan, et al. Vid2robot: End-to-end video-\nconditioned policy learning with cross-attention transformers. arXiv preprint arXiv:2403.12943,\n2024.\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot\nlearning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019–\n3026, 2020.\nEric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine,\nand Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Confer-\nence on Robot Learning, pp. 991–1002. PMLR, 2022.\nKanchan Jha, Sriparna Saha, and Hiteshi Singh. Prediction of protein–protein interaction using\ngraph neural networks. Scientific Reports, 12(1):8360, 2022.\n11\n\nIvan Kapelyukh and Edward Johns. My house, my rules: Learning tidying preferences with graph\nneural networks. In Conference on robot learning, pp. 740–749. PMLR, 2022.\nIvan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale diffusion\nmodels to robotics. IEEE Robotics and Automation Letters, 8(7):3956–3963, 2023.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pp. 4015–4026, 2023.\nJames J Kuffner and Steven M LaValle. Rrt-connect: An efficient approach to single-query path\nplanning. In Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference\non Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065), volume 2, pp. 995–\n1001. IEEE, 2000.\nI Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\nCamillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays,\nFan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: A framework\nfor building perception pipelines. arXiv preprint arXiv:1906.08172, 2019.\nAjay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan,\nYuke Zhu, and Dieter Fox. Mimicgen: A data generation system for scalable robot learning using\nhuman demonstrations. arXiv preprint arXiv:2310.17596, 2023.\nMatthew Matl. Pyrender. https://github.com/mmatl/pyrender, 2019.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Oc-\ncupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 4460–4470, 2019.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99–106, 2021.\nGeorgios Papagiannis, Norman Di Palo, Pietro Vitiello, and Edward Johns. R+ x: Retrieval and\nexecution from everyday human videos. arXiv preprint arXiv:2407.12957, 2024.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems 32, pp.\n8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical fea-\nture learning on point sets in a metric space. Advances in neural information processing systems,\n30, 2017.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning, pp. 8821–8831. Pmlr, 2021.\nN Reimers.\nSentence-bert: Sentence embeddings using siamese bert-networks.\narXiv preprint\narXiv:1908.10084, 2019.\nCarmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, and Pieter Abbeel. Body trans-\nformer: Leveraging robot embodiment for policy learning. arXiv preprint arXiv:2408.06316,\n2024.\n12\n\nChuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. Heterogeneous information network\nembedding for recommendation. IEEE transactions on knowledge and data engineering, 31(2):\n357–370, 2018.\nYunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label\nprediction: Unified message passing model for semi-supervised classification. arXiv preprint\narXiv:2009.03509, 2020.\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-actor: A multi-task transformer for\nrobotic manipulation. In Conference on Robot Learning, pp. 785–799. PMLR, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nJulen Urain, Niklas Funk, Jan Peters, and Georgia Chalvatzaki. Se (3)-diffusionfields: Learning\nsmooth cost functions for joint grasp and motion optimization through diffusion. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 5923–5930. IEEE, 2023.\nDavid Vogt, Simon Stepputtis, Steve Grehl, Bernhard Jung, and Heni Ben Amor. A system for\nlearning continuous human-robot interactions from human-human demonstrations. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 2882–2889. IEEE, 2017.\nVitalis Vosylius and Edward Johns. Few-shot in-context imitation learning via implicit graph align-\nment. arXiv preprint arXiv:2310.12238, 2023a.\nVitalis Vosylius and Edward Johns. Where to start? transferring simple skills to complex environ-\nments. In Conference on Robot Learning, pp. 471–481. PMLR, 2023b.\nVitalis Vosylius, Younggyo Seo, Jafar Uruc¸, and Stephen James. Render and diffuse: Aligning image\nand action spaces for diffusion-based behaviour cloning. arXiv preprint arXiv:2405.18196, 2024.\nTingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with\ngraph neural networks. In International conference on learning representations, 2018.\nTianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh,\nClayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imag-\nined experience. arXiv preprint arXiv:2302.11550, 2023.\nXinyu Zhang and Abdeslam Boularias. One-shot imitation learning with invariance matching for\nrobotic manipulation. arXiv preprint arXiv:2405.13178, 2024.\nTony Z Zhao, Jonathan Tompson, Danny Driess, Pete Florence, Seyed Kamyar Seyed Ghasemipour,\nChelsea Finn, and Ayzaan Wahid. Aloha unleashed: A simple recipe for robot dexterity. In 8th\nAnnual Conference on Robot Learning.\nAllan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, and Chelsea Finn. Nerf in the palm of\nyour hand: Corrective augmentation for robotics via novel-view synthesis. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17907–17917, 2023.\n13\n\nAPPENDIX\nA\nGEOMETRY ENCODER\nHere, we describe the local geometry encoder used to represent observations of the environment\nas a set of nodes. Formally, the local encoder encodes the dense point cloud into a set of feature\nvectors together with their associated positions as: {Fi, pi}M\ni=1 = ϕ(P). Here, each feature Fi\ndescribes the local geometry around the point pi. We ensure this by pre-training an occupancy\nnetwork (Mescheder et al., 2019), that consists of an encoder ϕe, which embeds local point clouds,\nand a decoder ψe which given this embedding and a query point is tasked to determine whether\nthe query lays on the surface of the object: ψe(ϕe(P), q) →[0, 1]. The high-level structure of our\noccupancy network can be seen in Figure 9. Note that each local embedding is used to reconstruct\nonly a part of the object, reducing the complexity of the problem and allowing it to generalise more\neasily.\nPointNet \nEncoder\nOccupancy \nNet Decoder\nFigure 9: High-level structure of the occupancy network.\nWe parameterise ϕe as a network composed of 2 Set Abstraction layers (Qi et al., 2017) enhanced\nwith Nerf-like sine/cosine embeddings (Mildenhall et al., 2021). It samples M centroids from the\ndense point cloud and embeds the local geometries around them into feature vectors of size 512.\nInstead of expressing positions of points relative to the sampled centroids pi as pj −pi ∈R3,\nwe express them as (sin(20π(pj −pi)), cos(20π(pj −pi)), ..., sin(29π(pj −pi)), cos(29π(pj −\npi))), enabling the model to capture high-frequency changes in the position of the dense points and\ncapture the local geometry more precisely. We parametrise ψe as an eight-layer MLP with residual\nconnections, that uses the same Nerf-like embeddings to represent the position of the query point.\nWe use objects from a diverse ShapeNet (Chang et al., 2015) dataset to generate the training data\nneeded to train the occupancy network. For training Instant Policy, we do not use the decoder and\nkeep the encoder frozen.\nB\nTRAINING\nTraining our diffusion model involves a forward and backward Markov chain diffusion process,\nwhich is outlined in Equations 1 and 2. Intuitively, we add noise to the ground truth robot actions\nand learn how to remove this noise in the graph space (see Figure 10).\nIn practice, training includes 4 main steps: 1) noise is added to the ground truth actions, 2) noisy ac-\ntions are used to construct our graph representation, 3) the network predicts how nodes representing\nrobot actions need to be adjusted to effectively remove the added noise, and 4) the prediction and\nground truth labels are used to calculate the loss function, and weights of the network are updated\naccordingly.\nTo add noise to the action expressed as (TEA ∈SE(3), ag ∈R, we first project TEA to se(3)\nusing a Logmap, normalise the resulting vectors, add the noise as described by Ho et al. (2020),\nunnormalise the result and extract the noisy end-effector transformation T k\nEA using Expmap. Such\na process can be understood as adding noise to a SE(3) transformation in its tangential space. We\n14\n\nFigure 10: High-level overview of the training process. (Left) A data point is sampled from the\ndataset. (Middle) Noise is added to the ground truth actions. (Right) Using demonstrations, current\nobservation and noisy actions, a graph representation is constructed, which is used to predict, how\nto remove the added noise in the graph space.\ncan do this because around actions (end-effector displacements) are sufficiently small. For bigger\ndisplacements, unnormalised noise should be projected onto the SE(3) manifold directly, as done\nby Vosylius & Johns (2023a) and Urain et al. (2023). Adding noise to real-valued gripper actions\ncan be done directly using the process described by Ho et al. (2020).\nC\nNETWORK ARCHITECTURE\nHere we describe the neural network used to learn the denoising process on graphs, enabling us\nto generate graphs G and implicitly model the conditional action probability. Our parametrised\nneural network takes the constructed graph representation as input and predicts the gradient field\nfor each gripper node representing the actions: εθ(Gk). These predictions are then used in the\ndiffusion process allowing to iteratively update the graph and ultimately extract desired low-level\nrobot actions. In practice, for computational efficiency and more controlled information propagation,\nwe are using three separate networks σ, ϕ and ψ, updating relevant parts of the graph in sequence\nas:\nεθ(Gk) = ψ(G(σ(Ga\nl ), ϕ(Gc(σ(Gt\nl ), {σ(G1:L\nl\n)}N\n1 )))\n(6)\nHere, σ operates on local subgraphs Gl and propagates initial information about the point cloud ob-\nservation to the gripper nodes, φ additionally propagates information through the demonstrated tra-\njectories and allows all the relevant information from the context to be gathered at the gripper nodes\nof the current subgraph. Finally, ψ propagates information to nodes in the graph representing the ac-\ntions. Using such a structured and controlled propagation of information through the graph, together\nwith the learnable attention mechanism described in Equation 3, allows the model to continuously\naggregate relevant information from the context and make accurate predictions about the actions.\nAdditionally, it also results in a clear and meaningful bottleneck in the network with all the relevant\ninformation from the context aggregated in a specific set of nodes (ϕ(Gc(σ(Gt\nl ), {σ(G1:L\nl\n)}N\n1 ))).\nThis bottleneck representation could be used for retrieval or as shown in our experiments, to switch\nmodalities, for example to language, via a smaller annotated dataset and a contrastive objective.\nEach of the three separate networks is a heterogeneous graph transformer (Equation 3) with 2 lay-\ners and a hidden dimension of size 1024 (16 heads, each with 64 dimensions). As we are using\nheterogeneous graphs, each node and edge type are processed with separate learnable weights and\naggregated via summation to produce all node-wise embeddings. This can be understood as a set of\ncross-attention mechanisms, each responsible for processing different parts of the graph representa-\ntion. We use layer normalisation layers (Ba, 2016) between every attention layer and add additional\n15\n\nresidual connections to ensure good propagation of gradients throughout the network. Finally, fea-\ntures of the nodes representing robot actions are processed with a 2-layer MLP equipped with GeLU\nactivations (Hendrycks & Gimpel, 2016) to produce the per-node denoising directions.\nD\nDATA GENERATION\nOur data generation process, firstly, includes populating a scene with objects with which the robot\nwill interact. We do so by sampling two objects from the ShapeNet dataset and placing them ran-\ndomly on a plane. Next, we define a pseudo-task by sampling a sequence of waypoints on or near\nthose objects. The number of these waypoints is also randomly selected to be between 2 and 6,\ninherently modelling various manipulation tasks. We assign one or more waypoints to change the\ngripper state, mimicking the rigid robotic grasp and release. We then sample a starting pose for the\ngripper, where we initialise a mesh of a Robotiq 2F-85 gripper. By moving the gripper between\nthe aforementioned waypoints and attaching or detaching the closest object to it when the gripper\nstate changes, we create a pseudo-demonstration. To further increase the diversity in pseudo-tasks,\nwe use different interpolation strategies between the waypoints (e.g. linear, cubic or interpolating\nwhile staying on a spherical manifold). We record gripper poses and segmented point cloud obser-\nvations using PyRender (Matl, 2019) and three simulated depth cameras. We ensure that the spacing\nbetween the subsequent spaces is constant and uniform (1cm and 3 degrees, same as used for the\nnormalisation of actions). Moving objects to different poses, choosing a different starting gripper\npose and repeating the process results in several pseudo-demonstrations for the same pseudo-task,\nwhich we use to train our In-Context model. As mentioned in Section 3.4, we do not need to ensure\nthat these generated trajectories are dynamically or even kinematically feasible, as the environment\ndynamics and task specifications, such as feasible grasp, are defined as context at inference.\nBias Sampling. To facilitate more efficient learning of common skills, we bias the sampling to\nfavour waypoints resembling common tasks such as grasping or pick-and-place. This does not\nrequire creating dynamically feasible trajectories but rather involves designing sampling strategies\nfor waypoints that loosely approximate these tasks. For instance, by selecting a specific part of an\nobject, moving the simulated gripper to that location, and closing the gripper, we can simulate a\ngrasping task, even if the grasp itself is not physically feasible. We design such sampling strategies\nfor common tasks, such as grasping, pick-and-place, opening or closing. Pseudo-demonstrations\nare generated using these strategies for half of the samples, while the rest use completely random\nwaypoints.\nData Augmentation. To facilitate the emergence of recovery behaviour of the learnt policies, we\nfurther augment the generated trajectories. Firstly, for 30% of the trajectories, we add local distur-\nbances associated with actions that would bring the robot back to the reference trajectory, similarly\nto how it is done by Zhou et al. (2023). Secondly, for 10% of the data points, we purposely change\nthe gripper’s open-close state. This, we found to be crucial, as, without it, the policy would never\ntry to re-grasp an object after initially closing the gripper.\nE\nIMPLEMENTATION DETAILS\nHere we discuss the implementation details of the Instant Policy, which we found to be important in\nmaking the method perform well.\nDemo Processing. Although our network can handle an arbitrary number of demonstrations of any\nlength, we downsample the demo trajectories to a fixed length (L = 10 in our experiments). First,\nwe record demonstrations at a rate of 25Hz and 10Hz in simulation and the real world, respectively.\nThe lower rate in the real world is caused by the simultaneous segmentation of objects of interest\nby Xmem++ (Bekuzarov et al., 2023). We then include the start and end of the trajectories and all\nthe waypoints where the open-close state of the gripper changed. We then include the waypoints\nin the trajectory, where the gripper sufficiently slowed down, indicating important trajectory stages\n(similar to Shridhar et al. (2023)). Finally, if the current number of the trajectory waypoint is less\nthan L, we add intermediate waypoints between the already extracted ones.\nData Augmentation. To achieve robust policies, we found that it is crucial to randomise current\nobservations and subsequent actions during training. The network can easily overfit to binary gripper\n16\n\nstates (if it is open, just keep it open, and if it is closed, just keep it closed). To tackle this, we, with\na 10% probability, flip the current gripper state used as an input to the model. This greatly increased\nthe robustness of the resulting policies. Additionally, during the pseudo-demonstration generation\nprocess, we added local perturbations to the pose of the gripper (adjusting point cloud observations\nand actions accordingly), further increasing robustness and enabling recovery behaviour.\nNormalisation. We normalise all the outputs of our mode to be [−1, 1], a step that we found\nto be crucial. To this end, we manually define the maximum end-effector displacement between\nsubsequent action predictions to be no more than 1cm in translation and 3 deg in rotation and clamp\nthe noisy actions to be within this range. Thus the flow prediction is capped to be at most twice the\nsize of this range. Knowing this, we normalise ∇ˆpt and ∇ˆpr to be between −1 and 1 independently,\nenabling efficient network training. For the gripper opening-closing actions, this can be done easily\nas they are expressed as binary states {0, 1}. We do not normalise the position of the point cloud\nobservations but rather rely on the sine/cosine embeddings, a strategy that we found to be sufficient.\nPoint Cloud Representation. We use segmented point cloud observations of objects of interest in\nthe environment as our observations. These segmented point clouds do not include the points on the\nrobot or other static objects such as the table or distractors. In practice, We downsample the point\nclouds to contain 2048 points and express them in the end-effector frame as TEW × P to achieve\nstronger spatial generalisation capabilities. These point clouds are then processed with a geometry\nencoder, described in Section A, producing M = 16 nodes used to construct our devised graph\nrepresentation.\nAction Denoising. When updating TEA during our denoising process, we use calculated Tk,k−1 (as\ndescribed in Section 3.3) and calculate the transformation representing end-effector actions during\nthe denoising process as T k−1\nEA = Tk,k−1 × T k\nEA. These actions are then used to construct a graph\nrepresentation that is used in the next denoising step. In practice, because we express point cloud\nobservations in the end-effector frame, we apply the inverse of these actions to the M points repre-\nsenting the scene and construct local graphs of actions as Ga\nl (T −1\nEA × P t, T t\nW E, ag). As there are no\nabsolute positions in the graph, this is equivalent to applying the actions to the gripper pose T t\nW E,\nbut it allows us to recompute the geometry embeddings of the point clouds at their new pose, better\nmatching the ones from the demonstrations.\nTraining. We trained our model using AdamW (Loshchilov, 2017) optimiser with a 1e−5 learn-\ning rate for 2.5M optimisation steps (approx. 5 days on a single NVIDIA GeForce RTX 3080-ti)\nfollowed by a 50K steps learning rate cool-down period. For efficient training, we used float16\nprecision and compiled our models using torch compile capabilities (Paszke et al., 2019). Training\ndata in the form of pseudo-demonstrations were continuously generated during training, replacing\nthe older sample to ensure that the model did not see the same data point several times, preventing\noverfitting.\nF\nSIMULATION EXPERIMENTAL SETUP\nHere, we describe the 2 changes we made to a standard RLBench setup (James et al., 2020) when\nconducting our experiments. 1) We generated all the demonstrations (for the context and for those\nused during training as described in Section 4) using only Cartesian Space planning - we disregarded\nall demonstrations that were generated using an RRT-based motion planner (Kuffner & LaValle,\n2000). We did so to ensure that the demonstrations did not have arbitrary motions that would not\nbe captured by our observations of segmented point clouds and end-effector poses. 2) We restricted\nthe orientations of the objects in the environment to be within [−π/3, π/3]. We did so to match\nthe distribution of object poses to the one present in our generated pseudo-demonstrations. It also\nensured that most tasks could be solved without complex motions requiring motion planners.\nG\nFAILURE CASES\nHere we discuss the observed failure modes of Instant Policy during our experiments. Given dif-\nferent setups and assumptions, we do so for each of our experiments independently. However, the\ndiscussed failure modes are shared across the experiments.\n17\n\nSimulated Experiments. During our simulated experiments using RLBench (James et al., 2020),\nwe observed several common failure modes of Instant Policy. First of all, tasks such as Open Mi-\ncrowave or Put Umbrella into a Rack require extremely high precision in action predictions, oth-\nerwise, the inaccurate dynamics of the simulator will prevent the task from being completed. As\nsuch, sometimes the handle of the microwave would slip from the gripper, or the umbrella would\nfly off when in contact with the robot. Second, tasks such as Flipping a Switch or Pushing a Button\nterminate immediately after the task condition is met. As we predict actions of not doing anything\nat the end of the trajectory, this resulted in the policy stopping before the task is fully completed\nat a state virtually the same as the desired one. Moreover, our generated pseudo-demonstrations do\nnot include any collision avoidance, which has proven to be a problem for tasks such as Turning\nthe Lamp On, where the robot occasionally collides with the lamp by moving in a straight line to-\nwards the button. Finally, other failure modes usually included policy stalling at a certain point or\noscillating between two configurations. We hypothesise that such behaviour is caused by conflicting\ninformation in the provided demonstrations and violating the Markovian assumption. In the future,\nthis could be addressed by incorporating past observations into the graph representation.\nReal-World Tasks. By far, the most common failure mode in our real-world experiments was the\nsegmentation failure caused by several occlusions. Additionally, imperfect segmentation sometimes\nincluded parts of the robot or the table, causing the policy to perform irrelevant actions. This also\nsometimes degraded the quality of the demonstrations by including irrelevant points (and thus nodes\nin the constructed graph). Moreover, we observed that the overall quality of the demonstrations, in\nterms of smoothness and clearly directed motions, had a major impact on the performance of Instant\nPolicy. If recorded demonstrations included inconsistent and arbitrary motions, information in the\ncontext was conflicting, resulting in the policy stalling or oscillating. Finally, other observed failure\ncases mainly involved policy not completing the task due to the lack of precision.\nGeneralisation to Novel Geometries. When evaluating Instant Policy using objects unseen neither\nduring training nor demonstration at inference, policy sometimes just mimicked the motion observed\nduring the demonstrations without achieving the desired outcome. With an increasing number of\ndiversity demos in the context, such behaviour was minimised. However, some tasks (e.g. placing\na mug on a plate) were completed mainly due to the high tolerance of the task rather than true\ngeneralisation capabilities.\nCross-Embodiment Transfer. The main failure cases during our cross-embodiment transfer exper-\niments were caused by incorrect mapping of hand poses to end-effector poses and an insufficient\nfield of view in our observations. This caused the robot to occasionally miss the precise grasp-\ning locations, closing the gripper at stages where it was not intended, and, in general, resulted in\ndemonstrations of poorer quality.\nModality Transfer.\nReplacing demonstrations with language descriptions of the task yielded\npromising results in our qualitative experiments. However, the observed behaviour was sometimes\nmismatched with the object geometries in the environment. For instance, the policy would execute\nappropriate motions (e.g., pushing or closing) but at incorrect locations relative to the objects. This\nissue likely stems from object features containing only geometric information without any seman-\ntic context. Incorporating additional features from vision foundation models into the point cloud\nobservations and expanding the language-annotated dataset could help address this limitation.\nH\nTHINGS THAT DID NOT WORK\nHere we discuss various design choices we considered before settling for the approach, described in\nSection 3.\nFully-Connected Graph. Initially, we experimented with a fully connected graph, which effectively\nacts as a transformer with dense self-attention. While the attention mechanism should, in theory,\nlearn the structure relevant to the task, this approach failed to produce good results, even for simple\ntasks.\nOne Big Network. Instead of using three separate networks in sequence (as described in Section C),\nwe experimented with a single larger network, which led to a significant drop in performance. We\nhypothesize that this is because, early on, the nodes lack sufficient information to reason about ac-\n18\n\ntions, causing much of the computation to be wasted and potentially resulting in conflicting learning\nsignals.\nMore Gripper Nodes. We express the robot state as a set of six nodes in the graph. In theory,\nwe can use an arbitrary number (> 3) of such nodes, allowing more flexible aggregation of relevant\ninformation. We experimented with different numbers of such nodes and observed minimal changed\nin performance, while the computational requirements increased significantly.\nNo Pre-trained Geometry Encoder. During the training of Instant Policy, we keep the geometry\nencoder frozen. We experimented with training this model from scratch end-to-end, as well as fine-\ntuning it. Training from scratch did not work at all, while fine-tuning resulted in significantly worse\nperformance. We also experimented with larger sizes of the encoder and saw no improvement,\nindicating that the geometry information was already well represented.\nHomogeneous Graph. Instead of using a heterogenous graph transformer, which processes differ-\nent types of nodes and edges using separate sets of learnable weights, we tried using a homogeneous\nvariant with distinct embeddings added to the nodes and edges. This approach resulted in signifi-\ncantly worse performance, given the same number of trainable parameters. This indicates that by\nnot sharing the same weights, different parts of the network can better focus on aggregating and\ninterpreting relevant information, resulting in more efficient learning.\nPredicting Waypoints. Initially, we tried predicting spare waypoints instead of low-level actions,\ne.g. velocities, that progress the execution of a task. We found, that because of these waypoints\nrepresent larger end-effector displacements, predicting them with high precision was challenging.\nIntuitively, this is the result of the increased action space that, when normalised, needs to be repre-\nsented in an interval [−1, 1].\nLarger Learning Rates. For our experiments, we used a relatively small learning rate of 1e−5.\nTo speed up the training process, we tried increasing it. However, with increased learning rate we\nfound the training process to be unstable, resulting in large gradients and increasing training loss.\nWe also tried using several different optimisers, using AdamW (Loshchilov, 2017) resulting in the\nbest performance.\nI\nCROSS-EMBODIMENT TRANSFER\nAs described in Section 4.3.1, our approach allows us to provide demonstrations using one embod-\niment (e.g. using human hands) and instantly deploy a policy on a robot, given a mapping between\ndifferent embodiments is known. This is because our observations are composed of segmented point\nclouds that do not include points on the robot and its end-effector pose. Thus, by mapping the pose\nof a human hand to the robot’s end-effector pose, we can effectively obtain the same observations. In\nour experiments, we achieve this mapping using a hand keypoint detector from Mediapipe (Lugaresi\net al., 2019) and manually designing a mapping between these key points and the corresponding\nrobot’s end-effector pose. We model the position of the end-effector to be represented by the mid-\nway position between the index finger and the thumb and estimate the orientation using an additional\npoint on the palm of the hand. In this way, we effectively overparametrise the SE(3) pose of the\nhand, modelled as a parallel gripper, using a set of positions. This allows us to complete simple\ntasks, such as grasping or pick-and-place. However, for more precise tasks, such a crude mapping\ncan be insufficient. It could be addressed by using more elaborate mappings between human hands\nand robot grippers, for example, as done by Papagiannis et al. (2024).\nJ\nMODALITY TRANSFER\nUsing our graph representation together with network architecture, discussed in Section C, results\nin a clear information bottleneck with all the relevant information from the context aggregated in\na specific set of nodes (ϕ(Gc(σ(Gt\nl ), {σ(G1:L\nl\n)}N\n1 ))). Information present in the nodes of the graph\nrepresenting the current information holds all the necessary information to compute precise robot\naction appropriate for the current situation, and a trained ψ(.) has the capacity to do it. We exploit\nthis bottleneck and learn to approximate it using the current observation and a language description\nof a task and utilise a frozen ψ(.) to compute the desired robot actions in the same way as done\nwhen the context includes demonstrations. We learn this approximation using the local graph rep-\n19\n\nresentation of the current observation Gt\nl and a language embedding of the task description flang,\nproduced by Sentence-BERT (Reimers, 2019). We use a graph transformer architecture, similar\nto the one used to learn σ, and incorporate flang as an additional type of node in the graph. We\ntrain this network using a language-annotated dataset comprising demonstrations from RLBench\nand rollouts from our experiments, along with a contrastive objective. At inference, we provide a\nlanguage description of a task and, based on the current observation, compute the embeddings of\nthe aforementioned bottleneck. We then use it to compute robot actions that are executed closed-\nlooped, allowing for zero-shot generalisation to tasks described by language. Although showing\npromising performance using only a small language-annotated dataset, further improvements could\nbe achieved by incorporating semantic information into the observation, using a variational learning\nframework and expanding the dataset size. We leave these investigations for future work.\n20",
    "pdf_filename": "Instant_Policy_In-Context_Imitation_Learning_via_Graph_Diffusion.pdf"
}