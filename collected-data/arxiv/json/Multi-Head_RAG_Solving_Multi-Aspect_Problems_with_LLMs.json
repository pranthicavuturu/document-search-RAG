{
    "title": "Multi-Head RAG Solving Multi-Aspect Problems with LLMs",
    "abstract": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not fo- cus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer’s multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving moti- vation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent var- ious facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets that we release online, and real-world use cases to demonstrate MRAG’s effec- tiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores. Website & code: https://github.com/spcl/MRAG 1 INTRODUCTION Large Language Models (LLMs) transformed many machine learning tasks using in-context learning abilities. They achieved such accuracy by leveraging an increasing number of parameters, which in recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources. It also comes with the danger of leaking confidential data into model weights (Yan et al., 2024; Shi et al., 2024; Patil et al., 2024). Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date. Even using the newest data, LLMs display an ongoing problem of hallucinations (Zhang et al., 2023; Xu et al., 2024c; Huang et al., 2023) by providing factually incorrect information. Retrieval Augmented Generation (RAG) was proposed (Lewis et al., 2020; Guu et al., 2020) in order to address these issues as well as others and make LLMs more trustworthy. The key idea behind RAG is to enhance the generative model’s capabilities by integrating a retrieval system that fetches relevant passages from a large corpus of data. In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model’s context for a more accurate and relevant response. Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing ∗corresponding author 1 arXiv:2406.05085v2  [cs.CL]  19 Nov 2024",
    "body": "MULTI-HEAD RAG: SOLVING\nMULTI-ASPECT PROBLEMS WITH LLMS\nMaciej Besta∗\nETH Zurich\nAles Kubicek\nETH Zurich\nRoman Niggli\nETH Zurich\nRobert Gerstenberger\nETH Zurich\nLucas Weitzendorf\nETH Zurich\nMingyuan Chi\nETH Zurich\nPatrick Iff\nETH Zurich\nJoanna Gajda\nCledar\nPiotr Nyczyk\nCledar\nJ¨urgen M¨uller\nBASF SE\nHubert Niewiadomski\nCledar\nMarcin Chrapek\nETH Zurich\nMichał Podstawski\nWarsaw University of Technology\nTorsten Hoefler\nETH Zurich\nABSTRACT\nRetrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not fo-\ncus on queries that may require fetching multiple documents with substantially\ndifferent contents. Such queries occur frequently, but are challenging because the\nembeddings of these documents may be distant in the embedding space, making\nit hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG),\na novel scheme designed to address this gap with a simple yet powerful idea:\nleveraging activations of Transformer’s multi-head attention layer, instead of the\ndecoder layer, as keys for fetching multi-aspect documents. The driving moti-\nvation is that different attention heads can learn to capture different data aspects.\nHarnessing the corresponding activations results in embeddings that represent var-\nious facets of data items and queries, improving the retrieval accuracy for complex\nqueries. We provide an evaluation methodology and metrics, multi-aspect datasets\nthat we release online, and real-world use cases to demonstrate MRAG’s effec-\ntiveness, showing improvements of up to 20% in relevance over standard RAG\nbaselines. MRAG can be seamlessly integrated with existing RAG frameworks\nand benchmarking tools like RAGAS as well as different classes of data stores.\nWebsite & code: https://github.com/spcl/MRAG\n1\nINTRODUCTION\nLarge Language Models (LLMs) transformed many machine learning tasks using in-context learning\nabilities. They achieved such accuracy by leveraging an increasing number of parameters, which in\nrecent models have grown to hundreds of billions, making LLM training expensive in terms of\nboth time and resources. It also comes with the danger of leaking confidential data into model\nweights (Yan et al., 2024; Shi et al., 2024; Patil et al., 2024). Additionally, continuous training\nthrough fine-tuning is necessary to keep LLMs up-to-date. Even using the newest data, LLMs\ndisplay an ongoing problem of hallucinations (Zhang et al., 2023; Xu et al., 2024c; Huang et al.,\n2023) by providing factually incorrect information. Retrieval Augmented Generation (RAG) was\nproposed (Lewis et al., 2020; Guu et al., 2020) in order to address these issues as well as others and\nmake LLMs more trustworthy.\nThe key idea behind RAG is to enhance the generative model’s capabilities by integrating a retrieval\nsystem that fetches relevant passages from a large corpus of data. In this setting, when a query is\nreceived, the retrieval system first identifies and retrieves pertinent information, which is fed into\nthe generative model’s context for a more accurate and relevant response. Instead of the model\nstoring information within its weights, RAG effectively leverages external knowledge, reducing\n∗corresponding author\n1\narXiv:2406.05085v2  [cs.CL]  19 Nov 2024\n\nhallucinations (by grounding the LLM reply in reliable sources), and ensuring that responses contain\nup-to-date knowledge (e.g., by accessing the Internet), all without requiring expensive training.\nMore specifically, there are two main stages in a RAG pipeline: data preparation and query execu-\ntion. During data preparation, one constructs a vector database (DB) populated with embeddings\nand their corresponding data items such as documents. During query execution, one constructs an\nembedding of that query and retrieves data items in the store with similar embeddings.\nIntense recent research efforts have been put into RAG (Gao et al., 2024; Zhao et al., 2024; Hu &\nLu, 2024; Huang & Huang, 2024; Yu et al., 2024; Mialon et al., 2023; Li et al., 2022). On one\nhand, different RAG designs have been proposed, for example RAPTOR (Sarthi et al., 2024), Self-\nRAG (Asai et al., 2023), Chain-of-Note (Yu et al., 2023), and many others (Abdallah & Jatowt, 2024;\nDelile et al., 2024; Edge et al., 2024; Manathunga & Illangasekara, 2023; Zeng et al., 2024; Wewer\net al., 2021; Xu et al., 2024b). In general, these schemes focus on making the retrieved data more\naccurate and relevant to the query. There have also been efforts into benchmarking and datasets for\nRAG evaluation (Chen et al., 2024b; Xiong et al., 2024; Lyu et al., 2024; Es et al., 2023).\nDespite all these advances, we observe that no existing RAG scheme or evaluation methodology\nexplicitly targets an important class of problems that come with a high degree of multi-aspectuality.\nThese are problems that require combining several (potentially many) significantly different aspects\nin a single query. As a simple illustrative example of such a query, consider the question “What\ncar did Alexander the Great drive?”, and assume that the queried model has not been trained on\nhistory. When using RAG, to answer this question accurately, one would retrieve two documents,\none describing Alexander the Great and one outlining the history of car manufacturing. However,\nthe embeddings of these two documents could be far away from each other in the embedding space.\nAt the same time, such queries are common in different industry settings, as indicated by exten-\nsive discussions with our industry collaborators. Imagine a chemical processing plant experiencing\nan equipment accident. One could use an LLM to find the accident cause, which might require\nthe retrieval of multiple, potentially confidential documents to provide the necessary context. These\ndocuments could be related to different aspects, for example psychological profiles of workers (“Was\nthe accident due to mismanaging a worker?”), equipment purchase records (“Was some equipment\npart too old?”), maintenance (“Was some equipment part rusty?”), weather (“Was there a particu-\nlarly strong thunderstorm at the accident time that could have caused dangerous power spikes in the\ngrid?”), or even microclimate (“Was it too humid for an extended period of time in the production\nhall?”). As we illustrate in Section 4, such problems pose challenges for existing RAG schemes and\nhave been unaddressed by modern RAG benchmarking pipelines.\nTransformer\nAttention\nAttention-head 1\nFeed-\nforward\nAttention-head 2\nAttention-head h\nembedding\nused in \nstandard\nRAG\nembeddings\nused in\nMRAG\nLast\nblock\nSecond\nblock\nFirst\nblock\nDecoder blocks\nKey idea: use multiple smaller embeddings from attention heads\nGenerating all single-aspect embeddings \nneeded in MRAG requires encoding the \ncorresponding input data once\nFigure 1: An overview of the decoder architecture, and a\ncomparison of how standard RAG and Multi-Head RAG\nembeddings are generated.\nIn this work, we propose Multi-Head RAG (MRAG): a\nscheme that addresses the above problem. Common prac-\ntice in modern RAG designs is the use of embeddings\nbased on last-layer decoder block activations. Our key\nidea is to use instead the activations of the multi-head\nattention part of the decoder block as embeddings. The\nTransformer architecture can be seen as a pipeline with\nmany (e.g., 96 for GPT-3 (Wang et al., 2024b)) blocks,\nwhere a single block consists of an attention module and\na feed-forward module. Each individual attention mod-\nule is multi-headed: it consists of multiple parts called\nheads that learn different sets of weight matrices; see Fig-\nure 1 for an overview. It is conjectured that these differ-\nent heads could capture different aspects of the processed\ndata. We use this as a driving design feature that facili-\ntates capturing the potential multi-aspectuality of the data\nwithout increasing space requirements compared to stan-\ndard RAG, and without any fine-tuning or other modifi-\ncations to the harnessed model (contribution 1).\nSuch multi-aspect embeddings are then directly used for\nboth data items and query representation. Considering multi-aspectuality explicitly comes with\nchallenges. For example, it is unclear how to assess whether a RAG solution does indeed harness\nmultiple aspects when fetching documents. For this, we establish an evaluation methodology as\n2\n\nwell as a full data construction and query processing pipeline that implements the multi-aspect\nembedding idea (contribution 2). Our datasets facilitate broad evaluation by considering both fully-\nautomatically generated, synthetic data as well as data based on specific industry use cases that\nshow the benefits of MRAG (contribution 3). We ensure the relevance of our RAG datasets in\nreal use cases by working directly with tech leaders (e.g., a generative AI division head) from 3\ncorporations, all of which actively use RAG in their own LLM infrastructures. Our evaluation\nillustrates the benefits in the relevance of retrieved documents, for example 20% over a modern\nRAG baseline for fetching multi-aspect Wikipedia articles, and comparable performance for single-\naspect queries (contribution 4). We also show how MRAG and its benchmarking principles can\nbe seamlessly integrated with both existing RAG solutions and benchmarking frameworks such as\nRAGAS (contribution 5). MRAG’s code is publicly available1.\n2\nTHE MRAG FORMULATION & PIPELINE\nWe now present in detail the mathematical underpinning of MRAG and its corresponding pipeline.\nDecoder Formulation We first introduce formally the decoder architecture. We omit, for clarity,\nunnecessary details such as layer normalizations. The input is a text chunk that consists of n tokens.\nThe output of attention head h for the ith token xi is defined as follows (Vaswani et al., 2017):\nheadh(xi) =\nX\nj\nwijvh\nj ,\nwhere\n(1)\nwij = softmax\n\u0010\u0000qh\ni\n\u0001T kh\nj\n\u0011\n,\nqh\ni = Wh\nq xi,\nkh\nj = Wh\nkxj,\nvh\nj = Wh\nvxj\n(2)\nwhere Wh\nq , Wh\nk, Wh\nv are, respectively, learnable query, key, and value projections associated with\nhead h, and xj is the vector embedding of the jth token xj. These outputs get combined to form the\noutput of the ith multi-head attention block as follows:\nmulti-head(xi) = Wo concat(head1(xi), . . . , headh(xi))T\n(3)\nwhere matrix Wo is the linear layer that combines the outcomes of all the attention heads. This step\nis then followed by the Transformer feed-forward layer.\nStandard RAG Formulation Assume a sequence of n tokens as the input text chunk. The embed-\nding for that chunk is obtained as the activation vector after the feed-forward decoder layer for the\nlast nth token of this chunk, i.e., feed-forward(multi-head(xn)), generated in the last decoder block.\nMulti-Head RAG Formulation The key idea behind MRAG is simple: instead of the single acti-\nvation vector generated by the last feed-forward decoder layer for the last token, we harness the H\nseparate activation vectors generated by the last attention layer for the last token, before processing\nit via Wo. This can be formulated as a set of embeddings S:\nS = {ek∀k}\nwhere\nek = headk(xn)\n(4)\nwhich is simply the set of all outputs from the attention heads on the last token xn of the input. As\nprocessing with multiple heads does not change the size of the output vector, S has the same space\nrequirements as standard RAG. However, because we capture the separate embeddings before their\nmixing with Wo, we conjecture that it gives more information about what the different parts of the\ninput attend to, facilitating capturing multi-aspectuality.\nNaming We use the terms “single-aspect embedding” and “multi-aspect embedding” to refer to,\nrespectively, a small embedding extracted from a single attention head and a collection of all single-\naspect embeddings extracted from an attention layer.\n2.1\nOVERVIEW OF THE MULTI-HEAD RAG PIPELINE\nWe now describe how the above embedding model fits the RAG pipeline. Figure 2 shows a summary\nof the design. The MRAG pipeline consists of two main parts, dedicated to data preparation A and\nquery execution B. Both parts heavily use the data store D (vector DB).\n2.1.1\nDATA PREPARATION\nWhen preparing data A, we populate a data store D with multi-aspect MRAG text embeddings\nand\ntheir corresponding documents\nor text chunks\n(MRAG is orthogonal to the type of data being\n1https://github.com/spcl/MRAG\n3\n\nSource documents\nText chunks\nText embeddings\n(see Section 2.2)\nEmbedding\nmodel\n(see Figure 1)\nAssessment\n(see Section 3)\nLanguage model\nReply to user\nSynthetic\ndata generator\n(see Section 3)\nOther data\nsources\nQuery embedding \n(see Section 2.2)\nRetrieval engine\n(see Section 2.3)\nUser\nQuery\nSynthetic\nquery generator\n(see Section 3)\nData store\nData preparation (see Section 2.1.1)\nQuery execution (see Section 2.1.2)\nText chunks\ninsert (embedding,chunk)-pairs\nﬁnd closest embeddings and\nretrieve associated chunks\nThe query contains\ntwo aspects (     ),\nbut the single\nembedding space\nof the standard\nRAG only captures\none of them (  ).\nWe retrieve the\ntwo closest items:\na relevant (  ) and\nan irrelevant (  ) one.\nStandard RAG\nEmbedding space\nMRAG\nEmbedding space 1\nEmbedding space 2\nText chunks\nThe query contains\ntwo aspects (     ),\none of them (  )\nis captured by\nembedding space 1\nand one of them (  )\nis captured by\nembedding space 2.\nWe retrieve the two\nclosest items (     )\nwhich are the\ntwo relevant ones.\nEmbedding space h\nA\nC\nB\nD\nWe store a key for each \nhead as a column in the \ndatabase (no added \nmemory cost).\nFigure 2: Overview of the MRAG pipeline, consisting of two parts: data preparation A and query execution B. The embedding model C and\nthe data store D are used by both parts. The data store D contains text embeddings\nlinking to text chunks\nreflecting three different aspects\n(cyan, magenta, yellow). Blocks marked by a star\nare a novelty of this work.\nembedded, and while we primarily use chunking of documents in order to reflect modern RAG\npipelines, one can also embed whole documents or even other types of data). We create the multi-\naspect embedding\nof each text chunk\nusing a selected decoder-based embedding model C (this\npart is detailed in Section 2.2). The user of the pipeline can plug in their model C of choice as well\nas use their input data. We also offer a dedicated synthetic data generator\nthat can be used to\nconstruct multi-aspect input documents\n(we detail this part in Section 3) for evaluation purposes.\nMRAG stores data differently than standard RAG, where a single embedding\npoints to a single\ntext chunk\n. For MRAG, each multi-aspect embedding consists of h single-aspect embeddings ,\neach pointing to the original text chunk\n. So the data store D contains h embedding spaces, each\ncapturing a different aspect of the text. This crucial feature allows MRAG to compare query\nand\ntext chunks\nin multiple embedding spaces that capture multiple aspects of the data.\n2.1.2\nQUERY EXECUTION\nDuring query execution B, we first generate a multi-aspect embedding\nof the input query\n, using\nthe selected embedding model C (details in Section 2.2). Then, we find the nearest multi-aspect\nembeddings\nand their corresponding text chunks\nin the data store D using a special multi-aspect\nretrieval strategy\n(detailed in Section 2.3). We ensure that there is no overhead in latency due\nto multiple aspects because computing these different smaller embeddings is done fully in parallel.\nFinally, the retrieved data can optionally be assessed\nwith novel metrics regarding how well it\ncorresponds to the multi-aspect requirements (detailed in Section 3). As with the data preparation\nA stage, the query execution B stage is flexible, and the user can plug in their models C /\nof\nchoice and use their own queries\n. We also offer a dedicated synthetic query generator\nthat can\nbe used to construct multi-aspect input queries\n(detailed in Section 3) for evaluation purposes.\n2.2\nCONSTRUCTING MULTI-ASPECT EMBEDDINGS\nMRAG can leverage any embedding model with multi-head attention support to construct the multi-\naspect embeddings for a given input text. In this work, we consider two embedding models from the\nMTEB leaderboard (Huggingface, 2024) as potential candidates. Specifically, the SFR-Embedding-\nModel (Meng et al., 2024) and the e5-mistral-7b-instruct (Wang et al., 2024a), both based on the\nMistral 7B architecture with 32 decoder blocks and 32 attention heads per multi-head attention.\nWhile our approach allows for extracting and using the multi-aspect embeddings from any decoder\nblock, and from different layers within a block, we found that multi-aspect embeddings extracted\n4\n\nfrom the last multi-head attention worked best in our experimental setting. We provide further\ndiscussion on the carried out experiments in Section 4.\n2.3\nRETRIEVAL STRATEGIES FOR MULTI-ASPECT DATA\nA retrieval strategy determines how we select the closest text chunks from the DB given a multi-\naspect embedding of the user query. In general, the MRAG retrieval strategy consists of three steps.\nFirst, during data preparation, we assign importance scores to all h embedding spaces. Intuitively,\nthese scores capture the fact that different spaces (and the corresponding heads) may be more or less\nrelevant for the used data. Then, during query execution, MRAG starts by applying the traditional\nRAG retrieval separately for each embedding space. This returns a list of c closest text chunks for\neach embedding space (a total of h lists). Here, we use a special voting strategy to pick overall top\nk out of all hc chunks, using the pre-computed importance scores.\nAlgorithm 1 details the construction of importance scores.\nIt is a heuristic based on\nextensive empirical evaluation;\nit gives high-quality results across the tested datasets and\ntasks.\nIntuitively, the score si of a given head hi consists of two parts, ai and bi.\nai\nis the average of L2 norms of all embeddings in the vector space i; it represents how im-\nportant a given head is:\nthe larger the norms, the more attention was given to this at-\ntention head.\nbi is the average of cosine distances between all (or a randomly sam-\npled subset, if the user wants to reduce pre-compute time) embeddings in vector space i.\nAlgorithm 1 Importance scores for heads.\nfor each head hi do\nai ←0; bi ←0\ncount ai ←0; count bi ←0\nfor each embedding eij in hi do\nai ←ai + ||eij||\ncount ai ←count ai + 1\nfor each embedding eih do\nbi ←bi + cosine-distance(eij, eih)\ncount bi ←count bi + 1\nend for\nend for\nai ←ai/count ai; bi ←bi/count bi\nsi ←ai · bi\nend for\nIntuitively, bi is a proxy for measuring the\n“spread” of vector space i: the larger bi, the\nlarger the average angle between different em-\nbeddings in this space is. Deriving si as a prod-\nuct ai·bi ensures that we reward heads with high\naverage attention and high average spread, but\nsimultaneously penalize heads with lower aver-\nage attention or with low average spread (both\nai and bi are appropriately scaled).\nThe used voting strategy combines the con-\nstructed lists of text chunks from individual em-\nbedding spaces into a single list of top k chunks.\nThe strategy is very simple (the corresponding\nAlgorithm 2 is in the Appendix).\nEach text\nchunk from a list i of the vector space i has a\ncertain position on this list, we denote this posi-\ntion with p. We obtain a weight for this chunk as\nsi · 2−p; si is the previously defined importance\nscore of the space i. Multiplying si with 2−p exponentially lowers the significance of less relevant\ntext chunks. Finally, all chunks from all lists are sorted using their weights and the top k chunks\nform the final list.\n2.3.1\nINTEGRATION WITH DATA STORES & OTHER TYPES OF MODELS\nMRAG can be seamlessly used with different classes of data stores C and nearest neighbor (NN)\nsearch approaches. It can be combined with both the exact and the approximate NN to find the\nmatching (embedding, chunk)-pairs. These two parts of the broader RAG processing pipeline are\northogonal to MRAG. Similarly, MRAG does not depend specifically on the embedding form, as\nlong as it is based on a model that harnesses multi-head attention any such approach that results in a\nvalid embedding can be used. As such, it could also be used with models such as RetroMAE (Xiao\net al., 2022) and the classic BGE-embeddings (Xiao et al., 2022; Chen et al., 2024a).\n3\nMULTI-ASPECT DATASETS, QUERIES, AND METRICS\nTo assess how well MRAG performs on multi-aspect queries, and to compare it to modern RAG\nschemes, we need (1) datasets of documents that capture multi-aspectuality, (2) queries to the LLM\nthat touch upon multi-aspectuality and require retrieving different documents from the multi-aspect\ndataset, and (3) metrics that asses how well a given RAG scheme retrieves such multi-aspect data.\nWe now describe these three elements. In Section 4, we also discuss real-world data and queries\nused to evaluate MRAG.\n5\n\n3.1\nMULTI-ASPECT DATASETS\nWe first select conceptually different categories of documents for a synthetic dataset. Here, we\nharness publicly available Wikipedia articles. In the dataset construction pipeline, the user selects\na given number of categories (e.g., countries, board games, historical swords, shipwrecks, etc.)\nand then, for each category, they sample a specified number of documents. The first part of the\ndocument (overview) is used as a text chunk to be embedded. We enforce that each overview must\nhave at least 800 characters, matching commonly used chunk sizes in RAG schemes. We also\nuse multi-aspect real-world inspired datasets consisting of NDAs and reports describing industry\naccidents in chemical processing plants. We ensure the usefulness of these datasets by working\ndirectly with tech leaders from 3 corporations that rely on RAG in their in-house LLM-driven report\ngeneration and analytics frameworks. Example categories of the legal documents are legal areas\n(energy law, family law, criminal law, etc.) or document language style (aggressive, mild, neutral,\netc.). Examples of accident causes are natural disasters, human mistakes, or lack of proper training.\nWe fully release these datasets to propel RAG research. Details on all three datasets can be found in\nthe Appendix B.2. In our evaluation, we use a total of 13,750 documents.\n3.2\nMULTI-ASPECT QUERY GENERATION\nWe also require queries that touch upon a given number of n aspects. For example, a query with 10\naspects must contain a question about 10 different documents from 10 different categories. We create\nsuch queries by selecting n categories, sampling a document from each selected category (ensuring\nthere are no duplicates overall), and then generating a story that combines these documents, using\nan LLM (GPT-3.5 Turbo). We construct 25 queries with 1, 5, 10, 15 and 20 aspects (125 queries in\ntotal). An example multi-aspect query sent to the LLM that requires retrieving 10 documents from\n10 different categories, is pictured in the top part of Figure 3.\n3.3\nMETRICS\nWe also design novel metrics to assess how well a given RAG scheme supports multi-aspectuality.\nFor a query Q, a used retrieval strategy S (detailed in Section 2.3), and n documents from n cat-\negories to retrieve, Qrel denotes the ideal set of documents that should be retrieved for Q. Then,\nS(Q, n) is the set of the actually retrieved documents. We define the Retrieval Success Ratio as\nΞ(Q, n) =\n|S(Q,n)∩Qrel|\n|Qrel|\n, i.e., the ratio of successfully retrieved relevant documents. Moreover,\nthere is a case when a RAG scheme does not retrieve the exact desired document, but it still retrieves\nsuccessfully some other document from the same category. While less desired, it still increases\nchances for a more accurate LLM answer following the retrieval. For example, when asking the\nLLM to determine the cause of an industry accident, fetching the documents in the same category\nas the accident being queried about, improves the chances for the LLM to give a more relevant an-\nswer. To consider such cases, we use another measure, the Category Retrieval Success Ratio or\nΞc. It has the same form as Ξ(Q, n) above, with one difference: S(Q, n) is now the set of all the\nretrieved documents that belong to categories of the ideal desired documents. Finally, to combine\nthese two metrics, we use the Weighted Retrieval Success Ratio Ξw as Ξw = w·Ξ+Ξc\nw+1 . By varying\nw, the user can adjust the importance of exact document matches and category matches. An example\nof using these metrics to assess how well MRAG and Standard RAG capture multi-aspectuality is\npictured in the bottom part of Figure 3.\n4\nEVALUATION\nWe now illustrate the advantages of MRAG over the state of the art.\nComparison Baselines We consider three main baselines: Standard RAG, Split RAG, and Fusion\nRAG (Rackauckas, 2024). The first represents a modern RAG pipeline in which each document uses\nthe activations of the last decoder layer as its embedding. The second is a blend between Standard\nRAG and MRAG. Specifically, it splits the activation of the last decoder layer in the same way as\nMRAG and applies a voting strategy. The purpose of Split RAG is to show that MRAG’s benefits\ncome from using the multi-head output as embedding and not merely using multiple embedding\nspaces. Additionally, we consider Fusion RAG (Rackauckas, 2024), an optional mechanism that\nwe harness to further enhance the benefits of MRAG at the cost of additional tokens (detailed in\nSection 4.3).\n6\n\nIn a realm where the digital realm merged with reality, the phenomenon of Twitch Plays Pokémon captivated the masses, blurring the lines between player and spectator, much like the elusive \nconcept of Money Illusion that tricked minds into perceiving value where none truly existed. And in the midst of it all, a strategic dance unfolded on the board of Camelot, where tactics intertwined \nwith skill in a timeless battle of wits.\nAmidst the vast expanse of the cosmos, the majestic Kongō-class battlecruisers sailed through the stars, their presence a testament to both honor and sacrifice in the raging tides of war. Their \nlegacy echoed through the ages, much like the volumes of knowledge meticulously preserved in ancient libraries, each page a treasure trove of insights waiting to be discovered.\nIn a realm where the echoes of music intertwined with the whispers of ancient battles, a curious scholar, named Luc Montagnier, delved into the mysteries of a peculiar instrument known as the \nTheremin. As he studied its ethereal melodies that seemed to bridge the gap between reality and the unknown, memories of the enigmatic disappearance of the esteemed mayor Celso Daniel \nhaunted his thoughts.\n5 SRAG:     MRAG:\nExample Prompt\nSRAG: Standard RAG\nLegend:\nMRAG: Multi-Head RAG (this work)\nDocument match\nDocument ID\nCategory match\nRepeated category match\nNo match\nMeanwhile, in a land where dreams took flight on the wings of imagination, children gathered to watch the fantastical tale of \"James and the Giant Peach\" unfold on the silver screen. The \nwhimsical story transported them to a world beyond their own, much like the desert planet of Arrakis in the epic novel \"Dune,\" where the precious spice held the key to power and destiny.\nAnd so, the scholar pondered these diverse threads of existence, seeking to unravel the intricate tapestry that connected Luc Montagnier to the Theremin, Celso Daniel to the mysteries of power, \nand the timeless saga of Dune to the strategic depths of Camelot. In this weaving of tales, each article found its place, like pieces of a puzzle coming together to reveal a grand design hidden \nwithin the annals of history.\nGiven a story, retrieve relevant documents that provide contextual information about topics brought up in the story.\n4 SRAG:     MRAG:\n1 SRAG:     MRAG:\nGround Truth\nLuc Montagnier\nTheremin\nCelso Daniel\nJames and the Giant Peach \nDune (novel)\nKongō-class battlecruiser\nVolume (bibliography)\nTwitch Plays Pokémon\nMoney illusion\nCamelot (board game)\nNobel\nInstruments\nAssassinated\nDisney\nSci-fi Novels\nShipwrecks\nBooks\nMemes\nCognitive Bias\nBoard Games\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDocument\nID\nCategory\nDune (novel)\nThe Most Mysterious Song on the Internet\nTheremin\nThe Dry Salvages (novella)\nA Canticle for Leibowitz\nJourney to the Center of the Earth\nThe Narrative of Arthur Gordon Pym \nFahrenheit 451\nThe Giver\nThe Left Hand of Darkness\nSci-fi Novels\nMemes\nInstruments\nSci-fi Novels\nSci-fi Novels\nSci-fi Novels\nSci-fi Novels\nSci-fi Novels\nSci-fi Novels\nSci-fi Novels\n5\n8\n2\nDocument\nCategory\nMatch\nStandard RAG (SRAG)\nRetrieval Success Ratio (Document Match):\nRetrieval Success Ratio (Category Match):\n2/10\n3/10\nWeighted Retrieval Success Ratio (2:1):\n0.23\nTheremin\nThe Most Mysterious Song on the Internet\nLuc Montagnier\nThe Decameron\nDune (novel)\nFictional book\nMoney illusion\nJames and the Giant Peach\nNicole Kidman AMC Theatres commercial\nCool Runnings\nInstruments\nMemes\nNobel\nBooks\nSci-fi Novels\nBooks\nCognitive Bias\nDisney\nMemes\nDisney\n2\n8\n1\n7\n5\n9\n4\nDocument\nCategory\nMulti-Head RAG (MRAG)\nRetrieval Success Ratio (Document Match):\nRetrieval Success Ratio (Category Match):\n5/10\n7/10\nWeighted Retrieval Success Ratio (2:1):\n0.56\n3 SRAG:     MRAG:\n10 SRAG:     MRAG:\n6 SRAG:     MRAG:\n7 SRAG:     MRAG:\n2 SRAG:     MRAG:\n9 SRAG:     MRAG:\n8 SRAG:     MRAG:\nMatch\n1\n2.1\n2.2\nFigure 3: An example query used to evaluate different RAG strategies. We mention the documents to be fetched in the text and then assess\nthe success ratio of different RAG strategies in finding these documents and their categories. We mark exact document matches\n, category\nmatches\n, documents that match a category multiple times\n, and text segments with no matching document\n. Finally, we show the\nweighted success ratio for each strategy, taking a 2:1 weighting (prioritizing the exact article matches).\nWe use queries and metrics introduced in Section 3. We use the weighted retrieval success ratio\nwith 2:1 weighting, which considers category matches as relevant but prioritizes the exact document\nmatches. Figure 3 shows an example query and metrics usage. Each query requires retrieving a\nspecific number of documents and the corresponding non-overlapping categories which define the\nground truth. We fetch the top k documents from a database, where k is the “total number of\ndocuments fetched for a tested RAG scheme” (including potentially mismatches). Among these k\ndocuments, we search for matches with the ground truth.\nSamples & Summaries Each data point in our plots corresponds to 25 queries. We present the data\nusing standard boxplots to showcase the distribution. Our primary focus is on the average retrieval\nperformance among those 25 queries.\n4.1\nSUPERIOR PERFORMANCE FOR MULTI-ASPECT QUERIES\nWe start from the query example in Figure 3 and show first the absolute retrieval performance of\nMRAG over Standard RAG in Figure 4. We fix the number of aspects present in the queries to\n10, and vary the total number of retrieved documents from 10 to 30. MRAG consistently outper-\nforms Standard RAG (> 10% increase in the retrieval success ratio on average for exact document\nmatches). Moreover, the retrieval performance increase is even more significant on category matches\n(> 25% increase in the retrieval success ratio on average). The performance increase is further de-\ntailed in the histograms on the right side. Here, for a specific number of documents fetched, MRAG’s\nhistogram indicates a better distribution of retrieval success ratios (across all 25 queries).\nNext, Figure 5 shows the relative weighted performance improvement of MRAG with respect to\nStandard RAG as we vary the number of aspects present in the queries. We show data for two\ndifferent embedding models (SFR and e5). MRAG consistently outperforms the Standard RAG by\n10-20% on average, not only across the number of documents fetched, but also across the number\nof aspects present in the replies, for both models.\n4.2\nCOMPARABLE PERFORMANCE FOR SINGLE-ASPECT QUERIES\nWe additionally show in Table 1 that MRAG performs on-par with Standard RAG on queries from\nour multi-aspect dataset where only a single aspect is expected. Hence, our approach does not suffer\nfrom significant decrease in performance for single-aspect tasks.\n7\n\nFigure 4: Retrieval success ratio over 25 queries between MRAG and Standard RAG; each query uses 10 different aspects. The top part\npresents exact document matches, the bottom part presents category only matches (we explain the metrics in Section 3). A histogram is\npresented for a specific sample to showcase the detailed distribution among the 25 queries (the number of documents fetched for each query is\n30).\nTable 1: Retrieval success ratio (the exact document match) for 25 queries with a single aspect.\nDocuments Fetched\nMulti-Aspect Dataset\nLegal Dataset\nAccidents Dataset\nSFR\ne5\nSFR\nSFR\nMRAG Standard RAG — MRAG Standard RAG — MRAG Standard RAG — MRAG Standard RAG\n1\n24/25\n25/25\n24/25\n25/25\n24/25\n24/25\n25/25\n25/25\n2\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n3\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n25/25\n4.3\nFURTHER IMPROVEMENTS WITH ADDITIONAL TOKENS\nWe now show that MRAG can be seamlessly integrated with other RAG approaches: We combine\nMRAG with Fusion RAG, representing RAG schemes that use an LLM (additional token cost) for\nmore accurate retrieval. Fusion RAG uses an LLM to create a fixed number of questions about the\nRAG query. Each question is separately applied through an embedding model using Standard RAG.\nWe apply MRAG’s approach to each of these questions and denote the combined scheme as Fusion\nMRAG. Red plots of Figure 6 show that both Fusion RAG and Fusion MRAG perform better than\nStandard RAG, on average gaining 10 to 30% in accuracy. Fusion MRAG performs consistently\nbetter than pure Fusion RAG, indicating that these optimizations can be combined together. How-\never, both Fusion strategies introduce a greater variance than MRAG and additional costs in terms\nof compute, latency, and tokens.\n4.4\nBENEFITS FROM MULTI-HEAD ATTENTION SOLELY\nWe also compare MRAG to the Split RAG baseline in Figure 6. The blue plots show the relative\nweighted performance of MRAG and Split RAG over Standard RAG. MRAG performs better than\nSplit RAG, illustrating that its high accuracy is due to the actual multi-head part, and not merely\njust partitioning the vector and using multiple embedding spaces.\nFigure 5: Relative retrieval improvement of MRAG over Standard RAG across queries with different numbers of aspects and different\nembedding models (SFR in the left side, e5 in the right side).\n8\n\nFigure 6: Relative retrieval improvements of MRAG over Standard RAG for the SFR embedding model compared with Split RAG (the\nblue plots), and the relative retrieval improvements of Fusion MRAG over Standard RAG compared with Fusion RAG (the red plots).\nFigure 7: Average improvement of the retrieval success ratio of MRAG and Split RAG over Standard RAG for two real-world workloads\nconstructing legal documents (left) and discovering causes of industry accidents (right).\n4.5\nREAL-WORLD WORKLOADS\nTo further illustrate advantages of MRAG, we also consider two real-word use cases from in-house\nindustry data analytics projects, namely, the synthesis of legal documents and the analysis of causes\nof chemical plant accidents. The results are in Figure 7. In the former (the left side), the task is to\ncreate a document based on user requirements that may be related to different aspects, for example\nto the law being considered (e.g., the British or the US one), the subject (e.g., energetic or civil),\nthe style of the document (e.g., aggressive or mild), etc.. This task is executed with RAG that can\nfetch documents from a database. In the latter (the right side), the task is to discover a cause of an\naccident. Here, one also wants to retrieve documents from a database that should be used in the LLM\ncontext to facilitate discovering the cause of the accident. The causes are grouped in categories such\nas utility impact due to severe weather, lack of preparedness and planning, incorrect installation of\nequipment, lack of maintenance, etc.. Similarly to the previous analyses, we measure the retrieval\nsuccess ratio over corresponding databases. MRAG offers advantages over other schemes.\n4.6\nADDITIONAL ANALYSES\nWe also analyze the impact of using embeddings from different decoder blocks for MRAG (instead\nof the last one). Here, we consider taking multi-aspect embeddings from three different layers of the\nembedding model: after the first multi-head attention block, after multi-head attention block 16 (in\nthe middle of the decoder architecture), and the final multi-head attention. We discover that the last\nmulti-head attention performs the best when compared with the Standard RAG.\nWe also illustrate selected representative data from a long investigation into two additional voting\nstrategies for MRAG. We compare MRAG (1) where only the exponential lowering of significance\nof selected chunks is applied (wi,p = 2−p), and MRAG (2) which assigns the weight for each\ntext chunk based on the distance between the particular text chunk (di,p) and the query (q) (wi =\n1\ndistance(di,p,q)). Figure 8 shows that these voting strategies perform worse on average than our\nFigure 8: Evaluation of different voting strategies for MRAG and Split RAG.\n9\n\nselected strategy for MRAG, justifying its design and selection (described in Section 2.3). We also\nconsider two voting strategies for Split RAG, to further deepen the empirical evaluation. Split (1)\nonly uses the exponential lowering of significance (wi,p = 2−p) and Split (2) which uses the same\nstrategy as MRAG (wi,p = si · 2−p). Figure 8 (on the right) shows that these voting strategies\nare on-par with each other while being worse than MRAG, further showcasing the advantages of\nMRAG.\nThe complexity of the importance score calculation (Algorithm 1) is O(n2) where n is the number\nof the embedded documents; it is dominated by calculating the pair-wise cosine similarity and the\ncalculation of the norm. Please note that this step needs to be done only once for each dataset and is\nnot a bottleneck.\nFinally, in addition to the performance evaluation, we also investigated the attention heads of the\nSFR-Embedding-Mistral model as well as LLaMA-2 7B model (model not fine-tuned for text-\nembedding tasks). This analysis is presented in Appendix C.\n5\nRELATED WORK\nOur work touches on many areas which we now briefly discuss.\nMany RAG schemes appeared recently (Gao et al., 2024), using the output of the last decoder layer\nfor embedding generation. In contrast, MRAG leverages different embedding spaces of attention\nheads to focus on different aspects of documents and queries. As such, it can be combined with\nother schemes to further improve RAG pipelines.\nRetrieval is sometimes enhanced by a cross-encoder reranking phase (Rosa et al., 2022; Nogueira\n& Cho, 2020; Nogueira et al., 2020; Li et al., 2021; Gao et al., 2021; MacAvaney et al., 2019). In\nsuch solutions, typically after retrieving a set of relevant chunks, they are re-ranked using specialized\nmodels. In this work, we focus solely on the first retrieval phase, so MRAG can be seamlessly used\nin conjunction with such cross-encoders.\nStructure-enhanced RAG schemes employ different strategies for structuring text to improve re-\ntrieval quality. A common idea is to construct a Knowledge Graph from text, which enables retrieval\namongst entities and relationships (Jiang et al., 2024; Delile et al., 2024; Hussien et al., 2024; Bui\net al., 2024; Xu et al., 2024a). RAPTOR (Sarthi et al., 2024) generates multi-level summaries for\nclusters of related chunks, building a tree of summaries with increasing levels of abstraction to better\ncapture the meaning of the text. Graph RAG (Edge et al., 2024) creates a Knowledge Graph, and\nsummarizes communities in the graph, which provide data at the different levels of abstraction. All\nthese systems try to improve RAG quality by utilizing additional structures that describe entity re-\nlationships or the inner organization of text. Usually, they need a sophisticated preprocessing phase\nto prepare such structures. MRAG achieves the improvement solely based on the embedding model\nand has no additional storage requirements, and can be combined with any of these schemes.\n6\nCONCLUSION\nRetrieval Augmented Generation (RAG) is pivotal for democratizing access to accurate and relevant\noutputs from large language models (LLMs). Enhancing the precision and relevance of these outputs\nis a critical goal, especially given the challenges posed by queries requiring the retrieval of multiple\ndocuments with significantly different contents. These complex queries are common across various\ndomains, but existing RAG solutions struggle because the embeddings of the necessary documents\ncan be far apart in the embedding space, complicating their retrieval.\nTo address this gap, we introduced Multi-Head RAG (MRAG), a novel scheme that leverages the\nactivations from the multi-head attention layer of decoder models instead of the traditional feed-\nforward layer. This approach is grounded in the insight that different attention heads can capture\ndistinct aspects of the data. By using these diverse activations, MRAG creates embeddings that better\nrepresent the multifaceted nature of data items and queries, thus enhancing the retrieval accuracy for\ncomplex, multi-aspect queries. The simplicity and versatility of this idea allow it to be seamlessly\nintegrated into any modern RAG pipeline or data analytics framework.\nOur comprehensive evaluation methodology, including specific metrics, synthetic datasets, and real-\nworld use cases, demonstrates MRAG’s effectiveness. The results indicate a significant improve-\nment in the relevance of retrieved documents, with up to 20% better performance compared to\n10\n\nmodern RAG baselines. This validates MRAG’s potential to handle the intricacies of multi-aspect\nqueries effectively.\nMoreover, MRAG proves to be both cost-effective and energy-efficient. It does not require additional\nLLM queries, multiple model instances, increased storage, or multiple inference passes over the\nembedding model. This efficiency, combined with the enhanced retrieval accuracy, positions MRAG\nas a valuable advancement in the field of LLMs and RAG systems. By addressing the challenges of\nmulti-aspectuality in queries, MRAG paves the way for more reliable and accurate LLM applications\nacross diverse industries.\nACKNOWLEDGEMENTS\nWe thank Hussein Harake, Colin McMurtrie, Mark Klein, Angelo Mangili, and the whole CSCS\nteam granting access to the Ault and Daint machines, and for their excellent technical support. We\nthank Timo Schneider for help with infrastructure at SPCL. This project received funding from the\nEuropean Research Council (Project PSAP, No. 101002047), and the European High-Performance\nComputing Joint Undertaking (JU) under grant agreement No. 955513 (MAELSTROM). This\nproject was supported by the ETH Future Computing Laboratory (EFCL), financed by a donation\nfrom Huawei Technologies. This project received funding from the European Union’s HE research\nand innovation programme under the grant agreement No. 101070141 (Project GLACIATION).\nWe gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Cen-\nter: ACK Cyfronet AGH) for providing computer facilities and support within computational grant\nno. PLG/2024/017103.\nREFERENCES\nAbdelrahman Abdallah and Adam Jatowt.\nGenerator-Retriever-Generator Approach for Open-\nDomain Question Answering, March 2024.\nURL https://arxiv.org/abs/2307.11278.\narXiv:2307.11278.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to\nRetrieve, Generate, and Critique through Self-Reflection, October 2023. URL https://arxiv.\norg/abs/2310.11511. arXiv:2310.11511.\nTuan Bui, Oanh Tran, Phuong Nguyen, Bao Ho, Long Nguyen, Thang Bui, and Tho Quan. Cross-\nData Knowledge Graph Construction for LLM-enabled Educational Question-Answering System:\nA Case Study at HCMUT. In Proceedings of the 1st ACM Workshop on AI-Powered Q&A Systems\nfor Multimedia, AIQAM ’24, pp. 36–43, Phuket, Thailand, June 2024. Association for Computing\nMachinery. ISBN 9798400705472. doi: 10.1145/3643479.3662055. URL https://doi.org/\n10.1145/3643479.3662055.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.\nBGE M3-\nEmbedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through\nSelf-Knowledge Distillation,\nJune 2024a.\nURL https://arxiv.org/abs/2402.03216.\narXiv:2402.03216.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking Large Language Models in\nRetrieval-Augmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence,\n38(16):17754–17762, March 2024b. doi: 10.1609/aaai.v38i16.29728. URL https://ojs.aaai.\norg/index.php/AAAI/article/view/29728.\nJulien Delile, Srayanta Mukherjee, Anton Van Pamel, and Leonid Zhukov. Graph-Based Retriever\nCaptures the Long Tail of Biomedical Knowledge, February 2024. URL https://arxiv.org/\nabs/2402.12352. arXiv:2402.12352.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Tru-\nitt, and Jonathan Larson. From Local to Global: A Graph RAG Approach to Query-Focused\nSummarization, April 2024. URL https://arxiv.org/abs/2404.16130. arXiv:2404.16130.\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated Eval-\nuation of Retrieval Augmented Generation, September 2023. URL https://arxiv.org/abs/\n2309.15217. arXiv:2309.15217.\n11\n\nLuyu Gao, Zhuyun Dai, and Jamie Callan. Rethink Training of BERT Rerankers in Multi-stage\nRetrieval Pipeline. In Advances in Information Retrieval: Proceedings of the 43rd European\nConference on IR Research, Part II, ECIR ’21, pp. 280–286, Virtual Event, March 2021. Springer.\nISBN 978-3-030-72239-5. doi: 10.1007/978-3-030-72240-1 26. URL https://doi.org/10.\n1007/978-3-030-72240-1 26.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng\nWang, and Haofen Wang. Retrieval-Augmented Generation for Large Language Models: A Sur-\nvey, March 2024. URL https://arxiv.org/abs/2312.10997. arXiv:2312.10997.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-\nAugmented Language Model Pre-Training, February 2020.\nURL https://arxiv.org/abs/\n2002.08909. arXiv:2002.08909.\nYucheng Hu and Yuxing Lu. RAG and RAU: A Survey on Retrieval-Augmented Language Model\nin Natural Language Processing, April 2024.\nURL https://arxiv.org/abs/2404.19543.\narXiv:2404.19543.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong\nChen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in\nLarge Language Models: Principles, Taxonomy, Challenges, and Open Questions, November\n2023. URL https://arxiv.org/abs/2311.05232. arXiv:2311.05232.\nYizheng Huang and Jimmy Huang.\nA Survey on Retrieval-Augmented Text Generation\nfor Large Language Models, August 2024.\nURL https://arxiv.org/abs/2404.10981.\narXiv:2404.10981.\nHuggingface.\nMassive Text Embeddings Benchmark Leaderboard, 2024.\nURL https://\nhuggingface.co/spaces/mteb/leaderboard. Accessed: 2024-05-18.\nMohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Mal-\ndonado, Rub´en Izquierdo, and Miguel ´Angel Sotelo. RAG-based Explainable Prediction of Road\nUsers Behaviors for Automated Driving using Knowledge Graphs and Large Language Models,\nMay 2024. URL https://arxiv.org/abs/2405.00449. arXiv:2405.00449.\nXinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang,\nHongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang. HyKGE: A Hypothesis Knowledge\nGraph Enhanced Framework for Accurate and Reliable Medical LLMs Responses, April 2024.\nURL https://arxiv.org/abs/2312.15883. arXiv:2312.15883.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Proceedings of the Thirty-fourth Annual\nConference on Neural Information Processing Systems (NeurIPS ’20), volume 33 of Advances\nin Neural Information Processing Systems, pp. 9459–9474, Virtual Event, December 2020. Cur-\nran Associates.\nURL https://proceedings.neurips.cc/paper files/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.pdf.\nCanjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. PARADE: Passage Rep-\nresentation Aggregation for Document Reranking, June 2021. URL https://arxiv.org/abs/\n2008.09093. arXiv:2008.09093.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu.\nA Survey on Retrieval-\nAugmented Text Generation, February 2022.\nURL https://arxiv.org/abs/2202.01110.\narXiv:2202.01110.\nYuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu,\nTong Xu, and Enhong Chen. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-\nAugmented Generation of Large Language Models, July 2024. URL https://arxiv.org/abs/\n2401.17043. arXiv:2401.17043.\n12\n\nSean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. CEDR: Contextualized Em-\nbeddings for Document Ranking. In Proceedings of the 42nd International ACM SIGIR Con-\nference on Research and Development in Information Retrieval, SIGIR ’19, pp. 1101–1104,\nParis, France, July 2019. Association for Computing Machinery. ISBN 9781450361729. doi:\n10.1145/3331184.3331317. URL http://doi.org/10.1145/3331184.3331317.\nS. S. Manathunga and Y. A. Illangasekara. Retrieval Augmented Generation and Representative\nVector Summarization for Large Unstructured Textual Data in Medical Education, August 2023.\nURL https://arxiv.org/abs/2308.00479. arXiv:2308.00479.\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. SFR-\nEmbedding-Mistral: Enhance Text Retrieval with Transfer Learning. Salesforce AI Research\nBlog, 2024. URL https://blog.salesforceairesearch.com/sfr-embedded-mistral/. Ac-\ncessed: 2024-05-17.\nGr´egoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\nRoberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\nGrave, Yann LeCun, and Thomas Scialom. Augmented Language Models: A Survey. Transac-\ntions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/\nforum?id=jh7wH2AzKK. Survey Certification.\nRodrigo Nogueira and Kyunghyun Cho. Passage Re-ranking with BERT, April 2020. URL https:\n//arxiv.org/abs/1901.04085. arXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.\nDocument Ranking with a Pretrained\nSequence-to-Sequence Model, March 2020.\nURL https://arxiv.org/abs/2003.06713.\narXiv:2003.06713.\nVaidehi Patil, Peter Hase, and Mohit Bansal. Can Sensitive Information Be Deleted From LLMs?\nObjectives for Defending Against Extraction Attacks. In Proceedings of the Twelfth International\nConference on Learning Representations, ICLR ’24, Vienna, Austria, May 2024. URL https:\n//openreview.net/forum?id=7erlRDoaV8.\nZackary Rackauckas. RAG-Fusion: A New Take on Retrieval-Augmented Generation. International\nJournal on Natural Language Computing, 13(1):37–47, February 2024. ISSN 2319-4111. doi:\n10.5121/ijnlc.2024.13103. URL https://doi.org/10.5121/ijnlc.2024.13103.\nGuilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nand Rodrigo Nogueira. In Defense of Cross-Encoders for Zero-Shot Retrieval, December 2022.\nURL https://arxiv.org/abs/2212.06121. arXiv:2212.06121.\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Man-\nning. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, January 2024.\nURL https://arxiv.org/abs/2401.18059. arXiv:2401.18059.\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi\nChen, and Luke Zettlemoyer.\nDetecting Pretraining Data from Large Language Models.\nIn\nProceedings of the Twelfth International Conference on Learning Representations, ICLR ’24,\nVienna, Austria, May 2024. URL https://openreview.net/forum?id=zWqr3MQuNs.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Proceedings of the\nThirty-first Annual Conference on Neural Information Processing Systems (NIPS ’17), volume 30\nof Advances in Neural Information Processing Systems, pp. 5998–6008, Long Beach, CA, USA,\nDecember 2017. Curran Associates. URL https://proceedings.neurips.cc/paper files/\npaper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\njumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, Febru-\nary 2024a. URL https://arxiv.org/abs/2212.03533. arXiv:2212.03533.\n13\n\nZichong Wang, Zhibo Chu, Thang Viet Doan, Shiwen Ni, Min Yang, and Wenbin Zhang. His-\ntory, Development, and Principles of Large Language Models-An Introductory Survey, September\n2024b. URL https://arxiv.org/abs/2402.06853. arXiv:2402.06853.\nChristopher Wewer, Florian Lemmerich, and Michael Cochez.\nUpdating Embeddings for Dy-\nnamic Knowledge Graphs, September 2021.\nURL https://arxiv.org/abs/2109.10896.\narXiv:2109.10896.\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. RetroMAE: Pre-Training Retrieval-oriented\nLanguage Models Via Masked Auto-Encoder. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP ’22, pp. 538–548, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.35. URL\nhttps://aclanthology.org/2022.emnlp-main.35.\nGuangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking Retrieval-Augmented\nGeneration for Medicine, February 2024.\nURL https://arxiv.org/abs/2402.13178.\narXiv:2402.13178.\nZhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang,\nand Zheng Li. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service\nQuestion Answering. In Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval, SIGIR ’24, Washington, DC, USA, July 2024a.\nAssociation for Computing Machinery. URL https://doi.org/10.48550/arXiv.2404.17723.\nZhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan\nLiu, and Ge Yu. ActiveRAG: Revealing the Treasures of Knowledge via Active Learning, Febru-\nary 2024b. URL https://arxiv.org/abs/2402.13547. arXiv:2402.13547.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate Limita-\ntion of Large Language Models, January 2024c. URL https://arxiv.org/abs/2401.11817.\narXiv:2401.11817.\nBiwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng.\nOn Protecting the Data Privacy of Large Language Models (LLMs): A Survey, March 2024. URL\nhttps://arxiv.org/abs/2403.05156. arXiv:2403.05156.\nHao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. Evaluation of Retrieval-\nAugmented Generation: A Survey, July 2024.\nURL https://arxiv.org/abs/2405.07437.\narXiv:2405.07437.\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-\nNote: Enhancing Robustness in Retrieval-Augmented Language Models, November 2023. URL\nhttps://arxiv.org/abs/2311.09210. arXiv:2311.09210.\nHuimin Zeng, Zhenrui Yue, Qian Jiang, and Dong Wang. Federated Recommendation via Hybrid\nRetrieval Augmented Generation, March 2024. URL https://arxiv.org/abs/2403.04256.\narXiv:2403.04256.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,\nYu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.\nSiren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models, September\n2023. URL https://arxiv.org/abs/2309.01219. arXiv:2309.01219.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling\nYang, Wentao Zhang, Jie Jiang, and Bin Cui. Retrieval-Augmented Generation for AI-Generated\nContent: A Survey, June 2024. URL https://arxiv.org/abs/2402.19473. arXiv:2402.19473.\n14\n\nAPPENDIX\nA\nMODEL DESIGN: ADDITIONAL DETAILS\nA.1\nRETRIEVAL STRATEGIES FOR MULTI-ASPECT DATA\nAlgorithm 2 Voting strategy.\nl ←[]\nfor each head hi and its score si do\nfind best matching k text chunks\nfor each chunk di,p with index p in top k\ndo\nwi,p ←si · 2−p\nadd tuple (di,p, wi,p) to l\nend for\nend for\nsort l using weights wi,p; return top k elems\nB\nEVALUATION METHODOLOGY: ADDITIONAL DETAILS\nB.1\nCOMPUTE RESOURCES\nOur experiments were executed with compute nodes containing 4x NVIDIA GH200 and a total\nmemory of 800 GB. In general one GPU with at least 40GB of memory should suffice. We used\nat most 50GB of storage and the OpenAI API as an external resource. The full experiments took\nat most three hours of GPU time and the cost for the OpenAI API were at most $15. We carried\nout additional experiments, which amounted to around 20 hours of GPU time and cost of $25 for\nthe OpenAI API. Additional evaluation was executed with a mix of compute resources including\nNVIDIA A100 and V100 GPUs.\nB.2\nDATASET DETAILS\nTable 2: Overview of the structure and the number of documents in the respective datasets.\ndataset\n#categories\n#topics\n#documents total #documents\nWikipedia\n25\n50 documents per category\n1250\nLegal Documents\n25\n25 per category 10 per topic\n6250\nAccident Reports\n25\n25 per category 10 per topic\n6250\n15\n\nB.3\nPROMPT TEMPLATE FOR THE SYNTHETIC DATASET GENERATION\nTable 3: Prompt template for query generation.\nPlease create a story about the attached <number of articles> articles on the topics <list of titles>.\nIt is very important that each of the attached articles is relevant to the story, in a way that references\nthe content of the article, not just its title. But please also mention each title at least once. Please\nmake sure that all of the attached articles are relevant to your story, and that each article is refer-\nenced in at least two sentences! They do not necessarily have to be referenced in the same order,\nbut make sure no article is forgotten.\nImportant: Output only the story, no additional text. And do not use bullet points, or paragraphs.\nArticles:\n———\nArticle <title>:\n<body>\n<...>\n———\nAgain, make sure that you reference all the following topics in your story: <list of titles>\nC\nATTENTION HEAD ANALYSIS\nWe investigated the attention heads of two models in detail: LLaMA-2 7B and SFR-Embedding-\nMistral. We selected these two models for a detailed investigation because the former represents\nmodels that are not fine-tuned for text embeddings, while the latter is specifically the text embedding\nmodel that we used for our experiments. For each model, we looked specifically at the attention\nscores within each attention head, i.e., how much attention each head pays to each input token during\nthe inference. Knowing the semantics of the input tokens enables then deriving certain conclusions\nabout multi-aspectuality and attention heads.\nWe plot selected results in Figure 9. Each heatmap shows the dot-product between key- and value-\nprojections inside a given specified attention head, where line i of a heatmap for attention head h\nindicates the dot-products between the query-projection of token i and the key-projections of all\nprevious tokens j < i (both models use causal attention).\n0\n250\n500\n750\n1000\ntoken\n0\n200\n400\n600\n800\n1000\nstep\n(a) Head 4 LLaMA-2\n0\n50\n100\n150\n200\ntoken\n0\n50\n100\n150\n200\nstep\n(b) Head 8 SFR\n0\n250\n500\n750\n1000\ntoken\n0\n200\n400\n600\n800\n1000\nstep\n(c) Head 22 LLaMA-2\n0\n50\n100\n150\n200\ntoken\n0\n50\n100\n150\n200\nstep\n(d) Head 21 SFR\nFigure 9: Heatmap plots for selected attention heads of the LLaMA-2 7B and SFR-Embedding-Mistral models.\nFor both models, we found out that the attention patterns vary significantly between the different\nattention heads. Still, we encountered two distinct patterns. First, the diagonal lines in Figures 9a\nand 9b indicate that, when processing a certain input token x, elevated attention is paid to some\ntokens that came a constant numbers of steps before x. We postulate that this pattern is likely\nbeneficial to understanding the overall rhythm of a natural language, allowing the model to better\nidentify which words are semantically connected, and which parts of the input text refer to each\nother. Second, horizontal and vertical lines in Figures 9c and 9d show that these heads learned to\npay attention to specific tokens, regardless of how far apart they are within the input sequence. An\nintuitive justification for such patterns is the focus on certain semantic aspects of the input sequence.\n16\n\nI\nhave\ngot\nten\nvery\nfasc\nin\nated\nwith\nAlexander\nthe\nGreat\nrecently\n.\nIt\nis\njust\nun\nbel\niev\nable\nhow\nmuch\nhe\nwas\nable\nto\nachieve\n,\nand\nat\nhow\nyoung\nan\nage\nhe\ndid\nso\n.\nAt\nthe\nsame\ntime\nt\nough\n,\nit\nalso\nfeels\nlike\nhe\nreally\nwas\ngiven\nevery\nadvantage\nin\nthe\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nattention-score\n(a) Attention Head 0 LLaMA-2 7B\nIn\nstruct\n:\nGiven\na\nweb\nsearch\nquery\n,\nretrieve\nrelevant\npass\nages\nthat\nanswer\nthe\nquery\n.\nQuery\n:\nDid\nthe\nKingdom\nof\nPoland\nhave\nany\nhistorical\ninteractions\nwith\nterritor\nies\nor\nregions\nthat\nwere\nof\nsignificance\nduring\nthe\nSie\nge\nof\nCy\nropol\nis\n?\nIs\nthere\nany\nrecord\nof\nmilitary\nor\nsie\n0.012\n0.014\n0.016\n0.018\n0.020\n0.022\n0.024\nattention-score\n(b) Attention Head 8 SFR\nI\nhave\ngot\nten\nvery\nfasc\nin\nated\nwith\nAlexander\nthe\nGreat\nrecently\n.\nIt\nis\njust\nun\nbel\niev\nable\nhow\nmuch\nhe\nwas\nable\nto\nachieve\n,\nand\nat\nhow\nyoung\nan\nage\nhe\ndid\nso\n.\nAt\nthe\nsame\ntime\nt\nough\n,\nit\nalso\nfeels\nlike\nhe\nreally\nwas\ngiven\nevery\nadvantage\nin\nthe\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nattention-score\n(c) Attention Head 14 LLaMA-2 7B\nIn\nstruct\n:\nGiven\na\nweb\nsearch\nquery\n,\nretrieve\nrelevant\npass\nages\nthat\nanswer\nthe\nquery\n.\nQuery\n:\nDid\nthe\nKingdom\nof\nPoland\nhave\nany\nhistorical\ninteractions\nwith\nterritor\nies\nor\nregions\nthat\nwere\nof\nsignificance\nduring\nthe\nSie\nge\nof\nCy\nropol\nis\n?\nIs\nthere\nany\nrecord\nof\nmilitary\nor\nsie\n0.016\n0.018\n0.020\n0.022\nattention-score\n(d) Attention Head 21 SFR\nFigure 10: Attention scores for selected attention heads of the LLaMA-2 7B and SFR-Embedding-Mistral models.\nWe also detail attention scores (after applying softmax) of selected heads in Figures 10 and 11,\nwhen the model is processing the last token of its input. We see that some tokens gather a lot of\nattention from most heads, yet there is always a plethora of passages which are attended differently\nby any two attention heads. An interesting pattern we encountered was that for the SFR-Embedding-\nMistral model (see Figure 11), all heads’ attention spiked significantly on the first line-break in the\ninput sequence - either positively or negatively. We conjecture that this is a consequence of how\nthe embedding model was fine-tuned and its intended usage pattern: embedding queries are usually\nprepended with a retrieval instruction, which is terminated by a line-break. The model likely learnt\nto summarise the necessary information about this instruction inside the terminating line-break.\nIn\nstruct\n:\nGiven\na\nweb\nsearch\nquery\n,\nretrieve\nrelevant\npass\nages\nthat\nanswer\nthe\nquery\n.\nQuery\n:\nDid\nthe\nKingdom\nof\nPoland\nhave\nany\nhistorical\ninteractions\nwith\nterritor\nies\nor\nregions\nthat\nwere\nof\nsignificance\nduring\nthe\nSie\nge\nof\nCy\nropol\nis\n?\nIs\nthere\nany\nrecord\nof\nmilitary\nor\nsie\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nattention-score\nFigure 11: Attention scores for all attention heads of the SFR-Embedding-Mistral model.\n17",
    "pdf_filename": "Multi-Head_RAG_Solving_Multi-Aspect_Problems_with_LLMs.pdf"
}