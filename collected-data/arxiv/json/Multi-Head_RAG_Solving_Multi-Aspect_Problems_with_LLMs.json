{
    "title": "MULTI-HEAD RAG: SOLVING",
    "abstract": "RetrievalAugmentedGeneration(RAG)enhancestheabilitiesofLargeLanguage Models (LLMs) by enabling the retrieval of documents into the LLM context to providemoreaccurateandrelevantresponses. ExistingRAGsolutionsdonotfo- cus on queries that may require fetching multiple documents with substantially differentcontents. Suchqueriesoccurfrequently,butarechallengingbecausethe embeddingsofthesedocumentsmaybedistantintheembeddingspace, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveragingactivationsofTransformer’smulti-headattentionlayer, insteadofthe decoder layer, as keys for fetching multi-aspect documents. The driving moti- vationisthatdifferentattentionheadscanlearntocapturedifferentdataaspects. Harnessingthecorrespondingactivationsresultsinembeddingsthatrepresentvar- iousfacetsofdataitemsandqueries,improvingtheretrievalaccuracyforcomplex queries.Weprovideanevaluationmethodologyandmetrics,multi-aspectdatasets that we release online, and real-world use cases to demonstrate MRAG’s effec- tiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks andbenchmarkingtoolslikeRAGASaswellasdifferentclassesofdatastores. Website&code: https://github.com/spcl/MRAG 1 INTRODUCTION LargeLanguageModels(LLMs)transformedmanymachinelearningtasksusingin-contextlearning abilities. Theyachievedsuchaccuracybyleveraginganincreasingnumberofparameters,whichin recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources. It also comes with the danger of leaking confidential data into model weights (Yan et al., 2024; Shi et al., 2024; Patil et al., 2024). Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date. Even using the newest data, LLMs display an ongoing problem of hallucinations (Zhang et al., 2023; Xu et al., 2024c; Huang et al., 2023) by providing factually incorrect information. Retrieval Augmented Generation (RAG) was proposed(Lewisetal.,2020;Guuetal.,2020)inordertoaddresstheseissuesaswellasothersand makeLLMsmoretrustworthy. ThekeyideabehindRAGistoenhancethegenerativemodel’scapabilitiesbyintegratingaretrieval system that fetches relevant passages from a large corpus of data. In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model’s context for a more accurate and relevant response. Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing ∗correspondingauthor 1 4202 voN 91 ]LC.sc[ 2v58050.6042:viXra",
    "body": "MULTI-HEAD RAG: SOLVING\nMULTI-ASPECT PROBLEMS WITH LLMS\nMaciejBesta∗ AlesKubicek RomanNiggli RobertGerstenberger LucasWeitzendorf\nETHZurich ETHZurich ETHZurich ETHZurich ETHZurich\nMingyuanChi PatrickIff JoannaGajda PiotrNyczyk Ju¨rgenMu¨ller\nETHZurich ETHZurich Cledar Cledar BASFSE\nHubertNiewiadomski MarcinChrapek MichałPodstawski TorstenHoefler\nCledar ETHZurich WarsawUniversityofTechnology ETHZurich\nABSTRACT\nRetrievalAugmentedGeneration(RAG)enhancestheabilitiesofLargeLanguage\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovidemoreaccurateandrelevantresponses. ExistingRAGsolutionsdonotfo-\ncus on queries that may require fetching multiple documents with substantially\ndifferentcontents. Suchqueriesoccurfrequently,butarechallengingbecausethe\nembeddingsofthesedocumentsmaybedistantintheembeddingspace, making\nit hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG),\na novel scheme designed to address this gap with a simple yet powerful idea:\nleveragingactivationsofTransformer’smulti-headattentionlayer, insteadofthe\ndecoder layer, as keys for fetching multi-aspect documents. The driving moti-\nvationisthatdifferentattentionheadscanlearntocapturedifferentdataaspects.\nHarnessingthecorrespondingactivationsresultsinembeddingsthatrepresentvar-\niousfacetsofdataitemsandqueries,improvingtheretrievalaccuracyforcomplex\nqueries.Weprovideanevaluationmethodologyandmetrics,multi-aspectdatasets\nthat we release online, and real-world use cases to demonstrate MRAG’s effec-\ntiveness, showing improvements of up to 20% in relevance over standard RAG\nbaselines. MRAG can be seamlessly integrated with existing RAG frameworks\nandbenchmarkingtoolslikeRAGASaswellasdifferentclassesofdatastores.\nWebsite&code: https://github.com/spcl/MRAG\n1 INTRODUCTION\nLargeLanguageModels(LLMs)transformedmanymachinelearningtasksusingin-contextlearning\nabilities. Theyachievedsuchaccuracybyleveraginganincreasingnumberofparameters,whichin\nrecent models have grown to hundreds of billions, making LLM training expensive in terms of\nboth time and resources. It also comes with the danger of leaking confidential data into model\nweights (Yan et al., 2024; Shi et al., 2024; Patil et al., 2024). Additionally, continuous training\nthrough fine-tuning is necessary to keep LLMs up-to-date. Even using the newest data, LLMs\ndisplay an ongoing problem of hallucinations (Zhang et al., 2023; Xu et al., 2024c; Huang et al.,\n2023) by providing factually incorrect information. Retrieval Augmented Generation (RAG) was\nproposed(Lewisetal.,2020;Guuetal.,2020)inordertoaddresstheseissuesaswellasothersand\nmakeLLMsmoretrustworthy.\nThekeyideabehindRAGistoenhancethegenerativemodel’scapabilitiesbyintegratingaretrieval\nsystem that fetches relevant passages from a large corpus of data. In this setting, when a query is\nreceived, the retrieval system first identifies and retrieves pertinent information, which is fed into\nthe generative model’s context for a more accurate and relevant response. Instead of the model\nstoring information within its weights, RAG effectively leverages external knowledge, reducing\n∗correspondingauthor\n1\n4202\nvoN\n91\n]LC.sc[\n2v58050.6042:viXra\nhallucinations(bygroundingtheLLMreplyinreliablesources),andensuringthatresponsescontain\nup-to-dateknowledge(e.g.,byaccessingtheInternet),allwithoutrequiringexpensivetraining.\nMorespecifically,therearetwomainstagesinaRAGpipeline: datapreparationandqueryexecu-\ntion. During data preparation, one constructs a vector database (DB) populated with embeddings\nandtheircorrespondingdataitemssuchasdocuments. Duringqueryexecution, oneconstructsan\nembeddingofthatqueryandretrievesdataitemsinthestorewithsimilarembeddings.\nIntenserecentresearcheffortshavebeenputintoRAG(Gaoetal.,2024;Zhaoetal.,2024;Hu&\nLu, 2024; Huang & Huang, 2024; Yu et al., 2024; Mialon et al., 2023; Li et al., 2022). On one\nhand,differentRAGdesignshavebeenproposed,forexampleRAPTOR(Sarthietal.,2024),Self-\nRAG(Asaietal.,2023),Chain-of-Note(Yuetal.,2023),andmanyothers(Abdallah&Jatowt,2024;\nDelileetal.,2024;Edgeetal.,2024;Manathunga&Illangasekara,2023;Zengetal.,2024;Wewer\netal.,2021;Xuetal.,2024b). Ingeneral, theseschemesfocusonmakingtheretrieveddatamore\naccurateandrelevanttothequery. Therehavealsobeeneffortsintobenchmarkinganddatasetsfor\nRAGevaluation(Chenetal.,2024b;Xiongetal.,2024;Lyuetal.,2024;Esetal.,2023).\nDespite all these advances, we observe that no existing RAG scheme or evaluation methodology\nexplicitlytargetsanimportantclassofproblemsthatcomewithahighdegreeofmulti-aspectuality.\nTheseareproblemsthatrequirecombiningseveral(potentiallymany)significantlydifferentaspects\nin a single query. As a simple illustrative example of such a query, consider the question “What\ncar did Alexander the Great drive?”, and assume that the queried model has not been trained on\nhistory. When using RAG, to answer this question accurately, one would retrieve two documents,\none describing Alexander the Great and one outlining the history of car manufacturing. However,\ntheembeddingsofthesetwodocumentscouldbefarawayfromeachotherintheembeddingspace.\nAt the same time, such queries are common in different industry settings, as indicated by exten-\nsivediscussionswithourindustrycollaborators. Imagineachemicalprocessingplantexperiencing\nan equipment accident. One could use an LLM to find the accident cause, which might require\ntheretrievalofmultiple,potentiallyconfidentialdocumentstoprovidethenecessarycontext. These\ndocumentscouldberelatedtodifferentaspects,forexamplepsychologicalprofilesofworkers(“Was\ntheaccidentduetomismanagingaworker?”),equipmentpurchaserecords(“Wassomeequipment\nparttooold?”),maintenance(“Wassomeequipmentpartrusty?”),weather(“Wasthereaparticu-\nlarlystrongthunderstormattheaccidenttimethatcouldhavecauseddangerouspowerspikesinthe\ngrid?”),orevenmicroclimate(“Wasittoohumidforanextendedperiodoftimeintheproduction\nhall?”).AsweillustrateinSection4,suchproblemsposechallengesforexistingRAGschemesand\nhavebeenunaddressedbymodernRAGbenchmarkingpipelines.\nIn this work, we propose Multi-Head RAG (MRAG): a\nschemethataddressestheaboveproblem.Commonprac- Transformer\ntice in modern RAG designs is the use of embeddings\nbased on last-layer decoder block activations. Our key Decoderblocks\nidea is to use instead the activations of the multi-head\nFirst Second Last\nattention part of the decoder block as embeddings. The\nblock block block\nTransformer architecture can be seen as a pipeline with\nmany (e.g., 96 for GPT-3 (Wang et al., 2024b)) blocks,\nwhereasingleblockconsistsofanattentionmoduleand embedding\nFeed- used in\na feed-forward module. Each individual attention mod- Attention forward standard\nRAG\nule is multi-headed: it consists of multiple parts called\nheadsthatlearndifferentsetsofweightmatrices;seeFig- G ne en ee dr ea dti n ing Mal Rl Asi Gn g rl ee q-a us irp ee sc et ne cm ob de ind gd i tn hg es\ncorresponding input data once\nure1foranoverview. Itisconjecturedthatthesediffer-\nAttention-head 1\nentheadscouldcapturedifferentaspectsoftheprocessed\nembeddings\ndata. We use this as a driving design feature that facili- Attention-head 2 used in\nMRAG\ntatescapturingthepotentialmulti-aspectualityofthedata\nAttention-head h\nwithoutincreasingspacerequirementscomparedtostan-\nKey idea: use multiple smaller embeddings from attention heads\ndard RAG, and without any fine-tuning or other modifi-\ncationstotheharnessedmodel(contribution1). Figure1:Anoverviewofthedecoderarchitecture,anda\ncomparisonofhowstandardRAGandMulti-HeadRAG\nSuch multi-aspect embeddings are then directly used for embeddingsaregenerated.\nboth data items and query representation. Considering multi-aspectuality explicitly comes with\nchallenges. For example, it is unclear how to assess whether a RAG solution does indeed harness\nmultiple aspects when fetching documents. For this, we establish an evaluation methodology as\n2\nwell as a full data construction and query processing pipeline that implements the multi-aspect\nembeddingidea(contribution2).Ourdatasetsfacilitatebroadevaluationbyconsideringbothfully-\nautomatically generated, synthetic data as well as data based on specific industry use cases that\nshow the benefits of MRAG (contribution 3). We ensure the relevance of our RAG datasets in\nreal use cases by working directly with tech leaders (e.g., a generative AI division head) from 3\ncorporations, all of which actively use RAG in their own LLM infrastructures. Our evaluation\nillustrates the benefits in the relevance of retrieved documents, for example 20% over a modern\nRAGbaselineforfetchingmulti-aspectWikipediaarticles,andcomparableperformanceforsingle-\naspect queries (contribution 4). We also show how MRAG and its benchmarking principles can\nbeseamlesslyintegratedwithbothexistingRAGsolutionsandbenchmarkingframeworkssuchas\nRAGAS(contribution5). MRAG’scodeispubliclyavailable1.\n2 THE MRAG FORMULATION & PIPELINE\nWenowpresentindetailthemathematicalunderpinningofMRAGanditscorrespondingpipeline.\nDecoderFormulation We first introduce formally the decoder architecture. We omit, for clarity,\nunnecessarydetailssuchaslayernormalizations. Theinputisatextchunkthatconsistsofntokens.\nTheoutputofattentionheadhfortheithtokenx isdefinedasfollows(Vaswanietal.,2017):\ni\n(cid:88)\nheadh(x )= w vh, where (1)\ni ij j\nj\nw =softmax(cid:16)(cid:0) qh(cid:1)T kh(cid:17) , qh =Whx , kh =Whx , vh =Whx (2)\nij i j i q i j k j j v j\nwhereWh,Wh,Wh are,respectively,learnablequery,key,andvalueprojectionsassociatedwith\nq k v\nheadh,andx isthevectorembeddingofthejthtokenx . Theseoutputsgetcombinedtoformthe\nj j\noutputoftheithmulti-headattentionblockasfollows:\nmulti-head(x )=W concat(head1(x ),...,headh(x ))T (3)\ni o i i\nwherematrixW isthelinearlayerthatcombinestheoutcomesofalltheattentionheads. Thisstep\no\nisthenfollowedbytheTransformerfeed-forwardlayer.\nStandardRAGFormulationAssumeasequenceofntokensastheinputtextchunk. Theembed-\ndingforthatchunkisobtainedastheactivationvectorafterthefeed-forward decoderlayerforthe\nlastnthtokenofthischunk,i.e.,feed-forward(multi-head(x )),generatedinthelastdecoderblock.\nn\nMulti-HeadRAGFormulation The keyidea behindMRAG issimple: instead ofthe single acti-\nvationvectorgeneratedbythelastfeed-forwarddecoderlayerforthelasttoken,weharnesstheH\nseparateactivationvectorsgeneratedbythelastattentionlayerforthelasttoken,beforeprocessing\nitviaW . ThiscanbeformulatedasasetofembeddingsS:\no\nS ={e ∀ } where e =headk(x ) (4)\nk k k n\nwhichissimplythesetofalloutputsfromtheattentionheadsonthelasttokenx oftheinput. As\nn\nprocessingwithmultipleheadsdoesnotchangethesizeoftheoutputvector,S hasthesamespace\nrequirementsasstandardRAG.However,becausewecapturetheseparateembeddingsbeforetheir\nmixingwithW ,weconjecturethatitgivesmoreinformationaboutwhatthedifferentpartsofthe\no\ninputattendto,facilitatingcapturingmulti-aspectuality.\nNaming We use the terms “single-aspect embedding” and “multi-aspect embedding” to refer to,\nrespectively,asmallembeddingextractedfromasingleattentionheadandacollectionofallsingle-\naspectembeddingsextractedfromanattentionlayer.\n2.1 OVERVIEWOFTHEMULTI-HEADRAGPIPELINE\nWenowdescribehowtheaboveembeddingmodelfitstheRAGpipeline.Figure2showsasummary\nofthedesign.TheMRAGpipelineconsistsoftwomainparts,dedicatedtodatapreparation A and\nqueryexecution B. Bothpartsheavilyusethedatastore D (vectorDB).\n2.1.1 DATAPREPARATION\nWhenpreparingdata A,wepopulateadatastore D withmulti-aspectMRAGtextembeddings and\ntheircorrespondingdocuments ortextchunks (MRAGisorthogonaltothetypeofdatabeing\n1https://github.com/spcl/MRAG\n3\nAData preparation (see Section 2.1.1) B Query execution (see Section 2.1.2)\nSynthetic Synthetic\nOtherdata\ndata generator query generator Reply to user\n(see Section 3) sources User (see Section 3)\nSourcedocuments Query Language model\nC\nEmbedding\nmodel Query embedding Assessment\nText chunks (see Section 2.2) (see Section 3)\n(see Figure 1)\nText embeddings Retrieval engine\n(see Section 2.2) (see Section 2.3) Textchunks\nfind closest embeddings and\ninsert (embedding,chunk)-pairs retrieve associated chunks\nDData store\nStandard RAG Text chunks MRAG\nThe query contains Embedding space Embedding space 1 The query contains\ntwo aspects ( ), two aspects ( ),\nbut the single one of them ( )\nembedding space is captured by\nof the standard embedding space 1\nRAG only captures Embedding space 2 and one of them ( )\none of them ( ). is captured by\nWe retrieve the embedding space 2.\ntwo closest items: We retrieve the two\na relevant ( ) and closest items ( )\nan irrelevant ( ) one. which are the\ntwo relevant ones.\nEmbedding space h\nWe store a key for each\nhead as a column in the\ndatabase (no added\nmemory cost).\nFigure2:OverviewoftheMRAGpipeline,consistingoftwoparts:datapreparationAandqueryexecutionB.TheembeddingmodelC and\nthedatastoreDareusedbybothparts.ThedatastoreDcontainstextembeddings linkingtotextchunks reflectingthreedifferentaspects\n(cyan,magenta,yellow).Blocksmarkedbyastar areanoveltyofthiswork.\nembedded, and while we primarily use chunking of documents in order to reflect modern RAG\npipelines,onecanalsoembedwholedocumentsorevenothertypesofdata). Wecreatethemulti-\naspectembedding ofeachtextchunk usingaselecteddecoder-basedembeddingmodel C (this\npartisdetailedinSection2.2). Theuserofthepipelinecanplugintheirmodel C ofchoiceaswell\nas use their input data. We also offer a dedicated synthetic data generator that can be used to\nconstructmulti-aspectinputdocuments (wedetailthispartinSection3)forevaluationpurposes.\nMRAG stores data differently than standard RAG, where a single embedding points to a single\ntextchunk . ForMRAG,eachmulti-aspectembeddingconsistsofhsingle-aspectembeddings ,\neachpointingtotheoriginaltextchunk . Sothedatastore D containshembeddingspaces,each\ncapturingadifferentaspectofthetext. ThiscrucialfeatureallowsMRAGtocomparequery and\ntextchunks inmultipleembeddingspacesthatcapturemultipleaspectsofthedata.\n2.1.2 QUERYEXECUTION\nDuringqueryexecution B,wefirstgenerateamulti-aspectembedding oftheinputquery ,using\nthe selected embedding model C (details in Section 2.2). Then, we find the nearest multi-aspect\nembeddings andtheircorrespondingtextchunks inthedatastore D usingaspecialmulti-aspect\nretrieval strategy (detailed in Section 2.3). We ensure that there is no overhead in latency due\ntomultipleaspectsbecausecomputingthesedifferentsmallerembeddingsisdonefullyinparallel.\nFinally, the retrieved data can optionally be assessed with novel metrics regarding how well it\ncorrespondstothemulti-aspectrequirements(detailedinSection3). Aswiththedatapreparation\nA stage, the query execution B stage is flexible, and the user can plug in their models C / of\nchoiceandusetheirownqueries . Wealsoofferadedicatedsyntheticquerygenerator thatcan\nbeusedtoconstructmulti-aspectinputqueries (detailedinSection3)forevaluationpurposes.\n2.2 CONSTRUCTINGMULTI-ASPECTEMBEDDINGS\nMRAGcanleverageanyembeddingmodelwithmulti-headattentionsupporttoconstructthemulti-\naspectembeddingsforagiveninputtext. Inthiswork,weconsidertwoembeddingmodelsfromthe\nMTEBleaderboard(Huggingface,2024)aspotentialcandidates. Specifically,theSFR-Embedding-\nModel (Meng et al., 2024) and the e5-mistral-7b-instruct (Wang et al., 2024a), both based on the\nMistral7Barchitecturewith32decoderblocksand32attentionheadspermulti-headattention.\nWhileourapproachallowsforextractingandusingthemulti-aspectembeddingsfromanydecoder\nblock, and from different layers within a block, we found that multi-aspect embeddings extracted\n4\nfrom the last multi-head attention worked best in our experimental setting. We provide further\ndiscussiononthecarriedoutexperimentsinSection4.\n2.3 RETRIEVALSTRATEGIESFORMULTI-ASPECTDATA\nA retrieval strategy determines how we select the closest text chunks from the DB given a multi-\naspectembeddingoftheuserquery. Ingeneral,theMRAGretrievalstrategyconsistsofthreesteps.\nFirst,duringdatapreparation,weassignimportancescorestoallhembeddingspaces. Intuitively,\nthesescorescapturethefactthatdifferentspaces(andthecorrespondingheads)maybemoreorless\nrelevantfortheuseddata. Then,duringqueryexecution,MRAGstartsbyapplyingthetraditional\nRAGretrievalseparatelyforeachembeddingspace. Thisreturnsalistofcclosesttextchunksfor\neachembeddingspace(atotalofhlists). Here,weuseaspecialvotingstrategytopickoveralltop\nkoutofallhcchunks,usingthepre-computedimportancescores.\nAlgorithm 1 details the construction of importance scores. It is a heuristic based on\nextensive empirical evaluation; it gives high-quality results across the tested datasets and\ntasks. Intuitively, the score s of a given head h consists of two parts, a and b . a\ni i i i i\nis the average of L2 norms of all embeddings in the vector space i; it represents how im-\nportant a given head is: the larger the norms, the more attention was given to this at-\ntention head. b is the average of cosine distances between all (or a randomly sam-\ni\npled subset, if the user wants to reduce pre-compute time) embeddings in vector space i.\nIntuitively, b is a proxy for measuring the\ni\n“spread” of vector space i: the larger b , the\ni Algorithm1Importancescoresforheads.\nlarger the average angle between different em-\nbeddingsinthisspaceis. Derivings asaprod- foreachheadh do\ni i\nucta ·b ensuresthatwerewardheadswithhigh a ←0;b ←0\ni i i i\naverage attention and high average spread, but count a ←0;count b ←0\ni i\nsimultaneously penalize heads with lower aver- foreachembeddinge inh do\nij i\nage attention or with low average spread (both a ←a +||e ||\ni i ij\na andb areappropriatelyscaled). count a ←count a +1\ni i i i\nforeachembeddinge do\nThe used voting strategy combines the con- ih\nb ←b +cosine-distance(e ,e )\nstructedlistsoftextchunksfromindividualem- i i ij ih\ncount b ←count b +1\nbeddingspacesintoasinglelistoftopkchunks. i i\nendfor\nThe strategy is very simple (the corresponding\nendfor\nAlgorithm 2 is in the Appendix). Each text\na ←a /count a ;b ←b /count b\nchunk from a list i of the vector space i has a i i i i i i\ns ←a ·b\ncertainpositiononthislist,wedenotethisposi- i i i\nendfor\ntionwithp. Weobtainaweightforthischunkas\ns ·2−p;s isthepreviouslydefinedimportance\ni i\nscoreofthespacei. Multiplyings with2−p exponentiallylowersthesignificanceoflessrelevant\ni\ntext chunks. Finally, all chunks from all lists are sorted using their weights and the top k chunks\nformthefinallist.\n2.3.1 INTEGRATIONWITHDATASTORES&OTHERTYPESOFMODELS\nMRAG can be seamlessly used with different classes of data stores C and nearest neighbor (NN)\nsearch approaches. It can be combined with both the exact and the approximate NN to find the\nmatching (embedding, chunk)-pairs. These two parts of the broader RAG processing pipeline are\northogonal to MRAG. Similarly, MRAG does not depend specifically on the embedding form, as\nlongasitisbasedonamodelthatharnessesmulti-headattentionanysuchapproachthatresultsina\nvalidembeddingcanbeused. Assuch,itcouldalsobeusedwithmodelssuchasRetroMAE(Xiao\netal.,2022)andtheclassicBGE-embeddings(Xiaoetal.,2022;Chenetal.,2024a).\n3 MULTI-ASPECT DATASETS, QUERIES, AND METRICS\nTo assess how well MRAG performs on multi-aspect queries, and to compare it to modern RAG\nschemes,weneed(1)datasetsofdocumentsthatcapturemulti-aspectuality,(2)queriestotheLLM\nthattouchuponmulti-aspectualityandrequireretrievingdifferentdocumentsfromthemulti-aspect\ndataset, and(3)metricsthatasseshowwellagivenRAGschemeretrievessuchmulti-aspectdata.\nWe now describe these three elements. In Section 4, we also discuss real-world data and queries\nusedtoevaluateMRAG.\n5\n3.1 MULTI-ASPECTDATASETS\nWe first select conceptually different categories of documents for a synthetic dataset. Here, we\nharness publicly available Wikipedia articles. In the dataset construction pipeline, the user selects\na given number of categories (e.g., countries, board games, historical swords, shipwrecks, etc.)\nand then, for each category, they sample a specified number of documents. The first part of the\ndocument(overview)isusedasatextchunktobeembedded. Weenforcethateachoverviewmust\nhave at least 800 characters, matching commonly used chunk sizes in RAG schemes. We also\nusemulti-aspectreal-worldinspireddatasetsconsistingofNDAsandreportsdescribingindustry\naccidents in chemical processing plants. We ensure the usefulness of these datasets by working\ndirectlywithtechleadersfrom3corporationsthatrelyonRAGintheirin-houseLLM-drivenreport\ngeneration and analytics frameworks. Example categories of the legal documents are legal areas\n(energylaw, familylaw, criminallaw, etc.) ordocumentlanguagestyle(aggressive, mild, neutral,\netc.). Examplesofaccidentcausesarenaturaldisasters,humanmistakes,orlackofpropertraining.\nWefullyreleasethesedatasetstopropelRAGresearch. Detailsonallthreedatasetscanbefoundin\ntheAppendixB.2. Inourevaluation,weuseatotalof13,750documents.\n3.2 MULTI-ASPECTQUERYGENERATION\nWealsorequirequeriesthattouchuponagivennumberofnaspects. Forexample,aquerywith10\naspectsmustcontainaquestionabout10differentdocumentsfrom10differentcategories.Wecreate\nsuchqueriesbyselectingncategories,samplingadocumentfromeachselectedcategory(ensuring\ntherearenoduplicatesoverall), andthengeneratingastorythatcombinesthesedocuments, using\nanLLM(GPT-3.5Turbo). Weconstruct25querieswith1,5,10,15and20aspects(125queriesin\ntotal). Anexamplemulti-aspectquerysenttotheLLMthatrequiresretrieving10documentsfrom\n10differentcategories,ispicturedinthetoppartofFigure3.\n3.3 METRICS\nWealsodesignnovelmetricstoassesshowwellagivenRAGschemesupportsmulti-aspectuality.\nFor a query Q, a used retrieval strategy S (detailed in Section 2.3), and n documents from n cat-\negories to retrieve, Q denotes the ideal set of documents that should be retrieved for Q. Then,\nrel\nS(Q,n) is the set of the actually retrieved documents. We define the Retrieval Success Ratio as\nΞ(Q,n) = |S(Q,n)∩Qrel|, i.e., the ratio of successfully retrieved relevant documents. Moreover,\n|Qrel|\nthereisacasewhenaRAGschemedoesnotretrievetheexactdesireddocument,butitstillretrieves\nsuccessfully some other document from the same category. While less desired, it still increases\nchances for a more accurate LLM answer following the retrieval. For example, when asking the\nLLMtodeterminethecauseofanindustryaccident, fetchingthedocumentsinthesamecategory\nastheaccidentbeingqueriedabout,improvesthechancesfortheLLMtogiveamorerelevantan-\nswer. To consider such cases, we use another measure, the Category Retrieval Success Ratio or\nΞ . IthasthesameformasΞ(Q,n)above, withonedifference: S(Q,n)isnowthesetofallthe\nc\nretrieved documents that belong to categories of the ideal desired documents. Finally, to combine\nthesetwometrics,weusetheWeightedRetrievalSuccessRatioΞ\nw\nasΞ\nw\n= w· wΞ ++ 1Ξc. Byvarying\nw,theusercanadjusttheimportanceofexactdocumentmatchesandcategorymatches.Anexample\nof using these metrics to assess how well MRAG and Standard RAG capture multi-aspectuality is\npicturedinthebottompartofFigure3.\n4 EVALUATION\nWenowillustratetheadvantagesofMRAGoverthestateoftheart.\nComparisonBaselinesWeconsiderthreemainbaselines:StandardRAG,SplitRAG,andFusion\nRAG(Rackauckas,2024).ThefirstrepresentsamodernRAGpipelineinwhicheachdocumentuses\ntheactivationsofthelastdecoderlayerasitsembedding. ThesecondisablendbetweenStandard\nRAG and MRAG. Specifically, it splits the activation of the last decoder layer in the same way as\nMRAG and applies a voting strategy. The purpose of Split RAG is to show that MRAG’s benefits\ncome from using the multi-head output as embedding and not merely using multiple embedding\nspaces. Additionally, we consider Fusion RAG (Rackauckas, 2024), an optional mechanism that\nwe harness to further enhance the benefits of MRAG at the cost of additional tokens (detailed in\nSection4.3).\n6\nLegend: SRAG: Standard RAG MRAG: Multi-Head RAG (this work) Document ID Document match Category match Repeated category match No match\n1Example Prompt\nGiven a story, retrieve relevant documents that provide contextual information about topics brought up in the story.\n1 SRAG: MRAG: 2 SRAG: MRAG:\nIn a realm where the echoes of music intertwined with the whispers of ancient battles, a curious scholar, named LucMontagnier, delved into the mysteries of a peculiar instrument known as the\nTheremin. As he studied its ethereal melodies that seemed to bridge the gap between reality and the unknown, memories of the enigmatic disappearance of the esteemed mayor Celso Daniel\nhaunted his thoughts. 4 SRAG: MRAG: 3 SRAG: MRAG:\nMeanwhile, in a land where dreams took flight on the wings of imagination, children gathered to watch the fantastical tale of \"James and the Giant Peach\" unfold on the silver screen. The\nwhimsical story transported them to a world beyond their own, much like the desert planet of Arrakis in the epic novel \"Dune,\" where the precious spice held the key to power and destiny.\n6 SRAG: MRAG: 5 SRAG: MRAG:\nAmidst the vast expanse of the cosmos, the majestic Kongō-class battlecruisers sailed through the stars, their presence a testament to both honor and sacrifice in the raging tides of war. Their\nlegacy echoed through the ages, much like the volumes of knowledge meticulously preserved in ancient libraries, each page a treasure trove of insights waiting to be discovered.\n7 SRAG: MRAG: 8 SRAG: MRAG: 9 SRAG: MRAG:\nIn a realm where the digital realm merged with reality, the phenomenon of Twitch Plays Pokémon captivated the masses, blurring the lines between player and spectator, much like the elusive\nconcept of Money Illusion that tricked minds into perceiving value where none truly existed. And in the midst of it all, a strategic dance unfolded on the board of Camelot, where tactics intertwined\nwithskill in a timeless battle of wits. 10 SRAG: MRAG:\nAnd so, the scholar pondered these diverse threads of existence, seeking to unravel the intricate tapestry that connected Luc Montagnier to the Theremin, Celso Daniel to the mysteries of power,\nand the timeless saga of Dune to the strategic depths of Camelot. In this weaving of tales, each article found its place, like pieces of a puzzle coming together to reveal a grand design hidden\nwithin the annals of history.\nGround Truth 2.1Standard RAG (SRAG) 2.2Multi-Head RAG (MRAG)\nID Document Category Document Category Match Document Category Match\n1 Luc Montagnier Nobel Dune (novel) Sci-fi Novels 5 Theremin Instruments 2\n2 Theremin Instruments The Most Mysterious Song on the InternetMemes 8 The Most Mysterious Song on the InternetMemes 8\n3 Celso Daniel Assassinated Theremin Instruments 2 Luc Montagnier Nobel 1\n4 James and the Giant Peach Disney The Dry Salvages (novella) Sci-fi Novels The Decameron Books 7\n5 Dune (novel) Sci-fi Novels A Canticle for Leibowitz Sci-fi Novels Dune (novel) Sci-fi Novels 5\n6 Kongō-class battlecruiser Shipwrecks Journey to the Center of the Earth Sci-fi Novels Fictional book Books\n7 Volume (bibliography) Books The Narrative of Arthur Gordon Pym Sci-fi Novels Money illusion Cognitive Bias 9\n8 Twitch Plays Pokémon Memes Fahrenheit 451 Sci-fi Novels James and the Giant Peach Disney 4\n9 Money illusion Cognitive Bias The Giver Sci-fi Novels Nicole Kidman AMC Theatres commercialMemes\n10 Camelot (board game) Board Games The Left Hand of Darkness Sci-fi Novels Cool Runnings Disney\nRetrieval Success Ratio (Document Match): 2/10 Retrieval Success Ratio (Document Match): 5/10\nRetrieval Success Ratio (Category Match): 3/10 Retrieval Success Ratio (Category Match): 7/10\nWeighted Retrieval Success Ratio (2:1): 0.23 Weighted Retrieval Success Ratio (2:1): 0.56\nFigure3:AnexamplequeryusedtoevaluatedifferentRAGstrategies. Wementionthedocumentstobefetchedinthetextandthenassess\nthesuccessratioofdifferentRAGstrategiesinfindingthesedocumentsandtheircategories. Wemarkexactdocumentmatches ,category\nmatches ,documentsthatmatchacategorymultipletimes ,andtextsegmentswithnomatchingdocument . Finally,weshowthe\nweightedsuccessratioforeachstrategy,takinga2:1weighting(prioritizingtheexactarticlematches).\nWe use queries and metrics introduced in Section 3. We use the weighted retrieval success ratio\nwith2:1weighting,whichconsiderscategorymatchesasrelevantbutprioritizestheexactdocument\nmatches. Figure 3 shows an example query and metrics usage. Each query requires retrieving a\nspecific number of documents and the corresponding non-overlapping categories which define the\nground truth. We fetch the top k documents from a database, where k is the “total number of\ndocumentsfetchedforatestedRAGscheme”(includingpotentiallymismatches). Amongthesek\ndocuments,wesearchformatcheswiththegroundtruth.\nSamples&SummariesEachdatapointinourplotscorrespondsto25queries. Wepresentthedata\nusingstandardboxplotstoshowcasethedistribution. Ourprimaryfocusisontheaverageretrieval\nperformanceamongthose25queries.\n4.1 SUPERIORPERFORMANCEFORMULTI-ASPECTQUERIES\nWe start from the query example in Figure 3 and show first the absolute retrieval performance of\nMRAG over Standard RAG in Figure 4. We fix the number of aspects present in the queries to\n10, and vary the total number of retrieved documents from 10 to 30. MRAG consistently outper-\nformsStandardRAG(> 10%increaseintheretrievalsuccessratioonaverageforexactdocument\nmatches).Moreover,theretrievalperformanceincreaseisevenmoresignificantoncategorymatches\n(> 25%increaseintheretrievalsuccessratioonaverage). Theperformanceincreaseisfurtherde-\ntailedinthehistogramsontherightside.Here,foraspecificnumberofdocumentsfetched,MRAG’s\nhistogramindicatesabetterdistributionofretrievalsuccessratios(acrossall25queries).\nNext, Figure 5 shows the relative weighted performance improvement of MRAG with respect to\nStandard RAG as we vary the number of aspects present in the queries. We show data for two\ndifferentembeddingmodels(SFRande5). MRAGconsistentlyoutperformstheStandardRAGby\n10-20%onaverage, notonlyacrossthenumberofdocumentsfetched, butalsoacrossthenumber\nofaspectspresentinthereplies,forbothmodels.\n4.2 COMPARABLEPERFORMANCEFORSINGLE-ASPECTQUERIES\nWeadditionallyshowinTable1thatMRAGperformson-parwithStandardRAGonqueriesfrom\nourmulti-aspectdatasetwhereonlyasingleaspectisexpected. Hence,ourapproachdoesnotsuffer\nfromsignificantdecreaseinperformanceforsingle-aspecttasks.\n7\nFigure4:Retrievalsuccessratioover25queriesbetweenMRAGandStandardRAG;eachqueryuses10differentaspects.Thetoppart\npresentsexactdocumentmatches,thebottompartpresentscategoryonlymatches(weexplainthemetricsinSection3). Ahistogramis\npresentedforaspecificsampletoshowcasethedetaileddistributionamongthe25queries(thenumberofdocumentsfetchedforeachqueryis\n30).\nTable1:Retrievalsuccessratio(theexactdocumentmatch)for25querieswithasingleaspect.\nMulti-AspectDataset LegalDataset AccidentsDataset\nDocumentsFetched SFR e5 SFR SFR\nMRAGStandardRAG—MRAGStandardRAG—MRAGStandardRAG—MRAGStandardRAG\n1 24/25 25/25 24/25 25/25 24/25 24/25 25/25 25/25\n2 25/25 25/25 25/25 25/25 25/25 25/25 25/25 25/25\n3 25/25 25/25 25/25 25/25 25/25 25/25 25/25 25/25\n4.3 FURTHERIMPROVEMENTSWITHADDITIONALTOKENS\nWenowshowthatMRAGcanbeseamlesslyintegratedwithotherRAGapproaches: Wecombine\nMRAGwithFusionRAG,representingRAGschemesthatuseanLLM(additionaltokencost)for\nmoreaccurateretrieval. FusionRAGusesanLLMtocreateafixednumberofquestionsaboutthe\nRAGquery. EachquestionisseparatelyappliedthroughanembeddingmodelusingStandardRAG.\nWeapplyMRAG’sapproachtoeachofthesequestionsanddenotethecombinedschemeasFusion\nMRAG.RedplotsofFigure6showthatbothFusionRAGandFusionMRAGperformbetterthan\nStandard RAG, on average gaining 10 to 30% in accuracy. Fusion MRAG performs consistently\nbetterthanpureFusionRAG,indicatingthattheseoptimizationscanbecombinedtogether. How-\never, bothFusionstrategiesintroduceagreatervariancethanMRAGandadditionalcostsinterms\nofcompute,latency,andtokens.\n4.4 BENEFITSFROMMULTI-HEADATTENTIONSOLELY\nWe also compare MRAG to the Split RAG baseline in Figure 6. The blue plots show the relative\nweightedperformanceofMRAGandSplitRAGoverStandardRAG.MRAGperformsbetterthan\nSplit RAG, illustrating that its high accuracy is due to the actual multi-head part, and not merely\njustpartitioningthevectorandusingmultipleembeddingspaces.\nFigure5: RelativeretrievalimprovementofMRAGoverStandardRAGacrossquerieswithdifferentnumbersofaspectsanddifferent\nembeddingmodels(SFRintheleftside,e5intherightside).\n8\nFigure6:RelativeretrievalimprovementsofMRAGoverStandardRAGfortheSFRembeddingmodelcomparedwithSplitRAG(the\nblueplots),andtherelativeretrievalimprovementsofFusionMRAGoverStandardRAGcomparedwithFusionRAG(theredplots).\nFigure7:AverageimprovementoftheretrievalsuccessratioofMRAGandSplitRAGoverStandardRAGfortworeal-worldworkloads\nconstructinglegaldocuments(left)anddiscoveringcausesofindustryaccidents(right).\n4.5 REAL-WORLDWORKLOADS\nTofurtherillustrateadvantagesofMRAG,wealsoconsidertworeal-wordusecasesfromin-house\nindustrydataanalyticsprojects,namely,thesynthesisoflegaldocumentsandtheanalysisofcauses\nofchemicalplantaccidents. TheresultsareinFigure7. Intheformer(theleftside),thetaskisto\ncreateadocumentbasedonuserrequirementsthatmayberelatedtodifferentaspects,forexample\nto the law being considered (e.g., the British or the US one), the subject (e.g., energetic or civil),\nthestyleofthedocument(e.g., aggressiveormild), etc.. ThistaskisexecutedwithRAGthatcan\nfetchdocumentsfromadatabase. Inthelatter(therightside),thetaskistodiscoveracauseofan\naccident.Here,onealsowantstoretrievedocumentsfromadatabasethatshouldbeusedintheLLM\ncontexttofacilitatediscoveringthecauseoftheaccident. Thecausesaregroupedincategoriessuch\nasutilityimpactduetosevereweather,lackofpreparednessandplanning,incorrectinstallationof\nequipment, lackofmaintenance, etc.. Similarlytothepreviousanalyses, wemeasuretheretrieval\nsuccessratioovercorrespondingdatabases. MRAGoffersadvantagesoverotherschemes.\n4.6 ADDITIONALANALYSES\nWealsoanalyzetheimpactofusingembeddingsfromdifferentdecoderblocksforMRAG(instead\nofthelastone).Here,weconsidertakingmulti-aspectembeddingsfromthreedifferentlayersofthe\nembeddingmodel: afterthefirstmulti-headattentionblock,aftermulti-headattentionblock16(in\nthemiddleofthedecoderarchitecture),andthefinalmulti-headattention. Wediscoverthatthelast\nmulti-headattentionperformsthebestwhencomparedwiththeStandardRAG.\nWealsoillustrateselectedrepresentativedatafromalonginvestigationintotwoadditionalvoting\nstrategiesforMRAG.WecompareMRAG(1)whereonlytheexponentialloweringofsignificance\nof selected chunks is applied (w = 2−p), and MRAG (2) which assigns the weight for each\ni,p\ntext chunk based on the distance between the particular text chunk (d ) and the query (q) (w =\ni,p i\n1 ). Figure 8 shows that these voting strategies perform worse on average than our\ndistance(di,p,q)\nFigure8:EvaluationofdifferentvotingstrategiesforMRAGandSplitRAG.\n9\nselectedstrategyforMRAG,justifyingitsdesignandselection(describedinSection2.3). Wealso\nconsidertwovotingstrategiesforSplitRAG,tofurtherdeepentheempiricalevaluation. Split(1)\nonlyusestheexponentialloweringofsignificance(w = 2−p)andSplit(2)whichusesthesame\ni,p\nstrategy as MRAG (w = s · 2−p). Figure 8 (on the right) shows that these voting strategies\ni,p i\nare on-par with each other while being worse than MRAG, further showcasing the advantages of\nMRAG.\nThecomplexityoftheimportancescorecalculation(Algorithm1)isO(n2)wherenisthenumber\noftheembeddeddocuments; itisdominatedbycalculatingthepair-wisecosinesimilarityandthe\ncalculationofthenorm. Pleasenotethatthisstepneedstobedoneonlyonceforeachdatasetandis\nnotabottleneck.\nFinally, in addition to the performance evaluation, we also investigated the attention heads of the\nSFR-Embedding-Mistral model as well as LLaMA-2 7B model (model not fine-tuned for text-\nembeddingtasks). ThisanalysisispresentedinAppendixC.\n5 RELATED WORK\nOurworktouchesonmanyareaswhichwenowbrieflydiscuss.\nManyRAGschemesappearedrecently(Gaoetal.,2024),usingtheoutputofthelastdecoderlayer\nfor embedding generation. In contrast, MRAG leverages different embedding spaces of attention\nheads to focus on different aspects of documents and queries. As such, it can be combined with\notherschemestofurtherimproveRAGpipelines.\nRetrievalissometimesenhancedbyacross-encoderrerankingphase(Rosaetal.,2022;Nogueira\n&Cho,2020;Nogueiraetal.,2020;Lietal.,2021;Gaoetal.,2021;MacAvaneyetal.,2019). In\nsuchsolutions,typicallyafterretrievingasetofrelevantchunks,theyarere-rankedusingspecialized\nmodels. Inthiswork,wefocussolelyonthefirstretrievalphase,soMRAGcanbeseamlesslyused\ninconjunctionwithsuchcross-encoders.\nStructure-enhanced RAG schemes employ different strategies for structuring text to improve re-\ntrievalquality.AcommonideaistoconstructaKnowledgeGraphfromtext,whichenablesretrieval\namongstentitiesandrelationships(Jiangetal.,2024;Delileetal.,2024;Hussienetal.,2024;Bui\net al., 2024; Xu et al., 2024a). RAPTOR (Sarthi et al., 2024) generates multi-level summaries for\nclustersofrelatedchunks,buildingatreeofsummarieswithincreasinglevelsofabstractiontobetter\ncapture the meaning of the text. Graph RAG (Edge et al., 2024) creates a Knowledge Graph, and\nsummarizescommunitiesinthegraph,whichprovidedataatthedifferentlevelsofabstraction. All\nthesesystemstrytoimproveRAGqualitybyutilizingadditionalstructuresthatdescribeentityre-\nlationshipsortheinnerorganizationoftext. Usually,theyneedasophisticatedpreprocessingphase\ntopreparesuchstructures. MRAGachievestheimprovementsolelybasedontheembeddingmodel\nandhasnoadditionalstoragerequirements,andcanbecombinedwithanyoftheseschemes.\n6 CONCLUSION\nRetrievalAugmentedGeneration(RAG)ispivotalfordemocratizingaccesstoaccurateandrelevant\noutputsfromlargelanguagemodels(LLMs).Enhancingtheprecisionandrelevanceoftheseoutputs\nisacriticalgoal,especiallygiventhechallengesposedbyqueriesrequiringtheretrievalofmultiple\ndocumentswithsignificantlydifferentcontents. Thesecomplexqueriesarecommonacrossvarious\ndomains,butexistingRAGsolutionsstrugglebecausetheembeddingsofthenecessarydocuments\ncanbefarapartintheembeddingspace,complicatingtheirretrieval.\nTo address this gap, we introduced Multi-Head RAG (MRAG), a novel scheme that leverages the\nactivations from the multi-head attention layer of decoder models instead of the traditional feed-\nforward layer. This approach is grounded in the insight that different attention heads can capture\ndistinctaspectsofthedata.Byusingthesediverseactivations,MRAGcreatesembeddingsthatbetter\nrepresentthemultifacetednatureofdataitemsandqueries,thusenhancingtheretrievalaccuracyfor\ncomplex, multi-aspectqueries. Thesimplicityandversatilityofthisideaallowittobeseamlessly\nintegratedintoanymodernRAGpipelineordataanalyticsframework.\nOurcomprehensiveevaluationmethodology,includingspecificmetrics,syntheticdatasets,andreal-\nworld use cases, demonstrates MRAG’s effectiveness. The results indicate a significant improve-\nment in the relevance of retrieved documents, with up to 20% better performance compared to\n10\nmodernRAGbaselines. ThisvalidatesMRAG’spotentialtohandletheintricaciesofmulti-aspect\nquerieseffectively.\nMoreover,MRAGprovestobebothcost-effectiveandenergy-efficient.Itdoesnotrequireadditional\nLLM queries, multiple model instances, increased storage, or multiple inference passes over the\nembeddingmodel.Thisefficiency,combinedwiththeenhancedretrievalaccuracy,positionsMRAG\nasavaluableadvancementinthefieldofLLMsandRAGsystems. Byaddressingthechallengesof\nmulti-aspectualityinqueries,MRAGpavesthewayformorereliableandaccurateLLMapplications\nacrossdiverseindustries.\nACKNOWLEDGEMENTS\nWe thank Hussein Harake, Colin McMurtrie, Mark Klein, Angelo Mangili, and the whole CSCS\nteamgrantingaccesstotheAultandDaintmachines,andfortheirexcellenttechnicalsupport. We\nthankTimoSchneiderforhelpwithinfrastructureatSPCL.Thisprojectreceivedfundingfromthe\nEuropeanResearchCouncil(ProjectPSAP,No.101002047),andtheEuropeanHigh-Performance\nComputing Joint Undertaking (JU) under grant agreement No. 955513 (MAELSTROM). This\nproject was supported by the ETH Future Computing Laboratory (EFCL), financed by a donation\nfromHuaweiTechnologies. ThisprojectreceivedfundingfromtheEuropeanUnion’sHEresearch\nand innovation programme under the grant agreement No. 101070141 (Project GLACIATION).\nWe gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Cen-\nter: ACKCyfronetAGH)forprovidingcomputerfacilitiesandsupportwithincomputationalgrant\nno.PLG/2024/017103.\nREFERENCES\nAbdelrahman Abdallah and Adam Jatowt. Generator-Retriever-Generator Approach for Open-\nDomain Question Answering, March 2024. URL https://arxiv.org/abs/2307.11278.\narXiv:2307.11278.\nAkariAsai,ZeqiuWu,YizhongWang,AvirupSil,andHannanehHajishirzi.Self-RAG:Learningto\nRetrieve,Generate,andCritiquethroughSelf-Reflection,October2023. URLhttps://arxiv.\norg/abs/2310.11511. arXiv:2310.11511.\nTuanBui,OanhTran,PhuongNguyen,BaoHo,LongNguyen,ThangBui,andThoQuan. Cross-\nDataKnowledgeGraphConstructionforLLM-enabledEducationalQuestion-AnsweringSystem:\nACaseStudyatHCMUT.InProceedingsofthe1stACMWorkshoponAI-PoweredQ&ASystems\nforMultimedia,AIQAM’24,pp.36–43,Phuket,Thailand,June2024.AssociationforComputing\nMachinery. ISBN 9798400705472. doi: 10.1145/3643479.3662055. URL https://doi.org/\n10.1145/3643479.3662055.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. BGE M3-\nEmbedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through\nSelf-Knowledge Distillation, June 2024a. URL https://arxiv.org/abs/2402.03216.\narXiv:2402.03216.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking Large Language Models in\nRetrieval-AugmentedGeneration. ProceedingsoftheAAAIConferenceonArtificialIntelligence,\n38(16):17754–17762,March2024b.doi:10.1609/aaai.v38i16.29728.URLhttps://ojs.aaai.\norg/index.php/AAAI/article/view/29728.\nJulienDelile,SrayantaMukherjee,AntonVanPamel,andLeonidZhukov. Graph-BasedRetriever\nCapturestheLongTailofBiomedicalKnowledge, February2024. URLhttps://arxiv.org/\nabs/2402.12352. arXiv:2402.12352.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Tru-\nitt, and Jonathan Larson. From Local to Global: A Graph RAG Approach to Query-Focused\nSummarization,April2024. URLhttps://arxiv.org/abs/2404.16130. arXiv:2404.16130.\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated Eval-\nuation of Retrieval Augmented Generation, September 2023. URL https://arxiv.org/abs/\n2309.15217. arXiv:2309.15217.\n11\nLuyu Gao, Zhuyun Dai, and Jamie Callan. Rethink Training of BERT Rerankers in Multi-stage\nRetrieval Pipeline. In Advances in Information Retrieval: Proceedings of the 43rd European\nConferenceonIRResearch,PartII,ECIR’21,pp.280–286,VirtualEvent,March2021.Springer.\nISBN 978-3-030-72239-5. doi: 10.1007/978-3-030-72240-1 26. URL https://doi.org/10.\n1007/978-3-030-72240-1 26.\nYunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,Meng\nWang,andHaofenWang. Retrieval-AugmentedGenerationforLargeLanguageModels: ASur-\nvey,March2024. URLhttps://arxiv.org/abs/2312.10997. arXiv:2312.10997.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang.REALM:Retrieval-\nAugmented Language Model Pre-Training, February 2020. URL https://arxiv.org/abs/\n2002.08909. arXiv:2002.08909.\nYuchengHuandYuxingLu. RAGandRAU:ASurveyonRetrieval-AugmentedLanguageModel\nin Natural Language Processing, April 2024. URL https://arxiv.org/abs/2404.19543.\narXiv:2404.19543.\nLeiHuang, WeijiangYu, WeitaoMa, WeihongZhong, ZhangyinFeng, HaotianWang, Qianglong\nChen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in\nLarge Language Models: Principles, Taxonomy, Challenges, and Open Questions, November\n2023. URLhttps://arxiv.org/abs/2311.05232. arXiv:2311.05232.\nYizheng Huang and Jimmy Huang. A Survey on Retrieval-Augmented Text Generation\nfor Large Language Models, August 2024. URL https://arxiv.org/abs/2404.10981.\narXiv:2404.10981.\nHuggingface. Massive Text Embeddings Benchmark Leaderboard, 2024. URL https://\nhuggingface.co/spaces/mteb/leaderboard. Accessed: 2024-05-18.\nMohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Mal-\ndonado,Rube´nIzquierdo,andMiguelA´ngelSotelo. RAG-basedExplainablePredictionofRoad\nUsersBehaviorsforAutomatedDrivingusingKnowledgeGraphsandLargeLanguageModels,\nMay2024. URLhttps://arxiv.org/abs/2405.00449. arXiv:2405.00449.\nXinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang,\nHongxin Ding, Xu Chu, Junfeng Zhao, and Yasha Wang. HyKGE: A Hypothesis Knowledge\nGraph Enhanced Framework for Accurate and Reliable Medical LLMs Responses, April 2024.\nURLhttps://arxiv.org/abs/2312.15883. arXiv:2312.15883.\nPatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,\nHeinrich Ku¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, Sebastian Riedel, and Douwe\nKiela. Retrieval-AugmentedGenerationforKnowledge-IntensiveNLPTasks. InH.Larochelle,\nM.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(eds.),ProceedingsoftheThirty-fourthAnnual\nConference on Neural Information Processing Systems (NeurIPS ’20), volume 33 of Advances\ninNeuralInformationProcessingSystems,pp.9459–9474,VirtualEvent,December2020.Cur-\nran Associates. URL https://proceedings.neurips.cc/paper files/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.pdf.\nCanjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. PARADE: Passage Rep-\nresentationAggregationforDocumentReranking, June2021. URLhttps://arxiv.org/abs/\n2008.09093. arXiv:2008.09093.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A Survey on Retrieval-\nAugmented Text Generation, February 2022. URL https://arxiv.org/abs/2202.01110.\narXiv:2202.01110.\nYuanjieLyu,ZhiyuLi,SiminNiu,FeiyuXiong,BoTang,WenjinWang,HaoWu,HuanyongLiu,\nTongXu,andEnhongChen. CRUD-RAG:AComprehensiveChineseBenchmarkforRetrieval-\nAugmentedGenerationofLargeLanguageModels,July2024. URLhttps://arxiv.org/abs/\n2401.17043. arXiv:2401.17043.\n12\nSeanMacAvaney,AndrewYates,ArmanCohan,andNazliGoharian. CEDR:ContextualizedEm-\nbeddings for Document Ranking. In Proceedings of the 42nd International ACM SIGIR Con-\nference on Research and Development in Information Retrieval, SIGIR ’19, pp. 1101–1104,\nParis, France, July 2019. Association for Computing Machinery. ISBN 9781450361729. doi:\n10.1145/3331184.3331317. URLhttp://doi.org/10.1145/3331184.3331317.\nS. S. Manathunga and Y. A. Illangasekara. Retrieval Augmented Generation and Representative\nVectorSummarizationforLargeUnstructuredTextualDatainMedicalEducation,August2023.\nURLhttps://arxiv.org/abs/2308.00479. arXiv:2308.00479.\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. SFR-\nEmbedding-Mistral: Enhance Text Retrieval with Transfer Learning. Salesforce AI Research\nBlog,2024.URLhttps://blog.salesforceairesearch.com/sfr-embedded-mistral/.Ac-\ncessed: 2024-05-17.\nGre´goire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\nRobertaRaileanu, BaptisteRoziere, TimoSchick, JaneDwivedi-Yu, AsliCelikyilmaz, Edouard\nGrave,YannLeCun,andThomasScialom. AugmentedLanguageModels: ASurvey. Transac-\ntionsonMachineLearningResearch,2023. ISSN2835-8856. URLhttps://openreview.net/\nforum?id=jh7wH2AzKK. SurveyCertification.\nRodrigoNogueiraandKyunghyunCho. PassageRe-rankingwithBERT,April2020. URLhttps:\n//arxiv.org/abs/1901.04085. arXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Document Ranking with a Pretrained\nSequence-to-Sequence Model, March 2020. URL https://arxiv.org/abs/2003.06713.\narXiv:2003.06713.\nVaidehiPatil, PeterHase, andMohitBansal. CanSensitiveInformationBeDeletedFromLLMs?\nObjectivesforDefendingAgainstExtractionAttacks. InProceedingsoftheTwelfthInternational\nConference on Learning Representations, ICLR ’24, Vienna, Austria, May 2024. URL https:\n//openreview.net/forum?id=7erlRDoaV8.\nZackaryRackauckas.RAG-Fusion:ANewTakeonRetrieval-AugmentedGeneration.International\nJournal on Natural Language Computing, 13(1):37–47, February 2024. ISSN 2319-4111. doi:\n10.5121/ijnlc.2024.13103. URLhttps://doi.org/10.5121/ijnlc.2024.13103.\nGuilhermeRosa,LuizBonifacio,VitorJeronymo,HugoAbonizio,MarziehFadaee,RobertoLotufo,\nandRodrigoNogueira. InDefenseofCross-EncodersforZero-ShotRetrieval,December2022.\nURLhttps://arxiv.org/abs/2212.06121. arXiv:2212.06121.\nParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,andChristopherD.Man-\nning. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, January 2024.\nURLhttps://arxiv.org/abs/2401.18059. arXiv:2401.18059.\nWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi\nChen, and Luke Zettlemoyer. Detecting Pretraining Data from Large Language Models. In\nProceedings of the Twelfth International Conference on Learning Representations, ICLR ’24,\nVienna,Austria,May2024. URLhttps://openreview.net/forum?id=zWqr3MQuNs.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Proceedings of the\nThirty-firstAnnualConferenceonNeuralInformationProcessingSystems(NIPS’17),volume30\nofAdvancesinNeuralInformationProcessingSystems,pp.5998–6008,LongBeach,CA,USA,\nDecember2017.CurranAssociates. URLhttps://proceedings.neurips.cc/paper files/\npaper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-\njumder,andFuruWei. TextEmbeddingsbyWeakly-SupervisedContrastivePre-training,Febru-\nary2024a. URLhttps://arxiv.org/abs/2212.03533. arXiv:2212.03533.\n13\nZichong Wang, Zhibo Chu, Thang Viet Doan, Shiwen Ni, Min Yang, and Wenbin Zhang. His-\ntory,Development,andPrinciplesofLargeLanguageModels-AnIntroductorySurvey,September\n2024b. URLhttps://arxiv.org/abs/2402.06853. arXiv:2402.06853.\nChristopher Wewer, Florian Lemmerich, and Michael Cochez. Updating Embeddings for Dy-\nnamic Knowledge Graphs, September 2021. URL https://arxiv.org/abs/2109.10896.\narXiv:2109.10896.\nShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao. RetroMAE:Pre-TrainingRetrieval-oriented\nLanguage Models Via Masked Auto-Encoder. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP ’22, pp. 538–548, Abu Dhabi, United Arab Emirates, December\n2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.35. URL\nhttps://aclanthology.org/2022.emnlp-main.35.\nGuangzhiXiong, QiaoJin, ZhiyongLu, andAidongZhang. BenchmarkingRetrieval-Augmented\nGeneration for Medicine, February 2024. URL https://arxiv.org/abs/2402.13178.\narXiv:2402.13178.\nZhentaoXu,MarkJeromeCruz,MatthewGuevara,TieWang,ManasiDeshpande,XiaofengWang,\nand Zheng Li. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service\nQuestion Answering. In Proceedings of the 47th International ACM SIGIR Conference on Re-\nsearchandDevelopmentinInformationRetrieval,SIGIR’24,Washington,DC,USA,July2024a.\nAssociationforComputingMachinery.URLhttps://doi.org/10.48550/arXiv.2404.17723.\nZhipengXu,ZhenghaoLiu,YibinLiu,ChenyanXiong,YukunYan,ShuoWang,ShiYu,Zhiyuan\nLiu,andGeYu. ActiveRAG:RevealingtheTreasuresofKnowledgeviaActiveLearning,Febru-\nary2024b. URLhttps://arxiv.org/abs/2402.13547. arXiv:2402.13547.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate Limita-\ntion of Large Language Models, January 2024c. URL https://arxiv.org/abs/2401.11817.\narXiv:2401.11817.\nBiwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng.\nOnProtectingtheDataPrivacyofLargeLanguageModels(LLMs):ASurvey,March2024.URL\nhttps://arxiv.org/abs/2403.05156. arXiv:2403.05156.\nHaoYu,AoranGan,KaiZhang,ShiweiTong,QiLiu,andZhaofengLiu. EvaluationofRetrieval-\nAugmented Generation: A Survey, July 2024. URL https://arxiv.org/abs/2405.07437.\narXiv:2405.07437.\nWenhaoYu,HongmingZhang,XiaomanPan,KaixinMa,HongweiWang,andDongYu. Chain-of-\nNote: EnhancingRobustnessinRetrieval-AugmentedLanguageModels,November2023. URL\nhttps://arxiv.org/abs/2311.09210. arXiv:2311.09210.\nHuiminZeng, ZhenruiYue, QianJiang, andDongWang. FederatedRecommendationviaHybrid\nRetrieval Augmented Generation, March 2024. URL https://arxiv.org/abs/2403.04256.\narXiv:2403.04256.\nYueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,TingchenFu,XintingHuang,EnboZhao,\nYuZhang, YulongChen, Longyue Wang, AnhTuanLuu, WeiBi, FredaShi, andShuming Shi.\nSiren’sSongintheAIOcean: ASurveyonHallucinationinLargeLanguageModels,September\n2023. URLhttps://arxiv.org/abs/2309.01219. arXiv:2309.01219.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling\nYang,WentaoZhang,JieJiang,andBinCui. Retrieval-AugmentedGenerationforAI-Generated\nContent:ASurvey,June2024.URLhttps://arxiv.org/abs/2402.19473.arXiv:2402.19473.\n14\nAPPENDIX\nA MODEL DESIGN: ADDITIONAL DETAILS\nA.1 RETRIEVALSTRATEGIESFORMULTI-ASPECTDATA\nAlgorithm2Votingstrategy.\nl←[]\nforeachheadh anditsscores do\ni i\nfindbestmatchingktextchunks\nfor each chunk d with index p in top k\ni,p\ndo\nw ←s ·2−p\ni,p i\naddtuple(d ,w )tol\ni,p i,p\nendfor\nendfor\nsortlusingweightsw ;returntopkelems\ni,p\nB EVALUATION METHODOLOGY: ADDITIONAL DETAILS\nB.1 COMPUTERESOURCES\nOur experiments were executed with compute nodes containing 4x NVIDIA GH200 and a total\nmemory of 800 GB. In general one GPU with at least 40GB of memory should suffice. We used\nat most 50GB of storage and the OpenAI API as an external resource. The full experiments took\nat most three hours of GPU time and the cost for the OpenAI API were at most $15. We carried\nout additional experiments, which amounted to around 20 hours of GPU time and cost of $25 for\nthe OpenAI API. Additional evaluation was executed with a mix of compute resources including\nNVIDIAA100andV100GPUs.\nB.2 DATASETDETAILS\nTable2:Overviewofthestructureandthenumberofdocumentsintherespectivedatasets.\ndataset #categories #topics #documents total#documents\nWikipedia 25 50documentspercategory 1250\nLegalDocuments 25 25percategory 10pertopic 6250\nAccidentReports 25 25percategory 10pertopic 6250\n15\nB.3 PROMPTTEMPLATEFORTHESYNTHETICDATASETGENERATION\nTable3:Prompttemplateforquerygeneration.\nPleasecreateastoryabouttheattached<numberofarticles>articlesonthetopics<listoftitles>.\nItisveryimportantthateachoftheattachedarticlesisrelevanttothestory,inawaythatreferences\nthecontentofthearticle,notjustitstitle. Butpleasealsomentioneachtitleatleastonce. Please\nmakesurethatalloftheattachedarticlesarerelevanttoyourstory, andthateacharticleisrefer-\nencedinatleasttwosentences! Theydonotnecessarilyhavetobereferencedinthesameorder,\nbutmakesurenoarticleisforgotten.\nImportant: Outputonlythestory,noadditionaltext. Anddonotusebulletpoints,orparagraphs.\nArticles:\n———\nArticle<title>:\n<body>\n<...>\n———\nAgain,makesurethatyoureferenceallthefollowingtopicsinyourstory: <listoftitles>\nC ATTENTION HEAD ANALYSIS\nWe investigated the attention heads of two models in detail: LLaMA-2 7B and SFR-Embedding-\nMistral. We selected these two models for a detailed investigation because the former represents\nmodelsthatarenotfine-tunedfortextembeddings,whilethelatterisspecificallythetextembedding\nmodel that we used for our experiments. For each model, we looked specifically at the attention\nscoreswithineachattentionhead,i.e.,howmuchattentioneachheadpaystoeachinputtokenduring\ntheinference. Knowingthesemanticsoftheinputtokensenablesthenderivingcertainconclusions\naboutmulti-aspectualityandattentionheads.\nWeplotselectedresultsinFigure9. Eachheatmapshowsthedot-productbetweenkey-andvalue-\nprojections inside a given specified attention head, where line i of a heatmap for attention head h\nindicates the dot-products between the query-projection of token i and the key-projections of all\nprevioustokensj <i(bothmodelsusecausalattention).\n0 0 0 0\n200 200\n50 50\n400 400\n600 100 600 100\n800 800\n150 150\n1000 1000\n200 200\n0 250 500 750 1000 0 50 100 150 200 0 250 500 750 1000 0 50 100 150 200\ntoken token token token\n(a)Head4LLaMA-2 (b)Head8SFR (c)Head22LLaMA-2 (d)Head21SFR\nFigure9:HeatmapplotsforselectedattentionheadsoftheLLaMA-27BandSFR-Embedding-Mistralmodels.\nFor both models, we found out that the attention patterns vary significantly between the different\nattentionheads. Still, weencounteredtwodistinctpatterns. First, thediagonallinesinFigures9a\nand 9b indicate that, when processing a certain input token x, elevated attention is paid to some\ntokens that came a constant numbers of steps before x. We postulate that this pattern is likely\nbeneficial to understanding the overall rhythm of a natural language, allowing the model to better\nidentify which words are semantically connected, and which parts of the input text refer to each\nother. Second, horizontal and vertical lines in Figures 9c and 9d show that these heads learned to\npayattentiontospecifictokens,regardlessofhowfaraparttheyarewithintheinputsequence. An\nintuitivejustificationforsuchpatternsisthefocusoncertainsemanticaspectsoftheinputsequence.\n16\npets pets pets pets\n0.12 0.024\n0.10 0.022\n0.08 0.020\n0.06 0.018\n0.04 0.016\n0.02 0.014\n0.00 0.012\n(a)AttentionHead0LLaMA-27B (b)AttentionHead8SFR\n0.06\n0.022\n0.05 0.04 0.020\n0.03 0.018\n0.02\n0.01 0.016\n0.00\n(c)AttentionHead14LLaMA-27B (d)AttentionHead21SFR\nFigure10:AttentionscoresforselectedattentionheadsoftheLLaMA-27BandSFR-Embedding-Mistralmodels.\nWe also detail attention scores (after applying softmax) of selected heads in Figures 10 and 11,\nwhen the model is processing the last token of its input. We see that some tokens gather a lot of\nattentionfrommostheads,yetthereisalwaysaplethoraofpassageswhichareattendeddifferently\nbyanytwoattentionheads.AninterestingpatternweencounteredwasthatfortheSFR-Embedding-\nMistralmodel(seeFigure11),allheads’attentionspikedsignificantlyonthefirstline-breakinthe\ninput sequence - either positively or negatively. We conjecture that this is a consequence of how\ntheembeddingmodelwasfine-tunedanditsintendedusagepattern: embeddingqueriesareusually\nprependedwitharetrievalinstruction,whichisterminatedbyaline-break. Themodellikelylearnt\ntosummarisethenecessaryinformationaboutthisinstructioninsidetheterminatingline-break.\n0.08\n0.07\n0.06\n0.05\n0.04\n0.03\n0.02\n0.01\nFigure11:AttentionscoresforallattentionheadsoftheSFR-Embedding-Mistralmodel.\n17\nerocs-noitnetta\nerocs-noitnetta\netI e ee. , . ,t nn e e eto ty n nn t yelvt eos d dd he y w w tee e yh s nhc s st g oe er e AI a iao se i la as kr rel lt sh hh sh h hg mv ll mv gue a ii c gn ie et n eo ot lb bb due eg eida w wl aiuia et t t tat v an a uuw aeh h laaj iarv va nf emh i it te ooG fh gs ena rc yc ax e ae vr dlA a\netI e ee. , . ,t nn e e eto ty n nn t yelvt eos d dd he y w w tee e yh s nhc s st g oe er e AI a iao se i la as kr rel lt sh hh sh h hg mv ll mv gue a ii c gn ie et n eo ot lb bb due eg eida w wl aiuia et t t tat v an a uuw aeh h laaj iarv va nf emh i it te ooG fh gs ena rc yc ax e ae vr dlA a\nerocs-noitnetta\n: :a , . een yeb dy yth stt n s r manc ees rr r hhvI ce ie Du oee eha wa gr e ttwv ar duu utv p aii st rG e ge Qq qns ts l ne ae ir r K fo dnaloP evah yna\nerocs-noitnetta\nerocs-noitnetta\nlacirotsih snoitcaretni htiw rotirret\n: a ., en eb yy th stt n s ranc ees rr hvI ceeu ee ha wagr e twv ar uu tvpaii st rG e e qq ns ts le aerr\n: a ., en eb yy th stt n s ranc ees rr hvI ceeu ee ha wagr e twv ar uu tvpaii st rG e e qq ns ts le aerr\ne er ef fes t gs eo oaoe n r gh ic n Sehi o n tirwti a ug c de ifr ingis\ne: e y ?re ef f fdy ed sy se t lh gsrm ls e oao o oaoo ie nn Cr rn gh hiv cni t n pScD oe ehti i oo naat ta iwii ordu wtrr ii ahl ugt r roogQ cc e deP tn ia ts fri iriK nhe gtn isi\ne: e y ?re ef f fdy ed sy se t lh gsrm ls e oao o oaoo ie nn Cr rn gh hiv cni t n pScD oe ehti i oo naat ta iwii ordu wtrr ii ahl ugt r roogQ cc e deP tn ia ts fri iriK nhe gtn isi\ney ? rfss y edl yo o oIiC r n r irp se aoao h tc ir et lir m\ne rfs y ed yo oIrn r ir se aoah tc iet lir m\ne rfs y ed yo oIrn r ir se aoah tc iet lir m",
    "pdf_filename": "Multi-Head_RAG_Solving_Multi-Aspect_Problems_with_LLMs.pdf"
}