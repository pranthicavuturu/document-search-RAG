{
    "title": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action",
    "abstract": "v1 v1 v1 Skeleton-basedactionrecognitionhasachievedremarkable v2 v6 v3 performance with the development of graph convolutional v6 networks(GCNs). However,mostofthesemethodstendto v4 v7 v5 0 2 … reactivate constructcomplextopologylearningmechanismswhilene- v8 v9 v11 glecting the inherent symmetry of the human body. Addi- v10 v11 tionally,theuseoftemporalconvolutionswithcertainfixed v11 v11 receptive fields limits their capacity to effectively capture dependencies in time sequences. To address the issues, Figure 1. Topology reactivation with symmetry awareness. The we (1) propose a novel Topological Symmetry Enhanced correlationsbetweenv andv ,v ,v ,v inthesharedtopology 6 2 3 8 9 Graph Convolution (TSE-GC) to enable distinct topology are activated due to the scale mask derived from the correlation learningacrossdifferentchannelpartitionswhileincorpo- betweenv andv .Darkercolorsandthickerlinesstandforlarger 6 3 rating topological symmetry awareness and (2) construct weights.Bestviewedincolor. a Multi-Branch Deformable Temporal Convolution (MB- DTC)forskeleton-basedactionrecognition. Theproposed TSE-GC emphasizes the inherent symmetry of the human recognition[5,16,24,37]hasbecomeincreasinglypopular. body while enabling efficient learning of dynamic topolo- Skeleton is a type of structured data with each joint of the gies. Meanwhile,thedesignofMBDTCintroducesthecon- humanbodyidentifiedbyitsjointtype,frameindexand3D cept of deformable modeling, leading to more flexible re- position,whichshowsgreatpotentialinpreservingprivacy ceptive fields and stronger modeling capacity of temporal anddemonstratesstrongrobustnessagainstthevariationsof dependencies. CombiningTSE-GCwithMBDTC,ourfinal illumination,viewpointsandotherbackgroundchanges. model, TSE-GCN, achieves competitive performance with Inspired by the inherent structure of skeleton data, fewer parameters compared with state-of-the-art methods graph convolutional networks (GCNs) have emerged as a on three large datasets, NTU RGB+D, NTU RGB+D 120, dominant solution for skeleton-based action recognition. and NW-UCLA. On the cross-subject and cross-set evalu- STGCN[37] was the first work to encode human body as ations of NTU RGB+D 120, the accuracies of our model spatialtemporalgraphs,aggregatingfeaturesalongthenat- reach 90.0% and 91.1%, with 1.1M parameters and 1.38 ural connectivity of the human body. Since then, many GFLOPSforonestream. works[5,18,20,22,27,28]havedelvedintotheoptimiza- tion of graph convolutional mechanisms, with a focus on adaptively building topologies to effectively capture mo- 1.Introduction tion patterns. However, most of these approaches lever- age learnable adjacency matrices combined with attention Humanactionrecognitionhasattractedmuchattentiondue mechanisms and other techniques derived from dynamics to its wide range of applications[2, 13, 15, 25], including theory[35], information bottleneck theory[10], topological video surveillance, virtual reality, health care and so on. data analysis[44] and so on, neglecting or underestimating Withthedevelopmentofdepthsensors[39,42]andhuman the inherent symmetry of the human body and its poten- pose estimation methods[3, 7, 32], skeleton-based action tial role in topology learning. For instance, in the case of * Correspondingauthor. a“brushhair”action,theinteractionbetweentheheadand 1 4202 voN 91 ]VC.sc[ 1v06521.1142:viXra",
    "body": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action\nRecognition\nZeyuLiang HailunXia* NaichuanZheng HuanXu\nSchoolofInformationandCommunicationEngineering\nBeijingUniversityofPostsandTelecommunications\n{lzy sfading, xiahailun, 2022110134zhengnaichuan, xuhuan}@bupt.edu.cn\nAbstract\nv1 v1\nv1\nSkeleton-basedactionrecognitionhasachievedremarkable v2 v6 v3\nperformance with the development of graph convolutional v6\nnetworks(GCNs). However,mostofthesemethodstendto\nv4\nv7\nv5 0 2\n…\nreactivate\nconstructcomplextopologylearningmechanismswhilene- v8 v9 v11\nglecting the inherent symmetry of the human body. Addi- v10 v11\ntionally,theuseoftemporalconvolutionswithcertainfixed v11 v11\nreceptive fields limits their capacity to effectively capture\ndependencies in time sequences. To address the issues, Figure 1. Topology reactivation with symmetry awareness. The\nwe (1) propose a novel Topological Symmetry Enhanced correlationsbetweenv andv ,v ,v ,v inthesharedtopology\n6 2 3 8 9\nGraph Convolution (TSE-GC) to enable distinct topology are activated due to the scale mask derived from the correlation\nlearningacrossdifferentchannelpartitionswhileincorpo- betweenv andv .Darkercolorsandthickerlinesstandforlarger 6 3\nrating topological symmetry awareness and (2) construct weights.Bestviewedincolor.\na Multi-Branch Deformable Temporal Convolution (MB-\nDTC)forskeleton-basedactionrecognition. Theproposed\nTSE-GC emphasizes the inherent symmetry of the human recognition[5,16,24,37]hasbecomeincreasinglypopular.\nbody while enabling efficient learning of dynamic topolo- Skeleton is a type of structured data with each joint of the\ngies. Meanwhile,thedesignofMBDTCintroducesthecon- humanbodyidentifiedbyitsjointtype,frameindexand3D\ncept of deformable modeling, leading to more flexible re- position,whichshowsgreatpotentialinpreservingprivacy\nceptive fields and stronger modeling capacity of temporal anddemonstratesstrongrobustnessagainstthevariationsof\ndependencies. CombiningTSE-GCwithMBDTC,ourfinal illumination,viewpointsandotherbackgroundchanges.\nmodel, TSE-GCN, achieves competitive performance with Inspired by the inherent structure of skeleton data,\nfewer parameters compared with state-of-the-art methods graph convolutional networks (GCNs) have emerged as a\non three large datasets, NTU RGB+D, NTU RGB+D 120, dominant solution for skeleton-based action recognition.\nand NW-UCLA. On the cross-subject and cross-set evalu- STGCN[37] was the first work to encode human body as\nations of NTU RGB+D 120, the accuracies of our model spatialtemporalgraphs,aggregatingfeaturesalongthenat-\nreach 90.0% and 91.1%, with 1.1M parameters and 1.38 ural connectivity of the human body. Since then, many\nGFLOPSforonestream. works[5,18,20,22,27,28]havedelvedintotheoptimiza-\ntion of graph convolutional mechanisms, with a focus on\nadaptively building topologies to effectively capture mo-\n1.Introduction tion patterns. However, most of these approaches lever-\nage learnable adjacency matrices combined with attention\nHumanactionrecognitionhasattractedmuchattentiondue\nmechanisms and other techniques derived from dynamics\nto its wide range of applications[2, 13, 15, 25], including\ntheory[35], information bottleneck theory[10], topological\nvideo surveillance, virtual reality, health care and so on.\ndata analysis[44] and so on, neglecting or underestimating\nWiththedevelopmentofdepthsensors[39,42]andhuman\nthe inherent symmetry of the human body and its poten-\npose estimation methods[3, 7, 32], skeleton-based action\ntial role in topology learning. For instance, in the case of\n* Correspondingauthor. a“brushhair”action,theinteractionbetweentheheadand\n1\n4202\nvoN\n91\n]VC.sc[\n1v06521.1142:viXra\nthelefthandshouldbehight-lightedandlearnedalongside art methods on three large datasets for skeleton-based\nthe interaction between the head and the right hand, while actionrecognitionwithfewerparameters.\ntherolesofotherjointsmaybelesscritical.\nBased on the analysis above, we argue that integrat- 2.RelatedWork\ningtopologicalsymmetryconstraintsintotopologylearning\n2.1.Skeleton-basedActionRecognition\ncouldleadtoastraightforwardyeteffectivemechanismfor\ntopologicallearning.Inthispaper,weproposeatopological Early deep-learning methods utilized recurrent neural net-\nsymmetryenhancedgraphconvolution,namedTSE-GC,to works (RNNs)[12, 33] and convolutional neural networks\nenable distinct topology learning across different channels (CNNs)[19,31]tocaptureactionrepresentationsbyencod-\ngroups while incorporating topological symmetry aware- ing skeleton data as feature sequences or pseudo-images,\nness.Specifically,TSE-GClearnsthescalemaskofinterest but they overlooked the inherent relations between joints.\nforeachsampleandreactivatesasharedtopologyintodis- Consequently,thesemethodsfailedtoeffectivelymodelhu-\ntinctcopiesformultipleseparatechannelgroups(illustrated man body topology, limiting their overall performance. In\ninFig.1). Thescalemaskislearnedviak-nearestneighbor contrast,GCNsrepresentthehumanbodyasgraphs,where\n(k-NN) algorithm based on the Euclidean distance, where jointsaretreatedasnodesandtheirrelationsaretreatedas\ntheknearestneighborsofeachjointandthecorresponding edges. Such a design enables GCNs to effectively capture\nscalesareidentified. Duringthelearningofscalemask,we dependenciesbetweenjointsandbringsthepotentialtoun-\nalso introduce the calculated Euclidean distance as a cali- ravelcomplexmotionpatterns.\nbrationforthetopologytailoredtoeachsample. Withfew\nextra parameters introduced, our method facilitates topol- 2.2.GCNsforSkeleton-basedActionRecognition\nogy learning in a symmetric manner and effectively cap-\nTopology learning is a key focus of GCNs in the context\nturestheintricatecorrelationsbetweenjoints.\nof skeleton-based action recognition, enabling the effec-\nIn terms of feature aggregation across different frames, tive modeling of human joint correlations. The pioneer,\nmost of these approaches[5, 22, 44] tend to apply multi- STGCN[37]predefinesthephysicalstructureofthehuman\nscaletemporalconvolutiontoeffectivelycapturebothshort- body as a fixed topology. 2s-AGCN[28] introduces adap-\nrange and long-range dependencies, yet their representa- tiveness to topology learning with a self-attention mech-\ntion capacity is still limited due to essentially fixed re- anism. Since then, numerous studies[5, 10, 18, 29, 36,\nceptive fields of temporal convolution. To remedy the is- 38,44]haveaimedtocapturetheintrinsiccorrelationsbe-\nsue, we introduce the concept of deformable convolution tweenhumanjointsusinglearnableadjacencymatrices,at-\nand construct a multi-branch deformable temporal convo- tentionmechanismsorothertechniques. Amongthem,AS-\nlution, namedMBDTC,forskeleton-basedactionrecogni- GCN[20] and MS-G3D[22] leverage adjacency powering\ntion.MBDTCincorporateslearnableoffsetstothesampling formulti-scalemodeling,whichisalsoadoptedinourmeth-\nlocations of the temporal convolution filter, enabling more odstoestablishtopologicalsymmetryconstraints.\nflexiblereceptivefieldsandbetterrepresentationcapacity. A previous work, DEGCN[23], introduces deformable\nCombiningTSE-GCwithMBDTC,weconstructanovel temporalconvolutionforskeleton-basedactionrecognition.\ntopologicalsymmetryenhancedgraphconvolutionnetwork However,DEGCNappliesauniformoffsettoallframesin\n(TSE-GCN) for skeleton-based action recognition. Ex- adata-independentmanner,resultinginthesamereceptive\ntensive experimental results on NTU RGB+D[26], NTU field across different frames. In contrast, our TSE-GCN\nRGB+D120[21],andNW-UCLA[34]showthatTSE-GCN learns a unique offset for each individual frame, which is\nachieves competitive performance with fewer parameters data-dependentandenablesmoreflexiblereceptivefields.\ncomparedwithstate-of-the-artmethods.\n2.3.DeformableModeling\nOurcontributionscanbesummarizedasfollows:\n1. We propose a topological symmetry enhanced graph The concept of deformable modeling originates from\nconvolution(TSE-GC)whichfacilitatestopologylearn- CNNs, withthemaingoalofdirectingimportantlocations\ningbyincorporatingphysicalconstraintsrootedintopo- on images. As CNNs employ fixed convolutional filters,\nlogicalsymmetryandflexibility. they struggle to adapt to complex and irregular shapes in\nimages. Deformableconvolution[11]addressesthislimita-\n2. We introduce the concept of deformable modeling and\ntion by allowing the convolutional kernels to learn spatial\nconstruct a multi-branch deformable temporal convo-\noffsets,enablingthemtosamplefromnon-gridlocationsin\nlution (MBDTC) for skeleton-based action recognition\ntheinputfeaturemap. Deformable-DETR[45]buildsupon\nwithenhancedrepresentationcapacity.\nthe original DETR[4] architecture with deformable atten-\n3. Integrating TSE-GC with MBDTC, Our TSE-GCN tion mechanisms. Our MBDTC applies 1D temporal de-\nachieves competitive performance against state-of-the- formableconvolutionmoduleswithmultipledistinctorigin\n2\nxx LL\nTTeemmppoorraall FFllooww\nPPEE TTT MMM\nFFCC\nEEESSS\n-G-G-G\nTTTDDDBBB\nGGAAPP FC + Softmax\nPrediction\nCCC CCC\n...\nIInnppuutt\nInput\nθθ((11xx11)) φφ((11xx11))\ndd == --ddiiss((eeii,, eejj))22\nkk--nneeaarreesstt\nnneeiigghhbboorr CCoonnvv 11xx11 CCoonnvv 11xx11 CCoonnvv 11xx11\nttoopp--kk σσσ\nsspplliitt\nrreeaaccttiivvaattee fff DDDTTTCCC DDDTTTCCC\nrreess ooorrriiigggiiinnnaaalll dddiiilllaaatttooonnn === 111 ooorrriiigggiiinnnaaalll dddiiilllaaatttooonnn === 222\n·· BB MMaaxxPPooooll 33xx11 CCoonnvv 11xx11\nAAss\n... ...\nCCss\nconcat\nωω((11xx11))\nFigure2. ArchitectureoverviewoftheproposedTSE-GCN.PEdenotesthelearnableabsolutepositionalembedding[10]. Ldenotesthe\nnumberofstackedlayers. InTSE-GCmodule,f representsthegenerationfunctionofscalemaskwheretheindexesoftop-kneighbors\naremappedtorelevantscales. ⊗,⊙,⊕denotematrixmultiplication,element-wisemultiplication,element-wisesum,respectively. Best\nviewedincolor.\nsamplingpositions,alongwithseveralmodificationstobet- of channels. A is the adjacency matrix, with its elements\ntersuittheskeleton-basedactionrecognitionscenario. a representingthecorrelationbetweenv andv .\nij i,: j,:\nFormulationsofGraphConvolution. Withintherealmof\n3.Method\nskeleton-basedactionrecognition,thevanillagraphconvo-\nlutionproposedby[17]iswidelyadopted:\nIn this section, we first define related notations and revisit\ntheformulationsforvanillagraphconvolutionanditsvari-\nants.ThenweelaboratethedesignofourTopologicalSym- X(l+1) =σ(AˆX(l)W(l)), (1)\nt t\nmetry Enhanced Graph Convolution (TSE-GC) in Section\n3.2 and Multi-Branch Deformable Temporal Convolution\n(MBDTC)inSection3.3. TheoverallarchitectureofTSE- where X(l) ∈ RN×d(l) denotes hidden feature representa-\nt\nGCNispresentedintheend. tionofthel-thlayeratframet,Aˆ = D−1 2(A+I)D− 21 is\nthenormalizedadjacencymatrix,Disthediagonaldegree\n3.1.Preliminaries of A+I, W(l) ∈ Rd(l)×d(l+1) is learnable parameters of\nthel-thlayer,andσindicatesactivationfunction.\nNotations. InGCNs, thehumanbodywithinamotionse-\nquenceisrepresentedasaspatio-temporalgraph.Thegraph VariantsofEq. 1typicallyadoptapartitioningstrategy\nis denoted as G = (V,E), where V = {v ,v ,...,v } is withmultiplesubsetsinvolved. Forexample,ST-GCN[37]\n1 2 N\nthe set of N vertices representing joints and E is the edge dividesA+Iintoself-loop,centrifugalandcentripetalcom-\nset representingthe correlations between joints. Typically, ponents, Info-GCN[10] utilizes a multi-head self-attention\nG isformulatedbyX ∈ RN×T×C andA ∈ RN×N. Xis mechanism where each head corresponds to a subset, and\nthe feature tensor of N vertices across T frames, and v ’s AS-GCN[20]employsamulti-scaleaggregationwitheach\ni\nfeatureatframetisdenotedasx ∈RC. C isthenumber subsetrepresentingadistinctscale.Theunifiedformcanbe\ni,t\n3\nformulatedas: inFig.2. AsharedtopologyM ∈ RN×N isutilizedforall\nsubsets and reactivated as A ∈ RK×N×N. The reactiva-\nX( tl+1) =σ((cid:88) f s(Aˆ)g s(X( tl))W s(l)), (2) tioncouldbeinterpretedasaformofresampling,wherethe\nS correlationsbetweenjointsattherelevantscalesorhopsare\nsampled. We use the term reactivate to describe the char-\nwheref (·)andg (·)representmappingfunctionsforadja-\ns s\nacteristic of constraining topology learning in a symmetry\ncencymatricesandfeaturetensors,respectively.\nmanner,becausethereactivatedinteractionscanbeupdated\n3.2.TopologyLearningwithSymmetryAwareness through the training phase, while the other interactions re-\nmain unchanged. During the inference phase, only the re-\nTopological symmetry is an inherent characteristic of the\nactivatedinteractionsareutilizedforgraphfeatureaggrega-\nhumanbody,wherecorrespondinglimbsmirroreachother.\ntion. Tomaintainflexibilityoftopologymodeling,wealso\nManyphysicalactivities,suchaswalking,running,ordanc-\nintroduceadata-dependentcalibrationmatrixB ∈ RN×N,\ning, exhibit symmetrical patterns that should be captured\naswellasalearnableadjacencymatrixC ∈ RK×N×N for\nthroughtheprocessofgraphrepresentationlearning. How-\neachsubset.\never, previous works tend to employ complex graph learn-\nSpecifically, we first employ another linear embedding\ning mechanisms without explicitly incorporating topologi-\nfunction θ to embed the input feature for the subsequent\ncalsymmetryduringtopologylearning.\ntopologylearning,asinEq. 3. ThenegativesquareofEu-\nOn the other hand, most prior approaches typically uti-\nclideandistanceD∈RN×N isthencalculatedbasedonthe\nlizelearnableadjacencymatricestoadaptivelycapturecor-\nembeddingsbetweendistinctjoints,asd =−dis(e ,e )2,\nrelationsbetweenjoints,allowingthetopologytobelearned ij i j\nwheree ande representtheembeddingsofthejoints. D\nwithout any constraints based on physical bone connec- i j\nisfurtherutilizedtoobtainthecalibrationmatrixB:\ntions. In contrast, directly using the physical structure as\nthetopologyforgraphfeatureaggregation,asdonein[37],\nB=ξ(D), (6)\nyields suboptimal performance. Intuitively, a balanced ap-\nproach that incorporates both flexibility and physical con-\nwhereξ(·)istheactivationfunction.\nstraintsisworthexploring.\nBasedonthecalculatedEuclideandistance,theindexes\nBased on the analysis above, we argue that integrat- of k-nearest neighbors can be identified as IN ∈ RN×K,\ningtopologicalsymmetryconstraintsintotopologylearning\nwhich is mapped to the relevant scales or hops. Relevant\ncouldenhancethemodelingcapacityofgraphconvolution,\nscalesaremarkedas1inthescalemask. Thegenerationof\nwhichleadstoTSE-GC.ThearchitectureofourTSE-GCis\nscalemaskcanbeformulatedas:\nillustratedinthebottomleftcornerofFig.2,andtheforward\nprocesscanbedividedintotwopathways,onefortopology H=f(IN,D )=f(KNN(θ(X),K),D ), (7)\nsp sp\nlearningandoneforfeaturepartitioning.\nFeaturepartitioning. GiventheinputfeatureX∈RN×C, whereH∈RK×N×N isthescalemask,f isthegeneration\nwe first transform it into high-level representation with a function of scale mask where the indexes of top-k neigh-\nlinearembeddingfunctionψ,whichisformulatedas: borsaremappedtorelevantscales,D isamappingtable\nsp\nbasedon theshortest pathdistance (SPD).The calculation\nX˜ =ψ(X)=XW , (3)\nψ ofSPDisbasedonthephysicalconnections,whichcanbe\nformulatedas:\nwhere W ∈ RC×(C′K) is the weight matrix and X˜ ∈\nψ\nRN×(C′K) isthetransformedfeature. X˜ isfurtherdivided d = min {|P|,P =v ,P =v }, (8)\nalongthechanneldimensionC′K intoK partitions.\nspi,j\nP∈Paths(G)\n1 i |P| j\nTheentirefeaturepartitioningprocess,denotedasg (·)\ns whered representsSPDbetweennodesv andv ,and\ninEq. 2,isformulatedas: spi,j i j\nP denotestheshortestpathinthegraphG connectingthese\nX˜ =g (X)=ψ (X)=[X˜ ||X˜ ||...||X˜ ] (4) nodes.Inpractice,wecalculateitwithadjacencypowering.\ns s 0 1 K−1\nWith the scale mask, we reactivate the shared topology\nX˜ =X˜ , (5) tointroducetopologicalsymmetryconstraints:\nk :,kC′:(k+1)C′\nwhere || is concatenate function and X˜ k ∈ RN×C′ repre- A s =H s⊙M, (9)\nsentsthek-thpartitionofthetransformedfeatureX˜. Note\nthat ψ (·) represents distinct instances of ψ(·) applied to where ⊙ is element-wise multiplication operation, A ∈\ns s\ndifferentsubsets. RK×N×N is the reactivated topology. We use subscript s\nTopology learning. The topology learning pathway is de- to indicate distinct scale masks and activated topology for\ntailed in the left section of the illustrated TSE-GC shown differentsubsets.\n4\nmultiplefixedreceptivefields,whichissuboptimal. Anin-\ntuitive solution is to introduce attention mechanisms, but\ntheir computational cost can be excessively high for long\nvv ... vv\n11 1111\nsequences. Therefore,weadopttheideaofintroducingde-\nGraph readout formablemodelingandconstructourMBDTCwithflexible\nPoint-wise receptive fields for skeleton-based action recognition, en-\nConv\nablingdynamictemporalmodeling.\nDepth-wise\noffsets To begin with, we revisit the deformable convolution\nConv\nfrom[11]. Givena2Dconvolution,thesamplinglocations\naredefinedbyaregulargridR,resultinginafixedreceptive\nfield. Thedeformableconvolutionaddslearnableoffsetsto\nsample\nmakethesamplinglocationsirregular,formulatedas:\n(cid:88)\ny(p )= w(p )·x(p +p +∆p ), (13)\n0 n 0 n n\npn∈R\nwherep isalocationontheoutputfeaturemapandp is\n0 n\nFigure3.SamplingmechanisminourDTCModule.Thereceptive\nthefixedoffsetsinR.∆p representsthelearnableoffsetat\nn\nfieldsfortheframeenclosedbythereddashedlineareenclosed\neachsamplingpointp +p ,whichallowstheconvolution\nbythebluedashedlinewiththeoffsets. Weassumetheoffsetsto 0 n\ntoadaptivelyadjustthereceptivefield.\nbeintegersforclarity.Bestviewedincolor.\nFor spatio-temporal graphs in GCNs, spatial features\nare inherently structured, while temporal features are dis-\nEventually,thelearnedtopologyforgraphfeatureaggre- cretely sampled from continuous frames. Typically, only\ngationinourTSE-GCmoduleisformulatedas: correlations among the same joints across different frames\nare considered. Based on the distinct characteristic, we\nZ=f (A)=α·A +β·B+C (10) redesign the vanilla deformable convolution into our de-\ns s s\nformabletemporalconvolution(DTC)whichlearnstheoff-\nZ k =Z k,:,:, (11) sets for each frame with spatial graph feature extracted\nthrough weighted graph pooling. An example of our sam-\nwhereαandβ aretrainablescalarsthatcontrolthebalance\nbetween flexibility and physical constraints. Z ∈ RN×N pling mechanism is illustrated in Fig.3, where we insert a\nk\nreadout operation with graph pooling between depth-wise\nrepresentsthek-thpartitionofthelearnedtopology,which\nseparableconvolution.\nisobtainedbycombiningtheactivatedtopology,thelearn-\nSpecifically, we first embed the input feature X ∈\nablecompensationtermandthecalibrationterm.\nRN×T×C into Y ∈ RN×T′×C with a depth-wise convo-\nWith Eq. 4 and Eq. 11 substituted into Eq. 2, we have\nlution. The potential reduction in frames from T to T′ is\nourfinalformulaforTSE-GC:\ncausedbythestrideappliedtoourDTC.Thespatialgraph\nX(l+1) =σ((cid:88) [Z(l)X˜(l)||...||Z(l) X˜(l) ]W(l)), (12) featureoftheembeddingisthenpooled,formulatedas:\nt 0 0 K−1 K−1 s\nS\nN\n1 (cid:88)\nwhere σ(·) is the activation function. Some subscripts are Z=\nN\nw nY n, (14)\nsimplified for readability. In general, our TSE-GC intro- n=1\nducestopologicalsymmetryconstraintsforthelearningof\nwherew istheweightscalarforjointv andZ ∈ RT′×C\nA whilemaintainingacertainlevelofflexibilitywithC , n n\ns s representsthegraphreadoutsforT′ frames. Subsequently,\nleading to an effective combination of prior-knowledge-\nthe offsets are obtained with learnable parameters and Z,\nbasedconstraintsandadaptabilityfortopologylearning.\nformulatedas:\n3.3.Multi-BranchDeformableTemporalModeling P=ZW, (15)\nTemporalmodelingisessentialforcapturingcomplexmo- where P ∈ RT′×R indicates the offsets for T′ frames. R\ntiondynamicsinhumanactionrecognition. ForGCNs, an is the kernel size applied to our DTC. Notably, R is also\neffective temporal modeling module should be capable of appliedtotheprecedingdepth-wiseconvolution.\nflexibly capturing temporal dependencies, or correlations Withtheoffsetsandtheoriginalsamplinglocations,we\nwithinthetemporalgraph. Previousworks[5,22,44]typi- adoptthelinearinterpolationforsamplingandintroducea\ncallyutilizemulti-scaletemporalconvolutiontoeffectively weight matrix M ∈ RT′×R, which is obtained from the\ncapturebothshort-rangeandlong-rangedependencieswith offsets through a linear transformation. Our DTC can be\n5\nTable 1. Action classification performance on NTU RGB+D 60, NTU RGB+D 120 and NW-UCLA. †indicates methods that are not\ndirectlycomparablewithfurtherdetailsinSection4.3. *indicatestheparametersorFLOPsarerecalculatedfromtheirpubliclyavailable\ncodesforasinglestreaminNTURGB+D120,thenscaledbythenumberofstreamsforafaircomparison. TheFLOPSarecalculated\nusingthefvcorelibrary*.Weomittheresultswithoutpubliclyavailablecodesforreproduction.\nNTURGB+D60 NTURGB+D120\nMethods Publication Modalities Params(M) FLOPs(G) NW-UCLA\nX-Sub(%) X-View(%) X-Sub(%) X-Set(%)\nST-GCN[37] AAAI2018 J 3.1* 3.48* 81.5 88.3 70.7 73.2 -\n2S-AGCN[28] CVPR2019 J+B 6.8* 7.96* 88.5 95.1 82.5 84.2 -\nDC-GCN+ADG[8] ECCV2020 J+B+JM+BM 19.6 7.32 90.8 96.6 86.5 88.1 95.3\nShift-GCN[9] CVPR2020 J+B+JM+BM - - 90.7 96.5 85.9 87.6 94.6\nMS-G3D[22] CVPR2020 J+B 12.8* 28.24* 91.5 96.2 86.9 88.4 -\nMST-GCN[6] AAAI2021 J+B+JM+BM 12.0 - 91.5 96.6 87.5 88.8 -\nCTR-GCN[5] ICCV2021 J+B+JM+BM 5.6* 7.88* 92.4 96.4 88.9 90.4 96.5\nEfficientGCN-B4[30] TPAMI2022 J+B+JM+BM 8.0 - 91.7 95.7 88.3 89.1 -\nInfo-GCN[10] CVPR2022 J+B+JM+BM 6.3 6.72 92.3 96.7 89.2 90.7 96.6\nFR-Head[43] CVPR2023 J+B+JM+BM 8.0 - 92.8 96.8 89.5 90.9 96.8\nBlockGCN[44] CVPR2024 J+B+JM+BM 5.2* 8.20* 93.1 97.0 90.3 91.5 96.9\nInfo-GCN†[10] CVPR2022 - 9.4 10.08 93.0 97.1 89.8 91.2 97.0\nHD-GCN†[18] ICCV2023 - 10.1 9.60 93.4 97.2 90.1 91.6 97.2\nShap-Mix†[40] CVPR2024 J+B+JM+BM - - 93.7 97.1 90.4 91.7 -\nTSE-GCN(Ours) J 1.1 1.38 90.8 95.3 86.6 88.2 95.3\nTSE-GCN(Ours) J+B+JM+BM 4.4 5.52 92.9 96.8 90.0 91.1 96.9\nformulatedas: arefedintoastackofourbasicblocks,eachconsistingofa\nTSE-GC module, a MBDTC module and residual connec-\nR\n(cid:88) tions. L = 9, is the number of times our basic block is\nX(l+1) = W Γ(P,X(l+1),M), (16)\nR stacked. Thenumberofchannelsfornineblocksare64-64-\nr=1\n64-64-128-128-128-256-256. Following the stacked basic\nwhere Γ(·,·,·) is the sampling and reweighting function, blocks, a global average pooling operation and a softmax\nW is learnable parameters and X(l+1) ∈ RN×T′×C is classifierisappliedtogeneratepredictionsacrossdifferent\nR\ntheoutputwithaggregatedtemporalfeatures. actionclasses.\nWith our DTC module, we construct MBDTC mod-\nulefollowingthemulti-scaletemporalconvolutionmodule 4.Experiments\n(MS-TCN) adopted by [5]. The architecture of our MB-\n4.1.Datasets\nDTCisillustratedinthebottomrightcornerofFig.2,which\nisconsistsoffourbranches,witheachbranchcontaininga\nNTU RGB+D. NTU RGB+D[26] is a large-scale human\n1×1 convolution to reduce channel dimension. The first\naction recognition dataset containing 56,880 skeleton ac-\ntwo branches utilize our DTC with different original sam-\ntionsequencesover60classes. Theactionsamplesareper-\nplinglocationsdefinedbydifferentdilations,theothertwo\nformedby40participantsindifferentagegroups,andeach\nbranchesimplementamax-poolingoperationandaniden-\nsamplecontainsanactionandisguaranteedtohaveatmost\ntityfunction,respectively. Theoutputsareconcatenatedas\n2subjects.Foreachsample,theskeletondataof25jointsis\nthefinaloutput.\ncapturedbythreeMicrosoftKinectv2camerasfromdiffer-\nent views concurrently. Two benchmarks are provided by\n3.4.ModelArchitecture\ntheauthors: (1)cross-subject(X-sub): trainingdatacomes\nBased on the aforementioned TSE-GC and MBDTC, we from20subjects,andtestingdatacomesfromtheother20\nconstructatopologicalsymmetryenhancedgraphconvolu- subjects. (2) cross-view (X-view): data captured from the\ntional network TSE-GCN for skeleton-based action recog- frontandtwosideviewsisusedfortraining,anddatacap-\nnition. ThemodelarchitectureisillustratedinFig.2. Based tured from the left and right 45-degree views is used for\non experimental results, we select GeLU as the activation testing.\nfunctioninthenetwork,exceptforEq.6whereweuseTanh\nforamoresuitableoutputrangeinmodelingtopology. NTU RGB+D 120. NTU RGB+D 120[21] is an extended\nTheinputfeatureisfirstembeddedwithalineartransfor- version of NTU RGB+D with additional 57,367 skeleton\nmation and then combined with a learnable absolute posi- sequencesover60extraactionclasses,whichisoneofthe\ntionalembeddingfrom[10]. Subsequently,theembeddings largest datasets with 3D joints annotations for human ac-\n6\ntionrecognition. Totally113,945samplesover120classes and32.7%fewerFLOPs.ComparedwithFR-Head[43],the\nare performed by 106 volunteers with 32 distinct setups proposed TSE-GCN surpasses it with 45% fewer parame-\nforlocationsandbackgrounds,capturedwiththreecameras ters. Itisnoteworthythatweprioritizeabettertrade-offbe-\nviews. Theauthorsrecommendtwobenchmarks: (1)cross- tweenmodelsizeandperformanceinourhyper-parameter\nsubject(X-sub): trainingdatacomesfrom53subjects,and setting instead of accuracies, as detailed in Section 4.4.\ntesting data comes from the other 53 subjects. (2) cross- Meanwhile, our method can serve as a backbone and be\nsetup(X-set): trainingdatacomesfromsampleswitheven combined with Shap-Mix[40] or FR-Head[43] to further\nsetup IDs, and testing data comes from samples with odd improveperformance.\nsetupIDs.\n4.4.AblationStudy\nNW-UCLA. NW-UCLA[34] is a small human action\nIn this subsection, we analyze the proposed TSE-GC and\nrecognitiondatasetcontaining1494videoclipsover10ac-\nitsconfigurationsalongwithMBTDContheX-subbench-\ntion categories. Each action is performed by 10 subjects,\nmark of the NTU RGB+D 120 dataset, using joint stream\nwith the skeleton data captured by three Kinect cameras\ndata. WereconstructST-GCN[37]withourselectionofac-\nsimultaneously from multiple viewpoints. Following the\ntivation function as the baseline in our experiments, with\nevaluation protocol recommended by the authors, we use\nthe2-ndlayerremoved.\ntheviewpointsofthefirsttwocamerasfortrainingandthe\notherfortesting.\nTable 2. Comparisons of performance when adding PE and\nour TSE-GCN gradually. TSE-GC is examined through distinct\n4.2.ImplementationDetails\ntopologiestoassesstheireffectiveness. PEdenotesthelearnable\nAll experiments are conducted on two RTX 3090 GPUs absolutepositionalembedding[10]addedtotheinputfeaturebe-\nforethefirstlayerofournetwork.\nwiththePyTorchdeeplearningframework. TheSGDopti-\nmizerisemployedwithaNestrovmomentumof0.9anda\nTSE-GC\nweightdeacyof0.004forNTURGB+DandNTURGB+D Baseline MBTDC PE Params(M) Acc(%)\nC A +B\ns s\n120, and0.002forNW-UCLA.Ourmethodutilizescross-\n✓ - - - - 3.0 84.9\nentropy loss. The learning rate is initialized at 0.05 and\n✓ ✓ - - 1.2(-1.9) 85.4\nreducedbyafactorof0.1atepoch110and120,withato- ✓ ✓ ✓ - - 1.2 85.5\ntal epoch 140. For NTU datasets, we set a batch size of ✓ ✓ ✓ ✓ - 1.0 86.4\n64 with each sample resized to 64 frames and adopt the ✓ ✓ ✓ - ✓ 1.1 86.2\n✓ ✓ ✓ ✓ ✓ 1.1 86.6\ndata-processing in [41]. For NW-UCLA, the batch size is\nselectedas16andweadopthedata-processingin[9].\n4.3.ComparisonwiththeState-of-the-art Table 3. Comparisons of our TSE-GCN with different settings.\nKdenotesthenumberofpartitionsofinputfeatureandadjacency\nIn this subsection, we compare the proposed TSE-GCN\nmatrices.Rdenotestheratiooftheembeddednumberofchannels\nagainst state-of-the-art methods on NTU RGB+D, NTU\ntotheoriginal.\nRGB+D 120 and NW-UCLA in Table 1. To establish a\nfair comparison, we follow the commonly adopted four-\nK R Params(M) Acc(%)\nstreamfusionapproachinourexperiment. Specifically,we\nfusetheresultsofjoint,bone,jointmotionandbonemotion 2 4 1.3 86.4\nmodalities. Notably, some of the methods are not directly 8 1.0 86.5\ncomparabletoourmethods,whichareshowninthesecond 3 4 1.7 86.7\npart of the table, for example, HD-GCN[18] applies a six- 8 1.1 86.6\nstreamensemblewiththeirhand-craftedmodalities,aswell 16 0.9 86.5\nasInfo-GCN[10],andShap-Mix[40]utilizestheadditional\n4 8 1.3 86.5\nmixed data from their data augmentation approach which\n5 8 1.5 86.3\ndoubles the number of samples. The performance of Info-\nGCNinthefirstpartisfromthereproductionfrom[14]for\nafaircomparison,following[44]. Effectiveness of MBDTC and TSE-GC. We examine\nExperimentalresultsdemonstratethatourmethod,TSE- the effectiveness of each component of our TSE-GCN by\nGCN achieves state-of-the-art performance on the com- addingthemtothebaselinegradually. Theexperimentalre-\nmon benchmarks. Compared with the state-of-the-art sults are shown in Table 2. First, we replace the original\nBlockGCN[44], the proposed TSE-GCN achieves 0.3% TCN with our MBTDC, resulting in a 0.5% improvement\nlower accuracies on average with 15.4% fewer parameters in accuracy while reducing the number of parameters by\n7\n4.5.PerformanceAnalysisonSingleStream\n\u0000K \u0000S\u0000L \u0000X\u0000W \u0000Q\u0000\u0003\u0000R \u0000F\u0000W \u0000K\u0000K \u0000L\u0000H \u0000Q\u0000U \u0000J\u0000\u0010 \u0000\u0003\u0000\u0013 \u0000S \u0000S\u0000\u0012\u0000V\u0000\u0011 \u0000H \u0000X\u0000\u001b \u0000O \u0000\u0010\u0000\u0010\u0000\u0010 \u0000\u0010\u0000U \u0000D \u0000W\u0000\u0010 \u0000\u0013\u0000F\u0000\u001a \u0000\u0013\u0000\u0013 \u0000\u0013\u0000V \u0000\u0003\u0000\u0013 \u0000S \u0000U\u0000V \u0000W \u0000\u0011\u0000\u0011\u0000\u0011 \u0000\u0011\u0000R \u0000\u001a\u0000\u001a\u0000\u001a \u0000\u001a\u0000\u0011 \u0000K\u0000K \u0000R\u0000S\u0000\u001a\u0000Q \u0000\u0016\u0000\u0016\u0000\u0016 \u0000\u0015\u0000V\u0000H\u0000R \u0000L\u0000\u0013\u0000\u0003 \u0000Q \u0000V\u0000\u0003\u0000Z \u0000R \u0000S\u0000F \u0000\u0003 \u0000E\u0000J\u0000V \u0000K\u0000W\u0000U\u0000L \u0000D\u0000W \u0000\u0003 \u0000\u0003 \u0000U\u0000W \u0000K \u0000D\u0000D\u0000R \u0000R\u0000U \u0000\u0010\u0000O \u0000X\u0000K \u0000\u0013\u0000P\u0000Q\u0000H \u0000D\u0000W\u0000V \u0000V \u0000W\u0000D \u0000V\u0000\u0003 \u0000\u0003 \u0000K\u0000F \u0000\u0011\u0000V \u0000T\u0000W \u0000Q \u0000G\u0000V \u0000W \u0000\u0016 \u0000K\u0000U \u0000V\u0000F \u0000\u0003 \u0000U \u0000V \u0000K \u0000H\u0000R \u0000X\u0000K\u0000W \u0000G \u0000V \u0000\u0019\u0000\u0003\u0000P\u0000K \u0000V\u0000K \u0000L\u0000R\u0000\u0010 \u0000F\u0000M\u0000K\u0000R \u0000U \u0000W \u0000K\u0000P \u0000\u0003\u0000H\u0000D \u0000Q\u0000D\u0000L \u0000\u0013 \u0000V \u0000X\u0000L \u0000O\u0000L \u0000R\u0000V\u0000\u0003 \u0000\u0003\u0000G \u0000D\u0000H \u0000\u0003 \u0000Q\u0000\u0003 \u0000K \u0000R\u0000D\u0000J \u0000R \u0000W \u0000S\u0000Q \u0000J\u0000\u0011 \u0000V \u0000E \u0000P\u0000J\u0000F\u0000H \u0000\u0003 \u0000\u0014 \u0000\u0003\u0000N\u0000H \u0000S\u0000S\u0000K \u0000Q \u0000\u0003 \u0000\u0003\u0000D\u0000W \u0000\u0003\u0000G \u0000H\u0000G\u0000L\u0000W \u0000D \u0000H\u0000D \u0000I \u0000W\u0000\u001a\u0000\u0003\u0000R \u0000H\u0000U \u0000S\u0000N \u0000S\u0000S\u0000U\u0000K\u0000\u0010 \u0000H\u0000N \u0000U\u0000R \u0000H\u0000W\u0000V \u0000V\u0000U \u0000\u0003\u0000I \u0000X \u0000F \u0000V \u0000K\u0000V \u0000R\u0000\u0003 \u0000\u0003\u0000L \u0000L\u0000L \u0000H\u0000L \u0000I \u0000N\u0000Z\u0000L\u0000L \u0000P\u0000X \u0000X\u0000O \u0000Q \u0000R \u0000Q\u0000Q\u0000Q\u0000H\u0000F \u0000L\u0000F\u0000Y \u0000W\u0000Q\u0000H\u0000H \u0000H\u0000V\u0000O \u0000S \u0000Q\u0000Q \u0000S\u0000N \u0000K\u0000K \u0000J \u0000J\u0000J\u0000J\u0000H \u0000U\u0000V\u0000V \u0000W \u0000W \u0000W\u0000I \u0000E\u0000\u000e\u0000\u000e\u0000U\u0000Q\u0000X\u0000W\u0000U\u0000E\u0000\u000e\u0000I \u0000R\u0000L \u0000X\u0000X \u0000U\u0000R\u0000V\u0000X \u0000\u0013\u0000\u0013\u0000\u0013\u0000Q \u0000X \u0000H \u0000X\u0000E\u0000Q \u0000G \u0000\u0011\u0000\u0011\u0000\u0011 \u0000W\u0000J \u0000\u0013\u0000\u0013\u0000\u0013 \u0000\u0003\u0000F\u0000W \u0000V\u0000\u0003\u0000Q\u0000\u000e \u0000\u000e \u0000\u0003\u0000D\u0000H \u0000\u0003 \u0000W \u0000\u0013\u0000\u0013\u0000\u0013 \u0000K \u0000K \u0000K\u0000N\u0000U \u0000L \u0000Z\u0000\u0003\u0000\u0013 \u0000\u0013 \u0000I\u0000\u0003\u0000Q \u0000H \u0000L\u0000\u0010 \u0000L \u0000R\u0000\u0011 \u0000\u0011 \u0000D \u0000Q\u0000J \u0000R\u0000D\u0000F \u0000\u0014 \u0000\u0014\u0000J \u0000W \u0000Q\u0000N \u0000\u000e \u0000J\u0000X \u0000\u0003\u0000G\u0000\u001a \u0000\u001a\u0000\u0003 \u0000K \u0000K\u0000R \u0000\u0012 \u0000\u0003\u0000V \u0000\u0013\u0000H \u0000\u0012\u0000H \u0000I \u0000K\u0000D\u0000E\u0000Q\u0000V \u0000\u0011 \u0000H\u0000U\u0000\u0016 \u0000D\u0000Q\u0000V \u0000R\u0000\u0003 \u0000\u000e \u0000H\u0000\u0003\u0000W \u0000\u0019\u0000L \u0000L\u0000S \u0000G\u0000Z\u0000O\u0000K \u0000\u0013\u0000Q \u0000U\u0000L\u0000H \u0000V\u0000Q\u0000\u0011\u0000H\u0000J \u0000\u0018 \u0000U \u0000\u0003\u0000\u0003\u0000\u000e \u0000\u000e \u0000J \u0000W\u0000\u0003 \u0000V \u0000V\u0000\u0015 \u0000\u000e\u0000J \u0000R\u0000\u0003\u0000\u0013 \u0000\u0013 \u0000S \u0000R \u0000Z\u0000\u0013\u0000D \u0000J\u0000\u0011 \u0000\u0011 \u0000Q\u0000R\u0000\u0019 \u0000\u001a \u0000\u0011\u0000P \u0000H\u0000D\u0000\u000e \u0000\u001a\u0000W \u0000\n\u0000\u001c \u0000\u0013 \u0000W\u0000V \u0000U\u0000\u0015\u0000\u0013 \u0000K\u0000H \u0000\u0003 \u0000P\u0000S\u0000\u0011 \u0000H\u0000\u001b \u0000R \u0000U\u0000\u000e\u0000\u001a \u0000F\u0000\u0014 \u0000N\u0000\u0011\u0000\u0013 \u0000H\u0000\u0018 \u0000W\u0000\u000e \u0000\u000e\u0000\u0014\u0000\u0011 \u0000\u0014\u0000\u0016 \u0000\u0011\u0000\u001c \u0000\u0017\u0000\u0018 \u0000\u000e\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0016\u0000\u000e\u0000\u0015\u0000\u0011\u0000\u0014\u0000\u001c\n\u0000\u000e\u0000\u0015\u0000\u0011\u0000\u0018\u0000\u0019\n\u0000\u000e\u0000\u0016\u0000\u0011\u0000\u001b\u0000\u0016 \u0000\u000e\u0000\u0017\u0000\u0011\u0000\u0019\u0000\u001c \u0000K \u0000S\u0000L \u0000X\u0000W \u0000Q\u0000\u0003\u0000R \u0000F\u0000W \u0000K\u0000K \u0000L\u0000H \u0000Q\u0000U \u0000\u0010\u0000J\u0000\u0003 \u0000\u0014\u0000\u0010\u0000S \u0000S\u0000\u0012\u0000\u0014 \u0000\u0011\u0000V\u0000H \u0000X \u0000\u001b\u0000I \u0000O\u0000\u0011\u0000U\u0000L \u0000\u001a \u0000D \u0000\u0016\u0000W\u0000\u0010 \u0000F\u0000Q \u0000V \u0000\u0003\u0000\u0014\u0000\u0017 \u0000S \u0000U\u0000V \u0000W\u0000R\u0000J \u0000\u0011 \u0000K\u0000K \u0000R\u0000S\u0000\u0017\u0000Q\u0000H \u0000V\u0000H\u0000R \u0000L\u0000U \u0000\u0018 \u0000\u0010\u0000\u0003 \u0000Q \u0000V\u0000\u0003\u0000Z \u0000R \u0000\u0014\u0000\u0010 \u0000S\u0000\u0003 \u0000E\u0000J\u0000J \u0000K\u0000W \u0000\u0011\u0000Q\u0000L \u0000D \u0000\u0013\u0000\u0003 \u0000\u0003 \u0000U\u0000X \u0000W \u0000K \u0000D\u0000D \u0000R\u0000R \u0000O\u0000\u0010 \u0000\u001c\u0000\u0010 \u0000X\u0000K\u0000H \u0000P\u0000Q\u0000D\u0000W \u0000\u0013 \u0000\u0013\u0000G\u0000V \u0000W \u0000V\u0000\u0003\u0000V \u0000\u0003 \u0000K\u0000F \u0000T \u0000Q \u0000\u0011 \u0000\u0011\u0000G\u0000V \u0000\u0010\u0000\u0010 \u0000\u0010 \u0000\u0003\u0000W \u0000K\u0000V \u0000V\u0000\u001a \u0000\u001a\u0000U \u0000K\u0000\u0013\u0000\u0013 \u0000\u0013 \u0000V \u0000K \u0000H\u0000R \u0000X\u0000K \u0000G \u0000V\u0000L \u0000\u0003\u0000V \u0000L\u0000R \u0000\u0016 \u0000\u0010\u0000\u0015 \u0000F\u0000K\u0000\u0010\u0000\u0010\u0000Q \u0000U \u0000H\u0000W\u0000\u0011\u0000\u0011 \u0000\u0011\u0000P \u0000\u0003\u0000H\u0000D \u0000Q\u0000D\u0000L \u0000\u0013\u0000V\u0000\u0018\u0000\u0018 \u0000\u0018\u0000\u0013\u0000\u0013 \u0000L \u0000O\u0000R\u0000V \u0000\u0003 \u0000D\u0000G \u0000D \u0000\u0003\u0000J \u0000Q\u0000K \u0000D\u0000W \u0000S\u0000Q \u0000J\u0000\u0011\u0000V\u0000\u0015\u0000\u0015 \u0000\u0015 \u0000E\u0000\u0011\u0000\u0011 \u0000J\u0000H \u0000G \u0000\u0016\u0000\u0003\u0000N\u0000H\u0000\u0003 \u0000\u0016\u0000\u0016 \u0000S\u0000\u0003 \u0000\u0003\u0000D \u0000\u0003\u0000G \u0000H\u0000G\u0000J \u0000W \u0000D \u0000H\u0000D \u0000I \u0000W\u0000\u0019\u0000\u0003 \u0000\u0012\u0000H\u0000\u0018\u0000\u0018 \u0000N \u0000S\u0000U\u0000K \u0000N \u0000U \u0000E\u0000R \u0000H\u0000W\u0000D \u0000V \u0000V\u0000U \u0000\u0003 \u0000V \u0000K\u0000R\u0000\u0003 \u0000L \u0000L \u0000H\u0000L \u0000I \u0000N\u0000Z\u0000L\u0000P \u0000R\u0000P\u0000X \u0000Q \u0000R \u0000Q\u0000Q \u0000F \u0000L \u0000W\u0000Q\u0000H\u0000H \u0000Z\u0000V\u0000S \u0000Q\u0000Q\u0000N \u0000K\u0000J \u0000J\u0000J \u0000U\u0000H \u0000V \u0000W \u0000W \u0000W \u0000E\u0000K\u0000M\u0000U\u0000\u000e\u0000X\u0000W\u0000D\u0000U\u0000E\u0000F\u0000V\u0000K \u0000X\u0000R\u0000W \u0000X\u0000X\u0000U \u0000U \u0000U\u0000R\u0000V\u0000X\u0000L \u0000\u0013\u0000U \u0000\u000e \u0000X\u0000J \u0000R \u0000P\u0000P \u0000H \u0000X\u0000E\u0000Q \u0000S\u0000\u0011\u0000W\u0000H \u0000\u0013\u0000\u0013\u0000K \u0000V \u0000\u0003\u0000F\u0000W\u0000\u000e \u0000V\u0000\u0003\u0000Q \u0000\u000e\u0000W \u0000S\u0000D \u0000S\u0000V \u0000\u0003\u0000\u0003 \u0000W\u0000\u0013\u0000K\u0000\u0011\u0000\u0010 \u0000F \u0000K\u0000F\u0000N \u0000\u0014\u0000\u0013 \u0000L \u0000Z\u0000\u0003\u0000\u0003 \u0000\u0013 \u0000L\u0000I \u0000\u0003\u0000K \u0000I\u0000\u0003\u0000Q\u0000W \u0000Q\u0000L \u0000L\u0000X\u0000L \u0000\u001a\u0000L\u0000\u0011 \u0000R \u0000\u0011\u0000D\u0000R \u0000U \u0000Q\u0000Y\u0000\u0016 \u0000R\u0000F \u0000\u0016\u0000\u0003 \u0000J \u0000J\u0000S\u0000W\u0000F\u0000R \u0000\u000e \u0000Q\u0000H\u0000N\u0000H \u0000J\u0000\u0018 \u0000\u0003\u0000\u0019\u0000\u0003 \u0000K \u0000K\u0000O\u0000\u0003\u0000Q \u0000R \u0000\u0013 \u0000\u0012 \u0000\u0003\u0000V \u0000H\u0000W \u0000H \u0000I \u0000K\u0000D\u0000Q\u0000H \u0000\u0011\u0000R \u0000V \u0000H\u0000U\u0000\u001a\u0000\u000e \u0000D\u0000Q\u0000V \u0000\u0003\u0000X \u0000\u000e\u0000H\u0000\u0003\u0000\u0015\u0000W\u0000H \u0000L\u0000S\u0000\u0014 \u0000G\u0000O\u0000F \u0000K \u0000\u0014\u0000U\u0000O \u0000L\u0000\u0011 \u0000H\u0000K \u0000V\u0000I \u0000Q \u0000\u0011\u0000H\u0000\u0013 \u0000\u0014\u0000U \u0000\u0003\u0000\u0003 \u0000J\u0000\u0018 \u0000W\u0000V \u0000V \u0000\u0013\u0000R\u0000\u000e \u0000\u0003\u0000S \u0000R \u0000Z \u0000J\u0000\u0014 \u0000Q\u0000R \u0000H\u0000D\u0000\u0011 \u0000\u000e\u0000W \u0000\n\u0000\u0018 \u0000W\u0000V \u0000U \u0000\u0014 \u0000K\u0000\u001a \u0000\u0003 \u0000P\u0000S \u0000\u0011 \u0000H\u0000\u000e \u0000\u001b\u0000R \u0000U\u0000\u0015 \u0000\u0014\u0000F\u0000\u0011 \u0000N\u0000\u0013\u0000\u001c \u0000H\u0000W\u0000\u000e\u0000\u0015\u0000\u000e \u0000\u0011\u0000\u0015 \u0000\u0019\u0000\u0011 \u0000\u0015\u0000\u001a\u0000\u001b \u0000\u000e \u0000\u000e\u0000\u0017 \u0000\u0017\u0000\u0011 \u0000\u0011\u0000\u0013 \u0000\u0013\u0000\u0014\n\u0000\u0016\n\u0000\u000e\u0000\u001c\u0000\u0011\u0000\u001a\u0000\u0017 I T p thn eS etEt i la t o- ib vG wl ee C ep sN4 e t, r cfww oo mrie t mh pc a uo o n tm t ach tep e ioora nr nGe aN lCs cTi Nn oUg s m.l Re p- O G ls et u xBr r ie + ta ym Dm .o 1dp 2ee 0lrf ja oo c ir h nm i tea svn te rc ese aa mo cf woo m iu th-r\nTable4.ComparisonsofGCNsonsinglestream.\n(a)ComparisonwithST-GC. (b)ComparisonwithCTR-GC.\nFigure4. Accuracydifference(%)betweenTSE-GCandtworep- Methods X-Sub(%) X-Set(%) Params(M) FLOPs(G)\nresentativeGCsonsymmetryrelatedclassesgeneratedbyGPT4 CTR-GCN 84.9 86.5 1.5 1.97\n[1].Greenbarsindicateimprovements. Info-GCN 85.1 86.3 1.6 1.68\nHD-GCN 85.7 87.3 1.7 1.60\nFR-Head 85.5 87.3 2.0 -\nBlockGCN 86.9 88.2 1.3 2.05\n1.9M.Thisenhancementdemonstratestheeffectivenessof\nTSE-GCN 86.6 88.2 1.1 1.38\nMBTDC. The introduction of PE marginally increases the\nparameter count with a slight improvement of 0.1% in ac-\nWe further analyze the accuracies of certain classes to\ncuracy.\nconfirm that the experimental results align with the in-\nWe then validate the effect of our TSE-GC by decou- tended design to incorporate topological symmetry aware-\npling it into separate learned topologies C s and A s +B. ness. Specifically, we utilize GPT4[1] to identify symme-\nForexample,C sindicatesasettingofourTSE-GCwithout try related classes within NTU RGB+D 120, as the corre-\nthelearningofA s +B. A s +Bisnotfurtherdecoupled lationbetweencertainactionsandtopologicalsymmetryis\nbecausethelearningofBcontributestothegradientsofthe abstract and hard to be defined by human. The final list\nembedding function at the initial stages of topology learn- contains 27 classes, including clapping, hands up, cross\ningpathway. ThefullyflexiblesettingofourTSE-GC(C s) arms and so on. We calculate the accuracy difference of\nimproves the performance by 0.9%, while the constrained theseclassesbetweenTSE-GCandtworepresentativeGCs\nsetting(A s+B)yieldsanimprovementof0.7%. Byinte- in Fig.4. We select ST-GC and CTR-GC as the topology\ngratingboth,ourTSE-GCoutperformthebaselineby1.7% of ST-GC is fully physical, while the topology of CTR-\nwhilereducing63%ofitsparameters. GCisnearlyfullyflexibleduetothecatastrophicforgetting\nofskeletaltopology[44]. Experimentalresultsdemonstrate\nConfigurationsofTSE-GC.Weexploredifferentconfigu-\nthat TSE-GC shows an average improvement of +0.6%\nrationsofourTSE-GC,specificallythenumberofpartitions\ncompared to ST-GC and +0.8% compared to CTR-GC on\nK andthereductionrateRofembeddingfunctionψandθ.\ntheseclasses,whichcorrespondstotheirdistincttopologies.\nSinceKalsorepresentsdifferentscaleswithinTSE-GC,we\nMeanwhile, overall performance improvement is observed\nexaminesettingsofK = 2,3,4,5toanalyzeitsimpacton\nin both cases, with a few classes showing gains exceeding\nperformance. When K = 1, A falls into I as we do not\ns\ntheaverage,suchas+4.69%for“shakefist”inST-GCcase\nexcludethejointitselfinourKNNimplementationtopre-\nand+9.74%for“hitotherpersonwithsomething”inCTR-\nserveself-connection. WhenK = 7andeachneighboring\nGCcase.\njoint represents a different scale, A functions as a fully\ns\nconnectedlayeronNTUdatasets.\n5.Conclusion\nAs shown in Table 3, our TSE-GC consistently outper-\nforms the baseline under all configurations, demonstrating In this work, we propose a novel topological symme-\nitsrobustnesstodiversesettings. Bycomparingtheexperi- try enhanced graph convolution network (TSE-GCN) for\nmentalresultsofK =2,3,4,5,wefindourmodelsachieve skeleton-based action recognition. Our TSE-GC module\nbest results when K = 3. We argue that fewer partitions leverages the inherent topological symmetry of the human\nleadtoaninsufficientmodelingwhilemorepartitionsintro- body to achieve an effective and balanced topology learn-\nduceredundancy. AsforthesettingofR,alowervalueof ingbetweenflexibilityandphysicalconstraints. Ourmulti-\nRleadstomoreadjacencymatricesandadditionalparame- branch deformable temporal convolution (MBTDC) mod-\nters,therebyincreasingtheoverallcomputationalcomplex- uleincorporatesdeformablemodelingtoenableflexiblere-\nity. OuroptimalperformancecanbeachievedwithK = 3 ceptivefields.TSE-GCNachievescompetitiveperformance\nandR = 4. However,inordertobalanceperformanceand compared with state-of-the-art methods on three datasets,\nefficiency, weselectK = 3andR = 8asthefinalchoice with the effectiveness of each component validated. Our\nofhyperparametersforTSE-GC. futureresearchmayproceedintwomaindirections:\n8\n(1) Efficient cross-spacetime feature aggregation. With the IEEE/CVF conference on computer vision and pattern\nflexible receptive fields in deformable temporal convolu- recognition,pages183–192,2020. 6,7\ntion, it is possible to achieve a learnable cross-spacetime [10] Hyung-gun Chi, Myoung Hoon Ha, Seunggeun Chi,\nfeatureaggregation, leadingtoaunifiedandpowerfulrep- Sang Wan Lee, Qixing Huang, and Karthik Ramani. In-\nresentationforGCNs. fogcn: Representation learning for human skeleton-based\n(2)Redundancyreductioningraphconvolution. Someac- action recognition. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\ntions can be identified with only topological symmetric\n20186–20196,2022. 1,2,3,6,7\njoints, such as “clapping” and “hands up”. Removing re-\ndundantcorrelationsandpreservinginformativeonescould [11] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang,HanHu,andYichenWei. Deformableconvolutional\nenhanceefficiencyandfacilitatepracticalapplications.\nnetworks. InProceedingsoftheIEEEinternationalconfer-\nenceoncomputervision,pages764–773,2017. 2,5\nReferences\n[12] YongDu,WeiWang,andLiangWang. Hierarchicalrecur-\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah- rentneuralnetworkforskeletonbasedactionrecognition.In\nmad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida, ProceedingsoftheIEEEconferenceoncomputervisionand\nJankoAltenschmidt, SamAltman, ShyamalAnadkat, etal. patternrecognition,pages1110–1118,2015. 2\nGpt-4 technical report. arXiv preprint arXiv:2303.08774, [13] Amr Elkholy, Mohamed E Hussein, Walid Gomaa, Dima\n2023. 8 Damen,andEmmanuelSaba. Efficientandrobustskeleton-\n[2] Tamas Bates, Karinne Ramirez-Amaro, Tetsunari Inamura, based quality assessment and abnormality detection in hu-\nand Gordon Cheng. On-line simultaneous learning and man action performance. IEEE journal of biomedical and\nrecognition of everyday activities from virtual reality per- healthinformatics,24(1):280–291,2019. 1\nformances. In2017IEEE/RSJInternationalConferenceon [14] Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng,\nIntelligent Robots and Systems (IROS), pages 3510–3515. Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang,\nIEEE,2017. 1 Wenyu Liu, and Bin Feng. Graph contrastive learn-\n[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. ing for skeleton-based action recognition. arXiv preprint\nRealtimemulti-person2dposeestimationusingpartaffinity arXiv:2301.10900,2023. 7\nfields. InProceedingsoftheIEEEconferenceoncomputer\n[15] Yu-Gang Jiang, Qi Dai, Wei Liu, Xiangyang Xue, and\nvisionandpatternrecognition,pages7291–7299,2017. 1\nChong-Wah Ngo. Human action recognition in uncon-\n[4] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas strained videos by explicit motion modeling. IEEE Trans-\nUsunier,AlexanderKirillov,andSergeyZagoruyko.End-to- actionsonImageProcessing,24(11):3781–3795,2015. 1\nendobjectdetectionwithtransformers. InEuropeanconfer-\n[16] HyunjikKimandAndriyMnih. Disentanglingbyfactoris-\nenceoncomputervision,pages213–229.Springer,2020. 2\ning. InInternationalconferenceonmachinelearning,pages\n[5] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying\n2649–2658.PMLR,2018. 1\nDeng,andWeimingHu. Channel-wisetopologyrefinement\n[17] ThomasNKipfandMaxWelling. Semi-supervisedclassi-\ngraphconvolutionforskeleton-basedactionrecognition. In\nficationwithgraphconvolutionalnetworks. arXivpreprint\nProceedings of the IEEE/CVF international conference on\narXiv:1609.02907,2016. 3\ncomputervision,pages13359–13368,2021. 1,2,5,6\n[18] Jungho Lee, Minhyeok Lee, Dogyoon Lee, and Sangyoun\n[6] ZhanChen,SichengLi,BingYang,QinghanLi,andHong\nLee. Hierarchically decomposed graph convolutional net-\nLiu. Multi-scale spatial temporal graph convolutional net-\nworksforskeleton-basedactionrecognition. InProceedings\nwork for skeleton-based action recognition. In Proceed-\noftheIEEE/CVFInternationalConferenceonComputerVi-\ningsoftheAAAIconferenceonartificialintelligence,pages\nsion,pages10444–10453,2023. 1,2,6,7\n1113–1122,2021. 6\n[7] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, [19] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu.\nThomas S Huang, and Lei Zhang. Higherhrnet: Scale- Skeleton-basedactionrecognitionwithconvolutionalneural\nawarerepresentationlearningforbottom-uphumanposees- networks. In2017IEEEinternationalconferenceonmulti-\ntimation. In Proceedings of the IEEE/CVF conference on media&expoworkshops(ICMEW),pages597–600.IEEE,\ncomputervisionandpatternrecognition,pages5386–5395, 2017. 2\n2020. 1 [20] Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng\n[8] KeCheng,YifanZhang,CongqiCao,LeiShi,JianCheng, Wang,andQiTian. Actional-structuralgraphconvolutional\nand Hanqing Lu. Decoupling gcn with dropgraph module networksforskeleton-basedactionrecognition. InProceed-\nforskeleton-basedactionrecognition. InComputerVision– ings of the IEEE/CVF conference on computer vision and\nECCV2020: 16thEuropeanConference,Glasgow,UK,Au- patternrecognition,pages3595–3603,2019. 1,2,3\ngust 23–28, 2020, Proceedings, Part XXIV 16, pages 536– [21] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,\n553.Springer,2020. 6 Ling-YuDuan, andAlexCKot. Nturgb+d120: Alarge-\n[9] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian scalebenchmarkfor3dhumanactivityunderstanding.IEEE\nCheng,andHanqingLu. Skeleton-basedactionrecognition transactions on pattern analysis and machine intelligence,\nwith shift graph convolutional network. In Proceedings of 42(10):2684–2701,2019. 2,6\n9\n[22] ZiyuLiu,HongwenZhang,ZhenghaoChen,ZhiyongWang, [35] Xinghan Wang, Xin Xu, and Yadong Mu. Neural koop-\nandWanliOuyang.Disentanglingandunifyinggraphconvo- man pooling: Control-inspired temporal dynamics encod-\nlutionsforskeleton-basedactionrecognition.InProceedings ingforskeleton-basedactionrecognition. InProceedingsof\noftheIEEE/CVFconferenceoncomputervisionandpattern theIEEE/CVFConferenceonComputerVisionandPattern\nrecognition,pages143–152,2020. 1,2,5,6 Recognition,pages10597–10607,2023. 1\n[23] Woomin Myung, Nan Su, Jing-Hao Xue, and Guijin [36] Haojun Xu, Yan Gao, Zheng Hui, Jie Li, and Xinbo\nWang. Degcn: Deformable graph convolutional networks Gao. Language knowledge-assisted representation learn-\nforskeleton-basedactionrecognition.IEEETransactionson ing for skeleton-based action recognition. arXiv preprint\nImageProcessing,33:2477–2490,2024. 2 arXiv:2305.12398,2023. 2\n[24] Chiara Plizzari, Marco Cannici, and Matteo Matteucci. [37] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nSkeleton-based action recognition via spatial and temporal ral graph convolutional networks for skeleton-based action\ntransformernetworks. ComputerVisionandImageUnder- recognition. InProceedingsoftheAAAIconferenceonarti-\nstanding,208:103219,2021. 1 ficialintelligence,2018. 1,2,3,4,6,7\n[38] FanfanYe,ShiliangPu,QiaoyongZhong,ChaoLi,DiXie,\n[25] Ronald Poppe. A survey on vision-based human action\nandHuimingTang. Dynamicgcn: Context-enrichedtopol-\nrecognition. Image and vision computing, 28(6):976–990,\nogylearningforskeleton-basedactionrecognition. InPro-\n2010. 1\nceedingsofthe28thACMinternationalconferenceonmul-\n[26] AmirShahroudy,JunLiu,Tian-TsongNg,andGangWang.\ntimedia,pages55–63,2020. 2\nNturgb+d:Alargescaledatasetfor3dhumanactivityanal-\n[39] Ling-FungYeung,ZhenqunYang,KennethChik-ChiCheng,\nysis. In Proceedings of the IEEE conference on computer\nDan Du, and Raymond Kai-Yu Tong. Effects of camera\nvisionandpatternrecognition,pages1010–1019,2016.2,6\nviewing angles on tracking kinematic gait patterns using\n[27] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\nazurekinect,kinectv2andorbbecastraprov2. Gait&pos-\nSkeleton-basedactionrecognitionwithdirectedgraphneu-\nture,87:19–26,2021. 1\nralnetworks.InProceedingsoftheIEEE/CVFconferenceon\n[40] Jiahang Zhang, Lilang Lin, and Jiaying Liu. Shap-mix:\ncomputervisionandpatternrecognition,pages7912–7921,\nShapleyvalueguidedmixingforlong-tailedskeletonbased\n2019. 1\nactionrecognition. arXivpreprintarXiv:2407.12312,2024.\n[28] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two- 6,7\nstreamadaptivegraphconvolutionalnetworksforskeleton-\n[41] Pengfei Zhang, Cuiling Lan, Wenjun Zeng, Junliang Xing,\nbasedactionrecognition. InProceedingsoftheIEEE/CVF\nJianru Xue, and Nanning Zheng. Semantics-guided neural\nconference on computer vision and pattern recognition,\nnetworks for efficient skeleton-based human action recog-\npages12026–12035,2019. 1,2,6\nnition. In proceedings of the IEEE/CVF conference on\n[29] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. computervisionandpatternrecognition,pages1112–1121,\nSkeleton-based action recognition with multi-stream adap- 2020. 7\ntive graph convolutional networks. IEEE Transactions on [42] Zhengyou Zhang. Microsoft kinect sensor and its effect.\nImageProcessing,29:9532–9545,2020. 2 IEEEmultimedia,19(2):4–10,2012. 1\n[30] Yi-FanSong,ZhangZhang,CaifengShan,andLiangWang. [43] Huanyu Zhou, Qingjie Liu, and Yunhong Wang. Learn-\nConstructingstrongerandfasterbaselinesforskeleton-based ingdiscriminativerepresentationsforskeletonbasedaction\naction recognition. IEEE transactions on pattern analysis recognition. In Proceedings of the IEEE/CVF Conference\nandmachineintelligence,45(2):1474–1488,2022. 6 onComputerVisionandPatternRecognition,pages10608–\n[31] TaeSooKimandAustinReiter. Interpretable3dhumanac- 10617,2023. 6,7\ntionanalysiswithtemporalconvolutionalnetworks. InPro- [44] YuxuanZhou,XudongYan,Zhi-QiCheng,YanYan,QiDai,\nceedingsoftheIEEEconferenceoncomputervisionandpat- andXian-ShengHua. Blockgcn: Redefinetopologyaware-\nternrecognitionworkshops,pages20–28,2017. 2 nessforskeleton-basedactionrecognition.InProceedingsof\n[32] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep theIEEE/CVFConferenceonComputerVisionandPattern\nhigh-resolution representation learning for human pose es- Recognition,pages2049–2058,2024. 1,2,5,6,7,8\ntimation. In Proceedings of the IEEE/CVF conference on [45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\ncomputervisionandpatternrecognition,pages5693–5703, Wang,andJifengDai. Deformabledetr: Deformabletrans-\n2019. 1 formers for end-to-end object detection. arXiv preprint\narXiv:2010.04159,2020. 2\n[33] Hongsong Wang and Liang Wang. Modeling temporal\ndynamics and spatial configurations of actions using two-\nstream recurrent neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion,pages499–508,2017. 2\n[34] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-\nChunZhu. Cross-viewactionmodeling,learningandrecog-\nnition. InProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition,pages2649–2656,2014.2,7\n10",
    "pdf_filename": "Topological_Symmetry_Enhanced_Graph_Convolution_for_Skeleton-Based_Action_Recognition.pdf"
}