{
    "title": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action Recognition",
    "context": "Skeleton-based action recognition has achieved remarkable performance with the development of graph convolutional networks (GCNs). However, most of these methods tend to construct complex topology learning mechanisms while ne- glecting the inherent symmetry of the human body. Addi- tionally, the use of temporal convolutions with certain fixed receptive fields limits their capacity to effectively capture dependencies in time sequences. To address the issues, we (1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to enable distinct topology learning across different channel partitions while incorpo- rating topological symmetry awareness and (2) construct a Multi-Branch Deformable Temporal Convolution (MB- DTC) for skeleton-based action recognition. The proposed TSE-GC emphasizes the inherent symmetry of the human body while enabling efficient learning of dynamic topolo- gies. Meanwhile, the design of MBDTC introduces the con- cept of deformable modeling, leading to more flexible re- ceptive fields and stronger modeling capacity of temporal dependencies. Combining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive performance with fewer parameters compared with state-of-the-art methods on three large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the cross-subject and cross-set evalu- ations of NTU RGB+D 120, the accuracies of our model reach 90.0% and 91.1%, with 1.1M parameters and 1.38 GFLOPS for one stream. Human action recognition has attracted much attention due to its wide range of applications[2, 13, 15, 25], including video surveillance, virtual reality, health care and so on. With the development of depth sensors[39, 42] and human pose estimation methods[3, 7, 32], skeleton-based action * Corresponding author. v1 v6 v2 v3 v4 v5 v7 v8 v9 v10 v11 v1 v11 v11 v1 reactivate 0 2 … v6 v11 Figure 1. Topology reactivation with symmetry awareness. The correlations between v6 and v2, v3, v8, v9 in the shared topology are activated due to the scale mask derived from the correlation between v6 and v3. Darker colors and thicker lines stand for larger weights. Best viewed in color. recognition[5, 16, 24, 37] has become increasingly popular. Skeleton is a type of structured data with each joint of the human body identified by its joint type, frame index and 3D position, which shows great potential in preserving privacy and demonstrates strong robustness against the variations of illumination, viewpoints and other background changes. Inspired by the inherent structure of skeleton data, graph convolutional networks (GCNs) have emerged as a dominant solution for skeleton-based action recognition. STGCN[37] was the first work to encode human body as spatial temporal graphs, aggregating features along the nat- ural connectivity of the human body. Since then, many works[5, 18, 20, 22, 27, 28] have delved into the optimiza- tion of graph convolutional mechanisms, with a focus on adaptively building topologies to effectively capture mo- tion patterns. However, most of these approaches lever- age learnable adjacency matrices combined with attention mechanisms and other techniques derived from dynamics theory[35], information bottleneck theory[10], topological data analysis[44] and so on, neglecting or underestimating the inherent symmetry of the human body and its poten- tial role in topology learning. For instance, in the case of a “brush hair” action, the interaction between the head and 1 arXiv:2411.12560v1  [cs.CV]  19 Nov 2024",
    "body": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action\nRecognition\nZeyu Liang\nHailun Xia* Naichuan Zheng\nHuan Xu\nSchool of Information and Communication Engineering\nBeijing University of Posts and Telecommunications\n{lzy sfading, xiahailun, 2022110134zhengnaichuan, xuhuan}@bupt.edu.cn\nAbstract\nSkeleton-based action recognition has achieved remarkable\nperformance with the development of graph convolutional\nnetworks (GCNs). However, most of these methods tend to\nconstruct complex topology learning mechanisms while ne-\nglecting the inherent symmetry of the human body. Addi-\ntionally, the use of temporal convolutions with certain fixed\nreceptive fields limits their capacity to effectively capture\ndependencies in time sequences.\nTo address the issues,\nwe (1) propose a novel Topological Symmetry Enhanced\nGraph Convolution (TSE-GC) to enable distinct topology\nlearning across different channel partitions while incorpo-\nrating topological symmetry awareness and (2) construct\na Multi-Branch Deformable Temporal Convolution (MB-\nDTC) for skeleton-based action recognition. The proposed\nTSE-GC emphasizes the inherent symmetry of the human\nbody while enabling efficient learning of dynamic topolo-\ngies. Meanwhile, the design of MBDTC introduces the con-\ncept of deformable modeling, leading to more flexible re-\nceptive fields and stronger modeling capacity of temporal\ndependencies. Combining TSE-GC with MBDTC, our final\nmodel, TSE-GCN, achieves competitive performance with\nfewer parameters compared with state-of-the-art methods\non three large datasets, NTU RGB+D, NTU RGB+D 120,\nand NW-UCLA. On the cross-subject and cross-set evalu-\nations of NTU RGB+D 120, the accuracies of our model\nreach 90.0% and 91.1%, with 1.1M parameters and 1.38\nGFLOPS for one stream.\n1. Introduction\nHuman action recognition has attracted much attention due\nto its wide range of applications[2, 13, 15, 25], including\nvideo surveillance, virtual reality, health care and so on.\nWith the development of depth sensors[39, 42] and human\npose estimation methods[3, 7, 32], skeleton-based action\n*\nCorresponding author.\nv1\nv6\nv2\nv3\nv4\nv5\nv7\nv8\nv9\nv10\nv11\nv1\nv11\nv11\nv1\nreactivate\n0\n2\n…\nv6\nv11\nFigure 1. Topology reactivation with symmetry awareness. The\ncorrelations between v6 and v2, v3, v8, v9 in the shared topology\nare activated due to the scale mask derived from the correlation\nbetween v6 and v3. Darker colors and thicker lines stand for larger\nweights. Best viewed in color.\nrecognition[5, 16, 24, 37] has become increasingly popular.\nSkeleton is a type of structured data with each joint of the\nhuman body identified by its joint type, frame index and 3D\nposition, which shows great potential in preserving privacy\nand demonstrates strong robustness against the variations of\nillumination, viewpoints and other background changes.\nInspired by the inherent structure of skeleton data,\ngraph convolutional networks (GCNs) have emerged as a\ndominant solution for skeleton-based action recognition.\nSTGCN[37] was the first work to encode human body as\nspatial temporal graphs, aggregating features along the nat-\nural connectivity of the human body.\nSince then, many\nworks[5, 18, 20, 22, 27, 28] have delved into the optimiza-\ntion of graph convolutional mechanisms, with a focus on\nadaptively building topologies to effectively capture mo-\ntion patterns.\nHowever, most of these approaches lever-\nage learnable adjacency matrices combined with attention\nmechanisms and other techniques derived from dynamics\ntheory[35], information bottleneck theory[10], topological\ndata analysis[44] and so on, neglecting or underestimating\nthe inherent symmetry of the human body and its poten-\ntial role in topology learning. For instance, in the case of\na “brush hair” action, the interaction between the head and\n1\narXiv:2411.12560v1  [cs.CV]  19 Nov 2024\n\nthe left hand should be hight-lighted and learned alongside\nthe interaction between the head and the right hand, while\nthe roles of other joints may be less critical.\nBased on the analysis above, we argue that integrat-\ning topological symmetry constraints into topology learning\ncould lead to a straightforward yet effective mechanism for\ntopological learning. In this paper, we propose a topological\nsymmetry enhanced graph convolution, named TSE-GC, to\nenable distinct topology learning across different channels\ngroups while incorporating topological symmetry aware-\nness. Specifically, TSE-GC learns the scale mask of interest\nfor each sample and reactivates a shared topology into dis-\ntinct copies for multiple separate channel groups (illustrated\nin Fig.1). The scale mask is learned via k-nearest neighbor\n(k-NN) algorithm based on the Euclidean distance, where\nthe k nearest neighbors of each joint and the corresponding\nscales are identified. During the learning of scale mask, we\nalso introduce the calculated Euclidean distance as a cali-\nbration for the topology tailored to each sample. With few\nextra parameters introduced, our method facilitates topol-\nogy learning in a symmetric manner and effectively cap-\ntures the intricate correlations between joints.\nIn terms of feature aggregation across different frames,\nmost of these approaches[5, 22, 44] tend to apply multi-\nscale temporal convolution to effectively capture both short-\nrange and long-range dependencies, yet their representa-\ntion capacity is still limited due to essentially fixed re-\nceptive fields of temporal convolution. To remedy the is-\nsue, we introduce the concept of deformable convolution\nand construct a multi-branch deformable temporal convo-\nlution, named MBDTC, for skeleton-based action recogni-\ntion. MBDTC incorporates learnable offsets to the sampling\nlocations of the temporal convolution filter, enabling more\nflexible receptive fields and better representation capacity.\nCombining TSE-GC with MBDTC, we construct a novel\ntopological symmetry enhanced graph convolution network\n(TSE-GCN) for skeleton-based action recognition.\nEx-\ntensive experimental results on NTU RGB+D[26], NTU\nRGB+D 120[21], and NW-UCLA[34] show that TSE-GCN\nachieves competitive performance with fewer parameters\ncompared with state-of-the-art methods.\nOur contributions can be summarized as follows:\n1. We propose a topological symmetry enhanced graph\nconvolution (TSE-GC) which facilitates topology learn-\ning by incorporating physical constraints rooted in topo-\nlogical symmetry and flexibility.\n2. We introduce the concept of deformable modeling and\nconstruct a multi-branch deformable temporal convo-\nlution (MBDTC) for skeleton-based action recognition\nwith enhanced representation capacity.\n3. Integrating TSE-GC with MBDTC, Our TSE-GCN\nachieves competitive performance against state-of-the-\nart methods on three large datasets for skeleton-based\naction recognition with fewer parameters.\n2. Related Work\n2.1. Skeleton-based Action Recognition\nEarly deep-learning methods utilized recurrent neural net-\nworks (RNNs)[12, 33] and convolutional neural networks\n(CNNs)[19, 31] to capture action representations by encod-\ning skeleton data as feature sequences or pseudo-images,\nbut they overlooked the inherent relations between joints.\nConsequently, these methods failed to effectively model hu-\nman body topology, limiting their overall performance. In\ncontrast, GCNs represent the human body as graphs, where\njoints are treated as nodes and their relations are treated as\nedges. Such a design enables GCNs to effectively capture\ndependencies between joints and brings the potential to un-\nravel complex motion patterns.\n2.2. GCNs for Skeleton-based Action Recognition\nTopology learning is a key focus of GCNs in the context\nof skeleton-based action recognition, enabling the effec-\ntive modeling of human joint correlations.\nThe pioneer,\nSTGCN[37] predefines the physical structure of the human\nbody as a fixed topology. 2s-AGCN[28] introduces adap-\ntiveness to topology learning with a self-attention mech-\nanism.\nSince then, numerous studies[5, 10, 18, 29, 36,\n38, 44] have aimed to capture the intrinsic correlations be-\ntween human joints using learnable adjacency matrices, at-\ntention mechanisms or other techniques. Among them, AS-\nGCN[20] and MS-G3D[22] leverage adjacency powering\nfor multi-scale modeling, which is also adopted in our meth-\nods to establish topological symmetry constraints.\nA previous work, DEGCN[23], introduces deformable\ntemporal convolution for skeleton-based action recognition.\nHowever, DEGCN applies a uniform offset to all frames in\na data-independent manner, resulting in the same receptive\nfield across different frames. In contrast, our TSE-GCN\nlearns a unique offset for each individual frame, which is\ndata-dependent and enables more flexible receptive fields.\n2.3. Deformable Modeling\nThe concept of deformable modeling originates from\nCNNs, with the main goal of directing important locations\non images. As CNNs employ fixed convolutional filters,\nthey struggle to adapt to complex and irregular shapes in\nimages. Deformable convolution[11] addresses this limita-\ntion by allowing the convolutional kernels to learn spatial\noffsets, enabling them to sample from non-grid locations in\nthe input feature map. Deformable-DETR[45] builds upon\nthe original DETR[4] architecture with deformable atten-\ntion mechanisms. Our MBDTC applies 1D temporal de-\nformable convolution modules with multiple distinct origin\n2\n\nInput\nθ(1x1) \nφ(1x1)  \nω(1x1) \nsplit\nres\nCs\nd = -dis(ei, ej)2\nσ\nB\nAs\ntop-k\nf\n·\nk-nearest \nneighbor\nreactivate\nInput\nθ(1x1) \nφ(1x1)  \nω(1x1) \nsplit\nres\nCs\nd = -dis(ei, ej)2\nσ\nB\nAs\ntop-k\nf\n·\nk-nearest \nneighbor\nreactivate\nFC + Softmax\nPE\nFC\nPE\nFC\nx L\nTSE-GC\nTSE-GC\nMBDTC\nMBDTC\nx L\nTSE-GC\nMBDTC\nGAP\nGAP\nPrediction\nDTC \noriginal dilaton = 1\nDTC \noriginal dilaton = 1\nDTC \noriginal dilaton = 2\nDTC \noriginal dilaton = 2\nConv 1x1\nConv 1x1\nConv 1x1\nMaxPool 3x1\nConv 1x1\nDTC \noriginal dilaton = 1\nDTC \noriginal dilaton = 2\nConv 1x1\nConv 1x1\nConv 1x1\nMaxPool 3x1\nConv 1x1\nInput\nconcat\nTemporal Flow\nTemporal Flow\n...\n...\n...\nFigure 2. Architecture overview of the proposed TSE-GCN. PE denotes the learnable absolute positional embedding[10]. L denotes the\nnumber of stacked layers. In TSE-GC module, f represents the generation function of scale mask where the indexes of top-k neighbors\nare mapped to relevant scales. ⊗, ⊙, ⊕denote matrix multiplication, element-wise multiplication, element-wise sum, respectively. Best\nviewed in color.\nsampling positions, along with several modifications to bet-\nter suit the skeleton-based action recognition scenario.\n3. Method\nIn this section, we first define related notations and revisit\nthe formulations for vanilla graph convolution and its vari-\nants. Then we elaborate the design of our Topological Sym-\nmetry Enhanced Graph Convolution (TSE-GC) in Section\n3.2 and Multi-Branch Deformable Temporal Convolution\n(MBDTC) in Section 3.3. The overall architecture of TSE-\nGCN is presented in the end.\n3.1. Preliminaries\nNotations. In GCNs, the human body within a motion se-\nquence is represented as a spatio-temporal graph. The graph\nis denoted as G = (V, E), where V = {v1, v2, . . . , vN} is\nthe set of N vertices representing joints and E is the edge\nset representing the correlations between joints. Typically,\nG is formulated by X ∈RN×T ×C and A ∈RN×N. X is\nthe feature tensor of N vertices across T frames, and vi’s\nfeature at frame t is denoted as xi,t ∈RC. C is the number\nof channels. A is the adjacency matrix, with its elements\naij representing the correlation between vi,: and vj,:.\nFormulations of Graph Convolution. Within the realm of\nskeleton-based action recognition, the vanilla graph convo-\nlution proposed by [17] is widely adopted:\nX(l+1)\nt\n= σ( ˆAX(l)\nt W(l)),\n(1)\nwhere X(l)\nt\n∈RN×d(l) denotes hidden feature representa-\ntion of the l-th layer at frame t, ˆA = D−1\n2 (A + I)D−1\n2 is\nthe normalized adjacency matrix, D is the diagonal degree\nof A + I, W(l) ∈Rd(l)×d(l+1) is learnable parameters of\nthe l-th layer, and σ indicates activation function.\nVariants of Eq. 1 typically adopt a partitioning strategy\nwith multiple subsets involved. For example, ST-GCN[37]\ndivides A+I into self-loop, centrifugal and centripetal com-\nponents, Info-GCN[10] utilizes a multi-head self-attention\nmechanism where each head corresponds to a subset, and\nAS-GCN[20] employs a multi-scale aggregation with each\nsubset representing a distinct scale. The unified form can be\n3\n\nformulated as:\nX(l+1)\nt\n= σ(\nX\nS\nfs( ˆA)gs(X(l)\nt )W(l)\ns ),\n(2)\nwhere fs(·) and gs(·) represent mapping functions for adja-\ncency matrices and feature tensors, respectively.\n3.2. Topology Learning with Symmetry Awareness\nTopological symmetry is an inherent characteristic of the\nhuman body, where corresponding limbs mirror each other.\nMany physical activities, such as walking, running, or danc-\ning, exhibit symmetrical patterns that should be captured\nthrough the process of graph representation learning. How-\never, previous works tend to employ complex graph learn-\ning mechanisms without explicitly incorporating topologi-\ncal symmetry during topology learning.\nOn the other hand, most prior approaches typically uti-\nlize learnable adjacency matrices to adaptively capture cor-\nrelations between joints, allowing the topology to be learned\nwithout any constraints based on physical bone connec-\ntions. In contrast, directly using the physical structure as\nthe topology for graph feature aggregation, as done in [37],\nyields suboptimal performance. Intuitively, a balanced ap-\nproach that incorporates both flexibility and physical con-\nstraints is worth exploring.\nBased on the analysis above, we argue that integrat-\ning topological symmetry constraints into topology learning\ncould enhance the modeling capacity of graph convolution,\nwhich leads to TSE-GC. The architecture of our TSE-GC is\nillustrated in the bottom left corner of Fig.2, and the forward\nprocess can be divided into two pathways, one for topology\nlearning and one for feature partitioning.\nFeature partitioning. Given the input feature X ∈RN×C,\nwe first transform it into high-level representation with a\nlinear embedding function ψ, which is formulated as:\n˜X = ψ(X) = XWψ,\n(3)\nwhere Wψ ∈RC×(C′K) is the weight matrix and ˜X ∈\nRN×(C′K) is the transformed feature. ˜X is further divided\nalong the channel dimension C′K into K partitions.\nThe entire feature partitioning process, denoted as gs(·)\nin Eq. 2, is formulated as:\n˜X = gs(X) = ψs(X) = [ ˜X0|| ˜X1||...|| ˜XK−1]\n(4)\n˜Xk = ˜X:,kC′:(k+1)C′,\n(5)\nwhere || is concatenate function and ˜Xk ∈RN×C′ repre-\nsents the k-th partition of the transformed feature ˜X. Note\nthat ψs(·) represents distinct instances of ψ(·) applied to\ndifferent subsets.\nTopology learning. The topology learning pathway is de-\ntailed in the left section of the illustrated TSE-GC shown\nin Fig.2. A shared topology M ∈RN×N is utilized for all\nsubsets and reactivated as A ∈RK×N×N. The reactiva-\ntion could be interpreted as a form of resampling, where the\ncorrelations between joints at the relevant scales or hops are\nsampled. We use the term reactivate to describe the char-\nacteristic of constraining topology learning in a symmetry\nmanner, because the reactivated interactions can be updated\nthrough the training phase, while the other interactions re-\nmain unchanged. During the inference phase, only the re-\nactivated interactions are utilized for graph feature aggrega-\ntion. To maintain flexibility of topology modeling, we also\nintroduce a data-dependent calibration matrix B ∈RN×N,\nas well as a learnable adjacency matrix C ∈RK×N×N for\neach subset.\nSpecifically, we first employ another linear embedding\nfunction θ to embed the input feature for the subsequent\ntopology learning, as in Eq. 3. The negative square of Eu-\nclidean distance D ∈RN×N is then calculated based on the\nembeddings between distinct joints, as dij = −dis(ei, ej)2,\nwhere ei and ej represent the embeddings of the joints. D\nis further utilized to obtain the calibration matrix B:\nB = ξ(D),\n(6)\nwhere ξ(·) is the activation function.\nBased on the calculated Euclidean distance, the indexes\nof k-nearest neighbors can be identified as IN ∈RN×K,\nwhich is mapped to the relevant scales or hops. Relevant\nscales are marked as 1 in the scale mask. The generation of\nscale mask can be formulated as:\nH = f(IN, Dsp) = f(KNN(θ(X), K), Dsp),\n(7)\nwhere H ∈RK×N×N is the scale mask, f is the generation\nfunction of scale mask where the indexes of top-k neigh-\nbors are mapped to relevant scales, Dsp is a mapping table\nbased on the shortest path distance (SPD). The calculation\nof SPD is based on the physical connections, which can be\nformulated as:\ndspi,j =\nmin\nP ∈P aths(G){|P|, P1 = vi, P|P | = vj},\n(8)\nwhere dspi,j represents SPD between nodes vi and vj, and\nP denotes the shortest path in the graph G connecting these\nnodes. In practice, we calculate it with adjacency powering.\nWith the scale mask, we reactivate the shared topology\nto introduce topological symmetry constraints:\nAs = Hs ⊙M,\n(9)\nwhere ⊙is element-wise multiplication operation, As ∈\nRK×N×N is the reactivated topology. We use subscript s\nto indicate distinct scale masks and activated topology for\ndifferent subsets.\n4\n\nv1\nv11\nv1\nv11\nDepth-wise \nConv\nGraph readout\noffsets\n...\nPoint-wise \nConv\nsample\nFigure 3. Sampling mechanism in our DTC Module. The receptive\nfields for the frame enclosed by the red dashed line are enclosed\nby the blue dashed line with the offsets. We assume the offsets to\nbe integers for clarity. Best viewed in color.\nEventually, the learned topology for graph feature aggre-\ngation in our TSE-GC module is formulated as:\nZ = fs(A) = α · As + β · B + Cs\n(10)\nZk = Zk,:,:,\n(11)\nwhere α and β are trainable scalars that control the balance\nbetween flexibility and physical constraints. Zk ∈RN×N\nrepresents the k-th partition of the learned topology, which\nis obtained by combining the activated topology, the learn-\nable compensation term and the calibration term.\nWith Eq. 4 and Eq. 11 substituted into Eq. 2, we have\nour final formula for TSE-GC:\nX(l+1)\nt\n= σ(\nX\nS\n[Z(l)\n0 ˜X(l)\n0 ||...||Z(l)\nK−1 ˜X(l)\nK−1]W(l)\ns ), (12)\nwhere σ(·) is the activation function. Some subscripts are\nsimplified for readability. In general, our TSE-GC intro-\nduces topological symmetry constraints for the learning of\nAs while maintaining a certain level of flexibility with Cs,\nleading to an effective combination of prior-knowledge-\nbased constraints and adaptability for topology learning.\n3.3. Multi-Branch Deformable Temporal Modeling\nTemporal modeling is essential for capturing complex mo-\ntion dynamics in human action recognition. For GCNs, an\neffective temporal modeling module should be capable of\nflexibly capturing temporal dependencies, or correlations\nwithin the temporal graph. Previous works[5, 22, 44] typi-\ncally utilize multi-scale temporal convolution to effectively\ncapture both short-range and long-range dependencies with\nmultiple fixed receptive fields, which is suboptimal. An in-\ntuitive solution is to introduce attention mechanisms, but\ntheir computational cost can be excessively high for long\nsequences. Therefore, we adopt the idea of introducing de-\nformable modeling and construct our MBDTC with flexible\nreceptive fields for skeleton-based action recognition, en-\nabling dynamic temporal modeling.\nTo begin with, we revisit the deformable convolution\nfrom [11]. Given a 2D convolution, the sampling locations\nare defined by a regular grid R, resulting in a fixed receptive\nfield. The deformable convolution adds learnable offsets to\nmake the sampling locations irregular, formulated as:\ny(p0) =\nX\npn∈R\nw(pn) · x(p0 + pn + ∆pn),\n(13)\nwhere p0 is a location on the output feature map and pn is\nthe fixed offsets in R. ∆pn represents the learnable offset at\neach sampling point p0 +pn, which allows the convolution\nto adaptively adjust the receptive field.\nFor spatio-temporal graphs in GCNs, spatial features\nare inherently structured, while temporal features are dis-\ncretely sampled from continuous frames. Typically, only\ncorrelations among the same joints across different frames\nare considered.\nBased on the distinct characteristic, we\nredesign the vanilla deformable convolution into our de-\nformable temporal convolution (DTC) which learns the off-\nsets for each frame with spatial graph feature extracted\nthrough weighted graph pooling. An example of our sam-\npling mechanism is illustrated in Fig.3, where we insert a\nreadout operation with graph pooling between depth-wise\nseparable convolution.\nSpecifically, we first embed the input feature X\n∈\nRN×T ×C into Y ∈RN×T ′×C with a depth-wise convo-\nlution. The potential reduction in frames from T to T ′ is\ncaused by the stride applied to our DTC. The spatial graph\nfeature of the embedding is then pooled, formulated as:\nZ = 1\nN\nN\nX\nn=1\nwnYn,\n(14)\nwhere wn is the weight scalar for joint vn and Z ∈RT ′×C\nrepresents the graph readouts for T ′ frames. Subsequently,\nthe offsets are obtained with learnable parameters and Z,\nformulated as:\nP = ZW,\n(15)\nwhere P ∈RT ′×R indicates the offsets for T ′ frames. R\nis the kernel size applied to our DTC. Notably, R is also\napplied to the preceding depth-wise convolution.\nWith the offsets and the original sampling locations, we\nadopt the linear interpolation for sampling and introduce a\nweight matrix M ∈RT ′×R, which is obtained from the\noffsets through a linear transformation. Our DTC can be\n5\n\nTable 1. Action classification performance on NTU RGB+D 60, NTU RGB+D 120 and NW-UCLA. †indicates methods that are not\ndirectly comparable with further details in Section 4.3. * indicates the parameters or FLOPs are recalculated from their publicly available\ncodes for a single stream in NTU RGB+D 120, then scaled by the number of streams for a fair comparison. The FLOPS are calculated\nusing the fvcore library*. We omit the results without publicly available codes for reproduction.\nMethods\nPublication\nModalities\nParams(M)\nFLOPs(G)\nNTU RGB+D 60\nNTU RGB+D 120\nNW-UCLA\nX-Sub(%)\nX-View(%)\nX-Sub(%)\nX-Set(%)\nST-GCN[37]\nAAAI 2018\nJ\n3.1*\n3.48*\n81.5\n88.3\n70.7\n73.2\n-\n2S-AGCN[28]\nCVPR 2019\nJ+B\n6.8*\n7.96*\n88.5\n95.1\n82.5\n84.2\n-\nDC-GCN+ADG[8]\nECCV 2020\nJ+B+JM+BM\n19.6\n7.32\n90.8\n96.6\n86.5\n88.1\n95.3\nShift-GCN[9]\nCVPR 2020\nJ+B+JM+BM\n-\n-\n90.7\n96.5\n85.9\n87.6\n94.6\nMS-G3D[22]\nCVPR 2020\nJ+B\n12.8*\n28.24*\n91.5\n96.2\n86.9\n88.4\n-\nMST-GCN[6]\nAAAI 2021\nJ+B+JM+BM\n12.0\n-\n91.5\n96.6\n87.5\n88.8\n-\nCTR-GCN[5]\nICCV 2021\nJ+B+JM+BM\n5.6*\n7.88*\n92.4\n96.4\n88.9\n90.4\n96.5\nEfficientGCN-B4[30]\nTPAMI 2022\nJ+B+JM+BM\n8.0\n-\n91.7\n95.7\n88.3\n89.1\n-\nInfo-GCN[10]\nCVPR 2022\nJ+B+JM+BM\n6.3\n6.72\n92.3\n96.7\n89.2\n90.7\n96.6\nFR-Head[43]\nCVPR 2023\nJ+B+JM+BM\n8.0\n-\n92.8\n96.8\n89.5\n90.9\n96.8\nBlockGCN[44]\nCVPR 2024\nJ+B+JM+BM\n5.2*\n8.20*\n93.1\n97.0\n90.3\n91.5\n96.9\nInfo-GCN†[10]\nCVPR 2022\n-\n9.4\n10.08\n93.0\n97.1\n89.8\n91.2\n97.0\nHD-GCN†[18]\nICCV 2023\n-\n10.1\n9.60\n93.4\n97.2\n90.1\n91.6\n97.2\nShap-Mix†[40]\nCVPR 2024\nJ+B+JM+BM\n-\n-\n93.7\n97.1\n90.4\n91.7\n-\nTSE-GCN(Ours)\nJ\n1.1\n1.38\n90.8\n95.3\n86.6\n88.2\n95.3\nTSE-GCN(Ours)\nJ+B+JM+BM\n4.4\n5.52\n92.9\n96.8\n90.0\n91.1\n96.9\nformulated as:\nX(l+1) =\nR\nX\nr=1\nWRΓ(P, X(l+1), M),\n(16)\nwhere Γ(·, ·, ·) is the sampling and reweighting function,\nWR is learnable parameters and X(l+1) ∈RN×T ′×C is\nthe output with aggregated temporal features.\nWith our DTC module, we construct MBDTC mod-\nule following the multi-scale temporal convolution module\n(MS-TCN) adopted by [5]. The architecture of our MB-\nDTC is illustrated in the bottom right corner of Fig.2, which\nis consists of four branches, with each branch containing a\n1 × 1 convolution to reduce channel dimension. The first\ntwo branches utilize our DTC with different original sam-\npling locations defined by different dilations, the other two\nbranches implement a max-pooling operation and an iden-\ntity function, respectively. The outputs are concatenated as\nthe final output.\n3.4. Model Architecture\nBased on the aforementioned TSE-GC and MBDTC, we\nconstruct a topological symmetry enhanced graph convolu-\ntional network TSE-GCN for skeleton-based action recog-\nnition. The model architecture is illustrated in Fig.2. Based\non experimental results, we select GeLU as the activation\nfunction in the network, except for Eq. 6 where we use Tanh\nfor a more suitable output range in modeling topology.\nThe input feature is first embedded with a linear transfor-\nmation and then combined with a learnable absolute posi-\ntional embedding from [10]. Subsequently, the embeddings\nare fed into a stack of our basic blocks, each consisting of a\nTSE-GC module, a MBDTC module and residual connec-\ntions. L = 9, is the number of times our basic block is\nstacked. The number of channels for nine blocks are 64-64-\n64-64-128-128-128-256-256. Following the stacked basic\nblocks, a global average pooling operation and a softmax\nclassifier is applied to generate predictions across different\naction classes.\n4. Experiments\n4.1. Datasets\nNTU RGB+D. NTU RGB+D[26] is a large-scale human\naction recognition dataset containing 56,880 skeleton ac-\ntion sequences over 60 classes. The action samples are per-\nformed by 40 participants in different age groups, and each\nsample contains an action and is guaranteed to have at most\n2 subjects. For each sample, the skeleton data of 25 joints is\ncaptured by three Microsoft Kinect v2 cameras from differ-\nent views concurrently. Two benchmarks are provided by\nthe authors: (1) cross-subject (X-sub): training data comes\nfrom 20 subjects, and testing data comes from the other 20\nsubjects. (2) cross-view (X-view): data captured from the\nfront and two side views is used for training, and data cap-\ntured from the left and right 45-degree views is used for\ntesting.\nNTU RGB+D 120. NTU RGB+D 120[21] is an extended\nversion of NTU RGB+D with additional 57,367 skeleton\nsequences over 60 extra action classes, which is one of the\nlargest datasets with 3D joints annotations for human ac-\n6\n\ntion recognition. Totally 113,945 samples over 120 classes\nare performed by 106 volunteers with 32 distinct setups\nfor locations and backgrounds, captured with three cameras\nviews. The authors recommend two benchmarks: (1) cross-\nsubject (X-sub): training data comes from 53 subjects, and\ntesting data comes from the other 53 subjects. (2) cross-\nsetup (X-set): training data comes from samples with even\nsetup IDs, and testing data comes from samples with odd\nsetup IDs.\nNW-UCLA. NW-UCLA[34] is a small human action\nrecognition dataset containing 1494 video clips over 10 ac-\ntion categories. Each action is performed by 10 subjects,\nwith the skeleton data captured by three Kinect cameras\nsimultaneously from multiple viewpoints.\nFollowing the\nevaluation protocol recommended by the authors, we use\nthe viewpoints of the first two cameras for training and the\nother for testing.\n4.2. Implementation Details\nAll experiments are conducted on two RTX 3090 GPUs\nwith the PyTorch deep learning framework. The SGD opti-\nmizer is employed with a Nestrov momentum of 0.9 and a\nweight deacy of 0.004 for NTU RGB+D and NTU RGB+D\n120, and 0.002 for NW-UCLA. Our method utilizes cross-\nentropy loss. The learning rate is initialized at 0.05 and\nreduced by a factor of 0.1 at epoch 110 and 120, with a to-\ntal epoch 140. For NTU datasets, we set a batch size of\n64 with each sample resized to 64 frames and adopt the\ndata-processing in [41]. For NW-UCLA, the batch size is\nselected as 16 and we adopt he data-processing in [9].\n4.3. Comparison with the State-of-the-art\nIn this subsection, we compare the proposed TSE-GCN\nagainst state-of-the-art methods on NTU RGB+D, NTU\nRGB+D 120 and NW-UCLA in Table 1. To establish a\nfair comparison, we follow the commonly adopted four-\nstream fusion approach in our experiment. Specifically, we\nfuse the results of joint, bone, joint motion and bone motion\nmodalities. Notably, some of the methods are not directly\ncomparable to our methods, which are shown in the second\npart of the table, for example, HD-GCN[18] applies a six-\nstream ensemble with their hand-crafted modalities, as well\nas Info-GCN[10], and Shap-Mix[40] utilizes the additional\nmixed data from their data augmentation approach which\ndoubles the number of samples. The performance of Info-\nGCN in the first part is from the reproduction from [14] for\na fair comparison, following [44].\nExperimental results demonstrate that our method, TSE-\nGCN achieves state-of-the-art performance on the com-\nmon benchmarks.\nCompared with the state-of-the-art\nBlockGCN[44], the proposed TSE-GCN achieves 0.3%\nlower accuracies on average with 15.4% fewer parameters\nand 32.7% fewer FLOPs. Compared with FR-Head[43], the\nproposed TSE-GCN surpasses it with 45% fewer parame-\nters. It is noteworthy that we prioritize a better trade-off be-\ntween model size and performance in our hyper-parameter\nsetting instead of accuracies, as detailed in Section 4.4.\nMeanwhile, our method can serve as a backbone and be\ncombined with Shap-Mix[40] or FR-Head[43] to further\nimprove performance.\n4.4. Ablation Study\nIn this subsection, we analyze the proposed TSE-GC and\nits configurations along with MBTDC on the X-sub bench-\nmark of the NTU RGB+D 120 dataset, using joint stream\ndata. We reconstruct ST-GCN[37] with our selection of ac-\ntivation function as the baseline in our experiments, with\nthe 2-nd layer removed.\nTable 2.\nComparisons of performance when adding PE and\nour TSE-GCN gradually. TSE-GC is examined through distinct\ntopologies to assess their effectiveness. PE denotes the learnable\nabsolute positional embedding [10] added to the input feature be-\nfore the first layer of our network.\nBaseline\nMBTDC\nPE\nTSE-GC\nParams(M)\nAcc(%)\nCs\nAs + B\n✓\n-\n-\n-\n-\n3.0\n84.9\n✓\n✓\n-\n-\n1.2(-1.9)\n85.4\n✓\n✓\n✓\n-\n-\n1.2\n85.5\n✓\n✓\n✓\n✓\n-\n1.0\n86.4\n✓\n✓\n✓\n-\n✓\n1.1\n86.2\n✓\n✓\n✓\n✓\n✓\n1.1\n86.6\nTable 3. Comparisons of our TSE-GCN with different settings.\nK denotes the number of partitions of input feature and adjacency\nmatrices. R denotes the ratio of the embedded number of channels\nto the original.\nK\nR\nParams(M)\nAcc(%)\n2\n4\n1.3\n86.4\n8\n1.0\n86.5\n3\n4\n1.7\n86.7\n8\n1.1\n86.6\n16\n0.9\n86.5\n4\n8\n1.3\n86.5\n5\n8\n1.5\n86.3\nEffectiveness of MBDTC and TSE-GC. We examine\nthe effectiveness of each component of our TSE-GCN by\nadding them to the baseline gradually. The experimental re-\nsults are shown in Table 2. First, we replace the original\nTCN with our MBTDC, resulting in a 0.5% improvement\nin accuracy while reducing the number of parameters by\n7\n\nbrushing teeth\n+2.56\nbrushing hair\n-0.73\nclapping\n+1.83\nhopping\n+0.00\njump up\n+0.00\nrub two hands together\n-0.36\nnod head/bow\n-0.72\nput the palms together\n+1.45\ncross hands in front\n+0.72\nuse a fan/feeling warm\n-0.73\npunching/slapping other person\n+2.19\ntouch other person's pocket\n-0.73\nhandshaking\n+0.36\nshoot at the basket\n+0.52\nsquat down\n+0.17\nshake fist\n+4.69\nhands up\n+1.05\ncross arms\n+1.39\narm circles\n+0.17\nrunning on the spot\n-0.17\nbutt kicks\n-0.70\ncross toe touch\n+0.70\nside kick\n+0.00\nstretch oneself\n+0.87\nhit other person with something\n+3.83\nhigh-five\n+0.69\nfinger-guessing game\n-0.87\n(a) Comparison with ST-GC.\nbrushing teeth\n+4.03\nbrushing hair\n-1.83\nclapping\n+1.10\nhopping\n-0.36\njump up\n-0.72\nrub two hands together\n-1.09\nnod head/bow +0.00\nput the palms together\n+1.81\ncross hands in front\n+0.36\nuse a fan/feeling warm\n-0.73\npunching/slapping other person\n+4.01\ntouch other person's pocket\n-1.45\nhandshaking\n+0.72\nshoot at the basket\n+2.62\nsquat down\n+0.17\nshake fist\n+2.78\nhands up\n+2.09\ncross arms\n+1.57\narm circles\n-0.35\nrunning on the spot\n-0.52\nbutt kicks\n-0.52\ncross toe touch\n-0.52\nside kick\n+1.05\nstretch oneself\n-1.74\nhit other person with something\n+9.74\nhigh-five\n-0.35\nfinger-guessing game\n+0.35\n(b) Comparison with CTR-GC.\nFigure 4. Accuracy difference(%) between TSE-GC and two rep-\nresentative GCs on symmetry related classes generated by GPT4\n[1]. Green bars indicate improvements.\n1.9M. This enhancement demonstrates the effectiveness of\nMBTDC. The introduction of PE marginally increases the\nparameter count with a slight improvement of 0.1% in ac-\ncuracy.\nWe then validate the effect of our TSE-GC by decou-\npling it into separate learned topologies Cs and As + B.\nFor example, Cs indicates a setting of our TSE-GC without\nthe learning of As + B. As + B is not further decoupled\nbecause the learning of B contributes to the gradients of the\nembedding function at the initial stages of topology learn-\ning pathway. The fully flexible setting of our TSE-GC (Cs)\nimproves the performance by 0.9%, while the constrained\nsetting (As + B) yields an improvement of 0.7%. By inte-\ngrating both, our TSE-GC outperform the baseline by 1.7%\nwhile reducing 63% of its parameters.\nConfigurations of TSE-GC. We explore different configu-\nrations of our TSE-GC, specifically the number of partitions\nK and the reduction rate R of embedding function ψ and θ.\nSince K also represents different scales within TSE-GC, we\nexamine settings of K = 2, 3, 4, 5 to analyze its impact on\nperformance. When K = 1, As falls into I as we do not\nexclude the joint itself in our KNN implementation to pre-\nserve self-connection. When K = 7 and each neighboring\njoint represents a different scale, As functions as a fully\nconnected layer on NTU datasets.\nAs shown in Table 3, our TSE-GC consistently outper-\nforms the baseline under all configurations, demonstrating\nits robustness to diverse settings. By comparing the experi-\nmental results of K = 2, 3, 4, 5, we find our models achieve\nbest results when K = 3. We argue that fewer partitions\nlead to an insufficient modeling while more partitions intro-\nduce redundancy. As for the setting of R, a lower value of\nR leads to more adjacency matrices and additional parame-\nters, thereby increasing the overall computational complex-\nity. Our optimal performance can be achieved with K = 3\nand R = 4. However, in order to balance performance and\nefficiency, we select K = 3 and R = 8 as the final choice\nof hyper parameters for TSE-GC.\n4.5. Performance Analysis on Single Stream\nIn table 4, we compare single-stream performance of our\nTSE-GCN with other GCNs. Our model achieves a com-\npetitive performance on NTU RGB+D 120 joint stream with\nthe lowest computational complexity.\nTable 4. Comparisons of GCNs on single stream.\nMethods\nX-Sub(%)\nX-Set(%)\nParams(M)\nFLOPs(G)\nCTR-GCN\n84.9\n86.5\n1.5\n1.97\nInfo-GCN\n85.1\n86.3\n1.6\n1.68\nHD-GCN\n85.7\n87.3\n1.7\n1.60\nFR-Head\n85.5\n87.3\n2.0\n-\nBlockGCN\n86.9\n88.2\n1.3\n2.05\nTSE-GCN\n86.6\n88.2\n1.1\n1.38\nWe further analyze the accuracies of certain classes to\nconfirm that the experimental results align with the in-\ntended design to incorporate topological symmetry aware-\nness. Specifically, we utilize GPT4[1] to identify symme-\ntry related classes within NTU RGB+D 120, as the corre-\nlation between certain actions and topological symmetry is\nabstract and hard to be defined by human. The final list\ncontains 27 classes, including clapping, hands up, cross\narms and so on. We calculate the accuracy difference of\nthese classes between TSE-GC and two representative GCs\nin Fig.4. We select ST-GC and CTR-GC as the topology\nof ST-GC is fully physical, while the topology of CTR-\nGC is nearly fully flexible due to the catastrophic forgetting\nof skeletal topology[44]. Experimental results demonstrate\nthat TSE-GC shows an average improvement of +0.6%\ncompared to ST-GC and +0.8% compared to CTR-GC on\nthese classes, which corresponds to their distinct topologies.\nMeanwhile, overall performance improvement is observed\nin both cases, with a few classes showing gains exceeding\nthe average, such as +4.69% for “shake fist” in ST-GC case\nand +9.74% for “hit other person with something” in CTR-\nGC case.\n5. Conclusion\nIn this work, we propose a novel topological symme-\ntry enhanced graph convolution network (TSE-GCN) for\nskeleton-based action recognition.\nOur TSE-GC module\nleverages the inherent topological symmetry of the human\nbody to achieve an effective and balanced topology learn-\ning between flexibility and physical constraints. Our multi-\nbranch deformable temporal convolution (MBTDC) mod-\nule incorporates deformable modeling to enable flexible re-\nceptive fields. TSE-GCN achieves competitive performance\ncompared with state-of-the-art methods on three datasets,\nwith the effectiveness of each component validated. Our\nfuture research may proceed in two main directions:\n8\n\n(1) Efficient cross-spacetime feature aggregation.\nWith\nflexible receptive fields in deformable temporal convolu-\ntion, it is possible to achieve a learnable cross-spacetime\nfeature aggregation, leading to a unified and powerful rep-\nresentation for GCNs.\n(2) Redundancy reduction in graph convolution. Some ac-\ntions can be identified with only topological symmetric\njoints, such as “clapping” and “hands up”. Removing re-\ndundant correlations and preserving informative ones could\nenhance efficiency and facilitate practical applications.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 8\n[2] Tamas Bates, Karinne Ramirez-Amaro, Tetsunari Inamura,\nand Gordon Cheng.\nOn-line simultaneous learning and\nrecognition of everyday activities from virtual reality per-\nformances. In 2017 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3510–3515.\nIEEE, 2017. 1\n[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7291–7299, 2017. 1\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213–229. Springer, 2020. 2\n[5] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying\nDeng, and Weiming Hu. Channel-wise topology refinement\ngraph convolution for skeleton-based action recognition. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 13359–13368, 2021. 1, 2, 5, 6\n[6] Zhan Chen, Sicheng Li, Bing Yang, Qinghan Li, and Hong\nLiu. Multi-scale spatial temporal graph convolutional net-\nwork for skeleton-based action recognition.\nIn Proceed-\nings of the AAAI conference on artificial intelligence, pages\n1113–1122, 2021. 6\n[7] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi,\nThomas S Huang, and Lei Zhang.\nHigherhrnet: Scale-\naware representation learning for bottom-up human pose es-\ntimation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5386–5395,\n2020. 1\n[8] Ke Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian Cheng,\nand Hanqing Lu. Decoupling gcn with dropgraph module\nfor skeleton-based action recognition. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XXIV 16, pages 536–\n553. Springer, 2020. 6\n[9] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian\nCheng, and Hanqing Lu. Skeleton-based action recognition\nwith shift graph convolutional network. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 183–192, 2020. 6, 7\n[10] Hyung-gun Chi,\nMyoung Hoon Ha,\nSeunggeun Chi,\nSang Wan Lee, Qixing Huang, and Karthik Ramani.\nIn-\nfogcn: Representation learning for human skeleton-based\naction recognition. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n20186–20196, 2022. 1, 2, 3, 6, 7\n[11] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764–773, 2017. 2, 5\n[12] Yong Du, Wei Wang, and Liang Wang. Hierarchical recur-\nrent neural network for skeleton based action recognition. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1110–1118, 2015. 2\n[13] Amr Elkholy, Mohamed E Hussein, Walid Gomaa, Dima\nDamen, and Emmanuel Saba. Efficient and robust skeleton-\nbased quality assessment and abnormality detection in hu-\nman action performance. IEEE journal of biomedical and\nhealth informatics, 24(1):280–291, 2019. 1\n[14] Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng,\nJunyu Han, Errui Ding, Jingdong Wang, Xinggang Wang,\nWenyu Liu, and Bin Feng.\nGraph contrastive learn-\ning for skeleton-based action recognition.\narXiv preprint\narXiv:2301.10900, 2023. 7\n[15] Yu-Gang Jiang, Qi Dai, Wei Liu, Xiangyang Xue, and\nChong-Wah Ngo.\nHuman action recognition in uncon-\nstrained videos by explicit motion modeling. IEEE Trans-\nactions on Image Processing, 24(11):3781–3795, 2015. 1\n[16] Hyunjik Kim and Andriy Mnih. Disentangling by factoris-\ning. In International conference on machine learning, pages\n2649–2658. PMLR, 2018. 1\n[17] Thomas N Kipf and Max Welling. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907, 2016. 3\n[18] Jungho Lee, Minhyeok Lee, Dogyoon Lee, and Sangyoun\nLee.\nHierarchically decomposed graph convolutional net-\nworks for skeleton-based action recognition. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 10444–10453, 2023. 1, 2, 6, 7\n[19] Chao Li, Qiaoyong Zhong, Di Xie, and Shiliang Pu.\nSkeleton-based action recognition with convolutional neural\nnetworks. In 2017 IEEE international conference on multi-\nmedia & expo workshops (ICMEW), pages 597–600. IEEE,\n2017. 2\n[20] Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng\nWang, and Qi Tian. Actional-structural graph convolutional\nnetworks for skeleton-based action recognition. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 3595–3603, 2019. 1, 2, 3\n[21] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,\nLing-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-\nscale benchmark for 3d human activity understanding. IEEE\ntransactions on pattern analysis and machine intelligence,\n42(10):2684–2701, 2019. 2, 6\n9\n\n[22] Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang,\nand Wanli Ouyang. Disentangling and unifying graph convo-\nlutions for skeleton-based action recognition. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 143–152, 2020. 1, 2, 5, 6\n[23] Woomin Myung, Nan Su, Jing-Hao Xue, and Guijin\nWang. Degcn: Deformable graph convolutional networks\nfor skeleton-based action recognition. IEEE Transactions on\nImage Processing, 33:2477–2490, 2024. 2\n[24] Chiara Plizzari, Marco Cannici, and Matteo Matteucci.\nSkeleton-based action recognition via spatial and temporal\ntransformer networks. Computer Vision and Image Under-\nstanding, 208:103219, 2021. 1\n[25] Ronald Poppe.\nA survey on vision-based human action\nrecognition. Image and vision computing, 28(6):976–990,\n2010. 1\n[26] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1010–1019, 2016. 2, 6\n[27] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\nSkeleton-based action recognition with directed graph neu-\nral networks. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 7912–7921,\n2019. 1\n[28] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-\nstream adaptive graph convolutional networks for skeleton-\nbased action recognition. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 12026–12035, 2019. 1, 2, 6\n[29] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\nSkeleton-based action recognition with multi-stream adap-\ntive graph convolutional networks. IEEE Transactions on\nImage Processing, 29:9532–9545, 2020. 2\n[30] Yi-Fan Song, Zhang Zhang, Caifeng Shan, and Liang Wang.\nConstructing stronger and faster baselines for skeleton-based\naction recognition. IEEE transactions on pattern analysis\nand machine intelligence, 45(2):1474–1488, 2022. 6\n[31] Tae Soo Kim and Austin Reiter. Interpretable 3d human ac-\ntion analysis with temporal convolutional networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition workshops, pages 20–28, 2017. 2\n[32] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose es-\ntimation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5693–5703,\n2019. 1\n[33] Hongsong Wang and Liang Wang.\nModeling temporal\ndynamics and spatial configurations of actions using two-\nstream recurrent neural networks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 499–508, 2017. 2\n[34] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-\nChun Zhu. Cross-view action modeling, learning and recog-\nnition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2649–2656, 2014. 2, 7\n[35] Xinghan Wang, Xin Xu, and Yadong Mu.\nNeural koop-\nman pooling: Control-inspired temporal dynamics encod-\ning for skeleton-based action recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10597–10607, 2023. 1\n[36] Haojun Xu, Yan Gao, Zheng Hui, Jie Li, and Xinbo\nGao.\nLanguage knowledge-assisted representation learn-\ning for skeleton-based action recognition.\narXiv preprint\narXiv:2305.12398, 2023. 2\n[37] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nral graph convolutional networks for skeleton-based action\nrecognition. In Proceedings of the AAAI conference on arti-\nficial intelligence, 2018. 1, 2, 3, 4, 6, 7\n[38] Fanfan Ye, Shiliang Pu, Qiaoyong Zhong, Chao Li, Di Xie,\nand Huiming Tang. Dynamic gcn: Context-enriched topol-\nogy learning for skeleton-based action recognition. In Pro-\nceedings of the 28th ACM international conference on mul-\ntimedia, pages 55–63, 2020. 2\n[39] Ling-Fung Yeung, Zhenqun Yang, Kenneth Chik-Chi Cheng,\nDan Du, and Raymond Kai-Yu Tong.\nEffects of camera\nviewing angles on tracking kinematic gait patterns using\nazure kinect, kinect v2 and orbbec astra pro v2. Gait & pos-\nture, 87:19–26, 2021. 1\n[40] Jiahang Zhang, Lilang Lin, and Jiaying Liu.\nShap-mix:\nShapley value guided mixing for long-tailed skeleton based\naction recognition. arXiv preprint arXiv:2407.12312, 2024.\n6, 7\n[41] Pengfei Zhang, Cuiling Lan, Wenjun Zeng, Junliang Xing,\nJianru Xue, and Nanning Zheng. Semantics-guided neural\nnetworks for efficient skeleton-based human action recog-\nnition.\nIn proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1112–1121,\n2020. 7\n[42] Zhengyou Zhang.\nMicrosoft kinect sensor and its effect.\nIEEE multimedia, 19(2):4–10, 2012. 1\n[43] Huanyu Zhou, Qingjie Liu, and Yunhong Wang.\nLearn-\ning discriminative representations for skeleton based action\nrecognition. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10608–\n10617, 2023. 6, 7\n[44] Yuxuan Zhou, Xudong Yan, Zhi-Qi Cheng, Yan Yan, Qi Dai,\nand Xian-Sheng Hua. Blockgcn: Redefine topology aware-\nness for skeleton-based action recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2049–2058, 2024. 1, 2, 5, 6, 7, 8\n[45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection.\narXiv preprint\narXiv:2010.04159, 2020. 2\n10",
    "pdf_filename": "Topological_Symmetry_Enhanced_Graph_Convolution_for_Skeleton-Based_Action_Recognition.pdf"
}