{
    "title": "Homeostatic motion planning with innate physics knowledge",
    "abstract": "Living organisms interact with their surroundings in a closed-loop fashion, where sen- sory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in",
    "body": "arXiv:2402.15384v2  [cs.RO]  18 Nov 2024\n1\nHomeostatic motion planning with innate physics\nknowledge\nLafratta, G.1\nPorr, B.1\nChandler, C.2\nMiller, A.2\n1School of Engineering, University of Glasgow\n2School of Computing Science, University of Glasgow.\nKeywords: Planning, homeostasis, closed loop, autonomous agents\nAbstract\nLiving organisms interact with their surroundings in a closed-loop fashion, where sen-\nsory inputs dictate the initiation and termination of behaviours. Even simple animals\nare able to develop and execute complex plans, which has not yet been replicated in\n\nrobotics using pure closed-loop input control. We propose a solution to this problem\nby deﬁning a set of discrete and temporary closed-loop controllers, called “tasks”, each\nrepresenting a closed-loop behaviour. We further introduce a supervisory module which\nhas an innate understanding of physics and causality, through which it can simulate the\nexecution of task sequences over time and store the results in a model of the environ-\nment. On the basis of this model, plans can be made by chaining temporary closed-loop\ncontrollers. The proposed framework was implemented for a real robot and tested in\ntwo scenarios as proof of concept.\n1\nIntroduction\nLiving organisms interact with their surroundings through sensory inputs in a closed-\nloop fashion (Maturana and Varela, 1980; Porr and W¨org¨otter, 2005). To recreate closed-\nloop navigation in a robot it is sufﬁcient to directly connect a robot’s sensors to its mo-\ntor effectors, and the speciﬁc excitatory or inhibitory connections determine the control\nstrategy, as detailed in Braitenberg (1986). These sensor-effector connections represent\na single closed-loop controller, which produces stereotyped behaviour in response to\nan immediate stimulus. For this reason, closed-loop control by itself is not suitable to\nformulate complex navigation plans.\nA wide variety of techniques have been developed over the years to achieve safe\nspatial navigation in complex scenarios. These can be learning-based, reactive, or a\ncombination of the two. Reinforcement Learning (RL) is possibly the most popular\nlearning-based paradigm. In RL, an agent learns to associate actions with speciﬁc inputs\n2\n\nor ”states” through repeated interactions with the environment, aiming to maximise the\ncumulative reward (Sun et al., 2021). Rewards represent feedback for the action taken,\nmaking for a closed-loop output control paradigm. RL has been very successful in\nrecent years, when supplemented with techniques such as deep learning (Mnih et al.,\n2015; Silver et al., 2016; OpenAI, 2023). RL models are inherently slow to train for\ntwo main reasons. Firstly, rewards are sparse. Secondly, in output control, feedback\nabout the action can only be obtained after the action has been completed; the action\nwill therefore be corrected in the next training episode. Contrast this with input control,\nwhere feedback on the action is continuously provided by sensors and motor outputs\ncan be corrected immediately. The outputs of a naive RL model are therefore typically\nassociated with high error and/or low reward (cfr. benchmarks in Thodoroff et al. 2022).\nFor example, Mila and Pal (2019) developed a Real-Time Actor-Critic paradigm which,\ndespite being able to achieve faster convergence than the classical Soft Actor-Critic\none, still required hundreds of thousands of training steps, which could take hours or\ndays to complete (Dalton and Frosio, 2020). As a consequence, the training of RL\nmodels typically occurs ofﬂine. To this day, fully online RL remains unfeasible, with the\nconsequence that an automated agent is not able to acquire knowledge tailored to its own\nenvironment in real-time. Instead, large models must be trained in order for a system to\nbe able to generalise to a variety of unseen scenarios. This is not only computationally\nexpensive, but also compromises the transparency of the model learned, especially if\nvery large and/or trained on cloud platforms (Vasudevan et al., 2021).\nReactive algorithms, on the other hand, simply generate trajectories for a robot to\nfollow based on the position of obstacles or targets in the environment without requiring\n3\n\nany training. Trajectories are usually calculated based on nodes in a graph, randomly\ngenerated (see Karur et al., 2021 for an in-depth review), or tailored to sensory inputs\nusing force ﬁelds (e.g. Vector Field Histogram (Borenstein and Koren, 1991) or Artiﬁ-\ncial Potential Fields (Khatib, 1985)). From a control theory perspective, trajectories can\nbe viewed as a series of desired inputs for a single controller, representing the desired\nlocations of a robot in a global frame of reference. Loop closure in this case consists\nof providing feedback on how closely the robot follows its trajectory. Trajectory fol-\nlowing is a form of input control; however, feedback input is usually provided by an\nexternal observer (e.g. a camera placed on the ceiling), as a robot is oblivious to its\nown objective trajectory. Contrast this with a Braitenberg vehicle, which only needs\ninformation about the location of a disturbance (e.g. obstacle or target) relative to itself\nto successfully perform a control action.\nLike in RL, motor plans for robots should consist of sequences of actions, taken in\nresponse to inputs. However, in even simple animals like the common fruit ﬂy, complex\nplans in unknown environments can be formulated completely on-the-ﬂy (Honkanen et al.,\n2019). This challenges the notion that learning through trial and error is necessary at\nall for planning. In Spelke and Kinzler (2007) the concept of “core knowledge” is in-\ntroduced. This is intended as a seemingly innate understanding of basic properties of\none’s environment such as physics and causality, and is backed by studies on newborn\nanimals. An agent equipped with core knowledge can form reasonable predictions over\nthe outcome of an action without needing to perform it, which beneﬁts the efﬁciency\nof the decision making process. A contribution in the direction of providing a system\nwith innate knowledge has been attempted in few-shot RL. In this paradigm, the goal\n4\n\nis for a system to be able to choose and adapt policies to unseen scenarios with just\na few training examples. This is achieved by using meta-learning (Luo et al., 2021;\nBing et al., 2023) or transfer-learning (Sun et al., 2023) algorithms, or both (Shu et al.,\n2023) which, in brief, use previously trained RL models as a foundation for a novel\nmodel to be trained. While able to outperform pure RL in terms of training time, both\nthe preexisting and the novel models are trained ofﬂine, suggesting that the response\ntime of few-shot learning may still be unsuitable for real-time training.\nOur research addresses the problems of how to achieve multi-step-ahead planning\nusing closed-loop control, and of how to implement core knowledge in an agent as a\ntool to aid decision-making. To tackle the ﬁrst problem, we deﬁne various control loops,\nwhich we will call “tasks”, each representing a distinct control strategy. For example,\nwe may have a loop which only counteracts obstacles by turning left, or one that only\nturns right, etc. Control loops are contingent to speciﬁc sensory inputs that make them\nnecessary (Porr and W¨org¨otter, 2005). They are therefore temporary and can be chained\nsequentially in a variety of combinations. As for the second problem, we introduce core\nknowledge (Spelke and Kinzler, 2007) as a simulation which runs in parallel to and mir-\nrors the real world. This allows us to simulate sequences of tasks representing potential\nplans. Our framework, therefore, is characterised by the the concurrent operation of\ntemporary closed-loop controllers both physically and in simulation. This setup neces-\nsitates an overarching supervising module (Porr and W¨org¨otter, 2005) which oversees\nthe task creation process. This module, which we will call “Conﬁgurator” (cfr. LeCun\n(2022)) directs the simulation, interprets its results to formulate a plan, and executes\nplans by creating and terminating tasks accordingly. Crucially, both task execution and\n5\n\nplanning are formulated as homeostatic control problems: in the former, motor outputs\nare generated in order to counteract sensory stimuli which pose a disturbances to a de-\nsired state, and in the latter, a plan is chosen which maximises the time spent in that\nstate.\nThe paper is organised as follows. In Section 2.2 we describe in detail the compo-\nnents of the proposed framework; in Section 2.3 we discuss the features of our speciﬁc\nimplementation; in Section 3 we present the results of the experimental validation of\nthe framework; in Section 4 we discuss the results in light of the state of the art.\n2\nMethods\nH\nP\nE\nM\nD\nAgent\nEnvironment\n+\nA\nB\nD\nE\nM\nFigure 1: A) closed-loop obstacle avoidance. B) a closed-loop controller. D: dis-\nturbance (i.e. collision point), E: sensor error signal between robot and disturbance,\nM: motor output, P: environmental transfer function, H: transfer function of the\nagent/robot.\n6\n\n2.1\nOverview of proposed approach\nIn Figure 1A we present an informal deﬁnition of a task. A task begins when a distur-\nbance (in this case obstacle D) enters the robot’s sensor range. The sensors produce a\nsignal E which is relayed directly to the motor effectors which produce reﬂex motor\noutput M. A task executes until the disturbance ceases to produce a signal, at which\npoint the task simply terminates. When no disturbance is present, the system falls back\non a “default” task which characterises the resting state. Importantly, unlike in classical\nclosed-loop control, where the control problem is tackled by a single controller at all\ntimes, each task is discarded upon termination and may be replaced by any other task.\nFigure 2 provides an overview of how tasks and core knowledge can be combined\nto generate plans in the form of task sequences. Tasks are denoted with letter T, core\nknowledge is depicted as a thought balloon (Figure 2A-B), and the Conﬁgurator is\nrepresented in Figure 2C. The “default” behaviour is in this case arbitrarily deﬁned as\ndriving straight.\nIn the beginning, the Conﬁgurator is executing an initial task T0, depicted in Fig-\nure 2D-E, where a robot by default drives straight ahead (this task type is denoted by\nTS) with no disturbance to counteract (D0 = ∅). Figure 2A represents the use by the\nConﬁgurator of core knowledge to simulate tasks ahead of the present: the simulation\nresults represent the scenarios likely to arise from the execution of the sequence. These\nscenarios, comprised of tasks and the disturbances encountered as a result of executing\nthem, can be collected in a searchable structure, as depicted in Figure 2B. Nodes in this\nstructure are denoted as q, and they each contain a task and any disturbance (denoted as\na star) encountered in its simulation .\n7\n\nheading\ndirection\nT is destroyed\nwhen D1=\nR\nØ\nD1\nDestruct\nD\nE\nF\nG\nCon!gurator\nD0=Ø\nH\nP\nE\nMS\n+\nMR\nH\nE\nD2=Ø\nH\nI\nT1\nT2\nT3\nT4\nT5\nT0\nT2\nT4\nT5\nA\nB\nC\nT =T\n0\nS\nT =T\n0\nR\nT =T\n0\nS\nq0\nq2\nq3\nq4\nq5\nD1\nD1\nT1 q1\nP\n+\nH\nP\nE\nMS\n+\nT3\nFigure 2: Example of the multi-step-ahead planning procedure carried out by the Con-\nﬁgurator. Disturbances are indicated with stars.\nThe initial task T0 is represented in Figure 2B in node q0, and from it stem three\ntask sequences. First, default task T1 is created: through its simulation (Figure 2A),\nthe Conﬁgurator ﬁnds that the robot will collide with obstacle D1 (depicted as a star)\nrepresented by the wall in front. Task T1 is in node q1 in Figure 2B. The second control\nstrategy is a left turn (denoted with type TL) represented by task T2, which executes\nsuccessfully in simulation. Task T2 is followed by default task T3, which ends in a\ncollision. This sequence can be observed in Figure 2B as nodes q2, q3. Lastly, the\n8\n\nConﬁgurator explores a third strategy of following T0 with task T4, a right turn (denoted\nwith type TR). T4 executes without disturbances and is followed by default task T5.\nThese tasks are nodes q4, q5 in Figure 2B.\nThe process of ﬁnding a plan involves a search over the structure in Figure 2B. In\nour case, this search produces the disturbance-free sequence T4, T5 (indicated by thick\narrows in Figure 2B), which will be queued for execution. The execution is shown in\nFigures 2F-G: initial task T0 has been overwritten as a task of type TR, constructed to\ncounteract the obstacle D1. As predicted, no disturbances are found to arise during\nits execution (the simulation is omitted): as in Figures 2H-I, when T0 ends, it is again\noverwritten as another default task TS, initialised with a void disturbance D2 = ∅.\n2.2\nTheoretical foundations\n2.2.1\nTasks as closed-loop controllers\nFormally, we deﬁne tasks as closed-loop controllers, as depicted in Figure 1B. Tasks are\ncontingent to a disturbance D which enters the Environment transfer function P, and\ngenerates an error signal E. This signal is used by the Agent (i.e. the robot) transfer\nfunction H to generate a motor output M aimed at counteracting the disturbance (e.g.\navoiding the obstacle in Figure 1A). Once the disturbance is counteracted, the error\nsignal becomes zero: at this point, the control motor output M is no longer needed and\nthe execution of the control loop terminates.\nWe deﬁne three discrete types of tasks representing the behaviours of driving straight\nahead (TS), turning left (TL), and turning right (TR), and we call the corresponding out-\nputs MS, ML, MR, respectively. Of these, we arbitrarily set tasks TS as the “default”\n9\n\n(see Section 1) behaviour, so that in a disturbance-free scenario the robot just drives\nstraight ahead. Further, we allow our robot to exhibit obstacle avoidance and target pur-\nsuit. We therefore divide disturbances between obstacles and targets by assigning them,\nrespectively, a label through the function type : D →{obstacle, target}.\n2.2.2\nHybrid control\nThe simulated task-chaining process has a discrete domain i.e. the task which is being\nsimulated, and a continuous one, i.e. the evolution of sensory inputs over time dur-\ning task simulation. For this reason, the rules underlying task simulation can be sum-\nmarised using a nondeterministic hybrid automaton (Henzinger, 2000; Raskin, 2005;\nLafferriere et al., 1999).\nA hybrid automaton is a tuple HA = (T, C, F, K, I, J, R), where\n• T is a ﬁnite set of control modes, or the discrete states of the system.\n• C is a set of variables representing the continuous state of the system. We use ˙C\nto represent the derivatives of C, or the evolution of the continuous variables over\ntime, and “primed” variables C ′ to denote the valuation of C with which the next\ncontrol mode is initialised.\n• K ⊆T×T represents the edges, or permitted transitions between control modes\n• F, I are functions assigning each a predicate to each control mode.\nF : T →C ∪˙C is the ﬂow function. F(Tx) deﬁnes the possible continuous\nevolutions ˙C of the system in control mode Tx.\n10\n\nI : T →C is the invariant function. I(Tx) represents the possible valuations for\nvariables C in control mode Tx.\n• J, R are functions which assign a predicate to each edge.\nJ : K →C is the jump function. J(k) is a guard which determines when the\ndiscrete control mode change represented by edge k ∈K is allowed based on the\nvaluation of the continuous component C.\nR : K →C∪C ′ is the reset function. R(k) assigns to edge k possible updates for\nthe variables when a discrete change occurs. This is represented by an assignment\nto primed variables C ′.\nIn our implementation (discussed in more depth in Section 2.3), the discrete “control\nmodes” are tasks and the continuous variables are sensory inputs. The ﬂow functions\nare the velocities corresponding to task-speciﬁc motor outputs, the invariant predicates\ndeﬁne inputs to which each task is contingent, the jump guards set conditions which\nmust be met by the continuous inputs for a transition along a certain edge, and resets\nassign values to the continuous variables C ′ which will determine the initial inputs at\nthe start of a certain task.\nThe unfolding over time of a hybrid automaton can be represented as a transition\nsystem (Q, ֒→), where Q = T × C is a set of states, and ֒→⊆Q × Q is a transition\nrelation corresponding to edges connecting states. This transition system represents a\nmodel of the environment constructed through simulated interaction. We denote\nPre(q) = {q ′ ∈Q | (q ′ ֒→q)}\n(1)\n11\n\nPost(q) = {q ′ ∈Q | (q ֒→q ′)}\n(2)\nFurther, we deﬁne a path ρ = q0q1...qn where qi ∈Post(qi−1)∀i ≥1, where q0 is\nthe root state in the transition system. Paths represent sequences that may potentially\nconstitute a plan.\n2.2.3\nThe Conﬁgurator as a supervisor\nWe deﬁne the Conﬁgurator as a type of supervisor (Ramadge and Wonham, 1984):\nConﬁgurator = (HA, E, P, goal)\n(3)\nwhere HA is a hybrid automaton as described in Section 2.2.2, goal = {Dgoal} is\nthe system’s overarching goal, or the disturbance which the plan aims to counteract,\nE : HA →Q× ֒→is an “explorer” function which generates transition system G =\n(Q, ֒→) representing possible runs of HA, and P : Q× ֒→×goal →Q is a “planner”\nfunction which extracts a goal-directed path from a transition system.\n2.3\nImplementation\n2.3.1\nRobot\nWe implement the outlined framework on an indoor robot based on the Alphabot1\n(see Figure 3A). The robot is equipped with a Rasbperry Pi Model 3b+ (Broadcom\nBCM2837B0 processor with a 1.4 GHz clock speed and 1 GB RAM), an A1 Slamtec\n2D LIDAR sensor and two 360o continuous rotation servos which move the wheels.\n1https://www.waveshare.com/alphabot-robot.htm\n12\n\nThe LIDAR callback runs at 5 Hz and the motors are updated at 10 Hz. We deﬁne the\nmotor update as stepmotor.\n2.3.2\nCore knowledge as a physics simulation\nWe represent core knowledge as simulation in the physics engine Box2D2 where the co-\nordinates represents the robot’s local frame of reference. All data pertaining to position\nand its derivatives are expressed as 2D transforms of the form {u, θ}, where u = (x, y)\nrepresents the position in Cartesian coordinates in meters and θ is the orientation in\nradians of the object. Obstacles in Box2D are objects of dimensions 0.001m x 0.001m\nlocated at coordinates extracted from LiDAR input. The robot was modelled as a simple\nrectangle of dimensions 0.22m× 0.18m on the local x and y axis, respectively, with the\ncenter of mass shifted forward on the local x axis by 0.05m (see Figure 3A).\nThe physics simulation may progress for an arbitrarily long (simulated) time, and\nthe world and its content are updated with each arbitrarily small time step stepBox2D.\nSimulated sensors can be added to objects which report collisions between objects. Task\nsimulation is interrupted as soon as a collision is reported, and the ﬁrst point of contact\nbetween ﬁxtures is returned as the disturbance.\nWe limit the range of the sensor data used for the simulation to r = 1m from the\norigin. This represents a planning horizon, and the simulation stops when its limit is\nreached. The simulation is advanced using a kinematic model of the robot with a time\nstep (stepBox2D =\n1\n60s), 3 position iterations, 8 velocity iterations so that a simulation\ncarries on for a variable number of steps, indicated as nBox2D.\n2https://box2d.org/\n13\n\nchassis centroid\n0.05 m\nwheel\n+\n0.18 m\n0.22 m\nLIDAR\ncenter of mass\n+\nA\nB\n+\nx\ny\nstepDistance\n0.2m\nC\n+\n0.16m\nTS\nTL , TR\nFigure 3: A: Schematic representation of the robot with dimensions and center of mass\nannotated. The shaded areas in B and C represent, the boundaries for the inclusion of\nobjects in the simulation of tasks TS (B), TL and TR (C).\nThe speed of the Box2D simulation is sensitive to the number of simulated objects:\nto reduce computational demands, we perform some simple ﬁltering on the LiDAR co-\nordinates as depicted in Figure 3B-C. For simulating tasks of type TS, we only represent\npoints located within a distance stepDistance from the robot’s center of mass, where\nstepDistance denotes a ﬁnite distance range in meters for that task type. For left and\nright turns TL and TR in Figure 3C we only retain points located within a radius of 0.16m\nfrom the robot’s center of mass. The ﬁltered data thus obtained is further ﬁltered us-\ning one of the following techniques. The ﬁrst technique, “worldbuilding1”, consists of\napproximating each coordinate to two decimal places and eliminating redundant value.\nFor the second, “worldbuilding2”, only every other point in the set constructed using\n“worldbuilding1” is retained. Simulation results can be used to determine motor com-\nmands for task execution in the real world. We calculate each motor command as a\nduration expressed as a number of motor update intervals\nnmotor = nBox2D · stepmotor · stepBox2D\n(4)\nwhere nBox2D is the number of steps of size stepBox2D taken to bring a task to termina-\n14\n\ntion in simulation and nBox2D · stepBox2D calculates the simulated time which the task\nhas taken.\n2.3.3\nHybrid automaton\nThe hybrid automaton modelling the simulated task-switching process is depicted in\nFigure 4. The discrete control modes corresponding to tasks T = {TS, TR, TL}, are\nTS\nD\n=Ø\nobs\n||u ||\nsD\ndist ≤\nD=Ø, Reset 1\nReset 1\nD’= {D\n}\ngoal\nc’ =c\nself\nself\nc’ ={(0,0),0}\ndist\nTL\nD= {D } AND\ninit\n||\n||\nθ\nπ\ndist ≤/2\nc =M\nself\nL\nc =M\ndist\nL\n.\n.\nD\nØ,\nobs≠\nReset 1\nReset 2\nD’ = D\ninit\ninter\nc’ =c -c\nself\nself\ndist\nc’ ={(0,0),0}\ndist\nD=Ø, Reset 1\nReset 1 if D\n=Ø\nobs\nReset 2 if D\nØ\nobs ≠\nReset 1 if D\n=Ø\nobs\nReset 2 if D\nØ\nobs ≠\nTR\nD= {D } AND\ninit\n||\n||\n-\nθ\nπ\ndist ≤\n/2\nc =M\nself\nR\nc =M\ndist\nR\n.\n.\nFigure 4: HA: nondeterministic hybrid automaton modelling the closed-loop behaviour\nof the robot. We deﬁne udist, θdist ∈cdist, and sD = stepDistance.\ninscribed in circles connected by arrows representing the edges K. Continuous vari-\nables C = {cself, cdist, D} correspond to sensor inputs in the form of 2D transforms.\nThese inputs are: the position of the robot cself, the change in the robot’s position since\nthe start of the task cdist, and disturbance set D = {Dinit, Dinter}, comprised of the\n15\n\ndisturbance to which the task is contingent Dinit, and the disturbance interrupting task\nexecution Dinter. Flow F predicates are annotated outside of each circle. Invariant I\npredicates are annotated inside each circle. Jump J and reset R predicates are located\nby the corresponding edges.\nTask TS models the behaviour of the system when it is driving straight, while tasks\nTL and TR model the behaviours of turning left and right, respectively. As indicated\nby the respective ﬂow predicates, the position of the robot cself and the position change\nsince the start of the task cdist evolve at a rate denoted by task-speciﬁc velocities MS =\n{(0.098m/s, 0m/s), 0rad/s} for driving stright, ML = {(0m/s, 0m/s), 1.04rad/s} for left\nturns, and MR = {(0m/s, 0m/s), −1.04rad/s} for right turns, all in local coordinates.\nThese values were calculated applying simple robot kinematics to empirical velocity\nmeasurements.\nAs indicated by the corresponding invariant predicate, task TS executes as long as\nno obstacles (deﬁned as Dobs = {D ∈D | type(D) = obstacle}) are present and the\nrobot has not yet covered a ﬁxed distance stepDistance (abbreviated to sD in Figure).\nAs indicated by the respective invariant predicates, tasks TL and TR keep executing as\nlong as the only disturbance in the continuous variables is the one on which the task\nis contingent (predicate D = {Dinit}), and the angle described by the turn has not yet\nreached π\n2 rad (for task TL), and −π\n2rad for task TR. Note that these invariant predicates\nstill allow for a turn to execute in the case a task may be initialised with an empty\ndisturbance.\nTransitions along edge (TS, TS) are permitted if the terminated task has not encoun-\ntered obstacles, as indicated by jump predicate Dobs ̸= ∅. Transitions along edges\n16\n\n(TL, TS) and (TR, TS), are permitted if the left or right turns are terminated with no\ndisturbances, as indicated by jump predicates D = ∅. Transitions along edges (TS, TL)\nand (TS, TR) are always permitted, as indicated by the lack of jump guards.\nThe boxes on the right-hand side of Figure 4 contain the two reset predicates,\n“Reset1” and “Reset2”, which assign to primed variables C ′ depending on whether\nthe most recently simulated task is terminated without disturbances, or if it encoun-\nters an obstacle, respectively. For all reset predicates, every time the system switches\ncontrol mode, the distance covered in the previous task is reset to a zero transform\nc ′\ndist = {(0m, 0m), 0rad}. The “Reset1” predicate states that the next task will start\nwhere the previous one ended (c ′\nself = cself) and will be aimed at reaching the goal (\nD ′ = {Dgoal}). The “Reset2” predicate states that the next task will be primed to coun-\nteract the obstacle (D ′\ninit = Dinter) which interrupted the previous task, and it will start\nwhere the previous task began (c ′\nself = cself −cdist). “Reset1” always applies to edges\n(TS, TS), (TL, TS), (TR, TS), and to edges (TS, TL) and (TS, TR) if the continuous state\nC does not contain obstacles (Dobs = ∅). If obstacles are present (Dobs ̸= ∅) in these\nlast two edges, “Reset2” applies.\n2.3.4\nThe explorer E\nThe explorer E uses a best-ﬁrst approach to construct transition system G from automa-\nton HA. The priority of expansion of a state is deﬁned by its cumulative past and\nexpected cost (cfr. Hart et al. 1968), which we will deﬁne as function φ : Q →R. Ad-\nditionally, we use a heuristic function σ : Q →{0, 1} to prevent the robot from turning\nmore than 180 degrees on the spot and getting stuck rotating in place.\n17\n\nWe calculate cost evaluation function φ as:\nφ(q) = γ(q) + χ(q)\n(5)\nwhere q are states in transition system G, γ(q) calculates the past cost associated with\nstate q and χ(q) is a heuristic function which estimates the cost to reach goal Dgoal from\nstate q.\nWe deﬁne the past cost function γ as:\nγ(q) =\n\n\n\n\n\n\n\n\n\n0\nif Dinter = ∅\n2r−∥ustart−uinter∥\n2r\nif Dinter ̸= ∅\n(6)\nwhere Dinter is a disturbance encountered in state q, 2r represents the maximum possi-\nble distance that the robot can be from a disturbance, and ∥ustart−uinter∥represents the\nEuclidean distance between task start position ustart = uself −udist (see Section 2.2.2)\nand disturbance Dinter.\nWe deﬁne the cost heuristic function:\nχ(q) =\n\n\n\n\n\n\n\n\n\n0\nif goal = ∅\n∥uself−ugoal∥\n4r\notherwise\n(7)\nwhere ∥uself −ugoal∥represents the distance between the robot’s position at the end of\nthe simulation of state q and target Dgoal. We further deﬁne heuristic function σ as:\nσ(q) : ∥uself −upre∥= 0 ∧|θself −θpre| ≥π\n(8)\n18\n\nwhere 2D transforms {uself, θself} and {upre, θpre} represent the robot’s position and\norientation at the end of the simulation of state q and Pre(Pre(q)), respectively.\nAlgorithm 1 provides pseudocode for explorer E. We start from a transition system\nG = {q0} and priority queue PQ = {q0}, where q0 is a starting state representing the\ntask being executed by the robot. The corresponding task is immediately terminated,\nin order for the system to explore the scenarios that would arise if the robot were to\nchange its behaviour immediately, without waiting for the task at state q0 to terminate.\nWe use qexp to denote the state with the lowest evaluation φ(qexp) in the priority\nqueue; its out-edges are explored with the highest priority. We further denote the state\nwhose out-edges are actively being explored with qexp0 and the frontier state with qexp1.\nAny permitted transitions from state qexp0 are explored in a depth-ﬁrst fashion until\nfrontier state qexp1 contains a task TS (zero to two states ahead of state qexp). After the\nexpansion of qexp is complete, qexp is reset to the node which has the smallest φ value in\nthe priority queue, and the process repeats until a path is found which leads to a goal or\nthe planning horizon. Note how this implies that only tasks of the TS type are evaluated\nand that qexp is, therefore, always a task TS.\n2.3.5\nPlanner P\nThe model generated by the explorer E is used by planner P deﬁned in Section 2.2.3,\nwhich returns a path ρplan which brings the robot as close to the goal as possible. For-\nmally we write:\nρplan = {q0, q1, q2, ...qn ∈G | Post(qn) = ∅∧qn = min\nq∈Q φ(q)}\n(9)\n19\n\nAlgorithm 1 Explorer E\ninputs: HA\ninitialise PQ = {q0}, G = {q0}, qexp = q0\nsimulate the termination of q0: set D assumed empty, cself = {(0m, 0m), 0rad}\nrepeat\nremove qexp from PQ\nfor all edges k = (qexp, qexp1) for which jumps are permitted do\nqexp0 = qexp\nrepeat\nsimulate Texp1 ∈qexp1\nset qexp0 = qexp1\nuntil Texp1 = TS\nif σ(qexp1) = 0 then\nadd Texp1 to PQ\nend if\nend for\nset qexp = minq∈P Q φ(q)\nuntil no more jumps are permitted from qexp or Dgoal has been reached\nreturn G\n3\nResults\nWe test the decision-making process by applying it to two real-world problems: one of\navoiding of being trapped in a cul-de-sac and one of simultaneous collision avoidance\nand target pursuit. We contrast our new approach against a Braitenberg-style control\nstrategy which uses a single closed-loop controller and therefore exhibits stereotyped\nbehaviour.\nSequences of tasks and the speciﬁcations of the motor commands pertaining to each\ntask are planned at the beginning of the experimental task and do not undergo correction\nthroughout execution. The amount of time taken by the robot to formulate a plan is\nmeasured, as well as the total amount of objects simulated in Box2D and the number of\nstates in the transition system, which are equivalent to the number of tasks simulated.\n20\n\n3.1\nExperiment 1: cul-de-sac avoidance\nOur ﬁrst experiment tests our approach in a cul-de-sac avoidance scenario. This is a\nclassical scenario where reactive behaviour is not sufﬁcient to prevent getting stuck in a\ncul-de-sac. We set goal = ∅and stepDistance = 0.5m. The experiment was repeated\nsix times with “worldbuilding1”, and twelve with “worldbuilding2”.\nFigure 5 depicts a comparison of the behaviour planned using our new multiple-\nloop framework (Figure 5A), and the single-loop reactive behavioural case (Figure 5B),\nand the reasoning process employed using our framework (Figure 5C). As can be ob-\nserved from the trance in Figure 5A, when the robot uses our multi-step-ahead planning\nstrategy, it identiﬁes the cul-de-sac immediately and, to prevent entering it, turns to\nits right which allows it to carry on undisturbed. By contrast, in the single-loop con-\ntrol experiment, the robot enters the cul-de-sac, as it is outside of the single loop’s range\nstepDistance. Moreover, as signiﬁed in yellow lightning, some collisions occur during\nexecution.\nFigure 5C offers a visual representation of the construction of transition system\nG constructed in one of the experimental runs and the plan extracted from it. Initial\nstate q0 is immediately terminated and from its end position (see robot schematic in\nFigure 5C), transition system building starts. The robot may safely proceed straight\n(state q1) or turn right and then advance (states q4, q5). The sequence q2, q3 cannot be\nsafely executed since, in state q3, the robot collides with the wall ahead of it. States q1\nand q5 will have the same φ value and rank higher in the priority queue than state q3.\nThe cul-de-sac is discovered as all the sequences following state q1 (namely, state\nq6, states q7, q8 and states q9, q10. State q5 will now therefore have highest priority,\n21\n\nand will be expanded. Three other state sequences are added to this node. Of these,\ndriving right and straight (states q12, q13) leads to a crash, while state q11 and sequence\nq14, q15 are disturbance-free. In this case, state q11 reaches the limit of the planning\nhorizon before state q15, for which reason planning can end. The ﬁnal plan, indicated\nby the bold arrow in Figure 5C, is ρplan = q0q4q5q11, where q0 = (TS, ·), q4 = (TR, ·),\nq5 = (TS, ·), q11 = (TS, ·).\nFigure 5D displays the descriptive statistics pertaining to simulation and planning\nspeed. In runs where worldbuilding1 was used to create objects in the Box2D environ-\nment (in Figure, column labelled ”cul-de-sac, wb1”), an average of 19.3 states (min: 16,\nmax: 26) were simulated with 93.8 average total objects (SD=39.93) created over the\ncourse of the simulation. This took on average 0.295s (SD=0.069s). With worldbuild-\ning2 (in Figure, column labelled ”cul-de-sac, wb2”), an average of 20.16 states (min:\n11, max: 21) were simulated with 40.75 average total objects (SD=20.67) in 0.166s on\naverage (SD= 0.062s).\n3.2\nExperiment 2: avoidance and target behaviour\nFigure 6 represents the results of an experiment where the robot had to reach a tar-\nget goal = {Dgoal = {(1.0m, 0m), 0rad}} while driving around an obstacle. We set\nstepDistance = 0.22m corresponding to the diameter of the robot, as in (Ulrich and Borenstein,\n2000). The experiment was repeated ten times with “worldbuilding1”, and eight with\n“worldbuilding2”.\nAs shown in Figure 6B, in the reactive behaviour experiment the robot remains\nstuck behind the obstacle, while in our approach (Figure 6A) the agent is able to safely\n22\n\nq14\nq1\nq2\nq3\nq4\nq5\nq6\nq7\nq8\nq9\nq10\nq11\nq12\nq13\nq15\nC\nAA\nB\nAverage times for transition\nsystem construction (collision avoidance)\nD\nTime (s)\nFigure 5: Behaviour exhibited by the robot in the obstacle-avoidance experiment. Panel\nA: with the proposed multiple-loop planning, panel B: with planning over a single con-\ntrol loop, panel C: reasoning in multiple-loop planning, D: descriptive statistics.\nformulate a plan to reach the target.\nFigure 6C depicts the transition system produced in one of the experimental runs.\nAs shown, the robot can safely advance towards the target Dgoal by entering states\n23\n\nA\nB\nq1\nq2 q4\nq5\nq3\nq6\nq7\nq8\nq9 q10\nq11\nq12\nq13\nq14\nq15\nq18\nq17\nq16\nq19\nq20\nq21\nq22\nq23\nq24\nq25\nq26\nq31\nq28\nq30\nq33\nq35\nq27 q29\n34\nq\nq32\nC D\nAverage times for transition\nsystem construction (target pursuit)\nTime (s)\nFigure 6: Initial frame and tracking of the behaviour exhibited by the robot in the target-\nseeking/overtaking experiment. Panel A: with the proposed multiple-loop planning,\npanel B: with one-loop-ahead disturbance detection, panel C: reasoning in multiple-\nloop planning, D: descriptive statistics. The target is represented in A-B by a grey\ncross.\n24\n\nq1, q6. Further advancement through state q11 is prevented by an obstacle. The only\ncollision-free option for the robot is to turn left (state q12) and proceed in a direction\nperpendicular to the target (state q13). The robot will then turn towards the disturbance\nby choosing to turn right (state q19) and then proceed straight (q20). The robot will then\nimmediately attempt to return in line with the target by turning left and then proceeding\nstraight (states q24, q25), but these options are not collision-free. By transitioning to\nstate q21, the robot will drive straight towards the target; after this, it may safely turn\nright (state q29) and return in line with the target (state q30). The target is now very\nclose: by turning left (state q34), the robot positions itself to face it and advances (state\nq35) until it reaches it, at which point transition system G construction terminates. As\nindicated by the bold arrow in Figure 6C, the plan followed in Figure 6A is ρplan =\nq1q6q12q13q19q20q21q29q30q34q35, where q1 = (TS, ·), q6 = (TR, ·), q12 = (TL, ·), q13 =\n(TS, ·), q19 = (TR, ·), q20 = (TS, ·), q21 = (TS, ·), q29 = (TR, ·), q30 = (TS, ·), q34 =\n(TL, ·), q35 = (TS, ·).\nDescriptive statistics for the simulation and planning speed are shown in Figure 6D.\nUsing worldbuilding1 (in Figure, column labelled ”target, wb1”), the transition system\nwas made up of an average of 37.8 states (min: 36, max: 39), and each simulation\ncontained on average 114.4 objects (SD=21.08). The mean simulation time was 0.273s\n(SD= 0.016s). When worldbuilding2 (in Figure, column labelled ”target, wb2”) was\nemployed, the mean number of states was similar at 38.25 with the same minimum and\nmaximum values. The average number of objects was reduced to 64.38 (SD=11.70),\nand average simulation time dropped to 0.133s (SD=0.011s).\n25\n\n4\nDiscussion\nIn this paper we have introduced a novel decision-making process which exploits su-\npervisory control of a hybrid automaton representing temporary simulated closed-loop\ncontrollers, called “tasks”, representing each a different control strategy. The simu-\nlation of the task-switching process occurs in a physics engine which constitutes the\nsystem’s core knowledge. The performance of the decision-making process was tested\nin a real robot in a collision avoidance problem and a target pursuit one. The robot\nwas in both cases able to generate, in real-time, a model of tasks and their resulting\ndisturbances over a 1m range, which was used to extract a plan. We contrasted this\nperformance with a classical closed-loop system where planning can only occur one\ndisturbance into the future and the system behaves deterministically. This control case\nproduced less efﬁcient behaviour in the best case scenario and failed by colliding with\na wall or remaining stuck in the worst.\nWe construct a model of the environment as a hybrid transition system represent-\ning possible runs of a hybrid automaton. Problems concerned with the combination\nof discrete and hybrid control are referred to as Task And Motion Problems (TAMP).\nTAMPs are notoriously difﬁcult to solve in real-time: in recent years neural networks\nhave been used to train complex models aimed at combining planning over a hybrid\ntransition system and learning control policies in a variety of scenarios (Kim et al.,\n2022; Driess et al., 2021). This obviously requires ofﬂine training and has potential\nfor limited transparency and/or robustness depending on the depth of the network used.\nAdditionally, even with previous training, planning times in Kim et al. (2022) fell into\nthe order of hundreds of seconds; this is not suitable for real-world applications.\n26\n\nOnline approaches to hybrid control for navigation have also been proposed. In\nMavridis et al. (2019); Constantinou and Loizou (2018); Migimatsu and Bohg (2020),\nhybrid automata are constructed using various types of logic to check that they sat-\nisfy certain properties. Whether the control modes may be immediately available to\nthe system (Migimatsu and Bohg, 2020) or may need to be computed (Mavridis et al.,\n2019), the computation of the control-mode-speciﬁc ﬂow function can be an inef-\nﬁcient process.\nComputation statistics were not reported in Mavridis et al. (2019);\nConstantinou and Loizou (2018), however Migimatsu and Bohg (2020) report that the\nminimum time taken for convergence to an optimal continuous control was 0.85 sec-\nonds. A model-checking approach was proposed by\nChandler et al. (2023) over a\ndiscrete-state system, which was able to plan over a sequence of ﬁve actions in around\n0.01s. This suggests that the optimisation over continuous controls may be the bottle-\nneck for TAMP problems.\nWhile we do not focus on achieving smooth trajectories or more complex tasks such\nas grasping, we were able to achieve planning in real-time or near-real-time in a robot\nwith very limited computational power. The closed-loop task simulation allowed us to\neasily obtain an estimation of the commands for the motor effectors. Starting from a\nreasonable estimation of a control output may relieve some of the computational load\nresulting from its optimisation, which should occur online and in a closed-loop fashion.\nDespite the increased planning time in our experiments compared to Chandler et al.\n(2023), the speed performance recorded in our experiments is still within human reac-\ntion time range (Wong et al., 2015) and within real-time requirements of the robot.\nIn conclusion, our ﬁndings demonstrate that not only it is possible to formulate\n27\n\noptimal plans based on pure input control, but it is also achievable in real-time over\nthe discrete and continuous domains, thanks to core knowledge. We believe that our\nframework has potential to scale well to TAMP problems, in at least two dimensions,\nand provide a valuable complement to the state-of-the-art.\n5\nAcknowledgements\nThis work was supported by a grant from the UKRI Engineering and Physical Sci-\nences Research Council Doctoral Training Partnership award [EP/T517896/1-312561-\n05]; the UKRI Strategic Priorities Fund to the UKRI Research Node on Trustwor-\nthy Autonomous Systems Governance and Regulation [EP/V026607/1, 2020-2024];\nand the UKRI Centre for Doctoral Training in Socially Intelligent Artiﬁcial Agents\n[EP/S02266X/1].\nReferences\nZhenshan Bing, David Lerch, Kai Huang, and Alois Knoll. Meta-reinforcement learn-\ning in non-stationary and dynamic environments.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45:3476–3491, 3 2023. ISSN 19393539. doi:\n10.1109/TPAMI.2022.3185549.\nJohann Borenstein and Yoram Koren. The vector ﬁeld histogram—fast obstacle avoid-\nance for mobile robots. IEEE Transactions on Robotics and Automation, 7:278–288,\n1991. doi: 10.1109/70.88137.\n28\n\nV. Braitenberg. Vehicles: Experiments in Synthetic Psychology. MIT Press, 1986.\nChristopher Chandler, Bernd Porr, Alice Miller, and Giulia Lafratta. Model checking\nfor closed-loop robot reactive planning. Electronic Proceedings in Theoretical Com-\nputer Science, 395:77–94, 11 2023. doi: 10.4204/EPTCS.395.6.\nChristos C. Constantinou and Savvas G. Loizou. Automatic controller synthesis of\nmotion-tasks with real-time objectives.\nProceedings of the IEEE Conference on\nDecision and Control, 2018-December:403–408, 7 2018. doi: 10.1109/CDC.2018.\n8619825.\nSteven Dalton and Iuri Frosio. Accelerating reinforcement learning through gpu atari\nemulation. Advances in Neural Information Processing, 33:19773–19782, 2020.\nDanny Driess, Jung Su Ha, and Marc Toussaint. Learning to solve sequential physical\nreasoning problems from a scene image. International Journal of Robotics Research,\n40:1435–1466, 12 2021. ISSN 17413176. doi: 0.1177/02783649211056967.\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic\ndetermination of minimum cost paths. IEEE Transactions on Systems Science and\nCybernetics, 4:100–107, 1968. ISSN 21682887. doi: 10.1109/TSSC.1968.300136.\nThomas A. Henzinger.\nThe theory of hybrid automata.\nVeriﬁcation of Digital\nand Hybrid Systems, pages 265–292, 2000.\nISSN 1043-6871.\ndoi: 10.1007/\n978-3-642-59615-5 13.\nAnna Honkanen, Andrea Adden, Josiane Da Silva Freitas, and Stanley Heinze. The\ninsect central complex and the neural basis of navigational strategies. Journal of\n29\n\nExperimental Biology, 222, 2 2019. ISSN 00220949. doi: 10.1242/JEB.188854/\n2882.\nKarthik Karur, Nitin Sharma, Chinmay Dharmatti, and Joshua E. Siegel. A survey of\npath planning algorithms for mobile robots. Vehicles 2021, Vol. 3, Pages 448-468, 3:\n448–468, 8 2021. ISSN 2624-8921. doi: 10.3390/VEHICLES3030027.\nO. Khatib. Real-time obstacle avoidance for manipulators and mobile robots. Proceed-\nings - IEEE International Conference on Robotics and Automation, pages 500–505,\n1985. ISSN 10504729. doi: 10.1109/ROBOT.1985.1087247.\nBeomjoon Kim, Luke Shimanuki, Leslie Pack Kaelbling, and Tom´as Lozano-P´erez.\nRepresentation, learning, and planning algorithms for geometric task and motion\nplanning. International Journal of Robotics Research, 41:210–231, 2 2022. ISSN\n17413176. doi: 10.1177/02783649211038280.\nGerardo Lafferriere, George J Pappas, and Sergio Yovine. A new class of decidable\nhybrid systems. Hybrid Systems: Computation and Control: Second International\nWorkshop, pages 137–151, 1999.\nYann LeCun. A path towards autonomous machine intelligence. OpenReview.net, 6\n2022.\nQian Luo, Maks Sorokin, and Sehoon Ha. A few shot adaptation of visual navigation\nskills to new observations using meta-learning. Proceedings - IEEE International\nConference on Robotics and Automation, 2021-May:13231–13237, 2021.\nISSN\n10504729. doi: 10.1109/ICRA48506.2021.9561056.\n30\n\nHumberto R. Maturana and Francisco J. Varela. Autopoiesis and Cognition, volume 42.\nBoston Studies in the Philosophy of Science New York, N.Y., 1980. ISBN 978-90-\n277-1016-1. doi: 10.1007/978-94-009-8947-4.\nChristos N. Mavridis, Constantinos Vrohidis, John S. Baras, and Kostas J. Kyriakopou-\nlos. Robot navigation under mitl constraints using time-dependent vector ﬁeld based\ncontrol.\nProceedings of the IEEE Conference on Decision and Control, 2019-\nDecember:232–237, 12 2019.\nISSN 25762370.\ndoi: 10.1109/CDC40024.2019.\n9028890.\nToki Migimatsu and Jeannette Bohg. Object-centric task and motion planning in dy-\nnamic environments. IEEE Robotics and Automation Letters, 5:844–851, 4 2020.\nISSN 23773766. doi: 10.1109/LRA.2020.2965875.\nSimon Ramstedt Mila and Christopher Pal. Real-time reinforcement learning. 2019.\ndoi: https://doi.org/10.48550/arXiv.1911.04448.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,\nMarc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\nKing, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-\nlevel control through deep reinforcement learning. Nature, 518:529–542, 2 2015.\nISSN 00280836. doi: 10.1038/NATURE14236.\nOpenAI. Gpt-4 technical report, 2023.\n31\n\nB Porr and F W¨org¨otter. Inside embodiment-what means embodiment to radical con-\nstructivists? Kybernetes, 34:105–117, 2005.\nP. J. Ramadge and W. M. Wonham. Supervisory control of a class of discrete event\nprocesses. Analysis and Optimization of Systems, pages 475–498, 10 1984. doi:\n10.1007/BFB0006306.\nJean-Franc¸ois Raskin. An introduction to hybrid automata. Handbook of Networked\nand Embedded Control Systems, pages 491–517, 2005. doi: 10.1007/0-8176-4404-0\n21.\nYang Shu, Zhangjie Cao, Jinghan Gao, Jianmin Wang, Philip S. Yu, and Mingsheng\nLong. Omni-training: Bridging pre-training and meta-training for few-shot learning.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 45:15275–15291,\n12 2023. ISSN 19393539. doi: 10.1109/TPAMI.2023.3319517.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-\nbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529:484–490, 1 2016. ISSN 00280836. doi:\n10.1038/NATURE16961.\nElizabeth S. Spelke and Katherine D. Kinzler. Core knowledge. Developmental Science,\n10:89–96, 1 2007. ISSN 1363755X. doi: 10.1111/J.1467-7687.2007.00569.X.\n32\n\nHuihui Sun, Weijie Zhang, Runxiang Yu, and Yujie Zhang. Motion planning for mobile\nrobots - focusing on deep reinforcement learning: A systematic review. IEEE Access,\n9:69061–69081, 2021. ISSN 21693536. doi: 10.1109/ACCESS.2021.3076530.\nYuewen Sun, Kun Zhang, and Changyin Sun.\nModel-based transfer reinforcement\nlearning based on graphical model representations. IEEE Transactions on Neural\nNetworks and Learning Systems, 34:1035–1048, 2 2023.\nISSN 21622388.\ndoi:\n10.1109/TNNLS.2021.3107375.\nPierre Thodoroff, Wenyu Li, and Neil D Lawrence. Benchmarking real-time reinforce-\nment learning. Proceedings of Machine Learning Research, 181:26–41, 2022.\nIwan Ulrich and Johann Borenstein. Vfh*: Local obstacle avoidance with look-ahead\nveriﬁcation. pages 2505–2511, 2000.\nRama K. Vasudevan, Maxim Ziatdinov, Lukas Vlcek, and Sergei V. Kalinin. Off-the-\nshelf deep learning is not enough, and requires parsimony, bayesianity, and causality.\nnpj Computational Materials 2021 7:1, 7:1–6, 1 2021. ISSN 2057-3960. doi: 10.\n1038/s41524-020-00487-0.\nAaron L. Wong, Adrian M. Haith, and John W. Krakauer. Motor planning. Neurosci-\nentist, 21:385–398, 8 2015. ISSN 10894098. doi: 10.1177/1073858414541484.\n33",
    "pdf_filename": "Homeostatic_motion_planning_with_innate_physics_knowledge.pdf"
}