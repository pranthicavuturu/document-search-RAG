{
    "title": "1",
    "abstract": "Living organisms interact with their surroundings in a closed-loop fashion, where sen- sory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in 4202 voN 81 ]OR.sc[ 2v48351.2042:viXra",
    "body": "1\nHomeostatic motion planning with innate physics\nknowledge\n1\nLafratta, G.\n1\nPorr, B.\n2\nChandler, C.\n2\nMiller, A.\n1\nSchool ofEngineering,UniversityofGlasgow\n2\nSchool ofComputingScience, UniversityofGlasgow.\nKeywords: Planning,homeostasis,closedloop,autonomousagents\nAbstract\nLiving organisms interact with their surroundings in a closed-loop fashion, where sen-\nsory inputs dictate the initiation and termination of behaviours. Even simple animals\nare able to develop and execute complex plans, which has not yet been replicated in\n4202\nvoN\n81\n]OR.sc[\n2v48351.2042:viXra\nrobotics using pure closed-loop input control. We propose a solution to this problem\nbydefiningasetofdiscreteandtemporaryclosed-loopcontrollers,called“tasks”,each\nrepresentingaclosed-loopbehaviour. Wefurtherintroduceasupervisorymodulewhich\nhas aninnateunderstandingofphysicsandcausality,throughwhichit can simulatethe\nexecution of task sequences over time and store the results in a model of the environ-\nment. Onthebasisofthismodel,planscanbemadebychainingtemporaryclosed-loop\ncontrollers. The proposed framework was implemented for a real robot and tested in\ntwoscenarios as proofofconcept.\n1 Introduction\nLiving organisms interact with their surroundings through sensory inputs in a closed-\nloopfashion(Maturanaand Varela,1980;Porr andWo¨rgo¨tter,2005). Torecreateclosed-\nloopnavigationin arobot it issufficient to directlyconnect arobot’ssensors to itsmo-\ntoreffectors,andthespecificexcitatoryorinhibitoryconnectionsdeterminethecontrol\nstrategy,asdetailedinBraitenberg(1986). Thesesensor-effectorconnectionsrepresent\na single closed-loop controller, which produces stereotyped behaviour in response to\nan immediate stimulus. For this reason, closed-loop control by itself is not suitable to\nformulatecomplexnavigationplans.\nA wide variety of techniques have been developed over the years to achieve safe\nspatial navigation in complex scenarios. These can be learning-based, reactive, or a\ncombination of the two. Reinforcement Learning (RL) is possibly the most popular\nlearning-basedparadigm. InRL,anagentlearnstoassociateactionswithspecificinputs\n2\nor”states”throughrepeated interactionswiththeenvironment,aimingtomaximisethe\ncumulativereward (Sun et al., 2021). Rewards represent feedback for theaction taken,\nmaking for a closed-loop output control paradigm. RL has been very successful in\nrecent years, when supplemented with techniques such as deep learning (Mnihet al.,\n2015; Silveretal., 2016; OpenAI, 2023). RL models are inherently slow to train for\ntwo main reasons. Firstly, rewards are sparse. Secondly, in output control, feedback\nabout the action can only be obtained after the action has been completed; the action\nwillthereforebecorrectedinthenexttrainingepisode. Contrastthiswithinputcontrol,\nwhere feedback on the action is continuously provided by sensors and motor outputs\ncan be corrected immediately. The outputs of a naiveRL model are therefore typically\nassociatedwithhigherrorand/orlowreward(cfr. benchmarksinThodoroffet al.2022).\nForexample,Milaand Pal(2019)developedaReal-TimeActor-Criticparadigmwhich,\ndespite being able to achieve faster convergence than the classical Soft Actor-Critic\none, still required hundreds of thousands of training steps, which could take hours or\ndays to complete (Daltonand Frosio, 2020). As a consequence, the training of RL\nmodelstypicallyoccursoffline. Tothisday,fullyonlineRLremainsunfeasible,withthe\nconsequencethatanautomatedagentisnotabletoacquireknowledgetailoredtoitsown\nenvironmentinreal-time. Instead,largemodelsmustbetrainedinorderforasystemto\nbe able to generaliseto a variety of unseen scenarios. This is not only computationally\nexpensive, but also compromises the transparency of the model learned, especially if\nverylarge and/ortrained oncloud platforms(Vasudevanet al.,2021).\nReactive algorithms, on the other hand, simply generate trajectories for a robot to\nfollowbasedonthepositionofobstaclesortargetsintheenvironmentwithoutrequiring\n3\nany training. Trajectories are usually calculated based on nodes in a graph, randomly\ngenerated (see Karuret al., 2021 for an in-depth review), or tailored to sensory inputs\nusing force fields (e.g. Vector Field Histogram(Borenstein and Koren, 1991) or Artifi-\ncialPotentialFields(Khatib,1985)). Fromacontroltheoryperspective,trajectoriescan\nbe viewed as a series of desired inputs for a single controller, representing the desired\nlocations of a robot in a global frame of reference. Loop closure in this case consists\nof providing feedback on how closely the robot follows its trajectory. Trajectory fol-\nlowing is a form of input control; however, feedback input is usually provided by an\nexternal observer (e.g. a camera placed on the ceiling), as a robot is oblivious to its\nown objective trajectory. Contrast this with a Braitenberg vehicle, which only needs\ninformationaboutthelocationofadisturbance(e.g. obstacleortarget)relativetoitself\ntosuccessfullyperform acontrolaction.\nLike in RL, motor plans for robots should consist of sequences of actions, taken in\nresponsetoinputs. However,inevensimpleanimalslikethecommonfruitfly,complex\nplansinunknownenvironmentscanbeformulatedcompletelyon-the-fly(Honkanen etal.,\n2019). This challenges the notion that learning through trial and error is necessary at\nall for planning. In Spelke andKinzler (2007) the concept of “core knowledge” is in-\ntroduced. This is intended as a seemingly innate understanding of basic properties of\none’s environment such as physics and causality, and is backed by studies on newborn\nanimals. Anagentequippedwithcoreknowledgecanformreasonablepredictionsover\nthe outcome of an action without needing to perform it, which benefits the efficiency\nof the decision making process. A contribution in the direction of providing a system\nwith innate knowledge has been attempted in few-shot RL. In this paradigm, the goal\n4\nis for a system to be able to choose and adapt policies to unseen scenarios with just\na few training examples. This is achieved by using meta-learning (Luoet al., 2021;\nBing et al., 2023) or transfer-learning (Sun et al., 2023) algorithms, or both (Shu et al.,\n2023) which, in brief, use previously trained RL models as a foundation for a novel\nmodel to be trained. While able to outperform pure RL in terms of training time, both\nthe preexisting and the novel models are trained offline, suggesting that the response\ntimeoffew-shotlearningmay stillbeunsuitableforreal-timetraining.\nOur research addresses the problems of how to achieve multi-step-ahead planning\nusing closed-loop control, and of how to implement core knowledge in an agent as a\ntooltoaiddecision-making. Totacklethefirstproblem,wedefinevariouscontrolloops,\nwhich we will call “tasks”, each representing a distinct control strategy. For example,\nwe may have a loop which only counteracts obstacles by turning left, or one that only\nturns right, etc. Control loops are contingent to specific sensory inputs that make them\nnecessary(Porrand Wo¨rgo¨tter,2005). Theyarethereforetemporaryandcanbechained\nsequentiallyinavarietyofcombinations. Asforthesecondproblem,weintroducecore\nknowledge(Spelkeand Kinzler,2007)asasimulationwhichrunsinparalleltoandmir-\nrorstherealworld. Thisallowsustosimulatesequencesoftasksrepresentingpotential\nplans. Our framework, therefore, is characterised by the the concurrent operation of\ntemporary closed-loop controllers both physicallyand in simulation. This setup neces-\nsitates an overarching supervising module (Porr andWo¨rgo¨tter, 2005) which oversees\nthe task creation process. This module, which we will call “Configurator” (cfr. LeCun\n(2022)) directs the simulation, interprets its results to formulate a plan, and executes\nplansby creating and terminatingtasksaccordingly. Crucially,both taskexecutionand\n5\nplanningare formulated as homeostaticcontrolproblems: intheformer, motoroutputs\nare generated in order to counteract sensory stimuliwhich pose a disturbances to a de-\nsired state, and in the latter, a plan is chosen which maximises the time spent in that\nstate.\nThe paper is organised as follows. In Section 2.2 we describe in detail the compo-\nnents oftheproposed framework; in Section 2.3 wediscuss thefeatures ofourspecific\nimplementation; in Section 3 we present the results of the experimental validation of\ntheframework;inSection 4 wediscusstheresultsin lightofthestateoftheart.\n2 Methods\nA B\nD\nH\nE\nM\nAgent\nE\nM\nEnvironment\nP +\nD\nFigure 1: A) closed-loop obstacle avoidance. B) a closed-loop controller. D: dis-\nturbance (i.e. collision point), E: sensor error signal between robot and disturbance,\nM: motor output, P: environmental transfer function, H: transfer function of the\nagent/robot.\n6\n2.1 Overview of proposed approach\nIn Figure 1A we present an informal definition of a task. A task begins when a distur-\nbance (in this case obstacle D) enters the robot’s sensor range. The sensors produce a\nsignal E which is relayed directly to the motor effectors which produce reflex motor\noutput M. A task executes until the disturbance ceases to produce a signal, at which\npointthetask simplyterminates. Whenno disturbanceispresent, thesystemfalls back\nona“default”taskwhichcharacterisestherestingstate. Importantly,unlikeinclassical\nclosed-loop control, where the control problem is tackled by a single controller at all\ntimes,each task isdiscarded uponterminationand maybereplaced byanyothertask.\nFigure 2 provides an overview of how tasks and core knowledge can be combined\nto generate plans in the form of task sequences. Tasks are denoted with letter T, core\nknowledge is depicted as a thought balloon (Figure 2A-B), and the Configurator is\nrepresented in Figure 2C. The “default” behaviour is in this case arbitrarily defined as\ndrivingstraight.\nIn the beginning, the Configurator is executing an initial task T , depicted in Fig-\n0\nure 2D-E, where a robot by default drives straight ahead (this task type is denoted by\nT S) with no disturbance to counteract (D 0 = ∅). Figure 2A represents the use by the\nConfigurator of core knowledge to simulate tasks ahead of the present: the simulation\nresultsrepresentthescenarioslikelytoarisefromtheexecutionofthesequence. These\nscenarios, comprisedoftasksand thedisturbancesencountered as a resultofexecuting\nthem,canbecollectedinasearchablestructure,asdepictedinFigure2B.Nodesinthis\nstructurearedenotedasq,andtheyeachcontainataskandanydisturbance(denotedas\nastar)encountered initssimulation.\n7\nq q\nA B\n2 3\nD\nT T\n1 2 3\nT\nq q\n1 T T\nT T 0 0 1 1\n2 4\nT T\n4 5\nT T q q\n3 5 4 5\nDestruct\nC\nCon!gurator\nD F H\nT is destroyed\nR\nwhen D=Ø\n1\nE E E\nH H H\nM M M\nS R S\nP + P + P +\nD=Ø D D=Ø\n0 1 2\nE G I\nT =T T =T T =T\n0 S 0 R 0 S\nD\n1\nheading\ndirection\nFigure 2: Exampleof the multi-step-ahead planning procedure carried out by the Con-\nfigurator. Disturbancesare indicatedwithstars.\nThe initial task T is represented in Figure 2B in node q , and from it stem three\n0 0\ntask sequences. First, default task T is created: through its simulation (Figure 2A),\n1\nthe Configurator finds that the robot will collide with obstacle D (depicted as a star)\n1\nrepresentedbythewallinfront. TaskT isinnodeq inFigure2B.Thesecondcontrol\n1 1\nstrategy is a left turn (denoted with type T ) represented by task T , which executes\nL 2\nsuccessfully in simulation. Task T is followed by default task T , which ends in a\n2 3\ncollision. This sequence can be observed in Figure 2B as nodes q ,q . Lastly, the\n2 3\n8\nConfiguratorexploresathirdstrategyoffollowingT withtaskT ,arightturn(denoted\n0 4\nwith type T ). T executes without disturbances and is followed by default task T .\nR 4 5\nThesetasksare nodesq ,q inFigure2B.\n4 5\nThe process of finding a plan involves a search over the structure in Figure 2B. In\nour case, this search produces the disturbance-free sequence T , T (indicated by thick\n4 5\narrows in Figure 2B), which will be queued for execution. The execution is shown in\nFigures 2F-G: initial task T has been overwritten as a task of type T , constructed to\n0 R\ncounteract the obstacle D . As predicted, no disturbances are found to arise during\n1\nits execution (the simulation is omitted): as in Figures 2H-I, when T ends, it is again\n0\noverwrittenas anotherdefaulttask T S, initialisedwitha voiddisturbanceD 2 = ∅.\n2.2 Theoretical foundations\n2.2.1 Tasks asclosed-loopcontrollers\nFormally,wedefinetasksasclosed-loopcontrollers,asdepictedinFigure1B.Tasksare\ncontingent to a disturbance D which enters the Environment transfer function P, and\ngenerates an error signal E. This signal is used by the Agent (i.e. the robot) transfer\nfunction H to generate a motor output M aimed at counteracting the disturbance (e.g.\navoiding the obstacle in Figure 1A). Once the disturbance is counteracted, the error\nsignalbecomes zero: at thispoint,thecontrol motoroutputM isno longerneeded and\ntheexecutionofthecontrol loopterminates.\nWedefinethreediscretetypesoftasksrepresentingthebehavioursofdrivingstraight\nahead(T ),turningleft(T ),andturningright(T ),andwecallthecorrespondingout-\nS L R\nputs M ,M ,M , respectively. Of these, we arbitrarily set tasks T as the “default”\nS L R S\n9\n(see Section 1) behaviour, so that in a disturbance-free scenario the robot just drives\nstraightahead. Further,weallowourrobottoexhibitobstacleavoidanceandtargetpur-\nsuit. Wethereforedividedisturbancesbetweenobstaclesandtargetsbyassigningthem,\nrespectively,alabelthroughthefunctiontype : D → {obstacle,target}.\n2.2.2 Hybridcontrol\nThe simulated task-chaining process has a discrete domain i.e. the task which is being\nsimulated, and a continuous one, i.e. the evolution of sensory inputs over time dur-\ning task simulation. For this reason, the rules underlying task simulation can be sum-\nmarised using a nondeterministic hybrid automaton (Henzinger, 2000; Raskin, 2005;\nLafferriere et al., 1999).\nA hybridautomatonis atupleHA = (T,C,F,K,I,J,R),where\n• Tis afiniteset ofcontrolmodes,orthediscretestates ofthesystem.\n• C is a set of variables representing thecontinuous stateof the system. We use C˙\ntorepresentthederivativesofC,ortheevolutionofthecontinuousvariablesover\ntime,and“primed”variablesC′ todenotethevaluationofC withwhichthenext\ncontrolmodeis initialised.\n• K ⊆ T×Trepresentstheedges,orpermittedtransitionsbetweencontrolmodes\n• F,I are functionsassigningeach apredicateto each controlmode.\nF : T → C ∪ C˙ is the flow function. F(T ) defines the possible continuous\nx\nevolutionsC˙ ofthesystemin controlmodeT .\nx\n10\nI : T → C is the invariant function. I(T ) represents the possiblevaluationsfor\nx\nvariablesC incontrolmodeT .\nx\n• J,R arefunctionswhich assigna predicatetoeach edge.\nJ : K → C is the jump function. J(k) is a guard which determines when the\ndiscretecontrolmodechangerepresentedbyedgek ∈ K isallowedbasedonthe\nvaluationofthecontinuouscomponentC.\nR : K → C∪C′istheresetfunction. R(k)assignstoedgekpossibleupdatesfor\nthevariableswhenadiscretechangeoccurs. Thisisrepresentedbyanassignment\ntoprimedvariablesC′.\nIn our implementation (discussed in more depth in Section 2.3), the discrete “control\nmodes” are tasks and the continuous variables are sensory inputs. The flow functions\nare the velocitiescorresponding to task-specific motoroutputs, the invariant predicates\ndefine inputs to which each task is contingent, the jump guards set conditions which\nmust be met by the continuous inputs for a transition along a certain edge, and resets\nassign values to the continuous variables C′ which will determine the initial inputs at\nthestartofacertain task.\nThe unfolding over time of a hybrid automaton can be represented as a transition\nsystem (Q,֒→), where Q = T × C is a set of states, and ֒→⊆ Q × Q is a transition\nrelation corresponding to edges connecting states. This transition system represents a\nmodeloftheenvironmentconstructedthroughsimulatedinteraction. We denote\nPre(q) = {q′ ∈ Q | (q′ ֒→ q)} (1)\n11\nPost(q) = {q′ ∈ Q | (q ֒→ q′)} (2)\nFurther, we define a path ρ = q 0q 1...q n where q i ∈ Post(q i−1)∀i ≥ 1, where q 0 is\nthe root state in the transition system. Paths represent sequences that may potentially\nconstituteaplan.\n2.2.3 The Configurator asa supervisor\nWedefine theConfiguratoras atypeofsupervisor(Ramadgeand Wonham, 1984):\nConfigurator = (HA,E,P,goal) (3)\nwhere HA is a hybrid automaton as described in Section 2.2.2, goal = {D } is\ngoal\nthe system’s overarching goal, or the disturbance which the plan aims to counteract,\nE : HA → Q× ֒→ is an “explorer” function which generates transition system G =\n(Q,֒→) representing possibleruns of HA, and P : Q× ֒→ ×goal → Q is a “planner”\nfunctionwhichextracts agoal-directedpath fromatransitionsystem.\n2.3 Implementation\n2.3.1 Robot\n1\nWe implement the outlined framework on an indoor robot based on the Alphabot\n(see Figure 3A). The robot is equipped with a Rasbperry Pi Model 3b+ (Broadcom\nBCM2837B0 processor with a 1.4 GHz clock speed and 1 GB RAM), an A1 Slamtec\n2D LIDAR sensor and two 360o continuous rotation servos which move the wheels.\n1\nhttps://www.waveshare.com/alphabot-robot.htm\n12\nThe LIDAR callback runs at 5 Hz and the motors are updated at 10 Hz. We define the\nmotorupdateas step .\nmotor\n2.3.2 Coreknowledge asa physics simulation\n2\nWerepresentcoreknowledgeassimulationinthephysicsengineBox2D wheretheco-\nordinatesrepresentstherobot’slocalframeofreference. Alldatapertainingtoposition\nand itsderivativesare expressedas 2Dtransformsoftheform {u,θ},where u = (x,y)\nrepresents the position in Cartesian coordinates in meters and θ is the orientation in\nradians of the object. Obstacles in Box2D are objects of dimensions 0.001m x 0.001m\nlocatedatcoordinatesextractedfromLiDARinput. Therobotwasmodelledasasimple\nrectangleofdimensions0.22m×0.18monthelocalxandyaxis,respectively,withthe\ncenterofmassshiftedforward onthelocalx axis by0.05m(seeFigure3A).\nThe physics simulation may progress for an arbitrarily long (simulated) time, and\nthe world and its content are updated with each arbitrarily small time step step .\nBox2D\nSimulatedsensorscanbeaddedtoobjectswhichreportcollisionsbetweenobjects. Task\nsimulationisinterruptedas soon as acollisionis reported,and thefirst pointofcontact\nbetween fixtures isreturned as thedisturbance.\nWe limit the range of the sensor data used for the simulation to r = 1m from the\norigin. This represents a planning horizon, and the simulation stops when its limit is\nreached. The simulation is advanced using a kinematic model of the robot with a time\n1\nstep (step Box2D = 60s), 3 position iterations, 8 velocity iterations so that a simulation\ncarries on foravariablenumberofsteps,indicatedas n .\nBox2D\n2\nhttps://box2d.org/\n13\nA 0.22 m B T C T T\nS L , R\nwheel LIDAR\ny\nm\n8 center of mass\n0.1 ++ +\nx\n+\n0.16m\nchassis centroid\n0.2m\n0.05 m\nstepDistance\nFigure3: A: Schematicrepresentationoftherobotwithdimensionsandcenter ofmass\nannotated. The shaded areas in B and C represent, the boundaries for the inclusion of\nobjectsin thesimulationoftasksT (B), T andT (C).\nS L R\nThespeed oftheBox2Dsimulationissensitivetothenumberofsimulatedobjects:\ntoreducecomputationaldemands,weperformsomesimplefilteringontheLiDARco-\nordinatesasdepictedinFigure3B-C.ForsimulatingtasksoftypeT ,weonlyrepresent\nS\npoints located within a distance stepDistance from the robot’s center of mass, where\nstepDistance denotes a finite distance range in meters for that task type. For left and\nrightturnsT andT inFigure3Cweonlyretainpointslocatedwithinaradiusof0.16m\nL R\nfrom the robot’s center of mass. The filtered data thus obtained is further filtered us-\ning one of the following techniques. The first technique, “worldbuilding1”,consists of\napproximatingeach coordinatetotwo decimal places and eliminatingredundantvalue.\nFor the second, “worldbuilding2”, only every other point in the set constructed using\n“worldbuilding1” is retained. Simulation results can be used to determine motor com-\nmands for task execution in the real world. We calculate each motor command as a\ndurationexpressedas anumberofmotorupdateintervals\nn motor = n Box2D ·step motor ·step Box2D (4)\nwheren isthenumberofstepsofsizestep takentobringatasktotermina-\nBox2D Box2D\n14\ntion in simulationand n\nBox2D\n·step\nBox2D\ncalculates the simulated time which the task\nhas taken.\n2.3.3 Hybridautomaton\nThe hybrid automaton modelling the simulated task-switching process is depicted in\nFigure 4. The discrete control modes corresponding to tasks T = {T ,T ,T }, are\nS R L\nReset 1\nT D’= {D }\nD ≠Ø, goal\nobs S c’ =c\nReset 1 self self\nc’ ={(0,0),0}\nD =Ø\ndist\nobs\n||u ||≤sD\ndist\nReset 2\n1\nset =Ø\n≠\nØ R\ne\nD\n=\nD’ init= D\ninter\nD=Ø,\nR ee\nt\n1if\n2D\nif\nob Ds\nobs\nR e s\ne t\ns 2e t\ni f\n1 i\nf\nØ D,\nR e\ns\ne t\nc c’ ’s de islf t= =c {(s 0elf ,- 0c )di ,s 0t\n}\nes et Do bs 1\nT\nR Res o bs ≠=\nØ\nT\nØ\nL R\nD= {D } AND D= {D } AND\ninit init\n||θ ||≤π/2 ||θ ||≤-π/2\ndist dist\n. .\nc =M c =M\n.self L .self R\nc =M c =M\ndist L dist R\nFigure4: HA: nondeterministichybridautomatonmodellingtheclosed-loopbehaviour\noftherobot. Wedefineu ,θ ∈ c , and sD = stepDistance.\ndist dist dist\ninscribed in circles connected by arrows representing the edges K. Continuous vari-\nables C = {c ,c ,D} correspond to sensor inputs in the form of 2D transforms.\nself dist\nTheseinputs are: thepositionofthe robotc , thechange in therobot’spositionsince\nself\nthe start of the task c , and disturbance set D = {D ,D }, comprised of the\ndist init inter\n15\ndisturbance to which the task is contingent D , and the disturbance interrupting task\ninit\nexecution D . Flow F predicates are annotated outside of each circle. Invariant I\ninter\npredicates are annotated inside each circle. Jump J and reset R predicates are located\nby thecorrespondingedges.\nTask T models the behaviour of thesystem when it is driving straight, whiletasks\nS\nT and T model the behaviours of turning left and right, respectively. As indicated\nL R\nby therespectiveflow predicates, the positionof therobot c and thepositionchange\nself\nsincethestartofthetaskc evolveat aratedenotedbytask-specificvelocitiesM =\ndist S\n{(0.098m/s,0m/s),0rad/s} for driving stright, M = {(0m/s,0m/s),1.04rad/s} for left\nL\nturns, and M = {(0m/s,0m/s),−1.04rad/s} for right turns, all in local coordinates.\nR\nThese values were calculated applying simple robot kinematics to empirical velocity\nmeasurements.\nAs indicated by the corresponding invariant predicate, task T executes as long as\nS\nno obstacles (defined as D = {D ∈ D|type(D) = obstacle}) are present and the\nobs\nrobothasnotyetcoveredafixeddistancestepDistance (abbreviatedtosD inFigure).\nAs indicated by the respective invariant predicates, tasks T and T keep executing as\nL R\nlong as the only disturbance in the continuous variables is the one on which the task\nis contingent (predicate D = {D }), and the angle described by the turn has not yet\ninit\nreached π rad(fortaskT ),and−πradfortaskT . Notethattheseinvariantpredicates\n2 L 2 R\nstill allow for a turn to execute in the case a task may be initialised with an empty\ndisturbance.\nTransitionsalongedge(T ,T )arepermittediftheterminatedtaskhasnotencoun-\nS S\ntered obstacles, as indicated by jump predicate D 6= ∅. Transitions along edges\nobs\n16\n(T ,T ) and (T ,T ), are permitted if the left or right turns are terminated with no\nL S R S\ndisturbances,as indicated by jumppredicates D = ∅. Transitionsalong edges (T ,T )\nS L\nand (T ,T ) are alwayspermitted,as indicatedbythelack ofjumpguards.\nS R\nThe boxes on the right-hand side of Figure 4 contain the two reset predicates,\n“Reset1” and “Reset2”, which assign to primed variables C′ depending on whether\nthe most recently simulated task is terminated without disturbances, or if it encoun-\nters an obstacle, respectively. For all reset predicates, every time the system switches\ncontrol mode, the distance covered in the previous task is reset to a zero transform\nc′ = {(0m,0m),0rad}. The “Reset1” predicate states that the next task will start\ndist\nwhere the previous one ended (c′ = c ) and will be aimed at reaching the goal (\nself self\nD′ = {D }). The“Reset2”predicatestatesthatthenexttaskwillbeprimedtocoun-\ngoal\nteracttheobstacle(D′ = D )whichinterruptedtheprevioustask,anditwillstart\ninit inter\nwhere the previous task began (c′ = c −c ). “Reset1” always applies to edges\nself self dist\n(T ,T ),(T ,T ),(T ,T ), and to edges (T ,T ) and (T ,T ) if the continuousstate\nS S L S R S S L S R\nC does not contain obstacles (D = ∅). If obstacles are present (D 6= ∅) in these\nobs obs\nlasttwo edges,“Reset2” applies.\n2.3.4 The explorer E\nTheexplorerE usesabest-firstapproachtoconstructtransitionsystemG fromautoma-\nton HA. The priority of expansion of a state is defined by its cumulative past and\nexpected cost (cfr. Hart et al. 1968), which we will define as function φ : Q → R. Ad-\nditionally,weuseaheuristicfunctionσ : Q → {0,1}topreventtherobotfromturning\nmorethan180degrees on thespotand gettingstuckrotatinginplace.\n17\nWe calculatecostevaluationfunctionφas:\nφ(q) = γ(q)+χ(q) (5)\nwhere q are states in transition system G, γ(q) calculates the past cost associated with\nstateq andχ(q)isaheuristicfunctionwhichestimatesthecosttoreachgoalD from\ngoal\nstateq.\nWe definethepastcostfunctionγ as:\n 0 ifD = ∅\ninter\n\nγ(q) =   (6)\n\n2r−kustart−uinterk\nifD 6= ∅\n\n\n2r inter\n\n\nwhereD isadisturbanceencountered instateq, 2r representsthemaximumpossi-\ninter\nbledistancethattherobotcanbefromadisturbance,andku −u krepresentsthe\nstart inter\nEuclideandistancebetweentask startpositionu = u −u (see Section 2.2.2)\nstart self dist\nand disturbanceD .\ninter\nWe definethecostheuristicfunction:\n 0 ifgoal = ∅\n\nχ(q) =   (7)\n\nku self−u goalk otherwise\n\n\n4r\n\n\nwhere ku −u k represents the distance between the robot’s positionat the end of\nself goal\nthesimulationofstateq and target D . We furtherdefineheuristicfunctionσ as:\ngoal\nσ(q) : ku −u k = 0∧|θ −θ | ≥ π (8)\nself pre self pre\n18\nwhere 2D transforms {u ,θ } and {u ,θ } represent the robot’s position and\nself self pre pre\norientationattheend ofthesimulationofstateq and Pre(Pre(q)),respectively.\nAlgorithm 1providespseudocodeforexplorerE. Westartfromatransitionsystem\nG = {q 0} and priority queue PQ = {q 0}, where q 0 is a starting state representing the\ntask being executed by the robot. The corresponding task is immediately terminated,\nin order for the system to explore the scenarios that would arise if the robot were to\nchangeitsbehaviourimmediately,withoutwaitingforthetaskat stateq to terminate.\n0\nWe use q to denote the state with the lowest evaluation φ(q ) in the priority\nexp exp\nqueue; its out-edges are explored with the highest priority. We further denote the state\nwhoseout-edgesareactivelybeingexploredwithq andthefrontierstatewithq .\nexp0 exp1\nAny permitted transitions from state q are explored in a depth-first fashion until\nexp0\nfrontier state q contains a task T (zero to two states ahead of state q ). After the\nexp1 S exp\nexpansionofq iscomplete,q isresettothenodewhichhasthesmallestφvaluein\nexp exp\nthepriorityqueue,andtheprocessrepeatsuntilapathisfoundwhichleadstoagoalor\ntheplanninghorizon. NotehowthisimpliesthatonlytasksoftheT typeareevaluated\nS\nand thatq is, therefore, always ataskT .\nexp S\n2.3.5 PlannerP\nThe model generated by the explorer E is used by planner P defined in Section 2.2.3,\nwhich returns a path ρ which brings the robot as close to the goal as possible. For-\nplan\nmallywewrite:\nρ\nplan\n= {q 0,q 1,q 2,...q\nn\n∈ G|Post(q n) = ∅∧q\nn\n= minφ(q)} (9)\nq∈Q\n19\nAlgorithm 1ExplorerE\ninputs: HA\ninitialisePQ = {q 0},G = {q 0},q exp = q 0\nsimulatetheterminationofq 0: set D assumedempty,c\nself\n= {(0m,0m),0rad}\nrepeat\nremoveq from PQ\nexp\nforalledges k = (q exp,q exp1) forwhichjumpsarepermitteddo\nq exp0 = q exp\nrepeat\nsimulateT exp1 ∈ q exp1\nset q exp0 = q exp1\nuntil T exp1 = T S\nifσ(q exp1) = 0 then\nadd T to PQ\nexp1\nendif\nend for\nset q = min φ(q)\nexp q∈PQ\nuntil nomorejumpsarepermittedfromq orD has been reached\nexp goal\nreturn G\n3 Results\nWe test thedecision-makingprocess by applyingit to two real-world problems: one of\navoiding of being trapped in a cul-de-sac and one of simultaneous collision avoidance\nand target pursuit. We contrast our new approach against a Braitenberg-style control\nstrategy which uses a single closed-loop controller and therefore exhibits stereotyped\nbehaviour.\nSequencesoftasksandthespecificationsofthemotorcommandspertainingtoeach\ntaskareplannedatthebeginningoftheexperimentaltaskanddonotundergocorrection\nthroughout execution. The amount of time taken by the robot to formulate a plan is\nmeasured,aswellasthetotalamountofobjectssimulatedinBox2Dandthenumberof\nstatesin thetransitionsystem,which areequivalentto thenumberoftaskssimulated.\n20\n3.1 Experiment 1: cul-de-sac avoidance\nOur first experiment tests our approach in a cul-de-sac avoidance scenario. This is a\nclassicalscenariowherereactivebehaviourisnotsufficienttopreventgettingstuckina\ncul-de-sac. We set goal = ∅ and stepDistance = 0.5m. The experiment was repeated\nsixtimeswith“worldbuilding1”,andtwelvewith “worldbuilding2”.\nFigure 5 depicts a comparison of the behaviour planned using our new multiple-\nloopframework(Figure5A),andthesingle-loopreactivebehaviouralcase(Figure5B),\nand the reasoning process employed using our framework (Figure 5C). As can be ob-\nservedfromthetranceinFigure5A,whentherobotusesourmulti-step-aheadplanning\nstrategy, it identifies the cul-de-sac immediately and, to prevent entering it, turns to\nits right which allows it to carry on undisturbed. By contrast, in the single-loop con-\ntrolexperiment,therobotentersthecul-de-sac,asitisoutsideofthesingleloop’srange\nstepDistance. Moreover,assignifiedinyellowlightning,somecollisionsoccurduring\nexecution.\nFigure 5C offers a visual representation of the construction of transition system\nG constructed in one of the experimental runs and the plan extracted from it. Initial\nstate q is immediately terminated and from its end position (see robot schematic in\n0\nFigure 5C), transition system building starts. The robot may safely proceed straight\n(state q ) or turn right and then advance (states q ,q ). The sequence q ,q cannot be\n1 4 5 2 3\nsafely executed since, in state q , the robot collides with the wall ahead of it. States q\n3 1\nand q willhavethesameφ valueand rank higherinthepriorityqueuethanstateq .\n5 3\nThe cul-de-sac is discovered as all the sequences following state q (namely, state\n1\nq , states q ,q and states q ,q . State q will now therefore have highest priority,\n6 7 8 9 10 5\n21\nand will be expanded. Three other state sequences are added to this node. Of these,\ndriving right and straight (states q ,q ) leads to a crash, while state q and sequence\n12 13 11\nq ,q are disturbance-free. In this case, state q reaches the limit of the planning\n14 15 11\nhorizon before state q , for which reason planning can end. The final plan, indicated\n15\nby the bold arrow in Figure 5C, is ρ plan = q 0q 4q 5q 11, where q 0 = (T S,·), q 4 = (T R,·),\nq 5 = (T S,·),q 11 = (T S,·).\nFigure 5D displays the descriptive statistics pertaining to simulation and planning\nspeed. In runs where worldbuilding1was used to create objects in theBox2D environ-\nment(inFigure,columnlabelled”cul-de-sac,wb1”),anaverageof19.3states(min: 16,\nmax: 26) were simulated with 93.8 average total objects (SD=39.93) created over the\ncourse of the simulation. This took on average 0.295s (SD=0.069s). With worldbuild-\ning2 (in Figure, column labelled ”cul-de-sac, wb2”), an average of 20.16 states (min:\n11, max: 21) were simulatedwith 40.75 average total objects (SD=20.67) in 0.166s on\naverage(SD= 0.062s).\n3.2 Experiment 2: avoidance and target behaviour\nFigure 6 represents the results of an experiment where the robot had to reach a tar-\nget goal = {D = {(1.0m,0m),0rad}} while driving around an obstacle. We set\ngoal\nstepDistance = 0.22mcorrespondingtothediameteroftherobot,asin(Ulrich andBorenstein,\n2000). The experiment was repeated ten times with “worldbuilding1”, and eight with\n“worldbuilding2”.\nAs shown in Figure 6B, in the reactive behaviour experiment the robot remains\nstuckbehind theobstacle, whilein ourapproach (Figure6A)theagent isableto safely\n22\nAA\nB\nq C D\n15\nq\nq\n11 14 Average times for transition\nq system construction (collision avoidance)\n12 q\n5\nq\n13 q q\n4 3\nq\nq q 2\n10\n1\nq\n9\nq\n7\nq\nq\n8\n6\nFigure5: Behaviourexhibitedbytherobotintheobstacle-avoidanceexperiment. Panel\nA:withtheproposedmultiple-loopplanning,panelB:withplanningoverasinglecon-\ntrolloop,panel C: reasoningin multiple-loopplanning,D:descriptivestatistics.\nformulateaplan toreach thetarget.\nFigure 6C depicts the transition system produced in one of the experimental runs.\nAs shown, the robot can safely advance towards the target D by entering states\ngoal\n23\n)s(\nemiT\nA B\nC D\nq q Average times for transition\n26 35\nq q q q q system construction (target pursuit)\n28 27 29 30 34 q\nq q 31\n21 q 32\n33\nq q q\n23 22 24\nq q\nq 25 11\n20\nq q q q q\n18 19 13 12 14 q\nq q 15\n17 6\nq 16 q 8 q 7 q 9 q 10\nq\n1\nq q\n2 4\nq q\n3 5\nFigure6: Initialframeandtrackingofthebehaviourexhibitedbytherobotinthetarget-\nseeking/overtaking experiment. Panel A: with the proposed multiple-loop planning,\npanel B: with one-loop-ahead disturbance detection, panel C: reasoning in multiple-\nloop planning, D: descriptive statistics. The target is represented in A-B by a grey\ncross.\n24\n)s(\nemiT\nq ,q . Further advancement through state q is prevented by an obstacle. The only\n1 6 11\ncollision-free option for the robot is to turn left (state q ) and proceed in a direction\n12\nperpendicular to the target (state q ). The robot will then turn towards the disturbance\n13\nbychoosingtoturnright(stateq )andthen proceed straight(q ). Therobotwillthen\n19 20\nimmediatelyattempttoreturninlinewiththetargetbyturningleftandthenproceeding\nstraight (states q ,q ), but these options are not collision-free. By transitioning to\n24 25\nstate q , the robot will drive straight towards the target; after this, it may safely turn\n21\nright (state q ) and return in line with the target (state q ). The target is now very\n29 30\nclose: by turning left (state q ), the robot positions itselfto face it and advances (state\n34\nq 35) until it reaches it, at which point transition system G construction terminates. As\nindicated by the bold arrow in Figure 6C, the plan followed in Figure 6A is ρ =\nplan\nq 1q 6q 12q 13q 19q 20q 21q 29q 30q 34q 35, where q 1 = (T S,·), q 6 = (T R,·), q 12 = (T L,·), q 13 =\n(T S,·), q 19 = (T R,·), q 20 = (T S,·), q 21 = (T S,·), q 29 = (T R,·), q 30 = (T S,·), q 34 =\n(T L,·),q 35 = (T S,·).\nDescriptivestatisticsforthesimulationandplanningspeedareshowninFigure6D.\nUsing worldbuilding1(in Figure, column labelled ”target, wb1”), the transitionsystem\nwas made up of an average of 37.8 states (min: 36, max: 39), and each simulation\ncontainedonaverage114.4objects(SD=21.08). Themeansimulationtimewas 0.273s\n(SD= 0.016s). When worldbuilding2 (in Figure, column labelled ”target, wb2”) was\nemployed,themeannumberofstateswassimilarat38.25withthesameminimumand\nmaximum values. The average number of objects was reduced to 64.38 (SD=11.70),\nand averagesimulationtimedroppedto0.133s(SD=0.011s).\n25\n4 Discussion\nIn this paper we have introduced a novel decision-making process which exploits su-\npervisory control of a hybrid automaton representing temporary simulated closed-loop\ncontrollers, called “tasks”, representing each a different control strategy. The simu-\nlation of the task-switching process occurs in a physics engine which constitutes the\nsystem’s core knowledge. The performance of the decision-making process was tested\nin a real robot in a collision avoidance problem and a target pursuit one. The robot\nwas in both cases able to generate, in real-time, a model of tasks and their resulting\ndisturbances over a 1m range, which was used to extract a plan. We contrasted this\nperformance with a classical closed-loop system where planning can only occur one\ndisturbance into the future and the system behaves deterministically. This control case\nproduced less efficient behaviour in the best case scenario and failed by colliding with\nawall orremainingstuck intheworst.\nWe construct a model of the environment as a hybrid transition system represent-\ning possible runs of a hybrid automaton. Problems concerned with the combination\nof discrete and hybrid control are referred to as Task And Motion Problems (TAMP).\nTAMPs are notoriously difficult to solve in real-time: in recent years neural networks\nhave been used to train complex models aimed at combining planning over a hybrid\ntransition system and learning control policies in a variety of scenarios (Kimet al.,\n2022; Driesset al., 2021). This obviously requires offline training and has potential\nforlimitedtransparencyand/orrobustnessdependingonthedepthofthenetworkused.\nAdditionally, even with previous training, planning times in Kimet al. (2022) fell into\ntheorderofhundredsofseconds;thisisnotsuitableforreal-world applications.\n26\nOnline approaches to hybrid control for navigation have also been proposed. In\nMavridisetal. (2019); Constantinouand Loizou (2018); Migimatsuand Bohg (2020),\nhybrid automata are constructed using various types of logic to check that they sat-\nisfy certain properties. Whether the control modes may be immediately available to\nthe system (Migimatsuand Bohg, 2020) or may need to be computed (Mavridiset al.,\n2019), the computation of the control-mode-specific flow function can be an inef-\nficient process. Computation statistics were not reported in Mavridiset al. (2019);\nConstantinouand Loizou (2018), however Migimatsuand Bohg (2020) report that the\nminimum time taken for convergence to an optimal continuous control was 0.85 sec-\nonds. A model-checking approach was proposed by Chandleret al. (2023) over a\ndiscrete-state system, which was able to plan over a sequence of five actions in around\n0.01s. This suggests that the optimisation over continuous controls may be the bottle-\nneck forTAMPproblems.\nWhilewedonotfocusonachievingsmoothtrajectoriesormorecomplextaskssuch\nas grasping, we were able to achieve planning in real-time or near-real-time in a robot\nwith very limited computational power. The closed-loop task simulation allowed us to\neasily obtain an estimation of the commands for the motor effectors. Starting from a\nreasonable estimation of a control output may relieve some of the computational load\nresultingfromitsoptimisation,whichshouldoccuronlineandinaclosed-loopfashion.\nDespite the increased planning time in our experiments compared to Chandleret al.\n(2023), the speed performance recorded in our experiments is still within human reac-\ntiontimerange(Wong et al., 2015) andwithinreal-timerequirementsoftherobot.\nIn conclusion, our findings demonstrate that not only it is possible to formulate\n27\noptimal plans based on pure input control, but it is also achievable in real-time over\nthe discrete and continuous domains, thanks to core knowledge. We believe that our\nframework has potential to scale well to TAMP problems, in at least two dimensions,\nand provideavaluablecomplementtothestate-of-the-art.\n5 Acknowledgements\nThis work was supported by a grant from the UKRI Engineering and Physical Sci-\nences Research Council Doctoral Training Partnership award [EP/T517896/1-312561-\n05]; the UKRI Strategic Priorities Fund to the UKRI Research Node on Trustwor-\nthy Autonomous Systems Governance and Regulation [EP/V026607/1, 2020-2024];\nand the UKRI Centre for Doctoral Training in Socially Intelligent Artificial Agents\n[EP/S02266X/1].\nReferences\nZhenshan Bing, David Lerch, Kai Huang, and Alois Knoll. Meta-reinforcement learn-\ning in non-stationary and dynamic environments. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45:3476–3491, 3 2023. ISSN 19393539. doi:\n10.1109/TPAMI.2022.3185549.\nJohann Borenstein and Yoram Koren. Thevectorfield histogram—fastobstacleavoid-\nanceformobilerobots. IEEETransactionsonRoboticsandAutomation,7:278–288,\n1991. doi: 10.1109/70.88137.\n28\nV. Braitenberg. Vehicles: ExperimentsinSyntheticPsychology. MITPress, 1986.\nChristopher Chandler, Bernd Porr, Alice Miller, and Giulia Lafratta. Model checking\nforclosed-looprobotreactiveplanning. ElectronicProceedingsinTheoreticalCom-\nputerScience, 395:77–94,112023. doi: 10.4204/EPTCS.395.6.\nChristos C. Constantinou and Savvas G. Loizou. Automatic controller synthesis of\nmotion-tasks with real-time objectives. Proceedings of the IEEE Conference on\nDecision and Control, 2018-December:403–408, 7 2018. doi: 10.1109/CDC.2018.\n8619825.\nSteven Dalton and Iuri Frosio. Accelerating reinforcement learning through gpu atari\nemulation. Advancesin NeuralInformationProcessing,33:19773–19782,2020.\nDanny Driess, Jung Su Ha, and Marc Toussaint. Learning to solve sequential physical\nreasoningproblemsfromasceneimage. InternationalJournalofRoboticsResearch,\n40:1435–1466,12 2021. ISSN 17413176. doi: 0.1177/02783649211056967.\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic\ndetermination of minimum cost paths. IEEE Transactions on Systems Science and\nCybernetics,4:100–107,1968. ISSN 21682887. doi: 10.1109/TSSC.1968.300136.\nThomas A. Henzinger. The theory of hybrid automata. Verification of Digital\nand Hybrid Systems, pages 265–292, 2000. ISSN 1043-6871. doi: 10.1007/\n978-3-642-59615-5 13.\nAnna Honkanen, Andrea Adden, Josiane Da Silva Freitas, and Stanley Heinze. The\ninsect central complex and the neural basis of navigational strategies. Journal of\n29\nExperimental Biology, 222, 2 2019. ISSN 00220949. doi: 10.1242/JEB.188854/\n2882.\nKarthik Karur, Nitin Sharma, Chinmay Dharmatti, and Joshua E. Siegel. A survey of\npathplanningalgorithmsformobilerobots. Vehicles2021,Vol.3, Pages448-468,3:\n448–468,8 2021. ISSN 2624-8921. doi: 10.3390/VEHICLES3030027.\nO.Khatib. Real-timeobstacleavoidanceformanipulatorsandmobilerobots. Proceed-\nings - IEEE International Conference on Robotics and Automation, pages 500–505,\n1985. ISSN 10504729. doi: 10.1109/ROBOT.1985.1087247.\nBeomjoon Kim, Luke Shimanuki, Leslie Pack Kaelbling, and Toma´s Lozano-Pe´rez.\nRepresentation, learning, and planning algorithms for geometric task and motion\nplanning. International Journal of Robotics Research, 41:210–231, 2 2022. ISSN\n17413176. doi: 10.1177/02783649211038280.\nGerardo Lafferriere, George J Pappas, and Sergio Yovine. A new class of decidable\nhybrid systems. Hybrid Systems: Computation and Control: Second International\nWorkshop,pages 137–151,1999.\nYann LeCun. A path towards autonomous machine intelligence. OpenReview.net, 6\n2022.\nQian Luo, Maks Sorokin, and Sehoon Ha. A few shot adaptation of visual navigation\nskills to new observations using meta-learning. Proceedings - IEEE International\nConference on Robotics and Automation, 2021-May:13231–13237, 2021. ISSN\n10504729. doi: 10.1109/ICRA48506.2021.9561056.\n30\nHumbertoR.MaturanaandFranciscoJ.Varela. AutopoiesisandCognition,volume42.\nBoston Studies in the Philosophy of Science New York, N.Y., 1980. ISBN 978-90-\n277-1016-1. doi: 10.1007/978-94-009-8947-4.\nChristosN.Mavridis,ConstantinosVrohidis,JohnS.Baras,andKostasJ.Kyriakopou-\nlos. Robotnavigationundermitlconstraintsusingtime-dependentvectorfield based\ncontrol. Proceedings of the IEEE Conference on Decision and Control, 2019-\nDecember:232–237, 12 2019. ISSN 25762370. doi: 10.1109/CDC40024.2019.\n9028890.\nToki Migimatsu and Jeannette Bohg. Object-centric task and motion planning in dy-\nnamic environments. IEEE Robotics and Automation Letters, 5:844–851, 4 2020.\nISSN 23773766. doi: 10.1109/LRA.2020.2965875.\nSimon Ramstedt Mila and Christopher Pal. Real-time reinforcement learning. 2019.\ndoi: https://doi.org/10.48550/arXiv.1911.04448.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness,\nMarc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\nKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.Human-\nlevel control through deep reinforcement learning. Nature, 518:529–542, 2 2015.\nISSN 00280836. doi: 10.1038/NATURE14236.\nOpenAI. Gpt-4technical report,2023.\n31\nB Porr and F Wo¨rgo¨tter. Inside embodiment-what means embodiment to radical con-\nstructivists? Kybernetes, 34:105–117,2005.\nP. J. Ramadge and W. M. Wonham. Supervisory control of a class of discrete event\nprocesses. Analysis and Optimization of Systems, pages 475–498, 10 1984. doi:\n10.1007/BFB0006306.\nJean-Franc¸ois Raskin. An introduction to hybrid automata. Handbook of Networked\nandEmbeddedControlSystems,pages491–517,2005.doi: 10.1007/0-8176-4404-0\n21.\nYang Shu, Zhangjie Cao, Jinghan Gao, Jianmin Wang, Philip S. Yu, and Mingsheng\nLong. Omni-training: Bridgingpre-trainingandmeta-trainingforfew-shotlearning.\nIEEE Transactionson Pattern Analysis and Machine Intelligence, 45:15275–15291,\n122023. ISSN 19393539. doi: 10.1109/TPAMI.2023.3319517.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-\nbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529:484–490, 1 2016. ISSN 00280836. doi:\n10.1038/NATURE16961.\nElizabethS.SpelkeandKatherineD.Kinzler.Coreknowledge.DevelopmentalScience,\n10:89–96,12007. ISSN 1363755X. doi: 10.1111/J.1467-7687.2007.00569.X.\n32\nHuihuiSun,WeijieZhang,RunxiangYu,andYujieZhang. Motionplanningformobile\nrobots-focusingondeepreinforcementlearning: Asystematicreview. IEEEAccess,\n9:69061–69081,2021. ISSN 21693536. doi: 10.1109/ACCESS.2021.3076530.\nYuewen Sun, Kun Zhang, and Changyin Sun. Model-based transfer reinforcement\nlearning based on graphical model representations. IEEE Transactions on Neural\nNetworks and Learning Systems, 34:1035–1048, 2 2023. ISSN 21622388. doi:\n10.1109/TNNLS.2021.3107375.\nPierreThodoroff,WenyuLi,andNeil DLawrence. Benchmarking real-timereinforce-\nmentlearning. Proceedingsof MachineLearningResearch,181:26–41,2022.\nIwan Ulrich and Johann Borenstein. Vfh*: Local obstacle avoidance with look-ahead\nverification. pages2505–2511,2000.\nRama K. Vasudevan, Maxim Ziatdinov, Lukas Vlcek, and Sergei V. Kalinin. Off-the-\nshelfdeeplearningisnotenough,andrequiresparsimony,bayesianity,andcausality.\nnpj Computational Materials 2021 7:1, 7:1–6, 1 2021. ISSN 2057-3960. doi: 10.\n1038/s41524-020-00487-0.\nAaron L. Wong, Adrian M. Haith, and John W. Krakauer. Motor planning. Neurosci-\nentist,21:385–398,8 2015. ISSN 10894098. doi: 10.1177/1073858414541484.\n33",
    "pdf_filename": "Homeostatic_motion_planning_with_innate_physics_knowledge.pdf"
}