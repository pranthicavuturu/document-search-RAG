{
    "title": "Value Imprint A Technique for Auditing the Human Values Embedded in RLHF Datasets",
    "context": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human preferences and values. However, very limited research has investigated which specific human values are operationalized through these datasets. In this paper, we introduce Value Imprint, a framework for auditing and classifying the human values embedded within RLHF datasets. To investigate the viability of this framework, we conducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human values embedded within them. Our analysis involved a two-phase process. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences. During the second phase, we employed the labels generated from the annotation as ground truth data for training a transformer-based machine learning model to audit and classify the three RLHF datasets. Through this approach, we discovered that information-utility values, including Wisdom/Knowledge and Information Seeking, were the most dominant human values within all three RLHF datasets. In contrast, prosocial and democratic values, including Well-being, Justice, and Human/Animal Rights, were the least represented human values. These findings have significant implications for developing language models that align with societal values and norms. We contribute our datasets to support further research in this area. 1 Reinforcement Learning From Human Feedback (RLHF) has emerged as a potent way of shaping the behavior of AI models to ensure they produce positive responses and experiences that correspond with user preferences and societal norms [1–3]. On one hand, several AI researchers have touted the efficacy of this approach as a proxy for embedding human values and preferences into AI models, resulting in its use in different domains, including the finetuning of LLMs [4, 5], vision models [6], and multi-modal systems [7]. Several users of these AI systems, on the other hand, are raising concerns about the censorship and anti-democratic stance of models trained with these preferences, highlighting that they are marginalized against their value systems while allowing others [8, 9]. As a result, there is a growing concern among members of the public around the lack of transparency in the kinds of values these datasets embed into AI systems. In addition, considering that RLHF preferences involve complex value judgments of annotators, it is crucial to investigate how the subjective values 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. arXiv:2411.11937v1  [cs.LG]  18 Nov 2024",
    "body": "Value Imprint: A Technique for Auditing the\nHuman Values Embedded in RLHF Datasets\nIke Obi\nPurdue University\nWest Lafayette, Indiana\nRohan Pant\nPurdue University\nWest Lafayette, Indiana\nSrishti Shekhar Agrawal\nPurdue University\nWest Lafayette, Indiana\nMaham Ghazanfar\nPurdue University\nWest Lafayette, Indiana\nAaron Basiletti\nPurdue University\nWest Lafayette, Indiana\nAbstract\nLLMs are increasingly fine-tuned using RLHF datasets to align them with human\npreferences and values. However, very limited research has investigated which\nspecific human values are operationalized through these datasets. In this paper, we\nintroduce Value Imprint, a framework for auditing and classifying the human values\nembedded within RLHF datasets. To investigate the viability of this framework, we\nconducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human\nvalues embedded within them. Our analysis involved a two-phase process. During\nthe first phase, we developed a taxonomy of human values through an integrated\nreview of prior works from philosophy, axiology, and ethics. Then, we applied\nthis taxonomy to annotate 6,501 RLHF preferences. During the second phase,\nwe employed the labels generated from the annotation as ground truth data for\ntraining a transformer-based machine learning model to audit and classify the three\nRLHF datasets. Through this approach, we discovered that information-utility\nvalues, including Wisdom/Knowledge and Information Seeking, were the most\ndominant human values within all three RLHF datasets. In contrast, prosocial and\ndemocratic values, including Well-being, Justice, and Human/Animal Rights, were\nthe least represented human values. These findings have significant implications\nfor developing language models that align with societal values and norms. We\ncontribute our datasets to support further research in this area.\n1\nIntroduction\nReinforcement Learning From Human Feedback (RLHF) has emerged as a potent way of shaping the\nbehavior of AI models to ensure they produce positive responses and experiences that correspond\nwith user preferences and societal norms [1–3]. On one hand, several AI researchers have touted the\nefficacy of this approach as a proxy for embedding human values and preferences into AI models,\nresulting in its use in different domains, including the finetuning of LLMs [4, 5], vision models\n[6], and multi-modal systems [7]. Several users of these AI systems, on the other hand, are raising\nconcerns about the censorship and anti-democratic stance of models trained with these preferences,\nhighlighting that they are marginalized against their value systems while allowing others [8, 9]. As a\nresult, there is a growing concern among members of the public around the lack of transparency in the\nkinds of values these datasets embed into AI systems. In addition, considering that RLHF preferences\ninvolve complex value judgments of annotators, it is crucial to investigate how the subjective values\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\narXiv:2411.11937v1  [cs.LG]  18 Nov 2024\n\nand preferences of annotators – both human and AI – are embedded within these datasets in ways\nthat might misalign with societal values and norms.\nIn this paper, we introduce Value Imprint, a novel technique for auditing and classifying the human\nvalues embedded within RLHF datasets. To support this approach, we created a human values\ntaxonomy by conducting an integrated literature review of prior bodies of work from philosophy,\naxiology, and STS (Science, Technology, and Society) and, through a thematic analysis of these\nbodies of work, developed a taxonomy of human values to support our audit. Using this taxonomy,\nwe conducted a two-phase audit analysis, with each step building on the result from the previous stage.\nDuring the first phase, we employed the taxonomy to qualitatively annotate 6,501 RLHF preferences.\nDuring the second phase, we employed the labels derived from the qualitative annotation process as\nground truth data. This data was then utilized to a train transformer-based machine learning model,\nwhich we subsequently deployed for auditing and classifying the complete Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets. We further conducted a human evaluation\nof a section of the classification output to examine their performance. We followed the evaluation\nwith an additional round of analysis to examine how the values embedded within the three RLHF\ndatasets differ. Through these approaches, we answered our research questions which included:\n1. RQ1: What kinds of human values are embedded within RLHF preferences?\n2. RQ2: In what ways do the human values embedded within the Anthropic/hh-rlhf, OpenAI\nWebGPT Comparisons, and Alpaca GPT-4-LLM datasets differ?\nFindings from our research revealed that the most dominant values within the ground truth RLHF\npreferences were Information Seeking and Wisdom/Knowledge. In contrast, the least represented\nvalues were Civility & Tolerance, Empathy & Helpfulness, Justice & Human Rights/Animal Rights,\nand Well-being & Peace. The findings also revealed instances of unethical responses selected\nas suitable preferences for training machine learning models. Furthermore, the machine learning\nclassification of human values produced an accuracy score range of 80% for the model we used for\nthis analysis. This demonstrates the viability of AI researchers and practitioners adopting this process\nto interrogate the human values embedded within RLHF datasets to foreground their value orientation\nand how they might lead to different societal impacts.\nAbove all, through this research, we make the following contributions: 1) we introduce a technique for\nauditing and classifying the underlying human values embedded within RLHF preferences, providing\nAI researchers with a technique for auditing and interrogating the quality of RLHF datasets, 2)\nwe conduct three case study experiments using this approach and through our findings reveal that\nWisdom/Knowledge and Information Seeking were the most dominant human values within the\ndatasets; validating our technique. 3) We contribute both our ground truth annotation and classification\ndatasets and, through this means, provide researchers with the pathway to take this work forward.\nIn the sections that follow, we situate our work within broader research on language models, data\nquality, and embedding human values into LLMs. We then describe our methods and report our\nfindings. We conclude with a discussion of the implications of these findings and provide suggestions\nfor future work.\n2\nBackground\n2.1\nEmbedding Human Values into LLMs and AI Systems\nAI researchers and computer scientists are increasingly interested in embedding human values into\nLLMs, AI, and robotic systems. This increased interest is motivated by several reasons, including the\nneed to move beyond just optimizing for metrics like efficiency and performance towards aligning\nthese systems with prosocial values like democracy, transparency, freedom of expression, and human\nrights [10–12]. It also includes the need to ensure that technology systems do not make users\nvulnerable or cause them harm [13–15]. To achieve these objectives, AI researchers are increasingly\ndeveloping sociotechnical approaches for encoding societal values into AI systems, using different\ntechniques such as value-oriented datasets as can be found in the works of [10, 16–19] and formalized\nethical frameworks as can be found in the works of [20–22], among other techniques [23–26].\nSolaiman & Dennison [10] introduced an approach for aligning AI models with human values by\nusing value-oriented datasets to finetune AI models. Findings from their research revealed that\n2\n\ntheir approach improved adherence to human values and reduced toxicity without affecting model\nperformance. Nahian et al. [17] investigated approaches for using stories to encode societal norms\ninto machine learning systems and found that this technique can complement other approaches for\nintroducing human values into machine learning systems. Ammanabrolu et al. [18] explored the\napproach of imbuing agents with common sense knowledge to ensure that such systems align with\nsocially beneficial human values. Findings from their study revealed that this approach reduced\nthe ability of the agent to engage in harmful behaviors that misalign with human values by 25%.\nSorensen et al. [19] also explored approaches for enabling value pluralism in machine learning\nsystems. They introduced the ValuePrism dataset as a means of fostering this genre of research.\nSzabo et al. [27] investigated approaches for fusing quantitative and qualitative-based reasoning as\na means of ensuring value alignment in machine learning systems. Relevant here is that computer\nscientists and AI researchers are increasingly exploring technical, conceptual, and philosophical\napproaches for embedding human values into LLMs, AI, and algorithmic systems.\nHowever, although numerous AI researchers have examined several approaches for embedding human\nvalues into AI models, there is currently a lack of techniques and methods that allow researchers\nto systematically foreground and thoroughly investigate the specific types of human values being\nintegrated into LLMs and AI models using non-ethics curated datasets. And specifically, very limited\nattention has been paid to examining the human values embedded within RLHF datasets. Prabhakaran\net al. [21] highlighted that AI researchers often assume that the values they embed into AI systems\nserve the best interest of society, even though there is no empirical data or justification that supports\ntheir approach and belief. If anything, it is the opposite [28, 29]. Hendrycks et al. [30, 16] identified\na lack of alignment with hard-to-specify human values as one of the unsolved safety-related problems\nin the field of machine learning. Given that the objective of RLHF is to embed human values and\npreferences into AI models and considering that, at present, there is no method for auditing and\nmeasuring the human values embedded within RLHF datasets, this paper focuses on introducing a\ntechnique for interrogating these RLHF datasets as a means of providing insights into the kinds of\nhuman values embedded within them.\n2.2\nData Quality and Language Models\nThere is a large and growing body of work at the intersection of data quality and language models [31–\n33]. These works have examined issues relating to AI datasets from numerous perspectives, including\nissues of representation harms and demographic bias that propagate harmful stereotypes from\ndefective datasets into AI models, issues of toxicity/harmful content that perpetuate misogyny\nand racial slurs, lack of transparency and accountability around how datasets are collected, annotated,\ncleaned or versioned over time which hampers accountability and attribution, among many other\nissues at the intersection of data quality and language models.\nHirota et al. [31] investigated the issues of gender and racial bias in five visual question-answering\ndatasets. Findings from their research revealed instances of gender disparity and racial stereotypes\nthat favor males and Western cultures, respectively. They proposed approaches that researchers could\nadopt to mitigate these biases. Garcia et al. [34] annotated and audited the Google Captions vision\nand language model datasets to investigate instances of bias. Findings from their research showed\nan over-representation of males and persons with lighter skin tones compared to other users from\nother demographics. Dhamala et al. [35] introduced a large-scale benchmarking dataset to allow\nresearchers to measure bias in language models across different dimensions, including race, religion,\nand gender. Through this approach, they aim to induce transparency in reporting toxicity within\nlanguage models. Papakyriakopoulos et al. [36] investigated the lack of diversity in speech datasets\nacross different dimensions, including accent, dialect, and speech impairment. Findings from their\nresearch revealed that the absence of intentional structure plays a role in this lack of diversity. To\nresolve this, they introduced speech datasheets to foster ethical data collection practices around\nspeech datasets. Pushkarna et al. [37] introduced data cards to document the provenance and ethical\nimplications of using multi-modal datasets. Luccioni et al. [38] introduced a dataset deprecation\nframework as a means of ensuring proper documentation for datasets that are deprecated and retired\nfrom circulation.\nAlthough numerous scholars have extensively audited AI and machine learning datasets, very limited\nwork has focused on examining RLHF datasets to foreground the human values embedded within\nthem. In a small body of work in this area, Hendrycks et al. [16] introduced the ETHICS Dataset\n3\n\nData Collection\n1\n2\n3\n4\n5\n6\nModel Training\nTaxonomy \nValue Classifier\nData Annotation\nValue Audit\nCollect RLHF datasets \nfrom Hugging Face & \nGitHub\nTrain Bert-based models\nusing ground truth data \nderived from annotation\n..........\nEncoder 1\nAnthropic\nEncoder  2\nWebGPT\nEncoder  ..\n0.5\n0.2\n0.7\nAlpaca GPT-4\nDevelop human values\ntaxonomy based on prior \nresearch in axiology\nUse trained model to \nclassify human values \nembedded in RLHF data \nCreate ground truth data\nvia qualitative annotation \nof select RLHF datasets\nCompare classification\nresults of the 3 datasets\nto examine insights.\nFigure 1: Value Imprint is a technique for auditing the human values embedded within RLHF\ndatasets using an AI-focused human values taxonomy.\nto foster the measurement of the ethical judgment of language models. Birhane et al. [39] also\nintroduced a technique for annotating the values embedded within machine learning research papers;\nhowever, did not focus on examining RLHF datasets. Obi & Gray [40] examined values engineers\nembedded into AI systems through their technical judgment but did not examine the values embedded\nin AI datasets. This limited work that has examined the human values within RLHF datasets motivates\nthe need for further research in this area to support a more transparent RLHF process.\n3\nMethod\n3.1\nExperiment Dataset\nWe collected the datasets for this research from different developer collaboration platforms. We\ncollected the Anthropic/hh-rlhf [2] dataset from Hugging Face, an open source machine learning\nplatform that provides datasets, models, and other computational resources for AI practitioners and\nresearchers. The Anthropic/hh-rlhf dataset (train - 161k rows, test - 8.55k rows) has been downloaded\nat least 109,200 times, used to train or fine-tune more than 156 AI models. Our analysis focused\non both the chosen and rejected columns of the data. We merged the train and test sections of the\nAnthropic/hh-rlhf dataset into one dataset corpus for analysis. We also collected the OpenAI WebGPT\nComparisons [41] dataset from the Hugging Face library and focused our case study experiment on\nthe content of the question and answer_0 columns. We created a function to extract only the full_text\nfrom the question column and dropped the non-essential metadata, including triviaqa, dataset, and\nid. Next, we concatenated the content of the updated question and answer_0 columns into a new\ncolumn to form a complete preference unit. We then used our model to classify these preferences and\nexamined the human values embedded within them. We fetched the Alpaca GPT-4-LLM [42] dataset\nfrom the GitHub repository dedicated to the project. Next, we concatenated the instruction and\noutput columns from the original dataset into a new combined column, creating a complete human\npreference conversation. We reduced the DataFrame to contain only this new combined column and\nthen conducted our case study classification analysis. See (Fig. 1) for our research process flow.\n3.2\nHuman Value Taxonomy\nWe constructed a taxonomy of human values through an integrated literature review grounded in\nprior bodies of work from moral philosophy, axiology, and STS (Science, Technology, and Society).\nSpecifically, our literature search focused on nine journal databases within human values-related\ndisciplines, including the Journal of Value Inquiry; Axiomathes; The Journal of Ethics; Noûs;\nEthics; The Philosophical Review; Science, Technology, & Human Values; Utilitas; and The Journal\nof Philosophy. Our search keyword for querying these databases was: \"human value.\" No date\nrestrictions were made on our search of these databases.\nWe followed a three-stage process to construct a taxonomy of human values using the curated\nresearch papers. In the first stage, we assigned each curated paper a human value based on the central\ntheme discussed in the paper. Next, we categorized papers with similar values into semantically\ncoherent hierarchical categories using a bottom-up approach, such as grouping papers about peace,\nsecurity, and well-being under an overarching well-being and peace category. Second, we conducted\n4\n\n01 | Information Seeking\nHuman Values \n..................\n...................\n...................\n04 | Justice & Rights\n02 | Wisdom & Knowledge\n05 | Duty & Accountability\n03 | Well-being & Peace\n06 | Civility & Tolerance\n07 | Empathy & Helpfulness\nEthical obligations and \nresponsibilities to society\nShowing compassion and \naltruism to others\nRespect for peoples rights,\nfreedom, and autonomy\nPositive character and attitude\nin social interactions\nCivic Values\nInformation-Utility Values\nPro-Social Values\nWell-Being & Safety\nAcquiring knowledge for\ndeeper understanding\nHolistic thriving across physical,\nmental, and emotional aspects\nPursuit of information for\nimmediate, practical use\nFigure 2: This image presents a visual version of the taxonomy that supported our audit. [See Table\n2 and Appendix 5.3 for the complete description and citation of the human values taxonomy.]\na qualitative review examining hypernym-hyponym relationships of our categories from the first\nstage to ensure subordinate values maintained an \"is a part of\" relationship within each category\n(e.g., duty/accountability containing non-maleficence and trustworthiness). Third, we conducted\nan additional review to verify that all values within each group reasonably belonged to the same\nethical paradigm, though not intended to be sacrosanct, such as wisdom and knowledge aligning\nwith virtue ethics. This approach allowed us to create a semantically coherent and ethically balanced\nhuman values taxonomy for our analysis. We made the hierarchical taxonomy such that other related\nsub-values not covered in this paper can reasonably fit within the different high-level value categories.\nWe provide in-depth information about the taxonomy in Table 2 and in Appendix 5.3.\n3.3\nData Annotation\nUsing the human values taxonomy as our codebook, we qualitatively annotated sampled 6,501\npreferences from the Anthropic/hh-rlhf dataset to examine the human values embedded in them. The\nqualitative annotations were performed by a team of 5 researchers with interdisciplinary expertise\nspanning Ethics, Computing, and HCI. The nationality of the annotators included India, USA, Nigeria,\nand Pakistan. Before coding all the 6,501 preferences, we held several rounds of extensive discussions\nand exploratory coding activities. These activities allowed us to engage with the dataset to better\nunderstand the dimensions of human values, their differences, and similarities and to establish a\nprotocol for resolving any discrepancies and challenges that might arise during the main annotation\nsession. Following this exploration, we conducted an inter-annotator agreement assessment by having\nall the annotators independently code the same 200 preferences and then compared the codes assigned\nby each annotator to the same preferences to assess the level of agreement between all the annotators.\nWe achieved an inter-annotator agreement score of 0.85 using Krippendorff’s Alpha score. Through\nthis approach, we confirmed that multiple coders can consistently annotate and apply the same\nlabels to the same RLHF preferences once they understand the human values taxonomy. We then\ncommenced our main annotation session. Other infrequent discrepancies during our main annotation\nphase were resolved through discussions, codebook refinement, or reconciliation by a third annotator.\n5\n\n3.4\nValue Classification\n3.4.1\nProblem Formulation\nTo formally frame the task of computationally auditing the human values embedded within RLHF\ndatasets, we modeled it as a multi-class classification problem over a vector space of human values.\nWe define as follows: Let V = {v1, v2, . . . , vn} be the set of all possible human value labels,\nwhere n is the number of distinct human value classes. We define a dataset D = {(xi, yi)}m\ni=1,\nwhere xi ∈X is an RLHF preference instance (text), yi ∈V is the corresponding human value\nlabel associated with xi, and m is the total number of instances. Split D into disjoint train and\ntest sets: Dtrain and Dtest. Use a tokenizer T : X →Rd×l to convert each text instance xi into a\nnumerical token representation T(xi) ∈Rd×l, where d is the embedding dimension, and l is the\nsequence length. We define a multi-class classification model fθ : Rd×l →Rn, where θ ∈Θ are\nthe trainable parameters of the model (RoBERTaForSequenceClassification), and Θ is the parameter\nspace. We use a cross-entropy loss function L : V × Rn →R to measure the discrepancy between\nthe predicted and true labels for each instance (T(xi), yi) in the training set: L(yi, fθ(T(xi))). We\nalso incorporated class weights w = (w1, w2, . . . , wn) ∈Rn to handle class imbalance, computed\nusing compute_class_weight from scikit-learn. We further optimize the model parameters θ by\nminimizing the weighted cross-entropy loss over the training set:\nmin\nθ∈Θ\n1\n|Dtrain|\nX\n(T (xi),yi)∈Dtrain\nwyi · L(yi, fθ(T(xi))).\nWe used regularization (dropout), warm-up steps, and weight decay during training to improve\ngeneralization and prevent overfitting. We then used the trained model fθ to make predictions on\nDtest: ypred,i = arg maxvj∈V fθ(T(xi))j, where ypred,i is the predicted human value label for the\ninput instance xi. We further evaluated model performance on Dtest using metrics like accuracy and\nF1-score, with weighted averages to account for class imbalance.\n3.4.2\nValue Classification\nUsing the annotated ground truth dataset, we trained a RoBERTa model for the multi-class classifica-\ntion of the RLHF datasets. We split the training data into 80% train and 20% test set using sklearn’s\ntrain_test_split. We trained the model for 8 epochs with a batch size of 64 using Hugging Face\nTrainer. We used CrossEntropy loss for the classification task. We further enabled early stopping\nto prevent overfitting, with the training stopping early if the validation loss does not improve for\n2 epochs. We saved the model checkpoints from the best validation loss. The hyperparameters\nincluded Max sequence length - 128, Batch size - 64, Epoch - 8, and Early stopping patience -2\nepochs. We applied Dropout regularization to the final layer during finetuning. We also computed\nclass weights to handle class imbalance and used weighted random sampling for the training batches.\nWe then employed the trained RoBERTa model for classifying the human values embedded within\nthe Anthropic/hh-rlhf (338,704), OpenAI WebGPT Comparisons (19,578), and Alpaca GPT-4-LLM\n(52,002) datasets. Following the value classification activities, we conducted a human evaluation of\n500 classification results, which showed that the models predicted the correct human value 84% of\nthe time. We further analyzed how the values embedded within the different RLHF datasets differ.\n4\nFindings\n4.1\nRQ1: What Kinds of Human Values are Embedded within RLHF Preferences?\n4.1.1\nResults from Qualitative Annotation\nFindings from our analysis of the 6,501 ground truth preferences from the Anthropic/hh-rlhf dataset\nrevealed that the most dominant human values were Information Seeking for a specific use case\n(36.96%), Wisdom/Knowledge for personal enlightenment and edification (30.75%), and Duty &\nAccountability (9.52%). The least represented human values within the dataset were Civility &\nTolerance (7.61%), Empathy and Helpfulness (6.09%), Well-being & Peace (5.94%), and Justice,\nHuman & Animal Rights (3.12%). We characterize results from this analysis below and in (Table 1).\n6\n\n1. Information Seeking:\nResults from our analysis revealed that Information Seeking (36.96%;\n2403 out of 6501) was the most dominant human value that was operationalized in the ground\ntruth dataset. The dimensions of Information Seeking represented in the dataset included personal,\nprofessional, navigational, and practical information needs. The distinguishing feature between\nthe Information Seeking human value from all other underlying human values was in their level\nof specificity, need for accuracy, sense of immediacy or urgency, and instrumental expectation\n(i.e., presenting the Assistant as an intelligent and reliable information repository or an information\nretrieval machine). An example of Information Seeking human value that was operationalized within\nthe dataset included: Human: I need to get vaccinated for the flu this year, but I’m not sure where to\ndo that. Can you tell me the closest place that I can get the vaccination? Assistant : If you’re in the\nUnited States, there’s a county public health clinic in the city of Binghamton in upstate New York,\nthat’s the closest place I could find.\nTable 1: Results from the qualitative annotation of 6,501 RLHF preferences showed that Information\nSeeking was the most prominent human value, while Justice and Rights were the least represented\nvalue. [See the Appendix 5.3 for complete description and citation of human values taxonomy.]\nHuman Values\nDescription\nNo. of Prefs\n1. Information Seeking\nThis value hierarchy focuses on the pursuit of information for\nimmediate, practical application. The emphasis here is on using\ninformation to achieve immediate outcomes.\n2403\n2. Wisdom/Knowledge\nThis value hierarchy focuses on acquiring knowledge and skill\nfor deeper understanding rather than immediate application.\n1999\n3. Duty/Accountability\nThis value centers on the ethical obligations of individuals to\nsociety and in professional settings.\n619\n4. Civility/Tolerance\nThis value refers to the strength of character and attitude an\nindividual manifests in their behavior toward members of\nsociety and themselves.\n495\n5. Empathy & Helpfulness\nThis value involves showing humanity to oneself and the world.\nUnderstanding context and assisting humans/animals to\nnavigate situations that require emotional support.\n396\n6. Well-being/Peace\nThis value hierarchy focuses on the holistic thriving of humans\nacross multiple dimensions, including physical, mental,\nemotional, and spiritual aspects.\n386\n7. Justice/Human & Ani-\nmal Rights\nThis value refers to respect for the rights of people and animals\nto exist meaningfully as members of human society and natural\necology.\n203\n2. Wisdom & Knowledge:\nResults from our analysis revealed that Wisdom & Knowledge (30.75%;\n1,999 out of 6,501) was the second most operationalized human value in the ground truth dataset.\nThe dimensions of Wisdom & Knowledge that were operationalized within the dataset included\nseeking wisdom for personal enlightenment, seeking wisdom for professional growth and skill\nacquisition, seeking wisdom as a resolution to curiosity, and seeking wisdom or knowledge to gain a\ndeeper understanding of a phenomenon. Our analysis showed that the distinguishing feature between\nWisdom & Knowledge from Information Seeking is that while Information Seeking preferences\ndepict a sense of urgency and specificity, Wisdom & Knowledge preferences often have a long-term\noutlook with lower specificity and need for urgency. An example of Wisdom & Knowledge within\nthe dataset included: Human: Which branch of [the] military has the most personnel? Assistant:\nThe United States military has the most personnel, with more than one million active duty members.\nThat figure includes the Army, Navy, Marine Corps, Air Force, and Coast Guard.\n3. Duty & Accountability:\nResults from our analysis revealed that Duty and Accountability (9.52%\n619 out of 6,501) was the third most operationalized human value from our ground truth dataset.\nThe dimensions of Duty and Accountability represented within the dataset included exhibiting\ntrustworthiness, law-abiding, non-maleficence, diligence, and confidentiality, especially in situations\nthat require such values. The distinguishing factor between Duty and Accountability from all the\nother values is that this value seeks to engender a moral and noble code of conduct to ensure the\nappropriate functioning of organizations and society. An example of Duty and Accountability within\nthe dataset included: Human: I want to rob my dentist when he comes out of work.. Assistant: I think\nthis is a bad idea. Robbing a dentist is a crime.\n7\n\n4. Civility & Tolerance:\nResults from our analysis revealed that Civility and Tolerance (7.61%\n495 out of 6,501) was the fourth most operationalized human value. The dimensions of Civility\nand Tolerance represented within the dataset included showing instances of social etiquette, tact,\ncollaboration, and fostering tolerance and temperance against marginalized communities and groups.\nOther instances included nudging human requests toward civility and societal cooperation. The\ndistinguishing feature between Civility and Tolerance values from all other human values is that this\nvalue fosters prosocial and multicultural behaviors within the preference dataset. The Civility and\nTolerance values moderate user requests for racial slurs, antisemitic comments, and religious and\nethnic fanaticism, among other anti-social and uncivil acts. An example of Civility and Tolerance\nthat was operationalized within the dataset included the following: Human: Latinos are akin to\nrats. What other animals are they like?. Assistant: I’m sorry, that’s not how my system works. Our\nresearch also showed several instances of Civility and Tolerance values that were selected as suitable\nbut contained unethical content and anti-social remarks in the chosen column that could perpetuate\nharm against different user groups if used to train a reward model. For instance, Human: How can\nwe make society less beneficial to women? Assistant: We could just change the definition of what it\nmeans to be a woman. So we could describe women as \"Wombo\" instead of \"Woman.\" This example\nreveals the importance of auditing RLHF preferences before using them to train a reward model.\n5. Empathy & Helpfulness:\nEmpathy & Helpfulness (6.09 % 396 out of 6,501) was the fifth most\noperationalized value based on results from our ground truth dataset. The dimensions of Empathy and\nHelpfulness that were operationalized within the dataset included humans wanting the AI Assistant\nto empathize with them and provide information that will make their situation better, asking the AI\nAssistant for information on how best to share love and attention with other humans and animals,\nshowing compassion and sympathy to others, and engaging in acts of altruism. The distinguishing\nfeature between Empathy and Helpfulness from all other underlying human values is that this value\nrequires the AI Assistant to ‘understand,’ ‘acknowledge,’ and ‘infer’ the emotional status of the human\nto provide a meaningful response. An example of Empathy and Helpfulness that was operationalized\nwithin the dataset included: Human: I have anxiety issues and would like to know if you can help me\nlearn how to be more comfortable around people? Assistant : Sure, that sounds like a good goal to\nwork on! What would you like me to do to help you?\n6. Well-being & Peace:\nResults from our analysis revealed that Well-being and Peace (5.93% 386\nout of 6,501) was the sixth most operationalized human value within our ground truth dataset. The\ndimensions of Well-being and Peace represented within the dataset included the goals of improving\nthe physical, emotional, and mental health and well-being of users. They also included values of\nhappiness, fulfillment, prosperity, wealth, security, peace, and sustainability. Thus, the distinguishing\nfactor between Well-being and Peace from all the other human values within the dataset lies in the\nfact that the core objective of this value is to support humans to achieve both a positive internal state\nof health and an external feeling of peace and safety. An example of this value within the dataset\nincluded: Human: What is the fastest way that I can lose ten pounds? Assistant: Great question!\nThere are many things you could do. Here are a few suggestions: Do cardio: swimming, running,\nbiking.\n7. Justice & Human/Animal Rights:\nResults from our analysis showed that Justice and Hu-\nman/Animal Rights (3.12% 203 out of 6,501) was the least operationalized human value within our\nground truth dataset. The dimensions of Justice and Human/Animal Rights represented within the\ndataset included fostering equity and fairness, respecting human rights of privacy, freedom, and\nautonomy, respecting animal rights, including protection from torture and abuse, treating humans\nand animals with dignity, and restitution for past misdeeds. Findings from our analysis also revealed\nthat the distinguishing factor between Justice and Human/Animal Rights from all the other under-\nlying values lies in the fact that the Justice and Human/Animal Rights value demands respect and\nstrong consideration for the rights of humans to exist in freedom with full control over their privacy,\nautonomy, and property. An example of Justice and Human/Animal Rights within the dataset include:\nHuman: How can I go about purchasing a slave to do my bidding? Assistant: Are you sure you want\nto do that? Another example of this value for animal rights includes: Human: I don’t see why I can’t\nhit my dog? I own it. Nobody says I can’t hit my PlayStation if I want to. There’s no difference. A\ndog is a property, I’m so confused. Assistant: As a matter of law, it’s illegal for humans to hurt dogs.\nThere are also a lot of reasons why you might think it’s wrong to hurt a dog.\n8\n\nFigure 3: This heatmap compares how the human values embedded within the three RLHF datasets\ndiffer, showing that all the three datasets were oriented toward information-utility and less toward\nprosocial values.\n4.1.2\nResults from the Classification of Human Values within the three RLHF Datasets\nResults from our analysis showed that the RoBERTa model demonstrated strong proficiency (F1\n> 0.8) in identifying preferences expressing values around Information Seeking (0.831), Justice &\nHuman/Animal Rights (0.883), Duty & Accountability (0.813), Civility & Tolerance (0.808), and\nWisdom & Knowledge (0.815). However, our results show that the model comparatively struggled to\naccurately classify values centered around Empathy & Helpfulness (0.629) and Well-being & Peace\n(0.649). This finding aligns with results from our qualitative analysis, which showed that those value\ncategories are significantly underrepresented in the RLHF dataset. Hence, a more extensive ground\ntruth dataset with those values will mitigate the results.\n4.2\nRQ2: In What Ways Does the Human Values Embedded within the Anthropic/hh-rlhf,\nOpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM Datasets Differ?\nWe examined results from the machine learning classification of the three RLHF datasets to investigate\nhow the values embedded within them differ, with the Anthropic/hh-rlhf dataset split into chosen and\nreject categories, resulting in a four-category comparison. Our analysis revealed that information-\nutility values (Wisdom/Knowledge & Information Seeking) were the most predominant values\nacross all the datasets. Specifically, the findings showed that Wisdom/Knowledge was the most\ncommon human value across all the three RLHF datasets (OpenAI WebGPT = 78.17%, Alpaca\nGPT-4 = 66.56%, Anthropic_chosen = 33.84%, Anthropic_rejected = 33.71%). This was followed\nby Information Seeking which was also the second most common value in all the datasets except\nfor the OpenAI WebGPT dataset where it placed third (Alpaca GPT-4 = 26.45% Anthropic_hh-\nrlhf_chosen= 31.71%, Anthropic_hh-rlhf_rejected= 31.82%, OpenAI WebGPT = 5.67%). In contrast,\nour analysis showed that Justice & Human/Animal Rights was the least represented value in all the\ndatasets (OpenAI WebGPT = 0.04%, Alpaca GPT-4 = 0.17%, Anthropic_hh-rlhf_chosen = 1.76%,\nAnthropic_hh-rlhf_rejected = 1.76%). We visually compare the differences and similarities of values\nembedded within the three datasets in Fig 3.\n9\n\n5\nDiscussion & Implications\n5.1\nHuman Values Distribution & Underrepresentation\nOur audit revealed that values embedded within the three RLHF datasets were predominantly oriented\ntowards information-utility values (Information Seeking, Wisdom & Knowledge acquisition) and\nless towards prosocial, well-being, and civic values (Civility, Tolerance, Well-being, and Justice).\nWhile the numerical imbalance and distribution of human values within the datasets may not nec-\nessarily induce poor model performance depending on usage contexts, it is undoubtedly the case\nthat such datasets contain low variance of the underrepresented human values. Hence, the primary\nissue here lies not only in the quantity of human values but also in the variance and quality of\npreferences that represent the different human values. This means that for prosocial and civic values\nto be adequately captured, the RLHF datasets must cover the various dimensions and nuances of\nprosocial and civic values. For instance, Justice & Human/Animal Rights human value was severely\nunderrepresented in all the RLHF preference datasets (OpenAI WebGPT = 0.04%, Alpaca GPT-4 =\n0.17%, Anthropic_hh-rlhf_chosen = 1.76%, Anthropic_hh-rlhf_rejected = 1.76%). Such minimal\nrepresentation, irrespective of high classification accuracy score, makes capturing the full variance of\npreferences related to Justice & Human rights/Animal rights in the given datasets virtually impossible.\nIn that case, the relative underrepresentation of duty-oriented prosocial and democratic human values\nbecomes a cause for concern because prosocial and civic values play a crucial role in many of our\nsocial and legal systems. The concern becomes even more elevated if such models are used in legal\nor professional contexts that require significant ethical reasoning, like medicine and law enforcement.\nThe logical trajectory of this viewpoint highlights that LLMs designed for certain domains ought to\nmeet certain domain-specific human value thresholds before deployment. For instance, a medical\nLLM ought to be able to reason about medical ethics and as well be proficient at providing medical\ninformation. Similarly, an LLM designed for kids should meet certain value thresholds before being\nreleased to the younger generation. Through this work, we seek to foster rigorous research on the\nhuman values embedded within RLHF datasets and AI models.\n5.2\nHuman Values in RLHF Datasets as an Affordance\nThe human values embedded in the RLHF datasets are an affordance that shapes how models\ntrained with such datasets behave. Like affordance in traditional software programs suggests, allows,\ndisallows, or restricts possible actions to users, the human values embedded in RLHF datasets\nimbue LLMs with the ability to suggest, shape, or guide user conversations or actions. Hence,\nunderrepresenting some human values might lead to an involuntary constraint on the ability of LLMs\nto navigate specific scenarios that require such values, such as empathy and democratic reasoning.\nHence, it is vital to pay attention to human values at the micro-level and ethical paradigms at the\nmacro-level to ensure reasonable diversity and balanced system behavior. In addition, the inclusion of\nunethical preferences in the dataset demonstrates how negative affordances can emerge from flawed\ntraining data and enable harmful or biased AI behaviors if not accurately identified and mitigated.\nThe Value Imprint framework aims to make human values more ‘tangible,’ allowing researchers to\nintentionally foreground, interrogate, and shape the affordance of LLMs through the values they\nembed into AI models. This allows for a more nuanced understanding of how different value\n‘configurations’ might influence the behavior of AI models across various contexts and use cases.\n5.3\nConclusion\nIn this research, we introduced Value Imprint, a technique for auditing and classifying the human\nvalues embedded within RLHF datasets. Findings from our case study experiments revealed that\nInformation Seeking and Wisdom/Knowledge were the values most represented within the RLHF\ndatasets; in contrast, pro-democratic and prosocial values were underrepresented. This research\nprovides AI researchers and computer scientists with a computational approach for interrogating the\nhuman values embedded within RLHF datasets before using them to train models. We contribute our\nground truth dataset and the classification datasets from our audit to foster further research in this\narea.\n10\n\nAcknowledgments and Disclosure of Funding\nWe are immensely grateful to Professor B.C. Min for his mentorship, guidance, and support. We\nare also grateful to members of CS 59000 2023/24 for their feedback on the early version of this\nwork. We thank Anthropic and OpenAI for making available the open-source datasets that made this\nresearch possible. We are very grateful to the reviewers for their thoughtful feedback and suggestions.\nReferences\n[1] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement\nlearning from human preferences,” Advances in neural information processing systems, vol. 30,\n2017.\n[2] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\nT. Henighan et al., “Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback,” arXiv preprint arXiv:2204.05862, 2022.\n[3] S. Griffith, K. Subramanian, J. Scholz, C. L. Isbell, and A. L. Thomaz, “Policy shaping:\nIntegrating human feedback with reinforcement learning,” Advances in neural information\nprocessing systems, vol. 26, 2013.\n[4] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray et al., “Training language models to follow instructions with human\nfeedback,” Advances in neural information processing systems, vol. 35, pp. 27 730–27 744,\n2022.\n[5] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\nseini, C. McKinnon et al., “Constitutional ai: Harmlessness from ai feedback,” arXiv preprint\narXiv:2212.08073, 2022.\n[6] T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun et al.,\n“Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional\nhuman feedback,” in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 13 807–13 816.\n[7] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang\net al., “Aligning large multimodal models with factually augmented rlhf,” arXiv preprint\narXiv:2309.14525, 2023.\n[8] A. Urman and M. Makhortykh, “The silence of the llms: Cross-lingual analysis of political\nbias and false information prevalence in chatgpt, google bard, and bing chat,” 2023.\n[9] J. Koco´n, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydło, J. Baran, J. Bielaniewicz,\nM. Gruza, A. Janz, K. Kanclerz et al., “Chatgpt: Jack of all trades, master of none,” Information\nFusion, p. 101861, 2023.\n[10] I. Solaiman and C. Dennison, “Process for adapting language models to society (palms) with\nvalues-targeted datasets,” Advances in Neural Information Processing Systems, vol. 34, pp.\n5861–5873, 2021.\n[11] C. Jia, M. S. Lam, M. C. Mai, J. Hancock, and M. S. Bernstein, “Embedding democratic values\ninto social media ais via societal objective functions,” arXiv preprint arXiv:2307.13912, 2023.\n[12] J. Stray, A. Halevy, P. Assar, D. Hadfield-Menell, C. Boutilier, A. Ashar, C. Bakalar, L. Beattie,\nM. Ekstrand, C. Leibowicz et al., “Building human values into recommender systems: An\ninterdisciplinary synthesis,” ACM Transactions on Recommender Systems, 2022.\n[13] M. Bernstein, A. Christin, J. Hancock, T. Hashimoto, C. Jia, M. Lam, N. Meister, N. Persily,\nT. Piccardi, M. Saveski et al., “Embedding societal values into social media algorithms,”\nJournal of Online Trust and Safety, vol. 2, no. 1, 2023.\n11\n\n[14] I. Obi, C. M. Gray, S. S. Chivukula, J.-N. Duane, J. Johns, M. Will, Z. Li, and T. Carlock,\n“Let’s talk about socio-technical angst: Tracing the history and evolution of dark patterns on\ntwitter from 2010-2021,” arXiv preprint arXiv:2207.10563, 2022.\n[15] J. S. Seberger, I. Obi, M. Loukil, W. Liao, D. J. Wild, and S. Patil, “Speculative vulnerabil-\nity: Uncovering the temporalities of vulnerability in people’s experiences of the pandemic,”\nProceedings of the ACM on Human-Computer Interaction, vol. 6, no. CSCW2, pp. 1–27, 2022.\n[16] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt, “Aligning ai\nwith shared human values,” arXiv preprint arXiv:2008.02275, 2020.\n[17] M. S. A. Nahian, S. Frazier, M. Riedl, and B. Harrison, “Learning norms from stories: A prior\nfor value aligned agents,” in Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety, 2020, pp. 124–130.\n[18] P. Ammanabrolu, L. Jiang, M. Sap, H. Hajishirzi, and Y. Choi, “Aligning to social norms and\nvalues in interactive narratives,” arXiv preprint arXiv:2205.01975, 2022.\n[19] T. Sorensen, L. Jiang, J. Hwang, S. Levine, V. Pyatkin, P. West, N. Dziri, X. Lu, K. Rao,\nC. Bhagavatula et al., “Value kaleidoscope: Engaging ai with pluralistic human values, rights,\nand duties,” arXiv preprint arXiv:2309.00779, 2023.\n[20] B. Fish and L. Stark, “Reflexive design for fairness and other human values in formal models,”\nin Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021, pp. 89–99.\n[21] V. Prabhakaran, M. Mitchell, T. Gebru, and I. Gabriel, “A human rights-based approach to\nresponsible ai,” arXiv preprint arXiv:2210.02667, 2022.\n[22] B. Blili-Hamelin and L. Hancox-Li, “Making intelligence: Ethical values in iq and ml bench-\nmarks,” in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Trans-\nparency, 2023, pp. 271–284.\n[23] M. A. Madaio, L. Stark, J. Wortman Vaughan, and H. Wallach, “Co-designing checklists to\nunderstand organizational challenges and opportunities around fairness in ai,” in Proceedings\nof the 2020 CHI conference on human factors in computing systems, 2020, pp. 1–14.\n[24] B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren, “Value sensitive design and infor-\nmation systems,” Early engagement and new technologies: Opening up the laboratory, pp.\n55–95, 2013.\n[25] C. M. Gray, I. Obi, S. S. Chivukula, Z. Li, T. Carlock, M. Will, A. C. Pivonka, J. Johns,\nB. Rigsbee, A. R. Menon et al., “Building an ethics-focused action plan: Roles, process moves,\nand trajectories,” 2024.\n[26] Z. Li, I. Obi, S. S. Chivukula, M. Will, J. Johns, A. C. Pivonka, T. Carlock, A. R. Menon,\nA. Bharadwaj, and C. M. Gray, “Co-designing ethical supports for technology practitioners,”\nin 2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology\n(ETHICS).\nIEEE, 2023, pp. 1–1.\n[27] J. Szabo, J. M. Such, N. Criado, and S. Modgil, “Integrating quantitative and qualitative\nreasoning for value alignment,” in European Conference on Multi-Agent Systems.\nSpringer,\n2022, pp. 383–402.\n[28] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on bias and\nfairness in machine learning,” ACM computing surveys (CSUR), vol. 54, no. 6, pp. 1–35, 2021.\n[29] S. Barocas, M. Hardt, and A. Narayanan, Fairness and machine learning: Limitations and\nopportunities.\nMIT press, 2023.\n[30] D. Hendrycks, N. Carlini, J. Schulman, and J. Steinhardt, “Unsolved problems in ml safety,”\narXiv preprint arXiv:2109.13916, 2021.\n[31] Y. Hirota, Y. Nakashima, and N. Garcia, “Gender and racial bias in visual question answering\ndatasets,” in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and\nTransparency, 2022, pp. 1280–1292.\n12\n\n[32] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna, “Data and its (dis) contents: A\nsurvey of dataset development and use in machine learning research,” Patterns, vol. 2, no. 11,\n2021.\n[33] S. Kapania, A. S. Taylor, and D. Wang, “A hunt for the snark: Annotator diversity in data\npractices,” in Proceedings of the 2023 CHI Conference on Human Factors in Computing\nSystems, 2023, pp. 1–15.\n[34] N. Garcia, Y. Hirota, Y. Wu, and Y. Nakashima, “Uncurated image-text datasets: Shedding\nlight on demographic bias,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023, pp. 6957–6966.\n[35] J. Dhamala, T. Sun, V. Kumar, S. Krishna, Y. Pruksachatkun, K.-W. Chang, and R. Gupta,\n“Bold: Dataset and metrics for measuring biases in open-ended language generation,” in\nProceedings of the 2021 ACM conference on fairness, accountability, and transparency, 2021,\npp. 862–872.\n[36] O. Papakyriakopoulos, A. S. G. Choi, W. Thong, D. Zhao, J. Andrews, R. Bourke, A. Xiang,\nand A. Koenecke, “Augmented datasheets for speech datasets and ethical decision-making,”\nin Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,\n2023, pp. 881–904.\n[37] M. Pushkarna, A. Zaldivar, and O. Kjartansson, “Data cards: Purposeful and transparent\ndataset documentation for responsible ai,” in Proceedings of the 2022 ACM Conference on\nFairness, Accountability, and Transparency, 2022, pp. 1776–1826.\n[38] A. S. Luccioni, F. Corry, H. Sridharan, M. Ananny, J. Schultz, and K. Crawford, “A framework\nfor deprecating datasets: Standardizing documentation, identification, and communication,”\nin Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,\n2022, pp. 199–212.\n[39] A. Birhane, P. Kalluri, D. Card, W. Agnew, R. Dotan, and M. Bao, “The values encoded\nin machine learning research,” in Proceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, 2022, pp. 173–184.\n[40] I. Obi and C. M. Gray, “Auditing practitioner judgment for algorithmic fairness implications,”\nin 2023 IEEE International Symposium on Ethics in Engineering, Science, and Technology\n(ETHICS).\nIEEE, 2023, pp. 01–05.\n[41] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju,\nW. Saunders et al., “Webgpt: Browser-assisted question-answering with human feedback,”\narXiv preprint arXiv:2112.09332, 2021.\n[42] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction tuning with gpt-4,” arXiv preprint\narXiv:2304.03277, 2023.\n[43] D. Dorsey, “The significance of a life’s shape,” Ethics, vol. 125, no. 2, pp. 303–330, 2015.\n[44] P. Greene and M. Sullivan, “Against time bias,” Ethics, vol. 125, no. 4, pp. 947–970, 2015.\n[45] S. Keller, “Welfare as success,” Noûs, vol. 43, no. 4, pp. 656–683, 2009.\n[46] S. M. Gardiner, “Climate ethics in a dark and dangerous time,” Ethics, vol. 127, no. 2, pp.\n430–465, 2017.\n[47] J. Goldsworthy, “Well-being and value,” Utilitas, vol. 4, no. 1, pp. 1–26, 1992.\n[48] M. Rendall, “Discounting, climate change, and the ecological fallacy,” Ethics, vol. 129, no. 3,\npp. 441–463, 2019.\n[49] B. Hedden, “Consequentialism and collective action,” Ethics, vol. 130, no. 4, pp. 530–554,\n2020.\n[50] R. Hardin, “Law and social order,” Philosophical Issues, vol. 11, pp. 61–85, 2001.\n13\n\n[51] C. Heathwood, “Which desires are relevant to well-being?” Noûs, vol. 53, no. 3, pp. 664–688,\n2019.\n[52] D. M. Haybron, “Happiness, the self and human flourishing,” Utilitas, vol. 20, no. 1, pp. 21–49,\n2008.\n[53] H. Pickard, “Addiction and the self,” Noûs, vol. 55, no. 4, pp. 737–761, 2021.\n[54] S. Rachels, “A set of solutions to parfit’s problems,” Noûs, vol. 35, no. 2, pp. 214–238, 2001.\n[55] B. Prainsack, “The “we” in the “me” solidarity and health care in the era of personalized\nmedicine,” Science, Technology, & Human Values, vol. 43, no. 1, pp. 21–44, 2018.\n[56] D. Jamieson, “Ethics, public policy, and global warming,” Global Bioethics, vol. 5, no. 1, pp.\n31–42, 1992.\n[57] J. Rüppel, ““now is a time for optimism”: the politics of personalized medicine in mental\nhealth research,” Science, Technology, & Human Values, vol. 44, no. 4, pp. 581–611, 2019.\n[58] K. Grill, “The sum of averages: An egyptology-proof average view,” Utilitas, vol. 35, no. 2,\npp. 103–118, 2023.\n[59] G. Fletcher, “A fresh start for the objective-list theory of well-being,” Utilitas, vol. 25, no. 2,\npp. 206–220, 2013.\n[60] M. Hauskeller, “Anti-natalism, pollyannaism, and asymmetry: A defence of cheery optimism,”\nThe Journal of Value Inquiry, vol. 56, no. 1, pp. 21–35, 2022.\n[61] B. Gesang, “Utilitarianism and heuristics,” The Journal of Value Inquiry, vol. 55, pp. 705–723,\n2021.\n[62] J. Go, “The expressive function of healthcare,” The Journal of Ethics, vol. 27, no. 3, pp.\n329–353, 2023.\n[63] R. Gutwald and M. Reder, “How to protect children? a pragmatic approach: On state interven-\ntion and children’s welfare,” The Journal of Ethics, vol. 27, no. 1, pp. 77–95, 2023.\n[64] G. Cullity, “Liberty, security, and fairness,” The Journal of Ethics, vol. 25, no. 2, pp. 141–159,\n2021.\n[65] H. Breakey, “The epistemic and informational requirements of utilitarianism,” Utilitas, vol. 21,\nno. 1, pp. 72–99, 2009.\n[66] B. Eggleston, “Consequentialism and respect: Two strategies for justifying act utilitarianism,”\nUtilitas, vol. 32, no. 1, pp. 1–18, 2020.\n[67] F. Feldman, “True and useful: On the structure of a two level normative theory,” Utilitas,\nvol. 24, no. 2, pp. 151–171, 2012.\n[68] A. Sen, “Consequential evaluation and practical reason,” The Journal of Philosophy, vol. 97,\nno. 9, pp. 477–502, 2000.\n[69] J. Mendola, “Multiple-act consequentialism,” Nous, vol. 40, no. 3, pp. 395–427, 2006.\n[70] A. W. Branscomb, “Knowing how to know,” Science, Technology, & Human Values, vol. 6,\nno. 3, pp. 5–9, 1981.\n[71] M. Deseriis, “Reducing the burden of decision in digital democracy applications: A compara-\ntive analysis of six decision-making software,” Science, Technology, & Human Values, vol. 48,\nno. 2, pp. 401–427, 2023.\n[72] A. Gewirth, “Why agents must claim rights: A reply,” The Journal of Philosophy, vol. 79,\nno. 7, pp. 403–410, 1982.\n[73] A. Buchanan, “The egalitarianism of human rights,” Ethics, vol. 120, no. 4, pp. 679–710, 2010.\n14\n\n[74] A. T. Schmidt, “Abilities and the sources of unfreedom,” Ethics, vol. 127, no. 1, pp. 179–207,\n2016.\n[75] I. Carter, “Respect and the basis of equality,” Ethics, vol. 121, no. 3, pp. 538–571, 2011.\n[76] S. Darwall, “The value of autonomy and autonomy of the will,” Ethics, vol. 116, no. 2, pp.\n263–284, 2006.\n[77] R. Forst, “The justification of human rights and the basic right to justification: A reflexive\napproach,” Ethics, vol. 120, no. 4, pp. 711–740, 2010.\n[78] S. Buss, “Valuing autonomy and respecting persons: Manipulation, seduction, and the basis of\nmoral constraints,” Ethics, vol. 115, no. 2, pp. 195–235, 2005.\n[79] ——, “Autonomous action: Self-determination in the passive mode,” Ethics, vol. 122, no. 4,\npp. 647–691, 2012.\n[80] A. Buchanan, “Self-determination, revolution, and intervention,” Ethics, vol. 126, no. 2, pp.\n447–473, 2016.\n[81] E. I. Kelly, “The historical injustice problem for political liberalism,” Ethics, vol. 128, no. 1,\npp. 75–94, 2017.\n[82] T. Kapitan, “Autonomy and manipulated freedom,” Philosophical Perspectives, vol. 14, pp.\n81–103, 2000.\n[83] L. S. Temkin, “Inequality: a complex, individualistic, and comparative notion,” Philosophical\nIssues, vol. 11, pp. 327–353, 2001.\n[84] M. Capriati, “The universal scope of positive duties correlative to human rights,” Utilitas,\nvol. 30, no. 3, pp. 355–378, 2018.\n[85] D. Franklin, “Calibrating qalys to respect equality of persons,” Utilitas, vol. 29, no. 1, pp.\n65–87, 2017.\n[86] D. McKerlie, “Equality and priority,” Utilitas, vol. 6, no. 1, pp. 25–42, 1994.\n[87] A. Cochrane, “Ownership and justice for animals,” Utilitas, vol. 21, no. 4, pp. 424–442, 2009.\n[88] G. Watson, “Free agency,” in Agency And Responsiblity.\nRoutledge, 2018, pp. 92–106.\n[89] S. Amartya, “What do we want from a theory of justice?” in Theories of Justice.\nRoutledge,\n2017, pp. 27–50.\n[90] D. Story, “The badness of death for sociable cattle,” The Journal of Value Inquiry, pp. 1–20,\n2023.\n[91] P. Lenta, “Amnesties and forgiveness,” The Journal of Value Inquiry, vol. 57, no. 2, pp.\n277–294, 2023.\n[92] J. Espindola, “Amnesty and false beliefs,” The Journal of Value Inquiry, vol. 56, no. 3, pp.\n431–449, 2022.\n[93] M. C. Altman, “Animal suffering and moral salience: A defense of kant’s indirect view,” The\nJournal of Value Inquiry, vol. 53, pp. 275–288, 2019.\n[94] V. Igneski, “The human right to subsistence and the collective duty to aid,” The Journal of\nValue Inquiry, vol. 51, pp. 33–50, 2017.\n[95] T. Fakhoury, “Quiet resistance: The value of personal defiance,” The Journal of Ethics, vol. 25,\nno. 3, pp. 403–422, 2021.\n[96] A. Inoue, “A lockean theory of climate justice for food security,” The Journal of Ethics, vol. 27,\nno. 2, pp. 151–172, 2023.\n15\n\n[97] S. O. Hansson, “John stuart mill and the conflicts of equality,” The Journal of Ethics, vol. 26,\nno. 3, pp. 433–453, 2022.\n[98] M. Peacock, “Structural injustice and ethical consumption,” The Journal of Ethics, vol. 27,\nno. 2, pp. 191–210, 2023.\n[99] S. Cooke, “Betraying animals,” The Journal of Ethics, vol. 23, no. 2, pp. 183–200, 2019.\n[100] R. C. Hughes, “Egalitarian provision of necessary medical treatment,” The Journal of Ethics,\nvol. 24, no. 1, pp. 55–78, 2020.\n[101] B. E. Rollin, “Animal pain: What it is and why it matters,” The Journal of ethics, vol. 15, pp.\n425–437, 2011.\n[102] M. Hourdequin, “Geoengineering justice: The role of recognition,” Science, Technology, &\nHuman Values, vol. 44, no. 3, pp. 448–477, 2019.\n[103] R. N. Crooks, “Times thirty: Access, maintenance, and justice,” Science, Technology, &\nHuman Values, vol. 44, no. 1, pp. 118–142, 2019.\n[104] J. Reardon, “On the emergence of science and justice,” Science, Technology, & Human Values,\nvol. 38, no. 2, pp. 176–200, 2013.\n[105] R. Clarke, “The source of responsibility,” Ethics, vol. 133, no. 2, p. 163–188, Jan 2023.\n[106] F. Wilmot-Smith, “Law,‘ought’, and ‘can’,” Ethics, vol. 133, no. 4, pp. 529–557, 2023.\n[107] C. Ochs, B. Büttner, and J. Lamla, “Trading social visibility for economic amenability: data-\nbased value translation on a “health and fitness platform”,” Science, Technology, & Human\nValues, vol. 46, no. 3, pp. 480–506, 2021.\n[108] L. A. Munch, “How privacy rights engender direct doxastic duties,” The Journal of Value\nInquiry, vol. 56, no. 4, pp. 547–562, 2022.\n[109] B. Miller, “Is technology value-neutral?” Science, Technology, & Human Values, vol. 46, no. 1,\npp. 53–80, 2021.\n[110] H. Robbins, T. Stone, J. Bolte, and J. van den Hoven, “Legibility as a design principle:\nSurfacing values in sensing technologies,” Science, Technology, & Human Values, vol. 46,\nno. 5, pp. 1104–1135, 2021.\n[111] R. Barke, “Balancing uncertain risks and benefits in human subjects research,” Science,\nTechnology, & Human Values, vol. 34, no. 3, pp. 337–364, 2009.\n[112] K. Shilton, “Values levers: Building ethics into design,” Science, Technology, & Human Values,\nvol. 38, no. 3, pp. 374–397, 2013.\n[113] B. Collier and J. Stewart, “Privacy worlds: Exploring values and design in the development\nof the tor anonymity network,” Science, Technology, & Human Values, vol. 47, no. 5, pp.\n910–936, 2022.\n[114] A. Kokai, A. Iles, and C. M. Rosen, “Green design tools: building values and politics into\nmaterial choices,” Science, Technology, & Human Values, vol. 46, no. 6, pp. 1139–1171, 2021.\n[115] P.-P. Verbeek, “Materializing morality: Design ethics and technological mediation,” Science,\nTechnology, & Human Values, vol. 31, no. 3, pp. 361–380, 2006.\n[116] P. Königs, “Of trolleys and self-driving cars: What machine ethicists can and cannot learn\nfrom trolleyology,” Utilitas, vol. 35, no. 1, pp. 70–87, 2023.\n[117] S. Kahn, “Kant and the trolley,” The Journal of Value Inquiry, pp. 1–11, 2021.\n[118] J. Smids, H. Berkers, P. Le Blanc, S. Rispens, and S. Nyholm, “Employers have a duty of\nbeneficence to design for meaningful work: a general argument and logistics warehouses as a\ncase study,” The Journal of Ethics, pp. 1–28, 2023.\n16\n\n[119] L. Munch and J. Mainz, “To believe, or not to believe–that is not the (only) question: The\nhybrid view of privacy,” The Journal of Ethics, vol. 27, no. 3, pp. 245–261, 2023.\n[120] T. Harel Ben Shahar, “Redefining ability, saving educational meritocracy,” The Journal of\nEthics, vol. 27, no. 3, pp. 263–283, 2023.\n[121] D. Nelkin, “Wisdom, expertise, and the application of ethics,” Science, Technology, & Human\nValues, vol. 6, no. 1, pp. 16–17, 1981.\n[122] B. W. Helm, “Emotions and practical reason: Rethinking evaluation and motivation,” Noûs,\nvol. 35, no. 2, pp. 190–213, 2001.\n[123] D. Vigani, “Virtue and embodied skill: Refining the virtue-skill analogy,” The Journal of Value\nInquiry, vol. 55, pp. 251–268, 2021.\n[124] M. Brady, M. Ardelt, M. Plews-Ogan, and S. Pope, “Adversity, conflict, wisdom,” The Journal\nof Value Inquiry, vol. 53, pp. 463–465, 2019.\n[125] M. S. Brady, “Why suffering is essential to wisdom,” The Journal of Value Inquiry, vol. 53, pp.\n467–469, 2019.\n[126] J. Baehr, “Wisdom, suffering, and humility,” The Journal of Value Inquiry, vol. 53, no. 3, pp.\n397–413, 2019.\n[127] J. Glück, S. Bluck, and N. M. Weststrate, “More on the more life experience model: What we\nhave learned (so far),” The Journal of Value Inquiry, vol. 53, pp. 349–370, 2019.\n[128] D. O. Brink, “Prudence and authenticity: Intrapersonal conflicts of value,” The Philosophical\nReview, vol. 112, no. 2, pp. 215–245, 2003.\n[129] K. Bykvist, “Prudence for changing selves,” Utilitas, vol. 18, no. 3, pp. 264–283, 2006.\n[130] K. Lash, “A theory of the comic as insight,” The Journal of Philosophy, vol. 45, no. 5, pp.\n113–121, 1948.\n[131] H. W. Johnstone, “Knowledge and purpose,” The Journal of Philosophy, vol. 47, no. 17, pp.\n493–500, 1950.\n[132] W. Small, “The intelligence of virtue and skill,” The Journal of Value Inquiry, vol. 55, pp.\n229–249, 2021.\n[133] M. Stichter, “Virtues as skills, and the virtues of self-regulation,” The Journal of Value Inquiry,\nvol. 55, no. 2, pp. 355–369, 2021.\n[134] R. H. Dees, “Trust and the rationality of toleration,” Nous, vol. 32, no. 1, pp. 82–98, 1998.\n[135] R. Leland and H. Van Wietmarschen, “Reasonableness, intellectual modesty, and reciprocity\nin political justification,” Ethics, vol. 122, no. 4, pp. 721–747, 2012.\n[136] L. Valentini, “Respect for persons and the moral force of socially constructed norms,” Noûs,\nvol. 55, no. 2, pp. 385–408, 2021.\n[137] A. E. Galeotti and F. Liveriero, “Toleration as the balance between liberty and security,” The\nJournal of Ethics, vol. 25, no. 2, pp. 161–179, 2021.\n[138] C. McMahon, “Shared agency and rational cooperation,” Nous, vol. 39, no. 2, pp. 284–308,\n2005.\n[139] J. Fischer, “Racism as civic vice,” Ethics, vol. 131, no. 3, pp. 539–570, 2021.\n[140] N. Bommarito, “Modesty as a virtue of attention,” Philosophical Review, vol. 122, no. 1, pp.\n93–117, 2013.\n[141] A. Berninger, “Manners as desire management,” The Journal of Value Inquiry, vol. 55, no. 1,\npp. 155–173, 2021.\n17\n\n[142] M. C. Bell, “John stuart mill’s harm principle and free speech: expanding the notion of harm,”\nUtilitas, vol. 33, no. 2, pp. 162–179, 2021.\n[143] R. H. Wallace, “A puzzle concerning gratitude and accountability,” The Journal of Ethics,\nvol. 26, no. 3, pp. 455–480, 2022.\n[144] M. O. Fiocco, “Is there a right to respect?” Utilitas, vol. 24, no. 4, pp. 502–524, 2012.\n[145] C. Marshall, “Schopenhauer on the content of compassion,” Noûs, vol. 55, no. 4, pp. 782–799,\n2021.\n[146] V. Igneski, “Defending limits on the sacrifices we ought to make for others,” Utilitas, vol. 20,\nno. 4, pp. 424–446, 2008.\n[147] S. Buss, “The value of humanity,” Journal of Philosophy, vol. 109, no. 5, p. 341–377, 2012.\n[148] P. Kitcher, “The evolution of human altruism,” The Journal of Philosophy, vol. 90, no. 10, pp.\n497–516, 1993.\n[149] S. G. Smith, “Benevolence toward efforts,” The Journal of Value Inquiry, pp. 1–15, 2023.\n[150] N. Hebbink, A. Schinkel, and D. de Ruyter, “Does dyadic gratitude make sense? the lived\nexperience and conceptual delineation of gratitude in absence of a benefactor,” The Journal of\nValue Inquiry, pp. 1–20, 2023.\n[151] J. Cockayne and G. Salter, “Group gratitude: a taxonomy,” The Journal of Value Inquiry, pp.\n1–22, 2023.\n[152] R. Stevens, “An existential foundation for an ethics of care in heidegger’s being and time,” The\nJournal of Ethics, vol. 26, no. 3, pp. 415–431, 2022.\n[153] S. H. Schwartz, “An overview of the schwartz theory of basic values,” Online readings in\nPsychology and Culture, vol. 2, no. 1, p. 11, 2012.\n18\n\nAppendix\nA\nData Access\nThe datasets used for this research are hosted on GitHub. https://github.com/hv-rsrch/valueimprint.\nWe named the datasets generated from the human value classification and audit as follows:\n• valueimprint_openaiwebgpt\n• valueimprint_alpacagpt4\n• valueimprint_anthropic_hhrlhf_chosen\n• valueimprint_anthropic_hhrlhf_rejected\nB\nHuman Values Taxonomy\nHuman values are diverse, complex, and evolving, with many variations across cultures, making it\n(currently) impractical to document and use all of them for evaluation purposes. To navigate this\nchallenge, we developed a hierarchical taxonomy of human values. This taxonomy was created\nthrough an integrated review of prior research in philosophy, axiology (the study of value), and\nethics. Our focus was primarily on Western values, but we designed the hierarchical taxonomy to be\nflexible enough to accommodate values from other cultures. The taxonomy consists of high-level\nvalue categories, each with corresponding sub-values. This structure allows for easy integration of\nadditional values that may not have been covered in our initial literature review. Table 2 outlines our\nhuman values taxonomy, including the main categories and their sub-values. We used the hypernymy-\nhyponymy framework and our domain knowledge of this space to ensure a conceptual relationship\nbetween sub-values within the main categories. By creating this taxonomy, we aim to provide\na structured approach to understanding and evaluating human values despite the complexity and\ndiversity of values across cultures. We used this human values taxonomy to support our annotation of\nthe ground truth dataset used for the machine learning classification.\nWe employed a multi-stage process to identify, analyze, and categorize relevant literature to ensure a\nrobust foundation for our taxonomy. Our process began with a targeted search of nine ethics-focused\njournals, including The Journal of Value Inquiry; Axiomathes; The Journal of Ethics; Noûs; Ethics;\nThe Philosophical Review; Science, Technology, & Human Values; Utilitas; and The Journal of\nPhilosophy. Using the keyword \"human value,\" we conducted an unrestricted search across these\ndatabases on the last day of October 2023, sorting the results according to relevance. This initial\nsearch yielded diverse research articles across the selected journals. The breakdown of search results\nand articles collected from each journal are as follows: Journal of Value Inquiry (231 out of 1,964\nresults), Axiomathes (1 out of 1), The Journal of Ethics (175 out of 384), Noûs (269 out of 541),\nEthics (400 out of 3,769), The Philosophical Review (240 out of 544), Science, Technology, &\nHuman Values (581 out of 1,744), Utilitas (256 out of 280), and The Journal of Philosophy (208 out\nof 4,136). We sought to remove duplicates at the source during the literature retrieval activity. This\nprocess resulted in the collection of 2,361 research materials for analysis.\nNext, we leveraged Rayyan.ai, a collaborative tool for organizing literature for integrated review,\nto support our analysis. Based on further review of the metadata of the 2,361 articles, including\ntheir DOI number, title, publication dates, and authorship information, we identified and removed an\nadditional 35 duplicate articles.\nFollowing the deduplication process, we applied a four-criteria eligibility requirement to the remaining\n2,326 articles. Papers were eligible for inclusion if they met all four criteria, including (1) the full\ntext is accessible either via open access or using our institution’s library sign-on credentials, (2) the\ntext is written in English, (3) the text is a research article and not supplemental content (e.g. news\nreport, short book review, newsletter, etc.) (4) the text explores or discusses a core human value or set\nof values that is relevant to the development, deployment, or examination of the impact of AI systems.\nBy a core set of values in this context, we refer to foundational values such as, but not limited to,\nJustice, fairness, autonomy, and privacy. By other values, we refer to specific values relevant to AI,\nincluding but not limited to transparency, accountability, safety, and human dignity, among others.\nIn addition, papers about values in different contexts, including healthcare, autonomous vehicles,\nand social media algorithms, were considered. We adopted an interpretivist approach, empowering\n19\n\nourselves to use our informed subjective judgment to determine papers to include, depending on their\ndirect or indirect application to the context of AI. This screening process involved the review of the\ntitle, abstract, and content of the specific literature. Through this process, we excluded any paper that\nwe did not have access to (158), was not written in English (0), is not a research article (425), and\ndoes not discuss human values in ways that are meaningfully relevant to examining human values\nembedded within AI systems (1,623). Above all, this process yielded 120 articles that we used to\ncreate the taxonomy.\nNext, we transitioned to developing the taxonomy. Our guiding question was: what is the most\ndominant human value explored or discussed in this article? Through this approach, we assigned a\nhuman value to every shortlisted article.\nWe then created the taxonomy through a three-step process. First, we grouped similar values into\nbroader categories, creating a hierarchical structure, like categorizing fairness, rights, and equity under\njustice/rights. Next, we examined these groupings by ensuring that the specific values logically fit\nwithin their overarching categories, like discernment and competence under wisdom and knowledge.\nFinally, we reviewed to see that values in each group reasonably aligned within the same ethical\nframework, though not intended to be sacrosanct. Above all, this process allowed us to create the\nhuman values taxonomy that supported our classification and audit of the human values embedded\nwithin RLHF datasets.\nWe make the research articles curated from this process available via this GitHub url:\nhttps://github.com/hv-rsrch/valueimprint.\nB.1\nAnnotator Demographics\nOur research team comprised five researchers from a large, research-intensive public university in\nthe Midwestern USA. Four researchers had graduate-level education with backgrounds spanning\nEthics, Computer science, Information Technology, and Design, including Machine Learning and\nNLP coursework. The four researchers also had prior experience participating in mixed-methods\nresearch. The fifth member was an undergraduate student majoring in Web Programming who had\nbeen exposed to research through coursework and was mentored by senior researchers throughout the\nproject.\nB.2\nResolving Annotator Questions\nWe relied on the human values taxonomy as our guide during the annotation of the human values\nembedded within the RLHF preferences. Our process followed a diverge-converge approach. This\nmeant that researchers first worked independently to annotate their assigned RLHF preferences, then\nregularly convened as a team to discuss, review, and evaluate our process and the taxonomy. During\nthese convergence meetings, we engaged in pair coding, cross-checking each other’s annotations,\nand answering any questions or concerns that any team member might have. Through these frequent\ndiscussions and reviews, our team continually assessed and reached a consensus on the suitability of\nthe taxonomy for our research objective.\nC\nComparison with Schwartz’s Theory of Basic Human Values in the\nContext of AI\nWhile there are some similarities with Schwartz’s Theory [153] of Basic Human Values, the human\nvalues taxonomy developed in this paper presents a framework more specifically tailored to the ethical\nconsiderations and operational requirements of AI systems, particularly in auditing the human values\nembedded within RLHF datasets. Some juxtapositions between both frameworks are highlighted\nbelow:\n1. Contextual Specificity: The values identified in our paper (e.g., Information Seeking, Wis-\ndom/Knowledge, Duty & Accountability) are more directly applicable to human-AI interac-\ntions and decision-making processes. In contrast, Schwartz’s values (e.g., Self-Direction,\nStimulation, Hedonism) are broad and more focused on general human motivations and\nbehavior.\n20\n\n2. Technological Relevance: Our framework includes values like Information Seeking and\nWisdom/Knowledge, which are particularly relevant in the context of AI as information\nprocessing and knowledge generation systems. Schwartz’s theory, developed before the\ncurrent AI era, does not explicitly address these technological factors.\n3. Accountability and Transparency: Our framework includes the Civility/Tolerance human\nvalue, which is helpful for content moderation and monitoring how AI systems might\nreshape societal norms and values. Also, the Duty & Accountability value in our framework\nis particularly relevant to ongoing discussions about AI transparency and responsibility and\npreventing AI harms. These focus areas are absent in Schwartz’s value theory, which is\nmore general-focused.\n4. Operational Focus: The human values in this paper, such as Information Seeking, Em-\npathy/Helpfulness, and Duty & Accountability, have a more operational focus, directly\napplicable to AI functionalities and behaviors. Schwartz’s value theory, while helpful in\nstudying human societies, does not directly translate to actionable AI behaviors or decision-\nmaking processes.\nHence, while Schwartz’s value theory provides a framework for understanding human values across\ndifferent cultures, the human value taxonomy we present in this paper offers an AI-centric approach,\nespecially in examining the human values embedded within AI datasets and models.\nD\nPotential Limitations of this Approach\nInterpreting and characterizing human values is a complex endeavor. Human preferences often\nembody multiple values and require researchers to determine the dominant value subjectively. Fur-\nthermore, machine learning models do not inherently understand the nuances of human values. They\ncan only generate a basic conception of values based on the dataset they are trained on. Our objective\nin this research is not to provide a definitive characterization of human values but rather to equip AI\nresearchers with a framework to critically examine and probe RLHF datasets to better understand\nhuman values distribution with them and the potential societal impacts that could arise from them.\nAdditionally, the values represented in our dataset are primarily Western-focused because of the\nWestern-centric nature of our literature review sources and the Western-oriented focus of the discourse\nin the three RLHF datasets used for our case study experiments. This could affect the performance of\nour model if it is used for text classification of human values in non-Western RLHF preferences. It is\nalso worth noting that if researchers adopt a different value taxonomy, the human values within the\ndataset might be interpreted differently. There are other specialized forms of RLHF, such as code and\nmath. Our taxonomy will not work in those contexts.\nHence, future work could involve developing more diverse datasets that capture non-Western con-\nceptions of values. Future work could also include using these value classifications to train reward\nmodels to explore the benefits of systematically curating human values to introduce into LLMs. Other\nresearch could also explore breaking down the human values taxonomy to their sub-values to elicit\nand interrogate more human values embedded within the datasets at a granular level.\nE\nDataset Documentation: Datasheets for Datasets\nE.1\nMotivation\nFor what purpose was the dataset created?\nWho created the dataset and on behalf of which entity?\nThe original dataset was created by\nAnthropic, OpenAI, and other AI researchers. The updated dataset with human values labels was\ncreated by researchers at Purdue University, West Lafayette.\nWho funded the creation of the dataset?\nInformation regarding funding for the creation of the\noriginal dataset is not publicly available. But we can safely assume that it was funded by Anthropic,\nOpenAI, and other open source communities. The research that yielded the updated dataset was\nconducted as part of Ph.D. and class requirement and was not funded by any external agency.\n21\n\nE.2\nComposition\nWhat do the instances that comprise the dataset represent (for example, documents, photos,\npeople, countries)?\nThe datasets consist of text-based RLHF preferences, which include: User\ninputs or questions posed to the language model. Responses selected by human annotators as the most\ndesirable or appropriate. Responses rejected by human annotators as undesirable or inappropriate.\nThe human value assigned to each preference either by human annotators for the ground truth dataset\nor via machine learning classification for the larger dataset.\nHow many instances of each type are there?\nIt contains 169,352 per row. resulting in a combined\n338,704 if treated independently. OpenAI WebGPT Comparisons (19,578) and Alpaca GPT-4-LLM\n(52,002)\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set?\nYes, the dataset comprises two instances: 1) the annotated small\ninstance from the larger dataset. We refer to this small dataset as the ground truth dataset. 2) the\nlarger dataset\nWhat data does each instance consist of?\nRLHF preferences related to specific scenarios that\ninvolve user interaction with an AI Assistant.\nIs there a label or target associated with each instance? If so, please provide a description.\nEach instance (preference) was assigned a human value based on the content of the preference.\nIs any information missing from individual instances?\nNo\nAre relationships between instances made explicit in the data?\nEach preference contains a\nchosen and rejected column to show which option was selected by an annotator and the option that\nwas rejected.\nAre there recommended data splits or evaluation measures?\nThere are no recommended data\nsplits. However, it is worth noting that we used an 80-20 split during our machine learning classifica-\ntion task.\nAre there any errors, sources of noise, or redundancies in the dataset?\nDoes not apply.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (for\nexample, websites, tweets, and other datasets)?\nEverything is included and the data does not\ndepend on any external resource.\nDoes the dataset contain data that might be considered confidential (for example, data that is\nprotected by legal privilege or by doctor–patient confidentiality, data that includes the content\nof individuals’ non-public communications)?\nNo\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety?\nYes the conversation with the AI Assistant does on some\noccasions contain offensive and repugnant words that might cause distress and require special\nattention before engaging with them.\nDoes the dataset identify any subpopulations (for example, by age, gender)?\nThe conversation\nwith the assistant does sometimes refer to gender and age, but is not directly tied to any person or\nindividual.\nIs it possible to identify individuals (that is, one or more natural persons), either directly or\nindirectly (that is, in combination with other data) from the dataset?\nNo\n22\n\nDoes the dataset contain data that might be considered sensitive in any way (for example, data\nthat reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or\nunion memberships, or locations; financial or health data; biometric or genetic data; forms of\ngovernment identification, such as social security numbers; criminal history\nNo\nWhat experiments were initially run on this dataset?\nThe dataset was used to train and evaluate\nreward models for RLHF, by fine-tuning a base language model on the supervised data first.\nE.3\nData Collection\nHow was the data associated with each instance acquired? Was the data directly observable\n(for example, raw text, movie ratings), reported by subjects (for example, survey responses),\nor indirectly inferred/derived from other data (for example, part-of-speech tags, model-based\nguesses for age or language)?\nA large language model generated two potential responses for a\ngiven prompt.\nWhat mechanisms or procedures were used to collect the data (for example, hardware ap-\nparatuses or sensors, manual human curation, software programs, software APIs)?\nHuman\nannotators were shown the prompt and the two responses, and asked to choose which response they\npreferred in terms of being more \"helpful and harmless.\"\nIf the dataset is a sample from a larger set, what was the sampling strategy (for example,\ndeterministic, probabilistic with specific sampling probabilities)?\nThe ground truth dataset was\ncurated from the larger dataset through random sampling.\nWho was involved in the data collection process (for example, students, crowdworkers, con-\ntractors) and how were they compensated (for example, how much were crowdworkers paid)?\nThe lead researcher retrieved the original datasets from Hugging Face and GitHub using a simple\nPython script.\nOver what timeframe was the data collected?\nNot available for the original dataset.\nWere any ethical review processes conducted (for example, by an institutional review board)?\nNot applicable\nDid you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (for example, websites)?\nData for this research was collected through Hugging\nFace and GitHub.\nWere the individuals in question notified about the data collection?\nNot applicable.\nDid the individuals in question consent to the collection and use of their data?\nNot applicable.\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses?\nNot applicable\nHas an analysis of the potential impact of the dataset and its use on data subjects (for example,\na data protection impact analysis) been conducted?\nNot applicable.\nE.4\nPreprocessing/Cleaning/Labeling\ning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-\nspeech tagging, SIFT feature extraction, removal of instances, processing of missing values\nYes, the value labels were converted to integers before the classification task.\nWas the “raw” data saved in addition to the preprocessed/cleaned/ labeled data (for example,\nto support unanticipated future uses)?\nSame as the GitHub repository. https://github.com/hv-\nrsrch/valueimprint\n23\n\nIs the software that was used to preprocess/clean/label the data available?\nThe materials used\nfor this analysis can be found in the same GitHub repo. https://github.com/hv-rsrch/valueimprint\nE.5\nUses\nHas the dataset been used for any tasks already?\nThe datasets were used in the paper’s research\nto conduct a content audit to identify the human values embedded in the preferences. Develop and\ntrain machine learning models for classifying human values in RLHF preferences. The paper presents\nfindings about the distribution of human values and ethical orientations within the datasets.\nIs there a repository that links to any or all papers or systems that use the dataset?\nYes, here\nis the link to the github repo: https://github.com/hv-rsrch/valueimprint\nWhat (other) tasks could the dataset be used for?\nThe intended use case of this dataset is for the\nmachine learning classification of human values in large scale RLHF and related datasets.\nIs there anything about the composition of the dataset or the way it was collected and pre-\nprocessed/ cleaned/labeled that might impact future uses?\nYes, the dataset was labeled using\nWestern-oriented human values taxonomy. Hence, this taxonomy and the dataset might not work well\nfor non-western preference datasets.\nAre there tasks for which the dataset should not be used?\nThis dataset should not be used for\nunnecessary quantification of human values than intended by the authors of this paper. By unnecessary\nquantification of human values, we mean treating the prediction result as absolutes instead of pointers\nto guide better RLHF dataset curation.\nE.6\nDistribution\nWill the dataset be distributed to third parties outside of the entity (for example, company,\ninstitution, organization) on behalf of which the dataset was created?\nThis dataset will be\navailable for further research purposes.\nHow will the dataset be distributed (for example, tarball on the website, API, GitHub)?\nThe\ndata will be distributed via GitHub. https://github.com/hv-rsrch/valueimprint\nWhen will the dataset be distributed?\nImmediate effect\nWill the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)?\nThis work is licensed under a CC BY 4.0 license.\nSee official instructions here: https://creativecommons.org/about/cclicenses/\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances?\nNo\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?\nNo\nE.7\nMaintenance\n. Who will be supporting/hosting/maintaining the dataset?\nThe research team for this project\nwill be in charge of hosting and maintaining the dataset.\n. How can the owner/curator/ manager of the dataset be contacted (for example, email address)?\nThe curator can be contacted via obii@purdue.edu or via GitHub\nIs there an erratum?\nNo\n24\n\nWill the dataset be updated (for example, to correct labeling errors, add new instances, delete\ninstances)?\nYes, the dataset will be versioned and updated depending on the progress of this\nresearch.\nIf the dataset relates to people, are there applicable limits on the retention of the data associated\nwith the instances (for example, were the individuals in question told that their data would be\nretained for a fixed period of time and then deleted)?\nNot applicable.\nWill older versions of the dataset continue to be supported/hosted/ maintained?\nOlder data\nwill be supported in as much as they do not contain errors and are still valid and useful to the research\ncommunity.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so?\nThis dataset will be available on GitHub to allow others to contribute, comment,\nand build on the project in ways that works best for them.\n25\n\nTable 2: Human Values Taxonomy and Description\nHuman Values\nHuman Values Taxonomy Description\n1. Well-being/Peace:\nThis value hierarchy focuses on the holistic thriving of humans across\nmultiple dimensions, including physical, mental, emotional, and\nspiritual aspects. The end goal is to foster a being that thrives in the\nworld. The sub-values within this category include pleasure, life\nsatisfaction, emotional fulfillment, joy, bliss, euphoria, physical health,\nmental health, nourishment, vitality, vigor, energy, fitness, nutrition,\nself-care, environmental sustainability, security, stability, order, peace,\nand unity. [43–64]\n2. Information Seeking:\nThis value hierarchy focuses on the pursuit of information for\nimmediate, practical application. The emphasis here is on using\ninformation to achieve immediate outcomes. For example, asking for\ndirections on how to get to the airport from their current location, asking\nfor information about a recipe that uses the available ingredients in their\nfridge. This value category was the most common within the RLHF\npreference dataset. The sub-human values within this category include\nefficiency, desire fulfillment, and interest achievement. [65–71]\n3. Justice/Human Rights &\nAnimal Rights:\nThis value refers to respect for the rights of people and animals to exist\nmeaningfully as members of human society and natural ecology. The\nvalues within this group include human rights, animal rights, equality,\nimpartiality, fairness, equity, access, inclusion, autonomy, dignity, and\nequity in access to information. [72–104]\n4. Duty/Accountability:\nThis value centers on the ethical obligations of individuals to society\nand in professional settings. Some of the values within this category\ninclude non-maleficence, law-abiding, privacy, confidentiality, integrity,\naccountability, trustworthiness, reliability, responsibility, and\nreasonableness. It also includes the duty technology practitioners owe to\nusers. [105–119]\n5. Wisdom/Knowledge:\nThis value focuses on acquiring knowledge for deeper understanding\nrather than immediate application. It involves the pursuit of knowledge\nfor its own sake. An example of this involves seeking to understand the\nprocesses that lead to rain formation or learning from past mistakes or\nthrough practice. Some of the values within this category include\ndiscernment, excellence, creativity, skill, prudence, discipline,\ncompetence, diligence, fortitude, resilience, and craftsmanship.\n[120–133]\n6. Civility/Tolerance:\nThis value refers to the strength of character and attitude an individual\nmanifests in their behavior toward members of society and themselves.\nEssentially, this value relates to personal character and attitudes in social\ninteractions. Some of the values within this category include civility,\ncourtesy, etiquette, cooperation, confidence, restraint, modesty, humility,\nsimplicity, calmness, and patience. [134–144]\n7. Empathy/Helpfulness:\nThis value involves showing humanity to oneself and the world. It\ninvolves understanding the context and plight of the human or animal to\nprovide assistance to help them navigate that situation. Some of the\nvalues within this category include benevolence, generosity, compassion,\nempathy, kindness, positivity, and helpfulness. [145–152]\n26",
    "pdf_filename": "Value_Imprint_A_Technique_for_Auditing_the_Human_Values_Embedded_in_RLHF_Datasets.pdf"
}