{
    "title": "CodeXEmbed: A Generalist Embedding Model Family",
    "abstract": "trievalmodels(Wangetal.,2022;Chenetal.,2024) Despite the success of text retrieval in many basedonBERT(KentonandToutanova,2019)and NLPtasks,coderetrievalremainsalargelyun- otherLLMswithsizesexceeding1billionparame- derexploredarea.Mosttextretrievalsystems ters(Wangetal.,2023;Moreiraetal.,2024;Meng aretailoredfornaturallanguagequeries,often etal.,2024).Despitetheseadvancements,standard neglecting the specific challenges of retriev- IRmethods,whileeffectiveintext-basedretrieval, ingcode.Thisgapleavesexistingmodelsun- abletoeffectivelycapturethediversityofpro- oftenfallshortinspecializeddomainssuchascode gramming languages and tasks across differ- retrieval(Husainetal.,2019). ent domains, highlighting the need for more Coderetrievaliscriticalforacceleratingdevel- focusedresearchincoderetrieval.Toaddress opmentprocessesandimprovingcodequality.Un- this, we introduce CODEXEMBED, a family like general text retrieval, code retrieval enables of large-scale code embedding models rang- developerstoquicklylocaterelevantcodesnippets, ing from 400M to 7B parameters. Our novel explanations,buganalyses,summaries,andsimi- trainingpipelineunifiesmultipleprogramming languagesandtransformsvariouscode-related larinstances.Effectivecoderetrievalsystemsare tasksintoacommonretrievalframework,en- nowintegratedintocommercialproductslikeVS hancing model generalizability and retrieval Code(DelSole,2021)andGitHubCopilot(Wer- performance.Our7Bmodelsetsanewstate-of- melinger, 2023; Yetis¸tiren et al., 2023), enhanc- the-art(SOTA)incoderetrieval,outperforming ingproductivity.Code-RAGsystems(Parvezetal., thepreviousleadingmodel,Voyage-Code,by 2021;Liuetal.;Jimenezetal.,2024;Wangetal., over20%onCoIRbenchmark.Inadditionto 2024)alsoleveragecoderetrievaltominimizehal- excellingincoderetrieval,ourmodelsdemon- lucinations in generated code from LLMs, ensur- stratecompetitiveperformanceonthewidely adoptedBeIRtextretrievalbenchmark,offer- ing more accurate outputs. However, traditional ing versatility across domains. Experimental text retrieval models often struggle with code be- results demonstrate that improving retrieval causetheyfocusonlinguisticpatterns,whilecode performancesignificantlyenhancesend-to-end retrieval must handle elements like syntax, vari- Retrieval-Augmented Generation (RAG) per- able dependencies, control flow, and API usage. formanceforcode-relatedtasks. Despite the importance of code-specific models, existingoneslikeCodeBERT(Fengetal.,2020), 1 Introduction CodeGPT(Luetal.),andUniXcoder(Guoetal., Large Language Models (LLMs) have demon- 2022)arebasedonsmallerBERTmodels(Kenton stratedexceptionalperformanceacrossnumerous and Toutanova, 2019). While large-scale models Natural Language Processing (NLP) tasks. How- for retrieval have become popular, only Voyage- ever, they often struggle to produce faithful an- Codehasfollowedthisapproachforcoderetrieval, swersandmaylackup-to-dateordomain-specific but it remains a closed model, leaving a gap for knowledge.Tobridgethisgap,retrieval-augmented open-source,large-scalecoderetrievalmodels. generation (RAG) (Cai et al., 2022; Cheng et al., In this work, we introduce CODEXEMBED, 2024)techniqueshavegainedprominence,integrat- a family of open-source embedding models tai- ingInformationRetrieval(IR)systemswithLLMs lored for bothcode and text, available in sizes of toenhancetheiraccesstorelevantexternalinforma- 400 million, 2 billion, and 7 billion parameters. tion.Thissynergyhasdrawnsignificantattention CODEXEMBEDintroducesageneralizabletraining 1 4202 voN 91 ]ES.sc[ 1v44621.1142:viXra",
    "body": "CodeXEmbed: A Generalist Embedding Model Family\nfor Multiligual and Multi-task Code Retrieval\nYeLiu,RuiMeng,ShafiqJot,SilvioSavarese,CaimingXiong,YingboZhou,SemihYavuz\nSalesforceAIResearch\nyeliu@salesforce.com\nAbstract recently,leadingtothedevelopmentofvariousre-\ntrievalmodels(Wangetal.,2022;Chenetal.,2024)\nDespite the success of text retrieval in many basedonBERT(KentonandToutanova,2019)and\nNLPtasks,coderetrievalremainsalargelyun- otherLLMswithsizesexceeding1billionparame-\nderexploredarea.Mosttextretrievalsystems\nters(Wangetal.,2023;Moreiraetal.,2024;Meng\naretailoredfornaturallanguagequeries,often\netal.,2024).Despitetheseadvancements,standard\nneglecting the specific challenges of retriev-\nIRmethods,whileeffectiveintext-basedretrieval, ingcode.Thisgapleavesexistingmodelsun-\nabletoeffectivelycapturethediversityofpro- oftenfallshortinspecializeddomainssuchascode\ngramming languages and tasks across differ- retrieval(Husainetal.,2019).\nent domains, highlighting the need for more Coderetrievaliscriticalforacceleratingdevel-\nfocusedresearchincoderetrieval.Toaddress\nopmentprocessesandimprovingcodequality.Un-\nthis, we introduce CODEXEMBED, a family\nlike general text retrieval, code retrieval enables\nof large-scale code embedding models rang-\ndeveloperstoquicklylocaterelevantcodesnippets,\ning from 400M to 7B parameters. Our novel\nexplanations,buganalyses,summaries,andsimi-\ntrainingpipelineunifiesmultipleprogramming\nlanguagesandtransformsvariouscode-related larinstances.Effectivecoderetrievalsystemsare\ntasksintoacommonretrievalframework,en- nowintegratedintocommercialproductslikeVS\nhancing model generalizability and retrieval Code(DelSole,2021)andGitHubCopilot(Wer-\nperformance.Our7Bmodelsetsanewstate-of- melinger, 2023; Yetis¸tiren et al., 2023), enhanc-\nthe-art(SOTA)incoderetrieval,outperforming\ningproductivity.Code-RAGsystems(Parvezetal.,\nthepreviousleadingmodel,Voyage-Code,by\n2021;Liuetal.;Jimenezetal.,2024;Wangetal.,\nover20%onCoIRbenchmark.Inadditionto\n2024)alsoleveragecoderetrievaltominimizehal-\nexcellingincoderetrieval,ourmodelsdemon-\nlucinations in generated code from LLMs, ensur-\nstratecompetitiveperformanceonthewidely\nadoptedBeIRtextretrievalbenchmark,offer- ing more accurate outputs. However, traditional\ning versatility across domains. Experimental text retrieval models often struggle with code be-\nresults demonstrate that improving retrieval causetheyfocusonlinguisticpatterns,whilecode\nperformancesignificantlyenhancesend-to-end\nretrieval must handle elements like syntax, vari-\nRetrieval-Augmented Generation (RAG) per-\nable dependencies, control flow, and API usage.\nformanceforcode-relatedtasks.\nDespite the importance of code-specific models,\nexistingoneslikeCodeBERT(Fengetal.,2020),\n1 Introduction\nCodeGPT(Luetal.),andUniXcoder(Guoetal.,\nLarge Language Models (LLMs) have demon- 2022)arebasedonsmallerBERTmodels(Kenton\nstratedexceptionalperformanceacrossnumerous and Toutanova, 2019). While large-scale models\nNatural Language Processing (NLP) tasks. How- for retrieval have become popular, only Voyage-\never, they often struggle to produce faithful an- Codehasfollowedthisapproachforcoderetrieval,\nswersandmaylackup-to-dateordomain-specific but it remains a closed model, leaving a gap for\nknowledge.Tobridgethisgap,retrieval-augmented open-source,large-scalecoderetrievalmodels.\ngeneration (RAG) (Cai et al., 2022; Cheng et al., In this work, we introduce CODEXEMBED,\n2024)techniqueshavegainedprominence,integrat- a family of open-source embedding models tai-\ningInformationRetrieval(IR)systemswithLLMs lored for bothcode and text, available in sizes of\ntoenhancetheiraccesstorelevantexternalinforma- 400 million, 2 billion, and 7 billion parameters.\ntion.Thissynergyhasdrawnsignificantattention CODEXEMBEDintroducesageneralizabletraining\n1\n4202\nvoN\n91\n]ES.sc[\n1v44621.1142:viXra\nframeworkthatconvertsmultipleprogramminglan- Onesuchtaskiscodecontestgeneration(Billah\nguagesanddiversecode-relatedtasksintoretrieval et al., 2024; Kadir et al., 2024), where a natural\ntasks.Ourapproachhandles12programminglan- language description of a programming problem\nguages and five distinct code retrieval categories isprovided,andthegoalistoretrieveorgenerate\nacrosseightdifferentcodetasks,includingcode-to- a code snippet that correctly solves the problem.\ntext,text-to-code,code-to-code,text-to-textandhy- This setting closely aligns with the Text-to-Code\nbridtextandcodetasks.Thiscomprehensivesetup retrievalframeworkasitinvolvesfindingcodeso-\nenables CODEXEMBED to generalize effectively lutionsbasedonaquestionorproblemstatement.\nacrossvariouscodedomains.Ourcontributionscan AnotherexampleistheText-to-SQLtask(Finegan-\nbesummarizedasfollows: Dollaketal.,2018;Lietal.,2024a),wheretheaim\nistogenerateorretrieveaSQLquerybasedona\n• Weproposeageneralizabletrainingapproach user’snaturallanguagequestion.Thetaskfitswell\nfor code embedding that transforms diverse withintheText-to-Codecategory,asitrequiresun-\nprogramming languages and code-related derstandingtheintentofthequeryandretrievingor\ntasksintoretrievaltasks.Thismethodallows generatingtheappropriateSQLcodethatmatches\nforaunifiedframeworkthateffectivelyhan- thegivendescription.\ndles multiple languages and task types, im- Code-to-Text Retrieval. The Code-to-Text re-\nprovingretrievalperformanceacrossvarious trievalsettingisdesignedtoretrieverelevanttex-\ncodetasks. tual documents or descriptions based on a given\ncode query. This setup is particularly valuable in\n• Our7Bmodel,theopen-sourcecodeembed-\nscenarioswheredevelopersorsystemsseektoun-\ndingmodeltodate,achievesstate-of-the-art\nderstandorexplaincodebymappingittohuman-\nperformance on code retrieval CoIR bench-\nreadabledocumentationorsummaries.\nmark, setting a new standard for code re-\nOnesuchtaskiscodesummarization(Sontakke\ntrieval.Wealsoevaluate CODEXEMBED on\netal.,2022;Sunetal.,2024),whereacodefile,or\nRepoEvalandSWE-Bench-Lite,demonstrat-\nevenanentirerepository,isprovidedasinput,and\ning that improved retrieval significantly en-\nthegoalistogenerateaconcisetextualsummary\nhancesend-to-endRetrieval-AugmentedGen-\nof the code’s functionality. This task aligns with\neration(RAG)performanceforcodetasks.\nthe Code-to-Text retrieval setting, as it involves\ntakingpotentiallycomplexcodeandproducinga\n• Inadditiontothe7Bmodel,wetrainsmaller\nhuman-readabledescriptionorexplanation,which\nmodels (400M and 2B) that outperform the\nis critical for documentation, knowledge sharing,\npreviousstate-of-the-artincoderetrieval.Fur-\nandonboardinginsoftwareprojects.\nthermore, our models demonstrate compet-\nCode-to-CodeRetrieval. TheCode-to-Codere-\nitive performance on text retrieval, making\ntrievalsettingfocusesonretrievingrelevantcode\nthem versatile across both code and text re-\ndocuments based on a code query. This retrieval\ntrievaltasks.\nsettingisparticularlyusefulfortasksthatinvolve\nunderstanding, transforming, or complementing\n2 Method\ncode snippets. Several tasks naturally align with\nWetransformgeneralcode-relatedtasksintoauni- thisframework,makingitanessentialpartofthe\nfiedretrievalframeworkbycategorizingtheminto broadercoderetrievalecosystem.\ndistinct retrieval settings. As shown in Figure 1, One such task is code translation (Pan et al.,\neachsettingservesaspecificpurposeinmapping 2024; Karanjai et al., 2024), where the goal is to\nbetweencodeandtext,enhancingthemodel’sabil- translatecodesnippetsfromoneprogramminglan-\nitytogeneralizeacrossvariousretrievaltasks. guagetoanother.Forinstance,adevelopermight\nText-to-Code Retrieval. The Text-to-Code re- provideacodesnippetinPythonandseektotrans-\ntrievalsettingfocusesonretrievingrelevantcode lateitintoC++orJava.Thistaskfitswellintothe\ndocuments based on a given textual query. This Code-to-Coderetrievalsetting,asitinvolvesfind-\nsetupisparticularlyusefulinscenarioswherenat- ingtheequivalentfunctionalityinadifferentpro-\nurallanguagedescriptionsaremappedtocodeso- gramminglanguage,ensuringthetranslatedcode\nlutions.Severalcodegenerationtaskscanbetrans- maintainsthesamelogicandstructure.\nformedtofitintothisretrievalparadigm. Anotherimportanttaskiscodecompletion(Ding\n2\nCode Agent Conversation\nCode Issue Fix Code Summary\nPlot a chart of CRM A summary of the code\nstock price change YTD. could be as following:\nimport matplotlib.pyplot as plt\nimport pandas as pd A method that returns\ndata = { \"Date\": the average color of\npd.date_range(start=\"2024-01-01\", pixels in a neighbor-\nend=\"2024-09-30\", freq=\"M\"), hood around a given\n….\npixel in an image.\nCould you explain line\n12-20?\nSure, line 12-20 use the pandas Code Clone Detection\npackage to ….\n… Code block 1 … //define\nText2SQL Query / fo/s ro (Ir =t 1a ,r r I<a 1y 0A , I++) C l o f o n fre o (2 ra 7 (= c 1( =a , a , ab ,> c,c b >, 1d , a ,) c+ -+ -) )\nfor (J=i, j> 1, J--) if (d[c] > d(c-1))\nQuestion if (A[j] > A(j-1)) Swap(d[a], d[c])\nSwmp(A[I], A[J])\n//sort array X\n… Code block 2 … for (x=1, x<10, x++)\nCode Contest Code Translation Code Completion\nRemove Digit From Number def remove_digit(number: str, digit:\nstr) -> str:\nProblem: You are given a string number return number.replace(digit, '', 1)\nrepresenting a positive integer and a\ncharacter digit. # Example usage\nnumber = \"123\"\nExample 1: digit = \"3\"\nInput: number=“123”, digit=“3” result = remove_digit(number, digit)\nOutput: “12” print(\"Output:\", result)\nFigure1:ThecodetrainingdataofCODEXEMBEDcontainsfourparts:Text-to-Code,Code-to-Code,Code-to-Text\nandHybridCode.EachCategoriescontainsseveraltypesofcodetasks.\net al., 2024; Phan et al., 2024; Liu et al., 2024), bothcodeandtextualcontexttogeneratemeaning-\nwhereapartiallywrittencodesnippetisprovided, fulresponsesorretrieverelevantdocuments.\nand the system needs to retrieve or generate the\nOne such task is code agent conversa-\nmissingportionstocompletethefunctionormod-\ntion(ArteagaGarciaetal.,2024;Jinetal.,2024),\nule. This is commonly seen in integrated devel-\nwhere a code agent must generate either a code\nopmentenvironments(IDEs)whereautocomplete\nsnippetoratextualresponsebasedonauser’sques-\nfeatures assist developers by suggesting the next\ntion. This task requires the agent to interpret the\nlogicallinesofcodebasedonthecurrentcontext.\nuser’squestion,whichmightcontainbothcodeand\nCode completion aligns perfectly with the Code-\nnatural language, and retrieve or generate the ap-\nto-Code retrieval setting, as it requires matching\npropriatecodesolutionorexplanation.Thehybrid\nthe partial input code with relevant snippets that\nquery allows the agent to navigate between tech-\nseamlesslycomplementtheexistingcode.\nnical (code) and descriptive (textual) domains to\nAdditionally, code clone detection (Martinez- provideacomprehensiveresponse.\nGil, 2024), which involves identifying code snip- Anotherkeytaskiscodeissuefixing(Yangetal.,\npets that arefunctionally or structurally identical 2024;Jimenezetal.,2024),whereahybridquery\nbut may differ in syntax or formatting, also fits containingadescriptionofthecodeissueandthe\nwithinthisretrievalframework.Clonedetectionis problematiccodesnippetisprovided.Thegoalis\ncrucialforidentifyingredundantcode,refactoring togeneratecodethatresolvestheissue.Byincor-\ncodebases,andensuringconsistencyacrossdiffer- porating both the textual description (e.g., error\nent sections of a project. This task leverages the message) and the code itself, hybrid retrieval al-\nCode-to-Coderetrievalsettingbyretrievingcode lowsthemodeltoidentifyrelevantcodesnippets,\nsnippetsthatexhibitsimilaroridenticalbehavior, detectsimilarissues,andproposefixesthatalign\neveniftheyappeardifferentatthesurfacelevel. withtheoverallproject.Thismakeshybridretrieval\nessential in complex development environments,\nHybridCodeRetrieval.Thehybridcoderetrieval\nwherebothdebuggingandproblem-solvingrequire\nsetting involves retrieving a combination of both\nanuancedunderstandingofbothcodeandtext.\ncode and textual documents in response to a hy-\nbridquery,whichmaycontainbothcodeandnat- Text-to-Text Retrieval. In order to capture the\nural language. This setting is particularly useful model’sgeneralretrievalabilityacrossbothcode\nfortasksthatrequireunderstandingandintegrating andnaturallanguagedomains,weincorporatetext-\n3\nto-textretrievaldataintothetrainingpipeline.This such as question answering, duplicate detection,\nallowsthemodeltoperformwellintraditionaltext fact-checking,andmore.Itsupportsretrievalover\nretrieval tasks, enhancing its versatility. The ex- awiderangeofdatasetsandprovidesastandard-\nperimentsstudytheratiooftextandcodedatato izedbenchmarkforevaluatingtextretrievalmodels\ndetermine the optimal balance between the two, acrossdifferentdomains.\naimingtoensurethemodel’seffectivenessinboth ImplementationDetails.Weconductgeneraltrain-\ntextandcoderetrievaltaskswithoutsacrificingper- ingonourproposedcodeandtextpairdatasetus-\nformanceineitherdomain. ingthreemodelsizes:400M,2B,and7B.Forthe\nRetrieval Training. To train a unified code and CodeXEmbed ,weusethebasemodelAlibaba-\n400M\ntextretrievalmodel,wetransformalltheaforemen- NLP/gte-large-en-v1.5(Lietal.,2023b),applying\ntionedtasksintoaquery-and-answerformat,where full model fine-tuning. For the CodeXEmbed ,\n2B\neitherthequeryortheanswercanbeatextorcode we initialize our embedding model from the gen-\ndocument.Thisunifiedformatallowsustohandle erationmodelgoogle/gemma-2-2b-it(Teametal.,\na variety of tasks such as Text-to-Code, Code-to- 2024),usinglow-rankadaption(LoRA)(Huetal.,\nText,andCode-to-Coderetrievalwithinthesame 2022) with a rank of 8. For the CodeXEmbed ,\n7B\ntrainingframework.Thetraininglossisdesigned we initialize our embedding model from the gen-\nasacontrastiveloss(Liuetal.,2021),whichaims erationmodelmistralai/Mistral-7B-Instruct-v0.3,\ntomaximizethesimilaritybetweenthequeryand also using LoRA with a rank of 8. Follow-\nthe correct answer while minimizing the similar- ing prior work (Meng et al., 2024), we apply:\nity to the negative samples. The loss function is (i) last token pooling for CodeXEmbed and\n2B\nrepresentedas: CodeXEmbed ,and(ii)beginningtokenpooling\nL=−\n1 (cid:88)N (cid:34)\nlog\nexp(q i·d+\ni\n) (cid:35) forCodeXEmb7B ed\n400M\ntogeneratesemanticvector\nN i=1 exp(q i·d+ i )+(cid:80)K j=1exp(q i·d− j ) representations(Lietal.,2023b).Cosinesimilarity\n(1)\nis used to compute the similarity between query\nwhereq representsthequery,whichcanbeeither\ni and corpus for ranking. The batch size is set to\ntextorcode.Thetermd+ referstothecorrespond-\ni 1024acrossallthreemodelsizes,with7hardneg-\ningpositivedocument,whichcouldalsobeinthe\natives. The learning rate is set to 5e−5, and the\nform of text or code. On the other hand, d− de-\nj end learning rate to 5e−6, with linear decay and\nnotesasetofhardnegativedocuments,whichare\na50-stepwarmup.Toimprovetrainingefficiency\nselectedtobesimilarincontexttothequerybutin-\nandreduceGPUmemoryusage,weadoptgradient\ncorrect,makingtheretrievaltaskmorechallenging.\ncaching(Gaoetal.,2021).Themoreimplementa-\nThesimilaritybetweenthequeryandadocumentis\ntiondetailscanbefoundinAppendixA.2.\nmeasuredbysim(q,d).ThevariableN represents\nBaselineModels.Forcode-domain-specificmod-\nthetotalnumberoftrainingsamples,indicatingthe\nels,weincludedUniXcoder(Guoetal.,2022)and\nnumber of queries used during training, while K\nVoyage-Code-0021,bothofwhicharepre-trained\ndefines the number of hard negatives associated\non code data, making them strong baselines for\nwitheachquery.\ncomparison.Forgeneralretrievalmodels,weeval-\nuatedE5-Base(Wangetal.,2022),GTE-Base(Li\n3 Experiments\netal.,2023b),BGE-Base(Xiaoetal.,2023),Con-\nEvaluation Benchmarks. We mainly use two triever (Izacard et al., 2023), E5-Mistral (Wang\nbenchmarks to evaluate code and text retrieval et al., 2023), BGE-M3 (Chen et al., 2024), NV-\nperformance. COIR (Li et al., 2024b) is a com- Embed-V2(Moreiraetal.,2024),SFR-V2(Meng*\nprehensive benchmark specifically designed for etal.,2024)andOpenAI-Ada-0022.\ncode retrieval tasks. COIR covers a wide range Evaluation Metrics. In code retrieval, selecting\nofretrievalchallenges,including8fine-grainedre- therightevaluationmetriciskeyforassessingboth\ntrievalsubtasks,spanning14majorprogramming rankingsensitivityandrelevance.Buildingonprior\nlanguages.Thedatasetiscomposedof10distinct work(Wangetal.,2013),NormalizedDiscounted\ndatasets, with a total corpus exceeding 2 million CumulativeGain(NDCG)ispreferredforitsabil-\nentries.BEIR(Thakuretal.)isawidely-adopted\nbenchmarkdesignedfortextretrievaltasks.BEIR\n1https://blog.voyageai.com/2024/01/23/\nvoyage-code-2-elevate-your-code-retrieval/\nencompassesadiversesetofretrievalchallenges,\n2https://platform.openai.com/docs/\ncovering 9 distinct tasks across various domains guides/embeddings\n4\nText-to-Code Code-to-Text Code-to-Code HybridCode\nModel CodeTrans StackOverFlow CodeFeedBack Avg\nApps CosQA Text2SQL CSN CSN-CCR\n-Contest -DL QA -ST -MT\nBaselines\nContriever(110M) 5.14 14.21 45.46 34.72 35.74 44.16 24.21 66.05 55.11 39.23 36.40\nE5-base(110M) 11.52 32.59 52.31 67.99 56.87 62.50 21.87 86.86 74.52 41.99 50.90\nBGE-Base(110M) 4.05 32.76 45.59 69.60 45.56 38.50 21.71 73.55 64.99 31.42 42.77\nGTE-Base(110M) 3.24 30.24 46.19 43.35 35.50 33.81 28.80 55.19 55.19 28.48 36.75\nUniXcoder(123M) 1.36 25.14 50.45 60.20 58.36 41.82 31.03 44.67 36.02 24.21 37.33\nBGE-M3(567M) 7.37 22.73 48.76 43.23 47.55 47.86 31.16 51.04 49.94 33.46 39.31\nE5-Mistral(7B) 21.33 31.27 65.98 54.25 65.27 82.55 33.24 91.54 72.71 33.65 55.18\nOpenAI-Ada-002 8.70 28.88 58.32 74.21 69.13 53.34 26.04 72.40 47.12 17.74 45.59\nVoyage-Code-002 26.52 29.79 69.26 81.79 73.45 72.77 27.28 67.68 65.35 28.74 56.26\nGeneralTraining\nCodeXEmbed400M 48.57 34.05 58.96 72.53 80.15 75.67 34.85 89.51 78.87 45.75 61.89\nCodeXEmbed2B 74.99 36.31 59.00 73.50 85.77 86.63 33.17 90.54 81.15 53.08 67.41\nCodeXEmbed7B 85.22 33.27 64.57 78.84 86.77 90.64 32.31 94.25 80.93 57.83 70.46\nIn-domainTraining\nCodeXEmbed400M 45.91 41.28 61.29 81.23 93.74 82.72 40.81 92.35 83.36 61.51 68.42\nCodeXEmbed2B 76.86 40.47 78.42 87.87 97.66 90.30 38.57 94.47 86.36 65.51 75.65\nCodeXEmbed7B 85.38 42.47 78.94 89.67 97.95 94.45 40.46 96.33 87.53 68.83 78.20\nTable1:PerformanceofthePretrainedCODEXEMBEDModelFamilyacrossvarioustasks,includingText-to-Code,\nCode-to-Text,Code-to-Code,andHybridCode,alongwiththeiraveragescores.CSNstandsforCodeSearchNet.\nitytoaccountforbothrankorderandvaryingrele- 90 4 20 B0 GM T GT\nvance,unlikeprecision,recallorMRR.Therefore, 7 4B 00 G MT ID-GT\n80 2B ID-GT\nwe use NDCG@10 to evaluate performance on 7B ID-GT\n70 CoIRandBEIR.\n60\n3.1 GeneralTrainingEvaluation\n50\nIn the General Training block of Table 1, we\n40\npresent the results of models trained exclusively\n30\non our proposed general training data, without Text-to-Code Code-to-Text Code-to-Code Hybrid AVG\nusing any CoIR in-domain data. When averaged Figure2:TheperformancecomparisonbetweenGeneral\nover all 10 datasets in the CoIR benchmark, Training(GT)andIn-domainTraining(ID)acrossthree\nCodeXEmbed model achieves the best results, model sizes (400M, 2B, and 7B) on different CoIR\n7B\ncategoriesandtheoverallaverage.\nexceedingtheSOTAcode-domainspecificmodel\nVoyage-Code-002byover20%,whichshowsour\ngeneral code and text training stage significantly CodeXEmbed 7B by7.74pointsonaverageacross\nimprovesmodelperformanceoncodetasks. all 10 datasets. In Figure 2, the top of each bar\nAs shown in Table 1, CodeXEmbed and representstheimprovementfromin-domaintrain-\n400M\nCodeXEmbed alsoprovideasignificantimprove- ing.Amongallcategories,theCode-to-Textcate-\n2B\nment over Voyage-Code-002 and offer a great al- goryshowsthelargestimprovement,evenoutper-\nternative to the 7B model with substantial practi- formingVoyage-Code-002.Inothercategories,the\ncaladvantagesonthelatencyandcost.Moreover, modelalsoachievesovera5-pointimprovement.\ntheir success further validates the transferability\n3.3 UnifiedTextandCodeRetrieval\nandgeneralizabilityofourproposedtrainingrecipe\nforcodeembeddingmodels. Toevaluatethetextandcoderetrievalcapabilities\nwithinasingleembeddingmodel,wealsopresent\n3.2 In-domainTrainingEvaluation the BEIR text retrieval performance of CODEX-\nWe further trained the model on the CoIR in- EMBEDacrossvarioussizes.AsshowninTable2,\ndomain dataset. As shown in the In-domain our7Bmodelachievesanaveragescoreofover60\nTraining block of Table 1, further training across15datasets,placingitamongthetoptieron\non in-domain data results in consistent perfor-\ntheMTEBleaderboard3.ComparedtoE5-Mistral-\nmance improvements across all model sizes. 7B-Instruct(E5-Mistral)(Jiangetal.,2023),which\nSpecifically, CodeXEmbed improves by 6.5\n400M 3https://huggingface.co/spaces/mteb/\npoints, CodeXEmbed by 8.24 points, and leaderboard\n2B\n5\n01@GCDN\nBM25 gte-large gte-Qwen2 E5-Mistral CODEXEMBED CODEXEMBED CODEXEMBED\nModel\n400M 1.5B 7B 400M 2B 7B\nMSMARCO 22.8 42.93 43.36 43.06 42.77 41.26 42.05\nTREC-Covid 65.6 77.49 85.38 87.03 77.47 84.58 79.04\nNFCorpus 32.5 36.95 39.34 38.58 35.76 41.56 43.14\nNQ 32.9 56.08 56.08 63.53 63.38 67.25 74.11\nHotpotQA 60.3 68.18 64.00 75.72 74.93 74.39 79.33\nFiQA 23.6 63.23 63.23 56.81 60.20 56.17 60.41\nArguAna 31.5 72.11 54.70 61.65 69.67 61.39 63.58\nTouche-2020 36.7 22.55 27.89 26.27 20.18 26.10 25.80\nCQADupStack 29.9 42.16 44.76 42.97 46.07 47.46 51.45\nQuora 78.9 89.67 89.64 89.61 89.05 89.27 89.51\nDBPedia 31.3 46.30 48.69 48.89 46.68 47.33 49.27\nScidocs 15.8 26.35 24.98 16.32 25.05 23.36 25.25\nFever 75.3 93.81 91.57 87.84 93.86 89.03 91.94\nClimate-Fever 21.3 48.36 42.91 38.35 42.70 32.08 36.93\nScifact 66.5 82.43 78.44 76.42 87.37 84.79 89.10\nAverage 41.7 57.91 58.29 56.87 58.34 57.73 60.06\nBeston 1 5 1 1 1 0 6\nTable2:ComparisonofperformanceacrosstextretrievalBEIRdatasetswithdifferentmodelsize.\nis trained on both text and synthetic data, initial- 3.4 Retrieval-AugmentedCodeGeneration\nized from the Mistral series, our model employs\nIn this section, we explore how different retriev-\nsingle-stagetrainingwithbothcodeandtextdata.\ners influence the final code completion and issue\nOurmodelinaperformanceimprovementof3.19\nresolutionperformanceinrepository-leveltasks.\npointsoverE5-Mistral.\nInthe400Mmodels,CODEXEMBEDachievesa 3.4.1 RepoEval\n0.43performanceboostcomparedtoGTE-large(Li Toaddressthis,weutilizeRepoEval(Zhangetal.,\net al., 2023b), on which it is trained. This under- 2023)forrepository-levelcodecompletion.While\nscorestheadvantageofourapproach,demonstrat- RepoEvalconsistsofthreesplits(function,class,\ning the potential to improve text retrieval perfor- andmodule),wereportresultsonlyonthefunction\nmance by incorporating code data. There are not split,asitistheonlyonethatsupportsexecution-\nmany2B-sizedlanguagemodelsavailable,andwe basedevaluation.WeadoptPass@1asourevalua-\nselected Gemma 2B (Team et al., 2024) due to tionmetric,whichmeasurestheaccuracyofthetop-\nits strong performance in code retrieval. For text 1generatedcodepassingtheprovidedtestcases.\nretrieval, it achieves comparable performance to For code generation, we supply the top-5 re-\ngte-Qwen2ofsimilarsize. trievedcodesnippetsasinputtotheGPT-3.5-turbo\ngeneration model. As shown in Table 4, all sizes\nNV-Embed-V2 SFR-v2 CODEXEMBED of CODEXEMBEDoutperformthecanonicalsetup.\nModel\n7B 7B 7B Whilesomefilesmaynotcontaindirectsolutions,\nCoIR 59.10 61.48 70.46 asinthecanonicaldocuments,theyofteninclude\nBEIR 62.65 60.18 60.06\nvaluable function definitions or usage examples\nAVG 60.88 60.83 65.26 thatimprovecodegenerationoutcomes.Thissug-\nTable3:Comparisonofcodeandtextretrievalbench- geststhatourembeddingseffectivelycapturethe\nmarks between our model and the top models on the\nrepositorystructureandretrievecontextsthatim-\nMTEBleaderboard.\nplicitlysupportproblem-solving.\nWe present the performance of leading text re- 3.4.2 SWE-Bench-Lite\ntrieval models from the MTEB leaderboard4 on In our experiments, we use SWE-bench-Lite6, a\nboth code and text tasks, in Table 3. Compared curatedsubsetof300problemsfromtheoriginal\ntothetop-rankedmodel,NV-Embed-V2(Moreira SWE-bench benchmark. It focuses on resolving\net al., 2024)5, our model outperforms it by 4.38 GitHubissuesbyrequiringmodelstomodifymul-\npoints,withanaveragescoreacrosstextandcode tiplefilestopasstestcases,providingamanageable\nretrievaldatasets. andreproducibledataset.SWE-bench-Litealsoin-\n4https://huggingface.co/spaces/mteb/leaderboard 6https://huggingface.co/datasets/\n5Top-performingmodelasof10/15/2024. princeton-nlp/SWE-bench_Lite\n6\nRepo-Level gte-Qwen2 Gemma-v2 SFR-v2 Mistral\nModel Model\nInitial GT GT Initial GT GT\nRepoEval SWE-Bench-Lite\nSize 1.5B 2B 7B 7B\nNone 23.9 0.7\nCoIR 62.96 68.52 67.41 61.28 69.72 70.40\nBM25 30.8 1.0 BEIR 58.29 59.12 57.73 60.18 60.62 60.06\nVoyage,code 43.2 0.7\nOpenAI,small 48.0 0.3 Table5:Comparisonofbasemodels:Retrievalvs.Gen-\nOpenAI,rerank 49.6 0.0 erationModels.GTrepresentsourgeneraltraining.\nCodeXEmbed 52.5 0.6\n400M\nCodeXEmbed 66.3 1.6\n2B Code-SpecificLLMs General-DomainLLMs\nCodeXEmbed 57.8 2.6\n7B StarCoder-v2 DeepSeek-Coder Gemma-v2 Mistral\nGold 39.1 2.7 Model 3B 6.7B 2B 7B\nCoIR 66.95 71.66 67.41 70.40\nTable4:Performanceofrepository-levelcoderetrieval BEIR 49.06 50.22 57.73 60.06\naugmentedgenerationusinggpt-3.5-turbo.\nTable6:Comparisonofbasemodels:Code-specificvs.\nGeneral-domainGenerationModels.\ncludesapre-configuredDockercontainer,ensuring\nconsistent evaluation across systems and further\n3.5.2 Code-SpecificLLMsv.s.GeneralLLMs\nstandardizingthetestingenvironment.Ourresults\nshow that incorporating improved retrieval meth- Weevaluatewhethertochoosecode-specificmod-\nods can significantly enhance the end-to-end per- els(Lozhkovetal.,2024;Guoetal.,2024)orgen-\nformanceofcoderetrieval-augmentedgeneration, eralLLMs(Jiangetal.,2023;Teametal.,2024).\nbringingitclosertousinggoldcontentandboost- AsshowninTable6,code-specificLLMsexcelin\ningproblem-solvingefficiencyandaccuracy. code tasks but underperform in text tasks, while\ngeneralLLMsperformwellinboth.Thissuggests\n3.5 ImpactoftheBaseModels that recent advancements in general LLMs have\nintegratedcodedataintotheirtraining(Teametal.,\nTounderstandthebasemodel’simpact,weexam-\n2024),andthiscapabilitycanbeeffectivelytrans-\nine:(1)iftrainingfromatextretrievalmodelout-\nferredtocoderetrieval.Thisfindinghighlightsthe\nperforms a generation model, and (2) if a code-\nversatility of general LLMs, making them viable\nspecificgenerationmodeloffersmoreadvantages\nfor both text and code retrieval without the need\nthanagenerallanguagemodel.\nforspecializedmodels.\n3.5.1 EmbeddingModelsv.s.LLMs\n3.6 ProgrammingLanguageTransferability\nAsshowninTable5,thetextretrievalmodeloffers\na stronger starting point, and additional training We aim to explore the diversity of programming\nwithourapproachenhancesbothitstextandcode languagesandtheiruniquefeatures.Thedetailsof\nretrievalcapabilities.Forinstance,gte-Qwen27’s ourcodetrainingdataset,includinglanguagecov-\nCoIRperformanceimprovesfrom62.96to68.52, erage,areprovidedinAppendixA.1.Ourdataset\nwhile its text performance increases from 58.29 comprises12programminglanguages,withPython\nto59.12.Incontrast,thetextgenerationmodelre- representingthehighestpercentageofthedata.For\nquiresmoreextensivefine-tuningtoreachsimilar testing, we selected Python and Java due to their\nperformance. However, the advantage of text re- distinctprogrammingparadigms:Pythonisknown\ntrievalmodelscansometimeshindercoderetrieval foritsscriptingcapabilitiesandJavaforitsstrong\nperformance,asseenwithSFR-V2(Meng*etal., object-orienteddesign.Thisselectionallowsusto\n2024)underperformingcomparedtoMistralinspe- evaluateourmodel’sperformanceacrossarangeof\ncifictasks.Thissuggeststhatpriorknowledgefrom programmingstyles,reflectingtheversatilityand\ntext-focusedmodelsmaynotalwaystransferwell adaptabilityoftheembeddingmodel.\ntocode-specificscenarios.Topursueamoregen- As shown in Table 7, training with all 12 pro-\neral training approach, we chose to train using a gramming languages yields the best average per-\ngenerationmodelratherthanatextretrievalmodel. formanceacross7targetlanguages,comparedto\ntraining with a single language. However, train-\n7https://www.aimodels.\ningonJava-onlyconsistentlyachievesthehighest\nfyi/models/huggingFace/\ngte-qwen2-15b-instruct-alibaba-nlp performanceforJavaanddeliverscomparablere-\n7\nModelSize Python Java Go PHP Javascript Ruby SQL AVG ∆-All\n400M 59.70 72.09 79.28 70.96 70.18 73.01 58.87 69.16 -\nAll 2B 65.73 79.40 81.01 79.39 76.76 78.42 58.23 74.14 -\n7B 67.56 81.36 85.76 81.89 78.37 83.11 65.74 77.68 -\n400M 55.43 49.57 49.56 40.57 44.57 44.57 44.21 46.93 −32.1%\nPython 2B 60.86 71.46 68.69 57.88 62.95 72.21 56.09 64.31 −13.3%\n7B 67.02 78.35 81.56 70.88 75.73 79.19 62.56 73.61 −5.2%\n400M 54.79 78.37 72.94 70.85 69.85 68.78 50.31 66.56 −3.8%\nJava 2B 63.03 82.17 79.35 80.30 76.00 76.76 58.03 73.66 −0.6%\n7B 66.24 84.25 83.72 80.18 79.82 82.82 64.59 77.37 −0.4%\nTable7:Python/JavaindicatesthatonlythePythonandJavaportionswereusedtotrainCODEXEMBED,while\nAllindicatesthatallprogramminglanguageswereusedtotrainCODEXEMBED.∆-Allrepresentsthedifference\nbetweenthePython/JavaAVGscoreandtheAllAVGscoreforthesamemodelsize.\nsultstousingalllanguagesacrossallmodelsizes. Guo et al., 2024; Team, 2024) to solve various\nForexample,theJava-only7Bmodelscores77.37, tasks(Chenetal.,2021;Laietal.,2023;Jimenez\nwhiletheall-languagesmodelscores77.68.When et al., 2024). However, most LMs generate code\ncomparingmodelstrainedexclusivelyonPython solelyfromnaturallanguageproblemdescriptions\norJava,theJava-trainedmodelconsistentlyoutper- and the models’ parametric knowledge, without\nforms.Thismaybebecausemodernlanguagemod- leveragingexternalprogrammingresourcesorus-\nels are already heavily trained on Python during ing a retrieval-augmented generation approach.\nthegenerationphase,sorelyingsolelyonPython Whilepriorworkhasfocusedontext-centrictasks\nintheretrievalphasemaymissimportantnuances, withgeneral-domaincorporalikeWikipedia(Asai\nresultinginsuboptimalperformance. etal.,2024),somestudieshaveusedretrievedpro-\ngramming context from repositories (Ding et al.,\n4 RelatedWork\n2024;Yangetal.,2024)ordocumentation(Zhou\netal.,2023).Coderetrievaliscrucialforenhancing\n4.1 RetrievalModelsforTextandCode\ncodegenerationinRAGsystemsbecauseitallows\nTextretrievalmodelshavesignificantlyadvanced\nmodelstoaccessrelevantexternalcoderesources,\nby exploiting supervision from natural language\nleadingtomoreaccurateandcontext-awarecode\ninference tasks and labeled query-document\noutputs.Ourcoderetrievalmodeldemonstratessig-\npairs, such as the MS-MARCO passage ranking\nnificantimprovementsincodeRAGperformance,\ndataset (Bajaj et al., 2016), to train effective text\nunderscoringtheimportanceofeffectivecodere-\nembeddings (Izacard et al., 2021; Wang et al.,\ntrievalincodegenerationtasks.\n2022;Xiaoetal.,2024).Recently,researchershave\nleveraged large language models (LLMs) (Jiang\n5 Conclusion\netal.,2023)asthebasefortrainingretrievalmod-\nels,resultinginstate-of-the-artperformance.No- In the underexplored field of code retrieval, we\ntable examples include E5-Mistral (Wang et al., introduce CODEXEMBED, a family of code em-\n2023), SFR-Embedding (Meng et al., 2024), and beddingmodelsrangingfrom400Mto7Bparame-\nNV-Embedding(Leeetal.,2024).Whilenumerous ters.Ourtrainingpipelineunifiesmultipleprogram-\nmodelshavebeendevelopedfortextretrievaltasks, minglanguagesandconvertsdiversecode-related\nfew have focused specifically on code retrieval. tasksintoacommonretrievalframework.Ourmod-\nAmong the few are the Voyage code model (AI, els surpass the previous SOTA code retrieval by\n2024) and OpenAI’s embeddings (Neelakantan over20%ontheCoIRbenchmarkandachieveper-\netal.,2022);however,bothareclosed-sourcemod- formance comparable to SOTA text retrievals on\nels,limitingtheiraccessibilityandadaptabilityfor theBEIRbenchmark.Byenhancingretrievalcapa-\nthewiderresearchcommunity. bilities,wedemonstratesignificantimprovements\nin end-to-end retrieval-augmented generation for\n4.2 CodeRetrievalAugmentedGeneration\ncode-related tasks. By bridging the gap between\nNeural code generation has been an important textandcoderetrievaldomainsandreleasingour\ntask (Lu et al.), and increasingly strong code modelstothecommunity,weaimtopromotefur-\nlanguage models have been developed (Roziere therresearchandinnovationindevelopertoolsand\netal.,2023;Nijkampetal.,2023;Lietal.,2023a; programminglanguageunderstanding.\n8\n6 Limitations Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recentadvancesinretrieval-augmentedtext\nWhile CODEXEMBED achievesstate-of-the-artre- generation. InProceedingsofthe45thinternational\nsults in code and text retrieval, there are certain ACMSIGIRconferenceonresearchanddevelopment\nininformationretrieval,pages3417–3419.\nlimitationstoconsider.First,themodel’slargesize,\nparticularlythe7Bparametervariant,leadstosig- Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nnificantstorageandcomputationalcosts.Withem- Luo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nbeddings of 4096 dimensions, the model is more\nmulti-granularity text embeddings through self-\nresource-intensivecomparedtosmallerormoreef-\nknowledgedistillation. InFindingsoftheAssocia-\nficientrepresentations.Althoughembeddingsize tionforComputationalLinguisticsACL2024,pages\nreductiontechniques,suchasMatryoshkarepresen- 2318–2335.\ntationlearning(Kusupatietal.,2022),haveshown\nMarkChen,JerryTworek,HeewooJun,QimingYuan,\npromiseinmaintainingperformancewithlowerdi- Henrique Ponde De Oliveira Pinto, Jared Kaplan,\nmensions,furtherresearchisneededtoapplythese HarriEdwards,YuriBurda,NicholasJoseph,Greg\nBrockman, et al. 2021. Evaluating large lan-\napproaches effectively to code retrieval without\nguage models trained on code. arXiv preprint\ncompromising performance. Additionally, while\narXiv:2107.03374.\nCODEXEMBED generalizeswellacrossdifferent\nXin Cheng, Di Luo, Xiuying Chen, Lemao Liu,\nprogramminglanguagesandtextretrievaltasks,it\nDongyan Zhao, and Rui Yan. 2024. Lift yourself\nmaystillstrugglewithnicheprogrammingdomains\nup: Retrieval-augmented text generation with self-\norhighlyspecializedlanguagesthatwereunderrep- memory. AdvancesinNeuralInformationProcess-\nresented in the training data. Future work could ingSystems,36.\nexplore adapting the model for these underrepre- AlessandroDelSole.2021. Introducingvisualstudio\nsentedlanguagesandimprovingefficiencywithout code. InVisualStudioCodeDistilled:EvolvedCode\nsacrificingaccuracy. EditingforWindows,macOS,andLinux,pages1–15.\nSpringer.\nYangruiboDing,ZijianWang,WasiAhmad,Hantian\nReferences Ding, Ming Tan, Nihal Jain, Murali Krishna Ra-\nmanathan,RameshNallapati,ParminderBhatia,Dan\nVoyage AI. 2024. Voyage code 2: El- Roth,etal.2024. Crosscodeeval:Adiverseandmul-\nevate your code retrieval. https: tilingualbenchmarkforcross-filecodecompletion.\n//blog.voyageai.com/2024/01/23/ AdvancesinNeuralInformationProcessingSystems,\nvoyage-code-2-elevate-your-code-retrieval/. 36.\n[Online;accessed8-October-2024].\nZhangyinFeng,DayaGuo,DuyuTang,NanDuan,Xi-\nEmilyJudithArteagaGarcia,JoãoFelipeNicolaciPi- aochengFeng,MingGong,LinjunShou,BingQin,\nmentel, Zixuan Feng, Marco Gerosa, Igor Stein- Ting Liu, Daxin Jiang, et al. 2020. Codebert: A\nmacher, and Anita Sarma. 2024. How to support pre-trainedmodelforprogrammingandnaturallan-\nmlend-userprogrammersthroughaconversational guages. InFindingsoftheAssociationforComputa-\nagent. InProceedingsofthe46thIEEE/ACMInter- tionalLinguistics:EMNLP2020,pages1536–1547.\nnationalConferenceonSoftwareEngineering,pages\nCatherine Finegan-Dollak, Jonathan K Kummerfeld,\n1–12.\nLiZhang,KarthikRamanathan,SeshSadasivam,Rui\nZhang,andDragomirRadev.2018. Improvingtext-\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei\nto-sqlevaluationmethodology. InProceedingsofthe\nKoh, Luke Zettlemoyer, Hannaneh Hajishirzi, and\n56thAnnualMeetingoftheAssociationforCompu-\nWen-tau Yih. 2024. Reliable, adaptable, and at-\ntationalLinguistics(Volume1:LongPapers),pages\ntributable language models with retrieval. arXiv\n351–360.\npreprintarXiv:2403.03187.\nLuyuGao,YunyiZhang,JiaweiHan,andJamieCallan.\nPayalBajaj,DanielCampos,NickCraswell,LiDeng,\n2021. Scalingdeepcontrastivelearningbatchsize\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nundermemorylimitedsetup. InProceedingsofthe\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\n6thWorkshoponRepresentationLearningforNLP\net al. 2016. Ms marco: A human generated ma-\n(RepL4NLP-2021),pages316–321.\nchinereadingcomprehensiondataset. arXivpreprint\narXiv:1611.09268. DayaGuo,ShuaiLu,NanDuan,YanlinWang,Ming\nZhou,andJianYin.2022. Unixcoder:Unifiedcross-\nMdMustakimBillah,PalashRanjanRoy,ZadiaCod- modalpre-trainingforcoderepresentation. InPro-\nabux, and Banani Roy. 2024. Are large language ceedingsofthe60thAnnualMeetingoftheAssocia-\nmodels a threat to programming platforms? an ex- tionforComputationalLinguistics(Volume1:Long\nploratorystudy. arXivpreprintarXiv:2409.05824. Papers),pages7212–7225.\n9\nDayaGuo,QihaoZhu,DejianYang,ZhendaXie,Kai Aditya Kusupati, Gantavya Bhatt, Aniket Rege,\nDong, Wentao Zhang, Guanting Chen, Xiao Bi, MatthewWallingford,AdityaSinha,VivekRamanu-\nYKLi,etal.2024. Deepseek-coder:Whenthelarge jan,WilliamHoward-Snyder,KaifengChen,Sham\nlanguagemodelmeetsprogramming–theriseofcode Kakade,PrateekJain,etal.2022. Matryoshkarepre-\nintelligence. arXivpreprintarXiv:2401.14196. sentationlearning. AdvancesinNeuralInformation\nProcessingSystems,35:30233–30249.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, YuhangLai,ChengxiLi,YimingWang,TianyiZhang,\netal.2022. Lora:Low-rankadaptationoflargelan- RuiqiZhong,LukeZettlemoyer,Wen-tauYih,Daniel\nguagemodels. InInternationalConferenceonLearn- Fried, Sida Wang, and Tao Yu. 2023. Ds-1000: A\ningRepresentations. naturalandreliablebenchmarkfordatasciencecode\ngeneration. InInternationalConferenceonMachine\nHamelHusain,Ho-HsiangWu,TiferetGazit,Miltiadis Learning,pages18319–18345.PMLR.\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnetchallenge:Evaluatingthestateofsemantic Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\ncodesearch. arXivpreprintarXiv:1909.09436. Raiman,MohammadShoeybi,BryanCatanzaro,and\nWeiPing.2024. Nv-embed:Improvedtechniquesfor\nGautierIzacard,MathildeCaron,LucasHosseini,Sebas- trainingllmsasgeneralistembeddingmodels. arXiv\ntianRiedel,PiotrBojanowski,ArmandJoulin,and preprintarXiv:2405.17428.\nEdouardGrave.2021. Towardsunsuperviseddense\ninformationretrievalwithcontrastivelearning. arXiv Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua\npreprintarXiv:2112.09118,2(3). Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying\nGeng,NanHuo,etal.2024a. Canllmalreadyserve\nGautierIzacard,MathildeCaron,LucasHosseini,Sebas- asadatabaseinterface?abigbenchforlarge-scale\ntianRiedel,PiotrBojanowski,ArmandJoulin,and databasegroundedtext-to-sqls. AdvancesinNeural\nEdouardGrave.2023. Unsuperviseddenseinforma- InformationProcessingSystems,36.\ntionretrievalwithcontrastivelearning. Transactions\nonMachineLearningResearch. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff,DenisKocetkov,ChenghaoMou,Marc\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men- Marone,ChristopherAkiki,JiaLi,JennyChim,etal.\nsch,ChrisBamford,DevendraSinghChaplot,Diego 2023a. Starcoder:maythesourcebewithyou! arXiv\ndelasCasas,FlorianBressand,GiannaLengyel,Guil- preprintarXiv:2305.06161.\nlaumeLample,LucileSaulnier,etal.2023. Mistral\n7b. arXivpreprintarXiv:2310.06825. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia,\nYichunYin,HaoZhang,YongLiu,YashengWang,\nCarlos E Jimenez, John Yang, Alexander Wettig, andRuimingTang.2024b. Coir:Acomprehensive\nShunyu Yao, Kexin Pei, Ofir Press, and Karthik R benchmark for code information retrieval models.\nNarasimhan.2024. Swe-bench:Canlanguagemod- arXivpreprintarXiv:2407.02883.\nelsresolvereal-worldgithubissues? InTheTwelfth\nInternational Conference on Learning Representa- ZehanLi,XinZhang,YanzhaoZhang,DingkunLong,\ntions. PengjunXie,andMeishanZhang.2023b. Towards\ngeneraltextembeddingswithmulti-stagecontrastive\nHyoungwookJin,SeongheeLee,HyungyuShin,and learning. arXivpreprintarXiv:2308.03281.\nJuhoKim.2024. Teachaihowtocode:Usinglarge\nlanguage models as teachable agents for program- ShangqingLiu,YuChen,XiaofeiXie,JingKaiSiow,\nmingeducation. InProceedingsoftheCHIConfer- andYangLiu. Retrieval-augmentedgenerationfor\nenceonHumanFactorsinComputingSystems,pages codesummarizationviahybridgnn. InInternational\n1–28. ConferenceonLearningRepresentations.\nMdEushaKadir,TasnimRahman,SouravBarman,and TianyangLiu,CanwenXu,andJulianMcAuley.2024.\nMd Al-Amin. 2024. Exploring the competency of Repobench: Benchmarking repository-level code\nchatgpt in solving competitive programming chal- auto-completion systems. In The Twelfth Interna-\nlenges. InternationalJournal,13(1). tionalConferenceonLearningRepresentations.\nRabimba Karanjai, Lei Xu, and Weidong Shi. 2024. Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\nSolmover:Smartcontractcodetranslationbasedon Yavuz,CaimingXiong,andSYuPhilip.2021. Dense\nconcepts. In Proceedings of the 1st ACM Interna- hierarchicalretrievalforopen-domainquestionan-\ntional Conference on AI-Powered Software, pages swering. InFindingsoftheAssociationforCompu-\n112–121. tationalLinguistics:EMNLP2021,pages188–200.\nJacobDevlinMing-WeiChangKentonandLeeKristina AntonLozhkov,RaymondLi,LoubnaBenAllal,Fed-\nToutanova.2019. Bert:Pre-trainingofdeepbidirec- ericoCassano,JoelLamy-Poirier,NouamaneTazi,\ntionaltransformersforlanguageunderstanding. In AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,\nProceedingsofnaacL-HLT,volume1,page2.Min- etal.2024. Starcoder2andthestackv2:Thenext\nneapolis,Minnesota. generation. arXivpreprintarXiv:2402.19173.\n10\nShuaiLu,DayaGuo,ShuoRen,JunjieHuang,Alexey Ankita Nandkishor Sontakke, Manasi Patwardhan,\nSvyatkovskiy, Ambrosio Blanco, Colin Clement, LovekeshVig,RaveendraKumarMedicherla,Ravin-\nDawn Drain, Daxin Jiang, Duyu Tang, et al. draNaik,andGautamShroff.2022. Codesumma-\nCodexglue:Amachinelearningbenchmarkdataset rization:Dotransformersreallyunderstandcode? In\nfor code understanding and generation. In Thirty- DeepLearningforCodeWorkshop.\nfifthConferenceonNeuralInformationProcessing\nSystemsDatasetsandBenchmarksTrack(Round1). WeisongSun,ChunrongFang,YuchenChen,Quanjun\nZhang,GuanhongTao,YuduYou,TingxuHan,Yifei\nJorgeMartinez-Gil.2024. Sourcecodeclonedetection Ge,YulingHu,BinLuo,etal.2024. Anextractive-\nusingunsupervisedsimilaritymeasures. InInterna- and-abstractiveframeworkforsourcecodesumma-\ntionalConferenceonSoftwareQuality,pages21–37. rization. ACMTransactionsonSoftwareEngineering\nSpringer. andMethodology,33(3):1–39.\nRui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming CodeGemma Team. 2024. Codegemma: Open\nXiong,YingboZhou,andSemihYavuz.2024. Sfr- code models based on gemma. arXiv preprint\nembedding-2:Advancedtextembeddingwithmulti- arXiv:2406.11409.\nstagetraining.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming RobertDadashi,SuryaBhupatiraju,ShreyaPathak,\nXiong,YingboZhou,andSemihYavuz.2024. Sfr- LaurentSifre,MorganeRivière,MihirSanjayKale,\nembedding-mistral:Enhancetextretrievalwithtrans- Juliette Love, et al. 2024. Gemma: Open models\nferlearning. SalesforceAIResearchBlog. based on gemini research and technology. arXiv\npreprintarXiv:2403.08295.\nGabrieldeSouzaPMoreira,RadekOsmulski,Mengyao\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nXu, Ronay Ak, Benedikt Schifferer, and Even\nhishekSrivastava,andIrynaGurevych. Beir:Ahet-\nOldridge. 2024. Nv-retriever: Improving text em-\nerogeneous benchmark for zero-shot evaluation of\nbeddingmodelswitheffectivehard-negativemining.\ninformation retrieval models. In Thirty-fifth Con-\narXivpreprintarXiv:2407.15831.\nferenceonNeuralInformationProcessingSystems\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad- DatasetsandBenchmarksTrack(Round2).\nford,JesseMichaelHan,JerryTworek,QimingYuan,\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nNikolasTezak,JongWookKim,ChrisHallacy,etal.\nJiao,LinjunYang,DaxinJiang,RanganMajumder,\n2022. Textandcodeembeddingsbycontrastivepre-\nand Furu Wei. 2022. Text embeddings by weakly-\ntraining. arXivpreprintarXiv:2201.10005.\nsupervisedcontrastivepre-training. arXivpreprint\narXiv:2212.03533.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-\nvio Savarese, and Yingbo Zhou. 2023. Codegen2:\nLiangWang,NanYang,XiaolongHuang,LinjunYang,\nLessonsfortrainingllmsonprogrammingandnatu-\nRanganMajumder,andFuruWei.2023. Improving\nrallanguages. arXivpreprintarXiv:2305.02309.\ntextembeddingswithlargelanguagemodels. arXiv\npreprintarXiv:2401.00368.\nRangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna,\nDivya Sankar, Lambert Pouguem Wassi, Michele\nYiningWang,LiweiWang,YuanzhiLi,DiHe,andTie-\nMerler,BorisSobolev,RajuPavuluri,SaurabhSinha,\nYanLiu.2013. Atheoreticalanalysisofndcgtype\nand Reyhaneh Jabbarvand. 2024. Lost in transla-\nrankingmeasures. InConferenceonlearningtheory,\ntion:Astudyofbugsintroducedbylargelanguage\npages25–54.PMLR.\nmodelswhiletranslatingcode. InProceedingsofthe\nIEEE/ACM 46th International Conference on Soft- Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu,\nwareEngineering,pages1–13. FrankFXu,YiqingXie,GrahamNeubig,andDaniel\nFried.2024. Coderag-bench:Canretrievalaugment\nMdRizwanParvez,WasiAhmad,SaikatChakraborty,\ncodegeneration? arXivpreprintarXiv:2406.14497.\nBaishakhiRay,andKai-WeiChang.2021. Retrieval\naugmentedcodegenerationandsummarization. In Michel Wermelinger. 2023. Using github copilot to\nFindingsoftheAssociationforComputationalLin- solvesimpleprogrammingproblems. InProceedings\nguistics:EMNLP2021,pages2719–2734. ofthe54thACMTechnicalSymposiumonComputer\nScienceEducationV.1,pages172–178.\nHuy N Phan, Hoang N Phan, Tien N Nguyen, and\nNghiDQBui.2024. Repohyper:Bettercontextre- Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\ntrievalisallyouneedforrepository-levelcodecom- Muennighof.2023. C-pack:Packagedresourcesto\npletion. arXivpreprintarXiv:2403.06095. advancegeneralchineseembedding. arXivpreprint\narXiv:2309.07597.\nBaptisteRoziere,JonasGehring,FabianGloeckle,Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, ShitaoXiao,ZhengLiu,PeitianZhang,NiklasMuen-\nJingyuLiu,RomainSauvestre,TalRemez,etal.2023. nighoff,DefuLian,andJian-YunNie.2024. C-pack:\nCodellama:Openfoundationmodelsforcode. arXiv Packedresourcesforgeneralchineseembeddings. In\npreprintarXiv:2308.12950. Proceedings of the 47th International ACM SIGIR\n11\nConferenceonResearchandDevelopmentinInfor- A Appendix\nmationRetrieval,pages641–649.\nA.1 DatasetDetails\nJohnYang,CarlosEJimenez,AlexanderWettig,Kil-\nian Lieret, Shunyu Yao, Karthik Narasimhan, and\nOurtrainingdatasetcontainsatotalof3.36Mtrain-\nOfirPress.2024. Swe-agent:Agent-computerinter-\ningpairs,covering12differentprogramminglan-\nfacesenableautomatedsoftwareengineering. arXiv\npreprintarXiv:2405.15793. guages.AsshowninFigure3,thedistributionof\nprogramming languages is imbalanced, with the\nBurakYetis¸tiren,Is¸ıkÖzsoy,MirayAyerdem,andEray\nmajorityofthedataconcentratedinafewpopular\nTüzün. 2023. Evaluating the code quality of ai-\nassistedcodegenerationtools:Anempiricalstudyon languages.Pythonrepresentsthelargestportionof\ngithubcopilot,amazoncodewhisperer,andchatgpt. thedatasetat27.1%,followedbyGowith25.2%,\narXivpreprintarXiv:2304.10778.\nand JavaScript and PHP at 17.0% and 17.2%, re-\nFengjiZhang,BeiChen,YueZhang,JackyKeung,Jin spectively. The remaining languages, including\nLiu,DaoguangZan,YiMao,Jian-GuangLou,and Java,SQL,Ruby,andothers,accountforsmaller\nWeizhu Chen. 2023. Repocoder: Repository-level\nproportions,withRust,Kotlin,andC#makingup\ncodecompletionthroughiterativeretrievalandgen-\nthesmallestsharesofthedataset.\neration. InProceedingsofthe2023Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,\npages2471–2484. A.2 ImplementationDetails\nShuyanZhou,UriAlon,FrankFXu,ZhengbaoJiang, WesummarizethehyperparametersinTable8.For\nandGrahamNeubig.2023. Docprompting:Gener-\nthecodetrainingdata,weprependaprompttothe\natingcodebyretrievingthedocs. InTheEleventh\nInternational Conference on Learning Representa- queryintheformat:“Instruct:GivenCodeorText,\ntions. retrieverelevantcontent.Query:”.\n12\nPopularity of Programming Languages (Log Scale)\n1089K\n1,000,000\n528K 482K\n308K 282K PHPJavascript\nGo\n204K 192K Others\n100,000 109K 5.0%5.7%6.1% 9.2% C++\n14.3%\n51K 50K 49K\n3.2%\n32.4% C#\nPython 8.4%\n15.7% C\n17K\nPython Java C++ Go C Javascript PHP C# Ruby Rust Kotlin SQL\nJava\nProgramming Languages\nFigure3:Theprogramminglanguagedistributionofcodetrainingdatainthegeneraltrainingstage.\nCodeXEmbed CodeXEmbed CodeXEmbed\n400M 2B 7B\nbasemodel gte-large-en-v1.5 gemma-2-2b-it Mistral-7B-Instruct-v0.3\nmaxlearningrate 5×10−5 5×10−5 5×10−5\nGradCache 4 16 32\ntuningparameters FullyModel LoRA LoRA\nLoraRank - 8 8\nwarmupsteps 100 50 50\nbatchsize 1024 1024 1024\nmaxlength 512 512 512\nweightdecay 0.01 0.01 0.01\nhardnegatives 7 7 7\nbidirectionalattention ✓ ✓ ✓\ntext:coderatio 1:1 1:3 1:3\npooling bos eos eos\nTable8:Hyperparametersforcontrastivecodeandtexttraining\n13\ngnal\nhcae\nrednu\nsriap\ngniniarT\neht\nfo\neziS",
    "pdf_filename": "CodeXEmbed_A_Generalist_Embedding_Model_Family_for_Multiligual_and_Multi-task_Code_Retrieval.pdf"
}