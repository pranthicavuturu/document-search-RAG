{
    "title": "CodeXEmbed A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval",
    "abstract": "Despite the success of text retrieval in many NLP tasks, code retrieval remains a largely un- derexplored area. Most text retrieval systems are tailored for natural language queries, often neglecting the specific challenges of retriev- ing code. This gap leaves existing models un- able to effectively capture the diversity of pro- gramming languages and tasks across differ- ent domains, highlighting the need for more focused research in code retrieval. To address this, we introduce CODEXEMBED, a family of large-scale code embedding models rang- ing from 400M to 7B parameters. Our novel training pipeline unifies multiple programming languages and transforms various code-related tasks into a common retrieval framework, en- hancing model generalizability and retrieval performance. Our 7B model sets a new state-of- the-art (SOTA) in code retrieval, outperforming the previous leading model, Voyage-Code, by over 20% on CoIR benchmark. In addition to excelling in code retrieval, our models demon- strate competitive performance on the widely adopted BeIR text retrieval benchmark, offer- ing versatility across domains. Experimental results demonstrate that improving retrieval performance significantly enhances end-to-end Retrieval-Augmented Generation (RAG) per- formance for code-related tasks. 1 Introduction Large Language Models (LLMs) have demon- strated exceptional performance across numerous Natural Language Processing (NLP) tasks. How- ever, they often struggle to produce faithful an- swers and may lack up-to-date or domain-specific knowledge. To bridge this gap, retrieval-augmented generation (RAG) (Cai et al., 2022; Cheng et al., 2024) techniques have gained prominence, integrat- ing Information Retrieval (IR) systems with LLMs to enhance their access to relevant external informa- tion. This synergy has drawn significant attention recently, leading to the development of various re- trieval models (Wang et al., 2022; Chen et al., 2024) based on BERT (Kenton and Toutanova, 2019) and other LLMs with sizes exceeding 1 billion parame- ters (Wang et al., 2023; Moreira et al., 2024; Meng et al., 2024). Despite these advancements, standard IR methods, while effective in text-based retrieval, often fall short in specialized domains such as code retrieval (Husain et al., 2019). Code retrieval is critical for accelerating devel- opment processes and improving code quality. Un- like general text retrieval, code retrieval enables developers to quickly locate relevant code snippets, explanations, bug analyses, summaries, and simi- lar instances. Effective code retrieval systems are now integrated into commercial products like VS Code (Del Sole, 2021) and GitHub Copilot (Wer- melinger, 2023; Yeti¸stiren et al., 2023), enhanc- ing productivity. Code-RAG systems (Parvez et al., 2021; Liu et al.; Jimenez et al., 2024; Wang et al., 2024) also leverage code retrieval to minimize hal- lucinations in generated code from LLMs, ensur- ing more accurate outputs. However, traditional text retrieval models often struggle with code be- cause they focus on linguistic patterns, while code retrieval must handle elements like syntax, vari- able dependencies, control flow, and API usage. Despite the importance of code-specific models, existing ones like CodeBERT (Feng et al., 2020), CodeGPT (Lu et al.), and UniXcoder (Guo et al., 2022) are based on smaller BERT models (Kenton and Toutanova, 2019). While large-scale models for retrieval have become popular, only Voyage- Code has followed this approach for code retrieval, but it remains a closed model, leaving a gap for open-source, large-scale code retrieval models. In this work, we introduce CODEXEMBED, a family of open-source embedding models tai- lored for both code and text, available in sizes of 400 million, 2 billion, and 7 billion parameters. CODEXEMBED introduces a generalizable training 1 arXiv:2411.12644v1  [cs.SE]  19 Nov 2024",
    "body": "CodeXEmbed: A Generalist Embedding Model Family\nfor Multiligual and Multi-task Code Retrieval\nYe Liu, Rui Meng, Shafiq Jot, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz\nSalesforce AI Research\nyeliu@salesforce.com\nAbstract\nDespite the success of text retrieval in many\nNLP tasks, code retrieval remains a largely un-\nderexplored area. Most text retrieval systems\nare tailored for natural language queries, often\nneglecting the specific challenges of retriev-\ning code. This gap leaves existing models un-\nable to effectively capture the diversity of pro-\ngramming languages and tasks across differ-\nent domains, highlighting the need for more\nfocused research in code retrieval. To address\nthis, we introduce CODEXEMBED, a family\nof large-scale code embedding models rang-\ning from 400M to 7B parameters. Our novel\ntraining pipeline unifies multiple programming\nlanguages and transforms various code-related\ntasks into a common retrieval framework, en-\nhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-\nthe-art (SOTA) in code retrieval, outperforming\nthe previous leading model, Voyage-Code, by\nover 20% on CoIR benchmark. In addition to\nexcelling in code retrieval, our models demon-\nstrate competitive performance on the widely\nadopted BeIR text retrieval benchmark, offer-\ning versatility across domains. Experimental\nresults demonstrate that improving retrieval\nperformance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) per-\nformance for code-related tasks.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated exceptional performance across numerous\nNatural Language Processing (NLP) tasks. How-\never, they often struggle to produce faithful an-\nswers and may lack up-to-date or domain-specific\nknowledge. To bridge this gap, retrieval-augmented\ngeneration (RAG) (Cai et al., 2022; Cheng et al.,\n2024) techniques have gained prominence, integrat-\ning Information Retrieval (IR) systems with LLMs\nto enhance their access to relevant external informa-\ntion. This synergy has drawn significant attention\nrecently, leading to the development of various re-\ntrieval models (Wang et al., 2022; Chen et al., 2024)\nbased on BERT (Kenton and Toutanova, 2019) and\nother LLMs with sizes exceeding 1 billion parame-\nters (Wang et al., 2023; Moreira et al., 2024; Meng\net al., 2024). Despite these advancements, standard\nIR methods, while effective in text-based retrieval,\noften fall short in specialized domains such as code\nretrieval (Husain et al., 2019).\nCode retrieval is critical for accelerating devel-\nopment processes and improving code quality. Un-\nlike general text retrieval, code retrieval enables\ndevelopers to quickly locate relevant code snippets,\nexplanations, bug analyses, summaries, and simi-\nlar instances. Effective code retrieval systems are\nnow integrated into commercial products like VS\nCode (Del Sole, 2021) and GitHub Copilot (Wer-\nmelinger, 2023; Yeti¸stiren et al., 2023), enhanc-\ning productivity. Code-RAG systems (Parvez et al.,\n2021; Liu et al.; Jimenez et al., 2024; Wang et al.,\n2024) also leverage code retrieval to minimize hal-\nlucinations in generated code from LLMs, ensur-\ning more accurate outputs. However, traditional\ntext retrieval models often struggle with code be-\ncause they focus on linguistic patterns, while code\nretrieval must handle elements like syntax, vari-\nable dependencies, control flow, and API usage.\nDespite the importance of code-specific models,\nexisting ones like CodeBERT (Feng et al., 2020),\nCodeGPT (Lu et al.), and UniXcoder (Guo et al.,\n2022) are based on smaller BERT models (Kenton\nand Toutanova, 2019). While large-scale models\nfor retrieval have become popular, only Voyage-\nCode has followed this approach for code retrieval,\nbut it remains a closed model, leaving a gap for\nopen-source, large-scale code retrieval models.\nIn this work, we introduce CODEXEMBED,\na family of open-source embedding models tai-\nlored for both code and text, available in sizes of\n400 million, 2 billion, and 7 billion parameters.\nCODEXEMBED introduces a generalizable training\n1\narXiv:2411.12644v1  [cs.SE]  19 Nov 2024\n\nframework that converts multiple programming lan-\nguages and diverse code-related tasks into retrieval\ntasks. Our approach handles 12 programming lan-\nguages and five distinct code retrieval categories\nacross eight different code tasks, including code-to-\ntext, text-to-code, code-to-code, text-to-text and hy-\nbrid text and code tasks. This comprehensive setup\nenables CODEXEMBED to generalize effectively\nacross various code domains. Our contributions can\nbe summarized as follows:\n• We propose a generalizable training approach\nfor code embedding that transforms diverse\nprogramming languages and code-related\ntasks into retrieval tasks. This method allows\nfor a unified framework that effectively han-\ndles multiple languages and task types, im-\nproving retrieval performance across various\ncode tasks.\n• Our 7B model, the open-source code embed-\nding model to date, achieves state-of-the-art\nperformance on code retrieval CoIR bench-\nmark, setting a new standard for code re-\ntrieval. We also evaluate CODEXEMBED on\nRepoEval and SWE-Bench-Lite, demonstrat-\ning that improved retrieval significantly en-\nhances end-to-end Retrieval-Augmented Gen-\neration (RAG) performance for code tasks.\n• In addition to the 7B model, we train smaller\nmodels (400M and 2B) that outperform the\nprevious state-of-the-art in code retrieval. Fur-\nthermore, our models demonstrate compet-\nitive performance on text retrieval, making\nthem versatile across both code and text re-\ntrieval tasks.\n2\nMethod\nWe transform general code-related tasks into a uni-\nfied retrieval framework by categorizing them into\ndistinct retrieval settings. As shown in Figure 1,\neach setting serves a specific purpose in mapping\nbetween code and text, enhancing the model’s abil-\nity to generalize across various retrieval tasks.\nText-to-Code Retrieval.\nThe Text-to-Code re-\ntrieval setting focuses on retrieving relevant code\ndocuments based on a given textual query. This\nsetup is particularly useful in scenarios where nat-\nural language descriptions are mapped to code so-\nlutions. Several code generation tasks can be trans-\nformed to fit into this retrieval paradigm.\nOne such task is code contest generation (Billah\net al., 2024; Kadir et al., 2024), where a natural\nlanguage description of a programming problem\nis provided, and the goal is to retrieve or generate\na code snippet that correctly solves the problem.\nThis setting closely aligns with the Text-to-Code\nretrieval framework as it involves finding code so-\nlutions based on a question or problem statement.\nAnother example is the Text-to-SQL task (Finegan-\nDollak et al., 2018; Li et al., 2024a), where the aim\nis to generate or retrieve a SQL query based on a\nuser’s natural language question. The task fits well\nwithin the Text-to-Code category, as it requires un-\nderstanding the intent of the query and retrieving or\ngenerating the appropriate SQL code that matches\nthe given description.\nCode-to-Text Retrieval.\nThe Code-to-Text re-\ntrieval setting is designed to retrieve relevant tex-\ntual documents or descriptions based on a given\ncode query. This setup is particularly valuable in\nscenarios where developers or systems seek to un-\nderstand or explain code by mapping it to human-\nreadable documentation or summaries.\nOne such task is code summarization (Sontakke\net al., 2022; Sun et al., 2024), where a code file, or\neven an entire repository, is provided as input, and\nthe goal is to generate a concise textual summary\nof the code’s functionality. This task aligns with\nthe Code-to-Text retrieval setting, as it involves\ntaking potentially complex code and producing a\nhuman-readable description or explanation, which\nis critical for documentation, knowledge sharing,\nand onboarding in software projects.\nCode-to-Code Retrieval. The Code-to-Code re-\ntrieval setting focuses on retrieving relevant code\ndocuments based on a code query. This retrieval\nsetting is particularly useful for tasks that involve\nunderstanding, transforming, or complementing\ncode snippets. Several tasks naturally align with\nthis framework, making it an essential part of the\nbroader code retrieval ecosystem.\nOne such task is code translation (Pan et al.,\n2024; Karanjai et al., 2024), where the goal is to\ntranslate code snippets from one programming lan-\nguage to another. For instance, a developer might\nprovide a code snippet in Python and seek to trans-\nlate it into C++ or Java. This task fits well into the\nCode-to-Code retrieval setting, as it involves find-\ning the equivalent functionality in a different pro-\ngramming language, ensuring the translated code\nmaintains the same logic and structure.\nAnother important task is code completion (Ding\n2\n\nCode Contest\nRemove Digit From Number\nProblem: You are given a string number \nrepresenting a positive integer and a \ncharacter digit.\nExample 1:\nInput: number=“123”, digit=“3”\nOutput: “12”\ndef remove_digit(number: str, digit: \nstr) -> str:\n    return number.replace(digit, '', 1)\n# Example usage\nnumber = \"123\"\ndigit = \"3\"\nresult = remove_digit(number, digit)\nprint(\"Output:\", result)\nText2SQL\nQuery\nQuestion\nCode Translation\nCode Summary\nCode Clone Detection\nCode Completion \nCode Issue Fix\n… Code block 1 …\nA summary of the code \ncould be as following:\nA method that returns \nthe average color of \npixels in a neighbor-\nhood around a given \npixel in an image.\nCode Agent Conversation\nPlot a chart of CRM \nstock price change YTD.\nimport matplotlib.pyplot as plt \nimport pandas as pd\ndata = { \"Date\": \npd.date_range(start=\"2024-01-01\", \nend=\"2024-09-30\", freq=\"M\"), \n….\nCould you explain line \n12-20?\nSure, line 12-20 use the pandas \npackage to ….\n//sort array A\nfor (I=1, I<10, I++)\n     for (J=i, j> 1, J--)\n          if (A[j] > A(j-1))\n               Swmp(A[I], A[J])\n//define\nClone27 (a,b,c,d)\n     for (a=1, a> b, a++)\n         for (c=a, c>1, c--)\n          if (d[c] > d(c-1))\n               Swap(d[a], d[c])\n//sort array X\nfor (x=1, x<10, x++)\n… Code block 2 …\nFigure 1: The code training data of CODEXEMBED contains four parts: Text-to-Code, Code-to-Code, Code-to-Text\nand Hybrid Code. Each Categories contains several types of code tasks.\net al., 2024; Phan et al., 2024; Liu et al., 2024),\nwhere a partially written code snippet is provided,\nand the system needs to retrieve or generate the\nmissing portions to complete the function or mod-\nule. This is commonly seen in integrated devel-\nopment environments (IDEs) where autocomplete\nfeatures assist developers by suggesting the next\nlogical lines of code based on the current context.\nCode completion aligns perfectly with the Code-\nto-Code retrieval setting, as it requires matching\nthe partial input code with relevant snippets that\nseamlessly complement the existing code.\nAdditionally, code clone detection (Martinez-\nGil, 2024), which involves identifying code snip-\npets that are functionally or structurally identical\nbut may differ in syntax or formatting, also fits\nwithin this retrieval framework. Clone detection is\ncrucial for identifying redundant code, refactoring\ncodebases, and ensuring consistency across differ-\nent sections of a project. This task leverages the\nCode-to-Code retrieval setting by retrieving code\nsnippets that exhibit similar or identical behavior,\neven if they appear different at the surface level.\nHybrid Code Retrieval. The hybrid code retrieval\nsetting involves retrieving a combination of both\ncode and textual documents in response to a hy-\nbrid query, which may contain both code and nat-\nural language. This setting is particularly useful\nfor tasks that require understanding and integrating\nboth code and textual context to generate meaning-\nful responses or retrieve relevant documents.\nOne\nsuch\ntask\nis\ncode\nagent\nconversa-\ntion (Arteaga Garcia et al., 2024; Jin et al., 2024),\nwhere a code agent must generate either a code\nsnippet or a textual response based on a user’s ques-\ntion. This task requires the agent to interpret the\nuser’s question, which might contain both code and\nnatural language, and retrieve or generate the ap-\npropriate code solution or explanation. The hybrid\nquery allows the agent to navigate between tech-\nnical (code) and descriptive (textual) domains to\nprovide a comprehensive response.\nAnother key task is code issue fixing (Yang et al.,\n2024; Jimenez et al., 2024), where a hybrid query\ncontaining a description of the code issue and the\nproblematic code snippet is provided. The goal is\nto generate code that resolves the issue. By incor-\nporating both the textual description (e.g., error\nmessage) and the code itself, hybrid retrieval al-\nlows the model to identify relevant code snippets,\ndetect similar issues, and propose fixes that align\nwith the overall project. This makes hybrid retrieval\nessential in complex development environments,\nwhere both debugging and problem-solving require\na nuanced understanding of both code and text.\nText-to-Text Retrieval. In order to capture the\nmodel’s general retrieval ability across both code\nand natural language domains, we incorporate text-\n3\n\nto-text retrieval data into the training pipeline. This\nallows the model to perform well in traditional text\nretrieval tasks, enhancing its versatility. The ex-\nperiments study the ratio of text and code data to\ndetermine the optimal balance between the two,\naiming to ensure the model’s effectiveness in both\ntext and code retrieval tasks without sacrificing per-\nformance in either domain.\nRetrieval Training. To train a unified code and\ntext retrieval model, we transform all the aforemen-\ntioned tasks into a query-and-answer format, where\neither the query or the answer can be a text or code\ndocument. This unified format allows us to handle\na variety of tasks such as Text-to-Code, Code-to-\nText, and Code-to-Code retrieval within the same\ntraining framework. The training loss is designed\nas a contrastive loss (Liu et al., 2021), which aims\nto maximize the similarity between the query and\nthe correct answer while minimizing the similar-\nity to the negative samples. The loss function is\nrepresented as:\nL = −1\nN\nN\nX\ni=1\n\"\nlog\nexp(qi · d+\ni )\nexp(qi · d+\ni ) + PK\nj=1 exp(qi · d−\nj )\n#\n(1)\nwhere qi represents the query, which can be either\ntext or code. The term d+\ni refers to the correspond-\ning positive document, which could also be in the\nform of text or code. On the other hand, d−\nj de-\nnotes a set of hard negative documents, which are\nselected to be similar in context to the query but in-\ncorrect, making the retrieval task more challenging.\nThe similarity between the query and a document is\nmeasured by sim(q, d). The variable N represents\nthe total number of training samples, indicating the\nnumber of queries used during training, while K\ndefines the number of hard negatives associated\nwith each query.\n3\nExperiments\nEvaluation Benchmarks. We mainly use two\nbenchmarks to evaluate code and text retrieval\nperformance. COIR (Li et al., 2024b) is a com-\nprehensive benchmark specifically designed for\ncode retrieval tasks. COIR covers a wide range\nof retrieval challenges, including 8 fine-grained re-\ntrieval subtasks, spanning 14 major programming\nlanguages. The dataset is composed of 10 distinct\ndatasets, with a total corpus exceeding 2 million\nentries. BEIR (Thakur et al.) is a widely-adopted\nbenchmark designed for text retrieval tasks. BEIR\nencompasses a diverse set of retrieval challenges,\ncovering 9 distinct tasks across various domains\nsuch as question answering, duplicate detection,\nfact-checking, and more. It supports retrieval over\na wide range of datasets and provides a standard-\nized benchmark for evaluating text retrieval models\nacross different domains.\nImplementation Details. We conduct general train-\ning on our proposed code and text pair dataset us-\ning three model sizes: 400M, 2B, and 7B. For the\nCodeXEmbed400M, we use the base model Alibaba-\nNLP/gte-large-en-v1.5 (Li et al., 2023b), applying\nfull model fine-tuning. For the CodeXEmbed2B,\nwe initialize our embedding model from the gen-\neration model google/gemma-2-2b-it (Team et al.,\n2024), using low-rank adaption(LoRA) (Hu et al.,\n2022) with a rank of 8. For the CodeXEmbed7B,\nwe initialize our embedding model from the gen-\neration model mistralai/Mistral-7B-Instruct-v0.3,\nalso using LoRA with a rank of 8. Follow-\ning prior work (Meng et al., 2024), we apply:\n(i) last token pooling for CodeXEmbed2B and\nCodeXEmbed7B, and (ii) beginning token pooling\nfor CodeXEmbed400M to generate semantic vector\nrepresentations (Li et al., 2023b). Cosine similarity\nis used to compute the similarity between query\nand corpus for ranking. The batch size is set to\n1024 across all three model sizes, with 7 hard neg-\natives. The learning rate is set to 5e−5, and the\nend learning rate to 5e−6, with linear decay and\na 50-step warmup. To improve training efficiency\nand reduce GPU memory usage, we adopt gradient\ncaching (Gao et al., 2021). The more implementa-\ntion details can be found in Appendix A.2.\nBaseline Models. For code-domain-specific mod-\nels, we included UniXcoder (Guo et al., 2022) and\nVoyage-Code-0021, both of which are pre-trained\non code data, making them strong baselines for\ncomparison. For general retrieval models, we eval-\nuated E5-Base (Wang et al., 2022), GTE-Base (Li\net al., 2023b), BGE-Base (Xiao et al., 2023), Con-\ntriever (Izacard et al., 2023), E5-Mistral (Wang\net al., 2023), BGE-M3 (Chen et al., 2024), NV-\nEmbed-V2 (Moreira et al., 2024), SFR-V2 (Meng*\net al., 2024) and OpenAI-Ada-0022.\nEvaluation Metrics. In code retrieval, selecting\nthe right evaluation metric is key for assessing both\nranking sensitivity and relevance. Building on prior\nwork (Wang et al., 2013), Normalized Discounted\nCumulative Gain (NDCG) is preferred for its abil-\n1https://blog.voyageai.com/2024/01/23/\nvoyage-code-2-elevate-your-code-retrieval/\n2https://platform.openai.com/docs/\nguides/embeddings\n4\n\nText-to-Code\nCode-to-Text\nCode-to-Code\nHybrid Code\nModel\nApps\nCosQA\nText2SQL\nCSN\nCSN-CCR\nCodeTrans\nStackOverFlow\nCodeFeedBack\nAvg\n-Contest\n-DL\nQA\n-ST\n-MT\nBaselines\nContriever (110M)\n5.14\n14.21\n45.46\n34.72\n35.74\n44.16\n24.21\n66.05\n55.11\n39.23\n36.40\nE5-base (110M)\n11.52\n32.59\n52.31\n67.99\n56.87\n62.50\n21.87\n86.86\n74.52\n41.99\n50.90\nBGE-Base (110M)\n4.05\n32.76\n45.59\n69.60\n45.56\n38.50\n21.71\n73.55\n64.99\n31.42\n42.77\nGTE-Base (110M)\n3.24\n30.24\n46.19\n43.35\n35.50\n33.81\n28.80\n55.19\n55.19\n28.48\n36.75\nUniXcoder (123M)\n1.36\n25.14\n50.45\n60.20\n58.36\n41.82\n31.03\n44.67\n36.02\n24.21\n37.33\nBGE-M3 (567M)\n7.37\n22.73\n48.76\n43.23\n47.55\n47.86\n31.16\n51.04\n49.94\n33.46\n39.31\nE5-Mistral (7B)\n21.33\n31.27\n65.98\n54.25\n65.27\n82.55\n33.24\n91.54\n72.71\n33.65\n55.18\nOpenAI-Ada-002\n8.70\n28.88\n58.32\n74.21\n69.13\n53.34\n26.04\n72.40\n47.12\n17.74\n45.59\nVoyage-Code-002\n26.52\n29.79\n69.26\n81.79\n73.45\n72.77\n27.28\n67.68\n65.35\n28.74\n56.26\nGeneral Training\nCodeXEmbed400M\n48.57\n34.05\n58.96\n72.53\n80.15\n75.67\n34.85\n89.51\n78.87\n45.75\n61.89\nCodeXEmbed2B\n74.99\n36.31\n59.00\n73.50\n85.77\n86.63\n33.17\n90.54\n81.15\n53.08\n67.41\nCodeXEmbed7B\n85.22\n33.27\n64.57\n78.84\n86.77\n90.64\n32.31\n94.25\n80.93\n57.83\n70.46\nIn-domain Training\nCodeXEmbed400M\n45.91\n41.28\n61.29\n81.23\n93.74\n82.72\n40.81\n92.35\n83.36\n61.51\n68.42\nCodeXEmbed2B\n76.86\n40.47\n78.42\n87.87\n97.66\n90.30\n38.57\n94.47\n86.36\n65.51\n75.65\nCodeXEmbed7B\n85.38\n42.47\n78.94\n89.67\n97.95\n94.45\n40.46\n96.33\n87.53\n68.83\n78.20\nTable 1: Performance of the Pretrained CODEXEMBED Model Family across various tasks, including Text-to-Code,\nCode-to-Text, Code-to-Code, and Hybrid Code, along with their average scores. CSN stands for CodeSearchNet.\nity to account for both rank order and varying rele-\nvance, unlike precision, recall or MRR. Therefore,\nwe use NDCG@10 to evaluate performance on\nCoIR and BEIR.\n3.1\nGeneral Training Evaluation\nIn the General Training block of Table 1, we\npresent the results of models trained exclusively\non our proposed general training data, without\nusing any CoIR in-domain data. When averaged\nover all 10 datasets in the CoIR benchmark,\nCodeXEmbed7B model achieves the best results,\nexceeding the SOTA code-domain specific model\nVoyage-Code-002 by over 20%, which shows our\ngeneral code and text training stage significantly\nimproves model performance on code tasks.\nAs shown in Table 1, CodeXEmbed400M and\nCodeXEmbed2B also provide a significant improve-\nment over Voyage-Code-002 and offer a great al-\nternative to the 7B model with substantial practi-\ncal advantages on the latency and cost. Moreover,\ntheir success further validates the transferability\nand generalizability of our proposed training recipe\nfor code embedding models.\n3.2\nIn-domain Training Evaluation\nWe further trained the model on the CoIR in-\ndomain dataset. As shown in the In-domain\nTraining block of Table 1, further training\non in-domain data results in consistent perfor-\nmance improvements across all model sizes.\nSpecifically, CodeXEmbed400M improves by 6.5\npoints, CodeXEmbed2B by 8.24 points, and\nText-to-Code\nCode-to-Text\nCode-to-Code\nHybrid\nAVG\n30\n40\n50\n60\n70\n80\n90\nNDCG@10\n400M GT\n2B GT\n7B GT\n400M ID-GT\n2B ID-GT\n7B ID-GT\nFigure 2: The performance comparison between General\nTraining (GT) and In-domain Training (ID) across three\nmodel sizes (400M, 2B, and 7B) on different CoIR\ncategories and the overall average.\nCodeXEmbed7B by 7.74 points on average across\nall 10 datasets. In Figure 2, the top of each bar\nrepresents the improvement from in-domain train-\ning. Among all categories, the Code-to-Text cate-\ngory shows the largest improvement, even outper-\nforming Voyage-Code-002. In other categories, the\nmodel also achieves over a 5-point improvement.\n3.3\nUnified Text and Code Retrieval\nTo evaluate the text and code retrieval capabilities\nwithin a single embedding model, we also present\nthe BEIR text retrieval performance of CODEX-\nEMBED across various sizes. As shown in Table 2,\nour 7B model achieves an average score of over 60\nacross 15 datasets, placing it among the top tier on\nthe MTEB leaderboard3. Compared to E5-Mistral-\n7B-Instruct (E5-Mistral) (Jiang et al., 2023), which\n3https://huggingface.co/spaces/mteb/\nleaderboard\n5\n\nModel\nBM25\ngte-large\ngte-Qwen2\nE5-Mistral\nCODEXEMBED\nCODEXEMBED\nCODEXEMBED\n400M\n1.5B\n7B\n400M\n2B\n7B\nMS MARCO\n22.8\n42.93\n43.36\n43.06\n42.77\n41.26\n42.05\nTREC-Covid\n65.6\n77.49\n85.38\n87.03\n77.47\n84.58\n79.04\nNFCorpus\n32.5\n36.95\n39.34\n38.58\n35.76\n41.56\n43.14\nNQ\n32.9\n56.08\n56.08\n63.53\n63.38\n67.25\n74.11\nHotpotQA\n60.3\n68.18\n64.00\n75.72\n74.93\n74.39\n79.33\nFiQA\n23.6\n63.23\n63.23\n56.81\n60.20\n56.17\n60.41\nArguAna\n31.5\n72.11\n54.70\n61.65\n69.67\n61.39\n63.58\nTouche-2020\n36.7\n22.55\n27.89\n26.27\n20.18\n26.10\n25.80\nCQADupStack\n29.9\n42.16\n44.76\n42.97\n46.07\n47.46\n51.45\nQuora\n78.9\n89.67\n89.64\n89.61\n89.05\n89.27\n89.51\nDBPedia\n31.3\n46.30\n48.69\n48.89\n46.68\n47.33\n49.27\nScidocs\n15.8\n26.35\n24.98\n16.32\n25.05\n23.36\n25.25\nFever\n75.3\n93.81\n91.57\n87.84\n93.86\n89.03\n91.94\nClimate-Fever\n21.3\n48.36\n42.91\n38.35\n42.70\n32.08\n36.93\nScifact\n66.5\n82.43\n78.44\n76.42\n87.37\n84.79\n89.10\nAverage\n41.7\n57.91\n58.29\n56.87\n58.34\n57.73\n60.06\nBest on\n1\n5\n1\n1\n1\n0\n6\nTable 2: Comparison of performance across text retrieval BEIR datasets with different model size.\nis trained on both text and synthetic data, initial-\nized from the Mistral series, our model employs\nsingle-stage training with both code and text data.\nOur model in a performance improvement of 3.19\npoints over E5-Mistral.\nIn the 400M models, CODEXEMBED achieves a\n0.43 performance boost compared to GTE-large (Li\net al., 2023b), on which it is trained. This under-\nscores the advantage of our approach, demonstrat-\ning the potential to improve text retrieval perfor-\nmance by incorporating code data. There are not\nmany 2B-sized language models available, and we\nselected Gemma 2B (Team et al., 2024) due to\nits strong performance in code retrieval. For text\nretrieval, it achieves comparable performance to\ngte-Qwen2 of similar size.\nModel\nNV-Embed-V2\nSFR-v2\nCODEXEMBED\n7B\n7B\n7B\nCoIR\n59.10\n61.48\n70.46\nBEIR\n62.65\n60.18\n60.06\nAVG\n60.88\n60.83\n65.26\nTable 3: Comparison of code and text retrieval bench-\nmarks between our model and the top models on the\nMTEB leaderboard.\nWe present the performance of leading text re-\ntrieval models from the MTEB leaderboard4 on\nboth code and text tasks, in Table 3. Compared\nto the top-ranked model, NV-Embed-V2 (Moreira\net al., 2024)5, our model outperforms it by 4.38\npoints, with an average score across text and code\nretrieval datasets.\n4https://huggingface.co/spaces/mteb/leaderboard\n5Top-performing model as of 10/15/2024.\n3.4\nRetrieval-Augmented Code Generation\nIn this section, we explore how different retriev-\ners influence the final code completion and issue\nresolution performance in repository-level tasks.\n3.4.1\nRepoEval\nTo address this, we utilize RepoEval (Zhang et al.,\n2023) for repository-level code completion. While\nRepoEval consists of three splits (function, class,\nand module), we report results only on the function\nsplit, as it is the only one that supports execution-\nbased evaluation. We adopt Pass@1 as our evalua-\ntion metric, which measures the accuracy of the top-\n1 generated code passing the provided test cases.\nFor code generation, we supply the top-5 re-\ntrieved code snippets as input to the GPT-3.5-turbo\ngeneration model. As shown in Table 4, all sizes\nof CODEXEMBED outperform the canonical setup.\nWhile some files may not contain direct solutions,\nas in the canonical documents, they often include\nvaluable function definitions or usage examples\nthat improve code generation outcomes. This sug-\ngests that our embeddings effectively capture the\nrepository structure and retrieve contexts that im-\nplicitly support problem-solving.\n3.4.2\nSWE-Bench-Lite\nIn our experiments, we use SWE-bench-Lite6, a\ncurated subset of 300 problems from the original\nSWE-bench benchmark. It focuses on resolving\nGitHub issues by requiring models to modify mul-\ntiple files to pass test cases, providing a manageable\nand reproducible dataset. SWE-bench-Lite also in-\n6https://huggingface.co/datasets/\nprinceton-nlp/SWE-bench_Lite\n6\n\nModel\nRepo-Level\nRepoEval\nSWE-Bench-Lite\nNone\n23.9\n0.7\nBM25\n30.8\n1.0\nVoyage, code\n43.2\n0.7\nOpenAI, small\n48.0\n0.3\nOpenAI, rerank\n49.6\n0.0\nCodeXEmbed400M\n52.5\n0.6\nCodeXEmbed2B\n66.3\n1.6\nCodeXEmbed7B\n57.8\n2.6\nGold\n39.1\n2.7\nTable 4: Performance of repository-level code retrieval\naugmented generation using gpt-3.5-turbo.\ncludes a pre-configured Docker container, ensuring\nconsistent evaluation across systems and further\nstandardizing the testing environment. Our results\nshow that incorporating improved retrieval meth-\nods can significantly enhance the end-to-end per-\nformance of code retrieval-augmented generation,\nbringing it closer to using gold content and boost-\ning problem-solving efficiency and accuracy.\n3.5\nImpact of the Base Models\nTo understand the base model’s impact, we exam-\nine: (1) if training from a text retrieval model out-\nperforms a generation model, and (2) if a code-\nspecific generation model offers more advantages\nthan a general language model.\n3.5.1\nEmbedding Models v.s. LLMs\nAs shown in Table 5, the text retrieval model offers\na stronger starting point, and additional training\nwith our approach enhances both its text and code\nretrieval capabilities. For instance, gte-Qwen2 7’s\nCoIR performance improves from 62.96 to 68.52,\nwhile its text performance increases from 58.29\nto 59.12. In contrast, the text generation model re-\nquires more extensive fine-tuning to reach similar\nperformance. However, the advantage of text re-\ntrieval models can sometimes hinder code retrieval\nperformance, as seen with SFR-V2 (Meng* et al.,\n2024) underperforming compared to Mistral in spe-\ncific tasks. This suggests that prior knowledge from\ntext-focused models may not always transfer well\nto code-specific scenarios. To pursue a more gen-\neral training approach, we chose to train using a\ngeneration model rather than a text retrieval model.\n7https://www.aimodels.\nfyi/models/huggingFace/\ngte-qwen2-15b-instruct-alibaba-nlp\nModel\ngte-Qwen2\nGemma-v2\nSFR-v2\nMistral\nInitial\nGT\nGT\nInitial\nGT\nGT\nSize\n1.5B\n2B\n7B\n7B\nCoIR\n62.96\n68.52\n67.41\n61.28\n69.72\n70.40\nBEIR\n58.29\n59.12\n57.73\n60.18\n60.62\n60.06\nTable 5: Comparison of base models: Retrieval vs. Gen-\neration Models. GT represents our general training.\nCode-Specific LLMs\nGeneral-Domain LLMs\nModel\nStarCoder-v2\nDeepSeek-Coder\nGemma-v2\nMistral\n3B\n6.7B\n2B\n7B\nCoIR\n66.95\n71.66\n67.41\n70.40\nBEIR\n49.06\n50.22\n57.73\n60.06\nTable 6: Comparison of base models: Code-specific vs.\nGeneral-domain Generation Models.\n3.5.2\nCode-Specific LLMs v.s. General LLMs\nWe evaluate whether to choose code-specific mod-\nels (Lozhkov et al., 2024; Guo et al., 2024) or gen-\neral LLMs (Jiang et al., 2023; Team et al., 2024).\nAs shown in Table 6, code-specific LLMs excel in\ncode tasks but underperform in text tasks, while\ngeneral LLMs perform well in both. This suggests\nthat recent advancements in general LLMs have\nintegrated code data into their training (Team et al.,\n2024), and this capability can be effectively trans-\nferred to code retrieval. This finding highlights the\nversatility of general LLMs, making them viable\nfor both text and code retrieval without the need\nfor specialized models.\n3.6\nProgramming Language Transferability\nWe aim to explore the diversity of programming\nlanguages and their unique features. The details of\nour code training dataset, including language cov-\nerage, are provided in Appendix A.1. Our dataset\ncomprises 12 programming languages, with Python\nrepresenting the highest percentage of the data. For\ntesting, we selected Python and Java due to their\ndistinct programming paradigms: Python is known\nfor its scripting capabilities and Java for its strong\nobject-oriented design. This selection allows us to\nevaluate our model’s performance across a range of\nprogramming styles, reflecting the versatility and\nadaptability of the embedding model.\nAs shown in Table 7, training with all 12 pro-\ngramming languages yields the best average per-\nformance across 7 target languages, compared to\ntraining with a single language. However, train-\ning on Java-only consistently achieves the highest\nperformance for Java and delivers comparable re-\n7\n\nModel Size\nPython\nJava\nGo\nPHP\nJavascript\nRuby\nSQL\nAVG\n∆-All\nAll\n400M\n59.70\n72.09\n79.28\n70.96\n70.18\n73.01\n58.87\n69.16\n-\n2B\n65.73\n79.40\n81.01\n79.39\n76.76\n78.42\n58.23\n74.14\n-\n7B\n67.56\n81.36\n85.76\n81.89\n78.37\n83.11\n65.74\n77.68\n-\nPython\n400M\n55.43\n49.57\n49.56\n40.57\n44.57\n44.57\n44.21\n46.93\n−32.1%\n2B\n60.86\n71.46\n68.69\n57.88\n62.95\n72.21\n56.09\n64.31\n−13.3%\n7B\n67.02\n78.35\n81.56\n70.88\n75.73\n79.19\n62.56\n73.61\n−5.2%\nJava\n400M\n54.79\n78.37\n72.94\n70.85\n69.85\n68.78\n50.31\n66.56\n−3.8%\n2B\n63.03\n82.17\n79.35\n80.30\n76.00\n76.76\n58.03\n73.66\n−0.6%\n7B\n66.24\n84.25\n83.72\n80.18\n79.82\n82.82\n64.59\n77.37\n−0.4%\nTable 7: Python/Java indicates that only the Python and Java portions were used to train CODEXEMBED, while\nAll indicates that all programming languages were used to train CODEXEMBED. ∆-All represents the difference\nbetween the Python/Java AVG score and the All AVG score for the same model size.\nsults to using all languages across all model sizes.\nFor example, the Java-only 7B model scores 77.37,\nwhile the all-languages model scores 77.68. When\ncomparing models trained exclusively on Python\nor Java, the Java-trained model consistently outper-\nforms. This may be because modern language mod-\nels are already heavily trained on Python during\nthe generation phase, so relying solely on Python\nin the retrieval phase may miss important nuances,\nresulting in suboptimal performance.\n4\nRelated Work\n4.1\nRetrieval Models for Text and Code\nText retrieval models have significantly advanced\nby exploiting supervision from natural language\ninference tasks and labeled query-document\npairs, such as the MS-MARCO passage ranking\ndataset (Bajaj et al., 2016), to train effective text\nembeddings (Izacard et al., 2021; Wang et al.,\n2022; Xiao et al., 2024). Recently, researchers have\nleveraged large language models (LLMs) (Jiang\net al., 2023) as the base for training retrieval mod-\nels, resulting in state-of-the-art performance. No-\ntable examples include E5-Mistral (Wang et al.,\n2023), SFR-Embedding (Meng et al., 2024), and\nNV-Embedding (Lee et al., 2024). While numerous\nmodels have been developed for text retrieval tasks,\nfew have focused specifically on code retrieval.\nAmong the few are the Voyage code model (AI,\n2024) and OpenAI’s embeddings (Neelakantan\net al., 2022); however, both are closed-source mod-\nels, limiting their accessibility and adaptability for\nthe wider research community.\n4.2\nCode Retrieval Augmented Generation\nNeural code generation has been an important\ntask (Lu et al.), and increasingly strong code\nlanguage models have been developed (Roziere\net al., 2023; Nijkamp et al., 2023; Li et al., 2023a;\nGuo et al., 2024; Team, 2024) to solve various\ntasks (Chen et al., 2021; Lai et al., 2023; Jimenez\net al., 2024). However, most LMs generate code\nsolely from natural language problem descriptions\nand the models’ parametric knowledge, without\nleveraging external programming resources or us-\ning a retrieval-augmented generation approach.\nWhile prior work has focused on text-centric tasks\nwith general-domain corpora like Wikipedia (Asai\net al., 2024), some studies have used retrieved pro-\ngramming context from repositories (Ding et al.,\n2024; Yang et al., 2024) or documentation (Zhou\net al., 2023). Code retrieval is crucial for enhancing\ncode generation in RAG systems because it allows\nmodels to access relevant external code resources,\nleading to more accurate and context-aware code\noutputs. Our code retrieval model demonstrates sig-\nnificant improvements in code RAG performance,\nunderscoring the importance of effective code re-\ntrieval in code generation tasks.\n5\nConclusion\nIn the underexplored field of code retrieval, we\nintroduce CODEXEMBED, a family of code em-\nbedding models ranging from 400M to 7B parame-\nters. Our training pipeline unifies multiple program-\nming languages and converts diverse code-related\ntasks into a common retrieval framework. Our mod-\nels surpass the previous SOTA code retrieval by\nover 20% on the CoIR benchmark and achieve per-\nformance comparable to SOTA text retrievals on\nthe BEIR benchmark. By enhancing retrieval capa-\nbilities, we demonstrate significant improvements\nin end-to-end retrieval-augmented generation for\ncode-related tasks. By bridging the gap between\ntext and code retrieval domains and releasing our\nmodels to the community, we aim to promote fur-\nther research and innovation in developer tools and\nprogramming language understanding.\n8\n\n6\nLimitations\nWhile CODEXEMBED achieves state-of-the-art re-\nsults in code and text retrieval, there are certain\nlimitations to consider. First, the model’s large size,\nparticularly the 7B parameter variant, leads to sig-\nnificant storage and computational costs. With em-\nbeddings of 4096 dimensions, the model is more\nresource-intensive compared to smaller or more ef-\nficient representations. Although embedding size\nreduction techniques, such as Matryoshka represen-\ntation learning (Kusupati et al., 2022), have shown\npromise in maintaining performance with lower di-\nmensions, further research is needed to apply these\napproaches effectively to code retrieval without\ncompromising performance. Additionally, while\nCODEXEMBED generalizes well across different\nprogramming languages and text retrieval tasks, it\nmay still struggle with niche programming domains\nor highly specialized languages that were underrep-\nresented in the training data. Future work could\nexplore adapting the model for these underrepre-\nsented languages and improving efficiency without\nsacrificing accuracy.\nReferences\nVoyage\nAI.\n2024.\nVoyage\ncode\n2:\nEl-\nevate\nyour\ncode\nretrieval.\nhttps:\n//blog.voyageai.com/2024/01/23/\nvoyage-code-2-elevate-your-code-retrieval/.\n[Online; accessed 8-October-2024].\nEmily Judith Arteaga Garcia, João Felipe Nicolaci Pi-\nmentel, Zixuan Feng, Marco Gerosa, Igor Stein-\nmacher, and Anita Sarma. 2024. How to support\nml end-user programmers through a conversational\nagent. In Proceedings of the 46th IEEE/ACM Inter-\nnational Conference on Software Engineering, pages\n1–12.\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei\nKoh, Luke Zettlemoyer, Hannaneh Hajishirzi, and\nWen-tau Yih. 2024.\nReliable, adaptable, and at-\ntributable language models with retrieval.\narXiv\npreprint arXiv:2403.03187.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016.\nMs marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nMd Mustakim Billah, Palash Ranjan Roy, Zadia Cod-\nabux, and Banani Roy. 2024. Are large language\nmodels a threat to programming platforms? an ex-\nploratory study. arXiv preprint arXiv:2409.05824.\nDeng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recent advances in retrieval-augmented text\ngeneration. In Proceedings of the 45th international\nACM SIGIR conference on research and development\nin information retrieval, pages 3417–3419.\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nLuo, Defu Lian, and Zheng Liu. 2024.\nM3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. In Findings of the Associa-\ntion for Computational Linguistics ACL 2024, pages\n2318–2335.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde De Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021.\nEvaluating large lan-\nguage models trained on code.\narXiv preprint\narXiv:2107.03374.\nXin Cheng, Di Luo, Xiuying Chen, Lemao Liu,\nDongyan Zhao, and Rui Yan. 2024. Lift yourself\nup: Retrieval-augmented text generation with self-\nmemory. Advances in Neural Information Process-\ning Systems, 36.\nAlessandro Del Sole. 2021. Introducing visual studio\ncode. In Visual Studio Code Distilled: Evolved Code\nEditing for Windows, macOS, and Linux, pages 1–15.\nSpringer.\nYangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian\nDing, Ming Tan, Nihal Jain, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Parminder Bhatia, Dan\nRoth, et al. 2024. Crosscodeeval: A diverse and mul-\ntilingual benchmark for cross-file code completion.\nAdvances in Neural Information Processing Systems,\n36.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020.\nCodebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1536–1547.\nCatherine Finegan-Dollak, Jonathan K Kummerfeld,\nLi Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui\nZhang, and Dragomir Radev. 2018. Improving text-\nto-sql evaluation methodology. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n351–360.\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.\n2021. Scaling deep contrastive learning batch size\nunder memory limited setup. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 316–321.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225.\n9\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nYK Li, et al. 2024. Deepseek-coder: When the large\nlanguage model meets programming–the rise of code\nintelligence. arXiv preprint arXiv:2401.14196.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2022. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2021. Towards unsupervised dense\ninformation retrieval with contrastive learning. arXiv\npreprint arXiv:2112.09118, 2(3).\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2023. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nCarlos E Jimenez, John Yang, Alexander Wettig,\nShunyu Yao, Kexin Pei, Ofir Press, and Karthik R\nNarasimhan. 2024. Swe-bench: Can language mod-\nels resolve real-world github issues? In The Twelfth\nInternational Conference on Learning Representa-\ntions.\nHyoungwook Jin, Seonghee Lee, Hyungyu Shin, and\nJuho Kim. 2024. Teach ai how to code: Using large\nlanguage models as teachable agents for program-\nming education. In Proceedings of the CHI Confer-\nence on Human Factors in Computing Systems, pages\n1–28.\nMd Eusha Kadir, Tasnim Rahman, Sourav Barman, and\nMd Al-Amin. 2024. Exploring the competency of\nchatgpt in solving competitive programming chal-\nlenges. International Journal, 13(1).\nRabimba Karanjai, Lei Xu, and Weidong Shi. 2024.\nSolmover: Smart contract code translation based on\nconcepts. In Proceedings of the 1st ACM Interna-\ntional Conference on AI-Powered Software, pages\n112–121.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of naacL-HLT, volume 1, page 2. Min-\nneapolis, Minnesota.\nAditya Kusupati, Gantavya Bhatt, Aniket Rege,\nMatthew Wallingford, Aditya Sinha, Vivek Ramanu-\njan, William Howard-Snyder, Kaifeng Chen, Sham\nKakade, Prateek Jain, et al. 2022. Matryoshka repre-\nsentation learning. Advances in Neural Information\nProcessing Systems, 35:30233–30249.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,\nRuiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel\nFried, Sida Wang, and Tao Yu. 2023. Ds-1000: A\nnatural and reliable benchmark for data science code\ngeneration. In International Conference on Machine\nLearning, pages 18319–18345. PMLR.\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan\nRaiman, Mohammad Shoeybi, Bryan Catanzaro, and\nWei Ping. 2024. Nv-embed: Improved techniques for\ntraining llms as generalist embedding models. arXiv\npreprint arXiv:2405.17428.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua\nLi, Bowen Li, Bailin Wang, Bowen Qin, Ruiying\nGeng, Nan Huo, et al. 2024a. Can llm already serve\nas a database interface? a big bench for large-scale\ndatabase grounded text-to-sqls. Advances in Neural\nInformation Processing Systems, 36.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023a. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nXiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia,\nYichun Yin, Hao Zhang, Yong Liu, Yasheng Wang,\nand Ruiming Tang. 2024b. Coir: A comprehensive\nbenchmark for code information retrieval models.\narXiv preprint arXiv:2407.02883.\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\nPengjun Xie, and Meishan Zhang. 2023b. Towards\ngeneral text embeddings with multi-stage contrastive\nlearning. arXiv preprint arXiv:2308.03281.\nShangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,\nand Yang Liu. Retrieval-augmented generation for\ncode summarization via hybrid gnn. In International\nConference on Learning Representations.\nTianyang Liu, Canwen Xu, and Julian McAuley. 2024.\nRepobench: Benchmarking repository-level code\nauto-completion systems. In The Twelfth Interna-\ntional Conference on Learning Representations.\nYe Liu, Kazuma Hashimoto, Yingbo Zhou, Semih\nYavuz, Caiming Xiong, and S Yu Philip. 2021. Dense\nhierarchical retrieval for open-domain question an-\nswering. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, pages 188–200.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Fed-\nerico Cassano, Joel Lamy-Poirier, Nouamane Tazi,\nAo Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei,\net al. 2024. Starcoder 2 and the stack v2: The next\ngeneration. arXiv preprint arXiv:2402.19173.\n10\n\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, et al.\nCodexglue: A machine learning benchmark dataset\nfor code understanding and generation. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 1).\nJorge Martinez-Gil. 2024. Source code clone detection\nusing unsupervised similarity measures. In Interna-\ntional Conference on Software Quality, pages 21–37.\nSpringer.\nRui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming\nXiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfr-\nembedding-2: Advanced text embedding with multi-\nstage training.\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\nXiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfr-\nembedding-mistral: Enhance text retrieval with trans-\nfer learning. Salesforce AI Research Blog.\nGabriel de Souza P Moreira, Radek Osmulski, Mengyao\nXu, Ronay Ak, Benedikt Schifferer, and Even\nOldridge. 2024. Nv-retriever: Improving text em-\nbedding models with effective hard-negative mining.\narXiv preprint arXiv:2407.15831.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-\nvio Savarese, and Yingbo Zhou. 2023. Codegen2:\nLessons for training llms on programming and natu-\nral languages. arXiv preprint arXiv:2305.02309.\nRangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna,\nDivya Sankar, Lambert Pouguem Wassi, Michele\nMerler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha,\nand Reyhaneh Jabbarvand. 2024. Lost in transla-\ntion: A study of bugs introduced by large language\nmodels while translating code. In Proceedings of the\nIEEE/ACM 46th International Conference on Soft-\nware Engineering, pages 1–13.\nMd Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,\nBaishakhi Ray, and Kai-Wei Chang. 2021. Retrieval\naugmented code generation and summarization. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 2719–2734.\nHuy N Phan, Hoang N Phan, Tien N Nguyen, and\nNghi DQ Bui. 2024. Repohyper: Better context re-\ntrieval is all you need for repository-level code com-\npletion. arXiv preprint arXiv:2403.06095.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAnkita Nandkishor Sontakke, Manasi Patwardhan,\nLovekesh Vig, Raveendra Kumar Medicherla, Ravin-\ndra Naik, and Gautam Shroff. 2022. Code summa-\nrization: Do transformers really understand code? In\nDeep Learning for Code Workshop.\nWeisong Sun, Chunrong Fang, Yuchen Chen, Quanjun\nZhang, Guanhong Tao, Yudu You, Tingxu Han, Yifei\nGe, Yuling Hu, Bin Luo, et al. 2024. An extractive-\nand-abstractive framework for source code summa-\nrization. ACM Transactions on Software Engineering\nand Methodology, 33(3):1–39.\nCodeGemma Team. 2024.\nCodegemma: Open\ncode models based on gemma.\narXiv preprint\narXiv:2406.11409.\nGemma Team, Thomas Mesnard, Cassidy Hardin,\nRobert Dadashi, Surya Bhupatiraju, Shreya Pathak,\nLaurent Sifre, Morgane Rivière, Mihir Sanjay Kale,\nJuliette Love, et al. 2024. Gemma: Open models\nbased on gemini research and technology.\narXiv\npreprint arXiv:2403.08295.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. Beir: A het-\nerogeneous benchmark for zero-shot evaluation of\ninformation retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2023. Improving\ntext embeddings with large language models. arXiv\npreprint arXiv:2401.00368.\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, and Tie-\nYan Liu. 2013. A theoretical analysis of ndcg type\nranking measures. In Conference on learning theory,\npages 25–54. PMLR.\nZora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu,\nFrank F Xu, Yiqing Xie, Graham Neubig, and Daniel\nFried. 2024. Coderag-bench: Can retrieval augment\ncode generation? arXiv preprint arXiv:2406.14497.\nMichel Wermelinger. 2023. Using github copilot to\nsolve simple programming problems. In Proceedings\nof the 54th ACM Technical Symposium on Computer\nScience Education V. 1, pages 172–178.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighof. 2023. C-pack: Packaged resources to\nadvance general chinese embedding. arXiv preprint\narXiv:2309.07597.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of the 47th International ACM SIGIR\n11\n\nConference on Research and Development in Infor-\nmation Retrieval, pages 641–649.\nJohn Yang, Carlos E Jimenez, Alexander Wettig, Kil-\nian Lieret, Shunyu Yao, Karthik Narasimhan, and\nOfir Press. 2024. Swe-agent: Agent-computer inter-\nfaces enable automated software engineering. arXiv\npreprint arXiv:2405.15793.\nBurak Yeti¸stiren, I¸sık Özsoy, Miray Ayerdem, and Eray\nTüzün. 2023.\nEvaluating the code quality of ai-\nassisted code generation tools: An empirical study on\ngithub copilot, amazon codewhisperer, and chatgpt.\narXiv preprint arXiv:2304.10778.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin\nLiu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. 2023. Repocoder: Repository-level\ncode completion through iterative retrieval and gen-\neration. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2471–2484.\nShuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang,\nand Graham Neubig. 2023. Docprompting: Gener-\nating code by retrieving the docs. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nA\nAppendix\nA.1\nDataset Details\nOur training dataset contains a total of 3.36M train-\ning pairs, covering 12 different programming lan-\nguages. As shown in Figure 3, the distribution of\nprogramming languages is imbalanced, with the\nmajority of the data concentrated in a few popular\nlanguages. Python represents the largest portion of\nthe dataset at 27.1%, followed by Go with 25.2%,\nand JavaScript and PHP at 17.0% and 17.2%, re-\nspectively. The remaining languages, including\nJava, SQL, Ruby, and others, account for smaller\nproportions, with Rust, Kotlin, and C# making up\nthe smallest shares of the dataset.\nA.2\nImplementation Details\nWe summarize the hyperparameters in Table 8. For\nthe code training data, we prepend a prompt to the\nquery in the format: “Instruct: Given Code or Text,\nretrieve relevant content. Query:”.\n12\n\nPython\nJava\nC++\nGo\nC\nJavascript\nPHP\nC#\nRuby\nRust\nKotlin\nSQL\nProgramming Languages\n100,000\n1,000,000\nSize of the Training pairs under each lang\n1089K\n528K\n482K\n308K\n282K\n204K\n192K\n109K\n51K\n50K\n49K\n17K\nPopularity of Programming Languages (Log Scale)\nPython\n32.4%\nJava\n15.7%\nC\n8.4%\nC#\n3.2%\nC++\n14.3%\nGo\n9.2%\nJavascript\n6.1%\nPHP\n5.7%\nOthers\n5.0%\nFigure 3: The programming language distribution of code training data in the general training stage.\nCodeXEmbed400M\nCodeXEmbed2B\nCodeXEmbed7B\nbase model\ngte-large-en-v1.5\ngemma-2-2b-it\nMistral-7B-Instruct-v0.3\nmax learning rate\n5 × 10−5\n5 × 10−5\n5 × 10−5\nGradCache\n4\n16\n32\ntuning parameters\nFully Model\nLoRA\nLoRA\nLora Rank\n-\n8\n8\nwarmup steps\n100\n50\n50\nbatch size\n1024\n1024\n1024\nmax length\n512\n512\n512\nweight decay\n0.01\n0.01\n0.01\nhard negatives\n7\n7\n7\nbidirectional attention\n✓\n✓\n✓\ntext:code ratio\n1:1\n1:3\n1:3\npooling\nbos\neos\neos\nTable 8: Hyperparameters for contrastive code and text training\n13",
    "pdf_filename": "CodeXEmbed_A_Generalist_Embedding_Model_Family_for_Multiligual_and_Multi-task_Code_Retrieval.pdf"
}