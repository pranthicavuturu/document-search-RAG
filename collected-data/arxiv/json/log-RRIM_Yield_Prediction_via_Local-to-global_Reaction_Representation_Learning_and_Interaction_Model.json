{
    "title": "log-RRIM Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Model",
    "context": "",
    "body": "log-RRIM: Yield Prediction via Local-to-global Reaction Representation\nLearning and Interaction Modeling\nXiao Hu1, Ziqi Chen1, Bo Peng1, Daniel Adu-Ampratwum2, Xia Ning1,2,3,4 B\n1Computer Science and Engineering, The Ohio State University, Columbus, OH 43210. 2Division of Medicinal Chem-\nistry and Pharmacognosy, College of Pharmacy, The Ohio State University, Columbus, Ohio 43210. 3Biomedical In-\nformatics, The Ohio State University, Columbus, OH 43210. 4Translational Data Analytics Institute, The Ohio State\nUniversity, Columbus, OH, 43210. Bning.104@osu.edu\nAccurate prediction of chemical reaction yields is crucial for optimizing organic synthesis, potentially reduc-\ning time and resources spent on experimentation. With the rise of artificial intelligence (AI), there is growing\ninterest in leveraging AI-based methods to accelerate yield predictions without conducting in vitro experiments.\nWe present log-RRIM, an innovative graph transformer-based framework designed for predicting chemical reac-\ntion yields. Our approach implements a unique local-to-global reaction representation learning strategy. This\napproach initially captures detailed molecule-level information and then models and aggregates intermolecular\ninteractions, ensuring that the impact of varying-sizes molecular fragments on yield is accurately accounted\nfor. Another key feature of log-RRIM is its integration of a cross-attention mechanism that focuses on the inter-\nplay between reagents and reaction centers. This design reflects a fundamental principle in chemical reactions:\nthe crucial role of reagents in influencing bond-breaking and formation processes, which ultimately affect reac-\ntion yields. log-RRIM outperforms existing methods in our experiments, especially for medium to high-yielding\nreactions, proving its reliability as a predictor. Its advanced modeling of reactant-reagent interactions and sen-\nsitivity to small molecular fragments make it a valuable tool for reaction planning and optimization in chemical\nsynthesis. The data and codes of log-RRIM are accessible through https://github.com/ninglab/Yield log RRIM.\nChemical yield prediction is crucial for optimizing organic synthesis, offering chemists an efficient tool to identify high-\nyielding reactions while reducing time and resource expenditure.1 Traditionally, chemists have relied on expertise and\nsystematic experimentation to optimize reactions.2 While foundational, these methods can become resource-intensive\nwhen scaling up.3 Consequently, there is an increasing interest in developing artificial intelligence (AI)-based meth-\nods.4–10 These AI-based methods allow chemists to accelerate precise yield prediction without doing in vitro experiments,\npotentially enhancing the efficiency of organic synthesis optimization. Despite the importance of the task, AI-based com-\nputational methods have received comparatively little attention in yield prediction compared to other chemistry-related\ntasks (e.g. forward prediction,11,12 retrosynthesis13,14). We aim to bridge the gap and introduce novel and effective AI\nmethods for yield prediction.\nEarly AI-based methods focused on identifying effective chemical knowledge-based reaction descriptors15,16 and em-\nploying traditional machine learning models17,18 over such descriptors for chemical yield prediction. However, these\nmethods often produce unsatisfactory results, suggesting the limitation of the chemical knowledge-based descriptors, as\nwell as the companion traditional machine learning models. The advent of language models19,20 has enabled sequence-\nbased approaches for chemistry-related tasks.4–8 These models are typically pre-trained on large molecular datasets21\nusing SMILES22 representations and then fine-tuned on specific datasets for yield prediction with the entire reaction’s\nSMILES string as input. However, this pre-training and fine-tuning framework may not be optimal for chemistry-specific\ntasks like yield prediction,4,6 as it lacks features that account for unique characteristics of yield prediction, such as explicit\nmodeling of reactant-reagent interactions. Moreover, these models, using the entire reaction as input, tend to overlook the\ncontributions of small yet influential molecular fragments,23 as their attention mechanisms may not be sensitive enough\nto focus on these critical elements. Additionally, building such pre-trained foundation models is resource-intensive. In con-\ntrast to the sequence-based models, graph neural networks (GNNs) have recently been employed to represent molecules\nand reactions as graphs, learning molecular structural information for yield prediction.9,10 This approach allows for a\nmore intuitive representation of molecular structure compared to sequence-based models. However, most GNN-based\nmethods lack effective modeling of molecular interactions. This limitation is particularly significant in yield prediction,\nas the interactions between reactants and reagents, like catalysts, can substantially impact reaction outcomes.24,25\nTo address these challenges, we introduce log-RRIM: a graph transformer-based local-to-global reaction representation\nlearning and interaction modeling for yield prediction. log-RRIM employs a local-to-global graph transformer-based re-\naction representation learning process, which first learns representations at the molecule level for each component indi-\nvidually and then models their interactions. This information is then aggregated, ensuring a more balanced attention\nmechanism that considers molecules of all sizes, preventing small fragments from being overlooked in the whole reaction\nfor yield prediction. Additionally, log-RRIM incorporates a cross-attention mechanism between the reagents and reaction\ncenter atoms to simulate a principle of chemical reactions: reagents have a huge impact on the bond-breaking and for-\nmation of the reaction, thus affecting the yield changes. This design more effectively captures the interactions between\nmolecules (reactants and reagents), thereby improving the prediction accuracy.\n1\narXiv:2411.03320v3  [q-bio.BM]  19 Nov 2024\n\nPerformance evaluation on the commonly investigated datasets6,26,27 demonstrates log-RRIM’s superior prediction\naccuracy, particularly for medium to high-yielding reactions. This suggests its potential for enhancing reaction yield\noptimization accuracy in practical synthetic chemistry. Our analyses further reveal log-RRIM’s effectiveness in capturing\ncomplex molecular (reactant-reagent) interactions and accurately assessing small molecular fragments’ contributions to\nyield. These capabilities highlight log-RRIM’s potential for optimizing synthetic routes through informed modifications of\nreactants and reagents, providing chemists with a sophisticated instrument for reaction design and optimization.\nRelated Work\nReaction yield prediction has evolved primarily through three types of approaches, each addressing the challenges of\nrepresenting complex molecular structures and modeling their interactions in different ways. The approaches started\nwith traditional machine learning models based on chemical knowledge-based descriptors. Next, sequence-based models\nwere developed, representing each molecule as a SMILES string. These models are typically pre-trained on large molecule\ndatasets to learn general molecule representations and then fine-tuned specifically for yield prediction tasks. Most recently,\ngraph-based models have emerged as a powerful tool for learning molecular structures, treating molecules as graphs, and\naggregating molecular information for prediction.\nTraditional Machine Learning Models\nEarly approaches to yield prediction utilized traditional machine learning models, such as random forest (RF)18 and\nsupport vector machine (SVM),17 to predict yields. These models relied on chemical knowledge-based descriptors to depict\nthe molecule properties, which include density functional theory calculations,15,16 one-hot encoding ,28 and fingerprint\nfeatures .29 These methods were primarily evaluated on reaction datasets containing a single reaction class.26,30 However,\nthey often demonstrated unsatisfactory performance.15,16,28,29 This highlighted two main limitations. First, the modeling\nability of traditional machine learning methods is insufficient for this complex problem. Second, relying solely on pre-\ndefined chemical descriptors for constructing reaction representations is inadequate. The suboptimal results obtained from\nthese methods suggest that more sophisticated and effective approaches are needed to capture the complex information\nbetween molecular structures and reaction yields.\nSequence-based Models\nTransformer-based models have recently gained prominence in chemical tasks.4–8 These models are typically pre-trained\non large molecular datasets represented by SMILES strings, learning general molecular representations. They are then\nfine-tuned on specific datasets containing yield information for the prediction. During fine-tuning, the models learn to\nprocess the SMILES string of the entire reaction as input, enabling them to capture relationships between all reaction\ncomponents. For example, Schwaller et al. introduced YieldBERT,4 which employs the SMILES string of a whole reaction\nas input to a BERT-based yield predictor.20 This BERT-based yield predictor is obtained from fine-tuning a yield\nregression head layer on a reaction encoder.31 Similarly, Lu and Zhang developed T5Chem,6 utilizing the Text-to-Text\nTransfer Transformer (T5) model.19 T5Chem, pre-trained on the PubChem dataset,21 is designed for multiple reaction\nprediction tasks (e.g., product prediction, retrosynthesis) and employs a fine-tuned regression head for yield prediction\npurposes. The sophisticated sequence modeling techniques enable these methods to learn more informative reaction\nrepresentation than handcrafted chemical knowledge-based descriptors by capturing contextual information embedded\nin the SMILES string of the entire reactions. Consequently, they demonstrate commendable prediction performance on\ndatasets containing a single reaction class.\nHowever, the efficacy diminishes when testing on datasets with a wide variety of reaction types and diverse substances,\nsuch as the US Patent database (USPTO).32 Additionally, treating the whole reaction as input makes it challenging for\nthe sequence-based models to distinguish the effects of different components in a reaction, as reactants and reagents have\ndistinct impacts on yield. Also, small modifications in the molecules, even those involving only a few fragments (atoms,\nfunctional groups, or small-size molecules), can significantly affect reaction outcomes.23 When sequence-based models\ntreat the entire reactions as inputs, they tend to overlook the contributions of those small yet influential fragments. This\noccurs because the attention mechanisms used in these sequence-based models may not be sufficiently sensitive to those\ncritical fragments, potentially leading to inaccurate predictions.\nTo address these challenges, we propose to apply a local-to-global learning process to ensure equal attention is\nallocated to molecules of varying sizes. The local-to-global learning process treats each reactant, reagent, and product\nseparately before interacting and aggregating their information, intuitively depicting the role of different components\nin the reaction. This prevents the model from ignoring the impact of small fragments. Our experiment and analysis\ndemonstrate the effectiveness of our modeling design.\nGraph-based Models\nRecent advancements have established graph neural networks (GNNs) as powerful tools for analyzing molecules and pre-\ndicting reaction yields.9,10,33–36 These approaches represent chemical structures as graphs, using GNNs to learn structural\ninformation and typically employing multilayer perceptrons (MLPs) to predict yields after aggregating molecular infor-\nmation into vector representations. Saebi et al. developed YieldGNN,9 which uses Weisfeiler-Lehman networks (WLNs)37\nto aggregate atom and bond features over their neighborhood and finally obtain the high-order structural information.\nThese learned structural features and the selected chemical knowledge-based reaction descriptors are then combined to\npredict the reaction yield through a linear layer. Their results highlight the importance of learned molecular structural\n2\n\nfeatures over the chemical descriptors. Yarish et al. introduced RD-MPNN,36 which first uses directed message passing\nnetworks (D-MPNN)33 to generate atom and bond embeddings from reactant and product graphs. Then, it creates the\nchemical transformation encoding according to the atom and bond mapping between the reactants and the products,\nwhich is combined with pre-computed molecular descriptors to predict the yield. Li et al. proposed SEMG-MIGNN,10 which\nsimilarly employs a GNN to update atom features and obtain molecule representations. Then, it applies an attention\nmechanism based on all involved components to model the molecular interplays and derive the reaction representation\nfor prediction.\nWhile these graph-based methods demonstrate satisfactory performance on datasets of a single reaction class, they\nhave not been extensively tested on more challenging datasets like USPTO. Furthermore, these approaches exhibit certain\nlimitations in molecular interaction design. RD-MPNN and YieldGNN lack explicit modeling of interactions among reac-\ntants and reagents, while SEMG-MIGNN’s design may not effectively capture the full complexity of molecular interactions.\nTo address these limitations and better enable the model to learn the interactions between reactants and reagents,\nwe propose to explicitly characterize the function of reagents on the reaction center. This approach uses a cross-attention\nmechanism38 to capture the complex interplay between different reaction components (reactants and reagents) more\neffectively, potentially leading to improved yield predictions. Our experiments and analysis demonstrate that this design\nimproves the effectiveness of molecular interaction modeling.\nMaterials\nDatasets\nUSPTO500MT Dataset\nUSPTO500MT is derived from USTPO-TPL31 by the authors of T5Chem.6 USTPO-TPL comprises 445,000 reactions,\nwith yield reported, partitioned into 1,000 strongly imbalanced reaction types. USPTO500MT is obtained by extracting\nthe top 500 most frequently occurring reaction types from USPTO-TPL. It consists of 116,360 reactions for training,\n12,937 reactions for validation, and 14,238 reactions for testing purposes. The reactants, reagents, and products are\nencoded as SMILES strings. The yield distribution is summarized in Figure 1b and the entire dataset is skewed towards\nhigh-yielding reactions. Within the USPTO500MT dataset, approximately 95.5% of the reactions (129,437) are unique.\nAdditionally, about 3.7% of the products (4,949) are documented with two distinct synthesized processes. Only a small\nfraction (0.1%) of products are synthesized through over five different processes. Moreover, the number and the function\nof reagents are varying among each reaction. These showcase the diversity and complexity of the reactions within the\ndataset.\nBuchwald–Hartwig Amination Reaction Dataset\nThe Buchwald-Hartwig dataset, constructed by Ahneman et al. ,26 has become a benchmark for assessing the performance\nof yield prediction models. This dataset comprises 3,955 palladium-catalyzed C-N cross-coupling reactions, with yields\nobtained through high-throughput experimentation (HTE). The dataset encodes information on reactants, reagents, and\nproducts as SMILES strings. It includes 15 distinct aryl halides paired with a single amine as reactants. These reactant\npairs undergo experimentation with 3 different bases, 4 Buchwald ligands, and 22 isoxazole additives, resulting in 5\ndifferent products. The yield distribution, illustrated in Figure 1a, reveals a notable skew due to a substantial proportion\nof non-yielding reactions.\nIn comparison to broader datasets such as USPTO500MT, the Buchwald-Hartwig dataset is limited to a single reaction\ntype and features a constrained set of reaction components. Moreover, reagent information is consistently organized, with\neach reaction entry containing ligand, base, and solvent information in a consistent order. While this structured format\nmay facilitate easier predictive model learning, it potentially misrepresents real-world scenarios where chemical data is\noften comprehensive and less organized. This underscores the limitation in this dataset’s ability to reflect the complexity\nand variability of practical chemical information, despite its value as a benchmark for yield prediction models.\n(a) Buchwald–Hartwig reactions yield distribution\n(b) USPTO500MT reactions yield distribution\nFig. 1 | Overview reactions yield distributions of the two datasets\n3\n\nTraining data generation\nBasic atom features\nWe follow Maziarka et al.39 and employ the open-source RDKit toolkit to extract the basic chemical features for atoms in\nmolecules represented by SMILES strings. The basic atom features utilized in log-RRIM are delineated in Table 1. These\nfeatures describe the basic chemical properties and environment, serving as the input of log-RRIMb.\nTable 1 | Basic atom features used in log-RRIM\nIndices\nDescription\n0-11\nAtom type of B, N, C, O, F, P, S, CL, BR, I, Dummy, Other (One-hot encoded)\n12-17\nNumber of connected heavy atoms of 0, 1, 2, 3, 4, 5 (One-hot encoded)\n18-22\nNumber of connected hydrogen of 0, 1, 2, 3, 4 (One-hot encoded)\n23-25\nFormal charge of -1, 0, 1 (One-hot encoded)\n26\nIf the atom is in a ring (Binary)\n27\nIf it is aromatic (Binary)\nLearned atom representations from pre-trained models\nTo investigate the impact of atom features chosen on log-RRIM, we employ two approaches: one using the basic atom\nfeatures directly, and another using learned atom representations derived from a pre-trained model MAT by Maziarka\net al..39 We name the log-RRIM trained on basic atom features as log-RRIMb, and the version trained on learned atom\nrepresentations as log-RRIMl. The pre-trained model MAT takes the basic atom features as input and utilizes node-level\nself-supervised learning 40 on a subset of 2 million molecules from the Zinc15 dataset 41 for molecule representation\nlearning. These learned atom representations are then input for log-RRIMl, potentially capturing more complex atomic\nrelations and information. The hyperparameters of the pre-trained model are delineated in Table A8 and remain consistent\nacross all experiments.\nReaction center identification\nIdentifying reaction centers is crucial for log-RRIM as it allows us to pinpoint the specific atoms involved in the chemical\ntransformation. We follow GraphRetro’s42 approach to identify these reaction center atoms by comparing the changed\nbonds between the mapped reactant and product molecules. In log-RRIM, we model the interactions between these\nreaction centers and reagents, which enables us to more effectively capture the key information (reagents have an impact\non bond-breaking and formation) that influences the reaction yield, potentially improving the accuracy of predictions.\nExperimental setting\nFor the USPTO500MT dataset, we adopt the training, validation, and testing split used by T5Chem. We adhere to the\ndata-splitting protocol for the Buchwald-Hartwig dataset as YieldGNN, using 10-fold 70/30 random train/test splits. We\nfurther allocate 10% of the training data for validation. After determining the optimal hyperparameters using three data\nsplits, we apply the model across all ten data splits and compare its performance against other baselines. In addition, we\nexclude reactions that cannot be processed by the reaction center identification method. The reaction center identification\nprocess ensures that all reactions in the dataset have well-defined reaction centers and identifiable mechanistic pathways,\nwhich is critical for accurate modeling of reaction mechanisms and yield predictions. While this process does not filter out\nany reactions in the Buchwald–Hartwig dataset, it results in 78,201 reactions filtered out for training, 8,716 for validation,\nand 9,497 for testing in USPTO500MT. This curation enhances the overall integrity of USPTO500MT, allowing for more\nprecise reactions to be considered. All performance comparisons are conducted on these curated datasets to maintain\nconsistency in our evaluations.\nModel evaluation\nWe use mean absolute error (MAE) and root mean squared error (RMSE) for evaluation purposes. Their calculations are\ngiven by the following equations:\nMAE =\nPN\ni=1 |yi −ˆyi|\nN\n,\n(1)\nRMSE =\nsPN\ni=1 (yi −ˆyi)2\nN\n,\n(2)\nwhere ˆyi is the predicted yield, yi is the ground-truth yield, and N is the number of samples. The smaller the MAE and\nRMSE are, the more accurate the yield predictor model is. Previous methods6,26 use the coefficient of determination (R2)\nto evaluate the goodness of fit of the regression model, which is defined as follows:\nR2 = 1 −\nPN\ni=1 (yi −ˆyi)2\nPN\ni=1 (yi −¯y)2 ,\n(3)\nwhere ¯y is the mean of N ground-truth yields and a larger value of R2 implies a better goodness of fit of the models.\nHowever, R2 is not an ideal metric to evaluate the accuracy and relationship, as it has several limitations.43 One significant\nissue is that R2 can be heavily influenced by outliers, potentially giving a distorted view of the model’s overall fit. This\n4\n\nsensitivity means that a few extreme error predictions can lead to a very low R2, even if the majority of predictions\nare accurate. Therefore, it is challenging to draw definitive conclusions from R2, especially when it is low. While we still\npresent the results in R2 in line with the literature, the evaluation is primarily via MAE and RMSE.\nExperiment results\nPerformance on the USPTO500MT dataset\nOverall performance\nTable 2 presents the performance comparison of log-RRIMb, log-RRIMl, and baseline methods YieldBERT and T5Chem\non the USPTO500MT dataset. log-RRIMl demonstrates the best performance in terms of MAE and RMSE, achieving\nTable 2 | Model performance comparison on USPTO500MT\nMethod\nMAE\nRMSE\nR2\nYieldBERT\n0.191\n0.245\n0.090\nT5Chem\n0.190\n0.249\n0.212\nlog-RRIMb\n0.181\n0.228\n0.122\nlog-RRIMl\n0.179\n0.226\n0.144\nThe best performance is highlighted in bold.\nthe lowest MAE of 0.179 and RMSE of 0.226. These results represent statistically significant improvements of 5.8% on\nMAE over the previous best-performing method T5Chem. The statistical significance of this improvement is underscored\nby a p-value of 5e-12 at a significance level of 5%, obtained from a paired t-test comparing the Absolute Errors (AE) of\nlog-RRIMl and T5Chem (Unless otherwise specified, the p-values mentioned in the following paper are all derived from\nthis paired t-test).\nlog-RRIMb, which utilizes the basic atom features in contrast to log-RRIMl utilizing the learned atom representations,\nachieved comparable results to log-RRIMl with an MAE of 0.181. log-RRIMb is still significantly better than T5Chem (p-\nvalue = 1e-8). We attribute the superior performance of log-RRIM to its effective framework design, specifically engineered\nto model and learn fundamental factors influencing reaction yield. The local-to-global learning scheme employed by\nlog-RRIM allows for equal attention to all molecules of varying sizes before modeling their interactions, preventing the\noversight of the contributions from small yet influential fragments (e.g., atoms, functional groups, or small molecules).\nThis approach contrasts with sequence-based models like T5Chem and YieldBERT, which treat the entire reaction as input,\nwhere the attention mechanisms may not be sufficiently sensitive to critical fragments. Furthermore, log-RRIM’s molecular\ninteraction design explicitly models the function of reagents on reaction centers, more closely mimicking the synthetic\nreaction principle: reagents like catalysts have a huge impact on bond-breaking and formation. This targeted design\nis more effective than T5Chem and YieldBERT’s interaction modeling, which indiscriminately applies global attention\nto all atoms. It is also worth noting that log-RRIM is pre-training-free, whereas T5Chem and YieldBERT are based on\nfoundation models pre-trained on extensive molecule datasets (e.g. 97 million molecules from PubChem21). log-RRIM’s\nsuperior performance suggests that pre-training may not be necessary if the training dataset is sufficiently large (e.g.,\n78K for USPTO500MT) when the reactions are modeled in a targeted and explicit way. By incorporating more effective\ndesigns, log-RRIM achieves better performance while saving huge resources required for pre-training.\nlog-RRIMb and log-RRIMl exhibit nearly identical performance, with MAE values of 0.181 and 0.179, respectively. The\nformer employs basic atom features, while the latter utilizes atom representations derived from the pre-trained MAT\nmodel.39 The incorporation of learned representations does not obtain a substantial improvement in yield prediction\naccuracy over basic features. This outcome suggests that the atom representations acquired through the MAT model,\nwhich was originally developed for general molecule representation learning39, lack the specificity required for reaction-\noriented tasks. Although basic atom features only provide elementary information about molecular properties, our findings\nunderscore that the key to enhancing yield prediction accuracy lies in more sophisticated and effective modeling of\nintermolecular interactions.\nWhile other graph-based yield prediction methods9,10,36 exist, they are primarily designed for datasets with fixed\nreagent structures, such as the Buchwald-Hartwig dataset, which includes very specific reagent information (additive,\nbase, solvent, and ligand).26 However, these methods do not apply to the USPTO500MT dataset used in this study due to\nits varying number of reagents across reactions and lack of standardized reagent information. However, the USPTO500MT\ndataset more closely resembles real-world scenarios where reaction compositions are not strictly structured. In this context,\nlog-RRIM, T5Chem, and YieldBERT demonstrate greater potential for practical applications compared to the graph-based\nmethods just mentioned. log-RRIM’s superior performance among those methods, as demonstrated in the previous results,\ncombined with its flexibility in handling diverse inputs, positions it as a promising approach for accurate yield prediction\nin practical usage.\nPerformance comparison over different yield ranges\nTo gain deeper insights into the performance differences between log-RRIMb and T5Chem, we conducted a detailed analysis\nof predictions across various yield ranges. Figure 2 visualizes these comparisons, with stacked asterisks indicating the\nlevel of statistical significance of the performance difference across yield ranges (see Table A1 for exact values). Figure 2a\nshows that log-RRIMb outperforms T5Chem in predicting yields within the 40% to 100% with t-test p-values all less than\n0.05, indicating statistical significance at the 5% level. This pattern suggests that log-RRIMb is a more reliable predictor\nfor medium to high-yielding reactions, a crucial advantage in practical synthesis scenarios.44,45 Also, Figure 2b suggests\n5\n\n(a) Performance on reactions within each yield range\n(b) Performance on reactions within cumulative yield range\nFig. 2 | Performance comparison of log-RRIMb and T5Chem across yield ranges on the USPTO500MT testing set. Left y-axis: MAE of predicted\nyields. Right y-axis: percentage of reactions in the testing set for each yield range. 5% significance level: * for p-values < 0.05, ** for p-values < 0.005,\n*** for p-values < 0.0005.\nthe overall prediction performance of log-RRIMb is significantly better. This improved overall accuracy is particularly\nvaluable in the context of exploring new reactions, where precise yield data may not be available for reference. In\nsuch scenarios, log-RRIMb’s overall more reliable predictions can offer more accurate guidance for reaction planning\nand optimization. However, for reaction yields below 40%, log-RRIMb exhibits inferior performance than T5Chem. We\nattributed this to T5Chem’s leveraging of foundation models pre-trained on extensive molecule datasets, compensating\nfor a potential shortage of training samples encountered by log-RRIMb on reactions with yields below 40% (18.1% of\nthe training set). Nevertheless, log-RRIMb remains the preferred choice for chemists seeking reliable yield predictions,\nparticularly for medium to high-yielding reactions or when no preliminary reaction yield data can be referred to. This\nreliability can significantly aid chemists in experimental planning, reducing the number of optimization iterations and\nminimizing resource consumption.\nEffectiveness in reactant-reagent interactions modeling\nTo assess the model’s capacity to capture the influence of molecular interactions on yield, specifically how reactants and\nreagents affect each other in the context of a reaction, we conducted two analyses on the testing set of USPTO500MT.\nFirst, we identified 76 reaction pairs (152 reactions) with identical reactants but different reagents and yields. This setup\nallowed us to evaluate how our method is sensitive to the effects of reagents on yields. In this context, ”interactions”\nrefer to how the introduction of different reagents influences the reaction outcome with the same reactants. log-RRIMb\nachieved a prediction MAE of 0.145, outperforming T5Chem’s 0.182. Furthermore, log-RRIMb correctly predicted the yield\ndifference (how much the yield increases or decreases) in 62% (47 out of 76) of reaction pairs, compared to T5Chem’s\n38%. This suggests that log-RRIMb is more sensitive to reagent changes and their effects on yield. Case 1 in Figure 4\nillustrates this: in two identical aryl nitration reactions, adding ether as a solvent increases the ground-truth yield from\n42.0% to 57.7%. log-RRIMb correctly predicts this upward trend, while T5Chem does not. This shows log-RRIMb’s ability\nto capture how the addition of a solvent (ether) interacts with the existing reactants to influence the yield.\nSecondly, we examined 3,698 reactions grouped into 619 sets, each containing two or more reactions with identical\nreagents but different reactants. This analysis aimed to evaluate the models’ ability to predict yields when the same\nreagents interact with various reactants. Here, ”interactions” refer to how the same set of reagents behaves differently\nwith varying reactants. log-RRIMb exhibited more accurate predictions in 58% of sets (357 out of 619), with a lower MAE\nof 0.147 compared to T5Chem’s 0.222. Case 2 in Figure 4 demonstrates log-RRIMb’s consistently more accurate predictions\nwhen the same reagents (carbon disulfide and bromine) interact with two different reactants. This indicates log-RRIMb’s\nenhanced capability to learn and model specific reagent functions across different reaction contexts, capturing how the\nsame reagents behave differently with varying reactants.\nOverall, These analyses suggest that log-RRIMb is more sensitive to changes in reactant-reagent combinations, indi-\ncating better modeling of their interactions. This enhanced capability makes log-RRIMb a potential aid for chemists in\nselecting and optimizing reactants or reagents during synthesis planning. We attribute this superiority to log-RRIMb’s\nexplicit modeling of reagent function to reaction centersd. This approach, implemented through a cross-attention mech-\nanism, aligns with fundamental reaction principles. It allows log-RRIMb to directly model how reagents influence the\nreaction center, providing a more nuanced understanding of the reaction process. An ablation study on the removal\nof explicit reagent function modeling, provided in Table A5, further supports this design choice. As a result, log-RRIMb\ndemonstrates an enhanced ability to capture and interpret complex reactant-reagent interactions, leading to more accurate\nyield predictions across diverse reaction component combinations.\nSensitivity to small fragments modifications\nTo evaluate the models’ ability to capture the influence of involved small fragments on reaction yields, we conducted a\ncomparative analysis of their performance on similar reactions with small differences only on a few small fragments in\n6\n\nreactants or reagents. Given the absence of a standardized method for quantifying reaction similarity, we propose a novel\nsimilarity metric Sim(Xi, Xj) between reactions Xi and Xj, defined as the average of reactant and reagent similarities:\nSim(Xi, Xj) = 1\n2 [s (Ri, Rj) + s (Ai, Aj)]\n(4)\nwhere X : R\nA\n−→P refers to the reaction, R and A are the concatenation of all reactants and reagents in the reaction,\nrespectively. s(·, ·) is the Tanimoto coefficient between the two chemical structures of Morgan fingerprint.46\nFig. 3 | Model performance on reaction pairs categorized by similarity. The left y-axis displays the number of reaction pairs on a logarithmic scale.\nGrey bars indicate the number of reaction pairs within each similarity range. Green bars represent the number of reaction pairs where log-RRIMb\npredicts more accurately than T5Chem. The right y-axis shows the percentage of reaction pairs with more accurate predictions by log-RRIMb relative to\nthe total number of reactions in each similarity range, as depicted by the red line.\nWe evaluated reaction pairs across a range of similarity thresholds (0.8-0.95), comparing the performance of log-RRIMb\nand T5Chem in predicting yield differences between the two reactions in the pair. The results are illustrated in Figure 3.\nSpecifically, for reaction pairs with Sim ≥0.80 (1526 pairs, 3052 reactions), log-RRIMb outperformed T5Chem on 53%\n(813/1526) pairs, with overall MAEs of 0.158 and 0.159 respectively. This advantage becomes more pronounced as the\nreaction similarity increases. On 221 pairs with Sim ≥0.9, log-RRIMb surpassed T5Chem on 56% (123/221), with MAEs\nof 0.162 and 0.165 respectively. The trend culminated with highly similar reaction pairs (Sim ≥0.95, 24 pairs), where\nlog-RRIMb demonstrated marked superiority, outperforming T5Chem on 71% (17/24), with MAEs of 0.150 and 0.170\nrespectively. These results reveal a clear trend: log-RRIMb’s accuracy in capturing yield differences improves as reaction\nsimilarity increases. This indicates that log-RRIMb exhibits enhanced sensitivity to subtle component changes that impact\nreaction yields, particularly for highly similar reactions.\nThe capability is also demonstrated in several cases. In Figure 4 case 3, the two reactions differ only in their ortho-\nsubstitution (methoxy vs fluoro group), resulting in a yield decrease from 68.2% to 48.9%. log-RRIM correctly predicts\nthis change, while T5Chem incorrectly predicts the opposite trend. Similarly, case 4 in Figure 4 presents two alkylations\nof hydroxyquinoline with different alkylating agents. The ground-truth yield changes minimally in this situation, which\nlog-RRIMb correctly predicts, whereas T5Chem makes an erroneous prediction. These results indicate that the log-RRIMb\nexcels in predicting yield changes triggered by those small modifications in atoms, functional groups, or small molecules\nin reactants or reagents. This capability is essential for optimizing reactions in complex chemical systems, where small\nadjustments to reactants and reagents can significantly impact yields. log-RRIMb’s precision in predicting the effects of\nthese subtle changes enhances its utility for guiding synthetic strategies and fine-tuning reactions. By offering reliable\nforecasts for small modifications, log-RRIMb can potentially streamline the optimization process, reducing the number of\nexperimental iterations required and saving time and resources in research and industrial settings.\nThis capability stems from log-RRIMb’s unique local-to-global learning strategy. By first analyzing each molecule\nseparately and then modeling their interactions, the model ensures equal consideration of all molecules, regardless of\ntheir size. This approach differs from sequence-based models like T5Chem, which process the entire reaction SMILES\nstring simultaneously. Such models may overlook crucial smaller fragments that significantly impact the overall yield, as\nthe global attention mechanisms might not be sufficiently sensitive to these critical molecular fragments.\nOverall, the performance differences presented in these analyses underscore our belief that model frameworks should\nbe carefully designed based on specific task characteristics rather than solely relying on foundation models. While they\nhave shown great promise in many areas, a basic fine-tuning strategy may not always be optimal for specialized tasks like\nreaction yield prediction. Such an approach lacks task-specific module designs that capture the intricate characteristics\nof chemical reactions, potentially limiting the performance.\n7\n\nCase 1\nCase 2\nCase 3\nCase 4\nReactants\nReagents\nProduct\nYield\nGround-truth: 42.0%\nT5Chem: 83.5% (𝐴𝐸= 41.5%)\nlog-RRIMb: 58.2% (𝐴𝐸= 16.2%)\nGround-truth: 57.7% (∆= +15.7%)\nT5Chem: 74.6% (𝐴𝐸= 16.9%, ∆= −8.9%)\nlog-RRIMb: 65.6% (𝐴𝐸= 7.9%, ∆= +7.4%)\nGround-truth: 85.1%\nT5Chem: 66.3% (𝐴𝐸= 18.8%)\nlog-RRIMb: 70.4% (𝐴𝐸= 14.7%)\nGround-truth: 57.1% (∆= −28.6%)\nT5Chem: 80.6% (𝐴𝐸= 23.5%, ∆= +14.3%)\nlog-RRIMb: 58.4% (𝐴𝐸= 1.3%, ∆= −12.0%)\nGround-truth: 68.2%\nT5Chem: 62.9% (𝐴𝐸= 5.3%)\nlog-RRIMb: 73.8% (𝐴𝐸= 5.6%)\nGround-truth: 46.9% (∆= −21.3%)\nT5Chem: 66.8% (𝐴𝐸= 19.9%, ∆= +3.9%)\nlog-RRIMb: 64.7% (𝐴𝐸= 17.8%, ∆= −9.1%)\nGround-truth: 53.9%\nT5Chem: 44.0% (𝐴𝐸= 9.9%)\nlog-RRIMb: 53.9% (𝐴𝐸= 0.0%)\nGround-truth: 52.2% (∆= −1.7%)\nT5Chem: 36.0% (𝐴𝐸= 16.2%, ∆= −8.0%)\nlog-RRIMb: 53.3% (𝐴𝐸= 1.1%, ∆= −0.6%)\nFig. 4 | Cases analysis on the USPTO500MT dataset. Each reaction is reported with reactants, reagents, products, and the ground-truth and\npredicted yields by T5Chem and log-RRIMb. AE in parentheses represents the Absolute Error between the predicted and ground-truth yields. ∆in\nparentheses represents the change of the ground-truth and predicted yields in the second reaction to the corresponding value in the first reaction.\nPerformance on the external dataset CJHIF\nTo assess our model’s performance on external datasets, we conducted an evaluation using a subset of the CJHIF\ndataset.27 This approach involves using models trained on USPTO500MT and testing them on a subset of the CJHIF\ndataset, which comprises 3,219,165 reactions sourced from high-impact factor journals. Our assessment involved 1,000\nzero-yielding chemical reactions randomly selected from the initial 50,000 reactions in the CJHIF dataset. We specifically\nchose reactions with reported non-zero yields because CJHIF treats unreported yields as zeros, and we aimed to evaluate\nour model on reactions with confirmed, measurable outcomes. Importantly, these 1,000 reactions are not included in the\ntraining or testing data of USPTO500MT, thus providing an independent testing set for assessing our model’s performance\non external reactions.\nOverall, log-RRIMb achieved an MAE of 0.149, representing a 16.8% improvement over T5Chem’s MAE of 0.179.\nThe results of analyzing performance across yield ranges are illustrated in Figure 5. log-RRIMb significantly outperformed\nT5Chem for reactions with yields between 60% to 100% (confidence level 95%, more details are provided in Table A2). This\nsuperior performance aligns closely with our observations from the USPTO500MT dataset, particularly in log-RRIMb’s\nenhanced accuracy for medium to high-yielding reactions, which suggests that log-RRIMb’s improved predictive power\nfor high-yielding reactions is a generalizable feature, not limited to a specific dataset. We attribute this generalizability\nto log-RRIMb’s molecular interaction design which uses the cross-attention mechanism to effectively model the function\nof reagents in relation to the reaction center. This allows log-RRIMb to learn fundamental principles about how reagents\nimpact bond-breaking and formation, which are key factors affecting reaction yield. The extensive data in USPTO500MT\ntraining data enables log-RRIMb to learn such principles to achieve better test performance on external datasets.\nTo further validate that log-RRIM has effectively learned key factors influencing reaction yield, we visualized the\ncontribution (weight) of each atom when log-RRIMb aggregates atom embeddings and constructs the molecule representa-\ntion. Three exemplar reactions are shown in Figure 6. In reaction A, a sulfonylation reaction, the sulfur-bearing sulfonyl\nchloride group on the p-toluenesulfonyl chloride and the free hydroxyl (OH) group on the alcohol are the two reacting\ncenters. The oxygen (O) acts as the nucleophile that displaces the chlorine (Cl) atom, and these atoms influence the yield\nof the reaction. Reaction B is an imine reduction reaction of the compound N-(4-methoxyphenyl)-1-phenylethylamine.\nThe polar C=N bond between the Nitrogen (N) and Carbon (C) is the reactive site, and these two atoms influence the\nyield of the reaction, which results in the single C-N bond in the corresponding amine. In reaction C, the two atoms that\nultimately influence the yield are Sulfur (S) of benzene sulfonyl chloride and Nitrogen (N) of the indole, producing the\nfinal compound. Combined with the weights highlighted by the colormap in Figure 6, we found that the atoms mentioned\nabove that have a greater impact on yield are given higher weights by log-RRIMb, and these atoms are also the atoms in\nthe reaction center. This finding aligns with the fundamental chemical principle that reaction center atoms play a crucial\nrole in the bond-breaking and bond-forming steps in the transition state, thereby exerting substantial influence on the\n8\n\nFig. 5 | log-RRIMb and T5Chem performance comparison over each yield range on a subset of CJHIF. Left y-axis: MAE of predicted yields. Right\ny-axis: percentage of reactions in the testing set for each yield range. 5% significance level: * for p-values < 0.05, ** for p-values < 0.005, *** for\np-values < 0.0005.\nyield. The ability of log-RRIMb to prioritize these critical atoms in learning the molecule representation is essential for\nbuilding more accurate models for predicting reaction yield, and our method demonstrates particular effectiveness in this\nregard.\nA\nB\nC\nCc1ccc(cc1)S(Cl)(=O)=O.OCCOCCOCCOCCN1C(=O)c2ccccc2C1=O>>Cc1ccc(cc1)S(=O)(=O)OCCOCCOCCOCCN1C(=O)c2ccccc2C1=O\nGround-truth: 91.0%   log-RRIMb prediction: 66.8%   T5Chem prediction: 19.2%\nCOc1ccc(cc1)\\N=C(/C)c1ccccc1>>COc1ccc(NC(C)c2ccccc2)cc1\nGround-truth: 99.0%   log-RRIMb prediction: 83.7%   T5Chem prediction: 71.6%\nc1cc2ccccc2[nH]1.ClS(=O)(=O)c1ccccc1>>O=S(=O)(c1ccccc1)n1ccc2ccccc12\nGround-truth: 84.0%   log-RRIMb prediction: 83.3%   T5Chem prediction: 62.3%\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nAtomic Weight\nFig. 6 | Visualizations of atom contribution in learning molecule representation. The contribution is quantified by the color and the three exemplar\nreactions are selected from the CJHIF dataset.\nPerformance on the Buchwald-Hartwig dataset\nOn the Buchwald-Hartwig dataset, we conducted a performance comparison among pre-training-free models (YieldGNN,\nSEMG-MIGNN, and RD-MPNN), using 10-fold cross-validation,47 and reported the testing results averaged over the 10\nfolds. Table 3 reports the mean and standard deviation (in parentheses) of MAE, RMSE, and R2. Table A4 provides a\ndetailed comparison of testing MAE values for different models on each fold. Our method, log-RRIMb, outperforms other\npre-training-free (also graph-based) methods across all evaluation metrics (MAE 0.0348, RMSE 0.0544, and R2 0.953).\nNotably, it achieves a 14.7% improvement in MAE over the best-performing baseline, YieldGNN. We attribute log-RRIMb’s\nsuperior performance to its more effective molecular interaction design, explicitly modeling the reagents’ function to the\nreaction center. By incorporating this design, log-RRIMb captures crucial chemical insights that other methods may\noverlook, leading to more accurate predictions. Compared to the second-best baseline method, SEMG-MIGNN, log-RRIMb\nimproves the MAE by 17.9%. To put this improvement in context, it’s worth recalling that SEMG-MIGNN focuses on\nbuilding more informative atom features (digitalized steric and electronic information). In contrast, log-RRIMb emphasizes\nlearning the characteristics of the reaction itself and molecular interactions. The performance difference between these\napproaches suggests that for yield prediction tasks, the latter strategy may be more effective. In summary, log-RRIMb\n9\n\noutperforms other pre-training-free and graph-based models substantially. These results demonstrate the importance of\nfocusing on reaction characteristics and molecular interactions in yield prediction tasks.\nAs shown in Table 4, when compared to the pre-training-based methods, which are also sequence-based models\n(T5Chem and YieldBERT), log-RRIMl also shows competitive performance by achieving the MAE of 0.0347, which has a\n16.4% improvement over YieldBERT but inferior to T5Chem by 11.6%. These results indicate our method is comparable\nto the best-performing sequence-based model T5Chem while using only 2% of the pre-training dataset size compared to\nT5Chem (2M vs 97M). To further validate this comparison, we conducted statistical analysis on the predicted values of\nlog-RRIMl and T5Chem for each yield range on the first data split (with detailed p-values shown in Table A3). The analysis\nshows that, for all ranges except the 10%-20%, 40%-50%, and 70%-80% ranges (a total of 27.6% of the test reactions), the\ndifferences between log-RRIMl and T5Chem are not statistically significant at the 95% confidence interval. This indicates\nthat the performance of log-RRIMl and T5Chem is largely comparable across most yield ranges. Note that the Buchwald-\nHartwig dataset involves only a single reaction type with limited components. On this specific reaction type, log-RRIM\ncould underperform T5Chem. However, real-world chemical synthesis generally involves reactions of multiple types. Thus,\nmethods that could accurately predict yields of various types are highly demanded. As shown on the USPTO500MT and\nCJHIF datasets, which contain numerous reaction types, our method demonstrates superior performance. On these more\ndiverse and complex datasets, log-RRIMb outperforms T5Chem. These results demonstrate the potential superior utility\nof log-RRIMb over T5Chem in real-world chemical synthesis applications.\nWe also note that the performance of log-RRIMb and log-RRIMl are nearly identical (MAE 0.0348 vs 0.0347) on the\nBuchwald-Hartwig dataset. This observation aligns with the results we obtained on the USPTO500MT dataset. These\nconsistent findings across different datasets suggest that effective molecular interaction modeling may play a more crucial\nrole than using pre-trained models to generate informative atom representations in yield prediction tasks.\nTable 3 | Pre-training-free models performance comparison on the Buchwald-Hartwig dataset\nMethod\nMetrics\nMAE\nRMSE\nR2\nRD-MPNN\n0.0746(0.005)\n0.1040(0.007)\n0.854(0.018)\nSEMG-MIGNN\n0.0424(0.001)\n0.0605(0.002)\n0.951(0.004)\nYieldGNN\n0.0408(0.002)\n0.0575(0.002)\n0.956(0.003)\nlog-RRIMb\n0.0348(0.002)\n0.0544(0.004)\n0.953(0.009)\nEach value is the mean and standard deviation (in parentheses), averaging\n10 folds. The best performance is highlighted in bold.\nTable 4 | Pre-training-based models performance comparison on the Buchwald-Hartwig dataset\nMethod\nMetrics\nMAE\nRMSE\nR2\nYieldBERT\n0.0415(0.001)\n0.0641(0.005)\n0.945(0.008)\nT5Chem\n0.0311(0.001)\n0.0482(0.002)\n0.971(0.002)\nlog-RRIMl\n0.0347(0.001)\n0.0528(0.003)\n0.957(0.006)\nEach value is the mean and standard deviation (in parentheses), averaging\n10 folds. The best performance is highlighted in bold.\nDiscussion\nIn conclusion, in this paper, we present log-RRIM, a novel graph-transformer-based reaction representation learning\nframework for yield prediction. log-RRIM leverages a local-to-global representation learning process and incorporates a\ncross-attention mechanism to model reagent-reaction center interactions, facilitating improved capture of small fragment\ncontributions and interactions between reactant and reagent molecules. This approach allows log-RRIM to tap into crucial\naspects of chemical knowledge, particularly the importance of reagent effects and reaction center dynamics in determining\nreaction outcomes. Without reliance on pre-training tasks, log-RRIM demonstrates superior accuracy and effectiveness\ncompared to other graph-based methods and state-of-the-art sequence-based approaches, particularly for medium to\nhigh-yielding reactions. Our analyses further show log-RRIM’s advanced modeling of reactant-reagent interactions and\nsensitivity to small molecular fragments, making it a valuable asset for reaction planning and optimization in chemical\nsynthesis.\nThe log-RRIM framework requires that predicted reactions consist of three parts (reactant, reagent, and product) and\nthat reaction center atoms be correctly identifiable. While this may limit its practical applications in some scenarios, it\nenables log-RRIM to more effectively model the crucial intermolecular dynamics that significantly influence reaction out-\ncomes. This approach underscores the importance of incorporating chemical-specific information into model architecture\ndesign, rather than directly adapting general-purpose foundation models for chemical tasks like yield prediction.\nWhile log-RRIM makes significant strides in leveraging chemical knowledge, particularly in modeling reagent-reaction\ncenter interactions, there remains a vast body of chemical expertise that could potentially be incorporated to further\nenhance the performance. For instance, research has elucidated detailed mechanisms for different reaction types, like\ntransition states,48,49 which are not yet explicitly incorporated into our model. Furthermore, chemists have developed a\ndeep understanding of the relative reactivity of different functional groups under various conditions,50,51 which represents\nanother rich source of knowledge that could be integrated into the model. Incorporating such additional aspects of\nchemical knowledge presents both a challenge and an opportunity for future research. It could potentially enhance the\n10\n\nFig. 7 | Performance comparison of log-RRIMl and T5Chem across yield ranges on the first data split of the Buchwald-Hartwig dataset. Left y-axis:\nMAE of predicted yields. Right y-axis: percentage of reactions in the testing set for each yield range. 5% significance level: * for p-values < 0.05, **\nfor p-values < 0.005, *** for p-values < 0.0005.\nmodel’s predictive power, improve its generalization to diverse reaction types, and provide more interpretable insights\ninto the factors driving yield predictions. Another promising direction for future research is the exploration of multi-task\nlearning approaches, where the model could be trained simultaneously on yield prediction, reaction condition optimization,\nretrosynthesis planning, etc. This could lead to a more comprehensive understanding of chemical reactivity and potentially\nimprove performance across all tasks.\nlog-RRIM represents a significant step forward in reaction yield prediction by leveraging graph-based representations\nand modeling reagent-reaction center interactions, and there is still room for further integration of chemical knowledge\nand enhancement of the model’s capabilities. By continuing to merge data-driven techniques with established chemical\nprinciples, it is crucial to develop more robust, versatile, and reliable models for computational chemistry.\nData Availability\nThe data used in this manuscript is made publicly available at https://github.com/ninglab/Yield log RRIM.\nCode Availability\nThe code for log-RRIM is made publicly at https://github.com/ninglab/Yield log RRIM.\nAcknowledgements\nThis project was made possible, in part, by support from the National Science Foundation grant no. IIS-2133650 (X.N.)\nand the National Library of Medicine grant no. 1R01LM014385-01 (X.N., D.A.). Any opinions, findings and conclusions\nor recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the\nfunding agency.\n11\n\nMethod\nOur method, log-RRIM, is a novel local-to-global graph-transformer-based reaction representation learning and molecular\ninteraction modeling for yield prediction. It employs a local-to-global learning process for reaction representation learning,\nbeginning with molecule (reactants, reagents, and product) representation learning. It subsequently models the molecule\ninteractions (between reactants and reagents) and ultimately represents the entire reaction. log-RRIM further uses the\nreaction representation to predict yield.\nSpecifically, log-RRIM consists of the following three modules: (1) Molecule Representation Learning (MRL) mod-\nule: which uses graph transformers39 with multi-head self-attention layers to encode molecular structural information\ninto atom embeddings, and then aggregate atom embeddings into molecule embeddings through Atomic Integration\n(AI). (2) Molecule Interaction (MIT) module: which learns the interactions between reactants and reagents through\nthe cross-attention mechanism, resulting in interaction-aware embeddings for reaction centers. (3) Reaction Information\nAggregation (RIA) module: which employs Molecular Integration (MI) to derive a comprehensive reaction representation\nfrom all involved molecules and their interaction representations. Finally, this reaction representation is utilized to predict\nthe yield. An overview of log-RRIM is depicted in Figure 8.\nGT2\nGT1\nGT2\nGT3\nYield\nPredictor\nReactant 1 (ℳ1\n𝑅)\nCross\nAttention\nAI\nGT1\nAI\nAI\nAI\nAI\nMI\nMI\nMI\nGT1\nGT2\nGT3\nAI\nMI\nReactants Graph Transformer\nReagents Graph Transformer\nProducts Graph Transformer\nAtomic Integration\nMolecular Integration\nReactant 2 (ℳ2\n𝑅)\nReagent 1 (ℳ1\n𝐴)\nReagent 2 (ℳ2\n𝐴)\nProduct 1 (ℳ1\n𝑃)\n𝑀1\n𝑅\n𝑀2\n𝑅\n𝑀1\n𝐴\n𝑀2\n𝐴\n𝑀1\n𝑃\n𝐶\nሚ𝐶\n෪\n𝑀1\n𝑅\n෪\n𝑀2\n𝑅\n𝐡1\n𝑅\n𝐫\n𝐚\n𝐩\n𝐱\nMolecule Interaction\nMolecular Representation Learning\nReaction Information Aggregation\n𝐡2\n𝑅\n𝐡1\n𝐴\n𝐡2\n𝐴\n𝐡1\n𝑃\nFig. 8 | Pipeline of log-RRIM\nNotations\nIn a reaction X, each reactant, reagent, and product is a molecule. We view each molecule M as a graph, with basic node\n(atom) features I ∈Rn×s, graph adjacent matrix J ∈{0, 1}n×n, and inter-atomic distance matrix D ∈Rn×n, where n is\nthe number of atoms in the molecule and can be different for each molecule, s is the dimension of basic atom features. The\nreaction X is represented as (R, A, P, y), where R =\n\b\nMR\n1 , . . . , MR\nnr\n\t\n, A =\n\b\nMA\n1 , . . . , MA\nna\n\t\n, and P =\nn\nMP\n1 , . . . , MP\nnp\no\nare the set of nr reactants, na reagents and np products in the reaction, and y is the reaction yield. nr, na, and np can be\ndifferent for each reaction. Notably, we denote the reaction center atom embeddings in reactants as C ∈R|C|×d, where\n|C| refers to the number of reaction center atoms.\nIn MRL module, the atom embeddings of each molecule after the l ∈[1, nl]-th self-attention layer is denoted as\nM(l) ∈Rn×d, where nl is the number of self-attention layers used in MRL and d is the model dimension.\nhm ∈\nn\nhR\n1 , . . . , hR\nnr, hA\n1 , . . . , hA\nna, hP\n1 , . . . , hP\nnp\no\nis a d-dimension vector and denoted as the representation of each\nmolecule in reactants, reagents, and products. The reactant, reagent, and product representations are named as r ∈Rd,\na ∈Rd and p ∈Rd. The representation of the whole reaction is x and the predicted yield is denoted by ˆy. We summarize\nthe key notations in Table 5. We use uppercase letters to denote matrices, lowercase bold letters to denote row vectors,\nand lower-case non-bold letters to represent scalars.\nMolecule Representation Learning (MRL)\nGiven a reaction consisting of molecules represented in graphs, we first employ the Molecule Attention Transformer\n(MAT) 39 to learn the molecule representations. MAT recursively propagates information between atoms to learn the\n12\n\nTable 5 | Key Notations\nNotation\nDescription\nX\nReaction\nx\nReaction representation\ny, ˆy\nGround truth reaction yield and the predicted yield\nR, A, P\nReactant, reagent and product\nr, a, p\nReactant, reagent, and product representation\nnr, na, np\nNumber of molecules in the reactant, reagent, and product\nM\nMolecule\nn\nNumber of atoms of the molecule\nM\nAtom embeddings of the molecule\nhm\nMolecule representation\nstructural information of molecules via multi-head molecule self-attention layers as follows:\nQl\nj = M(l −1)El\nj, Kl\nj = M(l −1)F l\nj, V l\nj = M(l −1)Gl\nj,\nHEADl\nj =\n \nλaρ\n \nQl\nj\n\u0000Kl\nj\n\u0001T\n√\nd\n!\n+ λdρ(D) + λgJ\n!\nV l\nj ,\nHl =\nh\nHEADl\n1, HEADl\n2, . . . , HEADl\nnh\ni\nOl,\n(5)\nwhere M(l −1) is the atom embeddings from the (l −1)-th molecule self-attention layer and M(0) = I is the input of first\nlayer ; Ql\nj, Kl\nj, and V l\nj are the query, key, and value matrix derived from M(l −1) with learnable parameters El\nj, F l\nj and\nGl\nj ∈Rd× d\nnh ; λa, λd, λg are the scalars to balance the importance of the self-attention, distance matrix, and adjacency\nmatrix, and ρ is the softmax function. Each molecule attention layer has nh heads and HEADl\nj is the output from the\nj-th attention head; Ol ∈Rd×d is a learnable matrix to integrate the attention heads. After each molecule self-attention\nlayer, MAT includes a feed-forward layer to introduce non-linearity which is a fully connected network (FCN) described\nbelow:\nM(l) = σ\n\u0000HlW l + Bl\u0001\n,\n(6)\nwhere W l ∈Rd×d and Bl ∈Rn×d are learnable parameters, σ(·) is ReLU52 activation function. After nl molecule self-\nattention layers, the molecule’s structural information is encoded into the atom embeddings M(nl). When no ambiguity\narises, for simplicity, we eliminate nl for M(nl) and only use M to represent atom embeddings of molecule M as the\noutput of the last molecule self-attention layer.\nCompared to the original Transformer,38 MAT integrates the interactions among atoms, the geometric information in\nthe molecule, and the topology of the molecule to better learn expressive atom embeddings, and captures the structural\ninformation of the molecule. Given the atom embeddings M learned from MAT, we utilize the Atomic Integration(AI)\nmodule to aggregate atom embeddings and generate the molecule representation hm. Particularly, AI uses a gating\nmechanism to capture the importance of different atoms in the aggregation as follows:\nα = Mw1,\nhm =\nn\nX\nk=1\n[M]k × [α]k,\n(7)\nwhere w1 ∈Rd is a learnable vector and α is the vector where each element represents the contribution of each atom\nembedding to the molecule representation.\nAdditionally, in the MRL module, AI is only performed on reagents and products to get their molecule representations\nand is omitted for reactants. This is because the reaction center atom embeddings in the reactants will undergo further\nupdates in the Molecule Interaction(MIT) module. The reactant molecule representations will be obtained through AI\nafterward.\nMolecule Interaction (MIT)\nReagents, such as catalysts, significantly impact reaction yield by promoting or inhibiting bond breaking and formation.\nWe explicitly model their function to the reaction center atoms to better capture the interaction between reactants and\nreagents. Specifically, given the reaction center atom embeddings C ∈R|C|×d in reactant molecules, and the reagent\nmolecule representations hA\ni ∈\n\b\nhA\n1 , . . . , hA\nna\n\t\n, we update the reaction center atom embeddings by applying a multi-head\n13\n\ncross-attention mechanism, described as follows:\nQj = CW Q\nj , Kj = HAW K\nj , Vj = HAW V\nj ,\nHEADj = ρ\n \nQj (Kj)T\n√\nd\n!\nVj,\nH = [HEAD1, HEAD2, . . . , HEADnh] O,\n(8)\nwhere Qj is the linear projection of reaction center atom embeddings C; Kj, and Vj are the linear projection of the\nconcatenated molecule representations of reagents HA = [hA\n1 , . . . , hA\nna] ∈Rna×d. A cross attention layer has nh attention\nheads and HEADj is the output from j-th attention head, O ∈Rd×d is a learnable parameter to integrate the attention\nheads. The updated reaction center atom embeddings ˜C are obtained by passing H to an FCN:\n˜C = σ (HW c + Bc) ,\n(9)\nwhere W c ∈Rd×d and Bc ∈R|C|×d are learnable parameters. After updating the reaction center atom embeddings in\nthe reactants, we use AI to derive the reactant molecule representations hR\ni ∈\n\b\nhR\n1 , . . . , hR\nnr\n\t\n.\nMIT uses a cross-attention layer to transform and integrate reagent information into the reaction center atoms, enabling\nthe model to consider relationships between various reaction components. This makes log-RRIM learn a more chemically\nmeaningful reaction representation by emphasizing reaction centers and reagent interactions. We further show and analyze\nthe benefits that MIT brings to log-RRIM in Table A5, demonstrating its contribution to the overall performance of our\nmodel.\nReaction Information Aggregation (RIA)\nAfter the derivation of representations for all the molecules involved in reactants, reagents, and products, we introduce\nRIA to aggregate all the molecular information. This module explicitly describes the interaction of the involved molecules\nin the reaction and their contribution to yield.\nSpecifically, given the reactant molecule representations hR\ni ∈\n\b\nhR\n1 , . . . , hR\nnr\n\t\n, reagent molecule representations hA\ni ∈\n\b\nhA\n1 , . . . , hA\nna\n\t\n, and the product molecule representations hP\ni ∈\nn\nhP\n1 , . . . , hP\nnp\no\n, we first apply MI to respectively derive\nthree representations r, a, and p for reactant, reagent, and product. MI uses a gating mechanism to aggregate the\ninformation from involved molecules. Taking reactant molecules as an example, this process can be described as follows:\nβi = ⟨hR\ni , w2⟩\nr =\nnr\nX\ni=1\nhR\ni × βi,\n(10)\nwhere w2 ∈Rd is a learnable vector to map each molecule representation to its weight βi. This step allows log-RRIM to\ncapture the collective properties within each group of molecules, providing a more compact and informative representation\nfor subsequent processing. These three representations are then concatenated to form a comprehensive representation of\nthe entire reaction x = [r, a, p] ∈R3d. x then serves as the input for the yield predictor.\nRIA processes reactant, reagent, and product molecules separately and aggregates information hierarchically to achieve\na nuanced representation of the reaction. This design allows log-RRIM to capture each component’s unique role and\ncontribution to the reaction process, leading to a nuanced overall representation.\nYield Predictor\nProvided with the comprehensive reaction representation x, we stack two FCNs to predict the yield ˆy. The process is\ndescribed below:\nˆy = (x) = f (σ (xW3 + b1) w4 + b2) ,\n(11)\nwhere W3 ∈R3d×d, w4 ∈Rd, b1 ∈Rd and b2 ∈R1 are learnable parameters, and f(·) is a sigmoid function to control\nthe predicted yield within the range [0%, 100%].\nModel training and hyperparameters optimization\nDuring training, the mean absolute error (MAE) loss is optimized using adaptive moment estimation (Adam).53\nMAE =\nPN\ni=1 |yi −ˆyi|\nN\n(12)\nThe initial learning rate is treated as a hyperparameter. Additionally, we utilize the validation set to schedule the learning\nrate decay patience and decay factor required in lr scheduler.ReduceLROnPlateau provided by PyTorch.54 All the searched\nhyperparameters and their respective search ranges are summarized in Table A6 and Table A7, respectively.\n14\n\nAppendix\nExact paired t-test p-values between log-RRIM and T5Chem on each dataset\nThe tables below include all the MAE differences and p-values of the paired t-tests between log-RRIMb and T5Chem.\nTable A1 | log-RRIMb and T5Chem performance comparison across different yield ranges on USPTO500MT\nYield range\n[0%,10%]\n[10%,20%]\n[20%,30%]\n[30%,40%]\n[40%,50%]\n[50%,60%]\n[60%,70%]\n[70%,80%]\n[80%,90%]\n[90%,100%]\nMAE difference (log-RRIMb- T5Chem)\n0.104\n0.095\n0.051\n0.032\n-0.020\n-0.030\n-0.048\n-0.050\n-0.021\n0.008\nT-test p-values\n1e-10\n2e-17\n2e-1\n1e-6\n5e-5\n3e-12\n7e-40\n3e-40\n6e-8\n3e-2\nCumulative yield range\n[90%, 100%] [80%, 100%]\n[70%, 100%] [60%, 100%] [50%, 100%] [40%, 100%] [30%, 100%] [20%, 100%] [10%, 100%]\n[0%, 100%]\nMAE difference (log-RRIMb- T5Chem)\n0.008\n-0.006\n-0.018\n-0.024\n-0.025\n-0.024\n-0.020\n-0.015\n-0.011\n-0.009\nT-test p-values\n3e-2\n3e-2\n4e-17\n3e-37\n2e-46\n3e-50\n4e-35\n1e-22\n4e-12\n1e-8\nTable A2 | log-RRIMb and T5Chem performance comparison across different yield range on 1,000 sampled non-zero-yielding reactions from CJHIF\nYield range\n(0%,10%]\n(10%,20%]\n(20%,30%]\n(30%,40%] (40%,50%]\n(50%,60%] (60%,70%]\n(70%,80%] (80%,90%]\n(90%,100%]\nMAE difference (log-RRIMb- T5Chem)\n0.215\n0.140\n0.043\n0.059\n-0.004\n0.030\n-0.040\n-0.086\n-0.045\n-0.016\nT-test p-values\n2e-1\n5e-2\n4e-1\n3e-1\n9e-1\n2e-1\n8e-3\n3e-14\n4e-7\n4e-2\nTable A3 | log-RRIMl and T5Chem performance comparison across different yield ranges on the first data split of Buchwald-Hartwig dataset\nYield range\n[0%,10%] (10%,20%]\n(20%,30%]\n(30%,40%]\n(40%,50%]\n(50%,60%]\n(60%,70%] (70%,80%]\n(80%,90%]\n(90%,100%]\nMAE difference (log-RRIMl- T5Chem)\n0.0027\n0.0089\n-0.0027\n-0.0039\n0.0082\n0.0007\n0.0073\n0.0223\n0.0028\n0.0030\nT-test p-values\n6e-2\n1e-2\n5e-1\n3e-1\n3e-2\n9e-1\n2e-1\n2e-2\n5e-1\n6e-1\nPerformance on each Buchwald-Hartwig data split\nIn Table A4, we present the performance (MAE on the testing set) of each model across every data split.\nImpact of explicit reagents function modeling\nWe conducted an ablation study to investigate the importance of explicitly modeling reagent effects on reaction yield\nby removing the second module Molecular Interaction (MIT) from our proposed log-RRIMb framework. In this modified\nversion, atom embeddings of the reactant molecules are directly passed to the Atomic Interaction (AI) module to de-\nrive reactant molecule representations without using the cross-attention mechanism to update the reaction center atom\nembeddings. We compared the performance of this ablated model with the original log-RRIMb on the first data split of\nthe Buchwald-Hartwig dataset. The results are summarized in Table A5. Results show that removing the MIT module\nhugely decreased prediction performance, with the MAE of log-RRIMb increasing by 45.0%. This substantial drop in\naccuracy underscores the critical role of explicitly modeling reagent effects in yield prediction tasks. The observed per-\nformance degradation can be attributed to the MIT module’s ability to capture fundamental characteristics of chemical\nreactions, particularly the influence of substances such as catalysts on bond breaking and formation at reaction centers.\nBy incorporating this knowledge, log-RRIM enhances its capacity to construct more informative molecular and reaction\nrepresentations, ultimately making more accurate reaction yield predictions.\nHyperparameters\nTable A6 and Table A7 summarizes the searched hyperparameters and their ranges on the USPTO500MT and Buchwald-\nHartwig datasets. The selected values for log-RRIMb are highlighted by underlining, while those for log-RRIMl are indicated\nin bold. Table A8 summarizes the hyperparameters used in the pre-trained MAT model.\n15\n\nTable A4 | Model performance over each data split on Buchwald-Hartwig\nMethod\nData split\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nmean(std)\nYieldBERT\n0.0424\n0.0425\n0.0408\n0.0410\n0.0417\n0.0411\n0.0401\n0.0424\n0.0403\n0.0432\n0.0416(0.001)\nT5Chem\n0.0323 0.0311 0.0311 0.0314 0.0303 0.0297 0.0315 0.0332 0.0298 0.0309 0.0311(0.001)\nRD-MPNN\n0.0758\n0.0698\n0.0694\n0.0758\n0.0802\n0.0726\n0.0727\n0.0866\n0.0697\n0.0734\n0.0746(0.005)\nSEMG-MIGNN\n0.0440\n0.0418\n0.0397\n0.0432\n0.0439\n0.0418\n0.0433\n0.0426\n0.0410\n0.0424\n0.0424(0.001)\nYieldGNN\n0.0423\n0.0415\n0.0391\n0.0397\n0.0410\n0.0418\n0.0386\n0.0394\n0.0406\n0.0439\n0.0408(0.002)\nlog-RRIMb\n0.0338\n0.0339\n0.0331\n0.0364\n0.0360\n0.0328\n0.0344\n0.0379\n0.0344\n0.0352\n0.0348(0.002)\nlog-RRIMl\n0.0363\n0.0342\n0.0326\n0.0371\n0.0346\n0.0340\n0.0338\n0.0364\n0.0338\n0.0343\n0.0347(0.001)\nEach value is the model’s MAE on the corresponding testing set, the best performance is highlighted in bold.\nThe mean and standard deviation (in parentheses) are obtained by averaging across 10 data splits.\nTable A5 | log-RRIMb without MIT performance on the first data split of the Buchwald-Hartwig dataset\nMethod\nMetrics\nMAE\nRMSE\nR2\nlog-RRIMb without MIT\n0.0490\n0.0651\n0.931\nlog-RRIMb\n0.0338\n0.0530\n0.957\nTable A6 | Hyperparameters and their searched ranges on the USPTO500MT dataset\nName\nDescription\nRange\neb Nlayer\nThe number of pre-trained self-attention layers to initialize atom features\n0, 8\nNlayer\nThe number of self-attention layers\n4, 5, 6\nNheads\nThe number of attention heads\n16\nhs\nModel dimension\n128, 256, 512, 1024\ne\nEpoch\n35\nbs\nBatch size\n32\ninit lr\nInitial learning rate\n1e-5, 3e-5, 1e-4\nlr decay step\nLearning rate decay patience\n5, 10, 15\nlr decay factor\nLearning rate decay factor\n0.85, 0.90, 0.95\ndp\nDropout\n0, 0.1, 0.2\ngnorm\nGradient norm clipping threshold\n0.5, 1, 5, None\nwd\nWeight Decay\n0, 1e-6, 1e-5\nTable A7 | Hyperparameters and their searched ranges on the Buchwald-Hartwig dataset\nName\nDescription\nRange\neb Nlayer\nThe number of pre-trained self-attention layers to initialize atom features\n0, 8\nNlayer\nThe number of self-attention layers\n1, 2, 3, 4, 5, 6, 7, 8\nNheads\nThe number of attention heads\n16\nhs\nModel dimension\n128, 256, 512, 1024\ne\nEpoch\n300\nbs\nBatch size\n32\ninit lr\nInitial learning rate\n1e-5, 3e-4, 1e-4, 3e-4\nlr decay step\nLearning rate decay patience\n10, 15, 20\nlr decay factor\nLearning rate decay factor\n0.85, 0.90, 0.95\ndp\nDropout\n0, 0.1, 0.2\ngnorm\nGradient norm clipping threshold\n0.5, 1, 5, None\nwd\nWeight Decay\n0, 1e-6, 1e-5\nTable A8 | Hyperparameters used in the pre-trained MAT model\nName\nDescription\nValue\neb Nlayer\nThe number of pre-trained MAT self-attention layers\n8\nhs pretrain\nPre-trained MAT model dimension\n1024\nNheads pretrain\nPre-trained MAT attention heads number\n16\nNpff\nThe number of dense layers in the position-wise feed-forward block\n1\nk\nDistance matrix kernel\n’EXP’\ndp pretrain\nDropout\n0\nwd pretrain\nWeight decay\n0\nλa, λd, λg\nThe scalars weighting the naive self-attention, distance, and adjacency matrices\n0.33\n16\n\nReferences\n[1] Shields, B. J. et al. Bayesian reaction optimization as a tool for chemical synthesis. Nature 590, 89–96 (2021).\n[2] Reizman, B. J. & Jensen, K. F. An automated continuous-flow platform for the estimation of multistep reaction kinetics.\nOrganic Process Research & Development 16, 1770–1782 (2012).\n[3] Sigman, M. S., Harper, K. C., Bess, E. N. & Milo, A. The development of multidimensional analysis tools for asymmetric\ncatalysis and beyond. Accounts of chemical research 49, 1292–1301 (2016).\n[4] Schwaller, P., Vaucher, A. C., Laino, T. & Reymond, J.-L. Prediction of chemical reaction yields using deep learning. Machine\nlearning: science and technology 2, 015016 (2021).\n[5] Probst, D., Schwaller, P. & Reymond, J.-L. Reaction classification and yield prediction using the differential reaction fingerprint\ndrfp. Digital discovery 1, 91–97 (2022).\n[6] Lu, J. & Zhang, Y. Unified deep learning model for multitask reaction predictions with explanation. Journal of chemical\ninformation and modeling 62, 1376–1387 (2022).\n[7] Chen, X. et al.\nSequence-based peptide identification, generation, and property prediction with deep learning: a review.\nMolecular Systems Design & Engineering 6, 406–428 (2021).\n[8] Li, J. & Jiang, X. Mol-bert: An effective molecular representation with bert for molecular property prediction. Wireless\nCommunications and Mobile Computing 2021, 7181815 (2021).\n[9] Saebi, M. et al. On the use of real-world datasets for reaction yield prediction. Chemical science 14, 4997–5005 (2023).\n[10] Li, S.-W., Xu, L.-C., Zhang, C., Zhang, S.-Q. & Hong, X.\nReaction performance prediction with an extrapolative and\ninterpretable graph model based on chemical knowledge. Nature Communications 14, 3569 (2023).\n[11] Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science\n5, 1572–1583 (2019).\n[12] Coley, C. W. et al. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science\n10, 370–377 (2019).\n[13] Liu, B. et al. Retrosynthetic reaction prediction using neural sequence-to-sequence models. ACS central science 3, 1103–1113\n(2017).\n[14] Chen, Z., Ayinde, O. R., Fuchs, J. R., Sun, H. & Ning, X. G 2 retro as a two-step graph generative models for retrosynthesis\nprediction. Communications Chemistry 6, 102 (2023).\n[15] Kariofillis, S. K. et al. Using data science to guide aryl bromide substrate scope analysis in a ni/photoredox-catalyzed cross-\ncoupling with acetals as alcohol-derived radical sources. Journal of the American Chemical Society 144, 1045–1055 (2022).\n[16] Yada, A. et al. Machine learning approach for prediction of reaction yield with simulated catalyst parameters. Chemistry\nLetters 47, 284–287 (2018).\n[17] Cortes, C. & Vapnik, V. Support-vector networks. Machine learning 20, 273–297 (1995).\n[18] Breiman, L. Random forests. Machine learning 45, 5–32 (2001).\n[19] Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning\nresearch 21, 1–67 (2020).\n[20] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K.\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[21] Kim, S. et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research 47, D1102–D1109 (2019).\n[22] Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal\nof chemical information and computer sciences 28, 31–36 (1988).\n[23] Neel, A. J., Milo, A., Sigman, M. S. & Toste, F. D. Enantiodivergent fluorination of allylic alcohols: data set design reveals\nstructural interplay between achiral directing group and chiral anion. Journal of the American Chemical Society 138, 3863–\n3875 (2016).\n[24] Carlson, R. & Carlson, J. E. Design and optimization in organic synthesis (Elsevier, 2005).\n[25] Coley, C. W., Green, W. H. & Jensen, K. F. Machine learning in computer-aided synthesis planning. Accounts of chemical\nresearch 51, 1281–1289 (2018).\n[26] Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G. Predicting reaction performance in c–n cross-coupling\nusing machine learning. Science 360, 186–190 (2018).\n[27] Jiang, S. et al. When smiles smiles, practicality judgment and yield prediction of chemical reaction via deep chemical language\nprocessing. IEEE Access 9, 85071–85083 (2021).\n[28] Chuang, K. V. & Keiser, M. J. Comment on “predicting reaction performance in c–n cross-coupling using machine learning”.\nScience 362, eaat8603 (2018).\n[29] Sandfort, F., Strieth-Kalthoff, F., K¨uhnemund, M., Beecks, C. & Glorius, F. A structure-based platform for predicting chemical\nreactivity. Chem 6, 1379–1390 (2020).\n[30] Perera, D. et al. A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. Science\n359, 429–434 (2018).\n[31] Schwaller, P. et al. Mapping the space of chemical reactions using attention-based neural networks. Nature machine intelligence\n3, 144–152 (2021).\n[32] Lowe, D. Chemical reactions from us patents (1976-sep2016) (2017). URL ”https://figshare.com/articles/dataset/Chemical\nreactions from US patents 1976-Sep2016 /5104873”.\n[33] Yang, K. et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and\nmodeling 59, 3370–3388 (2019).\n[34] Jo, J., Kwak, B., Choi, H.-S. & Yoon, S. The message passing neural networks for chemical property prediction on smiles.\nMethods 179, 65–72 (2020).\n[35] Tang, M., Li, B. & Chen, H. Application of message passing neural networks for molecular property prediction. Current\nOpinion in Structural Biology 81, 102616 (2023).\n[36] Yarish, D. et al.\nAdvancing molecular graphs with descriptors for the prediction of chemical reaction yields.\nJournal of\nComputational Chemistry 44, 76–92 (2023).\n[37] Lei, T., Jin, W., Barzilay, R. & Jaakkola, T. Deriving neural architectures from sequence and graph kernels. In International\nConference on Machine Learning, 2024–2033 (PMLR, 2017).\n17\n\n[38] Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[39] Maziarka,  L. et al. Molecule attention transformer. arXiv preprint arXiv:2002.08264 (2020).\n[40] Hu, W. et al. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019).\n[41] Sterling, T. & Irwin, J. J. Zinc 15–ligand discovery for everyone. Journal of chemical information and modeling 55, 2324–2337\n(2015).\n[42] Somnath, V. R., Bunne, C., Coley, C. W., Krause, A. & Barzilay, R.\nLearning graph models for retrosynthesis predic-\ntion. In Thirty-Fifth Conference on Neural Information Processing Systems (2021). URL https://openreview.net/forum?id=\nSnONpXZ uQ .\n[43] Balaji, N. N. A., Beaulieu, C. L., Bogner, J. & Ning, X. Traumatic brain injury rehabilitation outcome prediction using\nmachine learning methods. Archives of Rehabilitation Research and Clinical Translation 5, 100295 (2023).\n[44] Ma, Y. et al. Are we making much progress? revisiting chemical reaction yield prediction from an imbalanced regression\nperspective. In Companion Proceedings of the ACM on Web Conference 2024, 790–793 (2024).\n[45] Kawasaki, H., Kihara, N. & Takata, T. High yielding and practical synthesis of rotaxanes by acylative end-capping catalyzed\nby tributylphosphine. Chemistry Letters 28, 1015–1016 (1999).\n[46] Morgan, H. L. The generation of a unique machine description for chemical structures-a technique developed at chemical\nabstracts service. Journal of chemical documentation 5, 107–113 (1965).\n[47] Geisser, S. The predictive sample reuse method with applications. Journal of the American statistical Association 70, 320–328\n(1975).\n[48] Carey, F. A. & Sundberg, R. J. Advanced organic chemistry: part A: structure and mechanisms (Springer Science & Business\nMedia, 2007).\n[49] Anslyn, E. Modern Physical Organic Chemistry, vol. 227 (University Science Books, 2006).\n[50] Clayden, J., Greeves, N. & Warren, S. Organic chemistry (Oxford University Press, USA, 2012).\n[51] Smith, M. B. March’s advanced organic chemistry: reactions, mechanisms, and structure (John Wiley & Sons, 2020).\n[52] Nair, V. & Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international\nconference on machine learning (ICML-10), 807–814 (2010).\n[53] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n[54] Paszke, A. et al.\nPytorch: An imperative style, high-performance deep learning library.\nAdvances in neural information\nprocessing systems 32 (2019).\n18",
    "pdf_filename": "log-RRIM_Yield_Prediction_via_Local-to-global_Reaction_Representation_Learning_and_Interaction_Model.pdf"
}