{
    "title": "log-RRIM: Yield Prediction via Local-to-global Reaction Representation",
    "abstract": "",
    "body": "log-RRIM: Yield Prediction via Local-to-global Reaction Representation\nLearning and Interaction Modeling\nXiao Hu1, Ziqi Chen1, Bo Peng1, Daniel Adu-Ampratwum2, Xia Ning1,2,3,4(cid:66)\n1Computer Science and Engineering, The Ohio State University, Columbus, OH 43210. 2Division of Medicinal Chem-\nistry and Pharmacognosy, College of Pharmacy, The Ohio State University, Columbus, Ohio 43210. 3Biomedical In-\nformatics, The Ohio State University, Columbus, OH 43210. 4Translational Data Analytics Institute, The Ohio State\nUniversity, Columbus, OH, 43210. (cid:66)ning.104@osu.edu\nAccuratepredictionofchemicalreactionyieldsiscrucialforoptimizingorganicsynthesis,potentiallyreduc-\ning time and resources spent on experimentation. With the rise of artificial intelligence (AI), there is growing\ninterestinleveragingAI-basedmethodstoaccelerateyieldpredictionswithoutconductinginvitroexperiments.\nWepresentlog-RRIM,aninnovativegraphtransformer-basedframeworkdesignedforpredictingchemicalreac-\ntion yields. Our approach implements a unique local-to-global reaction representation learning strategy. This\napproachinitiallycapturesdetailedmolecule-levelinformationandthenmodelsandaggregatesintermolecular\ninteractions, ensuring that the impact of varying-sizes molecular fragments on yield is accurately accounted\nfor.Anotherkeyfeatureoflog-RRIMisitsintegrationofacross-attentionmechanismthatfocusesontheinter-\nplaybetweenreagentsandreactioncenters.Thisdesignreflectsafundamentalprincipleinchemicalreactions:\nthecrucialroleofreagentsininfluencingbond-breakingandformationprocesses,whichultimatelyaffectreac-\ntion yields. log-RRIM outperforms existing methods in our experiments, especially for medium to high-yielding\nreactions,provingitsreliabilityasapredictor.Itsadvancedmodelingofreactant-reagentinteractionsandsen-\nsitivitytosmallmolecularfragmentsmakeitavaluabletoolforreactionplanningandoptimizationinchemical\nsynthesis.Thedataandcodesoflog-RRIMareaccessiblethroughhttps://github.com/ninglab/Yield log RRIM.\nChemicalyieldpredictioniscrucialforoptimizingorganicsynthesis,offeringchemistsanefficienttooltoidentifyhigh-\nyielding reactions while reducing time and resource expenditure.1 Traditionally, chemists have relied on expertise and\nsystematic experimentation to optimize reactions.2 While foundational, these methods can become resource-intensive\nwhen scaling up.3 Consequently, there is an increasing interest in developing artificial intelligence (AI)-based meth-\nods.4â€“10 TheseAI-basedmethodsallowchemiststoacceleratepreciseyieldpredictionwithoutdoinginvitroexperiments,\npotentially enhancing the efficiency of organic synthesis optimization. Despite the importance of the task, AI-based com-\nputational methods have received comparatively little attention in yield prediction compared to other chemistry-related\ntasks (e.g. forward prediction,11,12 retrosynthesis13,14). We aim to bridge the gap and introduce novel and effective AI\nmethods for yield prediction.\nEarly AI-based methods focused on identifying effective chemical knowledge-based reaction descriptors15,16 and em-\nploying traditional machine learning models17,18 over such descriptors for chemical yield prediction. However, these\nmethods often produce unsatisfactory results, suggesting the limitation of the chemical knowledge-based descriptors, as\nwell as the companion traditional machine learning models. The advent of language models19,20 has enabled sequence-\nbased approaches for chemistry-related tasks.4â€“8 These models are typically pre-trained on large molecular datasets21\nusing SMILES22 representations and then fine-tuned on specific datasets for yield prediction with the entire reactionâ€™s\nSMILES string as input. However, this pre-training and fine-tuning framework may not be optimal for chemistry-specific\ntaskslikeyieldprediction,4,6 asitlacksfeaturesthataccountforuniquecharacteristicsofyieldprediction,suchasexplicit\nmodelingofreactant-reagentinteractions.Moreover,thesemodels,usingtheentirereactionasinput,tendtooverlookthe\ncontributions of small yet influential molecular fragments,23 as their attention mechanisms may not be sensitive enough\ntofocusonthesecriticalelements.Additionally,buildingsuchpre-trainedfoundationmodelsisresource-intensive.Incon-\ntrast to the sequence-based models, graph neural networks (GNNs) have recently been employed to represent molecules\nand reactions as graphs, learning molecular structural information for yield prediction.9,10 This approach allows for a\nmore intuitive representation of molecular structure compared to sequence-based models. However, most GNN-based\nmethods lack effective modeling of molecular interactions. This limitation is particularly significant in yield prediction,\nas the interactions between reactants and reagents, like catalysts, can substantially impact reaction outcomes.24,25\nToaddressthesechallenges,weintroducelog-RRIM:agraphtransformer-basedlocal-to-globalreactionrepresentation\nlearning and interaction modeling for yield prediction. log-RRIM employs a local-to-global graph transformer-based re-\naction representation learning process, which first learns representations at the molecule level for each component indi-\nvidually and then models their interactions. This information is then aggregated, ensuring a more balanced attention\nmechanism that considers molecules of all sizes, preventing small fragments from being overlooked in the whole reaction\nfor yield prediction. Additionally, log-RRIM incorporates a cross-attention mechanism between the reagents and reaction\ncenter atoms to simulate a principle of chemical reactions: reagents have a huge impact on the bond-breaking and for-\nmation of the reaction, thus affecting the yield changes. This design more effectively captures the interactions between\nmolecules (reactants and reagents), thereby improving the prediction accuracy.\n1\n4202\nvoN\n91\n]MB.oib-q[\n3v02330.1142:viXra\nPerformance evaluation on the commonly investigated datasets6,26,27 demonstrates log-RRIMâ€™s superior prediction\naccuracy, particularly for medium to high-yielding reactions. This suggests its potential for enhancing reaction yield\noptimization accuracy in practical synthetic chemistry. Our analyses further reveal log-RRIMâ€™s effectiveness in capturing\ncomplex molecular (reactant-reagent) interactions and accurately assessing small molecular fragmentsâ€™ contributions to\nyield.Thesecapabilitieshighlightlog-RRIMâ€™spotentialforoptimizingsyntheticroutesthroughinformedmodificationsof\nreactants and reagents, providing chemists with a sophisticated instrument for reaction design and optimization.\nRelated Work\nReaction yield prediction has evolved primarily through three types of approaches, each addressing the challenges of\nrepresenting complex molecular structures and modeling their interactions in different ways. The approaches started\nwith traditional machine learning models based on chemical knowledge-based descriptors. Next, sequence-based models\nweredeveloped,representingeachmoleculeasaSMILESstring.Thesemodelsaretypicallypre-trainedonlargemolecule\ndatasetstolearngeneralmoleculerepresentationsandthenfine-tunedspecificallyforyieldpredictiontasks.Mostrecently,\ngraph-based models have emerged as a powerful tool for learning molecular structures, treating molecules as graphs, and\naggregating molecular information for prediction.\nTraditionalMachineLearningModels\nEarly approaches to yield prediction utilized traditional machine learning models, such as random forest (RF)18 and\nsupportvectormachine(SVM),17topredictyields.Thesemodelsreliedonchemicalknowledge-baseddescriptorstodepict\nthe molecule properties, which include density functional theory calculations,15,16 one-hot encoding ,28 and fingerprint\nfeatures.29 Thesemethodswereprimarilyevaluatedonreactiondatasetscontainingasinglereactionclass.26,30 However,\ntheyoftendemonstratedunsatisfactoryperformance.15,16,28,29 Thishighlightedtwomainlimitations.First,themodeling\nability of traditional machine learning methods is insufficient for this complex problem. Second, relying solely on pre-\ndefinedchemicaldescriptorsforconstructingreactionrepresentationsisinadequate.Thesuboptimalresultsobtainedfrom\nthese methods suggest that more sophisticated and effective approaches are needed to capture the complex information\nbetween molecular structures and reaction yields.\nSequence-basedModels\nTransformer-based models have recently gained prominence in chemical tasks.4â€“8 These models are typically pre-trained\non large molecular datasets represented by SMILES strings, learning general molecular representations. They are then\nfine-tuned on specific datasets containing yield information for the prediction. During fine-tuning, the models learn to\nprocess the SMILES string of the entire reaction as input, enabling them to capture relationships between all reaction\ncomponents.Forexample,Schwalleret al.introducedYieldBERT,4 whichemploystheSMILESstringofawholereaction\nas input to a BERT-based yield predictor.20 This BERT-based yield predictor is obtained from fine-tuning a yield\nregression head layer on a reaction encoder.31 Similarly, Lu and Zhang developed T5Chem,6 utilizing the Text-to-Text\nTransfer Transformer (T5) model.19 T5Chem, pre-trained on the PubChem dataset,21 is designed for multiple reaction\nprediction tasks (e.g., product prediction, retrosynthesis) and employs a fine-tuned regression head for yield prediction\npurposes. The sophisticated sequence modeling techniques enable these methods to learn more informative reaction\nrepresentation than handcrafted chemical knowledge-based descriptors by capturing contextual information embedded\nin the SMILES string of the entire reactions. Consequently, they demonstrate commendable prediction performance on\ndatasets containing a single reaction class.\nHowever,theefficacydiminisheswhentestingondatasetswithawidevarietyofreactiontypesanddiversesubstances,\nsuch as the US Patent database (USPTO).32 Additionally, treating the whole reaction as input makes it challenging for\nthesequence-basedmodelstodistinguishtheeffectsofdifferentcomponentsinareaction,asreactantsandreagentshave\ndistinct impacts on yield. Also, small modifications in the molecules, even those involving only a few fragments (atoms,\nfunctional groups, or small-size molecules), can significantly affect reaction outcomes.23 When sequence-based models\ntreat the entire reactions as inputs, they tend to overlook the contributions of those small yet influential fragments. This\noccurs because the attention mechanisms used in these sequence-based models may not be sufficiently sensitive to those\ncritical fragments, potentially leading to inaccurate predictions.\nTo address these challenges, we propose to apply a local-to-global learning process to ensure equal attention is\nallocated to molecules of varying sizes. The local-to-global learning process treats each reactant, reagent, and product\nseparately before interacting and aggregating their information, intuitively depicting the role of different components\nin the reaction. This prevents the model from ignoring the impact of small fragments. Our experiment and analysis\ndemonstrate the effectiveness of our modeling design.\nGraph-basedModels\nRecent advancements have established graph neural networks (GNNs) as powerful tools for analyzing molecules and pre-\ndictingreactionyields.9,10,33â€“36 Theseapproachesrepresentchemicalstructuresasgraphs,usingGNNstolearnstructural\ninformation and typically employing multilayer perceptrons (MLPs) to predict yields after aggregating molecular infor-\nmation into vector representations. Saebi et al. developed YieldGNN,9 which uses Weisfeiler-Lehman networks (WLNs)37\nto aggregate atom and bond features over their neighborhood and finally obtain the high-order structural information.\nThese learned structural features and the selected chemical knowledge-based reaction descriptors are then combined to\npredict the reaction yield through a linear layer. Their results highlight the importance of learned molecular structural\n2\nfeatures over the chemical descriptors. Yarish et al. introduced RD-MPNN,36 which first uses directed message passing\nnetworks (D-MPNN)33 to generate atom and bond embeddings from reactant and product graphs. Then, it creates the\nchemical transformation encoding according to the atom and bond mapping between the reactants and the products,\nwhichiscombinedwithpre-computedmoleculardescriptorstopredicttheyield.Liet al.proposedSEMG-MIGNN,10which\nsimilarly employs a GNN to update atom features and obtain molecule representations. Then, it applies an attention\nmechanism based on all involved components to model the molecular interplays and derive the reaction representation\nfor prediction.\nWhile these graph-based methods demonstrate satisfactory performance on datasets of a single reaction class, they\nhavenotbeenextensivelytestedonmorechallengingdatasetslikeUSPTO.Furthermore,theseapproachesexhibitcertain\nlimitations in molecular interaction design. RD-MPNN and YieldGNN lack explicit modeling of interactions among reac-\ntantsandreagents,whileSEMG-MIGNNâ€™sdesignmaynoteffectivelycapturethefullcomplexityofmolecularinteractions.\nTo address these limitations and better enable the model to learn the interactions between reactants and reagents,\nweproposetoexplicitlycharacterizethefunctionofreagentsonthereactioncenter.Thisapproachusesacross-attention\nmechanism38 to capture the complex interplay between different reaction components (reactants and reagents) more\neffectively, potentially leading to improved yield predictions. Our experiments and analysis demonstrate that this design\nimproves the effectiveness of molecular interaction modeling.\nMaterials\nDatasets\nUSPTO500MTDataset\nUSPTO500MT is derived from USTPO-TPL31 by the authors of T5Chem.6 USTPO-TPL comprises 445,000 reactions,\nwith yield reported, partitioned into 1,000 strongly imbalanced reaction types. USPTO500MT is obtained by extracting\nthe top 500 most frequently occurring reaction types from USPTO-TPL. It consists of 116,360 reactions for training,\n12,937 reactions for validation, and 14,238 reactions for testing purposes. The reactants, reagents, and products are\nencoded as SMILES strings. The yield distribution is summarized in Figure 1b and the entire dataset is skewed towards\nhigh-yielding reactions. Within the USPTO500MT dataset, approximately 95.5% of the reactions (129,437) are unique.\nAdditionally, about 3.7% of the products (4,949) are documented with two distinct synthesized processes. Only a small\nfraction (0.1%) of products are synthesized through over five different processes. Moreover, the number and the function\nof reagents are varying among each reaction. These showcase the diversity and complexity of the reactions within the\ndataset.\nBuchwaldâ€“HartwigAminationReactionDataset\nTheBuchwald-Hartwigdataset,constructedbyAhnemanet al.,26 hasbecomeabenchmarkforassessingtheperformance\nof yield prediction models. This dataset comprises 3,955 palladium-catalyzed C-N cross-coupling reactions, with yields\nobtained through high-throughput experimentation (HTE). The dataset encodes information on reactants, reagents, and\nproducts as SMILES strings. It includes 15 distinct aryl halides paired with a single amine as reactants. These reactant\npairs undergo experimentation with 3 different bases, 4 Buchwald ligands, and 22 isoxazole additives, resulting in 5\ndifferent products. The yield distribution, illustrated in Figure 1a, reveals a notable skew due to a substantial proportion\nof non-yielding reactions.\nIncomparisontobroaderdatasetssuchasUSPTO500MT,theBuchwald-Hartwigdatasetislimitedtoasinglereaction\ntypeandfeaturesaconstrainedsetofreactioncomponents.Moreover,reagentinformationisconsistentlyorganized,with\neach reaction entry containing ligand, base, and solvent information in a consistent order. While this structured format\nmay facilitate easier predictive model learning, it potentially misrepresents real-world scenarios where chemical data is\noften comprehensive and less organized. This underscores the limitation in this datasetâ€™s ability to reflect the complexity\nand variability of practical chemical information, despite its value as a benchmark for yield prediction models.\n(a) Buchwaldâ€“Hartwigreactionsyielddistribution (b) USPTO500MTreactionsyielddistribution\nFig. 1 | Overviewreactionsyielddistributionsofthetwodatasets\n3\nTrainingdatageneration\nBasicatomfeatures\nWefollowMaziarkaet al.39 andemploytheopen-sourceRDKittoolkittoextractthebasicchemicalfeaturesforatomsin\nmolecules represented by SMILES strings. The basic atom features utilized in log-RRIM are delineated in Table 1. These\nfeatures describe the basic chemical properties and environment, serving as the input of log-RRIM .\nb\nTable 1 | Basicatomfeaturesusedinlog-RRIM\nIndices Description\n0-11 AtomtypeofB,N,C,O,F,P,S,CL,BR,I,Dummy,Other(One-hotencoded)\n12-17 Numberofconnectedheavyatomsof0,1,2,3,4,5(One-hotencoded)\n18-22 Numberofconnectedhydrogenof0,1,2,3,4(One-hotencoded)\n23-25 Formalchargeof-1,0,1(One-hotencoded)\n26 Iftheatomisinaring(Binary)\n27 Ifitisaromatic(Binary)\nLearnedatomrepresentationsfrompre-trainedmodels\nTo investigate the impact of atom features chosen on log-RRIM, we employ two approaches: one using the basic atom\nfeatures directly, and another using learned atom representations derived from a pre-trained model MAT by Maziarka\net al..39 We name the log-RRIM trained on basic atom features as log-RRIM , and the version trained on learned atom\nb\nrepresentations as log-RRIM. The pre-trained model MAT takes the basic atom features as input and utilizes node-level\nl\nself-supervised learning 40 on a subset of 2 million molecules from the Zinc15 dataset 41 for molecule representation\nlearning. These learned atom representations are then input for log-RRIM, potentially capturing more complex atomic\nl\nrelationsandinformation.Thehyperparametersofthepre-trainedmodelaredelineatedinTableA8andremainconsistent\nacross all experiments.\nReactioncenteridentification\nIdentifying reaction centers is crucial for log-RRIM as it allows us to pinpoint the specific atoms involved in the chemical\ntransformation. We follow GraphRetroâ€™s42 approach to identify these reaction center atoms by comparing the changed\nbonds between the mapped reactant and product molecules. In log-RRIM, we model the interactions between these\nreaction centers and reagents, which enables us to more effectively capture the key information (reagents have an impact\non bond-breaking and formation) that influences the reaction yield, potentially improving the accuracy of predictions.\nExperimentalsetting\nFor the USPTO500MT dataset, we adopt the training, validation, and testing split used by T5Chem. We adhere to the\ndata-splitting protocol for the Buchwald-Hartwig dataset as YieldGNN, using 10-fold 70/30 random train/test splits. We\nfurther allocate 10% of the training data for validation. After determining the optimal hyperparameters using three data\nsplits, we apply the model across all ten data splits and compare its performance against other baselines. In addition, we\nexcludereactionsthatcannotbeprocessedbythereactioncenteridentificationmethod.Thereactioncenteridentification\nprocessensuresthatallreactionsinthedatasethave well-definedreactioncentersandidentifiablemechanistic pathways,\nwhichiscriticalforaccuratemodelingofreactionmechanismsandyieldpredictions.Whilethisprocessdoesnotfilterout\nanyreactionsintheBuchwaldâ€“Hartwigdataset,itresultsin78,201reactionsfilteredoutfortraining,8,716forvalidation,\nand9,497fortestinginUSPTO500MT.ThiscurationenhancestheoverallintegrityofUSPTO500MT,allowingformore\nprecise reactions to be considered. All performance comparisons are conducted on these curated datasets to maintain\nconsistency in our evaluations.\nModelevaluation\nWeusemeanabsoluteerror(MAE)androotmeansquarederror(RMSE)forevaluationpurposes.Theircalculationsare\ngiven by the following equations:\n(cid:80)N\n|y âˆ’yË†|\nMAE = i=1 i i , (1)\nN\n(cid:115)\n(cid:80)N (y âˆ’yË†)2\nRMSE = i=1 i i , (2)\nN\nwhere yË† is the predicted yield, y is the ground-truth yield, and N is the number of samples. The smaller the MAE and\ni i\nRMSEare,themoreaccuratetheyieldpredictormodelis.Previousmethods6,26 usethecoefficientofdetermination(R2)\nto evaluate the goodness of fit of the regression model, which is defined as follows:\n(cid:80)N (y âˆ’yË†)2\nR2 =1âˆ’ i=1 i i , (3)\n(cid:80)N (y âˆ’yÂ¯)2\ni=1 i\nwhere yÂ¯ is the mean of N ground-truth yields and a larger value of R2 implies a better goodness of fit of the models.\nHowever,R2isnotanidealmetrictoevaluatetheaccuracyandrelationship,asithasseverallimitations.43Onesignificant\nissue is that R2 can be heavily influenced by outliers, potentially giving a distorted view of the modelâ€™s overall fit. This\n4\nsensitivity means that a few extreme error predictions can lead to a very low R2, even if the majority of predictions\nare accurate. Therefore, it is challenging to draw definitive conclusions from R2, especially when it is low. While we still\npresent the results in R2 in line with the literature, the evaluation is primarily via MAE and RMSE.\nExperiment results\nPerformanceontheUSPTO500MTdataset\nOverallperformance\nTable 2 presents the performance comparison of log-RRIM , log-RRIM, and baseline methods YieldBERT and T5Chem\nb l\non the USPTO500MT dataset. log-RRIM demonstrates the best performance in terms of MAE and RMSE, achieving\nl\nTable 2 | ModelperformancecomparisononUSPTO500MT\nMethod MAE RMSE R2\nYieldBERT 0.191 0.245 0.090\nT5Chem 0.190 0.249 0.212\nlog-RRIMb 0.181 0.228 0.122\nlog-RRIMl 0.179 0.226 0.144\nThebestperformanceishighlightedinbold.\nthe lowest MAE of 0.179 and RMSE of 0.226. These results represent statistically significant improvements of 5.8% on\nMAE over the previous best-performing method T5Chem. The statistical significance of this improvement is underscored\nby a p-value of 5e-12 at a significance level of 5%, obtained from a paired t-test comparing the Absolute Errors (AE) of\nlog-RRIM and T5Chem (Unless otherwise specified, the p-values mentioned in the following paper are all derived from\nl\nthis paired t-test).\nlog-RRIM , which utilizes the basic atom features in contrast to log-RRIM utilizing the learned atom representations,\nb l\nachieved comparable results to log-RRIM with an MAE of 0.181. log-RRIM is still significantly better than T5Chem (p-\nl b\nvalue=1e-8).Weattributethesuperiorperformanceoflog-RRIMtoitseffectiveframeworkdesign,specificallyengineered\nto model and learn fundamental factors influencing reaction yield. The local-to-global learning scheme employed by\nlog-RRIM allows for equal attention to all molecules of varying sizes before modeling their interactions, preventing the\noversight of the contributions from small yet influential fragments (e.g., atoms, functional groups, or small molecules).\nThisapproachcontrastswithsequence-basedmodelslikeT5ChemandYieldBERT,whichtreattheentirereactionasinput,\nwheretheattentionmechanismsmaynotbesufficientlysensitivetocriticalfragments.Furthermore,log-RRIMâ€™smolecular\ninteraction design explicitly models the function of reagents on reaction centers, more closely mimicking the synthetic\nreaction principle: reagents like catalysts have a huge impact on bond-breaking and formation. This targeted design\nis more effective than T5Chem and YieldBERTâ€™s interaction modeling, which indiscriminately applies global attention\nto all atoms. It is also worth noting that log-RRIM is pre-training-free, whereas T5Chem and YieldBERT are based on\nfoundation models pre-trained on extensive molecule datasets (e.g. 97 million molecules from PubChem21). log-RRIMâ€™s\nsuperior performance suggests that pre-training may not be necessary if the training dataset is sufficiently large (e.g.,\n78K for USPTO500MT) when the reactions are modeled in a targeted and explicit way. By incorporating more effective\ndesigns, log-RRIM achieves better performance while saving huge resources required for pre-training.\nlog-RRIM andlog-RRIM exhibitnearlyidenticalperformance,withMAEvaluesof0.181and0.179,respectively.The\nb l\nformer employs basic atom features, while the latter utilizes atom representations derived from the pre-trained MAT\nmodel.39 The incorporation of learned representations does not obtain a substantial improvement in yield prediction\naccuracy over basic features. This outcome suggests that the atom representations acquired through the MAT model,\nwhich was originally developed for general molecule representation learning39, lack the specificity required for reaction-\norientedtasks.Althoughbasicatomfeaturesonlyprovideelementaryinformationaboutmolecularproperties,ourfindings\nunderscore that the key to enhancing yield prediction accuracy lies in more sophisticated and effective modeling of\nintermolecular interactions.\nWhile other graph-based yield prediction methods9,10,36 exist, they are primarily designed for datasets with fixed\nreagent structures, such as the Buchwald-Hartwig dataset, which includes very specific reagent information (additive,\nbase,solvent,andligand).26 However,thesemethodsdonotapplytotheUSPTO500MTdatasetusedinthisstudydueto\nitsvaryingnumberofreagentsacrossreactionsandlackofstandardizedreagentinformation.However,theUSPTO500MT\ndatasetmorecloselyresemblesreal-worldscenarioswherereactioncompositionsarenotstrictlystructured.Inthiscontext,\nlog-RRIM,T5Chem,andYieldBERTdemonstrategreaterpotentialforpracticalapplicationscomparedtothegraph-based\nmethodsjustmentioned.log-RRIMâ€™ssuperiorperformanceamongthosemethods,asdemonstratedinthepreviousresults,\ncombined with its flexibility in handling diverse inputs, positions it as a promising approach for accurate yield prediction\nin practical usage.\nPerformancecomparisonoverdifferentyieldranges\nTogaindeeperinsightsintotheperformancedifferencesbetweenlog-RRIM andT5Chem,weconductedadetailedanalysis\nb\nof predictions across various yield ranges. Figure 2 visualizes these comparisons, with stacked asterisks indicating the\nlevelofstatisticalsignificanceoftheperformancedifferenceacrossyieldranges(seeTableA1forexactvalues).Figure2a\nshows that log-RRIM outperforms T5Chem in predicting yields within the 40% to 100% with t-test p-values all less than\nb\n0.05, indicating statistical significance at the 5% level. This pattern suggests that log-RRIM is a more reliable predictor\nb\nfor medium to high-yielding reactions, a crucial advantage in practical synthesis scenarios.44,45 Also, Figure 2b suggests\n5\n(a) Performanceonreactionswithineachyieldrange (b) Performanceonreactionswithincumulativeyieldrange\nFig. 2 | Performancecomparisonoflog-RRIMb andT5ChemacrossyieldrangesontheUSPTO500MTtestingset.Lefty-axis:MAEofpredicted\nyields.Righty-axis:percentageofreactionsinthetestingsetforeachyieldrange.5%significancelevel:*forp-values<0.05,**forp-values<0.005,\n***forp-values<0.0005.\nthe overall prediction performance of log-RRIM is significantly better. This improved overall accuracy is particularly\nb\nvaluable in the context of exploring new reactions, where precise yield data may not be available for reference. In\nsuch scenarios, log-RRIM â€™s overall more reliable predictions can offer more accurate guidance for reaction planning\nb\nand optimization. However, for reaction yields below 40%, log-RRIM exhibits inferior performance than T5Chem. We\nb\nattributed this to T5Chemâ€™s leveraging of foundation models pre-trained on extensive molecule datasets, compensating\nfor a potential shortage of training samples encountered by log-RRIM on reactions with yields below 40% (18.1% of\nb\nthe training set). Nevertheless, log-RRIM remains the preferred choice for chemists seeking reliable yield predictions,\nb\nparticularly for medium to high-yielding reactions or when no preliminary reaction yield data can be referred to. This\nreliability can significantly aid chemists in experimental planning, reducing the number of optimization iterations and\nminimizing resource consumption.\nEffectivenessinreactant-reagentinteractionsmodeling\nTo assess the modelâ€™s capacity to capture the influence of molecular interactions on yield, specifically how reactants and\nreagents affect each other in the context of a reaction, we conducted two analyses on the testing set of USPTO500MT.\nFirst, we identified 76 reaction pairs (152 reactions) with identical reactants but different reagents and yields. This setup\nallowed us to evaluate how our method is sensitive to the effects of reagents on yields. In this context, â€interactionsâ€\nrefer to how the introduction of different reagents influences the reaction outcome with the same reactants. log-RRIM\nb\nachievedapredictionMAEof0.145,outperformingT5Chemâ€™s0.182.Furthermore,log-RRIM correctlypredictedtheyield\nb\ndifference (how much the yield increases or decreases) in 62% (47 out of 76) of reaction pairs, compared to T5Chemâ€™s\n38%. This suggests that log-RRIM is more sensitive to reagent changes and their effects on yield. Case 1 in Figure 4\nb\nillustrates this: in two identical aryl nitration reactions, adding ether as a solvent increases the ground-truth yield from\n42.0% to 57.7%. log-RRIM correctly predicts this upward trend, while T5Chem does not. This shows log-RRIM â€™s ability\nb b\nto capture how the addition of a solvent (ether) interacts with the existing reactants to influence the yield.\nSecondly, we examined 3,698 reactions grouped into 619 sets, each containing two or more reactions with identical\nreagents but different reactants. This analysis aimed to evaluate the modelsâ€™ ability to predict yields when the same\nreagents interact with various reactants. Here, â€interactionsâ€ refer to how the same set of reagents behaves differently\nwithvaryingreactants.log-RRIM exhibitedmoreaccuratepredictionsin58%ofsets(357outof619),withalowerMAE\nb\nof0.147comparedtoT5Chemâ€™s0.222.Case2inFigure4demonstrateslog-RRIM â€™sconsistentlymoreaccuratepredictions\nb\nwhen the same reagents (carbon disulfide and bromine) interact with two different reactants. This indicates log-RRIM â€™s\nb\nenhanced capability to learn and model specific reagent functions across different reaction contexts, capturing how the\nsame reagents behave differently with varying reactants.\nOverall, These analyses suggest that log-RRIM is more sensitive to changes in reactant-reagent combinations, indi-\nb\ncating better modeling of their interactions. This enhanced capability makes log-RRIM a potential aid for chemists in\nb\nselecting and optimizing reactants or reagents during synthesis planning. We attribute this superiority to log-RRIM â€™s\nb\nexplicit modeling of reagent function to reaction centersd. This approach, implemented through a cross-attention mech-\nanism, aligns with fundamental reaction principles. It allows log-RRIM to directly model how reagents influence the\nb\nreaction center, providing a more nuanced understanding of the reaction process. An ablation study on the removal\nof explicit reagent function modeling, provided in Table A5, further supports this design choice. As a result, log-RRIM\nb\ndemonstratesanenhancedabilitytocaptureandinterpretcomplexreactant-reagentinteractions,leadingtomoreaccurate\nyield predictions across diverse reaction component combinations.\nSensitivitytosmallfragmentsmodifications\nTo evaluate the modelsâ€™ ability to capture the influence of involved small fragments on reaction yields, we conducted a\ncomparative analysis of their performance on similar reactions with small differences only on a few small fragments in\n6\nreactantsorreagents.Giventheabsenceofastandardizedmethodforquantifyingreactionsimilarity,weproposeanovel\nsimilarity metric Sim(X ,X ) between reactions X and X , defined as the average of reactant and reagent similarities:\ni j i j\n1\nSim(X ,X )= [s(R ,R )+s(A ,A )] (4)\ni j 2 i j i j\nA\nwhere X : R âˆ’â†’ P refers to the reaction, R and A are the concatenation of all reactants and reagents in the reaction,\nrespectively. s(Â·,Â·) is the Tanimoto coefficient between the two chemical structures of Morgan fingerprint.46\nFig. 3 | Modelperformanceonreactionpairscategorizedbysimilarity.Thelefty-axisdisplaysthenumberofreactionpairsonalogarithmicscale.\nGreybarsindicatethenumberofreactionpairswithineachsimilarityrange.Greenbarsrepresentthenumberofreactionpairswherelog-RRIMb\npredictsmoreaccuratelythanT5Chem.Therighty-axisshowsthepercentageofreactionpairswithmoreaccuratepredictionsbylog-RRIMb relativeto\nthetotalnumberofreactionsineachsimilarityrange,asdepictedbytheredline.\nWeevaluatedreactionpairsacrossarangeofsimilaritythresholds(0.8-0.95),comparingtheperformanceoflog-RRIM\nb\nand T5Chem in predicting yield differences between the two reactions in the pair. The results are illustrated in Figure 3.\nSpecifically, for reaction pairs with Sim â‰¥ 0.80 (1526 pairs, 3052 reactions), log-RRIM outperformed T5Chem on 53%\nb\n(813/1526) pairs, with overall MAEs of 0.158 and 0.159 respectively. This advantage becomes more pronounced as the\nreaction similarity increases. On 221 pairs with Simâ‰¥0.9, log-RRIM surpassed T5Chem on 56% (123/221), with MAEs\nb\nof 0.162 and 0.165 respectively. The trend culminated with highly similar reaction pairs (Sim â‰¥ 0.95, 24 pairs), where\nlog-RRIM demonstrated marked superiority, outperforming T5Chem on 71% (17/24), with MAEs of 0.150 and 0.170\nb\nrespectively. These results reveal a clear trend: log-RRIM â€™s accuracy in capturing yield differences improves as reaction\nb\nsimilarityincreases.Thisindicatesthatlog-RRIM exhibitsenhancedsensitivitytosubtlecomponentchangesthatimpact\nb\nreaction yields, particularly for highly similar reactions.\nThe capability is also demonstrated in several cases. In Figure 4 case 3, the two reactions differ only in their ortho-\nsubstitution (methoxy vs fluoro group), resulting in a yield decrease from 68.2% to 48.9%. log-RRIM correctly predicts\nthis change, while T5Chem incorrectly predicts the opposite trend. Similarly, case 4 in Figure 4 presents two alkylations\nof hydroxyquinoline with different alkylating agents. The ground-truth yield changes minimally in this situation, which\nlog-RRIM correctly predicts, whereas T5Chem makes an erroneous prediction. These results indicate that the log-RRIM\nb b\nexcels in predicting yield changes triggered by those small modifications in atoms, functional groups, or small molecules\nin reactants or reagents. This capability is essential for optimizing reactions in complex chemical systems, where small\nadjustments to reactants and reagents can significantly impact yields. log-RRIM â€™s precision in predicting the effects of\nb\nthese subtle changes enhances its utility for guiding synthetic strategies and fine-tuning reactions. By offering reliable\nforecasts for small modifications, log-RRIM can potentially streamline the optimization process, reducing the number of\nb\nexperimental iterations required and saving time and resources in research and industrial settings.\nThis capability stems from log-RRIM â€™s unique local-to-global learning strategy. By first analyzing each molecule\nb\nseparately and then modeling their interactions, the model ensures equal consideration of all molecules, regardless of\ntheir size. This approach differs from sequence-based models like T5Chem, which process the entire reaction SMILES\nstring simultaneously. Such models may overlook crucial smaller fragments that significantly impact the overall yield, as\nthe global attention mechanisms might not be sufficiently sensitive to these critical molecular fragments.\nOverall, the performance differences presented in these analyses underscore our belief that model frameworks should\nbe carefully designed based on specific task characteristics rather than solely relying on foundation models. While they\nhaveshowngreatpromiseinmanyareas,abasicfine-tuningstrategymaynotalwaysbeoptimalforspecializedtaskslike\nreaction yield prediction. Such an approach lacks task-specific module designs that capture the intricate characteristics\nof chemical reactions, potentially limiting the performance.\n7\nReactants Reagents Product Yield\nGround-truth: 42.0%\nT5Chem: 83.5% (ð´ð¸=41.5%)\nlog-RRIM: 58.2% (ð´ð¸=16.2%)\nb\nCase 1\nGround-truth: 57.7% (âˆ†=+15.7%)\nT5Chem: 74.6% (ð´ð¸=16.9%,âˆ†=âˆ’8.9%)\nlog-RRIM: 65.6% (ð´ð¸=7.9%,âˆ†=+7.4%)\nb\nGround-truth: 85.1%\nT5Chem: 66.3% (ð´ð¸=18.8%)\nlog-RRIM: 70.4% (ð´ð¸=14.7%)\nb\nCase 2\nGround-truth: 57.1% (âˆ†=âˆ’28.6%)\nT5Chem: 80.6% (ð´ð¸=23.5%,âˆ†=+14.3%)\nlog-RRIM: 58.4% (ð´ð¸=1.3%,âˆ†=âˆ’12.0%)\nb\nGround-truth: 68.2%\nT5Chem: 62.9% (ð´ð¸=5.3%)\nlog-RRIM: 73.8% (ð´ð¸=5.6%)\nb\nCase 3\nGround-truth: 46.9% (âˆ†=âˆ’21.3%)\nT5Chem: 66.8% (ð´ð¸=19.9%,âˆ†=+3.9%)\nlog-RRIM: 64.7% (ð´ð¸=17.8%,âˆ†=âˆ’9.1%)\nb\nGround-truth: 53.9%\nT5Chem: 44.0% (ð´ð¸=9.9%)\nlog-RRIM: 53.9% (ð´ð¸=0.0%)\nb\nCase 4\nGround-truth: 52.2% (âˆ†=âˆ’1.7%)\nT5Chem: 36.0% (ð´ð¸=16.2%,âˆ†=âˆ’8.0%)\nlog-RRIM: 53.3% (ð´ð¸=1.1%,âˆ†=âˆ’0.6%)\nb\nFig. 4 | CasesanalysisontheUSPTO500MTdataset.Eachreactionisreportedwithreactants,reagents,products,andtheground-truthand\npredictedyieldsbyT5Chemandlog-RRIMb.AE inparenthesesrepresentstheAbsoluteErrorbetweenthepredictedandground-truthyields.âˆ†in\nparenthesesrepresentsthechangeoftheground-truthandpredictedyieldsinthesecondreactiontothecorrespondingvalueinthefirstreaction.\nPerformanceontheexternaldatasetCJHIF\nTo assess our modelâ€™s performance on external datasets, we conducted an evaluation using a subset of the CJHIF\ndataset.27 This approach involves using models trained on USPTO500MT and testing them on a subset of the CJHIF\ndataset, which comprises 3,219,165 reactions sourced from high-impact factor journals. Our assessment involved 1,000\nzero-yieldingchemicalreactionsrandomlyselectedfromtheinitial50,000reactionsintheCJHIFdataset.Wespecifically\nchose reactionswith reported non-zeroyieldsbecause CJHIFtreats unreported yields aszeros, andwe aimedto evaluate\nour model on reactions with confirmed, measurable outcomes. Importantly, these 1,000 reactions are not included in the\ntrainingortestingdataofUSPTO500MT,thusprovidinganindependenttestingsetforassessingourmodelâ€™sperformance\non external reactions.\nOverall, log-RRIM achieved an MAE of 0.149, representing a 16.8% improvement over T5Chemâ€™s MAE of 0.179.\nb\nTheresultsofanalyzingperformanceacrossyieldrangesareillustratedinFigure5.log-RRIM significantlyoutperformed\nb\nT5Chemforreactionswithyieldsbetween60%to100%(confidencelevel95%,moredetailsareprovidedinTableA2).This\nsuperior performance aligns closely with our observations from the USPTO500MT dataset, particularly in log-RRIM â€™s\nb\nenhanced accuracy for medium to high-yielding reactions, which suggests that log-RRIM â€™s improved predictive power\nb\nfor high-yielding reactions is a generalizable feature, not limited to a specific dataset. We attribute this generalizability\nto log-RRIM â€™s molecular interaction design which uses the cross-attention mechanism to effectively model the function\nb\nof reagents in relation to the reaction center. This allows log-RRIM to learn fundamental principles about how reagents\nb\nimpactbond-breakingandformation,whicharekeyfactorsaffectingreactionyield.TheextensivedatainUSPTO500MT\ntraining data enables log-RRIM to learn such principles to achieve better test performance on external datasets.\nb\nTo further validate that log-RRIM has effectively learned key factors influencing reaction yield, we visualized the\ncontribution(weight)ofeachatomwhenlog-RRIM aggregatesatomembeddingsandconstructsthemoleculerepresenta-\nb\ntion. Three exemplar reactions are shown in Figure 6. In reaction A, a sulfonylation reaction, the sulfur-bearing sulfonyl\nchloride group on the p-toluenesulfonyl chloride and the free hydroxyl (OH) group on the alcohol are the two reacting\ncenters.Theoxygen(O)actsasthenucleophilethatdisplacesthechlorine(Cl)atom,andtheseatomsinfluencetheyield\nof the reaction. Reaction B is an imine reduction reaction of the compound N-(4-methoxyphenyl)-1-phenylethylamine.\nThe polar C=N bond between the Nitrogen (N) and Carbon (C) is the reactive site, and these two atoms influence the\nyieldofthereaction,whichresultsinthesingleC-Nbondinthecorrespondingamine.InreactionC,thetwoatomsthat\nultimately influence the yield are Sulfur (S) of benzene sulfonyl chloride and Nitrogen (N) of the indole, producing the\nfinalcompound.CombinedwiththeweightshighlightedbythecolormapinFigure6,wefoundthattheatomsmentioned\nabove that have a greater impact on yield are given higher weights by log-RRIM , and these atoms are also the atoms in\nb\nthe reactioncenter. Thisfinding alignswith the fundamental chemical principlethat reaction center atomsplaya crucial\nrole in the bond-breaking and bond-forming steps in the transition state, thereby exerting substantial influence on the\n8\nFig. 5 | log-RRIMb andT5ChemperformancecomparisonovereachyieldrangeonasubsetofCJHIF.Lefty-axis:MAEofpredictedyields.Right\ny-axis:percentageofreactionsinthetestingsetforeachyieldrange.5%significancelevel:*forp-values<0.05,**forp-values<0.005,***for\np-values<0.0005.\nyield. The ability of log-RRIM to prioritize these critical atoms in learning the molecule representation is essential for\nb\nbuildingmoreaccuratemodelsforpredictingreactionyield,andourmethoddemonstratesparticulareffectivenessinthis\nregard.\nA Cc1ccc(cc1)S(Cl)(=O)=O.OCCOCCOCCOCCN1C(=O)c2ccccc2C1=O>>Cc1ccc(cc1)S(=O)(=O)OCCOCCOCCOCCN1C(=O)c2ccccc2C1=O\nGround-truth: 91.0% log-RRIMb prediction: 66.8% T5Chem prediction: 19.2%\n1.0\n0.8\nB COc1ccc(cc1)\\N=C(/C)c1ccccc1>>COc1ccc(NC(C)c2ccccc2)cc1\nGround-truth: 99.0% log-RRIMb prediction: 83.7% T5Chem prediction: 71.6%\n0.6\n0.4\n0.2\nC c1cc2ccccc2[nH]1.ClS(=O)(=O)c1ccccc1>>O=S(=O)(c1ccccc1)n1ccc2ccccc12\n0.0\nGround-truth: 84.0% log-RRIMb prediction: 83.3% T5Chem prediction: 62.3%\nAtomic Weight\nFig. 6 | Visualizationsofatomcontributioninlearningmoleculerepresentation.Thecontributionisquantifiedbythecolorandthethreeexemplar\nreactionsareselectedfromtheCJHIFdataset.\nPerformanceontheBuchwald-Hartwigdataset\nOn the Buchwald-Hartwig dataset, we conducted a performance comparison among pre-training-free models (YieldGNN,\nSEMG-MIGNN, and RD-MPNN), using 10-fold cross-validation,47 and reported the testing results averaged over the 10\nfolds. Table 3 reports the mean and standard deviation (in parentheses) of MAE, RMSE, and R2. Table A4 provides a\ndetailed comparison of testing MAE values for different models on each fold. Our method, log-RRIM , outperforms other\nb\npre-training-free (also graph-based) methods across all evaluation metrics (MAE 0.0348, RMSE 0.0544, and R2 0.953).\nNotably,itachievesa14.7%improvementinMAEoverthebest-performingbaseline,YieldGNN.Weattributelog-RRIM â€™s\nb\nsuperior performance to its more effective molecular interaction design, explicitly modeling the reagentsâ€™ function to the\nreaction center. By incorporating this design, log-RRIM captures crucial chemical insights that other methods may\nb\noverlook,leadingtomoreaccuratepredictions.Comparedtothesecond-bestbaselinemethod,SEMG-MIGNN,log-RRIM\nb\nimproves the MAE by 17.9%. To put this improvement in context, itâ€™s worth recalling that SEMG-MIGNN focuses on\nbuildingmoreinformativeatomfeatures(digitalizedstericandelectronicinformation).Incontrast,log-RRIM emphasizes\nb\nlearning the characteristics of the reaction itself and molecular interactions. The performance difference between these\napproaches suggests that for yield prediction tasks, the latter strategy may be more effective. In summary, log-RRIM\nb\n9\noutperforms other pre-training-free and graph-based models substantially. These results demonstrate the importance of\nfocusing on reaction characteristics and molecular interactions in yield prediction tasks.\nAs shown in Table 4, when compared to the pre-training-based methods, which are also sequence-based models\n(T5Chem and YieldBERT), log-RRIM also shows competitive performance by achieving the MAE of 0.0347, which has a\nl\n16.4% improvement over YieldBERT but inferior to T5Chem by 11.6%. These results indicate our method is comparable\nto the best-performing sequence-based model T5Chem while using only 2% of the pre-training dataset size compared to\nT5Chem (2M vs 97M). To further validate this comparison, we conducted statistical analysis on the predicted values of\nlog-RRIM andT5Chemforeachyieldrangeonthefirstdatasplit(withdetailedp-valuesshowninTableA3).Theanalysis\nl\nshowsthat,forallrangesexceptthe10%-20%,40%-50%,and70%-80%ranges(atotalof27.6%ofthetestreactions),the\ndifferences between log-RRIM and T5Chem are not statistically significant at the 95% confidence interval. This indicates\nl\nthat the performance of log-RRIM and T5Chem is largely comparable across most yield ranges. Note that the Buchwald-\nl\nHartwig dataset involves only a single reaction type with limited components. On this specific reaction type, log-RRIM\ncouldunderperformT5Chem.However,real-worldchemicalsynthesisgenerallyinvolvesreactionsofmultipletypes.Thus,\nmethods that could accurately predict yields of various types are highly demanded. As shown on the USPTO500MT and\nCJHIFdatasets,whichcontainnumerousreactiontypes,ourmethoddemonstratessuperiorperformance.Onthesemore\ndiverse and complex datasets, log-RRIM outperforms T5Chem. These results demonstrate the potential superior utility\nb\nof log-RRIM over T5Chem in real-world chemical synthesis applications.\nb\nWe also note that the performance of log-RRIM and log-RRIM are nearly identical (MAE 0.0348 vs 0.0347) on the\nb l\nBuchwald-Hartwig dataset. This observation aligns with the results we obtained on the USPTO500MT dataset. These\nconsistentfindingsacrossdifferentdatasetssuggestthateffectivemolecularinteractionmodelingmayplayamorecrucial\nrole than using pre-trained models to generate informative atom representations in yield prediction tasks.\nTable 3 | Pre-training-freemodelsperformancecomparisonontheBuchwald-Hartwigdataset\nMetrics\nMethod\nMAE RMSE R2\nRD-MPNN 0.0746(0.005) 0.1040(0.007) 0.854(0.018)\nSEMG-MIGNN 0.0424(0.001) 0.0605(0.002) 0.951(0.004)\nYieldGNN 0.0408(0.002) 0.0575(0.002) 0.956(0.003)\nlog-RRIMb 0.0348(0.002) 0.0544(0.004) 0.953(0.009)\nEachvalueisthemeanandstandarddeviation(inparentheses),averaging\n10folds.Thebestperformanceishighlightedinbold.\nTable 4 | Pre-training-basedmodelsperformancecomparisonontheBuchwald-Hartwigdataset\nMetrics\nMethod\nMAE RMSE R2\nYieldBERT 0.0415(0.001) 0.0641(0.005) 0.945(0.008)\nT5Chem 0.0311(0.001) 0.0482(0.002) 0.971(0.002)\nlog-RRIMl 0.0347(0.001) 0.0528(0.003) 0.957(0.006)\nEachvalueisthemeanandstandarddeviation(inparentheses),averaging\n10folds.Thebestperformanceishighlightedinbold.\nDiscussion\nIn conclusion, in this paper, we present log-RRIM, a novel graph-transformer-based reaction representation learning\nframework for yield prediction. log-RRIM leverages a local-to-global representation learning process and incorporates a\ncross-attention mechanism to model reagent-reaction center interactions, facilitating improved capture of small fragment\ncontributionsandinteractionsbetweenreactantandreagentmolecules.Thisapproachallowslog-RRIMtotapintocrucial\naspectsofchemicalknowledge,particularlytheimportanceofreagenteffectsandreactioncenterdynamicsindetermining\nreaction outcomes. Without reliance on pre-training tasks, log-RRIM demonstrates superior accuracy and effectiveness\ncompared to other graph-based methods and state-of-the-art sequence-based approaches, particularly for medium to\nhigh-yielding reactions. Our analyses further show log-RRIMâ€™s advanced modeling of reactant-reagent interactions and\nsensitivity to small molecular fragments, making it a valuable asset for reaction planning and optimization in chemical\nsynthesis.\nThelog-RRIMframeworkrequiresthatpredictedreactionsconsistofthreeparts(reactant,reagent,andproduct)and\nthat reaction center atoms be correctly identifiable. While this may limit its practical applications in some scenarios, it\nenables log-RRIM to more effectively model the crucial intermolecular dynamics that significantly influence reaction out-\ncomes. This approach underscores the importance of incorporating chemical-specific information into model architecture\ndesign, rather than directly adapting general-purpose foundation models for chemical tasks like yield prediction.\nWhile log-RRIM makes significant strides in leveraging chemical knowledge, particularly in modeling reagent-reaction\ncenter interactions, there remains a vast body of chemical expertise that could potentially be incorporated to further\nenhance the performance. For instance, research has elucidated detailed mechanisms for different reaction types, like\ntransition states,48,49 which are not yet explicitly incorporated into our model. Furthermore, chemists have developed a\ndeepunderstandingoftherelativereactivityofdifferentfunctionalgroupsundervariousconditions,50,51 whichrepresents\nanother rich source of knowledge that could be integrated into the model. Incorporating such additional aspects of\nchemical knowledge presents both a challenge and an opportunity for future research. It could potentially enhance the\n10\nFig. 7 | Performancecomparisonoflog-RRIMl andT5ChemacrossyieldrangesonthefirstdatasplitoftheBuchwald-Hartwigdataset.Lefty-axis:\nMAEofpredictedyields.Righty-axis:percentageofreactionsinthetestingsetforeachyieldrange.5%significancelevel:*forp-values<0.05,**\nforp-values<0.005,***forp-values<0.0005.\nmodelâ€™s predictive power, improve its generalization to diverse reaction types, and provide more interpretable insights\ninto the factors driving yield predictions. Another promising direction for future research is the exploration of multi-task\nlearningapproaches,wherethemodelcouldbetrainedsimultaneouslyonyieldprediction,reactionconditionoptimization,\nretrosynthesisplanning,etc.Thiscouldleadtoamorecomprehensiveunderstandingofchemicalreactivityandpotentially\nimprove performance across all tasks.\nlog-RRIM represents a significant step forward in reaction yield prediction by leveraging graph-based representations\nand modeling reagent-reaction center interactions, and there is still room for further integration of chemical knowledge\nand enhancement of the modelâ€™s capabilities. By continuing to merge data-driven techniques with established chemical\nprinciples, it is crucial to develop more robust, versatile, and reliable models for computational chemistry.\nData Availability\nThe data used in this manuscript is made publicly available at https://github.com/ninglab/Yield log RRIM.\nCode Availability\nThe code for log-RRIM is made publicly at https://github.com/ninglab/Yield log RRIM.\nAcknowledgements\nThis project was made possible, in part, by support from the National Science Foundation grant no. IIS-2133650 (X.N.)\nand the National Library of Medicine grant no. 1R01LM014385-01 (X.N., D.A.). Any opinions, findings and conclusions\nor recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the\nfunding agency.\n11\nMethod\nOurmethod,log-RRIM,isanovellocal-to-globalgraph-transformer-basedreactionrepresentationlearningandmolecular\ninteractionmodelingforyieldprediction.Itemploysalocal-to-globallearningprocessforreactionrepresentationlearning,\nbeginning with molecule (reactants, reagents, and product) representation learning. It subsequently models the molecule\ninteractions (between reactants and reagents) and ultimately represents the entire reaction. log-RRIM further uses the\nreaction representation to predict yield.\nSpecifically, log-RRIM consists of the following three modules: (1) Molecule Representation Learning (MRL) mod-\nule: which uses graph transformers39 with multi-head self-attention layers to encode molecular structural information\ninto atom embeddings, and then aggregate atom embeddings into molecule embeddings through Atomic Integration\n(AI). (2) Molecule Interaction (MIT) module: which learns the interactions between reactants and reagents through\nthe cross-attention mechanism, resulting in interaction-aware embeddings for reaction centers. (3) Reaction Information\nAggregation (RIA) module: which employs Molecular Integration (MI) to derive a comprehensive reaction representation\nfromallinvolvedmoleculesandtheirinteractionrepresentations.Finally,thisreactionrepresentationisutilizedtopredict\nthe yield. An overview of log-RRIM is depicted in Figure 8.\nMolecular Representation Learning Molecule Interaction Reaction Information Aggregation\nGT1 AI\nReactant 1 (â„³1ð‘…) ð‘€1ð‘… ð¶ AC ttero ns tis on ð¶áˆš ð‘€à·ª 1ð‘… ð¡ 1ð‘… MI\nð«\nGT1 AI\nReactant 2 (â„³ 2ð‘…) ð‘€ð‘… ð‘€à·ªð‘… ð¡ 2ð‘…\n2 2\nGT2 AI Yield\nð± Predictor\nReagent 1(â„³ 1ð´) ð‘€ 1ð´ ð¡ 1ð´ MI\nðš\nGT2 AI\nReagent 2(â„³ 2ð´) ð‘€ 2ð´ ð¡ 2ð´ GT1 Reactants Graph Transformer\nGT2 Reagents Graph Transformer\nGT3 AI MI GT3 Products Graph Transformer\nAI Atomic Integration\nProduct1(â„³ 1ð‘ƒ) ð‘€ 1ð‘ƒ ð¡ 1ð‘ƒ ð©\nMI Molecular Integration\nFig. 8 | Pipelineoflog-RRIM\nNotations\nInareactionX,eachreactant,reagent,andproductisamolecule.WevieweachmoleculeMasagraph,withbasicnode\n(atom) features I âˆˆRnÃ—s, graph adjacent matrix J âˆˆ{0,1}nÃ—n, and inter-atomic distance matrix D âˆˆRnÃ—n, where n is\nthenumberofatomsinthemoleculeandcanbedifferentforeachmolecule,sisthedimensionofbasicatomfeatures.The\nreactionX isrepresentedas(R,A,P,y),whereR=(cid:8) MR,...,MR (cid:9) ,A=(cid:8) MA,...,MA (cid:9) ,andP =(cid:110) MP,...,MP (cid:111)\n1 nr 1 na 1 np\narethesetofn reactants,n reagentsandn productsinthereaction,andy isthereactionyield.n ,n ,andn canbe\nr a p r a p\ndifferent for each reaction. Notably, we denote the reaction center atom embeddings in reactants as C âˆˆ R|C|Ã—d, where\n|C| refers to the number of reaction center atoms.\nIn MRL module, the atom embeddings of each molecule after the l âˆˆ [1,n ]-th self-attention layer is denoted as\nl\nM(l)âˆˆRnÃ—d, where n is the number of self-attention layers used in MRL and d is the model dimension.\nl\n(cid:110) (cid:111)\nh âˆˆ hR,...,hR ,hA,...,hA ,hP,...,hP is a d-dimension vector and denoted as the representation of each\nm 1 nr 1 na 1 np\nmolecule in reactants, reagents, and products. The reactant, reagent, and product representations are named as râˆˆRd,\naâˆˆRd and pâˆˆRd. Therepresentation ofthe wholereaction is x andthe predictedyield isdenoted by yË†. We summarize\nthe key notations in Table 5. We use uppercase letters to denote matrices, lowercase bold letters to denote row vectors,\nand lower-case non-bold letters to represent scalars.\nMoleculeRepresentationLearning(MRL)\nGiven a reaction consisting of molecules represented in graphs, we first employ the Molecule Attention Transformer\n(MAT) 39 to learn the molecule representations. MAT recursively propagates information between atoms to learn the\n12\nTable 5 | KeyNotations\nNotation Description\nX Reaction\nx Reaction representation\ny, yË† Ground truth reaction yield and the predicted yield\nR, A, P Reactant, reagent and product\nr,a,p Reactant, reagent, and product representation\nn ,n ,n Number of molecules in the reactant, reagent, and product\nr a p\nM Molecule\nn Number of atoms of the molecule\nM Atom embeddings of the molecule\nh Molecule representation\nm\nstructural information of molecules via multi-head molecule self-attention layers as follows:\nQl =M(lâˆ’1)El,Kl =M(lâˆ’1)Fl,Vl =M(lâˆ’1)Gl,\nj j j j j j\n(cid:32) (cid:32) Ql (cid:0) Kl(cid:1)T(cid:33) (cid:33)\nHEADl j = Î» aÏ jâˆš j +Î» dÏ(D)+Î» gJ V jl, (5)\nd\n(cid:104) (cid:105)\nHl = HEADl,HEADl,...,HEADl Ol,\n1 2 nh\nwhereM(lâˆ’1)istheatomembeddingsfromthe(lâˆ’1)-thmoleculeself-attentionlayerandM(0)=I istheinputoffirst\nlayer ; Ql, Kl, and Vl are the query, key, and value matrix derived from M(lâˆ’1) with learnable parameters El, Fl and\nj j j j j\nGl\nj\nâˆˆ RdÃ— nd h; Î» a,Î» d,Î»\ng\nare the scalars to balance the importance of the self-attention, distance matrix, and adjacency\nmatrix, and Ï is the softmax function. Each molecule attention layer has n heads and HEADl is the output from the\nh j\nj-th attention head; Ol âˆˆRdÃ—d is a learnable matrix to integrate the attention heads. After each molecule self-attention\nlayer, MAT includes a feed-forward layer to introduce non-linearity which is a fully connected network (FCN) described\nbelow:\nM(l)=Ïƒ(cid:0) HlWl+Bl(cid:1)\n, (6)\nwhere Wl âˆˆ RdÃ—d and Bl âˆˆ RnÃ—d are learnable parameters, Ïƒ(Â·) is ReLU52 activation function. After n molecule self-\nl\nattention layers, the moleculeâ€™s structural information is encoded into the atom embeddings M(n ). When no ambiguity\nl\narises, for simplicity, we eliminate n for M(n ) and only use M to represent atom embeddings of molecule M as the\nl l\noutput of the last molecule self-attention layer.\nComparedtotheoriginalTransformer,38 MATintegratestheinteractionsamongatoms,thegeometricinformationin\nthe molecule, and the topology of the molecule to better learn expressive atom embeddings, and captures the structural\ninformation of the molecule. Given the atom embeddings M learned from MAT, we utilize the Atomic Integration(AI)\nmodule to aggregate atom embeddings and generate the molecule representation h . Particularly, AI uses a gating\nm\nmechanism to capture the importance of different atoms in the aggregation as follows:\nÎ±=Mw ,\n1\n(cid:88)n (7)\nh = [M] Ã—[Î±] ,\nm k k\nk=1\nwhere w âˆˆ Rd is a learnable vector and Î± is the vector where each element represents the contribution of each atom\n1\nembedding to the molecule representation.\nAdditionally,intheMRLmodule,AIisonlyperformedonreagentsandproductstogettheirmoleculerepresentations\nand is omitted for reactants. This is because the reaction center atom embeddings in the reactants will undergo further\nupdates in the Molecule Interaction(MIT) module. The reactant molecule representations will be obtained through AI\nafterward.\nMoleculeInteraction(MIT)\nReagents, such as catalysts, significantly impact reaction yield by promoting or inhibiting bond breaking and formation.\nWe explicitly model their function to the reaction center atoms to better capture the interaction between reactants and\nreagents. Specifically, given the reaction center atom embeddings C âˆˆ R|C|Ã—d in reactant molecules, and the reagent\nmolecule representations hA âˆˆ(cid:8) hA,...,hA (cid:9) , we update the reaction center atom embeddings by applying a multi-head\ni 1 na\n13\ncross-attention mechanism, described as follows:\nQ =CWQ,K =HAWK,V =HAWV,\nj j j j j j\n(cid:32) (cid:33)\nQ (K )T\nHEAD =Ï jâˆš j V , (8)\nj j\nd\nH =[HEAD ,HEAD ,...,HEAD ]O,\n1 2 nh\nwhere Q is the linear projection of reaction center atom embeddings C; K , and V are the linear projection of the\nj j j\nc ho en ac da st aen na dte Hd Em Ao Dlec iu sle thr eep or ue ts pe unt ta ft ri oo mns jo -f thre aa tg te en nt ts ioH nA he= ad[h\n,\nA\n1\nO,. âˆˆ.. R, dh Ã—A\nn\nda] isâˆˆ aR ln eaaÃ— rnd a. bA lec pro as rs amat et te en rti to on inla ty ee gr rah ta es tn\nhh\ne\na at tt te en nt ti io on\nn\nj\nheads. The updated reaction center atom embeddings CËœ are obtained by passing H to an FCN:\nCËœ =Ïƒ(HWc+Bc), (9)\nwhere Wc âˆˆ RdÃ—d and Bc âˆˆ R|C|Ã—d are learnable parameters. After updating the reaction center atom embeddings in\nthe reactants, we use AI to derive the reactant molecule representations hR âˆˆ(cid:8) hR,...,hR (cid:9) .\ni 1 nr\nMITusesacross-attentionlayertotransformandintegratereagentinformationintothereactioncenteratoms,enabling\nthe model to consider relationships between various reaction components. This makes log-RRIM learn a more chemically\nmeaningfulreactionrepresentationbyemphasizingreactioncentersandreagentinteractions.Wefurthershowandanalyze\nthe benefits that MIT brings to log-RRIM in Table A5, demonstrating its contribution to the overall performance of our\nmodel.\nReactionInformationAggregation(RIA)\nAfter the derivation of representations for all the molecules involved in reactants, reagents, and products, we introduce\nRIAtoaggregateallthemolecularinformation.Thismoduleexplicitlydescribestheinteractionoftheinvolvedmolecules\nin the reaction and their contribution to yield.\nSpecifically, given the reactant molecule representations hR âˆˆ(cid:8) hR,...,hR (cid:9) , reagent molecule representations hA âˆˆ\n(cid:8) hA,...,hA (cid:9) , and the product molecule representations hPi âˆˆ (cid:110) hP1 ,...,hPn (cid:111)r , we first apply MI to respectively deri ive\n1 na i 1 np\nthree representations r, a, and p for reactant, reagent, and product. MI uses a gating mechanism to aggregate the\ninformation from involved molecules. Taking reactant molecules as an example, this process can be described as follows:\nÎ² =âŸ¨hR,w âŸ©\ni i 2\n(cid:88)nr (10)\nr= hRÃ—Î² ,\ni i\ni=1\nwhere w âˆˆRd is a learnable vector to map each molecule representation to its weight Î² . This step allows log-RRIM to\n2 i\ncapturethecollectivepropertieswithineachgroupofmolecules,providingamorecompactandinformativerepresentation\nfor subsequent processing. These three representations are then concatenated to form a comprehensive representation of\nthe entire reaction x=[r,a,p]âˆˆR3d. x then serves as the input for the yield predictor.\nRIAprocessesreactant,reagent,andproductmoleculesseparatelyandaggregatesinformationhierarchicallytoachieve\na nuanced representation of the reaction. This design allows log-RRIM to capture each componentâ€™s unique role and\ncontribution to the reaction process, leading to a nuanced overall representation.\nYieldPredictor\nProvided with the comprehensive reaction representation x, we stack two FCNs to predict the yield yË†. The process is\ndescribed below:\nyË†=(x)=f(Ïƒ(xW +b )w +b ), (11)\n3 1 4 2\nwhere W âˆˆ R3dÃ—d, w âˆˆ Rd, b âˆˆ Rd and b âˆˆ R1 are learnable parameters, and f(Â·) is a sigmoid function to control\n3 4 1 2\nthe predicted yield within the range [0%,100%].\nModeltrainingandhyperparametersoptimization\nDuring training, the mean absolute error (MAE) loss is optimized using adaptive moment estimation (Adam).53\n(cid:80)N\n|y âˆ’yË†|\nMAE = i=1 i i (12)\nN\nTheinitiallearningrateistreatedasahyperparameter.Additionally,weutilizethevalidationsettoschedulethelearning\nratedecaypatienceanddecayfactorrequiredinlr scheduler.ReduceLROnPlateau providedbyPyTorch.54Allthesearched\nhyperparameters and their respective search ranges are summarized in Table A6 and Table A7, respectively.\n14\nAppendix\nExactpairedt-testp-valuesbetweenlog-RRIMandT5Chemoneachdataset\nThe tables below include all the MAE differences and p-values of the paired t-tests between log-RRIM and T5Chem.\nb\nTable A1 | log-RRIMb andT5ChemperformancecomparisonacrossdifferentyieldrangesonUSPTO500MT\nYieldrange [0%,10%] [10%,20%] [20%,30%] [30%,40%] [40%,50%] [50%,60%] [60%,70%] [70%,80%] [80%,90%] [90%,100%]\nMAEdifference(log-RRIMb-T5Chem) 0.104 0.095 0.051 0.032 -0.020 -0.030 -0.048 -0.050 -0.021 0.008\nT-testp-values 1e-10 2e-17 2e-1 1e-6 5e-5 3e-12 7e-40 3e-40 6e-8 3e-2\nCumulativeyieldrange [90%,100%] [80%,100%] [70%,100%] [60%,100%] [50%,100%] [40%,100%] [30%,100%] [20%,100%] [10%,100%] [0%,100%]\nMAEdifference(log-RRIMb-T5Chem) 0.008 -0.006 -0.018 -0.024 -0.025 -0.024 -0.020 -0.015 -0.011 -0.009\nT-testp-values 3e-2 3e-2 4e-17 3e-37 2e-46 3e-50 4e-35 1e-22 4e-12 1e-8\nTable A2 | log-RRIMb andT5Chemperformancecomparisonacrossdifferentyieldrangeon1,000samplednon-zero-yieldingreactionsfromCJHIF\nYieldrange (0%,10%] (10%,20%] (20%,30%] (30%,40%] (40%,50%] (50%,60%] (60%,70%] (70%,80%] (80%,90%] (90%,100%]\nMAEdifference(log-RRIMb-T5Chem) 0.215 0.140 0.043 0.059 -0.004 0.030 -0.040 -0.086 -0.045 -0.016\nT-testp-values 2e-1 5e-2 4e-1 3e-1 9e-1 2e-1 8e-3 3e-14 4e-7 4e-2\nTable A3 | log-RRIMl andT5ChemperformancecomparisonacrossdifferentyieldrangesonthefirstdatasplitofBuchwald-Hartwigdataset\nYieldrange [0%,10%] (10%,20%] (20%,30%] (30%,40%] (40%,50%] (50%,60%] (60%,70%] (70%,80%] (80%,90%] (90%,100%]\nMAEdifference(log-RRIMl-T5Chem) 0.0027 0.0089 -0.0027 -0.0039 0.0082 0.0007 0.0073 0.0223 0.0028 0.0030\nT-testp-values 6e-2 1e-2 5e-1 3e-1 3e-2 9e-1 2e-1 2e-2 5e-1 6e-1\nPerformanceoneachBuchwald-Hartwigdatasplit\nIn Table A4, we present the performance (MAE on the testing set) of each model across every data split.\nImpactofexplicitreagentsfunctionmodeling\nWe conducted an ablation study to investigate the importance of explicitly modeling reagent effects on reaction yield\nby removing the second module Molecular Interaction (MIT) from our proposed log-RRIM framework. In this modified\nb\nversion, atom embeddings of the reactant molecules are directly passed to the Atomic Interaction (AI) module to de-\nrive reactant molecule representations without using the cross-attention mechanism to update the reaction center atom\nembeddings. We compared the performance of this ablated model with the original log-RRIM on the first data split of\nb\nthe Buchwald-Hartwig dataset. The results are summarized in Table A5. Results show that removing the MIT module\nhugely decreased prediction performance, with the MAE of log-RRIM increasing by 45.0%. This substantial drop in\nb\naccuracy underscores the critical role of explicitly modeling reagent effects in yield prediction tasks. The observed per-\nformance degradation can be attributed to the MIT moduleâ€™s ability to capture fundamental characteristics of chemical\nreactions, particularly the influence of substances such as catalysts on bond breaking and formation at reaction centers.\nBy incorporating this knowledge, log-RRIM enhances its capacity to construct more informative molecular and reaction\nrepresentations, ultimately making more accurate reaction yield predictions.\nHyperparameters\nTableA6andTableA7summarizesthesearchedhyperparametersandtheirrangesontheUSPTO500MTandBuchwald-\nHartwigdatasets.Theselectedvaluesforlog-RRIM arehighlightedbyunderlining,whilethoseforlog-RRIM areindicated\nb l\nin bold. Table A8 summarizes the hyperparameters used in the pre-trained MAT model.\n15\nTable A4 | ModelperformanceovereachdatasplitonBuchwald-Hartwig\nData split\nMethod\n1 2 3 4 5 6 7 8 9 10 mean(std)\nYieldBERT 0.0424 0.0425 0.0408 0.0410 0.0417 0.0411 0.0401 0.0424 0.0403 0.0432 0.0416(0.001)\nT5Chem 0.0323 0.0311 0.0311 0.0314 0.0303 0.0297 0.0315 0.0332 0.0298 0.0309 0.0311(0.001)\nRD-MPNN 0.0758 0.0698 0.0694 0.0758 0.0802 0.0726 0.0727 0.0866 0.0697 0.0734 0.0746(0.005)\nSEMG-MIGNN 0.0440 0.0418 0.0397 0.0432 0.0439 0.0418 0.0433 0.0426 0.0410 0.0424 0.0424(0.001)\nYieldGNN 0.0423 0.0415 0.0391 0.0397 0.0410 0.0418 0.0386 0.0394 0.0406 0.0439 0.0408(0.002)\nlog-RRIMb 0.0338 0.0339 0.0331 0.0364 0.0360 0.0328 0.0344 0.0379 0.0344 0.0352 0.0348(0.002)\nlog-RRIMl 0.0363 0.0342 0.0326 0.0371 0.0346 0.0340 0.0338 0.0364 0.0338 0.0343 0.0347(0.001)\nEachvalueisthemodelâ€™sMAEonthecorrespondingtestingset,thebestperformanceishighlightedinbold.\nThemeanandstandarddeviation(inparentheses)areobtainedbyaveragingacross10datasplits.\nTable A5 | log-RRIMb withoutMITperformanceonthefirstdatasplitoftheBuchwald-Hartwigdataset\nMetrics\nMethod\nMAE RMSE R2\nlog-RRIMb withoutMIT 0.0490 0.0651 0.931\nlog-RRIMb 0.0338 0.0530 0.957\nTable A6 | HyperparametersandtheirsearchedrangesontheUSPTO500MTdataset\nName Description Range\neb Nlayer The number of pre-trained self-attention layers to initialize atom features 0, 8\nNlayer The number of self-attention layers 4, 5, 6\nNheads The number of attention heads 16\nhs Model dimension 128, 256, 512, 1024\ne Epoch 35\nbs Batch size 32\ninit lr Initial learning rate 1e-5, 3e-5, 1e-4\nlr decay step Learning rate decay patience 5, 10, 15\nlr decay factor Learning rate decay factor 0.85, 0.90, 0.95\ndp Dropout 0, 0.1, 0.2\ngnorm Gradient norm clipping threshold 0.5, 1, 5, None\nwd Weight Decay 0, 1e-6, 1e-5\nTable A7 | HyperparametersandtheirsearchedrangesontheBuchwald-Hartwigdataset\nName Description Range\neb Nlayer The number of pre-trained self-attention layers to initialize atom features 0, 8\nNlayer The number of self-attention layers 1, 2, 3, 4, 5, 6, 7, 8\nNheads The number of attention heads 16\nhs Model dimension 128, 256, 512, 1024\ne Epoch 300\nbs Batch size 32\ninit lr Initial learning rate 1e-5, 3e-4, 1e-4, 3e-4\nlr decay step Learning rate decay patience 10, 15, 20\nlr decay factor Learning rate decay factor 0.85, 0.90, 0.95\ndp Dropout 0, 0.1, 0.2\ngnorm Gradient norm clipping threshold 0.5, 1, 5, None\nwd Weight Decay 0, 1e-6, 1e-5\nTable A8 | Hyperparametersusedinthepre-trainedMATmodel\nName Description Value\neb Nlayer The number of pre-trained MAT self-attention layers 8\nhs pretrain Pre-trained MAT model dimension 1024\nNheads pretrain Pre-trained MAT attention heads number 16\nNpff The number of dense layers in the position-wise feed-forward block 1\nk Distance matrix kernel â€™EXPâ€™\ndp pretrain Dropout 0\nwd pretrain Weight decay 0\nÎ» ,Î» ,Î» The scalars weighting the naive self-attention, distance, and adjacency matrices 0.33\na d g\n16\nReferences\n[1] Shields, B. J. et al. Bayesian reaction optimization as a tool for chemical synthesis. Nature 590, 89â€“96 (2021).\n[2] Reizman, B. J. & Jensen, K. F. An automated continuous-flow platform for the estimation of multistep reaction kinetics.\nOrganic Process Research & Development 16, 1770â€“1782 (2012).\n[3] Sigman, M. S., Harper, K. C., Bess, E. N. & Milo, A. The development of multidimensional analysis tools for asymmetric\ncatalysis and beyond. Accounts of chemical research 49, 1292â€“1301 (2016).\n[4] Schwaller,P.,Vaucher,A.C.,Laino,T.&Reymond,J.-L. Predictionofchemicalreactionyieldsusingdeeplearning. Machine\nlearning: science and technology 2, 015016 (2021).\n[5] Probst,D.,Schwaller,P.&Reymond,J.-L.Reactionclassificationandyieldpredictionusingthedifferentialreactionfingerprint\ndrfp. Digital discovery 1, 91â€“97 (2022).\n[6] Lu, J. & Zhang, Y. Unified deep learning model for multitask reaction predictions with explanation. Journal of chemical\ninformation and modeling 62, 1376â€“1387 (2022).\n[7] Chen, X. et al. Sequence-based peptide identification, generation, and property prediction with deep learning: a review.\nMolecular Systems Design & Engineering 6, 406â€“428 (2021).\n[8] Li, J. & Jiang, X. Mol-bert: An effective molecular representation with bert for molecular property prediction. Wireless\nCommunications and Mobile Computing 2021, 7181815 (2021).\n[9] Saebi, M. et al. On the use of real-world datasets for reaction yield prediction. Chemical science 14, 4997â€“5005 (2023).\n[10] Li, S.-W., Xu, L.-C., Zhang, C., Zhang, S.-Q. & Hong, X. Reaction performance prediction with an extrapolative and\ninterpretable graph model based on chemical knowledge. Nature Communications 14, 3569 (2023).\n[11] Schwaller,P.etal.Moleculartransformer:amodelforuncertainty-calibratedchemicalreactionprediction.ACScentralscience\n5, 1572â€“1583 (2019).\n[12] Coley, C. W. et al. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science\n10, 370â€“377 (2019).\n[13] Liu,B.et al. Retrosyntheticreactionpredictionusingneuralsequence-to-sequencemodels. ACS central science 3,1103â€“1113\n(2017).\n[14] Chen,Z.,Ayinde,O.R.,Fuchs,J.R.,Sun,H.&Ning,X. G2retroasatwo-stepgraphgenerativemodelsforretrosynthesis\nprediction. Communications Chemistry 6, 102 (2023).\n[15] Kariofillis, S. K. et al. Using data science to guide aryl bromide substrate scope analysis in a ni/photoredox-catalyzed cross-\ncoupling with acetals as alcohol-derived radical sources. Journal of the American Chemical Society 144, 1045â€“1055 (2022).\n[16] Yada, A. et al. Machine learning approach for prediction of reaction yield with simulated catalyst parameters. Chemistry\nLetters 47, 284â€“287 (2018).\n[17] Cortes, C. & Vapnik, V. Support-vector networks. Machine learning 20, 273â€“297 (1995).\n[18] Breiman, L. Random forests. Machine learning 45, 5â€“32 (2001).\n[19] Raffel,C.et al. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. Journal of machine learning\nresearch 21, 1â€“67 (2020).\n[20] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 (2018).\n[21] Kim, S. et al. Pubchem 2019 update: improved access to chemical data. Nucleic acids research 47, D1102â€“D1109 (2019).\n[22] Weininger,D.Smiles,achemicallanguageandinformationsystem.1.introductiontomethodologyandencodingrules.Journal\nof chemical information and computer sciences 28, 31â€“36 (1988).\n[23] Neel, A. J., Milo, A., Sigman, M. S. & Toste, F. D. Enantiodivergent fluorination of allylic alcohols: data set design reveals\nstructural interplay between achiral directing group and chiral anion. Journal of the American Chemical Society 138, 3863â€“\n3875 (2016).\n[24] Carlson, R. & Carlson, J. E. Design and optimization in organic synthesis (Elsevier, 2005).\n[25] Coley, C. W., Green, W. H. & Jensen, K. F. Machine learning in computer-aided synthesis planning. Accounts of chemical\nresearch 51, 1281â€“1289 (2018).\n[26] Ahneman,D.T.,Estrada,J.G.,Lin,S.,Dreher,S.D.&Doyle,A.G. Predictingreactionperformanceincâ€“ncross-coupling\nusing machine learning. Science 360, 186â€“190 (2018).\n[27] Jiang,S.etal. Whensmilessmiles,practicalityjudgmentandyieldpredictionofchemicalreactionviadeepchemicallanguage\nprocessing. IEEE Access 9, 85071â€“85083 (2021).\n[28] Chuang,K.V.&Keiser,M.J. Commentonâ€œpredictingreactionperformanceincâ€“ncross-couplingusingmachinelearningâ€.\nScience 362, eaat8603 (2018).\n[29] Sandfort,F.,Strieth-Kalthoff,F.,KuÂ¨hnemund,M.,Beecks,C.&Glorius,F.Astructure-basedplatformforpredictingchemical\nreactivity. Chem 6, 1379â€“1390 (2020).\n[30] Perera, D. et al. A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow. Science\n359, 429â€“434 (2018).\n[31] Schwaller,P.etal.Mappingthespaceofchemicalreactionsusingattention-basedneuralnetworks.Naturemachineintelligence\n3, 144â€“152 (2021).\n[32] Lowe, D. Chemical reactions from us patents (1976-sep2016) (2017). URL â€https://figshare.com/articles/dataset/Chemical\nreactions from US patents 1976-Sep2016 /5104873â€.\n[33] Yang, K. et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and\nmodeling 59, 3370â€“3388 (2019).\n[34] Jo, J., Kwak, B., Choi, H.-S. & Yoon, S. The message passing neural networks for chemical property prediction on smiles.\nMethods 179, 65â€“72 (2020).\n[35] Tang, M., Li, B. & Chen, H. Application of message passing neural networks for molecular property prediction. Current\nOpinion in Structural Biology 81, 102616 (2023).\n[36] Yarish, D. et al. Advancing molecular graphs with descriptors for the prediction of chemical reaction yields. Journal of\nComputational Chemistry 44, 76â€“92 (2023).\n[37] Lei,T.,Jin,W.,Barzilay,R.&Jaakkola,T. Derivingneuralarchitecturesfromsequenceandgraphkernels. InInternational\nConference on Machine Learning, 2024â€“2033 (PMLR, 2017).\n17\n[38] Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[39] Maziarka, L(cid:32). et al. Molecule attention transformer. arXiv preprint arXiv:2002.08264 (2020).\n[40] Hu, W. et al. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265 (2019).\n[41] Sterling,T.&Irwin,J.J. Zinc15â€“liganddiscoveryforeveryone. Journalofchemicalinformationandmodeling 55,2324â€“2337\n(2015).\n[42] Somnath, V. R., Bunne, C., Coley, C. W., Krause, A. & Barzilay, R. Learning graph models for retrosynthesis predic-\ntion. In Thirty-Fifth Conference on Neural Information Processing Systems (2021). URL https://openreview.net/forum?id=\nSnONpXZ uQ .\n[43] Balaji, N. N. A., Beaulieu, C. L., Bogner, J. & Ning, X. Traumatic brain injury rehabilitation outcome prediction using\nmachine learning methods. Archives of Rehabilitation Research and Clinical Translation 5, 100295 (2023).\n[44] Ma, Y. et al. Are we making much progress? revisiting chemical reaction yield prediction from an imbalanced regression\nperspective. In Companion Proceedings of the ACM on Web Conference 2024, 790â€“793 (2024).\n[45] Kawasaki,H.,Kihara,N.&Takata,T. Highyieldingandpracticalsynthesisofrotaxanesbyacylativeend-cappingcatalyzed\nby tributylphosphine. Chemistry Letters 28, 1015â€“1016 (1999).\n[46] Morgan, H. L. The generation of a unique machine description for chemical structures-a technique developed at chemical\nabstracts service. Journal of chemical documentation 5, 107â€“113 (1965).\n[47] Geisser,S. Thepredictivesamplereusemethodwithapplications. JournaloftheAmericanstatisticalAssociation 70,320â€“328\n(1975).\n[48] Carey,F.A.&Sundberg,R.J. Advanced organic chemistry: part A: structure and mechanisms (SpringerScience&Business\nMedia, 2007).\n[49] Anslyn, E. Modern Physical Organic Chemistry, vol. 227 (University Science Books, 2006).\n[50] Clayden, J., Greeves, N. & Warren, S. Organic chemistry (Oxford University Press, USA, 2012).\n[51] Smith, M. B. Marchâ€™s advanced organic chemistry: reactions, mechanisms, and structure (John Wiley & Sons, 2020).\n[52] Nair,V.&Hinton,G.E.Rectifiedlinearunitsimproverestrictedboltzmannmachines.InProceedingsofthe27thinternational\nconference on machine learning (ICML-10), 807â€“814 (2010).\n[53] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n[54] Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information\nprocessing systems 32 (2019).\n18",
    "pdf_filename": "log-RRIM_Yield_Prediction_via_Local-to-global_Reaction_Representation_Learning_and_Interaction_Model.pdf"
}