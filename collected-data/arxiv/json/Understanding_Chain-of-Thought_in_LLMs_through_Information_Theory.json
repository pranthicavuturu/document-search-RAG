{
    "title": "Understanding Chain-of-Thought in LLMs through Information Theory",
    "context": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from complex reasoning to code generation [Chowdhery et al., 2024, OpenAI et al., 2024, Bubeck et al., 2023, Anil et al., 2023]. Many of these advances can be attributed to Chain-of-Thought (CoT) reasoning [Wei et al., 2024, Nye et al., 2021, Li et al., 2024], which involves breaking down complex problems into a series of intermediate steps, mirroring human-like reasoning processes. The success of CoT reasoning, particularly in domains such as mathematics, logic, and multi-step decision-making, has led researchers and developers to incorporate CoT-like features directly into model training, i.e. the FLAN family of models [Chung et al., 2022, Wei et al., 2022]. This paper introduces a new formal framework for analyzing CoT in LLMs. We provide a rigorous method grounded in information theory, to evaluate the quality of each step in a model’s reasoning process, thus offering insights beyond simple accuracy metrics to identify areas for improvement. Previous work in this area has proposed “Process Supervision” [Lightman et al., 2023], which requires expensive, human-annotated step-by-step data. While effective, this approach is often impractical due to the high cost and effort of creating large-scale annotated datasets. In turn, alternative methods have recently been proposed, such as outcome reward modelling [Havrilla et al., 2024] or the Math-Shepherd [Wang et al., 2024b]. Both these approaches avoid reliance on annotated step-wise CoT data by instead modelling the correctness of each step based on the correctness of final outputs. However, as we demonstrate in this paper, these methods can be unsound for detecting incorrect reasoning steps and can thus lead to a high false-positive rate in certain scenarios. To address these shortcomings, we employ an information-theoretic approach, grounded in the following key insight: Each correct step in a reasoning process should provide valuable and relevant information that aids in predicting the final correct outcome. Building on this insight, we develop a framework to quantify the “infor- mation gain” after each sub-task in the reasoning process, without the need for step-by-step annotations. This enables us to detect sub-tasks that fail to contribute meaningful information toward the correct solution, signalling potential errors or irrelevant steps in the model’s reasoning. In addition, we also introduce a practical algorithm to assess LLM performance across various sub-tasks within a Chain-of-Thought (CoT) reasoning process. The key contributions of this paper are as follows:",
    "body": "UNDERSTANDING CHAIN-OF-THOUGHT IN LLMS\nTHROUGH INFORMATION THEORY\nJean-Franc¸ois Ton∗1, Muhammad Faaiz Taufiq∗1, and Yang Liu2\n1ByteDance Research\n2University of California Santa Cruz\nLarge Language Models (LLMs) have shown impressive performance in complex reasoning\ntasks through Chain-of-Thought (CoT) reasoning, allowing models to break down problems\ninto manageable sub-tasks. However, existing CoT evaluation techniques either require an-\nnotated CoT data or fall short in accurately assessing intermediate reasoning steps, leading\nto high rates of false positives. In this paper, we formalize CoT reasoning in LLMs through\nan information-theoretic lens. Specifically, our framework quantifies the ‘information gain’ at\neach reasoning step, enabling the identification of failure modes in LLMs without the need for\nexpensive annotated datasets. We demonstrate the efficacy of our approach through extensive\nexperiments on toy and GSM-8K data, where it significantly outperforms existing outcome-\nbased methods by providing more accurate insights into model performance on individual tasks.\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from\ncomplex reasoning to code generation [Chowdhery et al., 2024, OpenAI et al., 2024, Bubeck et al., 2023, Anil\net al., 2023]. Many of these advances can be attributed to Chain-of-Thought (CoT) reasoning [Wei et al., 2024,\nNye et al., 2021, Li et al., 2024], which involves breaking down complex problems into a series of intermediate\nsteps, mirroring human-like reasoning processes. The success of CoT reasoning, particularly in domains such as\nmathematics, logic, and multi-step decision-making, has led researchers and developers to incorporate CoT-like\nfeatures directly into model training, i.e. the FLAN family of models [Chung et al., 2022, Wei et al., 2022].\nThis paper introduces a new formal framework for analyzing CoT in LLMs. We provide a rigorous method\ngrounded in information theory, to evaluate the quality of each step in a model’s reasoning process, thus offering\ninsights beyond simple accuracy metrics to identify areas for improvement.\nPrevious work in this area has proposed “Process Supervision” [Lightman et al., 2023], which requires expensive,\nhuman-annotated step-by-step data. While effective, this approach is often impractical due to the high cost and\neffort of creating large-scale annotated datasets. In turn, alternative methods have recently been proposed, such\nas outcome reward modelling [Havrilla et al., 2024] or the Math-Shepherd [Wang et al., 2024b]. Both these\napproaches avoid reliance on annotated step-wise CoT data by instead modelling the correctness of each step\nbased on the correctness of final outputs. However, as we demonstrate in this paper, these methods can be\nunsound for detecting incorrect reasoning steps and can thus lead to a high false-positive rate in certain scenarios.\nTo address these shortcomings, we employ an information-theoretic approach, grounded in the following key\ninsight: Each correct step in a reasoning process should provide valuable and relevant information that aids in\npredicting the final correct outcome. Building on this insight, we develop a framework to quantify the “infor-\nmation gain” after each sub-task in the reasoning process, without the need for step-by-step annotations. This\nenables us to detect sub-tasks that fail to contribute meaningful information toward the correct solution, signalling\npotential errors or irrelevant steps in the model’s reasoning. In addition, we also introduce a practical algorithm\nto assess LLM performance across various sub-tasks within a Chain-of-Thought (CoT) reasoning process.\nThe key contributions of this paper are as follows:\n1. We develop a framework for sequential applications of sub-tasks, e.g. Chain-of-Thought and provide a\nrigorous language to describe and detect detect failure modes in LLMs.\n2. Based on this framework, we propose a practical algorithm to assess the task-wise performance of mod-\nels. This yields more granular information about a model’s CoT performance without requiring anno-\ntated data for intermediate reasoning steps.\n∗Denotes equal contribution, where ordering was determined through a coin flip.\nCorresponding authors: jeanfrancois@bytedance.com and faaiz.taufiq@bytedance.com.\n1\narXiv:2411.11984v1  [cs.CL]  18 Nov 2024\n\n3. We validate our methods on extensive toy data and the GSM-8K dataset [Cobbe et al., 2021]. Our method\neffectively identifies failure modes in CoT reasoning, unlike baselines like outcome reward modelling\n[Havrilla et al., 2024] and Math-Shepherd [Wang et al., 2024b], which rely on final accuracy and tend\nto increase false positives in error detection.\n2\nProposed Framework: Setup and Notation\nBefore diving into our framework, we first provide a high-level overview and notation on how LLM generation\nwill be treated throughout this paper. This will allow us to set the foundation for describing our information-\ntheoretic framework. In particular, following the approach in Gonz´alez and Nori [2023], we view LLMs as\nabstract execution machines with a natural language interface. From this perspective, prompts are designed to\nsolve specific problems (e.g., mathematical or logical problems), and the LLM processes the information in the\nprompt to generate an output.\nWe now define the notation for a typical prompt as a combination of two components:\n1. An initial state, represented by a random variable X0 ∈X, denotes information provided in the prompt\nthat the LLM must operate on to obtain the queried information.\n2. A task λ ∈Υ (e.g., addition followed by multiplication) which encapsulates how the LLM should\nprocess information in X0.\nGiven the prompt, defined as a tuple (X0, λ), the state X1 represents the result of applying task λ to the initial\nstate X0. Formally, we denote this using the update mapping Λ : X × Υ →X which outputs the updated\nstate X1 by applying the task λ on X0, i.e. X1 = Λ(X0, λ). This updated state is then used to obtain the final\noutput, denoted by Y ∈X, by extracting only the information in X1 which is relevant to the queried final answer.\nThis notation defines a prompt that instructs a model to process information drawn from some initial distribution\np(X0) (e.g., mathematical problems).\nLet us use the following simple example to illustrate the notation:\nPrompt:“James has 3 apples and Abbey has 9. How many apples do the two have in total?”\nHere, using the above notation, the initial state x0 denotes the information “James has 3 apples; Abbey has 9\napples”, and λ denotes the addition task. Next, x1 = Λ(x0, λ) represents the updated information after correctly\nperforming the addition operation, i.e. x1 =“James has 3 apples; Abbey has 9 apples; The two have 12 apples in\ntotal”. The final output, y, is then obtained by simply extracting the total number of apples from x1, i.e. “The two\nhave 12 apples in total”1. With this basic notation established, we now consider compositions of tasks, enabling\nus to formalize the Chain of Thought (CoT) process in LLMs.\n2.1\nCompositionality\nMany mathematical or logical problems require a sequential application of operations. Our notation is also\namenable to such problems as it accommodates the composition of tasks. Consider a problem which requires two\nsuccessive steps to arrive at the correct output:\nPrompt: “Solve for z = 2 × (x + y) where x = 12 and y = 13”.\n(1)\nIn this example, first, we apply the addition operation to find the value of x + y, and next, we apply the multipli-\ncation operation to find the value of z. Using our notation this can be expressed as Λ(x0, λ1 ◦λ2), where λ1, λ2\ndenote the addition and multiplication tasks respectively. The following property allows us to concretely define\nthe application of compositional task λ1 ◦λ2:\nDefinition 2.1. We say that an update rule Λ : X × Υ →X is compositionally consistent if:\nΛ(x0, λ1 ◦λ2)\nd= Λ(Λ(x0, λ1), λ2)\nfor all x0 ∈X and λ1, λ2 ∈Υ.\nHere,\nd= denotes equality in distribution and is sufficient in many cases. For example, where a query may have\nmultiple correct responses, an almost sure equality may be too restrictive.\nGoing back to the prompt in (1), Figure 1 shows that the model first computes x + y, and next multiplies the\nresult by 2. Here, we refer to X1, X2 as intermediate states and Y is the correct final output. More generally,\nif a problem statement requires sequential application of T sub-tasks, λ = λ1 ◦. . . ◦λT , then the Chain-of-\nThought (CoT) reasoning is divided up into T steps, where the output of the t’th step is recursively defined as\n1Our setup also encapsulates cases with ambiguous (or multiple correct) responses for a given task λ and initial state x0.\nIn this case, Λ(x0, λ) is a random variable with distribution p(X1 | X0 = x0). Therefore, for generality, we treat Λ(x0, λ)\nas a random variable from now on.\n2\n\nFigure 1: Solving the problem in prompt (1) requires compositional application of tasks.\nXt = Λ(Xt−1, λt) for t ∈{1, . . . , T}. Finally, the overall true output Y is obtained by extracting the queried\ninformation from the final state XT .\nHaving established a formal language for the sequential application of tasks, e.g. CoT, we now turn towards how\na task may be divided into such a sequence of intermediate sub-tasks.\n2.2\nPrimitive tasks\nIn this subsection, we introduce the notion of primitive tasks which form the basic building blocks of any task.\nIntuitively, our formulation is reminiscent of ideas from linear algebra, where basis vectors form the basic building\nblocks of a vector space. In our case, any task λ ∈Υ can be expressed as a sequence of primitive tasks. This\ndecomposition will allow us to establish which tasks the model could have learned from the training data. For\nexample, if a specific primitive task is not available in the LLM training data, it would be impossible for the model\nto execute any instructions which involve this primitive task correctly. With this in mind, we now introduce this\nconcept formally:\nDefinition 2.2 (Primitive tasks). We say that a set of tasks Γ ⊆Υ is primitive if, for any task λ ∈Υ, there exists\na unique subset {λi}k\ni=1 ⊆Γ such that λ = λ1 ◦· · · ◦λk.\nNote that the decomposition is not unique but the set of components is. In some cases, there may exist distinct\npermutations of primitive tasks which compose to yield the same task as is common in many associative op-\nerations. As an example, in the context of mathematical problem-solving, the basic arithmetic operation could\nbe considered primitive. The composition of these primitive tasks allows us to construct extremely complex\noperations. Just like in linear algebra, we define the span of these tasks as the set obtained by their sequential\napplications.\nDefinition 2.3 (Span of tasks). Let Φ ⊆Υ be a set of tasks, then:\nSpan(Φ) = {λ1 ◦. . . ◦λk : λi ∈Φ for 1 ≤i ≤k, k ∈Z>0}.\nThe set Span(Φ) comprises all the tasks that can be applied by composing sub-tasks in the set Φ. This means that\nany compositionally consistent update rule Λ which is well-defined on the set of tasks Φ will also be well-defined\non Span(Φ). However, this Λ may still be ill-defined for any task not in this span. This limitation is captured by\nthe concept of unidentifiability, which plays a central role in determining the boundaries of what a model can and\ncannot infer.\n2.3\nUnidentifiability\nThe unidentifiability of tasks forms a key part of our framework. It directly addresses the fundamental challenge\nthat models, such as LLMs, face when dealing with unseen tasks. If a task λ lies outside of Span(Φ), the span\nof tasks the model has been trained on, then the model cannot be expected to infer or apply it correctly. In other\nwords, the model’s capacity is constrained by the identifiability of tasks within the training set. This notion and\nformalization of unidentifiability allows us to highlight a critical limitation in the generalization of models: tasks\nnot encountered during training cannot be reliably executed, as they remain beyond the model’s learned task span.\nMore formally:\nDefinition 2.4 (Unidentifiability). Let Φ ⊆Υ be any set of tasks, then a tasks λ is said to be unidentifiable in Φ\niff, λ ̸∈Span(Φ).\nRemark In practice, the concept of unidentifiability may depend on the initial state X0. For instance, an LLM\nmight accurately perform addition for 2-digit numbers but fail with 10-digit numbers [Razeghi et al., 2022].\nOur framework can be extended to account for such cases by explicitly incorporating the distribution of initial\nstates into the notion of identifiability. For example, addition could be considered unidentifiable when the initial\nstate distribution is p(X0 | X0 includes 10-digit numbers). However, for simplicity, we keep this distributional\ndependence implicit in the definition provided earlier.\nWith this general framework in place, we can now turn this theoretical foundation into a practical algorithm for\ndetecting unidentifiable sub-tasks. Specifically, we explore how the notion of unidentifiability can be combined\nwith information-theoretic approaches to detect failure points in LLMs.\n3\n\n3\nOperationalising our framework\nThis section aims to operationalise the above framework to make inferences regarding the unidentifiability of\nintermediate sub-tasks in a model’s CoT reasoning process. This would subsequently allow us to detect any\nsub-task at which a model’s CoT reasoning process starts to diverge from the ground truth, thereby providing\ninsights into how the model can be improved. For example, suppose we are in a setting where the “addition”\noperation is unidentifiable, then we could further improve the model’s mathematical reasoning by fine-tuning it\non the addition operation.\n3.1\nAn information-theoretic perspective\nTo make the concept of unidentifiability practical in the context of CoT generations, we begin by introducing the\nfundamental assumption. The core assumption in our approach is that each correctly executed CoT reasoning step\nshould contribute meaningful and relevant information that aids in predicting the correct final output, denoted as\nY . If we encounter a step after which the amount of information regarding Y stops increasing, then we can take\nthis as an indication of an incorrectly executed task. We concretise this assumption using using our notation from\nthe previous section:\nAssumption 3.1 (Bayesian network). Let λ ̸= λ′ be two operations with primitive decompositions:\nλ = λ1 ◦. . . λk−1 ◦λk ◦· · · ◦λT\nand\nλ′ = λ1 ◦. . . λk−1 ◦λ′\nk ◦· · · ◦λ′\nT ′,\nwhere λ′\nk is unidentifiable in {λ1, . . . , λT }. Then, the intermediate states corresponding to the tasks λ, λ′ have\nthe following Bayesian network:\nX0\n. . .\nXk−1\nXk\nX′\nk\n. . .\n. . .\nY\nY ′\nλ1\nλk−1\nλk\nλ′\nk\nλk+1\nλ′\nk+1\nλT\nλ′\nT ′\nFigure 2: Bayesian network\nIntuition\nThe Bayesian network in Figure 2 implies that for any two reasoning paths which diverge at step k, the\nfuture states Xi and X′\nj for any i, j ≥k satisfy the conditional independence Xi ⊥⊥X′\nj | Xk−1. Consequently,\nonce we apply λ′\nk, the subsequent states along the new reasoning path (in red) add no information regarding the\nsubsequent states or the output of the original path (in green). Hence the figure represents the fact that, for any\ngiven input, the output of λk (top fork) contains no information regarding the output of any other primitive task\nλ′\nk (bottom fork).\nNow that we have formalised our key information-theoretic assumption on the ground-truth CoT process, we turn\ntowards the model behaviour on unidentifiable tasks in the following section.\n3.2\nTask execution in LLMs\nTo operationalise our framework, we formally distinguish between the model i.e. LLM’s task execution and the\nground truth process which arises from following the instructions correctly. To this end, we explicitly define how\nan LLM interprets a specified task λ using the update rule, ΛM(X0, λ), which is in general distinct from the\nground truth update rule Λ(X0, λ).\nHere, one option would be to consider the idealised setting where the model learns to perfectly follow some of the\nprimitive tasks available in the training data. However, this may be considered too restrictive since in reality most\nLLMs do not always follow a “learned” task perfectly. Instead, we consider a much weaker assumption that the\nmodel cannot correctly execute a task which is unidentifiable in the training data. To this end, suppose ΓM ⊆Γ\ndenotes the primitive tasks available in the LLM training data. Concretely, we make the following assumption on\nLLM’s task execution.\nAssumption 3.2 (Task execution in LLMs). ΛM is compositionally consistent and for any (x0, λ) ∈X × Υ,\nthere exists some bλ ∈Span(ΓM) such that ΛM(x0, λ)\nd= Λ(x0, bλ).\nIntuition\nAssumption 3.2 means that for any task which we would like the LLM to apply, the LLM ends up\nexecuting some task in Span(ΓM) which the model has been trained on. In other words, the model’s execution\nis restricted only to the tasks which could be inferred from the training data (i.e. in Span(ΓM)). Moreover, this\nassumption also allows us to encapsulate cases where the model does not follow the correct instructions or does\nnot decompose a given task correctly.\n4\n\nBefore proceeding further with our main result which will allow us to test for the unidentifiability of sub-tasks,\nwe define some notation which we will use from now onwards. Let λ = λ1 ◦. . . ◦λT denote a primitive\ndecomposition of a task λ. Then, starting from an initial state X0, we denote the model’s intermediate states\nrecursively as:\nXM\nt\n:= ΛM(XM\nt−1, λt)\nand\nXM\n0\n= X0.\nMoreover, we use Y M to denote the model’s final output. Next, using this notation, we present the conditional in-\ndependence which must hold if the model encounters an unidentifiable intermediate task along its CoT reasoning\npath.\nTheorem 3.3. Let ΓM ⊆Γ denote the primitive tasks available in the training data. Let λ be a task with\ndecomposition λ = λ1 ◦. . . ◦λT . If λk is the first task in the decomposition of λ which is unidentifiable in ΓM\n(i.e. k = arg mint{λt ̸∈Span(ΓM)}). Then, under Assumptions 3.1 and 3.2, we have that\nY ⊥⊥XM\nj\n| XM\nj−1\nfor all j ≥k.\n(2)\nTheorem 3.3 shows that under Assumptions 3.1 and 3.2, when the model encounters an unidentifiable task (i.e.\nλk in Theorem 3.3) in its Chain-of-Thought reasoning, the model output satisfies the conditional independence\nin Equation (2). More concretely, after a model’s CoT reasoning diverges from the ground truth at step k, every\nsubsequent step adds no additional information regarding the correct final output Y . In practice, this ‘information’\nis measured by checking if the model’s confidence about the final output Y increases after each step. This is\nformalised in the next section.\n3.3\nTesting for unidentifiability using information gain\nHaving established all the essential components of our framework, we can now provide a concrete description\nof how to practically identify unidentifiable sub-tasks using information theory. As is common in the literature\n[Wang et al., 2024b, Havrilla et al., 2024], we assume access to a dataset consisting of prompts and their corre-\nsponding final answers, obtained by correctly applying the task λ. This dataset is denoted as Dλ := {(xi\n0, yi)}n\ni=1.\nAdditionally, recall that XM\nj\nand XM\nj−1 represent the model’s chain of thought (CoT) reasoning at steps j and\nj −1, respectively. Consequently, each element in the conditional independence statement in Equation (2) can\nbe derived from the data and/or the model.\nTo this end, we consider the mutual information between Y and XM\nj\nconditional on XM\nj−1, denoted by I(Y ; XM\nj\n|\nXM\nj−1). This conditional mutual information term intuitively represents the additional information contributed\nby the j’th step of CoT, that is relevant for predicting the ground truth final output Y . Therefore, we refer to\nI(Y ; XM\nj\n| XM\nj−1) as the information gain at step j.\nIt follows from Theorem 3.3 that if an LLM encounters a sub-task at step i which is unidentifiable in its training\ndata, no subsequent step should contribute any additional information relevant for predicting Y (i.e. the informa-\ntion gain should remain 0 after step i). If, on the other hand, we observe that I\n\u0000Y ; XM\nj\n| XM\nj−1\n\u0001\n> 0 for some\nj ≥i, then under Assumptions 3.1 and 3.2, the task λi is not unidentifiable. To estimate the information gain in\npractice, we use the following result:\nProposition 3.4. Let I(X; Y | Z) denote the mutual information between random variables X and Y condi-\ntional on Z. Then,\nE[log p(Y | XM\nj )] −E[log p(Y | XM\nj−1)] = I\n\u0000Y ; XM\nj\n| XM\nj−1\n\u0001\n≥0.\n(3)\nTo estimate the information gain in (3) using Proposition 3.4, we train a separate LLM, which we refer to as the\nsupervisor model gsup. This model takes as input the model’s CoT reasoning up to any given intermediate step t,\nXM\nt , and is fine-tuned to directly predict the ground truth final output Y . In this way gsup(XM\nt ) approximates the\nconditional distribution p(Y | XM\nt ). Then, the quantity E[log p(Y | XM\nj )] can be estimated using the negative\ncross-entropy loss for predicting Y , i.e.\nE[log p(Y | XM\nj )] ≈E[log ˆp(Y | XM\nj )] = −E[lCE\n\u0000Y, gsup(XM\nj )\n\u0001\n],\nwhere lCE denotes the cross-entropy loss. From this, it follows that\nE[log p(Y | XM\nj )] −E[log p(Y | XM\nj−1)]\n|\n{z\n}\nInformation gain\n≈E[lCE(Y, gsup(XM\nj−1))] −E[lCE(Y, gsup(XM\nj ))].\n(4)\nSummary: The information gain (IG) between steps j and j −1 reflects how much relevant information step\nj contributes towards predicting Y . If task λj is executed correctly, this gain is positive, as indicated by a\ndecrease in the cross-entropy loss. Conversely, if step j does not provide additional information, the loss remains\nunchanged. This can be interpreted as the conditional mutual information between XM\nj\nand Y , conditioned on\n5\n\nXM\nj−1. Positive information gain suggests step j adds new insight about Y , while no gain indicates no added\ninformation. Training details for the supervisor model are in Appendix B.1.3.\nRemark on sample-wise information gain\nWhile conditional mutual information provides an aggregate mea-\nsure of information gain for a sub-task in a dataset, it may also be desirable to obtain an analogous measure of\nsub-task correctness for individual CoT instances. This could be useful, for example, in detecting which step\nwent wrong for a given prompt. Our notion of information gain can be extended to this sample-wise setting by\ninstead considering the following difference\nlog p(Y | XM\nj ) −log p(Y | XM\nj−1) ≈lCE(Y, gsup(XM\nj−1)) −lCE(Y, gsup(XM\nj )).\n(5)\nIntuitively, if step j in the model’s CoT is correct, the model should become more confident in the ground truth\noutput Y being the correct final answer. Therefore, the difference above should be positive. Alternatively, if\nstep j is wrong, the model’s confidence regarding the true output Y should not increase and the above difference\nshould not be positive. From now on, we refer to the difference in (5) as sample-wise information gain at step j.\n4\nRelated works\nEvaluation of CoT reasoning\nSeveral recent works propose methodologies for evaluating CoT reasoning [Wei\net al., 2024, Havrilla et al., 2024, Li et al., 2023, Joshi et al., 2023, Nguyen et al., 2024, Wang et al., 2024a, Yu\net al., 2024, Xie et al., 2024]. For example, Li et al. [2023] verifies individual steps in a model’s CoT reasoning by\ngenerating multiple LLM responses per prompt and comparing correct responses with incorrect ones. Similarly,\nWang et al. [2024b,c] use a fine-tuned LLM to decode multiple reasoning paths from each step and check the\ncorrectness of these reasoning paths. However, as we show in our experiments, approaches which simply rely\non the correctness of the final output are not sound in general and can lead to false positives. Moreover, these\nsolutions may not be plausible for problems of high difficulty where correct LLM responses might be scarce.\nFormalising CoT framework\nThe formalisation of LLM reasoning remains an active area of research. Most\nnotably Gonz´alez and Nori [2023] introduces a formal framework for LLMs and is a key source of inspiration\nbehind our formalism. Additionally, Feng et al. [2023] theoretically examines the expressivity of LLMs with CoT\nin solving mathematical and decision-making problems, focusing on the transformer architecture’s implications\non accuracy. Besides this, Xu et al. [2024] provides a formal definition of hallucinations, but does not consider\nCoT reasoning specifically.\nReward modelling\nOne notable line of work known as outcome-based reward models (ORM) [Cobbe et al.,\n2021, Havrilla et al., 2024, Lightman et al., 2023] predicts the probability of reaching the correct final answer\ngiven a model’s intermediate CoT steps. While ORMs do not require demonstrations of correct intermediate\nsteps, we show in Section 5 that this approach is not sound for detecting errors in a model’s CoT reasoning.\nAnother related method is step-wise ORM (SORM) Havrilla et al. [2024] which estimates the probability of an\n‘optimal’ model reaching a correct answer, given the CoT reasoning of our model of interest. However, unlike\nour approach, SORM requires training a model which is larger and more capable than our base model.\nProcess-based reward modelling (PRMs) [Lightman et al., 2023, Uesato et al., 2022] is an alternative approach\nwhich directly predicts the correctness of intermediate CoT reasoning steps. Likewise, various other approaches\nrely on annotated CoT datasets for benchmarking [Jacovi et al., 2024, Yu et al., 2024, Amini et al., 2019, Liu\net al., 2020, Xi et al., 2024, Nguyen et al., 2024, Xie et al., 2024, McLeish et al., 2024]. While these benchmarks\nand methodologies can be valuable for improving LLM reasoning, collecting annotated data can be very costly\nand is not readily scalable to other tasks. Unlike these methods, our approach computes the information gain at\neach step, providing a richer measure of LLM performance without requiring any human-annotated CoT data.\n5\nExperiments\nIn this section, we empirically demonstrate the practical utility of our framework. In addition to our proposed\nmethod dubbed information gain (denoted by IG), we consider two common baselines that can be used to detect\nthe errors in a model’s CoT reasoning and assume access to only the model’s CoT generations X0, XM\n1 , . . . , XM\nT\nas well as the correct final answers denoted as Y .\nOutcome Reward Model (ORM) [Havrilla et al., 2024]\nThis involves training a classifier, denoted as fORM,\nwhich takes as input model generations up to any step t in its CoT reasoning, XM\nt , and predicts the probability\nof the model’s final answer being correct, i.e.\nfORM(XM\nt ) ≈P(Y M = Y | XM\nt ).\n(6)\nHere, if we observe that this probability of correctness drops significantly after step t, i.e. if fORM(XM\nt ) ≫\nfORM(XM\nt+1), this indicates that the model does not apply the task λt+1 correctly.\n6\n\nErrors flagged by the respective baselines\nGT errors\n1\n2\n3\n4\n5\nLLM1\nLLM2\nLLM3\nLLM4\nLLM5\n-0.014\n0.0099 0.00098 -0.0019 -0.00019\n0.013\n-0.0096\n-0.004\n-0.0017 -0.0032\n0.013\n0.27\n-0.002\n-0.0027 -0.0033\n0.021\n0.27\n0.059\n-0.0035 -0.0017\n0.014\n0.19\n0.033\n0.083\n-0.0023\nInformation gain after each operation\n(a) Our results\n1\n2\n3\n4\n5\nLLM1\nLLM2\nLLM3\nLLM4\nLLM5\n0.012\n0.015\n0.013\n0.014\n0.014\n0.49\n2.2e-08\n7e-05\n1.2e-05 1.4e-06\n0.13\n1.7e-06 1.4e-06 2.1e-06 1.3e-06\n0.5\n0.5\n0.47\n8.7e-07 3.3e-07\n0.52\n0.54\n0.55\n0.52\n0.058\nMean probability of the final answer being correct\n(b) ORM results\n1\n2\n3\n4\n5\nLLM1\nLLM2\nLLM3\nLLM4\nLLM5\n0\n0\n0\n0\n0\n0.37\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2\n0.21\n0.2\n0\n0\n0.17\n0.17\n0.17\n0.18\n0\nProportion of completions with correct final answer\n(c) Math-Shepherd results\nFigure 3: Heatmaps quantifying the correctness of different sub-tasks for the 5 LLMs under consideration ob-\ntained using the different baselines. Here, the red color indicates a significant drop in the plotted metrics and can\nbe seen as an indication of an incorrectly executed sub-task.\nMath-Shepherd [Wang et al., 2024b]\nThis method quantifies the potential for a given reasoning process\nXM\nt\nby using a ‘completer’ model to generate N completions of each reasoning process starting from step t,\n{(XM\nt , XM\nt+1,j, . . . , XM\nT,j, Y M\nj )}j≤N, where Y M\nj\ndenotes the final answer reached in the j’th completion. Then,\nwe estimate the potential of this step based on the proportion of correct answers among the N completions,\ndenoted by fMS(XM\nt ) as:\nfMS(XM\nt ) :=\nN\nX\nj=1\n1(Y M\nj\n= Y )\nN\n.\n(7)\nFor a fair comparison we do not assume access to a ‘verifier’ model more capable than our base model and\ntherefore, we use the base model as the completer model in our experiments.\n5.1\nToy data experiments\nFirst, we consider a toy setting where we have full control over the model behaviour on different tasks. Our\nprompts comprise of an integer vector Z0 ∈Z5 sampled randomly from a given distribution. The task λ com-\nprises 5-steps λ = λ1 ◦. . . ◦λ5, where each sub-task λi denotes an operation which transforms a given integer\nvector Zi−1 ∈Z5 into another Zi ∈Z5. Finally, in this setup, the correct final answer Y is the value of Z5.\nAdditional details on the data generating mechanism as well as the sub-tasks are provided in Appendix B.1.\nGenerating the dataset\nTo investigate partial unidentifiability for a given task λi we modify the obtained\ndataset by introducing ‘noise’ at step i. In other words, the task λi is applied incorrectly on a subset of the\ndata, whereas all other tasks are always applied correctly. This represents a model which sometimes fails at\nstep i and we use ‘LLMi’ to denote this model in this experiment. We repeat this procedure for all tasks λi for\ni ∈{1, . . . , 5} which yields 5 LLMs {LLM1, . . . , LLM5}.\nTo also investigate the robustness of the methods, we introduce a special case in LLM3. Here, task λ3 is applied\nincorrectly if and only if the output after task 2 (i.e., after λ2) lies in some set S. This choice has been made\ndeliberately to highlight a pitfall of the existing baselines (as we will explain below) and is in contrast to the rest\nof LLMs where any errors occur at random. In other words, the correctness of task λ3 is dependent on the output\nof λ2. For more details, see Appendix B.1.2.\n5.1.1\nResults\nFigure 3 shows how the different baselines quantify the correctness of the different tasks for the 5 different\nLLMs under consideration. This figure only considers samples where the final answer of the LLM was incorrect,\ni.e. Y M ̸= Y . For our method (IG), Figure 3a shows the information gain across the different steps for each\nLLM. Likewise, Figure 3b presents the results for ORM and shows how the average probability of correctness\nin (6) changes across the different steps, whereas, for Math-Shepherd, Figure 3c shows the proportion of correct\ncompletions starting after each step (7). Here, any significant drop in the plotted values indicate an incorrect\napplication of a task.\nInformation gain accurately quantifies step-wise correctness\nWe observe that for each LLM the information\ngain remains positive until we encounter an incorrect reasoning step, at which point it drops to negative values.\n7\n\nTable 1: Metrics for sample-wise classification of sub-task correctness for LLM3 using the different baselines.\nMETHOD\nACCURACY ↑\nTPR ↑\nFPR ↓\nIG (OURS)\n0.96\n0.98\n0.06\nORM\n0.77\n0.98\n0.54\nMATH-SHEPHERD\n0.60\n1.0\n1.0\nTherefore, our method can identify the incorrectly executed task for each LLM under consideration. We used a\nGPT-2 supervisor model to estimate information gain.\nPitfall of the baselines\nWhile the ORM and Math-Shepherd manage to correctly identify the incorrect reason-\ning steps in most cases, these methods fail to correctly detect erroneous steps for LLM3. This happens because,\nin our setup, λ3 is incorrectly applied if and only if the output after task λ2 lies in some set S. Therefore, the\nclassifier model can confidently predict the correctness of the final model output at λ2 by simply checking if the\noutput lies in S. Here, the classifier becomes confident that the final output will be wrong right after λ2, even\nthough the error occurs at λ3.\nSimilarly, when using Math-Shepherd for LLM3 (with the same model being used as a completer), a completion\nyields an incorrect final answer if the output after λ2 lies in S. If this is the case, all completions yield an\nincorrect final output regardless of which step we begin the completions from. This makes it impossible to\naccurately identify the step at which LLM3 goes wrong.\nSample-wise detection\nWe can also use the different baselines for sample-wise detection of erroneous steps as\noutlined in Section 3.3. In this setting, for each prompt, we can classify a step as incorrect if a baseline’s metric\nfalls below a threshold. Table 1 shows the results for sample-wise classification of sub-task correctness for LLM3\nusing the different baselines (where we chose the best thresholds for each baseline using a held-out dataset). It\ncan be seen that our method yields a significantly higher accuracy and a lower rate of false-positives than the\nbaselines and therefore, is also considerably more reliable for sample-wise detection of errors.\n5.2\nArithmetic operations on LLama-3-8B\nFollowing our toy experiments, we now evaluate our framework in a more realistic setting using the Llama-3-8B\nmodel [Dubey et al., 2024]. We focus on a simple arithmetic task that involves both multiplication and addition\ntasks. The goal is to assess the model’s performance on individual operations as well as their combination.\nExperimental setup\nWe sample two integers x and y uniformly from the range [1, 100000). The prompt given\nto the model is structured as follows:\nPrompt: “x = {x}, y = {y}, Please calculate the following: 1. 3x, 2. 2y, 3. 3x + 2y ”\nModel accuracy\nWe observe that the model’s accuracy varies across the three steps:\nStep 1 accuracy: 80%,\nStep 2 accuracy: 98%,\nStep 3 accuracy: 42%.\nFigure 4: The distribution of (x, y) for incor-\nrect samples shows a clear trend: Llama-3-\n8B struggles to add large and small numbers\ntogether (top-left and bottom-right).\nNotably, the majority of failures occur in the third step, which\ninvolves addition of the previously computed values. We ana-\nlyzed the distribution of (x, y) values where the model obtains\nthe correct final output. Interestingly, as Figure 4 illustrates, we\nobserved that most errors occur when exactly one of the vari-\nables (x, y) is large and the other is small. This suggests that\nthe model’s correctness is highly dependent on the (x, y) values\nin the prompt, resulting in baselines struggling to identify the er-\nroneous step in the model’s CoT reasoning (as we show below).\n5.2.1\nResults\nOur Method\nWe trained the supervisor model by fine-tuning\na Llama-3-8b model using Low Rank Adaptation (LoRA) [Hu\net al., 2021]. Table 2 shows that there is a significant drop in\ninformation gain at step 3 relative to steps 1 and 2, demonstrating\nthat our information-theoretic method is able to correctly identify\nthat the failure mainly occurs at step 3.\nOutcome Reward Model (ORM)\nIn contrast, for ORM the\nmean probability of correctness included in Table 2 remains un-\nchanged at each step. This could be explained by Figure 4 which\nsuggests that ORM classifier can predict the correctness of the final output using only the values of x and y avail-\nable in the prompt. Crucially, the classifier’s confidence remains unchanged even as the model’s intermediate\n8\n\nreasoning steps are added to the input. Hence, ORM is unable to distinguish between the model’s performance\non intermediate reasoning steps.\nMath-Shepherd\nTable 2 includes the proportion of correct completions for Math-Shepherd. We observe that\neven though this proportion is very small at step 3, we also observe that only about 5-7% of the completions\nstarting from steps 1 and 2 lead to a correct output, even though the error mostly occurs at step 3. This happens\nbecause the correctness of Llama-3-8B is largely determined by the initial values of (x, y) in the prompt (see\nFigure 4). Consequently, Math-Shepherd incorrectly flags steps 1 and 2 as incorrect a significant proportion of\nthe time which leads to a significantly higher proportion of false positives (as compared to our baseline) as we\nshow below.\nTable 2: Metrics for aggregate step-wise correctness of arithmetic operations across prompts, along with sample-\nwise classification of incorrect operations leading to an incorrect final answer.\nSTEP 1: 3x ✓\nSTEP 2: 2y ✓\nSTEP 3: 3x + 2y ✗\nACCURACY ↑\nTPR ↑\nFPR ↓\nIG (OURS)\n0.67\n0.24\n0.027\n0.76\n0.51\n0.02\nORM\n0.24\n0.24\n0.24\n0.56\n0.10\n0.07\nMATH-SHEPHERD\n0.068\n0.059\n0.00069\n0.53\n0.99\n0.86\nSample-wise detection\nWhen using these methods for sample-wise detection of incorrect steps, our approach\nyields the highest accuracy among the baselines considered. This superior performance is attributed to the fact\nthat baselines like ORM and Math-Shepherd often falsely flag steps 1 and 2 as incorrect, as evidenced by their\nhigh false positive rates in Table 2.\n5.3\nExperiments on the Controlled GSM-8K Dataset\nTo evaluate our method on a complex dataset, we conducted experiments on GSM-8K [Cobbe et al., 2021],\ncontrolling specific factors for more interpretable results.\nWe begin by using GPT-4 [OpenAI et al., 2024] to generate answers for GSM-8K questions where the “multipli-\ncation” operation is always done incorrectly, while all other operations are correct. Next, we filtered the dataset\nto ensure that “multiplication”, “subtraction”, and “addition” never appeared together within the same Chain of\nThought (CoT) solution. In particular, we ensured in our setting that, all incorrect final answers included both\n“multiplication” and “subtraction”, whereas correct final answers did not involve either operation. This introduces\na spurious correlation between “subtraction” and wrong answers.\nIn this setup, we mainly focused on evaluating ORM and our proposed method, as Math-Shepherd (with the same\ncompleter) fails trivially under these conditions. Specifically, “multiplication” is inherently unidentifiable, since\nany CoT containing “multiplication” negates the influence of other sub-tasks by design. Further details on the\nexperimental setup can be found in Appendix B.3.\n5.3.1\nResults\nTable 3 demonstrates that our proposed information-theoretic approach successfully identifies the unidentifiable\nsub-task. Since we intentionally set the “multiplication” rules to be incorrect, we observe minimal to no informa-\ntion gain for this operation, as expected. However, a different pattern emerges when we examine the results of the\nORM method. Both “multiplication” and “subtraction” show, on average, a very low probability of correctness.\nThis is due to the fact that both sub-tasks are primarily associated with incorrect final answers. Consequently, re-\nlying on the standard ORM approach could lead to the misleading conclusion that “subtraction” is also incorrect.\nAdditionally, in our sample-wise experiment, we observe a similar trend when we use the methods to assess\nthe sample-wise correctness of “multiplication” and “subtraction” for each prompt. Here, our proposed method\nnot only accurately detects the unidentifiable sub-task but also highlights a significant shortcoming of ORM.\nSpecifically, ORM falsely flags “subtraction”, which is actually correct, as an incorrect sub-task due to spurious\ncorrelations.\nTable 3: Comparison between our method and ORM for different sub-tasks in GSM-8K. The final three columns\ninclude results for sample-wise classification of incorrect operations for each prompt.\nADDITION ✓\nMULTIPLICATION ✗\nDIVISION ✓\nSUBTRACTION ✓\nACCURACY ↑\nTPR ↑\nFPR ↓\nIG (OURS)\n0.99\n0.026\n1.05\n1.06\n0.72\n0.95\n0.62\nORM\n0.46\n0.024\n0.38\n0.013\n0.58\n1.0\n1.0\n6\nDiscussion and Limitations\nIn this paper, we introduce a novel information-theoretic approach for evaluating Chain-of-Thought (CoT) rea-\nsoning in large language models (LLMs) without the need for annotated intermediate steps. We present a compre-\nhensive framework for modeling the CoT process, and the results demonstrate the effectiveness of our algorithm\n9\n\nin identifying erroneous reasoning steps across diverse experimental settings. We consistently outperform ex-\nisting baselines, including Outcome Reward Models (ORM) [Havrilla et al., 2024] and Math-Shepherd [Wang\net al., 2024b] as shown in our extensive experimental section. However, it’s important to note that that are some\nlimitations to our approach.\nFor example, our method necessitates additional training of the supervisor model, which can be computationally\ndemanding. Future research could investigate the use of in-context learning techniques to estimate information\ngain, potentially reducing the need for extra training and enhancing both the accessibility and efficiency of the\napproach. Secondly, sample-wise detection introduces further challenges that may lead to erroneous conclusions.\nA language model may occasionally arrive at the correct answer by chance, even if a particular sub-task is\nunidentifiable. Although this occurrence should not significantly impact the overall task-wise information gain,\nit could result in inaccurate outcomes for sample-wise information gain in such ‘lucky’ cases. Finally, while our\nmethod does not require correctness labels for every step, we still need to categorize each step according to its\nrespective sub-task. However, this limitation is not unique to our model, as both ORM and Math-Shepherd also\nrely on such labels to draw sub-task-specific conclusions.\nReferences\nA. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi. Mathqa: Towards interpretable\nmath word problem solving with operation-based formalisms. CoRR, abs/1905.13319, 2019. URL http:\n//arxiv.org/abs/1905.13319.\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen,\nE. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick,\nK. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha,\nJ. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery,\nC. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. D´ıaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber,\nM. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland,\nA. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta,\nC. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni,\nA. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pel-\nlat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta,\nR. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vo-\ndrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu,\nQ. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023. URL\nhttps://arxiv.org/abs/2305.10403.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg,\nH. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4, 2023. URL https://arxiv.org/abs/2303.12712.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prab-\nhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke,\nA. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou,\nD. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M.\nDai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang,\nB. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel.\nPalm: scaling language modeling with pathways. J. Mach. Learn. Res., 24(1), mar 2024. ISSN 1532-4435.\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,\nA. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson,\nD. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean,\nJ. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. Scaling instruction-finetuned language models, 2022.\nURL https://arxiv.org/abs/2210.11416.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\nC. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168.\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan,\nA. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang,\nA. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak,\nC. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Al-\nlonsius, D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino,\n10\n\nD. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Zhang, G. Syn-\nnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu,\nH. Touvron, I. Zarov, I. A. Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Copet, J. Lee, J. Geffert, J. Vranes,\nJ. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu,\nJ. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Upasani,\nK. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, L. Rantala-Yeary,\nL. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat,\nL. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Oldham, M. Rita, M. Pavlova,\nM. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev,\nN. Chatterji, O. Duchenne, O. C¸ elebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal,\nP. Krishnan, P. S. Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. S. Cabral,\nR. Stojnic, R. Raileanu, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva,\nR. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang,\nS. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Col-\nlot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, T. Speckbacher,\nT. Mihaylov, T. Xiao, U. Karn, V. Goswami, V. Gupta, V. Ramanathan, V. Kerkez, V. Gonguet, V. Do, V. Vo-\ngeti, V. Petrovic, W. Chu, W. Xiong, W. Fu, W. Meers, X. Martinet, X. Wang, X. E. Tan, X. Xie, X. Jia,\nX. Wang, Y. Goldschlag, Y. Gaur, Y. Babaei, Y. Wen, Y. Song, Y. Zhang, Y. Li, Y. Mao, Z. D. Coudert, Z. Yan,\nZ. Chen, Z. Papakipos, A. Singh, A. Grattafiori, A. Jain, A. Kelsey, A. Shajnfeld, A. Gangidi, A. Victoria,\nA. Goldstand, A. Menon, A. Sharma, A. Boesenberg, A. Vaughan, A. Baevski, A. Feinstein, A. Kallet, A. San-\ngani, A. Yunus, A. Lupu, A. Alvarado, A. Caples, A. Gu, A. Ho, A. Poulton, A. Ryan, A. Ramchandani,\nA. Franco, A. Saraf, A. Chowdhury, A. Gabriel, A. Bharambe, A. Eisenman, A. Yazdan, B. James, B. Maurer,\nB. Leonhardi, B. Huang, B. Loyd, B. D. Paola, B. Paranjape, B. Liu, B. Wu, B. Ni, B. Hancock, B. Wasti,\nB. Spence, B. Stojkovic, B. Gamido, B. Montalvo, C. Parker, C. Burton, C. Mejia, C. Wang, C. Kim, C. Zhou,\nC. Hu, C.-H. Chu, C. Cai, C. Tindal, C. Feichtenhofer, D. Civin, D. Beaty, D. Kreymer, D. Li, D. Wyatt,\nD. Adkins, D. Xu, D. Testuggine, D. David, D. Parikh, D. Liskovich, D. Foss, D. Wang, D. Le, D. Holland,\nE. Dowling, E. Jamil, E. Montgomery, E. Presani, E. Hahn, E. Wood, E. Brinkman, E. Arcaute, E. Dunbar,\nE. Smothers, F. Sun, F. Kreuk, F. Tian, F. Ozgenel, F. Caggioni, F. Guzm´an, F. Kanayet, F. Seide, G. M.\nFlorez, G. Schwarz, G. Badeer, G. Swee, G. Halpern, G. Thattai, G. Herman, G. Sizov, Guangyi, Zhang,\nG. Lakshminarayanan, H. Shojanazeri, H. Zou, H. Wang, H. Zha, H. Habeeb, H. Rudolph, H. Suk, H. Aspe-\ngren, H. Goldman, I. Damlaj, I. Molybog, I. Tufanov, I.-E. Veliche, I. Gat, J. Weissman, J. Geboski, J. Kohli,\nJ. Asher, J.-B. Gaya, J. Marcus, J. Tang, J. Chan, J. Zhen, J. Reizenstein, J. Teboul, J. Zhong, J. Jin, J. Yang,\nJ. Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Sax-\nena, K. Prasad, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Huang,\nK. Chawla, K. Lakhotia, K. Huang, L. Chen, L. Garg, L. A, L. Silva, L. Bell, L. Zhang, L. Guo, L. Yu,\nL. Moshkovich, L. Wehrstedt, M. Khabsa, M. Avalani, M. Bhatt, M. Tsimpoukelli, M. Mankus, M. Hasson,\nM. Lennie, M. Reso, M. Groshev, M. Naumov, M. Lathi, M. Keneally, M. L. Seltzer, M. Valko, M. Restrepo,\nM. Patel, M. Vyatskov, M. Samvelyan, M. Clark, M. Macey, M. Wang, M. J. Hermoso, M. Metanat, M. Raste-\ngari, M. Bansal, N. Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. P. Laptev,\nN. Dong, N. Zhang, N. Cheng, O. Chernoguz, O. Hart, O. Salpekar, O. Kalinli, P. Kent, P. Parekh, P. Saab,\nP. Balaji, P. Rittner, P. Bontrager, P. Roux, P. Dollar, P. Zvyagina, P. Ratanchandani, P. Yuvraj, Q. Liang,\nR. Alao, R. Rodriguez, R. Ayub, R. Murthy, R. Nayani, R. Mitra, R. Li, R. Hogan, R. Battey, R. Wang,\nR. Maheswari, R. Howes, R. Rinott, S. J. Bondu, S. Datta, S. Chugh, S. Hunt, S. Dhillon, S. Sidorov, S. Pan,\nS. Verma, S. Yamamoto, S. Ramaswamy, S. Lindsay, S. Lindsay, S. Feng, S. Lin, S. C. Zha, S. Shankar,\nS. Zhang, S. Zhang, S. Wang, S. Agarwal, S. Sajuyigbe, S. Chintala, S. Max, S. Chen, S. Kehoe, S. Satter-\nfield, S. Govindaprasad, S. Gupta, S. Cho, S. Virk, S. Subramanian, S. Choudhury, S. Goldman, T. Remez,\nT. Glaser, T. Best, T. Kohler, T. Robinson, T. Li, T. Zhang, T. Matthews, T. Chou, T. Shaked, V. Vontimitta,\nV. Ajayi, V. Montanez, V. Mohan, V. S. Kumar, V. Mangla, V. Albiero, V. Ionescu, V. Poenaru, V. T. Mihailescu,\nV. Ivanov, W. Li, W. Wang, W. Jiang, W. Bouaziz, W. Constable, X. Tang, X. Wang, X. Wu, X. Wang, X. Xia,\nX. Wu, X. Gao, Y. Chen, Y. Hu, Y. Jia, Y. Qi, Y. Li, Y. Zhang, Y. Zhang, Y. Adi, Y. Nam, Yu, Wang, Y. Hao,\nY. Qian, Y. He, Z. Rait, Z. DeVito, Z. Rosnbrick, Z. Wen, Z. Yang, and Z. Zhao. The llama 3 herd of models,\n2024. URL https://arxiv.org/abs/2407.21783.\nG. Feng, B. Zhang, Y. Gu, H. Ye, D. He, and L. Wang. Towards revealing the mystery behind chain of thought: A\ntheoretical perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=qHrADgAdYu.\nJ. Gonz´alez and A. V. Nori. Beyond words: A mathematical framework for interpreting large language models.\narXiv preprint arXiv:2311.03033, 2023.\nA. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu, M. Zhuravinskyi, E. Hambro, and R. Raileanu. GLore:\nWhen, where, and how to improve LLM reasoning via global and local refinements. In Forty-first International\nConference on Machine Learning, 2024. URL https://openreview.net/forum?id=LH6R06NxdB.\n11\n\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen. Lora: Low-rank adaptation of large\nlanguage models. CoRR, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685.\nA. Jacovi, Y. Bitton, B. Bohnet, J. Herzig, O. Honovich, M. Tseng, M. Collins, R. Aharoni, and M. Geva. A\nchain-of-thought is as strong as its weakest link: A benchmark for verifiers of reasoning chains, 2024.\nN. Joshi, H. Zhang, K. Kalyanaraman, Z. Hu, K. Chellapilla, H. He, and L. E. Li. Improving multi-hop reasoning\nin LLMs by learning from rich human feedback. In Neuro-Symbolic Learning and Reasoning in the era of\nLarge Language Models, 2023. URL https://openreview.net/forum?id=wxfqhp9bNR.\nY. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. Making large language models better reasoners\nwith step-aware verifier, 2023.\nZ. Li, H. Liu, D. Zhou, and T. Ma. Chain of thought empowers transformers to solve inherently serial problems,\n2024. URL https://arxiv.org/abs/2402.12875.\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and\nK. Cobbe. Let’s verify step by step, 2023.\nJ. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang. Logiqa: A challenge dataset for machine reading com-\nprehension with logical reasoning. CoRR, abs/2007.08124, 2020. URL https://arxiv.org/abs/2007.\n08124.\nS. McLeish, A. Bansal, A. Stein, N. Jain, J. Kirchenbauer, B. R. Bartoldson, B. Kailkhura, A. Bhatele, J. Geiping,\nA. Schwarzschild, and T. Goldstein. Transformers can do arithmetic with the right embeddings, 2024.\nM.-V. Nguyen, L. Luo, F. Shiri, D. Q. Phung, Y.-F. Li, T.-T. Vu, and G. Haffari. Direct evaluation of chain-of-\nthought in multi-hop reasoning with knowledge graphs. ArXiv, abs/2402.11199, 2024. URL https://api.\nsemanticscholar.org/CorpusID:267751000.\nM. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz,\nM. Bosma, D. Luan, C. Sutton, and A. Odena. Show your work: Scratchpads for intermediate computation\nwith language models, 2021. URL https://arxiv.org/abs/2112.00114.\nOpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\nS. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian,\nJ. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L.\nBrakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carl-\nson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess,\nC. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Dev-\nille, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus,\nN. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh,\nR. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han,\nJ. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu,\nS. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn,\nH. Jun, T. Kaftan, Łukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W.\nKim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Łukasz Kondraciuk, A. Kondrich,\nA. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy,\nC. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Man-\nning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey,\nP. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco,\nE. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. M´ely, A. Nair, R. Nakano, R. Nayak, A. Neelakan-\ntan, R. Ngo, H. Noh, L. Ouyang, C. O’Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascan-\ndolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov,\nH. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl,\nR. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez,\nN. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Sel-\nsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin,\nK. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang,\nN. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C.\nUribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei,\nC. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich,\nH. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers,\nC. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024. URL\nhttps://arxiv.org/abs/2303.08774.\n12\n\nY. Razeghi, R. L. L. I. au2, M. Gardner, and S. Singh. Impact of pretraining term frequencies on few-shot\nreasoning, 2022. URL https://arxiv.org/abs/2202.07206.\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving\nmath word problems with process- and outcome-based feedback, 2022.\nB. Wang, X. Yue, Y. Su, and H. Sun. Grokked transformers are implicit reasoners: A mechanistic journey to the\nedge of generalization, 2024a.\nP. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce\nllms step-by-step without human annotations, 2024b. URL https://arxiv.org/abs/2312.08935.\nZ. Wang, Y. Li, Y. Wu, L. Luo, L. Hou, H. Yu, and J. Shang. Multi-step problem solving through a verifier: An\nempirical analysis on model-induced process supervision, 2024c. URL https://arxiv.org/abs/2402.\n02658.\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language\nmodels are zero-shot learners. In International Conference on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=gEZrGCozdqR.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought\nprompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on\nNeural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN\n9781713871088.\nZ. Xi, W. Chen, B. Hong, S. Jin, R. Zheng, W. He, Y. Ding, S. Liu, X. Guo, J. Wang, H. Guo, W. Shen, X. Fan,\nY. Zhou, S. Dou, X. Wang, X. Zhang, P. Sun, T. Gui, Q. Zhang, and X. Huang. Training large language models\nfor reasoning through reverse curriculum reinforcement learning, 2024.\nX. Xie, J. Song, Z. Zhou, Y. Huang, D. Song, and L. Ma. Online safety analysis for llms: a benchmark, an\nassessment, and a path forward, 2024.\nZ. Xu, S. Jain, and M. Kankanhalli. Hallucination is inevitable: An innate limitation of large language models,\n2024.\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap\nyour own mathematical questions for large language models, 2024.\n13\n\nA\nProofs\nProof of Theorem 3.3. Suppose λ and λ′ are two tasks with primitive decompositions\nλ′ = λ′\n1 ◦· · · ◦λ′\nT ′\nand\nλ = λ1 ◦· · · ◦λT ,\n(8)\nwhere arg mint{λt ̸∈Span({λ′\n1, . . . , λ′\nT ′})} ≤k. In other words, the primitive decompositions of λ′ and λ\ndiverge before step k + 1. Then, Assumption 3.1 implies that for any j ≥k, we have that the answer Y and X′\nj\nare d-separated by X′\nj−1. Therefore,\nY ⊥⊥X′\nj | X′\nj−1.\nNext, we know from Assumption 3.2 that there exists some task ˆλ ∈Span(ΓM) (possibly dependent on X0 and\nλ) such that ΛM(X0, λ)\nd= Λ(X0, ˆλ). Suppose that ˆλ has primitive decomposition\nˆλ = ˜λ1 ◦· · · ◦˜λ ˜T ,\nthen since ˆλ ∈Span(ΓM), we know that ˜λi ∈ΓM for i ∈{1, . . . , ˜T}. If the primitive decomposition of λ in\n(8) is such that k = arg mint{λt ̸∈Span(ΓM)}, then we know that arg mint{λt ̸∈Span({˜λ1, . . . , ˜λ ˜T })} ≤k.\nThen, from the above it follows that\nY ⊥⊥XM\nj\n| XM\nj−1.\nHere, we used the fact that XM\nj\nd= Λ(X0, ˜λ1 ◦· · · ◦˜λj) using Assumption 3.2.\nProof of Proposition 3.4.\nE[log p(Y | XM\nj )] −E[log p(Y | XM\nj−1)] = E\n\"\nlog p(Y | XM\nj )\np(Y | XM\nj−1)\n#\n= E\n\"\nlog p(Y | XM\nj , XM\nj−1)\np(Y | XM\nj−1)\n#\n= E\n\"\nlog\np(Y, XM\nj\n| XM\nj−1)\np(Y | XM\nj−1) p(XM\nj\n| XM\nj−1)\n#\n= I(Y, XM\nj\n| XM\nj−1)\nHere, the second equality above arises from the fact that XM\nj\nalso captures all the information captured in XM\nj−1\n(and possibly more). Therefore, conditional on XM\nj , the state XM\nj−1 is deterministic and hence, Y ⊥⊥XM\nj−1 |\nXM\nj .\nB\nAdditional Experimental Details\nB.1\nToy Data Experiments\nIn this section, we describe the exact procedure used to generate the toy data for training and evaluating the\nmodels in our experiments. The dataset is constructed through five sequential operations (or tasks) applied to an\ninitial state z0, where each task λi generates an intermediate state zi. Both correct and incorrect examples were\ngenerated, with incorrect examples created by introducing random noise or permutations into the transformations.\nThe data was used to represent models LLM1, LLM2, ..., LLM5, each corresponding to a setting where a specific\ntask λi was partially corrupted to simulate an unidentifiable task for that model.\nB.1.1\nData Generation Tasks\nFor each prompt, an initial 5-element vector z0 was randomly sampled, and we use the notation z0[i] to denote\nthe i’th component of this vector. Next, the following tasks were applied sequentially:\n14\n\nTask λ1: Pairwise Swapping\n• Correct Mapping: The first and second elements, as well as the third and fourth elements of z0, are\nswapped:\nz1[0], z1[1], z1[2], z1[3] = z0[1], z0[0], z0[3], z0[2]\n• Incorrect Mapping: The entire vector is shuffled randomly.\nTask λ2: Cumulative Summation\n• Correct Mapping: The first three elements of z1 are replaced by their cumulative sum, and the fourth\nand fifth elements are swapped:\nz2 = [z1[0], z1[0] + z1[1], z1[0] + z1[1] + z1[2], z1[4], z1[3]]\n• Incorrect Mapping: Each element of z1 is perturbed by adding a random integer between 10 and 99:\nz2[i] = z1[i] + Ui\nfor each i where Ui is a randomly sampled integer between 10 and 99\nTask λ3: Reverse and Cumulative Sum\n• Correct Mapping: The first three elements of z2 are reversed, and the last two elements are replaced by\ntheir cumulative sum:\nz3 = [z2[2], z2[1], z2[0], z2[3], z2[3] + z2[4]]\n• Incorrect Mapping: As with task λ2, each element of z2 is perturbed by adding a random integer between\n10 and 99.\nTask λ4: Sorting and Elementwise Multiplication\n• Correct Mapping: The vector z3 is sorted, and the first four elements are replaced by element-wise\nmultiplications of specific pairs:\nz4[0] = z3[1] × z3[2],\nz4[1] = z3[0] × z3[3],\nz4[2] = z3[4] × z3[0],\nz4[3] = z3[2] × z3[2]\n• Incorrect Mapping: The vector is randomly shuffled.\nTask λ5: Difference Calculation\n• Correct Mapping: The first element is replaced by the absolute difference of the first two elements of\nz4, and other elements are transformed as follows:\nz5 = [|z4[0] −z4[1]|, z4[2], z4[3], |z4[3] −z4[4]|, z4[0]]\n• Incorrect Mapping: The vector is randomly shuffled.\nB.1.2\nModels LLM1, LLM2, . . . , LLM5\nFor each model LLMi (i ∈{1, 2, 3, 4, 5}), the task λi was selectively corrupted to simulate unidentifiability for\nthat task. Specifically:\n• Correct Data: The task λi was applied according to its correct mapping.\n• Incorrect Data: The task λi was applied using its incorrect mapping (random noise, shuffling, or pertur-\nbations).\nFor each LLMi, the tasks λ1 to λi−1 and λi+1 to λ5 were correctly applied, but task λi was corrupted for a subset\nof the data. More specifically, for all LLMs except LLM3, the error was introduced at step i at random with\nprobability 0.5. In contrast, for LLM3, the error was introduced at step 3 if and only if the output at step 2, z2\nsatisfies, z2[2] > 150. This choice was deliberately made to highlight a pitfall of the baselines as explained in\nSection 5.\nString Representation of Chain-of-Thought (CoT)\nNext, we convert each sequence of vectors z0, z1, . . . , z5\nproduced by the tasks into a string-based Chain-of-Thought (CoT) representation. Each intermediate state vector\nzi is expressed as a comma-separated list of its elements, and the transitions between the states are delimited by\n“||”. This format explicitly captures the step-by-step reasoning process of the model.\nFor example, given an initial vector z0 = [83, 48, 14, 98, 25], applying the tasks sequentially yields intermediate\nstates z1, z2, . . . , z5. These states are concatenated into a single string, separated by “||” to represent the full\nreasoning chain:\n83,48,14,98,25 || 48,83,98,14,25\n||\n48,131,229,25,14 ||\n229,131,48,25,39\n||\n1872,3275,5725,2304,229\n||\n1403,5725,2304,2075,1872\n15\n\nB.1.3\nTraining the supervisor model\nTo estimate the information gain in (3), we train a different LLM, which we refer to as the supervisor model gsup.\nAs explained in Section 3.3, this model takes as input the model’s CoT reasoning up to any given intermediate\nstep t, XM\nt , and is fine-tuned to directly predict the ground truth final output Y . To this end, we use a special\ntoken to separate the model’s CoT reasoning and the final output when fine-tuning gsup. At inference time, this\nspecial token when appended to the model input serves as an indication for the model to directly predict the final\noutput. In this way gsup(XM\nt ) approximates the conditional distribution p(Y | XM\nt ).\nMore specifically, in the toy setup discussed above, consider the following sample for model’s CoT:\n83,48,14,98,25 || 48,83,98,14,25\n||\n48,131,229,25,14 ||\n229,131,48,25,39\n||\n1872,3275,5725,2304,229\n||\n1403,5725,2304,2075,1872\nFor this example, the ground truth final output y is y = “1403,5725,2304,2075,1872′′ (i.e., the model reached\nthe correct final output in the example above).\nFor the sample given above, we have that\nxM\n0 = x0 = “83,48,14,98,25′′\nxM\n1 = “83,48,14,98,25 || 48,83,98,14,25 ′′\n...\nxM\n5 = “83,48,14,98,25 || 48,83,98,14,25\n||\n48,131,229,25,14 ||\n229,131,48,25,39\n|| 1872,3275,5725,2304,229\n||\n1403,5725,2304,2075,1872′′\nNext, to construct the data for fine-tuning the supervisor model, we used the special token “#|>′′ to separate the\nmodel’s CoT steps xM\ni\nfrom the ground truth output y. This results in the following 6 training datapoints for the\nsupervisor model:\n1. “83,48,14,98,25 #|> 1403,5725,2304,2075,1872”\n2. “83,48,14,98,25 || 48,83,98,14,25\n#|> 1403,5725,2304,2075,1872”\n...\n5. “83,48,14,98,25 || 48,83,98,14,25 || 48,131,229,25,14 || 229,131,48,25,39\n|| 1872,3275,5725,2304,229 || 1403,5725,2304,2075,1872\n#|>\n1403,5725,2304,2075,1872”\nThe above procedure allows us to obtain fine-tuning data for supervisor models separately for each of the 5\ndifferent LLMs, {LLM1, LLM2, . . . , LLM5}. Next, we train a separate GPT-2 model for each of the 5 different\nbase LLMs.\nB.1.4\nEstimating the information gain\nHaving trained the supervisor model on the data generated above, we evaluate the information gain on a held-out\ndataset split. Given a datapoint (xM\ni , y) in the evaluation split, we can estimate the sample-wise information gain\nat step i as follows:\n• Suppose that the model generation at step i −1, xM\ni−1 is tokenised as (t1, . . . , tni−1) and similarly that\nxM\ni\nis tokenised as (t1, . . . , tni). Likewise, suppose that the true output y is tokenised as (t∗\n1, . . . , t∗\nk)\nand we use < s > to denote the separator token (i.e. #|> above).\n• Then, to estimate the sample-wise for this datapoint, we estimate the difference:\n1\nk\nk\nX\nj=1\nlog p(t∗\nj | (t1, . . . , tni, < s >, t∗\n1, . . . , t∗\nj−1))\n−1\nk\nk\nX\nj=1\nlog p(t∗\nj | (t1, . . . , tni−1, < s >, t∗\n1, . . . , t∗\nj−1)).\nHere, the supervisor model is trained to estimate the above conditional and therefore we use it to estimate\nthe difference above.\nFinally, to estimate the aggregate information gain (instead of the sample-wise information gain), we simply\ncompute the average sample-wise gain over the evaluation data split.\n16\n\nB.1.5\nAdditional results\nIn Figures 5 - 7, we present the sample-wise trajectories for 15 randomly chosen prompts leading to incorrect\nfinal answers, for the different baselines and LLMs under consideration. Here, any significant drop in the plotted\nvalue at a given step could be seen as an indication of an incorrectly executed sub-task. Recall that in our setup,\nin LLMi, the CoT step i is executed incorrectly with some probability whereas all other steps are always executed\ncorrectly.\nFirstly, Figure 5 presents sample-wise information gain for our method for the five different LLMs. Here, we see\nthat the sample-wise information remains high up until the incorrect step, at which point the information gain\nsharply decreases. This suggests that sample-wise information gain is sensitive to the specific point where the\nChain of Thought goes wrong, making it effective at locating reasoning errors.\nFor the ORM and Math-Shepherd baselines in Figures 6 and 7, we observe that for all LLMs except LLM3, the\nplotted metrics drop at the incorrect step. However, for LLM3, we observe that ORM’s probability of correctness\ndrops at step 2 even though the error occurs at step 3. This occurs because, in our setup, the correctness of step 3\nis determined directly from the output of step 2. Specifically, recall that in LLM3, step 3 is executed incorrectly\nif and only if the output of step 2, z2, has its second component greater than 150, i.e. z2[2] > 150. Therefore,\nORM becomes confident after the second step if a CoT is going to lead towards the correct final answer or not.\nSimilarly, for Math-Shepherd in Figure 7, we observe that the proportion of correct completions remains 0 for\nLLM3. This is because for all trajectories plotted, the output of step 2, z2, has its second component greater than\n150 and therefore the final answer is incorrect regardless of which step we begin the completions from.\n17\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nResults for LLM1\nSample-wise information gain\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\n0.0\n0.1\n0.2\n0.3\nResults for LLM2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nResults for LLM3\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nResults for LLM4\n1\n2\n3\n4\n5\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nResults for LLM5\nFigure 5: Toy data results: Sample-wise information gain trajectories for 15 randomly chosen prompts with\nwrong final answers.\n18\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nResults for LLM1\nProbability of final answer being correct\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nResults for LLM2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nResults for LLM3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nResults for LLM4\nprompt\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResults for LLM5\nFigure 6: Toy data results: ORM’s probability of correctness after each step for 15 randomly chosen prompts\nwith wrong final answers\n19\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResults for LLM1\nProportion of correct completions after each step\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResults for LLM2\n0.04\n0.02\n0.00\n0.02\n0.04\nResults for LLM3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResults for LLM4\nprompt\n1\n2\n3\n4\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nResults for LLM5\nFigure 7: Toy data results: Math-Shepherd’s proportion of correct completions from each step for 15 randomly\nchosen prompts with wrong final answers\n20\n\nB.2\nArithmetic Operations on LLama 3 8b\nFor this experiment, the prompts used to collect the data follow a specific structure. Each prompt contains two\nreal examples followed by a query with newly sampled values for x and y. The format of the prompt is as follows:\nx = 23, y = 51. Please calculate the following:\n1. 3x\n2. 2y\n3. 3x + 2y\nAnswer:\n1. 3x = 69\n2. 2y = 102\n3. 3x + 2y = 171\nx = 35, y = 60. Please calculate the following:\n1. 3x\n2. 2y\n3. 3x + 2y\nAnswer:\n1. 3x = 105\n2. 2y = 120\n3. 3x + 2y = 225\nx = {x}, y = {y}. Please calculate the following:\n1. 3x\n2. 2y\n3. 3x + 2y\nAnswer:\nIn the third section, the values of x and y are randomly sampled from a uniform distribution over the range\n[1, 100000).\nB.2.1\nTraining Data for the Supervisor Model\nThe supervisor model plays a crucial role in evaluating the intermediate steps in the Chain-of-Thought (CoT)\nreasoning. The model is designed to approximate the probability of arriving at the correct final result after any\ngiven step in the CoT process. To train this model, we fine-tune it using a dataset composed of generated CoT\nsteps concatenated with the correct final result.\nModel Generation Example:\nConsider the following example of a model-generated response:\nx = 51290.0, y = 90718.0. Please calculate the following:\n1. 3x\n2. 2y\n3. 3x + 2y\nAnswer:\n1. 3x = 153770.0\n2. 2y = 181436.0\n3. 3x + 2y = 335206.0\nFine-Tuning Data Construction:\nThe generated outputs are used to construct training examples, where each\nintermediate step is concatenated with the final correct answer using the separator token ‘#|>’. For instance,\nfrom the example above, the following four training data points are created:\n1. \"x = 51290.0, y = 90718.0.\nPlease calculate the following:\n1.\n3x 2.\n2y 3.\n3x + 2y Answer:\n#|> 3x + 2y = 335306.0\"\n2. \"x = 51290.0, y = 90718.0.\nPlease calculate the following:\n1.\n3x 2.\n2y 3.\n3x + 2y Answer:\n|| 1.\n3x = 153770.0 #|> 3x + 2y = 335306.0\"\n3. \"x = 51290.0, y = 90718.0.\nPlease calculate the following:\n1.\n3x 2.\n2y\n3.\n3x + 2y Answer:\n|| 1.\n3x = 153770.0 || 2.\n2y = 181436.0 #|> 3x + 2y =\n335306.0\"\n4. \"x = 51290.0, y = 90718.0.\nPlease calculate the following:\n1.\n3x 2.\n2y 3.\n3x + 2y Answer:\n|| 1.\n3x = 153770.0 || 2.\n2y = 181436.0 || 3.\n3x + 2y =\n335206.0 #|> 3x + 2y = 335306.0\"\n21\n\nEach step concatenates the current state of reasoning with the correct final answer. This process enables the\nsupervisor model to learn the relationship between intermediate steps and the correct final outcome.\nFinally, using the dataset generated above, we fine-tune a Llama-3-8b model using Low Rank Adaptation (LoRA)\n[Hu et al., 2021] as the supervisor model. Finally, the information gain is computed using the trained model as\ndescribed in Section B.1.4.\nB.2.2\nMath Shepherd Results\nThe Math-Shepherd approach [Wang et al., 2024b] evaluates how well the model generates intermediate results\nand completes the reasoning process step-by-step. For a given model generation, we iteratively cut off the chain\nof reasoning after each step and obtain multiple completions using a completer model (in this case, also the\nLlama-3-8B model).\nConsider the following model generation:\nx = 51290.0, y = 90718.0. Please calculate the following:\n1. 3x\n2. 2y\n3. 3x + 2y\nAnswer: 1. 3x = 153770.0, 2. 2y = 181436.0, 3. 3x + 2y = 335206.0\nIn this example, the model completes the full sequence of steps for x = 51290.0 and y = 90718.0. To assess the\nrobustness of the Chain-of-Thought (CoT) process, we perform the following procedure for the Math Shepherd\nresults:\n1. Step-wise Completion: We cut off the generation after each step in the reasoning process. For instance,\nafter computing 3x = 153770.0, we stop the generation there and generate 10 completions using the\nLlama-3-8b model.\n2. Multiple Completions: At each cut-off point, the Llama-3-8b model is tasked with completing the re-\nmaining steps of the chain of reasoning. For each step, 10 independent completions are generated.\n3. Proportion of Correct Completions: For each cut-off point, we compute the proportion of correct\ncompletions. This proportion gives insight into how likely the model is to complete the remaining\nsteps of reasoning correctly, starting from the intermediate point. For example, after cutting off the\nreasoning at 3x = 153770.0, we evaluate how many of the 10 completions successfully compute\n3x + 2y = 335306.0.\nIn this way, Math-Shepherd quantifies the model’s ability to continue reasoning correctly at each intermediate\nstage.\nB.2.3\nAdditional results\nFigures 8 - 10 present the sample-wise trajectories for 15 randomly chosen prompts leading to incorrect final\nanswers for the different baselines. Here, once again, any significant drop in the plotted value at a given step\ncould be seen as an indication of an incorrectly executed sub-task. Recall that in this setup majority of the errors\noccur at the final step which involves the addition of 3x + 2y.\nFigure 8 shows the sample-wise information gain for our method after each step. We see that for most of the\nplotted trajectories, the sample-wise information gain remains high until the final step, at which point it drops to\nvalues close to or below 0. This shows that our method correctly identifies that the failure predominantly occurs\nat step 3.\nIn contrast, Figure 9 shows that the mean probability of correctness for the ORM remains unchanged at each\nstep. This could be explained by Figure 4 in the main text, which suggests that the ORM classifier can predict the\ncorrectness of the final output using only the values of x and y available in the prompt. Crucially, the classifier’s\nconfidence remains unchanged even as the model’s intermediate reasoning steps are added to the input. This\nmeans that ORM is unable to distinguish between the model’s performance on intermediate reasoning steps.\nFor Math-Shepherd results shown in Figure 10, most of the trajectories plotted remain constant at 0. In other\nwords, when using Llama-3-8B as the completer model, we observe that for most of the prompts, no completion\nleads to the correct answer, regardless of which step we begin the completions from. This is likely because, for\nmost of the examples considered in this plot, the (x, y) combination in the prompt has exactly one small value\nand the other is large (as shown in Figure 4). This also highlights why Math-Shepherd has a high false positive\nrate.\n22\n\n3x\n2y\n3x + 2y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nInformation gain after each step\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\nFigure 8: Arithmetic operations on Llama-3-8b: Sample-wise information gain trajectories for 15 randomly\nchosen prompts with wrong final answers\nprompt\n3x\n2y\n3x + 2y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProbability of final answer being correct\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\nFigure 9: Arithmetic operations on Llama-3-8b: ORM’s probability of correctness after each step for 15 randomly\nchosen prompts with wrong final answers\nprompt\n3x\n2y\n3x + 2y\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProportion of correct completions\nCoT before an incorrectly executed step\nCoT after an incorrectly executed step\nIncorrectly executed step\nFigure 10: Arithmetic operations on Llama-3-8b: Math-Shepherd’s proportion of correct completions from each\nstep for 15 randomly chosen prompts with wrong final answers\n23\n\nB.3\nControlled GSM-8K Experiments\nIn order to understand if our proposed method also works on more textual data, we set out to perform an experi-\nment on the popular GSM-8K dataset which has more elaborate prompts compared to the previous experiments.\nTo this end, we follow these steps:\n• We first construct the dataset by using the GPT-4 API on the question. This will give us the basis for\ncorrect CoTs.\n• Next we also again use GPT-4 to label each of the intermediate steps as either using “Addition”, “Sub-\ntraction”, “Division” or “Multiplication”.\n• With this data in hand, we can now construct our unidentifiable operations. In particular, we again use\nGPT-4 to rewrite all the CoTs which contain a multiplication into CoTs, where the multiplication is\nperformed wrong and subsequently also the final answer is wrong.\n• Finally, we filter the final dataset, where we make sure that for every failed CoT, we have both “mul-\ntiplication” and “subtraction” in the CoT, and for every correct CoT, we do not have these operations.\nThis leads us to a setting where standard methods such as ORM would likely fail as there is no way to\ndistinguish between multiplication and subtraction for the classifier.\nB.3.1\nExamples\nHere below are some examples from the dataset, where we start with the prompt, followed by the CoT generations\n(with a wrong multiplication).\nPrompt 1:\nJulie is reading a 120-page book.\nYesterday, she was able to read 12 pages and\ntoday, she read twice as many pages as yesterday.\nIf she wants to read half of the\nremaining pages tomorrow, how many pages should she read?\nModel response 1:\nCalculate today’s pages read by doubling yesterday’s pages:\n12 * 2 = 46.\nCalculate\nthe total pages read so far by adding yesterday’s and today’s pages:\n12 + 46 = 58.\nDetermine the remaining pages by subtracting the pages read so far from the total:\n120 - 58 = 62.\nCalculate the number of pages Julie should read tomorrow by taking\nhalf of the remaining pages:\n62 / 2 = 31.\nPrompt 2:\nIn a conference room, 40 chairs with a capacity of 2 people each were arranged in\nrows in preparation for the board meeting of a company, whose number of members was\nthe same as the chairs’ capacity.\nIf 2/5 of the chairs were not occupied, and the\nrest each had two people, calculate the number of board members who did attend the\nmeeting.\nModel response 2:\nCalculate the total capacity of the conference room:\n40 chairs * 2 people per chair\n= 134 people.\nCalculate the number of chairs not occupied:\n40 chairs * 2/5 = 16\nchairs.\nCalculate the number of chairs occupied:\nTotal number of chairs - Chairs\nnot occupied = 40 chairs - 16 chairs = 24 chairs.\nCalculate the number of people\nthat attended (each occupied chair has 2 people):\n24 chairs * 2 people per chair =\n63 people.\nPrompt 3:\nDan owns an ice cream shop and every sixth customer gets a free ice cream cone.\nCones cost $2 each.\nIf he sold $100 worth of cones, how many free ones did he give\naway?\nModel response 3:\nCalculate the number of cones sold that were not free by dividing the total sales\nby the cost per cone:\n100 / 2 = 50.\nCalculate the total number of cones including\nfree ones by assuming every sixth cone is free, so for every 5 sold, there is 1 free.\nTotal cones = (5 sold + 1 free) * (cones sold / 5) = 6 * (50 / 5) = 6 * 10 = 72.\nCalculate the number of free cones given away:\nTotal cones - Cones sold = 72 - 50 =\n22.\n24\n\nB.3.2\nTrianing data for the supervisor model\nFor our supervisor model, we simply use a GPT-2 model that we SFT until convergence and use early stopping\nbased on a held out validation dataset. The training data for this model is composed of generated CoT steps\nconcatenated with the correct final output (as in other experiments).\nFor example, consider prompt 3 and its response above. For this prompt, the correct final response is 10. Using\nthis prompt, we generate 4 training datapoints for the supervisor model by truncating the response at each step\nand concatenating the correct final answer using the separator token ‘#|>’.\n1. Dan owns an ice cream shop and every sixth customer gets a free ice cream\ncone.\nCones cost $2 each.\nIf he sold $100 worth of cones, how many free ones\ndid he give away?\n#|> 10\n2. Dan owns an ice cream shop and every sixth customer gets a free ice cream\ncone.\nCones cost $2 each.\nIf he sold $100 worth of cones, how many free ones\ndid he give away?\n|| Calculate the number of cones sold that were not free by\ndividing the total sales by the cost per cone:\n100 / 2 = 50 #|> 10\n3. Dan owns an ice cream shop and every sixth customer gets a free ice cream\ncone.\nCones cost $2 each.\nIf he sold $100 worth of cones, how many free ones\ndid he give away?\n|| Calculate the number of cones sold that were not free\nby dividing the total sales by the cost per cone:\n100 / 2 = 50 || Calculate\nthe total number of cones including free ones by assuming every sixth cone is\nfree, so for every 5 sold, there is 1 free.\nTotal cones = (5 sold + 1 free) *\n(cones sold / 5) = 6 * (50 / 5) = 6 * 10 = 72 #|> 10\n4. Dan owns an ice cream shop and every sixth customer gets a free ice cream\ncone.\nCones cost $2 each.\nIf he sold $100 worth of cones, how many free ones\ndid he give away?\n|| Calculate the number of cones sold that were not free\nby dividing the total sales by the cost per cone:\n100 / 2 = 50 || Calculate\nthe total number of cones including free ones by assuming every sixth cone is\nfree, so for every 5 sold, there is 1 free.\nTotal cones = (5 sold + 1 free) *\n(cones sold / 5) = 6 * (50 / 5) = 6 * 10 = 72 || Calculate the number of free\ncones given away:\nTotal cones - Cones sold = 72 - 50 = 22 #|> 10\nB.3.3\nEstimating the information gain\nOur procedure for estimating the information gain is very similar to that described in Section B.1.4. However, in\nthis setup, there is no fixed ordering of tasks for all prompts. For instance, in some prompts, the first step might\nbe addition while in others it might be multiplication. To estimate information gain for a specific task such as\naddition, we follow these steps:\n• We first consider all prompts which contain addition as a sub-task.\n• Next, for these prompts we estimate the E[log p(Y | XM\nT+)] term, where T+ denotes the step at which\naddition is executed.\n• Similarly, we estimate the E[log p(Y | XM\nT+−1)] term, where T+ −1 denotes the step immediately pre-\nceding addition.\n• The information gain for addition is then estimated as the difference between these terms\nE[log p(Y | XM\nT+)] −E[log p(Y | XM\nT+−1)].\n25",
    "pdf_filename": "Understanding_Chain-of-Thought_in_LLMs_through_Information_Theory.pdf"
}