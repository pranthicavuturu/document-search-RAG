{
    "title": "Admiring the Great Mountain - A Celebration Special Issue in Honor of Stephen Grossbergs 80th Birthday",
    "context": "This editorial summarizes selected key contributions of Prof. Stephen Grossberg and describes the papers in this 80th birthday special issue in his honor. His productivity, creativity, and vision would each be enough to mark a scientist of the first caliber. In combination, they have resulted in contributions that have changed the entire discipline of neural networks. Grossberg has been tremendously influential in engineering, dynamical systems, and artificial intelligence as well. Indeed, he has been one of the most important mentors and role models in my career, and has done so with extraordinary generosity and encouragement. All authors in this special issue have taken great pleasure in hereby commemorating his extraordinary career and contributions.   Key contributions of Stephen Grossberg In brief, Grossberg as a biological neural modeler stands without peer, particularly in view of the predictive power of his models, and of their efficacy in unsupervised learning. His work has been independently touted (Hestenes, 1983) as the theoretical harbinger of a revolution in brain science, and subsequent confirmation by experimental psychologists of these models has borne out this assessment. It has been a constant marvel that mathematics works so well to model the many subtleties of scientific phenomena. Grossberg’s work is as strong an example of this principle as any.  He introduced, and has done more than anyone to develop, one of the most important computational paradigms ever; autonomous, self-correcting, biological intelligence. This includes but goes beyond unsupervised learning. The paradigm explains how individual humans or animals can learn to autonomously adapt in real time to complex and changing environments that are filled with unexpected events.  This work began in 1957, when Grossberg was still a freshman in college. Motivated by clearly- open questions in his Psychology coursework, Grossberg introduced the computational paradigm for linking brain mechanisms to psychological functions using real-time nonlinear neural networks. As part of this breakthrough, he derived classical laws for short-term memory, medium-term memory, and long-term memory that are still used, in some variant, by essentially all biological neural network modelers today.  Over the next six decades, Grossberg made a continuous stream of ground-breaking, and often revolutionary, theoretical breakthroughs continuing to the present. There are far too many to catalog in one or even several reviews; in fact, several books have been published on the subject.",
    "body": "1 \n \nPlease cite as: \nDonald C. Wunsch II, “Admiring the Great Mountain: A Celebration Special Issue in Honor of \nStephen Grossberg’s 80th Birthday,” Neural Networks, December 2019. \n \nAdmiring the Great Mountain: A Celebration Special Issue in Honor of \nStephen Grossberg’s 80th Birthday \n \nAbstract \n \nThis editorial summarizes selected key contributions of Prof. Stephen Grossberg and describes \nthe papers in this 80th birthday special issue in his honor. His productivity, creativity, and vision \nwould each be enough to mark a scientist of the first caliber. In combination, they have resulted \nin contributions that have changed the entire discipline of neural networks. Grossberg has been \ntremendously influential in engineering, dynamical systems, and artificial intelligence as well. \nIndeed, he has been one of the most important mentors and role models in my career, and has \ndone so with extraordinary generosity and encouragement. All authors in this special issue have \ntaken great pleasure in hereby commemorating his extraordinary career and contributions. \n \n \nKey contributions of Stephen Grossberg \nIn brief, Grossberg as a biological neural modeler stands without peer, particularly in view of the \npredictive power of his models, and of their efficacy in unsupervised learning. His work has been \nindependently touted (Hestenes, 1983) as the theoretical harbinger of a revolution in brain \nscience, and subsequent confirmation by experimental psychologists of these models has borne \nout this assessment. It has been a constant marvel that mathematics works so well to model the \nmany subtleties of scientific phenomena. Grossberg’s work is as strong an example of this \nprinciple as any.   \n \nHe introduced, and has done more than anyone to develop, one of the most important \ncomputational paradigms ever; autonomous, self-correcting, biological intelligence. This \nincludes but goes beyond unsupervised learning. The paradigm explains how individual humans \nor animals can learn to autonomously adapt in real time to complex and changing environments \nthat are filled with unexpected events. \n \nThis work began in 1957, when Grossberg was still a freshman in college. Motivated by clearly-\nopen questions in his Psychology coursework, Grossberg introduced the computational paradigm \nfor linking brain mechanisms to psychological functions using real-time nonlinear neural \nnetworks. As part of this breakthrough, he derived classical laws for short-term memory, \nmedium-term memory, and long-term memory that are still used, in some variant, by essentially \nall biological neural network modelers today.  \n \nOver the next six decades, Grossberg made a continuous stream of ground-breaking, and often \nrevolutionary, theoretical breakthroughs continuing to the present. There are far too many to \ncatalog in one or even several reviews; in fact, several books have been published on the subject. \n\n \n2 \n \nI will therefore focus on just three of my favorite themes in his research. \n \n1. His fundamental breakthrough for which he is best known was the introduction of \nAdaptive Resonance Theory (ART) (Grossberg, 1976). Grossberg and his colleagues \nhave continually developed ART to its present status as the most advanced cognitive and \nneural theory about how humans and other animals learn to attend, recognize, and predict \nobjects and events in a changing world. ART presently enjoys an unrivaled explanatory \nrange. It has explained the data from many hundreds of psychological and \nneurobiological experiments, and scores of its predictions have been confirmed by \npsychological and neurobiological data, including all of the predictions about the theory’s \nfoundational mechanisms. The unique combination of learning, memory, and prediction \nproperties of ART is equally important, with far-reaching consequences. ART explains \nhow to carry out arbitrary combinations of both unsupervised and supervised learning. \nThe learning of sequences of events is incremental, fast or slow—fast learning can even \nlearn a database in one learning trial—and automatically adjustable to learn both concrete \nand abstract information in response to arbitrarily large non-stationary databases. The \nability to adjust category generality to match the statistics of a particular database was \ninvented by Grossberg as part of ART and is called vigilance control. Grossberg has \ndescribed the brain mechanisms of vigilance control in great detail, including their \nspecific anatomical, neurophysiological and biochemical substrates (Grossberg & \nVersace, 2008) and has shown how, when vigilance control breaks down in specific \nways, it can lead to symptoms of mental disorders such as Alzheimer’s disease, autism, \nmedial temporal amnesia, and abnormal slow wave sleep (Grossberg, 2017a). After \nlearning occurs in ART, it enables direct access to the globally best matching learned \ncategory in its repertoire, without any memory search, no matter how many additional \ncategories are learned subsequently. Otherwise expressed, ART solves the global \nminimum problem. Even more fundamentally, ART solves what Grossberg called the \nstability-plasticity dilemma. In systems that solve this problem, learned memories are \ndynamically self-stabilizing to prevent unpredictable forgetting of previously learned, but \nstill useful, information in response to future learning. Without such a guarantee against \ncatastrophic forgetting, a learning algorithm is unreliable. It is therefore significant that \nother machine learning algorithms are well known to suffer catastrophic forgetting. To \nhelp place these results in context, progress since then is well-summarized in (Grossberg, \n2013) and (Grossberg, 2017b). I’ll add a personal favorite; the discussion in (Grossberg, \n1980) is a superb interplay between his bold but clear thought experiments, the \npsychological principles he was elucidating, and the mathematical formulations needed \nfor the tools he was developing. It is essential to emphasize that ART is not a neural \nnetwork architecture or even a collection of them. Instead, ART is a learning theory. \nSuccinctly summarized, it states that, for neural networks that use feedback, that is to say, \nfor the most powerful ones, learning is regulated by resonance. For many people who \nappreciate the beauty and importance of resonance in feedback systems, reading this \ntheory for the first time is a vicarious Eureka moment. Digging deeper is certain to \nincrease one’s appreciation even further. Much evidence for this theory has been gathered \n(and documented in the most recent papers cited above,) and its applied efficacy is also \n\n \n3 \n \nwell-demonstrated, as elucidated in the engineering-oriented review article (D. C. \nWunsch, 2009) and in an updated article at the end of this issue, discussed later in this \neditorial. This is an appropriate segue to the many neural network architectures based on \nART, described below.  \n \n2. As compelling as ART is theoretically, for solving practical problems the many related \nneural networks architectures have been useful to many researchers. The first of these \n(Carpenter & Grossberg, 1987), often referred to as ART1, is his most highly-cited paper \nto date. This breakthrough article proves theorems about the learning, recognition, and \nprediction properties of ART1, an algorithm that has been used in many large-scale \napplications to technology, including a parts retrieval system that was used for design \nretrieval of structures in the Boeing 777 (Caudell, Smith, Johnson, Wunsch, & Escobedo, \n1992; Caudell, Smith, Johnson, & Wunsch, 1991; Smith, Escobedo, Anderson, & \nCaudell, 1997). Grossberg and his collaborators have developed many other \narchitectures. Key contributions to these architectures (within this one major facet of his \nresearch) are Fuzzy ART (Carpenter, Grossberg, & Rosen, 1991), ARTMAP (Carpenter, \nGrossberg, & Reynolds, 1991), and Fuzzy ARTMAP (Carpenter, Grossberg, Markuzon, \nReynolds, & Rosen, 1992) – although these emphasize neural network models with very \ngeneral capabilities. Grossberg has also contributed many ART-based neural network \nmodels that fit more specific applications, as well as models not relying on ART. From \nhis website (Grossberg, 2019), he develops “brain models of vision and visual object \nrecognition; audition, speech, and language; development; attentive learning and \nmemory; cognitive information processing and social cognition; reinforcement learning \nand motivation; cognitive-emotional interactions; navigation; sensory-motor control and \nrobotics; and mental disorders. These models involve many parts of the brain, ranging \nfrom perception to action, and multiple levels of brain organization, ranging from \nindividual spikes and their synchronization to cognition.”  (The website goes on to \ncharacterize even more areas of seminal contributions, substantiating all of them.) While \nI’ve read and admire many of these, I can’t do justice to them within the scope of this \neditorial. Building just on the most general class of ART architectures, there are \nnumerous other examples, including independent contributions from various researchers, \nnotably LAPART (Healy, Caudell, & Smith, 1993), Fuzzy Min-Max ART (Carpenter et \nal., 1992), TD-FALCON (Tan, Lu, & Xiao, 2008), TopoART (Tscherepanow, 2010), \nDual-Vigilance ART (Brito da Silva, Elnabarawy, & Wunsch, 2019), Distributed Dual-\nVigilance ART, and for biclustering, BARTMAP (Xu & Wunsch, 2011). The point is \nthat Grossberg’s insights have inspired many neural network models by himself and a \nlarge number of other researchers – evidence of the extraordinary impact of his work. \n \n3. Just as important as the proliferation of neural network designs built upon ART are the \nmathematical foundations of Grossberg’s theories and the resulting neural architectures. \nGrossberg used the aforementioned thought experiment approach to develop \nmathematical models that were both original and far-reaching. The reason these models \nwere so fundamental is that he typically started with a question of the form, “What is the \nminimal model that would be capable of …” I’ll constrain my remarks to a few favorites; \n\n \n4 \n \nthere are many more. An early example, regarding Pavlovian conditioning, appeared in \nthe Proceedings of the National Academy of Sciences (Grossberg, 1971). He made \narguments about why certain physiological conditions created necessary mathematical \nassumptions, resulting in elegant theorems for simple but powerful mathematical models. \nThis has been his method before and since. He published a seminal analysis of \nfeedforward and feedback, cooperative-competitive dynamical systems as early as 1973, \nbut my favorite summary is in chapter 2 of (Grossberg, Stephen; Kuperstein, 1986). It \ncovers a tremendous amount of material, and a solid understanding of it is necessary for a \nmuch more efficient design of adaptive or hardwired neural networks (e.g. (Cai, \nProkhorov, & Wunsch, 2007; Wunsch, 2000) and citing papers.) As Grossberg’s \nmathematical analyses developed further, he continued to ask simple questions while \ndeveloping new methods to answer them. The most famous of these, (Cohen & \nGrossberg, 1983), (often referred to by others as the Cohen-Grossberg theorem) \nintroduces a general class of nonlinear dynamical systems for which a Lyapunov function \nis defined as part of the proof of global convergence to possibly infinitely many \nequilibrium points. These systems are interpreted as content addressable memories in a \nbroad range of recurrent neural network architectures. The 1983 article builds on a few of \nGrossberg’s earlier works, my favorite of which is (Grossberg, 1978), which is deeply \nintricate, stunningly creative, and broad-ranging in its implications. This article \nintroduces a large class of competitive dynamical systems and introduces a highly \noriginal Method of Jumps to embed a discrete decision scheme into it, thereby making \nrigorous the intuition that one way to understand a competition is to keep track of who is \nwinning it through time. Each Jump is a discrete decision that occurs when a new \npopulation begins to win. Grossberg used his Method of Jumps to prove global theorems \nabout both oscillatory and convergence dynamics in competitive systems. Persistent \noscillations tend to occur when there are Jump cycles, and convergence tends to occur \nwhen there are not. The article focuses on a general class of competitive systems that \nalways converge to one of possibly infinitely many equilibrium points. It is prescient in \nthat it explicitly describes implications not only for neural networks, but also economic \nand social networks, decades before most people were even thinking about such topics! \nOne intriguing economic application proves sufficient conditions that guarantee that a \ncompetitive market’s price will be stable, while every firm can track publicly known \nmarket prices to make its own decisions about production and savings, with no \nknowledge about the policies of the possibly arbitrarily many other firms with which it is \ncompeting. The theorem provides, in other words, a mathematical description of how \nAdam Smith’s Invisible Hand (Smith, 1776) can work in a competitive market. \n \nAs impressive as these accomplishments of Grossberg are, they are augmented by his \naccomplishments in technological leadership of the highest order via his development of the \nprofession, including his pivotal role in the formation of the International Neural Networks \nSociety and the journal Neural Networks.  \n \nI hope that Grossberg’s genius will continue to illuminate science with his continued research for \nmany years to come. His visionary discoveries and tireless leadership have already created an \n\n \n5 \n \nextraordinary legacy. On the occasion of his 80th birthday it is a delight to honor his leading \nresearch on introducing, developing, and mathematically characterizing processes of biological \nlearning.  \n \nThis Issue \n \nThe articles in this special issue are a glimpse into how Grossberg’s research has influenced \nothers. We begin with a paper by his closest collaborator and spouse. (Carpenter, 2019) discusses \nusing Self-supervised ART for continual learning in unpredictable environments, placing this \ndiscussion into the context of modern challenges in Artificial Intelligence. (Kosko, 2019) shows \nthat noise injection benefits the performance of (ART-inspired) bidirectional backpropagation on \nclassifiers as well as generative adversarial networks. (Seiffertt, 2019) extends his previous \nseminal contributions relating time-scales calculus (Bohner & Peterson, 2001) to reinforcement \nlearning (Seiffertt, Sanyal, & Wunsch, 2008) and backpropagation (Seiffertt & Wunsch, 2010) to \nnow relate this important generalization between continuous and discrete time for the first time \nto ART. Those who use machine learning with both continuous and discrete time signals would \ndo well to study this paper and its references. (Healy & Caudell, 2019) use category theory to \ndevelop a representation of hierarchical episodic memory building on various important memory \nmodels including Grossberg’s early work. ART has long been considered a suitable architecture \nfor sensor fusion. (Tan, Subagdja, Wang, & Meng, 2019) makes the case that Fusion ART can \ngo beyond sensor fusion by combining several information channels, resulting in more complex \ntasks such as combining multiple learning modalities. (Levine, 2019) explore the consistency of \ndecision making that goes beyond economic utility maximization to consider the broader range \nof behaviors encountered in real life. (Zeid & Bullock, 2019) describes models and evidence for \ncomplex sequence learning, such as musical sequences. (Patel, Hazan, Saunders, Siegelmann, & \nKozma, 2019) converts deep Q-learning to a spiking neural networks model, showing improved \nrobustness in the process. (Wandeto & Dresp-Langley, 2019) explores how quantization error in \nneural networks can reliably discriminate fine differences in local contrast. (Brna et al., 2019) \nuses modulation of uncertainty in an ART context to enable Lifelong Learning, including self-\nsupervised and one-shot learning. (Meng, Tan, & Miao, 2019) introduces Salient-Aware ART, \nwhich builds on his previous works (Meng, Tan, & Wunsch, 2016; Meng, Tan, & Wunsch II, \n2019) that customize update rules to each ART cluster; while retaining linear computational \ncomplexity. This is useful for the important problem of clustering sparse data. (Pessoa, 2019) \nmakes the case for investigating the dynamic, multivariate structure of brain data to understand \nemotion and cognition. Finally, (Brito da Silva, Elnabarawy, & Wunsch II, 2019) provides the \nmost comprehensive survey yet on various modifications to ART for engineering applications.  \nAll the special issue authors are pleased to offer this enduring commemoration of Grossberg’s \nextraordinary career.  \n \nIn addition to the valuable contributions of each author in this issue, discussions of an early \nversion of this editorial with Mario Aguilar-Simon, Jose L. Contreras-Vidal, Morris W. Hirsch, \nDaniel S. Levine and Hava T. Siegelmann are gratefully acknowledged. The author would also \nlike to thank Stephen Grossberg and Gail Carpenter for their incredible intellectual and personal \nhospitality and encouragement.  \n\n \n6 \n \nThis research was sponsored by the Missouri University of  Science  and  Technology  Mary  K.  \nFinley  Endowment and Intelligent Systems Center; the Coordena¸c˜ao de Aperfei¸coamento de \nPessoal de N´ıvel Superior - Brazil (CAPES) - Finance code BEX 13494/13-9; the Army Research \nLaboratory (ARL) and the Lifelong Learning Machines program from DARPA/MTO, and it was \naccomplished under Cooperative Agreement Number W911NF-18-2-0260. The views and \nconclusions contained in this document are those of the authors and should not be interpreted as \nrepresenting the official policies, either expressed or implied, of the Army Research Laboratory \nor the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints \nfor Government purposes notwithstanding any copyright notation herein. \n  \n \nReferences \n \nBohner, M., & Peterson, A. (2001). Dynamic Equations on Time Scales. \nhttps://doi.org/10.1007/978-1-4612-0201-1 \nBrito da Silva, L. E., Elnabarawy, I., & Wunsch, D. C. (2019). Dual vigilance fuzzy adaptive \nresonance theory. Neural Networks, 109, 1–5. https://doi.org/10.1016/j.neunet.2018.09.015 \nBrito da Silva, L. E., Elnabarawy, I., & Wunsch II, D. C. (2019). A Survey of Adaptive \nResonance Theory Neural Network Models for Engineering Applications. Neural Networks. \nBrna, A., Brown, R., Connolly, P., Simons, S., Shimizu, R., & Aguilar-Simon, M. (2019). \nUncertainty-based Modulation for Lifelong Learning. Neural Networks. \nCai, X., Prokhorov, D. V, & Wunsch, D. C. (2007). Training winner-take-all simultaneous \nrecurrent neural networks. IEEE Transactions on Neural Networks / a Publication of the \nIEEE Neural Networks Council, 18(3), 674–684. https://doi.org/10.1109/TNN.2007.891685 \nCarpenter, G. A. (2019). Looking to the future: Learning from experience, averting catastrophe. \nNeural Networks. \nCarpenter, G. A., & Grossberg, S. (1987). A massively parallel architecture for a self-organizing \nneural pattern recognition machine. Computer Vision, Graphics and Image Processing, \n37(1), 54–115. https://doi.org/10.1016/S0734-189X(87)80014-2 \nCarpenter, G. A., Grossberg, S., Markuzon, N., Reynolds, J. H., & Rosen, D. B. (1992). Fuzzy \nARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog \nMultidimensional Maps. IEEE Transactions on Neural Networks, 3(5), 698–713. \nhttps://doi.org/10.1109/72.159059 \nCarpenter, G. A., Grossberg, S., & Reynolds, J. H. (1991). ARTMAP: Supervised real-time \nlearning and classification of nonstationary data by a self-organizing neural network. Neural \nNetworks, 4(5), 565–588. https://doi.org/10.1016/0893-6080(91)90012-T \nCarpenter, G. A., Grossberg, S., & Rosen, D. B. (1991). Fuzzy ART: Fast stable learning and \ncategorization of analog patterns by an adaptive resonance system. Neural Networks, 4(6), \n759–771. https://doi.org/10.1016/0893-6080(91)90056-B \nCaudell, T. P., Smith, S. D. G., Johnson, G. C., Wunsch, D. C., & Escobedo, R. (1992). An \nindustrial application to neural networks to reusable design. Proceedings. IJCNN - \nInternational Joint Conference on Neural Networks, 919. Publ by IEEE. \nCaudell, T. P., Smith, S. D., Johnson, G. C., & Wunsch, D. C. (1991). Application of neural \n\n \n7 \n \nnetworks to group technology. Proceedings of SPIE - The International Society for Optical \nEngineering, 1469(pt 2), 612–621. Publ by Int Soc for Optical Engineering. \nCohen, M. A., & Grossberg, S. (1983). Absolute stability of global pattern formation and parallel \nmemory storage by competitive neural networks. IEEE Transactions on Systems, Man and \nCybernetics, SMC-13(5), 815–826. https://doi.org/10.1016/S0166-4115(08)60913-9 \nGrossberg, Stephen; Kuperstein, M. (1986). Neural Dynamics of Adaptive Sensory-Motor \nControl: Ballistic Eye Movements. Amsterdam, The Netherlands: North-Holland. \nGrossberg, S. (1971). Pavlovian pattern learning by nonlinear neural networks. Proceedings of \nthe National Academy of Sciences, 68(4), 828–831. https://doi.org/10.1073/pnas.68.4.828 \nGrossberg, S. (1976). Adaptive pattern classification and universal recoding: II. Feedback, \nexpectation, olfaction, illusions. Biological Cybernetics, 23(4), 187–202. \nhttps://doi.org/10.1007/BF00340335 \nGrossberg, S. (1978). Competition, Decision, and Consensus. Journal of Mathematical Analysis \nand Applications, 66, 470–493. \nGrossberg, S. (1980). How does a brain build a cognitive code? Psychological Review, 87(1), 1–\n51. https://doi.org/10.1037/0033-295X.87.1.1 \nGrossberg, S. (2013). Adaptive Resonance Theory: How a brain learns to consciously attend, \nlearn, and recognize a changing world. Neural Networks, 37, 1–47. \nhttps://doi.org/10.1016/j.neunet.2012.09.017 \nGrossberg, S. (2017a). Acetylcholine neuromodulation in normal and abnormal learning and \nmemory: Vigilance control in waking, sleep, autism, amnesia, and Alzheimer’s disease. \nFrontiers in Neural Circuits. \nGrossberg, S. (2017b). Towards solving the hard problem of consciousness: The varieties of \nbrain resonances and the conscious experiences that they support. Neural Networks. \nhttps://doi.org/10.1016/j.neunet.2016.11.003 \nGrossberg, S. (2019). Stephen Grossberg: Wang Professor of Cognitive Systems, Boston \nUniversity. Retrieved from http://sites.bu.edu/steveg/ \nGrossberg, S., & Versace, M. (2008). Spikes, synchrony, and attentive learning by laminar \nthalamocortical circuits. Brain Research, 1218, 278–312. \nhttps://doi.org/10.1016/j.brainres.2008.04.024 \nHealy, M.J., Caudell, T. P., & Smith, S. D. G. (1993). A neural architecture for pattern sequence \nverification through inferencing. IEEE Transactions on Neural Networks, 4(1), 9–20. \nhttps://doi.org/10.1109/72.182691 \nHealy, Michael John, & Caudell, T. P. (2019). Episodic Memory: A Hierarchy of Spatiotemporal \nConcepts. Neural Networks. \nHestenes, D. (1983). How the Brain Works: The Next Great Scientific Revolution. Third \nWorkshop on Maximum Entropy and Bayesian Methods in Applied Statistics, 1–57. \nKosko, B. (2019). Noise-Boosted Bidirectional Backpropagation and Adversarial Learning. \nNeural Networks. \nLevine, D. S. (2019). One or Two Minds? Neural Network Modeling of Decision Making by the \nUnified Self. Neural Networks. \nMeng, L., Tan, A.-H., & Miao, C. (2019). Salience-Aware Adaptive Resonance Theory for \nLarge-Scale Sparse Data Clustering. Neural Networks. \nMeng, L., Tan, A.-H., & Wunsch, D. C. (2016). Adaptive Scaling of Cluster Boundaries for \n\n \n8 \n \nLarge-Scale Social Media Data Clustering. IEEE Transactions on Neural Networks and \nLearning Systems, 27(12), 2656–2669. https://doi.org/10.1109/TNNLS.2015.2498625 \nMeng, L., Tan, A.-H., & Wunsch II, D. C. (2019). Adaptive Resonance Theory in Social Media \nData Clustering. https://doi.org/10.1007/978-3-030-02985-2 \nPatel, D., Hazan, H., Saunders, D., Siegelmann, H. T., & Kozma, R. (2019). Improved \nrobustness of reinforcement learning policies upon conversion to spiking neuronal network \nplatforms applied to Atari Breakout game. Neural Networks. \nPessoa, L. (2019). Neural dynamics of emotion and cognition. Neural Networks. \nSeiffertt, J., Sanyal, S., & Wunsch, D. C. (2008). Hamilton–Jacobi–Bellman Equations and \nApproximate Dynamic Programming on Time Scales. IEEE Transactions on Systems, Man, \nand Cybernetics, Part B (Cybernetics), 38(4), 918–923. \nhttps://doi.org/10.1109/TSMCB.2008.923532 \nSeiffertt, J, & Wunsch, D. C. (2010). Backpropagation and Ordered Derivatives in the Time \nScales Calculus. IEEE Transactions on Neural Networks, 21(8), 1262–1269. \nhttps://doi.org/10.1109/TNN.2010.2050332 \nSeiffertt, John. (2019). Adaptive Resonance Theory in the Time Scales Calculus. Neural \nNetworks. \nSmith, A. (1776). An Inquiry into the Nature and Causes of the Wealth of Nations. Liberty \nClassics. \nSmith, S. G., Escobedo, R., Anderson, M., & Caudell, T. P. (1997). A deployed engineering \ndesign retrieval system using neural networks. IEEE Transactions on Neural Networks, \n8(4), 847–851. https://doi.org/10.1109/72.595882 \nTan, A.-H., Subagdja, B., Wang, D., & Meng, L. (2019). Self-Organizing Neural Networks for \nUniversal Learning and Multimodal Memory Encoding. Neural Networks. \nTan, A. H., Lu, N., & Xiao, D. (2008). Integrating temporal difference methods and self-\norganizing neural networks for reinforcement learning with delayed evaluative feedback. \nIEEE Transactions on Neural Networks, 19(2), 230–244. \nhttps://doi.org/10.1109/TNN.2007.905839 \nTscherepanow, M. (2010). TopoART : A Topology Learning Hierarchical. ICANN 2010, Part \nIII, LNCS 6354, 157–167. \nWandeto, J. M., & Dresp-Langley, B. (2019). The quantization error in a Self-Organizing Map \nas indicator of change in random-dot images. Neural Networks. \nWunsch, D. (2000). The cellular simultaneous recurrent network adaptive critic design for the \ngeneralized maze problem has a simple closed-form solution. Proceedings of the IEEE-\nINNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural \nComputing: New Challenges and Perspectives for the New Millennium, 79–82 vol.3. \nhttps://doi.org/10.1109/IJCNN.2000.861284 \nWunsch, D. C. (2009). ART properties of interest in engineering applications. 2009 International \nJoint Conference on Neural Networks, 3380–3383. \nhttps://doi.org/10.1109/IJCNN.2009.5179094 \nXu, R., & Wunsch, D. (2011). BARTMAP: A viable structure for biclustering. Neural Networks, \n24(7), 709–716. https://doi.org/10.1016/j.neunet.2011.03.020 \nZeid, O., & Bullock, D. (2019). Moving in time: Simulating how neural circuits enable rhythmic \nenactment of planned sequences. Neural Networks. \n\n \n9",
    "pdf_filename": "Admiring the Great Mountain - A Celebration Special Issue in Honor of Stephen Grossbergs 80th Birthday.pdf"
}