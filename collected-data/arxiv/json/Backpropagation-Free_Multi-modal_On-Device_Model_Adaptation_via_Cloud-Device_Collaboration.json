{
    "title": "Backpropagation-Free Multi-modal On-Device Model",
    "abstract": "priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org. ©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. 0004-5411/2018/8-ART111$15.00 https://doi.org/XXXXXXX.XXXXXXX J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018. 4202 voN 81 ]CD.sc[ 3v10610.6042:viXra",
    "body": "Backpropagation-Free Multi-modal On-Device Model\nAdaptation via Cloud-Device Collaboration\nWEIJI∗,\nNationalUniversityofSingapore,Singapore\nLILI∗,\nNationalUniversityofSingapore,Singapore\nZHEQILV∗,\nZhejiangUniversity,China\nWENQIAOZHANG,\nZhejiangUniversity,China\nMENGZELI,\nZhejiangUniversity,China\nZHENWAN,\nFudanUniversity,China\nWENQIANGLEI,\nSichuanUniversity,China\nROGERZIMMERMANN,\nNationalUniversityofSingapore,Singapore\nInourincreasinglyinterconnectedworld,whereintelligentdevicescontinuallyamasscopiouspersonalized\nmulti-modaldata,apressingneedarisestodeliverhigh-quality,personalizeddevice-awareservices.However,\nthisendeavorpresentsamultifacetedchallengetoprevailingartificialintelligence(AI)systemsprimarily\nrootedinthecloud.Asthesesystemsgrapplewithshiftingdatadistributionsbetweenthecloudanddevices,\nthetraditionalapproachoffine-tuning-basedadaptation(FTA)existsthefollowingissues:thecostlyand\ntime-consumingdataannotationrequiredbyFTAandtheloomingriskofmodeloverfitting.Tosurmountthese\nchallenges,weintroduceaUniversalOn-DeviceMulti-modalModelAdaptationFramework,revolutionizingon-\ndevicemodeladaptationbystrikingabalancebetweenefficiencyandeffectiveness.Theframeworkfeaturesthe\nFastDomainAdaptor(FDA)hostedinthecloud,providingtailoredparametersfortheLightweightMulti-modal\nModelondevices.Toenhanceadaptabilityacrossmulti-modaltasks,theAnchorFrameDistributionReasoner\n(ADR)minimizescommunicationcosts.Ourcontributions,encapsulatedintheCloud-DeviceCollaboration\nMulti-modalParameterGeneration(CDC-MMPG)framework,representapioneeringsolutionforon-Device\nMulti-modalModelAdaptation(DMMA).Extensiveexperimentsvalidatetheefficiencyandeffectivenessof\nourmethod,particularlyinvideoquestionansweringandretrievaltasks,drivingforwardtheintegrationof\nintelligentdevicesintoourdailylives.\nCCSConcepts:•Informationsystems→Mobileinformationprocessingsystems;Personalization;•\nHuman-centeredcomputing→Mobilecomputing.\nAdditionalKeyWordsandPhrases:Cloud-devicecollaboration,modeladaptation,multi-modal\nACMReferenceFormat:\nWeiJi,LiLi,ZheqiLv,WenqiaoZhang,MengzeLi,ZhenWan,WenqiangLei,andRogerZimmermann.2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration.J.ACM37,\n4,Article111(August2018),17pages.https://doi.org/XXXXXXX.XXXXXXX\n∗Theseauthorscontributedequallytothisresearch.\nAuthors’addresses:WeiJi,NationalUniversityofSingapore,Singapore,jiwei@nus.edu.sg;LiLi,NationalUniversityof\nSingapore,Singapore,lili02@u.nus.edu;ZheqiLv,ZhejiangUniversity,China,zheqilv@zju.edu.cn;WenqiaoZhang,Zhejiang\nUniversity,China,wenqiaozhang@zju.edu.cn;MengzeLi,ZhejiangUniversity,China,mengzeli@zju.edu.cn;ZhenWan,\nFudanUniversity,China,wz2311602492@gmail.com;WenqiangLei,SichuanUniversity,China,wenqianglei@gmail.com;\nRogerZimmermann,NationalUniversityofSingapore,Singapore,dcsrz@nus.edu.sg.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee\n111\nprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe\nfullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.\nAbstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires\npriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.\n©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\n0004-5411/2018/8-ART111$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n4202\nvoN\n81\n]CD.sc[\n3v10610.6042:viXra\n111:2 Trovatoetal.\nCloud\nDevice Device\n(𝑖𝑖) (𝑗𝑗)\n𝑑𝑑 Distribution Distribution 𝑑𝑑\nShift Shift\nDeploy Deploy Device\n(𝑖𝑖)\n𝑑𝑑\nMulti-Modal Multi-Modal Multi-Modal\nModel Model Model\n𝑀𝑀 𝑀𝑀 𝑀𝑀\nMulti-Modal Multi-Modal\nModel Parameters Parameters Model\n𝑖𝑖 FastDomainAdaptor 𝑗𝑗\n𝑀𝑀𝑑𝑑 𝑀𝑀𝑑𝑑\n𝑀𝑀𝑔𝑔\nFig.1. (a)Multi-modaldataoncloudanddifferentdevicesexistindifferentdistributionsduetothepersonal-\nizedpreferenceofusers.(b)Comparedwithconventionalmethodsofdeployingmodelsondifferentdevices,\nweproposeanFDAthatcanachieveabalanceofefficiencyandeffectiveness.\n1 INTRODUCTION\nIntoday’sinterconnectedworld,theproliferationofintelligentdevices,rangingfromubiquitous\nsmartphonestotheever-expandingInternetofThings(IoT)ecosystem,hasbecomeanintegralpart\nofourdailylives.Thesedevicesserveasdatacollectionpowerhouses,continuouslyamassingvast\nrepositoriesofpersonalizedmulti-modaldata,whichcanincludeawidearrayofinputmodalities\nsuchastext,imagesandvideos.Thepotentiallockedwithinthistroveofmulti-modaldataarriving\ncontinuouslyisimmense,promisingtounlockhigh-qualityandtailoreddevice-awareservices\nforindividualusers.Despitepromising,thepersonalizeddeviceserviceinvolvesanalyzingthe\ndynamicnatureofthemulti-modaldatathatunderscoreusers’intentions.Theprevailingartificial\nintelligence (AI) systems, primarily trained and deployed in cloud-based environments, face a\nprofoundchallengeinadaptingtothedynamicdevicedatawhenusingastaticcloudmodelfor\nallindividualusers,mainlyduetothedistributionshiftofthecloudanddevicedata,asshownin\nFigure1.Inotherwords,high-qualitypersonalizedservicerequiresAIsystemstoundergocontinual\nrefinementandadaptationtoaccommodatetheevolvinglandscapeofpersonalizedmulti-modal\ndata.\nIntuitively,oneofthestraightforwardadaptationstrategiesistofine-tunethecloudmodelbased\nonthedevice’smulti-modaldata,whichcankindlyalleviatethecloud-devicedatadistributionshift\ntomodelusers’intentions.Nevertheless,wecontendthatthefine-tuning-adaptation(FTA)paradigm\nmaynotsatisfactorilyresolvedevicemodelpersonalization,whichcanbesummarizedastwokey\naspects:(1)UndesirableAnnotation.FTAoftennecessitatesmanuallyannotatingdatatoguide\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nsdohteM\n)b(\neciveD-duolC\n)a(\nnosirapmoC\ntfihS\nnoitubirtsiD\nlanoitnevnoC\nsruO\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:3\nmodeladaptation,whichtypicallyhingesonexpensiveandlabor-intensivedevicedatalabeling.\nAdditionally,thisretrainingprocesscanresultinsubstantialdelays,hinderingtheAIsystem’s\nabilitytodeliverreal-time,context-awareresponsiveness.Thissituationisfurtherexacerbatedby\ntheinherentcomplexityofmulti-modaldataunderstanding,i.e.,withdatastreamsthatencompass\ntextual,visual,andauditoryinformation,theintricaciesoflabelingandretrainingaremoretime-\nconsuming, resulting in a higher time delay and thus diminishing its practicality for users. (2)\nOverfitting Risk. In real-world applications, the majority of devices could be characterized\nby sparse and specialized multi-modal data where model fine-tuning may inadvertently lead\nto overfitting issues, even leading to device model performance degradation. In other words,\nFTAimposessignificantdemandsforon-devicedataquantity,hamperingitsabilitytogeneralize\neffectivelyacrossdiversedeviceecosystems.Basedontheaforementionedinsights,ameaningful\noptimizationgoalofdevicemodelpersonalizationistoappropriatelytapontothepersonalized\nmulti-modaldata,andthusstrikethedelicatebalancebetweentheeffectivenessandefficiencyof\npersonalizedadaptation.\nTo address these multifaceted challenges and pave the way for intelligent device-driven AI\nsystemsthatharmonizeadaptabilityandcomputationalefficiency,wepresentauniversaldevice\nmulti-modalmodeladaptationframework,agroundbreakingapproachthatredefinesthelandscape\nofon-deviceAIadaptation.ThisframeworkisdesignedtorevolutionizethewayAIsystemsharness\nmulti-modaldata,simultaneouslyunlockingefficiencyandeffectivenessintheadaptationprocess.\nInourapproach,weintroducedaFastDomainAdaptor(FDA)hostedonthecloud,whichtakes\ndevice-capturedimagesasinputandproducescustomizedparametersfortheLightweightMulti-\nmodalModelonthedevice.Thesecustomizedparametersaretailoredtodifferentdatadistributions\npresentonthedevice.Moreover,theFDAadaptstovariousmulti-modaltasksbyrequiringdistinct\ndatainputs.However,certainmulti-modaltasks,likeVideoQA,necessitatetransmittingextensive\ndatasuchasmultipleframesortheentirevideotoFDA,incurringsubstantialcommunicationcosts\nandbandwidthrequirements.TomitigatethischallengeandenhanceFDA’sadaptabilityacross\ndifferentmulti-modaltasks,wedevelopedtheAnchorFrameDistributionReasoner(ADR).ADR\nstandardizestheinputfortheFDAacrossvariousmulti-modaltasks.Forinstance,inthecaseof\nVideoQA,ADRselectsthefirstframeofeachvideoastheAnchorFrame.Usingacombinationof\nVariationAutoEncoder(VAE)andKeyFrame,ADRmapstheAnchorFrametoastandarddistribution\nthroughtraining.OncebothFDAandADRaretrained,theyaredeployedonthecloudtodeliver\npersonalizedmodelparameterservicesforlightweightmulti-modalmodelsonthedevice.The\ndevice’smodelcanthenuploadtheAnchorFrametoobtainpersonalizedmodelparameterstailored\ntothedatadistributionofthecurrentimageorvideoinput.Animportanthighlightofourframework\nistheabsenceofbackpropagationduringthedomainadaptationprocessforthedevicemodel.This\nuniqueapproachenablesourframeworktoachieverapiddomainadaptationofthedevicemodel,\nresultinginexceptionallylowlatency.\nSummingup,ourcontributionsaresummarizedbelow:\n(1) WeproposeaCloud-DeviceCollaborationMulti-ModalParameterGeneration(CDC-MMPG)\nframeworktoaccomplishefficienton-DeviceMulti-modalModelAdaptation(DMMA).Its\ncore is Fast Domain Adaptor (FDA) which can generate multi-modal equipment model\nparametersbasedonthedatadistributionofmulti-modaldata.Tothebestofourknowledge,\nwearethefirstonestosolvetheDMMA.OurproposedCDC-MMPGframeworkisauniversal\nonethatcanbeadaptivetoothermodalities.\n(2) WeproposetheAnchorFrameDistributionReasoner(ADR)tofurtherreducethecommu-\nnicationcostoftheCDC-MMPG.ADRsuccessfullyaddressestheissueofhighbandwidth\ndependencewhilemaintainingnearlyconsistentperformance.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:4 Trovatoetal.\n(3) Extensiveexperimentsonvideoquestionansweringandvideoretrievaltaskshaveverified\ntheefficiencyandeffectivenessofourproposedmethod.\n2 RELATEDWORKS\nCloud-DeviceCollaboration.Intherapidlyevolvingsphereofcloud-devicecollaboration,deep\nlearninghasbecomeacrucialcomponent,strategicallycombiningthefeaturesofcloud-basedand\non-devicemachinelearning.AlthoughFederatedLearning(FL),notablyrepresentedbyFedAVG[5],\nhas traditionally been a significant player, its perceived simplicity has limited its applicability\ninvariouspracticalsituations.Thisdynamicfieldhasseentheriseofinnovativemethodsand\ntechniquesdesignedtooptimizetheinteractionbetweencloudresourcesandmobiledevices.For\ninstance,MPDA[41]skillfullyutilizescloud-basedsamplestoenhancetheperformanceofon-\ndevice models. Another interesting approach involves deploying multiple models with similar\nfunctionalities, coordinated by a Meta Controller for optimal task allocation, thus expanding\nthepossibilitiesofcollaborativemodeladaptation[3,49].Moreover,DUET[23],inspiredbythe\nHyperNetworkconcept[26,38,47,50],simplifiesthedevicemodeladaptationprocess,eliminating\ntheneedforon-devicetraining[55].IDEAL[22]expandsparametergeneration-basedmodelsto\nthedomainofrecommendersystems,particularlyDC-CDR[52].Thisenhancesthegeneralization\ncapabilitiesofdevicerecommendationmodels,thoughitrequiresconsiderationoffactorssuch\nas request frequency and communication revenue. However, the above works are tailored for\nrecommendersystem,whichcannotbedirectlymigratedtothemulti-modaldomainfield,mainly\nduetothattheyignoretheinherentcomplexityofmulti-modaldata.Inthispaper,wepresent\nCDC-MMPG,tailoredformulti-modalon-devicemodeladaptationviacloud-devicecollaboration,\nwhichsimultaneouslyobtainedeffectivenessandefficiency.\nDomainAdaptation.DomainAdaptation(DA)playsacrucialroleintransferringanetwork\nthatwasinitiallytrainedonalabeledsourcedomaintoeffectivelyperformonatargetdomain.\nVarioustechniqueshavebeendevelopedtoalignthefeaturedistributionsbetweenthesourceand\ntargetdomains,suchasmaximummeandiscrepancyandcorrelationalignment.Thesemethods\naim to reduce the distribution discrepancy and enable the network to generalize well across\ndifferentdomains,improvingitsperformanceinthetargetdomain.Lotsofmethodsareproposedto\nfacilitatethedevelopmentofthedomainadaptation,suchasmulti-stagemethods[4,12,14,15,53],\ngradualtransferstrategies[2,16,31,33],andsoon.Recentadvancementsindomainadaptation\nembracecurriculum-basedstrategiesinspiredbycurriculumlearningtoenhancetheeffectiveness\noftheadaptationprocess[13,27,29],Source-freedomainadaptation(SFDA)hasemergedasa\nresponsetoprivacyandcopyrightconcerns,whereaccesstothesourcedomaindataisrestricted\norunavailable[2,4,8,10,18,20,27,29,30,35,44,53].SFDApresentsamorechallengingvariant\nknownasTest-TimeAdaptation(TTA),whichnecessitatesonlineadaptationduringtheinference\nphase.Inaddition,manyresearchersintroducethedomainadaptationsettingintodifferenttasks\ntoenhancemodelgeneralization[1,6,19,24,25,28,36,37,42,43,48,51,54].Insummary,domain\nadaptation(DA)methodsfocusonaligningfeaturedistributionsbetweenthesourceandtarget\ndomains.Recentadvancementshavehighlightedtheimportanceofcurriculum-basedstrategiesto\nimprovetheadaptationprocess.[2,4,27,29,46,53].Inthispaper,weproposeamodelgeneration\nframeworktosolvethepersonalizedadaptionproblem.\n3 METHODOLOGY\nInthissection,weprovideadetaileddescriptionofourproposedCloud-DeviceCollaborationMulti-\nModalParameterGeneration(CDC-MMPG)framework,whichincludesFastDomainAdaptor\n(FDA)andAnchorFrameDistributionReasoner(ADR).\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:5\n(a) Fast Domain Adaptor Parameter Parameter\nGenerator Feature\nVideo&Text Feature\nDevice → Cloud Cloud → Device\n(b) AnchorFrameDistribution Reasoner Training Inference\nProcedure Procedure\nVideo&Text Feature Sampling\n𝐷% R Loe sc sonstruction Im Fag eae t& urT eext Sampling\n𝜎2 !!\n𝜇2 \"!\n!!\n(c) Lightweight Multi-modal Model Legend\nFast Domain Adaptation\n𝑡+ Anc Fh eo ar tuF rr eame\nParameter\nFilling\nKeyframe\nFeature\nStructure Structure\nOnly Para& meter Sampled\nFeature\nText Embedding\n𝑡- Prediction: Table\nVideo Feature Jumping\nQuestion: On Device\nWhat is the Answer: Entropy\nman doing? Dancing Loss On Cloud\nText Feature\nFig.2. Illustrationoftheoverallpipelineofourmethod,CDC-MMPG.(a)and(b)representtheCloudmodel,\nwhichreconstructsthevideofeaturesuploadedfromthedeviceandreasonsoutthepersonalparameters\nofthedevicemodelbasedonthereconstructedvideofeatures.(c)representsthelightweightmulti-modal\ndevice-sidemodel,whichextractsthemulti-modalfeatures,anduploadsthevideofeaturestothecloud\nmodelforthepersonaldevice-modelparameterprediction.Afterbeingupdatedwiththepersonalparameters,\nthelightweightmulti-modaldevice-sidemodelwillfurtheranalyzethemulti-modalfeaturesandmakethe\nfinalprediction.\n3.1 Preliminary\nProblemFormulation.Fortheon-DeviceMulti-modalModelAdaptation(DMMA)inthedevice-\ncloudcollaborationsystem,wehaveaccesstoasetofdevices D = {𝑑(𝑖)}N𝑑,eachdevicewith\n𝑖=1\nitspersonali.i.dmulti-modalhistorysamplesS 𝐻(𝑖) andmulti-modalreal-timesamplesS 𝑅(𝑖) in\ncurrentsession,whereN𝑑 representsthenumberofdevices.Thegoalofon-DeviceMulti-modal\nModelAdaptationistogeneralizeatrainedcloudmodelM𝑔(·;Θ 𝑔)learnedfrom{S 𝐻(𝑖)}N 𝑗=𝑑 1toeach\nspecificlocaldevicemodelM 𝑑(𝑖)(·;Θ 𝑑(𝑖)) conditionedonreal-timesamplesS 𝑅(𝑖),whereΘ 𝑔 and\nΘ\n𝑑(𝑖)\nrespectivelydenotethelearnedparametersforthecloudmodelandthe𝑖-thdevicemodel:\nC (cid:124)(cid:32)D (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)C (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)- (cid:123)M (cid:122)M (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)P (cid:32)(cid:32)(cid:32)(cid:32)G (cid:125):M (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)𝑔 (cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)(cid:32){ (cid:32)(cid:32)(cid:32)S (cid:32)(cid:32)(cid:32)(cid:32)𝐻 (cid:32)(cid:32) (cid:123)(𝑖 (cid:122))} (cid:32)(cid:32)(cid:32)𝑖(cid:32)N (cid:32)=(cid:32)(cid:32)𝑑 (cid:32)1(cid:32)(cid:32)(cid:32); (cid:32)(cid:32)(cid:32)Θ (cid:32)(cid:32)(cid:32)(cid:32)𝑔 (cid:32) (cid:125)) → (cid:124)M (cid:32)(cid:32)(cid:32)(cid:32)𝑑(cid:32)(cid:32)(cid:32)((cid:32)𝑖 (cid:32)(cid:32))(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)S (cid:32)(cid:32) (cid:123)𝑅 (cid:122)(𝑖 (cid:32))(cid:32)(cid:32); (cid:32)(cid:32)(cid:32)Θ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)𝑑(cid:32)(cid:32)(cid:32)((cid:32)𝑖 (cid:32)(cid:32)) (cid:125)). (1)\nDMMAModel CloudModel DeviceModel\nFigure2illustratestheoverviewofourCDC-MMPGframeworkwhichconsistsoftwomodules\ntoimprovethegeneralizationabilityofthetrainedmodelsonthedevice:(a)FastDomainAdaptor\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nTokenizer\nPatchifier\nText\nShared\nEncoder\nMulti-modal\nFeature\nExtractor\nConverter Parameters Personalized\nretrevnoC retemaraP rotareneG\n111:6 Trovatoetal.\n(FDA)aimstolearnaglobalbenchmarkmodelbasedonthehistorysamplesofalldistributionsand\ngeneratethenetworkparametersforthedistribution-specificdevicemodelbasedonthereal-time\ndevicesamples(inSec.3.2);(b)AnchorFrameDistributionReasoner (ADR)standardizestheinput\nfortheFDAacrossvariousmulti-modelsamples.(inSec.3.3).\nModelPipeline.AsshowninAlgorithm1,therearethreestepsinthepipelineoftheCDC-\nMMPGmodel:(1)TrainingthecloudmodelM𝑔(.),includingtheFDAmoduleandtheADRmodule,\nwiththehistorysamplesS𝐻.(2)Uploadingthereal-timesamplesS 𝑅(𝑖) fromthedevicesideto\nthecloudside.Then,thecloudsidemodelM𝑔(.) generatesthepersonalizedparametersforthe\ndevicemodelM 𝑑(𝑖)(.).(3)Withthemodelparameterspassedfromthecloudside,thedevicemodel\nM 𝑑(𝑖)(.)isupdatedandmakesthefinalpredictionbasedontheinputread-timesamples.\nMulti-modalFeatureExtraction.Weextractthemulti-modalrepresentationneededforthe\nsubsequentprocessesfromtheinputexamples.Specifically,givenareal-timeinputvideo𝑉 𝑟 anda\ncorrespondinglanguagequery𝑄 𝑟,weemploytheDeiT[32]asthevisualfeatureextractor 𝑓 𝑣(.)\nandtheBERT-baselanguagemodel[7]asthetextencoder𝑓 𝑡(.),toextractthefeaturesoftheinput\nsample.\n𝐹 𝑣 = 𝑓 𝑣(𝑉 𝑟), (2)\n𝐹 𝑡 = 𝑓 𝑡(𝑄 𝑟), (3)\nwhere𝐹 𝑣 = {𝐹 𝑣𝑖} 𝑖𝑁 =𝑓 1representsthefeaturesofallvideoframesand𝑁 𝑓 isthevideoframenumber.\n𝐹 𝑡 isthelanguagequeryfeature.\nWeadoptthespatial-temporalpositionalembeddingandmodalitytypeembeddingfollowing\n[34]totheextractedfeatures𝐹 𝑣 and𝐹 𝑡 andgetthecorrespondingembeddedfeatures𝐸 𝑣 = {𝐸𝑖 𝑣} 𝑖𝑁 =𝑓\n1\nand𝐸 𝑡.Then,thecross-modalfusionmodule𝑔(.)consistedof𝑡 transformerlayersisemployedto\nfusethevisualfeatures𝐸 𝑣 andthelanguagefeature𝐸 𝑡:\n𝐹 𝑚 =𝑔(𝐸 𝑣,𝐸 𝑡), (4)\nwhere𝐹 𝑚 = {𝐹 𝑚𝑖 } 𝑖𝑁 =𝑓 1isthegeneratedmulti-modalfeaturesafterthefeaturefusion.\n3.2 FastDomainAdaptor\nWhenadaptingthecloudmodeltothepersonalizeddevicereal-timesamplesofspecificdistribution,\nonestraightforwardstrategyisfine-tuningthecloudmodelonthedevice.However,itmaybe\nunreachableduetoalackofsufficienttrainingsamplesandcorrespondingannotationsonthe\ndevice.Totackletheaforementionedchallenge,weproposeanovelsolutioncalledtheFastDomain\nAdaptor(FDA),whichisimplementedasacloud-basedservice.TheFDAisdesignedtoprocess\ndevice-capturedimagesasinputandgeneratecustomizedparametersspecificallytailoredtothe\nLightweightMulti-modalModeldeployedonthedevice.Thesecustomizedparametersarecarefully\ncraftedtoadapttotheuniquedatadistributionsencounteredoneachindividualdevice.\nSpecifically,wefurthersample𝐷(𝐷 >1)framesfrom𝐹 𝑚 randomly,uploadthesampledmulti-\nmodal features from the device side to the cloud side, and average the included 𝐷 frames of\nmulti-modalfeaturesintotheglobalrepresentation𝐹 𝑔.Then,weprojectthefeature𝐹 𝑔 beforeitis\nfurtheranalyzed:\n𝐸 𝑔 = 𝑓 𝑝(𝐹 𝑔), (5)\nwhere𝑓 𝑝(.)representsthemulti-layerperceptronconsistingoftwolinearlayersandanormalization\nlayer.Theoutputfeature𝐸 𝑔istreatedasthereal-timesampleembeddingforhyper-network[9,11]\ninput:\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:7\nΘ 𝑑 = 𝑓 ℎ(𝐸 𝑔), (6)\nwhere𝑓 ℎ(.)isthemulti-layerperceptron.ThegeneratedΘ 𝑑 istheparametersforasinglelinear\nlayerincludingboththelinearweightsandbias,whichwillbepassedtothedevicemodel.\nThegeneratedmodelparametersΘ 𝑑 arepassedfromthecloudsidetothedeviceside.Thedevice\nmodelM𝑑 isupdatedwiththemodelparametersΘ 𝑑 andmakestheprediction𝑃 𝑛 fortheinput\nmulti-modelreal-timesample:\n𝑃 𝑛 =M𝑑(𝐹 𝑚;Θ 𝑑), (7)\nwhere𝐹 𝑚 istheextractedmulti-modalfeature.\n3.3 AnchorFrameDistributionReasoner\nThe FDA is designed to adapt to diverse multi-modal tasks by utilizing specific data inputs.\nHowever,certaintasks,suchasVideoQA,requiretransmittingsignificantamountsofdata,suchas\nmultipleframesortheentirevideo,totheFDA.Thisresultsinconsiderablecommunicationcosts\nandbandwidthrequirements.ToaddressthischallengeandimprovetheadaptabilityoftheFDA\nacrossvariousmulti-modaltasks,weintroducetheAnchorFrameDistributionReasoner(ADR).\nADRplaysacrucialroleinstandardizingtheinputformatfortheFDA,ensuringcompatibility\nacrossdifferentmulti-modaltasks.\nSpecifically,duringtheinferenceprocess,werandomlyselectoneframefromthewholevideoon\nthedevicesideanduploadthemulti-modalfeature𝐹 𝑚𝑖 correspondingtotheselected𝑖-thframeto\nthecloudside.Then,theVariationalAuto-Encoder(VAE)includinganencoderΨ 𝐸 andadecoder\nΨ 𝐷areappliedtofurtheranalyze.Firstly,weobtainthelatentdistributionbyanalyzingtheuploaded\nfeature𝐹 𝑚𝑖 withtheVAEencoderΨ 𝐸:\nΨ 𝐸(𝐹 𝑚𝑖 ) ∼N(𝜇𝑟𝑒𝑎𝑙,𝜎𝑟𝑒𝑎𝑙), (8)\nwhereN(𝜇𝑟𝑒𝑎𝑙,𝜎𝑟𝑒𝑎𝑙)representstheGaussiandistributionwithmean𝜇𝑟𝑒𝑎𝑙\nandstandarddeviation\n𝜎𝑟𝑒𝑎𝑙 .Thenwesampleanewrepresentation𝑅 𝑚𝑖 fromtheobtainedspecificdistribution:\n𝑅 𝑚𝑖 ∼N(𝜇𝑟𝑒𝑎𝑙,𝜎𝑟𝑒𝑎𝑙). (9)\nThe𝑅 𝑚𝑖 isthenreasonedonbytheVAEdecoderΨ 𝐷:\n𝐹 𝑚𝑖 ′ =Ψ 𝐷(𝑅 𝑚𝑖 ). (10)\nThe 𝐹 𝑚𝑖 ′ and 𝐹 𝑚𝑖 are utilized to adaptively generate the new feature 𝐹 𝑔′ through the adaptive\ngeneratorΦ 𝑎(.).Thegeneratednewfeatureswillbeforwardedtothehyper-networkwiththegoal\nofleadingittogeneralizetothetargetdomain.Thus,wecanupdatetheEq.5to:\n𝐹 𝑔′ =Φ 𝑎(𝐹 𝑚𝑖 ′,𝐹 𝑚𝑖 ). (11)\nThefeature𝐹 𝑔′ isappliedtoreplacetheglobalrepresentationoftheinputsample𝐹 𝑔 definedin\nSection3.2forfurtherpredictionofthepersonalizedmodelparameters.\nTheframeworkuseslimitedcommunicationbandwidth:one-framemulti-modalfeatureupload\nandhyper-networkparameters(afewlinearlayers)download,promisingthegeneralizationability\nandextremelylowcommunicationdelayofthedevicemodel.\nDuringthetrainingprocess,allhistorysamplesaredirectlysavedonthecloudsideandpartici-\npatedinthetrainingprocess,followingthesettingofpreviousmethods[45].Giventhesampled\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:8 Trovatoetal.\nAlgorithm1:CDC-MMPGFramework\n1\nInitializethemulti-modalfeatureextractionmodel:M𝑣𝑙;thehyper-networkmodelinFDA:\nℎ(·);theencoderandthedecoderinADR:Ψ 𝐸,Ψ 𝐷;theadaptivegenerator:Φ 𝑎.\n2\nPhaseI:Input:HistoricaldataS𝐻.\n3 1)TraintheADRonthecloudusingS𝐻 andthetrainingobjectivesinEq.18.\n4\nOutputthetrainedmodels:M 𝑣′ 𝑙,ℎ′(·),Ψ 𝐸′,Ψ 𝐷′,Φ 𝑎′.\n5\nPhaseII:Input:Real-timedataS𝑅.\n6 1)Extractmulti-modalfeaturesandprocesstheone-framefeaturetothecloud\n𝐹 𝑐𝑖 𝑙𝑜𝑢𝑑,𝐹 𝑔 ←M 𝑣′ 𝑙(S𝑅).\n7\n2)Obtainlatentdistributionof𝐹 𝑐𝑙𝑜𝑢𝑑:N(𝜇,𝜎),where𝜎,𝜇 ←Ψ 𝐸′(𝐹 𝑐𝑖 𝑙𝑜𝑢𝑑).\n8\n3)Randomlysamplefromthisdistribution𝐹\n𝑟𝑎𝑛𝑑\n←N(𝜇,𝜎).\n9\n4)Decodetherandomlysampleddistribution𝐹 𝑟′\n𝑎𝑛𝑑\n←Ψ 𝐷′ (𝐹 𝑟𝑎𝑛𝑑).\n10 5)Getadaptivelayerembedding𝐸 𝑚 ←Φ 𝑎(,𝐹 𝑟′ 𝑎𝑛𝑑,𝐹 𝑐𝑖 𝑙𝑜𝑢𝑑).\n11 6)GenerateadaptiveparametersusingthelayerembeddingΘ 𝑎 ←ℎ′(𝐸 𝑚).\n12 Outputtheparameters:Θ 𝑎.\n13\nPhaseIII:Input:Real-timedataS𝑅.\n14 1)DownloadtheadaptiveparameterstothedeviceΘ𝑑 𝑎 ←Θ 𝑎.\n15\n2)SharefeaturebackbonebetweenthecloudanddeviceM𝑑 ←M𝑣𝑙.\n16 3)MakethepredictionwithΘ𝑑 𝑎:𝑃 𝑛 ←M𝑑(S𝑅;Θ𝑑 𝑎)\n17 Outputthe𝑃 𝑛.\nmulti-modal features 𝐹 𝑔 and the randomly selected feature 𝐹 𝑚𝑖 , the variational auto-encoder is\nappliedtofurtheranalyzethesameastheinferenceprocess.Firstly,theencoderΨ 𝐸 project𝐹 𝑚 into\nthelatentspaceandgenerateanewfeature𝐹 ℎ:\n𝐹 ℎ =Ψ 𝐸(𝐹 𝑔). (12)\nThen,toreconstructtheinputfeature𝐹 𝑔,thedecoderΨ\n𝑑\ndecodesthefeature𝐹\nℎ\ntothefeature𝐹 𝑔′:\n𝐹 𝑔′ =Ψ 𝐷(𝐹 ℎ). (13)\nInordertotrainthevariationalauto-encoder,weusetheKullback-Leibler(KL)loss𝐿 𝑘𝑙 and\nreconstructionloss𝐿 𝑅𝑒𝑐 asfollows:\n𝐿 𝑘𝑙 =D 𝑘𝑙(N(𝜇′,𝜎′)||N(0,I)), (14)\n𝐿 =MSE(F ,F′), (15)\nRec g g\nwhereN(0,I)representsthenormaldistribution,D 𝑘𝑙(.)representstheKullback-Leiblerdivergence\ncalculationandMSE(·)denotesthemeansquareloss.\nInspiredby[21],inordertoincorporatethevariousdistributionshiftsintothefeatureofthe\nsinglevideoframe,theadaptivegeneratorgeneratesnewfeatures𝐹 𝑎 byre-normalizingthe𝐹 𝑔′ to\nhavethesamechannel-wisemeanandstandarddeviationasthe𝐹 𝑚𝑖 .Thisprocesscanbeformulated\nasfollows:\n(cid:18)𝐹𝑖 −𝛿(𝐹𝑖 )(cid:19)\nΦ 𝑎(𝐹 𝑔′,𝐹 𝑚𝑖 ) =𝛿(𝐹 𝑔′) 𝑚\n𝛾(𝐹𝑖\n)𝑚 +𝛾(𝐹 𝑔′), (16)\n𝑚\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:9\nwhere𝛿(·)and𝛾(·)denoteschannel-wisemeanandstandarddeviationoperations,respectively.\nFollowingpreviousworksonvariationalauto-encodergeneralization,weadditionallycompute\nthelossvaluesbetweenthegeneratednewfeaturesand𝐹 𝑔,𝐹 𝑚𝑖 ,respectively,totrainthewhole\nmodule:\n𝐿 =𝜆MSE(F ,F )+MSE(F ,Fi ), (17)\nAG a g a m\nwhere𝜆denotesahyper-parameter,and𝐹 denotesthefeaturegeneratedbytheadaptivegenerator.\na\nThus,theoveralltrainingloss𝐿forthewholeframeworkis:\n𝐿 =𝐿 +𝐿 +𝐿 . (18)\nAG Rec KL\nTable1. ResultsofourproposedmethodinOpen-endedVideoQAtaskonMSRVTT-QAandMSVD-QA.We\nadoptaccuracyastheevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberof\nlearnableparametersonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofour\nproposedmethod.“F-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeatureextractor.\n“F-hyper\"denotesonlyfine-tuningtheclassifiersandasimplehyper-networkwithoutanadaptivegenerator.\nVSRVTT-QA MVSD-QA\nMethods\nAcc. D-Param. C-Param. TimeDelay Acc. D-Param. C-Param. TimeDelay\nF-linear 13.6 11.6M / 60000ms 17.3 11.5M / 60000ms\nFine-tuning 36.7 11.6M / 60000ms 34.3 11.5M / 60000ms\nF-hyper 11.1 11.6M 28.1M 5.55ms 6.34 11.5M 18.7M 3.71ms\nOurs 37.1 11.6M 55.9M 5.55ms 35.4 11.5M 46.5M 3.71ms\nTable2. ResultsofourproposedmethodinOpen-endedVideoQAtaskonTGIF.Weadoptaccuracyasthe\nevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberoflearnableparameters\nonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofourproposedmethod.\n“F-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeatureextractor.“F-hyper\"denotes\nonlyfine-tuningtheclassifiersandasimplehyper-networkwithoutanadaptivegenerator.\nTGIF\nMethods\nAccuracy D-Param. C-Param. TimeDelay\nF-linear 27.3 11.6M / 60000ms\nFine-tuning 54.7 11.6M / 60000ms\nF-hyper 19.6 11.6M 18.7M 5.70ms\nOurs 55.2 11.6M 46.5M 5.70ms\n4 EXPERIMENT\nInthissection,weevaluateourproposedmethodonthreepopularmulti-modaltasksandfour\nrelevantdatasets.Additionally,weperformablationstudiestoinvestigatetheimpactofeachmodule\ninourmethod,theinfluenceofthenumberofframessampledfromthewholevideo,andthetime\ndelaybetweenthecloudandthedevicewithvariousinternetconditions.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:10 Trovatoetal.\n4.1 Datasets\nWeconductedourexperimentsonthreemulti-modaltasks:multiple-choiceVideoQA,open-ended\nVideoQA,andvideo-textretrieval.Fortheopen-endedVideoQAtask,weusetheMSRVTT-QA\ndataset[39],MSVD-QAdataset[39]andtheTGIFdataset[17].Forthemultiple-choiceVideoQA\ntask,weusetheMSRVTT-QAdataset[39].Forthevideo-textretrievaltask,weexperimentonthe\nMSRVTTdataset[40].\n(1) MSRVTT.TheMSRVTTisalarge-scaledatasetfortheopen-domainvideocaptioning,which\nconsistsof10Kvideoclipsandeachvideoclipisannotatedwith20Englishsentences.The\nstandardsplitsuse6,513clipsfortraining,497clipsforvalidation,and2,990clipsfortesting.\n(2) MSRVTT-QA.TheMSRVTT-QAdatasetcontains10Kvideoclipsand243kquestionanswer\npairs.Specifically,weusedMSRVTT-QAdataset[39]foropen-endedVQAandmultiple-\nchoiceVQA.Following[39],Weusethetraditionaldatasplitwhichis65%forthetraining\nset,5%forthevalidationsetand30%fortestset.\n(3) MSVD-QA.TheMSVD-QAdatasethasatotalnumberof1,970videoclipsand50,505question\nanswerpairs.Following[39],wesplitthedatasetbasedonvideosthattrainingsettakes61%,\nvalidationsettakes13%,andtestsettakes26%ofthetotalnumberofvideos.\n(4) TGIF. The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences\ndescribingvisualcontentoftheanimatedGIFs.Onesentenceisprovidedforeveryanimated\nGIF for the training and validation splits, and three sentences per GIF for the test split.\nSpecifically,thereare80Ktrainingsamples,11Kvalidationsamplesand11ktestsamples.\n4.2 EvaluationMetrics\nFollowing previous works, we adopt accuracy for the open-ended Video QA task, and VR@K,\nTR@Kforvideo-textretrievaltask.Specifically,VR@Kdenotestherecallofvideototextretrieval\nandTR@Kdenotestherecalloftexttovideoretrieval.Forbothofthesetwotasks,Kissetto\n1,2,5respectively.Additionally,wecalculatethenumberoflearnableparametersineachmodel.\nMeanwhile,forthepracticalscenarioofcloud-devicecollaboration,wealsomeasurethetimedelay\nforthecloud-devicecommunicationprocess.\n4.3 TasksandImplementationDetails\nWeevaluateourmethodonthreetasks:\n(1) Video-textRetrieval:Weevaluatetwosub-tasksincludingvideo-to-textretrievalandtext-\nto-videoretrieval.Thetext-to-videoretrievaltaskrequiresretrievingthetargetvideousing\nTable3. ResultsofourproposedmethodinText-videoRetrievaltask.Thenumberoflearnableparameterson\nthedevice,thenumberoflearnableparametersonthecloud,andthetimedelayareadditionallymeasured\ntoshowtheefficiencyofourproposedmethod.\nMSRVTT\nMethods\nVR@1 VR@5 VR@10 TR@1 TR@5 TR@10\nF-linear 2.0 7.7 14.1 3.1 10.1 16.9\nFine-tuning 4.8 17.6 27.9 6.2 20.7 31.8\nF-hyper 2.6 8.6 14.7 2.7 11.4 17.6\nOurs 6.6 21.3 33.8 5.6 21.2 33.3\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:11\nTable4. Ablationstudiesofdifferentnumbersofsampledframes.Wemaketheevaluationonthreedatasets\ninOpen-endedVideo-questionAnsweringtask.“Time/Epoch\"denotesthetimeoffine-tuningforeachepoch.\nMSRVTT-QA MSVD-QA TGIF\nFrames\nAccuracy Time/Epoch Accuracy Time/Epoch Accuracy Time/Epoch\n2 36.1 740s 34.1 204s 54.3 245s\n3 36.7 897s 34.3 228s 54.7 310s\n4 37.0 1309s 34.3 318s 54.7 448s\n5 37.1 2092s 34.0 377s 55.2 702s\nlanguagequeries.Thevideo-to-textretrievaltaskrequiresretrievingthetargettextusing\nvideos.\n(2) Open-endedVideo-questionAnswering:Itrequiresansweringquestionsaccordingto\nthecontextofthevideo.Theanswersareoriginallyinfree-formnaturallanguage,butitisa\ncommonpracticetoconvertthetasktoaclassificationtaskbyrepresentingtheanswerwith\naclasslabel.\n(3) Multiple-choiceVideo-questionAnswering:Givenavideowithaqueryand5candidate\ncaptions,thetaskistofindtheonethatfitsthequeryoutof5possiblecandidates.Thecorrect\nansweristheground-truth(GT)caption,andfourothernegativesarechosenfromother\ncaptionsthathavedifferentactivity-phraselabelsfromthecorrectanswer.\nImplementation Details. We employ the All-in-One-Ti [34] as the baseline model, which\nincludestheDeiT[32]asitsvisualbackboneandBERT-base[7]asitssemanticencoder.Specifically,\nweonlyusetheembeddinglayersofBERTfortextembedding.TheAll-in-one-Tiisadoptedas\nthemulti-modalencoderbothforthedevicemodelandthecloudmodel.Thevalueof𝐷 isset\nto3.DuringthetrainingprocessofADR,weemploytheadamwoptimizerwithapolynomial\ndecayscheduler.Thelearningrateissetto2e-5andthetrainingepochissetto10.Duringthe\ntrainingprocessoftherestofthemodules,wefrozetheparametersintheadaptivegenerator,\nandthesameadamwoptimizerisusedwiththe1e-4learningrateused.Thetotaltrainingepoch\nfortherestmodulesissetto40.Forthehyper-parametersandothersettingsduringthetraining\nprocess,𝜆issetto0.1,andthehiddenlayerofthehyper-networkissetto96(forOpen-endedVQA\nandMultiple-choiceVQA)and256(forVideo-textRetrieval).Duringtheinferenceprocess,allthe\nparametersonthedevicemodelandthecloudmodelarefrozenexceptthelastfewdynamiclinear\nlayersofthedevicemodel.Thecloudmodelandthedevicemodelsharethesameparametersof\nthemulti-modalencoder.Thenthecloudgeneratesdynamicparametersusinghyper-networkfor\nthedevicemodelforbettergeneralization.\n4.4 PerformanceComparison\nInthissection,weintroducehowwedesignthebaselinemethodtoshowcasetheefficiencyofour\nproposedmethod,andwepresenttheexperimentalresultsofourmethodonvarioustasksand\ndatasets.\n4.4.1 BaselineMethods.\nForwearethefirsttoexplorethisfield,wedesigntheotherthreebaselinemethodstoshow\nthesuperiorityofourproposedmethod.AsshowninTab.1andTab.2,weadoptaccuracyasthe\nevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberoflearnable\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:12 Trovatoetal.\nparametersonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofour\nproposedmethod.“F-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeature\nextractor.“F-hyper\"denotesonlyfine-tuningtheclassifiersandasimplehyper-networkwithout\nanadaptivegenerator.Besidesourproposedmethod,wealsodesignthefine-tuningapproach,\nfine-tuningonlythelinearclassifiersapproach(F-linear),andfine-tuningboththelinearclassifiers\nandsolelyhyper-networkapproach.Forthefine-tuningapproach,wesimplyaddlayersofMLPs\nafterthemulti-modalfeatureextractoranddofine-tuningonthewholemodel.Forfine-tuning\nonlythelinearclassifiersapproach,wefrozetheparametersinthemulti-modalfeatureextractor\nandfine-tunedthelayersofMLPsafterthefeatureextractor.Forthefine-tuningofboththelinear\nclassifiersandthesolelyhyper-networkapproach,weemployasimplehyper-networkwithout\nthe proposed adaptive generator and do fine-tuning for both linear classifiers and the simple\nhyper-network.\nOpen-endedVideo-questionAnswering.ForOpen-endedVideo-questionAnswering,the\nresponsesaretypicallyexpressedinunrestrictednaturallanguage.However,aprevalentapproach\ninvolvestransformingthistaskintoaclassificationproblembyencodingtheanswersasclasslabels.\nToachievethis,weincorporatelayersofMLPswithahiddenlayerdimensionof96followingthe\nextractionofmulti-modalfeatures.ThedimensionoftheMLP’soutputlayervariesaccordingto\nthespecificlabelsizeassociatedwiththedatasets,forinstance,1501labelsfortheMSRVTT-QA\ndataset.\nMultiple-choiceVideo-questionAnswering.ForMultiple-choiceVideo-questionAnswering,\nwhereboththequestionsandcandidateanswersarepresentedassentences,ourapproachinvolves\nconcatenatingthequestionandtheanswercandidates.Todistinguishbetweenthem,weemploy\nthespecialtoken[SEP].Subsequently,wedeterminethepredictionbyselectingthecandidatewith\nthehighestoutputlogit,signifyingitslikelihoodofbeingthecorrectanswer.\nVideo-textRetrieval.ForVideo-textRetrieval,theretrievalprocessencompassestwodirections:\ntext-to-videoretrievalandvideo-to-textretrieval.Ineachdirection,therespectivemodalityisfirst\nextracted,andsubsequently,acomparativeanalysisisconducted.Thepredictionsareultimately\ngeneratedthroughtheapplicationofMLPs.\n4.4.2 MainResults.\nInthissection,wepresenttheexperimentalresultsofopen-endedVideoQAinTab.1andTab.2,\nmultiple-choiceVideoQAinTab.5,andVideo-textRetrievalinTab.3onfourpopulardatasets.\nEffectiveness.Tab.1,Tab.2,Tab.5,andTab.3provideacomprehensiveoverviewofthesuperi-\norityofourmethodcomparedtootherbaselineapproachesacrossvariousdatasetsandevaluation\ncriteria.Notably,ourmethodconsistentlyoutperformsalternativebaselinemodelsinnearlyall\naspectsacrossthesedatasets.Forexample,considerthetaskofOpen-endedVideo-questionAn-\nsweringontheMSRVTT-QAdataset.Ourproposedmethodexcelsintermsofbothaccuracyand\ntimeefficiencywhencomparedtoallotherbaselinemodels.Theconventionalfine-tuningmethod\ndemandsasignificantamountoftimetocompletethefine-tuningprocess.Asdemonstratedin\nTab.4,whenselectingthreeframesfromtheentirevideo,thetimerequiredforeachfine-tuning\nepochontheMSRVTTdatasetisapproximately897seconds.Incontrast,ourmethodachieves\nnearlyreal-timecommunication,effectivelyreducingthetimedelaybetweenthecloudandthe\ndevicetoaslittleas5.55ms,contingentoninternetconditions.Furthermore,ourproposedmethod\neven surpasses the fine-tuning approach in terms of performance, indicating its superior fast\ngeneralizationcapabilitiesonpersonalizedsamples.Tab.1andTab.2,whichdenotetheresultson\ntheMSVD-QAandTGIFdatasets,itisevidentthatourmethodsignificantlyoutperformsother\nbaselinemodels,thusaffirmingtheeffectivenessofourproposedapproach.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:13\nTable5. ResultsofourproposedCDC-MMPGframeworkinthemultiple-choiceVideoQAtask.Thenumber\noflearnableparametersonthedevice,thenumberoflearnableparametersonthecloud,andthetimedelay\naremeasuredtoshowtheefficiencyofourproposedmethod.\nMSRVTT-QA\nMethods TimeDelay\nAcc. D-Param. C-Param.\nF-linear 3.58 11.4M / ≥60000ms\nFine-tuning 75.6 11.4M / ≥60000ms\nF-hyper 46.0 11.4M 37.2K ≥5.70ms\nOurs 77.0 11.4M 20.3M ≥5.70ms\nTable6. Timedelayofourframeworkindifferentcircumstances(variousinternetspeeds)ondifferentdatasets.\nOurproposedmethodlargelyreducesthetimedelaybetweenthecloudandthedevice,realizingalmost\nreal-timecommunication.“ ↑ ”denotestheuploadprocessfromthedevicetocloud.“ ↓ ”denotesthe\nparametersdownloadedfromthecloudtodevice.\nDatasets Size 4G:5MB/s 4G:15MB/s 5G:50MB/s 5G:100MB/s\n↑:0.75KB ↑:0.15ms ↑:0.05ms ↑:0.01ms ↑:0.007ms\nMSRVTT\n↓:568.5KB ↓:111ms ↓:37.0ms ↓:11.1ms ↓:5.55ms\n↑:0.75KB ↑:0.15ms ↑:0.05ms ↑:0.01ms ↑:0.007ms\nMSVD\n↓:379.4KB ↓:74ms ↓:24.7ms ↓:7.41ms ↓:3.71ms\n↑:0.75KB ↑:0.15ms ↑:0.05ms ↑:0.01ms ↑:0.007ms\nTGIF\n↓:583.8KB ↓:114ms ↓:38.0ms ↓:11.4ms ↓:5.70ms\nExtensibility.Ourproposedapproachexhibitsthecapacitytoenhanceaccuracywhileconcur-\nrentlyreducingtimedelaysacrossalldatasetsassociatedwiththethreespecifictasks.Asdelineated\nin Tab. 3, our methodology yields a significant performance improvement, particularly in the\ndomainoftext-videoretrieval.Ourassessmentencompassesatwo-directionalapproach,addressing\nboth text-to-video retrieval and video-to-text retrieval. Remarkably, our method surpasses the\nmajorityofbaselinemethods,notonlyintermsofaccuracybutalsowithrespecttotimedelays.In\nthecontextofmultiple-choiceVideoQA,ourapproachconsistentlyassertsitssuperiorityacross\nallevaluationmetrics.ThisisevidentfromthedatapresentedinTab.5.\n4.5 AblationStudies\nNumberofsampledframes.InTab.4,weinvestigatetheinfluenceofthenumberofsampled\nframes𝐷 fromtheentirevideoonthefinalresults.Simultaneously,wemeasurethetimeoffine-\ntuningforeachepoch.Tobespecific,weemploythestandardfine-tuningapproachtoevaluate\nthisfactoracrossthreedistinctdatasets,allintheOpen-endedVideo-questionAnsweringtask.\nThebatchsizeforallthedatasetsissetto256,andthehardwareisasingleA100GPU.Itbecomes\nevidentthatasthenumberofsampledframesincreases,thetimeoffine-tuningforeachepoch\nexperiencesarapidescalation,whichisinlinewithintuition.Forinstance,whenonlytwoframes\naresampled,thefine-tuningprocessdemandsapproximately740secondsonMSRVTT-QAdataset.\nHowever,iffiveframesaresampledfromeachvideo,thetimerequirementextendstoaround2092\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:14 Trovatoetal.\nseconds,whichisalmostthreetimeslongerthantheformerscenario.Interestingly,themodel’s\nperformancedoesnotexhibitaconsistentupwardtrendwiththeincreasingnumberofsampled\nframes.Besides,themodel’sperformancereachesitspeakwhenthenumberofsampledframesis\nsetatthreeorfour.Tomakeatrade-offbetweenperformanceandtrainingcost,wesetthenumber\nofsampledframesasthreeinourmodel.\nDifferentmodulesinourproposedCDC-MMPGframework.Inaccordancewiththeresults\ninTab.1andTab.2,wehavedevisedthreeadditionalbaselinemethodstoassesstheefficacyofour\ncomprehensiveframework.Tobemorespecific,ourassessmentinvolvesacomparativeanalysis\noftheperformanceoutcomesbetweenthefine-tuningmodelandourproposedmodel,serving\ntoemphasizethelatter’ssuperiority.Furthermore,wehaveundertakenanevaluationofthe\"F-\nlinear\"modeltofacilitateanablationstudyoftheclassifierssubsequenttothemulti-modalfeature\nextractor.Inaddition,the\"F-hyper\"modelhasbeenexaminedinordertoconductanablationstudy\nfocusedonthehyper-networkandtheadaptivegenerator.Itisnoteworthythattheomissionofour\nproposedadaptivegeneratorrendersasimplehyper-networkpronetoencounteringsub-optimal\nperformance, particularly evident in certain datasets, including MSRVTT-QA, MSVD-QA, and\nTGIF. This diminished performance is primarily attributed to the issue of over-fitting, leading\nto the emergence of spurious correlations between visual cues and predictions. Our adaptive\ngenerator,asproposed,substantiallyimprovestheperformanceofthehyper-networkthroughthe\nimplementationofanchor-framedistributionreasoning.\nTimedelaybetweencloudanddeviceinvariousinternetconditions.Tab.6illustratesthe\ntimedelayassociatedwithdatatransmissionbetweencloudservicesanddevicesundervarious\ninternetconnectivityconditions.Specifically,wehaveconductedthisexperimentinfourdistinct\nscenarios,eachreflectingadifferentinternetcondition:4G(5MB/s),4G(15MB/s),5G(50MB/s),and\n5G(100MB/s).Thesescenarioscorrespondtoreal-worldsituationsinvolvingoldermobiledevices\nwithlimited4Gconnectivity,oldermobiledeviceswithoptimal4Gconnectivity,modernmobile\ndeviceswith5Gconnectivity,andmodernmobiledeviceswithadvanced5Ginternetconnectivity.\nOurproposedmethoddemonstratesremarkableimprovementsintermsoftimedelay.Forexample,\nduringtheinferenceprocess,amere0.75KBoffeaturesisrequiredtobeuploadedtothecloud,a\ntaskthatconsumesonlyabout0.007msina5G(100MB/s)environment.Eveninthepresenceof\nconstrained4G(5MB/s)internet,thetimerequiredforuploadingremainsnegligible.Thedurationof\nparameterdownloading,however,fluctuatesaccordingtothedatasetinuse,largelyduetovariations\ninparametersize.Forinstance,theparametersizeis583.8KBfortheTGIFdataset,thelargestamong\nthedatasets,whereastheparametersizefortheMSVD-QAdataset,thesmallest,amountsto379.4KB.\nNotably,evenwhendealingwiththedemandingTGIFdataset,ourproposedapproachdemonstrates\nthecapabilitytoachievereal-timepredictions.Forinstance,ina5G(100MB/s)environment,the\ntimerequiredforparameterdownloadingismerely5.70ms.Eveninscenariosfeaturinglimited4G\n(5MB/s)connectivity,thetimedelayremainsnearlyimperceptibleinpracticalterms.\n5 CONCLUSION\nInthispaper,wefocusonthechallengeofenablingefficientandeffectiveadaptationofAIsystems\ntopersonalizedmulti-modaldatageneratedbyintelligentdevices.Theshiftingdatadistribution\nbetween the cloud and devices necessitates novel approaches to ensure high-quality personal-\nizedservices.Weintroducedauniversalon-deviceMulti-modalModelAdaptationFramework,\nfeaturing the Fast Domain Adaptor (FDA) and the AnchorFrame Distribution Reasoner (ADR).\nFDA,hostedinthecloud,tailorsmodelparametersforon-deviceLightweightMulti-modalModels,\noptimizingadaptabilityacrossdiversedatadistributions.ADRfurtherstandardizesinput,reducing\ncommunication costs. Our contributions, consolidated within the Cloud-Device Collaboration\nMulti-modalParameterGeneration(CDC-MMPG)framework,constituteapioneeringsolutionfor\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:15\non-DeviceMulti-modalModelAdaptation(DMMA),demonstratedthroughextensiveexperiments.\nLookingforward,promisingdirectionsincludeexpandingtheframeworktoaccommodatevarious\nmodalities,refiningpersonalizeddatahandlingtechniques,andfurtherreducingcommunication\ncostsinmulti-modaltasks.\nREFERENCES\n[1] SabbirAhmed,AbdullahAlArafat,MamshadNayeemRizve,RahimHossain,ZhishanGuo,andAdnanSirajRakin.\n2023.SSDA:SecureSource-FreeDomainAdaptation.InICCV.\n[2] ChaoqiChen,WeipingXie,WenbingHuang,YuRong,XinghaoDing,YueHuang,TingyangXu,andJunzhouHuang.\n2019.Progressivefeaturealignmentforunsuperviseddomainadaptation.InCVPR.\n[3] Hong-YouChen,JikeZhong,MingdaZhang,XuhuiJia,HangQi,BoqingGong,Wei-LunChao,andLiZhang.2023.\nFederatedLearningofShareableBasesforPersonalization-FriendlyImageClassification. arXiv:2304.07882[cs.CV]\nhttps://arxiv.org/abs/2304.07882\n[4] PengfeiChen,LeidaLi,JinjianWu,WeishengDong,andGuangmingShi.2021. Unsupervisedcurriculumdomain\nadaptationforno-referencevideoqualityassessment.InICCV.\n[5] LiamCollins,HamedHassani,AryanMokhtari,andSanjayShakkottai.2022.Fedavgwithfinetuning:Localupdates\nleadtorepresentationlearning.NIPS(2022).\n[6] VictorG.TurrisidaCosta,GiacomoZara,PaoloRota,ThiagoOliveira-Santos,NicuSebe,VittorioMurino,andElisa\nRicci.2022.Dual-HeadContrastiveDomainAdaptationforVideoActionRecognition.InWACV.\n[7] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectional\nTransformersforLanguageUnderstanding.InNAACL.\n[8] NingDing,YixingXu,YehuiTang,ChaoXu,YunheWang,andDachengTao.2022.Source-FreeDomainAdaptation\nviaDistributionEstimation.InCVPR.\n[9] TanM.Dinh,AnhTuanTran,RangNguyen,andBinh-SonHua.2022.HyperInverter:ImprovingStyleGANInversion\nviaHypernetwork.InCVPR.\n[10] RinonGal,OrPatashnik,HaggaiMaron,AmitH.Bermano,GalChechik,andDanielCohen-Or.2022.StyleGAN-NADA:\nCLIP-GuidedDomainAdaptationofImageGenerators.ACMTrans.Graph.(2022).\n[11] DavidHa,AndrewDai,andQuocV.Le.2017.HyperNetworks.InICLR.\n[12] WeiJi,LiLi,HaoFei,XiangyanLiu,XunYang,JunchengLi,andRogerZimmermann.2024.TowardsComplex-query\nReferringImageSegmentation:ANovelBenchmark. ACMTrans.MultimediaComput.Commun.Appl.(Nov.2024).\nhttps://doi.org/10.1145/3701733\n[13] LiLi,WeiJi,YimingWu,MengzeLi,YouQin,LinaWei,andRogerZimmermann.2024.PanopticSceneGraphGeneration\nwithSemantics-PrototypeLearning.AAAI38,4(Mar.2024),3145–3153. https://doi.org/10.1609/aaai.v38i4.28098\n[14] LiLi,YouQin,WeiJi,YuxiaoZhou,andRogerZimmermann.2024. Domain-WiseInvariantLearningforPanoptic\nSceneGraphGeneration.InICASSP.3165–3169. https://doi.org/10.1109/ICASSP48485.2024.10447193\n[15] LiLi,ChenweiWang,YouQin,WeiJi,andRenjieLiang.2023.Biased-PredicateAnnotationIdentificationviaUnbiased\nVisualPredicateRepresentation.InACMMM(MM’23).4410–4420. https://doi.org/10.1145/3581783.3611847\n[16] MingLi,JikeZhong,ChenxinLi,LiuzhuozhengLi,NieLin,andMasashiSugiyama.2024.Vision-LanguageModel\nFine-TuningviaSimpleParameter-EfficientModification. arXiv:2409.16718[cs.CV] https://arxiv.org/abs/2409.16718\n[17] YunchengLi,YaleSong,LiangliangCao,JoelTetreault,LarryGoldberg,AlejandroJaimes,andJieboLuo.2016.TGIF:\nANewDatasetandBenchmarkonAnimatedGIFDescription.InCVPR.\n[18] MattiaLitrico,AlessioDelBue,andPietroMorerio.2023.GuidingPseudo-LabelsWithUncertaintyEstimationfor\nSource-FreeUnsupervisedDomainAdaptation.InCVPR.\n[19] NannanLu,HanhanXiao,ZhanguoMa,TongYan,andMinHan.2022. DomainAdaptationWithSelf-Supervised\nLearningandFeatureClusteringforIntelligentFaultDiagnosis.IEEETransactionsonNeuralNetworksandLearning\nSystems(2022),1–14. https://doi.org/10.1109/TNNLS.2022.3219896\n[20] YaweiLuo,PingLiu,TaoGuan,JunqingYu,andYiYang.2020.AdversarialStyleMiningforOne-ShotUnsupervised\nDomainAdaptation.InNIPS.\n[21] YaweiLuo,PingLiu,TaoGuan,JunqingYu,andYiYang.2020.AdversarialStyleMiningforOne-ShotUnsupervised\nDomainAdaptation.InNIPS,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.).\n[22] ZheqiLv,ZhengyuChen,ShengyuZhang,KunKuang,WenqiaoZhang,MengzeLi,BengChinOoi,andFeiWu.\n2023.Ideal:Towardhigh-efficiencydevice-cloudcollaborativeanddynamicrecommendationsystem.arXivpreprint\narXiv:2302.07335(2023).\n[23] ZheqiLv,WenqiaoZhang,ShengyuZhang,KunKuang,FengWang,YongweiWang,ZhengyuChen,TaoShen,Hongxia\nYang,BengChinOoi,etal.2023.DUET:ATuning-FreeDevice-CloudCollaborativeParametersGenerationFramework\nforEfficientDeviceModelGeneralization.InWWW.3077–3085.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:16 Trovatoetal.\n[24] XianzhengMa,ZhixiangWang,YachengZhan,YinqiangZheng,ZhengWang,DengxinDai,andChia-WenLin.2022.\nBothStyleandFogMatter:CumulativeDomainAdaptationforSemanticFoggySceneUnderstanding.InCVPR.\n[25] AhmadrezaMosallanezhad,MansoorehKarami,KaiShu,MichelleVMancenido,andHuanLiu.2022.Domainadaptive\nfakenewsdetectionviareinforcementlearning.InWWW.3632–3640.\n[26] HodLipsonOscarChang,LamprosFlokas.2020.PrincipledWeightInitializationforHypernetworks.InICLR.\n[27] SubhankarRoy,EvgenyKrivosheev,ZhunZhong,NicuSebe,andElisaRicci.2021.Curriculumgraphco-teachingfor\nmulti-targetdomainadaptation.InCVPR.\n[28] CristianoSaltori,FabioGalasso,GiuseppeFiameni,NicuSebe,FabioPoiesi,andElisaRicci.2023. Compositional\nSemanticMixforDomainAdaptationinPointCloudSegmentation. TPAMI(2023),1–14. https://doi.org/10.1109/\nTPAMI.2023.3310261\n[29] YangShu,ZhangjieCao,MingshengLong,andJianminWang.2019.Transferablecurriculumforweakly-supervised\ndomainadaptation.InAAAI.\n[30] TaoSun,ChengLu,TianshuoZhang,andHaibinLing.2022.SafeSelf-RefinementforTransformer-BasedDomain\nAdaptation.InCVPR.\n[31] ZhensuSun,LiLi,YanLiu,XiaoningDu,andLiLi.2022.Ontheimportanceofbuildinghigh-qualitytrainingdatasets\nforneuralcodesearch.InICSE.1609–1620. https://doi.org/10.1145/3510003.3510160\n[32] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerveJegou.2021.\nTrainingdata-efficientimagetransformersamp&distillationthroughattention.PMLR.\n[33] Cheng-HaoTu,Hong-YouChen,ZhedaMai,JikeZhong,VardaanPahuja,TanyaBerger-Wolf,SongGao,Charles\nStewart,YuSu,andWei-LunChao.2023.HolisticTransfer:TowardsNon-DisruptiveFine-TuningwithPartialTarget\nData. arXiv:2311.01420[cs.LG] https://arxiv.org/abs/2311.01420\n[34] AlexJinpengWang,YixiaoGe,RuiYan,GeYuying,XudongLin,GuanyuCai,JianpingWu,YingShan,XiaohuQie,\nandMikeZhengShou.2023.AllinOne:ExploringUnifiedVideo-LanguagePre-training.CVPR(2023).\n[35] FanWang,ZhongyiHan,YongshunGong,andYilongYin.2022.ExploringDomain-InvariantParametersforSource\nFreeDomainAdaptation.InCVPR.\n[36] HaixinWang,JinanSun,XiangWei,ShikunZhang,ChongChen,Xian-ShengHua,andXiaoLuo.2023. DANCE:\nLearningADomainAdaptiveFrameworkforDeepHashing.InWWW.3319–3330.\n[37] RuiWang,ZuxuanWu,ZejiaWeng,JingjingChen,Guo-JunQi,andYu-GangJiang.2023.Cross-DomainContrastive\nLearningforUnsupervisedDomainAdaptation.TMM25(2023),1665–1673.\n[38] ZhouXian,ShamitLal,Hsiao-YuTung,EmmanouilAntoniosPlatanios,andKaterinaFragkiadaki.2021.HyperDynam-\nics:Meta-LearningObjectandAgentDynamicswithHypernetworks.InICLR.\n[39] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang.2017.VideoQuestion\nAnsweringviaGraduallyRefinedAttentionoverAppearanceandMotion.ACMMM.\n[40] JunXu,TaoMei,TingYao,andYongRui.2016.MSR-VTT:ALargeVideoDescriptionDatasetforBridgingVideoand\nLanguage.CVPR.\n[41] YikaiYan,ChaoyueNiu,RenjieGu,FanWu,ShaojieTang,LifengHua,ChengfeiLyu,andGuihaiChen.2022.On-Device\nLearningforModelPersonalizationwithLarge-ScaleCloud-CoordinatedDomainAdaption.InKDD.2180–2190.\n[42] CeyuanYang,YujunShen,ZhiyiZhang,YinghaoXu,JiapengZhu,ZhirongWu,andBoleiZhou.2023. One-Shot\nGenerativeDomainAdaptation.InICCV.\n[43] JihanYang,ShaoshuaiShi,ZheWang,HongshengLi,andXiaojuanQi.2023. ST3D++:DenoisedSelf-Trainingfor\nUnsupervisedDomainAdaptationon3DObjectDetection.TPAMI45,5(2023),6354–6371. https://doi.org/10.1109/\nTPAMI.2022.3216606\n[44] ShiqiYang,yaxingwang,kaiwang,ShanglingJui,andJoostvandeWeijer.2022.AttractingandDispersing:ASimple\nApproachforSource-freeDomainAdaptation.InNIPS,S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,and\nA.Oh(Eds.).\n[45] JiangchaoYao,FengWang,KunyangJia,BoHan,JingrenZhou,andHongxiaYang.2021.Device-CloudCollaborative\nLearningforRecommendation.InKDD.\n[46] JunjieYe,ChanghongFu,GuangzeZheng,DandaPaniPaudel,andGuangChen.2022.UnsupervisedDomainAdaptation\nforNighttimeAerialTracking.InCVPR.\n[47] LipingYi,XiaorongShi,NanWang,ZiyueXu,GangWang,andXiaoguangLiu.2023. pFedLHNs:Personalized\nFederatedLearningviaLocalHypernetworks.InICANN2023.\n[48] AoZhang,HaoFei,YuanYao,WeiJi,LiLi,ZhiyuanLiu,andTat-SengChua.2023.VPGTrans:TransferVisualPrompt\nGeneratoracrossLLMs.InNeurIPS,Vol.36.20299–20319.\n[49] ChengZhang,Tai-YuPan,TianleChen,JikeZhong,WenjinFu,andWei-LunChao.2022.LearningwithFreeObject\nSegmentsforLong-TailedInstanceSegmentation. arXiv:2202.11124[cs.CV] https://arxiv.org/abs/2202.11124\n[50] ChrisZhang,MengyeRen,andRaquelUrtasun.2020.GraphHyperNetworksforNeuralArchitectureSearch.InICLR.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:17\n[51] JingyiZhang,JiaxingHuang,ZichenTian,andShijianLu.2022.SpectralUnsupervisedDomainAdaptationforVisual\nRecognition.InCVPR.\n[52] RuohanZhang,TianziZang,YanminZhu,ChunyangWang,KeWang,andJiadiYu.2023.DisentangledContrastive\nLearningforCross-DomainRecommendation.InInternationalConferenceonDatabaseSystemsforAdvancedApplications.\nSpringer.\n[53] YangZhang,PhilipDavid,HassanForoosh,andBoqingGong.2019.Acurriculumdomainadaptationapproachtothe\nsemanticsegmentationofurbanscenes.TPAMI42,8(2019),1823–1841.\n[54] YuyangZhao,ZhunZhong,ZhimingLuo,GimHeeLee,andNicuSebe.2022.Source-FreeOpenCompoundDomain\nAdaptationinSemanticSegmentation.TCSVT32,10(2022),7019–7032. https://doi.org/10.1109/TCSVT.2022.3179021\n[55] JikeZhong,Hong-YouChen,andWei-LunChao.2024.MakingBatchNormalizationGreatinFederatedDeepLearning.\narXiv:2303.06530[cs.LG] https://arxiv.org/abs/2303.06530\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.",
    "pdf_filename": "Backpropagation-Free_Multi-modal_On-Device_Model_Adaptation_via_Cloud-Device_Collaboration.pdf"
}