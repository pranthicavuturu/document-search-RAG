{
    "title": "Backpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration",
    "context": "prior specific permission and/or a fee. Request permissions from permissions@acm.org. Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0004-5411/2018/8-ART111 $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. arXiv:2406.01601v3  [cs.DC]  18 Nov 2024",
    "body": "111\nBackpropagation-Free Multi-modal On-Device Model\nAdaptation via Cloud-Device Collaboration\nWEI JIâˆ—, National University of Singapore, Singapore\nLI LIâˆ—, National University of Singapore, Singapore\nZHEQI LVâˆ—, Zhejiang University, China\nWENQIAO ZHANG, Zhejiang University, China\nMENGZE LI, Zhejiang University, China\nZHEN WAN, Fudan University, China\nWENQIANG LEI, Sichuan University, China\nROGER ZIMMERMANN, National University of Singapore, Singapore\nIn our increasingly interconnected world, where intelligent devices continually amass copious personalized\nmulti-modal data, a pressing need arises to deliver high-quality, personalized device-aware services. However,\nthis endeavor presents a multifaceted challenge to prevailing artificial intelligence (AI) systems primarily\nrooted in the cloud. As these systems grapple with shifting data distributions between the cloud and devices,\nthe traditional approach of fine-tuning-based adaptation (FTA) exists the following issues: the costly and\ntime-consuming data annotation required by FTA and the looming risk of model overfitting. To surmount these\nchallenges, we introduce a Universal On-Device Multi-modal Model Adaptation Framework, revolutionizing on-\ndevice model adaptation by striking a balance between efficiency and effectiveness. The framework features the\nFast Domain Adaptor (FDA) hosted in the cloud, providing tailored parameters for the Lightweight Multi-modal\nModel on devices. To enhance adaptability across multi-modal tasks, the AnchorFrame Distribution Reasoner\n(ADR) minimizes communication costs. Our contributions, encapsulated in the Cloud-Device Collaboration\nMulti-modal Parameter Generation (CDC-MMPG) framework, represent a pioneering solution for on-Device\nMulti-modal Model Adaptation (DMMA). Extensive experiments validate the efficiency and effectiveness of\nour method, particularly in video question answering and retrieval tasks, driving forward the integration of\nintelligent devices into our daily lives.\nCCS Concepts: â€¢ Information systems â†’Mobile information processing systems; Personalization; â€¢\nHuman-centered computing â†’Mobile computing.\nAdditional Key Words and Phrases: Cloud-device collaboration, model adaptation, multi-modal\nACM Reference Format:\nWei Ji, Li Li, Zheqi Lv, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, and Roger Zimmermann. 2018.\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration. J. ACM 37,\n4, Article 111 (August 2018), 17 pages. https://doi.org/XXXXXXX.XXXXXXX\nâˆ—These authors contributed equally to this research.\nAuthorsâ€™ addresses: Wei Ji, National University of Singapore, Singapore, jiwei@nus.edu.sg; Li Li, National University of\nSingapore, Singapore, lili02@u.nus.edu; Zheqi Lv, Zhejiang University, China, zheqilv@zju.edu.cn; Wenqiao Zhang, Zhejiang\nUniversity, China, wenqiaozhang@zju.edu.cn; Mengze Li, Zhejiang University, China, mengzeli@zju.edu.cn; Zhen Wan,\nFudan University, China, wz2311602492@gmail.com; Wenqiang Lei, Sichuan University, China, wenqianglei@gmail.com;\nRoger Zimmermann, National University of Singapore, Singapore, dcsrz@nus.edu.sg.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n0004-5411/2018/8-ART111 $15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\narXiv:2406.01601v3  [cs.DC]  18 Nov 2024\n\n111:2\nTrovato et al.\nDistribution \nShift\nCloud\nDevice ğ‘‘ğ‘‘(ğ‘–ğ‘–)\nDistribution \nShift\n(a) Cloud-Device\nDistribution Shift\n(b) Methods\nComparison\nConventional\nOurs\nDeploy\nDeploy\nParameters\nParameters\nMulti-Modal\nModel ğ‘€ğ‘€ğ‘‘ğ‘‘ğ‘–ğ‘–\nDevice ğ‘‘ğ‘‘(ğ‘—ğ‘—)\nMulti-Modal\nModel ğ‘€ğ‘€ğ‘‘ğ‘‘ğ‘—ğ‘—\nMulti-Modal\nModel ğ‘€ğ‘€\nMulti-Modal\nModel ğ‘€ğ‘€\nMulti-Modal\nModel ğ‘€ğ‘€\nFast Domain Adaptor ğ‘€ğ‘€ğ‘”ğ‘”\nFig. 1. (a) Multi-modal data on cloud and different devices exist in different distributions due to the personal-\nized preference of users. (b) Compared with conventional methods of deploying models on different devices,\nwe propose an FDA that can achieve a balance of efficiency and effectiveness.\n1\nINTRODUCTION\nIn todayâ€™s interconnected world, the proliferation of intelligent devices, ranging from ubiquitous\nsmartphones to the ever-expanding Internet of Things (IoT) ecosystem, has become an integral part\nof our daily lives. These devices serve as data collection powerhouses, continuously amassing vast\nrepositories of personalized multi-modal data, which can include a wide array of input modalities\nsuch as text, images and videos. The potential locked within this trove of multi-modal data arriving\ncontinuously is immense, promising to unlock high-quality and tailored device-aware services\nfor individual users. Despite promising, the personalized device service involves analyzing the\ndynamic nature of the multi-modal data that underscore usersâ€™ intentions. The prevailing artificial\nintelligence (AI) systems, primarily trained and deployed in cloud-based environments, face a\nprofound challenge in adapting to the dynamic device data when using a static cloud model for\nall individual users, mainly due to the distribution shift of the cloud and device data, as shown in\nFigure 1. In other words, high-quality personalized service requires AI systems to undergo continual\nrefinement and adaptation to accommodate the evolving landscape of personalized multi-modal\ndata.\nIntuitively, one of the straightforward adaptation strategies is to fine-tune the cloud model based\non the deviceâ€™s multi-modal data, which can kindly alleviate the cloud-device data distribution shift\nto model usersâ€™ intentions. Nevertheless, we contend that the fine-tuning-adaptation (FTA) paradigm\nmay not satisfactorily resolve device model personalization, which can be summarized as two key\naspects: (1) Undesirable Annotation. FTA often necessitates manually annotating data to guide\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:3\nmodel adaptation, which typically hinges on expensive and labor-intensive device data labeling.\nAdditionally, this retraining process can result in substantial delays, hindering the AI systemâ€™s\nability to deliver real-time, context-aware responsiveness. This situation is further exacerbated by\nthe inherent complexity of multi-modal data understanding, i.e., with data streams that encompass\ntextual, visual, and auditory information, the intricacies of labeling and retraining are more time-\nconsuming, resulting in a higher time delay and thus diminishing its practicality for users. (2)\nOverfitting Risk. In real-world applications, the majority of devices could be characterized\nby sparse and specialized multi-modal data where model fine-tuning may inadvertently lead\nto overfitting issues, even leading to device model performance degradation. In other words,\nFTA imposes significant demands for on-device data quantity, hampering its ability to generalize\neffectively across diverse device ecosystems. Based on the aforementioned insights, a meaningful\noptimization goal of device model personalization is to appropriately tap onto the personalized\nmulti-modal data, and thus strike the delicate balance between the effectiveness and efficiency of\npersonalized adaptation.\nTo address these multifaceted challenges and pave the way for intelligent device-driven AI\nsystems that harmonize adaptability and computational efficiency, we present a universal device\nmulti-modal model adaptation framework, a groundbreaking approach that redefines the landscape\nof on-device AI adaptation. This framework is designed to revolutionize the way AI systems harness\nmulti-modal data, simultaneously unlocking efficiency and effectiveness in the adaptation process.\nIn our approach, we introduced a Fast Domain Adaptor (FDA) hosted on the cloud, which takes\ndevice-captured images as input and produces customized parameters for the Lightweight Multi-\nmodal Model on the device. These customized parameters are tailored to different data distributions\npresent on the device. Moreover, the FDA adapts to various multi-modal tasks by requiring distinct\ndata inputs. However, certain multi-modal tasks, like Video QA, necessitate transmitting extensive\ndata such as multiple frames or the entire video to FDA, incurring substantial communication costs\nand bandwidth requirements. To mitigate this challenge and enhance FDAâ€™s adaptability across\ndifferent multi-modal tasks, we developed the AnchorFrame Distribution Reasoner (ADR). ADR\nstandardizes the input for the FDA across various multi-modal tasks. For instance, in the case of\nVideo QA, ADR selects the first frame of each video as the AnchorFrame. Using a combination of\nVariation AutoEncoder (VAE) and KeyFrame, ADR maps the AnchorFrame to a standard distribution\nthrough training. Once both FDA and ADR are trained, they are deployed on the cloud to deliver\npersonalized model parameter services for lightweight multi-modal models on the device. The\ndeviceâ€™s model can then upload the AnchorFrame to obtain personalized model parameters tailored\nto the data distribution of the current image or video input. An important highlight of our framework\nis the absence of backpropagation during the domain adaptation process for the device model. This\nunique approach enables our framework to achieve rapid domain adaptation of the device model,\nresulting in exceptionally low latency.\nSumming up, our contributions are summarized below:\n(1) We propose a Cloud-Device Collaboration Multi-Modal Parameter Generation (CDC-MMPG)\nframework to accomplish efficient on-Device Multi-modal Model Adaptation (DMMA). Its\ncore is Fast Domain Adaptor (FDA) which can generate multi-modal equipment model\nparameters based on the data distribution of multi-modal data. To the best of our knowledge,\nwe are the first ones to solve the DMMA. Our proposed CDC-MMPG framework is a universal\none that can be adaptive to other modalities.\n(2) We propose the AnchorFrame Distribution Reasoner (ADR) to further reduce the commu-\nnication cost of the CDC-MMPG. ADR successfully addresses the issue of high bandwidth\ndependence while maintaining nearly consistent performance.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:4\nTrovato et al.\n(3) Extensive experiments on video question answering and video retrieval tasks have verified\nthe efficiency and effectiveness of our proposed method.\n2\nRELATED WORKS\nCloud-Device Collaboration. In the rapidly evolving sphere of cloud-device collaboration, deep\nlearning has become a crucial component, strategically combining the features of cloud-based and\non-device machine learning. Although Federated Learning (FL), notably represented by FedAVG [5],\nhas traditionally been a significant player, its perceived simplicity has limited its applicability\nin various practical situations. This dynamic field has seen the rise of innovative methods and\ntechniques designed to optimize the interaction between cloud resources and mobile devices. For\ninstance, MPDA [41] skillfully utilizes cloud-based samples to enhance the performance of on-\ndevice models. Another interesting approach involves deploying multiple models with similar\nfunctionalities, coordinated by a Meta Controller for optimal task allocation, thus expanding\nthe possibilities of collaborative model adaptation [3, 49]. Moreover, DUET [23], inspired by the\nHyperNetwork concept [26, 38, 47, 50], simplifies the device model adaptation process, eliminating\nthe need for on-device training [55]. IDEAL [22] expands parameter generation-based models to\nthe domain of recommender systems, particularly DC-CDR [52]. This enhances the generalization\ncapabilities of device recommendation models, though it requires consideration of factors such\nas request frequency and communication revenue. However, the above works are tailored for\nrecommender system, which can not be directly migrated to the multi-modal domain field, mainly\ndue to that they ignore the inherent complexity of multi-modal data. In this paper, we present\nCDC-MMPG, tailored for multi-modal on-device model adaptation via cloud-device collaboration,\nwhich simultaneously obtained effectiveness and efficiency.\nDomain Adaptation. Domain Adaptation (DA) plays a crucial role in transferring a network\nthat was initially trained on a labeled source domain to effectively perform on a target domain.\nVarious techniques have been developed to align the feature distributions between the source and\ntarget domains, such as maximum mean discrepancy and correlation alignment. These methods\naim to reduce the distribution discrepancy and enable the network to generalize well across\ndifferent domains, improving its performance in the target domain. Lots of methods are proposed to\nfacilitate the development of the domain adaptation, such as multi-stage methods [4, 12, 14, 15, 53],\ngradual transfer strategies [2, 16, 31, 33], and so on. Recent advancements in domain adaptation\nembrace curriculum-based strategies inspired by curriculum learning to enhance the effectiveness\nof the adaptation process [13, 27, 29], Source-free domain adaptation (SFDA) has emerged as a\nresponse to privacy and copyright concerns, where access to the source domain data is restricted\nor unavailable [2, 4, 8, 10, 18, 20, 27, 29, 30, 35, 44, 53]. SFDA presents a more challenging variant\nknown as Test-Time Adaptation (TTA), which necessitates online adaptation during the inference\nphase. In addition, many researchers introduce the domain adaptation setting into different tasks\nto enhance model generalization [1, 6, 19, 24, 25, 28, 36, 37, 42, 43, 48, 51, 54]. In summary, domain\nadaptation (DA) methods focus on aligning feature distributions between the source and target\ndomains. Recent advancements have highlighted the importance of curriculum-based strategies to\nimprove the adaptation process. [2, 4, 27, 29, 46, 53]. In this paper, we propose a model generation\nframework to solve the personalized adaption problem.\n3\nMETHODOLOGY\nIn this section, we provide a detailed description of our proposed Cloud-Device Collaboration Multi-\nModal Parameter Generation (CDC-MMPG) framework, which includes Fast Domain Adaptor\n(FDA) and AnchorFrame Distribution Reasoner (ADR).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:5\n(c) Lightweight Multi-modal Model\nPatchifier\nText \nTokenizer\nQuestion: \nWhat is the\nman doing?\n(a) Fast Domain Adaptor\nShared Encoder\nMulti-modal Feature Extractor \nğ‘¡+\nğ‘¡-\nEntropy\nLoss\nAnswer:\nDancing\nPrediction:\nJumping\nPersonalized\nParameters\nConverter\nDevice â†’ Cloud\nCloud â†’ Device\nVideo Feature\nText Feature\nParameter\nGenerator\nFast Domain Adaptation\nParameter \nFilling\nStructure\nOnly\nStructure\n&\nParameter\nParameter\nFeature\nLegend\nConverter\nParameter\nGenerator\nAnchorFrame \nFeature\nKeyframe \nFeature\nSampled\nFeature\nText Embedding\nTable\nOn Device\nOn Cloud\nğ·%\nğœ‡2\nğœ2\n(b) AnchorFrame Distribution Reasoner\nSampling\nInference\nProcedure\n!!\n!!\n\"!\nSampling    \nReconstruction\nLoss\nTraining\nProcedure\nVideo&Text Feature\nVideo&Text Feature\nImage&Text \nFeature\nFig. 2. Illustration of the overall pipeline of our method, CDC-MMPG. (a) and (b) represent the Cloud model,\nwhich reconstructs the video features uploaded from the device and reasons out the personal parameters\nof the device model based on the reconstructed video features. (c) represents the lightweight multi-modal\ndevice-side model, which extracts the multi-modal features, and uploads the video features to the cloud\nmodel for the personal device-model parameter prediction. After being updated with the personal parameters,\nthe lightweight multi-modal device-side model will further analyze the multi-modal features and make the\nfinal prediction.\n3.1\nPreliminary\nProblem Formulation. For the on-Device Multi-modal Model Adaptation (DMMA) in the device-\ncloud collaboration system, we have access to a set of devices D = {ğ‘‘(ğ‘–)}Nğ‘‘\nğ‘–=1, each device with\nits personal i.i.d multi-modal history samples Sğ»(ğ‘–) and multi-modal real-time samples Sğ‘…(ğ‘–) in\ncurrent session, where Nğ‘‘represents the number of devices. The goal of on-Device Multi-modal\nModel Adaptation is to generalize a trained cloud model Mğ‘”(Â·; Î˜ğ‘”) learned from {Sğ»(ğ‘–) }Nğ‘‘\nğ‘—=1 to each\nspecific local device model Mğ‘‘(ğ‘–) (Â·; Î˜ğ‘‘(ğ‘–) ) conditioned on real-time samples Sğ‘…(ğ‘–) , where Î˜ğ‘”and\nÎ˜ğ‘‘(ğ‘–) respectively denote the learned parameters for the cloud model and the ğ‘–-th device model:\nCDC-MMPG\n|           {z           }\nDMMA Model\n: Mğ‘”({Sğ»(ğ‘–) }Nğ‘‘\nğ‘–=1; Î˜ğ‘”)\n|                   {z                   }\nCloud Model\nâ†’Mğ‘‘(ğ‘–) (Sğ‘…(ğ‘–) ; Î˜ğ‘‘(ğ‘–) )\n|                 {z                 }\nDevice Model\n.\n(1)\nFigure 2 illustrates the overview of our CDC-MMPG framework which consists of two modules\nto improve the generalization ability of the trained models on the device: (a) Fast Domain Adaptor\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:6\nTrovato et al.\n(FDA) aims to learn a global benchmark model based on the history samples of all distributions and\ngenerate the network parameters for the distribution-specific device model based on the real-time\ndevice samples (in Sec. 3.2); (b) AnchorFrame Distribution Reasoner (ADR) standardizes the input\nfor the FDA across various multi-model samples. (in Sec. 3.3).\nModel Pipeline. As shown in Algorithm 1, there are three steps in the pipeline of the CDC-\nMMPG model: (1) Training the cloud model Mğ‘”(.), including the FDA module and the ADR module,\nwith the history samples Sğ». (2) Uploading the real-time samples Sğ‘…(ğ‘–) from the device side to\nthe cloud side. Then, the cloud side model Mğ‘”(.) generates the personalized parameters for the\ndevice model Mğ‘‘(ğ‘–) (.). (3) With the model parameters passed from the cloud side, the device model\nMğ‘‘(ğ‘–) (.) is updated and makes the final prediction based on the input read-time samples.\nMulti-modal Feature Extraction. We extract the multi-modal representation needed for the\nsubsequent processes from the input examples. Specifically, given a real-time input video ğ‘‰ğ‘Ÿand a\ncorresponding language query ğ‘„ğ‘Ÿ, we employ the DeiT [32] as the visual feature extractor ğ‘“ğ‘£(.)\nand the BERT-base language model [7] as the text encoder ğ‘“ğ‘¡(.), to extract the features of the input\nsample.\nğ¹ğ‘£= ğ‘“ğ‘£(ğ‘‰ğ‘Ÿ),\n(2)\nğ¹ğ‘¡= ğ‘“ğ‘¡(ğ‘„ğ‘Ÿ),\n(3)\nwhere ğ¹ğ‘£= {ğ¹ğ‘–\nğ‘£}\nğ‘ğ‘“\nğ‘–=1 represents the features of all video frames and ğ‘ğ‘“is the video frame number.\nğ¹ğ‘¡is the language query feature.\nWe adopt the spatial-temporal positional embedding and modality type embedding following\n[34] to the extracted features ğ¹ğ‘£and ğ¹ğ‘¡and get the corresponding embedded features ğ¸ğ‘£= {ğ¸ğ‘–\nğ‘£}\nğ‘ğ‘“\nğ‘–=1\nand ğ¸ğ‘¡. Then, the cross-modal fusion module ğ‘”(.) consisted of ğ‘¡transformer layers is employed to\nfuse the visual features ğ¸ğ‘£and the language feature ğ¸ğ‘¡:\nğ¹ğ‘š= ğ‘”(ğ¸ğ‘£, ğ¸ğ‘¡),\n(4)\nwhere ğ¹ğ‘š= {ğ¹ğ‘–\nğ‘š}\nğ‘ğ‘“\nğ‘–=1 is the generated multi-modal features after the feature fusion.\n3.2\nFast Domain Adaptor\nWhen adapting the cloud model to the personalized device real-time samples of specific distribution,\none straightforward strategy is fine-tuning the cloud model on the device. However, it may be\nunreachable due to a lack of sufficient training samples and corresponding annotations on the\ndevice. To tackle the aforementioned challenge, we propose a novel solution called the Fast Domain\nAdaptor (FDA), which is implemented as a cloud-based service. The FDA is designed to process\ndevice-captured images as input and generate customized parameters specifically tailored to the\nLightweight Multi-modal Model deployed on the device. These customized parameters are carefully\ncrafted to adapt to the unique data distributions encountered on each individual device.\nSpecifically, we further sample ğ·(ğ·> 1) frames from ğ¹ğ‘šrandomly, upload the sampled multi-\nmodal features from the device side to the cloud side, and average the included ğ·frames of\nmulti-modal features into the global representation ğ¹ğ‘”. Then, we project the feature ğ¹ğ‘”before it is\nfurther analyzed:\nğ¸ğ‘”= ğ‘“ğ‘(ğ¹ğ‘”),\n(5)\nwhere ğ‘“ğ‘(.) represents the multi-layer perceptron consisting of two linear layers and a normalization\nlayer. The output feature ğ¸ğ‘”is treated as the real-time sample embedding for hyper-network [9, 11]\ninput:\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:7\nÎ˜ğ‘‘= ğ‘“â„(ğ¸ğ‘”),\n(6)\nwhere ğ‘“â„(.) is the multi-layer perceptron. The generated Î˜ğ‘‘is the parameters for a single linear\nlayer including both the linear weights and bias, which will be passed to the device model.\nThe generated model parameters Î˜ğ‘‘are passed from the cloud side to the device side. The device\nmodel Mğ‘‘is updated with the model parameters Î˜ğ‘‘and makes the prediction ğ‘ƒğ‘›for the input\nmulti-model real-time sample:\nğ‘ƒğ‘›= Mğ‘‘(ğ¹ğ‘š; Î˜ğ‘‘),\n(7)\nwhere ğ¹ğ‘šis the extracted multi-modal feature.\n3.3\nAnchorFrame Distribution Reasoner\nThe FDA is designed to adapt to diverse multi-modal tasks by utilizing specific data inputs.\nHowever, certain tasks, such as Video QA, require transmitting significant amounts of data, such as\nmultiple frames or the entire video, to the FDA. This results in considerable communication costs\nand bandwidth requirements. To address this challenge and improve the adaptability of the FDA\nacross various multi-modal tasks, we introduce the AnchorFrame Distribution Reasoner (ADR).\nADR plays a crucial role in standardizing the input format for the FDA, ensuring compatibility\nacross different multi-modal tasks.\nSpecifically, during the inference process, we randomly select one frame from the whole video on\nthe device side and upload the multi-modal feature ğ¹ğ‘–\nğ‘šcorresponding to the selected ğ‘–-th frame to\nthe cloud side. Then, the Variational Auto-Encoder (VAE) including an encoder Î¨ğ¸and a decoder\nÎ¨ğ·are applied to further analyze. Firstly, we obtain the latent distribution by analyzing the uploaded\nfeature ğ¹ğ‘–\nğ‘šwith the VAE encoder Î¨ğ¸:\nÎ¨ğ¸(ğ¹ğ‘–\nğ‘š) âˆ¼N (ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™, ğœğ‘Ÿğ‘’ğ‘ğ‘™),\n(8)\nwhere N (ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™, ğœğ‘Ÿğ‘’ğ‘ğ‘™) represents the Gaussian distribution with mean ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™and standard deviation\nğœğ‘Ÿğ‘’ğ‘ğ‘™. Then we sample a new representation ğ‘…ğ‘–\nğ‘šfrom the obtained specific distribution:\nğ‘…ğ‘–\nğ‘šâˆ¼N (ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™, ğœğ‘Ÿğ‘’ğ‘ğ‘™).\n(9)\nThe ğ‘…ğ‘–\nğ‘šis then reasoned on by the VAE decoder Î¨ğ·:\nğ¹ğ‘–\nğ‘š\nâ€² = Î¨ğ·(ğ‘…ğ‘–\nğ‘š).\n(10)\nThe ğ¹ğ‘–\nğ‘š\nâ€² and ğ¹ğ‘–\nğ‘šare utilized to adaptively generate the new feature ğ¹â€²\nğ‘”through the adaptive\ngenerator Î¦ğ‘(.). The generated new features will be forwarded to the hyper-network with the goal\nof leading it to generalize to the target domain. Thus, we can update the Eq. 5 to:\nğ¹â€²\nğ‘”= Î¦ğ‘(ğ¹ğ‘–\nğ‘š\nâ€², ğ¹ğ‘–\nğ‘š).\n(11)\nThe feature ğ¹â€²\nğ‘”is applied to replace the global representation of the input sample ğ¹ğ‘”defined in\nSection 3.2 for further prediction of the personalized model parameters.\nThe framework uses limited communication bandwidth: one-frame multi-modal feature upload\nand hyper-network parameters (a few linear layers) download, promising the generalization ability\nand extremely low communication delay of the device model.\nDuring the training process, all history samples are directly saved on the cloud side and partici-\npated in the training process, following the setting of previous methods [45]. Given the sampled\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:8\nTrovato et al.\nAlgorithm 1: CDC-MMPG Framework\n1 Initialize the multi-modal feature extraction model: Mğ‘£ğ‘™; the hyper-network model in FDA:\nâ„(Â·); the encoder and the decoder in ADR: Î¨ğ¸, Î¨ğ·; the adaptive generator: Î¦ğ‘.\n2 Phase I: Input: Historical data Sğ».\n3\n1) Train the ADR on the cloud using Sğ»and the training objectives in Eq. 18.\n4 Output the trained models: Mâ€²\nğ‘£ğ‘™,â„â€²(Â·), Î¨â€²\nğ¸, Î¨â€²\nğ·, Î¦â€²\nğ‘.\n5 Phase II: Input: Real-time data Sğ‘….\n6\n1) Extract multi-modal features and process the one-frame feature to the cloud\nğ¹ğ‘–\nğ‘ğ‘™ğ‘œğ‘¢ğ‘‘, ğ¹ğ‘”â†Mâ€²\nğ‘£ğ‘™(Sğ‘…).\n7\n2) Obtain latent distribution of ğ¹ğ‘ğ‘™ğ‘œğ‘¢ğ‘‘: N (ğœ‡, ğœ), where ğœ, ğœ‡â†Î¨â€²\nğ¸(ğ¹ğ‘–\nğ‘ğ‘™ğ‘œğ‘¢ğ‘‘).\n8\n3) Randomly sample from this distribution ğ¹ğ‘Ÿğ‘ğ‘›ğ‘‘â†N (ğœ‡, ğœ).\n9\n4) Decode the randomly sampled distribution ğ¹â€²\nğ‘Ÿğ‘ğ‘›ğ‘‘â†Î¨â€²\nğ·(ğ¹ğ‘Ÿğ‘ğ‘›ğ‘‘).\n10\n5) Get adaptive layer embedding ğ¸ğ‘šâ†Î¦ğ‘(, ğ¹â€²\nğ‘Ÿğ‘ğ‘›ğ‘‘, ğ¹ğ‘–\nğ‘ğ‘™ğ‘œğ‘¢ğ‘‘).\n11\n6) Generate adaptive parameters using the layer embedding Î˜ğ‘â†â„â€²(ğ¸ğ‘š).\n12 Output the parameters: Î˜ğ‘.\n13 Phase III: Input: Real-time data Sğ‘….\n14\n1) Download the adaptive parameters to the device Î˜ğ‘‘\nğ‘â†Î˜ğ‘.\n15\n2) Share feature backbone between the cloud and device Mğ‘‘â†Mğ‘£ğ‘™.\n16\n3) Make the prediction with Î˜ğ‘‘\nğ‘: ğ‘ƒğ‘›â†Mğ‘‘(Sğ‘…; Î˜ğ‘‘\nğ‘)\n17 Output the ğ‘ƒğ‘›.\nmulti-modal features ğ¹ğ‘”and the randomly selected feature ğ¹ğ‘–\nğ‘š, the variational auto-encoder is\napplied to further analyze the same as the inference process. Firstly, the encoder Î¨ğ¸project ğ¹ğ‘šinto\nthe latent space and generate a new feature ğ¹â„:\nğ¹â„= Î¨ğ¸(ğ¹ğ‘”).\n(12)\nThen, to reconstruct the input feature ğ¹ğ‘”, the decoder Î¨ğ‘‘decodes the feature ğ¹â„to the feature ğ¹â€²\nğ‘”:\nğ¹â€²\nğ‘”= Î¨ğ·(ğ¹â„).\n(13)\nIn order to train the variational auto-encoder, we use the Kullback-Leibler (KL) loss ğ¿ğ‘˜ğ‘™and\nreconstruction loss ğ¿ğ‘…ğ‘’ğ‘as follows:\nğ¿ğ‘˜ğ‘™= Dğ‘˜ğ‘™(N (ğœ‡â€², ğœâ€²)||N (0, I)),\n(14)\nğ¿Rec = MSE(Fg, Fâ€²\ng),\n(15)\nwhere N (0, I) represents the normal distribution, Dğ‘˜ğ‘™(.) represents the Kullback-Leibler divergence\ncalculation and MSE(Â·) denotes the mean square loss.\nInspired by [21], in order to incorporate the various distribution shifts into the feature of the\nsingle video frame, the adaptive generator generates new features ğ¹ğ‘by re-normalizing the ğ¹â€²\nğ‘”to\nhave the same channel-wise mean and standard deviation as the ğ¹ğ‘–\nğ‘š. This process can be formulated\nas follows:\nÎ¦ğ‘(ğ¹â€²\nğ‘”, ğ¹ğ‘–\nğ‘š) = ğ›¿(ğ¹â€²\nğ‘”)\n\u0012ğ¹ğ‘–\nğ‘šâˆ’ğ›¿(ğ¹ğ‘–\nğ‘š)\nğ›¾(ğ¹ğ‘–ğ‘š)\n\u0013\n+ ğ›¾(ğ¹â€²\nğ‘”),\n(16)\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:9\nwhere ğ›¿(Â·) and ğ›¾(Â·) denotes channel-wise mean and standard deviation operations, respectively.\nFollowing previous works on variational auto-encoder generalization, we additionally compute\nthe loss values between the generated new features and ğ¹ğ‘”, ğ¹ğ‘–\nğ‘š, respectively, to train the whole\nmodule:\nğ¿AG = ğœ†MSE(Fa, Fg) + MSE(Fa, Fi\nm),\n(17)\nwhere ğœ†denotes a hyper-parameter, and ğ¹a denotes the feature generated by the adaptive generator.\nThus, the overall training loss ğ¿for the whole framework is:\nğ¿= ğ¿AG + ğ¿Rec + ğ¿KL.\n(18)\nTable 1. Results of our proposed method in Open-ended Video QA task on MSRVTT-QA and MSVD-QA. We\nadopt accuracy as the evaluation metric. The number of learnable parameters on the device, the number of\nlearnable parameters on the cloud, and the time delay are additionally measured to show the efficiency of our\nproposed method. â€œF-linear\" denotes only fine-tuning the classifiers after the multi-modal feature extractor.\nâ€œF-hyper\" denotes only fine-tuning the classifiers and a simple hyper-network without an adaptive generator.\nMethods\nVSRVTT-QA\nMVSD-QA\nAcc. D-Param. C-Param. Time Delay Acc. D-Param. C-Param. Time Delay\nF-linear\n13.6\n11.6M\n/\n60000ms\n17.3\n11.5M\n/\n60000ms\nFine-tuning 36.7\n11.6M\n/\n60000ms\n34.3\n11.5M\n/\n60000ms\nF-hyper\n11.1\n11.6M\n28.1M\n5.55ms\n6.34\n11.5M\n18.7M\n3.71ms\nOurs\n37.1\n11.6M\n55.9M\n5.55ms\n35.4\n11.5M\n46.5M\n3.71ms\nTable 2. Results of our proposed method in Open-ended Video QA task on TGIF. We adopt accuracy as the\nevaluation metric. The number of learnable parameters on the device, the number of learnable parameters\non the cloud, and the time delay are additionally measured to show the efficiency of our proposed method.\nâ€œF-linear\" denotes only fine-tuning the classifiers after the multi-modal feature extractor. â€œF-hyper\" denotes\nonly fine-tuning the classifiers and a simple hyper-network without an adaptive generator.\nMethods\nTGIF\nAccuracy\nD-Param.\nC-Param.\nTime Delay\nF-linear\n27.3\n11.6M\n/\n60000ms\nFine-tuning\n54.7\n11.6M\n/\n60000ms\nF-hyper\n19.6\n11.6M\n18.7M\n5.70ms\nOurs\n55.2\n11.6M\n46.5M\n5.70ms\n4\nEXPERIMENT\nIn this section, we evaluate our proposed method on three popular multi-modal tasks and four\nrelevant datasets. Additionally, we perform ablation studies to investigate the impact of each module\nin our method, the influence of the number of frames sampled from the whole video, and the time\ndelay between the cloud and the device with various internet conditions.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:10\nTrovato et al.\n4.1\nDatasets\nWe conducted our experiments on three multi-modal tasks: multiple-choice Video QA, open-ended\nVideo QA, and video-text retrieval. For the open-ended Video QA task, we use the MSRVTT-QA\ndataset [39], MSVD-QA dataset [39] and the TGIF dataset [17]. For the multiple-choice Video QA\ntask, we use the MSRVTT-QA dataset [39]. For the video-text retrieval task, we experiment on the\nMSRVTT dataset [40].\n(1) MSRVTT. The MSRVTT is a large-scale dataset for the open-domain video captioning, which\nconsists of 10K video clips and each video clip is annotated with 20 English sentences. The\nstandard splits use 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\n(2) MSRVTT-QA. The MSRVTT-QA dataset contains 10K video clips and 243k question answer\npairs. Specifically, we used MSRVTT-QA dataset [39] for open-ended VQA and multiple-\nchoice VQA. Following [39], We use the traditional data split which is 65% for the training\nset, 5% for the validation set and 30% for test set.\n(3) MSVD-QA. The MSVD-QA dataset has a total number of 1,970 video clips and 50,505 question\nanswer pairs. Following [39], we split the dataset based on videos that training set takes 61%,\nvalidation set takes 13%, and test set takes 26% of the total number of videos.\n(4) TGIF. The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences\ndescribing visual content of the animated GIFs. One sentence is provided for every animated\nGIF for the training and validation splits, and three sentences per GIF for the test split.\nSpecifically, there are 80K training samples, 11K validation samples and 11k test samples.\n4.2\nEvaluation Metrics\nFollowing previous works, we adopt accuracy for the open-ended Video QA task, and VR@K,\nTR@K for video-text retrieval task. Specifically, VR@K denotes the recall of video to text retrieval\nand TR@K denotes the recall of text to video retrieval. For both of these two tasks, K is set to\n1,2,5 respectively. Additionally, we calculate the number of learnable parameters in each model.\nMeanwhile, for the practical scenario of cloud-device collaboration, we also measure the time delay\nfor the cloud-device communication process.\n4.3\nTasks and Implementation Details\nWe evaluate our method on three tasks:\n(1) Video-text Retrieval: We evaluate two sub-tasks including video-to-text retrieval and text-\nto-video retrieval. The text-to-video retrieval task requires retrieving the target video using\nTable 3. Results of our proposed method in Text-video Retrieval task. The number of learnable parameters on\nthe device, the number of learnable parameters on the cloud, and the time delay are additionally measured\nto show the efficiency of our proposed method.\nMethods\nMSRVTT\nVR@1\nVR@5\nVR@10\nTR@1\nTR@5\nTR@10\nF-linear\n2.0\n7.7\n14.1\n3.1\n10.1\n16.9\nFine-tuning\n4.8\n17.6\n27.9\n6.2\n20.7\n31.8\nF-hyper\n2.6\n8.6\n14.7\n2.7\n11.4\n17.6\nOurs\n6.6\n21.3\n33.8\n5.6\n21.2\n33.3\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:11\nTable 4. Ablation studies of different numbers of sampled frames. We make the evaluation on three datasets\nin Open-ended Video-question Answering task. â€œTime/Epoch\" denotes the time of fine-tuning for each epoch.\nFrames\nMSRVTT-QA\nMSVD-QA\nTGIF\nAccuracy\nTime/Epoch\nAccuracy\nTime/Epoch\nAccuracy\nTime/Epoch\n2\n36.1\n740s\n34.1\n204s\n54.3\n245s\n3\n36.7\n897s\n34.3\n228s\n54.7\n310s\n4\n37.0\n1309s\n34.3\n318s\n54.7\n448s\n5\n37.1\n2092s\n34.0\n377s\n55.2\n702s\nlanguage queries. The video-to-text retrieval task requires retrieving the target text using\nvideos.\n(2) Open-ended Video-question Answering: It requires answering questions according to\nthe context of the video. The answers are originally in free-form natural language, but it is a\ncommon practice to convert the task to a classification task by representing the answer with\na class label.\n(3) Multiple-choice Video-question Answering: Given a video with a query and 5 candidate\ncaptions, the task is to find the one that fits the query out of 5 possible candidates. The correct\nanswer is the ground-truth (GT) caption, and four other negatives are chosen from other\ncaptions that have different activity-phrase labels from the correct answer.\nImplementation Details. We employ the All-in-One-Ti [34] as the baseline model, which\nincludes the DeiT [32] as its visual backbone and BERT-base [7] as its semantic encoder. Specifically,\nwe only use the embedding layers of BERT for text embedding. The All-in-one-Ti is adopted as\nthe multi-modal encoder both for the device model and the cloud model. The value of ğ·is set\nto 3. During the training process of ADR, we employ the adamw optimizer with a polynomial\ndecay scheduler. The learning rate is set to 2e-5 and the training epoch is set to 10. During the\ntraining process of the rest of the modules, we froze the parameters in the adaptive generator,\nand the same adamw optimizer is used with the 1e-4 learning rate used. The total training epoch\nfor the rest modules is set to 40. For the hyper-parameters and other settings during the training\nprocess, ğœ†is set to 0.1, and the hidden layer of the hyper-network is set to 96 (for Open-ended VQA\nand Multiple-choice VQA) and 256 (for Video-text Retrieval). During the inference process, all the\nparameters on the device model and the cloud model are frozen except the last few dynamic linear\nlayers of the device model. The cloud model and the device model share the same parameters of\nthe multi-modal encoder. Then the cloud generates dynamic parameters using hyper-network for\nthe device model for better generalization.\n4.4\nPerformance Comparison\nIn this section, we introduce how we design the baseline method to showcase the efficiency of our\nproposed method, and we present the experimental results of our method on various tasks and\ndatasets.\n4.4.1\nBaseline Methods.\nFor we are the first to explore this field, we design the other three baseline methods to show\nthe superiority of our proposed method. As shown in Tab. 1 and Tab. 2, we adopt accuracy as the\nevaluation metric. The number of learnable parameters on the device, the number of learnable\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:12\nTrovato et al.\nparameters on the cloud, and the time delay are additionally measured to show the efficiency of our\nproposed method. â€œF-linear\" denotes only fine-tuning the classifiers after the multi-modal feature\nextractor. â€œF-hyper\" denotes only fine-tuning the classifiers and a simple hyper-network without\nan adaptive generator. Besides our proposed method, we also design the fine-tuning approach,\nfine-tuning only the linear classifiers approach (F-linear), and fine-tuning both the linear classifiers\nand solely hyper-network approach. For the fine-tuning approach, we simply add layers of MLPs\nafter the multi-modal feature extractor and do fine-tuning on the whole model. For fine-tuning\nonly the linear classifiers approach, we froze the parameters in the multi-modal feature extractor\nand fine-tuned the layers of MLPs after the feature extractor. For the fine-tuning of both the linear\nclassifiers and the solely hyper-network approach, we employ a simple hyper-network without\nthe proposed adaptive generator and do fine-tuning for both linear classifiers and the simple\nhyper-network.\nOpen-ended Video-question Answering. For Open-ended Video-question Answering, the\nresponses are typically expressed in unrestricted natural language. However, a prevalent approach\ninvolves transforming this task into a classification problem by encoding the answers as class labels.\nTo achieve this, we incorporate layers of MLPs with a hidden layer dimension of 96 following the\nextraction of multi-modal features. The dimension of the MLPâ€™s output layer varies according to\nthe specific label size associated with the datasets, for instance, 1501 labels for the MSRVTT-QA\ndataset.\nMultiple-choice Video-question Answering. For Multiple-choice Video-question Answering,\nwhere both the questions and candidate answers are presented as sentences, our approach involves\nconcatenating the question and the answer candidates. To distinguish between them, we employ\nthe special token [SEP]. Subsequently, we determine the prediction by selecting the candidate with\nthe highest output logit, signifying its likelihood of being the correct answer.\nVideo-text Retrieval. For Video-text Retrieval, the retrieval process encompasses two directions:\ntext-to-video retrieval and video-to-text retrieval. In each direction, the respective modality is first\nextracted, and subsequently, a comparative analysis is conducted. The predictions are ultimately\ngenerated through the application of MLPs.\n4.4.2\nMain Results.\nIn this section, we present the experimental results of open-ended Video QA in Tab. 1 and Tab. 2,\nmultiple-choice Video QA in Tab. 5, and Video-text Retrieval in Tab. 3 on four popular datasets.\nEffectiveness. Tab. 1,Tab. 2, Tab. 5, and Tab. 3 provide a comprehensive overview of the superi-\nority of our method compared to other baseline approaches across various datasets and evaluation\ncriteria. Notably, our method consistently outperforms alternative baseline models in nearly all\naspects across these datasets. For example, consider the task of Open-ended Video-question An-\nswering on the MSRVTT-QA dataset. Our proposed method excels in terms of both accuracy and\ntime efficiency when compared to all other baseline models. The conventional fine-tuning method\ndemands a significant amount of time to complete the fine-tuning process. As demonstrated in\nTab. 4, when selecting three frames from the entire video, the time required for each fine-tuning\nepoch on the MSRVTT dataset is approximately 897 seconds. In contrast, our method achieves\nnearly real-time communication, effectively reducing the time delay between the cloud and the\ndevice to as little as 5.55ms, contingent on internet conditions. Furthermore, our proposed method\neven surpasses the fine-tuning approach in terms of performance, indicating its superior fast\ngeneralization capabilities on personalized samples. Tab. 1 and Tab. 2, which denote the results on\nthe MSVD-QA and TGIF datasets, it is evident that our method significantly outperforms other\nbaseline models, thus affirming the effectiveness of our proposed approach.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:13\nTable 5. Results of our proposed CDC-MMPG framework in the multiple-choice Video QA task. The number\nof learnable parameters on the device, the number of learnable parameters on the cloud, and the time delay\nare measured to show the efficiency of our proposed method.\nMethods\nMSRVTT-QA\nTime Delay\nAcc.\nD-Param.\nC-Param.\nF-linear\n3.58\n11.4M\n/\nâ‰¥60000ms\nFine-tuning\n75.6\n11.4M\n/\nâ‰¥60000ms\nF-hyper\n46.0\n11.4M\n37.2K\nâ‰¥5.70ms\nOurs\n77.0\n11.4M\n20.3M\nâ‰¥5.70ms\nTable 6. Time delay of our framework in different circumstances (various internet speeds) on different datasets.\nOur proposed method largely reduces the time delay between the cloud and the device, realizing almost\nreal-time communication. â€œ â†‘â€ denotes the upload process from the device to cloud. â€œ â†“â€ denotes the\nparameters downloaded from the cloud to device.\nDatasets\nSize\n4G: 5MB/s\n4G: 15MB/s\n5G: 50MB/s\n5G: 100MB/s\nMSRVTT\nâ†‘:0.75KB\nâ†“:568.5KB\nâ†‘:0.15ms\nâ†“:111ms\nâ†‘:0.05ms\nâ†“:37.0ms\nâ†‘:0.01ms\nâ†“:11.1ms\nâ†‘:0.007ms\nâ†“:5.55ms\nMSVD\nâ†‘:0.75KB\nâ†“:379.4KB\nâ†‘:0.15ms\nâ†“:74ms\nâ†‘:0.05ms\nâ†“:24.7ms\nâ†‘:0.01ms\nâ†“:7.41ms\nâ†‘:0.007ms\nâ†“:3.71ms\nTGIF\nâ†‘:0.75KB\nâ†“:583.8KB\nâ†‘:0.15ms\nâ†“:114ms\nâ†‘:0.05ms\nâ†“:38.0ms\nâ†‘:0.01ms\nâ†“:11.4ms\nâ†‘:0.007ms\nâ†“:5.70ms\nExtensibility. Our proposed approach exhibits the capacity to enhance accuracy while concur-\nrently reducing time delays across all datasets associated with the three specific tasks. As delineated\nin Tab. 3, our methodology yields a significant performance improvement, particularly in the\ndomain of text-video retrieval. Our assessment encompasses a two-directional approach, addressing\nboth text-to-video retrieval and video-to-text retrieval. Remarkably, our method surpasses the\nmajority of baseline methods, not only in terms of accuracy but also with respect to time delays. In\nthe context of multiple-choice Video QA, our approach consistently asserts its superiority across\nall evaluation metrics. This is evident from the data presented in Tab. 5.\n4.5\nAblation Studies\nNumber of sampled frames. In Tab. 4, we investigate the influence of the number of sampled\nframes ğ·from the entire video on the final results. Simultaneously, we measure the time of fine-\ntuning for each epoch. To be specific, we employ the standard fine-tuning approach to evaluate\nthis factor across three distinct datasets, all in the Open-ended Video-question Answering task.\nThe batch size for all the datasets is set to 256, and the hardware is a single A100 GPU. It becomes\nevident that as the number of sampled frames increases, the time of fine-tuning for each epoch\nexperiences a rapid escalation, which is in line with intuition. For instance, when only two frames\nare sampled, the fine-tuning process demands approximately 740 seconds on MSRVTT-QA dataset.\nHowever, if five frames are sampled from each video, the time requirement extends to around 2092\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:14\nTrovato et al.\nseconds, which is almost three times longer than the former scenario. Interestingly, the modelâ€™s\nperformance does not exhibit a consistent upward trend with the increasing number of sampled\nframes. Besides, the modelâ€™s performance reaches its peak when the number of sampled frames is\nset at three or four. To make a trade-off between performance and training cost, we set the number\nof sampled frames as three in our model.\nDifferent modules in our proposed CDC-MMPG framework. In accordance with the results\nin Tab. 1 and Tab. 2, we have devised three additional baseline methods to assess the efficacy of our\ncomprehensive framework. To be more specific, our assessment involves a comparative analysis\nof the performance outcomes between the fine-tuning model and our proposed model, serving\nto emphasize the latterâ€™s superiority. Furthermore, we have undertaken an evaluation of the \"F-\nlinear\" model to facilitate an ablation study of the classifiers subsequent to the multi-modal feature\nextractor. In addition, the \"F-hyper\" model has been examined in order to conduct an ablation study\nfocused on the hyper-network and the adaptive generator. It is noteworthy that the omission of our\nproposed adaptive generator renders a simple hyper-network prone to encountering sub-optimal\nperformance, particularly evident in certain datasets, including MSRVTT-QA, MSVD-QA, and\nTGIF. This diminished performance is primarily attributed to the issue of over-fitting, leading\nto the emergence of spurious correlations between visual cues and predictions. Our adaptive\ngenerator, as proposed, substantially improves the performance of the hyper-network through the\nimplementation of anchor-frame distribution reasoning.\nTime delay between cloud and device in various internet conditions. Tab. 6 illustrates the\ntime delay associated with data transmission between cloud services and devices under various\ninternet connectivity conditions. Specifically, we have conducted this experiment in four distinct\nscenarios, each reflecting a different internet condition: 4G (5MB/s), 4G (15MB/s), 5G (50MB/s), and\n5G (100MB/s). These scenarios correspond to real-world situations involving older mobile devices\nwith limited 4G connectivity, older mobile devices with optimal 4G connectivity, modern mobile\ndevices with 5G connectivity, and modern mobile devices with advanced 5G internet connectivity.\nOur proposed method demonstrates remarkable improvements in terms of time delay. For example,\nduring the inference process, a mere 0.75KB of features is required to be uploaded to the cloud, a\ntask that consumes only about 0.007ms in a 5G (100MB/s) environment. Even in the presence of\nconstrained 4G (5MB/s) internet, the time required for uploading remains negligible. The duration of\nparameter downloading, however, fluctuates according to the dataset in use, largely due to variations\nin parameter size. For instance, the parameter size is 583.8KB for the TGIF dataset, the largest among\nthe datasets, whereas the parameter size for the MSVD-QA dataset, the smallest, amounts to 379.4KB.\nNotably, even when dealing with the demanding TGIF dataset, our proposed approach demonstrates\nthe capability to achieve real-time predictions. For instance, in a 5G (100MB/s) environment, the\ntime required for parameter downloading is merely 5.70ms. Even in scenarios featuring limited 4G\n(5MB/s) connectivity, the time delay remains nearly imperceptible in practical terms.\n5\nCONCLUSION\nIn this paper, we focus on the challenge of enabling efficient and effective adaptation of AI systems\nto personalized multi-modal data generated by intelligent devices. The shifting data distribution\nbetween the cloud and devices necessitates novel approaches to ensure high-quality personal-\nized services. We introduced a universal on-device Multi-modal Model Adaptation Framework,\nfeaturing the Fast Domain Adaptor (FDA) and the AnchorFrame Distribution Reasoner (ADR).\nFDA, hosted in the cloud, tailors model parameters for on-device Lightweight Multi-modal Models,\noptimizing adaptability across diverse data distributions. ADR further standardizes input, reducing\ncommunication costs. Our contributions, consolidated within the Cloud-Device Collaboration\nMulti-modal Parameter Generation (CDC-MMPG) framework, constitute a pioneering solution for\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:15\non-Device Multi-modal Model Adaptation (DMMA), demonstrated through extensive experiments.\nLooking forward, promising directions include expanding the framework to accommodate various\nmodalities, refining personalized data handling techniques, and further reducing communication\ncosts in multi-modal tasks.\nREFERENCES\n[1] Sabbir Ahmed, Abdullah Al Arafat, Mamshad Nayeem Rizve, Rahim Hossain, Zhishan Guo, and Adnan Siraj Rakin.\n2023. SSDA: Secure Source-Free Domain Adaptation. In ICCV.\n[2] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang.\n2019. Progressive feature alignment for unsupervised domain adaptation. In CVPR.\n[3] Hong-You Chen, Jike Zhong, Mingda Zhang, Xuhui Jia, Hang Qi, Boqing Gong, Wei-Lun Chao, and Li Zhang. 2023.\nFederated Learning of Shareable Bases for Personalization-Friendly Image Classification. arXiv:2304.07882 [cs.CV]\nhttps://arxiv.org/abs/2304.07882\n[4] Pengfei Chen, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. 2021. Unsupervised curriculum domain\nadaptation for no-reference video quality assessment. In ICCV.\n[5] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. 2022. Fedavg with fine tuning: Local updates\nlead to representation learning. NIPS (2022).\n[6] Victor G. Turrisi da Costa, Giacomo Zara, Paolo Rota, Thiago Oliveira-Santos, Nicu Sebe, Vittorio Murino, and Elisa\nRicci. 2022. Dual-Head Contrastive Domain Adaptation for Video Action Recognition. In WACV.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In NAACL.\n[8] Ning Ding, Yixing Xu, Yehui Tang, Chao Xu, Yunhe Wang, and Dacheng Tao. 2022. Source-Free Domain Adaptation\nvia Distribution Estimation. In CVPR.\n[9] Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. 2022. HyperInverter: Improving StyleGAN Inversion\nvia Hypernetwork. In CVPR.\n[10] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. StyleGAN-NADA:\nCLIP-Guided Domain Adaptation of Image Generators. ACM Trans. Graph. (2022).\n[11] David Ha, Andrew Dai, and Quoc V. Le. 2017. HyperNetworks. In ICLR.\n[12] Wei Ji, Li Li, Hao Fei, Xiangyan Liu, Xun Yang, Juncheng Li, and Roger Zimmermann. 2024. Towards Complex-query\nReferring Image Segmentation: A Novel Benchmark. ACM Trans. Multimedia Comput. Commun. Appl. (Nov. 2024).\nhttps://doi.org/10.1145/3701733\n[13] Li Li, Wei Ji, Yiming Wu, Mengze Li, You Qin, Lina Wei, and Roger Zimmermann. 2024. Panoptic Scene Graph Generation\nwith Semantics-Prototype Learning. AAAI 38, 4 (Mar. 2024), 3145â€“3153. https://doi.org/10.1609/aaai.v38i4.28098\n[14] Li Li, You Qin, Wei Ji, Yuxiao Zhou, and Roger Zimmermann. 2024. Domain-Wise Invariant Learning for Panoptic\nScene Graph Generation. In ICASSP. 3165â€“3169. https://doi.org/10.1109/ICASSP48485.2024.10447193\n[15] Li Li, Chenwei Wang, You Qin, Wei Ji, and Renjie Liang. 2023. Biased-Predicate Annotation Identification via Unbiased\nVisual Predicate Representation. In ACM MM (MM â€™23). 4410â€“4420. https://doi.org/10.1145/3581783.3611847\n[16] Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, and Masashi Sugiyama. 2024. Vision-Language Model\nFine-Tuning via Simple Parameter-Efficient Modification. arXiv:2409.16718 [cs.CV] https://arxiv.org/abs/2409.16718\n[17] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. 2016. TGIF:\nA New Dataset and Benchmark on Animated GIF Description. In CVPR.\n[18] Mattia Litrico, Alessio Del Bue, and Pietro Morerio. 2023. Guiding Pseudo-Labels With Uncertainty Estimation for\nSource-Free Unsupervised Domain Adaptation. In CVPR.\n[19] Nannan Lu, Hanhan Xiao, Zhanguo Ma, Tong Yan, and Min Han. 2022. Domain Adaptation With Self-Supervised\nLearning and Feature Clustering for Intelligent Fault Diagnosis. IEEE Transactions on Neural Networks and Learning\nSystems (2022), 1â€“14. https://doi.org/10.1109/TNNLS.2022.3219896\n[20] Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. 2020. Adversarial Style Mining for One-Shot Unsupervised\nDomain Adaptation. In NIPS.\n[21] Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. 2020. Adversarial Style Mining for One-Shot Unsupervised\nDomain Adaptation. In NIPS, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.).\n[22] Zheqi Lv, Zhengyu Chen, Shengyu Zhang, Kun Kuang, Wenqiao Zhang, Mengze Li, Beng Chin Ooi, and Fei Wu.\n2023. Ideal: Toward high-efficiency device-cloud collaborative and dynamic recommendation system. arXiv preprint\narXiv:2302.07335 (2023).\n[23] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia\nYang, Beng Chin Ooi, et al. 2023. DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework\nfor Efficient Device Model Generalization. In WWW. 3077â€“3085.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:16\nTrovato et al.\n[24] Xianzheng Ma, Zhixiang Wang, Yacheng Zhan, Yinqiang Zheng, Zheng Wang, Dengxin Dai, and Chia-Wen Lin. 2022.\nBoth Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding. In CVPR.\n[25] Ahmadreza Mosallanezhad, Mansooreh Karami, Kai Shu, Michelle V Mancenido, and Huan Liu. 2022. Domain adaptive\nfake news detection via reinforcement learning. In WWW. 3632â€“3640.\n[26] Hod Lipson Oscar Chang, Lampros Flokas. 2020. Principled Weight Initialization for Hypernetworks. In ICLR.\n[27] Subhankar Roy, Evgeny Krivosheev, Zhun Zhong, Nicu Sebe, and Elisa Ricci. 2021. Curriculum graph co-teaching for\nmulti-target domain adaptation. In CVPR.\n[28] Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Fabio Poiesi, and Elisa Ricci. 2023. Compositional\nSemantic Mix for Domain Adaptation in Point Cloud Segmentation. TPAMI (2023), 1â€“14. https://doi.org/10.1109/\nTPAMI.2023.3310261\n[29] Yang Shu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. 2019. Transferable curriculum for weakly-supervised\ndomain adaptation. In AAAI.\n[30] Tao Sun, Cheng Lu, Tianshuo Zhang, and Haibin Ling. 2022. Safe Self-Refinement for Transformer-Based Domain\nAdaptation. In CVPR.\n[31] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the importance of building high-quality training datasets\nfor neural code search. In ICSE. 1609â€“1620. https://doi.org/10.1145/3510003.3510160\n[32] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. 2021.\nTraining data-efficient image transformers amp & distillation through attention. PMLR.\n[33] Cheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike Zhong, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles\nStewart, Yu Su, and Wei-Lun Chao. 2023. Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target\nData. arXiv:2311.01420 [cs.LG] https://arxiv.org/abs/2311.01420\n[34] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Ge Yuying, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. 2023. All in One: Exploring Unified Video-Language Pre-training. CVPR (2023).\n[35] Fan Wang, Zhongyi Han, Yongshun Gong, and Yilong Yin. 2022. Exploring Domain-Invariant Parameters for Source\nFree Domain Adaptation. In CVPR.\n[36] Haixin Wang, Jinan Sun, Xiang Wei, Shikun Zhang, Chong Chen, Xian-Sheng Hua, and Xiao Luo. 2023. DANCE:\nLearning A Domain Adaptive Framework for Deep Hashing. In WWW. 3319â€“3330.\n[37] Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. 2023. Cross-Domain Contrastive\nLearning for Unsupervised Domain Adaptation. TMM 25 (2023), 1665â€“1673.\n[38] Zhou Xian, Shamit Lal, Hsiao-Yu Tung, Emmanouil Antonios Platanios, and Katerina Fragkiadaki. 2021. HyperDynam-\nics: Meta-Learning Object and Agent Dynamics with Hypernetworks. In ICLR.\n[39] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video Question\nAnswering via Gradually Refined Attention over Appearance and Motion. ACM MM.\n[40] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSR-VTT: A Large Video Description Dataset for Bridging Video and\nLanguage. CVPR.\n[41] Yikai Yan, Chaoyue Niu, Renjie Gu, Fan Wu, Shaojie Tang, Lifeng Hua, Chengfei Lyu, and Guihai Chen. 2022. On-Device\nLearning for Model Personalization with Large-Scale Cloud-Coordinated Domain Adaption. In KDD. 2180â€“2190.\n[42] Ceyuan Yang, Yujun Shen, Zhiyi Zhang, Yinghao Xu, Jiapeng Zhu, Zhirong Wu, and Bolei Zhou. 2023. One-Shot\nGenerative Domain Adaptation. In ICCV.\n[43] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. 2023. ST3D++: Denoised Self-Training for\nUnsupervised Domain Adaptation on 3D Object Detection. TPAMI 45, 5 (2023), 6354â€“6371. https://doi.org/10.1109/\nTPAMI.2022.3216606\n[44] Shiqi Yang, yaxing wang, kai wang, Shangling Jui, and Joost van de Weijer. 2022. Attracting and Dispersing: A Simple\nApproach for Source-free Domain Adaptation. In NIPS, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh (Eds.).\n[45] Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang. 2021. Device-Cloud Collaborative\nLearning for Recommendation. In KDD.\n[46] Junjie Ye, Changhong Fu, Guangze Zheng, Danda Pani Paudel, and Guang Chen. 2022. Unsupervised Domain Adaptation\nfor Nighttime Aerial Tracking. In CVPR.\n[47] Liping Yi, Xiaorong Shi, Nan Wang, Ziyue Xu, Gang Wang, and Xiaoguang Liu. 2023. pFedLHNs: Personalized\nFederated Learning via Local Hypernetworks. In ICANN 2023.\n[48] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. 2023. VPGTrans: Transfer Visual Prompt\nGenerator across LLMs. In NeurIPS, Vol. 36. 20299â€“20319.\n[49] Cheng Zhang, Tai-Yu Pan, Tianle Chen, Jike Zhong, Wenjin Fu, and Wei-Lun Chao. 2022. Learning with Free Object\nSegments for Long-Tailed Instance Segmentation. arXiv:2202.11124 [cs.CV] https://arxiv.org/abs/2202.11124\n[50] Chris Zhang, Mengye Ren, and Raquel Urtasun. 2020. Graph HyperNetworks for Neural Architecture Search. In ICLR.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nBackpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration\n111:17\n[51] Jingyi Zhang, Jiaxing Huang, Zichen Tian, and Shijian Lu. 2022. Spectral Unsupervised Domain Adaptation for Visual\nRecognition. In CVPR.\n[52] Ruohan Zhang, Tianzi Zang, Yanmin Zhu, Chunyang Wang, Ke Wang, and Jiadi Yu. 2023. Disentangled Contrastive\nLearning for Cross-Domain Recommendation. In International Conference on Database Systems for Advanced Applications.\nSpringer.\n[53] Yang Zhang, Philip David, Hassan Foroosh, and Boqing Gong. 2019. A curriculum domain adaptation approach to the\nsemantic segmentation of urban scenes. TPAMI 42, 8 (2019), 1823â€“1841.\n[54] Yuyang Zhao, Zhun Zhong, Zhiming Luo, Gim Hee Lee, and Nicu Sebe. 2022. Source-Free Open Compound Domain\nAdaptation in Semantic Segmentation. TCSVT 32, 10 (2022), 7019â€“7032. https://doi.org/10.1109/TCSVT.2022.3179021\n[55] Jike Zhong, Hong-You Chen, and Wei-Lun Chao. 2024. Making Batch Normalization Great in Federated Deep Learning.\narXiv:2303.06530 [cs.LG] https://arxiv.org/abs/2303.06530\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.",
    "pdf_filename": "Backpropagation-Free_Multi-modal_On-Device_Model_Adaptation_via_Cloud-Device_Collaboration.pdf"
}