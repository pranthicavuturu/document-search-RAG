{
    "title": "Backpropagation-Free Multi-modal On-Device Model",
    "abstract": "priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org. Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. 0004-5411/2018/8-ART111$15.00 https://doi.org/XXXXXXX.XXXXXXX J.ACM,Vol.37,No.4,Article111.Publicationdate:August2018. 4202 voN 81 ]CD.sc[ 3v10610.6042:viXra",
    "body": "Backpropagation-Free Multi-modal On-Device Model\nAdaptation via Cloud-Device Collaboration\nWEIJIâˆ—,\nNationalUniversityofSingapore,Singapore\nLILIâˆ—,\nNationalUniversityofSingapore,Singapore\nZHEQILVâˆ—,\nZhejiangUniversity,China\nWENQIAOZHANG,\nZhejiangUniversity,China\nMENGZELI,\nZhejiangUniversity,China\nZHENWAN,\nFudanUniversity,China\nWENQIANGLEI,\nSichuanUniversity,China\nROGERZIMMERMANN,\nNationalUniversityofSingapore,Singapore\nInourincreasinglyinterconnectedworld,whereintelligentdevicescontinuallyamasscopiouspersonalized\nmulti-modaldata,apressingneedarisestodeliverhigh-quality,personalizeddevice-awareservices.However,\nthisendeavorpresentsamultifacetedchallengetoprevailingartificialintelligence(AI)systemsprimarily\nrootedinthecloud.Asthesesystemsgrapplewithshiftingdatadistributionsbetweenthecloudanddevices,\nthetraditionalapproachoffine-tuning-basedadaptation(FTA)existsthefollowingissues:thecostlyand\ntime-consumingdataannotationrequiredbyFTAandtheloomingriskofmodeloverfitting.Tosurmountthese\nchallenges,weintroduceaUniversalOn-DeviceMulti-modalModelAdaptationFramework,revolutionizingon-\ndevicemodeladaptationbystrikingabalancebetweenefficiencyandeffectiveness.Theframeworkfeaturesthe\nFastDomainAdaptor(FDA)hostedinthecloud,providingtailoredparametersfortheLightweightMulti-modal\nModelondevices.Toenhanceadaptabilityacrossmulti-modaltasks,theAnchorFrameDistributionReasoner\n(ADR)minimizescommunicationcosts.Ourcontributions,encapsulatedintheCloud-DeviceCollaboration\nMulti-modalParameterGeneration(CDC-MMPG)framework,representapioneeringsolutionforon-Device\nMulti-modalModelAdaptation(DMMA).Extensiveexperimentsvalidatetheefficiencyandeffectivenessof\nourmethod,particularlyinvideoquestionansweringandretrievaltasks,drivingforwardtheintegrationof\nintelligentdevicesintoourdailylives.\nCCSConcepts:â€¢Informationsystemsâ†’Mobileinformationprocessingsystems;Personalization;â€¢\nHuman-centeredcomputingâ†’Mobilecomputing.\nAdditionalKeyWordsandPhrases:Cloud-devicecollaboration,modeladaptation,multi-modal\nACMReferenceFormat:\nWeiJi,LiLi,ZheqiLv,WenqiaoZhang,MengzeLi,ZhenWan,WenqiangLei,andRogerZimmermann.2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration.J.ACM37,\n4,Article111(August2018),17pages.https://doi.org/XXXXXXX.XXXXXXX\nâˆ—Theseauthorscontributedequallytothisresearch.\nAuthorsâ€™addresses:WeiJi,NationalUniversityofSingapore,Singapore,jiwei@nus.edu.sg;LiLi,NationalUniversityof\nSingapore,Singapore,lili02@u.nus.edu;ZheqiLv,ZhejiangUniversity,China,zheqilv@zju.edu.cn;WenqiaoZhang,Zhejiang\nUniversity,China,wenqiaozhang@zju.edu.cn;MengzeLi,ZhejiangUniversity,China,mengzeli@zju.edu.cn;ZhenWan,\nFudanUniversity,China,wz2311602492@gmail.com;WenqiangLei,SichuanUniversity,China,wenqianglei@gmail.com;\nRogerZimmermann,NationalUniversityofSingapore,Singapore,dcsrz@nus.edu.sg.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee\n111\nprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe\nfullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.\nAbstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires\npriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.\nÂ©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\n0004-5411/2018/8-ART111$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n4202\nvoN\n81\n]CD.sc[\n3v10610.6042:viXra\n111:2 Trovatoetal.\nCloud\nDevice Device\n(ğ‘–ğ‘–) (ğ‘—ğ‘—)\nğ‘‘ğ‘‘ Distribution Distribution ğ‘‘ğ‘‘\nShift Shift\nDeploy Deploy Device\n(ğ‘–ğ‘–)\nğ‘‘ğ‘‘\nMulti-Modal Multi-Modal Multi-Modal\nModel Model Model\nğ‘€ğ‘€ ğ‘€ğ‘€ ğ‘€ğ‘€\nMulti-Modal Multi-Modal\nModel Parameters Parameters Model\nğ‘–ğ‘– FastDomainAdaptor ğ‘—ğ‘—\nğ‘€ğ‘€ğ‘‘ğ‘‘ ğ‘€ğ‘€ğ‘‘ğ‘‘\nğ‘€ğ‘€ğ‘”ğ‘”\nFig.1. (a)Multi-modaldataoncloudanddifferentdevicesexistindifferentdistributionsduetothepersonal-\nizedpreferenceofusers.(b)Comparedwithconventionalmethodsofdeployingmodelsondifferentdevices,\nweproposeanFDAthatcanachieveabalanceofefficiencyandeffectiveness.\n1 INTRODUCTION\nIntodayâ€™sinterconnectedworld,theproliferationofintelligentdevices,rangingfromubiquitous\nsmartphonestotheever-expandingInternetofThings(IoT)ecosystem,hasbecomeanintegralpart\nofourdailylives.Thesedevicesserveasdatacollectionpowerhouses,continuouslyamassingvast\nrepositoriesofpersonalizedmulti-modaldata,whichcanincludeawidearrayofinputmodalities\nsuchastext,imagesandvideos.Thepotentiallockedwithinthistroveofmulti-modaldataarriving\ncontinuouslyisimmense,promisingtounlockhigh-qualityandtailoreddevice-awareservices\nforindividualusers.Despitepromising,thepersonalizeddeviceserviceinvolvesanalyzingthe\ndynamicnatureofthemulti-modaldatathatunderscoreusersâ€™intentions.Theprevailingartificial\nintelligence (AI) systems, primarily trained and deployed in cloud-based environments, face a\nprofoundchallengeinadaptingtothedynamicdevicedatawhenusingastaticcloudmodelfor\nallindividualusers,mainlyduetothedistributionshiftofthecloudanddevicedata,asshownin\nFigure1.Inotherwords,high-qualitypersonalizedservicerequiresAIsystemstoundergocontinual\nrefinementandadaptationtoaccommodatetheevolvinglandscapeofpersonalizedmulti-modal\ndata.\nIntuitively,oneofthestraightforwardadaptationstrategiesistofine-tunethecloudmodelbased\nonthedeviceâ€™smulti-modaldata,whichcankindlyalleviatethecloud-devicedatadistributionshift\ntomodelusersâ€™intentions.Nevertheless,wecontendthatthefine-tuning-adaptation(FTA)paradigm\nmaynotsatisfactorilyresolvedevicemodelpersonalization,whichcanbesummarizedastwokey\naspects:(1)UndesirableAnnotation.FTAoftennecessitatesmanuallyannotatingdatatoguide\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nsdohteM\n)b(\neciveD-duolC\n)a(\nnosirapmoC\ntfihS\nnoitubirtsiD\nlanoitnevnoC\nsruO\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:3\nmodeladaptation,whichtypicallyhingesonexpensiveandlabor-intensivedevicedatalabeling.\nAdditionally,thisretrainingprocesscanresultinsubstantialdelays,hinderingtheAIsystemâ€™s\nabilitytodeliverreal-time,context-awareresponsiveness.Thissituationisfurtherexacerbatedby\ntheinherentcomplexityofmulti-modaldataunderstanding,i.e.,withdatastreamsthatencompass\ntextual,visual,andauditoryinformation,theintricaciesoflabelingandretrainingaremoretime-\nconsuming, resulting in a higher time delay and thus diminishing its practicality for users. (2)\nOverfitting Risk. In real-world applications, the majority of devices could be characterized\nby sparse and specialized multi-modal data where model fine-tuning may inadvertently lead\nto overfitting issues, even leading to device model performance degradation. In other words,\nFTAimposessignificantdemandsforon-devicedataquantity,hamperingitsabilitytogeneralize\neffectivelyacrossdiversedeviceecosystems.Basedontheaforementionedinsights,ameaningful\noptimizationgoalofdevicemodelpersonalizationistoappropriatelytapontothepersonalized\nmulti-modaldata,andthusstrikethedelicatebalancebetweentheeffectivenessandefficiencyof\npersonalizedadaptation.\nTo address these multifaceted challenges and pave the way for intelligent device-driven AI\nsystemsthatharmonizeadaptabilityandcomputationalefficiency,wepresentauniversaldevice\nmulti-modalmodeladaptationframework,agroundbreakingapproachthatredefinesthelandscape\nofon-deviceAIadaptation.ThisframeworkisdesignedtorevolutionizethewayAIsystemsharness\nmulti-modaldata,simultaneouslyunlockingefficiencyandeffectivenessintheadaptationprocess.\nInourapproach,weintroducedaFastDomainAdaptor(FDA)hostedonthecloud,whichtakes\ndevice-capturedimagesasinputandproducescustomizedparametersfortheLightweightMulti-\nmodalModelonthedevice.Thesecustomizedparametersaretailoredtodifferentdatadistributions\npresentonthedevice.Moreover,theFDAadaptstovariousmulti-modaltasksbyrequiringdistinct\ndatainputs.However,certainmulti-modaltasks,likeVideoQA,necessitatetransmittingextensive\ndatasuchasmultipleframesortheentirevideotoFDA,incurringsubstantialcommunicationcosts\nandbandwidthrequirements.TomitigatethischallengeandenhanceFDAâ€™sadaptabilityacross\ndifferentmulti-modaltasks,wedevelopedtheAnchorFrameDistributionReasoner(ADR).ADR\nstandardizestheinputfortheFDAacrossvariousmulti-modaltasks.Forinstance,inthecaseof\nVideoQA,ADRselectsthefirstframeofeachvideoastheAnchorFrame.Usingacombinationof\nVariationAutoEncoder(VAE)andKeyFrame,ADRmapstheAnchorFrametoastandarddistribution\nthroughtraining.OncebothFDAandADRaretrained,theyaredeployedonthecloudtodeliver\npersonalizedmodelparameterservicesforlightweightmulti-modalmodelsonthedevice.The\ndeviceâ€™smodelcanthenuploadtheAnchorFrametoobtainpersonalizedmodelparameterstailored\ntothedatadistributionofthecurrentimageorvideoinput.Animportanthighlightofourframework\nistheabsenceofbackpropagationduringthedomainadaptationprocessforthedevicemodel.This\nuniqueapproachenablesourframeworktoachieverapiddomainadaptationofthedevicemodel,\nresultinginexceptionallylowlatency.\nSummingup,ourcontributionsaresummarizedbelow:\n(1) WeproposeaCloud-DeviceCollaborationMulti-ModalParameterGeneration(CDC-MMPG)\nframeworktoaccomplishefficienton-DeviceMulti-modalModelAdaptation(DMMA).Its\ncore is Fast Domain Adaptor (FDA) which can generate multi-modal equipment model\nparametersbasedonthedatadistributionofmulti-modaldata.Tothebestofourknowledge,\nwearethefirstonestosolvetheDMMA.OurproposedCDC-MMPGframeworkisauniversal\nonethatcanbeadaptivetoothermodalities.\n(2) WeproposetheAnchorFrameDistributionReasoner(ADR)tofurtherreducethecommu-\nnicationcostoftheCDC-MMPG.ADRsuccessfullyaddressestheissueofhighbandwidth\ndependencewhilemaintainingnearlyconsistentperformance.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:4 Trovatoetal.\n(3) Extensiveexperimentsonvideoquestionansweringandvideoretrievaltaskshaveverified\ntheefficiencyandeffectivenessofourproposedmethod.\n2 RELATEDWORKS\nCloud-DeviceCollaboration.Intherapidlyevolvingsphereofcloud-devicecollaboration,deep\nlearninghasbecomeacrucialcomponent,strategicallycombiningthefeaturesofcloud-basedand\non-devicemachinelearning.AlthoughFederatedLearning(FL),notablyrepresentedbyFedAVG[5],\nhas traditionally been a significant player, its perceived simplicity has limited its applicability\ninvariouspracticalsituations.Thisdynamicfieldhasseentheriseofinnovativemethodsand\ntechniquesdesignedtooptimizetheinteractionbetweencloudresourcesandmobiledevices.For\ninstance,MPDA[41]skillfullyutilizescloud-basedsamplestoenhancetheperformanceofon-\ndevice models. Another interesting approach involves deploying multiple models with similar\nfunctionalities, coordinated by a Meta Controller for optimal task allocation, thus expanding\nthepossibilitiesofcollaborativemodeladaptation[3,49].Moreover,DUET[23],inspiredbythe\nHyperNetworkconcept[26,38,47,50],simplifiesthedevicemodeladaptationprocess,eliminating\ntheneedforon-devicetraining[55].IDEAL[22]expandsparametergeneration-basedmodelsto\nthedomainofrecommendersystems,particularlyDC-CDR[52].Thisenhancesthegeneralization\ncapabilitiesofdevicerecommendationmodels,thoughitrequiresconsiderationoffactorssuch\nas request frequency and communication revenue. However, the above works are tailored for\nrecommendersystem,whichcannotbedirectlymigratedtothemulti-modaldomainfield,mainly\nduetothattheyignoretheinherentcomplexityofmulti-modaldata.Inthispaper,wepresent\nCDC-MMPG,tailoredformulti-modalon-devicemodeladaptationviacloud-devicecollaboration,\nwhichsimultaneouslyobtainedeffectivenessandefficiency.\nDomainAdaptation.DomainAdaptation(DA)playsacrucialroleintransferringanetwork\nthatwasinitiallytrainedonalabeledsourcedomaintoeffectivelyperformonatargetdomain.\nVarioustechniqueshavebeendevelopedtoalignthefeaturedistributionsbetweenthesourceand\ntargetdomains,suchasmaximummeandiscrepancyandcorrelationalignment.Thesemethods\naim to reduce the distribution discrepancy and enable the network to generalize well across\ndifferentdomains,improvingitsperformanceinthetargetdomain.Lotsofmethodsareproposedto\nfacilitatethedevelopmentofthedomainadaptation,suchasmulti-stagemethods[4,12,14,15,53],\ngradualtransferstrategies[2,16,31,33],andsoon.Recentadvancementsindomainadaptation\nembracecurriculum-basedstrategiesinspiredbycurriculumlearningtoenhancetheeffectiveness\noftheadaptationprocess[13,27,29],Source-freedomainadaptation(SFDA)hasemergedasa\nresponsetoprivacyandcopyrightconcerns,whereaccesstothesourcedomaindataisrestricted\norunavailable[2,4,8,10,18,20,27,29,30,35,44,53].SFDApresentsamorechallengingvariant\nknownasTest-TimeAdaptation(TTA),whichnecessitatesonlineadaptationduringtheinference\nphase.Inaddition,manyresearchersintroducethedomainadaptationsettingintodifferenttasks\ntoenhancemodelgeneralization[1,6,19,24,25,28,36,37,42,43,48,51,54].Insummary,domain\nadaptation(DA)methodsfocusonaligningfeaturedistributionsbetweenthesourceandtarget\ndomains.Recentadvancementshavehighlightedtheimportanceofcurriculum-basedstrategiesto\nimprovetheadaptationprocess.[2,4,27,29,46,53].Inthispaper,weproposeamodelgeneration\nframeworktosolvethepersonalizedadaptionproblem.\n3 METHODOLOGY\nInthissection,weprovideadetaileddescriptionofourproposedCloud-DeviceCollaborationMulti-\nModalParameterGeneration(CDC-MMPG)framework,whichincludesFastDomainAdaptor\n(FDA)andAnchorFrameDistributionReasoner(ADR).\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:5\n(a) Fast Domain Adaptor Parameter Parameter\nGenerator Feature\nVideo&Text Feature\nDevice â†’ Cloud Cloud â†’ Device\n(b) AnchorFrameDistribution Reasoner Training Inference\nProcedure Procedure\nVideo&Text Feature Sampling\nğ·% R Loe sc sonstruction Im Fag eae t& urT eext Sampling\nğœ2 !!\nğœ‡2 \"!\n!!\n(c) Lightweight Multi-modal Model Legend\nFast Domain Adaptation\nğ‘¡+ Anc Fh eo ar tuF rr eame\nParameter\nFilling\nKeyframe\nFeature\nStructure Structure\nOnly Para& meter Sampled\nFeature\nText Embedding\nğ‘¡- Prediction: Table\nVideo Feature Jumping\nQuestion: On Device\nWhat is the Answer: Entropy\nman doing? Dancing Loss On Cloud\nText Feature\nFig.2. Illustrationoftheoverallpipelineofourmethod,CDC-MMPG.(a)and(b)representtheCloudmodel,\nwhichreconstructsthevideofeaturesuploadedfromthedeviceandreasonsoutthepersonalparameters\nofthedevicemodelbasedonthereconstructedvideofeatures.(c)representsthelightweightmulti-modal\ndevice-sidemodel,whichextractsthemulti-modalfeatures,anduploadsthevideofeaturestothecloud\nmodelforthepersonaldevice-modelparameterprediction.Afterbeingupdatedwiththepersonalparameters,\nthelightweightmulti-modaldevice-sidemodelwillfurtheranalyzethemulti-modalfeaturesandmakethe\nfinalprediction.\n3.1 Preliminary\nProblemFormulation.Fortheon-DeviceMulti-modalModelAdaptation(DMMA)inthedevice-\ncloudcollaborationsystem,wehaveaccesstoasetofdevices D = {ğ‘‘(ğ‘–)}Nğ‘‘,eachdevicewith\nğ‘–=1\nitspersonali.i.dmulti-modalhistorysamplesS ğ»(ğ‘–) andmulti-modalreal-timesamplesS ğ‘…(ğ‘–) in\ncurrentsession,whereNğ‘‘ representsthenumberofdevices.Thegoalofon-DeviceMulti-modal\nModelAdaptationistogeneralizeatrainedcloudmodelMğ‘”(Â·;Î˜ ğ‘”)learnedfrom{S ğ»(ğ‘–)}N ğ‘—=ğ‘‘ 1toeach\nspecificlocaldevicemodelM ğ‘‘(ğ‘–)(Â·;Î˜ ğ‘‘(ğ‘–)) conditionedonreal-timesamplesS ğ‘…(ğ‘–),whereÎ˜ ğ‘” and\nÎ˜\nğ‘‘(ğ‘–)\nrespectivelydenotethelearnedparametersforthecloudmodelandtheğ‘–-thdevicemodel:\nC (cid:124)(cid:32)D (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)C (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)- (cid:123)M (cid:122)M (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)P (cid:32)(cid:32)(cid:32)(cid:32)G (cid:125):M (cid:124)(cid:32)(cid:32)(cid:32)(cid:32)ğ‘” (cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)(cid:32){ (cid:32)(cid:32)(cid:32)S (cid:32)(cid:32)(cid:32)(cid:32)ğ» (cid:32)(cid:32) (cid:123)(ğ‘– (cid:122))} (cid:32)(cid:32)(cid:32)ğ‘–(cid:32)N (cid:32)=(cid:32)(cid:32)ğ‘‘ (cid:32)1(cid:32)(cid:32)(cid:32); (cid:32)(cid:32)(cid:32)Î˜ (cid:32)(cid:32)(cid:32)(cid:32)ğ‘” (cid:32) (cid:125)) â†’ (cid:124)M (cid:32)(cid:32)(cid:32)(cid:32)ğ‘‘(cid:32)(cid:32)(cid:32)((cid:32)ğ‘– (cid:32)(cid:32))(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)S (cid:32)(cid:32) (cid:123)ğ‘… (cid:122)(ğ‘– (cid:32))(cid:32)(cid:32); (cid:32)(cid:32)(cid:32)Î˜ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)ğ‘‘(cid:32)(cid:32)(cid:32)((cid:32)ğ‘– (cid:32)(cid:32)) (cid:125)). (1)\nDMMAModel CloudModel DeviceModel\nFigure2illustratestheoverviewofourCDC-MMPGframeworkwhichconsistsoftwomodules\ntoimprovethegeneralizationabilityofthetrainedmodelsonthedevice:(a)FastDomainAdaptor\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nTokenizer\nPatchifier\nText\nShared\nEncoder\nMulti-modal\nFeature\nExtractor\nConverter Parameters Personalized\nretrevnoC retemaraP rotareneG\n111:6 Trovatoetal.\n(FDA)aimstolearnaglobalbenchmarkmodelbasedonthehistorysamplesofalldistributionsand\ngeneratethenetworkparametersforthedistribution-specificdevicemodelbasedonthereal-time\ndevicesamples(inSec.3.2);(b)AnchorFrameDistributionReasoner (ADR)standardizestheinput\nfortheFDAacrossvariousmulti-modelsamples.(inSec.3.3).\nModelPipeline.AsshowninAlgorithm1,therearethreestepsinthepipelineoftheCDC-\nMMPGmodel:(1)TrainingthecloudmodelMğ‘”(.),includingtheFDAmoduleandtheADRmodule,\nwiththehistorysamplesSğ».(2)Uploadingthereal-timesamplesS ğ‘…(ğ‘–) fromthedevicesideto\nthecloudside.Then,thecloudsidemodelMğ‘”(.) generatesthepersonalizedparametersforthe\ndevicemodelM ğ‘‘(ğ‘–)(.).(3)Withthemodelparameterspassedfromthecloudside,thedevicemodel\nM ğ‘‘(ğ‘–)(.)isupdatedandmakesthefinalpredictionbasedontheinputread-timesamples.\nMulti-modalFeatureExtraction.Weextractthemulti-modalrepresentationneededforthe\nsubsequentprocessesfromtheinputexamples.Specifically,givenareal-timeinputvideoğ‘‰ ğ‘Ÿ anda\ncorrespondinglanguagequeryğ‘„ ğ‘Ÿ,weemploytheDeiT[32]asthevisualfeatureextractor ğ‘“ ğ‘£(.)\nandtheBERT-baselanguagemodel[7]asthetextencoderğ‘“ ğ‘¡(.),toextractthefeaturesoftheinput\nsample.\nğ¹ ğ‘£ = ğ‘“ ğ‘£(ğ‘‰ ğ‘Ÿ), (2)\nğ¹ ğ‘¡ = ğ‘“ ğ‘¡(ğ‘„ ğ‘Ÿ), (3)\nwhereğ¹ ğ‘£ = {ğ¹ ğ‘£ğ‘–} ğ‘–ğ‘ =ğ‘“ 1representsthefeaturesofallvideoframesandğ‘ ğ‘“ isthevideoframenumber.\nğ¹ ğ‘¡ isthelanguagequeryfeature.\nWeadoptthespatial-temporalpositionalembeddingandmodalitytypeembeddingfollowing\n[34]totheextractedfeaturesğ¹ ğ‘£ andğ¹ ğ‘¡ andgetthecorrespondingembeddedfeaturesğ¸ ğ‘£ = {ğ¸ğ‘– ğ‘£} ğ‘–ğ‘ =ğ‘“\n1\nandğ¸ ğ‘¡.Then,thecross-modalfusionmoduleğ‘”(.)consistedofğ‘¡ transformerlayersisemployedto\nfusethevisualfeaturesğ¸ ğ‘£ andthelanguagefeatureğ¸ ğ‘¡:\nğ¹ ğ‘š =ğ‘”(ğ¸ ğ‘£,ğ¸ ğ‘¡), (4)\nwhereğ¹ ğ‘š = {ğ¹ ğ‘šğ‘– } ğ‘–ğ‘ =ğ‘“ 1isthegeneratedmulti-modalfeaturesafterthefeaturefusion.\n3.2 FastDomainAdaptor\nWhenadaptingthecloudmodeltothepersonalizeddevicereal-timesamplesofspecificdistribution,\nonestraightforwardstrategyisfine-tuningthecloudmodelonthedevice.However,itmaybe\nunreachableduetoalackofsufficienttrainingsamplesandcorrespondingannotationsonthe\ndevice.Totackletheaforementionedchallenge,weproposeanovelsolutioncalledtheFastDomain\nAdaptor(FDA),whichisimplementedasacloud-basedservice.TheFDAisdesignedtoprocess\ndevice-capturedimagesasinputandgeneratecustomizedparametersspecificallytailoredtothe\nLightweightMulti-modalModeldeployedonthedevice.Thesecustomizedparametersarecarefully\ncraftedtoadapttotheuniquedatadistributionsencounteredoneachindividualdevice.\nSpecifically,wefurthersampleğ·(ğ· >1)framesfromğ¹ ğ‘š randomly,uploadthesampledmulti-\nmodal features from the device side to the cloud side, and average the included ğ· frames of\nmulti-modalfeaturesintotheglobalrepresentationğ¹ ğ‘”.Then,weprojectthefeatureğ¹ ğ‘” beforeitis\nfurtheranalyzed:\nğ¸ ğ‘” = ğ‘“ ğ‘(ğ¹ ğ‘”), (5)\nwhereğ‘“ ğ‘(.)representsthemulti-layerperceptronconsistingoftwolinearlayersandanormalization\nlayer.Theoutputfeatureğ¸ ğ‘”istreatedasthereal-timesampleembeddingforhyper-network[9,11]\ninput:\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:7\nÎ˜ ğ‘‘ = ğ‘“ â„(ğ¸ ğ‘”), (6)\nwhereğ‘“ â„(.)isthemulti-layerperceptron.ThegeneratedÎ˜ ğ‘‘ istheparametersforasinglelinear\nlayerincludingboththelinearweightsandbias,whichwillbepassedtothedevicemodel.\nThegeneratedmodelparametersÎ˜ ğ‘‘ arepassedfromthecloudsidetothedeviceside.Thedevice\nmodelMğ‘‘ isupdatedwiththemodelparametersÎ˜ ğ‘‘ andmakesthepredictionğ‘ƒ ğ‘› fortheinput\nmulti-modelreal-timesample:\nğ‘ƒ ğ‘› =Mğ‘‘(ğ¹ ğ‘š;Î˜ ğ‘‘), (7)\nwhereğ¹ ğ‘š istheextractedmulti-modalfeature.\n3.3 AnchorFrameDistributionReasoner\nThe FDA is designed to adapt to diverse multi-modal tasks by utilizing specific data inputs.\nHowever,certaintasks,suchasVideoQA,requiretransmittingsignificantamountsofdata,suchas\nmultipleframesortheentirevideo,totheFDA.Thisresultsinconsiderablecommunicationcosts\nandbandwidthrequirements.ToaddressthischallengeandimprovetheadaptabilityoftheFDA\nacrossvariousmulti-modaltasks,weintroducetheAnchorFrameDistributionReasoner(ADR).\nADRplaysacrucialroleinstandardizingtheinputformatfortheFDA,ensuringcompatibility\nacrossdifferentmulti-modaltasks.\nSpecifically,duringtheinferenceprocess,werandomlyselectoneframefromthewholevideoon\nthedevicesideanduploadthemulti-modalfeatureğ¹ ğ‘šğ‘– correspondingtotheselectedğ‘–-thframeto\nthecloudside.Then,theVariationalAuto-Encoder(VAE)includinganencoderÎ¨ ğ¸ andadecoder\nÎ¨ ğ·areappliedtofurtheranalyze.Firstly,weobtainthelatentdistributionbyanalyzingtheuploaded\nfeatureğ¹ ğ‘šğ‘– withtheVAEencoderÎ¨ ğ¸:\nÎ¨ ğ¸(ğ¹ ğ‘šğ‘– ) âˆ¼N(ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™,ğœğ‘Ÿğ‘’ğ‘ğ‘™), (8)\nwhereN(ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™,ğœğ‘Ÿğ‘’ğ‘ğ‘™)representstheGaussiandistributionwithmeanğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™\nandstandarddeviation\nğœğ‘Ÿğ‘’ğ‘ğ‘™ .Thenwesampleanewrepresentationğ‘… ğ‘šğ‘– fromtheobtainedspecificdistribution:\nğ‘… ğ‘šğ‘– âˆ¼N(ğœ‡ğ‘Ÿğ‘’ğ‘ğ‘™,ğœğ‘Ÿğ‘’ğ‘ğ‘™). (9)\nTheğ‘… ğ‘šğ‘– isthenreasonedonbytheVAEdecoderÎ¨ ğ·:\nğ¹ ğ‘šğ‘– â€² =Î¨ ğ·(ğ‘… ğ‘šğ‘– ). (10)\nThe ğ¹ ğ‘šğ‘– â€² and ğ¹ ğ‘šğ‘– are utilized to adaptively generate the new feature ğ¹ ğ‘”â€² through the adaptive\ngeneratorÎ¦ ğ‘(.).Thegeneratednewfeatureswillbeforwardedtothehyper-networkwiththegoal\nofleadingittogeneralizetothetargetdomain.Thus,wecanupdatetheEq.5to:\nğ¹ ğ‘”â€² =Î¦ ğ‘(ğ¹ ğ‘šğ‘– â€²,ğ¹ ğ‘šğ‘– ). (11)\nThefeatureğ¹ ğ‘”â€² isappliedtoreplacetheglobalrepresentationoftheinputsampleğ¹ ğ‘” definedin\nSection3.2forfurtherpredictionofthepersonalizedmodelparameters.\nTheframeworkuseslimitedcommunicationbandwidth:one-framemulti-modalfeatureupload\nandhyper-networkparameters(afewlinearlayers)download,promisingthegeneralizationability\nandextremelylowcommunicationdelayofthedevicemodel.\nDuringthetrainingprocess,allhistorysamplesaredirectlysavedonthecloudsideandpartici-\npatedinthetrainingprocess,followingthesettingofpreviousmethods[45].Giventhesampled\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:8 Trovatoetal.\nAlgorithm1:CDC-MMPGFramework\n1\nInitializethemulti-modalfeatureextractionmodel:Mğ‘£ğ‘™;thehyper-networkmodelinFDA:\nâ„(Â·);theencoderandthedecoderinADR:Î¨ ğ¸,Î¨ ğ·;theadaptivegenerator:Î¦ ğ‘.\n2\nPhaseI:Input:HistoricaldataSğ».\n3 1)TraintheADRonthecloudusingSğ» andthetrainingobjectivesinEq.18.\n4\nOutputthetrainedmodels:M ğ‘£â€² ğ‘™,â„â€²(Â·),Î¨ ğ¸â€²,Î¨ ğ·â€²,Î¦ ğ‘â€².\n5\nPhaseII:Input:Real-timedataSğ‘….\n6 1)Extractmulti-modalfeaturesandprocesstheone-framefeaturetothecloud\nğ¹ ğ‘ğ‘– ğ‘™ğ‘œğ‘¢ğ‘‘,ğ¹ ğ‘” â†M ğ‘£â€² ğ‘™(Sğ‘…).\n7\n2)Obtainlatentdistributionofğ¹ ğ‘ğ‘™ğ‘œğ‘¢ğ‘‘:N(ğœ‡,ğœ),whereğœ,ğœ‡ â†Î¨ ğ¸â€²(ğ¹ ğ‘ğ‘– ğ‘™ğ‘œğ‘¢ğ‘‘).\n8\n3)Randomlysamplefromthisdistributionğ¹\nğ‘Ÿğ‘ğ‘›ğ‘‘\nâ†N(ğœ‡,ğœ).\n9\n4)Decodetherandomlysampleddistributionğ¹ ğ‘Ÿâ€²\nğ‘ğ‘›ğ‘‘\nâ†Î¨ ğ·â€² (ğ¹ ğ‘Ÿğ‘ğ‘›ğ‘‘).\n10 5)Getadaptivelayerembeddingğ¸ ğ‘š â†Î¦ ğ‘(,ğ¹ ğ‘Ÿâ€² ğ‘ğ‘›ğ‘‘,ğ¹ ğ‘ğ‘– ğ‘™ğ‘œğ‘¢ğ‘‘).\n11 6)GenerateadaptiveparametersusingthelayerembeddingÎ˜ ğ‘ â†â„â€²(ğ¸ ğ‘š).\n12 Outputtheparameters:Î˜ ğ‘.\n13\nPhaseIII:Input:Real-timedataSğ‘….\n14 1)DownloadtheadaptiveparameterstothedeviceÎ˜ğ‘‘ ğ‘ â†Î˜ ğ‘.\n15\n2)SharefeaturebackbonebetweenthecloudanddeviceMğ‘‘ â†Mğ‘£ğ‘™.\n16 3)MakethepredictionwithÎ˜ğ‘‘ ğ‘:ğ‘ƒ ğ‘› â†Mğ‘‘(Sğ‘…;Î˜ğ‘‘ ğ‘)\n17 Outputtheğ‘ƒ ğ‘›.\nmulti-modal features ğ¹ ğ‘” and the randomly selected feature ğ¹ ğ‘šğ‘– , the variational auto-encoder is\nappliedtofurtheranalyzethesameastheinferenceprocess.Firstly,theencoderÎ¨ ğ¸ projectğ¹ ğ‘š into\nthelatentspaceandgenerateanewfeatureğ¹ â„:\nğ¹ â„ =Î¨ ğ¸(ğ¹ ğ‘”). (12)\nThen,toreconstructtheinputfeatureğ¹ ğ‘”,thedecoderÎ¨\nğ‘‘\ndecodesthefeatureğ¹\nâ„\ntothefeatureğ¹ ğ‘”â€²:\nğ¹ ğ‘”â€² =Î¨ ğ·(ğ¹ â„). (13)\nInordertotrainthevariationalauto-encoder,weusetheKullback-Leibler(KL)lossğ¿ ğ‘˜ğ‘™ and\nreconstructionlossğ¿ ğ‘…ğ‘’ğ‘ asfollows:\nğ¿ ğ‘˜ğ‘™ =D ğ‘˜ğ‘™(N(ğœ‡â€²,ğœâ€²)||N(0,I)), (14)\nğ¿ =MSE(F ,Fâ€²), (15)\nRec g g\nwhereN(0,I)representsthenormaldistribution,D ğ‘˜ğ‘™(.)representstheKullback-Leiblerdivergence\ncalculationandMSE(Â·)denotesthemeansquareloss.\nInspiredby[21],inordertoincorporatethevariousdistributionshiftsintothefeatureofthe\nsinglevideoframe,theadaptivegeneratorgeneratesnewfeaturesğ¹ ğ‘ byre-normalizingtheğ¹ ğ‘”â€² to\nhavethesamechannel-wisemeanandstandarddeviationastheğ¹ ğ‘šğ‘– .Thisprocesscanbeformulated\nasfollows:\n(cid:18)ğ¹ğ‘– âˆ’ğ›¿(ğ¹ğ‘– )(cid:19)\nÎ¦ ğ‘(ğ¹ ğ‘”â€²,ğ¹ ğ‘šğ‘– ) =ğ›¿(ğ¹ ğ‘”â€²) ğ‘š\nğ›¾(ğ¹ğ‘–\n)ğ‘š +ğ›¾(ğ¹ ğ‘”â€²), (16)\nğ‘š\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:9\nwhereğ›¿(Â·)andğ›¾(Â·)denoteschannel-wisemeanandstandarddeviationoperations,respectively.\nFollowingpreviousworksonvariationalauto-encodergeneralization,weadditionallycompute\nthelossvaluesbetweenthegeneratednewfeaturesandğ¹ ğ‘”,ğ¹ ğ‘šğ‘– ,respectively,totrainthewhole\nmodule:\nğ¿ =ğœ†MSE(F ,F )+MSE(F ,Fi ), (17)\nAG a g a m\nwhereğœ†denotesahyper-parameter,andğ¹ denotesthefeaturegeneratedbytheadaptivegenerator.\na\nThus,theoveralltraininglossğ¿forthewholeframeworkis:\nğ¿ =ğ¿ +ğ¿ +ğ¿ . (18)\nAG Rec KL\nTable1. ResultsofourproposedmethodinOpen-endedVideoQAtaskonMSRVTT-QAandMSVD-QA.We\nadoptaccuracyastheevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberof\nlearnableparametersonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofour\nproposedmethod.â€œF-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeatureextractor.\nâ€œF-hyper\"denotesonlyfine-tuningtheclassifiersandasimplehyper-networkwithoutanadaptivegenerator.\nVSRVTT-QA MVSD-QA\nMethods\nAcc. D-Param. C-Param. TimeDelay Acc. D-Param. C-Param. TimeDelay\nF-linear 13.6 11.6M / 60000ms 17.3 11.5M / 60000ms\nFine-tuning 36.7 11.6M / 60000ms 34.3 11.5M / 60000ms\nF-hyper 11.1 11.6M 28.1M 5.55ms 6.34 11.5M 18.7M 3.71ms\nOurs 37.1 11.6M 55.9M 5.55ms 35.4 11.5M 46.5M 3.71ms\nTable2. ResultsofourproposedmethodinOpen-endedVideoQAtaskonTGIF.Weadoptaccuracyasthe\nevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberoflearnableparameters\nonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofourproposedmethod.\nâ€œF-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeatureextractor.â€œF-hyper\"denotes\nonlyfine-tuningtheclassifiersandasimplehyper-networkwithoutanadaptivegenerator.\nTGIF\nMethods\nAccuracy D-Param. C-Param. TimeDelay\nF-linear 27.3 11.6M / 60000ms\nFine-tuning 54.7 11.6M / 60000ms\nF-hyper 19.6 11.6M 18.7M 5.70ms\nOurs 55.2 11.6M 46.5M 5.70ms\n4 EXPERIMENT\nInthissection,weevaluateourproposedmethodonthreepopularmulti-modaltasksandfour\nrelevantdatasets.Additionally,weperformablationstudiestoinvestigatetheimpactofeachmodule\ninourmethod,theinfluenceofthenumberofframessampledfromthewholevideo,andthetime\ndelaybetweenthecloudandthedevicewithvariousinternetconditions.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:10 Trovatoetal.\n4.1 Datasets\nWeconductedourexperimentsonthreemulti-modaltasks:multiple-choiceVideoQA,open-ended\nVideoQA,andvideo-textretrieval.Fortheopen-endedVideoQAtask,weusetheMSRVTT-QA\ndataset[39],MSVD-QAdataset[39]andtheTGIFdataset[17].Forthemultiple-choiceVideoQA\ntask,weusetheMSRVTT-QAdataset[39].Forthevideo-textretrievaltask,weexperimentonthe\nMSRVTTdataset[40].\n(1) MSRVTT.TheMSRVTTisalarge-scaledatasetfortheopen-domainvideocaptioning,which\nconsistsof10Kvideoclipsandeachvideoclipisannotatedwith20Englishsentences.The\nstandardsplitsuse6,513clipsfortraining,497clipsforvalidation,and2,990clipsfortesting.\n(2) MSRVTT-QA.TheMSRVTT-QAdatasetcontains10Kvideoclipsand243kquestionanswer\npairs.Specifically,weusedMSRVTT-QAdataset[39]foropen-endedVQAandmultiple-\nchoiceVQA.Following[39],Weusethetraditionaldatasplitwhichis65%forthetraining\nset,5%forthevalidationsetand30%fortestset.\n(3) MSVD-QA.TheMSVD-QAdatasethasatotalnumberof1,970videoclipsand50,505question\nanswerpairs.Following[39],wesplitthedatasetbasedonvideosthattrainingsettakes61%,\nvalidationsettakes13%,andtestsettakes26%ofthetotalnumberofvideos.\n(4) TGIF. The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences\ndescribingvisualcontentoftheanimatedGIFs.Onesentenceisprovidedforeveryanimated\nGIF for the training and validation splits, and three sentences per GIF for the test split.\nSpecifically,thereare80Ktrainingsamples,11Kvalidationsamplesand11ktestsamples.\n4.2 EvaluationMetrics\nFollowing previous works, we adopt accuracy for the open-ended Video QA task, and VR@K,\nTR@Kforvideo-textretrievaltask.Specifically,VR@Kdenotestherecallofvideototextretrieval\nandTR@Kdenotestherecalloftexttovideoretrieval.Forbothofthesetwotasks,Kissetto\n1,2,5respectively.Additionally,wecalculatethenumberoflearnableparametersineachmodel.\nMeanwhile,forthepracticalscenarioofcloud-devicecollaboration,wealsomeasurethetimedelay\nforthecloud-devicecommunicationprocess.\n4.3 TasksandImplementationDetails\nWeevaluateourmethodonthreetasks:\n(1) Video-textRetrieval:Weevaluatetwosub-tasksincludingvideo-to-textretrievalandtext-\nto-videoretrieval.Thetext-to-videoretrievaltaskrequiresretrievingthetargetvideousing\nTable3. ResultsofourproposedmethodinText-videoRetrievaltask.Thenumberoflearnableparameterson\nthedevice,thenumberoflearnableparametersonthecloud,andthetimedelayareadditionallymeasured\ntoshowtheefficiencyofourproposedmethod.\nMSRVTT\nMethods\nVR@1 VR@5 VR@10 TR@1 TR@5 TR@10\nF-linear 2.0 7.7 14.1 3.1 10.1 16.9\nFine-tuning 4.8 17.6 27.9 6.2 20.7 31.8\nF-hyper 2.6 8.6 14.7 2.7 11.4 17.6\nOurs 6.6 21.3 33.8 5.6 21.2 33.3\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:11\nTable4. Ablationstudiesofdifferentnumbersofsampledframes.Wemaketheevaluationonthreedatasets\ninOpen-endedVideo-questionAnsweringtask.â€œTime/Epoch\"denotesthetimeoffine-tuningforeachepoch.\nMSRVTT-QA MSVD-QA TGIF\nFrames\nAccuracy Time/Epoch Accuracy Time/Epoch Accuracy Time/Epoch\n2 36.1 740s 34.1 204s 54.3 245s\n3 36.7 897s 34.3 228s 54.7 310s\n4 37.0 1309s 34.3 318s 54.7 448s\n5 37.1 2092s 34.0 377s 55.2 702s\nlanguagequeries.Thevideo-to-textretrievaltaskrequiresretrievingthetargettextusing\nvideos.\n(2) Open-endedVideo-questionAnswering:Itrequiresansweringquestionsaccordingto\nthecontextofthevideo.Theanswersareoriginallyinfree-formnaturallanguage,butitisa\ncommonpracticetoconvertthetasktoaclassificationtaskbyrepresentingtheanswerwith\naclasslabel.\n(3) Multiple-choiceVideo-questionAnswering:Givenavideowithaqueryand5candidate\ncaptions,thetaskistofindtheonethatfitsthequeryoutof5possiblecandidates.Thecorrect\nansweristheground-truth(GT)caption,andfourothernegativesarechosenfromother\ncaptionsthathavedifferentactivity-phraselabelsfromthecorrectanswer.\nImplementation Details. We employ the All-in-One-Ti [34] as the baseline model, which\nincludestheDeiT[32]asitsvisualbackboneandBERT-base[7]asitssemanticencoder.Specifically,\nweonlyusetheembeddinglayersofBERTfortextembedding.TheAll-in-one-Tiisadoptedas\nthemulti-modalencoderbothforthedevicemodelandthecloudmodel.Thevalueofğ· isset\nto3.DuringthetrainingprocessofADR,weemploytheadamwoptimizerwithapolynomial\ndecayscheduler.Thelearningrateissetto2e-5andthetrainingepochissetto10.Duringthe\ntrainingprocessoftherestofthemodules,wefrozetheparametersintheadaptivegenerator,\nandthesameadamwoptimizerisusedwiththe1e-4learningrateused.Thetotaltrainingepoch\nfortherestmodulesissetto40.Forthehyper-parametersandothersettingsduringthetraining\nprocess,ğœ†issetto0.1,andthehiddenlayerofthehyper-networkissetto96(forOpen-endedVQA\nandMultiple-choiceVQA)and256(forVideo-textRetrieval).Duringtheinferenceprocess,allthe\nparametersonthedevicemodelandthecloudmodelarefrozenexceptthelastfewdynamiclinear\nlayersofthedevicemodel.Thecloudmodelandthedevicemodelsharethesameparametersof\nthemulti-modalencoder.Thenthecloudgeneratesdynamicparametersusinghyper-networkfor\nthedevicemodelforbettergeneralization.\n4.4 PerformanceComparison\nInthissection,weintroducehowwedesignthebaselinemethodtoshowcasetheefficiencyofour\nproposedmethod,andwepresenttheexperimentalresultsofourmethodonvarioustasksand\ndatasets.\n4.4.1 BaselineMethods.\nForwearethefirsttoexplorethisfield,wedesigntheotherthreebaselinemethodstoshow\nthesuperiorityofourproposedmethod.AsshowninTab.1andTab.2,weadoptaccuracyasthe\nevaluationmetric.Thenumberoflearnableparametersonthedevice,thenumberoflearnable\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:12 Trovatoetal.\nparametersonthecloud,andthetimedelayareadditionallymeasuredtoshowtheefficiencyofour\nproposedmethod.â€œF-linear\"denotesonlyfine-tuningtheclassifiersafterthemulti-modalfeature\nextractor.â€œF-hyper\"denotesonlyfine-tuningtheclassifiersandasimplehyper-networkwithout\nanadaptivegenerator.Besidesourproposedmethod,wealsodesignthefine-tuningapproach,\nfine-tuningonlythelinearclassifiersapproach(F-linear),andfine-tuningboththelinearclassifiers\nandsolelyhyper-networkapproach.Forthefine-tuningapproach,wesimplyaddlayersofMLPs\nafterthemulti-modalfeatureextractoranddofine-tuningonthewholemodel.Forfine-tuning\nonlythelinearclassifiersapproach,wefrozetheparametersinthemulti-modalfeatureextractor\nandfine-tunedthelayersofMLPsafterthefeatureextractor.Forthefine-tuningofboththelinear\nclassifiersandthesolelyhyper-networkapproach,weemployasimplehyper-networkwithout\nthe proposed adaptive generator and do fine-tuning for both linear classifiers and the simple\nhyper-network.\nOpen-endedVideo-questionAnswering.ForOpen-endedVideo-questionAnswering,the\nresponsesaretypicallyexpressedinunrestrictednaturallanguage.However,aprevalentapproach\ninvolvestransformingthistaskintoaclassificationproblembyencodingtheanswersasclasslabels.\nToachievethis,weincorporatelayersofMLPswithahiddenlayerdimensionof96followingthe\nextractionofmulti-modalfeatures.ThedimensionoftheMLPâ€™soutputlayervariesaccordingto\nthespecificlabelsizeassociatedwiththedatasets,forinstance,1501labelsfortheMSRVTT-QA\ndataset.\nMultiple-choiceVideo-questionAnswering.ForMultiple-choiceVideo-questionAnswering,\nwhereboththequestionsandcandidateanswersarepresentedassentences,ourapproachinvolves\nconcatenatingthequestionandtheanswercandidates.Todistinguishbetweenthem,weemploy\nthespecialtoken[SEP].Subsequently,wedeterminethepredictionbyselectingthecandidatewith\nthehighestoutputlogit,signifyingitslikelihoodofbeingthecorrectanswer.\nVideo-textRetrieval.ForVideo-textRetrieval,theretrievalprocessencompassestwodirections:\ntext-to-videoretrievalandvideo-to-textretrieval.Ineachdirection,therespectivemodalityisfirst\nextracted,andsubsequently,acomparativeanalysisisconducted.Thepredictionsareultimately\ngeneratedthroughtheapplicationofMLPs.\n4.4.2 MainResults.\nInthissection,wepresenttheexperimentalresultsofopen-endedVideoQAinTab.1andTab.2,\nmultiple-choiceVideoQAinTab.5,andVideo-textRetrievalinTab.3onfourpopulardatasets.\nEffectiveness.Tab.1,Tab.2,Tab.5,andTab.3provideacomprehensiveoverviewofthesuperi-\norityofourmethodcomparedtootherbaselineapproachesacrossvariousdatasetsandevaluation\ncriteria.Notably,ourmethodconsistentlyoutperformsalternativebaselinemodelsinnearlyall\naspectsacrossthesedatasets.Forexample,considerthetaskofOpen-endedVideo-questionAn-\nsweringontheMSRVTT-QAdataset.Ourproposedmethodexcelsintermsofbothaccuracyand\ntimeefficiencywhencomparedtoallotherbaselinemodels.Theconventionalfine-tuningmethod\ndemandsasignificantamountoftimetocompletethefine-tuningprocess.Asdemonstratedin\nTab.4,whenselectingthreeframesfromtheentirevideo,thetimerequiredforeachfine-tuning\nepochontheMSRVTTdatasetisapproximately897seconds.Incontrast,ourmethodachieves\nnearlyreal-timecommunication,effectivelyreducingthetimedelaybetweenthecloudandthe\ndevicetoaslittleas5.55ms,contingentoninternetconditions.Furthermore,ourproposedmethod\neven surpasses the fine-tuning approach in terms of performance, indicating its superior fast\ngeneralizationcapabilitiesonpersonalizedsamples.Tab.1andTab.2,whichdenotetheresultson\ntheMSVD-QAandTGIFdatasets,itisevidentthatourmethodsignificantlyoutperformsother\nbaselinemodels,thusaffirmingtheeffectivenessofourproposedapproach.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:13\nTable5. ResultsofourproposedCDC-MMPGframeworkinthemultiple-choiceVideoQAtask.Thenumber\noflearnableparametersonthedevice,thenumberoflearnableparametersonthecloud,andthetimedelay\naremeasuredtoshowtheefficiencyofourproposedmethod.\nMSRVTT-QA\nMethods TimeDelay\nAcc. D-Param. C-Param.\nF-linear 3.58 11.4M / â‰¥60000ms\nFine-tuning 75.6 11.4M / â‰¥60000ms\nF-hyper 46.0 11.4M 37.2K â‰¥5.70ms\nOurs 77.0 11.4M 20.3M â‰¥5.70ms\nTable6. Timedelayofourframeworkindifferentcircumstances(variousinternetspeeds)ondifferentdatasets.\nOurproposedmethodlargelyreducesthetimedelaybetweenthecloudandthedevice,realizingalmost\nreal-timecommunication.â€œ â†‘ â€denotestheuploadprocessfromthedevicetocloud.â€œ â†“ â€denotesthe\nparametersdownloadedfromthecloudtodevice.\nDatasets Size 4G:5MB/s 4G:15MB/s 5G:50MB/s 5G:100MB/s\nâ†‘:0.75KB â†‘:0.15ms â†‘:0.05ms â†‘:0.01ms â†‘:0.007ms\nMSRVTT\nâ†“:568.5KB â†“:111ms â†“:37.0ms â†“:11.1ms â†“:5.55ms\nâ†‘:0.75KB â†‘:0.15ms â†‘:0.05ms â†‘:0.01ms â†‘:0.007ms\nMSVD\nâ†“:379.4KB â†“:74ms â†“:24.7ms â†“:7.41ms â†“:3.71ms\nâ†‘:0.75KB â†‘:0.15ms â†‘:0.05ms â†‘:0.01ms â†‘:0.007ms\nTGIF\nâ†“:583.8KB â†“:114ms â†“:38.0ms â†“:11.4ms â†“:5.70ms\nExtensibility.Ourproposedapproachexhibitsthecapacitytoenhanceaccuracywhileconcur-\nrentlyreducingtimedelaysacrossalldatasetsassociatedwiththethreespecifictasks.Asdelineated\nin Tab. 3, our methodology yields a significant performance improvement, particularly in the\ndomainoftext-videoretrieval.Ourassessmentencompassesatwo-directionalapproach,addressing\nboth text-to-video retrieval and video-to-text retrieval. Remarkably, our method surpasses the\nmajorityofbaselinemethods,notonlyintermsofaccuracybutalsowithrespecttotimedelays.In\nthecontextofmultiple-choiceVideoQA,ourapproachconsistentlyassertsitssuperiorityacross\nallevaluationmetrics.ThisisevidentfromthedatapresentedinTab.5.\n4.5 AblationStudies\nNumberofsampledframes.InTab.4,weinvestigatetheinfluenceofthenumberofsampled\nframesğ· fromtheentirevideoonthefinalresults.Simultaneously,wemeasurethetimeoffine-\ntuningforeachepoch.Tobespecific,weemploythestandardfine-tuningapproachtoevaluate\nthisfactoracrossthreedistinctdatasets,allintheOpen-endedVideo-questionAnsweringtask.\nThebatchsizeforallthedatasetsissetto256,andthehardwareisasingleA100GPU.Itbecomes\nevidentthatasthenumberofsampledframesincreases,thetimeoffine-tuningforeachepoch\nexperiencesarapidescalation,whichisinlinewithintuition.Forinstance,whenonlytwoframes\naresampled,thefine-tuningprocessdemandsapproximately740secondsonMSRVTT-QAdataset.\nHowever,iffiveframesaresampledfromeachvideo,thetimerequirementextendstoaround2092\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:14 Trovatoetal.\nseconds,whichisalmostthreetimeslongerthantheformerscenario.Interestingly,themodelâ€™s\nperformancedoesnotexhibitaconsistentupwardtrendwiththeincreasingnumberofsampled\nframes.Besides,themodelâ€™sperformancereachesitspeakwhenthenumberofsampledframesis\nsetatthreeorfour.Tomakeatrade-offbetweenperformanceandtrainingcost,wesetthenumber\nofsampledframesasthreeinourmodel.\nDifferentmodulesinourproposedCDC-MMPGframework.Inaccordancewiththeresults\ninTab.1andTab.2,wehavedevisedthreeadditionalbaselinemethodstoassesstheefficacyofour\ncomprehensiveframework.Tobemorespecific,ourassessmentinvolvesacomparativeanalysis\noftheperformanceoutcomesbetweenthefine-tuningmodelandourproposedmodel,serving\ntoemphasizethelatterâ€™ssuperiority.Furthermore,wehaveundertakenanevaluationofthe\"F-\nlinear\"modeltofacilitateanablationstudyoftheclassifierssubsequenttothemulti-modalfeature\nextractor.Inaddition,the\"F-hyper\"modelhasbeenexaminedinordertoconductanablationstudy\nfocusedonthehyper-networkandtheadaptivegenerator.Itisnoteworthythattheomissionofour\nproposedadaptivegeneratorrendersasimplehyper-networkpronetoencounteringsub-optimal\nperformance, particularly evident in certain datasets, including MSRVTT-QA, MSVD-QA, and\nTGIF. This diminished performance is primarily attributed to the issue of over-fitting, leading\nto the emergence of spurious correlations between visual cues and predictions. Our adaptive\ngenerator,asproposed,substantiallyimprovestheperformanceofthehyper-networkthroughthe\nimplementationofanchor-framedistributionreasoning.\nTimedelaybetweencloudanddeviceinvariousinternetconditions.Tab.6illustratesthe\ntimedelayassociatedwithdatatransmissionbetweencloudservicesanddevicesundervarious\ninternetconnectivityconditions.Specifically,wehaveconductedthisexperimentinfourdistinct\nscenarios,eachreflectingadifferentinternetcondition:4G(5MB/s),4G(15MB/s),5G(50MB/s),and\n5G(100MB/s).Thesescenarioscorrespondtoreal-worldsituationsinvolvingoldermobiledevices\nwithlimited4Gconnectivity,oldermobiledeviceswithoptimal4Gconnectivity,modernmobile\ndeviceswith5Gconnectivity,andmodernmobiledeviceswithadvanced5Ginternetconnectivity.\nOurproposedmethoddemonstratesremarkableimprovementsintermsoftimedelay.Forexample,\nduringtheinferenceprocess,amere0.75KBoffeaturesisrequiredtobeuploadedtothecloud,a\ntaskthatconsumesonlyabout0.007msina5G(100MB/s)environment.Eveninthepresenceof\nconstrained4G(5MB/s)internet,thetimerequiredforuploadingremainsnegligible.Thedurationof\nparameterdownloading,however,fluctuatesaccordingtothedatasetinuse,largelyduetovariations\ninparametersize.Forinstance,theparametersizeis583.8KBfortheTGIFdataset,thelargestamong\nthedatasets,whereastheparametersizefortheMSVD-QAdataset,thesmallest,amountsto379.4KB.\nNotably,evenwhendealingwiththedemandingTGIFdataset,ourproposedapproachdemonstrates\nthecapabilitytoachievereal-timepredictions.Forinstance,ina5G(100MB/s)environment,the\ntimerequiredforparameterdownloadingismerely5.70ms.Eveninscenariosfeaturinglimited4G\n(5MB/s)connectivity,thetimedelayremainsnearlyimperceptibleinpracticalterms.\n5 CONCLUSION\nInthispaper,wefocusonthechallengeofenablingefficientandeffectiveadaptationofAIsystems\ntopersonalizedmulti-modaldatageneratedbyintelligentdevices.Theshiftingdatadistribution\nbetween the cloud and devices necessitates novel approaches to ensure high-quality personal-\nizedservices.Weintroducedauniversalon-deviceMulti-modalModelAdaptationFramework,\nfeaturing the Fast Domain Adaptor (FDA) and the AnchorFrame Distribution Reasoner (ADR).\nFDA,hostedinthecloud,tailorsmodelparametersforon-deviceLightweightMulti-modalModels,\noptimizingadaptabilityacrossdiversedatadistributions.ADRfurtherstandardizesinput,reducing\ncommunication costs. Our contributions, consolidated within the Cloud-Device Collaboration\nMulti-modalParameterGeneration(CDC-MMPG)framework,constituteapioneeringsolutionfor\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:15\non-DeviceMulti-modalModelAdaptation(DMMA),demonstratedthroughextensiveexperiments.\nLookingforward,promisingdirectionsincludeexpandingtheframeworktoaccommodatevarious\nmodalities,refiningpersonalizeddatahandlingtechniques,andfurtherreducingcommunication\ncostsinmulti-modaltasks.\nREFERENCES\n[1] SabbirAhmed,AbdullahAlArafat,MamshadNayeemRizve,RahimHossain,ZhishanGuo,andAdnanSirajRakin.\n2023.SSDA:SecureSource-FreeDomainAdaptation.InICCV.\n[2] ChaoqiChen,WeipingXie,WenbingHuang,YuRong,XinghaoDing,YueHuang,TingyangXu,andJunzhouHuang.\n2019.Progressivefeaturealignmentforunsuperviseddomainadaptation.InCVPR.\n[3] Hong-YouChen,JikeZhong,MingdaZhang,XuhuiJia,HangQi,BoqingGong,Wei-LunChao,andLiZhang.2023.\nFederatedLearningofShareableBasesforPersonalization-FriendlyImageClassification. arXiv:2304.07882[cs.CV]\nhttps://arxiv.org/abs/2304.07882\n[4] PengfeiChen,LeidaLi,JinjianWu,WeishengDong,andGuangmingShi.2021. Unsupervisedcurriculumdomain\nadaptationforno-referencevideoqualityassessment.InICCV.\n[5] LiamCollins,HamedHassani,AryanMokhtari,andSanjayShakkottai.2022.Fedavgwithfinetuning:Localupdates\nleadtorepresentationlearning.NIPS(2022).\n[6] VictorG.TurrisidaCosta,GiacomoZara,PaoloRota,ThiagoOliveira-Santos,NicuSebe,VittorioMurino,andElisa\nRicci.2022.Dual-HeadContrastiveDomainAdaptationforVideoActionRecognition.InWACV.\n[7] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectional\nTransformersforLanguageUnderstanding.InNAACL.\n[8] NingDing,YixingXu,YehuiTang,ChaoXu,YunheWang,andDachengTao.2022.Source-FreeDomainAdaptation\nviaDistributionEstimation.InCVPR.\n[9] TanM.Dinh,AnhTuanTran,RangNguyen,andBinh-SonHua.2022.HyperInverter:ImprovingStyleGANInversion\nviaHypernetwork.InCVPR.\n[10] RinonGal,OrPatashnik,HaggaiMaron,AmitH.Bermano,GalChechik,andDanielCohen-Or.2022.StyleGAN-NADA:\nCLIP-GuidedDomainAdaptationofImageGenerators.ACMTrans.Graph.(2022).\n[11] DavidHa,AndrewDai,andQuocV.Le.2017.HyperNetworks.InICLR.\n[12] WeiJi,LiLi,HaoFei,XiangyanLiu,XunYang,JunchengLi,andRogerZimmermann.2024.TowardsComplex-query\nReferringImageSegmentation:ANovelBenchmark. ACMTrans.MultimediaComput.Commun.Appl.(Nov.2024).\nhttps://doi.org/10.1145/3701733\n[13] LiLi,WeiJi,YimingWu,MengzeLi,YouQin,LinaWei,andRogerZimmermann.2024.PanopticSceneGraphGeneration\nwithSemantics-PrototypeLearning.AAAI38,4(Mar.2024),3145â€“3153. https://doi.org/10.1609/aaai.v38i4.28098\n[14] LiLi,YouQin,WeiJi,YuxiaoZhou,andRogerZimmermann.2024. Domain-WiseInvariantLearningforPanoptic\nSceneGraphGeneration.InICASSP.3165â€“3169. https://doi.org/10.1109/ICASSP48485.2024.10447193\n[15] LiLi,ChenweiWang,YouQin,WeiJi,andRenjieLiang.2023.Biased-PredicateAnnotationIdentificationviaUnbiased\nVisualPredicateRepresentation.InACMMM(MMâ€™23).4410â€“4420. https://doi.org/10.1145/3581783.3611847\n[16] MingLi,JikeZhong,ChenxinLi,LiuzhuozhengLi,NieLin,andMasashiSugiyama.2024.Vision-LanguageModel\nFine-TuningviaSimpleParameter-EfficientModification. arXiv:2409.16718[cs.CV] https://arxiv.org/abs/2409.16718\n[17] YunchengLi,YaleSong,LiangliangCao,JoelTetreault,LarryGoldberg,AlejandroJaimes,andJieboLuo.2016.TGIF:\nANewDatasetandBenchmarkonAnimatedGIFDescription.InCVPR.\n[18] MattiaLitrico,AlessioDelBue,andPietroMorerio.2023.GuidingPseudo-LabelsWithUncertaintyEstimationfor\nSource-FreeUnsupervisedDomainAdaptation.InCVPR.\n[19] NannanLu,HanhanXiao,ZhanguoMa,TongYan,andMinHan.2022. DomainAdaptationWithSelf-Supervised\nLearningandFeatureClusteringforIntelligentFaultDiagnosis.IEEETransactionsonNeuralNetworksandLearning\nSystems(2022),1â€“14. https://doi.org/10.1109/TNNLS.2022.3219896\n[20] YaweiLuo,PingLiu,TaoGuan,JunqingYu,andYiYang.2020.AdversarialStyleMiningforOne-ShotUnsupervised\nDomainAdaptation.InNIPS.\n[21] YaweiLuo,PingLiu,TaoGuan,JunqingYu,andYiYang.2020.AdversarialStyleMiningforOne-ShotUnsupervised\nDomainAdaptation.InNIPS,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.).\n[22] ZheqiLv,ZhengyuChen,ShengyuZhang,KunKuang,WenqiaoZhang,MengzeLi,BengChinOoi,andFeiWu.\n2023.Ideal:Towardhigh-efficiencydevice-cloudcollaborativeanddynamicrecommendationsystem.arXivpreprint\narXiv:2302.07335(2023).\n[23] ZheqiLv,WenqiaoZhang,ShengyuZhang,KunKuang,FengWang,YongweiWang,ZhengyuChen,TaoShen,Hongxia\nYang,BengChinOoi,etal.2023.DUET:ATuning-FreeDevice-CloudCollaborativeParametersGenerationFramework\nforEfficientDeviceModelGeneralization.InWWW.3077â€“3085.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\n111:16 Trovatoetal.\n[24] XianzhengMa,ZhixiangWang,YachengZhan,YinqiangZheng,ZhengWang,DengxinDai,andChia-WenLin.2022.\nBothStyleandFogMatter:CumulativeDomainAdaptationforSemanticFoggySceneUnderstanding.InCVPR.\n[25] AhmadrezaMosallanezhad,MansoorehKarami,KaiShu,MichelleVMancenido,andHuanLiu.2022.Domainadaptive\nfakenewsdetectionviareinforcementlearning.InWWW.3632â€“3640.\n[26] HodLipsonOscarChang,LamprosFlokas.2020.PrincipledWeightInitializationforHypernetworks.InICLR.\n[27] SubhankarRoy,EvgenyKrivosheev,ZhunZhong,NicuSebe,andElisaRicci.2021.Curriculumgraphco-teachingfor\nmulti-targetdomainadaptation.InCVPR.\n[28] CristianoSaltori,FabioGalasso,GiuseppeFiameni,NicuSebe,FabioPoiesi,andElisaRicci.2023. Compositional\nSemanticMixforDomainAdaptationinPointCloudSegmentation. TPAMI(2023),1â€“14. https://doi.org/10.1109/\nTPAMI.2023.3310261\n[29] YangShu,ZhangjieCao,MingshengLong,andJianminWang.2019.Transferablecurriculumforweakly-supervised\ndomainadaptation.InAAAI.\n[30] TaoSun,ChengLu,TianshuoZhang,andHaibinLing.2022.SafeSelf-RefinementforTransformer-BasedDomain\nAdaptation.InCVPR.\n[31] ZhensuSun,LiLi,YanLiu,XiaoningDu,andLiLi.2022.Ontheimportanceofbuildinghigh-qualitytrainingdatasets\nforneuralcodesearch.InICSE.1609â€“1620. https://doi.org/10.1145/3510003.3510160\n[32] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerveJegou.2021.\nTrainingdata-efficientimagetransformersamp&distillationthroughattention.PMLR.\n[33] Cheng-HaoTu,Hong-YouChen,ZhedaMai,JikeZhong,VardaanPahuja,TanyaBerger-Wolf,SongGao,Charles\nStewart,YuSu,andWei-LunChao.2023.HolisticTransfer:TowardsNon-DisruptiveFine-TuningwithPartialTarget\nData. arXiv:2311.01420[cs.LG] https://arxiv.org/abs/2311.01420\n[34] AlexJinpengWang,YixiaoGe,RuiYan,GeYuying,XudongLin,GuanyuCai,JianpingWu,YingShan,XiaohuQie,\nandMikeZhengShou.2023.AllinOne:ExploringUnifiedVideo-LanguagePre-training.CVPR(2023).\n[35] FanWang,ZhongyiHan,YongshunGong,andYilongYin.2022.ExploringDomain-InvariantParametersforSource\nFreeDomainAdaptation.InCVPR.\n[36] HaixinWang,JinanSun,XiangWei,ShikunZhang,ChongChen,Xian-ShengHua,andXiaoLuo.2023. DANCE:\nLearningADomainAdaptiveFrameworkforDeepHashing.InWWW.3319â€“3330.\n[37] RuiWang,ZuxuanWu,ZejiaWeng,JingjingChen,Guo-JunQi,andYu-GangJiang.2023.Cross-DomainContrastive\nLearningforUnsupervisedDomainAdaptation.TMM25(2023),1665â€“1673.\n[38] ZhouXian,ShamitLal,Hsiao-YuTung,EmmanouilAntoniosPlatanios,andKaterinaFragkiadaki.2021.HyperDynam-\nics:Meta-LearningObjectandAgentDynamicswithHypernetworks.InICLR.\n[39] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang.2017.VideoQuestion\nAnsweringviaGraduallyRefinedAttentionoverAppearanceandMotion.ACMMM.\n[40] JunXu,TaoMei,TingYao,andYongRui.2016.MSR-VTT:ALargeVideoDescriptionDatasetforBridgingVideoand\nLanguage.CVPR.\n[41] YikaiYan,ChaoyueNiu,RenjieGu,FanWu,ShaojieTang,LifengHua,ChengfeiLyu,andGuihaiChen.2022.On-Device\nLearningforModelPersonalizationwithLarge-ScaleCloud-CoordinatedDomainAdaption.InKDD.2180â€“2190.\n[42] CeyuanYang,YujunShen,ZhiyiZhang,YinghaoXu,JiapengZhu,ZhirongWu,andBoleiZhou.2023. One-Shot\nGenerativeDomainAdaptation.InICCV.\n[43] JihanYang,ShaoshuaiShi,ZheWang,HongshengLi,andXiaojuanQi.2023. ST3D++:DenoisedSelf-Trainingfor\nUnsupervisedDomainAdaptationon3DObjectDetection.TPAMI45,5(2023),6354â€“6371. https://doi.org/10.1109/\nTPAMI.2022.3216606\n[44] ShiqiYang,yaxingwang,kaiwang,ShanglingJui,andJoostvandeWeijer.2022.AttractingandDispersing:ASimple\nApproachforSource-freeDomainAdaptation.InNIPS,S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,and\nA.Oh(Eds.).\n[45] JiangchaoYao,FengWang,KunyangJia,BoHan,JingrenZhou,andHongxiaYang.2021.Device-CloudCollaborative\nLearningforRecommendation.InKDD.\n[46] JunjieYe,ChanghongFu,GuangzeZheng,DandaPaniPaudel,andGuangChen.2022.UnsupervisedDomainAdaptation\nforNighttimeAerialTracking.InCVPR.\n[47] LipingYi,XiaorongShi,NanWang,ZiyueXu,GangWang,andXiaoguangLiu.2023. pFedLHNs:Personalized\nFederatedLearningviaLocalHypernetworks.InICANN2023.\n[48] AoZhang,HaoFei,YuanYao,WeiJi,LiLi,ZhiyuanLiu,andTat-SengChua.2023.VPGTrans:TransferVisualPrompt\nGeneratoracrossLLMs.InNeurIPS,Vol.36.20299â€“20319.\n[49] ChengZhang,Tai-YuPan,TianleChen,JikeZhong,WenjinFu,andWei-LunChao.2022.LearningwithFreeObject\nSegmentsforLong-TailedInstanceSegmentation. arXiv:2202.11124[cs.CV] https://arxiv.org/abs/2202.11124\n[50] ChrisZhang,MengyeRen,andRaquelUrtasun.2020.GraphHyperNetworksforNeuralArchitectureSearch.InICLR.\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.\nBackpropagation-FreeMulti-modalOn-DeviceModelAdaptationviaCloud-DeviceCollaboration 111:17\n[51] JingyiZhang,JiaxingHuang,ZichenTian,andShijianLu.2022.SpectralUnsupervisedDomainAdaptationforVisual\nRecognition.InCVPR.\n[52] RuohanZhang,TianziZang,YanminZhu,ChunyangWang,KeWang,andJiadiYu.2023.DisentangledContrastive\nLearningforCross-DomainRecommendation.InInternationalConferenceonDatabaseSystemsforAdvancedApplications.\nSpringer.\n[53] YangZhang,PhilipDavid,HassanForoosh,andBoqingGong.2019.Acurriculumdomainadaptationapproachtothe\nsemanticsegmentationofurbanscenes.TPAMI42,8(2019),1823â€“1841.\n[54] YuyangZhao,ZhunZhong,ZhimingLuo,GimHeeLee,andNicuSebe.2022.Source-FreeOpenCompoundDomain\nAdaptationinSemanticSegmentation.TCSVT32,10(2022),7019â€“7032. https://doi.org/10.1109/TCSVT.2022.3179021\n[55] JikeZhong,Hong-YouChen,andWei-LunChao.2024.MakingBatchNormalizationGreatinFederatedDeepLearning.\narXiv:2303.06530[cs.LG] https://arxiv.org/abs/2303.06530\nJ.ACM,Vol.37,No.4,Article111.Publicationdate:August2018.",
    "pdf_filename": "Backpropagation-Free_Multi-modal_On-Device_Model_Adaptation_via_Cloud-Device_Collaboration.pdf"
}