{
    "title": "Retrieval-Augmented Personalization for Multimodal Large Language Models",
    "abstract": "specific information, we design a pipeline for data collec- Thedevelopmentoflargelanguagemodels(LLMs)hassig- tionandcreateaspecializeddatasetforpersonalizedtrain- nificantly enhanced the capabilities of multimodal LLMs ing of MLLMs. Based on the dataset, we train a series (MLLMs) as general assistants. However, lack of user- of MLLMs as personalized multimodal assistants. By pre- specific knowledge still restricts their application in hu- trainingonlarge-scaledataset, RAP-MLLMscangeneral- man’s daily life. In this paper, we introduce the Retrieval ize to infinite visual concepts without additional finetun- AugmentedPersonalization(RAP)frameworkforMLLMs’ ing. Our models demonstrate outstanding flexibility and personalization. Starting from a general MLLM, we turn generation quality across a variety of tasks, such as per- it into a personalized assistant in three steps. (a) Re- sonalizedimagecaptioning,questionansweringandvisual member: We design a key-value database to store user- recognition. The code, data and models are available at related information, e.g., user’s name, avatar and other https://hoar012.github.io/RAP-Project/. attributes. (b) Retrieve: When the user initiates a con- versation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The 1.Introduction inputqueryandretrievedconcepts’informationarefedinto MLLMs to generate personalized, knowledge-augmented Recently, the development of large language models responses. Unlikepreviousmethods,RAPallowsreal-time (LLMs)hassignificantlyenhancedtheirlanguageprocess- ing and generating capabilities [59]. Building on this ∗Equalcontribution †Correspondingauthor foundation, the integration of visual and textual ability 1 4202 voN 81 ]VC.sc[ 2v06331.0142:viXra",
    "body": "Retrieval-Augmented Personalization for Multimodal Large Language Models\nHaoranHao1,2∗,JiamingHan1∗,ChangshengLi3,Yu-FengLi2,XiangyuYue1†\n1MMLab,TheChineseUniversityofHongKong\n2NationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity\n3BeijingInstituteofTechnology\nPersonalized Captioning\nThis is <K>, User’s database\n<K> and <J> enjoying a relaxing afternoon at a\nShe lives in Korea. Remember trendy café.\nPersonalized Conversation\nThis is <J>,\nHe is <K>’sboyfriend. What is <K> doing?\n<J> <K> <K> is sitting at a table in a café, wearing a blue\nRetrieve\npolka-dot dress. She is holding a glass of a\npinkish drink with a straw and appears to be\nsipping from it.\nGenerate\nWhat’s the relationship between them?\n<J> is <K>’s boyfriend, based on the image, they\nRAP-LLaVA appear to be enjoying each other's company at a\ncafé, Their relaxed body language and the casual\nsetting imply a close and comfortable relationship.\nInput image\nFigure1.Introducesomeuser-specificconceptstoourRAP-LLaVA,itcanrememberthemandachieveexcellentperformanceinavariety\nofpersonalizedmultimodalgenerationtasks.\nconcepteditingviaupdatingtheexternaldatabase. Tofur-\nAbstract ther improve generation quality and alignment with user-\nspecific information, we design a pipeline for data collec-\nThedevelopmentoflargelanguagemodels(LLMs)hassig- tionandcreateaspecializeddatasetforpersonalizedtrain-\nnificantly enhanced the capabilities of multimodal LLMs ing of MLLMs. Based on the dataset, we train a series\n(MLLMs) as general assistants. However, lack of user- of MLLMs as personalized multimodal assistants. By pre-\nspecific knowledge still restricts their application in hu- trainingonlarge-scaledataset, RAP-MLLMscangeneral-\nman’s daily life. In this paper, we introduce the Retrieval ize to infinite visual concepts without additional finetun-\nAugmentedPersonalization(RAP)frameworkforMLLMs’ ing. Our models demonstrate outstanding flexibility and\npersonalization. Starting from a general MLLM, we turn generation quality across a variety of tasks, such as per-\nit into a personalized assistant in three steps. (a) Re- sonalizedimagecaptioning,questionansweringandvisual\nmember: We design a key-value database to store user- recognition. The code, data and models are available at\nrelated information, e.g., user’s name, avatar and other https://hoar012.github.io/RAP-Project/.\nattributes. (b) Retrieve: When the user initiates a con-\nversation, RAP will retrieve relevant information from the\ndatabase using a multimodal retriever. (c) Generate: The 1.Introduction\ninputqueryandretrievedconcepts’informationarefedinto\nMLLMs to generate personalized, knowledge-augmented Recently, the development of large language models\nresponses. Unlikepreviousmethods,RAPallowsreal-time (LLMs)hassignificantlyenhancedtheirlanguageprocess-\ning and generating capabilities [59]. Building on this\n∗Equalcontribution †Correspondingauthor foundation, the integration of visual and textual ability\n1\n4202\nvoN\n81\n]VC.sc[\n2v06331.0142:viXra\nTable 1. Comparison of Different Personalization Methods. RAP needs only 1 image with its personalized description, showing\noutstandingconvenienceandflexibilityinpracticalapplications.\nNumberofImage DataRequirementsforPersonalization Support\nMethod Positive Negative Caption Description Question-Answer Recognition Real-timeedit Text-onlyQA\nFine-tuning n - Yes Yes No No ✗ ✓\nMyVLM[2] n 150 Yes No Yes Yes ✗ ✗\nYo’LLaVA[31] n 200 No No Yes Yes ✗ ✓\nRAP(Ours) 1 - No Yes No No ✓ ✓\nthrough vision-language alignment brings powerful mul- modalretriever.(c)Generate:Theinputqueryandretrieved\ntimodal LLMs (MLLMs) [12, 15, 28, 32, 44, 50, 55]. concepts information are incorporated into the MLLM’s\nMLLMs have shown significant improvement in various input for personalized, knowledge-augmented generation.\ntasks, such as image description and question answering, RAPrequiresonlyoneimageperconceptwithitsbasicin-\nhighlightingtheirpotentialashuman’sassistants. However, formationforpersonalization. Itallowsuserstomakereal-\ntheirlackofuser-specificknowledgecontinuestolimittheir timeadjustmentstothemodel’soutputsbymodifyingtheir\neffectivenessaspersonalizedassistantsindailylife. personal databases, eliminating the need for retraining. A\nAqualifiedpersonalizedassistantfirstshouldbeableto moredetailedcomparisonispresentedinTable1.\nrecognizeandrememberuser-relatedconcepts,suchasthe Another significant challenge is the lack of large-scale\ndog named 〈Lala〉 adopted by the user. Although existing datasets for training MLLMs’ personalized generation ca-\nMLLMshavebeentrainedonlarge-scaledatasetsandpos- pabilities. To address this, we design a pipeline to col-\nsess strong recognition and classification capabilities, di- lect extensive training data and create a comprehensive\nrectlytransferringthisknowledgetoauser’spersonalcon- dataset, which enables to train MLLMs to effectively un-\ncepts remains challenging. For instance, current leading derstandandutilizeuser-relatedinformationforgeneration.\nMLLMs cannot remember your dog’s name, even if you Basedonthisdataset,wetrainLLaVA[28]andPhi3-V[35]\nhavementioneditbefore, andtheylackawarenessofyour as novel personalized assistants and evaluate their perfor-\nidentityandpreferences. Furthermore,theassistantshould mance across various tasks, including personalized image\ngenerateresponsestailoredtotheuser’spreferencesandre- captioning,questionanswering,andvisualrecognition.Ex-\nquirements. However,collectingextensivepersonaldatato perimentalresultsdemonstratethatourRAP-MLLMsexcel\ntrainauniqueassistantforeachuserisimpractical. inwiderangeofpersonalizedgenerationtasks,showcasing\nexcellentgenerationqualityandflexibility.\nToaddressthisissue,thepersonalizationofMLLMshas\nOurcontributionsaresummarizedasfollows:\nbecomeatopicofgrowinginterest,withseveralapproaches\n• WeproposetheRAPframeworkforMLLMs’person-\nalreadybeingproposed. MyVLM[2]utilizesexternalclas-\nalization,allowingmodelspre-trainedonourdatasetto\nsification heads to recognize specific concepts, and learns\nadapttodiverseusersandinfinitenewconceptswithout\nan embedding for each concept to personalize the outputs\nfurthertraining.\nofvisionlanguagemodels(VLMs)[23,28]. Anothercon-\n• We develop a pipeline for collecting large-scale data\ncurrentwork,Yo’LLaVA[31],learnsafewspecialtokensto\nand create a dataset specifically designed for the per-\nrepresenteachconcept. However,bothapproachesnecessi-\nsonalized training and evaluation of MLLMs. This\ntatecontinuouslearningandupdatingofthemodelasnew\ndatasetenablesustotrainaseriesofMLLMstofunc-\nconcepts emerge. This presents a challenge in dynamic,\ntionaspersonalizedassistants.\never-changing real-world scenarios, where the computing\npower of users’ personal devices is often limited, and all • Our models demonstrate exceptional performance\ndatamustbestoredlocallyforprivacyconcerns. across various personalized multimodal generation\ntasks,includingimagecaptioningandquestionanswer-\nTo address these challenges, we propose the Retrieval\ning. Additionally, they exhibit a strong capability to\nAugmentedPersonalization(RAP)framework,designedto\nrecognizepersonalconceptswithinimages.\nallow MLLMs to update their supported concepts without\nadditional training. Specifically, RAP works in three key 2.RelatedWork\nsteps. (a)Remember: RAPincludesadesigneddatabaseto\nhelpremembereachconceptviastoringitsimageandbasic Multimodal Large Language Models. Recently, numer-\ninformation,e.g.,name,avatarandotherattributes. (b)Re- ousadvancedlargelanguagemodels(LLMs)[1,8,42,43,\ntrieve: When a user initiates a conversation, RAP will re- 56]havebeenproposed, showingremarkableperformance\ntrieverelevantinformationfromthedatabaseusingamulti- inaddressingawiderangeoftasks. Therapiddevelopment\n2\nof LLMs has led to the emergence of multimodal LLMs dings to capture finer-grained relevance between queries\n(MLLMs)[12,15,28,32,44,55,60],whichexcelingen- and documents, achieving significant improvement on the\neralvisualunderstandingandcomplexreasoningtasks. For RA-VQA setting. While existing methods primarily en-\ninstance,LLaVA[27,28]andMiniGPT-4[60]alignvisual hance models’ performance by retrieving from external\nand language modalities through visual instruction tuning, knowledge bases, few of them consider the personaliza-\nshowcasing impressive capabilities in multimodal conver- tion task. Although RAG has been applied to image gen-\nsations. GPT4RoI[57]andRegionGPT[13]enhancefine- eration[4,54]andimagecaptioning[24,34], thereiscur-\ngrainedunderstandingandreasoningforspecificregionsby rentlynoexistingworkfocusingonpersonalizingMLLMs\ntraining on region-level instruction datasets. Despite these viaRAG,tothebestofourknowledge.\nadvancementsintaskssuchasimagecaptioningandques-\ntionanswering,thelackofuser-specificknowledgerestricts 3.RetrievalAugmentedPersonalization\nthe generation of personalized content. In this work, we\nExistingMLLMstypicallyalignothermodalitieswithlan-\nfocus on the personalization of MLLMs, enabling them to\nguage. For instance, LLaVA [28] projects visual tokens\nremember and understanduser-specific concepts, and gen-\ninto text space, and then generates subsequent tokens us-\neratepersonalizedcontenttailoredtouser’spreferences.\ning an LLM. While these MLLMs perform well in vari-\nPersonalization of MLLMs. In the realm of artificial in- ous tasks, the lack of memory and comprehension of per-\ntelligence,personalizationtypicallyreferstotheprocessof sonalconceptshinderseffectiveuser-specificresponses. In\ntailoringasystem,application,ormodeltomeettheindivid- this work, we mainly focus on personalizing MLLMs to\nual needs and preferences [41, 45, 46, 49]. Substantial ef- generatetailoredlanguageresponses,suchascreatingper-\nfortshavebeenmadetogenerateimagesofuser’spersonal sonalized captions for user’s images and answering ques-\nobjects or in certain context [10, 14, 21, 22, 37, 40, 48]. tions about personal concepts. In this section, we detail\nFor example, Dreambooth [37] employs transfer learning the implementation steps of our proposed Retrieval Aug-\nin text-to-image diffusion models via fine-tuning all pa- mentedPersonalization(RAP)framework. Unlikeprevious\nrameters for new concepts. In this paper, we mainly aim approaches that usually necessitate additional data collec-\nat enabling MLLMs to remember and understand user- tion and further training to learn new concepts, after pre-\nspecificconcepts, andgeneratepersonalizedlanguageout- trainingonourdataset, RAP-MLLMscanadapttodiverse\nputs. Thereareseveralworksfocusingonthepersonaliza- usersandinfinitenewconceptswithoutfurthertraining. In\ntionofMLLMs,amongwhichthemostrelevantworksare section3.1,wepresenttheRAPframeworkthatisapplica-\nMyVLM[2]andYo’LLaVA[31]. MyVLMintroducesthe bletovarioustypesofMLLMs,andtheninsection3.2,we\ntask of personalizing VLMs. It utilizes external classifi- providedetailsoftheproposeddataset.\ncation heads to recognize specific concepts, and learns an\nembedding for each concept to personalize the outputs of 3.1.RAPFramework\nVLMs. Yo’LLaVA personalizes LLaVA by extending its\nRAPworksinthreesteps: Remember,RetrieveandGener-\nvocabulary and learning specific tokens for each concept.\nate. AnoverviewoftheframeworkisshowninFigure2.\nHowever, both approaches require continuous model up- Remember. The premise of personalization is that the\ndates as new concepts emerge, which presents challenges\nmodelcanrememberpersonalconceptsandrelevantinfor-\nin dynamic real-world applications. In this work, we pro- mation, suchasthedognamed〈Lala〉adoptedby〈A〉. To\npose RAP framework for the personalization of MLLMs, facilitatethis,weconstructadatabaseMtostorepersonal\nenablingmodelspre-trainedonourdatasettocontinuously concepts,whichcomprisesanavatarI ,anamealongwith\nj\nupdatingsupportedconceptswithoutfurtherfine-tuning. a brief description T for each concept. The key k for\nj i\nRetrievalAugmentedGeneration. Retrieval-basedmeth- each concept in the database is its visual feature, obtained\nods for incorporating external knowledge have proven by feeding its image I into a pre-trained image encoder\nj\neffective in enhancing generation across a variety of E(·). Examples of our database are presented in Figure 2.\nknowledge-intensivetasks[3,11,26,47,51,58]. DPR[19] When a user initiates a conversation, the input can be rep-\nintroduces Dense Passage Retrieval, marking a shift from resentedasQ = (X ,X ),whichmayincludebothimage\nv q\nsparsetodenseretrievaltechniques.Later,MuRAG[5]pro- X andsometextualinstructionsX .Thefirststepinvolves\nv q\nposes to use multimodal knowledge to augment language identifying possible concepts within the input image that\ngeneration. Self-Rag[3]introducesspecialtokenstomake havebeenpreviouslystoredinthedatabase.Previousmeth-\nretrievaladaptiveandcontrollable. ERAGent[41]presents ods[2]typicallyneedtolearnanexternalclassifiertodeter-\na comprehensive system for retrieval-augmented language minewhetheraconceptappearsintheinputimage, which\nmodels. With the advancements in MLLMs, RAG has requiresasubstantialamountoftrainingdataandcanonly\nbeen widely applied to multimodal generative tasks. For applytospecificconcept.Toenhancethegeneralizabilityof\ninstance, FLMR [25] employs multi-dimensional embed- therecognitionprocess, wedonotconstructspecificmod-\n3\nCrop region of interest\nName: <A>\nA young woman who loves dog. Age: 22.\nRetriever Name: <Lala>\nA German Shepherd dog. User’s database\nOpen world … …\ndetector User’s input Name: <H>\nA man from America. Hobby: Traveling.\nReal-time update\nUser-related information\nInstruction: Please give Name: <A>\nName: <Lala>\na caption of this image. A young woman who\nA German Shepherd dog.\nloves dog. Age: 22\nProj. Proj. Proj.\nMultimodal Large Language Model\nResponse: <A> enjoys a sunny grooming session with her dog <Lala> in the countryside.\nFigure2.Retrieval-AugmentedPersonalizationFramework.Region-of-interestdetectedbyanopenworlddetectorareusedtoretrieve\nconceptsfromthedatabase.TheimagesandinformationoftheretrievedconceptsarethenintegratedintotheinputfortheMLLM.\nules for each concept. Instead, we employ a universal de- parametersθ.GiventhelengthLoftheoutputsequence,the\ntectionmodel,suchasYOLO[36]andYOLO-World[7],as probabilityofthetargetanswerX iscomputedas:\na\nrecognitionmodelR(·).GiventhepredefinedsettingP that\nL\nspecifieswhichcategoriesshouldberemembered,theuser’s (cid:89)\np(X )= p (X |X ,X ,M ,···M ,X ). (1)\nregion-of-interestcanbeacquiredviaX =R(X ,X |P). a θ a,i v q 1 K a,<i\nu v q\ni=1\nRetrieve.Identifiedregion-of-interestwillbeusedasquery\n3.2.PersonalizationDataset\ntoretrievefromthedatabase. Foreachrecognizedcompo-\nnent Xi u, we feed the image crop into the image encoder Most existing MLLMs struggle to generate personalized\nE(·) to get its visual feature v i = E(Xi u), which is a n- outputsevenifadditionalconceptinformationisprovided,\ndimensional vector. Then we calculate the euclidean dis- andthereiscurrentlynolarge-scaledatasetforpersonalized\ntance between the visual feature and each key k j ∈ M, training of MLLMs. To this end, we design a pipeline for\nwhichiscalculatedasDist(v i,k j) = ∥v i−k j∥. TheTop- datacreationandcurateanoveldatasetspecificallyforthe\nKimage-textpairs{(I 1,T 1),(I 2,T 2),···(I K,T K)}with personalizedtrainingofMLLMs. WeuseGemini-1.5[12]\nthelowestdistancesareselectedfromthedatabase.Wealso togenerateannotationsforourdataset. Anoverviewofour\nintroduceretrievalusingconceptnames,suchas⟨sks⟩fora pipelineanddatasetispresentedinFigure3.\nunique concept. When the user mentions the name of an The first component of our dataset is dedicated to vi-\nobject documented in the database, our model retrieves its sual grounding. In this task, a MLLM is trained to deter-\nrelatedinformationfromthedatabase.Thisalsoenablesour mine whether a specific concept is in an image, particu-\nmodeltorespondtotext-onlyquerieseffectively. larlyidentifyingifthepersonorobjectinareferenceimage\nGenerate.EachpairM =(I ,T )providesrelatedinfor- appears in the given image. When a positive match is de-\nj j j\nmation about a user’s personal concept and will be incor- tected, we also require the model to provide the bounding\nporated into the input of the MLLM. Take LLaVA [28] as box for the identified concept. For single-concept ground-\nanexample. TheimageI isfirstencodedbyapre-trained ing, we primarily use the RefCOCO dataset [20]. Based\nj\nvision encoder, such as CLIP [33], to obtain their visual on RefCOCO’s annotations, we crop target concepts from\ntokensZ . Theseimagetokensarethenprojectedbyapro- the images and assign names to them, which serve as ref-\nj\njectorintolanguagetokensHv,whichcouldbeunderstood erences for specific concepts. We then query Gemini to\nj\nbythelanguagemodel. Simultaneously,correspondingtext generate concise descriptions about properties of the con-\ninformation T are transformed into text tokens Hq . This cepts in these cropped regions, by which we construct a\nj j\nprocess is also applied to both X and X . All these to- large-scaledatabaseincludingnumerousdifferentconcepts.\nv q\nkens are incorporated into the MLLM’s input to generate The training data pairs images and these descriptions as\nlanguageresponse. Duringtraining,wekeepparametersof queries and the corresponding bounding boxes as outputs.\nboththedetectorandretrieverfrozen,justtraintheMLLM’s However, data generated in this way is insufficient to sim-\n4\nRotate Flip Novel view\nCrop Augment\n3D model\nThis is <D>, give a description of him.\nConcept Information • A man wearing a bright orange T-shirt… Background\nInstruction type 1: Visual Grounding\nInstruction:Specify the rectangular boundaries of <D> in the image.\n<N> Answer: [0.12, 0.15, 0.44, 0.94]\nInstruction type 2: Image Captioning & Description\nInstruction: Please give a caption of the image.\nAnswer: <D> is playing ultimate frisbee, he's getting ready to pick up the frisbee.\nNoiseConcept The game is played on a field of grass.\nInstruction type 3: Question Answering:\nInstruction: What color is <D>’s shorts?\nAnswer: Black.\nFigure3. OurPipelineforDataCollection. Wefirstcropthetargetconceptfromtheimagebasedonthedatasetannotationsandthen\nqueryGemini[12]togenerateitspersonalizeddescription. Wealsoapplydataaugmentationtodiversifythesecroppedimages. Thenwe\ncombinethemwiththeoriginalimagetoderiveaseriesofinstructionsandanswersfromGemini.Whennoiseconceptsareincludedinthe\nadditionalinformation,theanswerremainsunchanged,helpingtotraintheMLLMs’abilitytofilteroutirrelevantconcepts.\nulate the complexity of real-world recognition, especially randomrotationsandflipsonthecroppedimages,aswellas\nwhenthetargetconceptinthereferenceandinputimageis generatingnovelviewsoftheconceptsbydiffusionmodels.\ncaptured from different perspectives. To address this, we Specifically, we use Inpaint-Anything [52] to separate the\nincorporate the ILSVRC2015-VID video object detection foreground from the background, and use Wonder3D [30]\ndataset [38], TAO [9] and CustomConcept101 [21] to en- andSiTH[16]tosynthesizenovelviewsofforegroundob-\nrich our dataset. For multi-object grounding, we use the ject or person respectively. Finally, we combine these ele-\nObject365dataset[39]toconstructourtrainingdata. mentstogenerateimagesofthetargetconceptfromdiffer-\nThe second component of our dataset is designed for entperspectives.\ninstruction following. This section includes training data To support multi-concept personalized generation, it is\nfor tasks such as image captioning, image description and necessary to retrieve multiple potential concepts from the\nquestionanswering. Fortheimagecaptioninganddescrip- database. Inthegenerationstep,theMLLMmustprioritize\ntion data, we provide cropped images of target concepts, accurate and contextually relevant information. Consider-\naccompanied by their names and related information from ingthatretrievalresultscanbeinaccurate,potentiallylead-\nthe large-scale database, then query Gemini to generate a ingtounreasonableanswers,weconstructnegativesamples\ncaptionordescriptionthatreflectstheconceptsdepictedin byaddingnoiseconceptstotheinputwhilepreservingthe\ntheentireimage. Forquestionanswering,wefirstdesigna originaloutput. Thisapproachtrainsthemodel’sdiscrimi-\nsetofseedquestionstoserveasexamples. Theseexamples nationability. ByexposingtheMLLMtobothrelevantand\nareusedtoprompttheannotator, Gemini, togeneratenew irrelevant information during training, it learns to discern\nquestions and corresponding answers. This iterative pro- andfilteroutnoiseconcepts, therebyenhancingitsrobust-\ncessfacilitatesthecreationofarichanddiversecollection nessduringinference. Additionally,weincludeasubsetof\nofconversationsthatMLLMscanlearnfrom. Weconstruct the LLaVA-Instruct-665k visual instruction dataset [27] to\nsuch data using RefCOCO [20], Object365 [39], TAO [9] retaingeneralknowledgefromtheoriginalMLLM.Further\nandCustomConcept101[21]dataset. detailsaboutourdatasetcanbefoundinAppendixD.\nToenhancealignmentwithreal-worldscenarios,itises-\nsential to collect data featuring the same identity in vari- 4.Experiment\nous environments. Thus, we also include multiple images\naboutthesameindividualfromtheCelebAdataset[29]and Implementation Details. We conduct experiments on\nproduce question answering data about the individual. To LLaVA-1.5-13B [27] and Phi3-V-3.8B [35], resulting in\nfurther diversify the dataset, we apply image editing tech- two personalized MLLMs, RAP-LLaVA and RAP-Phi3-\nniques for data augmentation. This includes performing V. We select YOLO-Worldv2 [7] as the detector and con-\n5\nTable2.QualitativeComparisononImageCaptioning.Imageexamplesoftargetconceptsareshownintheleftandcaptionsgenerated\nareshownintheright.Weusegreentexttodenotecorrecttargetconcepts.\nImage Caption\nLLaVA:Amanissittingatatablewithadog,andtherearewineglassesand\naforkonthetable.\nLLaVA-LoRA: 〈collie dog〉 looking pleased as she shares a meal with her\nowner.\nMyVLM: 〈my dog〉 positioned on a chair by a black table, holding a wine\nglassinherhand. Awhitedogsitsonthefloorbesideher...\nRAP-LLaVA(Ours): 〈my dog〉 is a very good boy, and he loves to sit at a\nmy dog tablewithhisowner. Theyareenjoyingameal.\nLLaVA: A man and a woman are standing in a kitchen, preparing food to-\ngether. The woman is cutting lettuce on a cutting board, while the man\nwatchesher. Thereareseveraltomatoes...\nH LLaVA-LoRA:〈H〉and〈K〉arepreparingamealtogether.\nMyVLM:〈T〉andherfriend〈H〉arelookingveryseriousastheytakeinthe\nscenery.\nT\nRAP-LLaVA(Ours): 〈H〉ishelping〈T〉prepareasaladinthekitchen.\nPhi3-V: A group of stuffed animals, including a blue one, are sitting on a\nB\nblacksurface.\nLLaVA-LoRA:〈B〉,〈G〉and〈W〉arehappilyexploringthegrassland.\nG\nMyVLM:〈G〉andhiscrewarealwaysreadytojumpintoanewadventure.\nRAP-Phi3-V(Ours): 〈W〉ishangingoutwith〈G〉and〈B〉onthelawn. They\nW arehavingagreattimeplaying!\nstructamultimodalretrieverusingFacebookAISimilarity presentsadditionalchallengesforpersonalization.\nSearch (FAISS) [18], employing a pre-trained CLIP ViT- Settings. We compare our models with MyVLM [2] and\nL/14-336 [33] as the visual encoder. Due to the context fine-tuningbasedmethodLLaVA-LoRA[17]. ForLLaVA-\nlengthlimitationofthebackbonelanguagemodel,forRAP- LoRA and MyVLM, the training dataset contains 1 image\nLLaVA and RAP-Phi3-V, we retrieve the 2 and 3 different accompaniedby5captionsforeachconcept. ForLLaVA-\nconceptswiththehighestsimilarity,respectively. Moreim- LoRA,wetrainitwithcaptionsofthetrainingimagesfor3\nplementationdetailscanbefoundinAppendixC. epochs, applying low-rank adapters [17] and the same hy-\nTraining. In the training phase, we skip the recognition perparametersasourmodels. ForMyVLM,followingtheir\nand retrieval procedures, instead perform instruction tun- training process, we first train the classification head with\ning to train the MLLMs. We adhere to most settings from the positive and 150 negative images, then train the cor-\nthe original experiment of LLaVA [27], except for using a responding concept embedding with the provided captions\nmaximum learning rate of 1e-4 and training for 1 epoch. foreachconcept. Forourmodels, weconstructadatabase\nWeemploylow-rankadapters[17]toreducethenumberof whereeachconceptisrepresentedbyacroppedimageand\ntrainableparameters,andtrainourmodelson8A100GPUs a text description. Details of our database could be found\nwithavalidbatchsizeof64. inAppendixG.Allremainingimagesareusedastestsam-\nples. This evaluation process is repeated three times with\n4.1.PersonalizedImageCaptioning\ndifferentseeds,resultinginatotalof1,182imagesusedfor\nIn this section, we evaluate our models on generating per- evaluation,andwereporttheaverageresults.\nsonalizedimagecaptionswithuser’sspecificconcepts. We Qualitative Comparison. In Table 2, we present image\nextendthedatasetintroducedbyMyVLM[2]viaadding16 captionsgeneratedbydifferentmethodstomakeacompari-\nnewconcepts,includingbothobjectsandhumans,forming son.WhileLLaVA[27]andPhi3-V[35]generallyprovides\n8 concept pairs that appear together. For each pair, there briefandclearcaptionsformosttestimages,itslackofun-\nare8-13imagesusedfortesting. Thismulti-conceptsetting derstanding of the user’s specific concepts restricts it from\n6\nTable3.QuantitativeEvaluationonImageCaptioning.WereportRecall, Figure4. Performanceundervaryingnumberofper-\nPrecisionandF1-scoreinthetable,thebestresultineachmetricisboldand sonalizedconcepts.\nthesecondisunderlined.\nMyVLM RAP-Phi3-V\nLLaVA-LoRA RAP-LLaVA\nMethod LLM Recall Precision F1-score\n95\nLLaVA[27]+Retriever Vicuna-13B 1.260 48.76 2.450\n90\nLLaVA-LoRA[17] Vicuna-13B 82.97 93.28 87.82\nMyVLM-LLaVA[2] Vicuna-13B 84.65 86.37 85.50\n85\nRAP-LLaVA Vicuna-13B 93.51 96.47 94.97\nRAP-Phi3-V Phi3-V-3.8B 88.14 95.10 91.49 50 100 150 200 250 300\nNumberofConcepts\nTable4.QuantitativeEvaluationonQuestionAnsweringandVisualRecognition.Thebestresultineachsettingisboldandthesecond\nisunderlined.EvaluationresultsofGPT-4V[32]arealsoprovidedasreference.Weightedresultsarecomputedasarithmeticmeans.\nQuestionAnswering VisualRecognition\nMethod LLM Train #Image\nVisual Text-only Weighted Positive Negative Weighted\nGPT-4V[32]+Prompt GPT-4V ✗ 1 0.866 0.982 0.924 0.809 0.992 0.901\nGPT-4V[32]+Prompt GPT-4V ✗ 5 0.887 0.987 0.937 0.851 0.998 0.925\nLLaVA[27] Vicuna-13B ✗ - 0.899 0.659 0.779 0.000 1.000 0.500\nLLaVA[27]+Retriever Vicuna-13B ✗ 1 0.912 0.863 0.887 1.000 0.025 0.513\nLLaVA-LoRA[17] Vicuna-13B ✓ 1 0.900 0.583 0.741 0.988 0.662 0.825\nLLaVA-LoRA[17] Vicuna-13B ✓ 5 0.935 0.615 0.775 0.997 0.444 0.721\nMyVLM-LLaVA[2] Vicuna-13B ✓ 5 0.912 - - 0.994 0.845 0.919\nYo’LLaVA[31] Vicuna-13B ✓ 5 0.929 0.883 0.906 0.949 0.898 0.924\nRAP-LLaVA(Ours) Vicuna-13B ✗ 1 0.935 0.938 0.936 0.979 0.982 0.980\nRAP-Phi3-V(Ours) Phi3-V-3.8B ✗ 1 0.941 0.850 0.896 0.922 0.988 0.955\ngeneratingamorepersonalizedcaption. LLaVA-LoRAand theadvantagesofourRAP-MLLMsindataefficiency.\nMyVLM can generate personalized captions, however, the InfluenceofNumberofLearnedConcepts. Inreal-world\nlimitedtrainingdataoftenresultsinimpreciseoutputs,par- scenario, users’ personal databases typically expand over\nticularly noticeable when multiple concepts are present in time. Next, we evaluate the performance of various meth-\nthesameimage. Incontrast,ourmodelsproduceclearand ods with varying numbers of learned concepts. We extend\naccuratecaptionsbasedonthedatabasecontent,whichalso the database with hundreds of new concepts selected from\nensures the reliability of the outputs. Additional examples RefCOCO dataset [20], ensuring no overlap with the test\nofpersonalizedcaptionsgeneratedbythemodelscouldbe dataset. For LLaVA-LoRA and MyVLM, we provide im-\nfoundinAppendixE. agescontainingthetargetconceptsalongwiththeircaptions\nastrainingdata,andweassessthemodels’performanceon\nQuantitativeEvaluation. Weemployrecall,precisionand\nthe original test dataset. The results are presented in Fig-\nthe comprehensive metric F1-score as our evaluation met-\nure4.Morelearnedconceptsresultinincreasedrecognition\nrics. Recall is calculated as the percentage of correct oc-\nerrors,leadingtoadeclineinperformanceforeachmodel.\ncurrences of target concepts, while precision is the ratio\nOurRAP-MLLMsmaintainthehighestperformanceunder\nof correct concept names to the total number of concept\ndifferentsettings.\nnames presented. The experimental results are shown in\nTable 3. Notably, the classification heads of MyVLM ex-\n4.2.PersonalizedQuestionAnswering\nhibithighererrorrateswhenthenumberofpositiveimages\nis limited, leading to weaker performance. Even with ad- Settings. Inthissection,weevaluatedifferentmethodson\nditionalconceptinformationprovidedthroughretrieval,the the benchmark of personalized question answering intro-\nvanillaLLaVA[27]stillfailstoeffectivelyaccomplishthe duced by Yo’LLaVA [31], which contains both visual and\npersonalizedgenerationtask,underscoringthenecessityof text-onlyquestionsaboutuserconcepts. Foreachconcept,\ntheproposeddataset.Ourmodelsdemonstratesuperiorper- wegenerateadescriptiontoserveasitsinformationinour\nformanceinbothrecallandprecisionmetrics,highlighting database. For LLaVA-LoRA, we feed these descriptions\n7\nerocs-1F\nandcorrespondingimagestotrainthemodeltodescribethe N=45 N=300\nN=150 N=500\nconcept’sproperties.Additionally,weincorporatetext-only\nqueries and answers to enhance the model’s textual under-\nstanding. ThetrainingdatasetforYo’LLaVAandMyVLM 0.9\nconsistsof5positiveimageswithquestionansweringpairs\nand 200 negative images per concept. For GPT-4V [32],\n0.8\nimages and related concept information are provided as a\nsupplementary prompt. The evaluation metric is accuracy.\n0.7\nAdditionaldetailsareprovidedinAppendixC.\n1 2 3 4 5\nResults and Analysis. The experimental results are pro- K\nvidedinTable4. LLaVAandLLaVA-LoRAbothperform Figure 5. Performance of Our Retriever. Top-K recall rates\nwell in visual based question answering, because substan- undervaryingdatabasesizeN.\ntialinformationofthetargetconceptcanbeobtainedfrom\ntheimages. However,theirperformanceisquitepoorwhen Table5. Weevaluatemodel’sperformancewithperfectretrieval,\nimagesofthetargetconceptmentionedinthequestionare andtestcontributionsofeachdatasetcomponent.\nnot available. MyVLM performs well in visual question\nSetting Recall Precision F1-score\nansweringbutdoesnotsupporttext-onlyquestionanswer-\ning. Yo’LLaVAexcelsintext-onlyquestionanswering,but RAP-LLaVA 93.51 96.47 94.97\nits performance is still limited by the insufficient informa- Skipretrieval 96.16(+2.7) 100.0(+3.5) 98.04(+3.1)\ntion provided by the learned tokens of a concept. In con- -Textinformation 94.91(+1.4) 88.66(-7.8) 91.68(-3.3)\ntrast,ourmodelsdemonstratebalancedperformanceinboth -Dataaugmentation 89.25(-4.3) 98.01(+1.5) 93.42(-1.6)\nvisual and text-only question answering. By providing a -Negativesamples 95.74(+2.2) 58.21(-38.3) 72.40(-22.6)\nsingleimage,ourRAP-LLaVAsurpassesbaselinemethods\nand achieves performance comparable to that of GPT-4V.\nWepresentadditionalqualitativeresultsinTable13and14. andalargenumberofnegativeimages,whichposessignifi-\nVisual Recognition. We also evaluate the models’ recog- cantchallengesfordatacollection. Incontrast,ourRAPre-\nnitionabilitiesforamorecomprehensivecomparison. The quiresonly1imagewithitsrelatedinformationprovidedby\nMLLMsarerequiredtodeterminewhetherapersonalcon- theuser,achievingoutstandingperformanceacrossvarious\ncept is present in an image. We query them with ”Is 〈sks〉 personalizedgenerationtasks. Atthesametime, bymodi-\nin the image? Answer with a single word.”, where 〈sks〉 fyingimagesanddescriptionsinthedatabase,RAPenables\nis replaced by corresponding concept name. For positive real-time editing of personalized generation settings. We\nimages, the desired response is ”Yes” and ”No” for nega- presentexamplesofreal-timeconcepteditinginTable11.\ntive. Resultsshowthatwithoutknowledgeofpersonalcon-\n4.4.AblationStudy\ncepts, the vanilla LLaVA consistently outputs negative re-\nsponses. After training on target concepts, LLaVA-LoRA,\nRetriever. The recall rate of the retriever is crucial for a\nMyVLMandYoLLaVAtendtogivepositiveresponses,but\nRAG system. We first assess the retriever’s performance\nstruggletodifferentiatebetweensimilarconcepts,resulting\non the personalized captioning dataset. We use the detec-\nin weaker performance on negative images. Our models\ntion model to identify potential concepts and retrieve the\ndemonstrate exceptional performance in both positive and\nK concepts with the highest similarity from the database.\nnegative scenarios, achieving the best overall results. We\nTheTop-KrecallratesforvaryingvaluesofKanddatabase\npresentadditionalexamplesinTable19and20.\nsizes N are presented in Figure 5. Results indicate that as\nthedatabasesizeincreases,theretriever’sperformancede-\n4.3.CostofPersonalization\nclines, while a larger K generally enhances the recall rate.\nWefurthercomparethecostsofpersonalization. Asshown Notably,evenwith500personalconceptstoremember,the\nin Table 1, existing methods usually struggle with contin- Top-5recallrateisstillabletosurpass90%,whichguaran-\nuous updates or have high demands for training data. For teestheeffectivenessofourRAPframework.\nfine-tuning based method like LLaVA-LoRA, while they Generation Ability of MLLM. We skip the recognition\ncanachievesatisfactoryperformance,fine-tuningthemodel andretrievalprocesses,providingtheMLLMwithrelevant\neachtimeanewconceptemergesincurssubstantialcompu- informationofeachconceptpresentintheimagetoevaluate\ntational costs. MyVLM and Yo’LLaVA learn an embed- thegenerationcapabilityofthetrainedMLLM.Theresults,\ndingorsomenewtokenstorepresentthenewconceptwith- shown in Table 5, indicate that when relevant concept in-\noutupdatingthepre-trainedMLLM’sparameters,however, formation is supplied, our RAP-LLaVA achieves superior\nthey require multiple labeled images of the target concept generationperformance,obtaining100%precisionwithout\n8\netaRllaceR\noutputtingirrelevantconceptsaswellasahigherrecallrate. ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,\nTextInformation. Weremoveadditionaltextinformation November28-December9,2022,2022. 3\nto examine its impact on personalization. The results are [5] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nshowninTable5and7. Thisadditionaltextprovidesextra WilliamWCohen. Murag:Multimodalretrieval-augmented\ninformation that helps the model understand specific con- generatorforopenquestionansweringoverimagesandtext.\narXivpreprintarXiv:2210.02928,2022. 3\ncepts, effectively improving the accuracy of the answers\n[6] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-\nand benefiting personalized generation. Furthermore, it is\nravit Changpinyo, Alan Ritter, and Ming-Wei Chang.\nespeciallyhelpfulforquestionanswering,asitcanprovide\nCan pre-trained vision and language models answer vi-\ninformationnotdirectlyobtainablefromtheimage,suchas\nsual information-seeking questions? arXiv preprint\nauser’sageandhobbies,orapet’spreferredfood,asillus-\narXiv:2302.11713,2023. 12\ntratedintheexampleinTable11.\n[7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xing-\nDataset Composition. We conduct experiments to assess\ngang Wang, and Ying Shan. Yolo-world: Real-time\ncontributionofeachcomponentinourdataset. First,were- open-vocabulary object detection. In Proceedings of the\nmove data generated through data augmentation and train IEEE/CVF Conference on Computer Vision and Pattern\nthe original LLaVA. The results displayed in Table 5 indi- Recognition,pages16901–16911,2024. 4,5,12\ncateaobviousdecreaseintherecallmetricforimagecap- [8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao\ntioning,resultinginloweroverallperformance. Wefurther Wu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao\nexcludeconstructednegativesamplesfromthedatasetand Zhuang,JosephEGonzalez,etal. Vicuna: Anopen-source\nretrain the model, then we find that it performs poorly on chatbot impressing gpt-4 with 90%* chatgpt quality. See\nprecisionmetric. Thissuggestsadiminishedabilitytodis- https://vicuna.lmsys.org(accessed14April2023), 2(3):6,\n2023. 2\ncriminateagainstnoisyconceptsnotpresentintheimage.\n[9] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia\nSchmid, and Deva Ramanan. Tao: A large-scale bench-\n5.Conclusion\nmark for tracking any object. In Computer Vision–ECCV\n2020:16thEuropeanConference,Glasgow,UK,August23–\nInthispaper,weintroducetheRAPframeworkforperson-\n28,2020,Proceedings,PartV16,pages436–454.Springer,\nalizing MLLMs. This framework enables MLLMs to un-\n2020. 5,14\nderstand infinite user-specific concepts, generate personal-\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nized captions and respond to user-related queries. To en-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nhance the quality of the generated content and better align\nOr. An image is worth one word: Personalizing text-to-\noutputs with user’s configuration, we curate a large-scale image generation using textual inversion. arXiv preprint\ndatasetforpersonalizedtrainingofMLLMsandtrainase- arXiv:2208.01618,2022. 3\nries of MLLMsto function as personalizedassistants. Ex- [11] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jin-\nperimental results show that RAP-MLLMs achieve excep- liu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.\ntionalperformanceinvariouspersonalizedgenerationtasks, Retrieval-augmentedgenerationforlargelanguagemodels:\nwhileallowingreal-timeadjustmentstogenerationsettings. Asurvey. arXivpreprintarXiv:2312.10997,2023. 3\n[12] Gemini-Team. Gemini 1.5: Unlocking multimodal under-\nReferences standingacrossmillionsoftokensofcontext. arXivpreprint\narXiv:2403.05530,2024. 2,3,4,5\n[1] MarahAbdin,JyotiAneja,HanyAwadalla,AhmedAwadal- [13] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin\nlah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 Liu. Regiongpt: Towards region understanding vision lan-\ntechnical report: A highly capable language model locally guagemodel. InProceedingsoftheIEEE/CVFConference\nonyourphone. arXivpreprintarXiv:2404.14219,2024. 2 onComputerVisionandPatternRecognition,pages13796–\n[2] YuvalAlaluf,EladRichardson,SergeyTulyakov,KfirAber- 13806,2024. 3\nman, and Daniel Cohen-Or. Myvlm: Personalizing vlms [14] CusuhHam,MatthewFisher,JamesHays,NicholasKolkin,\nforuser-specificqueries. arXivpreprintarXiv:2403.14599, Yuchen Liu, Richard Zhang, and Tobias Hinz. Personal-\n2024. 2,3,6,7 ized residuals for concept-driven text-to-image generation.\n[3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and In Proceedings of the IEEE/CVF Conference on Computer\nHannaneh Hajishirzi. Self-rag: Learning to retrieve, gen- VisionandPatternRecognition,pages8186–8195,2024. 3\nerate, and critique through self-reflection. arXiv preprint [15] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang,\narXiv:2310.11511,2023. 3 Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xi-\n[4] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas angyuYue. Onellm: Oneframeworktoalignallmodalities\nMu¨ller, and Bjo¨rn Ommer. Retrieval-augmented diffusion withlanguage. InProceedingsoftheIEEE/CVFConference\nmodels. InAdvancesinNeuralInformationProcessingSys- onComputerVisionandPatternRecognition,pages26584–\ntems35:AnnualConferenceonNeuralInformationProcess- 26595,2024. 2,3\n9\n[16] IHo,JieSong,OtmarHilliges,etal. Sith: Single-viewtex- Retrieval-augmenteddualinstructiontuning. arXivpreprint\ntured human reconstruction with image-conditioned diffu- arXiv:2310.01352,2023. 3\nsion. InProceedingsoftheIEEE/CVFConferenceonCom- [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nputerVisionandPatternRecognition,pages538–549,2024. Improvedbaselineswithvisualinstructiontuning,2023. 3,\n5 5,6,7,12,13,14\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- [28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\nZhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen. Visualinstructiontuning,2023. 2,3,4\nLora: Low-rank adaptation of large language models. In\n[29] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nTheTenthInternationalConferenceonLearningRepresen-\nDeeplearningfaceattributesinthewild. InProceedingsof\ntations,ICLR2022,VirtualEvent,April25-29,2022.Open-\nInternationalConferenceonComputerVision(ICCV),2015.\nReview.net,2022. 6,7,12\n5,14\n[18] Jeff Johnson, Matthijs Douze, and Herve´ Je´gou. Billion-\n[30] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nscale similarity search with gpus. IEEE Trans. Big Data,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\n7(3):535–547,2021. 6,12\nMarcHabermann,ChristianTheobalt,etal. Wonder3d:Sin-\n[19] VladimirKarpukhin,BarlasOguz,SewonMin,PatrickS.H. gleimageto3dusingcross-domaindiffusion. InProceed-\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen- ingsoftheIEEE/CVFConferenceonComputerVisionand\ntauYih. Densepassageretrievalforopen-domainquestion PatternRecognition,pages9970–9980,2024. 5\nanswering. InProceedingsofthe2020ConferenceonEm-\n[31] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh\npirical Methods in Natural Language Processing, EMNLP\nOjha, and Yong Jae Lee. Yo’llava: Your personalized lan-\n2020,Online,November16-20,2020,pages6769–6781.As-\nguageandvisionassistant.arXivpreprintarXiv:2406.09400,\nsociationforComputationalLinguistics,2020. 3\n2024. 2,3,7,13\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\n[32] OpenAI. Gpt-4 technical report. arXiv preprint\nTamaraL.Berg. Referitgame: Referringtoobjectsinpho-\narXiv:2303.08774,2023. 2,3,7,8\ntographsofnaturalscenes. InProceedingsofthe2014Con-\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nferenceonEmpiricalMethodsinNaturalLanguageProcess-\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\ning, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nmeeting of SIGDAT, a Special Interest Group of the ACL,\nKrueger, and Ilya Sutskever. Learning transferable visual\npages787–798.ACL,2014. 4,5,7,14\nmodelsfromnaturallanguagesupervision. InProceedings\n[21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nofthe38thInternationalConferenceonMachineLearning,\nShechtman,andJun-YanZhu. Multi-conceptcustomization\nICML 2021, 18-24 July 2021, Virtual Event, pages 8748–\noftext-to-imagediffusion. InProceedingsoftheIEEE/CVF\n8763.PMLR,2021. 4,6,12\nConference on Computer Vision and Pattern Recognition,\n[34] Rita Ramos, Desmond Elliott, and Bruno Martins.\npages1931–1941,2023. 3,5,14\nRetrieval-augmented image captioning. arXiv preprint\n[22] DongxuLi,JunnanLi,andStevenHoi. Blip-diffusion: Pre-\narXiv:2302.08268,2023. 3\ntrainedsubjectrepresentationforcontrollabletext-to-image\ngenerationandediting.AdvancesinNeuralInformationPro- [35] HanoonaRasheed,MuhammadMaaz,SalmanKhan,andFa-\ncessingSystems,36,2024. 3 had S. Khan. Llava++: Extending visual capabilities with\nllama-3andphi-3,2024. 2,5,6\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with [36] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nfrozen image encoders and large language models. In In- Farhadi. Youonlylookonce: Unified,real-timeobjectde-\nternational conference on machine learning, pages 19730– tection. InProceedingsoftheIEEEconferenceoncomputer\n19742.PMLR,2023. 2 visionandpatternrecognition,pages779–788,2016. 4\n[24] Jiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, and Hideki [37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nNakayama. Evcap: Retrieval-augmented image captioning MichaelRubinstein,andKfirAberman. Dreambooth: Fine\nwithexternalvisual-namememoryforopen-worldcompre- tuning text-to-image diffusion models for subject-driven\nhension. In Proceedings of the IEEE/CVF Conference on generation. In IEEE/CVF Conference on Computer Vi-\nComputer Vision and Pattern Recognition, pages 13733– sionandPatternRecognition, CVPR2023, Vancouver, BC,\n13742,2024. 3 Canada, June 17-24, 2023, pages 22500–22510. IEEE,\n[25] WeizheLin,JinghongChen,JingbiaoMei,AlexandruCoca, 2023. 3\nandBillByrne.Fine-grainedlate-interactionmulti-modalre- [38] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San-\ntrievalforretrievalaugmentedvisualquestionanswering. In jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,\nAdvancesinNeuralInformationProcessingSystems36:An- Aditya Khosla, Michael Bernstein, et al. Imagenet large\nnualConferenceonNeuralInformationProcessingSystems scalevisualrecognitionchallenge. Internationaljournalof\n2023,NeurIPS2023,NewOrleans,LA,USA,December10 computervision,115:211–252,2015. 5,14\n-16,2023,2023. 3 [39] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang\n[26] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Yu,XiangyuZhang,JingLi,andJianSun. Objects365: A\nShi, Maria Lomeli, Rich James, Pedro Rodriguez, Ja- large-scale,high-qualitydatasetforobjectdetection.In2019\ncob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: IEEE/CVF International Conference on Computer Vision,\n10\nICCV 2019, Seoul, Korea (South), October 27 - November [53] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,Ruoqi\n2,2019,pages8429–8438.IEEE,2019. 5,14 Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\n[40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. In- Ren,YuxuanSun,etal. Mmmu:Amassivemulti-discipline\nstantbooth: Personalized text-to-image generation without multimodalunderstandingandreasoningbenchmarkforex-\ntest-timefinetuning. InProceedingsoftheIEEE/CVFCon- pert agi. In Proceedings of the IEEE/CVF Conference\nferenceonComputerVisionandPatternRecognition,pages on Computer Vision and Pattern Recognition, pages 9556–\n8543–8552,2024. 3 9567,2024. 12\n[41] YunxiaoShi,XingZi,ZijingShi,HaiminZhang,QiangWu, [54] MingyuanZhang,XinyingGuo,LiangPan,ZhongangCai,\nandMinXu. Eragent: Enhancingretrieval-augmentedlan- FangzhouHong,HuirongLi,LeiYang,andZiweiLiu. Re-\nguage models with improved accuracy, efficiency, and per- modiffuse:Retrieval-augmentedmotiondiffusionmodel. In\nsonalization. arXivpreprintarXiv:2405.06683,2024. 3 IEEE/CVF International Conference on Computer Vision,\n[42] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois, ICCV 2023, Paris, France, October 1-6, 2023, pages 364–\nXuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB 373.IEEE,2023. 3\nHashimoto.Stanfordalpaca:Aninstruction-followingllama [55] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui\nmodel,2023. 2 Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang,\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Linke Ouyang, et al. Internlm-xcomposer-2.5: A versatile\nMartinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste largevisionlanguagemodelsupportinglong-contextualin-\nRozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. putandoutput. arXivpreprintarXiv:2407.03320,2024. 2,\nLlama: Open and efficient foundation language models. 3\narXivpreprintarXiv:2302.13971,2023. 2 [56] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-\n[44] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nHe, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of\nWang, et al. Internvideo2: Scaling video foundation mod- language models with zero-init attention. arXiv preprint\nels for multimodal video understanding. arXiv preprint arXiv:2303.16199,2023. 2\narXiv:2403.15377,2024. 2,3 [57] ShilongZhang,PeizeSun,ShoufaChen, MinXiao,Wenqi\nShao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.\n[45] Stanisław Woz´niak, Bartłomiej Koptyra, Arkadiusz Janz,\nGpt4roi:Instructiontuninglargelanguagemodelonregion-\nPrzemysław Kazienko, and Jan Kocon´. Personalized large\nlanguagemodels.arXivpreprintarXiv:2402.09269,2024.3 of-interest. arXivpreprintarXiv:2307.03601,2023. 3\n[58] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao,\n[46] Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao,\nXuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao\nand Jin Yu. Understanding human preferences: Towards\nmore personalized video to text generation. In Proceed- Guo,MinzhiLi,XingxuanLi,etal. Retrievingmultimodal\ningsoftheACMonWebConference2024,pages3952–3963, information for augmented generation: A survey. arXiv\npreprintarXiv:2303.10868,2023. 3\n2024. 3\n[59] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,Xiaolei\n[47] PengXu,WeiPing,XianchaoWu,LawrenceMcAfee,Chen\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhtu-\nZhang,ZicanDong,etal. Asurveyoflargelanguagemod-\nrina,MohammadShoeybi,andBryanCatanzaro. Retrieval\nmeets long context large language models. arXiv preprint els. arXivpreprintarXiv:2303.18223,2023. 1\narXiv:2310.03025,2023. 3 [60] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\n[48] HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-\nunderstandingwithadvancedlargelanguagemodels. arXiv\nadapter: Textcompatibleimagepromptadapterfortext-to-\nimagediffusionmodels. arXivpreprintarXiv:2308.06721, preprintarXiv:2304.10592,2023. 3\n2023. 3\n[49] Chun-Hsiao Yeh, Bryan Russell, Josef Sivic, Fabian Caba\nHeilbron, and Simon Jenni. Meta-personalizing vision-\nlanguagemodelstofindnamedinstancesinvideo. InPro-\nceedingsoftheIEEE/CVFConferenceonComputerVision\nandPatternRecognition,pages19123–19132,2023. 3\n[50] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTongXu,andEnhongChen. Asurveyonmultimodallarge\nlanguagemodels.arXivpreprintarXiv:2306.13549,2023.2\n[51] OriYoran,TomerWolfson,OriRam,andJonathanBerant.\nMakingretrieval-augmentedlanguagemodelsrobusttoirrel-\nevantcontext. arXivpreprintarXiv:2310.01558,2023. 3\n[52] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\nJin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:\nSegment anything meets image inpainting. arXiv preprint\narXiv:2304.06790,2023. 5\n11\nA.AppendixOverview MyVLM LLaVA-LoRA\nYo’LLaVA RAP-LLaVA\n• SectionB:Additionalevaluationsofourmodels. 105\n• SectionC:Moreexperimentaldetails. 104\n• SectionD:MoredetailsofRAPdataset. 103\n• SectionE:Additionaldemonstrations. 102\n• SectionF:Analysisonlimitationsofourwork. 101\n• SectionG:Examplesofthepersonalizeddatabase.\n100 200 300 400 500\nNumberofConcepts\nB.AdditionalEvaluationResults Figure6.TimeCostofPersonalization.Weconductexperiment\nwith2A800GPUs.\nB.1.AblationStudy\nB.2.TimeCostofPersonalization\nDataset Composition. We conduct ablation experiments\nonthequestionansweringandrecognitionbenchmark,ex- We also evaluate the time cost associated with different\nperimental results are present in Table 7. The results fur- methods for learning a set of user’s concepts. The results\nther demonstrate that our data augmentation and the con- arepresentedinFigure6. MyVLMhastotrainanexternal\nstructednegativesamplesalsocontributetothemodel’sper- recognitionmodelforeachconceptandlearnanembedding\nformance. to adjust the model’s outputs. Similarly, Yo’LLaVA needs\ntolearnnewtokensforeachconcept. Duringtheoptimiza-\nTable 6. Evaluation on Knowledge-intensive Multimodal tionprocess, bothapproachesnecessitatemultipleforward\nBenchmarks.KB:KnowledgeBase. and backward pass of the MLLM, resulting in significant\nMethod MMMU[53] InfoSeek[6] timeconsumption. Incontrast,ourRAPonlyrequirestime\nfor encoding the image and adding its embedding to the\nLLaVA[27] 0.364 0.205\ndatabase,whichcanbeaccomplishedinjustafewseconds.\nLLaVA-LoRA[17] 0.359 0.205\nThissignificantlyenhancestheconvenienceandpracticality\nRAP-LLaVA 0.361 0.218\nofourmodelsinpracticalapplications.\nRAP-LLaVA(WithKB) 0.369 0.344\nB.3.StandardImageCaptioningMetrics\nWe further evaluate the models using standard image cap-\nMultimodal Benchmark. We also evaluate our model’s\ntioningmetrics. TheresultsareshowninTable8. Despite\nperformanceonseveraltraditionalmultimodalbenchmarks,\ntheadditionalconceptinformationisoffered,LLaVA’sper-\nincluding MMMU [53] and InfoSeek [6]. We assess our\nformance declines due to its inability to effectively utilize\nmodels’performancebothwithandwithoutexternalknowl-\ntheinformation. Incontrast,RAP-LLaVAachievescompa-\nedge base. For MMMU [53], we use 30K images paired\nrable performance to LLaVA on general image captioning\nwith corresponding captions from Wikipedia as the exter-\ntaskswhileenablingpersonalization.\nnal knowledge base. During testing, we retrieve the three\nmostsimilarimagesbasedonthequestion’simageandin-\nC.MoreExperimentalDetails\ncorporate only the textual knowledge to the input. For In-\nfoSeek[6],werandomlysample5Kquestionsfromtheval- Implementationdetails. WeutilizeYOLO-Worldv2-X[7]\nidationsetandconstructaknowledgebasecontaining50K asthedetectionmodel, settingdetectionclassestoinclude\nentities from Wikipedia database provided by the authors, allcategoriesstoredinthedatabasetoreducetheinterven-\nwhichincludesallrelevantentitiesassociatedwiththeques- tionsfromunrelatedobjects.Weconstructamultimodalre-\ntions. Foreachquestion,weretrievethemostsimilarentity trieverusingFacebookAISimilaritySearch(FAISS)[18],\nandaddonlythetextualknowledgetotheinput. employingapre-trainedCLIPViT-L/14-336[33]asthevi-\nWe evaluate on the validation set of MMMU, and 5K sual encoder. Each key in the database is generated by in-\nquestionssampledfromthevalidationsetofInfoSeek. We puttingaconcept’simageintotheCLIPvisualencoder,re-\nuse the official scripts to get the results, which are pre- sulting in a 768-dimensional vector. Considering the re-\nsentedinTable6.Fromtheresults,ourRAP-LLaVAretains strictionofcontextlengthofthebackbonelanguagemodel,\nmostgeneralknowledgeoftheoriginalLLaVA[27]. Italso weretrievethe2mostsimilarimagesfromthedatabasefor\nequips the MLLM with the ability to retrieve information each region of interest. Then we select 2 and 3 different\nfrom an external knowledge base, demonstrating superior concepts with the highest similarity among all as supple-\nperformanceinknowledgeintensivetasks. mentary inputs for RAP-LLaVA and RAP-Phi3-V, respec-\n12\n)dnoces(tsoCemiT\nTable7.AblationstudiesonQuestionAnsweringandVisualRecognition.Weightedresultsarecomputedasarithmeticmeans.\nQuestionAnswering VisualRecognition\nMethod\nVisual Text Weighted Positive Negative Weighted\nRAP-LLaVA 0.935 0.938 0.936 0.979 0.982 0.980\n-Dataaugmentation 0.924(-0.011) 0.918(-0.020) 0.921(-0.015) 0.943(-0.036) 0.988(+0.006) 0.965(-0.015)\n-Negativesamples 0.918(-0.017) 0.933(-0.005) 0.925(-0.011) 0.958(-0.021) 0.985(+0.003) 0.971(-0.009)\n-Textinformation 0.935 0.908(-0.030) 0.921(-0.015) 0.988(+0.009) 0.930(-0.052) 0.959(-0.021)\nTable 8. Quantitative Evaluation with Standard Image Captioning Metrics. We compute standard image captioning metrics for\npersonalizedcaptionsgeneratedbyRAP-LLaVA.Foreachimage,wetreatall5augmentedcaptionsasthesetofgroundtruthcaptions.\nTheresultsareaveragedacrossallthreeevaluationsandallconcepts.\nModel B1 B2 B3 B4 METEOR ROUGE L CIDEr SPICE\nLLaVA[27] 0.177 0.100 0.057 0.032 0.138 0.240 0.428 0.136\nLLaVA[27]+Retriever 0.087 0.038 0.014 0.005 0.074 0.132 0.025 0.020\nRAP-LLaVA 0.178 0.094 0.058 0.037 0.215 0.189 0.388 0.108\ntively. E.AdditionalDemonstrations\nBaselines. For MyVLM, we find that when the train-\nInthissection,weprovidemorequalitativeresultsobtained\ning data is very limited, it is quite hard for the classifica-\nbyvariousmodels.\ntion head to work effectively. Therefore, we use data aug-\n• In Table 11, we demonstrate how our models\nmentationtohelpimproveitsperformance.Specifically,we\nachievereal-timeeditingofconceptsbymodifyingthe\ncropthesingleimageintoseveralpiecescontainingthetar-\ndatabase.\ngetconcepttoimprovetheaccuracyofclassificationheads.\nTodistinguishbetweenmultiplepossibledifferentconcepts • In Table 12, we demonstrate the real-time addition of\nthat may appear in the image, we use 〈sks1〉, 〈sks2〉... as newconceptsbyupdatingthedatabase.\nconcept identifiers. For YoLLaVA, we present its experi- • InTable13,wepresentqualitativeresultsonpersonal-\nmentalresultsasreportedintheoriginalpaper[31]. izedconversationofRAP-LLaVA.\n• InTable14,wepresentqualitativeresultsonpersonal-\nD.DetailsofDataset\nizedconversationofRAP-Phi3-V.\nD.1.DatasetComposition • InTable15,wepresentadditionalimagecaptionsgen-\neratedbyRAP-LLaVAandothermethods.\n• We provide a summary of the composition of our\n• InTable16,wepresentadditionalimagecaptionsgen-\ndataset in Figure 7, which visually represents the dis-\neratedbyRAP-Phi3-Vandothermethods.\ntributionofdifferentcomponents.\n• In Table 17, we provide demonstrations of image de-\n• Table9presentsdetailednumericaldataforeachpart.\nscriptiongeneratedbyRAP-LLaVAandLLaVA.\n• InTable10,wespecifythesourcesforeachcomponent\n• In Table 18, we provide demonstrations of image de-\nofourdataset.\nscriptiongeneratedbyRAP-Phi3-VandPhi3-V.\nD.2.Instructions • InTable19and20,weprovideresultsonvisualrecog-\nnition of RAP-LLaVA. It also has the ability to give\nInthissection,wepresenttheinstructiontemplatesusedto preciseboundingboxofspecificconceptintheimage.\ncreateourdataset:\n• Table21containsinstructionsforvisualgroundingand F.Limitation\nrecognition.\nOurproposedRAPframeworkisaretrieval-basedmethod.\n• Table22includesexampleinstructionsforimagecap-\nThelimitationsofRAPmainlyconcerntheadditionalcom-\ntioning.\nputational cost of generation and the precision of the re-\n• Table 23 presents example instructions for image de- triever. While incorporating external information effec-\nscription. tively generates more specific answers, it inevitably in-\n• Table24presentsexamplequestionsusedforquestion creasesthecontextlengthforMLLMs,leadingtoadditional\nansweringsynthesis. computationaloverheadduringthegenerationprocess. We\n13\nFigure7.Compositionofourdataset. Table9.Statisticsofourdataset.\nType Size\nVisualGrounding 100K\nRecognition 40K\nCaption&Description 37K\nQuestionAnswering 16K\nLLaVA-Instruction 67K\nTotal 260K\nImage Description Question Answering\nImage Captioning LLaVA-Instruction\nRecognition Visual Grounding\nTable10.Datasource.\nType SourceDataset\nRefCOCO[20],TAO[9]\nVisualGrounding\nILSVRC2015-VID[38],Object365[39]\nRecognition CustomConcept101[21],CelebA[29]\nRefCOCO[20],TAO[9]\nCaption&Description\nObject365[39],CustomConcept101[21]\nRefCOCO[20],TAO[9]\nQuestionAnswering Object365[39],CustomConcept101[21]\nCelebA[29]\nLLaVA-Instruction LLaVA-Instruct-665K[27]\nwillfurtherexplorewaystomitigatethiscomputationalbur-\nden. Anotherlimitationisthepersonalizationperformance\nof our RAP-MLLMs depends on the retriever’s capability\nThis proposes need for a robust multi-modal retriever that\ncandiscernintricatefeaturestoenhanceretrievalprecision.\nDespite these limitations, RAP offers a timely solution for\nMLLMpersonalization.Byretrievingfromauser’sspecific\ndatabase,RAPfacilitatesreliableandflexiblepersonalized\ngeneration,whichisvaluableinpracticalapplications.\nG.ExamplesofthePersonalizedDatabase\nWegivesomevisualizedexamplesofourdatabaseinTable\n25. For each concept in the database, users need to pro-\nvideanimagewithitsnameandoptionaltextinformation.\nDuringinference,theimages,namesandotherinformation\nof retrieved concepts are integrated into the input for the\nMLLM. Users have the flexibility to define the name and\npersonalizeddescriptionbasedontheirpreferences,andour\nRAP-MLLMs will generate answers according to the pro-\nvidedinformation.\n14\nTable11. ExamplesofConceptEditing. Basedontheinformationrecordedinthedatabase,ourRAP-LLaVAcanprovidereliableand\naccurateanswers.\nImage Concept InformationintheDatabase\n1. Awhiteandgraydogwithlongfur. Hehasblackeyes. Hisfavorite\nfoodischicken.\nmydog\n2. Awhiteandgraydogwithlongfur. Hehasblackeyes. Hisfavorite\nfoodisbeef.\nQuestion: Whatis〈mydog〉’sfavoritefood?\n1.RAP-LLaVA:Chicken.\n2.RAP-LLaVA:Hisfavoritefoodisbeef.\nTable12.ExamplesofConceptUpdating.Thefirstcaptionisgeneratedwhentoy2notyetstoredinthedatabase.Oncethenewconcept\nisadded,RAP-LLaVAcanrecognizebothtoy1andtoy2.\nImage Concept InformationintheDatabase\ntoy1 Aplushtoy. Itisorangewithayellowbellyandabrownnose.\nThisisaplushtoyoftheblueycharacter. Itisalightbluecolorwitha\ntoy2\npurplepatchonitshead,anditsearsareyellow.\nQuestion: Giveacaptionofthisimage.\n1.RAP-LLaVA:〈toy1〉isreadyforbed! He’ssnuggledupwithhisfriend, a\nblueandyellowdogplushie. They’rebothlookingsocozyandreadyfora\ngoodnight’ssleep!\n(Addnewconcept〈toy2〉tothedatabase)\n2.RAP-LLaVA:〈toy1〉and〈toy2〉arereadyforbed! Theyarebothsnuggled\nunderthecoverswiththeirfavoritestuffedanimals.\n15\nTable13.ExamplesofpersonalizedconversationsobtainedbyRAP-LLaVA.\nImage Concept InformationintheDatabase\nA tabby cat with its eyes closed is laying on a light grey surface. The\nmycat\ncat’sfurisamixoflightanddarkbrownstripes.\nQuestion: Whereis〈mycat〉?\nRAP-LLaVA:〈mycat〉isonastaircase.\nQuestion: Whatisthetypeof〈mycat〉?\nRAP-LLaVA:〈mycat〉isatabbycat.\nQuestion: Whatisthecolorof〈mycat〉?\nRAP-LLaVA: 〈my cat〉 is a tabby cat with a mix of light and dark brown\nstripes.\nImage Concept InformationintheDatabase\n〈A〉isasmilingAsianmanwithshortblackhair,wearingglassesanda\nA\ngreysweater.\nQuestion: Canyousee〈A〉inthisphoto?\nRAP-LLaVA:Yes,〈A〉iscapturedinthisimage.\nQuestion: Whatcoloris〈A〉’sshirt?\nRAP-LLaVA:〈A〉iswearingapurpleshirt.\nQuestion: Whatis〈A〉’srighthandholding?\nRAP-LLaVA:Chopsticks.\nImage Concept InformationintheDatabase\n〈dug〉isagoldenretrieverfromthemovieUp. Hehasabrowncoatand\ndug\nabig,goofysmile,wearingacollarwithsomeroundobjects.\nQuestion: Canyoudescribe〈dug〉indetail?\nRAP-LLaVA:〈dug〉isagoldenretrieverfromthemovieUp.Hehasabrown\n(Text-onlyconversation)\ncoatandabig,goofysmile. Heiswearingacollarwithsomeroundobjects.\nHelooksveryhappyandplayful.\nImage Concept InformationintheDatabase\nP 〈P〉isabaldmanwitharedbeard.\nQuestion: Canyoudescribe〈P〉’sappearenceindetail?\n(Text-onlyconversation) RAP-LLaVA:〈P〉isabaldmanwitharedbeard.Heiswearingorangeshorts\nandablueshirt. Heissittinginablackdirector’schairwithhislegscrossed.\n16\nTable14.ExamplesofpersonalizedconversationsobtainedbyRAP-Phi3-V.\nImage Concept InformationintheDatabase\n〈bo〉isawell-groomed,medium-sizedShibaInuwithathick,cinnamon-\nbo\ncoloredcoat,creamaccents,alerteyes,andablackcollar.\nQuestion: Whatis〈bo〉doing?\nRAP-Phi3-V:〈bo〉islyingonitssideonafluffy,graydogbed. Itisresting\nitsheadonawhitestuffedtoy. 〈bo〉hasathick,cinnamon-coloredcoatwith\ncream accents, alert eyes, and a black collar. 〈bo〉 looks very relaxed and\ncontent.\nQuestion: Wheredoyouthinkhelikestoplay?\nRAP-Phi3-V:It’shardtosayforsure,butit’spossiblethat〈bo〉enjoysgoing\ntothepark. Manydogsliketogototheparktoplayandgetexercise.\nImage Concept InformationintheDatabase\nAnya Ayounggirlwithpinkhairandbiggreeneyes.\nBond Awhitedogwearingablackbowtie.\nQuestion: Whataretheydoing,describeindetail.\nRAP-Phi3-V:〈Anya〉isfeeding〈Bond〉,awhitedogwearingablackbowtie.\n〈Bond〉iseatingoutofabowl.〈Anya〉iskneelingdownandwatching〈Bond〉\neat. 〈Bond〉 is standing on a tiled floor and looking at the bowl. 〈Anya〉 is\nwearingapinksweaterandhaspinkhair. 〈Bond〉iswearingablackbowtie.\nThesceneisindoors,andthereisaTVinthebackground.\nQuestion: Whatis〈Anya〉wearing?\nRAP-Phi3-V:〈Anya〉iswearingablacksweater.\nImage Concept InformationintheDatabase\n〈C〉isayoungwomanwithlongblondehair,wearingawhitesleeveless\nC\nshirt. Shehasbrightredlipstickandlightmakeup.\nQuestion: Whatcoloris〈C〉’shair?\nRAP-Phi3-V:〈C〉’shairisblonde.\n(Text-onlyconversation)\nQuestion: Whatcolorofclothesdoesshelike?\nRAP-Phi3-V:Shelikeswhiteclothes.\n17\nTable15.AdditionalqualitativecomparisononimagecaptioningbetweenRAP-LLaVAandothermethods.\nImage Caption\nLLaVA:Abluecoffeecupsitsnexttotwodollsonawindowsill.\nLLaVA-LoRA:〈doll*〉positionedonawindowsillwithabluemugcontain-\ningpencils\nMyVLM:〈doll*〉isfoundsittingbesideawindowledgewithacupandtooth-\nbrushesnearby. Intheimage,〈doll*〉appearstobepeacefullyresting,possi-\nblyenjoyingtheviewortakingabreak.\nRAP-LLaVA(Ours): 〈doll*〉 and her friend are ready for a fun day at the\ndoll* office! They’rereadytotakeonanychallengethatcomestheirway.\nLLaVA: A wooden shelf holds several potted plants, including a large clay\npot and a small vase. The shelf is positioned near a wall, and the plants are\narrangedinavisuallyappealingmanner.\nLLaVA-LoRA: 〈mug*〉 on a wooden shelf next to a plant and two potted\nplants.\nMyVLM: 〈mug*〉 on a a blue and white tiled floor next to indoor plants, a\nshelfwithpotsandavase.\nRAP-LLaVA(Ours): Abrownceramictikimugwiththefaceofatikihead,\nmug* 〈mug*〉,sitsonawoodenshelfwithtwopottedplants.\nLLaVA:Acatissittingonaledgenearastaircase.\nLLaVA-LoRA:〈mycat〉sittingonastaircase,lookingdown.\nMyVLM:〈mycat〉inatypicalpose,grippingthebanisterofastaircase. He\niswearingacollar.\nRAP-LLaVA(Ours): 〈my cat〉 is relaxing on the stairs. Look at those eyes!\nWhatabeautifulcat.\nmy cat\nLLaVA:Acatislayingonablanketonacouch,nexttoacolorfulteapot.\nLLaVA-LoRA:〈mycat〉recliningonachairwitha〈teapot*〉beside\nmy cat MyVLM:〈teapot*〉onthecouchnearanorange,yellow,andblueteapot.The\ncatislayingonablanket,andtheteapotisplacedonatable.\nRAP-LLaVA(Ours):〈mycat〉istakingawell-deservednapnextto〈teapot*〉.\nteapot*\nLLaVA:Amanandawomanarewalkingdownastreet,withthemantaking\naselfieusinghiscellphone. Theyarebothsmilingastheywalk,andtheman\nisholdinghisphone...\nH LLaVA-LoRA:〈H〉and〈K〉arelookinghappyastheywalkdownthestreet\ntogether.\nMyVLM:〈H〉and〈T〉arelookingveryseriousastheytakeinthescenery.\nRAP-LLaVA(Ours): 〈T〉iswalkingdownthestreetwithherfriend〈H〉.\nT\nLLaVA:Twocolorfulparrotsareperchedonapalmtree,sittingnexttoeach\nother.\nLLaVA-LoRA:〈parrot1〉and〈parrot2〉arelookingveryseriousastheyperch\nparrot1 onapalmfrond.\nMyVLM:〈parrot1〉and〈parrot2〉looksocutetogether.\nRAP-LLaVA(Ours): 〈parrot1〉 and 〈parrot2〉 are both beautiful birds. 〈par-\nrot1〉hasabrightredbeakand〈parrot2〉hasalightblueheadandpinkfeet.\nparrot2 Theyarebothperchedonabranch.\n18\nTable16.AdditionalqualitativecomparisononimagecaptioningbetweenRAP-Phi3-Vandothermethods.\nImage Caption\nPhi3-V: A small white sheep figurine is sitting on a wooden table next to a\nplant.\nLLaVA-LoRA:〈sheep*〉positionednexttoapottedplantonawoodentable.\nMyVLM: 〈sheep*〉 positioned within a square shape within a vase made of\nmetalpositionedonatablemadeofwood.\nRAP-Phi3-V(Ours): 〈sheep*〉isreadytoplayontheshelfwiththeplant!\nsheep*\nPhi3-V:Abrownandwhitedogissittingonthegrasswithaleashattachedto\nit.\nLLaVA-LoRA:〈dog1〉sittingonthegrasswithalookofdetermination.\nMyVLM: on a cheerful note, 〈dog1〉 restfully resides, surrounded by the\nsoothingsoundsofnature.\nRAP-Phi3-V(Ours): Thisisapictureof〈dog1〉,acutedogwithlightbrown\nandwhitefur. Helookshappyandplayful.\ndog1\nPhi3-V:Abottleofwineisdisplayednexttoawoodencowstatue.\nLLaVA-LoRA:〈bull*〉positionednexttoabottleofsupremecabernetsauvi-\ngnon.\nMyVLM:〈goldpineapple〉parsthebottleofwine. Abottleofwinesitsnext\ntoabottleofwine.\nRAP-Phi3-V(Ours): 〈bull*〉standsguardoverabottleofSUPREMEwine.\nbull*\nPhi3-V:Asmallfigurineofabasketballplayerisplacedontopofabox.\nLLaVA-LoRA:〈funkopop*〉sittingonadesknexttoaboxandamonitor.\nMyVLM:〈funkopop*〉standsoutinaphotosetagainstadarkbackground.\nRAP-Phi3-V(Ours): 〈funkopop*〉isreadytodominatethecourt!\nfunko pop*\nPhi3-V: A man and woman are sitting at a table in a restaurant, having a\nconversationandenjoyingtheirtimetogether.\nJ LLaVA-LoRA:〈J〉and〈K〉enjoyingamealtogether.\nMyVLM:〈J〉and〈K〉enjoyingtheirtimetogether.\nRAP-Phi3-V(Ours): 〈J〉and〈K〉areenjoyingadrinkatabar.\nK\nPhi3-V:Asmallstatueofamanwearingglassesissittingnexttoacupona\ntable.\nLLaVA-LoRA: A table hosts 〈cup*〉 and a sculpture of a head wearing\ncup*\nglasses.\nMyVLM:Atablewith〈cup*〉,anespressocup,andaceramicmugdepicting\nafacewearingglassessitsonawoodensurface.\nRAP-Phi3-V(Ours): 〈head*〉and〈cup*〉arereadyfortheirmorningcoffee.\nhead*\n19\nTable17.QualitativeresultsofpersonalizedimagedescriptionobtainedbyRAP-LLaVA.\nImage Description\nLLaVA:Theimagefeaturesalarge,fluffycatsittingonacountertop,possibly\nin a kitchen. The cat is positioned near a sink and appears to be resting or\nsleeping. The countertop is covered with a towel, providing a comfortable\nsurfaceforthecattolayon...\nRAP-LLaVA(Ours): The image shows 〈my cat〉, a tabby cat with its eyes\nclosed,layingonalightgreysurface. Thecat’sfurisamixoflightanddark\nbrownstripes. Thecatisrelaxedandcomfortable.\nmy cat\nLLaVA: The image features a small metal robot figure, possibly a toy,\nstanding on a table or counter. The robot is positioned in the center of the\nscene,surroundedbyavarietyofleavesscatteredaroundit. Theleavesareof\ndifferentsizesand...\nRAP-LLaVA(Ours): The image shows a toy figurine of Iron Man, a super-\nherofromMarvelcomics. Heiswearingasilverandredsuitofarmorandis\nstandingonasurfacecoveredinleaves. Thetoyiscalled〈toy3〉.\ntoy3\nLLaVA: The image features a chair with a pillow on it, which has a picture\nofasheeponit. Thechairispositionedinfrontofadeskwithalaptopand\na monitor on it. The laptop is placed on the left side of the desk, while the\nmonitorisontherightside...\nRAP-LLaVA(Ours): The image shows a chair with a 〈pillow*〉 on it. The\n〈pillow*〉 is a pattern of flowers, leaves, and a sheep wearing glasses with a\nflower crown. There is also a laptop and a computer monitor in the back-\npillow* ground.\nLLaVA: The image features a bookshelf filled with various books. On the\nshelf, there are two Rubik’s cubes, one green and one white, sitting next to\neachother. Thebookshelfiswell-stockedwithbooks,occupyingmostof...\nRAP-LLaVA(Ours): Theimageshowsabookshelfwithbooksonit. There\naretwotoysontheshelf. Oneisa〈toy4〉,whichlookslikeacatwithorange\nfuzzyearsandabrowncordattached. TheotherisaRubik’scube.\ntoy4\nLLaVA:Theimagefeaturesabedwithtwostuffedanimals,adogandacat,\nlyingonit. Thedogispositionedontheleftsideofthebed,whilethecatis\non the right side. The stuffed animals are placed close to each other, giving\ntoy1 theimpressionofacozyand...\nRAP-LLaVA(Ours): The image shows two plush toys, 〈toy1〉 and 〈toy2〉,\nlying in bed together. 〈toy2〉 is a light blue plush with a purple patch on its\nhead and yellow ears. 〈toy1〉 is an orange plush with a yellow belly and a\ntoy2 brownnose. Theyarebothlookingatthecamera.\nLLaVA:Theimagefeaturesayounggirlhuggingalargewhitedoginaroom.\nThegirliswearingablackdressandappearstobeenjoyinghertimewiththe\ndog. The dog is positioned close to the girl, and they seem to be sharing a\ntendermomenttogether...\nAnya RAP-LLaVA(Ours): 〈Anya〉 is a young girl with pink hair and big green\neyes. Sheiswearingablackdressandblackshoes. Sheisstandinginaroom\nwith wooden floors and is hugging a white dog named 〈Bond〉. 〈Bond〉 is\nwearingablackbowtie. Thedogisstandingonitshindlegsandislookingat\nBond\nthecamera. Theroomisdecoratedwithabluerugandawoodencabinet...\n20\nTable18.QualitativeresultsofpersonalizedimagedescriptionobtainedbyRAP-Phi3-V.\nImage Description\nPhi3-V:Theimagefeaturesadisplayofvariousanimalfigurinesonashelf.\nTherearethreegiraffes,twoofwhicharestandingandonesitting,placednext\ntoeachother. Asheepfigurineisalsopresentontheshelf, locatedbetween\nthegiraffes...\nRAP-Phi3-V(Ours): The image shows a white 〈sheep*〉 sitting on a shelf\nnexttoagiraffefigurine. The〈sheep*〉ismadeoffeltandhasblackeyesand\nablacknose.Thegiraffefigurineismadeofmetalandhasacolorfulnecklace\nsheep*\narounditsneck. Theshelfiswhite...\nPhi3-V: The image features a wooden shelf with two wine bottles and two\nwineglasses. Oneofthewinebottlesispositionedtowardstheleftsideofthe\nshelf, while the other is located more towards the center. The wine glasses\nareplacedontherightsideofthe...\nRAP-Phi3-V(Ours): Theimageshowsawoodenshelfwiththreeglassware\nitemsonit. Thetopglasswareitemisa〈mug*〉withatikiheaddesign. The\notherglasswareitemsarewineglasses. Thereisasignonthewinebottlethat\nmug* says”SUPREME”. Thereisalsoaplantinapotinthebackground.\nPhi3-V:Theimagefeaturestwodogsplayingtogetherinagrassyyard. One\ndogislocatedontheleftsideoftheyard,whiletheotherdogisontheright\nside. They are both running and enjoying their time outside. In the back-\nground,thereareseveralchairs...\nRAP-Phi3-V(Ours):Theimageshowstwodogsplayinginagrassybackyard.\nThedogontheleftisawhiteandgraydogwithlongfur.Thedogontheright\nisablackandbrowndogwithlongfur.Thedogsarebothrunningandplaying\nmy dog onthegrass. Thedogontheleftisnamed〈mydog〉.\nPhi3-V:Theimagefeaturesawhiteshelfwithseveraldecorativeitemsonit.\nTherearethreechicken-shapeddecorationshangingfromtheshelf,eachwith\na different design. Additionally, there are three more chicken decorations\nplacedontheshelf,making...\nRAP-Phi3-V(Ours):Theimageshowsthreehandmadestuffedtoys.Thetoys\nare all shaped like chickens and are hanging on a white shelf. The toys are\nalldifferentcolorsandhavedifferentdesignsonthem. Thetoyontheleftis\ntoy4 called〈toy4〉.\nPhi3-V:Theimagefeaturesagroupofthreecartooncharacters,twoofwhich\narewearinghats.Theyarestandingtogetheronasidewalk,withonecharacter\npositionedtowardstheleftsideoftheframe,anotherinthecenter,and...\ncharacter1 RAP-Phi3-V(Ours): The image shows 〈character1〉 and 〈character2〉 stand-\ning in front of a brick wall. 〈character1〉 is a cartoon fox wearing a yellow\nbutton-downshirtandapurpletie. Hehasbigeyes,fluffyfur,andaslygrin\nonhisface. 〈character2〉isacartoonrabbitwithlargeears,apoliceofficer’s\ncharacter2 hat,andaredandyellowsafetyvest. Shehasasweet,innocentlookon...\nPhi3-V:Theimagefeaturesababylyingonabed,withadogrestingonthe\nfloor nearby. The baby appears to be playing with a toy, possibly a puzzle,\nwhilethedogwatchesattentively...\nbaby* RAP-Phi3-V(Ours): Theimageshowsababy,〈baby*〉,sleepinginacarrier.\nThebabyiswrappedinacolorfulblanketandissecuredbyablackstrap. A\nfrench bulldog, 〈dog2〉, is sleeping next to the baby. The baby and dog are\nbothsleepingpeacefully.\ndog2\n21\nTable19. QualitativeresultsofpersonalizedconceptrecognitionobtainedbyRAP-LLaVA.Weusegreenrectangletoshowthebounding\nboxintheimage.\nImage Conversation\nQuestion: Is〈head*〉inthisimage?\nRAP-LLaVA:Yes,〈head*〉isinthephoto.\nQuestion: Give〈head*〉’sboundingboxintheimage.\nRAP-LLaVA:[0.57,0.67,0.68,0.84]\nhead*\nQuestion: Is〈head*〉inthisimage?\nRAP-LLaVA:No,〈head*〉isnotpartofthisphotograph.\nhead*\nQuestion: Canyouidentifyif〈toy*〉iscapturedinthispicture?\nRAP-LLaVA:Yes,〈toy*〉isvisibleinthispicture.\nQuestion: Pleaseprovide〈toy*〉’sboundingcoordinatesintheimage.\nRAP-LLaVA:[0.42,0.19,0.54,0.39]\ntoy*\nQuestion: Canyouidentifyif〈toy*〉iscapturedinthispicture?\nRAP-LLaVA:〈toy*〉isnotcapturedinthisimage.\ntoy*\nQuestion: Canyouconfirmif〈A〉appearsinthisphoto?\nRAP-LLaVA:Yes,〈A〉isinthisphoto.\nQuestion: Describe〈A〉’spositionintheimage.\nRAP-LLaVA:[0.24,0.12,0.72,0.98]\nA\nQuestion: Canyouconfirmif〈A〉appearsinthisphoto?\nRAP-LLaVA:〈A〉isnotpresentinthisphotograph.\nA\n22\nTable20. QualitativeresultsofpersonalizedconceptrecognitionobtainedbyRAP-LLaVA.Weusegreenrectangletoshowthebounding\nboxintheimage.\nImage Conversation\nQuestion: Is〈statue1〉includedinthisshot?\nRAP-LLaVA:Yes,〈statue1〉isincludedinthisshot.\nQuestion: Give〈statue1〉’sboundingboxintheimage.\nRAP-LLaVA:[0.36,0.16,0.59,0.41]\nstatue1\nQuestion: Is〈statue1〉includedinthisshot?\nRAP-LLaVA:〈statue1〉isnotcapturedinthispicture.\nstatue1\nQuestion: Is〈mydog〉capturedinthisimage?\nRAP-LLaVA:Yes,Irecognize〈mydog〉inthispicture.\nQuestion: Specify〈mydog〉’sboundingboxinthephotograph.\nRAP-LLaVA:[0.33,0.23,1.0,0.69]\nmy dog\nQuestion: Is〈mydog〉capturedinthisimage?\nRAP-LLaVA:No,〈mydog〉isnotincludedinthisshot.\nmy dog\nQuestion: Is〈teapot*〉visibleinthisimage?\nRAP-LLaVA:Yes,〈teapot*〉isvisibleinthispicture.\nQuestion: Describe〈teapot*〉’spositionintheimage.\nRAP-LLaVA:[0.52,0.69,0.73,0.91]\nteapot*\nQuestion: Is〈teapot*〉visibleinthisimage?\nRAP-LLaVA:No,〈teapot*〉isnotvisibleinthispicture.\nteapot*\n23\n(cid:19) Table21.Instructionsforvisualgroundingandrecognition. (cid:16)\nVisualrecognition:\n• Is⟨V⟩visibleinthispicture?\n• Is⟨V⟩inthisimage?\n• Doyousee⟨V⟩inthephoto?\n• Is⟨V⟩presentinthisphotograph?\n• Canyouidentifyif⟨V⟩iscapturedinthispicture?\n• Is⟨V⟩depictedinthisimage?\n• Doesthepicturefeature⟨V⟩?\n• Canyouconfirmif⟨V⟩appearsinthisphoto?\n• Is⟨V⟩includedinthisshot?\n• Is⟨V⟩showninthisimage?\n• Canyoutellif⟨V⟩ispartofthisphotograph?\n• Isthereanysignof⟨V⟩inthispicture?\n• Canyoudetect⟨V⟩inthephoto?\n• Is⟨V⟩capturedinthisimage?\n• Doyourecognize⟨V⟩inthispicture?\nVisualgrounding:\n• Give〈V〉’sboundingboxintheimage.\n• Describe〈V〉’spositionintheimage.\n• Pleaseprovidethecoordinatesoftheboundingboxfor〈V〉inthegivenimage.\n• Specifytherectangularboundariesof〈V〉intheimage.\n• Give〈V〉’spositioninthefollowingimage.\n• Pleaseprovide〈V〉’sboundingcoordinatesintheimage.\n• Indicatetheboundingboxfor〈V〉intheimage.\n• Showtheboundingboxfor〈V〉inthepicture.\n• Specify〈V〉’sboundingboxinthephotograph.\n• Mark〈V〉’sboundingboxwithintheimage.\n(cid:18) (cid:17)\n(cid:19) Table22.Instructionsforimagecaptioning. (cid:16)\nImagecaption:\n• Giveacaptionoftheimage.\n• Giveapersonalizedcaptionofthisimage.\n• Provideabriefcaptionoftheimage.\n• Summarizethevisualcontentoftheimage.\n• Createashortcaptionoftheimage.\n• Offerashortandclearinterpretationoftheimage.\n• Describetheimageconcisely.\n• Renderaconcisesummaryofthephoto.\n• Provideacaptionofthegivenimage.\n• Canyouprovideapersonalizedcaptionofthisphoto?\n• Couldyoudescribethisimageconcisely?\n(cid:18) (cid:17)\n24\n(cid:19) Table23.Instructionsforimagedescription. (cid:16)\nImagedescription:\n• Describetheimage.\n• Giveadescriptionoftheimage.\n• Giveadescriptionoftheimageindetail.\n• Giveashortdescriptionoftheimage.\n• Describetheimageindetail.\n• Pleaseprovideadescriptionoftheimage.\n• Canyougivemedetailsabouttheimage?\n• Couldyouexplainwhat’sshownintheimage?\n(cid:18) (cid:17)\n(cid:19) Table24.Seedquestionsusedforquestionansweringsynthesis. (cid:16)\nPerson:\n• Whatis〈H〉’shaircolor?\n• Whatis〈H〉’sheight(estimated)?\n• Whatis〈H〉’sskintone?\n• Whatis〈H〉’seyecolor?\n• Whatstyleofclothingis〈H〉wearing?\n• Does〈H〉haveanyvisibletattoos?\n• Does〈H〉wearglassesorcontactlenses?\n• Does〈H〉haveanyfacialhair?\n• Whatis〈H〉’sapproximateage?\n• Whatis〈H〉’sbuildorbodytype?\n• Whatis〈H〉doing?\nObject:\n• Whatcoloris〈O〉?\n• Whatpatternison〈O〉?\n• Whatshapedoes〈O〉have?\n• Whatsizeis〈O〉?\n• Whatisthetextureof〈O〉?\n• Is〈O〉shinyormatte?\n• Whatmaterialis〈O〉madeof?\n• Does〈O〉haveanypatternsordesignsonit?\n• Is〈O〉neworworn?\n• Does〈O〉haveanyvisiblebrandorlogo?\n• Is〈O〉functionalordecorative?\nMulti-conceptquestion:\n• Whatdo〈C 〉and〈C 〉haveincommon?\n1 2\n• Whatactivityare〈C 〉and〈C 〉engagedin?\n1 2\n• Wherecould〈C 〉and〈C 〉belocated?\n1 2\n• Whatisthemostnoticeabledifferencebetween〈C 〉and〈C 〉?\n1 2\n• Whataretheydoing?\n(cid:18) (cid:17)\n25\nTable25.Examplesofourdatabase.Aconceptshouldbeprovidedwithanimageanditspersonalizeddescription.\nImage Concept Information\nAnya Ayounggirlwithpinkhairandbiggreeneyes.\nThisisacutefigurineofagirlwearingapinkandbluedress,holdinga\ndoll*\nwhitebubble.\ntoy1 Aplushtoy. Itisorangewithayellowbellyandabrownnose.\nThisisaplushtoyoftheblueycharacter. Itisalightbluecolorwitha\ntoy2\npurplepatchonitshead,anditsearsareyellow.\nThisisafigurineofacat. Thecathasabluebodywithyellow,red,and\nstatue*\ngreenstripesandalongtailthatisalsostriped.\ncat* Asmallgingerkittenwithbrightblueeyeslooksupatthecamera.\nH Ayoungmaniswearingaplaintant-shirt. Hishairisshortandcurly.\nmydog Awhiteandgraydogwithlongfur. Hehasblackeyes.\nAyoungwomanwithblondehairiswearingawhitetanktopandblue\nT\njeans.\n26",
    "pdf_filename": "Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models.pdf"
}