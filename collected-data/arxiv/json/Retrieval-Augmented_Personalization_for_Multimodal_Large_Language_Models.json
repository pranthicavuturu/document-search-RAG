{
    "title": "Retrieval-Augmented Personalization for Multimodal Large Language Models",
    "context": "The development of large language models (LLMs) has sig- nificantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user- specific knowledge still restricts their application in hu- man’s daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs’ personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Re- member: We design a key-value database to store user- related information, e.g., user’s name, avatar and other attributes. (b) Retrieve: When the user initiates a con- versation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts’ information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time ∗Equal contribution † Corresponding author concept editing via updating the external database. To fur- ther improve generation quality and alignment with user- specific information, we design a pipeline for data collec- tion and create a specialized dataset for personalized train- ing of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pre- training on large-scale dataset, RAP-MLLMs can general- ize to infinite visual concepts without additional finetun- ing. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as per- sonalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/. Recently, the development of large language models (LLMs) has significantly enhanced their language process- ing and generating capabilities [59]. Building on this foundation, the integration of visual and textual ability 1 arXiv:2410.13360v2  [cs.CV]  18 Nov 2024",
    "body": "Retrieval-Augmented Personalization for Multimodal Large Language Models\nHaoran Hao1,2∗, Jiaming Han1∗, Changsheng Li3, Yu-Feng Li2, Xiangyu Yue1†\n1MMLab, The Chinese University of Hong Kong\n2National Key Laboratory for Novel Software Technology, Nanjing University\n3Beijing Institute of Technology\nPersonalized Captioning\nThis is <K>, \nShe lives in Korea.\nInput image\nThis is <J>,\nHe is <K>’s boyfriend.\n<J>\n<K>\n<K> and <J> enjoying a relaxing afternoon at a \ntrendy café.\nPersonalized Conversation\nWhat is <K> doing?\nRemember\nRetrieve\nGenerate\nWhat’s the relationship between them?\nUser’s database\nRAP-LLaVA\n<K> is sitting at a table in a café, wearing a blue \npolka-dot dress. She is holding a glass of a \npinkish drink with a straw and appears to be \nsipping from it. \n<J> is <K>’s boyfriend, based on the image, they \nappear to be enjoying each other's company at a \ncafé, Their relaxed body language and the casual \nsetting imply a close and comfortable relationship.\nFigure 1. Introduce some user-specific concepts to our RAP-LLaVA, it can remember them and achieve excellent performance in a variety\nof personalized multimodal generation tasks.\nAbstract\nThe development of large language models (LLMs) has sig-\nnificantly enhanced the capabilities of multimodal LLMs\n(MLLMs) as general assistants.\nHowever, lack of user-\nspecific knowledge still restricts their application in hu-\nman’s daily life. In this paper, we introduce the Retrieval\nAugmented Personalization (RAP) framework for MLLMs’\npersonalization. Starting from a general MLLM, we turn\nit into a personalized assistant in three steps.\n(a) Re-\nmember: We design a key-value database to store user-\nrelated information, e.g., user’s name, avatar and other\nattributes.\n(b) Retrieve: When the user initiates a con-\nversation, RAP will retrieve relevant information from the\ndatabase using a multimodal retriever. (c) Generate: The\ninput query and retrieved concepts’ information are fed into\nMLLMs to generate personalized, knowledge-augmented\nresponses. Unlike previous methods, RAP allows real-time\n∗Equal contribution † Corresponding author\nconcept editing via updating the external database. To fur-\nther improve generation quality and alignment with user-\nspecific information, we design a pipeline for data collec-\ntion and create a specialized dataset for personalized train-\ning of MLLMs.\nBased on the dataset, we train a series\nof MLLMs as personalized multimodal assistants. By pre-\ntraining on large-scale dataset, RAP-MLLMs can general-\nize to infinite visual concepts without additional finetun-\ning.\nOur models demonstrate outstanding flexibility and\ngeneration quality across a variety of tasks, such as per-\nsonalized image captioning, question answering and visual\nrecognition. The code, data and models are available at\nhttps://hoar012.github.io/RAP-Project/.\n1. Introduction\nRecently,\nthe development of large language models\n(LLMs) has significantly enhanced their language process-\ning and generating capabilities [59].\nBuilding on this\nfoundation, the integration of visual and textual ability\n1\narXiv:2410.13360v2  [cs.CV]  18 Nov 2024\n\nTable 1. Comparison of Different Personalization Methods. RAP needs only 1 image with its personalized description, showing\noutstanding convenience and flexibility in practical applications.\nNumber of Image\nData Requirements for Personalization\nSupport\nMethod\nPositive Negative Caption Description Question-Answer Recognition Real-time edit Text-only QA\nFine-tuning\nn\n-\nYes\nYes\nNo\nNo\n✗\n✓\nMyVLM [2]\nn\n150\nYes\nNo\nYes\nYes\n✗\n✗\nYo’LLaVA [31]\nn\n200\nNo\nNo\nYes\nYes\n✗\n✓\nRAP(Ours)\n1\n-\nNo\nYes\nNo\nNo\n✓\n✓\nthrough vision-language alignment brings powerful mul-\ntimodal LLMs (MLLMs) [12, 15, 28, 32, 44, 50, 55].\nMLLMs have shown significant improvement in various\ntasks, such as image description and question answering,\nhighlighting their potential as human’s assistants. However,\ntheir lack of user-specific knowledge continues to limit their\neffectiveness as personalized assistants in daily life.\nA qualified personalized assistant first should be able to\nrecognize and remember user-related concepts, such as the\ndog named 〈Lala〉adopted by the user. Although existing\nMLLMs have been trained on large-scale datasets and pos-\nsess strong recognition and classification capabilities, di-\nrectly transferring this knowledge to a user’s personal con-\ncepts remains challenging. For instance, current leading\nMLLMs cannot remember your dog’s name, even if you\nhave mentioned it before, and they lack awareness of your\nidentity and preferences. Furthermore, the assistant should\ngenerate responses tailored to the user’s preferences and re-\nquirements. However, collecting extensive personal data to\ntrain a unique assistant for each user is impractical.\nTo address this issue, the personalization of MLLMs has\nbecome a topic of growing interest, with several approaches\nalready being proposed. MyVLM [2] utilizes external clas-\nsification heads to recognize specific concepts, and learns\nan embedding for each concept to personalize the outputs\nof vision language models (VLMs) [23, 28]. Another con-\ncurrent work, Yo’LLaVA [31], learns a few special tokens to\nrepresent each concept. However, both approaches necessi-\ntate continuous learning and updating of the model as new\nconcepts emerge. This presents a challenge in dynamic,\never-changing real-world scenarios, where the computing\npower of users’ personal devices is often limited, and all\ndata must be stored locally for privacy concerns.\nTo address these challenges, we propose the Retrieval\nAugmented Personalization (RAP) framework, designed to\nallow MLLMs to update their supported concepts without\nadditional training. Specifically, RAP works in three key\nsteps. (a) Remember: RAP includes a designed database to\nhelp remember each concept via storing its image and basic\ninformation, e.g., name, avatar and other attributes. (b) Re-\ntrieve: When a user initiates a conversation, RAP will re-\ntrieve relevant information from the database using a multi-\nmodal retriever. (c) Generate: The input query and retrieved\nconcepts information are incorporated into the MLLM’s\ninput for personalized, knowledge-augmented generation.\nRAP requires only one image per concept with its basic in-\nformation for personalization. It allows users to make real-\ntime adjustments to the model’s outputs by modifying their\npersonal databases, eliminating the need for retraining. A\nmore detailed comparison is presented in Table 1.\nAnother significant challenge is the lack of large-scale\ndatasets for training MLLMs’ personalized generation ca-\npabilities.\nTo address this, we design a pipeline to col-\nlect extensive training data and create a comprehensive\ndataset, which enables to train MLLMs to effectively un-\nderstand and utilize user-related information for generation.\nBased on this dataset, we train LLaVA [28] and Phi3-V [35]\nas novel personalized assistants and evaluate their perfor-\nmance across various tasks, including personalized image\ncaptioning, question answering, and visual recognition. Ex-\nperimental results demonstrate that our RAP-MLLMs excel\nin wide range of personalized generation tasks, showcasing\nexcellent generation quality and flexibility.\nOur contributions are summarized as follows:\n• We propose the RAP framework for MLLMs’ person-\nalization, allowing models pre-trained on our dataset to\nadapt to diverse users and infinite new concepts without\nfurther training.\n• We develop a pipeline for collecting large-scale data\nand create a dataset specifically designed for the per-\nsonalized training and evaluation of MLLMs.\nThis\ndataset enables us to train a series of MLLMs to func-\ntion as personalized assistants.\n• Our models demonstrate exceptional performance\nacross various personalized multimodal generation\ntasks, including image captioning and question answer-\ning. Additionally, they exhibit a strong capability to\nrecognize personal concepts within images.\n2. Related Work\nMultimodal Large Language Models. Recently, numer-\nous advanced large language models (LLMs) [1, 8, 42, 43,\n56] have been proposed, showing remarkable performance\nin addressing a wide range of tasks. The rapid development\n2\n\nof LLMs has led to the emergence of multimodal LLMs\n(MLLMs) [12, 15, 28, 32, 44, 55, 60], which excel in gen-\neral visual understanding and complex reasoning tasks. For\ninstance, LLaVA [27, 28] and MiniGPT-4 [60] align visual\nand language modalities through visual instruction tuning,\nshowcasing impressive capabilities in multimodal conver-\nsations. GPT4RoI [57] and RegionGPT [13] enhance fine-\ngrained understanding and reasoning for specific regions by\ntraining on region-level instruction datasets. Despite these\nadvancements in tasks such as image captioning and ques-\ntion answering, the lack of user-specific knowledge restricts\nthe generation of personalized content. In this work, we\nfocus on the personalization of MLLMs, enabling them to\nremember and understand user-specific concepts, and gen-\nerate personalized content tailored to user’s preferences.\nPersonalization of MLLMs. In the realm of artificial in-\ntelligence, personalization typically refers to the process of\ntailoring a system, application, or model to meet the individ-\nual needs and preferences [41, 45, 46, 49]. Substantial ef-\nforts have been made to generate images of user’s personal\nobjects or in certain context [10, 14, 21, 22, 37, 40, 48].\nFor example, Dreambooth [37] employs transfer learning\nin text-to-image diffusion models via fine-tuning all pa-\nrameters for new concepts. In this paper, we mainly aim\nat enabling MLLMs to remember and understand user-\nspecific concepts, and generate personalized language out-\nputs. There are several works focusing on the personaliza-\ntion of MLLMs, among which the most relevant works are\nMyVLM [2] and Yo’LLaVA [31]. MyVLM introduces the\ntask of personalizing VLMs. It utilizes external classifi-\ncation heads to recognize specific concepts, and learns an\nembedding for each concept to personalize the outputs of\nVLMs. Yo’LLaVA personalizes LLaVA by extending its\nvocabulary and learning specific tokens for each concept.\nHowever, both approaches require continuous model up-\ndates as new concepts emerge, which presents challenges\nin dynamic real-world applications. In this work, we pro-\npose RAP framework for the personalization of MLLMs,\nenabling models pre-trained on our dataset to continuously\nupdating supported concepts without further fine-tuning.\nRetrieval Augmented Generation. Retrieval-based meth-\nods for incorporating external knowledge have proven\neffective in enhancing generation across a variety of\nknowledge-intensive tasks [3, 11, 26, 47, 51, 58]. DPR [19]\nintroduces Dense Passage Retrieval, marking a shift from\nsparse to dense retrieval techniques. Later, MuRAG [5] pro-\nposes to use multimodal knowledge to augment language\ngeneration. Self-Rag [3] introduces special tokens to make\nretrieval adaptive and controllable. ERAGent [41] presents\na comprehensive system for retrieval-augmented language\nmodels.\nWith the advancements in MLLMs, RAG has\nbeen widely applied to multimodal generative tasks. For\ninstance, FLMR [25] employs multi-dimensional embed-\ndings to capture finer-grained relevance between queries\nand documents, achieving significant improvement on the\nRA-VQA setting.\nWhile existing methods primarily en-\nhance models’ performance by retrieving from external\nknowledge bases, few of them consider the personaliza-\ntion task. Although RAG has been applied to image gen-\neration [4, 54] and image captioning [24, 34], there is cur-\nrently no existing work focusing on personalizing MLLMs\nvia RAG, to the best of our knowledge.\n3. Retrieval Augmented Personalization\nExisting MLLMs typically align other modalities with lan-\nguage.\nFor instance, LLaVA [28] projects visual tokens\ninto text space, and then generates subsequent tokens us-\ning an LLM. While these MLLMs perform well in vari-\nous tasks, the lack of memory and comprehension of per-\nsonal concepts hinders effective user-specific responses. In\nthis work, we mainly focus on personalizing MLLMs to\ngenerate tailored language responses, such as creating per-\nsonalized captions for user’s images and answering ques-\ntions about personal concepts. In this section, we detail\nthe implementation steps of our proposed Retrieval Aug-\nmented Personalization (RAP) framework. Unlike previous\napproaches that usually necessitate additional data collec-\ntion and further training to learn new concepts, after pre-\ntraining on our dataset, RAP-MLLMs can adapt to diverse\nusers and infinite new concepts without further training. In\nsection 3.1, we present the RAP framework that is applica-\nble to various types of MLLMs, and then in section 3.2, we\nprovide details of the proposed dataset.\n3.1. RAP Framework\nRAP works in three steps: Remember, Retrieve and Gener-\nate. An overview of the framework is shown in Figure 2.\nRemember.\nThe premise of personalization is that the\nmodel can remember personal concepts and relevant infor-\nmation, such as the dog named 〈Lala〉adopted by 〈A〉. To\nfacilitate this, we construct a database M to store personal\nconcepts, which comprises an avatar Ij, a name along with\na brief description Tj for each concept. The key ki for\neach concept in the database is its visual feature, obtained\nby feeding its image Ij into a pre-trained image encoder\nE(·). Examples of our database are presented in Figure 2.\nWhen a user initiates a conversation, the input can be rep-\nresented as Q = (Xv, Xq), which may include both image\nXv and some textual instructions Xq. The first step involves\nidentifying possible concepts within the input image that\nhave been previously stored in the database. Previous meth-\nods [2] typically need to learn an external classifier to deter-\nmine whether a concept appears in the input image, which\nrequires a substantial amount of training data and can only\napply to specific concept. To enhance the generalizability of\nthe recognition process, we do not construct specific mod-\n3\n\nUser’s database\nName: <A>\nA young woman who loves dog. Age: 22.\nCrop region of interest\nName: <A>\nA young woman who \nloves dog. Age: 22\nName: <Lala>\nA German Shepherd dog.\nInstruction: Please give \na caption of this image.\nMultimodal Large Language Model\nUser-related information\nResponse: <A> enjoys a sunny grooming session with her dog <Lala> in the countryside.\nUser’s input\nName: <Lala>\nA German Shepherd dog.\nName: <H>\nA man from America. Hobby: Traveling.\n…            …\nReal-time update\nProj.\nProj.\nProj.\nRetriever\nOpen world \ndetector\nFigure 2. Retrieval-Augmented Personalization Framework. Region-of-interest detected by an open world detector are used to retrieve\nconcepts from the database. The images and information of the retrieved concepts are then integrated into the input for the MLLM.\nules for each concept. Instead, we employ a universal de-\ntection model, such as YOLO [36] and YOLO-World [7], as\nrecognition model R(·). Given the predefined setting P that\nspecifies which categories should be remembered, the user’s\nregion-of-interest can be acquired via Xu = R(Xv, Xq|P).\nRetrieve. Identified region-of-interest will be used as query\nto retrieve from the database. For each recognized compo-\nnent Xi\nu, we feed the image crop into the image encoder\nE(·) to get its visual feature vi = E(Xi\nu), which is a n-\ndimensional vector. Then we calculate the euclidean dis-\ntance between the visual feature and each key kj ∈M,\nwhich is calculated as Dist(vi, kj) = ∥vi −kj∥. The Top-\nK image-text pairs {(I1, T1), (I2, T2), · · · (IK, TK)} with\nthe lowest distances are selected from the database. We also\nintroduce retrieval using concept names, such as ⟨sks⟩for a\nunique concept. When the user mentions the name of an\nobject documented in the database, our model retrieves its\nrelated information from the database. This also enables our\nmodel to respond to text-only queries effectively.\nGenerate. Each pair Mj = (Ij, Tj) provides related infor-\nmation about a user’s personal concept and will be incor-\nporated into the input of the MLLM. Take LLaVA [28] as\nan example. The image Ij is first encoded by a pre-trained\nvision encoder, such as CLIP [33], to obtain their visual\ntokens Zj. These image tokens are then projected by a pro-\njector into language tokens Hv\nj, which could be understood\nby the language model. Simultaneously, corresponding text\ninformation Tj are transformed into text tokens Hq\nj. This\nprocess is also applied to both Xv and Xq. All these to-\nkens are incorporated into the MLLM’s input to generate\nlanguage response. During training, we keep parameters of\nboth the detector and retriever frozen, just train the MLLM’s\nparameters θ. Given the length L of the output sequence, the\nprobability of the target answer Xa is computed as:\np(Xa) =\nL\nY\ni=1\npθ(Xa,i|Xv, Xq, M1, · · · MK, Xa,<i). (1)\n3.2. Personalization Dataset\nMost existing MLLMs struggle to generate personalized\noutputs even if additional concept information is provided,\nand there is currently no large-scale dataset for personalized\ntraining of MLLMs. To this end, we design a pipeline for\ndata creation and curate a novel dataset specifically for the\npersonalized training of MLLMs. We use Gemini-1.5 [12]\nto generate annotations for our dataset. An overview of our\npipeline and dataset is presented in Figure 3.\nThe first component of our dataset is dedicated to vi-\nsual grounding. In this task, a MLLM is trained to deter-\nmine whether a specific concept is in an image, particu-\nlarly identifying if the person or object in a reference image\nappears in the given image. When a positive match is de-\ntected, we also require the model to provide the bounding\nbox for the identified concept. For single-concept ground-\ning, we primarily use the RefCOCO dataset [20]. Based\non RefCOCO’s annotations, we crop target concepts from\nthe images and assign names to them, which serve as ref-\nerences for specific concepts. We then query Gemini to\ngenerate concise descriptions about properties of the con-\ncepts in these cropped regions, by which we construct a\nlarge-scale database including numerous different concepts.\nThe training data pairs images and these descriptions as\nqueries and the corresponding bounding boxes as outputs.\nHowever, data generated in this way is insufficient to sim-\n4\n\nCrop\nRotate\nFlip\nNovel view\nThis is <D>, give a description of him.\n•\nA man wearing a bright orange T-shirt…\nAugment\nInstruction type 1: Visual Grounding\nInstruction: Specify the rectangular boundaries of <D> in the image.\nAnswer: [0.12, 0.15, 0.44, 0.94]\nInstruction type 2: Image Captioning & Description \nInstruction: Please give a caption of the image.\nAnswer: <D> is playing ultimate frisbee, he's getting ready to pick up the frisbee. \n              The game is played on a field of grass.\nInstruction type 3: Question Answering: \nInstruction: What color is <D>’s shorts?\nAnswer: Black.\n3D model\nBackground\nConcept Information\nNoise Concept\n<N>\nFigure 3. Our Pipeline for Data Collection. We first crop the target concept from the image based on the dataset annotations and then\nquery Gemini [12] to generate its personalized description. We also apply data augmentation to diversify these cropped images. Then we\ncombine them with the original image to derive a series of instructions and answers from Gemini. When noise concepts are included in the\nadditional information, the answer remains unchanged, helping to train the MLLMs’ ability to filter out irrelevant concepts.\nulate the complexity of real-world recognition, especially\nwhen the target concept in the reference and input image is\ncaptured from different perspectives. To address this, we\nincorporate the ILSVRC2015-VID video object detection\ndataset [38], TAO [9] and CustomConcept101 [21] to en-\nrich our dataset. For multi-object grounding, we use the\nObject365 dataset [39] to construct our training data.\nThe second component of our dataset is designed for\ninstruction following. This section includes training data\nfor tasks such as image captioning, image description and\nquestion answering. For the image captioning and descrip-\ntion data, we provide cropped images of target concepts,\naccompanied by their names and related information from\nthe large-scale database, then query Gemini to generate a\ncaption or description that reflects the concepts depicted in\nthe entire image. For question answering, we first design a\nset of seed questions to serve as examples. These examples\nare used to prompt the annotator, Gemini, to generate new\nquestions and corresponding answers. This iterative pro-\ncess facilitates the creation of a rich and diverse collection\nof conversations that MLLMs can learn from. We construct\nsuch data using RefCOCO [20], Object365 [39], TAO [9]\nand CustomConcept101 [21] dataset.\nTo enhance alignment with real-world scenarios, it is es-\nsential to collect data featuring the same identity in vari-\nous environments. Thus, we also include multiple images\nabout the same individual from the CelebA dataset [29] and\nproduce question answering data about the individual. To\nfurther diversify the dataset, we apply image editing tech-\nniques for data augmentation.\nThis includes performing\nrandom rotations and flips on the cropped images, as well as\ngenerating novel views of the concepts by diffusion models.\nSpecifically, we use Inpaint-Anything [52] to separate the\nforeground from the background, and use Wonder3D [30]\nand SiTH [16] to synthesize novel views of foreground ob-\nject or person respectively. Finally, we combine these ele-\nments to generate images of the target concept from differ-\nent perspectives.\nTo support multi-concept personalized generation, it is\nnecessary to retrieve multiple potential concepts from the\ndatabase. In the generation step, the MLLM must prioritize\naccurate and contextually relevant information. Consider-\ning that retrieval results can be inaccurate, potentially lead-\ning to unreasonable answers, we construct negative samples\nby adding noise concepts to the input while preserving the\noriginal output. This approach trains the model’s discrimi-\nnation ability. By exposing the MLLM to both relevant and\nirrelevant information during training, it learns to discern\nand filter out noise concepts, thereby enhancing its robust-\nness during inference. Additionally, we include a subset of\nthe LLaVA-Instruct-665k visual instruction dataset [27] to\nretain general knowledge from the original MLLM. Further\ndetails about our dataset can be found in Appendix D.\n4. Experiment\nImplementation Details.\nWe conduct experiments on\nLLaVA-1.5-13B [27] and Phi3-V-3.8B [35], resulting in\ntwo personalized MLLMs, RAP-LLaVA and RAP-Phi3-\nV. We select YOLO-Worldv2 [7] as the detector and con-\n5\n\nTable 2. Qualitative Comparison on Image Captioning. Image examples of target concepts are shown in the left and captions generated\nare shown in the right. We use green text to denote correct target concepts.\nImage\nCaption\nmy dog\nLLaVA: A man is sitting at a table with a dog, and there are wine glasses and\na fork on the table.\nLLaVA-LoRA: 〈collie dog〉looking pleased as she shares a meal with her\nowner.\nMyVLM: 〈my dog〉positioned on a chair by a black table, holding a wine\nglass in her hand. A white dog sits on the floor beside her...\nRAP-LLaVA(Ours): 〈my dog〉is a very good boy, and he loves to sit at a\ntable with his owner. They are enjoying a meal.\nH\nT\nLLaVA: A man and a woman are standing in a kitchen, preparing food to-\ngether.\nThe woman is cutting lettuce on a cutting board, while the man\nwatches her. There are several tomatoes ...\nLLaVA-LoRA: 〈H〉and 〈K〉are preparing a meal together.\nMyVLM: 〈T〉and her friend 〈H〉are looking very serious as they take in the\nscenery.\nRAP-LLaVA(Ours): 〈H〉is helping 〈T〉prepare a salad in the kitchen.\nB\nW\nG\nPhi3-V: A group of stuffed animals, including a blue one, are sitting on a\nblack surface.\nLLaVA-LoRA: 〈B〉, 〈G〉and 〈W〉are happily exploring the grassland.\nMyVLM: 〈G〉and his crew are always ready to jump into a new adventure.\nRAP-Phi3-V(Ours): 〈W〉is hanging out with 〈G〉and 〈B〉on the lawn. They\nare having a great time playing!\nstruct a multimodal retriever using Facebook AI Similarity\nSearch (FAISS) [18], employing a pre-trained CLIP ViT-\nL/14-336 [33] as the visual encoder. Due to the context\nlength limitation of the backbone language model, for RAP-\nLLaVA and RAP-Phi3-V, we retrieve the 2 and 3 different\nconcepts with the highest similarity, respectively. More im-\nplementation details can be found in Appendix C.\nTraining. In the training phase, we skip the recognition\nand retrieval procedures, instead perform instruction tun-\ning to train the MLLMs. We adhere to most settings from\nthe original experiment of LLaVA [27], except for using a\nmaximum learning rate of 1e-4 and training for 1 epoch.\nWe employ low-rank adapters [17] to reduce the number of\ntrainable parameters, and train our models on 8 A100 GPUs\nwith a valid batch size of 64.\n4.1. Personalized Image Captioning\nIn this section, we evaluate our models on generating per-\nsonalized image captions with user’s specific concepts. We\nextend the dataset introduced by MyVLM [2] via adding 16\nnew concepts, including both objects and humans, forming\n8 concept pairs that appear together. For each pair, there\nare 8-13 images used for testing. This multi-concept setting\npresents additional challenges for personalization.\nSettings. We compare our models with MyVLM [2] and\nfine-tuning based method LLaVA-LoRA [17]. For LLaVA-\nLoRA and MyVLM, the training dataset contains 1 image\naccompanied by 5 captions for each concept. For LLaVA-\nLoRA, we train it with captions of the training images for 3\nepochs, applying low-rank adapters [17] and the same hy-\nperparameters as our models. For MyVLM, following their\ntraining process, we first train the classification head with\nthe positive and 150 negative images, then train the cor-\nresponding concept embedding with the provided captions\nfor each concept. For our models, we construct a database\nwhere each concept is represented by a cropped image and\na text description. Details of our database could be found\nin Appendix G. All remaining images are used as test sam-\nples. This evaluation process is repeated three times with\ndifferent seeds, resulting in a total of 1,182 images used for\nevaluation, and we report the average results.\nQualitative Comparison. In Table 2, we present image\ncaptions generated by different methods to make a compari-\nson. While LLaVA [27] and Phi3-V [35] generally provides\nbrief and clear captions for most test images, its lack of un-\nderstanding of the user’s specific concepts restricts it from\n6\n\nTable 3. Quantitative Evaluation on Image Captioning. We report Recall,\nPrecision and F1-score in the table, the best result in each metric is bold and\nthe second is underlined.\nMethod\nLLM\nRecall Precision F1-score\nLLaVA [27] + Retriever Vicuna-13B\n1.260\n48.76\n2.450\nLLaVA-LoRA [17]\nVicuna-13B\n82.97\n93.28\n87.82\nMyVLM-LLaVA [2]\nVicuna-13B\n84.65\n86.37\n85.50\nRAP-LLaVA\nVicuna-13B\n93.51\n96.47\n94.97\nRAP-Phi3-V\nPhi3-V-3.8B 88.14\n95.10\n91.49\nFigure 4. Performance under varying number of per-\nsonalized concepts.\n50\n100\n150\n200\n250\n300\nNumber of Concepts\n85\n90\n95\nF1-score\nMyVLM\nLLaVA-LoRA\nRAP-Phi3-V\nRAP-LLaVA\nTable 4. Quantitative Evaluation on Question Answering and Visual Recognition. The best result in each setting is bold and the second\nis underlined. Evaluation results of GPT-4V [32] are also provided as reference. Weighted results are computed as arithmetic means.\nMethod\nLLM\nTrain\n#Image\nQuestion Answering\nVisual Recognition\nVisual Text-only Weighted\nPositive Negative Weighted\nGPT-4V [32] + Prompt\nGPT-4V\n✗\n1\n0.866\n0.982\n0.924\n0.809\n0.992\n0.901\nGPT-4V [32] + Prompt\nGPT-4V\n✗\n5\n0.887\n0.987\n0.937\n0.851\n0.998\n0.925\nLLaVA [27]\nVicuna-13B\n✗\n-\n0.899\n0.659\n0.779\n0.000\n1.000\n0.500\nLLaVA [27] + Retriever\nVicuna-13B\n✗\n1\n0.912\n0.863\n0.887\n1.000\n0.025\n0.513\nLLaVA-LoRA [17]\nVicuna-13B\n✓\n1\n0.900\n0.583\n0.741\n0.988\n0.662\n0.825\nLLaVA-LoRA [17]\nVicuna-13B\n✓\n5\n0.935\n0.615\n0.775\n0.997\n0.444\n0.721\nMyVLM-LLaVA [2]\nVicuna-13B\n✓\n5\n0.912\n-\n-\n0.994\n0.845\n0.919\nYo’LLaVA [31]\nVicuna-13B\n✓\n5\n0.929\n0.883\n0.906\n0.949\n0.898\n0.924\nRAP-LLaVA(Ours)\nVicuna-13B\n✗\n1\n0.935\n0.938\n0.936\n0.979\n0.982\n0.980\nRAP-Phi3-V(Ours)\nPhi3-V-3.8B\n✗\n1\n0.941\n0.850\n0.896\n0.922\n0.988\n0.955\ngenerating a more personalized caption. LLaVA-LoRA and\nMyVLM can generate personalized captions, however, the\nlimited training data often results in imprecise outputs, par-\nticularly noticeable when multiple concepts are present in\nthe same image. In contrast, our models produce clear and\naccurate captions based on the database content, which also\nensures the reliability of the outputs. Additional examples\nof personalized captions generated by the models could be\nfound in Appendix E.\nQuantitative Evaluation. We employ recall, precision and\nthe comprehensive metric F1-score as our evaluation met-\nrics. Recall is calculated as the percentage of correct oc-\ncurrences of target concepts, while precision is the ratio\nof correct concept names to the total number of concept\nnames presented. The experimental results are shown in\nTable 3. Notably, the classification heads of MyVLM ex-\nhibit higher error rates when the number of positive images\nis limited, leading to weaker performance. Even with ad-\nditional concept information provided through retrieval, the\nvanilla LLaVA [27] still fails to effectively accomplish the\npersonalized generation task, underscoring the necessity of\nthe proposed dataset. Our models demonstrate superior per-\nformance in both recall and precision metrics, highlighting\nthe advantages of our RAP-MLLMs in data efficiency.\nInfluence of Number of Learned Concepts. In real-world\nscenario, users’ personal databases typically expand over\ntime. Next, we evaluate the performance of various meth-\nods with varying numbers of learned concepts. We extend\nthe database with hundreds of new concepts selected from\nRefCOCO dataset [20], ensuring no overlap with the test\ndataset. For LLaVA-LoRA and MyVLM, we provide im-\nages containing the target concepts along with their captions\nas training data, and we assess the models’ performance on\nthe original test dataset. The results are presented in Fig-\nure 4. More learned concepts result in increased recognition\nerrors, leading to a decline in performance for each model.\nOur RAP-MLLMs maintain the highest performance under\ndifferent settings.\n4.2. Personalized Question Answering\nSettings. In this section, we evaluate different methods on\nthe benchmark of personalized question answering intro-\nduced by Yo’LLaVA [31], which contains both visual and\ntext-only questions about user concepts. For each concept,\nwe generate a description to serve as its information in our\ndatabase. For LLaVA-LoRA, we feed these descriptions\n7\n\nand corresponding images to train the model to describe the\nconcept’s properties. Additionally, we incorporate text-only\nqueries and answers to enhance the model’s textual under-\nstanding. The training dataset for Yo’LLaVA and MyVLM\nconsists of 5 positive images with question answering pairs\nand 200 negative images per concept. For GPT-4V [32],\nimages and related concept information are provided as a\nsupplementary prompt. The evaluation metric is accuracy.\nAdditional details are provided in Appendix C.\nResults and Analysis. The experimental results are pro-\nvided in Table 4. LLaVA and LLaVA-LoRA both perform\nwell in visual based question answering, because substan-\ntial information of the target concept can be obtained from\nthe images. However, their performance is quite poor when\nimages of the target concept mentioned in the question are\nnot available. MyVLM performs well in visual question\nanswering but does not support text-only question answer-\ning. Yo’LLaVA excels in text-only question answering, but\nits performance is still limited by the insufficient informa-\ntion provided by the learned tokens of a concept. In con-\ntrast, our models demonstrate balanced performance in both\nvisual and text-only question answering. By providing a\nsingle image, our RAP-LLaVA surpasses baseline methods\nand achieves performance comparable to that of GPT-4V.\nWe present additional qualitative results in Table 13 and 14.\nVisual Recognition. We also evaluate the models’ recog-\nnition abilities for a more comprehensive comparison. The\nMLLMs are required to determine whether a personal con-\ncept is present in an image. We query them with ”Is 〈sks〉\nin the image? Answer with a single word.”, where 〈sks〉\nis replaced by corresponding concept name. For positive\nimages, the desired response is ”Yes” and ”No” for nega-\ntive. Results show that without knowledge of personal con-\ncepts, the vanilla LLaVA consistently outputs negative re-\nsponses. After training on target concepts, LLaVA-LoRA,\nMyVLM and YoLLaVA tend to give positive responses, but\nstruggle to differentiate between similar concepts, resulting\nin weaker performance on negative images. Our models\ndemonstrate exceptional performance in both positive and\nnegative scenarios, achieving the best overall results. We\npresent additional examples in Table 19 and 20.\n4.3. Cost of Personalization\nWe further compare the costs of personalization. As shown\nin Table 1, existing methods usually struggle with contin-\nuous updates or have high demands for training data. For\nfine-tuning based method like LLaVA-LoRA, while they\ncan achieve satisfactory performance, fine-tuning the model\neach time a new concept emerges incurs substantial compu-\ntational costs. MyVLM and Yo’LLaVA learn an embed-\nding or some new tokens to represent the new concept with-\nout updating the pre-trained MLLM’s parameters, however,\nthey require multiple labeled images of the target concept\n1\n2\n3\n4\n5\nK\n0.7\n0.8\n0.9\nRecall Rate\nN=45\nN=150\nN=300\nN=500\nFigure 5. Performance of Our Retriever. Top-K recall rates\nunder varying database size N.\nTable 5. We evaluate model’s performance with perfect retrieval,\nand test contributions of each dataset component.\nSetting\nRecall\nPrecision\nF1-score\nRAP-LLaVA\n93.51\n96.47\n94.97\nSkip retrieval\n96.16 (+2.7) 100.0 (+3.5) 98.04 (+3.1)\n- Text information\n94.91 (+1.4) 88.66 (-7.8)\n91.68 (-3.3)\n- Data augmentation 89.25 (-4.3) 98.01 (+1.5) 93.42 (-1.6)\n- Negative samples\n95.74 (+2.2) 58.21 (-38.3) 72.40 (-22.6)\nand a large number of negative images, which poses signifi-\ncant challenges for data collection. In contrast, our RAP re-\nquires only 1 image with its related information provided by\nthe user, achieving outstanding performance across various\npersonalized generation tasks. At the same time, by modi-\nfying images and descriptions in the database, RAP enables\nreal-time editing of personalized generation settings. We\npresent examples of real-time concept editing in Table 11.\n4.4. Ablation Study\nRetriever. The recall rate of the retriever is crucial for a\nRAG system. We first assess the retriever’s performance\non the personalized captioning dataset. We use the detec-\ntion model to identify potential concepts and retrieve the\nK concepts with the highest similarity from the database.\nThe Top-K recall rates for varying values of K and database\nsizes N are presented in Figure 5. Results indicate that as\nthe database size increases, the retriever’s performance de-\nclines, while a larger K generally enhances the recall rate.\nNotably, even with 500 personal concepts to remember, the\nTop-5 recall rate is still able to surpass 90%, which guaran-\ntees the effectiveness of our RAP framework.\nGeneration Ability of MLLM. We skip the recognition\nand retrieval processes, providing the MLLM with relevant\ninformation of each concept present in the image to evaluate\nthe generation capability of the trained MLLM. The results,\nshown in Table 5, indicate that when relevant concept in-\nformation is supplied, our RAP-LLaVA achieves superior\ngeneration performance, obtaining 100% precision without\n8\n\noutputting irrelevant concepts as well as a higher recall rate.\nText Information. We remove additional text information\nto examine its impact on personalization. The results are\nshown in Table 5 and 7. This additional text provides extra\ninformation that helps the model understand specific con-\ncepts, effectively improving the accuracy of the answers\nand benefiting personalized generation. Furthermore, it is\nespecially helpful for question answering, as it can provide\ninformation not directly obtainable from the image, such as\na user’s age and hobbies, or a pet’s preferred food, as illus-\ntrated in the example in Table 11.\nDataset Composition. We conduct experiments to assess\ncontribution of each component in our dataset. First, we re-\nmove data generated through data augmentation and train\nthe original LLaVA. The results displayed in Table 5 indi-\ncate a obvious decrease in the recall metric for image cap-\ntioning, resulting in lower overall performance. We further\nexclude constructed negative samples from the dataset and\nretrain the model, then we find that it performs poorly on\nprecision metric. This suggests a diminished ability to dis-\ncriminate against noisy concepts not present in the image.\n5. Conclusion\nIn this paper, we introduce the RAP framework for person-\nalizing MLLMs. This framework enables MLLMs to un-\nderstand infinite user-specific concepts, generate personal-\nized captions and respond to user-related queries. To en-\nhance the quality of the generated content and better align\noutputs with user’s configuration, we curate a large-scale\ndataset for personalized training of MLLMs and train a se-\nries of MLLMs to function as personalized assistants. Ex-\nperimental results show that RAP-MLLMs achieve excep-\ntional performance in various personalized generation tasks,\nwhile allowing real-time adjustments to generation settings.\nReferences\n[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadal-\nlah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree,\nArash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3\ntechnical report: A highly capable language model locally\non your phone. arXiv preprint arXiv:2404.14219, 2024. 2\n[2] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aber-\nman, and Daniel Cohen-Or.\nMyvlm: Personalizing vlms\nfor user-specific queries. arXiv preprint arXiv:2403.14599,\n2024. 2, 3, 6, 7\n[3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. Self-rag: Learning to retrieve, gen-\nerate, and critique through self-reflection.\narXiv preprint\narXiv:2310.11511, 2023. 3\n[4] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas\nM¨uller, and Bj¨orn Ommer. Retrieval-augmented diffusion\nmodels. In Advances in Neural Information Processing Sys-\ntems 35: Annual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022, 2022. 3\n[5] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam W Cohen. Murag: Multimodal retrieval-augmented\ngenerator for open question answering over images and text.\narXiv preprint arXiv:2210.02928, 2022. 3\n[6] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-\nravit Changpinyo,\nAlan Ritter,\nand Ming-Wei Chang.\nCan pre-trained vision and language models answer vi-\nsual information-seeking questions?\narXiv preprint\narXiv:2302.11713, 2023. 12\n[7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xing-\ngang Wang, and Ying Shan.\nYolo-world:\nReal-time\nopen-vocabulary object detection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16901–16911, 2024. 4, 5, 12\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6,\n2023. 2\n[9] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia\nSchmid, and Deva Ramanan.\nTao: A large-scale bench-\nmark for tracking any object.\nIn Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part V 16, pages 436–454. Springer,\n2020. 5, 14\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[11] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jin-\nliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.\nRetrieval-augmented generation for large language models:\nA survey. arXiv preprint arXiv:2312.10997, 2023. 3\n[12] Gemini-Team. Gemini 1.5: Unlocking multimodal under-\nstanding across millions of tokens of context. arXiv preprint\narXiv:2403.05530, 2024. 2, 3, 4, 5\n[13] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin\nByeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei\nLiu. Regiongpt: Towards region understanding vision lan-\nguage model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13796–\n13806, 2024. 3\n[14] Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin,\nYuchen Liu, Richard Zhang, and Tobias Hinz.\nPersonal-\nized residuals for concept-driven text-to-image generation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8186–8195, 2024. 3\n[15] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang,\nKaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xi-\nangyu Yue. Onellm: One framework to align all modalities\nwith language. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 26584–\n26595, 2024. 2, 3\n9\n\n[16] I Ho, Jie Song, Otmar Hilliges, et al. Sith: Single-view tex-\ntured human reconstruction with image-conditioned diffu-\nsion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 538–549, 2024.\n5\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models.\nIn\nThe Tenth International Conference on Learning Represen-\ntations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net, 2022. 6, 7, 12\n[18] Jeff Johnson, Matthijs Douze, and Herv´e J´egou.\nBillion-\nscale similarity search with gpus. IEEE Trans. Big Data,\n7(3):535–547, 2021. 6, 12\n[19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H.\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-\ntau Yih. Dense passage retrieval for open-domain question\nanswering. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 6769–6781. As-\nsociation for Computational Linguistics, 2020. 3\n[20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara L. Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language Process-\ning, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A\nmeeting of SIGDAT, a Special Interest Group of the ACL,\npages 787–798. ACL, 2014. 4, 5, 7, 14\n[21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931–1941, 2023. 3, 5, 14\n[22] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-\ntrained subject representation for controllable text-to-image\ngeneration and editing. Advances in Neural Information Pro-\ncessing Systems, 36, 2024. 3\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 2\n[24] Jiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, and Hideki\nNakayama. Evcap: Retrieval-augmented image captioning\nwith external visual-name memory for open-world compre-\nhension. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13733–\n13742, 2024. 3\n[25] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca,\nand Bill Byrne. Fine-grained late-interaction multi-modal re-\ntrieval for retrieval augmented visual question answering. In\nAdvances in Neural Information Processing Systems 36: An-\nnual Conference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10\n- 16, 2023, 2023. 3\n[26] Xi Victoria Lin,\nXilun Chen,\nMingda Chen,\nWeijia\nShi, Maria Lomeli, Rich James, Pedro Rodriguez, Ja-\ncob Kahn, Gergely Szilvasy, Mike Lewis, et al.\nRa-dit:\nRetrieval-augmented dual instruction tuning. arXiv preprint\narXiv:2310.01352, 2023. 3\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning, 2023. 3,\n5, 6, 7, 12, 13, 14\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning, 2023. 2, 3, 4\n[29] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings of\nInternational Conference on Computer Vision (ICCV), 2015.\n5, 14\n[30] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al. Wonder3d: Sin-\ngle image to 3d using cross-domain diffusion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9970–9980, 2024. 5\n[31] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh\nOjha, and Yong Jae Lee. Yo’llava: Your personalized lan-\nguage and vision assistant. arXiv preprint arXiv:2406.09400,\n2024. 2, 3, 7, 13\n[32] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 2, 3, 7, 8\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, pages 8748–\n8763. PMLR, 2021. 4, 6, 12\n[34] Rita\nRamos,\nDesmond\nElliott,\nand\nBruno\nMartins.\nRetrieval-augmented image captioning.\narXiv preprint\narXiv:2302.08268, 2023. 3\n[35] Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fa-\nhad S. Khan. Llava++: Extending visual capabilities with\nllama-3 and phi-3, 2024. 2, 5, 6\n[36] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779–788, 2016. 4\n[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2023, Vancouver, BC,\nCanada, June 17-24, 2023, pages 22500–22510. IEEE,\n2023. 3\n[38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115:211–252, 2015. 5, 14\n[39] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In 2019\nIEEE/CVF International Conference on Computer Vision,\n10\n\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 8429–8438. IEEE, 2019. 5, 14\n[40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung.\nIn-\nstantbooth: Personalized text-to-image generation without\ntest-time finetuning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8543–8552, 2024. 3\n[41] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu,\nand Min Xu. Eragent: Enhancing retrieval-augmented lan-\nguage models with improved accuracy, efficiency, and per-\nsonalization. arXiv preprint arXiv:2405.06683, 2024. 3\n[42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B\nHashimoto. Stanford alpaca: An instruction-following llama\nmodel, 2023. 2\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[44] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan\nHe, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun\nWang, et al. Internvideo2: Scaling video foundation mod-\nels for multimodal video understanding.\narXiv preprint\narXiv:2403.15377, 2024. 2, 3\n[45] Stanisław Wo´zniak, Bartłomiej Koptyra, Arkadiusz Janz,\nPrzemysław Kazienko, and Jan Koco´n. Personalized large\nlanguage models. arXiv preprint arXiv:2402.09269, 2024. 3\n[46] Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao,\nand Jin Yu.\nUnderstanding human preferences: Towards\nmore personalized video to text generation.\nIn Proceed-\nings of the ACM on Web Conference 2024, pages 3952–3963,\n2024. 3\n[47] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen\nZhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhtu-\nrina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval\nmeets long context large language models. arXiv preprint\narXiv:2310.03025, 2023. 3\n[48] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3\n[49] Chun-Hsiao Yeh, Bryan Russell, Josef Sivic, Fabian Caba\nHeilbron, and Simon Jenni.\nMeta-personalizing vision-\nlanguage models to find named instances in video. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 19123–19132, 2023. 3\n[50] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,\nTong Xu, and Enhong Chen. A survey on multimodal large\nlanguage models. arXiv preprint arXiv:2306.13549, 2023. 2\n[51] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.\nMaking retrieval-augmented language models robust to irrel-\nevant context. arXiv preprint arXiv:2310.01558, 2023. 3\n[52] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin\nJin, Wenjun Zeng, and Zhibo Chen.\nInpaint anything:\nSegment anything meets image inpainting. arXiv preprint\narXiv:2304.06790, 2023. 5\n[53] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi\nLiu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming\nRen, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for ex-\npert agi.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 9556–\n9567, 2024. 12\n[54] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-\nmodiffuse: Retrieval-augmented motion diffusion model. In\nIEEE/CVF International Conference on Computer Vision,\nICCV 2023, Paris, France, October 1-6, 2023, pages 364–\n373. IEEE, 2023. 3\n[55] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui\nQian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang,\nLinke Ouyang, et al. Internlm-xcomposer-2.5: A versatile\nlarge vision language model supporting long-contextual in-\nput and output. arXiv preprint arXiv:2407.03320, 2024. 2,\n3\n[56] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-\njun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nLi, and Yu Qiao.\nLlama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention.\narXiv preprint\narXiv:2303.16199, 2023. 2\n[57] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-\nof-interest. arXiv preprint arXiv:2307.03601, 2023. 3\n[58] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao,\nXuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao\nGuo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal\ninformation for augmented generation: A survey.\narXiv\npreprint arXiv:2303.10868, 2023. 3\n[59] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. A survey of large language mod-\nels. arXiv preprint arXiv:2303.18223, 2023. 1\n[60] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 3\n11\n\nA. Appendix Overview\n• Section B: Additional evaluations of our models.\n• Section C: More experimental details.\n• Section D: More details of RAP dataset.\n• Section E: Additional demonstrations.\n• Section F: Analysis on limitations of our work.\n• Section G: Examples of the personalized database.\nB. Additional Evaluation Results\nB.1. Ablation Study\nDataset Composition. We conduct ablation experiments\non the question answering and recognition benchmark, ex-\nperimental results are present in Table 7. The results fur-\nther demonstrate that our data augmentation and the con-\nstructed negative samples also contribute to the model’s per-\nformance.\nTable 6.\nEvaluation on Knowledge-intensive Multimodal\nBenchmarks. KB: Knowledge Base.\nMethod\nMMMU [53] InfoSeek [6]\nLLaVA [27]\n0.364\n0.205\nLLaVA-LoRA [17]\n0.359\n0.205\nRAP-LLaVA\n0.361\n0.218\nRAP-LLaVA(With KB)\n0.369\n0.344\nMultimodal Benchmark. We also evaluate our model’s\nperformance on several traditional multimodal benchmarks,\nincluding MMMU [53] and InfoSeek [6]. We assess our\nmodels’ performance both with and without external knowl-\nedge base. For MMMU [53], we use 30K images paired\nwith corresponding captions from Wikipedia as the exter-\nnal knowledge base. During testing, we retrieve the three\nmost similar images based on the question’s image and in-\ncorporate only the textual knowledge to the input. For In-\nfoSeek [6], we randomly sample 5K questions from the val-\nidation set and construct a knowledge base containing 50K\nentities from Wikipedia database provided by the authors,\nwhich includes all relevant entities associated with the ques-\ntions. For each question, we retrieve the most similar entity\nand add only the textual knowledge to the input.\nWe evaluate on the validation set of MMMU, and 5K\nquestions sampled from the validation set of InfoSeek. We\nuse the official scripts to get the results, which are pre-\nsented in Table 6. From the results, our RAP-LLaVA retains\nmost general knowledge of the original LLaVA [27]. It also\nequips the MLLM with the ability to retrieve information\nfrom an external knowledge base, demonstrating superior\nperformance in knowledge intensive tasks.\n100\n200\n300\n400\n500\nNumber of Concepts\n101\n102\n103\n104\n105\nTime Cost(second)\nMyVLM\nYo’LLaVA\nLLaVA-LoRA\nRAP-LLaVA\nFigure 6. Time Cost of Personalization. We conduct experiment\nwith 2 A800 GPUs.\nB.2. Time Cost of Personalization\nWe also evaluate the time cost associated with different\nmethods for learning a set of user’s concepts. The results\nare presented in Figure 6. MyVLM has to train an external\nrecognition model for each concept and learn an embedding\nto adjust the model’s outputs. Similarly, Yo’LLaVA needs\nto learn new tokens for each concept. During the optimiza-\ntion process, both approaches necessitate multiple forward\nand backward pass of the MLLM, resulting in significant\ntime consumption. In contrast, our RAP only requires time\nfor encoding the image and adding its embedding to the\ndatabase, which can be accomplished in just a few seconds.\nThis significantly enhances the convenience and practicality\nof our models in practical applications.\nB.3. Standard Image Captioning Metrics\nWe further evaluate the models using standard image cap-\ntioning metrics. The results are shown in Table 8. Despite\nthe additional concept information is offered, LLaVA’s per-\nformance declines due to its inability to effectively utilize\nthe information. In contrast, RAP-LLaVA achieves compa-\nrable performance to LLaVA on general image captioning\ntasks while enabling personalization.\nC. More Experimental Details\nImplementation details. We utilize YOLO-Worldv2-X [7]\nas the detection model, setting detection classes to include\nall categories stored in the database to reduce the interven-\ntions from unrelated objects. We construct a multimodal re-\ntriever using Facebook AI Similarity Search (FAISS) [18],\nemploying a pre-trained CLIP ViT-L/14-336 [33] as the vi-\nsual encoder. Each key in the database is generated by in-\nputting a concept’s image into the CLIP visual encoder, re-\nsulting in a 768-dimensional vector. Considering the re-\nstriction of context length of the backbone language model,\nwe retrieve the 2 most similar images from the database for\neach region of interest. Then we select 2 and 3 different\nconcepts with the highest similarity among all as supple-\nmentary inputs for RAP-LLaVA and RAP-Phi3-V, respec-\n12\n\nTable 7. Ablation studies on Question Answering and Visual Recognition. Weighted results are computed as arithmetic means.\nMethod\nQuestion Answering\nVisual Recognition\nVisual\nText\nWeighted\nPositive\nNegative\nWeighted\nRAP-LLaVA\n0.935\n0.938\n0.936\n0.979\n0.982\n0.980\n- Data augmentation\n0.924 (-0.011)\n0.918 (-0.020)\n0.921 (-0.015)\n0.943 (-0.036)\n0.988 (+0.006)\n0.965 (-0.015)\n- Negative samples\n0.918 (-0.017)\n0.933 (-0.005)\n0.925 (-0.011)\n0.958 (-0.021)\n0.985 (+0.003)\n0.971 (-0.009)\n- Text information\n0.935\n0.908 (-0.030)\n0.921 (-0.015)\n0.988 (+0.009)\n0.930 (-0.052)\n0.959 (-0.021)\nTable 8. Quantitative Evaluation with Standard Image Captioning Metrics. We compute standard image captioning metrics for\npersonalized captions generated by RAP-LLaVA. For each image, we treat all 5 augmented captions as the set of ground truth captions.\nThe results are averaged across all three evaluations and all concepts.\nModel\nB1\nB2\nB3\nB4\nMETEOR\nROUGE L\nCIDEr\nSPICE\nLLaVA [27]\n0.177\n0.100\n0.057\n0.032\n0.138\n0.240\n0.428\n0.136\nLLaVA [27] + Retriever\n0.087\n0.038\n0.014\n0.005\n0.074\n0.132\n0.025\n0.020\nRAP-LLaVA\n0.178\n0.094\n0.058\n0.037\n0.215\n0.189\n0.388\n0.108\ntively.\nBaselines. For MyVLM, we find that when the train-\ning data is very limited, it is quite hard for the classifica-\ntion head to work effectively. Therefore, we use data aug-\nmentation to help improve its performance. Specifically, we\ncrop the single image into several pieces containing the tar-\nget concept to improve the accuracy of classification heads.\nTo distinguish between multiple possible different concepts\nthat may appear in the image, we use 〈sks1〉, 〈sks2〉. . . as\nconcept identifiers. For YoLLaVA, we present its experi-\nmental results as reported in the original paper [31].\nD. Details of Dataset\nD.1. Dataset Composition\n• We provide a summary of the composition of our\ndataset in Figure 7, which visually represents the dis-\ntribution of different components.\n• Table 9 presents detailed numerical data for each part.\n• In Table 10, we specify the sources for each component\nof our dataset.\nD.2. Instructions\nIn this section, we present the instruction templates used to\ncreate our dataset:\n• Table 21 contains instructions for visual grounding and\nrecognition.\n• Table 22 includes example instructions for image cap-\ntioning.\n• Table 23 presents example instructions for image de-\nscription.\n• Table 24 presents example questions used for question\nanswering synthesis.\nE. Additional Demonstrations\nIn this section, we provide more qualitative results obtained\nby various models.\n• In Table 11,\nwe demonstrate how our models\nachieve real-time editing of concepts by modifying the\ndatabase.\n• In Table 12, we demonstrate the real-time addition of\nnew concepts by updating the database.\n• In Table 13, we present qualitative results on personal-\nized conversation of RAP-LLaVA.\n• In Table 14, we present qualitative results on personal-\nized conversation of RAP-Phi3-V.\n• In Table 15, we present additional image captions gen-\nerated by RAP-LLaVA and other methods.\n• In Table 16, we present additional image captions gen-\nerated by RAP-Phi3-V and other methods.\n• In Table 17, we provide demonstrations of image de-\nscription generated by RAP-LLaVA and LLaVA.\n• In Table 18, we provide demonstrations of image de-\nscription generated by RAP-Phi3-V and Phi3-V.\n• In Table 19 and 20, we provide results on visual recog-\nnition of RAP-LLaVA. It also has the ability to give\nprecise bounding box of specific concept in the image.\nF. Limitation\nOur proposed RAP framework is a retrieval-based method.\nThe limitations of RAP mainly concern the additional com-\nputational cost of generation and the precision of the re-\ntriever.\nWhile incorporating external information effec-\ntively generates more specific answers, it inevitably in-\ncreases the context length for MLLMs, leading to additional\ncomputational overhead during the generation process. We\n13\n\nFigure 7. Composition of our dataset.\nImage Description\nQuestion Answering\nImage Captioning\nLLaVA-Instruction\nRecognition\nVisual Grounding\nTable 9. Statistics of our dataset.\nType\nSize\nVisual Grounding\n100K\nRecognition\n40K\nCaption & Description\n37K\nQuestion Answering\n16K\nLLaVA-Instruction\n67K\nTotal\n260K\nTable 10. Data source.\nType\nSource Dataset\nVisual Grounding\nRefCOCO [20], TAO [9]\nILSVRC2015-VID [38], Object365 [39]\nRecognition\nCustomConcept101 [21], CelebA [29]\nCaption & Description\nRefCOCO [20], TAO [9]\nObject365 [39], CustomConcept101 [21]\nQuestion Answering\nRefCOCO [20], TAO [9]\nObject365 [39], CustomConcept101 [21]\nCelebA [29]\nLLaVA-Instruction\nLLaVA-Instruct-665K [27]\nwill further explore ways to mitigate this computational bur-\nden. Another limitation is the personalization performance\nof our RAP-MLLMs depends on the retriever’s capability\nThis proposes need for a robust multi-modal retriever that\ncan discern intricate features to enhance retrieval precision.\nDespite these limitations, RAP offers a timely solution for\nMLLM personalization. By retrieving from a user’s specific\ndatabase, RAP facilitates reliable and flexible personalized\ngeneration, which is valuable in practical applications.\nG. Examples of the Personalized Database\nWe give some visualized examples of our database in Table\n25. For each concept in the database, users need to pro-\nvide an image with its name and optional text information.\nDuring inference, the images, names and other information\nof retrieved concepts are integrated into the input for the\nMLLM. Users have the flexibility to define the name and\npersonalized description based on their preferences, and our\nRAP-MLLMs will generate answers according to the pro-\nvided information.\n14\n\nTable 11. Examples of Concept Editing. Based on the information recorded in the database, our RAP-LLaVA can provide reliable and\naccurate answers.\nImage\nConcept\nInformation in the Database\nmy dog\n1. A white and gray dog with long fur. He has black eyes. His favorite\nfood is chicken.\n2. A white and gray dog with long fur. He has black eyes. His favorite\nfood is beef.\nQuestion: What is 〈my dog〉’s favorite food?\n1. RAP-LLaVA: Chicken.\n2. RAP-LLaVA: His favorite food is beef.\nTable 12. Examples of Concept Updating. The first caption is generated when toy2 not yet stored in the database. Once the new concept\nis added, RAP-LLaVA can recognize both toy1 and toy2.\nImage\nConcept\nInformation in the Database\ntoy1\nA plush toy. It is orange with a yellow belly and a brown nose.\ntoy2\nThis is a plush toy of the bluey character. It is a light blue color with a\npurple patch on its head, and its ears are yellow.\nQuestion: Give a caption of this image.\n1. RAP-LLaVA: 〈toy1〉is ready for bed! He’s snuggled up with his friend, a\nblue and yellow dog plushie. They’re both looking so cozy and ready for a\ngood night’s sleep!\n(Add new concept 〈toy2〉to the database)\n2. RAP-LLaVA: 〈toy1〉and 〈toy2〉are ready for bed! They are both snuggled\nunder the covers with their favorite stuffed animals.\n15\n\nTable 13. Examples of personalized conversations obtained by RAP-LLaVA.\nImage\nConcept\nInformation in the Database\nmy cat\nA tabby cat with its eyes closed is laying on a light grey surface. The\ncat’s fur is a mix of light and dark brown stripes.\nQuestion: Where is 〈my cat〉?\nRAP-LLaVA: 〈my cat〉is on a staircase.\nQuestion: What is the type of 〈my cat〉?\nRAP-LLaVA: 〈my cat〉is a tabby cat.\nQuestion: What is the color of 〈my cat〉?\nRAP-LLaVA: 〈my cat〉is a tabby cat with a mix of light and dark brown\nstripes.\nImage\nConcept\nInformation in the Database\nA\n〈A〉is a smiling Asian man with short black hair, wearing glasses and a\ngrey sweater.\nQuestion: Can you see 〈A〉in this photo?\nRAP-LLaVA: Yes, 〈A〉is captured in this image.\nQuestion: What color is 〈A〉’s shirt?\nRAP-LLaVA: 〈A〉is wearing a purple shirt.\nQuestion: What is 〈A〉’s right hand holding?\nRAP-LLaVA: Chopsticks.\nImage\nConcept\nInformation in the Database\ndug\n〈dug〉is a golden retriever from the movie Up. He has a brown coat and\na big, goofy smile, wearing a collar with some round objects.\n(Text-only conversation)\nQuestion: Can you describe 〈dug〉in detail?\nRAP-LLaVA: 〈dug〉is a golden retriever from the movie Up. He has a brown\ncoat and a big, goofy smile. He is wearing a collar with some round objects.\nHe looks very happy and playful.\nImage\nConcept\nInformation in the Database\nP\n〈P〉is a bald man with a red beard.\n(Text-only conversation)\nQuestion: Can you describe 〈P〉’s appearence in detail?\nRAP-LLaVA: 〈P〉is a bald man with a red beard. He is wearing orange shorts\nand a blue shirt. He is sitting in a black director’s chair with his legs crossed.\n16\n\nTable 14. Examples of personalized conversations obtained by RAP-Phi3-V.\nImage\nConcept\nInformation in the Database\nbo\n〈bo〉is a well-groomed, medium-sized Shiba Inu with a thick, cinnamon-\ncolored coat, cream accents, alert eyes, and a black collar.\nQuestion: What is 〈bo〉doing?\nRAP-Phi3-V: 〈bo〉is lying on its side on a fluffy, gray dog bed. It is resting\nits head on a white stuffed toy. 〈bo〉has a thick, cinnamon-colored coat with\ncream accents, alert eyes, and a black collar. 〈bo〉looks very relaxed and\ncontent.\nQuestion: Where do you think he likes to play?\nRAP-Phi3-V: It’s hard to say for sure, but it’s possible that 〈bo〉enjoys going\nto the park. Many dogs like to go to the park to play and get exercise.\nImage\nConcept\nInformation in the Database\nAnya\nA young girl with pink hair and big green eyes.\nBond\nA white dog wearing a black bow tie.\nQuestion: What are they doing, describe in detail.\nRAP-Phi3-V: 〈Anya〉is feeding 〈Bond〉, a white dog wearing a black bow tie.\n〈Bond〉is eating out of a bowl. 〈Anya〉is kneeling down and watching 〈Bond〉\neat. 〈Bond〉is standing on a tiled floor and looking at the bowl. 〈Anya〉is\nwearing a pink sweater and has pink hair. 〈Bond〉is wearing a black bow tie.\nThe scene is indoors, and there is a TV in the background.\nQuestion: What is 〈Anya〉wearing?\nRAP-Phi3-V: 〈Anya〉is wearing a black sweater.\nImage\nConcept\nInformation in the Database\nC\n〈C〉is a young woman with long blonde hair, wearing a white sleeveless\nshirt. She has bright red lipstick and light makeup.\n(Text-only conversation)\nQuestion: What color is 〈C〉’s hair?\nRAP-Phi3-V: 〈C〉’s hair is blonde.\nQuestion: What color of clothes does she like?\nRAP-Phi3-V: She likes white clothes.\n17\n\nTable 15. Additional qualitative comparison on image captioning between RAP-LLaVA and other methods.\nImage\nCaption\ndoll*\nLLaVA: A blue coffee cup sits next to two dolls on a window sill.\nLLaVA-LoRA: 〈doll*〉positioned on a windowsill with a blue mug contain-\ning pencils\nMyVLM: 〈doll*〉is found sitting beside a window ledge with a cup and tooth-\nbrushes nearby. In the image, 〈doll*〉appears to be peacefully resting, possi-\nbly enjoying the view or taking a break.\nRAP-LLaVA(Ours): 〈doll*〉and her friend are ready for a fun day at the\noffice! They’re ready to take on any challenge that comes their way.\nmug*\nLLaVA: A wooden shelf holds several potted plants, including a large clay\npot and a small vase. The shelf is positioned near a wall, and the plants are\narranged in a visually appealing manner.\nLLaVA-LoRA: 〈mug*〉on a wooden shelf next to a plant and two potted\nplants.\nMyVLM: 〈mug*〉on a a blue and white tiled floor next to indoor plants, a\nshelf with pots and a vase.\nRAP-LLaVA(Ours): A brown ceramic tiki mug with the face of a tiki head,\n〈mug*〉, sits on a wooden shelf with two potted plants.\nmy cat\nLLaVA: A cat is sitting on a ledge near a staircase.\nLLaVA-LoRA: 〈my cat〉sitting on a staircase, looking down.\nMyVLM: 〈my cat〉in a typical pose, gripping the banister of a staircase. He\nis wearing a collar.\nRAP-LLaVA(Ours): 〈my cat〉is relaxing on the stairs. Look at those eyes!\nWhat a beautiful cat.\nteapot*\nmy cat\nLLaVA: A cat is laying on a blanket on a couch, next to a colorful teapot.\nLLaVA-LoRA: 〈my cat〉reclining on a chair with a 〈teapot*〉beside\nMyVLM: 〈teapot*〉on the couch near an orange, yellow, and blue teapot. The\ncat is laying on a blanket, and the teapot is placed on a table.\nRAP-LLaVA(Ours): 〈my cat〉is taking a well-deserved nap next to 〈teapot*〉.\nH\nT\nLLaVA: A man and a woman are walking down a street, with the man taking\na selfie using his cell phone. They are both smiling as they walk, and the man\nis holding his phone ...\nLLaVA-LoRA: 〈H〉and 〈K〉are looking happy as they walk down the street\ntogether.\nMyVLM: 〈H〉and 〈T〉are looking very serious as they take in the scenery.\nRAP-LLaVA(Ours): 〈T〉is walking down the street with her friend 〈H〉.\nparrot1\nparrot2\nLLaVA: Two colorful parrots are perched on a palm tree, sitting next to each\nother.\nLLaVA-LoRA: 〈parrot1〉and 〈parrot2〉are looking very serious as they perch\non a palm frond.\nMyVLM: 〈parrot1〉and 〈parrot2〉look so cute together.\nRAP-LLaVA(Ours): 〈parrot1〉and 〈parrot2〉are both beautiful birds. 〈par-\nrot1〉has a bright red beak and 〈parrot2〉has a light blue head and pink feet.\nThey are both perched on a branch.\n18\n\nTable 16. Additional qualitative comparison on image captioning between RAP-Phi3-V and other methods.\nImage\nCaption\nsheep*\nPhi3-V: A small white sheep figurine is sitting on a wooden table next to a\nplant.\nLLaVA-LoRA: 〈sheep*〉positioned next to a potted plant on a wooden table.\nMyVLM: 〈sheep*〉positioned within a square shape within a vase made of\nmetal positioned on a table made of wood.\nRAP-Phi3-V(Ours): 〈sheep*〉is ready to play on the shelf with the plant!\ndog1\nPhi3-V: A brown and white dog is sitting on the grass with a leash attached to\nit.\nLLaVA-LoRA: 〈dog1〉sitting on the grass with a look of determination.\nMyVLM: on a cheerful note, 〈dog1〉restfully resides, surrounded by the\nsoothing sounds of nature.\nRAP-Phi3-V(Ours): This is a picture of 〈dog1〉, a cute dog with light brown\nand white fur. He looks happy and playful.\nbull*\nPhi3-V: A bottle of wine is displayed next to a wooden cow statue.\nLLaVA-LoRA: 〈bull*〉positioned next to a bottle of supreme cabernet sauvi-\ngnon.\nMyVLM: 〈gold pineapple〉pars the bottle of wine. A bottle of wine sits next\nto a bottle of wine.\nRAP-Phi3-V(Ours): 〈bull*〉stands guard over a bottle of SUPREME wine.\nfunko pop*\nPhi3-V: A small figurine of a basketball player is placed on top of a box.\nLLaVA-LoRA: 〈funko pop*〉sitting on a desk next to a box and a monitor.\nMyVLM: 〈funko pop*〉stands out in a photo set against a dark background.\nRAP-Phi3-V(Ours): 〈funko pop*〉is ready to dominate the court!\nJ\nK\nPhi3-V: A man and woman are sitting at a table in a restaurant, having a\nconversation and enjoying their time together.\nLLaVA-LoRA: 〈J〉and 〈K〉enjoying a meal together.\nMyVLM: 〈J〉and 〈K〉enjoying their time together.\nRAP-Phi3-V(Ours): 〈J〉and 〈K〉are enjoying a drink at a bar.\ncup*\nhead*\nPhi3-V: A small statue of a man wearing glasses is sitting next to a cup on a\ntable.\nLLaVA-LoRA: A table hosts 〈cup*〉and a sculpture of a head wearing\nglasses.\nMyVLM: A table with 〈cup*〉, an espresso cup, and a ceramic mug depicting\na face wearing glasses sits on a wooden surface.\nRAP-Phi3-V(Ours): 〈head*〉and 〈cup*〉are ready for their morning coffee.\n19\n\nTable 17. Qualitative results of personalized image description obtained by RAP-LLaVA.\nImage\nDescription\nmy cat\nLLaVA: The image features a large, fluffy cat sitting on a countertop, possibly\nin a kitchen. The cat is positioned near a sink and appears to be resting or\nsleeping. The countertop is covered with a towel, providing a comfortable\nsurface for the cat to lay on ...\nRAP-LLaVA(Ours): The image shows 〈my cat〉, a tabby cat with its eyes\nclosed, laying on a light grey surface. The cat’s fur is a mix of light and dark\nbrown stripes. The cat is relaxed and comfortable.\ntoy3\nLLaVA: The image features a small metal robot figure, possibly a toy,\nstanding on a table or counter. The robot is positioned in the center of the\nscene, surrounded by a variety of leaves scattered around it. The leaves are of\ndifferent sizes and ...\nRAP-LLaVA(Ours): The image shows a toy figurine of Iron Man, a super-\nhero from Marvel comics. He is wearing a silver and red suit of armor and is\nstanding on a surface covered in leaves. The toy is called 〈toy3〉.\npillow*\nLLaVA: The image features a chair with a pillow on it, which has a picture\nof a sheep on it. The chair is positioned in front of a desk with a laptop and\na monitor on it. The laptop is placed on the left side of the desk, while the\nmonitor is on the right side ...\nRAP-LLaVA(Ours): The image shows a chair with a 〈pillow*〉on it. The\n〈pillow*〉is a pattern of flowers, leaves, and a sheep wearing glasses with a\nflower crown. There is also a laptop and a computer monitor in the back-\nground.\ntoy4\nLLaVA: The image features a bookshelf filled with various books. On the\nshelf, there are two Rubik’s cubes, one green and one white, sitting next to\neach other. The bookshelf is well-stocked with books, occupying most of ...\nRAP-LLaVA(Ours): The image shows a bookshelf with books on it. There\nare two toys on the shelf. One is a 〈toy4〉, which looks like a cat with orange\nfuzzy ears and a brown cord attached. The other is a Rubik’s cube.\ntoy2\ntoy1\nLLaVA: The image features a bed with two stuffed animals, a dog and a cat,\nlying on it. The dog is positioned on the left side of the bed, while the cat is\non the right side. The stuffed animals are placed close to each other, giving\nthe impression of a cozy and ...\nRAP-LLaVA(Ours): The image shows two plush toys, 〈toy1〉and 〈toy2〉,\nlying in bed together. 〈toy2〉is a light blue plush with a purple patch on its\nhead and yellow ears. 〈toy1〉is an orange plush with a yellow belly and a\nbrown nose. They are both looking at the camera.\nBond\nAnya\nLLaVA: The image features a young girl hugging a large white dog in a room.\nThe girl is wearing a black dress and appears to be enjoying her time with the\ndog. The dog is positioned close to the girl, and they seem to be sharing a\ntender moment together ...\nRAP-LLaVA(Ours): 〈Anya〉is a young girl with pink hair and big green\neyes. She is wearing a black dress and black shoes. She is standing in a room\nwith wooden floors and is hugging a white dog named 〈Bond〉. 〈Bond〉is\nwearing a black bow tie. The dog is standing on its hind legs and is looking at\nthe camera. The room is decorated with a blue rug and a wooden cabinet ...\n20\n\nTable 18. Qualitative results of personalized image description obtained by RAP-Phi3-V.\nImage\nDescription\nsheep*\nPhi3-V: The image features a display of various animal figurines on a shelf.\nThere are three giraffes, two of which are standing and one sitting, placed next\nto each other. A sheep figurine is also present on the shelf, located between\nthe giraffes ...\nRAP-Phi3-V(Ours): The image shows a white 〈sheep*〉sitting on a shelf\nnext to a giraffe figurine. The 〈sheep*〉is made of felt and has black eyes and\na black nose. The giraffe figurine is made of metal and has a colorful necklace\naround its neck. The shelf is white ...\nmug*\nPhi3-V: The image features a wooden shelf with two wine bottles and two\nwine glasses. One of the wine bottles is positioned towards the left side of the\nshelf, while the other is located more towards the center. The wine glasses\nare placed on the right side of the ...\nRAP-Phi3-V(Ours): The image shows a wooden shelf with three glassware\nitems on it. The top glassware item is a 〈mug*〉with a tiki head design. The\nother glassware items are wine glasses. There is a sign on the wine bottle that\nsays ”SUPREME”. There is also a plant in a pot in the background.\nmy dog\nPhi3-V: The image features two dogs playing together in a grassy yard. One\ndog is located on the left side of the yard, while the other dog is on the right\nside. They are both running and enjoying their time outside. In the back-\nground, there are several chairs ...\nRAP-Phi3-V(Ours): The image shows two dogs playing in a grassy backyard.\nThe dog on the left is a white and gray dog with long fur. The dog on the right\nis a black and brown dog with long fur. The dogs are both running and playing\non the grass. The dog on the left is named 〈my dog〉.\ntoy4\nPhi3-V: The image features a white shelf with several decorative items on it.\nThere are three chicken-shaped decorations hanging from the shelf, each with\na different design. Additionally, there are three more chicken decorations\nplaced on the shelf, making ...\nRAP-Phi3-V(Ours): The image shows three handmade stuffed toys. The toys\nare all shaped like chickens and are hanging on a white shelf. The toys are\nall different colors and have different designs on them. The toy on the left is\ncalled 〈toy4〉.\ncharacter1\ncharacter2\nPhi3-V: The image features a group of three cartoon characters, two of which\nare wearing hats. They are standing together on a sidewalk, with one character\npositioned towards the left side of the frame, another in the center, and ...\nRAP-Phi3-V(Ours): The image shows 〈character1〉and 〈character2〉stand-\ning in front of a brick wall. 〈character1〉is a cartoon fox wearing a yellow\nbutton-down shirt and a purple tie. He has big eyes, fluffy fur, and a sly grin\non his face. 〈character2〉is a cartoon rabbit with large ears, a police officer’s\nhat, and a red and yellow safety vest. She has a sweet, innocent look on ...\ndog2\nbaby*\nPhi3-V: The image features a baby lying on a bed, with a dog resting on the\nfloor nearby. The baby appears to be playing with a toy, possibly a puzzle,\nwhile the dog watches attentively ...\nRAP-Phi3-V(Ours): The image shows a baby, 〈baby*〉, sleeping in a carrier.\nThe baby is wrapped in a colorful blanket and is secured by a black strap. A\nfrench bulldog, 〈dog2〉, is sleeping next to the baby. The baby and dog are\nboth sleeping peacefully.\n21\n\nTable 19. Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We use green rectangle to show the bounding\nbox in the image.\nImage\nConversation\nhead*\nQuestion: Is 〈head*〉in this image?\nRAP-LLaVA: Yes, 〈head*〉is in the photo.\nQuestion: Give 〈head*〉’s bounding box in the image.\nRAP-LLaVA: [0.57, 0.67, 0.68, 0.84]\nhead*\nQuestion: Is 〈head*〉in this image?\nRAP-LLaVA: No, 〈head*〉is not part of this photograph.\ntoy*\nQuestion: Can you identify if 〈toy*〉is captured in this picture?\nRAP-LLaVA: Yes, 〈toy*〉is visible in this picture.\nQuestion: Please provide 〈toy*〉’s bounding coordinates in the image.\nRAP-LLaVA: [0.42, 0.19, 0.54, 0.39]\ntoy*\nQuestion: Can you identify if 〈toy*〉is captured in this picture?\nRAP-LLaVA: 〈toy*〉is not captured in this image.\nA\nQuestion: Can you confirm if 〈A〉appears in this photo?\nRAP-LLaVA: Yes, 〈A〉is in this photo.\nQuestion: Describe 〈A〉’s position in the image.\nRAP-LLaVA: [0.24, 0.12, 0.72, 0.98]\nA\nQuestion: Can you confirm if 〈A〉appears in this photo?\nRAP-LLaVA: 〈A〉is not present in this photograph.\n22\n\nTable 20. Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We use green rectangle to show the bounding\nbox in the image.\nImage\nConversation\nstatue1\nQuestion: Is 〈statue1〉included in this shot?\nRAP-LLaVA: Yes, 〈statue1〉is included in this shot.\nQuestion: Give 〈statue1〉’s bounding box in the image.\nRAP-LLaVA: [0.36, 0.16, 0.59, 0.41]\nstatue1\nQuestion: Is 〈statue1〉included in this shot?\nRAP-LLaVA: 〈statue1〉is not captured in this picture.\nmy dog\nQuestion: Is 〈my dog〉captured in this image?\nRAP-LLaVA: Yes, I recognize 〈my dog〉in this picture.\nQuestion: Specify 〈my dog〉’s bounding box in the photograph.\nRAP-LLaVA: [0.33, 0.23, 1.0, 0.69]\nmy dog\nQuestion: Is 〈my dog〉captured in this image?\nRAP-LLaVA: No, 〈my dog〉is not included in this shot.\nteapot*\nQuestion: Is 〈teapot*〉visible in this image?\nRAP-LLaVA: Yes, 〈teapot*〉is visible in this picture.\nQuestion: Describe 〈teapot*〉’s position in the image.\nRAP-LLaVA: [0.52, 0.69, 0.73, 0.91]\nteapot*\nQuestion: Is 〈teapot*〉visible in this image?\nRAP-LLaVA: No, 〈teapot*〉is not visible in this picture.\n23\n\nTable 21. Instructions for visual grounding and recognition.\n\u0013\n\u0012\n\u0010\n\u0011\nVisual recognition:\n• Is ⟨V⟩visible in this picture?\n• Is ⟨V⟩in this image?\n• Do you see ⟨V⟩in the photo?\n• Is ⟨V⟩present in this photograph?\n• Can you identify if ⟨V⟩is captured in this picture?\n• Is ⟨V⟩depicted in this image?\n• Does the picture feature ⟨V⟩?\n• Can you confirm if ⟨V⟩appears in this photo?\n• Is ⟨V⟩included in this shot?\n• Is ⟨V⟩shown in this image?\n• Can you tell if ⟨V⟩is part of this photograph?\n• Is there any sign of ⟨V⟩in this picture?\n• Can you detect ⟨V⟩in the photo?\n• Is ⟨V⟩captured in this image?\n• Do you recognize ⟨V⟩in this picture?\nVisual grounding:\n• Give 〈V〉’s bounding box in the image.\n• Describe 〈V〉’s position in the image.\n• Please provide the coordinates of the bounding box for 〈V〉in the given image.\n• Specify the rectangular boundaries of 〈V〉in the image.\n• Give 〈V〉’s position in the following image.\n• Please provide 〈V〉’s bounding coordinates in the image.\n• Indicate the bounding box for 〈V〉in the image.\n• Show the bounding box for 〈V〉in the picture.\n• Specify 〈V〉’s bounding box in the photograph.\n• Mark 〈V〉’s bounding box within the image.\nTable 22. Instructions for image captioning.\n\u0013\n\u0012\n\u0010\n\u0011\nImage caption:\n• Give a caption of the image.\n• Give a personalized caption of this image.\n• Provide a brief caption of the image.\n• Summarize the visual content of the image.\n• Create a short caption of the image.\n• Offer a short and clear interpretation of the image.\n• Describe the image concisely.\n• Render a concise summary of the photo.\n• Provide a caption of the given image.\n• Can you provide a personalized caption of this photo?\n• Could you describe this image concisely?\n24\n\nTable 23. Instructions for image description.\n\u0013\n\u0012\n\u0010\n\u0011\nImage description:\n• Describe the image.\n• Give a description of the image.\n• Give a description of the image in detail.\n• Give a short description of the image.\n• Describe the image in detail.\n• Please provide a description of the image.\n• Can you give me details about the image?\n• Could you explain what’s shown in the image?\nTable 24. Seed questions used for question answering synthesis.\n\u0013\n\u0012\n\u0010\n\u0011\nPerson:\n• What is 〈H〉’s hair color?\n• What is 〈H〉’s height (estimated)?\n• What is 〈H〉’s skin tone?\n• What is 〈H〉’s eye color?\n• What style of clothing is 〈H〉wearing?\n• Does 〈H〉have any visible tattoos?\n• Does 〈H〉wear glasses or contact lenses?\n• Does 〈H〉have any facial hair?\n• What is 〈H〉’s approximate age?\n• What is 〈H〉’s build or body type?\n• What is 〈H〉doing?\nObject:\n• What color is 〈O〉?\n• What pattern is on 〈O〉?\n• What shape does 〈O〉have?\n• What size is 〈O〉?\n• What is the texture of 〈O〉?\n• Is 〈O〉shiny or matte?\n• What material is 〈O〉made of?\n• Does 〈O〉have any patterns or designs on it?\n• Is 〈O〉new or worn?\n• Does 〈O〉have any visible brand or logo?\n• Is 〈O〉functional or decorative?\nMulti-concept question:\n• What do 〈C1〉and 〈C2〉have in common?\n• What activity are 〈C1〉and 〈C2〉engaged in?\n• Where could 〈C1〉and 〈C2〉be located?\n• What is the most noticeable difference between 〈C1〉and 〈C2〉?\n• What are they doing?\n25\n\nTable 25. Examples of our database. A concept should be provided with an image and its personalized description.\nImage\nConcept\nInformation\nAnya\nA young girl with pink hair and big green eyes.\ndoll*\nThis is a cute figurine of a girl wearing a pink and blue dress, holding a\nwhite bubble.\ntoy1\nA plush toy. It is orange with a yellow belly and a brown nose.\ntoy2\nThis is a plush toy of the bluey character. It is a light blue color with a\npurple patch on its head, and its ears are yellow.\nstatue*\nThis is a figurine of a cat. The cat has a blue body with yellow, red, and\ngreen stripes and a long tail that is also striped.\ncat*\nA small ginger kitten with bright blue eyes looks up at the camera.\nH\nA young man is wearing a plain tan t-shirt. His hair is short and curly.\nmy dog\nA white and gray dog with long fur. He has black eyes.\nT\nA young woman with blonde hair is wearing a white tank top and blue\njeans.\n26",
    "pdf_filename": "Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models.pdf"
}