{
    "title": "Technology Ethics in Action - Critical and Interdisciplinary Perspectives",
    "context": "",
    "body": "SPECIAL ISSUE ON TECHNOLOGY ETHICS IN ACTION: CRITICAL AND INTERDISCIPLINARY PERSPECTIVES\nEditorial\ni–ii\nThe  Contestation  of  Tech  Ethics:  A  Sociotechnical  Approach  to\nTechnology Ethics in Practice\nBen Green\n209–225\nFraming  and  Language  of  Ethics:  Technology,  Persuasion,  and  Cultural\nContext\nJasmine E. McNealy\n226–237\nTech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\nLily Hu\n238–248\nData Science as Political Action: Grounding Data Science in a Politics of\nJustice\nBen Green\n249–265\nFrom  Ethics  Washing  to  Ethics  Bashing:  A  Moral  Philosophy  View  on\nTech Ethics\nElettra Bietti\n266–283\nThe Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\nSalomé Viljoen\n284–296\nVol. 2,  No. 3,  September  2021 \nCONTENTS \n\nApologos: A Lightweight Design Method for Sociotechnical Inquiry\nLuke Stark\n297–308\nCreating Technology Worthy of the Human Spirit\nAden Van Noppen\n309–322\nReal  Estate  Politik:  Democracy  and  the  Financialization  of  Social\nNetworks\nJoanne Cheung\n323–336\nAlgorithmic Silence: A Call to Decomputerize\nJonnie Penn\n337–356\nConnecting Race to Ethics Related to Technology: A Call for Critical Tech\nEthics\nJenny Ungbha Korn\n357–364\nCritical Technical Awakenings\nMaya Malik and Momin M. Malik\n365–384\nTOTAL CONTENTS\nI–II\nVol. 2,  No. 4,  December  2021 \nCONTENTS \nSPECIAL ISSUE ON TECHNOLOGY ETHICS IN ACTION: CRITICAL AND INTERDISCIPLINARY PERSPECTIVES\n\n \nEditorial\n \nWe are thrilled to present this special issue of the Journal of Social Computing, entitled “Technology Ethics in Action:\nCritical and Interdisciplinary Perspectives”.\nThis special issue interrogates the meaning and impacts of “tech ethics”: the embedding of ethics into digital\ntechnology research, development, use, and governance. In response to concerns about the social harms associated\nwith digital technologies, many individuals and institutions have articulated the need for a greater emphasis on ethics\nin digital technology. Yet as more groups embrace the concept of ethics, critical discourses have emerged questioning\nwhose ethics are being centered, whether “ethics” is the appropriate frame for improving technology, and what it means\nto develop “ethical” technology in practice.\nThis interdisciplinary issue takes up these questions, interrogating the relationships among ethics, technology, and\nsociety in action. This special issue engages with the normative and contested notions of ethics itself, how ethics has\nbeen integrated with technology across domains, and potential paths forward to support more just and egalitarian\ntechnology. Rather than starting from philosophical theories, the authors in this issue orient their articles around the\nreal-world discourses and impacts of tech ethics—i.e., tech ethics in action. In many cases, this focus derives from the\nauthors’ own engagements with tech ethics as scholars, practitioners, and activists.\nThis special issue emerged from the Ethical Tech Working Group at the Berkman Klein Center for Internet & Society\nat Harvard. The working group was co-founded in 2017 by Mary Gray and Kathy Pham, two trailblazers in embedding\nethics, responsibility, and justice into technology. Every contributor in this special issue was a regular participant in\nthe Working Group’s weekly meetings between 2017 and 2020.\nAfter its founding, the Ethical Tech Working Group quickly blossomed into a flourishing, interdisciplinary, and\nwelcoming community. It became a hub of trust, respect, and friendship. Weekly meetings were structured to provide\neveryone with a platform and to welcome newcomers. We began by having everyone in the room introduce themself\nand answer the question: “What is on your mind in terms of ethics and technology this week?” Our discussions explored\ntechnology ethics through lenses that include art, anthropology, communications, computer science, divinity, history,\nlabor, law, race and gender studies, philosophy, political science, and STS (science, technology, and society). We\naimed to recognize and honor everyone’s perspective and knowledge, gaining an understanding across fields and\nviewpoints. It was a common occurrence to hear: “You just said [this word]. What does that word mean to you? Because\nit means something different to me.”\nThe articles in this issue reflect this spirit of community and dialogue: each article represents its author’s distinct\nperspective  yet  is  simultaneously  deeply  informed  by  conversation  with  the  other  members  of  the  Ethical  Tech\nWorking Group.\nThe articles in this special issue are split across two sections. The first section begins with the introduction to the\nspecial issue: Ben Green summarizes recent developments and challenges in tech ethics, suggesting the need to study\ntech ethics through a sociotechnical lens. The rest of the first section focuses on the value and limits of ethics for\nimproving digital technology. With its connections to philosophy and connotations of moral behavior, ethics appears\nwell-suited for improving the development and applications of digital technology. Moral philosophy indeed sheds light\non the normative principles and obligations that arise in complex sociotechnical contexts. Yet the superficial and\nlegitimizing role that ethics often plays in digital technology and other domains suggests that ethics may suffer from\nsignificant shortcomings as an organizing principle for reform. Diagnosing the contours of these limits is thus an\nessential task for achieving more just technologies moving forward.\nJasmine E. McNealy sets the stage for the particular frames and terms used to discuss ethics, describing several\nframes that provide vague and misleading promises and calling for counternarratives. Lily Hu draws parallels between\nethics and the history of human rights, suggesting the need to be suspicious of moral language that evades political\nand ideological battles. Ben Green advocates for an explicit embrace of politics instead of ethics, articulating how the\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN       2688-5255       pp i−ii\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0036\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nfield of data science can productively evolve toward a politics of social justice. Elettra Bietti reflects on critical\ndiscourses about tech ethics, laying out a philosophically informed middle ground between “ethics-washing” and\n“ethics-bashing”. Salomé Viljoen analyzes calls to embrace law rather than ethics, describing how the law (much like\nethics) is a terrain of contestation and how the law has structured the ethical crises in tech.\nThe second section (to be published in Issue 4) considers broader frames and strategies, beyond the explicit label\nof tech ethics, for improving digital technology. The limits of tech ethics indicate a need to expand the scope of ethical\nanalysis of technology, not to abandon ethical analysis altogether. Doing so broadens the focus beyond technology\ndesign to consider the entire lifecycle, infrastructure, and governance of sociotechnical systems, thus opening up new\nterrains for contestation and action. On this view, many of the central problems related to technology ethics are less\nproblems of technology itself than problems connected to broader social and political injustices. Exploring how these\nlarger contexts shape technology—and how to reform them—is therefore essential to a more expansive approach to\nremediating the impacts of digital technologies. This section moves from interventions focused on design processes\nto broader reorientations of pedagogy, culture, and institutions.\nLuke Stark introduces Apologos as a method for eliciting ethics, norms, and human values in sociotechnical design\nprocesses on a compressed time scale. Aden Van Noppen describes how tech companies can improve technology design\nby adopting practices of spiritual care. Joanne Cheung draws an analogy between the financialization of land and the\nfinancialization of social media, emphasizing the need to focus on business models (in addition to design decisions)\nand describing possibilities to steward social networks in the public interest. Jonnie Penn challenges the myth of\nautomation, describing how the pursuit of digital automation systematically extracts human labor and proposing the\ncorrective of “algorithmic silence”. Jenny Ungbha Korn calls for a critical tech ethics that embraces critical race\ntheory, intersectional feminist theory, and critical race feminist theory. Maya Malik and Momin M. Malik introduce\nand  explore  the  process  of  how  people  from  technical  disciplines  come  to  embrace  more  critical  orientations,\ndescribing the importance of these awakenings for technology ethics and social justice. Sabelo Mhlambi critiques data\ncolonialism and surveillance capitalism, arguing for technology development grounded in the Sub-Saharan African\nphilosophy of Ubuntu to AI.\nWe are deeply grateful to the many, many people who contributed to this special issue. All of the authors shared their\nwisdom and care in crafting their articles. In addition to these authors, many Ethical Tech Working Group peers have\nheavily influenced our perspectives, including Doaa Abu Elyounes, Kendra Albert, Bao Kham Chau, Mary Gray, Jenn\nHalen, Dean Jansen, Ram Kumar, Keith Porcaro, Boaz Sender, and Suchana Seth. We could not have done our work\nwithout the support of the Berkman Klein Center Staff: Carey Anderson, Sebastian Diaz, Daniel Jones, Reuben\nLangevin, and Ellen Popko, who made all of our events and convenings possible; and Becca Tabasky, who tirelessly\nbuilt community and taught us how to effectively bring everyone to the table.\nWe are also grateful to the Journal of Social Computing for the opportunity to publish this special issue and all of\nthe labor that went into it. Peaks Krafft provided the initial connection that made this special issue possible and managed\nmany of the logistics, including the peer review process. The Editors-in-Chief—Xiaoming Fu, James Evans, and Jar-\nDer Luo—embraced the vision of the special issue and supported all of our efforts. The staff at Information Science\nDivision, Tsinghua University Press managed the entire publication process. Finally, the many peer reviewers provided\nthoughtful feedback and substantially improved each article.\nCollectively, the articles in this special issue provide a range of perspectives and proposals regarding the value of\ntech ethics and paths forward for improving digital technologies. These varied perspectives embody our commitment\nto honoring all forms of knowledge and expertise, particularly those forms that technologists all too often ignore. While\nmany of the articles reflect common viewpoints, there is also disagreement across articles—in some cases explicit, in\nothers  implicit —regarding  questions  such  as  the  proper  role  for “ethics” and  strategies  for  interacting  with  the\ntechnology industry. Our goal is not to produce precisely defined answers. Instead, we aim to explore the contours of\ndebate and action related to technology ethics, in the service of a more just society. We hope that this special issue\nwill help you to do the same.\nIn solidarity,\nBen Green (Special Issue Editor)\nKathy Pham (Berkman Klein Center Ethical Tech Working Group Co-Founder)\n    ii\nJournal of Social Computing, September 2021, 2(3): i−ii    \n \n\n \nThe Contestation of Tech Ethics: A Sociotechnical Approach to\nTechnology Ethics in Practice\nBen Green*\nAbstract:    This  article  introduces  the  special  issue “Technology  Ethics  in  Action:  Critical  and\nInterdisciplinary Perspectives”. In response to recent controversies about the harms of digital technology,\ndiscourses and practices of “tech ethics” have proliferated across the tech industry, academia, civil society, and\ngovernment. Yet despite the seeming promise of ethics, tech ethics in practice suffers from several significant\nlimitations: tech ethics is vague and toothless, has a myopic focus on individual engineers and technology\ndesign, and is subsumed into corporate logics and incentives. These limitations suggest that tech ethics enables\ncorporate “ethics-washing”:  embracing  the  language  of  ethics  to  defuse  criticism  and  resist  government\nregulation, without committing to ethical behavior. Given these dynamics, I describe tech ethics as a terrain of\ncontestation where the central debate is not whether ethics is desirable, but what “ethics” entails and who gets\nto define it. Current approaches to tech ethics are poised to enable technologists and technology companies to\nlabel themselves as “ethical” without substantively altering their practices. Thus, those striving for structural\nimprovements in digital technologies must be mindful of the gap between ethics as a mode of normative inquiry\nand ethics as a practical endeavor. In order to better evaluate the opportunities and limits of tech ethics, I propose\na sociotechnical approach that analyzes tech ethics in light of who defines it and what impacts it generates in\npractice.\nKey  words:   technology  ethics;  AI  ethics;  ethics-washing;  Science,  Technology,  and  Society  (STS);\nsociotechnical systems\n1    Introduction: A Crisis of Conscience\nIf digital technology production in the beginning of the\n2010s was characterized by the brash spirit of Facebook’s\nmotto “move fast and break things” and the superficial\nassurances of Google’s motto “do not be evil”, digital\ntechnology  toward  the  end  of  the  decade  was\ncharacterized by a “crisis of conscience”[1]. While many\nhave long been aware of digital technology’s harms, an\ninflux of stories about salient harms led to widespread\ncritique  of  digital  technology.  The  response  was  the\n“techlash”:  a  growing  public  animosity  toward  major\ntechnology  companies.  In  2018,  Oxford  Dictionaries\nand the Financial Times both deemed techlash to be one\nof the words of the year[2, 3].\nConsider just a few of the controversies that prompted\nthis crisis of conscience within tech and the associated\ntechlash:\nDisinformation: \nThroughout\n the\n 2016\n US\npresidential election between Donald Trump and Hillary\nClinton,  social  media  was  plagued  with  fraudulent\nstories  that  went  viral[4, 5].  In  turn,  numerous\ncommentators—including\n Hillary\n Clinton—blamed\nFacebook  for  Donald  Trump’s  presidential  election\nvictory[6−9].  Later  reporting  revealed  that  Facebook’s\nleadership has actively resisted taking strong measures\nto curb disinformation, instead prioritizing the company’s\nbusiness strategies[10, 11].\nCambridge Analytica: In 2018, The New York Times\n \n • Ben Green is with  the Society of Fellows and the Gerald R. Ford\nSchool of Public Policy, University of Michigan, Ann Arbor, MI\n48109, USA. E-mail: bzgreen@umich.edu.\n * To whom correspondence should be addressed.\n    Manuscript received: 2021-05-20; accepted: 2021-10-20\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   01/06  pp209−225\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0018\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nand The Guardian reported that the voter-profiling firm\nCambridge  Analytica  had  harvested  information  from\nmillions of Facebook users, without their knowledge or\npermission, in order to target political ads for Donald\nTrump’s 2016 presidential campaign[12, 13]. Cambridge\nAnalytica had acquired these data by exploiting the sieve-\nlike nature of Facebook’s privacy policy.\nMilitary  and  ICE  Contracts:  In  2018,  journalists\nrevealed  that  Google  was  working  with  the  US\nDepartment of Defense (DoD) to develop software that\nanalyzes drone footage[14]. This effort, known as Project\nMaven, was part of a ＄7.4 billion investment in AI by\nthe DoD in 2017[14] and represented an opportunity for\nGoogle  to  gain  billions  of  dollars  in  future  defense\ncontracts[15].  Another  story  revealed  that  Palantir  was\ndeveloping  software  for  Immigration  and  Customs\nEnforcement (ICE) to facilitate deportations[16].\nAlgorithmic Bias: In 2016, ProPublica revealed that\nan algorithm used in criminal courts was biased against\nBlack defendants, mislabeling them as future criminals\nat  twice  the  rates  of  white  defendants[17].  Through\npopular books about the harms and biases of algorithms\nin  settings  such  as  child  welfare,  online  search,  and\nhiring[18−20], the public began to recognize algorithms as\nboth fallible and discriminatory.\nThese  and  other  tech-related  controversies  were  a\nshock to many, as they arrived in an era of widespread\n(elite)  optimism  about  the  beneficence  of  technology.\nYet these controversies also brought public attention to\nwhat scholars in fields such as Science, Technology, and\nSociety (STS), philosophy of science, critical data and\nalgorithm studies, and law have long argued: technology\nis shaped by social forces, technology structures society\noften in deleterious ways, and technology cannot solve\nevery  social  problem.  Broadly  speaking,  these  fields\nbring\n a \n“sociotechnical” \napproach\n to\n studying\ntechnologies,  analyzing  how  technologies  shape,  are\nshaped  by,  and  interact  with  society[21−24].  As  tech\nscandals mounted, a variety of sociotechnical insights,\nlong ignored by most technologists and journalists, were\nnewly recognized (or in some form recreated).\nMany in the tech sector and academia saw the harms\nof digital technology as the result of an inattention to\nethics. On this view, unethical technologies result from\na  lack  of  training  in  ethical  reasoning  for  engineers\nand  a  dearth  of  ethical  principles  in  engineering\npractice[1, 25−28]. In response, academics, technologists,\ncompanies,  governments,  and  more  have  embraced  a\nbroad set of goals often characterized with the label “tech\nethics”: the introduction of ethics into digital technology\neducation, research, development, use, and governance.\nIn the span of just a few years, tech ethics has become\na\n dominant\n discourse\n discussed\n in\n technology\ncompanies,  academia,  civil  society  organizations,  and\ngovernments.\nThis article reviews the growth of tech ethics and the\ndebates that this growth has prompted. I first describe the\nprimary forms of tech ethics in practice. I focus on the\npeople  and  organizations  that  explicitly  embrace  the\nlabel of “tech ethics” (and closely related labels, such as\nAI ethics and algorithmic fairness). I then summarize the\ncentral critiques made against these efforts, which call\ninto question the effects and desirability of tech ethics.\nAgainst the backdrop of these critiques, I argue that tech\nethics is a terrain of contestation: the central debate is not\nwhether ethics is desirable but what ethics entails and\nwho has the authority to define it. These debates suggest\nthe need for a sociotechnical approach to tech ethics that\nfocuses on the social construction and real-world effects\nof  tech  ethics,  disambiguating  between  the  value  of\nethics as a discipline and the limits of tech ethics as a\npractical  endeavor.  I  introduce  this  approach  through\nfour  frames:  objectivity  and  neutrality,  determinism,\nsolutionism, and sociotechnical systems.\n2    The Rise of Tech Ethics\nAlthough some scholars, activists, and others have long\nconsidered the ethics of technology, attention to digital\ntechnology  ethics  has  rapidly  grown  across  the  tech\nindustry,  academia,  civil  society,  and  government  in\nrecent  years.  As  we  will  see,  tech  ethics  typically\ninvolves applied forms of ethics such as codes of ethics\nand  research  ethics,  rather  than  philosophical  inquiry\n(i.e.,  moral  philosophy).  For  instance,  one  common\ntreatment of tech ethics is statements of ethical principles.\nOne analysis of 36 prominent AI principles documents\nshows the sharp rise in these statements, from 2 in 2014\nto  16  in  2018[29].  These  documents  tend  to  cover  the\nthemes  of  fairness  and  non-discrimination,  privacy,\naccountability, and transparency and explainability[29].\nMany  documents  also  reference  human  rights,  with\nsome  taking  international  human  rights  as  the\nframework for ethics[29].\n    210\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\n2.1    Tech industry\nThe most pervasive treatment of tech ethics within tech\ncompanies has come in the form of ethics principles and\nethics  oversight  bodies.  Companies  like  Microsoft,\nGoogle, and IBM have developed and publicly shared\nAI ethics principles, which include statements such as\n“AI  systems  should  treat  all  people  fairly” and “AI\nshould be socially beneficial”[30−32]. These principles are\noften  supported  through  dedicated  ethics  teams  and\nadvisory boards within companies, with such bodies in\nplace  at  companies  including  Microsoft,  Google,\nFacebook, DeepMind, and Axon[33−37]. Companies such\nas Google and Accenture have also begun offering tech\nethics consulting services[38, 39].\nAs part of these efforts, the tech industry has formed\nseveral coalitions aimed at promoting safe and ethical\nartificial  intelligence.  In  2015,  Elon  Musk  and  Sam\nAltman  created  OpenAI,  a  research  organization  that\naims to mitigate the “existential threat” presented by AI,\nwith more than ＄1 billion in donations from major tech\nexecutives  and  companies[40].  A  year  later,  Amazon,\nFacebook, DeepMind, IBM, and Microsoft founded the\nPartnership on AI (PAI), a nonprofit coalition to shape\nbest  practices  in  AI  development,  advance  public\nunderstanding  of  AI,  and  support  socially  beneficial\napplications of AI[41, 42].①\n2.2    Academia\nComputer  and  information  science  programs  at\nuniversities  have  rapidly  increased  their  emphasis  on\nethics  training.  While  some  universities  have  taught\ncomputing  ethics  courses  for  many  years[44−46],  the\nemphasis  on  ethics  within  computing  education  has\nincreased  dramatically  in  recent  years[47].  One\ncrowdsourced list of tech ethics classes contains more\nthan 300 courses[48]. This plethora of courses represents\na dramatic shift in computer science training and culture,\nwith ethics becoming a popular topic of discussion and\nstudy after being largely ignored by the mainstream of\nthe field just a few years prior.\nResearch in computer science and related fields has\nalso  become  more  focused  on  the  ethics  and  social\nimpacts of computing. This trend is observable in the\nrecent increase in conferences and workshops related to\ncomputing  ethics.  The  ACM  Conference  on  Fairness,\nAccountability,  and  Transparency  (FAccT)  and  the\nAAAI/ACM  Conference  on  AI,  Ethics,  and  Society\n(AIES) both held their first annual meetings in February\n2018  and  have  since  grown  rapidly.  There  have  been\nseveral dozen workshops related to fairness and ethics\nat  major  computer  science  conferences[49].  Many\nuniversities  have  supported  these  efforts  by  creating\ninstitutes  focused  on  the  social  implications  of\ntechnology. 2017 alone saw the launch of the AI Now\nInstitute at NYU[50], the Princeton Dialogues on AI and\nEthics[51], and the MIT/Harvard Ethics and Governance\nof  Artificial  Intelligence  Initiative[52].  More  recently\nformed\n centers\n include\n the\n MIT\n College\n of\nComputing[53];  the  Stanford  Institute  for  Human-\nCentered Artificial Intelligence[54]; and the University of\nMichigan Center of Ethics, Society, and Computing[55].\n2.3    Civil society\nNumerous  civil  society  organizations  have  coalesced\naround  tech  ethics,  with  strategies  that  include\ngrantmaking  and  developing  principles.  Organizations\nsuch as the MacArthur and Ford Foundations have begun\nexploring  and  making  grants  in  tech  ethics[56].  For\ninstance,  the  Omidyar  Network,  Mozilla  Foundation,\nSchmidt  Futures,  and  Craig  Newmark  Philanthropies\npartnered  on  the  Responsible  Computer  Science\nChallenge, which awarded ＄3.5 million between 2018\nand  2020  to  support  efforts  to  embed  ethics  into\nundergraduate  computer  science  education[57].  Many\nfoundations also contribute to the research, conferences,\nand institutes that have emerged in recent years.\nOther  organizations  have  been  created  or  have\nexpanded their scope to consider the implications and\ngovernance  of  digital  technologies.  For  example,  the\nAmerican  Civil  Liberties  Union  (ACLU)  has  begun\nhiring  technologists  and  is  increasingly  engaged  in\ndebates  and  legislation  related  to  new  technology.\nOrganizations  such  as  Data  &  Society,  Upturn,  the\nCenter for Humane Technology, and Tactical Tech study\nthe social implications of technology and advocate for\nimproved technology governance and design practices.\nMany in civil society call for engineers to follow an\nethical  oath  modeled  after  the  Hippocratic  Oath  (an\nethical oath taken by physicians)[20, 58−60]. In 2018, for\ninstance, the organization Data for Democracy partnered\n① Although  PAI  also  includes  civil  society  partners,  these\norganizations do not appear to have significant influence. In 2020, the\nhuman rights organization Access Now resigned from PAI, explaining\nthat “there is an increasingly smaller role for civil society to play within\nPAI” and that “we did not find that PAI influenced or changed the attitude\nof member companies”[43].\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n211    \n \n\nwith  Bloomberg  and  the  data  platform  provider\nBrightHive to develop a code of ethics for data scientists,\ndeveloping  20  principles  that  include “I  will  respect\nhuman dignity” and “It is my responsibility to increase\nsocial benefit while minimizing harm”[61]. Former US\nChief Data Scientist DJ Patil described the event as the\n“Constitutional  Convention” for  data  science[58].  A\nrelated effort, produced by the Institute for the Future\nand the Omidyar Network, is the Ethical OS Toolkit, a\nset  of  prompts  and  checklists  to  help  technology\ndevelopers “anticipate  the  future  impact  of  today’s\ntechnology” and “not regret the things you will build”[62].\n2.4    Government\nMany  governments  developed  commissions  and\nprinciples dedicated to tech ethics. In the United States,\nfor example, the National Science Foundation formed a\nCouncil  for  Big  Data,  Ethics,  and  Society[63];  the\nNational Science and Technology Council published a\nreport  about  AI  that  emphasized  ethics[64];  and  the\nDepartment  of  Defense  adopted  ethical  principles  for\nAI[65].  Elsewhere,  governing  bodies  in  Dubai[66],\nEurope[67],  Japan[68],  and  Mexico[69],  as  well  as\ninternational organizations such as the OECD[70], have\nall stated principles for ethical AI.\n3    The Limits of Tech Ethics\nAlongside  its  rapid  growth,  tech  ethics  has  been\ncritiqued  along  several  lines.  First,  the  principles\nespoused by tech ethics statements are too abstract and\ntoothless  to  reliably  spur  ethical  behavior  in  practice.\nSecond,  by  emphasizing  the  design  decisions  of\nindividual engineers, tech ethics overlooks the structural\nforces that shape technology’s harmful social impacts.\nThird,  as  ethics  is  incorporated  into  tech  companies,\nethical  ideals  are  subsumed  into  corporate  logics  and\nincentives. Collectively, these issues suggest that tech\nethics  represents  a  strategy  of  technology  companies\n“ethics-washing” their behavior with a façade of ethics\nwhile largely continuing with business-as-usual.\n3.1    Tech ethics principles are abstract and toothless\nTech ethics codes deal in broad principles[71]. In 2016,\nfor  example,  Accenture  published  a  report  explicitly\noutlining “a universal code of data ethics”[72]. A 2019\nanalysis of global AI ethics guidelines found 84 such\ndocuments, espousing a common set of broad principles:\ntransparency,  justice  and  fairness,  non-maleficence,\nresponsibility, and privacy[73]. Professional computing\nsocieties also present ethical commitments in a highly\nabstract form, encouraging computing professionals “to\nbe  ever  aware  of  the  social,  economic,  cultural,  and\npolitical impacts of their actions” and to “contribute to\nsociety  and  human  well-being”[74].  Ethics  codes  in\ncomputing and information science are notably lacking\nin explicit commitments to normative principles[74].\nThe emphasis on universal principles papers over the\nfault  lines  of  debate  and  disagreement  spurred  the\nemergence of tech ethics in the first place. Tech ethics\nprinciples embody a remarkable level of agreement: two\n2019  reports  on  global  AI  ethics  guidelines  noted  a\n“global  convergence”[73] and  a “consensus”[29] in  the\nprinciples espoused. Although these documents tend to\nreflect  a  common  set  of  global  principles,  the  actual\ninterpretation  and  implementation  of  these  principles\nraise  substantive  conflicts[73].  Furthermore,  these\nprinciples have been primarily developed in the US and\nUK, with none from Africa or South America[73]. The\nsuperficial  consensus  around  abstract  ideals  may  thus\nhinder  substantive  deliberation  regarding  whether  the\nchosen values are appropriate, how those values should\nbe balanced in different contexts, and what those values\nactually entail in practice.\nThe abstraction of tech ethics is particularly troubling\ndue  to  a  lack  of  mechanisms  to  enact  or  enforce  the\nespoused principles. When framed at such a high level\nof abstraction, values such as fairness and respect are\nunable to guide specific actions[75]. In companies, ethics\noversight boards and ethics principles lack the authority\nto  veto  projects  or  require  certain  behaviors[76, 77].\nSimilarly, professional computing organizations such as\nthe  IEEE  and  ACM  lack  the  power  to  meaningfully\nsanction individuals who violate their codes of ethics[75].\nMoreover, unlike fields such as medicine, which has a\nstrong and established emphasis on professional ethics,\ncomputing  lacks  a  common  aim  or  fiduciary  duty  to\nunify disparate actors around shared ethical practices[75].\nAll  told, “Principles  alone  cannot  guarantee  ethical\nAI”[75].\n3.2    Tech  ethics  has  a  myopic  focus  on  individual\nengineers and technology design\nTech  ethics  typically  emphasizes  the  roles  and\nresponsibilities  of  engineers,  paying  relatively  little\nattention  to  the  broader  environments  in  which  these\n    212\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nindividuals  work.  Although  professional  codes  in\ncomputing\n and\n related\n fields\n assert\n general\ncommitments  to  the  public,  profession,  and  one’s\nemployer, “the morality of a profession’s or an employer’s\nmotives are not scrutinized”[74]. Similarly, ethics within\ncomputer  science  curricula  tends  to  focus  on  ethical\ndecision making for individual engineers[78].\nFrom this individualistic frame comes an emphasis on\nappealing to the good intentions of engineers, with the\nassumption that better design practices and procedures\nwill lead to better technology. Ethics becomes a matter\nof individual engineers and managers “doing the right\nthing” “for  the  right  reasons”[79].  Efforts  to  provide\nethical guidance for tech CEOs rest on a similar logic:\n“if a handful of people have this much power—if they\ncan,  simply  by  making  more  ethical  decisions,  cause\nbillions  of  users  to  be  less  addicted  and  isolated  and\nconfused  and  miserable—then,  is  not  that  worth  a\nshot?”[1]. The broader public beyond technical experts is\nnot seen as having a role in defining ethical concerns or\nshaping the responses to these concerns[71].\nTech  ethics  therefore  centers  debates  about  how  to\nbuild better technology rather than whether or in what\nform to build technology (let alone who gets to make\nsuch decisions). Tech ethics follows the assumption that\nartificial  intelligence  and  machine  learning  are\n“inevitable”,  such  that “‘better  building’ is  the  only\nethical  path  forward”[71].  In  turn,  tech  ethics  efforts\npursue  technical  and  procedural  solutions  for  the\nharmful\n social\n consequences\n of\n technology[79].\nFollowing  this  logic,  tech  companies  have  developed\nnumerous ethics and fairness toolkits[80−84].\nAlthough efforts to improve the design decisions of\nindividual  engineers  can  be  beneficial,  the  focus  on\nindividual design choices relies on a narrow theory of\nchange  for  how  to  reform  technology.  Regardless  of\ntheir  intentions  and  the  design  frameworks  at  their\ndisposal, individual engineers typically have little power\nto  shift  corporate  strategy.  Executives  can  prevent\nengineers  from  understanding  the  full  scope  of  their\nwork,  limiting  knowledge  and  internal  dissent  about\ncontroversial  projects[85, 86].  Even  when  engineers  do\nknow about and protest projects, the result is often them\nresigning  or  being  replaced  rather  than  the  company\nchanging course[60, 85]. The most notable improvements\nin  technology  use  and  regulation  have  come  from\ncollective  action  among  activists,  tech  workers,\njournalists, and scholars, rather than individual design\nefforts[87, 88].\nMore  broadly,  the  emphasis  on  design  ignores  the\nstructural sources of technological harms. The injustices\nassociated  with  digital  technologies  result  from\nbusiness models that rely on collecting massive amounts\nof  data  about  the  public[89, 90];  companies  that  wield\nmonopolistic  power[91, 92];  technologies  that  are  built\nthrough the extraction of natural resources and the abuse\nof  workers[93−96];  and  the  exclusion  of  women,\nminorities, and non-technical experts from technology\ndesign and governance[97, 98].\nThese structural conditions place significant barriers\non the extent to which design-oriented tech ethics can\nguide efforts to achieve reform. As anthropologist Susan\nSilbey  notes, “while  we  might  want  to  acknowledge\nhuman  agency  and  decision-making  at  the  heart  of\nethical action, we blind ourselves to the structure of those\nchoices—incentives, content, and pattern—if we focus\ntoo closely on the individual and ignore the larger pattern\nof opportunities and motives that channel the actions we\ncall  ethics”[78].  To  the  extent  that  it  defines  ethical\ntechnology in terms of individual design decisions, tech\nethics will divert scrutiny away from the economic and\npolitical factors that drive digital injustice, limiting our\nability to address these forces.\n3.3    Tech ethics is subsumed into corporate logics\nand incentives\nDigital technology companies have embraced ethics as\na  matter  of  corporate  concern,  aiming  to  present  the\nappearance  of  ethical  behavior  for  scrutinizing\naudiences. As Alphabet and Microsoft noted in recent\nSEC filings, products that are deemed unethical could\nlead to reputational and financial harms[99]. Companies\nare  eager  to  avoid  any  backlash,  yet  do  not  want  to\njeopardize  their  business  plans.  An  ethnography  of\nethics work in Silicon Valley found that “performing, or\neven showing off, the seriousness with which a company\ntakes ethics becomes a more important sign of ethical\npractices than real changes to a product”[79]. For instance,\nafter  an  effort  at  Twitter  to  reduce  online  harassment\nstalled,  an  external  researcher  involved  in  the  effort\nnoted, “The  impression  I  came  away  with  from  this\nexperience  is  that  Twitter  was  more  sensitive  to\ndeflecting  criticism  than  in  solving  the  problem  of\nharassment”[100].\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n213    \n \n\nCorporate tech ethics is therefore framed in terms of its\ndirect  alignment  with  business  strategy.  A  software\nengineer at LinkedIn described algorithmic fairness as\nbeing profitable for companies, arguing, “If you are very\nbiased,  you  might  only  cater  to  one  population,  and\neventually that limits the growth of your user base, so\nfrom a business perspective you actually want to have\neveryone come on board, so it is actually a good business\ndecision in the long run”[101]. Similarly, one of the people\nbehind the Ethical OS toolkit described being motivated\nto produce “a tool that helps you think through societal\nconsequences and makes sure what you are designing is\ngood for the world and good for your longer-term bottom\nline”[102].\nFinding this alignment between ethics and business is\nan  important  task  for  those  charged  with  promoting\nethics  in  tech  companies.  Recognizing  that “market\nsuccess trumps ethics”, individuals focused on ethics in\nSilicon Valley feel pressure to align ethical principles\nwith  corporate  revenue  sources[79].  As  one  senior\nresearcher in a tech company notes, “the ethics system\nthat you create has to be something that people feel adds\nvalue and is not a massive roadblock that adds no value,\nbecause if it is a roadblock that has no value, people\nliterally will not do it, because they do not have to”[79].\nWhen ethical ideals are at odds with a company’s bottom\nline, they are met with resistance[1].\nThis emphasis on business strategy creates significant\nconflicts with ethics. Corporate business models often\nrely on extractive and exploitative practices, leading to\nmany of the controversies at the heart of the techlash.\nIndeed,\n efforts\n to\n improve\n privacy\n and\n curb\ndisinformation  have  led  Facebook  and  Twitter  stock\nvalues  to  decline  rapidly[103, 104].  Thus,  even  as  tech\ncompanies espouse a devotion to ethics, they continue to\ndevelop products and services that raise ethical red flags\nbut promise significant profits. For example, even after\nreleasing AI ethics principles that include safety, privacy,\nand  inclusiveness[31] and  committing  not  to “deploy\nfacial  recognition  technology  in  scenarios  that  we\nbelieve  will  put  democratic  freedoms  at  risk”[105],\nMicrosoft  invested  in  AnyVision,  an  Israeli  facial\nrecognition company that supports military surveillance\nof Palestinians in the West Bank[106]. Similarly, several\nyears after Google withdrew from Project Maven due to\nethical concerns among employees, and then created AI\nethics  guidelines,  the  company  began  aggressively\npursuing  new  contracts  with  the  Department  of\nDefense[107].\nIn sum, tech ethics is being subsumed into existing\ntech company logics and business practices rather than\nchanging  those  logics  and  practices  (even  if  some\nindividuals  within  companies  do  want  to  create\nmeaningful change). This absorption allows companies\nto  take  up  the  mantle  of  ethics  without  making\nsubstantive  changes  to  their  processes  or  business\nstrategies.  The  goal  in  companies  is  to  find  practices\n“which the organization is not yet doing but is capable\nof  doing”[79],  indicating  an  effort  to  find  relatively\ncostless  reforms  that  provide  the  veneer  of  ethical\nbehavior.  Ethics  statements “co-opt  the  language  of\nsome critics”, taking critiques grounded in a devotion to\nequity and social justice and turning them into principles\nakin to “conventional business ethics”[71]. As they adopt\nthese principles, tech companies “are learning to speak\nand  perform  ethics  rather  than  make  the  structural\nchanges  necessary  to  achieve  the  social  values\nunderpinning the ethical fault lines that exist”[79].\nThese limits to corporate tech ethics are exemplified\nby Google’s firings of Timnit Gebru and Meg Mitchell.\nDespite Gebru’s and Mitchell’s supposed charge as co-\nleads of Google’s Ethical AI team, Google objected to\na paper they had written (alongside several internal and\nexternal co-authors) about the limitations and harms of\nlarge language models, which are central to Google’s\nbusiness[108]. Google attempted to force the authors to\nretract  the  paper,  claiming  that  they  failed  to\nacknowledge  recent  technical  advances  that  mitigate\nmany of the paper’s concerns[108]. Soon after, journalists\nrevealed  that  this  incident  reflected  a  larger  pattern:\nGoogle had expanded its review of papers that discuss\n“sensitive  topics”,  telling  researchers,  for  instance,  to\n“take  great  care  to  strike  a  positive  tone” regarding\nGoogle’s technologies and products[109]. Thus, even as\nGoogle publicly advertised its care for ethics, internally\nthe company was carefully reviewing research to curtail\nethical criticisms that it deemed threatening to its core\nbusiness interests.\n3.4    Tech ethics has become an avenue for ethics-\nwashing\nAs evidence of tech ethics’ limitations has grown, many\nhave  critiqued  tech  ethics  as  a  strategic  effort  among\ntechnology companies to maintain autonomy and profits.\n    214\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nThis  strategy  has  been  labeled “ethics-washing” (i.e.,\n“ethical  white-washing”):  adopting  the  language  of\nethics to diminish public scrutiny and avoid regulations\nthat would require substantive concessions[110−112]. As\nan ethnography of ethics in Silicon Valley found, “It is\na routine experience at ‘ethics’ events and workshops in\nSilicon Valley to hear ethics framed as a form of self-\nregulation necessary to stave off increased governmental\nregulation”[79].  This  suggests  that  the  previously\ndescribed issues with tech ethics might be features rather\nthan bugs: by focusing public attention on the actions of\nindividual engineers and on technical dilemmas (such as\nalgorithmic bias), companies perform a sleight-of-hand\nthat shifts structural questions about power and profit out\nof view. Companies can paint a self-portrait of ethical\nbehavior without meaningfully altering their practices.\nThomas Metzinger, a philosopher who served on the\nEuropean Commission’s High-Level Expert Group on\nArtificial\n Intelligence\n (AI\n HLEG),\n provides\n a\nparticularly  striking  account  of  ethics-washing  in\naction[110]. The AI HLEG contained only four ethicists\nout  of  52  total  people  and  was  dominated  by\nrepresentatives  from  industry.  Metzinger  was  tasked\nwith developing “Red Lines” that AI applications should\nnot  cross.  However,  the  proposed  red  lines  were\nultimately removed by industry representatives eager for\na “positive vision” for AI. All told, Metzinger describes\nthe AI HLEG’s guidelines as “lukewarm, short-sighted,\nand  deliberately  vague” and  concludes  that  the  tech\nindustry  is “using  ethics  debates  as  elegant  public\ndecorations for a large-scale investment strategy”[110].\nTech companies have further advanced this “ethics-\nwashing” agenda  through  funding  academic  research\nand conferences. Many of the scholars writing about tech\npolicy and ethics are funded by Google, Microsoft, and\nother  companies,  yet  often  do  not  disclose  this\nfunding[113, 114]. Tech companies also provide funding\nfor  prominent  academic  conferences,  including  the\nACM  Conference  on  Fairness,  Accountability,  and\nTransparency (FAccT); the AAAI/ACM Conference on\nArtificial Intelligence, Ethics, and Society (AIES); and\nthe Privacy Law Scholars Conference (PLSC). Even if\nthese  funding  practices  do  not  directly  influence  the\nresearch output of individual scholars, they allow tech\ncompanies  to  shape  the  broader  academic  and  public\ndiscourse  regarding  tech  ethics,  raising  certain  voices\nand conversations at the expense of others.②\nIn  December  2019,  then-MIT  graduate  student\nRodrigo  Ochigame  provided  a  particularly  pointed\naccount\n of\n ethics-washing[119].\n Describing\n his\nexperiences working in the Media Lab’s AI ethics group\nand collaborating with the Partnership on AI, Ochigame\narticulated  how “the  discourse  of ‘ethical  AI’ was\naligned strategically with a Silicon Valley effort seeking\nto avoid legally enforceable restrictions of controversial\ntechnologies”. Ochigame described witnessing firsthand\nhow the Partnership on AI made recommendations that\n“aligned  consistently  with  the  corporate  agenda” by\nreducing  political  questions  about  the  criminal  justice\nsystem to matters of technical consideration. A central\npart  of  this  effort  was  tech  companies  strategically\nfunding researchers and conferences in order to generate\na  widespread  discourse  about “ethical” technology.\nFinding  that “the  corporate  lobby’s  effort  to  shape\nacademic\n research\n was\n extremely\n successful”,\nOchigame concluded that “big tech money and direction\nproved  incompatible  with  an  honest  exploration  of\nethics”.\nOchigame’s article prompted heated debate about the\nvalue  and  impacts  of  tech  ethics.  Some  believed  that\nOchigame\n oversimplified\n the\n story,\n failing\n to\nacknowledge the many people behind tech ethics[120−122].\nOn  this  view,  tech  ethics  is  a  broad  movement  that\nincludes efforts by scholars and activists to expose and\nresist  technological  harms.  Yet  many  of  the  people\ncentrally  involved  in  those  efforts  see  their  work  as\ndistinct  from  tech  ethics.  Safiya  Noble  described\nOchigame’s article as “All the way correct and worth the\ntime  to  read”[123].  Lilly  Irani  and  Ruha  Benjamin\nexpressed similar sentiments, noting that “AI ethics is\nnot a movement”[124] and that “many of us do not frame\nour work as ‘ethical AI’”[125]. On this view, tech ethics\nrepresents  the  narrow  domain  of  efforts,  typically\npromulgated by tech companies, that explicitly embrace\nthe label of “tech ethics”.\nThe debate over Ochigame’s article exposed the fault\nlines at the heart of tech ethics. The central question is\nwhat tech ethics actually entails in practice. While some\nframe  tech  ethics  as  encompassing  broad  societal\ndebates about the social impacts of technology, others\ndefine  tech  ethics  as  narrower  industry-led  efforts  to\n② The integrity of academic tech ethics has been further called into\nquestion\n due\n to\n funding\n from\n other\n sources\n beyond\n tech\ncompanies[115−117]. A related critique of academic tech ethics institutes is\nthe lack of diversity within their leadership[118].\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n215    \n \n\nexplicitly promote “ethics” in technology. On the former\nview,  tech  ethics  is  an  important  and  beneficial\nmovement  for  improving  digital  technology.  On  the\nlatter view, tech ethics is a distraction that hinders efforts\nto achieve more equitable technology.\n4    The Contestation of Tech Ethics\nThe debates described in the previous section reveal that\nthe central question regarding tech ethics is not whether\nit is desirable to be ethical, but what “ethics” entails and\nwho gets to define it. Although the label of ethics carries\nconnotations  of  moral  philosophy,  in  practice  the\n“ethics” in tech ethics tends to take on four overlapping\nyet often conflicting definitions: moral justice, corporate\nvalues, legal risk, and compliance[126]. With all of these\nmeanings  conflated  in  the  term  ethics,  superficially\nsimilar calls for tech ethics can imply distinct and even\ncontradictory goals. There is a significant gap between\nthe potential benefits of applying ethics (as in rigorous\nnormative reasoning) to technology and the real-world\neffects of applying ethics (as in narrow and corporate-\ndriven principles) to technology.\nAs  a  result,  tech  ethics  represents  a  terrain  of\ncontestation. The contestation of tech ethics centers on\ncertain actors attempting to claim legitimate authority\nover what it means for technology to be “ethical”, at the\nexpense of other actors. These practices of “boundary-\nwork”[127] enable engineers and companies to maintain\nintellectual authority and professional autonomy, often\nin  ways  that  exclude  women,  minorities,  the  Global\nSouth,  and  other  publics[128−130].  We  can  see  this\nbehavior in technology companies projecting procedural\ntoolkits  as  solutions  to  ethical  dilemmas,  computer\nscientists\n reducing\n normative\n questions\n into\nmathematical  metrics,  academic  tech  ethics  institutes\nbeing funded by billionaires and led primarily by white\nmen,  and  tech  ethics  principles  being  disseminated\npredominantly  by  the  US  and  Western  Europe.\nFurthermore,  many  of  the  most  prominent  voices\nregarding tech ethics are white men who claim expertise\nwhile  ignoring  the  work  of  established  fields  and\nscholars,  many  of  whom  are  women  and  people  of\ncolor[131, 132].\nTwo examples of how ethics has been implemented in\nother domains—science and business—shed light on the\nstakes of present debates about tech ethics.\n4.1    Ethics in science\nMany areas of science have embraced ethics in recent\ndecades  following  public  concerns  about  the  social\nimplications  of  emerging  research  and  applications.\nDespite the seeming promise of science ethics, however,\nexisting  approaches  fail  to  raise  debates  about  the\nstructure of scientific research or to promote democratic\ngovernance of science.\nRather  than  interrogating  fundamental  questions\nabout  the  purposes  of  research  or  who  gets  to  shape\nthat\n research,\n ethics\n has\n become\n increasingly\ninstitutionalized, instrumentalized, and professionalized,\nwith an emphasis on filling out forms and checking off\nboxes[133].  Science  ethics  bodies  suffer  from  limited\n“ethical  imaginations” and  are  often  primarily\nconcerned with “keeping the wheels of research turning\nwhile satisfying publics that ethical standards are being\nmet”[133]. “Ethical analysis that does not advance such\ninstrumental  purposes  tends  to  be  downgraded  as  not\nworthy of public support”[133].\nIn turn, “systems of ethics play key roles in eliding\nfundamental  social  and  political  issues” related  to\nscientific research[134]. For instance, efforts to introduce\nethics into genetic research throughout the 1990s and\n2000s treated ethics “as something that could be added\nonto science—and not something that was unavoidably\nimplicit in it”[134]. The effort to treat ethics as an add-on\nobscured how “ethical choices inhered in efforts to study\nhuman genetic variation, regardless of any explicit effort\nto  practice  ethics”[134].  As  a  result,  these  research\nprojects “bypassed responsibility for their roles in co-\nconstituting  natural  and  moral  orderings  of  human\ndifference, despite efforts to address ethics at the earliest\nstages of research design”[134].\nThe  turn  to  ethics  can  also  entail  an  explicit  effort\namong  scientists  to  defuse  external  scrutiny  and  to\ndevelop  a  regime  of  self-governance.  In  the  1970s,\nfrightened  by  calls  for  greater  public  participation  in\ngenetic engineering, biologists organized a conference\nat the Asilomar Conference Center in California[135]. The\nscientific  community  at  Asilomar  pursued  two,\nintertwined  goals.  First,  to  present  a  unified  and\nresponsible  public  image,  the  Asilomar  organizers\nrestricted the agenda to eschew discussions of the most\ncontroversial  applications  of  genetic  engineering\n(biological  warfare  and  human  genetic  engineering).\n    216\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nSecond, to convince the American public and politicians\nthat  allow  biologists  could  self-govern  genetic\nengineering research, the Asilomar attendees “redefined\nthe genetic engineering problem as a technical one” that\nonly  biologists  could  credibly  discuss[135].  Although\nAsilomar  is  often  hailed  as  a  remarkable  occasion  of\nscientific  self-sacrifice  for  the  greater  good,  accounts\nfrom the conference itself present a different account.\n“Self-interest,  not  altruism,  was  most  evident  at\nAsilomar”, as not making any sacrifices and appearing\nself-serving  would  have  invited  stringent,  external\nregulation[135].\nTech  ethics  mirrors  many  of  these  attributes  in\nscientific  ethics.  As  with  ethics  in  other  fields  of\nscience, tech ethics involves a significant emphasis on\ninstitutionalized  design  practices,  often  entailing\nchecklists and worksheets. Mirroring ethics in genetic\nresearch, the emphasis on ethical design treats ethics as\nsomething that can be added on to digital technologies\nby individual engineers, overlooking the epistemologies\nand economic structures that shape these technologies\nand  their  harms.  Just  like  the  molecular  biologists  at\nAsilomar, tech companies and computer scientists are\ndefining moral questions as technical challenges in order\nto retain authority and autonomy.③ The removal of red\nlines in the European Commission’s High-Level Expert\nGroup on AI resembles the exclusion of controversial\ntopics from the agenda at Asilomar.\n4.2    Corporate ethics and co-optation\nCodes of ethics have long been employed by groups of\nexperts (e.g., doctors and lawyers) to codify a profession’s\nexpected  behavior  and  to  shore  up  the  profession’s\npublic reputation[137, 138]. Similarly, companies across a\nwide  range  of  sectors  have  embraced  ethics  codes,\ntypically in response to public perceptions of unethical\nbehavior[139].\nYet it has long been clear that the public benefits of\ncorporate ethics codes are minimal. While ethics codes\ncan help make a group appear ethical, they do little to\npromote a culture of ethical behavior[139]. The primary\ngoal of business ethics has instead been the “inherently\nunethical” motivation of corporate self-preservation: to\nreduce public and regulatory scrutiny by promoting a\nvisible  appearance  of  ethical  behavior[139, 140].  Ethics\ncodes  promote  corporate  reputation  and  profit  by\nmaking  universal  moral  claims  that “are  extremely\nimportant as claims but extremely vague as rules” and\nemphasizing individual actors and behaviors, leading to\na narrow, “one-case-at-a-time approach to control and\ndiscipline”[137]. Ethics codes in the field of information\nsystems have long exhibited a notable lack of explicit\nmoral obligations for computing professionals[74, 141].\nBusiness  ethics  is  indicative  of  the  broader\nphenomenon of co-optation: an institution incorporating\nelements of external critiques from groups such as social\nmovements—often  gaining  the  group’s  support  and\nimproving\n the\n institution’s\n image—without\nmeaningfully\n acting\n on\n that\n group’s\n demands\nor\n providing\n that\n group\n with\n decision-making\nauthority[142−144]. The increasing centrality of companies\nas the target of social movements has led to a particular\nform of co-optation called “corporatization”, in which\n“corporate  interests  come  to  engage  with  ideas  and\npractices initiated by a social movement and, ultimately,\nto significantly shape discourses and practices initiated\nby  the  movement”[145].  Through  this  process,  large\ncorporate  actors  in  the  United  States  have  embraced\n“diluted  and  deradicalized” elements  of  social\nmovements “that  could  be  scaled  up  and  adapted  for\nmass  markets”[145].  Two  factors  make  movements\nparticularly\n susceptible\n to\n corporatization:\nheterogeneity  (movement  factions  that  are  willing  to\nwork with companies gain influence through access to\nfunding)  and  materiality  (structural  changes  get\noverlooked\n in\n favor\n of\n easily\n commodifiable\ntechnological “fixes”). By participating in movement-\ninitiated  discourses,  companies  are  able  to  present\nthemselves as part of the solution rather than part of the\nproblem,  and  in  doing  so  can  avoid  more  restrictive\ngovernment regulations.\nTech  ethics  closely  resembles  corporate  ethics.\nAbstract and individualized tech ethics codes reproduce\nthe  virtue  signaling  and  self-preservation  behind\ntraditional business ethics. In a notable example of co-\noptation  and  corporatization,  technology  companies\nhave promoted tech ethics as a diluted and commoditized\nversion of tech-critical discourses that originated among\nactivists,  journalists,  and  critical  scholars.  Because\nsocietal efforts to improve technology are often aimed\nat  companies  and  include  both  heterogeneity  and\nmateriality,\n it\n is\n particularly\n vulnerable\n to\n③ In  an  ironic  parallel,  the  Future  of  Life  Institute  organized  an\nAsilomar  Conference  on  Beneficial  AI  in  2017,  leading  to  the\ndevelopment of 23 “Asilomar AI Principles”[136].\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n217    \n \n\ncorporatization.\n Through\n corporatization,\n tech\ncompanies use ethics to present themselves as part of the\nsolution rather than part of the problem and use funding\nto empower the voices of certain scholars and academic\ncommunities. In doing so, tech companies shore up their\nreputation and hinder external regulation. The success\nof  tech  ethics  corporatization  can  be  seen  in  the\nexpanding scope of work that is published and discussed\nunder the banner of “tech ethics”. Even scholars who do\nnot  embrace  the  tech  ethics  label  are  increasingly\nsubsumed  into  this  category,  either  lumped  into  it  by\nothers or compelled into it as opportunities to publish\nresearch,  impact  policymakers,  and  receive  grants  are\nincreasingly shifting to the terrain of “tech ethics”.\n4.3    The stakes of tech ethics\nThese examples of ethics in science and business suggest\ntwo  conclusions  about  tech  ethics.  First,  tech  ethics\ndiscourse\n enables\n technologists\n and\n technology\ncompanies  to  label  themselves  as “ethical” without\nsubstantively  altering  their  practices.  Tech  ethics\nfollows the model of science ethics and business ethics,\nwhich present case studies for how ethics-washing can\nstymie democratic debate and oversight. Continuing the\nprocess  already  underway,  tech  companies  and\ntechnologists are poised to define themselves as “ethical”\neven  while  continuing  to  generate  significant  social\nharm.  Although  some  individuals  and  groups  are\npursuing expansive forms of tech ethics, tech companies\nhave sufficient influence to promote their narrow vision\nof “tech  ethics” as  the  dominant  understanding  and\nimplementation.\nSecond, those striving for substantive and structural\nimprovements in digital technologies must be mindful\nof the gap between ethics as normative inquiry and ethics\nas a practical endeavor. Moral philosophy is essential to\nstudying  and  improving  technology,  suggesting  that\nethics is inherently desirable. However, the examples of\nethics in technology, science, and business indicate that\nethics  in  practical  contexts  can  be  quite  distinct  from\nethics as a mode of moral reasoning. It is necessary to\nrecognize  these  simultaneous  and  conflicting  roles  of\nethics. Defenders of ethics-as-moral-philosophy must be\nmindful  not  to  inadvertently  legitimize  ethics-as-\nsuperficial-practice  when  asserting  the  importance  of\nethics. Meanwhile, critics who would cede ethics to tech\ncompanies and engineers as a denuded concept should\nbe mindful that ethics-as-moral-philosophy has much to\noffer their own critiques of ethics-as-superficial-practice.\nAttending to these porous and slippery boundaries is\nessential  for  supporting  efforts  to  resist  oppressive\ndigital  technologies.  As  indicated  by  the  responses\nto  Ochigame’s  critique  of  ethics-washing,  many  of\nthe  more  radical  critics  of  digital  technology  see\nthemselves as outside of—if not in opposition to—the\ndominant strains of tech ethics. Activists, communities,\nand  scholars  have  developed  alternative  discourses\nand\n practices:\n refusal[85, \n146, \n147],\n resistance[148],\ndefense[149, 150],  abolition[150, 151],  and  decentering\ntechnology[152].  Although  some  may  see  these\nalternative  movements  as  falling  under  the  broad\numbrella of tech ethics, they embody distinct aspirations\nfrom  the  narrow  mainstream  of  tech  ethics.  Labeling\nthese burgeoning practices as part of tech ethics risks\ngiving  tech  ethics  the  imprimatur  of  radical,  justice-\noriented  work  even  as  its  core  tenets  and  practices\neschew such commitments.\n5    A Sociotechnical Approach to Tech Ethics\nRather than presenting a unifying and beneficent set of\nprinciples and practices, tech ethics has emerged as a\ncentral  site  of  struggle  regarding  the  future  of  digital\narchitectures, governance, and economies. Given these\ndynamics of contestation surrounding tech ethics, ethics\nwill not, on its own, provide a salve for technology’s\nsocial harms. In order to better evaluate the opportunities\nand limits of tech ethics, it is necessary to shift our focus\nfrom the value of ethics in theory to the impacts of ethics\nin practice.\nThis  task  calls  for  analyzing  tech  ethics  through  a\nsociotechnical  lens.  A  sociotechnical  approach  to\ntechnology emphasizes that artifacts cannot be analyzed\nin  isolation.  Instead,  it  is  necessary  to  focus  on\ntechnology’s social impacts and on how artifacts shape\nand are shaped by society. Similarly, a sociotechnical\napproach  to  tech  ethics  emphasizes  that  tech  ethics\ncannot be analyzed in isolation. Instead, it is necessary\nto focus on the social impacts of tech ethics and on how\ntech  ethics  shapes  and  is  shaped  by  society.  If\n“technologies can be assessed only in their relations to\nthe sites of their production and use”[22], then so too, we\nmight say, tech ethics can be assessed only in relation to\nthe sites of its conception and use. With this aim in mind,\nit is fruitful to consider tech ethics through the lens of\n    218\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nfour  sociotechnical  frames:  objectivity  and  neutrality,\ndeterminism, solutionism, and sociotechnical systems.\n5.1    Objectivity and neutrality\nA sociotechnical lens on technology sheds light on how\nscientists and engineers are not objective and on how\ntechnologies  are  not  neutral.  It  makes  clear  that\nimproving digital technologies requires grappling with\nthe\n normative\n commitments\n of\n engineers\n and\nincorporating  more  voices  into  the  design  of\ntechnology[153, 154].  Similarly,  it  is  necessary  to\nrecognize that the actors promoting tech ethics are not\nobjective and that tech ethics is not neutral. Currently,\nthe range of perspectives reflected in ethics principles is\nquite  narrow  and  ethics  is  treated  as  an  objective,\nuniversal  body  of  principles[29, 71, 73].  In  many  cases,\nwhite and male former technology company employees\nare cast to the front lines of public influence regarding\ntech ethics[131, 132]. As a result, the seeming consensus\naround particular ethical principles may say less about\nthe objective universality of these ideals than about the\nnarrow range of voices that influence tech ethics. Thus,\nrather than treating tech ethics as a body of objective and\nuniversal moral principles, it is necessary to grapple with\nthe  standpoints  and  power  of  different  actors,  the\nnormative  principles  embodied  in  different  ethical\nframeworks, and potential mechanisms for adjudicating\nbetween conflicting ethical commitments.\n5.2    Determinism\nA  central  component  of  a  sociotechnical  approach  to\ntechnology is rejecting technological determinism: the\nbelief  that  technology  evolves  autonomously  and\ndetermines\n social\n outcomes[155, \n156].\n Scholarship\ndemonstrates  that  even  as  technology  plays  a  role  in\nshaping society, technology and its social impacts are\nalso  simultaneously  shaped  by  society[21, 23, 157, 158].\nSimilarly, it is necessary to recognize the various factors\nthat  influence  the  impacts  of  tech  ethics  in  practice.\nCurrently, ethics in digital technology is often treated\nthrough  a  view  of “ethical  determinism”,  with  an\nunderlying assumption that adopting “ethics” will lead\nto  ethical  technologies.  Yet  evidence  from  science,\nbusiness,  and  digital  technology  demonstrates  that\nembracing “ethics” is typically not sufficient to prompt\nsubstantive changes. As with technology, ethics does not\non  its  own  determine  sociotechnical  outcomes.  We\ntherefore  need  to  consider  the  indeterminacy  of  tech\nethics: i.e., how the impacts of tech ethics are shaped by\nsocial, political, and economic forces.\n5.3    Solutionism\nClosely  intertwined  with  a  belief  in  technological\ndeterminism is the practice of technological solutionism:\nthe  expectation  that  technology  can  solve  all  social\nproblems[159]. A great deal of sociotechnical scholarship\nhas demonstrated how digital technology “solutions” to\nsocial  problems  not  only  typically  fail  to  provide  the\nintended solutions, but also can exacerbate the problems\nthey  are  intended  to  solve[160−163].  Similarly,  it  is\nnecessary to recognize the limits of what tech ethics can\naccomplish. Currently, even as tech ethics debates have\nhighlighted how technology is not always the answer to\nsocial  problems,  a  common  response  has  been  to\nembrace  an “ethical  solutionism”:  promoting  ethics\nprinciples  and  practices  as  the  solution  to  these\nsociotechnical problems. A notable example (at the heart\nof  many  tech  ethics  agendas)  is  the  response  to\nalgorithmic discrimination through algorithmic fairness,\nwhich often centers narrow mathematical definitions of\nfairness but leaves in place the structural and systemic\nconditions  that  generate  a  great  deal  of  algorithmic\nharms[164, 165].  Efforts  to  introduce  ethics  in  digital\ntechnology function similarly, providing an addendum\nof  ethical  language  and  practices  on  top  of  existing\nstructures  and  epistemologies  which  themselves  are\nlargely\n uninterrogated.\n Thus,\n just\n as\n technical\nspecifications of algorithmic fairness are insufficient to\nguarantee  fair  algorithms,  tech  ethics  principles  are\ninsufficient  to  guarantee  ethical  technologies.  Ethics\nprinciples, toolkits, and training must be integrated into\nbroader  approaches  for  improving  digital  technology\nthat  include  activism,  policy  reforms,  and  new\nengineering practices.\n5.4    Sociotechnical systems\nA  key  benefit  of  analyzing  technologies  through  a\nsociotechnical lens is expanding the frame of analysis\nbeyond the technical artifact itself. Rather than operating\nin\n isolation,\n artifacts\n are\n embedded\n within\nsociotechnical systems, such that the artifact and society\n“co-produce” social  outcomes[21].  Similarly,  it  is\nnecessary to view tech ethics as embedded within social,\neconomic, and legal environments, which shape the uses\nand impacts of tech ethics. Currently, efforts to promote\nethical  technology  typically  focus  on  the  internal\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n219    \n \n\ncharacteristics  of  tech  ethics—which  principles  to\npromote,  for  instance—with  little  attention  to  the\nimpacts  of  these  efforts  when  integrated  into  a  tech\ncompany or computer science curriculum. In turn, tech\nethics has had limited effects on technology production\nand  has  played  a  legitimizing  role  for  technology\ncompanies.  Attempts  to  promote  more  equitable\ntechnology  must  instead  consider  the  full  context  in\nwhich  tech  ethics  is  embedded.  The  impacts  of  tech\nethics are shaped by the beliefs and actions of engineers,\nthe  economic  incentives  of  companies,  cultural  and\npolitical  pressures,  and  regulatory  environments.\nEvaluating  tech  ethics  in  light  of  these  factors  can\ngenerate better predictions about how particular efforts\nwill  fare  in  practice.  Furthermore,  focusing  on  these\ncontextual factors can illuminate reforms that are more\nlikely to avoid the pitfalls associated with tech ethics.\n6    Conclusion\nA  sociotechnical  lens  on  tech  ethics  will  not  provide\nclear answers for how to improve digital technologies.\nThe technological, social, legal, economic, and political\nchallenges  are  far  too  entangled  and  entrenched  for\nsimple  solutions  or  prescriptions.  Nonetheless,  a\nsociotechnical  approach  can  help  us  reason  about  the\nbenefits and limits of tech ethics in practice. Doing so\nwill  inform  efforts  to  develop  rigorous  strategies  for\nreforming digital technologies.\nThat  is  the  task  of  this  special  issue: “Technology\nEthics  in  Action:  Critical  and  Interdisciplinary\nPerspectives”. The articles in this issue provide a range\nof perspectives regarding the value of tech ethics and the\ndesirable\n paths\n forward.\n By\n interrogating\n the\nrelationships  between  ethics,  technology,  and  society,\nwe hope to prompt reflection, debate, and action in the\nservice of a more just society.\nAcknowledgment\nB. Green thanks Elettra Bietti, Anna Lauren Hoffmann,\nJenny  Korn,  Kathy  Pham,  and  Luke  Stark  for  their\ncomments  on  this  article.  B.  Green  also  thanks  the\nHarvard STS community, particularly Sam Weiss Evans,\nfor feedback on an earlier iteration of this article.\nReferences\n A.  Marantz,  Silicon  Valley’s  crisis  of  conscience, The\nNew\n Yorker, \nhttps://www.newyorker.com/magazine/\n[1]\n2019/08/26/silicon-valleys-crisis-of-conscience, 2019.\n Oxford  Languages,  Word  of  the  year  2018:  Shortlist,\nOxford  Languages, https://languages.oup.com/word-of-\nthe-year/2018-shortlist/, 2018.\n[2]\n R. Foroohar, Year in a word: Techlash, Financial Times,\nhttps://www.ft.com/content/76578fba-fca1-11e8-ac00-\n57a2a826423e, 2018.\n[3]\n C.  E.  Emery  Jr.,  Evidence  ridiculously  thin  for\nsensational  claim  of  huge  underground  Clinton  sex\nnetwork, PolitiFact, https://www.politifact.com/factchecks\n/2016/nov/04/conservative-daily-post/evidence-\nridiculously-thin-sensational-claim-huge-/, 2016.\n[4]\n H.  Ritchie,  Read  all  about  it:  The  biggest  fake  news\nstories  of  2016, CNBC, https://www.cnbc.com/2016/\n12/30/read-all-about-it-the-biggest-fake-news-stories-of-\n2016.html, 2016.\n[5]\n A.  Blake,  A  new  study  suggests  fake  news  might  have\nwon  Donald  Trump  the  2016  election, The  Washington\nPost, https://www.washingtonpost.com/news/the-fix/wp/\n2018/04/03/a-new-study-suggests-fake-news-might-have-\nwon-donald-trump-the-2016-election/, 2018.\n[6]\n J.  Graham,  Hillary  Clinton—tech  has  to  fix  fake  news,\nUSA\n Today, \nhttps://www.usatoday.com/story/tech/\ntalkingtech/2017/05/31/hrc-tech-has-fix-fake-news/\n102357904/, 2017.\n[7]\n M. Read, Donald Trump won because of Facebook, New\nYork  Magazine, https://nymag.com/intelligencer/2016/\n11/donald-trump-won-because-of-facebook.html, 2016.\n[8]\n O.  Solon,  Facebook’s  failure:  Did  fake  news  and\npolarized  politics  get  Trump  elected? The  Guardian,\nhttps://www.theguardian.com/technology/2016/nov/10/fa\ncebook-fake-news-election-conspiracy-theories, 2016.\n[9]\n N. Perlroth, S. Frenkel, and S. Shane, Facebook exit hints\nat dissent on handling of Russian Trolls, The New York\nTimes, https://www.nytimes.com/2018/03/19/technology/\nfacebook-alex-stamos.html?mtrref=undefined, 2018.\n[10]\n K.  Hao,  How  Facebook  got  addicted  to  spreading\nmisinformation, \nMIT\n Technology\n Review,\nhttps://www.technologyreview.com/2021/03/11/1020600\n/facebook-responsible-ai-misinformation/, 2021.\n[11]\n C.  Cadwalladr  and  E.  Graham-Harrison,  Revealed:  50\nmillion  Facebook  profiles  harvested  for  Cambridge\nAnalytica  in  major  data  breach, The  Guardian,\nhttps://www.theguardian.com/news/2018/mar/17/cambrid\nge-analytica-facebook-influence-us-election, 2018.\n[12]\n M. Rosenberg, N. Confessore, and C. Cadwalladr, How\nTrump  consultants  exploited  the  Facebook  data  of\nmillions, The New York Times, https://www.nytimes.com/\n2018/03/17/us/politics/cambridge-analytica-trump-\ncampaign.html, 2018.\n[13]\n D.  Cameron  and  K.  Conger,  Google  is  helping  the\nPentagon\n build\n AI\n for\n drones, \nGizmodo,\nhttps://gizmodo.com/google-is-helping-the-pentagon-\nbuild-ai-for-drones-1823464533, 2018.\n[14]\n N.  Tiku,  Three  years  of  misery  inside  Google,  the\nhappiest  company  in  tech, Wired, https://www.wired.\ncom/story/inside-google-three-years-misery-happiest-\ncompany-tech/, 2019.\n[15]\n S.  Woodman,  Palantir  provides  the  engine  for  Donald\nTrump’s\n deportation\n machine, \nThe\n Intercept,\n[16]\n    220\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nhttps://theintercept.com/2017/03/02/palantir-provides-\nthe-engine-for-donald-trumps-deportation-machine/,\n2017.\n J. Angwin, J. Larson, S. Mattu, and L. Kirchner, Machine\nbias, \nProPublica, \nhttps://www.propublica.org/article/\nmachine-bias-risk-assessments-in-criminal-sentencing,\n2016.\n[17]\n V.  Eubanks, Automating  Inequality:  How  High-Tech\nTools  Profile,  Police,  and  Punish  the Poor.  New  York,\nNY, USA: St. Martin's Press, 2018.\n[18]\n S.  U.  Noble, Algorithms  of  Oppression:  How  Search\nEngines Reinforce Racism. New York, NY, USA: NYU\nPress, 2018.\n[19]\n C. O'Neil, Weapons of Math Destruction: How Big Data\nIncreases  Inequality  and  Threatens  Democracy.  New\nYork, NY, USA: Broadway Books, 2017.\n[20]\n S.  Jasanoff,  The  idiom  of  co-production,  in States  of\nKnowledge: The Co-Production of Science and the Social\nOrder,  S.  Jasanoff,  ed.  London,  UK:  Routledge,  2004,\npp. 1–12.\n[21]\n L.  Suchman,  J.  Blomberg,  J.  E.  Orr,  and  R.  Trigg,\nReconstructing technologies as social practice, American\nBehavioral Scientist, vol. 43, no. 3, pp. 392–408, 1999.\n[22]\n W. E. Bijker and J. Law, Shaping Technology / Building\nSociety:  Studies  in  Sociotechnical  Change.  Cambridge,\nMA, USA: MIT Press, 1992.\n[23]\n D.  G.  Johnson  and  J.  M.  Wetmore,  STS  and  ethics:\nImplications for engineering ethics, in The Handbook of\nScience  and  Technology  Studies,  Third  Edition,  E.  J.\nHackett,  O.  Amsterdamska,  M.  E.  Lynch,  and  J.\nWajcman, eds. Cambridge, MA, USA: MIT Press, 2007,\npp. 567–581.\n[24]\n C. Fiesler, What our tech ethics crisis says about the state\nof  computer  science  education, How  We  Get  to  Next,\nhttps://howwegettonext.com/what-our-tech-ethics-crisis-\nsays-about-the-state-of-computer-science-education-\na6a5544e1da6, 2018.\n[25]\n P.  Karoff,  Embedding  ethics  in  computer  science\ncurriculum, The  Harvard  Gazette, https://news.harvard.\nedu/gazette/story/2019/01/harvard-works-to-embed-\nethics-in-computer-science-curriculum/, 2019.\n[26]\n I. Raicu, Rethinking ethics training in Silicon Valley, The\nAtlantic, https://www.theatlantic.com/technology/archive/\n2017/05/rethinking-ethics-training-in-silicon-\nvalley/525456/, 2017.\n[27]\n Y. Zunger, Computer science faces an ethics crisis. The\nCambridge  Analytica  scandal  proves  it. The  Boston\nGlobe, https://www.bostonglobe.com/ideas/2018/03/22/\ncomputer-science-faces-ethics-crisis-the-cambridge-\nanalytica-scandal-proves/IzaXxl2BsYBtwM4nxezgcP/\nstory.html, 2018.\n[28]\n J.  Fjeld,  N.  Achten,  H.  Hilligoss,  A.  C.  Nagy,  and  M.\nSrikumar,  Principled  artificial  intelligence:  Mapping\nconsensus  in  ethical  and  rights-based  approaches  to\nprinciples  for  AI, Berkman  Klein  Center  Research\nPublication\n No.\n 2020-1, \nhttps://cyber.harvard.edu/\npublication/2020/principled-ai, 2020.\n[29]\n IBM,\n Everyday\n ethics\n for\n artificial\n intelligence,\nhttps://www.ibm.com/watson/assets/duo/pdf/everydayeth\nics.pdf, 2018.\n[30]\n Microsoft,\n Microsoft\n AI\n principles, \nhttps://www.\nmicrosoft.com/en-us/ai/responsible-ai, 2018.\n[31]\n S. Pichai, AI at Google: Our principles, https://www.blog.\ngoogle/technology/ai/ai-principles/, 2018.\n[32]\n S.  Legassick  and  V.  Harding,  Why  we  launched\nDeepMind\n ethics\n &\n society, \nDeepMind\n Blog,\nhttps://deepmind.com/blog/announcements/why-we-\nlaunched-deepmind-ethics-society, 2017.\n[33]\n S.  Nadella,  Embracing  our  future:  Intelligent  cloud  and\nintelligent\n edge, \nMicrosoft\n News\n Center,\nhttps://news.microsoft.com/2018/03/29/satya-nadella-\nemail-to-employees-embracing-our-future-intelligent-\ncloud-and-intelligent-edge/, 2018.\n[34]\n J. Novet, Facebook forms a special ethics team to prevent\nbias in its A. I. software, CNBC, https://www.cnbc.com/\n2018/05/03/facebook-ethics-team-prevents-bias-in-ai-\nsoftware.html, 2018.\n[35]\n J.  Vincent  and  R.  Brandom,  Axon  launches  AI  ethics\nboard  to  study  the  dangers  of  facial  recognition, The\nVerge, \nhttps://www.theverge.com/2018/4/26/17285034/\naxon-ai-ethics-board-facial-recognition-racial-bias, 2018.\n[36]\n K. Walker, Google AI principles updates, six months in,\nThe  Keyword, https://www.blog.google/technology/ai/\ngoogle-ai-principles-updates-six-months/, 2018.\n[37]\n T. Simonite, Google offers to help others with the tricky\nethics of AI, Wired, https://www.wired.com/story/google-\nhelp-others-tricky-ethics-ai/, 2020.\n[38]\n Accenture,  AI  ethics  &  governance, https://www.\naccenture.com/us-en/services/applied-intelligence/ai-\nethics-governance, 2021.\n[39]\n M. Dowd, Elon Musk’s billion-dollar crusade to stop the\nA. I. apocalypse, Vanity Fair, https://www.vanityfair.com/\nnews/2017/03/elon-musk-billion-dollar-crusade-to-stop-\nai-space-x, 2017.\n[40]\n K. Finley, Tech giants team up to keep AI from getting\nout  of  hand, Wired, https://www.wired.com/2016/09/\ngoogle-facebook-microsoft-tackle-ethics-ai/, 2016.\n[41]\n A.  Hern,  'Partnership  on  AI'  formed  by  Google,\nFacebook, Amazon, IBM and Microsoft, The Guardian,\nhttps://www.theguardian.com/technology/2016/sep/28/go\nogle-facebook-amazon-ibm-microsoft-partnership-on-ai-\ntech-firms, 2016.\n[42]\n Access Now, Access now resigns from the partnership on\nAI, https://www.accessnow.org/access-now-resignation-\npartnership-on-ai/, 2020.\n[43]\n B. J. Grosz, D. G. Grant, K. Vredenburgh, J. Behrends, L.\nHu,  A.  Simmons,  and  J.  Waldo,  Embedded  EthiCS:\nIntegrating  ethics  broadly  across  computer  science\neducation, Communications  of  the  ACM,  vol. 62,  no. 8,\npp. 54–61, 2019.\n[44]\n R.  Reich,  M.  Sahami,  J.  M.  Weinstein,  and  H.  Cohen,\nTeaching  computer  ethics:  A  deeply  multidisciplinary\napproach,  in Proc.  the  51st  ACM  Technical  Symposium\non  Computer  Science  Education,  Portland,  OR,  USA,\n2020, pp. 296–302.\n[45]\n K.  Shilton,  M.  Zimmer,  C.  Fiesler,  A.  Narayanan,  J.\nMetcalf, M. Bietz, and J. Vitak, We’re awake — But we’re\nnot  at  the  wheel, PERVADE:  Pervasive  Data  Ethics,\nhttps://medium.com/pervade-team/were-awake-but-we-\nre-not-at-the-wheel-7f0a7193e9d5, 2017.\n[46]\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n221    \n \n\n C. Fiesler, N. Garrett, and N. Beard, What do we teach\nwhen we teach tech ethics? A syllabi analysis, in Proc.\nthe  51st  ACM  Technical  Symposium  on  Computer\nScience  Education,  Portland,  OR,  USA,  2020,  pp.\n289–295.\n[47]\n C. Fiesler, Tech ethics curricula: A collection of syllabi,\nhttps://medium.com/@cfiesler/tech-ethics-curricula-a-\ncollection-of-syllabi-3eedfb76be18, 2018.\n[48]\n ACM  FAccT  Conference,  ACM  FAccT  network,\nhttps://facctconference.org/network/, 2021.\n[49]\n AI Now Institute, The AI now institute launches at NYU\nto  examine  the  social  effects  of  artificial  intelligence,\nhttps://ainowinstitute.org/press-release-ai-now-launch,\n2017.\n[50]\n M. Sharlach, Princeton collaboration brings new insights\nto  the  ethics  of  artificial  intelligence, https://www.\nprinceton.edu/news/2019/01/14/princeton-collaboration-\nbrings-new-insights-ethics-artificial-intelligence, 2019.\n[51]\n MIT Media Lab, MIT Media Lab to participate in ＄27\nmillion  initiative  on  AI  ethics  and  governance,\nMIT  News, https://news.mit.edu/2017/mit-media-lab-to-\nparticipate-in-ai-ethics-and-governance-initiative-0110,\n2017.\n[52]\n MIT  News  Office,  MIT  reshapes  itself  to  shape  the\nfuture, MIT News, http://news.mit.edu/2018/mit-reshapes-\nitself-stephen-schwarzman-college-of-computing-1015,\n2018.\n[53]\n A. Adams, Stanford University launches the institute for\nhuman-centered  artificial  intelligence, Stanford  News,\nhttps://news.stanford.edu/2019/03/18/stanford_university\n_launches_human-centered_ai/, 2019.\n[54]\n S.  Marowski,  Artificial  intelligence  researchers  create\nethics  center  at  University  of  Michigan, MLive,\nhttps://www.mlive.com/news/ann-arbor/2020/01/\nartificial-intelligence-researchers-create-ethics-center-at-\nuniversity-of-michigan.html, 2020.\n[55]\n D. Robinson and M. Bogen, Data ethics: Investing wisely\nin  data  at  scale, Upturn, https://www.upturn.org/static/\nreports/2016/data-ethics/files/Upturn_-_Data%20Ethics_v.\n1.0.pdf, 2016.\n[56]\n Mozilla,  Announcing  a  competition  for  ethics  in\ncomputer science, with up to ＄3.5 million in prizes, The\nMozilla  Blog, https://blog.mozilla.org/blog/2018/10/10/\nannouncing-a-competition-for-ethics-in-computer-\nscience-with-up-to-3-5-million-in-prizes/, 2018.\n[57]\n V.  Eubanks,  A  hippocratic  oath  for  data  science,\nhttps://virginia-eubanks.com/2018/02/21/a-hippocratic-\noath-for-data-science/, 2018.\n[58]\n D.  J.  Patil,  A  code  of  ethics  for  data  science,\nhttps://www.linkedin.com/pulse/code-ethics-data-\nscience-dj-patil/, 2018.\n[59]\n T.  Simonite,  Should  data  scientists  adhere  to  a\nhippocratic  oath? Wired, https://www.wired.com/story/\nshould-data-scientists-adhere-to-a-hippocratic-oath/,\n2018.\n[60]\n Data4Democracy,  Ethics  resources, https://github.com/\nData4Democracy/ethics-resources, 2018.\n[61]\n The  Institute  for  the  Future  and  Omidyar  Network,\nEthical OS Toolkit, https://ethicalos.org, 2018.\n[62]\n D. boyd, G. Bowker, K. Crawford, and H. Nissenbaum,\n[63]\nCouncil  for  Big  Data,  Ethics,  and  Society, https://\nbdes.datasociety.net, 2014.\n National  Science  and  Technology  Council,  Preparing\nfor\n the\n future\n of\n artificial\n intelligence,\nhttps://obamawhitehouse.archives.gov/sites/default/files/\nwhitehouse_files/microsites/ostp/NSTC/preparing_for_th\ne_future_of_ai.pdf, 2018.\n[64]\n U.  S.  Department  of  Defense,  DOD  adopts  ethical\nprinciples\n for\n artificial\n intelligence, \nhttps://www.\ndefense.gov/Newsroom/Releases/Release/Article/209199\n6/dod-adopts-ethical-principles-for-artificial-\nintelligence/, 2020.\n[65]\n Smart  Dubai,  AI  ethics  principles  &  guidelines,\nhttps://www.smartdubai.ae/docs/default-source/ai-\nprinciples-resources/ai-ethics.pdf?sfvrsn=d4184f8d_6,\n2018.\n[66]\n High-Level  Expert  Group  on  AI,  Ethics  guidelines  for\ntrustworthy\n AI, \nhttps://ec.europa.eu/newsroom/dae/\ndocument.cfm?doc_id=60419, 2019.\n[67]\n Integrated Innovation Strategy Promotion Council, AI for\neveryone:  People,  industries,  regions  and  governments,\nhttps://www8.cao.go.jp/cstp/english/humancentricai.pdf,\n2019.\n[68]\n E.  Martinho-Truswell,  H.  Miller,  I.  N.  Asare,  A.\nPetheram,  R.  Stirling,  C.  G.  Mont,  and  C.  Martínez,\nHacia una Estrategia de IA en México: Aprovechando la\nRevolución de la IA (Towards an AI strategy in Mexico:\nLeveraging  the  AI  revolution), https://docs.wixstatic.\ncom/ugd/7be025_ba24a518a53a4275af4d7ff63b4cf594.p\ndf, 2018.\n[69]\n Organisation\n for\n Economic\n Co-operation\n and\nDevelopment,  Recommendation  of  the  Council  on\nArtificial Intelligence, https://legalinstruments.oecd.org/en/\ninstruments/OECD-LEGAL-0449, 2019.\n[70]\n D. Greene, A. L. Hoffmann, and L. Stark, Better, nicer,\nclearer, fairer: A critical assessment of the movement for\nethical  artificial  intelligence  and  machine  learning,  in\nProc.  the  52nd  Hawaii  International  Conference  on\nSystem  Sciences,  Grand  Wailea,  HI,  USA,  2019,  pp.\n2122–2131.\n[71]\n Accenture,\n Universal\n principles\n of\n data\n ethics,\nhttps://www.accenture.com/_acnmedia/pdf-24/accenture-\nuniversal-principles-data-ethics.pdf, 2016.\n[72]\n A. Jobin, M. Ienca, and E. Vayena, The global landscape\nof  AI  ethics  guidelines, Nature  Machine  Intelligence,\nvol. 1, no. 9, pp. 389–399, 2019.\n[73]\n L.  Stark  and  A.  L.  Hoffmann,  Data  is  the  new  what?\nPopular  metaphors  &  professional  ethics  in  emerging\ndata  cultures, Journal  of  Cultural  Analytics,  doi:\n10.22148/16.036.\n[74]\n B. Mittelstadt, Principles alone cannot guarantee ethical\nAI, \nNature\n Machine\n Intelligence,\n vol. 1,\n no. 11,\npp. 501–507, 2019.\n[75]\n D. Harwell, Facial recognition may be coming to a police\nbody\n camera\n near\n you, \nThe\n Washington\n Post,\nhttps://www.washingtonpost.com/news/the-switch/wp/2018/\n04/26/facial-recognition-may-be-coming-to-a-police-\nbody-camera-near-you/, 2018.\n[76]\n W. Knight, Google appoints an “AI council” to head off\ncontroversy, but it proves controversial, MIT Technology\n[77]\n    222\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\nReview, https://www.technologyreview.com/2019/03/26/\n136376/google-appoints-an-ai-council-to-head-off-\ncontroversy-but-it-proves-controversial, 2019.\n S.  S.  Silbey,  How  not  to  teach  ethics, MIT  Faculty\nNewsletter, \nhttps://web.mit.edu/fnl/volume/311/silbey.\nhtml, 2018.\n[78]\n J.  Metcalf,  E.  Moss,  and  D.  Boyd,  Owning  ethics:\nCorporate\n logics,\n Silicon\n Valley,\n and\n the\ninstitutionalization  of  ethics, Social  Research,  vol. 86,\nno. 2, pp. 449–476, 2019.\n[79]\n T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan,\nH. Wallach, H. Daumé III, and K. Crawford, Datasheets\nfor datasets, arXiv preprint arXiv: 1803.09010, 2018.\n[80]\n M.  Mitchell,  S.  Wu,  A.  Zaldivar,  P.  Barnes,  L.\nVasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T.\nGebru,  Model  cards  for  model  reporting,  in Proc.  the\nConference\n on\n Fairness,\n Accountability,\n and\nTransparency, Atlanta, GA, USA, 2019, pp. 220–229.\n[81]\n K.  R.  Varshney,  Introducing  AI  fairness  360, IBM\nResearch\n Blog, \nhttps://www.ibm.com/blogs/research/\n2018/09/ai-fairness-360/, 2018.\n[82]\n A. Peters, This tool lets you see–and correct–the bias in\nan  algorithm, Fast  Company, https://www.fastcompany.\ncom/40583554/this-tool-lets-you-see-and-correct-the-\nbias-in-an-algorithm, 2018.\n[83]\n D. Gershgorn, Facebook says it has a tool to detect bias\nin  its  artificial  intelligence, Quartz, https://qz.com/\n1268520/facebook-says-it-has-a-tool-to-detect-bias-in-\nits-artificial-intelligence/, 2018.\n[84]\n K. Conger and C. Metz, Tech workers now want to know:\nWhat  are  we  building  this  for? The  New  York  Times,\nhttps://www.nytimes.com/2018/10/07/technology/tech-\nworkers-ask-censorship-surveillance.html, 2018.\n[85]\n R. Gallagher, Google shut out privacy and security teams\nfrom  secret  China  project, The  Intercept, https://\ntheintercept.com/2018/11/29/google-china-censored-\nsearch/, 2018.\n[86]\n K. Crawford, R. Dobbe, T. Dryer, G. Fried, B. Green, E.\nKaziunas,  A.  Kak,  V.  Mathur,  E.  McElroy,  A.  N.\nSánchez,\n et\n al.,\n AI\n now\n 2019\n report,\nhttps://ainowinstitute.org/AI_Now_2019_Report.pdf,\n2019.\n[87]\n C. Haskins, The Los Angeles police department says it is\ndumping  a  controversial  predictive  policing  tool,\nBuzzFeed  News, https://www.buzzfeednews.com/article/\ncarolinehaskins1/los-angeles-police-department-\ndumping-predpol-predictive, 2020.\n[88]\n B.  Schneier, Data  and  Goliath:  The  Hidden  Battles  to\nCollect Your Data and Control Your World. New York,\nNY, USA: W. W. Norton & Company, 2015.\n[89]\n S. Viljoen, A relational theory of data governance, Yale\nLaw Journal, vol. 131, no. 2, pp. 573–654, 2021.\n[90]\n L. M. Khan, Amazon’s antitrust paradox, The Yale Law\nJournal, vol. 126, no. 3, pp. 564–907, 2017.\n[91]\n T. Wu, The Curse of Bigness: Antitrust in the New Gilded\nAge.  New  York,  NY,  USA:  Columbia  Global  Reports,\n2018.\n[92]\n K. Crawford and V. Joler, Anatomy of an AI system: The\nAmazon Echo as an anatomical map of human labor, data\nand planetary resources, https://anatomyof.ai, 2018.\n[93]\n R. Dobbe and M. Whittaker, AI and climate change: How\nthey’re connected, and what we can do about it, AI Now\nInstitute, \nhttps://medium.com/@AINowInstitute/ai-and-\nclimate-change-how-theyre-connected-and-what-we-can-\ndo-about-it-6aa8d0f5b32c, 2019.\n[94]\n W.  Evans,  Ruthless  quotas  at  Amazon  are  maiming\nemployees, The  Atlantic, https://www.theatlantic.com/\ntechnology/archive/2019/11/amazon-warehouse-reports-\nshow-worker-injuries/602530/, 2019.\n[95]\n M. L. Gray and S. Suri, Ghost Work: How to Stop Silicon\nValley from Building a New Global Underclass. Boston,\nMA, USA: Houghton Mifflin Harcourt, 2019.\n[96]\n S. Jasanoff, Technology as a site and object of politics, in\nThe  Oxford  Handbook  of  Contextual  Political  Analysis,\nR.  E.  Goodin  and  C.  Tilly,  eds.  New  York,  NY,  USA:\nOxford University Press, 2006, pp. 745–766.\n[97]\n S.  M.  West,  M.  Whittaker,  and  K.  Crawford,\nDiscriminating systems: Gender, race, and power in AI,\nhttps://ainowinstitute.org/discriminatingsystems.pdf,\n2019.\n[98]\n T. Simonite, Google and Microsoft warn that AI may do\ndumb\n things, \nWired, \nhttps://www.wired.com/story/\ngoogle-microsoft-warn-ai-may-do-dumb-things/, 2019.\n[99]\n D. Seetharaman, Jack Dorsey’s push to clean up Twitter\nstalls,  researchers  say, The  Wall  Street  Journal,\nhttps://www.wsj.com/articles/jack-dorseys-push-to-clean-\nup-twitter-stalls-researchers-say-11584264600, 2020.\n[100]\n K.\n Johnson,\n How\n to\n operationalize\n AI\n ethics,\nVentureBeat, https://venturebeat.com/2019/10/07/how-to-\noperationalize-ai-ethics/, 2019.\n[101]\n A. Pardes, Silicon Valley writes a playbook to help avert\nethical  disasters, Wired, https://www.wired.com/story/\nethical-os/, 2018.\n[102]\n R. Neate, Twitter stock plunges 20% in wake of 1m user\ndecline, The  Guardian, https://www.theguardian.com/\ntechnology/2018/jul/27/twitter-share-price-tumbles-after-\nit-loses-1m-users-in-three-months, 2018.\n[103]\n S.  Subin,  Facebook,  Twitter  and  digital  ad  stocks  drop\nsharply  after  Snap  earnings, CNBC, https://www.cnbc.\ncom/2021/10/21/facebook-twitter-and-digital-ad-stocks-\ndrop-sharply-after-snap-earnings.html, 2021.\n[104]\n B.  Smith,  Facial  recognition:  It’s  time  for  action,\nMicrosoft On The Issues, https://blogs.microsoft.com/on-\nthe-issues/2018/12/06/facial-recognition-its-time-for-\naction/, 2018.\n[105]\n O.  Solon,  Why  did  Microsoft  fund  an  Israeli  firm  that\nsurveils\n West\n Bank\n Palestinians? \nNBC\n News,\nhttps://www.nbcnews.com/news/all/why-did-microsoft-\nfund-israeli-firm-surveils-west-bank-palestinians-\nn1072116, 2019.\n[106]\n D. Wakabayashi and K. Conger, Google wants to work\nwith the Pentagon again, despite employee concerns, The\nNew  York  Times, https://www.nytimes.com/2021/11/\n03/technology/google-pentagon-artificial-intelligence.\nhtml, 2021.\n[107]\n K. Hao, We read the paper that forced Timnit Gebru out\nof Google. Here’s what it says. MIT Technology Review,\nhttps://www.technologyreview.com/2020/12/04/1013294\n/google-ai-ethics-research-paper-forced-out-timnit-\ngebru/, 2020.\n[108]\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n223    \n \n\n P. Dave and J. Dastin, Google told its scientists to ‘strike\na  positive  tone’ in  AI  research  -  documents, Reuters,\nhttps://www.reuters.com/article/us-alphabet-google-\nresearch-focus/google-told-its-scientists-to-strike-a-\np o s i t i v e - t o n e - i n - a i - r e s e a r c h - d o c u m e n t s -\nidUSKBN28X1CB, 2020.\n[109]\n T.  Metzinger,  Ethics  washing  made  in  Europe, Der\nTagesspiegel, \nhttps://www.tagesspiegel.de/politik/eu-\nguidelines-ethics-washing-made-in-europe/24195496.\nhtml, 2019.\n[110]\n P.  Nemitz,  Constitutional  democracy  and  technology  in\nthe\n age\n of\n artificial\n intelligence, \nPhilosophical\nTransactions  of  the  Royal  Society  A:Mathematical,\nPhysical  and  Engineering  Sciences,  vol. 376,  no. 2133,\np. 20180089, 2018.\n[111]\n B.  Wagner,  Ethics  as  escape  from  regulation:  From\nethics-washing  to  ethics-shopping?  in Being  Profiling.\nCogitas Ergo Sum, E. Bayamlioglu, I. Baraliuc, L. A. W.\nJanssens,  and  M.  Hildebrandt,  eds.  Amsterdam,  the\nNetherlands:  Amsterdam  University  Press,  2018,  pp.\n84–89.\n[112]\n Google  Transparency  Project,  Google  Academics  Inc.,\nhttps://www.techtransparencyproject.org/sites/default/file\ns/Google-Academics-Inc.pdf, 2017.\n[113]\n O. Williams, How big tech funds the debate on AI ethics,\nNew Statesman, https://www.newstatesman.com/science-\ntech/technology/2019/06/how-big-tech-funds-debate-ai-\nethics, 2019.\n[114]\n A.  E.  Domínguez,  R.  Bassett-Audain,  H.  Karimi,  B.\nEstrada, C. I. Webb, R. Perry, S. Haslanger, J. King, K.\nLeonardo, S. Aladetan, et al., Celebrating war criminals\nat  MIT’s ‘ethical’ College  of  Computing, The  Tech,\nhttps://thetech.com/2019/02/14/celebrating-war-\ncriminals, 2019.\n[115]\n R.  Farrow,  How  an  Élite  University  Research  Center\nconcealed its relationship with Jeffrey Epstein, The New\nYorker, \nhttps://www.newyorker.com/news/news-desk/\nhow-an-elite-university-research-center-concealed-its-\nrelationship-with-jeffrey-epstein, 2019.\n[116]\n A.  Mboya,  Why  Joi  Ito  needs  to  resign, The  Tech,\nhttps://thetech.com/2019/08/29/joi-ito-needs-to-resign,\n2019.\n[117]\n D.\n Gershgorn,\n Stanford’s\n new\n AI\n institute\n is\ninadvertently showcasing one of tech’s biggest problems,\nQuartz, \nhttps://qz.com/1578617/stanfords-new-diverse-\nai-institute-is-overwhelmingly-white-and-male/, 2019.\n[118]\n R.  Ochigame,  The  Invention  of “Ethical  AI”:  How  big\ntech  manipulates  academia  to  avoid  regulation, The\nIntercept, https://theintercept.com/2019/12/20/mit-ethical-\nai-artificial-intelligence/, 2019.\n[119]\n K. Darling, https://twitter.com/grok_/status/12084349725\n64037633, 2019.\n[120]\n G. Epstein, https://twitter.com/gregmepstein/status/12087\n98637221974016, 2019.\n[121]\n C. Sinders, https://twitter.com/carolinesinders/status/1208\n443559998873601, 2019.\n[122]\n S. U. Noble, https://twitter.com/safiyanoble/status/120881\n2440403660800, 2019.\n[123]\n L.  Irani, https://twitter.com/gleemie/status/12087934425\n09152258, 2019.\n[124]\n R.  Benjamin, https://twitter.com/ruha9/status/12088319\n99940714496, 2019.\n[125]\n E.  Moss  and  J.  Metcalf,  Too  Big  a  Word, Data  &\nSociety:  Points, https://points.datasociety.net/too-big-a-\nword-13e66e62a5bf, 2020.\n[126]\n T.  F.  Gieryn,  Boundary-work  and  the  demarcation  of\nscience  from  non-science:  Strains  and  interests  in\nprofessional\n ideologies\n of\n scientists, \nAmerican\nSociological Review, vol. 48, no. 6, pp. 781–795, 1983.\n[127]\n P.  H.  Collins, Black  Feminist  Thought:  Knowledge,\nConsciousness,  and  the  Politics  of  Empowerment.\nLondon, UK: Routledge, 2008.\n[128]\n D. Haraway, Situated knowledges: The science question\nin  feminism  and  the  privilege  of  partial  perspective,\nFeminist studies, vol. 14, no. 3, pp. 575–599, 1988.\n[129]\n S.  Visvanathan,  Knowledge,  justice  and  democracy,  in\nScience and Citizens: Globalization and the Challenge of\nEngagement, M. Leach, I. Scoones, and B. Wynne, eds.\nLondon, UK: Zed Books, 2005, pp. 83–94.\n[130]\n Mozilla, \nhttps://twitter.com/mozilla/status/130854290\n8291661824, 2020.\n[131]\n L.  Irani  and  R.  Chowdhury,  To  really ‘disrupt,’ tech\nneeds to listen to actual researchers, Wired, https://www.\nwired.com/story/tech-needs-to-listen-to-actual-researchers/,\n2019.\n[132]\n S. Jasanoff, The Ethics of Invention: Technology and the\nHuman Future. New York, NY, USA: W. W. Norton &\nCompany, 2016.\n[133]\n J.  Reardon,  Human  population  genomics  and  the\ndilemma\n of\n difference,\n in \nReframing\n Rights:\nBioconstitutionalism in the Genetic Age, S. Jasanoff, ed.\nCambridge, MA, USA: MIT Press, 2011, pp. 217–238.\n[134]\n S.\n Wright,\n Legitimating\n genetic\n engineering,\nPerspectives  in  Biology  and  Medicine,  vol. 44,  no. 2,\npp. 235–247, 2001.\n[135]\n Future\n of\n Life\n Institute,\n Beneficial\n AI\n 2017,\nhttps://futureoflife.org/bai-2017/, 2017.\n[136]\n A.  Abbott,  Professional  ethics, American  Journal  of\nSociology, vol. 88, no. 5, pp. 855–885, 1983.\n[137]\n J. Metcalf, Ethics codes: History, context, and challenges,\nhttps://bdes.datasociety.net/wp-content/uploads/\n2016/10/EthicsCodes.pdf, 2014.\n[138]\n G. Wood and M. Rimmer, Codes of ethics: What are they\nreally and what should they be? International Journal of\nValue-Based  Management,  vol. 16,  no. 2,  pp. 181–195,\n2003.\n[139]\n D. R. Cressey and C. A. Moore, Managerial values and\ncorporate  codes  of  ethics, California  Management\nReview, vol. 25, no. 4, pp. 53–77, 1983.\n[140]\n E.  Oz,  Ethical  standards  for  information  systems\nprofessionals: A case for a unified code, MIS quarterly,\nvol. 16, no. 4, pp. 423–433, 1992.\n[141]\n W.  A.  Gamson, The  Strategy  of  Social  Protest.\nHomewood, IL, USA: The Dorsey Press, 1975.\n[142]\n P.  Selznick,  Foundations  of  the  theory  of  organization,\nAmerican Sociological Review, vol. 13, no. 1, pp. 25–35,\n1948.\n[143]\n A.  J.  Trumpy,  Subject  to  negotiation:  The  mechanisms\nbehind  co-optation  and  corporate  reform, Social\nProblems, vol. 55, no. 4, pp. 480–500, 2014.\n[144]\n L. King and J. Busa, When corporate actors take over the\n[145]\n    224\nJournal of Social Computing, September 2021, 2(3): 209−225    \n \n\ngame: the corporatization of organic, recycling and breast\ncancer activism, Social Movement Studies, vol. 16, no. 5,\npp. 549–563, 2017.\n E. Graeff, The responsibility to not design and the need\nfor  citizen  professionalism, Tech  Otherwise,  doi:\n10.21428/93b2c832.c8387014.\n[146]\n M. Cifor, P. Garcia, T. L. Cowan, J. Rault, T. Sutherland,\nA. S. Chan, J. Rode, A. L. Hoffmann, N. Salehi, and L.\nNakamura,  Feminist  Data  Manifest-No, https://www.\nmanifestno.com, 2019.\n[147]\n A. -E. M. Project, Counterpoints: A San Francisco Bay\nArea Atlas of Displacement & Resistance. Oakland, CA,\nUSA: PM Press, 2021.\n[148]\n T.  Lewis,  S.  P.  Gangadharan,  M.  Saba,  and  T.  Petty,\nDigital  defense  playbook:  Community  power  tools  for\nreclaiming  data,  Technical  report,  Our  data  bodies,\nDetroit, MI, USA, 2018.\n[149]\n S.  T.  Hamid,  Community  defense:  Sarah  T.  Hamid  on\nabolishing carceral technologies, Logic, https://logicmag.\nio/care/community-defense-sarah-t-hamid-on-abolishing-\ncarceral-technologies/, 2020.\n[150]\n Stop  LAPD  spying  coalition  and  free  radicals,  the\nalgorithmic  ecology:  An  abolitionist  tool  for  organizing\nagainst\n algorithms, \nMedium, \nhttps://stoplapdspying.\nmedium.com/the-algorithmic-ecology-an-abolitionist-\ntool-for-organizing-against-algorithms-14fcbd0e64d0,\n2020.\n[151]\n S. P. Gangadharan and J. Niklas, Decentering technology\nin\n discourse\n on\n discrimination, \nInformation,\nCommunication & Society,  vol. 22,  no. 7,  pp. 882–899,\n2019.\n[152]\n S.  Costanza-Chock, Design  Justice:  Community-Led\nPractices to Build the Worlds We Need. Cambridge, MA,\nUSA: MIT Press, 2020.\n[153]\n B. Green and S. Viljoen, Algorithmic realism: Expanding\nthe boundaries of algorithmic thought, in Proc. the 2020\nConference\n on\n Fairness,\n Accountability,\n and\nTransparency, Barcelona, Spain, 2020, pp. 19–31.\n[154]\n A.  Dafoe,  On  technological  determinism:  A  typology,\nscope conditions, and a mechanism, Science, Technology,\n& Human Values, vol. 40, no. 6, pp. 1047–1076, 2015.\n[155]\n L.  Marx  and  M.  R.  Smith,  Introduction,  in Does\nTechnology\n Drive\n Hisstory?:\n The\n Dilemma\n of\nTechnological  Determinism,  M.  R.  Smith  and  L.  Marx,\n[156]\neds.  Cambridge,  MA,  USA:  MIT  Press,  1994,  pp.\nIX–XV.\n L.  Winner, The  Whale  and  the  Reactor:  A  Search  for\nLimits in an Age of High Technology. Chicago, IL, USA:\nUniversity of Chicago Press, 1986.\n[157]\n T. J. Pinch and W. E. Bijker, The social construction of\nfacts and artifacts: Or how the sociology of science and\nthe sociology of technology might benefit each other, in\nThe Social Construction of Technological Systems, W. E.\nBijker, T. P. Hughes, and T. Pinch, eds. Cambridge, MA,\nUSA: MIT Press, 1987, pp. 17–50.\n[158]\n E. Morozov, To Save Everything, Click Here: The Folly\nof Technological Solutionism. PublicAffairs, New York,\nNY, USA: PublicAffairs, 2014.\n[159]\n B. Green, The Smart Enough City: Putting Technology in\nIts Place to Reclaim Our Urban Future. Cambridge, MA,\nUSA: MIT Press, 2019.\n[160]\n L.  Irani, Chasing  Innovation:  Making  Entrepreneurial\nCitizens in Modern India. Princeton, NJ, USA: Princeton\nUniversity Press, 2019.\n[161]\n M.  G.  Ames, The  Charisma  Machine:  The  Life,  Death,\nand Legacy of One Laptop per Child. Cambridge, MA,\nUSA: MIT Press, 2019.\n[162]\n D.  Greene, The  Promise  of  Access:  Technology,\nInequality,  and  the  Political  Economy  of  Hope.\nCambridge, MA, USA: MIT Press, 2021.\n[163]\n A. L. Hoffmann, Where fairness fails: Data, algorithms,\nand\n the\n limits\n of\n antidiscrimination\n discourse,\nInformation, Communication & Society,  vol. 22,  no. 7,\npp. 900–915, 2019.\n[164]\n B.  Green,  Escaping  the  impossibility  of  fairness:  From\nformal to substantive algorithmic fairness, arXiv preprint\narXiv: 2107.04642, 2021.\n[165]\nBen Green is a postdoctoral scholar in the\nSociety  of  Fellows  and  an  assistant\nprofessor in the Gerald R. Ford School of\nPublic Policy, University of Michigan. He\nreceived  the  PhD  degree  in  applied  math\n(with  a  secondary  field  in  STS)  from\nHarvard  University  and  the  BS  degree  in\nmathematics & physics from Yale College\nin 2020 and 2014, respectively.\n  Ben Green:   The Contestation of Tech Ethics: A Sociotechnical Approach to Technology Ethics in Practice\n225    \n \n\n \nFraming and Language of Ethics: Technology,\nPersuasion, and Cultural Context\nJasmine E. McNealy*\nAbstract:    What are the consequences of the language we use for technology, and, how we describe the\nframeworks  regarding  technology  and  its  creation,  use,  and  deployment?  The  language  used  to  describe\ntechnology has the possibility to deceive and be abusive. How language is used demonstrates what can occur\nwhen one party is able to assert linguistic power over another. The way in which organizations frame their\nrelationships with technology is one such power asymmetry. This article examines the complications of the\nimagery used for ethics in technology. Then, the author offers a brief overview of how language influences our\nperceptions. The frames used to describe phenomena, including ethical frameworks and technology, allow for\nthe creation of heuristics, or shortcuts that are “good enough” for understanding what is being described and\nfor decision-making. Therefore, descriptions matter for relaying meaning and constructing narratives related\nto ethical uses of technical systems. After this, the author investigates what we mean by ethics and the codes\nthat corporate, governmental, and other organizations use to depict how they understand their relationship to\nthe technology they create and deploy. The author explores three examples of frames of ethics and descriptions\nof technology, which though appearing progressive, once understood holistically, fail to adequately describe\ntechnology and its possible impact. The author ends this article with a discussion of the complexity of describing\nand communicating ethical uses of technology.\nKey  words:   language; framing; ethics; technology; culture\n“…metaphors  give  rise  to  technical  models,  which\ninform  design  processes,  which  in  turn  shape\nknowledges and politics…”\n– Shannon Mattern[1]\n1    Introduction\nIn her piece, “The City is not a Computer”, is noted by\nscholar  of  media  and  spaces  Shannon  Mattern[1].\nShe  described  the  error  in  analogizing  a  city  to  a\ncomputational device. Theorists and technologists used\nthis analogy in discussions of emerging smart cities and\nthe implementation of civic technology. Mattern argued\nthat  the  representation  of  a  city  as  a  computer  was\ninaccurate;  cities  were  neither  programmable  nor\nrational. The description of a city as a computer was also\ninadequate — life in cities did not always follow specific\naims  and  plans,  and  urban  environments  were  not\nsimply “apparati  for  record-keeping  and  information\nmanagement”[1].  More  happens  in  cities  than  simple\ninformation processing and storage. And the image does\nmore than fail to correctly describe the environment, it\nsets the stage for the implementation of policy and the\nreification of structural issues that can be harmful. It also\nignores  the  complexity  of  describing  urban  society.\nViewing  data  as  the  fuel  upon  which  a  city  runs\ndisregards\n other\n kinds\n of\n information\n flowing\nthroughout a city that cannot be reduced to 1 s and 0 s;\nit overlooks the voices of people and history.\nThe  repeated  use  of  certain  language  to  describe\n \n • Jasmine  E.  McNealy  is with  the  College  of  Journalism  and\nCommunications, University of Florida, Gainesville, FL 32601,\nUSA. E-mail: jmcnealy@ufl.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-20;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   02/06  pp226−237\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0027\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nsomething informs how we construct, or imagine, that\nthing. Therefore, if we view urban spaces as simply parts\nof an information processor, we may ignore or miss the\nvital social, political, and cultural activities that need to\nbe  considered  in  designing  things  like  parks,  social\nservices, and policy.\nThe same can be said for the kind of language and\ndescriptions used in talking about ethics in technology.\nConsiderations of what it means for technology, and the\nfirms  that  create  and  implement  it,  to  be  ethical  are\ncritically important in a world in which the fruit of failing\nto consider the implications of technology is manifesting\nin  amplified  ways.  This  includes  how  we  discuss\ntechnology in general, as how we describe something\ninfluences how we understand its purpose and what it\ncan do.\nTake, for instance, the controversy over DeepNudes,\nan app that uses machine learning to make photos appear\nas though the women in them were naked. The app is an\nexample of the deepfakes phenomenon, in which images,\nvideos, and sound can be altered by using cheap artificial\nintelligence  to  produce  products  that  look  and  sound\nreal[2]. The creators of DeepNudes, who pulled down the\nservice  after  receiving  widespread  negative  attention,\nclaimed  they  created  it  for  entertainment  purposes[3].\nThis framing of such a potent technology as for mere\namusement, like framing a city as a computer, ignores\nthe ramifications of creating a system that targets women\nfor abuse. What’s more, the “entertainment” designation\nmakes it appear as though legitimate uses of the app exist.\nWhat,  then,  are  the  consequences  of  the  language\nwe  use  for  technology,  and,  how  we  describe  the\nframeworks  regarding  technology  and  its  creation,\nuse,  and  deployment?  This  article  examines  the\ncomplications  of  the  imagery  used  for  ethics  in\ntechnology.  In  Section  2,  the  author  offers  a  brief\noverview of how language influences our perceptions.\nThe  frames  used  to  describe  phenomena,  including\nethical  frameworks  and  technology,  allow  for  the\ncreation  of  heuristics,  or  shortcuts,  that  are “good\nenough” for understanding what is being described and\nfor decision-making[4, 5]. Therefore, descriptions matter\nfor relaying meaning and constructing narratives related\nto ethical uses of technical systems. This section, though\nacknowledging  that  framing  theory  has  been  used  in\nseveral  different  fields,  focuses  on  framing  from  the\nperspective of mass communication scholars. In Section 3,\nthe author investigates what we mean by ethics and the\ncodes\n that\n corporate,\n governmental,\n and\n other\norganizations use to depict how they understand their\nrelationship to the technology they create and deploy.\nSection 4 explores three examples of frames of ethics\nand descriptions of technology, which though appearing\nprogressive,  once  understood  holistically,  fail  to\nadequately describe technology and its possible impact.\nThe  author  ends  this  article  with  a  discussion  of  the\ncomplexity  of  describing  and  communicating  ethical\nuses of technology in Section 5.\n2    Constructing Descriptions\nIn June 2019, Google announced the construction of its\nthird  subsea  cable  stretching  from  Portugal  to  South\nAfrica[6, 7]. The cable would connect Africa to Europe and\nincrease Google’s cloud infrastructure. The significance\nof a new, private, and international telecommunications\ncable project from the mega-corporation was appreciable\nalone.  But  what  garnered  noteworthy  attention  was\nGoogle’s choice in name for the project, “Equiano”. This\nname, chosen by the large global tech company, may\nhave  appeared  benign  to  some,  but  others  noted  the\nhistorical context.\nEquiano was the surname of Olaudah Equiano, also\nknown as Gustavas Vassa, a formerly enslaved man who\nbecame a writer and abolitionist in the late 18th century.\nAccording to his own memoir, Equiano was kidnapped\nas a child from his home in what is now Nigeria and sold\ninto slavery[8]. As an adult Equiano was able to purchase\nhis  freedom  and  then  moved  to  London  where  he\nwas  instrumental  in  founding  the  Sons  of  Africa,  an\nabolitionist  group  composed  of  formerly  enslaved\nAfricans. He spent the rest of his life advocating for poor\nBlack people in London. Therefore, when Google chose\nto use Equiano as the name for its underwater cable, it\nevoked  connections  to  colonialism,  imperialism,  and\nhuman subjugation.\nEquiano, like all words, has meaning. How we define\nwords  is  linked  to  social  and  cultural  context[9].  And\nmeanings  change,  and  words  may  have  multiple\nmeanings  that  are  created  and  adapted  for  specific\nsituations. Meanings are situated, or assembled out of\nmany different features, while at the same time, being\ncreated from what linguistics and literacy scholar James\nPaul Gee calls cultural models[9]. Cultural models, also\ncalled explanatory theories, help to clarify patterns that\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n227    \n \n\nemerge in sense-making, or interpreting everyday life[10].\nThese theories are based in sociocultural practices that\ninclude beliefs about the meanings. What, then, is the\nsociocultural context of Equiano and how is meaning\nassembled for this word in relation to the construction\nof an underwater cable connecting Europe to Africa?\nWithin  context,  it  is  possible  to  view  Google’s\nname  choice  as  a  mirror  image  of  the  extractive\nhistorical\n relationship\n that\n Europe,\n representing\nWestern  or  Global  North  countries,  have  had  with\nAfrica and other Global South countries. Countries of the\nGlobal South offer raw material and resources — food,\nminerals,  oil,  and  people — of  use  for  firms  and\ngovernments  in  the  North,  and  these  resources  have\nhistorically  been  exploited.  In  a  more  modern  sense,\nthe data from Internet users who will be connected to this\ncable  are  the  raw  material  awaiting exploitation —\nGoogle  is  not  the  only  organization creating  data\ninfrastructure  in  an  attempt  to  connect  with  Africa;\nFacebook, too, has plans for an underwater cable to be\nnamed “Simba”[11] and  Microsoft  and  Amazon  are\nopening data centers on the continent[12]. The data flows\nembody all five dimensions of anthropologist of media\nand communication Arjun Appadurai’s global cultural\nflows — ethnoscapes,  mediascapes,  technoscapes,\nfinancescapes,  and  ideoscapes — which  explain  the\ncomplexity of the global economy as being the result\nof “fundamental  disjunctures  between  economy,\nculture, and politics”[13].\nGoogle’s choice in name, of course, could be viewed\nas an ode to Equiano; the company has another private\nsubsea cable off the coast of Brazil named “Curie” and\na third that runs from Virginia Beach in the United States\nto France called “Dunant”. All three cables would be\nregarded as being named in recognition of an individual\nof significant achievement. But this view would be based\nin cultural context that ignores the history of extraction\nand exploitation. A cable named after Marie or Pierre\nCurie,  both  French  scientists,  off  the  coast  of  South\nAmerica,  a  continent  to  which  they  may  only  have\ntraveled is not the same as using the name of a man for\ncable laid off the coast from which he was stolen.\nHow people interpret the use of the name, or any word\nor idea, is based on the frames used. Culture — along\nwith the communicator, the text, and the receiver — is a\nlocation of frames in the process of communication[14, 15].\nFrames  are  created  when  a  communicator  chooses  to\nmake  certain  aspects  of  an  idea  more  salient  than\nothers[14, 15]. According to mass communication scholar\nVreese[13],  framing  involves  a  source  of  information\npresenting  and  defining  an  issue.  Frames  may  also\nidentify the origin of an issue, evaluate the causes and\npossible impacts, and offer remedies[15, 16]. Frames are,\nby  definition,  selective  descriptions — highlighting\ncertain  information  and  downplaying  or  omitting\nothers[17, 18]. Frames offer a construction of reality to the\naudience and, therefore, exert power[15]. Put another way,\nframes communicated to an audience can affect how that\naudience understands the subject[19, 20].\nFraming  theory  has  a  long  history  and  has  been\nexamined  in  related  several  different  academic  fields,\nthe  author  focuses  on  framing  theory  as  it  has  been\nexplored  in  mass  communication  or  media  studies\nresearch.  Mass  communication  is  relevant  as  it  is  the\ncommunications  of  a  message  to  a  large  audience[21].\nScholarship  has  identified  two  kinds  of  frames:\nequivalency frames and emphasis frames[22−24]. While\nequivalency  frames  tend  to  be  associated  most  with\nmedia  effect  because  they “involve  manipulating  the\npresentation  of  logically  equivalent  information”,\nemphasis frames involve the manipulation of content[22].\nAccording to Cacciatore et al.[22], emphasis framing is\nsociologically oriented — it focuses on the messages the\naudience  receives,  and  emphasizes  one  set  of\nconsiderations over another.\nIt  is  appropriate,  then,  to  reexamine  Google’s  own\nannouncement of the cable Equiano, then, with emphasis\nframing analysis in mind. The blog post mentions that\nthe cable is named for the abolitionist and notes that he\nhad been enslaved as a child. The announcement further\nreferences that all three of its subsea cables have been\nnamed for “historical luminaries”[6]. Yet, the blog post\nomits the history European exploitation and extraction.\nIt  offers  no  further  explanation  of  slavery,  Africa,  or\nEquiano’s life. What is made salient — highlighted as\nmost  important — is  that  the  company  was  creating\nprivate infrastructure connecting Europe to Africa, and\nthat  the  cable  is  named  after  an  important  historical\nperson. The power in this construction of reality is that\nthe  use  of  the  name  is  promoted  as  a  tribute,  while\nthe\n historical\n context—connection,\n infrastructure,\ncolonialism, and subjugation — is omitted or ignored.\nGoogle may have done a great thing in honoring Equiano,\nbut the company also fails to note the larger picture, and\n    228\nJournal of Social Computing, September 2021, 2(3): 226−237    \n \n\nthe reality constructed for the audience is one that omits\nimportant background issues. In doing this, the mega-\ncorporation creates an assemblage of frames that center\nthe honorific while ignoring the extraction.\nLikewise,  how  we  communicate  about  ethics  and\ntechnology may omit or ignore important sociocultural\nand historical contexts, offering a construction of reality\nthat is both inaccurate and inadequate for assessing our\nrelationship  to  technology.  In  Section  3,  the  author\nconsiders  what  it  is  we  mean  by  ethics,  and  how\norganizations and individuals are framing the discussion\nof ethics and technology.\n3    Communicating Ethics\nDiscussions about the ethics of the creation and use of\ncertain  technology  have  proliferated[25].  Yet,  we  are\noften confronted with a lack of a definitive or unified\ndefinition. Studies of business ethics have found several\ndifferent descriptions of ethics in the textbooks used to\nteach business majors at universities[26], and ethics are\nat  times  conflated  with  social  responsibility[27].  It  is\nimportant, then, to define what it is we mean when we\nsay ethics.\nEthics have been defined as a system for determining\nwhat  is  right  and  proscribing  what  is  wrong[28].  This\nsystem is based in rationality and must be applied in a\nconsistent manner to be valuable. Further, the decision\nof whether an act is right or wrong is situational, based\non  the  context  of  the  action  or  policy.  But  even  this\ndefinition  does  not  provide  us  with  a  universal\ndescription of the kinds of behaviors or actions that will\nbe  considered  ethical.  This  may  be  because  there  are\nmany different ethical systems, some of which conflict.\nDeontological  ethics,  for  instance,  frame  certain\nstandards of conduct as being right based on normative\nideas of duty and morality[29]. It is a view that no matter\nhow good the effects of a choice are, some choices are\nwrong based on specific values. Therefore, if a choice is\nconsidered wrong, an individual should not make it, even\nif  the  outcome  of  that  choice  is  positive.  As  an\nillustration, imagine a society in which the normative\nbelief system says that the destruction of forestland, for\nany  reason,  is  morally  wrong.  Say  then,  a  particular\nforest is between one town and another larger city where\na large trauma medicine center is located. Ambulances\nand  others  must  navigate  around  the  forest,  spending\nmore time than if a road were to be cut directly through\nthe  trees.  Under  a  system  of  deontological  ethics,\nalthough a consequence of destroying part of nature in\nthis case would mean achieving the good of being able\nto reach emergency and other health services faster, the\nact  of  clearing  the  land  would  still  be  considered\nwrongful.\nContrast  deontological  ideas  of  ethics  with  a\nconsequential\n ethical\n system\n like\n utilitarianism.\nConsequential ethics, as it sounds, looks at the effects of\nan action or policy[30]. In a consequential ethics system\nlike act-based utilitarianism, the ethics of an action are\njudged  based  on  the  consequences  as  they  relate  to\noutcomes  like  happiness  or  welfare.  In  rule-based\nutilitarian systems, rules are only created if they result in\noverwhelming benefits[31]. In the forest scenario above,\nunder a consequentialist ethical system, we would first\nexamine the effects of clearing a path in the forest for a\nroad, a significant one of which would be the decrease in\ntime for emergency health services, among other things,\nbetween  the  two  towns.  These  outcomes  would  be\nconsidered a benefit to society, most-likely outweighing\nthe costs of losing some of the forested area.\nSo far only two kinds of ethical systems have been\nidentified, both of which come from a Western-centric\nphilosophy. Many other systems of ethics and morality\nexist  across  the  globe[32].  The  various  indigenous\nsystems, for example, have dramatically different ideas\nabout what should be considered right or wrong and how\nsociety should deal with non-conformance. How, then,\ncan we know what we mean when we talk about ethics,\nparticularly  ethics  as  applied  to  technology  and  the\norganizations  that  create  technology?  According  to\nEllwood[33], how a society functions — the purview of\nthe social sciences — furnishes the “raw material” for\nthe  creation  and  study  of  ethics.  Social  sciences\ninvestigate  humans  and  their  relationships.  Therefore,\nthe  author  defines  ethics  here  as  describing  the\nrelationship that individuals and organizations have with\nthe “thing” at issue, in this case technology, in making\ndeterminations about right and wrong.\nThe  explicit  language  of  an  organization’s  code  of\nethics  or  how  it  talks  about  its  relationship  to  its\ntechnology, in theory, tells us about how an organization\nperceives certain products and behaviors. Codes of ethics\nand  ethical  statements  are  public-facing  expressions\nof organizational standards[34]. Codes express to others\nhow an organization views its product or service within\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n229    \n \n\nthe context of societal ideas of what is right or wrong.\nAnd these codes can also be a way for organizations to\nappear ethical to the public[35]. Codes, though expressing\nstatements of how an organization perceives behavior,\nare not self-enforcing, nor do all codes even consider or\nexpress the consequences of noncompliance. According\nto Wood and Rimmer[35], a code by itself is only a façade\nfor  an  organization  to  behave  ethically  and  could  be\nconsidered deceptive.\nBut  codes  are  the  steps  that  tech  companies  and\norganizations  employing  technology  are  being  used\nto  express  their  understanding  of  the  relationship\nbetween  their  use  or  creation  of  a  product  or  service\nand  the  greater  society.  Three  kinds  of  ethics  codes\nexist:  regulatory,  aspirational,  and  educational[36, 37].\nRegulatory codes use language that express imperatives.\nBehaviors  and  activities  are  expressly  prescribed\naccording to the specified rules of the organization. An\norganization  enforces  these  codes  through  monitoring\nand bad actors can be sanctioned for failure to comply.\nAn example of a regulatory code of ethics is that like the\nprofessional responsibility rules that lawyers admitted to\na bar in the United States must follow. Failure to follow\nthe  rules  of  professional  responsibility  could  mean\nsuspension from practicing law, and sometimes, loss of\nlaw license.\nAspirational\n codes\n are\n those\n expressing\n an\norganization’s ideals. These are statements of levels to\nwhich an organization would like to reach, but behaviors\nare not mandated. Therefore, an aspirational code for a\ntech-related organization could contain a clause stating\nthat the company will “strive to recognize the humanity\nof all people”. This certainly reads like an important way\nfor  an  organization  to  behave.  Language  centering\nhumanity would persuade some that the organization is\ndoing something with respect to how it will treat people\nin relation to its use or creation of technology. But the\nstatement  does  not  offer  an  articulation  of  what\n“recognize the humanity of all people” means, nor does\nit include a description or inference as to the kind of\nactions that do not meet this standard. It certainly does\nnot provide any indication of what will happen when it\nfails to meet this code, if that ever could happen under\nsuch a vague standard.\nLastly,  educational  codes  are  those  that  may  offer\nproscriptions,  but  also  provide  commentary,  with  the\ngoal  of  offering  the  reader  an  understanding  of  its\ninterpretation  of  the  language  used.  The  rules  of\nprofessional  responsibility  mentioned  above,  those  to\nwhich  attorneys  in  the  US  must  adhere,  are  often\nannotated  to  provide  commentary  and  example\nscenarios by which readers can judge the ethics of their\nactions.\nMany  organizational,  professional,  and  societal\nethical  codes  for  tech  organizations  are  aspirational.\nOrganizations  may  provide  vague  statements  and  are\nallowed  to  self-police.  According  to  Stark  and\nHoffmann[38], these codes “elide granular attention” to\nactual actions. And these ethical codes are also represented\nin  the  pronouncements  organizations  make  about\nthemselves and their products. Google, for instance, for\na long time used the assertion “Do not be evil”. Certainly,\nstaying away from building and using technology for bad\nacts  was  a  laudable  goal,  but  the  mega-corporation\noffered no definition of what it considered evil, nor did\nit provide a way for the public to hold it accountable for\nfailing  to  meet  this  standard.  Significantly,  Google\nremoved “Do not be evil” from its code of conduct in\n2018[39, 40].  Section  4  considers  some  of  the  other\naspirational language used not only by organizations, but\nby others in considering ethics in technology.\n4    Framing Ethics and Technology\nHow  an  organization  or  individual  describes  its\nrelationship, its ethics, to technology has implications\nfor  how  that  technology — its  creation,  use,  and\ndeployment — is  understood  by  society.  Several\ndifferent ways of expressing this relationship exist. This\nsection explores three common frames of emphasis for\ndescribing  this  relationship  to  technology  and  the\nimplications  of  how  these  frames  construct  reality\nsurrounding  technology.  To  do  this,  the  author\ninvestigates how organizations communicate messages\nabout technology to the public and by examining how\nthey define and identify important ideas and issues and\nby  evaluating  which  descriptions  are  made  salient\nor  omitted.  These  frames  are  neutrality,  property,\nand user-centeredness.\n4.1    Neutrality\nAn enduring frame of technology is that a tool, system,\nor structure is neutral, or not programmed with biases or\nvalues.  A  popular  topic  in  philosophy  of  science  and\ntechnology studies, this framing of the relationship with\n    230\nJournal of Social Computing, September 2021, 2(3): 226−237    \n \n\ntechnology,  value  neutrality  in  technology,  can  be\ninterpreted in three ways: that the technology is neutral\nbecause  it  has  many  different  purposes,  that  the\ntechnology  is  neutral  before  it  is  used,  and  that  the\ntechnology  is  only  an  application  of “a  scientific,\nmathematized, and value-free view of nature”[41]. The\nfirst and second value-neutrality interpretations align as\nthey focus on the use and purposes of technology and\nlead to the principal question of whether a “thing” can\nbe created without having any inherent values or biases.\nThough  this  interpretation  of  value-neutrality  is  still\npopular,  the  evidence,  both  empirical  and  otherwise,\ndemonstrates that the answer is “no”. Recent scholarship\nby Safiya Umoja Noble[42] and Virginia Eubanks[43] has\nillustrated the danger in believing in, and relying on, the\nneutrality  of  algorithms  used  for  search  and  to\nimplement  civic  policy.  Technology,  broadly  defined,\nhas been shown to be endowed with the “politics”, or the\nviewpoints, of its creator in both its use and the impact[44].\nThe  third  interpretation  of  value-neutrality  in\ntechnology, too, is popular in that it denies the existence\nof politics in technology by pointing to laws of nature,\nscience,  and  math.  This  imagining  of  value-neutrality\nconstructs  technology  as  reflecting  only  what  occurs\nnaturally and outside of human or social control. This\nreflects  technological  determinism,  a  theory  that\ntechnology evolves by itself and has the power to shape\nsociety[42, 43].  Of  course,  the  dispute  between  social\nconstructivists — who  believe  that  humans  shape\ntechnology — and  determinists  has  been  ongoing[45].\nBut  to  ignore  that  our  understanding  of  science\nand technology, itself, is grounded in societal context\nis  to  ignore  that  what  we  think  of  as  scientific\nand  technological  laws  are  based  on  interpretation\nby humans.\nFacial  recognition  technology  (FRT)  provides  an\nemergent  technology  for  the  exploration  of  frames  of\nvalue-neutrality. FRT, which scans the human face for\nsupposedly unique characteristics to create a map akin to\na fingerprint, has been framed as a neutral technology by\nsome technologists[46, 47]. FRT systems have been touted\nas systems for good in law enforcement, anti-terrorism,\nand finding missing persons[48]. Amazon has created its\nown FRT, Rekognition, which it frames as a system that\nallows the user to “detect, analyze, and compare faces for\na wide variety of user verification, people counting, and\npublic safety use cases”[49]. In promoting its system, the\ncompany focuses on the capability of the product, calling\nit fast and accurate. Beyond simply facial recognition,\nAmazon promotes Rekognition as useful for six other\nservices, including facial analysis, celebrity recognition,\nand  unsafe  content  detection.  Rekognition  is  also\ndescribed as offering benefits to the user like low costs,\nand real-time analysis. Lastly, Amazon employs images\nas part of its description of the technology. These photos\nare used to demonstrate how the system works.\nAs with Google’s use of Equiano for its subsea cable,\nAmazon’s framing of its FRT fails to provide context for\nits technology. In fact, the description provided for the\nRekognition makes it appear as though the technology is\nneutral when evidence has proven that FRT is anything\nbut.  Research  published  by  computer  scientists  Joy\nBuolamwini and Timnit Gebru found that because FRT\nis  trained  on  biased  datasets,  in  this  case  datasets  in\nwhich the majority of faces in the training data were of\nlighter  skin,  the  systems  could  not  provide  accurate\nresults,  especially  for  women  with  darker  skin[50].\nInaccurate  results  from  FRT  disparately  impact\nindividuals  from  already  marginalized  communities,\nparticularly Black people[51].\nIn  describing  its  relationship,  then,  with  this\ntechnology, Amazon omits the significant consequences\nof the uses of FRT. Of course, negative impacts are not\noften  selling-points  for  any  product.  But  in  failing  to\nprovide any explanation of how the system is trained,\nand the ramifications of that training, the organization\npresents FRT as though the technology were devoid of\nany inherent values.\n4.2    Property\nAnother common emphasis frame used in discussing our\nrelationship  with  technology  is  that  of  property.\nProperty-centered  language  is  used  to  describe  our\ndata and the rights we may assert over another individual\nor  organization  attempting  to  access,  use,  or  control\npersonal  information.  Examples  of  property-based\nlanguage can be found in the discussion of privacy, and\nlegal  scholars  have  found  that “the  right  of  privacy\noriginates in property-based ideas, whereas one of the\nfunctions of property law is to protect private interests”[52].\nBut property is a creation of society. This allows the\nownership  of  a “thing”,  be  it  land  or  intangible\ninformation[53].  Therefore  property,  and  the  rights\nassociated  with  the  ownership  of  property,  can  be\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n231    \n \n\nrestrained. What is often used in relation to property is\nthe  metaphor  of  a “bundle  or  rights”,  signifying  the\nconstraints on the property owner[52, 53]. The bundle of\nrights has been interpreted as conceptualizing property\nas  the  relationships  between  people  and  not  between\npeople and a thing[54, 55]. Within this bundle is the ability\nto possess, use, and sell the property. But these rights can\nbe overcome for reasons of public policy. For example,\nalthough we technically own our own bodies, it is illegal\nin the US to sell your organs.\nBut property ownership has never, historically, been\na  right  allowed  to  all  people,  nor  has  everyone  been\nallowed  to  have  the “sticks” in  the  bundle  of  rights\nrespected. In the US, for instance, property rights have\nbeen  found  to  be “rooted  in  racial  domination”[56].\nNative  Americans  had  their  rights  to  land  stripped\nby US Supreme Court decision[57]; African Americans\nwere  deemed  property  and  denied  rights  related  to\nself-fulfillment[58]. Other groups, too, have been denied\nrights in property or possession in both land and self.\nAnd the language of property conflicts with that of\nhumanity.  To  look  upon  a  human  as  property  allows\nindividuals  and  organizations  to  behave  toward  that\nperson in ways that would not be sanctioned in relation\nto others. The same can be said for property language in\nconnection to personal data. Discussions of data, broadly\ndefined,  use  property  language  and  the  rhetoric  of\nownership,  control,  and  access  in  relation  to  personal\ndata, thereby creating a definition of data divorced from\nthe  individual  and  ignoring  the  very  real  harms  of\npersonal  data  aggregation.  When  it  is  just  data  being\ncollected,  the  consequences  of  that  collection  can  be\nignored; when those data are more closely attached to\nhumans, the possible harms become more tangible.\nDetroit’s  Project  Greenlight  provides  a  case  for\nexploration.  The  City’s  own  website  describes  the\nproject  as  partnership  between  the  Detroit  Police\nDepartment  and  businesses  with  the  aim  of  fighting\ncrime[59]. Businesses and other organizations involved\nin the partnership must install surveillance cameras and\nhigh-speed  internet.  The  videos,  the  data,  from  the\ncameras are streamed to the DPD for analysis. Instead\nof using officers to watch the stream, the DPD employs\n“civilian  crime  analysts” tasked  with  identifying  and\nfinding crime suspects[60]. The DPD page fails to provide\nany mention that the department uses FRT to analyze the\nvideo[61].\nViewing  the  people  who  may  appear  in  the  video\nstreams  as  points  of  data  used  to  solve  crime  has\nramifications. Although city officials have denied it, it is\npossible that FRT may be used to identify people who are\nnot  necessarily  suspected  of  any  crime[62, 63].  Video\nsurveillance provides more than just points of data; it can\noffer a construction of the life of a private individual who\nis not breaking any law. Points of data, then, are more\nthan simple places for analysis; these observations are\nwhat  makes  individuals  unique  and  identifiable[64].\nTherefore,  the  DPD  framing  of  its  Project  Greenlight\nomits  the  possible  consequences  of  the  technology  to\npersonal privacy.\n4.3    User-centeredness\nLastly, a common emphasis frame used by organizations\nexplaining  their  relationship  to  technology  is  that  of\nbeing “user-centered”.  The  user-centered  approach  to\ndesign  calls  for  involving  the  user  in  the  process  of\ndesign[65]. The process focuses on ensuring usability by\nunderstanding how users may interact with a product or\nservice[66]. Users participate with designers throughout\nthe development of the item and the system evolves in\niterations.  Throughout  the  development,  designers  are\nsupposed  to  make “explicit  and  conscious  design”\nchoices[66].\nFrom  the  outset,  there  are  various  reasons  why  the\nframing of product design as “user-centered” is fraught.\nTo make their process truly user-centered, the designers\nand researcher would have to be able to consider all the\npossible kinds of users, their abilities, and the social and\ncultural  systems  in  which  they  will  interact  with\na  product.  User-centered  language,  as  currently\nimplemented, may ignore that there are many kinds of\nusers, each with their own needs, wants, and desires. It\nmay  further  ignore  that  different  users  have  different\nabilities  and  experiences  that  are  important  for\nconsideration  in  design.  For  the  most  part,  creators\ndesign for what is considered the default — white and\nmale[67, 68] — thereby,  ignoring  the  diverse  sets  of\nindividuals that may use, come in contact, or be impacted\nby the products they create[69].\nUser-centered framing also ignores that a product or\nservice may impact non-users. For the most part, then,\nuser-centered design is a customer-centered approach[70].\nUser  experience  researchers  create  personas  that\n    232\nJournal of Social Computing, September 2021, 2(3): 226−237    \n \n\nimagine how someone adopting a product may use it and\nwhat their needs may be. These are usually constructed\nout  of  the  imaginary  of  the  prototypical  product\nuser[71]. But there may be several reasons that someone\nmay not adopt technology, including lack of access and\ncomplete rejection.\nThe  City  of  Boston’s  neighborhood  resources  site\nprovides  an  example  of  a  technology  for  analysis.  In\nApril 2019, the City announced that it was working to\nport over its old “My Neighborhood Resource” tool to its\nnew  Boston.gov  website[72].  The  My  Neighborhood\npage provides access to information on properties within\nthe City including city services and resources, landmarks,\ninformation  on  voting,  and  political  representation.\nIn  describing  the  revisions  to  the  site,  the  City’s\nannouncement  states  that  the  team  tasked  with  the\nmakeover “wanted to make sure they were creating a\nuser focused application”, and that it wanted “to make\nsure we always keep our users’ needs in focus”[72]. The\nannouncement  goes  on  to  describe  the  various  tests\nconducted  in  revising  the  app  and  provides  interview\ndata from some of the users.\nThis framing of a user-centered civic technology, on\nthe  surface,  appears  to  be  a  great  approach  for  a\nmunicipality attempting to provide the services that its\nconstituents need. Certainly, a city providing ways for its\nresidents to access information and services efficiently\nis laudable and conducting research to ensure that it was\nbest  serving  the  people  who  may  use  the  technology\nshould also be commended. This framing of the tool as\nuser-friendly, however, ignores the residents and others\nwho may not have access to the information available on\nthe app.\nCivic technology aimed at connecting city residents to\ninformation and services has proliferated[73]. But not all\npossible users can take advantage of these systems. At\nthe very least using these resources requires a phone, in\nthe  case  of  311  numbers — which  allow  residents  to\nreport  issues  to  government  departments — or  an\ninternet connection for those who want to access online\nservices.  The  users  that  a  city  focuses  on,  then,  are\nthose who can afford these connections. Further, some\nresidents  may  consciously  choose  to  reject  a  civic\nservice for fear of surveillance. Therefore, although a\nuser-centered approach to civic technology, like Boston’s\nMy Neighborhood, appears to target all users, it ignores\nthe impediments to adoption of the technology.\n5    Reframing Relationship with Technology\nIn  her  1992  book, Talking  Power,  linguist  Robin\nTolmach  Lakoff  asserted, “Language  is  powerful;\nlanguage is power. Language is a change-creating force\nand therefore to be feared and used, if at all, with great\ncare, not unlike fire”[74]. Like Mattern, who argued that\nhow we imagine a city had the power to influence design\nand policy, Lakoff, too, focused on how language was\nused to seek power and was, therefore, always political.\nFor Lakoff, language was always used to persuade, but\nof particular interest was the possibility of deception and\nabuse that can occur when one party is able to assert\nlinguistic power over another.\nThe  ways  in  which  organizations  frame  their\nrelationships  with  technology  is  one  such  power\nasymmetry. Emphasis frames like neutrality, property,\nand user-centeredness offer surface-level interpretations\nof technology that appear benign. Sometimes, as in the\ncase of user-centered, the language used constructs an\nimaginary of a progressive way of thinking about how\nwe  should  create,  use,  and  implement  technology.\nOrganizations  prioritize  to  the  public  narratives  that\nappear  advanced  or  enlightened,  like  naming  a\ntelecommunication  cable  after  a  formerly  enslaved\nabolitionist. But these frames, and many others, fall short\nof  offering  express  and  complete  descriptions  and\nexplanations  of  the  technology  and  the  organization’s\nrelationship with it. They are reductive, oversimplified\nways  of  viewing  impactful  systems  and  relationships.\nAt  most,  these  frames  provide  aspirational  goals  for\norganizations to reach. More realistically, these frames\noffer  vague  and  inaccurate  views  of  the  possible\nimplications of the technology for which an organization\nis responsible. Like the story told about the choice of\nEquiano  as  a  name  for  the  subsea  cable,  these\nexplanations  hinder  progress  by  obstructing  true\nexaminations of power dynamics through framing.\nFrames can be composed of four possible elements: a\ndefinition,  an  identified  origin,  an  evaluation,  and\nremediation.  The  three  frames  explored — neutrality,\nproperty, and user-centered — were missing elements of\nthe\n frames\n that\n might\n produce\n alternative\nunderstandings,  thereby  demonstrating  the  creation  of\nsalience relevant to one aspect of a discussion. In the\nneutrality  frame,  both  the  evaluation  of  causes  and\npossible  impacts  and  the  remedies  were  lacking.  For\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n233    \n \n\nneutrality, omitted in the promotion of facial recognition\ntechnology was an explanation of the underlying bias in\nhow the technology is trained, as well as the possible\nimpacts  to  already  marginalized  populations.  For\nproperty, omitted was an evaluation of the consequences\nof not viewing data collected as representative of people.\nFinally, for user-centered, omitted was an evaluation of\nthe impediments to use and how this might affect who\nan organization includes when designing a technology.\nThe  influence  of  these  emphasis  frames  on  public\nunderstanding of the implications of various technology\nis  significant.  As  both  Lakoff  and  Mattern  assert,\nlanguage has the power to shape our perspectives[1, 74].\nThese frames, along with others used by organizations\nand individuals to explain relationships with technology,\nshape our interpretations of how or whether technology\nshould be designed and used. DeepNudes, the service\nthat used AI to allow users to alter photos to make it\nappear as though the women in them were naked, offers\nan  example  for  consideration.  In  their  attempt  at\nexplaining their relationship with the app, the creators\nframed  it  as  being  created  for  entertainment[75].  As  a\nframe,  the  label  entertainment  colored  how  people\ninitially understood the service. Entertainment, usually,\nsupposes  amusement  and  light-hearted  fun.  But  this\nframe ignores the consequences of virtually disrobing\nunsuspecting women without their consent.\nThe  example  technology  and  the  language  used  in\nconnection identified throughout this article offer some\ninferences.  Perhaps  the  primary  conclusion  is  that\ntalking is hard[76]. Language is constructed of words and\ndescriptions  situated  in  culture,  that  helps  us  form\nexplanations for ideas and phenomena. Consequently, it\nis important for organizations to understand the various\ncultural models that may arise with the descriptions of\ntechnology. Although Google’s Equiano announcement\ndid note that the abolitionist had been a slave, it did not\nconsider the context of extraction and imperialism that\ncould change the tenor of the statement for some.\nContext matters. The audiences for these statements\nwill have divergent views based on their understandings\nand  experience  with  both  the  organization  and  the\nsubject technology. How a Black Detroiter understands\nthe police department’s Project Greenlight may be very\ndifferent than how a white business owner interprets the\ninitiative.  They  arrive  at  their  understandings  of  the\nproject  based  in  their  experiences  and  histories.  It  is\nimportant,  then,  for  government,  corporate,  and  other\norganizations  to  reckon  with  the  context  of  the\ntechnology they create and deploy.\nNo simple solution exists ensuring that organizations\nuse  adequate  and  accurate  descriptions  for  their\nrelationships  with  technology.  This  is  not  to  say  that\nthere is one, definitive depiction or phrasing that would\ncover  the  entirety  of  the  history  and  implications  of\ntechnology.  But  current  portrayals  are  woefully\ninsufficient. And these representations are political and\npersuasive[77].  It  would  be  beneficial,  then,  for\norganizations to reconsider how they approach creation\nand  use  of  these  systems.  This  may  require  changing\ncodes of ethics from aspirational to more educational as\nwell as reexamining the frames used in public statements.\nMore importantly, it requires a rethinking of the power\nthat  certain  organizations  are  allowed  to  amass  with\nrespect to technology. This power is partially derived\nfrom how firms are able to persuade the public[78]. A way\nto  shift  power,  perhaps,  may  be  in  modifying  how\norganizations  promote  their  product  or  service  to  the\npublic. It will also take continued vigilance in ensuring\nthat counternarratives revealing the risks of technology\nare exposed to the public through advocacy. In the end,\nlanguage matters.\nAt the same time, expecting organizations, including\ngovernment  organizations,  to  agree  with  shifts  in\nburdens and power seems too simplistic of a resolution\nto  a  complex  issue.  Further,  under  this  idea  the  onus\nwould remain on individuals and communities to protect\nthemselves from manipulative message tactics used by\norganizations.  It  is  important,  then,  for  regulators  to\nbring reforms to how these messages are communicated\nto audiences.\nOrganizational  messages  are  commercial  messages\nmade to persuade individuals into believing ideas that\nbenefit the firm. In the United States regulations related\nto  the  kinds  of  claims  made  in  organizational  ethics\ncodes already exist, especially as these claims are public-\nfacing  and  material  to  whether  individuals  choose  to\nadopt  software  or  services.  When  companies  fail  to\nmeasure  up  to  their  claims,  or  an  individual  acting\nrationally could be deceived, regulators can step in to\npunish  organizations  for  these  deceptive  or  unfair\npractices.  The  author  leaves  the  specifics  to  future\nresearch,  but  regulators  have  the  power  to  prohibit\nand  punish  commercial  messages  that  are  otherwise\n    234\nJournal of Social Computing, September 2021, 2(3): 226−237    \n \n\ndeceptive.  This  could  be  necessary  step  in  having\norganizations  rethink  the  language  they  use  in\nconnection with ethics and impacts.\nAcknowledgment\nJ. E. McNealy would like to acknowledge the collective\nwisdom of the Ethical Tech Collective and those who\nparticipate in the Ethical Tech Working Group.\nReferences\n S. Mattern, A city is not a computer, Places Journal, doi:\n10.22269/170207.\n[1]\n B. Paris and J. Donovan, Deepfakes are troubling. But so\nare  the ‘cheapfakes’ that  are  already  here, https://slate.\ncom/technology/2019/06/drunk-pelosi-deepfakes-\ncheapfakes-artificial-intelligence-disinformation.html,\n2019.\n[2]\n S. Samuel, A guy made a deepfake app to turn photos of\nwomen  into  nudes.  It  didn’t  go  well, https://www.vox.\ncom/2019/6/27/18761639/ai-deepfake-deepnude-app-\nnude-women-porn, 2019.\n[3]\n F. Ferreira, K. G. D. Bailey, and V. Ferraro, Good-enough\nrepresentations  in  language  comprehension, Current\nDirection in Psychological Science, vol. 11, no. 1, pp. 11–\n15, 2002.\n[4]\n F.  Ferreira  and  N.  D.  Patson,  The ‘Good  Enough’\napproach  to  language  comprehension, Language  and\nLinguistics Compass, vol. 1, no. 1&2, pp. 71–83, 2007.\n[5]\n M.  D.  Francois,  C.  George,  and  J.  Stowell,  Introducing\nEquiano,  a  subsea  cable  from  Portugal  to  South  Africa,\nhttps://cloud.google.com/blog/products/infrastructure/intro\nducing-equiano-a-subsea-cable-from-portugal-to-south-\nafrica/, 2019.\n[6]\n S. Shankland, Google’s third subsea cable will pump data\nfrom  Portugal  to  South  Africa, https://www.cnet.com/\nnews/google-third-subsea-cable-equiano-connect-portugal-\nsouth-africa-nigeria/, 2019.\n[7]\n O. Equiano, The Life of Olaudah Equiano. New York, NY,\nUSA: Cosimo, Inc., 2009.\n[8]\n J. P. Gee, An Introduction to Discourse Analysis: Theory\nand Method. London, UK: Taylor & Francis, 1999.\n[9]\n E.  Goffman, Frame  Analysis:  An  Essay  on  the\nOrganization  of  Experience.  Cambridge,  MA,  USA:\nHarvard University Press, 1974.\n[10]\n Y. Kazeem, Google and Facebook are circling Africa with\nhuge undersea cables to get millions online, https://qz.com/\nafrica/1656262/google-facebook-building-undersea-\ninternet-cable-for-africa/, 2019.\n[11]\n P. Sawers, Google announces Equiano, a privately funded\nsubsea  cable  that  connects  Europe  with  Africa, https://\nventurebeat.com/2019/06/28/google-announces-equiano-a-\nprivately-funded-subsea-cable-that-connects-europe-with-\nafrica/, 2019.\n[12]\n A.  Appadurai,  Disjuncture  and  difference  in  the  global\ncultural  economy, Theory,  vol. 7,  no. 2,  pp. 295–310,\n1990.\n[13]\n C.  H.  de  Vreese,  News  framing:  Theory  and  typology,\n[14]\nInformation  Design  Journal,  vol. 13,  no. 1,  pp. 51–62,\n2005.\n R.  M.  Entman,  Framing:  Toward  clarification  of  a\nfractured  paradigm, Journal  of  Communication,  vol. 43,\nno. 4, pp. 51–58, 1993.\n[15]\n W.  A.  Gamson, Talking  Politics.  New  York,  NY,  USA:\nCambridge University Press, 1992.\n[16]\n A.  Tversky  and  D.  Kahneman,  Rational  choice  and  the\nframing  of  decisions,  in Multiple  Criteria  Decision\nMaking  and  Risk  Analysis  Using  Microcomputers,  B.\nKarpak  and  S.  Zionts,  eds.  Berlin,  Germany:  Springer,\n1989, pp. 81–126.\n[17]\n M.  Edelman,  Contestable  categories  and  public  opinion,\nPolitical  Communication,  vol. 10,  no. 3,  pp. 231–242,\n1993.\n[18]\n D.  A.  Scheufele,  Framing  as  a  theory  of  media  effects,\nJournal  of  Communication,  vol. 49,  no. 1,  pp. 103–122,\n1999.\n[19]\n D.  A.  Scheufele,  Agenda-setting,  priming,  and  framing\nrevisited:  Another  look  at  cognitive  effects  of  political\ncommunication, Mass Communicaiton and Society, vol. 3,\nno. 2&3, pp. 297–316, 2000.\n[20]\n P.  M.  Napoli,  Revisiting ‘mass  communication’ and  the\n‘work’ of  the  audience  in  the  new  media  environment,\nMedia, Culture & Society,  vol. 32,  no. 3,  pp. 505–516,\n2010.\n[21]\n M.  A.  Cacciatore,  D.  A.  Scheufele,  and  S.  Iyengar,  The\nend of framing as we know it … and the future of media\neffects, Mass Communication and Society, vol. 19, no. 1,\npp. 7–23, 2016.\n[22]\n D.  Chong  and  J.  N.  Druckman,  Framing  theory, Annual\nReview  of  Political  Science,  vol. 10,  no. 1,  pp. 103–126,\n2007.\n[23]\n D.  A.  Scheufele  and  D.  Tewksbury,  Framing,  agenda\nsetting, and priming: The evolution of three media effects\nmodels, Journal  of  Communication,  vol. 57,  no. 1,\npp. 9–20, 2007.\n[24]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  technology  ethics  in  practice, Journal  of\nSocial Computing, doi:10.23919/JSC.2021.0018.\n[25]\n P. V. Lewis, Defining ‘business ethics’: Like nailing jello\nto  a  wall, Journal  of  Business  Ethics,  vol. 4,  no. 5,\npp. 377–383, 1985.\n[26]\n J. Fischer, Social responsibility and ethics: Clarifying the\nconcepts, Journal  of  Business  Ethics,  vol. 52,  no. 4,\npp. 381–390, 2004.\n[27]\n D.  Berdichevsky  and  E.  Neuenschwander,  Toward  an\nethics  of  persuasive  technology, Communications  of  the\nACM, vol. 42, no. 5, pp. 51–58, 1999.\n[28]\n W.  J.  Waluchow, The  Dimensions  of  Ethics:  An\nIntroduction  to  Ethical  Theory.  Calgary,  Canada:\nBroadview Press, 2003.\n[29]\n J.  Driver, Consequentialism.  Florence,  KY,  USA:\nRoutledge, 2011.\n[30]\n K. de Lazari-Radek and P. Singer, Utilitarianism: A Very\nShort Introduction. Oxford, UK: Oxford University Press,\n2017.\n[31]\n S. Mhlambi, From rationality to relationality: Ubuntu as an\nethical  and  human  rights  framework  for  artificial\nintelligence  governance,  Carr  Center  for  Human  Rights\nPolicy  Discussion  Paper  Series,  https://carrcenter.hks.\n[32]\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n235    \n \n\nharvard.edu/publications/rationality-relationality-ubuntu-\nethical-and-human-rights-framework-artificial, 2020.\n C.  A.  Ellwood,  The  sociological  basis  of  ethics, Int.  J.\nEthics, vol. 20, no. 3, pp. 314–329, 1910.\n[33]\n J. Lichtenberg, What are codes of ethics for? in Codes of\nEthics and the Professions, M. Coady and S. Bloch, eds.\nMelbourne, Australia: Melbourne University Press, 1996,\npp. 13–27.\n[34]\n G. Wood and M. Rimmer, Codes of ethics: What are they\nreally and what should they be? International Journal of\nValue-Bsaed  Management,  vol. 16,  no. 2,  pp. 181–195,\n2003.\n[35]\n M.  S.  Frankel,  Professional  codes:  Why,  how,  and  with\nwhat impact? Journal of Business Ethics, vol. 8, no. 2&3,\npp. 109–115, 1989.\n[36]\n G.  Vinten,  Business  ethics:  Busybody  or  corporate\nconscience? Managerial  Auditing  Journal,  vol. 5,  no. 2,\npp. 4–11, 1990.\n[37]\n L.  Stark  and  A.  L.  Hoffmann,  Data  is  the  new  what?\nPopular metaphors & professional ethics in emerging data\nculture, Journal  of  Cultural  Analytics,  doi:  10.22148/16.\n036.\n[38]\n K. Conger, Google removes ‘Don’t Be Evil’ clause from\nits code of conduct, https://gizmodo.com/google-removes-\nnearly-all-mentions-of-dont-be-evil-from-1826153393,\n2018.\n[39]\n D. Mayer, Why Google was smart to drop its ‘Don’t Be\nEvil’ motto, https://www.fastcompany.com/3056389/why-\ngoogle-was-smart-to-drop-its-dont-be-evil-motto, 2016.\n[40]\n P.  Sundstrom,  Interpreting  the  notion  that  technology  is\nvalue-neutral, Medicine  Health  Care  and  Philosophy,\nvol. 1, no. 1, pp. 41–45, 1998.\n[41]\n S.  U.  Noble, Algorithms  of  Oppression:  How  Search\nEngines  Reinforce  Racism. New  York,  NY,  USA:  New\nYork University Press, 2018.\n[42]\n V. Eubanks, Automating Inequality: How High-Tech Tools\nProfile,  Police,  and  Punish  the  Poor. New  York,  NY,\nUSA: St. Martin’s Press, 2018.\n[43]\n L. Winner, Do artifacts have politics? Daedalus, vol. 109,\nno. 1, pp. 121–136, 1980.\n[44]\n M.  R.  Smith  and  L.  Marx, Does  Technology  Drive\nHistory?:  The  Dilemma  of  Technological  Determinism.\nCambridge, MA, USA: MIT Press, 1994.\n[45]\n K.  A.  Gates, Our  Biometric  Future:  Facial  Recognition\nTechnology  and  the  Culture  of  Surveillance. New  York,\nNY, USA: New York University Press, 2011.\n[46]\n J.  Woodward,  C.  Horn,  J.  Gatune,  and  A.  Thomas,\nBiometrics: A Look at Facial Recognition. Santa Monica,\nCA, USA: RAND Corporation, 2003.\n[47]\n Y.  Usigan,  7  surprising  ways  facial  recognition  is  used,\nhttps://www.cbsnews.com/pictures/7-surprising-ways-\nfacial-recognition-is-used/, 2011.\n[48]\n Amazon  Rekognition – Video  and  Image – AWS,\nhttps://aws.amazon.com/rekognition/, 2019.\n[49]\n J.\n Buolamwini\n and\n T.\n Gebru,\n Gender\n shades:\nIntersectional  accuracy  disparities  in  commercial  gender\nclassification,  in Proc.  the  1st  Conference  on  Fairness,\nAccountability and Transparency, New York, NY, USA,\n2018, pp. 77–91.\n[50]\n C. Garvie, A. Bedoya, and J. Frankle, The perpetual line-\nup:  Unregulated  police  face  recognition  in  America,\n[51]\nhttps://www.perpetuallineup.org/, 2016.\n M. B. Kent, Pavesich, property and privacy: The common\norigins  of  property  rights  and  privacy  rights  in  Georgia,\nJohn Marshall Law Journal, vol. 2, no. 1, 2009.\n[52]\n H. Demsetz, Toward a theory of property rights, in Classic\nPapers in Natural Resource Economics, C. Gopalakrishnan,\ned. London, UK: Palgrave Macmillan, 2000, pp. 163–177.\n[53]\n J.  B.  Baron,  Rescuing  the  bundle-of-rights  metaphor  in\nproperty  law, University  of  Cincinnati  Law  Review,\nvol. 82, no. 1, pp. 57–102, 2014.\n[54]\n D. R. Johnson, Reflections on the bundle of rights lecture,\nVermont Law Review, vol. 32, pp. 247–272, 2007.\n[55]\n C. I. Harris, Whiteness as property, Harvard Law Review,\nvol. 106, no. 8, pp. 1707–1791, 1993.\n[56]\n U.S.  Supreme  Court,  Johnson’s  Lessee  v.  McIntosh,\nUnited States Reports, vol. 21, pp. 543–605. 1823.\n[57]\n M.\n Armstrong,\n African\n Americans\n and\n property\nownership:  Creating  our  own  meanings,  redefining  our\nrelationships, African-American  Law  and  Policy  Report,\nvol. 1, pp. 79–88, 1994.\n[58]\n City  of  Detroit,  Project  Green  Light  Detroit, https://\ndetroitmi.gov/departments/police-department/project-\ngreen-light-detroit, 2016.\n[59]\n W.  Feuer,  Controversial  project  Green  Light  comes  to\nCorktown, https://www.metrotimes.com/news-hits/archives/\n2018/10/31/controversial-project-green-light-comes-to-\ncorktown, 2018.\n[60]\n C.  Garvie  and  L.  M.  Moy,  America  under  watch:  Face\nsurveillance in the United States, https://www.americaunder\nwatch.com, 2019.\n[61]\n S. Neavling, Researchers alarmed by Detroit’s pervasive,\nexpanding facial-recognition surveillance program, https://\nwww.metrotimes.com/news-hits/archives/2019/05/17/\nresearchers-alarmed-by-detroits-pervasive-expanding-\nfacial-recognition-surveillance-program, 2019.\n[62]\n S. Neavling, A condescending Chief Craig breaks silence\nabout Detroit’s facial-recognition technology, https://www.\nmetrotimes.com/news-hits/archives/2019/06/27/a-\ncondescending-chief-craig-breaks-silence-about-detroits-\nfacial-recognition-technology, 2019.\n[63]\n L. Sweeney, A. Abu, and J. Winn, Identifying participants\nin\n the\n personal\n Genome\n Project\n by\n name\n (A\nreidentification\n experiment),\n arXiv\n preprint\n aXiv:\n13047605, 2013.\n[64]\n J.  Karat,  Evolving  the  sope  of  uer-cntered  dsign,\nCommunication  of  the  ACM,  vol. 40,  no. 7,  pp. 33–38,\n1997.\n[65]\n J.  Gulliksen,  B.  Göransson,  I.  Boivie,  S.  Blomkvist,  J.\nPersson, and Å. Cajander, Key principles for user-centred\nsystems  design, Behaviour  and  Information  Technology,\nvol. 22, no. 6, pp. 397–409, 2003.\n[66]\n I.  Bogost,  The  problem  with  diversity  in  computing,\nhttps://www.theatlantic.com/technology/archive/2019/06/\ntech-computers-are-bigger-problem-diversity/592456/,\n2019.\n[67]\n M.  R.  Dickey,  The  future  of  diversity  and  inclusion  in\ntech, http://social.techcrunch.com/2019/06/17/the-future-\nof-diversity-and-inclusion-in-tech/, 2019.\n[68]\n C.  C.  Perez, Invisible  Women:  Data  Bias  in  a  World\nDesigned for Men. New York, NY, USA: Abrams, 2019.\n[69]\n T.  Miaskiewicz  and  K.  A.  Kozar,  Personas  and  user-\n[70]\n    236\nJournal of Social Computing, September 2021, 2(3): 226−237    \n \n\ncentered design: How can personas benefit product design\nprocesses? Design  Studies,  vol. 32,  no. 5,  pp. 417–430,\n2011.\n A.  L.  Massanari,  Designing  for  imaginary  friends:\nInformation architecture, personas and the politics of user-\ncentered  design, New  Media  and  Society,  vol. 12,  no. 3,\npp. 401–416, 2010.\n[71]\n Digital  Team,  Creating  a  user-friendly  way  to  find\nneighborhood  resources, https://www.boston.gov/news/\ncreating-user-friendly-way-find-neighborhood-resources,\n2019.\n[72]\n S.  Goldsmith  and  S.  Crawford, The  Responsive  City:\nEngaging Communities Through Data-Smart Governance.\nHoboken, NJ, USA: John Wiley & Sons Inc., 2014.\n[73]\n R.  T.  Lakoff, Talking  Power:  The  Politics  of  Language.\nNew York, NY, USA: Basic Books, 1990.\n[74]\n T.  Telford, ‘The  world  is  not  yet  ready  for  DeepNude’:\nCreator  kills  app  that  uses  AI  to  fake  naked  images  of\nwomen, https://www.washingtonpost.com/business/2019/\n06/28/the-world-is-not-yet-ready-deepnude-creator-kills-\n[75]\napp-that-uses-ai-fake-naked-images-women/, 2019.\n Walk  the  Moon,  Talking  is  hard.  RCA  Records,  no.\n88843-09809-2, 2014.\n[76]\n C.  Schwarz-Plaschg,  The  power  of  analogies  for\nimagining\n and\n governing\n emerging\n technologies,\nNanoEthics, vol. 12, no. 2, pp. 139–153, 2018.\n[77]\n L.  Hu,  Tech  ethics:  Speaking  ethics  to  power,  or  power\nspeaking\n ethics? \nJournal\n of\n Social\n Computing,\ndoi:10.23919/JSC.2021.0033.\n[78]\nJasmine  E.  McNealy is  an  associate\nprofessor  in  College  of  Journalism  and\nCommunications,  University  of  Florida.\nHer\n research\n focuses\n on\n emerging\ntechnology,  with  an  emphasis  on  privacy,\nsurveillance, and data governance. She has\npublished in internationally and nationally\nrecognized journals.\n  Jasmine E. McNealy:   Framing and Language of Ethics: Technology, Persuasion, and Cultural Context\n237    \n \n\n \nTech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\nLily Hu*\nAbstract:    In  recent  years,  tens  of  product  teams,  research  institutes,  academic  conferences,  and  college\ncourses—the list goes on—have cropped up under the banner of tech ethics to grapple with the social and\npolitical impact of technology. For some, an orientation around ethics indicates a moment of humility in an\nindustry  characterized  by  hubris.  Now  even  major  tech  corporations  are  seeking  expertise  outside  of  the\ntechnical sphere. In speaking tech ethics, we speak ethics to power. For others, the outlook is less rosy. Critical\nobservers take tech ethics to just be the latest tool in the same-old corporate toolshed—new rhetoric in service\nof old interests. Tech ethics is a wolf in sheep’s clothing. It is power speaking ethics. But debate about tech ethics\nconcerns more than descriptive analyses of current efforts as such. The capacities of ethical tech as a political\nmovement are also up for scrutiny. What is the political payoff of anyone speaking ethics at all? In this article,\nthe author approaches the question by drawing on a critical history of another moral-turned-political movement.\nA critical inquiry into the ascendency of human rights, the author suggests, elucidates the multiple functions\nof moral reasoning and rhetoric in political movements and lends insight into how they may ultimately bear on\npolitical efficacy. The 20th century history of human rights gives reason to be suspicious of moral language that\nis evasive of engaging political and ideological battles. However, it also points to the possibility that long-\nstanding moral ideals may be renewed and refashioned into new claims. Tech ethics may yet play such a role:\nplacing explicitly moral demands on those typically taken to be exempt from moral standards. This demand\nreaches beyond what the specialized moniker of “tech ethics” suggests.\nKey  words:   ethics of technology; political movements; human rights\n1    Introduction\nEvery  year  in  late-April  and  early-May,  thousands  of\ntech enthusiasts gather in convention center auditoriums,\nusually in the San Francisco and Seattle Areas, to watch\nthe  industry’s  biggest  names  unveil  their  companies’\nlatest  innovations.  The  sequential  late-spring  slate  of\ndeveloper conferences—Google I/O, Facebook F8, and\nthe Apple Worldwide Developers Conference—are like\na techie’s West Coast Met Gala: celebrities don signature\noutfits  and  dazzle  star-struck  fans;  press  and  critics\nreport on who best captured the zeitgeist; at the center\nof the events, the products themselves shine (in the case\nof most digital devices, literally emitting light) as though\nbeaming at the crowds.\nBut unlike the Met Gala, which showcases reactions to\na theme announced ahead of time, developer conferences\nalso set an agenda to come. Like all things in the tech\nworld, conferences are about the future. We are shown\nsnippets of our soon-to-be world—if the tech companies\nget their way, that is—a world of fancy wrist devices that\n“watch” much  more  than  time,  of  cylindrical  home\nassistants that serve as home stage lighting directors, of\nphones that unlock at a glance. The futures imagined by\nSilicon  Valley  are  idealized  visions  of  the  human-\ntechnology partnership: technologies are our tools. They\nhelp  us  do  what  we  want.  The  more  we  develop,  the\nbetter  off  we  will  be  in  attaining  what  we  want.  The\nquestions that follow these assumptions—What do we\n \n • Lily  Hu  is with  the  Applied  Mathematics  and  Philosophy\nDepartments, Harvard University, Cambridge, MA 02138, USA.\nE-mail: lilyhu@g.harvard.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-26;\naccepted: 2021-11-28\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   03/06  pp238−248\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0033\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nwant?  What  should  we  want?  To  what  extent  are\ntechnologies  really  mere  tools?—are  best  left  to  be\npondered by others outside the conference center.\nIt  was  striking,  then,  that  Microsoft  CEO  Satya\nNadella’s opening keynote at his company’s 2018 Build\nconference looked not to the future but to the past—first,\nto  the  Industrial  Revolution  by  way  of  the  economist\nRobert  Gordon’s  anti-techno-optimist  book The  Rise\nand  Fall  of  American  Growth,  and  then,  even  more\nsurprisingly, to the mid-century existentialist philosopher\nof  technology  Hans  Jonas.  Nadella  recounted  Jonas’s\nclaim that acting responsibly is to “act so that the effects\nof your action are compatible with the permanence of\ngenuine life” . Transitioning to a discussion of Jonas’s\n1973 essay “Technology and Responsibility” , Nadella\ncontinued: “That is something that we need to reflect on,\nbecause he was talking about the power of technology\nbeing such that it far outstrips our ability to completely\ncontrol  it,  especially  its  impact  even  on  future\ngenerations.” Nadella then segued from Jonas’s words\non responsibility to outlining the three core pillars he\nclaims  will  guide  Microsoft’s  plans  for  the  future:\nprivacy, cybersecurity, and ethical artificial intelligence.\nTechnology  might  well  still  be  a  tool,  but  it  is  also\nsomething  that  needs  to  be  controlled  and  even\nconstrained.\nIt is notable that this dual challenge of the contemporary\ntech  moment  found  its  way  into  Microsoft’s  biggest\npublicity event of the year by way of philosophy. What\nare we to make of Nadella’s choice—which we now see\nrepeated in the rhetoric of many institutions grappling\nwith the rapidly growing social and political impact of\ntechnology—to adopt the language of ethics in response\nto tech’s crisis of legitimacy? It is likely, of course, that\nthe  invocation  of  Jonas  and  Nadella’s  entreaty  to  his\nfellow engineers to consider “not only what computers\ncan  do  but  what  computers  should  do” was  mere\npublicity stunts aimed at humanizing both Microsoft and\nthe tech industry more widely. But granting, for the sake\nof argument, his commitment to the matter, we might\nstill ask what good it would serve? What can philosophy\nand ethics do in the harsh technological realities of our\npresent world?\n2    A Turn Toward, and Away From, Tech Ethics\nWhile discourse in and about tech continues to be largely\nruled by a spirit of optimism, a tempered tone, tales of\ncaution, and attention to societal risks and harms have\nbecome features of the industry’s narrative, too. Concern\nabout  such  wide-ranging  matters  as  the  role  of  social\nmedia in our civic landscape to a digital economy built\non  surveillance  to  automation-driven  joblessness  has\ntechnology  companies  under  greater  public  scrutiny.\nIncreasing attention to the ways technology makes and\nremakes  society  has  been  followed  by  demands  for\noversight,  regulation,  and  more  generally  for  a\nreassertion of values into the discussion of what we build.\nLatter  values-focused  approaches  to  technology’s\nimplications  for  society,  often  centering  around  the\nlanguage of moral ideals and principles, appear under the\nbroad umbrella term of “tech ethics”, and their rise both\ninside and outside of the companies themselves has been\naccompanied by two kinds of responses. On one view, to\nthe extent that philosophy can be useful at all for building\na just society, some amount of moral theorizing needs to\nmake  its  way  down  from  the  heavens  to  affect  the\npractices and politics of our earthly institutions. If we can\ncome to an agreement on the content of certain shared\nmoral ideals (a tough proposition to be sure, but one that\nis  not  impossible),  public  declarations  infused  with\nethical language can give shape to those moral ideals in\nthe real world and give directedness to actions aimed at\nachieving them. “Ethics” can force a shift in companies’\nnormative orientations, from their own bottom lines to\nthe roles they play in society: the duties and obligations\nthey  owe  to  a  broader  public.  In  doing  so,  thinking\nmorally  can  help  companies  avoid  potential  future\nmissteps and their accompanying social consequences.\nOthers  see  tech’s  adoption  of  ethical  language  as\nserving less honorable purposes. Skeptics not only doubt\nthe extent to which ethics can transform tech’s practices\nbut  have  questioned  whether  ethics,  as  deployed,  is\nmeant to even serve those purposes at all. Commentators\nsuch as Ben Wagner have decried the recent onslaught\nof company principles, frameworks, and guidelines, and\nas  mere “ethics-washing”,  aimed  at  masking  deeper\nstructural critiques and preventing regulatory actions[1].\nIn an industry ridden with scandals and rapidly losing\npublic trust, critics wonder why ethics has been chosen\nas the rallying cry. For companies that have as of yet been\nlargely unconstrained by state and legal forces, why have\nethical frameworks, promises, and principles appeared\nas safe policies to embrace? The tech ethics cynic sees\nthe  easy  co-optation  of  ethics  language  as,  to  use  a\npopular phrase in tech, “features, not bugs” of the ideas\n  Lily Hu:   Tech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\n239    \n \n\nthemselves:  vague  claims  to  center  human  values,\nconsider the social good, and avoid bias and unfairness.\nIn  his  book Radical  Technologies,  Adam  Greenfield\ncharacterizes messaging like Nadella’s as a “fig leaf of\n‘ethical development’”, allowing corporations to carry on\nwith business as usual, so long as they assure the public\nof their attention to various ethical considerations[2].\nBut critics like Wagner and Greenfield are skeptical\nmore generally of the tech ethics program, even when\nformulated  by  seemingly  independent  tech  advocacy\ngroups.  Their  reason  for  suspicion  is  clear  once  you\nfollow  the  money.  Besides  those  initiatives  that  are\nofficial  company  efforts,  many  organizations  that\nforward  an  ethics-centric  agenda—the  Association  of\nComputing Machinery’s Fairness, Accountability, and\nTransparency\n (FAccT)\n conference,\n the\n Good\nTechnology  Collective  think  tank,  and  the  Center  for\nHumane  Technology,  to  name  a  few—are  financially\nbacked by Big Tech. Partnership on AI is a non-profit\ncollaborative  effort  between  several  of  the  most\nprominent  tech  firms  (Amazon,  Apple,  Facebook,\nGoogle,  IBM,  and  Microsoft).  OpenAI,  similarly,  is\nsponsored by Amazon and Microsoft, and supported by\nElon Musk and Peter Thiel. Even the ivory tower, often\ncaricatured as fetishizing separation from the concerns\nof reality in favor of high-minded independent inquiry,\nhas sought a slice of the tech ethics pie and the money\nguaranteed to come along with it. The Stanford Institute\nfor  Human-Centered  Artificial  Intelligence  (HAI),  an\nendeavor  set  on  incorporating  human  values  into\ntechnology design and policy, is advised by a roster of\nSilicon Valley and Wall Street executives; meanwhile,\nthe Technical University of Munich Institute for Ethics\nin  Artificial  Intelligence  has  received  $7.5  million  in\nfunding from Facebook.\nAs  a  practical  matter,  this  relationship  between\n“independent” research and corporate cash is par for our\nneoliberal  course.  Ours  is  an  era  of  unprecedented\nslashes\n to\n the\n public\n financing\n of\n non-profit\norganizations,  as  well  as  some  of  our  most  important\ndemocratic institutions: elections, libraries, universities,\nand  public  service  broadcasting.  Whereas  ethics\nventures might have received public support in the past,\nshrinking budgets in funding agencies such as the offices\nof the National Endowment for the Humanities have left\ninstitutions  increasingly  reliant  on  the  graces,  whims,\nand self-interest of private philanthropy, both corporate\nand individual. Thus, given that programs and promises\nto be ethical need funding, institutions are left with little\nplace  to  go  but  to  the  standard  stock  of  elite  private\ndonors.  Recent  unveilings  of  colleges  and  university\ncenters  dedicated  to  the “social  good” demonstrate\nshocking  cases  of  short-term  memory  loss:  Stephen\nSchwarzman,  known  in  part  for  allying  with  Saudi\nCrown  Prince  Mohammed  Bin  Salman,  will  have  his\nname  forever  emblazoned  on  an “ethical” College  of\nComputing  at  Massachusetts  Institute  of  Technology\n(MIT)  in  exchange  for  a  financial  setback  of  $350\nmillion,  while  Henry  Kissinger  spoke  at  the  MIT\nCollege’s  inaugural  festivities  and  attended  the  HAI\nlaunch. It is clear why these collaborations are also in the\ninterest of their patrons: for the rich, famous, and morally\ndubious, paying for tech ethics buys a seat at the table and\nan  opportunity  to  eclipse  the  more  unsavory  parts  of\none’s history. Tech ethics are indulgences; universities\ngladly sell.\nMuch  of  the  ethics-washing  discourse  has  well\nidentified  the  at-best-amoral  coffers  of  tech  ethics\ninitiatives,  which  allow  corporations  to  maintain\noversight and even steer the public conversation about\ntheir growing power. On this view, Silicon Valley, with\nits long financial strings, plays the tech ethics marionette;\n“ethics” is a show, and they know it.\nBut  while  an  ethics-washing  story  that  centers\ncorporate control over the terms of political conversation\ncaptures  one  important  aspect  of  tech  ethics,  it\nunderplays  another  critical  feature  of  the  dynamic\nbetween ethics ventures and tech companies in today’s\nmovement. The author wants to suggest that there is a\nmuch  deeper  dependence  than  the  mere  financial  one\nbetween tech ethics initiatives and the corporations they\nattempt  to  keep  in  check—one  that  is  relatively  less\nexplored and lies in the political rationality of tech ethics.\nThis  dependency  is  mutual.  Just  as  Big  Tech  needs\n“ethics” on its side to maintain public goodwill, “ethics”\nventures need Big Tech for their own legitimacy. It is an\nuncomfortable fact that however much external advisory\nboards  and  universities  claim  to  be “third  parties”,\nethical  tech  institutions  are  in  fact  parasitic  on  the\ncontinual  moral  failures  and  disappointments  of  a\nhegemonic  tech  industry.  These  groups  and  efforts\nsurvive only because Big Tech has chosen to engage the\nethics discourse while it has blocked most other political\nmovement-building.  Up  to  now,  the  tech  ethics\n    240\nJournal of Social Computing, September 2021, 2(3): 238−248    \n \n\ndiscourse has only been able to make headway to the\nextent that corporate power has remained largely intact.\nThis mutual dependency, however, also suggests that\napplying  the  common  ethics-washing  critique  is  less\nstraightforward than we might expect. All sorts of tech\ncritiques  now  appear  in  the  language  of  ethics  for  a\nvariety of reasons—some might take on “ethics” as a\nconvenient label that now happens to hold sway with\ncompanies; others might masquerade as “ethics” simply\nto survive in the space; still others might intentionally\nchoose to reinsert “ethics” in our political discourse. Still,\nthe endorsement of ethics by corporate board members\nand  organizing  tech  workers  alike  is  unexpected  and\nalso  unsettling.  How  should  we  understand  such  a\nmultifaceted movement that lies at the convergence of so\nmany different political motives and ideologies? Does\nthe mutual dependency between tech ethics efforts and\ntech corporations expose the minimal political capacities\nof the movement? Or is it evidence of shifting tides in the\npublic’s expectations of corporate behavior? How can\nwe interpret and update the ethics-washing critique in\nlight  of  the  highly  varied  nature  of  the  tech  ethics\nlandscape and of the political moment in which it sits?\nThe author considers these questions through a lens that\nfocuses  on  the  place  of  moral  rhetoric  in  political\nmovements.  What  are  the  political  affordances  of  the\ntech  ethics  movement’s  self-conscious  orientation\naround the language of ethics?\nThe  author  wants  to  note  from  the  outset  that  her\ninvestment in these questions is not that of a disinterested\nonlooker. As a researcher, the author has worked in the\nbroader tech ethics area. The author has participated in\nconferences,  organized  workshops,  and  even  taught\nclasses on the field. This article is equal parts personal\nand academic interest. On one hand, self-reflection and\nanxiety  about  the  author's  own  experiences  and\nrelationship to this burgeoning tech ethics space. On the\nother hard, diagnosis and analysis of what tech ethics\ndoes and can do as a trend, a practical strategy, and a field\nof study. The exercise here is an attempt at scrutinizing\na movement and community of which the author is a\npart—recognizing  all  the  limitations  of  theorizing\nwithout remove.\n3    Moral Ideals and Political Movements\nMoral ideals occupy a delicate position within political\nprograms. The capacity for a moral political campaign to\nachieve democratic victories is highly contingent on its\nsurrounding  political  conditions.  Interpreting  the\npolitical capacities of the tech ethics movement requires\nan  analysis  of  both  the  material  and  ideological\nconditions under which such ideas and activism are able\nto flourish today. But tech ethics is not the first inspiring\npolitical  movement  to  self-consciously  center  moral\nideals. In this article, the author looks to another moral-\nturned-political human rights, which rose to become the\nlingua franca of global justice in the latter half of the\n20th century, as a frame through which to analyze the\ncontemporary  tech  ethics  moment.  The  author  shows\nthat in the cases of both tech ethics and human rights,\nthere  arises  a  mutual  dependency  between  the\nmovements  and  their  moral  ideals  on  one  hand  and\nreigning institutions and their logic on the other. Just as\nreigning  institutions  appeal  to  higher  moral  ideals  to\nbolster claims of legitimacy, both the present-day tech\nethics and the 20th century human rights movements rely\non  the  power  of  sponsor  institutions  to  ensure  their\ncontinued political relevance. The tension at the nexus\nof  moral  ideal,  political  practice,  and  institutional\ninstrumentalization is a central feature of the history of\nhuman  rights  and  one  that  the  author  argues  is\ncrucial to interpreting “tech ethics” as a contemporary\nphenomenon.\nSecond, the author reads the tech ethics movements,\nboth the corporate and tech worker movement one, as\npart of broader projects that look to (re)claim the role of\nmoral reasoning and language in our political sphere. If\nthe author is right, then the stakes of the movement are\nmuch  greater  than  the  specialized  title “tech  ethics”\nsuggests. Here again, the history of the rise of human\nrights has something to offer. If the political demands\nwe make are at-bottom moral claims about living in a just\nsociety, what factors influence the fate of these moral\narguments? Knowing an answer to this question can help\nus  assess  our  current  ethical  movement—has  it  been\nirredeemably captured by tech industry elites? Or does\nit have political potential? In looking to the post-World\nWar  II  development  of  human  rights,  we  gain  a  new\nperspective on ethics-washing charges and can in turn,\nbetter evaluate the opportunities and risks of today’s tech\nethics efforts.\n3.1    Human rights: Moral or political?\nHuman rights, those rights we are entitled to simply by\n  Lily Hu:   Tech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\n241    \n \n\nvirtue of being human, project an unconditional moral\nobjectivity,  justifying  their  priority  over  the  more\ncontingent facts of our worldly existence: what leaders\nwe might have, what government happens to rule us, and\nwhat political system we currently live under. Though\nhuman rights are genealogically descendant of a natural\nrights tradition that reaches back centuries, their rapid\ninternational ascendancy in the 1970s spawned a new\norientation to global justice that emphasized individual\nrights  separate  from  those  entitled  by  citizenship.\nHumans rights claimed higher moral ground than those\nenshrined by positive law; hence, Sen’s description that\nthey  are  often  seen  as “parents  of law”[3].  By\nemphasizing  rights  outside  of,  and  indeed  above,\ngovernance  structures,  the  modern-day  human  rights\nmovement did not have to confront perennial challenges\nof political organization.\nSome scholars who study the political circumstances\nsurrounding human rights see a less rosy picture of their\ninternational prominence. Moyn’s account of the 20th\ncentury history of human rights locates their ascendency\nat  a  time  of  exhaustion  with  ambitious  egalitarian\nvisions[4, 5]. Moyn sees such timing as evidence of the\ncompatibility of modern-day human rights activism with\nwhatever dominant ideological order happens to reign.\nThis  is  perhaps  a  first  hazard  of  relying  on  moral\nlanguage—even moral language that we more or less “all\nagree  with”—as  political  speech.  Far  from  offering  a\nstable moral lens through which to appraise the well-\nbeings of humans and their rights, the concept of “human\nrights” has always functioned as a political tool, to be\nfolded  into,  rather  than  to  destabilize,  the  reigning\ngeopolitical calculus of those who choose to wield it, be\nit  watchdog  NGOs,  international  political  bodies,  or\nnation-state governments.\nConsider, as example, the state of US foreign policy\nbefore and after Jimmy Carter’s famously human rights-\ncentric inaugural address in 1977. In the decades leading\nup  to  the “golden  era” of  human  rights  in  the  1970s,\nthe  US  amassed  a  remarkable  record  of  toppling\nregimes  and  replacing  them  with  right-wing  military\ndictatorships—most  notably  in  Latin  America  with\nGuatemala in 1954, Brazil in 1964, Chile in 1973, and\nArgentina  in  1976.  But  to  believe  that  foreign  policy\nprinciples  and  strategies  were  fundamentally  altered\nafter  1977  is  to  fail  to  appreciate  the  fundamentally\npolitically-embedded and instrumental nature of moral\ndiscourse. Moralistic human rights language could also\nbe  easily  incorporated  into  pre-existing  interstate\nallegiances and conflicts. “Good” human rights-focused\nforeign  policies  became  entangled  with  the  more\nmorally-ambiguous  ideal  of “democracy  promotion”.\nInterventions  originally  justified  in  the  name  of  the\nformer were frequently later defended by reference to the\nlatter. Human rights rhetoric reached new heights of dark\nirony  in  the  1980s  when  the  Reagan  Administration\nembarked on its bloody foreign policy strategy in Central\nAmerica that left dead hundreds of thousands of civilians,\nmuch  of  which  was  pursued  under  the  direction  of\nAssistant  Secretary  of  State  for  Human  Rights  and\nHumanitarian\n Affairs,\n Elliott\n Abrams.\n Abrams\nrepeatedly upheld the human rights record of the right-\nwing military junta in El Salvador that was responsible\nfor  an  estimated  seventy-thousands  civilian  deaths\nduring the course of the country’s civil war.① He even\ncontinued to push for more US aid to the Salvadoran\ngovernment, explicitly saying, “The purpose of our aid\nis to permit people who are fighting on our side to use\nmore violence”[10]. The real human rights mission was to\nprotect  American-style  democracy,  and  on  Abrams’\nview, the junta were “freedom fighters”, so the moral\nchoice was clear.②\nSuch blatantly self-serving rhetoric remained so much\na feature of the Reagan Administration’s human rights-\ncentric  foreign  policy  that  in  1985,  advocacy  groups\nexplicitly accused Abrams of developing and articulating\na “human  rights  ideology  which  complements\nand  justifies  Administration  policies”[11].  Funnily\nenough, the same charge has often been levied against\nthe entire realm of human rights practice and politics\nitself. From their United Nations declaration in 1948 to\nCarter’s  human  rights  inaugural  speech  to  Amnesty\nInternational’s Nobel Peace Prize in the 1977, human\nrights have always relied on the approval of the reigning\nWestern political bodies for the legitimacy of its moral\nforce. It is for this reason that the contemporary human\nrights agenda has retained a largely liberal approach to\njustice,  eschewing  the  broad  egalitarian  economic\nconcerns that have been at the center of other notable 20th\n① On Elliott Abrams’ human rights offenses and defenses of the US\nforeign policy in Central America[6–9].\n② The  descriptor  “freedom  fighters” was  oft  used  in  the  Reagan\nAdministration.  President  Reagan  used  the  term  to  refer  to  anti-\nCommunist insurgents everywhere in his first State of the Union of his\nsecond term in 1985. Elliot Abrams adopted the term to refer both to the\nContras in Nicaragua and to the insurgents in El Salvador.\n    242\nJournal of Social Computing, September 2021, 2(3): 238−248    \n \n\ncentury  political  movements,  such  as  socialism.\nReliance on institutional endorsement has thus limited\nthe  extent  to  which  human  rights  can  stand\nindependently of the larger animating political ideals of\nthe dominant powers that be, let alone challenge them.\nAs  the  US’s  war  in  Iraq  so  devastatingly  showed,\n“humanitarian” campaigns have proven compatible with\na  diverse  set  of  political  frameworks  and  agendas.\nWithout their own positive independent vision for global\njustice, human rights, even when pursued earnestly as a\nguide to moral political action, have been continually\nsubordinated to more assertive ideologies—in the case\nof the US, ideologies of neoconservatism, of imperial\nexpansion, and of global capitalism.\n3.2    Tech ethics: Moral or political?\nIf there are lessons to draw from this recent history of\nhuman  rights  for  the  purpose  of  understanding  tech\nethics, this transition from moral theory to institutional\npolitical instrumentalization is a good place to start. Just\nas causes of all sorts have marched under the banner of\nhuman rights, so we see the same in conversations about\ntech  ethics:  Google’s  capacity  to  bring  high-quality\ninformation to people across the globe becomes a social\nresponsibility to augment its user base. At the WIRED25\nSummit, Sundar Pichai portrayed the business decision\nto expand into global markets as an urgent moral choice,\nsaying, “Today, people either get fake cancer treatments,\nor they actually get useful information”[12]. Following\nthis line of reasoning—in which Google withholds life-\nsaving information when it fails to service populations—\nPichai arrived at the conclusion that Google is in fact\nethically “obliged” to  consider  how  it  can  expand  its\nservices to the 1.4 billion people in state-censored China.\nFor Apple’s Tim Cook, taking ethics seriously means\ncalling the business model of ad tech what it is: platforms\nbuilt  on  exploitation  and  surveillance.  What  is  the\nsolution to this “data industrial complex”[13]? Ensuring\nstrong protections against personal data extraction via\nhardware  solutions—luxury  good  devices  that  feature\npremium encryption for users. Fortunately for Google\nand Apple, doing ethics-aligned business is not so hard\nafter all. The business instrumentalization of tech ethics\nfollows the same pattern as that of the state’s deployment\nof  human  rights  rhetoric:  enlisted  to  complement  and\njustify more fundamental strategies that protect political\nand economic interests.\nEthics-washing  critics  have  called  attention  this\ncorporate ethics charade, but as the author has suggested,\nthe  incorporation  of  ethical  language  into  business\npitches represents only one modality of the tech industry\nand tech ethics interdependence. While firms might refer\nto  ethics  to  stave-off  greater  public  scrutiny,  the\nlegitimacy of tech ethics as a viable political program\nalso in part depends on the recognition that the effort is\nawarded  by  corporations.  Ethics-washing  critics  have\nmuch less noted this second type of reliance. Beyond the\nmaterial  consequences  that  tech  ethics  groups  would\nface if they issued a genuine challenge to tech power,\nmany mainstream organizations adhere to a theory of\nchange that requires corporate approval—a dependency\non  institutional  heavyweights  that,  as  the  author  has\nshown, echoes the logic and geopolitical power relations\nof the human rights political landscape. In the case of\ntech  ethics,  proposals  to  be  ethical  can  only  remain\nrelevant if tech firms choose to endorse them. Ironically\nthen,  tech  ethics  groups  become  reliant  on  a  certain\nsweet-spot  of  crisis:  enough  to  sustain  their  sense  of\npurpose and urgency, but not too much to spur calls for\na rejection of industry elites and a radical revision of our\ninstitutions. That is, a deeper ethics-washing charge may\nin  fact  cut  both  ways—corporations  use  ethics  as  a\ndiversion that distracts from meeting more substantial\npolitical  demands;  independent  tech  advocacy  groups\nuse ethics to bolster their own relevance as institutional\nchangemakers.\nThis joint convergence on a weak political program is\nno surprise to critical scholars of human rights. We see\nthe same with modern advocates of “human rights” who\nenvision a global community of watchdogs for abuse but\nrarely ask whether the baseline from which urgent crises\ndeviate is itself morally and politically acceptable. Much\nof human rights appears now so obvious to the Western\npublic  that  the  moral  consensus  seems  to  justify  a\nmovement  that  retreats  from  the  political  sphere.  Of\ncourse, the de facto reliance of human rights on dominant\npolitical  powers  and  their  governing  ideologies\ncontinues  to  demonstrate  political  allegiance,  albeit  a\nsilent one. The failure of human rights and humanitarian\norganizations to see their work as politically inflected\nsimply  serves  to  naturalize  these  dominant  political\nconditions and ideology. Here we notice a superficial but\nrather  telling  trend  in  how  tech  ethics  institutions  are\n  Lily Hu:   Tech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\n243    \n \n\nnamed. Many of their names emphasize an alignment\nwith “humanity”— the  Stanford  Institute  for  Human-\nCentered  Artificial  Intelligence,  Center  for  Human-\nCompatible AI, All Tech is Human, and the Center for\nHumane Technology, for example. The obviously-good\nalignment  with “humans” provides  groups  cover  for\nfailing  to  commit  to  more  specific  political  projects.\nTech ethics proposals have thus existed mainly as the\nnegative of crisis moments: every breach of our privacy\nand revelation of biased technological design is fodder\nfor ethics watchdogs, which can then prompt (gentle)\nintervention to correct the aberration.\nOne such example of how the ethics-washing charges\nmay indict all do-good tech organizations who push the\nmainstream tech ethics agenda is well illustrated by the\nactivism pursued by the Center for Humane Technology\n(CHT),  an  organization  which  boldly  declares  on  its\nwebsite that, “Technology is hijacking our minds and\nsociety”. Its ethical concerns have primarily cashed out\nin  the  form  of  advocating  for  more  conscientious\nconsumption of technology and greater emphasis on the\ndesign of applications that allow users to better monitor\ntheir digital activity. Tristan Harris, co-founder of CHT\nand former Google Design Ethicist, sees the roll-out of\nrecent phone use limiting features built into Apple’s iOS 12\nand Google’s Well-being tool as encouraging responses\nto  CHT’s “Time  Well  Spent” campaign  against\n“attention-hacking”.  Although  Harris  acknowledges\nthat such apps represent only baby steps in a larger battle,\nhe  sees “Time  Well  Spent” as  flipping  a  switch,\ntriggering a “race to the top for who can care more about\nthe fabric of society”[14]. On Harris’s view, then, profit-\ndriven market interactions still operate as the fixed point\nof institutional behavior with which ethical aspirations\nmust align. A movement that takes this tack can hardly\nsee a role for tech beyond serving as either our harvester\nor our caretaker.\nInterdisciplinary\n tech\n ethics-adjacent\n research\nventures  like  the  Fairness,  Accountability,  and\nTransparency  (FAccT)  conference  illustrate  a  more\nspecialized form of mutual dependency in which the tech\nethics academic discourse feeds on the shortcomings of\nBig Tech, while Big Tech bolsters the legitimacy of tech\nethics by engaging the ethics discourse. While FAccT as\nan  academic  venue  does  shine  light  on  important\nnormative, technical, and critical inquiry in the fields of\ncomputer  science,  law,  and  tech-concerned  social\nsciences, it is also likely the case that without the support\nof large tech companies, the field would not be seen as\nurgent, impactful, and generally as “hot” of a research\narea as it is today. Applied research of this sort greatly\nbenefits when large tech companies adopt their proposed\n“more  fair” technical  practices  or  ethical  guidelines.\nFAccT researchers are, generally-speaking, not shouting\ninto the void; quite the opposite, many are in fact meeting\nat post-conference corporate-sponsored cocktail parties\nto discuss collaborations across institutions and interests.\nIn environments like these, it is easy for considerations\nabout making a real-world positive impact to become\nconsiderations about how companies can be convinced\nto  adopt  such  reforms.  Sadly,  this  thought  process\neffectively  subordinates  questions  about  what  justice\nrequires to questions about what companies will likely\nfind agreeable. The scope of the tech ethics discourse can\nthus be easily hemmed by the naturalization of corporate\nlogic.\nIt bears noting that the limitations of CHT, FAccT, and\nsimilar  organizations  are  not  specific  to  the  groups\nthemselves; they have arisen due to a general shift in our\npolitical economy, in which the realm of the economic\nincreasingly shapes and even displaces the realm of the\npolitical. Just as the ascension of human rights cannot be\nunderstood absent the parallel dawn of the neoliberal age,\ntech  ethics  efforts  must  also  be  situated  within  this\ngreater political context. Mainstream ethics efforts fill a\nvacuum of institutional political activism in an area that\nexists  due  to  a  variety  of  factors:  successful  political\ncapture, insufficiencies of collective action, a significant\nstructural advantage of Big Tech in the economy, and a\ngenuine uncertainty among both policymakers and the\ngeneral  public  about  the  harms  and  benefits  of\ntechnology. This stalemate, along with the chilling effect\nof financial sponsorship, limits the extent to which ethics\ngroups are willing and able to agitate for more ambitious\nstructural  change.  What  remains  is  the  narrow  ability\nto  challenge  those  impacts  and  behaviors  that\norganizations view as clearly morally objectionable—\nhence  the  language  of  ensuring “humane” tech\nsolutions—in order to ameliorate those particular ills.\n4    Inevitability  and  Contingency  in  the\nPolitics of Tech Ethics\nIn  pointing  out  the  mutual  dependency  that  underlies\nmuch of the mainstream tech ethics movement today, the\n    244\nJournal of Social Computing, September 2021, 2(3): 238−248    \n \n\nauthor  does  not  intend  to  immediately  undercut  the\ncritical  value  and  independent  integrity  of  all  such\nventures  of  research.  The  interdependency  does,\nhowever, bring to the fore important questions about the\npolitics and morality of conducting “ethical” research in\nan area that is shot through with neoliberal logic. As a\nresearcher  who  has  participated  in  FAccT,  the  author\nfinds these conflicting desires exceedingly difficult to\nnegotiate.  On  one  hand,  the  author  has  an  interest  in\nproducing work that speaks with courage and honesty to\nher normative political commitments; on the other hand,\nthe author has an interest in being accepted by a larger\ncommunity  of  scholars,  many  of  whom  reside  at  Big\nTech,  and  the  author  carries  a  (faint)  hope  that  tech\ncompanies will consider her scholarship in a way that\ndestabilizes unjust yet profitable business practices. On\none hand, the author has an interest in scholarship that\ndispels with the siren song of political neutrality on the\nmost urgent questions of ethical tech; on the other hand,\nthe author has an interest in the community’s continual\nappeal to Big Tech, which allows it to persist as a model\nof productive discourse between academia and industry.\nIn ideal conditions of practical discourse, perhaps these\ntwo  visions  would  be  reconcilable.  But  such  a  rosy\ninterpretation  refuses  to  confront  the  necessity  of\npolitical struggle in a sphere well overdue for it.\nThe  problem,  then,  is  that  the  success  of  FAccT’s\nconstructive  cross-sector  exchange  cuts  both  ways.  It\nproves that tech companies’ products and processes can\nbe shaped by thoughtful ethics-adjacent research, but it\nalso shows how a symbiotic relationship between tech\nfirms and tech ethics can obscure the fundamental fact\nof political contestation undergirding the ethical issues\nat stake. This latter consequence is what the author finds\nto  be  most  worrying  about  tech  ethics  collaborations\ntoday. If the story of ethical technology has, up to now,\nbeen  one  of  effective  assimilation  under  corporate\ninfluence,  then  we  may  have  to  face  up  to  a  great\npotential irony of tech ethics: that pursuing the ethical\nmovement we most need would actually compel us to\nimmediately cast many of our current campaigns into\nobsolescence. This, in fact, is the ethics-washing charge\nat its strongest: a claim about the use and norms of tech\nethics in a corporatized language-game inimical to our\ndire need for a genuine redistribution of power.\nThe  strong  ethics-washing  claim  that  the  political\nvirility of tech ethics language has been doomed from the\nstart shares notable similarities with another influential\nidea in the scholarship on human rights. The view that\nan appeal to “ethical technology” undermines the larger\npolitical project parallels historian Lynn Hunt’s “logic\nof  rights” account  of  how  the  inexorable  cascade  of\nnatural  rights  philosophizing  led  to  the  current  wide\nacceptance of human rights[15]. On Hunt’s view, once\nhuman rights were born in the 18th century America and\nFrance, it was only a matter of time before they would\ndevelop into a full-fledged form as they did in the latter\nhalf of the 20th century. Whereas Hunt claims that rights\nlanguage could only lead to an earnest commitment to\ntheir undergirding moral principles by the powers that\nbe, the tech ethics cynic sees that ethics language in tech\ncould only lead to a full absorption of such principles into\ncorporate  logic.  The  two  perspectives  share  a  belief\nabout  inevitability,  though  their  conclusions  are\ndiametrically opposed.\nHunt’s account, however, sees only continuities in the\nintellectual history of natural rights stretching to human\nrights  practice  today,  overlooking  broader  political\ncontext as a force shaping the course of the movement.\nAn “ethics-washing” tale  about  the  inevitability  of\ncorporate  capture  of  ethical  language  in  our  current\nmoment commits a similar oversight and fails to account\nfor  the  significance  of  historical  contingency  to  all\nintellectual and political movements. In The Last Utopia,\nSamuel Moyn reminds us that a more complete history\nof human rights is not a tale of ripening—a slow but sure\ncoming into being—but a tale about the breakdown of\npolitical  alternatives:  a  national  sovereignty  mission\ntoward\n social\n democracy\n accompanied\n by\n a\ndecolonization  project  toward  a  more  egalitarian\ninternational order. Neither were these projects doomed\nfrom the start. The New International Economic Order\n(NIEO), proposed in 1974, sought to upend the reigning\nglobal  economic  order  by  calling  for  redistributive\njustice and an international body in which every nation-\nstate, regardless of its size or economic power, would be\ngiven one vote in matters of global import. Leaders of\nnew nations in the Global South were especially focused\non gaining the ability to override the liberal notions of\nfree trade and economic ownership that had been taken\nas central in matters of international governance. They\nasserted a “right to development”, a collective claim by\nformer colonized people against their colonizers in the\nNorth  to  both  take  their  national  fates  into  their  own\n  Lily Hu:   Tech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\n245    \n \n\nhands and to a fundamental equality on the international\nstage. Alas, in the late 1970s, when a political future like\nthat proposed by NIEO was seen no longer as viable, a\nlimp moral individualism dressed up as human rights was\nleft to take up the mantle of global justice. It is important,\nhowever, to recognize that the NIEO did not fail of its\nown  accord—politics  are  always  operating  beneath\nthe  surface.  Rather,  elite  neoliberals  who  feared  the\neffects that runaway democracy would have on the reach\nof property and capital undertook a concerted effort to\nmake known the great danger that the NIEO posed to\nWestern civilization. General Agreement on Tariffs and\nTrade (GATT) chief economist Jan Tumlir scoffed at the\naudacity  of  the  Global  South’s  attempt  to  restructure\ninternational politics to achieve redistribution, speaking\nabout NIEO with a sneer, “Not only do nations claim to\nbe determining their own future within a global order;\nnow that order itself is to be transcended”[16].\nThe human rights of the past fifty years must be read\nin light of a shattered NIEO. The present movement, in\ncontrast, has not sought such bold plans as restructuring\ninternational  governance.  It  has  largely  defended  a\nminimalist  conception  of  global  justice,  aimed  at\nmitigating  the  harms  of  famine,  severe  poverty,  and\nthose reprehensible political leaders who starve, torture,\nand kill. Transformations of the social, economic, and\npolitical order within the nation, along with aspirations\nof solidarity and egalitarianism at the international level,\nhave been left behind.\n5    An Outlook on Tech Ethics\nThe cynic who sees human rights as descendent of an\nAnglo-American  tradition  of  liberal  individualism\ninterprets this to be an unavoidable outcome of moral\nideals with inherently impoverished political capacities.\nBut this conclusion is wrong. Alternative histories and\norigins of human rights can be found, even within the\nnarrow confines of the Enlightenment.③ In the nearby\nFrench  tradition,  human  rights  were  closely  tied  with\negalitarian  (though,  it  should  be  noted,  still  largely\nexclusionary)  ideas  of  democratic  self-rule  and\nparticipatory government. There is no reason that a 20th\ncentury human rights practice built on these tenets could\nnever  have  flourished.  Nevertheless,  few  critics  of\nhuman rights now hold out hope for this possibility: the\nthin  moral  individualist  capture  of  human  rights  has\nproven too successful. It is better to pursue other ways\nforward now.\nWith this framing in mind, the question for our own\nmovement is simple: has the corporate capture of tech\nethics proven too successful as well? Commitments to\nethics  and  social  responsibility  now  sit  comfortably\nwithin a corporation’s standard stock of business-speak,\nwhile  even  the  nominally-independent-but-flush-with-\ncorporate-cash  tech  ethics  sphere  can  only  plea  for\ndecency. What role now remains for ethical language to\nplay in a movement that wishes for a genuine challenge\nto corporate power?\nReturning to the importance of political and historical\ncontingency to the development of human rights practice\nis instructive. Even if the global justice affordances of\nthe human rights project have more or less been settled,\nthe same question about the capacity for justice within\nthe tech ethics movement has not been. If Moyn is right\nthat the fate of a movement is as much dictated by the fate\nof alternatives, then declaring the larger fight for tech\nethics as dead on arrival is premature.\nFirst, there is good reason to believe that the happy\nillusion  of  consensus  enforced  by  steady  economic\ngrowth and Third Way politics is coming to an end. The\n2000s have already brought startling revelations that the\nUnited States (and capitalist liberal democracies more\nbroadly) is neither economically nor politically stable.\nReform in the form of technocratic tinkering is no longer\nthe horizon of our mainstream political imaginary. If the\nascent of a sufficientarian human rights program could\nonly sit comfortably once egalitarian internationalisms\nhad lost out to a rising neoliberal agenda, then the return\nof  politics  means  a  resurgence  of  ideological\ndebate—and  a  potential  overthrow  of  previously\nreigning\n conceptions\n of\n justice.\n Perhaps\n tech\ncorporations will no longer be able to smooth over their\ncrisis  of  legitimacy  with  good  ethics  messaging.  The\never-louder  ethics-washing  chorus  itself  demonstrates\nthe mounting challenges that corporations face in trying\nto  assert  their  own  visions  of  ethics.  The  public  is\nincreasingly  keeping  their  eyes  on  the  capture  and\nsubversion of our ideals.\nSecond, the decision to place ethical language at the\ncenter of a promise of better behavior is not a risk-free\nstrategy.  Companies  that  choose  to  do  so  make  the\nexplicit  and  important  concession  that  their  conduct\n③For two recently published books that look elsewhere for origins of\nhuman rights[17, 18].\n    246\nJournal of Social Computing, September 2021, 2(3): 238−248    \n \n\nshould be held accountable to normative principles and\ndemands from the public. In this renewed era of political\nmobilization, it is possible that—or one can only hope\nthat—attempts to pervert ethical language for business\npurposes  represent  such  a  clear  transgression  against\nthe  urgency  of  reevaluating  our  society’s  moral\ncommitments that the tech ethics strategy can backfire:\ncompanies  might  find  themselves  unable  to  tame\ndemands for ethical tech and instead need to commit to\nthem in earnest.\nWhether  this  will  in  fact  happen  will  of  course  be\ndetermined by a variety of factors, but there is reason for\ncautious  hope.  Tech  workers  protest  against  their\ncompanies’ unethical  practices  have  already  been  a\nsurprising instance of collective mobilization in direct\nresponse to the hypocrisy of tech ethics: for example,\nGoogle\n employees\n successfully\n pressured\n their\nemployer  to  cancel  its  multiple  bids  for  government\nmilitary  contracts  that  would  contribute  to  more\neffective killing operations[19, 20], as well as to retract a\ncontroversial  external  advisory  board  on  ethics  that\nincluded a member with anti-LGBTQ, anti-immigrant,\nand  climate  denialist  views,  mere  days  after  it  was\nannounced[21].  In  their  activism,  tech  workers  are\nincreasingly  recognizing  the  role  that  ethics  language\nhas served for companies up to now, but rather than cede\nthe conceptual ground, they have continued to insist on\nan  ethics  that,  in  the  words  of  legal  scholar  Rashida\nRichardson,  serves  as  a “moral  compass” rather  than\n“just another rubber stamp”—an ethics that refuses to be\ncontrolled  by  tech  but  instead  seeks  to  holds  power\nwithin  it[22].  As  an  ideological  transformation  beyond\njust  a  policy  one,  neoliberalism  expunges  our  social\nworld  of  ethical  commitments  to  anything  other  than\nprivate economic interests. Rejecting neoliberalism—and\npreserving democratic politics—requires this exact kind\nof struggle to reclaim ethics from those who attempt to\nredefine its meaning and possibility.\nOne can recognize the historical contingency of ideas\nand  the  performativity  of  words  while  also  still\nacknowledging that some bannered slogans will be more\neffective  than  others  in  achieving  a  political  vision.\nChoosing language is a task of political strategizing. But\nin the end, no words, even the most carefully selected and\nperfectly suited, predispose a movement to victory. A\nbelief in the inherent lack of certain concepts and the\nsuperior natures of others can mask the fact that political\nefforts  are  never  descriptive;  they  are  always\naspirational. Taking ethical principles and language to\nalways be deployed as speech acts should help us to re-\ninterpret our current tech ethics moment as a failure of\ndeeds, not only a failure of words. Moral principles, be\nthat of human rights or of ethical tech, communicate a\npolitical end that we insist on. Their assimilation under\nother  logics  is  dangerous  precisely  because  they  risk\nredefining not only the words themselves but the terms\nof the larger political project. Their successful capture\ndisciplines our ambitions for a better world.\nAsserting  a  tech  ethics  that  insists  on  the  moral\ncommitments between us and our institutions, each of us\nto each other, is political work that can never be carried\nout by corporations and the elite, orchestrating conduct\nfrom above, but only by all of us from below, collectively\nbuilding and agitating for a future that is fully our own.\nAcknowledgment\nL. Hu would like to thank Will Holub-Moorman for his\ninvaluable feedback on this article and Ben Green for his\nhard  work  and  patience  in  corralling  together  this\nselection  of  articles  and  making  this  special  issue\npossible. L. Hu would also like to acknowledge support\nfrom  the  National  Science  Foundation  and  the  Jain\nFamily Institute.\nReferences\n B.  Wagner,  Ethics  as  an  escape  from  regulation:  From\n‘ethics-washing’ to  ethics-shopping?  in Being  Profiled:\nCogitas Ergo Sum, E. Bayamiloglu, I. Baraliuc, L. A. W.\nJanssens,  and  M.  Hildebrandt,  eds.  Amsterdam,  the\nNetherland:  Amsterdam  University  Press,  2018,  pp.\n84–89.\n[1]\n A.  Greenfield, Radical  Technologies:  The  Design  of\nEveryday Life. New York, NY, USA: Verso Books, 2017.\n[2]\n A. Sen, Elements of a theory of human rights, Philosophy\n& Public Affairs, vol. 32, no. 4, pp. 315–356, 2004.\n[3]\n S.  Moyn, The  Last  Utopia:  Human  Rights  in  History.\nCambridge, MA, USA: Harvard University Press, 2010.\n[4]\n S.  Moyn, Not  Enough:  Human  Rights  in  an  Unequal\nWorld. Cambridge, MA, USA: Harvard University Press,\n2018.\n[5]\n W. M. LeoGrande, Our Own Backyard: The United States\nin Central America, 1977−1992. Chapel Hill, NC, USA:\nUniversity of North Carolina Press, 1998.\n[6]\n G. Gugliotta and D. Farah, 12 years of tortured truth on El\nSalvador, The Washington Post, March 21, 1993.\n[7]\n C. Krauss, How U. S. actions helped hide salvador human\nrights abuses, The New York Times, March 21, 1993.\n[8]\n M.  Danner,  The  truth  of  El  Mozote, The  New  Yorker,\nNovember 28, 1993.\n[9]\n R.  Pear,  Push  the  Russians,  Intellectuals  say, The  New\n[10]\n  Lily Hu:   Tech Ethics: Speaking Ethics to Power, or Power Speaking Ethics?\n247    \n \n\nYork Times, November 25, 1985.\n T.  Jacoby,  The  Reagan  turnaroud  on  human  rights,\nForeign Affairs, vol. 64, no. 5, pp. 1066–1086, 1986.\n[11]\n N.  Tiku,  Google’s  CEO  says  tests  of  censored  Chinese\nsearch engine turned out great, Wired Magazine, October\n15, 2018.\n[12]\n S. Schechner and E. Peker, Apple CEO condemns ‘data-\nindustrial complex’, The Wall Street Journal, October 24,\n2018.\n[13]\n T.  Harris,  How  to  stop  technology  from  destabilizing\nthe world, https://www.salesforce.com/video/3402946/?bc=\noth, 2018.\n[14]\n L. Hunt, Inventing Human Rights: A History. New York,\nNY, USA: W. W. Norton, 2007.\n[15]\n Q.  Slobodian, Globalists:  The  End  of  Empire  and  The\nBirth  of  Neoliberalism.  Cambridge,  MA,  USA:  Harvard\nUniversity Press, 2018.\n[16]\n H.  Rosenblatt,  The  Lost  History  of  Liberalism:  From\nAncient Rome to the Twenty-First Century. Princeton, NJ,\nUSA: Princeton University Press, 2018.\n[17]\n D. Edelstein, On the Spirit of Rights. Chicago, IL, USA:\nUniversity of Chicago Press, 2018.\n[18]\n A. F. Campbell, How tech employees are pushing Silicon\nValley to put ethics before profit, Vox Media, October 18,\n2018.\n[19]\n N. Nix, Google drops out of Pentagon’s $10 billion cloud\ncompetition, Blooming News, October 8, 2018.\n[20]\n N. Slatt, Google dissolves AI ethics board just one week\nafter forming it, The Verge, April 4, 2019.\n[21]\n B. Johnson and G. Lichfield, Hey Google, sorry you lost\nyour  ethics  council,  so  we  made  one  for  you, MIT\nTechnology Review, April 6, 2019.\n[22]\nLily  Hu is  a  PhD  candidate  in  applied  mathematics  and\nphilosophy at Harvard University. She received the AB degree in\nmathematics from Harvard College in 2015. She will start as an\nassistant professor of philosophy at Yale University in 2022. Her\nwork is presently supported by a fellowship from the Edmond J.\nSafra Center for Ethics and the Jain Family Institute.\n    248\nJournal of Social Computing, September 2021, 2(3): 238−248    \n \n\n \nData Science as Political Action: Grounding Data Science in\na Politics of Justice\nBen Green*\nAbstract:    In response to public scrutiny of data-driven algorithms, the field of data science has adopted ethics\ntraining and principles. Although ethics can help data scientists reflect on certain normative aspects of their\nwork, such efforts are ill-equipped to generate a data science that avoids social harms and promotes social\njustice. In this article, I argue that data science must embrace a political orientation. Data scientists must\nrecognize themselves as political actors engaged in normative constructions of society and evaluate their work\naccording to its downstream impacts on people’s lives. I first articulate why data scientists must recognize\nthemselves as political actors. In this section, I respond to three arguments that data scientists commonly invoke\nwhen challenged to take political positions regarding their work. In confronting these arguments, I describe\nwhy attempting to remain apolitical is itself a political stance—a fundamentally conservative one—and why\ndata science’s attempts to promote “social good” dangerously rely on unarticulated and incrementalist political\nassumptions. I then propose a framework for how data science can evolve toward a deliberative and rigorous\npolitics of social justice. I conceptualize the process of developing a politically engaged data science as a\nsequence of four stages. Pursuing these new approaches will empower data scientists with new methods for\nthoughtfully and rigorously contributing to social justice.\nKey  words:   data science; ethics; politics; social justice; social change; social good; pedagogy\n1    Introduction\nThe  field  of  data  science  has  entered  a  period  of\nreflection and reevaluation.① Alongside its rapid growth\nin both size and stature in recent years, data science has\nbecome  beset  by  controversies  and  scrutiny.  Machine\nlearning algorithms that guide decisions in areas such as\nhiring, healthcare, criminal sentencing, and welfare are\noften\n biased,\n inscrutable,\n and\n proprietary[1−6].\nAlgorithms  that  drive  social  media  feeds  manipulate\npeople’s  emotions[7],  spread  misinformation[8],  and\namplify  political  extremism[9].  Facilitating  these  and\nother  algorithms  are  massive  datasets,  often  gained\nillicitly  or  without  meaningful  consent,  that  reveal\nsensitive and intimate information about people[10−13].\nMany  individuals  and  organizations  responded  to\nthese controversies by advocating for a focus on ethics\nin computing training and practice[14]. Universities have\ncreated new courses that train students to consider the\nethical  implications  of  computer  science[15−18];  one\ncrowdsourced  list  includes  more  than  300  such\nclasses[19]. Former US Chief Data Scientist D. J. Patil has\nargued that data scientists need a code of ethics[20]. The\nAssociation  for  Computing  Machinery  (ACM),  the\nworld’s  largest  educational  and  scientific  computing\nsociety,  updated  its  Code  of  Ethics  and  Professional\nConduct in 2018 for the first time since 1992[21]. The\nbroad motivation behind these efforts is the assumption\nthat,  if  only  data  scientists  were  more  attuned  to  the\n \n • Ben Green is with  the Society of Fellows and the Gerald R. Ford\nSchool of Public Policy, University of Michigan, Ann Arbor, MI\n48109, USA. E-mail: bzgreen@umich.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-19;\naccepted: 2021-11-25\n① Throughout  this  article,  “data  science” encompasses  the  use  of\ncomputational  methods  (including  artificial  intelligence  and  machine\nlearning) to derive patterns from data in order to make predictions about\nthe future. In this sense, a data scientist is anyone who works with data\nand algorithms in these settings. My particular focus is on the application\nof data science methods to social and political contexts.\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   04/06  pp249−265\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0029\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nethical  implications  of  their  work,  many  harms\nassociated with data science could be avoided[14].\nAlthough emphasizing ethics is an important step in\ndata  science’s  development  toward  greater  socially\nresponsibility, it is an insufficient response to the broad\nissues  of  social  justice  that  are  implicated  by  data\nscience.② As described in the introductory article for this\nspecial  issue,  technology  ethics  as  applied  in  practice\nsuffers  from  four  significant  limitations[14].  First,\ntechnology  ethics  principles  are  abstract  and  lack\nmechanisms  to  ensure  that  engineers  follow  ethical\nprinciples.  Second,  technology  ethics  has  a  myopic\nfocus on individual engineers and on technology design,\noverlooking  the  structural  sources  of  technological\nharms.  Third,  technology  ethics  is  subsumed  into\ncorporate logics and practices rather than substantively\naltering behavior. All told, the rise of technology ethics\noften reflects a practice dubbed “ethics-washing”: tech\ncompanies  deploying  the  language  of  ethics  to  resist\nmore structural reforms that would curb their power and\nprofits.\nThus, while ethics provides useful frameworks to help\ndata scientists reflect on their practice and the impacts\nof  their  work,  these  approaches  are  insufficient  for\ngenerating a data science that avoids social harms and\nthat\n promotes\n social\n justice.\n The\n normative\nresponsibilities  of  data  scientists  cannot  be  managed\nthrough  to  a  narrow  professional  ethics  that  lacks\nnormative  weight  and  supposes  that,  with  some\nreflection  and  a  commitment  to  best  practices,  data\nscientists  will  make  the “right” decisions  that  lead  to\n“good” technology. Instead of relying on vague moral\nprinciples that obscure the structural drivers of injustice,\ndata scientists must engage in politics: the process of\nnegotiating  between  competing  perspectives,  values,\nand goals.\nIn other words, we must recognize data science as a\nform of political action. Data scientists must recognize\nthemselves  as  political  actors  engaged  in  normative\nconstructions  of  society.  In  turn,  data  scientists  must\nevaluate  their  efforts  according  to  the  downstream\nimpacts on people’s lives.\nBy  politics  and  political,  I  do  not  refer  directly  to\npartisan or electoral debates about specific parties and\ncandidates. Instead, I invoke these terms in a broader\nsense that transcends activity directly pertaining to the\ngovernment,  its  laws,  and  its  representatives.  Two\naspects  of  politics  are  paramount.  First,  politics  is\neverywhere in the social world. As defined by politics\nprofessor Adrian Leftwich, “politics is at the heart of all\ncollective  social  activity,  formal  and  informal,  public\nand  private,  in  all  human  groups,  institutions,  and\nsocieties”[23].  Second,  politics  has  a  broad  reach.\nPolitical scientist Harold Lasswell describes politics as\n“who gets what, when, how”[24]. The “what” here could\nmean  many  things:  money,  goods,  status,  influence,\nrespect, rights, and so on. Understood in these terms,\npolitics  comprises  any  activities  that  affect  or  make\nclaims about the who, what, when, and how in social\ngroups, both small and large.\nData scientists are political actors in that they play an\nincreasingly\n powerful\n role\n in\n determining\n the\ndistribution  of  rights,  status,  and  goods  across  many\nsocial  contexts.  As  data  scientists  develop  tools  that\ninform  important  social  and  political  decisions—who\nreceives a job offer, what news people see, where police\npatrols—they shape social outcomes around the world.\nData scientists are some of today’s most powerful (and\nobscured) political actors, structuring how institutions\nconceive of problems and make decisions.\nThis article will justify and develop the notion of data\nscience  as  political  action.  My  argument  raises  two\nquestions:  (1)  Why  must  data  scientists  recognize\nthemselves  as  political  actors?  and  (2)  How  can  data\nscientists  ground  their  practice  in  a  politics  of  social\njustice? The two primary sections of this article will take\nup these questions in turn.\nMy aim is to support data science toward playing a\nmore  productive  role  in  promoting  equity  and  social\njustice. I do not intend to stop data science in its tracks,\ncritique  individual  practitioners,  or  discourage  data\nscientists  from  working  on  social  problems.  The  path\nahead does not require data scientists to abandon their\ntechnical expertise, but it does require data scientists to\nexpand their notions of what problems to work on and\nhow to engage with society. This process may involve\nan uncomfortable period of change. But I am confident\nthat  exciting  new  areas  for  research  and  practice  will\nemerge, producing a field that can contribute to a more\negalitarian and just society.\n② In Black Feminist Thought, Patricia Hill Collins defines a “social\njustice project” as “an organized, long-term effort to eliminate oppression\nand empower individuals and groups within a just society”. Oppression,\nshe writes, is “an unjust situation where, systematically and over a long\nperiod of time, one group denies another group access to the resources of\nsociety”[22].\n    250\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\n2    Why  Must  Data  Scientists  Recognize\nThemselves as Political Actors?\nThe first part of this article will attempt to answer this\nquestion  in  the  form  of  a  dialogue  with  a  well-\nintentioned skeptic. I will respond to three arguments\nthat are commonly invoked by data scientists when they\nare challenged to take political stances regarding their\nwork. These arguments have been expressed in a variety\nof  public  and  private  settings  and  will  be  familiar  to\nanyone who has engaged in discussions about the social\nresponsibilities of data scientists.\nThese are by no means the only arguments proffered\nin this larger debate, nor do they represent any sort of\nunified  position  among  data  scientists.  In  practice,\ncomputer  scientists  are “diverse  and  ambivalent\ncharacters”[25] who engage in “nuanced, contextualized,\nand  reflexive  practices”[26].  Some  computer  science\nsubfields  (such  as  CSCW[27])  have  long  histories  of\nengaging  with  sociotechnical  practices  and  normative\nimplications, while others (such as the ACM Conference\non Fairness, Accountability, and Transparency (FAccT))\nare actively developing such approaches. Nonetheless,\nin my experience, the three positions considered here are\nthe  most  common  and  compelling  arguments  made\nagainst  a  politically  oriented  data  science.  Any\npromotion  of  a  more  politically  engaged  data  science\nmust contend with them.\n2.1    Argument 1: “I am just an engineer”\nThis first argument represents a common attitude among\nengineers. In this view, although engineers develop new\ntools, their work does not determine how a tool will be\nused. Artifacts are seen as neutral objects that lack any\ninherent normative character and that can simply be used\nin good or bad ways. By this logic, engineers bear no\nresponsibility for the impacts of their creations.\nIt  is  common  for  data  scientists  to  argue  that  the\nimpacts of technology are unknowable. As one computer\nscientist  who  faced  criticism  for  developing  facial\nrecognition  software  argued  in  defense  of  his  work,\n“Anything can be used for good. Anything can be used\nfor bad”[28]. Similarly, during a 2019 NeurIPS workshop,\nin which two panelists highlighted the harmful impacts\nof  AI  on  communities  of  color,  several  computer\nscientists in the audience countered that it is impossible\nto  know  what  the  impacts  of  research  will  be  or  to\nprevent others from misusing products[29].\nBy articulating their limited role as neutral researchers,\ndata  scientists  provide  themselves  with  an  excuse  to\nabdicate  responsibility  for  the  social  and  political\nimpacts of their work. When a paper that used neural\nnetworks  to  classify  crimes  as  gang-related  was\nchallenged for its potentially harmful effects on minority\ncommunities,  a  senior  author  on  the  paper  deflected\nresponsibility by arguing, “It’s basic research”[30].\nAlthough it is common for engineers to see themselves\nas separate from politics, many scholars have thoroughly\narticulated how technology embeds politics and shapes\nsocial outcomes. As political theorist Langdon Winner\ndescribes, “technological  innovations  are  similar  to\nlegislative  acts  or  political  foundings  that  establish  a\nframework for public order that will endure over many\ngenerations. For that reason, the same careful attention\none would give to the rules, roles, and relationships of\npolitics must also be given to such things as the building\nof highways, the creation of television networks, and the\ntailoring  of  seemingly  insignificant  features  on  new\nmachines.  The  issues  that  divide  or  unite  people  in\nsociety  are  settled  not  only  in  the  institutions  and\npractices of politics proper, but also, and less obviously,\nin tangible arrangements of steel and concrete, wires and\nsemiconductors, and nuts and bolts”[31].\nEven  though  technology  does  not  conform  to\nconventional notions of politics, it often shapes society\nin much the same way as laws, elections, and judicial\nopinions.  In  this  sense, “the  scientific  workplace\nfunctions as a key site for the production of social and\npolitical order”[32]. Thus, as with many other types of\nscientists,  data  scientists  possess “a  source  of  fresh\npower that escapes the routine and easy definition of a\nstated political power”[33].\nThere are many examples of engineers developing and\ndeploying technologies that, by structuring behavior and\nshifting power, shape aspects of society. As one example,\nWinner famously (and controversially[34, 35]) describes\nhow  Robert  Moses  designed  the  bridges  over  the\nparkways  on  Long  Island,  New  York  with  low\noverpasses[31].  Moses  purportedly  did  this  to  prevent\nbuses (which predominantly carried lower-class and non-\nwhite urban residents) from navigating these parkways\nand accessing the parks to which they led.\nAnother  historical  example  similarly  demonstrates\nhow the design of traffic technologies can have social\nand  political  ramifications.  As  historian  Peter  Norton\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n251    \n \n\ndescribes, when automobiles were introduced onto city\nstreets in the 1920s, they created chaos and conflict in the\nexisting  social  order[36].  Many  cities  turned  to  traffic\nengineers  as “disinterested  experts” whose  scientific\nmethods could provide a neutral and optimal solution.\nBut  the  engineers’ solution  contained  unexamined\nassumptions and values, namely, that “traffic efficiency\nworked  for  the  benefit  of  all”.  As  traffic  engineers\nchanged the timings of traffic signals to enable cars to\nflow freely, their so-called solution “helped to redefine\nstreets as motor thoroughfares where pedestrians did not\nbelong”. These actions by traffic engineers helped shape\nthe next several decades of automobile-focused urban\ndevelopment in US cities.\nAlthough these particular outcomes could be chalked\nup to unthoughtful design, any decisions that the traffic\nengineers  made  would  have  had  some  such  impact:\ndetermining how to time streetlights requires judgments\nabout what outcomes and whose interests to prioritize.\nWhatever they and the public may have believed, traffic\nengineers  were  never “just” engineers  optimizing\nsociety “for  the  benefit  of  all”.  Instead,  they  were\nengaged  in  the  process—via  formulas  and  signal\ntimings—of  defining  which  street  uses  should  be\nsupported and which should be constrained. The traffic\nengineers may not have decreed by law that streets were\nfor cars, but their technological intervention assured this\noutcome by other means.\nData  scientists  today  risk  repeating  this  pattern  of\ndesigning tools with inherently political characters yet\nlargely overlooking their own agency and responsibility.\nBy imagining an artificially limited role for themselves,\nengineers\n create\n an\n environment\n of\n scientific\ndevelopment  that  requires  few  moral  or  political\nresponsibilities. But this conception of engineering has\nalways  been  a  mirage.  Developing  any  technology\ncontributes to the particular “social contract implied by\nbuilding  that  technological  system  in  a  particular\nform”[31].\nOf  course,  we  must  also  resist  placing  too  much\nresponsibility on data scientists. The point is not that, if\nonly  they  recognized  their  social  impacts,  engineers\ncould themselves solve social issues. Technology is at\nbest just one tool among many for addressing complex\nsocial problems[37]. Nor should we uncritically accept\nthe  social  influence  that  data  scientists  have.  Having\nunelected  and  unaccountable  technical  experts  make\ncore decisions about governance away from the public\neye  imperils  essential  notions  of  how  a  democratic\nsociety ought to function. As Science, Technology, and\nSociety (STS) scholar Sheila Jasanoff argues, “The very\nmeaning  of  democracy  increasingly  hinges  on\nnegotiating the limits of the expert’s power in relation to\nthat of the publics served by technology”[38].\nNonetheless,  the  design  and  implementation  of\ntechnology  does  rely,  at  some  level,  on  trained\npractitioners. This raises several questions that animate\nthe rest of this article. What responsibilities should data\nscientists\n bear?\n How\n must\n data\n scientists\nreconceptualize their scientific and societal roles in light\nof these responsibilities?\n2.2    Argument 2: “Our job is not to take political\nstances”\nData scientists adhering to this second argument likely\naccept the response to Argument 1 but feel stuck, unsure\nhow to appropriately act as more than “just” an engineer.\n“Sure, I am developing tools that impact people’s lives”,\nthey may acknowledge, before asking, “But is not the\nbest thing to just be as neutral as possible?”\nAlthough  it  is  understandable  how  data  scientists\ncome to this position, their desire for neutrality suffers\nfrom  two  important  failings.  First,  neutrality  is  an\nunachievable  goal,  as  it  is  impossible  to  engage  in\nscience  or  politics  without  being  influenced  by  one’s\nbackground, values, and interests. Second, striving to be\nneutral is not itself a politically neutral position. Instead,\nit is a fundamentally conservative one.③\nAn ethos of objectivity has long been prevalent among\nscientists. Since the nineteenth century, objectivity has\nevolved into a set of widespread ethical and normative\nscientific  practices.  Conducting  good  science—and\nbeing a good scientist—meant suppressing one’s own\nperspective  so  that  it  would  not  contaminate  the\ninterpretations of observations[39].\nYet this conception of science was always rife with\ncontradictions and oversights. Knowledge is shaped and\nbounded by the social contexts that generated it. This\ninsight forms the backbone of standpoint theory, which\narticulates that “nothing in science can be protected from\ncultural  influence—not  its  methods,  its  research\ntechnologies,  its  conceptions  of  nature’s  fundamental\nordering  principles,  its  other  concepts,  metaphors,\n③ I use conservative here in the sense of maintaining the status quo\nrather than in relation to any specific political party or movement.\n    252\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nmodels,\n narrative\n structures,\n or\n even\n formal\nlanguages”[40].\n Although\n scientific\n standards\n of\nobjectivity  account  for  certain  kinds  of  individual\nsubjectivity, they are too narrowly construed: “methods\nfor maximizing objectivism have no way of detecting\nvalues,  interests,  discursive  resources,  and  ways  of\norganizing  the  production  of  knowledge  that  first\nconstitute  scientific  problems,  and  then  select  central\nconcepts,  hypotheses  to  be  tested,  and  research\ndesigns”[40].\nThese  processes  make  the  supposedly  objective\nscientific “gaze from nowhere” nothing more than “an\nillusion”[41]. Every aspect of science is imbued with the\ncharacteristics  and  interests  of  those  who  produce  it.\nThis  does  not  invalidate  every  scientific  finding  as\narbitrary,  but  points  to  science’s  contingency  and\nreliance on its practitioners: all research and engineering\nare developed within particular institutions and cultures\nand with particular problems and purposes in mind.\nJust as it is impossible to conduct science in any truly\nneutral  way,  there  is  no  such  thing  as  a  neutral  (or\napolitical) approach to politics. As philosopher Roberto\nUnger  writes,  political  neutrality  is  an “illusory  and\nultimately idolatrous goal” because “no set of practices\nand institutions can be neutral among conceptions of the\ngood”[42].\nInstead of being neutral and apolitical, attempts to be\nneutral and apolitical embody an implicitly conservative\npolitics. Because neutrality does not mean value-free—\nit means acquiescence to dominant social and political\nvalues, freezing the status quo in place. Neutrality may\nappear to be apolitical, but that is only because the status\nquo  is  taken  as  a  neutral  default.  Anything  that\nchallenges  the  status  quo—which  efforts  to  promote\nsocial justice must do by definition—will therefore be\nseen  as  political.  But  efforts  for  reform  are  no  more\npolitical than efforts to resist reform or even the choice\nsimply  to  not  act,  both  of  which  preserve  existing\nsystems.\nAlthough surely not the intent of every scientist and\nengineer  who  strives  for  neutrality,  broad  cultural\nconceptions  of  science  as  neutral  entrench  the\nperspectives of dominant social groups, who are the only\nones  entitled  to  legitimate  claims  of  neutrality.  For\nexample,  many  scholars  have  noted  that  neutrality  is\ndefined  by  a  masculine  perspective,  making  it\nimpossible  for  women  to  be  seen  as  objective  or  for\nneutral positions to consider female standpoints[40, 43−45].\nThe voices of Black women are particularly subjugated\nas  partisan  and  anecdotal[22].  Because  of  these\nperceptions,  when  people  from  marginalized  groups\ncritique scientific findings, they are cast off as irrational,\npolitical, and representing a particular perspective[41]. In\ncontrast, the practices of science and the perspectives of\nthe dominant groups that uphold it are rarely considered\nto suffer from the same maladies.\nData  science  exists  on  this  political  landscape.\nWhether articulated by their developers or not, machine\nlearning  systems  already  embed  political  stances.\nOverlooking  this  reality  merely  allows  these  political\njudgments to pass without scrutiny, in turn granting data\nscience systems with more credence and legitimacy than\nthey deserve.\nPredictive  policing  algorithms  offer  a  particularly\npointed  example  of  how  striving  to  remain  neutral\nentrenches and legitimize existing political conditions.\nThe  issue  is  not  simply  that  the  training  data  behind\npredictive policing algorithms are biased due to a history\nof  overenforcement  in  minority  neighborhoods.  In\naddition,  our  very  definitions  of  crime  and  how  to\naddress it are the product of racist and classist historical\nprocesses.  Dating  back  to  the  eras  of  slavery  and\nreconstruction, cultural associations of Black men with\ncriminality have justified extensive police forces with\nbroad powers[46]. The War on Drugs, often identified as\na significant cause of mass incarceration, emerged out\nof  an  explicit  agenda  by  the  Nixon  administration  to\ntarget people of color[47].④ Meanwhile, crimes like wage\ntheft are systemically underenforced by police and do\nnot  even  register  as  relevant  to  conversations  about\npredictive policing.⑤\nMoreover,  predictive  policing  rests  on  a  model  of\npolicing that is itself unjust. Predictive policing software\ncould exist only in a society that deploys vast punitive\nresources to prevent social disorder, following “broken\n④ As Nixon’s special counsel John Ehrlichman explained years later,\n“We knew we could not make it illegal to be either against the war or\nblack. But by getting the public to associate the hippies with marijuana\nand blacks with heroin, and then criminalizing both heavily, we could\ndisrupt those communities. We could arrest their leaders, raid their homes,\nbreak up their meetings, and vilify them night after night on the evening\nnews. Did we know we were lying about the drugs? Of course we did.”[48]\n⑤ Wage theft occurs when employers deny their employees the wages\nor benefits to which they are legally entitled (e.g., not paying employees\nfor overtime work). Wage theft steals more value than all other kinds of\ntheft  (such  as  burglaries)  combined,  typically  carried  out  by  business\nowners against low-income workers[49].\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n253    \n \n\nwindows” tactics.  Policing  has  always  been  far  from\nneutral: “the basic nature of the law and the police, since\nits earliest origins, is to be a tool for managing inequality\nand  maintaining  the  status  quo”[50].  The  issues  with\npolicing are not flaws of training or methods or “bad\napple” officers, but are endemic to policing itself[46, 50].\nAgainst this backdrop, choosing to develop predictive\npolicing algorithms is not neutral. Accepting common\ndefinitions of crime and how to address it may seem to\nallow data scientists to remove themselves from politics,\nbut instead upholds historical politics of social hierarchy.\nAlthough  predictive  policing  represents  a  notably\nsalient example of how data science cannot be neutral,\nthe same could be said of all applied data science. Biased\ndata  are  certainly  one  piece  of  the  story,  but  so  are\nexisting social and political conditions, definitions and\nclassifications  of  social  problems,  and  the  set  of\ninstitutions  that  respond  to  those  problems.  None  of\nthese factors are neutral and removed from politics. And\nwhile data scientists are of course not responsible for\ncreating these aspects of society, they are responsible for\nchoosing how to interact with them. Neutrality in the\nface  of  injustice  only  reinforces  that  injustice.  When\nengaging with aspects of the world steeped in history and\npolitics, in other words, it is impossible for data scientists\nto not take political stances.\nI  do  not  mean  to  suggest  that  every  data  scientist\nshould share a singular political vision—that would be\nwildly unrealistic. It is precisely because the field (and\nworld) hosts a diversity of normative perspectives that\nwe must surface political debates and recognize the role\nthey play in shaping data science practice. Nor is my\nargument  meant  to  suggest  that  articulating  one’s\npolitical commitments is a simple task. Normative ideals\ncan  be  complex  and  conflicting,  and  one’s  own\nprinciples can evolve over time. Data scientists need not\nhave  precise  answers  about  every  political  question.\nHowever, they must act in light of articulated principles\nand  grapple  with  the  uncertainty  that  surrounds  these\nideals.\n2.3    Argument 3: “We should not let the perfect be\nthe enemy of the good”\nFollowing  the  responses  to  Arguments  1  and  2,  data\nscientists\n asserting\n this\n third\n argument\n likely\nacknowledge that their creations will unavoidably have\nsocial impacts and that neutrality is not possible. Yet still\nholding  out  against  a  thorough  political  engagement,\nthey  fall  back  on  a  seemingly  pragmatic  position:\nbecause  data  science  tools  can  improve  society  in\nincremental but important ways, we should support their\ndevelopment  rather  than  argue  about  what  a  perfect\nsolution might be.\nDespite  being  the  most  sophisticated  of  the  three\narguments,\n this\n position\n suffers\n from\n several\nunderdeveloped  principles.  First,  data  science  lacks\nrobust  theories  regarding  what “perfect” and “good”\nactually entail. As a result, the field typically adopts a\nsuperficial  approach  to  reform  that  involves  making\nvague  (almost  tautological)  claims  about  what  social\nconditions are desirable. Second, this argument fails to\narticulate how to evaluate or navigate the relationship\nbetween  the  perfect  and  the  good.  Efforts  to  promote\nsocial good thus tend to take for granted that technology-\ncentric incremental reform is an appropriate strategy for\nsocial progress. Yet, considered from a perspective of\nsubstantive  equality  and  anti-oppression,  many  data\nscience efforts to do good are not, in fact, consistently\ndoing good.\n2.3.1    Data  science  lacks  a  thorough  definition  of\n“social good”\nAcross the broad world of data science, from academic\ninstitutes  to  conferences  to  companies  to  volunteer\norganizations, “social  good” (or  just “good”)  has\nbecome a popular term. Numerous universities across\nthe  United  States  and  Europe  have  hosted  the  Data\nScience for Social Good Summer Fellowship.⑥ Several\nmajor computer science conferences have hosted AI for\nSocial Good workshops,⑦ and in 2014 the theme of the\nentire  ACM  SIGKDD  Conference  on  Knowledge\nDiscovery and Data Mining (KDD) was “Data Mining\nfor  Social  Good”.⑧ Since  2014,  the  company\nBloomberg  has  hosted  an  annual  Data  for  Good\nExchange.⑨ The  non-profit  Delta  Analytics  strives  to\npromote “Data-driven solutions for social good”.⑩\nWhile this energy to do good among the data science\ncommunity is both commendable and exciting, the field\nhas not developed (nor even much debated) any working\ndefinitions of the term “social good” to guide its efforts.\nInstead, the field seems to operate on a “know it when\n⑥ http://www.dssgfellowship.org\n⑦ https: //aiforsocialgood.github.io/\n⑧ https: //www.kdd.org/kdd2014/\n⑨ https: //www.bloomberg.com/company/d4gx/\n⑩ http://www.deltanalytics.org\n    254\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nyou see it” approach, relying on rough proxies such as\ncrime = bad, poverty = bad, and so on. The term’s lack\nof precision prompted one of Delta Analytics’ founders\nto write that “‘data for good’ has become an arbitrary\nterm to the detriment of the goals of the movement”[51].\nThe notable exception is Mechanism Design for Social\nGood  (MD4SG),  which  articulates  a  clear  research\nagenda “to improve access to opportunity, especially for\ncommunities  of  individuals  for  whom  opportunities\nhave historically been limited”[52].\nIn  fact,  the  term “social  good” lacks  a  thorough\ndefinition even beyond the realm of data science. It is not\ndefined  in  dictionaries  like  Merriam-Webster,  the\nOxford  English  Dictionary,  and  Dictionary.com,  nor\ndoes it have a page on Wikipedia.⑪ To find a definition\none  must  look  to  the  financial  education  website\nInvestopedia, which defines social good as “something\nthat benefits the largest number of people in the largest\npossible way, such as clean air, clean water, healthcares,\nand literacy”[54]. There is, of course, extensive literature\n(spanning  philosophy,  STS,  and  other  fields)  that\nconsiders  what  is  socially  desirable,  yet  data  science\nefforts  to  promote “social  good” rarely  reference  this\nliterature.\nThis lack of definition leads to “data science for social\ngood” projects  that  span  a  wide  range  of  conflicting\npolitical orientations. For example, some work under the\n“social  good” umbrella  is  explicitly  developed  to\nenhance police accountability and promote non-punitive\nalternatives  to  incarceration[55, 56].  In  contrast,  other\nwork  under  the “social  good” label  aims  to  enhance\npolice operations. One such paper aimed to classify gang\ncrimes  in  Los  Angeles[30, 57].  This  project  involved\ntaking  for  granted  the  legitimacy  of  the  Los  Angeles\nPolice  Department’s  gang  data—a  notoriously  biased\ntype of data[58] from a police department that has a long\nhistory  of  abusing  minorities  in  the  name  of  gang\nsuppression[50].  That  such  politically  disparate  and\nconflicting  work  could  be  similarly  characterized  as\n“social  good” should  prompt  a  reconsideration  of  the\ncore terms and principles. When the term encompasses\neverything, it means nothing.\nThe  point  is  not  that  there  exists  a  single  optimal\ndefinition of “social good”, nor that every data scientist\nshould agree on one set of principles. Instead, there is a\nmultiplicity  of  perspectives  that  must  be  openly\nacknowledged  to  surface  debates  about  what “good”\nactually entails. Currently, however, the field lacks the\nlanguage  and  perspective  to  sufficiently  evaluate  and\ndebate differing visions of what is “good”. By framing\ntheir  notions  of “good” in  such  vague  and  undefined\nterms, data scientists get to have their cake and eat it too:\nthey can receive praise and publications based on broad\nclaims about solving social challenges, while avoiding\nsubstantive  engagement  with  social  and  political\nimpacts.\nMost  dangerously,  data  science’s  vague  framing  of\nsocial good allows those already in power to present their\nnormative judgments about what is “good” as neutral\nfacts  that  are  difficult  to  challenge.  As  discussed  in\nSection 2.2, neutrality is an impossible goal and attempts\nto be neutral tend to reinforce the status quo. Thus, if the\nfield does not openly debate definitions of “perfect” and\n“good”, the assumptions and values of dominant groups\nwill tend to win out. Projects that purport to enhance\nsocial  good  but  fail  to  reflexively  engage  with  the\npolitical context are likely to reproduce the exact forms\nof social oppression that many working towards “social\ngood” seek to dismantle.⑫\n2.3.2    Pursuing an incremental “good” can reinforce\noppression\nEven if data scientists acknowledge that “social good” is\noften  poorly  defined,  they  may  still  adhere  to  the\nargument that “we should not let the perfect be the enemy\nof the good”. “After all”, they might say, “is not some\nsolution, however imperfect, better than nothing?” As\none paper asserts, “we should not delay solutions over\nconcerns of optimal” outcomes[60].\nAt  this  point  the  second  failure  of  Argument  3\nbecomes clear: it tells us nothing about the relationship\nbetween the perfect and the good. Data science has thus\nfar  not  developed  any  rigorous  methodology  for\nconsidering  the  relationship  between  algorithmic\ninterventions  and  social  impacts.  Although  data\nscientists  generally  acknowledge  that  data  science\ncannot provide perfect solutions to social problems, the\nfield typically takes for granted that incremental reforms\nusing data science contribute to the “social good”. On\nthis logic, we should applaud any attempts to alleviate\nissues  such  as  crime,  poverty,  and  discrimination.\nMeanwhile,  because “the  perfect” represents  an\n⑪ Searching Wikipedia for “social good” automatically redirects to the\npage  for “common  good”,  a  term  similarly  undefined  in  data  science\nparlance[53].\n⑫ Reflexivity  refers  to  the  practice  of  treating  one’s  own  scientific\ninquiry as a subject of analysis[59].\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n255    \n \n\nunrealizable utopia we should not waste time and energy\ndebating the ideal solution.\nAlthough efforts to promote “social good” using data\nscience can be productive,⑬ pursuing such applications\nwithout a rigorous theory of social change can lead to\nharmful  consequences.  A  reform  that  seems  desirable\nfrom  a  narrow  perspective  focused  on  immediate\nimprovements  can  be  undesirable  from  a  broader\nperspective  focused  on  long-term,  structural  reforms.\nUnderstood in these terms, the dichotomy between the\nidealized “perfect” and the incremental “good” is a false\none: articulating visions of an ideal society is an essential\nstep for developing and evaluating incremental reforms.\nIn order to rigorously conceive of and compare potential\nincremental reforms, we must first debate and refine our\nconceptions of the society we want to create; following\nthose  ideals,  we  can  then  evaluate  whether  potential\nincremental reforms push society in the desired direction.\nBecause there is a multiplicity of imagined “perfects”,\nwhich  in  turn  suggest  an  even  larger  multiplicity  of\nincremental “goods”, reforms must be evaluated based\non what type of society they promote in both the short\nand long term. In other words, rather than treating any\nincremental  reform  as  desirable,  data  scientists  must\nrecognize  that  different  incremental  reforms  can  push\nsociety down drastically different paths.\nWhen attempting to achieve reform, an essential task\nis  to  evaluate  the  relationship  between  incremental\nchanges and long-term agendas for a more just society.\nAs social philosopher André Gorz proposes, we must\ndistinguish  between “reformist  reforms” and “non-\nreformist  reforms”[61].  Gorz  explains, “A  reformist\nreform is one which subordinates its objectives to the\ncriteria of rationality and practicability of a given system\nand  policy.” In  contrast,  a  non-reformist  reform “is\nconceived not in terms of what is possible within the\nframework of a given system and administration, but in\nview of what should be made possible in terms of human\nneeds and demands”.\nReformist  and  non-reformist  reforms  are  both\ncategories of incremental reform, but they are conceived\nthrough  distinct  processes.  Reformist  reformers  start\nwithin  existing  systems,  looking  for  ways  to  improve\nthem. In contrast, non-reformist reformers start beyond\nexisting  systems,  looking  for  ways  to  achieve\nemancipatory social conditions. Because of the distinct\nways that these two types of reforms are conceived, the\npursuit  of  one  versus  the  other  can  lead  to  widely\ndivergent social and political outcomes.\nThe solutions proposed by data scientists are almost\nentirely  reformist  reforms.  The  standard  logic  of  data\nscience—grounded  in  accuracy  and  efficiency—tends\ntoward accepting and working within the parameters of\nexisting  systems.  Data  science  interventions  are\ntherefore typically proposed to improve the performance\nof  a  system  rather  than  to  substantively  alter  it.  And\nwhile these types of reforms have value under certain\nconditions,  such  an  ethos  of  reformist  reforms  is\nunequipped to identify and pursue the larger changes that\nare necessary across many institutions. This approach\nmay even serve to entrench and legitimize the status quo.\nFrom the standpoint of existing systems, it is impossible\nto\n imagine\n alternative\n ways\n of\n structuring\nsociety—when reform is conceived in this way, “only\nthe most narrow parameters of change are possible and\nallowable”[62].\nIn  this  sense,  data  science’s  dominant  strategy  of\npursuing  a  reformist,  incremental  good  resembles  a\ngreedy algorithm: at every point in time, the strategy is\nto make immediate improvements in the local vicinity\nof  the  status  quo.  Although  a  greedy  strategy  can  be\nuseful for simple problems, it is unreliable in complex\nsearch spaces: we may quickly find a local maximum but\nwill  never  reach  a  further-afield  terrain  of  far  better\nsolutions. Moves that are immediately beneficial can be\ncounterproductive  for  finding  the  global  optimum.\nSimilarly, although reformist reforms can lead to certain\nimprovements, a strategy limited to reformist reforms\ncannot  guide  robust  responses  to  complex  political\nproblems.  Reforms  that  appear  desirable  within  the\nnarrow  scope  of  a  reformist  strategy  can  be\ncounterproductive  for  achieving  structural  reforms.\nEven  though  the  optimal  political  solution  is  rarely\nachievable (and is often subject to significant debate), it\nis necessary to fully characterize the space of possible\nreforms  and  to  evaluate  how  reliably  different\napproaches can generate more egalitarian outcomes.\nThe US criminal justice system, a domain where data\nscientists  are  increasingly  striving  to  do  good,\nexemplifies the limits of a reformist mindset. Because\ncriminal  justice  reform  can  be “superficial  and\ndeceptive”[63],  it  is  necessary  to  couch  reform  efforts\n⑬ See e.g., the set of projects completed by the Data Science for Social\nGood Fellowship: http://www.dssgfellowship.org/projects/.\n    256\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nwithin  a  broader  vision  of  long-term,  non-reformist\nchange. This is the approach taken by the movement for\npolice and prison abolition. Notably, prison abolitionists\nobject\n to\n reforms\n that \n“render\n criminal\n law\nadministration  more  humane,  but  fail  to  substitute\nalternative  institutions  or  approaches  to  realize  social\norder  maintenance  goals”[64].  Instead,  abolitionists\npursue  only  reforms  that  reduce  or  replace  carceral\nresponses to social disorder.\nIn  contrast  with  this  abolitionist  ethos,  most  data\nscience efforts to contribute “good” are grounded in the\nexisting  practices  of  the  criminal  justice  system.  A\nnotable  example  is  pretrial  risk  assessments.  Even  if\nthey  lead  to  incremental  improvements,  these  tools\nlegitimize policies that drive racial injustice and mass\nincarceration[65].  Meanwhile,  an  entirely  separate\nincremental reform—an abolitionist and non-reformist\n(and non-technological) one—is possible: ending cash\nbail and pretrial detention. Recent surveys show public\nsupport for such reforms[66, 67].\nAdopting  pretrial  risk  assessments  and  abolishing\npretrial  detention  appear  to  respond  to  the  same\nproblems, suggesting that these two reforms are aligned.\nHowever, these reforms derive from conflicting visions\nof the “perfect”. Reformers supporting risk assessments\naccept  pretrial  detention  as  part  of  criminal  justice\nsystem, aiming merely to improve the means by which\npeople  are  selected  for  pretrial  detention.  Meanwhile,\nreformers aiming to abolishing pretrial detention reject\npretrial  detention,  aiming  to  abolish  the  practice\naltogether.  In  other  words,  the  debate  about  risk\nassessments hinges on political questions about how the\ncriminal justice system should be structured. It is only\nby articulating our imagined perfects that we can even\nrecognize  the  underlying  tension  between  these  two\nincremental  reforms,  let  alone  properly  debate  which\none to pursue.\nThe  point  is  not  that  data  science  is  incapable  of\nimproving society. However, data science interventions\nmust be evaluated against alternative reforms as just one\nof many options, rather than compared merely against\nthe status quo as the only possible reform. There should\nnot  a  default  presumption  that  machine  learning\nprovides an appropriate reform for every problem.\nIn sum, attempts by data scientists to avoid politics\noverlook  technology’s  social  impacts,  privilege  the\nstatus quo, and narrow the range of possible reforms. The\nfield  of  data  science  will  be  unable  to  meaningfully\nadvance  social  justice  without  accepting  itself  as\npolitical. The question that remains is how it can do so.\n3    How  Can  Data  Scientists  Ground  Their\nPractice in Politics?\nThe first part of this article argued that data scientists\nmust  recognize  themselves  as  political  actors.  Yet\nseveral questions remain: What would it look like for\ndata science to be explicitly grounded in a politics of\nsocial justice? How might the field evolve toward this\nend?\nI conceptualize the process of incorporating politics\ninto data science as following four stages, with reforms\nat both the individual and the institutional/cultural levels.\nStage  1  (Interest)  involves  data  scientists  becoming\ninterested in working directly on addressing social issues.\nIn Stage 2 (Reflection), the data scientists involved in\nthat work come to recognize the politics that underlie\nthese issues and their attempts to address them.⑭ This\nleads to Stage 3 (Applications), in which data scientists\ndirect the methods at their disposal toward new problems.\nFinally, Stage 4 (Practice) involves the long-term project\nof  developing  new  methods  and  structures  that  orient\ndata science around a politics of social justice.\nI discuss each stage in more detail below. While not\nevery person or project will follow this precise trajectory,\nit  presents  a  possible  path  for  data  scientists  to\nincorporate politics into their practice. In fact, many data\nscientists already are following some version of these\nstages toward a politically informed data science.\n3.1    Stage 1: Interest\nThe first step toward infusing a deliberate politics into\ndata science is for data scientists to orient their work\naround addressing social issues. Such efforts are already\nwell underway, from “data for good” programs to civic\ntechnology  groups  to  the  growing  numbers  of  data\nscientists  working  in  governments  and  non-profits.\nAlthough  they  may  not  have  an  articulated  vision  of\n“social good”, many data scientists are eager to apply\n⑭ Some might argue that the order of Stages 1 and 2 should be reversed:\ndata scientists should reflect first, then act to address social issues. This\nwould  be  the  most  responsible  approach  and  is  the  practice  that  data\nscientists should follow in the long term. In my experience, however, data\nscientists’ engagements  with  politics  tend  to  begin  with  an  interest  in\naddressing social challenges, which then leads to reflection on the politics\nof  data  science.  New  pedagogical  approaches  could  merge  these  two\nstages.  For  instance,  a “public  interest  tech” program  could  integrate\nreflection on the political nature of data science into its efforts to apply\ndata science in practice.\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n257    \n \n\ntheir work to pressing societal challenges.\nHowever, relative to the excitement around such work,\nthere is a dearth of opportunities for data scientists to\napply their skills to an articulated vision of social benefit.\nMany academic departments and conferences tend not to\nconsider such work to be valid research, companies can\nfind more profit elsewhere, and governments and non-\nprofits have few internal data science roles. Thus, many\ndata scientists who want to do socially impactful work\noften settle for more traditional research or jobs, in which\ntechnical contributions and profit provide the primary\nimperatives.\nData science programs should work towards a model\nof “public interest technology” that trains data scientists\nto  address  social  issues.  This  involves  not  simply\nadopting  this  label,  but  also  providing  methods,\npathways,  and  a  broader  culture  of  support  for  data\nscientists to improve society. For example, data science\nprograms should develop clinics where students provide\ntechnical  and  policy  assistance  to “clients” such  as\nactivists  and  government  agencies.  Programs  should\nalso provide funding and guidance for students to find\ninternships and jobs focused on social impact.⑮\nIt is essential that “social good” and “public interest\ntech” programs  prioritize  social  and  political  reforms\nover deploying technology. The driving goal should be\nto  positively  impact  society  rather  than  to  develop\nsophisticated  tools.  This  requires  an  attitude  of\nagnosticism: “approaching  algorithms  instrumentally,\nrecognizing them as just one type of intervention, one\nthat cannot provide the solution to every problem”[68].\nThe  more  that  data  scientists  work  directly  with\ngovernments,  communities,  and  service  providers\n(rather than on abstract technology problems), the more\nthoroughly  they  will  come  to  see  technology  as  an\nimperfect means rather than as an end in itself. Without\nthis technology-agnostic focus on social impacts, efforts\nto apply data science to social problems will reproduce\nthe  issues  described  in  Section  2.3  and  will  prevent\nprogression to the following stages.\n3.2    Stage 2: Reflection\nAs they work on data science for social good projects,\ndata scientists will encounter the political nature of both\nthe issues at hand and their own efforts to address these\nissues. To the extent that they maintain an open-minded\nand critical approach grounded in impact, data scientists\nwill begin to reflect on political questions.\nWe have seen this process play out most clearly with\nrespect  to  algorithmic  bias  and  fairness.  Where  just\na  few  years  ago  it  was  common  to  hear  claims\nthat  data represents “facts” and  that  algorithms  are\n“objective”[69, 70], today  it  is  widely  acknowledged\nwithin  data  science  that  data  contains  biases  and  that\nalgorithms can discriminate. In addition to the annual\nACM  Conference  on  Fairness,  Accountability,  and\nTransparency  (FAccT),  there  have  been  numerous\nworkshops dedicated to these issues at major computer\nscience  conferences[71].  Moreover,  there  is  also  an\nemerging  literature  that  articulates  the  limitations  and\npolitics  of  common  approaches  to  studying  and\npromoting algorithmic fairness[72−74].\nOver time, data scientists must expand this critical and\nreflexive lens to increasingly interrogate how all aspects\nof their work are political. For example, returning to the\ndiscussion of predictive policing from Section 2.2, it is\nnot  sufficient  to  develop  algorithms  just  with  a\nrecognition that crime data are biased. It is necessary to\nalso recognize that our definitions of crime, the set of\ninstitutions that are tasked with responding to it, and the\ninterventions that those institutions provide are all the\nresult  of  historical  political  processes  laden  with\ndiscrimination.\nReflection  of  this  sort  is  propelled  by  approaching\nresearch with an open mind and honoring the expertise\nof  other  disciplines,  policymakers,  and  affected\ncommunities.  Such  reflection  will  be  particularly\nenhanced by fluency in fields such as STS and critical\nalgorithm  studies.  Exposure  to  these  fields  should\nbecome  central  to  data  science  training  programs,\nparticularly those with an emphasis on applications of\ndata science for social good. For data scientists hoping\nto  improve  society,  familiarity  with  STS  and  related\nfields is just as essential as knowledge of databases and\nstatistics.\n3.3    Stage 3: Applications\nIn the short term, the insights provided in Stage 2 are not\nlikely to shake the fundamental structures and practices\nof data science. Instead, these insights will empower data\nscientists to seek new applications for how existing data\nscience methods can address injustice and shift power.\n⑮ See e.g., a list of job boards and other resources that I have compiled:\nhttps: //www.benzevgreen.com/jobs/.\n    258\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nThese  effects  will  demonstrate  how  incorporating  a\npolitical  perspective  into  data  science  produces  new\ndirections  for  research  and  applications  rather  than  a\ndead end.\nSeveral frameworks can guide data scientists in these\nefforts.  For  example,  André  Gorz’s  schema  of  non-\nreformist reforms and the framework of prison abolition\nprovide conceptual tools for moving beyond the false\ndichotomy\n between\n incremental\n and\n radical\nreform[61, 64]. The notion of “critical design” embodies\na similar approach: in contrast to “affirmative design,\nwhich “reinforces how things are now”, “critical design\nprovides  a  critique  of  how  things  are  now  through\ndesigns  that  embody  alternative  social,  cultural,\ntechnical, or economic values”[75]. A related framework\nis “anti-oppressive design”, which provides “a guide for\nhow  best  to  expend  resources,  be  it  the  choice  of  a\nresearch topic, the focus of a new social enterprise, or the\nselection of clients and projects, rather than relying on\nvague  intentions  or  received  wisdom  about  what\nconstitutes good”[76].\nAt each stage of the research and design process, data\nscientists should evaluate their efforts according to these\nframeworks:  Should  the  design  of  this  algorithm  be\naffirmative or critical? Would the implementation of this\nmodel  represent  a  reformist  or  non-reformist  reform?\nWould empowering our project partner with this system\nchallenge  or  entrench  oppression?  Such  analyses  can\nhelp data scientists interrogate their notions of “good” to\nengage  in  non-reformist,  critical,  and  anti-oppressive\ndata  science.  These  approaches  can  also  help  data\nscientists\n recognize\n situations\n in\n which\n non-\ntechnological  reforms  are  more  desirable  than\ntechnological ones[37, 77].\nThis ethos of pursuing different, politically motivated\ndata science applications can inform work in areas such\nas  policing.  One  dimension  of  this  shift  involves  a\ncritical and anti-oppressive approach to selecting project\npartners.  For  example,  some  researchers  explicitly\narticulate an intention to work with community groups\nand  social  service  providers  rather  than  with  law\nenforcement,  recognizing  that  the  latter  tend  to\ncontribute  to  structural  oppression[55, 78, 79].  Another\ndimension of this shift involves orienting the analytic\ngaze  away  from  individuals  and  towards  institutions.\nOne  example  of  this  work  used  machine  learning  to\npredict which police officers will be involved in adverse\nevents such as racial profiling or inappropriate use of\nforce[56]. Others have used new algorithmic methods to\nfind evidence of racial bias in police behavior[80, 81].\nAlthough Stage 3 represents a significant evolution of\ndata science toward politics, it suffers from three notable\nshortcomings. First, it is possible to operate in Stage 3\nwithout ever articulating an explicit politics. Although\nnot raising a project’s political motivations may enable\nsome projects to pass without scrutiny, it does little to\nprovide language or direction for other data scientists.\nThe  field  will  not  evolve  if  political  debates  remain\nshrouded.  Moreover,  only  relatively  minor  reforms\ncould be successfully promoted in this covert manner:\nmore significant reforms will likely be challenged and\nwill advance only if they can be explicitly defended.\nSecond, existing data science methods have a limited\nability to promote social justice. Because of data science’s\nadherence to mathematical formalism, current methods\nare incapable of rigorously representing and reasoning\nabout  social  contexts  and  political  impacts[68].  Thus,\neven well-intentioned and seemingly well-designed data\nscience tools can promote injustice[74].\nThird,  merely  directing  data  science  toward  new\napplications  remains  fundamentally  undemocratic:  it\nallows  data  scientists  to  shape  society  without\ndeliberation or accountability. In this frame, a cadre of\ndata\n scientists—no\n matter\n their\n intentions\n or\nactions—retain an outsized power to shape institutions\nand decision-making processes. Even when their actions\nare grounded in anti-oppressive ideals, the efforts of data\nscientists  can  serve  coercive  functions  if  they  are  not\ngrounded in the needs and desires of the communities\nsupposedly being served. In order to promote long-term\nstructural change and social justice, larger shifts in data\nscience practice are necessary.\n3.4    Stage 4: Practice\nThe  final  stage  is  to  develop  new  modes  for  what  it\nmeans to practice data science. Achieving changes along\nthese  lines  requires  developing  new  epistemologies,\nmethodologies, and cultures for data science. While the\npath ahead remains somewhat speculative, several broad\ndirections are clear.\n3.4.1    Participatory data science\nData scientists must abandon their desire for a removed\nobjectivity  in  favor  of  participation  and  deliberation\namong  diverse  perspectives.  STS  scholar  Donna\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n259    \n \n\nHaraway  argues  for  a  new  approach  centered  on\n“situated knowledges”: she articulates the need “for a\ndoctrine  and  practice  of  objectivity  that  privileges\ncontestation  and  deconstruction”,  one  that  recognizes\nthat  every  claim  emerges  from  the  perspective  of  a\nparticular person or group of people[41]. Following this\nlogic,  the “neutral” data  scientist  who  attempts  to\nminimize  position-taking  must  be  replaced  by  a  data\nscience\n of\n situated\n values—a \n“participatory\ncounterculture  of  data  science”[82].  This  perspective\nhighlights  the  importance  of  groups  such  as  Black  in\nAI,⑯ LatinX  in  AI,⑰ Queer  in  AI,⑱ and  Women  in\nMachine Learning,⑲ all of which work to increase the\npresence  of  underrepresented  groups  in  the  field  of\nartificial  intelligence.  Given  that  data  science  is\ninfluenced by practitioners’ perceptions of problems and\nof  how  to  address  them,  it  is  essential  to  encourage\ngreater diversity in data science[83].\nComplementing this participatory approach is for data\nscience to focus more directly on “designing with” rather\nthan “designing  for” affected  communities  and  social\nmovements. Data scientists must develop procedures for\nincorporating  a  multitude  of  public  voices  into  their\nwork. When engineers privilege their own perspectives\nand fail to consider the multiplicity of needs and values\nacross society, they tend to erase and subjugate those\nwho\n are\n already\n marginalized[84−90].\n To\n avoid\nparticipating  in  these  oppressive  (even  if  inadvertent)\nacts, data scientists must center affected communities in\ntheir work. One approach toward this end is the principle\nof “Nothing  about  us  without  us”,  which  has  been\ninvoked in numerous social movements (in particular,\namong disability rights activists in the 1990s) to signify\nthat  no  policies  should  be  developed  without  direct\nparticipation from the people most directly affected by\nthose  policies[91].  The  Design  Justice  Network\narticulates a powerful enactment of these values, with its\ncommitments  to “center  the  voices  of  those  who  are\ndirectly  impacted” and  to “look  for  what  is  already\nworking at the community level”[92].\nThis type of approach represents a notable departure\nfrom\n traditional\n data\n science\n practice\n and\nvalues—efficiency\n and\n convenience—toward\ndemocracy and empowerment. A great deal of work in\nrecent  years  has  exemplified  this  approach[79, 93−100].\nMechanisms  for  participatory  design  and  decision\nmaking—such  as  charrettes,  participatory  budgeting,\nand co-production—present further models of designing\nwith  communities.  Any  participatory  practices  should\nentail not just the design of an algorithm, but also broader\nquestions  such  as  whether  an  algorithm  should  be\ndeveloped in the first place and how it should be used.\nAdditionally,  an  essential  component  of  developing  a\nmore democratic data science is to bring data scientists,\ntechnology  companies,  and  governments  within  the\nambit of democratic oversight and accountability[101].\n3.4.2    New methods and cultures\nAdapting data science to a political orientation and to\nparticipatory  practices  will  require  new  methods.\nBroadly  speaking,  data  science  must  move  toward  a\n“critical  technical  practice” that  rejects “the  false\nprecision of mathematical formalism” to engage with the\npolitical world in its full complexity and ambiguity[102].\nIt  is  necessary  to  expand  the  bounds  of  algorithmic\nreasoning,  shifting  from  the  dominant  method  of\n“algorithmic  formalism” to  the  alternative  method  of\n“algorithmic  realism” that  better  accounts  for  the\nrealities  of  social  life  and  the  impacts  of  algorithmic\ninterventions[68].\nAs  a  central  component  of  this  evolution,  the  field\nshould  change  its  internal  structures  to  incentivize\ngreater attention to the implementation and impacts of\ndata  science.  To  embrace  justice  and  tackle  the  most\npressing social issues related to algorithms, data science\nmust  take  a  more  expansive  approach  to  research\ncontributions  that  looks  for  more  than  technical\ncontributions.  Actually  improving  people’s  lives  with\ndata  science  requires  far  more  than  just  developing  a\ntechnical  tool—it  also  requires  thoughtfully  adapting\ndata  science  methods  to  the  needs  of  a  particular\norganization or community[37]. If data scientists are to\ncontribute  to  improving  society,  they  need  a  more\nrigorous  methodology  for  ensuring  that  data  science\ntools produce beneficial impacts when implemented in\nreal-world contexts. New workshops, conferences, and\njournals will be essential mechanisms for fostering novel\nmethods  that  blend  technical  and  nontechnical\napproaches.\nAlong  these  lines,  data  scientists  must  also  adopt  a\nreflexive political standpoint that grounds their efforts in\n⑯ https: //blackinai.github.io/\n⑰ http://www.latinxinai.org/\n⑱ https: //sites.google.com/view/queer-in-ai/\n⑲ https: //wimlworkshop.org\n    260\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nrigorous evaluations of downstream social and political\nconsequences.  What  ultimately  matters  is  not  how  an\nalgorithm performs in the abstract, but what impacts an\nalgorithm\n has\n when\n introduced\n into\n complex\nsociopolitical  environments.  Data  scientists  cannot  be\nexpected to perfectly predict the impacts of their work—\nthe entanglements between technology and society are\nfar too complex. However, through collaborations with\ncommunities and with scholars from other fields, well-\ngrounded analyses are possible. Just as data scientists\nwould  demand  rigor  in  claims  that  one  algorithm  is\nsuperior to another, they should also demand rigor in\nclaims that a technology will have any particular impacts.\nToward  this  end,  one  necessary  direction  for  future\nresearch is to develop interdisciplinary frameworks that\nwill  help  data  scientists  consider  the  downstream\nimpacts  of  their  interventions.  This  requires  being\nmindful  of  the  various  forms  of “indeterminacy” that\nmay lead an algorithm to generate different impacts than\nits developers expect[68].\nAs one example of a reform that emphasizes impacts\nas  a  central  concern,  in  2018  the  ACM  Future  of\nComputing  Academy  proposed  that  peer  reviewers\nshould  consider  the  potential  negative  implications  of\nsubmitted  work  and  that  conducting “anti-social\nresearch” should factor negatively into promotion and\ntenure  cases[103].  Just  two  years  later,  the  Neural\nInformation\n Processing\n Systems\n Conference\n(NeurIPS)—one\n of\n the\n world’s\n top\n AI\nconferences—announced that every paper at the 2020\nconference must include a “broader impact” section that\ndiscusses the positive and negative social consequences\nof the research[104].\n3.4.3    Engaging with the broader political context\nOf course, shifts in data science practice do not occur in\na vacuum. Shifts in data science practice require broader\nstructural reforms that contribute to a more just society.\nAs historian Elizabeth Fee notes, “we can expect a sexist\nsociety  to  develop  a  sexist  science;  equally,  we  can\nexpect  a  feminist  society  to  develop  a  feminist\nscience”[105].  Similarly,  we  can  expect  a  militarized\nsociety of economic inequality to produce a militarized\nand unequal data science[106, 107].\nData scientists committed to social justice must work\ntoward  more  structural  reforms  against  the  harms  of\ndigital  technologies.  For  instance,  building  solidarity\nand power among workers can shift the development of\ndata science away from the most harmful applications.\nIn  recent  years,  tech  workers  have  organized  against\ntheir  companies’ partnerships  with  the  United  States\nDepartments of Defense and Homeland Security. Rather\nthan perceiving themselves as “just an engineer”, these\ntechnologists  recognize  their  position  within  larger\nsociotechnical  systems,  recognize  the  connection\nbetween their work and its social ramifications, and hold\nthemselves (and their companies) accountable for these\nimpacts.  Building  on  this  movement,  thousands  of\ncomputer science students from more than a dozen US\nuniversities pledged in 2019 that they will not work for\nPalantir  due  to  its  partnerships  with  Immigration  and\nCustoms Enforcement (ICE)[108]. Data scientists should\nalso  provide  support  for  communities  and  activists\norganizing in opposition to oppressive algorithms.\nData scientists alone cannot be held responsible for\npromoting social and political progress. They are just\none set of actors among many. The task of data scientists\nis not to eradicate social challenges on their own, but to\nact  as  thoughtful  and  productive  partners  in  broad\ncoalitions and social movements striving for a more just\nsociety.\n4    Conclusion\nThe  field  of  data  science  must  abandon  its  self-\nconception of being neutral to recognize how, despite\nnot being engaged in what is typically seen as political\nactivity, data science logics, methods, and technologies\nshape society. Restructuring the values and practices of\ndata science around a political vision of social justice\nwill not be easy or immediate, but it is necessary. Given\nthe political stakes of algorithms, it is not enough to have\ngood  intentions—data  scientists  must  ground  their\nefforts  in  clear  political  commitments  and  rigorous\nevaluations of the consequences.\nAs  a  form  of  political  action,  data  science  can  no\nlonger  be  separated  from  broader  analyses  of  social\nstructures,  public  policies,  and  social  movements.\nInstead, the field must debate what impacts are desirable\nand how to promote those outcomes—thus prompting\nrigorous evaluations of the issues at hand and openness\nto the possibility of non-technological alternatives. Such\ndeliberation needs to occur not just among data scientists,\nbut also with scholars from other fields, policymakers,\nand communities affected by data science systems.\nRecognizing data science as a form of political action\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n261    \n \n\nwill  empower  and  enlighten  data  scientists  with  new\nframeworks to improve society. By deliberating about\npolitical  goals  and  strategies  and  by  developing  new\nmethods and norms, data scientists can more rigorously\ncontribute to social justice.\nAcknowledgment\nB. Green is grateful to the Berkman Klein Center Ethical\nTech  Working  Group  for  fostering  his  thinking  on\nmatters of technology, ethics, and politics. B. Green also\nthanks  Catherine  D’Ignazio,  Anna  Lauren  Hoffmann,\nLily  Hu,  Momin  Malik,  Dan  McQuillan,  Luke  Stark,\nSalomé Viljoen, and the reviewers for providing helpful\ndiscussions and suggestions.\nReferences\n C. O' Neil, Weapons of Math Destruction: How Big Data\nIncreases  Inequality  and  Threatens  Democracy.  New\nYork, NY, USA: Broadway Books, 2017.\n[1]\n J. Angwin, J. Larson, S. Mattu, and L. Kirchner, Machine\nbias, \nhttps://www.propublica.org/article/machine-bias-\nrisk-assessments-in-criminal-sentencing, 2016.\n[2]\n V.  Eubanks, Automating  Inequality:  How  High-Tech\nTools  Profile,  Police,  and  Punish  the Poor.  New  York,\nNY, USA: St. Martin’s Press, 2018.\n[3]\n R.  Wexler,  Life,  liberty,  and  trade  secrets:  Intellectual\nproperty  in  the  criminal  justice  system, Stanford  Law\nReview, vol. 70, no. 5, pp. 1343–1429, 2018.\n[4]\n Z.  Obermeyer,  B.  Powers,  C.  Vogeli,  and  S.\nMullainathan, Dissecting racial bias in an algorithm used\nto  manage  the  health  of  populations, Science,  vol. 366,\nno. 6464, pp. 447–453, 2019.\n[5]\n J.\n Buolamwini\n and\n T.\n Gebru,\n Gender\n shades:\nIntersectional accuracy disparities in commercial gender\nclassification, Proceedings  of  the  1st  Conference  on\nFairness, Accountability  and  Transparency,  vol. 81,\npp. 77–91, 2018.\n[6]\n A.  D.  I.  Kramer,  J.  E.  Guillory,  and  J.  T.  Hancock,\nExperimental  evidence  of  massive-scale  emotional\ncontagion  through  social  networks, Proceedings  of  the\nNational\n Academy\n of\n Sciences,\n vol. 111,\n no. 24,\npp. 8788–8790, 2014.\n[7]\n S. Vosoughi, D. Roy, and S. Aral, The spread of true and\nfalse\n news\n online, \nScience,\n vol. 359,\n no. 6380,\npp. 1146–1151, 2018.\n[8]\n J.  Nicas,  How  YouTube  drives  people  to  the  internet’s\ndarkest\n corners, \nhttps://www.wsj.com/articles/how-\nyoutube-drives-viewers-to-the-internets-darkest-corners-\n1518020478, 2018.\n[9]\n M.  Rosenberg,  N.  Confessore,  and  C.  Cadwalladr,  How\nTrump consultants exploited the facebook data of millions,\nhttps://www.nytimes.com/2018/03/17/us/politics/\ncambridge-analytica-trump-campaign.html, 2018.\n[10]\n M. Kosinski, D. Stillwell, and T. Graepel, Private traits\nand  attributes  are  predictable  from  digital  records  of\n[11]\nhuman behavior, Proceedings of the National Academy of\nSciences  of  the  United  States  of  America,  vol. 110,\nno. 15, pp. 5802–5805, 2013.\n Y. -A. de Montjoye, L. Radaelli, V. K. Singh, and A. S.\nPentland,  Unique  in  the  shopping  mall:  On  the\nreidentifiability  of  credit  card  metadata, Science,\nvol. 347, no. 6221, pp. 536–539, 2015.\n[12]\n S. A. Thompson and C. Warzel, How to track president\ntrump, https://www.nytimes.com/interactive/2019/12/20/\nopinion/location-data-national-security.html, 2019.\n[13]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach to technology ethics in practice, Journal of Social\nComputing, doi: 10.23919/JSC.2021.0018\n[14]\n W.  L.  Wang,  Computer  science,  philosophy  join  forces\non  ethics  and  technology, https://www.thecrimson.com/\narticle/2017/11/7/cs-philosophy-collab/, 2017.\n[15]\n N. Singer, Tech’s ethical ‘Dark Side’: Harvard, Stanford\nand  others  want  to  address  it, https://www.nytimes.\ncom/2018/02/12/business/computer-science-ethics-\ncourses.html, 2018.\n[16]\n B. J. Grosz, D. G. Grant, K. Vredenburgh, J. Behrends, L.\nHu,  A.  Simmons,  and  J.  Waldo,  Embedded  EthiCS:\nIntegrating ethics across CS education, Communications\nof the ACM, vol. 62, no. 8, pp. 54–61, 2019.\n[17]\n C. Fiesler, N. Garrett, and N. Beard, What do we teach\nwhen we teach tech ethics? A syllabi analysis, in Proc.\nthe  51st  ACM  Technical  Symposium  on  Computer\nScience  Education  (SIGCSE’20),  Portland,  OR,  USA,\n2020, pp. 289–295.\n[18]\n C. Fiesler, Tech ethics curricula: A collection of syllabi,\nhttps://medium.com/@cfiesler/tech-ethics-curricula-a-\ncollection-of-syllabi-3eedfb76be18, 2018.\n[19]\n D.  J.  Patil,  A  code  of  ethics  for  data  science,\nhttps://medium.com/@dpatil/a-code-of-ethics-for-data-\nscience-cda27d1fac1, 2018.\n[20]\n Association  for  Computing  Machinery,  ACM  code  of\nethics  and  professional  conduct, https://www.acm.org/\ncode-of-ethics, 2018.\n[21]\n P.  H.  Collins, Black  Feminist  Thought:  Knowledge,\nConsciousness,  and  the  Politics  of  Empowerment. New\nYork, NY, USA: Routledge, 2000.\n[22]\n A.  Leftwich,  Politics:  People,  resources,  and  power,  in\nWhat is Politics? The Activity and its Study, A. Leftwich,\ned. Oxford, UK: Basil Blackwell, 1984, pp. 62–84.\n[23]\n H.  D.  Lasswell, Politics:  Who  Gets  What,  When,  How.\nNew York, NY, USA: Whittlesey House, 1936.\n[24]\n N.  Seaver,  Algorithms  as  culture:  Some  tactics  for  the\nethnography of algorithmic systems, Big Data & Society,\nvol. 4, no. 2, p. 205395171773810, 2017.\n[25]\n G. Neff, A. Tanweer, B. Fiore-Gartland, and L. Osburn,\nCritique and contribute: A practice-based framework for\nimproving  critical  data  studies  and  data  science, Big\nData, vol. 5, no. 2, pp. 85–97, 2017.\n[26]\n G.  C.  Bowker,  S.  L.  Star,  W.  Turner,  and  L.  Gasser,\nSocial  Science,  Technical  Systems,  and  Cooperative\nWork:  Beyond  the  Great  Divide.  London,  UK:\nPsychology Press, 1997.\n[27]\n J.  Vincent,  Drones  taught  to  spot  violent  behavior  in\ncrowds\n using\n AI, \nhttps://www.theverge.com/2018/\n6/6/17433482/ai-automated-surveillance-drones-spot-\n[28]\n    262\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\nviolent-behavior-crowds, 2018.\n D. Adjodah, AISG Panel at NeurIPS 2019. We have a lot\nto\n learn, \nhttps://medium.com/@_dval_/aisg-panel-at-\nneurips-2019-we-have-a-lot-to-learn-b69b573bd5af,\n2019.\n[29]\n M.  Hutson,  Artificial  intelligence  could  identify  gang\ncrimes—and\n ignite\n an\n ethical\n firestorm,\nhttps://www.sciencemag.org/news/2018/02/artificial-\nintelligence-could-identify-gang-crimes-and-ignite-\nethical-firestorm, 2018.\n[30]\n L.  Winner, The  Whale  And  the  Reactor:  A  Search  For\nLimits in An Age of High Technology. Chicago, IL, USA:\nUniversity of Chicago Press, 1986.\n[31]\n S.  Jasanoff,  In  a  constitutional  moment:  Science  and\nsocial  order  at  the  millennium,  in Social  Studies  of\nScience  and  Technology:  Looking  Back,  Ahead,  B.\nJoerges and H. Nowotny, eds. Dordrecht, the Netherland:\nSpringer, 2003, p. 155–180.\n[32]\n B.  Latour,  Give  me  a  laboratory  and  I  will  raise  the\nworld,  in Science  Observed: Perspectives  on  the  Social\nStudy of Science, K. Knorr-Cetina and M. J. Mulkay, eds.\nLondon, UK: Sage, 1983, pp. 141–170.\n[33]\n B. Joerges, Do politics have artefacts? Social Studies of\nScience, vol. 29, no. 3, pp. 411–431, 1999.\n[34]\n S.  Woolgar  and  G.  Cooper,  Do  artefacts  have\nambivalence: Moses’ bridges, Winner’s bridges and other\nurban  legends  in  S&TS, Social  Studies  of  Science,\nvol. 29, no. 3, pp. 433–449, 1999.\n[35]\n P. D. Norton, Fighting Traffic: The Dawn of the Motor\nAge in the American City. Cambridge, MA, USA: MIT\nPress, 2011.\n[36]\n B. Green, The Smart Enough City: Putting Technology in\nIts Place to Reclaim Our Urban Future. Cambridge, MA,\nUSA: MIT Press, 2019.\n[37]\n S. Jasanoff, Technology as a site and object of politics, in\nThe  Oxford  Handbook  of  Contextual  Political  Analysis,\nR.  E.  Goodin  and  C.  Tilly,  eds.  Oxford,  UK:  Oxford\nUniversity Press, 2006, pp. 745–763.\n[38]\n L.  Daston  and  P.  Galison, Objectivity.  New  York,  NY,\nUSA: Zone Books, 2007.\n[39]\n S. Harding, Is Science Multicultural?: Postcolonialisms,\nFeminisms, and Epistemologies. Bloomington, IN, USA:\nIndiana University Press, 1998.\n[40]\n D. Haraway, Situated knowledges: The science question\nin  feminism  and  the  privilege  of  partial  perspective,\nFeminist Studies, vol. 14, no. 3, pp. 575–599, 1988.\n[41]\n R. M. Unger, False Necessity: Anti-Necessitarian Social\nTheory in the Service of Radical Democracy. Cambridge,\nUK: Cambridge University Press, 1987.\n[42]\n G. Lloyd, Maleness, metaphor, and the “crisis” of reason,\nin A Mind of One’s Own: Feminist Essays on Reason and\nObjectivity, L. M. Antony and C. E. Witt, eds. Boulder,\nCO, USA: Westview Press, 1993, pp. 73–89.\n[43]\n E.  F.  Keller, Reﬂections  on  Gender  and  Science.  New\nHeaven, CT, USA: Yale University Press, 1985.\n[44]\n C. A. MacKinnon, Feminism, marxism, method, and the\nstate: An agenda for theory, Signs: Journal of Women in\nCulture and Society, vol. 7, no. 3, pp. 515–544, 1982.\n[45]\n P.  Butler, Chokehold:  Policing  Black  Men.  New  York,\nNY, USA: The New Press, 2017.\n[46]\n M. Alexander, The New Jim Crow: Mass Incarceration\n[47]\nin the Age of Colorblindness. New York, NY, USA: The\nNew Press, 2012.\n D.  Baum,  Legalize  it  all, https://harpers.org/archive/\n2016/04/legalize-it-all/, 2016.\n[48]\n B. Meixell and R. Eisenbrey, An epidemic of wage theft\nis costing workers hundreds of millions of dollars a year,\nhttps://www.epi.org/publication/epidemic-wage-theft-\ncosting-workers-hundreds/, 2014.\n[49]\n A.  S.  Vitale, The  End  of  Policing.  London,  UK:  Verso\nBooks, 2017.\n[50]\n S.  Hooker,  Why “data  for  good” lacks  precision,\nhttps://towardsdatascience.com/why-data-for-good-lacks-\nprecision-87fb48e341f1, 2018.\n[51]\n R. Abebe and K. Goldner, Mechanism design for social\ngood, AI Matters, vol. 4, no. 3, pp. 27–34, 2018.\n[52]\n B.  Berendt,  AI  for  the  common  good?!  Pitfalls,\nchallenges,  and  ethics  pen-testing, Paladyn, Journal  of\nBehavioral Robotics, vol. 10, no. 1, pp. 44–65, 2019.\n[53]\n W.  Kenton,  Social  good, Investopedia, https://www.\ninvestopedia.com/terms/s/social_good.asp, 2021.\n[54]\n M.  J.  Bauman,  K.  S.  Boxer,  T.  -Y.  Lin,  E.  Salmon,  H.\nNaveed,  L.  Haynes,  J.  Walsh,  J.  Helsby,  S.  Yoder,  R.\nSullivan, et al., Reducing incarceration through prioritized\ninterventions, in Proc. the 1st ACM SIGCAS Conference\non Computing and Sustainable Societies, Menlo Park and\nSan Jose, CA, USA, 2018, pp. 1–8.\n[55]\n S. Carton, J. Helsby, K. Joseph, A. Mahmud, Y. Park, J.\nWalsh, C. Cody, E. Patterson, L. Haynes, and R. Ghani,\nIdentifying  police  officers  at  risk  of  adverse  events,  in\nProc. the 22nd ACM SIGKDD International Conference\non  Knowledge  Discovery  and  Data  Mining,  San\nFrancisco, CA, USA, 2016, pp. 67–76.\n[56]\n S. Seo, H. Chan, P. J. Brantingham, J. Leap, P. Vayanos,\nM.  Tambe,  and  Y.  Liu,  Partially  generative  neural\nnetworks  for  gang  crime  classification  with  partial\ninformation, in Proc. the 2018 AAAI/ACM Conference on\nAI, Ethics and Society (AIES), New Orleans, LA, USA,\n2018, pp. 257–263.\n[57]\n E.  Felton,  Gang  databases  are  a  life  sentence  for  black\nand\n latino\n communities, \nhttps://psmag.com/social-\njustice/gang-databases-life-sentence-for-black-and-latino-\ncommunities, 2018.\n[58]\n D.  Bloor, Knowledge  and  Social  Imagery.  Chicago,  IL,\nUSA: University of Chicago Press, 1991.\n[59]\n J.  Sylvester  and  E.  Raff,  What  about  applied  fairness?\npresented at Machine Learning: The Debates Workshop\nat  the  35th  International  Conference  on  Machine\nLearning, Stockholm, Sweden, 2018.\n[60]\n A. Gorz, Strategy for Labor. Boston, MA, USA: Beacon\nPress, 1967.\n[61]\n A.  Lorde,  The  master’s  tools  will  never  dismantle  the\nmaster’s  house,  in Sister  Outsider:  Essays  &  Speeches.\nTrumansburg,  NY,  USA:  Crossing  Press,  1984,  p.\n110–113.\n[62]\n A.  Karakatsanis,  The  punishment  bureaucracy:  How  to\nthink  about “criminal  justice  reform”, The  Yale  Law\nJournal Forum, vol. 128, pp. 848–935, 2019.\n[63]\n A. M. McLeod, Confronting criminal law’s violence: The\npossibilities of unfinished alternatives, Unbound: Harvard\nJournal of the Legal Left, vol. 8, pp. 109–132, 2013.\n[64]\n B.  Green,  The  false  promise  of  risk  assessments:\n[65]\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n263    \n \n\nEpistemic reform and the limits of fairness, in Proc. the\n2020  Conference  on  Fairness,  Accountability,  and\nTransparency, Barcelona, Spain, 2020, pp. 594–606.\n FWD.  us,  Broad,  bipartisan  support  for  bold  pre-trial\nreforms  in  New  York  state, https://www.fwd.us/wp-\ncontent/uploads/2018/03/NYCJR-poll-memo-Final.pdf,\n2018.\n[66]\n Data\n for\n Progress,\n Polling\n the\n left\n agenda,\nhttps://www.dataforprogress.org/polling-the-left-agenda/,\n2018.\n[67]\n B. Green and S. Viljoen, Algorithmic realism: Expanding\nthe boundaries of algorithmic thought, in Proc. the 2020\nConference\n on\n Fairness,\n Accountability,\n and\nTransparency, Barcelona, Spain, 2020, pp. 19–31.\n[68]\n J. Jouvenal, Police are using software to predict crime. Is\nit  a ‘holy  grail’ or  biased  against  minorities?\nhttps://www.washingtonpost.com/local/public-\nsafety/police-are-using-software-to-predict-crime-is-it-a-\nholy-grail-or-biased-against-minorities/2016/11/17/\n525a6649-0472-440a-aae1-b283aa8e5de8_story.html,\n2016.\n[69]\n J.  Smith, ‘Minority  report’ is  real  —  and  it’s  really\nreporting\n minorities, \nhttps://mic.com/articles/127739/\nminority-reports-predictive-policing-technology-is-\nreally-reporting-minorities, 2015.\n[70]\n ACM  FAccT  Conference,  ACM  FAccT  network,\nhttps://facctconference.org/network/, 2021.\n[71]\n A. L. Hoffmann, Where fairness fails: Data, algorithms,\nand\n the\n limits\n of\n antidiscrimination\n discourse,\nInformation, Communication & Society,  vol. 22,  no. 7,\npp. 900–915, 2019.\n[72]\n A.\n D.\n Selbst,\n D.\n Boyd,\n S.\n A.\n Friedler,\n S.\nVenkatasubramanian,  and  J.  Vertesi,  Fairness  and\nabstraction  in  sociotechnical  systems,  in Proc.  the\nConference\n on\n Fairness,\n Accountability,\n and\nTransparency, Atlanta, GA, USA, 2019, pp. 59–68.\n[73]\n B.  Green,  Escaping  the  impossibility  of  fairness:  From\nformal  to  substantive  algorithmic  fairness, https://\npapers.ssrn.com/sol3/papers.cfm?abstract_id=3883649,\n2021.\n[74]\n A. Dunne and F. Raby, Design Noir: The Secret Life of\nElectronic Objects. Basle, Switzerland: Birkhauser, 2001.\n[75]\n T.  Smyth  and  J.  Dimond,  Anti-oppressive  design,\nInteractions, vol. 21, no. 6, pp. 68–71, 2014.\n[76]\n E. Graeff, The responsibility to not design and the need\nfor citizen professionalism, Computing Professionals for\nSocial  Responsibility:  The  Past,  Present and  Future\nValues of Participatory Design, doi: 10.21428/93b2c832.\nc8387014\n[77]\n B.  Green,  T.  Horel,  and  A.  V.  Papachristos,  Modeling\ncontagion through social networks to explain and predict\ngunshot  violence  in  Chicago,  2006  to  2014, JAMA\nInternal Medicine, vol. 177, no. 3, pp. 326–333, 2017.\n[78]\n W.  R.  Frey,  D.  U.  Patton,  M.  B.  Gaskell,  and  K.  A.\nMcGregor, Artificial intelligence and inclusion: Formerly\ngang-involved  youth  as  domain  experts  for  analyzing\nunstructured  twitter  data, Social  Science  Computer\nReview, vol. 38, no. 1, pp. 42–56, 2020.\n[79]\n S. Goel, J. M. Rao, and R. Shroff, Precinct or prejudice?\nUnderstanding racial disparities in New York City’s stop-\n[80]\nand-frisk policy The Annals of Applied Statistics, vol. 10,\nno. 1, pp. 365–394, 2016.\n R. Voigt, N. P. Camp, V. Prabhakaran, W. L. Hamilton,\nR. C. Hetey, C. M. Griffiths, D. Jurgens, D. Jurafsky, and\nJ.  L.  Eberhardt,  Language  from  police  body  camera\nfootage  shows  racial  disparities  in  officer  respect,\nProceedings  of  the  National  Academy  of  Sciences,\nvol. 114, no. 25, pp. 6521–6526, 2017.\n[81]\n D.  McQuillan,  Data  science  as  machinic  neoplatonism,\nPhilosophy & Technology, vol. 31, pp. 253–272, 2018.\n[82]\n S.  M.  West,  M.  Whittaker,  and  K.  Crawford,\nDiscriminating systems: Gender, race, and power in AI,\nhttps://ainowinstitute.org/discriminatingsystems.pdf,\n2019.\n[83]\n A. L. Hoffmann, Data violence and how bad engineering\nchoices\n can\n damage\n society, \nhttps://medium.com/\ns/story/data-violence-and-how-bad-engineering-choices-\ncan-damage-society-39e44150e1d4, 2018.\n[84]\n R.  Srinivasan, Whose  Global  Village?:  Rethinking  How\nTechnology  Shapes  Our  World.  New  York,  NY,  USA:\nNYU Press, 2017.\n[85]\n C. Harrington, S. Erete, and A. M. Piper, Deconstructing\ncommunity-based  collaborative  design:  Towards  more\nequitable participatory design engagements, Proceedings\nof  the  ACM  on  Human-Computer  Interaction,  vol. 3,\nno. CSCW, pp. 1–25, 2019.\n[86]\n A.  Birhane,  The  Algorithmic  Colonization  of  Africa,\nhttps://reallifemag.com/the-algorithmic-colonization-of-\nafrica/, 2019.\n[87]\n S.  Costanza-Chock, Design  Justice:  Community-Led\nPractices to Build the Worlds We Need. Cambridge, MA,\nUSA: MIT Press, 2020.\n[88]\n R.  Benjamin, Race  After  Technology.  Cambridge,  UK:\nPolity, 2019.\n[89]\n C.  D’Ignazio  and  L.  F.  Klein, Data  Feminism.\nCambridge, MA, USA: MIT Press, 2020.\n[90]\n M.  Whittaker,  M.  Alper,  C.  L.  Bennett,  S.  Hendren,  L.\nKaziunas, M. Mills, M. R. Morris, J. Rankin, E. Rogers,\nM.\n Salas,\n et\n al.,\n Disability,\n bias,\n and\n AI,\nhttps://ainowinstitute.org/disabilitybiasai-2019.pdf, 2019.\n[91]\n Design  Justice,  Design  Justice  Network  Principles,\nhttps://designjustice.org/read-the-principles, 2018.\n[92]\n A.\n Meng\n and\n C.\n DiSalvo,\n Grassroots\n resource\nmobilization  through  counter-data  action, Big  Data &\nSociety, vol. 5, no. 2, p. 205395171879686, 2018.\n[93]\n S. Costanza-Chock, M. Wagoner, B. Taye, C. Rivas, C.\nSchweidler,  G.  Bullen,  and  the  Tech  for  Social  Justice\nProject,  #More  than  code:  Practitioners  reimagine  the\nlandscape  of  technology  for  justice  and  equity,\nhttps://morethancode.cc/T4SJ_fullreport_082018_AY_w\neb.pdf, 2018.\n[94]\n N. Scheiber and K. Conger, Uber and Lyft Drivers Gain\nLabor\n Clout,\n With\n Help\n From\n an\n App.\nhttps://www.nytimes.com/2019/09/20/business/uber-lyft-\ndrivers.html, 2019.\n[95]\n J.  Dickinson,  M.  Díaz,  C.  A.  L.  Dantec,  and  S.  Erete,\n“The  cavalry  ain’t  coming  in  to  save  us”:  Supporting\ncapacities\n and\n relationships\n through\n civic\n tech,\nProceedings\n of\n the\n ACM\n on\n Human-Computer\nInteraction, vol. 3, no. CSCW, pp. 1–21, 2019.\n[96]\n    264\nJournal of Social Computing, September 2021, 2(3): 249−265    \n \n\n J. N. Matias and M. Mou, CivilServant: Community-led\nexperiments  in  platform  governance,  in Proc.  the  2018\nCHI  Conference  on  Human  Factors  in  Computing\nSystems, Montreal, Canada, 2018, pp. 1–13.\n[97]\n M. Asad, Prefigurative design as a method for research\njustice, Proceedings  of  the  ACM  on  Human-Computer\nInteraction, vol. 3, no. CSCW, pp. 1–18, 2019.\n[98]\n M.  M.  Maharawal  and  E.  McElroy,  The  anti-eviction\nmapping  project:  Counter  mapping  and  oral  history\ntoward bay area housing justice, Annals of the American\nAssociation\n of\n Geographers,\n vol. 108,\n no. 2,\npp. 380–389, 2018.\n[99]\n T.  Lewis,  S.  P.  Gangadharan,  M.  Saba,  and  T.  Petty,\nDigital  defense  playbook:  Community  power  tools  for\nreclaiming  data,  Technical  report,  Our  Data  Bodies,\nDetroit, MI, USA, 2018.\n[100]\n S.  Viljoen,  The  promise  and  limits  of  lawfulness:\nInequality,  law,  and  the  techlash,  Journal  of  Social\nComputing, doi: 10.23919/JSC.2021.0025.\n[101]\n P. E. Agre, Toward a critical technical practice: Lessons\nlearned  in  trying  to  reform  AI,  in Social  Science,\nTechnical  Systems,  and  Cooperative  Work:  Beyond  the\nGreat Divide, G. C. Bowker, S. L. Star, W. Turner, and\nL. Gasser, eds. London, UK: Psychology Press, 1997, pp.\n131–158.\n[102]\n B.  Hecht,  L.  Wilcox,  J.  P.  Bigham,  J.  Schöning,  E.\nHoque,  J.  Ernst,  Y.  Bisk,  L.  D.  Russis,  L.  Yarosh,  B.\nAnjam, et al., It’s time to do something: Mitigating the\n[103]\nnegative  impacts  of  computing  through  a  change  to  the\npeer  review  process, ACM  Future  of  Computing  Blog,\nhttps://acm-fca.org/2018/03/29/negativeimpacts/, 2018.\n Neural  Information  Processing  Systems  Conference,\nGetting started with NeurIPS 2020, https://medium.com/\n@NeurIPSConf/getting-started-with-neurips-2020-\ne350f9b39c28, 2020.\n[104]\n E.  Fee,  Women’s  nature  and  scientific  objectivity,  in\nWoman’s  Nature:  Rationalizations  of  Inequality,  M.\nLowe  and  R.  Hubbard,  eds.  New  York,  Ny,  USA:\nPergamon Press, 1983, pp. 9–27.\n[105]\n C. Pein, Blame the computer, https://thebaffler.com/salvos/\nblame-the-computer-pein, 2018.\n[106]\n S. Viljoen, A relational theory of data governance, Yale\nLaw Journal, vol. 131, no. 2, pp. 573–654, 2021.\n[107]\n Mijente,  1,  200+  students  at  17  universities  launch\ncampaign  targeting  Palantir, https://notechforice.com/\n20190916-2/, 2019.\n[108]\nBen Green is a postdoctoral scholar in the\nSociety  of  Fellows  and  an  assistant\nprofessor in the Gerald R. Ford School of\nPublic Policy, University of Michigan . He\nreceived  the  PhD  degree  in  applied  math\n(with  a  secondary  field  in  STS)  from\nHarvard  University  and  the  BS  degree  in\nmathematics & physics from Yale College\nin 2020 and 2014, respactively.\n  Ben Green:   Data Science as Political Action: Grounding Data Science in a Politics of Justice\n265    \n \n\n \nFrom Ethics Washing to Ethics Bashing: A Moral\nPhilosophy View on Tech Ethics\nElettra Bietti*\nAbstract:    Weaponized in support of deregulation and self-regulation, “ethics” is increasingly identified with\ntechnology companies’ self-regulatory efforts and with shallow appearances of ethical behavior. So-called\n“ethics washing” by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech\ncommunity.  The  author  defines “ethics  bashing” as  the  parallel  tendency  to  trivialize  ethics  and  moral\nphilosophy. Underlying these two attitudes are a few misunderstandings: (1) philosophy is understood in\nopposition and as alternative to law, political representation, and social organizing; (2) philosophy and “ethics”\nare  perceived  as  formalistic,  vulnerable  to  instrumentalization,  and  ontologically  flawed;  and  (3)  moral\nreasoning is portrayed as mere “ivory tower” intellectualization of complex problems that need to be dealt with\nthrough  other  methodologies.  This  article  argues  that  the  rhetoric  of  ethics  and  morality  should  not  be\nreductively instrumentalized, either by the industry in the form of “ethics washing”, or by scholars and policy-\nmakers in the form of “ethics bashing”. Grappling with the role of philosophy and ethics requires moving\nbeyond simplification and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech\npolicy strategies. We must resist reducing moral philosophy’s role and instead must celebrate its special worth\nas a mode of knowledge-seeking and inquiry. Far from mandating self-regulation, moral philosophy facilitates\nthe scrutiny of various modes of regulation, situating them in legal, political, and economic contexts. Moral\nphilosophy indeed can explainin the relationship between technology and other worthy goals and can situate\ntechnology within the human, the social, and the political.\nKey  words:   ethics;  technology;  artificial  intelligence;  big  tech;  ethics  washing;  law;  regulation;  moral\nphilosophy; political philosophy\n1    Introduction\nOn May 26th, 2019, Google announced that it would put\nin place an external advisory council for the responsible\ndevelopment of AI, the Advanced Technology External\nAdvisory  Council  (ATEAC).[1] Following  a  petition\nsigned by 2556 Google workers demanding the removal\nof  one  of  the  body’s  board  members,  anti-LGBT\nadvocate  Kay  Coles  James,  the  advisory  body  was\nwithdrawn\n approximately\n one\n week\n after\n its\nannouncement.[2, 3] On  December  3rd,  2020,  Timnit\nGebru, a Google AI researcher, was abruptly fired for\nsending an internal letter to Google employees which\ndiscussed her superiors’ questionable resistance to the\npublication of a research paper she co-authored.[4−6] Her\nTweet produced a wave of reactions in academia and\nbeyond,  with  many  Google  employees  subsequently\nquitting.[7] These  episodes  and  the  backlash  they\nproduced  provide  a  salient  illustration  of  the  tensions\naround  the  corporate  use  of “ethics” language  in\ntechnology\n circles.\n Corporate\n and\n policy\ninstrumentalization  and  misuse  of  such  language  in\ntechnology policy have taken two forms.\n \n • Elettra  Bietti  is with  the  Harvard  Law  School,  Harvard\nUniversity,\n Cambridge,\n MA\n 02138,\n USA.\n E-mail:\nebietti@sjd.law.harvard.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-19;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   05/06  pp266−283\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0031\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nOn one hand, the term has been used by companies as\nan  acceptable  façade  that  justifies  deregulation,  self-\nregulation  or  market  driven  governance,  and  is\nincreasingly identified with technology companies’ self-\ninterested adoption of appearances of ethical behavior.\nSuch growing instrumentalization of ethical language by\ntech  companies  has  been  called “ethics  washing”.[8]\nBeyond AI ethics councils or AI Ethics researchers, the\nethics  washing  critique  extends  to  corporate  practices\nthat have tended to co-opt the value of ethical work: the\nhiring of in-house moral philosophers who have little\npower to shape internal company policies; the careful\nselection of employees that will not question the status\nquo; the focus on humane design—e.g., nudging users to\nreduce time spent on apps—that does not address the\nrisks inherent in tech products themselves;[9] the funding\nof “fair” machine  learning  systems  combined  to  the\ndefunding of work on algorithmic systems that questions\nthe broader impacts of those systems on society.[10, 11]\nOn  the  other  hand,  the  technology  community’s\ncriticism  and  scrutiny  of  instances  of  ethics  washing,\nwhen  imprecise,  have  sometimes  bordered  into  the\nopposite fallacy, which the author calls “ethics bashing”.\nThis is a tendency, common amongst non-philosophers,\nto simplify the issues around tech “ethics” and “moral\nphilosophy” either  by  drawing  a  sharp  distinction\nbetween ethics and law and defining ethics as that which\noperates in the absence of law[12] or by conflating all\nforms of moral inquiry with routine politics, for instance\nby  merging  or  drawing  artificial  separations  between\nthe  frameworks  of “ethics”, “justice”,  and “political\naction”.[13, 14] Distinguishing between “law” and “ethics”\nis a common legal positivist move, configurable within\na long philosophical tradition that sees the practice of\nmaking,  interpreting,  and  applying  law  as  processes\nwhose existence and relevance are distinct and separable\nfrom  their  moral  and  societal  implications.[15] The\nrelation  between “ethics”, “justice”,  and “political\naction” instead  is  complex.  Understanding  ethics  and\nmoral inquiry as either a mode of political action or a\ndiscrete, individual-centric, and particularized exercise\nthat is easily instrumentalized and is unsuited to tackling\npolitical  and  institutional  questions  is  misleading  yet\nfrequent. As described by Jacob Metcalf, Emanuel Moss,\nand Danah Boyd, the distinction between narrow “ethics”\nand  capacious “justice” became  a  central  focus  of\ndiscussions  during  the  2019  ACM  Conference  on\nFairness, Accountability and Transparency.[13]\nEquating serious engagement in moral argument with\nthe social and political dynamics within ethics boards or\nunderstanding ethics as a methodological stance that is\nantithetic to—instead of complementary to and inherent\nin—serious engagement in law-making and democratic\ndecision-making,  is  a  frequent  and  dangerous  fallacy.\nThe  misunderstandings  underlying  the  broad  trend  of\nethics bashing are at least three-fold: (1) philosophy is\neither  confused  with “self-interested  politics” or\nunderstood  in  opposition  to  law,  justice,  political\nrepresentation,  and  social  organizing;  (2)  philosophy\nand “ethics” are  seen  as  a  formalistic  methodology,\nvulnerable  to  instrumentalization  and  abuse,  and  thus\nontologically  flawed;  and  (3)  engagement  in  moral\nphilosophy is downplayed and portrayed as mere “ivory\ntower” intellectualization  of  complex  problems  that\nneed  to  be  dealt  with  through  alternative  and  more\npractical methodologies.\nGrappling with the role of ethics in tech policy requires\nmoving beyond both ethics washing and ethics bashing\nand seeing ethics as a mode of inquiry that informs work\nin  law,  policy,  and  technological  design  alike  in\nemancipatory\n directions.\n Policy-makers,\n lawyers,\ntechnologists,  corporates,  and  academics  do  moral\ntheorizing  all  the  time.  Asking  whether  a  corporate\nethics  council  can  improve  internal  policy-making,\nwhether a given machine learning system can lead to\nfairer criminal justice enforcement, or whether a given\ncorporate  decision  to  fire  a  researcher  or  ban  facial\nrecognition  is  acceptable  in  context  involves  asking\nmoral questions that, if properly framed, can lead to a\nbetter  understanding  of  these  phenomena  and  also  to\nbetter policies. Awareness of the ubiquity of morality\nwould enable all actors in the technological and AI space\nto  contextualize  their  work  with  greater  subtlety,  at\nseveral  levels  of  abstraction,  and  to  more  rigorously\nassess  the  legitimacy  of  corporate  self-regulation  and\nother ethics initiatives.\nOne aim of this article is to distinguish between what\nethics  is  often  thought  to  be  (a  neutral  and  context-\nindependent  methodology,  a  self-interested  corporate\nrhetoric)  and  what  ethics  could  be  (a  principled\nmethodology  for  evaluating  political  disagreements\naround  technology).  To  understand  that  distinction,\nanother  distinction  must  be  captured  between  the\nintrinsic  and  the  instrumental  value  of  ethics.  The\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n267    \n \n\nintrinsic  perspective  sees  ethics  as  a  mode  of  inquiry\nwhich  is  independently  valuable  as  an  aspirational\nprocess,  particularly  for  those  engaging  in  it.  The\ninstrumental perspective instead sees the value of ethics\nas lying in its results. The value of ethics understood in\nthis way depends on its end-results, ethics’ causal role in\nbringing  about  desired  results,  such  as  reputation,\ninnovation,  and  profit.  Intrinsic  and  instrumental\nperspectives  on  ethics  and  moral  inquiry  are  not\nmutually  exclusive.  One  can  understand  ethics  as  an\nintrinsically  valuable  process  with  valuable  results.\nHowever,  distinguishing  facial  appearances  of  ethics\nfrom approaches that emphasize ethics’ potential entails\nemphasizing  intrinsic  value  over  instrumental  value.\nThe  author  will  argue  that  the  more  the  process  of\nengaging  in  ethics  is  motivated  by  outcomes\nindependent of the process itself—the less ethics is taken\nas  an  intrinsically  valuable  process—the  weaker  its\nmoral  value  becomes  for  society.  Ethics  washing  and\nethics bashing are instrumental understandings of ethics,\nin  that  both  positions  or  tendencies  envision  or\nexperience ethics as a means to an end and nothing more.\nWhat  is  at  stake  in  recent  controversies  around  the\nweaponization of “ethics” rhetoric are also competing\nmoral  conceptions  of  technology  companies’ role.\nCorporate-friendly  conceptions  benefit  from  inserting\nethical work within larger communications and public\nrelations strategies.[13, 16−18] Critical conceptions reject\nthese  corporate  efforts  and  prefer  participatory\ndemocracy  and  activism.[11, 19] Yet  both  corporations\nand  their  critics  obscure  the  potential  role  that  moral\ninquiry  can  and  must  play  in  developing  a  thicker\nconception of technology politics. There is no neutral\nperspective “outside  morality” from  which  the\nnormative implications of technology can be teased out.\nIt should thus be possible to maintain a critical outlook\non  the  instrumentalization  of  ethics  in  technology\nsettings,  while  also  recognizing  the  special  value  and\ncentrality of moral inquiry to expanding horizons.\nThis article has two goals. First, it aims to articulate the\nweaknesses  of  both  the  ethics  washing  and  ethics\nbashing fallacies, explaining why both are impoverished\nviews of the relationship between technology and ethics.\nSecond,  it  aims  to  clarify  the  potential  of  moral\nphilosophy  in  debates  about  the  impact  of  new\ntechnologies  on  society  and  thereby  to  dissipate\nmisunderstandings  of  moral  philosophy  as  either  too\nabstract to inform concrete policy or as a red herring that\nprevents proper focus on political and social action. Far\nfrom constituting a barrier to appropriate governance,\nmoral philosophy enables us to seriously scrutinize the\nfuture of technology governance, law, and policy, and to\nunderstand what humans need from new technologies\nand innovation from a unique vantage point.\nThe article is structured as follows. In Sections 2−4,\nthe  article  begins  by  explaining  the  function  and\nmeaning of ethics and moral philosophy, some common\ncriticisms of moral philosophy, and what it is for. Section 5\nof the article then provides background on the rise of\nethics  in  tech  and  the  advent  of  so-called “ethics\nwashing”. In Section 6 it explains the limits of existing\ncritiques of ethics washing, identifying “ethics bashing”\nas  a  fallacious  depiction  of  ethics  as  opposed  to  law,\npolitics, or justice. In Sections 7 and 8, adopting a view\ninternal  to  moral  philosophy,  the  author  engages  in  a\nmoral argument and shows that commitment to moral\nprinciples and engagement in moral reasoning also leads\nto the conclusion that corporate ethics efforts are by and\nlarge wrong and that ethics is antithetic to what happens\ninside corporate settings. Finally, Section 9 of this article\nsuggests a way forward that moves beyond both ethics\nwashing  and  ethics  bashing,  that  adopts  a  less\ninstrumentalist  position  on  ethics,  and  that  requires\ndeveloping  governance  frameworks  that  enable  the\nemergence  of  renewed  moral,  political,  and  legal\nthinking and action outside corporate settings.\n2    Ethics and Moral Philosophy\nThe English word “ethics” is derived from the ancient\nGreek words ēthikós and êthos which refer to character\nand  moral  nature.[20] Morality  comes  from  the  Latin\nmoralis  which  means  manner,  character,  and  proper\nbehavior. Both “ethics” and “morality” thus refer to the\nstudy of good and bad character, appropriate behavior,\nand  virtue.  The  two  terms  are  often  employed\ninterchangeably  but  have  slightly  distinct  uses  and\nconnotations. Morality is often associated with etiquette\nand rules of appropriate social behavior, whereas ethics\nhas instead a more personal connotation. Ethics pertains\nto  the  cultivation  of  individual  virtue  abstracted  from\nsociety and is sometimes used to refer to personal and\nprofessional standards of behavior embodied in “codes\nof ethics”. In Confucian philosophy, morality is about\nrespecting the family and pursuing social harmony and\n    268\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nstability  through  virtues  including  altruism,  loyalty,\nand piety.[21]\nIn the discussion to follow, the term “ethics” will refer\nto  the  rhetoric  of  morality  employed  in  technology\ncircles, and “moral philosophy” will instead refer to the\nphilosophical  discipline  that  investigates  questions\naround human agency, freedom, responsibility, blame,\nand  the  relationships  between  individuals,  amongst\nother questions. The author adopts a primarily Anglo-\nAmerican  liberal  approach  to  the  practice  and\nunderstanding of moral philosophy[22] but the author’s\nperspective is by no means intended to close the door to\nalternative approaches to moral philosophy and ethics.\nAccording to some accounts, moral philosophy’s scope\nis limited to relationships between humans and ethics\nextends instead beyond humans to animals and nature.\nSome  would  also  distinguish  moral  from  political\nphilosophy  while  others  such  as  Ronald  Dworkin  see\nthem  as  interconnected.[23] Like  Ronald  Dworkin,  the\nauthor construes the “moral” widely as consisting of the\ndomain of “value”, i.e., an evaluative mode of inquiry\nwhich is distinguishable from scientific or descriptive\nmodes  of  inquiry,  which  focus  on  facts.[23, 24] The\ndomain of “value” is the specific domain of inquiry of\nmoral philosophers.\nTo better illustrate what moral philosophy is, consider\nthe example of surveillance. Let us ask: what is wrong\nor  unethical  about  big  data  and  certain  forms  of\nsurveillance?  Disparate  arguments  can  be  offered  to\nshow that big data and surveillance are wrong in some\nrespects  or  worth  carrying  out  in  other  respects.\nDifferent  persons  will  likely  have  different  views  on\nwhich of these arguments are strongest. As philosophers\nmight put it: the morality of surveillance is an evaluative\nmatter,  i.e.,  a  matter  on  which  reasonable  people\ndisagree\n because\n they\n hold\n competing\n moral\ninterpretations  of  what  is  at  stake.  Numerous  lines  of\nreasoning  support  the  wrongness  of  surveillance  and\nbusiness  models  that  rely  on  data  extraction.\nSurveillance is objectionable on self-development and\nvirtue  ethics  grounds  because  it  incentivizes  self-\ncensorship, reducing human beings’ ability to develop\nthemselves or to engage in other valuable causes for fear\nthat  these  actions  will  be  held  against  them.  Another\nargument focuses on harm: some surveillance and big\ndata activities cause harm to individuals (e.g., they lead\nto unjustified and stereotype-enhancing discriminatory\ntreatment,  they  create  asymmetries  of  knowledge  and\npower,  they  perpetuate  pre-existing  and  unjustified\ninequalities). A third line of reasoning focuses on equal\ndignity  and  respect  for  persons:  some  forms  of  data\nprocessing and surveillance fail to treat individuals as\nequally worthy of respect because they are covert and\nbecause some people are surveilled more than others.\nEach  line  of  argument  entails  a  different  way  of\nevaluating  policy.  For  instance,  if  someone  considers\nthat surveillance inhibits the pursuit of worthy behavior\nor individuality, they might be satisfied with aspects of\nbig  data  and  surveillance  practices  that  enhance  the\npursuit  of  certain  worthy  life  goals,  including  certain\ntargeted and personalized work opportunities, as long as\nthey  are  empowering  and  equally  distributed.  On  the\nother hand, if one believes that the core problem is that\nthe  data  collected  can  cause  unintended  harm  to\nindividuals,  they  might  advocate  for  solutions  that\nminimize discriminatory impacts and ensure that harms\nare  reduced.  Finally,  someone  who  believes  that\nsurveillance  and  the  opacity  of  big  data  activities  are\ndenials of respect for the persons surveilled might be\nkeen  to  ban  surveillance  completely  or  to  reduce  any\ntolerable surveillance to a de minimis threshold.\nWhich reasons we find most weighty is a matter of\ncommitment and deliberation on how to actualize moral\nvalues  such  as  autonomy,  equality,  and  human\nflourishing.  The  process  of  weighing  some  reasons\nagainst others allows us to overcome the intuitive belief\nthat “surveillance feels creepy”,[25] and to instead ground\nor  re-evaluate  one’s  commitment  to  privacy  or  its\nlimitation based on carefully weighed argument on how\ndifferent forms of surveillance and data extraction might\ninteract  with  autonomy,  dignity,  equality,  and  human\nflourishing.  Identifying  the  drawbacks  of  surveillance\nbusiness  models  and  their  morally  unacceptable  core\nalso facilitates the design of nuanced concrete strategies\nfor addressing them.\nThis  process  of  revising  and  refining  moral  beliefs\nthrough  philosophical  inquiry  is  what  John  Rawls\nhas  called  reflective  equilibrium.[26] What  Rawls’\nmethodology  and  other  analogous  modes  of  moral\nevaluation have in common is that they provide a lens\nthrough which to interpret issues of societal importance,\nto  locate  them  within  existing  debates,  consider  them\nfrom all relevant standpoints, and evaluate which angle\nor way of approaching them is capable of shedding the\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n269    \n \n\nmost  valuable  light  on  the  issues  themselves.  When\nengaging  in  this  process,  the  broader  the  spectrum  of\nconsiderations  that  are  taken  into  account  in  moral\ntheorizing, the more interesting, capacious, and morally\nsignificant are the outcomes, and the more inspiring and\nvaluable are its practical implications.\nIt is also important to emphasize that moral philosophy\nand ethics can mean different things as part of different\nfields of study and intellectual traditions. The above is\nintended to capture only a glimpse of a larger roadmap\nof possible uses of the terminology of ethics and moral\nphilosophy in technology governance and policy. It is\nnot intended to fix the meaning of these rich and complex\nmodes of inquiry.\n3    What Moral Philosophy Is For\nA key question is what ethics and moral philosophy are\nfor and what they can contribute to existing technology\npolicy  debates.  In  asking  this  question,  The  author\nfocuses  on  the  reflexive  value  of  engaging  in  moral\nreasoning from the perspective of those engaging in it,\ni.e., “from  within”.  In  the  technology  policy  context,\nmoral and other philosophical work is valuable in at least\nfour ways for those who pursue it.\nFirst,  philosophical  reasoning  and  deliberation  can\nprovide a meta-level perspective from which to consider\nany  disagreement  relating  to  the  governance  of\ntechnology.  Instead  of  taking  arguments  narrowly,\nintuitively,  or  personally,  philosophical  reasoning\nprovides a framework for stepping back, situating any\nproblem within its broader context and understanding it\nwithin  or  in  relation  to  other  relevant  or  analogous\ndebates. As such, the practice or method of engaging in\nmoral argument allows us to broaden our perspective and\nto  look  at  a  debate  from  a  wider  lens,  overcoming\nconfusions,  filling  in  gaps,  correcting  inconsistencies,\nand  drawing  clarifying  distinctions.  In  debates  on  the\nacceptability  or  necessity  of  facial  recognition\ntechnologies, for instance, a philosophical method can\nhelp us rethink our reasons for rejecting or promoting\nexisting  technologies,  clarify  points  of  agreement\nbetween a variety of opponents to these technologies,\nand  focus  on  where  disagreements  lie  and  what  they\nentail in practice: what freedom, equality, and human\nflourishing require in an era of structural surveillance\nand systemic inequality. Otherwise put, philosophy is a\ngood antidote to knee-jerk reactions: it can help reduce\nunbridgeable  value  conflicts  and  make  agreement\npossible by moving discussions between different levels\nof  specificity  or  abstraction.  This  is  not  to  say  that\nideology and value conflicts are unimportant, but merely\nto recognize the importance of philosophy as a method\naimed at overcoming or clarifying those conflicts.\nA second, related, contribution of moral philosophy to\ntech debates is that it adds rigor principled thinking to\nvalue-laden, emotional, or subjective discussions. Moral\nphilosophy  should  be  understood  as  an  explanatory\nmode  of  inquiry  which  requires  us  to  set  out  the\njustifications and reasons for advancing one view and\nnot  a  different  one.  By  centering  attention  on  the\nexplanation  and  the  justification  for  a  position,\nphilosophy enables a dialectic to take place, a Socratic\ndialogue which we can have internally with ourselves or\nexternally with others, that sheds light on blind spots and\nenables  fluid  and  iterative  repositioning.  Winning  the\nargument is not as important as laying all its facets on the\ntable.  Such  principled  and  disinterested  inquiry  is\nfrequently absent in technology policy and governance\ndiscussions  for  at  least  two  reasons.  The  first  is  that\ncurrent  policy  debates  are  instinctive,  emotional,\npolarizing  and  inimical  to  measured  reflection.  The\nsecond is that many of these debates are mediated by\nplatforms  whose  corporate  incentives  are  difficult\nto  align  with  disinterested  reflection  on  societal\nimpacts.[27, 28]\nThird,\n a\n normative\n philosophical\n lens\n can\nsubstantively  move  us  beyond  a  narrow  focus  on\nprocedural  fairness,  diversity,  and  representation  in\ntechnology  governance,  and  towards  substantive  goal\nevaluation.  As  explained  in  more  detail  below,  the\nproblem  is  not  just  whether  an  AI  ethics  board’s\nmembers  have  diverse  perspectives  and  backgrounds,\nbut  also  whether  the  board’s  decisions  can  actually\nconstrain Google’s profit-motivated actions. Similarly,\nthe  question  is  not  just  whether  a  facial  recognition\nalgorithm properly recognizes black faces, but whether\nsuch algorithm is deployed in circumstances where it can\nharm  black  people.  A  capacious  moral  philosophy\napproach  can  help  us  move  beyond  checklists  and\nproceduralism to question whether an existing or future\nstructural  governance  framework  and  its  substantive\noutcomes are morally acceptable and worth pursuing.\nFourth, far from obscuring ideological conflicts and\nstructural divisions[19, 29] engaging in moral philosophy\n    270\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\ncan  facilitate  dialogue,  encourage  the  building  of\ncommon ground, and provide a basis for collaborative\nand participatory approaches to policy-making capable\nof  bridging  divides  in  a  polarized  landscape.  An\nimportant  drawback  of  critical  work  that  centers  on\npower,  value  conflicts,  and  unbridgeable  ideological\ndivides  is  that  it  renders  dialogue  between  people\nholding  different  views  or  occupying  different  social\npositions more difficult. Pursuing such strategies has its\nadvantages but it can also lead to fragmentation in an\nalready  polarized  and  emotions-driven  public  sphere.\nUnderstanding philosophy as a dialectic discipline that\nenables  empathy  and  grounds  methodology  in  the\naspirational possibilities of commonality, justification,\nand  conflict  resolution  can  instead  help  navigate\nfragmentation  and  polarization  today.  The  many\n“embedded ethics” initiatives at computer science and\nphilosophy departments in the United States and beyond\nare fostering greater debates and have been shown to\npromote  the  building  of  common  ground  across\ndisciplinary boundaries.[30−33]\nStill,\n while\n acknowledging\n the\n important\ncontributions of Western philosophy to the promotion of\nan inclusive and discursive public sphere, awareness of\nhow  power  and  inequality  manifest  within  such\ndiscursive public sphere is key. Not every person has the\nsame  voice  and  the  same  ability  to  be  heard.[34]\nEqualizing a space in the face of structural inequality\nmust  thus  be  one  of  the  first  considerations  when\nbuilding  spaces  for  dialogue  and “ethical” reflection.\nContemporary  approaches  that  embed  ideology  and\nstructural\n power\n asymmetries\n within\n normative\nphilosophical  inquiry[19, 29, 35] account  for  the  advan-\ntages  of  a  discursive  methodology  while  expanding\nthe horizon of philosophical inquiry to include issues of\nstructural\n inequality,\n power,\n domination,\n and\nideological entrenchment.\n4    How  to  Criticize  Ethics  and  Moral\nPhilosophy\nWork in moral philosophy and ethics has a number of\nlimitations. Before turning to the rise of ethics discourse\nin technology and the fallacies associated with that trend,\nhere are six ways of criticizing moral philosophy that are\ntargeted at moral philosophy as a reflexive exercise and\nas  a  methodology.  By  addressing  these  important\ncriticisms, my aim is to shed light on moral philosophy\nas a critical method, showing that it can channel change,\nre-assessment, and revision of commonly held beliefs.\nFirst, philosophy can be criticized for being abstract\nand  for  not  being  accessible  to  large  audiences.  This\nmakes philosophical work often unsuited to advocacy or\nactivism or to making provocative contributions to time-\nsensitive issues. Philosophy is also rarely suited to opeds,\nfor example, or to those who aim at quick and easy policy\nfixes.  Yet  depth  and  abstraction  are  also  one  of  the\ndiscipline’s advantages: engaging in philosophical work\nprompts us to pause and think, to shield our thinking\nfrom pragmatic pressures, to enlarge the temporal and\ngeographical scope of our research scope. As we engage\nin  this  process,  our  intuitions  change,  we  extend  our\nthoughts or revise them so that they can connect with and\nmake sense of other problems, we learn how to think\nslower,  to  think  with  more  depth  and  more\nsystematically.  To  achieve  meaningful  cultural  and\nsocial renewal in the technology industry, countering a\ntechnological  culture  of  fast-paced  permissionless\ninnovation driven by an ethos of “move fast and break\nthings”, slowness needs to be taken more seriously.[36]\nSecond, some work in moral philosophy, particularly\nin its connections with technology, is seen as not going\nfar enough prescriptively or as doing harm in practice.\nRecent  work  in  social  science,  for  example,  has\nattempted to rely on the philosophical heuristic of the\ntrolley  problem[37] to  address  the  regulation  of\nAutonomous  Vehicles  (AVs),  with  scarce  practical\nsuccess  and  generating  significant  controversy.  The\nMoral Machines experiment at MIT,[38, 39] a large-scale\nexperiment  that  gamifies  the  trolley  problem  to\nextrapolate  aggregate  data  and  then  guidelines  for\nprogramming AVs, has been criticized for simplifying,\nscaling,  and  misusing  a  case-specific  and  contextual\nphilosophical mode of reasoning.[40] Similarly, Basl and\nBehrends  argued  that  attempts  at  applying  trolley\nproblem  insights  directly  to  AV  policy  are  flawed\nbecause they fail to take into account the complexity and\ncontextuality of machine learning development.[41]\nMore generally, entrenching high level principles for\nethical AI in Codes[42] also arguably remains too abstract\nto  guide  individuals  and  policy-makers’ actions  in\npractice on AI questions.[11, 43, 44] In the absence of a\ndeep understanding of context, focusing on the trolley\nproblem  or  outlining  high  level  theoretical  principles\nfor  ethical  AI  appears  unlikely  to  lead  to  workable\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n271    \n \n\nand  morally  compelling  regulatory  strategies.  These\nexamples leave us perplexed: much philosophical work\nseems  irrelevant  or  unsuited  to  resolving  pressing\nproblems  in  technological  contexts.  What  is  needed\nhowever  is  not  less  philosophical  work,  but  more\nthinking on what moral principles can do in practice, and\nwhat they mean contextually. Helen Nissenbaum’s work\non contextual privacy is an important example of how\nthoroughly  articulating  the  contextual  implications  of\nabstract privacy norms can impactfully guide the work\nof communities of practice.[45]\nThird, the application of philosophical work can have\neffects  in  practice  that  sometimes  contradict  the\nphilosopher’s  motivations.  Hegel  and  Nietzsche’s\nphilosophical ideas have been instrumentalized by the\nGerman  Nazi  regime  to  pursue  inhumane  ends,  an\ninstrumentalization  that  had  little  connection  to  what\nthese philosophers were actually doing or thinking.[46, 47]\nMore  concretely,  philosophers  frequently  understand\nreflection and engagement with the politics and context\nof their work as corrupting, and thereby fail to prevent\nmisuses of their ideas for unworthy ends. The hiring of\nmoral philosophers by technology companies is but one\ninstance  in  which  philosophical  ideas  need  to  be\nscrutinized in context; such work cannot be taken at face\nvalue  just  because  they  are  the  ideas  of  a  trained\nphilosopher. Philosophers are hired, and then their skills\nare  subordinated  to  the  commercial  goals  of  their\nemployers. In this way, work that might have seemed\napolitical in an academic setting acquires a new politics.\nThis  work  can  become  harmful  if  it  hides  under  the\nappearance of neutral thinking allowing the legitimation\nof controversial states of affairs, such as the secrecy of\nalgorithms and their control by private companies. As\nimportant as it is, this criticism however should not be\nseen  as  fatal  to  the  kind  of  work  philosophers  do.\nThe  emergence  of  in-house  philosophers  means\nphilosophical  work  must  be  scrutinized  with  even\ngreater  care,  must  be  publicly  accountable,  and\nphilosophers must exercise an enhanced level of caution\nregarding the context and consequences of what they do.\nImportantly, the funding of philosophical work in the\ntechnology and governance field must be disclosed and\ndiscussed more openly.\nFourth,  work  in  ethics  can  be  understood  as\nnormalizing, as an attempt to discipline social life by\ndevising and applying universally applicable norms of\nconduct  that  entrench  existing  power  dynamics  by\nplacing  them  outside  the  realm  of  contestation.[48]\nMarxist critics of moral philosophy have also argued that\ncapitalist incentives can influence philosophical work in\ndirections  that  favor  the  interests  of  businesses  and\nelites.[49] Ethnographers  speak  of “ordinary  ethics” as\nthe descriptive way ethics and morality structure routine\nsocial  interaction.[13] Zigon  however  emphasizes  the\nimportance  of  distinguishing  routine  and  unconscious\nmoral  claims  from  conscious  ethical  claims  that  arise\nduring “breakdown” moments  and  are  aimed  at\nchanging a culture and at “returning to the unreflective\nmode of everyday moral dispositions”.[50] While Zigon’s\nanthropological  perspective  on  morality  and  ethics\ncaptures  the  pivotal  role  played  by  moments  of\nbreakdown and moral dilemma, he still sees morality and\nethics  as  fundamentally  about  the  need  to  return  to\nunreflected normality, to revise beliefs so they can be\nfixed, routinized, and remain unchallenged once again.\nFor  philosophers,  instead,  morality  and  ethics  are\ncentrally  about  reflectiveness,  conscious  revising  of\nbeliefs and constant changes to the status quo. Contrary\nto\n anthropologists\n and\n ethnographers,\n moral\nphilosophers  and  ethicists  are  only  marginally\nconcerned with the normalization of moral beliefs. For\na philosopher, the task is indeed to engage in direct moral\nquestioning about these beliefs and to bring them to the\nforeground\n of\n our\n consciousness,\n instead\n of\nemphasizing  their  regularities  and  embeddedness  in\nsocial norms and cultural contexts.\nFifth, philosophical theorizing is frequently criticized\nfor  creating  an  appearance  of  principled  reasoning,\nneutrality, and objectivity when much of what is at play\nare a philosopher’s subjective views.[19, 51] There is some\nvalidity to this criticism, but it is less powerful than it first\nappears. Good normative philosophical work does not\nattempt to convey an appearance of absolute objectivity.\nQuite the contrary, such work is very clear regarding the\nuncertain  bases  on  which  it  stands.  A  large  share  of\nAnglo-American  moral  philosophy  follows  Rawls’\nreflective  equilibrium  or  a  similar  method,  to\nprogressively match intuitions and beliefs to considered\njudgments.  This  iterative  process  is  one  of  many\napproaches  that  Anglo-American  philosophers  use  to\nformulate\n normative\n conclusions.\n Although\n any\nphilosophical  conclusion  necessarily  originates  in  a\nthinker’s subjective intuitions and beliefs, it is also the\n    272\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nproduct  of  structured  and  iterative  revisions.  It  gives\nconclusions  a  normative  weight  or  subtlety  that  raw\nintuitions do not have. Far from presenting ultimate and\nfinal  words  on  a  subject,  good  philosophical  work  is\nrigorous yet porous and open to scrutiny: its aim is to\nbroaden perspectives, allowing us to see the limits of the\nexisting and to constantly revise our beliefs.\nFinally, sociologists have argued, often rightly, that\nphilosophy is not sufficiently from a gender and racial\nperspective in particular, dominated instead by Western\nmale figures.[52]\nThese criticisms are grounded in the idea that moral\nphilosophy  can  be  a  worthy  enterprise  but  that  its\nobjective  appearance  or  moral  weight  too  often  leads\nphilosophers in the wrong direction. Philosophers and\ntheorists interested in the potential of ethical reflection\nin  technology  should  not  only  be  aware  of  these\nvulnerabilities but must also combat them by embedding\ninclusion  and  resistance  to  the  exploitation  and\ninstrumentalization  of  moral  inquiry  into  their  very\nmethodologies and practices.\nAs shown, moral philosophy is a reflexive pursuit that\nis valuable as a process for those who engage in it in view\nof making sense of the world around them with caution\nand empathy. Moral philosophy in this sense is not a\nsynonym  of  the  ethical  initiatives  that  occur  within\ncorporate  settings  which  are  mostly  self-centered  and\ninstrumental;[18] it  is  an  exercise  that,  if  construed\nradically as an inclusive emancipatory methodology, is\nin inherent tension with industry players’ profit logics.\nIn Section 5, the author explains the development and\nrise  of  technology  ethics  and  its  entrenchment  within\nprivate companies, a trend often aimed at reputational\nenhancement which has been called “ethics washing”.[8]\n5    The Rise of Tech Ethics and Ethics Washing\nIn  an  important  essay  in  1980,  Winner  showed  that\nartifacts  have  politics  in  two  important  ways:\ntechnologies embed and express the biases and power\nrelations of the society and people who design them, and\nthe deployment and use of these artifactual affordances\nin turn change and shape the politics and power relations\nin society.[53] The rise and promise of machine learning\nand  artificial  intelligence  technologies  have  brought\nabout a renewed urgency to the debate on the political\nnature  of  technology  and  its  ethical  implications.  A\nnumber of prominent books and articles on the subject\nhave shown that the deployment of artificial intelligence\ncan have significant consequences for privacy, human\ndignity, equality and non-discrimination, gender, social,\nracial,  and  economic  justice.[54−61] The  growing\nawareness  of  AI’s  societal  implications  and  political\nnature,  and  a  significant “techlash”,[62] have  led\ncompanies  involved  in  developing  AI  systems  to  pay\nattention to the ethical implications of data science and\nartificial intelligence.\nIn the last few years technology ethics has grown in\npopularity and been adopted and endorsed in a multitude\nof  overlapping  forms.[43] High-level  statements  of\nprincipled  artificial  intelligence  have  been  created\nor  endorsed  by  private  companies,  civil  society,\ngovernments,  as  well  as  transnational  and  multi-\nstakeholder  entities.[42] Ethics  training  has  been\ndeveloped  and  embedded  in  the  computer  science\ncurriculum of a growing number of universities.[30−32, 63]\nThe growing research field of AI and the growing body\nof research around its ethical and societal implications\nhas led to the creation of a number of new conferences\nand dedicated research institutes.[42]\nPrivate companies have been involved in these efforts\nat  each  level:  developing  and  publicly  sharing\nstatements  of  AI  principles,[42] hiring  in-house\nethicists,[64] forming ethics councils and bodies,[3] and\nputting  in  place  ethics  and  diversity  trainings  and\nstructures for their employees.[18] As regards principles,\nGoogle,\n for\n instance,\n has\n published\n principles\nemphasizing the need for AI applications to be socially\nbeneficial, to avoid creating or reinforcing bias, to be\nsafe and accountable.[65] Microsoft and IBM have also\nengaged in codifying principles and procedures for safe\nand trustworthy AI.[66, 67] Microsoft’s website states the\nneed\n to\n move\n beyond\n principles\n and\n toward\nimplementation  of  ethical  AI  through  ad  hoc  internal\nbodies:\nWe  put  our  responsible  AI  principles  into  practice\nthrough the Office of Responsible AI (ORA) and the AI,\nEthics, and Effects in Engineering and Research (Aether)\nCommittee.  The  Aether  Committee  advises  our\nleadership on the challenges and opportunities presented\nby AI innovations. ORA sets our rules and governance\nprocesses,  working  closely  with  teams  across  the\ncompany to enable the effort.[67]\nWhen  they  do  not  engage  directly  in  crafting\nstatements  of  principles  and  setting  up  internal  ethics\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n273    \n \n\nboards,  private  companies  sponsor  AI  conferences,\nresearch  institutes  and  efforts  that  shape  the  research\nagenda  and  discourse  around  the  societal  impact  of\nAI.[68] The Partnership on AI, a non-profit established to\nstudy and formulate best practices on AI technologies,\nwas founded by Amazon, Facebook, Google, DeepMind,\nMicrosoft, and IBM, and is entirely funded by industry\nstakeholders. Palantir, Google, and Facebook frequently\nfund  major  law,  computer  science,  and  privacy\nconferences.[18, 43] In  turn,  AI  ethics  is  becoming  a\nbusiness,  with  consultancy  firms  and  law  firms\ndeveloping AI ethics expertise to assist tech companies\nin their compliance efforts.[69, 70]\nAs these instances show, companies such as Google,\nFacebook, Microsoft, and Palantir are concerned about\ntheir ethical reputation in the face of new technological\ndevelopments in data science and beyond. Their efforts\nto  promote  and  arguably  build  more  trustworthy  and\nethical  AI  indicate  a  calculative  stance,  a  method  for\npreempting financial and reputational risk, more than a\nrecognition  of  the  political  nature  of  AI  and  its\nimplications.[13, 14, 16] Even though it might be argued\nthat the intentions behind these initiatives are good, the\npractices themselves are too limited and opportunistic to\nbe  in  line  with  a  conception  of  morality  and  ethics\nas  reflexive  capacious  exercises  that  can  foster\ndisinterested  selfless  change.  Overall,  speaking  of\nAI “ethics” instead of AI “politics” can be seen as a way\nto depoliticize and normalize the impacts of company\nefforts in this space,[14] allowing companies to “ethics\nwash” their reputations and to narrow the space for real\ndebate and change in AI.[8, 71]\n6    Critiques of Ethics Washing: Merits and\nLimits\nEfforts such as embedding ethicists or ethical guidelines\nwithin industry practices and creating codes of ethical\nprinciples  aimed  at  more  responsible  and  trustworthy\ntechnological design have been criticized by scholars for\nnormalizing  and  depoliticizing  data  science  and  AI\n(Green,  this  issue).  They  have  been  criticized  for\nbringing about a performative “transformation of ethics\nand design into discourses about ethics and design”,[11]\na routinized checklist approach to ethics that is powered\nby capitalist logics and a technosolutionist mindset.[13]\nCompanies  are “learning  to  speak  and  perform  ethics\nrather  than  make  the  structural  changes  necessary  to\nachieve the social values underpinning the ethical fault\nlines that exist”.[13] For Greene, Hoffmann, and Stark,\nthese practices are both too focused on technical tweaks,\nblinded  by  technical  concerns  about  how  to  embed\nfairness  and  accountability  within  machine  learning\nsystems  and  neglectful  of  structural  injustice,  and  are\nuniversalist  projects “justified  by  reference  to  a  hazy\nbiological  essentialism”.[11] For  human  rights  experts\nsuch as Paul Nemitz[12] and Phillip Alston who jokingly\nsaid  at  a  2018  AI  Now  conference  that  he  wanted  to\n“strangle  ethics”,[13] technology  ethics  is  seen  as  a\nsubstitute  or  an  alternative  to  more  adequate  human\nrights laws.[16]\nAs argued further below, these critiques ought to be\ntaken seriously. They shed light on the politics of AI and\non  crucial  blind  spots  that  are  performatively  and\nvoluntarily obscured by corporate ethics practices. Yet\nthey are at their weakest when, instead of understanding\nthat legal and technological governance are necessarily\nembedded in ethical and moral thinking, they draw sharp\ndichotomies  between “ethics” and “law”,  between\n“ethics” and “justice”,  as  if  these  were  incompatible\nalternatives  and  they  often  misconstrue  the  relation\nbetween “ethics” and “politics” failing to take them as\nall  ingredients  playing  complementary  roles  in  a\ndesirable understanding of technology governance. The\nauthor calls ethics bashing the reduction and dismissal\nof ethics as a simplistic alternative to law or justice, and\nthe lazy conflation of moral thinking and inquiry with a\npolitics  of  neutral  thinking  and  with  appearances  of\n“ethics” that  are  hardly  in  line  with  what  morality\nrequires.  The  author  identifies  three  fallacies  that\ncharacterize ethics bashing positions.\nFirst,  Nemitz  has  drawn  sharp  distinctions  between\nethics and law as separable and discrete practices: the\nkey question, writes Nemitz, is “which of the challenges\nof  AI  can  be  safely  and  with  good  conscience  left  to\nethics, and which challenges of AI need to be addressed\nby rules which are enforceable and based on democratic\nprocess, thus laws”.[12] Such distinctions operate on the\npositivist\n assumption\n that\n law—its\n making,\ninterpretation,  and  application—are  institutional  facts\nwhose existence and relevance are entirely distinct and\nseparable  from  its  societal  and  moral  implications.\nPositivists, frequently relying on a Humean separation\nof “is” and “ought”, or fact and value, argue that law\nbelongs to the realm of positive facts while morality is\n    274\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\ncompletely distinct and belongs to the realm of moral\nvalue and of the “ought”.[72] An understanding of law as\nconceptually separate from morality obscures how law\nis  constructed—written,  interpreted,  and  applied—in\nways\n that\n embed\n certain\n moral\n and\n political\ncommitments.  As  Dworkin  understood  and  theorized,\nlaw has no factual existence other than the existence we\ngive  it  through  the  principled  moral  and  political\ncommitments we express as we interpret and apply it.[24]\nConsequently, the task of understanding, applying, and\nre-making law is inseparable from engagement in the\ninternal  reflexive  exercise  of  moral  commitment  and\nethical evaluation. Instead of saying that law is superior\nto ethics, we might want to respond to obtuse corporate\nethics efforts by saying that a capacious understanding\nof  morality  and  ethics  is  incompatible  with  ethics\nwashing and extensive self-regulation and that morality\ninstead  requires  effective  laws  and  robust  external\nchecks  and  accountability  mechanisms  on  machine\nlearning systems, especially when they affect vulnerable\npopulations.[73]\nThe  second  and  third  fallacies,  the  conflation  of\n“ethics” and “self-interested politics” and the distinction\nbetween “ethics” and “social  justice”,  are  connected.\nBoth  attitudes  are  grounded  in  a  relatively  narrow\nunderstanding of moral inquiry as a discrete, individual-\ncentric, and particularized exercise whose politics and\nimpact lie in its separateness from broader political and\ninstitutional questions. As described by Metcalf et al.,\nthe distinction between narrow “ethics” and capacious\n“justice” became a central focus of discussions during\nthe 2019 ACM Conference on Fairness, Accountability\nand Transparency.[13] However, justice and morality are\ninseparably intertwined. Critics are right to argue that the\nfocus on design and on embedding fairness in machine\nlearning is too narrow to address more urgent questions\naround these technical systems’ political dimensions and\neffects  on  structural  inequality,  capitalist  exploitation,\nsurveillance,\n disinformation,\n and\n environmental\ndegradation.[10, 13, 14] However,  responding  to  narrow\nand techno-solutionist corporate approaches on “ethics”\nis  not  exhaustively  done  by  arguing  somewhat\nsimplistically that justice is superior to ethics, whatever\nthat means, or that ethics has a flawed politics. It must\nbe done by showing that any meaningful understanding\nof  ethics  (or  politics)  must  include  concerns  about\nstructural\n inequality,\n capitalist\n extraction,\n and\nenvironmental justice, or else it is an empty exercise that\nhas little to do with the ethics, justice, and politics of new\ntechnologies and their societal impacts.\nThe answer to instrumentalized ethics is not to draw\nsimplistic dichotomies, but to provide a richer account\nof how ethics, politics, and law are connected and can\nwork together to enable a better understanding of AI’s\nshortcomings and to foster political and other change.\nBy  addressing  ethics  from  the  outside,  as  a  discrete\npractice that does not include them, critics of corporate\nethics often fail to recognize that ethics is something they\nalso engage in and that existing corporate practices are\nin fact morally flawed. The task is therefore to change the\nway we collectively engage in moral inquiry, equipping\nourselves  with  a  better  understanding  of  injustice,\ninequality, and other digital harms. Corporate logics of\nprofit, expanding production, capitalist exploitation, and\nso on are often incompatible with a capacious view of\nmorality.\nIn the remainder of this article, the author articulates\nwhat  the  role  of  moral  philosophy  should  be  in\ntechnology policy debates and how a view that takes the\nreflexive internal exercise of moral inquiry as valuable\ncan shed light on the “ethics washing” debate. The author\nthen concludes with what ethics in technology must look\nlike going forward.\n7    The Moral Limits of Corporate Ethics and\nSelf-Regulation\nEquipped with a richer understanding of what ethics and\nmoral philosophy are and can do, the question now is\nwhat  role  moral  philosophy  can  play  in  informing\ntechnology policy and particularly the question of what\nmakes ethics-based efforts as practiced in corporate tech\nsettings  particularly  problematic  from  a  moral\nphilosophical  perspective.  Moral  philosophy  can\nprovide a lens to evaluate the moral wrongness of some\nof these efforts.\nAs described above, companies such as Google, Apple,\nMicrosoft,  OpenAI,  Palantir,  and  Facebook  are\nincreasingly  making  efforts  to  consider  an  ethical\nstandpoint. The intentions behind their proactive efforts\nare  often  presented  as  good,  but  the  practices  remain\ndriven  by  market  incentives  and  techno-centric\nperspectives  and  motivated  primarily  by  the  need  to\navoid  financial  and  other  company  risk.[11, 13]\nNotwithstanding good intentions, therefore, embedding\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n275    \n \n\nphilosophers or ethicists within technology companies\nappears  to  be  a  façade  that  is  frequently  used  to\nlegitimate  certain  pre-existing  practices  and  to  shield\ncompanies from measures more protective of consumers.\nThis  is  true  of  corporate  settings  but  also  of  public\ninstitutions.  Taylor  and  Dencik  for  example  have\ndescribed  the  political  dynamics  within  the  European\nCommission’s High Level Expert Group on AI, showing\nthat  instead  of  having  outcomes  guided  by  processes\nof  reflection  and  philosophical  principles,  ethical\nreflections are often designed to produce pre-determined\ninstrumental outcomes.[18] They state that after months\nof  discussion  around “red  lines” on  the  use  of  AI,\ncorporate participants in the High Level Group stated: “the\nword ‘red lines’ cannot be in this document … at any\npoint … and the word ‘non-negotiable’ has to be out of\nthis document.”[18] As Taylor and Dencik point out, “if\nthe possibility of delineating meaningful boundaries for\ntechnology … is off the table, then so is an important part\nof the task of ethics.”[18]\nAs we assess these ethics initiatives, we are therefore\npulled in two directions. On one hand, we are tempted to\nwelcome some of these developments as positive. On the\nother hand, we are moved to criticize these efforts for the\nopportunism  they  represent.  Where  we  stand  on  this\nspectrum  will  often  be  informed  by  our  situated\nperspective,  our  training,  by  who  pays  us,  etc.  What\nmoral philosophy as a method enables us to do is to take\na step back, to consider these attitudes along a spectrum\nof  nuanced  positions  on  companies’ ethical  behavior,\nand to evaluate our reasons for supporting or resisting\ninitiatives such as a corporate ethics council or an AI\nPanel of Experts at EU level. It allows us to suspend our\nintuitive reactions and take a less polarized perspective\non  the  question:  What  is  wrong  with  the  instrument-\nalization of ethics language? And what is wrong with\nethics boards and self-regulation?\nAs seen, much of the debate has centered on ethics\nas  a  self-regulatory  modality  of  governance  and  an\nalternative to law and government regulation. As Javier\nRuiz is reported to have stated, “a lot of the data ethics\ndebate is really about how … we avoid regulation. It is\nabout  saying  this  is  too  complex,  regulation  cannot\ncapture it, we cannot just tell people what to do because\nwe do not really know the detail.”[18] Self-regulation and\nself-publicity at first both seem benign. Self-regulation\nin  certain  cases  is  not  only  tolerable  but  actually\nwelcome, for instance where regulatory interference by\na public agency is unlikely to be effective and where a\nself-regulatory approach can lead to substantive policy\nimprovements  for  individuals  and  society.  Further,  in\nprinciple it does not seem morally objectionable to fund\nand develop initiatives that foster a positive image of one’s\nbusiness, nor does it seem wrong for a business to engage\nin  self-publicity  and  self-advocacy.  However,  when\nlooking further the reality is more complex.\nTo use an example, let us focus on the case of self-\nregulation in relation to online content moderation on\nFacebook. In the United States, governmental regulation\nof  online  speech  is  seen  with  suspicion.[74, 75] The\nsolution to the regulation of online speech on Facebook\nhas consequently materialized in the form of an internal\nFacebook Oversight Board (FOB), a quasi-judicial body\nset-up  internally  but  composed  of  external  experts  to\nadjudicate  on  the  acceptability  of  controversial  user\ncontent on the platform.[76] The body has been praised\nas “one  of  the  most  ambitious  constitution-making\nprojects of the modern era”,[77] and is seen as a workable\nand promising approach for taming Facebook’s power\nover  online  content  in  the  face  of  First  Amendment\nrestrictions on government regulation.[78] Nonetheless,\nwhile the Board may bring about needed transparency\nand  an  appearance  that  content  moderation  is  being\ntackled  fairly,  we  must  look  beyond  Facebook’s\nmessaging  to  find  its  shortcomings,  procedural  and\notherwise. In spite of its carefully crafted set-up and the\nwell-intentioned  messaging  around  its  existence,  it  is\nlikely that the FOB will serve the interests of Facebook\nmore than those of users. First, it provides a way to shield\nFacebook from other forms of regulation and scrutiny\non  matters  of  content  moderation  and  community\nguidelines,  including  the  intervention  of  national  or\ninternational  courts  but  also  the  formulation  and\nenforcement  of  legislative  redlines  and  constraints.\nSecond,  by  centering  attention  on  content  moderation\nand  community  guidelines,  it  allows  Facebook  to\ncontinue  developing  its  News  Feed  algorithms  as  it\npleases, and to continue showing individuals lucrative\ncontent, without interference from regulators or courts.\nThus, far from addressing all questions of online speech\nharms, the FOB seems to divert attention toward some\nissues and away from the most pressing concerns around\nmisinformation and political propaganda.[79]\nThe  case  of  facial  recognition  technologies  is\n    276\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nanalogous. In the United States, much state regulation of\nprivate  technology  firms  is  made  difficult  by  the\nFirst  Amendment.[80] The  solution  to  making  facial\nrecognition more ethical was thus for some time believed\nto  be  something  that  must  originate  within  the\nproprietary walls of tech companies and not something\nthat  can  be  initiated  by  government  entities  or  the\nFederal  Trade  Commission  (FTC).  But  things  are\nchanging.  Following  activist  efforts,  companies  like\nIBM,  Amazon,  and  Microsoft  have  scaled  back  on\ntheir  offering  of  general  purpose  facial  recognition\nsoftware.[81, 82] More  recently  Facebook  has  declared\nthat  it  will  cease  to  use  facial  recognition.[83] Earlier,\ncompany  ethics  boards  themselves,  such  as  Axon’s,\nrecognized the importance of public oversight on these\ntechnologies.[84] In spite of litigation by tech companies\nto defend their self-regulatory immunities, it seems that\nthe nomination of Alvaro Bedoya to the FTC will mark\na turning point in the relationship between state power\nand self-regulatory power in this space.\nSelf-regulatory and ethics washing initiatives such as\nthe FOB, Google’s ATEAC Board or Axon’s Report on\nfacial recognition technologies should prompt us to look\nbeyond  appearances  and  ask  whether  their  very\nexistence, in spite of appearing useful and a step forward,\nmight  in  fact  performatively  obscure  more  pressing\nproblems and risk long-term harm.\n8    A Critique of Ethics Washing from Within\nMoral Philosophy\nTo explore the moral limits of these internal corporate\nefforts  superficially  aimed  at  developing  more  ethical\nartificial  intelligence,  we  must  again  turn  to  moral\nphilosophy. At least three moral arguments can be raised\nagainst initiatives that co-opt ethics language and self-\nregulation  for  selfish  corporate  purposes  that  include\nprofits and reputation.\nFirst,  the  type  of  ethics  work  carried  out  within\ncompanies or ethics boards more often than not seems to\nlack  instrumental  value:  it  does  not  have  beneficial\neffects  on  individuals  and  society,  because  it  is\nundertaken under conditions that deny these beneficial\neffects. Second, these practices also seem to lack much\nof the intrinsic, or independent, value associated with\nphilosophical inquiry insofar as they do not seem to be\nundertaken in ways that value the process itself and with\nthe aim of achieving overall justice. Third, even if these\nethics-based practices were carried out in absolute good\nfaith and in pursuit of justice, and thus maintained both\ntheir instrumental and intrinsic value, instrumentalizing\nethics reasoning and language to reach company goals\nentails a specific kind of epistemic concern. Indeed, it\nseems  that  the  performative  role  of  ethics  language\nremains  problematic  even  where,  as  the  cases  of  the\nFacebook  Oversight  Board  or  the  Axon  Ethics  Board\nhave illustrated, these efforts are intended to address real\nissues  and  in  fact  could  have  positive  effects.  This\nhappens  where,  in  spite  of  having  some  instrumental\nvalue, these efforts instrumentalize ethics for the sake of\nother selfish or less valuable ends yet are presented as\npanaceas that serve the public interest. In what follows\nI explore these three arguments.\nThe  first  critique  of  self-regulation  and  company\nethics is an argument grounded in the poor instrumental\nvalue,  or  small  positive  impact,  of  ethical  work\nperformed within a company. Ethics bodies or in-house\nphilosophers are purportedly set up and hired to make a\ndifference to a company’s social impact. Yet as long as\nphilosophical  inquiry  is  mandated  and  funded  by  a\ncompany,  and  carried  out  within  closed  corporate\nproprietary  walls,  its  primary  function  is  to  benefit\ncompanies and fulfill their pre-existing mandates, and\ncannot be to benefit society at large and lead to social\nrenewal. Internal AI ethics practices are frequently put\nin  place  for  compliance  purposes,  to  pre-empt\nreputational and financial risk.[13] They are subjected to\ninternal limits, subordinated to the endorsement of high\nmanagement, and dependent on company funding. This\ndependency  on  the  company’s  control  renders  ethics\nrhetoric  inadequate  for  addressing  serious  cases  of\ncompany  misconduct  and  also  unfit  for  achieving\nsocietal change.\nThe narrow impact of ethics-based efforts carried out\nwithin tech companies is due in part to formal limitations\non employee-philosophers’ or ethics boards’ mandates\nand  in  part  to  more  diffuse  pressures  that  companies\nexert on technological discourse and context. Formally,\nfor example, Apple’s philosopher in residence Joshua\nCohen  has  been  forbidden  from  making  public\nappearances since he started working for the company\nand Microsoft’s AI ethics board does not disclose the\nreasons for its decisions.[85] The firing of former Google\nemployees  Timnit  Gebru  and  Margaret  Mitchell  for\nwriting allegedly controversial papers and pushing for\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n277    \n \n\na  prosocial  AI  agenda  inside  the  company  illustrates\ncompanies’ power  to  formally  police  internal  ethics\nefforts.[6, 7] It also however shows the potentially strong\ninstrumental value of social media backlash following\nthese  episodes.[4] Less  visibly,  companies  also  exert\ndiffuse  influence  on  the  broader  discourse  around\ntechnological innovation and ethics by funding research\nand  policy  initiatives  that  favor  their  agendas  and\nselecting  people  to  engage  with  (and  whose  ideas  to\nhighlight), including the people these companies choose\nto have as part of their ethics-based initiatives.[68, 86]\nThese internal pressures in turn shape the substance\nand conservative nature of resulting ethics-based work.\nStrong  pushes  for  data  protection  guarantees,  data\nminimization  mandates,  redlines  on  the  use  of  AI  in\ncredit scoring, policing, criminal procedure, or antitrust\nenforcement  can  hardly  be  initiated  by  a  company’s\nethics board or in-house philosopher. Their role remains\nconfined to steering, reviewing, and advising on policies\nand  product  launches  within  the  confines  of  existing\nbusiness models, so as to preserve those business models.\nFor example, in June 2020, IBM publicly announced it\nwould stop offering general purpose facial recognition\nor  analysis  software.[81] This  move,  which  was  a\nsignificant departure from IBM’s long-standing position\non  facial  recognition  and  was  followed  by  similar\nannouncements by Amazon and Microsoft, came as a\nresult  of  external  political  pressures  in  the  wake  of\nGeorge Floyd’s death in Minneapolis, not as a result of\nthe company’s internal ethical compliance processes.[82]\nYet it is precisely at moments of political and moral\nbreakdown, where a company’s activities and general\ngoals  clearly  come  into  conflict  with  the  interests  of\nsociety, that ethics can acquire central importance[13] and\ncan provide a fruitful lens for evaluating and deciding the\nway forward. In most cases, instead, the breakthrough\npotential of ethics as a mechanism for learning from and\nfacing dilemmas and contradictions is missed. As long\nas the ultimate decision-maker on any given AI policy is\nthe company itself, as long as internal ethics programs\nare focused on rhetoric more than on substance, these\ninitiatives will keep benefiting the industry more than\nusers and their instrumental value for society is limited.\nThe second critique of so-called ethics washing looks\nat the act of engaging in these efforts by philosophers-in-\nresidence, or members of ethics boards, and examines\nthe  intrinsic  or  independent  value  of  these  people’s\nengagement in moral thinking. Moral philosophy as a\npractice  has  value  when  followed  in  pursuit  of\nindependently valuable goals such as truth, justice, or the\nwell-being  of  society.  To  be  intrinsically  valuable,\nengaging  in  moral  argument  must  be  done  to  a\nsubstantial extent out of commitment to moral principle,\nin the belief that it can lead to a better understanding of\nmoral questions. If instead it is undertaken for the sake\nof  earning  money,  pleasing  employers,  or  obtaining\nhonors  and  recognitions,  it  loses  some  of  its  special\nworth.\nWe might think that this critique is about the actual\nmotivations of the philosophers and experts that engage\nin the exercise. When looking at cases of philosophers-\nin-residence,  ethics  boards,  or  academics  who  work\nclosely with these companies, there are doubtless some\nindividuals  who  do  it  to  raise  their  profile  or  create\nconnections that can lead to further work in the field, or\neven to obtain promotions, honors, or greater impact and\nsalience  for  their  work.  Yet  many  also  do  it  simply\nbecause they believe that their involvement might lead\nto  a  positive  overall  impact  or  in  the  hope  of  getting\ninsights into how the company works. It is tempting to\nfocus  on  these  people’s  intentions  and  blame  their\nshortsighted mindsets, but focusing on intentions seems\nunhelpful: the road to hell is paved with good intentions.\nTo better characterize the independent value of ethics-\nbased work, we must look beyond intentions and instead\nat scope: actual commitment to moral principle requires\nquestioning  what  an  employer  requires.  Philosophical\nthinking  must  have  the  potential  to  reach  beyond  the\nlimits imposed by companies in corporate settings. For\nexample,  saying  that  a  facial  recognition  algorithm\nshould be reviewed because it systematically identifies\nwhite people more accurately than black people seems\nright but is not sufficient. Rectifying bias requires more\nthan acknowledging that the algorithm needs “fixing”. It\nrequires making sure that the algorithm is not deployed\nin settings where it might cause irreparable harm to black\npeople.  It  also  possibly  involves  thinking  about\npreventing the use of such algorithms by the police, or\nby  society  at  large,  and  replacing  them  with  human\ndecision-making.[10, 56] To the extent an ethics board or\nin-house philosopher engages in moral argument with a\nview to correcting the algorithm yet is prevented from\nconsidering\n or\n voluntarily\n ignores\n these\n other\nconsiderations,  their  moral  inquiry  seems  to  lack\n    278\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nsubstantive  independent  value.  Philosophical  inquiry\nachieves its full potential only when it comes with full\nand  unrestricted  substantive  commitment  to  moral\nprinciple and justice.\nThird  and  finally,  even  if  these  efforts  did  have\nintrinsic  and/or  instrumental  value,  the  expression\n“ethics washing” denotes a particular epistemic function\nof  the  activities  in  question  which  requires  distinct\nanalysis. Ethics rhetoric, as it is funded and constructed\nin academic and corporate circles, may have the effect\nof freezing popular imagination and of preventing the\nemergence of valuable alternatives.[68] It may promote\nand  reinforce  a  narrow  and  confined  vision  of  the\npossibilities for regulatory and societal change.\nIt can, for example, mislead the public into believing\nthat  previously  contested  policies  have  now  become\nacceptable,  thus  creating  a  legitimacy  buffer  for\nobjectionable  corporate  action.  Immunizing  corporate\naction from public scrutiny is dangerous for more than\none  reason:  apathy  strengthens  corporations  and\nweakens activists, it shifts the burden of policing new\ntechnologies from deep-pocketed security and defense\ndepartments  and  private  companies  to  poorly  funded\nactivist groups and other marginalized stakeholders. It\ncan  also  discredit  awareness-enhancing  efforts  and\nnarrow the spectrum of contestation and debate. Self-\nregulatory  efforts,  such  as  the  example  of  the  FOB\nprovided above, tend to narrow the scope of a debate,\nmarginalizing  questions  of  structural  injustice  or\ndisruptive  change  and  instead  centering  attention  on\nprocedural  fairness  and  fixable  tweaks.  This—\npredictably—ends  up  favoring  incumbents.  Although\nthe  performative  dimensions  of  ethics  washing  are\nhardly visible by a majority of consumers, they are in fact\ncrucial  to  a  comprehensive  analysis  of  corporate  and\ngovernmental stakeholders’ strategies in this space and\nof the moral value and acceptability of their efforts.\nOverall,  an  analysis  from  the  perspective  of  moral\nphilosophy confirms the view of many critics of ethics\nwashing efforts. It helps us see many of these in-house\nethics initiatives as lacking significant instrumental and\nintrinsic  value  and  also  as  playing  a  performative\nfunction that can negatively affect persons. There are no\ndoubt exceptions of companies really working to ensure\nthat internal ethical work is independent and valuably\ncontributes to a more just society. However, in general\npolicymakers  should  not  overlook  the  salience  and\nweight of these critiques of ethics as a self-interested\nrhetoric.  Many  existing  internal  efforts  to  construct  a\ncorporate ethics, particularly around AI, largely remain\na façade.\n9    Avoiding Ethics Bashing\nIf the reasons for criticizing and resisting ethics washing\nare ones found within moral philosophy, where does this\nleave us on the role of moral philosophy? How should\nwe  understand  corporate  ethics?  Two  main  fallacies\nseem  at  play  in  overbroad  critiques  of  ethics  that  see\nethics  as  distinct  from  law,  politics,  justice  or  social\norganizing: a linguistic misunderstanding, that is to say\nthe conflation of instrumentalized ethics washing efforts\nwith  moral  philosophy  as  a  reflexive  exercise,  and\nignorance  of  or  resistance  to  the  possibilities  and\nimportance  of  moral  philosophy  as  a  discipline  and\nmethod.\nThe  linguistic  misunderstanding  is  due  to  what  the\nauthor has described above as companies’ cooptation of\nthe language and performative function of “ethics” to\npursue  self-promotional  goals.  Instrumentalized  and\nemptied  of  its  instrumental  and  intrinsic  value,  what\nremains  of “ethics” is  an  empty  construct  trapped\nbetween meanings and signifying timid instances of self-\nregulation, static and finite lists of guiding principles,\nand other forms of narrow and conservative regulative\n“fixes”.  None  of  these  embodied  instances  of  the\npractice of ethics are actually likely to be fully morally\ndefensible, but as the word quickly gains traction, it gets\ndefended or criticized at face value by corporations and\ncritics alike. These dynamics further entrench the misuse\nand  instrumentalization  of  ethics  language.  In  policy\ncircles,  the  word  becomes  a  red  herring,  a  mode  of\ngovernance  or  a  communications  strategy  to  dismiss.\nYet the misunderstanding at bottom is this: what is called\n“ethics” may have nothing “ethical” in it. It may have no\nintrinsic value for those who perform it and may have\ninstrumental value only for those who commission it and\nnot for society at large.\nMuch of the ink used to bash “ethics” was perhaps\njustified  but  it  could  have  been  used  more  wisely  by\ndistinguishing corporate ethics, or ethics washing, from\nthe  practice  of  moral  philosophy.  We  too  frequently\nneglect that “ethics” can and must encompass more than\nwhat companies make of it: that properly contextualized,\nethics can be a valuable methodology for rethinking the\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n279    \n \n\ncompeting or complementary merits of different kinds\nof regulation, including self-regulation and other forms\nof law and policy-making.\nA richer critique of corporate self-regulatory efforts\ntherefore  demands  that  we  operate  at  two  levels:  be\ncritical of ethics washing, while also being aware that our\nvery critique positions ourselves distinctly within moral\nphilosophy.  In  other  words,  when  criticizing  certain\npractices we necessarily adopt a distinct moral stance\nthat is within moral philosophy—not outside of it. We\nmust thus be ready to engage more thoroughly with the\nflaws of narrow approaches to ethics and to accept that\ndefending more capacious ethical stances is related to a\nbetter understanding and awareness of moral philosophy’s\npotential—not  a  blank  rejection  of  it  as  a  language,\npractice, discipline, and mode of inquiry. This requires\na deep societal reckoning with the values and limits of\nmoral philosophy.\nTo change tech ethics, it is urgent to rethink the way\ntechnology  ethics  comes  to  exist  and  is  talked  about.\nSince ethics washing is broadly antithetic to meaningful\nand capacious ethics, it is important for policy change to\noriginate  primarily  outside  formal  and  informal\ncorporate  settings.  To  be  effective,  the  role  of\nphilosophers,  boards,  and  other  formalized  bodies\nconcerned to bring about ethical AI must be re-imagined,\ntheir scope of action and mandate must extend outside\nthe corporate walls of companies such as Google or IBM,\nthey  cannot  be  exclusively  or  primarily  funded  by\ncompanies such as Facebook or Palantir, they must to the\nextent possible safeguard themselves from opportunistic\ncorporate  discourse  around “ethical  AI”.  A  deep\nreinvention  of  the  structures,  processes  and  modes  of\ngovernance  through  which  technological  impacts  on\nsociety  are  evaluated  is  urgent.  At  their  core,  these\nprocesses  must  facilitate  the  moral  evaluation,\nquestioning,\n and\n constant\n re-assessment\n of\ntechnological\n developments.\n Far\n from\n treating\ntechnological  developments  as  moments  of  ethical\nbreakdown, technology as a whole must be seen as a\nsystem  that  endemically  tends  toward  societal\nbreakdown,  and  therefore  requires  constant  reflexive\nreconsideration, revision, and re-imagination.\nCriticized  as  complex,  abstract,  apolitical,  and\nmisleadingly  neutral  or  objective,  philosophy  is\nfrequently dismissed in areas such as technology policy\nwhich are fast moving, full of ideological conflicts, and\nin need of quick and effective responses. However, it is\nclear that quick and effective fixes are not the answer.\nIdeological conflicts and the pace of innovation are not\nbarriers  to  doing  more  impactful  and  valuable\nphilosophical  work  in  this  sector.  Indeed,  the  current\ntechnological\n zeitgeist\n of\n strong\n resistance\n to\nsurveillance  capitalism;  new  data  privacy  laws;  the\ncomplicated relationship between big tech, big oil, and\nclimate  justice;  tech  employee  movements  and\nwhistleblowing;  COVID-19  and  Black  Lives  Matter\nsuggests that something within technology is changing,\nand  that  it  is  time  we  adopt  new  tools  and  modes  of\nthinking to fight technological injustice. What the tech\necosystem is in greatest need of today, in fact, seems to\nbe a slower, richer, more comprehensive investigation\nof what various technology companies and stakeholders\nowe  to  humans,  to  animals,  and  to  the  planet.  New\ntechnologies  are  also  making  us  reinvestigate  and\nquestion the commitments we humans owe to each other,\nas  well  as  to  other  beings  and  to  the  global  planet\necosystem. This is precisely what moral philosophy is\nfor. We may want to stop bashing it and instead invest in\nre-imagining it.\n10    Conclusion\nThis article has argued that ethics washing and ethics\nbashing  are  both  reductive  tendencies  that  rely  on  a\nlimited  understanding  of  what  ethics  actually  entails.\nEthical  reasoning  or  moral  inquiry  can  have  intrinsic\nvalue as a process and instrumental value as a means to\nthe achievement of other valuable outcomes. The author\nhas argued that the more ethics is used in tech circles as\na performative façade and the more it is instrumentalized\nand voided of its intrinsic reflexive value, the less value\nethics can have overall as a practice and mode of inquiry.\nAdopting  a  perspective  internal  to  moral  philosophy\nhelps us see the limits and actual similarities of what\nseem  like  polar  opposites—ethics  washing  and  ethics\nbashing—as  two  instances  of  instrumentalized  ethics\nlanguage.\nThe way to combat ethics washing, therefore, is not to\ninstrumentalize,  reduce,  and  then  dispose  of  ethical\nlanguage,  but  rather  to  distinguish  performative  and\ninstrumentalized  forms  of  ethics  from  valuable\ncommitments\n to\n moral\n principle\n that\n promote\nadvancements  in  self-knowledge,  understanding,  and\nsocial change. Although philosophers might never fully\n    280\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nadapt  their  methodology  to  fast-paced  and  politicized\ntechnology  environments,  we  cannot  disregard  the\nimmense depth and richness that philosophy can bring to\nany debate, not least ones about technology governance.\nWe all ask moral questions as part of our daily pursuits.\nTechnology scholars and policymakers should embrace\nmoral philosophy and value its porous, principled, and\nopen-ended richness, yet resist its instrumentalization or\nreduction to a performative ethics. Moral philosophers\nshould take on the difficult task of rethinking how new\ntechnologies  interact  with  humans  so  as  to  provide\nanswers to questions in urgent need of theorization. We\nall ask moral questions as part of our daily pursuits. To\navoid  falling  into  reductive  epistemic  and  ideological\ntraps, it is everyone’s duty to nourish curiosity for ethics’\nand  moral  philosophy’s  role  in  tech  and  beyond.\nHowever,  before  we  can  re-center  attention  on\ntechnology  ethics,  value  it  in  our  daily  pursuits,  and\nrenew  interest  in  the  interconnections  between  moral\nphilosophy, justice, politics, and law, it is urgent to de-\ncenter  the  structures  for  engaging  in  theoretical  and\nethical  thinking  from  corporate  settings.  Making  a\ncommitment  to  moral  principle  in  technology  is\nimpossible without a new governance framework that\nensures that ethics in technology remains independent\nand capacious.\nAcknowledgment\nE. Bietti thanks Jeff Behrends, Yochai Benkler, Brian\nBerkey,  Reuben  Binns,  Mark  Budolfson,  Urs  Gasser,\nBen  Green,  Lily  Hu,  Lucas  Stanczyk,  Luke  Stark,\nJonathan Zittrain, and some anonymous reviewers for\ntheir valuable input on this article.\nReferences\n K. Walker, An external advisory council to help advance\nthe  responsible  development  of  AI, https://blog.google/\ntechnology/ai/external-advisory-council-help-advance-\nresponsible-development-ai/, 2019.\n[1]\n Googlers  Against  Transphobia  and  Hate,  Google  must\nremove Kay Coles James from its Advanced Technology\nExternal Advisory Council (ATEAC), https://medium.com/\n@against.transphobia/googlers-against-transphobia-and-\nhate-b1b0a5dbf76, 2019.\n[2]\n S. Levin, Google scraps AI ethics council after backlash:\n“Back  to  the  drawing  board”, https://www.theguardian.\ncom/technology/2019/apr/04/google-ai-ethics-council-\nbacklash, 2019.\n[3]\n T. Gebru, I was fired by @JeffDean for my email to Brain\nwomen and Allies. My corp account has been cutoff. So\n[4]\nI've\n been\n immediately\n fired, \nhttps://twitter.com/\ntimnitGebru/status/1334352694664957952, 2020.\n E.  M.  Bender,  T.  Gebru,  A.  McMillan-Major,  and  S.\nShmitchell,  On  the  dangers  of  stochastic  parrots:  Can\nlanguage  models  be  too  big?  in Proc.  the  2021  ACM\nConference\n on\n Fairness,\n Accountability,\n and\nTransparency (FAccT ’21), Virtual Event, Canada, 2021,\npp. 610–623.\n[5]\n C.  Newton,  The  withering  email  that  got  an  ethical  AI\nresearcher fired at Google, https://www.platformer.news/p/\nthe-withering-email-that-got-an-ethical, 2020.\n[6]\n J.  Vincent,  Google  is  poisoning  its  reputation with  AI\nresearchers, https://www.theverge.com/2021/4/13/22370158\n/google-ai-ethics-timnit-gebru-margaret-mitchell-firing-\nreputation, 2021.\n[7]\n B.  Wagner,  Ethics  as  an  escape  from  regulation:  From\nethics-washing  to  ethics  shopping?  in Being  Profiled:\nCogitas  Ergo  Sum:  10  Years  of  Profiling  the  European\nCitizen, E. Bayamlioğlu, I. baraliuc, L. Janssens, and M.\nHildebrandt,\n eds.\n Amsterdam,\n the\n Netherlands:\nAmsterdam University Press, 2018, pp. 84–89.\n[8]\n T. Harris, http://www.tristanharris.com/, 2021.\n[9]\n J. Powles and H. Nissenbaum, The seductive diversion of\n‘solving’ bias in artificial intelligence, https://medium.com/\ns/story/the-seductive-diversion-of-solving-bias-in-arti-\nficial-intelligence-890df5e5ef53, 2018, .\n[10]\n D.  Greene,  A.  L.  Hoffmann,  and  L.  Stark,  Better,  nicer,\nclearer, fairer: A critical assessment of the movement for\nethical  artificial  intelligence  and  machine  learning,  in\nProc.  of  the  52nd Hawaii  International  Conference  on\nSystem  Sciences,  Honolulu,  HI,  USA,  2019,  pp.\n2122–2131.\n[11]\n P.  Nemitz,  Constitutional  democracy  and  technology  in\nthe\n age\n of\n artificial\n intelligence, \nPhilosophical\nTransactions  of  the  Royal  Society  A,  vol. 376,  no. 2133,\np. 20180089, 2018.\n[12]\n J.  Metcalf,  E.  Moss,  and  D.  Boyd,  Owning  ethics:\nCorporate\n logics,\n Sillicon\n Valley,\n and\n the\ninstitutionalization  of  ethics, Social  Research: An\nInternational Quarterly, vol. 82, no. 2, pp. 449–476, 2019.\n[13]\n B. Green, Data science as political action: Grounding data\nscience in a politics of justice, Journal of Social Computing,\ndoi:10.23919/JSC.2021.0029.\n[14]\n H.  L.  A.  Hart, The  Concept  of  Law,  Oxford,  UK:\nClarendon Press, 1961.\n[15]\n L.  Hu,  Tech  ethics:  Speaking  ethics  to  power,  or  power\nspeaking  ethics? Journal  of  Social  Computing,  doi:\n10.23919/JSC.2021.0033.\n[16]\n J.  E.  McNealy,  Framing  and  the  language  of  ethics:\nTechnology,  persuasion,  and  cultural  context, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0027.\n[17]\n L.  Taylor  and  L.  Dencik,  Constructing  commercial  data\nethics,  2020, https://doi.org/10.26116/techreg.2020.001,\n2020.\n[18]\n A.  L.  Hoffmann,  Where  fairness  fails:  Data,  algorithms,\nand the limits of antidiscrimination discourse, Information\nCommunication  and  Society,  vol. 22,  no. 7,  pp. 900–915,\n2019.\n[19]\n C.  Grannan,  What’s  the  difference  between  morality\nand\n ethics? \nEncyclopedia\n Britannica, \nhttps://www.\n[20]\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n281    \n \n\nbritannica.com/story/whats-the-difference-between-morality-\nand-ethics, 2021.\n D.  D.  Runes, The  Dictionary  of  Philosophy.  New  York,\nNY, USA: Philosophical Library, 1983.\n[21]\n T.  Scanlon, What  We  Owe  to  Each  Other.  Cambridge,\nMA, USA: Harvard University Press, 1998.\n[22]\n R.  Dworkin, Justice  for  Hedgehogs.  Cambridge,  MA,\nUSA: Belknap Press, 2011.\n[23]\n R.  Dworkin, Law’s  Empire.  Cambridge,  MA,  USA:\nBelknap Press, 1986.\n[24]\n O.  Tene  and  J.  Polonetsky,  A  theory  of  creepy:\nTechnology,  privacy,  and  shifting  social  norms, Yale\nJournal of Law, and Technology, vol. 16, no. 1, p. 2, 2014.\n[25]\n J.  Rawls, A  Theory  of  Justice.  Cambridge,  MA,  USA:\nHarvard University Press, 1971.\n[26]\n J. Cobbe and E. Bietti, Rethinking digital platforms for the\npost-COVID-19  era, https://www.cigionline.org/articles/\nrethinking-digital-platforms-post-covid-19-era, 2020.\n[27]\n J.  Cheung,  Real  estate  politik:  Democracy  and  the\nfinancialization  of  social  networks, Journal  of  Social\nComputing, doi: 10.23919/JSC.2021.0030.\n[28]\n C.  Pateman  and  C.  Mills, Contract  and  Domination.\nMalden, MA, USA: Polity Press, 2007.\n[29]\n B. J. Grosz, D. G. Grant, K. Vredenburgh, J. Behrends, L.\nHu,  A.  Simmons,  and  J.  Waldo,  Embedded  EthiCS:\nIntegrating  ethics  across  CS  Education, Communications\nof the ACM, vol. 62, no. 8, pp. 54–61, 2019.\n[30]\n C.  Fiesler,  N.  Garrett,  and  N.  Beard,  What  do  we  teach\nwhen  we  teach  tech  ethics?:  A  syllabi  analysis,  in Proc.\nthe 51st ACM Technical Symposium on Computer Science\nEducation  (SIGCSE ’20),  Portland,  OR,  USA,  2020,  pp.\n289–295.\n[31]\n R.  Reich,  M.  Sahami,  J.  M.  Weinstein,  and  H.  Cohen,\nTeaching  computer  ethics:  A  deeply  multidisciplinary\napproach, in Proc. the 51st ACM Technical Symposium on\nComputer  Science  Education,  Portland,  OR,  USA,  2020,\npp. 296–302.\n[32]\n R.  Ferreira  and  M.  Y.  Vardi,  Deep  tech  ethics:  An\napproach to teaching social justice in computer science, in\nProc.  the  52nd ACM  Technical  Symposium  on  Computer\nScience  Education  (SIGCSE  '21),  Virtual  Event,  USA,\n2021, pp. 1041–1047.\n[33]\n N. Fraser, Rethinking the public sphere: A contribution to\nthe  critique  of  actually  existing  democracy, Social  Text,\nno. 25/26, pp. 56–80, 1990.\n[34]\n M. Hildebrandt, Closure: On ethics, code and law, in Law\nfor Computer Scientists and Other Folk, M. Hildebrandt,\ned. Cambridge, MA, USA: Oxford University Press, 2020,\npp. 283–318.\n[35]\n J. Taplin, Move Fast and Break Things: How Facebook,\nGoogle, and Amazon Cornered Culture and Undermined\nDemocracy.  New  York,  NY,  USA:  Little,  Brown  and\nCompany, 2017.\n[36]\n P. Foot, The problem of abortion and the doctrine of the\ndouble effect, Oxford Review, vol. 5, pp. 5–15, 1967.\n[37]\n E.  Awad,  S.  Dsouza,  R.  Kim,  J.  Schulz,  J.  Henrich,  A.\nShariff, J-F. Bonnefon, and I. Rahwan, The moral machine\nexperiment, Nature, vol. 563, no. 7729, pp. 59–64, 2018.\n[38]\n E.  Awad,  S.  Dsouza,  A.  Shariff,  J.  -F.  Bonnefon,  and  I.\nRahwan, Crowdsourcing moral machines, Communications\nof the ACM, vol. 63, no. 3, pp. 48–55, 2020.\n[39]\n A.  E.  Jaques,  Why  the  moral  machine  is  a  monster,\nhttps://robots.law.miami.edu/2019/wp-content/uploads/2019/03/\nMoralMachineMonster.pdf, 2019.\n[40]\n J. Basl and J. Behrends, Why everyone has it wrong about\nthe  ethics  of  autonomous  vehicles,  in Frontiers  of\nEngineering  Reports  on  Leading-Edge  Engineering  from\nthe 2019  Symposium,  National Academy  of  Engineering,\ned.  Washington,  DC,  USA:  The  National  Academies\nPress, 2020, pp. 75–82.\n[41]\n J.  Fjeld,  N.  Achten,  H.  Hilligoss,  A.  C.  Nagy,  and  M.\nSrikumar,  Principled  artificial  intelligence:  Mapping\nconsensus  in  ethical  and  rights-based  approaches  to\nprinciples  for  AI, Berkman  Klein  Center  Research\nPublication  No.  2020-1,  doi: http//dx.doi.org/10.2139/\nssrn.3518482 .\n[42]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  technology  ethics  in  practice, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0018.\n[43]\n B.  Mittelstadt,  Principles  alone  cannot  guarantee  ethical\nAI, Nature  Machine  Intelligence,  vol. 1,  pp. 501–507,\n2019.\n[44]\n H. Nissenbaum, Privacy in Context. Stanford, CA, USA:\nStanford University Press, 2009.\n[45]\n S.  Prideaux, I  Am  Dynamite!  A  Life  of  Nietzsche. New\nYork, NY, USA: Tim Duggan Books, 2018.\n[46]\n C. Baumann, Was Hegel an authoritarian thinker? Reading\nHegel's Philosophy  of  History on  the  basis  of  his\nmetaphysics, Archiv  für  Geschichte  der  Philosophie,\nvol. 103, no. 1, pp. 120–147, 2019.\n[47]\n M.  Foucault, Naissance  de  la  Biopolitique:  Cours  au\nCollège de France, 1978–1979. Paris, France: Editions du\nSeuil, 2004.\n[48]\n M. Rosen, The Marxist critique of morality and the theory\nof  ideology,  in Morality,  Reflection  and  Ideology,  E.\nHarcourt,  ed.  Cambridge,  MA,  USA:  Oxford  University\nPress, 2000, pp. 21–43.\n[49]\n J.  Zigon,  Moral  breakdown  and  the  ethical  demand:  A\ntheoretical  framework  for  an  anthropology  of  moralities,\nAnthropological Theory, vol. 7, no. 2, pp. 131–150, 2007.\n[50]\n J.  Habermas,  Reconciliation  through  the  public  use  of\nreason: Remarks on John Rawls's political liberalism, The\nJournal of Philosophy, vol. 92, no. 3, pp. 109–131, 1995.\n[51]\n K.  Dotson,  How  is  this  paper  philosophy? Comparative\nPhilosophy, vol. 3, no. 1, pp. 3–29, 2012.\n[52]\n L. Winner, Do artifacts have politics? Daedalus, vol. 109,\nno. 1, pp. 121–136, 1980.\n[53]\n M.  Hildebrandt, Smart  Technologies  and  the  End(s)  of\nLaw:  Novel  Entanglements  of  Law  and  Technology.\nCheltenham, UK: Edward Elgar Publishing Limited, 2015.\n[54]\n J. Angwin, J. Larson, S. Mattu, and L. Kirchner, Machine\nBias:  There’s  software  used  across  the  country  to\npredict  future  criminals.  And  it’s  biased  against  blacks,\nhttps://www.propublica.org/article/machine-bias-risk-\nassessments-in-criminal-sentencing, 2016.\n[55]\n C. O’Neill, Weapons of Math Destruction: How Big Data\nIncreases  Inequality  and  Threatens  Democracy.  New\nYork, NY, USA: Broadway Books, 2017.\n[56]\n S.  U.  Noble, Algorithms  of  Oppression:  How  Search\nEngines  Reinforce  Racism.  New  York,  NY,  USA:  NYU\nPress, 2018.\n[57]\n V. Eubanks, Automating Inequality: How High-Tech Tools\n[58]\n    282\nJournal of Social Computing, September 2021, 2(3): 266−283    \n \n\nProfile,  Police  and  Punish  the  Poor.  New  York,  NY,\nUSA: St Martin’s Press, 2018.\n R.  Benjamin, Race  After  Technology:  Abolitionist  Tools\nfor the New Jim Code. Cambridge, UK: Polity Press, 2019.\n[59]\n S.  Constanza-Chock, Design  Justice:  Community-Led\nPractices to Build the Worlds We Need. Cambridge, MA,\nUSA: MIT Press, 2020.\n[60]\n C.  D’Ignazio  and  L.  Klein, Data  Feminism.  Cambridge,\nMA, USA: MIT Press, 2020.\n[61]\n R.  Foroohar,  Year  in  a  word:  Techlash, https://www.ft.-\ncom/content/76578fba-fca1-11e8-ac00-57a2a826423e,\n2018.\n[62]\n K.  Shilton,  M.  Zimmer,  C.  Fiesler,  A.  Narayanan,  J.\nMetcalf, M. Bietz, and J. Vitak, We’re awake —but we’re\nnot at the wheel, https://medium.com/pervade-team/were-\nawake-but-we-re-not-at-the-wheel-7f0a7193e9d5, 2017.\n[63]\n T.  Rees,  Why  tech  companies  need  philosophers—and\nhow  I  convinced  Google  to  hire  them, https://perma.cc/\n2967-8H5R, 2019.\n[64]\n Google,  Artificial  intelligence  at  Google:  Our  principles,\nhttps://ai.google/principles/, 2020.\n[65]\n IBM,  Report:  Advancing  AI  ethics  beyond  compliance,\nhttps://www.ibm.com/thought-leadership/institute-\nbusiness-value/report/ai-ethics, 2020.\n[66]\n Microsoft,  Responsible  AI, https://www.microsoft.com/\nen-us/ai/responsible-ai?activetab=pivot1:primaryr6, 2020.\n[67]\n R.  Ochigame,  How  big  tech  manipulates  academia  to\navoid  regulation, https://theintercept.com/2019/12/20/mit-\nethical-ai-artificial-intelligence/, 2019.\n[68]\n AI  Multiple,  AI  consulting:  In-depth  guide  with  top  AI\nconsultants  of  2020, https://research.aimultiple.com/ai-\nconsulting/2020, 2020.\n[69]\n Clifford Chance, Tech Group, https://www.cliffordchance.\ncom/hubs/tech-group-hub/tech-group.html, 2020.\n[70]\n L.  Floridi,  Translating  principles  into  practices  of  digital\nethics:  Five  risks  of  being  unethical, Philosophy  and\nTechnology, vol. 32, pp. 185–193, 2019.\n[71]\n D.  Hume, A  Treatise  of  Human  Nature,  London,  UK:\nPenguin Classics, 1739.\n[72]\n S.  Viljoen,  The  promise  and  limits  of  lawfulness:\nInequality,  law,  and  the  techlash, Journal  of  Social\nComputing, doi: 10.23919/JSC.2021.0025.\n[73]\n J. Balkin, Free speech is a triangle, Colorado Law Review,\nvol. 118, p. 201, 2018.\n[74]\n A.  Shanor,  The  new  Lochner, Wisconsin  Law  Review,\nvol. 1, pp. 133–208, 2016.\n[75]\n C.  Botero-Marino,  J.  Greene,  M.  W.  McConnell,  and  H.\nThorning-Schmidt,  We  are  a  new  board  overseeing\nFacebook.  Here’s  what  we’ll  decide, https://www.\nnytimes.com/2020/05/06/opinion/facebook-oversight-\n[76]\nboard.html, 2020.\n E. Douek, Facebook’s “oversight board:” Move fast with\nstable infrastructure and humility, North Carolina Journal\nof Law and Technology, vol. 21, no. 1, pp. 1–78, 2019.\n[77]\n T. Kadri and K. Klonick, Facebook v. Sullivan: Building\nconstitutional law for online speech, Southern California\nLaw Review, vol. 93, p. 37, 2019.\n[78]\n S.  Vaidhyanathan,  Facebook  and  the  folly  of  self-\nregulation, \nhttps://www.wired.com/story/facebook-and-\nthe-folly-of-self-regulation/, 2020.\n[79]\n J. Jaffer and R. Krishnan, Clearview AI’s first amendment\ntheory\n threatens\n privacy—and\n free\n speech,\n too,\nhttps://slate.com/technology/2020/11/clearview-ai-first-\namendment-illinois-lawsuit.html, 2020.\n[80]\n IBM, IBM CEO’s letter to congress on racial justice reform,\nhttps://www.ibm.com/blogs/policy/facial-recognition-\nsusset-racial-justice-reforms/, 2020.\n[81]\n A. Smith, IBM will no longer develop facial recognition\ntechnology\n following\n George\n Floyd\n protests,\nhttps://www.independent.co.uk/life-style/gadgets-and-\ntech/news/ibm-facial-recognition-george-floyd-protests-\na9556061.html, 2020.\n[82]\n J.  Pesenti,  An  update  on  our  use  of  face  recognition,\nhttps://about.fb.com/news/2021/11/update-on-use-of-face-\nrecognition/, 2021.\n[83]\n Axon,  First  report  of  the  Axon  AI  ethics  board:  Face\nrecognition, https://www.policingproject.org/axon-fr, 2019.\n[84]\n A.  Papazolgou,  Silicon  Valley’s  secret  philosophers\nshould  share  their  work, https://perma.cc/6KZR-ASJ9,\n2019.\n[85]\n O. Williams, How big tech funds the debate on AI ethics,\nhttps://perma.cc/5999-57BW, 2019.\n[86]\nElettra Bietti is pursuing the PhD degree at\nHarvard  Law  School.  She  is  an  incoming\njoint  postdoctoral  fellow  at  NYU  Law’s\nInformation Law Institute and at the Digital\nLife  Initiative,  Cornell  Tech.  She  is\naffiliated  to  Harvard’s  Berkman-Klein\nCenter, Harvard’s Weatherhead Center and\nYale  Law  School’s  Information  Society\nProject.  Prior  to  her  doctorate,  she  was  a  competition  and\nintellectual  property  lawyer  in  London  and  Brussels,  handling\ncorporate transactions and patent disputes. She received the LLB\ndegree in law from University College London, the LLM degree\nfrom  Harvard  Law  School,  and  the  professional  diploma  in\nintellectual property law and practice from Oxford University in\n2011, 2012, and 2016, respectively, and is admitted to practice\nlaw in New York, NY, USA and England and Wales, UK.\n  Elettra Bietti:   From Ethics Washing to Ethics Bashing: A Moral Philosophy View on Tech Ethics\n283    \n \n\n \nThe Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\nSalomé Viljoen*\nAbstract:    In response to widespread skepticism about the recent rise of “tech ethics”, many critics have called\nfor legal reform instead. In contrast with the “ethics response”, critics consider the “lawfulness response” more\ncapable of disciplining the excesses of the technology industry. In fact, both are simultaneously vulnerable to\nindustry capture and capable of advancing a more democratic egalitarian agenda for the information economy.\nBoth ethics and law offer a terrain of contestation, rather than a predetermined set of commitments by which\nto achieve more democratic and egalitarian technological production. In advancing this argument, the essay\nfocuses  on  two  misunderstandings  common  among  proponents  of  the  lawfulness  response.  First,  they\nmisdiagnose the harms of the techlash as arising from law’s absence. In fact, law mediates the institutions that\nit enacts, the productive activities it encases, and the modes and myths of production it upholds and legitimates.\nSecond, this distinction between law’s absence and presence implies that once law’s presence is secured, the\nproblems of the techlash will be addressed. This concedes the legitimacy of the very regimes currently at issue\nin law’s own legitimacy crisis, and those that have presided over the techlash. The twin moment of reckoning\nin tech and law thus poses a challenge to those looking to address discontent with technology with promises\nof future lawfulness.\nKey  words:   law; technology; ethics; tech ethics; inequality; regulation\n“Laws have to determine what is legal, but you can not\nban  technology.  Sure,  that  might  lead  to  a  dystopian\nfuture or something, but you can not ban it.”\n−David Scalzo, Kirenaga Partners[1]\n \n“Ferment is abroad in the law.”\n−K. N. Llewellyn[2]\n1    The Techlash\nIn the past several years, the prevailing role of Silicon\nValley’s California Ideology as the source of hope and\ninspiration  for  the “Western  capitalist  imaginary” has\nbegun to falter[3]. No longer does the tech industry stand\nfor  the  propositions  of  inclusive  capitalism  and\ntechnological progress that benefit all. In the wake of\nFacebook’s\n Cambridge\n Analytica\n scandal\n the\ntechnology  industry  has  been  the  focus  of  increased\npublic distrust, civil and worker activism, and regulatory\nscrutiny—a collective curdling of goodwill referred to\nas the “techlash”.①\nThe techlash is remarkable for its depth of field. The\n2020  Edelman  Trust  Barometer  noted  a  continued\ndecline  in  trust  both  globally  and  in  the  U.S.  in\ntechnology  and  a  significant  distrust  of  artificial\nintelligence[4],  both  linked  to  increased  numbers  of\npeople who believe these sectors should be regulated.\nA 2019 study conducted by the Pew Research Center\n \n • Salomé Viljoen is with  Columbia Law School, New York City,\nNY 10019, USA. E-mail: sviljoen@law.columbia.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-23;\naccepted: 2021-11-25\n① The  techlash —and  the  inequality  it  is  a  response  to—is  a  global\nphenomenon. However, this piece will predominantly draw its examples\nand focus its analysis on the United States. This is in part a reflection of\nthe author’s expertise as a US legal scholar (law is a jurisdiction-specific\ndiscipline;  particularly  in  the  United  States).  But  it  is  also  due  to  the\nEssay’s  extensive  engagement  with—and  analytical  reliance  on—the\nparticularities of the U.S. common law judicial system. The role of courts\nin the U.S. system, as well as the specific legal intellectual tradition in and\nabout U.S. law, informs, constrains, and limits much of the discussion\nbelow.\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   06/06  pp284−296\nVolume 2, Number 3, September  2021\nDOI:  10.23919/JSC.2021.0025\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\nfound that from 2015 to 2019, the number of Americans\nwho  held  a  positive  view  of  technology  fell  by  21\npercentage points[5]. In 2018, a majority of Americans\n(55%) said tech companies have too much power and\ninfluence[5]. Former executives have spoken out against\ntheir company’s actions[6–8], and senior engineers and\ncivil  society  groups  have  called  for  moratoriums  or\noutright  bans  on  facial  recognition  technology,\nespecially  for  police  and  immigration  enforcement[9].\nStudent groups at universities have protested or banned\ncompanies  like  Palantir  recruiting  at  their  schools[10].\nCommunity  groups  have  pushed  to  dismantle  and\ndelegitimize the close ties between law enforcement and\nsurveillance technology companies[11]. The technology\nindustry has been the site of increased worker activism\nfrom  Amazon  warehouses  workers[12],  Uber  and  Lyft\ndrivers[13],  line  engineers  at  Google[14],  and  the  tech\nindustry writ large[15, 16]. Digital rights’ activists have\npressured  companies  about  their  policies  and  labor\npractices\n on\n everything\n ranging\n from\n content\nmoderation, polarization, lack of diversity, surveillance,\nand manipulative and extractive data collection practices.\nAlongside the popular backlash, technology’s harmful\nsocial  effects  have  become  the  subject  of  increased\nacademic inquiry. Scholars seek to diagnose and address\nthe  worst  excesses  of  industry  harm,  and  to  develop\ntechnical methods and fields of practice less conducive\nto  committing  them.  These  methods  produce  systems\nthat  are  normatively  relevant  to  the  areas  of  life  they\ngovern: they can amplify and reproduce inequality and\nentrench  unjust  means  of  social  ordering.  Scholars  of\nscientific  method  (science  and  techonlongy  studies,\nhistory  of  science,  philosophy  of  science,  and  critical\ndigital  studies),  as  well  as  computer  scientists  have\nhighlighted methodological limits in how algorithms are\ndeveloped and the need for interventions better attuned\nto the social causes and effects in which such systems are\nentangled[17].\n Increased\n attention\n to\n engineering\npedagogy  has  placed  renewed  attention  on  need  to\neducate  future  data  scientists  and  engineers  about  the\nethical and social dimensions of their work[18].\nThe  techlash  involves  significant  political  stakes.\nGrowing  worker  activism  and  agitation  at  companies\nlike Google and Amazon have led to these companies\nfiring  senior  engineers[19].  Oppressive  and  biased\ntechnologies such as facial recognition and the capacity\nof social media to manufacture dis- and misinformation\ncampaigns  are  being  used  by  authoritarian  regimes\nabroad and reactionaries at home[20, 21]. Companies like\nAirBnB and Uber erode workers’ rights and redistribute\nsignificant surplus wealth away from local renters and\nworkers[22, 23].  The  dominance  of  a  handful  of  large\ntechnology  companies  (Facebook,  Amazon,  Apple,\nMicrosoft, and Google) is spurring renewed debates over\nmarket concentration and monopoly. The pervasive data\ncollection,  processing,  and  analytic  practices  that\nundergird controversial technologies continue to erode\nour collective privacy (and contribute to the oppressive\npower of autonomous surveillance systems) amidst an\nindustry-wide gold rush for data[24].\nDigital  activism  is  not  new—in  the  United  States,\ngroups like the Electronic Frontier Foundation and the\nAmerican Civil Liberties Union have long advocated for\ncivil rights protections online. Yet these organizations\nhave traditionally focused on civil libertarian concerns\nover  privacy,  strong  free  speech  protections,  and\ngovernment  overreach.  As  a  result,  their  advocacy\nefforts  focused  on  issues  like  the  Edward  Snowden\nrevelations  over  extensive  US  security  surveillance\nprograms,  the  Digital  Millennium  Copyright  Act  and\nSection  230  of  the  Communications  Decency  Act.  In\neach  instance,  digital  advocates  defended  free  speech\n(and the absence of government surveillance necessary\nfor free speech to thrive) of users and content creators\nonline.  This  strain  of  digital  advocacy  emphasized\nprotecting  individuals’ online  freedom  but  did  not\ntypically focus on other forms of injustice, such as the\nwealth\n accumulation\n that\n motivated\n corporate\nadvertising-based  surveillance  practices  or  on  the\ndistributive or relational effects of the digital economy\nwrit  large.  In  short,  while  there  is  a  long  history  of\nconcern over surveillance online, this tradition of digital\nactivism did not historically focus on the social problems\nof  inequality  that  arise  because  of  surveillance-based\neconomic activity.\nThe  techlash,  on  the  other  hand,  evinces  marked\negalitarian\n concerns\n over\n the\n highly\n unequal\ndistributions  of  wealth  and  power  within  the  digital\neconomy. It expresses a rejection of the tech industry’s\njustificatory narrative for the inequality it generates: that\ntechnological progress on its terms will, in the long-run,\nbenefit  everyone.  There  is  growing  skepticism  over\ntechnological  advancement  as  a  project  of  shared\nprosperity  and  a  growing  understanding  of  the\ntechnology political economy as one that works to the\nbenefit  of  the  few  to  the  detriment  of  the  many[5, 25].\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n285    \n \n\nCritics  of  digital  technology  firms  argue  that  their\ntechnological progress relies upon extractive practices\nand oppressive purposes. This begs the question of how\nto achieve an alternative result, and what role (if any)\n“tech ethics” will play in achieving it.\n2    Ethics Response and Lawfulness Response:\nTroubling the Distinction\nIn the ensuing public debate, some have advocated for\ntech to become more “human”, and more “ethical”[18].\nOthers suspect that appeals to traditions of ethics and\nhumanism have less to do with the moral lessons such\ntraditions offer, and more to do with their rhetorical and\npublic-relations capacity to forestall legal and regulatory\naction[26–32]. Such debates set off second order debates\nover  whether  appeals  to “ethics” negate  rather  than\nrequire  regulatory  action[33, 34].  These  in  turn  spawn\ntertiary debates over what such appeals substantively or\nmaterially entail, under what conditions appeals become\ndemands, and who gets to decide what ethical practice\nmeans for the technology industry[18, 35, 36].\nThis  initial  emphasis  on “responsible”, “humane”,\n“human-centered”,  or “ethical” technology  and  the\nresulting set of discursive moves are all part of what I call\nthe ethics response. The ethics response has real power\nto marshal bureaucratic and material resources. The call\nfor  more  ethical  technology  has  spawned  a  series  of\nethics boards, company-funded corporate wellness and\nsocial responsibility initiatives, the rise of “ethical AI”\nconsultancy practices, and a flurry of publications that\noutline ethical AI principles for industry[18, 37, 38]. This\nresponse  has  received  much  attention  and  been  the\nsubject of considerable debate.\nAlongside  an  increased  emphasis  on  ethics,  a  risk-\naverse,  law-abiding  modus  operandi  pervades  the  C-\nsuites  of  Silicon  Valley  that  recalls  a  certain  attitude\namong banks post-2008 crisis: a patina of cowed mea\nculpa  alongside  assurances  that  lessons  have  been\nlearned. This second response is marked by an initial\ncommitment from executives that the era of “move fast\nand  break  things” is  over,  and  that  the  strictest\ninterpretation of legal protections will be followed. Like\nthe ethics response, this legalistic mode coalesces from\na particular set of discursive moves. Critics call for legal\ninvestigations, lawsuits, or new regulation. Companies\nseek  to  comply  with  these  calls  or  proactively  offer\nalternatives as simultaneous signal of compromise and\nseriousness. Like the ethics response, this response can\nmarshal  resources  for  meaningful  new  regulatory\nagendas. Companies change corporate governance and\nbusiness  practices[39, 40],  embrace  regulatory  agendas\nthey had previously fought[41, 42], and even join activist\ncalls for increased oversight and regulation[43, 44]. This\nattitude marks another strain of response to the techlash\nthat I call the lawfulness response.\nThe lawfulness response is often positioned in contrast\nto the ethics response as a more serious alternative[31–33].\nWhile critics view the ethics response as ineffectual (or\neven a harmful distraction), the lawfulness response is\noften  advanced  as  more  capable  of  disciplining  the\nexcesses of the technology industry: “we do not need\nethics, we need regulation.” And indeed, the lawfulness\nresponse\n generally\n accompanies\n companies’\nacquiescence to a more significant regulatory agenda.\nDepending on how such demands were articulated and\nthen  negotiated  by  industry  actors,  the  lawfulness\nresponse may result in private regulation—a change in\ncorporate governance or firm policy, often in response to\nthreatened  or  actual  litigation—or  legislative  action,\nwith companies joining advocates in calling for industry\nregulation. Where the ethics response is viewed as either\ntoo  vague  or  too  readily  co-opted  to  provide  a\nmeaningful form of discipline, the lawfulness response\nappears to offer a more robust vehicle for realizing the\nsocial demands of the techlash.\nDespite  this  perception,  the  ethics  and  lawfulness\nresponses  function  quite  similarly.  Like  the  ethics\nresponse, the lawfulness response may also yield anti-\negalitarian results. Cynical actors may appeal to law to\nseek  moral  cover  for  instituting  (and  then  complying\nwith) with a low standard of behavior. But well-meaning\ncritics  may  also  appeal  to  legal  solutions  that\ninadvertently legitimize the very business practices they\nseek  to  reform.  Similar  to  the  ethics  response,  the\ncapacity  for  the  lawfulness  response  to  discipline  the\ntechnology industry depends on its capacity to express\nand enforce egalitarian demands.\nTwo  examples  of  the  lawfulness  response  are\ninstructive.\nThe first involves Uber. In its early rise to prominence,\nUber  gained  considerable  notoriety  and  begrudging\nadmiration for operating at the edge of legality in pursuit\nof  rapid  and  aggressive  growth[45, 46].  In  2017,  this\nstrategy appeared finally to be catching up with Uber. In\nthat  year  alone,  Uber  faced  a  federal  criminal\n    286\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\ninvestigation  into  its  Project  Greyball② became\nembroiled in a legal fight with Waymo over its alleged\ntheft  of  self-driving  car  technology,  and  was\nexperiencing growing backlash from drivers over low\npay and poor working conditions[47–49]. In addition, the\ncompany  was  embroiled  in  allegations  of  sexual\nharassment  and  a  toxic  work  culture  for  women  and\nminorities[50].\n Many\n commentators\n thought\n this\ncollection of scandals marked the end of the company—\na fate that longstanding critics of Uber welcomed.\nFocusing  on  the  workplace  culture  allegations,  the\ncompany’s  board  of  directors  promptly  hired  former\nU.S. Attorney General Eric Holder (then at Covington\n& Burling) to conduct an internal investigation and issue\na report, a high-profile step that was extensively covered\nin the media. The report resulted in the board adopting\na series of corporate governance practices and ultimately\nfiring  then-CEO  Travis  Kalanick.  This  change  in\nleadership  and  attendant  set  of  institutional  changes\nwere generally understood to end the company’s “wild\nwest days” and to usher in a new era of a law-abiding\nUber  focused  on “ensur[ing]  a  tone  of  support  and  a\nculture  of  compliance”[40, 51].  In  line  with  this  new\nculture,  Uber  dropped  many  of  its  more  openly\naggressive tactics, such as Project Greyball.\nUber’s lawfulness response was an impressive display\nof threading the needle: it addressed the public attitude\nof Uber (as a deviant and morally suspect company) by\nsignaling  legal  seriousness,  while  keeping  intact  a\nbusiness  model  that  was  also  a  primary  subject  of\ncritique[51]. Focusing its response on workplace culture\nallegations at its headquarters, Uber drew fire away from\nits  continued  use  of  pricing  manipulations  and  other\ntechniques to squeeze profit from drivers.\nA second, more proactive example of the lawfulness\nresponse  is  Microsoft’s  approach  to  developing  facial\nrecognition  technology.  As  questionable  business\npractices of facial recognition companies have come to\nlight[1],  the  social  pressure  to  ban[52] or  place  a\nmoratorium[53] on  facial  recognition  technologies  has\ngrown—even  Alphabet  CEO  Sundar  Pichai  has\nsuggested a temporary moratorium on facial recognition\ntechnologies may be needed[43].\nMicrosoft has called for legalistic restraint as one way\nto temper concerns while continuing development. The\ncompany is publicly refusing to sell their technology to\nCalifornia police (citing Fourth Amendment concerns),\nendorsing  federal  regulation,  such  as  the  Commercial\nFacial Recognition Privacy Act, and introducing its six\nprinciples  for  facial  recognition  software  that  include\n“lawful  surveillance” and  prohibitions  against  use  for\n“unlawful discrimination”[54]. This middle path appeals\nto the restraint of law to narrow public critique of facial\nrecognition to its most egregious (and, it is proposed,\nunlawful) applications, while preserving other areas of\napplication intact. Microsoft’s chief legal officer Brad\nSmith likened a wholesale ban to “try[ing] to solve a\nproblem  with  a  meat  cleaver” when  a “scalpel” is\nrequired  to “enable  good  things  to  get  done  and  bad\nthings  to  stop  happening”[9].  The  lawfulness  response\nprovides  precisely  such  a  scalpel-like  approach:  a\ncautious-yet-optimistic\n program\n of\n continued\ndevelopment of facial recognition technology under the\nguiderails  of  existing  law.  As  Smith  notes, “This  is\nyoung technology. It will get better. But the only way to\nmake it better is actually to continue developing it. And\nthe only way to continue developing it actually is to have\nmore people using it”[9].\nAs these two examples show, the turn to lawfulness\nduring moments of popular backlash serves an important\nrole for companies. In the case of Uber, bringing in a\nhigh-profile  legal  investigator  like  Eric  Holder—the\nembodiment of a trusted form of lawful authority, the\nObama  Justice  Department—shifted  perception  of  the\ncompany  from  lawless  adolescence  to  reformed  and\nresponsible  corporate  adulthood,  while  preserving  its\ncore business model. In the case of Microsoft, faced with\na far more aggressive regulatory alternative in the form\nof bans or moratoriums, the company emphasized the\nimportance of continued, yet responsible, development\nof the technology. This tack grants Microsoft the ability\nto craft through law a basis for its own legitimacy: by\nproceeding with its business under the imprimatur of law,\nthe  company  may  reap  the  financial  benefits  of  the\ntechnology without suffering reputational harm. In both\nexamples, the lawfulness response is marshaled to chart\na middle path, softening calls for abolition—of an entire\n② Beginning around 2014, Uber used a program called Greyball. It\noperated this scheme in cities like Boston, Paris, Portland, and countries\nlike Australia and China—all places Uber had been restricted or banned—\nto  evade  detection  by  using  geo-fencing  around  government  buildings\nand “greyballing” users identified as law enforcement or city officials[47].\nWhile approved by Uber’s general counsel at the time, other legal experts\nthought the program may constitute a violation of the Computer Fraud\nand Abuse Act or an act of intentional obstruction of justice, and a federal\ncriminal investigation into the company’s misleading tactics with local\nregulators soon followed.\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n287    \n \n\nbusiness  model  or  a  technology—into  steps  for\ncontinuation,  just  in  a  more  procedurally  robust  and\naccountable manner.\nThe lawfulness response offers companies a pathway\nto regain or retain legitimacy for their business in the face\nof  accusations  of  injustice.  It  does  so  in  part  by\ncollapsing  the  distinction  between  lawfulness  and\nlegitimacy in the company’s actions. This separates out\nunlawful/illegitimate  actions  from  lawful/legitimate\nones—an  important  separation  that  distances  those\npractices that are of central importance to a company’s\nbusiness from those that are not. By dealing seriously\nwith  the  unlawful/illegitimate  practices,  the  category\ndistinction between these practices and the rest of the\nbusiness  is  reinforced.  This  reinforced  separation  has\nsignificant  material  stakes.  In  the  case  of  Uber,  the\nlawfulness  response  undergirds  an  all-important\ndistinction for the company: that sexual harassment at\nwork  is  illegal,  whereas  harsh  contracting  terms  for\nindependent contractors are not. In the case of Microsoft,\nthis\n distinction\n is\n proactive—a\n campaign\n to\ndisambiguate  the  illegitimate/unlawful  uses  of  facial\nrecognition  (backroom  deals  with  law  enforcement,\nwarrantless searches), from the legitimate/lawful ones\n(a  category  the  company  argues  requires  further\nexploration). The unlawful actions thus identified and\naddressed, the company’s remaining actions regain or\nretain legitimacy.\n3    Lawfulness As Anti-Regulatory\nLending credence to the lawfulness response is that a\ncorollary version of it—what I call the legalist-reform\nresponse—is  accepted  and  even  championed  among\nsome  of  big  tech’s  fiercest  critics.  When  such  critics\nemphasize the lawlessness of company actions, it sets up\ntechnology  companies  to  reply  credibly  to  popular\nfrustrations with the lawfulness response.\nThe  legalist-reform  response  suffers  from  two\nlimitations  as  a  strategy  for  democratic  egalitarian\nreform. First, it misdiagnoses the role of law in current\nprocesses of technological production as one of absence.\nSecond, and more importantly, by invoking an absence\nof law or a failure to comply with existing law, such\nresponses concede the status of such law as capable of\nexpressing  the  particular  demands  of  justice  in  the\ntechlash. Such responses thus concede the legitimacy of\nlawfulness responses without specifying the substantive\nand normative commitments such an intervention should\naim to secure and upon which legitimacy would seem to\nbe  contingent.  Legalist-reform  responses  may  thus\narticulate a claim that “compliance” or “regulation” is\nneeded,  but  do  not,  in  and  of  themselves,  provide\nsubstantive  or  conceptual  specificity  regarding  what\nsuch  law  should  achieve  or  enact  in  order  to  be\nsatisfactory.\nBoth limitations combine to make this form of critique\nconceptually  vulnerable  to  anti-egalitarian  agendas.\nCritics\n advancing\n legalist-reform\n agendas\n risk\nmisdiagnosing  the  role  of  law  and  conceding  the\nlegitimacy of law. This then allows companies to defend\nexploitative  business  models  as  lawful  and  therefore\nlegitimate, particularly by applying the “scalpel” of legal\nintervention to separate and excise the worst instances\nof abuse while preserving the core business practices that\ngive rise to them. Both invoke a popular imagination of\nthe role of law that is quite distinct from the role that law\nin fact plays.\n3.1    Law as absent, law as present\nSome  of  big  tech’s  fiercest  critics  propose  legalist-\nreform solutions. For example, Zuboff[55] reserves a key\nrole  for  data  protection  and  greater  transparency  in\naverting  the  disasters  of  surveillance  capitalism.  Her\ncritique focuses on the lawless and un-governed “dark\ndata continent of… inner life” that, absent any regulatory\nprotection against plunder, is “summoned into the light\nfor others’ profit”. She cites the General Data Protection\nRegulation (GDPR) as a significant positive force that\nmay  help  make “the  life  of  the  law …  move  against\nsurveillance  capitalism”.  In  her  account,  such  laws\nprovide a way to turn the interrogatory spotlight back\nonto tech companies. Others have similarly advocated\nthe  need  for  applying  existing  law,  particularly\nfundamental  rights  protections,  as “able,  agile,  and\nflexible”[56] when used against technology companies to\n“shape, apply, and enforce” data rights[57].\nThe enormity of injustice catalogued by these critics\nappears  at  odds  with  the  solutions  they  propose  in\nresponse  to  them.  Indeed  such  proposals  suggest  that\nonce companies do comply with laws like the GDPR—\nonce the law has trained a spotlight on these companies’\ninner workings—they may credibly claim to engage in\nan “acceptable  form” of  surveillance  capitalism:  a\ntransparent  and  compliant  version.  Legalist-reform\nresponses concede the essential legitimacy of the legal\n    288\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\nframeworks that bind these companies, and in so doing\nconcede the essential legitimacy of the business models\nthat  have  developed  within  those  frameworks.  Under\nthis  account,  the  problem  is  not  whether  such\ntechnology—a platform optimized to exploit drivers, a\ntechnology designed for at-scale personal surveillance—\nshould exist at all, but simply one of law’s absence in\nensuring its use is “up to code”. Once companies achieve\nthis standard of compliance, the problem is addressed.\nFaith in a new regulatory regime to fill tech’s legal\nlacuna can be misplaced, as companies actively work to\nshape such regimes and use them to further their ends.\nFor  instance,  both  critics  and  industry  executives\nexpected companies like Facebook and Google to come\nunder  harsh  penalties  and  increased  scrutiny  for  new\nattempts at aggressive data extraction under the GDPR.\nBut  enforcement  has  been  largely  absent,  as  under-\nresourced  European  authorities  struggle  to  build\ncomplex  investigations  against  wealthy  international\ncompanies  (though  defenders  would  rightly  point  out\nthat enforcement has picked up as of last year). More\ntroublingly, companies have used the GDPR’s consent\nrules to re-introduce technologies previously banned in\nthe region[58]. In the U.S., state attempts to pass privacy\nlegislation  have  come  under  heavy  scrutiny  from\nindustry  lobbyists;  in  Virginia,  Amazon  increased\npolitical  donations  tenfold  over  four  years  before\nsuccessfully  getting  lawmakers  to  pass  an  industry-\nfriendly  privacy  bill  that  Amazon  itself  drafted[59].  In\nWashington,  Amazon  lobbyists  negotiated  to  have\nlanguage  inserted  verbatim  in  the  state’s  pioneering\nbiometrics bill that meant the law, when it passed in 2017,\nwould have “little, if any, direct impact on Amazon’s\nservices”[59].  Companies  do  not  just  advance  new\nbusiness-friendly  regulatory  regimes,  but  also  shape\nexisting  doctrines  into  shields  from  accountability,\ndistorting the doctrines of trade secrecy and commercial\nspeech protections to protect valuable data assets[24, 60].\nLegal observers have long understood that injustice is\nrarely a matter of law being absent. Instead, claims of\ninjustice  often  arise  from  the  ways  that  existing  law\nstructures  patterns  of  exchange  and  establishes  a\nparticular distribution of power among actors[61, 62].\nKatharina Pistor provides a compelling example in her\naccount of the role law plays in facilitating contemporary\ncapitalism by encoding global capital using certain well-\ntrodden legal properties[63]. Her account makes clear that\nglobal inequality does not arise due to the capacity of\nassets and their owners to escape the law, but instead\nthrough their ability to use the law (and, by extension, the\nstate)  to  distribute  risk  and  reward  in  maximally\nbeneficial ways. In his history of global neoliberalism,\nQuinn Slobodian further troubles the easy supposition\nof  law’s  absence  from  the  neoliberal  justificatory\nnarrative.  He  shows  how  the  policy  package  of\n“privatization,\n deregulation,\n and\n liberalization”\nassociated with the neoliberal mode of governance was\nat  its  core  a  project  of  legal  institution  building  that\nembraced, rather than shrank from, active re-working of\nglobal  projects  of  governance[64].  Britton-Purdy  and\nGrewal[65] provided a similar account of law’s active role\nin furthering and bolstering a neoliberal form of market-\nstyle governance. Cohen’s[24] account of how law and\ntechnology  shape  one  another  in  the  emergence  of\ninformational  capitalism  similarly  refutes  the  simple\naccount of law as a powerful yet regrettably absent tool\nfor disciplining the information economy. Instead, she\nshows  how  the  formation  of  informational  capitalism\nwas as much a product of legal innovation as technical\ninnovation.\nWhat these analyses make clear is that law is a terrain\nof  contestation  for  the  regulatory  arrangements  that\nstructure any social process—including our technology\neconomy. Just as companies actively shape the ethics\nresponse to enhance their interests and shield them from\naccountability,  so  too  does  the  daily  business  of\ninformational  capitalism  actively  rely  on  specific\ntheories  and  forms  of  law.  The  problem  is  not  law’s\nabsence  from  the  technology  industry,  the  digital\nmarketplace or platform, and informational capitalism.\nThe problem is precisely how existing law mediates the\ninstitutions  that  it  enacts,  the  productive  activities  it\nencases,  and  the  modes  and  myths  of  production  it\nupholds and legitimates.\n3.2    Conceding law’s democratic legitimacy\nThe second (and perhaps more conceptually significant)\nlimitation of the lawfulness/legalist-reform response is\nthat it concedes the democratic legitimacy of law absent\nany interrogation of why such legitimacy may or may not\nbe warranted, or under what conditions it may not hold.\nInvoking  law  as  a  backstop  against  the  harms  of\ntechnology  relies  on  the  premise  that  law  enacts  our\npopular will regarding such harms. In other words, the\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n289    \n \n\nlawfulness response implicitly or explicitly relies on the\nview that law: (1) can express our democratic will, (2)\ndoes  express  our  democratic  will,  and  therefore  (3)\noffers a legitimate democratic response to the popular\nfrustration of the techlash and social egalitarian claims\nthat arise from it. The legalist-reform response appeals\nto law’s role as a moral floor on what we owe one another:\nwe may not trust technology companies, but we can trust\nthe laws to which they are beholden.\nThis  tees  up  corporate  interests  to  invoke  the\nlawfulness response as a way to trade on the authority\nand legitimacy of law itself. Where law is proposed and\nthen  invoked  as  moral  cover,  it  serves  to  justify  the\npatterns  of  wealth  accumulation  or  technological\ndevelopment that law itself facilitates. Pistor notes that\n“strategic  and  well-resourced  actors” quietly  push  for\nchange outside the limelight of the public sphere; they\ncouple such efforts with “claims to the authority of law\nto fend of critique and legitimize success”[66]. Indeed,\nfew claims to legitimacy are more powerful at present\nthan that something is “legal”[63].\nSuch  normative  appeals  to  law  only  warrant  the\nlegitimacy they invoke insofar as the law itself is widely\naccepted as a (sufficiently) legitimate expression of our\nsocial  code  of  conduct  and  thus  a  viable  channel  for\nenforcing collective accountability. Yet a gap persists\nbetween  the  moral  standing  the  lawfulness  response\nmeans to invoke and the obligations its invocation in fact\nincurs—law’s  actual  response  to  claims  of  injustice.\nThis  gap  complicates  how  one  evaluates  the  political\npurpose  of  the  lawfulness  response  as  well  as  the\npolitical limitations of its legalist-reform corollary. As\na result, the lawfulness response (like the ethics response)\nmay also be anti-regulatory, albeit in a more complex\nway.\nTo understand how this reliance on the legitimacy of\nlaw may be in tension with the project of democratizing\ntechnological progress, we need to turn from the techlash\nto a parallel phenomenon: the growing legitimacy crisis\nof law. The discipline of law itself is in foment over the\nnormative gap between (1) the political ideals that form\nthe basis of law’s legitimacy and (2) how the law actually\nserves to bind and obligate agents to such ideals. This\nposes  a  significant  challenge  to  the  normative  and\npolitical appeal of the lawfulness response. What does it\nmean to address the crisis of legitimacy in tech with the\ntools of law at a time when law is undergoing its own\ngrowing legitimacy crisis?\n4    Law’s Legitimacy Crisis\nIn  near  parallel  with  the  emergence  of  the  techlash,\nferment is once again abroad in the law (to paraphrase\nLlewellyn[2]). This ferment has engulfed a broad swathe\nof legal regimes and institutions, but for the purposes of\nillustration, a focus on the Supreme Court is instructive.\nThe Court is the paradigmatic institution of U.S. law. It\nenjoys cultural significance as a stand-in for the legal\nsystem more generally, and debates regarding the Court\ncan  plausibly  be  read  to  reflect  broader  political\nsentiment towards the legal system writ large. The Court\nis not just a cultural talisman; due to the practice of (and\ncurrent  standard  for)  judicial  review,  it  has  immense\nimportance for the substance of U.S. law: how lawyers\nand regulators practice, interpret, and implement the law.\nMany  who  once  looked  to  the  law  as  the  primary\nmeans by which progressive justice is advanced have\nlost confidence that the Third Branch provides fruitful\nterrain  on  which  to  champion  progress[67, 68].  Though\nstill in its early days, this shift is noteworthy. The liberal-\nlegalist mythos of the Supreme Court and liberal Justices\nas  champions  of  progressive  change  has  persisted  for\ndecades. This is despite the general trend over the last 40-\nodd years of the Court (and the justice system over which\nit  presides)  prioritizing  the  constitutional  rights  of\ncorporate  entities  over  human  citizens[69, 70],  eroding\nprotections\n erected\n against\n discrimination[71–74],\ndiminishing  democratic  governance  at  work  and\nrestricting\n employee\n and\n consumer\n access\n to\nrecourse[75–78].  As  recently  as  the  spring  of  2016,  the\nSupreme  Court  was  widely  celebrated  for  providing\nprogressive wins like Obergefell (2015)[79] and Whole\nWomen’s  Health (2016)[80].  Liberal  Justices,  most\nnotably Ruth Bader Ginsburg, were fêted as icons of the\nprogressive  movement,  and  many  observed  with\noptimism the gradual leftward drift of Justice Kennedy,\nthe moderate swing-vote of the bench, on issues of free\nspeech  and  criminal  justice  reform[81].  Yet  four  years\nlater, the appointment of Brett Kavanaugh (despite the\ntestimony of Christine Blasey Ford and mass protests in\nthe wake of #MeToo) prompted popular liberal dismay\nat the inability of the justice system to hold itself above,\nlet  alone  discipline,  the  political  turmoil  of  our  time.\nKavanaugh’s appointment marked, for many, a turning\npoint in coming to terms with the politics—conservative\n    290\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\npolitics—of not just this Court, but the Court[82].\nOf  course,  most  reasonably  sophisticated  observers\nhave always acknowledged that politics play some role\nin judicial reasoning and the workings of law. But the\nexplanatory power of this role tended (in the “correct”\naccount of both legal scholars and mainstream observers\nof the long 1990s) to be downplayed. On this view, while\nthere is some partisan flavor to the judiciary, this has less\nto do with vulgar partisanship and far more to do with\ndifferent  theories  of  constitutional  and  statutory\ninterpretation  among  judges  that  happen  to  fall  along\nideological boundaries③. On the whole, the prevailing\nsense was—and in notable swathes of the legal academy,\nstill  is—that  there  exists  a  meaningful “residual” in\njudicial  reasoning  once  ideological  affinity  has  been\naccounted for, a space that may be won through appeals\nto  reason  and  precedent.  For  liberal-legal  political\nreformers of the long 1990’s this “residual” comprised\na  primary  terrain  of  major  progressive  political\ncampaigns  such  as  the  fight  for  LGBTQ  rights,\ndisabilities rights, and reproductive justice.\nYet  in  the  span  of  a  few  years,  political\nideology—while  still  far  from  a  dominant  view—has\nbecome\n an\n ascendant\n explanans\n of\n judicial\ndecisionmaking, as presumptions of apolitical judicial\nreasoning decline. On this account, the judiciary is not\nabove  and  immune  from  politics;  instead,  it  plays  an\nactive  and  willing  role  in  conservative  power\nconsolidation.  Three  recent  developments  strengthen\nthis alternative account. First, the mass appointment of\nunder-qualified (by the old standards of the elite bar)\npartisan Trump appointees to the federal bench. Second,\nthe  failure  of  liberal-legalist  tactics  to  discipline  the\nexcesses  of  Trump  White  House  (e.g.,  the  Mueller\ninvestigation and the Impeachment proceedings). Third,\nthe willingness of the judiciary to play a deciding role in\nhotly-contested and highly political issues[83].\nThis  turning  point  in  ideological  understanding\ncoincides with the emergence of a community of legal\nscholars  interested  in  methodological  interventions  in\nlaw. These aim to promote (1) a renewed sociological\nturn in jurisprudence[84, 85], (2) a greater attentiveness to\nthe  role  law  has  played  in  facilitating  inequality  and\nexcessive private power, and (3) a renewed ideological\ncommitment to law’s role in addressing these challenges.\nLoosely grouped under the banner of “Law and Political\nEconomy (LPE)”, this methodological agenda unsettles\nthe  neat  analytic  separation  between  the  economic\nconsiderations  in  private  law  and  the  political\nconsiderations\n in\n public\n law.\n LPE\n traces\n a\nmethodological lineage to Legal Realism, a tradition that\nwas itself closely allied with progressive aims. Like their\nLegal  Realist  forebears,  LPE  scholars  largely  share  a\ncommitment to social democratic or democratic socialist\npolitical reform, expanding the terrain on which legal\nreasoning and decision-making should be judged, and\nincorporating  a  more  complete  accounting  of  law’s\nsocial consequences and structuring capacities.\nSimilar to reformers responding to the techlash, these\nlegal  reform  projects  aim  to  produce  methodological\ninterventions  and  agendas  to  develop  and  advance\negalitarian  and  democratizing  projects  in  legal\nscholarship and legal pedagogy.\nProgressive critique of the anti-democratic nature of\nlaw is not new. The judicial branch has long been viewed\nas  anti-majoritarian  and  operating  at  a  technocratic\nremove from popular politics. Democrats as far back as\nBentham  have  attacked  the  undue  power  of  courts,\nrecognizing  the  ideological  power  concealed  in  the\njudicial power to decide “what the law is”[68, 86].\nIn  the  U.S.,  progressives  once  similarly  viewed  the\ncourts  as  the  enemies  of  democracy.  The  American\ntradition of using “judges as secret agents of political\ntransformation” has its roots in conservative, rather than\nprogressive,  fears  of  the  majority[67, 68, 86].  In  1885,\nEnglishman Sir Henry James Sumner Maine “sang the\npraises of the U.S. Supreme Court, as one of the many\n‘expedients’ in the U.S. Constitution that would allow\nthe ‘difficulties’ of any country ‘transforming itself’ into\na  democracy  to  be ‘greatly  mitigated’ or ‘altogether\novercome’”[86].  American  conservatives  of  the  era,\nfearing the effects of mass suffrage, revived the then-\nobscure case Marbury v. Madison (1803)[87] to establish\nthe\n constitutionality\n of\n judicial\n review\n over\nCongressional  legislation  (a  reading  of  the  case  in\ncontrast to how it was interpreted in its own time), and\njudges  used  this  newfound  power  to  invalidate\nprogressive legislation. It “took the strife of the Great\nDepression, and fear of Franklin Roosevelt” to force the\nSupreme  Court  into  granting  many  of  the  most\nsignificant pieces of legislation of that era, and which\nform  the  basis  of  the  modern  U.S.  state.  While  the\nProgressives ultimately prevailed, FDR noted in 1937\n③ Living Constitutionalism being a progressive or liberal theory and\nOriginalism being prominent among the conservative judiciary.\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n291    \n \n\nthat victory came at a “terrible cost”[86].\nThis  antagonistic  history  makes  the  more  recent\nprogressive embrace of the Court all the more unusual.\nThese critiques, both long-standing and renewed, are not\nfor nothing. As the emerging crisis in law makes clear,\nthe progressive embrace of legalist strategies to secure\ndemocratic agendas has produced meager results. The\nWarren court (the high point of progressive power on the\nCourt)  undoubtedly  achieved  victories  for  popular\njustice. Yet it “is worth asking whether the courts were\nnecessary to the outcomes”—and whether it was worth\nexpanding the political prominence of an antidemocratic\npower  that “the  right  has  now  turned  against\nprogressives”[86].\nThe  most  prominent  progressive  victories  in  the\ncourt—de-segregation,  voting  rights,  and  legalizing\nabortion—have all been subjects of sustained erosion.④\nBy achieving these political goals as legal wins, their\nstrength became subject to, and conditioned upon, the\ninterpretative methods of judicial review—a method that\nin some sense marks the limits of these reforms. As the\nliberal character of the court waned and these victories\nhave been reinterpreted ever more narrowly, the result\nhas been to enshrine formal protections of these legal\nvictories even as the functional social forms of injustice\nthey were meant to prevent gain new purchase.\nTo  take  school  de-segregation  as  one  prominent\nexample, more than sixty years after Brown v. Board of\nEducation (1954)[88],  functional  segregation  thrives\neven  while  being  formally  prohibited⑤.  Despite  this\nlandmark judicial victory, more than half of American\nschoolchildren  are  in  racially  concentrated  districts\nwhere over 75 percent of students are either white or\nnonwhite[89].  Even  the  districts  most  committed  to\nintegration  have  experienced  notable  re-segregation\nfollowing  successful  court  challenges  from  white\nparents[90].\nThe courts’ dubious record presents a puzzle: should\nthe  project  of  democratizing  tech  and  reviving  an\negalitarian spirit in law be to reclaim or reduce the power\nof the legal system over the substantive conditions of\npolitical wins and losses? If law is terrain on which the\nstruggles of the techlash must take place, is this terrain\nwe should seek to shield from the vicissitudes of political\nlife or to expose further to popular accountability, access,\nand rule? Such questions go to the heart of longstanding\ndebates regarding the emancipatory potential of the legal\nsystem  and  force  us  to  contend  with  the  limits  of\narticulating the demands of justice in the language of\ncourts, judges, and lawyers.\n5    Democratizing Tech, Democratizing Law:\nRescuing What Law May Offer\nDespite  the  shortcomings  of  the  lawfulness  response,\nlaw will nevertheless play a key role in addressing the\nharms  of  the  techlash.  Yet  doing  so  in  line  with\negalitarian political aims will require re-invigorating the\npossibility of law to channel and enact democratic will\nrather than serving as a means for powerful interests to\ncircumvent that will.\nAs discussed above, the processes of wealth extraction\nand social oppression at issue in the techlash exist by\nvirtue  of  their  encasement  in  law.  The  lawfulness\nresponse  offers  moral  cover  to  continue  engaging  in\nthese  practices;  the  legalist-reform  response  either\nmisdiagnoses  these  processes  as  occurring  in  the\nabsence  of  law  or  appeals  to  existing  legal  tools\nincapable  of  addressing  them.  Instead,  technology\nreformers can recast the problems of the technology’s\nfailure as problems  of  law’s  failure.  Two  clarifying\nreformulations of the twin crises of law and technology\narise as a result.\nFirst, this makes clear that both the crisis of law and the\ncrisis  in  technology  are  part  of  a  larger  egalitarian\npolitical  response  to  growing  social  inequality.  Both\nlegal  and  technical  institutions  structure  (and  drive)\neconomic exchange, and thus serve to distribute power\nand  resources.  Both  also  enforce  and  enact  the\nhierarchical relations that give shape to the social and\ncultural  experience  of  contemporary  life.  Thus,  both\nplay a role in institutionalizing the current “justificatory\nnarrative” \nof \n“property,\n entrepreneurship,\n and\nmeritocracy” that informs how enduring inequalities are\njustified[25]. As this justificatory narrative grows more\n④ The  2015  decision  upholding  constitutional  protection  of  gay\nmarriage undoubtedly ranks among the key progressive victories for the\nCourt.  Unlike  the  other  examples  noted  here,  the  constitutional  and\nstatutory  protections  won  in  2015  for  members  of  the  LGBTQ\ncommunity have simply not been enshrined in law long enough to endure\nthe sustained, decades-long legal attack that other progressive victories\nface. It remains an open question therefore whether these protections will\nface a similar fate of strong formal, negative protection, while the positive\nconditions required to obtain and exercise such freedoms remain out of\nreach for many.\n⑤ It is worth noting that Brown is as much a legislative and democratic\nvictory as a judicial one. Though decided in 1954, school integration in\nthe South did not genuinely begin until a full ten years later, precisely\nbecause it ultimately required federal legislative action to enforce.\n    292\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\nfragile and contestable, so too, do the legal and technical\nmethods that encode and enact it. The role of both law\nand technology in facilitating this narrative informs how\npeople evaluate our technology-based economy and our\nlegal system.\nThat  inequality  has  grown  should  come  as  no\nsurprise—the hypercapitalist, neoliberal, or radical neo-\npropertarian ideology that gained prominence during the\npast several decades espouses the view that inequality is\na necessary byproduct of freer markets. Under this view,\ninequality  is  required  to  produce  a  more  efficient\nallocation of goods and to increase overall productivity\n(and thus overall wealth). Yet this has not turned out to\nbe the case. Socioeconomic inequality has increased in\nall regions of the world since the 1980s and identitarian\nviolence has accompanied the faith in market action and\nefficient  allocation[25].  Inequality  has  had  particularly\npernicious effects in the US. While the top decile’s share\nof income (not wealth, where differences are even more\npronounced) has risen almost everywhere, in the US it\nrose from 35% to 48% of total national income. This\nincrease for those at the top “has come at the expense of\nthe bottom 50 percent” of the population, which as of\n2018,  commanded  only  10% of  the  total  national\nincome[25] (emphasis my own).\nIn response to increasing inequality and its harmful\nsocial  and  political  effects,  reformers  of  law  and\ntechnology share a broad methodological commitment\nto expanding the epistemic capacity of technical or legal\nmethods to recognize and act on inequality and a broad\npolitical  agenda  of  reforming  technology  or  law  to\nfurther  social  justice  goals.  Both  express  the  growing\ndemocratic and egalitarian response to the challenges of\nrising inequality and social oppression.\nSecond,  and  perhaps  of  more  importance  for  any\npositive legal and political agenda, we may reformulate\nthe crisis of techlash as, at least in part, a crisis of the\nfailure of law. Many of the tech’s democracy problems\nmay be reinterpreted as instances of law’s democracy\nproblem.  Law  has  been  instrumental  in  creating  the\nsocial challenges of the techlash, and law, as a terrain\nupon  which  to  create,  enact,  and  enforce  democratic\nreform,  will  be  instrumental  in  addressing  those\nchallenges.\nBoth popularly and intellectually, the legal system’s\ncase for its own democratic legitimacy is increasingly\nthin. If the primary interests served by the law are those\nof the powerful against the powerless, how does such a\nlegal system continue to justify itself in a democratic\nsociety,  particularly  in  light  of  growing  public\negalitarian challenges against the failures of the status\nquo? If the legal system systematically cannot serve to\ncorrect  for  problems  of  inequality,  unfairness,  and\noppression, or even provide basic recourse to make one’s\ncase against such social effects, then what, precisely, is\nit for?\nCritiques of law as inherently anti-democratic suggest\nthat  one  priority  may  be  reducing  the  prominence  of\nexisting law (and the courts that uphold it) as the primary\nterrain  on  which  we  pursue  the  democratization  of\ntechnology  production,  and  focus  instead  on  political\nbattles  to  remake  the  law  governing  technology\nproduction. Yet even in its reduced role, law remains a\nprimary means by which democratic will is expressed\nand enforced. The legal system is failing to provide its\nmost  basic  function:  to  provide  recourse  and\nenforcement of our popular expression of justice through\nlaw. Its capacity to do so has been eroded over time and\nacross core functions of law in ways that have, if not\ncaused,  then  certainly  exacerbated  the  crisis  of\ndemocratic legitimacy in tech.\nAnother pathway is to embrace the terrain of law as\nessential  to  the  project  of  democratizing  technology\nproduction. This strategy, too, has a notable progressive\ntradition. Reflecting on E. P. Thompson’s understanding\nof law’s role in traditions of radical dissent, Gordon[62]\nnotes that the Marxist historian was well aware of law’s\ninstrumental function as “a bag of weapons and tricks for\nthe rich and powerful to use against the poor”, but he\n“never  succumbed  to  a  crudely  instrumental  view  of\nlaw”. Instead, he understood law to be a “crucial element\nin the constitution of markets and relations of power and\nof  production” that  has  the  capacity  to  enact  many\ndifferent social roles and relations and is thus important\nterrain for radical dissent.\nOn this view, enacting meaningful legal institutions to\ndiscipline  technology  will  require  a  democratic\nreinvigoration  of  law’s  capacity  to  express  and  enact\npopular democratic strength of will. Willy Forbath offers\none robust positive vision of democratizing legal reform\nin form of constitutional political economy, developing\na  theory  of  constitutional  law  that  does  not  ask  what\nforms of redistribution the law permits, but instead what\nforms  of  redistribution  the  law  requires:  grounding\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n293    \n \n\npolitical claims to the social and material conditions of\nfreedom as necessary conditions for equal citizenship.\nThese in turn produce a series of affirmative duties to\nsecure  these  conditions  against  oligarchy[91].  Others\ndisagree  on  whether  a  positive  democratizing  legal\nagenda  needs  to  extend  to  constitutionalism,  or  focus\ninstead  on  diminishing  the  power  of  constitutional\nconstraints over popular legislation[92]. Yet both views\nhold that democratizing law will require departing from\nthe  predominant  mode  of reinterpreting law  in  anti-\ndemocratic courts in favor of remaking law in popular\nlegislative political wins. These wins may occur at the\nlocal, state, or national level, take the form of new law\n(such  as  facial  recognition  bans  or  surveillance\nordinances) or renewed law (such as revivals of FTC\nunfairness  enforcement  or  substantive  standards  of\nmerger review).\nWaldron[93] notes  that “a  lot  of  what  makes  law\nworthwhile, … is that it commits us to a certain method\nof arguing about the exercise of public power”. Situating\nthe  problems  of  techlash  on  legal  terrain  gives  us\nrecourse  to  this  method,  both  to  contend  with  the\nproblems  of  the  digital  economy  and  to  develop  the\ndemocratic  legal  institutions  in  respond  to  them.\nProperly  attending  to  the  techlash  and  the  lawfulness\nresponse will require re-politicizing “critical questions\nof  self-governance” that  have  been  lost  as  we  cede\ndemocratic  control  of  law  in  ways  that  facilitated\nmobility for some at the expense of the rest[66]. In other\nwords,  what  we  need  is  not  technology  that  is  more\nethical, humane, or lawful. Instead, we must make our\nsocial institutions—including those of law and our tech-\nbased economy—more democratic.\nAcknowledgment\nThis material is based on work undertaken at the Digital\nLife  Initiative,  supported  in  part  by  Microsoft.  Many\nthanks to the ILI NYU fellows for their comments, as\nwell as Elettra Bietti, Jake Goldenfein, and Ben Green.\nReferences\n K. Hill, The secretive company that might end privacy as\nwe\n know\n it, \nhttps://cyber.harvard.edu/story/2020-01/\nsecretive-company-might-end-privacy-we-know-it, 2020.\n[1]\n K. N. Llewellyn, Some realism about realism: Responding\nto  dean  pound, Harv. Law Rev.,  vol.  44,  no.  8,  pp.\n1222–1264, 1931.\n[2]\n E.\n Morozov,\n Digital\n socialism? \nNew\n Left\n Rev.,\nhttps://newleftreview.org/issues/ii116/articles/evgeny-\nmorozov-digital-socialism, 2019.\n[3]\n Edelman,  2020  Edelman  Trust  Barometer  special  report:\nTrust  in  technology,  Edelman,  research, https://www.\nedelman.com/sites/g/files/aatuss191/files/2020-\n02/2020%20Edelman%20Trust%20Barometer%20Tech%\n20Sector%20Report_1.pdf, 2020.\n[4]\n C.  Doherty  and  J.  Kiley,  Americans  have  become  much\nless positive about tech companies’ impact on the U. S.,\nhttps://www.pewresearch.org/fact-tank/2019/07/29/\namericans-have-become-much-less-positive-about-tech-\ncompanies-impact-on-the-u-s/, 2019.\n[5]\n N.  Statt,  WhatsApp  co-founder  Jan  Koum  is  leaving\nFacebook\n after\n clashing\n over\n data\n privacy,\nhttps://www.theverge.com/2018/4/30/17304792/whatsapp-\njan-koum-facebook-data-privacy-encryption, 2018.\n[6]\n O. Solon, Ex-Facebook president Sean Parker: Site made\nto exploit human ‘vulnerability’, https://www.theguardian.\ncom/technology/2017/nov/09/facebook-sean-parker-\nvulnerability-brain-psychology, 2017.\n[7]\n R.  Sini, ‘You  are  being  programmed’,  former  Facebook\nexecutive\n warns, \nhttps://www.bbc.com/news/blogs-\ntrending-42322746, 2017.\n[8]\n J. Vincent, Google favors temporary facial recognition ban\nas  Microsoft  pushes  back, https://www.theverge.com/\n2020/1/21/21075001/facial-recognition-ban-google-\nmicrosoft-eu-sundar-pichai-brad-smith, 2020.\n[9]\n “#NoTechForICE”, https://notechforice.com/, 2021.\n[10]\n Stop  LAPD  spying  coalition, https://stoplapdspying.org/,\n2021.\n[11]\n J.  Dzieza,  Amazon  warehouse  workers  strike  to  protest\ninhumane  conditions, https://www.theverge.com/2019/7/\n16/20696154/amazon-prime-day-2019-strike-warehouse-\nworkers-inhumane-conditions-the-rate-productivity, 2019.\n[12]\n L.  Wamsley  and  V.  Romo,  Uber  and  Lyft  drivers  are\nstriking—and\n call\n on\n passengers\n to\n boycott,\nhttps://www.npr.org/2019/05/08/721333408/uber-and-lyft-\ndrivers-are-striking-and-call-on-passengers-to-boycott,\n2019.\n[13]\n D.  Harwell,  Google  to  drop  Pentagon  AI  contract  after\nemployee\n objections\n to\n the \n‘business\n of\n war’,\nhttps://www.washingtonpost.com/news/the-switch/wp/\n2018/06/01/google-to-drop-pentagon-ai-contract-after-\nemployees-called-it-the-business-of-war/, 2018.\n[14]\n Tech  workers  coalition, https://techworkerscoalition.org/,\n2021.\n[15]\n J.\n Wu,\n Optimize\n What? \nhttps://communemag.com/\noptimize-what/, 2019.\n[16]\n B. Green and S. Viljoen, Algorithmic realism: Expanding\nthe boundaries of algorithmic thought, in Proc. 2020 Conf.\nFairness, Accountability, and  Transparency,  New  York,\nNY, USA, 2020, pp. 19–31.\n[17]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  ethics  and  technology  in  Action,  arXiv\npreprint arXiv: 2106.01784, 2021.\n[18]\n R.  Mac,  Four  engineers  allege  Google  fired  them  for\nspeaking  up.  Now  they  want  the  NLRB  to  investigate,\nhttps://www.buzzfeednews.com/article/ryanmac/fired-\ngoogle-employees-nlrb-investigation-cbp, 2019.\n[19]\n S. Poonam and S. Bansal, Misinformation is endangering\nIndia’s election, https://www.theatlantic.com/international/\narchive/2019/04/india-misinformation-election-fake-\n[20]\n    294\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\nnews/586123/, 2019.\n W. Phillips, The oxygen of amplification: Better practices\nfor reporting on extremists, antagonists, and manipulators,\nhttps://datasociety.net/library/oxygen-of-amplification/,\n2018.\n[21]\n JPMorgan Chase & Co, The online platform economy in\n2018, https://www.jpmorganchase.com/institute/research/\nlabor-markets/report-ope-2018.htm, 2018.\n[22]\n K.  Barron,  E.  Kung,  and  D.  Proserpio,  Research:  When\nAirbnb  listings  in  a  city  increase,  so  do  rent  prices,\nhttps://hbr.org/2019/04/research-when-airbnb-listings-in-\na-city-increase-so-do-rent-prices, 2019.\n[23]\n J.  E.  Cohen, Between  Truth  and  Power: The  Legal\nConstructions  of  Informational.  New  York,  NY,  USA:\nOxford University Press, 2019.\n[24]\n T. Piketty, Capital and Ideology. Cambridge, MA, USA:\nHarvard University Press, 2020.\n[25]\n Governance with Teeth: How human rights can strengthen\nFAT  and  ethics  initiatives  on  artificial  intelligence,\nhttps://www.article19.org/wp-content/uploads/2019/04/\nGovernance-with-teeth_A19_April_2019.pdf, 2019.\n[26]\n D.  Greene,  A.  L.  Hoffmann,  and  L.  Stark,  Better,  nicer,\nclearer, fairer: A critical assessment of the movement for\nethical  artificial  intelligence  and  machine  learning,  in\nProc. 52nd Hawaii  Int. Conf. System  Sciences,  Grand\nWailea, HI, USA, 2019, pp. 2122–2131.\n[27]\n T. Hagendorff, The ethics of AI ethics: An evaluation of\nguidelines, Minds Mach., vol. 30, no. 1, pp. 99–120, 2020.\n[28]\n A. Jobin, M. Ienca, and E. Vayena, The global landscape\nof  AI  ethics  guidelines, Nat.  Mach.  Intell.,  vol. 1,  no. 9,\npp. 389–399, 2019.\n[29]\n C.  Klöver  and  A.  Fanta,  No  red  lines:  Industry  defuses\nethics\n guidelines\n for\n artificial\n intelligence,\nhttps://algorithmwatch.org/en/industry-defuses-ethics-\nguidelines-for-artificial-intelligence/, 2021.\n[30]\n B.  Mittelstadt,  Principles  alone  cannot  guarantee  ethical\nAI, Nat. Mach. Intell., vol. 1, no. 11, pp. 501–507, 2019.\n[31]\n B.  Wagner,  Ethics  as  an  escape  from  regulation:  From\n“ethics-washing” to  ethics-shopping,  in Being  Profiled:\nCogitas  Ergo  Sum: 10  Years  of  Profiling  the  European\nCitizen, E. Bayamlioğlu, I. Baraliuc, L. Janssens, and M.\nHildebrandt,\n eds.\n Amsterdam,\n The\n Netherlands:\nAmsterdam University Press, 2018, pp. 84–89.\n[32]\n A.  Rességuier  and  R.  Rodrigues,  AI  ethics  should  no\nremain toothless! A call to bring back the teeth of ethics,\nBig Data Soc., doi: 10.1177/2053951720942541.\n[33]\n E. Bietti, From ethics washing to ethics bashing: A view\non  tech  ethics  from  within  moral  philosophy,  in Proc.\n2020  Conf. Fairness, Accountability, and  Transparency,\nBarcelona, Spain, 2021, pp. 210−219.\n[34]\n J.  E.  McNealy,  Framing  and  the  language  of  ethics:\nTechnology,  persuasion,  and  cultural  context, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0027.\n[35]\n A.  Van  Noppen,  Creating  a  technology  worthy  of  the\nhuman  Spirit, Journal  of  Social  Computing,  doi:\n10.23919/JSC.2021.0024.\n[36]\n Accenture,  Responsible  AI  |  AI  ethics  &  governance  |\nAccenture, \nhttps://www.accenture.com/us-en/services/\napplied-intelligence/ai-ethics-governance, 2021.\n[37]\n R.  Burkhardt,  N.  Hohn,  and  C.  Wigley,  Leading  your\norganization to responsible AI, https://www.mckinsey.com/\n[38]\nbusiness-functions/mckinsey-analytics/our-insights/leading-\nyour-organization-to-responsible-ai, 2019.\n M.  Zuckerberg,  A  privacy-focused  vision  for  social\nnetworking, https://about.fb.com/news/2019/03/vision-for-\nsocial-networking/, 2019.\n[39]\n Holder\n recommendations\n on\n Uber, \nhttps://www.\nnytimes.com/interactive/2017/06/13/technology/document-\nThe-Holder-Report-on-Uber.html, 2017.\n[40]\n L.  Fang,  Google  and  Facebook  are  quietly  fighting\nCalifornia’s  privacy  rights  initiative,  emails  reveal,\nhttps://theintercept.com/2018/06/26/google-and-facebook-\nare-quietly-fighting-californias-privacy-rights-initiative-\nemails-reveal/, 2018.\n[41]\n I.  Lapowsky,  Tech  lobbyists  push  to  defang  California’s\nlandmark\n privacy\n law, \nhttps://www.wired.com/story/\ncalifornia-privacy-law-tech-lobby-bills-weaken/, 2019.\n[42]\n S.\n Pichai,\n An\n insight,\n an\n idea\n with\n Sundar\nPichai—quantum  computing,  presented  at  the  world\neconomic forum annual meeting, https://www.weforum.org/\nagenda/2020/01/this-is-how-quantum-computing-will-\nchange-our-lives-8a0d33657f/, 2020.\n[43]\n B.  Smith,  Facial  recognition  technology:  The  need  for\npublic  regulation  and  corporate  responsibility, https://\nblogs.microsoft.com/on-the-issues/2018/07/13/facial-\nrecognition-technology-the-need-for-public-regulation-\nand-corporate-responsibility/, 2018.\n[44]\n R.  Claypool,  Disrupting  democracy:  How  Uber  deploys\ncorporate  power  to  overwhelm  and  undermine  local\ngovernment, \nhttps://www.citizen.org/article/disrupting-\ndemocracy-2/, 2016.\n[45]\n M. Isaac, Super Pumped: The Battle for Uber. New York,\nNY, USA: W. W. Norton & Company, 2019.\n[46]\n M.  Isaac,  How  Uber  deceives  the  authorities  worldwide,\nhttps://www.nytimes.com/2017/03/03/technology/uber-\ngreyball-program-evade-authorities.html, 2017.\n[47]\n A. Marshall, Uber and Waymo abruptly settle for ＄245\nmillion, https://www.wired.com/story/uber-waymo-lawsuit-\nsettlement/, 2018.\n[48]\n T. B. Lee, Why it’ll be hard for Uber to fire CEO Travis\nKalanick,\n no\n matter\n how\n bad\n things\n get,\nhttps://www.vox.com/new-money/2017/6/12/15779178/\nuber-travis-kalanick-scandals, 2017.\n[49]\n S.  Fowler,  Reflecting  on  one  very,  very  strange  year\nat  Uber, https://www.susanjfowler.com/blog/2017/2/19/\nreflecting-on-one-very-strange-year-at-uber, 2017.\n[50]\n E. Ongweso Jr, Uber became big by ignoring laws (and it\nplans to keep doing that), https://www.vice.com/en/article/\n8xwxyv/uber-became-big-by-ignoring-laws-and-it-plans-\nto-keep-doing-that, 2019.\n[51]\n E. Selinger and W. Hartzog, Opinion | What happens when\nemployers\n can\n read\n your\n facial\n expressions?\nhttps://www.nytimes.com/2019/10/17/opinion/facial-\nrecognition-ban.html, 2019.\n[52]\n American Civil Liberties Union, Letter to house oversight\nand\n reform\n committee, \nhttps://www.aclu.org/sites/\ndefault/files/field_document/2019-06-03_coalition_\nletter_calling_for_federal_moratorium_on_face_recognitio\nn.pdf, 2019.\n[53]\n R.  Sauer,  Six  principles  to  guide  Microsoft’s  facial\nrecognition\n work, \nhttps://blogs.microsoft.com/on-the-\n[54]\n  Salomé Viljoen:   The Promise and Limits of Lawfulness: Inequality, Law, and the Techlash\n295    \n \n\nissues/2018/12/17/six-principles-to-guide-microsofts-\nfacial-recognition-work/, 2018.\n S. Zuboff, The Age of Surveillance Capitalism: The Fight\nfor a Human Future at the New Frontier of Power. New\nYork, NY, USA: Public Affairs, 2019.\n[55]\n E.  M.  Renieris,  R.  Naik,  and  J.  Penn,  You  really  don’t\nwant  to  sell  your  data, https://slate.com/technology/\n2020/04/sell-your-own-data-bad-idea.html, 2020.\n[56]\n AWO, AWO Agency, https://awo.agency/, 2021.\n[57]\n M.  Scott,  L.  Cerulus,  and  S.  Overly,  How  silicon  valley\ngamed  Europe’s  privacy  rules, https://www.politico.eu/\narticle/europe-data-protection-gdpr-general-data-\nprotection-regulation-facebook-google/, 2019.\n[58]\n J.  Dastin,  C.  Kirkham,  and  A.  Kalra,  The  Amazon\nlobbyists  who  kill  U.  S.  consumer  privacy  protections,\nhttps://www.reuters.com/investigates/special-report/\namazon-privacy-lobbying/, 2021.\n[59]\n A. Kapczynski, The law of informational capitalism, Yale\nLaw J., vol. 129, no. 5, pp. 1460–1515, 2020.\n[60]\n R. L. Hale, Coercion and distribution in a supposedly non-\ncoercive\n state, \nPolit.\n Sci.\n Quart.,\n vol. 38,\n no. 3,\npp. 470–494, 1923.\n[61]\n R.  W.  Gordon,  E.  P.  Thompson’s  legacies, Georgetown\nLaw J., vol. 82, pp. 2005–2011, 1994.\n[62]\n K.  Pistor, The  Code  of  Capital: How  the  Law  Creates\nWealth  and  Inequality.  Princeton,  NJ,  USA:  Princeton\nUniversity Press, 2019.\n[63]\n Q. Slobodian, Globalists: The End of Empire and the Birth\nof  Neoliberalism.  Cambridge,  MA,  USA:  Harvard\nUniversity Press, 2018.\n[64]\n J. Britton-Purdy and D. S. Grewal, Law & neoliberalism,\nhttps://lpeproject.org/blog/law-neoliberalism/, 2017.\n[65]\n K.\n Pistor,\n Ideas\n alone\n won’t\n tame\n capital,\nhttps://www.publicbooks.org/ideas-alone-wont-tame-\ncapital/, 2020.\n[66]\n J.  Bouie,  Opinion  |  Down  with  judicial  supremacy!\nhttps://www.nytimes.com/2020/09/22/opinion/down-with-\njudicial-supremacy.html, 2020.\n[67]\n M.  Karp,  How  Abraham  Lincoln  fought  the  supreme\ncourt, \nhttps://jacobinmag.com/2020/09/abraham-lincoln-\nsupreme-court-slavery, 2020.\n[68]\n B.\n Duignan,\n Citizens\n United\n v.\n federal\n election\ncommission, \nhttp://www.europeanrights.eu/public/\nprovvedimenti/Supreme-Court_19_2010.pdf, 2010.\n[69]\n Burwell v. hobby lobby stores, Inc., https://scholar.google.\ncom/scholar_case?case=5322529599500468186, 2014.\n[70]\n S.  Smith,  Grutter  v.  Bollinger, Virtual  Mentor.,  doi:\n10.1001/virtualmentor.2003.5.6.medu1-0306.\n[71]\n Fisher  v.  university  of  Texas, https://en.wikipedia.org/\nwiki/Fisher_v._University_of_Texas_(2016), 2016.\n[72]\n Shelby  county  v.  holder, https://en.wikipedia.org/wiki/\nShelby_County_v._Holder, 2013.\n[73]\n Milliken v. bradley, https://h2o.law.harvard.edu/cases/943,\n1974.\n[74]\n D.  L.  Hudson  Jr,  Janus  v.  American  federation  of  state,\ncounty,\n and\n municipal\n employees,\n council\n 31,\nhttps://mtsu.edu/first-amendment/article/1595/janus-v-\namerican-federation-of-state-county-and-municipal-\nemployees-council-31, 2018.\n[75]\n Epic  systems  corp.  v.  Lewis, https://www.law.cornell.\nedu/supct/cert/16-285#, 2018.\n[76]\n AT&T  mobility  LLC  v.  Concepcion, https://casetext.\ncom/case/att-mobility-llc-v-concepcion-2, 2011.\n[77]\n Southland  corp.  v.  Keating, https://caselaw.findlaw.\ncom/us-supreme-court/465/1.html, 1984.\n[78]\n Obergefell  v.  Hodges, https://www.scotusblog.com/case-\nfiles/cases/obergefell-v-hodges/, 2015.\n[79]\n Whole  woman’s  health  v.  Hellerstedt, https://www.law.\ncornell.edu/supct/cert/15-274, 2016.\n[80]\n C. Steiker and J. Steiker, Justice Kennedy: He swung left\non the death penalty but declined to swing for the fences,\nhttps://www.scotusblog.com/2018/07/justice-kennedy-he-\nswung-left-on-the-death-penalty-but-declined-to-swing-\nfor-the-fences/, 2018.\n[81]\n M.  Yglesias,  Brett  Kavanaugh’s  confirmation  will\ndelegitimize  the  Supreme  Court  —  and  that’s  good,\nhttps://www.vox.com/2018/10/5/17941312/brett-kavanaugh-\nsupreme-court-legitimacy, 2018.\n[82]\n Republican  national  committee  v.  democratic  national\ncommittee, \nhttps://www.scotusblog.com/case-files/cases/\nrepublican-national-committee-v-democratic-national-\ncommittee-2/, 2020.\n[83]\n R. Pound, Liberty of contract, Yale Law J., vol. 18, no. 7,\npp. 454–487, 1909.\n[84]\n R. Pound, Law in books and law in action, Am. Law Rev.,\nvol. 44, pp. 12–36, 1910.\n[85]\n S.  Moyn,  The  court  is  not  your  friend, https://www.\ndissentmagazine.org/article/the-court-is-not-your-friend,\n2020.\n[86]\n Marbury v. madison, https://casetext.com/case/marbury-v-\nmadison, 1803.\n[87]\n Brown v. board of education, https://caselaw.findlaw.com/\nus-supreme-court/347/483.html, 1954.\n[88]\n EdBuild,  ＄23  billion, https://edbuild.org/content/23-\nbillion/full-report.pdf, 2019.\n[89]\n Capacchione  v.  Charlotte-Mecklenburg  schools, https://\nlaw.justia.com/cases/federal/district-courts/FSupp2/\n80/557/2565474/, 1999.\n[90]\n W.  E.  Forbath,  A  political  economy  the  constitution\nrequires, https://lpeproject.org/blog/title-tk/, 2019.\n[91]\n S.  Moyn,  The  relevance  of  Weimar, https://lpeproject.\norg/blog/the-relevance-of-weimar/, 2020.\n[92]\n J.  Waldron,  The  rule  of  law  as  a  theater  of  debate,  in\nDworkin  and  His  Critics: With  Replies  by  Dworkin,  J.\nBurley, ed. Malden, MA, USA: Blackwell, 2004, p. 319.\n[93]\nSalomé  Viljoen is  an  academic  fellow  at\nColumbia  Law  School,  and  the  former\nfellow and current affiliate at the Berkman\nKlein  Center  for  Internet  and  Society  at\nHarvard  University.  She  studies  how\ninformation law structures inequality in the\ninformation  economy  and  how  alternative\nlegal  arrangements  might  address  that\ninequality. Her current work focuses on the political economy of\nsocial data. She is particularly interested in how (and whether)\nchanges in the information economy create new kinds of legal\nclaims to social and economic equality in social data production.\n    296\nJournal of Social Computing, September 2021, 2(3): 284−296    \n \n\n \nApologos: A Lightweight Design Method for Sociotechnical Inquiry\nLuke Stark*\nAbstract:    While scholars involved in studying the ethics and politics flowing from digital information and\ncommunication systems have sought to impact the design and deployment of digital technologies, the fast pace\nand iterative tempo of technical development in these contexts, and the lack of structured engagement with\nsociotechnical questions, have been major barriers to ensuring values are considered explicitly in the R&D\nprocess. Here I introduce Apologos, a lightweight design methodology informed by the author’s experience\nof  the  challenges  and  opportunities  of  interdisciplinary  collaboration  between  computational  and  social\nsciences over a five-year period. Apologos, which is inspired by “design apologetics”, is intended as an initial\nmechanism to introduce technologists to the process of considering how human values impact the digital design\nprocess.\nKey  words:   values  in  design;  values  sensitive  design  (VSD);  artificial  intelligence;  Values@Play;  design\nmethods; sociotechnical; ethics; values\n1    Introduction\nHuman values pervade technical systems of all kinds[1],\nincluding computational technologies such as machine\nlearning  (ML)  and  other  digital  automation  systems\noften  termed  artificial  intelligence  (AI)[2–4].  Over  the\npast  thirty  years,  work  in  fields  such  as  science  and\ntechnology studies (STS)[5, 6], social computing[7, 8], and\ncritical  studies  of  technology  and  race,  gender,  and\nsexuality[9–13] has  interrogated  the  ways  in  which\nsociotechnical  systems  are  conceived  from  and\nmaintained  in  webs  of  normative  preferences.  Recent\nscholarship  has  paid  particular  attention  to  unpacking\nthe\n granular\n technical\n affordances\n and\n design\nmechanisms in AI/ML[4, 14–19], through which particular\nhuman values are operationalized—and particular kinds\nof  asymmetric  power,  injustice,  and  inequality\nmaintained—in  the  everyday  impacts  of  algorithmic\ntechnologies.\nAlongside\n these\n academic\n developments,\n the\nincreasingly obvious and deleterious societal impacts of\nsocial media platforms, artificial intelligence ventures,\nand other Silicon Valley firms have pushed the broad\ntopic of “tech ethics”, and the harm digital technologies\ndo\n to\n marginalized\n groups,\n into\n international\nprominence[20–24]. Thanks to pressure from civil society\ngroups,  social  justice  organizations,  activist  scholars,\nand ordinary citizens, digital technology firms have been\nforced to begin to take responsibility for, and move to\naddress their role in perpetuating and exacerbating social\ninequalities and power asymmetries.\nAs a result, digital technology firms have put much\nemphasis on high-level codes of ethical conduct around\nthe development of technologies like AI systems, and\nhave  even  begun  to  invest,  albeit  sporadically,  in\ninterdisciplinary teams of experts versed in the social\nimpacts of computational media. However, methods and\nmechanisms to translate these high-level principles and\ndiverse insights into actual decisions about products and\nsystems are, on the whole, lacking[25]. Scholars involved\nin studying the ethics and politics of digital information\nand communication systems have long sought to have a\nconcrete impact on the design and deployment of such\nartifacts in technical research and development (R&D)\ncontexts such as academic laboratories and commercial\n \n • Luke  Stark  is with  the  Faculty  of   Information  and  Media\nStudies, Western University, London, N6A 0A2, Canada. E-mail:\ncstark23@uwo.ca.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-20;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   01/06  pp297−308\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0028\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\ndevelopment spaces[26–29]. However, the fast pace and\niterative  tempo  of  technical  development  in  these\ncontexts, and the importance of business decisions over\nwider  societal  concerns,  have  been  major  barriers  to\nensuring consideration of human norms and values is at\nplay at every stage in the R&D process[30].\nUrgent calls to grapple with the social, ethical, and\nnormative\n implications\n of\n AI/ML\n and\n other\ncomputational  systems  on  societies  around  the  world\nmake understanding and evaluating the ethics, norms,\nand values of all sorts of novel digital systems a necessity.\nFirms  must  move  beyond  lip-service  to  broader\nnormative\n frameworks;\n critical\n and\n progressive\nresponses  to  such  technologies  from  lawmakers,\nregulators, civil society groups, and citizens in general\ncan also benefit from such evaluations[21, 23, 31, 32].\nHere,  the  author  outlines  a  lightweight  method  for\neliciting and evaluating ethics, norms, and human values\nin sociotechnical systems on a compressed time scale:\nApologos.  As  a  method,  Apologos  is  inspired  by  the\nnotion\n of \n“design\n apologetics”[33],\n which\n uses\nspeculation  to  appraise  technologies  and  their  social\nimpacts.  This  method  seeks  to  present  a  coherent,\npractical,  and  principled  approach  to  the  problem  of\nactively identifying norms, ethics, and values in a design\nprocess  quickly.  This  method  extends  existing\nmethodological\n frameworks[34–37] \nand\n draws\n on\nobservations  from  a  five-year  case  study  of  how\nconflicting  human  values  intersect  with  the  often\ncomplicated  and  contingent  dynamics  of  designing\ndigital systems[8].\nThe author also deploys insights from scholarship in\ndesign  fiction[38],  speculative  design[39, 40],  and  the\nnotion  of “design  apologetics”—thought  experiments\nthrough  which  participants  in  a  design  process  work\nbackwards  from  existing  or  prospective  artifacts  to\ndestabilize and make novel the normative notions behind\ndigital\n technologies\n and\n systems.\n Such\n design\napologetics\n ideally\n stimulate\n productive\ndisorientation[41], and generative reflection about these\ntechnologies’ possible  social  effects.  This  strategy  is\ngrounded  in  understanding  and  articulating  how\nparticular  human  norms,  ethics,  and  values  become\nincorporated  in  multifarious  ways  into  the  technical\nfeatures of digital artifacts through design choices (both\nconscious and unconscious), and how such norms are\nmade manifest in the use of technologies across diverse\ncontexts[42].\nAs a method, Apologos seeks to use time itself as a\nvisceral prompt to encourage novel thinking, and spark\nfurther in-depth reflection on and attention to the social\ncontexts and lived realities of our everyday experience\nof technologies. Apologos is not intended as a panacea\nor replacement for more longitudinal or reflective design\nmethods. As is a lightweight approach, it is potentially\nuseful  both  in  commercial  settings  and  in  broader\nparticipatory  design  contexts  as  an  introduction  to\nconsidering  how  human  values  are  expressed  in\nsociotechnical  systems[43–45].  Apologos  would  be\nappropriate as an initial diagnostic exercise in a wide\nvariety  of  digital  design  contexts:  to  begin  to  surface\npotentially\n confounding\n or\n complicated\n values\ntradeoffs[36]; point to spaces in the design process that\nmight act as “values levers”[46]; and support space for\nnovel engineering, participatory design, and refusal[47].\nThe theoretical and conceptual stakes of this article are\nthus  twofold.  First,  what  does  the “nitty-gritty” of\ncollaboration  between  computer  scientists,  social\nscientists,  and  humanists  tell  us  about  how  to  work\nacross the socio-technical divide? Second, how can this\nexperience shape an efficacious method for introducing\naudiences to the rich existing literature exploring how\nhuman values come to bear on the process of designing\ncomputational systems?\n2    Lessons\n from\n Future\n Internet\nArchitecture Project\nAlongside already existing work on design prompts and\nmethods such as Friedman’s Valuse Sensitive Design[48]\nand  Flanagan  and  Nissenbaum’s  Values@Play[36]\nmethods, Apologos has been shaped by observations and\ninsights  drawn  from  the  author’s  experience  with  the\nFuture\n Internet\n Architecture\n (FIA)\n project,\n a\nmulti-million dollar research project sponsored in two\nphases from 2010 to 2016 by the Computer and Network\nSystems  (NETS)  Division  within  the  Directorate  for\nComputer  and  Information  Science  and  Engineering\n(CISE) of the National Science Foundation (NSF). The\nproject  involved  four  multi-institution  research  teams\ncomprised primarily of computer scientists (henceforth\nreferred to as the “computational teams”). These teams\ninvolved  dozens  of  senior  researchers  and  graduate\nstudents  from  more  than  fifteen  different  institutions.\nCISE also engaged several outside technical experts to\n    298\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\nadvise and interact with the projects on an ad-hoc basis.\nThe  whole  initiative  was  led  by  David  Clarke  of  the\nMassachusetts  Institute  of  Technology  (MIT),  a\npioneer  in  the  current  Internet’s  early  network\narchitecture[49–52].\nThe  FIA  technical  teams  were  asked  to “explore,\ndesign,  and  evaluate  trustworthy  future  Internet\narchitectures”.  Braman[53] notes, “decisions  about\ntechnology design and network architecture are, today,\nde facto social policy.” Recognizing this fact, the FIA\nProject also included the participation of the Values in\nDesign Council, a group “funded by NSF to involve a set\nof social scientists, lawyers, and economists in the FIA\ndesign process”. This group of experts, for which the\nauthor  were  a  research  assistant,  included  more  than\ntwenty  leading  scholars  from  information  science,\nscience  and  technology  studies,  technology  law,  and\ndigital media studies. The FIA-VID collaboration was a\nrare example of a large, ongoing, and formal attempt to\nbring  technical  expertise  together  with  sociocultural,\nlegal, and policy insight in the service of computational\ndesign.  As  such,  the  lessons  learned  from  both  the\nsuccesses\n and\n productive\n failures\n of\n these\ncollaborations—which\n were\n multiple\n and\nnuanced—have already served as the basis for insights\naround\n how\n interdisciplinary\n teams\n can\n work\ncollaboratively to design systems with human values in\nmind[8, 49].\nThis  portion  of  the  paper  is  thus  grounded  in\nqualitative  materials,  including  collaborative  and\nindividual\n field\n notes,\n reports,\n and\n participant\nobservation  of  more  than  a  dozen  FIA  Principal\nInvestigator  meetings  from  2010  through  2016.  The\nexperience\n of\n the\n Values\n in\n Design\n Council\ndemonstrated\n the\n need\n to\n get\n interdisciplinary\ncollaboration  right  between  computer  science  and  the\nsocial  sciences/humanities.  It  also  showed  the  related\nnecessity, which became ever clearer over the course of\nthe project, to develop a tool with which to introduce and\nfamiliarize computer scientists and engineers with the\nnotion  of  engaging  with  human  values  in  the  design\nprocess quickly and efficaciously. As Shilton[8] observes\nin  reference  to  her  own  experience  as  part  of  the\nFIA-VID project, such “interventions struggled to make\nvalues reflection consistently relevant and engaging” to\nmembers  of  the  computational  teams  involved  in  the\neffort.  These  challenging  elements  of  the  FIA  project\nexperience  have  shaped  Apologos  as  a  method,  and\nsuggest avenues for further refinement and elaboration.\nWhat  ontological  and  epistemological  assumptions\nclash in the conversations between computer scientists\nand social scientists/humanists? What material practices\nand  institutional  or  disciplinary  norms  help  or  hinder\ncollaborations? And how do shared aims, desires, and\nvalues motivate or inhibit working together? Answers to\nthese  questions  have  shaped  the  development  of\nApologos as a method.\n2.1    Key  emergent  theme:  Clashing  technical\nlanguages & epistemologies\nPerhaps unsurprisingly, each academic discipline works\nwith and within a particular technical language. While\nthese technical languages are often related depending on\nthe shared history and concepts of disciplines, the same\nterms in each discipline can suggest not just different\ntechnical definitions, but also imply radically different\nepistemologies  (theories  of  professional  knowledge)\nthat require work to be made commensurate.\nThroughout the course of the FIA-VID project, one of\nthe  chief  practical  obstacles  to  collaboration  between\ncomputational and STS scholars was the way in which\nboth  linguistic  definitions  and  epistemological  priors\nwere  misunderstood  by  project  participants,  at  least\nsome (if not most) of the time. Shilton[8] observed one\nkey  definitional  confusion  in  the  context  of  FIA-VID\nwas the status of the term “interoperable”, which for the\ncomputational  teams  was  implicitly  synonymous  for\ntechnologically \n“neutral”.\n Shilton\n observes\n this\ndefinitional overlap prompted a key value assumption:\n“the asserted belief in the neutrality of architecture was\nat  least  partially  an  expression  of  a  core  value:  the\ninteroperability of infrastructure”[8]. Central to Shilton’s\nobservation  is  that  definitions,  and  indeed  their\nepistemological foundations, are grounded and guided\nby value judgments: in this case, the historical legacy of\nvalues  consensus  in  network  engineering  has  made\nend-to-end interoperability synonymous with technical\nand societal impartiality (ibid.).\nDebates  about  the  definitions  and  the  broader\nepistemological meanings of terms such as privacy and\nsecurity also exemplify how FIA-VID participants from\nthe  computational  and  STS  teams  struggled  with\ndeveloping  common  definitions.  Two  FIA  Principal\nInvestigator (PI) meetings focused their agendas on the\n  Luke Stark:   Apologos: A Lightweight Design Method for Sociotechnical Inquiry\n299    \n \n\nsecurity  and  privacy  implications  of  the  various\nprototype architectures—the first, early in the project in\n2011, and the second in 2015. The 2011 meeting also\ninvolved a group of outside technical security experts not\nattached  to  the  four  computational  teams,  who  were\ntasked with advising those teams on their security and\nprivacy  plans  as  the  projects  moved  forward  from\nconceptualization to working prototypes. As privacy and\nsecurity happened to be areas of considerable expertise\nfor many Values in Design (VID) Council members, the\nmeeting  was  also  framed  as  an  opportunity  for  the\nCouncil and technical teams to engage on an issue of\ncommon interest.\nUnfortunately, it became apparent over the course of\nthe meeting that “security” and “privacy” had different\n“technical” meanings for both groups. For the technical\nteams, “privacy” was generally articulated as a feature\nof  their  novel  network  architectures:  something  to  be\nadded as an additional layer or modification above more\nfundamental programming, but which was not in and of\nitself necessary for the functioning of the network. For\nthe  members  of  the  VID  Council,  privacy  was\nunderstood  as  a  necessary  outcome  of  network\narchitecture, a default end state for ordinary users that\nought to govern the technical teams’ design decisions.\nOne  member  of  the  VID  Council  noted  that  ordinary\nusers of digital technologies rarely changed their default\nsettings within their personal devices—and that privacy\nas a core human value ought to be integral to the teams’\nthinking.\nIn  plain  language,  members  of  the  technical  teams\nwanted to know how to build systems and architectures\nthat  promoted  user  privacy  as  an  actionable  and\nimperative  procedure,  whereas  members  of  the  VID\nCouncil articulated privacy as an overarching end that\ncould be enabled through a variety of different material,\ntechnological,\n and\n discursive\n means\n by\n the\ncomputational teams. The computational teams, a group\nmade  up  primarily  of  computer  scientists,  operated\nprimarily through what Abelson, Sussman, and Sussman\nterm “procedural epistemology”—or in their words, “the\nstudy of the structure of knowledge from an imperative\npoint of view, as opposed to the more declarative point\nof view taken by classical mathematical subjects.” This\nepistemological  logic,  according  to  the  authors,\n“provides  a  framework  for  dealing  precisely  with\nnotions of ‘how to’”[54]. This difference led to a focus on\nsearching  for  concrete  mechanisms  through  which  to\ntranslate  values  into  technical  features.  Differences\nbetween  VID  members  and  the  computational  teams\nthus  did  not  seem  to  be  grounded  in  a  difference  of\nunderlying lived or embodied values per se. Rather, it\nwas  a  disagreement  around “lingo” tied  to  a  deeper\ndifference  in  epistemological  constructions  and  prior\nassumptions held by the two groups.\nIn contrast, the members of the VID Council, as social\nscientists  and  humanists,  operated  through  several\noverlapping  epistemological  frames.  One,  to  borrow\nagain  from  Abelson,  Sussman,  and  Sussman,  was  a\n“declarative  point  of  view”—which  the  authors\nassociate  with  classical  mathematical  subjects,  but\nwhich we here suggest is an epistemological frame that\nalso fits with certain strands of humanistic and social\nscientific  thought.  Abelson,  Sussman,  and  Sussman\nsuggest  that, “Mathematics  provides  a  framework  for\ndealing with precisely with notions of ‘what is’.” While\nthe  humanities  and  social  sciences  have  never  been\naccused  of  ontological  clarity,  their  interest  is  often\ndeclarative  or  descriptive.  Crucially,  declarative\nepistemology  is  not  the  only  flavor  of  knowledge\nconstruction  in  the  human  sciences—members  of  the\nVID  Council  also  articulated  discursive  and  critical\nepistemological frames through their comments.\nIn\n this\n particular\n instance\n of\n collaborative\nconversation, one that replayed itself around many other\nconcepts across the life of the FIA project, both sides of\nthe discussion were confused as to why a value each side\nagreed was valuable and important—privacy—seemed\nto nonetheless cause consternation (critically, in some\ncontexts it became clear that privacy was not perceived\nas  valuable  by  some  in  the  conversation—a  problem\ndiscussed later in the paper).\n2.2    Key emergent theme: Conflicting disciplinary\nnorms and incentives\nSome  of  the  most  basic  structural  barriers  to\ncollaboration  between  the  computational  experts  and\nVID Council members during the span of the FIA project\ninvolved  the  disparate  and  unaligned  disciplinary\nincentives  (and  disincentives)  around  pursuing  joint\ninterdisciplinary  projects.  Studying  interdisciplinary\ncollaboration,  particularly  in  the  natural  sciences,  has\nbecome  something  of  a  cottage  industry  in  recent\nyears[55, 56].  However,  as  Callard  and  Fitzgerald[57]\nobserve, this methodological focus has not extended to\n    300\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\nother academic arenas: with very few exceptions[58], the\nauthors  note, “there  has  not  yet  been  any  significant\nemergence of research on practices of interdisciplinarity\nwithin the social sciences and humanities.”[57] Worse,\ninterdisciplinary  collaborations  between  natural  and\nsocial\n scientists,\n though\n increasingly\n vital\n to\nunderstanding\n and\n engaging\n with\n complex\nsociotechnical  problems,  are  both  rare  in  practice\nand  understudied  in  terms  of  their  collaborative\ndynamics[59].\nComputer science as both an academic discipline and\na  set  of  professional  practice  is  structured  quite\ndifferently than the disciplines from which members of\nthe VID Council were drawn (chiefly law, media and\ninformation studies, and the social sciences). Moreover,\nthe specific structure of the FIA project also introduced\nstructural  incentives  that  discouraged  efforts  at\ninterdisciplinary\n collaboration\n between\n the\ncomputational\n teams\n and\n VID\n Council.\n These\nchallenges around interdisciplinary collaboration were\nexemplified  by  the  relative  physical  and  temporal\nseparation of the VID Council from the four technical\nteams from the outset of the project. Over the course of\nthe FIA project, VID Council members and members of\nthe technical teams met at a series of semi-annual two-\nday PI meetings hosted by various member institutions.\nBecause of the size of both the technical teams and of the\nVID  Council,  few  if  any  project  participants  were\npresent for every meeting. Because of the length of the\nproject, there was considerable turnover among junior\nresearchers\n (doctoral\n students\n and\n postdoctoral\nresearchers) involved in the technical teams. And while\nmembers of each individual technical team were bound\ntogether both by the shared content of the project and by\npotentially  a  sense  of  competition vis  a  vis the  other\ntechnical  teams,  there  was  more  physical,  social,  and\nintellectual separation between the individual teams, and\nbetween the teams and the VID Council, across the life\nof  the  project.  This  segmentation  kept  project\nparticipants in both disciplinary and project-based silos,\nwhich made communication, let alone collaboration, an\nongoing challenge.\nThese  gaps  did  not  go  unnoticed  by  FIA  Project\norganizers, who eventually sought to reduce the distance\nbetween FIA Council members and the technical teams.\nAt the May 2012 PI meeting, the NSF announced that it\nwould provide extra funding to “embed” members of the\nFIA  Council  within  particular  technical  teams,  and\nseveral VID Council members were ultimately affiliated\nto  a  greater  or  lesser  extent  with  particular  technical\nprojects[8].  However,  the  default  case  for  interactions\nremained  large,  structured  presentations  and  Q&A\nsessions\n at\n PI\n meetings.\n While\n disciplinary\nsegmentation\n is\n well-known\n challenge\n for\ninterdisciplinary work, the experience of the FIA project\nhighlighted an under-noted effect of these silos: the ways\nin  which  a  lack  of  sociality  and  a  high  degree  of\nemotional  distance  hindered  the  project’s  intellectual\nand scholarly goals. Previous scholarship has found that\nsustained  social  bonds  and  shared  physical  space  are\nhelpful  in  supporting  interdisciplinary  inquiry  by\nconnecting  individuals  interpersonally  as  well  as\nintellectually[60].\nAs  Callard  et  al.[60] note,  hierarchies  within\ninterdisciplinary  collaborations  can  often  short-circuit\nsustained interactions. Given the intermittent nature of\nmost  interactions  between  Council  participants  and\ntechnical team members, it is unsurprising that relatively\nfew social bonds formed over the course of the project\nbetween  the  two  groups.  Divergent  publishing\nconventions and expected project outputs and timelines\nalso  posed  challenges  for  sustained  cross-disciplinary\ncollaboration across the life of the FIA project. For the\ncomputer  scientists  involved  in  the  project,  the  core\ntechnical  problems  around  the  design  of  network\narchitecture required a different collaborative cadence\nand  pace  of  publication  than  studies  by  the  social\nscientists  in  the  VID  Council.  Interdisciplinary\ncollaborations  were  curtailed  as  much  by  divergent\nprofessional schedules and incentives as they were by a\nlack  of  social  connection  between  VID  Council\nmembers and members of technical teams.\n3    Apologos as Design Prompt\nOne important lesson of the FIA project is that many\ntechnical  experts  have  little  vocabulary  or  formal\ntraining in engaging with sociotechnical questions, but\nare eager and excited to do so if supported by appropriate\nresearch  and  design  methods.  One  strategy  that  the\nValues in Design Council deployed with some success\nover the course of the FIA project was around design\nscenarios:  suggesting  the  computational  teams  sketch\nout  how  their  proposed  systems  might  work  in\nreal-world  conditions  and  what  such  conditions\nindicated  about  the  values  of  the  systems  at  hand.①\nApologos  is  first  and  foremost  inspired  by  this\n① Particular credit goes to VID Council members James Grimmelmann\nand Chris Hoofnagle for their initial championing of this approach.\n  Luke Stark:   Apologos: A Lightweight Design Method for Sociotechnical Inquiry\n301    \n \n\nexperience, which could nonetheless have perhaps been\nimproved by a standardized, lightweight set of prompts\nto help initiate such discussions across both the Council\nand computational groups.\nAs noted above, Apologos draws from the rich body\nof extant scholarship and design practice on values and\nhuman  design,  including  how  human  values  can  be\nelicited  or  translated  into  technical  means  through\nparticular design prompts or toolkits. Structured design\nprompts have become a popular method for facilitating\ndesign research and participatory co-design over the past\ndecade in critical HCI and related fields. These prompts\noften feature a toolkit of playing cards or make use of\nother gamic elements as mechanisms to structure and\nvary deign outcomes[61]. Popular examples of such card-\nbased  design  prompts  include  the Envisioning  Cards\nfrom  the  University  of  Washington’s  Value-Sensitive\nDesign (VSD) group[34, 62] among many others[63]. More\nbroadly,  the  notion  of “design  sprints” and  similar\ntime-constrained methods to facilitate design work has\nproliferated in both academia and the digital technology\nsector[64]. Recent frameworks and toolkits for applying\nVSD  methods  to  contemporary  tech  development\nenvironments  are  a  salutary  means  of  effectively\nintegrating  these  design  traditions  to  an  applied\ncontext[65].\nApologos  is  inspired  by  these  existing  design\ntraditions and methods. It also draws on, and is named\nafter, the notion of “design apologetics”, proposed by\ninterface  designers  Nathan  Shedroff  and  Christoper\nNoessel in Make It So: Interaction Design Lessons from\nScience Fiction (2012). Shedroff and Noessel borrow the\nterm “apologetics” from  theology,  where  it  refers  to\nreasoned argument justifying a religious doctrine. In the\ncontext of design in science fiction films and television\nprograms, Shedroff and Noessel note they searched for\nways  fictional  technologies “could” be  explained  to\nwork, which “led to some interesting insights about the\nway technology should work”[33]. Other recent academic\nwork has centered using speculative fiction as a prompt\nto produce novel design insights around human values\nand more broadly as a tool for critical design practice in\ncomputational settings[38, 66, 67].\nBuilding on this prior work, Apologos is intended as\nan  intervention  to  enable  interdisciplinary  groups  to\nconcisely  consider  a  technology’s  sociotechnical\nimpacts using design apologetics (as outlined in Table 1).\nAn Apologos session is essentially a highly compressed\nversion  of  the  three-stage  process  laid  out  in\nRefs. [35, 36]: discovery of relevant values in a particular\nsituation,  implementation  of  those  values  as  technical\nfeatures, and verification that the assumptions made in\nthese  first  two  steps  are  broadly  pertinent.  Ideally,\nApologos should be deployed either at the very outset of\na particular design project or as a pedagogical prompt to\nintroduce  new  audiences  to  sociotechnical  analysis.\nEach session should be relatively brief: no less than 1 h,\nand potentially two hours or slightly more. The exercise\nis most practically tractable if undertaken by groups of\n3−4 individuals; it is beneficial for larger design teams\nto break up into these smaller cadres and then reassemble\nas a larger unit for the final phase of the initial exercise\nas per below.\nThe first step in an Apologos session is for the session\nfacilitator to ground participants in two definitions. First,\nethos: a moral habit, character, disposition, or custom.\n \nTable 1    Apologos summary.\nPhase\nComponent (total 60 min)\nDiscovery\nDefinitions (8 min)\nBrainstorming (2 min)\nList development (5 min)\nImplementation\nDesign apologetics (2 min)\nApologetics application (8 min)\n(Break for 5 min)\nValue judgment (5 min)\nRe-design (5 min)\nEvaluation & follow-up\nReflection (5 min)\nSharing across groups (8 min)\nExercise feedback (7 min)\nMethod transition (indeterminate)\n \n    302\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\nSecond, techne: a variable and context-dependent art or\ncraft (approximately 8 min, in a 1 h session). Attentive\nreaders will note here how the notion of ethos stands in\nfor  the  broader  definition  of  the “social” common  to\nscience  and  technology  studies  (STS)  literature  on\nsociotechnical  systems.  Any  ethos  is  inherently  a\ncommunal, and thus social, undertaking, but the term’s\nnormative connotation, and its emphasis on habituation,\nmake it particularly apt for this exercise. After laying out\nthese  definitions,  the  facilitator  should  note  the\nsignificant  practical  overlap  between  these  two\ndefinitions\n around\n usual\n custom\n and\n lived\nconcreteness—by\n extension\n the\n ways\n particular\nsociotechnical systems are inherently ones with unique\nsets of norms and values.\nThe participating small groups should then brainstorm\nthree to four examples of everyday situations, scenarios,\nor activities where the definitions of ethics and technics\nalready  provided  might  be  at  play:  a  sociotechnical\nsituation involving both aspects (2 min in a 1 h session/\ntime  elapsed:  10  min).  Once  participants  have\nidentified a short list of scenarios or situations, such as\nflagging down a self-driving ride share vehicle or having\none’s mobile devices searched at a national border, each\ngroup should collectively develop, for just one of those\nscenarios, two parallel lists: one of 3−4 ethical principles\nor values the group associates with the situation, and one\nof  3−4  technical/material  elements  or  features  of  the\nsituation (5 min in a 1 h session/time elapsed: 15 min).\nThe  facilitator  should  encourage  these  lists  to  be  in\nparallel vertical columns  on  one sheet of  paper.  As  a\nfurther  prompt,  the  facilitator  can  also  emphasize  the\nutility  of  considering  what  Flanagan  and  Nissenbaum\nterm “values  seams”[36],  or “places  where  multiple\nvalues are held in tension” within particular technologies\nor technical systems, as a means to remember that no\ntechnology is in any way “neutral”.\nThe initial steps of an Apologos session—highlighting\nboth  the  difference  and  potential  overlaps  in  the\ndefinitions  of techne and ethos and  grounding  this\ngeneral  discussion  in  particular  domain  contexts—are\nintended to immediately highlight the processual versus\ndeclarative divide in considering a value such as privacy\nin  particular  sociotechnical  milieus.  The  group\ndiscussion  intended  to  support  these  determination\nserves  as  what  Shilton  terms  a “values  lever”[46],  or\n“practices  that  pry  open  discussions  about  values  in\ndesign and help the team build consensus around social\nvalues as design criteria”. Values levers are valuable to\ndelicately  de-lace  Flanagan  and  Nissenbaum’s “value\nseams”,  enabling  collaborative  examination  and\ndiscussion of the multiple value perspectives within a\nparticular sociotechnical apparatus. Through these steps,\ndefinitional and even epistemological differences can be,\nif not resolved, at least surfaced and recognized as such.\nWhen these first stages of the exercise are complete,\nthe  facilitator  should  introduce  the  notion  of  design\napologetics (2  min  in  a  1  h  session/time  elapsed:\n17 min) to participants (as a reminder, apologetics are\nreasoned  arguments  or  writings  in  justification  of\nsomething,  typically  a  theory  or  religious  doctrine).\nGroups should not be informed about the details of the\ndesign apologetics stage before they formulate their two\ninitial lists.\nIn  their  existing  groups,  participants  should  then\nperform apologetics across their lists (8 min in a 1 h\nsession/time  elapsed:  25  min),  speculating  about  or\nimagining  reasonable  ways  a  designer  might  pair  the\nethical principles or values discerned by the group in the\nfirst phase of the exercise with the technical features the\ngroup  has  picked,  inasmuch  as  technical  features  can\nexpress a value or make it concrete. It is critical that\nparticipants do not change the content of either list to\nmake this exercise easier or tidier. If members of the\ngroup  cannot  draw  a  reasonable  connection  between\nthe  values  they  first  identified  and  the  initial\nmaterial  or  technical  features,  this  failure  should  be\nspecifically noted as another example of Flanagan and\nNissenbaum’s  notion  of “values  seams”.  It  is  to\nencourage the exposure of such values seams that groups\nshould  not  be  informed  about  the  design  apologetics\nstage before they formulate their initial lists. This stage\nshould be immediately followed by a stretch break for\ndecompression  and  informal  conversation  about  the\nexercise (5  min  in  a  1  h  session/time  elapsed:\n30 min).\nAfter  the  break,  the  facilitator  should  focus\nparticipants on the most discordant or least convincing\npairing of principles and technical features out of their\nlist.  Group  members  should  then  decide  together\nwhether they judge, in the context of the other values and\nfeatures identified, whether it is the ethical value or the\ntechnical feature that is more important to the broader\ngoals of the project, situation, or milieu (5 min in a 1 h\n  Luke Stark:   Apologos: A Lightweight Design Method for Sociotechnical Inquiry\n303    \n \n\nsession/time elapsed: 35 min). For groups with little\ndesign  experience,  the  facilitator  might  note  that  in\nmaking  such  a  judgment,  participants  are  expressing\ntheir  own  values  as  designers.  For  groups  with  more\nexperience with sociotechnical analysis, the facilitator\ncan  observe  beforehand  that  how  each  group\nunderstands and bounds their project or situation is itself\na values judgment that inevitably shapes the decisions in\nthis step.\nAfter each group has made a judgment collectively,\nthe facilitator should encourage group members to either\n(1) brainstorm a new technical feature that better makes\nconcrete the value or principal the group has decided to\nprioritize, or (2) identify another principal or value (even\na “negative” or  unwanted  one)  suggested  by  the\ntechnical feature being prioritized to replace the initial\nill-matched value (5 min in a 1 h session/time elapsed:\n40 min). Group members should then record answers to\nthe  following  questions  collectively  (5  min  in  a  1  h\nsession/time elapsed: 45 min): How might this planned\nchange  interact  with  the  other  values  and  technical\nfeatures  already  identified  in  the  scenario  or  milieu?\nHow might this change affect the broader parameters of\nthe scenario or situation originally laid out?\nFinally,  the  facilitator  should  quickly  organize  a\njigsaw in which each small group splits and new groups\nof 2−3 people are constituted; if there is only one small\ngroup, then the following two steps can be combined.\nEach group member should briefly present the work of\ntheir original group to their new group mates and ask\nthem to imagine themselves as people in the scenario the\noriginal group explored (8 min in a 1 h session/time\nelapsed:  53  min). Participants  should  ask  their  new\ngroupmates  whether  there  are  values  or  technical\nelements of the original group’s assessment with which\nthey  disagree,  or  that  they  would  add  or  take  away.\nMoreover, each member should ask the others how the\nchange made by the original group around either a value\nor  technical  feature  would  affect  them  as  imagined\nsubjects in the scenario. For the last portion of the hour\nspan (7 min in a 1 h session/time elapsed: 60 min),\nparticipants should reconvene into a large group and the\nfacilitator should ask for feedback on the design exercise,\nincluding  about  what  was  satisfying,  enlightening,  or\nuseful; what was unsatisfying, frustrating, or incomplete;\nand  if  appropriate,  what  changes  would  improve  the\nexercise for participants in a particular context.\nOne  benefit  to  a  lightweight  design  exercise  like\nApologos is that it can be deployed expeditiously and\nreadily  understood  by  teams  from  diverse  disciplines\nand  backgrounds.  However,  Apologos  should  by  no\nmeans  be  the  final  step  in  a  design  and  development\nprocess: as a final step, the group should make a plan to\ntransition  to  a  more  fully  developed  method  for\ndesigning  with  values  in  mind  as  appropriate.\nFrameworks\n such\n as\n Value\n Sensitive\n Design\n(VSD)[34, 48, 65], worth-centered design[68, 69], reflective\ndesign[70],\n adversarial\n design[71],\n and\n Values@\nPlay[35, 36], and working with values hypotheses[42] are\nall potential options for such a framework.\nAs  an  exercise,  Apologos  was  designed  by\nconsidering  the  experience  of  FIA  participants,  who\nfaced a related set of conflicting norms grounded in the\ndisciplinary  and  institutional  nature  of  large-scale\nacademic  research.  Insights  from  these  successes  and\nfailures point to ways design exercises like Apologos can\nhelp bridge these institutional and professional gaps and\nto  moments  where  broader  and  longer-term  work  is\nneeded  above  and  beyond  any  individual  design\ntechnique. With both social and disciplinary challenges\nin mind, Apologos is designed to be a group activity that\nensures a large group of disparate experts can participate\nin discussions around values and technologies and to be\nsufficiently brief that it can be deployed even at one- or\ntwo-day  meetings.  The  social  element  of  Apologos\nprovides one possible scaffold for broader collaboration,\nor  at  least  engagement,  between  members  of  diffuse\ninterdisciplinary  teams.  Given  sustained  interactions\nand shared physical proximity are clearly ideal in this\nregard[60], Apologos aims to begin the process of social\nmixing,  while  building  shared  incentives  around\nspeculative design.\n4    Conclusion: Disagreeing Over Values\nHere,  the  author  has  proposed  a  lightweight  design\nmethod, Apologos, intended to elicit and evaluate ethics,\nnorms, and human values in sociotechnical systems on\na compressed time scale. As already noted, Apologos is\nnot a panacea or replacement for more comprehensive\ndesign  methods  or  structural  mechanisms  to  facilitate\nboth  interdisciplinary  collaboration  and  broad,  truly\nparticipatory  responses  to  sociotechnical  problems[72].\nThe  author  notes  two  particular  limitations:  the\ncomposition of the participants in an Apologos session,\n    304\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\nand the potentially limited impact of such a speculative\ndesign exercise.\nAs with all design, the possible outputs of an Apologos\nsession will be bounded by the experiences, positionality,\nand perspectives of those in the room—as well as the\nshortcomings  inherent  in  the  ideology  of “design”\nthinking  itself[73].  This  is  of  course  an  argument  for\nengaging diverse teams of participants in the work of\ndesign[19]. However, it is also a warning that, as should\nbe obvious, no single Apologos session is sufficient to\nadequately  explore  the  sociotechnical  terrain  of  any\ngiven computational artifact. The broader literature on\nthe  merits  and  challenges  of  participatory  design  in\ncomputing and digital media points to the shortcomings\nof  even  comprehensive  methods  for  widespread\ninclusion  in  the  design  process,  and  Apologos  should\nthus be understood as an introduction to this much larger\nand thornier area of practice. Apologos is an entry point\ninto  sociotechnical/interdisciplinary  collaboration  but\nsustaining and fostering that collaboration require much\nadditional work.\nA second limitation, related to the first, concerns how\nimpactful the outcomes of one, or even many Apologos\nsessions  can  realistically  be  in  changing  the  usual\nactivities  of  a  startup,  corporation,  or  public\ninstitution[21, 74]. An Apologos session, or even several,\nwill  provide  sufficient  insight  to  change  policy.\nHowever,  as  an  entry  to  thinking  critically  about  the\nsociotechnical landscape, Apologos is potentially useful\nin providing a language and framework for technologists\nand  others  who  have  not  had  a  structured  means  to\nconsider such questions before. Apologos should be seen\nthus as an introductory component of a much broader set\nof  developments  around  the  training,  education,\nregulation, and design of digital technologies.\nTo conclude, the author wants to flag one challenge\nthat can be adequately solved only through such broader\nstructural mechanisms: the way divergent disciplinary,\nprofessional,  and  personal  norms  and  expectations\nfundamentally  shape  how  values  are  articulated  and\nincorporated  into  design  decisions,  and  how  such\ndisagreements  can  reflect  a  fundamental  clash  of\nvalues[42]. Although differences in social, professional,\nand  epistemological  norms  comprised  many  of  the\nroadblocks to collaboration during the course of the FIA\nproject,  at  times  project  participants  disagreed  on  an\nontological level: about the fundamental values at issue\nin the development of network architecture and about\nwhich values are more or less important. Privacy is one\nclear  example:  some  members  of  the  computational\nteams were simply not convinced that human privacy as\na  value  superseded  others  such  as  speed,  user\nconvenience, or network security.\nThese  disagreements  entail  what  Flanagan  and\nNissenbaum  term “values  trade-offs”,  or  moments\nwithin the process of creation in which some values are\nprioritized over others[36]. While Apologos provides a\nmechanism for surfacing and highlighting such values\ntrade-offs and their possible effects, it does not provide\nguidance per  se on  how  to  adjudicate  between  such\ntradeoffs. As Flanagan and Nissenbaum observe, “It is\nnot surprising to find that design projects (particularly\nthose with multiple requirements, goals, constituencies,\nand constraints) are rife with clashes and conflicts”[36].\nAnd  while  Apologos  does  provide  for  identifying\n“negative” effects  of  potential  design  decisions  as  a\nmeans for some guidance, on what scale such negatives\nare  judged  remains  at  the  discretion  of  the  particular\ngroup of people doing the designing. Their clashes and\nconflicts,  and  the  broader  structural  conflicts  they\nrepresent,  will  be  both  unique  to  each  situation  and\nchallenging in all cases[19, 42].\nNonetheless,  Apologos  has  utility  as  one  method\namong many in the broader conversation around how to\naccount  for  human  values  and  ethics  in  digital\ntechnologies.  Lightweight  exploratory  methods  like\nApologos  will  ideally  support  space  for  broader\nconversations  around  novel  engineering  solutions[38],\nthe  necessity  of  participatory  design  across  technical\nfields[25], and the necessity of refusal or non-deployment\nas  an  R&D  option[47].  As  sociotechnical  analyses  of\npressing societal challenges become more urgent, and\nmore frequent, we need ongoing focus on the work of\ninterdisciplinary translation and design implementation\nas we navigate the ethics and values designed into and\nemerging from digital technologies.\nReferences\n L. Winner, Do artifacts have politics? in The Whale and\nthe  Reactor:  A  Search  for  Limits  in  an  Age  of  High\nTechnology, L. Winner, ed. Chicago, IL, USA: University\nof Chicago Press, 1986, pp. 19–39.\n[1]\n B.  Friedman  and  H.  Nissenbaum,  Bias  in  computer\nsystems, ACM  Transactions  on  Information  Systems,\nvol. 14, no. 3, pp. 330–347, 1996.\n[2]\n C. O’Neil, Weapons of Math Destruction. New York, NY,\n[3]\n  Luke Stark:   Apologos: A Lightweight Design Method for Sociotechnical Inquiry\n305    \n \n\nUSA: Broadway Books, 2017.\n A.  Selbst,  S.  Friedler,  D.  Boyd,  S.  Venkatasubramanian,\nand J. Vertesi, Fairness and abstraction in sociotechnical\nsystems,\n in \nProc.\n the\n Conference\n on\n Fairness,\nAccountability,  and  Transparency,  Atlanta,  GA,  USA,\n2019, pp. 59–68.\n[4]\n B. Pfaffenberger, “If I want it, it’s OK”: Usenet and the\n(outer)  limits  of  free  speech, The  Information  Society,\nvol. 12, no. 4, pp. 365–386, 1996.\n[5]\n L.  Suchman,  J.  Blomberg,  J.  E.  Orr,  and  R.  Trigg,\nReconstructing  technologies  as  social  practice, American\nBehavioral Scientist, vol. 43, no. 3, pp. 392–408, 1999.\n[6]\n B. Friedman, P. H. Kahn Jr., J. Hagman, R. L. Severson,\nand  B.  Gill,  The  watcher  and  the  watched:  Social\njudgments  about  privacy  in  a  public  place, Human-\nComputer Interaction, vol. 21, no. 2, pp. 235–272, 2006.\n[7]\n K.  Shilton,  Engaging  values  despite  neutrality, Science,\nTechnology, & Human Values, vol. 43, no. 2, pp. 247–269,\n2018.\n[8]\n J.  Gaboury,  Becoming  NULL:  Queer  relations  in  the\nexcluded  middle, Women & Performance: A  Journal  of\nFeminist Theory, vol. 28, no. 2, pp. 143–158, 2018.\n[9]\n M. K. Scheuerman, E. Denton, and A. Hanna, Do datasets\nhave  politics?  Disciplinary  values  in  computer  vision\ndataset development, Proceedings of the ACM on Human-\nComputer Interaction, vol. 5, no. CSCW2, pp. 1–37, 2021.\n[10]\n R. Benjamin, Assessing risk, automating racism, Science,\nvol. 366, no. 6464, pp. 421–422, 2019.\n[11]\n S.  U.  Noble, Algorithms  of  Oppression:  How  Search\nEngines  Reinforce  Racism.  New  York,  NY,  USA:  New\nYork University Press, 2018.\n[12]\n J.\n Buolamwini\n and\n T.\n Gebru,\n Gender\n shades:\nIntersectional  accuracy  disparities  in  commercial  gender\nclassification,  in Proc.  the  1st Conference  on  Fairness,\nAccountability and Transparency, New York, NY, USA,\n2018, pp. 77–91.\n[13]\n S. Barocas and A. D. Selbst, Big data’s disparate impact,\nCalifornia Law Review, vol. 104, pp. 671–732, 2016.\n[14]\n A. L. Hoffmann, Data violence and how bad engineering\nchoices  can  damage  society,  Medium,  https://medium.\ncom/s/story/data-violence-and-how-bad-engineering-\nchoices-can-damage-society-39e44150e1d4 , 2018.\n[15]\n A. L. Hoffmann and L. Stark, Hard feelings — inside out,\nSilicon  Valley,  and  why  technologizing  emotion  and\nmemory  is  a  dangerous  idea, https://lareviewofbooks.\norg/essay/hard-feelings-inside-out-silicon-valley-and-why-\ntechnologizing-emotion-and-memory-is-a-dangerous-idea,\n2015.\n[16]\n M.  M.  Malik,  A  hierarchy  of  limitations  in  machine\nlearning, https://arxiv.org/pdf/2002.05193.pdf, 2020.\n[17]\n M.  Raghavan,  S.  Barocas,  J.  Kleinberg,  and  K.  Levy,\nMitigating  bias  in  algorithmic  employment  screening:\nEvaluating  claims  and  practices,  arXiv  preprint  arXiv:\n1906.09208, 2019.\n[18]\n B.  Fish  and  L.  Stark,  Reflexive  design  for  fairness  and\nother  human  values  in  formal  models,  in Proc.  2021\nAAAI/ACM Conference on AI, Ethics and Society, Virtual\nEvent, USA, 2021, pp. 89–99.\n[19]\n V. Eubanks, Automating Inequality: How High-Tech Tools\nProfile,  Police,  and  Punish  the  Poor.  New  York,  NY,\n[20]\nUSA: St. Martin’s Press, 2018.\n D.  Greene,  A.  L.  Hoffmann,  and  L.  Stark,  Better,  nicer,\nclearer, fairer: A critical assessment of the movement for\nethical  artificial  intelligence  and  machine  learning,\nhttps://hdl.handle.net/10125/59651, 2019.\n[21]\n A.  Rességuier  and  R.  Rodrigues,  AI  ethics  should  not\nremain toothless! A call to bring back the teeth of ethics,\nBig  Data & Society,  vol. 7,  no. 2,  p. 205395172094254,\n2020.\n[22]\n S.  Mohamed,  M.  -T.  Png,  and  W.  Isaac,  Decolonial  AI:\nDecolonial  theory  as  sociotechnical  foresight  in  artificial\nintelligence, \nPhilosophy \n& \nTechnology,\n vol. 33,\npp. 659–684, 2020.\n[23]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  technology  ethics  in  practice, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0018.\n[24]\n M. A. Madaio, L. Stark, J. W. Vaughan, and H. Wallach,\nCo-designing  checklists  to  understand  organizational\nchallenges  and  opportunities  around  fairness  in  AI,  in\nProc.  the  2020  CHI  Conference  on  Human  Factors  in\nComputing System, Honolulu, HI, USA, 2020, pp. 1–14.\n[25]\n N. Manders-Huits and M. Zimmer, Values and pragmatic\naction:  The  challenges  of  introducing  ethical  intelligence\nin technical design communities, International Review of\nInformation Ethics, pp. 1–8, 2009.\n[26]\n I. Poel, An ethical framework for evaluating experimental\ntechnology, Science  and  Engineering  Ethics,  vol. 22,\nno. 3, pp. 667–686, 2015.\n[27]\n J.  van  den  Hoven,  P.  E.  Vermaas,  and  I.  van  de  Poel,\nDesign for values: An introduction, in Handbook of Ethics,\nValues, and Technological Design, J. van den Hoven, P. E.\nVermaas,  and  I.  van  de  Poel,  eds.  Dordrecht,  the\nNetherlands: Springer, 2015, pp. 1–7.\n[28]\n I. van de Poel, J. N. Fahlquist, N. Doorn, S. Zwart, and L.\nRoyakkers, The problem of many hands: Climate change\nas  an  example, Science  and  Engineering  Ethics,  vol. 18,\nno. 1, pp. 49–67, 2011.\n[29]\n S. Gürses and J. van Hoboken, Privacy after the agile turn,\nin Cambridge  Handbook  of  Consumer  Privacy,  J.\nPolonetsky,  O.  Tene,  and  E.  Selinger,  eds.  Cambridge,\nUK: Cambridge University Press, 2018, pp. 579–601.\n[30]\n E. Donahoe and M. M. Metzger, Artificial intelligence and\nhuman  rights, Journal  of  Democracy,  vol. 30,  no. 2,\npp. 115–126, 2019.\n[31]\n Y.  Stevens  and  A.  Brandusescu,  Weak  privacy,  weak\nprocurement:  The  state  of  facial  recognition  in  Canada,\nSSRN Electron J, doi: 10.2139/ssrn.3857355.\n[32]\n N.  Shedroff  and  C.  Noessel, Make  It  So:  Interaction\nDesign  Lessons  from  Science  Fiction.  New  York,  NY,\nUSA: Rosenfeld Books, 2012.\n[33]\n B. Friedman, D. G. Hendry, and A. Borning, A survey of\nvalue sensitive design methods, Foundations and Trends®\nin\n Human–Computer\n Interaction,\n vol. 11,\n no. 2,\npp. 63–125, 2017.\n[34]\n M.  Flanagan,  D.  C.  Howe,  and  H.  Nissenbaum,\nEmbodying values in technology: Theory and practice, in\nInformation Technology and Moral Philosophy, J. van den\nHoven  and  J.  Weckert,  eds.  Cambridge,  UK:  Cambridge\nUniversity Press, 2008, pp. 322–353.\n[35]\n M. Flanagan and H. Nissenbaum, Values at Play in Digital\n[36]\n    306\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\nGames. Cambridge, MA, USA: The MIT Press, 2014.\n K.  Shilton,  Values  and  ethics  in  human-computer\ninteraction, \nFoundations\n and\n Trends®\n in\nHuman–Computer Interaction, vol. 12, no. 2, pp. 107–171,\n2018.\n[37]\n R. Y. Wong and D. K. Mulligan, Bringing design to the\nprivacy  table,  in Proc.  the  2019  CHI  Conference  on\nHuman  Factors  in  Computing  Systems,  Glasgow,  UK,\n2019, pp. 1–17.\n[38]\n A.  Dunne  and  F.  Raby,  Towards  a  critical  design:\nConsuming\n monsters:\n Big,\n perfect,\n infectious,\nhttp://dunneandraby.co.uk/content/bydandr/42/0, 2005.\n[39]\n S.  Lawson,  B.  Kirman,  C.  Linehan,  T.  Feltwell,  and  L.\nHopkins,  Problematising  upstream  technology  through\nspeculative  design,  in Proc.  the  33rd Annual  ACM\nConference  on  Human  Factors  in  Computing  Systems,\nSeoul, Republic of Korea, 2015, pp. 2663–2672.\n[40]\n M. Malik and M. M. Malik, Critical technical awakenings,\nJournal\n of\n Social\n Computing,\n doi:\n10.23919/JSC.2021.0035.\n[41]\n N.  JafariNaimi,  L.  Nathan,  and  I.  Hargraves,  Values  as\nhypotheses:  Design,  inquiry,  and  the  service  of  values,\nDesign Issues, vol. 31, no. 4, pp. 91–104, 2015.\n[42]\n C. A. L. Dantec and C. DiSalvo, Infrastructuring and the\nformation of publics in participatory design, Social Studies\nof Science, vol. 43, no. 2, pp. 241–264, 2013.\n[43]\n K. M. MacQueen, N. T. Eley, M. Frick, L. R. Mingote, A.\nChou,  S.  S.  Seidel,  S.  Hannah,  and  C.  Hamilton,\nDeveloping a framework for evaluating ethical outcomes\nof good participatory practices in TB clinical drug trials,\nJournal  of  Empirical  Research  on  Human  Research\nEthics, vol. 11, no. 3, pp. 203–213, 2016.\n[44]\n D. J. Mir, Y. Shvartzshnaider, and M. Latonero, It takes a\nvillage:  A  community  based  participatory  framework  for\nprivacy  design,  presented  at  2018  IEEE  European\nSymposium\n on\n Security\n and\n Privacy\n Workshops\n(EuroS&PW), London, UK, 2018.\n[45]\n K.  Shilton,  Values  levers:  Building  ethics  into  design,\nScience, Technology, & Human  Values,  vol. 38,  no. 3,\npp. 374–397, 2013.\n[46]\n E.  P.  S.  Baumer  and  M.  S.  Silberman,  When  the\nimplication  isnot  to  design  (technology),  in Proc.  the\nSIGCHI  Conference  on  Human  Factors  in  Computing\nSystems, Vancouver, Canada, 2011, pp. 2271–2274.\n[47]\n B.  Friedman  and  D.  G.  Hendry, Value  Sensitive  Design.\nCambridge, MA, USA: The MIT Press, 2019.\n[48]\n D.  D.  Clark, Designing  an  Internet.  Cambridge,  MA,\nUSA: The MIT Press, 2018.\n[49]\n D. D. Clark, The contingent internet, Daedalus, vol. 145,\nno. 1, pp. 9–17, 2016.\n[50]\n D.  D.  Clark,  The  design  philosophy  of  the  DARPA\nInternet\n Protocols, \nACM\n SIGCOMM\n Computer\nCommunication Review, vol. 25, no. 1, pp. 102–111, 1995.\n[51]\n D. D. Clark, J. Wroclawski, K. R. Sollins, and R. Braden,\nTussle  in  cyberspace:  Defining  tomorrow’s  Internet,\nIEEE/ACM  Transactions  on  Networking,  vol. 13,  no. 3,\npp. 462–475, 2005.\n[52]\n S.  Braman,  The  interpenetration  of  technical  and  legal\ndecision-making\n for\n the\n internet, \nInformation,\nCommunication & Society,  vol. 13,  no. 3,  pp. 309–324,\n[53]\n2010.\n H. Abelson, G. J. Sussman, Structure and Interpretation of\nComputer  Programs.  Cambridge,  MA,  USA:  The  MIT\nPress, 1996.\n[54]\n E. Fisher, M. O’Rourke, R. Evans, E. B. Kennedy, M. E.\nGorman, and T. P. Seager, Mapping the integrative field:\nTaking stock of socio-technical collaborations, Journal of\nResponsible Innovation, vol. 2, no. 1, pp. 39–61, 2015.\n[55]\n A.  S.  Balmer,  J.  Calvert,  C.  Marris,  S.  Molyneux-\nHodgson, E. Frow, M. Kearnes, K. Bulpin, P. Schyfter, A.\nMackenzie,\n and\n P.\n Martin,\n Taking\n roles\n in\ninterdisciplinary collaborations: Reflections on working in\nPost-ELSI spaces in the UK synthetic biology community,\nScience and Technology Studies, vol. 28, no. 3, pp. 3–25,\n2015.\n[56]\n F. Callard and D. Fitzgerald, Rethinking Interdisciplinarity\nAcross  the  Social  Sciences  and  Neurosciences.  London,\nUK: Palgrave MacMillan, 2015.\n[57]\n A.  Quan-Haase,  J.  L.  Suarez,  and  D.  M.  Brown,\nCollaborating,\n connecting,\n and\n clustering\n in\n the\nhumanities, American Behavioral Scientist, vol. 59, no. 5,\npp. 565–581, 2015.\n[58]\n D.  Fitzgerald  and  F.  Callard,  Social  science  and\nneuroscience  beyond  interdisciplinarity:  Experimental\nentanglements, Theory, Culture & Society, vol. 32, no. 1,\npp. 3–32, 2015.\n[59]\n F. Callard, D. Fitzgerald, and A. Woords, Interdisciplinary\ncollaboration  in  action:  Tracking  the  signal,  tracing  the\nnoise, Palgrave  Communications,  vol. 1,  no. 1,  p. 15019,\n2015.\n[60]\n A.  Lucero,  P.  Dalsgaard,  K.  Halskov,  and  J.  Buur,\nDesigning  with  cards,  in Collaboration  in  Creative\nDesign,  P.  Markopoulos,  J.  -B.  Martens,  J.  Malins,  K.\nConinx, and A. Liapis, eds. Cham, Switzerland: Springer\nInternational Publishing, 2016, pp. 75–95.\n[61]\n B. Friedman and D. G. Hendry, The envisioning cards: A\ntoolkit\n for\n catalyzing\n humanistic\n and\n technical\nimaginations, in Proc. the SIGCHI Conference on Human\nFactors  in  Computing  Systems,  Austin,  TX,  USA,  2012,\npp. 1145–1148.\n[62]\n A. Fedosov, M. Kitazaki, W. Odom, and M. Langheinrich,\nSharing  economy  design  cards,  in Proc.  the  2019  CHI\nConference  on  Human  Factors  in  Computing  Systems,\nGlasgow, UK, 2019, pp. 1–14.\n[63]\n J. Knapp, J. Zeratsky, and B. Kowitz, Sprint: How to Solve\nBig Problems and Test New Ideas in Just Five Days. New\nYork, NY, USA: Simon and Schuster, 2016.\n[64]\n S.  Umbrello  and  O.  Gambelin,  Agile  as  a  vehicle  for\nvalues:\n A\n value\n sensitive\n design\n toolkit,\n doi:\n10.13140/RG.2.2.17064.08965/1.\n[65]\n C.  Fiesler,  Ethical  considerations  for  research  involving\n(speculative)  public  data,  in Proc.  the  ACM  on  Human-\nComputer Interaction, vol. 3, no. GROUP, pp. 1–13, 2019.\n[66]\n R. Y. Wong and V. Khovanskaya, Speculative design in\nHCI: From corporate imaginations to critical orientations,\nin New  Directions  in  Third  Wave  Human-Computer\nInteraction:  Volume  2 – Methodologies,  M.  Filimowicz\nand  V.  Tzankova,  eds.  Cham,  Switzerland:  Springer\nInternational Publishing, 2018, pp. 175–202.\n[67]\n G. Cockton, Designing worth is worth designing, in Proc.\n[68]\n  Luke Stark:   Apologos: A Lightweight Design Method for Sociotechnical Inquiry\n307    \n \n\nthe\n 4th \nNordic\n Conference\n on\n Human-Computer\nInteraction:  Changing  Roles,  Oslo,  Norway,  2006,  pp.\n165–174.\n G. Cockton, From doing to being: Bringing emotion into\ninteraction, Interacting  with  Computers,  vol. 14,  no. 2,\npp. 89–92, 2002.\n[69]\n P.  Sengers,  K.  Boehner,  S.  David,  and  J. “Jofish” Kaye,\nReflective design, in Proc. the 4th Decennial Conference\non  Critical  Computing:  Between  Sense  and  Sensibility,\nAarhus, Denmark, 2005, pp. 49–58.\n[70]\n C.  DiSalvo, Adversarial  Design.  Cambridge,  MA,  USA:\nThe MIT Press, 2012.\n[71]\n M.  Sloan,  E.  Moss,  O.  Awomolo,  and  L.  Forlano,\nParticipation  is  not  a  design  fix  for  machine  learning,\narXiv preprint arXiv: 2007.02423, 2020.\n[72]\n L.  Irani, Chasing  Innovation:  Making  Entrepreneurial\nCitizens in Modern India. Princeton, NJ, USA: Princeton\nUniversity Press, 2019.\n[73]\n L.  Stark,  D.  Greene,  and  A.  L.  Hoffmann,  Critical\nperspectives  on  governance  mechanisms  for  AI/ML\n[74]\nsystems,  in The  Cultural  Life  of  Machine  Learning,  J.\nRoberge  and  M.  Castelle,  eds.  Cham,  Switzerland:\nPalgrave Macmillan, 2021, pp. 257–280.\nLuke Stark is an assistant professor in the\nFaculty of Information and Media Studies\nat  Western  University  in  London,  ON,\nCanada.\n His\n work\n interrogating\n the\nhistorical,  social,  and  ethical  impacts  of\ncomputing\n and\n AI\n technologies\n has\nappeared\n in\n journals\n including\n the\nInformation  Society,  Social  Studies  of\nScience,  and  New  Media  &  Society.  He  was  previously  a\npostdoctoral researcher in AI ethics at Microsoft Research, and a\npostdoctoral  fellow  in  sociology  at  Dartmouth  College;  he\nreceived the PhD degree from Department of Media, Culture, and\nCommunication at New York University in 2016, and the BA and\nMA  degrees  from  University  of  Toronto  in  2006  and  2008,\nrespectively.\n    308\nJournal of Social Computing, December 2021, 2(4): 297−308    \n \n\n \nCreating Technology Worthy of the Human Spirit\nAden Van Noppen*\nAbstract:    Spiritual caretakers have been present in every culture throughout human history. We know them\nas ministers, rabbis, lamas, shamans, imams, chaplains, gurus, and wise elders. In modern, secular times, they\nalso include therapists, social workers, meditation teachers, and more. These caretakers support us through birth,\ndeath, and many of the most intimate and complex parts of the human experience. They use skills honed over\nmany years that require paying radical attention to the humanity of others. Yet where is this expertise to be found\nin the creation of the digital technologies that have become portals through which we live, love, learn, grieve,\nand connect with our communities? Those who design and build digital technology must accept that we have\nbecome de-facto spiritual caretakers with the power to treat the well-being of humanity with care or with\nnegligence. Unfortunately, caretaking is a role that computer science degrees do not prepare people for, few\nbusiness models optimize for, and algorithms can not easily solve. This article outlines two concrete best\npractices that can help foster genuine responsibility and care on the part of technologists and technology\ncompanies. First, technologists must recognize that what we create is an expression of our own inner state. Our\nspiritual and emotional health is inextricably linked with our ability to build technology with responsibility and\nwisdom. Second, technologists must create an empowered seat at the table for those with the expertise and\norientation needed to care for our souls, whether from a religious or secular lens.\nKey  words:   spirituality; ethics; well-being; humane technology\n1    Introduction\nSpiritual caretakers have been present in every culture\nthroughout human history. They support people through\nbirth, death, and many of the most intimate and complex\nparts  of  the  human  experience  that  exist  in  between.\nSpiritual caretaking requires paying radical attention to\nthe  humanity  of  others.  Yet  its  nature  is  shifting  in\ndramatic  ways  in  the  Digital  Age,  when  technology\nmediates many aspects of the human experience. When\nSiri  and  Alexa  are  on  the  receiving  end  of  suicidal\npleas[1] and  vaccine  misinformation  spread  on  social\nmedia is killing tens of thousands of people, we live in\na world in which spiritual care is frequently in the hands\nof  algorithms.  This  means  that  the  technologists  who\ncreate them are de-facto spiritual caretakers of our world.\nUnfortunately, caretaking is a role that computer science\ndegrees do not prepare people for, few business models\noptimize for, and algorithms can not easily solve.\nProviding spiritual care has traditionally been among\nthe most respected roles in a society. Spiritual caretakers\ninclude  ministers,  rabbis,  lamas,  shamans,  imams,\nchaplains,  gurus,  wise  elders,  and  more.  In  modern,\nsecular  times,  they  also  include  therapists,  social\nworkers, and meditation teachers. In most cases, they\ndraw  on  tradition,  training,  and  ritual  that  have  been\npassed down for thousands of years. People in these roles\ndeal with some of the most ineffable yet fundamental\ndimensions of the human experience, from our deepest\ngrief to our greatest joy, and help us maintain a sense of\nconnection  to  something  larger  than  ourselves.  This\nwork, often referred to as “pastoral care” in Christian\ntraditions, requires wise attention, compassion, and an\nunderstanding  of  the  responsibility  that  comes  with\n \n • Aden Van Noppen is with  Mobius, San Geronimo, CA 94963,\nUSA. E-mail: Aden@mobi.us.org.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-23;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   02/06  pp309−322\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0024\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\naccompanying  people  through  existential  questions  of\nmeaning, purpose, and our very existence. In 590 AD,\nPope Gregory the Great wrote a manual of pastoral care\nthat is still a foundational teaching text in seminaries and\ndivinity  schools  around  the  world.  In  the  manual,\nGregory writes that “the care of souls is the art of arts”[2].\nIt is not enough for individual technologists to accept\nthe\n spiritual\n implications\n of\n our\n work—the\nresponsibility for spiritual care extends to the institutions\nwithin which we are housed. The interfaith Association\nof  Professional  Chaplains  states,  in  reference  to\nproviding  spiritual  care  in  a  hospital  setting, “many\npersons  both  inside  and  outside  traditional  religious\nstructures report profound experiences of transcendence,\nwonder,  awe,  joy,  and  connection  to  nature,  self,  and\nothers as they strive to make their lives meaningful and\nto maintain hope when illness strikes... Institutions that\nignore the spiritual dimension in their mission statement\nor daily provision of care increase their risk of becoming\nonly ‘biological  garages  where  dysfunctional  human\nparts are repaired or replaced’ (Gibbons & Miller, 1989).\nSuch ‘prisons of technical mercy’ (Berry, 1994) obscure\nthe integrity and scope of persons”.[3] Tech companies\nthat ignore the spiritual dimensions of their work become\nlike  these  hospitals:  garages  where  superficial  desires\nare met but the impacts of their products on our holistic\nwell-being are overlooked.\nThose individuals and institutions wishing to rise to\nthe task of true spiritual care, which we must in order to\nthrive, will need to allow this commitment to lead us past\nour comfort zones. Rising to the task means seeing and\naccepting the suffering we cause ourselves and others by\nadhering to the status quo, and then taking brave action\nto change course at a crucial moment in the history of\nhumanity and technology. It will challenge us to face our\nfears and the dark sides of human nature and capitalism.\nIt  may  mean  altering  the  underlying  structures,  belief\nsystems,  and  assumptions  that  drive  technologies,\nbusiness models, cultures, and organizations as we know\nthem.\nWhether  we  realize  it  or  not,  technologists  and\ntechnology companies are in a position to decide if we\ntreat humanity with care or with negligence. Seriously\naccepting the responsibility of spiritual caretaking will\nrequire valuing care for human souls over care for profit.\nChoosing  profit  will  have  grave  implications  for  the\nwell-being of humanity and the planet.①\nThis article illustrates two concrete best practices that\ncan help foster genuine responsibility and care on the\npart  of  technology  companies.  These  suggestions  are\nbased  on  my  personal  experience  working  at  the\nintersection of technology, ethics, and justice as a senior\nadvisor  to  the  US  Chief  Technology  Officer  in  the\nObama  White  House,  as  a  resident  fellow  at  Harvard\nDivinity  School,  and  more  recently,  founding  and\nleading Mobius. Mobius is a collective of technologists,\nentrepreneurs, scientists, spiritual teachers, artists, and\norganizers working together to create a more responsible,\ncompassionate, and just tech ecosystem.\nThe two interventions I offer here are by no means a\ncomplete solution. Meaningfully addressing the harms\nof  technology  requires  an  ecosystem  of  interventions,\nincluding\n regulation,\n employee\n and\n consumer\nmovements,\n values-oriented\n business\n models,\nempowered  ethics  teams  inside  companies,  and\naddressing the toxicity of the underlying systems that\ngave rise to them in the first place. But all of these efforts\nwill not create technology that is worthy of the human\nspirit—technology  that  shifts  us  from  greed  to\ngenerosity, from anxiety to ease, that heals us and brings\nus together—unless we broaden the frame. Curing what\nails the tech sector also requires us to see the role of\ntechnologist through the lens of caretaking.\nFirst, technologists must recognize that what we create\nis an expression of our own inner state. Our spiritual and\nemotional health is inextricably linked with our ability to\nbuild  technology  with  responsibility  and  wisdom.\nSecond, technologists must create an empowered seat at\nthe  table  for  those  with  the  expertise  and  orientation\nneeded to care for our spiritual and emotional well-being.\nBoth practices have been key to spiritual caretaking for\nmillennia. If adopted as part of a larger ecosystem of\nchanges,  they  could  help  mitigate  the  harms  of\ntechnology, and perhaps even lead to more technology\nthat brings out the best in humanity.\n1.1    A note on language\nThis article attempts to bridge between the spiritual and\nthe  technological.  Despite  the  fact  that  these  two\ndomains  are  inextricably  linked,  they  rarely  speak  to\neach  other.  This  makes  language  inherently  difficult.\n① The  planet  is  included  here  since  digital  technology  so  often\ndisconnects humans from the natural world and makes it easy to “numb\nout” instead of seriously engaging with the realities of climate change, the\nimplications of our treatment of the planet, and the action that is called for\nin response.\n    310\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\nWords\n such\n as \n“soul” \nand \n“spirituality” \ncan\nunderstandably be alienating in secular contexts, but one\ndoes  not  have  to  believe  in  God,  associate  with  a\nreligious tradition, or use this language to connect to the\nunderlying concepts. When I say “spiritual well-being”,\nI am referring to a healthy inner life, sense of wholeness,\nand  connection  to  something  larger  than  oneself.  The\nnearly ubiquitous use of “well-being” in secular spaces\nrefers  to  many  of  the  same  aspects  of  the  human\nexperience.\nI also use “technologist” to refer to a wide range of\nroles and orientations. For the purposes of this piece, a\ntechnologist is anyone making decisions that influence\ntechnology products or services, regardless of their role.\nFor this reason, I include myself in this category. Finally,\nI  recognize  that  there  are  many  kinds  of  technology.\nWhen I say “technology” in this article, I am referring\nprimarily to consumer-facing digital technology.\n1.2    How we got here\nAccepting  and  meeting  the  responsibility  of “care  of\nsouls” contain unprecedented challenges when mediated\nthrough  technology  built  to  succeed  in  the  context  of\ncapitalism,  an  economic  system  that  rewards  greed,\ndivision,  and  competition.  Barriers  include  incentives\nstructures, societal norms and narratives, and the culture\nof the tech sector, to name a few. These interconnected\nsystemic conditions give rise to an endlessly complex\nweb of technologies that are integrated into the fabric of\nnearly every aspect of the human experience.\nWhile there are many benefits to this integration—the\ndemocratization  of  information  access,  the  spread  of\nsocial movements, and the ability to connect with loved\nones  across  continents—the  dark  side  is  also\nincreasingly clear—hacking of elections, the spread of\nviolent extremism via social media, fake news and the\ndegradation of truth, and the mental health implications\nof  tech  addiction.  When  business  models  are  built  to\nmaximize the time we spend engaging with technology,\nit is no wonder we become afraid, violent, polarized, and\naddicted.  When  selling  our  data  is  a  primary  revenue\nstream, it is no wonder we are exposed to highly targeted\npolitical ads and our democracy breaks down.\nYet we may be at a tipping point. There is a perfect\nstorm that may create the conditions needed for greater\nalignment\n between\n technology\n and\n humanity.\nJournalists, academics, consumers, and tech employees\nare  speaking  out  about  the  negative  impacts  of\ntechnology.  Former  and  current  tech  executives  are\nadmitting to feelings of guilt over creating “tools that are\nripping apart the fabric of how society works”[4]. All of\nthese  recognition  and  vocalization  are  leading  to  a\nreckoning in the tech sector with unprecedented levels\nof  motivation  and  courage  to  address  the  negative\nimpacts of tech on our well-being.\nThis  makes  Silicon  Valley  akin  to  a  patient  with  a\nchronic illness in its first flare-up. Some are reacting by\ndeflecting and denying, trying to prevent anyone from\nknowing we are sick[5]. Some are focused on treating the\nsymptoms quickly and superficially to get through the\ncrisis of the moment[6]. A third group wants to find cures.\nThis group is growing and increasingly organized. We\nare  made  up  of  passionate  consumers,  academics,\nfoundations,  tech  employees,  and  civil  society\norganizations such as Data & Society, the UCLA Center\nfor  Critical  Internet  Inquiry,  the  Algorithmic  Justice\nLeague,  and  Mobius,  the  organization  that  I  lead.\nTogether, we are addressing the challenge from a variety\nof angles and beginning to create change that seemed\nimpossible until quite recently.\n1.3    An alternative\nAs long as technologists build tools that touch nearly\nevery aspect of our lives, rising to the task of spiritual\ncare in the Digital Age will be an essential component\nnot just of ethical and responsible design, but also of the\nlarger  systems  change  that  is  needed.  I  outline  two\npowerful yet realistic strategies as places to start. They\nwill  not  come  close  to  shifting  the  direction  of  tech\nalone—they are intended to complement but not replace\nother  regulatory,  cultural,  economic,  and  educational\nreforms to the tech industry.\nFirst,  technologists  must  recognize  that  our  own\nspiritual  and  emotional  states  are  inextricably  linked\nwith  the  ability  to  create  responsible  and  humane\ntechnology. Systems theorist and senior lecturer in the\nMIT  Sloan  School  of  Management,  Otto  Scharmer,\nwrites  about  a  major  blind  spot  in  leadership  theory,\norganizational development, and our everyday lives: we\nrarely recognize the importance of the inner state from\nwhich  our  actions,  decisions,  and  creations  originate.\nScharmer writes that the “inner state of the intervener is\nperhaps  the  most  important  determinant  of  the\nintervention”[7]. Put another way by Wheatley, “without\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n311    \n \n\nreflection,  we  go  blindly  on  our  way,  creating  more\nunintended  consequences,  and  failing  to  achieve\nanything useful[8].” It is no wonder there are so many\nnegative  consequences  of  technology  when  we  are\nsurrounded by innovation created from states of anxiety,\nrushing, and greed.\nI recognize that slowing down is exceedingly difficult\nin  many  tech  companies,  where  company  cultures,\nincentives,  working  conditions,  and  even  job  security\nrely on moving as quickly as possible. Even if it was easy,\nslowing  down  and  bringing  reflection,  mindfulness,\nmeditation, and other well-being practices into tech and\nentrepreneurial cultures are also not enough. While this\ncan  set  important  groundwork  for  shifting  out  of\ndestructive inner states like anxiety and greed and into\nthe thoughtful, clear, and compassionate states needed to\nresponsibly  design  and  build  tech,  it  must  be\naccompanied by an awareness that these very same tools\ncan  be  dangerous  when  used  primarily  as  coping\nmechanisms to feel less anxious and more productive at\nthe individual or company level. In doing so, there is a\nrisk that they become like numbing agents that actually\nkeep the status quo in place. Their misuse can make it\neasier to ignore pain, including the pain caused by the\nproducts technologists build. True spiritual growth will\nactually lead a person to more uncomfortable places and\nsupport the clarity and strength needed to change course.\nChögyam Trungpa, Tibetan Buddhist meditation master\nwho  played  a  major  role  in  the  dissemination  of\nBuddhism in the West, wrote, “meditation is not a matter\nof trying to achieve ecstasy, spiritual bliss or tranquility,\nnor is it attempting to be a better person. It is simply the\ncreation of a space in which we are able to expose and\nundo  our  neurotic  games,  our  self-deceptions,  our\nhidden fears and hopes.”[9] This deeper work is required\nto  create  the  spiritual  and  emotional  states  needed  to\nbuild responsible and humane technology.\nSecond, technologists must create an empowered seat\nat the table for those with the expertise and orientation\nneeded to care for our spiritual and emotional well-being.\nDealing with the delicate territory of the soul requires\nknowledge, skills, and methods that are largely absent in\ntech companies. I am not saying that technologists need\nto be expert caretakers. In fact, it would be dangerous to\nassume we could be. We do not expect everyone to have\nthe  legal  knowledge  of  a  lawyer,  but  no  major  tech\ncompany  would  imagine  creating  a  product  without\nconsulting  one.  Similarly,  we  need  the  humility  to\nrecognize  the  nuanced  caretaking  knowledge  and\nwisdom that exists outside the walls of our companies\nand seek out that expertise. Their perspective should be\nembedded in product design and strategy at all levels.②\nI offer these two strategies based on my experiences\nsupporting tech leaders who are committed to taking the\nresponsibility of spiritual caretaking seriously. I work\nwith technology leaders who share the mission to put our\nindividual and collective well-being at the center of what\nthey are building. Some of these people are among the\nmost  influential  in  Silicon  Valley:  they  control\nmultibillion-dollar portfolios, oversee tens of thousands\nof  employees,  and  influence  the  direction  of\ntechnologies that affect billions of people globally. Yet,\neven  with  this  mission  and  power,  they  are  working\nwithin  systems,  incentive  structures,  and  cultures  that\nare designed to keep the status quo in place.③\nMobius supports these mission-aligned leaders in two\noverlapping ways. Each contributes to the shifts called\nfor above. First, we bring these leaders together, across\ncompetitors, into a nurturing and supportive community\nthat\n builds\n the\n trust\n needed\n to\n make\n their\ncompany-specific work bigger than their sum of its parts.\nSecond, we curate groups of the world’s leading experts\non well-being and caretaking to advise on product and\nstrategy. These experts have deep wisdom on how to care\nfor  our  well-being.  They  span  from  senior  spiritual\nteachers (such as Jack Kornfield and Roshi Joan Hallifax)\nto prominent neuroscientists studying the development\nof compassion and empathy (such as Dr. Sará King and\nDr.  Emiliana  Simon-Thomas),  and  scholars  of  racial\njustice and healing (such as Dr. Angel Acosta and john\na powell). While some have previously been invited to\n② This often requires bringing in people who are not currently on tech\nteams,  but  one  must  be  careful  of  creating  the  false  dichotomy  that\ntechnologists cannot also be spiritual caretakers and spiritual caretakers\ncannot also be technologists. There are brilliant people who bridge that\ndivide, but it is rare to find that combination in a single person or existing\ntech team.\n③ Some may argue that senior leaders at the tech giants are inherently\nunethical  and  should  not  be  supported.  We  choose  to  support  these\nleaders because we believe that systemic change requires shifts from both\ninside  and  outside  the  major  tech  companies.  We  know  firsthand  that\nthere are many people working at Google, Facebook, Twitter, and other\nbig  tech  companies  who  are  deeply  concerned  with  the  negative\nconsequences of their technologies. Instead of being in denial, they are\npushing for responsible strategies to change course. These employees are\nfound at all levels of the companies, from the most junior employees to\nthe C-suite. Mobius works with senior executives because of the scale of\ntheir  influence,  and  we  collaborate  closely  with  other  civil  society\norganizations  who  are  supporting  mission-aligned  tech  employees\nthroughout all levels of the companies.\n    312\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\nvisit a tech company to lead a meditation or give a talk,\nthey are almost never in the rooms where products are\ndesigned. Mobius also weaves ancient practices such as\nmeditation, reflection, and ritual into our facilitation in\norder to create inner states of compassion, clarity, and\ncourage while decisions are being made about products.\nEach of these strategies is complex, and we recognize\nthat\n there\n are\n potential\n unintended\n negative\nconsequences  of  our  work  as  well,  including  the\npossibility of “ethics washing” when tech companies are\nable  to  say  they  consulted  with  experts  regardless  of\nwhether they integrate the recommendations.\nWhile our work is far from a silver bullet, our hope is\nto help equip technologists to more responsibly take on\nthe work of spiritual and emotional caretaking in tech’s\nnext chapter. This article is grounded in what I see as we\nwork to support tech leaders and their teams to create\ntechnology that not only avoids harm, but also brings out\nthe best in humanity.\nFrom this perspective, this article takes a close look at\ntechnology’s  complex  impact  on  our  individual  and\ncollective  well-being,  followed  by  a  deeper  dive  into\neach of the two interventions called for above and some\nof the promising interventions that are already or should\nbe happening. Finally, I discuss what this all adds up to\nand how it fits into the growing ecosystem of changes\nthat, even though we have a long way to go, are pushing\nthe tech sector to value our shared well-being over the\nfastest route to profit.\n2    The Status Quo\nSome say we are in the midst of a “Fourth Industrial\nRevolution”, driven by the rapidly growing and nearly\nubiquitous integration of digital technology into all parts\nof  society[10].  This  is  by  no  means  humanity’s  first\ntechnological\n transformation,\n but\n never\n has\n a\ntransformation  been  so  intimately  linked  with  nearly\nevery  aspect  of  our  lives.  Billions  of  people  use\ntechnology as a primary portal through which to work,\nplay, learn, and love. As a consequence, the direction of\ntechnology has profound and rapidly shifting effects on\nour individual and collective well-being.\n2.1    The dark side of tech—implications of negligent\n“spiritual caretakers”\nThe dominant business models, cultures, and norms in\nthe tech sector have led to technology that frequently and\noften consciously preys on the most vulnerable parts of\nhuman  nature.  We  are  surrounded  by  devices  and\nplatforms  that  hijack  our  attention  and  keep  us  from\nconnecting  deeply  with  ourselves,  others,  and  the\nphysical world around us. The negative implications of\nsuch technology are increasingly clear. Tech executives\nand their teams are facing one ethical quandary after the\nnext,  ranging  from  the  spread  of  misinformation\nbreaking  apart  our  civic  fabric,  to  the  mental  health\nimplications  of  seventy  two  percent  of  teens  in  the\nUnited States feeling the need to immediately respond to\nnotifications  on  their  phones,  to  a  steady  stream  of\natrocities  such  as  Facebook  posts  inciting  genocide\nagainst the Rohingya Muslims[11].\nMany problems stem from the mental and emotional\neffects of spending more time connected to our digital\ndevices. Adults in the United States spend an average of\neleven hours a day interacting with screens—nearly half\nour lives[12]. Netflix’s CEO recently said that sleep is\ntheir biggest competitor[13]. I would argue that the health\nof our intimate relationships is a close second. As Turkle\nwrites, “We have become accustomed to a new way of\nbeing ‘alone together’. Technology-enabled, we are able\nto be with one another, and also elsewhere, connected to\nwherever we want to be.”[14]\nAs just one example, millions of young people allow\ntheir friendships to hang in the balance of whether they\nmaintain their Snapchat “streak”, a feature that relies on\nfriends sending direct snaps back and forth with each\nother every day. The longer one goes without breaking\nthe chain of communication, the longer the streak and the\n“stronger” the  friendship.  Some  Snap  users  manage\nhundreds of streaks simultaneously, and many go so far\nas to have their friends log into their accounts to maintain\ntheir streaks if their phone is taken away by parents[15].\nThis highly addictive feature preys on a wide swath of\na  psychologically  vulnerable  population—sixty  nine\npercent of American teenagers use Snapchat[16].\n2.1.1    Designing for addiction\nIt makes sense that there are so many negative impacts\nwhen  we  look  at  the  context  within  which  these\ntechnologies  are  created.  Engineers  and  designers  are\nfrequently  driven  to  build  highly  addictive  features\nbecause of the business models of the companies that\nemploy  them. “It  is  as  if  they  are  taking  behavioral\ncocaine and just sprinkling it all over your interface and\nthat is the thing that keeps you coming back and back and\nback”, said Aza Raskin, former senior leader at Mozilla\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n313    \n \n\nand  Jawbone.  Raskin  invented  the “infinite  scroll” in\n2006, an extremely common feature of apps that allows\nusers to endlessly swipe down through content without\nextra  click[17].  The  infinite  scroll  was  designed  to  be\n“maximally addictive … if you do not give your brain\ntime  to  catch  up  with  your  impulses  you  just  keep\nscrolling”. This matters because “in order to get the next\nround of funding, in order to get your stock price up, the\namount of time that people spend on your app has to go\nup”, Raskin said. “So, when you put that much pressure\non that one number, you are going to start trying to invent\nnew ways of getting people to stay hooked.” Raskin was\nironically  working  at  a  tech  company  called\n“Humanized” when  he  invented  the  infinite  scroll.  In\naddition to the interventions and mind shifts discussed in\nthis article, Raskin’s point reinforces the importance of\nactions such as changes in policy, funding, and business\nmodels.\nRaskin  went  on  to  cofound  the  Center  for  Humane\nTechnology (CHT) in 2018 with former Googler Tristan\nHarris.  CHT  is  part  of  a  growing  set  of  advocacy\norganizations that are building a movement to “realign\ntechnology with humanity”. Raskin is among a relatively\nlarge community of technologists who admit feelings of\nguilt about the consequences of the tools they helped\ncreate and are working to shift the direction of the tech\nsector as former and current tech insiders.\nChamath  Palihapitiya,  Facebook’s  former  vice\npresident for User Growth, left the company in 2018 said\nhe felt “tremendous guilt” over his role in creating “tools\nthat  are  ripping  apart  the  social  fabric”.  He  said,  in\nreference not just to Facebook, but to the wider online\necosystem,  that, “The  short-term,  dopamine-driven\nfeedback loops that we have created are destroying how\nsociety  works.  No  civil  discourse,  no  cooperation,\nmisinformation, mistruth. This is not about Russian ads,”\nhe added. “This is a global problem. It is eroding the core\nfoundations of how people behave by and between each\nother”[4].\nIt is not just former big tech executives speaking out.\nThere are countless scholars and activists who have been\nsounding the alarm bell for decades. As many point out,\nseemingly minor design choices, such as Snapchat’s fire\nemoji that indicates whether a streak is still going, the\nbuzz of the phone with each new email, or the infinite\nscroll that keeps us refreshing our feeds by swiping down\non  the  screen,  add  up  to  a  bigger  picture  with  grave\nimplications for our mental health and the health of our\nclose relationships, civic fabric, and even our planet[18].\n2.1.2    The fuel of toxic culture\nIn addition to the business models, the dominant culture\nof Silicon Valley drives people to create technology that\ntreats the well-being of humanity with recklessness. This\nis true on the company and industry levels. “Move fast\nand break things” is not how pastoral care works. Even\nthough Facebook founder Mark Zuckerberg has publicly\nchanged this company motto, it is in their cultural DNA.\nFacebook  structures  their  strategic  planning  and\nperformance  reviews  in “halves”,  or  six-month\nhorizons[19].  The  public  pays  the  price  of  Facebook’s\nshort-term thinking. For example, algorithms designed\nto  maximize  our  time  on  the  site  have  numerous\nconsequences,  many  of  which  can  be  avoided  with\nscenario planning and foresight. One such consequence\nis that these algorithms separate us into “filter bubbles”\nwithin  which  we  are  primarily  fed  content  that  we\nalready  agree  with,  thus  making  our  worlds  smaller\ninstead  of  bringing  us  together[20].  Moving  fast  and\nbreaking things do not stay within Facebook’s walls. It\nis indicative of a larger culture of “disruption” and the\ncommon belief that more and faster is always better. This\norientation  runs  completely  counter  to  acting  with\nawareness, intention, and care.\nThis culture of speed and recklessness is not unique to\nthe  tech  sector,  or  even  to  the  private  sector.  It  is\npronounced  across  most  industries  and  seeps  into\npeople’s private lives by the nearly ubiquitous presence\nof our devices. People suffer information overload and\nthe expectation that we are constantly plugged in and\navailable. In his manual of pastoral care, Pope Gregory\nthe  Great  warned  about  the  impact  of  this  fractured\nattention. When the minister distracts their heart “with\na diversity of things, and as his mind is divided among\nmany interests and becomes confused, he finds he is unfit\nfor any of them and becomes so preoccupied during its\njourney as to forget what its destination was”[2]. Jack\nKornfield,  a  well-known  American  Buddhist  teacher\nand Mobius founding senior advisor, explains it another\nway. “We live in a society that almost demands life at\ndouble time, speed and addictions numb us to our own\nexperience. In such a society, it is almost impossible to\nsettle into our bodies or stay connected with our hearts,\nlet alone connect with one another or the earth where we\nlive”[21]. Even those who go into a tech company with a\n    314\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\nclear social mission are prone to forget the destination\nwhen they are swimming in rapid currents of short-term\ntargets,  emails  and  slack  notifications,  and  rushing  to\nrelease new features before the competition does.\nThe  tech  sector’s  bias  toward  speed  and  short-term\nthinking are also compounded by the nature of the ethical\nreckoning we are going through. Since it is like being\ndiagnosed with an illness that has no simple cure and\nconstantly evolving symptoms, there is understandably\na  new  level  of  fear  and  overwhelm  that  puts  many\ntechnologists into crisis mode, even as we try to work\ntoward solutions. This means strategies to make things\nbetter  are  often  created  within  the  same  short-term,\nquick-fix,  and  fearful  approach  that  got  us  into  this\npredicament in the first place.\nIn contrast, nearly every spiritual tradition teaches us\nthat contemplative practices and slowing down to gather\nand  focus  attention  are  a  necessary  step  towards\nresponsible and wise action. Jews observe the Sabbath\nby taking a full day of rest, reflection, and prayer every\nweek. Jews and non-Jews are putting a modern spin on\nShabbat  by  observing “tech  sabbath” as  a  sustained\nperiod  of  unplugging[22].  Observant  Muslims  perform\nritualized prayer called “Salah” five times a day. This\npractice of stepping away at regular intervals is not only\nto connect with God, but also to “purify the heart”, which\nin Islam is considered to be the center of all feelings,\nemotions,  desires,  remembrance,  and  attention.  This\npractice\n of\n stopping,\n resting,\n reflecting,\n and\nreconnecting  with  the  heart  is  a  foil  to  the  modus\noperandi of most tech companies.\n2.2    The  high  side  of  tech—enabling  the  most\npositive human qualities\nDigital technology can and often does enable us to live\nmore connected lives of meaning. For example, there are\ntransgender teens in rural America who develop their\nemerging queer identities online through social media\naffinity  group[23].  Facebook  introduced  thoughtful\nmemorialization  features  that  recognize  the  complex\nemotions that are intertwined with the Facebook page of\nsomeone who passed away. Loved ones can activate a\ntribute page and new algorithms prevent memorialized\nprofiles from showing up in “places that might cause\ndistress”,  like  event  recommendations  and  birthday\nreminders[24].  Caring  for  someone’s  community  when\nthey die is a classic pastoral role. Not coincidentally the\ndesign of these features was led by a Buddhist chaplain\nwho was trained in how to provide this care offline. All\nspiritual traditions have rituals and practices related to\ndeath, and Facebook’s tribute page for the deceased is\nalso reminiscent of the Jewish practice of sitting shiva.\nFamily members observe seven days of mourning during\nwhich the community brings food and shares memories\nof the person who has died. Facebook’s memorialization\nfeatures are a concrete example of what it looks like to\ndraw on offline ancient and sacred rituals to care for us\nonline.\nOn the societal level, just as tech divides, polarizes,\nand dehumanizes, it also enables us to come together at\nunprecedented  scales.  Many  of  the  most  significant\nsocial  movements  of  our  time  were  fueled  in  part  by\nhashtags. In July of 2020, shortly following the murder\nof  George  Floyd,  the  #BlackLivesMatter  hashtag  had\nbeen used 47.8 million times on Twitter from May 26th\nto June 7th, 2020. That is just under 3.7 million times per\nday[25]. Since its origin in a Facebook post after the 2012\nshooting of 17-year-old Trayvon Martin, the hashtag has\nbecome a central unification and mobilization tool for\nthe most widespread and visible racial justice movement\nsince the 1960s[25]. From October 16th, 2017 until May\n1st, 2018, #MeToo appeared an average of 61911 times\nper  day  on  Twitter,  dramatically  shifting  the\nconversation about sexual assault in the United States[26].\nThe above examples show that it is possible to use\ntechnology as a tool to bring out the best in humanity.\nWhat  if  technologists  designed  for  that  instead  of\ndesigning to maximize the amount of time spent, and\nattention extracted? What if tech encouraged pausing,\nand approached every design decision with mindfulness\nand compassion? Above all, what if technologists valued\ndeep expertise on how to care for our well-being as much\nas the expertise of great engineering and design? And\nwhat  if  we  acted  as  if  the  care  of  our  souls  is  more\nimportant than how easy it is to refresh our Twitter feeds?\n3    Intervention\nThere is an increasing number of people looking for a\ncure to Silicon Valley’s chronic illness. This includes\npolicymakers, organizers and activists, tech employees,\nconsumers,  journalists,  scholars,  and  former  tech\ninsiders speaking out about the implications of what they\nbuilt. A true ethical transformation of the tech sector will\nrequire  bold  regulation,  outside  pressure,  values-\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n315    \n \n\noriented  business  models,  empowered  ethics  teams\ninside companies who are not reprimanded for speaking\nthe truth, and humane company cultures. It necessitates\nlifting  up  leadership  and  perspectives  that  are  often\nunrecognized by the mainstream technology sector and\nensuring that a multitude of world views and skills are\nshaping its future. And long-term solutions rest on the\nherculean  task  of  disentangling  ourselves  from  the\ntentacles of an economic system fueled on greed.\nThat said, technology companies are not monoliths.\nThey are made up of people with agency who are making\ndecisions every day. Many of the individuals working\ninside  the  tech  sector  were  drawn  in  part  by  the\ncompanies’ stated values and missions, many of which\nwe  now  know  are  dangerously  idealistic  and  naive.\nTwitter’s  mission  is  to “give  everyone  the  power  to\ncreate and share ideas and information instantly without\nbarriers”. Facebook’s mission is to “build community\nand bring the world closer together”, and Steve Jobs’\narticulation  of  Apple’s  mission  was  to “make  a\ncontribution to the world by making tools for the mind\nthat advance humankind”. Capitalism, culture, and the\ncomplexity of the relationship between tech and humans\nhave warped these missions at the expense of the long-\nterm health of society. Yet, it is important to remember\nthat the altruistic impulses of many of the people who\nmake up the tech sector remain and can be the seeds of\naccepting  the  moral  responsibility  that  comes  with\nholding our spiritual well-being in their hands.\nThere  are  increasingly  concrete  examples  of  tech\nexecutives making unconventional choices that return to\nthe original intentions behind their mission statements.\nFor  example,  despite  the  fact  that  it  clearly  hurts\nshort-term profits, Twitter’s CEO Jack Dorsey, banned\npolitical  ads  in  the  leadup  to  the  2020  presidential\nelection because “Internet political ads present entirely\nnew\n challenges\n to\n civic\n discourse:\n machine\nlearning-based\n optimization\n of\n messaging\n and\nmicro-targeting,  unchecked  misleading  information,\nand deep fakes. All at increasing velocity, sophistication,\nand overwhelming scale. These challenges will affect all\ninternet communication, not just political ads. Best to\nfocus  our  efforts  on  the  root  problems,  without  the\nadditional  burden  and  complexity  taking  money\nbrings”[27].  If  one  reads  between  the  lines,  Dorsey  is\nsaying  that  Twitter’s  mission  to “share  information\ninstantly  without  barriers” is  not  actually  in  the  best\ninterest of society. Twitter’s vice president of Revenue\nand Content Partnerships, Matt Derella, also stated that\n“We want to make sure we don’t create filter bubbles\nwith this powerful ad system we have”[28]. There is a\nlong  way  to  go,  but  both  Dorsey  and  Deralla  are\nacknowledging the moral responsibility that comes with\ntheir power and they are taking action as a result.\nBelow I present two shifts that, while only part of the\nsolution, are required for responsible spiritual care in the\nDigital  Age,  and  they  are  often  overlooked.  First,\ntechnologists  must  pay  closer  attention  to  their  own\nspiritual and emotional states, as that gives rise to the\nproducts  we  create.  Second,  we  must  make  sure  that\nthose with the wisdom and expertise to care for our souls\nare helping to shape tech products and strategies.\n3.1    Shifting the inner state of the intervener\nTechnologists must recognize that our own spiritual and\nemotional health is paramount, especially because of the\nways that power and stress blind us. Gregory the Great\nwarned ministers in 590 AD of the propensity for power\nto cloud the mind and the heart. “What else is power in\na post of superiority but a tempest in the mind, wherein\nthe  ship  of  the  heart  is  ever  shaken  by  hurricanes  of\nthought”[2]. Operating inside the clouds of power and\nprivilege\n makes\n it\n even\n more\n important\n that\ntechnologists  cultivate  the  awareness  and  spiritual\nfortitude to see clearly the implications of our decisions\nand to design from a place of wisdom and compassion.\nAs the systems theorist and author Margaret Wheatley\nsaid, “without  reflection,  we  go  blindly  on  our  way,\ncreating more unintended consequences, and failing to\nachieve anything useful”[8].\n3.1.1    Spiritual bypassing\nIronically,  much  of  the  tech  sector  already  embraces\nspiritual  language  and  ancient  practices,  but  often  for\nself-serving ends that unwittingly disrespect the sanctity,\ndepth,  and  intentions  behind  them.  Entrepreneurs  are\nusing  the  South  American  ceremonial  hallucinogen\nayahuasca  to  come  up  with  more  creative  business\nideas[29], there are thousands of people on the waitlist for\nGoogle’s two-day intensive mindfulness course[30], and\nwhole startup teams are fasting for 36 hours to improve\nclarity[31].\nIn  contrast,  most  spiritual  and  religious  traditions\ninclude fasting as a sacred act of renunciation, atonement,\nor  connection  with  God.  Fasting  during  Ramadan  is\nconsidered one of the five pillars of Islam. It is meant to\nreduce greed and increase empathy for those who are\n    316\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\npoor  and  hungry,  thus  encouraging  acts  of  generosity\nand  charity.  Using  fasting  to  increase  profit  is  an\noffensive perversion of the altruistic intention behind the\npractice.\nApplying  ancient  practices  in  modern,  secular\ncontexts  is  not  negative  in  principle,  but  when  these\npractices are primarily a way to feel less overwhelmed\nand more productive as individuals or companies, they\nrisk becoming a numbing agent that makes it easier to\nignore  our  own  pain  and  the  pain  caused  by  our\ninstitutions.  If  spiritual  work  does  not  go  beyond  our\nown  self-interest  we  risk  engaging  in  a  collective\n“spiritual bypass”, the use of spiritual ideas and practices\nto avoid facing reality, especially if it involves feeling\npain and discomfort[32].\nThere is a long history of spiritual bypassing and using\nspiritual practices to maintain destructive practices and\ninstitutions. The role of “chaplain” as we know it was\nestablished for the US Army in 1775, when Congress\nauthorized  one  chaplain  for  each  regiment  of  the\nContinental  Army.  Since  then,  the  official  mission  of\nArmy  Chaplains  has  been  to  assess  and  boost  the\n“spiritual  fitness” of  the  soldiers.  It  is  believed  that\nspiritual fitness is a key component of “soldier readiness\nand force protection”, and that it improves the soldier’s\nability to cope with the guilt of killing other people and\nthe  tragedy  of  losing  their  fellow  soldiers[33].  It  is\nundeniable  that  the  mental  health  and  spiritual\nwell-being of soldiers is important—the traumas many\nsoldiers experience are more extreme than most of us can\nimagine,  and  twenty  veterans  commit  suicide  every\nday[34]. But this focus on spiritual fitness puts band-aids\non  deep  wounds  long  enough  for  soldiers  to  keep\nfighting,  but  without  actually  addressing  their  well-\nbeing in the long run. At the collective level, it helps keep\na  violent  status  quo  in  place  even  when  there  are\ncountless moral and ethical reasons to question it.\nSpiritual bypassing is built into the very fabric of our\nculture  and  economy.  The  Cherokee  healer  and\npsychologist  Anne  Wilson  Schaef  writes, “the\nbest-adjusted person in our society is the person who is\nnot dead and not alive, just numb, a zombie. When you\nare dead you are not able to do the work of society. When\nyou  are  fully  alive  you  are  constantly  saying ‘No’ to\nmany of the processes of society, the racism, the polluted\nenvironment, the nuclear threat, the arms race ... Thus it\nis in the interest of our society to promote those things\nthat take the edge off, keep us busy with our fixes, and\nkeep us slightly numbed out and zombie-like. In this way\nour  modern  consumer  society  itself  functions  as  an\naddict”[35].\nThe tech sector is no exception. Many tech employees\nare  using  meditation  and  mindfulness  to  increase\nproductivity so they can build the tools that hijack our\nattention and make it harder for us to exist outside of the\ndigital realm. There is deep hypocrisy in the fact that\nMark Zuckerberg does not let his daughter use Facebook\nMessenger  Kids,  and  Steve  Jobs’ children  had  strict\nlimits  on  technology  use  at  home[36].  The  most\nsought-after  private  school  in  Silicon  Valley,  the\nWaldorf School of the Peninsula, bans technical devices\nfor  those  under  eleven  and  teaches  the  children  of\nGoogle, Uber, Ebay, and Apple how to make go-karts,\nknit,  and  cook,  saying  that  computers  inhibit  creative\nthinking,  movement,  human  interaction,  and  attention\nspans.  As  Alice  Thompson,  an  associate  editor  and\nweekly columnist for The Times in the UK said, “It is\nastonishing if you think about it: the more money you\nmake out of the tech industry, the more you appear to\nshield  your  family  of  its  effects”.[37] This  is  akin  to\ntobacco  executives  saying  cigarettes  have  no  harmful\nhealth effects while banning their own teenagers from\nsmoking.\nYet it is easier to maintain cognitive dissonance than\nto reckon with the deep hypocrisy of choosing to build\nsomething  that  one  knows  is  causing  harm.  As  the\nTibetan nun Pema Chödrön writes, “We can spend our\nwhole lives escaping from the monsters of our minds”,\nand the misuse of spiritual practices and rituals can be a\npowerful way to do this[38].\n3.1.2    Moving from spiritual bypassing to wise action\nSpiritual  practices  can  also  cultivate  the  courage  and\nresilience to be with discomfort and look more honestly\nat the implications of one’s actions. They can increase\nawareness  of  the  rampant  narratives  and  cultures  that\nmaintain the delusion of social benefit when the reality\nis far darker.\nThis can be difficult since humans are hardwired to run\naway from pain and seek pleasure. But moving beyond\nthe  use  of  spiritual  practices  purely  for  individual\nenhancement is a necessary step toward more ethical and\ncompassionate\n technology.\n It\n means\n taking\nresponsibility for the fact that our inner state shapes the\ndecisions we make and what we create. Therefore, it is\nreckless  not  to  cultivate  awareness  in  service  of  a\nmission that is larger than oneself.\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n317    \n \n\nFew people articulate the relationship between one’s\ninner state and what one creates better than the Quaker\nauthor and activist, Parker Palmer, in his explanation of\nthe  mobius  strip,  a  surface  with  the  mathematical\nproperty  of  being  unorientable,  causing  it  to  appear\ndouble-sided even though it has only one side[39].\n“If you take your index finger and trace what seems to\nbe  the  outside  surface,  you  suddenly  find  yourself  on\nwhat seems to be the inside surface. Continue along what\nseems to be the inside surface, and you suddenly find\nyourself on what seems to be the outside surface. What\nlooks  like  its  inner  and  outer  surfaces  flow  into  each\nother seamlessly, co-creating the whole. The first time I\nsaw a Mobius strip, I thought, ‘Amazing! That is exactly\nhow  life  works!’ Whatever  is  inside  of  us  continually\nflows  outward,  helping  to  form  or  deform  the\nworld—depending  on  what  we  send  out.  Whatever  is\noutside us continually flows inward, helping to form or\ndeform us—depending on how we take it in. Bit by bit,\nwe and our world are endlessly re-made in this eternal\ninner-outer exchange. Much depends on what we choose\nto put into the world from within ourselves—and much\ndepends on how we handle what the world sends back to\nus…\nHere’s the question I’ve been asking myself ever since\nI understood that we live our lives on the Mobius strip:\n‘How can I make more life-giving choices about what to\nput into the world and how to deal with what the world\nsends back—choices that might bring new life to me, to\nothers, and to the world we share?’”\nThe connection between inner and outer states means\nthat technologists have a moral responsibility to create\ncompany  cultures  that  encourage  reflection  and\ncompassion.\nPalmer’s  discussion  of  the  Mobius  strip  is  the\nmotivation  behind  my  organization’s  name.  Mobius’\ngoal  is  to  help  tech  leaders  shape  technology  for  the\nwell-being  of  humanity,  in  part  by  helping  them,  as\nPalmer suggests, “make more life-giving choices about\nwhat to put into the world and how to deal with what the\nworld  sends  back”.  In  doing  so,  we  aim  to  help\ntechnology leaders and their teams act ethically as they\ndesign the products that shape our experience of being\nhuman. We try to create the conditions for them to treat\ntechnology development as an act of pastoral care by\n“paying  radical  attention” to  their  humanity  and  the\nhumanity of those who use their products.\nEven moving beyond spiritual bypassing is not enough\nif the awareness that results does not influence product\ndecisions. This requires responsibly integrating spiritual\npractices  into  the  design  process  itself,  moment  to\nmoment. This can feel uncomfortable in work settings,\nwhere culture often discourages merging the “spiritual”\nwith  the “professional”.  This  is  especially  true  in\npredominantly  secular  environments  such  as  Silicon\nValley. Seventy percent of adults in the San Francisco\nBay Area, the heart of the tech industry in the US, are\nreligiously unaffiliated, atheist, or agnostic. There are\noften appropriate reasons for separating religion and the\nworkplace,  especially  with  the  risk  of  discomfort  or\ndiscrimination  based  on  religious  beliefs.  However,\nthere  are  ways  to  sensitively  bring  the  benefits  of\nspiritual practices into the workplace without including\nthe baggage that so often understandably accompanies it.\nIt may sound insignificant in comparison to the scale of\nthe challenge, and in many ways, it is, but inserting small\nmoments of mindfulness that are explicitly connected to\nimpact can shift the inner states of the people building\ntechnology, so we are more reflective and connected to\nour own intentions and the implications of our decisions.\nGiven that tech companies are made up of individuals\nmaking  decisions  all  day,  this  can  have  an  outsized\nimpact.  And,  even  so,  it  is  important  to  note  the\nlimitations. Simply being more reflective will not get us\nto where we need to go. That claim would ignore the\nrealities of working within institutions that incentivize\nbehavior  that  is  often  in  direct  contrast  with  ethical\ndecisions.\nHowever,\n a\n masterclass\n on\n the\n impact\n of\ncontemplative practices supporting social change comes\nfrom the Leadership Conference of Women Religious,\nthe leadership body of Catholic nuns in the US. In 2012,\nthe nuns were being investigated by the Vatican for their\nfeminist beliefs and political advocacy for LGBTQ and\nreproductive rights, which, they were told, ran counter to\nchurch  doctrine.  Their  meetings  began  with  thirty\nminutes of silent contemplation, a simple practice that\nbolstered  their  courage,  resilience,  and  ability  to  act\nwisely  while  under  fire[40].  Similarly,  the  Quaker\npractice of silent listening, followed by speaking when\nmoved, arguably helped create the foundation of clarity\nand bravery that enabled Quakers to become some of the\nfirst White abolitionists. In the realm of physical design,\ntraditional Chinese gardens build bridges according to\nZen philosophy and teachings. The bridges proceed in\n    318\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\nright  angles,  not  straight  lines,  such  that  the  person\nwalking needs to slow down and be mindful. Otherwise,\nthey  risk  falling  into  the  water.  These  are  just  three\nexamples of what a culture of more deeply integrated\nmindful practice might look like.\nCatholic nuns, Quakers, and Zen philosophers have\nunderstood for centuries how even small amounts of this\nkind  of  pause,  especially  amidst  crisis  and  urgency,\nprovide the clarity to take courageous and ethical action\nincluding in the fight for feminist rights and the abolition\nof slavery. This tipping point moment in the tech sector\ncalls for similar levels of courage.\nMobius is witness to the power of small moments of\nmindful  pause  when  we  facilitate  advising  sessions\ninside  tech  companies.  Thirty  minutes  of  silence  is\nambitious  in  standard  corporate  settings,  but  even\nsmaller moments of intentional pause and reflection can\nmake  a  difference.  Pauses,  especially  in  the  midst  of\noverwhelming  to-do  lists  and  overflowing  inboxes,\nincrease  the  possibility  of  making  more  conscious\nchoices. Especially if there is a deliberate effort to go\nbeyond spiritual bypassing, these pauses can help set the\nfoundation for transformation and changing course.\nOne example comes from Mobius’ work with one of\nthe largest tech companies to create more nuanced and\nresponsible  well-being  metrics  to  understand  how  the\nplatform affects peoples’ mental and emotional health.\nWhat they find will inform product decisions across the\ncompany. Their definition of “well-being” will have a\nglobal impact. We facilitated a workshop that brought\ntogether  outside  experts,  including  spiritual  teachers,\nwith  the  company’s  well-being  team.  The  meditation\nteacher, Jack Kornfield, began by leading a meditative\nreflection on the fact that, given their reach, influencing\nthe company’s definition of well-being directly impacts\nthe well-being of humanity. He named that this is both\na privilege and includes great responsibilities. We then\nled the team through a process of envisioning the impact\nthey want to have on people and setting intentions. These\nefforts  grounded  the  rest  of  the  advising  session  in  a\nsense  of  purpose  that  was  much  deeper  than  meeting\ntheir  six-month  targets.  The  moment  of  pause  was\nsimple, and yet we heard from the team that this was a\nradical act of slowing down in the context of a company\nculture that is dominated by rushing and anxiety about\nmeeting performance metrics.\nIntegrating  heartfelt  reflection  in  that  workshop  did\nnot change the course of the company. Advising tech\ncompanies on well-being has shown me over and over\nthat, when the rubber hits the road, meaningful change\nrequires making tradeoffs that value responsibility and\ncare over core metrics of engagement, speed, and profit.\nUsually,  these  tradeoffs  do  not  happen  and  the  work\nbecomes a band aid or is not sustained. However, if more\npauses and guided reflection were built into the overall\ncompany  culture  and  practice,  people  might  be\nmore\n likely\n to\n make\n those\n tradeoffs.\n These\nmicro-interventions are a small piece of what is needed\nin  the tech  sector,  but they  help  create conditions  for\nmore  ethical  and  brave  action  in  the  moment  and\ncontribute to culture change over time.\nAdopting  practices  like  is  difficult  on  one’s  own,\nregardless of the context. Community has always been\nkey to the spiritual path. This is true of lay people who\nare part of religious congregations as well as of monks\nand  nuns  who  support  each  other  in  lifelong\ncommitments spiritual practice.\nMobius is also experimenting with how to meet this\nneed  in  the  tech  sector  by  building  an  intimate\ncommunity  of  mission-aligned  tech  leaders  across\ncompanies. This is another method to shift the “interior\ncondition of the intervener”, counter the ways in which\npower and stress can blind well-intentioned people, and\nsupport people to move from good intentions to wise\naction. We host gatherings for senior leaders from across\nthe major tech companies who share the mission to put\nour shared humanity at the center of their products and\nservices. These gatherings are often hosted in someone’s\nhome and integrate spiritual practice in order to foster\ndeeper connections to ourselves, each other, and a shared\nsense of purpose. The people who are part of the Mobius\ncommunity work for competitors, so there are limits to\nwhat they can and will share with each other: they can\nrarely talk about specific product features. But there is\nan  increasing  desire  to  discuss  common  challenges,\ndevelop shared standards and principles, and envision\nnew forms of industry-level responses.\nWe are certainly not the only community-builders in\nthe  ethical  tech  movement.  The  Trust  and  Safety\nProfessional  Association  is  a  new  entity  to  foster\ncommunity  and  cross-company  learning  for  those  in\nTrust  and  Safety  roles  across  the  tech  sector.\nNew_Public is a community of people from a range of\ndisciplines working to create healthier online spaces, and\nthe list goes on.\nThe Mobius cross-company is particularly inspired by\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n319    \n \n\nthe  Buddhist  concept  of  the  sangha,  a  community  of\nBuddhists who gather consistently to practice together.\nSanghas emphasize that members of the community are\nall walking a spiritual path together, even when not in the\nsame physical space. This can create powerful levels of\npsychological  safety  to  see  the  implications  of  one’s\nactions and what it will take to change these actions.\nAs  the  Vietnamese  Buddhist  monk,  activist  and\nteacher Thich Nhat Hanh wrote:\n“The sangha is not a place to hide in order to avoid\nyour responsibilities. The sangha is a place to practice\nfor the transformation and the healing of self and society.\nWhen you are strong, you can be there in order to help\nsociety. If your society is in trouble, if your family is\nbroken, if your church is no longer capable of providing\nyou with spiritual life, then you work to take refuge in the\nsangha  so  that  you  can  restore  your  strength,  your\nunderstanding, your compassion, your confidence. And\nthen in turn you can use that strength, understanding and\ncompassion to rebuild your family and society, to renew\nyour church, to restore communication and harmony.\nThis  can  only  be  done  as  a  community—not  as  an\nindividual, but as a sangha.”[41]\nBuilding a community among leaders is a radical act\nin  the  context  of  a  sector  that  is  usually  allergic  to\ncollaboration.  There  are  rare  exceptions,  such  as  the\nGlobal  Network  Initiative,  a  cross-tech  industry\ncoalition  that  was  created  to  prevent  human  rights\nviolations  in  response  to  the  Chinese  government\nfinding and torturing political dissidents using data that\nit  accessed  from  Yahoo.  But  as  Thich  Nhat  Hahn\nexplains,  community  has  the  power  to  bolster  greater\nmoral  courage  and  provide  the  fortitude  to  do  the\ndifficult work of social transformation.\nThat  fortitude  is  sorely  needed  in  this  case.  While\nbuilding community takes patience and requires trust,\nmany  of  these  leaders  are  lonely,  overwhelmed,\nswimming upstream, and deeply hungry for like-minded\nindividuals who share a commitment to responsibility\nand  well-being.  They  are  fighting  against  the  strong\nforces of our economic system and how that translates\ninto the incentives, structures, and cultures within which\nthey  are  trying  to  create  change.  Locking  arms  in\ncommunity can help provide the strength to see more\nclearly and act more radically in service of the larger\nwhole.\n3.2    Bringing the spiritual caretaker to the table\nIn  his  manual  of  Pastoral  Care,  Gregory  the  Great\nimplores those in power to maintain a “humility of office”\nthat  allows  them  to  identify  clouded  perspectives,\nsubconscious motivations, and blind spots.[2] In the tech\nsector, this humility needs to extend to a recognition that\ncaring  for  the  soul  warrants  expertise  that  rarely  is\npresent  in  tech  companies.  Whether  in  the  form  of  a\nminister,  Rabbi,  Buddhist  meditation  teacher,  or\npsychologist,  these  are  experts  on  timeless  questions\nabout how to be healthy and whole human beings and\ncommunities.\nThroughout much of human history, these roles have\nbeen  accompanied  by  many  different  forms  of\npreparation  that  include  the  cultivation  of  wisdom\nthrough deep spiritual practices that have been passed\ndown  for  thousands  of  years.  As  such,  it  would  be\nunrealistic and even dangerous to assume that everyone\nwho  touches  product  decisions  could  have  the\nknowledge,  skills,  wisdom,  and  methods  required  to\nresponsibly  care  for  our  souls—or  that  these  people\ncould acquire such expertise through a few meditation\nor spiritual retreats. We do not expect everyone to have\nthe  legal  knowledge  of  a  lawyer,  but  no  major  tech\ncompany  would  imagine  shipping  a  product  without\nconsulting one. The same should apply to spiritual care\nwhen  humans  and  technology  are  so  intimately\nintertwined. It should not be acceptable to decide how\nSiri or Alexa talks a teenager out of a suicidal attempt\nwithout involving experts on nuanced and responsible\nspiritual care.\nTech companies are increasingly hiring the equivalent\nof chief ethics officers who, given the nature of the crises\nat  hand,  are  scrambling  to  define  their  role,  put  out\nconstant fires, and develop long-term ethical processes\nand  principles[42].  Companies  also  bring  in  outside\nexperts,  mostly  academics,  to  build  their  knowledge\nbase  about  well-being.  But  these  experts  are  often\nconsulted  in  superficial  and  one-off  ways  rather  than\nbeing  deeply  integrated  into  the  design  and  strategy\nprocess. While these new ethics roles are important steps,\nthey do not create the conditions for true pastoral care for\nthe users of technology.\nFor  example,  Alexa  is  increasingly  the  only\ncompanion for many older people in a given day. Mobius\nconvened a group of caretakers, meditation teachers, and\nneuroscientists  to  advise  a  team  at  Amazon  that  is\nexploring how Alexa might help alleviate loneliness and\nsocial  isolation  among  the  elderly.  Alexa  is  suddenly\n“caring” for millions of older people around the world.\n    320\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\nAlexa’s engineers could either treat this as an interesting\nfact that is good for their business but does not influence\nhow  they  define  the  success  of  their  product,  or  they\ncould accept the caretaker role with the responsibility it\ndeserves.  Thankfully,  this  particular  Alexa  team  is\ntaking  their  responsibility  seriously.  The  experts  we\nassembled  worked  with  the  Amazon  technologists  to\nimagine  a  world  in  which  Alexa  connects  people  via\nvideo to others who share their interests, collect stories\nand memories for their families by “interviewing” them\nover  time  (with  consent),  and  helps  people  live  in\naccordance with their values and goals for this stage of\ntheir lives. This workshop was early in the Alexa team’s\nvisioning process, so whether the ideas make their way\ninto the product is yet to be seen. Regardless, this kind\nof intervention is unlikely to create sustainable change\nuntil  expertise  like  this  is  present  in  the  tech  teams\nthemselves  or  otherwise  integrally  woven  into  the\ndecision-making process.\nIn  the  Alexa  case,  it  is  worth  noting  that  being\nthoughtful about addressing loneliness most likely helps\nAmazon’s  bottom  line.  The  real  test  is  whether\ncompanies will make the necessary tradeoffs to value\nwell-being  over  the  fastest  route  to  a  profit.\nMeaningfully  integrating  caretaking  expertise  into\nproduct teams does not address that root cause, and it is\nimportant  to  be  realistic  about  what  that  kind  of\nintervention can and cannot accomplish without shifting\nwhat is incentivized and valued in the company.\nThe integration of such care could take a variety of\nforms, at the product and strategy levels. There could be\nresident chaplains who are part of product teams, cohorts\nof graduates from divinity schools and seminaries who\nare trained in tech and ethics and embedded in tech teams,\nengineers who attend tailored programs on spiritual care,\nor ethical councils that include faith leaders in addition\nto ethicists, lawyers, and tech policy experts. There are\nmany strategies to explore, none of which should be one-\noff or treated as a silver bullet. They should be built into\nevery part of the design, build, and launch process. It is\nonly  at  the  intersection  of  a  wide  range  of  wisdom,\nknowledge, skills, and life experiences that we can begin\nto create technology that is truly worthy of the human\nspirit.\n4    Conclusion\nThe past few years were key to pointing out and naming\nthe negative impacts of technology. We know there is an\nillness and the symptoms are undeniable. But now it is\ntime to focus on a cure without succumbing to denial,\nband-aids, or purely putting out the latest fire. We need\nchange  at  a  greater  depth  and  scale  than  any  of  the\ninterventions discussed in this piece can create on their\nown. There is now a vibrant and growing ecosystem of\nindividuals  and  organizations  who  are addressing  this\nchallenge from a myriad of angles. People are shifting\nbusiness  models,  pushing  for  anti-trust  regulation,\nincreasing the diversity of the tech workforce, creating\nnew ethical design principles and performance metrics,\nand organizing employee movements and walkouts. We\nneed all of these efforts working in concert.\nBut  if  we  fail  to  see  solutions  to  tech’s  impact  on\nhumanity within the broader frame of care for souls, we\nwill  continue  to  create  quick  fixes  and  small\ninterventions  that  are  misaligned  with  the  fact  that\ntechnology  is  influencing  nearly  every  aspect  of  the\nhuman  experience.  Thankfully,  we  are  surrounded  by\nwisdom that has a great deal to teach us about how to\nbring  technology  and  humanity  into  alignment.  We\nknow what practices shift us from greed to compassion.\nWe  know  how  to  create  space  for  awareness  and\nacceptance.  We  know  how  to  provide  pastoral  care\nthrough the greatest joys and sorrows of life. Translating\nthis into the digital world is not simple, but it is necessary.\nReferences\n L.\n Stevens,\n Alexa,\n can\n you\n prevent\n suicide?\nhttps://www.wsj.com/articles/alexa-can-you-prevent-\nsuicide-1508762311, 2017.\n[1]\n St.  Gregory  the  Great, Pastoral  Care.  New  York,  NY,\nUSA: Newman, 1978.\n[2]\n L. Burton and L. VandeCreek, Professional chaplaincy: Its\nrole  and  importance  in  healthcare, Journal  of  Pastoral\nCare, vol. 55, no. 1, pp. 81–97, 2001.\n[3]\n J.  Vincent,  Former  Facebook  exec  says  social  media  is\nripping  apart  society, https://www.theverge.com/2017/\n12/11/16761016/former-facebook-exec-ripping-apart-\nsociety, 2017.\n[4]\n S. Frenkel, N. Confessore, C. Kang, M. Rosenberg, and J.\nNicas, Delay, deny and deflect: How Facebook’s leaders\nfought  through  crisis, https://www.nytimes.com/2018/\n11/14/technology/facebook-data-russia-election-\nracism.html, 2018.\n[5]\n S. Levin, YouTube alters search algorithm over fake Las\nVegas\n conspiracy\n videos, \nhttps://www.theguardian.\ncom/us-news/2017/oct/06/youtube-alters-search-algorithm-\nover-fake-las-vegas-conspiracy-videos, 2017.\n[6]\n C.  O.  Scharmer,  The  blind  spot  of  leadership,\nhttps://zampellagroup.com/wp-content/uploads/2014/08/\n2003_TheBlindSpot.pdf, 2003.\n[7]\n M.\n Wheatley,\n It’s\n an\n interconnected\n world,\nhttps://margaretwheatley.com/wp-content/uploads/2014/12/\nIts-An-Interconnected-World.pdf, 2002\n[8]\n C.  Trungpa, The  Myth  of  Freedom  and  the  Way  of\n[9]\n  Aden Van Noppen:   Creating Technology Worthy of the Human Spirit\n321    \n \n\nMeditation. Boston, MA, USA: Shambhala, 2002.\n K.\n Schwab,\n The\n fourth\n industrial\n revolution,\nhttps://www.weforum.org/about/the-fourth-industrial-\nrevolution-by-klaus-schwab, 2020.\n[10]\n P.  Mozur,  A  genocide  incited  on  facebook,  with  posts\nfrom Myanmar’s military, https://www.nytimes.com/2018/\n10/15/technology/myanmar-facebook-genocide.html,\n2018.\n[11]\n Media,  The  Nielsen  total  audience  report:  Q1  2018,\nhttps://www.nielsen.com/us/en/insights/report/2018/q1-\n2018-total-audience-report/, 2018.\n[12]\n R.  Raphael,  Netflix  CEO  Reed  Hastings:  Sleep  is\nour competition, https://www.fastcompany.com/40491939/\nnetflix-ceo-reed-hastings-sleep-is-our-competition, 2017.\n[13]\n S. Turkle, Reclaiming Conversation: The Power of Talk in\na Digital Age. New York, NY, USA: Penguin Press, 2015.\n[14]\n T.  Lorenz,  Teens  explain  the  World  of  Snapchat’s\naddictive  streaks,  where  friendships  live  or  die,\nhttp://www.businessinsider.com/teens-explain-snapchat-\nstreaks-why-theyre-so-addictive-and-important-to-\nfriendships-2017-4, 2017.\n[15]\n S. Aslam, Snapchat by the numbers: Stats, demographics\n&  fun  facts, https://www.omnicoreagency.com/snapchat-\nstatistics/, 2021.\n[16]\n A.  Raskin,  No  more  more  pages? https://web.archive.\norg/web/20120606053221/http:/humanized.com/weblog/\n2006/04/25/no_more_more_pages/, 2006.\n[17]\n H.  Andersson,  Social  media  apps  are ‘Deliberately’\naddictive to users, https://www.bbc.com/news/technology-\n44640959, 2018.\n[18]\n S.  Murphy,  Facebook  changes  its ‘move  fast  and  break\nthings’ motto, https://mashable.com/2014/04/30/facebooks-\nnew-mantra-move-fast-with-stability/, 2014.\n[19]\n E. Pariser, The Filter Bubble: How the New Personalized\nWeb is Changing What We Read and How We Think. New\nYork, NY, USA: Penguin Books, 2012.\n[20]\n J.  Kornfield, The  Wise  Heart.  New  York,  NY,  USA:\nRandom House, 2009.\n[21]\n T.  Shlain,  Tech’s  best  feature:  The  off  Switch, https://\nhbr.org/2013/03/techs-best-feature-the-off-swi, 2013.\n[22]\n M.  L.  Gray, Out  in  the  Country.  New  York,  NY,  USA:\nNYU Press, 2009.\n[23]\n S.  Sandberg,  Making  it  easier  to  honor  a  loved  one  on\nFacebook  after  they  pass  away, https://about.fb.com/\nnews/2019/04/updates-to-memorialization/, 2019.\n[24]\n M.  Anderson,  M.  Barthel,  A.  Perrin,  and  E.  Vogels,\n#BlackLivesMatter surges on Twitter after George Floyd’s\ndeath, https://www.pewresearch.org/fact-tank/2020/06/10/\nblacklivesmatter-surges-on-twitter-after-george-floyds-\ndeath, 2020.\n[25]\n M.  Anderson,  S.  Toor,  L.  Rainie,  and  A.  Smith,  An\nanalysis of #BlackLivesMatter and other Twitter hashtags\nrelated to political or social issues, https://www.pewinternet.\norg/2018/07/11/an-analysis-of-blacklivesmatter-and-other-\ntwitter-hashtags-related-to-political-or-social-issues/,\n2018.\n[26]\n J.  Dorsey,  We’ve  made  the  decision  to  stop  all  political\nadvertising  on  Twitter  Globally.  We  believe  political\nmessage reach should be earned, not bought. Why? A few\nreasons, \nhttps://twitter.com/jack/status/118963436047\n2829952?ref_src=twsrc.tfw|twcamp.tweetembed&ref_url=\n[27]\nhttps://www.cnbc.com/2019/10/30/twitter-bans-political-\nads-after-facebook-refused-to-do-so.html, 2019.\n K. Stankiewicz, Twitter executive on political ad ban: ‘We\nwant  to  make  sure  we  don’t  create  filter  bubbles’,\nhttps://www.cnbc.com/2020/01/07/twitters-matt-derella-\non-political-ad-ban-and-filter-bubbles.html, 2020.\n[28]\n T.  Benson,  New  business  fad:  Tripping  on  ayahuasca,\nhttps://www.thedailybeast.com/new-business-fad-tripping-\non-ayahuasca, 2017.\n[29]\n V.  Giang,  Inside  Google’s  insanely  popular  emotional-\nintelligence course, https://www.fastcompany.com/3044157/\ninside-googles-insanely-popular-emotional-intelligence-\ncourse, 2015.\n[30]\n A. Groth, Silicon Valley is hacking spiritual practices to\nboost  productivity, https://qz.com/728887/silicon-valley-\nis-hacking-spiritual-practices-to-boost-productivity, 2016.\n[31]\n T.  Fossella,  Human  nature,  buddha  nature:  An  interview\nwith John Welwood, https://tricycle.org/magazine/human-\nnature-buddha-nature/, 2011.\n[32]\n U.  S.  Army  Chaplain  Corps, https://armyhistory.org/u-s-\narmy-chaplain-corps/, 2015.\n[33]\n L. Shane III and P. Kime, New VA study finds 20 veterans\ncommit suicide each day, https://www.militarytimes.com/\nveterans/2016/07/07/new-va-study-finds-20-veterans-\ncommit-suicide-each-day/, 2016.\n[34]\n A.  W.  Schaef, When  Society  Becomes  an  Addict.  San\nFrancisco, CA, USA: Harper & Row, 1987.\n[35]\n D.  Evon,  Did  Bill  Gates,  Steve  Jobs,  and  other  tech\nbillionaire parents advocate limiting children’s technology\nuse, \nhttp://www.snopes.com/fact-check/tech-billionaire-\nparents-limit, 2018.\n[36]\n A.  Thomson,  Help  kids  to  kick  social  media  addiction,\nhttps://www.thetimes.co.uk/article/help-kids-to-kick-\nsocial-media-addiction-x7xjqh9rf, 2018.\n[37]\n P. Chödrön, When Things Fall Apart: Heartfelt Advice for\nHard Times. Shaftesbury, UK: Element, 2005.\n[38]\n P. J. Palmer, Life on the Mobius strip, https://onbeing.org/\nblog/life-on-the-mobius-strip/, 2016.\n[39]\n D.  Pereda, However  Long  the  Night.  Santa  Rosa,  CA,\nUSA: Eternal Press, 2012.\n[40]\n T. N. Hanh, What is Sangha? https://www.lionsroar.com/\nthe-practice-of-sangha/, 2019.\n[41]\n D.  Boyd,  J.  Metcalf,  and  E.  Moss,  Owning  ethics:\nCorporate logics, Silicon Valley, and the institutionalization\nof ethics, https://datasociety.net/wp-content/uploads/2019/\n09/Owning-Ethics-PDF-version-2.pdf, 2019.\n[42]\nAden Van Noppen received the BA degree\nfrom Brown University in 2009 and was a\nresident fellow at Harvard Divinity School\nin the 2017−2018 academic year. She was\na senior advisor to the United States Chief\nTechnology  Officer  during  the  Obama\nAdministration\n and\n was\n a\n founding\norganizer  of  the  Sanctuaries,  the  first\ninterfaith arts organization in the United States. She is currently\nthe  founder  and  executive  director  of  Mobius,  a  non-profit\nfocused on creating a more responsible, compassionate, and just\ntech sector. Her works have been featured in the New Yorker, the\nNew York Times, Wired, and elsewhere.\n    322\nJournal of Social Computing, December 2021, 2(4): 309−322    \n \n\n \nReal Estate Politik: Democracy and the Financialization\nof Social Networks\nJoanne Cheung*\nAbstract:    The power of social network platforms to amplify the scale, speed, and significance of everyday\ncommunication is increasingly weaponized against democracy. Analyses of social networks predominantly\nfocus on design and its effects on politics. This article shifts the debate to their business model. Built as platform\nbusinesses, social networks are privately owned public spaces with structurally limited democratic affordances.\nDrawing from the history, theory, and practice of land use, I develop an analogy between the financialization\nof land by commercial real estate development and the financialization of attention by platform businesses.\nHistorical policies, such as incentive zoning and exclusionary zoning, shed light on how platform businesses\nuse systems of measurement and valuation to conflate users’ roles, tokenize the incentives that drive behavior,\nand defer the ethical responsibilities businesses have to the public. While the real estate framing reveals social\nnetworks’ structural  flaws  and  colonial  roots,  lessons  from  urban  planning,  community  land  trusts,  and\nIndigenous land stewardship can inform their regulation and reform. Building on the broader effort to embed\nethics in the development of technology, I describe possibilities to steward social networks in the public interest.\nKey  words:   social networks; social media; platform studies; financialization; urban planning; land use\n1    Introduction\nWhile  in  the  past  there  may  have  been  difficulty  in\nidentifying the most important places (in a spatial sense)\nfor the exchange of views, today the answer is clear. It\nis  cyberspace—the “vast  democratic  forums  of  the\nInternet” in general, and social media in particular.\n– Anthony Kennedy, Former Associate Justice of the\nSupreme Court of the United States[1]\nThe  size  of  our  user  base  and  our  users’ level  of\nengagement are critical to our success…We generate\nsubstantially all of our revenue from selling advertising\nplacements to marketers.\n– Annual Report Pursuant to Section 13 or 15(d) of the\nSecurities  Exchange  Act  of  1934  for  the  fiscal  year\nended December 31, 2020 filed by Facebook, Inc.[2]\nCities have arisen through geographical and social\nconcentration  of  a  surplus  product.  Urbanization  has\nalways been a class phenomenon, since surpluses are\nextracted  from  somewhere  and  someone,  while  the\ncontrol over their disbursement typically lies in a few\nhands.\n– David Harvey, “The Right to the City”[3]\nIn the Republic, Plato claimed the democratic affordance\nof the ideal city was measured by the distance of a herald’s\ncry. In the “virtual city” of social network platforms, the\nspeed and distance at which a user’s voice travels are\ndecoupled from physical constraints. Like providing a\nspeaker  with  a  hyper-visible  soapbox  and  a hyper-\namplified  megaphone,  social  network  platforms  boost\nthe  political  power  of  an  individual’s  everyday\nconversations. While these platforms have the potential\nto expand democracy, their power instead has a growing\ndark side. From misinformation and polarization to hate\nspeech and the incitation of violence, the power of social\nnetworks is increasingly weaponized against democracy.\nThe  history  of  technology  for  communication  is\n \n • Joanne Cheung is with  the Haas School of Business, University\nof\n California,\n Berkeley,\n CA\n 94720,\n USA.\n E-mail:\nj@joannekcheung.com.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-19;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   03/06  pp323−336\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0030\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\ndeeply  intertwined  with  the  political  history  of  the\nempire[4–8].\n With\n Meta\n Platforms\n (the\n parent\norganization  of  Facebook)  passing  $1  trillion  market\ncapitalization, social network platforms are effectively\na contemporary form of empire. By linking individuals’\ncommunicative  power  with  their  spending  power,  the\nplatform  simultaneously  extracts  market  information\nfrom  individuals  on  the  network  and  expands  the\nfinancial  market  for  the  network  itself.  The  historical\norigins of this model have been characterized by cultural\ntheorist  Nicholas  Mirzoeff  as  the “colonial  complex”\n(local  surveillance  of  individuals)  and  the “imperial\ncomplex” (the control of “primitive” remote populations\nby a “cultured” centralized authority)[9]. As a platform\ndesigned,  developed,  and  headquartered  in  Silicon\nValley and deployed globally to its 2.91 billion monthly\nactive  users[10],  Facebook  resembles  both  complexes:\nthe extent of its reach is planetary and the specificity of\nits surveillance is intimate.\nIf communication at a distance enabled the creation of\nempire, then the distance between the site of extraction\n(the colony) and the site of authority (the administrative\ncenter)  is  the  basis  of  its  power.  The  various\nnomenclatures used to address social network platforms\nconceal  this  distance.  The  term “social  network\nplatforms” itself  collapses  the  linguistic  distance\nbetween the site of extraction (the social network) and\nthe site of authority (the platform), thereby subsuming\nthe  platform’s  business-facing  dimension  within\ndiscussions  of  public-facing  social  issues.  The  term\n“social  media” similarly  overly  focuses  on  the\nconsequences of user actions such as content moderation\nand\n information\n integrity[11–14] \nand\n behavioral\nimplications of interface designs such as dark patterns,\npersuasive  design,  and  technological  seduction[15–17].\nWhile these analyses are critical for understanding the\nsymptoms  and  gravity  of  the  problem,  they  are\ninsufficient for exposing the mechanisms underlying the\nplatform that are critical for their regulation and reform.\nIn order to expose the hidden mechanisms of platform\npower  and  their  effects  on  democracy,  I  will  first\ndecouple “social network” from “platform” in analysis.\n“Platform” is  a  multi-sided  marketplace  business  that\ndevelops  and  owns  the  technology  infrastructure  that\ncreates  social  networks.  Situating  social  networks  in\ntheir current form within Lawrence Lessig’s framework\nfor regulation[18], I believe the market force bears direct\nresponsibility  for  the  systemic  problems.  Given  the\noutsized influence of the financial dimension, I direct the\ncritique  from  the  platform’s  design  and  technical\nmechanisms to its financialization.\nThis article proceeds in three sections. The first section\nlinks  democratic  practice  in  public  space  to  social\nnetworks,  frames  platform  businesses  as  commercial\nreal estate development, and explores their democratic\naffordances  as “privately  owned  public  space”.  The\nsecond  section  contextualizes  the  financialization  of\nattention  in  terms  of  the  financialization  of  land,\nhistoricizes how the platform business model encodes\ncolonial assumptions into its management systems for\nmeasurement\n and\n valuation,\n and\n unpacks\n its\nmechanisms and systemic effects on democracy. Finally,\nthe third section adapts lessons from urban planning and\nland justice practices to the context of social networks\nand proposes new possibilities for roles, incentives, and\nresponsibilities to steward social networks in the public\ninterest.\n2    Democracy  in  Privately  Owned  Public\nSpace\nThe health of democracy is sustained by communication\nin  everyday  life[19–21].  In Democracy  and  Education,\nphilosopher  John  Dewey[22] described  democracy  as\n“more than a form of government; it is primarily a mode\nof  associated  living,  of  conjoint  communicated\nexperience”. While American voters cast their ballot in\nthe presidential elections every four years, the decision\nrecorded  in  that  instance  forms  over  time.  Through\neveryday interactions with their community, individuals\ndeliberate political opinions that shape their democratic\ndecision-making. Spaces beyond the voting booth—the\ncommunity  center,  the  neighborhood  park,  and  the\nlibrary—bear  a  democratic  purpose:  to  enable  a\nheterogeneous  population  to  recognize  and  celebrate\ntheir  differences[23–25].  They  set  the  conditions  for\ndemocratic life.\nSocial  networks  expand  the  space  for  the “conjoint\ncommunicated experience” defined by Dewey. Between\n2010  and  2021,  the  percentage  of  Americans  using\nplatforms to regularly communicate increased from 47%\nto  72%[26].  Alongside  this  growth,  a  host  of  design,\nregulatory,  and  ethical  challenges  arise  when  the\ndemocratic affordance of physical space extends into the\nvirtual realm. From Facebook groups acting as virtual\n    324\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\nassemblies to hashtag activism[27–29], social networks’\nunique  communication  features[30, 31] and  digital\narchitecture[32] introduce new dynamics and risks. While\nparticipation on social networks transformed the ways\npeople engage politically, how they should be governed\nas political forums remains under debate. In 2017, the\nSupreme Court case Packingham v. North Carolina (582\nUS __) held that a North Carolina statute prohibiting sex\noffenders  from  accessing  social  network  platforms\nviolated  the  First  Amendment  to  the  United  States\nConstitution.  In  the  holdings  of  the  case,  Justice\nKennedy  described  social  networks  as  the “modern\npublic square”, drawing an analogy between access to\nonline  communication  and  access  to  public  space.  In\n2021, Knight Institute v. Trump ruled that by blocking\nseveral  users,  President  Trump  had  violated  the  First\nAmendment  because  comment  threads  on  Twitter\nconstituted a public forum; the case was later rendered\nmoot in the ruling for Biden v. Knight First Amendment\nInstitute  at  Columbia  University (593  U.S.  __)  after\nTrump’s presidency ended and his own Twitter access\nwas  terminated.  In  his  opinion,  Justice  Thomas\nexpressed  the  urgent  need  to  regulate  platforms  that\nbuild social networks: “We will soon have no choice but\nto  address  how  our  legal  doctrines  apply  to  highly\nconcentrated,\n privately\n owned\n information\ninfrastructure such as digital platforms.”\nPlatforms  are  hybrid  entities:  privately  owned\nbusinesses that offer public service. Borrowing a term\nfrom urban planning[33], a platform is a privately owned\npublic space. Although users may experience them as a\n“modern  public  square”,  their  underlying  economic\nincentive and legal constructs are much closer to that of\ncommercial  real  estate  development.  The  goals  they\nserve are inherently dissonant: a public square exists to\nserve the public interest, while commercial real estate\nexists to generate return on capital. Platforms’ private\nownership  structurally  constrains  their  democratic\naffordance.  When  public  and  private  interests  clash,\nplatforms’ allegiances  will  favor  the  private  over  the\npublic.\nThis  dynamic  played  out  during  the  Occupy  Wall\nStreet protests in Zuccotti Park, a privately owned public\nspace in the Financial District of Manhattan, New York\nCity. Zuccotti Park was constructed in 1972 alongside\nOne  Liberty  Plaza,  a  2.3-million-square-foot  office\ntower currently valued at $1.55 billion. Both the tower\nand the park are owned by Brookfield Office Properties\n(with  the  park  named  after  company  chairman  John\nZuccotti). Brookfield Office Properties is a subsidiary of\nthe  commercial  real  estate  company,  Brookfield\nProperty Partners, which itself is a subsidiary of one of\nthe  world’s  largest  alternative  asset  management\ncompanies  with  over  $625  billion  of  assets  under\nmanagement.  Zuccotti  Park  is  one  of  more  than  500\nprivately owned public spaces in New York City created\nthrough a “Floor Area Bonus for a Plaza” regulation in\nthe  1961  New  York  City  Zoning  Resolution.  This\nregulation—commonly\n known\n as\n Incentive\nZoning—incentivized  the  creation  of  open  spaces  in\nurban  areas  with  high  real  estate  value  by  permitting\ndevelopers an additional ten square feet of built space in\nexchange for one square foot of “an open area accessible\nto the public at all times”[34]. “The equivalent of thirty\naverage New York City blocks” was created as a result,\n“at  no  direct  cost  to  the  public”[35].  Privately  owned\npublic space was meant to be a win-win for both the\npublic and the private sectors.\nThe Occupy Wall Street protest revealed the limits of\nthe  democratic  affordances  of  the  privately  owned\npublic  space.  Occupy  Wall  Street  protesters  turned\nZuccotti  Park  into  a  makeshift  village  with  tents  and\nshared communal resources[36] and exercised consensus\ndecision  making.  As  the  park’s  owners,  Brookfield\nOffice  Properties  maintained  the  power  to  amend  the\npark’s  code  of  conduct—and  they  did.  The  new\namendments  banned “tents,  sleeping  bags,  and  lying\ndown”[37],  which  were  then  used  as  grounds  to  evict\nOccupy  protesters  from  the  park.  Zuccotti  Park’s\ndemocratic  affordance  was  weakened  by  its  private\nownership,  and  a  movement  centered  on  financial\ninequality  was  ultimately  evicted  by  the  center  of\nfinancial power.\nPrivately owned public spaces like Zuccotti Park exist\nnot because of a direct investment in democratic public\nspaces.  Rather,  they  are  byproducts  of  high-profit\nskyscrapers developed during real estate market booms\nwhen the speculative value of the building soars above\nthe material value of the land[38, 39]. The incentive behind\nthe creation of the park therefore lies precisely in profit\nmaximization.  Born  from  the  dissonance  of  extreme\npublic  and  private  interest,  Zuccotti  Park  and  One\nLiberty Plaza are inextricably linked; the park could not\nhave  existed  without  the  tower.  This  dependency\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n325    \n \n\nfundamentally\n weakens\n the\n park’s\n democratic\naffordance.  Public  good  is  subsumed  by  the  logic  of\nfinancial capital and public interest is lodged within the\nmost extreme expression of private interests.\nSocial  networks  and  platform  businesses  have  an\ninterdependent relationship similar to Zuccotti Park and\nOne  Liberty  Plaza.  Social  networks  would  not  exist\nwithout platform businesses and their social benefits are\nintertwined with their profit-seeking purpose.\nBusinesses  like  Facebook  operate  under  a  platform\nbusiness  model  that  relies  on  the  public  to  serve  its\nprivate interests. They are multi-sided marketplaces with\n“many faces”[40]. Rather than creating value on a linear\nsupply chain, a platform business generates revenue by\nconnecting the multiple groups and brokering exchanges\nbetween them[41, 42]. Different from a traditional market,\nwhere the transaction occurs directly between the buyer\nand  the  supplier,  exchanges  between  buyers  of  ads\n(advertisers)  and  suppliers  (users)  on  a  platform  are\nindirect. Users not only supply attention for the ads, they\nalso  supply  data  about  how  they  use  their  attention,\nwhich  helps  continuously  improve  the  accuracy  and\nvalue of Facebook ads. No real goods is exchanged on\na  multi-sided  advertising  market.  Instead,  advertisers\nwho buy Facebook ads are buying the possibility to turn\nFacebook\n users\n into\n their\n future\n customers.\nConsumption begets more consumption.\nFacebook  principally  mediates  three  layers  of\nexchanges: (1) between users and their social groups,\n(2)  between  individuals  and  the  platform,  and  (3)\nbetween  the  platform  and  its  clients,  third-party\nadvertisers. The terms of exchange across these layers\nare  not  equal.  In  the  user-facing  layer,  the  exchange\ncenters  on  everyday  communication.  On  the  client-\nfacing  side,  the  exchange  centers  on  conversion:  the\nmoment the audience of the advertisement performs an\naction desired by the advertiser, such as discovering and\npurchasing  a  product.  The  platforms’ objective  is  to\nreduce  conversion  time  and  increase  the  number  of\nconverted\n people.\n The\n second\n layer\n of\nexchange—between users and the platform—is the most\nopaque  and  hidden  in  black-box  algorithms[43].  By\npositioning itself as a free service whose purpose is to\nenable users to connect and build community, Facebook\nturns  non-financial  exchanges  (between  users)  into\nfinancial ones. In other words, the platform financializes\neveryday  communication  into  sellable  data,  social\nrelations  into  marketing  channels,  and  users  into\nconsumers,\n while\n obscuring\n the\n terms\n of\n its\nfinancialization.\nTo  disentangle  the  conflicting  private  and  public\ninterests  in  platforms,  we  must  understand  how\nfinancialization works. In the next section, I build on the\nanalogy between platforms and real estate development\nand  use  the  financialization  of  land  to  illuminate\nproblematic  mechanisms  and  their  effects  in  the\nfinancialization of attention.\n3    Financialization of Land and Attention\nWith  its  multifaceted  dynamics,  financialization  has\nbeen  characterized  as  cognitive  capitalism[44],  supply\nchain  capitalism[45],  racial  capitalism[46],  platform\ncapitalism[47],\n surveillance\n capitalism[48],\n rentier\ncapitalism[49, 50],  technoscientific  capitalism[51],  and\nterror capitalism[52], as well as part of the growing field\nof “platform studies”[53, 54]. Due to its extractive effects,\nfinancialization of human cognitive capacities has been\ntermed “immaterial labor”[55], “attention brokerage”[56],\nand\n the \n“subprime\n attention\n crisis”[57].\n While\nfinancialization  has  extensively  reconfigured  the\nlanguage,  culture,  and  patterns  of  contemporary\nlife[58–60], its mechanisms are not entirely new. Rather,\nthey  bear  striking  resemblance  to  colonial  patterns  of\ndispossession[61–67].\nBoth  commercial  real  estate  development  and\nplatform  businesses  financialize  finite  resources: land\nand attention. They do so through systems that measure\nand assign value, and in the process, reconstruct colonial\nmyths in everyday life.\nMeasurement  systems  serve  the  purpose  of  the\nauthority  who  institutes  them.  Scholars  across\nphilosophy,  geography,  anthropology,  and  more  have\ntermed this process “classification”[9], “the nomination\nof the visible”[68], “commensuration”[69], and “modular\nsimplification”[70].  Measurement  systems  not  only\nrepresent but also construct what they measure[71, 72]. As\nsociologist Donald MacKenzie writes, they are “not a\ncamera, but an engine”. The distinguishing feature of\nboth  land  and  attention  measurement  lies  in  how\nextensively  the  process  subsumes  otherwise  non-\nfinancial entities[73–76] within its financial logic and as a\nconsequence reduces the relevance of local contexts[77]\nin service of increasing the efficiency of the financial\nexchange.\n    326\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\nHistorical processes of land measurement encoded a\ncolonial  myth:  before  being  made  legible  to  the\ncentralized  authority,  settlers  of  the  American  West\ndeclared that land existed in a “pristine” state, untouched\nby “humans”.  By  discounting  the  humanity  and\nstewardship  of  indigenous  populations,  the “pristine”\nmyth  helped  justify  the  dispossession  of  Indigenous\npopulations[78–83] and  normalize  the  exploitation  of\n“virgin  land”[84] as  monocultural  fields  optimized  for\ncommodity  crops[85–88].  This  myth  sheds  light  on  the\nextractive  assumption  that  platforms  make  about\nparticipants on social networks: that people’s attention,\nlike land, is “unprocessed data”[89] ready to be converted\ninto and exchanged as financial assets.\nOnce an entity has been converted into a financial asset,\nit is then assigned value by a centralized authority with\nthe  purpose  of  accruing  it.  The  asset  becomes  sorted\nbased on its perceived productivity—that is, its ability to\ngenerate profit. For example, the practice of scientific\nforestry optimized forests for lumber output—an asset\nthat  could  generate  profit—while  excluding  all  other\nvegetation,\n which\n resulted\n in\n the\n systematic\ndeterioration of soil health and ecosystem collapse[90].\nBeing embedded within the definition of productivity is\na value judgement: the idea that certain entities are more\nvaluable than others is an assumption and not a fact of\nnature, and the assumption is often made by a centralized\nauthority  about  a  site  of  extraction.  The  myth  of\nproductivity helps maintain the power of the centralized\nauthority by intentionally obfuscating the subjectivity of\nits  value  system,  rendering  the  systemic  biases\nembedded within this judgment to appear normalized in\npractice. This myth, based on the colonial assumption\nthat  certain  uses  of  land  (agricultural  cultivation)  are\nmore “productive” than  others  (Indigenous  land  use),\nwas used to exclude Indigenous land and people[91].\nThe  productivity  myth  is  deeply  ingrained  into  the\nfinancialization of land today. “Highest and best use” is\na framework commonly used in commercial real estate\ndevelopment to appraise the potential profit of a piece\nof  land  and  decide  on  its  use.  Created  by  economist\nIrving Fisher, the framework assesses land use based on\nfour  criteria:  the  development  must  be  (1)  legally\npermissible,  (2)  physically  possible,  (3)  financially\nfeasible,  and  (4)  maximally  productive.  The  last\ncriterion, “maximally  productive”,  means  that  the\nchosen development should prioritize a type of use (for\nexample,  hotel  over  housing)  that  could  generate\nmaximum  profit,  disregarding  the  parcel’s  current\npurpose[92].\n Uses\n that\n are\n not \n“maximally\nproductive”—the\n balance\n of\n various\n types\n of\ncommercial, civic, and residential programs, a diverse\nmix  of  residents,  or  the  availability  of  transportation\ninfrastructure  that  promotes  active  lifestyles—do  not\nfactor into this analysis because their benefits cannot be\nquantified as direct revenue.\nSystems of measurement and valuation do not operate\nlinearly,  they  reinforce  one  another  iteratively.  The\nhistory  of  land  use  demonstrates  this  self-reinforcing\ndynamic:\n measurement\n serves\n to\n progressively\nsubdivide land while the value of the land progressively\nincreases[93].  This  dynamic  also  plays  out  in  the\nfinancialization  of  attention  by  platform  businesses.\nWith the more granular subdivision of attention through\nuser engagement, such as “like” and “share” and more\naccurate  valuation  of  user  behaviors,  the  value  of\nattention  increases  in  turn.  When  left  unchecked,  this\ndynamic  complicates  the  roles,  incentives,  and\nresponsibilities that are key to maintaining the health of\ndemocratic practice.\nUsing lessons from real estate and urban development,\nI  expose  three  mechanisms  platforms  used  to\nfinancialize attention: (1) conflate user roles in ways that\nundermine  their  agency,  (2)  tokenize  the  incentives\nbehind\n everyday\n communication\n to\n drive\n up\nengagement, and (3) use proxy metrics to defer the social\nresponsibility inherent in exclusionary practices.\n3.1    Conflate roles\nPlatforms  exploit  the  intersection  of  surveillance\ncapitalism and identity politics. Individuals are valued\nfor their authenticity while being asked to play multiple\nroles. Engagement metrics, such as “like” and “share”,\nprivilege  the  quantity  and  frequency  of  individuals’\nimmediate reactions while reducing their agency in their\nactions.  These  mechanisms  conflate  roles  in  two\nways.  In  an  era  of  information  overflow,  authentic\nself-expressions—which  is  scarce  by  nature—has\nbecome  a  valuable  commodity.  How  people  express\nthemselves  reveals  their  preferences,  interests,  and\nconnections and affirms their position as a member of\ntheir  social  network.  However,  these  expressions  are\nalso  the  key  input  into  platforms’ mechanisms  for\nincreasing\n conversion—algorithmic\n ranking\n and\npersonalized\n advertisements—that\n influence\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n327    \n \n\nindividuals’ purchasing and political decisions. Herein\nlies  platform  extraction:  using  the  authenticity  of  an\nindividual’s role as a member of their social group to\ncategorize  and  predict  their  role as  a  consumer  (of\ncommercial products and political advertising). The two\nroles that an individual is asked to play on a platform are\nnot  equally  consensual.  To  an  individual  user,  the\nplatform markets itself as a provider of communication\ninfrastructure  and  not  as  an  advertising  channel\npersonalized  based  on  their  personal  data.  In  blurring\nthese  two  roles,  the  extent  to  which  an  individual’s\nbehavior in their first role (as a member of their social\ngroup) is influenced by their second role (as a consumer)\nbecomes obscured as well.\nOptimizing for engagement also conflates otherwise\nseparate  roles  in  the  information  supply  chain:\nindividual users are not only consumers—they are also\nproducers  and  distributors.  These  separate  roles\ntypically enable the terms of transaction for each activity\nto be clearly delineated. However, interactions on the\nplatform  are  designed  to  encourage  all  three  types  of\nactivities at once. On Facebook, for example, all posts in\nthe News Feed are followed with the “like” button, the\n“comment” button, and the “share” button. Furthermore,\nthese engagement interactions are all reward-based. The\nquantity of “likes” given to a piece of content rewards\nproducers  with  a  sense  of  popularity.  Badges  such  as\n“Top\n Fan”—awarded\n to\n the\n most\n active\nparticipants—turn communication into competition. By\ncommunicating  through  the  platforms,  individuals\nbecome unwitting contestants in the commodification of\ntheir authenticity.\n3.2    Tokenize incentives\nSimilar to how casinos turn cash into token currencies in\nthe form of gambling chips, platforms turn incentives\ndriving social interactions into token currencies in the\nform of “likes” and “shares”. Token currencies increase\nthe  psychological  distance  between  the  cost  of\nconsumption and the action of consumption, and as a\nconsequence, they make it easier for users to consume\nmore[94]. Management scientist and economist Drazen\nPrelec refers to token currencies as “hedonic buffers”:\n“by  buffering  themselves  between  real  money  and\nconsumption, they protect consumption from the moral\ntax”[95].  When  purchasing  a  token  currency,  the\nconsumer does not need to specify how the currency will\nbe used. When the consumer spends the currency, they\ndo not feel the need to evaluate the implications of the\ntransaction  as  carefully  as  they  would  with  a  cash\ntransaction.\nSimilarly, when a user posts on the platform, he/she\ndoes  not  need  to  specify  his/her  intended  audience.\nEvery interaction on Facebook—be it a post, a comment,\nor a “like”—is, by default, broadcast to the entirety of the\nuser’s social network. If a user is required to specify to\nwhom they are speaking every time they write a post,\nthey  would  be  more  likely  to  consider  the  immediate\nconsequences of their action. By removing choice in one’s\naudience, the psychological distance between the social\ncost of an interaction and the interaction itself widens,\nand user engagement increases as a result.\nConsidering current debates around the limits of social\ncognition  online[96–98],  tokenizing  social  incentives\nexploits  and  undermines  the  cognitive  limits  of\nindividuals  on  social  networks.  Responses  to  this\nextractive  pattern  have  themselves  been  subsumed  by\nfinancialization.  The  rise  of  the  social  quantification\nsector[67] capitalizes on the extraction of attention as well\nas  its  conservation.  In  the  last  decade, “digital\nmindfulness”—from  meditation  apps  to  features  like\nScreen Time—has become a billion-dollar business; in\nparallel, social network platforms feed emotions into the\n“outrage industry”[99]. Like the false dichotomy of land\nas either a pristine wilderness[78] or a site of extraction,\nseeing  people’s  attention  as  either “protected” or\n“exploited” ultimately  distracts  from  the  extent  of\ndisempowerment caused by platforms and the fact that\nboth result in the commodification of authenticity.\n3.3    Defer responsibilities\nProxy metrics defer social responsibility to the technical\nimplementations of the system. This form of obfuscation\nmakes the values (such as racial discrimination or the\nrelentless  pursuit  of  profit)  that  fundamentally  drive\ndecisions shielded from direct critique.\nIn  the  context  of  cities,  the  ongoing  struggle  for\nsegregation\n demonstrates\n the\n extent\n to\n which\nexclusionary practices have co-evolved with the history\nof urban development. Discrimination acts and persists,\nindirectly,  through  proxy  metrics  that  encode  bias.  If\nmeasures\n to\n counteract\n discrimination\n are\n not\nproactively  instituted,  exclusionary  practices  will\nreinforce  discrimination.  In  the  early  1900s,  White\nhomeowners who perceived people of color as a threat\nto their property value began adopting racially restrictive\ncovenants to bar people of color from home ownership\n    328\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\nin their neighborhoods. When the federal government\ncreated the Home Owners’ Loan Corporation with the\naim to expand home ownership opportunities as a part\nof the New Deal, rather than proactively mitigating the\nracial discrimination, government surveyors based their\nneighborhood  ranking  system  on  local  officials’ and\nbankers’ racially charged risk assessments. In this way,\nthey  encoded  racial  discrimination  into  the  value  of\nland[100, 101],  which  resulted  in  racial  segregation  and\nconcentrated  poverty  that  still  persist  today[102–106].\nBeyond the direct encoding of exclusion, single-family\nzoning  ordinances  conceal  the  discrimination  behind\nproxy metrics like building density. Institutionalized by\nVillage of Arlington Heights v. Metropolitan Housing\nDevelopment Corp.(1977), single-family zoning de facto\nseparates lower-income populations—disproportionately\nracial  minorities—from  wealthier  populations[107],\nperpetuating systemic disinvestments.\nProxy metrics for revenue used by platforms make the\nprioritization  of  private  profit  at  the  expense  of  other\npublic  good  an  unquestioned  practice,  and  they\nunderscore how extensively the entanglements between\nexclusionary practice and finance have been normalized.\nInstead of proactively integrating different perspectives,\nplatforms  by  default  algorithmically  rank  messages\nbased  on  relevance,  measured  as “the  number  of\ncomments,  likes,  and  reactions  a  post  receives”[108].\nAlgorithmic  ranking  prioritizes  messages  that  support\none’s  preexisting  beliefs  and  exclude  ones  that  may\nchallenge those beliefs. Changes to the default sorting\nmethod, such as chronological sorting, must be manually\nselected by the user. Although Facebook’s News Feed\npreferences proclaim to let individuals “take control and\ncustomize” the  feed,  the  only  way  a  user  can  make\nchanges is to prune their News Feed: to add or remove\nup to 30 users to be prioritized to “see first”. Individuals\nhave no power to meaningfully change the exclusionary\nranking mechanism that determines the value of what\nthey see.\nThis  systematic  reinforcement  of  confirmation  bias\nundermines  a  fundamental  condition  for  a  healthy\ndemocracy:  a  shared  context  that  includes  divergent\nbeliefs,  founded  on  a  spirit  of  generosity  rather  than\nanimosity. The efficacy of democratic practice lies in the\ncollective ability to empathize, internalize, and reconcile\ndiffering  opinions  and  beliefs.  As  anthropologist\nElizabeth Povinelli writes, “The power of a particular\nform of communication to commensurate morally and\nepistemologically  divergent  social  groups  lies  at  the\nheart of liberal hopes for a nonviolent democratic form\nof governmentality”.\nFiltering  one’s  interactions  based  on  existing\npreferences and social connections narrows the context\nof  one’s  preexisting  beliefs.  In Liberalism  and  Social\nAction,  John  Dewey[109] writes, “The  method  of\ndemocracy is to bring conflicts out into the open where\ntheir special claims can be seen and appraised, where\nthey  can  be  discussed  and  judged  in  light  of  more\ninclusive interests than are represented by either of them\nseparately”.\n The \n“meaningful\n inefficiencies”[110]\ninherent  in  the  integration  of  diverse  perspectives  is\nfoundation for democracy and yet is at odds with the\nplatforms’ exclusive  focus  on  productivity.  If  social\nnetworks are to exist in service of democracy, then they\nneed  to  proactively  create  the  conditions  for\npluralism—to make it possible and desirable to reconcile\ndifferences rather than obscuring or exploiting them for\nprofit.\nIn  order  to  mitigate  the  systemic  biases  inherent  in\nsocial  networks  and  the  detrimental  effects  of  social\nexclusion,  the  first  step  is  to  question  the  status  quo.\nHistorically, land use that supports democracy has not\nbeen a given; it needed to be directly advocated for and\nformalized through law. The same expectations should\nbe set for the democratic affordance of social networks.\nThe  Montgomery  bus  boycott,  Freedom  Rides,  and\nmany other protests of the civil rights movement were\nfights  for  African  Americans  to  gain  equal  rights  in\npublic  space.  The  Civil  Rights  Act  of  1964  ended\nsegregation in the public space, and the Fair Housing Act\nof  1978  made  it  illegal  to  write  racially  restrictive\ncovenants  into  property  deeds.  A  part  of  the  work  of\nchanging  the  system  is  to  expose  its  mechanisms.  As\nRichard  Rothstein  advocated  in The  Color  of  Law,\nrevealing\n how\n the\n mechanisms\n work\n creates\nopportunities for their reform. In the next section, I draw\nfrom  the  practice  of  urban  planning  and  land  justice\nmovements  to  imagine  new  roles,  incentives,  and\nresponsibilities for social networks.\n4    Reclaim\n Social\n Networks\n from\nFinancialization\nReclaiming  social  networks  from  financialization  will\nrequire creating mechanisms that align the incentives of\nthe platform with the public interest. This begins with\nrecognizing  the  colonial  underpinnings  of  American\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n329    \n \n\ndemocracy[111–114] and relinquishing the nostalgic vision\nof  the  colonial  New  England  town  halls.  To  create  a\ncollective space for an experimentalist democracy fit for\nour time, we need to embrace rather than obscure the\n“contingency of context”[115] of our globally connected\nsociety. Using lessons from urban planning, land justice,\nand  Indigenous  land  stewardship,  I  propose  three\nmechanisms  to  help  reclaim  social  networks  from\nfinancialization and reorient them to the public interest:\n(1) use urban planning to redefine roles that have been\nconflated by platforms, (2) use community land trusts to\nillustrate  how  public  interest  can  be  protected  from\nmarket forces, and (3) use the practice of Indigenous land\nstewardship to inspire new thinking about the meaning\nof social responsibility.\n4.1    Redefine roles: “urban planning”\nUrban planning can serve as a model for a professional\nrole that serves the public interest. As designers of public\nspace,  urban  planners  must  wrestle  with  large  private\ninterests while they “continuously pursue and faithfully\nserve  the  public  interest”[116].  In  order  to  receive  the\nlicensure  to  practice—and  to  ensure  that “the  public\ninterest” prevails in these negotiations—urban planners\nmust  follow  Ethical  Principles  set  by  the  American\nPlanning Association’s Institute of Certified Planners:\n(1) Recognize the rights of citizens to participate in\nplanning decisions;\n(2) Strive to give citizens (including those who lack\nformal  organization  or  influence)  full,  clear,  and\naccurate  information  on  planning  issues  and  the\nopportunity  to  have  a  meaningful  role  in  the\ndevelopment of plans and programs;\n(3)  Strive  to  expand  choice  and  opportunity  for  all\npersons, recognizing a special responsibility to plan for\nthe needs of disadvantaged groups and persons;\n(4)  Assist  in  the  clarification  of  community  goals,\nobjectives, and policies in plan-making;\n(5)  Ensure  that  reports,  records,  and  any  other\nnon-confidential  information  which  is,  or  will  be,\navailable  to  decision  makers  is  made  available  to  the\npublic in a convenient format and sufficiently in advance\nof any decision;\n(6)  Strive  to  protect  the  integrity  of  the  natural\nenvironment and the heritage of the built environment;\n(7)  Pay  special  attention  to  the  interrelatedness  of\ndecisions  and  the  long-range  consequences  of  present\nactions.\nGiven  the  similarity  in  challenges  faced  by  urban\nplanners and stewards of social networks, the American\nPlanning\n Association’s\n Ethical\n Principles\n seem\neminently applicable to their roles. Values of inclusivity,\nfairness,  and  transparency  are  all  values  that  should\nguide the design of a healthy digital democracy. As a\nthought  experiment,  what  if  platforms  adopted  the\nfollowing ethical principles, based on the ones set forth\nby the American Planning Association?\n(1)  Recognize  the  rights  of  people  to  participate  in\nplatform design decisions;\n(2) Strive to give people (including those who lack\nformal  organization  or  influence)  full,  clear,  and\naccurate information on product development issues and\nthe opportunity to have a meaningful role in the design\nand development of the platform;\n(3)  Strive  to  expand  choice  and  opportunity  for  all\npersons, recognizing a special responsibility to plan for\nthe needs of disadvantaged groups and persons;\n(4)  Assist  in  the  clarification  of  community  goals,\nobjectives, and policies in plan-making;\n(5)  Ensure  that  reports,  records,  and  any  other\nnon-confidential  information  which  is,  or  will  be,\navailable  to  decision  makers  is  made  available  to  the\npublic in a convenient format and sufficiently in advance\nof any decision;\n(6) Strive to protect the integrity of the digital public\nsphere;\n(7)  Pay  special  attention  to  the  interrelatedness  of\ndecisions  and  the  long-range  consequences  of  present\nactions.\nThese  ethical  principles  would  encourage  better\ndecisions on the level of individual designers. However,\nthough these principles reflect core democratic values, to\nsubstantively  improve  the  business  of  the  platforms,\nethical  principles  are  far  from  enough.  Individuals’\ndecisions  and  responsibilities  correlate  to  their\ndecision-making power and scope of accountability. A\ndesigner  or  engineer  at  the  level  of  an “individual\ncontributor” in  a  technology  company  may  be\nresponsible for their own output, such as a “share” button\nor refresh content controls. While their design decisions\npotentially shape the communication systems between\nbillions of people, their social impact massively exceeds\ntheir power within the organization. Even if the designer\nor engineer adopted these ethical principles in the public\n    330\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\ninterest, they would face enormous barriers in practice\nand would personally bear the risk of acting against the\ninterest  of  their  employers.  Ethical  principles  must\noperate in a context that is greater than any individual\ndesigner or organization; they need to align with or shift\nthe incentive structure of the business model.\nIn  land  use,  urban  planning  and  real  estate\ndevelopment are different fields with distinct duties and\nethics.  Social  network  platforms,  in  their  current\nformation,\n collapse\n incentives,\n roles,\n and\nresponsibilities  that  help  preserve  meaningful  checks\nand balances between the private and public interests. In\nthe absence of an equivalent field of “urban planning”\ndedicated  to  the  public  interest  for  social  networks,\nplatforms  are  driven  exclusively  as  commercial  real\nestate\n development.\n Recognizing\n the\n different\nincentives  behind  these  two  professions  is  critical  for\ndiscussions on technology ethics. Unlike urban planners,\ncommercial real estate developers have no professional\nassociation  nor  explicit  ethical  principles.  Codes  of\nethics  are  often  found  in  fiduciary  duty-defined\nrelationships, which obligates a practitioner to act solely\nin the interests of their client—for example, doctor and\npatient. Real estate developers, on the other hand, do not\nhave a fiduciary duty towards the users of buildings they\ndevelop. Instead, they act in accordance with the profit\nmotives of their investors, whose interest in maximizing\nthe bottom line is often at odds with the interest of users.\nSimilarly,  social  network  platforms  act  in  the  best\ninterest  of  their  investors,  shareholders,  and  clients\n(advertisers),  which  leads  neither  to  the  benefit  nor\nprotection of the participants in the network.\nBecause  of  this  misalignment  of  individual  and\norganizational  values  inherent  in  social  network\nplatforms, it will be critical to develop “public interest”\nroles  for  social  networks,  the  equivalent  of  urban\nplanners and land justice activists—professions with a\nfiduciary  duty  that  aligns  with  their  democratic\nresponsibilities. In addition, beyond growing the field of\npublic interest designers and technologists, institutions\nneed to continue to create permanent positions for these\nroles to ensure their long-term viability.\n4.2    Restore incentives: “community land trusts”\nCurrent debates around individual data ownership apply\nproperty rights to address inequities in monetization, but\nthis approach is limited. In Colonial Lives of Property,\nlegal  scholar  Brenna  Bhandar  unpacks  how  colonial\nlogics have shaped modern conceptions of property and\ncreated “the  racial  regimes  of  ownership”[117, 118].\nFocusing on individual data ownership shifts the burden\nto individuals without addressing the commodification\nof their attention in the first place. Similarly, fixes that\nregulate\n individual\n user\n behavior—such\n as\nautomatically  limiting  the  time  a  user  can  spend  on\nplatforms\n (Social\n Media\n Addiction\n Reduction\nTechnology Act 2019)—do not address the root of the\nproblem. Real change requires creating alternatives to\nexisting platforms that differentiate ownership from use\nand remove attention from the commodity market.\nCommunity  land  trusts  are  nonprofit  organizations\nthat own and hold land in perpetuity in the permanent\nbenefit of the communities they serve[119]. Robert Swann,\nwho formalized the concept of the community land trust\nin Community Land Trust: A Guide to a New Model of\nLand  Tenure  in  America, connects  the  concept  to\nhistorical  roots  in  Indigenous  land  stewardship:\n“American Indian tradition holds that the land belongs to\nGod. Individual ownership and personal possession of\nland  and  resources  were  unknown”[120].  Community\nland  trusts  remove  land  from  the  commodity  market,\nthereby buffering it from the booms and busts of the real\nestate market cycles. Crucially, a community land trust\ndecouples the incentive of ownership from the incentive\nof use. Ownership is maintained in the public interest,\nwhile use allows for private interests through 99-year\nground leases, the longest possible term of lease of real\nestate  property.  The  community  ownership  of  land\naligns the incentives of the users and the owners; users\nhave long-term access to affordable space, and the trust\nhas  a  strong  legal  position  to  serve  its  mission  and\npreserve affordability.\nTo  develop  an  analogous  mechanism  for  social\nnetworks that could incentivize the platform to serve its\npeople  in  the  long  term,  we  must  recognize  how  the\ncommunity  land  trust  is  inextricably  linked  to  place.\nWhile  the  legal  arrangement  can  be  replicated  across\ngeographies  and  adapted  to  suit  local  needs,  a\ncommunity  land  trust  is  anchored  in  its  specific\ncommunity.  This  usage  of “community” is  entirely\ndifferent  from  the “community” used  in  Facebook’s\nmission  statement  (“Facebook’s  mission  is  to  give\npeople  the  power  to  build  community  and  bring  the\nworld closer together”). The community of a community\nland trust is defined by and bound by place, whereas the\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n331    \n \n\n“community” of Facebook refers to its user base and is\nboth decoupled from place and ever-expanding. Further,\nthe residents in the community have voting power by\nholding  board  seats  in  the  community  land  trust;\nFacebook users have no such power. Reclaiming social\nnetworks for the real benefit of communities means that\na  community,  defined  by  place,  should  own  the\ntechnology infrastructure and decision-making power in\nits use.\n4.3    Reframe  responsibilities: “Indigenous  land\nstewardship”\nIndigenous land stewardship is an example of collective\nstewardship  that  creates  systems-level  ecological\nbenefits like biodiversity[121] and resilience[122–125]. The\nsuccess  of  this  practice  depends  on  a  mutually\nconstitutive  relationship  between  people  and  land.  As\nNative American poet Paula Gunn Allen writes, “We are\nthe land... The land is not really the place (separate from\nourselves)  where  we  act  out  the  drama  of  our  isolate\ndestinies. It is not a means of survival, a setting for our\naffairs...  It  is  rather  a  part  of  our  being,  dynamic,\nsignificant, real. It is ourself”[126]. The responsibility to\ncare for the land and care for the self are one and the same.\nLearning  from  this  practice,  we  can  reorient  social\nnetworks  from  financialization  to  care.  This  shift\nsuggests  a  new  approach  to  thinking  about  social\nresponsibilities: from being external to being embodied.\nLikewise, the technology that serves this community\nmust not act from a distance; it must be co-designed.\nLaura Mannell, Frank Palermo, and Crispin Smith wrote\nin Reclaiming  Indigenous  Urban  Planning, “A\ncommunity plan cannot be developed from the outside\nlooking in. It cannot be done for a community, it must\nbe done with and by a community”. [127] A community’s\nsocial network, similarly, must be created with and by\nthe  community.  Technology  that  supports  social\nnetworks in the public interest begins with honoring the\nexisting  knowledge,  capacities,  and  practices  in  a\ncommunity as its starting point.\nThe fact that urban planning, community land trusts,\nand  Indigenous  land  stewardship  are all not-for-profit\npractices that exist to primarily serve social good brings\nup a natural question: can social network reforms based\non lessons from these practices be achieved from within\nexisting\n for-profit\n platform\n businesses?\n Fully\naddressing this question—which is fundamentally about\ntransforming the political economy of data—is beyond\nthe scope of this article. However, I do wish to highlight\nthree  entry  points  that  are  specifically  relevant  to\nbusinesses.  First,  businesses  comprise  groups  of\nemployees  who  hold  a  plurality  of  motivations  and\nbeliefs; these differences can and should be channeled\ntowards social change. Second, profit and social good\nare not necessarily mutually exclusive; businesses with\nbroad-based\n shared\n ownership\n and\n cooperative\ngovernance  structures  naturally  align  with  democratic\npractice. Third, coloniality runs deep in the culture of\ntechnology;  recognizing  colonial  inequities  within\norganizational culture itself is a critical first step.\n5    Conclusion\nSocial networks are now an undeniable public forum.\nHowever, their democratic potential has been undercut\nby  the  goals  of  platform  businesses  and  their\nmechanisms of financialization. The incentives driving\nthe  platform  set  private  and  public  interests  in  direct\nconflict.  As  publicly  traded  companies,  platforms  are\nultimately  accountable  to  their  shareholders  and  must\nprioritize private interests—the health of the business,\ndefined by its profitability and market share—over the\npublic interest and the health of democracy. As privately\nowned  public  spaces  in  their  current  form,  social\nnetworks’ public-facing experiences, which purport to\nchampion  connection  and  community  in  practice\nobscure the extractive nature of their business model.\nConnection is exploited for its network power to expand\nthe customer base; community is exploited as an input\ninto  a  platform’s  advertising  product.  The  language,\ninteractions, and relationships of social networks have\nbeen coopted.\nIn\n order\n to\n reclaim\n social\n networks\n from\nfinancialization by platform businesses, we need to first\nexpose the systems and mechanisms driving the process.\nAs this article has shown, the financialization of land\nprovides a critical lens for examining the systems that\nenable  the  financialization  of  attention  and  the\nconstitutive  role  colonialism  played  in  shaping  them.\nExamples from land use also demonstrate possibilities\nfor  rethinking  roles,  incentives,  and  responsibilities,\nshifting social networks from extraction to mutualism,\nfrom expansion to place. Given this new understanding,\nI  hope  we  can  move  from  critique  to  creation  and\ncollaboratively build the theoretical frameworks, legal\ninstruments,  funding  models,  technical  infrastructure,\n    332\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\nand social norms to steward social networks in the public\ninterest.\nAcknowledgment\nThe author wishes to thank the Ethical Tech Working\nGroup  at  the  Harvard  Berkman  Klein  Center  for\nInternet & Society.\nReferences\n D. Harvey, The right to the city, New Left Rev., vol. 53,\npp. 23–40, 2008.\n[1]\n Facebook, Inc., Annual report pursuant to section 13 or\n15(d) of the securities exchange act of 1934 for the fiscal\nyear ended December 31, 2020, https://d18rn0p25nwr6d.\ncloudfront.net/CIK-0001326801/4dd7fa7f-1a51-4ed9-\nb9df-7f42cc3321eb.pdf, 2020.\n[2]\n A. Kennedy, Packingham v. North Carolina (582 US __),\nhttps://casetext.com/case/packingham-v-north-carolina-1,\n2017.\n[3]\n H. A. Innis, Empire and Communications. Oxford, UK:\nClarendon Press, 1950.\n[4]\n D.  R.  Headrick, The  Tools  of  Empire: Technology  and\nEuropean  Imperialism  in  the  Nineteenth  Century.\nOxford, UK: Oxford University Press, 1981.\n[5]\n D. Q. Yang, Technology of Empire: Telecommunications\nand\n Japanese\n Expansion\n in\n Asia, \n1883−1945.\nCambridge, UK: Harvard University Asia Center, 2011.\n[6]\n A.  S.  Moore, Constructing  East  Asia: Technology,\nIdeology, and  Empire  in  Japan’s  Wartime  Era,\n1931−1945.  Stanford,  CA,  USA:  Stanford  University\nPress, 2013.\n[7]\n F. Meiton, Electrical Palestine: Capital and Technology\nfrom Empire to Nation. Oakland, CA, USA: University of\nCalifornia Press, 2019.\n[8]\n N. Mirzoeff, The right to look, Crit. Inq., vol. 37, no. 3,\npp. 473–496, 2011.\n[9]\n Statista,  Facebook  monthly  active  user, https://www.\nstatista.com/statistics/264810/number-of-monthly-active-\nfacebook-users-worldwide/, 2021.\n[10]\n S. T. Roberts, Behind the Screen: Content Moderation in\nthe  Shadows  of  Social  Media.  New  Haven,  CT,  USA:\nYale University Press, 2019.\n[11]\n K.  Langvardt,  Regulating  online  content  moderation,\nGeorgetown  Law  J.,  vol. 106,  no. 5,  pp. 1353–1388,\n2018.\n[12]\n S. M. West, Censored, suspended, shadowbanned: User\ninterpretations  of  content  moderation  on  social  media\nplatforms, New  Media  Soc.,  vol. 20,  no. 11,  pp. 4366–\n4383, 2018.\n[13]\n T. Gillespie, Content moderation, AI, and the question of\nscale, Big  Data  Soc., https://journals.sagepub.com/\ndoi/10.1177/2053951720943234, 2021\n[14]\n M. Alfano, J. A. Carter, and M. Cheong, Technological\nseduction  and  self-radicalization, J.  Am.  Philos.  Assoc.,\nvol. 4, no. 3, pp. 298–322, 2018.\n[15]\n H.  Brignull,  What  are  dark  patterns? https://www.\ndarkpatterns.org/, 2021.\n[16]\n A.  Mathur,  M.  Kshirsagar,  and  J.  Mayer.  2021.  What\n[17]\nmakes a dark pattern...dark?: Design attributes, normative\nconsiderations, and measurement methods, in Proc. 2021\nCHI  Conf. Human  Factors  in  Computing  Systems,\nYokohama, Japan, 2021, p. 360.\n L.  Lessig, Code  and  Other  Laws  of  Cyberspace.  New\nYork, NY, USA: Basic Books, 1999.\n[18]\n J. M. Hobson and L. Seabrooke, Everyday Politics of the\nWorld Economy. Cambridge, UK: Cambridge University\nPress, 2007.\n[19]\n H.  C.  Boyte,  Reframing  democracy:  Governance,  civic\nagency,  and  politics, Public  Adm.  Rev.,  vol. 65,  no. 5,\npp. 536–546, 2005.\n[20]\n B. J. T. Kerkvliet, Everyday politics in peasant societies\n(and ours), J. Peasant Stud., vol. 36, no. 1, pp. 227–243,\n2009.\n[21]\n J. Dewey, Democracy and Education: An Introduction to\nthe  Philosophy  of  Education.  New  York,  NY,  USA:\nMacmillan, 1916.\n[22]\n S.  Low,  D.  Taplin,  and  S.  Scheld, Rethinking  urban\nparks: Public Space and Cultural Diversity. Austin, TX,\nUSA: University of Texas Press, 2005.\n[23]\n S. S. Fainstein, New directions in planning theory, Urban\nAff. Rev., vol. 35, no. 4, pp. 451–478, 2000.\n[24]\n S.  Watson, City  Publics: The (Dis)enchantments  of\nUrban Encounters. London, UK: Routledge, 2006.\n[25]\n B. Auxier and M. Anderson, Social media use in 2021,\nhttps://davidleeking.com/social-media-use-in-2021/,\n2021.\n[26]\n D.  Freelon,  Discourse  architecture,  ideology,  and\ndemocratic  norms  in  online  political  discussion, New\nMedia Soc., vol. 17, no. 5, pp. 772–791, 2015.\n[27]\n D. Freelon, C. McIlwain, and M. Clark, Quantifying the\npower  and  consequences  of  social  media  protest, New\nMedia Soc., vol. 20, no. 3, pp. 990–1011, 2018.\n[28]\n S. J. Jackson and B. F. Welles, #Ferguson is everywhere:\nInitiators  in  emerging  counterpublic  networks, Inf.,\nCommun. Soc., vol. 19, no. 3, pp. 397–418, 2016.\n[29]\n D.  Boyd,  Social  network  sites  as  networked  publics:\nAffordances, dynamics, and implications, in A Networked\nSelf: Identity, Community, and Culture on Social Network\nSites,  Z.  Papacharissi,  ed.  New  York,  NY,  USA:\nRoutledge, 2010, pp. 39−58.\n[30]\n N. K. Baym and D. Boyd, Socially mediated publicness:\nAn introduction, J. Broadcast. Electron. Media, vol. 56,\nno. 3, pp. 320–329, 2012.\n[31]\n M.  Bossetta,  The  digital  architectures  of  social  media:\nComparing political campaigning on Facebook, Twitter,\nInstagram,  and  Snapchat  in  the  2016  U.S.  election, J.\nMass Commun. Q., vol. 95, no. 2, pp. 471–496, 2018.\n[32]\n J.  S.  Kayden, Privately  Owned  Public  Space: The  New\nYork City Experience. New York, NY, USA: John Wiley,\n2000.\n[33]\n City Planning Commission, Department of City Planning,\nZoning Handbook: A Guide to the Zoning Resolution of\nThe  City  of  New  York, https://www1.nyc.gov/assets/\nplanning/download/pdf/about/city-planning-history/\nzoning_handbook_1961.pdf, 1961.\n[34]\n M. J. Kiefer, Privatizing creation of the public realm: The\nfruits  of  New  York  City's  Incentive  Zoning  Ordinance,\nBoston  Coll.  Environ.  Aff.  Law  Rev.,  vol. 28,  no. 4,\npp. 637–649, 2001.\n[35]\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n333    \n \n\n M.  Kimmelman,  In  protest,  the  power  of  place,  New\nYork  Times, https://cpb-us-w2.wpmucdn.com/portfolio.\nnewschool.edu/dist/2/8848/files/2015/12/kimmelman-\nmichael-in-protest-the-power-of-place-1o2cch3.pdf,\n2011.\n[36]\n D.  Woodward,  Rules  of  conduct,  urban  omnibus,\nhttps://urbanomnibus.net/2012/05/rules-of-conduct/,\n2012.\n[37]\n C.  Willis, Form  Follows  Finance: Skyscrapers  and\nSkylines in New York and Chicago. Princeton, NJ, USA:\nPrinceton Architectural Press, 1995.\n[38]\n M.  Soules,  Constant  object, Log, https://www.jstor.\norg/stable/26323875. 2021.\n[39]\n C.  Arun,  Facebook's  faces, Forthcoming  Harvard  Law\nReview  Forum, https://papers.ssrn.com/sol3/papers.cfm?\nabstract_id=3805210, 2021.\n[40]\n B. H. Bratton, The Stack: on Software and Sovereignty.\nCambridge, UK: MIT Press, 2015.\n[41]\n S. P. Choudary, M. W. Van Alstyne, and G. G. Parker,\nPlatform  Revolution: How  Networked  Markets  Are\nTransforming  the  Economy  and  How  to  Make  Them\nWork for You. New York, NY, USA: W. W. Norton &\nCompany, 2016.\n[42]\n F.  Pasquale, The  Black  Box  Society: the  Secret\nAlgorithms  that  Control  Money  and  Information.\nCambridge, UK: Harvard University Press, 2015.\n[43]\n Y.  Moulier-Boutang, Cognitive  Capitalism.  Cambridge,\nUK: Polity, 2004.\n[44]\n A.  Tsing,  Supply  chains  and  the  human  condition,\nRethinking Marxism, vol. 21, no. 2, pp. 148–176, 2009.\n[45]\n N.  Leong,  Racial  capitalism, Harv.  Law  Rev.,  vol. 126,\nno. 8, pp. 2151–2226, 2013.\n[46]\n N. Srnicek, Platform Capitalism. Cambridge, UK: Polity\nPress, 2016.\n[47]\n S. Zuboff, The Age of Surveillance Capitalism: The Fight\nfor a Human Future at the New Frontier of Power. New\nYork, NY, USA: Public Affairs, 2019.\n[48]\n B.  Christophers, Rentier  Capitalism: Who  Owns  the\nEconomy, and Who Pays for It? New York, NY, USA:\nVerso Books, 2020.\n[49]\n J. Sadowski, The Internet of landlords: Digital platforms\nand  new  mechanisms  of  rentier  capitalism, Antipode,\nvol. 52, no. 2, pp. 562–580, 2020.\n[50]\n K.  Birch  and  F.  Muniesa, Assetization: Turning  Things\nInto  Assets  in  Technoscientific  Capitalism.  Cambridge,\nUK: MIT Press, 2020.\n[51]\n D.  Byler, Terror  Capitalism.  Durham,  UK:  Duke\nUniversity Press, 2022.\n[52]\n T. Gillespie, The politics of ‘platforms’, New Media Soc.,\nvol. 12, no. 3, pp. 347–364, 2010.\n[53]\n J. C. Plantin, C. Lagoze, P. N. Edwards, and C. Sandvig,\nInfrastructure studies meet platform studies in the age of\nGoogle  and  Facebook, New  Media  Soc.,  vol. 20,  no. 1,\npp. 293–310, 2018.\n[54]\n M.  Lazzarato,  Immaterial  labor,  in Radical  Thought  in\nItaly: A  Potential  Politics,  P.  Virno  and  M.  Hardt,  eds.\nMinneapolis, MN, USA: University of Minnesota Press,\n1996, pp. 133–147.\n[55]\n T. Wu, The Attention Merchants: The Epic Scramble to\nGet  Inside  Our  Heads.  New  York,  NY,  USA:  Knopf,\n2016.\n[56]\n T.  Hwang, Subprime  Attention  Crisis: Advertising  and\nthe Time Bomb at the Heart of the Internet. New York,\nNY, USA: FSG Originals, 2020.\n[57]\n A.  Appadurai, Banking  on  Words. The  Failure  of\nLanguage in the Age of Derivative Finance. Chicago, IL,\nUSA: University of Chicago Press, 2015.\n[58]\n R. Martin, From the critique of political economy to the\ncritique  of  finance,  in Derivatives  and  the  Wealth  of\nSocieties, B. Lee and R. Martin, eds. Chicago, IL, USA:\nUniversity of Chicago Press, 2016, pp. 174–196.\n[59]\n R.  Aitken, “A  Machine  For  Living”:  The  cultural\neconomy  of  financial  subjectivity,  in The  Routledge\nInternational  Handbook  of  Financialization,  P.  Mader,\nD.  Mertens,  and  N.  Van  der  Zwan,  eds.  London,  UK:\nRoutledge, 2020.\n[60]\n M.  Adas,  Imperialism  and  colonialism  in  comparative\nperspective, Int.  Hist.  Rev.,  vol. 20,  no. 2,  pp. 371–388,\n1998.\n[61]\n D.  Arnold,  Agriculture  and ‘improvement’ in  early\ncolonial  India:  A  pre-history  of  development, J.  Agrar.\nChange, vol. 5, no. 4, pp. 505–525, 2005.\n[62]\n A.  LaFrance,  Facebook  and  the  new  Colonialism,  The\nAtlantic, https://www.theatlantic.com/technology/archive/\n2016/02/facebook-and-the-new-colonialism/462393/,\n2016.\n[63]\n J.  Thatcher,  D.  O’Sullivan,  and  D.  Mahmoudi,  Data\ncolonialism through accumulation by dispossession: New\nmetaphors for daily data, Environ. Plan. D: Soc. Space,\nvol. 34, no. 6, pp. 990–1006, 2016.\n[64]\n S. Amrute, Bored techies being casually racist: Race as\nalgorithm, Sci., Technol., Human  Values,  vol. 45,  no. 5,\npp. 903–933, 2020.\n[65]\n S. Calzati, Decolonising “Data colonialism” propositions\nfor  investigating  the  realpolitik  of  today’s  networked\necology, Telev. New Media, vol. 22, no. 8, pp. 914–929,\n2021.\n[66]\n N.  Couldry  and  U.  A.  Mejias,  Data  colonialism:\nRethinking  Big  Data’s  relation  to  the  contemporary\nsubject, Telev.  New  Media,  vol. 20,  no. 4,  pp. 336–349,\n2019.\n[67]\n M.  Foucault,  The  archaeology  of  knowledge, Soc.  Sci.\nInf., vol. 9, no. 1, pp. 175–185, 1970.\n[68]\n W. N. Espeland and M. L. Stevens, Commensuration as a\nsocial  process, Annu.  Rev.  Sociol.,  vol. 24,  no. 1,\npp. 313–343, 1998.\n[69]\n A.  L.  Tsing,  A.  S.  Mathews,  and  N.  Bubandt,  Patchy\nanthropocene: Landscape structure, multispecies history,\nand the retooling of anthropology, Curr. Anthropol., vol.\n60, no. S20, pp. S186−S197, 2019.\n[70]\n A.  Linklater, Owning  the  Earth: the  Transforming\nHistory  of  Land  Ownership.  New  York,  NY,  USA:\nBloomsbury, 2013.\n[71]\n J.  Shaw,  Platform  Real  Estate:  Theory  and  practice  of\nnew  urban  real  estate  markets, Urban  Geogr.,  vol. 41,\nno. 8, pp. 1037–1064, 2020.\n[72]\n K. Birch, Rethinking value in the bio-economy: Finance,\nassetization, and the management of value, Sci., Technol.\nHuman Values, vol. 42, no. 3, pp. 460–490, 2017.\n[73]\n S. Botzem and L. Dobusch, Financialization as strategy:\nAccounting for inter-organizational value creation in the\n[74]\n    334\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\nEuropean real estate industry, Account., Organizat. Soc.,\nvol. 59, pp. 31–43, 2017.\n S.  Sassen,  Predatory  formations  dressed  in  Wall  Street\nsuits  and  algorithmic  math, Sci., Technol.  Soc.,  vol. 22,\nno. 1, pp. 6–20, 2017.\n[75]\n C. Ward and E. Swyngedouw, Neoliberalisation from the\nground  up:  Insurgent  capital,  regional  struggle,  and  the\nassetisation\n of\n land, \nAntipode,\n vol. 50,\n no. 4,\npp. 1077–1097, 2018.\n[76]\n E.  A.  Povinelli,  Radical  worlds:  The  anthropology  of\nincommensurability  and  inconceivability, Annu.  Rev.\nAnthropol., vol. 30, pp. 319–334, 2001.\n[77]\n A.  Gómez-Pompa  and  A.  Kaus,  Taming  the  wilderness\nmyth:  environmental  policy  and  education  are  currently\nbased  on  Western  beliefs  about  nature  rather  than  on\nreality, BioScience, vol. 42, no. 4, pp. 271–279, 1992.\n[78]\n W. M. Denevan, The pristine myth: The landscape of the\nAmericas  in  1492, Ann.  Assoc.  Am.  Geogr.,  vol. 82,\nno. 3, pp. 369–385, 1992.\n[79]\n W. Cronon, The trouble with wilderness; or, getting back\nto the wrong nature, in Uncommon Ground: Rethinking\nthe Human Place in Nature, W. Cronon, ed. New York,\nNY, USA: W. W. Norton & Co., 1995, pp. 69−90.\n[80]\n M.  D.  Spence, Dispossessing  the  Wilderness: Indian\nRemoval and the Making of the National Parks. Oxford,\nUK: Oxford University Press, 1999.\n[81]\n M.  Dowie, Conservation  Refugees: The  Hundred-Year\nConflict  Between  Global  Conservation  and  Native\nPeoples. Cambridge, UK: MIT Press, 2010.\n[82]\n D.  Gilio-Whitaker, As  Long  as  Grass  Grows: The\nIndigenous  Fight  for  Environmental  Justice, from\nColonization  to  Standing  Rock.  Boston,  MA,  USA:\nBeacon Press, 2019.\n[83]\n H. N. Smith, Virgin Land: the American West as Symbol\nand  Myth.  Cambridge,  UK:  Harvard  University  Press,\n1950.\n[84]\n S.  Striffler  and  M.  Moberg, Banana  Wars: Power,\nProduction, and History in the Americas. Durham, UK:\nDuke University Press, 2003.\n[85]\n A.  Tsing,  Unruly  edges:  Mushrooms  as  companion\nspecies: For Donna haraway, Environ. Humanit., vol. 1,\nno. 1, pp. 141–154, 2012.\n[86]\n L. Gill and S. Kasmir, History, politics, space, labor: On\nunevenness  as  an  anthropological  concept, Dialect.\nAnthropol., vol. 40, no. 2, pp. 87–102, 2016.\n[87]\n J. R. Eichen, Cheapness and (labor-) power: The role of\nearly  modern  Brazilian  sugar  plantations  in  the\nracializing  Capitalocene, Environ.  Plan.  D: Soc.  Space,\nvol. 38, no. 1, pp. 35–52, 2020.\n[88]\n J. Comaroff and J. L. Comaroff, Theory from the South:\nOr,  how  Euro-America  is  evolving  toward  Africa,\nAnthropol. Forum, vol. 22, no. 2, pp. 113–131, 2012.\n[89]\n J. C. Scott, Seeing Like a State: How Certain Schemes to\nImprove the Human Condition Have Failed. New Haven,\nCT, USA: Yale University Press, 1999.\n[90]\n L. Domínguez and C. Luoma, Decolonising conservation\npolicy:  How  colonial  land  and  conservation  ideologies\npersist  and  perpetuate  indigenous  injustices  at  the\nexpense  of  the  environment, Land,  vol.  9,  no.  3,  p.  65,\n2020.\n[91]\n M.  A.  Munizzo  and  L.  V.  Musial, General  Market\n[92]\nAnalysis and Highest and Best Use. Boston, MA, USA:\nSouth-Western Educational Pulishing, 2010.\n S. Lee and C. Webster, Enclosure of the urban commons.\nGeoJournal, vol. 66, nos. 1&2, pp. 27–42, 2006.\n[93]\n M.  D.  Griffiths,  Social networking  addiction: Emerging\nthemes and issues, J. Addict. Res. Ther., vol. 4, p. e118,\n2013.\n[94]\n D. Prelec, Consumer behavior and the future of consumer\npayments,  in Moving  Money: The  Future  of  Consumer\nPayments, R. E. Litan and M. N. Baily, eds. Washington\nDC, USA: Brookings Institution Press, 2009.\n[95]\n J.  De  Ruiter,  G.  Weston,  and  S.  M.  Lyon,  Dunbar’s\nnumber:  Group  size  and  brain  physiology  in  humans\nreexamined, Am.  Anthropol.,  vol. 113,  no. 4,  pp. 557–\n568, 2011.\n[96]\n R. I. M. Dunbar, Social cognition on the Internet: Testing\nconstraints  on  social  network  size, Philos.  Trans.  Roy.\nSoc.  B: Biol.  Sci.,  vol. 367,  no. 1599,  pp. 2192–2201,\n2012.\n[97]\n R.  I.  M.  Dunbar, The  Social  Brain  Hypothesis  and\nHuman  Evolution.  Oxford  Research  Encyclopedia  of\nPsychology, https://doi.org/10.1093/acrefore/9780190236\n557.013.44, 2021.\n[98]\n J.  M.  Berry  and  S.  Sobieraj, The  Outrage  Industry:\nPolitical  Opinion  Media  and  the  New  Incivility.  New\nYork, NY, USA: Oxford University Press, 2013.\n[99]\n T.  M.  Michney,  How  the  city  survey’s  redlining  maps\nwere  made:  A  closer  look  at  HOLC’s  Mortgagee\nRehabilitation  Division, J. Plan. Hist.,  doi:  10.1177/\n15385132211013361.\n[100]\n D. Aaronson, D. Hartley, and B. Mazumder, The effects\nof  the  1930s  HOLC “redlining” maps, Am.  Econ.  J.:\nEcon. Policy, vol. 13, no. 4, pp. 355–392, 2021.\n[101]\n K. T. Jackson, Race, ethnicity, and real estate appraisal:\nThe  home  owners  loan  corporation  and  the  Federal\nHousing  Administration, J.  Urban  Hist.,  vol. 6,  no. 4,\npp. 419–452, 1980.\n[102]\n D. R. Harris, “Property values drop when blacks move in,\nbecause…”:  Racial  and  socioeconomic  determinants  of\nneighborhood  desirability, Am.  Sociol.  Rev.,  vol. 64,\nno. 3, pp. 461–479, 1999.\n[103]\n K. B. Crossney and D. W. Bartelt, Residential security,\nrisk, and race: The Home Owners' Loan Corporation and\nmortgage  access  in  two  cities, Urban  Geogr.,  vol. 26,\nno. 8, pp. 707–736, 2005.\n[104]\n R. Rothstein, The Color of Law: A Forgotten History of\nHow  Our  Government  Segregated  America.  New  York,\nNY, USA: Liveright, 2017.\n[105]\n D.  Popescu,  The  importance  of  bearing  witness, East\nEur. Polit. Soc.: Cult., vol. 32, no. 2, pp. 315–319, 2018.\n[106]\n R. Plunz, A History of Housing in New York City. New\nYork, NY, USA: Columbia University Press, 1990.\n[107]\n Company Info, Meta, 2021, http://www.craftwindowsltd.\nco.uk/, 2021.\n[108]\n J. Dewey, Liberalism and Social Action. New York, NY,\nUSA: Capricorn Books, 1963.\n[109]\n E.  Gordon  and  G.  Mugar, Meaningful  Inefficiencies:\nCivic  Design  in  an  Age  of  Digital  Expediency.  Oxford,\nUK: Oxford University Press, 2020.\n[110]\n N.  Gordon,  Democracy  and  colonialism, Theory  Event,\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=\n[111]\n  Joanne Cheung:   Real Estate Politik: Democracy and the Financialization of Social Networks\n335    \n \n\n1862012, 2010.\n E. Atanassow, Colonization and democracy: Tocqueville\nreconsidered, Am.  Pol.  Sci.  Rev.,  vol. 111,  no. 1,\npp. 83–96, 2017.\n[112]\n A.  Lee  and  J.  Paine,  What  were  the  consequences  of\ndecolonization? Int. Stud. Q., vol. 63, no. 2, pp. 406–416,\n2019.\n[113]\n T. A. Koelble and E. Lipuma, Democratizing democracy:\nA postcolonial critique of conventional approaches to the\n‘measurement  of  democracy’, Democratization,  vol. 15,\nno. 1, pp. 1–28, 2008.\n[114]\n Z.  Bauman, Alone  Again  -  Ethics  After  Certainty.\nLondon, UK: Demos, 1996.\n[115]\n American  Planning  Association,  Ethical  principles\nin\n planning, \nhttps://www.planning.org/ethics/\nethicalprinciples/, 1992.\n[116]\n B. Bhandar, Colonial Lives of Property: Law, Land, and\nRacial  Regimes  of  Ownership.  Durham,  UK:  Duke\nUniversity Press, 2018.\n[117]\n N.  Blomley, Unsettling  the  City: Urban  Land  and  the\nPolitics  of  Property.  New  York,  NY,  USA:  Routledge,\n2003.\n[118]\n J.  E.  Davis, The  Community  Land  Trust  Reader.\nCambridge, UK: Lincoln Institute of Land Policy, 2010.\n[119]\n R. S. Swann, The Community Land Trust: A Guide to A\nNew Model for Land Tenure in America. North Hampton,\nUK:  Center  for  Community  Economic  Development,\n1972.\n[120]\n K. M. Hoffman, E. L. Davis, S. B. Wickham, K. Schang,\nA.  Johnson,  T.  L.  Patrick,  N.  Lauriault,  N.  Q.  Le,  E.\nSwerdfager,  and  A.  J.  Trant,  Conservation  of  Earth’s\nbiodiversity is embedded in Indigenous fire stewardship,\nProc.\n Natl.\n Acad.\n Sci.\n USA,\n vol. 118,\n no. 32,\np. e2105073118, 2021.\n[121]\n C.  D.  Becker  and  K.  Ghimire,  Synergy  between\ntraditional  ecological  knowledge  and  conservation\nscience supports forest preservation in Ecuador, Conserv.\n[122]\nEcol., vol. 8, no. 1, p. 1, 2003.\n R. W. Kimmerer and F. K. Lake, The role of Indigenous\nburning in land management, J. Forest., vol. 99, no. 11,\npp. 36–41, 2001.\n[123]\n F.  K.  Lake  and  A.  C.  Christianson,  Indigenous  fire\nstewardship, in Encyclopedia of Wildfires and Wildland-\nUrban Interface (WUI) Fires, S. L. Manzello, ed. Cham,\nSwitzerland: Springer, 2019, pp. 9.\n[124]\n C.  I.  Roos,  T.  W.  Swetnam,  T.  J.  Ferguson,  M.  J.\nLiebmann, R. A. Loehman, J. R. Welch, E. Q. Margolis,\nC. H. Guiterman, W. C. Hockaday, M. J. Aiuvalasit, J.\nBattillo, J. Farella, and C. A. Kiahtipes, Native American\nfire management at an ancient wildland-urban interface in\nthe Southwest United States, Proc. Natl. Acad. Sci. USA,\nvol. 118, no. 4, p. e2018733118, 2021.\n[125]\n A. L. Booth, We are the land: Native American views of\nnature,  in Nature  Across  Cultures,  H.  Selin,  ed.\nDordrecht, the Netherland: Springer, 2003, pp. 329−349.\n[126]\n L. Mannell, F. Palermo, and C. Smith, Community-based\nand  comprehensive:  Reflections  on  planning  and  action\nin  First  Nations,  in Reclaiming  Indigenous  Urban\nPlanning,  R.  Walter,  T.  Jojola,  and  D.  Natcher,  eds.,\nMontreal,  Canada:  McGill-Queen’s  University  Press,\n2013, pp. 113–140.\n[127]\nJoanne  Cheung is  a  lecturer  at  the  Haas\nSchool\n of\n Business,\n University\n of\nCalifornia, Berkeley, USA. She has been a\nfellow  at  the  Harvard  Berkman  Klein\nCenter  for  Internet  &  Society  and  the\nAmerican\n Association\n of\n University\nWomen.  She  received  her  M.Arch  degree\nfrom Harvard Graduate School of Design in\n2018, her MFA degree from Bard College Milton Avery Graduate\nSchool  of  the  Arts  in  2013,  her  BA  degree  from  Dartmouth\nCollege in 2009. Her current research focuses on climate justice\nand Indigenous data sovereignty.\n    336\nJournal of Social Computing, December 2021, 2(4): 323−336    \n \n\n \nAlgorithmic Silence: A Call to Decomputerize\nJonnie Penn*\nAbstract:    Tech critics become technocrats when they overlook the daunting administrative density of a\ndigital-first society. The author implores critics to reject structural dependencies on digital tools rather than\nnaturalize their integration through critique and reform. At stake is the degree to which citizens must defer to\nunelected experts to navigate such density. Democracy dies in the darkness of sysadmin. The argument and a\ncandidate solution proceed as follows. Since entropy is intrinsic to all physical systems, including digital\nsystems, perfect automation is a fiction. Concealing this fiction, however, are five historical forces usually\ntreated in isolation: ghost work, technical debt, intellectual debt, the labor of algorithmic critique, and various\ntypes of participatory labor. The author connects these topics to emphasize the systemic impositions of digital\ndecision tools, which compound entangled genealogies of oppression and temporal attrition. In search of a\nharmonious balance between the use of “AI” tools and the non-digital decision systems they are meant to\nsupplant, the author draws inspiration from an unexpected source: musical notation. Just as musical notes\nrequire silence to be operative, the author positions algorithmic silence—the deliberate exclusion of highly\nabstract digital decision systems from human decision-making environments—as a strategic corrective to the\nfiction  of  total  automation.  Facial  recognition  bans  and  the  Right  to  Disconnect  are  recent  examples  of\nalgorithmic silence as an active trend.\nKey  words:   technocracy;  algorithmic  silence;  history;  labor;  artificial  intelligence;  AI  ethics;  automation;\ndecomputerization\n1    Introduction\nIn 1948, in an article in Business Week, a Vice President\nat  the  Ford  Motor  Company  coined  the  term\n“automation” to  promote  the  use  of  mechanized\nself-governance in manufacturing. Since entropy, error,\nand deterioration are intrinsic to all physical systems,\nincluding digital systems, perfect automation is a fiction.\nEven still, economists, industrialists, and technologists\ncontinue to invoke idealizations of “automation” in their\ninfluential visions of society. In this article, the author\nchallenges  the  heightened  rhetoric  major  technology\ncompanies and computer scientists have recently used to\ncharacterise the autonomous and predictive capabilities\nof advanced digital decision tools, the current vogue of\nthe automated society. The author shows how reports of\na  looming “AI  Revolution” misrepresent  the  complex\nways in which such tools have been used, in practice, to\npreserve the political status quo in the United States and\nUnited Kingdom.① Yet this article is not just a critique.\nIn pursuit of a harmonious balance between the use of\nsuch tools, the use of the non-digital decision systems\nthey  are  meant  to  supplant,  and  the  modes  of\nadministrative labor required for each, the author draws\ninspiration from an unexpected source: musical notation.\nJust  as  musical  notes  require  silence  in  order  to  be\noperative,  the  author  argues  that  societies  must\nstrategically emphasize—rather than simply seeking to\ndisplace—non-digital decision systems by limiting their\nuse of digital alternatives. To crystallize this point, the\nauthor introduces the concept of algorithmic silence: the\n \n • Jonnie Penn is with  the Department of History and Philosophy\nof Science, University of Cambridge, Cambridge, CB2 3RH, UK,\nand also with the Berkman Klein Center, Harvard University,\nCambridge, MA 02138, USA. E-mail: jnpenn@gmail.com.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-11-23;\naccepted: 2021-11-25\n①The author reserves his comments to the two countries about which\nhe has most expertise.\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   04/06  pp337−356\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0023\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\ndesignation of a deliberate exclusion of highly abstract\ndigital decision systems from human decision-making\nenvironments.  Recent  bans  on  facial  recognition\ntechnologies are an example of algorithmic silence.\nWhile  the  rise  of  digital  automation  has  afforded\ntremendous  opportunities  for  social  transformation,  it\nhas also disguised growing administrative burdens. This\nunderappreciated  coupling  is,  by  my  account,  a  key\nreason to normalize algorithmic silence. As the cost of\ndigital decision systems decreases globally and their use\nbecomes  more  prolific,  the  accompanying  need  for\ndiverse  types  of  administrative  labor  will  escalate,\nperhaps precipitously. To evidence this trend, the author\nconnects  five  realms  of  scholarship  usually  treated  in\nisolation: ghost work, technical debt, intellectual debt,\nthe labor of algorithmic critique, and various types of\nparticipatory labor. The author emphasizes the systemic\nimpositions  that  digital  decision  systems  make  on\nhuman  beings  not  only  as  workers  and  members  of\ndifferent racial, class, or gender groups, as other scholars\nhave shown, but also as consumers, citizens, parents, or\nany other number of identity frames. These obligations\ncompound  in  idiosyncratic  proportions  depending  on\none’s  entangled  identities,  and  their  harms  should  be\nmitigated in respect to these differences. Yet, the author\nadds, the potential also exists to forge a cross-cutting\nform of solidarity that addresses broad exposures to the\nKafkaesque  cacophony  of  digital  decision  systems  in\noversupply. Modes of collective restraint, such acts of\nalgorithmic silence, could help distance AI development\nfrom  technocracy  and  align  it  with  traditions  of\nde-escalation, such as decomputerization and degrowth.\n2    Disingenuous  Rhetoric  and “The  AI\nRevolution”\nIn popular use today, the term “artificial intelligence” is\na palimpsest: etched over the disciplines’ mid-twentieth\ncentury origins, rife with theories of neural activity, is a\nradical  ethos  of  imminent  social  transformation  via\nautomation.② AI is a catch-all not just for a branch of\ncomputer science and its subsets, but for myriad other\ndigital  automation  techniques  as  well.  Yarden  Katz\nexcavates this layering to reveal how, in the early 2010s,\nmajor  American  technology  firms  lent  panache  to\nsales  of  their  data  science  and  machine  learning\nproducts and services by perpetuating the existence of\n“The  AI  Revolution”[1].  Their  campaigns  publicly\nconsummated[2] \nthe\n field’s\n longstanding\n but\nunderappreciated  entanglements  with  institutional\npatrons  intent  on  developing  sophisticated  tools  for\nsocial  analysis  and  control[3].  These  interventions\ncapitalized  on  tropes  of  imminent  technological\npotential  inherited  through  Western  myth,  science\nfiction,  religion,  economics,  and  popular  culture[4–9].\nBlade Runner, for example, which builds its narrative\naround the existence of synthetic human-like “replicants”,\nis  set  on  November  20,  2019,  the  rough  date  of  this\narticle’s writing[10]. The future, it seems, is now.\nThe AI Revolution, like the computer revolution, is\nnot a real revolution[11, 12].③ Proponents do not seek to\nforcibly overthrow an existing social order. Far from it.\nAs  Katz  shows,  the  AI  Revolution  is  largely  a\nconservative  push  to  preserve  and  benefit  from  the\npolitical status quo, which, as this issue attests, is marked\nby  historic  levels  of  financial  and  informational\ninequality. A growing body of scholarship clarifies how\nsuch  tool  and  services  repackage  and  reinforce\nanti-black[13, 14],\n anti-poor[15],\n and\n chauvinist\nlogics[16]—all  under  the  pretense  of  progress  and\nefficiency[11, 17–21]. The AI Revolution is thus genuinely\npolitical—just not in the ways it is made out to be[22].\nDisingenuous  rhetoric  plays  an  important  role  in\nconstructing  civic  imaginaries  about  the  future.  A\ncritical audit of the evocative terminology used in and\naround  AI  research  is  long  overdue[23–25].  A  1976\nmissive by an MIT AI engineer challenged the field’s\n“contagious” use  of  wishful  mnemonics:  words  that\nserved as “incantations” for a desired result, rather than\nsober descriptions of a mechanism or function[23, 26–29].④\nA recent framing captures this trick in action. In 2018,\na team at the Toronto Rotman School of Management\ncast  AI  as “a  drop  in  the  cost  of  prediction”[30].  As\nprediction became cheaper, the team reasoned, it would\nbe  used  to  solve  problems  that  were  not  traditionally\nprediction problems, such as autonomous driving. This\n②A  palimpsest  is  a  manuscript  on  which  later  writing  has  been\nsuperimposed  on  earlier  writing.  Thank  you  to  Sarah  Dillon  for  this\nmetaphor.\n③See  Hicks  for  a  critical  take  on  how  the  1950−1970s  computer\n“revolution” in the UK served to entrench existing gender inequalities.\nSummary in Ref. [12].\n④Naming  conventions  were  judged  to  have  warped  researcher’s\nrelationship  to  the  epistemic  significance  of  their  designs.  Artificial\nintelligence  is  itself  a  wishful  mnemonic,  unique  from  chemistry  and\nphysics in that the name portrays an intention. See Garvey for a survey of\nAI critique over the second half of the twentieth century and Dreyfus for\na glimpse into various eras of critique.\n    338\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\nis an insightful observation, but not necessarily for the\nreasons its authors intended. The AI Revolution does not\nmark a genuine drop in the cost of prediction, but it may,\ninstead,  mark  a  meaningful  drop  in  the  cost  to  feign\nprediction.  Stated  differently,  it  is  becoming  trivially\neasy  to  manufacture  the  pretense  of “predicting” an\noutcome in areas where prediction, in fact, defies natural\nlaw.\nCritics clarify that, at a technical level, contemporary\nAI capabilities are closer in substance to Katz’s account\nthan to the account put forward by those at the Rotman\nSchool[24，31]. Most so-called “predictive” analytics lack\nthe necessary relation to causality to genuinely foretell\nan outcome in advance. “I have not found a single paper\npredicting  a  future  result.  All  of  them  claim  that  a\nprediction could have been made; i.e., they are post-hoc\nanalysis”[31].  The  term  is  mistakenly  used  to  describe\nnovel statistical correlations after events have occurred,\nrather than identifying a determinate causal mechanism\nbeforehand. One example is the recently debunked claim\nthat AI can “predict” someone’s sexual preference from\ntheir photograph[32]. Prediction implies prophecy, which\nis  intimidating  and  inaccurate.  At  a  technical  level,\nargues  Momin  M.  Malik,  the  term “detect” is  more\nprecise, if still not totally satisfying.⑤\nThe  risks  involved  in  indulging  such  prophetic\nrhetoric  are  compounded  in  cases  in  which  a  user’s\nenvironment can be altered to make a product appear\nmore “predictive” than it is[33–35]. For instance, it is far\neasier  for  a  driverless  vehicle  to  appear  autonomous\nwithin the perpetually dry city grid of Phoenix, Arizona,\nthan it would be for that same vehicle to navigate the wet,\ntwisted lanes of Aberdeen, Scotland. Phoenix has fewer\ncharacteristic features, which makes changes easier for\nan “autonomous” vehicle to infer. Disingenuous rhetoric\narises when results from a constrained environment (e.g.,\nPhoenix)  are  treated  as  universally  applicable  (e.g.,\nadequate to navigate all locales, including Aberdeen).\nThese  claims  are  covertly  subjective  not  just  because\nthey overstate the competency of the algorithmic system\nin question under the guise of technological objectivity,\nbut  also  because  they  treat  the  value  of  certain\nconstraints  (e.g.,  a  city  in  a  grid  formation)  as\nself-evident,  as  if  worthy  of  mass  reproduction  along\nwith  the  new  autonomous  technology. “Prediction”\nrhetoric fuses a model with the environment it is most\nsuccessful  in,  incentivizing  the  recreation  of  those\nconstrained environments to accompany propagation of\nthose  models[36].  This  conservative  push  for  the\nhegemonic standardization of human environments and\nbehaviors  is  especially  pernicious  when  deployed  in\nvalue sensitive domains like healthcare.⑥\nThese  dynamics  are  not  new.  The  profundity  of\nautomatic  manufacturing  has  long  been  a  matter  of\ntraining  audiences’ perspective  to  notice  certain\ncontributing  features  at  the  expense  of  others.  In  the\nnineteenth  century  London,  recounts  Stephanie  Dick,\nKarl\n Marx\n criticized\n Charles\n Babbage\n for\nanthropomorphising\n cogs\n and\n gears\n while\nsimultaneously failing to recognize the humanity of his\nown craftsmen[38，39]. When the term “automation” was\ncoined in 1948 by a Vice President at the Ford Motor\nCompany,  economists,  industrialists,  and  unionists\nseized  the  term—under  inconsistent  definitions—to\narticulate their own competing visions of society[40]. In\npresent day, Astra Taylor coins the term “fauxtomation”\nto  provide  a  more  accurate  characterization  of  the\nconcealed  chains  of  labor  that  sustain  contemporary\nmodes  of  digital  automation[41].  The  notion  of\n“autonomy” is a fiction concealed through the chronic\nunderreporting\n and/or\n dehumanization\n of\n living\ncontributors, argues Taylor. It is a horizon sought for but\nnever reached, like an asymptote stretching hopelessly\ntoward zero.\nHaving  briefly  considered  how  various  rhetorical\nmaneuvres distort civic imaginaries of automation both\npast and present, it is appropriate to ask what is, in fact,\nrequired to sustain pursuit of the endless horizon that is\nubiquitous digital automation. In the section that follows,\nthe author connects five labor trends usually treated in\nisolation: ghost work, technical debt, intellectual debt,\nthe labor of algorithmic critique, and various types of\nparticipatory  labor. The  author’s  aim  in  connecting\nthese threads is to emphasize the systematic nature in\nwhich different modes of digital automation extract and\nappropriate human labor simultaneously. The shadowy\npolitics  active  in  these  systems  are  perhaps  best\nrecognized in cases of piecemeal low-pay tasks, as in the\ncategory  of ghost  work.  Here,  industrial  actors\n⑤Personal  correspondence.  Thank  you  to  Momin  for  these  critical\nreadings.\n⑥Rhetoric of this type has already been found to obscure the flawed\nscientific foundations of such tools[37] and to legitimize pseudoscience in\nareas  like  criminal  justice,  human  resources,  credit  scoring  and  in\nmedicine.\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n339    \n \n\ndehumanize contingent workers to rationalize indecent\nconditions and maximize profits. Yet ghost work, on its\nown, is not fully illustrative of the broad spectrum of\nunderappreciated  impositions  that  digital  automation\nmakes  upon  human  labor.  The  author  explores  four\nadditional categories. As the author will show, technical\ndebt and intellectual debt normalize poor craftsmanship\nand  pseudoscience  in  the  development  of  digital\nproducts and services, thereby offsetting an unspecified\nburden  of  maintenance  and  repair  labor  onto  future\ngenerations. In a similar vein, the labor of critique and\nvarious modes of participatory labor help to sustain the\nacceptability  and  reliability  of  these  products  and\nservices  today.  One  wonders,  in  view  of  these  labor\ntrends: if software eats the world… who will digest it?\n3    Performing “The  AI  Revolution” — A\nTaxonomy of Contingent Labor\n3.1    Ghost work\nThe first category of labor to explore is ghost work, a\nphenomenon  that  reveals  the  banality  of  the  AI\nRevolution in practice. Gray and Suri coined the term in\n2019  to  illuminate  the  opaque  world  of  digital  on-\ndemand  task  fulfillment,  in  which  online  platforms\naggregate piecemeal low-pay tasks and repackage them\nas the outputs of automation[42]. Examples of ghost work\ninclude  rideshare  driving  and  the  search  and\ncategorization  of  micro  tasks  online.  These  platform\nsystems  emerged  from  decades  of  corporate  led\ncasualization  and  outsourcing,  which  normalized\nprecarious modes of employment[43]. Their existence is\ncritical  to  AI.  For  example,  Fei-Fei  Li’s  AI  team  at\nStanford University estimated in 2007 that it would take\nnineteen  years  of  undergraduate  labor  to  create\nImageNet, a large, gold-standard database of accurately\nlabeled  images.  Using  ghost  work,  the  team  accessed\n49000  human  contributors  from  167  countries  to\nproduce  the  database  in  two  and  a  half  years[42].\nImageNet  has  been  celebrated  as  a  benchmark  for\ncomputer vision algorithms; one that fueled a surge of\nmedia attention around AI techniques. Ghost workers, in\ncontrast, remain “the AI revolution’s unsung heroes”[42].\nAs the title suggests, ghost work is predicated on a\nstatus of tortured impermanence. Workers are hired as\nindependent  contractors  rather  than  employees.  This\nmakes  precise  figures  on  the  scale  and  nature  of  the\nphenomenon difficult to source. In 2017, the platform\neconomy  employed  an  estimated  70  million  workers\nglobally, with estimates for 2025 as high as 540 million\n(as cited in Ref. [44]). In the post-industrial economies\nof the US and UK, statistics indicate that ghost work is\nlarge and growing[42].⑦ Recent news around the poor\nperformance  of  Facebook,  Inc.’s  platform  content\nmoderation  algorithms  provides  a  glimpse  into  how\nghost work intersects with a well-funded and large-scale\nAI  project.  In  this  domain,  content  moderators  are\ncontracted  to  sort  inappropriate  content,  often  in\nconjunction  with  algorithmic  systems.  In  2009,\nFacebook was cited as paying twelve content moderators\nfor its one hundred and twenty million users[45]. By 2017,\nthis number allegedly grew to 4500 moderators. By 2019,\nit  reached  between  15000−20000  moderators  for\nFacebook’s  two  and  a  quarter  billion  users[46–48].⑧\nBetween\n 2009−2019\n then,\n Facebook’s\n content\nmoderator-to-user ratio grew approximately sixty times.\nGhost work is core to the AI Revolution. Facebook is\none of many corporations now intent on reconfiguring\ntheir business around AI and, consequently, precarious\nlabor. In late 2017, YouTube LLC. declared it would hire\n10000 content moderators for its 1.5−1.8 billion viewers,\nmore than double the number of its current 5000-person\nemployee base[49–51]. The most well-known ghost work\nplatform  is  Amazon.com,  Inc.’s  Mechanical  Turk  (or\nMTurk)  system,  which  provides  businesses  and\nconsumers with structured access to a marketplace of\nlow-cost and globally situated click workers. Between\n2005−2016,\n MTurk\n grew\n five\n times,\n from\napproximately  100000  to  500000[42].  Amazon  touts\nMTurk\n as \n“artificial\n artificial\n intelligence”.\n In\ncomparison, DefinedCrowd, one of many start-ups now\ncompeting  with  MTurk,  claims  eighty  employees  and\n211468  click  workers,  more  than  the  163800  people\nworking  in  oil  and  gas  extraction  across  the  United\nStates[52–54].⑨ Sector analysts claim that the marketplace\nfor third-party data labeling will grow six times by 2023\n⑦In 2016, twenty million workers were estimated to earn money via\nthe completion of on-demand tasks in the United States. Estimates hold\nthat analogous modes of semi “automation” could reconfigure 38 percent\nof US jobs by 2030. In developing countries, where much of ghost work\nis based, there are not even these figures.\n⑧In comparison, Facebook, Inc. reported 27705 employees in 2018.\n⑨At time of writing, competing outlets include: Alegion, Appen, Cape\nStart,  Click  Work,  Cloud  Factory,  Cloud  Sight,  Data  Pure,  Defined\nCrowd, Figure8, Cloud AutoML Vision, hCaptcha, Gengo, Gems, Hive,\niMerit, Labelbox, Lotus Quality Assurance, Micro Workers, MightyAI,\nOC Lavi, Playment, Reef, Scale, Superb, and TaskUs.\n    340\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\ninto  a  one  billion  dollar  marketplace,  with  other\nestimates reaching as high as five billion dollars[55–57].\nThe federal government in the United States has yet to\nacknowledge or set labor protections for ghost workers,\nwhose  fight  for  recognition  has  only  recently\nmaterialized into legislation in a handful of US states[58].\nThe  job  category “Content  Moderator” remains\nunrecognized by the Bureau of Labor Statistics; it is also\nabsent from the 21000 industry and 31000 occupational\ntitles  measured  by  the  US  Census[59, 60].  This  uneasy\nstatus, along with the frequent lack of a shared worksite\nor  uniform  job  title,  deepens  workers’ precarity  by\nadding friction to collective action and the protections it\nyields[42, 61].⑩\nAs in the era of Babbage, automation remains a matter\nof perspective. Regulators maintain a stubborn faith in\nnarratives  of  imminent  technological  transformation.\nDespite  the  troubling  size  and  character  of  the  ghost\nwork  phenomenon,  regulators  fail  to  confront  the\npossibility of its persistence, and thus fail to accept it as\na site for reform. A 2015 World Bank report on online\noutsourcing claimed that forecasting beyond 2020 was\n“highly speculative” due to the sector’s susceptibility to\nrapid technological change[62]. Gray and Suri challenge\nthis  idleness.  They  revisit  how  Microsoft  leveraged\nPermatemp contracts as far back as the 1980s[42]. “We\ncan not be sure if the ‘last mile’ of the journey toward full\nautomation will ever be completed,” they warn, adding\nthat, “the great paradox of automation is that the desire\nto eliminate human labor always generates new tasks for\nhumans”[42]. Even as technological boundaries change,\nworkers’ precarious status remains the same.\n3.2    Technical debt\nThe second labor category to assess is technical debt.\nTechnical debt is a form of delayed labor normalized\nthrough the acceptance of poor craftsmanship. In recent\nyears, the programming community has used the term to\ncharacterize  the  compounding  maintenance  costs\nassociated with poor design choices in program writing.\nWard  Cunningham  coined  the  term  in  1992,  stating,\n“Shipping first time code is like going into debt. A little\ndebt  speeds  development  so  long  as  it  is  paid  back\npromptly with a rewrite... The danger occurs when the\ndebt is not repaid. Every minute spent on not-quite-right\ncode counts as interest on that debt.”[63] Attempts at a\nframework for how to measure and monitor technical\ndebt remain theoretical at best[64–69]. Estimates hold that\nin  the  development  of  machine  learning  systems,\ntechnical debt accrues at a rate comparable to that of a\nhigh-interest  credit  card[70, 71].  Researchers  at  Google,\nInc.  warn  of  compounding “correction  cascades” in\nthese fragile models, meaning hidden feedback loops,\nsignal entanglements, and other technical challenges due\nto  what  they  describe  as  the  CACE  principle,  for\n“Changing Anything Changes Everything”[70].\nTomorrow’s workers, both expert and not, will inherit\nthe labor required to constantly repair and maintain this\ndelicate  infrastructure.  That  Facebook’s moderator-to-\nuser  ratio  increased  sixty-fold  between  2009−2019\nspeaks  to  the  scope  of  the  labor  force  required  to\nalgorithmically  oblige  evolving  norms,  customs,  and\nlaws  in  an  ever-increasing  number  of  overlapping\ndomains.  The  European  Commission,  by  analogy,\nemploys a full-time “Protocol Service” to keep its human\nleadership tuned to ever-shifting cultural and political\nnorms  in  national  and  regional  contexts  within  that\nboundary[72].⑪ As  the  CACE  principle  distills,  it  is\ndifficult to design AI systems that integrate a similarly\nfluid and complex set of concerns in real-time without\nhuman  support.  This  difficulty  rises  further  as\ndevelopers  attempt  to  model  three  dimensional\nenvironments. Sally Applin argues that software active\nin  an “autonomous” vehicle  must,  in  principle,\nseamlessly  and  unfailingly  update  across  shifting\nmunicipal,  city,  regional,  state/province,  national,  and\ninternational  borders[73].  This  software  would  also\npresumably register and integrate all relevant changes to\nthe  unfixed  physical  world  (e.g.,  downed  trees,  new\nconstruction,  etc.).  These  are  Sisyphean  undertakings.\nNarratives of an AI “revolution” belie the distribution of\nlabor  that  make  these  performances  of  autonomy\nfeasible at all.\n3.3    Intellectual debt\nAs  with  technical  debt, intellectual  debt is  a  form  of\ndelayed labor. Zittrain uses the term to characterize the\nmanner\n in\n which\n AI—and\n machine\n learning\nspecifically—serve\n to \n“increase\n our\n collective\nintellectual credit line” by providing atomized solutions\nto problems without any clear explanation of the causal\n⑩Gray  and  Suri  caution  that  no  laws  yet  govern  who  counts  as  an\n“employer” or “employee” in this domain. Roberts explains that content\nmoderators are also hired under the work titles “screener” or “community\nmanager”.\n⑪ They are responsible to oversee appropriate gifts, actions, attire, and\neven choice in songs for events.\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n341    \n \n\nmechanisms  involved[74].  In  principle,  access  to  this\ncredit  line  could  normalize  widespread  offsetting  of\ntheoretical explanation, where isolated decisions not to\nidentify  causal  mechanisms  accrue  into  a  network  of\nunchecked faith. Despite digital tools being the primary\ncause of this phenomenon, they are also held up as a\nprimary solution, which fuels a feedback loop toward\ntrained  dependency  and  the  centralization  of  power\namidst  cacophony. “A  world  of  knowledge  without\nunderstanding  becomes  a  world  without  discernible\ncause and effect, in which we grow dependent on our\ndigital concierges to tell us what to do and when”[74].\nInfluential figures in the American technology sector\nhave extolled this horizon. In a 2008 article entitled “The\nEnd of Theory: The Data Deluge Makes the Scientific\nMethod  Obsolete”,  Chris  Anderson,  chief  editor  of\nWired Magazine,  called  on  his  readers  to  reimagine\nscience  in  the  mold  of  Google’s  data-intensive\nadvertising  business.  He  celebrated  an  explanatory\nparadigm  in  which  approximations  to  scientific  truth\nfollow  from  correlations  found  in  massive  stores  of\nbehavioral  data,  rather  than  from  hypothesis  and\ntesting[75].  Also  in  2008,  Peter  Norvig,  Google’s\nresearch director, advocated to update the statisticians’\nmaxim “All models are wrong but some are useful”, to\n“All  models  are  wrong,  and  increasingly  you  can\nsucceed  without  them”[75].⑫ Weinberger,  in  a  2017\nop-ed for Wired, reaffirmed Anderson’s vision for a new\ndecade,  claiming, “Knowing  the  world  may  require\ngiving up on understanding it.”[76]\nIntellectual debt is not unique to machine learning. As\nZittrain  notes,  it  is  routinely  accepted  in  areas  of\nmedicine. The drug Modafinil, for example, is sold with\na disclaimer stating that its reasons for being effective\nare  unknown.  In  the  healthcare  sector,  however,  such\ndecisions  face  significant  regulatory  scrutiny  and\noversight. These burdens do not yet weigh as heavily on\nthe  tech  establishment.  Nor  is  mistrust  of  intellectual\ndebt  a  guarantee  that  such  heavy  restrictions  will\nnaturally emerge over time. In the 1980s, automated and\nsemi-automated  document  retrieval  systems  were  met\nwith  a  similar  mistrust[77].  Indeed,  the  embrace  of\ninstrumentalist  statistics  in  the  United  States  can  be\ntraced back to the late nineteenth century[78]. Without\nregulatory oversights in place to ensure genuine social\nprogress,  the  merits  of  which  have  already  been\noverlooked by existing AI principles[79], this trend will\nlikely burden tomorrow’s workers with the mountain of\ntedious  responsibilities  that  accompany  navigating  an\nexperimental turn away from the reliability of causation.\n3.4    Critique\nA fourth category of labor is critique. This category is\nbroad: it could feasibly encompass the labor required to\ninvestigate, identify, articulate, remedy, and/or reject the\ndegenerative  aspects  of “autonomous” systems.  This\ncharacterization  provides  a  wide  enough  berth  to\nencompass  the  work  of  theorists  like,  say,  Langdon\nWinner,  activists  like  those  in  the  Carceral  Tech\nResistance  Network,  and  those  whose  labor  sustains\nmovements\n of\n technological\n prohibition\n like\nNeo-Luddism.  The  ACM  FAccT  conference,  which\nhighlights engineering critiques of algorithmic systems,\noffers a window into the growth of at least one aspect of\nthis broad domain: since the conference was formed in\nthe late 2010s submissions have increased roughly two\ntimes annually, from 73 in 2018 to 290 in 2020.⑬ While\nthe growth of the AI industry is now regularly indexed\nby  top  universities  and  businesses[80],  the  growth  of\nso-called AI Ethics, a contentious title for the body of\ncriticism  (as  this  issue  conveys),  is  not  as  well\nunderstood.\nOf  note  is  that,  at  present,  much  of  this  labor  is\nsubsidized  by  the  public.  Of  the  seventy  sets  of\nrecommendations on trustworthy AI produced between\n2017−2019,  industry  produced  roughly  a  fifth  of\nsubmissions,  and  civil  society  and  governments,\ntogether, roughly a half[81, 82]. Principled proposals for\ncitizen juries and government-run data trusts extend, in\ntheir orientation, a similar expectation for the public to\npay for the failures of automation. Zittrain, for instance,\npositions academia, along with public libraries, as the\nnatural home for new modes of critique. He proposes that\ndatasets and algorithms that meet a sufficiently broad\nlevel  of  public  use  could  be  tested  by  researchers  to\nmitigate\n errors\n and\n vulnerabilities\n before\n they\ncompound.\nIf adopted in tandem with structural reforms to labor\nstandards, such proposals could bear fruit. Regrettably,\nmost  academic  labor  is  now  precarious  and  prone  to\nexploitation. 73 percent of faculty in American higher\neducation  institutions  work  part-time  or  otherwise  off\nthe tenure track, which provides little job security[83]. 60\n⑫ The first maxim is commonly attributed to the statistician George Box.\n⑬ ACM FAccT (formerly FAT*) stands for Association for Computing\nMachinery’s Conference on Fairness, Accountability, and Transparency\nin machine learning. Thank you to Christo Wilson for the figures.\n    342\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\npercent  of  higher  education  staff  in  UK  universities\nstruggle to make ends meet, with part-time and hourly\npaid teachers doing, on average, 45 percent of their work\nwithout  compensation[84].  Meanwhile,  in  early  2020,\nGoogle, Inc.’s parent company Alphabet Inc. became the\nfourth US technology company to reach a market cap of\nover a trillion dollars, following Apple Inc., Amazon,\nand  the  Microsoft  Corporation,  with  Facebook  now\nclose  behind.  The  normalization  of  un-  or  low-paid\ncritique thus threatens to normalize public responsibility\nfor avoidable harms ill-managed by industry.\n3.5    Participatory labor\nThe final category of labor the author assesses defies\nreduction  to  a  single  classification.  This  cluster\nencompasses  the  surfeit  of  unpaid  and  often\nunrecognized  tasks  and  offerings  undertaken  by\nconsumers,  users,  and  citizens  when  they  engage,\npassively and actively, with digital modes of automation.\nThis includes but is not limited to:\n•  Do-it-yourself  economies  (e.g.,  self-checkouts,\nself-check-ins,  self-booking  systems,  solve-it-yourself\ncustomer service);\n•  Open-source  software  economies  (e.g.,  pro-bono\nsupport of for-profit infrastructures);\n• Inference  economies  (e.g.,  proprietary  model\ntraining  via  auto-complete,  CAPTCHA  or  service\nfulfillment,  such  as  traffic  patterns  inferred  from  a\ndriver’s rideshare activity without fair compensation);\n• Digital  labor and informational  labor economies\n(e.g., online community management, such as the labor\nvolunteered by women of color in response to misogyny\nand racism on platform systems[85–87]);\n• Covert agency economies (e.g., the unacknowledged\nworkarounds  users  employ  to  modify  or  overcome\nlimited affordances in an algorithmic system[88]);\n• Dark pattern economies (e.g., design affordances that\ntrick a user into signing up for something they do not\nwant[89]);\n• Reputation maintenance economies (e.g., labor\nundertaken  to  maintain  one’s  standing  when  it  is\nimpacted  by  a  system’s  shortcomings  or  outright\nfailings[90]).\nThese diverse types of labor substantiate the “human\ninfrastructure” required to integrate digital automation\ninto daily life[91]. When deployed into structurally racist,\nsexist,  and  ableist  societies,  such  structures  tend  to\ndisproportionately  penalize  marginalized  groups[92, 93].\nThese  burdens  are  normalized  through  appeals  to  a\nneoliberal conception of consent, which assumes a base\nlevel “capacity  for  consent” that is  unsubstantiated  in\nreality[94].  When  collective  harms  are  framed  as  the\nresponsibility of each individual to navigate, only those\nwith power can afford to understand and overcome them.\nOthers face exile or deprivation when they try to resist.\nRobust taxonomies and lines of solidarity are needed to\nmap, connect, reform, or reject these entangled forms of\nlabor,  and  to  identify  the  toll  of  their  collective\nimpositions.  These  taxonomies  might  also  be  used  to\nbuild toward renumeration and reparation structures that\nrecognize  and  respond  to  each  party’s  contingent\ninputs[95, 96].\nThis  brief  survey  of ghost  work, technical  debt,\nintellectual debt, the labor of critique and participatory\nlabor highlights  the  significant  labor—both  in  the\npresent  and  in  the  future—that  organizations  depend\nupon to further the sales friendly mythos of AI. “We are\nall system administrators now, whether we realize it or\nnot,” write Dick and Volmer, who assess user-supplied\nmaintenance  in  relation  to  Microsoft’s  Windows\nplatform[97]. Much of what the author has covered here\nreduces to the extended labor economies of error and\nanomaly management. Given this common source, it is\nworth noting that the earliest pioneers of computing had\nnot anticipated that such labor would be necessary. They\nbelieved, wrongly, that computers would not have bugs.\nIn his autobiography, Maurice Wilkes, who developed\nEDSAC,  the  first  practical  use  stored-program  digital\ncomputer,  grappled  with  the  realization  that  a  good\nportion of the remainder of his life would be spent fixing\nerrors  in  his  own  code[98]. “Debugging  had  to  be\ndiscovered,” he recalled[98].⑭ In that era, and again with\nAI’s maturation, the messy and irreducible complexities\nof  material  reality  interrupt  the  principled  but  all  too\nabstract  aspirations  of  even  the  most  accomplished\ncomputing engineers.\nSince the development of EDSAC in the 1940s, the\nlabor  required  to  analyze,  design,  test,  debug,  and\ndevelop computer programs has become a recognized\nand deeply influential employment category known as,\n“Software  Development  and  Programming”.  In  the\nUnited States, it is one of the few employment categories\nto have emerged over the past century that employs a\nsignificant  proportion  of  the  population.  As  of  2010,\n⑭ Emphasis mine.\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n343    \n \n\nthere  were  thirty-five  million  computer  experts\nemployed  around  the  globe,  five  orders  of  magnitude\nmore than the initial group of scientists, engineers, and\nsupport  staff  working  in  the  midcentury[99].  In  2016,\n1.7 million were employed as software developers in the\nUS alone, with an estimated 300000 expected to join in\nthe  decade  to  come[100].  Low-cost  fauxtomation\nbroadens this labor network even further, reaching into\nexploitational  labor  categories  that  remain  to  be\ntaxonomized  and  acknowledged  in  the  way  that\nSoftware  Development  and  Programming  was  during\nand after the 1960s.\nRemaining to be seen, as responsibility for integrating\nthese errors translates slowly into a tree of discernible\njob  categories  (e.g.,  content  moderator,  quality\nassurance officer for driverless vehicles), is the extent to\nwhich  the  accruing  errors,  harms,  and  sacrifices\ninvolved in adopting these systems should be absorbed\nby an already over-leveraged public. These impositions\nare particularly difficult to characterize, as is their chain\nof responsibility[101–103].⑮ By analogy, in 2016 analysts\npositioned medical errors as the third leading cause of\ndeath  in  the  US[104, 105].  A  2018  report  estimates  that\nsoftware bugs killed more than one thousand patients per\nyear in the UK, with blame often passed on to doctors or\nnurses[90, 106, 107].⑯ A decade prior to the AI Revolution,\nthe US Commerce Department estimated that computer\nusers shared half the cost of the ＄22.2−59.5 billion lost\nannually  as  a  result  of  inadequate  software  testing\ninfrastructure[108].  These  sacrifices—lost  lives,  lost\nwages, lost recognition, lost opportunity, lost insights,\nand lost time—are substantial, and they will grow larger\nstill.\n4    Automation’s  Impositions:  A  Structural\nView\nThe author’s reason for connecting these threads is to\ndraw  attention  to  the  outcomes  of  neglecting  digital\nautomation’s  systemic  impositions,  which  entangle  in\nways  that  resist  simple  reduction.  Notions  of  labor\nprovide one lens into this change, as the prior sections\ndemonstrates. Yet labor, alone, is not the only way to\nunderstand  this  change.  As “predictive” technologies\nswell and rescript the logic of daily behaviors in healthcare,\neducation,  and  beyond,  competing  automated  systems\nwill  vie  for  citizens’ finite  time  and  encode  their\nbehavior  with  sophisticated  interactivity[109].  Without\nadequate  protections  in  place  to  monitor  and/or\nmeaningfully  prohibit  such  impositions,  low-cost\ndecision  systems  will  compound  the  public’s  digital\nobligations  and  slowly  (or  perhaps  rapidly)  sap  their\navailability  to  non-digital  systems.  Existing  terms  of\ncritique fail to capture the full character of this levy. Loss\nis  treated  in  financial  terms,  as  technical  debt  or\nintellectual  debt,  rather  than  a  more  profound  loss  of\npossibility.  Ruha  Benjamin  subverts  this  trend  when\nsaying, in relation to technology’s role in perpetuating\nanti-black logics, “Most people are forced to live inside\nsomeone  else’s  imagination” (Ref.  [110];  see  also,  in\nrelation  to  critique  of  normative  conceptions  of\ntime[111–113]).\nAn analogy is useful here as a means to characterize\nthe scale of this type of systemic phenomenon and the\nrelated  power  that  new  vocabulary  can  have  to\ncommunicate the complex reasons for an equally broad\nshift in course. The terms “global warming”, “climate\nchange”, and “Anthropocene” introduced the public to\nthe idea that local environmental harms, when taken in\naggregate, amounted to a fatal error in cultural logic, one\nthat  now  threatens  the  survival  of  our  societies,  with\nmarginalized  groups  around  the  globe  faced  with  the\nmost  dire  risks[14].  These  marquee  terms  speak  to  the\nsum-total harm caused by a complex web of operators\nwhose default perspective was to treat carbon emissions\nas  an  acceptable  negative  externality.  Emissions\nwere  considered  someone  else’s  problem—just  as\nautomation’s  impositions  are  now. “Global  warming”\nand related terms interrupt that base assumption. They\nilluminate  the  inescapable  hazards  for  everyone  that\naccompany unrestrained material consumption.\nThat  a  climate  crisis  loomed  in  the  late  twentieth\ncentury was clear to many long before the invention of\nthose  aforementioned  terms.  In  1955,  John  Von\nNeumann, whose logical architecture laid the blueprint\n⑮ Hobbyists,  historians,  and  risk  researchers  maintain  venues  to\ncatalogue and characterize the impact of poor error management in digital\nsystems, but no sophisticated repository captures a broad picture of their\naggregate toll, both economic and otherwise. For a moderated forum on\nthe  safety  and  security  of  computer  and  related  systems  see  the  Risk\nDigest. For a hobbyist’s collection of serious or novel bugs see Huckle.\nFor recent research on the role of error in the history of computing, see\nSIGCIS.\n⑯ Elish calls this phenomenon of blame “the moral crumple zone” of\nautomated  systems. “Just  as  the  crumple  zone  in  a  car  is  designed  to\nabsorb the force of impact in a crash, the human in a highly complex and\nautomated  system  may  become  simply  a  component—accidentally  or\nintentionally—that bears the brunt of the moral and legal responsibilities\nwhen the overall system malfunctions.”\n    344\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\nfor the digital era, opined about this inflection point in\nan article entitled, “Can We Survive Technology?”[114].\nDuring the first industrial revolution, he reasoned, “It\nwas possible to accommodate the major tensions created\nby technological progress. Now this safety mechanism\nis being sharply inhibited; literally and figuratively, we\nare running out of room. At long last, we begin to feel the\neffects of the finite, actual size of the earth in a critical\nway.”[114] \nJohn\n Von\n Neumann\n reckoned\n with\ntechnology’s  aggregate  material  implications.  In  this\narticle,  the  author  gestures  to  its  aggregate  temporal\nimplications and administrative obligations.\nAs with climate change, the localized impositions of,\nin this case, low-cost decision systems, are dismissed by\nsociety at large as uncontentious in the short-term. Only\nonce a ceiling asserts itself might this fleet of impositions\nbe seen as degenerative and systemic. Regrettably, as\nwith  climate  change,  the  existence  of  this  ceiling  is\ndifficult to convey to the broader public—until it is not.\nInstead  of  fires,  floods,  and  ecosystem  collapse,\ntemporal erosion may come to resemble, say, a latent\ndenial-of-service  (DoS)  attack  on  a  society’s  daily\ndecision-making abilities. A DoS attack is a cyber-attack\nin  which  a  communication  pathway  is  flooded  with\nenough superfluous requests to make it unavailable. By\nanalogy, a poverty of time, caused by the proliferation\nof digital obligations and delights (deployed at low-cost),\ncould hobble the public’s collective capacity to consider\nor even imagine alternative modes of social organization,\nsuch as those that do not center on data, efficiency, or\ntechnological  progress.  Wood  writes,  from  a  related\nvantage, “Surely  the  most  wretched  unfreedom  of  all\nwould be to lose the ability even to conceive of what it\nwould  be  like  to  have  the  freedom  we  lack,  and  so\ndismiss  even  the  aspiration  to  freedom,  as  something\nwicked and dangerous” (as cited in Ref. [92]).⑰\nThe difficultly of conveying this complex problem to\nthe  public  is  that  time  attrition  is  the  product  of  a\nthreatening system, not a threatening character or object.\nThe  harms  of  automation  in  oversupply  are  captured\nnarratively in a folktale about The Sorcerer’s Apprentice,\nin  which  an  enchanted  broom  causes  a  flood  by\ncollecting and pouring out too much water for its new,\ninexperienced master. In the West, however, advanced\nautomation  is  often  personified,  through  characters\nlike  the  Terminator,  rather  than  being  cast  as\ninfrastructural  or  distributed.  These  accounts  of\nautomation-as-individual,  also  captured  in  narratives\nabout job losses to robots, distort the public’s sensitivity\nto  both  the  banality  of  the  AI  Revolution  and  its\ncontingent harms. These stories convey a threat, but as\nwith  climate  change,  they  may  underemphasize  the\ndecentered nature of that threat.\nAdding  to  the  challenge  of  effective  public\ncommunication of a world awash with low-cost decision\nsystems is that skeuomorphs (i.e., features passed from\none technology to another related technology, like the\nfamiliar “click” of a smartphone’s shutter, which does\nnot in fact exist or make a sound) have so far failed to\npreserve traditional prohibitory functions, such as those\nthat  ritualized  natural  limits  and  restraint.  Digital\nautomation techniques know no opening hours, holiday\nclosures, snow days, sick days, periods of grievance, nor\neven  strict  regulatory  limits  on  their  collective\nimpositions. These are the technological manifestations\nof  the  neoliberal  attitudes  that  preceded  them.\nInterventions  in  privacy  law,  labor  law,  consumer\nprotections, and in the digital wellbeing movement add\nfriction to select intrusions, as epitomized by worker’s\nright to disconnect in France and Germany. Yet, as with\nclimate change, reform is still often cast in relation to the\nindividual, as if the potential to meter excess is somehow\nunavailable at the group level. This is a false restriction.\nCollective remedies, as always, remain viable.\nThe  irony  of  this  dilemma  is  that  automation,  at  a\ncertain level of proliferation, eventually fails to fulfill on\nits own celebrated purpose: to save time. The endless\nneed to integrate different types of automation draws the\nideal  toward  self-contradiction.  Each  new  act  of\ncoordination creates a new labor requirement. This labor\ncan be automated, but then that new automated system\nmust be integrated, too. This feedback loop introduces\nnew types of administrative obligations that, as the five\nlabor trends outlined above adequately suggests, can be\neasily  overlooked  by  those  who  benefit  from  their\npresence. As with climate change, marginalized peoples\nsuffer these harms first. In the long run, however, as for\nthe  Sorcerer’s  Apprentice,  a  world  awash  with  such\nobligations  would  presumably  ensnare  their  elite\ncreators as well by interweaving them in a society shaped\nby the same scripted logics they have used to control\n⑰ Although this may sound alarmist, the emergence of light and sound\npollution evidence how impacted parties can overlook what is lost amidst\npoor regulation. The author once met a child who had never seen the stars\ndue to light pollution in his neighborhood.\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n345    \n \n\nothers.  The  unrestrained  use  of  low-cost  decision\nsystems would amount to death by a thousand paper cuts\nfor a society callous to the compounding effects of such\ntemporal pollutants.\nBy  my  account,  the  prolific  use  of  digital  decision\nsystems, fueled by low marginal costs for proliferation\nand ascendant narratives of an imminent AI Revolution,\nmarks a new stage in complex debates over the societal\nrole(s) of automation. The characteristic the author seeks\nto  denaturalize  is  the  assumption  that  digital\nautomation—by its own logic—merits recognition as a\nself-evident  form  of  cultural  progress.  In  the  author’s\nview, critics of automation who entertain this horizon\n(e.g.,  automation-as-progress)  without  also  embracing\nacts  of  prohibition  assume  too  readily  that  technical\nsolutions  can  be  found—eventually—and  that,  as  a\nresult, solutions should be labored toward. This endless-\nhorizon narrative permits systemic harms to persist, with\nmarginalized peoples bearing the brunt of tomorrow’s\nmaintenance. Acts of prohibition create decision making\nsystems  in  which  knowledge  of  such  tools  is  not  a\nprerequisite.  With  these  spaces,  critics  endorse  a\ngrowing  distance  between  them  and  the  non-expert\ncommunities  they  often  aim  to  represent.  Stated\ndifferently, advanced automation techniques may need\nto be resisted wholesale if tech ethics experts are to avoid\nbecoming the technocrats they seek to displace.\n5    On Formalization and Its Alternatives\nOne way to resist the encroachment of digital automation\nis to question the methodologies that clear a path for its\nuse. One such methodology is the use of formalization to\ndescribe a system’s presumed nature. In his introduction\nto  Minsky’s  1961  paper, “Steps  Toward  Artificial\nIntelligence”, which laid out a research agenda for that\ndiscipline[115],  guest-editor  Harry  T.  Larson  wrote,\n“When  the  practitioner  has  overcome  his  fear  of  the\nmachine,  and  when  the  scientist  and  practitioner  are\ncommunicating, the attack is relentless. The scientific\nmind has found an un-formalised field, and it cannot rest\nuntil  it  identifies,  understands,  and  organizes  basic\nelements  of  the  field”[116].  Aspects  of  contemporary\nresearch on fairness, accountability and transparency in\nmachine  learning  echo  Larson’s  positivist  dogma  by\nimplying that highly formalized engineering techniques\nwill muster adequate solutions, rather than re-inscribing\nunderlying\n harms\n or\n reifying\n ever\n more\nbureaucratization[78, 117].⑱ Intervening  at  the  point  at\nwhich attempts are made to formalize a social system\nhelps  to  provide  citizens  the  derivative  economic  or\nadministrative relief needed to decide on a civic future\nfor themselves. Operating this far upstream avoids their\nbeing automatically ensnared in debate over a decision\ntool or technique that continues ad nauseam.\nTo  conclude,  the  author  fosters  a  metaphor  that  he\nhopes will lend subtly to dialogue about how to reshape\npositivist  inclinations  in  the  automation  space  into\nsomething less brutal and domineering. In sheet music—\nindeed,  in  music  composition  generally—special\nnotation  is  used  to  convey  the  role  of  a  deliberative\nsilence.  These  constructions  build  negative  space\npurposefully,  as  a  mode  of  art.  Without  rests,  music\nwould be cacophony. A recent wave of legal prohibitions\non  facial  recognition  technologies  across  American\ncities  substantiate  deliberative  restraint  in  response  to\nautomation.  US  communities  have  opted  to  preserve\nwhat  the  author  calls  an  algorithmic  silence:  the\npurposeful  exclusion  of  highly  abstract  algorithmic\nmethods  from  human  decision-making  environments.\nA silence of this type asserts that the value of such theory\nis worth more to the community when left unrealized.\nSuch  acts  of  prohibition  leave  room  to  incorporate\nholistic thinking about the myriad ways that advanced\ndecision  systems  re-shape  and  bear  upon  human\nsocieties.  Bans  and  moratoriums  hold  a  space  for\nreflection  on  the  systemic  burdens  disguised  by\ndisingenuous  rhetoric  and  incremental  reformism.  It\nprovides  the  proverbial “frog” with  the  interruption\nnecessary to recognize that it is in the proverbial “boiling\npot”.\nAnother  benefit  of  this  approach  to  resisting\nautomation’s  impositions  is  that  it  reconfigures  the\ndistribution of labor involved in shaping the roles that\ndigital  decision  systems  ought  to  have  in  society.\nAlgorithmic  silence  places  the  burden  of  proof  on\nenthusiasts, rather than on critics, to prove why formal\ntechniques  and  technological  artifacts  should  be\nwelcomed  into  a  social  system  at  all.  Revoking\nentitlements to public goodwill reveals the actual toll of\nintegrating  such  systems  into  daily  life.  Enthusiasts\nwould need to prove ahead of time how their automated\nsystems function without access to no-pay and low-pay\nsurrogates to clean up the mess caused by piloting poor\n⑱ Jones uses “data positivism” to describe this instrumentalist model of\ninduction, which seeks functions that fit to the data, rather than functions\nthat fit to a corresponding law of nature.\n    346\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\ntech craftsmanship on the public. This tempts reflection\non automation’s full bill (and distribution) of costs, the\nnature of which transcend financial levees.\nA  third  additional  benefit  to  the  normalization  of\nprohibitions  as  a  response  to  the  excesses  of  an\nautomated  society  is  that  this  path  would  limit\ncorporations’ access  to  public  coffers.  By  this  route,\nuniversities and colleges would be spared reduction to\nthe role of algorithmic custodians; history departments\nwould need to be shuttered so that a new generation of\nscholars can find and resolve software errors on behalf\nof  Facebook.  Algorithmic  silence  asserts  that  the\nsignificant and underappreciated costs of experimenting\nwith automation in the wild are paid for by the scientist\nand their patrons, rather than by the communities those\ngroups treat as laboratories. Those who champion the\nhorizon politics of automation, meaning the notion that\ndecency will come “eventually” and that the status quo\nmust  remain  until  then,  are  handed  responsibility  for\nthese “acceptable” burdens instead.\nThe motive power of a well-timed silence rings loudly.\nRest, some forget, is its own vehicle. The ambience it\ncreates  is  inhabitable  and  thus  sacred.  By  this  view,\nalgorithmic  silence  is  another  safe  road  to  progress.\nSahlins—aware that declines in leisure time have been\nnaturalized\n over\n centuries\n and\n can\n thus\n be\ndenaturalized—famously  memorializes  hunter  gathers\nas the original affluent society given that they toiled only\nthree to five hours a day[118]. Via a far more theory-laden\napproach,  Mejias  introduces  the  term “paranode” to\ncharacterize the multitudes that lie beyond the network\nlogics used in contemporary life to model and assimilate\nall  that  is  social.  A  paranode  is  a  place  beyond  the\nconceptual  limits  of  networks[119];  a  structural\ncomponent  that  alters  network  outcomes  but  from\noutside the network’s reach. An act of paranodality is\none of disidentification with the logic of that network.\nConsider  a  broken  URL,  RFID  (radio-frequency\nidentification)  blocker,  or  pirate  radio.  Each  exists\nslightly beyond the validation of the networks designed\nto subsume it. By rejecting the hegemony of advanced\ndecision\n systems,\n algorithmic\n silence\n fosters\nparanodality.\nThis account of paranodality from Mejias implies that\nthose  who  resist  disidentification  from  a  network  are\nmore radical than those who cause it. By my account,\nthose who reject algorithmic silence are tantamount to\nthose who reject silence in music. This willingness to\ncreate cacophony is deeply political, since it is often not\nthose  enthusiasts  who  suffer  its  hazards.  In  response,\nthese parties claim that acts of prohibition are antithetical\nto  progress.  This  shaky  platform  would  seek  to\nundermine  that  silence  is  in  fact  co-constitutive  of\nharmony;\n the\n two\n cannot\n exist\n apart.\n Writes\nmusicologist Zofia Lissa, “In its symbiosis with sonority,\nsilence  is  one  of  the  structural  elements  of  the  sound\nfabric, though in itself silence is the very negation of a\nsound fabric.”[120] Mejias, too, positions paranodality as\nintrinsic  to  a  networks’ structure.  An  attack  on\ndisidentification is thus an attack on the structure of the\nnetwork.\nAt root, musical notation and network structures can\nbe understood as metaphors for epistemic sovereignty in\nthe  face  of  technoscientific  hegemony.  Each  makes  a\nvirtue of noncompliance. Algorithmic silence, likewise,\nprovides  an  ambience  that  is,  at  first,  epistemically\nnonhierarchical. What comes from this state, however,\nis  unpromised.  At  best,  respite  from  the  perils  of\nubiquitous  AI  could  provide  a  window  into  a  way  of\nknowing  that  colonialism  has  forcefully  displaced;  an\noccasion, per Nelson, to witness that “the human is not\na problem to move beyond”[121]. Silence for the sake of\nsilence constrains positivist technoscience by asserting\narbitrary\n limits\n to\n its\n valorization\n of\n hyper\nrationalization and administration. It is an invitation to\ntechnocrats to stand outside of that rationalist bubble; to\ngrieve,  instead,  the  presumptuous  fictions  of  progress\nand futurity. A chorus of algorithmic silences, the author\nwagers, could help to break the spell of AI by building\nharmony between its countless alternatives. Proponents\nof such techniques would arrive, instead, into the present,\noccupied as it is by the durability of imperialism[122] and\nthe permanence of pollution[123]. Here, a different set of\nexperts call the tune.\nThe  growing  ubiquity  of  advanced  low-cost\nautomation techniques has made strange bedfellows of\nthose who seek the dangers of unrestrained automation.\nMilitary  researchers,  both  in  the  US  and  India,  have\nrecently  framed  contemporary  information  flows  as  a\ngrowing impediment to their ideological aims rather than\na  cherished  resource[124, 125]. “The  desire  to  have\nmaximum  inputs  for  decision  making  is  a  tempting\nproposition  but  will  have  to  be  tempered  with  the\nnecessity of giving a decision in time. As time pressures\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n347    \n \n\nbecome  more  acute,  we  may  well  end  up  with\n‘information  decoherence’.”[126] This  is  a  remarkable\noutcome given that the US military played a definitive\nrole  in  pioneering  modern  information  management\ntechniques  via  the  development  of  systems  analysis,\noperations research, game theory, and digital computing\nand  digital  networking  generally[127, 128].  For  military\nresearchers  to  insinuate  the  need  to  de-escalate\ninformation management is telling of the hazardous path\ndependencies of unrestrained automation. It speaks to a\ncarrying capacity, or ceiling, after which even hardline\nproponents  see  diminishing  returns  from  the  logics\nbehind mass automation. Cowan, similarly, debunks the\npopular  myth  that  American  domestic  technologies\nsaved domestic laborers time through automation. In fact,\nCowan  shows,  such  tools  introduced  more  work  for\nthese laborers by upsetting the equitable models of labor\ndistribution assumed in prior centuries[93].\nIn raising these critiques, and the unique possibilities\nafforded by the thoughtful use of prohibition amidst the\nrapid development of low-cost automated systems, the\nauthor seeks to emphasize the search for harmony in the\ndevelopment of digital automation regimes, particularly\nin the value-sensitive realm of democratic governance.\nIt bears mention at this juncture that silence, on its own,\nis not harmonious, although the experience of it may be\npleasing at times. Harmony, by definition, requires the\nthoughtful combination of positive expressions and their\nopposites, rather than simply the preservation of a dead\nsignal  or  cacophony.  The  possibilities  for  proverbial\nharmony, in this regard, are vast[129]. In their 2020 book\nMeaningful  Inefficiencies,  for  instance,  Gordon  and\nMugar  argue  that  public  trust  in  civic  organizations\nrequires  that  such  systems  are  designed not to  be\nefficient[130].\nIn consideration of what precise balance to strike, it is\nworth  considering  that  contemporary  debates  over\nacceptable  levels  of  formalization  and  algorithmic\nmanagement in a given context mirror a longstanding\ndilemma  in  American  political  theory  about  the\nappropriate balance between democratic representation\nand the agents who administer it. Herein lies a thorny\ntrade-off: administrative decision makers in large-scale\ndemocracies, such as monetary experts, hold both the\nspecialist  knowledge  to  make  an  informed  judgement\nand a capricious discretion over outcomes that no elected\nrepresentative  could  ever  hope  to  oversee.  Sheer\nadministrative\n complexity\n stifles\n democratic\naccountability  by  furnishing  these  experts  with\ndeterminative  rather  than  consultative  capabilities[131].\nSince  there  are  too  many  experts  for  any  elected\nrepresentative to ever manage in these large systems, this\ngroup of specialists effectively skirt traditional modes of\ncivic accountability.\nThe  AI “revolution” teases  this  dilemma  into  new\nterritory.  As  in  industry,  political  administrators  are\neasily  tempted  toward  the  presumed  incentives  of\nfauxtomation—efficiency, self-regulation, cost savings,\netc.[79] This temptation leads them headlong toward a\nmurky  accounting  of  the  contingent  labor  required  to\naccomplish desired outcomes. The introduction of yet\nanother layer of abstraction into state administration puts\nyet  more  distance  between  the  public  and  their\nrepresentatives[132, 133].⑲ Worse, Kafkaesque modes of\nadministrative  accountability  fatigue  the  public’s\nsensitivity to their civic entitlements. “Decision-making\nstructures  become  systems  of  domination”,  warn\nDowney and Simons about the failings of contemporary\npre-automated\n democratic\n procedures, \n“Nobody\nappears  to  have  responsibility  for  the  reproduction  of\ninjustice  over  time:  not  elected  representatives,\ndelegated agencies or private corporations”[131]. As in\nthe  American  and  Indian  military  contexts  referenced\nabove, complexity has exhausted the system’s potential\nfor capacity.\nThe promise (or specter) of automation is that it can\nresolve complex administrative tradeoffs in a seemingly\nrational  fashion.  Regrettably,  as  demonstrated  in  the\nopening to this article, disingenuous rhetoric around the\ntrue  capabilities  of  such  techniques  distorts  a  clear\nappraisal of their worth. Confusion over this accounting\nbecomes,  in  the  process,  its  own  powerful  form  of\ndeflection. When questioned by the US Congress and\nSenate\n about\n Facebook’s\n content\n moderation\narchitecture  in  2018,  for  instance,  Mark  Zuckerberg\nmade  frequent  appeals  to  the  efficacy  of “artificial\nintelligence” to solve known problems[134], despite the\n⑲ Lanius  introduces  how  statistical  technologies  distort  expectations\nabout evidence amongst black and white communities. Hill shows how\naccess to evidence from sophisticated analytical tools privileges those in\nthe  criminal  justice  system  but  penalizes  marginalized  individuals.\nLiterature on the digital divide substantiates other disparities caused by\nthe politics of digitization, such as the fact that the majority of content on\nthe  internet  is  in  English,  which  alienates  people  who  speak  other\nlanguages,  and  that  this  content  is  most  often  developed  for  haptic\ninterfaces on computers and smartphones, which alienates people with\ndisabilities.\n    348\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\nefficacy of such methods remaining untested. From this\nperspective, Zuckerberg’s call for patience is in fact a\ncall for the public to subsidize the status quo; to absorb\nthe costs of his failure indefinitely in the hopes of an\nimminent technological solution—a simple expression\nof horizon politics in action. In the process, technical and\nintellectual  debts  continue  to  accrue,  along  with  the\nsocial costs of abuse, harassment, and misinformation\nthat traffic on his channels.\nWhile Zuckerberg and Facebook can, for the moment,\nsustain this violent charade, it is less clear that a genuine\nlarge-scale democracy can do so as well. Consider the\nright to a public defender. This right is made trivial if that\ndefender  is  too  overburdened  to  adequately  fulfil  the\nduty, as is now the case in areas in the United States[135].\nIn  this  instance,  a  failure  in  due  process  negates  the\npossibility  to  assert  hard-won  democratic  principles;\njustice delayed is justice denied. While new technologies\nare  held  up  as  solutions  as  such  problems,  their  total\ncompounded administrative costs remain unclear at best,\nas the author has argued. At worst, sophisticated digital\narchitecture is a known hazard to accountability. In an\nindicative case-study, Dick and Volmar capture what is\ncalled “dependency  hell” in  the  use  of  Microsoft’s\ninfrastructure[97].  In  this  hell,  individual  components\nfunction  precisely  as  intended  but  systemic  failure\nresults, nonetheless. “Who ultimately ‘owns’ a failure in\na system like this?” they ask, “More importantly, who\nfixes it?”[97]\nAlgorithmic silence tempts these obscure politics into\nthe light. The term connects acts of restraint that might\notherwise be read as dissimilar. If ubiquitous automation\nis liable for its burdens and not just it promises, then bans\non facial recognition technologies can be understood as\nof a kind with, say, the EU’s Working Time Directive\n(2003/88/EC)  and  Right  to  Disconnect,  which  set  out\nminimum requirements for rest in relation to telework.\nEach  intervention  imposes  regulatory  limits  on  the\nprospect of algorithmic optimization. Whether or not the\nhuman workplace or the human face is pliable to such\ntechniques is made moot. Regulators, following public\npressure, preserve the relatively intimate (if imperfect)\nmodes of accountability permitted by human-to-human\nscale interaction.\nThe  need  to  protect  time  and  space  from  the  AI\nRevolution  echoes  in  literature  on  AI  and  medicine.\nTopol  speculates  that  the  core  benefit  of  advanced\ndecision systems will be time savings gained by experts\nmoving away from automation[136]. US doctors currently\nface a degenerative cycle; more than 50 percent suffer\nburnout  and  25  percent  suffer  depression—pressures\nthat  beget  additional  medical  errors  and  strain,  which\nexacerbate suffering and can lead to suicide[136]. Topol\npositions  protections  on  time  as  a  promising  line  of\nresolution to this feedback loop, not just for clinician’s\nwork/life balance, but also for patient outcomes. A study\nof  60000  caregiver  visits  identified  the  provision  of\nadditional  patient-to-expert  time  as  the  most  reliable\npath to decreasing hospital readmissions, as other studies\nsupport[136].⑳\nIn medicine, human-to-human accountability regimes\nled to improved outcomes. Summarizing one of several\nsuch studies, Topol writes, “Taking the computer out of\nthe  exam  room  and  supporting  doctors  with  human\nmedical assistants led to a striking reduction in physician\nburnout,  from  53  percent  to  13  percent.”[136] This\nsolution  is  not  new.  On  the  contrary,  Topol’s  thesis\nechoes the sentiment of William Osler, co-founder of\nJohn Hopkins Hospital, who wrote in 1895, “A sick man\ncannot be satisfactorily examined in less than half an\nhour.”[136] Indra Joshi, Digital Health and AI Clinical\nLead  for  NHS  England,  agrees.  Joshi  describes  the\nexperience of waiting in the journey for treatment—for\nresults, a specialist, or a bed—not as a process, but as a\nstate  of  being, “A  feeling  of  being  neither  here  nor\nthere”[137]. This is the same torturous state of being that\nZuckerberg, Facebook, and other influential proponents\nof  ubiquitous  digital  automation  advocate  for  and\nenforce  through  the  tact  they  take  to  technological\ndevelopment[138]. Just hold on, the story goes, we are\nalmost there.\nTo  interrupt  this  rhetoric,  critics  must  adequately\ndiagnose  its  charm.  Crucially,  Zuckerberg  and  peers\nassume no finite constraints on time. This is their faux\nreality. Such appeals benefit from at least three levels of\nillocution[139] (Garvey characterizes the history of AI as\na string of illocutionary acts or promises). Within AI, as\nthe author has introduced, technical terms like “predict”\ndescribe a desired end state, not a procedure in time. The\nterm “artificial intelligence” is an exemplar of this trend;\na vague yet seemingly prophetic sign of a movement yet\nto come. Reckless critique overlooks this folly. It accepts\n⑳ Giving  a  patient  an  additional  minute  with  an  expert  reduced  their\nprobability of being readmitted by 18%, or 13% in the case of nurses. A\nseparate  study  found  that  additional  time  with  experts  reduced\nhospitalizations by twenty percent.\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n349    \n \n\nAI rhetoric without scrutiny and diverts attention from\nwishful mnemonics to “wishful worries”, Brock’s term\nfor “problems that it would be nice to have, in contrast\nto the actual agonies of the present”[140]. Meanwhile, a\nfleet  of  human  contributions,  both  paid  and  unpaid,\nperform, unknowingly and knowingly, a broad array of\ndiscreet  tasks  that,  if  overlooked  as  systemic  and\nconnected,  might  lend  AI  an  air  of  legitimacy  and\nimminence. Like the Church-Turing Thesis, AI provides\na tantalizing and multifaceted escape from the existence\nof time and space, but only for a privileged few.\nTo interrupt this wishful cycle, critics must situate AI\nwithin the post digital era, meaning the period in which,\n“The  revolutionary  phase  of  the  information  age  has\nsurely  passed”[141].  Cut  off  from  the  ability  to  escape\ntime or make vague appeals to imminent transformation,\nAI  advocates  would  be  pressed  to  justify  their\ninterventions  on  alternative  grounds.  One  option  the\nauthor has championed here is to audit the labor required\nto develop, deploy, maintain, critique, and use such tools.\nIf this was a norm, a clearer picture of AI’s proffered\nimpact  on  labor  could  begin  to  emerge.  More  likely,\nexpert-led calls for algorithmic accountability would be\nmet with a charge akin to “Luddite!”. The author, for one,\nfears that the history of Luddism is too disanalogous to\ntoday to accommodate the paradoxes of contemporary\nautomation,  replete  as  it  is  with  the  compounding\nintersectional realities of gender, race, class, coloniality,\nand\n globalization[21, 142].\n Digital\n tools\n embody\nopportunities\n and\n risk\n across\n many\n layers\nsimultaneously; their treatment deserves more nuance.\nEnter  algorithmic  silence.  If  unburdened  by  the\naccumulated  labor  required  to  perform  the  AI\nRevolution  ad  infinitum,  citizens  would  gain  the\nincremental derivative economic or administrative relief\nneeded to decide on a civic future for themselves. Their\nreliance on technocrats posturing as AI ethicists would\nbe diminished in proportion to the nonproliferation of\nfaux automation systems, since—in principle—the civic\nspace  in  which  they  operate  would  be  relatively  less\ninfluenced  by  unrestricted  impositions  on  their  finite\ntime.  Algorithmic  silence  provides  a  content  agnostic\nframework for solidarity across settings, be it restraint\nfor  workers,  consumers,  parents,  prisoners,  women,\nyouth,  etc.  The  prospect  of  solidarity  across  these\ncontexts  is,  in  principle,  broad  enough  to  answer\northogonal  pressures  from  data  science.  Ribes,  for\nexample, shows how the term “domain” presupposes a\nrole for computing in areas of life not yet conscripted into\nsuch  methods[143–145].  For  solidarity  to  emerge  across\ncountercultures,  interventions  must  evidence  a  larger\nmovement,  whatever  it  may  be  called.  Algorithmic\nsilence is a step toward that end.\nAs critics mobilize against automation’s harms, they\nmust  confront  the  possibility  of  achieving  a  Pyrrhic\nvictory.  Clearly  articulated  ethical  principles  would\nindeed be a positive result, but their enshrinement into\nlaw remains only half the battle (see also Ref. [146], this\nissue).  Commitments  to  due  process  must  also  be\nconsidered,  articulated,  enacted,  and  enforced,  or\nhard-won principles will be a farce, as is witnessed with\noverworked  public  defenders  and  caregivers.  The\npolitics of procedure and promise of automation merit\ndeep  contemplation  in  a  moment  when  indigenous\nleaders  and  scholars  in  particular  reaffirm  ancient\nnotions of accountability to place, planet, and people that\nstand to exceed the shortcomings of liberal democratic\nimaginaries[147–149].  Transformation  is  possible,  but\nlikely not via appeasement. By continuing to normalize\nthe  presumption  that  automation  can  be  refined  and\nimproved—that  satisfactory  tech  ethics  can  be\narticulated—those\n in\n the\n realm\n of\n automation\ndevelopment and critique point to a loadstar that either\nmisguides them, or makes real a system of politics that,\nin  fact,  they  endorse  but  have  not  yet  been  held\naccountable for.\n6    Conclusion\nArthur C. Clarke’s popular Third Law About the Future\nboasts, “Any  sufficiently  advanced  technology  is\nindistinguishable from magic.”[150] This literary “law” is\noften  cited  in  salesmanship  that  surrounds  the  AI\nRevolution. It is used to paint a boundary between those\nwho create technology and those who merely witness it.\nIn this article, the author has questioned that boundary\nby exploring the ways in which groups who experience\nthe “magic” of  digital  automation  is  often  made  into\nco-managers  of  that  performance  via  ghost  work,\ntechnical  debt,  intellectual  debt,  the  labor  of  critique,\nparticipatory  labor,  or  some  combination  therein.  The\nauthor  questions  how  the  experience  of  advanced\ntechnologies  changes  as  onlookers  participate  in  an\nincreasing number of performances simultaneously, day\nafter day, week after week, without structured relief to\ntheir expected vigilance. Clarke’s “law” claims to speak\n    350\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\nto the performative aspects of a new technology. Yet,\ntellingly,  it  speaks  not  at  all  to  experience  of  those\nperformers whose labor substantiates the act.\nGiven  the  need  for  public  awareness  around  the\nstructural impositions caused by an automated society,\nas  well  as  the  risk  of  paternalism  that  accompanies\nunchecked faith in a technocratic expert-led resistance,\nit  is  worthwhile  to  question  which  vocabularies\nadequately capture the character of the phenomenon the\nauthor has engaged herein. Algorithmic silence resists\nthe  tradition  of  highly  formalized  and  positivist\narticulations  of  social  dynamics  that  prefigure  and\ninform contemporary forms of digital automation. The\nconcept, instead, reifies the virtues of deliberate relief\nfrom  these  types  of  knowing.  At  best,  it  affords\ncollective freedoms from the onslaught of formalisms\nand encoded behaviors that are sure to accompany the\nprolific use of low-cost automation. Algorithmic silence\ntreats rest as its own dignified vehicle to progress—one\nthat  could  surface  lines  of  solidarity  across  otherwise\ndivisive  relationships  changed  by  a  rising  torrent  of\ndiscrete obligations. With each passing day, the global\ncommunity  awakens  to  the  reality  that,  as  Dick  and\nVolmar suggest, we are all system administers now (or\nwill  be,  eventually).  Servicing  the  need  for  spaces\nuntouched by algorithmic enclosure would allow civic\ncommunities the distance to reflect on and shape this\nunfolding phenomenon for themselves—or at least see\nthat it is occurring.\nActs of wholesale prohibition such as that which the\nauthor distills as algorithmic silence tempt reflection on\nthe  ethos  of  entitlement  that  sustains  contemporary\nmyths  about  digital  automation  and  a  looming  AI\nRevolution. If judged in relation to time and space, as\nopposed to the timelessness of an endless horizons, AI\nfits  more  neatly  into  the  post-digital  era  in  which  no\nsignificant change to the existing social order is to be\nexpected.  At  a  superficial  level,  this  reappraisal  of\nrhetoric could help to steer AI development in line with\nexisting\n traditions\n of\n de-escalation,\n such\n as\ndecomputerization and degrowth, although the nuances\nof  this  proposal  merit  closer  consideration  (since\nalgorithmic silence could also be abused). Those who\naddress  the  environmental  toll  of  machine  learning\nsystems,  however,  have  made  similar  calls  for\ndecomputerization[151, 152]. Such acts of relief color the\nedges  of  what  could  become  a  powerful  deindustrial\nrevolution: a transformation equal in magnitude to the\nfabled AI Revolution but led, instead, by communities\nrather than corporate needs.\nAcknowledgment\nSpecial  thanks  to  Sarah  T.  Hamid,  Sarah  Dillon,\nStephanie  Dick,  Richard  Staley,  Helen  Anne  Curry,\nMomin  M.  Malik,  Mustafa  Ali,  Mary  Gray,  Sean\nMcDonald,  William  Lazonick,  Ernesto  Oyarbide-\nMagaña, Ben Green, and attendees of the 2020 Istanbul\nPrivacy Symposium.\nReferences\n Y.  Katz,  Manufacturing  an  artificial  intelligence\nrevolution, SSRN  Electronic  Journal,  doi: 10.2139/ssrn.\n3078224.\n[1]\n J.  S.  Brennan,  A.  Schulz,  P.  N.  Howard,  and  R.  K.\nNielsen, Industry, experts, or industry experts? Academic\nsourcing  in  news  coverage  of  AI’,  Reuters  Institute  for\nthe  Study  of  Journalism, https://reutersinstitute.politics.\nox.ac.uk/sites/default/files/2019-12/Brennen_Academic_\nSourcing_in_News_Coverage_of_AI_FINAL.pdf, 2019.\n[2]\n J. Penn, Inventingintelligence: On the history of complex\ninformation  processing  and  artificial  intelligence  in  the\nUnited  States  in  the  Mid-Twentieth  Century,  PhD\nDissertation, University of Cambridge, Cambridge, UK,\n2020.\n[3]\n M.  I.  Ganesh  and  S.  Lohmüller,  #5  Spectres  of  AI,\nSpheres:  Journal  for  Digital  Cultures, https://spheres-\njournal.org/contribution/5-spectres-of-ai/, 2019.\n[4]\n S.  Cave  and  K.  Dihal,  Ancient  dreams  of  intelligent\nmachines:  3,  000  years  of  robots, Nature,  vol. 559,\nno. 7715, pp. 473–475, 2018.\n[5]\n S. Cave, C. Craig, K. Dihal, S. Dillon, J. Montgomery, B.\nSingler, and L. Taylor, Portrayals and perceptions of AI\nand\n why\n they\n matter,\n The\n Royal\n Society,\nhttps://royalsociety.org/-/media/policy/projects/ai-\nnarratives/AI-narratives-workshop-findings.pdf, 2018.\n[6]\n S. Cave, K. Dihal, and S. Dillon, eds., AI Narratives: A\nHistory  of  Imaginative  Thinking  About  Intelligent\nMachines.  New  York,  NY,  USA:  Oxford  University\nPress, 2020.\n[7]\n J.  Dinerstein,  Technology  and  its  discontents:  On  the\nverge  of  the  posthuman, American  Quarterly,  vol. 58,\nno. 3, pp. 569–595, 2006.\n[8]\n P.  Mirowski, Machine  Dreams:  Economics  Becomes  a\nCyborg Science. Cambridge, UK: Cambridge University\nPress, 2002.\n[9]\n R. Scott, Blade Runner, Movie, Jun. 25, 1982.\n[10]\n M.  Hicks, Programmed  Inequality:  How  Britain\nDiscarded  Women  Technologists  and  Lost  Its  Edge  in\nComputing. Cambridge, MA, USA: MIT Press, 2017.\n[11]\n J. Penn, Programmed inequality: How Britain discarded\nwomen technologists and lost its edge in computing, H-\nSci-Med-Tech,  no.  H-Net  Reviews, http://www.h-\nnet.org/reviews/showrev.php?id=52804, 2019.\n[12]\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n351    \n \n\n C.  Hauskeller  and  C.  Hick, Captivating  Technology:\nRace,\n Carceral\n Technoscience,\n and\n Liberatory\nImagination  in  Everyday  Life.  Durham,  UK:  Duke\nUniversity Press, 2019.\n[13]\n R. Benjamin, Race After Technology: Abolitionist Tools\nFor The New Jim Code. Cambridge, UK: Polity, 2019.\n[14]\n V.  Eubanks, Automating  Inequality:  How  High-Tech\nTools  Profile,  Police,  and  Punish  The  Poor,  First\nEdition. New York, NY, USA: St. Martin’s Press, 2017.\n[15]\n M. Broussard, Artificial Unintelligence: How Computers\nMisunderstand  The  World.  Cambridge,  MA,  USA:  The\nMIT Press, 2018.\n[16]\n C.  D’Ignazio  and  L.  F.  Klein, Data  Feminism.\nCambridge, MA, USA: The MIT Press, 2020.\n[17]\n J.\n Buolamwini\n and\n T.\n Gebru,\n Gender\n shades:\nIntersectional accuracy disparities in commercial gender\nclassification, Proceedings  of  the  1st  Conference  on\nFairness, Accountability  and  Transparency,  vol. 81,\npp. 77–91, 2018.\n[18]\n O.  Keyes,  The  misgendering  machines:  Trans/HCI\nimplications\n of\n automatic\n gender\n recognition,\nProceedings\n of\n the\n ACM\n on\n Human-Computer\nInteraction, vol. 2, no. 88, pp. 1–22, 2018.\n[19]\n K.  M.  Miltner,  Girls  who  coded:  Gender  in  twentieth\ncentury U. K. and U. S. computing, Science, Technology,\n& Human Values, doi: 10.1177/0162243918770287.\n[20]\n S. M. Ali, A brief introduction to decolonial computing,\nXRDS: Crossroads, The  ACM  Magazine  for  Students,\nvol. 22, no. 4, pp. 16–21, 2016.\n[21]\n B.\n Green,\n The\n contestation\n of\n tech\n ethics:\n A\nsociotechnical approach to technology ethics in practice,\nJournal  of  Social  Computing,  doi: 10.23919/JSC.2021.\n0018.\n[22]\n D.  McDermott,  Artificial  intelligence  meets  natural\nstupidity, ACM SIGART Bulletin, no. 57, pp. 4–9, 1976.\n[23]\n M.  M.  Malik,  A  hierarchy  of  limitations  in  machine\nlearning: Data biases and the social sciences, presented at\nthe  Ministry  of  Science,  Innovation,  Technology  and\nUniversity of Spain, Virtual, 2020.\n[24]\n Z.  C.  Lipton  and  J.  Steinhardt,  Troubling  trends  in\nmachine  learning  scholarship, Queue,  vol. 17,  no. 1,\npp. 45–77, 2019.\n[25]\n S. C. Garvey, The “general problem solver” doesn’t exist:\nMortimer taube & the art of AI criticism, presented at the\nSociety  for  the  History  of  Technology,  St.  Louis,  MO,\nUSA, 2018.\n[26]\n H. L. Dreyfus, What Computers Can’t Do: A Critique of\nArtificial Reason, 1st ed. New York, NY, USA: Harper &\nRow, 1972.\n[27]\n H.  L.  Dreyfus, What  Computers  Still  Can’t  Do:  A\nCritique  of  Artificial  Reason.  Cambridge,  MA,  USA:\nMIT Press, 1992.\n[28]\n H.  L.  Dreyfus,  Why  heideggerian  AI  failed  and  how\nfixing  it  would  require  making  it  more  heideggeria,  in\nThe  Mechanical  Mind  in  History,  P.  Husbands,  O.\nHolland,  and  M.  Wheeler,  eds.  Cambridge,  MA,  USA:\nMIT Press, 2008, pp. 331–372.\n[29]\n A.  Agrawal,  J.  Gans,  and  A.  Goldfarb, Prediction\n[30]\nMachines:\n The\n Simple\n Economics\n of\n Artificial\nIntelligence.  Boston,  MA,  USA:  Harvard  Business\nReview Press, 2018.\n D.  Gayo-Avello, “I  wanted  to  predict  elections  with\nTwitter and all I got was this lousy paper” — A balanced\nsurvey  on  election  prediction  using  Twitter  data,  arXiv\npreprint arXiv: 1204.6441, 2019.\n[31]\n M.  Mitchell,  B.  Agüera  y  Arca,  and  A.  Todorov,  Do\nalgorithms  reveal  sexual  orientation  or  just  expose  our\nstereotypes? https://medium.com/@blaisea/do-algorithms-\nreveal-sexual-orientation-or-just-expose-our-stereotypes-\nd998fafdf477, 2019.\n[32]\n M.  Malik,  Can  algorithms  themselves  be  biased,\nhttps://medium.com/berkman-klein-center/can-\nalgorithms-themselves-be-biased-cffecbf2302c, 2019.\n[33]\n J. C. Scott, Seeing Like a State: How Certain Schemes to\nImprove the Human Condition Have Failed. New Haven,\nCT, USA: Yale University Press, 2008.\n[34]\n G.  C.  Bowker  and  S.  L.  Star, Sorting  Things  Out:\nClassification  and  Its  Consequences.  Cambridge,  MA,\nUSA: MIT Press, 1999.\n[35]\n N. Ensmenger, Chess, cars, and cognition: How problem\nchoice shapes a discipline, Pavilion Room, Hughes Hall,\nCambridge, http://lcfi.ac.uk/events/origin-myths-artificial-\nintelligence-histories-tec/, 2018.\n[36]\n K. Crawford, R. Dobbe, T. Dryer, G. Fried, B. Green, E.\nKaziunas, A. Kak, V. Mathur, E. McElroy, A. N. Sánchez,\net al., AI now 2019 report, AI Now Institute, New York,\nhttps://ainowinstitute.org/AI_Now_2019_Report.html,\n2019.\n[37]\n E. Antonova, Y. Chaudhary, S. Dick, and S. McDonald,\nNext leaders: Bold visions for the future of AI, presented\nat  the  CogX  2019,  London, https://www.youtube.com/\nwatch?v=9ifNuQCdvJI, 2019.\n[38]\n S. Schaffer, Babbage’s intelligence: Calculating engines\nand  the  factory  system, Critical  Inquiry,  vol. 21,  no. 1,\npp. 203–227, 1994.\n[39]\n M.  W.  Marshall, “Automation” today  and  in  1662,\nAmerican Speech, vol. 32, no. 2, pp. 149–151, 1957.\n[40]\n A.  Taylor,  The  automation  charade, Logic  Magazine,\nhttps://logicmag.io/05-the-automation-charade/, 2018.\n[41]\n M. L. Gray and S. Suri, Ghost Work: How to Stop Silicon\nValley From Building A New Global Underclass. Boston,\nMA, USA: Houghton Mifflin Harcourt, 2019.\n[42]\n D. Weil, The Fissured Workplace: Why Work Became so\nBad For So Many And What Can Be Done To Improve It.\nCambridge, MA, USA: Harvard University Press, 2014.\n[43]\n J.  Woodcock  and  M.  Graham, The  Gig  Economy:  A\nCritical Introduction. Cambridge, UK: Polity, 2020.\n[44]\n J.  Koebler  and  J.  Cox,  The  impossible  job:  Inside\nFacebook’s  struggle  to  moderate  two  billion  people,\nMotherboard Vice, https://perma.cc/389C-Y2FN, 2019.\n[45]\n 2018 employer information report, consolidated report -\nType  2,  Facebook,  Inc.,  1  Hacker  Way,  CW14861,\nhttps://diversity.fb.com/wp-content/images/2018-\nConsolidated-EEO-1-Part-1.pdf, 2019.\n[46]\n C.  Newton,  The  trauma  floor:  The  secret  lives  of\nFacebook\n moderators\n in\n America, \nThe\n Verge,\n[47]\n    352\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\nhttps://perma.cc/SJ3V-N6PP, 2019.\n P.  Olson,  Image-recognition  technology  may  not  be  as\nsecure  as  we  think, The  Wall  Street  Journal,\nhttps://perma.cc/Z9T2-2S5Z, 2019.\n[48]\n LinkedIn:  YouTube  LLC, https://www.linkedin.com/\ncompany/youtube/about/, 2019.\n[49]\n S.  Levin,  Google  to  hire  thousands  of  moderators  after\noutcry over YouTube abuse videos, The Guardian, Dec.\n05, 2017.\n[50]\n Number  of  monthly  logged-in  YouTube  viewers\nworldwide as of May 2019 (in billions), Statistica. com,\nhttps://www.statista.com/statistics/859829/logged-in-\nyoutube-viewers-worldwide/, 2019.\n[51]\n DefinedCrowd\n triples\n team\n and\n keeps\n hiring,\nGlobalNewsWire, https://www.globenewswire.com/news-\nrelease/2019/02/26/1742790/0/en/DefinedCrowd-triples-\nteam-and-keeps-hiring.html, 2019.\n[52]\n Neevo. ai, https://www.neevo.ai/, 2019.\n[53]\n Economic  news  release;  Table  B-1.  Employees  on\nnonfarm payrolls by industry sector and selected industry\ndetail, Bureau  of  Labor  Statistics, https://www.bls.gov/\nnews.release/empsit.t17.htm, 2019.\n[54]\n Data  engineering,  preparation,  and  labeling  for  AI\n2019, Cognilytica, https://www.cognilytica.com/2019/03/\n06/report-data-engineering-preparation-and-labeling-for-\nai-2019/, 2019.\n[55]\n M.  Murgia,  AI’s  new  workforce:  The  data-labelling\nindustry\n spreads\n globally, \nFinancial\n Times,\nhttps://www.ft.com/content/56dde36c-aa40-11e9-984c-\nfac8325aaa04, 2019.\n[56]\n Data-labelling  startups  want  to  help  improve  corporate\nAI, The Economist, https://www.economist.com/business/\n2019/10/17/data-labelling-startups-want-to-help-\nimprove-corporate-ai, 2019.\n[57]\n A.  Sage,  California  Senate  passes  bill  to  tighten “gig”\nworker rule, Reuters, San Francisco, https://www.reuters.\ncom/article/us-employment-california/california-senate-\npasses-bill-to-tighten-gig-worker-rule-idUSKCN1VW\n0M7, 2020.\n[58]\n E.  Torpey,  You’re  a  what?  Social  media  specialist,\nCareer\n Outlook, \nBureau\n of\n Labor\n Statistics,\nhttps://www.bls.gov/careeroutlook/2016/youre-a-\nwhat/social-media-specialist.htm?view_full, 2016.\n[59]\n Industry  and  occupation  indexes, Census.gov, https://\nwww.census.gov/topics/employment/industry-occupation/\nguidance/indexes.html, 2019.\n[60]\n S. T. Roberts, Behind The Screen: Content Moderation in\nthe  Shadows  of  Social  Media.  New  Haven,  CT,  USA:\nYale University Press, 2019.\n[61]\n S. C. Kuek, Paradi-Guilford, Cecilia Maria, T. Fayomi, S.\nImaizumi,  and  P.  Ipeirotis,  The  Global  Opportunity  in\nOnline Outsourcing (English), Washington, D. C. : World\nBank  Group, http://documents.worldbank.org/curated/\nen/138371468000900555/The-global-opportunity-in-\nonline-outsourcing, 2015.\n[62]\n W.  Cunningham,  The  WyCash  portfolio  management\nsystem, Addendum to the proceedings on Object-oriented\nprogramming  systems,  languages,  and  applications\n[63]\n(Addendum) - OOPSLA ’92, doi: 10.1145/157709.157715.\n A.  Chatzigeorgiou,  A.  Ampatzoglou,  A.  Ampatzoglou,\nand  T.  Amanatidis,  Estimating  the  breaking  point  for\ntechnical  debt,  in Proc.  2015  IEEE  7th  International\nWorkshop on Managing Technical Debt (MTD), Bremen,\nGermany, 2015, pp. 53–56.\n[64]\n Z. Li, P. Avgeriou, and P. Liang, A systematic mapping\nstudy on technical debt and its management, Journal of\nSystems and Software, vol. 101, pp. 193–220, 2015.\n[65]\n C.  Seaman  and  Y.  Guo,  Measuring  and  monitoring\ntechnical\n debt, \nAdvances\n in\n Computers,\n vol. 82,\npp. 25–46, 2011.\n[66]\n B. Curtis, J. Sappidi, and A. Szynkarski, Estimating the\nsize, cost, and types of Technical Debt, presented at 2012\nThird  International  Workshop  on  Managing  Technical\nDebt (MTD), Zurich, Switzerland, 2012.\n[67]\n N.  Brown,  Y.  Cai,  Y.  Guo,  R.  Kazman,  M.  Kim,  P.\nKruchten, E. Lim, A. MacCormack, R. Nord, I. Ozkaya,\net  al.,  Managing  technical  debt  in  software-reliant\nsystems,  in Proc.  the  FSE/SDP  workshop  on  Future  of\nsoftware  engineering  research  -  FoSER’10,  Santa  Fe,\nNM, USA, 2010, pp. 47–51.\n[68]\n P. Kruchten, R. L. Nord, and I. Ozkaya, Technical debt:\nFrom  metaphor  to  theory  and  practice, IEEE  Software,\nvol. 29, no. 6, pp. 18–21, 2012.\n[69]\n D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,\nD.  Ebner,  V.  Chaudhary,  M.  Young,  J.  Crespo,  and  D.\nDennison,  Hidden  technical  debt  in  machine  learning\nsystems, Advances  in  Neural  Information  Processing\nSystems, \nhttps://proceedings.neurips.cc/paper/2015/file/\n86df7dcfd896fcaf2674f757a2463eba-Paper.pdf, 2015.\n[70]\n H.  Foidl,  M.  Felderer,  and  S.  Biffl,  Technical  debt  in\ndata-intensive  software  systems,  arXiv  preprint  arXiv:\n1905.13455, 2019.\n[71]\n Protocol Service of the European Commission, http://ec.\neuropa.eu/dgs/secretariat_general/corps/index.cfm?go=\nprotocol.protoco, 2019.\n[72]\n S. Applin, Autonomous vehicle ethics: Stock or custom?\nIEEE  Consumer  Electronics  Magazine,  vol. 6,  no. 3,\npp. 108–110, 2017.\n[73]\n J. Zittrain, The hidden costs of automated thinking, The\nNew Yorker, https://www.newyorker.com/tech/annals-of-\ntechnology/the-hidden-costs-of-automated-thinking,\n2019.\n[74]\n C. Anderson, The end of theory: The data deluge makes\nthe scientific method obsolete, Wired, https://www.wired.\ncom/2008/06/pb-theory/, 2008.\n[75]\n D. Weinberger, Our machines now have knowledge we’ll\nnever  understand, Wired, https://www.wired.com/story/\nour-machines-now-have-knowledge-well-never-\nunderstand/, 2017.\n[76]\n M.  Kochen,  How  well  do  we  acknowledge  intellectual\ndebts? Journal  of  Documentation,  vol. 43,  no. 1,\npp. 54–64, 1987.\n[77]\n M.  L.  Jones,  How  we  became  instrumentalists  (again):\nData positivism since World War II’, Historical Studies\nin  the  Natural  Sciences,  vol. 48,  no. 5,  pp. 673–684,\n[78]\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n353    \n \n\n2018.\n B. Green, Smart Enough City: Putting Technology in Its\nPlace to Reclaim Our Urban Future. Boston, MA, USA:\nMIT Press, 2019.\n[79]\n Human-Centered\n Artificial\n Intelligence,\n Stanford\nUniversity,  2019  AI  index:  Ground  the  conversation\nabout AI in data, https://aiindex.org/, 2019.\n[80]\n L. Floridi, Translating principles into practices of digital\nethics:  Five  risks  of  being  unethical, Philosophy &\nTechnology, vol. 32, no. 2, pp. 185–193, 2019.\n[81]\n J. Fjeld, H. Hilligoss, N. Achten, and A. Nagy, Principled\nartificial intelligence: Mapping consensus in ethical and\nrights-based  approaches  to  principles  for  AI, Berkman\nKlein Center Research Publication, vol. 2020, no. 1, p.\n39, 2020..\n[82]\n D.  S.  Murray,  The  precarious  new  faculty  majority:\nCommunication  and  instruction  research  and  contingent\nlabor  in  higher  education, Communication  Education,\nvol. 68, no. 2, pp. 235–245, 2019.\n[83]\n University  and  College  Union,  Counting  the  costs  of\ncasualisation in higher education, https://www.ucu.org.uk/\nmedia/10336/Counting-the-costs-of-casualisation-in-\nhigher-education-Jun-19/pdf/ucu_casualisation_in_\nHE_survey_report_Jun19.pdf, 2019.\n[84]\n U.  Huws, Labor  in  The  Global  Digital  Economy:  The\nCybertariat  Comes  of  Age.  New  York,  NY,  USA:\nMonthly Review Press, 2014.\n[85]\n L. Nakamura, Don’t hate the player, hate the game: The\nracialization  of  labor  in  World  of  Warcraft’, Critical\nStudies  in  Media  Communication,  vol. 26,  no. 2,\npp. 128–144, 2009.\n[86]\n L.  Nakamura,  The  unwanted  labour  of  social  media:\nWomen of colour call out culture as venture community\nmanagement, \nNew\n Formations,\n vol. 86,\n no. 86,\npp. 106–112, 2015.\n[87]\n S. A. Applin and M. D. Fischer, New technologies and\nmixed-use convergence: How humans and algorithms are\nadapting  to  each  other,  presented  at  2015  IEEE\nInternational  Symposium  on  Technology  and  Society\n(ISTAS), Dublin, Ireland, 2015.\n[88]\n A.\n Darlo\n and\n H.\n Brignull,\n Dark\n patterns,\nhttps://www.darkpatterns.org/, 2020.\n[89]\n M.  C.  Elish,  Moral  crumple  zones:  Cautionary  tales  in\nhuman-robot\n interaction\n (WeRobot\n 2016), \nSSRN\nElectronic Journal, doi: 10.2139/ssrn.2757236.\n[90]\n A. Mateescu and M. C. Elish, AI in context: The labor of\nintegrating  new  technologie, https://datasociety.net/wp-\ncontent/uploads/2019/01/DataandSociety_AIinContext.p\ndf, 2019.\n[91]\n N. Couldry and U. A. Mejias, The Costs of Connection:\nHow Data Is Colonizing Human Life and Appropriating\nIt  for  Capitalism.  Redwood  City,  CA,  USA:  Stanford\nUniversity Press, 2019.\n[92]\n R.  S.  Cowan,  More  Work  for  Mother: The  Ironies  of\nHousehold;  Technology  from  the  Open  Hearth  to  the\nMicrowave, Nachdr. New York, NY, USA: Basic Books,\n2011.\n[93]\n P.  Peña  and  J.  Varon,  Consent  to  our  Data  Bodies:\n[94]\nLessons from feminist theories to enforce data protection,\nhttps://codingrights.org/docs/ConsentToOurDataBodies.p\ndf, 2019.\n M.  Pasquinelli  and  V.  Joler,  The  nooscope  manifested:\nArtificial  intelligence  as  instrument  of  knowledge\nextractivism,  KIM  HfG  Karlsruhe  and  Share  Lab,\nhttp://nooscope.ai, 2020.\n[95]\n K. Crawford and V. Joler, Anatomy of an AI system: The\nAmazon Echo as an anatomical map of human labor, data\nand planetary resources, AI Now Institute and Share Lab,\nhttps://anatomyof.ai, 2018.\n[96]\n S.\n Dick\n and\n D.\n Volmar,\n DLL\n hell:\n Software\ndependencies, failure, and the maintenance of Microsoft\nWindows, IEEE  Annals  Hist.  Comput.,  vol. 40,  no. 4,\npp. 28–51, 2018.\n[97]\n M.  V.  Wilkes, Memoirs  of  A  Computer  Pioneer.\nCambridge, MA, USA: MIT Press, 1985.\n[98]\n N.  Ensmenger, The  Computer  Boys  Take  Over:\nComputers,  Programmers,  and  the  Politics  of  Tecnical\nExpertise. Cambridge, MA, USA: MIT Press, 2010.\n[99]\n Employment  by  detailed  occupation,  U.  S.  Bureau  of\nLabor  Statistics  |  Office  of  Occupational  Statistics  and\nEmployment  Projections,  Washington,  DC,  Table  1.2\nEmployment by detailed occupation, 2016 and projected\n2026 (Numbers in thousands), https://www.bls.gov/emp/\ntables/emp-by-detailed-occupation.htm, 2019.\n[100]\n P. G. Neumann, The risks digest: Forum on risks to the\npublic\n in\n computers\n and\n related\n systems,\nhttp://catless.ncl.ac.uk/Risks/, 2019.\n[101]\n T.\n Huckle,\n Collection\n of\n software\n bugs,\nhttps://www5.in.tum.de/~huckle/bugse.html, 2019.\n[102]\n SIGCIS 2019 - Exception error: Fatal, illegal, unknown,\nhttps://www.sigcis.org/node/621, 2019.\n[103]\n M. A. Makary and M. Daniel, Medical error—the third\nleading  cause  of  death  in  the  US, BMJ,  doi:\n10.1136/bmj.i2139.\n[104]\n K. G. Shojania and M. Dixon-Woods, Estimating deaths\ndue to medical error: The ongoing controversy and why it\nmatters:  Table  1’, BMJ  Quality  &  Safety,  doi:\n10.1136/bmjqs-2016-006144.\n[105]\n M. Thomas and H. Thimbleb, Computer bugs in hospitals:\nA new killer−professor Martyn Thomas CBE and professor\nHarold Thimbleb, Gresham College, London, https://www.\nyoutube.com/watch?v=seyaYL2ou14, 2018.\n[106]\n M.  Thomas  and  H.  Thimbleb,  Computer  bugs  in\nhospitals:\n An\n unnoticed\n killer, \nhttp://www.harold.\nthimbleby.net/killer.pdf, 2019.\n[107]\n National Institute of Standards and Technology, Program\nOffice Strategic Planning and Economic Analysis Group,\nThe  economic  impacts  of  inadequate  infrastructure  for\nsoftware  testing, https://www.nist.gov/sites/default/files/\ndocuments/director/planning/report02-3.pdf, 2002.\n[108]\n J.  Williams, Stand  Out  of  Our  Light:  Freedom  and\nResistance  in  the  Attention  Economy.  Cambridge,  UK:\nCambridge University Press, 2018.\n[109]\n R.  Benjamin  and  J.  McNealy,  A  new  Jim  code,\nhttps://cyber.harvard.edu/sites/default/files/2019-\n10/2019_09_24_RuhaBenjamin_Transcript.pdf, 2019.\n[110]\n    354\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\n B.  Cooper,  The  racial  politics  of  time,  presented  at  the\nTEDWomen, https://www.ted.com/talks/brittney_cooper_\nthe_racial_politics_of_time/transcript, 2016.\n[111]\n J. Halberstam, In A Queer Time and Place: Transgender\nBodies,  Subcultural  Lives.  New  York,  NY,  USA:  New\nYork University Press, 2005.\n[112]\n Western States Center, Dismantling Racism: A Resource\nBook  for  Social  Change  Groups,  Anti-Racism  Digital\nLibrary, https://sacred.omeka.net/items/show/221, 2022.\n[113]\n J. Von Neumann, Can we survive technology? Fortune,\nno. 15, Jun. 1955.\n[114]\n M.\n Minsky,\n Steps\n toward\n artificial\n intelligence,\nProceedings of the IRE, vol. 49, no. 1, pp. 8–30, 1961.\n[115]\n H.  T.  Larson,  The  computer  issue, Proceedings  of  the\nIRE, vol. 49, no. 1, pp. 4–7, 1961.\n[116]\n L.  Daston,  Enlightenment  calculations, Critical  Inquiry,\nvol. 21, no. 1, pp. 182–202, 1994.\n[117]\n M.  D.  Sahlins, Stone  Age  Economics.  New  Brunswick,\nNJ, USA: Transaction Publishers, 2011.\n[118]\n U.  A.  Mejias, Off  the  Network:  Disrupting  the  Digital\nWorld. Minneapolis, MN, USA: University of Minnesota\nPress, 2013.\n[119]\n Z.  Lissa,  Aesthetic  functions  of  silence  and  rests  in\nmusic, The  Journal  of  Aesthetics  and  Art  Criticism,\nvol. 22, no. 4, p. 443, 1964.\n[120]\n E.  Nelson,  Walking  to  the  future  in  the  steps  of  our\nancestors:\n Haudenosaunee\n traditional\n ecological\nknowledge and queer time in the climate change era, New\nGeographies, vol. 09, no. Posthuman, pp. 133–138, 2017.\n[121]\n A. L. Stoler, Imperial Debris: On Ruins and Ruination.\nDurham, UK: Duke University Press, 2013.\n[122]\n M. Liboiron, M. Tironi, and N. Calvillo, Toxic politics:\nActing in a permanently polluted world, Social Studies of\nScience, vol. 48, no. 3, pp. 331–349, 2018.\n[123]\n M. Richtel and T. Shanker, In new military, data overload\ncan  be  deadly, The  New  York  Times, https://www.\nnytimes.com/2011/01/17/technology/17brain.html, 2011.\n[124]\n A.\n Young,\n Too\n much\n information:\n Ineffective\nintelligence  collection, Harvard  International  Review,\nAug. 18, 2019.\n[125]\n M.  G.  D.  C.  Katoch,  Leadership  challenges  in  the\ninformation age, Journal of the United Service Institution\nof  India, https://usiofindia.org/publication/usi-journal/\nleadership-challenges-in-the-information-age/, 2015.\n[126]\n M. J. L. Campo, Information dominance or information\noverload?  Unintended  consequences  of “every  soldier\nand\n platform\n a\n sensor”,\n Naval\n War\n College,\nhttps://apps.dtic.mil/dtic/tr/fulltext/u2/a525256.pdf, 2010.\n[127]\n T.  Dryer,  Designing  certainty:  The  rise  of  algorithmic\ncomputing\n in\n an\n age\n of\n anxiety\n 1920−1970,\nhttps://escholarship.org/uc/item/4d02g6x3, 2019.\n[128]\n J.  Stilgoe,  R.  Owen,  and  P.  Macnaghten,  Developing  a\nframework  for  responsible  innovation, Research  Policy,\nvol. 42, no. 9, pp. 1568–1580, 2013.\n[129]\n E.  Gordon  and  G.  Mugar, Meaningful  Inefficiencies:\nDesigning  for  Public  Value  in  an  Age  of  Digital\nExpediency.  New  York,  NY,  USA:  Oxford  University\nPress, 2020.\n[130]\n L.  Downey  and  J.  Simons,  Charismatic  moments,  in A\n[131]\nPolitical Economy and Justice, D. Allen, Y. Benkler, and\nR.  Henderson,  eds.  Chicago,  IL,  USA:  Chicago\nUniversity Press, 2022.\n C. Lanius, Fact check: Your demand for statistical proof\nis\n racist, \nCyborgology, \nhttps://thesocietypages.org/\ncyborgology/2015/01/12/fact-check-your-demand-for-\nstatistical-proof-is-racist/, 2015.\n[132]\n K.  Hill,  Imagine  being  on  trial.  With  exonerating\nevidence  trapped  on  your  phone. The  New  York  Times,\nhttps://www.nytimes.com/2019/11/22/business/law-\ne n f o r c e m e n t - p u b l i c - d e f e n d e r - t e c h n o l o g y -\ngap.html?auth=login-email&login=email, 2019.\n[133]\n D.  Harwell,  AI  will  solve  Facebook’s  most  vexing\nproblems, Mark Zuckerberg says. Just don’t ask when or\nhow, The Washington Post, https://www.washingtonpost.\ncom/news/the-switch/wp/2018/04/11/ai-will-solve-\nfacebooks-most-vexing-problems-mark-zuckerberg-says-\njust-dont-ask-when-or-how/?utm_term=.8c71a2d658f6,\n2018.\n[134]\n R. A. Oppel Jr. and J. K. Patel, Onelawyer, 194 felony\ncases,  and  no  time, The  New  York  Times, https://www.\nnytimes.com/interactive/2019/01/31/us/public-defender-\ncase-loads.html, 2019.\n[135]\n E. J. Topol, Deep Medicine: How Artificial Intelligence\nCan Make Healthcare Human Again, First edition. New\nYork, NY, USA: Basic Books, 2019.\n[136]\n I. Joshi, Waiting for deep medicine, The Lancet, vol. 393,\nno. 10177, pp. 1193–1194, 2019.\n[137]\n Z.  Tufekci,  Why  Zuckerberg’s  14-year  apology  tour\nhasn’t  fixed  Facebook, Wired, https://www.wired.\ncom/story/why-zuckerberg-15-year-apology-tour-hasnt-\nfixed-facebook/, 2018.\n[138]\n C.  Garvey,  Broken  promises  and  empty  threats:  The\nevolution of AI in the USA, 1956−1996, Tech’s Stories,\ndoi: 10.15763/jou.ts.2018.03.16.02.\n[139]\n D. C. Brock, Our censors, ourselves: Commercial content\nmoderation, \nLos\n Angeles\n Review\n of\n Books,\nhttps://lareviewofbooks.org/article/our-censors-ourselves-\ncommercial-content-moderation/, 2019.\n[140]\n K.  Cascone,  The  aesthetics  of  failure: “Post-digital”\ntendencies  in  contemporary  computer  music, Computer\nMusic Journal, vol. 24, no. 4, pp. 12–18, 2000.\n[141]\n S. M. Ali, Fugitive decolonial luddism – A hauntology,\nPresentation,  Digital  Ethics  Lab,  Oxford  Internet\nInstitute, Dec. 10, 2019.\n[142]\n D. Ribes, A. S. Hoffman, S. C. Slota, and G. C. Bowker,\nThe logic of domains, Social Studies of Science, vol. 49,\nno. 3, pp. 281–309, 2019.\n[143]\n S. Kelkar, Reinventing expertise in the age of platforms:\nThe  case  of  data  science,  presented  at  the  BIDS  Data\nScience, \nhttps://www.youtube.com/watch?v=-Ba2Gq\n13dBI, 2019.\n[144]\n D. Forsythe and D. J. Hess, Studying Those Who Study\nUs:  An  Anthropologist  in  the  World  of  Artificial\nIntelligence.  Redwood  City,  CA,  USA:  Stanford\nUniversity Press, 2001.\n[145]\n S.  Viljoen,  The  promise  and  limits  of  lawfulness:\n[146]\n  Jonnie Penn:   Algorithmic Silence: A Call to Decomputerize\n355    \n \n\nInequality,  law,  and  the  techlash, Journal  of  Social\nComputing, doi: 10.23919/JSC.2021.0025.\n E. Tuck and M. McKenzie, Place in Research: Theory,\nMethodology,  and  Methods.  New  York,  NY,  USA:\nRoutledge, 2016.\n[147]\n E.  Tuck  and  M.  McKenzie,  Relational  validity  and  the\n“where” of  inquiry:  Place  and  land  in  qualitative\nresearch, Qualitative Inquiry, vol. 21, no. 7, pp. 633–638,\n2015.\n[148]\n E.  Tuck  and  K.  W.  Yang,  Decolonization  is  not  a\nmetaphor, Decolonization:  Indigeneity,  Education  &\nSociety, vol. 1, no. 1, pp. 1–40, 2012.\n[149]\n A. C. Clarke, Profiles of The Future: An Inquiry into the\nLimits of the Possible. London, UK: Indigo, 2000.\n[150]\n E.  Strubell,  A.  Ganesh,  and  A.  McCallum,  Energy  and\npolicy  considerations  for  deep  learning  in  NLP,  arXiv\npreprint arXiv: 1906.02243, 2019.\n[151]\n B. Tarnoff, To decarbonize we must decomputerize: why\n[152]\nwe\n need\n a\n Luddite\n revolution, \nThe\n Guardian,\nhttps://www.theguardian.com/technology/2019/sep/17/te\nch-climate-change-luddites-data, 2019.\nJonnie  Penn received  the  doctorate  and  MPhil  (First  Class\nHonors) degrees in philosophy from the Department of History\nand Philosophy of Science at the University of Cambridge in 2020\nand  2015,  respectively.  He  also  received  the  BA  degree  from\nMcGill  University  in  2013.  He  serves  as  an  affiliate  at  the\nBerkman Klein Center at Harvard University, a research fellow at\nSt.  Edmund’s  College,  and  as  an  associate  fellow  at  the\nLeverhulme Centre for the Future of Intelligence. He currently\nteaches  masters  on  AI  ethics  and  society  at  the  University  of\nCambridge (2021−2024), where is also a postdoctoral research\nfellow in the Department of History and Philosophy of Science\nand co-organizer of the Mellon Foundation Sawyer Seminar on\n“Histories of Artificial Intelligence: A Genealogy of Power”.\n    356\nJournal of Social Computing, December 2021, 2(4): 337−356    \n \n\n \nConnecting Race to Ethics Related to Technology:\nA Call for Critical Tech Ethics\nJenny Ungbha Korn*\nAbstract:    Critical tech ethics is my call for action to influencers, leaders, policymakers, and educators to help\nmove our society towards centering race, deliberately and intentionally, to tech ethics. For too long, when\n“ethics” is  applied  broadly  across  different  kinds  of  technology,  ethics  does  not  address  race  explicitly,\nincluding how diverse forms of technologies have contributed to violence against and the marginalization of\ncommunities of color. Across several years of research, I have studied online behavior to evaluate gender and\nracial biases. I have concluded that a way to improve technologies, including the Internet, is to create a specific\ntype of ethics termed “critical tech ethics” that connects race to ethics related to technology. This article covers\nguiding theories for discovering critical tech ethical challenges, contemporary examples for illustrating critical\ntech  ethical  challenges,  and  institutional  changes  across  business,  education,  and  civil  society  actors  for\nteaching critical tech ethics and encouraging the integration of critical tech ethics with undergraduate computer\nscience. Critical tech ethics has been developed with the imperative to help improve society through connecting\nrace  to  ethics  related  to  technology,  so  that  we  may  reduce  the  propagation  of  racial  injustices  currently\noccurring by educational institutions, technology corporations, and civil actors. My aim is to improve racial\nequity through the development of critical tech ethics as research, teaching, and practice in social norms, higher\neducation, policy making, and civil society.\nKey  words:   race; gender; ethics; tech; bias; equity; society; policy\n1    Sociocultural Introduction\nIt is July of 2020, and I am writing this article during a\ntime of racial unrest and personal loss. A few months ago,\nGeorge Perry Floyd Jr, a Black man, was killed by a\nWhite  police  officer,  Derek  Chauvin,  who  knelt  on\nFloyd’s  neck  for  nearly  eight  minutes.  Like  many\nactivists, I participated in protests to draw attention to\ncontinued racism, police brutality, and racial injustice.\nLocated  in  Alabama,  I  marched  in  my  hometown’s\nBlack Lives Matter protest, where police snipers were\nstationed  at  the  tops  of  buildings,  poised  to  shoot\nordinary citizens of different races engaged in peaceful\nactivism.  George  Floyd’s  murder  occurred  during  the\nCOVID pandemic, which attacked both of my parents,\ncausing them both to be hospitalized and intubated. My\ndaddy died within a week of George Floyd’s death.\nI  provide  details  about  this  particular  sociocultural\nmoment  to  make  the  point  that  the  time  for  a  closer\ninspection of how race relates to ethics and technology\nhas arrived. Over the past few months, I have received\ndozens  of  emails  from  companies  and  organizations\nstating  their  condemnation  of  racism,  promotion  of\nequality,  and  support of  inclusion.  Entire associations\nare now stating that Black Lives Matter. They are stating\nthat  anti-Asian  racism  and  medical  racism  related  to\nCOVID,  which  my  family  experienced[1],  are  wrong.\nThis  country  is  examining  racial  injustice  across  a\nvariety  of  contexts,  including  sports,  crime,  politics,\nmedicine, and technology. If there is a time to call for\ncritical tech ethics, it is most assuredly right now.\n \n • Jenny  Ungbha  Korn  is with  the  Berkman  Klein  Center  for\nInternet  and  Society,  Harvard  University,  Cambridge,  MA\n02138, USA. E-mail: jkorn@cyber.harvard.edu.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-06-22;\n revised:\n 2021-11-22;\naccepted: 2021-11-25\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   05/06  pp357−364\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0026\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\n2    Article Outline\nCritical tech ethics is my call for action to influencers,\nleaders, policymakers, and educators to help move our\nsociety  towards  centering  race,  deliberately  and\nintentionally, to tech ethics. For too long, when “ethics”\nis applied broadly across different kinds of technology,\nethics does not address race explicitly, including how\ndiverse  forms  of  technologies  have  contributed  to\nviolence against and the marginalization of communities\nof color. Across several years of research[2−11], I have\nstudied  online  behavior  to  evaluate  gender  and  racial\nbiases.  I  have  concluded  that  a  way  to  improve\ntechnologies,  including  the  Internet,  is  to  create  a\nspecific  type  of  ethics  termed  critical  tech  ethics  that\nconnects race to ethics related to technology.\nThis article covers:\n• Guiding theories for discovering critical tech ethical\nchallenges;\n• Contemporary examples for illustrating critical tech\nethical challenges;\n• Institutional changes across business, educational,\nand civil society actors for teaching critical tech ethics\nand  encouraging  the integration  of  critical tech  ethics\nwith undergraduate computer science.\n3    Guiding Theories\nThe theories that inform this article are all drawn from\ncritical\n theory,\n including\n critical\n race\n theory,\nintersectional feminist theory, and critical race feminist\ntheory. In this section of the article, I briefly highlight\nsignificant components of  all three related  theories to\ndemonstrate how their contributions on race, gender, and\ndiverse  axes  of  identity  intersect  with  digital  media\nethics to create critical tech ethics.\nBoth  critical  race  theory  and  intersectional  feminist\ntheory emerged in the 1980s[12, 13]. Immediately, critical\nrace theory became popular within academia, especially\nin the fields of law and education. In contrast, though\nintersections of race with gender had been highlighted\nby  prominent  feminists  of  color  in  the  1980s[12],\nintersectional feminist theory was slower in its adoption,\nnot gaining widespread recognition until the 1990s[14].\nCritical race theory has at least three tenets that are\nrelevant directly to critical tech ethics[13]:\n• Concepts that are held as “race-neutral” are tied to\nWhite supremacy and racism.\n• Racism is acknowledged as ordinary, fundamental,\nand embedded within American society.\n•  Awareness  of  examples  of  hegemonic  Whiteness\nshould lead practitioners of critical race theory to create\nand support interventions to transform social structures\nand advance social justice.\nIntersectional feminist theory informs this article by\nstressing the concurrent ways that axes of identity are\nactivated  in  their  oppressions[12].  Specifically,  the\napplications  of  intersectional  feminist  theory  used  in\nanalyses for this article are:\n• Race alone and gender alone are not adequate ways\nto analyze the results of the inputs and outputs related to\nonline behavior.\n• The intersection of race with gender lends important\ninsights  into  understanding  the  inputs  and  outputs\nrelated to online behavior.\nFinally,  critical  race  feminist  theory,  as  the  name\nimplies,  combines  components  of  critical  race  theory\nwith  intersectional  feminist  theory[15].  In  fact,  key\nadvocates of both critical race theory and intersectional\nfeminist theory have helped to form critical race feminist\ntheory. Since the mid-1990s, critical race feminist theory\nhas  been  forming  adherents,  but  it  lags  in  popularity\nbehind  critical  race  theory  and  intersectional  feminist\ntheory.\nThe key reminders from critical race feminist theory\nmost applicable to this work are[15]:\n•  The  socially-constructed  categories  of  race  with\ngender should not be reduced to essentialism. In other\nwords, women of color, men of color, and people of color\nwho do not identify with binary gender experience the\nworld  differently  from  one  another  across  genders,\nwhich  is  a  presumption  that  is  different  from  earlier\nforms of critical race theory that lumped men and women\nof color together under the umbrella term of race.\n• Women and people of color who do not identify with\nbinary gender are not monolithic. Perceived differences\nacross racial and  ethnic divides influence concepts of\nwhat  it  is  to  be  Indigenous,  African  American/Black,\nAsian/Asian American, Latina/Latine /Latinx, Caucasian/\nWhite, and so on. The presence of one woman of color\nonline  is  not  representative  of  all  women  of  color,\nparticularly across ethnicities.\nCritical  race  theory[16],\n intersectional  feminist\ntheory[17], and critical race feminist theory[15] encourage\nspecial attention be paid to race and gender. Following\n    358\nJournal of Social Computing, December 2021, 2(4): 357−364    \n \n\nsuch traditions set by scholars of color, I use this article\nto illustrate why and how critical tech ethics should be\ndeveloped as an area that connects criticality around race\nand  gender  with  technology  ethics,  including  digital\nmedia.  Indigenous  scholar  Ess[18] has  defined  digital\nmedia ethics as addressing the moral principles related\nto activities conducted via computing technologies and\nonline systems. A data practice that I challenge is how\nacritical  and  supportive  of  the  status  quo “ethics” in\nartificial  intelligence,  computing  technologies,  and\nonline systems has been. Digital media ethics has been\nheralded as a way to consider the social good, producing\ntech conferences devoted to the combination of ethics\nwith  artificial  intelligences.  Ethics  is  the  current\nbuzzword  for  the  funding  of  grants  for  civil  and\nacademic  artificial  intelligence  projects[19].  But  how\noften  does  tech  ethics  explicitly  engage  with  racial\nequity? I explore answers to this question here.\nAfter presenting real-life examples illustrating ethical\nchallenges  that  are  not  race-neutral,  I  advocate\ninstitutional changes for teaching critical tech ethics and\nmarketplace changes for encouraging the integration of\ncritical tech ethics into undergraduate computer science\neducation.  Though  intersectional  feminist  theory  does\nnot  include  an  action  component  in  its  application,\ncritical race theory does emphasize interventions as part\nof  analyses.  I  use  the  latter  sections  of  this  article  to\ncritique the algorithms that control so much of our online\nbehavior and highlight interventions that could empower\nfuture technology builders to create a healthier Internet\nfor all.\n4    Online Images\nImages influence our conceptions of the world[20, 21], and\nyet,  they  are  often  overlooked  in  examinations  of\ncomputer-mediated  communication[22, 23].  I  use  online\nimage  searches  to  highlight  the  reflexivity  between\nsociety and technology in (re)producing contemporary\nAmerican  socioeconomic  politics,  while  concurrently\nshaping attitudes, decisions, and actions about race and\ngender.\nI am a woman of color in the academy. When I entered\nthe keyword of “professor” in an online image search,\nthe algorithm produced a screen full of thirty images[4].\nUsing critical visual discourse analyses, I examined the\npresence  and  absence  of  diverse  embodiments  for\nprofessors  in  images  from  online  searches.  Of  those\nresults from the screen, 87% of the images were highly\nbiased  in  terms  of  age,  race,  gender,  and  appearance.\nTwenty-six  images  were  variations  of  elderly,  White\nmen that wore glasses or laboratory coats or appeared in\nfront of chalkboards in a conflation of “professor” with\nlaboratory scientist[24]. The background of a chalkboard\nmatches  the  emoji  suggestion  for  professor  made  by\niPhones running Apple’s iOS 10 for an emoji of a White\nman  standing  in  front  of  a  chalkboard[25].  Men  were\nshown as bedecked with grey or white hair that stuck out\nfrom the head in a hairstyle that has become associated\nwith Dr. Albert Einstein[26, 27], who was a well-known\nand  highly-regarded  professor  of  physics.  The  visual\nimages of “professor” tended to showcase individuals\nas rational and scientific, which has been an enduring\nperspective on the appearance of a professor since the\nlate 1960s[24, 28, 29]. The embodiment of a professor that\nis  normalized  through  these  online  image  results  is\nintertextual and upholds that a professor is expected to\nbe\n White,\n male,\n and\n weight-proportionate[30].\nRepresentations of professors within images influence\nstudents  and  their  preconceived  notions  of  whom  to\nexpect  as  authoritative  and  expert  in  the  classroom,\nwhich  lead  to  significant  implications  on  student\nevaluations of teaching.\nAnother example of how image searches are biased\nand have real-life consequences is an online query for\nmedical conditions related to skin. Those of us that have\nexperienced bumps or dry patches on our skin might turn\nto the Internet for images to figure out what might be\nailing  us.  Unfortunately,  like  other  mass  media\nrepresentations[31],  the  images  that  result  in  online\nsearches nearly always reinforce a dominance of White\nand  male.  In  2021,  image  examples  of “bumps” or\n“hives” yield 100% pale skin as examples. When race\nand gender are rendered invisible in images online, the\noutcome  may  be  classified  as  color-blind  and\ngender-blind. Color- and gender-blindness, often under\nthe guise of neutrality, maintain White racial and male\ngender  domination  by  normalizing  White  men  as  the\nstandard[13, 32, 33].  Pictures  of  diseases  related  to  skin\ntend to be on white skin in medical textbooks, physical\nand online, which leads to the perpetuation of biases in\nhealth  care,  limitations  on  health  diagnoses,  and\ninequities  in  medical  training  related  to  allergies  and\ndiseases  of  the  skin,  by  professionals  and  amateurs\n  Jenny Ungbha Korn:   Connecting Race to Ethics Related to Technology: A Call for Critical Tech Ethics\n359    \n \n\nalike[34, 35]. Omissions from these online images results\nbecome  othered[36]:  White  men  are  legitimated  as\nprofessors, and white skin is validated as the foundation\nfor visible detection of skin conditions.\nAlgorithmic  othering  is  happening.  As  algorithmic\nsystems\n become\n commonplace,\n we\n should\n be\nrepresented in algorithmic results. Examples of biases\nalong race and gender extend beyond search results. For\nexample,  facial  recognition  and  covert  surveillance\ntechnologies  have  been  used  by  those  in  power  to\noppress  communities  of  color  to  unjust  outcomes\naffecting employment, prosecution, and more. I choose\nexamples\n of\n representation\n online\n because\nrepresentation  continues  to  matter  and  because  their\nresults often go unquestioned by acritical search users\nthat believe online searches yield neutral findings. An\neducation  in  critical  tech  ethics  would  behoove  the\nindividuals that program and impact the creation of the\nalgorithms that increasingly construct our online world\nand the individuals that casually and critically use and\nbenefit from such algorithms.\n5    Institutional Changes\nA reason to promote critical tech ethics is to ensure that\nrace  and  gender  are  prioritized  within  digital  media\nethics. Earlier in this article, I questioned how often tech\nethics explicitly engages with racial equity. One domain\nthat provides empirical data on how ethics might connect\nwith race lies within the university system of the United\nStates.\nAs part of my keynote for Mozilla in 2019[37], to gain\nbetter understanding about the primacy of race within\nundergraduate computer science education, I analyzed\na  public,  online  listing  from  2018  of  the  names  of\ncrowdsourced  courses  identified  as  ethics  related  to\ntechnology[38]. As part of the listing, instructors could\nopt to share their syllabus. With syllabi as my units of\nanalyses,  I  used  curricula  by  faculty  to  analyze  how\nethics was defined by the individuals that were teaching\nself-identified  courses  in  ethics.  What  does “ethics”\nmean in praxis, not in theory, when ethics is taught to\nundergraduate  students?  And  how  often  does  such\neducation in ethics intersect with issues related to race?\nTo focus on how race is construed in the context of an\nAmerican computer science department, I audited when\nand how the topic of race was explicitly referenced by\nfaculty that used English as the primary language in their\neducation  of  ethics  to  undergraduate  students  in\ncomputer science in the United States.\nUsing thematic and critical discourse analyses on the\nresults  of  the  ten  syllabi  whose  entire  contents  were\navailable  publicly  online,  from  undergraduate  ethics\ncourses taught in computer science in the United States,\neight syllabi did not list race explicitly as a topic of focus\nfor any class of the entire school term, leaving only two\nsyllabi that featured race specifically for a class session.\nWhile stating race at all makes the faculty that created\nthose two syllabi exemplary, it was unfortunate that the\ntopic of race was constructed to fill only a single class\nsession, as opposed to having race in tech ethics as a\nregular  part  of  an  ongoing  discussion  across  all  class\nsessions. Each of the two syllabi construed “race” in two\ndifferent contexts: one syllabus defined race in terms of\nimproving the racial diversity of employees in the field\nof computer science[39]. Another syllabus identified race\nas  a  factor  for  influencing,  and  being  influenced  by,\nalgorithmic  data[40].  Outside  of  those  two  syllabi  that\nincluded a class session on race, four syllabi included\nlinks to supplemental readings that were aligned with the\nlatter definition of race, namely, algorithmic bias in terms\nof racist outcomes against the Black community[41−44].\nOne syllabus mentioned “algorithmic fairness” as a topic,\nbut  race  was  never  introduced;  instead,  ethical\nconsiderations about algorithmic fairness were defined\nin terms of the extinction of humanity by robots and the\nattachment of emotions related to robots. In other words,\nthe ethics of robotics was considered a priority by faculty,\nbut the ethics of race was rendered irrelevant for this\nundergraduate  course:  robots  appeared  as  an  ethical\nissue in artificial intelligence on this syllabus, but race\nas an ethical issue related to artificial intelligence did not\nmaterialize in the syllabus for this course. I provide these\nresults as a snapshot in time of how tech ethics is such\na  broad  area  that  the  topic  of  race  may  be  rendered\ninvisible.\nAs a topic for teaching, research, and discussion, race\nmay  be  more  uncomfortable,  and  therefore  more\nchallenging, for those that are untrained in critical race\ntheory or for those whose lived experiences represent the\ninstitutionally-dominant White community in the United\nStates. For the vast majority of undergraduate computer\nscience classes taught about ethics, ethics is acritical and\nsupportive of the status quo. While tech ethics might be\nheralded as a way to consider the social good[45], tech\n    360\nJournal of Social Computing, December 2021, 2(4): 357−364    \n \n\nethics training tends not to engage explicitly with racial\nequity. In practice, across the training and education of\ncivil society organizations, ethics tends to rely upon the\nwork of heterosexual White cis male philosophers and\ndoes  not  address  intersectional  justice  across  races,\ngenders, and sexualities.\nComputer science ethics classes are often taught by\ncomputer science faculty that have minimal training in\nethics, let alone any training in critical race theory. This\nlack of training is a systemic issue that reflects biases in\nwhat  expertise  is  seen  as  valuable:  computer  science\nethics  is  taught  by  technical  scholars  who  have\nself-studied some ethics, rather than people with deep\nexpertise in ethics and race. As I have advocated in my\npublic  scholarship[7, 37],  universities  supportive  of\ncritical  tech  ethics  should  seek  to  hire  faculty  with\ntraining in and whose scholarship promotes critical race\ntheory,  intersectional  feminist  theory,  or  critical  race\nfeminist  theory  connected  to  ethics  related  to\ntechnologies.\n6    Critical Tech Ethics\nEthics without intentional criticality results in a panacea\nfor people with the power to influence computer science,\ndigital systems, and artificial intelligence. Ethics devoid\nof critical race training is incomplete and deleterious. I\nam  concerned  about  a  responsibility  gap  between\ndecisions  made  by  people  designing  algorithms  and\npeople  experiencing  algorithmic  biases.  I  position\naccountability for racial fairness upon existing business,\neducational, and civil society institutions that train and\nhire individuals and upon established organizations that\ndesign and manage algorithms. A way to guide better\ninteractions between artificial intelligence and diverse\nhumans  is  to  provide  improved  academic  and  social\ninstruction related to racial equity to creators and users\nof technologies for academic communities, technology\norganizations,  and  civil  society  actors.  Rather  than\npresent ethics as race-neutral, reflecting a philosophy of\ncolor-blindness[10],\n I\n seek\n to\n institutionalize\nconsiderations of racial equity through the establishment\nof critical tech ethics.\nTechnology is not neutral. Algorithms have embedded\nvalues. The question then is whose truth is reflected and\nwhose truth is omitted in the design and use of algorithms.\nAlgorithmic  bias  happens  because  values  are  implicit\nwithin the programming and design of the algorithms for\nonline  behavior[46−48].  Algorithms  fit  with  and  help\nadvance  a  single  race  as  the  dominant  culture  in  the\nUnited States[48]. Critical tech ethics makes explicit the\nimplicit  ways  that  Whiteness  is  hegemonic  to  the\ndetriment of other races. Critical tech ethics is based on\ncritical  race  training  that  offers  both  intellectual  and\npolitical responses to challenge racial power and change\nAmerican  society.  I  encourage  readers  to  engage  in\ndigital acts of racial realism, as described by Bell Jr[49]\nto “challenge principles of racial equality” and to use\n“social mechanisms” to “have voice and outrage heard”.\nCritical tech ethics is an area of study and application\nthat includes:\n(1) Institutionalizing  critical  tech  ethics  through\nmandating  racially-aware  standards  for  reviewing\nresearch, awards, grants, and funding: Specifically, I\nseek to construct “racial implications of this proposal”\ninto  a  critical  tech  ethics  standard  for  civil  society\norganizations  because  downstream  uses  of  artificial\nintelligence should be part of the intellectual rigor that is\nvalued  for  judging  work  in  reviews[8].  In  doing  so,\norganizations\n and\n companies\n that\n mandate\nconsiderations of racial implications in their applications\nsignal  that  racial  awareness  is  a  significant  factor  in\nawarding  funding  and  awards,  which,  in  turn,\nencourages participants to reflect upon how their work\nis  impacted  by  and  imbricated  with  race,  racism,  and\nracial equity.\n(2) Setting expectations for teaching critical tech\nethics centered on racial equity: Required training in\ncritical  race  theory  would  help  those  creating  our\ntechnological worlds to understand better about ethical\nconsiderations  related  to  race.  Specifically,  such\neducation should be informed by critical race theory to\nchange norms and demonstrate how computer science,\ndigital systems, and artificial intelligence have played a\nrole in the episteme and techne of racism[8]. In doing so,\ncritical tech ethics actively builds in discussions of race,\nracism, and racial justice to minimize the reproduction\nand hegemony of Whiteness by those in programming,\ncoding, computer science, engineering, and open source\ncommunities.\n(3) Establishing  critical  tech  ethics  practices  for\nimproving industry norms aligned with the goal of\nimproving  racial  justice:  Due  to  the  influence  of\ncapitalism upon choices made by students for profitable\n  Jenny Ungbha Korn:   Connecting Race to Ethics Related to Technology: A Call for Critical Tech Ethics\n361    \n \n\ncareers  and  choices  made  by  universities  to  supply\nemployees for in-demand occupational niches, industry\nmust also be part of the equation to establish critical race\nthinking as part of everyday computer science education\nin  the  United  States.  To  encourage  institutions  to\nmandate the addition of critical tech ethics, employers\nwill need to update the requirement section of their job\nads  to  state  the  desirability  of  hiring  individuals  with\ntraining  in  considering  the  racial  implications  of\nartificial  intelligence[7, 8, 10].  In  doing  so,  technology\ncorporations  may  take  a  step  towards  contributing\ntowards racial justice, which involves tactics, actions,\nand  attitudes  that  challenge  racial  power,  resulting  in\nmore equitable opportunities and outcomes[50].\n7    Conclusion\nIncluded  within  this  article  is  a  call  for  action  to\ninfluencers, leaders, and policymakers to take note to\nhelp  move  our  society  towards  greater  justice  for\neveryone, particularly communities of color. To combat\nracism and sexism[51, 52], changes to existing curricula\nmust  occur.  Students  themselves  acknowledge  that\ncritical race thinking should be taught more frequently\nthan  they  are  available  currently[53].  Across  leading\ninstitutions globally, a lack of inclusion of race, gender,\nintersectionality,  and  power  leads  to  an  enactment  of\nethics  education  lacking  in  justice.  For  too  long,  the\nrhetoric  of  diversity  has  been  unaccompanied  by\ninstitutional change. We must recognize and address that\ncomputer science departments in the United States have\noverlooked  how  the  technologies  on  which  they  are\ntraining  future  programmers  are  impacted  by  and\nimbricated  with  race,  gender,  sexuality,  religion,  and\nother axes of identity. Presumptions about the neutrality\nof algorithms have resulted in the biases we see today in\nthe  inputs  and  outputs  of  various  technologies[46−48].\nCountering those biases through critical tech ethics will\nbe helpful in reducing unfair and unjust outcomes based\non algorithms. Rendering diversity in race and gender as\nvisible is a process that will take greater commitment by\nthose  producing  the  algorithms  and  those  using  the\nalgorithms because online data are a social enterprise[23].\nCritical  tech  ethics  has  been  developed  with  the\nimperative to help improve society through connecting\nrace  to  ethics  related  to  technology,  so  that  we  may\nreduce  the  propagation  of  racial  injustices  currently\noccurring  by  educational  institutions,  technology\ncorporations, and civil actors. We should live in a world\nin which the responsibilities for racial equity do not fall\non people of color only, but are borne by everyone that\ninfluences and is influenced by algorithms. My project\nis to improve racial equity through the development of\ncritical tech ethics as research, teaching, and practice in\nsocial norms, higher education, policy making, and civil\nsociety.\nReferences\n J.  U.  Korn,  Asian  embodiment  as  victim  and  survivor:\nSurveillance,\n racism,\n and\n race\n during\n COVID,\nSurveillance & Society, vol. 19, no. 4, pp. 494–500, 2021.\n[1]\n J.  U.  Korn,  Privileged  technology-mediation:  Gendered\nand  racialized  (re)productions  within  online  image\nsearches, presented at the 41st Annu. Meeting Organization\nof  Communication,  Language,  and  Gender,  Santa  Clara,\nCA, USA, 2014.\n[2]\n J.  U.  Korn,  Black  nerds,  asian  activists,  and  caucasian\ndogs:  Online  race-based  cultural  group  identities  within\nFacebook  groups, International  Journal  of  Interactive\nCommunication  Systems  and  Technologies,  vol. 5,  no. 1,\npp. 14–25, 2015.\n[3]\n J.  U.  Korn,  Bystander  conjecture  over  lived  experience:\n#Whitesplaining  race  and  racism,  presented  at  the  17th\nAnnu.  Meeting  Association  of  Internet  Researchers,\nBerlin, Germany, 2016.\n[4]\n J. U. Korn, Black women exercisers, Asian women artists,\nWhite  women  daters,  and  Latina  lesbians:  Cultural\nconstructions of race and gender within intersectionality-\nbased  facebook  groups,  in The  Intersectional  Internet:\nRace, Sex, Class and Culture Online, S. U. Noble and B.\nTynes, eds. New York, NY, USA: Peter Lang, 2016, pp.\n115–128.\n[5]\n J.  U.  Korn,  Expecting  penises  in  chatroulette:  Race,\ngender,  and  sexuality  in  anonymous  online  spaces,\nPopular Communication, vol. 15, no. 2, pp. 95–109, 2017.\n[6]\n J. U. Korn, From academia, to programmers: Critical race\ntraining  to  incorporate  race  into  ethics  and  technology,\npresented at Honoring all Expertise: Social Responsibility\nand Ethics in Tech, Cambridge, MA, USA, 2018.\n[7]\n J. U. Korn, Equitable cities instead of smart cities: Race and\nracism  within  the  race  for  smart  cities, Journal  of  Civic\nMedia, https://elabhome.blob.core.windows.net/downloads/\nJCM_fall2018_v2.pdf, 2018.\n[8]\n J. U. Korn, Race and resistance amid feminism, priming,\nand capitalism: The (surprisingly-globalized) visual of an\nAsian American woman activist, https://adanewmedia.org/\n2018/11/issue14-korn/, 2018.\n[9]\n J. U. Korn, Failures in ethics in training and praxis: The\nvalues  embedded  within  race,  technologies,  and  justice,\npresented  at  the  43rd Annu.  Meeting  Society  for  Social\nStudies of Science, New Orleans, LA, USA, 2019.\n[10]\n J. U. Korn, #IfTheyGunnedMeDown: How ethics, gender,\nand  race  intersect  when  researching  race  and  racism  on\n[11]\n    362\nJournal of Social Computing, December 2021, 2(4): 357−364    \n \n\nTumblr, Journal of Digital Social Research, vol. 1, no. 1,\npp. 41–44, 2019.\n A.  Denis,  Intersectional  analysis:  A  contribution  of\nfeminism  to  sociology, International  Sociology,  vol. 23,\nno. 5, pp. 677–694, 2008.\n[12]\n D.  Roithmayr,  Introduction  to  critical  race  theory  in\neducational research and praxis, in Race Is... Race Isn’t:\nCritical  Race  Theory  and  Qualitative  Studies  in\nEducation,  L.  Parker,  D.  Deyhle,  and  S.  Villenas,  eds.\nNew York, NY, USA: Taylor and Francis, 1999, pp. 1–6.\n[13]\n K.  Crenshaw,  Mapping  the  margins:  Intersectionality,\nidentity  politics,  and  violence  against  women  of  color,\nStanford Law Review, vol. 43, no. 6, pp. 1241–1299, 1991.\n[14]\n A.  K.  Wing,  Introduction,  in Critical  Race  Feminism:  A\nReader, A. K. Wing, ed. New York, NY, USA: New York\nUniversity Press, 2003, pp. 1–19.\n[15]\n K. Crenshaw, N. Gotanda, G. Peller, and K. Thomas, eds. ,\nCritical Race Theory: The Key Writings that Formed the\nMovement. New York, NY, USA: New Press, 1995.\n[16]\n K.  Crenshaw, On  Intersectionality:  Essential  Writings.\nNew York, NY, USA: New Press, 2015.\n[17]\n C.  Ess, Digital  Media  Ethics. Cambridge,  UK:  Polity\nPress, 2013.\n[18]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  technology  ethics  in  practice, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0018.\n[19]\n D. Jones, The representation of female athletes in online\nimages of successive Olympic Games, Pacific Journalism\nReview, vol. 12, no. 1, pp. 108–129, 2006.\n[20]\n J.  E.  McNealy,  Framing  and  the  language  of  ethics:\nTechnology,  persuasion,  and  cultural  context, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0027.\n[21]\n J.  Hemsley  and  J.  Snyder,  Dimensions  of  visual\nmisinformation  in  the  emerging  media  landscape,  in\nMisinformation and Mass Audiences, B. G. Southwell, E.\nA.  Thorson,  and  L.  Sheble,  eds.  Austin,  TX,  USA:\nUniversity of Texas Press, 2018, pp. 91–108.\n[22]\n F. Vis, A critical reflection on big data: Considering APIs,\nresearchers, and tools as data makers, First Monday, doi:\nhttps://doi.org/10.5210/fm.v18i10.4878.\n[23]\n D. W. Chambers, Stereotypic images of the scientist: The\ndraw-a-scientist  test, Science  Education,  vol. 67,  no. 2,\npp. 255–265, 1983.\n[24]\n F.  Zamudio-Suaréz,  Professors  are  nerds.  Or  so  your\niPhone would have you believe, The Chronicle of Higher\nEducation, https://www.chronicle.com/article/Professors-\nAre-Nerds-Or-So/237817, 2016.\n[25]\n S. Hussain, From indiana jones to Minerva Mcgonagall,\nprofessors  see  themselves  in  fiction, The  Chronicle  of\nHigher\n Education, \nhttps://www.chronicle.com/article/\nFrom-Indiana-Jones-to-Minerva/240308, 2017.\n[26]\n D. Polan, Professors, Discourse, vol. 16, no. 1, pp. 28–49,\n1993.\n[27]\n L.  S.  Lewis,  Students’ images  of  professors, The\nEducational Forum, vol. 32, no. 2, pp. 185–190, 1968.\n[28]\n D.  D.  McFarland,  Self-images  of  law  professors:\nRethinking the schism in legal education, Journal of Legal\nEducation, vol. 35, no. 2, pp. 232–260, 1985.\n[29]\n C.  Fisanick, “They  are  weighted  with  authority”:  Fat\nfemale  professors  in  academic  and  popular  cultures,\nFeminist Teacher, vol. 17, no. 3, pp. 237–255, 2007.\n[30]\n M.  Dagaz  and  B.  Harger,  Race,  gender,  and  research:\nImplications for teaching from depictions of professors in\npopular  film,  1985–2005, Teaching  Sociology,  vol. 39,\nno. 3, pp. 274–289, 2011.\n[31]\n N. Gotanda, A critique of ‘our constitution is color-blind’,\nStanford Law Review, vol. 44, no. 1, pp. 1–68, 1991.\n[32]\n D.  G.  Solorzano,  Critical  race  theory,  race  and  gender\nmicroaggressions,  and  the  experience  of  Chicana  and\nChicago  scholars, International  Journal  of  Qualitative\nStudies in Education, vol. 11, no. 1, pp. 121–136, 1998.\n[33]\n T.  Ebede  and  A.  Papier,  Disparities  in  dermatology\neducational resources, Journal of the American Academy\nof Dermatology, vol. 55, no. 4, pp. 687–690, 2006.\n[34]\n D. Prichep, Diagnostic gaps: Skin comes in many shades\nand  so  do  rashes, NPR, https://www.npr.org/sections/\nhealth-shots/2019/11/04/774910915/diagnostic-gaps-skin-\ncomes-in-many-shades-and-so-do-rashes, 2019.\n[35]\n B.  Hooks,  Eating  the  other:  Desire  and  resistance,  in\nMedia  and  Cultural  Studies:  Key  Works,  M.  G.  Durham\nand  D.  G.  Kellner,  eds.  Malden,  MA,  USA:  Blackwell\nPublishing, 2006, pp. 366–380.\n[36]\n J.  U.  Korn,  Teaching  digital  media  ethics  in  computer\nscience  to  improve  racial  Justice  in  technologies  and\nartificial  intelligence,  presented  at  Mozilla’s  Responsible\nComputer Science Challenge, Mountain View, CA, USA,\n2019.\n[37]\n C.  Fiesler,  N.  Garrett,  and  N.  Beard,  What  do  we  teach\nwhen  we  teach  tech  ethics?:  A  syllabi  analysis,  in Proc.\n51st ACM  Technical  Symposium  on  Computer  Science\nEducation, Portland, OR, USA, 2020, pp. 289–295.\n[38]\n M. Muro, A. Berube, and J. Whiton, Black and Hispanic\nunderrepresentation  in  tech:  It’s  time  to  change  the\nequation, Brookings, https://www.brookings.edu/research/\nblack-and-hispanic-underrepresentation-in-tech-its-time-\nto-change-the-equation, 2018.\n[39]\n R. Speer, How to make a racist AI without really trying,\nConceptNet, http://blog.conceptnet.io/posts/2017/how-to-\nmake-a-racist-ai-without-really-trying, 2017.\n[40]\n J.\n Buolamwini\n and\n T.\n Gebru,\n Gender\n shades:\nIntersectional  accuracy  disparities  in  commercial  gender\nclassification,  in Proc.  1st Conference  on  Fairness,\nAccountability, and Transparency, New York, NY, USA,\n2018, pp. 1–15.\n[41]\n S. Corbett-Davies, E. Pierson, and A. Feller, A computer\nprogram  used  for  bail  and  sentencing  decisions  was\nlabeled biased against blacks. It’s actually not that clear,\nThe  Washington  Post, https://www.washingtonpost.com/\nnews/monkey-cage/wp/2016/10/17/can-an-algorithm-be-\nracist-our-analysis-is-more-cautious-than-propublicas,\n2016.\n[42]\n N. Diakopoulos and S. Friedler, How to hold algorithms\naccountable, MIT  Technology  Review, https://www.\ntechnologyreview.com/s/602933/how-to-hold-algorithms-\naccountable, 2016.\n[43]\n  Jenny Ungbha Korn:   Connecting Race to Ethics Related to Technology: A Call for Critical Tech Ethics\n363    \n \n\n L.  Sweeney,  Discrimination  in  online  ad  delivery, Data\nPrivacy Lab at Harvard University, http://dataprivacylab.\norg/projects/onlineads/1071-1.pdf, 2013.\n[44]\n B. Green, Data science as political action: Grounding data\nscience in a politics of justice, Journal of Social Computing,\ndoi: 10.23919/JSC.2021.0029.\n[45]\n K.  Kirkpatrick,  Battling  algorithmic  bias:  How  do  we\nensure algorithms treat us fairly? Communications of the\nACM, vol. 59, no. 10, pp. 16–17, 2016.\n[46]\n W. Knight, Biased algorithms are everywhere, and no one\nseems  to  care, MIT  Technology  Review, https://www.\ntechnologyreview.com/s/608248/biased-algorithms-are-\neverywhere-and-no-one-seems-to-care, 2017.\n[47]\n S.  U.  Noble, Algorithms  of  Oppression:  How  Search\nEngines  Reinforce  Racism.  New  York,  NY,  USA:  New\nYork University Press, 2018.\n[48]\n D.  A.  Bell  Jr,  A  critique  of ‘our  constitution  is  color-\nblind’,  in Critical  Race  Theory:  The  Key  Writings  that\nFormed  the  Movement,  K.  Crenshaw,  N.  Gotanda,  G.\nPeller, and K. Thomas, eds. New York, NY, USA: New\nPress, 1995, pp. 302–314.\n[49]\n R.  L.  Brooks, Racial  Justice  in  the  Age  of  Obama.\nPrinceton, NJ, USA: Princeton University Press, 2009.\n[50]\n M.  Broussard, Artificial  Unintelligence:  How  Computers\nMisunderstand  The  World.  Cambridge,  MA,  USA:  MIT\nPress, 2018.\n[51]\n E.  S.  Fiorenza, But  She  Said:  Feminist  Practices  of\nBiblical Interpretation. Boston, MA, USA: Beacon Press,\n1992.\n[52]\n Y.  Beyene,  K.  Kumodzi,  and  D.  Simms,  Institutional\nracism lives at HKS, compromising its effectiveness as a\npublic  service  institution, The  Citizen, https://citizen.\nhkspublications.org/2019/11/29/institutional-racism-lives-\nat-hks-compromising-its-effectiveness-as-a-public-service-\ninstitution, 2019.\n[53]\nJenny Ungbha Korn is a feminist activist\nof color for social justice, a ciswoman scholar\nof race and gender in mass media, digital\nlife,  and  artificial  intelligence,  and  a\nmember  of  Mensa,  the  high  intelligence\nquotient (IQ) society. She is an affiliate and\nthe\n founding\n coordinator\n of\n the\nRace+Tech+Media  Working  Group  at  the\nBerkman  Klein  Center  for  Internet  and  Society  at  Harvard\nUniversity. She is the founding director of Princeton Diversity\nDiscussions, a free, public, ongoing series since 2014 of in-person\nand digital group gatherings to share personal opinions and lived\nexperiences focused on race, racism, and racial justice that meets\nweekly across the sponsorship of 30+ associations in the United\nStates. She is the author of numerous publications, she has won\nawards  from  the  Carl  Couch  Center  for  Social  and  Internet\nResearch;  the  Association  for  Information  Science  and\nTechnology; the Philosophy of Communication Division and the\nAfrican American Communication and Culture Division of the\nNational  Communication  Association;  the  Minorities  and\nCommunication  Division  and  the  Communication  Theory  and\nMethodology  Division  of  the  Association  for  Education  in\nJournalism and Mass Communication; and the Organization for\nthe  Study  of  Communication,  Language,  and  Gender.  She  has\ngiven over one hundred talks as invited keynote presentations,\nuniversity guest lectures, interactive community education, and\nrefereed conference presentations. As a public scholar, she has\nbeen  quoted  in  interviews  with NPR, CNN, SXSW, Bustle,\nColorlines, Fox News, Forbes, Mashable, Reader’s Digest, U.S.\nNews & World Report, Washington Post, and more. Drawing on\ncritical race and intersectional feminist theories, she explores how\nInternet  spaces  and  artificial  intelligences  influence,  and  are\ninfluenced, by assemblages of race and gender and how online\nproducers-consumers\n have\n constructed\n inventive\n digital\nrepresentations  and  computer-mediated  communications  of\nidentity.\n    364\nJournal of Social Computing, December 2021, 2(4): 357−364    \n \n\n \nCritical Technical Awakenings\nMaya Malik and Momin M. Malik*\nAbstract:    Starting with Philip E. Agre’s 1997 essay on “critical technical practice”, we consider examples\nof writings from computer science where authors describe “waking up” from a previously narrow technical\napproach to the world, enabling them to recognize how their previous efforts towards social change had been\nineffective. We use these examples first to talk about the underlying assumptions of a technology-centric\napproach to social problems, and second to theorize these awakenings in terms of Paulo Freire’s idea of critical\nconsciousness. Specifically, understanding these awakenings among technical practitioners as examples of this\nmore general phenomenon gives guidance for how we might encourage and guide critical awakenings in order\nto get more technologists working effectively towards positive social change.\nKey  words:   critical technical practice; critical consciousness; perspective transformation; education; machine\nlearning\n1    Introduction\nIn 1997, then-UCLA professor Philip E. Agre published\na  remarkable  essay,  entitled “Towards  a  Critical\nTechnical  Practice:  Lessons  Learned  in  Trying  to\nReform AI” [1]. In it, Agre describes his experience as a\ndoctoral student in AI at MIT in the 1980s undergoing\na crisis of faith in his discipline and looking to other\ndisciplines  for  answers.  Agre  writes  (bold  emphasis\nadded):\n“As an AI practitioner already well immersed in the\nAI  literature,  I  had  incorporated  the  field’s  taste  for\ntechnical  formalization  so  thoroughly  into  my  own\ncognitive  style  that I  literally  could  not  read  the\nliteratures of nontechnical fields at anything beyond\na  popular  level.  The  problem  was  not  exactly  that  I\ncould not understand the vocabulary, but that I insisted\non  trying  to  read  everything  as  a  narration  of  the\nworkings of a mechanism.\n“My  first  intellectual  breakthrough  came  when,  for\nreasons I do not recall, it finally occurred to me to stop\ntranslating  these  strange  disciplinary  languages  into\ntechnical schemata, and instead simply to learn them on\ntheir own terms.\n“I  still  remember  the  vertigo  I  felt  during  this\nperiod;  I  was  speaking  these  strange  disciplinary\nlanguages, in a wobbly fashion at first, without knowing\nwhat  they  meant—without  knowing  what sort of\nmeaning they had... in retrospect this was the period\nduring which I began to ‘wake up’, breaking out of\na  technical  cognitive  style  that  I  now  regard  as\nextremely constricting.”\nIn this paper, we use Agre’s essay as a foil to discuss\nwhat we call critical technical awakenings: when people\nfrom  technical  disciplines,  previously  committed  to  a\nnarrow technical view of the world, “wake up” from that\nperspective  to  what  we  identify  as  seeing  the  world\nthrough a critical, constructivist lens.\nOther articles in this special issue do a fantastic job of\nanalyzing the political economy of tech ethics[2, 3]. While\nrecognizing  that  structural  change  at  this  level  is  our\nultimate goal, our focus here is in taking up a specific\nslice of how to achieve this: what makes certain technical\n \n • Maya  Malik  is with  the  School  of  Social  Work,  McGill\nUniversity, Montréal, H3A 0G4, Canada. E-mail: maya.malik@\nmail.mcgill.ca.\n • Momin M. Malik is with  the Institute in Critical Quantitative,\nComputational,  &  Mixed  Methodologies,  Johns  Hopkins\nUniversity, Baltimore, MD 21218, USA. E-mail: momin.malik@\ngmail.com.\n * To whom correspondence should be addressed.\n    Manuscript\n received:\n 2021-05-20;\n revised:\n 2021-12-07;\naccepted: 2021-12-08\nJOURNAL  OF  SOCIAL  COMPUTING\nISSN   2688-5255   06/06  pp365−384\nVolume 2, Number 4, December  2021\nDOI:  10.23919/JSC.2021.0035\n \n©  The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n\npractitioners  come  to  care  about  understanding  this\nlarger  context,  and  how  do  some  individuals  become\ncommitted to working towards structural change? We do\nnot mean to imply that “ethics” are a problem at the level\nof  individuals;  but,  as  we  will  argue,  individual-level\nawakenings play a central role in building communities\nthat effectively work towards positive structural change,\nand so are crucial to consider.\nOur goal is not necessarily to convince people purely\nwithin a “technical perspective” that they should change\n(indeed, we argue that rational argumentation alone is\ninsufficient  to  cause  change),  but  rather  to  speak  to\npeople who are in the process of undergoing, or who\nhave  recently  undergone,  the  type  of  awakening  we\nidentify.  Awakenings  can  be  a  lonely  and  confusing\nprocess,  but  need  not  be.  By  pointing  to  existing\nexamples and theorizing this process, and by providing\nguidance about how to productively channel and shape\nawakenings,  we  hope  to  make  it  less  difficult  to  go\nthrough  an  awakening,  and  thereby  encourage  and\ncontribute to growing a community of critical technical\npractitioners\n within\n modern\n data\n practice\n and\ntechnology design.\nSpecifically, we aim to:\n• Review  the  existence  of  different  ways  of\napproaching  the  world  and  their  different  underlying\nassumptions (in Section 2);\n• Identify what is initially compelling about a\n“technical perspective”, but how and why some of its\nadherents  rightly  come  to  see  this  perspective  as\ninsufficient (in Section 3);\n• Draw on Paulo Freire’s idea of critical consciousness\nand subsequent theory from adult education[4], in order\nto theorize critical awakenings more broadly (in Section\n4);\n• Present a specific view of ethics and argue that this\nshould be the goal of critical technical awakenings (in\nSection 5);\n• Examine potential shortcomings of existing\nexamples  of  critical  technical  awakenings  in  light  of\nadult education’s prescriptive positions on what makes\na “complete” awakening,  and  by  advocating  for  a\ncare-based ethical code which the examples do not seem\nto have arrived at (in Section 6).\nAs a note, the awakenings we discuss are not technical\nin  nature.  Perhaps “critical-technical  awakenings”,\n“critical awakenings in tech”, or “critical sociotechnical\nawakenings” would  be  more  appropriate;  we  use  the\nphrase “critical technical awakenings” to emphasize a\nconnection to Agre’s critical technical practice and, in\ncontrast  to  other  examples  of  people  writing  about\n“critical awakenings”[5, 6] to emphasize the awakenings\nin  question  being  experienced  by  people  in technical\nfields.\n2    Paradigms of Social Research\nTraining in social research includes, as a basic part of any\nresearch  methods  course,  an  introduction  to  different\nresearch  paradigms.  For  people  who  carry  out  social\nresearch from a technical background, this may not be\nsomething they have been exposed to; but even if it is,\nthe abstract layout of different paradigms may not be\nmeaningful. To set up the remainder of the discussion,\nwe  first  present  our  take  on  the  standard  view  of  the\ncontours  of  social  research  in Table  1,  with  further\ndescriptions in a glossary Appendix, and try to point out\nhow  it  relates  to  a  technical  perspective  versus  what\npeople might awake to.\nThe rows correspond to subfields of philosophy, but\nhere more specifically and narrowly represent types of\nassumptions\n within\n that\n philosophical\n domain,\nrespectively about the nature of things (ontology), how\nwe can know  things  (epistemology),  and  how  we\nactually go about knowing things (methodology). While\nnot always present in charts like this one, axiology is an\nadditional  branch  of  philosophy  that  contains  ethics\n(what is good) and aesthetics (what is beautiful). Within\nthis, we specifically care about normative ethics, which\nare  choices  of  codes  of  conduct  to  which  we  should\nadhere (which are how we go about being ethical), as\nopposed to, say, descriptive ethics (descriptions of what\ncertain people believe to be ethical).\nThe columns represent different paradigms of social\nresearch,  and  the  cells  are  the  assumptions  that  each\nparadigm  makes.  These  assumptions  are  fundamental\nand  foundational,  and  cannot  be  debated,  justified,  or\nrefuted  through  empirical  means  (since,  among  other\nthings, these assumptions are about the very possibility,\nreliability, and even definition of empirical evidence). In\nthe  Appendix,  we  provide  a  glossary  with  extensive\ndescriptions of these columns and some specific terms\nthat appear in the cells.\nNeither the rows nor the columns are cleanly separated\nor singular; positions can bleed into one another, and a\n    366\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\nsingle  column  can  cover  a  variety  of  irreconcilable\ndifferent perspectives (for example, logical positivism\ntries to remove the ontological assumptions of realism\nfrom positivism’s quantitative empirical commitments,\nand  conversely,  mathematical  realism  often  disdains\nempiricism). We identify the purest form of a “technical\nperspective” as falling squarely within the “positivism”\ncolumn,  but  the  perspective  we  discuss  is  more\nspecifically  about  the  power  of  technology  to  effect\nsocial change.①\nThese  columns  are  not  exhaustive  or  mutually\nexclusive,  but  represent  useful  clusters.  But,  even\n \nTable 1    Assumptions of social research paradigms. Based on Guba and Lincoln’s “Basic beliefs (metaphysics) of alternative\ninquiry paradigms”[7]. See Appendix for details.\nIssue\nPositivism\nPostpositivism\nCritical theory\nConstructivism\nParticipatory\nOntology\n(assumptions\nabout the\nnature of\nthings)\nNaïve realism. Reality\nis independent of and\nprior to human\nconception of it, and\napprehensible.\nCritical realism: Reality\nis independent of and\nprior to human\nconception of it, but\nonly imperfectly and\napproximately\napprehensible.\nDisenchantment theory:\nthere is a reality, shaped\nby social, political,\ncultural, economic,\nethnic, and gender\nvalues and solidified\nover time, but it is\nsecret/hidden.\nRelativism: There are\nmultiple realities and\nexperiences of truth,\nconstructed in history\nthrough social\nprocesses.\nParticipative: multiple\nrealities, each co-\nconstructed through\ninteractions between\nspecific people and\nenvironments.\nEpistemology\n(assumptions\nabout how\ncan know\nthings)\nReality is knowable\nthrough reason and\nobservation. It is\npossible to have\nfindings that are\nsingular, perspective-\nindependent and\nneutral, atemporal, and\ntherefore universally\ntrue.\nFindings are\nprovisionally true;\nmultiple descriptions\ncan be valid but are\nprobably equivalent;\nfindings can be\naffected/distorted by\nsocial and cultural\nfactors.\nThe truth of findings is\nmediated by their value;\nhow we come to know\nsomething, or who\ncomes to know\nsomething, matters for\nhow meaningful it is.\nRelativistic: there\nis no neutral or\nobjective perspective\nfrom which to\nadjudicate competing\nperspective or truth\nclaims; truth is\nrelative to a given\nperspective.\nWe come to know\nthings, and create new\nunderstandings that can\ntransform the world, by\ninvolving other people\nin the process of\ninquiry.\nMethodology\n(how we go\nabout trying to\nknow things)\nExperimental/\nmanipulative\n(hypothetico-\ndeductive); hypotheses\ncan be verified as true.\nChiefly quantitative\nmethods, and\nmathematical\nrepresentation.\nModified experimental/\nmanipulative;\nfalsification of\nhypotheses; primacy of\nquantitative methods,\nbut may include\nqualitative and mixed\nmethods.\nDialogic (through\nconversation and\ndebate) or dialectical\n(through a process of\nthesis, antithesis, and a\nsynthesis which\nbecomes a new thesis)\nDialetical, or\nhermeneutical (a\nprocess of reading\nsources “against\nthemselves” to\nidentify\ninconsistencies,\nunderlying\nassumptions, or\nimplicit messages,\nand thereby interpret\nmeaning).\nCollaborative, action-\nfocused; flattening\nresearcher/\nparticipant hierarchies;\nengaging in self- and\ncollective reflection;\njointly deciding to\nengage in individual or\ncollective action.\nAxiology\n(ethics;\nvalues; who\nmatters, who\nis important,\nwho has\nstanding)\nKnowledge achieved\nthrough hypothetico-\ndeductive means is\nmore valuable than\nother knowledge. The\npeople who can carry\nout such investigation\nhave privileged access\nto the truth, and thus\nhave a special role and\nimportance (and\npotentially a special\nresponsibility).\nKnowledge achieved\nthrough hypothetico-\ndeductive is more\nvaluable, but can be\ndistorted by\nsocial/cultural factors,\nand this can sometimes\nonly be uncovered by\nqualitative means and\ninsight. Qualitative\nmethods can provide\nchecks and context, or\nraw material\nfor quantification.\nMarginalization is what\nis most important;\nexperience of\nmarginalization\nprovides unique\ninsights, and the\nknowledge of the\nmarginalized is more\nvaluable than the\nknowledge of\ndominant/legitimate\nparadigms.\nUnderstanding the\nprocess of\nconstruction is what\nis valuable; value\n(including valuing\nunderstanding the\nprocess of\nconstruction) is\nrelative to a given\nperspective.\nEveryone is valuable.\nReflexivity, co-created\nknowledge, and non-\nwestern ways of\nknowing are valuable\nand combat erasure and\ndehumanization.\n \n① This  includes  the  perspective  of  technological  determinism,  a\nposition largely rejected in social science that holds that given technology\ninherently  effects  certain  causal  changes,  independent  of  context.  See\nGreen’s article in this special issue[8] for details. A softer version allows\nfor context as a moderator, but still sees technology as having inherent\ncausal power.\n  Maya Malik et al.:   Critical Technical Awakenings\n367    \n \n\nbeyond  this,  as  individuals  we  human  beings  can  be\ninconsistent  or  even  contradictory  in  the  sets  of\nassumptions  we  make  (crossing  multiple  columns  at\ndifferent times or even at once), and we may not even be\nself-aware of the underlying assumptions we are making.\nTechnical  disciplines  in  particular  are  frequently\npositivist without realizing that it is a specific position,\nor that it is not the only way to see the world. Part of\nundergoing a critical awakening is coming to be aware\nthat a technical perspective is only one way of looking\nat the world, and starting to recognize its core underlying\nassumptions—and reject them.\n3    The Technical Perspective\nOne piece of Agre’s argument is about the importance\nof taking AI seriously:\n“The central practice of the field of AI, and its central\nvalue,  was  technical  formalization.  Inasmuch  as  they\nregarded technical formalization as the most scientific\nand  the  most  productive  of  all  known  intellectual\nmethods, the field’s most prominent members tended to\ntreat their research as the heir of virtually the whole of\nintellectual history. I have often heard AI people portray\nphilosophy, for example, as a failed project, and describe\nthe social sciences as intellectually sterile. In each case\ntheir  diagnosis  is  the  same:  lacking  the  precise  and\nexpressive  methods  of  AI,  these  fields  are  inherently\nimprecise, woolly, and vague. Any attempt at a critical\nengagement  with  AI  should  begin  with  an\nappreciation of the experiences that have made these\nextreme views seem so compelling.”\nThe target of Agre’s critique (and the focus of the first\nhalf of his essay) is the AI that existed in the 1980s and\n1990s, a very specific and peculiar field (seeing itself as\nseeking to understanding mechanisms of cognition, in\ncontrast  to  the  machine  learning  of  today  which  is\ninstrumentally focused on achieving specific tasks and\neffectively  unconcerned  with  cognition;  see  Ref.  [9]).\nBut  the  same  logic  remains:  we  begin  a  critical\nengagement with an appreciation of the experiences that\nmake extreme technical views seem so compelling.\nThe specific “technical perspective” we refer to here\nis  a  position  around  computation  and  digital\ntechnology  and  has  been  identified  and  critiqued\nunder  a  series  of  related  terms:  Morozov’s “tech\nsolutionism”[10]; Toyama’s “tech commandments”[11];\nBroussard’s “tech  chauvinism”[12];  and  Green’s “tech\ngoggles”[13].  These  labels  emphasize  something  about\nthe  arrogance  and  absolutism  of  the  technical\nperspective, and all authors emphasize how adherents\nare dazzled by the apparent ability of technology (or, if\nengaging  more  with  the  intellectual  content  than  the\nmaterial artifacts, being dazzled by the apparent power\nof  formalizing  goals,  operations,  and  human  concepts\ninto  mathematical  and/or  software  abstractions)  to\ncontrol and change the world.\nAs  we  noted  above,  at  their  purest,  technical\nperspectives fall purely within the “positivism” column.\nWe first review the overall appeal of positivism, before\nfocusing specifically on its tech solutionist variety.\nA  statement  by  physicists  Jean  Bricmont  and  Alan\nSokal[14] provides a pure expression:\n“In the same way that nearly everyone in his or her\neveryday\n life\n disregards\n solipsism\n and\n radical\nskepticism  and  spontaneously  adopts  a ‘realist’ or\n‘objectivist’ attitude  toward  the  external  world,\nscientists spontaneously do likewise in their professional\nwork.  Indeed,  scientists  rarely  use  the  word ‘realist,’\nbecause it is taken for granted: of course they want to\ndiscover (some aspects of) how the world really is! And\nof course they adhere to the so-called correspondence\ntheory  of  truth  (again,  a  word  that  is  barely  used):  if\nsomeone says that it is true that a given disease is caused\nby  a  given  virus,  she  means  that,  in  actual  fact,  the\ndisease is caused by the virus.\n“We  would  not  even  call  it  a ‘theory’;  rather,  we\nconsider  it  a precondition  for  the  intelligibility of\nassertions about the world.”\nThis captures something about the aesthetic appeal of\npositivism and specifically its realist ontology: the world\nis fundamentally knowable. Furthermore, the technical\nperson experiences the satisfaction of having command\nof the sole means by which to achieve that knowledge.\nWhile, as suggested in this quote, this perspective is\nwidespread  in  the  natural,  mathematical,  or “hard”\nsciences, “positivism” was  actually  coined  as  an\naspiration  for  social  science  in  the  19th  century  (see\nAppendix).  Past  that  period,  Porter[15] describes\npost-WWII\n behavioralists\n adopting\n quantitative\nmethodologies in social science in pursuit of “liberating\nessence of a proper objective methodology” that could\n“rise  above  stubborn  tradition  and  invisible  culture”\n(emphasis added). That is, they pursued a vision where\nit is possible to know how the social world “really is”,\nsuch  that  it  is  possible  to  have  intelligible  assertions\nabout  it  (rather  than “stubborn  tradition  and  invisible\n    368\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\nculture” getting in the way of intelligibility).\nThis idea of liberation through science leads to a view\nwhere  quantification  and  formalization  are  not  only\npractically superior,  but morally superior  as  well.\nEverything else in the world is anecdotal evidence, naïve\nheuristics,  and  armchair  philosophy—shackles  of\nignorance  either  useless  for  accomplishing  concrete\ngoals  and  characterized  only  by  failure,  or  achieving\nsuccess only through sheer luck or cheap trickery. That\nis, even if there is a case where technical approaches are\nnot practically superior (like, for example, convincing\nclimate  change  deniers),  there  is  a  view  that  they  are\nmorally superior:  even  if  attempting  to  understand  or\nintervene  in  the  world  through  means  other  than\nabstraction (i.e., through means like through rhetoric, or\nnarrative) may succeed, those alternatives are dishonest,\nunprincipled,  or  otherwise  somehow  ignoble  and\ncompromise our moral integrity.\nIn  addition  to  this  intrinsic  moral  superiority,\npositivism  seems  to  comport  well  with  a  basis  for\nmorality. An observer-independent external world also\njustifies universal morality—a standard which we can\nhope  to  define,  and  then  appeal  to  for  solving  moral\nquestions. Indeed, in the so-called “science wars” of the\n1990s, when some scientists (initially led by Alan Sokal)\ntook up arms against what they saw as the “fashionable\nnonsense” of science and technology studies (and related\nareas), those scientists also bemoaned that while they\nand the “postmodernists” seemed to share progressive\npolitical  goals  of  greater  justice  and  equity,  the\npostmodern perspective was undermining the basis for\npursuing that goal and the basis of forming coalitions.\nEven  worse  than  getting  in  the  way, “postmodern”\narguments are in fact deployed in support of[16, 17] and\nby climate change deniers, creationists, and all sorts of\nreligious nationalists and right-wing movements across\nthe world. These reactionary elements of society seek to\nundermine  the  legitimacy  of  science  in  pursuit  of  a\nregressive  political  agenda,  and  while  they  clearly\nbelieve in a single reality (corresponding to their own\nbeliefs),  they  co-opt  language  around  plurality  and\nrelativism to prevent critique. One of the more forceful\narguments around this is by Nanda[18], who argues how\nEnlightenment beliefs in universality are what we need\nto  defend  against  perspectives  like  those  of  Hindu\nnationalists, whose weaponization of science studies she\ndocuments.\nThe computation- and technology-focused variety of\npositivism  discussed  by  Morozov[10],  Toyama[11],\nBroussard[12],  and  Green[13] is  not  necessarily  about\nunderstanding  the  world,  but  about  acting  within  it.\nToyama discusses (before undergoing what seems like\nan  awakening)  thinking  technology  addresses “real\nproblems”; that both means that the problems are prior\nto  and  independent  of  the  perspective  of  the\ntechnologists, and that technology in itself can actually\naddress  and  solve  those  problems.  Morozov  lists\nexamples  of  Silicon  Valley  rhetoric  about  technology\nchanging  the  world  and  solving  global  problems.  He\nsummarized the implicit technologist vision of the future\nin a satirical prediction:\n“If Silicon Valley had a designated futurist, her bright\nvision of the near future... would go something like this:\nHumanity, equipped with powerful self-tracking devices,\nfinally conquers obesity, insomnia, and global warming\nas  everyone  eats  less,  sleeps  better,  and  emits  more\nappropriately.  The  fallibility  of  human  memory  is\nconquered too, as the very same tracking devices record\nand store everything we do. Car keys, faces, factoids:\nWe will never forget them again...\n“Politics, finally under the constant and far-reaching\ngaze  of  the  electorate,  is  freed  from  all  the  sleazy\ncorruption,  backroom  deals,  and  inefficient  horse\ntrading.  Parties  are  disaggregated  and  replaced  by\nGroupon-like  political  campaigns,  where  users  come\ntogether—once—to  weigh  in  on  issues  of  direct  and\nimmediate  relevance  to  their  lives,  only  to  disband\nshortly afterward. Now that every word—nay, sound—\never  uttered  by  politicians  is  recorded  and  stored  for\nposterity,  hypocrisy  has  become  obsolete  as  well.\nLobbyists of all stripes have gone extinct as the wealth\nof data about politicians—their schedules, lunch menus,\ntravel  expenses—are  posted  online  for  everyone  to\nreview...\n“Crime  is  a  distant  memory,  while  courts  are\noverstaffed and underworked. Both physical and virtual\nenvironments—walls,  pavements,  doors,  and  log-in\nscreens—have  become ‘smart.’ That  is,  they  have\nintegrated  the  plethora  of  data  generated  by  the\nself-tracking devices and social-networking services so\nthat now they can predict and prevent criminal behavior\nsimply by analyzing their users. And as users don’t even\nhave the chance to commit crimes, prisons are no longer\nneeded  either.  A  triumph  of  humanism,  courtesy  of\n  Maya Malik et al.:   Critical Technical Awakenings\n369    \n \n\nSilicon Valley.”\nThis  is  a  synthetic  caricature,  but  we  can  use  it  to\ndiscuss what might be compelling in the perspective that\nMorozov identifies and critiques. There is a view that\ntechnology is practically superior, in that it will succeed\nwhere  stubborn  tradition  and  invisible  culture  have\nfailed. But also, tradition and culture are the cause of\nsocial  problems  in  the  first  place;  technology  is  not\ncompromised  by  their  failings,  and  thus  to  approach\nsocial problems with technology rather than society is a\nmorally superior and more responsible move.\nThere is an ignominious aspect of the appeal of this\ntechnical perspective as well, which Broussard shows.\nShe argues that technologists, who are frequently white,\nmale, and upper-class, fixate on technology as a way to\ntry and solve social problems traditionally managed by\npeople who are Black, women, and/or poor. These men\nseek  to  use  technology  to  avoid  engaging  with  the\ncomplex  and  messy  labor  and  understandings  these\ngroups  have  mobilized  to  manage  and  address  social\nproblems.  That  is,  part  of  the  appeal  to  the  technical\nperspective is a chauvinistic one: of providing a means\nto distance oneself from the knowledge, labor, and even\nexistence  of  devalued  people  who  are  women  and/or\nnon-white.  If  we  just  invent  the  right  device,\nformalization, or processes, the thinking goes, we can\navoid needing to deal with all the ambiguities, nuances,\nand  emotional  labor  with  which,  say,  Black  women\nsocial workers engage.\nThese are the appeals of a technical perspective. What,\nthen,  leads  people  away  from  it?  In  awakenings,  a\ncommon  theme  seems  to  be  a  precipitating  event  or\nmoment that put the sleeper into a moment of crisis. For\nAgre, what he described is fairly abstract and intellectual:\nwhen trying to decide on a dissertation topic, he found\nthat “Every topic I investigated seemed driven by its own\npowerful internal logic into a small number of technical\nsolutions, each of which had already been investigated\nin the literature”. In his description, it was his search for\na  novel  topic  led  him  to  read  the  literatures  of  other\ndisciplines.\nAgre  does  allude  to  a “large  and  diverse  set  of\nhistorical  conditions” beyond  what  he  presents  in  the\nessay. But as he does not elaborate on this, we turn to two\nother  examples  of  described  awakenings,  respectively\nfrom Kentaro Toyama and Phil Rogaway.\nFirst, we consider Kentaro Toyama, who rejected a\ntechnical perspective in a rather “scientific” way. In his\nbook Geek Heresy: Rescuing Social Change from the\nCult of Technology[11], he describes working after his\nPhD  on “ICT4D”-type  projects  (Information  and\nCommunication  Technologies  for  Development)  for\nMicrosoft  in  India.  His  position  involved  expanding\ntechnology\n products’ \naudiences\n beyond\n the\neducationally advantaged Indian middle class to try and\nhelp  those  in  poverty.  But  he  repeatedly  found  his\nattempted interventions failing.\n“In  the  course  of  five  years,  I  oversaw  at  least  ten\ndifferent\n technology-for-education\n projects.\n We\nexplored  video-recorded  lessons  by  master  teachers;\npresentation  tools  that  minimized  prep  time;  learning\ngames  customizable  through  simple  text  editing;\ninexpensive  clickers  to  poll  and  track  student\nunderstanding;  software  to  convert  PowerPoint  slides\ninto discs for commonly available DVD players; split\nscreens to allow students to work side by side; and on and\non. Each time, we thought we were addressing a real\nproblem. But while the designs varied, in the end it didn’t\nmatter—technology never made up for a lack of good\nteachers  or  good  principals.  Indifferent  administrators\ndidn’t suddenly care more because their schools gained\nclever  gadgets;  undertrained  teachers  didn’t  improve\njust because they could use digital content; and school\nbudgets didn’t expand no matter how many ‘cost-saving’\nmachines  the  schools  purchased.  If  anything,  these\nproblems  were  exacerbated  by  the  technology,  which\nbrought its own burdens.\n“These revelations were hard to take. I was a computer\nscientist, a Microsoft employee, and the head of a group\nthat aimed to find digital solutions for the developing\nworld.  I  wanted  nothing  more  than  to  see  innovation\ntriumph, just as it always did in the engineering papers\nI  was  immersed  in.  But  exactly  where  the  need  was\ngreatest, technology seemed unable to make a difference.”\nThis was “scientific” in the sense that Toyama was\nopen to evidence by which he tested his assumption that\ntechnical tools can circumvent the messiness of society.\nBut the fact that he was even able to recognize that he had\nsuch foundational assumptions is not a given; Toyama\ncontrasts his insights to the perspective of a prominent\ntechnologist,  One  Laptop  Per  Child  founder  Nicholas\nNegroponte:\n“I was once on a panel at MIT with Negroponte where\nI  outlined  my  hard-won  lessons  about  technology  for\neducation. He didn’t like what I said, and he went on the\n    370\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\noffensive. But he did it with such confidence and self-\nassurance that, as I listened, I felt myself wanting to be\npersuaded: Children are naturally curious, aren’t they?\nWhy wouldn’t they teach themselves on a nice, friendly\nlaptop?\n“As I heard more of the technology hype, however, I\nrealized that it didn’t engage with rigorous evidence. It\nwas  empty  sloganeering  that  collapsed  under  critical\nthinking.”\nThat is, many scientists and technologists are not, in\nthis sense, open to a particular type of empirical evidence.\nThis is not inherently bad or even “unscientific”—work\nin  the  history,  sociology,  and  philosophy  of  science\npoints  out  that  interpretations  of  empirical  evidence\nrequire layers of theories and assumptions[19], including\nthe idea that evidence can be erroneous due to human\nerror, issues with instrumentation, or natural variability.\nIndeed,  skepticism  of  evidence  that  challenges\nestablished theory is an important part of science: but\nthis is all to say, evidence alone is not enough to change\nminds,  such  as  in  an  awakening.  Kuhn[20] famously\ntheorized that one-off failures in experimental science\nseldom  affect  theory,  but  strings  of  failures  can\nprecipitate  a crisis,  potentially  leading  to  a paradigm\nshift in  understanding  and  defining  basic  scientific\nconcepts differently (and, conversely, it takes a crisis and\nnot  simply  routine  failures  to  produce  a  paradigm\nshift).\nSecond, we look at the account of cryptographer Phil\nRogaway  in  his  essay, “The  Moral  Character  of\nCryptographic  Work”[21].  For  Rogaway  as  well,  there\nwas a discrete empirical event that led to his identifying\nand rethinking some fundamental assumptions, but here\nthe challenge posed was a moral one rather than one of\nassumptions  about  how  the  world  works  not  fitting\nevidence.\n“Most academic cryptographers seem to think that our\nfield is a fun, deep, and politically neutral game—a set\nof  puzzles  involving  communicating  parties  and\nnotional adversaries. This vision of who we are animates\na  field  whose  work  is  intellectually  impressive  and\nrapidly  produced,  but  also  quite  inbred  and  divorced\nfrom  real-world  concerns.  Is  this  what  cryptography\nshould be like? Is it how we should expend the bulk of\nour intellectual capital?\n“For  me,  these  questions  came  to  a  head  with  the\nSnowden  disclosures  of  2013.  If  cryptography’s  most\nbasic  aim  is  to  enable  secure  communications,  how\ncould  it  not  be  a  colossal  failure  of  our  field  when\nordinary people lack even a modicum of communication\nprivacy  when  interacting  electronically?  Yet  I  soon\nrealized that most cryptographers didn’t see it this way.\nMost  seemed  to  feel  that  the  disclosures  didn’t  even\nimplicate us cryptographers.”\nAlso noteworthy is how both Rogaway and Toyama\n(and Agre as well) describe resistance from their peers\nto their crisis of faith, and how the experience that led to\ntheir transformation did not succeed in triggering others.\nThis contrast again emphasizes that evidence, or external\ntriggers, are not sufficient to cause an awakening; they\nare only catalysts for already-existing potential.\nThese  accounts  do  not  reflect  on  what  made  their\nauthors  different  from  their  peers.  But  understanding\nthese accounts through the lens of adult education and\nspecifically  work  on  critical  consciousness  (see\nAppendix), below, will help fill in key answers.\nWe can also contrast these descriptions to others who,\nwhile  recognizing  the  limitations  of  purely  technical\napproaches, remain within a positivist paradigm (or, at\nmost, soften to a post-positivist one).\nPhysicist\n and\n applied\n mathematician\n turned\nsociologist  Duncan  Watts[22] wrote  that “many  of  the\nideas and metrics of the ‘new’ science of networks have\neither  been  borrowed  from,  or  else  rediscovered\nindependently  of,  a  distinguished  lineage  of  work  in\nmathematics,\n economics,\n and\n sociology”,\nacknowledging  sociological  contributions  but  reading\nthem in an essentially positivist light. Another person\ntrained in physics and working in network science, César\nHidalgo[23],  wrote  about  realizing  why “social  and\nnatural scientists fail to see eye to eye”: “Social scientists\nfocus  on  explaining  how  context  specific  social  and\neconomic mechanisms drive the structure of networks\nand  on  how  networks  shape  social  and  economic\noutcomes. By contrast, natural scientists focus primarily\non\n modeling\n network\n characteristics\n that\n are\nindependent of context, since their focus is to identify\nuniversal characteristics of systems instead of context\nspecific  mechanisms”.  This  again  positions  social\nscience’s  role  by  reference  to  the  task  of  finding\nuniversal and objective truths, rather than understanding\nthat (at least some) social science rejects the idea that\nthere could be universal characteristics.\nA  more  personal  potential  example  is  Hannah\n  Maya Malik et al.:   Critical Technical Awakenings\n371    \n \n\n,\nWallach’s viewpoint, “Computational Social Science \nComputer Science + Social Data”[24]. In this she writes,\n“Despite all the hype, machine learning is not a be-all\nand end-all solution. We still need social scientists if we\nare  going  to  use  machine  learning  to  study  social\nphenomena  in  a  responsible  and  ethical  manner.” A\ndilemma was only hinted at:\n“When I first started working in computational social\nscience,  I  kept  overhearing  conversations  between\ncomputer  scientists  and  social  scientists  that  involved\nsentences like, ‘I don’t get it—how is that even research?’\nAnd I could not understand why. But then I found this\nquote  by  Gary  King  and  Dan  Hopkins—two  political\nscientists—that, I think, really captures the heart of this\ndisconnect: ‘computer  scientists  may  be  interested  in\nfinding the needle in the haystack—such as... the right\nWeb page to display from a search—but social scientists\nare  more  commonly  interested  in  characterizing  the\nhaystack.’\n“In other words, the conversations I kept overhearing\nwere occurring because the goals typically pursued by\ncomputer  scientists  and  social  scientists  fall  into  two\nvery  different  categories...  models  for  prediction  are\noften  intended  to replace human  interpretation  or\nreasoning, whereas models for explanation are intended\nto inform or guide human reasoning.”\nBut what she describes overall only goes so far as to\nrecognize\n the\n importance\n of \nquantitative \nsocial\nscience—areas  of  economics  like  econometrics  and\ngame theory, and political science, all of which build\nformal  models  for  the  task  of  causal  understanding.\nThere is no mention of “thick” disciplines that do not use\nquantitative  modeling,  such  as  cultural  anthropology,\ncritical  sociology,  critical  race  studies,  human\ngeography,  critical  gender  studies,  media  studies,  or\ncultural  studies,  let  alone  any  mention  of  experiential\nways of knowing outside of academic disciplines.\nLike with Agre, from this piece alone it is impossible\nto know if this encapsulates Wallach’s understandings,\nor  if  it  is  rhetorical  strategy  (indeed,  in  a  later  piece,\nWallach[25] seems  to  go  beyond  post-positivism  in\nrecognizing  that  the  notions  of “objectivity” are  both\nill-defined and not desirable, as well as acknowledging\npositionality② [see  Appendix]).  After  all,  it  is  much\neasier to convince computer scientists of the value of the\nformalism- and data-heavy discipline of economics than\nof  interpretive  disciplines  like  cultural  studies,  or  of\nknowledge that comes from lived experience.\n4    Critical Awakenings\nEarlier, we mentioned Kuhn’s idea of paradigm shifts.\nRecognizing  that  this  may  be  too  simple  a  model  for\nscientific  development[26],  Mezirow[27, 28] offers  a\nsimilar  model  but  instead  describing  individual\npsychosocial development, which he called perspective\ntransformation.  More  immediately,  Mezirow’s  idea\ncomes from the work of Paulo Freire and his idea of\ncritical consciousness (see Appendix), and has a robust\nbody  of  follow-up  work  investigating  the  idea\nempirically[29] and developing it theoretically[30−32]. We\nwill  also  draw  on  subsequent  work  that  has  noted\nshortcomings in Mezirow’s theory not going far enough\nin considering context, other cultural settings, and the\nsignificance of interpersonal relationships[32].\nPerspective  transformation  came  from  Mezirow’s\nstudy  with  women  who  re-entered  college  programs\nmid-life.  He  identified  the  ultimate  value  of  such\nprograms as being in the personal transformation that\ntook place among the women, rather than any material\noutcomes. He theorized 10 stages of this process:\n“(1) A disorienting dilemma;\n“(2)  Self-examination  with  feelings  of  fear,  anger,\nguilt, or shame;\n“(3) A critical assessment of assumptions and a sense\nof  alienation  from  taken-for-granted  social  roles  and\nexpectations;\n“(4) Recognition that one’s discontent and the process\nof  transformation  are  shared  and  that  others  have\nnegotiated a similar change;\n“(5)\n Exploration\n of\n options\n for\n new\n roles,\nrelationships, and actions;\n“(6) Planning a course of action;\n“(7) Acquiring knowledge and skills for implementing\none’s plans;\n“(8) Provisional trying of new roles;\n“(9) Building competence and self-confidence in new\nroles and relationships;\n“(10) A reintegration into one’s life on the basis of\nconditions dictated by one’s new perspective.”\nThese ten stages are somewhere between descriptive\nand  normative.  They  are  descriptive,  insofar  as  they\n② “Will these changes of always having a sociotechnical lens make\nmachine  learning  less  fun?  Maybe,  for  some  people.  But  that  is  their\nprivilege  talking  about  their  ethical  debt.  Machine  learning  has  never\nbeen all that fun for people who are involuntarily represented in datasets\nor  subject  to  uncontestable  life-altering  decisions  made  by  machine\nlearning systems.”\n    372\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\ndescribe  a  process  undergone  by  the  subjects  of\nMezirow’s  study,  but  normative,  insofar  as  Mezirow\nidentified  perspective  transformation  as  something\nvaluable  and  possibly  aided  by  knowing  about  this\nsequence in advance and following it (following Freire,\nand  the  idea  of  critical  consciousness  as  a  normative\ngoal). While this alone does not necessarily shed light on\nwho would  experience  a  dilemma  as  disorienting  and\nchange in response (since Mezirow encountered women\nalready  pursuing  a  change),  it  does  point  to  how  this\nchange  does  not  happen  in  isolation,  and  indeed  how\nconnecting with others who have negotiated a similar\nchange  is  key  for  shaping  awakenings  towards\nproductive  ends.  But  Mezirow[33] does  provide  an\nanswer  for  the  question  of  what  is  needed  beyond\nevidence, observing that an additional condition is that\na  person reflect about  assumptions  and  beliefs  that\nstructured  how  they  understood  an  experience  (or\nevidence).\nAlso  noteworthy  are  the  examples  of  disorienting\ndilemmas:  they  included “the  death  of  a  husband,  a\ndivorce, the loss of a job, a change of city of residence,\nretirement, an empty nest, a remarriage, the near fatal\naccident of an only child, or jealousy of a friend who had\nlaunched a new career successfully”. In comparison, the\ndilemmas of Agre, Toyama, and Rogaway are decidedly\nelite and privileged experiences. Still, we can identify\ncritical  technical  awakenings  as  a  specific  form  of  a\nmuch\n more\n general\n phenomenon\n of\n critical\nconsciousness,  thus  making  it  appropriate  to  theorize\nwith perspective transformation.\nThere are several lessons to draw from this connection.\nThe first is how critical technical awakenings may relate\nto  critical  consciousness  (CC)  overall.  Jemal[34] notes\nthat  much  work  on  critical  consciousness  has\ndeliberately excluded privileged populations, but argues\nthis  exclusion “...may  inadvertently  support  the\nproposition  that  oppression  is  a  problem  for  the\noppressed to solve. When, in essence, CC is important\nfor  members  of  privileged  groups  who  have  greater\naccess to resources and power and can operate as allies\nprivileged  by  the  system  of  social  injustice,  unfair\ndistribution of resources and opportunities, and inequity,\nbe able to recognize unjust social processes and acquire\nthe knowledge and skills needed for social change.”\nDrawing from Freire, she continues：\n“It is imperative that those who may be privileged by\nthe  system  of  social  injustice,  unfair  distribution  of\nresources  and  opportunities,  and  inequity,  be  able  to\nrecognize  unjust  social  processes  and  acquire  the\nknowledge  and  skills  needed  for  social  change...  CC\nwould help individuals understand their role in a system\nof  oppression,  as  members  of  either  the  privileged  or\nstigmatized groups. Liberation requires true solidarity in\nwhich the oppressor not only fights at the side of the\noppressed, but also takes a radical posture of empathy by\n‘entering into the situation of those with whom one is\nsolidary’.[35] Thus, CC, with the goal of liberation, has\nthe  radical  requirement  that  the  oppressor,  those  who\ndeny  others  the  right  to  speak  their  word,  and  the\noppressed, those whose right to speak has been denied,\nmust collaborate to transform the structures that beget\noppression.[35]”\nThe second is that all of the descriptions of possible\ncritical technical awakenings do not recognize “that one’s\ndiscontent and the process of transformation are shared\nand that others have negotiated a similar change”. From\nthe perspective of Mezirow’s theory, this means they fall\nshort. Indeed, our article here is an attempt to directly\naddress the fragmentary nature of narratives of critical\ntechnical awakenings, and to draw connections between\npeople’s  experiences.  We  can  also  continue  the\nnormative route, and note that in order to fully achieve\nthe  potential  for  social  change  from  critical  technical\nawakenings, we should try to see how to continue past\nstage (5) and on to stages (6)−(10).\nWhat might new roles (stages (5)−(9)) be, in which\ntechnical  practitioners  should  build  competence  and\nself-confidence,  and  make  provisional  efforts?  We\nsuggest that one role might be in opposing gatekeeping.\nIt is rare even for qualitative researchers to have a seat\nat  the  table  of  technological  adoption,  let  alone\ncommunities affected by it. But by leveraging the social\nstanding  that  comes  with  quantitative  legitimacy,  and\ntranslating  concerns  into  terms  that  are  (more)\nacceptable  for  technical  audiences  as  a  first  step,\ntechnical  practitioners  can  help  bring  others  into  the\nprocesses  of  technology  development—whether  to\nparticipate, or to oppose development and deployment\nthat does not empower those communities.\nThe relationships that come with those roles would be\nwith allies outside of technical disciplines and sectors,\nand particularly through learning from and working with\ncommunities affected by technology (whether directly,\n  Maya Malik et al.:   Critical Technical Awakenings\n373    \n \n\nby  a  technology  itself,  or  indirectly,  such  as  in\ngentrification resulting from real estate expansions by\nthe tech industry or of universities who receive influxes\nof tech money). These would be new roles not only for\nthe technical practitioners, but indeed new social roles,\nand  would  require  weathering  all  the  difficulties  of\nnegotiating roles outside of recognized categories.\nDrawing on the follow-up work to Mezirow, we also\ndraw  attention  to  the  importance  of  looking  at\nperspective  transformations  outside  of  frames  of\nself-realization[31], and indeed outside of depicting the\nprocess as a deeply rational one in molds of western\nrationality.  One  example  is  a  study  that  identifies\ndisorienting dilemmas among women in Botswana that\nled to questioning assumptions, but with the value of the\noutcome\n being\n oriented\n towards\n the\n spiritual,\ncommunity responsibility and relationships, and gender\nroles[36]. Indeed, acknowledging other ways of knowing\nthat  are  not  expressed  in  the  language  of  rationality\nmakes  perspective  transformation  far  less  novel.\nJohnson-Bailey[37],  coming  from  the  perspective  of  a\nBlack woman, writes about “transformational learning\nas the only medium in which we exist, learn, and teach.\nSince it is the air we breathe, maybe we just take it for\ngranted and didn’t attend to or claim it sufficiently.” This\nis also an example of a more general issue; in “The Race\nfor  Theory”,  Barbara  Christian[38] wrote, “people  of\ncolor  have  always  theorized—but  in  forms  quite\ndifferent from the Western form of abstract logic... our\ntheorizing is often in narrative forms, in the stories we\ncreate, in riddles and proverb, in the play with language,\nsince dynamic rather than fixed ideas seem more to our\nliking. How else have we managed to survive with such\nspiritedness the assault on our bodies, social institutions,\ncountries our very humanity? ...My folk, in other words,\nhave always been a race for theory”.\nThe third is in looking at recommendations from adult\neducation about how we might encourage perspective\ntransformations. Unfortunately, as Taylor and Snyder[32]\nnote,  work  has  focused  on  support  based  around\nassumptions from Mezirow, “such as creating a safe and\ninclusive  learning  environment,  focusing  on  the\nindividual  learner’s  needs,  and  building  on  life\nexperiences”. One strand of work that does go beyond\nMezirow’s assumptions looks at how the significance of\nspontaneous action depends on social recognition. That\nwork finds that what would otherwise be a spontaneous\naction  becomes  personally  meaningful  when  others\npoint it out and provide positive feedback about it.\nCombining these strands together, we can say: those\nwho  have  undergone  a  critical  technical  awakening\nshould think about relationships with others in which we\ncreate  safe  and  inclusive  learning  environments,\nfacilitate opportunities for experience, serve as guides\nwho can give focus to specific learning needs, and give\npositive  feedback  around  disorienting  dilemmas  and\nother  opportunities  for  reflecting  and  questioning\nassumptions.\nWhile these principles were developed in opposition\nto  existing  formalized  education,  there  may  be\nopportunities to incorporate them into formal education\nas  well.  Trbušić[39] argues  for  integrating  critical\nmethods into engineering education as a way of making\nethics  more  than  a  superficial  part  of  training.  She\nspecifically suggests using Augusto Boal’s technique of\nTheatre of the Oppressed[40] (itself based on the work of\nFreire,\n with\n whom\n Boal\n was\n friends),\n using\nimprovisation  and  role-playing  to  encourage  critical\nconsciousness.\n Incorporating\n role-playing\n with\nscenarios where engineering students are put into ethical\ndilemmas  could  encourage  taking  an  active  stance,\ntrying different roles, and stimulating reflection in a way\nthat presenting formal models of ethics would not.\nEspecially  insofar  as  critical  technical  awakenings\nmay  fall  short  more  than  other  types  of  critical\nconsciousness, there is also a task for how to deepen our\nown  awareness  and  practice.  Taylor  and  Snyder[32]\nidentify  work  about “social  accountability”,  where  a\nmoral  underpinning  is  an  outcome  of  transformative\nlearning.\n More\n specifically, \n“the\n outcome\n of\ntransformative  learning  involves  recognizing  the\nreasons  why,  for  what  purpose,  and  for  whom  a  new\nidentity  was  constructed”,  especially  as  an  essential\ncomponent\n of\n trusting\n relationships[41].\n Having\ntransformations be ethically grounded for what kind of\nworld we want to see and work towards, and making this\na focus of interpersonal relationships and community-\nbuilding,  can  also  help  achieve  more  complete  and\npowerful transformations.\n5    Ethics\nEarlier, we raised reasons why it seems like positivism\nis compelling as a basis for ethics. But Rogaway and\nToyama’s accounts, in particular, get at how positivism\n    374\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\nand\n technical\n disciplines\n are\n harmful\n in\n the\nconsequences  of  their  epistemological  assumptions:  if\nquantitative forms of knowledge are superior, then other\nforms  of  knowledge  are  inferior.  Consequently,  those\nwho  do  not  hold  quantitative  knowledge  do  not  have\nanything to offer.\nDe Sousa Santos[42, 43] discusses the interconnection\nof ecologies of knowledge and how people are valued.\nWhen  knowledge  is  put  in  hierarchies,  it  also  places\npeople into hierarchies. Sylvia Wynter, in her landmark\nwork  on “No  Humans  Involved”[44],  has  a  stark\npresentation of this idea. Her title refers to a term used\nby the Los Angeles Police Department to classify police\nencounters where they enacted violence on young Black\nmen who were jobless in the inner city: by saying that\nthese  encounters  did  not  involved “humans”,  the\ndepartment excused themselves from documenting their\nuse of force and gave them a license to continue. The\nliteral,  administrative  category  reflected  metaphorical\ndehumanization: there is no brutality or injustice if the\ntargets are not human.\nCritical,  constructivist,  and  participatory  paradigms\nlink  epistemology  and  axiology,  saying:  how  do  we\nvalue people, if we do not value their knowledge? Even\npost-positivism  is  insufficient;  we  can  see  calls  for\n“Human-Centered  AI”,  or “Human-Centered  Machine\nLearning”, or “Human-Centered Data Science” as fitting\ninto a post-positivist frame, where we pursue objective\nknowledge and “real” technology that is focused around\nthe figure of the human and its subjectivity. But human-\ncenteredness does not address dehumanization, who gets\nrecognition as being in the category of “human”, and\nhow exclusion happens (e.g., being “human” is reserved\nfor people who look, talk, think, act, and exist in certain\nways).  Any  form  of  human-centered  computing  that\ntakes the category of “human” for granted will not undo\nthe  status  quo  of  what  Wynter  calls “narrative\ncondemnation”. Participatory approaches, in particular,\nstart with the proposition that everyone is valuable, and\nthen derive knowledge from there.\nAs in the premise of critical theory, the Enlightenment\nled  to  or  at  least  did  not  prevent  the  atrocities  of  the\nHolocaust, to which we can also add the atrocities of\nindigenous genocides in the Americas and Australia, the\nbrutality of colonialism like in the anthropogenic Bengal\nfamine  or  the  atrocities  in  Congo  Free  State,  and\nespecially the trans-Atlantic slave trade. Science was a\nweapon to dehumanize and make exclusionary standards\nfor moral standing throughout history[45]. It was utilized\nas a tool to control otherized populations, alienate them\nfrom the public sphere, and remove them from societal\nparticipation. Pretending these things did not happen, or\npretending  as  though  they  were  aberrations  from  the\nnatural course of science, does nothing to prevent them\nfrom happening in the future. Atrocity and oppression\ncannot happen without devaluing entire groups of people,\nand excluding them from belonging to the same sort of\ncategory  of  being;  this  is  the  only  way  we  can  apply\ndifferent  standards,  for  example,  of  surveillance  or\naccountability  or  resource  distribution  or  violence  to\npeople  based  on  different  labels  (e.g.,  criminal,\nimmigrant,  welfare  beneficiary,  and  foreign  citizen).\nThen, instead of making universal morality the basis of\nour  ethics,  we  should  seek  to  dismantle  knowledge\nhierarchies. We should valorize knowledge creation that\nresisted  and  persisted  through  dehumanization[46]\nthrough empirical but also artistic, narrative, and cultural\nmeans, and see these as no lesser than quantitative forms\nof knowledge.\nWe advocate specifically for the ethics of care from\nBlack\n feminist\n frameworks[47−50].\n Traditionally,\ndescriptive  ethics  have  linked  recognition,  belonging,\nand moral standing: normatively, the way to be ethical,\nand  achieve  justice,  is  to  extend  recognition,  equal\nstanding, and the protection of rights to people who have\nbeen marginalized and excluded (such as by bringing\nmarginalized people into full participation in the public\nsphere,  or  by  policies  framed  around  safeguarding\nhuman or civil rights). In contrast, the ethics of care is\na normative ethical position that reacts to the ethics of\nrecognition  and  how  it  descriptively  concedes  to\n“recognition” as being an acceptable basis for treatment.\nThis ethical position is found in a long history of the\nlabor of Black women (including potentially not under\nthe explicit label of “ethics of care”[51]), specifically in\nBlack feminist circles and in value-based social services\ndisciplines[52] like social work, thinking about how to\nhave ethical and holistic interpersonal relationships, and\nfocusing on care for marginalized people[53−55]. Instead\nof recognition, the basis of these ethics is empathy, love,\nand\n connection,\n coming\n from\n non-Eurocentric\nworld-views, and advocating treating every living being\nwith  care.  Scaling  up  interpersonal  care  to  systems\ncreates  a  principle  that  systems  must  serve  the  most\n  Maya Malik et al.:   Critical Technical Awakenings\n375    \n \n\nmarginalized  and  disadvantaged,  rather  than  those\npeople needing to fit into systems or gain social capital\nbefore they are respected or considered important.\n6    Traps\nA critical technical awakening destabilizes a positivist\nworldview, opening up the possibility of a perspective\ntransformation  that  leads  to  people  working  with\ndeliberation and awareness towards a better world. But\nit  is  not  sufficient.  In  a  reflection  of  the  language  of\nSelbst  et  al.[56] who  talk  about  five “traps” of  the\n(positivist) formalisms of computer science, we discuss\ntwo  traps  in  critical  technical  awakenings  that  reject\npositivism\n but\n may\n fail\n to\n achieve\n genuine\ntransformation.  There  are  other  traps  as  well,  for\nexample co-option, as discussed in other articles in this\nspecial  issue[2, 3],  but  here  we  discuss incomplete\nawakenings, and technical abandonment.\nThe  first  and  most  important  trap  is  of incomplete\nawakenings,  where  one’s  perspective  only  widens\nsomewhat, and specifically does not get past knowledge\nhierarchies. We have sketched out a particular normative\npath for an awakening, with this dismantlement as the\ngoal. But none of the critical technical awakenings we\nidentify necessarily get this far. Agre’s characterization\nof his awakening, for example, seemed more like it was\nabout  intellectual  fulfillment,  and  (at  least  from  the\ndescription) did not engage with positionality. What he\ndescribes  is  coming  to  see  some  other  forms  of  elite\nknowledge, namely those from the humanities and social\nsciences,  as  superior  to  his  former  narrow  technical\nworldview.\nThe blindness Broussard[12] identifies of technologists\nto other forms of knowledge from experience is not ever\nrecognized or addressed in Agre’s work. Again, the work\nmay not reflect the full extent of Agre’s experience, and\nit  may  do  so  in  a  particular  rhetorical  strategy  of  not\ntrying to overturn positivism and knowledge hierarchies\nall  at  once;  but,  this  is  a  theme  across  the  other\ndescriptions of awakenings as well. In none of them is\nthere a recognition of the existence and value of other\nvery different forms of knowledge, or the value of the\npeople who hold those other forms of knowledge.\nThe  second  trap  is  a  more  subjective  one:  that  of\nabandonment.  There  is  a  temptation,  upon  having  an\nawakening  and  becoming  disillusioned,  to  abandon\ntechnical work entirely. We argue this is bad for two\nreasons. The first is a strategic one: at the risk of reifying\nquantification and technology, we believe that there is\na role for those trained in these methods to push back and\ndevelop  critiques  in “internal” terms  that  can  be\nintelligible  to  those  still  in  a  technical  mindset  (and\nperhaps even leading others to having their own critical\ntechnical awakening). These are some of the potential\n“new roles”, as in Mezirow’s ten steps, we explore above.\nThis temptation is parallel to how, upon recognizing and\nbecoming disillusioned with privilege, one temptation is\nto attempt to reject that privilege; but, such attempts do\nnot actually erase the privilege one has benefitted from\nin the past. Finding ways to engage with and leverage\nthis privilege is the more responsible course.\nThe second reason we argue against abandonment is\nmore abstract and speculative. Just as modern qualitative\nresearch originated in the oppressive project of colonial\nanthropology but has since worked to reform on grounds\nof  being  reflexive  and  pursuing  justice,  so  too  might\nquantitative research move away from positivism[57].\nGiven that quantification is about abstraction[56], and\nabstraction flattens meanings[15], it is difficult to imagine\nquantitative  knowledge  that  can  be  reflexive  and\nacknowledge  other  forms  of  knowledge,  but  is  worth\nexploring. Agre’s own suggestion of a “critical technical\npractice” is itself a call to continue creating technical\nknowledge, but through a critical lens. What that might\nmean or how it might look is unclear from Agre’s work\nor the handful of subsequent works that have taken on\nthat label, but the development of technical knowledge\non  something  other  than  a  realist  ontology  and  a\nhierarchical  axiology  can  be  seen  as  a  worthwhile\nchallenge.\n7    The Path Forward\nDespite  being  a  powerful  expression  of  a  profound\nshared  experience,  Agre’s  call  for “critical  technical\npractice” has largely languished for the past two decades.\nFor personal reasons, Agre himself has not been active\nin  academia[58] to  continue  exploring  and  developing\nthis  idea  himself.  Critical  technical  practice  has  been\ncontinued by a few people, like Phoebe Sengers[59], but\neven that has been mostly within design and Human-\nComputer Interaction[60, 61], rather than in more formal\nmathematic  and  technical  areas  where  critical  and\nconstructivist approaches are most alien.\nAs discussed before, one key missing element from\n    376\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\nAgre’s  narrative  and  those  of  others  is  Mezirow’s\nstage (4), “Recognition that one’s discontent and process\nof  transformation  are  shared  and  that  others  have\nnegotiated a similar chang.” While it is hard to say why\ncritical technical practice failed to take hold—Agre no\nlonger being active in academia? The original essay not\nhaving  any  clear  statement  of  what,  exactly,  critical\ntechnical  practice  is  or  looks  like?  Critical  technical\npractice not being a good way to productively channel\nawakenings?  There  not  being  enough  awakenings  to\nform a critical mass? Agre simply being ahead of his\ntime[62]?—building community and coalitions seems to\nbe a critical missing step.\nSome  of  what  we  detail  in  sources  of  awakening\nsuggest ways that we can try to encourage more people\nwith  a  technical  perspective  to  undergo  critical\nawakenings: exposure to anti-positivist and anti-realist\nideas,  putting  them  in  contact  with  non-technical\nindividuals,\n and\n finding\n ways\n to\n attack\ncompartmentalization (as is done in other articles in this\ncollection  like  those  of  Green[8],  and  in  the  design\nmethod that Stark[63] offers). Or, if these were integrated\nin  technical  education  sufficiently  early on[25, 39],\nperhaps  people  would  never  develop  a  distinctly\ntechnical perspective and would not need (as abrupt of)\nan awakening, in a topic that also relates to the article in\nthis special issue by Korn[64]. This article (as well as that\nof  Hu[3])  also  partially  take  the  form  of  personal\nreflections,  which  are  central  in  critical  awakenings;\nwhile we have chosen, primarily for reasons of length\nand  coherence,  to  make  this  essay  a  primarily\ninformational and analytic one rather than discuss our\nexperiences, we cite these articles as examples of how\nwe should seek to create more opportunities for technical\npractitioners  to,  respectively,  engage  in  their  own\npersonal  reflections  as  a  technical  practitioner[3] and\nwith the experiences of others[64].\nSeeking\n out\n perspectives\n from\n others,\n both\ncontemporary and historical, is one way to break through\nossified  visions.  In “Informatics  of  the  Oppressed”,\nOchigame  describes  in  English  for  the  first  time  two\nLatin  American  informatics  projects[65].  First,  Cuban\nlibrarians and computer scientists in the 1980s, facing\nUS  embargoes,  set  up  an  alternative  information\nindexing  and  retrieval  system  whose  mathematical\nmodel, among other features, adjusted readership-based\nindexes by the number of librarians in recognition of the\n“author-reader  social  communication  that  happens  in\nlibraries”.  Second,  liberation  theologists  in  Brazil\nresisting the post-1964 military dictatorship set up a print\nand mail-based “intercommunication network” to solicit\nand  internationally  distribute  writings  by  those  most\nsubjected to domination, in a vision of advancing Freire’s\nproject  past  a  need  for  intermediaries  and  towards\n“‘inter-conscientization’ \nbetween\n the\n oppressed”.\nOchigame notes that these projects were, like libertarian\nfantasies coming out of California, overly optimistic in\nwhat  technology  (alone)  would  achieve;  but  these\nvisions were still valuable in the alternative they offered\nto ranking based only on productivity or popularity (in\nCuba), and in justifying and structuring dissemination\nnot  just  in  terms  of  free  speech  or  in  the  politics  of\n“whether one is free to speak, but whose voices one can\nhear  and  which  listeners  one’s  voice  can  reach” (in\nBrazil). We can take inspiration from these alternative\nvisions,  and  seek  out  others  that  have  similarly  been\nsilenced  and  pushed  aside  (indeed,  Ochigame’s\ndiscovery  of  these  projects  came  through  personal\nmeetings, and not online searches). Those of us trained\nin  technology  development  and  quantitative  forms  of\nknowing  should  try  to  build  on  these,  and  explore\nalternative visions. We hold that the potential value of\nquantitative knowledge outside of its connection to and\nrole  in  upholding  power,  hierarchy,  and  privileged\naccess to truth have yet to be fully explored.\nAnother  key  part  of  any  path  forward  is  to build\ncommunity to  encourage,  support,  and  guide  critical\ntechnical awakenings, and channel those who undergo\nsuch  awakenings  towards  developing  a  critical\ntechnical  practice.  Here,  we  can  point  to  conference\nworkshops[60, 61, 63, 66], networks  like  the  one  formed\nfrom the Ethical Tech Working Group that generated this\nspecial  issue,  fellowship  cohorts,  and  mentorship  as\npaths  forward.  But  as  a  caveat,  while  community-\nbuilding aimed at reaching technical practitioners will\nmost likely need to operate within institutional elitism\n(indeed, like the Ethical Tech Working Group being at\nHarvard),  this  should  only  be  one  part  of  larger\ncommunity-building.  After  all,  during  his  exile  under\nBrazil’s 21-year military dictatorship, Freire also spent\na  year  as  a  visiting  professor  at  Harvard;  but  he\neventually returned to Brazil and continued to develop\nboth  theory  and  practice,  including  serving  as  a\nmunicipal Secretary of Education.\n  Maya Malik et al.:   Critical Technical Awakenings\n377    \n \n\nBut questions remain. What is the value of quantitative\napproaches  outside  of  knowledge  hierarchies?  As\nBricmont  and  Sokal  suggest[14],  are  quantitative  and\ntechnical approaches to the world only valuable if they\nare  getting  at  a  single  universal  truth?  If  we  reject\npositivism, and choose participatory paradigms and the\nethics of care, must we reject technical approaches? Or\neven if not, how can we integrate the ethics of care into\ntechnology to achieve “doing no unintended harm”, and\nnot\n further\n marginalizing\n resource-deprived\ncommunities? What sorts of technical practices might\nemerge  not  from  an elite critical  stance,  but  from  a\ncritical pedagogical stance?\nIt seems daunting, but qualitative research also was\nonce positivist and hierarchical, for example, in seeing\nthe role of a colonial anthropologist as providing neutral\ndescription about colonized or imperialized peoples to\nbetter facilitate control.\nLastly,  we  hope  this  article  has  served  as  an\norientation, encouragement, and guidance for those who\nare undergoing the kind of vertigo that Agre described.\nThe  technical  variant  of  critical  consciousness  is  a\nprofound  and  important  experience,  just  like  critical\nconsciousness in general. But if it happens in isolation,\nit may be unnecessarily painful, and more importantly\nmay not overcome the most pernicious part of positivism:\ncreating  and  defending  hierarchies  of  knowledge  that\nstructure the ways we approach the world, value ideas,\nand treat other beings. We hope that this article points to\nhow this experience is not isolated, and gives support\ntowards  building  community,  overcoming  knowledge\nhierarchies, adopting an ethics of care, and taking action\ntowards more liberated ways of being.\nAppendix\nGlossary of Key Terms\nRealism is the belief in a single underlying reality that\nexists independent of and prior to human conception of\nit. A specific form of this was articulated by Plato, where\nmathematical forms are immutable and that invariance\nwhat determines what is “real”. Confusingly but perhaps\nmore  appropriately,  this  is  sometimes  also  called\n“idealism”, since reality is associated with ideas rather\nthan perception.\nPositivism was  coined  by  Auguste  Comte  in\nphilosophical  writings  around  1830−1842.  It  was  an\napplication  of  methodology  from  natural  sciences  to\nstudy  human  behavior  and  social  phenomena.  Comte\narticulated  positivism[57] in  terms  of  a  premise  that\nuniversal  truths  exist  for  human  behavior  and  social\nphenomenon (i.e., a realist ontology), and that empirical\nobservations  through  scientific  measurement  can\ndiscover  these  universal  truths  (i.e.,  an  empiricist\nepistemology and methodology).\nPositivism now describes any research paradigm that\nholds that a singular truth exists and can be uncovered\nby empirical observation, and covers natural sciences as\nwell  as  social  and  behavioral  sciences.  There  are\nversions  of  positivism  that  try  to  avoid  the  realist\ncommitment,  and  there  can  also  be  realism  without\nempiricism (such as in pure mathematics) but the key\npoint of either realism or positivism as compared to other\nsets of assumptions is belief in an external world that\ntakes  primacy  over  actors’ interpretations  and\nrenegotiations of it[67].\nPost-positivism is a softening of positivism, and held\nby  people  who  still  find  positivism  aesthetically\ncompelling,  but  acknowledge  that  contingent  and\nmalleable  (and  non-scientifically  measurable)  history,\nsociety, and culture can come in the way of our ability to\ndiscover  universal  truths  through  observation,  and  so\nmust be accounted for (potentially through qualitative\nmeans).\nCritical theory is a type of philosophy often viewed\nas  originating  from  a  specific  group  of  European\nintellectuals based in Frankfurt in the period between the\nWorld  Wars.  Against  the  prevailing  view  that  the\nEnlightenment had led to constant social improvement,\nthis  Frankfurt  School  and  their  successors  sought  to\ntheorize how the Enlightenment led to, or at least failed\nto prevent, World War I, the rise of anti-Semitism, and\nother  forms  of  oppression  (eventually  leading  to  the\nHolocaust)  in  liberal  capitalist  societies.  Of  course,\nearlier major atrocities—such as the trans-Atlantic slave\ntrade,\n or\n colonial\n genocide\n of\n indigenous\npopulations—tellingly\n did\n not\n lead\n to\n similar\nsoul-searching among European intellectuals about the\nconsequences of the Enlightenment. Still, the Frankfurt\nSchool\n represented\n when\n a\n major\n European\nphilosophical  school  caught  up  to  people  in  the\ncolonized world in acknowledging marginalization as a\ncentral philosophical question. For example, Rabaka[68]\nargues\n that\n Martinique-born\n psychiatrist\n and\n    378\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\nphilosopher Frantz Fanon (discussed more below), built\non prior work from the colonized world and went far\nbeyond the Frankfurt school in analyzing the nature of\nthe racism and exploitation of settler colonialism.\nIt is from the Frankfurt School’s use of “critical” that\nthe  term  is  applied  to  theories  that  dispute  prevailing\nassumptions  about  social  development  needing  only\ncontinue along its current course to eventually result in\nthe end of forms of oppression, e.g., around gender, race,\nsexuality, disability, etc.\nA  good  definition  of  what  makes  a “critical  social\nscience” is in Fay’s Critical Social Science: Liberation\nand its Limits[69]. Fay conceives of critical social science\nas a type of “estrangement theory”. This is a view of the\nworld that holds that there is a manifest/ordinary sphere\nin which most people live, but this keeps them trapped\nfrom  what  is  best  in  life,  which  exists  in  a\nhidden/extraordinary sphere. Specifically, critical social\nscience  is  a humanist variant  of  estrangement  theory,\nthat  locates  the  hidden/extraordinary  sphere  not  in  a\nreligious or spiritual plane (like religious and mystical\ntraditions do), but in the social plane. He additionally\ntheorizes that a complete critical theory includes a theory\nof\n false\n consciousness \n(identifying\n certain\nunderstandings and explaining how they are false and/or\nincoherent, and how they come to be and are maintained),\na theory of crisis (how a society is in a crisis from felt\ndissatisfactions that threaten social cohesion and cannot\nbe resolved within existing social organization and self-\nunderstandings),  a theory  of  education (the  necessary\nand  sufficient  conditions  for  overcoming  the  false\nconsciousness),  and  a theory  of  transformative  action\n(identifying what needs to change, and a plan of action\nfor who are “carriers” of anticipated social change and\nhow they will go about achieving it).\nNote  that  positivism  (or  realism)  can  have  an\nestrangement aspect as well, where there is a hidden truth\nthat  reality  is  apprehensible  through  the  language  of\nmathematics  and/or  experimental  methods,  leading  to\nliberation.  Indeed,  Plato’s  parable  of  the  cave,  and\nPlatonism (as well as the neo-Platonism of mystic cults\nthroughout the Mediterranean and West Asia centuries\nafter Plato) sees universal abstract mathematical forms\nas the truth from which the masses are estranged. But the\nestrangement aspect of positivism need not be present,\nwhereas it is an essential part of any critical theory.\nRelativism is a stance that potentially spans ontology,\nepistemology, and axiology. Ontological (or conceptual)\nrelativism holds that there is no observer-independent\nreality, and that an observer creates their own reality.\nEpistemically, relativism holds that there is no neutral\nframe in which we can arbitrate whether claims are “true”\nor “false”. This can be understood empirically (rather\nthan  normatively)③:  for  example,  speaking  purely\nempirically, there is no frame of reference to which a\nBiblical creationist and an evolutionary biologist would\nagree for arbitrating their competing claims about the\norigin of biological diversity. Each would insist on their\nown frame being the “neutral” or superior one, and any\nlogical  or  empirical  basis  for  deciding  between  the\nframes would itself rely on agreement over what counts\nas logical or empirical. Moral relativism holds that there\nis no neutral frame in which we can decide what is good\nor bad. Similar to epistemic relativism, moral relativism\nmay be a descriptive rather than a normative position,\nbuilt  on  the  observation  that  people  have  genuine\ndisagreements  about  morality  that  cannot  be  logically\nresolved by an appeal to universal underlying principles.\nThat is, a relativist can have their own (non-relativist)\nnormative morality that they believe is correct, alongside\na relativist ontology and/or relativist epistemology that\nthey also believe is correct, but they recognize that there\nis not necessarily any deeper universal principle to which\nto appeal and logically convince others. As a corollary,\nwe  can  account  for  people  with  perspectives  we  find\nbizarre or moral codes that we find abhorrent who cannot\nbe convinced through logical means, rather than needing\nto dismiss them as insane.\nRelativism represents a break from a singular truth,\nand  can  be  deeply  uncomfortable  and  threatening  for\nthose accustomed to the pursuit of certainty and finality.\nWorse, when every possible position and action can be\ncritiqued, relativized, destabilized, and once we know\nhow to do this, it can be debilitating. See below for how\nparticipatory paradigms provide a way out of this.\nConstructivism is built on relativism, and describes\n③ Barnes and Bloor[70] have a relatively simple response to the frequent\ninitial objection that relativism is paradoxical or self-refuting (i.e., if all\nperspectives  are  equally  valid,  then  by  its  own  admission  relativism\nconcedes to non-relativism): relativism is not saying we cannot hold our\nown perspective, or we cannot condemn those of others or say they are\nwrong (whether morally, or in terms of knowledge); relativism can be just\nthe recognition that others can and will reject our views or condemnations,\nand that our condemnations alone will not convince them otherwise. Of\ncourse, it is possible to interpret relativism in such a way as to defend the\nright  of  regressive  perspectives  to  exist,  but  that  treats  relativism  as  a\nstandard to which to aspire, rather than a description of how things are.\nAnd,  relativism  is  self-referential  and  can  create  paradoxes,  but  we\nbelieve  that  accepting  these  paradoxes  as  axiomatic  is  enormously\ninsightful.\n  Maya Malik et al.:   Critical Technical Awakenings\n379    \n \n\nthe process by which multiple “truths” come to exist. It\nis an idea coming out of the sociology of knowledge that\nholds that our experiences of the world, and knowledge,\nare  not  references  to  or  reflections  of  an  underlying\nexternal reality, but are the product of historical, cultural,\nand material forces that, had they been different, would\nhave  built  something  different.  Note  that  saying\nsomething  (like  scientific  knowledge)  is “constructed”\ndoes not mean that it is not real, or not solid, or not robust;\na metaphor used to illustrate this perspective[71] is that\nof a house, which is perfectly “real” but it came to exist\nat a certain point in time, and was built in one specific\nway out of specific materials out of many alternatives.\nWe can come to understand this building process without\nclaiming the building is anything other than solid and\ndurable.  However,  other  versions  of  constructivism\nstress  the  fluidity  of  things  like  scientific  knowledge,\nrejecting the idea of knowledge as hierarchical structures\nanchored  to,  if  not  a  solid  underlying  reality,  then  to\nsociety  and  history;  these  versions  of  constructivism\ninstead see knowledge as ungrounded webs of mutual\nreference. Then, the task of inquiry is to understand the\nconstruction and maintenance of these webs of mutual\nreference (with the inquiry being itself a part of the webs\nit considers).\nThere  is  a  tension  between  critical  theory  and\nconstructivism[72] in how critical perspectives can end\nup holding that there is an external world, just that it is\nsomething different than what most people think it is. So,\nfor example, Fay offers the Marxist-humanist model of\npolitical revolution as an example of a critical theory,\nwhere there is a “true nature” that bourgeoisie oppressors\nderive  power  from  the  self-understandings  of  the\noppressed working classes.\nHowever, they frequently appear together. Hacking[73]\npoints out how looking at how things are put together\nalso gives people grounds to see how they come apart,\nand deconstruct them. A crucial part of a critical toolbox\nis in showing the historical construction of ideas, forms\nof knowledge, institutions, and cultural forms, thereby\ndemonstrating that they are not inevitable, and letting us\nimagine and advocate for alternatives.\nFor example, in critical race studies and critical gender\nstudies, there is a “false consciousness” of thinking that\nthe  categories  by  which  people  are  marginalized  are\nbased on biological traits or even cultural ones. But there\nis no such thing as biological race or gender, let alone\ninferiority  by  them  (and  the “value” of  cultures,  like\nEuropean culture versus indigenous cultures, come from\nhow  they  are  valued,  and  not  something  intrinsic).\nInstead,  such  categories  and  their  value  are  socially\nconstructed  by  and  maintained  through  power\nrelationships. Going further, there is a second layer to the\nfalse  consciousness,  of  holding  marginalized  people\nindividually  responsible  for  their  suffering  and\ndeprivation. Once categories are so constructed, those\nthat fall within the marginalized categories like women\nof  color  and  others  with  individual  or  intersecting\nmarginalized identities are treated as inferior, in ways\noften enacted on an interpersonal level but structurally\nand culturally encouraged and permitted. The result is\nmarginalized  people  face  greater  mental  and  physical\nsuffering, and material deprivation, entirely apart from\ntheir individual “effort”, yet over which they are held\nresponsible. Even holding those who enact the double\nstandards individually responsible (i.e., seeing racism or\nsexism as an interpersonal problem), rather than seeing\nthe larger structure, is a false consciousness. Only by\nrecognizing  the  true  nature  of  modern  civilization  as\nfundamentally\n structured\n on\n white\n supremacy,\npatriarchy, colonialism, and other forms of domination\ncan we effect change and improve human life.\nIndeed, Agre’s[74] idea of “critical” is actually more\nabout  constructivism  (and  unfortunately  he  sets  it  up\nusing ableist language). In one entry from his Red Rock\nEater Newsletter (a listserv over which Agre sent out\nwritings that has been cited as a precedent for blogs), he\nwrote:\n“I  finally  comprehended  the  difference  between\ncritical thinking and its opposite. Technical people are\nnot dumb [sic], quite the contrary, but technical curricula\nrarely include critical thinking in the sense I have in mind.\nCritical thinking means that you can, so to speak, see\nyour glasses. You can look at the world, or you can\nback up and look at the framework of concepts and\nassumptions and practices through which you look\nat the world.”\nAgre continues: “Not that critical thinking makes you\nomniscient:  you’re  still  wearing  glasses  even  when\nyou’re  looking  at  your  glasses.” That  is,  there  is  no\nperspective  without  any  glasses,  no “view  from\nnowhere”.④ The experience of “seeing one’s glasses” is\ndifferent than just replacing one’s glasses; it opens the\n④ Ludwig Wittgenstein, another figure who underwent a transformation\nin his basic beliefs and how he saw the world, also used this metaphor\nmuch earlier[75]: “The ideal, as we think of it, is unshakable. You can\nnever  get  outside  it;  you  must  always  turn  back.  There  is  no  outside;\noutside you cannot breathe.—Where does this idea come from? It is like\na pair of glasses on our nose through which we see whatever we look at.\nIt never occurs to us to take them off.”\n    380\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\npath  to  understanding  endless  contingency  in  ideas,\nstructures, institutions, and frameworks.\nCritical consciousness is a theory that came out of\npolitical mobilization and community development, also\nknown as popular education, in the Global South[76−78],\nand  specifically  from  the  work  of  Brazilian  educator,\nphilosopher, and politician Paulo Freire (1921–1997).\nFreire  worked  in  the  1960s  with  populations  like\nmarginalized  sugarcane  harvesters  with  no  access  to\nformal  education.  He  started  education  programs  for\npolitical  mobilization  in  conjunction  with  them,  and\nused that mobilization to get the Brazilian government to\nfinancially support the programs they had created. He\nchallenged  a “banking” conception  of  education  that\nassumed  he  was  more  of  a  knowledge  holder  and\nknowledge creator than the farmers he worked with, and\nthat placed more value on him as a teacher, because he\nhad  access  to  formal  education.  He  inverted  the\nhierarchy  to  say  that  the  marginalized  are  valuable\nbecause  of  their  response  to  marginalization,  their\nresilience,  and  how  the  experience  of  marginalization\nshowed larger societal structures in a way that Freire,\nwith his privilege, had not seen. He theorized how to\nunseat the teacher or researcher as the expert, and sought\nto develop a model where we all bring something to the\ntable  and  learn  from  each  other,  and  understanding\nemerges from our interactions.\nAnother key input, on whose work Freire drew, was\nFrantz  Fanon  (1925–1961).  Fanon  was  hired  as  a\npsychiatrist  by  the  French  colonial  government  in\nAlgeria to treat mental illness in colonial subjects. There,\nFanon realized that his patients were not having mental\nhealth crises, but reacting to oppression, and the French\ngovernment did not understand that their reaction was\nthe  most  logical  response  to  being  otherized,\ndehumanized, and oppressed. Building on his previous\nwork theorizing his own experience being treated as a\nFrench  colonial  subject[79],  in  interacting  with  his\npatients in Algeria he learned about his own position in\na larger oppressive system and how it was causing harm\nto others[80, 81]. From this, he wrote about working with\nmarginalized  populations,  unlearning  harmful  frames,\nand mobilizing for revolution and equity, himself joining\nthe  Algerian  National  Liberation  Front  to  support\nAlgeria’s War of independence from France.\nFreire  gave  the  name conscientizaçao to  the\ntransformative  process  of  interacting  with  other\nindividuals  and  other  communities[82, 83],  translated  as\ncritical\n consciousness,\n or\n more\n literally\n as\n“conscientization”,  and  sometimes  as  consciousness-\nraising[84].⑤ From  there,  others  have  continued  to\nsystematically develop tools, strategies, and methods for\ncritical  consciousness,  including  dialogue  and  critical\nreflection, reflective questioning, psychosocial support,\nco-learning,  group  processes,  civic  engagement  and\nsociopolitical  action,  and  identity  development[34].\nCritical  consciousness  has  inspired  a  field  within\neducation known as critical pedagogy[86] which has been\ncarried forward particularly in adult education[4] and has\nhad a large impact on the development of Participatory\nAction Research[87] and Community Based Participatory\nResearch[88].\nPositionality is  awareness  and  discussion  of  ones’\nsocial and institutional position with regards to research,\nparticularly  of  power  imbalances,  and  limitations  the\nresearcher  may  have  because  of  differences  in  lived\nexperience.\nReflexivity is the process of “turning back on” and\nreflecting  on  experience  and  our  positionality.  For\nexample,  in  anthropology,  this  is  researchers  being\nexplicit  about  their  emotions  and  how  they  related  to\nresearch subjects[89]. Positivism, in particular, does not\nand cannot engage in reflexivity[90], since it holds that\nknowledge is independent of the knowledge-holder.\nParticipatory paradigms address an important moral\naspect lacking in both critical theory and constructivism.\nCertain  streams  of  critical  theory  frequently  have  a\ncondescending aspect to them: that people are unaware\nof their own oppression, and it is the role of the critical\ntheorist  to  educate  them.  On  the  other  hand,\nconstructivism  does  not  account  for  experiential\nknowing[90]. Building explicitly from the ideas of Freire,\nparticipatory paradigms value and highlight experience,\nfollowing  a  methodology  that  challenges  hierarchies\nbetween teacher and student, or researcher and subject,\nand  seeks  to  construct  knowledge  collectively.  Its\nmethodology and axiology prioritize understanding and\nimproving the world by changing it through collective,\nreflexive inquiry[91].\nThis paradigm has a relativistic component in seeing\nknowledge  as  malleable  and  multiple  rather  than\nabsolute and singular; by locating value in others and\ntheir  experiences,  rather  than  seeing  the  status  of\n⑤ Consciousness-raising also appears, without reference to Freire, in\nUS feminist movements in the 1960s[85].\n  Maya Malik et al.:   Critical Technical Awakenings\n381    \n \n\nknowledge  as  the  most  important  thing  in  life,  the\ninstability of knowledge does not become a reason to be\nnihilistic.\nAcknowledgment\nThanks to two anonymous reviewers for great feedback\nand  pointing  us  to  some  relevant  literature,  and  Ben\nGreen both for fantastic continuous comments and for\ncorralling and managing this special issue.\nReferences\n P. E. Agre, Toward a critical technical practice: Lessons\nlearned  in  trying  to  reform  AI,  in Bridging  the  Great\nDivide:\n Social\n Science,\n Technical\n Systems,\n and\nCooperative Work, G. Bowker, L. Star, B. Turner, and L.\nGasser,  eds.  Mahwah,  NJ,  USA:  Lawrence  Erlbaum\nAssociates, Inc., 1997, pp. 131–157.\n[1]\n B. Green, The contestation of tech ethics: A sociotechnical\napproach  to  technology  ethics  in  practice, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0018.\n[2]\n L.  Hu,  Tech  ethics:  Speaking  ethics  to  power,  or  power\nspeaking  ethics? Journal  of  Social  Computing,  doi:\n10.23919/JSC.2021.0033.\n[3]\n S. B. Merriam and L. M. Baumgartner, Transformational\nlearning,  in Learning  in  Adulthood:  A  Comprehensive\nGuide, 4th edition, S. B. Merriam and L. M. Baumgartner,\neds. Hoboken, NJ, USA: Wiley, 2020, pp. 166–195.\n[4]\n D.  A.  Henhawk,  My  critical  awakening:  A  process  of\nstruggles and decolonizing hope, International Review of\nQualitative Research, doi: 10.1525/irqr.2013.6.4.510.\n[5]\n J. Prada, The critical awakening of a pre-service teacher in\na  Spanish  graduate  program:  A  phenomenology  of\ntranslanguaging as pedagogy and as content, International\nJournal  of  Bilingual  Education  and  Bilingualism,  doi:\n10.1080/13670050.2021.1881945.\n[6]\n E. G. Guba and Y. S. Lincoln, Paradigmatic controversies,\ncontradictions,  and  emerging  confluences,  in The  SAGE\nHandbook of Qualitative Research, N. K. Denzin and Y.\nS. Lincoln, eds. London, UK: SAGE, 2005, pp. 191–215.\n[7]\n B. Green, Data science as political action: Grounding data\nscience  in  a  politics  of  justice, Journal  of  Social\nComputing, doi: 10.23919/JSC.2021.0029.\n[8]\n M.  L.  Jones,  How  we  became  instrumentalists  (again):\nData positivism since World War II, Historical Studies in\nthe Natural Sciences, doi: 10.1525/hsns.2018.48.5.673.\n[9]\n E. Morozov, To Save Everything, Click Here: The Folly of\nTechnological Solutionism. New York, NY, USA: Public\nAffairs, 2013.\n[10]\n K. Toyama, Geek Heresy: Rescuing Social Change from\nthe  Cult  of  Technology.  New  York,  NY,  USA:  Public\nAffairs, 2015.\n[11]\n M.  Broussard, Artificial  Unintelligence:  How  Computers\nMisunderstand  the  World.  Cambridge,  MA,  USA:  MIT\nPress, 2018.\n[12]\n B. Green, The Smart Enough City: Putting Technology in\nIts Place to Reclaim Our Urban Future. Cambridge, MA,\nUSA: MIT Press, 2019.\n[13]\n J.  Bricmont  and  A.  Sokal,  Science  and  sociology  of\n[14]\nscience:  Beyond  war  and  peace,  in The  One  Culture?  A\nConversation  about  Science,  J.  A.  Labinger  and  H.\nCollins,  eds.  Chicago,  IL,  USA:  The  University  of\nChicago Press, 2001, pp. 27–47.\n T.  M.  Porter,  Thin  description:  Surface  and  depth  in\nscience and science studies, Osiris, doi: 10.1086/667828.\n[15]\n G.  Edmond  and  D.  Mercer,  Anti-social  epistemologies,\nSocial Studies of Science, doi: 10.1177/0306312706067900.\n[16]\n M. Lynch, From ruse to farce, Social Studies of Science,\ndoi: 10.1177/0306312706067897.\n[17]\n M.  Nanda, Prophets  Facing  Backward:  Postmodern\nCritiques of Science and Hindu Nationalism in India. New\nBrunswick, NJ, USA: Rutgers University Press, 2003.\n[18]\n H. M. Collins, Son of seven sexes: The social destruction\nof a physical phenomenon, Social Studies of Science, doi:\n10.1177/030631278101100103.\n[19]\n T.  H.  Kuhn, The  Structure  of  Scientific  Revolutions,  3rd\ned. Chicago, IL, USA: The University of Chicago Press,\n1996.\n[20]\n P. Rogaway, The moral character of cryptographic work,\nhttps://web.cs.ucdavis.edu/~rogaway/papers/moral-fn.pdf,\n2015.\n[21]\n D.  J.  Watts,  The “new” science  of  networks, Annual\nReview of Sociology, doi: 10.1146/annurev.soc.30.020404.\n104342.\n[22]\n C.  A.  Hidalgo,  Disconnected,  fragmented,  or  united?  A\ntrans-disciplinary  review  of  network  science, Applied\nNetwork Science, doi: 10.1007/s41109-016-0010-3.\n[23]\n H.  Wallach,  Computational  social  science  ≠ computer\nscience  +  social  data, Communications  of  the  ACM,  doi:\n10.1145/3132698.\n[24]\n H.  Wallach,  Navigating  the  broader  impacts  of  machine\nlearning research, Medium, https://hannawallach.medium.\ncom/navigating-the-broader-impacts-of-machine-learning-\nresearch-f2d72a37a5b, 2021.\n[25]\n S.  E.  Toulmin,  Does  the  distinction  between  normal  and\nrevolutionary  science  hold  water?  in Criticism  and  the\nGrowth of Knowledge, I. Lakatos and A. Musgrave, eds.\nCambridge,  UK:  Cambridge  University  Press,  1970,  pp.\n39–48.\n[26]\n J.  Mezirow,  Perspective  transformation, Adult  Education\nQuarterly, doi: 10.1177/074171367802800202.\n[27]\n J. Mezirow, Learning to think like an adult: Core concepts\nof\n transformation\n theory,\n in \nThe\n Handbook\n of\nTransformative Learning: Theory, Research and Practice,\nE.  W.  Taylor  and  P.  Cranton,  eds.  San  Francisco,  CA,\nUSA: Jossey-Bass, 2012, pp. 73–95.\n[28]\n E.  W.  Taylor,  Building  upon  the  theoretical  debate:  A\ncritical  review  of  the  empirical  studies  of  Mezirow’s\ntransformative learning theory, Adult Education Quarterly,\ndoi: 10.1177/074171369704800104.\n[29]\n V.  Sheared,  Giving  voice:  An  inclusive  model  of\ninstruction—a  womanist  perspective, New  Directions  for\nAdult  and  Continuing  Education,  doi: 10.1002/ace.\n36719946105.\n[30]\n E. W. Taylor, Making meaning of the varied and contested\nperspectives of transformative learning, in Proceedings of\nthe  Sixth  International  Conference  on  Transformative\nLearning,  D.  Vlosak,  G.  Kielbaso,  and  J.  Radford,  eds.\nEast Lansing, MI, USA: Michigan State University, 2005,\n[31]\n    382\nJournal of Social Computing, December 2021, 2(4): 365−384    \n \n\npp. 459–464.\n E.  W.  Taylor  and  M.  J.  Snyder,  A  critical  review  of\nresearch on transformative learning theory, 2006–2010, in\nThe  Handbook  of  Transformative  Learning:  Theory,\nResearch and Practice, E. W. Taylor and P. Cranton, eds.\nSan Francisco, CA, USA: Jossey-Bass, 2012, pp. 37–55.\n[32]\n J.  Mezirow,  A  critical  theory  of  adult  learning  and\neducation, \nAdult\n Education\n Quarterly,\n doi:\n10.1177/074171368103200101.\n[33]\n A.  Jemal,  Critical  consciousness:  A  critique  and  critical\nanalysis  of  the  literature, The  Urban  Review,  doi:\n10.1007/s11256-017-0411-3.\n[34]\n P.  Freire, Pedagogy  of  the  Oppressed.  New  York,  NY,\nUSA: Continuum, 1970.\n[35]\n S. B. Merriam and G. Ntseane, Transformational learning\nin  Botswana:  How  culture  shapes  the  process, Adult\nEducation Quarterly, doi: 10.1177/0741713608314087.\n[36]\n J. Johnson-Bailey, Sistahs in College: Making a Way Out\nof No Way. Malabar, FL, USA: Krieger Publishing, 2001.\n[37]\n B. Christian, The race for theory, Cultural Critique, doi:\n10.2307/1354255.\n[38]\n H.  Trbušić,  Engineering  in  the  community:  Critical\nconsciousness\n and\n engineering\n education,\nInterdisciplinary  Description  of  Complex  Systems,  doi:\n10.7906/indecs.12.2.1.\n[39]\n A. Boal, Theatre of the Oppressed. New York, NY, USA:\nTheatre Communications Group, 1993.\n[40]\n K.  Jokikokko,  The  role  of  significant  others  in  the\nintercultural learning of teachers, Journal of Research in\nInternational Education, doi: 10.1177/1475240909105202.\n[41]\n B. de Sousa Santos, Public sphere and epistemologies of\nthe South, Africa Development, vol. 37, no. 1, pp. 43–67,\n2012.\n[42]\n B. de Sousa Santos, Epistemologies of the South: Justice\nAgainst  Epistemicide.  New  York,  NY,  USA:  Routledge,\n2014.\n[43]\n S.  Wynter,  No  humans  involved:  An  open  letter  to  my\ncolleagues,  in Forum  N.  H.  I.  :  Knowledge  for  the  21st\nCentury. Stanford, CA, USA: Giant Horse, Inc., 1994, pp.\n42–71.\n[44]\n S.  Wynter,  Unsettling  the  coloniality  of  being/power/\ntruth/freedom:  Towards  the  human,  after  man,  its\noverrepresentation—an argument, CR: The New Centennial\nReview, doi: 10.1353/ncr.2004.0015.\n[45]\n B. Rusert, Fugitive Science: Empiricism and Freedom in\nEarly  African  American  Culture.  New  York,  NY,  USA:\nNYU Press, 2017.\n[46]\n M.  Graham,  The  ethics  of  care,  Black  women  and  the\nsocial professions: Implications of a new analysis, Ethics\nand Social Welfare, doi: 10.1080/17496530701450372.\n[47]\n L.  Bass,  Fostering  an  ethic  of  care  in  leadership:  A\nconversation  with  five  African  American  women,\nAdvances in Developing Human Resources, doi: 10.1177/\n1523422309352075.\n[48]\n O. Hankivsky, Rethinking care ethics: On the promise and\npotential of an intersectional analysis, American Political\nScience Review, doi: 10.1017/S0003055414000094.\n[49]\n P.\n Raghuram,\n Race\n and\n feminist\n care\n ethics:\nIntersectionality as method, Gender, Place & Culture, doi:\n10.1080/0966369X.2019.1567471.\n[50]\n M. Lane, “For real love”: How Black girls benefit from a\n[51]\npoliticized  ethic  of  care, International  Journal  of\nEducation Reform, doi: 10.1177/105678791802700303.\n L. Bass, When care trumps justice: The operationalization\nof  Black  feminist  caring  in  educational  leadership,\nInternational Journal of Qualitative Studies in Education,\ndoi: 10.1080/09518398.2011.647721.\n[52]\n b.  hooks,  Love  as  the  practice  of  freedom,  in Outlaw\nCulture:  Resisting  Representations.  b.  hooks,  ed.  New\nYork, NY, USA: Routledge, 1994, pp. 243–250.\n[53]\n b. hooks, All About Love: New Visions. New York, NY,\nUSA: Harper, 2000.\n[54]\n P.  H.  Collins, Black  Feminist  Thought:  Knowledge,\nConsciousness, and the Politics of Empowerment, 2nd ed.\nNew York, NY, USA: Routledge, 2000.\n[55]\n A.  D.  Selbst,  d.  m.  boyd,  S.  A.  Friedler,  S.\nVenkatasubramanian,  and  J.  Vertesi,  Fairness  and\nabstraction  in  sociotechnical  systems,  in Proceedings  of\nthe\n Conference\n on\n Fairness,\n Accountability,\n and\nTransparency  (FAT* ’19),  Atlanta,  GA,  USA,  2019,  pp.\n59–68.\n[56]\n E.  J.  Dixon-Román,  Diffractive  possibilities:  Cultural\nstudies  and  quantification, Transforming  Anthropology,\ndoi: 10.1111/traa.12074.\n[57]\n J.  Masís,  Making  AI  philosophical  again:  On  Philip  E.\nAgre’s legacy, Continent, vol. 4, no. 1, pp. 58–70, 2014.\n[58]\n P.  Sengers  and  G.  Hertz,  Critical  technical  practice  and\ncritical  making:  Phoebe  Sengers  in  conversation  with\nGarnet  Hertz,  in Conversations  in  Critical  Making,  G.\nHertz, ed. CTheory Books, 2015, pp. 9–20.\n[59]\n K.  Boehner,  S.  David,  J.  Kaye,  and  P.  Sengers,  Critical\ntechnical practice as a methodology for values in design,\npresented at CHI 2005 Workshop on Quality, Values, and\nChoices, Portland, OR, USA, 2005.\n[60]\n P. Dourish, J. Finlay, P. Sengers, and P. Wright, Reflective\nHCI:  Towards  a  critical  technical  practice,  presented  at\nCHI ’04  Extended  Abstracts  on  Human  Factors  in\nComputing Systems, Vienna, Austria, 2004.\n[61]\n R. Albergotti, He predicted the dark side of the Internet 30\nyears ago. Why did no one listen? Philip Agre, a computer\nscientist turned humanities professor, was prescient about\nmany  of  the  ways  technology  would  impact  the  world,\nWashington Post, Aug 2021.\n[62]\n L.  Stark,  Apologos:  A  lightweight  design  method  for\nsociotechnical inquiry, Journal of Social Computing, doi:\n10.23919/JSC.2021.0028.\n[63]\n J.  U.  Korn,  Connecting  race  to  ethics  related  to\ntechnology:  A  call  for  critical  tech  ethics, Journal  of\nSocial Computing, doi: 10.23919/JSC.2021.0026.\n[64]\n R. Ochigame, Informatics of the oppressed, Logic, vol. 11,\npp. 53–74, 2020.\n[65]\n K.  Mayer  and  M.  M.  Malik,  Critical  data  scientists  at\nwork: Summary report of the ICWSM-2019 Workshop on\nCritical  Data  Science,  presented  at  the  Thirteenth\nInternational AAAI Conference on Web and Social Media\n(ICWSM-2019), Munich, Germany, 2019.\n[66]\n G.  Payne  and  J.  Payne,  Positivism  and  realism,  in Key\nConcepts  in  Social  Research. London,  UK:  SAGE\nPublications, 2004, pp. 171–174.\n[67]\n R. Rabaka, Forms of Fanonism: Frantz Fanon’s Critical\nTheory  and  the  Dialectics  of  Decolonization.  Lanham,\nMD, USA: Lexington Books, 2011.\n[68]\n  Maya Malik et al.:   Critical Technical Awakenings\n383    \n \n\n B. Fay, Critical Social Science: Liberation and Its Limits.\nIthaca, NY, USA: Cornell University Press, 1987.\n[69]\n B.  Barnes  and  D.  Bloor,  Relativism,  rationalism  and  the\nsociology of knowledge, in Rationality and Relativism, M.\nHollis  and  S.  Lukes,  eds.  Oxford,  UK:  Basil  Blackwell,\n1982, pp. 21–47.\n[70]\n S.  Shapin  and  S.  Schaffer, Leviathan  and  the Air-Pump:\nHobbes, Boyle, and the Experimental Life. Princeton, NJ,\nUSA: Princeton University Press, 1985.\n[71]\n L.  S.  Clark,  Critical  theory  and  constructivism,  Media,\nCulture and Meaning site, Center for Mass Media, School\nof  Journalism  and  Mass  Communication,  University  of\nColorado, https://web.archive.org/web/20051201184243/\nhttp://www.colorado.edu/journalism/mcm/qmr-crit-\ntheory.htm, 1999.\n[72]\n I. Hacking, The Social Construction of What? Cambridge,\nMA, USA: Harvard University Press, 1999.\n[73]\n P. E. Agre, Notes and recommendations for 12 July 2000,\nRed  Rock  Eater  Newsletter, https://pages.gseis.ucla.edu/\nfaculty/agre/notes/00-7-12.html, 2000.\n[74]\n L.  Wittgenstein, Philosophical  Investigations.  Oxford,\nUK: Wiley-Blackwell, 1953.\n[75]\n O. H. Jara, Popular education and social change in Latin\nAmerica, \nCommunity\n Development\n Journal,\n doi:\n10.1093/cdj/bsq022.\n[76]\n L. Kane, Popular Education and Social Change in Latin\nAmerica. London, UK: Latin American Bureau, 2001.\n[77]\n L.  A.  Hadfield, Liberation  and  Development:  Black\nConsciousness Community Programs in South Africa. East\nLansing, MI, USA: Michigan State University Press, 2016\n[78]\n F. Fanon, Black Skin, White Masks. New York, NY, USA:\nGrove Press, 1952.\n[79]\n N. C. Gibson, ed., Decolonizing Madness: The Psychiatric\nWritings of Frantz Fanon. New York, NY, USA: Palgrave\nMacmillan, 2014.\n[80]\n N.  C.  Gibson,  Decolonizing  madness:  The  psychiatric\nwritings of Frantz Fanon, in Fanon, Phenomenology, and\nPsychology, L. Laubscher, D. Hook, and M. U. Desai, eds.\nNew York, NY, USA: Routledge, 2021.\n[81]\n P.  Freire, Education  for  Critical  Consciousness.  New\nYork, NY, USA: Seabury Press, 1974.\n[82]\n P.  Freire, Education,  the  Practice  of  Freedom.  London,\nUK: Writers and Readers Publishing Cooperative, 1976.\n[83]\n T.  J.  L.  Belle,  From  consciousness  raising  to  popular\neducation\n in\n Latin\n America\n and\n the\n Caribbean,\nComparative Education Review, doi: 10.1086/446677.\n[84]\n K. Sarachild, Consciousness-raising: A radical weapon, in\nFeminist  evolution,  K.  Sarachild,  ed.  New  York,  NY,\nUSA: Random House, 1978, pp. 144–150.\n[85]\n A.\n Darder,\n Conscientizaçao:\n Awakening\n critical\nconsciousness,  in Freire  and  Education,  A.  Darder,  ed.\nNew York, NY, USA: Routledge, 2015, pp. 80–132.\n[86]\n J.  Cammarota  and  M.  Fine, Revolutionizing  Education:\nYouth  Participatory  Action  Research.  New  York,  NY,\n[87]\nUSA: Routledge, 2007.\n N.  Wallerstein  and  B.  Duran,  The  theoretical,  historical,\nand  practice  roots  of  CBPR,  in Community-Based\nParticipatory Research for Health: Advancing Social and\nHealth Equity, N. Wallerstein, B. Duran, J. Oetzel, and M.\nMinkler, eds. San Francisco, CA, USA: Jossey Bass, 2008,\npp. 25–46.\n[88]\n P.  C.  Salzman,  On  reflexivity, American  Anthropologist,\ndoi: 10.1525/aa.2002.104.3.805.\n[89]\n J. Heron and P. Reason, A participatory inquiry paradigm,\nQualitative Inquiry, doi: 10.1177/107780049700300302.\n[90]\n F.  Baum,  C.  MacDougall,  and  D.  Smith,  Participatory\naction research, Journal of Epidemiology and Community\nHealth, doi: 10.1136/jech.2004.028662.\n[91]\nMaya  Malik received  the  BA  degree  in\npsychology  from  Warren  Wilson  College\nin 2014 and the MS degree in Social Work\nfrom  Columbia  University’s  School  of\nSocial  Work  in  2017,  focusing  on\ninternational  social  welfare  and  rights  for\nimmigrants and refugees through program\ndesign, research, and evaluation. They are\ncurrently a doctoral student at the School of Social Work, McGill\nUniversity, where they are researching how to utilize arts-based\nYouth-Led  Participatory  Action  Research  (YPAR)  methods  to\nwork with Queer Black American youth who have been justice-\ninvolved to improve educational intervention programs. They are\nalso a researcher in the Participatory Axis of the McGill Global\nChild Research Group, and a research contributor at the Youth\nand Media project at the Berkman Klein Center for Internet &\nSociety  at  Harvard  University.  They  are  also  a  member  of  an\ninterdisciplinary public health team to address the possible impact\nof  reparations  on  quality  of  life  and  mortality  for  Black\nAmericans.\nMomin M. Malik received the AB degree\nin  History  and  Science  from  Harvard\nUniversity  in  2009,  the  MSc  degree  in\nSocial  Science  of  the  Internet  from  the\nUniversity of Oxford in 2012, and the MS\ndegree  in  machine  learning  and  the  PhD\ndegree\n in\n societal\n computing\n from\nCarnegie Mellon University both in 2018.\nPreviously, he was the Data Science Postdoctoral Fellow at the\nBerkman  Klein  Center  for  Internet  &  Society  at  Harvard\nUniversity,  and  the  Director  of  Data  Science  at  Avant-garde\nHealth. He is currently a Senior Data Scientist for AI Ethics at the\nCenter for Digital Health, Mayo Clinic, a fellow at the Institute in\nCritical  Quantitative,  Computational,  &  Mixed  Methodologies,\nand a lecturer for the School of Social Policy & Practice at the\nUniversity of Pennsylvania. He is a Senior Program Committee\nMember  for  the  International  Conference  for  Web  and  Social\nMedia.\n    384\nJournal of Social Computing, December 2021, 2(4): 365−384",
    "pdf_filename": "Technology Ethics in Action - Critical and Interdisciplinary Perspectives.pdf"
}