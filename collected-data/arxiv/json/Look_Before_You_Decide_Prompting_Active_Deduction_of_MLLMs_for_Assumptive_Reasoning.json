{
    "title": "Look Before You Decide: Prompting Active Deduction",
    "abstract": "as intermediate steps in its reasoning process to handle Recently, Multimodal Large Language Models (MLLMs) complex questions. Although these MLLMs achieved have achieved significant success across multiple disci- unprecedentedsuccess,wewonderwhethertheygenuinely plines due to their exceptional instruction-following capa- demonstrate human-like composite reasoning steps before bilities and extensive world knowledge. However, whether makingthedecision. theseMLLMspossesshuman-likecompositionalreasoning To uncover the reasoning behaviors of MLLMs, we abilities remains an open problem. To unveil their rea- refactoratypicalVQAsamplebyaddingapresupposition soningbehaviors,wefirstcurateaMultimodalAssumptive asshowninFig.1. Whilethismayseemstraightforwardto ReasoningBenchmark(MARS-Bench)inthispaper. Inter- ahuman,thisquestioncaneasilyconfusetheMLLM,lead- estingly, we find that most prevalent MLLMs can be easily ingittoprovideplausibleyetincorrectanswers. Forfurther fooledbytheintroductionofapresuppositionintotheques- analysis, we provide additionalguidance to theMLLM by tion,whereassuchpresuppositionsappearnaivetohuman employingtheChain-of-Thought(CoT)[28,35]technique, reasoning. Besides, we also propose a simple yet effective aiming to unleash its reasoning potential through multi- method,ActiveDeduction(AD),toencouragethemodelto turnreflection. Interestingly, asdemonstratedinFig.1, the activelyperformcompositedeductionbeforereachingafi- MLLM tends to generate a specious CoT process to sup- nal decision. Equipped with the proposed AD method, a port its incorrect answers. Through the above experimen- MLLM demonstrates significant improvements in assump- tal probe, it can be observed that the MLLM is prone to tive reasoning abilities without compromising its general- makedecisionsbasedonitsintuition,synthesizedfromthe purpose question-answering performance. We also pro- knowledgestoredinitsmemory. Wecallsuchbehaviorsof videextensiveevaluationsofbothopen-sourceandprivate theMLLMas“empiricalreasoning”inthispaper. MLLMsonMARS-Bench,alongwithexperimentalanalyses Compared to empirical reasoning, human cognition ex- oftheADmethod. hibits strong compositionality, allowing the expansion of new knowledge by deducing from a finite set of mastered concepts. To tackle the question in Fig.1, it is necessary 1.Introduction to (1) recognize the direction of movement of the car and motorcycle, and (2) comprehendthe meaningof “U-turn”, Recently, the Multimodal Large Language Models finally(3)combinetheresultsin(1)and(2)toreasonabout (MLLMs) [1, 2, 5, 8, 16, 23, 24, 31, 44] have been a the ultimate car direction. However, as previously demon- rising research hotspot due to their potential of serving strated, existing MLLMs fail to produce these crucial rea- as versatile generalists across multiple disciplines. With soning steps. The underlying reason for the empirical rea- world knowledge distilled from vast corpora, MLLMs soning nature of MLLMs lies in their tendency to mimic present remarkable reasoning capabilities in solving chal- behaviors that occur with the highest probabilities across lenging tasks. LISA [20] addresses the task of “reasoning vasttrainingdata,wheresamplesrequiringcomplexlogical segmentation” task by integrating a MLLM with the reasoningarerelativelyscarce. *Equalcontribution. To systematically assess the extent to which existing 4202 voN 91 ]VC.sc[ 4v66921.4042:viXra",
    "body": "Look Before You Decide: Prompting Active Deduction\nof MLLMs for Assumptive Reasoning\nYianLi1,2*,WentaoTian1,2*,YangJiao1,2,JingjingChen1,2,NaZhao3,Yu-GangJiang1,2\n1 ShanghaiKeyLabofIntell. Info. Processing,SchoolofCS,FudanUniversity\n2 ShanghaiCollaborativeInnovationCenteronIntelligentVisualComputing\n3 SingaporeUniversityofTechnologyandDesign\n{yali24, wttian22, yjiao23}@m.fudan.edu.cn\n{chenjingjing, ygj}@fudan.edu.cn, na zhao@sutd.edu.sg\nAbstract SAM[18]. Visual-CoT[33]groundsspecificimageregions\nas intermediate steps in its reasoning process to handle\nRecently, Multimodal Large Language Models (MLLMs) complex questions. Although these MLLMs achieved\nhave achieved significant success across multiple disci- unprecedentedsuccess,wewonderwhethertheygenuinely\nplines due to their exceptional instruction-following capa- demonstrate human-like composite reasoning steps before\nbilities and extensive world knowledge. However, whether makingthedecision.\ntheseMLLMspossesshuman-likecompositionalreasoning To uncover the reasoning behaviors of MLLMs, we\nabilities remains an open problem. To unveil their rea- refactoratypicalVQAsamplebyaddingapresupposition\nsoningbehaviors,wefirstcurateaMultimodalAssumptive asshowninFig.1. Whilethismayseemstraightforwardto\nReasoningBenchmark(MARS-Bench)inthispaper. Inter- ahuman,thisquestioncaneasilyconfusetheMLLM,lead-\nestingly, we find that most prevalent MLLMs can be easily ingittoprovideplausibleyetincorrectanswers. Forfurther\nfooledbytheintroductionofapresuppositionintotheques- analysis, we provide additionalguidance to theMLLM by\ntion,whereassuchpresuppositionsappearnaivetohuman employingtheChain-of-Thought(CoT)[28,35]technique,\nreasoning. Besides, we also propose a simple yet effective aiming to unleash its reasoning potential through multi-\nmethod,ActiveDeduction(AD),toencouragethemodelto turnreflection. Interestingly, asdemonstratedinFig.1, the\nactivelyperformcompositedeductionbeforereachingafi- MLLM tends to generate a specious CoT process to sup-\nnal decision. Equipped with the proposed AD method, a port its incorrect answers. Through the above experimen-\nMLLM demonstrates significant improvements in assump- tal probe, it can be observed that the MLLM is prone to\ntive reasoning abilities without compromising its general- makedecisionsbasedonitsintuition,synthesizedfromthe\npurpose question-answering performance. We also pro- knowledgestoredinitsmemory. Wecallsuchbehaviorsof\nvideextensiveevaluationsofbothopen-sourceandprivate theMLLMas“empiricalreasoning”inthispaper.\nMLLMsonMARS-Bench,alongwithexperimentalanalyses\nCompared to empirical reasoning, human cognition ex-\noftheADmethod.\nhibits strong compositionality, allowing the expansion of\nnew knowledge by deducing from a finite set of mastered\nconcepts. To tackle the question in Fig.1, it is necessary\n1.Introduction\nto (1) recognize the direction of movement of the car and\nmotorcycle, and (2) comprehendthe meaningof “U-turn”,\nRecently, the Multimodal Large Language Models\nfinally(3)combinetheresultsin(1)and(2)toreasonabout\n(MLLMs) [1, 2, 5, 8, 16, 23, 24, 31, 44] have been a\nthe ultimate car direction. However, as previously demon-\nrising research hotspot due to their potential of serving\nstrated, existing MLLMs fail to produce these crucial rea-\nas versatile generalists across multiple disciplines. With\nsoning steps. The underlying reason for the empirical rea-\nworld knowledge distilled from vast corpora, MLLMs\nsoning nature of MLLMs lies in their tendency to mimic\npresent remarkable reasoning capabilities in solving chal-\nbehaviors that occur with the highest probabilities across\nlenging tasks. LISA [20] addresses the task of “reasoning\nvasttrainingdata,wheresamplesrequiringcomplexlogical\nsegmentation” task by integrating a MLLM with the\nreasoningarerelativelyscarce.\n*Equalcontribution. To systematically assess the extent to which existing\n4202\nvoN\n91\n]VC.sc[\n4v66921.4042:viXra\n✓LLaVA-AD\nAnswer the following visual-language reasoning\nquestion, and feel free to break it down and think What is the current direction of the motorcycle and the car?\nthrough it if needed. If the car makes a U-turn, will the The motorcycle is moving in one direction, and the car is\nmotorcycle and the car be going in the same direction? moving in the opposite direction.\nWhat does it mean if the car makes a U-turn?\n✗LLaVA\nIt means the car turns around, changing its direction.\nAfter the car makes a U-turn, would the motorcycle and the\nNo. This is because if the car makes a U-turn, the car will car be going in the same direction?\nbe going in the opposite direction of the motorcycle, Yes, after the car makes a U-turn, the motorcycle and the car\nand the motorcycle is not turning with the car. would be moving in the same direction.\nFigure 1. The results comparison of LLaVA and the LLaVA equipped with our proposed Active Deduction (AD) method when\nfacedwithassumptivereasoningquestions.LLaVAfailstoprovidethecorrectanswerduetotheplausibleyetincorrectCoTitgenerates\n(highlightedinred).Incontrast,ourLLaVA-ADcandecomposetheproblem,deducingtheanswerbasedonthemasteredknowledge.We\nuseredtodenotethewronganswer,andbulbiconstodenotetheCoTinstructionsactivelygeneratedbyourmodel.\nMultimodalLargeLanguageModels(MLLMs)relyonem- itsgeneral-purposequestion-answeringabilities.\npiricalintuitionduringanswergeneration,wecurateanovel Ingeneral, ourcontributionscanbesummarizedasfol-\nMultimodal Assumptive Reasoning Benchmark, abbrevi- lows:\nated as MARS-Bench in this paper. In MARS-Bench, we • We propose a novel Multimodal Assumptive Reasoning\ndesign two sets of questions for obvious comparison. The Benchmark (MARS-Bench), on which we widely assess\nfirstsetofquestionsaimstoinquireaboutthedetailedcon- the assumptive reasoning capabilities of prevalent open-\ntent of the image. These questions are conventional and sourceandprivateMLLMs.\nserve as foundational queries. In the second set of ques- • We introduce an Active Deduction (AD) method to en-\ntions, we introduce a deliberately curated presupposition hance the existing MLLM’s assumptive reasoning abil-\nprior to each foundational question, imposing higher de- ity while not sacrificing its general-purpose question-\nmandsonthemodelto performcross-referentialreflection answeringperformances.\nandreasoninginordertoproducecorrectanswers.Bycom- • We also conduct extensive experiments and provide in-\nparingtheperformanceachievedonthesetwosetsofques- depthanalysestodemonstratethevalueofMARS-Bench\ntions, we can effectively examine a model’s susceptibility andtheeffectivenessoftheADmethod.\ntooverrelianceonitsempiricalintuition. Throughcompre-\nhensively evaluating eight leading open-source models as 2.RelatedWorks\nwellastheadvancedprivatemodel,GPT-4o,onourMARS-\nBench, we observe significant performance degradation MultimodalLargeLanguageModels.\nacross all open-source models, whereas GPT-4o demon- Recentadvancementsinlargelanguagemodels(LLMs)\nstrates considerable robustness, which could offer promis- have shown strong performance across various linguistic\ningavenuesforenhancingthereasoningcapabilitiesofex- tasks [3, 7, 34, 41]. Researchers have extended LLMs\nistingMLLMinthefutureresearch. to multimodal large language models (MLLMs), integrat-\ning visual and language modalities. Flamingo [2] and\nBesides, to enhance the MLLM’s assumptive reasoning Open-Flamingo [4] use modules like Perceiver Resam-\ncapability,wealsointroduceasimpleyeteffectivemethod plerandXAttn-Denseforimprovedfew-shotperformance.\ncalled Active Deduction (AD). Our core motivation lies in LLaMA-Adapter [40] and Otter [21] incorporate cross-\nthatquestionsofvaryingdifficultiesshouldbematchedwith attention layers for multimodal fusion. BLIP2 [23] intro-\ncorrespondinglevelsofcognitiveeffort. Therefore,thepro- duces Q-Former to link vision and frozen LLMs, and sub-\nposed AD method employs a divide-and-conquer strategy sequent models such as InstructBLIP [8] and MiniGPT-\nby introducing two new special tokens <ST> and <ET>, 4 [44] leverage high-quality data for better instruction fol-\nto denote the start and end of the model’s thinking pro- lowing. LLaVA[26]simplifiesvisualintegrationviaapro-\ncess. For simple questions, the model can directly gener- jection layer, while Shikra [6] and Kosmos-2 [31] further\nate answers based on its empirical intuition. When faced enhanceMLLMs’visualgroundingabilities. Qwen-VL[5]\nwithquestionsrequiringcomplexreasoning,themodelcan andMonkey[24]focusonhigh-resolutioninputfordetailed\nbe prompted to generate the <ST> token, thereby actively visualunderstanding.GPT-4V[1]alsodemonstratesexcep-\nengagingincompositionaldeductionbeforearrivingatthe tionalimagecomprehension.MARS-Benchevaluatesthese\nfinal decision. With this dynamic adjustment feature, our modelsformultimodalreasoningcapabilities.\nAD method can significantly promote the assumptive rea- Multimodal Reasoning Benchmarks. Various bench-\nsoningcapabilitiesoftheexistingMLLM,whilepreserving markshavebeenproposedtoevaluatethereasoningability\nof MLLMs. GQA [15] leverages the scene-graph struc- 3.1.ProblemDefinition\nture of Visual Genome [19] to create diverse questions,\nIn this paper, we refer to “assumptive questions” as the\nwhichfocusonmulti-stepreasoningandsceneunderstand-\nquestion that includes an imaginary presupposition to the\ning in real-world images. OK-VQA [30] focuses on ques-\nknown facts. Here, “facts” denote the actual information\ntions that require external resources to answer. Science-\npresented in the image, while “presuppositions” represent\nQA [28] presents a diverse collection of multimodal sci-\nhypotheticalassumptionsaboutchangesinthestateofthis\nence questions, complemented by lecture annotations and\ninformation. Weformulatetheaboveprocessasafunction\ndetailed explanations. MathVista [29] evaluates five pri- f: X→Y thatmapstheinputx∈X totheoutputy ∈Y:\nmary tasks in mathematical reasoning. In addition to the\naforementionedreasoning-specificbenchmarks,somecom- f(v,w ,w )=argmaxP(y′|v,w ,w ).\na q a q\nprehensivebenchmarksalsoencompassavarietyofreason- y′\ningtasks. MME[9]containscommonsensereasoning,nu-\nHere,y′istheoutputofMLLMobtainedthroughanappro-\nmerical calculations, text translation, and code reasoning.\npriate decoder. v, w , and w represent the image, imagi-\nSEED-Bench [22] evaluates reasoning capabilities of vi- a q\nnarypresupposition,andvisualquestionrespectively.\nsual reasoning, science knowledge, and action prediction.\nHowever,mostofthesebenchmarksfocusonconventional 3.2.DiverseAspectsofAssumptions\nmultimodalreasoningandmayoverlookcertainassumptive\nDifferent assumptions demand distinct reasoning capabil-\nscenariosintherealworld.\nities, thus we progressively challenge MLLMs by setting\nImprovingReasoningCapabilitiesofMLLMs. Since tasksacross6keyaspects:\nLLMs cannot directly access visual information, existing • Count:Calculatechangesinthequantityofobjectswhen\nmethods typically capitalize on their instruction-following specificitemsareaddedtoorremovedfromagroup.This\ncapabilitiesbyprovidingin-contextexamplestoguidethem challengesthemodel’sabilitytoperformnumericaloper-\nin utilizing various expert modules for visual reasoning. ationsandtrackquantitychanges.\nMM-React [38] designs a system paradigm that composes • Color: Inferhowthecolorofobjectschangeswhenthey\nnumerousvisionexpertswithChatGPTformultimodalrea- are exchanged or merged. This requires the model to\nsoningandaction. Visprog[12]providesin-contextexam- graspconceptsofcolormixingortransformations.\nples to enable LLMs to generate Python-like visual pro- • Shape: Analyze how objects deform or change shape\ngrams and create distinct modules for performing specific when external forces are applied or when they interact\ntasks. Cantor [10] crafts more sophisticated prompts, en- with other objects. This tests the model’s understanding\nablingMLLMstoemulatetherolesofdomain-specificex- ofspatialtransformationsandphysicalproperties.\nperts,andtherebyfacilitateenhancingrationalityanddepth • Size: Imaginechangesinthesizeofobjectsinspaceand\ninthereasoningprocess. Differentfromtheabovemethod, compare them with other objects. This challenges the\n[28] verifies the effectiveness of applying CoT on Sci- model’s ability to reason about spatial relationships and\nenceQA using LLMs (UnifiedQA, GPT-3). Multimodal- quantitativecomparisonsofmagnitude.\nCoT [42]incorporateslanguageandvisionmodalitiesinto • Direction:Envisionchangesintheorientationorposition\na two-stage framework that separates rationale generation ofobjects.Themodelneedstounderstandshiftsinspatial\nandanswerinference. Inthispaper, weproposeanActive location.\nDeduction framework to systematically improve assump- • Common: Modifyworldconditionsandaskthemodelto\ntive reasoning in MLLMs, backed by our novel MARS- reasonbasedonexternalknowledge.Thiscanencompass\nBenchevaluation. awiderangeofquestionsthatrequirethemodeltoapply\nlogicandknowledgetonewscenarios.\nExamples of each question type are illustrated in Fig.2.\nThishierarchicaldivisionaimstoassessMLLMs’abilityto\n3.MARS-Bench\ncomprehendandinferentities, attributes, andrelationships\nwithinimagesatdifferentlevels.\nWepresenttheMultimodalAssumptiveReaSoningBench-\n3.3.DatasetCuration\nmark (MARS-Bench), a manually curated benchmark to\nevaluate the assumptive reasoning capabilities of MLLMs. Data Source & Human Annotation. Assumptive ques-\nInthissection,wefirstintroducethedefinitionofassump- tionsinvolveimaginarypresuppositionthatentailschanges\ntivequestions,followedbyanoverviewoftheincludedas- in different aspects, which requires images to contain rich\nsumption aspects. Then we elaborate on the dataset con- scenarios and potential semantic information. We thus\nstructionprocess. Finally,wepresenttheevaluationproto- chose images from COCO [25] validation set to annotate\ncol. due to the diverse object categories and rich visual scenes\nCount\n[A] How many people are wearing hats? [A] How many cupcakes are there in the image?\n[B] If one person took off his hat, how many [B] If I ate half of them, how many cupcakes\npeople would left wearing hats? would there be in the image?\nOptions: A. 3 B. 1 Options: A. 12 B. 6\nColor\n[A] What color is the bird in the picture? [A] How many colors are there in the sign?\n[B] If the color of the bird changed to the color of [B] If \"SW\" were painted red, how many colors\nthe fruit next to it, what color would the bird be? would the sign have?\nOptions: A. black B. yellow Options: A. 3 B. 4\nSize\n[A] Which is larger, the elephant or the tire? [A] What is the largest in the picture?\n[B] If the size of the elephant shrank 1000 times, [B] If the cat were one-tenth its original size,\nwhich would be larger, the elephant or the tire? what would be the largest in the picture?\nOptions: A. elephant B. tire Options: A. cat B. remote control\nShape\n[A] What shape is the window? [A] What is the shape of the sign in the picture?\n[B] If I made the window the same shape as the [B] If the sign were the same shape as the tiles on\nlight, what shape would it be? the building behind it, what shape would it be?\nOptions: A. rectangle B. circle Options: A. round B. square\nDirection\n[A] Which direction might the woman go? [A] Where is the truck in relation to the bus?\n[B] If the woman turned around, which direction [B] If the truck and the bus switched places,\nmight she go? where would the truck be in relation to the bus?\nOptions: A. right B. left Options: A. left B. right\nCommon\n[A] Is the man on the elevator going up or down? [A] Can cars drive into the driveway?\n[A] If the suitcase on the elevator was red, would [B] If there is no \"Bus Only\" sign, could cars drive\nthe man on the elevator be going up or down? into the driveway in the picture?\nOptions: A. down B. up Options: A. No B. Yes\nFigure2.ExamplesofdifferenttypesofquestionsincludedinourMARS-Bench.Foreachimage,weprovidethecorrespondingbasic\nandassumptivequestions.Thecorrectanswerisindicatedinbracketsatthebeginningofthequestion.Notethatpresuppositionsmayalso\nactasdistractors,asshownintheleftcaseinthelastrow.\nitoffers,whichcapturecomplexeverydayscenariosclosely answertoaquestionisembeddedinthe“If”clause.For\nalignedwithreal-worlddistributions. instance,“IfIpaintedthisbusblue,whatcolorwouldit\nWe ask annotators to perform assumptive question an- be?”Inthiscase,themodelcanderiveanswersdirectly\nnotations, which follow these steps: (1) Given an image, fromthetextwithoutreferringtotheimagecontent.\nannotatorsfirstselectanappropriatequestiontype. Images (2) Ambiguityoftheanswerreferstotheinabilitytode-\nwithnosuitablequestiontypeswillbeexcluded. (2)After- rive a precise answer from the image. For example, it\nward,annotatorsareaskedtodesignabasicvisualquestion isnotappropriatetoaskaboutthenumberofelephants\nandthenmodifycertainconditionswithinittocreateanas- whenonlyasmallpartofanelephant’sbodyisvisible.\nsumptive question. (3) To further enhance the data quality\nAutomated Question Expansion. We expanded our\nofMARS-Bench,weemployarigorousdatafilterpipeline.\ndataset based on 1,200 manually annotated question pairs.\nSpecifically,eachquestionisverifiedmanuallywithregard\nLeveraging GPT-4’s powerful multimodal understanding\ntothefollowingtwokeyaspects:\nand instruction-following capabilities, we prompted it to\n(1) Information Leakage refers to a situation which the creativelygeneratenewcontentbymimickingexistingan-\n··· ··· ( )( <ST> ··· <ET> )\nLarge Language Model LoRA\n··· ··· ( )( <ST> ··· <ET> )\nProjector ASSISTANT (w/o AD): The answer is 2.\nTokenizer & Word Embedding\nASSISTANT (w/ AD):\nVisual Basic: Assumptive: 1. How many donuts are currently on the plate?\nencoder How many donuts If a donut was There are two donuts on the plate.\nare there on the eaten, how many 2. What happens if a donut is eaten?\nplate? donuts would be If a donut is eaten, one of the donuts will be removed.\non the plate?\n3. After a donut is eaten, how many donuts would be on the plate?\nThere would be one donut remaining on the plate.\nThe answer is 1.\nInference Stream w/ AD\nQuery Tokens <ST>/ <ET> Start/End Thinking Token Reasoning Tokens Answer Tokens\nInference Stream w/o AD\nFigure3.OverviewoftheActiveDeduction(AD)framework,demonstratingthemodel’sdynamicadjustmentbetweendirectempirical\nreasoning and structured deductive reasoning. For basic questions (left input), the model bypasses AD, directly outputting the answer\nbasedonintuition(tokensinthefirstbracket). Formorecomplexassumptivereasoningquestions(rightinput),themodelengagesADby\ngeneratingthe<ST>token,breakingdowntheproblemintointermediatesub-questions,andsynthesizingafinalansweruponreachingthe\n<ET>token(tokensinthesecondbracket).Thisadaptiveapproachallowsthemodeltobalanceefficiencywithreasoningdepth,applying\ndetaileddeductiononlywhennecessary.\nnotations. Specifically, we provided GPT with an image, responsetobasicandassumptivequestionsrespectively.\nalong with manually annotated questions and their corre-\nsponding answers. We then instructed GPT to modify the\n4.ActiveDeduction\npresuppositions while retaining the core question. After\nthis process, we ultimately obtained 6,000 questions. The\nIn this section, we first introduce the overall framework\npromptsusedforeachcategoryvariedslightly,withthede-\nLLaVA-AD, a LLaVA-based MLLM equipped with our\ntailedpromptsprovidedinthesupplementarymaterial. We\nproposed Active Deduction (AD) method. Afterward, we\nensure the newly generated questions match the image in-\nintroducethedatastrategyfortrainingthemodel.\nformation,arelogicallyconsistent,andthattheanswersare\nconciseandclearthroughmanualverification.\n4.1.LLaVA-ADFramework\n3.4.EvaluationProtocol\nThe overall framework of LLaVA-AD is shown in Fig.3.\nGiventhechallengeofquantitativelyevaluatingopen-ended We introduce two new special tokens <ST> and <ET> to\nanswers, we structure the answers in the form of common denote the start and end of the thinking process. The run-\nmultiple-choice questions with two candidates. However, ningpipelineoftheLLaVA-ADmodelincludesthefollow-\nthe instruction-following capabilities of existing MLLMs ingsteps: (1)Givenaquestion,LLaVA-ADfirstimplicitly\nare limited, even if explicitly asked to answer “A” or “B”, judges its complexity. (2) For simple questions, LLaVA-\nthemodelsmaystillproducesomefree-formtext. Further- ADdirectlygeneratesresponsesinamannersimilartocon-\nmore, we also observe that many MLLMs exhibit a pref- ventional MLLMs, as illustrated by the green pathway in\nerenceforcertainoptions,tendingtofavorthosepresented Fig.3. (3) For complex questions, LLaVA-AD first gen-\nearlier.Toeliminatetheinfluenceofinstructionsandoption erates <ST> token to initiate the cross-referential reason-\npreferences,weadoptthesameanswerrankingstrategyas ingprocessstepbystep. Oncethe<ET>isgenerated, the\nSEED-Bench[22]. Specifically,givenaquestion,wecom- modelproceedstoformulatethefinaljudgment. Thispro-\nputethegenerationlossforeachoptionandselectthemini- cess is illustrated by the pink pathway in Fig.3. Through\nmumlossasthemodel’sprediction. Weutilizeaccuracyas these active deduction behaviors, our model can allocate\nourevaluationmetric,whereacc andacc indicatecorrect morecomputationalresourcestochallengingtasks.\nb a\n4.2.DataStrategy pairs and 528 multi-turn dialogue samples from LLaVA-\n1.5-Instruct are used as negative examples for conducting\nTo endow the model with the aforementioned active de-\nextrareasoningsteps,whichenablesthemodeltolearnrea-\nductioncapability,weespeciallyannotateanswerswithde-\nsoningtokenswhileretainingcapabilitiesforbothstandard\ntailedcompositionalreasoningstepsforcomplexquestions.\nVQAtasksandmulti-turndialogue.\nWe utilized GPT-4V [1] for annotating and manually ver-\nBaseline. All baseline models are implemented accord-\nifythecorrectnessofthesethinkingstepsandfilterunreli-\ning to their original setup and evaluated using the strategy\nable ones, resulting in 704 QA pairs with explicit reason-\ndescribed in Sec.3.4. The CoT prompts are based on the\ning steps. We further blend these data with samples from\nprovenapproachoutlinedinScienceQA[28],leveraging1-\nLLaVA-Instruct-150K[26]ata1:1ratiotoobtainthesyn-\nshot manually annotated thinking processes as in-context\nthesizedtrainingdatasetwith1,408samplesintotal.\nlearningexamples.\nStructured reasoning steps annotation. For assumptive\nreasoning questions, we utilized GPT-4V to create struc-\n5.2.BaselineperformanceonMARS-Bench\ntured,stepwiseannotationsthatillustratethedecomposition\nprocessrequiredforaccurateresponses. WeprovidedGPT- Our MARS-Bench has posed formidable challenges to ex-\n4V with each assumptive question and the corresponding istingMLLMs.Tab.1showcasestheperformanceofvarious\nanswer,thenpromptedittoidentifydistinctstepsnecessary MLLMs on assumptive reasoning tasks from the MARS-\nfor reaching the final answer. This annotation process fol- Bench. ItcanbeseenthatallMLLMsperformwellonba-\nlowedaconsistenttemplate,whichinstructedGPT-4Vto: sic questions, achieving an accuracy of around 80%. For\n“color” questions, Qwen-VL-Chat even reaches an accu-\n• Identifytheprimarysub-componentsofthequestionthat\nracy of 93.2%. By comparison, a noticeable decline can\nshouldbeansweredsequentially.\nbe observed when evaluating assumptive questions, high-\n• Provide specific sub-questions and corresponding an-\nlighting the challenge of handling assumptions in reason-\nswersforeachreasoningstep.\ning. All open-source models perform close to the level of\nThese structured data, after being wrapped by <ST> and\nrandomguessing(50%). Amongtheevaluatedmodels,the\n<ET> tokens, provide the model with reliable deductive\ncommercialmodelGPT-4oleadsinoverallaccuracyforas-\nreasoningsamples.\nsumptivereasoningscores486.0,significantlyoutperform-\ningotherMLLMs,indicatingitsrobustnessagainstempiri-\n5.ExperimentsandResults\ncalreasoningtraps.\nIn this section, we first evaluate the performance of cur- Notably, 1-shot ICL and 1-shot CoT do not signifi-\nrentstate-of-the-artMLLMsonMARS-Bench. Weimple- cantly enhance the assumptive reasoning ability of 7B-\nmented0-shot,1-shotICLand1-shotCoT[35]asourbase- level MLLMs and may even lead to a performance drop\nline. Next, we evaluate our proposed “Active Deduction”. in some cases. In Wei et al.’s study [36], models at a cer-\nTheexperimentsshowthat,withminimalimpactongeneral tain scale, specifically those exceeding 100B parameters,\ndatasets,thereasoningtokenfacilitatesthemodel’sreason- demonstrated advanced performance when combined with\ningprocess,significantlyimprovingitsperformanceonas- CoT strategies. However, 7B-level models often lack suf-\nsumptivereasoning. ficient foundational knowledge and reasoning capabilities,\nmaking it challenging for CoT to generate correct reason-\n5.1.Experimentsetup\ningstepsandpotentiallyleadingtoissueslikeAIhallucina-\nTraining Details. We utilize pre-trained CLIP ViT-L/14- tions.\n336 [32] and LLaVA-v1.5-7B [26] as our vision encoder\n5.3.PerformanceofActiveDeduction\nand multimodal large language model, respectively. The\ntokenizer is extended for two special reasoning tokens, ResultsonAssumptiveReasoning. OurActiveDeduction\nwhiletheembed tokensandlm headcomponentsaretuned (AD) method provides a significant boost in performance\nto adapt to the new extended tokenizer. Our model is as shown in Tab.1, with the Active Deduction equipped,\ntrained on 8 NVIDIA RTX-4090 with LoRA [14] and uti- LLaVA-ADachievinganassumptivereasoningaccuracyof\nlizesbf16precisionformixed-precisiontraining.Optimizer 445.4,upfrom369.1inthebaselineLLaVA-1.5[26]. This\nAdamW[17]wasusedwithlearningrate5e-5,acosinede- improvement of 76.3 points illustrates Active Deduction’s\ncay schedule, and a warm-up ratio of 0.03. Training was efficacy in guiding the model through structured reason-\nconductedfor2epochswithabatchsizeof16. ingsteps,especiallyunderscenariosthatchallengeconven-\nDataset. Our training data includes 704 assumptive rea- tionalempiricalintuition.\nsoningproblemswithfinelyannotatedstructuredreasoning Performance gains with Active Deduction vary across\nsteps, wherereasoningstepsareencapsulatedusing<ST> categories. Tasks involving count and size see the most\nand <ET> tokens. 176 manually labeled standard VQA substantialimprovement,withanaccuracyimprovementof\nCount Color Size Shape Direction Common Total\nModel Prompt\nacc acc acc acc acc acc acc acc acc acc acc acc acc acc\nb a b a b a b a b a b a b a\n0-shot 85.9 50.8 89.0 75.3 71.5 52.1 85.2 54.6 70.3 54.5 79.9 58.1 481.8 345.4\nQwen-VL-Chat[5] 1-shot 88.4 63.6 93.2 76.3 74.6 62.6 88.9 59.6 74.2 58.8 83.5 68.7 502.8 389.6\nCoT 88.1 62.0 93.2 76.2 77.6 60.3 90.1 60.2 73.4 57.2 85.6 67.5 508.0 383.4\n0-shot 88.4 61.3 92.2 74.4 69.1 58.5 85.2 54.6 75.8 51.2 84.9 64.8 495.6 364.8\nMonkey[24] 1-shot 88.1 58.7 92.2 72.3 77.6 58.5 86.4 54.0 76.6 48.1 82.7 61.9 503.6 353.5\nCoT 87.1 60.9 92.2 73.2 77.6 58.9 86.4 58.3 76.6 49.2 82.7 65.3 502.6 365.8\n0-shot 72.9 53.3 81.7 69.6 67.9 65.3 70.4 50.0 53.9 52.3 81.3 60.1 428.1 350.6\nBLIP2[23] 1-shot 72.7 43.6 81.5 53.0 69.2 49.2 72.3 37.4 51.0 41.8 82.8 38.7 429.5 263.7\nCoT 73.3 50.7 80.5 52.4 68.6 49.2 69.5 36.4 54.8 41.4 81.3 38.0 428.0 268.1\n0-shot 84.3 67.4 89.5 72.6 61.8 55.9 75.3 55.9 58.6 53.7 80.6 61.9 450.1 367.4\nInstructBLIP[8] 1-shot 84.1 49.5 88.7 52.8 61.5 50.1 75.9 39.5 59.4 43.2 78.3 41.2 447.9 276.3\nCoT 85.4 48.4 89.5 55.1 61.7 47.6 76.0 40.7 58.3 48.0 80.3 41.4 451.3 281.3\n0-shot 86.2 73.3 81.7 70.4 63.0 56.4 67.9 71.6 64.1 52.2 69.8 61.7 432.7 385.6\nxGen-MM[37] 1-shot 85.5 77.9 83.3 68.6 66.1 57.0 67.9 71.3 69.5 51.4 69.8 62.1 442.1 388.3\nCoT 85.5 78.9 83.3 67.7 66.1 56.4 69.1 72.5 71.1 50.6 68.4 61.7 443.5 387.8\n0-shot 90.6 64.1 93.2 72.8 67.3 55.3 76.5 69.1 75.0 56.6 84.2 67.5 486.7 385.4\nInfMLLM[43] 1-shot 90.8 61.2 93.8 72.0 66.8 54.4 75.6 69.4 74.7 57.0 83.8 66.4 485.4 380.5\nCoT 91.1 59.8 92.3 73.3 68.9 54.2 75.9 67.6 75.5 57.0 85.4 66.2 489.0 378.2\n0-shot 86.2 50.6 90.6 71.9 60.6 53.2 71.6 72.5 68.0 57.0 76.3 63.9 453.2 369.1\nLLaVA-1.5[26] 1-shot 79.2 52.6 86.9 70.8 58.2 53.0 51.9 77.2 68.8 55.9 74.8 63.5 419.8 372.9\nCoT 79.2 54.1 90.0 73.2 55.8 55.0 58.0 77.8 67.2 57.4 76.3 63.3 426.5 380.8\nGPT-4o[1] N/A 90.9 91.7 94.2 87.3 88.9 87.4 88.2 80.3 85.2 66.0 87.6 73.4 535.0 486.0\nLLaVA-AD(Ours) N/A 81.8 80.6 89.5 78.1 77.0 73.0 77.8 82.1 71.1 59.8 82.7 71.8 479.9 445.4(+76.3)\nTable1. PerformanceofprevalentMLLMsonsixtaskswithinourproposedMARS-Bench. Here,acc representstheaccuracyfor\nb\ncorrectlyansweringbasicquestions,acc denotestheaccuracyforcorrectlyansweringassumptivequestions.Wecompared7opensource\na\nmodelsandonecommercialmodel,andweboldedthebestperformanceoftheopensourcemodelsonassumptivequestion.\nMethod VQAv2 VisWiz GQA MME MM-Vet MMBench SEED MARS-Assum.\nBLIP-2[23] 65.0 19.6 41.0 1293.8 22.4 - 49.7 350.6\nInstructBLIP[8] - 34.5 49.2 - 26.2 36.0 58.8 367.4\nMiniGPT-4[44] - - - 581.7 22.1 24.3 47.4 341.2\nQwen-VL-Chat[5] 78.2 38.9 57.5 1487.5 - 60.6 65.4 345.4\nLLaVA-AD(Ours) 77.3 50.6 60.3 1471.5 34.3 63.3 64.1 445.4\nTable2. ResultsonprevalentVQAbenchmarks. WeemployMMBench-en-dev,SEED-Bench-img,MME-testforevaluation. MARS-\nAssum. donatesthetotalscoreofassumptivequestionsonourMARS-Bench,andweuse“-”todenotetheperformancewhicharenot\nprovidedintheirpaper.\n30and19.8,respectively. Forinstance,counttasksrequire directional perception is bounded, which restricts its abil-\nthe model to handle hypothetical additions or removals of itytoproducesufficientandaccuratereasoningsteps. The\nitems accurately, a challenge well-suited to AD’s stepwise commonsensetasks,whichinvolvegeneralknowledgeap-\ndeductionapproach.Theshapetasks,involvingtransforma- plicationsunderpresuppositionscenarios,seeasatisfactory\ntions or deformations, similarly benefit from the method’s improvement,suggestingthatourActiveDeductionmethod\nstructuredbreakdown,whichhelpsthemodelmanagecom- is effective at guiding reasoning in knowledge-driven con-\nplex spatial changes logically. While, the effectiveness of textswhenthetaskcomplexityalignswiththemodel’sin-\nADreliesonthefoundationalcapabilitiesoftheunderlying herentcapabilities.\nmodel,meaningthatimprovementisultimatelyconstrained\nPerformance on standard VQA benchmarks. To ex-\nby the model’s base competence in each category. There\naminegeneralvisualunderstandingandinstructionfollow-\nare limited gains for direction tasks, since current models’\ning of our model, we measured its accuracy on standard\nVQAtaskswithoutassumptivereasoningcomponents. We Method De.%Basic↓ Em.%Assum.↓ Avg.E.R.↓\nfollowed common practices by conducting assessments on LLaVA-ADw/otoken 17.9% 5.8% 11.9%\n7 widely used benchmarks: VQAv2 [11], VisWiz [13],\nLLaVA-ADw/token 4.3% 6.1% 5.2%\nGQA [15], MME [9], MM-Vet [39], MMBench [27], and\nSEED-Bench [22]. As shown in Tab.2, our LLaVA-AD Table4. Impactofreasoningtokenonproblemdifficultyde-\nachieves comparable results to state-of-the-art MLLMs in termination. Here, De.%Basic, Em.%Assum. and Avg. E.R.\nVQAtasks,whilealsoenhancingitscapabilitiesinsystem- meanstheratioofdeductivereasoninginbasicsituation, empir-\naticreasoning,particularlyinassumptivereasoning. icalreasoninginassumptivequestion,andtheaverageerrorrate,\nrespectively.\n5.4.AblationStudy\nWe conducted an ablation study to isolate the effects of\nseen in Tab.3. Our evaluation on MARS-Bench examines\nthe<ST>and<ET>tokensandstructuredreasoningsteps,\nwhether reasoning tokens are used appropriately, avoiding\nboth integral to the AD framework. The LLaVA-AD w/o\nextrastepsforbasicquestionsandactivatingdeductionfor\ntoken model is trained on the same finely annotated struc-\nassumptiveones. Tab.4showsthatLLaVA-ADw/otokens\nturedreasoningstepswithout<ST>and<ET>tokens,with\noften over-processes simple questions, leading to its per-\nalltrainingconfigsaresame.\nformance drop in Tab.3. Introducing reasoning tokens im-\nproves problem difficulty determination, which is crucial\nMethod GQA MM-Vet MARS-B MARS-A\nforaccuratelysolvingproblemsofvaryingcomplexity.\nQwen-VL-Chat[5] 57.5 - 481.8 345.4\nLLaVA-1.5[26] 62.0 31.1 453.2 369.1\nLLaVA-ADw/otoken 59.4 27.5 431.2 447.1\nLLaVA-ADw/token 60.2 34.3 479.9 445.4\nTable 3. Comparison of Models across Various Benchmarks.\nLLaVA-ADw/tokenistrainedonthesamedatasetwithoutrea-\nsoning token wrapped. MARS-B. and MARS-A. represents the\ntotalscoreofbasicquestionsandassumptivequestionsonMARS-\nBench,respectively. I am a man. Should I go to the toilet on the left?\n1. What symbols are present on the sign?\nImpactonreasoningperformance. Weconductedanab-\nThe sign displays two yellow icons on a dark background.\nlation study to evaluate the impact of introducing reason- 2. What do the icons mean?\nThe left is a stick figure without a skirt, representing the male\ning tokens on model’s performance across MARS-Bench\nrestroom, and the one on the right is a stick figure with a skirt,\nand two additional general-purpose datasets. From Tab.3, representing the female restroom.\n3. Is the left side appropriate for a male user?\nit is evident that incorporating the reasoning tokens im-\nYes, it is the correct restroom for you as a man.\nproves the model’s performance on GQA [15] tasks, of-\nfering a modest advantage over the model without the to-\nken. In tasks assessed by MM-Vet [39], a large language\nmodel-basedevaluatorforopen-endedoutputs,modelwith\nreasoning tokens showed a more substantial improvement,\nsuggesting that the reasoning tokens effectively enhance\nthe model’s capability for systematic, open-ended reason- It's rainy outside. Is the thing on the left useful?\ning. LLaVA-AD w/o token demonstrates an advantage in\n1. What is the object on the left?\nassumptive reasoning tasks, due to its tendency to adopt a\nThe object on the left is a green umbrella.\ndirect output pattern associated with deductive reasoning, 2. What is the purpose of an umbrella?\nUmbrellas are typically used to shield people from rain.\nwhich excels in scenarios requiring assumptions. While,\n3. Is the object functioning properly to serve its purpose?\nthe use of reasoning tokens enables model to balance per- The umbrella appears to be upright and intact, suggesting it can\nbe used effectively to keep someone dry in the rain.\nformance, reducing unnecessary reasoning steps for basic\nquestions and significantly boosting results across other\nVQA benchmarks with minimal trade-offs in assumptive Figure 4. The generalizability of the LLaVA-AD beyond the\nreasoningtasks. domain. Themodelhaszero-shotcapabilityandcanactivatede-\nImpact on problem difficulty determination. Accurate ductionreasoningonotherreasoningproblems.\ndifficultypredictionisessentialtooptimizereasoning.Mis-\njudging complexity leads to unnecessary processing, in- Generalizabilitybeyondthedomain.LLaVA-ADdemon-\ncreasing costs and potentially reducing performance, as stratesacertaindegreeofgeneralizationabilityacrossother\nreasoning tasks. As shown in Fig.4, it can tackle general [7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nreasoning by decomposing the queries into a sequence of Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nsub-questions, whichallowsittosystematicallyextractin- Mostafa Dehghani, Siddhartha Brahma, et al. Scaling\nformationfrombothimagesandprovidedconditions. This instruction-finetuned language models. arXiv preprint\narXiv:2210.11416,2022. 2\napproach enables the model to mimic human-like logical\n[8] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat\nprogressioneveninunfamiliarcontexts.\nTiong, JunqiZhao, WeishengWang, BoyangLi, PascaleN\nFung, and Steven Hoi. Instructblip: Towards general-\n6.Conclusion\npurposevision-languagemodelswithinstructiontuning.Ad-\nvancesinNeuralInformationProcessingSystems,36,2024.\nIn conclusion, we presented MARS-Bench, a benchmark\n1,2,7\ntargetingAssumptiveReasoninginMultimodalLargeLan-\n[9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nguage Models (MLLMs), and introduced the Active De-\nMengdanZhang,XuLin,JinruiYang,XiawuZheng,KeLi,\nduction (AD) method to enhance these models’ reason-\nXingSun,YunshengWu,andRongrongJi.Mme:Acompre-\ning capabilities. Our findings show that current MLLMs hensiveevaluationbenchmarkformultimodallargelanguage\nstruggle with systematic reasoning problems like assump- models. arXivpreprintarXiv:2306.13394,2023. 3,8\ntivereasoning. Besides,ActiveDeductionsubstantiallyim- [10] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu,\nprovesMLLMs’performanceonassumptivetasksbyguid- Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu\ning structured, stepwise deductive reasoning without sac- Zheng, Xing Sun, Liujuan Cao, and Rongrong Ji. Cantor:\nrificing performance on simpler queries. This work un- Inspiringmultimodalchain-of-thoughtofmllm. InProceed-\ningsofthe32ndACMInternationalConferenceonMultime-\nderscores the limitations of empirical reasoning in current\ndia,page9096–9105,NewYork,NY,USA,2024.Associa-\nMLLMs and suggests a potential approach for fostering\ntionforComputingMachinery. 3\nmore human-like reasoning in systematic, presuppositions\n[11] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBa-\ncomplexscenarios.\ntra,andDeviParikh. Makingthevinvqamatter: Elevating\nthe role of image understanding in visual question answer-\nReferences\ning. In Proceedings of the IEEE conference on computer\nvisionandpatternrecognition,pages6904–6913,2017. 8\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\n[12] TanmayGuptaandAniruddhaKembhavi. Visualprogram-\nmad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,\nming: Compositionalvisualreasoningwithouttraining. In\nJankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.\nProceedingsoftheIEEE/CVFConferenceonComputerVi-\nGpt-4 technical report. arXiv preprint arXiv:2303.08774,\nsionandPatternRecognition,pages14953–14962,2023. 3\n2023. 1,2,6,7\n[13] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,Chi\n[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nVizwiz grand challenge: Answering visual questions from\nsch,KatherineMillican,MalcolmReynolds,etal.Flamingo:\nblind people. In Proceedings of the IEEE conference on\na visual language model for few-shot learning. Advances\ncomputervisionandpatternrecognition,pages3608–3617,\ninneuralinformationprocessingsystems,35:23716–23736,\n2018. 8\n2022. 1,2\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\n[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John- Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Lora: Low-rankadaptationoflargelanguagemodels. arXiv\nEmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2 preprintarXiv:2106.09685,2021. 6\ntechnicalreport. arXivpreprintarXiv:2305.10403,2023. 2\n[15] DrewAHudsonandChristopherDManning. Gqa: Anew\n[4] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,Yusuf dataset for real-world visual reasoning and compositional\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, questionanswering. InProceedingsoftheIEEE/CVFcon-\nSamirGadre,ShioriSagawa,etal. Openflamingo:Anopen- ference on computer vision and pattern recognition, pages\nsource framework for training large autoregressive vision- 6700–6709,2019. 3,8\nlanguage models. arXiv preprint arXiv:2308.01390, 2023. [16] YangJiao, ShaoxiangChen, ZequnJie, JingjingChen, Lin\n2 Ma, and Yu-Gang Jiang. Lumen: Unleashing versatile\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan vision-centriccapabilitiesoflargemultimodalmodels.arXiv\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren preprintarXiv:2403.07304,2024. 1\nZhou.Qwen-vl:Afrontierlargevision-languagemodelwith [17] Diederik P Kingma. Adam: A method for stochastic opti-\nversatileabilities.arXivpreprintarXiv:2308.12966,2023.1, mization. arXivpreprintarXiv:1412.6980,2014. 6\n2,7,8 [18] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multi- head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-\nmodal llm’s referential dialogue magic. arXiv preprint thing. InProceedingsoftheIEEE/CVFInternationalCon-\narXiv:2306.15195,2023. 2 ferenceonComputerVision,pages4015–4026,2023. 1\n[19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, benchmark requiring external knowledge. In Proceedings\nKenjiHata,JoshuaKravitz,StephanieChen,YannisKalan- oftheIEEE/cvfconferenceoncomputervisionandpattern\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome: recognition,pages3195–3204,2019. 3\nConnectinglanguageandvisionusingcrowdsourceddense [31] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan\nimageannotations.Internationaljournalofcomputervision, Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\n123:32–73,2017. 3 ingmultimodallargelanguagemodelstotheworld. arXiv\n[20] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui preprintarXiv:2306.14824,2023. 1,2\nYuan,ShuLiu,andJiayaJia. Lisa:Reasoningsegmentation [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nvialargelanguagemodel. InProceedingsoftheIEEE/CVF Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nConference on Computer Vision and Pattern Recognition, AmandaAskell,PamelaMishkin,JackClark,etal.Learning\npages9579–9589,2024. 1 transferable visual models from natural language supervi-\n[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, sion.InInternationalconferenceonmachinelearning,pages\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal 8748–8763.PMLR,2021. 6\nmodel with in-context instruction tuning. arXiv preprint [33] HaoShao, ShengjuQian, HanXiao, GuangluSong, Zhuo-\narXiv:2305.03726,2023. 2 fanZong,LetianWang,YuLiu,andHongshengLi. Visual\ncot: Unleashingchain-of-thoughtreasoninginmulti-modal\n[22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\nlanguagemodels.arXivpreprintarXiv:2403.16999,2024.1\nWang,RuimaoZhang,andYingShan. Seed-bench: Bench-\nmarkingmultimodallargelanguagemodels. InProceedings [34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\noftheIEEE/CVFConferenceonComputerVisionandPat- Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste\nternRecognition,pages13299–13308,2024. 3,5,8 Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\narXivpreprintarXiv:2302.13971,2023. 2\nBlip-2: Bootstrapping language-image pre-training with\n[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nfrozen image encoders and large language models. In In-\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nternational conference on machine learning, pages 19730–\nChain-of-thought prompting elicits reasoning in large lan-\n19742.PMLR,2023. 1,2,7\nguage models. Advances in neural information processing\n[24] ZhangLi, BiaoYang, QiangLiu, ZhiyinMa, ShuoZhang,\nsystems,35:24824–24837,2022. 1,6\nJingxuYang,YaboSun,YuliangLiu,andXiangBai. Mon-\n[36] JerryWei,JasonWei,YiTay,DustinTran,AlbertWebson,\nkey:Imageresolutionandtextlabelareimportantthingsfor\nYifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny\nlargemulti-modalmodels.arXivpreprintarXiv:2311.06607,\nZhou,etal. Largerlanguagemodelsdoin-contextlearning\n2023. 1,2,7\ndifferently. arXivpreprintarXiv:2303.03846,2023. 6\n[25] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,\n[37] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan,\nPietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence\nSenthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yu-\nZitnick. Microsoft coco: Common objects in context. In\ntong Dai, Michael S Ryoo, et al. xgen-mm (blip-3): A\nComputerVision–ECCV2014: 13thEuropeanConference,\nfamily of open large multimodal models. arXiv preprint\nZurich, Switzerland, September 6-12, 2014, Proceedings,\narXiv:2408.08872,2024. 7\nPartV13,pages740–755.Springer,2014. 3\n[38] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\n[26] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nVisual instruction tuning. Advances in neural information\nMichaelZeng,andLijuanWang.Mm-react:Promptingchat-\nprocessingsystems,36,2024. 2,6,7,8\ngptformultimodalreasoningandaction,2023. 3\n[27] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang\n[39] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nZhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe,\nKevinLin, ZichengLiu, XinchaoWang, andLijuanWang.\nZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan\nMm-vet: Evaluatinglargemultimodalmodelsforintegrated\nall-aroundplayer? arXivpreprintarXiv:2307.06281,2023. capabilities. arXivpreprintarXiv:2308.02490,2023. 8\n8\n[40] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\n[28] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-Wei ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Llama-adapter: Efficient fine-tuning of language models\nAshwinKalyan.Learntoexplain:Multimodalreasoningvia with zero-init attention. arXiv preprint arXiv:2303.16199,\nthought chains for science question answering. Advances 2023. 2\nin Neural Information Processing Systems, 35:2507–2521, [41] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,\n2022. 1,3,6 MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,\n[29] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi, XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtrans-\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel formerlanguagemodels. arXivpreprintarXiv:2205.01068,\nGalley,andJianfengGao. Mathvista:Evaluatingmathemat- 2022. 2\nicalreasoningoffoundationmodelsinvisualcontexts.arXiv [42] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\npreprintarXiv:2310.02255,2023. 3 George Karypis, and Alex Smola. Multimodal chain-of-\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and thought reasoning in language models. arXiv preprint\nRoozbeh Mottaghi. Ok-vqa: A visual question answering arXiv:2302.00923,2023. 3\n[43] QiangZhou, Zhibin Wang, Wei Chu, Yinghui Xu, HaoLi,\nand Yuan Qi. Infmllm: A unified framework for visual-\nlanguagetasks. arXivpreprintarXiv:2311.06791,2023. 7\n[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstandingwithadvancedlargelanguagemodels. arXiv\npreprintarXiv:2304.10592,2023. 1,2,7",
    "pdf_filename": "Look_Before_You_Decide_Prompting_Active_Deduction_of_MLLMs_for_Assumptive_Reasoning.pdf"
}