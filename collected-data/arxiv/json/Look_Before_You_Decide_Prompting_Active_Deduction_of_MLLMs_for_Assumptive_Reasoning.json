{
    "title": "Look Before You Decide Prompting Active Deduction of MLLMs for Assumptive Reasoning",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved significant success across multiple disci- plines due to their exceptional instruction-following capa- bilities and extensive world knowledge. However, whether these MLLMs possess human-like compositional reasoning abilities remains an open problem. To unveil their rea- soning behaviors, we first curate a Multimodal Assumptive Reasoning Benchmark (MARS-Bench) in this paper. Inter- estingly, we find that most prevalent MLLMs can be easily fooled by the introduction of a presupposition into the ques- tion, whereas such presuppositions appear naive to human reasoning. Besides, we also propose a simple yet effective method, Active Deduction (AD), to encourage the model to actively perform composite deduction before reaching a fi- nal decision. Equipped with the proposed AD method, a MLLM demonstrates significant improvements in assump- tive reasoning abilities without compromising its general- purpose question-answering performance. We also pro- vide extensive evaluations of both open-source and private MLLMs on MARS-Bench, along with experimental analyses of the AD method.",
    "body": "Look Before You Decide: Prompting Active Deduction\nof MLLMs for Assumptive Reasoning\nYian Li1,2*, Wentao Tian1,2*, Yang Jiao1,2, Jingjing Chen1,2, Na Zhao3, Yu-Gang Jiang1,2\n1 Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\n2 Shanghai Collaborative Innovation Center on Intelligent Visual Computing\n3 Singapore University of Technology and Design\n{yali24, wttian22, yjiao23}@m.fudan.edu.cn\n{chenjingjing, ygj}@fudan.edu.cn, na zhao@sutd.edu.sg\nAbstract\nRecently, Multimodal Large Language Models (MLLMs)\nhave achieved significant success across multiple disci-\nplines due to their exceptional instruction-following capa-\nbilities and extensive world knowledge. However, whether\nthese MLLMs possess human-like compositional reasoning\nabilities remains an open problem.\nTo unveil their rea-\nsoning behaviors, we first curate a Multimodal Assumptive\nReasoning Benchmark (MARS-Bench) in this paper. Inter-\nestingly, we find that most prevalent MLLMs can be easily\nfooled by the introduction of a presupposition into the ques-\ntion, whereas such presuppositions appear naive to human\nreasoning. Besides, we also propose a simple yet effective\nmethod, Active Deduction (AD), to encourage the model to\nactively perform composite deduction before reaching a fi-\nnal decision. Equipped with the proposed AD method, a\nMLLM demonstrates significant improvements in assump-\ntive reasoning abilities without compromising its general-\npurpose question-answering performance.\nWe also pro-\nvide extensive evaluations of both open-source and private\nMLLMs on MARS-Bench, along with experimental analyses\nof the AD method.\n1. Introduction\nRecently,\nthe\nMultimodal\nLarge\nLanguage\nModels\n(MLLMs) [1, 2, 5, 8, 16, 23, 24, 31, 44] have been a\nrising research hotspot due to their potential of serving\nas versatile generalists across multiple disciplines.\nWith\nworld knowledge distilled from vast corpora, MLLMs\npresent remarkable reasoning capabilities in solving chal-\nlenging tasks. LISA [20] addresses the task of “reasoning\nsegmentation” task by integrating a MLLM with the\n*Equal contribution.\nSAM [18]. Visual-CoT [33] grounds specific image regions\nas intermediate steps in its reasoning process to handle\ncomplex questions.\nAlthough these MLLMs achieved\nunprecedented success, we wonder whether they genuinely\ndemonstrate human-like composite reasoning steps before\nmaking the decision.\nTo uncover the reasoning behaviors of MLLMs, we\nrefactor a typical VQA sample by adding a presupposition\nas shown in Fig.1. While this may seem straightforward to\na human, this question can easily confuse the MLLM, lead-\ning it to provide plausible yet incorrect answers. For further\nanalysis, we provide additional guidance to the MLLM by\nemploying the Chain-of-Thought (CoT) [28, 35] technique,\naiming to unleash its reasoning potential through multi-\nturn reflection. Interestingly, as demonstrated in Fig.1, the\nMLLM tends to generate a specious CoT process to sup-\nport its incorrect answers. Through the above experimen-\ntal probe, it can be observed that the MLLM is prone to\nmake decisions based on its intuition, synthesized from the\nknowledge stored in its memory. We call such behaviors of\nthe MLLM as “empirical reasoning” in this paper.\nCompared to empirical reasoning, human cognition ex-\nhibits strong compositionality, allowing the expansion of\nnew knowledge by deducing from a finite set of mastered\nconcepts. To tackle the question in Fig.1, it is necessary\nto (1) recognize the direction of movement of the car and\nmotorcycle, and (2) comprehend the meaning of “U-turn”,\nfinally (3) combine the results in (1) and (2) to reason about\nthe ultimate car direction. However, as previously demon-\nstrated, existing MLLMs fail to produce these crucial rea-\nsoning steps. The underlying reason for the empirical rea-\nsoning nature of MLLMs lies in their tendency to mimic\nbehaviors that occur with the highest probabilities across\nvast training data, where samples requiring complex logical\nreasoning are relatively scarce.\nTo systematically assess the extent to which existing\narXiv:2404.12966v4  [cs.CV]  19 Nov 2024\n\nNo. This is because if the car makes a U-turn, the car will \nbe going in the opposite direction of the motorcycle, \nand the motorcycle is not turning with the car.\nLLaVA\nAnswer the following visual-language reasoning \nquestion, and feel free to break it down and think \nthrough it if needed. If the car makes a U-turn, will the \nmotorcycle and the car be going in the same direction?\nLLaVA-AD\nWhat is the current direction of the motorcycle and the car?\n      The motorcycle is moving in one direction, and the car is \nmoving in the opposite direction.\nWhat does it mean if the car makes a U-turn?\n      It means the car turns around, changing its direction.\nAfter the car makes a U-turn, would the motorcycle and the \ncar be going in the same direction?\n      Yes, after the car makes a U-turn, the motorcycle and the car \nwould be moving in the same direction.\n✗\n✓\nFigure 1. The results comparison of LLaVA and the LLaVA equipped with our proposed Active Deduction (AD) method when\nfaced with assumptive reasoning questions. LLaVA fails to provide the correct answer due to the plausible yet incorrect CoT it generates\n(highlighted in red). In contrast, our LLaVA-AD can decompose the problem, deducing the answer based on the mastered knowledge. We\nuse red to denote the wrong answer, and bulb icons to denote the CoT instructions actively generated by our model.\nMultimodal Large Language Models (MLLMs) rely on em-\npirical intuition during answer generation, we curate a novel\nMultimodal Assumptive Reasoning Benchmark, abbrevi-\nated as MARS-Bench in this paper. In MARS-Bench, we\ndesign two sets of questions for obvious comparison. The\nfirst set of questions aims to inquire about the detailed con-\ntent of the image. These questions are conventional and\nserve as foundational queries. In the second set of ques-\ntions, we introduce a deliberately curated presupposition\nprior to each foundational question, imposing higher de-\nmands on the model to perform cross-referential reflection\nand reasoning in order to produce correct answers. By com-\nparing the performance achieved on these two sets of ques-\ntions, we can effectively examine a model’s susceptibility\nto overreliance on its empirical intuition. Through compre-\nhensively evaluating eight leading open-source models as\nwell as the advanced private model, GPT-4o, on our MARS-\nBench, we observe significant performance degradation\nacross all open-source models, whereas GPT-4o demon-\nstrates considerable robustness, which could offer promis-\ning avenues for enhancing the reasoning capabilities of ex-\nisting MLLM in the future research.\nBesides, to enhance the MLLM’s assumptive reasoning\ncapability, we also introduce a simple yet effective method\ncalled Active Deduction (AD). Our core motivation lies in\nthat questions of varying difficulties should be matched with\ncorresponding levels of cognitive effort. Therefore, the pro-\nposed AD method employs a divide-and-conquer strategy\nby introducing two new special tokens <ST> and <ET>,\nto denote the start and end of the model’s thinking pro-\ncess. For simple questions, the model can directly gener-\nate answers based on its empirical intuition. When faced\nwith questions requiring complex reasoning, the model can\nbe prompted to generate the <ST> token, thereby actively\nengaging in compositional deduction before arriving at the\nfinal decision. With this dynamic adjustment feature, our\nAD method can significantly promote the assumptive rea-\nsoning capabilities of the existing MLLM, while preserving\nits general-purpose question-answering abilities.\nIn general, our contributions can be summarized as fol-\nlows:\n• We propose a novel Multimodal Assumptive Reasoning\nBenchmark (MARS-Bench), on which we widely assess\nthe assumptive reasoning capabilities of prevalent open-\nsource and private MLLMs.\n• We introduce an Active Deduction (AD) method to en-\nhance the existing MLLM’s assumptive reasoning abil-\nity while not sacrificing its general-purpose question-\nanswering performances.\n• We also conduct extensive experiments and provide in-\ndepth analyses to demonstrate the value of MARS-Bench\nand the effectiveness of the AD method.\n2. Related Works\nMultimodal Large Language Models.\nRecent advancements in large language models (LLMs)\nhave shown strong performance across various linguistic\ntasks [3, 7, 34, 41].\nResearchers have extended LLMs\nto multimodal large language models (MLLMs), integrat-\ning visual and language modalities.\nFlamingo [2] and\nOpen-Flamingo [4] use modules like Perceiver Resam-\npler and XAttn-Dense for improved few-shot performance.\nLLaMA-Adapter [40] and Otter [21] incorporate cross-\nattention layers for multimodal fusion. BLIP2 [23] intro-\nduces Q-Former to link vision and frozen LLMs, and sub-\nsequent models such as InstructBLIP [8] and MiniGPT-\n4 [44] leverage high-quality data for better instruction fol-\nlowing. LLaVA [26] simplifies visual integration via a pro-\njection layer, while Shikra [6] and Kosmos-2 [31] further\nenhance MLLMs’ visual grounding abilities. Qwen-VL [5]\nand Monkey [24] focus on high-resolution input for detailed\nvisual understanding. GPT-4V [1] also demonstrates excep-\ntional image comprehension. MARS-Bench evaluates these\nmodels for multimodal reasoning capabilities.\nMultimodal Reasoning Benchmarks. Various bench-\nmarks have been proposed to evaluate the reasoning ability\n\nof MLLMs.\nGQA [15] leverages the scene-graph struc-\nture of Visual Genome [19] to create diverse questions,\nwhich focus on multi-step reasoning and scene understand-\ning in real-world images. OK-VQA [30] focuses on ques-\ntions that require external resources to answer. Science-\nQA [28] presents a diverse collection of multimodal sci-\nence questions, complemented by lecture annotations and\ndetailed explanations. MathVista [29] evaluates five pri-\nmary tasks in mathematical reasoning. In addition to the\naforementioned reasoning-specific benchmarks, some com-\nprehensive benchmarks also encompass a variety of reason-\ning tasks. MME [9] contains commonsense reasoning, nu-\nmerical calculations, text translation, and code reasoning.\nSEED-Bench [22] evaluates reasoning capabilities of vi-\nsual reasoning, science knowledge, and action prediction.\nHowever, most of these benchmarks focus on conventional\nmultimodal reasoning and may overlook certain assumptive\nscenarios in the real world.\nImproving Reasoning Capabilities of MLLMs. Since\nLLMs cannot directly access visual information, existing\nmethods typically capitalize on their instruction-following\ncapabilities by providing in-context examples to guide them\nin utilizing various expert modules for visual reasoning.\nMM-React [38] designs a system paradigm that composes\nnumerous vision experts with ChatGPT for multimodal rea-\nsoning and action. Visprog [12] provides in-context exam-\nples to enable LLMs to generate Python-like visual pro-\ngrams and create distinct modules for performing specific\ntasks. Cantor [10] crafts more sophisticated prompts, en-\nabling MLLMs to emulate the roles of domain-specific ex-\nperts, and thereby facilitate enhancing rationality and depth\nin the reasoning process. Different from the above method,\n[28] verifies the effectiveness of applying CoT on Sci-\nenceQA using LLMs (UnifiedQA, GPT-3).\nMultimodal-\nCoT [42] incorporates language and vision modalities into\na two-stage framework that separates rationale generation\nand answer inference. In this paper, we propose an Active\nDeduction framework to systematically improve assump-\ntive reasoning in MLLMs, backed by our novel MARS-\nBench evaluation.\n3. MARS-Bench\nWe present the Multimodal Assumptive ReaSoning Bench-\nmark (MARS-Bench), a manually curated benchmark to\nevaluate the assumptive reasoning capabilities of MLLMs.\nIn this section, we first introduce the definition of assump-\ntive questions, followed by an overview of the included as-\nsumption aspects. Then we elaborate on the dataset con-\nstruction process. Finally, we present the evaluation proto-\ncol.\n3.1. Problem Definition\nIn this paper, we refer to “assumptive questions” as the\nquestion that includes an imaginary presupposition to the\nknown facts. Here, “facts” denote the actual information\npresented in the image, while “presuppositions” represent\nhypothetical assumptions about changes in the state of this\ninformation. We formulate the above process as a function\nf: X→Y that maps the input x ∈X to the output y ∈Y :\nf(v, wa, wq) = arg max\ny′\nP(y′|v, wa, wq).\nHere, y′ is the output of MLLM obtained through an appro-\npriate decoder. v, wa, and wq represent the image, imagi-\nnary presupposition, and visual question respectively.\n3.2. Diverse Aspects of Assumptions\nDifferent assumptions demand distinct reasoning capabil-\nities, thus we progressively challenge MLLMs by setting\ntasks across 6 key aspects:\n• Count: Calculate changes in the quantity of objects when\nspecific items are added to or removed from a group. This\nchallenges the model’s ability to perform numerical oper-\nations and track quantity changes.\n• Color: Infer how the color of objects changes when they\nare exchanged or merged.\nThis requires the model to\ngrasp concepts of color mixing or transformations.\n• Shape: Analyze how objects deform or change shape\nwhen external forces are applied or when they interact\nwith other objects. This tests the model’s understanding\nof spatial transformations and physical properties.\n• Size: Imagine changes in the size of objects in space and\ncompare them with other objects.\nThis challenges the\nmodel’s ability to reason about spatial relationships and\nquantitative comparisons of magnitude.\n• Direction: Envision changes in the orientation or position\nof objects. The model needs to understand shifts in spatial\nlocation.\n• Common: Modify world conditions and ask the model to\nreason based on external knowledge. This can encompass\na wide range of questions that require the model to apply\nlogic and knowledge to new scenarios.\nExamples of each question type are illustrated in Fig.2.\nThis hierarchical division aims to assess MLLMs’ ability to\ncomprehend and infer entities, attributes, and relationships\nwithin images at different levels.\n3.3. Dataset Curation\nData Source & Human Annotation. Assumptive ques-\ntions involve imaginary presupposition that entails changes\nin different aspects, which requires images to contain rich\nscenarios and potential semantic information.\nWe thus\nchose images from COCO [25] validation set to annotate\ndue to the diverse object categories and rich visual scenes\n\n[A] How many people are wearing hats?\n[B] If one person took off his hat, how many \npeople would left wearing hats?\nOptions: A. 3   B. 1\n[A] How many cupcakes are there in the image?\n[B] If I ate half of them, how many cupcakes \nwould there be in the image?\nOptions: A. 12   B. 6\nCount\n[A] What color is the bird in the picture?\n[B] If the color of the bird changed to the color of \nthe fruit next to it, what color would the bird be?\nOptions: A. black   B. yellow\n[A] How many colors are there in the sign?\n[B] If \"SW\" were painted red, how many colors \nwould the sign have?\nOptions: A. 3   B. 4\nColor\n[A] Which is larger, the elephant or the tire?\n[B] If the size of the elephant shrank 1000 times, \nwhich would be larger, the elephant or the tire?\nOptions: A. elephant   B. tire\n[A] What is the largest in the picture?\n[B] If the cat were one-tenth its original size, \nwhat would be the largest in the picture?\nOptions: A. cat   B. remote control\nSize\n[A] What shape is the window?\n[B] If I made the window the same shape as the \nlight, what shape would it be?\nOptions: A. rectangle   B. circle\n[A] What is the shape of the sign in the picture?\n[B] If the sign were the same shape as the tiles on \nthe building behind it, what shape would it be?\nOptions: A. round   B. square\nShape\n[A] Which direction might the woman go?\n[B] If the woman turned around, which direction \nmight she go?\nOptions: A. right   B. left\n[A] Where is the truck in relation to the bus?\n[B] If the truck and the bus switched places, \nwhere would the truck be in relation to the bus?\nOptions: A. left   B. right\nDirection\n[A] Is the man on the elevator going up or down?\n[A] If the suitcase on the elevator was red, would \nthe man on the elevator be going up or down?\nOptions: A. down   B. up\n[A] Can cars drive into the driveway?\n[B] If there is no \"Bus Only\" sign, could cars drive \ninto the driveway in the picture?\nOptions: A. No   B. Yes\nCommon\nFigure 2. Examples of different types of questions included in our MARS-Bench. For each image, we provide the corresponding basic\nand assumptive questions. The correct answer is indicated in brackets at the beginning of the question. Note that presuppositions may also\nact as distractors, as shown in the left case in the last row.\nit offers, which capture complex everyday scenarios closely\naligned with real-world distributions.\nWe ask annotators to perform assumptive question an-\nnotations, which follow these steps: (1) Given an image,\nannotators first select an appropriate question type. Images\nwith no suitable question types will be excluded. (2) After-\nward, annotators are asked to design a basic visual question\nand then modify certain conditions within it to create an as-\nsumptive question. (3) To further enhance the data quality\nof MARS-Bench, we employ a rigorous data filter pipeline.\nSpecifically, each question is verified manually with regard\nto the following two key aspects:\n(1) Information Leakage refers to a situation which the\nanswer to a question is embedded in the “If” clause. For\ninstance, “If I painted this bus blue, what color would it\nbe?” In this case, the model can derive answers directly\nfrom the text without referring to the image content.\n(2) Ambiguity of the answer refers to the inability to de-\nrive a precise answer from the image. For example, it\nis not appropriate to ask about the number of elephants\nwhen only a small part of an elephant’s body is visible.\nAutomated Question Expansion.\nWe expanded our\ndataset based on 1,200 manually annotated question pairs.\nLeveraging GPT-4’s powerful multimodal understanding\nand instruction-following capabilities, we prompted it to\ncreatively generate new content by mimicking existing an-\n\nVisual \nencoder\nTokenizer & Word Embedding\nBasic: \nHow many donuts \nare there on the \nplate? \n···\nLarge Language Model\nLoRA\nProjector\n···\n···\n···\n···\nASSISTANT (w/ AD): \n1. How many donuts are currently on the plate? \n    There are two donuts on the plate.\n2. What happens if a donut is eaten?\n    If a donut is eaten, one of the donuts will be removed.\n3. After a donut is eaten, how many donuts would be on the plate?\n    There would be one donut remaining on the plate. \nThe answer is 1.\nASSISTANT (w/o AD): The answer is 2.\nAssumptive:\nIf a donut was \neaten, how many \ndonuts would be \non the plate?\n<ET>\n<ST>\nQuery Tokens\nReasoning Tokens\nAnswer Tokens\nStart/End Thinking Token\n<ET>\nInference Stream w/ AD \nInference Stream w/o AD  \n/\n<ST>\n···\n(\n)\n(\n)\n(\n(\n)\n)\n<ST>\n<ET>\nFigure 3. Overview of the Active Deduction (AD) framework, demonstrating the model’s dynamic adjustment between direct empirical\nreasoning and structured deductive reasoning. For basic questions (left input), the model bypasses AD, directly outputting the answer\nbased on intuition (tokens in the first bracket). For more complex assumptive reasoning questions (right input), the model engages AD by\ngenerating the <ST> token, breaking down the problem into intermediate sub-questions, and synthesizing a final answer upon reaching the\n<ET> token (tokens in the second bracket). This adaptive approach allows the model to balance efficiency with reasoning depth, applying\ndetailed deduction only when necessary.\nnotations. Specifically, we provided GPT with an image,\nalong with manually annotated questions and their corre-\nsponding answers. We then instructed GPT to modify the\npresuppositions while retaining the core question.\nAfter\nthis process, we ultimately obtained 6,000 questions. The\nprompts used for each category varied slightly, with the de-\ntailed prompts provided in the supplementary material. We\nensure the newly generated questions match the image in-\nformation, are logically consistent, and that the answers are\nconcise and clear through manual verification.\n3.4. Evaluation Protocol\nGiven the challenge of quantitatively evaluating open-ended\nanswers, we structure the answers in the form of common\nmultiple-choice questions with two candidates. However,\nthe instruction-following capabilities of existing MLLMs\nare limited, even if explicitly asked to answer “A” or “B”,\nthe models may still produce some free-form text. Further-\nmore, we also observe that many MLLMs exhibit a pref-\nerence for certain options, tending to favor those presented\nearlier. To eliminate the influence of instructions and option\npreferences, we adopt the same answer ranking strategy as\nSEED-Bench [22]. Specifically, given a question, we com-\npute the generation loss for each option and select the mini-\nmum loss as the model’s prediction. We utilize accuracy as\nour evaluation metric, where accb and acca indicate correct\nresponse to basic and assumptive questions respectively.\n4. Active Deduction\nIn this section, we first introduce the overall framework\nLLaVA-AD, a LLaVA-based MLLM equipped with our\nproposed Active Deduction (AD) method. Afterward, we\nintroduce the data strategy for training the model.\n4.1. LLaVA-AD Framework\nThe overall framework of LLaVA-AD is shown in Fig.3.\nWe introduce two new special tokens <ST> and <ET> to\ndenote the start and end of the thinking process. The run-\nning pipeline of the LLaVA-AD model includes the follow-\ning steps: (1) Given a question, LLaVA-AD first implicitly\njudges its complexity. (2) For simple questions, LLaVA-\nAD directly generates responses in a manner similar to con-\nventional MLLMs, as illustrated by the green pathway in\nFig.3. (3) For complex questions, LLaVA-AD first gen-\nerates <ST> token to initiate the cross-referential reason-\ning process step by step. Once the <ET> is generated, the\nmodel proceeds to formulate the final judgment. This pro-\ncess is illustrated by the pink pathway in Fig.3. Through\nthese active deduction behaviors, our model can allocate\nmore computational resources to challenging tasks.\n\n4.2. Data Strategy\nTo endow the model with the aforementioned active de-\nduction capability, we especially annotate answers with de-\ntailed compositional reasoning steps for complex questions.\nWe utilized GPT-4V [1] for annotating and manually ver-\nify the correctness of these thinking steps and filter unreli-\nable ones, resulting in 704 QA pairs with explicit reason-\ning steps. We further blend these data with samples from\nLLaVA-Instruct-150K [26] at a 1:1 ratio to obtain the syn-\nthesized training dataset with 1,408 samples in total.\nStructured reasoning steps annotation. For assumptive\nreasoning questions, we utilized GPT-4V to create struc-\ntured, stepwise annotations that illustrate the decomposition\nprocess required for accurate responses. We provided GPT-\n4V with each assumptive question and the corresponding\nanswer, then prompted it to identify distinct steps necessary\nfor reaching the final answer. This annotation process fol-\nlowed a consistent template, which instructed GPT-4V to:\n• Identify the primary sub-components of the question that\nshould be answered sequentially.\n• Provide specific sub-questions and corresponding an-\nswers for each reasoning step.\nThese structured data, after being wrapped by <ST> and\n<ET> tokens, provide the model with reliable deductive\nreasoning samples.\n5. Experiments and Results\nIn this section, we first evaluate the performance of cur-\nrent state-of-the-art MLLMs on MARS-Bench. We imple-\nmented 0-shot, 1-shot ICL and 1-shot CoT [35] as our base-\nline. Next, we evaluate our proposed “Active Deduction”.\nThe experiments show that, with minimal impact on general\ndatasets, the reasoning token facilitates the model’s reason-\ning process, significantly improving its performance on as-\nsumptive reasoning.\n5.1. Experiment setup\nTraining Details. We utilize pre-trained CLIP ViT-L/14-\n336 [32] and LLaVA-v1.5-7B [26] as our vision encoder\nand multimodal large language model, respectively. The\ntokenizer is extended for two special reasoning tokens,\nwhile the embed tokens and lm head components are tuned\nto adapt to the new extended tokenizer.\nOur model is\ntrained on 8 NVIDIA RTX-4090 with LoRA [14] and uti-\nlizes bf16 precision for mixed-precision training. Optimizer\nAdamW [17] was used with learning rate 5e-5, a cosine de-\ncay schedule, and a warm-up ratio of 0.03. Training was\nconducted for 2 epochs with a batch size of 16.\nDataset. Our training data includes 704 assumptive rea-\nsoning problems with finely annotated structured reasoning\nsteps, where reasoning steps are encapsulated using <ST>\nand <ET> tokens. 176 manually labeled standard VQA\npairs and 528 multi-turn dialogue samples from LLaVA-\n1.5-Instruct are used as negative examples for conducting\nextra reasoning steps, which enables the model to learn rea-\nsoning tokens while retaining capabilities for both standard\nVQA tasks and multi-turn dialogue.\nBaseline.\nAll baseline models are implemented accord-\ning to their original setup and evaluated using the strategy\ndescribed in Sec.3.4. The CoT prompts are based on the\nproven approach outlined in ScienceQA [28], leveraging 1-\nshot manually annotated thinking processes as in-context\nlearning examples.\n5.2. Baseline performance on MARS-Bench\nOur MARS-Bench has posed formidable challenges to ex-\nisting MLLMs. Tab.1 showcases the performance of various\nMLLMs on assumptive reasoning tasks from the MARS-\nBench. It can be seen that all MLLMs perform well on ba-\nsic questions, achieving an accuracy of around 80%. For\n“color” questions, Qwen-VL-Chat even reaches an accu-\nracy of 93.2%. By comparison, a noticeable decline can\nbe observed when evaluating assumptive questions, high-\nlighting the challenge of handling assumptions in reason-\ning. All open-source models perform close to the level of\nrandom guessing (50%). Among the evaluated models, the\ncommercial model GPT-4o leads in overall accuracy for as-\nsumptive reasoning scores 486.0, significantly outperform-\ning other MLLMs, indicating its robustness against empiri-\ncal reasoning traps.\nNotably, 1-shot ICL and 1-shot CoT do not signifi-\ncantly enhance the assumptive reasoning ability of 7B-\nlevel MLLMs and may even lead to a performance drop\nin some cases. In Wei et al.’s study [36], models at a cer-\ntain scale, specifically those exceeding 100B parameters,\ndemonstrated advanced performance when combined with\nCoT strategies. However, 7B-level models often lack suf-\nficient foundational knowledge and reasoning capabilities,\nmaking it challenging for CoT to generate correct reason-\ning steps and potentially leading to issues like AI hallucina-\ntions.\n5.3. Performance of Active Deduction\nResults on Assumptive Reasoning. Our Active Deduction\n(AD) method provides a significant boost in performance\nas shown in Tab.1, with the Active Deduction equipped,\nLLaVA-AD achieving an assumptive reasoning accuracy of\n445.4, up from 369.1 in the baseline LLaVA-1.5 [26]. This\nimprovement of 76.3 points illustrates Active Deduction’s\nefficacy in guiding the model through structured reason-\ning steps, especially under scenarios that challenge conven-\ntional empirical intuition.\nPerformance gains with Active Deduction vary across\ncategories.\nTasks involving count and size see the most\nsubstantial improvement, with an accuracy improvement of\n\nModel\nPrompt\nCount\nColor\nSize\nShape\nDirection\nCommon\nTotal\naccb\nacca\naccb\nacca\naccb\nacca\naccb\nacca\naccb\nacca\naccb\nacca\naccb\nacca\nQwen-VL-Chat [5]\n0-shot\n85.9\n50.8\n89.0\n75.3\n71.5\n52.1\n85.2\n54.6\n70.3\n54.5\n79.9\n58.1\n481.8\n345.4\n1-shot\n88.4\n63.6\n93.2\n76.3\n74.6\n62.6\n88.9\n59.6\n74.2\n58.8\n83.5\n68.7\n502.8\n389.6\nCoT\n88.1\n62.0\n93.2\n76.2\n77.6\n60.3\n90.1\n60.2\n73.4\n57.2\n85.6\n67.5\n508.0\n383.4\nMonkey [24]\n0-shot\n88.4\n61.3\n92.2\n74.4\n69.1\n58.5\n85.2\n54.6\n75.8\n51.2\n84.9\n64.8\n495.6\n364.8\n1-shot\n88.1\n58.7\n92.2\n72.3\n77.6\n58.5\n86.4\n54.0\n76.6\n48.1\n82.7\n61.9\n503.6\n353.5\nCoT\n87.1\n60.9\n92.2\n73.2\n77.6\n58.9\n86.4\n58.3\n76.6\n49.2\n82.7\n65.3\n502.6\n365.8\nBLIP2 [23]\n0-shot\n72.9\n53.3\n81.7\n69.6\n67.9\n65.3\n70.4\n50.0\n53.9\n52.3\n81.3\n60.1\n428.1\n350.6\n1-shot\n72.7\n43.6\n81.5\n53.0\n69.2\n49.2\n72.3\n37.4\n51.0\n41.8\n82.8\n38.7\n429.5\n263.7\nCoT\n73.3\n50.7\n80.5\n52.4\n68.6\n49.2\n69.5\n36.4\n54.8\n41.4\n81.3\n38.0\n428.0\n268.1\nInstructBLIP [8]\n0-shot\n84.3\n67.4\n89.5\n72.6\n61.8\n55.9\n75.3\n55.9\n58.6\n53.7\n80.6\n61.9\n450.1\n367.4\n1-shot\n84.1\n49.5\n88.7\n52.8\n61.5\n50.1\n75.9\n39.5\n59.4\n43.2\n78.3\n41.2\n447.9\n276.3\nCoT\n85.4\n48.4\n89.5\n55.1\n61.7\n47.6\n76.0\n40.7\n58.3\n48.0\n80.3\n41.4\n451.3\n281.3\nxGen-MM [37]\n0-shot\n86.2\n73.3\n81.7\n70.4\n63.0\n56.4\n67.9\n71.6\n64.1\n52.2\n69.8\n61.7\n432.7\n385.6\n1-shot\n85.5\n77.9\n83.3\n68.6\n66.1\n57.0\n67.9\n71.3\n69.5\n51.4\n69.8\n62.1\n442.1\n388.3\nCoT\n85.5\n78.9\n83.3\n67.7\n66.1\n56.4\n69.1\n72.5\n71.1\n50.6\n68.4\n61.7\n443.5\n387.8\nInfMLLM [43]\n0-shot\n90.6\n64.1\n93.2\n72.8\n67.3\n55.3\n76.5\n69.1\n75.0\n56.6\n84.2\n67.5\n486.7\n385.4\n1-shot\n90.8\n61.2\n93.8\n72.0\n66.8\n54.4\n75.6\n69.4\n74.7\n57.0\n83.8\n66.4\n485.4\n380.5\nCoT\n91.1\n59.8\n92.3\n73.3\n68.9\n54.2\n75.9\n67.6\n75.5\n57.0\n85.4\n66.2\n489.0\n378.2\nLLaVA-1.5 [26]\n0-shot\n86.2\n50.6\n90.6\n71.9\n60.6\n53.2\n71.6\n72.5\n68.0\n57.0\n76.3\n63.9\n453.2\n369.1\n1-shot\n79.2\n52.6\n86.9\n70.8\n58.2\n53.0\n51.9\n77.2\n68.8\n55.9\n74.8\n63.5\n419.8\n372.9\nCoT\n79.2\n54.1\n90.0\n73.2\n55.8\n55.0\n58.0\n77.8\n67.2\n57.4\n76.3\n63.3\n426.5\n380.8\nGPT-4o [1]\nN/A\n90.9\n91.7\n94.2\n87.3\n88.9\n87.4\n88.2\n80.3\n85.2\n66.0\n87.6\n73.4\n535.0\n486.0\nLLaVA-AD (Ours)\nN/A\n81.8\n80.6\n89.5\n78.1\n77.0\n73.0\n77.8\n82.1\n71.1\n59.8\n82.7\n71.8\n479.9\n445.4 (+76.3)\nTable 1. Performance of prevalent MLLMs on six tasks within our proposed MARS-Bench. Here, accb represents the accuracy for\ncorrectly answering basic questions, acca denotes the accuracy for correctly answering assumptive questions. We compared 7 open source\nmodels and one commercial model, and we bolded the best performance of the open source models on assumptive question.\nMethod\nVQAv2\nVisWiz\nGQA\nMME\nMM-Vet\nMMBench\nSEED\nMARS-Assum.\nBLIP-2 [23]\n65.0\n19.6\n41.0\n1293.8\n22.4\n-\n49.7\n350.6\nInstructBLIP [8]\n-\n34.5\n49.2\n-\n26.2\n36.0\n58.8\n367.4\nMiniGPT-4 [44]\n-\n-\n-\n581.7\n22.1\n24.3\n47.4\n341.2\nQwen-VL-Chat [5]\n78.2\n38.9\n57.5\n1487.5\n-\n60.6\n65.4\n345.4\nLLaVA-AD (Ours)\n77.3\n50.6\n60.3\n1471.5\n34.3\n63.3\n64.1\n445.4\nTable 2. Results on prevalent VQA benchmarks. We employ MMBench-en-dev, SEED-Bench-img, MME-test for evaluation. MARS-\nAssum. donates the total score of assumptive questions on our MARS-Bench, and we use “-” to denote the performance which are not\nprovided in their paper.\n30 and 19.8, respectively. For instance, count tasks require\nthe model to handle hypothetical additions or removals of\nitems accurately, a challenge well-suited to AD’s stepwise\ndeduction approach. The shape tasks, involving transforma-\ntions or deformations, similarly benefit from the method’s\nstructured breakdown, which helps the model manage com-\nplex spatial changes logically. While, the effectiveness of\nAD relies on the foundational capabilities of the underlying\nmodel, meaning that improvement is ultimately constrained\nby the model’s base competence in each category. There\nare limited gains for direction tasks, since current models’\ndirectional perception is bounded, which restricts its abil-\nity to produce sufficient and accurate reasoning steps. The\ncommon sense tasks, which involve general knowledge ap-\nplications under presupposition scenarios, see a satisfactory\nimprovement, suggesting that our Active Deduction method\nis effective at guiding reasoning in knowledge-driven con-\ntexts when the task complexity aligns with the model’s in-\nherent capabilities.\nPerformance on standard VQA benchmarks.\nTo ex-\namine general visual understanding and instruction follow-\ning of our model, we measured its accuracy on standard\n\nVQA tasks without assumptive reasoning components. We\nfollowed common practices by conducting assessments on\n7 widely used benchmarks: VQAv2 [11], VisWiz [13],\nGQA [15], MME [9], MM-Vet [39], MMBench [27], and\nSEED-Bench [22].\nAs shown in Tab.2, our LLaVA-AD\nachieves comparable results to state-of-the-art MLLMs in\nVQA tasks, while also enhancing its capabilities in system-\natic reasoning, particularly in assumptive reasoning.\n5.4. Ablation Study\nWe conducted an ablation study to isolate the effects of\nthe <ST> and <ET> tokens and structured reasoning steps,\nboth integral to the AD framework. The LLaVA-AD w/o\ntoken model is trained on the same finely annotated struc-\ntured reasoning steps without <ST> and <ET> tokens, with\nall training configs are same.\nMethod\nGQA\nMM-Vet\nMARS-B\nMARS-A\nQwen-VL-Chat [5]\n57.5\n-\n481.8\n345.4\nLLaVA-1.5 [26]\n62.0\n31.1\n453.2\n369.1\nLLaVA-AD w/o token\n59.4\n27.5\n431.2\n447.1\nLLaVA-AD w/ token\n60.2\n34.3\n479.9\n445.4\nTable 3. Comparison of Models across Various Benchmarks.\nLLaVA-AD w/ token is trained on the same dataset without rea-\nsoning token wrapped. MARS-B. and MARS-A. represents the\ntotal score of basic questions and assumptive questions on MARS-\nBench, respectively.\nImpact on reasoning performance. We conducted an ab-\nlation study to evaluate the impact of introducing reason-\ning tokens on model’s performance across MARS-Bench\nand two additional general-purpose datasets. From Tab.3,\nit is evident that incorporating the reasoning tokens im-\nproves the model’s performance on GQA [15] tasks, of-\nfering a modest advantage over the model without the to-\nken. In tasks assessed by MM-Vet [39], a large language\nmodel-based evaluator for open-ended outputs, model with\nreasoning tokens showed a more substantial improvement,\nsuggesting that the reasoning tokens effectively enhance\nthe model’s capability for systematic, open-ended reason-\ning. LLaVA-AD w/o token demonstrates an advantage in\nassumptive reasoning tasks, due to its tendency to adopt a\ndirect output pattern associated with deductive reasoning,\nwhich excels in scenarios requiring assumptions. While,\nthe use of reasoning tokens enables model to balance per-\nformance, reducing unnecessary reasoning steps for basic\nquestions and significantly boosting results across other\nVQA benchmarks with minimal trade-offs in assumptive\nreasoning tasks.\nImpact on problem difficulty determination. Accurate\ndifficulty prediction is essential to optimize reasoning. Mis-\njudging complexity leads to unnecessary processing, in-\ncreasing costs and potentially reducing performance, as\nMethod\nDe.%Basic ↓\nEm.%Assum. ↓\nAvg. E.R. ↓\nLLaVA-AD w/o token\n17.9%\n5.8%\n11.9%\nLLaVA-AD w/ token\n4.3%\n6.1%\n5.2%\nTable 4. Impact of reasoning token on problem difficulty de-\ntermination. Here, De.%Basic, Em.%Assum. and Avg. E.R.\nmeans the ratio of deductive reasoning in basic situation, empir-\nical reasoning in assumptive question, and the average error rate,\nrespectively.\nseen in Tab.3. Our evaluation on MARS-Bench examines\nwhether reasoning tokens are used appropriately, avoiding\nextra steps for basic questions and activating deduction for\nassumptive ones. Tab.4 shows that LLaVA-AD w/o tokens\noften over-processes simple questions, leading to its per-\nformance drop in Tab.3. Introducing reasoning tokens im-\nproves problem difficulty determination, which is crucial\nfor accurately solving problems of varying complexity.\n1. What symbols are present on the sign?\n    The sign displays two yellow icons on a dark background. \n2. What do the icons mean?\n    The left is a stick figure without a skirt, representing the male \nrestroom, and the one on the right is a stick figure with a skirt, \nrepresenting the female restroom.\n3. Is the left side appropriate for a male user?\n    Yes, it is the correct restroom for you as a man.\nI am a man. Should I go to the toilet on the left?\n1. What is the object on the left? \n    The object on the left is a green umbrella. \n2. What is the purpose of an umbrella? \n    Umbrellas are typically used to shield people from rain.\n3. Is the object functioning properly to serve its purpose? \n    The umbrella appears to be upright and intact, suggesting it can \nbe used effectively to keep someone dry in the rain.\nIt's rainy outside. Is the thing on the left useful?\nFigure 4. The generalizability of the LLaVA-AD beyond the\ndomain. The model has zero-shot capability and can activate de-\nduction reasoning on other reasoning problems.\nGeneralizability beyond the domain. LLaVA-AD demon-\nstrates a certain degree of generalization ability across other\n\nreasoning tasks. As shown in Fig.4, it can tackle general\nreasoning by decomposing the queries into a sequence of\nsub-questions, which allows it to systematically extract in-\nformation from both images and provided conditions. This\napproach enables the model to mimic human-like logical\nprogression even in unfamiliar contexts.\n6. Conclusion\nIn conclusion, we presented MARS-Bench, a benchmark\ntargeting Assumptive Reasoning in Multimodal Large Lan-\nguage Models (MLLMs), and introduced the Active De-\nduction (AD) method to enhance these models’ reason-\ning capabilities. Our findings show that current MLLMs\nstruggle with systematic reasoning problems like assump-\ntive reasoning. Besides, Active Deduction substantially im-\nproves MLLMs’ performance on assumptive tasks by guid-\ning structured, stepwise deductive reasoning without sac-\nrificing performance on simpler queries.\nThis work un-\nderscores the limitations of empirical reasoning in current\nMLLMs and suggests a potential approach for fostering\nmore human-like reasoning in systematic, presuppositions\ncomplex scenarios.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 1, 2, 6, 7\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:\na visual language model for few-shot learning.\nAdvances\nin neural information processing systems, 35:23716–23736,\n2022. 1, 2\n[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2\n[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n2\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,\n2, 7, 8\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm’s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 2\n[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al.\nScaling\ninstruction-finetuned language models.\narXiv preprint\narXiv:2210.11416, 2022. 2\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n1, 2, 7\n[9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre-\nhensive evaluation benchmark for multimodal large language\nmodels. arXiv preprint arXiv:2306.13394, 2023. 3, 8\n[10] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu,\nYunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu\nZheng, Xing Sun, Liujuan Cao, and Rongrong Ji. Cantor:\nInspiring multimodal chain-of-thought of mllm. In Proceed-\nings of the 32nd ACM International Conference on Multime-\ndia, page 9096–9105, New York, NY, USA, 2024. Associa-\ntion for Computing Machinery. 3\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904–6913, 2017. 8\n[12] Tanmay Gupta and Aniruddha Kembhavi. Visual program-\nming: Compositional visual reasoning without training. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 14953–14962, 2023. 3\n[13] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608–3617,\n2018. 8\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 6\n[15] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700–6709, 2019. 3, 8\n[16] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin\nMa, and Yu-Gang Jiang.\nLumen:\nUnleashing versatile\nvision-centric capabilities of large multimodal models. arXiv\npreprint arXiv:2403.07304, 2024. 1\n[17] Diederik P Kingma. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980, 2014. 6\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 4015–4026, 2023. 1\n\n[19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al.\nVisual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123:32–73, 2017. 3\n[20] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 9579–9589, 2024. 1\n[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n[22] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\nWang, Ruimao Zhang, and Ying Shan. Seed-bench: Bench-\nmarking multimodal large language models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 13299–13308, 2024. 3, 5, 8\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 1, 2, 7\n[24] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Mon-\nkey: Image resolution and text label are important things for\nlarge multi-modal models. arXiv preprint arXiv:2311.06607,\n2023. 1, 2, 7\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014. 3\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 2, 6, 7, 8\n[27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n8\n[28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering.\nAdvances\nin Neural Information Processing Systems, 35:2507–2521,\n2022. 1, 3, 6\n[29] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts. arXiv\npreprint arXiv:2310.02255, 2023. 3\n[30] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge.\nIn Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019. 3\n[31] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 1, 2\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 6\n[33] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuo-\nfan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual\ncot: Unleashing chain-of-thought reasoning in multi-modal\nlanguage models. arXiv preprint arXiv:2403.16999, 2024. 1\n[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 1, 6\n[36] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson,\nYifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny\nZhou, et al. Larger language models do in-context learning\ndifferently. arXiv preprint arXiv:2303.03846, 2023. 6\n[37] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan,\nSenthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yu-\ntong Dai, Michael S Ryoo, et al.\nxgen-mm (blip-3): A\nfamily of open large multimodal models.\narXiv preprint\narXiv:2408.08872, 2024. 7\n[38] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang. Mm-react: Prompting chat-\ngpt for multimodal reasoning and action, 2023. 3\n[39] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 8\n[40] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter:\nEfficient fine-tuning of language models\nwith zero-init attention. arXiv preprint arXiv:2303.16199,\n2023. 2\n[41] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 2\n[42] Zhuosheng Zhang,\nAston Zhang,\nMu Li,\nHai Zhao,\nGeorge Karypis, and Alex Smola.\nMultimodal chain-of-\nthought reasoning in language models.\narXiv preprint\narXiv:2302.00923, 2023. 3\n\n[43] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li,\nand Yuan Qi.\nInfmllm: A unified framework for visual-\nlanguage tasks. arXiv preprint arXiv:2311.06791, 2023. 7\n[44] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2, 7",
    "pdf_filename": "Look_Before_You_Decide_Prompting_Active_Deduction_of_MLLMs_for_Assumptive_Reasoning.pdf"
}