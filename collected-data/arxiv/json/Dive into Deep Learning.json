{
    "title": "Dive into Deep Learning",
    "context": "",
    "body": "Dive into Deep Learning\nASTON ZHANG, ZACHARY C. LIPTON, MU LI, AND ALEXANDER J.\nSMOLA\n\n\nContents\nPreface\npage xxv\nInstallation\nxxxiv\nNotation\nxxxvii\n1 Introduction\n1\n1.1\nA Motivating Example\n2\n1.2\nKey Components\n4\n1.3\nKinds of Machine Learning Problems\n7\n1.4\nRoots\n20\n1.5\nThe Road to Deep Learning\n22\n1.6\nSuccess Stories\n25\n1.7\nThe Essence of Deep Learning\n27\n1.8\nSummary\n29\n1.9\nExercises\n29\n2 Preliminaries\n30\n2.1\nData Manipulation\n30\n2.1.1\nGetting Started\n30\n2.1.2\nIndexing and Slicing\n33\n2.1.3\nOperations\n34\n2.1.4\nBroadcasting\n35\n2.1.5\nSaving Memory\n36\n2.1.6\nConversion to Other Python Objects\n37\n2.1.7\nSummary\n37\n2.1.8\nExercises\n38\n2.2\nData Preprocessing\n38\n2.2.1\nReading the Dataset\n38\n2.2.2\nData Preparation\n39\n2.2.3\nConversion to the Tensor Format\n40\n2.2.4\nDiscussion\n40\n2.2.5\nExercises\n41\n2.3\nLinear Algebra\n41\n2.3.1\nScalars\n41\niii\n\n2.3.2\nVectors\n42\n2.3.3\nMatrices\n43\n2.3.4\nTensors\n44\n2.3.5\nBasic Properties of Tensor Arithmetic\n45\n2.3.6\nReduction\n46\n2.3.7\nNon-Reduction Sum\n47\n2.3.8\nDot Products\n48\n2.3.9\nMatrix–Vector Products\n49\n2.3.10\nMatrix–Matrix Multiplication\n50\n2.3.11\nNorms\n50\n2.3.12\nDiscussion\n52\n2.3.13\nExercises\n53\n2.4\nCalculus\n54\n2.4.1\nDerivatives and Diﬀerentiation\n55\n2.4.2\nVisualization Utilities\n56\n2.4.3\nPartial Derivatives and Gradients\n58\n2.4.4\nChain Rule\n59\n2.4.5\nDiscussion\n59\n2.4.6\nExercises\n59\n2.5\nAutomatic Diﬀerentiation\n60\n2.5.1\nA Simple Function\n61\n2.5.2\nBackward for Non-Scalar Variables\n62\n2.5.3\nDetaching Computation\n63\n2.5.4\nGradients and Python Control Flow\n63\n2.5.5\nDiscussion\n64\n2.5.6\nExercises\n65\n2.6\nProbability and Statistics\n65\n2.6.1\nA Simple Example: Tossing Coins\n66\n2.6.2\nA More Formal Treatment\n69\n2.6.3\nRandom Variables\n70\n2.6.4\nMultiple Random Variables\n71\n2.6.5\nAn Example\n73\n2.6.6\nExpectations\n75\n2.6.7\nDiscussion\n76\n2.6.8\nExercises\n78\n2.7\nDocumentation\n79\n2.7.1\nFunctions and Classes in a Module\n79\n2.7.2\nSpeciﬁc Functions and Classes\n80\n3 Linear Neural Networks for Regression\n82\n3.1\nLinear Regression\n82\n3.1.1\nBasics\n83\n3.1.2\nVectorization for Speed\n88\n3.1.3\nThe Normal Distribution and Squared Loss\n88\n3.1.4\nLinear Regression as a Neural Network\n90\niv\n\n3.1.5\nSummary\n92\n3.1.6\nExercises\n92\n3.2\nObject-Oriented Design for Implementation\n93\n3.2.1\nUtilities\n94\n3.2.2\nModels\n96\n3.2.3\nData\n97\n3.2.4\nTraining\n98\n3.2.5\nSummary\n99\n3.2.6\nExercises\n99\n3.3\nSynthetic Regression Data\n99\n3.3.1\nGenerating the Dataset\n100\n3.3.2\nReading the Dataset\n100\n3.3.3\nConcise Implementation of the Data Loader\n101\n3.3.4\nSummary\n102\n3.3.5\nExercises\n103\n3.4\nLinear Regression Implementation from Scratch\n103\n3.4.1\nDeﬁning the Model\n104\n3.4.2\nDeﬁning the Loss Function\n104\n3.4.3\nDeﬁning the Optimization Algorithm\n104\n3.4.4\nTraining\n105\n3.4.5\nSummary\n107\n3.4.6\nExercises\n108\n3.5\nConcise Implementation of Linear Regression\n109\n3.5.1\nDeﬁning the Model\n109\n3.5.2\nDeﬁning the Loss Function\n110\n3.5.3\nDeﬁning the Optimization Algorithm\n110\n3.5.4\nTraining\n111\n3.5.5\nSummary\n112\n3.5.6\nExercises\n112\n3.6\nGeneralization\n113\n3.6.1\nTraining Error and Generalization Error\n114\n3.6.2\nUnderﬁtting or Overﬁtting?\n116\n3.6.3\nModel Selection\n117\n3.6.4\nSummary\n118\n3.6.5\nExercises\n119\n3.7\nWeight Decay\n119\n3.7.1\nNorms and Weight Decay\n120\n3.7.2\nHigh-Dimensional Linear Regression\n121\n3.7.3\nImplementation from Scratch\n122\n3.7.4\nConcise Implementation\n124\n3.7.5\nSummary\n125\n3.7.6\nExercises\n125\n4 Linear Neural Networks for Classiﬁcation\n127\n4.1\nSoftmax Regression\n127\nv\n\n4.1.1\nClassiﬁcation\n128\n4.1.2\nLoss Function\n131\n4.1.3\nInformation Theory Basics\n133\n4.1.4\nSummary and Discussion\n134\n4.1.5\nExercises\n134\n4.2\nThe Image Classiﬁcation Dataset\n136\n4.2.1\nLoading the Dataset\n137\n4.2.2\nReading a Minibatch\n138\n4.2.3\nVisualization\n138\n4.2.4\nSummary\n139\n4.2.5\nExercises\n139\n4.3\nThe Base Classiﬁcation Model\n140\n4.3.1\nThe Classifier Class\n140\n4.3.2\nAccuracy\n141\n4.3.3\nSummary\n141\n4.3.4\nExercises\n141\n4.4\nSoftmax Regression Implementation from Scratch\n142\n4.4.1\nThe Softmax\n142\n4.4.2\nThe Model\n143\n4.4.3\nThe Cross-Entropy Loss\n144\n4.4.4\nTraining\n145\n4.4.5\nPrediction\n145\n4.4.6\nSummary\n146\n4.4.7\nExercises\n146\n4.5\nConcise Implementation of Softmax Regression\n147\n4.5.1\nDeﬁning the Model\n147\n4.5.2\nSoftmax Revisited\n147\n4.5.3\nTraining\n148\n4.5.4\nSummary\n149\n4.5.5\nExercises\n149\n4.6\nGeneralization in Classiﬁcation\n150\n4.6.1\nThe Test Set\n151\n4.6.2\nTest Set Reuse\n152\n4.6.3\nStatistical Learning Theory\n154\n4.6.4\nSummary\n155\n4.6.5\nExercises\n156\n4.7\nEnvironment and Distribution Shift\n157\n4.7.1\nTypes of Distribution Shift\n157\n4.7.2\nExamples of Distribution Shift\n160\n4.7.3\nCorrection of Distribution Shift\n162\n4.7.4\nA Taxonomy of Learning Problems\n165\n4.7.5\nFairness, Accountability, and Transparency in Machine\nLearning\n167\n4.7.6\nSummary\n168\n4.7.7\nExercises\n168\nvi\n\n5 Multilayer Perceptrons\n170\n5.1\nMultilayer Perceptrons\n170\n5.1.1\nHidden Layers\n171\n5.1.2\nActivation Functions\n174\n5.1.3\nSummary and Discussion\n178\n5.1.4\nExercises\n179\n5.2\nImplementation of Multilayer Perceptrons\n179\n5.2.1\nImplementation from Scratch\n179\n5.2.2\nConcise Implementation\n181\n5.2.3\nSummary\n182\n5.2.4\nExercises\n183\n5.3\nForward Propagation, Backward Propagation, and Computational Graphs\n183\n5.3.1\nForward Propagation\n184\n5.3.2\nComputational Graph of Forward Propagation\n185\n5.3.3\nBackpropagation\n185\n5.3.4\nTraining Neural Networks\n186\n5.3.5\nSummary\n187\n5.3.6\nExercises\n187\n5.4\nNumerical Stability and Initialization\n188\n5.4.1\nVanishing and Exploding Gradients\n188\n5.4.2\nParameter Initialization\n191\n5.4.3\nSummary\n193\n5.4.4\nExercises\n193\n5.5\nGeneralization in Deep Learning\n193\n5.5.1\nRevisiting Overﬁtting and Regularization\n194\n5.5.2\nInspiration from Nonparametrics\n195\n5.5.3\nEarly Stopping\n196\n5.5.4\nClassical Regularization Methods for Deep Networks\n197\n5.5.5\nSummary\n198\n5.5.6\nExercises\n198\n5.6\nDropout\n198\n5.6.1\nDropout in Practice\n199\n5.6.2\nImplementation from Scratch\n200\n5.6.3\nConcise Implementation\n202\n5.6.4\nSummary\n202\n5.6.5\nExercises\n203\n5.7\nPredicting House Prices on Kaggle\n204\n5.7.1\nDownloading Data\n204\n5.7.2\nKaggle\n204\n5.7.3\nAccessing and Reading the Dataset\n205\n5.7.4\nData Preprocessing\n206\n5.7.5\nError Measure\n208\n5.7.6\nK-Fold Cross-Validation\n209\n5.7.7\nModel Selection\n209\n5.7.8\nSubmitting Predictions on Kaggle\n210\nvii\n\n5.7.9\nSummary and Discussion\n211\n5.7.10\nExercises\n211\n6 Builders’ Guide\n212\n6.1\nLayers and Modules\n212\n6.1.1\nA Custom Module\n214\n6.1.2\nThe Sequential Module\n216\n6.1.3\nExecuting Code in the Forward Propagation Method\n216\n6.1.4\nSummary\n218\n6.1.5\nExercises\n218\n6.2\nParameter Management\n219\n6.2.1\nParameter Access\n219\n6.2.2\nTied Parameters\n221\n6.2.3\nSummary\n221\n6.2.4\nExercises\n222\n6.3\nParameter Initialization\n222\n6.3.1\nBuilt-in Initialization\n222\n6.3.2\nSummary\n224\n6.3.3\nExercises\n224\n6.4\nLazy Initialization\n225\n6.4.1\nSummary\n226\n6.4.2\nExercises\n226\n6.5\nCustom Layers\n227\n6.5.1\nLayers without Parameters\n227\n6.5.2\nLayers with Parameters\n228\n6.5.3\nSummary\n229\n6.5.4\nExercises\n229\n6.6\nFile I/O\n229\n6.6.1\nLoading and Saving Tensors\n230\n6.6.2\nLoading and Saving Model Parameters\n230\n6.6.3\nSummary\n232\n6.6.4\nExercises\n232\n6.7\nGPUs\n232\n6.7.1\nComputing Devices\n233\n6.7.2\nTensors and GPUs\n234\n6.7.3\nNeural Networks and GPUs\n236\n6.7.4\nSummary\n237\n6.7.5\nExercises\n238\n7 Convolutional Neural Networks\n239\n7.1\nFrom Fully Connected Layers to Convolutions\n240\n7.1.1\nInvariance\n240\n7.1.2\nConstraining the MLP\n242\n7.1.3\nConvolutions\n243\n7.1.4\nChannels\n244\nviii\n\n7.1.5\nSummary and Discussion\n245\n7.1.6\nExercises\n246\n7.2\nConvolutions for Images\n246\n7.2.1\nThe Cross-Correlation Operation\n246\n7.2.2\nConvolutional Layers\n248\n7.2.3\nObject Edge Detection in Images\n249\n7.2.4\nLearning a Kernel\n250\n7.2.5\nCross-Correlation and Convolution\n251\n7.2.6\nFeature Map and Receptive Field\n251\n7.2.7\nSummary\n253\n7.2.8\nExercises\n253\n7.3\nPadding and Stride\n254\n7.3.1\nPadding\n254\n7.3.2\nStride\n256\n7.3.3\nSummary and Discussion\n257\n7.3.4\nExercises\n258\n7.4\nMultiple Input and Multiple Output Channels\n258\n7.4.1\nMultiple Input Channels\n259\n7.4.2\nMultiple Output Channels\n260\n7.4.3\n1 × 1 Convolutional Layer\n261\n7.4.4\nDiscussion\n262\n7.4.5\nExercises\n263\n7.5\nPooling\n264\n7.5.1\nMaximum Pooling and Average Pooling\n264\n7.5.2\nPadding and Stride\n266\n7.5.3\nMultiple Channels\n267\n7.5.4\nSummary\n268\n7.5.5\nExercises\n268\n7.6\nConvolutional Neural Networks (LeNet)\n269\n7.6.1\nLeNet\n269\n7.6.2\nTraining\n272\n7.6.3\nSummary\n273\n7.6.4\nExercises\n273\n8 Modern Convolutional Neural Networks\n275\n8.1\nDeep Convolutional Neural Networks (AlexNet)\n276\n8.1.1\nRepresentation Learning\n277\n8.1.2\nAlexNet\n281\n8.1.3\nTraining\n283\n8.1.4\nDiscussion\n284\n8.1.5\nExercises\n285\n8.2\nNetworks Using Blocks (VGG)\n286\n8.2.1\nVGG Blocks\n286\n8.2.2\nVGG Network\n287\n8.2.3\nTraining\n289\nix\n\n8.2.4\nSummary\n289\n8.2.5\nExercises\n290\n8.3\nNetwork in Network (NiN)\n290\n8.3.1\nNiN Blocks\n291\n8.3.2\nNiN Model\n292\n8.3.3\nTraining\n293\n8.3.4\nSummary\n293\n8.3.5\nExercises\n294\n8.4\nMulti-Branch Networks (GoogLeNet)\n295\n8.4.1\nInception Blocks\n295\n8.4.2\nGoogLeNet Model\n296\n8.4.3\nTraining\n299\n8.4.4\nDiscussion\n299\n8.4.5\nExercises\n300\n8.5\nBatch Normalization\n300\n8.5.1\nTraining Deep Networks\n301\n8.5.2\nBatch Normalization Layers\n303\n8.5.3\nImplementation from Scratch\n305\n8.5.4\nLeNet with Batch Normalization\n306\n8.5.5\nConcise Implementation\n308\n8.5.6\nDiscussion\n308\n8.5.7\nExercises\n310\n8.6\nResidual Networks (ResNet) and ResNeXt\n311\n8.6.1\nFunction Classes\n311\n8.6.2\nResidual Blocks\n312\n8.6.3\nResNet Model\n315\n8.6.4\nTraining\n317\n8.6.5\nResNeXt\n317\n8.6.6\nSummary and Discussion\n319\n8.6.7\nExercises\n320\n8.7\nDensely Connected Networks (DenseNet)\n321\n8.7.1\nFrom ResNet to DenseNet\n321\n8.7.2\nDense Blocks\n322\n8.7.3\nTransition Layers\n323\n8.7.4\nDenseNet Model\n324\n8.7.5\nTraining\n325\n8.7.6\nSummary and Discussion\n325\n8.7.7\nExercises\n325\n8.8\nDesigning Convolution Network Architectures\n326\n8.8.1\nThe AnyNet Design Space\n327\n8.8.2\nDistributions and Parameters of Design Spaces\n329\n8.8.3\nRegNet\n331\n8.8.4\nTraining\n332\n8.8.5\nDiscussion\n333\n8.8.6\nExercises\n333\nx\n\n9 Recurrent Neural Networks\n334\n9.1\nWorking with Sequences\n336\n9.1.1\nAutoregressive Models\n337\n9.1.2\nSequence Models\n339\n9.1.3\nTraining\n341\n9.1.4\nPrediction\n342\n9.1.5\nSummary\n344\n9.1.6\nExercises\n345\n9.2\nConverting Raw Text into Sequence Data\n345\n9.2.1\nReading the Dataset\n346\n9.2.2\nTokenization\n346\n9.2.3\nVocabulary\n347\n9.2.4\nPutting It All Together\n348\n9.2.5\nExploratory Language Statistics\n348\n9.2.6\nSummary\n351\n9.2.7\nExercises\n351\n9.3\nLanguage Models\n352\n9.3.1\nLearning Language Models\n353\n9.3.2\nPerplexity\n354\n9.3.3\nPartitioning Sequences\n356\n9.3.4\nSummary and Discussion\n357\n9.3.5\nExercises\n357\n9.4\nRecurrent Neural Networks\n358\n9.4.1\nNeural Networks without Hidden States\n358\n9.4.2\nRecurrent Neural Networks with Hidden States\n359\n9.4.3\nRNN-Based Character-Level Language Models\n361\n9.4.4\nSummary\n362\n9.4.5\nExercises\n362\n9.5\nRecurrent Neural Network Implementation from Scratch\n362\n9.5.1\nRNN Model\n363\n9.5.2\nRNN-Based Language Model\n364\n9.5.3\nGradient Clipping\n366\n9.5.4\nTraining\n367\n9.5.5\nDecoding\n368\n9.5.6\nSummary\n369\n9.5.7\nExercises\n369\n9.6\nConcise Implementation of Recurrent Neural Networks\n370\n9.6.1\nDeﬁning the Model\n370\n9.6.2\nTraining and Predicting\n371\n9.6.3\nSummary\n372\n9.6.4\nExercises\n372\n9.7\nBackpropagation Through Time\n372\n9.7.1\nAnalysis of Gradients in RNNs\n373\n9.7.2\nBackpropagation Through Time in Detail\n376\n9.7.3\nSummary\n378\nxi\n\n9.7.4\nExercises\n378\n10 Modern Recurrent Neural Networks\n379\n10.1 Long Short-Term Memory (LSTM)\n380\n10.1.1\nGated Memory Cell\n380\n10.1.2\nImplementation from Scratch\n383\n10.1.3\nConcise Implementation\n385\n10.1.4\nSummary\n386\n10.1.5\nExercises\n387\n10.2 Gated Recurrent Units (GRU)\n387\n10.2.1\nReset Gate and Update Gate\n387\n10.2.2\nCandidate Hidden State\n388\n10.2.3\nHidden State\n389\n10.2.4\nImplementation from Scratch\n390\n10.2.5\nConcise Implementation\n391\n10.2.6\nSummary\n392\n10.2.7\nExercises\n392\n10.3 Deep Recurrent Neural Networks\n393\n10.3.1\nImplementation from Scratch\n394\n10.3.2\nConcise Implementation\n395\n10.3.3\nSummary\n396\n10.3.4\nExercises\n396\n10.4 Bidirectional Recurrent Neural Networks\n397\n10.4.1\nImplementation from Scratch\n398\n10.4.2\nConcise Implementation\n399\n10.4.3\nSummary\n399\n10.4.4\nExercises\n399\n10.5 Machine Translation and the Dataset\n400\n10.5.1\nDownloading and Preprocessing the Dataset\n400\n10.5.2\nTokenization\n401\n10.5.3\nLoading Sequences of Fixed Length\n403\n10.5.4\nReading the Dataset\n404\n10.5.5\nSummary\n405\n10.5.6\nExercises\n405\n10.6 The Encoder−Decoder Architecture\n406\n10.6.1\nEncoder\n406\n10.6.2\nDecoder\n406\n10.6.3\nPutting the Encoder and Decoder Together\n407\n10.6.4\nSummary\n407\n10.6.5\nExercises\n408\n10.7 Sequence-to-Sequence Learning for Machine Translation\n408\n10.7.1\nTeacher Forcing\n409\n10.7.2\nEncoder\n409\n10.7.3\nDecoder\n411\n10.7.4\nEncoder–Decoder for Sequence-to-Sequence Learning\n413\nxii\n\n10.7.5\nLoss Function with Masking\n413\n10.7.6\nTraining\n413\n10.7.7\nPrediction\n414\n10.7.8\nEvaluation of Predicted Sequences\n415\n10.7.9\nSummary\n416\n10.7.10\nExercises\n417\n10.8 Beam Search\n417\n10.8.1\nGreedy Search\n418\n10.8.2\nExhaustive Search\n419\n10.8.3\nBeam Search\n419\n10.8.4\nSummary\n420\n10.8.5\nExercises\n421\n11 Attention Mechanisms and Transformers\n422\n11.1 Queries, Keys, and Values\n424\n11.1.1\nVisualization\n426\n11.1.2\nSummary\n427\n11.1.3\nExercises\n427\n11.2 Attention Pooling by Similarity\n428\n11.2.1\nKernels and Data\n428\n11.2.2\nAttention Pooling via Nadaraya–Watson Regression\n430\n11.2.3\nAdapting Attention Pooling\n431\n11.2.4\nSummary\n432\n11.2.5\nExercises\n433\n11.3 Attention Scoring Functions\n433\n11.3.1\nDot Product Attention\n434\n11.3.2\nConvenience Functions\n435\n11.3.3\nScaled Dot Product Attention\n437\n11.3.4\nAdditive Attention\n438\n11.3.5\nSummary\n439\n11.3.6\nExercises\n440\n11.4 The Bahdanau Attention Mechanism\n440\n11.4.1\nModel\n441\n11.4.2\nDeﬁning the Decoder with Attention\n441\n11.4.3\nTraining\n444\n11.4.4\nSummary\n445\n11.4.5\nExercises\n445\n11.5 Multi-Head Attention\n446\n11.5.1\nModel\n446\n11.5.2\nImplementation\n447\n11.5.3\nSummary\n448\n11.5.4\nExercises\n449\n11.6 Self-Attention and Positional Encoding\n449\n11.6.1\nSelf-Attention\n449\n11.6.2\nComparing CNNs, RNNs, and Self-Attention\n450\nxiii\n\n11.6.3\nPositional Encoding\n451\n11.6.4\nSummary\n454\n11.6.5\nExercises\n454\n11.7 The Transformer Architecture\n454\n11.7.1\nModel\n455\n11.7.2\nPositionwise Feed-Forward Networks\n456\n11.7.3\nResidual Connection and Layer Normalization\n457\n11.7.4\nEncoder\n458\n11.7.5\nDecoder\n459\n11.7.6\nTraining\n461\n11.7.7\nSummary\n465\n11.7.8\nExercises\n465\n11.8 Transformers for Vision\n466\n11.8.1\nModel\n466\n11.8.2\nPatch Embedding\n467\n11.8.3\nVision Transformer Encoder\n468\n11.8.4\nPutting It All Together\n469\n11.8.5\nTraining\n470\n11.8.6\nSummary and Discussion\n470\n11.8.7\nExercises\n471\n11.9 Large-Scale Pretraining with Transformers\n471\n11.9.1\nEncoder-Only\n472\n11.9.2\nEncoder–Decoder\n474\n11.9.3\nDecoder-Only\n476\n11.9.4\nScalability\n479\n11.9.5\nLarge Language Models\n481\n11.9.6\nSummary and Discussion\n482\n11.9.7\nExercises\n483\n12 Optimization Algorithms\n484\n12.1 Optimization and Deep Learning\n484\n12.1.1\nGoal of Optimization\n485\n12.1.2\nOptimization Challenges in Deep Learning\n485\n12.1.3\nSummary\n489\n12.1.4\nExercises\n489\n12.2 Convexity\n490\n12.2.1\nDeﬁnitions\n490\n12.2.2\nProperties\n493\n12.2.3\nConstraints\n496\n12.2.4\nSummary\n498\n12.2.5\nExercises\n498\n12.3 Gradient Descent\n499\n12.3.1\nOne-Dimensional Gradient Descent\n499\n12.3.2\nMultivariate Gradient Descent\n502\n12.3.3\nAdaptive Methods\n505\nxiv\n\n12.3.4\nSummary\n509\n12.3.5\nExercises\n509\n12.4 Stochastic Gradient Descent\n510\n12.4.1\nStochastic Gradient Updates\n510\n12.4.2\nDynamic Learning Rate\n512\n12.4.3\nConvergence Analysis for Convex Objectives\n514\n12.4.4\nStochastic Gradients and Finite Samples\n516\n12.4.5\nSummary\n516\n12.4.6\nExercises\n517\n12.5 Minibatch Stochastic Gradient Descent\n517\n12.5.1\nVectorization and Caches\n517\n12.5.2\nMinibatches\n520\n12.5.3\nReading the Dataset\n521\n12.5.4\nImplementation from Scratch\n522\n12.5.5\nConcise Implementation\n525\n12.5.6\nSummary\n526\n12.5.7\nExercises\n527\n12.6 Momentum\n528\n12.6.1\nBasics\n528\n12.6.2\nPractical Experiments\n532\n12.6.3\nTheoretical Analysis\n535\n12.6.4\nSummary\n537\n12.6.5\nExercises\n538\n12.7 Adagrad\n538\n12.7.1\nSparse Features and Learning Rates\n538\n12.7.2\nPreconditioning\n539\n12.7.3\nThe Algorithm\n540\n12.7.4\nImplementation from Scratch\n542\n12.7.5\nConcise Implementation\n543\n12.7.6\nSummary\n543\n12.7.7\nExercises\n544\n12.8 RMSProp\n544\n12.8.1\nThe Algorithm\n545\n12.8.2\nImplementation from Scratch\n545\n12.8.3\nConcise Implementation\n547\n12.8.4\nSummary\n548\n12.8.5\nExercises\n548\n12.9 Adadelta\n549\n12.9.1\nThe Algorithm\n549\n12.9.2\nImplementation\n549\n12.9.3\nSummary\n551\n12.9.4\nExercises\n551\n12.10 Adam\n552\n12.10.1\nThe Algorithm\n552\n12.10.2\nImplementation\n553\nxv\n\n12.10.3\nYogi\n555\n12.10.4\nSummary\n556\n12.10.5\nExercises\n556\n12.11 Learning Rate Scheduling\n556\n12.11.1\nToy Problem\n557\n12.11.2\nSchedulers\n559\n12.11.3\nPolicies\n560\n12.11.4\nSummary\n565\n12.11.5\nExercises\n566\n13 Computational Performance\n567\n13.1 Compilers and Interpreters\n567\n13.1.1\nSymbolic Programming\n568\n13.1.2\nHybrid Programming\n569\n13.1.3\nHybridizing the Sequential Class\n570\n13.1.4\nSummary\n572\n13.1.5\nExercises\n572\n13.2 Asynchronous Computation\n572\n13.2.1\nAsynchrony via Backend\n573\n13.2.2\nBarriers and Blockers\n575\n13.2.3\nImproving Computation\n575\n13.2.4\nSummary\n575\n13.2.5\nExercises\n575\n13.3 Automatic Parallelism\n576\n13.3.1\nParallel Computation on GPUs\n576\n13.3.2\nParallel Computation and Communication\n577\n13.3.3\nSummary\n578\n13.3.4\nExercises\n579\n13.4 Hardware\n580\n13.4.1\nComputers\n580\n13.4.2\nMemory\n581\n13.4.3\nStorage\n582\n13.4.4\nCPUs\n584\n13.4.5\nGPUs and other Accelerators\n587\n13.4.6\nNetworks and Buses\n590\n13.4.7\nMore Latency Numbers\n591\n13.4.8\nSummary\n592\n13.4.9\nExercises\n592\n13.5 Training on Multiple GPUs\n593\n13.5.1\nSplitting the Problem\n594\n13.5.2\nData Parallelism\n596\n13.5.3\nA Toy Network\n597\n13.5.4\nData Synchronization\n597\n13.5.5\nDistributing Data\n598\n13.5.6\nTraining\n599\nxvi\n\n13.5.7\nSummary\n601\n13.5.8\nExercises\n601\n13.6 Concise Implementation for Multiple GPUs\n602\n13.6.1\nA Toy Network\n602\n13.6.2\nNetwork Initialization\n603\n13.6.3\nTraining\n603\n13.6.4\nSummary\n605\n13.6.5\nExercises\n605\n13.7 Parameter Servers\n605\n13.7.1\nData-Parallel Training\n606\n13.7.2\nRing Synchronization\n608\n13.7.3\nMulti-Machine Training\n610\n13.7.4\nKey–Value Stores\n611\n13.7.5\nSummary\n613\n13.7.6\nExercises\n613\n14 Computer Vision\n614\n14.1 Image Augmentation\n614\n14.1.1\nCommon Image Augmentation Methods\n615\n14.1.2\nTraining with Image Augmentation\n618\n14.1.3\nSummary\n621\n14.1.4\nExercises\n622\n14.2 Fine-Tuning\n622\n14.2.1\nSteps\n623\n14.2.2\nHot Dog Recognition\n623\n14.2.3\nSummary\n628\n14.2.4\nExercises\n628\n14.3 Object Detection and Bounding Boxes\n629\n14.3.1\nBounding Boxes\n630\n14.3.2\nSummary\n632\n14.3.3\nExercises\n632\n14.4 Anchor Boxes\n632\n14.4.1\nGenerating Multiple Anchor Boxes\n632\n14.4.2\nIntersection over Union (IoU)\n635\n14.4.3\nLabeling Anchor Boxes in Training Data\n636\n14.4.4\nPredicting Bounding Boxes with Non-Maximum Suppression\n642\n14.4.5\nSummary\n645\n14.4.6\nExercises\n646\n14.5 Multiscale Object Detection\n646\n14.5.1\nMultiscale Anchor Boxes\n646\n14.5.2\nMultiscale Detection\n648\n14.5.3\nSummary\n649\n14.5.4\nExercises\n650\n14.6 The Object Detection Dataset\n650\n14.6.1\nDownloading the Dataset\n650\nxvii\n\n14.6.2\nReading the Dataset\n651\n14.6.3\nDemonstration\n653\n14.6.4\nSummary\n653\n14.6.5\nExercises\n653\n14.7 Single Shot Multibox Detection\n654\n14.7.1\nModel\n654\n14.7.2\nTraining\n660\n14.7.3\nPrediction\n662\n14.7.4\nSummary\n663\n14.7.5\nExercises\n663\n14.8 Region-based CNNs (R-CNNs)\n666\n14.8.1\nR-CNNs\n666\n14.8.2\nFast R-CNN\n667\n14.8.3\nFaster R-CNN\n669\n14.8.4\nMask R-CNN\n670\n14.8.5\nSummary\n671\n14.8.6\nExercises\n672\n14.9 Semantic Segmentation and the Dataset\n672\n14.9.1\nImage Segmentation and Instance Segmentation\n672\n14.9.2\nThe Pascal VOC2012 Semantic Segmentation Dataset\n673\n14.9.3\nSummary\n678\n14.9.4\nExercises\n678\n14.10 Transposed Convolution\n679\n14.10.1\nBasic Operation\n679\n14.10.2\nPadding, Strides, and Multiple Channels\n681\n14.10.3\nConnection to Matrix Transposition\n682\n14.10.4\nSummary\n684\n14.10.5\nExercises\n684\n14.11 Fully Convolutional Networks\n684\n14.11.1\nThe Model\n685\n14.11.2\nInitializing Transposed Convolutional Layers\n687\n14.11.3\nReading the Dataset\n688\n14.11.4\nTraining\n689\n14.11.5\nPrediction\n689\n14.11.6\nSummary\n690\n14.11.7\nExercises\n691\n14.12 Neural Style Transfer\n691\n14.12.1\nMethod\n692\n14.12.2\nReading the Content and Style Images\n693\n14.12.3\nPreprocessing and Postprocessing\n694\n14.12.4\nExtracting Features\n694\n14.12.5\nDeﬁning the Loss Function\n696\n14.12.6\nInitializing the Synthesized Image\n698\n14.12.7\nTraining\n698\n14.12.8\nSummary\n699\nxviii\n\n14.12.9\nExercises\n700\n14.13 Image Classiﬁcation (CIFAR-10) on Kaggle\n700\n14.13.1\nObtaining and Organizing the Dataset\n701\n14.13.2\nImage Augmentation\n704\n14.13.3\nReading the Dataset\n704\n14.13.4\nDeﬁning the Model\n705\n14.13.5\nDeﬁning the Training Function\n705\n14.13.6\nTraining and Validating the Model\n706\n14.13.7\nClassifying the Testing Set and Submitting Results on Kaggle\n707\n14.13.8\nSummary\n707\n14.13.9\nExercises\n708\n14.14 Dog Breed Identiﬁcation (ImageNet Dogs) on Kaggle\n708\n14.14.1\nObtaining and Organizing the Dataset\n708\n14.14.2\nImage Augmentation\n710\n14.14.3\nReading the Dataset\n711\n14.14.4\nFine-Tuning a Pretrained Model\n711\n14.14.5\nDeﬁning the Training Function\n712\n14.14.6\nTraining and Validating the Model\n713\n14.14.7\nClassifying the Testing Set and Submitting Results on Kaggle\n714\n14.14.8\nSummary\n715\n14.14.9\nExercises\n715\n15 Natural Language Processing: Pretraining\n716\n15.1 Word Embedding (word2vec)\n717\n15.1.1\nOne-Hot Vectors Are a Bad Choice\n717\n15.1.2\nSelf-Supervised word2vec\n718\n15.1.3\nThe Skip-Gram Model\n718\n15.1.4\nThe Continuous Bag of Words (CBOW) Model\n720\n15.1.5\nSummary\n721\n15.1.6\nExercises\n722\n15.2 Approximate Training\n722\n15.2.1\nNegative Sampling\n722\n15.2.2\nHierarchical Softmax\n724\n15.2.3\nSummary\n725\n15.2.4\nExercises\n725\n15.3 The Dataset for Pretraining Word Embeddings\n725\n15.3.1\nReading the Dataset\n726\n15.3.2\nSubsampling\n726\n15.3.3\nExtracting Center Words and Context Words\n728\n15.3.4\nNegative Sampling\n729\n15.3.5\nLoading Training Examples in Minibatches\n730\n15.3.6\nPutting It All Together\n731\n15.3.7\nSummary\n732\n15.3.8\nExercises\n733\n15.4 Pretraining word2vec\n733\nxix\n\n15.4.1\nThe Skip-Gram Model\n733\n15.4.2\nTraining\n735\n15.4.3\nApplying Word Embeddings\n737\n15.4.4\nSummary\n738\n15.4.5\nExercises\n738\n15.5 Word Embedding with Global Vectors (GloVe)\n738\n15.5.1\nSkip-Gram with Global Corpus Statistics\n738\n15.5.2\nThe GloVe Model\n739\n15.5.3\nInterpreting GloVe from the Ratio of Co-occurrence Proba-\nbilities\n740\n15.5.4\nSummary\n741\n15.5.5\nExercises\n741\n15.6 Subword Embedding\n742\n15.6.1\nThe fastText Model\n742\n15.6.2\nByte Pair Encoding\n743\n15.6.3\nSummary\n746\n15.6.4\nExercises\n746\n15.7 Word Similarity and Analogy\n746\n15.7.1\nLoading Pretrained Word Vectors\n747\n15.7.2\nApplying Pretrained Word Vectors\n748\n15.7.3\nSummary\n750\n15.7.4\nExercises\n751\n15.8 Bidirectional Encoder Representations from Transformers (BERT)\n751\n15.8.1\nFrom Context-Independent to Context-Sensitive\n751\n15.8.2\nFrom Task-Speciﬁc to Task-Agnostic\n752\n15.8.3\nBERT: Combining the Best of Both Worlds\n752\n15.8.4\nInput Representation\n753\n15.8.5\nPretraining Tasks\n755\n15.8.6\nPutting It All Together\n758\n15.8.7\nSummary\n759\n15.8.8\nExercises\n760\n15.9 The Dataset for Pretraining BERT\n760\n15.9.1\nDeﬁning Helper Functions for Pretraining Tasks\n761\n15.9.2\nTransforming Text into the Pretraining Dataset\n763\n15.9.3\nSummary\n766\n15.9.4\nExercises\n766\n15.10 Pretraining BERT\n766\n15.10.1\nPretraining BERT\n767\n15.10.2\nRepresenting Text with BERT\n768\n15.10.3\nSummary\n770\n15.10.4\nExercises\n770\n16 Natural Language Processing: Applications\n771\n16.1 Sentiment Analysis and the Dataset\n772\n16.1.1\nReading the Dataset\n772\nxx\n\n16.1.2\nPreprocessing the Dataset\n773\n16.1.3\nCreating Data Iterators\n774\n16.1.4\nPutting It All Together\n774\n16.1.5\nSummary\n775\n16.1.6\nExercises\n775\n16.2 Sentiment Analysis: Using Recurrent Neural Networks\n775\n16.2.1\nRepresenting Single Text with RNNs\n776\n16.2.2\nLoading Pretrained Word Vectors\n777\n16.2.3\nTraining and Evaluating the Model\n778\n16.2.4\nSummary\n779\n16.2.5\nExercises\n779\n16.3 Sentiment Analysis: Using Convolutional Neural Networks\n780\n16.3.1\nOne-Dimensional Convolutions\n780\n16.3.2\nMax-Over-Time Pooling\n782\n16.3.3\nThe textCNN Model\n783\n16.3.4\nSummary\n786\n16.3.5\nExercises\n786\n16.4 Natural Language Inference and the Dataset\n787\n16.4.1\nNatural Language Inference\n787\n16.4.2\nThe Stanford Natural Language Inference (SNLI) Dataset\n788\n16.4.3\nSummary\n791\n16.4.4\nExercises\n791\n16.5 Natural Language Inference: Using Attention\n792\n16.5.1\nThe Model\n792\n16.5.2\nTraining and Evaluating the Model\n796\n16.5.3\nSummary\n798\n16.5.4\nExercises\n799\n16.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n799\n16.6.1\nSingle Text Classiﬁcation\n800\n16.6.2\nText Pair Classiﬁcation or Regression\n800\n16.6.3\nText Tagging\n801\n16.6.4\nQuestion Answering\n801\n16.6.5\nSummary\n803\n16.6.6\nExercises\n803\n16.7 Natural Language Inference: Fine-Tuning BERT\n803\n16.7.1\nLoading Pretrained BERT\n804\n16.7.2\nThe Dataset for Fine-Tuning BERT\n805\n16.7.3\nFine-Tuning BERT\n807\n16.7.4\nSummary\n808\n16.7.5\nExercises\n808\n17 Reinforcement Learning\n809\n17.1 Markov Decision Process (MDP)\n810\n17.1.1\nDeﬁnition of an MDP\n810\n17.1.2\nReturn and Discount Factor\n812\nxxi\n\n17.1.3\nDiscussion of the Markov Assumption\n812\n17.1.4\nSummary\n813\n17.1.5\nExercises\n813\n17.2 Value Iteration\n813\n17.2.1\nStochastic Policy\n813\n17.2.2\nValue Function\n814\n17.2.3\nAction-Value Function\n815\n17.2.4\nOptimal Stochastic Policy\n815\n17.2.5\nPrinciple of Dynamic Programming\n816\n17.2.6\nValue Iteration\n816\n17.2.7\nPolicy Evaluation\n816\n17.2.8\nImplementation of Value Iteration\n817\n17.2.9\nSummary\n819\n17.2.10\nExercises\n819\n17.3 Q-Learning\n819\n17.3.1\nThe Q-Learning Algorithm\n820\n17.3.2\nAn Optimization Problem Underlying Q-Learning\n820\n17.3.3\nExploration in Q-Learning\n821\n17.3.4\nThe “Self-correcting” Property of Q-Learning\n822\n17.3.5\nImplementation of Q-Learning\n822\n17.3.6\nSummary\n824\n17.3.7\nExercises\n825\n18 Gaussian Processes\n826\n18.1 Introduction to Gaussian Processes\n827\n18.1.1\nSummary\n838\n18.1.2\nExercises\n838\n18.2 Gaussian Process Priors\n839\n18.2.1\nDeﬁnition\n839\n18.2.2\nA Simple Gaussian Process\n840\n18.2.3\nFrom Weight Space to Function Space\n841\n18.2.4\nThe Radial Basis Function (RBF) Kernel\n842\n18.2.5\nThe Neural Network Kernel\n844\n18.2.6\nSummary\n844\n18.2.7\nExercises\n845\n18.3 Gaussian Process Inference\n846\n18.3.1\nPosterior Inference for Regression\n846\n18.3.2\nEquations for Making Predictions and Learning Kernel\nHyperparameters in GP Regression\n847\n18.3.3\nInterpreting Equations for Learning and Predictions\n848\n18.3.4\nWorked Example from Scratch\n849\n18.3.5\nMaking Life Easy with GPyTorch\n854\n18.3.6\nSummary\n856\n18.3.7\nExercises\n857\nxxii\n\n19 Hyperparameter Optimization\n859\n19.1 What Is Hyperparameter Optimization?\n859\n19.1.1\nThe Optimization Problem\n861\n19.1.2\nRandom Search\n863\n19.1.3\nSummary\n865\n19.1.4\nExercises\n866\n19.2 Hyperparameter Optimization API\n867\n19.2.1\nSearcher\n867\n19.2.2\nScheduler\n868\n19.2.3\nTuner\n869\n19.2.4\nBookkeeping the Performance of HPO Algorithms\n869\n19.2.5\nExample: Optimizing the Hyperparameters of a Convolu-\ntional Neural Network\n870\n19.2.6\nComparing HPO Algorithms\n872\n19.2.7\nSummary\n873\n19.2.8\nExercises\n873\n19.3 Asynchronous Random Search\n874\n19.3.1\nObjective Function\n876\n19.3.2\nAsynchronous Scheduler\n876\n19.3.3\nVisualize the Asynchronous Optimization Process\n883\n19.3.4\nSummary\n883\n19.3.5\nExercises\n884\n19.4 Multi-Fidelity Hyperparameter Optimization\n884\n19.4.1\nSuccessive Halving\n886\n19.4.2\nSummary\n893\n19.5 Asynchronous Successive Halving\n895\n19.5.1\nObjective Function\n901\n19.5.2\nAsynchronous Scheduler\n901\n19.5.3\nVisualize the Optimization Process\n911\n19.5.4\nSummary\n911\n20 Generative Adversarial Networks\n912\n20.1 Generative Adversarial Networks\n912\n20.1.1\nGenerate Some “Real” Data\n914\n20.1.2\nGenerator\n915\n20.1.3\nDiscriminator\n915\n20.1.4\nTraining\n915\n20.1.5\nSummary\n917\n20.1.6\nExercises\n918\n20.2 Deep Convolutional Generative Adversarial Networks\n918\n20.2.1\nThe Pokemon Dataset\n918\n20.2.2\nThe Generator\n919\n20.2.3\nDiscriminator\n921\n20.2.4\nTraining\n923\n20.2.5\nSummary\n924\nxxiii\n\nxxiv\nContents\n20.2.6\nExercises\n924\nAppendix A\nMathematics for Deep Learning\n926\nAppendix B\nTools for Deep Learning\n1068\nReferences\n1124\n\nPreface\nJust a few years ago, there were no legions of deep learning scientists developing intelligent\nproducts and services at major companies and startups. When we entered the ﬁeld, machine\nlearning did not command headlines in daily newspapers. Our parents had no idea what ma-\nchine learning was, let alone why we might prefer it to a career in medicine or law. Machine\nlearning was a blue skies academic discipline whose industrial signiﬁcance was limited to\na narrow set of real-world applications, including speech recognition and computer vision.\nMoreover, many of these applications required so much domain knowledge that they were\noften regarded as entirely separate areas for which machine learning was one small compo-\nnent. At that time, neural networks—the predecessors of the deep learning methods that we\nfocus on in this book—were generally regarded as outmoded.\nYet in just few years, deep learning has taken the world by surprise, driving rapid progress in\nsuch diverse ﬁelds as computer vision, natural language processing, automatic speech recog-\nnition, reinforcement learning, and biomedical informatics. Moreover, the success of deep\nlearning in so many tasks of practical interest has even catalyzed developments in theoreti-\ncal machine learning and statistics. With these advances in hand, we can now build cars that\ndrive themselves with more autonomy than ever before (though less autonomy than some\ncompanies might have you believe), dialogue systems that debug code by asking clarifying\nquestions, and software agents beating the best human players in the world at board games\nsuch as Go, a feat once thought to be decades away. Already, these tools exert ever-wider in-\nﬂuence on industry and society, changing the way movies are made, diseases are diagnosed,\nand playing a growing role in basic sciences—from astrophysics, to climate modeling, to\nweather prediction, to biomedicine.\nAbout This Book\nThis book represents our attempt to make deep learning approachable, teaching you the con-\ncepts, the context, and the code.\nOne Medium Combining Code, Math, and HTML\nxxv\n\nxxvi\nPreface\n1\nFor any computing technology to reach its full impact, it must be well understood, well doc-\numented, and supported by mature, well-maintained tools. The key ideas should be clearly\ndistilled, minimizing the onboarding time needed to bring new practitioners up to date. Ma-\nture libraries should automate common tasks, and exemplar code should make it easy for\npractitioners to modify, apply, and extend common applications to suit their needs.\nAs an example, take dynamic web applications. Despite a large number of companies, such\nas Amazon, developing successful database-driven web applications in the 1990s, the po-\ntential of this technology to aid creative entrepreneurs was realized to a far greater degree\nonly in the past ten years, owing in part to the development of powerful, well-documented\nframeworks.\nTesting the potential of deep learning presents unique challenges because any single appli-\ncation brings together various disciplines. Applying deep learning requires simultaneously\nunderstanding (i) the motivations for casting a problem in a particular way; (ii) the math-\nematical form of a given model; (iii) the optimization algorithms for ﬁtting the models to\ndata; (iv) the statistical principles that tell us when we should expect our models to general-\nize to unseen data and practical methods for certifying that they have, in fact, generalized;\nand (v) the engineering techniques required to train models eﬃciently, navigating the pitfalls\nof numerical computing and getting the most out of available hardware. Teaching the crit-\nical thinking skills required to formulate problems, the mathematics to solve them, and the\nsoftware tools to implement those solutions all in one place presents formidable challenges.\nOur goal in this book is to present a uniﬁed resource to bring would-be practitioners up to\nspeed.\nWhen we started this book project, there were no resources that simultaneously (i) remained\nup to date; (ii) covered the breadth of modern machine learning practices with suﬃcient\ntechnical depth; and (iii) interleaved exposition of the quality one expects of a textbook with\nthe clean runnable code that one expects of a hands-on tutorial. We found plenty of code\nexamples illustrating how to use a given deep learning framework (e.g., how to do basic\nnumerical computing with matrices in TensorFlow) or for implementing particular techniques\n(e.g., code snippets for LeNet, AlexNet, ResNet, etc.) scattered across various blog posts and\nGitHub repositories. However, these examples typically focused on how to implement a given\napproach, but left out the discussion of why certain algorithmic decisions are made. While\nsome interactive resources have popped up sporadically to address a particular topic, e.g., the\nengaging blog posts published on the website Distill 1 , or personal blogs, they only covered\nselected topics in deep learning, and often lacked associated code. On the other hand, while\nseveral deep learning textbooks have emerged—e.g., Goodfellow et al. (2016), which oﬀers\na comprehensive survey on the basics of deep learning—these resources do not marry the\ndescriptions to realizations of the concepts in code, sometimes leaving readers clueless as to\nhow to implement them. Moreover, too many resources are hidden behind the paywalls of\ncommercial course providers.\nWe set out to create a resource that could (i) be freely available for everyone; (ii) oﬀer suﬃ-\ncient technical depth to provide a starting point on the path to actually becoming an applied\nmachine learning scientist; (iii) include runnable code, showing readers how to solve prob-\n\nxxvii\nPreface\n2\nlems in practice; (iv) allow for rapid updates, both by us and also by the community at large;\nand (v) be complemented by a forum 2 for interactive discussion of technical details and to\nanswer questions.\nThese goals were often in conﬂict. Equations, theorems, and citations are best managed and\nlaid out in LaTeX. Code is best described in Python. And webpages are native in HTML\nand JavaScript. Furthermore, we want the content to be accessible both as executable code,\nas a physical book, as a downloadable PDF, and on the Internet as a website. No workﬂows\nseemed suited to these demands, so we decided to assemble our own (Section B.6). We settled\non GitHub to share the source and to facilitate community contributions; Jupyter notebooks\nfor mixing code, equations and text; Sphinx as a rendering engine; and Discourse as a dis-\ncussion platform. While our system is not perfect, these choices strike a compromise among\nthe competing concerns. We believe that Dive into Deep Learning might be the ﬁrst book\npublished using such an integrated workﬂow.\nLearning by Doing\nMany textbooks present concepts in succession, covering each in exhaustive detail. For ex-\nample, the excellent textbook of Bishop (2006), teaches each topic so thoroughly that getting\nto the chapter on linear regression requires a nontrivial amount of work. While experts love\nthis book precisely for its thoroughness, for true beginners, this property limits its usefulness\nas an introductory text.\nIn this book, we teach most concepts just in time. In other words, you will learn concepts at\nthe very moment that they are needed to accomplish some practical end. While we take some\ntime at the outset to teach fundamental preliminaries, like linear algebra and probability, we\nwant you to taste the satisfaction of training your ﬁrst model before worrying about more\nesoteric concepts.\nAside from a few preliminary notebooks that provide a crash course in the basic mathematical\nbackground, each subsequent chapter both introduces a reasonable number of new concepts\nand provides several self-contained working examples, using real datasets. This presented\nan organizational challenge. Some models might logically be grouped together in a single\nnotebook. And some ideas might be best taught by executing several models in succession.\nBy contrast, there is a big advantage to adhering to a policy of one working example, one\nnotebook: This makes it as easy as possible for you to start your own research projects by\nleveraging our code. Just copy a notebook and start modifying it.\nThroughout, we interleave the runnable code with background material as needed. In general,\nwe err on the side of making tools available before explaining them fully (often ﬁlling in the\nbackground later). For instance, we might use stochastic gradient descent before explaining\nwhy it is useful or oﬀering some intuition for why it works. This helps to give practitioners\nthe necessary ammunition to solve problems quickly, at the expense of requiring the reader\nto trust us with some curatorial decisions.\nThis book teaches deep learning concepts from scratch. Sometimes, we delve into ﬁne details\nabout models that would typically be hidden from users by modern deep learning frameworks.\n\nxxviii\nPreface\nThis comes up especially in the basic tutorials, where we want you to understand everything\nthat happens in a given layer or optimizer. In these cases, we often present two versions of\nthe example: one where we implement everything from scratch, relying only on NumPy-like\nfunctionality and automatic diﬀerentiation, and a more practical example, where we write\nsuccinct code using the high-level APIs of deep learning frameworks. After explaining how\nsome component works, we rely on the high-level API in subsequent tutorials.\nContent and Structure\nThe book can be divided into roughly three parts, dealing with preliminaries, deep learning\ntechniques, and advanced topics focused on real systems and applications (Fig. 1).\ntFig. 1\nBook structure.\n• Part 1: Basics and Preliminaries. Chapter 1 is an introduction to deep learning. Then,\nin Chapter 2, we quickly bring you up to speed on the prerequisites required for hands-\non deep learning, such as how to store and manipulate data, and how to apply various\nnumerical operations based on elementary concepts from linear algebra, calculus, and\nprobability. Chapter 3 and Chapter 5 cover the most fundamental concepts and tech-\nniques in deep learning, including regression and classiﬁcation; linear models; multilayer\nperceptrons; and overﬁtting and regularization.\n• Part 2: Modern Deep Learning Techniques. Chapter 6 describes the key computational\ncomponents of deep learning systems and lays the groundwork for our subsequent im-\nplementations of more complex models. Next, Chapter 7 and Chapter 8 present convo-\nlutional neural networks (CNNs), powerful tools that form the backbone of most modern\n\nxxix\nPreface\n3\ncomputer vision systems. Similarly, Chapter 9 and Chapter 10 introduce recurrent neu-\nral networks (RNNs), models that exploit sequential (e.g., temporal) structure in data\nand are commonly used for natural language processing and time series prediction. In\nChapter 11, we describe a relatively new class of models, based on so-called attention\nmechanisms, that has displaced RNNs as the dominant architecture for most natural lan-\nguage processing tasks. These sections will bring you up to speed on the most powerful\nand general tools that are widely used by deep learning practitioners.\n• Part 3: Scalability, Eﬃciency, and Applications (available online3). In Chapter 12, we\ndiscuss several common optimization algorithms used to train deep learning models.\nNext, in Chapter 13, we examine several key factors that inﬂuence the computational\nperformance of deep learning code. Then, in Chapter 14, we illustrate major applica-\ntions of deep learning in computer vision. Finally, in Chapter 15 and Chapter 16, we\ndemonstrate how to pretrain language representation models and apply them to natural\nlanguage processing tasks.\nCode\nMost sections of this book feature executable code. We believe that some intuitions are best\ndeveloped via trial and error, tweaking the code in small ways and observing the results.\nIdeally, an elegant mathematical theory might tell us precisely how to tweak our code to\nachieve a desired result. However, deep learning practitioners today must often tread where no\nsolid theory provides guidance. Despite our best attempts, formal explanations for the eﬃcacy\nof various techniques are still lacking, for a variety of reasons: the mathematics to characterize\nthese models can be so diﬃcult; the explanation likely depends on properties of the data that\ncurrently lack clear deﬁnitions; and serious inquiry on these topics has only recently kicked\ninto high gear. We are hopeful that as the theory of deep learning progresses, each future\nedition of this book will provide insights that eclipse those presently available.\nTo avoid unnecessary repetition, we capture some of our most frequently imported and used\nfunctions and classes in the d2l package. Throughout, we mark blocks of code (such as\nfunctions, classes, or collection of import statements) with #@save to indicate that they will\nbe accessed later via the d2l package. We oﬀer a detailed overview of these classes and\nfunctions in Section B.8. The d2l package is lightweight and only requires the following\ndependencies:\n#@save\nimport collections\nimport hashlib\nimport inspect\nimport math\nimport os\nimport random\nimport re\nimport shutil\nimport sys\nimport tarfile\n(continues on next page)\n\nxxx\nPreface\n4\n5\n6\n7\n(continued from previous page)\nimport time\nimport zipfile\nfrom collections import defaultdict\nimport pandas as pd\nimport requests\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nfrom matplotlib_inline import backend_inline\nd2l = sys.modules[__name__]\nMost of the code in this book is based on PyTorch, a popular open-source framework that\nhas been enthusiastically embraced by the deep learning research community. All of the code\nin this book has passed tests under the latest stable version of PyTorch. However, due to the\nrapid development of deep learning, some code in the print edition may not work properly\nin future versions of PyTorch. We plan to keep the online version up to date. In case you\nencounter any problems, please consult Installation (page xxxiv) to update your code and\nruntime environment. Below lists dependencies in our PyTorch implementation.\n#@save\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom scipy.spatial import distance_matrix\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nTarget Audience\nThis book is for students (undergraduate or graduate), engineers, and researchers, who seek\na solid grasp of the practical techniques of deep learning. Because we explain every concept\nfrom scratch, no previous background in deep learning or machine learning is required. Fully\nexplaining the methods of deep learning requires some mathematics and programming, but\nwe will only assume that you enter with some basics, including modest amounts of linear alge-\nbra, calculus, probability, and Python programming. Just in case you have forgotten anything,\nthe online Appendix4 provides a refresher on most of the mathematics you will ﬁnd in this\nbook. Usually, we will prioritize intuition and ideas over mathematical rigor. If you would\nlike to extend these foundations beyond the prerequisites to understand our book, we happily\nrecommend some other terriﬁc resources: Linear Analysis by Bollobás (1999) covers linear\nalgebra and functional analysis in great depth. All of Statistics (Wasserman, 2013) provides a\nmarvelous introduction to statistics. Joe Blitzstein’s books5 and courses6 on probability and\ninference are pedagogical gems. And if you have not used Python before, you may want to\nperuse this Python tutorial7.\n\nxxxi\nPreface\n8\n9\n10\nNotebooks, Website, GitHub, and Forum\nAll of our notebooks are available for download on the D2L.ai website 8 and on GitHub 9 .\nAssociated with this book, we have launched a discussion forum, located at discuss.d2l.ai10.\nWhenever you have questions on any section of the book, you can ﬁnd a link to the associated\ndiscussion page at the end of each notebook.\nAcknowledgments\nWe are indebted to the hundreds of contributors for both the English and the Chinese drafts.\nThey helped improve the content and oﬀered valuable feedback. This book was originally im-\nplemented with MXNet as the primary framework. We thank Anirudh Dagar and Yuan Tang\nfor adapting a majority part of earlier MXNet code into PyTorch and TensorFlow implemen-\ntations, respectively. Since July 2021, we have redesigned and reimplemented this book in\nPyTorch, MXNet, and TensorFlow, choosing PyTorch as the primary framework. We thank\nAnirudh Dagar for adapting a majority part of more recent PyTorch code into JAX imple-\nmentations. We thank Gaosheng Wu, Liujun Hu, Ge Zhang, and Jiehang Xie from Baidu for\nadapting a majority part of more recent PyTorch code into PaddlePaddle implementations\nin the Chinese draft. We thank Shuai Zhang for integrating the LaTeX style from the press\ninto the PDF building.\nOn GitHub, we thank every contributor of this English draft for making it better for ev-\neryone. Their GitHub IDs or names are (in no particular order): alxnorden, avinashingit,\nbowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarro-\nman, gkutiel, John Mitro, Liang Pu, Rahul Agarwal, Mohamed Ali Jamaoui, Michael (Stu)\nStewart, Mike Müller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sun-\ndeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB,\nYuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen,\npmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Ab-\nhishek Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Buddared-\ndygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete Lüer, Surbhi Vijay-\nvargeeya, Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pedro Larroy,\nlgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner, Maximilian\nBöther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong, Steve Sedlmeyer,\nRuslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas, ati-ozgur, qbaza, dchoi77, Adam\nGerson, Phuc Le, Mark Atwood, christabella, vn09, Haibin Lin, jjangga0214, RichyChen,\nnoelo, hansent, Giel Dops, dvincent1337, WhiteD3vil, Peter Kulits, codypenta, joseppinilla,\nahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep Vu, sﬁlip, mlxd, Kale-ab Tessera,\nSanjar Adilov, MatteoFerrara, hsneto, Katarzyna Biesialska, Gregory Bruss, Duy–Thanh\nDoan, paulaurel, graytowne, Duc Pham, sl7423, Jaedong Hwang, Yida Wang, cys4, clhm,\nJean Kaddour, austinmw, trebeljahr, tbaums, Cuong V. Nguyen, pavelkomarov, vzlamal, No-\n\nxxxii\nPreface\ntAnotherSystem, J-Arun-Mani, jancio, eldarkurtic, the-great-shazbot, doctorcolossus, gducharme,\ncclauss, Daniel-Mietchen, hoonose, biagiom, abhinavsp0730, jonathanhrandall, ysraell, Nodar\nOkroshiashvili, UgurKap, Jiyang Kang, StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya,\nNishantTharani, heiligerl, SportsTHU, Hoa Nguyen, manuel-arno-korfmann-webentwicklung,\naterzis-personal, nxby, Xiaoting He, Josiah Yoder, mathresearch, mzz2017, jroberayalas,\niluu, ghejc, BSharmi, vkramdev, simonwardjones, LakshKD, TalNeoran, djliden, Nikhil95,\nOren Barkan, guoweis, haozhu233, pratikhack, Yue Ying, tayfununal, steinsag, charleybeller,\nAndrew Lumsdaine, Jiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates, Adriaan\nTijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua\nBroyde, jake221, jonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hong-\nshen Yang, Vinney Cavallo, yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald,\nShivam Upadhyay, Quanshangze Du, Biswajit Sahoo, Parthe Pandit, Ishan Kumar, Homuncu-\nlusK, Lane Schwartz, varadgunjal, Jason Wiener, Armin Gholampoor, Shreshtha13, eigen-\narnav, Hyeonggyu Kim, EmilyOng, Bálint Mucsányi, Chase DuBois, Juntian Tao, Wenxi-\nang Xu, Lifu Huang, ﬁlevich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov,\nFrancesco “Fuma” Fumagalli, Peilin Sun, Vincent Gurgul, qingfengtommy, Janmey Shukla,\nMo Shan, Kaan Sancak, regob, AlexSauer, Gopalakrishna Ramachandra, Tobias Uelwer,\nChao Wang, Tian Cao, Nicolas Corthorn, akash5474, kxxt, zxydi1992, Jacob Britton, Shuangchi\nHe, zhmou, krahets, Jie-Han Chen, Atishay Garg, Marcel Flygare, adtygan, Nik Vaessen,\nbolded, Louis Schlessinger, Balaji Varatharajan, atgctg, Kaixin Li, Victor Barbaros, Ric-\ncardo Musto, Elizabeth Ho, azimjonn, Guilherme Miotto, Alessandro Finamore, Joji Joseph,\nAnthony Biel, Zeming Zhao, shjustinbaek, gab-chen, nantekoto, Yutaro Nishiyama, Oren\nAmsalem, Tian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen,\nMatthew Caseres, Andrew Walsh, pggPL, RohanKarthikeyan, Ryan Choi, and Likun Lei.\nWe thank Amazon Web Services, especially Wen-Ming Ye, George Karypis, Swami Siva-\nsubramanian, Peter DeSantis, Adam Selipsky, and Andrew Jassy for their generous support\nin writing this book. Without the available time, resources, discussions with colleagues, and\ncontinuous encouragement, this book would not have happened. During the preparation of\nthe book for publication, Cambridge University Press has oﬀered excellent support. We thank\nour commissioning editor David Tranah for his help and professionalism.\nSummary\nDeep learning has revolutionized pattern recognition, introducing technology that now powers\na wide range of technologies, in such diverse ﬁelds as computer vision, natural language\nprocessing, and automatic speech recognition. To successfully apply deep learning, you must\nunderstand how to cast a problem, the basic mathematics of modeling, the algorithms for\nﬁtting your models to data, and the engineering techniques to implement it all. This book\npresents a comprehensive resource, including prose, ﬁgures, mathematics, and code, all in\none place.\n\nxxxiii\nPreface\n11\n12\nExercises\n1. Register an account on the discussion forum of this book discuss.d2l.ai11.\n2. Install Python on your computer.\n3. Follow the links at the bottom of the section to the forum, where you will be able to seek\nout help and discuss the book and ﬁnd answers to your questions by engaging the authors\nand broader community.\nDiscussions12.\n\n13\n14\nInstallation\nIn order to get up and running, we will need an environment for running Python, the Jupyter\nNotebook, the relevant libraries, and the code needed to run the book itself.\nInstalling Miniconda\nYour simplest option is to install Miniconda13. Note that the Python 3.x version is required.\nYou can skip the following steps if your machine already has conda installed.\nVisit the Miniconda website and determine the appropriate version for your system based\non your Python 3.x version and machine architecture. Suppose that your Python version is\n3.9 (our tested version). If you are using macOS, you would download the bash script whose\nname contains the strings “MacOSX”, navigate to the download location, and execute the\ninstallation as follows (taking Intel Macs as an example):\n# The file name is subject to changes\nsh Miniconda3-py39_4.12.0-MacOSX-x86_64.sh -b\nA Linux user would download the ﬁle whose name contains the strings “Linux” and execute\nthe following at the download location:\n# The file name is subject to changes\nsh Miniconda3-py39_4.12.0-Linux-x86_64.sh -b\nA Windows user would download and install Miniconda by following its online instructions\n14 . On Windows, you may search for cmd to open the Command Prompt (command-line\ninterpreter) for running commands.\nNext, initialize the shell so we can run conda directly.\n~/miniconda3/bin/conda init\nThen close and reopen your current shell. You should be able to create a new environment as\nfollows:\nxxxiv\n\nxxxv\nInstallation\n15\n16\nconda create --name d2l python=3.9 -y\nNow we can activate the d2l environment:\nconda activate d2l\nInstalling the Deep Learning Framework and the\nd2l Package\nBefore installing any deep learning framework, please ﬁrst check whether or not you have\nproper GPUs on your machine (the GPUs that power the display on a standard laptop are\nnot relevant for our purposes). For example, if your computer has NVIDIA GPUs and has\ninstalled CUDA15, then you are all set. If your machine does not house any GPU, there is no\nneed to worry just yet. Your CPU provides more than enough horsepower to get you through\nthe ﬁrst few chapters. Just remember that you will want to access GPUs before running larger\nmodels.\nYou can install PyTorch (the speciﬁed versions are tested at the time of writing) with either\nCPU or GPU support as follows:\npip install torch==2.0.0 torchvision==0.15.1\nOur next step is to install the d2l package that we developed in order to encapsulate frequently\nused functions and classes found throughout this book:\npip install d2l==1.0.3\nDownloading and Running the Code\nNext, you will want to download the notebooks so that you can run each of the book’s code\nblocks. Simply click on the “Notebooks” tab at the top of any HTML page on the D2L.ai\nwebsite16 to download the code and then unzip it. Alternatively, you can fetch the notebooks\nfrom the command line as follows:\n\nxxxvi\nInstallation\n17\nmkdir d2l-en && cd d2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -o d2l-en.zip\nunzip d2l-en.zip && rm d2l-en.zip\ncd pytorch\nIf you do not already have unzip installed, ﬁrst run sudo apt-get install unzip. Now\nwe can start the Jupyter Notebook server by running:\njupyter notebook\nAt this point, you can open http://localhost:8888 (it may have already opened automatically)\nin your web browser. Then we can run the code for each section of the book. Whenever\nyou open a new command line window, you will need to execute conda activate d2l\nto activate the runtime environment before running the D2L notebooks, or updating your\npackages (either the deep learning framework or the d2l package). To exit the environment,\nrun conda deactivate.\nDiscussions17.\n\nNotation\nThroughout this book, we adhere to the following notational conventions. Note that some\nof these symbols are placeholders, while others refer to speciﬁc objects. As a general rule\nof thumb, the indeﬁnite article “a” often indicates that the symbol is a placeholder and that\nsimilarly formatted symbols can denote other objects of the same type. For example, “x:\na scalar” means that lowercased letters generally represent scalar values, but “Z: the set of\nintegers” refers speciﬁcally to the symbol Z.\nNumerical Objects\n• x: a scalar\n• x: a vector\n• X: a matrix\n• X: a general tensor\n• I: the identity matrix (of some given dimension), i.e., a square matrix with 1 on all diagonal\nentries and 0 on all oﬀ-diagonals\n• xi, [x]i: the ith element of vector x\n• xij, xi, j,[X]ij, [X]i, j: the element of matrix X at row i and column j.\nSet Theory\n• X: a set\n• Z: the set of integers\n• Z+: the set of positive integers\nxxxvii\n\nxxxviii\nNotation\n• R: the set of real numbers\n• Rn: the set of n-dimensional vectors of real numbers\n• Ra×b: The set of matrices of real numbers with a rows and b columns\n• |X|: cardinality (number of elements) of set X\n• A ∪B: union of sets A and B\n• A ∩B: intersection of sets A and B\n• A \\B: set subtraction of B from A (contains only those elements of A that do not belong\nto B)\nFunctions and Operators\n• f (·): a function\n• log(·): the natural logarithm (base e)\n• log2(·): logarithm to base 2\n• exp(·): the exponential function\n• 1(·): the indicator function; evaluates to 1 if the boolean argument is true, and 0 otherwise\n• 1X(z): the set-membership indicator function; evaluates to 1 if the element z belongs to\nthe set X and 0 otherwise\n• (·)⊤: transpose of a vector or a matrix\n• X−1: inverse of matrix X\n• ⊙: Hadamard (elementwise) product\n• [·, ·]: concatenation\n• ∥· ∥p: ℓp norm\n• ∥· ∥: ℓ2 norm\n• ⟨x, y⟩: inner (dot) product of vectors x and y\n• ∑: summation over a collection of elements\n• ∏: product over a collection of elements\n• def\n=: an equality asserted as a deﬁnition of the symbol on the left-hand side\n\nxxxix\nNotation\n18\nCalculus\n•\ndy\ndx : derivative of y with respect to x\n•\n∂y\n∂x : partial derivative of y with respect to x\n• ∇xy: gradient of y with respect to x\n•\n∫b\na f (x) dx: deﬁnite integral of f from a to b with respect to x\n•\n∫\nf (x) dx: indeﬁnite integral of f with respect to x\nProbability and Information Theory\n• X: a random variable\n• P: a probability distribution\n• X ∼P: the random variable X follows distribution P\n• P(X = x): the probability assigned to the event where random variable X takes value x\n• P(X | Y): the conditional probability distribution of X given Y\n• p(·): a probability density function (PDF) associated with distribution P\n• E[X]: expectation of a random variable X\n• X ⊥Y: random variables X and Y are independent\n• X ⊥Y | Z: random variables X and Y are conditionally independent given Z\n• σX: standard deviation of random variable X\n• Var(X): variance of random variable X, equal to σ2\nX\n• Cov(X,Y): covariance of random variables X and Y\n• ρ(X,Y): the Pearson correlation coeﬃcient between X and Y, equals Cov(X,Y)\nσX σY\n• H(X): entropy of random variable X\n• DKL(P∥Q): the KL-divergence (or relative entropy) from distribution Q to distribution P\nDiscussions18.\n\n\n1\nIntroduction\nUntil recently, nearly every computer program that you might have interacted with during an\nordinary day was coded up as a rigid set of rules specifying precisely how it should behave. Say\nthat we wanted to write an application to manage an e-commerce platform. After huddling\naround a whiteboard for a few hours to ponder the problem, we might settle on the broad\nstrokes of a working solution, for example: (i) users interact with the application through an\ninterface running in a web browser or mobile application; (ii) our application interacts with\na commercial-grade database engine to keep track of each user’s state and maintain records\nof historical transactions; and (iii) at the heart of our application, the business logic (you\nmight say, the brains) of our application spells out a set of rules that map every conceivable\ncircumstance to the corresponding action that our program should take.\nTo build the brains of our application, we might enumerate all the common events that our\nprogram should handle. For example, whenever a customer clicks to add an item to their\nshopping cart, our program should add an entry to the shopping cart database table, associ-\nating that user’s ID with the requested product’s ID. We might then attempt to step through\nevery possible corner case, testing the appropriateness of our rules and making any neces-\nsary modiﬁcations. What happens if a user initiates a purchase with an empty cart? While\nfew developers ever get it completely right the ﬁrst time (it might take some test runs to work\nout the kinks), for the most part we can write such programs and conﬁdently launch them\nbefore ever seeing a real customer. Our ability to manually design automated systems that\ndrive functioning products and systems, often in novel situations, is a remarkable cognitive\nfeat. And when you are able to devise solutions that work 100% of the time, you typically\nshould not be worrying about machine learning.\nFortunately for the growing community of machine learning scientists, many tasks that we\nwould like to automate do not bend so easily to human ingenuity. Imagine huddling around\nthe whiteboard with the smartest minds you know, but this time you are tackling one of the\nfollowing problems:\n• Write a program that predicts tomorrow’s weather given geographic information, satellite\nimages, and a trailing window of past weather.\n• Write a program that takes in a factoid question, expressed in free-form text, and answers\nit correctly.\n• Write a program that, given an image, identiﬁes every person depicted in it and draws\noutlines around each.\n1\n\n2\nIntroduction\n• Write a program that presents users with products that they are likely to enjoy but unlikely,\nin the natural course of browsing, to encounter.\nFor these problems, even elite programmers would struggle to code up solutions from scratch.\nThe reasons can vary. Sometimes the program that we are looking for follows a pattern that\nchanges over time, so there is no ﬁxed right answer! In such cases, any successful solution\nmust adapt gracefully to a changing world. At other times, the relationship (say between\npixels, and abstract categories) may be too complicated, requiring thousands or millions of\ncomputations and following unknown principles. In the case of image recognition, the precise\nsteps required to perform the task lie beyond our conscious understanding, even though our\nsubconscious cognitive processes execute the task eﬀortlessly.\nMachine learning is the study of algorithms that can learn from experience. As a machine\nlearning algorithm accumulates more experience, typically in the form of observational data\nor interactions with an environment, its performance improves. Contrast this with our deter-\nministic e-commerce platform, which follows the same business logic, no matter how much\nexperience accrues, until the developers themselves learn and decide that it is time to update\nthe software. In this book, we will teach you the fundamentals of machine learning, focusing\nin particular on deep learning, a powerful set of techniques driving innovations in areas as\ndiverse as computer vision, natural language processing, healthcare, and genomics.\n1.1 A Motivating Example\nBefore beginning writing, the authors of this book, like much of the work force, had to be-\ncome caﬀeinated. We hopped in the car and started driving. Using an iPhone, Alex called\nout “Hey Siri”, awakening the phone’s voice recognition system. Then Mu commanded “di-\nrections to Blue Bottle coﬀee shop”. The phone quickly displayed the transcription of his\ncommand. It also recognized that we were asking for directions and launched the Maps ap-\nplication (app) to fulﬁll our request. Once launched, the Maps app identiﬁed a number of\nroutes. Next to each route, the phone displayed a predicted transit time. While this story\nwas fabricated for pedagogical convenience, it demonstrates that in the span of just a few\nseconds, our everyday interactions with a smart phone can engage several machine learning\nmodels.\nImagine just writing a program to respond to a wake word such as “Alexa”, “OK Google”,\nand “Hey Siri”. Try coding it up in a room by yourself with nothing but a computer and\na code editor, as illustrated in Fig. 1.1.1. How would you write such a program from ﬁrst\nprinciples? Think about it… the problem is hard. Every second, the microphone will collect\nroughly 44,000 samples. Each sample is a measurement of the amplitude of the sound wave.\nWhat rule could map reliably from a snippet of raw audio to conﬁdent predictions {yes, no}\nabout whether the snippet contains the wake word? If you are stuck, do not worry. We do\n\n3\nA Motivating Example\nnot know how to write such a program from scratch either. That is why we use machine\nlearning.\nt\nFig. 1.1.1\nIdentify a wake word.\nHere is the trick. Often, even when we do not know how to tell a computer explicitly how\nto map from inputs to outputs, we are nonetheless capable of performing the cognitive feat\nourselves. In other words, even if you do not know how to program a computer to recog-\nnize the word “Alexa”, you yourself are able to recognize it. Armed with this ability, we can\ncollect a huge dataset containing examples of audio snippets and associated labels, indicat-\ning which snippets contain the wake word. In the currently dominant approach to machine\nlearning, we do not attempt to design a system explicitly to recognize wake words. Instead,\nwe deﬁne a ﬂexible program whose behavior is determined by a number of parameters. Then\nwe use the dataset to determine the best possible parameter values, i.e., those that improve\nthe performance of our program with respect to a chosen performance measure.\nYou can think of the parameters as knobs that we can turn, manipulating the behavior of the\nprogram. Once the parameters are ﬁxed, we call the program a model. The set of all distinct\nprograms (input–output mappings) that we can produce just by manipulating the parameters\nis called a family of models. And the “meta-program” that uses our dataset to choose the\nparameters is called a learning algorithm.\nBefore we can go ahead and engage the learning algorithm, we have to deﬁne the problem\nprecisely, pinning down the exact nature of the inputs and outputs, and choosing an appropri-\nate model family. In this case, our model receives a snippet of audio as input, and the model\ngenerates a selection among {yes, no} as output. If all goes according to plan the model’s\nguesses will typically be correct as to whether the snippet contains the wake word.\nIf we choose the right family of models, there should exist one setting of the knobs such\nthat the model ﬁres “yes” every time it hears the word “Alexa”. Because the exact choice of\nthe wake word is arbitrary, we will probably need a model family suﬃciently rich that, via\nanother setting of the knobs, it could ﬁre “yes” only upon hearing the word “Apricot”. We\nexpect that the same model family should be suitable for “Alexa” recognition and “Apricot”\nrecognition because they seem, intuitively, to be similar tasks. However, we might need a\ndiﬀerent family of models entirely if we want to deal with fundamentally diﬀerent inputs\nor outputs, say if we wanted to map from images to captions, or from English sentences to\nChinese sentences.\nAs you might guess, if we just set all of the knobs randomly, it is unlikely that our model will\nrecognize “Alexa”, “Apricot”, or any other English word. In machine learning, the learning\nis the process by which we discover the right setting of the knobs for coercing the desired\nbehavior from our model. In other words, we train our model with data. As shown in Fig.\n1.1.2, the training process usually looks like the following:\n\n4\nIntroduction\n1. Start oﬀwith a randomly initialized model that cannot do anything useful.\n2. Grab some of your data (e.g., audio snippets and corresponding {yes, no} labels).\n3. Tweak the knobs to make the model perform better as assessed on those examples.\n4. Repeat Steps 2 and 3 until the model is awesome.\nt\nFig. 1.1.2\nA typical training process.\nTo summarize, rather than code up a wake word recognizer, we code up a program that can\nlearn to recognize wake words, if presented with a large labeled dataset. You can think of\nthis act of determining a program’s behavior by presenting it with a dataset as programming\nwith data. That is to say, we can “program” a cat detector by providing our machine learning\nsystem with many examples of cats and dogs. This way the detector will eventually learn to\nemit a very large positive number if it is a cat, a very large negative number if it is a dog, and\nsomething closer to zero if it is not sure. This barely scratches the surface of what machine\nlearning can do. Deep learning, which we will explain in greater detail later, is just one among\nmany popular methods for solving machine learning problems.\n1.2 Key Components\nIn our wake word example, we described a dataset consisting of audio snippets and binary\nlabels, and we gave a hand-wavy sense of how we might train a model to approximate a\nmapping from snippets to classiﬁcations. This sort of problem, where we try to predict a\ndesignated unknown label based on known inputs given a dataset consisting of examples for\nwhich the labels are known, is called supervised learning. This is just one among many kinds\nof machine learning problems. Before we explore other varieties, we would like to shed more\nlight on some core components that will follow us around, no matter what kind of machine\nlearning problem we tackle:\n1. The data that we can learn from.\n2. A model of how to transform the data.\n3. An objective function that quantiﬁes how well (or badly) the model is doing.\n4. An algorithm to adjust the model’s parameters to optimize the objective function.\n\n5\nKey Components\n1.2.1 Data\nIt might go without saying that you cannot do data science without data. We could lose hun-\ndreds of pages pondering what precisely data is, but for now, we will focus on the key prop-\nerties of the datasets that we will be concerned with. Generally, we are concerned with a\ncollection of examples. In order to work with data usefully, we typically need to come up\nwith a suitable numerical representation. Each example (or data point, data instance, sample)\ntypically consists of a set of attributes called features (sometimes called covariates or inputs),\nbased on which the model must make its predictions. In supervised learning problems, our\ngoal is to predict the value of a special attribute, called the label (or target), that is not part\nof the model’s input.\nIf we were working with image data, each example might consist of an individual photograph\n(the features) and a number indicating the category to which the photograph belongs (the\nlabel). The photograph would be represented numerically as three grids of numerical values\nrepresenting the brightness of red, green, and blue light at each pixel location. For example,\na 200 × 200 pixel color photograph would consist of 200 × 200 × 3 = 120000 numerical\nvalues.\nAlternatively, we might work with electronic health record data and tackle the task of pre-\ndicting the likelihood that a given patient will survive the next 30 days. Here, our features\nmight consist of a collection of readily available attributes and frequently recorded measure-\nments, including age, vital signs, comorbidities, current medications, and recent procedures.\nThe label available for training would be a binary value indicating whether each patient in\nthe historical data survived within the 30-day window.\nIn such cases, when every example is characterized by the same number of numerical features,\nwe say that the inputs are ﬁxed-length vectors and we call the (constant) length of the vectors\nthe dimensionality of the data. As you might imagine, ﬁxed-length inputs can be convenient,\ngiving us one less complication to worry about. However, not all data can easily be represented\nas ﬁxed-length vectors. While we might expect microscope images to come from standard\nequipment, we cannot expect images mined from the Internet all to have the same resolution\nor shape. For images, we might consider cropping them to a standard size, but that strategy\nonly gets us so far. We risk losing information in the cropped-out portions. Moreover, text data\nresists ﬁxed-length representations even more stubbornly. Consider the customer reviews left\non e-commerce sites such as Amazon, IMDb, and TripAdvisor. Some are short: “it stinks!”.\nOthers ramble for pages. One major advantage of deep learning over traditional methods is\nthe comparative grace with which modern models can handle varying-length data.\nGenerally, the more data we have, the easier our job becomes. When we have more data,\nwe can train more powerful models and rely less heavily on preconceived assumptions. The\nregime change from (comparatively) small to big data is a major contributor to the success\nof modern deep learning. To drive the point home, many of the most exciting models in deep\nlearning do not work without large datasets. Some others might work in the small data regime,\nbut are no better than traditional approaches.\nFinally, it is not enough to have lots of data and to process it cleverly. We need the right data.\n\n6\nIntroduction\nIf the data is full of mistakes, or if the chosen features are not predictive of the target quantity\nof interest, learning is going to fail. The situation is captured well by the cliché: garbage in,\ngarbage out. Moreover, poor predictive performance is not the only potential consequence.\nIn sensitive applications of machine learning, like predictive policing, resume screening, and\nrisk models used for lending, we must be especially alert to the consequences of garbage data.\nOne commonly occurring failure mode concerns datasets where some groups of people are\nunrepresented in the training data. Imagine applying a skin cancer recognition system that\nhad never seen black skin before. Failure can also occur when the data does not only under-\nrepresent some groups but reﬂects societal prejudices. For example, if past hiring decisions\nare used to train a predictive model that will be used to screen resumes then machine learning\nmodels could inadvertently capture and automate historical injustices. Note that this can all\nhappen without the data scientist actively conspiring, or even being aware.\n1.2.2 Models\nMost machine learning involves transforming the data in some sense. We might want to build\na system that ingests photos and predicts smiley-ness. Alternatively, we might want to ingest\na set of sensor readings and predict how normal vs. anomalous the readings are. By model, we\ndenote the computational machinery for ingesting data of one type, and spitting out predic-\ntions of a possibly diﬀerent type. In particular, we are interested in statistical models that can\nbe estimated from data. While simple models are perfectly capable of addressing appropri-\nately simple problems, the problems that we focus on in this book stretch the limits of classical\nmethods. Deep learning is diﬀerentiated from classical approaches principally by the set of\npowerful models that it focuses on. These models consist of many successive transformations\nof the data that are chained together top to bottom, thus the name deep learning. On our way\nto discussing deep models, we will also discuss some more traditional methods.\n1.2.3 Objective Functions\nEarlier, we introduced machine learning as learning from experience. By learning here, we\nmean improving at some task over time. But who is to say what constitutes an improvement?\nYou might imagine that we could propose updating our model, and some people might dis-\nagree on whether our proposal constituted an improvement or not.\nIn order to develop a formal mathematical system of learning machines, we need to have\nformal measures of how good (or bad) our models are. In machine learning, and optimization\nmore generally, we call these objective functions. By convention, we usually deﬁne objective\nfunctions so that lower is better. This is merely a convention. You can take any function\nfor which higher is better, and turn it into a new function that is qualitatively identical but\nfor which lower is better by ﬂipping the sign. Because we choose lower to be better, these\nfunctions are sometimes called loss functions.\nWhen trying to predict numerical values, the most common loss function is squared error,\ni.e., the square of the diﬀerence between the prediction and the ground truth target. For clas-\nsiﬁcation, the most common objective is to minimize error rate, i.e., the fraction of examples\n\n7\nKinds of Machine Learning Problems\non which our predictions disagree with the ground truth. Some objectives (e.g., squared error)\nare easy to optimize, while others (e.g., error rate) are diﬃcult to optimize directly, owing to\nnon-diﬀerentiability or other complications. In these cases, it is common instead to optimize\na surrogate objective.\nDuring optimization, we think of the loss as a function of the model’s parameters, and treat\nthe training dataset as a constant. We learn the best values of our model’s parameters by\nminimizing the loss incurred on a set consisting of some number of examples collected for\ntraining. However, doing well on the training data does not guarantee that we will do well\non unseen data. So we will typically want to split the available data into two partitions: the\ntraining dataset (or training set), for learning model parameters; and the test dataset (or test\nset), which is held out for evaluation. At the end of the day, we typically report how our\nmodels perform on both partitions. You could think of training performance as analogous to\nthe scores that a student achieves on the practice exams used to prepare for some real ﬁnal\nexam. Even if the results are encouraging, that does not guarantee success on the ﬁnal exam.\nOver the course of studying, the student might begin to memorize the practice questions,\nappearing to master the topic but faltering when faced with previously unseen questions on\nthe actual ﬁnal exam. When a model performs well on the training set but fails to generalize\nto unseen data, we say that it is overﬁtting to the training data.\n1.2.4 Optimization Algorithms\nOnce we have got some data source and representation, a model, and a well-deﬁned objective\nfunction, we need an algorithm capable of searching for the best possible parameters for\nminimizing the loss function. Popular optimization algorithms for deep learning are based on\nan approach called gradient descent. In brief, at each step, this method checks to see, for each\nparameter, how that training set loss would change if you perturbed that parameter by just a\nsmall amount. It would then update the parameter in the direction that lowers the loss.\n1.3 Kinds of Machine Learning Problems\nThe wake word problem in our motivating example is just one among many that machine\nlearning can tackle. To motivate the reader further and provide us with some common lan-\nguage that will follow us throughout the book, we now provide a broad overview of the land-\nscape of machine learning problems.\n1.3.1 Supervised Learning\nSupervised learning describes tasks where we are given a dataset containing both features\nand labels and asked to produce a model that predicts the labels when given input features.\n\n8\nIntroduction\nEach feature–label pair is called an example. Sometimes, when the context is clear, we may\nuse the term examples to refer to a collection of inputs, even when the corresponding labels\nare unknown. The supervision comes into play because, for choosing the parameters, we (the\nsupervisors) provide the model with a dataset consisting of labeled examples. In probabilistic\nterms, we typically are interested in estimating the conditional probability of a label given\ninput features. While it is just one among several paradigms, supervised learning accounts for\nthe majority of successful applications of machine learning in industry. Partly that is because\nmany important tasks can be described crisply as estimating the probability of something\nunknown given a particular set of available data:\n• Predict cancer vs. not cancer, given a computer tomography image.\n• Predict the correct translation in French, given a sentence in English.\n• Predict the price of a stock next month based on this month’s ﬁnancial reporting data.\nWhile all supervised learning problems are captured by the simple description “predicting the\nlabels given input features”, supervised learning itself can take diverse forms and require tons\nof modeling decisions, depending on (among other considerations) the type, size, and quantity\nof the inputs and outputs. For example, we use diﬀerent models for processing sequences of\narbitrary lengths and ﬁxed-length vector representations. We will visit many of these problems\nin depth throughout this book.\nInformally, the learning process looks something like the following. First, grab a big collec-\ntion of examples for which the features are known and select from them a random subset,\nacquiring the ground truth labels for each. Sometimes these labels might be available data\nthat have already been collected (e.g., did a patient die within the following year?) and other\ntimes we might need to employ human annotators to label the data, (e.g., assigning images\nto categories). Together, these inputs and corresponding labels comprise the training set. We\nfeed the training dataset into a supervised learning algorithm, a function that takes as input a\ndataset and outputs another function: the learned model. Finally, we can feed previously un-\nseen inputs to the learned model, using its outputs as predictions of the corresponding label.\nThe full process is drawn in Fig. 1.3.1.\nt\nFig. 1.3.1\nSupervised learning.\nRegression\nPerhaps the simplest supervised learning task to wrap your head around is regression. Con-\nsider, for example, a set of data harvested from a database of home sales. We might construct\n\n9\nKinds of Machine Learning Problems\n19\na table, in which each row corresponds to a diﬀerent house, and each column corresponds\nto some relevant attribute, such as the square footage of a house, the number of bedrooms,\nthe number of bathrooms, and the number of minutes (walking) to the center of town. In\nthis dataset, each example would be a speciﬁc house, and the corresponding feature vector\nwould be one row in the table. If you live in New York or San Francisco, and you are not\nthe CEO of Amazon, Google, Microsoft, or Facebook, the (sq. footage, no. of bedrooms,\nno. of bathrooms, walking distance) feature vector for your home might look something like:\n[600, 1, 1, 60]. However, if you live in Pittsburgh, it might look more like [3000, 4, 3, 10].\nFixed-length feature vectors like this are essential for most classic machine learning algo-\nrithms.\nWhat makes a problem a regression is actually the form of the target. Say that you are in\nthe market for a new home. You might want to estimate the fair market value of a house,\ngiven some features such as above. The data here might consist of historical home listings\nand the labels might be the observed sales prices. When labels take on arbitrary numerical\nvalues (even within some interval), we call this a regression problem. The goal is to produce\na model whose predictions closely approximate the actual label values.\nLots of practical problems are easily described as regression problems. Predicting the rating\nthat a user will assign to a movie can be thought of as a regression problem and if you designed\na great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar\nNetﬂix prize 19 . Predicting the length of stay for patients in the hospital is also a regression\nproblem. A good rule of thumb is that any how much? or how many? problem is likely to be\nregression. For example:\n• How many hours will this surgery take?\n• How much rainfall will this town have in the next six hours?\nEven if you have never worked with machine learning before, you have probably worked\nthrough a regression problem informally. Imagine, for example, that you had your drains\nrepaired and that your contractor spent 3 hours removing gunk from your sewage pipes. Then\nthey sent you a bill of 350 dollars. Now imagine that your friend hired the same contractor for\n2 hours and received a bill of 250 dollars. If someone then asked you how much to expect on\ntheir upcoming gunk-removal invoice you might make some reasonable assumptions, such as\nmore hours worked costs more dollars. You might also assume that there is some base charge\nand that the contractor then charges per hour. If these assumptions held true, then given these\ntwo data examples, you could already identify the contractor’s pricing structure: 100 dollars\nper hour plus 50 dollars to show up at your house. If you followed that much, then you already\nunderstand the high-level idea behind linear regression.\nIn this case, we could produce the parameters that exactly matched the contractor’s prices.\nSometimes this is not possible, e.g., if some of the variation arises from factors beyond your\ntwo features. In these cases, we will try to learn models that minimize the distance between\nour predictions and the observed values. In most of our chapters, we will focus on minimizing\nthe squared error loss function. As we will see later, this loss corresponds to the assumption\nthat our data were corrupted by Gaussian noise.\n\n10\nIntroduction\nClassiﬁcation\nWhile regression models are great for addressing how many? questions, lots of problems do\nnot ﬁt comfortably in this template. Consider, for example, a bank that wants to develop a\ncheck scanning feature for its mobile app. Ideally, the customer would simply snap a photo\nof a check and the app would automatically recognize the text from the image. Assuming\nthat we had some ability to segment out image patches corresponding to each handwritten\ncharacter, then the primary remaining task would be to determine which character among\nsome known set is depicted in each image patch. These kinds of which one? problems are\ncalled classiﬁcation and require a diﬀerent set of tools from those used for regression, although\nmany techniques will carry over.\nIn classiﬁcation, we want our model to look at features, e.g., the pixel values in an image, and\nthen predict to which category (sometimes called a class) among some discrete set of options,\nan example belongs. For handwritten digits, we might have ten classes, corresponding to the\ndigits 0 through 9. The simplest form of classiﬁcation is when there are only two classes, a\nproblem which we call binary classiﬁcation. For example, our dataset could consist of images\nof animals and our labels might be the classes {cat, dog}. Whereas in regression we sought\na regressor to output a numerical value, in classiﬁcation we seek a classiﬁer, whose output is\nthe predicted class assignment.\nFor reasons that we will get into as the book gets more technical, it can be diﬃcult to optimize\na model that can only output a ﬁrm categorical assignment, e.g., either “cat” or “dog”. In these\ncases, it is usually much easier to express our model in the language of probabilities. Given\nfeatures of an example, our model assigns a probability to each possible class. Returning to\nour animal classiﬁcation example where the classes are {cat, dog}, a classiﬁer might see an\nimage and output the probability that the image is a cat as 0.9. We can interpret this number\nby saying that the classiﬁer is 90% sure that the image depicts a cat. The magnitude of the\nprobability for the predicted class conveys a notion of uncertainty. It is not the only one\navailable and we will discuss others in chapters dealing with more advanced topics.\nWhen we have more than two possible classes, we call the problem multiclass classiﬁca-\ntion. Common examples include handwritten character recognition {0, 1, 2, ... 9, a, b, c, ...}.\nWhile we attacked regression problems by trying to minimize the squared error loss function,\nthe common loss function for classiﬁcation problems is called cross-entropy, whose name will\nbe demystiﬁed when we introduce information theory in later chapters.\nNote that the most likely class is not necessarily the one that you are going to use for your\ndecision. Assume that you ﬁnd a beautiful mushroom in your backyard as shown in Fig.\n1.3.2.\nNow, assume that you built a classiﬁer and trained it to predict whether a mushroom is poi-\nsonous based on a photograph. Say our poison-detection classiﬁer outputs that the probability\nthat Fig. 1.3.2 shows a death cap is 0.2. In other words, the classiﬁer is 80% sure that our\nmushroom is not a death cap. Still, you would have to be a fool to eat it. That is because the\ncertain beneﬁt of a delicious dinner is not worth a 20% risk of dying from it. In other words,\nthe eﬀect of the uncertain risk outweighs the beneﬁt by far. Thus, in order to make a decision\n\n11\nKinds of Machine Learning Problems\nt\nFig. 1.3.2\nDeath cap - do not eat!\n20\nabout whether to eat the mushroom, we need to compute the expected detriment associated\nwith each action which depends both on the likely outcomes and the beneﬁts or harms as-\nsociated with each. In this case, the detriment incurred by eating the mushroom might be\n0.2 × ∞+ 0.8 × 0 = ∞, whereas the loss of discarding it is 0.2 × 0 + 0.8 × 1 = 0.8. Our\ncaution was justiﬁed: as any mycologist would tell us, the mushroom in Fig. 1.3.2 is actually\na death cap.\nClassiﬁcation can get much more complicated than just binary or multiclass classiﬁcation. For\ninstance, there are some variants of classiﬁcation addressing hierarchically structured classes.\nIn such cases not all errors are equal—if we must err, we might prefer to misclassify to a\nrelated class rather than a distant class. Usually, this is referred to as hierarchical classiﬁcation.\nFor inspiration, you might think of Linnaeus20, who organized fauna in a hierarchy.\nIn the case of animal classiﬁcation, it might not be so bad to mistake a poodle for a schnauzer,\nbut our model would pay a huge penalty if it confused a poodle with a dinosaur. Which hier-\narchy is relevant might depend on how you plan to use the model. For example, rattlesnakes\nand garter snakes might be close on the phylogenetic tree, but mistaking a rattler for a garter\ncould have fatal consequences.\nTagging\nSome classiﬁcation problems ﬁt neatly into the binary or multiclass classiﬁcation setups. For\nexample, we could train a normal binary classiﬁer to distinguish cats from dogs. Given the\ncurrent state of computer vision, we can do this easily, with oﬀ-the-shelf tools. Nonetheless,\nno matter how accurate our model gets, we might ﬁnd ourselves in trouble when the classiﬁer\nencounters an image of the Town Musicians of Bremen, a popular German fairy tale featuring\nfour animals (Fig. 1.3.3).\nAs you can see, the photo features a cat, a rooster, a dog, and a donkey, with some trees in\nthe background. If we anticipate encountering such images, multiclass classiﬁcation might\n\n12\nIntroduction\nt\nFig. 1.3.3\nA donkey, a dog, a cat, and a rooster.\n21\nnot be the right problem formulation. Instead, we might want to give the model the option of\nsaying the image depicts a cat, a dog, a donkey, and a rooster.\nThe problem of learning to predict classes that are not mutually exclusive is called multi-\nlabel classiﬁcation. Auto-tagging problems are typically best described in terms of multi-label\nclassiﬁcation. Think of the tags people might apply to posts on a technical blog, e.g., “machine\nlearning”, “technology”, “gadgets”, “programming languages”, “Linux”, “cloud computing”,\n“AWS”. A typical article might have 5–10 tags applied. Typically, tags will exhibit some\ncorrelation structure. Posts about “cloud computing” are likely to mention “AWS” and posts\nabout “machine learning” are likely to mention “GPUs”.\nSometimes such tagging problems draw on enormous label sets. The National Library of\nMedicine employs many professional annotators who associate each article to be indexed in\nPubMed with a set of tags drawn from the Medical Subject Headings (MeSH) ontology, a\ncollection of roughly 28,000 tags. Correctly tagging articles is important because it allows\nresearchers to conduct exhaustive reviews of the literature. This is a time-consuming process\nand typically there is a one-year lag between archiving and tagging. Machine learning can\nprovide provisional tags until each article has a proper manual review. Indeed, for several\nyears, the BioASQ organization has hosted competitions21 for this task.\n\n13\nKinds of Machine Learning Problems\n22\nSearch\nIn the ﬁeld of information retrieval, we often impose ranks on sets of items. Take web search\nfor example. The goal is less to determine whether a particular page is relevant for a query,\nbut rather which, among a set of relevant results, should be shown most prominently to a\nparticular user. One way of doing this might be to ﬁrst assign a score to every element in\nthe set and then to retrieve the top-rated elements. PageRank 22 , the original secret sauce\nbehind the Google search engine, was an early example of such a scoring system. Weirdly,\nthe scoring provided by PageRank did not depend on the actual query. Instead, they relied on\na simple relevance ﬁlter to identify the set of relevant candidates and then used PageRank to\nprioritize the more authoritative pages. Nowadays, search engines use machine learning and\nbehavioral models to obtain query-dependent relevance scores. There are entire academic\nconferences devoted to this subject.\nRecommender Systems\nRecommender systems are another problem setting that is related to search and ranking. The\nproblems are similar insofar as the goal is to display a set of items relevant to the user. The\nmain diﬀerence is the emphasis on personalization to speciﬁc users in the context of rec-\nommender systems. For instance, for movie recommendations, the results page for a science\nﬁction fan and the results page for a connoisseur of Peter Sellers comedies might diﬀer signif-\nicantly. Similar problems pop up in other recommendation settings, e.g., for retail products,\nmusic, and news recommendation.\nIn some cases, customers provide explicit feedback, communicating how much they liked a\nparticular product (e.g., the product ratings and reviews on Amazon, IMDb, or Goodreads). In\nother cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which might\nindicate dissatisfaction or maybe just indicate that the song was inappropriate in context.\nIn the simplest formulations, these systems are trained to estimate some score, such as an\nexpected star rating or the probability that a given user will purchase a particular item.\nGiven such a model, for any given user, we could retrieve the set of objects with the largest\nscores, which could then be recommended to the user. Production systems are considerably\nmore advanced and take detailed user activity and item characteristics into account when\ncomputing such scores. Fig. 1.3.4 displays the deep learning books recommended by Amazon\nbased on personalization algorithms tuned to capture Aston’s preferences.\nDespite their tremendous economic value, recommender systems naively built on top of pre-\ndictive models suﬀer some serious conceptual ﬂaws. To start, we only observe censored feed-\nback: users preferentially rate movies that they feel strongly about. For example, on a ﬁve-\npoint scale, you might notice that items receive many one- and ﬁve-star ratings but that there\nare conspicuously few three-star ratings. Moreover, current purchase habits are often a result\nof the recommendation algorithm currently in place, but learning algorithms do not always\ntake this detail into account. Thus it is possible for feedback loops to form where a recom-\nmender system preferentially pushes an item that is then taken to be better (due to greater pur-\n\n14\nIntroduction\nt\nFig. 1.3.4\nDeep learning books recommended by Amazon.\nchases) and in turn is recommended even more frequently. Many of these problems—about\nhow to deal with censoring, incentives, and feedback loops—are important open research\nquestions.\nSequence Learning\nSo far, we have looked at problems where we have some ﬁxed number of inputs and produce a\nﬁxed number of outputs. For example, we considered predicting house prices given a ﬁxed set\nof features: square footage, number of bedrooms, number of bathrooms, and the transit time\nto downtown. We also discussed mapping from an image (of ﬁxed dimension) to the predicted\nprobabilities that it belongs to each among a ﬁxed number of classes and predicting star ratings\nassociated with purchases based on the user ID and product ID alone. In these cases, once our\nmodel is trained, after each test example is fed into our model, it is immediately forgotten.\nWe assumed that successive observations were independent and thus there was no need to\nhold on to this context.\nBut how should we deal with video snippets? In this case, each snippet might consist of\na diﬀerent number of frames. And our guess of what is going on in each frame might be\nmuch stronger if we take into account the previous or succeeding frames. The same goes for\nlanguage. For example, one popular deep learning problem is machine translation: the task\nof ingesting sentences in some source language and predicting their translations in another\nlanguage.\nSuch problems also occur in medicine. We might want a model to monitor patients in the\n\n15\nKinds of Machine Learning Problems\nintensive care unit and to ﬁre oﬀalerts whenever their risk of dying in the next 24 hours\nexceeds some threshold. Here, we would not throw away everything that we know about the\npatient history every hour, because we might not want to make predictions based only on the\nmost recent measurements.\nQuestions like these are among the most exciting applications of machine learning and they\nare instances of sequence learning. They require a model either to ingest sequences of in-\nputs or to emit sequences of outputs (or both). Speciﬁcally, sequence-to-sequence learning\nconsiders problems where both inputs and outputs consist of variable-length sequences. Ex-\namples include machine translation and speech-to-text transcription. While it is impossible\nto consider all types of sequence transformations, the following special cases are worth men-\ntioning.\nTagging and Parsing. This involves annotating a text sequence with attributes. Here, the\ninputs and outputs are aligned, i.e., they are of the same number and occur in a corresponding\norder. For instance, in part-of-speech (PoS) tagging, we annotate every word in a sentence\nwith the corresponding part of speech, i.e., “noun” or “direct object”. Alternatively, we might\nwant to know which groups of contiguous words refer to named entities, like people, places,\nor organizations. In the cartoonishly simple example below, we might just want to indicate\nwhether or not any word in the sentence is part of a named entity (tagged as “Ent”).\nTom has dinner in Washington with Sally\nEnt\n-\n-\n-\nEnt\n-\nEnt\nAutomatic Speech Recognition. With speech recognition, the input sequence is an audio\nrecording of a speaker (Fig. 1.3.5), and the output is a transcript of what the speaker said. The\nchallenge is that there are many more audio frames (sound is typically sampled at 8kHz or\n16kHz) than text, i.e., there is no 1:1 correspondence between audio and text, since thousands\nof samples may correspond to a single spoken word. These are sequence-to-sequence learning\nproblems, where the output is much shorter than the input. While humans are remarkably\ngood at recognizing speech, even from low-quality audio, getting computers to perform the\nsame feat is a formidable challenge.\nt\nFig. 1.3.5\n-D-e-e-p- L-ea-r-ni-ng- in an audio recording.\nText to Speech. This is the inverse of automatic speech recognition. Here, the input is text\nand the output is an audio ﬁle. In this case, the output is much longer than the input.\nMachine Translation. Unlike the case of speech recognition, where corresponding inputs\nand outputs occur in the same order, in machine translation, unaligned data poses a new chal-\n\n16\nIntroduction\nlenge. Here the input and output sequences can have diﬀerent lengths, and the corresponding\nregions of the respective sequences may appear in a diﬀerent order. Consider the following\nillustrative example of the peculiar tendency of Germans to place the verbs at the end of\nsentences:\nGerman:\nHaben Sie sich schon dieses grossartige Lehrwerk angeschaut?\nEnglish:\nHave you already looked at this excellent textbook?\nWrong alignment:\nHave you yourself already this excellent textbook looked at?\nMany related problems pop up in other learning tasks. For instance, determining the order\nin which a user reads a webpage is a two-dimensional layout analysis problem. Dialogue\nproblems exhibit all kinds of additional complications, where determining what to say next\nrequires taking into account real-world knowledge and the prior state of the conversation\nacross long temporal distances. Such topics are active areas of research.\n1.3.2 Unsupervised and Self-Supervised Learning\nThe previous examples focused on supervised learning, where we feed the model a giant\ndataset containing both the features and corresponding label values. You could think of the\nsupervised learner as having an extremely specialized job and an extremely dictatorial boss.\nThe boss stands over the learner’s shoulder and tells them exactly what to do in every situation\nuntil they learn to map from situations to actions. Working for such a boss sounds pretty\nlame. On the other hand, pleasing such a boss is pretty easy. You just recognize the pattern\nas quickly as possible and imitate the boss’s actions.\nConsidering the opposite situation, it could be frustrating to work for a boss who has no idea\nwhat they want you to do. However, if you plan to be a data scientist, you had better get used\nto it. The boss might just hand you a giant dump of data and tell you to do some data science\nwith it! This sounds vague because it is vague. We call this class of problems unsupervised\nlearning, and the type and number of questions we can ask is limited only by our creativity.\nWe will address unsupervised learning techniques in later chapters. To whet your appetite for\nnow, we describe a few of the following questions you might ask.\n• Can we ﬁnd a small number of prototypes that accurately summarize the data? Given a set\nof photos, can we group them into landscape photos, pictures of dogs, babies, cats, and\nmountain peaks? Likewise, given a collection of users’ browsing activities, can we group\nthem into users with similar behavior? This problem is typically known as clustering.\n• Can we ﬁnd a small number of parameters that accurately capture the relevant properties\nof the data? The trajectories of a ball are well described by velocity, diameter, and\nmass of the ball. Tailors have developed a small number of parameters that describe\nhuman body shape fairly accurately for the purpose of ﬁtting clothes. These problems\nare referred to as subspace estimation. If the dependence is linear, it is called principal\ncomponent analysis.\n• Is there a representation of (arbitrarily structured) objects in Euclidean space such that\n\n17\nKinds of Machine Learning Problems\nsymbolic properties can be well matched? This can be used to describe entities and\ntheir relations, such as “Rome” −“Italy” + “France” = “Paris”.\n• Is there a description of the root causes of much of the data that we observe? For instance,\nif we have demographic data about house prices, pollution, crime, location, education,\nand salaries, can we discover how they are related simply based on empirical data? The\nﬁelds concerned with causality and probabilistic graphical models tackle such questions.\n• Another important and exciting recent development in unsupervised learning is the advent\nof deep generative models. These models estimate the density of the data, either explic-\nitly or implicitly. Once trained, we can use a generative model either to score examples\naccording to how likely they are, or to sample synthetic examples from the learned\ndistribution. Early deep learning breakthroughs in generative modeling came with the\ninvention of variational autoencoders (Kingma and Welling, 2014, Rezende et al., 2014)\nand continued with the development of generative adversarial networks (Goodfellow et\nal., 2014). More recent advances include normalizing ﬂows (Dinh et al., 2014, Dinh et\nal., 2017) and diﬀusion models (Ho et al., 2020, Sohl-Dickstein et al., 2015, Song and\nErmon, 2019, Song et al., 2021).\nA further development in unsupervised learning has been the rise of self-supervised learning,\ntechniques that leverage some aspect of the unlabeled data to provide supervision. For text,\nwe can train models to “ﬁll in the blanks” by predicting randomly masked words using their\nsurrounding words (contexts) in big corpora without any labeling eﬀort (Devlin et al., 2018)!\nFor images, we may train models to tell the relative position between two cropped regions\nof the same image (Doersch et al., 2015), to predict an occluded part of an image based\non the remaining portions of the image, or to predict whether two examples are perturbed\nversions of the same underlying image. Self-supervised models often learn representations\nthat are subsequently leveraged by ﬁne-tuning the resulting models on some downstream task\nof interest.\n1.3.3 Interacting with an Environment\nSo far, we have not discussed where data actually comes from, or what actually happens\nwhen a machine learning model generates an output. That is because supervised learning\nand unsupervised learning do not address these issues in a very sophisticated way. In each\ncase, we grab a big pile of data upfront, then set our pattern recognition machines in motion\nwithout ever interacting with the environment again. Because all the learning takes place\nafter the algorithm is disconnected from the environment, this is sometimes called oﬄine\nlearning. For example, supervised learning assumes the simple interaction pattern depicted\nin Fig. 1.3.6.\nThis simplicity of oﬄine learning has its charms. The upside is that we can worry about pat-\ntern recognition in isolation, with no concern about complications arising from interactions\nwith a dynamic environment. But this problem formulation is limiting. If you grew up read-\ning Asimov’s Robot novels, then you probably picture artiﬁcially intelligent agents capable\nnot only of making predictions, but also of taking actions in the world. We want to think\n\n18\nIntroduction\nt\nFig. 1.3.6\nCollecting data for supervised learning from an environment.\nabout intelligent agents, not just predictive models. This means that we need to think about\nchoosing actions, not just making predictions. In contrast to mere predictions, actions actu-\nally impact the environment. If we want to train an intelligent agent, we must account for the\nway its actions might impact the future observations of the agent, and so oﬄine learning is\ninappropriate.\nConsidering the interaction with an environment opens a whole set of new modeling ques-\ntions. The following are just a few examples.\n• Does the environment remember what we did previously?\n• Does the environment want to help us, e.g., a user reading text into a speech recognizer?\n• Does the environment want to beat us, e.g., spammers adapting their emails to evade spam\nﬁlters?\n• Does the environment have shifting dynamics? For example, would future data always\nresemble the past or would the patterns change over time, either naturally or in response\nto our automated tools?\nThese questions raise the problem of distribution shift, where training and test data are dif-\nferent. An example of this, that many of us may have met, is when taking exams written by\na lecturer, while the homework was composed by their teaching assistants. Next, we brieﬂy\ndescribe reinforcement learning, a rich framework for posing learning problems in which an\nagent interacts with an environment.\n1.3.4 Reinforcement Learning\nIf you are interested in using machine learning to develop an agent that interacts with an\nenvironment and takes actions, then you are probably going to wind up focusing on rein-\nforcement learning. This might include applications to robotics, to dialogue systems, and\neven to developing artiﬁcial intelligence (AI) for video games. Deep reinforcement learning,\nwhich applies deep learning to reinforcement learning problems, has surged in popularity.\nThe breakthrough deep Q-network, that beat humans at Atari games using only the visual\ninput (Mnih et al., 2015), and the AlphaGo program, which dethroned the world champion\nat the board game Go (Silver et al., 2016), are two prominent examples.\n\n19\nKinds of Machine Learning Problems\nReinforcement learning gives a very general statement of a problem in which an agent inter-\nacts with an environment over a series of time steps. At each time step, the agent receives\nsome observation from the environment and must choose an action that is subsequently trans-\nmitted back to the environment via some mechanism (sometimes called an actuator), when,\nafter each loop, the agent receives a reward from the environment. This process is illustrated\nin Fig. 1.3.7. The agent then receives a subsequent observation, and chooses a subsequent\naction, and so on. The behavior of a reinforcement learning agent is governed by a policy. In\nbrief, a policy is just a function that maps from observations of the environment to actions.\nThe goal of reinforcement learning is to produce good policies.\nt\nFig. 1.3.7\nThe interaction between reinforcement learning and an environment.\nIt is hard to overstate the generality of the reinforcement learning framework. For example,\nsupervised learning can be recast as reinforcement learning. Say we had a classiﬁcation prob-\nlem. We could create a reinforcement learning agent with one action corresponding to each\nclass. We could then create an environment which gave a reward that was exactly equal to\nthe loss function from the original supervised learning problem.\nFurther, reinforcement learning can also address many problems that supervised learning\ncannot. For example, in supervised learning, we always expect that the training input comes\nassociated with the correct label. But in reinforcement learning, we do not assume that, for\neach observation the environment tells us the optimal action. In general, we just get some re-\nward. Moreover, the environment may not even tell us which actions led to the reward.\nConsider the game of chess. The only real reward signal comes at the end of the game when\nwe either win, earning a reward of, say, 1, or when we lose, receiving a reward of, say,\n−1. So reinforcement learners must deal with the credit assignment problem: determining\nwhich actions to credit or blame for an outcome. The same goes for an employee who gets\na promotion on October 11. That promotion likely reﬂects a number of well-chosen actions\nover the previous year. Getting promoted in the future requires ﬁguring out which actions\nalong the way led to the earlier promotions.\nReinforcement learners may also have to deal with the problem of partial observability. That\nis, the current observation might not tell you everything about your current state. Say your\ncleaning robot found itself trapped in one of many identical closets in your house. Rescu-\ning the robot involves inferring its precise location which might require considering earlier\nobservations prior to it entering the closet.\nFinally, at any given point, reinforcement learners might know of one good policy, but there\n\n20\nIntroduction\n23\n24\n25\nmight be many other better policies that the agent has never tried. The reinforcement learner\nmust constantly choose whether to exploit the best (currently) known strategy as a policy, or\nto explore the space of strategies, potentially giving up some short-term reward in exchange\nfor knowledge.\nThe general reinforcement learning problem has a very general setting. Actions aﬀect subse-\nquent observations. Rewards are only observed when they correspond to the chosen actions.\nThe environment may be either fully or partially observed. Accounting for all this complex-\nity at once may be asking too much. Moreover, not every practical problem exhibits all this\ncomplexity. As a result, researchers have studied a number of special cases of reinforcement\nlearning problems.\nWhen the environment is fully observed, we call the reinforcement learning problem a Markov\ndecision process. When the state does not depend on the previous actions, we call it a contextual\nbandit problem. When there is no state, just a set of available actions with initially unknown\nrewards, we have the classic multi-armed bandit problem.\n1.4 Roots\nWe have just reviewed a small subset of problems that machine learning can address. For\na diverse set of machine learning problems, deep learning provides powerful tools for their\nsolution. Although many deep learning methods are recent inventions, the core ideas behind\nlearning from data have been studied for centuries. In fact, humans have held the desire to\nanalyze data and to predict future outcomes for ages, and it is this desire that is at the root\nof much of natural science and mathematics. Two examples are the Bernoulli distribution,\nnamed after Jacob Bernoulli (1655–1705) 23 , and the Gaussian distribution discovered by\nCarl Friedrich Gauss (1777–1855) 24 . Gauss invented, for instance, the least mean squares\nalgorithm, which is still used today for a multitude of problems from insurance calculations to\nmedical diagnostics. Such tools enhanced the experimental approach in the natural sciences—\nfor instance, Ohm’s law relating current and voltage in a resistor is perfectly described by a\nlinear model.\nEven in the middle ages, mathematicians had a keen intuition of estimates. For instance, the\ngeometry book of Jacob Köbel (1460–1533) 25 illustrates averaging the length of 16 adult\nmen’s feet to estimate the typical foot length in the population (Fig. 1.4.1).\nAs a group of individuals exited a church, 16 adult men were asked to line up in a row and\nhave their feet measured. The sum of these measurements was then divided by 16 to obtain\nan estimate for what now is called one foot. This “algorithm” was later improved to deal with\nmisshapen feet; The two men with the shortest and longest feet were sent away, averaging only\nover the remainder. This is among the earliest examples of a trimmed mean estimate.\nStatistics really took oﬀwith the availability and collection of data. One of its pioneers,\n\n21\nRoots\nt\nFig. 1.4.1\nEstimating the length of a foot.\n26\n27\n28\n29\nRonald Fisher (1890–1962) 26 , contributed signiﬁcantly to its theory and also its applica-\ntions in genetics. Many of his algorithms (such as linear discriminant analysis) and concepts\n(such as the Fisher information matrix) still hold a prominent place in the foundations of\nmodern statistics. Even his data resources had a lasting impact. The Iris dataset that Fisher\nreleased in 1936 is still sometimes used to demonstrate machine learning algorithms. Fisher\nwas also a proponent of eugenics, which should remind us that the morally dubious use of\ndata science has as long and enduring a history as its productive use in industry and the natural\nsciences.\nOther inﬂuences for machine learning came from the information theory of Claude Shannon\n(1916–2001) 27 and the theory of computation proposed by Alan Turing (1912–1954) 28 .\nTuring posed the question “can machines think?” in his famous paper Computing Machinery\nand Intelligence (Turing, 1950). Describing what is now known as the Turing test, he pro-\nposed that a machine can be considered intelligent if it is diﬃcult for a human evaluator to\ndistinguish between the replies from a machine and those of a human, based purely on textual\ninteractions.\nFurther inﬂuences came from neuroscience and psychology. After all, humans clearly exhibit\nintelligent behavior. Many scholars have asked whether one could explain and possibly re-\nverse engineer this capacity. One of the ﬁrst biologically inspired algorithms was formulated\nby Donald Hebb (1904–1985)29. In his groundbreaking book The Organization of Behavior\n\n22\nIntroduction\n(Hebb, 1949), he posited that neurons learn by positive reinforcement. This became known\nas the Hebbian learning rule. These ideas inspired later work, such as Rosenblatt’s perceptron\nlearning algorithm, and laid the foundations of many stochastic gradient descent algorithms\nthat underpin deep learning today: reinforce desirable behavior and diminish undesirable be-\nhavior to obtain good settings of the parameters in a neural network.\nBiological inspiration is what gave neural networks their name. For over a century (dating\nback to the models of Alexander Bain, 1873, and James Sherrington, 1890), researchers\nhave tried to assemble computational circuits that resemble networks of interacting neurons.\nOver time, the interpretation of biology has become less literal, but the name stuck. At its\nheart lie a few key principles that can be found in most networks today:\n• The alternation of linear and nonlinear processing units, often referred to as layers.\n• The use of the chain rule (also known as backpropagation) for adjusting parameters in the\nentire network at once.\nAfter initial rapid progress, research in neural networks languished from around 1995 un-\ntil 2005. This was mainly due to two reasons. First, training a network is computationally\nvery expensive. While random-access memory was plentiful at the end of the past century,\ncomputational power was scarce. Second, datasets were relatively small. In fact, Fisher’s Iris\ndataset from 1936 was still a popular tool for testing the eﬃcacy of algorithms. The MNIST\ndataset with its 60,000 handwritten digits was considered huge.\nGiven the scarcity of data and computation, strong statistical tools such as kernel methods,\ndecision trees, and graphical models proved empirically superior in many applications. More-\nover, unlike neural networks, they did not require weeks to train and provided predictable\nresults with strong theoretical guarantees.\n1.5 The Road to Deep Learning\nMuch of this changed with the availability of massive amounts of data, thanks to the World\nWide Web, the advent of companies serving hundreds of millions of users online, a dissem-\nination of low-cost, high-quality sensors, inexpensive data storage (Kryder’s law), and cheap\ncomputation (Moore’s law). In particular, the landscape of computation in deep learning was\nrevolutionized by advances in GPUs that were originally engineered for computer gaming.\nSuddenly algorithms and models that seemed computationally infeasible were within reach.\nThis is best illustrated in Table 1.5.1.\nTable 1.5.1: Dataset vs. computer memory and computational power\n\n23\nThe Road to Deep Learning\nDecade Dataset\nMem-\nory\nFloating point calculations per\nsecond\n1970\n100 (Iris)\n1 KB\n100 KF (Intel 8080)\n1980\n1 K (house prices in Boston)\n100\nKB\n1 MF (Intel 80186)\n1990\n10 K (optical character recog-\nnition)\n10 MB\n10 MF (Intel 80486)\n2000\n10 M (web pages)\n100\nMB\n1 GF (Intel Core)\n2010\n10 G (advertising)\n1 GB\n1 TF (NVIDIA C2050)\n2020\n1 T (social network)\n100\nGB\n1 PF (NVIDIA DGX-2)\nNote that random-access memory has not kept pace with the growth in data. At the same\ntime, increases in computational power have outpaced the growth in datasets. This means\nthat statistical models need to become more memory eﬃcient, and so they are free to spend\nmore computer cycles optimizing parameters, thanks to the increased compute budget. Con-\nsequently, the sweet spot in machine learning and statistics moved from (generalized) linear\nmodels and kernel methods to deep neural networks. This is also one of the reasons why\nmany of the mainstays of deep learning, such as multilayer perceptrons (McCulloch and\nPitts, 1943), convolutional neural networks (LeCun et al., 1998), long short-term memory\n(Hochreiter and Schmidhuber, 1997), and Q-Learning (Watkins and Dayan, 1992), were\nessentially “rediscovered” in the past decade, after lying comparatively dormant for consid-\nerable time.\nThe recent progress in statistical models, applications, and algorithms has sometimes been\nlikened to the Cambrian explosion: a moment of rapid progress in the evolution of species.\nIndeed, the state of the art is not just a mere consequence of available resources applied to\ndecades-old algorithms. Note that the list of ideas below barely scratches the surface of what\nhas helped researchers achieve tremendous progress over the past decade.\n• Novel methods for capacity control, such as dropout (Srivastava et al., 2014), have helped\nto mitigate overﬁtting. Here, noise is injected (Bishop, 1995) throughout the neural net-\nwork during training.\n• Attention mechanisms solved a second problem that had plagued statistics for over a cen-\ntury: how to increase the memory and complexity of a system without increasing the\nnumber of learnable parameters. Researchers found an elegant solution by using what\ncan only be viewed as a learnable pointer structure (Bahdanau et al., 2014). Rather than\nhaving to remember an entire text sequence, e.g., for machine translation in a ﬁxed-\ndimensional representation, all that needed to be stored was a pointer to the interme-\ndiate state of the translation process. This allowed for signiﬁcantly increased accuracy\nfor long sequences, since the model no longer needed to remember the entire sequence\nbefore commencing the generation of a new one.\n• Built solely on attention mechanisms, the Transformer architecture (Vaswani et al., 2017)\n\n24\nIntroduction\n30\nhas demonstrated superior scaling behavior: it performs better with an increase in dataset\nsize, model size, and amount of training compute (Kaplan et al., 2020). This architecture\nhas demonstrated compelling success in a wide range of areas, such as natural language\nprocessing (Brown et al., 2020, Devlin et al., 2018), computer vision (Dosovitskiy et al.,\n2021, Liu et al., 2021), speech recognition (Gulati et al., 2020), reinforcement learning\n(Chen et al., 2021), and graph neural networks (Dwivedi and Bresson, 2020). For ex-\nample, a single Transformer pretrained on modalities as diverse as text, images, joint\ntorques, and button presses can play Atari, caption images, chat, and control a robot\n(Reed et al., 2022).\n• Modeling probabilities of text sequences, language models can predict text given other text.\nScaling up the data, model, and compute has unlocked a growing number of capabilities\nof language models to perform desired tasks via human-like text generation based on\ninput text (Anil et al., 2023, Brown et al., 2020, Chowdhery et al., 2022, Hoﬀmann et al.,\n2022, OpenAI, 2023, Rae et al., 2021, Touvron et al., 2023a, Touvron et al., 2023b). For\ninstance, aligning language models with human intent (Ouyang et al., 2022), OpenAI’s\nChatGPT 30 allows users to interact with it in a conversational way to solve problems,\nsuch as code debugging and creative writing.\n• Multi-stage designs, e.g., via the memory networks (Sukhbaatar et al., 2015) and the neural\nprogrammer-interpreter (Reed and De Freitas, 2015) permitted statistical modelers to\ndescribe iterative approaches to reasoning. These tools allow for an internal state of the\ndeep neural network to be modiﬁed repeatedly, thus carrying out subsequent steps in a\nchain of reasoning, just as a processor can modify memory for a computation.\n• A key development in deep generative modeling was the invention of generative adver-\nsarial networks (Goodfellow et al., 2014). Traditionally, statistical methods for density\nestimation and generative models focused on ﬁnding proper probability distributions\nand (often approximate) algorithms for sampling from them. As a result, these algo-\nrithms were largely limited by the lack of ﬂexibility inherent in the statistical models.\nThe crucial innovation in generative adversarial networks was to replace the sampler by\nan arbitrary algorithm with diﬀerentiable parameters. These are then adjusted in such a\nway that the discriminator (eﬀectively a two-sample test) cannot distinguish fake from\nreal data. Through the ability to use arbitrary algorithms to generate data, density es-\ntimation was opened up to a wide variety of techniques. Examples of galloping zebras\n(Zhu et al., 2017) and of fake celebrity faces (Karras et al., 2017) are each testimony to\nthis progress. Even amateur doodlers can produce photorealistic images just based on\nsketches describing the layout of a scene (Park et al., 2019).\n• Furthermore, while the diﬀusion process gradually adds random noise to data samples, dif-\nfusion models (Ho et al., 2020, Sohl-Dickstein et al., 2015) learn the denoising process\nto gradually construct data samples from random noise, reversing the diﬀusion process.\nThey have started to replace generative adversarial networks in more recent deep gen-\nerative models, such as in DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al.,\n2022) for creative art and image generation based on text descriptions.\n• In many cases, a single GPU is insuﬃcient for processing the large amounts of data avail-\n\n25\nSuccess Stories\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\nable for training. Over the past decade the ability to build parallel and distributed training\nalgorithms has improved signiﬁcantly. One of the key challenges in designing scalable\nalgorithms is that the workhorse of deep learning optimization, stochastic gradient de-\nscent, relies on relatively small minibatches of data to be processed. At the same time,\nsmall batches limit the eﬃciency of GPUs. Hence, training on 1,024 GPUs with a mini-\nbatch size of, say, 32 images per batch amounts to an aggregate minibatch of about\n32,000 images. Work, ﬁrst by Li (2017) and subsequently by You et al. (2017) and\nJia et al. (2018) pushed the size up to 64,000 observations, reducing training time for\nthe ResNet-50 model on the ImageNet dataset to less than 7 minutes. By comparison,\ntraining times were initially of the order of days.\n• The ability to parallelize computation has also contributed to progress in reinforcement\nlearning. This has led to signiﬁcant progress in computers achieving superhuman per-\nformance on tasks like Go, Atari games, Starcraft, and in physics simulations (e.g., using\nMuJoCo) where environment simulators are available. See, e.g., Silver et al. (2016) for\na description of such achievements in AlphaGo. In a nutshell, reinforcement learning\nworks best if plenty of (state, action, reward) tuples are available. Simulation provides\nsuch an avenue.\n• Deep learning frameworks have played a crucial role in disseminating ideas. The ﬁrst gen-\neration of open-source frameworks for neural network modeling consisted of Caﬀe 31\n, Torch 32 , and Theano 33 . Many seminal papers were written using these tools. These\nhave now been superseded by TensorFlow 34 (often used via its high-level API Keras\n35 ), CNTK36 , Caﬀe 237 , and Apache MXNet38 . The third generation of frameworks\nconsists of so-called imperative tools for deep learning, a trend that was arguably ignited\nby Chainer39, which used a syntax similar to Python NumPy to describe models. This\nidea was adopted by both PyTorch40, the Gluon API41 of MXNet, and JAX42.\nThe division of labor between system researchers building better tools and statistical modelers\nbuilding better neural networks has greatly simpliﬁed things. For instance, training a linear\nlogistic regression model used to be a nontrivial homework problem, worthy to give to new\nmachine learning Ph.D. students at Carnegie Mellon University in 2014. By now, this task\ncan be accomplished with under 10 lines of code, putting it ﬁrmly within the reach of any\nprogrammer.\n1.6 Success Stories\nArtiﬁcial intelligence has a long history of delivering results that would be diﬃcult to accom-\nplish otherwise. For instance, mail sorting systems using optical character recognition have\nbeen deployed since the 1990s. This is, after all, the source of the famous MNIST dataset of\nhandwritten digits. The same applies to reading checks for bank deposits and scoring cred-\nitworthiness of applicants. Financial transactions are checked for fraud automatically. This\n\n26\nIntroduction\nforms the backbone of many e-commerce payment systems, such as PayPal, Stripe, AliPay,\nWeChat, Apple, Visa, and MasterCard. Computer programs for chess have been competi-\ntive for decades. Machine learning feeds search, recommendation, personalization, and rank-\ning on the Internet. In other words, machine learning is pervasive, albeit often hidden from\nsight.\nIt is only recently that AI has been in the limelight, mostly due to solutions to problems that\nwere considered intractable previously and that are directly related to consumers. Many of\nsuch advances are attributed to deep learning.\n• Intelligent assistants, such as Apple’s Siri, Amazon’s Alexa, and Google’s assistant, are able\nto respond to spoken requests with a reasonable degree of accuracy. This includes menial\njobs, like turning on light switches, and more complex tasks, such as arranging barber’s\nappointments and oﬀering phone support dialog. This is likely the most noticeable sign\nthat AI is aﬀecting our lives.\n• A key ingredient in digital assistants is their ability to recognize speech accurately. The\naccuracy of such systems has gradually increased to the point of achieving parity with\nhumans for certain applications (Xiong et al., 2018).\n• Object recognition has likewise come a long way. Identifying the object in a picture was\na fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC\nLabs and University of Illinois at Urbana-Champaign achieved a top-ﬁve error rate of\n28% (Lin et al., 2010). By 2017, this error rate was reduced to 2.25% (Hu et al., 2018).\nSimilarly, stunning results have been achieved for identifying birdsong and for diagnos-\ning skin cancer.\n• Prowess in games used to provide a measuring stick for human ability. Starting from TD-\nGammon, a program for playing backgammon using temporal diﬀerence reinforcement\nlearning, algorithmic and computational progress has led to algorithms for a wide range\nof applications. Compared with backgammon, chess has a much more complex state\nspace and set of actions. DeepBlue beat Garry Kasparov using massive parallelism,\nspecial-purpose hardware and eﬃcient search through the game tree (Campbell et al.,\n2002). Go is more diﬃcult still, due to its huge state space. AlphaGo reached human\nparity in 2015, using deep learning combined with Monte Carlo tree sampling (Silver et\nal., 2016). The challenge in Poker was that the state space is large and only partially ob-\nserved (we do not know the opponents’ cards). Libratus exceeded human performance\nin Poker using eﬃciently structured strategies (Brown and Sandholm, 2017).\n• Another indication of progress in AI is the advent of self-driving vehicles. While full au-\ntonomy is not yet within reach, excellent progress has been made in this direction, with\ncompanies such as Tesla, NVIDIA, and Waymo shipping products that enable partial\nautonomy. What makes full autonomy so challenging is that proper driving requires the\nability to perceive, to reason and to incorporate rules into a system. At present, deep\nlearning is used primarily in the visual aspect of these problems. The rest is heavily\ntuned by engineers.\nThis barely scratches the surface of signiﬁcant applications of machine learning. For instance,\n\n27\nThe Essence of Deep Learning\nrobotics, logistics, computational biology, particle physics, and astronomy owe some of their\nmost impressive recent advances at least in parts to machine learning, which is thus becoming\na ubiquitous tool for engineers and scientists.\nFrequently, questions about a coming AI apocalypse and the plausibility of a singularity have\nbeen raised in non-technical articles. The fear is that somehow machine learning systems\nwill become sentient and make decisions, independently of their programmers, that directly\nimpact the lives of humans. To some extent, AI already aﬀects the livelihood of humans in\ndirect ways: creditworthiness is assessed automatically, autopilots mostly navigate vehicles,\ndecisions about whether to grant bail use statistical data as input. More frivolously, we can\nask Alexa to switch on the coﬀee machine.\nFortunately, we are far from a sentient AI system that could deliberately manipulate its human\ncreators. First, AI systems are engineered, trained, and deployed in a speciﬁc, goal-oriented\nmanner. While their behavior might give the illusion of general intelligence, it is a combi-\nnation of rules, heuristics and statistical models that underlie the design. Second, at present,\nthere are simply no tools for artiﬁcial general intelligence that are able to improve themselves,\nreason about themselves, and that are able to modify, extend, and improve their own archi-\ntecture while trying to solve general tasks.\nA much more pressing concern is how AI is being used in our daily lives. It is likely that\nmany routine tasks, currently fulﬁlled by humans, can and will be automated. Farm robots\nwill likely reduce the costs for organic farmers but they will also automate harvesting op-\nerations. This phase of the industrial revolution may have profound consequences for large\nswaths of society, since menial jobs provide much employment in many countries. Further-\nmore, statistical models, when applied without care, can lead to racial, gender, or age bias\nand raise reasonable concerns about procedural fairness if automated to drive consequential\ndecisions. It is important to ensure that these algorithms are used with care. With what we\nknow today, this strikes us as a much more pressing concern than the potential of malevolent\nsuperintelligence for destroying humanity.\n1.7 The Essence of Deep Learning\nThus far, we have talked in broad terms about machine learning. Deep learning is the subset\nof machine learning concerned with models based on many-layered neural networks. It is\ndeep in precisely the sense that its models learn many layers of transformations. While this\nmight sound narrow, deep learning has given rise to a dizzying array of models, techniques,\nproblem formulations, and applications. Many intuitions have been developed to explain the\nbeneﬁts of depth. Arguably, all machine learning has many layers of computation, the ﬁrst\nconsisting of feature processing steps. What diﬀerentiates deep learning is that the operations\nlearned at each of the many layers of representations are learned jointly from data.\nThe problems that we have discussed so far, such as learning from the raw audio signal,\n\n28\nIntroduction\nthe raw pixel values of images, or mapping between sentences of arbitrary lengths and their\ncounterparts in foreign languages, are those where deep learning excels and traditional meth-\nods falter. It turns out that these many-layered models are capable of addressing low-level\nperceptual data in a way that previous tools could not. Arguably the most signiﬁcant com-\nmonality in deep learning methods is end-to-end training. That is, rather than assembling\na system based on components that are individually tuned, one builds the system and then\ntunes their performance jointly. For instance, in computer vision scientists used to separate\nthe process of feature engineering from the process of building machine learning models. The\nCanny edge detector (Canny, 1987) and Lowe’s SIFT feature extractor (Lowe, 2004) reigned\nsupreme for over a decade as algorithms for mapping images into feature vectors. In bygone\ndays, the crucial part of applying machine learning to these problems consisted of coming up\nwith manually-engineered ways of transforming the data into some form amenable to shal-\nlow models. Unfortunately, there is only so much that humans can accomplish by ingenuity\nin comparison with a consistent evaluation over millions of choices carried out automatically\nby an algorithm. When deep learning took over, these feature extractors were replaced by\nautomatically tuned ﬁlters that yielded superior accuracy.\nThus, one key advantage of deep learning is that it replaces not only the shallow models at\nthe end of traditional learning pipelines, but also the labor-intensive process of feature engi-\nneering. Moreover, by replacing much of the domain-speciﬁc preprocessing, deep learning\nhas eliminated many of the boundaries that previously separated computer vision, speech\nrecognition, natural language processing, medical informatics, and other application areas,\nthereby oﬀering a uniﬁed set of tools for tackling diverse problems.\nBeyond end-to-end training, we are experiencing a transition from parametric statistical de-\nscriptions to fully nonparametric models. When data is scarce, one needs to rely on simpli-\nfying assumptions about reality in order to obtain useful models. When data is abundant,\nthese can be replaced by nonparametric models that better ﬁt the data. To some extent, this\nmirrors the progress that physics experienced in the middle of the previous century with the\navailability of computers. Rather than solving by hand parametric approximations of how\nelectrons behave, one can now resort to numerical simulations of the associated partial dif-\nferential equations. This has led to much more accurate models, albeit often at the expense\nof interpretation.\nAnother diﬀerence from previous work is the acceptance of suboptimal solutions, dealing\nwith nonconvex nonlinear optimization problems, and the willingness to try things before\nproving them. This new-found empiricism in dealing with statistical problems, combined\nwith a rapid inﬂux of talent has led to rapid progress in the development of practical algo-\nrithms, albeit in many cases at the expense of modifying and re-inventing tools that existed\nfor decades.\nIn the end, the deep learning community prides itself on sharing tools across academic and\ncorporate boundaries, releasing many excellent libraries, statistical models, and trained net-\nworks as open source. It is in this spirit that the notebooks forming this book are freely\navailable for distribution and use. We have worked hard to lower the barriers of access for\nanyone wishing to learn about deep learning and we hope that our readers will beneﬁt from\nthis.\n\n29\nSummary\n43\n1.8 Summary\nMachine learning studies how computer systems can leverage experience (often data) to im-\nprove performance at speciﬁc tasks. It combines ideas from statistics, data mining, and opti-\nmization. Often, it is used as a means of implementing AI solutions. As a class of machine\nlearning, representational learning focuses on how to automatically ﬁnd the appropriate way\nto represent data. Considered as multi-level representation learning through learning many\nlayers of transformations, deep learning replaces not only the shallow models at the end of\ntraditional machine learning pipelines, but also the labor-intensive process of feature engi-\nneering. Much of the recent progress in deep learning has been triggered by an abundance of\ndata arising from cheap sensors and Internet-scale applications, and by signiﬁcant progress in\ncomputation, mostly through GPUs. Furthermore, the availability of eﬃcient deep learning\nframeworks has made design and implementation of whole system optimization signiﬁcantly\neasier, and this is a key component in obtaining high performance.\n1.9 Exercises\n1. Which parts of code that you are currently writing could be “learned”, i.e., improved by\nlearning and automatically determining design choices that are made in your code? Does\nyour code include heuristic design choices? What data might you need to learn the desired\nbehavior?\n2. Which problems that you encounter have many examples for their solution, yet no speciﬁc\nway for automating them? These may be prime candidates for using deep learning.\n3. Describe the relationships between algorithms, data, and computation. How do charac-\nteristics of the data and the current available computational resources inﬂuence the ap-\npropriateness of various algorithms?\n4. Name some settings where end-to-end training is not currently the default approach but\nwhere it might be useful.\nDiscussions43.\n\n2\nPreliminaries\nTo prepare for your dive into deep learning, you will need a few survival skills: (i) techniques\nfor storing and manipulating data; (ii) libraries for ingesting and preprocessing data from a\nvariety of sources; (iii) knowledge of the basic linear algebraic operations that we apply to\nhigh-dimensional data elements; (iv) just enough calculus to determine which direction to\nadjust each parameter in order to decrease the loss function; (v) the ability to automatically\ncompute derivatives so that you can forget much of the calculus you just learned; (vi) some\nbasic ﬂuency in probability, our primary language for reasoning under uncertainty; and (vii)\nsome aptitude for ﬁnding answers in the oﬃcial documentation when you get stuck.\nIn short, this chapter provides a rapid introduction to the basics that you will need to follow\nmost of the technical content in this book.\n2.1 Data Manipulation\nIn order to get anything done, we need some way to store and manipulate data. Generally,\nthere are two important things we need to do with data: (i) acquire them; and (ii) process\nthem once they are inside the computer. There is no point in acquiring data without some\nway to store it, so to start, let’s get our hands dirty with n-dimensional arrays, which we\nalso call tensors. If you already know the NumPy scientiﬁc computing package, this will\nbe a breeze. For all modern deep learning frameworks, the tensor class (ndarray in MXNet,\nTensor in PyTorch and TensorFlow) resembles NumPy’s ndarray, with a few killer features\nadded. First, the tensor class supports automatic diﬀerentiation. Second, it leverages GPUs\nto accelerate numerical computation, whereas NumPy only runs on CPUs. These properties\nmake neural networks both easy to code and fast to run.\n2.1.1 Getting Started\nTo start, we import the PyTorch library. Note that the package name is torch.\nimport torch\nA tensor represents a (possibly multidimensional) array of numerical values. In the one-\ndimensional case, i.e., when only one axis is needed for the data, a tensor is called a vector.\n30\n\n31\nData Manipulation\nWith two axes, a tensor is called a matrix. With k > 2 axes, we drop the specialized names\nand just refer to the object as a kth-order tensor.\nPyTorch provides a variety of functions for creating new tensors prepopulated with values.\nFor example, by invoking arange(n), we can create a vector of evenly spaced values, start-\ning at 0 (included) and ending at n (not included). By default, the interval size is 1. Unless\notherwise speciﬁed, new tensors are stored in main memory and designated for CPU-based\ncomputation.\nx = torch.arange(12, dtype=torch.float32)\nx\ntensor([ 0.,\n1.,\n2.,\n3.,\n4.,\n5.,\n6.,\n7.,\n8.,\n9., 10., 11.])\nEach of these values is called an element of the tensor. The tensor x contains 12 elements.\nWe can inspect the total number of elements in a tensor via its numel method.\nx.numel()\n12\nWe can access a tensor’s shape (the length along each axis) by inspecting its shape attribute.\nBecause we are dealing with a vector here, the shape contains just a single element and is\nidentical to the size.\nx.shape\ntorch.Size([12])\nWe can change the shape of a tensor without altering its size or values, by invoking reshape.\nFor example, we can transform our vector x whose shape is (12,) to a matrix X with shape (3,\n4). This new tensor retains all elements but reconﬁgures them into a matrix. Notice that the\nelements of our vector are laid out one row at a time and thus x[3] == X[0, 3].\nX = x.reshape(3, 4)\nX\ntensor([[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.]])\nNote that specifying every shape component to reshape is redundant. Because we already\nknow our tensor’s size, we can work out one component of the shape given the rest. For exam-\nple, given a tensor of size n and target shape (h, w), we know that w = n/h. To automatically\n\n32\nPreliminaries\ninfer one component of the shape, we can place a -1 for the shape component that should\nbe inferred automatically. In our case, instead of calling x.reshape(3, 4), we could have\nequivalently called x.reshape(-1, 4) or x.reshape(3, -1).\nPractitioners often need to work with tensors initialized to contain all 0s or 1s. We can con-\nstruct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the zeros function.\ntorch.zeros((2, 3, 4))\ntensor([[[0., 0., 0., 0.],\n[0., 0., 0., 0.],\n[0., 0., 0., 0.]],\n[[0., 0., 0., 0.],\n[0., 0., 0., 0.],\n[0., 0., 0., 0.]]])\nSimilarly, we can create a tensor with all 1s by invoking ones.\ntorch.ones((2, 3, 4))\ntensor([[[1., 1., 1., 1.],\n[1., 1., 1., 1.],\n[1., 1., 1., 1.]],\n[[1., 1., 1., 1.],\n[1., 1., 1., 1.],\n[1., 1., 1., 1.]]])\nWe often wish to sample each element randomly (and independently) from a given probability\ndistribution. For example, the parameters of neural networks are often initialized randomly.\nThe following snippet creates a tensor with elements drawn from a standard Gaussian (nor-\nmal) distribution with mean 0 and standard deviation 1.\ntorch.randn(3, 4)\ntensor([[ 0.3987,\n0.6606,\n1.1238, -0.2449],\n[-0.9343,\n0.8246, -0.8358,\n0.8182],\n[ 1.2168,\n0.0476, -0.9897,\n1.3701]])\nFinally, we can construct tensors by supplying the exact values for each element by supplying\n(possibly nested) Python list(s) containing numerical literals. Here, we construct a matrix with\na list of lists, where the outermost list corresponds to axis 0, and the inner list corresponds to\naxis 1.\ntorch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n\n33\nData Manipulation\ntensor([[2, 1, 4, 3],\n[1, 2, 3, 4],\n[4, 3, 2, 1]])\n2.1.2 Indexing and Slicing\nAs with Python lists, we can access tensor elements by indexing (starting with 0). To access\nan element based on its position relative to the end of the list, we can use negative indexing.\nFinally, we can access whole ranges of indices via slicing (e.g., X[start:stop]), where the\nreturned value includes the ﬁrst index (start) but not the last (stop). Finally, when only\none index (or slice) is speciﬁed for a kth-order tensor, it is applied along axis 0. Thus, in the\nfollowing code, [-1] selects the last row and [1:3] selects the second and third rows.\nX[-1], X[1:3]\n(tensor([ 8.,\n9., 10., 11.]),\ntensor([[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.]]))\nBeyond reading them, we can also write elements of a matrix by specifying indices.\nX[1, 2] = 17\nX\ntensor([[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5., 17.,\n7.],\n[ 8.,\n9., 10., 11.]])\nIf we want to assign multiple elements the same value, we apply the indexing on the left-\nhand side of the assignment operation. For instance, [:2, :] accesses the ﬁrst and second\nrows, where : takes all the elements along axis 1 (column). While we discussed indexing for\nmatrices, this also works for vectors and for tensors of more than two dimensions.\nX[:2, :] = 12\nX\ntensor([[12., 12., 12., 12.],\n[12., 12., 12., 12.],\n[ 8.,\n9., 10., 11.]])\n2.1.3 Operations\n\n34\nPreliminaries\nNow that we know how to construct tensors and how to read from and write to their elements,\nwe can begin to manipulate them with various mathematical operations. Among the most\nuseful of these are the elementwise operations. These apply a standard scalar operation to\neach element of a tensor. For functions that take two tensors as inputs, elementwise operations\napply some standard binary operator on each pair of corresponding elements. We can create\nan elementwise function from any function that maps from a scalar to a scalar.\nIn mathematical notation, we denote such unary scalar operators (taking one input) by the\nsignature f : R →R. This just means that the function maps from any real number onto\nsome other real number. Most standard operators, including unary ones like ex, can be applied\nelementwise.\ntorch.exp(x)\ntensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n162754.7969, 162754.7969, 162754.7969,\n2980.9580,\n8103.0840,\n22026.4648,\n59874.1406])\nLikewise, we denote binary scalar operators, which map pairs of real numbers to a (single)\nreal number via the signature f : R, R →R. Given any two vectors u and v of the same shape,\nand a binary operator f , we can produce a vector c = F(u, v) by setting ci ←f (ui, vi) for all\ni, where ci, ui, and vi are the ith elements of vectors c, u, and v. Here, we produced the vector-\nvalued F : Rd, Rd →Rd by lifting the scalar function to an elementwise vector operation.\nThe common standard arithmetic operators for addition (+), subtraction (-), multiplication\n(*), division (/), and exponentiation (**) have all been lifted to elementwise operations for\nidentically-shaped tensors of arbitrary shape.\nx = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n(tensor([ 3.,\n4.,\n6., 10.]),\ntensor([-1.,\n0.,\n2.,\n6.]),\ntensor([ 2.,\n4.,\n8., 16.]),\ntensor([0.5000, 1.0000, 2.0000, 4.0000]),\ntensor([ 1.,\n4., 16., 64.]))\nIn addition to elementwise computations, we can also perform linear algebraic operations,\nsuch as dot products and matrix multiplications. We will elaborate on these in Section 2.3.\nWe can also concatenate multiple tensors, stacking them end-to-end to form a larger one. We\njust need to provide a list of tensors and tell the system along which axis to concatenate. The\nexample below shows what happens when we concatenate two matrices along rows (axis 0)\ninstead of columns (axis 1). We can see that the ﬁrst output’s axis-0 length (6) is the sum of\nthe two input tensors’ axis-0 lengths (3 + 3); while the second output’s axis-1 length (8) is\nthe sum of the two input tensors’ axis-1 lengths (4 + 4).\n\n35\nData Manipulation\nX = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n(tensor([[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.],\n[ 2.,\n1.,\n4.,\n3.],\n[ 1.,\n2.,\n3.,\n4.],\n[ 4.,\n3.,\n2.,\n1.]]),\ntensor([[ 0.,\n1.,\n2.,\n3.,\n2.,\n1.,\n4.,\n3.],\n[ 4.,\n5.,\n6.,\n7.,\n1.,\n2.,\n3.,\n4.],\n[ 8.,\n9., 10., 11.,\n4.,\n3.,\n2.,\n1.]]))\nSometimes, we want to construct a binary tensor via logical statements. Take X == Y as an\nexample. For each position i, j, if X[i, j] and Y[i, j] are equal, then the corresponding\nentry in the result takes value 1, otherwise it takes value 0.\nX == Y\ntensor([[False,\nTrue, False,\nTrue],\n[False, False, False, False],\n[False, False, False, False]])\nSumming all the elements in the tensor yields a tensor with only one element.\nX.sum()\ntensor(66.)\n2.1.4 Broadcasting\nBy now, you know how to perform elementwise binary operations on two tensors of the same\nshape. Under certain conditions, even when shapes diﬀer, we can still perform elementwise\nbinary operations by invoking the broadcasting mechanism. Broadcasting works according to\nthe following two-step procedure: (i) expand one or both arrays by copying elements along\naxes with length 1 so that after this transformation, the two tensors have the same shape; (ii)\nperform an elementwise operation on the resulting arrays.\na = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1, 2))\na, b\n\n36\nPreliminaries\n(tensor([[0],\n[1],\n[2]]),\ntensor([[0, 1]]))\nSince a and b are 3 × 1 and 1 × 2 matrices, respectively, their shapes do not match up.\nBroadcasting produces a larger 3 × 2 matrix by replicating matrix a along the columns and\nmatrix b along the rows before adding them elementwise.\na + b\ntensor([[0, 1],\n[1, 2],\n[2, 3]])\n2.1.5 Saving Memory\nRunning operations can cause new memory to be allocated to host results. For example, if we\nwrite Y = X + Y, we dereference the tensor that Y used to point to and instead point Y at the\nnewly allocated memory. We can demonstrate this issue with Python’s id() function, which\ngives us the exact address of the referenced object in memory. Note that after we run Y = Y +\nX, id(Y) points to a diﬀerent location. That is because Python ﬁrst evaluates Y + X, allocating\nnew memory for the result and then points Y to this new location in memory.\nbefore = id(Y)\nY = Y + X\nid(Y) == before\nFalse\nThis might be undesirable for two reasons. First, we do not want to run around allocat-\ning memory unnecessarily all the time. In machine learning, we often have hundreds of\nmegabytes of parameters and update all of them multiple times per second. Whenever possi-\nble, we want to perform these updates in place. Second, we might point at the same parameters\nfrom multiple variables. If we do not update in place, we must be careful to update all of these\nreferences, lest we spring a memory leak or inadvertently refer to stale parameters.\nFortunately, performing in-place operations is easy. We can assign the result of an operation\nto a previously allocated array Y by using slice notation: Y[:] = <expression>. To illustrate\nthis concept, we overwrite the values of tensor Z, after initializing it, using zeros_like, to\nhave the same shape as Y.\n\n37\nData Manipulation\nZ = torch.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\nid(Z): 140496176783904\nid(Z): 140496176783904\nIf the value of X is not reused in subsequent computations, we can also use X[:] = X + Y\nor X += Y to reduce the memory overhead of the operation.\nbefore = id(X)\nX += Y\nid(X) == before\nTrue\n2.1.6 Conversion to Other Python Objects\nConverting to a NumPy tensor (ndarray), or vice versa, is easy. The torch tensor and NumPy\narray will share their underlying memory, and changing one through an in-place operation will\nalso change the other.\nA = X.numpy()\nB = torch.from_numpy(A)\ntype(A), type(B)\n(numpy.ndarray, torch.Tensor)\nTo convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s\nbuilt-in functions.\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n(tensor([3.5000]), 3.5, 3.5, 3)\n2.1.7 Summary\nThe tensor class is the main interface for storing and manipulating data in deep learning\nlibraries. Tensors provide a variety of functionalities including construction routines; indexing\n\n38\nPreliminaries\n44\n45\n46\nand slicing; basic mathematics operations; broadcasting; memory-eﬃcient assignment; and\nconversion to and from other Python objects.\n2.1.8 Exercises\n1. Run the code in this section. Change the conditional statement X == Y to X < Y or X >\nY, and then see what kind of tensor you can get.\n2. Replace the two tensors that operate by element in the broadcasting mechanism with other\nshapes, e.g., 3-dimensional tensors. Is the result the same as expected?\nDiscussions44.\n2.2 Data Preprocessing\nSo far, we have been working with synthetic data that arrived in ready-made tensors. However,\nto apply deep learning in the wild we must extract messy data stored in arbitrary formats, and\npreprocess it to suit our needs. Fortunately, the pandas library45 can do much of the heavy\nlifting. This section, while no substitute for a proper pandas tutorial46, will give you a crash\ncourse on some of the most common routines.\n2.2.1 Reading the Dataset\nComma-separated values (CSV) ﬁles are ubiquitous for the storing of tabular (spreadsheet-\nlike) data. In them, each line corresponds to one record and consists of several (comma-\nseparated) ﬁelds, e.g., “Albert Einstein,March 14 1879,Ulm,Federal polytechnic school,ﬁeld\nof gravitational physics”. To demonstrate how to load CSV ﬁles with pandas, we create a\nCSV ﬁle below ../data/house_tiny.csv. This ﬁle represents a dataset of homes, where\neach row corresponds to a distinct home and the columns correspond to the number of rooms\n(NumRooms), the roof type (RoofType), and the price (Price).\nimport os\nos.makedirs(os.path.join('..', 'data'), exist_ok=True)\ndata_file = os.path.join('..', 'data', 'house_tiny.csv')\nwith open(data_file, 'w') as f:\nf.write('''NumRooms,RoofType,Price\nNA,NA,127500\n2,NA,106000\n4,Slate,178100\nNA,NA,140000''')\nNow let’s import pandas and load the dataset with read_csv.\n\n39\nData Preprocessing\nimport pandas as pd\ndata = pd.read_csv(data_file)\nprint(data)\nNumRooms RoofType\nPrice\n0\nNaN\nNaN\n127500\n1\n2.0\nNaN\n106000\n2\n4.0\nSlate\n178100\n3\nNaN\nNaN\n140000\n2.2.2 Data Preparation\nIn supervised learning, we train models to predict a designated target value, given some set of\ninput values. Our ﬁrst step in processing the dataset is to separate out columns corresponding\nto input versus target values. We can select columns either by name or via integer-location\nbased indexing (iloc).\nYou might have noticed that pandas replaced all CSV entries with value NA with a special NaN\n(not a number) value. This can also happen whenever an entry is empty, e.g., “3,,,270000”.\nThese are called missing values and they are the “bed bugs” of data science, a persistent\nmenace that you will confront throughout your career. Depending upon the context, missing\nvalues might be handled either via imputation or deletion. Imputation replaces missing val-\nues with estimates of their values while deletion simply discards either those rows or those\ncolumns that contain missing values.\nHere are some common imputation heuristics. For categorical input ﬁelds, we can treat NaN\nas a category. Since the RoofType column takes values Slate and NaN, pandas can convert\nthis column into two columns RoofType_Slate and RoofType_nan. A row whose roof type\nis Slate will set values of RoofType_Slate and RoofType_nan to 1 and 0, respectively.\nThe converse holds for a row with a missing RoofType value.\ninputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\ninputs = pd.get_dummies(inputs, dummy_na=True)\nprint(inputs)\nNumRooms\nRoofType_Slate\nRoofType_nan\n0\nNaN\nFalse\nTrue\n1\n2.0\nFalse\nTrue\n2\n4.0\nTrue\nFalse\n3\nNaN\nFalse\nTrue\nFor missing numerical values, one common heuristic is to replace the NaN entries with the\nmean value of the corresponding column.\n\n40\nPreliminaries\n47\n48\n49\ninputs = inputs.fillna(inputs.mean())\nprint(inputs)\nNumRooms\nRoofType_Slate\nRoofType_nan\n0\n3.0\nFalse\nTrue\n1\n2.0\nFalse\nTrue\n2\n4.0\nTrue\nFalse\n3\n3.0\nFalse\nTrue\n2.2.3 Conversion to the Tensor Format\nNow that all the entries in inputs and targets are numerical, we can load them into a tensor\n(recall Section 2.1).\nimport torch\nX = torch.tensor(inputs.to_numpy(dtype=float))\ny = torch.tensor(targets.to_numpy(dtype=float))\nX, y\n(tensor([[3., 0., 1.],\n[2., 0., 1.],\n[4., 1., 0.],\n[3., 0., 1.]], dtype=torch.float64),\ntensor([127500., 106000., 178100., 140000.], dtype=torch.float64))\n2.2.4 Discussion\nYou now know how to partition data columns, impute missing variables, and load pandas\ndata into tensors. In Section 5.7, you will pick up some more data processing skills. While\nthis crash course kept things simple, data processing can get hairy. For example, rather than\narriving in a single CSV ﬁle, our dataset might be spread across multiple ﬁles extracted from\na relational database. For instance, in an e-commerce application, customer addresses might\nlive in one table and purchase data in another. Moreover, practitioners face myriad data types\nbeyond categorical and numeric, for example, text strings, images, audio data, and point\nclouds. Oftentimes, advanced tools and eﬃcient algorithms are required in order to prevent\ndata processing from becoming the biggest bottleneck in the machine learning pipeline. These\nproblems will arise when we get to computer vision and natural language processing. Finally,\nwe must pay attention to data quality. Real-world datasets are often plagued by outliers, faulty\nmeasurements from sensors, and recording errors, which must be addressed before feeding\nthe data into any model. Data visualization tools such as seaborn47, Bokeh48, or matplotlib49\ncan help you to manually inspect the data and develop intuitions about the type of problems\nyou may need to address.\n\n41\nLinear Algebra\n50\n51\n52\n53\n54\n2.2.5 Exercises\n1. Try loading datasets, e.g., Abalone from the UCI Machine Learning Repository 50 and\ninspect their properties. What fraction of them has missing values? What fraction of the\nvariables is numerical, categorical, or text?\n2. Try indexing and selecting data columns by name rather than by column number. The\npandas documentation on indexing51 has further details on how to do this.\n3. How large a dataset do you think you could load this way? What might be the limitations?\nHint: consider the time to read the data, representation, processing, and memory footprint.\nTry this out on your laptop. What happens if you try it out on a server?\n4. How would you deal with data that has a very large number of categories? What if the\ncategory labels are all unique? Should you include the latter?\n5. What alternatives to pandas can you think of? How about loading NumPy tensors from a\nﬁle52? Check out Pillow53, the Python Imaging Library.\nDiscussions54.\n2.3 Linear Algebra\nBy now, we can load datasets into tensors and manipulate these tensors with basic math-\nematical operations. To start building sophisticated models, we will also need a few tools\nfrom linear algebra. This section oﬀers a gentle introduction to the most essential concepts,\nstarting from scalar arithmetic and ramping up to matrix multiplication.\nimport torch\n2.3.1 Scalars\nMost everyday mathematics consists of manipulating numbers one at a time. Formally, we\ncall these values scalars. For example, the temperature in Palo Alto is a balmy 72 degrees\nFahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the ex-\npression c = 5\n9( f −32), setting f to 72. In this equation, the values 5, 9, and 32 are constant\nscalars. The variables c and f in general represent unknown scalars.\nWe denote scalars by ordinary lower-cased letters (e.g., x, y, and z) and the space of all\n(continuous) real-valued scalars by R. For expedience, we will skip past rigorous deﬁnitions\nof spaces: just remember that the expression x ∈R is a formal way to say that x is a real-\nvalued scalar. The symbol ∈(pronounced “in”) denotes membership in a set. For example,\nx, y ∈{0, 1} indicates that x and y are variables that can only take values 0 or 1.\n\n42\nPreliminaries\nScalars are implemented as tensors that contain only one element. Below, we assign two\nscalars and perform the familiar addition, multiplication, division, and exponentiation oper-\nations.\nx = torch.tensor(3.0)\ny = torch.tensor(2.0)\nx + y, x * y, x / y, x**y\n(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))\n2.3.2 Vectors\nFor current purposes, you can think of a vector as a ﬁxed-length array of scalars. As with their\ncode counterparts, we call these scalars the elements of the vector (synonyms include entries\nand components). When vectors represent examples from real-world datasets, their values\nhold some real-world signiﬁcance. For example, if we were training a model to predict the\nrisk of a loan defaulting, we might associate each applicant with a vector whose components\ncorrespond to quantities like their income, length of employment, or number of previous\ndefaults. If we were studying the risk of heart attack, each vector might represent a patient and\nits components might correspond to their most recent vital signs, cholesterol levels, minutes of\nexercise per day, etc. We denote vectors by bold lowercase letters, (e.g., x, y, and z).\nVectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary\nlengths, subject to memory limitations. Caution: in Python, as in most programming lan-\nguages, vector indices start at 0, also known as zero-based indexing, whereas in linear algebra\nsubscripts begin at 1 (one-based indexing).\nx = torch.arange(3)\nx\ntensor([0, 1, 2])\nWe can refer to an element of a vector by using a subscript. For example, x2 denotes the\nsecond element of x. Since x2 is a scalar, we do not bold it. By default, we visualize vectors\nby stacking their elements vertically.\nx =\n\nx1\n...\nxn\n\n,\n(2.3.1)\nHere x1, . . ., xn are elements of the vector. Later on, we will distinguish between such column\nvectors and row vectors whose elements are stacked horizontally. Recall that we access a\ntensor’s elements via indexing.\n\n43\nLinear Algebra\nx[2]\ntensor(2)\nTo indicate that a vector contains n elements, we write x ∈Rn. Formally, we call n the\ndimensionality of the vector. In code, this corresponds to the tensor’s length, accessible via\nPython’s built-in len function.\nlen(x)\n3\nWe can also access the length via the shape attribute. The shape is a tuple that indicates\na tensor’s length along each axis. Tensors with just one axis have shapes with just one ele-\nment.\nx.shape\ntorch.Size([3])\nOftentimes, the word “dimension” gets overloaded to mean both the number of axes and the\nlength along a particular axis. To avoid this confusion, we use order to refer to the number\nof axes and dimensionality exclusively to refer to the number of components.\n2.3.3 Matrices\nJust as scalars are 0th-order tensors and vectors are 1st-order tensors, matrices are 2nd-order\ntensors. We denote matrices by bold capital letters (e.g., X, Y, and Z), and represent them in\ncode by tensors with two axes. The expression A ∈Rm×n indicates that a matrix A contains\nm × n real-valued scalars, arranged as m rows and n columns. When m = n, we say that a\nmatrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual\nelement, we subscript both the row and column indices, e.g., aij is the value that belongs to\nA’s ith row and jth column:\nA =\n\na11\na12\n· · ·\na1n\na21\na22\n· · ·\na2n\n...\n...\n...\n...\nam1\nam2\n· · ·\namn\n\n.\n(2.3.2)\nIn code, we represent a matrix A ∈Rm×n by a 2nd-order tensor with shape (m, n). We can\nconvert any appropriately sized m×n tensor into an m×n matrix by passing the desired shape\nto reshape:\n\n44\nPreliminaries\nA = torch.arange(6).reshape(3, 2)\nA\ntensor([[0, 1],\n[2, 3],\n[4, 5]])\nSometimes we want to ﬂip the axes. When we exchange a matrix’s rows and columns, the\nresult is called its transpose. Formally, we signify a matrix A’s transpose by A⊤and if B =\nA⊤, then bij = aji for all i and j. Thus, the transpose of an m × n matrix is an n × m\nmatrix:\nA⊤=\n\na11\na21\n. . .\nam1\na12\na22\n. . .\nam2\n...\n...\n...\n...\na1n\na2n\n. . .\namn\n\n.\n(2.3.3)\nIn code, we can access any matrix’s transpose as follows:\nA.T\ntensor([[0, 2, 4],\n[1, 3, 5]])\nSymmetric matrices are the subset of square matrices that are equal to their own transposes:\nA = A⊤. The following matrix is symmetric:\nA = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nA == A.T\ntensor([[True, True, True],\n[True, True, True],\n[True, True, True]])\nMatrices are useful for representing datasets. Typically, rows correspond to individual records\nand columns correspond to distinct attributes.\n2.3.4 Tensors\nWhile you can go far in your machine learning journey with only scalars, vectors, and ma-\ntrices, eventually you may need to work with higher-order tensors. Tensors give us a generic\nway of describing extensions to nth-order arrays. We call software objects of the tensor class\n“tensors” precisely because they too can have arbitrary numbers of axes. While it may be\nconfusing to use the word tensor for both the mathematical object and its realization in code,\n\n45\nLinear Algebra\nour meaning should usually be clear from context. We denote general tensors by capital let-\nters with a special font face (e.g., X, Y, and Z) and their indexing mechanism (e.g., xijk and\n[X]1,2i−1,3) follows naturally from that of matrices.\nTensors will become more important when we start working with images. Each image arrives\nas a 3rd-order tensor with axes corresponding to the height, width, and channel. At each\nspatial location, the intensities of each color (red, green, and blue) are stacked along the\nchannel. Furthermore, a collection of images is represented in code by a 4th-order tensor,\nwhere distinct images are indexed along the ﬁrst axis. Higher-order tensors are constructed,\nas were vectors and matrices, by growing the number of shape components.\ntorch.arange(24).reshape(2, 3, 4)\ntensor([[[ 0,\n1,\n2,\n3],\n[ 4,\n5,\n6,\n7],\n[ 8,\n9, 10, 11]],\n[[12, 13, 14, 15],\n[16, 17, 18, 19],\n[20, 21, 22, 23]]])\n2.3.5 Basic Properties of Tensor Arithmetic\nScalars, vectors, matrices, and higher-order tensors all have some handy properties. For ex-\nample, elementwise operations produce outputs that have the same shape as their operands.\nA = torch.arange(6, dtype=torch.float32).reshape(2, 3)\nB = A.clone()\n# Assign a copy of A to B by allocating new memory\nA, A + B\n(tensor([[0., 1., 2.],\n[3., 4., 5.]]),\ntensor([[ 0.,\n2.,\n4.],\n[ 6.,\n8., 10.]]))\nThe elementwise product of two matrices is called their Hadamard product (denoted ⊙). We\ncan spell out the entries of the Hadamard product of two matrices A, B ∈Rm×n:\nA ⊙B =\n\na11b11\na12b12\n. . .\na1nb1n\na21b21\na22b22\n. . .\na2nb2n\n...\n...\n...\n...\nam1bm1\nam2bm2\n. . .\namnbmn\n\n.\n(2.3.4)\nA * B\n\n46\nPreliminaries\ntensor([[ 0.,\n1.,\n4.],\n[ 9., 16., 25.]])\nAdding or multiplying a scalar and a tensor produces a result with the same shape as the orig-\ninal tensor. Here, each element of the tensor is added to (or multiplied by) the scalar.\na = 2\nX = torch.arange(24).reshape(2, 3, 4)\na + X, (a * X).shape\n(tensor([[[ 2,\n3,\n4,\n5],\n[ 6,\n7,\n8,\n9],\n[10, 11, 12, 13]],\n[[14, 15, 16, 17],\n[18, 19, 20, 21],\n[22, 23, 24, 25]]]),\ntorch.Size([2, 3, 4]))\n2.3.6 Reduction\nOften, we wish to calculate the sum of a tensor’s elements. To express the sum of the elements\nin a vector x of length n, we write ∑n\ni=1 xi. There is a simple function for it:\nx = torch.arange(3, dtype=torch.float32)\nx, x.sum()\n(tensor([0., 1., 2.]), tensor(3.))\nTo express sums over the elements of tensors of arbitrary shape, we simply sum over all\nits axes. For example, the sum of the elements of an m × n matrix A could be written\n∑m\ni=1\n∑n\nj=1 aij.\nA.shape, A.sum()\n(torch.Size([2, 3]), tensor(15.))\nBy default, invoking the sum function reduces a tensor along all of its axes, eventually pro-\nducing a scalar. Our libraries also allow us to specify the axes along which the tensor should\nbe reduced. To sum over all elements along the rows (axis 0), we specify axis=0 in sum.\nSince the input matrix reduces along axis 0 to generate the output vector, this axis is missing\nfrom the shape of the output.\n\n47\nLinear Algebra\nA.shape, A.sum(axis=0).shape\n(torch.Size([2, 3]), torch.Size([3]))\nSpecifying axis=1 will reduce the column dimension (axis 1) by summing up elements of\nall the columns.\nA.shape, A.sum(axis=1).shape\n(torch.Size([2, 3]), torch.Size([2]))\nReducing a matrix along both rows and columns via summation is equivalent to summing up\nall the elements of the matrix.\nA.sum(axis=[0, 1]) == A.sum()\n# Same as A.sum()\ntensor(True)\nA related quantity is the mean, also called the average. We calculate the mean by dividing\nthe sum by the total number of elements. Because computing the mean is so common, it gets\na dedicated library function that works analogously to sum.\nA.mean(), A.sum() / A.numel()\n(tensor(2.5000), tensor(2.5000))\nLikewise, the function for calculating the mean can also reduce a tensor along speciﬁc axes.\nA.mean(axis=0), A.sum(axis=0) / A.shape[0]\n(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))\n2.3.7 Non-Reduction Sum\nSometimes it can be useful to keep the number of axes unchanged when invoking the func-\ntion for calculating the sum or mean. This matters when we want to use the broadcast mech-\nanism.\nsum_A = A.sum(axis=1, keepdims=True)\nsum_A, sum_A.shape\n\n48\nPreliminaries\n(tensor([[ 3.],\n[12.]]),\ntorch.Size([2, 1]))\nFor instance, since sum_A keeps its two axes after summing each row, we can divide A by\nsum_A with broadcasting to create a matrix where each row sums up to 1.\nA / sum_A\ntensor([[0.0000, 0.3333, 0.6667],\n[0.2500, 0.3333, 0.4167]])\nIf we want to calculate the cumulative sum of elements of A along some axis, say axis=0\n(row by row), we can call the cumsum function. By design, this function does not reduce the\ninput tensor along any axis.\nA.cumsum(axis=0)\ntensor([[0., 1., 2.],\n[3., 5., 7.]])\n2.3.8 Dot Products\nSo far, we have only performed elementwise operations, sums, and averages. And if this was\nall we could do, linear algebra would not deserve its own section. Fortunately, this is where\nthings get more interesting. One of the most fundamental operations is the dot product. Given\ntwo vectors x, y ∈Rd, their dot product x⊤y (also known as inner product, ⟨x, y⟩) is a sum\nover the products of the elements at the same position: x⊤y = ∑d\ni=1 xiyi.\ny = torch.ones(3, dtype = torch.float32)\nx, y, torch.dot(x, y)\n(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))\nEquivalently, we can calculate the dot product of two vectors by performing an elementwise\nmultiplication followed by a sum:\ntorch.sum(x * y)\ntensor(3.)\nDot products are useful in a wide range of contexts. For example, given some set of values,\n\n49\nLinear Algebra\ndenoted by a vector x ∈Rn, and a set of weights, denoted by w ∈Rn, the weighted sum of\nthe values in x according to the weights w could be expressed as the dot product x⊤w. When\nthe weights are nonnegative and sum to 1, i.e., (∑n\ni=1 wi = 1), the dot product expresses a\nweighted average. After normalizing two vectors to have unit length, the dot products express\nthe cosine of the angle between them. Later in this section, we will formally introduce this\nnotion of length.\n2.3.9 Matrix–Vector Products\nNow that we know how to calculate dot products, we can begin to understand the product\nbetween an m × n matrix A and an n-dimensional vector x. To start oﬀ, we visualize our\nmatrix in terms of its row vectors\nA =\n\na⊤\n1\na⊤\n2...\na⊤\nm\n\n,\n(2.3.5)\nwhere each a⊤\ni ∈Rn is a row vector representing the ith row of the matrix A.\nThe matrix–vector product Ax is simply a column vector of length m, whose ith element is\nthe dot product a⊤\ni x:\nAx =\n\na⊤\n1\na⊤\n2...\na⊤\nm\n\nx =\n\na⊤\n1 x\na⊤\n2 x\n...\na⊤\nmx\n\n.\n(2.3.6)\nWe can think of multiplication with a matrix A ∈Rm×n as a transformation that projects\nvectors from Rn to Rm. These transformations are remarkably useful. For example, we can\nrepresent rotations as multiplications by certain square matrices. Matrix–vector products also\ndescribe the key calculation involved in computing the outputs of each layer in a neural net-\nwork given the outputs from the previous layer.\nTo express a matrix–vector product in code, we use the mv function. Note that the column\ndimension of A (its length along axis 1) must be the same as the dimension of x (its length).\nPython has a convenience operator @ that can execute both matrix–vector and matrix–matrix\nproducts (depending on its arguments). Thus we can write A@x.\nA.shape, x.shape, torch.mv(A, x), A@x\n(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))\n2.3.10 Matrix–Matrix Multiplication\n\n50\nPreliminaries\nOnce you have gotten the hang of dot products and matrix–vector products, then matrix–\nmatrix multiplication should be straightforward.\nSay that we have two matrices A ∈Rn×k and B ∈Rk×m:\nA =\n\na11\na12\n· · ·\na1k\na21\na22\n· · ·\na2k\n...\n...\n...\n...\nan1\nan2\n· · ·\nank\n\n,\nB =\n\nb11\nb12\n· · ·\nb1m\nb21\nb22\n· · ·\nb2m\n...\n...\n...\n...\nbk1\nbk2\n· · ·\nbkm\n\n.\n(2.3.7)\nLet a⊤\ni ∈Rk denote the row vector representing the ith row of the matrix A and let bj ∈Rk\ndenote the column vector from the jth column of the matrix B:\nA =\n\na⊤\n1\na⊤\n2...\na⊤\nn\n\n,\nB =\n[\nb1\nb2\n· · ·\nbm\n]\n.\n(2.3.8)\nTo form the matrix product C ∈Rn×m, we simply compute each element cij as the dot\nproduct between the ith row of A and the jth column of B, i.e., a⊤\ni bj:\nC = AB =\n\na⊤\n1\na⊤\n2...\na⊤\nn\n\n[\nb1\nb2\n· · ·\nbm\n]\n=\n\na⊤\n1 b1\na⊤\n1 b2\n· · ·\na⊤\n1 bm\na⊤\n2 b1\na⊤\n2 b2\n· · ·\na⊤\n2 bm\n...\n...\n...\n...\na⊤\nnb1\na⊤\nnb2\n· · ·\na⊤\nnbm\n\n.\n(2.3.9)\nWe can think of the matrix–matrix multiplication AB as performing m matrix–vector prod-\nucts or m × n dot products and stitching the results together to form an n × m matrix. In the\nfollowing snippet, we perform matrix multiplication on A and B. Here, A is a matrix with two\nrows and three columns, and B is a matrix with three rows and four columns. After multipli-\ncation, we obtain a matrix with two rows and four columns.\nB = torch.ones(3, 4)\ntorch.mm(A, B), A@B\n(tensor([[ 3.,\n3.,\n3.,\n3.],\n[12., 12., 12., 12.]]),\ntensor([[ 3.,\n3.,\n3.,\n3.],\n[12., 12., 12., 12.]]))\nThe term matrix–matrix multiplication is often simpliﬁed to matrix multiplication, and should\nnot be confused with the Hadamard product.\n2.3.11 Norms\nSome of the most useful operators in linear algebra are norms. Informally, the norm of a\nvector tells us how big it is. For instance, the ℓ2 norm measures the (Euclidean) length of\n\n51\nLinear Algebra\na vector. Here, we are employing a notion of size that concerns the magnitude of a vector’s\ncomponents (not its dimensionality).\nA norm is a function ∥· ∥that maps a vector to a scalar and satisﬁes the following three\nproperties:\n1. Given any vector x, if we scale (all elements of) the vector by a scalar α ∈R, its norm\nscales accordingly:\n∥αx∥= |α|∥x∥.\n(2.3.10)\n2. For any vectors x and y: norms satisfy the triangle inequality:\n∥x + y∥≤∥x∥+ ∥y∥.\n(2.3.11)\n3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n∥x∥> 0 for all x , 0.\n(2.3.12)\nMany functions are valid norms and diﬀerent norms encode diﬀerent notions of size. The\nEuclidean norm that we all learned in elementary school geometry when calculating the hy-\npotenuse of a right triangle is the square root of the sum of squares of a vector’s elements.\nFormally, this is called the ℓ2 norm and expressed as\n∥x∥2 =\nv\nt n\n∑\ni=1\nx2\ni .\n(2.3.13)\nThe method norm calculates the ℓ2 norm.\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\ntensor(5.)\nThe ℓ1 norm is also common and the associated measure is called the Manhattan distance.\nBy deﬁnition, the ℓ1 norm sums the absolute values of a vector’s elements:\n∥x∥1 =\nn\n∑\ni=1\n|xi| .\n(2.3.14)\nCompared to the ℓ2 norm, it is less sensitive to outliers. To compute the ℓ1 norm, we compose\nthe absolute value with the sum operation.\ntorch.abs(u).sum()\ntensor(7.)\n\n52\nPreliminaries\nBoth the ℓ2 and ℓ1 norms are special cases of the more general ℓp norms:\n∥x∥p =\n( n\n∑\ni=1\n|xi|p\n)1/p\n.\n(2.3.15)\nIn the case of matrices, matters are more complicated. After all, matrices can be viewed both\nas collections of individual entries and as objects that operate on vectors and transform them\ninto other vectors. For instance, we can ask by how much longer the matrix–vector product\nXv could be relative to v. This line of thought leads to what is called the spectral norm. For\nnow, we introduce the Frobenius norm, which is much easier to compute and deﬁned as the\nsquare root of the sum of the squares of a matrix’s elements:\n∥X∥F =\nv\nu\nt m\n∑\ni=1\nn\n∑\nj=1\nx2\nij.\n(2.3.16)\nThe Frobenius norm behaves as if it were an ℓ2 norm of a matrix-shaped vector. Invoking\nthe following function will calculate the Frobenius norm of a matrix.\ntorch.norm(torch.ones((4, 9)))\ntensor(6.)\nWhile we do not want to get too far ahead of ourselves, we already can plant some intuition\nabout why these concepts are useful. In deep learning, we are often trying to solve optimiza-\ntion problems: maximize the probability assigned to observed data; maximize the revenue\nassociated with a recommender model; minimize the distance between predictions and the\nground truth observations; minimize the distance between representations of photos of the\nsame person while maximizing the distance between representations of photos of diﬀerent\npeople. These distances, which constitute the objectives of deep learning algorithms, are\noften expressed as norms.\n2.3.12 Discussion\nIn this section, we have reviewed all the linear algebra that you will need to understand a\nsigniﬁcant chunk of modern deep learning. There is a lot more to linear algebra, though, and\nmuch of it is useful for machine learning. For example, matrices can be decomposed into\nfactors, and these decompositions can reveal low-dimensional structure in real-world datasets.\nThere are entire subﬁelds of machine learning that focus on using matrix decompositions\nand their generalizations to high-order tensors to discover structure in datasets and solve\nprediction problems. But this book focuses on deep learning. And we believe you will be more\ninclined to learn more mathematics once you have gotten your hands dirty applying machine\nlearning to real datasets. So while we reserve the right to introduce more mathematics later\non, we wrap up this section here.\nIf you are eager to learn more linear algebra, there are many excellent books and online\n\n53\nLinear Algebra\nresources. For a more advanced crash course, consider checking out Strang (1993), Kolter\n(2008), and Petersen and Pedersen (2008).\nTo recap:\n• Scalars, vectors, matrices, and tensors are the basic mathematical objects used in linear\nalgebra and have zero, one, two, and an arbitrary number of axes, respectively.\n• Tensors can be sliced or reduced along speciﬁed axes via indexing, or operations such as\nsum and mean, respectively.\n• Elementwise products are called Hadamard products. By contrast, dot products, matrix–\nvector products, and matrix–matrix products are not elementwise operations and in gen-\neral return objects having shapes that are diﬀerent from the the operands.\n• Compared to Hadamard products, matrix–matrix products take considerably longer to\ncompute (cubic rather than quadratic time).\n• Norms capture various notions of the magnitude of a vector (or matrix), and are commonly\napplied to the diﬀerence of two vectors to measure their distance apart.\n• Common vector norms include the ℓ1 and ℓ2 norms, and common matrix norms include\nthe spectral and Frobenius norms.\n2.3.13 Exercises\n1. Prove that the transpose of the transpose of a matrix is the matrix itself: (A⊤)⊤= A.\n2. Given two matrices A and B, show that sum and transposition commute: A⊤+ B⊤=\n(A + B)⊤.\n3. Given any square matrix A, is A + A⊤always symmetric? Can you prove the result by\nusing only the results of the previous two exercises?\n4. We deﬁned the tensor X of shape (2, 3, 4) in this section. What is the output of len(X)?\nWrite your answer without implementing any code, then check your answer using code.\n5. For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain\naxis of X? What is that axis?\n6. Run A / A.sum(axis=1) and see what happens. Can you analyze the results?\n7. When traveling between two points in downtown Manhattan, what is the distance that you\nneed to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you\ntravel diagonally?\n8. Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs along\naxes 0, 1, and 2?\n9. Feed a tensor with three or more axes to the linalg.norm function and observe its output.\nWhat does this function compute for tensors of arbitrary shape?\n\n54\nPreliminaries\n55\n10. Consider three large matrices, say A ∈R210×216, B ∈R216×25 and C ∈R25×214, ini-\ntialized with Gaussian random variables. You want to compute the product ABC. Is\nthere any diﬀerence in memory footprint and speed, depending on whether you compute\n(AB)C or A(BC). Why?\n11. Consider three large matrices, say A ∈R210×216, B ∈R216×25 and C ∈R25×216. Is there\nany diﬀerence in speed depending on whether you compute AB or AC⊤? Why? What\nchanges if you initialize C = B⊤without cloning memory? Why?\n12. Consider three matrices, say A, B, C ∈R100×200. Construct a tensor with three axes by\nstacking [A, B, C]. What is the dimensionality? Slice out the second coordinate of the\nthird axis to recover B. Check that your answer is correct.\nDiscussions55.\n2.4 Calculus\nFor a long time, how to calculate the area of a circle remained a mystery. Then, in Ancient\nGreece, the mathematician Archimedes came up with the clever idea to inscribe a series of\npolygons with increasing numbers of vertices on the inside of a circle (Fig. 2.4.1). For a\npolygon with n vertices, we obtain n triangles. The height of each triangle approaches the\nradius r as we partition the circle more ﬁnely. At the same time, its base approaches 2πr/n,\nsince the ratio between arc and secant approaches 1 for a large number of vertices. Thus, the\narea of the polygon approaches n · r · 1\n2(2πr/n) = πr2.\nt\nFig. 2.4.1\nFinding the area of a circle as a limit procedure.\nThis limiting procedure is at the root of both diﬀerential calculus and integral calculus. The\nformer can tell us how to increase or decrease a function’s value by manipulating its argu-\nments. This comes in handy for the optimization problems that we face in deep learning, where\nwe repeatedly update our parameters in order to decrease the loss function. Optimization ad-\ndresses how to ﬁt our models to training data, and calculus is its key prerequisite. However,\ndo not forget that our ultimate goal is to perform well on previously unseen data. That problem\nis called generalization and will be a key focus of other chapters.\n\n55\nCalculus\n%matplotlib inline\nimport numpy as np\nfrom matplotlib_inline import backend_inline\nfrom d2l import torch as d2l\n2.4.1 Derivatives and Diﬀerentiation\nPut simply, a derivative is the rate of change in a function with respect to changes in its\narguments. Derivatives can tell us how rapidly a loss function would increase or decrease were\nwe to increase or decrease each parameter by an inﬁnitesimally small amount. Formally, for\nfunctions f : R →R, that map from scalars to scalars, the derivative of f at a point x is\ndeﬁned as\nf ′(x) = lim\nh→0\nf (x + h) −f (x)\nh\n.\n(2.4.1)\nThis term on the right hand side is called a limit and it tells us what happens to the value of\nan expression as a speciﬁed variable approaches a particular value. This limit tells us what\nthe ratio between a perturbation h and the change in the function value f (x + h) −f (x)\nconverges to as we shrink its size to zero.\nWhen f ′(x) exists, f is said to be diﬀerentiable at x; and when f ′(x) exists for all x on a\nset, e.g., the interval [a, b], we say that f is diﬀerentiable on this set. Not all functions are\ndiﬀerentiable, including many that we wish to optimize, such as accuracy and the area under\nthe receiving operating characteristic (AUC). However, because computing the derivative of\nthe loss is a crucial step in nearly all algorithms for training deep neural networks, we often\noptimize a diﬀerentiable surrogate instead.\nWe can interpret the derivative f ′(x) as the instantaneous rate of change of f (x) with respect\nto x. Let’s develop some intuition with an example. Deﬁne u = f (x) = 3x2 −4x.\ndef f(x):\nreturn 3 * x ** 2 - 4 * x\nSetting x = 1, we see that f (x+h)−f (x)\nh\napproaches 2 as h approaches 0. While this experiment\nlacks the rigor of a mathematical proof, we can quickly see that indeed f ′(1) = 2.\nfor h in 10.0**np.arange(-1, -6, -1):\nprint(f'h={h:.5f}, numerical limit={(f(1+h)-f(1))/h:.5f}')\nh=0.10000, numerical limit=2.30000\nh=0.01000, numerical limit=2.03000\nh=0.00100, numerical limit=2.00300\nh=0.00010, numerical limit=2.00030\nh=0.00001, numerical limit=2.00003\n\n56\nPreliminaries\nThere are several equivalent notational conventions for derivatives. Given y = f (x), the\nfollowing expressions are equivalent:\nf ′(x) = y′ = dy\ndx = df\ndx = d\ndx f (x) = D f (x) = Dx f (x),\n(2.4.2)\nwhere the symbols\nd\ndx and D are diﬀerentiation operators. Below, we present the derivatives\nof some common functions:\nd\ndx C = 0\nfor any constant C\nd\ndx xn = nxn−1\nfor n , 0\nd\ndx ex = ex\nd\ndx ln x = x−1.\n(2.4.3)\nFunctions composed from diﬀerentiable functions are often themselves diﬀerentiable. The\nfollowing rules come in handy for working with compositions of any diﬀerentiable functions\nf and g, and constant C.\nd\ndx [C f (x)] = C d\ndx f (x)\nConstant multiple rule\nd\ndx [ f (x) + g(x)] = d\ndx f (x) + d\ndx g(x)\nSum rule\nd\ndx [ f (x)g(x)] = f (x) d\ndx g(x) + g(x) d\ndx f (x)\nProduct rule\nd\ndx\nf (x)\ng(x) =\ng(x) d\ndx f (x) −f (x) d\ndx g(x)\ng2(x)\nQuotient rule\n(2.4.4)\nUsing this, we can apply the rules to ﬁnd the derivative of 3x2 −4x via\nd\ndx [3x2 −4x] = 3 d\ndx x2 −4 d\ndx x = 6x −4.\n(2.4.5)\nPlugging in x = 1 shows that, indeed, the derivative equals 2 at this location. Note that\nderivatives tell us the slope of a function at a particular location.\n2.4.2 Visualization Utilities\nWe can visualize the slopes of functions using the matplotlib library. We need to deﬁne a\nfew functions. As its name indicates, use_svg_display tells matplotlib to output graphics\nin SVG format for crisper images. The comment #@save is a special modiﬁer that allows us\nto save any function, class, or other code block to the d2l package so that we can invoke it\nlater without repeating the code, e.g., via d2l.use_svg_display().\ndef use_svg_display():\n#@save\n\"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\nbackend_inline.set_matplotlib_formats('svg')\n\n57\nCalculus\nConveniently, we can set ﬁgure sizes with set_figsize. Since the import statement from\nmatplotlib import pyplot as plt was marked via #@save in the d2l package, we can\ncall d2l.plt.\ndef set_figsize(figsize=(3.5, 2.5)):\n#@save\n\"\"\"Set the figure size for matplotlib.\"\"\"\nuse_svg_display()\nd2l.plt.rcParams['figure.figsize'] = figsize\nThe set_axes function can associate axes with properties, including labels, ranges, and\nscales.\n#@save\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n\"\"\"Set the axes for matplotlib.\"\"\"\naxes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\naxes.set_xscale(xscale), axes.set_yscale(yscale)\naxes.set_xlim(xlim),\naxes.set_ylim(ylim)\nif legend:\naxes.legend(legend)\naxes.grid()\nWith these three functions, we can deﬁne a plot function to overlay multiple curves. Much\nof the code here is just ensuring that the sizes and shapes of inputs match.\n#@save\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\nylim=None, xscale='linear', yscale='linear',\nfmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n\"\"\"Plot data points.\"\"\"\ndef has_one_axis(X):\n# True if X (tensor or list) has 1 axis\nreturn (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\nand not hasattr(X[0], \"__len__\"))\nif has_one_axis(X): X = [X]\nif Y is None:\nX, Y = [[]] * len(X), X\nelif has_one_axis(Y):\nY = [Y]\nif len(X) != len(Y):\nX = X * len(Y)\nset_figsize(figsize)\nif axes is None:\naxes = d2l.plt.gca()\naxes.cla()\nfor x, y, fmt in zip(X, Y, fmts):\naxes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)\nset_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nNow we can plot the function u = f (x) and its tangent line y = 2x −3 at x = 1, where the\ncoeﬃcient 2 is the slope of the tangent line.\n\n58\nPreliminaries\nx = np.arange(0, 3, 0.1)\nplot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])\n2.4.3 Partial Derivatives and Gradients\nThus far, we have been diﬀerentiating functions of just one variable. In deep learning, we also\nneed to work with functions of many variables. We brieﬂy introduce notions of the derivative\nthat apply to such multivariate functions.\nLet y = f (x1, x2, . . ., xn) be a function with n variables. The partial derivative of y with\nrespect to its ith parameter xi is\n∂y\n∂xi\n= lim\nh→0\nf (x1, . . ., xi−1, xi + h, xi+1, . . ., xn) −f (x1, . . ., xi, . . ., xn)\nh\n.\n(2.4.6)\nTo calculate ∂y\n∂xi , we can treat x1, . . ., xi−1, xi+1, . . ., xn as constants and calculate the deriva-\ntive of y with respect to xi. The following notational conventions for partial derivatives are\nall common and all mean the same thing:\n∂y\n∂xi\n= ∂f\n∂xi\n= ∂xi f = ∂i f = fxi = fi = Di f = Dxi f .\n(2.4.7)\nWe can concatenate partial derivatives of a multivariate function with respect to all its vari-\nables to obtain a vector that is called the gradient of the function. Suppose that the input\nof function f : Rn →R is an n-dimensional vector x = [x1, x2, . . ., xn]⊤and the output\nis a scalar. The gradient of the function f with respect to x is a vector of n partial deriva-\ntives:\n∇x f (x) = [∂x1 f (x), ∂x2 f (x), . . . ∂xn f (x)]⊤.\n(2.4.8)\nWhen there is no ambiguity, ∇x f (x) is typically replaced by ∇f (x). The following rules\ncome in handy for diﬀerentiating multivariate functions:\n• For all A ∈Rm×n we have ∇xAx = A⊤and ∇xx⊤A = A.\n\n59\nCalculus\n• For square matrices A ∈Rn×n we have that ∇xx⊤Ax = (A + A⊤)x and in particular\n∇x∥x∥2 = ∇xx⊤x = 2x.\nSimilarly, for any matrix X, we have ∇X∥X∥2\nF = 2X.\n2.4.4 Chain Rule\nIn deep learning, the gradients of concern are often diﬃcult to calculate because we are\nworking with deeply nested functions (of functions (of functions…)). Fortunately, the chain\nrule takes care of this. Returning to functions of a single variable, suppose that y = f (g(x))\nand that the underlying functions y = f (u) and u = g(x) are both diﬀerentiable. The chain\nrule states that\ndy\ndx = dy\ndu\ndu\ndx .\n(2.4.9)\nTurning back to multivariate functions, suppose that y = f (u) has variables u1, u2, . . ., um,\nwhere each ui = gi(x) has variables x1, x2, . . ., xn, i.e., u = g(x). Then the chain rule states\nthat\n∂y\n∂xi\n= ∂y\n∂u1\n∂u1\n∂xi\n+ ∂y\n∂u2\n∂u2\n∂xi\n+ . . . + ∂y\n∂um\n∂um\n∂xi\nand so ∇xy = A∇uy,\n(2.4.10)\nwhere A ∈Rn×m is a matrix that contains the derivative of vector u with respect to vector x.\nThus, evaluating the gradient requires computing a vector–matrix product. This is one of the\nkey reasons why linear algebra is such an integral building block in building deep learning\nsystems.\n2.4.5 Discussion\nWhile we have just scratched the surface of a deep topic, a number of concepts already come\ninto focus: ﬁrst, the composition rules for diﬀerentiation can be applied routinely, enabling\nus to compute gradients automatically. This task requires no creativity and thus we can focus\nour cognitive powers elsewhere. Second, computing the derivatives of vector-valued func-\ntions requires us to multiply matrices as we trace the dependency graph of variables from\noutput to input. In particular, this graph is traversed in a forward direction when we eval-\nuate a function and in a backwards direction when we compute gradients. Later chapters\nwill formally introduce backpropagation, a computational procedure for applying the chain\nrule.\nFrom the viewpoint of optimization, gradients allow us to determine how to move the pa-\nrameters of a model in order to lower the loss, and each step of the optimization algorithms\nused throughout this book will require calculating the gradient.\n2.4.6 Exercises\n1. So far we took the rules for derivatives for granted. Using the deﬁnition and limits prove\nthe properties for (i) f (x) = c, (ii) f (x) = xn, (iii) f (x) = ex and (iv) f (x) = log x.\n\n60\nPreliminaries\n56\n2. In the same vein, prove the product, sum, and quotient rule from ﬁrst principles.\n3. Prove that the constant multiple rule follows as a special case of the product rule.\n4. Calculate the derivative of f (x) = xx.\n5. What does it mean that f ′(x) = 0 for some x? Give an example of a function f and a\nlocation x for which this might hold.\n6. Plot the function y = f (x) = x3 −1\nx and plot its tangent line at x = 1.\n7. Find the gradient of the function f (x) = 3x2\n1 + 5ex2.\n8. What is the gradient of the function f (x) = ∥x∥2? What happens for x = 0?\n9. Can you write out the chain rule for the case where u = f (x, y, z) and x = x(a, b),\ny = y(a, b), and z = z(a, b)?\n10. Given a function f (x) that is invertible, compute the derivative of its inverse f −1(x). Here\nwe have that f −1( f (x)) = x and conversely f ( f −1(y)) = y. Hint: use these properties\nin your derivation.\nDiscussions56.\n2.5 Automatic Diﬀerentiation\nRecall from Section 2.4 that calculating derivatives is the crucial step in all the optimization\nalgorithms that we will use to train deep networks. While the calculations are straightforward,\nworking them out by hand can be tedious and error-prone, and these issues only grow as our\nmodels become more complex.\nFortunately all modern deep learning frameworks take this work oﬀour plates by oﬀering\nautomatic diﬀerentiation (often shortened to autograd). As we pass data through each succes-\nsive function, the framework builds a computational graph that tracks how each value depends\non others. To calculate derivatives, automatic diﬀerentiation works backwards through this\ngraph applying the chain rule. The computational algorithm for applying the chain rule in this\nfashion is called backpropagation.\nWhile autograd libraries have become a hot concern over the past decade, they have a long\nhistory. In fact the earliest references to autograd date back over half of a century (Wengert,\n1964). The core ideas behind modern backpropagation date to a PhD thesis from 1980\n(Speelpenning, 1980) and were further developed in the late 1980s (Griewank, 1989). While\nbackpropagation has become the default method for computing gradients, it is not the only\noption. For instance, the Julia programming language employs forward propagation (Revels\net al., 2016). Before exploring methods, let’s ﬁrst master the autograd package.\n\n61\nAutomatic Differentiation\nimport torch\n2.5.1 A Simple Function\nLet’s assume that we are interested in diﬀerentiating the function y = 2x⊤x with respect to\nthe column vector x. To start, we assign x an initial value.\nx = torch.arange(4.0)\nx\ntensor([0., 1., 2., 3.])\nBefore we calculate the gradient of y with respect to x, we need a place to store it. In general,\nwe avoid allocating new memory every time we take a derivative because deep learning re-\nquires successively computing derivatives with respect to the same parameters a great many\ntimes, and we might risk running out of memory. Note that the gradient of a scalar-valued\nfunction with respect to a vector x is vector-valued with the same shape as x.\n# Can also create x = torch.arange(4.0, requires_grad=True)\nx.requires_grad_(True)\nx.grad\n# The gradient is None by default\nWe now calculate our function of x and assign the result to y.\ny = 2 * torch.dot(x, x)\ny\ntensor(28., grad_fn=<MulBackward0>)\nWe can now take the gradient of y with respect to x by calling its backward method. Next,\nwe can access the gradient via x’s grad attribute.\ny.backward()\nx.grad\ntensor([ 0.,\n4.,\n8., 12.])\nWe already know that the gradient of the function y = 2x⊤x with respect to x should be\n4x. We can now verify that the automatic gradient computation and the expected result are\nidentical.\n\n62\nPreliminaries\n57\nx.grad == 4 * x\ntensor([True, True, True, True])\nNow let’s calculate another function of x and take its gradient. Note that PyTorch does not\nautomatically reset the gradient buﬀer when we record a new gradient. Instead, the new gra-\ndient is added to the already-stored gradient. This behavior comes in handy when we want\nto optimize the sum of multiple objective functions. To reset the gradient buﬀer, we can call\nx.grad.zero_() as follows:\nx.grad.zero_()\n# Reset the gradient\ny = x.sum()\ny.backward()\nx.grad\ntensor([1., 1., 1., 1.])\n2.5.2 Backward for Non-Scalar Variables\nWhen y is a vector, the most natural representation of the derivative of y with respect to a\nvector x is a matrix called the Jacobian that contains the partial derivatives of each component\nof y with respect to each component of x. Likewise, for higher-order y and x, the result of\ndiﬀerentiation could be an even higher-order tensor.\nWhile Jacobians do show up in some advanced machine learning techniques, more commonly\nwe want to sum up the gradients of each component of y with respect to the full vector x,\nyielding a vector of the same shape as x. For example, we often have a vector representing\nthe value of our loss function calculated separately for each example among a batch of train-\ning examples. Here, we just want to sum up the gradients computed individually for each\nexample.\nBecause deep learning frameworks vary in how they interpret gradients of non-scalar tensors,\nPyTorch takes some steps to avoid confusion. Invoking backward on a non-scalar elicits\nan error unless we tell PyTorch how to reduce the object to a scalar. More formally, we\nneed to provide some vector v such that backward will compute v⊤∂xy rather than ∂xy.\nThis next part may be confusing, but for reasons that will become clear later, this argument\n(representing v) is named gradient. For a more detailed description, see Yang Zhang’s\nMedium post57.\nx.grad.zero_()\ny = x * x\ny.backward(gradient=torch.ones(len(y)))\n# Faster: y.sum().backward()\nx.grad\n\n63\nAutomatic Differentiation\ntensor([0., 2., 4., 6.])\n2.5.3 Detaching Computation\nSometimes, we wish to move some calculations outside of the recorded computational graph.\nFor example, say that we use the input to create some auxiliary intermediate terms for which\nwe do not want to compute a gradient. In this case, we need to detach the respective com-\nputational graph from the ﬁnal result. The following toy example makes this clearer: suppose\nwe have z = x * y and y = x * x but we want to focus on the direct inﬂuence of x on\nz rather than the inﬂuence conveyed via y. In this case, we can create a new variable u that\ntakes the same value as y but whose provenance (how it was created) has been wiped out.\nThus u has no ancestors in the graph and gradients do not ﬂow through u to x. For example,\ntaking the gradient of z = x * u will yield the result u, (not 3 * x * x as you might have\nexpected since z = x * x * x).\nx.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x\nz.sum().backward()\nx.grad == u\ntensor([True, True, True, True])\nNote that while this procedure detaches y’s ancestors from the graph leading to z, the com-\nputational graph leading to y persists and thus we can calculate the gradient of y with respect\nto x.\nx.grad.zero_()\ny.sum().backward()\nx.grad == 2 * x\ntensor([True, True, True, True])\n2.5.4 Gradients and Python Control Flow\nSo far we reviewed cases where the path from input to output was well deﬁned via a function\nsuch as z = x * x * x. Programming oﬀers us a lot more freedom in how we compute\nresults. For instance, we can make them depend on auxiliary variables or condition choices on\nintermediate results. One beneﬁt of using automatic diﬀerentiation is that even if building the\ncomputational graph of a function required passing through a maze of Python control ﬂow\n\n64\nPreliminaries\n(e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of\nthe resulting variable. To illustrate this, consider the following code snippet where the number\nof iterations of the while loop and the evaluation of the if statement both depend on the\nvalue of the input a.\ndef f(a):\nb = a * 2\nwhile b.norm() < 1000:\nb = b * 2\nif b.sum() > 0:\nc = b\nelse:\nc = 100 * b\nreturn c\nBelow, we call this function, passing in a random value, as input. Since the input is a random\nvariable, we do not know what form the computational graph will take. However, whenever\nwe execute f(a) on a speciﬁc input, we realize a speciﬁc computational graph and can sub-\nsequently run backward.\na = torch.randn(size=(), requires_grad=True)\nd = f(a)\nd.backward()\nEven though our function f is, for demonstration purposes, a bit contrived, its dependence\non the input is quite simple: it is a linear function of a with piecewise deﬁned scale. As such,\nf(a) / a is a vector of constant entries and, moreover, f(a) / a needs to match the gradient\nof f(a) with respect to a.\na.grad == d / a\ntensor(True)\nDynamic control ﬂow is very common in deep learning. For instance, when processing text,\nthe computational graph depends on the length of the input. In these cases, automatic diﬀer-\nentiation becomes vital for statistical modeling since it is impossible to compute the gradient\na priori.\n2.5.5 Discussion\nYou have now gotten a taste of the power of automatic diﬀerentiation. The development\nof libraries for calculating derivatives both automatically and eﬃciently has been a massive\nproductivity booster for deep learning practitioners, liberating them so they can focus on less\nmenial. Moreover, autograd lets us design massive models for which pen and paper gradient\ncomputations would be prohibitively time consuming. Interestingly, while we use autograd\nto optimize models (in a statistical sense) the optimization of autograd libraries themselves\n\n65\nProbability and Statistics\n58\n(in a computational sense) is a rich subject of vital interest to framework designers. Here,\ntools from compilers and graph manipulation are leveraged to compute results in the most\nexpedient and memory-eﬃcient manner.\nFor now, try to remember these basics: (i) attach gradients to those variables with respect to\nwhich we desire derivatives; (ii) record the computation of the target value; (iii) execute the\nbackpropagation function; and (iv) access the resulting gradient.\n2.5.6 Exercises\n1. Why is the second derivative much more expensive to compute than the ﬁrst derivative?\n2. After running the function for backpropagation, immediately run it again and see what\nhappens. Investigate.\n3. In the control ﬂow example where we calculate the derivative of d with respect to a, what\nwould happen if we changed the variable a to a random vector or a matrix? At this point,\nthe result of the calculation f(a) is no longer a scalar. What happens to the result? How\ndo we analyze this?\n4. Let f (x) = sin(x). Plot the graph of f and of its derivative f ′. Do not exploit the fact\nthat f ′(x) = cos(x) but rather use automatic diﬀerentiation to get the result.\n5. Let f (x) = ((log x2) · sin x) + x−1. Write out a dependency graph tracing results from\nx to f (x).\n6. Use the chain rule to compute the derivative df\ndx of the aforementioned function, placing\neach term on the dependency graph that you constructed previously.\n7. Given the graph and the intermediate derivative results, you have a number of options\nwhen computing the gradient. Evaluate the result once starting from x to f and once from\nf tracing back to x. The path from x to f is commonly known as forward diﬀerentiation,\nwhereas the path from f to x is known as backward diﬀerentiation.\n8. When might you want to use forward, and when backward, diﬀerentiation? Hint: consider\nthe amount of intermediate data needed, the ability to parallelize steps, and the size of\nmatrices and vectors involved.\nDiscussions58.\n2.6 Probability and Statistics\nOne way or another, machine learning is all about uncertainty. In supervised learning, we want\nto predict something unknown (the target) given something known (the features). Depending\non our objective, we might attempt to predict the most likely value of the target. Or we might\n\n66\nPreliminaries\npredict the value with the smallest expected distance from the target. And sometimes we\nwish not only to predict a speciﬁc value but to quantify our uncertainty. For example, given\nsome features describing a patient, we might want to know how likely they are to suﬀer a\nheart attack in the next year. In unsupervised learning, we often care about uncertainty. To\ndetermine whether a set of measurements are anomalous, it helps to know how likely one\nis to observe values in a population of interest. Furthermore, in reinforcement learning, we\nwish to develop agents that act intelligently in various environments. This requires reasoning\nabout how an environment might be expected to change and what rewards one might expect\nto encounter in response to each of the available actions.\nProbability is the mathematical ﬁeld concerned with reasoning under uncertainty. Given a\nprobabilistic model of some process, we can reason about the likelihood of various events.\nThe use of probabilities to describe the frequencies of repeatable events (like coin tosses) is\nfairly uncontroversial. In fact, frequentist scholars adhere to an interpretation of probability\nthat applies only to such repeatable events. By contrast Bayesian scholars use the language\nof probability more broadly to formalize reasoning under uncertainty. Bayesian probability is\ncharacterized by two unique features: (i) assigning degrees of belief to non-repeatable events,\ne.g., what is the probability that a dam will collapse?; and (ii) subjectivity. While Bayesian\nprobability provides unambiguous rules for how one should update their beliefs in light of new\nevidence, it allows for diﬀerent individuals to start oﬀwith diﬀerent prior beliefs. Statistics\nhelps us to reason backwards, starting oﬀwith collection and organization of data and backing\nout to what inferences we might draw about the process that generated the data. Whenever we\nanalyze a dataset, hunting for patterns that we hope might characterize a broader population,\nwe are employing statistical thinking. Many courses, majors, theses, careers, departments,\ncompanies, and institutions have been devoted to the study of probability and statistics. While\nthis section only scratches the surface, we will provide the foundation that you need to begin\nbuilding models.\n%matplotlib inline\nimport random\nimport torch\nfrom torch.distributions.multinomial import Multinomial\nfrom d2l import torch as d2l\n2.6.1 A Simple Example: Tossing Coins\nImagine that we plan to toss a coin and want to quantify how likely we are to see heads (vs.\ntails). If the coin is fair, then both outcomes (heads and tails), are equally likely. Moreover\nif we plan to toss the coin n times then the fraction of heads that we expect to see should\nexactly match the expected fraction of tails. One intuitive way to see this is by symmetry:\nfor every possible outcome with nh heads and nt = (n −nh) tails, there is an equally likely\noutcome with nt heads and nh tails. Note that this is only possible if on average we expect\nto see 1/2 of tosses come up heads and 1/2 come up tails. Of course, if you conduct this\nexperiment many times with n = 1000000 tosses each, you might never see a trial where\nnh = nt exactly.\n\n67\nProbability and Statistics\nFormally, the quantity 1/2 is called a probability and here it captures the certainty with which\nany given toss will come up heads. Probabilities assign scores between 0 and 1 to outcomes\nof interest, called events. Here the event of interest is heads and we denote the corresponding\nprobability P(heads). A probability of 1 indicates absolute certainty (imagine a trick coin\nwhere both sides were heads) and a probability of 0 indicates impossibility (e.g., if both\nsides were tails). The frequencies nh/n and nt/n are not probabilities but rather statistics.\nProbabilities are theoretical quantities that underly the data generating process. Here, the\nprobability 1/2 is a property of the coin itself. By contrast, statistics are empirical quanti-\nties that are computed as functions of the observed data. Our interests in probabilistic and\nstatistical quantities are inextricably intertwined. We often design special statistics called es-\ntimators that, given a dataset, produce estimates of model parameters such as probabilities.\nMoreover, when those estimators satisfy a nice property called consistency, our estimates will\nconverge to the corresponding probability. In turn, these inferred probabilities tell about the\nlikely statistical properties of data from the same population that we might encounter in the\nfuture.\nSuppose that we stumbled upon a real coin for which we did not know the true P(heads).\nTo investigate this quantity with statistical methods, we need to (i) collect some data; and (ii)\ndesign an estimator. Data acquisition here is easy; we can toss the coin many times and record\nall the outcomes. Formally, drawing realizations from some underlying random process is\ncalled sampling. As you might have guessed, one natural estimator is the ratio of the number\nof observed heads to the total number of tosses.\nNow, suppose that the coin was in fact fair, i.e., P(heads) = 0.5. To simulate tosses of a fair\ncoin, we can invoke any random number generator. There are some easy ways to draw samples\nof an event with probability 0.5. For example Python’s random.random yields numbers in\nthe interval [0, 1] where the probability of lying in any sub-interval [a, b] ⊂[0, 1] is equal to\nb −a. Thus we can get out 0 and 1 with probability 0.5 each by testing whether the returned\nﬂoat number is greater than 0.5:\nnum_tosses = 100\nheads = sum([random.random() > 0.5 for _ in range(num_tosses)])\ntails = num_tosses - heads\nprint(\"heads, tails: \", [heads, tails])\nheads, tails:\n[45, 55]\nMore generally, we can simulate multiple draws from any variable with a ﬁnite number of\npossible outcomes (like the toss of a coin or roll of a die) by calling the multinomial function,\nsetting the ﬁrst argument to the number of draws and the second as a list of probabilities\nassociated with each of the possible outcomes. To simulate ten tosses of a fair coin, we assign\nprobability vector [0.5, 0.5], interpreting index 0 as heads and index 1 as tails. The function\nreturns a vector with length equal to the number of possible outcomes (here, 2), where the\nﬁrst component tells us the number of occurrences of heads and the second component tells\nus the number of occurrences of tails.\n\n68\nPreliminaries\nfair_probs = torch.tensor([0.5, 0.5])\nMultinomial(100, fair_probs).sample()\ntensor([44., 56.])\nEach time you run this sampling process, you will receive a new random value that may\ndiﬀer from the previous outcome. Dividing by the number of tosses gives us the frequency\nof each outcome in our data. Note that these frequencies, just like the probabilities that they\nare intended to estimate, sum to 1.\nMultinomial(100, fair_probs).sample() / 100\ntensor([0.4600, 0.5400])\nHere, even though our simulated coin is fair (we ourselves set the probabilities [0.5, 0.5]),\nthe counts of heads and tails may not be identical. That is because we only drew a relatively\nsmall number of samples. If we did not implement the simulation ourselves, and only saw the\noutcome, how would we know if the coin were slightly unfair or if the possible deviation from\n1/2 was just an artifact of the small sample size? Let’s see what happens when we simulate\n10,000 tosses.\ncounts = Multinomial(10000, fair_probs).sample()\ncounts / 10000\ntensor([0.4957, 0.5043])\nIn general, for averages of repeated events (like coin tosses), as the number of repetitions\ngrows, our estimates are guaranteed to converge to the true underlying probabilities. The\nmathematical formulation of this phenomenon is called the law of large numbers and the\ncentral limit theorem tells us that in many situations, as the sample size n grows, these errors\nshould go down at a rate of (1/√n). Let’s get some more intuition by studying how our\nestimate evolves as we grow the number of tosses from 1 to 10,000.\ncounts = Multinomial(1, fair_probs).sample((10000,))\ncum_counts = counts.cumsum(dim=0)\nestimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\nestimates = estimates.numpy()\nd2l.set_figsize((4.5, 3.5))\nd2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\nd2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\nd2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\nd2l.plt.gca().set_xlabel('Samples')\nd2l.plt.gca().set_ylabel('Estimated probability')\nd2l.plt.legend();\n\n69\nProbability and Statistics\nEach solid curve corresponds to one of the two values of the coin and gives our estimated\nprobability that the coin turns up that value after each group of experiments. The dashed\nblack line gives the true underlying probability. As we get more data by conducting more\nexperiments, the curves converge towards the true probability. You might already begin to\nsee the shape of some of the more advanced questions that preoccupy statisticians: How\nquickly does this convergence happen? If we had already tested many coins manufactured at\nthe same plant, how might we incorporate this information?\n2.6.2 A More Formal Treatment\nWe have already gotten pretty far: posing a probabilistic model, generating synthetic data,\nrunning a statistical estimator, empirically assessing convergence, and reporting error met-\nrics (checking the deviation). However, to go much further, we will need to be more pre-\ncise.\nWhen dealing with randomness, we denote the set of possible outcomes S and call it the sam-\nple space or outcome space. Here, each element is a distinct possible outcome. In the case of\nrolling a single coin, S = {heads, tails}. For a single die, S = {1, 2, 3, 4, 5, 6}. When ﬂipping\ntwo coins, possible outcomes are {(heads, heads), (heads, tails), (tails, heads), (tails, tails)}.\nEvents are subsets of the sample space. For instance, the event “the ﬁrst coin toss comes\nup heads” corresponds to the set {(heads, heads), (heads, tails)}. Whenever the outcome z of\na random experiment satisﬁes z ∈A, then event A has occurred. For a single roll of a die, we\ncould deﬁne the events “seeing a 5” (A = {5}) and “seeing an odd number” (B = {1, 3, 5}).\nIn this case, if the die came up 5, we would say that both A and B occurred. On the other\nhand, if z = 3, then A did not occur but B did.\nA probability function maps events onto real values P : A ⊆S →[0, 1]. The probability, de-\nnoted P(A), of an event A in the given sample space S, has the following properties:\n• The probability of any event A is a nonnegative real number, i.e., P(A) ≥0;\n\n70\nPreliminaries\n• The probability of the entire sample space is 1, i.e., P(S) = 1;\n• For any countable sequence of events A1, A2, . . . that are mutually exclusive (i.e., Ai ∩\nAj = ∅for all i , j), the probability that any of them happens is equal to the sum of\ntheir individual probabilities, i.e., P(∪∞\ni=1 Ai) = ∑∞\ni=1 P(Ai).\nThese axioms of probability theory, proposed by Kolmogorov (1933), can be applied to\nrapidly derive a number of important consequences. For instance, it follows immediately that\nthe probability of any event A or its complement A′ occurring is 1 (because A ∪A′ = S).\nWe can also prove that P(∅) = 0 because 1 = P(S ∪S′) = P(S ∪∅) = P(S) + P(∅) =\n1 + P(∅). Consequently, the probability of any event A and its complement A′ occurring\nsimultaneously is P(A ∩A′) = 0. Informally, this tells us that impossible events have zero\nprobability of occurring.\n2.6.3 Random Variables\nWhen we spoke about events like the roll of a die coming up odds or the ﬁrst coin toss coming\nup heads, we were invoking the idea of a random variable. Formally, random variables are\nmappings from an underlying sample space to a set of (possibly many) values. You might\nwonder how a random variable is diﬀerent from the sample space, since both are collections of\noutcomes. Importantly, random variables can be much coarser than the raw sample space. We\ncan deﬁne a binary random variable like “greater than 0.5” even when the underlying sample\nspace is inﬁnite, e.g., points on the line segment between 0 and 1. Additionally, multiple\nrandom variables can share the same underlying sample space. For example “whether my\nhome alarm goes oﬀ” and “whether my house was burgled” are both binary random variables\nthat share an underlying sample space. Consequently, knowing the value taken by one random\nvariable can tell us something about the likely value of another random variable. Knowing\nthat the alarm went oﬀ, we might suspect that the house was likely burgled.\nEvery value taken by a random variable corresponds to a subset of the underlying sample\nspace. Thus the occurrence where the random variable X takes value v, denoted by X = v, is\nan event and P(X = v) denotes its probability. Sometimes this notation can get clunky, and\nwe can abuse notation when the context is clear. For example, we might use P(X) to refer\nbroadly to the distribution of X, i.e., the function that tells us the probability that X takes any\ngiven value. Other times we write expressions like P(X,Y) = P(X)P(Y), as a shorthand to\nexpress a statement that is true for all of the values that the random variables X and Y can\ntake, i.e., for all i, j it holds that P(X = i and Y = j) = P(X = i)P(Y = j). Other times,\nwe abuse notation by writing P(v) when the random variable is clear from the context. Since\nan event in probability theory is a set of outcomes from the sample space, we can specify\na range of values for a random variable to take. For example, P(1 ≤X ≤3) denotes the\nprobability of the event {1 ≤X ≤3}.\nNote that there is a subtle diﬀerence between discrete random variables, like ﬂips of a coin\nor tosses of a die, and continuous ones, like the weight and the height of a person sampled at\nrandom from the population. In this case we seldom really care about someone’s exact height.\nMoreover, if we took precise enough measurements, we would ﬁnd that no two people on\n\n71\nProbability and Statistics\nthe planet have the exact same height. In fact, with ﬁne enough measurements, you would\nnever have the same height when you wake up and when you go to sleep. There is little point\nin asking about the exact probability that someone is 1.801392782910287192 meters tall.\nInstead, we typically care more about being able to say whether someone’s height falls into\na given interval, say between 1.79 and 1.81 meters. In these cases we work with probability\ndensities. The height of exactly 1.80 meters has no probability, but nonzero density. To work\nout the probability assigned to an interval, we must take an integral of the density over that\ninterval.\n2.6.4 Multiple Random Variables\nYou might have noticed that we could not even make it through the previous section without\nmaking statements involving interactions among multiple random variables (recall P(X,Y) =\nP(X)P(Y)). Most of machine learning is concerned with such relationships. Here, the sam-\nple space would be the population of interest, say customers who transact with a business,\nphotographs on the Internet, or proteins known to biologists. Each random variable would\nrepresent the (unknown) value of a diﬀerent attribute. Whenever we sample an individual\nfrom the population, we observe a realization of each of the random variables. Because the\nvalues taken by random variables correspond to subsets of the sample space that could be\noverlapping, partially overlapping, or entirely disjoint, knowing the value taken by one ran-\ndom variable can cause us to update our beliefs about which values of another random vari-\nable are likely. If a patient walks into a hospital and we observe that they are having trouble\nbreathing and have lost their sense of smell, then we believe that they are more likely to have\nCOVID-19 than we might if they had no trouble breathing and a perfectly ordinary sense of\nsmell.\nWhen working with multiple random variables, we can construct events corresponding to\nevery combination of values that the variables can jointly take. The probability function that\nassigns probabilities to each of these combinations (e.g. A = a and B = b) is called the joint\nprobability function and simply returns the probability assigned to the intersection of the\ncorresponding subsets of the sample space. The joint probability assigned to the event where\nrandom variables A and B take values a and b, respectively, is denoted P(A = a, B = b),\nwhere the comma indicates “and”. Note that for any values a and b, it follows that\nP(A = a, B = b) ≤P(A = a) and P(A = a, B = b) ≤P(B = b),\n(2.6.1)\nsince for A = a and B = b to happen, A = a has to happen and B = b also has to\nhappen. Interestingly, the joint probability tells us all that we can know about these random\nvariables in a probabilistic sense, and can be used to derive many other useful quantities,\nincluding recovering the individual distributions P(A) and P(B). To recover P(A = a) we\nsimply sum up P(A = a, B = v) over all values v that the random variable B can take:\nP(A = a) = ∑\nv P(A = a, B = v).\nThe ratio P(A=a,B=b)\nP(A=a)\n≤1 turns out to be extremely important. It is called the conditional\n\n72\nPreliminaries\nprobability, and is denoted via the “|” symbol:\nP(B = b | A = a) = P(A = a, B = b)/P(A = a).\n(2.6.2)\nIt tells us the new probability associated with the event B = b, once we condition on the\nfact A = a took place. We can think of this conditional probability as restricting attention\nonly to the subset of the sample space associated with A = a and then renormalizing so that\nall probabilities sum to 1. Conditional probabilities are in fact just ordinary probabilities and\nthus respect all of the axioms, as long as we condition all terms on the same event and thus\nrestrict attention to the same sample space. For instance, for disjoint events B and B′, we\nhave that P(B ∪B′ | A = a) = P(B | A = a) + P(B′ | A = a).\nUsing the deﬁnition of conditional probabilities, we can derive the famous result called\nBayes’ theorem. By construction, we have that P(A, B) = P(B | A)P(A) and P(A, B) =\nP(A | B)P(B). Combining both equations yields P(B | A)P(A) = P(A | B)P(B) and\nhence\nP(A | B) = P(B | A)P(A)\nP(B)\n.\n(2.6.3)\nThis simple equation has profound implications because it allows us to reverse the order of\nconditioning. If we know how to estimate P(B | A), P(A), and P(B), then we can estimate\nP(A | B). We often ﬁnd it easier to estimate one term directly but not the other and Bayes’\ntheorem can come to the rescue here. For instance, if we know the prevalence of symptoms\nfor a given disease, and the overall prevalences of the disease and symptoms, respectively, we\ncan determine how likely someone is to have the disease based on their symptoms. In some\ncases we might not have direct access to P(B), such as the prevalence of symptoms. In this\ncase a simpliﬁed version of Bayes’ theorem comes in handy:\nP(A | B) ∝P(B | A)P(A).\n(2.6.4)\nSince we know that P(A | B) must be normalized to 1, i.e., ∑\na P(A = a | B) = 1, we can\nuse it to compute\nP(A | B) =\nP(B | A)P(A)\n∑\na P(B | A = a)P(A = a).\n(2.6.5)\nIn Bayesian statistics, we think of an observer as possessing some (subjective) prior beliefs\nabout the plausibility of the available hypotheses encoded in the prior P(H), and a likelihood\nfunction that says how likely one is to observe any value of the collected evidence for each\nof the hypotheses in the class P(E | H). Bayes’ theorem is then interpreted as telling us\nhow to update the initial prior P(H) in light of the available evidence E to produce posterior\nbeliefs P(H | E) = P(E |H)P(H)\nP(E)\n. Informally, this can be stated as “posterior equals prior\ntimes likelihood, divided by the evidence”. Now, because the evidence P(E) is the same for\nall hypotheses, we can get away with simply normalizing over the hypotheses.\nNote that ∑\na P(A = a | B) = 1 also allows us to marginalize over random variables. That is,\nwe can drop variables from a joint distribution such as P(A, B). After all, we have that\n∑\na\nP(B | A = a)P(A = a) =\n∑\na\nP(B, A = a) = P(B).\n(2.6.6)\n\n73\nProbability and Statistics\nIndependence is another fundamentally important concept that forms the backbone of many\nimportant ideas in statistics. In short, two variables are independent if conditioning on the\nvalue of A does not cause any change to the probability distribution associated with B and\nvice versa. More formally, independence, denoted A ⊥B, requires that P(A | B) = P(A)\nand, consequently, that P(A, B) = P(A | B)P(B) = P(A)P(B). Independence is often an\nappropriate assumption. For example, if the random variable A represents the outcome from\ntossing one fair coin and the random variable B represents the outcome from tossing another,\nthen knowing whether A came up heads should not inﬂuence the probability of B coming up\nheads.\nIndependence is especially useful when it holds among the successive draws of our data from\nsome underlying distribution (allowing us to make strong statistical conclusions) or when\nit holds among various variables in our data, allowing us to work with simpler models that\nencode this independence structure. On the other hand, estimating the dependencies among\nrandom variables is often the very aim of learning. We care to estimate the probability of\ndisease given symptoms speciﬁcally because we believe that diseases and symptoms are not\nindependent.\nNote that because conditional probabilities are proper probabilities, the concepts of indepen-\ndence and dependence also apply to them. Two random variables A and B are conditionally\nindependent given a third variable C if and only if P(A, B | C) = P(A | C)P(B | C).\nInterestingly, two variables can be independent in general but become dependent when con-\nditioning on a third. This often occurs when the two random variables A and B correspond\nto causes of some third variable C. For example, broken bones and lung cancer might be\nindependent in the general population but if we condition on being in the hospital then we\nmight ﬁnd that broken bones are negatively correlated with lung cancer. That is because the\nbroken bone explains away why some person is in the hospital and thus lowers the probability\nthat they are hospitalized because of having lung cancer.\nAnd conversely, two dependent random variables can become independent upon conditioning\non a third. This often happens when two otherwise unrelated events have a common cause.\nShoe size and reading level are highly correlated among elementary school students, but this\ncorrelation disappears if we condition on age.\n2.6.5 An Example\nLet’s put our skills to the test. Assume that a doctor administers an HIV test to a patient. This\ntest is fairly accurate and fails only with 1% probability if the patient is healthy but reported\nas diseased, i.e., healthy patients test positive in 1% of cases. Moreover, it never fails to detect\nHIV if the patient actually has it. We use D1 ∈{0, 1} to indicate the diagnosis (0 if negative\nand 1 if positive) and H ∈{0, 1} to denote the HIV status.\nConditional probability\nH = 1\nH = 0\nP(D1 = 1 | H)\n1\n0.01\nP(D1 = 0 | H)\n0\n0.99\n\n74\nPreliminaries\nNote that the column sums are all 1 (but the row sums do not), since they are conditional\nprobabilities. Let’s compute the probability of the patient having HIV if the test comes back\npositive, i.e., P(H = 1 | D1 = 1). Intuitively this is going to depend on how common the\ndisease is, since it aﬀects the number of false alarms. Assume that the population is fairly\nfree of the disease, e.g., P(H = 1) = 0.0015. To apply Bayes’ theorem, we need to apply\nmarginalization to determine\nP(D1 = 1) =P(D1 = 1, H = 0) + P(D1 = 1, H = 1)\n=P(D1 = 1 | H = 0)P(H = 0) + P(D1 = 1 | H = 1)P(H = 1)\n=0.011485.\n(2.6.7)\nThis leads us to\nP(H = 1 | D1 = 1) = P(D1 = 1 | H = 1)P(H = 1)\nP(D1 = 1)\n= 0.1306.\n(2.6.8)\nIn other words, there is only a 13.06% chance that the patient actually has HIV, despite the\ntest being pretty accurate. As we can see, probability can be counterintuitive. What should a\npatient do upon receiving such terrifying news? Likely, the patient would ask the physician\nto administer another test to get clarity. The second test has diﬀerent characteristics and it is\nnot as good as the ﬁrst one.\nConditional probability\nH = 1\nH = 0\nP(D2 = 1 | H)\n0.98\n0.03\nP(D2 = 0 | H)\n0.02\n0.97\nUnfortunately, the second test comes back positive, too. Let’s calculate the requisite proba-\nbilities to invoke Bayes’ theorem by assuming conditional independence:\nP(D1 = 1, D2 = 1 | H = 0) = P(D1 = 1 | H = 0)P(D2 = 1 | H = 0) =\n0.0003,\nP(D1 = 1, D2 = 1 | H = 1) = P(D1 = 1 | H = 1)P(D2 = 1 | H = 1) =\n0.98.\n(2.6.9)\nNow we can apply marginalization to obtain the probability that both tests come back posi-\ntive:\nP(D1 = 1, D2 = 1)\n= P(D1 = 1, D2 = 1, H = 0) + P(D1 = 1, D2 = 1, H = 1)\n= P(D1 = 1, D2 = 1 | H = 0)P(H = 0) + P(D1 = 1, D2 = 1 | H = 1)P(H = 1)\n= 0.00176955.\n(2.6.10)\nFinally, the probability of the patient having HIV given that both tests are positive is\nP(H = 1 | D1 = 1, D2 = 1) = P(D1 = 1, D2 = 1 | H = 1)P(H = 1)\nP(D1 = 1, D2 = 1)\n= 0.8307.\n(2.6.11)\nThat is, the second test allowed us to gain much higher conﬁdence that not all is well. Despite\n\n75\nProbability and Statistics\nthe second test being considerably less accurate than the ﬁrst one, it still signiﬁcantly improved\nour estimate. The assumption of both tests being conditionally independent of each other was\ncrucial for our ability to generate a more accurate estimate. Take the extreme case where we\nrun the same test twice. In this situation we would expect the same outcome both times,\nhence no additional insight is gained from running the same test again. The astute reader\nmight have noticed that the diagnosis behaved like a classiﬁer hiding in plain sight where\nour ability to decide whether a patient is healthy increases as we obtain more features (test\noutcomes).\n2.6.6 Expectations\nOften, making decisions requires not just looking at the probabilities assigned to individual\nevents but composing them together into useful aggregates that can provide us with guid-\nance. For example, when random variables take continuous scalar values, we often care about\nknowing what value to expect on average. This quantity is formally called an expectation. If\nwe are making investments, the ﬁrst quantity of interest might be the return we can expect,\naveraging over all the possible outcomes (and weighting by the appropriate probabilities).\nFor instance, say that with 50% probability, an investment might fail altogether, with 40%\nprobability it might provide a 2× return, and with 10% probability it might provide a 10×\nreturn 10×. To calculate the expected return, we sum over all returns, multiplying each by the\nprobability that they will occur. This yields the expectation 0.5 · 0 + 0.4 · 2 + 0.1 · 10 = 1.8.\nHence the expected return is 1.8×.\nIn general, the expectation (or average) of the random variable X is deﬁned as\nE[X] = Ex∼P[x] =\n∑\nx\nxP(X = x).\n(2.6.12)\nLikewise, for densities we obtain E[X] =\n∫\nx dp(x). Sometimes we are interested in the\nexpected value of some function of x. We can calculate these expectations as\nEx∼P[ f (x)] =\n∑\nx\nf (x)P(x) and Ex∼P[ f (x)] =\n∫\nf (x)p(x) dx\n(2.6.13)\nfor discrete probabilities and densities, respectively. Returning to the investment example\nfrom above, f might be the utility (happiness) associated with the return. Behavior economists\nhave long noted that people associate greater disutility with losing money than the utility\ngained from earning one dollar relative to their baseline. Moreover, the value of money tends\nto be sub-linear. Possessing 100k dollars versus zero dollars can make the diﬀerence between\npaying the rent, eating well, and enjoying quality healthcare versus suﬀering through home-\nlessness. On the other hand, the gains due to possessing 200k versus 100k are less dramatic.\nReasoning like this motivates the cliché that “the utility of money is logarithmic”.\nIf the utility associated with a total loss were −1, and the utilities associated with returns of\n1, 2, and 10 were 1, 2 and 4, respectively, then the expected happiness of investing would be\n0.5 · (−1) + 0.4 · 2 + 0.1 · 4 = 0.7 (an expected loss of utility of 30%). If indeed this were\nyour utility function, you might be best oﬀkeeping the money in the bank.\n\n76\nPreliminaries\nFor ﬁnancial decisions, we might also want to measure how risky an investment is. Here, we\ncare not just about the expected value but how much the actual values tend to vary relative\nto this value. Note that we cannot just take the expectation of the diﬀerence between the\nactual and expected values. This is because the expectation of a diﬀerence is the diﬀerence\nof the expectations, i.e., E[X −E[X]] = E[X] −E[E[X]] = 0. However, we can look at\nthe expectation of any non-negative function of this diﬀerence. The variance of a random\nvariable is calculated by looking at the expected value of the squared diﬀerences:\nVar[X] = E\n[\n(X −E[X])2]\n= E[X2] −E[X]2.\n(2.6.14)\nHere the equality follows by expanding (X −E[X])2 = X2 −2XE[X] + E[X]2 and taking\nexpectations for each term. The square root of the variance is another useful quantity called\nthe standard deviation. While this and the variance convey the same information (either can\nbe calculated from the other), the standard deviation has the nice property that it is expressed\nin the same units as the original quantity represented by the random variable.\nLastly, the variance of a function of a random variable is deﬁned analogously as\nVarx∼P[ f (x)] = Ex∼P[ f 2(x)] −Ex∼P[ f (x)]2.\n(2.6.15)\nReturning to our investment example, we can now compute the variance of the investment.\nIt is given by 0.5 · 0 + 0.4 · 22 + 0.1 · 102 −1.82 = 8.36. For all intents and purposes\nthis is a risky investment. Note that by mathematical convention mean and variance are often\nreferenced as µ and σ2. This is particularly the case whenever we use it to parametrize a\nGaussian distribution.\nIn the same way as we introduced expectations and variance for scalar random variables, we\ncan do so for vector-valued ones. Expectations are easy, since we can apply them elemen-\ntwise. For instance, µ def\n= Ex∼P[x] has coordinates µi = Ex∼P[xi]. Covariances are more\ncomplicated. We deﬁne them by taking expectations of the outer product of the diﬀerence\nbetween random variables and their mean:\nΣ def\n= Covx∼P[x] = Ex∼P\n[\n(x −µ)(x −µ)⊤]\n.\n(2.6.16)\nThis matrix Σ is referred to as the covariance matrix. An easy way to see its eﬀect is to\nconsider some vector v of the same size as x. It follows that\nv⊤Σv = Ex∼P\n[\nv⊤(x −µ)(x −µ)⊤v\n]\n= Varx∼P[v⊤x].\n(2.6.17)\nAs such, Σ allows us to compute the variance for any linear function of x by a simple matrix\nmultiplication. The oﬀ-diagonal elements tell us how correlated the coordinates are: a value\nof 0 means no correlation, where a larger positive value means that they are more strongly\ncorrelated.\n2.6.7 Discussion\nIn machine learning, there are many things to be uncertain about! We can be uncertain about\nthe value of a label given an input. We can be uncertain about the estimated value of a pa-\n\n77\nProbability and Statistics\n59\n60\nrameter. We can even be uncertain about whether data arriving at deployment is even from\nthe same distribution as the training data.\nBy aleatoric uncertainty, we mean uncertainty that is intrinsic to the problem, and due to\ngenuine randomness unaccounted for by the observed variables. By epistemic uncertainty,\nwe mean uncertainty over a model’s parameters, the sort of uncertainty that we can hope to\nreduce by collecting more data. We might have epistemic uncertainty concerning the prob-\nability that a coin turns up heads, but even once we know this probability, we are left with\naleatoric uncertainty about the outcome of any future toss. No matter how long we watch\nsomeone tossing a fair coin, we will never be more or less than 50% certain that the next toss\nwill come up heads. These terms come from mechanical modeling, (see e.g., Der Kiureghian\nand Ditlevsen (2009) for a review on this aspect of uncertainty quantiﬁcation59). It is worth\nnoting, however, that these terms constitute a slight abuse of language. The term epistemic\nrefers to anything concerning knowledge and thus, in the philosophical sense, all uncertainty\nis epistemic.\nWe saw that sampling data from some unknown probability distribution can provide us with\ninformation that can be used to estimate the parameters of the data generating distribution.\nThat said, the rate at which this is possible can be quite slow. In our coin tossing example (and\nmany others) we can do no better than to design estimators that converge at a rate of 1/√n,\nwhere n is the sample size (e.g., the number of tosses). This means that by going from 10 to\n1000 observations (usually a very achievable task) we see a tenfold reduction of uncertainty,\nwhereas the next 1000 observations help comparatively little, oﬀering only a 1.41 times re-\nduction. This is a persistent feature of machine learning: while there are often easy gains, it\ntakes a very large amount of data, and often with it an enormous amount of computation, to\nmake further gains. For an empirical review of this fact for large scale language models see\nRevels et al. (2016).\nWe also sharpened our language and tools for statistical modeling. In the process of that\nwe learned about conditional probabilities and about one of the most important equations\nin statistics—Bayes’ theorem. It is an eﬀective tool for decoupling information conveyed by\ndata through a likelihood term P(B | A) that addresses how well observations B match a\nchoice of parameters A, and a prior probability P(A) which governs how plausible a particular\nchoice of A was in the ﬁrst place. In particular, we saw how this rule can be applied to assign\nprobabilities to diagnoses, based on the eﬃcacy of the test and the prevalence of the disease\nitself (i.e., our prior).\nLastly, we introduced a ﬁrst set of nontrivial questions about the eﬀect of a speciﬁc probability\ndistribution, namely expectations and variances. While there are many more than just linear\nand quadratic expectations for a probability distribution, these two already provide a good\ndeal of knowledge about the possible behavior of the distribution. For instance, Chebyshev’s\ninequality 60 states that P(|X −µ| ≥kσ) ≤1/k2, where µ is the expectation, σ2 is the\nvariance of the distribution, and k > 1 is a conﬁdence parameter of our choosing. It tells us\nthat draws from a distribution lie with at least 50% probability within a [−\n√\n2σ,\n√\n2σ] interval\ncentered on the expectation.\n\n78\nPreliminaries\n61\n62\n2.6.8 Exercises\n1. Give an example where observing more data can reduce the amount of uncertainty about\nthe outcome to an arbitrarily low level.\n2. Give an example where observing more data will only reduce the amount of uncertainty\nup to a point and then no further. Explain why this is the case and where you expect this\npoint to occur.\n3. We empirically demonstrated convergence to the mean for the toss of a coin. Calculate\nthe variance of the estimate of the probability that we see a head after drawing n samples.\n1. How does the variance scale with the number of observations?\n2. Use Chebyshev’s inequality to bound the deviation from the expectation.\n3. How does it relate to the central limit theorem?\n4. Assume that we draw m samples xi from a probability distribution with zero mean and unit\nvariance. Compute the averages zm\ndef\n= m−1 ∑m\ni=1 xi. Can we apply Chebyshev’s inequality\nfor every zm independently? Why not?\n5. Given two events with probability P(A) and P(B), compute upper and lower bounds on\nP(A ∪B) and P(A ∩B). Hint: graph the situation using a Venn diagram61.\n6. Assume that we have a sequence of random variables, say A, B, and C, where B only\ndepends on A, and C only depends on B, can you simplify the joint probability P(A, B, C)?\nHint: this is a Markov chain62.\n7. In Section 2.6.5, assume that the outcomes of the two tests are not independent. In partic-\nular assume that either test on its own has a false positive rate of 10% and a false negative\nrate of 1%. That is, assume that P(D = 1 | H = 0) = 0.1 and that P(D = 0 | H = 1) =\n0.01. Moreover, assume that for H = 1 (infected) the test outcomes are conditionally in-\ndependent, i.e., that P(D1, D2 | H = 1) = P(D1 | H = 1)P(D2 | H = 1) but that for\nhealthy patients the outcomes are coupled via P(D1 = D2 = 1 | H = 0) = 0.02.\n1. Work out the joint probability table for D1 and D2, given H = 0 based on the infor-\nmation you have so far.\n2. Derive the probability that the patient is diseased (H = 1) after one test returns posi-\ntive. You can assume the same baseline probability P(H = 1) = 0.0015 as before.\n3. Derive the probability that the patient is diseased (H = 1) after both tests return\npositive.\n8. Assume that you are an asset manager for an investment bank and you have a choice of\nstocks si to invest in. Your portfolio needs to add up to 1 with weights αi for each stock.\nThe stocks have an average return µ = Es∼P[s] and covariance Σ = Covs∼P[s].\n1. Compute the expected return for a given portfolio α.\n\n79\nDocumentation\n63\n64\n65\n66\n2. If you wanted to maximize the return of the portfolio, how should you choose your\ninvestment?\n3. Compute the variance of the portfolio.\n4. Formulate an optimization problem of maximizing the return while keeping the vari-\nance constrained to an upper bound. This is the Nobel-Prize winning Markovitz port-\nfolio63 (Mangram, 2013). To solve it you will need a quadratic programming solver,\nsomething way beyond the scope of this book.\nDiscussions64.\n2.7 Documentation\nWhile we cannot possibly introduce every single PyTorch function and class (and the infor-\nmation might become outdated quickly), the API documentation 65 and additional tutorials\n66 and examples provide such documentation. This section provides some guidance for how\nto explore the PyTorch API.\nimport torch\n2.7.1 Functions and Classes in a Module\nTo know which functions and classes can be called in a module, we invoke the dir func-\ntion. For instance, we can query all properties in the module for generating random num-\nbers:\nprint(dir(torch.distributions))\n['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial',\n,→'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform',\n,→'ContinuousBernoulli', 'CorrCholeskyTransform',\n,→'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform\n,→', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric\n,→', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform\n,→', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal',\n,→'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily',\n,→'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal',\n,→'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson',\n,→'PositiveDefiniteTransform', 'PowerTransform', 'RelaxedBernoulli',\n,→'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform',\n,→'SoftmaxTransform', 'SoftplusTransform', 'StackTransform',\n,→'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform',\n(continues on next page)\n\n80\nPreliminaries\n(continued from previous page)\n,→'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__\n,→all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '_\n,→_name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta',\n,→'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_\n,→registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution\n,→', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric',\n,→'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent',\n,→'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal\n,→', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family',\n,→'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_\n,→hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli',\n,→'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution\n,→', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\nGenerally, we can ignore functions that start and end with __ (special objects in Python) or\nfunctions that start with a single _(usually internal functions). Based on the remaining function\nor attribute names, we might hazard a guess that this module oﬀers various methods for\ngenerating random numbers, including sampling from the uniform distribution (uniform),\nnormal distribution (normal), and multinomial distribution (multinomial).\n2.7.2 Speciﬁc Functions and Classes\nFor speciﬁc instructions on how to use a given function or class, we can invoke the help func-\ntion. As an example, let’s explore the usage instructions for tensors’ ones function.\nhelp(torch.ones)\nHelp on built-in function ones in module torch:\nones(...)\nones(*size, *, out=None, dtype=None, layout=torch.strided, device=None,\n,→requires_grad=False) -> Tensor\nReturns a tensor filled with the scalar value 1, with the shape defined\nby the variable argument size.\nArgs:\nsize (int...): a sequence of integers defining the shape of the␣\n,→output tensor.\nCan be a variable number of arguments or a collection like a␣\n,→list or tuple.\nKeyword arguments:\nout (Tensor, optional): the output tensor.\ndtype (torch.dtype, optional): the desired data type of returned␣\n,→tensor.\n\n81\nDocumentation\n67\nDefault: if None, uses a global default (see torch.set_default_\n,→tensor_type()).\nlayout (torch.layout, optional): the desired layout of returned␣\n,→Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional): the desired device of returned␣\n,→tensor.\nDefault: if None, uses the current device for the default tensor␣\n,→type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor␣\n,→types.\nrequires_grad (bool, optional): If autograd should record operations␣\n,→on the\nreturned tensor. Default: False.\nExample::\n>>> torch.ones(2, 3)\ntensor([[ 1.,\n1.,\n1.],\n[ 1.,\n1.,\n1.]])\n>>> torch.ones(5)\ntensor([ 1.,\n1.,\n1.,\n1.,\n1.])\nFrom the documentation, we can see that the ones function creates a new tensor with the\nspeciﬁed shape and sets all the elements to the value of 1. Whenever possible, you should run\na quick test to conﬁrm your interpretation:\ntorch.ones(4)\ntensor([1., 1., 1., 1.])\nIn the Jupyter notebook, we can use ? to display the document in another window. For ex-\nample, list? will create content that is almost identical to help(list), displaying it in a\nnew browser window. In addition, if we use two question marks, such as list??, the Python\ncode implementing the function will also be displayed.\nThe oﬃcial documentation provides plenty of descriptions and examples that are beyond\nthis book. We emphasize important use cases that will get you started quickly with practical\nproblems, rather than completeness of coverage. We also encourage you to study the source\ncode of the libraries to see examples of high-quality implementations of production code. By\ndoing this you will become a better engineer in addition to becoming a better scientist.\nDiscussions67.\n\n3\nLinear Neural Networks for Regression\nBefore we worry about making our neural networks deep, it will be helpful to implement\nsome shallow ones, for which the inputs connect directly to the outputs. This will prove im-\nportant for a few reasons. First, rather than getting distracted by complicated architectures,\nwe can focus on the basics of neural network training, including parametrizing the output\nlayer, handling data, specifying a loss function, and training the model. Second, this class of\nshallow networks happens to comprise the set of linear models, which subsumes many classi-\ncal methods of statistical prediction, including linear and softmax regression. Understanding\nthese classical tools is pivotal because they are widely used in many contexts and we will often\nneed to use them as baselines when justifying the use of fancier architectures. This chapter\nwill focus narrowly on linear regression and the next one will extend our modeling repertoire\nby developing linear neural networks for classiﬁcation.\n3.1 Linear Regression\nRegression problems pop up whenever we want to predict a numerical value. Common ex-\namples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for\npatients in the hospital), forecasting demand (for retail sales), among numerous others. Not\nevery prediction problem is one of classical regression. Later on, we will introduce classiﬁ-\ncation problems, where the goal is to predict membership among a set of categories.\nAs a running example, suppose that we wish to estimate the prices of houses (in dollars)\nbased on their area (in square feet) and age (in years). To develop a model for predicting\nhouse prices, we need to get our hands on data, including the sales price, area, and age for\neach home. In the terminology of machine learning, the dataset is called a training dataset or\ntraining set, and each row (containing the data corresponding to one sale) is called an example\n(or data point, instance, sample). The thing we are trying to predict (price) is called a label (or\ntarget). The variables (age and area) upon which the predictions are based are called features\n(or covariates).\n%matplotlib inline\nimport math\n(continues on next page)\n82\n\n83\nLinear Regression\n(continued from previous page)\nimport time\nimport numpy as np\nimport torch\nfrom d2l import torch as d2l\n3.1.1 Basics\nLinear regression is both the simplest and most popular among the standard tools for tackling\nregression problems. Dating back to the dawn of the 19th century (Gauss, 1809, Legendre,\n1805), linear regression ﬂows from a few simple assumptions. First, we assume that the re-\nlationship between features x and target y is approximately linear, i.e., that the conditional\nmean E[Y | X = x] can be expressed as a weighted sum of the features x. This setup allows\nthat the target value may still deviate from its expected value on account of observation noise.\nNext, we can impose the assumption that any such noise is well behaved, following a Gaus-\nsian distribution. Typically, we will use n to denote the number of examples in our dataset.\nWe use superscripts to enumerate samples and targets, and subscripts to index coordinates.\nMore concretely, x(i) denotes the ith sample and x(i)\nj\ndenotes its jth coordinate.\nModel\nAt the heart of every solution is a model that describes how features can be transformed into\nan estimate of the target. The assumption of linearity means that the expected value of the\ntarget (price) can be expressed as a weighted sum of the features (area and age):\nprice = warea · area + wage · age + b.\n(3.1.1)\nHere warea and wage are called weights, and b is called a bias (or oﬀset or intercept). The\nweights determine the inﬂuence of each feature on our prediction. The bias determines the\nvalue of the estimate when all features are zero. Even though we will never see any newly-\nbuilt homes with precisely zero area, we still need the bias because it allows us to express all\nlinear functions of our features (rather than restricting us to lines that pass through the origin).\nStrictly speaking, (3.1.1) is an aﬃne transformation of input features, which is characterized\nby a linear transformation of features via a weighted sum, combined with a translation via\nthe added bias. Given a dataset, our goal is to choose the weights w and the bias b that, on\naverage, make our model’s predictions ﬁt the true prices observed in the data as closely as\npossible.\nIn disciplines where it is common to focus on datasets with just a few features, explicitly\nexpressing models long-form, as in (3.1.1), is common. In machine learning, we usually work\nwith high-dimensional datasets, where it is more convenient to employ compact linear algebra\nnotation. When our inputs consist of d features, we can assign each an index (between 1 and\nd) and express our prediction ˆy (in general the “hat” symbol denotes an estimate) as\nˆy = w1x1 + · · · + wdxd + b.\n(3.1.2)\n\n84\nLinear Neural Networks for Regression\nCollecting all features into a vector x ∈Rd and all weights into a vector w ∈Rd, we can\nexpress our model compactly via the dot product between w and x:\nˆy = w⊤x + b.\n(3.1.3)\nIn (3.1.3), the vector x corresponds to the features of a single example. We will often ﬁnd\nit convenient to refer to features of our entire dataset of n examples via the design matrix\nX ∈Rn×d. Here, X contains one row for every example and one column for every feature.\nFor a collection of features X, the predictions ˆy ∈Rn can be expressed via the matrix–vector\nproduct:\nˆy = Xw + b,\n(3.1.4)\nwhere broadcasting (Section 2.1.4) is applied during the summation. Given features of a\ntraining dataset X and corresponding (known) labels y, the goal of linear regression is to\nﬁnd the weight vector w and the bias term b such that, given features of a new data example\nsampled from the same distribution as X, the new example’s label will (in expectation) be\npredicted with the smallest error.\nEven if we believe that the best model for predicting y given x is linear, we would not expect\nto ﬁnd a real-world dataset of n examples where y(i) exactly equals w⊤x(i) + b for all 1 ≤\ni ≤n. For example, whatever instruments we use to observe the features X and labels y,\nthere might be a small amount of measurement error. Thus, even when we are conﬁdent that\nthe underlying relationship is linear, we will incorporate a noise term to account for such\nerrors.\nBefore we can go about searching for the best parameters (or model parameters) w and b,\nwe will need two more things: (i) a measure of the quality of some given model; and (ii) a\nprocedure for updating the model to improve its quality.\nLoss Function\nNaturally, ﬁtting our model to the data requires that we agree on some measure of ﬁtness\n(or, equivalently, of unﬁtness). Loss functions quantify the distance between the real and\npredicted values of the target. The loss will usually be a nonnegative number where smaller\nvalues are better and perfect predictions incur a loss of 0. For regression problems, the most\ncommon loss function is the squared error. When our prediction for an example i is ˆy(i) and\nthe corresponding true label is y(i), the squared error is given by:\nl(i)(w, b) = 1\n2\n(\nˆy(i) −y(i))2\n.\n(3.1.5)\nThe constant 1\n2 makes no real diﬀerence but proves to be notationally convenient, since it\ncancels out when we take the derivative of the loss. Because the training dataset is given\nto us, and thus is out of our control, the empirical error is only a function of the model\nparameters. In Fig. 3.1.1, we visualize the ﬁt of a linear regression model in a problem with\none-dimensional inputs.\n\n85\nLinear Regression\nt\nFig. 3.1.1\nFitting a linear regression model to one-dimensional data.\nNote that large diﬀerences between estimates ˆy(i) and targets y(i) lead to even larger contri-\nbutions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword;\nwhile it encourages the model to avoid large errors it can also lead to excessive sensitivity to\nanomalous data). To measure the quality of a model on the entire dataset of n examples, we\nsimply average (or equivalently, sum) the losses on the training set:\nL(w, b) = 1\nn\nn\n∑\ni=1\nl(i)(w, b) = 1\nn\nn\n∑\ni=1\n1\n2\n(\nw⊤x(i) + b −y(i))2\n.\n(3.1.6)\nWhen training the model, we seek parameters (w∗, b∗) that minimize the total loss across all\ntraining examples:\nw∗, b∗= argmin\nw,b\nL(w, b).\n(3.1.7)\nAnalytic Solution\nUnlike most of the models that we will cover, linear regression presents us with a surprisingly\neasy optimization problem. In particular, we can ﬁnd the optimal parameters (as assessed on\nthe training data) analytically by applying a simple formula as follows. First, we can subsume\nthe bias b into the parameter w by appending a column to the design matrix consisting of all\n1s. Then our prediction problem is to minimize ∥y −Xw∥2. As long as the design matrix\nX has full rank (no feature is linearly dependent on the others), then there will be just one\ncritical point on the loss surface and it corresponds to the minimum of the loss over the\nentire domain. Taking the derivative of the loss with respect to w and setting it equal to zero\nyields:\n∂w∥y −Xw∥2 = 2X⊤(Xw −y) = 0 and hence X⊤y = X⊤Xw.\n(3.1.8)\nSolving for w provides us with the optimal solution for the optimization problem. Note that\nthis solution\nw∗= (X⊤X)−1X⊤y\n(3.1.9)\nwill only be unique when the matrix X⊤X is invertible, i.e., when the columns of the design\nmatrix are linearly independent (Golub and Van Loan, 1996).\n\n86\nLinear Neural Networks for Regression\nWhile simple problems like linear regression may admit analytic solutions, you should not get\nused to such good fortune. Although analytic solutions allow for nice mathematical analysis,\nthe requirement of an analytic solution is so restrictive that it would exclude almost all exciting\naspects of deep learning.\nMinibatch Stochastic Gradient Descent\nFortunately, even in cases where we cannot solve the models analytically, we can still often\ntrain models eﬀectively in practice. Moreover, for many tasks, those hard-to-optimize models\nturn out to be so much better that ﬁguring out how to train them ends up being well worth\nthe trouble.\nThe key technique for optimizing nearly every deep learning model, and which we will call\nupon throughout this book, consists of iteratively reducing the error by updating the param-\neters in the direction that incrementally lowers the loss function. This algorithm is called\ngradient descent.\nThe most naive application of gradient descent consists of taking the derivative of the loss\nfunction, which is an average of the losses computed on every single example in the dataset.\nIn practice, this can be extremely slow: we must pass over the entire dataset before making\na single update, even if the update steps might be very powerful (Liu and Nocedal, 1989).\nEven worse, if there is a lot of redundancy in the training data, the beneﬁt of a full update is\nlimited.\nThe other extreme is to consider only a single example at a time and to take update steps based\non one observation at a time. The resulting algorithm, stochastic gradient descent (SGD) can\nbe an eﬀective strategy (Bottou, 2010), even for large datasets. Unfortunately, SGD has draw-\nbacks, both computational and statistical. One problem arises from the fact that processors are\na lot faster multiplying and adding numbers than they are at moving data from main memory\nto processor cache. It is up to an order of magnitude more eﬃcient to perform a matrix–\nvector multiplication than a corresponding number of vector–vector operations. This means\nthat it can take a lot longer to process one sample at a time compared to a full batch. A second\nproblem is that some of the layers, such as batch normalization (to be described in Section\n8.5), only work well when we have access to more than one observation at a time.\nThe solution to both problems is to pick an intermediate strategy: rather than taking a full\nbatch or only a single sample at a time, we take a minibatch of observations (Li et al., 2014).\nThe speciﬁc choice of the size of the said minibatch depends on many factors, such as the\namount of memory, the number of accelerators, the choice of layers, and the total dataset\nsize. Despite all that, a number between 32 and 256, preferably a multiple of a large power\nof 2, is a good start. This leads us to minibatch stochastic gradient descent.\nIn its most basic form, in each iteration t, we ﬁrst randomly sample a minibatch Bt consisting\nof a ﬁxed number |B| of training examples. We then compute the derivative (gradient) of\nthe average loss on the minibatch with respect to the model parameters. Finally, we mul-\ntiply the gradient by a predetermined small positive value η, called the learning rate, and\n\n87\nLinear Regression\nsubtract the resulting term from the current parameter values. We can express the update as\nfollows:\n(w, b) ←(w, b) −η\n|B|\n∑\ni∈Bt\n∂(w,b)l(i)(w, b).\n(3.1.10)\nIn summary, minibatch SGD proceeds as follows: (i) initialize the values of the model param-\neters, typically at random; (ii) iteratively sample random minibatches from the data, updating\nthe parameters in the direction of the negative gradient. For quadratic losses and aﬃne trans-\nformations, this has a closed-form expansion:\nw ←w −η\n|B|\n∑\ni∈Bt\n∂wl(i)(w, b)\n= w −η\n|B|\n∑\ni∈Bt\nx(i) (\nw⊤x(i) + b −y(i))\nb ←b −η\n|B|\n∑\ni∈Bt\n∂bl(i)(w, b)\n= b −η\n|B|\n∑\ni∈Bt\n(\nw⊤x(i) + b −y(i))\n.\n(3.1.11)\nSince we pick a minibatch B we need to normalize by its size |B|. Frequently minibatch\nsize and learning rate are user-deﬁned. Such tunable parameters that are not updated in the\ntraining loop are called hyperparameters. They can be tuned automatically by a number of\ntechniques, such as Bayesian optimization (Frazier, 2018). In the end, the quality of the solu-\ntion is typically assessed on a separate validation dataset (or validation set).\nAfter training for some predetermined number of iterations (or until some other stopping cri-\nterion is met), we record the estimated model parameters, denoted ˆw, ˆb. Note that even if our\nfunction is truly linear and noiseless, these parameters will not be the exact minimizers of the\nloss, nor even deterministic. Although the algorithm converges slowly towards the minimizers\nit typically will not ﬁnd them exactly in a ﬁnite number of steps. Moreover, the minibatches\nB used for updating the parameters are chosen at random. This breaks determinism.\nLinear regression happens to be a learning problem with a global minimum (whenever X\nis full rank, or equivalently, whenever X⊤X is invertible). However, the loss surfaces for\ndeep networks contain many saddle points and minima. Fortunately, we typically do not care\nabout ﬁnding an exact set of parameters but merely any set of parameters that leads to accurate\npredictions (and thus low loss). In practice, deep learning practitioners seldom struggle to ﬁnd\nparameters that minimize the loss on training sets (Frankle and Carbin, 2018, Izmailov et al.,\n2018). The more formidable task is to ﬁnd parameters that lead to accurate predictions on\npreviously unseen data, a challenge called generalization. We return to these topics throughout\nthe book.\nPredictions\nGiven the model ˆw⊤x + ˆb, we can now make predictions for a new example, e.g., predicting\nthe sales price of a previously unseen house given its area x1 and age x2. Deep learning prac-\ntitioners have taken to calling the prediction phase inference but this is a bit of a misnomer—\ninference refers broadly to any conclusion reached on the basis of evidence, including both\nthe values of the parameters and the likely label for an unseen instance. If anything, in the\nstatistics literature inference more often denotes parameter inference and this overloading of\n\n88\nLinear Neural Networks for Regression\nterminology creates unnecessary confusion when deep learning practitioners talk to statisti-\ncians. In the following we will stick to prediction whenever possible.\n3.1.2 Vectorization for Speed\nWhen training our models, we typically want to process whole minibatches of examples si-\nmultaneously. Doing this eﬃciently requires that we vectorize the calculations and leverage\nfast linear algebra libraries rather than writing costly for-loops in Python.\nTo see why this matters so much, let’s consider two methods for adding vectors. To start,\nwe instantiate two 10,000-dimensional vectors containing all 1s. In the ﬁrst method, we loop\nover the vectors with a Python for-loop. In the second, we rely on a single call to +.\nn = 10000\na = torch.ones(n)\nb = torch.ones(n)\nNow we can benchmark the workloads. First, we add them, one coordinate at a time, using\na for-loop.\nc = torch.zeros(n)\nt = time.time()\nfor i in range(n):\nc[i] = a[i] + b[i]\nf'{time.time() - t:.5f} sec'\n'0.16781 sec'\nAlternatively, we rely on the reloaded + operator to compute the elementwise sum.\nt = time.time()\nd = a + b\nf'{time.time() - t:.5f} sec'\n'0.00180 sec'\nThe second method is dramatically faster than the ﬁrst. Vectorizing code often yields order-\nof-magnitude speedups. Moreover, we push more of the mathematics to the library so we\ndo not have to write as many calculations ourselves, reducing the potential for errors and\nincreasing portability of the code.\n3.1.3 The Normal Distribution and Squared Loss\nSo far we have given a fairly functional motivation of the squared loss objective: the optimal\nparameters return the conditional expectation E[Y | X] whenever the underlying pattern is\n\n89\nLinear Regression\ntruly linear, and the loss assigns large penalties for outliers. We can also provide a more\nformal motivation for the squared loss objective by making probabilistic assumptions about\nthe distribution of noise.\nLinear regression was invented at the turn of the 19th century. While it has long been debated\nwhether Gauss or Legendre ﬁrst thought up the idea, it was Gauss who also discovered the\nnormal distribution (also called the Gaussian). It turns out that the normal distribution and\nlinear regression with squared loss share a deeper connection than common parentage.\nTo begin, recall that a normal distribution with mean µ and variance σ2 (standard deviation\nσ) is given as\np(x) =\n1\n√\n2πσ2 exp\n(\n−1\n2σ2 (x −µ)2\n)\n.\n(3.1.12)\nBelow we deﬁne a function to compute the normal distribution.\ndef normal(x, mu, sigma):\np = 1 / math.sqrt(2 * math.pi * sigma**2)\nreturn p * np.exp(-0.5 * (x - mu)**2 / sigma**2)\nWe can now visualize the normal distributions.\n# Use NumPy again for visualization\nx = np.arange(-7, 7, 0.01)\n# Mean and standard deviation pairs\nparams = [(0, 1), (0, 2), (3, 1)]\nd2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\nylabel='p(x)', figsize=(4.5, 2.5),\nlegend=[f'mean {mu}, std {sigma}' for mu, sigma in params])\nNote that changing the mean corresponds to a shift along the x-axis, and increasing the vari-\nance spreads the distribution out, lowering its peak.\nOne way to motivate linear regression with squared loss is to assume that observations arise\n\n90\nLinear Neural Networks for Regression\nfrom noisy measurements, where the noise ϵ follows the normal distribution N(0, σ2):\ny = w⊤x + b + ϵ where ϵ ∼N(0, σ2).\n(3.1.13)\nThus, we can now write out the likelihood of seeing a particular y for a given x via\nP(y | x) =\n1\n√\n2πσ2 exp\n(\n−1\n2σ2 (y −w⊤x −b)2\n)\n.\n(3.1.14)\nAs such, the likelihood factorizes. According to the principle of maximum likelihood, the\nbest values of parameters w and b are those that maximize the likelihood of the entire\ndataset:\nP(y | X) =\nn\n∏\ni=1\np(y(i) | x(i)).\n(3.1.15)\nThe equality follows since all pairs (x(i), y(i)) were drawn independently of each other. Es-\ntimators chosen according to the principle of maximum likelihood are called maximum like-\nlihood estimators. While, maximizing the product of many exponential functions, might look\ndiﬃcult, we can simplify things signiﬁcantly, without changing the objective, by maximizing\nthe logarithm of the likelihood instead. For historical reasons, optimizations are more often\nexpressed as minimization rather than maximization. So, without changing anything, we can\nminimize the negative log-likelihood, which we can express as follows:\n−log P(y | X) =\nn\n∑\ni=1\n1\n2 log(2πσ2) +\n1\n2σ2\n(\ny(i) −w⊤x(i) −b\n)2\n.\n(3.1.16)\nIf we assume that σ is ﬁxed, we can ignore the ﬁrst term, because it does not depend on w\nor b. The second term is identical to the squared error loss introduced earlier, except for the\nmultiplicative constant\n1\nσ2 . Fortunately, the solution does not depend on σ either. It follows\nthat minimizing the mean squared error is equivalent to the maximum likelihood estimation\nof a linear model under the assumption of additive Gaussian noise.\n3.1.4 Linear Regression as a Neural Network\nWhile linear models are not suﬃciently rich to express the many complicated networks that\nwe will introduce in this book, (artiﬁcial) neural networks are rich enough to subsume linear\nmodels as networks in which every feature is represented by an input neuron, all of which are\nconnected directly to the output.\nFig. 3.1.2 depicts linear regression as a neural network. The diagram highlights the connec-\ntivity pattern, such as how each input is connected to the output, but not the speciﬁc values\ntaken by the weights or biases.\nThe inputs are x1, . . ., xd. We refer to d as the number of inputs or the feature dimensionality\nin the input layer. The output of the network is o1. Because we are just trying to predict a\nsingle numerical value, we have only one output neuron. Note that the input values are all\ngiven. There is just a single computed neuron. In summary, we can think of linear regression\nas a single-layer fully connected neural network. We will encounter networks with far more\nlayers in later chapters.\n\n91\nLinear Regression\nt\nFig. 3.1.2\nLinear regression is a single-layer neural network.\nBiology\nBecause linear regression predates computational neuroscience, it might seem anachronistic\nto describe linear regression in terms of neural networks. Nonetheless, they were a natural\nplace to start when the cyberneticists and neurophysiologists Warren McCulloch and Walter\nPitts began to develop models of artiﬁcial neurons. Consider the cartoonish picture of a bio-\nlogical neuron in Fig. 3.1.3, consisting of dendrites (input terminals), the nucleus (CPU), the\naxon (output wire), and the axon terminals (output terminals), enabling connections to other\nneurons via synapses.\nDendrite\nCell body\nNode of\nRanvier\nAxon Terminal\nSchwann cell\nMyelin sheath\nAxon\nNucleus\nt\nFig. 3.1.3\nThe real neuron (source: “Anatomy and Physiology” by the US National Cancer\nInstitute’s Surveillance, Epidemiology and End Results (SEER) Program).\nInformation xi arriving from other neurons (or environmental sensors) is received in the\ndendrites. In particular, that information is weighted by synaptic weights wi, determining the\neﬀect of the inputs, e.g., activation or inhibition via the product xiwi. The weighted inputs ar-\nriving from multiple sources are aggregated in the nucleus as a weighted sum y = ∑\ni xiwi+b,\npossibly subject to some nonlinear postprocessing via a function σ(y). This information is\nthen sent via the axon to the axon terminals, where it reaches its destination (e.g., an actuator\nsuch as a muscle) or it is fed into another neuron via its dendrites.\nCertainly, the high-level idea that many such units could be combined, provided they have\nthe correct connectivity and learning algorithm, to produce far more interesting and complex\nbehavior than any one neuron alone could express arises from our study of real biological\nneural systems. At the same time, most research in deep learning today draws inspiration from\na much wider source. We invoke Russell and Norvig (2016) who pointed out that although\nairplanes might have been inspired by birds, ornithology has not been the primary driver\n\n92\nLinear Neural Networks for Regression\nof aeronautics innovation for some centuries. Likewise, inspiration in deep learning these\ndays comes in equal or greater measure from mathematics, linguistics, psychology, statistics,\ncomputer science, and many other ﬁelds.\n3.1.5 Summary\nIn this section, we introduced traditional linear regression, where the parameters of a lin-\near function are chosen to minimize squared loss on the training set. We also motivated this\nchoice of objective both via some practical considerations and through an interpretation of\nlinear regression as maximimum likelihood estimation under an assumption of linearity and\nGaussian noise. After discussing both computational considerations and connections to statis-\ntics, we showed how such linear models could be expressed as simple neural networks where\nthe inputs are directly wired to the output(s). While we will soon move past linear models\naltogether, they are suﬃcient to introduce most of the components that all of our models\nrequire: parametric forms, diﬀerentiable objectives, optimization via minibatch stochastic\ngradient descent, and ultimately, evaluation on previously unseen data.\n3.1.6 Exercises\n1. Assume that we have some data x1, . . ., xn ∈R. Our goal is to ﬁnd a constant b such that\n∑\ni(xi −b)2 is minimized.\n1. Find an analytic solution for the optimal value of b.\n2. How does this problem and its solution relate to the normal distribution?\n3. What if we change the loss from ∑\ni(xi −b)2 to ∑\ni |xi −b|? Can you ﬁnd the optimal\nsolution for b?\n2. Prove that the aﬃne functions that can be expressed by x⊤w + b are equivalent to linear\nfunctions on (x, 1).\n3. Assume that you want to ﬁnd quadratic functions of x, i.e., f (x) = b + ∑\ni wixi +\n∑\nj ≤i wij xixj. How would you formulate this in a deep network?\n4. Recall that one of the conditions for the linear regression problem to be solvable was that\nthe design matrix X⊤X has full rank.\n1. What happens if this is not the case?\n2. How could you ﬁx it? What happens if you add a small amount of coordinate-wise\nindependent Gaussian noise to all entries of X?\n3. What is the expected value of the design matrix X⊤X in this case?\n4. What happens with stochastic gradient descent when X⊤X does not have full rank?\n5. Assume that the noise model governing the additive noise ϵ is the exponential distribution.\nThat is, p(ϵ) = 1\n2 exp(−|ϵ|).\n\n93\nObject-Oriented Design for Implementation\n68\n69\n1. Write out the negative log-likelihood of the data under the model −log P(y | X).\n2. Can you ﬁnd a closed form solution?\n3. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What\ncould possibly go wrong (hint: what happens near the stationary point as we keep on\nupdating the parameters)? Can you ﬁx this?\n6. Assume that we want to design a neural network with two layers by composing two linear\nlayers. That is, the output of the ﬁrst layer becomes the input of the second layer. Why\nwould such a naive composition not work?\n7. What happens if you want to use regression for realistic price estimation of houses or\nstock prices?\n1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have\nnegative prices? What about ﬂuctuations?\n2. Why would regression to the logarithm of the price be much better, i.e., y = log price?\n3. What do you need to worry about when dealing with pennystock, i.e., stock with very\nlow prices? Hint: can you trade at all possible prices? Why is this a bigger problem\nfor cheap stock? For more information review the celebrated Black–Scholes model for\noption pricing (Black and Scholes, 1973).\n8. Suppose we want to use regression to estimate the number of apples sold in a grocery\nstore.\n1. What are the problems with a Gaussian additive noise model? Hint: you are selling\napples, not oil.\n2. The Poisson distribution68 captures distributions over counts. It is given by p(k | λ) =\nλke−λ/k!. Here λ is the rate function and k is the number of events you see. Prove\nthat λ is the expected value of counts k.\n3. Design a loss function associated with the Poisson distribution.\n4. Design a loss function for estimating log λ instead.\nDiscussions69.\n3.2 Object-Oriented Design for Implementation\nIn our introduction to linear regression, we walked through various components including the\ndata, the model, the loss function, and the optimization algorithm. Indeed, linear regression\nis one of the simplest machine learning models. Training it, however, uses many of the same\n\n94\nLinear Neural Networks for Regression\n70\ncomponents that other models in this book require. Therefore, before diving into the imple-\nmentation details it is worth designing some of the APIs that we use throughout. Treating\ncomponents in deep learning as objects, we can start by deﬁning classes for these objects and\ntheir interactions. This object-oriented design for implementation will greatly streamline the\npresentation and you might even want to use it in your projects.\nInspired by open-source libraries such as PyTorch Lightning70, at a high level we wish to have\nthree classes: (i) Module contains models, losses, and optimization methods; (ii) DataModule\nprovides data loaders for training and validation; (iii) both classes are combined using the\nTrainer class, which allows us to train models on a variety of hardware platforms. Most\ncode in this book adapts Module and DataModule. We will touch upon the Trainer class\nonly when we discuss GPUs, CPUs, parallel training, and optimization algorithms.\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n3.2.1 Utilities\nWe need a few utilities to simplify object-oriented programming in Jupyter notebooks. One\nof the challenges is that class deﬁnitions tend to be fairly long blocks of code. Notebook\nreadability demands short code fragments, interspersed with explanations, a requirement in-\ncompatible with the style of programming common for Python libraries. The ﬁrst utility\nfunction allows us to register functions as methods in a class after the class has been created.\nIn fact, we can do so even after we have created instances of the class! It allows us to split the\nimplementation of a class into multiple code blocks.\ndef add_to_class(Class):\n#@save\n\"\"\"Register functions as methods in created class.\"\"\"\ndef wrapper(obj):\nsetattr(Class, obj.__name__, obj)\nreturn wrapper\nLet’s have a quick look at how to use it. We plan to implement a class A with a method do.\nInstead of having code for both A and do in the same code block, we can ﬁrst declare the\nclass A and create an instance a.\nclass A:\ndef __init__(self):\nself.b = 1\na = A()\nNext we deﬁne the method do as we normally would, but not in class A’s scope. Instead, we\ndecorate this method by add_to_class with class A as its argument. In doing so, the method\n\n95\nObject-Oriented Design for Implementation\n71\nis able to access the member variables of A just as we would expect had it been included as\npart of A’s deﬁnition. Let’s see what happens when we invoke it for the instance a.\n@add_to_class(A)\ndef do(self):\nprint('Class attribute \"b\" is', self.b)\na.do()\nClass attribute \"b\" is 1\nThe second one is a utility class that saves all arguments in a class’s __init__ method as class\nattributes. This allows us to extend constructor call signatures implicitly without additional\ncode.\nclass HyperParameters:\n#@save\n\"\"\"The base class of hyperparameters.\"\"\"\ndef save_hyperparameters(self, ignore=[]):\nraise NotImplemented\nWe defer its implementation into Section B.7. To use it, we deﬁne our class that inherits from\nHyperParameters and calls save_hyperparameters in the __init__ method.\n# Call the fully implemented HyperParameters class saved in d2l\nclass B(d2l.HyperParameters):\ndef __init__(self, a, b, c):\nself.save_hyperparameters(ignore=['c'])\nprint('self.a =', self.a, 'self.b =', self.b)\nprint('There is no self.c =', not hasattr(self, 'c'))\nb = B(a=1, b=2, c=3)\nself.a = 1 self.b = 2\nThere is no self.c = True\nThe ﬁnal utility allows us to plot experiment progress interactively while it is going on. In\ndeference to the much more powerful (and complex) TensorBoard71 we name it Progress-\nBoard. The implementation is deferred to Section B.7. For now, let’s simply see it in ac-\ntion.\nThe draw method plots a point (x, y) in the ﬁgure, with label speciﬁed in the legend. The\noptional every_n smooths the line by only showing 1/n points in the ﬁgure. Their values are\naveraged from the n neighbor points in the original ﬁgure.\nclass ProgressBoard(d2l.HyperParameters):\n#@save\n\"\"\"The board that plots data points in animation.\"\"\"\ndef __init__(self, xlabel=None, ylabel=None, xlim=None,\nylim=None, xscale='linear', yscale='linear',\n(continues on next page)\n\n96\nLinear Neural Networks for Regression\n(continued from previous page)\nls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\nfig=None, axes=None, figsize=(3.5, 2.5), display=True):\nself.save_hyperparameters()\ndef draw(self, x, y, label, every_n=1):\nraise NotImplemented\nIn the following example, we draw sin and cos with a diﬀerent smoothness. If you run this\ncode block, you will see the lines grow in animation.\nboard = d2l.ProgressBoard('x')\nfor x in np.arange(0, 10, 0.1):\nboard.draw(x, np.sin(x), 'sin', every_n=2)\nboard.draw(x, np.cos(x), 'cos', every_n=10)\n3.2.2 Models\nThe Module class is the base class of all models we will implement. At the very least we need\nthree methods. The ﬁrst, __init__, stores the learnable parameters, the training_step\nmethod accepts a data batch to return the loss value, and ﬁnally, configure_optimizers\nreturns the optimization method, or a list of them, that is used to update the learnable pa-\nrameters. Optionally we can deﬁne validation_step to report the evaluation measures.\nSometimes we put the code for computing the output into a separate forward method to\nmake it more reusable.\nclass Module(nn.Module, d2l.HyperParameters):\n#@save\n\"\"\"The base class of models.\"\"\"\ndef __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\nsuper().__init__()\nself.save_hyperparameters()\nself.board = ProgressBoard()\ndef loss(self, y_hat, y):\n(continues on next page)\n\n97\nObject-Oriented Design for Implementation\n(continued from previous page)\nraise NotImplementedError\ndef forward(self, X):\nassert hasattr(self, 'net'), 'Neural network is defined'\nreturn self.net(X)\ndef plot(self, key, value, train):\n\"\"\"Plot a point in animation.\"\"\"\nassert hasattr(self, 'trainer'), 'Trainer is not inited'\nself.board.xlabel = 'epoch'\nif train:\nx = self.trainer.train_batch_idx / \\\nself.trainer.num_train_batches\nn = self.trainer.num_train_batches / \\\nself.plot_train_per_epoch\nelse:\nx = self.trainer.epoch + 1\nn = self.trainer.num_val_batches / \\\nself.plot_valid_per_epoch\nself.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n('train_' if train else 'val_') + key,\nevery_n=int(n))\ndef training_step(self, batch):\nl = self.loss(self(*batch[:-1]), batch[-1])\nself.plot('loss', l, train=True)\nreturn l\ndef validation_step(self, batch):\nl = self.loss(self(*batch[:-1]), batch[-1])\nself.plot('loss', l, train=False)\ndef configure_optimizers(self):\nraise NotImplementedError\nYou may notice that Module is a subclass of nn.Module, the base class of neural networks\nin PyTorch. It provides convenient features for handling neural networks. For example, if we\ndeﬁne a forward method, such as forward(self, X), then for an instance a we can invoke\nthis method by a(X). This works since it calls the forward method in the built-in __call__\nmethod. You can ﬁnd more details and examples about nn.Module in Section 6.1.\n3.2.3 Data\nThe DataModule class is the base class for data. Quite frequently the __init__ method\nis used to prepare the data. This includes downloading and preprocessing if needed. The\ntrain_dataloader returns the data loader for the training dataset. A data loader is a (Python)\ngenerator that yields a data batch each time it is used. This batch is then fed into the train-\ning_step method of Module to compute the loss. There is an optional val_dataloader to\nreturn the validation dataset loader. It behaves in the same manner, except that it yields data\nbatches for the validation_step method in Module.\n\n98\nLinear Neural Networks for Regression\nclass DataModule(d2l.HyperParameters):\n#@save\n\"\"\"The base class of data.\"\"\"\ndef __init__(self, root='../data', num_workers=4):\nself.save_hyperparameters()\ndef get_dataloader(self, train):\nraise NotImplementedError\ndef train_dataloader(self):\nreturn self.get_dataloader(train=True)\ndef val_dataloader(self):\nreturn self.get_dataloader(train=False)\n3.2.4 Training\nThe Trainer class trains the learnable parameters in the Module class with data speciﬁed\nin DataModule. The key method is fit, which accepts two arguments: model, an instance\nof Module, and data, an instance of DataModule. It then iterates over the entire dataset\nmax_epochs times to train the model. As before, we will defer the implementation of this\nmethod to later chapters.\nclass Trainer(d2l.HyperParameters):\n#@save\n\"\"\"The base class for training models with data.\"\"\"\ndef __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\nself.save_hyperparameters()\nassert num_gpus == 0, 'No GPU support yet'\ndef prepare_data(self, data):\nself.train_dataloader = data.train_dataloader()\nself.val_dataloader = data.val_dataloader()\nself.num_train_batches = len(self.train_dataloader)\nself.num_val_batches = (len(self.val_dataloader)\nif self.val_dataloader is not None else 0)\ndef prepare_model(self, model):\nmodel.trainer = self\nmodel.board.xlim = [0, self.max_epochs]\nself.model = model\ndef fit(self, model, data):\nself.prepare_data(data)\nself.prepare_model(model)\nself.optim = model.configure_optimizers()\nself.epoch = 0\nself.train_batch_idx = 0\nself.val_batch_idx = 0\nfor self.epoch in range(self.max_epochs):\nself.fit_epoch()\ndef fit_epoch(self):\nraise NotImplementedError\n\n99\nSynthetic Regression Data\n72\n73\n74\n3.2.5 Summary\nTo highlight the object-oriented design for our future deep learning implementation, the\nabove classes simply show how their objects store data and interact with each other. We\nwill keep enriching implementations of these classes, such as via @add_to_class, in the\nrest of the book. Moreover, these fully implemented classes are saved in the D2L library72\n, a lightweight toolkit that makes structured modeling for deep learning easy. In particular, it\nfacilitates reusing many components between projects without changing much at all. For in-\nstance, we can replace just the optimizer, just the model, just the dataset, etc.; this degree of\nmodularity pays dividends throughout the book in terms of conciseness and simplicity (this\nis why we added it) and it can do the same for your own projects.\n3.2.6 Exercises\n1. Locate full implementations of the above classes that are saved in the D2L library73. We\nstrongly recommend that you look at the implementation in detail once you have gained\nsome more familiarity with deep learning modeling.\n2. Remove the save_hyperparameters statement in the B class. Can you still print self.a\nand self.b? Optional: if you have dived into the full implementation of the HyperPa-\nrameters class, can you explain why?\nDiscussions74.\n3.3 Synthetic Regression Data\nMachine learning is all about extracting information from data. So you might wonder, what\ncould we possibly learn from synthetic data? While we might not care intrinsically about\nthe patterns that we ourselves baked into an artiﬁcial data generating model, such datasets\nare nevertheless useful for didactic purposes, helping us to evaluate the properties of our\nlearning algorithms and to conﬁrm that our implementations work as expected. For example,\nif we create data for which the correct parameters are known a priori, then we can check that\nour model can in fact recover them.\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch as d2l\n3.3.1 Generating the Dataset\n\n100\nLinear Neural Networks for Regression\nFor this example, we will work in low dimension for succinctness. The following code snippet\ngenerates 1000 examples with 2-dimensional features drawn from a standard normal distribu-\ntion. The resulting design matrix X belongs to R1000×2. We generate each label by applying\na ground truth linear function, corrupting them via additive noise ϵ, drawn independently and\nidentically for each example:\ny = Xw + b + ϵ.\n(3.3.1)\nFor convenience we assume that ϵ is drawn from a normal distribution with mean µ = 0\nand standard deviation σ = 0.01. Note that for object-oriented design we add the code to\nthe __init__ method of a subclass of d2l.DataModule (introduced in Section 3.2.3). It\nis good practice to allow the setting of any additional hyperparameters. We accomplish this\nwith save_hyperparameters(). The batch_size will be determined later.\nclass SyntheticRegressionData(d2l.DataModule):\n#@save\n\"\"\"Synthetic data for linear regression.\"\"\"\ndef __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\nbatch_size=32):\nsuper().__init__()\nself.save_hyperparameters()\nn = num_train + num_val\nself.X = torch.randn(n, len(w))\nnoise = torch.randn(n, 1) * noise\nself.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\nBelow, we set the true parameters to w = [2, −3.4]⊤and b = 4.2. Later, we can check our\nestimated parameters against these ground truth values.\ndata = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\nEach row in features consists of a vector in R2 and each row in labels is a scalar. Let’s\nhave a look at the ﬁrst entry.\nprint('features:', data.X[0],'\\nlabel:', data.y[0])\nfeatures: tensor([ 2.2793, -0.2246])\nlabel: tensor([9.5014])\n3.3.2 Reading the Dataset\nTraining machine learning models often requires multiple passes over a dataset, grabbing one\nminibatch of examples at a time. This data is then used to update the model. To illustrate how\nthis works, we implement the get_dataloader method, registering it in the SyntheticRe-\ngressionData class via add_to_class (introduced in Section 3.2.1). It takes a batch size,\na matrix of features, and a vector of labels, and generates minibatches of size batch_size.\n\n101\nSynthetic Regression Data\nAs such, each minibatch consists of a tuple of features and labels. Note that we need to be\nmindful of whether we’re in training or validation mode: in the former, we will want to read\nthe data in random order, whereas for the latter, being able to read data in a pre-deﬁned order\nmay be important for debugging purposes.\n@d2l.add_to_class(SyntheticRegressionData)\ndef get_dataloader(self, train):\nif train:\nindices = list(range(0, self.num_train))\n# The examples are read in random order\nrandom.shuffle(indices)\nelse:\nindices = list(range(self.num_train, self.num_train+self.num_val))\nfor i in range(0, len(indices), self.batch_size):\nbatch_indices = torch.tensor(indices[i: i+self.batch_size])\nyield self.X[batch_indices], self.y[batch_indices]\nTo build some intuition, let’s inspect the ﬁrst minibatch of data. Each minibatch of features\nprovides us with both its size and the dimensionality of input features. Likewise, our mini-\nbatch of labels will have a matching shape given by batch_size.\nX, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)\nX shape: torch.Size([32, 2])\ny shape: torch.Size([32, 1])\nWhile seemingly innocuous, the invocation of iter(data.train_dataloader()) illus-\ntrates the power of Python’s object-oriented design. Note that we added a method to the\nSyntheticRegressionData class after creating the data object. Nonetheless, the object\nbeneﬁts from the ex post facto addition of functionality to the class.\nThroughout the iteration we obtain distinct minibatches until the entire dataset has been ex-\nhausted (try this). While the iteration implemented above is good for didactic purposes, it is\nineﬃcient in ways that might get us into trouble with real problems. For example, it requires\nthat we load all the data in memory and that we perform lots of random memory access. The\nbuilt-in iterators implemented in a deep learning framework are considerably more eﬃcient\nand they can deal with sources such as data stored in ﬁles, data received via a stream, and\ndata generated or processed on the ﬂy. Next let’s try to implement the same method using\nbuilt-in iterators.\n3.3.3 Concise Implementation of the Data Loader\nRather than writing our own iterator, we can call the existing API in a framework to load data.\nAs before, we need a dataset with features X and labels y. Beyond that, we set batch_size\nin the built-in data loader and let it take care of shuﬄing examples eﬃciently.\n\n102\nLinear Neural Networks for Regression\n@d2l.add_to_class(d2l.DataModule)\n#@save\ndef get_tensorloader(self, tensors, train, indices=slice(0, None)):\ntensors = tuple(a[indices] for a in tensors)\ndataset = torch.utils.data.TensorDataset(*tensors)\nreturn torch.utils.data.DataLoader(dataset, self.batch_size,\nshuffle=train)\n@d2l.add_to_class(SyntheticRegressionData)\n#@save\ndef get_dataloader(self, train):\ni = slice(0, self.num_train) if train else slice(self.num_train, None)\nreturn self.get_tensorloader((self.X, self.y), train, i)\nThe new data loader behaves just like the previous one, except that it is more eﬃcient and\nhas some added functionality.\nX, y = next(iter(data.train_dataloader()))\nprint('X shape:', X.shape, '\\ny shape:', y.shape)\nX shape: torch.Size([32, 2])\ny shape: torch.Size([32, 1])\nFor instance, the data loader provided by the framework API supports the built-in __len__\nmethod, so we can query its length, i.e., the number of batches.\nlen(data.train_dataloader())\n32\n3.3.4 Summary\nData loaders are a convenient way of abstracting out the process of loading and manipulating\ndata. This way the same machine learning algorithm is capable of processing many diﬀerent\ntypes and sources of data without the need for modiﬁcation. One of the nice things about data\nloaders is that they can be composed. For instance, we might be loading images and then have\na postprocessing ﬁlter that crops them or modiﬁes them in other ways. As such, data loaders\ncan be used to describe an entire data processing pipeline.\nAs for the model itself, the two-dimensional linear model is about the simplest we might\nencounter. It lets us test out the accuracy of regression models without worrying about having\ninsuﬃcient amounts of data or an underdetermined system of equations. We will put this to\ngood use in the next section.\n3.3.5 Exercises\n\n103\nLinear Regression Implementation from Scratch\n75\n76\n1. What will happen if the number of examples cannot be divided by the batch size. How\nwould you change this behavior by specifying a diﬀerent argument by using the frame-\nwork’s API?\n2. Suppose that we want to generate a huge dataset, where both the size of the parameter\nvector w and the number of examples num_examples are large.\n1. What happens if we cannot hold all data in memory?\n2. How would you shuﬄe the data if it is held on disk? Your task is to design an eﬃcient\nalgorithm that does not require too many random reads or writes. Hint: pseudorandom\npermutation generators75 allow you to design a reshuﬄe without the need to store the\npermutation table explicitly (Naor and Reingold, 1999).\n3. Implement a data generator that produces new data on the ﬂy, every time the iterator is\ncalled.\n4. How would you design a random data generator that generates the same data each time it\nis called?\nDiscussions76.\n3.4 Linear Regression Implementation from Scratch\nWe are now ready to work through a fully functioning implementation of linear regression.\nIn this section, we will implement the entire method from scratch, including (i) the model;\n(ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the\ntraining function that stitches all of these pieces together. Finally, we will run our synthetic\ndata generator from Section 3.3 and apply our model on the resulting dataset. While modern\ndeep learning frameworks can automate nearly all of this work, implementing things from\nscratch is the only way to make sure that you really know what you are doing. Moreover,\nwhen it is time to customize models, deﬁning our own layers or loss functions, understanding\nhow things work under the hood will prove handy. In this section, we will rely only on tensors\nand automatic diﬀerentiation. Later, we will introduce a more concise implementation, taking\nadvantage of the bells and whistles of deep learning frameworks while retaining the structure\nof what follows below.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\n3.4.1 Deﬁning the Model\n\n104\nLinear Neural Networks for Regression\nBefore we can begin optimizing our model’s parameters by minibatch SGD, we need to have\nsome parameters in the ﬁrst place. In the following we initialize weights by drawing random\nnumbers from a normal distribution with mean 0 and a standard deviation of 0.01. The magic\nnumber 0.01 often works well in practice, but you can specify a diﬀerent value through the\nargument sigma. Moreover we set the bias to 0. Note that for object-oriented design we\nadd the code to the __init__ method of a subclass of d2l.Module (introduced in Section\n3.2.2).\nclass LinearRegressionScratch(d2l.Module):\n#@save\n\"\"\"The linear regression model implemented from scratch.\"\"\"\ndef __init__(self, num_inputs, lr, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\nself.b = torch.zeros(1, requires_grad=True)\nNext we must deﬁne our model, relating its input and parameters to its output. Using the same\nnotation as (3.1.4) for our linear model we simply take the matrix–vector product of the input\nfeatures X and the model weights w, and add the oﬀset b to each example. The product Xw\nis a vector and b is a scalar. Because of the broadcasting mechanism (see Section 2.1.4), when\nwe add a vector and a scalar, the scalar is added to each component of the vector. The resulting\nforward method is registered in the LinearRegressionScratch class via add_to_class\n(introduced in Section 3.2.1).\n@d2l.add_to_class(LinearRegressionScratch)\n#@save\ndef forward(self, X):\nreturn torch.matmul(X, self.w) + self.b\n3.4.2 Deﬁning the Loss Function\nSince updating our model requires taking the gradient of our loss function, we ought to deﬁne\nthe loss function ﬁrst. Here we use the squared loss function in (3.1.5). In the implementation,\nwe need to transform the true value y into the predicted value’s shape y_hat. The result\nreturned by the following method will also have the same shape as y_hat. We also return the\naveraged loss value among all examples in the minibatch.\n@d2l.add_to_class(LinearRegressionScratch)\n#@save\ndef loss(self, y_hat, y):\nl = (y_hat - y) ** 2 / 2\nreturn l.mean()\n3.4.3 Deﬁning the Optimization Algorithm\nAs discussed in Section 3.1, linear regression has a closed-form solution. However, our goal\nhere is to illustrate how to train more general neural networks, and that requires that we teach\n\n105\nLinear Regression Implementation from Scratch\nyou how to use minibatch SGD. Hence we will take this opportunity to introduce your ﬁrst\nworking example of SGD. At each step, using a minibatch randomly drawn from our dataset,\nwe estimate the gradient of the loss with respect to the parameters. Next, we update the\nparameters in the direction that may reduce the loss.\nThe following code applies the update, given a set of parameters, a learning rate lr. Since\nour loss is computed as an average over the minibatch, we do not need to adjust the learning\nrate against the batch size. In later chapters we will investigate how learning rates should be\nadjusted for very large minibatches as they arise in distributed large-scale learning. For now,\nwe can ignore this dependency.\nWe deﬁne our SGD class, a subclass of d2l.HyperParameters (introduced in Section 3.2.1),\nto have a similar API as the built-in SGD optimizer. We update the parameters in the step\nmethod. The zero_grad method sets all gradients to 0, which must be run before a back-\npropagation step.\nclass SGD(d2l.HyperParameters):\n#@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\ndef __init__(self, params, lr):\nself.save_hyperparameters()\ndef step(self):\nfor param in self.params:\nparam -= self.lr * param.grad\ndef zero_grad(self):\nfor param in self.params:\nif param.grad is not None:\nparam.grad.zero_()\nWe next deﬁne the configure_optimizers method, which returns an instance of the SGD\nclass.\n@d2l.add_to_class(LinearRegressionScratch)\n#@save\ndef configure_optimizers(self):\nreturn SGD([self.w, self.b], self.lr)\n3.4.4 Training\nNow that we have all of the parts in place (parameters, loss function, model, and optimizer),\nwe are ready to implement the main training loop. It is crucial that you understand this code\nfully since you will employ similar training loops for every other deep learning model covered\nin this book. In each epoch, we iterate through the entire training dataset, passing once through\nevery example (assuming that the number of examples is divisible by the batch size). In each\niteration, we grab a minibatch of training examples, and compute its loss through the model’s\ntraining_step method. Then we compute the gradients with respect to each parameter.\nFinally, we will call the optimization algorithm to update the model parameters. In summary,\nwe will execute the following loop:\n\n106\nLinear Neural Networks for Regression\n• Initialize parameters (w, b)\n• Repeat until done\n-- Compute gradient g ←∂(w,b)\n1\n|B |\n∑\ni∈B l(x(i), y(i), w, b)\n-- Update parameters (w, b) ←(w, b) −ηg\nRecall that the synthetic regression dataset that we generated in Section 3.3 does not pro-\nvide a validation dataset. In most cases, however, we will want a validation dataset to mea-\nsure our model quality. Here we pass the validation dataloader once in each epoch to mea-\nsure the model performance. Following our object-oriented design, the prepare_batch and\nfit_epoch methods are registered in the d2l.Trainer class (introduced in Section 3.2.4).\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef prepare_batch(self, batch):\nreturn batch\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef fit_epoch(self):\nself.model.train()\nfor batch in self.train_dataloader:\nloss = self.model.training_step(self.prepare_batch(batch))\nself.optim.zero_grad()\nwith torch.no_grad():\nloss.backward()\nif self.gradient_clip_val > 0:\n# To be discussed later\nself.clip_gradients(self.gradient_clip_val, self.model)\nself.optim.step()\nself.train_batch_idx += 1\nif self.val_dataloader is None:\nreturn\nself.model.eval()\nfor batch in self.val_dataloader:\nwith torch.no_grad():\nself.model.validation_step(self.prepare_batch(batch))\nself.val_batch_idx += 1\nWe are almost ready to train the model, but ﬁrst we need some training data. Here we use the\nSyntheticRegressionData class and pass in some ground truth parameters. Then we train\nour model with the learning rate lr=0.03 and set max_epochs=3. Note that in general, both\nthe number of epochs and the learning rate are hyperparameters. In general, setting hyper-\nparameters is tricky and we will usually want to use a three-way split, one set for training, a\nsecond for hyperparameter selection, and the third reserved for the ﬁnal evaluation. We elide\nthese details for now but will revise them later.\nmodel = LinearRegressionScratch(2, lr=0.03)\ndata = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\nBecause we synthesized the dataset ourselves, we know precisely what the true parameters\n\n107\nLinear Regression Implementation from Scratch\nare. Thus, we can evaluate our success in training by comparing the true parameters with\nthose that we learned through our training loop. Indeed they turn out to be very close to each\nother.\nwith torch.no_grad():\nprint(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')\nprint(f'error in estimating b: {data.b - model.b}')\nerror in estimating w: tensor([ 0.1163, -0.2020])\nerror in estimating b: tensor([0.2383])\nWe should not take the ability to exactly recover the ground truth parameters for granted.\nIn general, for deep models unique solutions for the parameters do not exist, and even for\nlinear models, exactly recovering the parameters is only possible when no feature is linearly\ndependent on the others. However, in machine learning, we are often less concerned with\nrecovering true underlying parameters, but rather with parameters that lead to highly accurate\nprediction (Vapnik, 1992). Fortunately, even on diﬃcult optimization problems, stochastic\ngradient descent can often ﬁnd remarkably good solutions, owing partly to the fact that, for\ndeep networks, there exist many conﬁgurations of the parameters that lead to highly accurate\nprediction.\n3.4.5 Summary\nIn this section, we took a signiﬁcant step towards designing deep learning systems by imple-\nmenting a fully functional neural network model and training loop. In this process, we built a\ndata loader, a model, a loss function, an optimization procedure, and a visualization and mon-\nitoring tool. We did this by composing a Python object that contains all relevant components\nfor training a model. While this is not yet a professional-grade implementation it is perfectly\nfunctional and code like this could already help you to solve small problems quickly. In the\ncoming sections, we will see how to do this both more concisely (avoiding boilerplate code)\nand more eﬃciently (using our GPUs to their full potential).\n\n108\nLinear Neural Networks for Regression\n77\n78\n79\n3.4.6 Exercises\n1. What would happen if we were to initialize the weights to zero. Would the algorithm still\nwork? What if we initialized the parameters with variance 1000 rather than 0.01?\n2. Assume that you are Georg Simon Ohm 77 trying to come up with a model for resis-\ntance that relates voltage and current. Can you use automatic diﬀerentiation to learn the\nparameters of your model?\n3. Can you use Planck’s Law 78 to determine the temperature of an object using spectral\nenergy density? For reference, the spectral density B of radiation emanating from a black\nbody is B(λ,T) = 2hc2\nλ5 ·\n(\nexp hc\nλkT −1\n)−1\n. Here λ is the wavelength, T is the temperature,\nc is the speed of light, h is Planck’s constant, and k is the Boltzmann constant. You measure\nthe energy for diﬀerent wavelengths λ and you now need to ﬁt the spectral density curve\nto Planck’s law.\n4. What are the problems you might encounter if you wanted to compute the second deriva-\ntives of the loss? How would you ﬁx them?\n5. Why is the reshape method needed in the loss function?\n6. Experiment using diﬀerent learning rates to ﬁnd out how quickly the loss function value\ndrops. Can you reduce the error by increasing the number of epochs of training?\n7. If the number of examples cannot be divided by the batch size, what happens to data_iter\nat the end of an epoch?\n8. Try implementing a diﬀerent loss function, such as the absolute value loss (y_hat -\nd2l.reshape(y, y_hat.shape)).abs().sum().\n1. Check what happens for regular data.\n2. Check whether there is a diﬀerence in behavior if you actively perturb some entries,\nsuch as y5 = 10000, of y.\n3. Can you think of a cheap solution for combining the best aspects of squared loss and\nabsolute value loss? Hint: how can you avoid really large gradient values?\n9. Why do we need to reshuﬄe the dataset? Can you design a case where a maliciously\nconstructed dataset would break the optimization algorithm otherwise?\nDiscussions79.\n\n109\nConcise Implementation of Linear Regression\n3.5 Concise Implementation of Linear Regression\nDeep learning has witnessed a sort of Cambrian explosion over the past decade. The sheer\nnumber of techniques, applications and algorithms by far surpasses the progress of previous\ndecades. This is due to a fortuitous combination of multiple factors, one of which is the\npowerful free tools oﬀered by a number of open-source deep learning frameworks. Theano\n(Bergstra et al., 2010), DistBelief (Dean et al., 2012), and Caﬀe (Jia et al., 2014) arguably\nrepresent the ﬁrst generation of such models that found widespread adoption. In contrast\nto earlier (seminal) works like SN2 (Simulateur Neuristique) (Bottou and Le Cun, 1988),\nwhich provided a Lisp-like programming experience, modern frameworks oﬀer automatic\ndiﬀerentiation and the convenience of Python. These frameworks allow us to automate and\nmodularize the repetitive work of implementing gradient-based learning algorithms.\nIn Section 3.4, we relied only on (i) tensors for data storage and linear algebra; and (ii) auto-\nmatic diﬀerentiation for calculating gradients. In practice, because data iterators, loss func-\ntions, optimizers, and neural network layers are so common, modern libraries implement\nthese components for us as well. In this section, we will show you how to implement the\nlinear regression model from Section 3.4 concisely by using high-level APIs of deep learning\nframeworks.\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n3.5.1 Deﬁning the Model\nWhen we implemented linear regression from scratch in Section 3.4, we deﬁned our model\nparameters explicitly and coded up the calculations to produce output using basic linear al-\ngebra operations. You should know how to do this. But once your models get more complex,\nand once you have to do this nearly every day, you will be glad of the assistance. The situation\nis similar to coding up your own blog from scratch. Doing it once or twice is rewarding and\ninstructive, but you would be a lousy web developer if you spent a month reinventing the\nwheel.\nFor standard operations, we can use a framework’s predeﬁned layers, which allow us to focus\non the layers used to construct the model rather than worrying about their implementation.\nRecall the architecture of a single-layer network as described in Fig. 3.1.2. The layer is called\nfully connected, since each of its inputs is connected to each of its outputs by means of a\nmatrix–vector multiplication.\nIn PyTorch, the fully connected layer is deﬁned in Linear and LazyLinear classes (available\nsince version 1.8.0). The latter allows users to specify merely the output dimension, while the\n\n110\nLinear Neural Networks for Regression\nformer additionally asks for how many inputs go into this layer. Specifying input shapes is\ninconvenient and may require nontrivial calculations (such as in convolutional layers). Thus,\nfor simplicity, we will use such “lazy” layers whenever we can.\nclass LinearRegression(d2l.Module):\n#@save\n\"\"\"The linear regression model implemented with high-level APIs.\"\"\"\ndef __init__(self, lr):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.LazyLinear(1)\nself.net.weight.data.normal_(0, 0.01)\nself.net.bias.data.fill_(0)\nIn the forward method we just invoke the built-in __call__ method of the predeﬁned layers\nto compute the outputs.\n@d2l.add_to_class(LinearRegression)\n#@save\ndef forward(self, X):\nreturn self.net(X)\n3.5.2 Deﬁning the Loss Function\nThe MSELoss class computes the mean squared error (without the 1/2 factor in (3.1.5)). By\ndefault, MSELoss returns the average loss over examples. It is faster (and easier to use) than\nimplementing our own.\n@d2l.add_to_class(LinearRegression)\n#@save\ndef loss(self, y_hat, y):\nfn = nn.MSELoss()\nreturn fn(y_hat, y)\n3.5.3 Deﬁning the Optimization Algorithm\nMinibatch SGD is a standard tool for optimizing neural networks and thus PyTorch supports\nit alongside a number of variations on this algorithm in the optim module. When we instan-\ntiate an SGD instance, we specify the parameters to optimize over, obtainable from our model\nvia self.parameters(), and the learning rate (self.lr) required by our optimization al-\ngorithm.\n@d2l.add_to_class(LinearRegression)\n#@save\ndef configure_optimizers(self):\nreturn torch.optim.SGD(self.parameters(), self.lr)\n3.5.4 Training\n\n111\nConcise Implementation of Linear Regression\nYou might have noticed that expressing our model through high-level APIs of a deep learning\nframework requires fewer lines of code. We did not have to allocate parameters individually,\ndeﬁne our loss function, or implement minibatch SGD. Once we start working with much\nmore complex models, the advantages of the high-level API will grow considerably.\nNow that we have all the basic pieces in place, the training loop itself is the same as the one\nwe implemented from scratch. So we just call the fit method (introduced in Section 3.2.4),\nwhich relies on the implementation of the fit_epoch method in Section 3.4, to train our\nmodel.\nmodel = LinearRegression(lr=0.03)\ndata = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\ntrainer = d2l.Trainer(max_epochs=3)\ntrainer.fit(model, data)\nBelow, we compare the model parameters learned by training on ﬁnite data and the actual\nparameters that generated our dataset. To access parameters, we access the weights and bias\nof the layer that we need. As in our implementation from scratch, note that our estimated\nparameters are close to their true counterparts.\n@d2l.add_to_class(LinearRegression)\n#@save\ndef get_w_b(self):\nreturn (self.net.weight.data, self.net.bias.data)\nw, b = model.get_w_b()\nprint(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\nprint(f'error in estimating b: {data.b - b}')\nerror in estimating w: tensor([ 0.0031, -0.0099])\nerror in estimating b: tensor([0.0127])\n3.5.5 Summary\n\n112\nLinear Neural Networks for Regression\n80\nThis section contains the ﬁrst implementation of a deep network (in this book) to tap into the\nconveniences aﬀorded by modern deep learning frameworks, such as MXNet (Chen et al.,\n2015), JAX (Frostig et al., 2018), PyTorch (Paszke et al., 2019), and Tensorﬂow (Abadi et\nal., 2016). We used framework defaults for loading data, deﬁning a layer, a loss function, an\noptimizer and a training loop. Whenever the framework provides all necessary features, it is\ngenerally a good idea to use them, since the library implementations of these components tend\nto be heavily optimized for performance and properly tested for reliability. At the same time,\ntry not to forget that these modules can be implemented directly. This is especially important\nfor aspiring researchers who wish to live on the leading edge of model development, where\nyou will be inventing new components that cannot possibly exist in any current library.\nIn PyTorch, the data module provides tools for data processing, the nn module deﬁnes a large\nnumber of neural network layers and common loss functions. We can initialize the parameters\nby replacing their values with methods ending with _. Note that we need to specify the input\ndimensions of the network. While this is trivial for now, it can have signiﬁcant knock-on\neﬀects when we want to design complex networks with many layers. Careful considerations\nof how to parametrize these networks is needed to allow portability.\n3.5.6 Exercises\n1. How would you need to change the learning rate if you replace the aggregate loss over the\nminibatch with an average over the loss on the minibatch?\n2. Review the framework documentation to see which loss functions are provided. In par-\nticular, replace the squared loss with Huber’s robust loss function. That is, use the loss\nfunction\nl(y, y′) =\n{\n|y −y′| −σ\n2\nif |y −y′| > σ\n1\n2σ (y −y′)2\notherwise\n(3.5.1)\n3. How do you access the gradient of the weights of the model?\n4. What is the eﬀect on the solution if you change the learning rate and the number of epochs?\nDoes it keep on improving?\n5. How does the solution change as you vary the amount of data generated?\n1. Plot the estimation error for ˆw−w and ˆb−b as a function of the amount of data. Hint:\nincrease the amount of data logarithmically rather than linearly, i.e., 5, 10, 20, 50, …,\n10,000 rather than 1000, 2000, …, 10,000.\n2. Why is the suggestion in the hint appropriate?\nDiscussions80.\n\n113\nGeneralization\n3.6 Generalization\nConsider two college students diligently preparing for their ﬁnal exam. Commonly, this prepa-\nration will consist of practicing and testing their abilities by taking exams administered in pre-\nvious years. Nonetheless, doing well on past exams is no guarantee that they will excel when\nit matters. For instance, imagine a student, Extraordinary Ellie, whose preparation consisted\nentirely of memorizing the answers to previous years’ exam questions. Even if Ellie were\nendowed with an extraordinary memory, and thus could perfectly recall the answer to any\npreviously seen question, she might nevertheless freeze when faced with a new (previously\nunseen) question. By comparison, imagine another student, Inductive Irene, with compara-\nbly poor memorization skills, but a knack for picking up patterns. Note that if the exam truly\nconsisted of recycled questions from a previous year, Ellie would handily outperform Irene.\nEven if Irene’s inferred patterns yielded 90% accurate predictions, they could never compete\nwith Ellie’s 100% recall. However, even if the exam consisted entirely of fresh questions,\nIrene might maintain her 90% average.\nAs machine learning scientists, our goal is to discover patterns. But how can we be sure that\nwe have truly discovered a general pattern and not simply memorized our data? Most of the\ntime, our predictions are only useful if our model discovers such a pattern. We do not want\nto predict yesterday’s stock prices, but tomorrow’s. We do not need to recognize already\ndiagnosed diseases for previously seen patients, but rather previously undiagnosed ailments\nin previously unseen patients. This problem—how to discover patterns that generalize—is the\nfundamental problem of machine learning, and arguably of all of statistics. We might cast this\nproblem as just one slice of a far grander question that engulfs all of science: when are we ever\njustiﬁed in making the leap from particular observations to more general statements?\nIn real life, we must ﬁt our models using a ﬁnite collection of data. The typical scales of that\ndata vary wildly across domains. For many important medical problems, we can only access a\nfew thousand data points. When studying rare diseases, we might be lucky to access hundreds.\nBy contrast, the largest public datasets consisting of labeled photographs, e.g., ImageNet\n(Deng et al., 2009), contain millions of images. And some unlabeled image collections such as\nthe Flickr YFC100M dataset can be even larger, containing over 100 million images (Thomee\net al., 2016). However, even at this extreme scale, the number of available data points remains\ninﬁnitesimally small compared to the space of all possible images at a megapixel resolution.\nWhenever we work with ﬁnite samples, we must keep in mind the risk that we might ﬁt our\ntraining data, only to discover that we failed to discover a generalizable pattern.\nThe phenomenon of ﬁtting closer to our training data than to the underlying distribution is\ncalled overﬁtting, and techniques for combatting overﬁtting are often called regularization\nmethods. While it is no substitute for a proper introduction to statistical learning theory (see\nBoucheron et al. (2005), Vapnik (1998)), we will give you just enough intuition to get going.\nWe will revisit generalization in many chapters throughout the book, exploring both what is\nknown about the principles underlying generalization in various models, and also heuristic\n\n114\nLinear Neural Networks for Regression\ntechniques that have been found (empirically) to yield improved generalization on tasks of\npractical interest.\n3.6.1 Training Error and Generalization Error\nIn the standard supervised learning setting, we assume that the training data and the test\ndata are drawn independently from identical distributions. This is commonly called the IID\nassumption. While this assumption is strong, it is worth noting that, absent any such assump-\ntion, we would be dead in the water. Why should we believe that training data sampled from\ndistribution P(X,Y) should tell us how to make predictions on test data generated by a dif-\nferent distribution Q(X,Y)? Making such leaps turns out to require strong assumptions about\nhow P and Q are related. Later on we will discuss some assumptions that allow for shifts in\ndistribution but ﬁrst we need to understand the IID case, where P(·) = Q(·).\nTo begin with, we need to diﬀerentiate between the training error Remp, which is a statistic\ncalculated on the training dataset, and the generalization error R, which is an expectation taken\nwith respect to the underlying distribution. You can think of the generalization error as what\nyou would see if you applied your model to an inﬁnite stream of additional data examples\ndrawn from the same underlying data distribution. Formally the training error is expressed\nas a sum (with the same notation as Section 3.1):\nRemp[X, y, f ] = 1\nn\nn\n∑\ni=1\nl(x(i), y(i), f (x(i))),\n(3.6.1)\nwhile the generalization error is expressed as an integral:\nR[p, f ] = E(x,y)∼P[l(x, y, f (x))] =\n∫∫\nl(x, y, f (x))p(x, y) dxdy.\n(3.6.2)\nProblematically, we can never calculate the generalization error R exactly. Nobody ever tells\nus the precise form of the density function p(x, y). Moreover, we cannot sample an inﬁnite\nstream of data points. Thus, in practice, we must estimate the generalization error by applying\nour model to an independent test set constituted of a random selection of examples X′ and\nlabels y′ that were withheld from our training set. This consists of applying the same formula\nthat was used for calculating the empirical training error but to a test set X′, y′.\nCrucially, when we evaluate our classiﬁer on the test set, we are working with a ﬁxed classiﬁer\n(it does not depend on the sample of the test set), and thus estimating its error is simply the\nproblem of mean estimation. However the same cannot be said for the training set. Note\nthat the model we wind up with depends explicitly on the selection of the training set and\nthus the training error will in general be a biased estimate of the true error on the underlying\npopulation. The central question of generalization is then when should we expect our training\nerror to be close to the population error (and thus the generalization error).\n\n115\nGeneralization\nModel Complexity\nIn classical theory, when we have simple models and abundant data, the training and gener-\nalization errors tend to be close. However, when we work with more complex models and/or\nfewer examples, we expect the training error to go down but the generalization gap to grow.\nThis should not be surprising. Imagine a model class so expressive that for any dataset of n\nexamples, we can ﬁnd a set of parameters that can perfectly ﬁt arbitrary labels, even if ran-\ndomly assigned. In this case, even if we ﬁt our training data perfectly, how can we conclude\nanything about the generalization error? For all we know, our generalization error might be\nno better than random guessing.\nIn general, absent any restriction on our model class, we cannot conclude, based on ﬁtting the\ntraining data alone, that our model has discovered any generalizable pattern (Vapnik et al.,\n1994). On the other hand, if our model class was not capable of ﬁtting arbitrary labels, then\nit must have discovered a pattern. Learning-theoretic ideas about model complexity derived\nsome inspiration from the ideas of Karl Popper, an inﬂuential philosopher of science, who\nformalized the criterion of falsiﬁability. According to Popper, a theory that can explain any\nand all observations is not a scientiﬁc theory at all! After all, what has it told us about the\nworld if it has not ruled out any possibility? In short, what we want is a hypothesis that could\nnot explain any observations we might conceivably make and yet nevertheless happens to be\ncompatible with those observations that we in fact make.\nNow what precisely constitutes an appropriate notion of model complexity is a complex mat-\nter. Often, models with more parameters are able to ﬁt a greater number of arbitrarily as-\nsigned labels. However, this is not necessarily true. For instance, kernel methods operate in\nspaces with inﬁnite numbers of parameters, yet their complexity is controlled by other means\n(Schölkopf and Smola, 2002). One notion of complexity that often proves useful is the range\nof values that the parameters can take. Here, a model whose parameters are permitted to\ntake arbitrary values would be more complex. We will revisit this idea in the next section,\nwhen we introduce weight decay, your ﬁrst practical regularization technique. Notably, it can\nbe diﬃcult to compare complexity among members of substantially diﬀerent model classes\n(say, decision trees vs. neural networks).\nAt this point, we must stress another important point that we will revisit when introducing\ndeep neural networks. When a model is capable of ﬁtting arbitrary labels, low training error\ndoes not necessarily imply low generalization error. However, it does not necessarily imply high\ngeneralization error either! All we can say with conﬁdence is that low training error alone is\nnot enough to certify low generalization error. Deep neural networks turn out to be just such\nmodels: while they generalize well in practice, they are too powerful to allow us to conclude\nmuch on the basis of training error alone. In these cases we must rely more heavily on our\nholdout data to certify generalization after the fact. Error on the holdout data, i.e., validation\nset, is called the validation error.\n3.6.2 Underﬁtting or Overﬁtting?\n\n116\nLinear Neural Networks for Regression\nWhen we compare the training and validation errors, we want to be mindful of two common\nsituations. First, we want to watch out for cases when our training error and validation error\nare both substantial but there is a little gap between them. If the model is unable to reduce\nthe training error, that could mean that our model is too simple (i.e., insuﬃciently expres-\nsive) to capture the pattern that we are trying to model. Moreover, since the generalization\ngap (Remp −R) between our training and generalization errors is small, we have reason to\nbelieve that we could get away with a more complex model. This phenomenon is known as\nunderﬁtting.\nOn the other hand, as we discussed above, we want to watch out for the cases when our\ntraining error is signiﬁcantly lower than our validation error, indicating severe overﬁtting.\nNote that overﬁtting is not always a bad thing. In deep learning especially, the best predictive\nmodels often perform far better on training data than on holdout data. Ultimately, we usually\ncare about driving the generalization error lower, and only care about the gap insofar as it\nbecomes an obstacle to that end. Note that if the training error is zero, then the generalization\ngap is precisely equal to the generalization error and we can make progress only by reducing\nthe gap.\nPolynomial Curve Fitting\nTo illustrate some classical intuition about overﬁtting and model complexity, consider the\nfollowing: given training data consisting of a single feature x and a corresponding real-valued\nlabel y, we try to ﬁnd the polynomial of degree d\nˆy =\nd\n∑\ni=0\nxiwi\n(3.6.3)\nfor estimating the label y. This is just a linear regression problem where our features are given\nby the powers of x, the model’s weights are given by wi, and the bias is given by w0 since\nx0 = 1 for all x. Since this is just a linear regression problem, we can use the squared error\nas our loss function.\nA higher-order polynomial function is more complex than a lower-order polynomial function,\nsince the higher-order polynomial has more parameters and the model function’s selection\nrange is wider. Fixing the training dataset, higher-order polynomial functions should always\nachieve lower (at worst, equal) training error relative to lower-degree polynomials. In fact,\nwhenever each data example has a distinct value of x, a polynomial function with degree equal\nto the number of data examples can ﬁt the training set perfectly. We compare the relationship\nbetween polynomial degree (model complexity) and both underﬁtting and overﬁtting in Fig.\n3.6.1.\n\n117\nGeneralization\nt\nFig. 3.6.1\nInﬂuence of model complexity on underﬁtting and overﬁtting.\nDataset Size\nAs the above bound already indicates, another big consideration to bear in mind is dataset\nsize. Fixing our model, the fewer samples we have in the training dataset, the more likely (and\nmore severely) we are to encounter overﬁtting. As we increase the amount of training data,\nthe generalization error typically decreases. Moreover, in general, more data never hurts. For\na ﬁxed task and data distribution, model complexity should not increase more rapidly than\nthe amount of data. Given more data, we might attempt to ﬁt a more complex model. Absent\nsuﬃcient data, simpler models may be more diﬃcult to beat. For many tasks, deep learning\nonly outperforms linear models when many thousands of training examples are available. In\npart, the current success of deep learning owes considerably to the abundance of massive\ndatasets arising from Internet companies, cheap storage, connected devices, and the broad\ndigitization of the economy.\n3.6.3 Model Selection\nTypically, we select our ﬁnal model only after evaluating multiple models that diﬀer in var-\nious ways (diﬀerent architectures, training objectives, selected features, data preprocessing,\nlearning rates, etc.). Choosing among many models is aptly called model selection.\nIn principle, we should not touch our test set until after we have chosen all our hyperparame-\nters. Were we to use the test data in the model selection process, there is a risk that we might\noverﬁt the test data. Then we would be in serious trouble. If we overﬁt our training data, there\nis always the evaluation on test data to keep us honest. But if we overﬁt the test data, how\nwould we ever know? See Ong et al. (2005) for an example of how this can lead to absurd\nresults even for models where the complexity can be tightly controlled.\nThus, we should never rely on the test data for model selection. And yet we cannot rely solely\non the training data for model selection either because we cannot estimate the generalization\nerror on the very data that we use to train the model.\nIn practical applications, the picture gets muddier. While ideally we would only touch the\n\n118\nLinear Neural Networks for Regression\n81\n82\ntest data once, to assess the very best model or to compare a small number of models with\neach other, real-world test data is seldom discarded after just one use. We can seldom aﬀord\na new test set for each round of experiments. In fact, recycling benchmark data for decades\ncan have a signiﬁcant impact on the development of algorithms, e.g., for image classiﬁcation\n81 and optical character recognition82.\nThe common practice for addressing the problem of training on the test set is to split our\ndata three ways, incorporating a validation set in addition to the training and test datasets.\nThe result is a murky business where the boundaries between validation and test data are\nworryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book we\nare really working with what should rightly be called training data and validation data, with\nno true test sets. Therefore, the accuracy reported in each experiment of the book is really\nthe validation accuracy and not a true test set accuracy.\nCross-Validation\nWhen training data is scarce, we might not even be able to aﬀord to hold out enough data\nto constitute a proper validation set. One popular solution to this problem is to employ K-\nfold cross-validation. Here, the original training data is split into K non-overlapping subsets.\nThen model training and validation are executed K times, each time training on K −1 subsets\nand validating on a diﬀerent subset (the one not used for training in that round). Finally,\nthe training and validation errors are estimated by averaging over the results from the K\nexperiments.\n3.6.4 Summary\nThis section explored some of the underpinnings of generalization in machine learning. Some\nof these ideas become complicated and counterintuitive when we get to deeper models; here,\nmodels are capable of overﬁtting data badly, and the relevant notions of complexity can be\nboth implicit and counterintuitive (e.g., larger architectures with more parameters generaliz-\ning better). We leave you with a few rules of thumb:\n1. Use validation sets (or K-fold cross-validation) for model selection;\n2. More complex models often require more data;\n3. Relevant notions of complexity include both the number of parameters and the range of\nvalues that they are allowed to take;\n4. Keeping all else equal, more data almost always leads to better generalization;\n5. This entire talk of generalization is all predicated on the IID assumption. If we relax this\nassumption, allowing for distributions to shift between the train and testing periods, then\nwe cannot say anything about generalization absent a further (perhaps milder) assumption.\n\n119\nWeight Decay\n83\n3.6.5 Exercises\n1. When can you solve the problem of polynomial regression exactly?\n2. Give at least ﬁve examples where dependent random variables make treating the problem\nas IID data inadvisable.\n3. Can you ever expect to see zero training error? Under which circumstances would you see\nzero generalization error?\n4. Why is K-fold cross-validation very expensive to compute?\n5. Why is the K-fold cross-validation error estimate biased?\n6. The VC dimension is deﬁned as the maximum number of points that can be classiﬁed with\narbitrary labels {±1} by a function of a class of functions. Why might this not be a good\nidea for measuring how complex the class of functions is? Hint: consider the magnitude\nof the functions.\n7. Your manager gives you a diﬃcult dataset on which your current algorithm does not per-\nform so well. How would you justify to him that you need more data? Hint: you cannot\nincrease the data but you can decrease it.\nDiscussions83.\n3.7 Weight Decay\nNow that we have characterized the problem of overﬁtting, we can introduce our ﬁrst regular-\nization technique. Recall that we can always mitigate overﬁtting by collecting more training\ndata. However, that can be costly, time consuming, or entirely out of our control, making\nit impossible in the short run. For now, we can assume that we already have as much high-\nquality data as our resources permit and focus the tools at our disposal when the dataset is\ntaken as a given.\nRecall that in our polynomial regression example (Section 3.6.2) we could limit our model’s\ncapacity by tweaking the degree of the ﬁtted polynomial. Indeed, limiting the number of fea-\ntures is a popular technique for mitigating overﬁtting. However, simply tossing aside features\ncan be too blunt an instrument. Sticking with the polynomial regression example, consider\nwhat might happen with high-dimensional input. The natural extensions of polynomials to\nmultivariate data are called monomials, which are simply products of powers of variables.\nThe degree of a monomial is the sum of the powers. For example, x2\n1 x2, and x3x2\n5 are both\nmonomials of degree 3.\nNote that the number of terms with degree d blows up rapidly as d grows larger. Given k\nvariables, the number of monomials of degree d is (k−1+d\nk−1\n). Even small changes in degree,\n\n120\nLinear Neural Networks for Regression\nsay from 2 to 3, dramatically increase the complexity of our model. Thus we often need a\nmore ﬁne-grained tool for adjusting function complexity.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n3.7.1 Norms and Weight Decay\nRather than directly manipulating the number of parameters, weight decay, operates by re-\nstricting the values that the parameters can take. More commonly called ℓ2 regularization\noutside of deep learning circles when optimized by minibatch stochastic gradient descent,\nweight decay might be the most widely used technique for regularizing parametric machine\nlearning models. The technique is motivated by the basic intuition that among all functions f ,\nthe function f = 0 (assigning the value 0 to all inputs) is in some sense the simplest, and that\nwe can measure the complexity of a function by the distance of its parameters from zero.\nBut how precisely should we measure the distance between a function and zero? There is\nno single right answer. In fact, entire branches of mathematics, including parts of functional\nanalysis and the theory of Banach spaces, are devoted to addressing such issues.\nOne simple interpretation might be to measure the complexity of a linear function f (x) =\nw⊤x by some norm of its weight vector, e.g., ∥w∥2. Recall that we introduced the ℓ2 norm\nand ℓ1 norm, which are special cases of the more general ℓp norm, in Section 2.3.11. The\nmost common method for ensuring a small weight vector is to add its norm as a penalty term\nto the problem of minimizing the loss. Thus we replace our original objective, minimizing the\nprediction loss on the training labels, with new objective, minimizing the sum of the prediction\nloss and the penalty term. Now, if our weight vector grows too large, our learning algorithm\nmight focus on minimizing the weight norm ∥w∥2 rather than minimizing the training error.\nThat is exactly what we want. To illustrate things in code, we revive our previous example\nfrom Section 3.1 for linear regression. There, our loss was given by\nL(w, b) = 1\nn\nn\n∑\ni=1\n1\n2\n(\nw⊤x(i) + b −y(i))2\n.\n(3.7.1)\nRecall that x(i) are the features, y(i) is the label for any data example i, and (w, b) are the\nweight and bias parameters, respectively. To penalize the size of the weight vector, we must\nsomehow add ∥w∥2 to the loss function, but how should the model trade oﬀthe standard loss\nfor this new additive penalty? In practice, we characterize this trade-oﬀvia the regularization\nconstant λ, a nonnegative hyperparameter that we ﬁt using validation data:\nL(w, b) + λ\n2 ∥w∥2.\n(3.7.2)\nFor λ = 0, we recover our original loss function. For λ > 0, we restrict the size of ∥w∥.\nWe divide by 2 by convention: when we take the derivative of a quadratic function, the 2 and\n1/2 cancel out, ensuring that the expression for the update looks nice and simple. The astute\n\n121\nWeight Decay\nreader might wonder why we work with the squared norm and not the standard norm (i.e., the\nEuclidean distance). We do this for computational convenience. By squaring the ℓ2 norm, we\nremove the square root, leaving the sum of squares of each component of the weight vector.\nThis makes the derivative of the penalty easy to compute: the sum of derivatives equals the\nderivative of the sum.\nMoreover, you might ask why we work with the ℓ2 norm in the ﬁrst place and not, say, the ℓ1\nnorm. In fact, other choices are valid and popular throughout statistics. While ℓ2-regularized\nlinear models constitute the classic ridge regression algorithm, ℓ1-regularized linear regres-\nsion is a similarly fundamental method in statistics, popularly known as lasso regression. One\nreason to work with the ℓ2 norm is that it places an outsize penalty on large components of\nthe weight vector. This biases our learning algorithm towards models that distribute weight\nevenly across a larger number of features. In practice, this might make them more robust\nto measurement error in a single variable. By contrast, ℓ1 penalties lead to models that con-\ncentrate weights on a small set of features by clearing the other weights to zero. This gives\nus an eﬀective method for feature selection, which may be desirable for other reasons. For\nexample, if our model only relies on a few features, then we may not need to collect, store,\nor transmit data for the other (dropped) features.\nUsing the same notation in (3.1.11), minibatch stochastic gradient descent updates for ℓ2-\nregularized regression as follows:\nw ←(1 −ηλ) w −η\n|B|\n∑\ni∈B\nx(i) (\nw⊤x(i) + b −y(i))\n.\n(3.7.3)\nAs before, we update w based on the amount by which our estimate diﬀers from the ob-\nservation. However, we also shrink the size of w towards zero. That is why the method is\nsometimes called “weight decay”: given the penalty term alone, our optimization algorithm\ndecays the weight at each step of training. In contrast to feature selection, weight decay oﬀers\nus a mechanism for continuously adjusting the complexity of a function. Smaller values of λ\ncorrespond to less constrained w, whereas larger values of λ constrain w more considerably.\nWhether we include a corresponding bias penalty b2 can vary across implementations, and\nmay vary across layers of a neural network. Often, we do not regularize the bias term. Besides,\nalthough ℓ2 regularization may not be equivalent to weight decay for other optimization algo-\nrithms, the idea of regularization through shrinking the size of weights still holds true.\n3.7.2 High-Dimensional Linear Regression\nWe can illustrate the beneﬁts of weight decay through a simple synthetic example.\nFirst, we generate some data as before:\ny = 0.05 +\nd\n∑\ni=1\n0.01xi + ϵ where ϵ ∼N(0, 0.012).\n(3.7.4)\nIn this synthetic dataset, our label is given by an underlying linear function of our inputs,\n\n122\nLinear Neural Networks for Regression\ncorrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative pur-\nposes, we can make the eﬀects of overﬁtting pronounced, by increasing the dimensionality of\nour problem to d = 200 and working with a small training set with only 20 examples.\nclass Data(d2l.DataModule):\ndef __init__(self, num_train, num_val, num_inputs, batch_size):\nself.save_hyperparameters()\nn = num_train + num_val\nself.X = torch.randn(n, num_inputs)\nnoise = torch.randn(n, 1) * 0.01\nw, b = torch.ones((num_inputs, 1)) * 0.01, 0.05\nself.y = torch.matmul(self.X, w) + b + noise\ndef get_dataloader(self, train):\ni = slice(0, self.num_train) if train else slice(self.num_train, None)\nreturn self.get_tensorloader([self.X, self.y], train, i)\n3.7.3 Implementation from Scratch\nNow, let’s try implementing weight decay from scratch. Since minibatch stochastic gradient\ndescent is our optimizer, we just need to add the squared ℓ2 penalty to the original loss\nfunction.\nDeﬁning ℓ2 Norm Penalty\nPerhaps the most convenient way of implementing this penalty is to square all terms in place\nand sum them.\ndef l2_penalty(w):\nreturn (w ** 2).sum() / 2\nDeﬁning the Model\nIn the ﬁnal model, the linear regression and the squared loss have not changed since Section\n3.4, so we will just deﬁne a subclass of d2l.LinearRegressionScratch. The only change\nhere is that our loss now includes the penalty term.\nclass WeightDecayScratch(d2l.LinearRegressionScratch):\ndef __init__(self, num_inputs, lambd, lr, sigma=0.01):\nsuper().__init__(num_inputs, lr, sigma)\nself.save_hyperparameters()\ndef loss(self, y_hat, y):\nreturn (super().loss(y_hat, y) +\nself.lambd * l2_penalty(self.w))\n\n123\nWeight Decay\nThe following code ﬁts our model on the training set with 20 examples and evaluates it on\nthe validation set with 100 examples.\ndata = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\ntrainer = d2l.Trainer(max_epochs=10)\ndef train_scratch(lambd):\nmodel = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\nmodel.board.yscale='log'\ntrainer.fit(model, data)\nprint('L2 norm of w:', float(l2_penalty(model.w)))\nTraining without Regularization\nWe now run this code with lambd = 0, disabling weight decay. Note that we overﬁt badly, de-\ncreasing the training error but not the validation error—a textbook case of overﬁtting.\ntrain_scratch(0)\nL2 norm of w: 0.009889112785458565\nUsing Weight Decay\nBelow, we run with substantial weight decay. Note that the training error increases but the\nvalidation error decreases. This is precisely the eﬀect we expect from regularization.\ntrain_scratch(3)\nL2 norm of w: 0.0014726519584655762\n\n124\nLinear Neural Networks for Regression\n3.7.4 Concise Implementation\nBecause weight decay is ubiquitous in neural network optimization, the deep learning frame-\nwork makes it especially convenient, integrating weight decay into the optimization algorithm\nitself for easy use in combination with any loss function. Moreover, this integration serves a\ncomputational beneﬁt, allowing implementation tricks to add weight decay to the algorithm,\nwithout any additional computational overhead. Since the weight decay portion of the up-\ndate depends only on the current value of each parameter, the optimizer must touch each\nparameter once anyway.\nBelow, we specify the weight decay hyperparameter directly through weight_decay when\ninstantiating our optimizer. By default, PyTorch decays both weights and biases simultane-\nously, but we can conﬁgure the optimizer to handle diﬀerent parameters according to diﬀer-\nent policies. Here, we only set weight_decay for the weights (the net.weight parameters),\nhence the bias (the net.bias parameter) will not decay.\nclass WeightDecay(d2l.LinearRegression):\ndef __init__(self, wd, lr):\nsuper().__init__(lr)\nself.save_hyperparameters()\nself.wd = wd\ndef configure_optimizers(self):\nreturn torch.optim.SGD([\n{'params': self.net.weight, 'weight_decay': self.wd},\n{'params': self.net.bias}], lr=self.lr)\nThe plot looks similar to that when we implemented weight decay from scratch. However,\nthis version runs faster and is easier to implement, beneﬁts that will become more pronounced\nas you address larger problems and this work becomes more routine.\nmodel = WeightDecay(wd=3, lr=0.01)\nmodel.board.yscale='log'\ntrainer.fit(model, data)\nprint('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))\n\n125\nWeight Decay\n84\nL2 norm of w: 0.01231398992240429\nSo far, we have touched upon one notion of what constitutes a simple linear function. How-\never, even for simple nonlinear functions, the situation can be much more complex. To see\nthis, the concept of reproducing kernel Hilbert space (RKHS)84 allows one to apply tools in-\ntroduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms\ntend to scale poorly to large, high-dimensional data. In this book we will often adopt the\ncommon heuristic whereby weight decay is applied to all layers of a deep network.\n3.7.5 Summary\nRegularization is a common method for dealing with overﬁtting. Classical regularization tech-\nniques add a penalty term to the loss function (when training) to reduce the complexity of\nthe learned model. One particular choice for keeping the model simple is using an ℓ2 penalty.\nThis leads to weight decay in the update steps of the minibatch stochastic gradient descent\nalgorithm. In practice, the weight decay functionality is provided in optimizers from deep\nlearning frameworks. Diﬀerent sets of parameters can have diﬀerent update behaviors within\nthe same training loop.\n3.7.6 Exercises\n1. Experiment with the value of λ in the estimation problem in this section. Plot training and\nvalidation accuracy as a function of λ. What do you observe?\n2. Use a validation set to ﬁnd the optimal value of λ. Is it really the optimal value? Does this\nmatter?\n3. What would the update equations look like if instead of ∥w∥2 we used ∑\ni |wi| as our\npenalty of choice (ℓ1 regularization)?\n4. We know that ∥w∥2 = w⊤w. Can you ﬁnd a similar equation for matrices (see the\nFrobenius norm in Section 2.3.11)?\n\n126\nLinear Neural Networks for Regression\n85\n5. Review the relationship between training error and generalization error. In addition to\nweight decay, increased training, and the use of a model of suitable complexity, what\nother ways might help us deal with overﬁtting?\n6. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via\nP(w | x) ∝P(x | w)P(w). How can you identify P(w) with regularization?\nDiscussions85.\n\n4\nLinear Neural Networks for Classiﬁcation\nNow that you have worked through all of the mechanics you are ready to apply the skills\nyou have learned to broader kinds of tasks. Even as we pivot towards classiﬁcation, most of\nthe plumbing remains the same: loading the data, passing it through the model, generating\noutput, calculating the loss, taking gradients with respect to weights, and updating the model.\nHowever, the precise form of the targets, the parametrization of the output layer, and the\nchoice of loss function will adapt to suit the classiﬁcation setting.\n4.1 Softmax Regression\nIn Section 3.1, we introduced linear regression, working through implementations from scratch\nin Section 3.4 and again using high-level APIs of a deep learning framework in Section 3.5\nto do the heavy lifting.\nRegression is the hammer we reach for when we want to answer how much? or how many?\nquestions. If you want to predict the number of dollars (price) at which a house will be sold,\nor the number of wins a baseball team might have, or the number of days that a patient\nwill remain hospitalized before being discharged, then you are probably looking for a re-\ngression model. However, even within regression models, there are important distinctions.\nFor instance, the price of a house will never be negative and changes might often be relative\nto its baseline price. As such, it might be more eﬀective to regress on the logarithm of the\nprice. Likewise, the number of days a patient spends in hospital is a discrete nonnegative ran-\ndom variable. As such, least mean squares might not be an ideal approach either. This sort\nof time-to-event modeling comes with a host of other complications that are dealt with in a\nspecialized subﬁeld called survival modeling.\nThe point here is not to overwhelm you but just to let you know that there is a lot more to\nestimation than simply minimizing squared errors. And more broadly, there is a lot more to\nsupervised learning than regression. In this section, we focus on classiﬁcation problems where\nwe put aside how much? questions and instead focus on which category? questions.\n• Does this email belong in the spam folder or the inbox?\n• Is this customer more likely to sign up or not to sign up for a subscription service?\n127\n\n128\nLinear Neural Networks for Classiﬁcation\n86\n87\n• Does this image depict a donkey, a dog, a cat, or a rooster?\n• Which movie is Aston most likely to watch next?\n• Which section of the book are you going to read next?\nColloquially, machine learning practitioners overload the word classiﬁcation to describe two\nsubtly diﬀerent problems: (i) those where we are interested only in hard assignments of ex-\namples to categories (classes); and (ii) those where we wish to make soft assignments, i.e.,\nto assess the probability that each category applies. The distinction tends to get blurred, in\npart, because often, even when we only care about hard assignments, we still use models that\nmake soft assignments.\nEven more, there are cases where more than one label might be true. For instance, a news\narticle might simultaneously cover the topics of entertainment, business, and space ﬂight, but\nnot the topics of medicine or sports. Thus, categorizing it into one of the above categories\non their own would not be very useful. This problem is commonly known as multi-label\nclassiﬁcation86. See Tsoumakas and Katakis (2007) for an overview and Huang et al. (2015)\nfor an eﬀective algorithm when tagging images.\n4.1.1 Classiﬁcation\nTo get our feet wet, let’s start with a simple image classiﬁcation problem. Here, each input\nconsists of a 2 × 2 grayscale image. We can represent each pixel value with a single scalar,\ngiving us four features x1, x2, x3, x4. Further, let’s assume that each image belongs to one\namong the categories “cat”, “chicken”, and “dog”.\nNext, we have to choose how to represent the labels. We have two obvious choices. Perhaps\nthe most natural impulse would be to choose y ∈{1, 2, 3}, where the integers represent\n{dog, cat, chicken} respectively. This is a great way of storing such information on a computer.\nIf the categories had some natural ordering among them, say if we were trying to predict\n{baby, toddler, adolescent, young adult, adult, geriatric}, then it might even make sense to cast\nthis as an ordinal regression 87 problem and keep the labels in this format. See Moon et al.\n(2010) for an overview of diﬀerent types of ranking loss functions and Beutel et al. (2014)\nfor a Bayesian approach that addresses responses with more than one mode.\nIn general, classiﬁcation problems do not come with natural orderings among the classes.\nFortunately, statisticians long ago invented a simple way to represent categorical data: the one-\nhot encoding. A one-hot encoding is a vector with as many components as we have categories.\nThe component corresponding to a particular instance’s category is set to 1 and all other\ncomponents are set to 0. In our case, a label y would be a three-dimensional vector, with\n(1, 0, 0) corresponding to “cat”, (0, 1, 0) to “chicken”, and (0, 0, 1) to “dog”:\ny ∈{(1, 0, 0), (0, 1, 0), (0, 0, 1)}.\n(4.1.1)\n\n129\nSoftmax Regression\nLinear Model\nIn order to estimate the conditional probabilities associated with all the possible classes, we\nneed a model with multiple outputs, one per class. To address classiﬁcation with linear models,\nwe will need as many aﬃne functions as we have outputs. Strictly speaking, we only need one\nfewer, since the ﬁnal category has to be the diﬀerence between 1 and the sum of the other\ncategories, but for reasons of symmetry we use a slightly redundant parametrization. Each\noutput corresponds to its own aﬃne function. In our case, since we have 4 features and 3\npossible output categories, we need 12 scalars to represent the weights (w with subscripts),\nand 3 scalars to represent the biases (b with subscripts). This yields:\no1 = x1w11 + x2w12 + x3w13 + x4w14 + b1,\no2 = x1w21 + x2w22 + x3w23 + x4w24 + b2,\no3 = x1w31 + x2w32 + x3w33 + x4w34 + b3.\n(4.1.2)\nThe corresponding neural network diagram is shown in Fig. 4.1.1. Just as in linear regression,\nwe use a single-layer neural network. And since the calculation of each output, o1, o2, and\no3, depends on every input, x1, x2, x3, and x4, the output layer can also be described as a\nfully connected layer.\nt\nFig. 4.1.1\nSoftmax regression is a single-layer neural network.\nFor a more concise notation we use vectors and matrices: o = Wx+b is much better suited\nfor mathematics and code. Note that we have gathered all of our weights into a 3 × 4 matrix\nand all biases b ∈R3 in a vector.\nThe Softmax\nAssuming a suitable loss function, we could try, directly, to minimize the diﬀerence between\no and the labels y. While it turns out that treating classiﬁcation as a vector-valued regression\nproblem works surprisingly well, it is nonetheless unsatisfactory in the following ways:\n• There is no guarantee that the outputs oi sum up to 1 in the way we expect probabilities to\nbehave.\n• There is no guarantee that the outputs oi are even nonnegative, even if their outputs sum\nup to 1, or that they do not exceed 1.\nBoth aspects render the estimation problem diﬃcult to solve and the solution very brittle to\noutliers. For instance, if we assume that there is a positive linear dependency between the\nnumber of bedrooms and the likelihood that someone will buy a house, the probability might\n\n130\nLinear Neural Networks for Classiﬁcation\n88\nexceed 1 when it comes to buying a mansion! As such, we need a mechanism to “squish” the\noutputs.\nThere are many ways we might accomplish this goal. For instance, we could assume that the\noutputs o are corrupted versions of y, where the corruption occurs by means of adding noise\nϵ drawn from a normal distribution. In other words, y = o + ϵ, where ϵi ∼N(0, σ2). This\nis the so-called probit model88, ﬁrst introduced by Fechner (1860). While appealing, it does\nnot work quite as well nor lead to a particularly nice optimization problem, when compared\nto the softmax.\nAnother way to accomplish this goal (and to ensure nonnegativity) is to use an exponential\nfunction P(y = i) ∝exp oi. This does indeed satisfy the requirement that the conditional\nclass probability increases with increasing oi, it is monotonic, and all probabilities are non-\nnegative. We can then transform these values so that they add up to 1 by dividing each by\ntheir sum. This process is called normalization. Putting these two pieces together gives us the\nsoftmax function:\nˆy = softmax(o)\nwhere\nˆyi =\nexp(oi)\n∑\nj exp(oj).\n(4.1.3)\nNote that the largest coordinate of o corresponds to the most likely class according to ˆy.\nMoreover, because the softmax operation preserves the ordering among its arguments, we\ndo not need to compute the softmax to determine which class has been assigned the highest\nprobability. Thus,\nargmax\nj\nˆyj = argmax\nj\noj.\n(4.1.4)\nThe idea of a softmax dates back to Gibbs (1902), who adapted ideas from physics. Dat-\ning even further back, Boltzmann, the father of modern statistical physics, used this trick to\nmodel a distribution over energy states in gas molecules. In particular, he discovered that the\nprevalence of a state of energy in a thermodynamic ensemble, such as the molecules in a gas,\nis proportional to exp(−E/kT). Here, E is the energy of a state, T is the temperature, and k\nis the Boltzmann constant. When statisticians talk about increasing or decreasing the “tem-\nperature” of a statistical system, they refer to changing T in order to favor lower or higher\nenergy states. Following Gibbs’ idea, energy equates to error. Energy-based models (Ranzato\net al., 2007) use this point of view when describing problems in deep learning.\nVectorization\nTo improve computational eﬃciency, we vectorize calculations in minibatches of data. As-\nsume that we are given a minibatch X ∈Rn×d of n examples with dimensionality (number\nof inputs) d. Moreover, assume that we have q categories in the output. Then the weights\nsatisfy W ∈Rd×q and the bias satisﬁes b ∈R1×q.\nO = XW + b,\nˆY = softmax(O).\n(4.1.5)\n\n131\nSoftmax Regression\nThis accelerates the dominant operation into a matrix–matrix product XW. Moreover, since\neach row in X represents a data example, the softmax operation itself can be computed\nrowwise: for each row of O, exponentiate all entries and then normalize them by the sum.\nNote, though, that care must be taken to avoid exponentiating and taking logarithms of large\nnumbers, since this can cause numerical overﬂow or underﬂow. Deep learning frameworks\ntake care of this automatically.\n4.1.2 Loss Function\nNow that we have a mapping from features x to probabilities ˆy, we need a way to optimize\nthe accuracy of this mapping. We will rely on maximum likelihood estimation, the very same\nmethod that we encountered when providing a probabilistic justiﬁcation for the mean squared\nerror loss in Section 3.1.3.\nLog-Likelihood\nThe softmax function gives us a vector ˆy, which we can interpret as the (estimated) condi-\ntional probabilities of each class, given any input x, such as ˆy1 = P(y = cat | x). In the\nfollowing we assume that for a dataset with features X the labels Y are represented using a\none-hot encoding label vector. We can compare the estimates with reality by checking how\nprobable the actual classes are according to our model, given the features:\nP(Y | X) =\nn\n∏\ni=1\nP(y(i) | x(i)).\n(4.1.6)\nWe are allowed to use the factorization since we assume that each label is drawn indepen-\ndently from its respective distribution P(y | x(i)). Since maximizing the product of terms is\nawkward, we take the negative logarithm to obtain the equivalent problem of minimizing the\nnegative log-likelihood:\n−log P(Y | X) =\nn\n∑\ni=1\n−log P(y(i) | x(i)) =\nn\n∑\ni=1\nl(y(i), ˆy(i)),\n(4.1.7)\nwhere for any pair of label y and model prediction ˆy over q classes, the loss function l\nis\nl(y, ˆy) = −\nq\n∑\nj=1\nyj log ˆyj.\n(4.1.8)\nFor reasons explained later on, the loss function in (4.1.8) is commonly called the cross-\nentropy loss. Since y is a one-hot vector of length q, the sum over all its coordinates j vanishes\nfor all but one term. Note that the loss l(y, ˆy) is bounded from below by 0 whenever ˆy is\na probability vector: no single entry is larger than 1, hence their negative logarithm cannot\nbe lower than 0; l(y, ˆy) = 0 only if we predict the actual label with certainty. This can\nnever happen for any ﬁnite setting of the weights because taking a softmax output towards\n1 requires taking the corresponding input oi to inﬁnity (or all other outputs oj for j , i to\n\n132\nLinear Neural Networks for Classiﬁcation\nnegative inﬁnity). Even if our model could assign an output probability of 0, any error made\nwhen assigning such high conﬁdence would incur inﬁnite loss (−log 0 = ∞).\nSoftmax and Cross-Entropy Loss\nSince the softmax function and the corresponding cross-entropy loss are so common, it is\nworth understanding a bit better how they are computed. Plugging (4.1.3) into the deﬁnition\nof the loss in (4.1.8) and using the deﬁnition of the softmax we obtain\nl(y, ˆy) = −\nq\n∑\nj=1\nyj log\nexp(oj)\n∑q\nk=1 exp(ok)\n=\nq\n∑\nj=1\nyj log\nq\n∑\nk=1\nexp(ok) −\nq\n∑\nj=1\nyjoj\n= log\nq\n∑\nk=1\nexp(ok) −\nq\n∑\nj=1\nyjoj.\n(4.1.9)\nTo understand a bit better what is going on, consider the derivative with respect to any logit\noj. We get\n∂oj l(y, ˆy) =\nexp(oj)\n∑q\nk=1 exp(ok) −yj = softmax(o)j −yj.\n(4.1.10)\nIn other words, the derivative is the diﬀerence between the probability assigned by our model,\nas expressed by the softmax operation, and what actually happened, as expressed by elements\nin the one-hot label vector. In this sense, it is very similar to what we saw in regression,\nwhere the gradient was the diﬀerence between the observation y and estimate ˆy. This is not\na coincidence. In any exponential family model, the gradients of the log-likelihood are given\nby precisely this term. This fact makes computing gradients easy in practice.\nNow consider the case where we observe not just a single outcome but an entire distribution\nover outcomes. We can use the same representation as before for the label y. The only dif-\nference is that rather than a vector containing only binary entries, say (0, 0, 1), we now have\na generic probability vector, say (0.1, 0.2, 0.7). The math that we used previously to deﬁne\nthe loss l in (4.1.8) still works well, just that the interpretation is slightly more general. It\nis the expected value of the loss for a distribution over labels. This loss is called the cross-\nentropy loss and it is one of the most commonly used losses for classiﬁcation problems. We\ncan demystify the name by introducing just the basics of information theory. In a nutshell, it\nmeasures the number of bits needed to encode what we see, y, relative to what we predict that\nshould happen, ˆy. We provide a very basic explanation in the following. For further details\non information theory see Cover and Thomas (1999) or MacKay (2003).\n4.1.3 Information Theory Basics\n\n133\nSoftmax Regression\nMany deep learning papers use intuition and terms from information theory. To make sense\nof them, we need some common language. This is a survival guide. Information theory deals\nwith the problem of encoding, decoding, transmitting, and manipulating information (also\nknown as data).\nEntropy\nThe central idea in information theory is to quantify the amount of information contained\nin data. This places a limit on our ability to compress data. For a distribution P its entropy,\nH[P], is deﬁned as:\nH[P] =\n∑\nj\n−P(j) log P(j).\n(4.1.11)\nOne of the fundamental theorems of information theory states that in order to encode data\ndrawn randomly from the distribution P, we need at least H[P] “nats” to encode it (Shannon,\n1948). If you wonder what a “nat” is, it is the equivalent of bit but when using a code with\nbase e rather than one with base 2. Thus, one nat is\n1\nlog(2) ≈1.44 bit.\nSurprisal\nYou might be wondering what compression has to do with prediction. Imagine that we have\na stream of data that we want to compress. If it is always easy for us to predict the next\ntoken, then this data is easy to compress. Take the extreme example where every token in\nthe stream always takes the same value. That is a very boring data stream! And not only it is\nboring, but it is also easy to predict. Because the tokens are always the same, we do not have\nto transmit any information to communicate the contents of the stream. Easy to predict, easy\nto compress.\nHowever if we cannot perfectly predict every event, then we might sometimes be surprised.\nOur surprise is greater when an event is assigned lower probability. Claude Shannon settled\non log\n1\nP(j) = −log P(j) to quantify one’s surprisal at observing an event j having assigned\nit a (subjective) probability P(j). The entropy deﬁned in (4.1.11) is then the expected sur-\nprisal when one assigned the correct probabilities that truly match the data-generating pro-\ncess.\nCross-Entropy Revisited\nSo if entropy is the level of surprise experienced by someone who knows the true probability,\nthen you might be wondering, what is cross-entropy? The cross-entropy from P to Q, denoted\nH(P, Q), is the expected surprisal of an observer with subjective probabilities Q upon seeing\ndata that was actually generated according to probabilities P. This is given by H(P, Q) def\n=\n\n134\nLinear Neural Networks for Classiﬁcation\n∑\nj −P(j) log Q(j). The lowest possible cross-entropy is achieved when P = Q. In this case,\nthe cross-entropy from P to Q is H(P, P) = H(P).\nIn short, we can think of the cross-entropy classiﬁcation objective in two ways: (i) as maxi-\nmizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus the\nnumber of bits) required to communicate the labels.\n4.1.4 Summary and Discussion\nIn this section, we encountered the ﬁrst nontrivial loss function, allowing us to optimize over\ndiscrete output spaces. Key in its design was that we took a probabilistic approach, treating\ndiscrete categories as instances of draws from a probability distribution. As a side eﬀect,\nwe encountered the softmax, a convenient activation function that transforms outputs of an\nordinary neural network layer into valid discrete probability distributions. We saw that the\nderivative of the cross-entropy loss when combined with softmax behaves very similarly to\nthe derivative of squared error; namely by taking the diﬀerence between the expected be-\nhavior and its prediction. And, while we were only able to scratch the very surface of it, we\nencountered exciting connections to statistical physics and information theory.\nWhile this is enough to get you on your way, and hopefully enough to whet your appetite, we\nhardly dived deep here. Among other things, we skipped over computational considerations.\nSpeciﬁcally, for any fully connected layer with d inputs and q outputs, the parametrization\nand computational cost is O(dq), which can be prohibitively high in practice. Fortunately,\nthis cost of transforming d inputs into q outputs can be reduced through approximation and\ncompression. For instance Deep Fried Convnets (Yang et al., 2015) uses a combination of\npermutations, Fourier transforms, and scaling to reduce the cost from quadratic to log-linear.\nSimilar techniques work for more advanced structural matrix approximations (Sindhwani et\nal., 2015). Lastly, we can use quaternion-like decompositions to reduce the cost to O( dq\nn ),\nagain if we are willing to trade oﬀa small amount of accuracy for computational and storage\ncost (Zhang et al., 2021) based on a compression factor n. This is an active area of research.\nWhat makes it challenging is that we do not necessarily strive for the most compact repre-\nsentation or the smallest number of ﬂoating point operations but rather for the solution that\ncan be executed most eﬃciently on modern GPUs.\n4.1.5 Exercises\n1. We can explore the connection between exponential families and softmax in some more\ndepth.\n1. Compute the second derivative of the cross-entropy loss l(y, ˆy) for softmax.\n2. Compute the variance of the distribution given by softmax(o) and show that it matches\nthe second derivative computed above.\n2. Assume that we have three classes which occur with equal probability, i.e., the probability\nvector is ( 1\n3, 1\n3, 1\n3).\n\n135\nSoftmax Regression\n89\n90\n91\n1. What is the problem if we try to design a binary code for it?\n2. Can you design a better code? Hint: what happens if we try to encode two independent\nobservations? What if we encode n observations jointly?\n3. When encoding signals transmitted over a physical wire, engineers do not always use bi-\nnary codes. For instance, PAM-389 uses three signal levels {−1, 0, 1} as opposed to two\nlevels {0, 1}. How many ternary units do you need to transmit an integer in the range\n{0, . . ., 7}? Why might this be a better idea in terms of electronics?\n4. The Bradley–Terry model 90 uses a logistic model to capture preferences. For a user to\nchoose between apples and oranges one assumes scores oapple and oorange. Our require-\nments are that larger scores should lead to a higher likelihood in choosing the associated\nitem and that the item with the largest score is the most likely one to be chosen (Bradley\nand Terry, 1952).\n1. Prove that softmax satisﬁes this requirement.\n2. What happens if you want to allow for a default option of choosing neither apples nor\noranges? Hint: now the user has three choices.\n5. Softmax gets its name from the following mapping: RealSoftMax(a, b) = log(exp(a) +\nexp(b)).\n1. Prove that RealSoftMax(a, b) > max(a, b).\n2. How small can you make the diﬀerence between both functions? Hint: without loss of\ngenerality you can set b = 0 and a ≥b.\n3. Prove that this holds for λ−1RealSoftMax(λa, λb), provided that λ > 0.\n4. Show that for λ →∞we have λ−1RealSoftMax(λa, λb) →max(a, b).\n5. Construct an analogous softmin function.\n6. Extend this to more than two numbers.\n6. The function g(x) def\n= log ∑\ni exp xi is sometimes also referred to as the log-partition\nfunction91.\n1. Prove that the function is convex. Hint: to do so, use the fact that the ﬁrst deriva-\ntive amounts to the probabilities from the softmax function and show that the second\nderivative is the variance.\n2. Show that g is translation invariant, i.e., g(x + b) = g(x).\n3. What happens if some of the coordinates xi are very large? What happens if they’re\nall very small?\n4. Show that if we choose b = maxixi we end up with a numerically stable implemen-\ntation.\n7. Assume that we have some probability distribution P. Suppose we pick another distribu-\ntion Q with Q(i) ∝P(i)α for α > 0.\n\n136\nLinear Neural Networks for Classiﬁcation\n92\n93\n1. Which choice of α corresponds to doubling the temperature? Which choice corre-\nsponds to halving it?\n2. What happens if we let the temperature approach 0?\n3. What happens if we let the temperature approach ∞?\nDiscussions92.\n4.2 The Image Classiﬁcation Dataset\nOne widely used dataset for image classiﬁcation is the MNIST dataset93 (LeCun et al., 1998)\nof handwritten digits. At the time of its release in the 1990s it posed a formidable challenge\nto most machine learning algorithms, consisting of 60,000 images of 28×28 pixels resolution\n(plus a test dataset of 10,000 images). To put things into perspective, back in 1995, a Sun\nSPARCStation 5 with a whopping 64MB of RAM and a blistering 5 MFLOPs was considered\nstate of the art equipment for machine learning at AT&T Bell Laboratories. Achieving high\naccuracy on digit recognition was a key component in automating letter sorting for the USPS\nin the 1990s. Deep networks such as LeNet-5 (LeCun et al., 1995), support vector machines\nwith invariances (Schölkopf et al., 1996), and tangent distance classiﬁers (Simard et al., 1998)\nall could reach error rates below 1%.\nFor over a decade, MNIST served as the point of reference for comparing machine learning\nalgorithms. While it had a good run as a benchmark dataset, even simple models by today’s\nstandards achieve classiﬁcation accuracy over 95%, making it unsuitable for distinguishing\nbetween strong models and weaker ones. Even more, the dataset allows for very high levels\nof accuracy, not typically seen in many classiﬁcation problems. This skewed algorithmic de-\nvelopment towards speciﬁc families of algorithms that can take advantage of clean datasets,\nsuch as active set methods and boundary-seeking active set algorithms. Today, MNIST serves\nas more of a sanity check than as a benchmark. ImageNet (Deng et al., 2009) poses a much\nmore relevant challenge. Unfortunately, ImageNet is too large for many of the examples and\nillustrations in this book, as it would take too long to train to make the examples interac-\ntive. As a substitute we will focus our discussion in the coming sections on the qualitatively\nsimilar, but much smaller Fashion-MNIST dataset (Xiao et al., 2017) which was released in\n2017. It contains images of 10 categories of clothing at 28 × 28 pixels resolution.\n%matplotlib inline\nimport time\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom d2l import torch as d2l\nd2l.use_svg_display()\n\n137\nThe Image Classiﬁcation Dataset\n4.2.1 Loading the Dataset\nSince the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed\nversions of it. We can download and read it into memory using built-in framework utili-\nties.\nclass FashionMNIST(d2l.DataModule):\n#@save\n\"\"\"The Fashion-MNIST dataset.\"\"\"\ndef __init__(self, batch_size=64, resize=(28, 28)):\nsuper().__init__()\nself.save_hyperparameters()\ntrans = transforms.Compose([transforms.Resize(resize),\ntransforms.ToTensor()])\nself.train = torchvision.datasets.FashionMNIST(\nroot=self.root, train=True, transform=trans, download=True)\nself.val = torchvision.datasets.FashionMNIST(\nroot=self.root, train=False, transform=trans, download=True)\nFashion-MNIST consists of images from 10 categories, each represented by 6000 images in\nthe training dataset and by 1000 in the test dataset. A test dataset is used for evaluating model\nperformance (it must not be used for training). Consequently the training set and the test set\ncontain 60,000 and 10,000 images, respectively.\ndata = FashionMNIST(resize=(32, 32))\nlen(data.train), len(data.val)\n(60000, 10000)\nThe images are grayscale and upscaled to 32 × 32 pixels in resolution above. This is similar\nto the original MNIST dataset which consisted of (binary) black and white images. Note,\nthough, that most modern image data has three channels (red, green, blue) and that hyper-\nspectral images can have in excess of 100 channels (the HyMap sensor has 126 channels). By\nconvention we store an image as a c × h × w tensor, where c is the number of color channels,\nh is the height and w is the width.\ndata.train[0][0].shape\ntorch.Size([1, 32, 32])\nThe categories of Fashion-MNIST have human-understandable names. The following con-\nvenience method converts between numeric labels and their names.\n@d2l.add_to_class(FashionMNIST)\n#@save\ndef text_labels(self, indices):\n\"\"\"Return text labels.\"\"\"\nlabels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n(continues on next page)\n\n138\nLinear Neural Networks for Classiﬁcation\n(continued from previous page)\n'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\nreturn [labels[int(i)] for i in indices]\n4.2.2 Reading a Minibatch\nTo make our life easier when reading from the training and test sets, we use the built-in data\niterator rather than creating one from scratch. Recall that at each iteration, a data iterator\nreads a minibatch of data with size batch_size. We also randomly shuﬄe the examples for\nthe training data iterator.\n@d2l.add_to_class(FashionMNIST)\n#@save\ndef get_dataloader(self, train):\ndata = self.train if train else self.val\nreturn torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,\nnum_workers=self.num_workers)\nTo see how this works, let’s load a minibatch of images by invoking the train_dataloader\nmethod. It contains 64 images.\nX, y = next(iter(data.train_dataloader()))\nprint(X.shape, X.dtype, y.shape, y.dtype)\ntorch.Size([64, 1, 32, 32]) torch.float32 torch.Size([64]) torch.int64\nLet’s look at the time it takes to read the images. Even though it is a built-in loader, it is\nnot blazingly fast. Nonetheless, this is suﬃcient since processing images with a deep network\ntakes quite a bit longer. Hence it is good enough that training a network will not be I/O\nconstrained.\ntic = time.time()\nfor X, y in data.train_dataloader():\ncontinue\nf'{time.time() - tic:.2f} sec'\n'5.41 sec'\n4.2.3 Visualization\nWe will often be using the Fashion-MNIST dataset. A convenience function show_images\ncan be used to visualize the images and the associated labels. Skipping implementation details,\nwe just show the interface below: we only need to know how to invoke d2l.show_images\nrather than how it works for such utility functions.\n\n139\nThe Image Classiﬁcation Dataset\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n#@save\n\"\"\"Plot a list of images.\"\"\"\nraise NotImplementedError\nLet’s put it to good use. In general, it is a good idea to visualize and inspect data that you\nare training on. Humans are very good at spotting oddities and because of that, visualization\nserves as an additional safeguard against mistakes and errors in the design of experiments.\nHere are the images and their corresponding labels (in text) for the ﬁrst few examples in the\ntraining dataset.\n@d2l.add_to_class(FashionMNIST)\n#@save\ndef visualize(self, batch, nrows=1, ncols=8, labels=[]):\nX, y = batch\nif not labels:\nlabels = self.text_labels(y)\nd2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)\nbatch = next(iter(data.val_dataloader()))\ndata.visualize(batch)\nWe are now ready to work with the Fashion-MNIST dataset in the sections that follow.\n4.2.4 Summary\nWe now have a slightly more realistic dataset to use for classiﬁcation. Fashion-MNIST is an\napparel classiﬁcation dataset consisting of images representing 10 categories. We will use\nthis dataset in subsequent sections and chapters to evaluate various network designs, from\na simple linear model to advanced residual networks. As we commonly do with images, we\nread them as a tensor of shape (batch size, number of channels, height, width). For now, we\nonly have one channel as the images are grayscale (the visualization above uses a false color\npalette for improved visibility).\nLastly, data iterators are a key component for eﬃcient performance. For instance, we might\nuse GPUs for eﬃcient image decompression, video transcoding, or other preprocessing.\nWhenever possible, you should rely on well-implemented data iterators that exploit high-\nperformance computing to avoid slowing down your training loop.\n4.2.5 Exercises\n1. Does reducing the batch_size (for instance, to 1) aﬀect the reading performance?\n\n140\nLinear Neural Networks for Classiﬁcation\n94\n2. The data iterator performance is important. Do you think the current implementation is\nfast enough? Explore various options to improve it. Use a system proﬁler to ﬁnd out where\nthe bottlenecks are.\n3. Check out the framework’s online API documentation. Which other datasets are available?\nDiscussions94.\n4.3 The Base Classiﬁcation Model\nYou may have noticed that the implementations from scratch and the concise implementa-\ntion using framework functionality were quite similar in the case of regression. The same\nis true for classiﬁcation. Since many models in this book deal with classiﬁcation, it is worth\nadding functionalities to support this setting speciﬁcally. This section provides a base class\nfor classiﬁcation models to simplify future code.\nimport torch\nfrom d2l import torch as d2l\n4.3.1 The Classifier Class\nWe deﬁne the Classifier class below. In the validation_step we report both the loss\nvalue and the classiﬁcation accuracy on a validation batch. We draw an update for every\nnum_val_batches batches. This has the beneﬁt of generating the averaged loss and accuracy\non the whole validation data. These average numbers are not exactly correct if the ﬁnal batch\ncontains fewer examples, but we ignore this minor diﬀerence to keep the code simple.\nclass Classifier(d2l.Module):\n#@save\n\"\"\"The base class of classification models.\"\"\"\ndef validation_step(self, batch):\nY_hat = self(*batch[:-1])\nself.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\nself.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)\nBy default we use a stochastic gradient descent optimizer, operating on minibatches, just as\nwe did in the context of linear regression.\n@d2l.add_to_class(d2l.Module)\n#@save\ndef configure_optimizers(self):\nreturn torch.optim.SGD(self.parameters(), lr=self.lr)\n\n141\nThe Base Classiﬁcation Model\n4.3.2 Accuracy\nGiven the predicted probability distribution y_hat, we typically choose the class with the\nhighest predicted probability whenever we must output a hard prediction. Indeed, many ap-\nplications require that we make a choice. For instance, Gmail must categorize an email into\n“Primary”, “Social”, “Updates”, “Forums”, or “Spam”. It might estimate probabilities inter-\nnally, but at the end of the day it has to choose one among the classes.\nWhen predictions are consistent with the label class y, they are correct. The classiﬁcation\naccuracy is the fraction of all predictions that are correct. Although it can be diﬃcult to\noptimize accuracy directly (it is not diﬀerentiable), it is often the performance measure that\nwe care about the most. It is often the relevant quantity in benchmarks. As such, we will\nnearly always report it when training classiﬁers.\nAccuracy is computed as follows. First, if y_hat is a matrix, we assume that the second\ndimension stores prediction scores for each class. We use argmax to obtain the predicted\nclass by the index for the largest entry in each row. Then we compare the predicted class with\nthe ground truth y elementwise. Since the equality operator == is sensitive to data types, we\nconvert y_hat’s data type to match that of y. The result is a tensor containing entries of 0\n(false) and 1 (true). Taking the sum yields the number of correct predictions.\n@d2l.add_to_class(Classifier)\n#@save\ndef accuracy(self, Y_hat, Y, averaged=True):\n\"\"\"Compute the number of correct predictions.\"\"\"\nY_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\npreds = Y_hat.argmax(axis=1).type(Y.dtype)\ncompare = (preds == Y.reshape(-1)).type(torch.float32)\nreturn compare.mean() if averaged else compare\n4.3.3 Summary\nClassiﬁcation is a suﬃciently common problem that it warrants its own convenience func-\ntions. Of central importance in classiﬁcation is the accuracy of the classiﬁer. Note that while\nwe often care primarily about accuracy, we train classiﬁers to optimize a variety of other ob-\njectives for statistical and computational reasons. However, regardless of which loss function\nwas minimized during training, it is useful to have a convenience method for assessing the\naccuracy of our classiﬁer empirically.\n4.3.4 Exercises\n1. Denote by Lv the validation loss, and let Lq\nv be its quick and dirty estimate computed by the\nloss function averaging in this section. Lastly, denote by lb\nv the loss on the last minibatch.\nExpress Lv in terms of Lq\nv, lb\nv , and the sample and minibatch sizes.\n2. Show that the quick and dirty estimate Lq\nv is unbiased. That is, show that E[Lv] = E[Lq\nv].\nWhy would you still want to use Lv instead?\n\n142\nLinear Neural Networks for Classiﬁcation\n95\n96\n3. Given a multiclass classiﬁcation loss, denoting by l(y, y′) the penalty of estimating y′\nwhen we see y and given a probabilty p(y | x), formulate the rule for an optimal selection\nof y′. Hint: express the expected loss, using l and p(y | x).\nDiscussions95.\n4.4 Softmax Regression Implementation from\nScratch\nBecause softmax regression is so fundamental, we believe that you ought to know how to\nimplement it yourself. Here, we limit ourselves to deﬁning the softmax-speciﬁc aspects of\nthe model and reuse the other components from our linear regression section, including the\ntraining loop.\nimport torch\nfrom d2l import torch as d2l\n4.4.1 The Softmax\nLet’s begin with the most important part: the mapping from scalars to probabilities. For a\nrefresher, recall the operation of the sum operator along speciﬁc dimensions in a tensor, as\ndiscussed in Section 2.3.6 and Section 2.3.7. Given a matrix X we can sum over all elements\n(by default) or only over elements in the same axis. The axis variable lets us compute row\nand column sums:\nX = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nX.sum(0, keepdims=True), X.sum(1, keepdims=True)\n(tensor([[5., 7., 9.]]),\ntensor([[ 6.],\n[15.]]))\nComputing the softmax requires three steps: (i) exponentiation of each term; (ii) a sum over\neach row to compute the normalization constant for each example; (iii) division of each row\nby its normalization constant, ensuring that the result sums to 1:\nsoftmax(X)ij =\nexp(Xij)\n∑\nk exp(Xik).\n(4.4.1)\nThe (logarithm of the) denominator is called the (log) partition function. It was introduced\nin statistical physics 96 to sum over all possible states in a thermodynamic ensemble. The\nimplementation is straightforward:\n\n143\nSoftmax Regression Implementation from Scratch\ndef softmax(X):\nX_exp = torch.exp(X)\npartition = X_exp.sum(1, keepdims=True)\nreturn X_exp / partition\n# The broadcasting mechanism is applied here\nFor any input X, we turn each element into a nonnegative number. Each row sums up to 1,\nas is required for a probability. Caution: the code above is not robust against very large or\nvery small arguments. While it is suﬃcient to illustrate what is happening, you should not use\nthis code verbatim for any serious purpose. Deep learning frameworks have such protections\nbuilt in and we will be using the built-in softmax going forward.\nX = torch.rand((2, 5))\nX_prob = softmax(X)\nX_prob, X_prob.sum(1)\n(tensor([[0.1373, 0.2025, 0.2170, 0.2233, 0.2199],\n[0.2669, 0.1146, 0.2801, 0.1372, 0.2012]]),\ntensor([1., 1.]))\n4.4.2 The Model\nWe now have everything that we need to implement the softmax regression model. As in our\nlinear regression example, each instance will be represented by a ﬁxed-length vector. Since\nthe raw data here consists of 28 × 28 pixel images, we ﬂatten each image, treating them as\nvectors of length 784. In later chapters, we will introduce convolutional neural networks,\nwhich exploit the spatial structure in a more satisfying way.\nIn softmax regression, the number of outputs from our network should be equal to the number\nof classes. Since our dataset has 10 classes, our network has an output dimension of 10.\nConsequently, our weights constitute a 784 × 10 matrix plus a 1 × 10 row vector for the\nbiases. As with linear regression, we initialize the weights W with Gaussian noise. The biases\nare initialized as zeros.\nclass SoftmaxRegressionScratch(d2l.Classifier):\ndef __init__(self, num_inputs, num_outputs, lr, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),\nrequires_grad=True)\nself.b = torch.zeros(num_outputs, requires_grad=True)\ndef parameters(self):\nreturn [self.W, self.b]\nThe code below deﬁnes how the network maps each input to an output. Note that we ﬂatten\neach 28 × 28 pixel image in the batch into a vector using reshape before passing the data\nthrough our model.\n\n144\nLinear Neural Networks for Classiﬁcation\n@d2l.add_to_class(SoftmaxRegressionScratch)\ndef forward(self, X):\nX = X.reshape((-1, self.W.shape[0]))\nreturn softmax(torch.matmul(X, self.W) + self.b)\n4.4.3 The Cross-Entropy Loss\nNext we need to implement the cross-entropy loss function (introduced in Section 4.1.2). This\nmay be the most common loss function in all of deep learning. At the moment, applications\nof deep learning easily cast as classiﬁcation problems far outnumber those better treated as\nregression problems.\nRecall that cross-entropy takes the negative log-likelihood of the predicted probability as-\nsigned to the true label. For eﬃciency we avoid Python for-loops and use indexing instead.\nIn particular, the one-hot encoding in y allows us to select the matching terms in ˆy.\nTo see this in action we create sample data y_hat with 2 examples of predicted probabilities\nover 3 classes and their corresponding labels y. The correct labels are 0 and 2 respectively\n(i.e., the ﬁrst and third class). Using y as the indices of the probabilities in y_hat, we can\npick out terms eﬃciently.\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\ntensor([0.1000, 0.5000])\nNow we can implement the cross-entropy loss function by averaging over the logarithms of\nthe selected probabilities.\ndef cross_entropy(y_hat, y):\nreturn -torch.log(y_hat[list(range(len(y_hat))), y]).mean()\ncross_entropy(y_hat, y)\ntensor(1.4979)\n@d2l.add_to_class(SoftmaxRegressionScratch)\ndef loss(self, y_hat, y):\nreturn cross_entropy(y_hat, y)\n4.4.4 Training\n\n145\nSoftmax Regression Implementation from Scratch\nWe reuse the fit method deﬁned in Section 3.4 to train the model with 10 epochs. Note\nthat the number of epochs (max_epochs), the minibatch size (batch_size), and learning\nrate (lr) are adjustable hyperparameters. That means that while these values are not learned\nduring our primary training loop, they still inﬂuence the performance of our model, both\nvis-à-vis training and generalization performance. In practice you will want to choose these\nvalues based on the validation split of the data and then, ultimately, to evaluate your ﬁnal\nmodel on the test split. As discussed in Section 3.6.3, we will regard the test data of Fashion-\nMNIST as the validation set, thus reporting validation loss and validation accuracy on this\nsplit.\ndata = d2l.FashionMNIST(batch_size=256)\nmodel = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n4.4.5 Prediction\nNow that training is complete, our model is ready to classify some images.\nX, y = next(iter(data.val_dataloader()))\npreds = model(X).argmax(axis=1)\npreds.shape\ntorch.Size([256])\nWe are more interested in the images we label incorrectly. We visualize them by comparing\ntheir actual labels (ﬁrst line of text output) with the predictions from the model (second line\nof text output).\nwrong = preds.type(y.dtype) != y\nX, y, preds = X[wrong], y[wrong], preds[wrong]\n(continues on next page)\n\n146\nLinear Neural Networks for Classiﬁcation\n(continued from previous page)\nlabels = [a+'\\n'+b for a, b in zip(\ndata.text_labels(y), data.text_labels(preds))]\ndata.visualize([X, y], labels=labels)\n4.4.6 Summary\nBy now we are starting to get some experience with solving linear regression and classiﬁcation\nproblems. With it, we have reached what would arguably be the state of the art of 1960–1970s\nof statistical modeling. In the next section, we will show you how to leverage deep learning\nframeworks to implement this model much more eﬃciently.\n4.4.7 Exercises\n1. In this section, we directly implemented the softmax function based on the mathematical\ndeﬁnition of the softmax operation. As discussed in Section 4.1 this can cause numerical\ninstabilities.\n1. Test whether softmax still works correctly if an input has a value of 100.\n2. Test whether softmax still works correctly if the largest of all inputs is smaller than\n−100?\n3. Implement a ﬁx by looking at the value relative to the largest entry in the argument.\n2. Implement a cross_entropy function that follows the deﬁnition of the cross-entropy loss\nfunction ∑\ni yi log ˆyi.\n1. Try it out in the code example of this section.\n2. Why do you think it runs more slowly?\n3. Should you use it? When would it make sense to?\n4. What do you need to be careful of? Hint: consider the domain of the logarithm.\n3. Is it always a good idea to return the most likely label? For example, would you do this\nfor medical diagnosis? How would you try to address this?\n4. Assume that we want to use softmax regression to predict the next word based on some\nfeatures. What are some problems that might arise from a large vocabulary?\n5. Experiment with the hyperparameters of the code in this section. In particular:\n\n147\nConcise Implementation of Softmax Regression\n97\n1. Plot how the validation loss changes as you change the learning rate.\n2. Do the validation and training loss change as you change the minibatch size? How large\nor small do you need to go before you see an eﬀect?\nDiscussions97.\n4.5 Concise Implementation of Softmax Regression\nJust as high-level deep learning frameworks made it easier to implement linear regression\n(see Section 3.5), they are similarly convenient here.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n4.5.1 Deﬁning the Model\nAs in Section 3.5, we construct our fully connected layer using the built-in layer. The built-in\n__call__ method then invokes forward whenever we need to apply the network to some\ninput.\nWe use a Flatten layer to convert the fourth-order tensor X to second order by keeping the\ndimensionality along the ﬁrst axis unchanged.\nclass SoftmaxRegression(d2l.Classifier):\n#@save\n\"\"\"The softmax regression model.\"\"\"\ndef __init__(self, num_outputs, lr):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(nn.Flatten(),\nnn.LazyLinear(num_outputs))\ndef forward(self, X):\nreturn self.net(X)\n4.5.2 Softmax Revisited\nIn Section 4.4 we calculated our model’s output and applied the cross-entropy loss. While\nthis is perfectly reasonable mathematically, it is risky computationally, because of numerical\nunderﬂow and overﬂow in the exponentiation.\n\n148\nLinear Neural Networks for Classiﬁcation\n98\nRecall that the softmax function computes probabilities via ˆyj =\nexp(oj)\n∑\nk exp(ok). If some of the\nok are very large, i.e., very positive, then exp(ok) might be larger than the largest number\nwe can have for certain data types. This is called overﬂow. Likewise, if every argument is\na very large negative number, we will get underﬂow. For instance, single precision ﬂoating\npoint numbers approximately cover the range of 10−38 to 1038. As such, if the largest term\nin o lies outside the interval [−90, 90], the result will not be stable. A way round this problem\nis to subtract ¯o def\n= maxk ok from all entries:\nˆyj =\nexp oj\n∑\nk exp ok\n=\nexp(oj −¯o) exp ¯o\n∑\nk exp(ok −¯o) exp ¯o =\nexp(oj −¯o)\n∑\nk exp(ok −¯o).\n(4.5.1)\nBy construction we know that oj−¯o ≤0 for all j. As such, for a q-class classiﬁcation problem,\nthe denominator is contained in the interval [1, q]. Moreover, the numerator never exceeds\n1, thus preventing numerical overﬂow. Numerical underﬂow only occurs when exp(oj −¯o)\nnumerically evaluates as 0. Nonetheless, a few steps down the road we might ﬁnd ourselves in\ntrouble when we want to compute log ˆyj as log 0. In particular, in backpropagation, we might\nﬁnd ourselves faced with a screenful of the dreaded NaN (Not a Number) results.\nFortunately, we are saved by the fact that even though we are computing exponential func-\ntions, we ultimately intend to take their log (when calculating the cross-entropy loss). By com-\nbining softmax and cross-entropy, we can escape the numerical stability issues altogether. We\nhave:\nlog ˆyj = log\nexp(oj −¯o)\n∑\nk exp(ok −¯o) = oj −¯o −log\n∑\nk\nexp(ok −¯o).\n(4.5.2)\nThis avoids both overﬂow and underﬂow. We will want to keep the conventional softmax\nfunction handy in case we ever want to evaluate the output probabilities by our model. But\ninstead of passing softmax probabilities into our new loss function, we just pass the logits and\ncompute the softmax and its log all at once inside the cross-entropy loss function, which does\nsmart things like the “LogSumExp trick”98.\n@d2l.add_to_class(d2l.Classifier)\n#@save\ndef loss(self, Y_hat, Y, averaged=True):\nY_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\nY = Y.reshape((-1,))\nreturn F.cross_entropy(\nY_hat, Y, reduction='mean' if averaged else 'none')\n4.5.3 Training\nNext we train our model. We use Fashion-MNIST images, ﬂattened to 784-dimensional fea-\nture vectors.\ndata = d2l.FashionMNIST(batch_size=256)\nmodel = SoftmaxRegression(num_outputs=10, lr=0.1)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n\n149\nConcise Implementation of Softmax Regression\nAs before, this algorithm converges to a solution that is reasonably accurate, albeit this time\nwith fewer lines of code than before.\n4.5.4 Summary\nHigh-level APIs are very convenient at hiding from their user potentially dangerous aspects,\nsuch as numerical stability. Moreover, they allow users to design models concisely with very\nfew lines of code. This is both a blessing and a curse. The obvious beneﬁt is that it makes\nthings highly accessible, even to engineers who never took a single class of statistics in their\nlife (in fact, they are part of the target audience of the book). But hiding the sharp edges also\ncomes with a price: a disincentive to add new and diﬀerent components on your own, since\nthere is little muscle memory for doing it. Moreover, it makes it more diﬃcult to ﬁx things\nwhenever the protective padding of a framework fails to cover all the corner cases entirely.\nAgain, this is due to lack of familiarity.\nAs such, we strongly urge you to review both the bare bones and the elegant versions of\nmany of the implementations that follow. While we emphasize ease of understanding, the\nimplementations are nonetheless usually quite performant (convolutions are the big exception\nhere). It is our intention to allow you to build on these when you invent something new that\nno framework can give you.\n4.5.5 Exercises\n1. Deep learning uses many diﬀerent number formats, including FP64 double precision\n(used extremely rarely), FP32 single precision, BFLOAT16 (good for compressed repre-\nsentations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8. Com-\npute the smallest and largest argument of the exponential function for which the result\ndoes not lead to numerical underﬂow or overﬂow.\n2. INT8 is a very limited format consisting of nonzero numbers from 1 to 255. How could\nyou extend its dynamic range without using more bits? Do standard multiplication and\naddition still work?\n\n150\nLinear Neural Networks for Classiﬁcation\n99\n3. Increase the number of epochs for training. Why might the validation accuracy decrease\nafter a while? How could we ﬁx this?\n4. What happens as you increase the learning rate? Compare the loss curves for several\nlearning rates. Which one works better? When?\nDiscussions99.\n4.6 Generalization in Classiﬁcation\nSo far, we have focused on how to tackle multiclass classiﬁcation problems by training (linear)\nneural networks with multiple outputs and softmax functions. Interpreting our model’s outputs\nas probabilistic predictions, we motivated and derived the cross-entropy loss function, which\ncalculates the negative log likelihood that our model (for a ﬁxed set of parameters) assigns\nto the actual labels. And ﬁnally, we put these tools into practice by ﬁtting our model to the\ntraining set. However, as always, our goal is to learn general patterns, as assessed empirically\non previously unseen data (the test set). High accuracy on the training set means nothing.\nWhenever each of our inputs is unique (and indeed this is true for most high-dimensional\ndatasets), we can attain perfect accuracy on the training set by just memorizing the dataset\non the ﬁrst training epoch, and subsequently looking up the label whenever we see a new\nimage. And yet, memorizing the exact labels associated with the exact training examples\ndoes not tell us how to classify new examples. Absent further guidance, we might have to fall\nback on random guessing whenever we encounter new examples.\nA number of burning questions demand immediate attention:\n1. How many test examples do we need to give a good estimate of the accuracy of our\nclassiﬁers on the underlying population?\n2. What happens if we keep evaluating models on the same test repeatedly?\n3. Why should we expect that ﬁtting our linear models to the training set should fare any\nbetter than our naive memorization scheme?\nWhereas Section 3.6 introduced the basics of overﬁtting and generalization in the context of\nlinear regression, this chapter will go a little deeper, introducing some of the foundational\nideas of statistical learning theory. It turns out that we often can guarantee generalization a\npriori: for many models, and for any desired upper bound on the generalization gap ϵ, we can\noften determine some required number of samples n such that if our training set contains at\nleast n samples, our empirical error will lie within ϵ of the true error, for any data generating\ndistribution. Unfortunately, it also turns out that while these sorts of guarantees provide a\nprofound set of intellectual building blocks, they are of limited practical utility to the deep\nlearning practitioner. In short, these guarantees suggest that ensuring generalization of deep\nneural networks a priori requires an absurd number of examples (perhaps trillions or more),\n\n151\nGeneralization in Classiﬁcation\neven when we ﬁnd that, on the tasks we care about, deep neural networks typically general-\nize remarkably well with far fewer examples (thousands). Thus deep learning practitioners\noften forgo a priori guarantees altogether, instead employing methods that have generalized\nwell on similar problems in the past, and certifying generalization post hoc through empiri-\ncal evaluations. When we get to Chapter 5, we will revisit generalization and provide a light\nintroduction to the vast scientiﬁc literature that has sprung in attempts to explain why deep\nneural networks generalize in practice.\n4.6.1 The Test Set\nSince we have already begun to rely on test sets as the gold standard method for assessing\ngeneralization error, let’s get started by discussing the properties of such error estimates. Let’s\nfocus on a ﬁxed classiﬁer f , without worrying about how it was obtained. Moreover suppose\nthat we possess a fresh dataset of examples D = (x(i), y(i))\nn\ni=1 that were not used to train the\nclassiﬁer f . The empirical error of our classiﬁer f on D is simply the fraction of instances for\nwhich the prediction f (x(i)) disagrees with the true label y(i), and is given by the following\nexpression:\nϵD( f ) = 1\nn\nn\n∑\ni=1\n1( f (x(i)) , y(i)).\n(4.6.1)\nBy contrast, the population error is the expected fraction of examples in the underlying pop-\nulation (some distribution P(X,Y) characterized by probability density function p(x, y)) for\nwhich our classiﬁer disagrees with the true label:\nϵ( f ) = E(x,y)∼P1( f (x) , y) =\n∫∫\n1( f (x) , y)p(x, y) dxdy.\n(4.6.2)\nWhile ϵ( f ) is the quantity that we actually care about, we cannot observe it directly, just as\nwe cannot directly observe the average height in a large population without measuring every\nsingle person. We can only estimate this quantity based on samples. Because our test set D\nis statistically representative of the underlying population, we can view ϵD( f ) as a statistical\nestimator of the population error ϵ( f ). Moreover, because our quantity of interest ϵ( f ) is an\nexpectation (of the random variable 1( f (X) , Y)) and the corresponding estimator ϵD( f )\nis the sample average, estimating the population error is simply the classic problem of mean\nestimation, which you may recall from Section 2.6.\nAn important classical result from probability theory called the central limit theorem guar-\nantees that whenever we possess n random samples a1, ..., an drawn from any distribution\nwith mean µ and standard deviation σ, then, as the number of samples n approaches inﬁnity,\nthe sample average ˆµ approximately tends towards a normal distribution centered at the true\nmean and with standard deviation σ/√n. Already, this tells us something important: as the\nnumber of examples grows large, our test error ϵD( f ) should approach the true error ϵ( f ) at\na rate of O(1/√n). Thus, to estimate our test error twice as precisely, we must collect four\ntimes as large a test set. To reduce our test error by a factor of one hundred, we must collect\nten thousand times as large a test set. In general, such a rate of O(1/√n) is often the best we\ncan hope for in statistics.\n\n152\nLinear Neural Networks for Classiﬁcation\nNow that we know something about the asymptotic rate at which our test error ϵD( f ) con-\nverges to the true error ϵ( f ), we can zoom in on some important details. Recall that the\nrandom variable of interest 1( f (X) , Y) can only take values 0 and 1 and thus is a Bernoulli\nrandom variable, characterized by a parameter indicating the probability that it takes value\n1. Here, 1 means that our classiﬁer made an error, so the parameter of our random variable\nis actually the true error rate ϵ( f ). The variance σ2 of a Bernoulli depends on its parameter\n(here, ϵ( f )) according to the expression ϵ( f )(1 −ϵ( f )). While ϵ( f ) is initially unknown,\nwe know that it cannot be greater than 1. A little investigation of this function reveals that\nour variance is highest when the true error rate is close to 0.5 and can be far lower when it is\nclose to 0 or close to 1. This tells us that the asymptotic standard deviation of our estimate\nϵD( f ) of the error ϵ( f ) (over the choice of the n test samples) cannot be any greater than\n√\n0.25/n.\nIf we ignore the fact that this rate characterizes behavior as the test set size approaches inﬁnity\nrather than when we possess ﬁnite samples, this tells us that if we want our test error ϵD( f )\nto approximate the population error ϵ( f ) such that one standard deviation corresponds to an\ninterval of ±0.01, then we should collect roughly 2500 samples. If we want to ﬁt two standard\ndeviations in that range and thus be 95% conﬁdent that ϵD( f ) ∈ϵ( f ) ± 0.01, then we will\nneed 10,000 samples!\nThis turns out to be the size of the test sets for many popular benchmarks in machine learn-\ning. You might be surprised to ﬁnd out that thousands of applied deep learning papers get\npublished every year making a big deal out of error rate improvements of 0.01 or less. Of\ncourse, when the error rates are much closer to 0, then an improvement of 0.01 can indeed\nbe a big deal.\nOne pesky feature of our analysis thus far is that it really only tells us about asymptotics, i.e.,\nhow the relationship between ϵD and ϵ evolves as our sample size goes to inﬁnity. Fortunately,\nbecause our random variable is bounded, we can obtain valid ﬁnite sample bounds by applying\nan inequality due to Hoeﬀding (1963):\nP(ϵD( f ) −ϵ( f ) ≥t) < exp (−2nt2) .\n(4.6.3)\nSolving for the smallest dataset size that would allow us to conclude with 95% conﬁdence that\nthe distance t between our estimate ϵD( f ) and the true error rate ϵ( f ) does not exceed 0.01,\nyou will ﬁnd that roughly 15,000 examples are required as compared to the 10,000 examples\nsuggested by the asymptotic analysis above. If you go deeper into statistics you will ﬁnd that\nthis trend holds generally. Guarantees that hold even in ﬁnite samples are typically slightly\nmore conservative. Note that in the scheme of things, these numbers are not so far apart,\nreﬂecting the general usefulness of asymptotic analysis for giving us ballpark ﬁgures even if\nthey are not guarantees we can take to court.\n4.6.2 Test Set Reuse\nIn some sense, you are now set up to succeed at conducting empirical machine learning re-\nsearch. Nearly all practical models are developed and validated based on test set performance\n\n153\nGeneralization in Classiﬁcation\nand you are now a master of the test set. For any ﬁxed classiﬁer f , you know how to evaluate\nits test error ϵD( f ), and know precisely what can (and cannot) be said about its population\nerror ϵ( f ).\nSo let’s say that you take this knowledge and prepare to train your ﬁrst model f1. Knowing\njust how conﬁdent you need to be in the performance of your classiﬁer’s error rate you apply\nour analysis above to determine an appropriate number of examples to set aside for the test\nset. Moreover, let’s assume that you took the lessons from Section 3.6 to heart and made\nsure to preserve the sanctity of the test set by conducting all of your preliminary analysis,\nhyperparameter tuning, and even selection among multiple competing model architectures\non a validation set. Finally you evaluate your model f1 on the test set and report an unbiased\nestimate of the population error with an associated conﬁdence interval.\nSo far everything seems to be going well. However, that night you wake up at 3am with a\nbrilliant idea for a new modeling approach. The next day, you code up your new model, tune\nits hyperparameters on the validation set and not only are you getting your new model f2 to\nwork but its error rate appears to be much lower than f1’s. However, the thrill of discovery\nsuddenly fades as you prepare for the ﬁnal evaluation. You do not have a test set!\nEven though the original test set D is still sitting on your server, you now face two formidable\nproblems. First, when you collected your test set, you determined the required level of pre-\ncision under the assumption that you were evaluating a single classiﬁer f . However, if you\nget into the business of evaluating multiple classiﬁers f1, ..., fk on the same test set, you\nmust consider the problem of false discovery. Before, you might have been 95% sure that\nϵD( f ) ∈ϵ( f ) ± 0.01 for a single classiﬁer f and thus the probability of a misleading re-\nsult was a mere 5%. With k classiﬁers in the mix, it can be hard to guarantee that there is\nnot even one among them whose test set performance is misleading. With 20 classiﬁers un-\nder consideration, you might have no power at all to rule out the possibility that at least one\namong them received a misleading score. This problem relates to multiple hypothesis testing,\nwhich despite a vast literature in statistics, remains a persistent problem plaguing scientiﬁc\nresearch.\nIf that is not enough to worry you, there is a special reason to distrust the results that you\nget on subsequent evaluations. Recall that our analysis of test set performance rested on the\nassumption that the classiﬁer was chosen absent any contact with the test set and thus we could\nview the test set as drawn randomly from the underlying population. Here, not only are you\ntesting multiple functions, the subsequent function f2 was chosen after you observed the test\nset performance of f1. Once information from the test set has leaked to the modeler, it can\nnever be a true test set again in the strictest sense. This problem is called adaptive overﬁtting\nand has recently emerged as a topic of intense interest to learning theorists and statisticians\n(Dwork et al., 2015). Fortunately, while it is possible to leak all information out of a holdout\nset, and the theoretical worst case scenarios are bleak, these analyses may be too conservative.\nIn practice, take care to create real test sets, to consult them as infrequently as possible, to\naccount for multiple hypothesis testing when reporting conﬁdence intervals, and to dial up\nyour vigilance more aggressively when the stakes are high and your dataset size is small. When\nrunning a series of benchmark challenges, it is often good practice to maintain several test\nsets so that after each round, the old test set can be demoted to a validation set.\n\n154\nLinear Neural Networks for Classiﬁcation\n4.6.3 Statistical Learning Theory\nPut simply, test sets are all that we really have, and yet this fact seems strangely unsatisfying.\nFirst, we seldom possess a true test set—unless we are the ones creating the dataset, someone\nelse has probably already evaluated their own classiﬁer on our ostensible “test set”. And even\nwhen we have ﬁrst dibs, we soon ﬁnd ourselves frustrated, wishing we could evaluate our\nsubsequent modeling attempts without the gnawing feeling that we cannot trust our numbers.\nMoreover, even a true test set can only tell us post hoc whether a classiﬁer has in fact gen-\neralized to the population, not whether we have any reason to expect a priori that it should\ngeneralize.\nWith these misgivings in mind, you might now be suﬃciently primed to see the appeal of\nstatistical learning theory, the mathematical subﬁeld of machine learning whose practitioners\naim to elucidate the fundamental principles that explain why/when models trained on empir-\nical data can/will generalize to unseen data. One of the primary aims of statistical learning\nresearchers has been to bound the generalization gap, relating the properties of the model\nclass to the number of samples in the dataset.\nLearning theorists aim to bound the diﬀerence between the empirical error ϵS( fS) of a\nlearned classiﬁer fS, both trained and evaluated on the training set S, and the true error\nϵ( fS) of that same classiﬁer on the underlying population. This might look similar to the\nevaluation problem that we just addressed but there is a major diﬀerence. Earlier, the classi-\nﬁer f was ﬁxed and we only needed a dataset for evaluative purposes. And indeed, any ﬁxed\nclassiﬁer does generalize: its error on a (previously unseen) dataset is an unbiased estimate\nof the population error. But what can we say when a classiﬁer is trained and evaluated on\nthe same dataset? Can we ever be conﬁdent that the training error will be close to the testing\nerror?\nSuppose that our learned classiﬁer fS must be chosen from some pre-speciﬁed set of functions\nF . Recall from our discussion of test sets that while it is easy to estimate the error of a single\nclassiﬁer, things get hairy when we begin to consider collections of classiﬁers. Even if the\nempirical error of any one (ﬁxed) classiﬁer will be close to its true error with high probability,\nonce we consider a collection of classiﬁers, we need to worry about the possibility that just\none of them will receive a badly estimated error. The worry is that we might pick such a\nclassiﬁer and thereby grossly underestimate the population error. Moreover, even for linear\nmodels, because their parameters are continuously valued, we are typically choosing from an\ninﬁnite class of functions (|F | = ∞).\nOne ambitious solution to the problem is to develop analytic tools for proving uniform con-\nvergence, i.e., that with high probability, the empirical error rate for every classiﬁer in the\nclass f ∈F will simultaneously converge to its true error rate. In other words, we seek a\ntheoretical principle that would allow us to state that with probability at least 1 −δ (for some\nsmall δ) no classiﬁer’s error rate ϵ( f ) (among all classiﬁers in the class F ) will be misesti-\nmated by more than some small amount α. Clearly, we cannot make such statements for all\nmodel classes F . Recall the class of memorization machines that always achieve empirical\nerror 0 but never outperform random guessing on the underlying population.\n\n155\nGeneralization in Classiﬁcation\nIn a sense the class of memorizers is too ﬂexible. No such a uniform convergence result\ncould possibly hold. On the other hand, a ﬁxed classiﬁer is useless—it generalizes perfectly,\nbut ﬁts neither the training data nor the test data. The central question of learning has thus\nhistorically been framed as a trade-oﬀbetween more ﬂexible (higher variance) model classes\nthat better ﬁt the training data but risk overﬁtting, versus more rigid (higher bias) model\nclasses that generalize well but risk underﬁtting. A central question in learning theory has\nbeen to develop the appropriate mathematical analysis to quantify where a model sits along\nthis spectrum, and to provide the associated guarantees.\nIn a series of seminal papers, Vapnik and Chervonenkis extended the theory on the conver-\ngence of relative frequencies to more general classes of functions (Vapnik and Chervonenkis,\n1964, Vapnik and Chervonenkis, 1968, Vapnik and Chervonenkis, 1971, Vapnik and Cher-\nvonenkis, 1981, Vapnik and Chervonenkis, 1991, Vapnik and Chervonenkis, 1974). One of\nthe key contributions of this line of work is the Vapnik–Chervonenkis (VC) dimension, which\nmeasures (one notion of) the complexity (ﬂexibility) of a model class. Moreover, one of their\nkey results bounds the diﬀerence between the empirical error and the population error as a\nfunction of the VC dimension and the number of samples:\nP (R[p, f ] −Remp[X, Y, f ] < α) ≥1 −δ for α ≥c\n√\n(VC −log δ)/n.\n(4.6.4)\nHere δ > 0 is the probability that the bound is violated, α is the upper bound on the general-\nization gap, and n is the dataset size. Lastly, c > 0 is a constant that depends only on the scale\nof the loss that can be incurred. One use of the bound might be to plug in desired values of\nδ and α to determine how many samples to collect. The VC dimension quantiﬁes the largest\nnumber of data points for which we can assign any arbitrary (binary) labeling and for each\nﬁnd some model f in the class that agrees with that labeling. For example, linear models on\nd-dimensional inputs have VC dimension d + 1. It is easy to see that a line can assign any\npossible labeling to three points in two dimensions, but not to four. Unfortunately, the theory\ntends to be overly pessimistic for more complex models and obtaining this guarantee typically\nrequires far more examples than are actually needed to achieve the desired error rate. Note\nalso that ﬁxing the model class and δ, our error rate again decays with the usual O(1/√n)\nrate. It seems unlikely that we could do better in terms of n. However, as we vary the model\nclass, VC dimension can present a pessimistic picture of the generalization gap.\n4.6.4 Summary\nThe most straightforward way to evaluate a model is to consult a test set comprised of pre-\nviously unseen data. Test set evaluations provide an unbiased estimate of the true error and\nconverge at the desired O(1/√n) rate as the test set grows. We can provide approximate\nconﬁdence intervals based on exact asymptotic distributions or valid ﬁnite sample conﬁdence\nintervals based on (more conservative) ﬁnite sample guarantees. Indeed test set evaluation is\nthe bedrock of modern machine learning research. However, test sets are seldom true test\nsets (used by multiple researchers again and again). Once the same test set is used to evaluate\nmultiple models, controlling for false discovery can be diﬃcult. This can cause huge prob-\nlems in theory. In practice, the signiﬁcance of the problem depends on the size of the holdout\n\n156\nLinear Neural Networks for Classiﬁcation\n100\nsets in question and whether they are merely being used to choose hyperparameters or if they\nare leaking information more directly. Nevertheless, it is good practice to curate real test sets\n(or multiple) and to be as conservative as possible about how often they are used.\nHoping to provide a more satisfying solution, statistical learning theorists have developed\nmethods for guaranteeing uniform convergence over a model class. If indeed every model’s\nempirical error simultaneously converges to its true error, then we are free to choose the\nmodel that performs best, minimizing the training error, knowing that it too will perform\nsimilarly well on the holdout data. Crucially, any one of such results must depend on some\nproperty of the model class. Vladimir Vapnik and Alexey Chernovenkis introduced the VC\ndimension, presenting uniform convergence results that hold for all models in a VC class.\nThe training errors for all models in the class are (simultaneously) guaranteed to be close\nto their true errors, and guaranteed to grow even closer at O(1/√n) rates. Following the\nrevolutionary discovery of VC dimension, numerous alternative complexity measures have\nbeen proposed, each facilitating an analogous generalization guarantee. See Boucheron et al.\n(2005) for a detailed discussion of several advanced ways of measuring function complexity.\nUnfortunately, while these complexity measures have become broadly useful tools in statis-\ntical theory, they turn out to be powerless (as straightforwardly applied) for explaining why\ndeep neural networks generalize. Deep neural networks often have millions of parameters (or\nmore), and can easily assign random labels to large collections of points. Nevertheless, they\ngeneralize well on practical problems and, surprisingly, they often generalize better, when\nthey are larger and deeper, despite incurring higher VC dimensions. In the next chapter, we\nwill revisit generalization in the context of deep learning.\n4.6.5 Exercises\n1. If we wish to estimate the error of a ﬁxed model f to within 0.0001 with probability\ngreater than 99.9%, how many samples do we need?\n2. Suppose that somebody else possesses a labeled test set D and only makes available the\nunlabeled inputs (features). Now suppose that you can only access the test set labels by\nrunning a model f (with no restrictions placed on the model class) on each of the unlabeled\ninputs and receiving the corresponding error ϵD( f ). How many models would you need to\nevaluate before you leak the entire test set and thus could appear to have error 0, regardless\nof your true error?\n3. What is the VC dimension of the class of ﬁfth-order polynomials?\n4. What is the VC dimension of axis-aligned rectangles on two-dimensional data?\nDiscussions100.\n\n157\nEnvironment and Distribution Shift\n4.7 Environment and Distribution Shift\nIn the previous sections, we worked through a number of hands-on applications of machine\nlearning, ﬁtting models to a variety of datasets. And yet, we never stopped to contemplate\neither where data came from in the ﬁrst place or what we ultimately plan to do with the\noutputs from our models. Too often, machine learning developers in possession of data rush\nto develop models without pausing to consider these fundamental issues.\nMany failed machine learning deployments can be traced back to this failure. Sometimes\nmodels appear to perform marvelously as measured by test set accuracy but fail catastroph-\nically in deployment when the distribution of data suddenly shifts. More insidiously, some-\ntimes the very deployment of a model can be the catalyst that perturbs the data distribution.\nSay, for example, that we trained a model to predict who will repay rather than default on a\nloan, ﬁnding that an applicant’s choice of footwear was associated with the risk of default (Ox-\nfords indicate repayment, sneakers indicate default). We might be inclined thereafter to grant\na loan to any applicant wearing Oxfords and to deny all applicants wearing sneakers.\nIn this case, our ill-considered leap from pattern recognition to decision-making and our fail-\nure to critically consider the environment might have disastrous consequences. For starters, as\nsoon as we began making decisions based on footwear, customers would catch on and change\ntheir behavior. Before long, all applicants would be wearing Oxfords, without any coincident\nimprovement in credit-worthiness. Take a minute to digest this because similar issues abound\nin many applications of machine learning: by introducing our model-based decisions to the\nenvironment, we might break the model.\nWhile we cannot possibly give these topics a complete treatment in one section, we aim here\nto expose some common concerns, and to stimulate the critical thinking required to detect\nsuch situations early, mitigate damage, and use machine learning responsibly. Some of the\nsolutions are simple (ask for the “right” data), some are technically diﬃcult (implement a\nreinforcement learning system), and others require that we step outside the realm of statistical\nprediction altogether and grapple with diﬃcult philosophical questions concerning the ethical\napplication of algorithms.\n4.7.1 Types of Distribution Shift\nTo begin, we stick with the passive prediction setting considering the various ways that data\ndistributions might shift and what might be done to salvage model performance. In one classic\nsetup, we assume that our training data was sampled from some distribution pS(x, y) but\nthat our test data will consist of unlabeled examples drawn from some diﬀerent distribution\npT(x, y). Already, we must confront a sobering reality. Absent any assumptions on how pS\nand pT relate to each other, learning a robust classiﬁer is impossible.\nConsider a binary classiﬁcation problem, where we wish to distinguish between dogs and cats.\nIf the distribution can shift in arbitrary ways, then our setup permits the pathological case in\n\n158\nLinear Neural Networks for Classiﬁcation\nwhich the distribution over inputs remains constant: pS(x) = pT(x), but the labels are all\nﬂipped: pS(y | x) = 1 −pT(y | x). In other words, if God can suddenly decide that in the\nfuture all “cats” are now dogs and what we previously called “dogs” are now cats—without\nany change in the distribution of inputs p(x), then we cannot possibly distinguish this setting\nfrom one in which the distribution did not change at all.\nFortunately, under some restricted assumptions on the ways our data might change in the\nfuture, principled algorithms can detect shift and sometimes even adapt on the ﬂy, improving\non the accuracy of the original classiﬁer.\nCovariate Shift\nAmong categories of distribution shift, covariate shift may be the most widely studied. Here,\nwe assume that while the distribution of inputs may change over time, the labeling function,\ni.e., the conditional distribution P(y | x) does not change. Statisticians call this covariate\nshift because the problem arises due to a shift in the distribution of the covariates (features).\nWhile we can sometimes reason about distribution shift without invoking causality, we note\nthat covariate shift is the natural assumption to invoke in settings where we believe that x\ncauses y.\nConsider the challenge of distinguishing cats and dogs. Our training data might consist of\nimages of the kind in Fig. 4.7.1.\nt\nFig. 4.7.1\nTraining data for distinguishing cats and dogs (illustrations: Lafeez Hossain / 500px /\nGetty Images; ilkermetinkursova / iStock / Getty Images Plus; GlobalP / iStock / Getty\nImages Plus; Musthafa Aboobakuru / 500px / Getty Images).\nAt test time we are asked to classify the images in Fig. 4.7.2.\nThe training set consists of photos, while the test set contains only cartoons. Training on a\ndataset with substantially diﬀerent characteristics from the test set can spell trouble absent a\ncoherent plan for how to adapt to the new domain.\n\n159\nEnvironment and Distribution Shift\nt\nFig. 4.7.2\nTest data for distinguishing cats and dogs (illustrations: SIBAS_minich / iStock / Getty\nImages Plus; Ghrzuzudu / iStock / Getty Images Plus; id-work / DigitalVision Vectors /\nGetty Images; Yime / iStock / Getty Images Plus).\nLabel Shift\nLabel shift describes the converse problem. Here, we assume that the label marginal P(y)\ncan change but the class-conditional distribution P(x | y) remains ﬁxed across domains. La-\nbel shift is a reasonable assumption to make when we believe that y causes x. For example,\nwe may want to predict diagnoses given their symptoms (or other manifestations), even as\nthe relative prevalence of diagnoses are changing over time. Label shift is the appropriate as-\nsumption here because diseases cause symptoms. In some degenerate cases the label shift and\ncovariate shift assumptions can hold simultaneously. For example, when the label is deter-\nministic, the covariate shift assumption will be satisﬁed, even when y causes x. Interestingly,\nin these cases, it is often advantageous to work with methods that ﬂow from the label shift\nassumption. That is because these methods tend to involve manipulating objects that look\nlike labels (often low-dimensional), as opposed to objects that look like inputs, which tend\nto be high-dimensional in deep learning.\nConcept Shift\nWe may also encounter the related problem of concept shift, which arises when the very deﬁ-\nnitions of labels can change. This sounds weird—a cat is a cat, no? However, other categories\nare subject to changes in usage over time. Diagnostic criteria for mental illness, what passes\nfor fashionable, and job titles, are all subject to considerable amounts of concept shift. It turns\nout that if we navigate around the United States, shifting the source of our data by geography,\nwe will ﬁnd considerable concept shift regarding the distribution of names for soft drinks as\nshown in Fig. 4.7.3.\nIf we were to build a machine translation system, the distribution P(y | x) might be dif-\nferent depending on our location. This problem can be tricky to spot. We might hope to\nexploit knowledge that shift only takes place gradually either in a temporal or geographic\nsense.\n4.7.2 Examples of Distribution Shift\n\n160\nLinear Neural Networks for Classiﬁcation\nt\nFig. 4.7.3\nConcept shift for soft drink names in the United States (CC-BY: Alan McConchie,\nPopVsSoda.com).\nBefore delving into formalism and algorithms, we can discuss some concrete situations where\ncovariate or concept shift might not be obvious.\nMedical Diagnostics\nImagine that you want to design an algorithm to detect cancer. You collect data from healthy\nand sick people and you train your algorithm. It works ﬁne, giving you high accuracy and you\nconclude that you are ready for a successful career in medical diagnostics. Not so fast.\nThe distributions that gave rise to the training data and those you will encounter in the wild\nmight diﬀer considerably. This happened to an unfortunate startup that some of we authors\nworked with years ago. They were developing a blood test for a disease that predominantly\naﬀects older men and hoped to study it using blood samples that they had collected from\npatients. However, it is considerably more diﬃcult to obtain blood samples from healthy men\nthan from sick patients already in the system. To compensate, the startup solicited blood\ndonations from students on a university campus to serve as healthy controls in developing\ntheir test. Then they asked whether we could help them to build a classiﬁer for detecting the\ndisease.\nAs we explained to them, it would indeed be easy to distinguish between the healthy and sick\ncohorts with near-perfect accuracy. However, that is because the test subjects diﬀered in age,\nhormone levels, physical activity, diet, alcohol consumption, and many more factors unrelated\nto the disease. This was unlikely to be the case with real patients. Due to their sampling\nprocedure, we could expect to encounter extreme covariate shift. Moreover, this case was\n\n161\nEnvironment and Distribution Shift\nunlikely to be correctable via conventional methods. In short, they wasted a signiﬁcant sum\nof money.\nSelf-Driving Cars\nSay a company wanted to leverage machine learning for developing self-driving cars. One\nkey component here is a roadside detector. Since real annotated data is expensive to get, they\nhad the (smart and questionable) idea to use synthetic data from a game rendering engine\nas additional training data. This worked really well on “test data” drawn from the rendering\nengine. Alas, inside a real car it was a disaster. As it turned out, the roadside had been rendered\nwith a very simplistic texture. More importantly, all the roadside had been rendered with the\nsame texture and the roadside detector learned about this “feature” very quickly.\nA similar thing happened to the US Army when they ﬁrst tried to detect tanks in the forest.\nThey took aerial photographs of the forest without tanks, then drove the tanks into the forest\nand took another set of pictures. The classiﬁer appeared to work perfectly. Unfortunately, it\nhad merely learned how to distinguish trees with shadows from trees without shadows—the\nﬁrst set of pictures was taken in the early morning, the second set at noon.\nNonstationary Distributions\nA much more subtle situation arises when the distribution changes slowly (also known as\nnonstationary distribution) and the model is not updated adequately. Below are some typical\ncases.\n• We train a computational advertising model and then fail to update it frequently (e.g., we\nforget to incorporate that an obscure new device called an iPad was just launched).\n• We build a spam ﬁlter. It works well at detecting all spam that we have seen so far. But\nthen the spammers wise up and craft new messages that look unlike anything we have\nseen before.\n• We build a product recommendation system. It works throughout the winter but then con-\ntinues to recommend Santa hats long after Christmas.\nMore Anecdotes\n• We build a face detector. It works well on all benchmarks. Unfortunately it fails on test\ndata—the oﬀending examples are close-ups where the face ﬁlls the entire image (no\nsuch data was in the training set).\n• We build a web search engine for the US market and want to deploy it in the UK.\n• We train an image classiﬁer by compiling a large dataset where each among a large set of\nclasses is equally represented in the dataset, say 1000 categories, represented by 1000\n\n162\nLinear Neural Networks for Classiﬁcation\nimages each. Then we deploy the system in the real world, where the actual label distri-\nbution of photographs is decidedly non-uniform.\n4.7.3 Correction of Distribution Shift\nAs we have discussed, there are many cases where training and test distributions P(x, y)\nare diﬀerent. In some cases, we get lucky and the models work despite covariate, label, or\nconcept shift. In other cases, we can do better by employing principled strategies to cope with\nthe shift. The remainder of this section grows considerably more technical. The impatient\nreader could continue on to the next section as this material is not prerequisite to subsequent\nconcepts.\nEmpirical Risk and Risk\nLet’s ﬁrst reﬂect on what exactly is happening during model training: we iterate over features\nand associated labels of training data {(x1, y1), . . ., (xn, yn)} and update the parameters of a\nmodel f after every minibatch. For simplicity we do not consider regularization, so we largely\nminimize the loss on the training:\nminimize\nf\n1\nn\nn\n∑\ni=1\nl( f (xi), yi),\n(4.7.1)\nwhere l is the loss function measuring “how bad” the prediction f (xi) is given the associated\nlabel yi. Statisticians call the term in (4.7.1) empirical risk. The empirical risk is an average\nloss over the training data for approximating the risk, which is the expectation of the loss over\nthe entire population of data drawn from their true distribution p(x, y):\nEp(x,y)[l( f (x), y)] =\n∫∫\nl( f (x), y)p(x, y) dxdy.\n(4.7.2)\nHowever, in practice we typically cannot obtain the entire population of data. Thus, empirical\nrisk minimization, which is minimizing the empirical risk in (4.7.1), is a practical strategy for\nmachine learning, with the hope of approximately minimizing the risk.\nCovariate Shift Correction\nAssume that we want to estimate some dependency P(y | x) for which we have labeled data\n(xi, yi). Unfortunately, the observations xi are drawn from some source distribution q(x)\nrather than the target distribution p(x). Fortunately, the dependency assumption means that\nthe conditional distribution does not change: p(y | x) = q(y | x). If the source distribu-\ntion q(x) is “wrong”, we can correct for that by using the following simple identity in the\nrisk:\n∫∫\nl( f (x), y)p(y | x)p(x) dxdy =\n∫∫\nl( f (x), y)q(y | x)q(x) p(x)\nq(x) dxdy.\n(4.7.3)\n\n163\nEnvironment and Distribution Shift\nIn other words, we need to reweigh each data example by the ratio of the probability that it\nwould have been drawn from the correct distribution to that from the wrong one:\nβi\ndef\n= p(xi)\nq(xi).\n(4.7.4)\nPlugging in the weight βi for each data example (xi, yi) we can train our model using weighted\nempirical risk minimization:\nminimize\nf\n1\nn\nn\n∑\ni=1\nβil( f (xi), yi).\n(4.7.5)\nAlas, we do not know that ratio, so before we can do anything useful we need to estimate it.\nMany methods are available, including some fancy operator-theoretic approaches that attempt\nto recalibrate the expectation operator directly using a minimum-norm or a maximum entropy\nprinciple. Note that for any such approach, we need samples drawn from both distributions—\nthe “true” p, e.g., by access to test data, and the one used for generating the training set q\n(the latter is trivially available). Note however, that we only need features x ∼p(x); we do\nnot need to access labels y ∼p(y).\nIn this case, there exists a very eﬀective approach that will give almost as good results as the\noriginal: namely, logistic regression, which is a special case of softmax regression (see Section\n4.1) for binary classiﬁcation. This is all that is needed to compute estimated probability ratios.\nWe learn a classiﬁer to distinguish between data drawn from p(x) and data drawn from q(x).\nIf it is impossible to distinguish between the two distributions then it means that the associated\ninstances are equally likely to come from either one of those two distributions. On the other\nhand, any instances that can be well discriminated should be signiﬁcantly overweighted or\nunderweighted accordingly.\nFor simplicity’s sake assume that we have an equal number of instances from both distribu-\ntions p(x) and q(x), respectively. Now denote by z labels that are 1 for data drawn from p\nand −1 for data drawn from q. Then the probability in a mixed dataset is given by\nP(z = 1 | x) =\np(x)\np(x) + q(x) and hence P(z = 1 | x)\nP(z = −1 | x) = p(x)\nq(x).\n(4.7.6)\nThus, if we use a logistic regression approach, where P(z = 1 | x) =\n1\n1+exp(−h(x)) (h is a\nparametrized function), it follows that\nβi =\n1/(1 + exp(−h(xi)))\nexp(−h(xi))/(1 + exp(−h(xi))) = exp(h(xi)).\n(4.7.7)\nAs a result, we need to solve two problems: the ﬁrst, to distinguish between data drawn from\nboth distributions, and then a weighted empirical risk minimization problem in (4.7.5) where\nwe weigh terms by βi.\nNow we are ready to describe a correction algorithm. Suppose that we have a training set\n{(x1, y1), . . ., (xn, yn)} and an unlabeled test set {u1, . . ., um}. For covariate shift, we as-\nsume that xi for all 1 ≤i ≤n are drawn from some source distribution and ui for all\n1 ≤i ≤m are drawn from the target distribution. Here is a prototypical algorithm for cor-\nrecting covariate shift:\n\n164\nLinear Neural Networks for Classiﬁcation\n1. Create a binary-classiﬁcation training set: {(x1, −1), . . ., (xn, −1), (u1, 1), . . ., (um, 1)}.\n2. Train a binary classiﬁer using logistic regression to get the function h.\n3. Weigh training data using βi = exp(h(xi)) or better βi = min(exp(h(xi)), c) for some\nconstant c.\n4. Use weights βi for training on {(x1, y1), . . ., (xn, yn)} in (4.7.5).\nNote that the above algorithm relies on a crucial assumption. For this scheme to work, we\nneed that each data example in the target (e.g., test time) distribution had nonzero probability\nof occurring at training time. If we ﬁnd a point where p(x) > 0 but q(x) = 0, then the\ncorresponding importance weight should be inﬁnity.\nLabel Shift Correction\nAssume that we are dealing with a classiﬁcation task with k categories. Using the same no-\ntation in Section 4.7.3, q and p are the source distribution (e.g., training time) and target\ndistribution (e.g., test time), respectively. Assume that the distribution of labels shifts over\ntime: q(y) , p(y), but the class-conditional distribution stays the same: q(x | y) = p(x | y).\nIf the source distribution q(y) is “wrong”, we can correct for that according to the following\nidentity in the risk as deﬁned in (4.7.2):\n∫∫\nl( f (x), y)p(x | y)p(y) dxdy =\n∫∫\nl( f (x), y)q(x | y)q(y) p(y)\nq(y) dxdy. (4.7.8)\nHere, our importance weights will correspond to the label likelihood ratios:\nβi\ndef\n= p(yi)\nq(yi).\n(4.7.9)\nOne nice thing about label shift is that if we have a reasonably good model on the source\ndistribution, then we can get consistent estimates of these weights without ever having to deal\nwith the ambient dimension. In deep learning, the inputs tend to be high-dimensional objects\nlike images, while the labels are often simpler objects like categories.\nTo estimate the target label distribution, we ﬁrst take our reasonably good oﬀ-the-shelf clas-\nsiﬁer (typically trained on the training data) and compute its “confusion” matrix using the\nvalidation set (also from the training distribution). The confusion matrix, C, is simply a k × k\nmatrix, where each column corresponds to the label category (ground truth) and each row\ncorresponds to our model’s predicted category. Each cell’s value cij is the fraction of total\npredictions on the validation set where the true label was j and our model predicted i.\nNow, we cannot calculate the confusion matrix on the target data directly because we do not\nget to see the labels for the examples that we see in the wild, unless we invest in a complex real-\ntime annotation pipeline. What we can do, however, is average all of our model’s predictions\nat test time together, yielding the mean model outputs µ(ˆy) ∈Rk, where the ith element µ(ˆyi)\nis the fraction of the total predictions on the test set where our model predicted i.\nIt turns out that under some mild conditions—if our classiﬁer was reasonably accurate in the\n\n165\nEnvironment and Distribution Shift\nﬁrst place, and if the target data contains only categories that we have seen before, and if\nthe label shift assumption holds in the ﬁrst place (the strongest assumption here)—we can\nestimate the test set label distribution by solving a simple linear system\nCp(y) = µ(ˆy),\n(4.7.10)\nbecause as an estimate ∑k\nj=1 cijp(yj) = µ(ˆyi) holds for all 1 ≤i ≤k, where p(yj) is the\njth element of the k-dimensional label distribution vector p(y). If our classiﬁer is suﬃciently\naccurate to begin with, then the confusion matrix C will be invertible, and we get a solution\np(y) = C−1µ(ˆy).\nBecause we observe the labels on the source data, it is easy to estimate the distribution\nq(y). Then, for any training example i with label yi, we can take the ratio of our estimated\np(yi)/q(yi) to calculate the weight βi, and plug this into weighted empirical risk minimiza-\ntion in (4.7.5).\nConcept Shift Correction\nConcept shift is much harder to ﬁx in a principled manner. For instance, in a situation where\nsuddenly the problem changes from distinguishing cats from dogs to one of distinguishing\nwhite from black animals, it will be unreasonable to assume that we can do much better than\njust collecting new labels and training from scratch. Fortunately, in practice, such extreme\nshifts are rare. Instead, what usually happens is that the task keeps on changing slowly. To\nmake things more concrete, here are some examples:\n• In computational advertising, new products are launched, old products become less popu-\nlar. This means that the distribution over ads and their popularity changes gradually and\nany click-through rate predictor needs to change gradually with it.\n• Traﬃc camera lenses degrade gradually due to environmental wear, aﬀecting image quality\nprogressively.\n• News content changes gradually (i.e., most of the news remains unchanged but new stories\nappear).\nIn such cases, we can use the same approach that we used for training networks to make\nthem adapt to the change in the data. In other words, we use the existing network weights and\nsimply perform a few update steps with the new data rather than training from scratch.\n4.7.4 A Taxonomy of Learning Problems\nArmed with knowledge about how to deal with changes in distributions, we can now consider\nsome other aspects of machine learning problem formulation.\n\n166\nLinear Neural Networks for Classiﬁcation\nBatch Learning\nIn batch learning, we have access to training features and labels {(x1, y1), . . ., (xn, yn)},\nwhich we use to train a model f (x). Later on, we deploy this model to score new data (x, y)\ndrawn from the same distribution. This is the default assumption for any of the problems\nthat we discuss here. For instance, we might train a cat detector based on lots of pictures of\ncats and dogs. Once we have trained it, we ship it as part of a smart catdoor computer vision\nsystem that lets only cats in. This is then installed in a customer’s home and is never updated\nagain (barring extreme circumstances).\nOnline Learning\nNow imagine that the data (xi, yi) arrives one sample at a time. More speciﬁcally, assume\nthat we ﬁrst observe xi, then we need to come up with an estimate f (xi). Only once we have\ndone this do we observe yi and so receive a reward or incur a loss, given our decision. Many\nreal problems fall into this category. For example, we need to predict tomorrow’s stock price,\nwhich allows us to trade based on that estimate and at the end of the day we ﬁnd out whether\nour estimate made us a proﬁt. In other words, in online learning, we have the following cycle\nwhere we are continuously improving our model given new observations:\nmodel ft −→data xt −→estimate ft(xt) −→\nobservation yt −→loss l(yt, ft(xt)) −→model ft+1\n(4.7.11)\nBandits\nBandits are a special case of the problem above. While in most learning problems we have\na continuously parametrized function f where we want to learn its parameters (e.g., a deep\nnetwork), in a bandit problem we only have a ﬁnite number of arms that we can pull, i.e., a\nﬁnite number of actions that we can take. It is not very surprising that for this simpler problem\nstronger theoretical guarantees in terms of optimality can be obtained. We list it mainly since\nthis problem is often (confusingly) treated as if it were a distinct learning setting.\nControl\nIn many cases the environment remembers what we did. Not necessarily in an adversarial\nmanner but it will just remember and the response will depend on what happened before.\nFor instance, a coﬀee boiler controller will observe diﬀerent temperatures depending on\nwhether it was heating the boiler previously. PID (proportional-integral-derivative) controller\nalgorithms are a popular choice there. Likewise, a user’s behavior on a news site will depend\non what we showed them previously (e.g., they will read most news only once). Many such\nalgorithms form a model of the environment in which they act so as to make their deci-\nsions appear less random. Recently, control theory (e.g., PID variants) has also been used to\n\n167\nEnvironment and Distribution Shift\nautomatically tune hyperparameters to achieve better disentangling and reconstruction qual-\nity, and improve the diversity of generated text and the reconstruction quality of generated\nimages (Shao et al., 2020).\nReinforcement Learning\nIn the more general case of an environment with memory, we may encounter situations where\nthe environment is trying to cooperate with us (cooperative games, in particular for non-zero-\nsum games), or others where the environment will try to win. Chess, Go, Backgammon, or\nStarCraft are some of the cases in reinforcement learning. Likewise, we might want to build\na good controller for autonomous cars. Other cars are likely to respond to the autonomous\ncar’s driving style in nontrivial ways, e.g., trying to avoid it, trying to cause an accident, or\ntrying to cooperate with it.\nConsidering the Environment\nOne key distinction between the diﬀerent situations above is that a strategy that might have\nworked throughout in the case of a stationary environment, might not work throughout in\nan environment that can adapt. For instance, an arbitrage opportunity discovered by a trader\nis likely to disappear once it is exploited. The speed and manner at which the environment\nchanges determines to a large extent the type of algorithms that we can bring to bear. For\ninstance, if we know that things may only change slowly, we can force any estimate to change\nonly slowly, too. If we know that the environment might change instantaneously, but only\nvery infrequently, we can make allowances for that. These types of knowledge are crucial for\nthe aspiring data scientist in dealing with concept shift, i.e., when the problem that is being\nsolved can change over time.\n4.7.5 Fairness, Accountability, and Transparency in Machine\nLearning\nFinally, it is important to remember that when you deploy machine learning systems you\nare not merely optimizing a predictive model—you are typically providing a tool that will be\nused to (partially or fully) automate decisions. These technical systems can impact the lives of\nindividuals who are subject to the resulting decisions. The leap from considering predictions to\nmaking decisions raises not only new technical questions, but also a slew of ethical questions\nthat must be carefully considered. If we are deploying a medical diagnostic system, we need\nto know for which populations it may work and for which it may not. Overlooking foreseeable\nrisks to the welfare of a subpopulation could cause us to administer inferior care. Moreover,\nonce we contemplate decision-making systems, we must step back and reconsider how we\nevaluate our technology. Among other consequences of this change of scope, we will ﬁnd that\naccuracy is seldom the right measure. For instance, when translating predictions into actions,\nwe will often want to take into account the potential cost sensitivity of erring in various ways.\n\n168\nLinear Neural Networks for Classiﬁcation\nIf one way of misclassifying an image could be perceived as a racial sleight of hand, while\nmisclassiﬁcation to a diﬀerent category would be harmless, then we might want to adjust\nour thresholds accordingly, accounting for societal values in designing the decision-making\nprotocol. We also want to be careful about how prediction systems can lead to feedback loops.\nFor example, consider predictive policing systems, which allocate patrol oﬃcers to areas with\nhigh forecasted crime. It is easy to see how a worrying pattern can emerge:\n1. Neighborhoods with more crime get more patrols.\n2. Consequently, more crimes are discovered in these neighborhoods, entering the training\ndata available for future iterations.\n3. Exposed to more positives, the model predicts yet more crime in these neighborhoods.\n4. In the next iteration, the updated model targets the same neighborhood even more heavily\nleading to yet more crimes discovered, etc.\nOften, the various mechanisms by which a model’s predictions become coupled to its training\ndata are unaccounted for in the modeling process. This can lead to what researchers call run-\naway feedback loops. Additionally, we want to be careful about whether we are addressing the\nright problem in the ﬁrst place. Predictive algorithms now play an outsize role in mediating\nthe dissemination of information. Should the news that an individual encounters be deter-\nmined by the set of Facebook pages they have Liked? These are just a few among the many\npressing ethical dilemmas that you might encounter in a career in machine learning.\n4.7.6 Summary\nIn many cases training and test sets do not come from the same distribution. This is called\ndistribution shift. The risk is the expectation of the loss over the entire population of data\ndrawn from their true distribution. However, this entire population is usually unavailable.\nEmpirical risk is an average loss over the training data to approximate the risk. In practice,\nwe perform empirical risk minimization.\nUnder the corresponding assumptions, covariate and label shift can be detected and corrected\nfor at test time. Failure to account for this bias can become problematic at test time. In some\ncases, the environment may remember automated actions and respond in surprising ways.\nWe must account for this possibility when building models and continue to monitor live sys-\ntems, open to the possibility that our models and the environment will become entangled in\nunanticipated ways.\n4.7.7 Exercises\n1. What could happen when we change the behavior of a search engine? What might the\nusers do? What about the advertisers?\n2. Implement a covariate shift detector. Hint: build a classiﬁer.\n\n169\nEnvironment and Distribution Shift\n101\n3. Implement a covariate shift corrector.\n4. Besides distribution shift, what else could aﬀect how the empirical risk approximates the\nrisk?\nDiscussions101.\n\n5\nMultilayer Perceptrons\nIn this chapter, we will introduce your ﬁrst truly deep network. The simplest deep networks\nare called multilayer perceptrons, and they consist of multiple layers of neurons each fully con-\nnected to those in the layer below (from which they receive input) and those above (which\nthey, in turn, inﬂuence). Although automatic diﬀerentiation signiﬁcantly simpliﬁes the im-\nplementation of deep learning algorithms, we will dive deep into how these gradients are\ncalculated in deep networks. Then we will be ready to discuss issues relating to numeri-\ncal stability and parameter initialization that are key to successfully training deep networks.\nWhen we train such high-capacity models we run the risk of overﬁtting. Thus, we will revisit\nregularization and generalization for deep networks. Throughout, we aim to give you a ﬁrm\ngrasp not just of the concepts but also of the practice of using deep networks. At the end of\nthis chapter, we apply what we have introduced so far to a real case: house price prediction.\nWe punt matters relating to the computational performance, scalability, and eﬃciency of our\nmodels to subsequent chapters.\n5.1 Multilayer Perceptrons\nIn Section 4.1, we introduced softmax regression, implementing the algorithm from scratch\n(Section 4.4) and using high-level APIs (Section 4.5). This allowed us to train classiﬁers\ncapable of recognizing 10 categories of clothing from low-resolution images. Along the way,\nwe learned how to wrangle data, coerce our outputs into a valid probability distribution, apply\nan appropriate loss function, and minimize it with respect to our model’s parameters. Now\nthat we have mastered these mechanics in the context of simple linear models, we can launch\nour exploration of deep neural networks, the comparatively rich class of models with which\nthis book is primarily concerned.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\n5.1.1 Hidden Layers\n170\n\n171\nMultilayer Perceptrons\nWe described aﬃne transformations in Section 3.1.1 as linear transformations with added\nbias. To begin, recall the model architecture corresponding to our softmax regression exam-\nple, illustrated in Fig. 4.1.1. This model maps inputs directly to outputs via a single aﬃne\ntransformation, followed by a softmax operation. If our labels truly were related to the in-\nput data by a simple aﬃne transformation, then this approach would be suﬃcient. However,\nlinearity (in aﬃne transformations) is a strong assumption.\nLimitations of Linear Models\nFor example, linearity implies the weaker assumption of monotonicity, i.e., that any increase\nin our feature must either always cause an increase in our model’s output (if the corresponding\nweight is positive), or always cause a decrease in our model’s output (if the corresponding\nweight is negative). Sometimes that makes sense. For example, if we were trying to predict\nwhether an individual will repay a loan, we might reasonably assume that all other things\nbeing equal, an applicant with a higher income would always be more likely to repay than one\nwith a lower income. While monotonic, this relationship likely is not linearly associated with\nthe probability of repayment. An increase in income from $0 to $50,000 likely corresponds\nto a bigger increase in likelihood of repayment than an increase from $1 million to $1.05\nmillion. One way to handle this might be to postprocess our outcome such that linearity\nbecomes more plausible, by using the logistic map (and thus the logarithm of the probability\nof outcome).\nNote that we can easily come up with examples that violate monotonicity. Say for example\nthat we want to predict health as a function of body temperature. For individuals with a normal\nbody temperature above 37°C (98.6°F), higher temperatures indicate greater risk. However,\nif the body temperatures drops below 37°C, lower temperatures indicate greater risk! Again,\nwe might resolve the problem with some clever preprocessing, such as using the distance from\n37°C as a feature.\nBut what about classifying images of cats and dogs? Should increasing the intensity of the\npixel at location (13, 17) always increase (or always decrease) the likelihood that the image\ndepicts a dog? Reliance on a linear model corresponds to the implicit assumption that the\nonly requirement for diﬀerentiating cats and dogs is to assess the brightness of individual\npixels. This approach is doomed to fail in a world where inverting an image preserves the\ncategory.\nAnd yet despite the apparent absurdity of linearity here, as compared with our previous ex-\namples, it is less obvious that we could address the problem with a simple preprocessing ﬁx.\nThat is, because the signiﬁcance of any pixel depends in complex ways on its context (the\nvalues of the surrounding pixels). While there might exist a representation of our data that\nwould take into account the relevant interactions among our features, on top of which a lin-\near model would be suitable, we simply do not know how to calculate it by hand. With deep\nneural networks, we used observational data to jointly learn both a representation via hidden\nlayers and a linear predictor that acts upon that representation.\n\n172\nMultilayer Perceptrons\nThis problem of nonlinearity has been studied for at least a century (Fisher, 1925). For in-\nstance, decision trees in their most basic form use a sequence of binary decisions to decide\nupon class membership (Quinlan, 1993). Likewise, kernel methods have been used for many\ndecades to model nonlinear dependencies (Aronszajn, 1950). This has found its way into non-\nparametric spline models (Wahba, 1990) and kernel methods (Schölkopf and Smola, 2002).\nIt is also something that the brain solves quite naturally. After all, neurons feed into other\nneurons which, in turn, feed into other neurons again (Ramón y Cajal and Azoulay, 1894).\nConsequently we have a sequence of relatively simple transformations.\nIncorporating Hidden Layers\nWe can overcome the limitations of linear models by incorporating one or more hidden layers.\nThe easiest way to do this is to stack many fully connected layers on top of one another. Each\nlayer feeds into the layer above it, until we generate outputs. We can think of the ﬁrst L −1\nlayers as our representation and the ﬁnal layer as our linear predictor. This architecture is\ncommonly called a multilayer perceptron, often abbreviated as MLP (Fig. 5.1.1).\nt\nFig. 5.1.1\nAn MLP with a hidden layer of ﬁve hidden units.\nThis MLP has four inputs, three outputs, and its hidden layer contains ﬁve hidden units. Since\nthe input layer does not involve any calculations, producing outputs with this network requires\nimplementing the computations for both the hidden and output layers; thus, the number of\nlayers in this MLP is two. Note that both layers are fully connected. Every input inﬂuences\nevery neuron in the hidden layer, and each of these in turn inﬂuences every neuron in the\noutput layer. Alas, we are not quite done yet.\nFrom Linear to Nonlinear\nAs before, we denote by the matrix X ∈Rn×d a minibatch of n examples where each example\nhas d inputs (features). For a one-hidden-layer MLP whose hidden layer has h hidden units,\nwe denote by H ∈Rn×h the outputs of the hidden layer, which are hidden representations.\nSince the hidden and output layers are both fully connected, we have hidden-layer weights\nW(1) ∈Rd×h and biases b(1) ∈R1×h and output-layer weights W(2) ∈Rh×q and biases\nb(2) ∈R1×q. This allows us to calculate the outputs O ∈Rn×q of the one-hidden-layer MLP\n\n173\nMultilayer Perceptrons\nas follows:\nH = XW(1) + b(1),\nO = HW(2) + b(2).\n(5.1.1)\nNote that after adding the hidden layer, our model now requires us to track and update ad-\nditional sets of parameters. So what have we gained in exchange? You might be surprised\nto ﬁnd out that—in the model deﬁned above—we gain nothing for our troubles! The reason\nis plain. The hidden units above are given by an aﬃne function of the inputs, and the out-\nputs (pre-softmax) are just an aﬃne function of the hidden units. An aﬃne function of an\naﬃne function is itself an aﬃne function. Moreover, our linear model was already capable of\nrepresenting any aﬃne function.\nTo see this formally we can just collapse out the hidden layer in the above deﬁnition, yielding\nan equivalent single-layer model with parameters W = W(1)W(2) and b = b(1)W(2) +\nb(2):\nO = (XW(1) + b(1))W(2) + b(2) = XW(1)W(2) + b(1)W(2) + b(2) = XW + b.\n(5.1.2)\nIn order to realize the potential of multilayer architectures, we need one more key ingredient:\na nonlinear activation function σ to be applied to each hidden unit following the aﬃne trans-\nformation. For instance, a popular choice is the ReLU (rectiﬁed linear unit) activation func-\ntion (Nair and Hinton, 2010) σ(x) = max(0, x) operating on its arguments elementwise.\nThe outputs of activation functions σ(·) are called activations. In general, with activation\nfunctions in place, it is no longer possible to collapse our MLP into a linear model:\nH = σ(XW(1) + b(1)),\nO = HW(2) + b(2).\n(5.1.3)\nSince each row in X corresponds to an example in the minibatch, with some abuse of notation,\nwe deﬁne the nonlinearity σ to apply to its inputs in a rowwise fashion, i.e., one example at a\ntime. Note that we used the same notation for softmax when we denoted a rowwise operation\nin Section 4.1.1. Quite frequently the activation functions we use apply not merely rowwise\nbut elementwise. That means that after computing the linear portion of the layer, we can\ncalculate each activation without looking at the values taken by the other hidden units.\nTo build more general MLPs, we can continue stacking such hidden layers, e.g., H(1) =\nσ1(XW(1) + b(1)) and H(2) = σ2(H(1)W(2) + b(2)), one atop another, yielding ever\nmore expressive models.\nUniversal Approximators\nWe know that the brain is capable of very sophisticated statistical analysis. As such, it is worth\nasking, just how powerful a deep network could be. This question has been answered multiple\ntimes, e.g., in Cybenko (1989) in the context of MLPs, and in Micchelli (1984) in the context\nof reproducing kernel Hilbert spaces in a way that could be seen as radial basis function\n(RBF) networks with a single hidden layer. These (and related results) suggest that even with\n\n174\nMultilayer Perceptrons\na single-hidden-layer network, given enough nodes (possibly absurdly many), and the right set\nof weights, we can model any function. Actually learning that function is the hard part, though.\nYou might think of your neural network as being a bit like the C programming language. The\nlanguage, like any other modern language, is capable of expressing any computable program.\nBut actually coming up with a program that meets your speciﬁcations is the hard part.\nMoreover, just because a single-hidden-layer network can learn any function does not mean\nthat you should try to solve all of your problems with one. In fact, in this case kernel methods\nare way more eﬀective, since they are capable of solving the problem exactly even in inﬁ-\nnite dimensional spaces (Kimeldorf and Wahba, 1971, Schölkopf et al., 2001). In fact, we\ncan approximate many functions much more compactly by using deeper (rather than wider)\nnetworks (Simonyan and Zisserman, 2014). We will touch upon more rigorous arguments in\nsubsequent chapters.\n5.1.2 Activation Functions\nActivation functions decide whether a neuron should be activated or not by calculating the\nweighted sum and further adding bias to it. They are diﬀerentiable operators for transforming\ninput signals to outputs, while most of them add nonlinearity. Because activation functions\nare fundamental to deep learning, let’s brieﬂy survey some common ones.\nReLU Function\nThe most popular choice, due to both simplicity of implementation and its good performance\non a variety of predictive tasks, is the rectiﬁed linear unit (ReLU) (Nair and Hinton, 2010).\nReLU provides a very simple nonlinear transformation. Given an element x, the function is\ndeﬁned as the maximum of that element and 0:\nReLU(x) = max(x, 0).\n(5.1.4)\nInformally, the ReLU function retains only positive elements and discards all negative ele-\nments by setting the corresponding activations to 0. To gain some intuition, we can plot the\nfunction. As you can see, the activation function is piecewise linear.\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\nWhen the input is negative, the derivative of the ReLU function is 0, and when the input is\npositive, the derivative of the ReLU function is 1. Note that the ReLU function is not dif-\nferentiable when the input takes value precisely equal to 0. In these cases, we default to the\nleft-hand-side derivative and say that the derivative is 0 when the input is 0. We can get away\nwith this because the input may never actually be zero (mathematicians would say that it is\nnondiﬀerentiable on a set of measure zero). There is an old adage that if subtle boundary\n\n175\nMultilayer Perceptrons\nconditions matter, we are probably doing (real) mathematics, not engineering. That conven-\ntional wisdom may apply here, or at least, the fact that we are not performing constrained\noptimization (Mangasarian, 1965, Rockafellar, 1970). We plot the derivative of the ReLU\nfunction below.\ny.backward(torch.ones_like(x), retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))\nThe reason for using ReLU is that its derivatives are particularly well behaved: either they\nvanish or they just let the argument through. This makes optimization better behaved and it\nmitigated the well-documented problem of vanishing gradients that plagued previous versions\nof neural networks (more on this later).\nNote that there are many variants to the ReLU function, including the parametrized ReLU\n(pReLU) function (He et al., 2015). This variation adds a linear term to ReLU, so some\ninformation still gets through, even when the argument is negative:\npReLU(x) = max(0, x) + α min(0, x).\n(5.1.5)\n\n176\nMultilayer Perceptrons\nSigmoid Function\nThe sigmoid function transforms those inputs whose values lie in the domain R, to outputs\nthat lie on the interval (0, 1). For that reason, the sigmoid is often called a squashing function:\nit squashes any input in the range (-inf, inf) to some value in the range (0, 1):\nsigmoid(x) =\n1\n1 + exp(−x).\n(5.1.6)\nIn the earliest neural networks, scientists were interested in modeling biological neurons that\neither ﬁre or do not ﬁre. Thus the pioneers of this ﬁeld, going all the way back to McCulloch\nand Pitts, the inventors of the artiﬁcial neuron, focused on thresholding units (McCulloch and\nPitts, 1943). A thresholding activation takes value 0 when its input is below some threshold\nand value 1 when the input exceeds the threshold.\nWhen attention shifted to gradient-based learning, the sigmoid function was a natural choice\nbecause it is a smooth, diﬀerentiable approximation to a thresholding unit. Sigmoids are still\nwidely used as activation functions on the output units when we want to interpret the outputs\nas probabilities for binary classiﬁcation problems: you can think of the sigmoid as a special\ncase of the softmax. However, the sigmoid has largely been replaced by the simpler and more\neasily trainable ReLU for most use in hidden layers. Much of this has to do with the fact that\nthe sigmoid poses challenges for optimization (LeCun et al., 1998) since its gradient vanishes\nfor large positive and negative arguments. This can lead to plateaus that are diﬃcult to escape\nfrom. Nonetheless sigmoids are important. In later chapters (e.g., Section 10.1) on recurrent\nneural networks, we will describe architectures that leverage sigmoid units to control the ﬂow\nof information across time.\nBelow, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid\nfunction approaches a linear transformation.\ny = torch.sigmoid(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))\nThe derivative of the sigmoid function is given by the following equation:\nd\ndx sigmoid(x) =\nexp(−x)\n(1 + exp(−x))2 = sigmoid(x) (1 −sigmoid(x)) .\n(5.1.7)\n\n177\nMultilayer Perceptrons\nThe derivative of the sigmoid function is plotted below. Note that when the input is 0, the\nderivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from 0\nin either direction, the derivative approaches 0.\n# Clear out previous gradients\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))\nTanh Function\nLike the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs,\ntransforming them into elements on the interval between −1 and 1:\ntanh(x) = 1 −exp(−2x)\n1 + exp(−2x).\n(5.1.8)\nWe plot the tanh function below. Note that as input nears 0, the tanh function approaches\na linear transformation. Although the shape of the function is similar to that of the sigmoid\nfunction, the tanh function exhibits point symmetry about the origin of the coordinate system\n(Kalman and Kwasny, 1992).\ny = torch.tanh(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))\nThe derivative of the tanh function is:\nd\ndx tanh(x) = 1 −tanh2(x).\n(5.1.9)\nIt is plotted below. As the input nears 0, the derivative of the tanh function approaches a\nmaximum of 1. And as we saw with the sigmoid function, as input moves away from 0 in\neither direction, the derivative of the tanh function approaches 0.\n\n178\nMultilayer Perceptrons\n# Clear out previous gradients\nx.grad.data.zero_()\ny.backward(torch.ones_like(x),retain_graph=True)\nd2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))\n5.1.3 Summary and Discussion\nWe now know how to incorporate nonlinearities to build expressive multilayer neural network\narchitectures. As a side note, your knowledge already puts you in command of a similar\ntoolkit to a practitioner circa 1990. In some ways, you have an advantage over anyone working\nback then, because you can leverage powerful open-source deep learning frameworks to build\nmodels rapidly, using only a few lines of code. Previously, training these networks required\nresearchers to code up layers and derivatives explicitly in C, Fortran, or even Lisp (in the case\nof LeNet).\nA secondary beneﬁt is that ReLU is signiﬁcantly more amenable to optimization than the\nsigmoid or the tanh function. One could argue that this was one of the key innovations that\nhelped the resurgence of deep learning over the past decade. Note, though, that research in\nactivation functions has not stopped. For instance, the GELU (Gaussian error linear unit)\nactivation function xΦ(x) by Hendrycks and Gimpel (2016) (Φ(x) is the standard Gaussian\n\n179\nImplementation of Multilayer Perceptrons\n102\ncumulative distribution function) and the Swish activation function σ(x) = x sigmoid(βx)\nas proposed in Ramachandran et al. (2017) can yield better accuracy in many cases.\n5.1.4 Exercises\n1. Show that adding layers to a linear deep network, i.e., a network without nonlinearity σ\ncan never increase the expressive power of the network. Give an example where it actively\nreduces it.\n2. Compute the derivative of the pReLU activation function.\n3. Compute the derivative of the Swish activation function x sigmoid(βx).\n4. Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise linear\nfunction.\n5. Sigmoid and tanh are very similar.\n1. Show that tanh(x) + 1 = 2 sigmoid(2x).\n2. Prove that the function classes parametrized by both nonlinearities are identical. Hint:\naﬃne layers have bias terms, too.\n6. Assume that we have a nonlinearity that applies to one minibatch at a time, such as the\nbatch normalization (Ioﬀe and Szegedy, 2015). What kinds of problems do you expect\nthis to cause?\n7. Provide an example where the gradients vanish for the sigmoid activation function.\nDiscussions102.\n5.2 Implementation of Multilayer Perceptrons\nMultilayer perceptrons (MLPs) are not much more complex to implement than simple linear\nmodels. The key conceptual diﬀerence is that we now concatenate multiple layers.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n5.2.1 Implementation from Scratch\nLet’s begin again by implementing such a network from scratch.\n\n180\nMultilayer Perceptrons\nInitializing Model Parameters\nRecall that Fashion-MNIST contains 10 classes, and that each image consists of a 28 × 28 =\n784 grid of grayscale pixel values. As before we will disregard the spatial structure among\nthe pixels for now, so we can think of this as a classiﬁcation dataset with 784 input fea-\ntures and 10 classes. To begin, we will implement an MLP with one hidden layer and 256\nhidden units. Both the number of layers and their width are adjustable (they are considered\nhyperparameters). Typically, we choose the layer widths to be divisible by larger powers of\n2. This is computationally eﬃcient due to the way memory is allocated and addressed in\nhardware.\nAgain, we will represent our parameters with several tensors. Note that for every layer, we\nmust keep track of one weight matrix and one bias vector. As always, we allocate memory\nfor the gradients of the loss with respect to these parameters.\nIn the code below we use nn.Parameter to automatically register a class attribute as a pa-\nrameter to be tracked by autograd (Section 2.5).\nclass MLPScratch(d2l.Classifier):\ndef __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\nself.b1 = nn.Parameter(torch.zeros(num_hiddens))\nself.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\nself.b2 = nn.Parameter(torch.zeros(num_outputs))\nModel\nTo make sure we know how everything works, we will implement the ReLU activation our-\nselves rather than invoking the built-in relu function directly.\ndef relu(X):\na = torch.zeros_like(X)\nreturn torch.max(X, a)\nSince we are disregarding spatial structure, we reshape each two-dimensional image into a\nﬂat vector of length num_inputs. Finally, we implement our model with just a few lines of\ncode. Since we use the framework built-in autograd this is all that it takes.\n@d2l.add_to_class(MLPScratch)\ndef forward(self, X):\nX = X.reshape((-1, self.num_inputs))\nH = relu(torch.matmul(X, self.W1) + self.b1)\nreturn torch.matmul(H, self.W2) + self.b2\n\n181\nImplementation of Multilayer Perceptrons\nTraining\nFortunately, the training loop for MLPs is exactly the same as for softmax regression. We de-\nﬁne the model, data, and trainer, then ﬁnally invoke the fit method on model and data.\nmodel = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\ndata = d2l.FashionMNIST(batch_size=256)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n5.2.2 Concise Implementation\nAs you might expect, by relying on the high-level APIs, we can implement MLPs even more\nconcisely.\nModel\nCompared with our concise implementation of softmax regression implementation (Section\n4.5), the only diﬀerence is that we add two fully connected layers where we previously added\nonly one. The ﬁrst is the hidden layer, the second is the output layer.\nclass MLP(d2l.Classifier):\ndef __init__(self, num_outputs, num_hiddens, lr):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\nnn.ReLU(), nn.LazyLinear(num_outputs))\nPreviously, we deﬁned forward methods for models to transform input using the model pa-\nrameters. These operations are essentially a pipeline: you take an input and apply a transfor-\nmation (e.g., matrix multiplication with weights followed by bias addition), then repetitively\nuse the output of the current transformation as input to the next transformation. However,\n\n182\nMultilayer Perceptrons\nyou may have noticed that no forward method is deﬁned here. In fact, MLP inherits the for-\nward method from the Module class (Section 3.2.2) to simply invoke self.net(X) (X is\ninput), which is now deﬁned as a sequence of transformations via the Sequential class. The\nSequential class abstracts the forward process enabling us to focus on the transformations.\nWe will further discuss how the Sequential class works in Section 6.1.2.\nTraining\nThe training loop is exactly the same as when we implemented softmax regression. This\nmodularity enables us to separate matters concerning the model architecture from orthogonal\nconsiderations.\nmodel = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\ntrainer.fit(model, data)\n5.2.3 Summary\nNow that we have more practice in designing deep networks, the step from a single to multiple\nlayers of deep networks does not pose such a signiﬁcant challenge any longer. In particular,\nwe can reuse the training algorithm and data loader. Note, though, that implementing MLPs\nfrom scratch is nonetheless messy: naming and keeping track of the model parameters makes\nit diﬃcult to extend models. For instance, imagine wanting to insert another layer between\nlayers 42 and 43. This might now be layer 42b, unless we are willing to perform sequential\nrenaming. Moreover, if we implement the network from scratch, it is much more diﬃcult for\nthe framework to perform meaningful performance optimizations.\nNonetheless, you have now reached the state of the art of the late 1980s when fully connected\ndeep networks were the method of choice for neural network modeling. Our next conceptual\nstep will be to consider images. Before we do so, we need to review a number of statistical\nbasics and details on how to compute models eﬃciently.\n\n183\nForward Propagation, Backward Propagation, and Computational Graphs\n103\n5.2.4 Exercises\n1. Change the number of hidden units num_hiddens and plot how its number aﬀects the\naccuracy of the model. What is the best value of this hyperparameter?\n2. Try adding a hidden layer to see how it aﬀects the results.\n3. Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n4. How does changing the learning rate alter your results? With all other parameters ﬁxed,\nwhich learning rate gives you the best results? How does this relate to the number of\nepochs?\n5. Let’s optimize over all hyperparameters jointly, i.e., learning rate, number of epochs,\nnumber of hidden layers, and number of hidden units per layer.\n1. What is the best result you can get by optimizing over all of them?\n2. Why it is much more challenging to deal with multiple hyperparameters?\n3. Describe an eﬃcient strategy for optimizing over multiple parameters jointly.\n6. Compare the speed of the framework and the from-scratch implementation for a chal-\nlenging problem. How does it change with the complexity of the network?\n7. Measure the speed of tensor–matrix multiplications for well-aligned and misaligned ma-\ntrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n1. How does this change between GPUs and CPUs?\n2. Determine the memory bus width of your CPU and GPU.\n8. Try out diﬀerent activation functions. Which one works best?\n9. Is there a diﬀerence between weight initializations of the network? Does it matter?\nDiscussions103.\n5.3 Forward Propagation, Backward Propagation,\nand Computational Graphs\nSo far, we have trained our models with minibatch stochastic gradient descent. However,\nwhen we implemented the algorithm, we only worried about the calculations involved in for-\nward propagation through the model. When it came time to calculate the gradients, we just\ninvoked the backpropagation function provided by the deep learning framework.\nThe automatic calculation of gradients profoundly simpliﬁes the implementation of deep\nlearning algorithms. Before automatic diﬀerentiation, even small changes to complicated\n\n184\nMultilayer Perceptrons\nmodels required recalculating complicated derivatives by hand. Surprisingly often, academic\npapers had to allocate numerous pages to deriving update rules. While we must continue\nto rely on automatic diﬀerentiation so we can focus on the interesting parts, you ought to\nknow how these gradients are calculated under the hood if you want to go beyond a shallow\nunderstanding of deep learning.\nIn this section, we take a deep dive into the details of backward propagation (more commonly\ncalled backpropagation). To convey some insight for both the techniques and their implemen-\ntations, we rely on some basic mathematics and computational graphs. To start, we focus our\nexposition on a one-hidden-layer MLP with weight decay (ℓ2 regularization, to be described\nin subsequent chapters).\n5.3.1 Forward Propagation\nForward propagation (or forward pass) refers to the calculation and storage of intermediate\nvariables (including outputs) for a neural network in order from the input layer to the output\nlayer. We now work step-by-step through the mechanics of a neural network with one hidden\nlayer. This may seem tedious but in the eternal words of funk virtuoso James Brown, you\nmust “pay the cost to be the boss”.\nFor the sake of simplicity, let’s assume that the input example is x ∈Rd and that our hidden\nlayer does not include a bias term. Here the intermediate variable is:\nz = W(1)x,\n(5.3.1)\nwhere W(1) ∈Rh×d is the weight parameter of the hidden layer. After running the inter-\nmediate variable z ∈Rh through the activation function ϕ we obtain our hidden activation\nvector of length h:\nh = ϕ(z).\n(5.3.2)\nThe hidden layer output h is also an intermediate variable. Assuming that the parameters\nof the output layer possess only a weight of W(2) ∈Rq×h, we can obtain an output layer\nvariable with a vector of length q:\no = W(2)h.\n(5.3.3)\nAssuming that the loss function is l and the example label is y, we can then calculate the loss\nterm for a single data example,\nL = l(o, y).\n(5.3.4)\nAs we will see the deﬁnition of ℓ2 regularization to be introduced later, given the hyperpa-\nrameter λ, the regularization term is\ns = λ\n2\n(\n∥W(1)∥2\nF + ∥W(2)∥2\nF\n)\n,\n(5.3.5)\n\n185\nForward Propagation, Backward Propagation, and Computational Graphs\nwhere the Frobenius norm of the matrix is simply the ℓ2 norm applied after ﬂattening the\nmatrix into a vector. Finally, the model’s regularized loss on a given data example is:\nJ = L + s.\n(5.3.6)\nWe refer to J as the objective function in the following discussion.\n5.3.2 Computational Graph of Forward Propagation\nPlotting computational graphs helps us visualize the dependencies of operators and variables\nwithin the calculation. Fig. 5.3.1 contains the graph associated with the simple network de-\nscribed above, where squares denote variables and circles denote operators. The lower-left\ncorner signiﬁes the input and the upper-right corner is the output. Notice that the directions\nof the arrows (which illustrate data ﬂow) are primarily rightward and upward.\nt\nFig. 5.3.1\nComputational graph of forward propagation.\n5.3.3 Backpropagation\nBackpropagation refers to the method of calculating the gradient of neural network param-\neters. In short, the method traverses the network in reverse order, from the output to the\ninput layer, according to the chain rule from calculus. The algorithm stores any intermediate\nvariables (partial derivatives) required while calculating the gradient with respect to some pa-\nrameters. Assume that we have functions Y = f (X) and Z = g(Y), in which the input and\nthe output X, Y, Z are tensors of arbitrary shapes. By using the chain rule, we can compute\nthe derivative of Z with respect to X via\n∂Z\n∂X = prod\n( ∂Z\n∂Y, ∂Y\n∂X\n)\n.\n(5.3.7)\nHere we use the prod operator to multiply its arguments after the necessary operations, such\nas transposition and swapping input positions, have been carried out. For vectors, this is\nstraightforward: it is simply matrix–matrix multiplication. For higher dimensional tensors, we\nuse the appropriate counterpart. The operator prod hides all the notational overhead.\nRecall that the parameters of the simple network with one hidden layer, whose computational\ngraph is in Fig. 5.3.1, are W(1) and W(2). The objective of backpropagation is to calculate\nthe gradients ∂J/∂W(1) and ∂J/∂W(2). To accomplish this, we apply the chain rule and\ncalculate, in turn, the gradient of each intermediate variable and parameter. The order of\n\n186\nMultilayer Perceptrons\ncalculations are reversed relative to those performed in forward propagation, since we need to\nstart with the outcome of the computational graph and work our way towards the parameters.\nThe ﬁrst step is to calculate the gradients of the objective function J = L + s with respect to\nthe loss term L and the regularization term s:\n∂J\n∂L = 1 and ∂J\n∂s = 1.\n(5.3.8)\nNext, we compute the gradient of the objective function with respect to variable of the output\nlayer o according to the chain rule:\n∂J\n∂o = prod\n( ∂J\n∂L, ∂L\n∂o\n)\n= ∂L\n∂o ∈Rq.\n(5.3.9)\nNext, we calculate the gradients of the regularization term with respect to both parame-\nters:\n∂s\n∂W(1) = λW(1) and\n∂s\n∂W(2) = λW(2).\n(5.3.10)\nNow we are able to calculate the gradient ∂J/∂W(2) ∈Rq×h of the model parameters closest\nto the output layer. Using the chain rule yields:\n∂J\n∂W(2) = prod\n( ∂J\n∂o,\n∂o\n∂W(2)\n)\n+ prod\n( ∂J\n∂s,\n∂s\n∂W(2)\n)\n= ∂J\n∂oh⊤+ λW(2).\n(5.3.11)\nTo obtain the gradient with respect to W(1) we need to continue backpropagation along\nthe output layer to the hidden layer. The gradient with respect to the hidden layer output\n∂J/∂h ∈Rh is given by\n∂J\n∂h = prod\n( ∂J\n∂o, ∂o\n∂h\n)\n= W(2)⊤∂J\n∂o.\n(5.3.12)\nSince the activation function ϕ applies elementwise, calculating the gradient ∂J/∂z ∈Rh\nof the intermediate variable z requires that we use the elementwise multiplication operator,\nwhich we denote by ⊙:\n∂J\n∂z = prod\n( ∂J\n∂h, ∂h\n∂z\n)\n= ∂J\n∂h ⊙ϕ′ (z) .\n(5.3.13)\nFinally, we can obtain the gradient ∂J/∂W(1) ∈Rh×d of the model parameters closest to\nthe input layer. According to the chain rule, we get\n∂J\n∂W(1) = prod\n( ∂J\n∂z,\n∂z\n∂W(1)\n)\n+ prod\n( ∂J\n∂s,\n∂s\n∂W(1)\n)\n= ∂J\n∂zx⊤+ λW(1).\n(5.3.14)\n5.3.4 Training Neural Networks\nWhen training neural networks, forward and backward propagation depend on each other. In\nparticular, for forward propagation, we traverse the computational graph in the direction of\ndependencies and compute all the variables on its path. These are then used for backpropa-\ngation where the compute order on the graph is reversed.\n\n187\nForward Propagation, Backward Propagation, and Computational Graphs\nTake the aforementioned simple network as an illustrative example. On the one hand, com-\nputing the regularization term (5.3.5) during forward propagation depends on the current\nvalues of model parameters W(1) and W(2). They are given by the optimization algorithm\naccording to backpropagation in the most recent iteration. On the other hand, the gradient\ncalculation for the parameter (5.3.11) during backpropagation depends on the current value\nof the hidden layer output h, which is given by forward propagation.\nTherefore when training neural networks, once model parameters are initialized, we alternate\nforward propagation with backpropagation, updating model parameters using gradients given\nby backpropagation. Note that backpropagation reuses the stored intermediate values from\nforward propagation to avoid duplicate calculations. One of the consequences is that we need\nto retain the intermediate values until backpropagation is complete. This is also one of the\nreasons why training requires signiﬁcantly more memory than plain prediction. Besides, the\nsize of such intermediate values is roughly proportional to the number of network layers and\nthe batch size. Thus, training deeper networks using larger batch sizes more easily leads to\nout-of-memory errors.\n5.3.5 Summary\nForward propagation sequentially calculates and stores intermediate variables within the com-\nputational graph deﬁned by the neural network. It proceeds from the input to the output layer.\nBackpropagation sequentially calculates and stores the gradients of intermediate variables\nand parameters within the neural network in the reversed order. When training deep learning\nmodels, forward propagation and backpropagation are interdependent, and training requires\nsigniﬁcantly more memory than prediction.\n5.3.6 Exercises\n1. Assume that the inputs X to some scalar function f are n × m matrices. What is the\ndimensionality of the gradient of f with respect to X?\n2. Add a bias to the hidden layer of the model described in this section (you do not need to\ninclude bias in the regularization term).\n1. Draw the corresponding computational graph.\n2. Derive the forward and backward propagation equations.\n3. Compute the memory footprint for training and prediction in the model described in this\nsection.\n4. Assume that you want to compute second derivatives. What happens to the computational\ngraph? How long do you expect the calculation to take?\n5. Assume that the computational graph is too large for your GPU.\n1. Can you partition it over more than one GPU?\n\n188\nMultilayer Perceptrons\n104\n2. What are the advantages and disadvantages over training on a smaller minibatch?\nDiscussions104.\n5.4 Numerical Stability and Initialization\nThus far, every model that we have implemented required that we initialize its parameters\naccording to some pre-speciﬁed distribution. Until now, we took the initialization scheme for\ngranted, glossing over the details of how these choices are made. You might have even gotten\nthe impression that these choices are not especially important. On the contrary, the choice of\ninitialization scheme plays a signiﬁcant role in neural network learning, and it can be crucial\nfor maintaining numerical stability. Moreover, these choices can be tied up in interesting ways\nwith the choice of the nonlinear activation function. Which function we choose and how we\ninitialize parameters can determine how quickly our optimization algorithm converges. Poor\nchoices here can cause us to encounter exploding or vanishing gradients while training. In\nthis section, we delve into these topics in greater detail and discuss some useful heuristics\nthat you will ﬁnd useful throughout your career in deep learning.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\n5.4.1 Vanishing and Exploding Gradients\nConsider a deep network with L layers, input x and output o. With each layer l deﬁned by\na transformation fl parametrized by weights W(l), whose hidden layer output is h(l) (let\nh(0) = x), our network can be expressed as:\nh(l) = fl(h(l−1)) and thus o = fL ◦· · · ◦f1(x).\n(5.4.1)\nIf all the hidden layer output and the input are vectors, we can write the gradient of o with\nrespect to any set of parameters W(l) as follows:\n∂W(l)o = ∂h(L−1)h(L)\n|        {z        }\nM(L)def\n=\n· · · ∂h(l)h(l+1)\n|       {z       }\nM(l+1)def\n=\n∂W(l)h(l)\n|     {z     }\nv(l)def\n=\n.\n(5.4.2)\nIn other words, this gradient is the product of L−l matrices M(L) · · · M(l+1) and the gradient\nvector v(l). Thus we are susceptible to the same problems of numerical underﬂow that often\ncrop up when multiplying together too many probabilities. When dealing with probabilities,\na common trick is to switch into log-space, i.e., shifting pressure from the mantissa to the\nexponent of the numerical representation. Unfortunately, our problem above is more serious:\n\n189\nNumerical Stability and Initialization\ninitially the matrices M(l) may have a wide variety of eigenvalues. They might be small or\nlarge, and their product might be very large or very small.\nThe risks posed by unstable gradients go beyond numerical representation. Gradients of un-\npredictable magnitude also threaten the stability of our optimization algorithms. We may\nbe facing parameter updates that are either (i) excessively large, destroying our model (the\nexploding gradient problem); or (ii) excessively small (the vanishing gradient problem), ren-\ndering learning impossible as parameters hardly move on each update.\nVanishing Gradients\nOne frequent culprit causing the vanishing gradient problem is the choice of the activation\nfunction σ that is appended following each layer’s linear operations. Historically, the sigmoid\nfunction 1/(1 + exp(−x)) (introduced in Section 5.1) was popular because it resembles a\nthresholding function. Since early artiﬁcial neural networks were inspired by biological neu-\nral networks, the idea of neurons that ﬁre either fully or not at all (like biological neurons)\nseemed appealing. Let’s take a closer look at the sigmoid to see why it can cause vanishing\ngradients.\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.sigmoid(x)\ny.backward(torch.ones_like(x))\nd2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],\nlegend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))\nAs you can see, the sigmoid’s gradient vanishes both when its inputs are large and when\nthey are small. Moreover, when backpropagating through many layers, unless we are in the\nGoldilocks zone, where the inputs to many of the sigmoids are close to zero, the gradients of\nthe overall product may vanish. When our network boasts many layers, unless we are careful,\nthe gradient will likely be cut oﬀat some layer. Indeed, this problem used to plague deep\nnetwork training. Consequently, ReLUs, which are more stable (but less neurally plausible),\nhave emerged as the default choice for practitioners.\n\n190\nMultilayer Perceptrons\nExploding Gradients\nThe opposite problem, when gradients explode, can be similarly vexing. To illustrate this\na bit better, we draw 100 Gaussian random matrices and multiply them with some initial\nmatrix. For the scale that we picked (the choice of the variance σ2 = 1), the matrix product\nexplodes. When this happens because of the initialization of a deep network, we have no\nchance of getting a gradient descent optimizer to converge.\nM = torch.normal(0, 1, size=(4, 4))\nprint('a single matrix \\n',M)\nfor i in range(100):\nM = M @ torch.normal(0, 1, size=(4, 4))\nprint('after multiplying 100 matrices\\n', M)\na single matrix\ntensor([[ 1.7785,\n1.4507,\n1.3437,\n0.6168],\n[-0.4228,\n0.0714,\n1.3975,\n1.6048],\n[-1.0973,\n0.2253,\n0.3940, -0.2097],\n[-0.2455, -0.2039, -0.9628,\n2.2473]])\nafter multiplying 100 matrices\ntensor([[-1.2044e+25,\n2.8908e+22, -1.9161e+24, -4.9610e+24],\n[-8.1750e+24,\n1.9621e+22, -1.3006e+24, -3.3674e+24],\n[ 2.8029e+24, -6.7275e+21,\n4.4592e+23,\n1.1545e+24],\n[-2.6804e+24,\n6.4328e+21, -4.2643e+23, -1.1041e+24]])\nBreaking the Symmetry\nAnother problem in neural network design is the symmetry inherent in their parametrization.\nAssume that we have a simple MLP with one hidden layer and two units. In this case, we\ncould permute the weights W(1) of the ﬁrst layer and likewise permute the weights of the\noutput layer to obtain the same function. There is nothing special diﬀerentiating the ﬁrst and\nsecond hidden units. In other words, we have permutation symmetry among the hidden units\nof each layer.\nThis is more than just a theoretical nuisance. Consider the aforementioned one-hidden-layer\nMLP with two hidden units. For illustration, suppose that the output layer transforms the\ntwo hidden units into only one output unit. Imagine what would happen if we initialized all\nthe parameters of the hidden layer as W(1) = c for some constant c. In this case, during\nforward propagation either hidden unit takes the same inputs and parameters producing the\nsame activation which is fed to the output unit. During backpropagation, diﬀerentiating the\noutput unit with respect to parameters W(1) gives a gradient all of whose elements take the\nsame value. Thus, after gradient-based iteration (e.g., minibatch stochastic gradient descent),\nall the elements of W(1) still take the same value. Such iterations would never break the\nsymmetry on their own and we might never be able to realize the network’s expressive power.\nThe hidden layer would behave as if it had only a single unit. Note that while minibatch\n\n191\nNumerical Stability and Initialization\nstochastic gradient descent would not break this symmetry, dropout regularization (to be\nintroduced later) would!\n5.4.2 Parameter Initialization\nOne way of addressing—or at least mitigating—the issues raised above is through careful\ninitialization. As we will see later, additional care during optimization and suitable regular-\nization can further enhance stability.\nDefault Initialization\nIn the previous sections, e.g., in Section 3.5, we used a normal distribution to initialize the\nvalues of our weights. If we do not specify the initialization method, the framework will\nuse a default random initialization method, which often works well in practice for moderate\nproblem sizes.\nXavier Initialization\nLet’s look at the scale distribution of an output oi for some fully connected layer without\nnonlinearities. With nin inputs xj and their associated weights wij for this layer, an output is\ngiven by\noi =\nnin\n∑\nj=1\nwij xj.\n(5.4.3)\nThe weights wij are all drawn independently from the same distribution. Furthermore, let’s\nassume that this distribution has zero mean and variance σ2. Note that this does not mean\nthat the distribution has to be Gaussian, just that the mean and variance need to exist. For\nnow, let’s assume that the inputs to the layer xj also have zero mean and variance γ2 and that\nthey are independent of wij and independent of each other. In this case, we can compute the\nmean of oi:\nE[oi] =\nnin\n∑\nj=1\nE[wij xj]\n=\nnin\n∑\nj=1\nE[wij]E[xj]\n= 0,\n(5.4.4)\n\n192\nMultilayer Perceptrons\nand the variance:\nVar[oi] = E[o2\ni ] −(E[oi])2\n=\nnin\n∑\nj=1\nE[w2\nij x2\nj ] −0\n=\nnin\n∑\nj=1\nE[w2\nij]E[x2\nj ]\n= ninσ2γ2.\n(5.4.5)\nOne way to keep the variance ﬁxed is to set ninσ2 = 1. Now consider backpropagation. There\nwe face a similar problem, albeit with gradients being propagated from the layers closer to\nthe output. Using the same reasoning as for forward propagation, we see that the gradients’\nvariance can blow up unless noutσ2 = 1, where nout is the number of outputs of this layer. This\nleaves us in a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead,\nwe simply try to satisfy:\n1\n2(nin + nout)σ2 = 1 or equivalently σ =\n√\n2\nnin + nout\n.\n(5.4.6)\nThis is the reasoning underlying the now-standard and practically beneﬁcial Xavier initial-\nization, named after the ﬁrst author of its creators (Glorot and Bengio, 2010). Typically, the\nXavier initialization samples weights from a Gaussian distribution with zero mean and vari-\nance σ2 =\n2\nnin+nout . We can also adapt this to choose the variance when sampling weights\nfrom a uniform distribution. Note that the uniform distribution U(−a, a) has variance a2\n3 .\nPlugging a2\n3 into our condition on σ2 prompts us to initialize according to\nU\n(\n−\n√\n6\nnin + nout\n,\n√\n6\nnin + nout\n)\n.\n(5.4.7)\nThough the assumption for nonexistence of nonlinearities in the above mathematical reason-\ning can be easily violated in neural networks, the Xavier initialization method turns out to\nwork well in practice.\nBeyond\nThe reasoning above barely scratches the surface of modern approaches to parameter ini-\ntialization. A deep learning framework often implements over a dozen diﬀerent heuristics.\nMoreover, parameter initialization continues to be a hot area of fundamental research in\ndeep learning. Among these are heuristics specialized for tied (shared) parameters, super-\nresolution, sequence models, and other situations. For instance, Xiao et al. (2018) demon-\nstrated the possibility of training 10,000-layer neural networks without architectural tricks\nby using a carefully-designed initialization method.\nIf the topic interests you we suggest a deep dive into this module’s oﬀerings, reading the\npapers that proposed and analyzed each heuristic, and then exploring the latest publications\n\n193\nGeneralization in Deep Learning\n105\non the topic. Perhaps you will stumble across or even invent a clever idea and contribute an\nimplementation to deep learning frameworks.\n5.4.3 Summary\nVanishing and exploding gradients are common issues in deep networks. Great care in param-\neter initialization is required to ensure that gradients and parameters remain well controlled.\nInitialization heuristics are needed to ensure that the initial gradients are neither too large nor\ntoo small. Random initialization is key to ensuring that symmetry is broken before optimiza-\ntion. Xavier initialization suggests that, for each layer, variance of any output is not aﬀected\nby the number of inputs, and variance of any gradient is not aﬀected by the number of out-\nputs. ReLU activation functions mitigate the vanishing gradient problem. This can accelerate\nconvergence.\n5.4.4 Exercises\n1. Can you design other cases where a neural network might exhibit symmetry that needs\nbreaking, besides the permutation symmetry in an MLP’s layers?\n2. Can we initialize all weight parameters in linear regression or in softmax regression to the\nsame value?\n3. Look up analytic bounds on the eigenvalues of the product of two matrices. What does\nthis tell you about ensuring that gradients are well conditioned?\n4. If we know that some terms diverge, can we ﬁx this after the fact? Look at the paper on\nlayerwise adaptive rate scaling for inspiration (You et al., 2017).\nDiscussions105.\n5.5 Generalization in Deep Learning\nIn Chapter 3 and Chapter 4, we tackled regression and classiﬁcation problems by ﬁtting lin-\near models to training data. In both cases, we provided practical algorithms for ﬁnding the\nparameters that maximized the likelihood of the observed training labels. And then, towards\nthe end of each chapter, we recalled that ﬁtting the training data was only an intermediate\ngoal. Our real quest all along was to discover general patterns on the basis of which we can\nmake accurate predictions even on new examples drawn from the same underlying popula-\ntion. Machine learning researchers are consumers of optimization algorithms. Sometimes, we\nmust even develop new optimization algorithms. But at the end of the day, optimization is\nmerely a means to an end. At its core, machine learning is a statistical discipline and we wish\n\n194\nMultilayer Perceptrons\nto optimize training loss only insofar as some statistical principle (known or unknown) leads\nthe resulting models to generalize beyond the training set.\nOn the bright side, it turns out that deep neural networks trained by stochastic gradient de-\nscent generalize remarkably well across myriad prediction problems, spanning computer vi-\nsion; natural language processing; time series data; recommender systems; electronic health\nrecords; protein folding; value function approximation in video games and board games; and\nnumerous other domains. On the downside, if you were looking for a straightforward account\nof either the optimization story (why we can ﬁt them to training data) or the generaliza-\ntion story (why the resulting models generalize to unseen examples), then you might want to\npour yourself a drink. While our procedures for optimizing linear models and the statistical\nproperties of the solutions are both described well by a comprehensive body of theory, our\nunderstanding of deep learning still resembles the wild west on both fronts.\nBoth the theory and practice of deep learning are rapidly evolving, with theorists adopting\nnew strategies to explain what’s going on, even as practitioners continue to innovate at a blis-\ntering pace, building arsenals of heuristics for training deep networks and a body of intuitions\nand folk knowledge that provide guidance for deciding which techniques to apply in which\nsituations.\nThe summary of the present moment is that the theory of deep learning has produced promis-\ning lines of attack and scattered fascinating results, but still appears far from a comprehensive\naccount of both (i) why we are able to optimize neural networks and (ii) how models learned\nby gradient descent manage to generalize so well, even on high-dimensional tasks. However,\nin practice, (i) is seldom a problem (we can always ﬁnd parameters that will ﬁt all of our\ntraining data) and thus understanding generalization is far the bigger problem. On the other\nhand, even absent the comfort of a coherent scientiﬁc theory, practitioners have developed\na large collection of techniques that may help you to produce models that generalize well in\npractice. While no pithy summary can possibly do justice to the vast topic of generalization\nin deep learning, and while the overall state of research is far from resolved, we hope, in this\nsection, to present a broad overview of the state of research and practice.\n5.5.1 Revisiting Overﬁtting and Regularization\nAccording to the “no free lunch” theorem of Wolpert and Macready (1995), any learning\nalgorithm generalizes better on data with certain distributions, and worse with other distri-\nbutions. Thus, given a ﬁnite training set, a model relies on certain assumptions: to achieve\nhuman-level performance it may be useful to identify inductive biases that reﬂect how humans\nthink about the world. Such inductive biases show preferences for solutions with certain prop-\nerties. For example, a deep MLP has an inductive bias towards building up a complicated\nfunction by the composition of simpler functions.\nWith machine learning models encoding inductive biases, our approach to training them typ-\nically consists of two phases: (i) ﬁt the training data; and (ii) estimate the generalization error\n(the true error on the underlying population) by evaluating the model on holdout data. The\ndiﬀerence between our ﬁt on the training data and our ﬁt on the test data is called the gen-\n\n195\nGeneralization in Deep Learning\neralization gap and when this is large, we say that our models overﬁt to the training data.\nIn extreme cases of overﬁtting, we might exactly ﬁt the training data, even when the test er-\nror remains signiﬁcant. And in the classical view, the interpretation is that our models are\ntoo complex, requiring that we either shrink the number of features, the number of nonzero\nparameters learned, or the size of the parameters as quantiﬁed. Recall the plot of model\ncomplexity compared with loss (Fig. 3.6.1) from Section 3.6.\nHowever deep learning complicates this picture in counterintuitive ways. First, for classiﬁ-\ncation problems, our models are typically expressive enough to perfectly ﬁt every training\nexample, even in datasets consisting of millions (Zhang et al., 2021). In the classical picture,\nwe might think that this setting lies on the far right extreme of the model complexity axis,\nand that any improvements in generalization error must come by way of regularization, either\nby reducing the complexity of the model class, or by applying a penalty, severely constrain-\ning the set of values that our parameters might take. But that is where things start to get\nweird.\nStrangely, for many deep learning tasks (e.g., image recognition and text classiﬁcation) we are\ntypically choosing among model architectures, all of which can achieve arbitrarily low training\nloss (and zero training error). Because all models under consideration achieve zero training\nerror, the only avenue for further gains is to reduce overﬁtting. Even stranger, it is often the\ncase that despite ﬁtting the training data perfectly, we can actually reduce the generalization\nerror further by making the model even more expressive, e.g., adding layers, nodes, or training\nfor a larger number of epochs. Stranger yet, the pattern relating the generalization gap to the\ncomplexity of the model (as captured, for example, in the depth or width of the networks) can\nbe non-monotonic, with greater complexity hurting at ﬁrst but subsequently helping in a so-\ncalled “double-descent” pattern (Nakkiran et al., 2021). Thus the deep learning practitioner\npossesses a bag of tricks, some of which seemingly restrict the model in some fashion and\nothers that seemingly make it even more expressive, and all of which, in some sense, are\napplied to mitigate overﬁtting.\nComplicating things even further, while the guarantees provided by classical learning theory\ncan be conservative even for classical models, they appear powerless to explain why it is that\ndeep neural networks generalize in the ﬁrst place. Because deep neural networks are capable\nof ﬁtting arbitrary labels even for large datasets, and despite the use of familiar methods such\nas ℓ2 regularization, traditional complexity-based generalization bounds, e.g., those based on\nthe VC dimension or Rademacher complexity of a hypothesis class cannot explain why neural\nnetworks generalize.\n5.5.2 Inspiration from Nonparametrics\nApproaching deep learning for the ﬁrst time, it is tempting to think of them as parametric\nmodels. After all, the models do have millions of parameters. When we update the models,\nwe update their parameters. When we save the models, we write their parameters to disk.\nHowever, mathematics and computer science are riddled with counterintuitive changes of\nperspective, and surprising isomorphisms between seemingly diﬀerent problems. While neu-\nral networks clearly have parameters, in some ways it can be more fruitful to think of them\n\n196\nMultilayer Perceptrons\nas behaving like nonparametric models. So what precisely makes a model nonparametric?\nWhile the name covers a diverse set of approaches, one common theme is that nonparamet-\nric methods tend to have a level of complexity that grows as the amount of available data\ngrows.\nPerhaps the simplest example of a nonparametric model is the k-nearest neighbor algorithm\n(we will cover more nonparametric models later, for example in Section 11.2). Here, at train-\ning time, the learner simply memorizes the dataset. Then, at prediction time, when confronted\nwith a new point x, the learner looks up the k nearest neighbors (the k points x′\ni that min-\nimize some distance d(x, x′\ni)). When k = 1, this algorithm is called 1-nearest neighbors,\nand the algorithm will always achieve a training error of zero. That however, does not mean\nthat the algorithm will not generalize. In fact, it turns out that under some mild conditions,\nthe 1-nearest neighbor algorithm is consistent (eventually converging to the optimal predic-\ntor).\nNote that 1-nearest neighbor requires that we specify some distance function d, or equiva-\nlently, that we specify some vector-valued basis function ϕ(x) for featurizing our data. For\nany choice of the distance metric, we will achieve zero training error and eventually reach\nan optimal predictor, but diﬀerent distance metrics d encode diﬀerent inductive biases and\nwith a ﬁnite amount of available data will yield diﬀerent predictors. Diﬀerent choices of\nthe distance metric d represent diﬀerent assumptions about the underlying patterns and the\nperformance of the diﬀerent predictors will depend on how compatible the assumptions are\nwith the observed data.\nIn a sense, because neural networks are over-parametrized, possessing many more parame-\nters than are needed to ﬁt the training data, they tend to interpolate the training data (ﬁtting\nit perfectly) and thus behave, in some ways, more like nonparametric models. More recent\ntheoretical research has established deep connection between large neural networks and non-\nparametric methods, notably kernel methods. In particular, Jacot et al. (2018) demonstrated\nthat in the limit, as multilayer perceptrons with randomly initialized weights grow inﬁnitely\nwide, they become equivalent to (nonparametric) kernel methods for a speciﬁc choice of the\nkernel function (essentially, a distance function), which they call the neural tangent kernel.\nWhile current neural tangent kernel models may not fully explain the behavior of modern\ndeep networks, their success as an analytical tool underscores the usefulness of nonparamet-\nric modeling for understanding the behavior of over-parametrized deep networks.\n5.5.3 Early Stopping\nWhile deep neural networks are capable of ﬁtting arbitrary labels, even when labels are as-\nsigned incorrectly or randomly (Zhang et al., 2021), this capability only emerges over many\niterations of training. A new line of work (Rolnick et al., 2017) has revealed that in the set-\nting of label noise, neural networks tend to ﬁt cleanly labeled data ﬁrst and only subsequently\nto interpolate the mislabeled data. Moreover, it has been established that this phenomenon\ntranslates directly into a guarantee on generalization: whenever a model has ﬁtted the cleanly\nlabeled data but not randomly labeled examples included in the training set, it has in fact\ngeneralized (Garg et al., 2021).\n\n197\nGeneralization in Deep Learning\nTogether these ﬁndings help to motivate early stopping, a classic technique for regularizing\ndeep neural networks. Here, rather than directly constraining the values of the weights, one\nconstrains the number of epochs of training. The most common way to determine the stopping\ncriterion is to monitor validation error throughout training (typically by checking once after\neach epoch) and to cut oﬀtraining when the validation error has not decreased by more\nthan some small amount ϵ for some number of epochs. This is sometimes called a patience\ncriterion. As well as the potential to lead to better generalization in the setting of noisy labels,\nanother beneﬁt of early stopping is the time saved. Once the patience criterion is met, one\ncan terminate training. For large models that might require days of training simultaneously\nacross eight or more GPUs, well-tuned early stopping can save researchers days of time and\ncan save their employers many thousands of dollars.\nNotably, when there is no label noise and datasets are realizable (the classes are truly sep-\narable, e.g., distinguishing cats from dogs), early stopping tends not to lead to signiﬁcant\nimprovements in generalization. On the other hand, when there is label noise, or intrinsic\nvariability in the label (e.g., predicting mortality among patients), early stopping is crucial.\nTraining models until they interpolate noisy data is typically a bad idea.\n5.5.4 Classical Regularization Methods for Deep Networks\nIn Chapter 3, we described several classical regularization techniques for constraining the\ncomplexity of our models. In particular, Section 3.7 introduced a method called weight decay,\nwhich consists of adding a regularization term to the loss function in order to penalize large\nvalues of the weights. Depending on which weight norm is penalized this technique is known\neither as ridge regularization (for ℓ2 penalty) or lasso regularization (for an ℓ1 penalty). In the\nclassical analysis of these regularizers, they are considered as suﬃciently restrictive on the\nvalues that the weights can take to prevent the model from ﬁtting arbitrary labels.\nIn deep learning implementations, weight decay remains a popular tool. However, researchers\nhave noted that typical strengths of ℓ2 regularization are insuﬃcient to prevent the networks\nfrom interpolating the data (Zhang et al., 2021) and thus the beneﬁts if interpreted as reg-\nularization might only make sense in combination with the early stopping criterion. Absent\nearly stopping, it is possible that just like the number of layers or number of nodes (in deep\nlearning) or the distance metric (in 1-nearest neighbor), these methods may lead to better\ngeneralization not because they meaningfully constrain the power of the neural network but\nrather because they somehow encode inductive biases that are better compatible with the\npatterns found in datasets of interests. Thus, classical regularizers remain popular in deep\nlearning implementations, even if the theoretical rationale for their eﬃcacy may be radically\ndiﬀerent.\nNotably, deep learning researchers have also built on techniques ﬁrst popularized in classical\nregularization contexts, such as adding noise to model inputs. In the next section we will\nintroduce the famous dropout technique (invented by Srivastava et al. (2014)), which has\nbecome a mainstay of deep learning, even as the theoretical basis for its eﬃcacy remains\nsimilarly mysterious.\n\n198\nMultilayer Perceptrons\n106\n5.5.5 Summary\nUnlike classical linear models, which tend to have fewer parameters than examples, deep\nnetworks tend to be over-parametrized, and for most tasks are capable of perfectly ﬁtting the\ntraining set. This interpolation regime challenges many hard fast-held intuitions. Functionally,\nneural networks look like parametric models. But thinking of them as nonparametric models\ncan sometimes be a more reliable source of intuition. Because it is often the case that all deep\nnetworks under consideration are capable of ﬁtting all of the training labels, nearly all gains\nmust come by mitigating overﬁtting (closing the generalization gap). Paradoxically, the inter-\nventions that reduce the generalization gap sometimes appear to increase model complexity\nand at other times appear to decrease complexity. However, these methods seldom decrease\ncomplexity suﬃciently for classical theory to explain the generalization of deep networks,\nand why certain choices lead to improved generalization remains for the most part a massive\nopen question despite the concerted eﬀorts of many brilliant researchers.\n5.5.6 Exercises\n1. In what sense do traditional complexity-based measures fail to account for generalization\nof deep neural networks?\n2. Why might early stopping be considered a regularization technique?\n3. How do researchers typically determine the stopping criterion?\n4. What important factor seems to diﬀerentiate cases when early stopping leads to big im-\nprovements in generalization?\n5. Beyond generalization, describe another beneﬁt of early stopping.\nDiscussions106.\n5.6 Dropout\nLet’s think brieﬂy about what we expect from a good predictive model. We want it to peform\nwell on unseen data. Classical generalization theory suggests that to close the gap between\ntrain and test performance, we should aim for a simple model. Simplicity can come in the\nform of a small number of dimensions. We explored this when discussing the monomial basis\nfunctions of linear models in Section 3.6. Additionally, as we saw when discussing weight\ndecay (ℓ2 regularization) in Section 3.7, the (inverse) norm of the parameters also represents\na useful measure of simplicity. Another useful notion of simplicity is smoothness, i.e., that\nthe function should not be sensitive to small changes to its inputs. For instance, when we\nclassify images, we would expect that adding some random noise to the pixels should be\nmostly harmless.\n\n199\nDropout\nBishop (1995) formalized this idea when he proved that training with input noise is equiv-\nalent to Tikhonov regularization. This work drew a clear mathematical connection between\nthe requirement that a function be smooth (and thus simple), and the requirement that it be\nresilient to perturbations in the input.\nThen, Srivastava et al. (2014) developed a clever idea for how to apply Bishop’s idea to the\ninternal layers of a network, too. Their idea, called dropout, involves injecting noise while\ncomputing each internal layer during forward propagation, and it has become a standard tech-\nnique for training neural networks. The method is called dropout because we literally drop out\nsome neurons during training. Throughout training, on each iteration, standard dropout con-\nsists of zeroing out some fraction of the nodes in each layer before calculating the subsequent\nlayer.\nTo be clear, we are imposing our own narrative with the link to Bishop. The original paper\non dropout oﬀers intuition through a surprising analogy to sexual reproduction. The authors\nargue that neural network overﬁtting is characterized by a state in which each layer relies\non a speciﬁc pattern of activations in the previous layer, calling this condition co-adaptation.\nDropout, they claim, breaks up co-adaptation just as sexual reproduction is argued to break\nup co-adapted genes. While such an justiﬁcation of this theory is certainly up for debate, the\ndropout technique itself has proved enduring, and various forms of dropout are implemented\nin most deep learning libraries.\nThe key challenge is how to inject this noise. One idea is to inject it in an unbiased manner\nso that the expected value of each layer—while ﬁxing the others—equals the value it would\nhave taken absent noise. In Bishop’s work, he added Gaussian noise to the inputs to a linear\nmodel. At each training iteration, he added noise sampled from a distribution with mean\nzero ϵ ∼N(0, σ2) to the input x, yielding a perturbed point x′ = x + ϵ. In expectation,\nE[x′] = x.\nIn standard dropout regularization, one zeros out some fraction of the nodes in each layer\nand then debiases each layer by normalizing by the fraction of nodes that were retained (not\ndropped out). In other words, with dropout probability p, each intermediate activation h is\nreplaced by a random variable h′ as follows:\nh′ =\n{\n0\nwith probability p\nh\n1−p\notherwise\n(5.6.1)\nBy design, the expectation remains unchanged, i.e., E[h′] = h.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n5.6.1 Dropout in Practice\nRecall the MLP with a hidden layer and ﬁve hidden units from Fig. 5.1.1. When we apply\ndropout to a hidden layer, zeroing out each hidden unit with probability p, the result can\n\n200\nMultilayer Perceptrons\nbe viewed as a network containing only a subset of the original neurons. In Fig. 5.6.1, h2\nand h5 are removed. Consequently, the calculation of the outputs no longer depends on h2\nor h5 and their respective gradient also vanishes when performing backpropagation. In this\nway, the calculation of the output layer cannot be overly dependent on any one element of\nh1, . . ., h5.\nt\nFig. 5.6.1\nMLP before and after dropout.\nTypically, we disable dropout at test time. Given a trained model and a new example, we do\nnot drop out any nodes and thus do not need to normalize. However, there are some excep-\ntions: some researchers use dropout at test time as a heuristic for estimating the uncertainty\nof neural network predictions: if the predictions agree across many diﬀerent dropout outputs,\nthen we might say that the network is more conﬁdent.\n5.6.2 Implementation from Scratch\nTo implement the dropout function for a single layer, we must draw as many samples from a\nBernoulli (binary) random variable as our layer has dimensions, where the random variable\ntakes value 1 (keep) with probability 1 −p and 0 (drop) with probability p. One easy way\nto implement this is to ﬁrst draw samples from the uniform distribution U[0, 1]. Then we\ncan keep those nodes for which the corresponding sample is greater than p, dropping the\nrest.\nIn the following code, we implement a dropout_layer function that drops out the elements\nin the tensor input X with probability dropout, rescaling the remainder as described above:\ndividing the survivors by 1.0-dropout.\ndef dropout_layer(X, dropout):\nassert 0 <= dropout <= 1\nif dropout == 1: return torch.zeros_like(X)\nmask = (torch.rand(X.shape) > dropout).float()\nreturn mask * X / (1.0 - dropout)\nWe can test out the dropout_layer function on a few examples. In the following lines of\ncode, we pass our input X through the dropout operation, with probabilities 0, 0.5, and 1,\nrespectively.\n\n201\nDropout\nX = torch.arange(16, dtype = torch.float32).reshape((2, 8))\nprint('dropout_p = 0:', dropout_layer(X, 0))\nprint('dropout_p = 0.5:', dropout_layer(X, 0.5))\nprint('dropout_p = 1:', dropout_layer(X, 1))\ndropout_p = 0: tensor([[ 0.,\n1.,\n2.,\n3.,\n4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11., 12., 13., 14., 15.]])\ndropout_p = 0.5: tensor([[ 0.,\n2.,\n4.,\n0.,\n8., 10., 12.,\n0.],\n[16., 18., 20., 22.,\n0.,\n0.,\n0., 30.]])\ndropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0., 0., 0., 0.]])\nDeﬁning the Model\nThe model below applies dropout to the output of each hidden layer (following the activation\nfunction). We can set dropout probabilities for each layer separately. A common choice is to\nset a lower dropout probability closer to the input layer. We ensure that dropout is only active\nduring training.\nclass DropoutMLPScratch(d2l.Classifier):\ndef __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper().__init__()\nself.save_hyperparameters()\nself.lin1 = nn.LazyLinear(num_hiddens_1)\nself.lin2 = nn.LazyLinear(num_hiddens_2)\nself.lin3 = nn.LazyLinear(num_outputs)\nself.relu = nn.ReLU()\ndef forward(self, X):\nH1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\nif self.training:\nH1 = dropout_layer(H1, self.dropout_1)\nH2 = self.relu(self.lin2(H1))\nif self.training:\nH2 = dropout_layer(H2, self.dropout_2)\nreturn self.lin3(H2)\nTraining\nThe following is similar to the training of MLPs described previously.\nhparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\nmodel = DropoutMLPScratch(**hparams)\ndata = d2l.FashionMNIST(batch_size=256)\n(continues on next page)\n\n202\nMultilayer Perceptrons\n(continued from previous page)\ntrainer = d2l.Trainer(max_epochs=10)\ntrainer.fit(model, data)\n5.6.3 Concise Implementation\nWith high-level APIs, all we need to do is add a Dropout layer after each fully connected\nlayer, passing in the dropout probability as the only argument to its constructor. During train-\ning, the Dropout layer will randomly drop out outputs of the previous layer (or equivalently,\nthe inputs to the subsequent layer) according to the speciﬁed dropout probability. When not\nin training mode, the Dropout layer simply passes the data through during testing.\nclass DropoutMLP(d2l.Classifier):\ndef __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(\nnn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),\nnn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),\nnn.Dropout(dropout_2), nn.LazyLinear(num_outputs))\nNext, we train the model.\nmodel = DropoutMLP(**hparams)\ntrainer.fit(model, data)\n5.6.4 Summary\nBeyond controlling the number of dimensions and the size of the weight vector, dropout is\nyet another tool for avoiding overﬁtting. Often tools are used jointly. Note that dropout is\nused only during training: it replaces an activation h with a random variable with expected\nvalue h.\n\n203\nDropout\n107\n5.6.5 Exercises\n1. What happens if you change the dropout probabilities for the ﬁrst and second layers? In\nparticular, what happens if you switch the ones for both layers? Design an experiment to\nanswer these questions, describe your results quantitatively, and summarize the qualitative\ntakeaways.\n2. Increase the number of epochs and compare the results obtained when using dropout with\nthose when not using it.\n3. What is the variance of the activations in each hidden layer when dropout is and is not\napplied? Draw a plot to show how this quantity evolves over time for both models.\n4. Why is dropout not typically used at test time?\n5. Using the model in this section as an example, compare the eﬀects of using dropout and\nweight decay. What happens when dropout and weight decay are used at the same time?\nAre the results additive? Are there diminished returns (or worse)? Do they cancel each\nother out?\n6. What happens if we apply dropout to the individual weights of the weight matrix rather\nthan the activations?\n7. Invent another technique for injecting random noise at each layer that is diﬀerent from the\nstandard dropout technique. Can you develop a method that outperforms dropout on the\nFashion-MNIST dataset (for a ﬁxed architecture)?\nDiscussions107.\n\n204\nMultilayer Perceptrons\n108\n109\n5.7 Predicting House Prices on Kaggle\nNow that we have introduced some basic tools for building and training deep networks and\nregularizing them with techniques including weight decay and dropout, we are ready to put\nall this knowledge into practice by participating in a Kaggle competition. The house price\nprediction competition is a great place to start. The data is fairly generic and do not exhibit\nexotic structure that might require specialized models (as audio or video might). This dataset,\ncollected by De Cock (2011), covers house prices in Ames, Iowa from the period 2006–2010.\nIt is considerably larger than the famous Boston housing dataset108 of Harrison and Rubinfeld\n(1978), boasting both more examples and more features.\nIn this section, we will walk you through details of data preprocessing, model design, and\nhyperparameter selection. We hope that through a hands-on approach, you will gain some\nintuitions that will guide you in your career as a data scientist.\n%matplotlib inline\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n5.7.1 Downloading Data\nThroughout the book, we will train and test models on various downloaded datasets. Here,\nwe implement two utility functions for downloading and extracting zip or tar ﬁles. Again, we\nskip implementation details of such utility functions.\ndef download(url, folder, sha1_hash=None):\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\ndef extract(filename, folder):\n\"\"\"Extract a zip/tar file into folder.\"\"\"\n5.7.2 Kaggle\nKaggle109 is a popular platform that hosts machine learning competitions. Each competition\ncenters on a dataset and many are sponsored by stakeholders who oﬀer prizes to the win-\nning solutions. The platform helps users to interact via forums and shared code, fostering\nboth collaboration and competition. While leaderboard chasing often spirals out of control,\nwith researchers focusing myopically on preprocessing steps rather than asking fundamental\nquestions, there is also tremendous value in the objectivity of a platform that facilitates direct\n\n205\nPredicting House Prices on Kaggle\nquantitative comparisons among competing approaches as well as code sharing so that every-\none can learn what did and did not work. If you want to participate in a Kaggle competition,\nyou will ﬁrst need to register for an account (see Fig. 5.7.1).\nt\nFig. 5.7.1\nThe Kaggle website.\nOn the house price prediction competition page, as illustrated in Fig. 5.7.2, you can ﬁnd the\ndataset (under the “Data” tab), submit predictions, and see your ranking, The URL is right\nhere:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\nt\nFig. 5.7.2\nThe house price prediction competition page.\n5.7.3 Accessing and Reading the Dataset\nNote that the competition data is separated into training and test sets. Each record includes the\nproperty value of the house and attributes such as street type, year of construction, roof type,\nbasement condition, etc. The features consist of various data types. For example, the year of\nconstruction is represented by an integer, the roof type by discrete categorical assignments,\nand other features by ﬂoating point numbers. And here is where reality complicates things:\n\n206\nMultilayer Perceptrons\nfor some examples, some data is altogether missing with the missing value marked simply as\n“na”. The price of each house is included for the training set only (it is a competition after\nall). We will want to partition the training set to create a validation set, but we only get to\nevaluate our models on the oﬃcial test set after uploading predictions to Kaggle. The “Data”\ntab on the competition tab in Fig. 5.7.2 has links for downloading the data.\nTo get started, we will read in and process the data using pandas, which we introduced\nin Section 2.2. For convenience, we can download and cache the Kaggle housing dataset.\nIf a ﬁle corresponding to this dataset already exists in the cache directory and its SHA-1\nmatches sha1_hash, our code will use the cached ﬁle to avoid clogging up your Internet with\nredundant downloads.\nclass KaggleHouse(d2l.DataModule):\ndef __init__(self, batch_size, train=None, val=None):\nsuper().__init__()\nself.save_hyperparameters()\nif self.train is None:\nself.raw_train = pd.read_csv(d2l.download(\nd2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,\nsha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))\nself.raw_val = pd.read_csv(d2l.download(\nd2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,\nsha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))\nThe training dataset includes 1460 examples, 80 features, and one label, while the validation\ndata contains 1459 examples and 80 features.\ndata = KaggleHouse(batch_size=64)\nprint(data.raw_train.shape)\nprint(data.raw_val.shape)\n(1460, 81)\n(1459, 80)\n5.7.4 Data Preprocessing\nLet’s take a look at the ﬁrst four and ﬁnal two features as well as the label (SalePrice) from\nthe ﬁrst four examples.\nprint(data.raw_train.iloc[:4, [0, 1, 2, 3, -3, -2, -1]])\nId\nMSSubClass MSZoning\nLotFrontage SaleType SaleCondition\nSalePrice\n0\n1\n60\nRL\n65.0\nWD\nNormal\n208500\n1\n2\n20\nRL\n80.0\nWD\nNormal\n181500\n2\n3\n60\nRL\n68.0\nWD\nNormal\n223500\n3\n4\n70\nRL\n60.0\nWD\nAbnorml\n140000\n\n207\nPredicting House Prices on Kaggle\nWe can see that in each example, the ﬁrst feature is the identiﬁer. This helps the model\ndetermine each training example. While this is convenient, it does not carry any information\nfor prediction purposes. Hence, we will remove it from the dataset before feeding the data\ninto the model. Furthermore, given a wide variety of data types, we will need to preprocess\nthe data before we can start modeling.\nLet’s start with the numerical features. First, we apply a heuristic, replacing all missing val-\nues by the corresponding feature’s mean. Then, to put all features on a common scale, we\nstandardize the data by rescaling features to zero mean and unit variance:\nx ←x −µ\nσ\n,\n(5.7.1)\nwhere µ and σ denote mean and standard deviation, respectively. To verify that this in-\ndeed transforms our feature (variable) such that it has zero mean and unit variance, note that\nE[ x−µ\nσ ] = µ−µ\nσ\n= 0 and that E[(x −µ)2] = (σ2 + µ2) −2µ2 + µ2 = σ2. Intuitively,\nwe standardize the data for two reasons. First, it proves convenient for optimization. Second,\nbecause we do not know a priori which features will be relevant, we do not want to penalize\ncoeﬃcients assigned to one feature more than any other.\nNext we deal with discrete values. These include features such as “MSZoning”. We replace\nthem by a one-hot encoding in the same way that we earlier transformed multiclass labels\ninto vectors (see Section 4.1.1). For instance, “MSZoning” assumes the values “RL” and\n“RM”. Dropping the “MSZoning” feature, two new indicator features “MSZoning_RL” and\n“MSZoning_RM” are created with values being either 0 or 1. According to one-hot encod-\ning, if the original value of “MSZoning” is “RL”, then “MSZoning_RL” is 1 and “MSZon-\ning_RM” is 0. The pandas package does this automatically for us.\n@d2l.add_to_class(KaggleHouse)\ndef preprocess(self):\n# Remove the ID and label columns\nlabel = 'SalePrice'\nfeatures = pd.concat(\n(self.raw_train.drop(columns=['Id', label]),\nself.raw_val.drop(columns=['Id'])))\n# Standardize numerical columns\nnumeric_features = features.dtypes[features.dtypes!='object'].index\nfeatures[numeric_features] = features[numeric_features].apply(\nlambda x: (x - x.mean()) / (x.std()))\n# Replace NAN numerical features by 0\nfeatures[numeric_features] = features[numeric_features].fillna(0)\n# Replace discrete features by one-hot encoding\nfeatures = pd.get_dummies(features, dummy_na=True)\n# Save preprocessed features\nself.train = features[:self.raw_train.shape[0]].copy()\nself.train[label] = self.raw_train[label]\nself.val = features[self.raw_train.shape[0]:].copy()\nYou can see that this conversion increases the number of features from 79 to 331 (excluding\nID and label columns).\n\n208\nMultilayer Perceptrons\ndata.preprocess()\ndata.train.shape\n(1460, 331)\n5.7.5 Error Measure\nTo get started we will train a linear model with squared loss. Not surprisingly, our linear\nmodel will not lead to a competition-winning submission but it does provide a sanity check\nto see whether there is meaningful information in the data. If we cannot do better than random\nguessing here, then there might be a good chance that we have a data processing bug. And\nif things work, the linear model will serve as a baseline giving us some intuition about how\nclose the simple model gets to the best reported models, giving us a sense of how much gain\nwe should expect from fancier models.\nWith house prices, as with stock prices, we care about relative quantities more than absolute\nquantities. Thus we tend to care more about the relative error y−ˆy\ny\nthan about the absolute\nerror y −ˆy. For instance, if our prediction is oﬀby $100,000 when estimating the price of\na house in rural Ohio, where the value of a typical house is $125,000, then we are probably\ndoing a horrible job. On the other hand, if we err by this amount in Los Altos Hills, California,\nthis might represent a stunningly accurate prediction (there, the median house price exceeds\n$4 million).\nOne way to address this problem is to measure the discrepancy in the logarithm of the price\nestimates. In fact, this is also the oﬃcial error measure used by the competition to evaluate\nthe quality of submissions. After all, a small value δ for | log y −log ˆy| ≤δ translates into\ne−δ ≤ˆy\ny ≤eδ. This leads to the following root-mean-squared-error between the logarithm\nof the predicted price and the logarithm of the label price:\nv\nt\n1\nn\nn\n∑\ni=1\n(log yi −log ˆyi)2.\n(5.7.2)\n@d2l.add_to_class(KaggleHouse)\ndef get_dataloader(self, train):\nlabel = 'SalePrice'\ndata = self.train if train else self.val\nif label not in data: return\nget_tensor = lambda x: torch.tensor(x.values.astype(float),\ndtype=torch.float32)\n# Logarithm of prices\ntensors = (get_tensor(data.drop(columns=[label])),\n# X\ntorch.log(get_tensor(data[label])).reshape((-1, 1)))\n# Y\nreturn self.get_tensorloader(tensors, train)\n\n209\nPredicting House Prices on Kaggle\n5.7.6 K-Fold Cross-Validation\nYou might recall that we introduced cross-validation in Section 3.6.3, where we discussed\nhow to deal with model selection. We will put this to good use to select the model design and\nto adjust the hyperparameters. We ﬁrst need a function that returns the ith fold of the data in a\nK-fold cross-validation procedure. It proceeds by slicing out the ith segment as validation data\nand returning the rest as training data. Note that this is not the most eﬃcient way of handling\ndata and we would deﬁnitely do something much smarter if our dataset was considerably\nlarger. But this added complexity might obfuscate our code unnecessarily so we can safely\nomit it here owing to the simplicity of our problem.\ndef k_fold_data(data, k):\nrets = []\nfold_size = data.train.shape[0] // k\nfor j in range(k):\nidx = range(j * fold_size, (j+1) * fold_size)\nrets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),\ndata.train.loc[idx]))\nreturn rets\nThe average validation error is returned when we train K times in the K-fold cross-validation.\ndef k_fold(trainer, data, k, lr):\nval_loss, models = [], []\nfor i, data_fold in enumerate(k_fold_data(data, k)):\nmodel = d2l.LinearRegression(lr)\nmodel.board.yscale='log'\nif i != 0: model.board.display = False\ntrainer.fit(model, data_fold)\nval_loss.append(float(model.board.data['val_loss'][-1].y))\nmodels.append(model)\nprint(f'average validation log mse = {sum(val_loss)/len(val_loss)}')\nreturn models\n5.7.7 Model Selection\nIn this example, we pick an untuned set of hyperparameters and leave it up to the reader to\nimprove the model. Finding a good choice can take time, depending on how many variables\none optimizes over. With a large enough dataset, and the normal sorts of hyperparameters,\nK-fold cross-validation tends to be reasonably resilient against multiple testing. However, if\nwe try an unreasonably large number of options we might ﬁnd that our validation performance\nis no longer representative of the true error.\ntrainer = d2l.Trainer(max_epochs=10)\nmodels = k_fold(trainer, data, k=5, lr=0.01)\n\n210\nMultilayer Perceptrons\naverage validation log mse = 0.18610747516155243\nNotice that sometimes the number of training errors for a set of hyperparameters can be very\nlow, even as the number of errors on K-fold cross-validation grows considerably higher. This\nindicates that we are overﬁtting. Throughout training you will want to monitor both numbers.\nLess overﬁtting might indicate that our data can support a more powerful model. Massive\noverﬁtting might suggest that we can gain by incorporating regularization techniques.\n5.7.8 Submitting Predictions on Kaggle\nNow that we know what a good choice of hyperparameters should be, we might calculate the\naverage predictions on the test set by all the K models. Saving the predictions in a csv ﬁle\nwill simplify uploading the results to Kaggle. The following code will generate a ﬁle called\nsubmission.csv.\npreds = [model(torch.tensor(data.val.values.astype(float), dtype=torch.\n,→float32))\nfor model in models]\n# Taking exponentiation of predictions in the logarithm scale\nensemble_preds = torch.exp(torch.cat(preds, 1)).mean(1)\nsubmission = pd.DataFrame({'Id':data.raw_val.Id,\n'SalePrice':ensemble_preds.detach().numpy()})\nsubmission.to_csv('submission.csv', index=False)\nNext, as demonstrated in Fig. 5.7.3, we can submit our predictions on Kaggle and see how\nthey compare with the actual house prices (labels) on the test set. The steps are quite sim-\nple:\n• Log in to the Kaggle website and visit the house price prediction competition page.\n• Click the “Submit Predictions” or “Late Submission” button.\n• Click the “Upload Submission File” button in the dashed box at the bottom of the page\nand select the prediction ﬁle you wish to upload.\n\n211\nPredicting House Prices on Kaggle\n110\n• Click the “Make Submission” button at the bottom of the page to view your results.\nt\nFig. 5.7.3\nSubmitting data to Kaggle\n5.7.9 Summary and Discussion\nReal data often contains a mix of diﬀerent data types and needs to be preprocessed. Rescaling\nreal-valued data to zero mean and unit variance is a good default. So is replacing missing\nvalues with their mean. Furthermore, transforming categorical features into indicator features\nallows us to treat them like one-hot vectors. When we tend to care more about the relative\nerror than about the absolute error, we can measure the discrepancy in the logarithm of the\nprediction. To select the model and adjust the hyperparameters, we can use K-fold cross-\nvalidation .\n5.7.10 Exercises\n1. Submit your predictions for this section to Kaggle. How good are they?\n2. Is it always a good idea to replace missing values by a mean? Hint: can you construct a\nsituation where the values are not missing at random?\n3. Improve the score by tuning the hyperparameters through K-fold cross-validation.\n4. Improve the score by improving the model (e.g., layers, weight decay, and dropout).\n5. What happens if we do not standardize the continuous numerical features as we have done\nin this section?\nDiscussions110.\n\n6\nBuilders’ Guide\nAlongside giant datasets and powerful hardware, great software tools have played an indis-\npensable role in the rapid progress of deep learning. Starting with the pathbreaking Theano\nlibrary released in 2007, ﬂexible open-source tools have enabled researchers to rapidly proto-\ntype models, avoiding repetitive work when recycling standard components while still main-\ntaining the ability to make low-level modiﬁcations. Over time, deep learning’s libraries have\nevolved to oﬀer increasingly coarse abstractions. Just as semiconductor designers went from\nspecifying transistors to logical circuits to writing code, neural networks researchers have\nmoved from thinking about the behavior of individual artiﬁcial neurons to conceiving of net-\nworks in terms of whole layers, and now often design architectures with far coarser blocks in\nmind.\nSo far, we have introduced some basic machine learning concepts, ramping up to fully-\nfunctional deep learning models. In the last chapter, we implemented each component of\nan MLP from scratch and even showed how to leverage high-level APIs to roll out the same\nmodels eﬀortlessly. To get you that far that fast, we called upon the libraries, but skipped\nover more advanced details about how they work. In this chapter, we will peel back the cur-\ntain, digging deeper into the key components of deep learning computation, namely model\nconstruction, parameter access and initialization, designing custom layers and blocks, read-\ning and writing models to disk, and leveraging GPUs to achieve dramatic speedups. These\ninsights will move you from end user to power user, giving you the tools needed to reap the\nbeneﬁts of a mature deep learning library while retaining the ﬂexibility to implement more\ncomplex models, including those you invent yourself! While this chapter does not introduce\nany new models or datasets, the advanced modeling chapters that follow rely heavily on these\ntechniques.\n6.1 Layers and Modules\nWhen we ﬁrst introduced neural networks, we focused on linear models with a single output.\nHere, the entire model consists of just a single neuron. Note that a single neuron (i) takes some\nset of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of associated\nparameters that can be updated to optimize some objective function of interest. Then, once\nwe started thinking about networks with multiple outputs, we leveraged vectorized arithmetic\n212\n\n213\nLayers and Modules\nto characterize an entire layer of neurons. Just like individual neurons, layers (i) take a set\nof inputs, (ii) generate corresponding outputs, and (iii) are described by a set of tunable\nparameters. When we worked through softmax regression, a single layer was itself the model.\nHowever, even when we subsequently introduced MLPs, we could still think of the model as\nretaining this same basic structure.\nInterestingly, for MLPs, both the entire model and its constituent layers share this structure.\nThe entire model takes in raw inputs (the features), generates outputs (the predictions), and\npossesses parameters (the combined parameters from all constituent layers). Likewise, each\nindividual layer ingests inputs (supplied by the previous layer) generates outputs (the inputs\nto the subsequent layer), and possesses a set of tunable parameters that are updated according\nto the signal that ﬂows backwards from the subsequent layer.\nWhile you might think that neurons, layers, and models give us enough abstractions to go\nabout our business, it turns out that we often ﬁnd it convenient to speak about components that\nare larger than an individual layer but smaller than the entire model. For example, the ResNet-\n152 architecture, which is wildly popular in computer vision, possesses hundreds of layers.\nThese layers consist of repeating patterns of groups of layers. Implementing such a network\none layer at a time can grow tedious. This concern is not just hypothetical—such design\npatterns are common in practice. The ResNet architecture mentioned above won the 2015\nImageNet and COCO computer vision competitions for both recognition and detection (He\net al., 2016) and remains a go-to architecture for many vision tasks. Similar architectures in\nwhich layers are arranged in various repeating patterns are now ubiquitous in other domains,\nincluding natural language processing and speech.\nTo implement these complex networks, we introduce the concept of a neural network mod-\nule. A module could describe a single layer, a component consisting of multiple layers, or\nthe entire model itself! One beneﬁt of working with the module abstraction is that they can\nbe combined into larger artifacts, often recursively. This is illustrated in Fig. 6.1.1. By deﬁn-\ning code to generate modules of arbitrary complexity on demand, we can write surprisingly\ncompact code and still implement complex neural networks.\nt\nFig. 6.1.1\nMultiple layers are combined into modules, forming repeating patterns of larger models.\nFrom a programming standpoint, a module is represented by a class. Any subclass of it must\n\n214\nBuilders’ Guide\ndeﬁne a forward propagation method that transforms its input into output and must store any\nnecessary parameters. Note that some modules do not require any parameters at all. Finally a\nmodule must possess a backpropagation method, for purposes of calculating gradients. Fortu-\nnately, due to some behind-the-scenes magic supplied by the auto diﬀerentiation (introduced\nin Section 2.5) when deﬁning our own module, we only need to worry about parameters and\nthe forward propagation method.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nTo begin, we revisit the code that we used to implement MLPs (Section 5.1). The following\ncode generates a network with one fully connected hidden layer with 256 units and ReLU\nactivation, followed by a fully connected output layer with ten units (no activation func-\ntion).\nnet = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\nX = torch.rand(2, 20)\nnet(X).shape\ntorch.Size([2, 10])\nIn this example, we constructed our model by instantiating an nn.Sequential, with layers in\nthe order that they should be executed passed as arguments. In short, nn.Sequential deﬁnes\na special kind of Module, the class that presents a module in PyTorch. It maintains an ordered\nlist of constituent Modules. Note that each of the two fully connected layers is an instance of\nthe Linear class which is itself a subclass of Module. The forward propagation (forward)\nmethod is also remarkably simple: it chains each module in the list together, passing the\noutput of each as input to the next. Note that until now, we have been invoking our models\nvia the construction net(X) to obtain their outputs. This is actually just shorthand for net.\n__call__(X).\n6.1.1 A Custom Module\nPerhaps the easiest way to develop intuition about how a module works is to implement one\nourselves. Before we do that, we brieﬂy summarize the basic functionality that each module\nmust provide:\n1. Ingest input data as arguments to its forward propagation method.\n2. Generate an output by having the forward propagation method return a value. Note that\nthe output may have a diﬀerent shape from the input. For example, the ﬁrst fully connected\nlayer in our model above ingests an input of arbitrary dimension but returns an output of\ndimension 256.\n\n215\nLayers and Modules\n3. Calculate the gradient of its output with respect to its input, which can be accessed via its\nbackpropagation method. Typically this happens automatically.\n4. Store and provide access to those parameters necessary for executing the forward propa-\ngation computation.\n5. Initialize model parameters as needed.\nIn the following snippet, we code up a module from scratch corresponding to an MLP with\none hidden layer with 256 hidden units, and a 10-dimensional output layer. Note that the MLP\nclass below inherits the class that represents a module. We will heavily rely on the parent\nclass’s methods, supplying only our own constructor (the __init__ method in Python) and\nthe forward propagation method.\nclass MLP(nn.Module):\ndef __init__(self):\n# Call the constructor of the parent class nn.Module to perform\n# the necessary initialization\nsuper().__init__()\nself.hidden = nn.LazyLinear(256)\nself.out = nn.LazyLinear(10)\n# Define the forward propagation of the model, that is, how to return the\n# required model output based on the input X\ndef forward(self, X):\nreturn self.out(F.relu(self.hidden(X)))\nLet’s ﬁrst focus on the forward propagation method. Note that it takes X as input, calculates\nthe hidden representation with the activation function applied, and outputs its logits. In this\nMLP implementation, both layers are instance variables. To see why this is reasonable, imagine\ninstantiating two MLPs, net1 and net2, and training them on diﬀerent data. Naturally, we\nwould expect them to represent two diﬀerent learned models.\nWe instantiate the MLP’s layers in the constructor and subsequently invoke these layers on\neach call to the forward propagation method. Note a few key details. First, our customized\n__init__ method invokes the parent class’s __init__ method via super().__init__()\nsparing us the pain of restating boilerplate code applicable to most modules. We then instan-\ntiate our two fully connected layers, assigning them to self.hidden and self.out. Note\nthat unless we implement a new layer, we need not worry about the backpropagation method\nor parameter initialization. The system will generate these methods automatically. Let’s try\nthis out.\nnet = MLP()\nnet(X).shape\ntorch.Size([2, 10])\nA key virtue of the module abstraction is its versatility. We can subclass a module to create\nlayers (such as the fully connected layer class), entire models (such as the MLP class above),\n\n216\nBuilders’ Guide\nor various components of intermediate complexity. We exploit this versatility throughout the\ncoming chapters, such as when addressing convolutional neural networks.\n6.1.2 The Sequential Module\nWe can now take a closer look at how the Sequential class works. Recall that Sequential\nwas designed to daisy-chain other modules together. To build our own simpliﬁed MySequen-\ntial, we just need to deﬁne two key methods:\n1. A method for appending modules one by one to a list.\n2. A forward propagation method for passing an input through the chain of modules, in the\nsame order as they were appended.\nThe following MySequential class delivers the same functionality of the default Sequential\nclass.\nclass MySequential(nn.Module):\ndef __init__(self, *args):\nsuper().__init__()\nfor idx, module in enumerate(args):\nself.add_module(str(idx), module)\ndef forward(self, X):\nfor module in self.children():\nX = module(X)\nreturn X\nIn the __init__ method, we add every module by calling the add_modules method. These\nmodules can be accessed by the children method at a later date. In this way the system\nknows the added modules, and it will properly initialize each module’s parameters.\nWhen our MySequential’s forward propagation method is invoked, each added module is\nexecuted in the order in which they were added. We can now reimplement an MLP using our\nMySequential class.\nnet = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\nnet(X).shape\ntorch.Size([2, 10])\nNote that this use of MySequential is identical to the code we previously wrote for the\nSequential class (as described in Section 5.1).\n6.1.3 Executing Code in the Forward Propagation Method\nThe Sequential class makes model construction easy, allowing us to assemble new archi-\ntectures without having to deﬁne our own class. However, not all architectures are simple\n\n217\nLayers and Modules\ndaisy chains. When greater ﬂexibility is required, we will want to deﬁne our own blocks. For\nexample, we might want to execute Python’s control ﬂow within the forward propagation\nmethod. Moreover, we might want to perform arbitrary mathematical operations, not simply\nrelying on predeﬁned neural network layers.\nYou may have noticed that until now, all of the operations in our networks have acted upon\nour network’s activations and its parameters. Sometimes, however, we might want to incor-\nporate terms that are neither the result of previous layers nor updatable parameters. We call\nthese constant parameters. Say for example that we want a layer that calculates the function\nf (x, w) = c · w⊤x, where x is the input, w is our parameter, and c is some speciﬁed con-\nstant that is not updated during optimization. So we implement a FixedHiddenMLP class as\nfollows.\nclass FixedHiddenMLP(nn.Module):\ndef __init__(self):\nsuper().__init__()\n# Random weight parameters that will not compute gradients and\n# therefore keep constant during training\nself.rand_weight = torch.rand((20, 20))\nself.linear = nn.LazyLinear(20)\ndef forward(self, X):\nX = self.linear(X)\nX = F.relu(X @ self.rand_weight + 1)\n# Reuse the fully connected layer. This is equivalent to sharing\n# parameters with two fully connected layers\nX = self.linear(X)\n# Control flow\nwhile X.abs().sum() > 1:\nX /= 2\nreturn X.sum()\nIn this model, we implement a hidden layer whose weights (self.rand_weight) are initial-\nized randomly at instantiation and are thereafter constant. This weight is not a model param-\neter and thus it is never updated by backpropagation. The network then passes the output of\nthis “ﬁxed” layer through a fully connected layer.\nNote that before returning the output, our model did something unusual. We ran a while-loop,\ntesting on the condition its ℓ1 norm is larger than 1, and dividing our output vector by 2 until\nit satisﬁed the condition. Finally, we returned the sum of the entries in X. To our knowledge,\nno standard neural network performs this operation. Note that this particular operation may\nnot be useful in any real-world task. Our point is only to show you how to integrate arbitrary\ncode into the ﬂow of your neural network computations.\nnet = FixedHiddenMLP()\nnet(X)\ntensor(-0.1635, grad_fn=<SumBackward0>)\n\n218\nBuilders’ Guide\n111\nWe can mix and match various ways of assembling modules together. In the following ex-\nample, we nest modules in some creative ways.\nclass NestMLP(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\nnn.LazyLinear(32), nn.ReLU())\nself.linear = nn.LazyLinear(16)\ndef forward(self, X):\nreturn self.linear(self.net(X))\nchimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\nchimera(X)\ntensor(0.2447, grad_fn=<SumBackward0>)\n6.1.4 Summary\nIndividual layers can be modules. Many layers can comprise a module. Many modules can\ncomprise a module.\nA module can contain code. Modules take care of lots of housekeeping, including parame-\nter initialization and backpropagation. Sequential concatenations of layers and modules are\nhandled by the Sequential module.\n6.1.5 Exercises\n1. What kinds of problems will occur if you change MySequential to store modules in a\nPython list?\n2. Implement a module that takes two modules as an argument, say net1 and net2 and\nreturns the concatenated output of both networks in the forward propagation. This is also\ncalled a parallel module.\n3. Assume that you want to concatenate multiple instances of the same network. Implement\na factory function that generates multiple instances of the same module and build a larger\nnetwork from it.\nDiscussions111.\n\n219\nParameter Management\n6.2 Parameter Management\nOnce we have chosen an architecture and set our hyperparameters, we proceed to the train-\ning loop, where our goal is to ﬁnd parameter values that minimize our loss function. After\ntraining, we will need these parameters in order to make future predictions. Additionally, we\nwill sometimes wish to extract the parameters perhaps to reuse them in some other context,\nto save our model to disk so that it may be executed in other software, or for examination in\nthe hope of gaining scientiﬁc understanding.\nMost of the time, we will be able to ignore the nitty-gritty details of how parameters are de-\nclared and manipulated, relying on deep learning frameworks to do the heavy lifting. How-\never, when we move away from stacked architectures with standard layers, we will sometimes\nneed to get into the weeds of declaring and manipulating parameters. In this section, we cover\nthe following:\n• Accessing parameters for debugging, diagnostics, and visualizations.\n• Sharing parameters across diﬀerent model components.\nimport torch\nfrom torch import nn\nWe start by focusing on an MLP with one hidden layer.\nnet = nn.Sequential(nn.LazyLinear(8),\nnn.ReLU(),\nnn.LazyLinear(1))\nX = torch.rand(size=(2, 4))\nnet(X).shape\ntorch.Size([2, 1])\n6.2.1 Parameter Access\nLet’s start with how to access parameters from the models that you already know.\nWhen a model is deﬁned via the Sequential class, we can ﬁrst access any layer by indexing\ninto the model as though it were a list. Each layer’s parameters are conveniently located in its\nattribute.\nWe can inspect the parameters of the second fully connected layer as follows.\n\n220\nBuilders’ Guide\nnet[2].state_dict()\nOrderedDict([('weight',\ntensor([[ 0.1842, -0.0048, -0.1618, -0.0667,\n0.1058,\n0.2521, ␣\n,→0.3208, -0.1132]])),\n('bias', tensor([0.1387]))])\nWe can see that this fully connected layer contains two parameters, corresponding to that\nlayer’s weights and biases, respectively.\nTargeted Parameters\nNote that each parameter is represented as an instance of the parameter class. To do anything\nuseful with the parameters, we ﬁrst need to access the underlying numerical values. There\nare several ways to do this. Some are simpler while others are more general. The following\ncode extracts the bias from the second neural network layer, which returns a parameter class\ninstance, and further accesses that parameter’s value.\ntype(net[2].bias), net[2].bias.data\n(torch.nn.parameter.Parameter, tensor([0.1387]))\nParameters are complex objects, containing values, gradients, and additional information.\nThat is why we need to request the value explicitly.\nIn addition to the value, each parameter also allows us to access the gradient. Because we\nhave not invoked backpropagation for this network yet, it is in its initial state.\nnet[2].weight.grad == None\nTrue\nAll Parameters at Once\nWhen we need to perform operations on all parameters, accessing them one-by-one can grow\ntedious. The situation can grow especially unwieldy when we work with more complex, e.g.,\nnested, modules, since we would need to recurse through the entire tree to extract each sub-\nmodule’s parameters. Below we demonstrate accessing the parameters of all layers.\n[(name, param.shape) for name, param in net.named_parameters()]\n\n221\nParameter Management\n[('0.weight', torch.Size([8, 4])),\n('0.bias', torch.Size([8])),\n('2.weight', torch.Size([1, 8])),\n('2.bias', torch.Size([1]))]\n6.2.2 Tied Parameters\nOften, we want to share parameters across multiple layers. Let’s see how to do this elegantly.\nIn the following we allocate a fully connected layer and then use its parameters speciﬁcally\nto set those of another layer. Here we need to run the forward propagation net(X) before\naccessing the parameters.\n# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared = nn.LazyLinear(8)\nnet = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\nshared, nn.ReLU(),\nshared, nn.ReLU(),\nnn.LazyLinear(1))\nnet(X)\n# Check whether the parameters are the same\nprint(net[2].weight.data[0] == net[4].weight.data[0])\nnet[2].weight.data[0, 0] = 100\n# Make sure that they are actually the same object rather than just having the\n# same value\nprint(net[2].weight.data[0] == net[4].weight.data[0])\ntensor([True, True, True, True, True, True, True, True])\ntensor([True, True, True, True, True, True, True, True])\nThis example shows that the parameters of the second and third layer are tied. They are not\njust equal, they are represented by the same exact tensor. Thus, if we change one of the\nparameters, the other one changes, too.\nYou might wonder, when parameters are tied what happens to the gradients? Since the model\nparameters contain gradients, the gradients of the second hidden layer and the third hidden\nlayer are added together during backpropagation.\n6.2.3 Summary\nWe have several ways of accessing and tying model parameters.\n6.2.4 Exercises\n\n222\nBuilders’ Guide\n112\n1. Use the NestMLP model deﬁned in Section 6.1 and access the parameters of the various\nlayers.\n2. Construct an MLP containing a shared parameter layer and train it. During the training\nprocess, observe the model parameters and gradients of each layer.\n3. Why is sharing parameters a good idea?\nDiscussions112.\n6.3 Parameter Initialization\nNow that we know how to access the parameters, let’s look at how to initialize them properly.\nWe discussed the need for proper initialization in Section 5.4. The deep learning framework\nprovides default random initializations to its layers. However, we often want to initialize our\nweights according to various other protocols. The framework provides most commonly used\nprotocols, and also allows to create a custom initializer.\nimport torch\nfrom torch import nn\nBy default, PyTorch initializes weight and bias matrices uniformly by drawing from a range\nthat is computed according to the input and output dimension. PyTorch’s nn.init module\nprovides a variety of preset initialization methods.\nnet = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\nX = torch.rand(size=(2, 4))\nnet(X).shape\ntorch.Size([2, 1])\n6.3.1 Built-in Initialization\nLet’s begin by calling on built-in initializers. The code below initializes all weight parameters\nas Gaussian random variables with standard deviation 0.01, while bias parameters are cleared\nto zero.\ndef init_normal(module):\nif type(module) == nn.Linear:\nnn.init.normal_(module.weight, mean=0, std=0.01)\n(continues on next page)\n\n223\nParameter Initialization\n(continued from previous page)\nnn.init.zeros_(module.bias)\nnet.apply(init_normal)\nnet[0].weight.data[0], net[0].bias.data[0]\n(tensor([-0.0073, -0.0050, -0.0133, -0.0025]), tensor(0.))\nWe can also initialize all the parameters to a given constant value (say, 1).\ndef init_constant(module):\nif type(module) == nn.Linear:\nnn.init.constant_(module.weight, 1)\nnn.init.zeros_(module.bias)\nnet.apply(init_constant)\nnet[0].weight.data[0], net[0].bias.data[0]\n(tensor([1., 1., 1., 1.]), tensor(0.))\nWe can also apply diﬀerent initializers for certain blocks. For example, below we initialize\nthe ﬁrst layer with the Xavier initializer and initialize the second layer to a constant value of\n42.\ndef init_xavier(module):\nif type(module) == nn.Linear:\nnn.init.xavier_uniform_(module.weight)\ndef init_42(module):\nif type(module) == nn.Linear:\nnn.init.constant_(module.weight, 42)\nnet[0].apply(init_xavier)\nnet[2].apply(init_42)\nprint(net[0].weight.data[0])\nprint(net[2].weight.data)\ntensor([ 0.2163, -0.2129, -0.6275,\n0.5273])\ntensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\nCustom Initialization\nSometimes, the initialization methods we need are not provided by the deep learning frame-\nwork. In the example below, we deﬁne an initializer for any weight parameter w using the\n\n224\nBuilders’ Guide\n113\nfollowing strange distribution:\nw ∼\n\n\nU(5, 10)\nwith probability 1\n4\n0\nwith probability 1\n2\nU(−10, −5)\nwith probability 1\n4\n(6.3.1)\nAgain, we implement a my_init function to apply to net.\ndef my_init(module):\nif type(module) == nn.Linear:\nprint(\"Init\", *[(name, param.shape)\nfor name, param in module.named_parameters()][0])\nnn.init.uniform_(module.weight, -10, 10)\nmodule.weight.data *= module.weight.data.abs() >= 5\nnet.apply(my_init)\nnet[0].weight[:2]\nInit weight torch.Size([8, 4])\nInit weight torch.Size([1, 8])\ntensor([[ 7.0262, -9.7639,\n7.1699, -7.3955],\n[-9.7688,\n7.5430,\n0.0000, -0.0000]], grad_fn=<SliceBackward0>)\nNote that we always have the option of setting parameters directly.\nnet[0].weight.data[:] += 1\nnet[0].weight.data[0, 0] = 42\nnet[0].weight.data[0]\ntensor([42.0000, -8.7639,\n8.1699, -6.3955])\n6.3.2 Summary\nWe can initialize parameters using built-in and custom initializers.\n6.3.3 Exercises\nLook up the online documentation for more built-in initializers.\nDiscussions113.\n\n225\nLazy Initialization\n6.4 Lazy Initialization\nSo far, it might seem that we got away with being sloppy in setting up our networks. Speciﬁ-\ncally, we did the following unintuitive things, which might not seem like they should work:\n• We deﬁned the network architectures without specifying the input dimensionality.\n• We added layers without specifying the output dimension of the previous layer.\n• We even “initialized” these parameters before providing enough information to determine\nhow many parameters our models should contain.\nYou might be surprised that our code runs at all. After all, there is no way the deep learning\nframework could tell what the input dimensionality of a network would be. The trick here is\nthat the framework defers initialization, waiting until the ﬁrst time we pass data through the\nmodel, to infer the sizes of each layer on the ﬂy.\nLater on, when working with convolutional neural networks, this technique will become even\nmore convenient since the input dimensionality (e.g., the resolution of an image) will aﬀect\nthe dimensionality of each subsequent layer. Hence the ability to set parameters without the\nneed to know, at the time of writing the code, the value of the dimension can greatly simplify\nthe task of specifying and subsequently modifying our models. Next, we go deeper into the\nmechanics of initialization.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nTo begin, let’s instantiate an MLP.\nnet = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\nAt this point, the network cannot possibly know the dimensions of the input layer’s weights\nbecause the input dimension remains unknown.\nConsequently the framework has not yet initialized any parameters. We conﬁrm by attempting\nto access the parameters below.\nnet[0].weight\n<UninitializedParameter>\nNext let’s pass data through the network to make the framework ﬁnally initialize parame-\nters.\n\n226\nBuilders’ Guide\n114\nX = torch.rand(2, 20)\nnet(X)\nnet[0].weight.shape\ntorch.Size([256, 20])\nAs soon as we know the input dimensionality, 20, the framework can identify the shape of the\nﬁrst layer’s weight matrix by plugging in the value of 20. Having recognized the ﬁrst layer’s\nshape, the framework proceeds to the second layer, and so on through the computational\ngraph until all shapes are known. Note that in this case, only the ﬁrst layer requires lazy ini-\ntialization, but the framework initializes sequentially. Once all parameter shapes are known,\nthe framework can ﬁnally initialize the parameters.\nThe following method passes in dummy inputs through the network for a dry run to infer\nall parameter shapes and subsequently initializes the parameters. It will be used later when\ndefault random initializations are not desired.\n@d2l.add_to_class(d2l.Module)\n#@save\ndef apply_init(self, inputs, init=None):\nself.forward(*inputs)\nif init is not None:\nself.net.apply(init)\n6.4.1 Summary\nLazy initialization can be convenient, allowing the framework to infer parameter shapes au-\ntomatically, making it easy to modify architectures and eliminating one common source of\nerrors. We can pass data through the model to make the framework ﬁnally initialize param-\neters.\n6.4.2 Exercises\n1. What happens if you specify the input dimensions to the ﬁrst layer but not to subsequent\nlayers? Do you get immediate initialization?\n2. What happens if you specify mismatching dimensions?\n3. What would you need to do if you have input of varying dimensionality? Hint: look at the\nparameter tying.\nDiscussions114.\n\n227\nCustom Layers\n6.5 Custom Layers\nOne factor behind deep learning’s success is the availability of a wide range of layers that\ncan be composed in creative ways to design architectures suitable for a wide variety of tasks.\nFor instance, researchers have invented layers speciﬁcally for handling images, text, looping\nover sequential data, and performing dynamic programming. Sooner or later, you will need\na layer that does not exist yet in the deep learning framework. In these cases, you must build\na custom layer. In this section, we show you how.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n6.5.1 Layers without Parameters\nTo start, we construct a custom layer that does not have any parameters of its own. This\nshould look familiar if you recall our introduction to modules in Section 6.1. The following\nCenteredLayer class simply subtracts the mean from its input. To build it, we simply need\nto inherit from the base layer class and implement the forward propagation function.\nclass CenteredLayer(nn.Module):\ndef __init__(self):\nsuper().__init__()\ndef forward(self, X):\nreturn X - X.mean()\nLet’s verify that our layer works as intended by feeding some data through it.\nlayer = CenteredLayer()\nlayer(torch.tensor([1.0, 2, 3, 4, 5]))\ntensor([-2., -1.,\n0.,\n1.,\n2.])\nWe can now incorporate our layer as a component in constructing more complex mod-\nels.\nnet = nn.Sequential(nn.LazyLinear(128), CenteredLayer())\nAs an extra sanity check, we can send random data through the network and check that the\nmean is in fact 0. Because we are dealing with ﬂoating point numbers, we may still see a very\nsmall nonzero number due to quantization.\n\n228\nBuilders’ Guide\nY = net(torch.rand(4, 8))\nY.mean()\ntensor(2.3283e-09, grad_fn=<MeanBackward0>)\n6.5.2 Layers with Parameters\nNow that we know how to deﬁne simple layers, let’s move on to deﬁning layers with parame-\nters that can be adjusted through training. We can use built-in functions to create parameters,\nwhich provide some basic housekeeping functionality. In particular, they govern access, ini-\ntialization, sharing, saving, and loading model parameters. This way, among other beneﬁts,\nwe will not need to write custom serialization routines for every custom layer.\nNow let’s implement our own version of the fully connected layer. Recall that this layer re-\nquires two parameters, one to represent the weight and the other for the bias. In this im-\nplementation, we bake in the ReLU activation as a default. This layer requires two input\narguments: in_units and units, which denote the number of inputs and outputs, respec-\ntively.\nclass MyLinear(nn.Module):\ndef __init__(self, in_units, units):\nsuper().__init__()\nself.weight = nn.Parameter(torch.randn(in_units, units))\nself.bias = nn.Parameter(torch.randn(units,))\ndef forward(self, X):\nlinear = torch.matmul(X, self.weight.data) + self.bias.data\nreturn F.relu(linear)\nNext, we instantiate the MyLinear class and access its model parameters.\nlinear = MyLinear(5, 3)\nlinear.weight\nParameter containing:\ntensor([[-1.3509, -0.6648, -0.3179],\n[-1.0579,\n1.0153,\n0.9971],\n[ 1.4325, -0.7638,\n0.6001],\n[ 0.1169, -1.3587, -0.2305],\n[ 0.0751,\n0.4427,\n1.0031]], requires_grad=True)\nWe can directly carry out forward propagation calculations using custom layers.\nlinear(torch.rand(2, 5))\n\n229\nFile I/O\n115\ntensor([[0.0000, 0.0000, 2.1424],\n[0.2937, 0.0000, 3.0267]])\nWe can also construct models using custom layers. Once we have that we can use it just like\nthe built-in fully connected layer.\nnet = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\nnet(torch.rand(2, 64))\ntensor([[0.7980],\n[5.1926]])\n6.5.3 Summary\nWe can design custom layers via the basic layer class. This allows us to deﬁne ﬂexible new\nlayers that behave diﬀerently from any existing layers in the library. Once deﬁned, custom\nlayers can be invoked in arbitrary contexts and architectures. Layers can have local parame-\nters, which can be created through built-in functions.\n6.5.4 Exercises\n1. Design a layer that takes an input and computes a tensor reduction, i.e., it returns yk =\n∑\ni,j Wijk xixj.\n2. Design a layer that returns the leading half of the Fourier coeﬃcients of the data.\nDiscussions115.\n6.6 File I/O\nSo far we have discussed how to process data and how to build, train, and test deep learning\nmodels. However, at some point we will hopefully be happy enough with the learned models\nthat we will want to save the results for later use in various contexts (perhaps even to make\npredictions in deployment). Additionally, when running a long training process, the best prac-\ntice is to periodically save intermediate results (checkpointing) to ensure that we do not lose\nseveral days’ worth of computation if we trip over the power cord of our server. Thus it is\ntime to learn how to load and store both individual weight vectors and entire models. This\nsection addresses both issues.\n\n230\nBuilders’ Guide\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n6.6.1 Loading and Saving Tensors\nFor individual tensors, we can directly invoke the load and save functions to read and write\nthem respectively. Both functions require that we supply a name, and save requires as input\nthe variable to be saved.\nx = torch.arange(4)\ntorch.save(x, 'x-file')\nWe can now read the data from the stored ﬁle back into memory.\nx2 = torch.load('x-file')\nx2\ntensor([0, 1, 2, 3])\nWe can store a list of tensors and read them back into memory.\ny = torch.zeros(4)\ntorch.save([x, y],'x-files')\nx2, y2 = torch.load('x-files')\n(x2, y2)\n(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))\nWe can even write and read a dictionary that maps from strings to tensors. This is convenient\nwhen we want to read or write all the weights in a model.\nmydict = {'x': x, 'y': y}\ntorch.save(mydict, 'mydict')\nmydict2 = torch.load('mydict')\nmydict2\n{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}\n6.6.2 Loading and Saving Model Parameters\nSaving individual weight vectors (or other tensors) is useful, but it gets very tedious if we\nwant to save (and later load) an entire model. After all, we might have hundreds of parameter\n\n231\nFile I/O\ngroups sprinkled throughout. For this reason the deep learning framework provides built-in\nfunctionalities to load and save entire networks. An important detail to note is that this saves\nmodel parameters and not the entire model. For example, if we have a 3-layer MLP, we need\nto specify the architecture separately. The reason for this is that the models themselves can\ncontain arbitrary code, hence they cannot be serialized as naturally. Thus, in order to reinstate\na model, we need to generate the architecture in code and then load the parameters from disk.\nLet’s start with our familiar MLP.\nclass MLP(nn.Module):\ndef __init__(self):\nsuper().__init__()\nself.hidden = nn.LazyLinear(256)\nself.output = nn.LazyLinear(10)\ndef forward(self, x):\nreturn self.output(F.relu(self.hidden(x)))\nnet = MLP()\nX = torch.randn(size=(2, 20))\nY = net(X)\nNext, we store the parameters of the model as a ﬁle with the name “mlp.params”.\ntorch.save(net.state_dict(), 'mlp.params')\nTo recover the model, we instantiate a clone of the original MLP model. Instead of randomly\ninitializing the model parameters, we read the parameters stored in the ﬁle directly.\nclone = MLP()\nclone.load_state_dict(torch.load('mlp.params'))\nclone.eval()\nMLP(\n(hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n(output): LazyLinear(in_features=0, out_features=10, bias=True)\n)\nSince both instances have the same model parameters, the computational result of the same\ninput X should be the same. Let’s verify this.\nY_clone = clone(X)\nY_clone == Y\ntensor([[True, True, True, True, True, True, True, True, True, True],\n[True, True, True, True, True, True, True, True, True, True]])\n6.6.3 Summary\n\n232\nBuilders’ Guide\n116\n117\nThe save and load functions can be used to perform ﬁle I/O for tensor objects. We can save\nand load the entire sets of parameters for a network via a parameter dictionary. Saving the\narchitecture has to be done in code rather than in parameters.\n6.6.4 Exercises\n1. Even if there is no need to deploy trained models to a diﬀerent device, what are the prac-\ntical beneﬁts of storing model parameters?\n2. Assume that we want to reuse only parts of a network to be incorporated into a network\nhaving a diﬀerent architecture. How would you go about using, say the ﬁrst two layers\nfrom a previous network in a new network?\n3. How would you go about saving the network architecture and parameters? What restric-\ntions would you impose on the architecture?\nDiscussions116.\n6.7 GPUs\nIn Table 1.5.1, we illustrated the rapid growth of computation over the past two decades. In\na nutshell, GPU performance has increased by a factor of 1000 every decade since 2000.\nThis oﬀers great opportunities but it also suggests that there was signiﬁcant demand for such\nperformance.\nIn this section, we begin to discuss how to harness this computational performance for your\nresearch. First by using a single GPU and at a later point, how to use multiple GPUs and\nmultiple servers (with multiple GPUs).\nSpeciﬁcally, we will discuss how to use a single NVIDIA GPU for calculations. First, make\nsure you have at least one NVIDIA GPU installed. Then, download the NVIDIA driver and\nCUDA 117 and follow the prompts to set the appropriate path. Once these preparations\nare complete, the nvidia-smi command can be used to view the graphics card informa-\ntion.\nIn PyTorch, every array has a device; we often refer it as a context. So far, by default, all vari-\nables and associated computation have been assigned to the CPU. Typically, other contexts\nmight be various GPUs. Things can get even hairier when we deploy jobs across multiple\nservers. By assigning arrays to contexts intelligently, we can minimize the time spent trans-\nferring data between devices. For example, when training neural networks on a server with a\nGPU, we typically prefer for the model’s parameters to live on the GPU.\nTo run the programs in this section, you need at least two GPUs. Note that this might be\n\n233\nGPUs\nextravagant for most desktop computers but it is easily available in the cloud, e.g., by using\nthe AWS EC2 multi-GPU instances. Almost all other sections do not require multiple GPUs,\nbut here we simply wish to illustrate data ﬂow between diﬀerent devices.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n6.7.1 Computing Devices\nWe can specify devices, such as CPUs and GPUs, for storage and calculation. By default,\ntensors are created in the main memory and then the CPU is used for calculations.\nIn PyTorch, the CPU and GPU can be indicated by torch.device('cpu') and torch.\ndevice('cuda'). It should be noted that the cpu device means all physical CPUs and mem-\nory. This means that PyTorch’s calculations will try to use all CPU cores. However, a gpu\ndevice only represents one card and the corresponding memory. If there are multiple GPUs,\nwe use torch.device(f'cuda:{i}') to represent the ith GPU (i starts at 0). Also, gpu:0\nand gpu are equivalent.\ndef cpu():\n#@save\n\"\"\"Get the CPU device.\"\"\"\nreturn torch.device('cpu')\ndef gpu(i=0):\n#@save\n\"\"\"Get a GPU device.\"\"\"\nreturn torch.device(f'cuda:{i}')\ncpu(), gpu(), gpu(1)\n(device(type='cpu'),\ndevice(type='cuda', index=0),\ndevice(type='cuda', index=1))\nWe can query the number of available GPUs.\ndef num_gpus():\n#@save\n\"\"\"Get the number of available GPUs.\"\"\"\nreturn torch.cuda.device_count()\nnum_gpus()\n2\nNow we deﬁne two convenient functions that allow us to run code even if the requested GPUs\ndo not exist.\n\n234\nBuilders’ Guide\ndef try_gpu(i=0):\n#@save\n\"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\nif num_gpus() >= i + 1:\nreturn gpu(i)\nreturn cpu()\ndef try_all_gpus():\n#@save\n\"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\nreturn [gpu(i) for i in range(num_gpus())]\ntry_gpu(), try_gpu(10), try_all_gpus()\n(device(type='cuda', index=0),\ndevice(type='cpu'),\n[device(type='cuda', index=0), device(type='cuda', index=1)])\n6.7.2 Tensors and GPUs\nBy default, tensors are created on the CPU. We can query the device where the tensor is\nlocated.\nx = torch.tensor([1, 2, 3])\nx.device\ndevice(type='cpu')\nIt is important to note that whenever we want to operate on multiple terms, they need to\nbe on the same device. For instance, if we sum two tensors, we need to make sure that both\narguments live on the same device—otherwise the framework would not know where to store\nthe result or even how to decide where to perform the computation.\nStorage on the GPU\nThere are several ways to store a tensor on the GPU. For example, we can specify a storage\ndevice when creating a tensor. Next, we create the tensor variable X on the ﬁrst gpu. The tensor\ncreated on a GPU only consumes the memory of this GPU. We can use the nvidia-smi\ncommand to view GPU memory usage. In general, we need to make sure that we do not\ncreate data that exceeds the GPU memory limit.\nX = torch.ones(2, 3, device=try_gpu())\nX\ntensor([[1., 1., 1.],\n[1., 1., 1.]], device='cuda:0')\n\n235\nGPUs\nAssuming that you have at least two GPUs, the following code will create a random tensor,\nY, on the second GPU.\nY = torch.rand(2, 3, device=try_gpu(1))\nY\ntensor([[0.2746, 0.9378, 0.7462],\n[0.9504, 0.7667, 0.0495]], device='cuda:1')\nCopying\nIf we want to compute X + Y, we need to decide where to perform this operation. For instance,\nas shown in Fig. 6.7.1, we can transfer X to the second GPU and perform the operation there.\nDo not simply add X and Y, since this will result in an exception. The runtime engine would\nnot know what to do: it cannot ﬁnd data on the same device and it fails. Since Y lives on the\nsecond GPU, we need to move X there before we can add the two.\nt\nFig. 6.7.1\nCopy data to perform an operation on the same device.\nZ = X.cuda(1)\nprint(X)\nprint(Z)\ntensor([[1., 1., 1.],\n[1., 1., 1.]], device='cuda:0')\ntensor([[1., 1., 1.],\n[1., 1., 1.]], device='cuda:1')\nNow that the data (both Z and Y) are on the same GPU), we can add them up.\nY + Z\ntensor([[1.2746, 1.9378, 1.7462],\n[1.9504, 1.7667, 1.0495]], device='cuda:1')\nBut what if your variable Z already lived on your second GPU? What happens if we still call\nZ.cuda(1)? It will return Z instead of making a copy and allocating new memory.\n\n236\nBuilders’ Guide\nZ.cuda(1) is Z\nTrue\nSide Notes\nPeople use GPUs to do machine learning because they expect them to be fast. But transferring\nvariables between devices is slow: much slower than computation. So we want you to be\n100% certain that you want to do something slow before we let you do it. If the deep learning\nframework just did the copy automatically without crashing then you might not realize that\nyou had written some slow code.\nTransferring data is not only slow, it also makes parallelization a lot more diﬃcult, since we\nhave to wait for data to be sent (or rather to be received) before we can proceed with more\noperations. This is why copy operations should be taken with great care. As a rule of thumb,\nmany small operations are much worse than one big operation. Moreover, several operations\nat a time are much better than many single operations interspersed in the code unless you\nknow what you are doing. This is the case since such operations can block if one device has\nto wait for the other before it can do something else. It is a bit like ordering your coﬀee\nin a queue rather than pre-ordering it by phone and ﬁnding out that it is ready when you\nare.\nLast, when we print tensors or convert tensors to the NumPy format, if the data is not in the\nmain memory, the framework will copy it to the main memory ﬁrst, resulting in additional\ntransmission overhead. Even worse, it is now subject to the dreaded global interpreter lock\nthat makes everything wait for Python to complete.\n6.7.3 Neural Networks and GPUs\nSimilarly, a neural network model can specify devices. The following code puts the model\nparameters on the GPU.\nnet = nn.Sequential(nn.LazyLinear(1))\nnet = net.to(device=try_gpu())\nWe will see many more examples of how to run models on GPUs in the following chapters,\nsimply because the models will become somewhat more computationally intensive.\nFor example, when the input is a tensor on the GPU, the model will calculate the result on\nthe same GPU.\nnet(X)\n\n237\nGPUs\ntensor([[0.6385],\n[0.6385]], device='cuda:0', grad_fn=<AddmmBackward0>)\nLet’s conﬁrm that the model parameters are stored on the same GPU.\nnet[0].weight.data.device\ndevice(type='cuda', index=0)\nLet the trainer support GPU.\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\nself.save_hyperparameters()\nself.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef prepare_batch(self, batch):\nif self.gpus:\nbatch = [a.to(self.gpus[0]) for a in batch]\nreturn batch\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef prepare_model(self, model):\nmodel.trainer = self\nmodel.board.xlim = [0, self.max_epochs]\nif self.gpus:\nmodel.to(self.gpus[0])\nself.model = model\nIn short, as long as all data and parameters are on the same device, we can learn models\neﬃciently. In the following chapters we will see several such examples.\n6.7.4 Summary\nWe can specify devices for storage and calculation, such as the CPU or GPU. By default, data\nis created in the main memory and then uses the CPU for calculations. The deep learning\nframework requires all input data for calculation to be on the same device, be it CPU or the\nsame GPU. You can lose signiﬁcant performance by moving data without care. A typical\nmistake is as follows: computing the loss for every minibatch on the GPU and reporting it\nback to the user on the command line (or logging it in a NumPy ndarray) will trigger a global\ninterpreter lock which stalls all GPUs. It is much better to allocate memory for logging inside\nthe GPU and only move larger logs.\n6.7.5 Exercises\n\n238\nBuilders’ Guide\n118\n1. Try a larger computation task, such as the multiplication of large matrices, and see the\ndiﬀerence in speed between the CPU and GPU. What about a task with a small number\nof calculations?\n2. How should we read and write model parameters on the GPU?\n3. Measure the time it takes to compute 1000 matrix–matrix multiplications of 100 × 100\nmatrices and log the Frobenius norm of the output matrix one result at a time. Compare\nit with keeping a log on the GPU and transferring only the ﬁnal result.\n4. Measure how much time it takes to perform two matrix–matrix multiplications on two\nGPUs at the same time. Compare it with computing in in sequence on one GPU. Hint:\nyou should see almost linear scaling.\nDiscussions118.\n\n7\nConvolutional Neural Networks\nImage data is represented as a two-dimensional grid of pixels, be the image monochromatic\nor in color. Accordingly each pixel corresponds to one or multiple numerical values respec-\ntively. So far we have ignored this rich structure and treated images as vectors of numbers by\nﬂattening them, irrespective of the spatial relation between pixels. This deeply unsatisfying\napproach was necessary in order to feed the resulting one-dimensional vectors through a fully\nconnected MLP.\nBecause these networks are invariant to the order of the features, we could get similar results\nregardless of whether we preserve an order corresponding to the spatial structure of the pixels\nor if we permute the columns of our design matrix before ﬁtting the MLP’s parameters.\nIdeally, we would leverage our prior knowledge that nearby pixels are typically related to\neach other, to build eﬃcient models for learning from image data.\nThis chapter introduces convolutional neural networks (CNNs) (LeCun et al., 1995), a power-\nful family of neural networks that are designed for precisely this purpose. CNN-based archi-\ntectures are now ubiquitous in the ﬁeld of computer vision. For instance, on the Imagnet col-\nlection (Deng et al., 2009) it was only the use of convolutional neural networks, in short Con-\nvnets, that provided signiﬁcant performance improvements (Krizhevsky et al., 2012).\nModern CNNs, as they are called colloquially, owe their design to inspirations from biology,\ngroup theory, and a healthy dose of experimental tinkering. In addition to their sample eﬃ-\nciency in achieving accurate models, CNNs tend to be computationally eﬃcient, both because\nthey require fewer parameters than fully connected architectures and because convolutions\nare easy to parallelize across GPU cores (Chetlur et al., 2014). Consequently, practition-\ners often apply CNNs whenever possible, and increasingly they have emerged as credible\ncompetitors even on tasks with a one-dimensional sequence structure, such as audio (Abdel-\nHamid et al., 2014), text (Kalchbrenner et al., 2014), and time series analysis (LeCun et al.,\n1995), where recurrent neural networks are conventionally used. Some clever adaptations of\nCNNs have also brought them to bear on graph-structured data (Kipf and Welling, 2016) and\nin recommender systems.\nFirst, we will dive more deeply into the motivation for convolutional neural networks. This\nis followed by a walk through the basic operations that comprise the backbone of all con-\nvolutional networks. These include the convolutional layers themselves, nitty-gritty details\nincluding padding and stride, the pooling layers used to aggregate information across adja-\ncent spatial regions, the use of multiple channels at each layer, and a careful discussion of\nthe structure of modern architectures. We will conclude the chapter with a full working ex-\n239\n\n240\nConvolutional Neural Networks\nample of LeNet, the ﬁrst convolutional network successfully deployed, long before the rise\nof modern deep learning. In the next chapter, we will dive into full implementations of some\npopular and comparatively recent CNN architectures whose designs represent most of the\ntechniques commonly used by modern practitioners.\n7.1 From Fully Connected Layers to Convolutions\nTo this day, the models that we have discussed so far remain appropriate options when we are\ndealing with tabular data. By tabular, we mean that the data consist of rows corresponding to\nexamples and columns corresponding to features. With tabular data, we might anticipate that\nthe patterns we seek could involve interactions among the features, but we do not assume any\nstructure a priori concerning how the features interact.\nSometimes, we truly lack the knowledge to be able to guide the construction of fancier ar-\nchitectures. In these cases, an MLP may be the best that we can do. However, for high-\ndimensional perceptual data, such structureless networks can grow unwieldy.\nFor instance, let’s return to our running example of distinguishing cats from dogs. Say that\nwe do a thorough job in data collection, collecting an annotated dataset of one-megapixel\nphotographs. This means that each input to the network has one million dimensions. Even\nan aggressive reduction to one thousand hidden dimensions would require a fully connected\nlayer characterized by 106×103 = 109 parameters. Unless we have lots of GPUs, a talent for\ndistributed optimization, and an extraordinary amount of patience, learning the parameters\nof this network may turn out to be infeasible.\nA careful reader might object to this argument on the basis that one megapixel resolution may\nnot be necessary. However, while we might be able to get away with one hundred thousand\npixels, our hidden layer of size 1000 grossly underestimates the number of hidden units that\nit takes to learn good representations of images, so a practical system will still require billions\nof parameters. Moreover, learning a classiﬁer by ﬁtting so many parameters might require\ncollecting an enormous dataset. And yet today both humans and computers are able to dis-\ntinguish cats from dogs quite well, seemingly contradicting these intuitions. That is because\nimages exhibit rich structure that can be exploited by humans and machine learning models\nalike. Convolutional neural networks (CNNs) are one creative way that machine learning has\nembraced for exploiting some of the known structure in natural images.\n7.1.1 Invariance\nImagine that we want to detect an object in an image. It seems reasonable that whatever\nmethod we use to recognize objects should not be overly concerned with the precise location\nof the object in the image. Ideally, our system should exploit this knowledge. Pigs usually do\nnot ﬂy and planes usually do not swim. Nonetheless, we should still recognize a pig were one\n\n241\nFrom Fully Connected Layers to Convolutions\nto appear at the top of the image. We can draw some inspiration here from the children’s game\n“Where’s Waldo” (which itself has inspired many real-life imitations, such as that depicted in\nFig. 7.1.1). The game consists of a number of chaotic scenes bursting with activities. Waldo\nshows up somewhere in each, typically lurking in some unlikely location. The reader’s goal\nis to locate him. Despite his characteristic outﬁt, this can be surprisingly diﬃcult, due to the\nlarge number of distractions. However, what Waldo looks like does not depend upon where\nWaldo is located. We could sweep the image with a Waldo detector that could assign a score\nto each patch, indicating the likelihood that the patch contains Waldo. In fact, many object\ndetection and segmentation algorithms are based on this approach (Long et al., 2015). CNNs\nsystematize this idea of spatial invariance, exploiting it to learn useful representations with\nfewer parameters.\nt\nFig. 7.1.1\nCan you ﬁnd Waldo (image courtesy of William Murphy (Infomatique))?\nWe can now make these intuitions more concrete by enumerating a few desiderata to guide\nour design of a neural network architecture suitable for computer vision:\n1. In the earliest layers, our network should respond similarly to the same patch, regardless of\nwhere it appears in the image. This principle is called translation invariance (or translation\nequivariance).\n2. The earliest layers of the network should focus on local regions, without regard for the\ncontents of the image in distant regions. This is the locality principle. Eventually, these\nlocal representations can be aggregated to make predictions at the whole image level.\n3. As we proceed, deeper layers should be able to capture longer-range features of the image,\nin a way similar to higher level vision in nature.\nLet’s see how this translates into mathematics.\n7.1.2 Constraining the MLP\n\n242\nConvolutional Neural Networks\nTo start oﬀ, we can consider an MLP with two-dimensional images X as inputs and their im-\nmediate hidden representations H similarly represented as matrices (they are two-dimensional\ntensors in code), where both X and H have the same shape. Let that sink in. We now imagine\nthat not only the inputs but also the hidden representations possess spatial structure.\nLet [X]i, j and [H]i,j denote the pixel at location (i, j) in the input image and hidden rep-\nresentation, respectively. Consequently, to have each of the hidden units receive input from\neach of the input pixels, we would switch from using weight matrices (as we did previously\nin MLPs) to representing our parameters as fourth-order weight tensors W. Suppose that U\ncontains biases, we could formally express the fully connected layer as\n[H]i, j = [U]i,j +\n∑\nk\n∑\nl\n[W]i,j,k,l[X]k,l\n= [U]i,j +\n∑\na\n∑\nb\n[V]i, j,a,b[X]i+a, j+b.\n(7.1.1)\nThe switch from W to V is entirely cosmetic for now since there is a one-to-one correspon-\ndence between coeﬃcients in both fourth-order tensors. We simply re-index the subscripts\n(k, l) such that k = i + a and l = j + b. In other words, we set [V]i, j,a,b = [W]i, j,i+a, j+b.\nThe indices a and b run over both positive and negative oﬀsets, covering the entire image. For\nany given location (i, j) in the hidden representation [H]i, j, we compute its value by summing\nover pixels in x, centered around (i, j) and weighted by [V]i,j,a,b. Before we carry on, let’s\nconsider the total number of parameters required for a single layer in this parametrization: a\n1000 × 1000 image (1 megapixel) is mapped to a 1000 × 1000 hidden representation. This\nrequires 1012 parameters, far beyond what computers currently can handle.\nTranslation Invariance\nNow let’s invoke the ﬁrst principle established above: translation invariance (Zhang et al.,\n1988). This implies that a shift in the input X should simply lead to a shift in the hidden\nrepresentation H. This is only possible if V and U do not actually depend on (i, j). As such,\nwe have [V]i, j,a,b = [V]a,b and U is a constant, say u. As a result, we can simplify the\ndeﬁnition for H:\n[H]i,j = u +\n∑\na\n∑\nb\n[V]a,b[X]i+a,j+b.\n(7.1.2)\nThis is a convolution! We are eﬀectively weighting pixels at (i + a, j + b) in the vicinity\nof location (i, j) with coeﬃcients [V]a,b to obtain the value [H]i,j. Note that [V]a,b needs\nmany fewer coeﬃcients than [V]i, j,a,b since it no longer depends on the location within the\nimage. Consequently, the number of parameters required is no longer 1012 but a much more\nreasonable 4 × 106: we still have the dependency on a, b ∈(−1000, 1000). In short, we\nhave made signiﬁcant progress. Time-delay neural networks (TDNNs) are some of the ﬁrst\nexamples to exploit this idea (Waibel et al., 1989).\n\n243\nFrom Fully Connected Layers to Convolutions\nLocality\nNow let’s invoke the second principle: locality. As motivated above, we believe that we should\nnot have to look very far away from location (i, j) in order to glean relevant information to\nassess what is going on at [H]i,j. This means that outside some range |a| > ∆or |b| > ∆,\nwe should set [V]a,b = 0. Equivalently, we can rewrite [H]i,j as\n[H]i,j = u +\n∆\n∑\na=−∆\n∆\n∑\nb=−∆\n[V]a,b[X]i+a,j+b.\n(7.1.3)\nThis reduces the number of parameters from 4 × 106 to 4∆2, where ∆is typically smaller\nthan 10. As such, we reduced the number of parameters by another four orders of magnitude.\nNote that (7.1.3), is what is called, in a nutshell, a convolutional layer. Convolutional neural\nnetworks (CNNs) are a special family of neural networks that contain convolutional layers.\nIn the deep learning research community, V is referred to as a convolution kernel, a ﬁlter, or\nsimply the layer’s weights that are learnable parameters.\nWhile previously, we might have required billions of parameters to represent just a single\nlayer in an image-processing network, we now typically need just a few hundred, without al-\ntering the dimensionality of either the inputs or the hidden representations. The price paid for\nthis drastic reduction in parameters is that our features are now translation invariant and that\nour layer can only incorporate local information, when determining the value of each hidden\nactivation. All learning depends on imposing inductive bias. When that bias agrees with real-\nity, we get sample-eﬃcient models that generalize well to unseen data. But of course, if those\nbiases do not agree with reality, e.g., if images turned out not to be translation invariant, our\nmodels might struggle even to ﬁt our training data.\nThis dramatic reduction in parameters brings us to our last desideratum, namely that deeper\nlayers should represent larger and more complex aspects of an image. This can be achieved\nby interleaving nonlinearities and convolutional layers repeatedly.\n7.1.3 Convolutions\nLet’s brieﬂy review why (7.1.3) is called a convolution. In mathematics, the convolution be-\ntween two functions (Rudin, 1973), say f, g : Rd →R is deﬁned as\n( f ∗g)(x) =\n∫\nf (z)g(x −z)dz.\n(7.1.4)\nThat is, we measure the overlap between f and g when one function is “ﬂipped” and shifted by\nx. Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors\nfrom the set of square-summable inﬁnite-dimensional vectors with index running over Z we\nobtain the following deﬁnition:\n( f ∗g)(i) =\n∑\na\nf (a)g(i −a).\n(7.1.5)\n\n244\nConvolutional Neural Networks\nFor two-dimensional tensors, we have a corresponding sum with indices (a, b) for f and\n(i −a, j −b) for g, respectively:\n( f ∗g)(i, j) =\n∑\na\n∑\nb\nf (a, b)g(i −a, j −b).\n(7.1.6)\nThis looks similar to (7.1.3), with one major diﬀerence. Rather than using (i + a, j + b),\nwe are using the diﬀerence instead. Note, though, that this distinction is mostly cosmetic\nsince we can always match the notation between (7.1.3) and (7.1.6). Our original deﬁnition in\n(7.1.3) more properly describes a cross-correlation. We will come back to this in the following\nsection.\n7.1.4 Channels\nReturning to our Waldo detector, let’s see what this looks like. The convolutional layer picks\nwindows of a given size and weighs intensities according to the ﬁlter V, as demonstrated in\nFig. 7.1.2. We might aim to learn a model so that wherever the “waldoness” is highest, we\nshould ﬁnd a peak in the hidden layer representations.\nt\nFig. 7.1.2\nDetect Waldo (image courtesy of William Murphy (Infomatique)).\nThere is just one problem with this approach. So far, we blissfully ignored that images consist\nof three channels: red, green, and blue. In sum, images are not two-dimensional objects but\nrather third-order tensors, characterized by a height, width, and channel, e.g., with shape\n1024 × 1024 × 3 pixels. While the ﬁrst two of these axes concern spatial relationships, the\nthird can be regarded as assigning a multidimensional representation to each pixel location.\nWe thus index X as [X]i,j,k. The convolutional ﬁlter has to adapt accordingly. Instead of\n[V]a,b, we now have [V]a,b,c.\nMoreover, just as our input consists of a third-order tensor, it turns out to be a good idea\nto similarly formulate our hidden representations as third-order tensors H. In other words,\nrather than just having a single hidden representation corresponding to each spatial location,\nwe want an entire vector of hidden representations corresponding to each spatial location.\n\n245\nFrom Fully Connected Layers to Convolutions\nWe could think of the hidden representations as comprising a number of two-dimensional\ngrids stacked on top of each other. As in the inputs, these are sometimes called channels.\nThey are also sometimes called feature maps, as each provides a spatialized set of learned\nfeatures for the subsequent layer. Intuitively, you might imagine that at lower layers that are\ncloser to inputs, some channels could become specialized to recognize edges while others\ncould recognize textures.\nTo support multiple channels in both inputs (X) and hidden representations (H), we can add\na fourth coordinate to V: [V]a,b,c,d. Putting everything together we have:\n[H]i,j,d =\n∆\n∑\na=−∆\n∆\n∑\nb=−∆\n∑\nc\n[V]a,b,c,d[X]i+a,j+b,c,\n(7.1.7)\nwhere d indexes the output channels in the hidden representations H. The subsequent con-\nvolutional layer will go on to take a third-order tensor, H, as input. We take (7.1.7), because\nof its generality, as the deﬁnition of a convolutional layer for multiple channels, where V is\na kernel or ﬁlter of the layer.\nThere are still many operations that we need to address. For instance, we need to ﬁgure\nout how to combine all the hidden representations to a single output, e.g., whether there is\na Waldo anywhere in the image. We also need to decide how to compute things eﬃciently,\nhow to combine multiple layers, appropriate activation functions, and how to make reasonable\ndesign choices to yield networks that are eﬀective in practice. We turn to these issues in the\nremainder of the chapter.\n7.1.5 Summary and Discussion\nIn this section we derived the structure of convolutional neural networks from ﬁrst principles.\nWhile it is unclear whether this was the route taken to the invention of CNNs, it is satisfy-\ning to know that they are the right choice when applying reasonable principles to how image\nprocessing and computer vision algorithms should operate, at least at lower levels. In partic-\nular, translation invariance in images implies that all patches of an image will be treated in\nthe same manner. Locality means that only a small neighborhood of pixels will be used to\ncompute the corresponding hidden representations. Some of the earliest references to CNNs\nare in the form of the Neocognitron (Fukushima, 1982).\nA second principle that we encountered in our reasoning is how to reduce the number of\nparameters in a function class without limiting its expressive power, at least, whenever certain\nassumptions on the model hold. We saw a dramatic reduction of complexity as a result of\nthis restriction, turning computationally and statistically infeasible problems into tractable\nmodels.\nAdding channels allowed us to bring back some of the complexity that was lost due to the\nrestrictions imposed on the convolutional kernel by locality and translation invariance. Note\nthat it is quite natural to add channels other than just red, green, and blue. Many satellite\nimages, in particular for agriculture and meteorology, have tens to hundreds of channels,\ngenerating hyperspectral images instead. They report data on many diﬀerent wavelengths.\n\n246\nConvolutional Neural Networks\n119\nIn the following we will see how to use convolutions eﬀectively to manipulate the dimen-\nsionality of the images they operate on, how to move from location-based to channel-based\nrepresentations, and how to deal with large numbers of categories eﬃciently.\n7.1.6 Exercises\n1. Assume that the size of the convolution kernel is ∆= 0. Show that in this case the\nconvolution kernel implements an MLP independently for each set of channels. This leads\nto the Network in Network architectures (Lin et al., 2013).\n2. Audio data is often represented as a one-dimensional sequence.\n1. When might you want to impose locality and translation invariance for audio?\n2. Derive the convolution operations for audio.\n3. Can you treat audio using the same tools as computer vision? Hint: use the spectrogram.\n3. Why might translation invariance not be a good idea after all? Give an example.\n4. Do you think that convolutional layers might also be applicable for text data? Which prob-\nlems might you encounter with language?\n5. What happens with convolutions when an object is at the boundary of an image?\n6. Prove that the convolution is symmetric, i.e., f ∗g = g ∗f .\nDiscussions119.\n7.2 Convolutions for Images\nNow that we understand how convolutional layers work in theory, we are ready to see how\nthey work in practice. Building on our motivation of convolutional neural networks as eﬃ-\ncient architectures for exploring structure in image data, we stick with images as our running\nexample.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n7.2.1 The Cross-Correlation Operation\nRecall that strictly speaking, convolutional layers are a misnomer, since the operations they\nexpress are more accurately described as cross-correlations. Based on our descriptions of\n\n247\nConvolutions for Images\nconvolutional layers in Section 7.1, in such a layer, an input tensor and a kernel tensor are\ncombined to produce an output tensor through a cross-correlation operation.\nLet’s ignore channels for now and see how this works with two-dimensional data and hidden\nrepresentations. In Fig. 7.2.1, the input is a two-dimensional tensor with a height of 3 and\nwidth of 3. We mark the shape of the tensor as 3 × 3 or (3, 3). The height and width of the\nkernel are both 2. The shape of the kernel window (or convolution window) is given by the\nheight and width of the kernel (here it is 2 × 2).\nt\nFig. 7.2.1\nTwo-dimensional cross-correlation operation. The shaded portions are the ﬁrst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19.\nIn the two-dimensional cross-correlation operation, we begin with the convolution window\npositioned at the upper-left corner of the input tensor and slide it across the input tensor,\nboth from left to right and top to bottom. When the convolution window slides to a certain\nposition, the input subtensor contained in that window and the kernel tensor are multiplied\nelementwise and the resulting tensor is summed up yielding a single scalar value. This result\ngives the value of the output tensor at the corresponding location. Here, the output tensor\nhas a height of 2 and width of 2 and the four elements are derived from the two-dimensional\ncross-correlation operation:\n0 × 0 + 1 × 1 + 3 × 2 + 4 × 3 = 19,\n1 × 0 + 2 × 1 + 4 × 2 + 5 × 3 = 25,\n3 × 0 + 4 × 1 + 6 × 2 + 7 × 3 = 37,\n4 × 0 + 5 × 1 + 7 × 2 + 8 × 3 = 43.\n(7.2.1)\nNote that along each axis, the output size is slightly smaller than the input size. Because the\nkernel has width and height greater than 1, we can only properly compute the cross-correlation\nfor locations where the kernel ﬁts wholly within the image, the output size is given by the input\nsize nh × nw minus the size of the convolution kernel kh × kw via\n(nh −kh + 1) × (nw −kw + 1).\n(7.2.2)\nThis is the case since we need enough space to “shift” the convolution kernel across the image.\nLater we will see how to keep the size unchanged by padding the image with zeros around its\nboundary so that there is enough space to shift the kernel. Next, we implement this process\nin the corr2d function, which accepts an input tensor X and a kernel tensor K and returns an\noutput tensor Y.\n\n248\nConvolutional Neural Networks\ndef corr2d(X, K):\n#@save\n\"\"\"Compute 2D cross-correlation.\"\"\"\nh, w = K.shape\nY = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\nfor i in range(Y.shape[0]):\nfor j in range(Y.shape[1]):\nY[i, j] = (X[i:i + h, j:j + w] * K).sum()\nreturn Y\nWe can construct the input tensor X and the kernel tensor K from Fig. 7.2.1 to validate the out-\nput of the above implementation of the two-dimensional cross-correlation operation.\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\nK = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\ncorr2d(X, K)\ntensor([[19., 25.],\n[37., 43.]])\n7.2.2 Convolutional Layers\nA convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an\noutput. The two parameters of a convolutional layer are the kernel and the scalar bias. When\ntraining models based on convolutional layers, we typically initialize the kernels randomly,\njust as we would with a fully connected layer.\nWe are now ready to implement a two-dimensional convolutional layer based on the corr2d\nfunction deﬁned above. In the __init__ constructor method, we declare weight and bias\nas the two model parameters. The forward propagation method calls the corr2d function and\nadds the bias.\nclass Conv2D(nn.Module):\ndef __init__(self, kernel_size):\nsuper().__init__()\nself.weight = nn.Parameter(torch.rand(kernel_size))\nself.bias = nn.Parameter(torch.zeros(1))\ndef forward(self, x):\nreturn corr2d(x, self.weight) + self.bias\nIn h × w convolution or an h × w convolution kernel, the height and width of the convolu-\ntion kernel are h and w, respectively. We also refer to a convolutional layer with an h × w\nconvolution kernel simply as an h × w convolutional layer.\n7.2.3 Object Edge Detection in Images\n\n249\nConvolutions for Images\nLet’s take a moment to parse a simple application of a convolutional layer: detecting the\nedge of an object in an image by ﬁnding the location of the pixel change. First, we construct\nan “image” of 6 × 8 pixels. The middle four columns are black (0) and the rest are white\n(1).\nX = torch.ones((6, 8))\nX[:, 2:6] = 0\nX\ntensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n[1., 1., 0., 0., 0., 0., 1., 1.],\n[1., 1., 0., 0., 0., 0., 1., 1.],\n[1., 1., 0., 0., 0., 0., 1., 1.],\n[1., 1., 0., 0., 0., 0., 1., 1.],\n[1., 1., 0., 0., 0., 0., 1., 1.]])\nNext, we construct a kernel K with a height of 1 and a width of 2. When we perform the\ncross-correlation operation with the input, if the horizontally adjacent elements are the same,\nthe output is 0. Otherwise, the output is nonzero. Note that this kernel is a special case of a\nﬁnite diﬀerence operator. At location (i, j) it computes xi, j −x(i+1),j, i.e., it computes the\ndiﬀerence between the values of horizontally adjacent pixels. This is a discrete approximation\nof the ﬁrst derivative in the horizontal direction. After all, for a function f (i, j) its derivative\n−∂i f (i, j) = limϵ→0\nf (i,j)−f (i+ϵ, j)\nϵ\n. Let’s see how this works in practice.\nK = torch.tensor([[1.0, -1.0]])\nWe are ready to perform the cross-correlation operation with arguments X (our input) and K\n(our kernel). As you can see, we detect 1 for the edge from white to black and −1 for the\nedge from black to white. All other outputs take value 0.\nY = corr2d(X, K)\nY\ntensor([[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.],\n[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.],\n[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.],\n[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.],\n[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.],\n[ 0.,\n1.,\n0.,\n0.,\n0., -1.,\n0.]])\nWe can now apply the kernel to the transposed image. As expected, it vanishes. The kernel K\nonly detects vertical edges.\ncorr2d(X.t(), K)\n\n250\nConvolutional Neural Networks\ntensor([[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.],\n[0., 0., 0., 0., 0.]])\n7.2.4 Learning a Kernel\nDesigning an edge detector by ﬁnite diﬀerences [1, -1] is neat if we know this is precisely\nwhat we are looking for. However, as we look at larger kernels, and consider successive layers\nof convolutions, it might be impossible to specify precisely what each ﬁlter should be doing\nmanually.\nNow let’s see whether we can learn the kernel that generated Y from X by looking at the input–\noutput pairs only. We ﬁrst construct a convolutional layer and initialize its kernel as a random\ntensor. Next, in each iteration, we will use the squared error to compare Y with the output of\nthe convolutional layer. We can then calculate the gradient to update the kernel. For the sake\nof simplicity, in the following we use the built-in class for two-dimensional convolutional\nlayers and ignore the bias.\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\nconv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, channel, height, width), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX = X.reshape((1, 1, 6, 8))\nY = Y.reshape((1, 1, 6, 7))\nlr = 3e-2\n# Learning rate\nfor i in range(10):\nY_hat = conv2d(X)\nl = (Y_hat - Y) ** 2\nconv2d.zero_grad()\nl.sum().backward()\n# Update the kernel\nconv2d.weight.data[:] -= lr * conv2d.weight.grad\nif (i + 1) % 2 == 0:\nprint(f'epoch {i + 1}, loss {l.sum():.3f}')\nepoch 2, loss 13.904\nepoch 4, loss 2.333\nepoch 6, loss 0.392\nepoch 8, loss 0.066\nepoch 10, loss 0.011\n\n251\nConvolutions for Images\nNote that the error has dropped to a small value after 10 iterations. Now we will take a look\nat the kernel tensor we learned.\nconv2d.weight.data.reshape((1, 2))\ntensor([[ 0.9797, -0.9816]])\nIndeed, the learned kernel tensor is remarkably close to the kernel tensor K we deﬁned ear-\nlier.\n7.2.5 Cross-Correlation and Convolution\nRecall our observation from Section 7.1 of the correspondence between the cross-correlation\nand convolution operations. Here let’s continue to consider two-dimensional convolutional\nlayers. What if such layers perform strict convolution operations as deﬁned in (7.1.6) instead\nof cross-correlations? In order to obtain the output of the strict convolution operation, we\nonly need to ﬂip the two-dimensional kernel tensor both horizontally and vertically, and then\nperform the cross-correlation operation with the input tensor.\nIt is noteworthy that since kernels are learned from data in deep learning, the outputs of con-\nvolutional layers remain unaﬀected no matter such layers perform either the strict convolution\noperations or the cross-correlation operations.\nTo illustrate this, suppose that a convolutional layer performs cross-correlation and learns the\nkernel in Fig. 7.2.1, which is here denoted as the matrix K. Assuming that other conditions\nremain unchanged, when this layer instead performs strict convolution, the learned kernel K′\nwill be the same as K after K′ is ﬂipped both horizontally and vertically. That is to say,\nwhen the convolutional layer performs strict convolution for the input in Fig. 7.2.1 and K′,\nthe same output in Fig. 7.2.1 (cross-correlation of the input and K) will be obtained.\nIn keeping with standard terminology in deep learning literature, we will continue to refer to\nthe cross-correlation operation as a convolution even though, strictly-speaking, it is slightly\ndiﬀerent. Furthermore, we use the term element to refer to an entry (or component) of any\ntensor representing a layer representation or a convolution kernel.\n7.2.6 Feature Map and Receptive Field\nAs described in Section 7.1.4, the convolutional layer output in Fig. 7.2.1 is sometimes called\na feature map, as it can be regarded as the learned representations (features) in the spatial\ndimensions (e.g., width and height) to the subsequent layer. In CNNs, for any element x of\nsome layer, its receptive ﬁeld refers to all the elements (from all the previous layers) that may\naﬀect the calculation of x during the forward propagation. Note that the receptive ﬁeld may\nbe larger than the actual size of the input.\nLet’s continue to use Fig. 7.2.1 to explain the receptive ﬁeld. Given the 2 × 2 convolution\n\n252\nConvolutional Neural Networks\nkernel, the receptive ﬁeld of the shaded output element (of value 19) is the four elements in\nthe shaded portion of the input. Now let’s denote the 2×2 output as Y and consider a deeper\nCNN with an additional 2×2 convolutional layer that takes Y as its input, outputting a single\nelement z. In this case, the receptive ﬁeld of z on Y includes all the four elements of Y, while\nthe receptive ﬁeld on the input includes all the nine input elements. Thus, when any element\nin a feature map needs a larger receptive ﬁeld to detect input features over a broader area, we\ncan build a deeper network.\nReceptive ﬁelds derive their name from neurophysiology. A series of experiments on a range\nof animals using diﬀerent stimuli (Hubel and Wiesel, 1959, Hubel and Wiesel, 1962, Hubel\nand Wiesel, 1968) explored the response of what is called the visual cortex on said stimuli.\nBy and large they found that lower levels respond to edges and related shapes. Later on, Field\n(1987) illustrated this eﬀect on natural images with, what can only be called, convolutional\nkernels. We reprint a key ﬁgure in Fig. 7.2.2 to illustrate the striking similarities.\nt\nFig. 7.2.2\nFigure and caption taken from Field (1987): An example of coding with six different\nchannels. (Left) Examples of the six types of sensor associated with each channel. (Right)\nConvolution of the image in (Middle) with the six sensors shown in (Left). The response\nof the individual sensors is determined by sampling these ﬁltered images at a distance\nproportional to the size of the sensor (shown with dots). This diagram shows the response\nof only the even symmetric sensors.\nAs it turns out, this relation even holds for the features computed by deeper layers of net-\nworks trained on image classiﬁcation tasks, as demonstrated in, for example, Kuzovkin et\nal. (2018). Suﬃce it to say, convolutions have proven to be an incredibly powerful tool for\ncomputer vision, both in biology and in code. As such, it is not surprising (in hindsight) that\nthey heralded the recent success in deep learning.\n\n253\nConvolutions for Images\n120\n7.2.7 Summary\nThe core computation required for a convolutional layer is a cross-correlation operation. We\nsaw that a simple nested for-loop is all that is required to compute its value. If we have multiple\ninput and multiple output channels, we are performing a matrix–matrix operation between\nchannels. As can be seen, the computation is straightforward and, most importantly, highly\nlocal. This aﬀords signiﬁcant hardware optimization and many recent results in computer\nvision are only possible because of that. After all, it means that chip designers can invest in\nfast computation rather than memory when it comes to optimizing for convolutions. While\nthis may not lead to optimal designs for other applications, it does open the door to ubiquitous\nand aﬀordable computer vision.\nIn terms of convolutions themselves, they can be used for many purposes, for example detect-\ning edges and lines, blurring images, or sharpening them. Most importantly, it is not necessary\nthat the statistician (or engineer) invents suitable ﬁlters. Instead, we can simply learn them\nfrom data. This replaces feature engineering heuristics by evidence-based statistics. Lastly,\nand quite delightfully, these ﬁlters are not just advantageous for building deep networks but\nthey also correspond to receptive ﬁelds and feature maps in the brain. This gives us conﬁdence\nthat we are on the right track.\n7.2.8 Exercises\n1. Construct an image X with diagonal edges.\n1. What happens if you apply the kernel K in this section to it?\n2. What happens if you transpose X?\n3. What happens if you transpose K?\n2. Design some kernels manually.\n1. Given a directional vector v = (v1, v2), derive an edge-detection kernel that detects\nedges orthogonal to v, i.e., edges in the direction (v2, −v1).\n2. Derive a ﬁnite diﬀerence operator for the second derivative. What is the minimum size\nof the convolutional kernel associated with it? Which structures in images respond most\nstrongly to it?\n3. How would you design a blur kernel? Why might you want to use such a kernel?\n4. What is the minimum size of a kernel to obtain a derivative of order d?\n3. When you try to automatically ﬁnd the gradient for the Conv2D class we created, what\nkind of error message do you see?\n4. How do you represent a cross-correlation operation as a matrix multiplication by changing\nthe input and kernel tensors?\nDiscussions120.\n\n254\nConvolutional Neural Networks\n7.3 Padding and Stride\nRecall the example of a convolution in Fig. 7.2.1. The input had both a height and width of 3\nand the convolution kernel had both a height and width of 2, yielding an output representation\nwith dimension 2 × 2. Assuming that the input shape is nh × nw and the convolution kernel\nshape is kh × kw, the output shape will be (nh −kh + 1) × (nw −kw + 1): we can only shift\nthe convolution kernel so far until it runs out of pixels to apply the convolution to.\nIn the following we will explore a number of techniques, including padding and strided con-\nvolutions, that oﬀer more control over the size of the output. As motivation, note that since\nkernels generally have width and height greater than 1, after applying many successive con-\nvolutions, we tend to wind up with outputs that are considerably smaller than our input. If\nwe start with a 240 × 240 pixel image, ten layers of 5 × 5 convolutions reduce the image to\n200 × 200 pixels, slicing oﬀ30% of the image and with it obliterating any interesting infor-\nmation on the boundaries of the original image. Padding is the most popular tool for handling\nthis issue. In other cases, we may want to reduce the dimensionality drastically, e.g., if we\nﬁnd the original input resolution to be unwieldy. Strided convolutions are a popular technique\nthat can help in these instances.\nimport torch\nfrom torch import nn\n7.3.1 Padding\nAs described above, one tricky issue when applying convolutional layers is that we tend to\nlose pixels on the perimeter of our image. Consider Fig. 7.3.1 that depicts the pixel utilization\nas a function of the convolution kernel size and the position within the image. The pixels in\nthe corners are hardly used at all.\nt\nFig. 7.3.1\nPixel utilization for convolutions of size 1 × 1, 2 × 2, and 3 × 3 respectively.\nSince we typically use small kernels, for any given convolution we might only lose a few pixels\nbut this can add up as we apply many successive convolutional layers. One straightforward\nsolution to this problem is to add extra pixels of ﬁller around the boundary of our input\nimage, thus increasing the eﬀective size of the image. Typically, we set the values of the\n\n255\nPadding and Stride\nextra pixels to zero. In Fig. 7.3.2, we pad a 3 × 3 input, increasing its size to 5 × 5. The\ncorresponding output then increases to a 4×4 matrix. The shaded portions are the ﬁrst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0 × 0 + 0 × 1 + 0 × 2 + 0 × 3 = 0.\nt\nFig. 7.3.2\nTwo-dimensional cross-correlation with padding.\nIn general, if we add a total of ph rows of padding (roughly half on top and half on bottom)\nand a total of pw columns of padding (roughly half on the left and half on the right), the\noutput shape will be\n(nh −kh + ph + 1) × (nw −kw + pw + 1).\n(7.3.1)\nThis means that the height and width of the output will increase by ph and pw, respec-\ntively.\nIn many cases, we will want to set ph = kh −1 and pw = kw −1 to give the input and output\nthe same height and width. This will make it easier to predict the output shape of each layer\nwhen constructing the network. Assuming that kh is odd here, we will pad ph/2 rows on both\nsides of the height. If kh is even, one possibility is to pad ⌈ph/2⌉rows on the top of the input\nand ⌊ph/2⌋rows on the bottom. We will pad both sides of the width in the same way.\nCNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5,\nor 7. Choosing odd kernel sizes has the beneﬁt that we can preserve the dimensionality while\npadding with the same number of rows on top and bottom, and the same number of columns\non left and right.\nMoreover, this practice of using odd kernels and padding to precisely preserve dimensionality\noﬀers a clerical beneﬁt. For any two-dimensional tensor X, when the kernel’s size is odd and\nthe number of padding rows and columns on all sides are the same, thereby producing an\noutput with the same height and width as the input, we know that the output Y[i, j] is\ncalculated by cross-correlation of the input and convolution kernel with the window centered\non X[i, j].\nIn the following example, we create a two-dimensional convolutional layer with a height and\nwidth of 3 and apply 1 pixel of padding on all sides. Given an input with a height and width\nof 8, we ﬁnd that the height and width of the output is also 8.\n# We define a helper function to calculate convolutions. It initializes the\n# convolutional layer weights and performs corresponding dimensionality\n(continues on next page)\n\n256\nConvolutional Neural Networks\n(continued from previous page)\n# elevations and reductions on the input and output\ndef comp_conv2d(conv2d, X):\n# (1, 1) indicates that batch size and the number of channels are both 1\nX = X.reshape((1, 1) + X.shape)\nY = conv2d(X)\n# Strip the first two dimensions: examples and channels\nreturn Y.reshape(Y.shape[2:])\n# 1 row and column is padded on either side, so a total of 2 rows or columns\n# are added\nconv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\nX = torch.rand(size=(8, 8))\ncomp_conv2d(conv2d, X).shape\ntorch.Size([8, 8])\nWhen the height and width of the convolution kernel are diﬀerent, we can make the output\nand input have the same height and width by setting diﬀerent padding numbers for height and\nwidth.\n# We use a convolution kernel with height 5 and width 3. The padding on either\n# side of the height and width are 2 and 1, respectively\nconv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\ncomp_conv2d(conv2d, X).shape\ntorch.Size([8, 8])\n7.3.2 Stride\nWhen computing the cross-correlation, we start with the convolution window at the upper-left\ncorner of the input tensor, and then slide it over all locations both down and to the right. In the\nprevious examples, we defaulted to sliding one element at a time. However, sometimes, either\nfor computational eﬃciency or because we wish to downsample, we move our window more\nthan one element at a time, skipping the intermediate locations. This is particularly useful if\nthe convolution kernel is large since it captures a large area of the underlying image.\nWe refer to the number of rows and columns traversed per slide as stride. So far, we have used\nstrides of 1, both for height and width. Sometimes, we may want to use a larger stride. Fig.\n7.3.3 shows a two-dimensional cross-correlation operation with a stride of 3 vertically and\n2 horizontally. The shaded portions are the output elements as well as the input and kernel\ntensor elements used for the output computation: 0 × 0 + 0 × 1 + 1 × 2 + 2 × 3 = 8,\n0 × 0 + 6 × 1 + 0 × 2 + 0 × 3 = 6. We can see that when the second element of the\nﬁrst column is generated, the convolution window slides down three rows. The convolution\nwindow slides two columns to the right when the second element of the ﬁrst row is generated.\nWhen the convolution window continues to slide two columns to the right on the input, there\n\n257\nPadding and Stride\nis no output because the input element cannot ﬁll the window (unless we add another column\nof padding).\nt\nFig. 7.3.3\nCross-correlation with strides of 3 and 2 for height and width, respectively.\nIn general, when the stride for the height is sh and the stride for the width is sw, the output\nshape is\n⌊(nh −kh + ph + sh)/sh⌋× ⌊(nw −kw + pw + sw)/sw⌋.\n(7.3.2)\nIf we set ph = kh −1 and pw = kw −1, then the output shape can be simpliﬁed to ⌊(nh +sh −\n1)/sh⌋×⌊(nw+sw−1)/sw⌋. Going a step further, if the input height and width are divisible by\nthe strides on the height and width, then the output shape will be (nh/sh) × (nw/sw).\nBelow, we set the strides on both the height and width to 2, thus halving the input height and\nwidth.\nconv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\ncomp_conv2d(conv2d, X).shape\ntorch.Size([4, 4])\nLet’s look at a slightly more complicated example.\nconv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\ncomp_conv2d(conv2d, X).shape\ntorch.Size([2, 2])\n7.3.3 Summary and Discussion\nPadding can increase the height and width of the output. This is often used to give the output\nthe same height and width as the input to avoid undesirable shrinkage of the output. Moreover,\nit ensures that all pixels are used equally frequently. Typically we pick symmetric padding\non both sides of the input height and width. In this case we refer to (ph, pw) padding. Most\ncommonly we set ph = pw, in which case we simply state that we choose padding p.\nA similar convention applies to strides. When horizontal stride sh and vertical stride sw match,\n\n258\nConvolutional Neural Networks\n121\nwe simply talk about stride s. The stride can reduce the resolution of the output, for example\nreducing the height and width of the output to only 1/n of the height and width of the input\nfor n > 1. By default, the padding is 0 and the stride is 1.\nSo far all padding that we discussed simply extended images with zeros. This has signiﬁcant\ncomputational beneﬁt since it is trivial to accomplish. Moreover, operators can be engineered\nto take advantage of this padding implicitly without the need to allocate additional memory.\nAt the same time, it allows CNNs to encode implicit position information within an image,\nsimply by learning where the “whitespace” is. There are many alternatives to zero-padding.\nAlsallakh et al. (2020) provided an extensive overview of those (albeit without a clear case\nfor when to use nonzero paddings unless artifacts occur).\n7.3.4 Exercises\n1. Given the ﬁnal code example in this section with kernel size (3, 5), padding (0, 1), and\nstride (3, 4), calculate the output shape to check if it is consistent with the experimental\nresult.\n2. For audio signals, what does a stride of 2 correspond to?\n3. Implement mirror padding, i.e., padding where the border values are simply mirrored to\nextend tensors.\n4. What are the computational beneﬁts of a stride larger than 1?\n5. What might be statistical beneﬁts of a stride larger than 1?\n6. How would you implement a stride of 1\n2? What does it correspond to? When would this\nbe useful?\nDiscussions121.\n7.4 Multiple Input and Multiple Output Channels\nWhile we described the multiple channels that comprise each image (e.g., color images have\nthe standard RGB channels to indicate the amount of red, green and blue) and convolutional\nlayers for multiple channels in Section 7.1.4, until now, we simpliﬁed all of our numerical\nexamples by working with just a single input and a single output channel. This allowed us to\nthink of our inputs, convolution kernels, and outputs each as two-dimensional tensors.\nWhen we add channels into the mix, our inputs and hidden representations both become\nthree-dimensional tensors. For example, each RGB input image has shape 3 × h × w. We\nrefer to this axis, with a size of 3, as the channel dimension. The notion of channels is as old\nas CNNs themselves: for instance LeNet-5 (LeCun et al., 1995) uses them. In this section,\n\n259\nMultiple Input and Multiple Output Channels\nwe will take a deeper look at convolution kernels with multiple input and multiple output\nchannels.\nimport torch\nfrom d2l import torch as d2l\n7.4.1 Multiple Input Channels\nWhen the input data contains multiple channels, we need to construct a convolution kernel\nwith the same number of input channels as the input data, so that it can perform cross-\ncorrelation with the input data. Assuming that the number of channels for the input data is ci,\nthe number of input channels of the convolution kernel also needs to be ci. If our convolution\nkernel’s window shape is kh × kw, then, when ci = 1, we can think of our convolution kernel\nas just a two-dimensional tensor of shape kh × kw.\nHowever, when ci > 1, we need a kernel that contains a tensor of shape kh × kw for ev-\nery input channel. Concatenating these ci tensors together yields a convolution kernel of\nshape ci × kh × kw. Since the input and convolution kernel each have ci channels, we can\nperform a cross-correlation operation on the two-dimensional tensor of the input and the\ntwo-dimensional tensor of the convolution kernel for each channel, adding the ci results to-\ngether (summing over the channels) to yield a two-dimensional tensor. This is the result of a\ntwo-dimensional cross-correlation between a multi-channel input and a multi-input-channel\nconvolution kernel.\nFig. 7.4.1 provides an example of a two-dimensional cross-correlation with two input chan-\nnels. The shaded portions are the ﬁrst output element as well as the input and kernel tensor\nelements used for the output computation: (1 × 1 + 2 × 2 + 4 × 3 + 5 × 4) + (0 × 0 + 1 ×\n1 + 3 × 2 + 4 × 3) = 56.\nt\nFig. 7.4.1\nCross-correlation computation with two input channels.\nTo make sure we really understand what is going on here, we can implement cross-correlation\noperations with multiple input channels ourselves. Notice that all we are doing is performing\na cross-correlation operation per channel and then adding up the results.\n\n260\nConvolutional Neural Networks\ndef corr2d_multi_in(X, K):\n# Iterate through the 0th dimension (channel) of K first, then add them up\nreturn sum(d2l.corr2d(x, k) for x, k in zip(X, K))\nWe can construct the input tensor X and the kernel tensor K corresponding to the values in\nFig. 7.4.1 to validate the output of the cross-correlation operation.\nX = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\nK = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\ncorr2d_multi_in(X, K)\ntensor([[ 56.,\n72.],\n[104., 120.]])\n7.4.2 Multiple Output Channels\nRegardless of the number of input channels, so far we always ended up with one output\nchannel. However, as we discussed in Section 7.1.4, it turns out to be essential to have multiple\nchannels at each layer. In the most popular neural network architectures, we actually increase\nthe channel dimension as we go deeper in the neural network, typically downsampling to trade\noﬀspatial resolution for greater channel depth. Intuitively, you could think of each channel\nas responding to a diﬀerent set of features. The reality is a bit more complicated than this.\nA naive interpretation would suggest that representations are learned independently per pixel\nor per channel. Instead, channels are optimized to be jointly useful. This means that rather\nthan mapping a single channel to an edge detector, it may simply mean that some direction\nin channel space corresponds to detecting edges.\nDenote by ci and co the number of input and output channels, respectively, and by kh and kw\nthe height and width of the kernel. To get an output with multiple channels, we can create a\nkernel tensor of shape ci×kh×kw for every output channel. We concatenate them on the output\nchannel dimension, so that the shape of the convolution kernel is co × ci × kh × kw. In cross-\ncorrelation operations, the result on each output channel is calculated from the convolution\nkernel corresponding to that output channel and takes input from all channels in the input\ntensor.\nWe implement a cross-correlation function to calculate the output of multiple channels as\nshown below.\ndef corr2d_multi_in_out(X, K):\n# Iterate through the 0th dimension of K, and each time, perform\n# cross-correlation operations with input X. All of the results are\n# stacked together\nreturn torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n\n261\nMultiple Input and Multiple Output Channels\nWe construct a trivial convolution kernel with three output channels by concatenating the\nkernel tensor for K with K+1 and K+2.\nK = torch.stack((K, K + 1, K + 2), 0)\nK.shape\ntorch.Size([3, 2, 2, 2])\nBelow, we perform cross-correlation operations on the input tensor X with the kernel tensor\nK. Now the output contains three channels. The result of the ﬁrst channel is consistent with\nthe result of the previous input tensor X and the multi-input channel, single-output channel\nkernel.\ncorr2d_multi_in_out(X, K)\ntensor([[[ 56.,\n72.],\n[104., 120.]],\n[[ 76., 100.],\n[148., 172.]],\n[[ 96., 128.],\n[192., 224.]]])\n7.4.3 1 × 1 Convolutional Layer\nAt ﬁrst, a 1×1 convolution, i.e., kh = kw = 1, does not seem to make much sense. After all,\na convolution correlates adjacent pixels. A 1 × 1 convolution obviously does not. Nonethe-\nless, they are popular operations that are sometimes included in the designs of complex deep\nnetworks (Lin et al., 2013, Szegedy et al., 2017). Let’s see in some detail what it actually\ndoes.\nBecause the minimum window is used, the 1 × 1 convolution loses the ability of larger con-\nvolutional layers to recognize patterns consisting of interactions among adjacent elements in\nthe height and width dimensions. The only computation of the 1 × 1 convolution occurs on\nthe channel dimension.\nFig. 7.4.2 shows the cross-correlation computation using the 1 × 1 convolution kernel with 3\ninput channels and 2 output channels. Note that the inputs and outputs have the same height\nand width. Each element in the output is derived from a linear combination of elements at\nthe same position in the input image. You could think of the 1 × 1 convolutional layer as\nconstituting a fully connected layer applied at every single pixel location to transform the ci\ncorresponding input values into co output values. Because this is still a convolutional layer, the\nweights are tied across pixel location. Thus the 1×1 convolutional layer requires co×ci weights\n(plus the bias). Also note that convolutional layers are typically followed by nonlinearities.\nThis ensures that 1 × 1 convolutions cannot simply be folded into other convolutions.\n\n262\nConvolutional Neural Networks\nt\nFig. 7.4.2\nThe cross-correlation computation uses the 1 × 1 convolution kernel with three input\nchannels and two output channels. The input and output have the same height and width.\nLet’s check whether this works in practice: we implement a 1 × 1 convolution using a fully\nconnected layer. The only thing is that we need to make some adjustments to the data shape\nbefore and after the matrix multiplication.\ndef corr2d_multi_in_out_1x1(X, K):\nc_i, h, w = X.shape\nc_o = K.shape[0]\nX = X.reshape((c_i, h * w))\nK = K.reshape((c_o, c_i))\n# Matrix multiplication in the fully connected layer\nY = torch.matmul(K, X)\nreturn Y.reshape((c_o, h, w))\nWhen performing 1 × 1 convolutions, the above function is equivalent to the previously im-\nplemented cross-correlation function corr2d_multi_in_out. Let’s check this with some\nsample data.\nX = torch.normal(0, 1, (3, 3, 3))\nK = torch.normal(0, 1, (2, 3, 1, 1))\nY1 = corr2d_multi_in_out_1x1(X, K)\nY2 = corr2d_multi_in_out(X, K)\nassert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n7.4.4 Discussion\nChannels allow us to combine the best of both worlds: MLPs that allow for signiﬁcant nonlin-\nearities and convolutions that allow for localized analysis of features. In particular, channels\nallow the CNN to reason with multiple features, such as edge and shape detectors at the same\ntime. They also oﬀer a practical trade-oﬀbetween the drastic parameter reduction arising\nfrom translation invariance and locality, and the need for expressive and diverse models in\ncomputer vision.\nNote, though, that this ﬂexibility comes at a price. Given an image of size (h × w), the cost\nfor computing a k × k convolution is O(h · w · k2). For ci and co input and output channels\nrespectively this increases to O(h · w · k2 · ci · co). For a 256 × 256 pixel image with a\n5 × 5 kernel and 128 input and output channels respectively this amounts to over 53 billion\noperations (we count multiplications and additions separately). Later on we will encounter\n\n263\nMultiple Input and Multiple Output Channels\n122\neﬀective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to\nbe block-diagonal, leading to architectures such as ResNeXt (Xie et al., 2017).\n7.4.5 Exercises\n1. Assume that we have two convolution kernels of size k1 and k2, respectively (with no\nnonlinearity in between).\n1. Prove that the result of the operation can be expressed by a single convolution.\n2. What is the dimensionality of the equivalent single convolution?\n3. Is the converse true, i.e., can you always decompose a convolution into two smaller\nones?\n2. Assume an input of shape ci × h × w and a convolution kernel of shape co × ci × kh × kw,\npadding of (ph, pw), and stride of (sh, sw).\n1. What is the computational cost (multiplications and additions) for the forward propa-\ngation?\n2. What is the memory footprint?\n3. What is the memory footprint for the backward computation?\n4. What is the computational cost for the backpropagation?\n3. By what factor does the number of calculations increase if we double both the number of\ninput channels ci and the number of output channels co? What happens if we double the\npadding?\n4. Are the variables Y1 and Y2 in the ﬁnal example of this section exactly the same? Why?\n5. Express convolutions as a matrix multiplication, even when the convolution window is not\n1 × 1.\n6. Your task is to implement fast convolutions with a k × k kernel. One of the algorithm\ncandidates is to scan horizontally across the source, reading a k-wide strip and computing\nthe 1-wide output strip one value at a time. The alternative is to read a k + ∆wide strip\nand compute a ∆-wide output strip. Why is the latter preferable? Is there a limit to how\nlarge you should choose ∆?\n7. Assume that we have a c × c matrix.\n1. How much faster is it to multiply with a block-diagonal matrix if the matrix is broken\nup into b blocks?\n2. What is the downside of having b blocks? How could you ﬁx it, at least partly?\nDiscussions122.\n\n264\nConvolutional Neural Networks\n7.5 Pooling\nIn many cases our ultimate task asks some global question about the image, e.g., does it\ncontain a cat? Consequently, the units of our ﬁnal layer should be sensitive to the entire input.\nBy gradually aggregating information, yielding coarser and coarser maps, we accomplish\nthis goal of ultimately learning a global representation, while keeping all of the advantages\nof convolutional layers at the intermediate layers of processing. The deeper we go in the\nnetwork, the larger the receptive ﬁeld (relative to the input) to which each hidden node is\nsensitive. Reducing spatial resolution accelerates this process, since the convolution kernels\ncover a larger eﬀective area.\nMoreover, when detecting lower-level features, such as edges (as discussed in Section 7.2),\nwe often want our representations to be somewhat invariant to translation. For instance, if\nwe take the image X with a sharp delineation between black and white and shift the whole\nimage by one pixel to the right, i.e., Z[i, j] = X[i, j + 1], then the output for the new\nimage Z might be vastly diﬀerent. The edge will have shifted by one pixel. In reality, objects\nhardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object,\nvibration of the camera due to the movement of the shutter might shift everything by a pixel\nor so (high-end cameras are loaded with special features to address this problem).\nThis section introduces pooling layers, which serve the dual purposes of mitigating the sensi-\ntivity of convolutional layers to location and of spatially downsampling representations.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n7.5.1 Maximum Pooling and Average Pooling\nLike convolutional layers, pooling operators consist of a ﬁxed-shape window that is slid over\nall regions in the input according to its stride, computing a single output for each location\ntraversed by the ﬁxed-shape window (sometimes known as the pooling window). However,\nunlike the cross-correlation computation of the inputs and kernels in the convolutional layer,\nthe pooling layer contains no parameters (there is no kernel). Instead, pooling operators are\ndeterministic, typically calculating either the maximum or the average value of the elements\nin the pooling window. These operations are called maximum pooling (max-pooling for short)\nand average pooling, respectively.\nAverage pooling is essentially as old as CNNs. The idea is akin to downsampling an image.\nRather than just taking the value of every second (or third) pixel for the lower resolution\nimage, we can average over adjacent pixels to obtain an image with better signal-to-noise\nratio since we are combining the information from multiple adjacent pixels. Max-pooling was\nintroduced in Riesenhuber and Poggio (1999) in the context of cognitive neuroscience to\n\n265\nPooling\ndescribe how information aggregation might be aggregated hierarchically for the purpose of\nobject recognition; there already was an earlier version in speech recognition (Yamaguchi et\nal., 1990). In almost all cases, max-pooling, as it is also referred to, is preferable to average\npooling.\nIn both cases, as with the cross-correlation operator, we can think of the pooling window\nas starting from the upper-left of the input tensor and sliding across it from left to right and\ntop to bottom. At each location that the pooling window hits, it computes the maximum or\naverage value of the input subtensor in the window, depending on whether max or average\npooling is employed.\nt\nFig. 7.5.1\nMax-pooling with a pooling window shape of 2 × 2. The shaded portions are the ﬁrst\noutput element as well as the input tensor elements used for the output computation:\nmax(0, 1, 3, 4) = 4.\nThe output tensor in Fig. 7.5.1 has a height of 2 and a width of 2. The four elements are\nderived from the maximum value in each pooling window:\nmax(0, 1, 3, 4) = 4,\nmax(1, 2, 4, 5) = 5,\nmax(3, 4, 6, 7) = 7,\nmax(4, 5, 7, 8) = 8.\n(7.5.1)\nMore generally, we can deﬁne a p × q pooling layer by aggregating over a region of said size.\nReturning to the problem of edge detection, we use the output of the convolutional layer as\ninput for 2 × 2 max-pooling. Denote by X the input of the convolutional layer input and Y the\npooling layer output. Regardless of whether or not the values of X[i, j], X[i, j + 1],\nX[i+1, j] and X[i+1, j + 1] are diﬀerent, the pooling layer always outputs Y[i, j] = 1.\nThat is to say, using the 2 × 2 max-pooling layer, we can still detect if the pattern recognized\nby the convolutional layer moves no more than one element in height or width.\nIn the code below, we implement the forward propagation of the pooling layer in the pool2d\nfunction. This function is similar to the corr2d function in Section 7.2. However, no kernel\nis needed, computing the output as either the maximum or the average of each region in the\ninput.\ndef pool2d(X, pool_size, mode='max'):\np_h, p_w = pool_size\nY = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\nfor i in range(Y.shape[0]):\nfor j in range(Y.shape[1]):\nif mode == 'max':\n(continues on next page)\n\n266\nConvolutional Neural Networks\n(continued from previous page)\nY[i, j] = X[i: i + p_h, j: j + p_w].max()\nelif mode == 'avg':\nY[i, j] = X[i: i + p_h, j: j + p_w].mean()\nreturn Y\nWe can construct the input tensor X in Fig. 7.5.1 to validate the output of the two-dimensional\nmax-pooling layer.\nX = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\npool2d(X, (2, 2))\ntensor([[4., 5.],\n[7., 8.]])\nAlso, we can experiment with the average pooling layer.\npool2d(X, (2, 2), 'avg')\ntensor([[2., 3.],\n[5., 6.]])\n7.5.2 Padding and Stride\nAs with convolutional layers, pooling layers change the output shape. And as before, we can\nadjust the operation to achieve a desired output shape by padding the input and adjusting the\nstride. We can demonstrate the use of padding and strides in pooling layers via the built-in\ntwo-dimensional max-pooling layer from the deep learning framework. We ﬁrst construct an\ninput tensor X whose shape has four dimensions, where the number of examples (batch size)\nand number of channels are both 1.\nX = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\nX\ntensor([[[[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.],\n[12., 13., 14., 15.]]]])\nSince pooling aggregates information from an area, deep learning frameworks default to\nmatching pooling window sizes and stride. For instance, if we use a pooling window of shape\n(3, 3) we get a stride shape of (3, 3) by default.\n\n267\nPooling\npool2d = nn.MaxPool2d(3)\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\ntensor([[[[10.]]]])\nNeedless to say, the stride and padding can be manually speciﬁed to override framework\ndefaults if required.\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\npool2d(X)\ntensor([[[[ 5.,\n7.],\n[13., 15.]]]])\nOf course, we can specify an arbitrary rectangular pooling window with arbitrary height and\nwidth respectively, as the example below shows.\npool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\npool2d(X)\ntensor([[[[ 5.,\n7.],\n[13., 15.]]]])\n7.5.3 Multiple Channels\nWhen processing multi-channel input data, the pooling layer pools each input channel sep-\narately, rather than summing the inputs up over channels as in a convolutional layer. This\nmeans that the number of output channels for the pooling layer is the same as the number of\ninput channels. Below, we will concatenate tensors X and X + 1 on the channel dimension to\nconstruct an input with two channels.\nX = torch.cat((X, X + 1), 1)\nX\ntensor([[[[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.],\n[12., 13., 14., 15.]],\n[[ 1.,\n2.,\n3.,\n4.],\n[ 5.,\n6.,\n7.,\n8.],\n[ 9., 10., 11., 12.],\n[13., 14., 15., 16.]]]])\n\n268\nConvolutional Neural Networks\nAs we can see, the number of output channels is still two after pooling.\npool2d = nn.MaxPool2d(3, padding=1, stride=2)\npool2d(X)\ntensor([[[[ 5.,\n7.],\n[13., 15.]],\n[[ 6.,\n8.],\n[14., 16.]]]])\n7.5.4 Summary\nPooling is an exceedingly simple operation. It does exactly what its name indicates, aggregate\nresults over a window of values. All convolution semantics, such as strides and padding apply\nin the same way as they did previously. Note that pooling is indiﬀerent to channels, i.e., it\nleaves the number of channels unchanged and it applies to each channel separately. Lastly, of\nthe two popular pooling choices, max-pooling is preferable to average pooling, as it confers\nsome degree of invariance to output. A popular choice is to pick a pooling window size of\n2 × 2 to quarter the spatial resolution of output.\nNote that there are many more ways of reducing resolution beyond pooling. For instance,\nin stochastic pooling (Zeiler and Fergus, 2013) and fractional max-pooling (Graham, 2014)\naggregation is combined with randomization. This can slightly improve the accuracy in some\ncases. Lastly, as we will see later with the attention mechanism, there are more reﬁned ways\nof aggregating over outputs, e.g., by using the alignment between a query and representation\nvectors.\n7.5.5 Exercises\n1. Implement average pooling through a convolution.\n2. Prove that max-pooling cannot be implemented through a convolution alone.\n3. Max-pooling can be accomplished using ReLU operations, i.e., ReLU(x) = max(0, x).\n1. Express max(a, b) by using only ReLU operations.\n2. Use this to implement max-pooling by means of convolutions and ReLU layers.\n3. How many channels and layers do you need for a 2 × 2 convolution? How many for a\n3 × 3 convolution?\n4. What is the computational cost of the pooling layer? Assume that the input to the pooling\nlayer is of size c × h × w, the pooling window has a shape of ph × pw with a padding of\n(ph, pw) and a stride of (sh, sw).\n5. Why do you expect max-pooling and average pooling to work diﬀerently?\n\n269\nConvolutional Neural Networks (LeNet)\n123\n6. Do we need a separate minimum pooling layer? Can you replace it with another operation?\n7. We could use the softmax operation for pooling. Why might it not be so popular?\nDiscussions123.\n7.6 Convolutional Neural Networks (LeNet)\nWe now have all the ingredients required to assemble a fully-functional CNN. In our earlier\nencounter with image data, we applied a linear model with softmax regression (Section 4.4)\nand an MLP (Section 5.2) to pictures of clothing in the Fashion-MNIST dataset. To make\nsuch data amenable we ﬁrst ﬂattened each image from a 28 × 28 matrix into a ﬁxed-length\n784-dimensional vector, and thereafter processed them in fully connected layers. Now that\nwe have a handle on convolutional layers, we can retain the spatial structure in our images.\nAs an additional beneﬁt of replacing fully connected layers with convolutional layers, we will\nenjoy more parsimonious models that require far fewer parameters.\nIn this section, we will introduce LeNet, among the ﬁrst published CNNs to capture wide\nattention for its performance on computer vision tasks. The model was introduced by (and\nnamed for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of recognizing\nhandwritten digits in images (LeCun et al., 1998). This work represented the culmination of\na decade of research developing the technology; LeCun’s team published the ﬁrst study to\nsuccessfully train CNNs via backpropagation (LeCun et al., 1989).\nAt the time LeNet achieved outstanding results matching the performance of support vector\nmachines, then a dominant approach in supervised learning, achieving an error rate of less\nthan 1% per digit. LeNet was eventually adapted to recognize digits for processing deposits\nin ATM machines. To this day, some ATMs still run the code that Yann LeCun and his\ncolleague Leon Bottou wrote in the 1990s!\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n7.6.1 LeNet\nAt a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consisting\nof two convolutional layers; and (ii) a dense block consisting of three fully connected layers.\nThe architecture is summarized in Fig. 7.6.1.\nThe basic units in each convolutional block are a convolutional layer, a sigmoid activation\nfunction, and a subsequent average pooling operation. Note that while ReLUs and max-\npooling work better, they had not yet been discovered. Each convolutional layer uses a 5 × 5\n\n270\nConvolutional Neural Networks\nt\nFig. 7.6.1\nData ﬂow in LeNet. The input is a handwritten digit, the output is a probability over 10\npossible outcomes.\nkernel and a sigmoid activation function. These layers map spatially arranged inputs to a\nnumber of two-dimensional feature maps, typically increasing the number of channels. The\nﬁrst convolutional layer has 6 output channels, while the second has 16. Each 2 × 2 pool-\ning operation (stride 2) reduces dimensionality by a factor of 4 via spatial downsampling.\nThe convolutional block emits an output with shape given by (batch size, number of channel,\nheight, width).\nIn order to pass output from the convolutional block to the dense block, we must ﬂatten each\nexample in the minibatch. In other words, we take this four-dimensional input and transform\nit into the two-dimensional input expected by fully connected layers: as a reminder, the two-\ndimensional representation that we desire uses the ﬁrst dimension to index examples in the\nminibatch and the second to give the ﬂat vector representation of each example. LeNet’s dense\nblock has three fully connected layers, with 120, 84, and 10 outputs, respectively. Because we\nare still performing classiﬁcation, the 10-dimensional output layer corresponds to the number\nof possible output classes.\nWhile getting to the point where you truly understand what is going on inside LeNet may\nhave taken a bit of work, we hope that the following code snippet will convince you that\nimplementing such models with modern deep learning frameworks is remarkably simple. We\nneed only to instantiate a Sequential block and chain together the appropriate layers, using\nXavier initialization as introduced in Section 5.4.2.\ndef init_cnn(module):\n#@save\n\"\"\"Initialize weights for CNNs.\"\"\"\nif type(module) == nn.Linear or type(module) == nn.Conv2d:\nnn.init.xavier_uniform_(module.weight)\nclass LeNet(d2l.Classifier):\n#@save\n\"\"\"The LeNet-5 model.\"\"\"\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\n(continues on next page)\n\n271\nConvolutional Neural Networks (LeNet)\n(continued from previous page)\nself.net = nn.Sequential(\nnn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\nnn.AvgPool2d(kernel_size=2, stride=2),\nnn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\nnn.AvgPool2d(kernel_size=2, stride=2),\nnn.Flatten(),\nnn.LazyLinear(120), nn.Sigmoid(),\nnn.LazyLinear(84), nn.Sigmoid(),\nnn.LazyLinear(num_classes))\nWe have taken some liberty in the reproduction of LeNet insofar as we have replaced the\nGaussian activation layer by a softmax layer. This greatly simpliﬁes the implementation, not\nleast due to the fact that the Gaussian decoder is rarely used nowadays. Other than that, this\nnetwork matches the original LeNet-5 architecture.\nLet’s see what happens inside the network. By passing a single-channel (black and white)\n28×28 image through the network and printing the output shape at each layer, we can inspect\nthe model to ensure that its operations line up with what we expect from Fig. 7.6.2.\nt\nFig. 7.6.2\nCompressed notation for LeNet-5.\n@d2l.add_to_class(d2l.Classifier)\n#@save\ndef layer_summary(self, X_shape):\nX = torch.randn(*X_shape)\nfor layer in self.net:\nX = layer(X)\nprint(layer.__class__.__name__, 'output shape:\\t', X.shape)\nmodel = LeNet()\nmodel.layer_summary((1, 1, 28, 28))\n\n272\nConvolutional Neural Networks\nConv2d output shape:\ntorch.Size([1, 6, 28, 28])\nSigmoid output shape:\ntorch.Size([1, 6, 28, 28])\nAvgPool2d output shape:\ntorch.Size([1, 6, 14, 14])\nConv2d output shape:\ntorch.Size([1, 16, 10, 10])\nSigmoid output shape:\ntorch.Size([1, 16, 10, 10])\nAvgPool2d output shape:\ntorch.Size([1, 16, 5, 5])\nFlatten output shape:\ntorch.Size([1, 400])\nLinear output shape:\ntorch.Size([1, 120])\nSigmoid output shape:\ntorch.Size([1, 120])\nLinear output shape:\ntorch.Size([1, 84])\nSigmoid output shape:\ntorch.Size([1, 84])\nLinear output shape:\ntorch.Size([1, 10])\nNote that the height and width of the representation at each layer throughout the convolutional\nblock is reduced (compared with the previous layer). The ﬁrst convolutional layer uses two\npixels of padding to compensate for the reduction in height and width that would otherwise\nresult from using a 5 × 5 kernel. As an aside, the image size of 28 × 28 pixels in the original\nMNIST OCR dataset is a result of trimming two pixel rows (and columns) from the original\nscans that measured 32 × 32 pixels. This was done primarily to save space (a 30% reduction)\nat a time when megabytes mattered.\nIn contrast, the second convolutional layer forgoes padding, and thus the height and width\nare both reduced by four pixels. As we go up the stack of layers, the number of channels\nincreases layer-over-layer from 1 in the input to 6 after the ﬁrst convolutional layer and 16\nafter the second convolutional layer. However, each pooling layer halves the height and width.\nFinally, each fully connected layer reduces dimensionality, ﬁnally emitting an output whose\ndimension matches the number of classes.\n7.6.2 Training\nNow that we have implemented the model, let’s run an experiment to see how the LeNet-5\nmodel fares on Fashion-MNIST.\nWhile CNNs have fewer parameters, they can still be more expensive to compute than sim-\nilarly deep MLPs because each parameter participates in many more multiplications. If you\nhave access to a GPU, this might be a good time to put it into action to speed up training.\nNote that the d2l.Trainer class takes care of all details. By default, it initializes the model\nparameters on the available devices. Just as with MLPs, our loss function is cross-entropy,\nand we minimize it via minibatch stochastic gradient descent.\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = LeNet(lr=0.1)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\ntrainer.fit(model, data)\n7.6.3 Summary\n\n273\nConvolutional Neural Networks (LeNet)\nWe have made signiﬁcant progress in this chapter. We moved from the MLPs of the 1980s\nto the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form\nof LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on\nFashion-MNIST achievable with LeNet-5 both to the very best possible with MLPs (Section\n5.2) and those with signiﬁcantly more advanced architectures such as ResNet (Section 8.6).\nLeNet is much more similar to the latter than to the former. One of the primary diﬀerences,\nas we shall see, is that greater amounts of computation enabled signiﬁcantly more complex\narchitectures.\nA second diﬀerence is the relative ease with which we were able to implement LeNet. What\nused to be an engineering challenge worth months of C++ and assembly code, engineering\nto improve SN, an early Lisp-based deep learning tool (Bottou and Le Cun, 1988), and ﬁ-\nnally experimentation with models can now be accomplished in minutes. It is this incredible\nproductivity boost that has democratized deep learning model development tremendously. In\nthe next chapter we will journey down this rabbit to hole to see where it takes us.\n7.6.4 Exercises\n1. Let’s modernize LeNet. Implement and test the following changes:\n1. Replace average pooling with max-pooling.\n2. Replace the softmax layer with ReLU.\n2. Try to change the size of the LeNet style network to improve its accuracy in addition to\nmax-pooling and ReLU.\n1. Adjust the convolution window size.\n2. Adjust the number of output channels.\n3. Adjust the number of convolution layers.\n4. Adjust the number of fully connected layers.\n\n274\nConvolutional Neural Networks\n124\n5. Adjust the learning rates and other training details (e.g., initialization and number of\nepochs).\n3. Try out the improved network on the original MNIST dataset.\n4. Display the activations of the ﬁrst and second layer of LeNet for diﬀerent inputs (e.g.,\nsweaters and coats).\n5. What happens to the activations when you feed signiﬁcantly diﬀerent images into the\nnetwork (e.g., cats, cars, or even random noise)?\nDiscussions124.\n\n125\n8\nModern Convolutional Neural Networks\nNow that we understand the basics of wiring together CNNs, let’s take a tour of modern\nCNN architectures. This tour is, by necessity, incomplete, thanks to the plethora of exciting\nnew designs being added. Their importance derives from the fact that not only can they be\nused directly for vision tasks, but they also serve as basic feature generators for more ad-\nvanced tasks such as tracking (Zhang et al., 2021), segmentation (Long et al., 2015), object\ndetection (Redmon and Farhadi, 2018), or style transformation (Gatys et al., 2016). In this\nchapter, most sections correspond to a signiﬁcant CNN architecture that was at some point\n(or currently) the base model upon which many research projects and deployed systems were\nbuilt. Each of these networks was brieﬂy a dominant architecture and many were winners or\nrunners-up in the ImageNet competition125 which has served as a barometer of progress on\nsupervised learning in computer vision since 2010. It is only recently that Transformers have\nbegun to displace CNNs, starting with Dosovitskiy et al. (2021) and followed by the Swin\nTransformer (Liu et al., 2021). We will cover this development later in Chapter 11.\nWhile the idea of deep neural networks is quite simple (stack together a bunch of layers),\nperformance can vary wildly across architectures and hyperparameter choices. The neural\nnetworks described in this chapter are the product of intuition, a few mathematical insights,\nand a lot of trial and error. We present these models in chronological order, partly to con-\nvey a sense of the history so that you can form your own intuitions about where the ﬁeld is\nheading and perhaps develop your own architectures. For instance, batch normalization and\nresidual connections described in this chapter have oﬀered two popular ideas for training and\ndesigning deep models, both of which have since also been applied to architectures beyond\ncomputer vision.\nWe begin our tour of modern CNNs with AlexNet (Krizhevsky et al., 2012), the ﬁrst large-\nscale network deployed to beat conventional computer vision methods on a large-scale vision\nchallenge; the VGG network (Simonyan and Zisserman, 2014), which makes use of a num-\nber of repeating blocks of elements; the network in network (NiN) that convolves whole\nneural networks patch-wise over inputs (Lin et al., 2013); GoogLeNet that uses networks\nwith multi-branch convolutions (Szegedy et al., 2015); the residual network (ResNet) (He et\nal., 2016), which remains one of the most popular oﬀ-the-shelf architectures in computer\nvision; ResNeXt blocks (Xie et al., 2017) for sparser connections; and DenseNet (Huang et\nal., 2017) for a generalization of the residual architecture. Over time many special optimiza-\ntions for eﬃcient networks have been developed, such as coordinate shifts (ShiftNet) (Wu\net al., 2018). This culminated in the automatic search for eﬃcient architectures such as Mo-\nbileNet v3 (Howard et al., 2019). It also includes the semi-automatic design exploration of\n275\n\n276\nModern Convolutional Neural Networks\nRadosavovic et al. (2020) that led to the RegNetX/Y which we will discuss later in this chap-\nter. The work is instructive insofar as it oﬀers a path for marrying brute force computation\nwith the ingenuity of an experimenter in the search for eﬃcient design spaces. Of note is\nalso the work of Liu et al. (2022) as it shows that training techniques (e.g., optimizers, data\naugmentation, and regularization) play a pivotal role in improving accuracy. It also shows that\nlong-held assumptions, such as the size of a convolution window, may need to be revisited,\ngiven the increase in computation and data. We will cover this and many more questions in\ndue course throughout this chapter.\n8.1 Deep Convolutional Neural Networks (AlexNet)\nAlthough CNNs were well known in the computer vision and machine learning communities\nfollowing the introduction of LeNet (LeCun et al., 1995), they did not immediately dominate\nthe ﬁeld. Although LeNet achieved good results on early small datasets, the performance and\nfeasibility of training CNNs on larger, more realistic datasets had yet to be established. In\nfact, for much of the intervening time between the early 1990s and the watershed results\nof 2012 (Krizhevsky et al., 2012), neural networks were often surpassed by other machine\nlearning methods, such as kernel methods (Schölkopf and Smola, 2002), ensemble methods\n(Freund and Schapire, 1996), and structured estimation (Taskar et al., 2004).\nFor computer vision, this comparison is perhaps not entirely accurate. That is, although the\ninputs to convolutional networks consist of raw or lightly-processed (e.g., by centering) pixel\nvalues, practitioners would never feed raw pixels into traditional models. Instead, typical com-\nputer vision pipelines consisted of manually engineering feature extraction pipelines, such as\nSIFT (Lowe, 2004), SURF (Bay et al., 2006), and bags of visual words (Sivic and Zisser-\nman, 2003). Rather than learning the features, the features were crafted. Most of the progress\ncame from having more clever ideas for feature extraction on the one hand and deep insight\ninto geometry (Hartley and Zisserman, 2000) on the other. The learning algorithm was often\nconsidered an afterthought.\nAlthough some neural network accelerators were available in the 1990s, they were not yet\nsuﬃciently powerful to make deep multichannel, multilayer CNNs with a large number\nof parameters. For instance, NVIDIA’s GeForce 256 from 1999 was able to process at\nmost 480 million ﬂoating-point operations, such as additions and multiplications, per second\n(MFLOPS), without any meaningful programming framework for operations beyond games.\nToday’s accelerators are able to perform in excess of 1000 TFLOPs per device. Moreover,\ndatasets were still relatively small: OCR on 60,000 low-resolution 28 × 28 pixel images was\nconsidered a highly challenging task. Added to these obstacles, key tricks for training neu-\nral networks including parameter initialization heuristics (Glorot and Bengio, 2010), clever\nvariants of stochastic gradient descent (Kingma and Ba, 2014), non-squashing activation\nfunctions (Nair and Hinton, 2010), and eﬀective regularization techniques (Srivastava et al.,\n2014) were still missing.\n\n277\nDeep Convolutional Neural Networks (AlexNet)\n126\nThus, rather than training end-to-end (pixel to classiﬁcation) systems, classical pipelines looked\nmore like this:\n1. Obtain an interesting dataset. In the early days, these datasets required expensive sensors.\nFor instance, the Apple QuickTake 100 126 of 1994 sported a whopping 0.3 megapixel\n(VGA) resolution, capable of storing up to 8 images, all for the price of $1000.\n2. Preprocess the dataset with hand-crafted features based on some knowledge of optics,\ngeometry, other analytic tools, and occasionally on the serendipitous discoveries by lucky\ngraduate students.\n3. Feed the data through a standard set of feature extractors such as the SIFT (scale-invariant\nfeature transform) (Lowe, 2004), the SURF (speeded up robust features) (Bay et al.,\n2006), or any number of other hand-tuned pipelines. OpenCV still provides SIFT ex-\ntractors to this day!\n4. Dump the resulting representations into your favorite classiﬁer, likely a linear model or\nkernel method, to train a classiﬁer.\nIf you spoke to machine learning researchers, they would reply that machine learning was\nboth important and beautiful. Elegant theories proved the properties of various classiﬁers\n(Boucheron et al., 2005) and convex optimization (Boyd and Vandenberghe, 2004) had be-\ncome the mainstay for obtaining them. The ﬁeld of machine learning was thriving, rigorous,\nand eminently useful. However, if you spoke to a computer vision researcher, you would hear\na very diﬀerent story. The dirty truth of image recognition, they would tell you, is that fea-\ntures, geometry (Hartley and Zisserman, 2000, Hartley and Kahl, 2009), and engineering,\nrather than novel learning algorithms, drove progress. Computer vision researchers justiﬁ-\nably believed that a slightly bigger or cleaner dataset or a slightly improved feature-extraction\npipeline mattered far more to the ﬁnal accuracy than any learning algorithm.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n8.1.1 Representation Learning\nAnother way to cast the state of aﬀairs is that the most important part of the pipeline was\nthe representation. And up until 2012 the representation was calculated mostly mechanically.\nIn fact, engineering a new set of feature functions, improving results, and writing up the\nmethod all featured prominently in papers. SIFT (Lowe, 2004), SURF (Bay et al., 2006),\nHOG (histograms of oriented gradient) (Dalal and Triggs, 2005), bags of visual words (Sivic\nand Zisserman, 2003), and similar feature extractors ruled the roost.\nAnother group of researchers, including Yann LeCun, GeoﬀHinton, Yoshua Bengio, An-\ndrew Ng, Shun-ichi Amari, and Juergen Schmidhuber, had diﬀerent plans. They believed\nthat features themselves ought to be learned. Moreover, they believed that to be reasonably\n\n278\nModern Convolutional Neural Networks\ncomplex, the features ought to be hierarchically composed with multiple jointly learned lay-\ners, each with learnable parameters. In the case of an image, the lowest layers might come\nto detect edges, colors, and textures, by analogy with how the visual system in animals pro-\ncesses its input. In particular, the automatic design of visual features such as those obtained\nby sparse coding (Olshausen and Field, 1996) remained an open challenge until the advent\nof modern CNNs. It was not until Dean et al. (2012), Le (2013) that the idea of generating\nfeatures from image data automatically gained signiﬁcant traction.\nThe ﬁrst modern CNN (Krizhevsky et al., 2012), named AlexNet after one of its inventors,\nAlex Krizhevsky, is largely an evolutionary improvement over LeNet. It achieved excellent\nperformance in the 2012 ImageNet challenge.\nt\nFig. 8.1.1\nImage ﬁlters learned by the ﬁrst layer of AlexNet. Reproduction courtesy of Krizhevsky et\nal. (2012).\nInterestingly, in the lowest layers of the network, the model learned feature extractors that\nresembled some traditional ﬁlters. Fig. 8.1.1 shows lower-level image descriptors. Higher\nlayers in the network might build upon these representations to represent larger structures,\nlike eyes, noses, blades of grass, and so on. Even higher layers might represent whole objects\nlike people, airplanes, dogs, or frisbees. Ultimately, the ﬁnal hidden state learns a compact\nrepresentation of the image that summarizes its contents such that data belonging to diﬀerent\ncategories can be easily separated.\nAlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This begs\nthe question: why did it take so long? A key diﬀerence was that, over the previous two decades,\nthe amount of data and the computing power available had increased signiﬁcantly. As such\nAlexNet was much larger: it was trained on much more data, and on much faster GPUs\ncompared to the CPUs available in 1995.\n\n279\nDeep Convolutional Neural Networks (AlexNet)\nMissing Ingredient: Data\nDeep models with many layers require large amounts of data in order to enter the regime\nwhere they signiﬁcantly outperform traditional methods based on convex optimizations (e.g.,\nlinear and kernel methods). However, given the limited storage capacity of computers, the\nrelative expense of (imaging) sensors, and the comparatively tighter research budgets in the\n1990s, most research relied on tiny datasets. Numerous papers relied on the UCI collection\nof datasets, many of which contained only hundreds or (a few) thousands of images captured\nin low resolution and often with an artiﬁcially clean background.\nIn 2009, the ImageNet dataset was released (Deng et al., 2009), challenging researchers to\nlearn models from 1 million examples, 1000 each from 1000 distinct categories of objects.\nThe categories themselves were based on the most popular noun nodes in WordNet (Miller,\n1995). The ImageNet team used Google Image Search to preﬁlter large candidate sets for\neach category and employed the Amazon Mechanical Turk crowdsourcing pipeline to conﬁrm\nfor each image whether it belonged to the associated category. This scale was unprecedented,\nexceeding others by over an order of magnitude (e.g., CIFAR-100 has 60,000 images). An-\nother aspect was that the images were at relatively high resolution of 224 × 224 pixels, unlike\nthe 80 million-sized TinyImages dataset (Torralba et al., 2008), consisting of 32 × 32 pixel\nthumbnails. This allowed for the formation of higher-level features. The associated com-\npetition, dubbed the ImageNet Large Scale Visual Recognition Challenge (Russakovsky et\nal., 2015), pushed computer vision and machine learning research forward, challenging re-\nsearchers to identify which models performed best at a greater scale than academics had\npreviously considered. The largest vision datasets, such as LAION-5B (Schuhmann et al.,\n2022) contain billions of images with additional metadata.\nMissing Ingredient: Hardware\nDeep learning models are voracious consumers of compute cycles. Training can take hun-\ndreds of epochs, and each iteration requires passing data through many layers of compu-\ntationally expensive linear algebra operations. This is one of the main reasons why in the\n1990s and early 2000s, simple algorithms based on the more-eﬃciently optimized convex\nobjectives were preferred.\nGraphical processing units (GPUs) proved to be a game changer in making deep learning fea-\nsible. These chips had earlier been developed for accelerating graphics processing to beneﬁt\ncomputer games. In particular, they were optimized for high throughput 4 × 4 matrix–vector\nproducts, which are needed for many computer graphics tasks. Fortunately, the math is strik-\ningly similar to that required for calculating convolutional layers. Around that time, NVIDIA\nand ATI had begun optimizing GPUs for general computing operations (Fernando, 2004),\ngoing as far as to market them as general-purpose GPUs (GPGPUs).\nTo provide some intuition, consider the cores of a modern microprocessor (CPU). Each of\nthe cores is fairly powerful running at a high clock frequency and sporting large caches (up to\nseveral megabytes of L3). Each core is well-suited to executing a wide range of instructions,\n\n280\nModern Convolutional Neural Networks\n127\nwith branch predictors, a deep pipeline, specialized execution units, speculative execution,\nand many other bells and whistles that enable it to run a large variety of programs with\nsophisticated control ﬂow. This apparent strength, however, is also its Achilles heel: general-\npurpose cores are very expensive to build. They excel at general-purpose code with lots of\ncontrol ﬂow. This requires lots of chip area, not just for the actual ALU (arithmetic logical\nunit) where computation happens, but also for all the aforementioned bells and whistles, plus\nmemory interfaces, caching logic between cores, high-speed interconnects, and so on. CPUs\nare comparatively bad at any single task when compared with dedicated hardware. Modern\nlaptops have 4–8 cores, and even high-end servers rarely exceed 64 cores per socket, simply\nbecause it is not cost-eﬀective.\nBy comparison, GPUs can consist of thousands of small processing elements (NIVIDA’s lat-\nest Ampere chips have up to 6912 CUDA cores), often grouped into larger groups (NVIDIA\ncalls them warps). The details diﬀer somewhat between NVIDIA, AMD, ARM and other\nchip vendors. While each core is relatively weak, running at about 1GHz clock frequency, it is\nthe total number of such cores that makes GPUs orders of magnitude faster than CPUs. For\ninstance, NVIDIA’s recent Ampere A100 GPU oﬀers over 300 TFLOPs per chip for spe-\ncialized 16-bit precision (BFLOAT16) matrix-matrix multiplications, and up to 20 TFLOPs\nfor more general-purpose ﬂoating point operations (FP32). At the same time, ﬂoating point\nperformance of CPUs rarely exceeds 1 TFLOPs. For instance, Amazon’s Graviton 3 reaches\n2 TFLOPs peak performance for 16-bit precision operations, a number similar to the GPU\nperformance of Apple’s M1 processor.\nThere are many reasons why GPUs are much faster than CPUs in terms of FLOPs. First,\npower consumption tends to grow quadratically with clock frequency. Hence, for the power\nbudget of a CPU core that runs four times faster (a typical number), you can use 16 GPU\ncores at 1\n4 the speed, which yields 16 × 1\n4 = 4 times the performance. Second, GPU cores\nare much simpler (in fact, for a long time they were not even able to execute general-purpose\ncode), which makes them more energy eﬃcient. For instance, (i) they tend not to support\nspeculative evaluation, (ii) it typically is not possible to program each processing element\nindividually, and (iii) the caches per core tend to be much smaller. Last, many operations in\ndeep learning require high memory bandwidth. Again, GPUs shine here with buses that are\nat least 10 times as wide as many CPUs.\nBack to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever im-\nplemented a deep CNN that could run on GPUs. They realized that the computational bot-\ntlenecks in CNNs, convolutions and matrix multiplications, are all operations that could be\nparallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, either of\nwhich was capable of 1.5 TFLOPs (still a challenge for most CPUs a decade later), they\nimplemented fast convolutions. The cuda-convnet127 code was good enough that for several\nyears it was the industry standard and powered the ﬁrst couple of years of the deep learning\nboom.\n8.1.2 AlexNet\n\n281\nDeep Convolutional Neural Networks (AlexNet)\nAlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recog-\nnition Challenge 2012 by a large margin (Russakovsky et al., 2013). This network showed,\nfor the ﬁrst time, that the features obtained by learning can transcend manually-designed\nfeatures, breaking the previous paradigm in computer vision.\nThe architectures of AlexNet and LeNet are strikingly similar, as Fig. 8.1.2 illustrates. Note\nthat we provide a slightly streamlined version of AlexNet removing some of the design quirks\nthat were needed in 2012 to make the model ﬁt on two small GPUs.\nt\nFig. 8.1.2\nFrom LeNet (left) to AlexNet (right).\nThere are also signiﬁcant diﬀerences between AlexNet and LeNet. First, AlexNet is much\ndeeper than the comparatively small LeNet-5. AlexNet consists of eight layers: ﬁve convolu-\ntional layers, two fully connected hidden layers, and one fully connected output layer. Second,\nAlexNet used the ReLU instead of the sigmoid as its activation function. Let’s delve into the\ndetails below.\nArchitecture\nIn AlexNet’s ﬁrst layer, the convolution window shape is 11 × 11. Since the images in Ima-\ngeNet are eight times taller and wider than the MNIST images, objects in ImageNet data tend\nto occupy more pixels with more visual detail. Consequently, a larger convolution window is\n\n282\nModern Convolutional Neural Networks\nneeded to capture the object. The convolution window shape in the second layer is reduced to\n5 × 5, followed by 3 × 3. In addition, after the ﬁrst, second, and ﬁfth convolutional layers, the\nnetwork adds max-pooling layers with a window shape of 3 × 3 and a stride of 2. Moreover,\nAlexNet has ten times more convolution channels than LeNet.\nAfter the ﬁnal convolutional layer, there are two huge fully connected layers with 4096 out-\nputs. These layers require nearly 1GB model parameters. Because of the limited memory in\nearly GPUs, the original AlexNet used a dual data stream design, so that each of their two\nGPUs could be responsible for storing and computing only its half of the model. Fortunately,\nGPU memory is comparatively abundant now, so we rarely need to break up models across\nGPUs these days (our version of the AlexNet model deviates from the original paper in this\naspect).\nActivation Functions\nFurthermore, AlexNet changed the sigmoid activation function to a simpler ReLU activa-\ntion function. On the one hand, the computation of the ReLU activation function is simpler.\nFor example, it does not have the exponentiation operation found in the sigmoid activation\nfunction. On the other hand, the ReLU activation function makes model training easier when\nusing diﬀerent parameter initialization methods. This is because, when the output of the sig-\nmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so\nthat backpropagation cannot continue to update some of the model parameters. By contrast,\nthe gradient of the ReLU activation function in the positive interval is always 1 (Section\n5.1.2). Therefore, if the model parameters are not properly initialized, the sigmoid function\nmay obtain a gradient of almost 0 in the positive interval, meaning that the model cannot be\neﬀectively trained.\nCapacity Control and Preprocessing\nAlexNet controls the model complexity of the fully connected layer by dropout (Section\n5.6), while LeNet only uses weight decay. To augment the data even further, the training\nloop of AlexNet added a great deal of image augmentation, such as ﬂipping, clipping, and\ncolor changes. This makes the model more robust and the larger sample size eﬀectively re-\nduces overﬁtting. See Buslaev et al. (2020) for an in-depth review of such preprocessing\nsteps.\nclass AlexNet(d2l.Classifier):\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(\nnn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),\nnn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),\nnn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),\n(continues on next page)\n\n283\nDeep Convolutional Neural Networks (AlexNet)\n(continued from previous page)\nnn.MaxPool2d(kernel_size=3, stride=2),\nnn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\nnn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),\nnn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),\nnn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\nnn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),\nnn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),\nnn.LazyLinear(num_classes))\nself.net.apply(d2l.init_cnn)\nWe construct a single-channel data example with both height and width of 224 to observe the\noutput shape of each layer. It matches the AlexNet architecture in Fig. 8.1.2.\nAlexNet().layer_summary((1, 1, 224, 224))\nConv2d output shape:\ntorch.Size([1, 96, 54, 54])\nReLU output shape:\ntorch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\ntorch.Size([1, 96, 26, 26])\nConv2d output shape:\ntorch.Size([1, 256, 26, 26])\nReLU output shape:\ntorch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\ntorch.Size([1, 256, 12, 12])\nConv2d output shape:\ntorch.Size([1, 384, 12, 12])\nReLU output shape:\ntorch.Size([1, 384, 12, 12])\nConv2d output shape:\ntorch.Size([1, 384, 12, 12])\nReLU output shape:\ntorch.Size([1, 384, 12, 12])\nConv2d output shape:\ntorch.Size([1, 256, 12, 12])\nReLU output shape:\ntorch.Size([1, 256, 12, 12])\nMaxPool2d output shape:\ntorch.Size([1, 256, 5, 5])\nFlatten output shape:\ntorch.Size([1, 6400])\nLinear output shape:\ntorch.Size([1, 4096])\nReLU output shape:\ntorch.Size([1, 4096])\nDropout output shape:\ntorch.Size([1, 4096])\nLinear output shape:\ntorch.Size([1, 4096])\nReLU output shape:\ntorch.Size([1, 4096])\nDropout output shape:\ntorch.Size([1, 4096])\nLinear output shape:\ntorch.Size([1, 10])\n8.1.3 Training\nAlthough AlexNet was trained on ImageNet in Krizhevsky et al. (2012), we use Fashion-\nMNIST here since training an ImageNet model to convergence could take hours or days even\non a modern GPU. One of the problems with applying AlexNet directly on Fashion-MNIST\nis that its images have lower resolution (28×28 pixels) than ImageNet images. To make things\nwork, we upsample them to 224 × 224. This is generally not a smart practice, as it simply\nincreases the computational complexity without adding information. Nonetheless, we do it\nhere to be faithful to the AlexNet architecture. We perform this resizing with the resize\nargument in the d2l.FashionMNIST constructor.\nNow, we can start training AlexNet. Compared to LeNet in Section 7.6, the main change\n\n284\nModern Convolutional Neural Networks\nhere is the use of a smaller learning rate and much slower training due to the deeper and\nwider network, the higher image resolution, and the more costly convolutions.\nmodel = AlexNet(lr=0.01)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ntrainer.fit(model, data)\n8.1.4 Discussion\nAlexNet’s structure bears a striking resemblance to LeNet, with a number of critical im-\nprovements, both for accuracy (dropout) and for ease of training (ReLU). What is equally\nstriking is the amount of progress that has been made in terms of deep learning tooling. What\nwas several months of work in 2012 can now be accomplished in a dozen lines of code using\nany modern framework.\nReviewing the architecture, we see that AlexNet has an Achilles heel when it comes to ef-\nﬁciency: the last two hidden layers require matrices of size 6400 × 4096 and 4096 × 4096,\nrespectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both\nof which are a nontrivial outlay, especially on smaller devices, such as mobile phones. This\nis one of the reasons why AlexNet has been surpassed by much more eﬀective architectures\nthat we will cover in the following sections. Nonetheless, it is a key step from shallow to deep\nnetworks that are used nowadays. Note that even though the number of parameters exceeds\nby far the amount of training data in our experiments (the last two layers have more than 40\nmillion parameters, trained on a datasets of 60 thousand images), there is hardly any overﬁt-\nting: training and validation loss are virtually identical throughout training. This is due to the\nimproved regularization, such as dropout, inherent in modern deep network designs.\nAlthough it seems that there are only a few more lines in AlexNet’s implementation than\nin LeNet’s, it took the academic community many years to embrace this conceptual change\nand take advantage of its excellent experimental results. This was also due to the lack of\neﬃcient computational tools. At the time neither DistBelief (Dean et al., 2012) nor Caﬀe\n(Jia et al., 2014) existed, and Theano (Bergstra et al., 2010) still lacked many distinguishing\n\n285\nDeep Convolutional Neural Networks (AlexNet)\n128\nfeatures. It was the availability of TensorFlow (Abadi et al., 2016) that dramatically changed\nthe situation.\n8.1.5 Exercises\n1. Following up on the discussion above, analyze the computational properties of AlexNet.\n1. Compute the memory footprint for convolutions and fully connected layers, respec-\ntively. Which one dominates?\n2. Calculate the computational cost for the convolutions and the fully connected layers.\n3. How does the memory (read and write bandwidth, latency, size) aﬀect computation?\nIs there any diﬀerence in its eﬀects for training and inference?\n2. You are a chip designer and need to trade oﬀcomputation and memory bandwidth. For\nexample, a faster chip requires more power and possibly a larger chip area. More mem-\nory bandwidth requires more pins and control logic, thus also more area. How do you\noptimize?\n3. Why do engineers no longer report performance benchmarks on AlexNet?\n4. Try increasing the number of epochs when training AlexNet. Compared with LeNet, how\ndo the results diﬀer? Why?\n5. AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the low\nresolution of the initial images.\n1. Try simplifying the model to make the training faster, while ensuring that the accuracy\ndoes not drop signiﬁcantly.\n2. Design a better model that works directly on 28 × 28 images.\n6. Modify the batch size, and observe the changes in throughput (images/s), accuracy, and\nGPU memory.\n7. Apply dropout and ReLU to LeNet-5. Does it improve? Can you improve things further\nby preprocessing to take advantage of the invariances inherent in the images?\n8. Can you make AlexNet overﬁt? Which feature do you need to remove or change to break\ntraining?\nDiscussions128.\n\n286\nModern Convolutional Neural Networks\n8.2 Networks Using Blocks (VGG)\nWhile AlexNet oﬀered empirical evidence that deep CNNs can achieve good results, it did\nnot provide a general template to guide subsequent researchers in designing new networks. In\nthe following sections, we will introduce several heuristic concepts commonly used to design\ndeep networks.\nProgress in this ﬁeld mirrors that of VLSI (very large scale integration) in chip design where\nengineers moved from placing transistors to logical elements to logic blocks (Mead, 1980).\nSimilarly, the design of neural network architectures has grown progressively more abstract,\nwith researchers moving from thinking in terms of individual neurons to whole layers, and\nnow to blocks, repeating patterns of layers. A decade later, this has now progressed to re-\nsearchers using entire trained models to repurpose them for diﬀerent, albeit related, tasks.\nSuch large pretrained models are typically called foundation models (Bommasani et al., 2021).\nBack to network design. The idea of using blocks ﬁrst emerged from the Visual Geometry\nGroup (VGG) at Oxford University, in their eponymously-named VGG network (Simonyan\nand Zisserman, 2014). It is easy to implement these repeated structures in code with any\nmodern deep learning framework by using loops and subroutines.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n8.2.1 VGG Blocks\nThe basic building block of CNNs is a sequence of the following: (i) a convolutional layer\nwith padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling\nlayer such as max-pooling to reduce the resolution. One of the problems with this approach\nis that the spatial resolution decreases quite rapidly. In particular, this imposes a hard limit\nof log2 d convolutional layers on the network before all dimensions (d) are used up. For\ninstance, in the case of ImageNet, it would be impossible to have more than 8 convolutional\nlayers in this way.\nThe key idea of Simonyan and Zisserman (2014) was to use multiple convolutions in between\ndownsampling via max-pooling in the form of a block. They were primarily interested in\nwhether deep or wide networks perform better. For instance, the successive application of\ntwo 3×3 convolutions touches the same pixels as a single 5×5 convolution does. At the same\ntime, the latter uses approximately as many parameters (25·c2) as three 3×3 convolutions do\n(3·9·c2). In a rather detailed analysis they showed that deep and narrow networks signiﬁcantly\noutperform their shallow counterparts. This set deep learning on a quest for ever deeper\nnetworks with over 100 layers for typical applications. Stacking 3×3 convolutions has become\na gold standard in later deep networks (a design decision only to be revisited recently by Liu et\n\n287\nNetworks Using Blocks (VGG)\nal. (2022)). Consequently, fast implementations for small convolutions have become a staple\non GPUs (Lavin and Gray, 2016).\nBack to VGG: a VGG block consists of a sequence of convolutions with 3 × 3 kernels with\npadding of 1 (keeping height and width) followed by a 2×2 max-pooling layer with stride of\n2 (halving height and width after each block). In the code below, we deﬁne a function called\nvgg_block to implement one VGG block.\nThe function below takes two arguments, corresponding to the number of convolutional layers\nnum_convs and the number of output channels num_channels.\ndef vgg_block(num_convs, out_channels):\nlayers = []\nfor _ in range(num_convs):\nlayers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\nlayers.append(nn.ReLU())\nlayers.append(nn.MaxPool2d(kernel_size=2,stride=2))\nreturn nn.Sequential(*layers)\n8.2.2 VGG Network\nLike AlexNet and LeNet, the VGG Network can be partitioned into two parts: the ﬁrst con-\nsisting mostly of convolutional and pooling layers and the second consisting of fully connected\nlayers that are identical to those in AlexNet. The key diﬀerence is that the convolutional layers\nare grouped in nonlinear transformations that leave the dimensonality unchanged, followed\nby a resolution-reduction step, as depicted in Fig. 8.2.1.\nThe convolutional part of the network connects several VGG blocks from Fig. 8.2.1 (also\ndeﬁned in the vgg_block function) in succession. This grouping of convolutions is a pat-\ntern that has remained almost unchanged over the past decade, although the speciﬁc choice\nof operations has undergone considerable modiﬁcations. The variable arch consists of a list\nof tuples (one per block), where each contains two values: the number of convolutional lay-\ners and the number of output channels, which are precisely the arguments required to call\nthe vgg_block function. As such, VGG deﬁnes a family of networks rather than just a spe-\nciﬁc manifestation. To build a speciﬁc network we simply iterate over arch to compose the\nblocks.\nclass VGG(d2l.Classifier):\ndef __init__(self, arch, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\nconv_blks = []\nfor (num_convs, out_channels) in arch:\nconv_blks.append(vgg_block(num_convs, out_channels))\nself.net = nn.Sequential(\n*conv_blks, nn.Flatten(),\nnn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\nnn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n(continues on next page)\n\n288\nModern Convolutional Neural Networks\nt\nFig. 8.2.1\nFrom AlexNet to VGG. The key difference is that VGG consists of blocks of layers,\nwhereas AlexNet’s layers are all designed individually.\n(continued from previous page)\nnn.LazyLinear(num_classes))\nself.net.apply(d2l.init_cnn)\nThe original VGG network had ﬁve convolutional blocks, among which the ﬁrst two have\none convolutional layer each and the latter three contain two convolutional layers each. The\nﬁrst block has 64 output channels and each subsequent block doubles the number of output\nchannels, until that number reaches 512. Since this network uses eight convolutional layers\nand three fully connected layers, it is often called VGG-11.\nVGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n(1, 1, 224, 224))\nSequential output shape:\ntorch.Size([1, 64, 112, 112])\nSequential output shape:\ntorch.Size([1, 128, 56, 56])\nSequential output shape:\ntorch.Size([1, 256, 28, 28])\nSequential output shape:\ntorch.Size([1, 512, 14, 14])\nSequential output shape:\ntorch.Size([1, 512, 7, 7])\nFlatten output shape:\ntorch.Size([1, 25088])\nLinear output shape:\ntorch.Size([1, 4096])\nReLU output shape:\ntorch.Size([1, 4096])\n(continues on next page)\n\n289\nNetworks Using Blocks (VGG)\n(continued from previous page)\nDropout output shape:\ntorch.Size([1, 4096])\nLinear output shape:\ntorch.Size([1, 4096])\nReLU output shape:\ntorch.Size([1, 4096])\nDropout output shape:\ntorch.Size([1, 4096])\nLinear output shape:\ntorch.Size([1, 10])\nAs you can see, we halve height and width at each block, ﬁnally reaching a height and width\nof 7 before ﬂattening the representations for processing by the fully connected part of the\nnetwork. Simonyan and Zisserman (2014) described several other variants of VGG. In fact,\nit has become the norm to propose families of networks with diﬀerent speed–accuracy trade-\noﬀwhen introducing a new architecture.\n8.2.3 Training\nSince VGG-11 is computationally more demanding than AlexNet we construct a network\nwith a smaller number of channels. This is more than suﬃcient for training on Fashion-\nMNIST. The model training process is similar to that of AlexNet in Section 8.1. Again ob-\nserve the close match between validation and training loss, suggesting only a small amount\nof overﬁtting.\nmodel = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n8.2.4 Summary\nOne might argue that VGG is the ﬁrst truly modern convolutional neural network. While\nAlexNet introduced many of the components of what make deep learning eﬀective at scale, it\nis VGG that arguably introduced key properties such as blocks of multiple convolutions and a\n\n290\nModern Convolutional Neural Networks\n129\npreference for deep and narrow networks. It is also the ﬁrst network that is actually an entire\nfamily of similarly parametrized models, giving the practitioner ample trade-oﬀbetween\ncomplexity and speed. This is also the place where modern deep learning frameworks shine.\nIt is no longer necessary to generate XML conﬁguration ﬁles to specify a network but rather,\nto assemble said networks through simple Python code.\nMore recently ParNet (Goyal et al., 2021) demonstrated that it is possible to achieve compet-\nitive performance using a much more shallow architecture through a large number of parallel\ncomputations. This is an exciting development and there is hope that it will inﬂuence archi-\ntecture designs in the future. For the remainder of the chapter, though, we will follow the\npath of scientiﬁc progress over the past decade.\n8.2.5 Exercises\n1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs\nmore GPU memory.\n1. Compare the number of parameters needed for AlexNet and VGG.\n2. Compare the number of ﬂoating point operations used in the convolutional layers and\nin the fully connected layers.\n3. How could you reduce the computational cost created by the fully connected layers?\n2. When displaying the dimensions associated with the various layers of the network, we\nonly see the information associated with eight blocks (plus some auxiliary transforms),\neven though the network has 11 layers. Where did the remaining three layers go?\n3. Use Table 1 in the VGG paper (Simonyan and Zisserman, 2014) to construct other com-\nmon models, such as VGG-16 or VGG-19.\n4. Upsampling the resolution in Fashion-MNIST eight-fold from 28×28 to 224×224 dimen-\nsions is very wasteful. Try modifying the network architecture and resolution conversion,\ne.g., to 56 or to 84 dimensions for its input instead. Can you do so without reducing the\naccuracy of the network? Consult the VGG paper (Simonyan and Zisserman, 2014) for\nideas on adding more nonlinearities prior to downsampling.\nDiscussions129.\n8.3 Network in Network (NiN)\nLeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting\nspatial structure via a sequence of convolutions and pooling layers and post-process the rep-\n\n291\nNetwork in Network (NiN)\nresentations via fully connected layers. The improvements upon LeNet by AlexNet and VGG\nmainly lie in how these later networks widen and deepen these two modules.\nThis design poses two major challenges. First, the fully connected layers at the end of the\narchitecture consume tremendous numbers of parameters. For instance, even a simple model\nsuch as VGG-11 requires a monstrous matrix, occupying almost 400MB of RAM in single\nprecision (FP32). This is a signiﬁcant impediment to computation, in particular on mobile\nand embedded devices. After all, even high-end mobile phones sport no more than 8GB of\nRAM. At the time VGG was invented, this was an order of magnitude less (the iPhone 4S had\n512MB). As such, it would have been diﬃcult to justify spending the majority of memory on\nan image classiﬁer.\nSecond, it is equally impossible to add fully connected layers earlier in the network to increase\nthe degree of nonlinearity: doing so would destroy the spatial structure and require potentially\neven more memory.\nThe network in network (NiN) blocks (Lin et al., 2013) oﬀer an alternative, capable of solving\nboth problems in one simple strategy. They were proposed based on a very simple insight: (i)\nuse 1 × 1 convolutions to add local nonlinearities across the channel activations and (ii) use\nglobal average pooling to integrate across all locations in the last representation layer. Note\nthat global average pooling would not be eﬀective, were it not for the added nonlinearities.\nLet’s dive into this in detail.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n8.3.1 NiN Blocks\nRecall Section 7.4.3. In it we said that the inputs and outputs of convolutional layers consist\nof four-dimensional tensors with axes corresponding to the example, channel, height, and\nwidth. Also recall that the inputs and outputs of fully connected layers are typically two-\ndimensional tensors corresponding to the example and feature. The idea behind NiN is to\napply a fully connected layer at each pixel location (for each height and width). The resulting\n1 × 1 convolution can be thought of as a fully connected layer acting independently on each\npixel location.\nFig. 8.3.1 illustrates the main structural diﬀerences between VGG and NiN, and their blocks.\nNote both the diﬀerence in the NiN blocks (the initial convolution is followed by 1 × 1\nconvolutions, whereas VGG retains 3 × 3 convolutions) and at the end where we no longer\nrequire a giant fully connected layer.\ndef nin_block(out_channels, kernel_size, strides, padding):\nreturn nn.Sequential(\nnn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\nnn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\nnn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())\n\n292\nModern Convolutional Neural Networks\nt\nFig. 8.3.1\nComparing the architectures of VGG and NiN, and of their blocks.\n8.3.2 NiN Model\nNiN uses the same initial convolution sizes as AlexNet (it was proposed shortly thereafter).\nThe kernel sizes are 11×11, 5×5, and 3×3, respectively, and the numbers of output channels\nmatch those of AlexNet. Each NiN block is followed by a max-pooling layer with a stride of\n2 and a window shape of 3 × 3.\nThe second signiﬁcant diﬀerence between NiN and both AlexNet and VGG is that NiN avoids\nfully connected layers altogether. Instead, NiN uses a NiN block with a number of output\nchannels equal to the number of label classes, followed by a global average pooling layer,\nyielding a vector of logits. This design signiﬁcantly reduces the number of required model\nparameters, albeit at the expense of a potential increase in training time.\nclass NiN(d2l.Classifier):\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\n(continues on next page)\n\n293\nNetwork in Network (NiN)\n(continued from previous page)\nself.net = nn.Sequential(\nnin_block(96, kernel_size=11, strides=4, padding=0),\nnn.MaxPool2d(3, stride=2),\nnin_block(256, kernel_size=5, strides=1, padding=2),\nnn.MaxPool2d(3, stride=2),\nnin_block(384, kernel_size=3, strides=1, padding=1),\nnn.MaxPool2d(3, stride=2),\nnn.Dropout(0.5),\nnin_block(num_classes, kernel_size=3, strides=1, padding=1),\nnn.AdaptiveAvgPool2d((1, 1)),\nnn.Flatten())\nself.net.apply(d2l.init_cnn)\nWe create a data example to see the output shape of each block.\nNiN().layer_summary((1, 1, 224, 224))\nSequential output shape:\ntorch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\ntorch.Size([1, 96, 26, 26])\nSequential output shape:\ntorch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\ntorch.Size([1, 256, 12, 12])\nSequential output shape:\ntorch.Size([1, 384, 12, 12])\nMaxPool2d output shape:\ntorch.Size([1, 384, 5, 5])\nDropout output shape:\ntorch.Size([1, 384, 5, 5])\nSequential output shape:\ntorch.Size([1, 10, 5, 5])\nAdaptiveAvgPool2d output shape:\ntorch.Size([1, 10, 1, 1])\nFlatten output shape:\ntorch.Size([1, 10])\n8.3.3 Training\nAs before we use Fashion-MNIST to train the model using the same optimizer that we used\nfor AlexNet and VGG.\nmodel = NiN(lr=0.05)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n8.3.4 Summary\nNiN has dramatically fewer parameters than AlexNet and VGG. This stems primarily from\nthe fact that it needs no giant fully connected layers. Instead, it uses global average pooling\nto aggregate across all image locations after the last stage of the network body. This obviates\nthe need for expensive (learned) reduction operations and replaces them by a simple average.\nWhat surprised researchers at the time was the fact that this averaging operation did not harm\n\n294\nModern Convolutional Neural Networks\n130\naccuracy. Note that averaging across a low-resolution representation (with many channels)\nalso adds to the amount of translation invariance that the network can handle.\nChoosing fewer convolutions with wide kernels and replacing them by 1×1 convolutions aids\nthe quest for fewer parameters further. It can cater for a signiﬁcant amount of nonlinearity\nacross channels within any given location. Both 1×1 convolutions and global average pooling\nsigniﬁcantly inﬂuenced subsequent CNN designs.\n8.3.5 Exercises\n1. Why are there two 1 × 1 convolutional layers per NiN block? Increase their number to\nthree. Reduce their number to one. What changes?\n2. What changes if you replace the 1 × 1 convolutions by 3 × 3 convolutions?\n3. What happens if you replace the global average pooling by a fully connected layer (speed,\naccuracy, number of parameters)?\n4. Calculate the resource usage for NiN.\n1. What is the number of parameters?\n2. What is the amount of computation?\n3. What is the amount of memory needed during training?\n4. What is the amount of memory needed during prediction?\n5. What are possible problems with reducing the 384 × 5 × 5 representation to a 10 × 5 × 5\nrepresentation in one step?\n6. Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19\nto design a family of NiN-like networks.\nDiscussions130.\n\n295\nMulti-Branch Networks (GoogLeNet)\n8.4 Multi-Branch Networks (GoogLeNet)\nIn 2014, GoogLeNet won the ImageNet Challenge (Szegedy et al., 2015), using a structure\nthat combined the strengths of NiN (Lin et al., 2013), repeated blocks (Simonyan and Zis-\nserman, 2014), and a cocktail of convolution kernels. It was arguably also the ﬁrst network\nthat exhibited a clear distinction among the stem (data ingest), body (data processing), and\nhead (prediction) in a CNN. This design pattern has persisted ever since in the design of\ndeep networks: the stem is given by the ﬁrst two or three convolutions that operate on the im-\nage. They extract low-level features from the underlying images. This is followed by a body\nof convolutional blocks. Finally, the head maps the features obtained so far to the required\nclassiﬁcation, segmentation, detection, or tracking problem at hand.\nThe key contribution in GoogLeNet was the design of the network body. It solved the problem\nof selecting convolution kernels in an ingenious way. While other works tried to identify which\nconvolution, ranging from 1×1 to 11×11 would be best, it simply concatenated multi-branch\nconvolutions. In what follows we introduce a slightly simpliﬁed version of GoogLeNet: the\noriginal design included a number of tricks for stabilizing training through intermediate loss\nfunctions, applied to multiple layers of the network. They are no longer necessary due to the\navailability of improved training algorithms.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n8.4.1 Inception Blocks\nThe basic convolutional block in GoogLeNet is called an Inception block, stemming from the\nmeme “we need to go deeper” from the movie Inception.\nt\nFig. 8.4.1\nStructure of the Inception block.\nAs depicted in Fig. 8.4.1, the inception block consists of four parallel branches. The ﬁrst\nthree branches use convolutional layers with window sizes of 1 × 1, 3 × 3, and 5 × 5 to\nextract information from diﬀerent spatial sizes. The middle two branches also add a 1 × 1\n\n296\nModern Convolutional Neural Networks\nconvolution of the input to reduce the number of channels, reducing the model’s complexity.\nThe fourth branch uses a 3 × 3 max-pooling layer, followed by a 1 × 1 convolutional layer\nto change the number of channels. The four branches all use appropriate padding to give\nthe input and output the same height and width. Finally, the outputs along each branch are\nconcatenated along the channel dimension and comprise the block’s output. The commonly-\ntuned hyperparameters of the Inception block are the number of output channels per layer,\ni.e., how to allocate capacity among convolutions of diﬀerent size.\nclass Inception(nn.Module):\n# c1--c4 are the number of output channels for each branch\ndef __init__(self, c1, c2, c3, c4, **kwargs):\nsuper(Inception, self).__init__(**kwargs)\n# Branch 1\nself.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n# Branch 2\nself.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\nself.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n# Branch 3\nself.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\nself.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n# Branch 4\nself.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\nself.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\ndef forward(self, x):\nb1 = F.relu(self.b1_1(x))\nb2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\nb3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\nb4 = F.relu(self.b4_2(self.b4_1(x)))\nreturn torch.cat((b1, b2, b3, b4), dim=1)\nTo gain some intuition for why this network works so well, consider the combination of the\nﬁlters. They explore the image in a variety of ﬁlter sizes. This means that details at diﬀerent\nextents can be recognized eﬃciently by ﬁlters of diﬀerent sizes. At the same time, we can\nallocate diﬀerent amounts of parameters for diﬀerent ﬁlters.\n8.4.2 GoogLeNet Model\nAs shown in Fig. 8.4.2, GoogLeNet uses a stack of a total of 9 inception blocks, arranged into\nthree groups with max-pooling in between, and global average pooling in its head to generate\nits estimates. Max-pooling between inception blocks reduces the dimensionality. At its stem,\nthe ﬁrst module is similar to AlexNet and LeNet.\nWe can now implement GoogLeNet piece by piece. Let’s begin with the stem. The ﬁrst mod-\nule uses a 64-channel 7 × 7 convolutional layer.\nclass GoogleNet(d2l.Classifier):\ndef b1(self):\nreturn nn.Sequential(\n(continues on next page)\n\n297\nMulti-Branch Networks (GoogLeNet)\nt\nFig. 8.4.2\nThe GoogLeNet architecture.\n(continued from previous page)\nnn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\nnn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nThe second module uses two convolutional layers: ﬁrst, a 64-channel 1×1 convolutional layer,\nfollowed by a 3 × 3 convolutional layer that triples the number of channels. This corresponds\nto the second branch in the Inception block and concludes the design of the body. At this\npoint we have 192 channels.\n@d2l.add_to_class(GoogleNet)\ndef b2(self):\nreturn nn.Sequential(\nnn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\nnn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\nnn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nThe third module connects two complete Inception blocks in series. The number of output\nchannels of the ﬁrst Inception block is 64+128+32+32 = 256. This amounts to a ratio of\nthe number of output channels among the four branches of 2 : 4 : 1 : 1. To achieve this, we\nﬁrst reduce the input dimensions by 1\n2 and by\n1\n12 in the second and third branch respectively\nto arrive at 96 = 192/2 and 16 = 192/12 channels respectively.\nThe number of output channels of the second Inception block is increased to 128 + 192 +\n96 + 64 = 480, yielding a ratio of 128 : 192 : 96 : 64 = 4 : 6 : 3 : 2. As before, we need\nto reduce the number of intermediate dimensions in the second and third channel. A scale of\n1\n2 and 1\n8 respectively suﬃces, yielding 128 and 32 channels respectively. This is captured by\nthe arguments of the following Inception block constructors.\n@d2l.add_to_class(GoogleNet)\ndef b3(self):\nreturn nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\nInception(128, (128, 192), (32, 96), 64),\nnn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nThe fourth module is more complicated. It connects ﬁve Inception blocks in series, and they\nhave 192+208+48+64 = 512, 160+224+64+64 = 512, 128+256+64+64 = 512,\n112+288+64+64 = 528, and 256+320+128+128 = 832 output channels, respectively.\nThe number of channels assigned to these branches is similar to that in the third module: the\n\n298\nModern Convolutional Neural Networks\nsecond branch with the 3 × 3 convolutional layer outputs the largest number of channels,\nfollowed by the ﬁrst branch with only the 1 × 1 convolutional layer, the third branch with\nthe 5 × 5 convolutional layer, and the fourth branch with the 3 × 3 max-pooling layer. The\nsecond and third branches will ﬁrst reduce the number of channels according to the ratio.\nThese ratios are slightly diﬀerent in diﬀerent Inception blocks.\n@d2l.add_to_class(GoogleNet)\ndef b4(self):\nreturn nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\nInception(160, (112, 224), (24, 64), 64),\nInception(128, (128, 256), (24, 64), 64),\nInception(112, (144, 288), (32, 64), 64),\nInception(256, (160, 320), (32, 128), 128),\nnn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nThe ﬁfth module has two Inception blocks with 256 + 320 + 128 + 128 = 832 and 384 +\n384 + 128 + 128 = 1024 output channels. The number of channels assigned to each branch\nis the same as that in the third and fourth modules, but diﬀers in speciﬁc values. It should be\nnoted that the ﬁfth block is followed by the output layer. This block uses the global average\npooling layer to change the height and width of each channel to 1, just as in NiN. Finally,\nwe turn the output into a two-dimensional array followed by a fully connected layer whose\nnumber of outputs is the number of label classes.\n@d2l.add_to_class(GoogleNet)\ndef b5(self):\nreturn nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\nInception(384, (192, 384), (48, 128), 128),\nnn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\nNow that we deﬁned all blocks b1 through b5, it is just a matter of assembling them all into\na full network.\n@d2l.add_to_class(GoogleNet)\ndef __init__(self, lr=0.1, num_classes=10):\nsuper(GoogleNet, self).__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\nself.b5(), nn.LazyLinear(num_classes))\nself.net.apply(d2l.init_cnn)\nThe GoogLeNet model is computationally complex. Note the large number of relatively arbi-\ntrary hyperparameters in terms of the number of channels chosen, the number of blocks prior\nto dimensionality reduction, the relative partitioning of capacity across channels, etc. Much\nof it is due to the fact that at the time when GoogLeNet was introduced, automatic tools for\nnetwork deﬁnition or design exploration were not yet available. For instance, by now we take\nit for granted that a competent deep learning framework is capable of inferring dimensional-\nities of input tensors automatically. At the time, many such conﬁgurations had to be speciﬁed\nexplicitly by the experimenter, thus often slowing down active experimentation. Moreover,\n\n299\nMulti-Branch Networks (GoogLeNet)\nthe tools needed for automatic exploration were still in ﬂux and initial experiments largely\namounted to costly brute-force exploration, genetic algorithms, and similar strategies.\nFor now the only modiﬁcation we will carry out is to reduce the input height and width\nfrom 224 to 96 to have a reasonable training time on Fashion-MNIST. This simpliﬁes the\ncomputation. Let’s have a look at the changes in the shape of the output between the various\nmodules.\nmodel = GoogleNet().layer_summary((1, 1, 96, 96))\nSequential output shape:\ntorch.Size([1, 64, 24, 24])\nSequential output shape:\ntorch.Size([1, 192, 12, 12])\nSequential output shape:\ntorch.Size([1, 480, 6, 6])\nSequential output shape:\ntorch.Size([1, 832, 3, 3])\nSequential output shape:\ntorch.Size([1, 1024])\nLinear output shape:\ntorch.Size([1, 10])\n8.4.3 Training\nAs before, we train our model using the Fashion-MNIST dataset. We transform it to 96 × 96\npixel resolution before invoking the training procedure.\nmodel = GoogleNet(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n8.4.4 Discussion\nA key feature of GoogLeNet is that it is actually cheaper to compute than its predecessors\nwhile simultaneously providing improved accuracy. This marks the beginning of a much more\ndeliberate network design that trades oﬀthe cost of evaluating a network with a reduction in\n\n300\nModern Convolutional Neural Networks\n131\nerrors. It also marks the beginning of experimentation at a block level with network design\nhyperparameters, even though it was entirely manual at the time. We will revisit this topic in\nSection 8.8 when discussing strategies for network structure exploration.\nOver the following sections we will encounter a number of design choices (e.g., batch nor-\nmalization, residual connections, and channel grouping) that allow us to improve networks\nsigniﬁcantly. For now, you can be proud to have implemented what is arguably the ﬁrst truly\nmodern CNN.\n8.4.5 Exercises\n1. GoogLeNet was so successful that it went through a number of iterations, progressively\nimproving speed and accuracy. Try to implement and run some of them. They include the\nfollowing:\n1. Add a batch normalization layer (Ioﬀe and Szegedy, 2015), as described later in Section\n8.5.\n2. Make adjustments to the Inception block (width, choice and order of convolutions), as\ndescribed in Szegedy et al. (2016).\n3. Use label smoothing for model regularization, as described in Szegedy et al. (2016).\n4. Make further adjustments to the Inception block by adding residual connection (Szegedy\net al., 2017), as described later in Section 8.6.\n2. What is the minimum image size needed for GoogLeNet to work?\n3. Can you design a variant of GoogLeNet that works on Fashion-MNIST’s native resolution\nof 28 × 28 pixels? How would you need to change the stem, the body, and the head of the\nnetwork, if anything at all?\n4. Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do\nthe latter two network architectures signiﬁcantly reduce the model parameter size?\n5. Compare the amount of computation needed in GoogLeNet and AlexNet. How does this\naﬀect the design of an accelerator chip, e.g., in terms of memory size, memory bandwidth,\ncache size, the amount of computation, and the beneﬁt of specialized operations?\nDiscussions131.\n8.5 Batch Normalization\nTraining deep neural networks is diﬃcult. Getting them to converge in a reasonable amount\nof time can be tricky. In this section, we describe batch normalization, a popular and eﬀective\n\n301\nBatch Normalization\ntechnique that consistently accelerates the convergence of deep networks (Ioﬀe and Szegedy,\n2015). Together with residual blocks—covered later in Section 8.6—batch normalization has\nmade it possible for practitioners to routinely train networks with over 100 layers. A secondary\n(serendipitous) beneﬁt of batch normalization lies in its inherent regularization.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n8.5.1 Training Deep Networks\nWhen working with data, we often preprocess before training. Choices regarding data pre-\nprocessing often make an enormous diﬀerence in the ﬁnal results. Recall our application of\nMLPs to predicting house prices (Section 5.7). Our ﬁrst step when working with real data\nwas to standardize our input features to have zero mean µ = 0 and unit variance Σ = 1\nacross multiple observations (Friedman, 1987), frequently rescaling the latter so that the di-\nagonal is unity, i.e., Σii = 1. Yet another strategy is to rescale vectors to unit length, possibly\nzero mean per observation. This can work well, e.g., for spatial sensor data. These prepro-\ncessing techniques and many others, are beneﬁcial for keeping the estimation problem well\ncontrolled. For a review of feature selection and extraction see the article of Guyon et al.\n(2008), for example. Standardizing vectors also has the nice side-eﬀect of constraining the\nfunction complexity of functions that act upon it. For instance, the celebrated radius-margin\nbound (Vapnik, 1995) in support vector machines and the Perceptron Convergence Theorem\n(Novikoﬀ, 1962) rely on inputs of bounded norm.\nIntuitively, this standardization plays nicely with our optimizers since it puts the parameters\na priori on a similar scale. As such, it is only natural to ask whether a corresponding nor-\nmalization step inside a deep network might not be beneﬁcial. While this is not quite the\nreasoning that led to the invention of batch normalization (Ioﬀe and Szegedy, 2015), it is a\nuseful way of understanding it and its cousin, layer normalization (Ba et al., 2016), within a\nuniﬁed framework.\nSecond, for a typical MLP or CNN, as we train, the variables in intermediate layers (e.g.,\naﬃne transformation outputs in MLP) may take values with widely varying magnitudes:\nwhether along the layers from input to output, across units in the same layer, and over time\ndue to our updates to the model parameters. The inventors of batch normalization postulated\ninformally that this drift in the distribution of such variables could hamper the convergence\nof the network. Intuitively, we might conjecture that if one layer has variable activations\nthat are 100 times that of another layer, this might necessitate compensatory adjustments in\nthe learning rates. Adaptive solvers such as AdaGrad (Duchi et al., 2011), Adam (Kingma\nand Ba, 2014), Yogi (Zaheer et al., 2018), or Distributed Shampoo (Anil et al., 2020) aim\nto address this from the viewpoint of optimization, e.g., by adding aspects of second-order\nmethods. The alternative is to prevent the problem from occurring, simply by adaptive nor-\nmalization.\nThird, deeper networks are complex and tend to be more liable to overﬁtting. This means\n\n302\nModern Convolutional Neural Networks\nthat regularization becomes more critical. A common technique for regularization is noise\ninjection. This has been known for a long time, e.g., with regard to noise injection for the\ninputs (Bishop, 1995). It also forms the basis of dropout in Section 5.6. As it turns out,\nquite serendipitously, batch normalization conveys all three beneﬁts: preprocessing, numerical\nstability, and regularization.\nBatch normalization is applied to individual layers, or optionally, to all of them: In each\ntraining iteration, we ﬁrst normalize the inputs (of batch normalization) by subtracting their\nmean and dividing by their standard deviation, where both are estimated based on the statistics\nof the current minibatch. Next, we apply a scale coeﬃcient and an oﬀset to recover the lost\ndegrees of freedom. It is precisely due to this normalization based on batch statistics that\nbatch normalization derives its name.\nNote that if we tried to apply batch normalization with minibatches of size 1, we would\nnot be able to learn anything. That is because after subtracting the means, each hidden unit\nwould take value 0. As you might guess, since we are devoting a whole section to batch\nnormalization, with large enough minibatches the approach proves eﬀective and stable. One\ntakeaway here is that when applying batch normalization, the choice of batch size is even\nmore signiﬁcant than without batch normalization, or at least, suitable calibration is needed\nas we might adjust batch size.\nDenote by B a minibatch and let x ∈B be an input to batch normalization (BN). In this case\nthe batch normalization is deﬁned as follows:\nBN(x) = γ ⊙x −ˆµB\nˆσB\n+ β.\n(8.5.1)\nIn (8.5.1), ˆµB is the sample mean and ˆσB is the sample standard deviation of the minibatch\nB. After applying standardization, the resulting minibatch has zero mean and unit variance.\nThe choice of unit variance (rather than some other magic number) is arbitrary. We recover\nthis degree of freedom by including an elementwise scale parameter γ and shift parameter β\nthat have the same shape as x. Both are parameters that need to be learned as part of model\ntraining.\nThe variable magnitudes for intermediate layers cannot diverge during training since batch\nnormalization actively centers and rescales them back to a given mean and size (via ˆµB and\nˆσB). Practical experience conﬁrms that, as alluded to when discussing feature rescaling, batch\nnormalization seems to allow for more aggressive learning rates. We calculate ˆµB and ˆσB in\n(8.5.1) as follows:\nˆµB =\n1\n|B|\n∑\nx∈B\nx and ˆσ2\nB =\n1\n|B|\n∑\nx∈B\n(x −ˆµB)2 + ϵ.\n(8.5.2)\nNote that we add a small constant ϵ > 0 to the variance estimate to ensure that we never\nattempt division by zero, even in cases where the empirical variance estimate might be very\nsmall or vanish. The estimates ˆµB and ˆσB counteract the scaling issue by using noisy esti-\nmates of mean and variance. You might think that this noisiness should be a problem. On the\ncontrary, it is actually beneﬁcial.\nThis turns out to be a recurring theme in deep learning. For reasons that are not yet well-\n\n303\nBatch Normalization\ncharacterized theoretically, various sources of noise in optimization often lead to faster train-\ning and less overﬁtting: this variation appears to act as a form of regularization. Teye et al.\n(2018) and Luo et al. (2018) related the properties of batch normalization to Bayesian priors\nand penalties, respectively. In particular, this sheds some light on the puzzle of why batch\nnormalization works best for moderate minibatch sizes in the 50–100 range. This particular\nsize of minibatch seems to inject just the “right amount” of noise per layer, both in terms\nof scale via ˆσ, and in terms of oﬀset via ˆµ: a larger minibatch regularizes less due to the\nmore stable estimates, whereas tiny minibatches destroy useful signal due to high variance.\nExploring this direction further, considering alternative types of preprocessing and ﬁltering\nmay yet lead to other eﬀective types of regularization.\nFixing a trained model, you might think that we would prefer using the entire dataset to esti-\nmate the mean and variance. Once training is complete, why would we want the same image\nto be classiﬁed diﬀerently, depending on the batch in which it happens to reside? During\ntraining, such exact calculation is infeasible because the intermediate variables for all data\nexamples change every time we update our model. However, once the model is trained, we\ncan calculate the means and variances of each layer’s variables based on the entire dataset.\nIndeed this is standard practice for models employing batch normalization; thus batch nor-\nmalization layers function diﬀerently in training mode (normalizing by minibatch statistics)\nthan in prediction mode (normalizing by dataset statistics). In this form they closely resemble\nthe behavior of dropout regularization of Section 5.6, where noise is only injected during\ntraining.\n8.5.2 Batch Normalization Layers\nBatch normalization implementations for fully connected layers and convolutional layers are\nslightly diﬀerent. One key diﬀerence between batch normalization and other layers is that\nbecause the former operates on a full minibatch at a time, we cannot just ignore the batch\ndimension as we did before when introducing other layers.\nFully Connected Layers\nWhen applying batch normalization to fully connected layers, Ioﬀe and Szegedy (2015), in\ntheir original paper inserted batch normalization after the aﬃne transformation and before\nthe nonlinear activation function. Later applications experimented with inserting batch nor-\nmalization right after activation functions. Denoting the input to the fully connected layer\nby x, the aﬃne transformation by Wx + b (with the weight parameter W and the bias\nparameter b), and the activation function by ϕ, we can express the computation of a batch-\nnormalization-enabled, fully connected layer output h as follows:\nh = ϕ(BN(Wx + b)).\n(8.5.3)\nRecall that mean and variance are computed on the same minibatch on which the transfor-\nmation is applied.\n\n304\nModern Convolutional Neural Networks\nConvolutional Layers\nSimilarly, with convolutional layers, we can apply batch normalization after the convolution\nbut before the nonlinear activation function. The key diﬀerence from batch normalization in\nfully connected layers is that we apply the operation on a per-channel basis across all locations.\nThis is compatible with our assumption of translation invariance that led to convolutions: we\nassumed that the speciﬁc location of a pattern within an image was not critical for the purpose\nof understanding.\nAssume that our minibatches contain m examples and that for each channel, the output of\nthe convolution has height p and width q. For convolutional layers, we carry out each batch\nnormalization over the m · p · q elements per output channel simultaneously. Thus, we col-\nlect the values over all spatial locations when computing the mean and variance and conse-\nquently apply the same mean and variance within a given channel to normalize the value at\neach spatial location. Each channel has its own scale and shift parameters, both of which are\nscalars.\nLayer Normalization\nNote that in the context of convolutions the batch normalization is well deﬁned even for mini-\nbatches of size 1: after all, we have all the locations across an image to average. Consequently,\nmean and variance are well deﬁned, even if it is just within a single observation. This con-\nsideration led Ba et al. (2016) to introduce the notion of layer normalization. It works just\nlike a batch norm, only that it is applied to one observation at a time. Consequently both the\noﬀset and the scaling factor are scalars. For an n-dimensional vector x, layer norms are given\nby\nx →LN(x) = x −ˆµ\nˆσ\n,\n(8.5.4)\nwhere scaling and oﬀset are applied coeﬃcient-wise and given by\nˆµ def\n= 1\nn\nn\n∑\ni=1\nxi and ˆσ2 def\n= 1\nn\nn\n∑\ni=1\n(xi −ˆµ)2 + ϵ.\n(8.5.5)\nAs before we add a small oﬀset ϵ > 0 to prevent division by zero. One of the major beneﬁts\nof using layer normalization is that it prevents divergence. After all, ignoring ϵ, the output\nof the layer normalization is scale independent. That is, we have LN(x) ≈LN(αx) for any\nchoice of α , 0. This becomes an equality for |α| →∞(the approximate equality is due to\nthe oﬀset ϵ for the variance).\nAnother advantage of the layer normalization is that it does not depend on the minibatch\nsize. It is also independent of whether we are in training or test regime. In other words, it is\nsimply a deterministic transformation that standardizes the activations to a given scale. This\ncan be very beneﬁcial in preventing divergence in optimization. We skip further details and\nrecommend that interested readers consult the original paper.\n\n305\nBatch Normalization\nBatch Normalization During Prediction\nAs we mentioned earlier, batch normalization typically behaves diﬀerently in training mode\nthan in prediction mode. First, the noise in the sample mean and the sample variance arising\nfrom estimating each on minibatches is no longer desirable once we have trained the model.\nSecond, we might not have the luxury of computing per-batch normalization statistics. For\nexample, we might need to apply our model to make one prediction at a time.\nTypically, after training, we use the entire dataset to compute stable estimates of the variable\nstatistics and then ﬁx them at prediction time. Hence, batch normalization behaves diﬀerently\nduring training than at test time. Recall that dropout also exhibits this characteristic.\n8.5.3 Implementation from Scratch\nTo see how batch normalization works in practice, we implement one from scratch be-\nlow.\ndef batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n# Use is_grad_enabled to determine whether we are in training mode\nif not torch.is_grad_enabled():\n# In prediction mode, use mean and variance obtained by moving average\nX_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\nelse:\nassert len(X.shape) in (2, 4)\nif len(X.shape) == 2:\n# When using a fully connected layer, calculate the mean and\n# variance on the feature dimension\nmean = X.mean(dim=0)\nvar = ((X - mean) ** 2).mean(dim=0)\nelse:\n# When using a two-dimensional convolutional layer, calculate the\n# mean and variance on the channel dimension (axis=1). Here we\n# need to maintain the shape of X, so that the broadcasting\n# operation can be carried out later\nmean = X.mean(dim=(0, 2, 3), keepdim=True)\nvar = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n# In training mode, the current mean and variance are used\nX_hat = (X - mean) / torch.sqrt(var + eps)\n# Update the mean and variance using moving average\nmoving_mean = (1.0 - momentum) * moving_mean + momentum * mean\nmoving_var = (1.0 - momentum) * moving_var + momentum * var\nY = gamma * X_hat + beta\n# Scale and shift\nreturn Y, moving_mean.data, moving_var.data\nWe can now create a proper BatchNorm layer. Our layer will maintain proper parameters for\nscale gamma and shift beta, both of which will be updated in the course of training. Addi-\ntionally, our layer will maintain moving averages of the means and variances for subsequent\nuse during model prediction.\nPutting aside the algorithmic details, note the design pattern underlying our implementation\nof the layer. Typically, we deﬁne the mathematics in a separate function, say batch_norm.\n\n306\nModern Convolutional Neural Networks\nWe then integrate this functionality into a custom layer, whose code mostly addresses book-\nkeeping matters, such as moving data to the right device context, allocating and initializing\nany required variables, keeping track of moving averages (here for mean and variance), and\nso on. This pattern enables a clean separation of mathematics from boilerplate code. Also\nnote that for the sake of convenience we did not worry about automatically inferring the in-\nput shape here; thus we need to specify the number of features throughout. By now all modern\ndeep learning frameworks oﬀer automatic detection of size and shape in the high-level batch\nnormalization APIs (in practice we will use this instead).\nclass BatchNorm(nn.Module):\n# num_features: the number of outputs for a fully connected layer or the\n# number of output channels for a convolutional layer. num_dims: 2 for a\n# fully connected layer and 4 for a convolutional layer\ndef __init__(self, num_features, num_dims):\nsuper().__init__()\nif num_dims == 2:\nshape = (1, num_features)\nelse:\nshape = (1, num_features, 1, 1)\n# The scale parameter and the shift parameter (model parameters) are\n# initialized to 1 and 0, respectively\nself.gamma = nn.Parameter(torch.ones(shape))\nself.beta = nn.Parameter(torch.zeros(shape))\n# The variables that are not model parameters are initialized to 0 and\n# 1\nself.moving_mean = torch.zeros(shape)\nself.moving_var = torch.ones(shape)\ndef forward(self, X):\n# If X is not on the main memory, copy moving_mean and moving_var to\n# the device where X is located\nif self.moving_mean.device != X.device:\nself.moving_mean = self.moving_mean.to(X.device)\nself.moving_var = self.moving_var.to(X.device)\n# Save the updated moving_mean and moving_var\nY, self.moving_mean, self.moving_var = batch_norm(\nX, self.gamma, self.beta, self.moving_mean,\nself.moving_var, eps=1e-5, momentum=0.1)\nreturn Y\nWe used momentum to govern the aggregation over past mean and variance estimates. This\nis somewhat of a misnomer as it has nothing whatsoever to do with the momentum term of\noptimization. Nonetheless, it is the commonly adopted name for this term and in deference\nto API naming convention we use the same variable name in our code.\n8.5.4 LeNet with Batch Normalization\nTo see how to apply BatchNorm in context, below we apply it to a traditional LeNet model\n(Section 7.6). Recall that batch normalization is applied after the convolutional layers or fully\nconnected layers but before the corresponding activation functions.\n\n307\nBatch Normalization\nclass BNLeNetScratch(d2l.Classifier):\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(\nnn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\nnn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\nnn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\nnn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\nnn.Flatten(), nn.LazyLinear(120),\nBatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\nBatchNorm(84, num_dims=2), nn.Sigmoid(),\nnn.LazyLinear(num_classes))\nAs before, we will train our network on the Fashion-MNIST dataset. This code is virtually\nidentical to that when we ﬁrst trained LeNet.\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = BNLeNetScratch(lr=0.1)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\nLet’s have a look at the scale parameter gamma and the shift parameter beta learned from\nthe ﬁrst batch normalization layer.\nmodel.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))\n(tensor([2.2417, 1.8804, 2.0719, 1.1316, 1.9629, 1.6801], device='cuda:0',\ngrad_fn=<ReshapeAliasBackward0>),\ntensor([ 0.3080, -1.0170,\n1.4802, -0.2111, -1.9058,\n0.6248], device='cuda:0\n,→',\ngrad_fn=<ReshapeAliasBackward0>))\n8.5.5 Concise Implementation\n\n308\nModern Convolutional Neural Networks\nCompared with the BatchNorm class, which we just deﬁned ourselves, we can use the Batch-\nNorm class deﬁned in high-level APIs from the deep learning framework directly. The code\nlooks virtually identical to our implementation above, except that we no longer need to pro-\nvide additional arguments for it to get the dimensions right.\nclass BNLeNet(d2l.Classifier):\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(\nnn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\nnn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\nnn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\nnn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\nnn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\nnn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\nnn.Sigmoid(), nn.LazyLinear(num_classes))\nBelow, we use the same hyperparameters to train our model. Note that as usual, the high-level\nAPI variant runs much faster because its code has been compiled to C++ or CUDA while\nour custom implementation must be interpreted by Python.\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128)\nmodel = BNLeNet(lr=0.1)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n8.5.6 Discussion\nIntuitively, batch normalization is thought to make the optimization landscape smoother.\nHowever, we must be careful to distinguish between speculative intuitions and true expla-\nnations for the phenomena that we observe when training deep models. Recall that we do not\neven know why simpler deep neural networks (MLPs and conventional CNNs) generalize\n\n309\nBatch Normalization\nwell in the ﬁrst place. Even with dropout and weight decay, they remain so ﬂexible that their\nability to generalize to unseen data likely needs signiﬁcantly more reﬁned learning-theoretic\ngeneralization guarantees.\nThe original paper proposing batch normalization (Ioﬀe and Szegedy, 2015), in addition to\nintroducing a powerful and useful tool, oﬀered an explanation for why it works: by reducing\ninternal covariate shift. Presumably by internal covariate shift they meant something like the\nintuition expressed above—the notion that the distribution of variable values changes over the\ncourse of training. However, there were two problems with this explanation: i) This drift is\nvery diﬀerent from covariate shift, rendering the name a misnomer. If anything, it is closer to\nconcept drift. ii) The explanation oﬀers an under-speciﬁed intuition but leaves the question\nof why precisely this technique works an open question wanting for a rigorous explanation.\nThroughout this book, we aim to convey the intuitions that practitioners use to guide their\ndevelopment of deep neural networks. However, we believe that it is important to separate\nthese guiding intuitions from established scientiﬁc fact. Eventually, when you master this\nmaterial and start writing your own research papers you will want to be clear to delineate\nbetween technical claims and hunches.\nFollowing the success of batch normalization, its explanation in terms of internal covariate\nshift has repeatedly surfaced in debates in the technical literature and broader discourse about\nhow to present machine learning research. In a memorable speech given while accepting a\nTest of Time Award at the 2017 NeurIPS conference, Ali Rahimi used internal covariate\nshift as a focal point in an argument likening the modern practice of deep learning to alchemy.\nSubsequently, the example was revisited in detail in a position paper outlining troubling trends\nin machine learning (Lipton and Steinhardt, 2018). Other authors have proposed alternative\nexplanations for the success of batch normalization, some (Santurkar et al., 2018) claiming\nthat batch normalization’s success comes despite exhibiting behavior that is in some ways\nopposite to those claimed in the original paper.\nWe note that the internal covariate shift is no more worthy of criticism than any of thousands\nof similarly vague claims made every year in the technical machine learning literature. Likely,\nits resonance as a focal point of these debates owes to its broad recognizability for the target\naudience. Batch normalization has proven an indispensable method, applied in nearly all de-\nployed image classiﬁers, earning the paper that introduced the technique tens of thousands of\ncitations. We conjecture, though, that the guiding principles of regularization through noise\ninjection, acceleration through rescaling and lastly preprocessing may well lead to further\ninventions of layers and techniques in the future.\nOn a more practical note, there are a number of aspects worth remembering about batch\nnormalization:\n• During model training, batch normalization continuously adjusts the intermediate output\nof the network by utilizing the mean and standard deviation of the minibatch, so that\nthe values of the intermediate output in each layer throughout the neural network are\nmore stable.\n• Batch normalization is slightly diﬀerent for fully connected layers than for convolutional\n\n310\nModern Convolutional Neural Networks\n132\nlayers. In fact, for convolutional layers, layer normalization can sometimes be used as\nan alternative.\n• Like a dropout layer, batch normalization layers have diﬀerent behaviors in training mode\nthan in prediction mode.\n• Batch normalization is useful for regularization and improving convergence in optimiza-\ntion. By contrast, the original motivation of reducing internal covariate shift seems not\nto be a valid explanation.\n• For more robust models that are less sensitive to input perturbations, consider removing\nbatch normalization (Wang et al., 2022).\n8.5.7 Exercises\n1. Should we remove the bias parameter from the fully connected layer or the convolutional\nlayer before the batch normalization? Why?\n2. Compare the learning rates for LeNet with and without batch normalization.\n1. Plot the increase in validation accuracy.\n2. How large can you make the learning rate before the optimization fails in both cases?\n3. Do we need batch normalization in every layer? Experiment with it.\n4. Implement a “lite” version of batch normalization that only removes the mean, or alter-\nnatively one that only removes the variance. How does it behave?\n5. Fix the parameters beta and gamma. Observe and analyze the results.\n6. Can you replace dropout by batch normalization? How does the behavior change?\n7. Research ideas: think of other normalization transforms that you can apply:\n1. Can you apply the probability integral transform?\n2. Can you use a full-rank covariance estimate? Why should you probably not do that?\n3. Can you use other compact matrix variants (block-diagonal, low-displacement rank,\nMonarch, etc.)?\n4. Does a sparsiﬁcation compression act as a regularizer?\n5. Are there other projections (e.g., convex cone, symmetry group-speciﬁc transforms)\nthat you can use?\nDiscussions132.\n\n311\nResidual Networks (ResNet) and ResNeXt\n8.6 Residual Networks (ResNet) and ResNeXt\nAs we design ever deeper networks it becomes imperative to understand how adding layers\ncan increase the complexity and expressiveness of the network. Even more important is the\nability to design networks where adding layers makes networks strictly more expressive rather\nthan just diﬀerent. To make some progress we need a bit of mathematics.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n8.6.1 Function Classes\nConsider F , the class of functions that a speciﬁc network architecture (together with learning\nrates and other hyperparameter settings) can reach. That is, for all f ∈F there exists some\nset of parameters (e.g., weights and biases) that can be obtained through training on a suitable\ndataset. Let’s assume that f ∗is the “truth” function that we really would like to ﬁnd. If it is\nin F , we are in good shape but typically we will not be quite so lucky. Instead, we will try to\nﬁnd some f ∗\nF which is our best bet within F . For instance, given a dataset with features X\nand labels y, we might try ﬁnding it by solving the following optimization problem:\nf ∗\nF\ndef\n= argmin\nf\nL(X, y, f ) subject to f ∈F .\n(8.6.1)\nWe know that regularization (Morozov, 1984, Tikhonov and Arsenin, 1977) may control\ncomplexity of F and achieve consistency, so a larger size of training data generally leads to\nbetter f ∗\nF. It is only reasonable to assume that if we design a diﬀerent and more powerful\narchitecture F ′ we should arrive at a better outcome. In other words, we would expect that\nf ∗\nF′ is “better” than f ∗\nF. However, if F ⊈F ′ there is no guarantee that this should even\nhappen. In fact, f ∗\nF′ might well be worse. As illustrated by Fig. 8.6.1, for non-nested function\nclasses, a larger function class does not always move closer to the “truth” function f ∗. For\ninstance, on the left of Fig. 8.6.1, though F3 is closer to f ∗than F1, F6 moves away and there\nis no guarantee that further increasing the complexity can reduce the distance from f ∗. With\nnested function classes where F1 ⊆· · · ⊆F6 on the right of Fig. 8.6.1, we can avoid the\naforementioned issue from the non-nested function classes.\nThus, only if larger function classes contain the smaller ones are we guaranteed that increasing\nthem strictly increases the expressive power of the network. For deep neural networks, if we\ncan train the newly-added layer into an identity function f (x) = x, the new model will be as\neﬀective as the original model. As the new model may get a better solution to ﬁt the training\ndataset, the added layer might make it easier to reduce training errors.\nThis is the question that He et al. (2016) considered when working on very deep computer\n\n312\nModern Convolutional Neural Networks\nt\nFig. 8.6.1\nFor non-nested function classes, a larger (indicated by area) function class does not\nguarantee we will get closer to the “truth” function (f ∗). This does not happen in nested\nfunction classes.\nvision models. At the heart of their proposed residual network (ResNet) is the idea that every\nadditional layer should more easily contain the identity function as one of its elements. These\nconsiderations are rather profound but they led to a surprisingly simple solution, a residual\nblock. With it, ResNet won the ImageNet Large Scale Visual Recognition Challenge in 2015.\nThe design had a profound inﬂuence on how to build deep neural networks. For instance,\nresidual blocks have been added to recurrent networks (Kim et al., 2017, Prakash et al., 2016).\nLikewise, Transformers (Vaswani et al., 2017) use them to stack many layers of networks\neﬃciently. It is also used in graph neural networks (Kipf and Welling, 2016) and, as a basic\nconcept, it has been used extensively in computer vision (Redmon and Farhadi, 2018, Ren et\nal., 2015). Note that residual networks are predated by highway networks (Srivastava et al.,\n2015) that share some of the motivation, albeit without the elegant parametrization around\nthe identity function.\n8.6.2 Residual Blocks\nLet’s focus on a local part of a neural network, as depicted in Fig. 8.6.2. Denote the input by\nx. We assume that f (x), the desired underlying mapping we want to obtain by learning, is\nto be used as input to the activation function on the top. On the left, the portion within the\ndotted-line box must directly learn f (x). On the right, the portion within the dotted-line box\nneeds to learn the residual mapping g(x) = f (x)−x, which is how the residual block derives\nits name. If the identity mapping f (x) = x is the desired underlying mapping, the residual\nmapping amounts to g(x) = 0 and it is thus easier to learn: we only need to push the weights\nand biases of the upper weight layer (e.g., fully connected layer and convolutional layer) within\nthe dotted-line box to zero. The right ﬁgure illustrates the residual block of ResNet, where the\nsolid line carrying the layer input x to the addition operator is called a residual connection (or\nshortcut connection). With residual blocks, inputs can forward propagate faster through the\nresidual connections across layers. In fact, the residual block can be thought of as a special\n\n313\nResidual Networks (ResNet) and ResNeXt\ncase of the multi-branch Inception block: it has two branches one of which is the identity\nmapping.\nt\nFig. 8.6.2\nIn a regular block (left), the portion within the dotted-line box must directly learn the\nmapping f (x). In a residual block (right), the portion within the dotted-line box needs to\nlearn the residual mapping g(x) = f (x) −x, making the identity mapping f (x) = x\neasier to learn.\nResNet has VGG’s full 3 × 3 convolutional layer design. The residual block has two 3 ×\n3 convolutional layers with the same number of output channels. Each convolutional layer\nis followed by a batch normalization layer and a ReLU activation function. Then, we skip\nthese two convolution operations and add the input directly before the ﬁnal ReLU activation\nfunction. This kind of design requires that the output of the two convolutional layers has to\nbe of the same shape as the input, so that they can be added together. If we want to change\nthe number of channels, we need to introduce an additional 1 × 1 convolutional layer to\ntransform the input into the desired shape for the addition operation. Let’s have a look at the\ncode below.\nclass Residual(nn.Module):\n#@save\n\"\"\"The Residual block of ResNet models.\"\"\"\ndef __init__(self, num_channels, use_1x1conv=False, strides=1):\nsuper().__init__()\nself.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\nstride=strides)\nself.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\nif use_1x1conv:\nself.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\nstride=strides)\nelse:\nself.conv3 = None\nself.bn1 = nn.LazyBatchNorm2d()\nself.bn2 = nn.LazyBatchNorm2d()\ndef forward(self, X):\nY = F.relu(self.bn1(self.conv1(X)))\n(continues on next page)\n\n314\nModern Convolutional Neural Networks\n(continued from previous page)\nY = self.bn2(self.conv2(Y))\nif self.conv3:\nX = self.conv3(X)\nY += X\nreturn F.relu(Y)\nThis code generates two types of networks: one where we add the input to the output before\napplying the ReLU nonlinearity whenever use_1x1conv=False; and one where we adjust\nchannels and resolution by means of a 1 × 1 convolution before adding. Fig. 8.6.3 illustrates\nthis.\nt\nFig. 8.6.3\nResNet block with and without 1 × 1 convolution, which transforms the input into the\ndesired shape for the addition operation.\nNow let’s look at a situation where the input and output are of the same shape, where 1 × 1\nconvolution is not needed.\nblk = Residual(3)\nX = torch.randn(4, 3, 6, 6)\nblk(X).shape\ntorch.Size([4, 3, 6, 6])\nWe also have the option to halve the output height and width while increasing the number\nof output channels. In this case we use 1 × 1 convolutions via use_1x1conv=True. This\ncomes in handy at the beginning of each ResNet block to reduce the spatial dimensionality\nvia strides=2.\n\n315\nResidual Networks (ResNet) and ResNeXt\nblk = Residual(6, use_1x1conv=True, strides=2)\nblk(X).shape\ntorch.Size([4, 6, 3, 3])\n8.6.3 ResNet Model\nThe ﬁrst two layers of ResNet are the same as those of the GoogLeNet we described before:\nthe 7×7 convolutional layer with 64 output channels and a stride of 2 is followed by the 3×3\nmax-pooling layer with a stride of 2. The diﬀerence is the batch normalization layer added\nafter each convolutional layer in ResNet.\nclass ResNet(d2l.Classifier):\ndef b1(self):\nreturn nn.Sequential(\nnn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\nnn.LazyBatchNorm2d(), nn.ReLU(),\nnn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nGoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four\nmodules made up of residual blocks, each of which uses several residual blocks with the\nsame number of output channels. The number of channels in the ﬁrst module is the same as\nthe number of input channels. Since a max-pooling layer with a stride of 2 has already been\nused, it is not necessary to reduce the height and width. In the ﬁrst residual block for each\nof the subsequent modules, the number of channels is doubled compared with that of the\nprevious module, and the height and width are halved.\n@d2l.add_to_class(ResNet)\ndef block(self, num_residuals, num_channels, first_block=False):\nblk = []\nfor i in range(num_residuals):\nif i == 0 and not first_block:\nblk.append(Residual(num_channels, use_1x1conv=True, strides=2))\nelse:\nblk.append(Residual(num_channels))\nreturn nn.Sequential(*blk)\nThen, we add all the modules to ResNet. Here, two residual blocks are used for each module.\nLastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully\nconnected layer output.\n@d2l.add_to_class(ResNet)\ndef __init__(self, arch, lr=0.1, num_classes=10):\nsuper(ResNet, self).__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(self.b1())\n(continues on next page)\n\n316\nModern Convolutional Neural Networks\n(continued from previous page)\nfor i, b in enumerate(arch):\nself.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\nself.net.add_module('last', nn.Sequential(\nnn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\nnn.LazyLinear(num_classes)))\nself.net.apply(d2l.init_cnn)\nThere are four convolutional layers in each module (excluding the 1 × 1 convolutional layer).\nTogether with the ﬁrst 7 × 7 convolutional layer and the ﬁnal fully connected layer, there are\n18 layers in total. Therefore, this model is commonly known as ResNet-18. By conﬁguring\ndiﬀerent numbers of channels and residual blocks in the module, we can create diﬀerent\nResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture\nof ResNet is similar to that of GoogLeNet, ResNet’s structure is simpler and easier to modify.\nAll these factors have resulted in the rapid and widespread use of ResNet. Fig. 8.6.4 depicts\nthe full ResNet-18.\nt\nFig. 8.6.4\nThe ResNet-18 architecture.\nBefore training ResNet, let’s observe how the input shape changes across diﬀerent modules\nin ResNet. As in all the previous architectures, the resolution decreases while the number\nof channels increases up until the point where a global average pooling layer aggregates all\nfeatures.\nclass ResNet18(ResNet):\ndef __init__(self, lr=0.1, num_classes=10):\nsuper().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\nlr, num_classes)\nResNet18().layer_summary((1, 1, 96, 96))\nSequential output shape:\ntorch.Size([1, 64, 24, 24])\nSequential output shape:\ntorch.Size([1, 64, 24, 24])\nSequential output shape:\ntorch.Size([1, 128, 12, 12])\nSequential output shape:\ntorch.Size([1, 256, 6, 6])\nSequential output shape:\ntorch.Size([1, 512, 3, 3])\nSequential output shape:\ntorch.Size([1, 10])\n\n317\nResidual Networks (ResNet) and ResNeXt\n8.6.4 Training\nWe train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a powerful\nand ﬂexible architecture. The plot capturing training and validation loss illustrates a signiﬁcant\ngap between both graphs, with the training loss being considerably lower. For a network of\nthis ﬂexibility, more training data would oﬀer distinct beneﬁt in closing the gap and improving\naccuracy.\nmodel = ResNet18(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model, data)\n8.6.5 ResNeXt\nOne of the challenges one encounters in the design of ResNet is the trade-oﬀbetween non-\nlinearity and dimensionality within a given block. That is, we could add more nonlinearity by\nincreasing the number of layers, or by increasing the width of the convolutions. An alterna-\ntive strategy is to increase the number of channels that can carry information between blocks.\nUnfortunately, the latter comes with a quadratic penalty since the computational cost of in-\ngesting ci channels and emitting co channels is proportional to O(ci · co) (see our discussion\nin Section 7.4).\nWe can take some inspiration from the Inception block of Fig. 8.4.1 which has informa-\ntion ﬂowing through the block in separate groups. Applying the idea of multiple indepen-\ndent groups to the ResNet block of Fig. 8.6.3 led to the design of ResNeXt (Xie et al.,\n2017). Diﬀerent from the smorgasbord of transformations in Inception, ResNeXt adopts the\nsame transformation in all branches, thus minimizing the need for manual tuning of each\nbranch.\nBreaking up a convolution from ci to co channels into one of g groups of size ci/g generating\ng outputs of size co/g is called, quite ﬁttingly, a grouped convolution. The computational cost\n(proportionally) is reduced from O(ci · co) to O(g · (ci/g) · (co/g)) = O(ci · co/g), i.e., it is\n\n318\nModern Convolutional Neural Networks\nt\nFig. 8.6.5\nThe ResNeXt block. The use of grouped convolution with g groups is g times faster than a\ndense convolution. It is a bottleneck residual block when the number of intermediate\nchannels b is less than c.\ng times faster. Even better, the number of parameters needed to generate the output is also\nreduced from a ci × co matrix to g smaller matrices of size (ci/g) × (co/g), again a g times\nreduction. In what follows we assume that both ci and co are divisible by g.\nThe only challenge in this design is that no information is exchanged between the g groups.\nThe ResNeXt block of Fig. 8.6.5 amends this in two ways: the grouped convolution with a\n3 × 3 kernel is sandwiched in between two 1 × 1 convolutions. The second one serves double\nduty in changing the number of channels back. The beneﬁt is that we only pay the O(c · b)\ncost for 1 × 1 kernels and can make do with an O(b2/g) cost for 3 × 3 kernels. Similar to\nthe residual block implementation in Section 8.6.2, the residual connection is replaced (thus\ngeneralized) by a 1 × 1 convolution.\nThe right-hand ﬁgure in Fig. 8.6.5 provides a much more concise summary of the resulting\nnetwork block. It will also play a major role in the design of generic modern CNNs in Section\n8.8. Note that the idea of grouped convolutions dates back to the implementation of AlexNet\n(Krizhevsky et al., 2012). When distributing the network across two GPUs with limited mem-\nory, the implementation treated each GPU as its own channel with no ill eﬀects.\nThe following implementation of the ResNeXtBlock class takes as argument groups (g),\nwith bot_channels (b) intermediate (bottleneck) channels. Lastly, when we need to reduce\nthe height and width of the representation, we add a stride of 2 by setting use_1x1conv=True,\nstrides=2.\nclass ResNeXtBlock(nn.Module):\n#@save\n(continues on next page)\n\n319\nResidual Networks (ResNet) and ResNeXt\n(continued from previous page)\n\"\"\"The ResNeXt block.\"\"\"\ndef __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\nstrides=1):\nsuper().__init__()\nbot_channels = int(round(num_channels * bot_mul))\nself.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\nself.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\nstride=strides, padding=1,\ngroups=bot_channels//groups)\nself.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\nself.bn1 = nn.LazyBatchNorm2d()\nself.bn2 = nn.LazyBatchNorm2d()\nself.bn3 = nn.LazyBatchNorm2d()\nif use_1x1conv:\nself.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\nstride=strides)\nself.bn4 = nn.LazyBatchNorm2d()\nelse:\nself.conv4 = None\ndef forward(self, X):\nY = F.relu(self.bn1(self.conv1(X)))\nY = F.relu(self.bn2(self.conv2(Y)))\nY = self.bn3(self.conv3(Y))\nif self.conv4:\nX = self.bn4(self.conv4(X))\nreturn F.relu(Y + X)\nIts use is entirely analogous to that of the ResNetBlock discussed previously. For instance,\nwhen using (use_1x1conv=False,\nstrides=1), the input and output are of the same\nshape. Alternatively, setting use_1x1conv=True, strides=2 halves the output height and\nwidth.\nblk = ResNeXtBlock(32, 16, 1)\nX = torch.randn(4, 32, 96, 96)\nblk(X).shape\ntorch.Size([4, 32, 96, 96])\n8.6.6 Summary and Discussion\nNested function classes are desirable since they allow us to obtain strictly more powerful rather\nthan also subtly diﬀerent function classes when adding capacity. One way of accomplishing\nthis is by letting additional layers to simply pass through the input to the output. Residual\nconnections allow for this. As a consequence, this changes the inductive bias from simple\nfunctions being of the form f (x) = 0 to simple functions looking like f (x) = x.\nThe residual mapping can learn the identity function more easily, such as pushing parameters\nin the weight layer to zero. We can train an eﬀective deep neural network by having residual\n\n320\nModern Convolutional Neural Networks\nblocks. Inputs can forward propagate faster through the residual connections across layers. As\na consequence, we can thus train much deeper networks. For instance, the original ResNet\npaper (He et al., 2016) allowed for up to 152 layers. Another beneﬁt of residual networks is\nthat it allows us to add layers, initialized as the identity function, during the training process.\nAfter all, the default behavior of a layer is to let the data pass through unchanged. This can\naccelerate the training of very large networks in some cases.\nPrior to residual connections, bypassing paths with gating units were introduced to eﬀec-\ntively train highway networks with over 100 layers (Srivastava et al., 2015). Using identity\nfunctions as bypassing paths, ResNet performed remarkably well on multiple computer vi-\nsion tasks. Residual connections had a major inﬂuence on the design of subsequent deep\nneural networks, of either convolutional or sequential nature. As we will introduce later, the\nTransformer architecture (Vaswani et al., 2017) adopts residual connections (together with\nother design choices) and is pervasive in areas as diverse as language, vision, speech, and\nreinforcement learning.\nResNeXt is an example for how the design of convolutional neural networks has evolved\nover time: by being more frugal with computation and trading it oﬀagainst the size of the\nactivations (number of channels), it allows for faster and more accurate networks at lower\ncost. An alternative way of viewing grouped convolutions is to think of a block-diagonal\nmatrix for the convolutional weights. Note that there are quite a few such “tricks” that lead\nto more eﬃcient networks. For instance, ShiftNet (Wu et al., 2018) mimicks the eﬀects of\na 3 × 3 convolution, simply by adding shifted activations to the channels, oﬀering increased\nfunction complexity, this time without any computational cost.\nA common feature of the designs we have discussed so far is that the network design is\nfairly manual, primarily relying on the ingenuity of the designer to ﬁnd the “right” network\nhyperparameters. While clearly feasible, it is also very costly in terms of human time and\nthere is no guarantee that the outcome is optimal in any sense. In Section 8.8 we will discuss\na number of strategies for obtaining high quality networks in a more automated fashion. In\nparticular, we will review the notion of network design spaces that led to the RegNetX/Y\nmodels (Radosavovic et al., 2020).\n8.6.7 Exercises\n1. What are the major diﬀerences between the Inception block in Fig. 8.4.1 and the resid-\nual block? How do they compare in terms of computation, accuracy, and the classes of\nfunctions they can describe?\n2. Refer to Table 1 in the ResNet paper (He et al., 2016) to implement diﬀerent variants of\nthe network.\n3. For deeper networks, ResNet introduces a “bottleneck” architecture to reduce model com-\nplexity. Try to implement it.\n4. In subsequent versions of ResNet, the authors changed the “convolution, batch normal-\n\n321\nDensely Connected Networks (DenseNet)\n133\nization, and activation” structure to the “batch normalization, activation, and convolution”\nstructure. Make this improvement yourself. See Figure 1 in He et al. (2016) for details.\n5. Why can’t we just increase the complexity of functions without bound, even if the function\nclasses are nested?\nDiscussions133.\n8.7 Densely Connected Networks (DenseNet)\nResNet signiﬁcantly changed the view of how to parametrize the functions in deep networks.\nDenseNet (dense convolutional network) is to some extent the logical extension of this (Huang\net al., 2017). DenseNet is characterized by both the connectivity pattern where each layer\nconnects to all the preceding layers and the concatenation operation (rather than the addition\noperator in ResNet) to preserve and reuse features from earlier layers. To understand how to\narrive at it, let’s take a small detour to mathematics.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n8.7.1 From ResNet to DenseNet\nRecall the Taylor expansion for functions. At the point x = 0 it can be written as\nf (x) = f (0) + x ·\n[\nf ′(0) + x ·\n[ f ′′(0)\n2!\n+ x ·\n[ f ′′′(0)\n3!\n+ · · ·\n]]]\n.\n(8.7.1)\nThe key point is that it decomposes a function into terms of increasingly higher order. In a\nsimilar vein, ResNet decomposes functions into\nf (x) = x + g(x).\n(8.7.2)\nThat is, ResNet decomposes f into a simple linear term and a more complex nonlinear one.\nWhat if we wanted to capture (not necessarily add) information beyond two terms? One such\nsolution is DenseNet (Huang et al., 2017).\nAs shown in Fig. 8.7.1, the key diﬀerence between ResNet and DenseNet is that in the latter\ncase outputs are concatenated (denoted by [, ]) rather than added. As a result, we perform\na mapping from x to its values after applying an increasingly complex sequence of func-\ntions:\nx →[x, f1(x), f2 ([x, f1 (x)]), f3 ([x, f1 (x), f2 ([x, f1 (x)])]), . . .] .\n(8.7.3)\nIn the end, all these functions are combined in MLP to reduce the number of features again.\n\n322\nModern Convolutional Neural Networks\nt\nFig. 8.7.1\nThe main difference between ResNet (left) and DenseNet (right) in cross-layer\nconnections: use of addition and use of concatenation.\nIn terms of implementation this is quite simple: rather than adding terms, we concatenate\nthem. The name DenseNet arises from the fact that the dependency graph between variables\nbecomes quite dense. The ﬁnal layer of such a chain is densely connected to all previous\nlayers. The dense connections are shown in Fig. 8.7.2.\nt\nFig. 8.7.2\nDense connections in DenseNet. Note how the dimensionality increases with depth.\nThe main components that comprise a DenseNet are dense blocks and transition layers. The\nformer deﬁne how the inputs and outputs are concatenated, while the latter control the number\nof channels so that it is not too large, since the expansion x →[x, f1(x), f2 ([x, f1 (x)]), . . .]\ncan be quite high-dimensional.\n8.7.2 Dense Blocks\nDenseNet uses the modiﬁed “batch normalization, activation, and convolution” structure of\nResNet (see the exercise in Section 8.6). First, we implement this convolution block struc-\nture.\ndef conv_block(num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn.ReLU(),\nnn.LazyConv2d(num_channels, kernel_size=3, padding=1))\nA dense block consists of multiple convolution blocks, each using the same number of out-\nput channels. In the forward propagation, however, we concatenate the input and output of\neach convolution block on the channel dimension. Lazy evaluation allows us to adjust the\ndimensionality automatically.\nclass DenseBlock(nn.Module):\ndef __init__(self, num_convs, num_channels):\n(continues on next page)\n\n323\nDensely Connected Networks (DenseNet)\n(continued from previous page)\nsuper(DenseBlock, self).__init__()\nlayer = []\nfor i in range(num_convs):\nlayer.append(conv_block(num_channels))\nself.net = nn.Sequential(*layer)\ndef forward(self, X):\nfor blk in self.net:\nY = blk(X)\n# Concatenate input and output of each block along the channels\nX = torch.cat((X, Y), dim=1)\nreturn X\nIn the following example, we deﬁne a DenseBlock instance with two convolution blocks\nof 10 output channels. When using an input with three channels, we will get an output with\n3+10+10 = 23 channels. The number of convolution block channels controls the growth in\nthe number of output channels relative to the number of input channels. This is also referred\nto as the growth rate.\nblk = DenseBlock(2, 10)\nX = torch.randn(4, 3, 8, 8)\nY = blk(X)\nY.shape\ntorch.Size([4, 23, 8, 8])\n8.7.3 Transition Layers\nSince each dense block will increase the number of channels, adding too many of them will\nlead to an excessively complex model. A transition layer is used to control the complexity\nof the model. It reduces the number of channels by using a 1 × 1 convolution. Moreover, it\nhalves the height and width via average pooling with a stride of 2.\ndef transition_block(num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn.ReLU(),\nnn.LazyConv2d(num_channels, kernel_size=1),\nnn.AvgPool2d(kernel_size=2, stride=2))\nApply a transition layer with 10 channels to the output of the dense block in the previ-\nous example. This reduces the number of output channels to 10, and halves the height and\nwidth.\nblk = transition_block(10)\nblk(Y).shape\n\n324\nModern Convolutional Neural Networks\ntorch.Size([4, 10, 4, 4])\n8.7.4 DenseNet Model\nNext, we will construct a DenseNet model. DenseNet ﬁrst uses the same single convolutional\nlayer and max-pooling layer as in ResNet.\nclass DenseNet(d2l.Classifier):\ndef b1(self):\nreturn nn.Sequential(\nnn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\nnn.LazyBatchNorm2d(), nn.ReLU(),\nnn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nThen, similar to the four modules made up of residual blocks that ResNet uses, DenseNet\nuses four dense blocks. As with ResNet, we can set the number of convolutional layers used\nin each dense block. Here, we set it to 4, consistent with the ResNet-18 model in Section 8.6.\nFurthermore, we set the number of channels (i.e., growth rate) for the convolutional layers\nin the dense block to 32, so 128 channels will be added to each dense block.\nIn ResNet, the height and width are reduced between each module by a residual block with\na stride of 2. Here, we use the transition layer to halve the height and width and halve the\nnumber of channels. Similar to ResNet, a global pooling layer and a fully connected layer are\nconnected at the end to produce the output.\n@d2l.add_to_class(DenseNet)\ndef __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\nlr=0.1, num_classes=10):\nsuper(DenseNet, self).__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(self.b1())\nfor i, num_convs in enumerate(arch):\nself.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs,\ngrowth_rate))\n# The number of output channels in the previous dense block\nnum_channels += num_convs * growth_rate\n# A transition layer that halves the number of channels is added\n# between the dense blocks\nif i != len(arch) - 1:\nnum_channels //= 2\nself.net.add_module(f'tran_blk{i+1}', transition_block(\nnum_channels))\nself.net.add_module('last', nn.Sequential(\nnn.LazyBatchNorm2d(), nn.ReLU(),\nnn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\nnn.LazyLinear(num_classes)))\nself.net.apply(d2l.init_cnn)\n\n325\nDensely Connected Networks (DenseNet)\n8.7.5 Training\nSince we are using a deeper network here, in this section, we will reduce the input height and\nwidth from 224 to 96 to simplify the computation.\nmodel = DenseNet(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\ntrainer.fit(model, data)\n8.7.6 Summary and Discussion\nThe main components that comprise DenseNet are dense blocks and transition layers. For\nthe latter, we need to keep the dimensionality under control when composing the network\nby adding transition layers that shrink the number of channels again. In terms of cross-layer\nconnections, in contrast to ResNet, where inputs and outputs are added together, DenseNet\nconcatenates inputs and outputs on the channel dimension. Although these concatenation op-\nerations reuse features to achieve computational eﬃciency, unfortunately they lead to heavy\nGPU memory consumption. As a result, applying DenseNet may require more memory-\neﬃcient implementations that may increase training time (Pleiss et al., 2017).\n8.7.7 Exercises\n1. Why do we use average pooling rather than max-pooling in the transition layer?\n2. One of the advantages mentioned in the DenseNet paper is that its model parameters are\nsmaller than those of ResNet. Why is this the case?\n3. One problem for which DenseNet has been criticized is its high memory consumption.\n1. Is this really the case? Try to change the input shape to 224 × 224 to compare the\nactual GPU memory consumption empirically.\n\n326\nModern Convolutional Neural Networks\n134\n2. Can you think of an alternative means of reducing the memory consumption? How\nwould you need to change the framework?\n4. Implement the various DenseNet versions presented in Table 1 of the DenseNet paper\n(Huang et al., 2017).\n5. Design an MLP-based model by applying the DenseNet idea. Apply it to the housing price\nprediction task in Section 5.7.\nDiscussions134.\n8.8 Designing Convolution Network Architectures\nThe previous sections have taken us on a tour of modern network design for computer vision.\nCommon to all the work we covered was that it greatly relied on the intuition of scientists.\nMany of the architectures are heavily informed by human creativity and to a much lesser\nextent by systematic exploration of the design space that deep networks oﬀer. Nonetheless,\nthis network engineering approach has been tremendously successful.\nEver since AlexNet (Section 8.1) beat conventional computer vision models on ImageNet, it\nhas become popular to construct very deep networks by stacking blocks of convolutions, all\ndesigned according to the same pattern. In particular, 3×3 convolutions were popularized by\nVGG networks (Section 8.2). NiN (Section 8.3) showed that even 1×1 convolutions could be\nbeneﬁcial by adding local nonlinearities. Moreover, NiN solved the problem of aggregating\ninformation at the head of a network by aggregating across all locations. GoogLeNet (Sec-\ntion 8.4) added multiple branches of diﬀerent convolution width, combining the advantages\nof VGG and NiN in its Inception block. ResNets (Section 8.6) changed the inductive bias\ntowards the identity mapping (from f (x) = 0). This allowed for very deep networks. Almost\na decade later, the ResNet design is still popular, a testament to its design. Lastly, ResNeXt\n(Section 8.6.5) added grouped convolutions, oﬀering a better trade-oﬀbetween parameters\nand computation. A precursor to Transformers for vision, the Squeeze-and-Excitation Net-\nworks (SENets) allow for eﬃcient information transfer between locations (Hu et al., 2018).\nThis was accomplished by computing a per-channel global attention function.\nUp to now we have omitted networks obtained via neural architecture search (NAS) (Liu\net al., 2018, Zoph and Le, 2016). We chose to do so since their cost is usually enormous,\nrelying on brute-force search, genetic algorithms, reinforcement learning, or some other form\nof hyperparameter optimization. Given a ﬁxed search space, NAS uses a search strategy\nto automatically select an architecture based on the returned performance estimation. The\noutcome of NAS is a single network instance. EﬃcientNets are a notable outcome of this\nsearch (Tan and Le, 2019).\nIn the following we discuss an idea that is quite diﬀerent to the quest for the single best\nnetwork. It is computationally relatively inexpensive, it leads to scientiﬁc insights on the way,\n\n327\nDesigning Convolution Network Architectures\nand it is quite eﬀective in terms of the quality of outcomes. Let’s review the strategy by\nRadosavovic et al. (2020) to design network design spaces. The strategy combines the strength\nof manual design and NAS. It accomplishes this by operating on distributions of networks\nand optimizing the distributions in a way to obtain good performance for entire families of\nnetworks. The outcome of it are RegNets, speciﬁcally RegNetX and RegNetY, plus a range\nof guiding principles for the design of performant CNNs.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n8.8.1 The AnyNet Design Space\nThe description below closely follows the reasoning in Radosavovic et al. (2020) with some\nabbreviations to make it ﬁt in the scope of the book. To begin, we need a template for the\nfamily of networks to explore. One of the commonalities of the designs in this chapter is\nthat the networks consist of a stem, a body and a head. The stem performs initial image pro-\ncessing, often through convolutions with a larger window size. The body consists of multiple\nblocks, carrying out the bulk of the transformations needed to go from raw images to object\nrepresentations. Lastly, the head converts this into the desired outputs, such as via a softmax\nregressor for multiclass classiﬁcation. The body, in turn, consists of multiple stages, operat-\ning on the image at decreasing resolutions. In fact, both the stem and each subsequent stage\nquarter the spatial resolution. Lastly, each stage consists of one or more blocks. This pattern\nis common to all networks, from VGG to ResNeXt. Indeed, for the design of generic AnyNet\nnetworks, Radosavovic et al. (2020) used the ResNeXt block of Fig. 8.6.5.\nLet’s review the structure outlined in Fig. 8.8.1 in detail. As mentioned, an AnyNet consists\nof a stem, body, and head. The stem takes as its input RGB images (3 channels), using a 3×3\nconvolution with a stride of 2, followed by a batch norm, to halve the resolution from r × r\nto r/2 × r/2. Moreover, it generates c0 channels that serve as input to the body.\nSince the network is designed to work well with ImageNet images of shape 224 × 224 × 3,\nthe body serves to reduce this to 7×7×c4 through 4 stages (recall that 224/21+4 = 7), each\nwith an eventual stride of 2. Lastly, the head employs an entirely standard design via global\naverage pooling, similar to NiN (Section 8.3), followed by a fully connected layer to emit an\nn-dimensional vector for n-class classiﬁcation.\nMost of the relevant design decisions are inherent to the body of the network. It proceeds in\nstages, where each stage is composed of the same type of ResNeXt blocks as we discussed\nin Section 8.6.5. The design there is again entirely generic: we begin with a block that halves\nthe resolution by using a stride of 2 (the rightmost in Fig. 8.8.1). To match this, the residual\nbranch of the ResNeXt block needs to pass through a 1×1 convolution. This block is followed\nby a variable number of additional ResNeXt blocks that leave both resolution and the number\nof channels unchanged. Note that a common design practice is to add a slight bottleneck in\nthe design of convolutional blocks. As such, with bottleneck ratio ki ≥1 we aﬀord some\n\n328\nModern Convolutional Neural Networks\nt\nFig. 8.8.1\nThe AnyNet design space. The numbers (c, r) along each arrow indicate the number of\nchannels c and the resolution r × r of the images at that point. From left to right: generic\nnetwork structure composed of stem, body, and head; body composed of four stages;\ndetailed structure of a stage; two alternative structures for blocks, one without\ndownsampling and one that halves the resolution in each dimension. Design choices\ninclude depth di, the number of output channels ci, the number of groups gi, and\nbottleneck ratio ki for any stage i.\nnumber of channels, ci/ki, within each block for stage i (as the experiments show, this is not\nreally eﬀective and should be skipped). Lastly, since we are dealing with ResNeXt blocks, we\nalso need to pick the number of groups gi for grouped convolutions at stage i.\nThis seemingly generic design space provides us nonetheless with many parameters: we\ncan set the block width (number of channels) c0, . . . c4, the depth (number of blocks) per\nstage d1, . . . d4, the bottleneck ratios k1, . . . k4, and the group widths (numbers of groups)\ng1, . . . g4. In total this adds up to 17 parameters, resulting in an unreasonably large number of\nconﬁgurations that would warrant exploring. We need some tools to reduce this huge design\nspace eﬀectively. This is where the conceptual beauty of design spaces comes in. Before we\ndo so, let’s implement the generic design ﬁrst.\nclass AnyNet(d2l.Classifier):\ndef stem(self, num_channels):\nreturn nn.Sequential(\nnn.LazyConv2d(num_channels, kernel_size=3, stride=2, padding=1),\nnn.LazyBatchNorm2d(), nn.ReLU())\nEach stage consists of depth ResNeXt blocks, where num_channels speciﬁes the block\nwidth. Note that the ﬁrst block halves the height and width of input images.\n\n329\nDesigning Convolution Network Architectures\n@d2l.add_to_class(AnyNet)\ndef stage(self, depth, num_channels, groups, bot_mul):\nblk = []\nfor i in range(depth):\nif i == 0:\nblk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\nuse_1x1conv=True, strides=2))\nelse:\nblk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\nreturn nn.Sequential(*blk)\nPutting the network stem, body, and head together, we complete the implementation of\nAnyNet.\n@d2l.add_to_class(AnyNet)\ndef __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\nsuper(AnyNet, self).__init__()\nself.save_hyperparameters()\nself.net = nn.Sequential(self.stem(stem_channels))\nfor i, s in enumerate(arch):\nself.net.add_module(f'stage{i+1}', self.stage(*s))\nself.net.add_module('head', nn.Sequential(\nnn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\nnn.LazyLinear(num_classes)))\nself.net.apply(d2l.init_cnn)\n8.8.2 Distributions and Parameters of Design Spaces\nAs just discussed in Section 8.8.1, parameters of a design space are hyperparameters of\nnetworks in that design space. Consider the problem of identifying good parameters in the\nAnyNet design space. We could try ﬁnding the single best parameter choice for a given amount\nof computation (e.g., FLOPs and compute time). If we allowed for even only two possible\nchoices for each parameter, we would have to explore 217 = 131072 combinations to ﬁnd the\nbest solution. This is clearly infeasible because of its exorbitant cost. Even worse, we do not\nreally learn anything from this exercise in terms of how one should design a network. Next\ntime we add, say, an X-stage, or a shift operation, or similar, we would need to start from\nscratch. Even worse, due to the stochasticity in training (rounding, shuﬄing, bit errors), no\ntwo runs are likely to produce exactly the same results. A better strategy would be to try to\ndetermine general guidelines of how the choices of parameters should be related. For instance,\nthe bottleneck ratio, the number of channels, blocks, groups, or their change between layers\nshould ideally be governed by a collection of simple rules. The approach in Radosavovic et\nal. (2019) relies on the following four assumptions:\n1. We assume that general design principles actually exist, so that many networks satisfying\nthese requirements should oﬀer good performance. Consequently, identifying a distribu-\ntion over networks can be a sensible strategy. In other words, we assume that there are\nmany good needles in the haystack.\n\n330\nModern Convolutional Neural Networks\n2. We need not train networks to convergence before we can assess whether a network is\ngood. Instead, it is suﬃcient to use the intermediate results as reliable guidance for ﬁnal\naccuracy. Using (approximate) proxies to optimize an objective is referred to as multi-\nﬁdelity optimization (Forrester et al., 2007). Consequently, design optimization is carried\nout, based on the accuracy achieved after only a few passes through the dataset, reducing\nthe cost signiﬁcantly.\n3. Results obtained at a smaller scale (for smaller networks) generalize to larger ones. Con-\nsequently, optimization is carried out for networks that are structurally similar, but with\na smaller number of blocks, fewer channels, etc. Only in the end will we need to verify\nthat the so-found networks also oﬀer good performance at scale.\n4. Aspects of the design can be approximately factorized so that it is possible to infer their\neﬀect on the quality of the outcome somewhat independently. In other words, the opti-\nmization problem is moderately easy.\nThese assumptions allow us to test many networks cheaply. In particular, we can sample\nuniformly from the space of conﬁgurations and evaluate their performance. Subsequently,\nwe can evaluate the quality of the choice of parameters by reviewing the distribution of er-\nror/accuracy that can be achieved with said networks. Denote by F(e) the cumulative dis-\ntribution function (CDF) for errors committed by networks of a given design space, drawn\nusing probability disribution p. That is,\nF(e, p) def\n= Pnet∼p{e(net) ≤e}.\n(8.8.1)\nOur goal is now to ﬁnd a distribution p over networks such that most networks have a very low\nerror rate and where the support of p is concise. Of course, this is computationally infeasible\nto perform accurately. We resort to a sample of networks Z def\n= {net1, . . . netn} (with errors\ne1, . . ., en, respectively) from p and use the empirical CDF ˆF(e, Z) instead:\nˆF(e, Z) = 1\nn\nn\n∑\ni=1\n1(ei ≤e).\n(8.8.2)\nWhenever the CDF for one set of choices majorizes (or matches) another CDF it follows that\nits choice of parameters is superior (or indiﬀerent). Accordingly Radosavovic et al. (2020)\nexperimented with a shared network bottleneck ratio ki = k for all stages i of the network.\nThis gets rid of three of the four parameters governing the bottleneck ratio. To assess whether\nthis (negatively) aﬀects the performance one can draw networks from the constrained and\nfrom the unconstrained distribution and compare the corresonding CDFs. It turns out that\nthis constraint does not aﬀect the accuracy of the distribution of networks at all, as can be\nseen in the ﬁrst panel of Fig. 8.8.2. Likewise, we could choose to pick the same group width\ngi = g occurring at the various stages of the network. Again, this does not aﬀect performance,\nas can be seen in the second panel of Fig. 8.8.2. Both steps combined reduce the number of\nfree parameters by six.\nNext we look for ways to reduce the multitude of potential choices for width and depth of\nthe stages. It is a reasonable assumption that, as we go deeper, the number of channels should\nincrease, i.e., ci ≥ci−1 (wi+1 ≥wi per their notation in Fig. 8.8.2), yielding AnyNetXD.\n\n331\nDesigning Convolution Network Architectures\nt\nFig. 8.8.2\nComparing error empirical distribution functions of design spaces. AnyNetA is the\noriginal design space; AnyNetB ties the bottleneck ratios, AnyNetC also ties group\nwidths, AnyNetD increases the network depth across stages. From left to right: (i) tying\nbottleneck ratios has no effect on performance; (ii) tying group widths has no effect on\nperformance; (iii) increasing network widths (channels) across stages improves\nperformance; (iv) increasing network depths across stages improves performance. Figure\ncourtesy of Radosavovic et al. (2020).\nLikewise, it is equally reasonable to assume that as the stages progress, they should become\ndeeper, i.e., di ≥di−1, yielding AnyNetXE. This can be experimentally veriﬁed in the third\nand fourth panel of Fig. 8.8.2, respectively.\n8.8.3 RegNet\nThe resulting AnyNetXE design space consists of simple networks following easy-to-interpret\ndesign principles:\n• Share the bottleneck ratio ki = k for all stages i;\n• Share the group width gi = g for all stages i;\n• Increase network width across stages: ci ≤ci+1;\n• Increase network depth across stages: di ≤di+1.\nThis leaves us with a ﬁnal set of choices: how to pick the speciﬁc values for the above pa-\nrameters of the eventual AnyNetXE design space. By studying the best-performing networks\nfrom the distribution in AnyNetXE one can observe the following: the width of the network\nideally increases linearly with the block index across the network, i.e., cj ≈c0 +ca j, where j\nis the block index and slope ca > 0. Given that we get to choose a diﬀerent block width only\nper stage, we arrive at a piecewise constant function, engineered to match this dependence.\nFurthermore, experiments also show that a bottleneck ratio of k = 1 performs best, i.e., we\nare advised not to use bottlenecks at all.\nWe recommend the interested reader reviews further details in the design of speciﬁc networks\nfor diﬀerent amounts of computation by perusing Radosavovic et al. (2020). For instance, an\neﬀective 32-layer RegNetX variant is given by k = 1 (no bottleneck), g = 16 (group width\nis 16), c1 = 32 and c2 = 80 channels for the ﬁrst and second stage, respectively, chosen to be\nd1 = 4 and d2 = 6 blocks deep. The astonishing insight from the design is that it still applies,\neven when investigating networks at a larger scale. Even better, it even holds for Squeeze-and-\n\n332\nModern Convolutional Neural Networks\nExcitation (SE) network designs (RegNetY) that have a global channel activation (Hu et al.,\n2018).\nclass RegNetX32(AnyNet):\ndef __init__(self, lr=0.1, num_classes=10):\nstem_channels, groups, bot_mul = 32, 16, 1\ndepths, channels = (4, 6), (32, 80)\nsuper().__init__(\n((depths[0], channels[0], groups, bot_mul),\n(depths[1], channels[1], groups, bot_mul)),\nstem_channels, lr, num_classes)\nWe can see that each RegNetX stage progressively reduces resolution and increases output\nchannels.\nRegNetX32().layer_summary((1, 1, 96, 96))\nSequential output shape:\ntorch.Size([1, 32, 48, 48])\nSequential output shape:\ntorch.Size([1, 32, 24, 24])\nSequential output shape:\ntorch.Size([1, 80, 12, 12])\nSequential output shape:\ntorch.Size([1, 10])\n8.8.4 Training\nTraining the 32-layer RegNetX on the Fashion-MNIST dataset is just like before.\nmodel = RegNetX32(lr=0.05)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\ntrainer.fit(model, data)\n8.8.5 Discussion\n\n333\nDesigning Convolution Network Architectures\n135\nWith desirable inductive biases (assumptions or preferences) like locality and translation in-\nvariance (Section 7.1) for vision, CNNs have been the dominant architectures in this area.\nThis remained the case from LeNet up until Transformers (Section 11.7) (Dosovitskiy et al.,\n2021, Touvron et al., 2021) started surpassing CNNs in terms of accuracy. While much of\nthe recent progress in terms of vision Transformers can be backported into CNNs (Liu et\nal., 2022), it is only possible at a higher computational cost. Just as importantly, recent hard-\nware optimizations (NVIDIA Ampere and Hopper) have only widened the gap in favor of\nTransformers.\nIt is worth noting that Transformers have a signiﬁcantly lower degree of inductive bias towards\nlocality and translation invariance than CNNs. That learned structures prevailed is due, not\nleast, to the availability of large image collections, such as LAION-400m and LAION-5B\n(Schuhmann et al., 2022) with up to 5 billion images. Quite surprisingly, some of the more\nrelevant work in this context even includes MLPs (Tolstikhin et al., 2021).\nIn sum, vision Transformers (Section 11.8) by now lead in terms of state-of-the-art per-\nformance in large-scale image classiﬁcation, showing that scalability trumps inductive biases\n(Dosovitskiy et al., 2021). This includes pretraining large-scale Transformers (Section 11.9)\nwith multi-head self-attention (Section 11.5). We invite the readers to dive into these chapters\nfor a much more detailed discussion.\n8.8.6 Exercises\n1. Increase the number of stages to four. Can you design a deeper RegNetX that performs\nbetter?\n2. De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How\ndoes your new model perform?\n3. Implement multiple instances of a “VioNet” family by violating the design principles of\nRegNetX. How do they perform? Which of (di, ci, gi, bi) is the most important factor?\n4. Your goal is to design the “perfect” MLP. Can you use the design principles introduced\nabove to ﬁnd good architectures? Is it possible to extrapolate from small to large networks?\nDiscussions135.\n\n9\nRecurrent Neural Networks\nUp until now, we have focused primarily on ﬁxed-length data. When introducing linear and\nlogistic regression in Chapter 3 and Chapter 4 and multilayer perceptrons in Chapter 5, we\nwere happy to assume that each feature vector xi consisted of a ﬁxed number of components\nx1, . . ., xd, where each numerical feature xj corresponded to a particular attribute. These\ndatasets are sometimes called tabular, because they can be arranged in tables, where each\nexample i gets its own row, and each attribute gets its own column. Crucially, with tabular\ndata, we seldom assume any particular structure over the columns.\nSubsequently, in Chapter 7, we moved on to image data, where inputs consist of the raw pixel\nvalues at each coordinate in an image. Image data hardly ﬁtted the bill of a protypical tabular\ndataset. There, we needed to call upon convolutional neural networks (CNNs) to handle the\nhierarchical structure and invariances. However, our data were still of ﬁxed length. Every\nFashion-MNIST image is represented as a 28×28 grid of pixel values. Moreover, our goal was\nto develop a model that looked at just one image and then outputted a single prediction. But\nwhat should we do when faced with a sequence of images, as in a video, or when tasked with\nproducing a sequentially structured prediction, as in the case of image captioning?\nA great many learning tasks require dealing with sequential data. Image captioning, speech\nsynthesis, and music generation all require that models produce outputs consisting of se-\nquences. In other domains, such as time series prediction, video analysis, and musical infor-\nmation retrieval, a model must learn from inputs that are sequences. These demands often\narise simultaneously: tasks such as translating passages of text from one natural language to\nanother, engaging in dialogue, or controlling a robot, demand that models both ingest and\noutput sequentially structured data.\nRecurrent neural networks (RNNs) are deep learning models that capture the dynamics of\nsequences via recurrent connections, which can be thought of as cycles in the network of\nnodes. This might seem counterintuitive at ﬁrst. After all, it is the feedforward nature of neu-\nral networks that makes the order of computation unambiguous. However, recurrent edges\nare deﬁned in a precise way that ensures that no such ambiguity can arise. Recurrent neural\nnetworks are unrolled across time steps (or sequence steps), with the same underlying pa-\nrameters applied at each step. While the standard connections are applied synchronously to\npropagate each layer’s activations to the subsequent layer at the same time step, the recurrent\nconnections are dynamic, passing information across adjacent time steps. As the unfolded\nview in Fig. 9.1 reveals, RNNs can be thought of as feedforward neural networks where each\nlayer’s parameters (both conventional and recurrent) are shared across time steps.\n334\n\n335\nRecurrent Neural Networks\nt\nFig. 9.1\nOn the left recurrent connections are depicted via cyclic edges. On the right, we unfold the\nRNN over time steps. Here, recurrent edges span adjacent time steps, while conventional\nconnections are computed synchronously.\nLike neural networks more broadly, RNNs have a long discipline-spanning history, originat-\ning as models of the brain popularized by cognitive scientists and subsequently adopted as\npractical modeling tools employed by the machine learning community. As we do for deep\nlearning more broadly, in this book we adopt the machine learning perspective, focusing on\nRNNs as practical tools that rose to popularity in the 2010s owing to breakthrough results\non such diverse tasks as handwriting recognition (Graves et al., 2008), machine translation\n(Sutskever et al., 2014), and recognizing medical diagnoses (Lipton et al., 2016). We point\nthe reader interested in more background material to a publicly available comprehensive re-\nview (Lipton et al., 2015). We also note that sequentiality is not unique to RNNs. For exam-\nple, the CNNs that we already introduced can be adapted to handle data of varying length,\ne.g., images of varying resolution. Moreover, RNNs have recently ceded considerable market\nshare to Transformer models, which will be covered in Chapter 11. However, RNNs rose to\nprominence as the default models for handling complex sequential structure in deep learning,\nand remain staple models for sequential modeling to this day. The stories of RNNs and of\nsequence modeling are inextricably linked, and this is as much a chapter about the ABCs of\nsequence modeling problems as it is a chapter about RNNs.\nOne key insight paved the way for a revolution in sequence modeling. While the inputs and\ntargets for many fundamental tasks in machine learning cannot easily be represented as ﬁxed-\nlength vectors, they can often nevertheless be represented as varying-length sequences of\nﬁxed-length vectors. For example, documents can be represented as sequences of words;\nmedical records can often be represented as sequences of events (encounters, medications,\nprocedures, lab tests, diagnoses); videos can be represented as varying-length sequences of\nstill images.\nWhile sequence models have popped up in numerous application areas, basic research in the\narea has been driven predominantly by advances on core tasks in natural language process-\ning. Thus, throughout this chapter, we will focus our exposition and examples on text data.\nIf you get the hang of these examples, then applying the models to other data modalities\nshould be relatively straightforward. In the next few sections, we introduce basic notation for\nsequences and some evaluation measures for assessing the quality of sequentially structured\nmodel outputs. After that, we discuss basic concepts of a language model and use this dis-\n\n336\nRecurrent Neural Networks\ncussion to motivate our ﬁrst RNN models. Finally, we describe the method for calculating\ngradients when backpropagating through RNNs and explore some challenges that are often\nencountered when training such networks, motivating the modern RNN architectures that\nwill follow in Chapter 10.\n9.1 Working with Sequences\nUp until now, we have focused on models whose inputs consisted of a single feature vector\nx ∈Rd. The main change of perspective when developing models capable of processing\nsequences is that we now focus on inputs that consist of an ordered list of feature vectors\nx1, . . ., xT, where each feature vector xt is indexed by a time step t ∈Z+ lying in Rd.\nSome datasets consist of a single massive sequence. Consider, for example, the extremely long\nstreams of sensor readings that might be available to climate scientists. In such cases, we might\ncreate training datasets by randomly sampling subsequences of some predetermined length.\nMore often, our data arrives as a collection of sequences. Consider the following examples:\n(i) a collection of documents, each represented as its own sequence of words, and each having\nits own length Ti; (ii) sequence representation of patient stays in the hospital, where each stay\nconsists of a number of events and the sequence length depends roughly on the length of the\nstay.\nPreviously, when dealing with individual inputs, we assumed that they were sampled inde-\npendently from the same underlying distribution P(X). While we still assume that entire se-\nquences (e.g., entire documents or patient trajectories) are sampled independently, we cannot\nassume that the data arriving at each time step are independent of each other. For example,\nthe words that likely to appear later in a document depend heavily on words occurring earlier\nin the document. The medicine a patient is likely to receive on the 10th day of a hospital visit\ndepends heavily on what transpired in the previous nine days.\nThis should come as no surprise. If we did not believe that the elements in a sequence were\nrelated, we would not have bothered to model them as a sequence in the ﬁrst place. Consider\nthe usefulness of the auto-ﬁll features that are popular on search tools and modern email\nclients. They are useful precisely because it is often possible to predict (imperfectly, but better\nthan random guessing) what the likely continuations of a sequence might be, given some initial\npreﬁx. For most sequence models, we do not require independence, or even stationarity, of\nour sequences. Instead, we require only that the sequences themselves are sampled from some\nﬁxed underlying distribution over entire sequences.\nThis ﬂexible approach allows for such phenomena as (i) documents looking signiﬁcantly dif-\nferent at the beginning than at the end; or (ii) patient status evolving either towards recovery or\ntowards death over the course of a hospital stay; or (iii) customer taste evolving in predictable\nways over the course of continued interaction with a recommender system.\n\n337\nWorking with Sequences\nWe sometimes wish to predict a ﬁxed target y given sequentially structured input (e.g., senti-\nment classiﬁcation based on a movie review). At other times, we wish to predict a sequentially\nstructured target (y1, . . ., yT) given a ﬁxed input (e.g., image captioning). Still other times,\nour goal is to predict sequentially structured targets based on sequentially structured inputs\n(e.g., machine translation or video captioning). Such sequence-to-sequence tasks take two\nforms: (i) aligned: where the input at each time step aligns with a corresponding target (e.g.,\npart of speech tagging); (ii) unaligned: where the input and target do not necessarily exhibit\na step-for-step correspondence (e.g., machine translation).\nBefore we worry about handling targets of any kind, we can tackle the most straightforward\nproblem: unsupervised density modeling (also called sequence modeling). Here, given a col-\nlection of sequences, our goal is to estimate the probability mass function that tells us how\nlikely we are to see any given sequence, i.e., p(x1, . . ., xT).\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n9.1.1 Autoregressive Models\nBefore introducing specialized neural networks designed to handle sequentially structured\ndata, let’s take a look at some actual sequence data and build up some basic intuitions and\nstatistical tools. In particular, we will focus on stock price data from the FTSE 100 index (Fig.\n9.1.1). At each time step t ∈Z+, we observe the price, xt, of the index at that time.\nt\nFig. 9.1.1\nFTSE 100 index over about 30 years.\nNow suppose that a trader would like to make short-term trades, strategically getting into\nor out of the index, depending on whether they believe that it will rise or decline in the\nsubsequent time step. Absent any other features (news, ﬁnancial reporting data, etc.), the\n\n338\nRecurrent Neural Networks\nonly available signal for predicting the subsequent value is the history of prices to date. The\ntrader is thus interested in knowing the probability distribution\nP(xt | xt−1, . . ., x1)\n(9.1.1)\nover prices that the index might take in the subsequent time step. While estimating the entire\ndistribution over a continuously valued random variable can be diﬃcult, the trader would be\nhappy to focus on a few key statistics of the distribution, particularly the expected value and\nthe variance. One simple strategy for estimating the conditional expectation\nE[(xt | xt−1, . . ., x1)],\n(9.1.2)\nwould be to apply a linear regression model (recall Section 3.1). Such models that regress\nthe value of a signal on the previous values of that same signal are naturally called autore-\ngressive models. There is just one major problem: the number of inputs, xt−1, . . ., x1 varies,\ndepending on t. In other words, the number of inputs increases with the amount of data that\nwe encounter. Thus if we want to treat our historical data as a training set, we are left with\nthe problem that each example has a diﬀerent number of features. Much of what follows in\nthis chapter will revolve around techniques for overcoming these challenges when engaging\nin such autoregressive modeling problems where the object of interest is P(xt | xt−1, . . ., x1)\nor some statistic(s) of this distribution.\nA few strategies recur frequently. First of all, we might believe that although long sequences\nxt−1, . . ., x1 are available, it may not be necessary to look back so far in the history when\npredicting the near future. In this case we might content ourselves to condition on some\nwindow of length τ and only use xt−1, . . ., xt−τ observations. The immediate beneﬁt is that\nnow the number of arguments is always the same, at least for t > τ. This allows us to train any\nlinear model or deep network that requires ﬁxed-length vectors as inputs. Second, we might\ndevelop models that maintain some summary ht of the past observations (see Fig. 9.1.2) and\nat the same time update ht in addition to the prediction ˆxt. This leads to models that estimate\nnot only xt with ˆxt = P(xt | ht) but also updates of the form ht = g(ht−1, xt−1). Since ht\nis never observed, these models are also called latent autoregressive models.\nt\nFig. 9.1.2\nA latent autoregressive model.\nTo construct training data from historical data, one typically creates examples by sampling\nwindows randomly. In general, we do not expect time to stand still. However, we often assume\nthat while the speciﬁc values of xt might change, the dynamics according to which each\nsubsequent observation is generated given the previous observations do not. Statisticians call\ndynamics that do not change stationary.\n\n339\nWorking with Sequences\n9.1.2 Sequence Models\nSometimes, especially when working with language, we wish to estimate the joint probabil-\nity of an entire sequence. This is a common task when working with sequences composed\nof discrete tokens, such as words. Generally, these estimated functions are called sequence\nmodels and for natural language data, they are called language models. The ﬁeld of sequence\nmodeling has been driven so much by natural language processing, that we often describe se-\nquence models as “language models”, even when dealing with non-language data. Language\nmodels prove useful for all sorts of reasons. Sometimes we want to evaluate the likelihood of\nsentences. For example, we might wish to compare the naturalness of two candidate outputs\ngenerated by a machine translation system or by a speech recognition system. But language\nmodeling gives us not only the capacity to evaluate likelihood, but the ability to sample se-\nquences, and even to optimize for the most likely sequences.\nWhile language modeling might not, at ﬁrst glance, look like an autoregressive problem, we\ncan reduce language modeling to autoregressive prediction by decomposing the joint density\nof a sequence p(x1, . . ., xT) into the product of conditional densities in a left-to-right fashion\nby applying the chain rule of probability:\nP(x1, . . ., xT) = P(x1)\nT\n∏\nt=2\nP(xt | xt−1, . . ., x1).\n(9.1.3)\nNote that if we are working with discrete signals such as words, then the autoregressive model\nmust be a probabilistic classiﬁer, outputting a full probability distribution over the vocabulary\nfor whatever word will come next, given the leftwards context.\nMarkov Models\nNow suppose that we wish to employ the strategy mentioned above, where we condition\nonly on the τ previous time steps, i.e., xt−1, . . ., xt−τ, rather than the entire sequence history\nxt−1, . . ., x1. Whenever we can throw away the history beyond the previous τ steps without\nany loss in predictive power, we say that the sequence satisﬁes a Markov condition, i.e., that\nthe future is conditionally independent of the past, given the recent history. When τ = 1, we say\nthat the data is characterized by a ﬁrst-order Markov model, and when τ = k, we say that the\ndata is characterized by a kth-order Markov model. For when the ﬁrst-order Markov condition\nholds (τ = 1) the factorization of our joint probability becomes a product of probabilities of\neach word given the previous word:\nP(x1, . . ., xT) = P(x1)\nT\n∏\nt=2\nP(xt | xt−1).\n(9.1.4)\nWe often ﬁnd it useful to work with models that proceed as though a Markov condition were\nsatisﬁed, even when we know that this is only approximately true. With real text documents we\ncontinue to gain information as we include more and more leftwards context. But these gains\ndiminish rapidly. Thus, sometimes we compromise, obviating computational and statistical\ndiﬃculties by training models whose validity depends on a kth-order Markov condition. Even\n\n340\nRecurrent Neural Networks\ntoday’s massive RNN- and Transformer-based language models seldom incorporate more\nthan thousands of words of context.\nWith discrete data, a true Markov model simply counts the number of times that each word\nhas occurred in each context, producing the relative frequency estimate of P(xt | xt−1).\nWhenever the data assumes only discrete values (as in language), the most likely sequence of\nwords can be computed eﬃciently using dynamic programming.\nThe Order of Decoding\nYou may be wondering why we represented the factorization of a text sequence P(x1, . . ., xT)\nas a left-to-right chain of conditional probabilities. Why not right-to-left or some other, seem-\ningly random order? In principle, there is nothing wrong with unfolding P(x1, . . ., xT) in\nreverse order. The result is a valid factorization:\nP(x1, . . ., xT) = P(xT)\n1\n∏\nt=T−1\nP(xt | xt+1, . . ., xT).\n(9.1.5)\nHowever, there are many reasons why factorizing text in the same direction in which we read it\n(left-to-right for most languages, but right-to-left for Arabic and Hebrew) is preferred for the\ntask of language modeling. First, this is just a more natural direction for us to think about.\nAfter all we all read text every day, and this process is guided by our ability to anticipate\nwhich words and phrases are likely to come next. Just think of how many times you have\ncompleted someone else’s sentence. Thus, even if we had no other reason to prefer such\nin-order decodings, they would be useful if only because we have better intuitions for what\nshould be likely when predicting in this order.\nSecond, by factorizing in order, we can assign probabilities to arbitrarily long sequences using\nthe same language model. To convert a probability over steps 1 through t into one that extends\nto word t + 1 we simply multiply by the conditional probability of the additional token given\nthe previous ones: P(xt+1, . . ., x1) = P(xt, . . ., x1) · P(xt+1 | xt, . . ., x1).\nThird, we have stronger predictive models for predicting adjacent words than words at arbi-\ntrary other locations. While all orders of factorization are valid, they do not necessarily all\nrepresent equally easy predictive modeling problems. This is true not only for language, but\nfor other kinds of data as well, e.g., when the data is causally structured. For example, we\nbelieve that future events cannot inﬂuence the past. Hence, if we change xt, we may be able\nto inﬂuence what happens for xt+1 going forward but not the converse. That is, if we change\nxt, the distribution over past events will not change. In some contexts, this makes it easier to\npredict P(xt+1 | xt) than to predict P(xt | xt+1). For instance, in some cases, we can ﬁnd\nxt+1 = f (xt) + ϵ for some additive noise ϵ, whereas the converse is not true (Hoyer et al.,\n2009). This is great news, since it is typically the forward direction that we are interested in\nestimating. The book by Peters et al. (2017) contains more on this topic. We barely scratch\nthe surface of it.\n9.1.3 Training\n\n341\nWorking with Sequences\nBefore we focus our attention on text data, let’s ﬁrst try this out with some continuous-valued\nsynthetic data.\nHere, our 1000 synthetic data will follow the trigonometric sin function, applied to 0.01 times\nthe time step. To make the problem a little more interesting, we corrupt each sample with\nadditive noise. From this sequence we extract training examples, each consisting of features\nand a label.\nclass Data(d2l.DataModule):\ndef __init__(self, batch_size=16, T=1000, num_train=600, tau=4):\nself.save_hyperparameters()\nself.time = torch.arange(1, T + 1, dtype=torch.float32)\nself.x = torch.sin(0.01 * self.time) + torch.randn(T) * 0.2\ndata = Data()\nd2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))\nTo begin, we try a model that acts as if the data satisﬁed a τth-order Markov condition,\nand thus predicts xt using only the past τ observations. Thus for each time step we have an\nexample with label y = xt and features xt = [xt−τ, . . ., xt−1]. The astute reader might have\nnoticed that this results in 1000 −τ examples, since we lack suﬃcient history for y1, . . ., yτ.\nWhile we could pad the ﬁrst τ sequences with zeros, to keep things simple, we drop them\nfor now. The resulting dataset contains T −τ examples, where each input to the model has\nsequence length τ. We create a data iterator on the ﬁrst 600 examples, covering a period of\nthe sin function.\n@d2l.add_to_class(Data)\ndef get_dataloader(self, train):\nfeatures = [self.x[i : self.T-self.tau+i] for i in range(self.tau)]\nself.features = torch.stack(features, 1)\nself.labels = self.x[self.tau:].reshape((-1, 1))\ni = slice(0, self.num_train) if train else slice(self.num_train, None)\nreturn self.get_tensorloader([self.features, self.labels], train, i)\n\n342\nRecurrent Neural Networks\nIn this example our model will be a standard linear regression.\nmodel = d2l.LinearRegression(lr=0.01)\ntrainer = d2l.Trainer(max_epochs=5)\ntrainer.fit(model, data)\n9.1.4 Prediction\nTo evaluate our model, we ﬁrst check how well it performs at one-step-ahead prediction.\nonestep_preds = model(data.features).detach().numpy()\nd2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\nlegend=['labels', '1-step preds'], figsize=(6, 3))\nThese predictions look good, even near the end at t = 1000.\nBut what if we only observed sequence data up until time step 604 (n_train + tau) and\nwished to make predictions several steps into the future? Unfortunately, we cannot directly\ncompute the one-step-ahead prediction for time step 609, because we do not know the cor-\nresponding inputs, having seen only up to x604. We can address this problem by plugging in\n\n343\nWorking with Sequences\nour earlier predictions as inputs to our model for making subsequent predictions, projecting\nforward, one step at a time, until reaching the desired time step:\nˆx605 = f (x601, x602, x603, x604),\nˆx606 = f (x602, x603, x604, ˆx605),\nˆx607 = f (x603, x604, ˆx605, ˆx606),\nˆx608 = f (x604, ˆx605, ˆx606, ˆx607),\nˆx609 = f (ˆx605, ˆx606, ˆx607, ˆx608),\n...\n(9.1.6)\nGenerally, for an observed sequence x1, . . ., xt, its predicted output ˆxt+k at time step t + k\nis called the k-step-ahead prediction. Since we have observed up to x604, its k-step-ahead\nprediction is ˆx604+k. In other words, we will have to keep on using our own predictions to\nmake multistep-ahead predictions. Let’s see how well this goes.\nmultistep_preds = torch.zeros(data.T)\nmultistep_preds[:] = data.x\nfor i in range(data.num_train + data.tau, data.T):\nmultistep_preds[i] = model(\nmultistep_preds[i - data.tau:i].reshape((1, -1)))\nmultistep_preds = multistep_preds.detach().numpy()\nd2l.plot([data.time[data.tau:], data.time[data.num_train+data.tau:]],\n[onestep_preds, multistep_preds[data.num_train+data.tau:]], 'time',\n'x', legend=['1-step preds', 'multistep preds'], figsize=(6, 3))\nUnfortunately, in this case we fail spectacularly. The predictions decay to a constant pretty\nquickly after a few steps. Why did the algorithm perform so much worse when predicting\nfurther into the future? Ultimately, this is down to the fact that errors build up. Let’s say\nthat after step 1 we have some error ϵ1 = ¯ϵ. Now the input for step 2 is perturbed by ϵ1,\nhence we suﬀer some error in the order of ϵ2 = ¯ϵ + cϵ1 for some constant c, and so on.\nThe predictions can diverge rapidly from the true observations. You may already be familiar\n\n344\nRecurrent Neural Networks\nwith this common phenomenon. For instance, weather forecasts for the next 24 hours tend\nto be pretty accurate but beyond that, accuracy declines rapidly. We will discuss methods for\nimproving this throughout this chapter and beyond.\nLet’s take a closer look at the diﬃculties in k-step-ahead predictions by computing predictions\non the entire sequence for k = 1, 4, 16, 64.\ndef k_step_pred(k):\nfeatures = []\nfor i in range(data.tau):\nfeatures.append(data.x[i : i+data.T-data.tau-k+1])\n# The (i+tau)-th element stores the (i+1)-step-ahead predictions\nfor i in range(k):\npreds = model(torch.stack(features[i : i+data.tau], 1))\nfeatures.append(preds.reshape(-1))\nreturn features[data.tau:]\nsteps = (1, 4, 16, 64)\npreds = k_step_pred(steps[-1])\nd2l.plot(data.time[data.tau+steps[-1]-1:],\n[preds[k - 1].detach().numpy() for k in steps], 'time', 'x',\nlegend=[f'{k}-step preds' for k in steps], figsize=(6, 3))\nThis clearly illustrates how the quality of the prediction changes as we try to predict further\ninto the future. While the 4-step-ahead predictions still look good, anything beyond that is\nalmost useless.\n9.1.5 Summary\nThere is quite a diﬀerence in diﬃculty between interpolation and extrapolation. Conse-\nquently, if you have a sequence, always respect the temporal order of the data when training,\ni.e., never train on future data. Given this kind of data, sequence models require specialized\nstatistical tools for estimation. Two popular choices are autoregressive models and latent-\nvariable autoregressive models. For causal models (e.g., time going forward), estimating the\n\n345\nConverting Raw Text into Sequence Data\n136\nforward direction is typically a lot easier than the reverse direction. For an observed sequence\nup to time step t, its predicted output at time step t + k is the k-step-ahead prediction. As we\npredict further in time by increasing k, the errors accumulate and the quality of the prediction\ndegrades, often dramatically.\n9.1.6 Exercises\n1. Improve the model in the experiment of this section.\n1. Incorporate more than the past four observations? How many do you really need?\n2. How many past observations would you need if there was no noise? Hint: you can write\nsin and cos as a diﬀerential equation.\n3. Can you incorporate older observations while keeping the total number of features\nconstant? Does this improve accuracy? Why?\n4. Change the neural network architecture and evaluate the performance. You may train\nthe new model with more epochs. What do you observe?\n2. An investor wants to ﬁnd a good security to buy. They look at past returns to decide which\none is likely to do well. What could possibly go wrong with this strategy?\n3. Does causality also apply to text? To which extent?\n4. Give an example for when a latent autoregressive model might be needed to capture the\ndynamic of the data.\nDiscussions136.\n9.2 Converting Raw Text into Sequence Data\nThroughout this book, we will often work with text data represented as sequences of words,\ncharacters, or word pieces. To get going, we will need some basic tools for converting raw text\ninto sequences of the appropriate form. Typical preprocessing pipelines execute the following\nsteps:\n1. Load text as strings into memory.\n2. Split the strings into tokens (e.g., words or characters).\n3. Build a vocabulary dictionary to associate each vocabulary element with a numerical in-\ndex.\n4. Convert the text into sequences of numerical indices.\n\n346\nRecurrent Neural Networks\n137\nimport collections\nimport random\nimport re\nimport torch\nfrom d2l import torch as d2l\n9.2.1 Reading the Dataset\nHere, we will work with H. G. Wells’ The Time Machine 137 , a book containing just over\n30,000 words. While real applications will typically involve signiﬁcantly larger datasets, this\nis suﬃcient to demonstrate the preprocessing pipeline. The following _download method\nreads the raw text into a string.\nclass TimeMachine(d2l.DataModule): #@save\n\"\"\"The Time Machine dataset.\"\"\"\ndef _download(self):\nfname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n'090b5e7e70c295757f55df93cb0a180b9691891a')\nwith open(fname) as f:\nreturn f.read()\ndata = TimeMachine()\nraw_text = data._download()\nraw_text[:60]\n'The Time Machine, by H. G. Wells [1898]nnnnnInnnThe Time Tra'\nFor simplicity, we ignore punctuation and capitalization when preprocessing the raw text.\n@d2l.add_to_class(TimeMachine)\n#@save\ndef _preprocess(self, text):\nreturn re.sub('[^A-Za-z]+', ' ', text).lower()\ntext = data._preprocess(raw_text)\ntext[:60]\n'the time machine by h g wells i the time traveller for so it'\n9.2.2 Tokenization\nTokens are the atomic (indivisible) units of text. Each time step corresponds to 1 token, but\nwhat precisely constitutes a token is a design choice. For example, we could represent the\nsentence “Baby needs a new pair of shoes” as a sequence of 7 words, where the set of all\nwords comprise a large vocabulary (typically tens or hundreds of thousands of words). Or\nwe would represent the same sentence as a much longer sequence of 30 characters, using a\n\n347\nConverting Raw Text into Sequence Data\nmuch smaller vocabulary (there are only 256 distinct ASCII characters). Below, we tokenize\nour preprocessed text into a sequence of characters.\n@d2l.add_to_class(TimeMachine)\n#@save\ndef _tokenize(self, text):\nreturn list(text)\ntokens = data._tokenize(text)\n','.join(tokens[:30])\n't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '\n9.2.3 Vocabulary\nThese tokens are still strings. However, the inputs to our models must ultimately consist of\nnumerical inputs. Next, we introduce a class for constructing vocabularies, i.e., objects that\nassociate each distinct token value with a unique index. First, we determine the set of unique\ntokens in our training corpus. We then assign a numerical index to each unique token. Rare\nvocabulary elements are often dropped for convenience. Whenever we encounter a token at\ntraining or test time that had not been previously seen or was dropped from the vocabulary,\nwe represent it by a special “<unk>” token, signifying that this is an unknown value.\nclass Vocab:\n#@save\n\"\"\"Vocabulary for text.\"\"\"\ndef __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n# Flatten a 2D list if needed\nif tokens and isinstance(tokens[0], list):\ntokens = [token for line in tokens for token in line]\n# Count token frequencies\ncounter = collections.Counter(tokens)\nself.token_freqs = sorted(counter.items(), key=lambda x: x[1],\nreverse=True)\n# The list of unique tokens\nself.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\ntoken for token, freq in self.token_freqs if freq >= min_freq])))\nself.token_to_idx = {token: idx\nfor idx, token in enumerate(self.idx_to_token)}\ndef __len__(self):\nreturn len(self.idx_to_token)\ndef __getitem__(self, tokens):\nif not isinstance(tokens, (list, tuple)):\nreturn self.token_to_idx.get(tokens, self.unk)\nreturn [self.__getitem__(token) for token in tokens]\ndef to_tokens(self, indices):\nif hasattr(indices, '__len__') and len(indices) > 1:\nreturn [self.idx_to_token[int(index)] for index in indices]\nreturn self.idx_to_token[indices]\n(continues on next page)\n\n348\nRecurrent Neural Networks\n(continued from previous page)\n@property\ndef unk(self):\n# Index for the unknown token\nreturn self.token_to_idx['<unk>']\nWe now construct a vocabulary for our dataset, converting the sequence of strings into a list\nof numerical indices. Note that we have not lost any information and can easily convert our\ndataset back to its original (string) representation.\nvocab = Vocab(tokens)\nindices = vocab[tokens[:10]]\nprint('indices:', indices)\nprint('words:', vocab.to_tokens(indices))\nindices: [21, 9, 6, 0, 21, 10, 14, 6, 0, 14]\nwords: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm']\n9.2.4 Putting It All Together\nUsing the above classes and methods, we package everything into the following build method\nof the TimeMachine class, which returns corpus, a list of token indices, and vocab, the\nvocabulary of The Time Machine corpus. The modiﬁcations we did here are: (i) we tokenize\ntext into characters, not words, to simplify the training in later sections; (ii) corpus is a single\nlist, not a list of token lists, since each text line in The Time Machine dataset is not necessarily\na sentence or paragraph.\n@d2l.add_to_class(TimeMachine)\n#@save\ndef build(self, raw_text, vocab=None):\ntokens = self._tokenize(self._preprocess(raw_text))\nif vocab is None: vocab = Vocab(tokens)\ncorpus = [vocab[token] for token in tokens]\nreturn corpus, vocab\ncorpus, vocab = data.build(raw_text)\nlen(corpus), len(vocab)\n(173428, 28)\n9.2.5 Exploratory Language Statistics\nUsing the real corpus and the Vocab class deﬁned over words, we can inspect basic statistics\nconcerning word use in our corpus. Below, we construct a vocabulary from words used in\nThe Time Machine and print the ten most frequently occurring of them.\n\n349\nConverting Raw Text into Sequence Data\nwords = text.split()\nvocab = Vocab(words)\nvocab.token_freqs[:10]\n[('the', 2261),\n('i', 1267),\n('and', 1245),\n('of', 1155),\n('a', 816),\n('to', 695),\n('was', 552),\n('in', 541),\n('that', 443),\n('my', 440)]\nNote that the ten most frequent words are not all that descriptive. You might even imagine\nthat we might see a very similar list if we had chosen any book at random. Articles like “the”\nand “a”, pronouns like “i” and “my”, and prepositions like “of”, “to”, and “in” occur often\nbecause they serve common syntactic roles. Such words that are common but not particularly\ndescriptive are often called stop words and, in previous generations of text classiﬁers based\non so-called bag-of-words representations, they were most often ﬁltered out. However, they\ncarry meaning and it is not necessary to ﬁlter them out when working with modern RNN-\nand Transformer-based neural models. If you look further down the list, you will notice that\nword frequency decays quickly. The 10th most frequent word is less than 1/5 as common\nas the most popular. Word frequency tends to follow a power law distribution (speciﬁcally\nthe Zipﬁan) as we go down the ranks. To get a better idea, we plot the ﬁgure of the word\nfrequency.\nfreqs = [freq for token, freq in vocab.token_freqs]\nd2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\nxscale='log', yscale='log')\nAfter dealing with the ﬁrst few words as exceptions, all the remaining words roughly follow a\nstraight line on a log–log plot. This phenomenon is captured by Zipf’s law, which states that\n\n350\nRecurrent Neural Networks\nthe frequency ni of the ith most frequent word is:\nni ∝1\niα,\n(9.2.1)\nwhich is equivalent to\nlog ni = −α logi + c,\n(9.2.2)\nwhere α is the exponent that characterizes the distribution and c is a constant. This should\nalready give us pause for thought if we want to model words by counting statistics. After all, we\nwill signiﬁcantly overestimate the frequency of the tail, also known as the infrequent words.\nBut what about the other word combinations, such as two consecutive words (bigrams), three\nconsecutive words (trigrams), and beyond? Let’s see whether the bigram frequency behaves\nin the same manner as the single word (unigram) frequency.\nbigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\nbigram_vocab = Vocab(bigram_tokens)\nbigram_vocab.token_freqs[:10]\n[('of--the', 309),\n('in--the', 169),\n('i--had', 130),\n('i--was', 112),\n('and--the', 109),\n('the--time', 102),\n('it--was', 99),\n('to--the', 85),\n('as--i', 78),\n('of--a', 73)]\nOne thing is notable here. Out of the ten most frequent word pairs, nine are composed of\nboth stop words and only one is relevant to the actual book—“the time”. Furthermore, let’s\nsee whether the trigram frequency behaves in the same manner.\ntrigram_tokens = ['--'.join(triple) for triple in zip(\nwords[:-2], words[1:-1], words[2:])]\ntrigram_vocab = Vocab(trigram_tokens)\ntrigram_vocab.token_freqs[:10]\n[('the--time--traveller', 59),\n('the--time--machine', 30),\n('the--medical--man', 24),\n('it--seemed--to', 16),\n('it--was--a', 15),\n('here--and--there', 15),\n('seemed--to--me', 14),\n('i--did--not', 14),\n('i--saw--the', 13),\n('i--began--to', 13)]\n\n351\nConverting Raw Text into Sequence Data\nNow, let’s visualize the token frequency among these three models: unigrams, bigrams, and\ntrigrams.\nbigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\ntrigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\nylabel='frequency: n(x)', xscale='log', yscale='log',\nlegend=['unigram', 'bigram', 'trigram'])\nThis ﬁgure is quite exciting. First, beyond unigram words, sequences of words also appear\nto be following Zipf’s law, albeit with a smaller exponent α in (9.2.1), depending on the\nsequence length. Second, the number of distinct n-grams is not that large. This gives us hope\nthat there is quite a lot of structure in language. Third, many n-grams occur very rarely.\nThis makes certain methods unsuitable for language modeling and motivates the use of deep\nlearning models. We will discuss this in the next section.\n9.2.6 Summary\nText is among the most common forms of sequence data encountered in deep learning. Com-\nmon choices for what constitutes a token are characters, words, and word pieces. To prepro-\ncess text, we usually (i) split text into tokens; (ii) build a vocabulary to map token strings to\nnumerical indices; and (iii) convert text data into token indices for models to manipulate. In\npractice, the frequency of words tends to follow Zipf’s law. This is true not just for individual\nwords (unigrams), but also for n-grams.\n9.2.7 Exercises\n1. In the experiment of this section, tokenize text into words and vary the min_freq argu-\nment value of the Vocab instance. Qualitatively characterize how changes in min_freq\nimpact the size of the resulting vocabulary.\n2. Estimate the exponent of Zipﬁan distribution for unigrams, bigrams, and trigrams in this\ncorpus.\n\n352\nRecurrent Neural Networks\n138\n3. Find some other sources of data (download a standard machine learning dataset, pick\nanother public domain book, scrape a website, etc). For each, tokenize the data at both the\nword and character levels. How do the vocabulary sizes compare with The Time Machine\ncorpus at equivalent values of min_freq. Estimate the exponent of the Zipﬁan distribution\ncorresponding to the unigram and bigram distributions for these corpora. How do they\ncompare with the values that you observed for The Time Machine corpus?\nDiscussions138.\n9.3 Language Models\nIn Section 9.2, we saw how to map text sequences into tokens, where these tokens can be\nviewed as a sequence of discrete observations such as words or characters. Assume that the\ntokens in a text sequence of length T are in turn x1, x2, . . ., xT. The goal of language models\nis to estimate the joint probability of the whole sequence:\nP(x1, x2, . . ., xT),\n(9.3.1)\nwhere statistical tools in Section 9.1 can be applied.\nLanguage models are incredibly useful. For instance, an ideal language model should generate\nnatural text on its own, simply by drawing one token at a time xt ∼P(xt | xt−1, . . ., x1).\nQuite unlike the monkey using a typewriter, all text emerging from such a model would pass\nas natural language, e.g., English text. Furthermore, it would be suﬃcient for generating a\nmeaningful dialog, simply by conditioning the text on previous dialog fragments. Clearly we\nare still very far from designing such a system, since it would need to understand the text\nrather than just generate grammatically sensible content.\nNonetheless, language models are of great service even in their limited form. For instance,\nthe phrases “to recognize speech” and “to wreck a nice beach” sound very similar. This can\ncause ambiguity in speech recognition, which is easily resolved through a language model that\nrejects the second translation as outlandish. Likewise, in a document summarization algorithm\nit is worthwhile knowing that “dog bites man” is much more frequent than “man bites dog”, or\nthat “I want to eat grandma” is a rather disturbing statement, whereas “I want to eat, grandma”\nis much more benign.\nimport torch\nfrom d2l import torch as d2l\n9.3.1 Learning Language Models\n\n353\nLanguage Models\n139\nThe obvious question is how we should model a document, or even a sequence of tokens.\nSuppose that we tokenize text data at the word level. Let’s start by applying basic probability\nrules:\nP(x1, x2, . . ., xT) =\nT\n∏\nt=1\nP(xt | x1, . . ., xt−1).\n(9.3.2)\nFor example, the probability of a text sequence containing four words would be given as:\nP(deep, learning, is, fun)\n=P(deep)P(learning | deep)P(is | deep, learning)P(fun | deep, learning, is).\n(9.3.3)\nMarkov Models and n-grams\nAmong those sequence model analyses in Section 9.1, let’s apply Markov models to lan-\nguage modeling. A distribution over sequences satisﬁes the Markov property of ﬁrst order\nif P(xt+1 | xt, . . ., x1) = P(xt+1 | xt). Higher orders correspond to longer dependencies.\nThis leads to a number of approximations that we could apply to model a sequence:\nP(x1, x2, x3, x4) = P(x1)P(x2)P(x3)P(x4),\nP(x1, x2, x3, x4) = P(x1)P(x2 | x1)P(x3 | x2)P(x4 | x3),\nP(x1, x2, x3, x4) = P(x1)P(x2 | x1)P(x3 | x1, x2)P(x4 | x2, x3).\n(9.3.4)\nThe probability formulae that involve one, two, and three variables are typically referred to as\nunigram, bigram, and trigram models, respectively. In order to compute the language model,\nwe need to calculate the probability of words and the conditional probability of a word given\nthe previous few words. Note that such probabilities are language model parameters.\nWord Frequency\nHere, we assume that the training dataset is a large text corpus, such as all Wikipedia en-\ntries, Project Gutenberg 139 , and all text posted on the web. The probability of words can\nbe calculated from the relative word frequency of a given word in the training dataset. For\nexample, the estimate ˆP(deep) can be calculated as the probability of any sentence starting\nwith the word “deep”. A slightly less accurate approach would be to count all occurrences of\nthe word “deep” and divide it by the total number of words in the corpus. This works fairly\nwell, particularly for frequent words. Moving on, we could attempt to estimate\nˆP(learning | deep) = n(deep, learning)\nn(deep)\n,\n(9.3.5)\nwhere n(x) and n(x, x′) are the number of occurrences of singletons and consecutive word\npairs, respectively. Unfortunately, estimating the probability of a word pair is somewhat more\ndiﬃcult, since the occurrences of “deep learning” are a lot less frequent. In particular, for\nsome unusual word combinations it may be tricky to ﬁnd enough occurrences to get accurate\n\n354\nRecurrent Neural Networks\nestimates. As suggested by the empirical results in Section 9.2.5, things take a turn for the\nworse for three-word combinations and beyond. There will be many plausible three-word\ncombinations that we likely will not see in our dataset. Unless we provide some solution to\nassign such word combinations a nonzero count, we will not be able to use them in a language\nmodel. If the dataset is small or if the words are very rare, we might not ﬁnd even a single\none of them.\nLaplace Smoothing\nA common strategy is to perform some form of Laplace smoothing. The solution is to add a\nsmall constant to all counts. Denote by n the total number of words in the training set and m\nthe number of unique words. This solution helps with singletons, e.g., via\nˆP(x) = n(x) + ϵ1/m\nn + ϵ1\n,\nˆP(x′ | x) = n(x, x′) + ϵ2 ˆP(x′)\nn(x) + ϵ2\n,\nˆP(x′′ | x, x′) = n(x, x′, x′′) + ϵ3 ˆP(x′′)\nn(x, x′) + ϵ3\n.\n(9.3.6)\nHere ϵ1, ϵ2, and ϵ3 are hyperparameters. Take ϵ1 as an example: when ϵ1 = 0, no smoothing\nis applied; when ϵ1 approaches positive inﬁnity, ˆP(x) approaches the uniform probability\n1/m. The above is a rather primitive variant of what other techniques can accomplish (Wood\net al., 2011).\nUnfortunately, models like this get unwieldy rather quickly for the following reasons. First,\nas discussed in Section 9.2.5, many n-grams occur very rarely, making Laplace smoothing\nrather unsuitable for language modeling. Second, we need to store all counts. Third, this\nentirely ignores the meaning of the words. For instance, “cat” and “feline” should occur in\nrelated contexts. It is quite diﬃcult to adjust such models to additional contexts, whereas,\ndeep learning based language models are well suited to take this into account. Last, long word\nsequences are almost certain to be novel, hence a model that simply counts the frequency of\npreviously seen word sequences is bound to perform poorly there. Therefore, we focus on\nusing neural networks for language modeling in the rest of the chapter.\n9.3.2 Perplexity\nNext, let’s discuss about how to measure the quality of the language model, which we will then\nuse to evaluate our models in the subsequent sections. One way is to check how surprising\nthe text is. A good language model is able to predict, with high accuracy, the tokens that\ncome next. Consider the following continuations of the phrase “It is raining”, as proposed by\ndiﬀerent language models:\n1. “It is raining outside”\n2. “It is raining banana tree”\n\n355\nLanguage Models\n3. “It is raining piouw;kcj pwepoiut”\nIn terms of quality, Example 1 is clearly the best. The words are sensible and logically co-\nherent. While it might not quite accurately reﬂect which word follows semantically (“in San\nFrancisco” and “in winter” would have been perfectly reasonable extensions), the model is\nable to capture which kind of word follows. Example 2 is considerably worse by producing\na nonsensical extension. Nonetheless, at least the model has learned how to spell words and\nsome degree of correlation between words. Last, Example 3 indicates a poorly trained model\nthat does not ﬁt data properly.\nWe might measure the quality of the model by computing the likelihood of the sequence.\nUnfortunately this is a number that is hard to understand and diﬃcult to compare. After all,\nshorter sequences are much more likely to occur than the longer ones, hence evaluating the\nmodel on Tolstoy’s magnum opus War and Peace will inevitably produce a much smaller\nlikelihood than, say, on Saint-Exupery’s novella The Little Prince. What is missing is the\nequivalent of an average.\nInformation theory comes handy here. We deﬁned entropy, surprisal, and cross-entropy when\nwe introduced the softmax regression (Section 4.1.3). If we want to compress text, we can\nask about predicting the next token given the current set of tokens. A better language model\nshould allow us to predict the next token more accurately. Thus, it should allow us to spend\nfewer bits in compressing the sequence. So we can measure it by the cross-entropy loss aver-\naged over all the n tokens of a sequence:\n1\nn\nn\n∑\nt=1\n−log P(xt | xt−1, . . ., x1),\n(9.3.7)\nwhere P is given by a language model and xt is the actual token observed at time step t from\nthe sequence. This makes the performance on documents of diﬀerent lengths comparable.\nFor historical reasons, scientists in natural language processing prefer to use a quantity called\nperplexity. In a nutshell, it is the exponential of (9.3.7):\nexp\n(\n−1\nn\nn\n∑\nt=1\nlog P(xt | xt−1, . . ., x1)\n)\n.\n(9.3.8)\nPerplexity can be best understood as the reciprocal of the geometric mean of the number of\nreal choices that we have when deciding which token to pick next. Let’s look at a number of\ncases:\n• In the best case scenario, the model always perfectly estimates the probability of the target\ntoken as 1. In this case the perplexity of the model is 1.\n• In the worst case scenario, the model always predicts the probability of the target token as\n0. In this situation, the perplexity is positive inﬁnity.\n• At the baseline, the model predicts a uniform distribution over all the available tokens of\nthe vocabulary. In this case, the perplexity equals the number of unique tokens of the\nvocabulary. In fact, if we were to store the sequence without any compression, this would\nbe the best we could do for encoding it. Hence, this provides a nontrivial upper bound\nthat any useful model must beat.\n\n356\nRecurrent Neural Networks\n9.3.3 Partitioning Sequences\nWe will design language models using neural networks and use perplexity to evaluate how\ngood the model is at predicting the next token given the current set of tokens in text sequences.\nBefore introducing the model, let’s assume that it processes a minibatch of sequences with\npredeﬁned length at a time. Now the question is how to read minibatches of input sequences\nand target sequences at random.\nSuppose that the dataset takes the form of a sequence of T token indices in corpus. We\nwill partition it into subsequences, where each subsequence has n tokens (time steps). To\niterate over (almost) all the tokens of the entire dataset for each epoch and obtain all possible\nlength-n subsequences, we can introduce randomness. More concretely, at the beginning of\neach epoch, discard the ﬁrst d tokens, where d ∈[0, n) is uniformly sampled at random.\nThe rest of the sequence is then partitioned into m = ⌊(T −d)/n⌋subsequences. Denote by\nxt = [xt, . . ., xt+n−1] the length-n subsequence starting from token xt at time step t. The\nresulting m partitioned subsequences are xd, xd+n, . . ., xd+n(m−1). Each subsequence will\nbe used as an input sequence into the language model.\nFor language modeling, the goal is to predict the next token based on the tokens we have seen\nso far; hence the targets (labels) are the original sequence, shifted by one token. The target\nsequence for any input sequence xt is xt+1 with length n.\nt\nFig. 9.3.1\nObtaining ﬁve pairs of input sequences and target sequences from partitioned length-5\nsubsequences.\nFig. 9.3.1 shows an example of obtaining ﬁve pairs of input sequences and target sequences\nwith n = 5 and d = 2.\n@d2l.add_to_class(d2l.TimeMachine)\n#@save\ndef __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\nsuper(d2l.TimeMachine, self).__init__()\nself.save_hyperparameters()\ncorpus, self.vocab = self.build(self._download())\narray = torch.tensor([corpus[i:i+num_steps+1]\nfor i in range(len(corpus)-num_steps)])\nself.X, self.Y = array[:,:-1], array[:,1:]\nTo train language models, we will randomly sample pairs of input sequences and target se-\nquences in minibatches. The following data loader randomly generates a minibatch from the\ndataset each time. The argument batch_size speciﬁes the number of subsequence examples\nin each minibatch and num_steps is the subsequence length in tokens.\n@d2l.add_to_class(d2l.TimeMachine)\n#@save\ndef get_dataloader(self, train):\n(continues on next page)\n\n357\nLanguage Models\n(continued from previous page)\nidx = slice(0, self.num_train) if train else slice(\nself.num_train, self.num_train + self.num_val)\nreturn self.get_tensorloader([self.X, self.Y], train, idx)\nAs we can see in the following, a minibatch of target sequences can be obtained by shifting\nthe input sequences by one token.\ndata = d2l.TimeMachine(batch_size=2, num_steps=10)\nfor X, Y in data.train_dataloader():\nprint('X:', X, '\\nY:', Y)\nbreak\nX: tensor([[10, 15, 12,\n0, 26, 16, 22,\n0, 24, 16],\n[ 9,\n0,\n2,\n0, 22, 15, 10,\n7, 16, 19]])\nY: tensor([[15, 12,\n0, 26, 16, 22,\n0, 24, 16, 22],\n[ 0,\n2,\n0, 22, 15, 10,\n7, 16, 19, 14]])\n9.3.4 Summary and Discussion\nLanguage models estimate the joint probability of a text sequence. For long sequences, n-\ngrams provide a convenient model by truncating the dependence. However, there is a lot of\nstructure but not enough frequency to deal eﬃciently with infrequent word combinations via\nLaplace smoothing. Thus, we will focus on neural language modeling in subsequent sections.\nTo train language models, we can randomly sample pairs of input sequences and target se-\nquences in minibatches. After training, we will use perplexity to measure the language model\nquality.\nLanguage models can be scaled up with increased data size, model size, and amount in train-\ning compute. Large language models can perform desired tasks by predicting output text given\ninput text instructions. As we will discuss later (e.g., Section 11.9), at the present moment\nlarge language models form the basis of state-of-the-art systems across diverse tasks.\n9.3.5 Exercises\n1. Suppose there are 100,000 words in the training dataset. How much word frequency and\nmulti-word adjacent frequency does a four-gram need to store?\n2. How would you model a dialogue?\n3. What other methods can you think of for reading long sequence data?\n4. Consider our method for discarding a uniformly random number of the ﬁrst few tokens at\nthe beginning of each epoch.\n1. Does it really lead to a perfectly uniform distribution over the sequences on the docu-\nment?\n\n358\nRecurrent Neural Networks\n140\n2. What would you have to do to make things even more uniform?\n5. If we want a sequence example to be a complete sentence, what kind of problem does this\nintroduce in minibatch sampling? How can we ﬁx it?\nDiscussions140.\n9.4 Recurrent Neural Networks\nIn Section 9.3 we described Markov models and n-grams for language modeling, where the\nconditional probability of token xt at time step t only depends on the n −1 previous tokens.\nIf we want to incorporate the possible eﬀect of tokens earlier than time step t −(n −1)\non xt, we need to increase n. However, the number of model parameters would also increase\nexponentially with it, as we need to store |V|n numbers for a vocabulary set V. Hence, rather\nthan modeling P(xt | xt−1, . . ., xt−n+1) it is preferable to use a latent variable model,\nP(xt | xt−1, . . ., x1) ≈P(xt | ht−1),\n(9.4.1)\nwhere ht−1 is a hidden state that stores the sequence information up to time step t −1. In\ngeneral, the hidden state at any time step t could be computed based on both the current\ninput xt and the previous hidden state ht−1:\nht = f (xt, ht−1).\n(9.4.2)\nFor a suﬃciently powerful function f in (9.4.2), the latent variable model is not an approx-\nimation. After all, ht may simply store all the data it has observed so far. However, it could\npotentially make both computation and storage expensive.\nRecall that we have discussed hidden layers with hidden units in Chapter 5. It is noteworthy\nthat hidden layers and hidden states refer to two very diﬀerent concepts. Hidden layers are, as\nexplained, layers that are hidden from view on the path from input to output. Hidden states are\ntechnically speaking inputs to whatever we do at a given step, and they can only be computed\nby looking at data at previous time steps.\nRecurrent neural networks (RNNs) are neural networks with hidden states. Before introducing\nthe RNN model, we ﬁrst revisit the MLP model introduced in Section 5.1.\nimport torch\nfrom d2l import torch as d2l\n9.4.1 Neural Networks without Hidden States\nLet’s take a look at an MLP with a single hidden layer. Let the hidden layer’s activation\nfunction be ϕ. Given a minibatch of examples X ∈Rn×d with batch size n and d inputs, the\n\n359\nRecurrent Neural Networks\nhidden layer output H ∈Rn×h is calculated as\nH = ϕ(XWxh + bh).\n(9.4.3)\nIn (9.4.3), we have the weight parameter Wxh ∈Rd×h, the bias parameter bh ∈R1×h, and\nthe number of hidden units h, for the hidden layer. So armed, we apply broadcasting (see\nSection 2.1.4) during the summation. Next, the hidden layer output H is used as input of the\noutput layer, which is given by\nO = HWhq + bq,\n(9.4.4)\nwhere O ∈Rn×q is the output variable, Whq ∈Rh×q is the weight parameter, and bq ∈\nR1×q is the bias parameter of the output layer. If it is a classiﬁcation problem, we can use\nsoftmax(O) to compute the probability distribution of the output categories.\nThis is entirely analogous to the regression problem we solved previously in Section 9.1,\nhence we omit details. Suﬃce it to say that we can pick feature-label pairs at random and\nlearn the parameters of our network via automatic diﬀerentiation and stochastic gradient\ndescent.\n9.4.2 Recurrent Neural Networks with Hidden States\nMatters are entirely diﬀerent when we have hidden states. Let’s look at the structure in some\nmore detail.\nAssume that we have a minibatch of inputs Xt ∈Rn×d at time step t. In other words, for\na minibatch of n sequence examples, each row of Xt corresponds to one example at time\nstep t from the sequence. Next, denote by Ht ∈Rn×h the hidden layer output of time step\nt. Unlike with MLP, here we save the hidden layer output Ht−1 from the previous time step\nand introduce a new weight parameter Whh ∈Rh×h to describe how to use the hidden layer\noutput of the previous time step in the current time step. Speciﬁcally, the calculation of the\nhidden layer output of the current time step is determined by the input of the current time\nstep together with the hidden layer output of the previous time step:\nHt = ϕ(XtWxh + Ht−1Whh + bh).\n(9.4.5)\nCompared with (9.4.3), (9.4.5) adds one more term Ht−1Whh and thus instantiates (9.4.2).\nFrom the relationship between hidden layer outputs Ht and Ht−1 of adjacent time steps, we\nknow that these variables captured and retained the sequence’s historical information up to\ntheir current time step, just like the state or memory of the neural network’s current time\nstep. Therefore, such a hidden layer output is called a hidden state. Since the hidden state\nuses the same deﬁnition of the previous time step in the current time step, the computation of\n(9.4.5) is recurrent. Hence, as we said, neural networks with hidden states based on recurrent\ncomputation are named recurrent neural networks. Layers that perform the computation of\n(9.4.5) in RNNs are called recurrent layers.\nThere are many diﬀerent ways for constructing RNNs. Those with a hidden state deﬁned by\n\n360\nRecurrent Neural Networks\n(9.4.5) are very common. For time step t, the output of the output layer is similar to the\ncomputation in the MLP:\nOt = HtWhq + bq.\n(9.4.6)\nParameters of the RNN include the weights Wxh ∈Rd×h, Whh ∈Rh×h, and the bias bh ∈\nR1×h of the hidden layer, together with the weights Whq ∈Rh×q and the bias bq ∈R1×q of\nthe output layer. It is worth mentioning that even at diﬀerent time steps, RNNs always use\nthese model parameters. Therefore, the parametrization cost of an RNN does not grow as the\nnumber of time steps increases.\nFig. 9.4.1 illustrates the computational logic of an RNN at three adjacent time steps. At any\ntime step t, the computation of the hidden state can be treated as: (i) concatenating the input\nXt at the current time step t and the hidden state Ht−1 at the previous time step t −1; (ii)\nfeeding the concatenation result into a fully connected layer with the activation function ϕ.\nThe output of such a fully connected layer is the hidden state Ht of the current time step t. In\nthis case, the model parameters are the concatenation of Wxh and Whh, and a bias of bh, all\nfrom (9.4.5). The hidden state of the current time step t, Ht, will participate in computing\nthe hidden state Ht+1 of the next time step t + 1. What is more, Ht will also be fed into the\nfully connected output layer to compute the output Ot of the current time step t.\nt\nFig. 9.4.1\nAn RNN with a hidden state.\nWe just mentioned that the calculation of XtWxh + Ht−1Whh for the hidden state is equiv-\nalent to matrix multiplication of the concatenation of Xt and Ht−1 and the concatenation\nof Wxh and Whh. Though this can be proven mathematically, in the following we just use a\nsimple code snippet as a demonstration. To begin with, we deﬁne matrices X, W_xh, H, and\nW_hh, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying X by W_xh,\nand H by W_hh, and then adding these two products, we obtain a matrix of shape (3, 4).\nX, W_xh = torch.randn(3, 1), torch.randn(1, 4)\nH, W_hh = torch.randn(3, 4), torch.randn(4, 4)\ntorch.matmul(X, W_xh) + torch.matmul(H, W_hh)\n\n361\nRecurrent Neural Networks\ntensor([[-1.5596, -1.2137,\n2.8331, -3.1458],\n[-2.2858,\n7.0869,\n0.7021, -3.2064],\n[ 0.4085, -2.1803, -1.2999,\n1.1916]])\nNow we concatenate the matrices X and H along columns (axis 1), and the matrices W_xh and\nW_hh along rows (axis 0). These two concatenations result in matrices of shape (3, 5) and of\nshape (5, 4), respectively. Multiplying these two concatenated matrices, we obtain the same\noutput matrix of shape (3, 4) as above.\ntorch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))\ntensor([[-1.5596, -1.2137,\n2.8331, -3.1459],\n[-2.2858,\n7.0869,\n0.7021, -3.2064],\n[ 0.4085, -2.1803, -1.2999,\n1.1916]])\n9.4.3 RNN-Based Character-Level Language Models\nRecall that for language modeling in Section 9.3, we aim to predict the next token based on\nthe current and past tokens; thus we shift the original sequence by one token as the targets (la-\nbels). Bengio et al. (2003) ﬁrst proposed to use a neural network for language modeling. In the\nfollowing we illustrate how RNNs can be used to build a language model. Let the minibatch\nsize be one, and the sequence of the text be “machine”. To simplify training in subsequent\nsections, we tokenize text into characters rather than words and consider a character-level\nlanguage model. Fig. 9.4.2 demonstrates how to predict the next character based on the cur-\nrent and previous characters via an RNN for character-level language modeling.\nt\nFig. 9.4.2\nA character-level language model based on the RNN. The input and target sequences are\n“machin” and “achine”, respectively.\nDuring the training process, we run a softmax operation on the output from the output layer\nfor each time step, and then use the cross-entropy loss to compute the error between the\nmodel output and the target. Because of the recurrent computation of the hidden state in the\nhidden layer, the output, O3, of time step 3 in Fig. 9.4.2 is determined by the text sequence\n“m”, “a”, and “c”. Since the next character of the sequence in the training data is “h”, the\n\n362\nRecurrent Neural Networks\n141\nloss of time step 3 will depend on the probability distribution of the next character generated\nbased on the feature sequence “m”, “a”, “c” and the target “h” of this time step.\nIn practice, each token is represented by a d-dimensional vector, and we use a batch size\nn > 1. Therefore, the input Xt at time step t will be an n × d matrix, which is identical to\nwhat we discussed in Section 9.4.2.\nIn the following sections, we will implement RNNs for character-level language models.\n9.4.4 Summary\nA neural network that uses recurrent computation for hidden states is called a recurrent neu-\nral network (RNN). The hidden state of an RNN can capture historical information of the\nsequence up to the current time step. With recurrent computation, the number of RNN model\nparameters does not grow as the number of time steps increases. As for applications, an RNN\ncan be used to create character-level language models.\n9.4.5 Exercises\n1. If we use an RNN to predict the next character in a text sequence, what is the required\ndimension for any output?\n2. Why can RNNs express the conditional probability of a token at some time step based on\nall the previous tokens in the text sequence?\n3. What happens to the gradient if you backpropagate through a long sequence?\n4. What are some of the problems associated with the language model described in this\nsection?\nDiscussions141.\n9.5 Recurrent Neural Network Implementation from\nScratch\nWe are now ready to implement an RNN from scratch. In particular, we will train this RNN\nto function as a character-level language model (see Section 9.4) and train it on a corpus\nconsisting of the entire text of H. G. Wells’ The Time Machine, following the data processing\nsteps outlined in Section 9.2. We start by loading the dataset.\n\n363\nRecurrent Neural Network Implementation from Scratch\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n9.5.1 RNN Model\nWe begin by deﬁning a class to implement the RNN model (Section 9.4.2). Note that the\nnumber of hidden units num_hiddens is a tunable hyperparameter.\nclass RNNScratch(d2l.Module):\n#@save\n\"\"\"The RNN model implemented from scratch.\"\"\"\ndef __init__(self, num_inputs, num_hiddens, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.W_xh = nn.Parameter(\ntorch.randn(num_inputs, num_hiddens) * sigma)\nself.W_hh = nn.Parameter(\ntorch.randn(num_hiddens, num_hiddens) * sigma)\nself.b_h = nn.Parameter(torch.zeros(num_hiddens))\nThe forward method below deﬁnes how to compute the output and hidden state at any time\nstep, given the current input and the state of the model at the previous time step. Note that the\nRNN model loops through the outermost dimension of inputs, updating the hidden state one\ntime step at a time. The model here uses a tanh activation function (Section 5.1.2).\n@d2l.add_to_class(RNNScratch)\n#@save\ndef forward(self, inputs, state=None):\nif state is None:\n# Initial state with shape: (batch_size, num_hiddens)\nstate = torch.zeros((inputs.shape[1], self.num_hiddens),\ndevice=inputs.device)\nelse:\nstate, = state\noutputs = []\nfor X in inputs:\n# Shape of inputs: (num_steps, batch_size, num_inputs)\nstate = torch.tanh(torch.matmul(X, self.W_xh) +\ntorch.matmul(state, self.W_hh) + self.b_h)\noutputs.append(state)\nreturn outputs, state\nWe can feed a minibatch of input sequences into an RNN model as follows.\nbatch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\nrnn = RNNScratch(num_inputs, num_hiddens)\nX = torch.ones((num_steps, batch_size, num_inputs))\noutputs, state = rnn(X)\n\n364\nRecurrent Neural Networks\nLet’s check whether the RNN model produces results of the correct shapes to ensure that the\ndimensionality of the hidden state remains unchanged.\ndef check_len(a, n):\n#@save\n\"\"\"Check the length of a list.\"\"\"\nassert len(a) == n, f'list\\'s length {len(a)} != expected length {n}'\ndef check_shape(a, shape):\n#@save\n\"\"\"Check the shape of a tensor.\"\"\"\nassert a.shape == shape, \\\nf'tensor\\'s shape {a.shape} != expected shape {shape}'\ncheck_len(outputs, num_steps)\ncheck_shape(outputs[0], (batch_size, num_hiddens))\ncheck_shape(state, (batch_size, num_hiddens))\n9.5.2 RNN-Based Language Model\nThe following RNNLMScratch class deﬁnes an RNN-based language model, where we pass\nin our RNN via the rnn argument of the __init__ method. When training language models,\nthe inputs and outputs are from the same vocabulary. Hence, they have the same dimension,\nwhich is equal to the vocabulary size. Note that we use perplexity to evaluate the model.\nAs discussed in Section 9.3.2, this ensures that sequences of diﬀerent length are compara-\nble.\nclass RNNLMScratch(d2l.Classifier):\n#@save\n\"\"\"The RNN-based language model implemented from scratch.\"\"\"\ndef __init__(self, rnn, vocab_size, lr=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.init_params()\ndef init_params(self):\nself.W_hq = nn.Parameter(\ntorch.randn(\nself.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\nself.b_q = nn.Parameter(torch.zeros(self.vocab_size))\ndef training_step(self, batch):\nl = self.loss(self(*batch[:-1]), batch[-1])\nself.plot('ppl', torch.exp(l), train=True)\nreturn l\ndef validation_step(self, batch):\nl = self.loss(self(*batch[:-1]), batch[-1])\nself.plot('ppl', torch.exp(l), train=False)\n\n365\nRecurrent Neural Network Implementation from Scratch\nOne-Hot Encoding\nRecall that each token is represented by a numerical index indicating the position in the\nvocabulary of the corresponding word/character/word piece. You might be tempted to build\na neural network with a single input node (at each time step), where the index could be fed\nin as a scalar value. This works when we are dealing with numerical inputs like price or\ntemperature, where any two values suﬃciently close together should be treated similarly. But\nthis does not quite make sense. The 45th and 46th words in our vocabulary happen to be\n“their” and “said”, whose meanings are not remotely similar.\nWhen dealing with such categorical data, the most common strategy is to represent each item\nby a one-hot encoding (recall from Section 4.1.1). A one-hot encoding is a vector whose length\nis given by the size of the vocabulary N, where all entries are set to 0, except for the entry\ncorresponding to our token, which is set to 1. For example, if the vocabulary had ﬁve elements,\nthen the one-hot vectors corresponding to indices 0 and 2 would be the following.\nF.one_hot(torch.tensor([0, 2]), 5)\ntensor([[1, 0, 0, 0, 0],\n[0, 0, 1, 0, 0]])\nThe minibatches that we sample at each iteration will take the shape (batch size, number of\ntime steps). Once representing each input as a one-hot vector, we can think of each minibatch\nas a three-dimensional tensor, where the length along the third axis is given by the vocabulary\nsize (len(vocab)). We often transpose the input so that we will obtain an output of shape\n(number of time steps, batch size, vocabulary size). This will allow us to loop more conve-\nniently through the outermost dimension for updating hidden states of a minibatch, time step\nby time step (e.g., in the above forward method).\n@d2l.add_to_class(RNNLMScratch)\n#@save\ndef one_hot(self, X):\n# Output shape: (num_steps, batch_size, vocab_size)\nreturn F.one_hot(X.T, self.vocab_size).type(torch.float32)\nTransforming RNN Outputs\nThe language model uses a fully connected output layer to transform RNN outputs into token\npredictions at each time step.\n@d2l.add_to_class(RNNLMScratch)\n#@save\ndef output_layer(self, rnn_outputs):\noutputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\nreturn torch.stack(outputs, 1)\n@d2l.add_to_class(RNNLMScratch)\n#@save\n(continues on next page)\n\n366\nRecurrent Neural Networks\n(continued from previous page)\ndef forward(self, X, state=None):\nembs = self.one_hot(X)\nrnn_outputs, _ = self.rnn(embs, state)\nreturn self.output_layer(rnn_outputs)\nLet’s check whether the forward computation produces outputs with the correct shape.\nmodel = RNNLMScratch(rnn, num_inputs)\noutputs = model(torch.ones((batch_size, num_steps), dtype=torch.int64))\ncheck_shape(outputs, (batch_size, num_steps, num_inputs))\n9.5.3 Gradient Clipping\nWhile you are already used to thinking of neural networks as “deep” in the sense that many\nlayers separate the input and output even within a single time step, the length of the sequence\nintroduces a new notion of depth. In addition to the passing through the network in the input-\nto-output direction, inputs at the ﬁrst time step must pass through a chain of T layers along\nthe time steps in order to inﬂuence the output of the model at the ﬁnal time step. Taking\nthe backwards view, in each iteration, we backpropagate gradients through time, resulting\nin a chain of matrix-products of length O(T). As mentioned in Section 5.4, this can result\nin numerical instability, causing the gradients either to explode or vanish, depending on the\nproperties of the weight matrices.\nDealing with vanishing and exploding gradients is a fundamental problem when designing\nRNNs and has inspired some of the biggest advances in modern neural network architectures.\nIn the next chapter, we will talk about specialized architectures that were designed in hopes of\nmitigating the vanishing gradient problem. However, even modern RNNs often suﬀer from\nexploding gradients. One inelegant but ubiquitous solution is to simply clip the gradients\nforcing the resulting “clipped” gradients to take smaller values.\nGenerally speaking, when optimizing some objective by gradient descent, we iteratively up-\ndate the parameter of interest, say a vector x, but pushing it in the direction of the negative\ngradient g (in stochastic gradient descent, we calculate this gradient on a randomly sampled\nminibatch). For example, with learning rate η > 0, each update takes the form x ←x −ηg.\nLet’s further assume that the objective function f is suﬃciently smooth. Formally, we say\nthat the objective is Lipschitz continuous with constant L, meaning that for any x and y, we\nhave\n| f (x) −f (y)| ≤L∥x −y∥.\n(9.5.1)\nAs you can see, when we update the parameter vector by subtracting ηg, the change in the\nvalue of the objective depends on the learning rate, the norm of the gradient and L as fol-\nlows:\n| f (x) −f (x −ηg)| ≤Lη∥g∥.\n(9.5.2)\n\n367\nRecurrent Neural Network Implementation from Scratch\nIn other words, the objective cannot change by more than Lη∥g∥. Having a small value for\nthis upper bound might be viewed as good or bad. On the downside, we are limiting the speed\nat which we can reduce the value of the objective. On the bright side, this limits by just how\nmuch we can go wrong in any one gradient step.\nWhen we say that gradients explode, we mean that ∥g∥becomes excessively large. In this\nworst case, we might do so much damage in a single gradient step that we could undo all of\nthe progress made over the course of thousands of training iterations. When gradients can be\nso large, neural network training often diverges, failing to reduce the value of the objective.\nAt other times, training eventually converges but is unstable owing to massive spikes in the\nloss.\nOne way to limit the size of Lη∥g∥is to shrink the learning rate η to tiny values. This has\nthe advantage that we do not bias the updates. But what if we only rarely get large gradients?\nThis drastic move slows down our progress at all steps, just to deal with the rare exploding\ngradient events. A popular alternative is to adopt a gradient clipping heuristic projecting the\ngradients g onto a ball of some given radius θ as follows:\ng ←min\n(\n1,\nθ\n∥g∥\n)\ng.\n(9.5.3)\nThis ensures that the gradient norm never exceeds θ and that the updated gradient is entirely\naligned with the original direction of g. It also has the desirable side-eﬀect of limiting the\ninﬂuence any given minibatch (and within it any given sample) can exert on the parameter\nvector. This bestows a certain degree of robustness to the model. To be clear, it is a hack.\nGradient clipping means that we are not always following the true gradient and it is hard to\nreason analytically about the possible side eﬀects. However, it is a very useful hack, and is\nwidely adopted in RNN implementations in most deep learning frameworks.\nBelow we deﬁne a method to clip gradients, which is invoked by the fit_epoch method of the\nd2l.Trainer class (see Section 3.4). Note that when computing the gradient norm, we are\nconcatenating all model parameters, treating them as a single giant parameter vector.\n@d2l.add_to_class(d2l.Trainer)\n#@save\ndef clip_gradients(self, grad_clip_val, model):\nparams = [p for p in model.parameters() if p.requires_grad]\nnorm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\nif norm > grad_clip_val:\nfor param in params:\nparam.grad[:] *= grad_clip_val / norm\n9.5.4 Training\nUsing The Time Machine dataset (data), we train a character-level language model (model)\nbased on the RNN (rnn) implemented from scratch. Note that we ﬁrst calculate the gradients,\nthen clip them, and ﬁnally update the model parameters using the clipped gradients.\n\n368\nRecurrent Neural Networks\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nrnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\ntrainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\n9.5.5 Decoding\nOnce a language model has been learned, we can use it not only to predict the next token but\nto continue predicting each subsequent one, treating the previously predicted token as though\nit were the next in the input. Sometimes we will just want to generate text as though we were\nstarting at the beginning of a document. However, it is often useful to condition the language\nmodel on a user-supplied preﬁx. For example, if we were developing an autocomplete feature\nfor a search engine or to assist users in writing emails, we would want to feed in what they\nhad written so far (the preﬁx), and then generate a likely continuation.\nThe following predict method generates a continuation, one character at a time, after in-\ngesting a user-provided prefix. When looping through the characters in prefix, we keep\npassing the hidden state to the next time step but do not generate any output. This is called\nthe warm-up period. After ingesting the preﬁx, we are now ready to begin emitting the sub-\nsequent characters, each of which will be fed back into the model as the input at the next\ntime step.\n@d2l.add_to_class(RNNLMScratch)\n#@save\ndef predict(self, prefix, num_preds, vocab, device=None):\nstate, outputs = None, [vocab[prefix[0]]]\nfor i in range(len(prefix) + num_preds - 1):\nX = torch.tensor([[outputs[-1]]], device=device)\nembs = self.one_hot(X)\nrnn_outputs, state = self.rnn(embs, state)\nif i < len(prefix) - 1:\n# Warm-up period\noutputs.append(vocab[prefix[i + 1]])\nelse:\n# Predict num_preds steps\nY = self.output_layer(rnn_outputs)\n(continues on next page)\n\n369\nRecurrent Neural Network Implementation from Scratch\n(continued from previous page)\noutputs.append(int(Y.argmax(axis=2).reshape(1)))\nreturn ''.join([vocab.idx_to_token[i] for i in outputs])\nIn the following, we specify the preﬁx and have it generate 20 additional characters.\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n'it has is a mere and the t'\nWhile implementing the above RNN model from scratch is instructive, it is not convenient.\nIn the next section, we will see how to leverage deep learning frameworks to whip up RNNs\nusing standard architectures, and to reap performance gains by relying on highly optimized\nlibrary functions.\n9.5.6 Summary\nWe can train RNN-based language models to generate text following the user-provided text\npreﬁx. A simple RNN language model consists of input encoding, RNN modeling, and output\ngeneration. During training, gradient clipping can mitigate the problem of exploding gradients\nbut does not address the problem of vanishing gradients. In the experiment, we implemented\na simple RNN language model and trained it with gradient clipping on sequences of text,\ntokenized at the character level. By conditioning on a preﬁx, we can use a language model to\ngenerate likely continuations, which proves useful in many applications, e.g., autocomplete\nfeatures.\n9.5.7 Exercises\n1. Does the implemented language model predict the next token based on all the past tokens\nup to the very ﬁrst token in The Time Machine?\n2. Which hyperparameter controls the length of history used for prediction?\n3. Show that one-hot encoding is equivalent to picking a diﬀerent embedding for each object.\n4. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of\ntime steps in a minibatch, and learning rate) to improve the perplexity. How low can you\ngo while sticking with this simple architecture?\n5. Replace one-hot encoding with learnable embeddings. Does this lead to better perfor-\nmance?\n6. Conduct an experiment to determine how well this language model trained on The Time\nMachine works on other books by H. G. Wells, e.g., The War of the Worlds.\n\n370\nRecurrent Neural Networks\n142\n7. Conduct another experiment to evaluate the perplexity of this model on books written by\nother authors.\n8. Modify the prediction method so as to use sampling rather than picking the most likely\nnext character.\n• What happens?\n• Bias the model towards more likely outputs, e.g., by sampling from q(xt | xt−1, . . ., x1) ∝\nP(xt | xt−1, . . ., x1)α for α > 1.\n9. Run the code in this section without clipping the gradient. What happens?\n10. Replace the activation function used in this section with ReLU and repeat the experiments\nin this section. Do we still need gradient clipping? Why?\nDiscussions142.\n9.6 Concise Implementation of Recurrent Neural\nNetworks\nLike most of our from-scratch implementations, Section 9.5 was designed to provide insight\ninto how each component works. But when you are using RNNs every day or writing pro-\nduction code, you will want to rely more on libraries that cut down on both implementation\ntime (by supplying library code for common models and functions) and computation time\n(by optimizing the heck out of these library implementations). This section will show you\nhow to implement the same language model more eﬃciently using the high-level API pro-\nvided by your deep learning framework. We begin, as before, by loading The Time Machine\ndataset.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n9.6.1 Deﬁning the Model\nWe deﬁne the following class using the RNN implemented by high-level APIs.\nclass RNN(d2l.Module):\n#@save\n\"\"\"The RNN model implemented with high-level APIs.\"\"\"\ndef __init__(self, num_inputs, num_hiddens):\nsuper().__init__()\n(continues on next page)\n\n371\nConcise Implementation of Recurrent Neural Networks\n(continued from previous page)\nself.save_hyperparameters()\nself.rnn = nn.RNN(num_inputs, num_hiddens)\ndef forward(self, inputs, H=None):\nreturn self.rnn(inputs, H)\nInheriting from the RNNLMScratch class in Section 9.5, the following RNNLM class deﬁnes a\ncomplete RNN-based language model. Note that we need to create a separate fully connected\noutput layer.\nclass RNNLM(d2l.RNNLMScratch):\n#@save\n\"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\ndef init_params(self):\nself.linear = nn.LazyLinear(self.vocab_size)\ndef output_layer(self, hiddens):\nreturn self.linear(hiddens).swapaxes(0, 1)\n9.6.2 Training and Predicting\nBefore training the model, let’s make a prediction with a model initialized with random\nweights. Given that we have not trained the network, it will generate nonsensical predic-\ntions.\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nrnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\nmodel.predict('it has', 20, data.vocab)\n'it hassuussuussuussuussuus'\nNext, we train our model, leveraging the high-level API.\ntrainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\nCompared with Section 9.5, this model achieves comparable perplexity, but runs faster due\nto the optimized implementations. As before, we can generate predicted tokens following the\nspeciﬁed preﬁx string.\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n'it has a mather a mather a'\n\n372\nRecurrent Neural Networks\n143\n9.6.3 Summary\nHigh-level APIs in deep learning frameworks provide implementations of standard RNNs.\nThese libraries help you to avoid wasting time reimplementing standard models. Moreover,\nframework implementations are often highly optimized, leading to signiﬁcant (computational)\nperformance gains when compared with implementations from scratch.\n9.6.4 Exercises\n1. Can you make the RNN model overﬁt using the high-level APIs?\n2. Implement the autoregressive model of Section 9.1 using an RNN.\nDiscussions143.\n9.7 Backpropagation Through Time\nIf you completed the exercises in Section 9.5, you would have seen that gradient clipping is\nvital for preventing the occasional massive gradients from destabilizing training. We hinted\nthat the exploding gradients stem from backpropagating across long sequences. Before intro-\nducing a slew of modern RNN architectures, let’s take a closer look at how backpropagation\nworks in sequence models in mathematical detail. Hopefully, this discussion will bring some\nprecision to the notion of vanishing and exploding gradients. If you recall our discussion of\nforward and backward propagation through computational graphs when we introduced MLPs\nin Section 5.3, then forward propagation in RNNs should be relatively straightforward. Ap-\nplying backpropagation in RNNs is called backpropagation through time (Werbos, 1990).\nThis procedure requires us to expand (or unroll) the computational graph of an RNN one\ntime step at a time. The unrolled RNN is essentially a feedforward neural network with the\n\n373\nBackpropagation Through Time\nspecial property that the same parameters are repeated throughout the unrolled network, ap-\npearing at each time step. Then, just as in any feedforward neural network, we can apply the\nchain rule, backpropagating gradients through the unrolled net. The gradient with respect to\neach parameter must be summed across all places that the parameter occurs in the unrolled\nnet. Handling such weight tying should be familiar from our chapters on convolutional neural\nnetworks.\nComplications arise because sequences can be rather long. It is not unusual to work with text\nsequences consisting of over a thousand tokens. Note that this poses problems both from a\ncomputational (too much memory) and optimization (numerical instability) standpoint. Input\nfrom the ﬁrst step passes through over 1000 matrix products before arriving at the output, and\nanother 1000 matrix products are required to compute the gradient. We now analyze what\ncan go wrong and how to address it in practice.\n9.7.1 Analysis of Gradients in RNNs\nWe start with a simpliﬁed model of how an RNN works. This model ignores details about\nthe speciﬁcs of the hidden state and how it is updated. The mathematical notation here does\nnot explicitly distinguish scalars, vectors, and matrices. We are just trying to develop some\nintuition. In this simpliﬁed model, we denote ht as the hidden state, xt as input, and ot as\noutput at time step t. Recall our discussions in Section 9.4.2 that the input and the hidden\nstate can be concatenated before being multiplied by one weight variable in the hidden layer.\nThus, we use wh and wo to indicate the weights of the hidden layer and the output layer,\nrespectively. As a result, the hidden states and outputs at each time step are\nht = f (xt, ht−1, wh),\not = g(ht, wo),\n(9.7.1)\nwhere f and g are transformations of the hidden layer and the output layer, respectively.\nHence, we have a chain of values {. . ., (xt−1, ht−1, ot−1), (xt, ht, ot), . . .} that depend on each\nother via recurrent computation. The forward propagation is fairly straightforward. All we\nneed is to loop through the (xt, ht, ot) triples one time step at a time. The discrepancy between\noutput ot and the desired target yt is then evaluated by an objective function across all the T\ntime steps as\nL(x1, . . ., xT, y1, . . ., yT, wh, wo) = 1\nT\nT\n∑\nt=1\nl(yt, ot).\n(9.7.2)\nFor backpropagation, matters are a bit trickier, especially when we compute the gradients\nwith regard to the parameters wh of the objective function L. To be speciﬁc, by the chain\nrule,\n∂L\n∂wh\n= 1\nT\nT\n∑\nt=1\n∂l(yt, ot)\n∂wh\n= 1\nT\nT\n∑\nt=1\n∂l(yt, ot)\n∂ot\n∂g(ht, wo)\n∂ht\n∂ht\n∂wh\n.\n(9.7.3)\n\n374\nRecurrent Neural Networks\nThe ﬁrst and the second factors of the product in (9.7.3) are easy to compute. The third\nfactor ∂ht/∂wh is where things get tricky, since we need to recurrently compute the eﬀect\nof the parameter wh on ht. According to the recurrent computation in (9.7.1), ht depends on\nboth ht−1 and wh, where computation of ht−1 also depends on wh. Thus, evaluating the total\nderivate of ht with respect to wh using the chain rule yields\n∂ht\n∂wh\n= ∂f (xt, ht−1, wh)\n∂wh\n+ ∂f (xt, ht−1, wh)\n∂ht−1\n∂ht−1\n∂wh\n.\n(9.7.4)\nTo derive the above gradient, assume that we have three sequences {at}, {bt}, {ct} satisfying\na0 = 0 and at = bt + ctat−1 for t = 1, 2, . . .. Then for t ≥1, it is easy to show\nat = bt +\nt−1\n∑\ni=1\n(\nt∏\nj=i+1\ncj\n)\nbi.\n(9.7.5)\nBy substituting at, bt, and ct according to\nat = ∂ht\n∂wh\n,\nbt = ∂f (xt, ht−1, wh)\n∂wh\n,\nct = ∂f (xt, ht−1, wh)\n∂ht−1\n,\n(9.7.6)\nthe gradient computation in (9.7.4) satisﬁes at = bt + ctat−1. Thus, per (9.7.5), we can\nremove the recurrent computation in (9.7.4) with\n∂ht\n∂wh\n= ∂f (xt, ht−1, wh)\n∂wh\n+\nt−1\n∑\ni=1\n(\nt∏\nj=i+1\n∂f (xj, hj−1, wh)\n∂hj−1\n)\n∂f (xi, hi−1, wh)\n∂wh\n.\n(9.7.7)\nWhile we can use the chain rule to compute ∂ht/∂wh recursively, this chain can get very long\nwhenever t is large. Let’s discuss a number of strategies for dealing with this problem.\nFull Computation\nOne idea might be to compute the full sum in (9.7.7). However, this is very slow and gradients\ncan blow up, since subtle changes in the initial conditions can potentially aﬀect the outcome\na lot. That is, we could see things similar to the butterﬂy eﬀect, where minimal changes\nin the initial conditions lead to disproportionate changes in the outcome. This is generally\nundesirable. After all, we are looking for robust estimators that generalize well. Hence this\nstrategy is almost never used in practice.\nTruncating Time Steps\nAlternatively, we can truncate the sum in (9.7.7) after τ steps. This is what we have been\ndiscussing so far. This leads to an approximation of the true gradient, simply by terminating\nthe sum at ∂ht−τ/∂wh. In practice this works quite well. It is what is commonly referred\n\n375\nBackpropagation Through Time\nto as truncated backpropgation through time (Jaeger, 2002). One of the consequences of\nthis is that the model focuses primarily on short-term inﬂuence rather than long-term con-\nsequences. This is actually desirable, since it biases the estimate towards simpler and more\nstable models.\nRandomized Truncation\nLast, we can replace ∂ht/∂wh by a random variable which is correct in expectation but trun-\ncates the sequence. This is achieved by using a sequence of ξt with predeﬁned 0 ≤πt ≤1,\nwhere P(ξt = 0) = 1 −πt and P(ξt = π−1\nt ) = πt, thus E[ξt] = 1. We use this to replace\nthe gradient ∂ht/∂wh in (9.7.4) with\nzt = ∂f (xt, ht−1, wh)\n∂wh\n+ ξt\n∂f (xt, ht−1, wh)\n∂ht−1\n∂ht−1\n∂wh\n.\n(9.7.8)\nIt follows from the deﬁnition of ξt that E[zt] = ∂ht/∂wh. Whenever ξt = 0 the recurrent\ncomputation terminates at that time step t. This leads to a weighted sum of sequences of\nvarying lengths, where long sequences are rare but appropriately overweighted. This idea was\nproposed by Tallec and Ollivier (2017).\nComparing Strategies\nt\nFig. 9.7.1\nComparing strategies for computing gradients in RNNs. From top to bottom: randomized\ntruncation, regular truncation, and full computation.\nFig. 9.7.1 illustrates the three strategies when analyzing the ﬁrst few characters of The Time\nMachine using backpropagation through time for RNNs:\n• The ﬁrst row is the randomized truncation that partitions the text into segments of varying\nlengths.\n• The second row is the regular truncation that breaks the text into subsequences of the same\nlength. This is what we have been doing in RNN experiments.\n• The third row is the full backpropagation through time that leads to a computationally\ninfeasible expression.\nUnfortunately, while appealing in theory, randomized truncation does not work much better\nthan regular truncation, most likely due to a number of factors. First, the eﬀect of an ob-\nservation after a number of backpropagation steps into the past is quite suﬃcient to capture\n\n376\nRecurrent Neural Networks\ndependencies in practice. Second, the increased variance counteracts the fact that the gradi-\nent is more accurate with more steps. Third, we actually want models that have only a short\nrange of interactions. Hence, regularly truncated backpropagation through time has a slight\nregularizing eﬀect that can be desirable.\n9.7.2 Backpropagation Through Time in Detail\nAfter discussing the general principle, let’s discuss backpropagation through time in detail.\nIn contrast to the analysis in Section 9.7.1, in the following we will show how to compute the\ngradients of the objective function with respect to all the decomposed model parameters. To\nkeep things simple, we consider an RNN without bias parameters, whose activation function\nin the hidden layer uses the identity mapping (ϕ(x) = x). For time step t, let the single\nexample input and the target be xt ∈Rd and yt, respectively. The hidden state ht ∈Rh and\nthe output ot ∈Rq are computed as\nht = Whxxt + Whhht−1,\not = Wqhht,\n(9.7.9)\nwhere Whx ∈Rh×d, Whh ∈Rh×h, and Wqh ∈Rq×h are the weight parameters. Denote by\nl(ot, yt) the loss at time step t. Our objective function, the loss over T time steps from the\nbeginning of the sequence is thus\nL = 1\nT\nT\n∑\nt=1\nl(ot, yt).\n(9.7.10)\nIn order to visualize the dependencies among model variables and parameters during compu-\ntation of the RNN, we can draw a computational graph for the model, as shown in Fig. 9.7.2.\nFor example, the computation of the hidden states of time step 3, h3, depends on the model\nparameters Whx and Whh, the hidden state of the previous time step h2, and the input of\nthe current time step x3.\nt\nFig. 9.7.2\nComputational graph showing dependencies for an RNN model with three time steps.\nBoxes represent variables (not shaded) or parameters (shaded) and circles represent\noperators.\nAs just mentioned, the model parameters in Fig. 9.7.2 are Whx, Whh, and Wqh. Gen-\nerally, training this model requires gradient computation with respect to these parameters\n∂L/∂Whx, ∂L/∂Whh, and ∂L/∂Wqh. According to the dependencies in Fig. 9.7.2, we can\n\n377\nBackpropagation Through Time\ntraverse in the opposite direction of the arrows to calculate and store the gradients in turn.\nTo ﬂexibly express the multiplication of matrices, vectors, and scalars of diﬀerent shapes in\nthe chain rule, we continue to use the prod operator as described in Section 5.3.\nFirst of all, diﬀerentiating the objective function with respect to the model output at any time\nstep t is fairly straightforward:\n∂L\n∂ot\n= ∂l(ot, yt)\nT · ∂ot\n∈Rq.\n(9.7.11)\nNow we can calculate the gradient of the objective with respect to the parameter Wqh in the\noutput layer: ∂L/∂Wqh ∈Rq×h. Based on Fig. 9.7.2, the objective L depends on Wqh via\no1, . . ., oT. Using the chain rule yields\n∂L\n∂Wqh\n=\nT\n∑\nt=1\nprod\n( ∂L\n∂ot\n,\n∂ot\n∂Wqh\n)\n=\nT\n∑\nt=1\n∂L\n∂ot\nh⊤\nt ,\n(9.7.12)\nwhere ∂L/∂ot is given by (9.7.11).\nNext, as shown in Fig. 9.7.2, at the ﬁnal time step T, the objective function L depends on the\nhidden state hT only via oT. Therefore, we can easily ﬁnd the gradient ∂L/∂hT ∈Rh using\nthe chain rule:\n∂L\n∂hT\n= prod\n( ∂L\n∂oT\n, ∂oT\n∂hT\n)\n= W⊤\nqh\n∂L\n∂oT\n.\n(9.7.13)\nIt gets trickier for any time step t < T, where the objective function L depends on ht via\nht+1 and ot. According to the chain rule, the gradient of the hidden state ∂L/∂ht ∈Rh at\nany time step t < T can be recurrently computed as:\n∂L\n∂ht\n= prod\n(\n∂L\n∂ht+1\n, ∂ht+1\n∂ht\n)\n+ prod\n( ∂L\n∂ot\n, ∂ot\n∂ht\n)\n= W⊤\nhh\n∂L\n∂ht+1\n+ W⊤\nqh\n∂L\n∂ot\n. (9.7.14)\nFor analysis, expanding the recurrent computation for any time step 1 ≤t ≤T gives\n∂L\n∂ht\n=\nT\n∑\ni=t\n(W⊤\nhh\n)T−iW⊤\nqh\n∂L\n∂oT+t−i\n.\n(9.7.15)\nWe can see from (9.7.15) that this simple linear example already exhibits some key problems\nof long sequence models: it involves potentially very large powers of W⊤\nhh. In it, eigenvalues\nsmaller than 1 vanish and eigenvalues larger than 1 diverge. This is numerically unstable,\nwhich manifests itself in the form of vanishing and exploding gradients. One way to address\nthis is to truncate the time steps at a computationally convenient size as discussed in Section\n9.7.1. In practice, this truncation can also be eﬀected by detaching the gradient after a given\nnumber of time steps. Later on, we will see how more sophisticated sequence models such\nas long short-term memory can alleviate this further.\nFinally, Fig. 9.7.2 shows that the objective function L depends on model parameters Whx and\nWhh in the hidden layer via hidden states h1, . . ., hT. To compute gradients with respect\nto such parameters ∂L/∂Whx ∈Rh×d and ∂L/∂Whh ∈Rh×h, we apply the chain rule\n\n378\nRecurrent Neural Networks\n144\ngiving\n∂L\n∂Whx\n=\nT\n∑\nt=1\nprod\n( ∂L\n∂ht\n,\n∂ht\n∂Whx\n)\n=\nT\n∑\nt=1\n∂L\n∂ht\nx⊤\nt ,\n∂L\n∂Whh\n=\nT\n∑\nt=1\nprod\n( ∂L\n∂ht\n,\n∂ht\n∂Whh\n)\n=\nT\n∑\nt=1\n∂L\n∂ht\nh⊤\nt−1,\n(9.7.16)\nwhere ∂L/∂ht which is recurrently computed by (9.7.13) and (9.7.14) is the key quantity\nthat aﬀects the numerical stability.\nSince backpropagation through time is the application of backpropagation in RNNs, as we\nhave explained in Section 5.3, training RNNs alternates forward propagation with back-\npropagation through time. Moreover, backpropagation through time computes and stores the\nabove gradients in turn. Speciﬁcally, stored intermediate values are reused to avoid dupli-\ncate calculations, such as storing ∂L/∂ht to be used in computation of both ∂L/∂Whx and\n∂L/∂Whh.\n9.7.3 Summary\nBackpropagation through time is merely an application of backpropagation to sequence mod-\nels with a hidden state. Truncation, such as regular or randomized, is needed for computa-\ntional convenience and numerical stability. High powers of matrices can lead to divergent or\nvanishing eigenvalues. This manifests itself in the form of exploding or vanishing gradients.\nFor eﬃcient computation, intermediate values are cached during backpropagation through\ntime.\n9.7.4 Exercises\n1. Assume that we have a symmetric matrix M ∈Rn×n with eigenvalues λi whose corre-\nsponding eigenvectors are vi (i = 1, . . ., n). Without loss of generality, assume that they\nare ordered in the order |λi| ≥|λi+1|.\n2. Show that Mk has eigenvalues λk\ni .\n3. Prove that for a random vector x ∈Rn, with high probability Mkx will be very much\naligned with the eigenvector v1 of M. Formalize this statement.\n4. What does the above result mean for gradients in RNNs?\n5. Besides gradient clipping, can you think of any other methods to cope with gradient ex-\nplosion in recurrent neural networks?\nDiscussions144.\n\n10\nModern Recurrent Neural Networks\nThe previous chapter introduced the key ideas behind recurrent neural networks (RNNs).\nHowever, just as with convolutional neural networks, there has been a tremendous amount\nof innovation in RNN architectures, culminating in several complex designs that have proven\nsuccessful in practice. In particular, the most popular designs feature mechanisms for miti-\ngating the notorious numerical instability faced by RNNs, as typiﬁed by vanishing and ex-\nploding gradients. Recall that in Chapter 9 we dealt with exploding gradients by applying a\nblunt gradient clipping heuristic. Despite the eﬃcacy of this hack, it leaves open the problem\nof vanishing gradients.\nIn this chapter, we introduce the key ideas behind the most successful RNN architectures\nfor sequences, which stem from two papers. The ﬁrst, Long Short-Term Memory (Hochreiter\nand Schmidhuber, 1997), introduces the memory cell, a unit of computation that replaces\ntraditional nodes in the hidden layer of a network. With these memory cells, networks are able\nto overcome diﬃculties with training encountered by earlier recurrent networks. Intuitively,\nthe memory cell avoids the vanishing gradient problem by keeping values in each memory\ncell’s internal state cascading along a recurrent edge with weight 1 across many successive\ntime steps. A set of multiplicative gates help the network to determine not only the inputs to\nallow into the memory state, but when the content of the memory state should inﬂuence the\nmodel’s output.\nThe second paper, Bidirectional Recurrent Neural Networks (Schuster and Paliwal, 1997),\nintroduces an architecture in which information from both the future (subsequent time steps)\nand the past (preceding time steps) are used to determine the output at any point in the\nsequence. This is in contrast to previous networks, in which only past input can aﬀect the\noutput. Bidirectional RNNs have become a mainstay for sequence labeling tasks in natural\nlanguage processing, among a myriad of other tasks. Fortunately, the two innovations are not\nmutually exclusive, and have been successfully combined for phoneme classiﬁcation (Graves\nand Schmidhuber, 2005) and handwriting recognition (Graves et al., 2008).\nThe ﬁrst sections in this chapter will explain the LSTM architecture, a lighter-weight ver-\nsion called the gated recurrent unit (GRU), the key ideas behind bidirectional RNNs and\na brief explanation of how RNN layers are stacked together to form deep RNNs. Subse-\nquently, we will explore the application of RNNs in sequence-to-sequence tasks, introducing\nmachine translation along with key ideas such as encoder–decoder architectures and beam\nsearch.\n379\n\n380\nModern Recurrent Neural Networks\n10.1 Long Short-Term Memory (LSTM)\nShortly after the ﬁrst Elman-style RNNs were trained using backpropagation (Elman, 1990),\nthe problems of learning long-term dependencies (owing to vanishing and exploding gra-\ndients) became salient, with Bengio and Hochreiter discussing the problem (Bengio et al.,\n1994, Hochreiter et al., 2001). Hochreiter had articulated this problem as early as 1991 in\nhis Master’s thesis, although the results were not widely known because the thesis was writ-\nten in German. While gradient clipping helps with exploding gradients, handling vanishing\ngradients appears to require a more elaborate solution. One of the ﬁrst and most successful\ntechniques for addressing vanishing gradients came in the form of the long short-term mem-\nory (LSTM) model due to Hochreiter and Schmidhuber (1997). LSTMs resemble standard\nrecurrent neural networks but here each ordinary recurrent node is replaced by a memory\ncell. Each memory cell contains an internal state, i.e., a node with a self-connected recurrent\nedge of ﬁxed weight 1, ensuring that the gradient can pass across many time steps without\nvanishing or exploding.\nThe term “long short-term memory” comes from the following intuition. Simple recurrent\nneural networks have long-term memory in the form of weights. The weights change slowly\nduring training, encoding general knowledge about the data. They also have short-term mem-\nory in the form of ephemeral activations, which pass from each node to successive nodes. The\nLSTM model introduces an intermediate type of storage via the memory cell. A memory cell\nis a composite unit, built from simpler nodes in a speciﬁc connectivity pattern, with the novel\ninclusion of multiplicative nodes.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n10.1.1 Gated Memory Cell\nEach memory cell is equipped with an internal state and a number of multiplicative gates that\ndetermine whether (i) a given input should impact the internal state (the input gate), (ii) the\ninternal state should be ﬂushed to 0 (the forget gate), and (iii) the internal state of a given\nneuron should be allowed to impact the cell’s output (the output gate).\nGated Hidden State\nThe key distinction between vanilla RNNs and LSTMs is that the latter support gating of the\nhidden state. This means that we have dedicated mechanisms for when a hidden state should\nbe updated and also for when it should be reset. These mechanisms are learned and they\naddress the concerns listed above. For instance, if the ﬁrst token is of great importance we\n\n381\nLong Short-Term Memory (LSTM)\nwill learn not to update the hidden state after the ﬁrst observation. Likewise, we will learn to\nskip irrelevant temporary observations. Last, we will learn to reset the latent state whenever\nneeded. We discuss this in detail below.\nInput Gate, Forget Gate, and Output Gate\nThe data feeding into the LSTM gates are the input at the current time step and the hidden\nstate of the previous time step, as illustrated in Fig. 10.1.1. Three fully connected layers\nwith sigmoid activation functions compute the values of the input, forget, and output gates.\nAs a result of the sigmoid activation, all values of the three gates are in the range of (0, 1).\nAdditionally, we require an input node, typically computed with a tanh activation function.\nIntuitively, the input gate determines how much of the input node’s value should be added\nto the current memory cell internal state. The forget gate determines whether to keep the\ncurrent value of the memory or ﬂush it. And the output gate determines whether the memory\ncell should inﬂuence the output at the current time step.\nt\nFig. 10.1.1\nComputing the input gate, the forget gate, and the output gate in an LSTM model.\nMathematically, suppose that there are h hidden units, the batch size is n, and the number\nof inputs is d. Thus, the input is Xt ∈Rn×d and the hidden state of the previous time step\nis Ht−1 ∈Rn×h. Correspondingly, the gates at time step t are deﬁned as follows: the input\ngate is It ∈Rn×h, the forget gate is Ft ∈Rn×h, and the output gate is Ot ∈Rn×h. They are\ncalculated as follows:\nIt = σ(XtWxi + Ht−1Whi + bi),\nFt = σ(XtWxf + Ht−1Whf + bf),\nOt = σ(XtWxo + Ht−1Who + bo),\n(10.1.1)\nwhere Wxi, Wxf, Wxo ∈Rd×h and Whi, Whf, Who ∈Rh×h are weight parameters and\nbi, bf, bo ∈R1×h are bias parameters. Note that broadcasting (see Section 2.1.4) is triggered\nduring the summation. We use sigmoid functions (as introduced in Section 5.1) to map the\ninput values to the interval (0, 1).\n\n382\nModern Recurrent Neural Networks\nInput Node\nNext we design the memory cell. Since we have not speciﬁed the action of the various gates\nyet, we ﬁrst introduce the input node ˜Ct ∈Rn×h. Its computation is similar to that of the\nthree gates described above, but uses a tanh function with a value range for (−1, 1) as the\nactivation function. This leads to the following equation at time step t:\n˜Ct = tanh(XtWxc + Ht−1Whc + bc),\n(10.1.2)\nwhere Wxc ∈Rd×h and Whc ∈Rh×h are weight parameters and bc ∈R1×h is a bias\nparameter.\nA quick illustration of the input node is shown in Fig. 10.1.2.\nt\nFig. 10.1.2\nComputing the input node in an LSTM model.\nMemory Cell Internal State\nIn LSTMs, the input gate It governs how much we take new data into account via ˜Ct and\nthe forget gate Ft addresses how much of the old cell internal state Ct−1 ∈Rn×h we retain.\nUsing the Hadamard (elementwise) product operator ⊙we arrive at the following update\nequation:\nCt = Ft ⊙Ct−1 + It ⊙˜Ct.\n(10.1.3)\nIf the forget gate is always 1 and the input gate is always 0, the memory cell internal state\nCt−1 will remain constant forever, passing unchanged to each subsequent time step. However,\ninput gates and forget gates give the model the ﬂexibility of being able to learn when to keep\nthis value unchanged and when to perturb it in response to subsequent inputs. In practice, this\ndesign alleviates the vanishing gradient problem, resulting in models that are much easier to\ntrain, especially when facing datasets with long sequence lengths.\nWe thus arrive at the ﬂow diagram in Fig. 10.1.3.\n\n383\nLong Short-Term Memory (LSTM)\nt\nFig. 10.1.3\nComputing the memory cell internal state in an LSTM model.\nHidden State\nLast, we need to deﬁne how to compute the output of the memory cell, i.e., the hidden state\nHt ∈Rn×h, as seen by other layers. This is where the output gate comes into play. In LSTMs,\nwe ﬁrst apply tanh to the memory cell internal state and then apply another point-wise mul-\ntiplication, this time with the output gate. This ensures that the values of Ht are always in\nthe interval (−1, 1):\nHt = Ot ⊙tanh(Ct).\n(10.1.4)\nWhenever the output gate is close to 1, we allow the memory cell internal state to impact\nthe subsequent layers uninhibited, whereas for output gate values close to 0, we prevent the\ncurrent memory from impacting other layers of the network at the current time step. Note\nthat a memory cell can accrue information across many time steps without impacting the rest\nof the network (as long as the output gate takes values close to 0), and then suddenly impact\nthe network at a subsequent time step as soon as the output gate ﬂips from values close to 0\nto values close to 1. Fig. 10.1.4 has a graphical illustration of the data ﬂow.\n10.1.2 Implementation from Scratch\nNow let’s implement an LSTM from scratch. As same as the experiments in Section 9.5, we\nﬁrst load The Time Machine dataset.\nInitializing Model Parameters\nNext, we need to deﬁne and initialize the model parameters. As previously, the hyperpa-\nrameter num_hiddens dictates the number of hidden units. We initialize weights following\na Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.\n\n384\nModern Recurrent Neural Networks\nt\nFig. 10.1.4\nComputing the hidden state in an LSTM model.\nclass LSTMScratch(d2l.Module):\ndef __init__(self, num_inputs, num_hiddens, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\ninit_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\ntriple = lambda: (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch.zeros(num_hiddens)))\nself.W_xi, self.W_hi, self.b_i = triple()\n# Input gate\nself.W_xf, self.W_hf, self.b_f = triple()\n# Forget gate\nself.W_xo, self.W_ho, self.b_o = triple()\n# Output gate\nself.W_xc, self.W_hc, self.b_c = triple()\n# Input node\nThe actual model is deﬁned as described above, consisting of three gates and an input node.\nNote that only the hidden state is passed to the output layer.\n@d2l.add_to_class(LSTMScratch)\ndef forward(self, inputs, H_C=None):\nif H_C is None:\n# Initial state with shape: (batch_size, num_hiddens)\nH = torch.zeros((inputs.shape[1], self.num_hiddens),\ndevice=inputs.device)\nC = torch.zeros((inputs.shape[1], self.num_hiddens),\ndevice=inputs.device)\nelse:\nH, C = H_C\noutputs = []\nfor X in inputs:\nI = torch.sigmoid(torch.matmul(X, self.W_xi) +\ntorch.matmul(H, self.W_hi) + self.b_i)\nF = torch.sigmoid(torch.matmul(X, self.W_xf) +\ntorch.matmul(H, self.W_hf) + self.b_f)\nO = torch.sigmoid(torch.matmul(X, self.W_xo) +\ntorch.matmul(H, self.W_ho) + self.b_o)\n(continues on next page)\n\n385\nLong Short-Term Memory (LSTM)\n(continued from previous page)\nC_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\ntorch.matmul(H, self.W_hc) + self.b_c)\nC = F * C + I * C_tilde\nH = O * torch.tanh(C)\noutputs.append(H)\nreturn outputs, (H, C)\nTraining and Prediction\nLet’s train an LSTM model by instantiating the RNNLMScratch class from Section 9.5.\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nlstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\ntrainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\n10.1.3 Concise Implementation\nUsing high-level APIs, we can directly instantiate an LSTM model. This encapsulates all the\nconﬁguration details that we made explicit above. The code is signiﬁcantly faster as it uses\ncompiled operators rather than Python for many details that we spelled out before.\nclass LSTM(d2l.RNN):\ndef __init__(self, num_inputs, num_hiddens):\nd2l.Module.__init__(self)\nself.save_hyperparameters()\nself.rnn = nn.LSTM(num_inputs, num_hiddens)\ndef forward(self, inputs, H_C=None):\nreturn self.rnn(inputs, H_C)\n\n386\nModern Recurrent Neural Networks\nlstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\ntrainer.fit(model, data)\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n'it has the the the the the'\nLSTMs are the prototypical latent variable autoregressive model with nontrivial state con-\ntrol. Many variants thereof have been proposed over the years, e.g., multiple layers, residual\nconnections, diﬀerent types of regularization. However, training LSTMs and other sequence\nmodels (such as GRUs) is quite costly because of the long range dependency of the sequence.\nLater we will encounter alternative models such as Transformers that can be used in some\ncases.\n10.1.4 Summary\nWhile LSTMs were published in 1997, they rose to great prominence with some victo-\nries in prediction competitions in the mid-2000s, and became the dominant models for se-\nquence learning from 2011 until the rise of Transformer models, starting in 2017. Even Tran-\nformers owe some of their key ideas to architecture design innovations introduced by the\nLSTM.\nLSTMs have three types of gates: input gates, forget gates, and output gates that control\nthe ﬂow of information. The hidden layer output of LSTM includes the hidden state and\nthe memory cell internal state. Only the hidden state is passed into the output layer while\nthe memory cell internal state remains entirely internal. LSTMs can alleviate vanishing and\nexploding gradients.\n10.1.5 Exercises\n\n387\nGated Recurrent Units (GRU)\n145\n1. Adjust the hyperparameters and analyze their inﬂuence on running time, perplexity, and\nthe output sequence.\n2. How would you need to change the model to generate proper words rather than just se-\nquences of characters?\n3. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden\ndimension. Pay special attention to the training and inference cost.\n4. Since the candidate memory cell ensures that the value range is between −1 and 1 by\nusing the tanh function, why does the hidden state need to use the tanh function again to\nensure that the output value range is between −1 and 1?\n5. Implement an LSTM model for time series prediction rather than character sequence\nprediction.\nDiscussions145.\n10.2 Gated Recurrent Units (GRU)\nAs RNNs and particularly the LSTM architecture (Section 10.1) rapidly gained popularity\nduring the 2010s, a number of researchers began to experiment with simpliﬁed architectures\nin hopes of retaining the key idea of incorporating an internal state and multiplicative gating\nmechanisms but with the aim of speeding up computation. The gated recurrent unit (GRU)\n(Cho et al., 2014) oﬀered a streamlined version of the LSTM memory cell that often achieves\ncomparable performance but with the advantage of being faster to compute (Chung et al.,\n2014).\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n10.2.1 Reset Gate and Update Gate\nHere, the LSTM’s three gates are replaced by two: the reset gate and the update gate. As with\nLSTMs, these gates are given sigmoid activations, forcing their values to lie in the interval\n(0, 1). Intuitively, the reset gate controls how much of the previous state we might still want\nto remember. Likewise, an update gate would allow us to control how much of the new state\nis just a copy of the old one. Fig. 10.2.1 illustrates the inputs for both the reset and update\ngates in a GRU, given the input of the current time step and the hidden state of the previous\ntime step. The outputs of the gates are given by two fully connected layers with a sigmoid\nactivation function.\n\n388\nModern Recurrent Neural Networks\nt\nFig. 10.2.1\nComputing the reset gate and the update gate in a GRU model.\nMathematically, for a given time step t, suppose that the input is a minibatch Xt ∈Rn×d\n(number of examples = n; number of inputs = d) and the hidden state of the previous time\nstep is Ht−1 ∈Rn×h (number of hidden units = h). Then the reset gate Rt ∈Rn×h and\nupdate gate Zt ∈Rn×h are computed as follows:\nRt = σ(XtWxr + Ht−1Whr + br),\nZt = σ(XtWxz + Ht−1Whz + bz),\n(10.2.1)\nwhere Wxr, Wxz ∈Rd×h and Whr, Whz ∈Rh×h are weight parameters and br, bz ∈R1×h\nare bias parameters.\n10.2.2 Candidate Hidden State\nNext, we integrate the reset gate Rt with the regular updating mechanism in (9.4.5), leading\nto the following candidate hidden state ˜Ht ∈Rn×h at time step t:\n˜Ht = tanh(XtWxh + (Rt ⊙Ht−1) Whh + bh),\n(10.2.2)\nwhere Wxh ∈Rd×h and Whh ∈Rh×h are weight parameters, bh ∈R1×h is the bias, and the\nsymbol ⊙is the Hadamard (elementwise) product operator. Here we use a tanh activation\nfunction.\nThe result is a candidate, since we still need to incorporate the action of the update gate.\nComparing with (9.4.5), the inﬂuence of the previous states can now be reduced with the\nelementwise multiplication of Rt and Ht−1 in (10.2.2). Whenever the entries in the reset\ngate Rt are close to 1, we recover a vanilla RNN such as that in (9.4.5). For all entries of the\nreset gate Rt that are close to 0, the candidate hidden state is the result of an MLP with Xt\nas input. Any pre-existing hidden state is thus reset to defaults.\nFig. 10.2.2 illustrates the computational ﬂow after applying the reset gate.\n\n389\nGated Recurrent Units (GRU)\nt\nFig. 10.2.2\nComputing the candidate hidden state in a GRU model.\n10.2.3 Hidden State\nFinally, we need to incorporate the eﬀect of the update gate Zt. This determines the extent to\nwhich the new hidden state Ht ∈Rn×h matches the old state Ht−1 compared with how much\nit resembles the new candidate state ˜Ht. The update gate Zt can be used for this purpose,\nsimply by taking elementwise convex combinations of Ht−1 and ˜Ht. This leads to the ﬁnal\nupdate equation for the GRU:\nHt = Zt ⊙Ht−1 + (1 −Zt) ⊙˜Ht.\n(10.2.3)\nWhenever the update gate Zt is close to 1, we simply retain the old state. In this case the\ninformation from Xt is ignored, eﬀectively skipping time step t in the dependency chain. By\ncontrast, whenever Zt is close to 0, the new latent state Ht approaches the candidate latent\nstate ˜Ht. Fig. 10.2.3 shows the computational ﬂow after the update gate is in action.\nt\nFig. 10.2.3\nComputing the hidden state in a GRU model.\nIn summary, GRUs have the following two distinguishing features:\n\n390\nModern Recurrent Neural Networks\n• Reset gates help capture short-term dependencies in sequences.\n• Update gates help capture long-term dependencies in sequences.\n10.2.4 Implementation from Scratch\nTo gain a better understanding of the GRU model, let’s implement it from scratch.\nInitializing Model Parameters\nThe ﬁrst step is to initialize the model parameters. We draw the weights from a Gaussian\ndistribution with standard deviation to be sigma and set the bias to 0. The hyperparame-\nter num_hiddens deﬁnes the number of hidden units. We instantiate all weights and biases\nrelating to the update gate, the reset gate, and the candidate hidden state.\nclass GRUScratch(d2l.Module):\ndef __init__(self, num_inputs, num_hiddens, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\ninit_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\ntriple = lambda: (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch.zeros(num_hiddens)))\nself.W_xz, self.W_hz, self.b_z = triple()\n# Update gate\nself.W_xr, self.W_hr, self.b_r = triple()\n# Reset gate\nself.W_xh, self.W_hh, self.b_h = triple()\n# Candidate hidden state\nDeﬁning the Model\nNow we are ready to deﬁne the GRU forward computation. Its structure is the same as that\nof the basic RNN cell, except that the update equations are more complex.\n@d2l.add_to_class(GRUScratch)\ndef forward(self, inputs, H=None):\nif H is None:\n# Initial state with shape: (batch_size, num_hiddens)\nH = torch.zeros((inputs.shape[1], self.num_hiddens),\ndevice=inputs.device)\noutputs = []\nfor X in inputs:\nZ = torch.sigmoid(torch.matmul(X, self.W_xz) +\ntorch.matmul(H, self.W_hz) + self.b_z)\nR = torch.sigmoid(torch.matmul(X, self.W_xr) +\ntorch.matmul(H, self.W_hr) + self.b_r)\nH_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\ntorch.matmul(R * H, self.W_hh) + self.b_h)\n(continues on next page)\n\n391\nGated Recurrent Units (GRU)\n(continued from previous page)\nH = Z * H + (1 - Z) * H_tilde\noutputs.append(H)\nreturn outputs, H\nTraining\nTraining a language model on The Time Machine dataset works in exactly the same manner\nas in Section 9.5.\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\ngru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\ntrainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\n10.2.5 Concise Implementation\nIn high-level APIs, we can directly instantiate a GRU model. This encapsulates all the con-\nﬁguration detail that we made explicit above.\nclass GRU(d2l.RNN):\ndef __init__(self, num_inputs, num_hiddens):\nd2l.Module.__init__(self)\nself.save_hyperparameters()\nself.rnn = nn.GRU(num_inputs, num_hiddens)\nThe code is signiﬁcantly faster in training as it uses compiled operators rather than Python.\ngru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\nmodel = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\ntrainer.fit(model, data)\n\n392\nModern Recurrent Neural Networks\n146\nAfter training, we print out the perplexity on the training set and the predicted sequence\nfollowing the provided preﬁx.\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n'it has of the fore and the'\n10.2.6 Summary\nCompared with LSTMs, GRUs achieve similar performance but tend to be lighter computa-\ntionally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs,\ncan better capture dependencies for sequences with large time step distances. GRUs contain\nbasic RNNs as their extreme case whenever the reset gate is switched on. They can also skip\nsubsequences by turning on the update gate.\n10.2.7 Exercises\n1. Assume that we only want to use the input at time step t′ to predict the output at time step\nt > t′. What are the best values for the reset and update gates for each time step?\n2. Adjust the hyperparameters and analyze their inﬂuence on running time, perplexity, and\nthe output sequence.\n3. Compare runtime, perplexity, and the output strings for rnn.RNN and rnn.GRU imple-\nmentations with each other.\n4. What happens if you implement only parts of a GRU, e.g., with only a reset gate or only\nan update gate?\nDiscussions146.\n\n393\nDeep Recurrent Neural Networks\n10.3 Deep Recurrent Neural Networks\nUp until now, we have focused on deﬁning networks consisting of a sequence input, a single\nhidden RNN layer, and an output layer. Despite having just one hidden layer between the\ninput at any time step and the corresponding output, there is a sense in which these networks\nare deep. Inputs from the ﬁrst time step can inﬂuence the outputs at the ﬁnal time stepT (often\n100s or 1000s of steps later). These inputs pass through T applications of the recurrent layer\nbefore reaching the ﬁnal output. However, we often also wish to retain the ability to express\ncomplex relationships between the inputs at a given time step and the outputs at that same\ntime step. Thus we often construct RNNs that are deep not only in the time direction but also\nin the input-to-output direction. This is precisely the notion of depth that we have already\nencountered in our development of MLPs and deep CNNs.\nThe standard method for building this sort of deep RNN is strikingly simple: we stack the\nRNNs on top of each other. Given a sequence of length T, the ﬁrst RNN produces a sequence\nof outputs, also of length T. These, in turn, constitute the inputs to the next RNN layer. In\nthis short section, we illustrate this design pattern and present a simple example for how to\ncode up such stacked RNNs. Below, in Fig. 10.3.1, we illustrate a deep RNN with L hidden\nlayers. Each hidden state operates on a sequential input and produces a sequential output.\nMoreover, any RNN cell (white box in Fig. 10.3.1) at each time step depends on both the\nsame layer’s value at the previous time step and the previous layer’s value at the same time\nstep.\nt\nFig. 10.3.1\nArchitecture of a deep RNN.\nFormally, suppose that we have a minibatch input Xt ∈Rn×d (number of examples = n;\nnumber of inputs in each example = d) at time step t. At the same time step, let the hidden\nstate of the lth hidden layer (l = 1, . . ., L) be H(l)\nt\n∈Rn×h (number of hidden units = h)\nand the output layer variable be Ot ∈Rn×q (number of outputs: q). Setting H(0)\nt\n= Xt,\n\n394\nModern Recurrent Neural Networks\nthe hidden state of the lth hidden layer that uses the activation function ϕl is calculated as\nfollows:\nH(l)\nt\n= ϕl(H(l−1)\nt\nW(l)\nxh + H(l)\nt−1W(l)\nhh + b(l)\nh ),\n(10.3.1)\nwhere the weights W(l)\nxh ∈Rh×h and W(l)\nhh ∈Rh×h, together with the bias b(l)\nh\n∈R1×h, are\nthe model parameters of the lth hidden layer.\nAt the end, the calculation of the output layer is only based on the hidden state of the ﬁnal\nLth hidden layer:\nOt = H(L)\nt\nWhq + bq,\n(10.3.2)\nwhere the weight Whq ∈Rh×q and the bias bq ∈R1×q are the model parameters of the\noutput layer.\nJust as with MLPs, the number of hidden layers L and the number of hidden units h are hy-\nperparameters that we can tune. Common RNN layer widths (h) are in the range (64, 2056),\nand common depths (L) are in the range (1, 8). In addition, we can easily get a deep-gated\nRNN by replacing the hidden state computation in (10.3.1) with that from an LSTM or a\nGRU.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n10.3.1 Implementation from Scratch\nTo implement a multilayer RNN from scratch, we can treat each layer as an RNNScratch\ninstance with its own learnable parameters.\nclass StackedRNNScratch(d2l.Module):\ndef __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.rnns = nn.Sequential(*[d2l.RNNScratch(\nnum_inputs if i==0 else num_hiddens, num_hiddens, sigma)\nfor i in range(num_layers)])\nThe multilayer forward computation simply performs forward computation layer by layer.\n@d2l.add_to_class(StackedRNNScratch)\ndef forward(self, inputs, Hs=None):\noutputs = inputs\nif Hs is None: Hs = [None] * self.num_layers\nfor i in range(self.num_layers):\noutputs, Hs[i] = self.rnns[i](outputs, Hs[i])\noutputs = torch.stack(outputs, 0)\nreturn outputs, Hs\n\n395\nDeep Recurrent Neural Networks\nAs an example, we train a deep GRU model on The Time Machine dataset (same as in Section\n9.5). To keep things simple we set the number of layers to 2.\ndata = d2l.TimeMachine(batch_size=1024, num_steps=32)\nrnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\nnum_hiddens=32, num_layers=2)\nmodel = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\ntrainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\n10.3.2 Concise Implementation\nFortunately many of the logistical details required to implement multiple layers of an RNN\nare readily available in high-level APIs. Our concise implementation will use such built-\nin functionalities. The code generalizes the one we used previously in Section 10.2, let-\nting us specify the number of layers explicitly rather than picking the default of only one\nlayer.\nclass GRU(d2l.RNN):\n#@save\n\"\"\"The multilayer GRU model.\"\"\"\ndef __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\nd2l.Module.__init__(self)\nself.save_hyperparameters()\nself.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\ndropout=dropout)\nThe architectural decisions such as choosing hyperparameters are very similar to those of\nSection 10.2. We pick the same number of inputs and outputs as we have distinct tokens,\ni.e., vocab_size. The number of hidden units is still 32. The only diﬀerence is that we now\nselect a nontrivial number of hidden layers by specifying the value of num_layers.\ngru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\nmodel = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\ntrainer.fit(model, data)\n\n396\nModern Recurrent Neural Networks\n147\nmodel.predict('it has', 20, data.vocab, d2l.try_gpu())\n'it has some about the time'\n10.3.3 Summary\nIn deep RNNs, the hidden state information is passed to the next time step of the current\nlayer and the current time step of the next layer. There exist many diﬀerent ﬂavors of deep\nRNNs, such as LSTMs, GRUs, or vanilla RNNs. Conveniently, these models are all available\nas parts of the high-level APIs of deep learning frameworks. Initialization of models requires\ncare. Overall, deep RNNs require considerable amount of work (such as learning rate and\nclipping) to ensure proper convergence.\n10.3.4 Exercises\n1. Replace the GRU by an LSTM and compare the accuracy and training speed.\n2. Increase the training data to include multiple books. How low can you go on the perplexity\nscale?\n3. Would you want to combine sources of diﬀerent authors when modeling text? Why is this\na good idea? What could go wrong?\nDiscussions147.\n\n397\nBidirectional Recurrent Neural Networks\n10.4 Bidirectional Recurrent Neural Networks\nSo far, our working example of a sequence learning task has been language modeling, where\nwe aim to predict the next token given all previous tokens in a sequence. In this scenario,\nwe wish only to condition upon the leftward context, and thus the unidirectional chaining\nof a standard RNN seems appropriate. However, there are many other sequence learning\ntasks contexts where it is perfectly ﬁne to condition the prediction at every time step on both\nthe leftward and the rightward context. Consider, for example, part of speech detection. Why\nshouldn’t we take the context in both directions into account when assessing the part of speech\nassociated with a given word?\nAnother common task—often useful as a pretraining exercise prior to ﬁne-tuning a model on\nan actual task of interest—is to mask out random tokens in a text document and then to train\na sequence model to predict the values of the missing tokens. Note that depending on what\ncomes after the blank, the likely value of the missing token changes dramatically:\n• I am ___.\n• I am ___ hungry.\n• I am ___ hungry, and I can eat half a pig.\nIn the ﬁrst sentence “happy” seems to be a likely candidate. The words “not” and “very”\nseem plausible in the second sentence, but “not” seems incompatible with the third sen-\ntences.\nFortunately, a simple technique transforms any unidirectional RNN into a bidirectional RNN\n(Schuster and Paliwal, 1997). We simply implement two unidirectional RNN layers chained\ntogether in opposite directions and acting on the same input (Fig. 10.4.1). For the ﬁrst RNN\nlayer, the ﬁrst input is x1 and the last input is xT, but for the second RNN layer, the ﬁrst\ninput is xT and the last input is x1. To produce the output of this bidirectional RNN layer, we\nsimply concatenate together the corresponding outputs of the two underlying unidirectional\nRNN layers.\nt\nFig. 10.4.1\nArchitecture of a bidirectional RNN.\nFormally for any time step t, we consider a minibatch input Xt ∈Rn×d (number of examples\n\n398\nModern Recurrent Neural Networks\n= n; number of inputs in each example = d) and let the hidden layer activation function be\nϕ. In the bidirectional architecture, the forward and backward hidden states for this time step\nare −→\nHt ∈Rn×h and ←−\nHt ∈Rn×h, respectively, where h is the number of hidden units. The\nforward and backward hidden state updates are as follows:\n−→\nHt = ϕ(XtW(f )\nxh + −→\nHt−1W(f )\nhh + b(f )\nh ),\n←−\nHt = ϕ(XtW(b)\nxh + ←−\nHt+1W(b)\nhh + b(b)\nh ),\n(10.4.1)\nwhere the weights W(f )\nxh ∈Rd×h, W(f )\nhh ∈Rh×h, W(b)\nxh ∈Rd×h, and W(b)\nhh ∈Rh×h, and the\nbiases b(f )\nh\n∈R1×h and b(b)\nh\n∈R1×h are all the model parameters.\nNext, we concatenate the forward and backward hidden states −→\nHt and ←−\nHt to obtain the hidden\nstate Ht ∈Rn×2h for feeding into the output layer. In deep bidirectional RNNs with multiple\nhidden layers, such information is passed on as input to the next bidirectional layer. Last, the\noutput layer computes the output Ot ∈Rn×q (number of outputs = q):\nOt = HtWhq + bq.\n(10.4.2)\nHere, the weight matrix Whq ∈R2h×q and the bias bq ∈R1×q are the model parameters of\nthe output layer. While technically, the two directions can have diﬀerent numbers of hidden\nunits, this design choice is seldom made in practice. We now demonstrate a simple imple-\nmentation of a bidirectional RNN.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n10.4.1 Implementation from Scratch\nTo implement a bidirectional RNN from scratch, we can include two unidirectional RNNScratch\ninstances with separate learnable parameters.\nclass BiRNNScratch(d2l.Module):\ndef __init__(self, num_inputs, num_hiddens, sigma=0.01):\nsuper().__init__()\nself.save_hyperparameters()\nself.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself.num_hiddens *= 2\n# The output dimension will be doubled\nStates of forward and backward RNNs are updated separately, while outputs of these two\nRNNs are concatenated.\n@d2l.add_to_class(BiRNNScratch)\ndef forward(self, inputs, Hs=None):\nf_H, b_H = Hs if Hs is not None else (None, None)\n(continues on next page)\n\n399\nBidirectional Recurrent Neural Networks\n148\n(continued from previous page)\nf_outputs, f_H = self.f_rnn(inputs, f_H)\nb_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\noutputs = [torch.cat((f, b), -1) for f, b in zip(\nf_outputs, reversed(b_outputs))]\nreturn outputs, (f_H, b_H)\n10.4.2 Concise Implementation\nUsing the high-level APIs, we can implement bidirectional RNNs more concisely. Here we\ntake a GRU model as an example.\nclass BiGRU(d2l.RNN):\ndef __init__(self, num_inputs, num_hiddens):\nd2l.Module.__init__(self)\nself.save_hyperparameters()\nself.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\nself.num_hiddens *= 2\n10.4.3 Summary\nIn bidirectional RNNs, the hidden state for each time step is simultaneously determined by\nthe data prior to and after the current time step. Bidirectional RNNs are mostly useful for se-\nquence encoding and the estimation of observations given bidirectional context. Bidirectional\nRNNs are very costly to train due to long gradient chains.\n10.4.4 Exercises\n1. If the diﬀerent directions use a diﬀerent number of hidden units, how will the shape of\nHt change?\n2. Design a bidirectional RNN with multiple hidden layers.\n3. Polysemy is common in natural languages. For example, the word “bank” has diﬀerent\nmeanings in contexts “i went to the bank to deposit cash” and “i went to the bank to sit\ndown”. How can we design a neural network model such that given a context sequence\nand a word, a vector representation of the word in the correct context will be returned?\nWhat type of neural architectures is preferred for handling polysemy?\nDiscussions148.\n\n400\nModern Recurrent Neural Networks\n149\n10.5 Machine Translation and the Dataset\nAmong the major breakthroughs that prompted widespread interest in modern RNNs was\na major advance in the applied ﬁeld of statistical machine translation. Here, the model is\npresented with a sentence in one language and must predict the corresponding sentence in\nanother. Note that here the sentences may be of diﬀerent lengths, and that corresponding\nwords in the two sentences may not occur in the same order, owing to diﬀerences in the two\nlanguage’s grammatical structure.\nMany problems have this ﬂavor of mapping between two such “unaligned” sequences. Exam-\nples include mapping from dialog prompts to replies or from questions to answers. Broadly,\nsuch problems are called sequence-to-sequence (seq2seq) problems and they are our focus for\nboth the remainder of this chapter and much of Chapter 11.\nIn this section, we introduce the machine translation problem and an example dataset that\nwe will use in the subsequent examples. For decades, statistical formulations of translation\nbetween languages had been popular (Brown et al., 1990, Brown et al., 1988), even before\nresearchers got neural network approaches working (methods were often lumped together\nunder the term neural machine translation).\nFirst we will need some new code to process our data. Unlike the language modeling that\nwe saw in Section 9.3, here each example consists of two separate text sequences, one in\nthe source language and another (the translation) in the target language. The following code\nsnippets will show how to load the preprocessed data into minibatches for training.\nimport os\nimport torch\nfrom d2l import torch as d2l\n10.5.1 Downloading and Preprocessing the Dataset\nTo begin, we download an English–French dataset that consists of bilingual sentence pairs\nfrom the Tatoeba Project149. Each line in the dataset is a tab-delimited pair consisting of an\nEnglish text sequence (the source) and the translated French text sequence (the target). Note\nthat each text sequence can be just one sentence, or a paragraph of multiple sentences.\nclass MTFraEng(d2l.DataModule):\n#@save\n\"\"\"The English-French dataset.\"\"\"\ndef _download(self):\nd2l.extract(d2l.download(\nd2l.DATA_URL+'fra-eng.zip', self.root,\n'94646ad1522d915e7b0f9296181140edcf86a4f5'))\nwith open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\nreturn f.read()\n\n401\nMachine Translation and the Dataset\ndata = MTFraEng()\nraw_text = data._download()\nprint(raw_text[:75])\nGo. Va !\nHi. Salut !\nRun!\nCours !\nRun!\nCourez !\nWho?\nQui ?\nWow!\nÇa alors !\nAfter downloading the dataset, we proceed with several preprocessing steps for the raw text\ndata. For instance, we replace non-breaking space with space, convert uppercase letters to\nlowercase ones, and insert space between words and punctuation marks.\n@d2l.add_to_class(MTFraEng)\n#@save\ndef _preprocess(self, text):\n# Replace non-breaking space with space\ntext = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n# Insert space between words and punctuation marks\nno_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\nout = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\nfor i, char in enumerate(text.lower())]\nreturn ''.join(out)\ntext = data._preprocess(raw_text)\nprint(text[:80])\ngo .\nva !\nhi .\nsalut !\nrun !\ncours !\nrun !\ncourez !\nwho ?\nqui ?\nwow !\nça alors !\n10.5.2 Tokenization\nUnlike the character-level tokenization in Section 9.3, for machine translation we prefer\nword-level tokenization here (today’s state-of-the-art models use more complex tokeniza-\ntion techniques). The following _tokenize method tokenizes the ﬁrst max_examples text\nsequence pairs, where each token is either a word or a punctuation mark. We append the spe-\ncial “<eos>” token to the end of every sequence to indicate the end of the sequence. When a\nmodel is predicting by generating a sequence token after token, the generation of the “<eos>”\ntoken can suggest that the output sequence is complete. In the end, the method below returns\ntwo lists of token lists: src and tgt. Speciﬁcally, src[i] is a list of tokens from the ith text\n\n402\nModern Recurrent Neural Networks\nsequence in the source language (English here) and tgt[i] is that in the target language\n(French here).\n@d2l.add_to_class(MTFraEng)\n#@save\ndef _tokenize(self, text, max_examples=None):\nsrc, tgt = [], []\nfor i, line in enumerate(text.split('\\n')):\nif max_examples and i > max_examples: break\nparts = line.split('\\t')\nif len(parts) == 2:\n# Skip empty tokens\nsrc.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\ntgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\nreturn src, tgt\nsrc, tgt = data._tokenize(text)\nsrc[:6], tgt[:6]\n([['go', '.', '<eos>'],\n['hi', '.', '<eos>'],\n['run', '!', '<eos>'],\n['run', '!', '<eos>'],\n['who', '?', '<eos>'],\n['wow', '!', '<eos>']],\n[['va', '!', '<eos>'],\n['salut', '!', '<eos>'],\n['cours', '!', '<eos>'],\n['courez', '!', '<eos>'],\n['qui', '?', '<eos>'],\n['ça', 'alors', '!', '<eos>']])\nLet’s plot the histogram of the number of tokens per text sequence. In this simple English–\nFrench dataset, most of the text sequences have fewer than 20 tokens.\n#@save\ndef show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n\"\"\"Plot the histogram for list length pairs.\"\"\"\nd2l.set_figsize()\n_, _, patches = d2l.plt.hist(\n[[len(l) for l in xlist], [len(l) for l in ylist]])\nd2l.plt.xlabel(xlabel)\nd2l.plt.ylabel(ylabel)\nfor patch in patches[1].patches:\npatch.set_hatch('/')\nd2l.plt.legend(legend)\nshow_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n'count', src, tgt);\n10.5.3 Loading Sequences of Fixed Length\n\n403\nMachine Translation and the Dataset\nRecall that in language modeling each example sequence, either a segment of one sentence\nor a span over multiple sentences, had a ﬁxed length. This was speciﬁed by the num_steps\n(number of time steps or tokens) argument from Section 9.3. In machine translation, each\nexample is a pair of source and target text sequences, where the two text sequences may have\ndiﬀerent lengths.\nFor computational eﬃciency, we can still process a minibatch of text sequences at one time\nby truncation and padding. Suppose that every sequence in the same minibatch should have\nthe same length num_steps. If a text sequence has fewer than num_steps tokens, we will\nkeep appending the special “<pad>” token to its end until its length reaches num_steps.\nOtherwise, we will truncate the text sequence by only taking its ﬁrst num_steps tokens and\ndiscarding the remaining. In this way, every text sequence will have the same length to be\nloaded in minibatches of the same shape. Furthermore, we also record length of the source\nsequence excluding padding tokens. This information will be needed by some models that we\nwill cover later.\nSince the machine translation dataset consists of pairs of languages, we can build two vo-\ncabularies for both the source language and the target language separately. With word-level\ntokenization, the vocabulary size will be signiﬁcantly larger than that using character-level\ntokenization. To alleviate this, here we treat infrequent tokens that appear less than twice as\nthe same unknown (“<unk>”) token. As we will explain later (Fig. 10.7.1), when training\nwith target sequences, the decoder output (label tokens) can be the same decoder input (tar-\nget tokens), shifted by one token; and the special beginning-of-sequence “<bos>” token will\nbe used as the ﬁrst input token for predicting the target sequence (Fig. 10.7.3).\n@d2l.add_to_class(MTFraEng)\n#@save\ndef __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\nsuper(MTFraEng, self).__init__()\nself.save_hyperparameters()\nself.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\nself._download())\n\n404\nModern Recurrent Neural Networks\n@d2l.add_to_class(MTFraEng)\n#@save\ndef _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\ndef _build_array(sentences, vocab, is_tgt=False):\npad_or_trim = lambda seq, t: (\nseq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\nsentences = [pad_or_trim(s, self.num_steps) for s in sentences]\nif is_tgt:\nsentences = [['<bos>'] + s for s in sentences]\nif vocab is None:\nvocab = d2l.Vocab(sentences, min_freq=2)\narray = torch.tensor([vocab[s] for s in sentences])\nvalid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\nreturn array, vocab, valid_len\nsrc, tgt = self._tokenize(self._preprocess(raw_text),\nself.num_train + self.num_val)\nsrc_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\ntgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\nreturn ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\nsrc_vocab, tgt_vocab)\n10.5.4 Reading the Dataset\nFinally, we deﬁne the get_dataloader method to return the data iterator.\n@d2l.add_to_class(MTFraEng)\n#@save\ndef get_dataloader(self, train):\nidx = slice(0, self.num_train) if train else slice(self.num_train, None)\nreturn self.get_tensorloader(self.arrays, train, idx)\nLet’s read the ﬁrst minibatch from the English–French dataset.\ndata = MTFraEng(batch_size=3)\nsrc, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\nprint('source:', src.type(torch.int32))\nprint('decoder input:', tgt.type(torch.int32))\nprint('source len excluding pad:', src_valid_len.type(torch.int32))\nprint('label:', label.type(torch.int32))\nsource: tensor([[101, 175,\n2,\n3,\n4,\n4,\n4,\n4,\n4],\n[ 86,\n22,\n2,\n3,\n4,\n4,\n4,\n4,\n4],\n[183, 105,\n2,\n3,\n4,\n4,\n4,\n4,\n4]], dtype=torch.int32)\ndecoder input: tensor([[\n3,\n6,\n0,\n4,\n5,\n5,\n5,\n5,\n5],\n[\n3, 108, 183,\n38,\n2,\n4,\n5,\n5,\n5],\n[\n3, 135,\n92,\n64,\n2,\n4,\n5,\n5,\n5]], dtype=torch.int32)\nsource len excluding pad: tensor([4, 4, 4], dtype=torch.int32)\nlabel: tensor([[\n6,\n0,\n4,\n5,\n5,\n5,\n5,\n5,\n5],\n[108, 183,\n38,\n2,\n4,\n5,\n5,\n5,\n5],\n[135,\n92,\n64,\n2,\n4,\n5,\n5,\n5,\n5]], dtype=torch.int32)\nWe show a pair of source and target sequences processed by the above _build_arrays\nmethod (in the string format).\n\n405\nMachine Translation and the Dataset\n150\n@d2l.add_to_class(MTFraEng)\n#@save\ndef build(self, src_sentences, tgt_sentences):\nraw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\nsrc_sentences, tgt_sentences)])\narrays, _, _ = self._build_arrays(\nraw_text, self.src_vocab, self.tgt_vocab)\nreturn arrays\nsrc, tgt, _,\n_ = data.build(['hi .'], ['salut .'])\nprint('source:', data.src_vocab.to_tokens(src[0].type(torch.int32)))\nprint('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))\nsource: ['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '\n,→<pad>']\ntarget: ['<bos>', 'salut', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '\n,→<pad>']\n10.5.5 Summary\nIn natural language processing, machine translation refers to the task of automatically map-\nping from a sequence representing a string of text in a source language to a string represent-\ning a plausible translation in a target language. Using word-level tokenization, the vocabulary\nsize will be signiﬁcantly larger than that using character-level tokenization, but the sequence\nlengths will be much shorter. To mitigate the large vocabulary size, we can treat infrequent\ntokens as some “unknown” token. We can truncate and pad text sequences so that all of them\nwill have the same length to be loaded in minibatches. Modern implementations often bucket\nsequences with similar lengths to avoid wasting excessive computation on padding.\n10.5.6 Exercises\n1. Try diﬀerent values of the max_examples argument in the _tokenize method. How does\nthis aﬀect the vocabulary sizes of the source language and the target language?\n2. Text in some languages such as Chinese and Japanese does not have word boundary in-\ndicators (e.g., space). Is word-level tokenization still a good idea for such cases? Why or\nwhy not?\nDiscussions150.\n\n406\nModern Recurrent Neural Networks\n10.6 The Encoder−Decoder Architecture\nIn general sequence-to-sequence problems like machine translation (Section 10.5), inputs and\noutputs are of varying lengths that are unaligned. The standard approach to handling this sort\nof data is to design an encoder–decoder architecture (Fig. 10.6.1) consisting of two major\ncomponents: an encoder that takes a variable-length sequence as input, and a decoder that\nacts as a conditional language model, taking in the encoded input and the leftwards context\nof the target sequence and predicting the subsequent token in the target sequence.\nt\nFig. 10.6.1\nThe encoder–decoder architecture.\nLet’s take machine translation from English to French as an example. Given an input sequence\nin English: “They”, “are”, “watching”, “.”, this encoder–decoder architecture ﬁrst encodes the\nvariable-length input into a state, then decodes the state to generate the translated sequence,\ntoken by token, as output: “Ils”, “regardent”, “.”. Since the encoder–decoder architecture\nforms the basis of diﬀerent sequence-to-sequence models in subsequent sections, this section\nwill convert this architecture into an interface that will be implemented later.\nfrom torch import nn\nfrom d2l import torch as d2l\n10.6.1 Encoder\nIn the encoder interface, we just specify that the encoder takes variable-length sequences as\ninput X. The implementation will be provided by any model that inherits this base Encoder\nclass.\nclass Encoder(nn.Module):\n#@save\n\"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\ndef __init__(self):\nsuper().__init__()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef forward(self, X, *args):\nraise NotImplementedError\n10.6.2 Decoder\nIn the following decoder interface, we add an additional init_state method to convert the\nencoder output (enc_all_outputs) into the encoded state. Note that this step may require\n\n407\nThe Encoder−Decoder Architecture\nextra inputs, such as the valid length of the input, which was explained in Section 10.5. To\ngenerate a variable-length sequence token by token, every time the decoder may map an input\n(e.g., the generated token at the previous time step) and the encoded state into an output token\nat the current time step.\nclass Decoder(nn.Module):\n#@save\n\"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\ndef __init__(self):\nsuper().__init__()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef init_state(self, enc_all_outputs, *args):\nraise NotImplementedError\ndef forward(self, X, state):\nraise NotImplementedError\n10.6.3 Putting the Encoder and Decoder Together\nIn the forward propagation, the output of the encoder is used to produce the encoded state,\nand this state will be further used by the decoder as one of its input.\nclass EncoderDecoder(d2l.Classifier):\n#@save\n\"\"\"The base class for the encoder--decoder architecture.\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder\nself.decoder = decoder\ndef forward(self, enc_X, dec_X, *args):\nenc_all_outputs = self.encoder(enc_X, *args)\ndec_state = self.decoder.init_state(enc_all_outputs, *args)\n# Return decoder output only\nreturn self.decoder(dec_X, dec_state)[0]\nIn the next section, we will see how to apply RNNs to design sequence-to-sequence models\nbased on this encoder–decoder architecture.\n10.6.4 Summary\nEncoder-decoder architectures can handle inputs and outputs that both consist of variable-\nlength sequences and thus are suitable for sequence-to-sequence problems such as machine\ntranslation. The encoder takes a variable-length sequence as input and transforms it into a\nstate with a ﬁxed shape. The decoder maps the encoded state of a ﬁxed shape to a variable-\nlength sequence.\n10.6.5 Exercises\n\n408\nModern Recurrent Neural Networks\n151\n1. Suppose that we use neural networks to implement the encoder–decoder architecture. Do\nthe encoder and the decoder have to be the same type of neural network?\n2. Besides machine translation, can you think of another application where the encoder–\ndecoder architecture can be applied?\nDiscussions151.\n10.7 Sequence-to-Sequence Learning for Machine\nTranslation\nIn so-called sequence-to-sequence problems such as machine translation (as discussed in Sec-\ntion 10.5), where inputs and outputs each consist of variable-length unaligned sequences,\nwe generally rely on encoder–decoder architectures (Section 10.6). In this section, we will\ndemonstrate the application of an encoder–decoder architecture, where both the encoder\nand decoder are implemented as RNNs, to the task of machine translation (Cho et al., 2014,\nSutskever et al., 2014).\nHere, the encoder RNN will take a variable-length sequence as input and transform it into\na ﬁxed-shape hidden state. Later, in Chapter 11, we will introduce attention mechanisms,\nwhich allow us to access encoded inputs without having to compress the entire input into a\nsingle ﬁxed-length representation.\nThen to generate the output sequence, one token at a time, the decoder model, consisting of\na separate RNN, will predict each successive target token given both the input sequence and\nthe preceding tokens in the output. During training, the decoder will typically be conditioned\nupon the preceding tokens in the oﬃcial “ground truth” label. However, at test time, we will\nwant to condition each output of the decoder on the tokens already predicted. Note that\nif we ignore the encoder, the decoder in a sequence-to-sequence architecture behaves just\nlike a normal language model. Fig. 10.7.1 illustrates how to use two RNNs for sequence-to-\nsequence learning in machine translation.\nt\nFig. 10.7.1\nSequence-to-sequence learning with an RNN encoder and an RNN decoder.\nIn Fig. 10.7.1, the special “<eos>” token marks the end of the sequence. Our model can\n\n409\nSequence-to-Sequence Learning for Machine Translation\nstop making predictions once this token is generated. At the initial time step of the RNN\ndecoder, there are two special design decisions to be aware of: First, we begin every input\nwith a special beginning-of-sequence “<bos>” token. Second, we may feed the ﬁnal hidden\nstate of the encoder into the decoder at every single decoding time step (Cho et al., 2014).\nIn some other designs, such as that of Sutskever et al. (2014), the ﬁnal hidden state of the\nRNN encoder is used to initiate the hidden state of the decoder only at the ﬁrst decoding\nstep.\nimport collections\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n10.7.1 Teacher Forcing\nWhile running the encoder on the input sequence is relatively straightforward, handling the\ninput and output of the decoder requires more care. The most common approach is some-\ntimes called teacher forcing. Here, the original target sequence (token labels) is fed into the\ndecoder as input. More concretely, the special beginning-of-sequence token and the original\ntarget sequence, excluding the ﬁnal token, are concatenated as input to the decoder, while\nthe decoder output (labels for training) is the original target sequence, shifted by one token:\n“<bos>”, “Ils”, “regardent”, “.” →“Ils”, “regardent”, “.”, “<eos>” (Fig. 10.7.1).\nOur implementation in Section 10.5.3 prepared training data for teacher forcing, where shift-\ning tokens for self-supervised learning is similar to the training of language models in Section\n9.3. An alternative approach is to feed the predicted token from the previous time step as the\ncurrent input to the decoder.\nIn the following, we explain the design depicted in Fig. 10.7.1 in greater detail. We will train\nthis model for machine translation on the English–French dataset as introduced in Section\n10.5.\n10.7.2 Encoder\nRecall that the encoder transforms an input sequence of variable length into a ﬁxed-shape\ncontext variable c (see Fig. 10.7.1).\nConsider a single sequence example (batch size 1). Suppose the input sequence is x1, . . ., xT,\nsuch that xt is the tth token. At time step t, the RNN transforms the input feature vector xt\nfor xt and the hidden state ht−1 from the previous time step into the current hidden state ht.\nWe can use a function f to express the transformation of the RNN’s recurrent layer:\nht = f (xt, ht−1).\n(10.7.1)\n\n410\nModern Recurrent Neural Networks\nIn general, the encoder transforms the hidden states at all time steps into a context variable\nthrough a customized function q:\nc = q(h1, . . ., hT).\n(10.7.2)\nFor example, in Fig. 10.7.1, the context variable is just the hidden state hT correspond-\ning to the encoder RNN’s representation after processing the ﬁnal token of the input se-\nquence.\nIn this example, we have used a unidirectional RNN to design the encoder, where the hidden\nstate only depends on the input subsequence at and before the time step of the hidden state. We\ncan also construct encoders using bidirectional RNNs. In this case, a hidden state depends on\nthe subsequence before and after the time step (including the input at the current time step),\nwhich encodes the information of the entire sequence.\nNow let’s implement the RNN encoder. Note that we use an embedding layer to obtain the\nfeature vector for each token in the input sequence. The weight of an embedding layer is a ma-\ntrix, where the number of rows corresponds to the size of the input vocabulary (vocab_size)\nand number of columns corresponds to the feature vector’s dimension (embed_size). For\nany input token index i, the embedding layer fetches the ith row (starting from 0) of the\nweight matrix to return its feature vector. Here we implement the encoder with a multilayer\nGRU.\ndef init_seq2seq(module):\n#@save\n\"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\nif type(module) == nn.Linear:\nnn.init.xavier_uniform_(module.weight)\nif type(module) == nn.GRU:\nfor param in module._flat_weights_names:\nif \"weight\" in param:\nnn.init.xavier_uniform_(module._parameters[param])\nclass Seq2SeqEncoder(d2l.Encoder):\n#@save\n\"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\ndef __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\ndropout=0):\nsuper().__init__()\nself.embedding = nn.Embedding(vocab_size, embed_size)\nself.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\nself.apply(init_seq2seq)\ndef forward(self, X, *args):\n# X shape: (batch_size, num_steps)\nembs = self.embedding(X.t().type(torch.int64))\n# embs shape: (num_steps, batch_size, embed_size)\noutputs, state = self.rnn(embs)\n# outputs shape: (num_steps, batch_size, num_hiddens)\n# state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, state\nLet’s use a concrete example to illustrate the above encoder implementation. Below, we in-\nstantiate a two-layer GRU encoder whose number of hidden units is 16. Given a minibatch\n\n411\nSequence-to-Sequence Learning for Machine Translation\nof sequence inputs X (batch size = 4; number of time steps = 9), the hidden states of the\nﬁnal layer at all the time steps (enc_outputs returned by the encoder’s recurrent layers) are\na tensor of shape (number of time steps, batch size, number of hidden units).\nvocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\nbatch_size, num_steps = 4, 9\nencoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\nX = torch.zeros((batch_size, num_steps))\nenc_outputs, enc_state = encoder(X)\nd2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\nSince we are using a GRU here, the shape of the multilayer hidden states at the ﬁnal time\nstep is (number of hidden layers, batch size, number of hidden units).\nd2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n10.7.3 Decoder\nGiven a target output sequence y1, y2, . . ., yT′ for each time step t′ (we use t′ to diﬀerenti-\nate from the input sequence time steps), the decoder assigns a predicted probability to each\npossible token occurring at step yt′+1 conditioned upon the previous tokens in the target\ny1, . . ., yt′ and the context variable c, i.e., P(yt′+1 | y1, . . ., yt′, c).\nTo predict the subsequent token t′ + 1 in the target sequence, the RNN decoder takes the\nprevious step’s target token yt′, the hidden RNN state from the previous time step st′−1, and\nthe context variable c as its input, and transforms them into the hidden state st′ at the current\ntime step. We can use a function g to express the transformation of the decoder’s hidden\nlayer:\nst′ = g(yt′−1, c, st′−1).\n(10.7.3)\nAfter obtaining the hidden state of the decoder, we can use an output layer and the softmax\noperation to compute the predictive distribution p(yt′+1 | y1, . . ., yt′, c) over the subsequent\noutput token t′ + 1.\nFollowing Fig. 10.7.1, when implementing the decoder as follows, we directly use the hidden\nstate at the ﬁnal time step of the encoder to initialize the hidden state of the decoder. This\nrequires that the RNN encoder and the RNN decoder have the same number of layers and\nhidden units. To further incorporate the encoded input sequence information, the context\nvariable is concatenated with the decoder input at all the time steps. To predict the probability\ndistribution of the output token, we use a fully connected layer to transform the hidden state\nat the ﬁnal layer of the RNN decoder.\nclass Seq2SeqDecoder(d2l.Decoder):\n\"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\ndef __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n(continues on next page)\n\n412\nModern Recurrent Neural Networks\n(continued from previous page)\ndropout=0):\nsuper().__init__()\nself.embedding = nn.Embedding(vocab_size, embed_size)\nself.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\nnum_layers, dropout)\nself.dense = nn.LazyLinear(vocab_size)\nself.apply(init_seq2seq)\ndef init_state(self, enc_all_outputs, *args):\nreturn enc_all_outputs\ndef forward(self, X, state):\n# X shape: (batch_size, num_steps)\n# embs shape: (num_steps, batch_size, embed_size)\nembs = self.embedding(X.t().type(torch.int32))\nenc_output, hidden_state = state\n# context shape: (batch_size, num_hiddens)\ncontext = enc_output[-1]\n# Broadcast context to (num_steps, batch_size, num_hiddens)\ncontext = context.repeat(embs.shape[0], 1, 1)\n# Concat at the feature dimension\nembs_and_context = torch.cat((embs, context), -1)\noutputs, hidden_state = self.rnn(embs_and_context, hidden_state)\noutputs = self.dense(outputs).swapaxes(0, 1)\n# outputs shape: (batch_size, num_steps, vocab_size)\n# hidden_state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, [enc_output, hidden_state]\nTo illustrate the implemented decoder, below we instantiate it with the same hyperparameters\nfrom the aforementioned encoder. As we can see, the output shape of the decoder becomes\n(batch size, number of time steps, vocabulary size), where the ﬁnal dimension of the tensor\nstores the predicted token distribution.\ndecoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\nstate = decoder.init_state(encoder(X))\ndec_outputs, state = decoder(X, state)\nd2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))\nThe layers in the above RNN encoder–decoder model are summarized in Fig. 10.7.2.\nt\nFig. 10.7.2\nLayers in an RNN encoder–decoder model.\n\n413\nSequence-to-Sequence Learning for Machine Translation\n10.7.4 Encoder–Decoder for Sequence-to-Sequence Learning\nPutting it all together in code yields the following:\nclass Seq2Seq(d2l.EncoderDecoder):\n#@save\n\"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\ndef __init__(self, encoder, decoder, tgt_pad, lr):\nsuper().__init__(encoder, decoder)\nself.save_hyperparameters()\ndef validation_step(self, batch):\nY_hat = self(*batch[:-1])\nself.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\ndef configure_optimizers(self):\n# Adam optimizer is used here\nreturn torch.optim.Adam(self.parameters(), lr=self.lr)\n10.7.5 Loss Function with Masking\nAt each time step, the decoder predicts a probability distribution for the output tokens. As\nwith language modeling, we can apply softmax to obtain the distribution and calculate the\ncross-entropy loss for optimization. Recall from Section 10.5 that the special padding tokens\nare appended to the end of sequences and so sequences of varying lengths can be eﬃciently\nloaded in minibatches of the same shape. However, prediction of padding tokens should be\nexcluded from loss calculations. To this end, we can mask irrelevant entries with zero values\nso that multiplication of any irrelevant prediction with zero equates to zero.\n@d2l.add_to_class(Seq2Seq)\ndef loss(self, Y_hat, Y):\nl = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\nmask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)\nreturn (l * mask).sum() / mask.sum()\n10.7.6 Training\nNow we can create and train an RNN encoder–decoder model for sequence-to-sequence\nlearning on the machine translation dataset.\ndata = d2l.MTFraEng(batch_size=128)\nembed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\nencoder = Seq2SeqEncoder(\nlen(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqDecoder(\nlen(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\nlr=0.005)\n(continues on next page)\n\n414\nModern Recurrent Neural Networks\n(continued from previous page)\ntrainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\n10.7.7 Prediction\nTo predict the output sequence at each step, the predicted token from the previous time step\nis fed into the decoder as an input. One simple strategy is to sample whichever token that\nhas been assigned by the decoder the highest probability when predicting at each step. As\nin training, at the initial time step the beginning-of-sequence (“<bos>”) token is fed into\nthe decoder. This prediction process is illustrated in Fig. 10.7.3. When the end-of-sequence\n(“<eos>”) token is predicted, the prediction of the output sequence is complete.\nt\nFig. 10.7.3\nPredicting the output sequence token by token using an RNN encoder–decoder.\nIn the next section, we will introduce more sophisticated strategies based on beam search\n(Section 10.8).\n@d2l.add_to_class(d2l.EncoderDecoder)\n#@save\ndef predict_step(self, batch, device, num_steps,\nsave_attention_weights=False):\nbatch = [a.to(device) for a in batch]\nsrc, tgt, src_valid_len, _ = batch\nenc_all_outputs = self.encoder(src, src_valid_len)\ndec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n(continues on next page)\n\n415\nSequence-to-Sequence Learning for Machine Translation\n(continued from previous page)\noutputs, attention_weights = [tgt[:, 0].unsqueeze(1), ], []\nfor _ in range(num_steps):\nY, dec_state = self.decoder(outputs[-1], dec_state)\noutputs.append(Y.argmax(2))\n# Save attention weights (to be covered later)\nif save_attention_weights:\nattention_weights.append(self.decoder.attention_weights)\nreturn torch.cat(outputs[1:], 1), attention_weights\n10.7.8 Evaluation of Predicted Sequences\nWe can evaluate a predicted sequence by comparing it with the target sequence (the ground\ntruth). But what precisely is the appropriate measure for comparing similarity between two\nsequences?\nBilingual Evaluation Understudy (BLEU), though originally proposed for evaluating machine\ntranslation results (Papineni et al., 2002), has been extensively used in measuring the quality\nof output sequences for diﬀerent applications. In principle, for any n-gram (Section 9.3.1)\nin the predicted sequence, BLEU evaluates whether this n-gram appears in the target se-\nquence.\nDenote by pn the precision of an n-gram, deﬁned as the ratio of the number of matched\nn-grams in the predicted and target sequences to the number of n-grams in the predicted\nsequence. To explain, given a target sequence A, B, C, D, E, F, and a predicted sequence A,\nB, B, C, D, we have p1 = 4/5, p2 = 3/4, p3 = 1/3, and p4 = 0. Now let lenlabel and lenpred\nbe the numbers of tokens in the target sequence and the predicted sequence, respectively.\nThen, BLEU is deﬁned as\nexp\n(\nmin\n(\n0, 1 −lenlabel\nlenpred\n))\nk\n∏\nn=1\np1/2n\nn\n,\n(10.7.4)\nwhere k is the longest n-gram for matching.\nBased on the deﬁnition of BLEU in (10.7.4), whenever the predicted sequence is the same\nas the target sequence, BLEU is 1. Moreover, since matching longer n-grams is more diﬃ-\ncult, BLEU assigns a greater weight when a longer n-gram has high precision. Speciﬁcally,\nwhen pn is ﬁxed, p1/2n\nn\nincreases as n grows (the original paper uses p1/n\nn\n). Furthermore,\nsince predicting shorter sequences tends to yield a higher pn value, the coeﬃcient before the\nmultiplication term in (10.7.4) penalizes shorter predicted sequences. For example, when\nk = 2, given the target sequence A, B, C, D, E, F and the predicted sequence A, B, although\np1 = p2 = 1, the penalty factor exp(1 −6/2) ≈0.14 lowers the BLEU.\nWe implement the BLEU measure as follows.\ndef bleu(pred_seq, label_seq, k):\n#@save\n\"\"\"Compute the BLEU.\"\"\"\n(continues on next page)\n\n416\nModern Recurrent Neural Networks\n(continued from previous page)\npred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\nlen_pred, len_label = len(pred_tokens), len(label_tokens)\nscore = math.exp(min(0, 1 - len_label / len_pred))\nfor n in range(1, min(k, len_pred) + 1):\nnum_matches, label_subs = 0, collections.defaultdict(int)\nfor i in range(len_label - n + 1):\nlabel_subs[' '.join(label_tokens[i: i + n])] += 1\nfor i in range(len_pred - n + 1):\nif label_subs[' '.join(pred_tokens[i: i + n])] > 0:\nnum_matches += 1\nlabel_subs[' '.join(pred_tokens[i: i + n])] -= 1\nscore *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\nreturn score\nIn the end, we use the trained RNN encoder–decoder to translate a few English sentences\ninto French and compute the BLEU of the results.\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\npreds, _ = model.predict_step(\ndata.build(engs, fras), d2l.try_gpu(), data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\ntranslation = []\nfor token in data.tgt_vocab.to_tokens(p):\nif token == '<eos>':\nbreak\ntranslation.append(token)\nprint(f'{en} => {translation}, bleu,'\nf'{bleu(\" \".join(translation), fr, k=2):.3f}')\ngo . => ['va', '!'], bleu,1.000\ni lost . => [\"j'ai\", 'perdu', '.'], bleu,1.000\nhe's calm . => ['soyez', 'calmes', '.'], bleu,0.000\ni'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000\n10.7.9 Summary\nFollowing the design of the encoder–decoder architecture, we can use two RNNs to design a\nmodel for sequence-to-sequence learning. In encoder–decoder training, the teacher forcing\napproach feeds original output sequences (in contrast to predictions) into the decoder. When\nimplementing the encoder and the decoder, we can use multilayer RNNs. We can use masks\nto ﬁlter out irrelevant computations, such as when calculating the loss. For evaluating output\nsequences, BLEU is a popular measure that matches n-grams between the predicted sequence\nand the target sequence.\n10.7.10 Exercises\n\n417\nBeam Search\n152\n1. Can you adjust the hyperparameters to improve the translation results?\n2. Rerun the experiment without using masks in the loss calculation. What results do you\nobserve? Why?\n3. If the encoder and the decoder diﬀer in the number of layers or the number of hidden\nunits, how can we initialize the hidden state of the decoder?\n4. In training, replace teacher forcing with feeding the prediction at the previous time step\ninto the decoder. How does this inﬂuence the performance?\n5. Rerun the experiment by replacing GRU with LSTM.\n6. Are there any other ways to design the output layer of the decoder?\nDiscussions152.\n10.8 Beam Search\nIn Section 10.7, we introduced the encoder–decoder architecture, and the standard tech-\nniques for training them end-to-end. However, when it came to test-time prediction, we men-\ntioned only the greedy strategy, where we select at each time step the token given the highest\npredicted probability of coming next, until, at some time step, we ﬁnd that we have predicted\nthe special end-of-sequence “<eos>” token. In this section, we will begin by formalizing this\ngreedy search strategy and identifying some problems that practitioners tend to run into. Sub-\nsequently, we compare this strategy with two alternatives: exhaustive search (illustrative but\nnot practical) and beam search (the standard method in practice).\nLet’s begin by setting up our mathematical notation, borrowing conventions from Section\n10.7. At any time step t′, the decoder outputs predictions representing the probability of each\ntoken in the vocabulary coming next in the sequence (the likely value of yt′+1), conditioned\non the previous tokens y1, . . ., yt′ and the context variable c, produced by the encoder to\nrepresent the input sequence. To quantify computational cost, denote by Y the output vocab-\nulary (including the special end-of-sequence token “<eos>”). Let’s also specify the maximum\nnumber of tokens of an output sequence as T ′. Our goal is to search for an ideal output from\nall O(|Y|T′) possible output sequences. Note that this slightly overestimates the number of\ndistinct outputs because there are no subsequent tokens once the “<eos>” token occurs. How-\never, for our purposes, this number roughly captures the size of the search space.\n10.8.1 Greedy Search\n\n418\nModern Recurrent Neural Networks\nConsider the simple greedy search strategy from Section 10.7. Here, at any time step t′, we\nsimply select the token with the highest conditional probability from Y, i.e.,\nyt′ = argmax\ny∈Y\nP(y | y1, . . ., yt′−1, c).\n(10.8.1)\nOnce our model outputs “<eos>” (or we reach the maximum length T ′) the output sequence\nis completed.\nThis strategy might look reasonable, and in fact it is not so bad! Considering how computa-\ntionally undemanding it is, you’d be hard pressed to get more bang for your buck. However,\nif we put aside eﬃciency for a minute, it might seem more reasonable to search for the most\nlikely sequence, not the sequence of (greedily selected) most likely tokens. It turns out that\nthese two objects can be quite diﬀerent. The most likely sequence is the one that maximizes\nthe expression ∏T′\nt′=1 P(yt′ | y1, . . ., yt′−1, c). In our machine translation example, if the de-\ncoder truly recovered the probabilities of the underlying generative process, then this would\ngive us the most likely translation. Unfortunately, there is no guarantee that greedy search\nwill give us this sequence.\nLet’s illustrate it with an example. Suppose that there are four tokens “A”, “B”, “C”, and\n“<eos>” in the output dictionary. In Fig. 10.8.1, the four numbers under each time step rep-\nresent the conditional probabilities of generating “A”, “B”, “C”, and “<eos>” respectively, at\nthat time step.\nt\nFig. 10.8.1\nAt each time step, greedy search selects the token with the highest conditional probability.\nAt each time step, greedy search selects the token with the highest conditional probability.\nTherefore, the output sequence “A”, “B”, “C”, and “<eos>” will be predicted (Fig. 10.8.1).\nThe conditional probability of this output sequence is 0.5 × 0.4 × 0.4 × 0.6 = 0.048.\nNext, let’s look at another example in Fig. 10.8.2. Unlike in Fig. 10.8.1, at time step 2 we\nselect the token “C”, which has the second highest conditional probability.\nt\nFig. 10.8.2\nThe four numbers under each time step represent the conditional probabilities of\ngenerating “A”, “B”, “C”, and “<eos>” at that time step. At time step 2, the token “C”,\nwhich has the second highest conditional probability, is selected.\n\n419\nBeam Search\nSince the output subsequences at time steps 1 and 2, on which time step 3 is based, have\nchanged from “A” and “B” in Fig. 10.8.1 to “A” and “C” in Fig. 10.8.2, the conditional\nprobability of each token at time step 3 has also changed in Fig. 10.8.2. Suppose that we\nchoose the token “B” at time step 3. Now time step 4 is conditional on the output subsequence\nat the ﬁrst three time steps “A”, “C”, and “B”, which has changed from “A”, “B”, and “C” in\nFig. 10.8.1. Therefore, the conditional probability of generating each token at time step 4 in\nFig. 10.8.2 is also diﬀerent from that in Fig. 10.8.1. As a result, the conditional probability of\nthe output sequence “A”, “C”, “B”, and “<eos>” in Fig. 10.8.2 is 0.5×0.3×0.6×0.6 = 0.054,\nwhich is greater than that of greedy search in Fig. 10.8.1. In this example, the output sequence\n“A”, “B”, “C”, and “<eos>” obtained by the greedy search is not optimal.\n10.8.2 Exhaustive Search\nIf the goal is to obtain the most likely sequence, we may consider using exhaustive search:\nenumerate all the possible output sequences with their conditional probabilities, and then\noutput the one that scores the highest predicted probability.\nWhile this would certainly give us what we desire, it would come at a prohibitive computa-\ntional cost of O(|Y|T′), exponential in the sequence length and with an enormous base given\nby the vocabulary size. For example, when |Y| = 10000 and T ′ = 10, both small numbers\nwhen compared with ones in real applications, we will need to evaluate 1000010 = 1040\nsequences, which is already beyond the capabilities of any foreseeable computers. On the\nother hand, the computational cost of greedy search is O(|Y| T ′): miraculously cheap but\nfar from optimal. For example, when |Y| = 10000 and T ′ = 10, we only need to evaluate\n10000 × 10 = 105 sequences.\n10.8.3 Beam Search\nYou could view sequence decoding strategies as lying on a spectrum, with beam search strik-\ning a compromise between the eﬃciency of greedy search and the optimality of exhaustive\nsearch. The most straightforward version of beam search is characterized by a single hyper-\nparameter, the beam size, k. Let’s explain this terminology. At time step 1, we select the\nk tokens with the highest predicted probabilities. Each of them will be the ﬁrst token of k\ncandidate output sequences, respectively. At each subsequent time step, based on the k can-\ndidate output sequences at the previous time step, we continue to select k candidate output\nsequences with the highest predicted probabilities from k |Y| possible choices.\nFig. 10.8.3 demonstrates the process of beam search with an example. Suppose that the output\nvocabulary contains only ﬁve elements: Y = {A, B, C, D, E}, where one of them is “<eos>”.\nLet the beam size be two and the maximum length of an output sequence be three. At time\nstep 1, suppose that the tokens with the highest conditional probabilities P(y1 | c) are A and\nC. At time step 2, for all y2 ∈Y, we compute\nP(A, y2 | c) = P(A | c)P(y2 | A, c),\nP(C, y2 | c) = P(C | c)P(y2 | C, c),\n(10.8.2)\n\n420\nModern Recurrent Neural Networks\nt\nFig. 10.8.3\nThe process of beam search (beam size = 2; maximum length of an output sequence = 3).\nThe candidate output sequences are A, C, AB, CE, ABD, and CED.\nand pick the largest two among these ten values, say P(A, B | c) and P(C, E | c). Then at\ntime step 3, for all y3 ∈Y, we compute\nP(A, B, y3 | c) = P(A, B | c)P(y3 | A, B, c),\nP(C, E, y3 | c) = P(C, E | c)P(y3 | C, E, c),\n(10.8.3)\nand pick the largest two among these ten values, say P(A, B, D | c) and P(C, E, D | c). As\na result, we get six candidates output sequences: (i) A; (ii) C; (iii) A, B; (iv) C, E; (v) A, B,\nD; and (vi) C, E, D.\nIn the end, we obtain the set of ﬁnal candidate output sequences based on these six sequences\n(e.g., discard portions including and after “<eos>”). Then we choose the output sequence\nwhich maximizes the following score:\n1\nLα log P(y1, . . ., yL | c) = 1\nLα\nL\n∑\nt′=1\nlog P(yt′ | y1, . . ., yt′−1, c);\n(10.8.4)\nhere L is the length of the ﬁnal candidate sequence and α is usually set to 0.75. Since a\nlonger sequence has more logarithmic terms in the summation of (10.8.4), the term Lα in\nthe denominator penalizes long sequences.\nThe computational cost of beam search is O(k |Y| T ′). This result is in between that of greedy\nsearch and that of exhaustive search. Greedy search can be treated as a special case of beam\nsearch arising when the beam size is set to 1.\n10.8.4 Summary\nSequence searching strategies include greedy search, exhaustive search, and beam search.\nBeam search provides a trade-oﬀbetween accuracy and computational cost via the ﬂexible\nchoice of the beam size.\n\n421\nBeam Search\n153\n10.8.5 Exercises\n1. Can we treat exhaustive search as a special type of beam search? Why or why not?\n2. Apply beam search in the machine translation problem in Section 10.7. How does the\nbeam size aﬀect the translation results and the prediction speed?\n3. We used language modeling for generating text following user-provided preﬁxes in Section\n9.5. Which kind of search strategy does it use? Can you improve it?\nDiscussions153.\n\n11\nAttention Mechanisms and Transformers\nThe earliest years of the deep learning boom were driven primarily by results produced using\nthe multilayer perceptron, convolutional network, and recurrent network architectures. Re-\nmarkably, the model architectures that underpinned many of deep learning’s breakthroughs\nin the 2010s had changed remarkably little relative to their antecedents despite the lapse\nof nearly 30 years. While plenty of new methodological innovations made their way into\nmost practitioner’s toolkits—ReLU activations, residual layers, batch normalization, dropout,\nand adaptive learning rate schedules come to mind—the core underlying architectures were\nclearly recognizable as scaled-up implementations of classic ideas. Despite thousands of pa-\npers proposing alternative ideas, models resembling classical convolutional neural networks\n(Chapter 7) retained state-of-the-art status in computer vision and models resembling Sepp\nHochreiter’s original design for the LSTM recurrent neural network (Section 10.1), dom-\ninated most applications in natural language processing. Arguably, to that point, the rapid\nemergence of deep learning appeared to be primarily attributable to shifts in the available\ncomputational resources (thanks to innovations in parallel computing with GPUs) and the\navailability of massive data resources (thanks to cheap storage and Internet services). While\nthese factors may indeed remain the primary drivers behind this technology’s increasing\npower we are also witnessing, at long last, a sea change in the landscape of dominant ar-\nchitectures.\nAt the present moment, the dominant models for nearly all natural language processing tasks\nare based on the Transformer architecture. Given any new task in natural language processing,\nthe default ﬁrst-pass approach is to grab a large Transformer-based pretrained model, (e.g.,\nBERT (Devlin et al., 2018), ELECTRA (Clark et al., 2020), RoBERTa (Liu et al., 2019), or\nLongformer (Beltagy et al., 2020)) adapting the output layers as necessary, and ﬁne-tuning\nthe model on the available data for the downstream task. If you have been paying attention to\nthe last few years of breathless news coverage centered on OpenAI’s large language models,\nthen you have been tracking a conversation centered on the GPT-2 and GPT-3 Transformer-\nbased models (Brown et al., 2020, Radford et al., 2019). Meanwhile, the vision Transformer\nhas emerged as a default model for diverse vision tasks, including image recognition, object\ndetection, semantic segmentation, and superresolution (Dosovitskiy et al., 2021, Liu et al.,\n2021). Transformers also showed up as competitive methods for speech recognition (Gulati\net al., 2020), reinforcement learning (Chen et al., 2021), and graph neural networks (Dwivedi\nand Bresson, 2020).\nThe core idea behind the Transformer model is the attention mechanism, an innovation that\nwas originally envisioned as an enhancement for encoder–decoder RNNs applied to sequence-\n422\n\n423\nAttention Mechanisms and Transformers\nto-sequence applications, such as machine translations (Bahdanau et al., 2014). You might\nrecall that in the ﬁrst sequence-to-sequence models for machine translation (Sutskever et al.,\n2014), the entire input was compressed by the encoder into a single ﬁxed-length vector to be\nfed into the decoder. The intuition behind attention is that rather than compressing the input,\nit might be better for the decoder to revisit the input sequence at every step. Moreover, rather\nthan always seeing the same representation of the input, one might imagine that the decoder\nshould selectively focus on particular parts of the input sequence at particular decoding steps.\nBahdanau’s attention mechanism provided a simple means by which the decoder could dy-\nnamically attend to diﬀerent parts of the input at each decoding step. The high-level idea is\nthat the encoder could produce a representation of length equal to the original input sequence.\nThen, at decoding time, the decoder can (via some control mechanism) receive as input a con-\ntext vector consisting of a weighted sum of the representations on the input at each time step.\nIntuitively, the weights determine the extent to which each step’s context “focuses” on each\ninput token, and the key is to make this process for assigning the weights diﬀerentiable so\nthat it can be learned along with all of the other neural network parameters.\nInitially, the idea was a remarkably successful enhancement to the recurrent neural networks\nthat already dominated machine translation applications. The models performed better than\nthe original encoder–decoder sequence-to-sequence architectures. Furthermore, researchers\nnoted that some nice qualitative insights sometimes emerged from inspecting the pattern of\nattention weights. In translation tasks, attention models often assigned high attention weights\nto cross-lingual synonyms when generating the corresponding words in the target language.\nFor example, when translating the sentence “my feet hurt” to “j’ai mal au pieds”, the neural\nnetwork might assign high attention weights to the representation of “feet” when generating\nthe corresponding French word “pieds”. These insights spurred claims that attention models\nconfer “interpretability” although what precisely the attention weights mean—i.e., how, if at\nall, they should be interpreted remains a hazy research topic.\nHowever, attention mechanisms soon emerged as more signiﬁcant concerns, beyond their\nusefulness as an enhancement for encoder–decoder recurrent neural networks and their puta-\ntive usefulness for picking out salient inputs. Vaswani et al. (2017) proposed the Transformer\narchitecture for machine translation, dispensing with recurrent connections altogether, and\ninstead relying on cleverly arranged attention mechanisms to capture all relationships among\ninput and output tokens. The architecture performed remarkably well, and by 2018 the Trans-\nformer began showing up in the majority of state-of-the-art natural language processing\nsystems. Moreover, at the same time, the dominant practice in natural language process-\ning became to pretrain large-scale models on enormous generic background corpora to op-\ntimize some self-supervised pretraining objective, and then to ﬁne-tune these models using\nthe available downstream data. The gap between Transformers and traditional architectures\ngrew especially wide when applied in this pretraining paradigm, and thus the ascendance\nof Transformers coincided with the ascendence of such large-scale pretrained models, now\nsometimes called foundation models (Bommasani et al., 2021).\nIn this chapter, we introduce attention models, starting with the most basic intuitions and\nthe simplest instantiations of the idea. We then work our way up to the Transformer archi-\n\n424\nAttention Mechanisms and Transformers\ntecture, the vision Transformer, and the landscape of modern Transformer-based pretrained\nmodels.\n11.1 Queries, Keys, and Values\nSo far all the networks we have reviewed crucially relied on the input being of a well-deﬁned\nsize. For instance, the images in ImageNet are of size 224×224 pixels and CNNs are specif-\nically tuned to this size. Even in natural language processing the input size for RNNs is well\ndeﬁned and ﬁxed. Variable size is addressed by sequentially processing one token at a time,\nor by specially designed convolution kernels (Kalchbrenner et al., 2014). This approach can\nlead to signiﬁcant problems when the input is truly of varying size with varying informa-\ntion content, such as in Section 10.7 in the transformation of text (Sutskever et al., 2014). In\nparticular, for long sequences it becomes quite diﬃcult to keep track of everything that has\nalready been generated or even viewed by the network. Even explicit tracking heuristics such\nas proposed by Yang et al. (2016) only oﬀer limited beneﬁt.\nCompare this to databases. In their simplest form they are collections of keys (k) and values\n(v). For instance, our database D might consist of tuples {(“Zhang”, “Aston”), (“Lipton”,\n“Zachary”), (“Li”, “Mu”), (“Smola”, “Alex”), (“Hu”, “Rachel”), (“Werness”, “Brent”)} with\nthe last name being the key and the ﬁrst name being the value. We can operate on D, for\ninstance with the exact query (q) for “Li” which would return the value “Mu”. If (“Li”, “Mu”)\nwas not a record in D, there would be no valid answer. If we also allowed for approximate\nmatches, we would retrieve (“Lipton”, “Zachary”) instead. This quite simple and trivial ex-\nample nonetheless teaches us a number of useful things:\n• We can design queries q that operate on (k,v) pairs in such a manner as to be valid regard-\nless of the database size.\n• The same query can receive diﬀerent answers, according to the contents of the database.\n• The “code” being executed for operating on a large state space (the database) can be quite\nsimple (e.g., exact match, approximate match, top-k).\n• There is no need to compress or simplify the database to make the operations eﬀective.\nClearly we would not have introduced a simple database here if it wasn’t for the purpose of\nexplaining deep learning. Indeed, this leads to one of the most exciting concepts introduced\nin deep learning in the past decade: the attention mechanism (Bahdanau et al., 2014). We will\ncover the speciﬁcs of its application to machine translation later. For now, simply consider\nthe following: denote by D def\n= {(k1, v1), . . . (km, vm)} a database of m tuples of keys and\nvalues. Moreover, denote by q a query. Then we can deﬁne the attention over D as\nAttention(q, D) def\n=\nm\n∑\ni=1\nα(q, ki)vi,\n(11.1.1)\n\n425\nQueries, Keys, and Values\nwhere α(q, ki) ∈R (i = 1, . . ., m) are scalar attention weights. The operation itself is typi-\ncally referred to as attention pooling. The name attention derives from the fact that the opera-\ntion pays particular attention to the terms for which the weight α is signiﬁcant (i.e., large). As\nsuch, the attention over D generates a linear combination of values contained in the database.\nIn fact, this contains the above example as a special case where all but one weight is zero. We\nhave a number of special cases:\n• The weights α(q, ki) are nonnegative. In this case the output of the attention mechanism\nis contained in the convex cone spanned by the values vi.\n• The weights α(q, ki) form a convex combination, i.e., ∑\ni α(q, ki) = 1 and α(q, ki) ≥0\nfor all i. This is the most common setting in deep learning.\n• Exactly one of the weights α(q, ki) is 1, while all others are 0. This is akin to a traditional\ndatabase query.\n• All weights are equal, i.e., α(q, ki) =\n1\nm for all i. This amounts to averaging across the\nentire database, also called average pooling in deep learning.\nA common strategy for ensuring that the weights sum up to 1 is to normalize them via\nα(q, ki) =\nα(q, ki)\n∑\njα(q, kj).\n(11.1.2)\nIn particular, to ensure that the weights are also nonnegative, one can resort to exponentiation.\nThis means that we can now pick any function a(q, k) and then apply the softmax operation\nused for multinomial models to it via\nα(q, ki) =\nexp(a(q, ki))\n∑\nj exp(a(q, kj)).\n(11.1.3)\nThis operation is readily available in all deep learning frameworks. It is diﬀerentiable and its\ngradient never vanishes, all of which are desirable properties in a model. Note though, the\nattention mechanism introduced above is not the only option. For instance, we can design a\nnon-diﬀerentiable attention model that can be trained using reinforcement learning methods\n(Mnih et al., 2014). As one would expect, training such a model is quite complex. Conse-\nquently the bulk of modern attention research follows the framework outlined in Fig. 11.1.1.\nWe thus focus our exposition on this family of diﬀerentiable mechanisms.\nWhat is quite remarkable is that the actual “code” for executing on the set of keys and values,\nnamely the query, can be quite concise, even though the space to operate on is signiﬁcant.\nThis is a desirable property for a network layer as it does not require too many parameters\nto learn. Just as convenient is the fact that attention can operate on arbitrarily large databases\nwithout the need to change the way the attention pooling operation is performed.\nimport torch\nfrom d2l import torch as d2l\n11.1.1 Visualization\n\n426\nAttention Mechanisms and Transformers\nt\nFig. 11.1.1\nThe attention mechanism computes a linear combination over values vi via attention\npooling, where weights are derived according to the compatibility between a query q and\nkeys ki.\nOne of the beneﬁts of the attention mechanism is that it can be quite intuitive, particularly\nwhen the weights are nonnegative and sum to 1. In this case we might interpret large weights\nas a way for the model to select components of relevance. While this is a good intuition, it is\nimportant to remember that it is just that, an intuition. Regardless, we may want to visualize\nits eﬀect on the given set of keys when applying a variety of diﬀerent queries. This function\nwill come in handy later.\nWe thus deﬁne the show_heatmaps function. Note that it does not take a matrix (of attention\nweights) as its input but rather a tensor with four axes, allowing for an array of diﬀerent queries\nand weights. Consequently the input matrices has the shape (number of rows for display,\nnumber of columns for display, number of queries, number of keys). This will come in handy\nlater on when we want to visualize the workings that are to design Transformers.\n#@save\ndef show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),\ncmap='Reds'):\n\"\"\"Show heatmaps of matrices.\"\"\"\nd2l.use_svg_display()\nnum_rows, num_cols, _, _ = matrices.shape\nfig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,\nsharex=True, sharey=True, squeeze=False)\nfor i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):\nfor j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\npcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\nif i == num_rows - 1:\nax.set_xlabel(xlabel)\nif j == 0:\nax.set_ylabel(ylabel)\nif titles:\nax.set_title(titles[j])\nfig.colorbar(pcm, ax=axes, shrink=0.6);\nAs a quick sanity check let’s visualize the identity matrix, representing a case where the\nattention weight is 1 only when the query and the key are the same.\n\n427\nQueries, Keys, and Values\n154\nattention_weights = torch.eye(10).reshape((1, 1, 10, 10))\nshow_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')\n11.1.2 Summary\nThe attention mechanism allows us to aggregate data from many (key, value) pairs. So far our\ndiscussion was quite abstract, simply describing a way to pool data. We have not explained yet\nwhere those mysterious queries, keys, and values might arise from. Some intuition might help\nhere: for instance, in a regression setting, the query might correspond to the location where\nthe regression should be carried out. The keys are the locations where past data was observed\nand the values are the (regression) values themselves. This is the so-called Nadaraya–Watson\nestimator (Nadaraya, 1964, Watson, 1964) that we will be studying in the next section.\nBy design, the attention mechanism provides a diﬀerentiable means of control by which a\nneural network can select elements from a set and to construct an associated weighted sum\nover representations.\n11.1.3 Exercises\n1. Suppose that you wanted to reimplement approximate (key, query) matches as used in\nclassical databases, which attention function would you pick?\n2. Suppose that the attention function is given by a(q, ki) = q⊤ki and that ki = vi for\ni = 1, . . ., m. Denote by p(ki; q) the probability distribution over keys when using the\nsoftmax normalization in (11.1.3). Prove that ∇q Attention(q, D) = Covp(ki;q)[ki].\n3. Design a diﬀerentiable search engine using the attention mechanism.\n4. Review the design of the Squeeze and Excitation Networks (Hu et al., 2018) and interpret\nthem through the lens of the attention mechanism.\nDiscussions154.\n\n428\nAttention Mechanisms and Transformers\n155\n11.2 Attention Pooling by Similarity\nNow that we have introduced the primary components of the attention mechanism, let’s use\nthem in a rather classical setting, namely regression and classiﬁcation via kernel density esti-\nmation (Nadaraya, 1964, Watson, 1964). This detour simply provides additional background:\nit is entirely optional and can be skipped if needed. At their core, Nadaraya–Watson estima-\ntors rely on some similarity kernel α(q, k) relating queries q to keys k. Some common kernels\nare\nα(q, k) = exp\n(\n−1\n2 ∥q −k∥2\n)\nGaussian;\nα(q, k) = 1 if ∥q −k∥≤1\nBoxcar;\nα(q, k) = max (0, 1 −∥q −k∥)\nEpanechikov.\n(11.2.1)\nThere are many more choices that we could pick. See a Wikipedia article155 for a more exten-\nsive review and how the choice of kernels is related to kernel density estimation, sometimes\nalso called Parzen Windows (Parzen, 1957). All of the kernels are heuristic and can be tuned.\nFor instance, we can adjust the width, not only on a global basis but even on a per-coordinate\nbasis. Regardless, all of them lead to the following equation for regression and classiﬁcation\nalike:\nf (q) =\n∑\ni\nvi\nα(q, ki)\n∑\nj α(q, kj).\n(11.2.2)\nIn the case of a (scalar) regression with observations (xi, yi) for features and labels respec-\ntively, vi = yi are scalars, ki = xi are vectors, and the query q denotes the new loca-\ntion where f should be evaluated. In the case of (multiclass) classiﬁcation, we use one-hot-\nencoding of yi to obtain vi. One of the convenient properties of this estimator is that it\nrequires no training. Even more so, if we suitably narrow the kernel with increasing amounts\nof data, the approach is consistent (Mack and Silverman, 1982), i.e., it will converge to some\nstatistically optimal solution. Let’s start by inspecting some kernels.\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\nd2l.use_svg_display()\n11.2.1 Kernels and Data\nAll the kernels α(k, q) deﬁned in this section are translation and rotation invariant; that is,\nif we shift and rotate k and q in the same manner, the value of α remains unchanged. For\n\n429\nAttention Pooling by Similarity\nsimplicity we thus pick scalar arguments k, q ∈R and pick the key k = 0 as the origin. This\nyields:\n# Define some kernels\ndef gaussian(x):\nreturn torch.exp(-x**2 / 2)\ndef boxcar(x):\nreturn torch.abs(x) < 1.0\ndef constant(x):\nreturn 1.0 + 0 * x\ndef epanechikov(x):\nreturn torch.max(1 - torch.abs(x), torch.zeros_like(x))\nfig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\nkernels = (gaussian, boxcar, constant, epanechikov)\nnames = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')\nx = torch.arange(-2.5, 2.5, 0.1)\nfor kernel, name, ax in zip(kernels, names, axes):\nax.plot(x.detach().numpy(), kernel(x).detach().numpy())\nax.set_xlabel(name)\nd2l.plt.show()\nDiﬀerent kernels correspond to diﬀerent notions of range and smoothness. For instance, the\nboxcar kernel only attends to observations within a distance of 1 (or some otherwise deﬁned\nhyperparameter) and does so indiscriminately.\nTo see Nadaraya–Watson estimation in action, let’s deﬁne some training data. In the following\nwe use the dependency\nyi = 2 sin(xi) + xi + ϵ,\n(11.2.3)\nwhere ϵ is drawn from a normal distribution with zero mean and unit variance. We draw 40\ntraining examples.\n\n430\nAttention Mechanisms and Transformers\ndef f(x):\nreturn 2 * torch.sin(x) + x\nn = 40\nx_train, _ = torch.sort(torch.rand(n) * 5)\ny_train = f(x_train) + torch.randn(n)\nx_val = torch.arange(0, 5, 0.1)\ny_val = f(x_val)\n11.2.2 Attention Pooling via Nadaraya–Watson Regression\nNow that we have data and kernels, all we need is a function that computes the kernel re-\ngression estimates. Note that we also want to obtain the relative kernel weights in order to\nperform some minor diagnostics. Hence we ﬁrst compute the kernel between all training\nfeatures (covariates) x_train and all validation features x_val. This yields a matrix, which\nwe subsequently normalize. When multiplied with the training labels y_train we obtain the\nestimates.\nRecall attention pooling in (11.1.1). Let each validation feature be a query, and each training\nfeature–label pair be a key–value pair. As a result, the normalized relative kernel weights\n(attention_w below) are the attention weights.\ndef nadaraya_watson(x_train, y_train, x_val, kernel):\ndists = x_train.reshape((-1, 1)) - x_val.reshape((1, -1))\n# Each column/row corresponds to each query/key\nk = kernel(dists).type(torch.float32)\n# Normalization over keys for each query\nattention_w = k / k.sum(0)\ny_hat = y_train@attention_w\nreturn y_hat, attention_w\nLet’s have a look at the kind of estimates that the diﬀerent kernels produce.\ndef plot(x_train, y_train, x_val, y_val, kernels, names, attention=False):\nfig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))\nfor kernel, name, ax in zip(kernels, names, axes):\ny_hat, attention_w = nadaraya_watson(x_train, y_train, x_val, kernel)\nif attention:\npcm = ax.imshow(attention_w.detach().numpy(), cmap='Reds')\nelse:\nax.plot(x_val, y_hat)\nax.plot(x_val, y_val, 'm--')\nax.plot(x_train, y_train, 'o', alpha=0.5);\nax.set_xlabel(name)\nif not attention:\nax.legend(['y_hat', 'y'])\nif attention:\nfig.colorbar(pcm, ax=axes, shrink=0.7)\n\n431\nAttention Pooling by Similarity\nplot(x_train, y_train, x_val, y_val, kernels, names)\nThe ﬁrst thing that stands out is that all three nontrivial kernels (Gaussian, Boxcar, and\nEpanechikov) produce fairly workable estimates that are not too far from the true function.\nOnly the constant kernel that leads to the trivial estimate f (x) = 1\nn\n∑\ni yi produces a rather\nunrealistic result. Let’s inspect the attention weighting a bit more closely:\nplot(x_train, y_train, x_val, y_val, kernels, names, attention=True)\nThe visualization clearly shows why the estimates for Gaussian, Boxcar, and Epanechikov\nare very similar: after all, they are derived from very similar attention weights, despite the\ndiﬀerent functional form of the kernel. This raises the question as to whether this is always\nthe case.\n11.2.3 Adapting Attention Pooling\nWe could replace the Gaussian kernel with one of a diﬀerent width. That is, we could use\nα(q, k) = exp\n(\n−\n1\n2σ2 ∥q −k∥2)\nwhere σ2 determines the width of the kernel. Let’s see\nwhether this aﬀects the outcomes.\nsigmas = (0.1, 0.2, 0.5, 1)\nnames = ['Sigma ' + str(sigma) for sigma in sigmas]\ndef gaussian_with_width(sigma):\nreturn (lambda x: torch.exp(-x**2 / (2*sigma**2)))\n(continues on next page)\n\n432\nAttention Mechanisms and Transformers\n(continued from previous page)\nkernels = [gaussian_with_width(sigma) for sigma in sigmas]\nplot(x_train, y_train, x_val, y_val, kernels, names)\nClearly, the narrower the kernel, the less smooth the estimate. At the same time, it adapts\nbetter to the local variations. Let’s look at the corresponding attention weights.\nplot(x_train, y_train, x_val, y_val, kernels, names, attention=True)\nAs we would expect, the narrower the kernel, the narrower the range of large attention\nweights. It is also clear that picking the same width might not be ideal. In fact, Silverman\n(1986) proposed a heuristic that depends on the local density. Many more such “tricks” have\nbeen proposed. For instance, Norelli et al. (2022) used a similar nearest-neighbor interpola-\ntion technique for designing cross-modal image and text representations.\nThe astute reader might wonder why we are providing this deep dive for a method that is over\nhalf a century old. First, it is one of the earliest precursors of modern attention mechanisms.\nSecond, it is great for visualization. Third, and just as importantly, it demonstrates the limits\nof hand-crafted attention mechanisms. A much better strategy is to learn the mechanism,\nby learning the representations for queries and keys. This is what we will embark on in the\nfollowing sections.\n11.2.4 Summary\nNadaraya–Watson kernel regression is an early precursor of the current attention mechanisms.\nIt can be used directly with little to no training or tuning, either for classiﬁcation or regression.\n\n433\nAttention Scoring Functions\n156\nThe attention weight is assigned according to the similarity (or distance) between query and\nkey, and according to how many similar observations are available.\n11.2.5 Exercises\n1. Parzen windows density estimates are given by ˆp(x) =\n1\nn\n∑\ni k(x, xi). Prove that for\nbinary classiﬁcation the function ˆp(x, y = 1) −ˆp(x, y = −1), as obtained by Parzen\nwindows is equivalent to Nadaraya–Watson classiﬁcation.\n2. Implement stochastic gradient descent to learn a good value for kernel widths in Nadaraya–\nWatson regression.\n1. What happens if you just use the above estimates to minimize ( f (xi) −yi)2 directly?\nHint: yi is part of the terms used to compute f .\n2. Remove (xi, yi) from the estimate for f (xi) and optimize over the kernel widths. Do\nyou still observe overﬁtting?\n3. Assume that all x lie on the unit sphere, i.e., all satisfy ∥x∥= 1. Can you simplify the\n∥x −xi∥2 term in the exponential? Hint: we will later see that this is very closely related\nto dot product attention.\n4. Recall that Mack and Silverman (1982) proved that Nadaraya–Watson estimation is con-\nsistent. How quickly should you reduce the scale for the attention mechanism as you get\nmore data? Provide some intuition for your answer. Does it depend on the dimensionality\nof the data? How?\nDiscussions156.\n11.3 Attention Scoring Functions\nIn Section 11.2, we used a number of diﬀerent distance-based kernels, including a Gaussian\nkernel to model interactions between queries and keys. As it turns out, distance functions are\nslightly more expensive to compute than dot products. As such, with the softmax operation\nto ensure nonnegative attention weights, much of the work has gone into attention scoring\nfunctions a in (11.1.3) and Fig. 11.3.1 that are simpler to compute.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n434\nAttention Mechanisms and Transformers\nt\nFig. 11.3.1\nComputing the output of attention pooling as a weighted average of values, where weights\nare computed with the attention scoring function a and the softmax operation.\n11.3.1 Dot Product Attention\nLet’s review the attention function (without exponentiation) from the Gaussian kernel for a\nmoment:\na(q, ki) = −1\n2 ∥q −ki∥2 = q⊤ki −1\n2 ∥ki∥2 −1\n2 ∥q∥2.\n(11.3.1)\nFirst, note that the ﬁnal term depends on q only. As such it is identical for all (q, ki) pairs.\nNormalizing the attention weights to 1, as is done in (11.1.3), ensures that this term disappears\nentirely. Second, note that both batch and layer normalization (to be discussed later) lead to\nactivations that have well-bounded, and often constant, norms ∥ki∥. This is the case, for\ninstance, whenever the keys ki were generated by a layer norm. As such, we can drop it from\nthe deﬁnition of a without any major change in the outcome.\nLast, we need to keep the order of magnitude of the arguments in the exponential function\nunder control. Assume that all the elements of the query q ∈Rd and the key ki ∈Rd are\nindependent and identically drawn random variables with zero mean and unit variance. The\ndot product between both vectors has zero mean and a variance of d. To ensure that the\nvariance of the dot product still remains 1 regardless of vector length, we use the scaled dot\nproduct attention scoring function. That is, we rescale the dot product by 1/\n√\nd. We thus arrive\nat the ﬁrst commonly used attention function that is used, e.g., in Transformers (Vaswani et\nal., 2017):\na(q, ki) = q⊤ki/\n√\nd.\n(11.3.2)\nNote that attention weights α still need normalizing. We can simplify this further via (11.1.3)\nby using the softmax operation:\nα(q, ki) = softmax(a(q, ki)) =\nexp(q⊤ki/\n√\nd)\n∑\nj=1 exp(q⊤kj/\n√\nd)\n.\n(11.3.3)\nAs it turns out, all popular attention mechanisms use the softmax, hence we will limit our-\nselves to that in the remainder of this chapter.\n\n435\nAttention Scoring Functions\n11.3.2 Convenience Functions\nWe need a few functions to make the attention mechanism eﬃcient to deploy. This includes\ntools for dealing with strings of variable lengths (common for natural language processing)\nand tools for eﬃcient evaluation on minibatches (batch matrix multiplication).\nMasked Softmax Operation\nOne of the most popular applications of the attention mechanism is to sequence models.\nHence we need to be able to deal with sequences of diﬀerent lengths. In some cases, such\nsequences may end up in the same minibatch, necessitating padding with dummy tokens\nfor shorter sequences (see Section 10.5 for an example). These special tokens do not carry\nmeaning. For instance, assume that we have the following three sentences:\nDive\ninto\nDeep\nLearning\nLearn to\ncode\n<blank>\nHello world <blank> <blank>\nSince we do not want blanks in our attention model we simply need to limit ∑n\ni=1 α(q, ki)vi\nto ∑l\ni=1 α(q, ki)vi for however long, l ≤n, the actual sentence is. Since it is such a common\nproblem, it has a name: the masked softmax operation.\nLet’s implement it. Actually, the implementation cheats ever so slightly by setting the values\nof vi, for i > l, to zero. Moreover, it sets the attention weights to a large negative number,\nsuch as −106, in order to make their contribution to gradients and values vanish in practice.\nThis is done since linear algebra kernels and operators are heavily optimized for GPUs and\nit is faster to be slightly wasteful in computation rather than to have code with conditional (if\nthen else) statements.\ndef masked_softmax(X, valid_lens):\n#@save\n\"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n# X: 3D tensor, valid_lens: 1D or 2D tensor\ndef _sequence_mask(X, valid_len, value=0):\nmaxlen = X.size(1)\nmask = torch.arange((maxlen), dtype=torch.float32,\ndevice=X.device)[None, :] < valid_len[:, None]\nX[~mask] = value\nreturn X\nif valid_lens is None:\nreturn nn.functional.softmax(X, dim=-1)\nelse:\nshape = X.shape\nif valid_lens.dim() == 1:\nvalid_lens = torch.repeat_interleave(valid_lens, shape[1])\nelse:\nvalid_lens = valid_lens.reshape(-1)\n# On the last axis, replace masked elements with a very large negative\n(continues on next page)\n\n436\nAttention Mechanisms and Transformers\n(continued from previous page)\n# value, whose exponentiation outputs 0\nX = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\nreturn nn.functional.softmax(X.reshape(shape), dim=-1)\nTo illustrate how this function works, consider a minibatch of two examples of size 2 × 4,\nwhere their valid lengths are 2 and 3, respectively. As a result of the masked softmax opera-\ntion, values beyond the valid lengths for each pair of vectors are all masked as zero.\nmasked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))\ntensor([[[0.5496, 0.4504, 0.0000, 0.0000],\n[0.5542, 0.4458, 0.0000, 0.0000]],\n[[0.3448, 0.2301, 0.4251, 0.0000],\n[0.3145, 0.3813, 0.3041, 0.0000]]])\nIf we need more ﬁne-grained control to specify the valid length for each of the two vectors of\nevery example, we simply use a two-dimensional tensor of valid lengths. This yields:\nmasked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n[0.2975, 0.4037, 0.2989, 0.0000]],\n[[0.5417, 0.4583, 0.0000, 0.0000],\n[0.1761, 0.2262, 0.3378, 0.2598]]])\nBatch Matrix Multiplication\nAnother commonly used operation is to multiply batches of matrices by one another. This\ncomes in handy when we have minibatches of queries, keys, and values. More speciﬁcally,\nassume that\nQ = [Q1, Q2, . . ., Qn] ∈Rn×a×b,\nK = [K1, K2, . . ., Kn] ∈Rn×b×c.\n(11.3.4)\nThen the batch matrix multiplication (BMM) computes the elementwise product\nBMM(Q, K) = [Q1K1, Q2K2, . . ., QnKn] ∈Rn×a×c.\n(11.3.5)\nLet’s see this in action in a deep learning framework.\nQ = torch.ones((2, 3, 4))\nK = torch.ones((2, 4, 6))\nd2l.check_shape(torch.bmm(Q, K), (2, 3, 6))\n\n437\nAttention Scoring Functions\n11.3.3 Scaled Dot Product Attention\nLet’s return to the dot product attention introduced in (11.3.2). In general, it requires that both\nthe query and the key have the same vector length, say d, even though this can be addressed\neasily by replacing q⊤k with q⊤Mk where M is a matrix suitably chosen for translating\nbetween both spaces. For now assume that the dimensions match.\nIn practice, we often think of minibatches for eﬃciency, such as computing attention for\nn queries and m key-value pairs, where queries and keys are of length d and values are of\nlength v. The scaled dot product attention of queries Q ∈Rn×d, keys K ∈Rm×d, and values\nV ∈Rm×v thus can be written as\nsoftmax\n(QK⊤\n√\nd\n)\nV ∈Rn×v.\n(11.3.6)\nNote that when applying this to a minibatch, we need the batch matrix multiplication intro-\nduced in (11.3.5). In the following implementation of the scaled dot product attention, we\nuse dropout for model regularization.\nclass DotProductAttention(nn.Module):\n#@save\n\"\"\"Scaled dot product attention.\"\"\"\ndef __init__(self, dropout):\nsuper().__init__()\nself.dropout = nn.Dropout(dropout)\n# Shape of queries: (batch_size, no. of queries, d)\n# Shape of keys: (batch_size, no. of key-value pairs, d)\n# Shape of values: (batch_size, no. of key-value pairs, value dimension)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\ndef forward(self, queries, keys, values, valid_lens=None):\nd = queries.shape[-1]\n# Swap the last two dimensions of keys with keys.transpose(1, 2)\nscores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\nself.attention_weights = masked_softmax(scores, valid_lens)\nreturn torch.bmm(self.dropout(self.attention_weights), values)\nTo illustrate how the DotProductAttention class works, we use the same keys, values,\nand valid lengths from the earlier toy example for additive attention. For the purpose of our\nexample we assume that we have a minibatch size of 2, a total of 10 keys and values, and that\nthe dimensionality of the values is 4. Lastly, we assume that the valid length per observation\nis 2 and 6 respectively. Given that, we expect the output to be a 2 × 1 × 4 tensor, i.e., one row\nper example of the minibatch.\nqueries = torch.normal(0, 1, (2, 1, 2))\nkeys = torch.normal(0, 1, (2, 10, 2))\nvalues = torch.normal(0, 1, (2, 10, 4))\nvalid_lens = torch.tensor([2, 6])\nattention = DotProductAttention(dropout=0.5)\nattention.eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\n\n438\nAttention Mechanisms and Transformers\nLet’s check whether the attention weights actually vanish for anything beyond the second and\nsixth column respectively (because of setting the valid length to 2 and 6).\nd2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\nxlabel='Keys', ylabel='Queries')\n11.3.4 Additive Attention\nWhen queries q and keys k are vectors of diﬀerent dimension, we can either use a matrix to\naddress the mismatch via q⊤Mk, or we can use additive attention as the scoring function.\nAnother beneﬁt is that, as its name indicates, the attention is additive. This can lead to some\nminor computational savings. Given a query q ∈Rq and a key k ∈Rk, the additive attention\nscoring function (Bahdanau et al., 2014) is given by\na(q, k) = w⊤\nv tanh(Wqq + Wkk) ∈R,\n(11.3.7)\nwhere Wq ∈Rh×q, Wk ∈Rh×k, and wv ∈Rh are the learnable parameters. This term\nis then fed into a softmax to ensure both nonnegativity and normalization. An equivalent\ninterpretation of (11.3.7) is that the query and key are concatenated and fed into an MLP\nwith a single hidden layer. Using tanh as the activation function and disabling bias terms, we\nimplement additive attention as follows:\nclass AdditiveAttention(nn.Module):\n#@save\n\"\"\"Additive attention.\"\"\"\ndef __init__(self, num_hiddens, dropout, **kwargs):\nsuper(AdditiveAttention, self).__init__(**kwargs)\nself.W_k = nn.LazyLinear(num_hiddens, bias=False)\nself.W_q = nn.LazyLinear(num_hiddens, bias=False)\nself.w_v = nn.LazyLinear(1, bias=False)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, queries, keys, values, valid_lens):\nqueries, keys = self.W_q(queries), self.W_k(keys)\n# After dimension expansion, shape of queries: (batch_size, no. of\n# queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n# key-value pairs, num_hiddens). Sum them up with broadcasting\nfeatures = queries.unsqueeze(2) + keys.unsqueeze(1)\nfeatures = torch.tanh(features)\n# There is only one output of self.w_v, so we remove the last\n# one-dimensional entry from the shape. Shape of scores: (batch_size,\n# no. of queries, no. of key-value pairs)\n(continues on next page)\n\n439\nAttention Scoring Functions\n157\n(continued from previous page)\nscores = self.w_v(features).squeeze(-1)\nself.attention_weights = masked_softmax(scores, valid_lens)\n# Shape of values: (batch_size, no. of key-value pairs, value\n# dimension)\nreturn torch.bmm(self.dropout(self.attention_weights), values)\nLet’s see how AdditiveAttention works. In our toy example we pick queries, keys and\nvalues of size (2, 1, 20), (2, 10, 2) and (2, 10, 4), respectively. This is identical to our choice\nfor DotProductAttention, except that now the queries are 20-dimensional. Likewise, we\npick (2, 6) as the valid lengths for the sequences in the minibatch.\nqueries = torch.normal(0, 1, (2, 1, 20))\nattention = AdditiveAttention(num_hiddens=8, dropout=0.1)\nattention.eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), (2, 1, 4))\nWhen reviewing the attention function we see a behavior that is qualitatively quite similar to\nthat of DotProductAttention. That is, only terms within the chosen valid length (2, 6) are\nnonzero.\nd2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),\nxlabel='Keys', ylabel='Queries')\n11.3.5 Summary\nIn this section we introduced the two key attention scoring functions: dot product and additive\nattention. They are eﬀective tools for aggregating across sequences of variable length. In\nparticular, the dot product attention is the mainstay of modern Transformer architectures.\nWhen queries and keys are vectors of diﬀerent lengths, we can use the additive attention\nscoring function instead. Optimizing these layers is one of the key areas of advance in recent\nyears. For instance, NVIDIA’s Transformer Library157 and Megatron (Shoeybi et al., 2019)\ncrucially rely on eﬃcient variants of the attention mechanism. We will dive into this in quite\na bit more detail as we review Transformers in later sections.\n11.3.6 Exercises\n\n440\nAttention Mechanisms and Transformers\n158\n1. Implement distance-based attention by modifying the DotProductAttention code. Note\nthat you only need the squared norms of the keys ∥ki∥2 for an eﬃcient implementation.\n2. Modify the dot product attention to allow for queries and keys of diﬀerent dimensionalities\nby employing a matrix to adjust dimensions.\n3. How does the computational cost scale with the dimensionality of the keys, queries, values,\nand their number? What about the memory bandwidth requirements?\nDiscussions158.\n11.4 The Bahdanau Attention Mechanism\nWhen we encountered machine translation in Section 10.7, we designed an encoder–decoder\narchitecture for sequence-to-sequence learning based on two RNNs (Sutskever et al., 2014).\nSpeciﬁcally, the RNN encoder transforms a variable-length sequence into a ﬁxed-shape con-\ntext variable. Then, the RNN decoder generates the output (target) sequence token by token\nbased on the generated tokens and the context variable.\nRecall Fig. 10.7.2 which we repeat (Fig. 11.4.1) with some additional detail. Conventionally,\nin an RNN all relevant information about a source sequence is translated into some internal\nﬁxed-dimensional state representation by the encoder. It is this very state that is used by the\ndecoder as the complete and exclusive source of information for generating the translated\nsequence. In other words, the sequence-to-sequence mechanism treats the intermediate state\nas a suﬃcient statistic of whatever string might have served as input.\nt\nFig. 11.4.1\nSequence-to-sequence model. The state, as generated by the encoder, is the only piece of\ninformation shared between the encoder and the decoder.\nWhile this is quite reasonable for short sequences, it is clear that it is infeasible for long\nones, such as a book chapter or even just a very long sentence. After all, before too long\nthere will simply not be enough “space” in the intermediate representation to store all that\nis important in the source sequence. Consequently the decoder will fail to translate long and\ncomplex sentences. One of the ﬁrst to encounter this was Graves (2013) who tried to design an\n\n441\nThe Bahdanau Attention Mechanism\nRNN to generate handwritten text. Since the source text has arbitrary length they designed a\ndiﬀerentiable attention model to align text characters with the much longer pen trace, where\nthe alignment moves only in one direction. This, in turn, draws on decoding algorithms in\nspeech recognition, e.g., hidden Markov models (Rabiner and Juang, 1993).\nInspired by the idea of learning to align, Bahdanau et al. (2014) proposed a diﬀerentiable\nattention model without the unidirectional alignment limitation. When predicting a token, if\nnot all the input tokens are relevant, the model aligns (or attends) only to parts of the input\nsequence that are deemed relevant to the current prediction. This is then used to update the\ncurrent state before generating the next token. While quite innocuous in its description, this\nBahdanau attention mechanism has arguably turned into one of the most inﬂuential ideas of\nthe past decade in deep learning, giving rise to Transformers (Vaswani et al., 2017) and many\nrelated new architectures.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n11.4.1 Model\nWe follow the notation introduced by the sequence-to-sequence architecture of Section 10.7,\nin particular (10.7.3). The key idea is that instead of keeping the state, i.e., the context variable\nc summarizing the source sentence, as ﬁxed, we dynamically update it, as a function of both\nthe original text (encoder hidden states ht) and the text that was already generated (decoder\nhidden states st′−1). This yields ct′, which is updated after any decoding time step t′. Suppose\nthat the input sequence is of lengthT. In this case the context variable is the output of attention\npooling:\nct′ =\nT\n∑\nt=1\nα(st′−1, ht)ht.\n(11.4.1)\nWe used st′−1 as the query, and ht as both the key and the value. Note that ct′ is then used\nto generate the state st′ and to generate a new token: see (10.7.3). In particular, the attention\nweight α is computed as in (11.3.3) using the additive attention scoring function deﬁned by\n(11.3.7). This RNN encoder–decoder architecture using attention is depicted in Fig. 11.4.2.\nNote that later this model was modiﬁed so as to include the already generated tokens in the\ndecoder as further context (i.e., the attention sum does not stop at T but rather it proceeds up\nto t′ −1). For instance, see Chan et al. (2015) for a description of this strategy, as applied to\nspeech recognition.\n11.4.2 Deﬁning the Decoder with Attention\nTo implement the RNN encoder–decoder with attention, we only need to redeﬁne the decoder\n(omitting the generated symbols from the attention function simpliﬁes the design). Let’s begin\n\n442\nAttention Mechanisms and Transformers\nt\nFig. 11.4.2\nLayers in an RNN encoder–decoder model with the Bahdanau attention mechanism.\nwith the base interface for decoders with attention by deﬁning the quite unsurprisingly named\nAttentionDecoder class.\nclass AttentionDecoder(d2l.Decoder):\n#@save\n\"\"\"The base attention-based decoder interface.\"\"\"\ndef __init__(self):\nsuper().__init__()\n@property\ndef attention_weights(self):\nraise NotImplementedError\nWe need to implement the RNN decoder in the Seq2SeqAttentionDecoder class. The state\nof the decoder is initialized with (i) the hidden states of the last layer of the encoder at all\ntime steps, used as keys and values for attention; (ii) the hidden state of the encoder at all\nlayers at the ﬁnal time step, which serves to initialize the hidden state of the decoder; and\n(iii) the valid length of the encoder, to exclude the padding tokens in attention pooling. At\neach decoding time step, the hidden state of the ﬁnal layer of the decoder, obtained at the\nprevious time step, is used as the query of the attention mechanism. Both the output of the\nattention mechanism and the input embedding are concatenated to serve as the input of the\nRNN decoder.\nclass Seq2SeqAttentionDecoder(AttentionDecoder):\ndef __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\ndropout=0):\nsuper().__init__()\nself.attention = d2l.AdditiveAttention(num_hiddens, dropout)\nself.embedding = nn.Embedding(vocab_size, embed_size)\nself.rnn = nn.GRU(\nembed_size + num_hiddens, num_hiddens, num_layers,\ndropout=dropout)\nself.dense = nn.LazyLinear(vocab_size)\nself.apply(d2l.init_seq2seq)\ndef init_state(self, enc_outputs, enc_valid_lens):\n# Shape of outputs: (num_steps, batch_size, num_hiddens).\n(continues on next page)\n\n443\nThe Bahdanau Attention Mechanism\n(continued from previous page)\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\noutputs, hidden_state = enc_outputs\nreturn (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\ndef forward(self, X, state):\n# Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\nenc_outputs, hidden_state, enc_valid_lens = state\n# Shape of the output X: (num_steps, batch_size, embed_size)\nX = self.embedding(X).permute(1, 0, 2)\noutputs, self._attention_weights = [], []\nfor x in X:\n# Shape of query: (batch_size, 1, num_hiddens)\nquery = torch.unsqueeze(hidden_state[-1], dim=1)\n# Shape of context: (batch_size, 1, num_hiddens)\ncontext = self.attention(\nquery, enc_outputs, enc_outputs, enc_valid_lens)\n# Concatenate on the feature dimension\nx = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n# Reshape x as (1, batch_size, embed_size + num_hiddens)\nout, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\noutputs.append(out)\nself._attention_weights.append(self.attention.attention_weights)\n# After fully connected layer transformation, shape of outputs:\n# (num_steps, batch_size, vocab_size)\noutputs = self.dense(torch.cat(outputs, dim=0))\nreturn outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\nenc_valid_lens]\n@property\ndef attention_weights(self):\nreturn self._attention_weights\nIn the following, we test the implemented decoder with attention using a minibatch of four\nsequences, each of which are seven time steps long.\nvocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\nbatch_size, num_steps = 4, 7\nencoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\ndecoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\nnum_layers)\nX = torch.zeros((batch_size, num_steps), dtype=torch.long)\nstate = decoder.init_state(encoder(X), None)\noutput, state = decoder(X, state)\nd2l.check_shape(output, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))\nd2l.check_shape(state[1][0], (batch_size, num_hiddens))\n11.4.3 Training\n\n444\nAttention Mechanisms and Transformers\nNow that we speciﬁed the new decoder we can proceed analogously to Section 10.7.6: specify\nthe hyperparameters, instantiate a regular encoder and a decoder with attention, and train this\nmodel for machine translation.\ndata = d2l.MTFraEng(batch_size=128)\nembed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\nencoder = d2l.Seq2SeqEncoder(\nlen(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqAttentionDecoder(\nlen(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\nlr=0.005)\ntrainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\nAfter the model is trained, we use it to translate a few English sentences into French and\ncompute their BLEU scores.\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\npreds, _ = model.predict_step(\ndata.build(engs, fras), d2l.try_gpu(), data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\ntranslation = []\nfor token in data.tgt_vocab.to_tokens(p):\nif token == '<eos>':\nbreak\ntranslation.append(token)\nprint(f'{en} => {translation}, bleu,'\nf'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')\ngo . => ['va', '!'], bleu,1.000\ni lost . => [\"j'ai\", 'perdu', '.'], bleu,1.000\nhe's calm . => ['je', 'suis', 'mouillé', '.'], bleu,0.000\ni'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000\nLet’s visualize the attention weights when translating the last English sentence. We see that\n\n445\nThe Bahdanau Attention Mechanism\n159\neach query assigns non-uniform weights over key–value pairs. It shows that at each decod-\ning step, diﬀerent parts of the input sequences are selectively aggregated in the attention\npooling.\n_, dec_attention_weights = model.predict_step(\ndata.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\nattention_weights = torch.cat(\n[step[0][0][0] for step in dec_attention_weights], 0)\nattention_weights = attention_weights.reshape((1, 1, -1, data.num_steps))\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(\nattention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),\nxlabel='Key positions', ylabel='Query positions')\n11.4.4 Summary\nWhen predicting a token, if not all the input tokens are relevant, the RNN encoder–decoder\nwith the Bahdanau attention mechanism selectively aggregates diﬀerent parts of the input\nsequence. This is achieved by treating the state (context variable) as an output of additive\nattention pooling. In the RNN encoder–decoder, the Bahdanau attention mechanism treats\nthe decoder hidden state at the previous time step as the query, and the encoder hidden states\nat all the time steps as both the keys and values.\n11.4.5 Exercises\n1. Replace GRU with LSTM in the experiment.\n2. Modify the experiment to replace the additive attention scoring function with the scaled\ndot-product. How does it inﬂuence the training eﬃciency?\nDiscussions159.\n\n446\nAttention Mechanisms and Transformers\n11.5 Multi-Head Attention\nIn practice, given the same set of queries, keys, and values we may want our model to combine\nknowledge from diﬀerent behaviors of the same attention mechanism, such as capturing de-\npendencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. Thus,\nit may be beneﬁcial to allow our attention mechanism to jointly use diﬀerent representation\nsubspaces of queries, keys, and values.\nTo this end, instead of performing a single attention pooling, queries, keys, and values can be\ntransformed with h independently learned linear projections. Then these h projected queries,\nkeys, and values are fed into attention pooling in parallel. In the end, h attention-pooling\noutputs are concatenated and transformed with another learned linear projection to produce\nthe ﬁnal output. This design is called multi-head attention, where each of the h attention\npooling outputs is a head (Vaswani et al., 2017). Using fully connected layers to perform\nlearnable linear transformations, Fig. 11.5.1 describes multi-head attention.\nt\nFig. 11.5.1\nMulti-head attention, where multiple heads are concatenated then linearly transformed.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n11.5.1 Model\nBefore providing the implementation of multi-head attention, let’s formalize this model math-\nematically. Given a query q ∈Rdq, a key k ∈Rdk , and a value v ∈Rdv, each attention head\nhi (i = 1, . . ., h) is computed as\nhi = f (W(q)\ni\nq, W(k)\ni\nk, W(v)\ni\nv) ∈Rpv,\n(11.5.1)\nwhere W(q)\ni\n∈Rpq×dq, W(k)\ni\n∈Rpk×dk , and W(v)\ni\n∈Rpv×dv are learnable parameters and\nf is attention pooling, such as additive attention and scaled dot product attention in Section\n\n447\nMulti-Head Attention\n11.3. The multi-head attention output is another linear transformation via learnable parame-\nters Wo ∈Rpo×hpv of the concatenation of h heads:\nWo\n\nh1\n...\nhh\n\n∈Rpo.\n(11.5.2)\nBased on this design, each head may attend to diﬀerent parts of the input. More sophisticated\nfunctions than the simple weighted average can be expressed.\n11.5.2 Implementation\nIn our implementation, we choose the scaled dot product attention for each head of the\nmulti-head attention. To avoid signiﬁcant growth of computational cost and parametriza-\ntion cost, we set pq = pk = pv = po/h. Note that h heads can be computed in parallel\nif we set the number of outputs of linear transformations for the query, key, and value to\npqh = pkh = pvh = po. In the following implementation, po is speciﬁed via the argument\nnum_hiddens.\nclass MultiHeadAttention(d2l.Module):\n#@save\n\"\"\"Multi-head attention.\"\"\"\ndef __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\nsuper().__init__()\nself.num_heads = num_heads\nself.attention = d2l.DotProductAttention(dropout)\nself.W_q = nn.LazyLinear(num_hiddens, bias=bias)\nself.W_k = nn.LazyLinear(num_hiddens, bias=bias)\nself.W_v = nn.LazyLinear(num_hiddens, bias=bias)\nself.W_o = nn.LazyLinear(num_hiddens, bias=bias)\ndef forward(self, queries, keys, values, valid_lens):\n# Shape of queries, keys, or values:\n# (batch_size, no. of queries or key-value pairs, num_hiddens)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n# After transposing, shape of output queries, keys, or values:\n# (batch_size * num_heads, no. of queries or key-value pairs,\n# num_hiddens / num_heads)\nqueries = self.transpose_qkv(self.W_q(queries))\nkeys = self.transpose_qkv(self.W_k(keys))\nvalues = self.transpose_qkv(self.W_v(values))\nif valid_lens is not None:\n# On axis 0, copy the first item (scalar or vector) for num_heads\n# times, then copy the next item, and so on\nvalid_lens = torch.repeat_interleave(\nvalid_lens, repeats=self.num_heads, dim=0)\n# Shape of output: (batch_size * num_heads, no. of queries,\n# num_hiddens / num_heads)\noutput = self.attention(queries, keys, values, valid_lens)\n# Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n(continues on next page)\n\n448\nAttention Mechanisms and Transformers\n(continued from previous page)\noutput_concat = self.transpose_output(output)\nreturn self.W_o(output_concat)\nTo allow for parallel computation of multiple heads, the above MultiHeadAttention class\nuses two transposition methods as deﬁned below. Speciﬁcally, the transpose_output method\nreverses the operation of the transpose_qkv method.\n@d2l.add_to_class(MultiHeadAttention)\n#@save\ndef transpose_qkv(self, X):\n\"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n# Shape of input X: (batch_size, no. of queries or key-value pairs,\n# num_hiddens). Shape of output X: (batch_size, no. of queries or\n# key-value pairs, num_heads, num_hiddens / num_heads)\nX = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n# Shape of output X: (batch_size, num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nX = X.permute(0, 2, 1, 3)\n# Shape of output: (batch_size * num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nreturn X.reshape(-1, X.shape[2], X.shape[3])\n@d2l.add_to_class(MultiHeadAttention)\n#@save\ndef transpose_output(self, X):\n\"\"\"Reverse the operation of transpose_qkv.\"\"\"\nX = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\nX = X.permute(0, 2, 1, 3)\nreturn X.reshape(X.shape[0], X.shape[1], -1)\nLet’s test our implemented MultiHeadAttention class using a toy example where keys and\nvalues are the same. As a result, the shape of the multi-head attention output is (batch_size,\nnum_queries, num_hiddens).\nnum_hiddens, num_heads = 100, 5\nattention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, num_kvpairs = 2, 4, 6\nvalid_lens = torch.tensor([3, 2])\nX = torch.ones((batch_size, num_queries, num_hiddens))\nY = torch.ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.5.3 Summary\nMulti-head attention combines knowledge of the same attention pooling via diﬀerent repre-\nsentation subspaces of queries, keys, and values. To compute multiple heads of multi-head\nattention in parallel, proper tensor manipulation is needed.\n11.5.4 Exercises\n\n449\nSelf-Attention and Positional Encoding\n160\n1. Visualize attention weights of multiple heads in this experiment.\n2. Suppose that we have a trained model based on multi-head attention and we want to prune\nless important attention heads to increase the prediction speed. How can we design ex-\nperiments to measure the importance of an attention head?\nDiscussions160.\n11.6 Self-Attention and Positional Encoding\nIn deep learning, we often use CNNs or RNNs to encode sequences. Now with attention\nmechanisms in mind, imagine feeding a sequence of tokens into an attention mechanism such\nthat at every step, each token has its own query, keys, and values. Here, when computing\nthe value of a token’s representation at the next layer, the token can attend (via its query\nvector) to any other’s token (matching based on their key vectors). Using the full set of query-\nkey compatibility scores, we can compute, for each token, a representation by building the\nappropriate weighted sum over the other tokens. Because every token is attending to each\nother token (unlike the case where decoder steps attend to encoder steps), such architectures\nare typically described as self-attention models (Lin et al., 2017, Vaswani et al., 2017), and\nelsewhere described as intra-attention model (Cheng et al., 2016, Parikh et al., 2016, Paulus\net al., 2017). In this section, we will discuss sequence encoding using self-attention, including\nusing additional information for the sequence order.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n11.6.1 Self-Attention\nGiven a sequence of input tokens x1, . . ., xn where any xi ∈Rd (1 ≤i ≤n), its self-attention\noutputs a sequence of the same length y1, . . ., yn, where\nyi = f (xi, (x1, x1), . . ., (xn, xn)) ∈Rd\n(11.6.1)\naccording to the deﬁnition of attention pooling in (11.1.1). Using multi-head attention, the\nfollowing code snippet computes the self-attention of a tensor with shape (batch size, number\nof time steps or sequence length in tokens, d). The output tensor has the same shape.\n\n450\nAttention Mechanisms and Transformers\nnum_hiddens, num_heads = 100, 5\nattention = d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, valid_lens = 2, 4, torch.tensor([3, 2])\nX = torch.ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.6.2 Comparing CNNs, RNNs, and Self-Attention\nLet’s compare architectures for mapping a sequence of n tokens to another one of equal\nlength, where each input or output token is represented by a d-dimensional vector. Speciﬁ-\ncally, we will consider CNNs, RNNs, and self-attention. We will compare their computational\ncomplexity, sequential operations, and maximum path lengths. Note that sequential opera-\ntions prevent parallel computation, while a shorter path between any combination of sequence\npositions makes it easier to learn long-range dependencies within the sequence (Hochreiter\net al., 2001).\nt\nFig. 11.6.1\nComparing CNN (padding tokens are omitted), RNN, and self-attention architectures.\nLet’s regard any text sequence as a “one-dimensional image”. Similarly, one-dimensional\nCNNs can process local features such as n-grams in text. Given a sequence of length n,\nconsider a convolutional layer whose kernel size is k, and whose numbers of input and output\nchannels are both d. The computational complexity of the convolutional layer is O(knd2).\nAs Fig. 11.6.1 shows, CNNs are hierarchical, so there are O(1) sequential operations and the\nmaximum path length is O(n/k). For example, x1 and x5 are within the receptive ﬁeld of a\ntwo-layer CNN with kernel size 3 in Fig. 11.6.1.\nWhen updating the hidden state of RNNs, multiplication of the d × d weight matrix and the\nd-dimensional hidden state has a computational complexity of O(d2). Since the sequence\nlength is n, the computational complexity of the recurrent layer is O(nd2). According to Fig.\n\n451\nSelf-Attention and Positional Encoding\n11.6.1, there are O(n) sequential operations that cannot be parallelized and the maximum\npath length is also O(n).\nIn self-attention, the queries, keys, and values are all n × d matrices. Consider the scaled dot\nproduct attention in (11.3.6), where an n × d matrix is multiplied by a d × n matrix, then\nthe output n × n matrix is multiplied by an n × d matrix. As a result, the self-attention has\na O(n2d) computational complexity. As we can see from Fig. 11.6.1, each token is directly\nconnected to any other token via self-attention. Therefore, computation can be parallel with\nO(1) sequential operations and the maximum path length is also O(1).\nAll in all, both CNNs and self-attention enjoy parallel computation and self-attention has the\nshortest maximum path length. However, the quadratic computational complexity with re-\nspect to the sequence length makes self-attention prohibitively slow for very long sequences.\n11.6.3 Positional Encoding\nUnlike RNNs, which recurrently process tokens of a sequence one-by-one, self-attention\nditches sequential operations in favor of parallel computation. Note that self-attention by\nitself does not preserve the order of the sequence. What do we do if it really matters that the\nmodel knows in which order the input sequence arrived?\nThe dominant approach for preserving information about the order of tokens is to represent\nthis to the model as an additional input associated with each token. These inputs are called\npositional encodings, and they can either be learned or ﬁxed a priori. We now describe a\nsimple scheme for ﬁxed positional encodings based on sine and cosine functions (Vaswani et\nal., 2017).\nSuppose that the input representation X ∈Rn×d contains the d-dimensional embeddings for\nn tokens of a sequence. The positional encoding outputs X+P using a positional embedding\nmatrix P ∈Rn×d of the same shape, whose element on the ith row and the (2j)th or the\n(2j + 1)th column is\npi,2j = sin\n(\ni\n100002j/d\n)\n,\npi,2j+1 = cos\n(\ni\n100002j/d\n)\n.\n(11.6.2)\nAt ﬁrst glance, this trigonometric function design looks weird. Before we give explanations\nof this design, let’s ﬁrst implement it in the following PositionalEncoding class.\nclass PositionalEncoding(nn.Module):\n#@save\n\"\"\"Positional encoding.\"\"\"\ndef __init__(self, num_hiddens, dropout, max_len=1000):\nsuper().__init__()\nself.dropout = nn.Dropout(dropout)\n# Create a long enough P\nself.P = torch.zeros((1, max_len, num_hiddens))\nX = torch.arange(max_len, dtype=torch.float32).reshape(\n(continues on next page)\n\n452\nAttention Mechanisms and Transformers\n(continued from previous page)\n-1, 1) / torch.pow(10000, torch.arange(\n0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\nself.P[:, :, 0::2] = torch.sin(X)\nself.P[:, :, 1::2] = torch.cos(X)\ndef forward(self, X):\nX = X + self.P[:, :X.shape[1], :].to(X.device)\nreturn self.dropout(X)\nIn the positional embedding matrix P, rows correspond to positions within a sequence and\ncolumns represent diﬀerent positional encoding dimensions. In the example below, we can see\nthat the 6th and the 7th columns of the positional embedding matrix have a higher frequency\nthan the 8th and the 9th columns. The oﬀset between the 6th and the 7th (same for the 8th and\nthe 9th) columns is due to the alternation of sine and cosine functions.\nencoding_dim, num_steps = 32, 60\npos_encoding = PositionalEncoding(encoding_dim, 0)\nX = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\nP = pos_encoding.P[:, :X.shape[1], :]\nd2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',\nfigsize=(6, 2.5), legend=[\"Col %d\" % d for d in torch.arange(6, 10)])\nAbsolute Positional Information\nTo see how the monotonically decreased frequency along the encoding dimension relates to\nabsolute positional information, let’s print out the binary representations of 0, 1, . . ., 7. As\nwe can see, the lowest bit, the second-lowest bit, and the third-lowest bit alternate on every\nnumber, every two numbers, and every four numbers, respectively.\nfor i in range(8):\nprint(f'{i} in binary is {i:>03b}')\n\n453\nSelf-Attention and Positional Encoding\n0 in binary is 000\n1 in binary is 001\n2 in binary is 010\n3 in binary is 011\n4 in binary is 100\n5 in binary is 101\n6 in binary is 110\n7 in binary is 111\nIn binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as\ndemonstrated in the heat map below, the positional encoding decreases frequencies along the\nencoding dimension by using trigonometric functions. Since the outputs are ﬂoat numbers,\nsuch continuous representations are more space-eﬃcient than binary representations.\nP = P[0, :, :].unsqueeze(0).unsqueeze(0)\nd2l.show_heatmaps(P, xlabel='Column (encoding dimension)',\nylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')\nRelative Positional Information\nBesides capturing absolute positional information, the above positional encoding also allows\na model to easily learn to attend by relative positions. This is because for any ﬁxed position\noﬀset δ, the positional encoding at position i + δ can be represented by a linear projection of\nthat at position i.\nThis projection can be explained mathematically. Denoting ωj = 1/100002j/d, any pair\nof (pi,2j, pi,2j+1) in (11.6.2) can be linearly projected to (pi+δ,2j, pi+δ,2j+1) for any ﬁxed\n\n454\nAttention Mechanisms and Transformers\n161\noﬀset δ:\n[ cos(δωj)\nsin(δωj)\n−sin(δωj)\ncos(δωj)\n] [ pi,2j\npi,2j+1\n]\n=\n[ cos(δωj) sin(iωj) + sin(δωj) cos(iωj)\n−sin(δωj) sin(iωj) + cos(δωj) cos(iωj)\n]\n=\n[sin ((i + δ)ωj\n)\ncos ((i + δ)ωj\n)\n]\n=\n[ pi+δ,2j\npi+δ,2j+1\n]\n,\n(11.6.3)\nwhere the 2 × 2 projection matrix does not depend on any position index i.\n11.6.4 Summary\nIn self-attention, the queries, keys, and values all come from the same place. Both CNNs and\nself-attention enjoy parallel computation and self-attention has the shortest maximum path\nlength. However, the quadratic computational complexity with respect to the sequence length\nmakes self-attention prohibitively slow for very long sequences. To use the sequence order\ninformation, we can inject absolute or relative positional information by adding positional\nencoding to the input representations.\n11.6.5 Exercises\n1. Suppose that we design a deep architecture to represent a sequence by stacking self-\nattention layers with positional encoding. What could the possible issues be?\n2. Can you design a learnable positional encoding method?\n3. Can we assign diﬀerent learned embeddings according to diﬀerent oﬀsets between queries\nand keys that are compared in self-attention? Hint: you may refer to relative position\nembeddings (Huang et al., 2018, Shaw et al., 2018).\nDiscussions161.\n11.7 The Transformer Architecture\nWe have compared CNNs, RNNs, and self-attention in Section 11.6.2. Notably, self-attention\nenjoys both parallel computation and the shortest maximum path length. Therefore, it is\nappealing to design deep architectures by using self-attention. Unlike earlier self-attention\nmodels that still rely on RNNs for input representations (Cheng et al., 2016, Lin et al., 2017,\nPaulus et al., 2017), the Transformer model is solely based on attention mechanisms with-\nout any convolutional or recurrent layer (Vaswani et al., 2017). Though originally proposed\n\n455\nThe Transformer Architecture\nfor sequence-to-sequence learning on text data, Transformers have been pervasive in a wide\nrange of modern deep learning applications, such as in areas to do with language, vision,\nspeech, and reinforcement learning.\nimport math\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n11.7.1 Model\nAs an instance of the encoder–decoder architecture, the overall architecture of the Trans-\nformer is presented in Fig. 11.7.1. As we can see, the Transformer is composed of an en-\ncoder and a decoder. In contrast to Bahdanau attention for sequence-to-sequence learning\nin Fig. 11.4.2, the input (source) and output (target) sequence embeddings are added with\npositional encoding before being fed into the encoder and the decoder that stack modules\nbased on self-attention.\nt\nFig. 11.7.1\nThe Transformer architecture.\n\n456\nAttention Mechanisms and Transformers\nNow we provide an overview of the Transformer architecture in Fig. 11.7.1. At a high level,\nthe Transformer encoder is a stack of multiple identical layers, where each layer has two\nsublayers (either is denoted as sublayer). The ﬁrst is a multi-head self-attention pooling and\nthe second is a positionwise feed-forward network. Speciﬁcally, in the encoder self-attention,\nqueries, keys, and values are all from the outputs of the previous encoder layer. Inspired by\nthe ResNet design of Section 8.6, a residual connection is employed around both sublayers.\nIn the Transformer, for any input x ∈Rd at any position of the sequence, we require that\nsublayer(x) ∈Rd so that the residual connection x + sublayer(x) ∈Rd is feasible. This\naddition from the residual connection is immediately followed by layer normalization (Ba et\nal., 2016). As a result, the Transformer encoder outputs a d-dimensional vector representation\nfor each position of the input sequence.\nThe Transformer decoder is also a stack of multiple identical layers with residual connec-\ntions and layer normalizations. As well as the two sublayers described in the encoder, the\ndecoder inserts a third sublayer, known as the encoder–decoder attention, between these two.\nIn the encoder–decoder attention, queries are from the outputs of the decoder’s self-attention\nsublayer, and the keys and values are from the Transformer encoder outputs. In the decoder\nself-attention, queries, keys, and values are all from the outputs of the previous decoder layer.\nHowever, each position in the decoder is allowed only to attend to all positions in the decoder\nup to that position. This masked attention preserves the autoregressive property, ensuring that\nthe prediction only depends on those output tokens that have been generated.\nWe have already described and implemented multi-head attention based on scaled dot prod-\nucts in Section 11.5 and positional encoding in Section 11.6.3. In the following, we will\nimplement the rest of the Transformer model.\n11.7.2 Positionwise Feed-Forward Networks\nThe positionwise feed-forward network transforms the representation at all the sequence po-\nsitions using the same MLP. This is why we call it positionwise. In the implementation below,\nthe input X with shape (batch size, number of time steps or sequence length in tokens, number\nof hidden units or feature dimension) will be transformed by a two-layer MLP into an output\ntensor of shape (batch size, number of time steps, ffn_num_outputs).\nclass PositionWiseFFN(nn.Module):\n#@save\n\"\"\"The positionwise feed-forward network.\"\"\"\ndef __init__(self, ffn_num_hiddens, ffn_num_outputs):\nsuper().__init__()\nself.dense1 = nn.LazyLinear(ffn_num_hiddens)\nself.relu = nn.ReLU()\nself.dense2 = nn.LazyLinear(ffn_num_outputs)\ndef forward(self, X):\nreturn self.dense2(self.relu(self.dense1(X)))\nThe following example shows that the innermost dimension of a tensor changes to the number\nof outputs in the positionwise feed-forward network. Since the same MLP transforms at\n\n457\nThe Transformer Architecture\nall the positions, when the inputs at all these positions are the same, their outputs are also\nidentical.\nffn = PositionWiseFFN(4, 8)\nffn.eval()\nffn(torch.ones((2, 3, 4)))[0]\ntensor([[-0.0458,\n0.2515, -0.4782, -0.0908, -0.3782,\n0.4959, -0.1183, -0.\n,→0020],\n[-0.0458,\n0.2515, -0.4782, -0.0908, -0.3782,\n0.4959, -0.1183, -0.\n,→0020],\n[-0.0458,\n0.2515, -0.4782, -0.0908, -0.3782,\n0.4959, -0.1183, -0.\n,→0020]],\ngrad_fn=<SelectBackward0>)\n11.7.3 Residual Connection and Layer Normalization\nNow let’s focus on the “add & norm” component in Fig. 11.7.1. As we described at the begin-\nning of this section, this is a residual connection immediately followed by layer normalization.\nBoth are key to eﬀective deep architectures.\nIn Section 8.5, we explained how batch normalization recenters and rescales across the ex-\namples within a minibatch. As discussed in Section 8.5.2, layer normalization is the same\nas batch normalization except that the former normalizes across the feature dimension, thus\nenjoying beneﬁts of scale independence and batch size independence. Despite its pervasive\napplications in computer vision, batch normalization is usually empirically less eﬀective than\nlayer normalization in natural language processing tasks, where the inputs are often variable-\nlength sequences.\nThe following code snippet compares the normalization across diﬀerent dimensions by layer\nnormalization and batch normalization.\nln = nn.LayerNorm(2)\nbn = nn.LazyBatchNorm1d()\nX = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n# Compute mean and variance from X in the training mode\nprint('layer norm:', ln(X), '\\nbatch norm:', bn(X))\nlayer norm: tensor([[-1.0000,\n1.0000],\n[-1.0000,\n1.0000]], grad_fn=<NativeLayerNormBackward0>)\nbatch norm: tensor([[-1.0000, -1.0000],\n[ 1.0000,\n1.0000]], grad_fn=<NativeBatchNormBackward0>)\nNow we can implement the AddNorm class using a residual connection followed by layer\nnormalization. Dropout is also applied for regularization.\n\n458\nAttention Mechanisms and Transformers\nclass AddNorm(nn.Module):\n#@save\n\"\"\"The residual connection followed by layer normalization.\"\"\"\ndef __init__(self, norm_shape, dropout):\nsuper().__init__()\nself.dropout = nn.Dropout(dropout)\nself.ln = nn.LayerNorm(norm_shape)\ndef forward(self, X, Y):\nreturn self.ln(self.dropout(Y) + X)\nThe residual connection requires that the two inputs are of the same shape so that the output\ntensor also has the same shape after the addition operation.\nadd_norm = AddNorm(4, 0.5)\nshape = (2, 3, 4)\nd2l.check_shape(add_norm(torch.ones(shape), torch.ones(shape)), shape)\n11.7.4 Encoder\nWith all the essential components to assemble the Transformer encoder, let’s start by imple-\nmenting a single layer within the encoder. The following TransformerEncoderBlock class\ncontains two sublayers: multi-head self-attention and positionwise feed-forward networks,\nwhere a residual connection followed by layer normalization is employed around both sub-\nlayers.\nclass TransformerEncoderBlock(nn.Module):\n#@save\n\"\"\"The Transformer encoder block.\"\"\"\ndef __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\nuse_bias=False):\nsuper().__init__()\nself.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself.addnorm1 = AddNorm(num_hiddens, dropout)\nself.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself.addnorm2 = AddNorm(num_hiddens, dropout)\ndef forward(self, X, valid_lens):\nY = self.addnorm1(X, self.attention(X, X, X, valid_lens))\nreturn self.addnorm2(Y, self.ffn(Y))\nAs we can see, no layer in the Transformer encoder changes the shape of its input.\nX = torch.ones((2, 100, 24))\nvalid_lens = torch.tensor([3, 2])\nencoder_blk = TransformerEncoderBlock(24, 48, 8, 0.5)\nencoder_blk.eval()\nd2l.check_shape(encoder_blk(X, valid_lens), X.shape)\nIn the following Transformer encoder implementation, we stack num_blks instances of the\n\n459\nThe Transformer Architecture\nabove TransformerEncoderBlock classes. Since we use the ﬁxed positional encoding whose\nvalues are always between −1 and 1, we multiply values of the learnable input embeddings\nby the square root of the embedding dimension to rescale before summing up the input em-\nbedding and the positional encoding.\nclass TransformerEncoder(d2l.Encoder):\n#@save\n\"\"\"The Transformer encoder.\"\"\"\ndef __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, use_bias=False):\nsuper().__init__()\nself.num_hiddens = num_hiddens\nself.embedding = nn.Embedding(vocab_size, num_hiddens)\nself.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\nself.blks = nn.Sequential()\nfor i in range(num_blks):\nself.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\ndef forward(self, X, valid_lens):\n# Since positional encoding values are between -1 and 1, the embedding\n# values are multiplied by the square root of the embedding dimension\n# to rescale before they are summed up\nX = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\nself.attention_weights = [None] * len(self.blks)\nfor i, blk in enumerate(self.blks):\nX = blk(X, valid_lens)\nself.attention_weights[\ni] = blk.attention.attention.attention_weights\nreturn X\nBelow we specify hyperparameters to create a two-layer Transformer encoder. The shape of\nthe Transformer encoder output is (batch size, number of time steps, num_hiddens).\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nd2l.check_shape(encoder(torch.ones((2, 100), dtype=torch.long), valid_lens),\n(2, 100, 24))\n11.7.5 Decoder\nAs shown in Fig. 11.7.1, the Transformer decoder is composed of multiple identical layers.\nEach layer is implemented in the following TransformerDecoderBlock class, which con-\ntains three sublayers: decoder self-attention, encoder–decoder attention, and positionwise\nfeed-forward networks. These sublayers employ a residual connection around them followed\nby layer normalization.\nAs we described earlier in this section, in the masked multi-head decoder self-attention (the\nﬁrst sublayer), queries, keys, and values all come from the outputs of the previous decoder\nlayer. When training sequence-to-sequence models, tokens at all the positions (time steps) of\nthe output sequence are known. However, during prediction the output sequence is generated\ntoken by token; thus, at any decoder time step only the generated tokens can be used in the\n\n460\nAttention Mechanisms and Transformers\ndecoder self-attention. To preserve autoregression in the decoder, its masked self-attention\nspeciﬁes dec_valid_lens so that any query only attends to all positions in the decoder up\nto the query position.\nclass TransformerDecoderBlock(nn.Module):\n# The i-th block in the Transformer decoder\ndef __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\nsuper().__init__()\nself.i = i\nself.attention1 = d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself.addnorm1 = AddNorm(num_hiddens, dropout)\nself.attention2 = d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself.addnorm2 = AddNorm(num_hiddens, dropout)\nself.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself.addnorm3 = AddNorm(num_hiddens, dropout)\ndef forward(self, X, state):\nenc_outputs, enc_valid_lens = state[0], state[1]\n# During training, all the tokens of any output sequence are processed\n# at the same time, so state[2][self.i] is None as initialized. When\n# decoding any output sequence token by token during prediction,\n# state[2][self.i] contains representations of the decoded output at\n# the i-th block up to the current time step\nif state[2][self.i] is None:\nkey_values = X\nelse:\nkey_values = torch.cat((state[2][self.i], X), dim=1)\nstate[2][self.i] = key_values\nif self.training:\nbatch_size, num_steps, _ = X.shape\n# Shape of dec_valid_lens: (batch_size, num_steps), where every\n# row is [1, 2, ..., num_steps]\ndec_valid_lens = torch.arange(\n1, num_steps + 1, device=X.device).repeat(batch_size, 1)\nelse:\ndec_valid_lens = None\n# Self-attention\nX2 = self.attention1(X, key_values, key_values, dec_valid_lens)\nY = self.addnorm1(X, X2)\n# Encoder-decoder attention. Shape of enc_outputs:\n# (batch_size, num_steps, num_hiddens)\nY2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\nZ = self.addnorm2(Y, Y2)\nreturn self.addnorm3(Z, self.ffn(Z)), state\nTo facilitate scaled dot product operations in the encoder–decoder attention and addition\noperations in the residual connections, the feature dimension (num_hiddens) of the decoder\nis the same as that of the encoder.\ndecoder_blk = TransformerDecoderBlock(24, 48, 8, 0.5, 0)\nX = torch.ones((2, 100, 24))\n(continues on next page)\n\n461\nThe Transformer Architecture\n(continued from previous page)\nstate = [encoder_blk(X, valid_lens), valid_lens, [None]]\nd2l.check_shape(decoder_blk(X, state)[0], X.shape)\nNow we construct the entire Transformer decoder composed of num_blks instances of Trans-\nformerDecoderBlock. In the end, a fully connected layer computes the prediction for all\nthe vocab_size possible output tokens. Both of the decoder self-attention weights and the\nencoder–decoder attention weights are stored for later visualization.\nclass TransformerDecoder(d2l.AttentionDecoder):\ndef __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout):\nsuper().__init__()\nself.num_hiddens = num_hiddens\nself.num_blks = num_blks\nself.embedding = nn.Embedding(vocab_size, num_hiddens)\nself.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\nself.blks = nn.Sequential()\nfor i in range(num_blks):\nself.blks.add_module(\"block\"+str(i), TransformerDecoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, i))\nself.dense = nn.LazyLinear(vocab_size)\ndef init_state(self, enc_outputs, enc_valid_lens):\nreturn [enc_outputs, enc_valid_lens, [None] * self.num_blks]\ndef forward(self, X, state):\nX = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\nself._attention_weights = [[None] * len(self.blks) for _ in range (2)]\nfor i, blk in enumerate(self.blks):\nX, state = blk(X, state)\n# Decoder self-attention weights\nself._attention_weights[0][\ni] = blk.attention1.attention.attention_weights\n# Encoder-decoder attention weights\nself._attention_weights[1][\ni] = blk.attention2.attention.attention_weights\nreturn self.dense(X), state\n@property\ndef attention_weights(self):\nreturn self._attention_weights\n11.7.6 Training\nLet’s instantiate an encoder–decoder model by following the Transformer architecture. Here\nwe specify that both the Transformer encoder and the Transformer decoder have two layers\nusing 4-head attention. As in Section 10.7.6, we train the Transformer model for sequence-\nto-sequence learning on the English–French machine translation dataset.\n\n462\nAttention Mechanisms and Transformers\ndata = d2l.MTFraEng(batch_size=128)\nnum_hiddens, num_blks, dropout = 256, 2, 0.2\nffn_num_hiddens, num_heads = 64, 4\nencoder = TransformerEncoder(\nlen(data.src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\ndecoder = TransformerDecoder(\nlen(data.tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nmodel = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\nlr=0.001)\ntrainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\ntrainer.fit(model, data)\nAfter training, we use the Transformer model to translate a few English sentences into French\nand compute their BLEU scores.\nengs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\nfras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\npreds, _ = model.predict_step(\ndata.build(engs, fras), d2l.try_gpu(), data.num_steps)\nfor en, fr, p in zip(engs, fras, preds):\ntranslation = []\nfor token in data.tgt_vocab.to_tokens(p):\nif token == '<eos>':\nbreak\ntranslation.append(token)\nprint(f'{en} => {translation}, bleu,'\nf'{d2l.bleu(\" \".join(translation), fr, k=2):.3f}')\ngo . => ['va', '!'], bleu,1.000\ni lost . => [\"j'ai\", 'perdu', '.'], bleu,1.000\nhe's calm . => ['il', 'est', 'mouillé', '.'], bleu,0.658\ni'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000\nLet’s visualize the Transformer attention weights when translating the ﬁnal English sentence\ninto French. The shape of the encoder self-attention weights is (number of encoder layers,\n\n463\nThe Transformer Architecture\nnumber of attention heads, num_steps or number of queries, num_steps or number of key-\nvalue pairs).\n_, dec_attention_weights = model.predict_step(\ndata.build([engs[-1]], [fras[-1]]), d2l.try_gpu(), data.num_steps, True)\nenc_attention_weights = torch.cat(model.encoder.attention_weights, 0)\nshape = (num_blks, num_heads, -1, data.num_steps)\nenc_attention_weights = enc_attention_weights.reshape(shape)\nd2l.check_shape(enc_attention_weights,\n(num_blks, num_heads, data.num_steps, data.num_steps))\nIn the encoder self-attention, both queries and keys come from the same input sequence.\nSince padding tokens do not carry meaning, with speciﬁed valid length of the input sequence\nno query attends to positions of padding tokens. In the following, two layers of multi-head\nattention weights are presented row by row. Each head independently attends based on a\nseparate representation subspace of queries, keys, and values.\nd2l.show_heatmaps(\nenc_attention_weights.cpu(), xlabel='Key positions',\nylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\nfigsize=(7, 3.5))\nTo visualize the decoder self-attention weights and the encoder–decoder attention weights,\nwe need more data manipulations. For example, we ﬁll the masked attention weights with\nzero. Note that the decoder self-attention weights and the encoder–decoder attention weights\nboth have the same queries: the beginning-of-sequence token followed by the output tokens\nand possibly end-of-sequence tokens.\ndec_attention_weights_2d = [head[0].tolist()\nfor step in dec_attention_weights\nfor attn in step for blk in attn for head in blk]\ndec_attention_weights_filled = torch.tensor(\n(continues on next page)\n\n464\nAttention Mechanisms and Transformers\n(continued from previous page)\npd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)\nshape = (-1, 2, num_blks, num_heads, data.num_steps)\ndec_attention_weights = dec_attention_weights_filled.reshape(shape)\ndec_self_attention_weights, dec_inter_attention_weights = \\\ndec_attention_weights.permute(1, 2, 3, 0, 4)\nd2l.check_shape(dec_self_attention_weights,\n(num_blks, num_heads, data.num_steps, data.num_steps))\nd2l.check_shape(dec_inter_attention_weights,\n(num_blks, num_heads, data.num_steps, data.num_steps))\nBecause of the autoregressive property of the decoder self-attention, no query attends to\nkey–value pairs after the query position.\nd2l.show_heatmaps(\ndec_self_attention_weights[:, :, :, :],\nxlabel='Key positions', ylabel='Query positions',\ntitles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))\nSimilar to the case in the encoder self-attention, via the speciﬁed valid length of the input\nsequence, no query from the output sequence attends to those padding tokens from the input\nsequence.\nd2l.show_heatmaps(\ndec_inter_attention_weights, xlabel='Key positions',\nylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\nfigsize=(7, 3.5))\nAlthough the Transformer architecture was originally proposed for sequence-to-sequence\nlearning, as we will discover later in the book, either the Transformer encoder or the Trans-\nformer decoder is often individually used for diﬀerent deep learning tasks.\n\n465\nThe Transformer Architecture\n162\n11.7.7 Summary\nThe Transformer is an instance of the encoder–decoder architecture, though either the en-\ncoder or the decoder can be used individually in practice. In the Transformer architecture,\nmulti-head self-attention is used for representing the input sequence and the output sequence,\nthough the decoder has to preserve the autoregressive property via a masked version. Both the\nresidual connections and the layer normalization in the Transformer are important for train-\ning a very deep model. The positionwise feed-forward network in the Transformer model\ntransforms the representation at all the sequence positions using the same MLP.\n11.7.8 Exercises\n1. Train a deeper Transformer in the experiments. How does it aﬀect the training speed and\nthe translation performance?\n2. Is it a good idea to replace scaled dot product attention with additive attention in the\nTransformer? Why?\n3. For language modeling, should we use the Transformer encoder, decoder, or both? How\nwould you design this method?\n4. What challenges can Transformers face if input sequences are very long? Why?\n5. How would you improve the computational and memory eﬃciency of Transformers? Hint:\nyou may refer to the survey paper by Tay et al. (2020).\nDiscussions162.\n\n466\nAttention Mechanisms and Transformers\n11.8 Transformers for Vision\nThe Transformer architecture was initially proposed for sequence-to-sequence learning, with\na focus on machine translation. Subsequently, Transformers emerged as the model of choice\nin various natural language processing tasks (Brown et al., 2020, Devlin et al., 2018, Radford\net al., 2018, Radford et al., 2019, Raﬀel et al., 2020). However, in the ﬁeld of computer\nvision the dominant architecture has remained the CNN (Chapter 8). Naturally, researchers\nstarted to wonder if it might be possible to do better by adapting Transformer models to image\ndata. This question sparked immense interest in the computer vision community. Recently,\nRamachandran et al. (2019) proposed a scheme for replacing convolution with self-attention.\nHowever, its use of specialized patterns in attention makes it hard to scale up models on\nhardware accelerators. Then, Cordonnier et al. (2020) theoretically proved that self-attention\ncan learn to behave similarly to convolution. Empirically, 2 × 2 patches were taken from\nimages as inputs, but the small patch size makes the model only applicable to image data\nwith low resolutions.\nWithout speciﬁc constraints on patch size, vision Transformers (ViTs) extract patches from\nimages and feed them into a Transformer encoder to obtain a global representation, which\nwill ﬁnally be transformed for classiﬁcation (Dosovitskiy et al., 2021). Notably, Transform-\ners show better scalability than CNNs: and when training larger models on larger datasets,\nvision Transformers outperform ResNets by a signiﬁcant margin. Similar to the landscape of\nnetwork architecture design in natural language processing, Transformers have also become\na game-changer in computer vision.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n11.8.1 Model\nFig. 11.8.1 depicts the model architecture of vision Transformers. This architecture consists\nof a stem that patchiﬁes images, a body based on the multilayer Transformer encoder, and a\nhead that transforms the global representation into the output label.\nConsider an input image with height h, width w, and c channels. Specifying the patch height\nand width both as p, the image is split into a sequence of m = hw/p2 patches, where each\npatch is ﬂattened to a vector of length cp2. In this way, image patches can be treated similarly\nto tokens in text sequences by Transformer encoders. A special “<cls>” (class) token and the\nm ﬂattened image patches are linearly projected into a sequence of m + 1 vectors, summed\nwith learnable positional embeddings. The multilayer Transformer encoder transforms m+1\ninput vectors into the same number of output vector representations of the same length. It\nworks exactly the same way as the original Transformer encoder in Fig. 11.7.1, only diﬀering\nin the position of normalization. Since the “<cls>” token attends to all the image patches via\n\n467\nTransformers for Vision\nt\nFig. 11.8.1\nThe vision Transformer architecture. In this example, an image is split into nine patches.\nA special “<cls>” token and the nine ﬂattened image patches are transformed via patch\nembedding and n Transformer encoder blocks into ten representations, respectively. The\n“<cls>” representation is further transformed into the output label.\nself-attention (see Fig. 11.6.1), its representation from the Transformer encoder output will\nbe further transformed into the output label.\n11.8.2 Patch Embedding\nTo implement a vision Transformer, let’s start with patch embedding in Fig. 11.8.1. Splitting\nan image into patches and linearly projecting these ﬂattened patches can be simpliﬁed as a\nsingle convolution operation, where both the kernel size and the stride size are set to the patch\nsize.\nclass PatchEmbedding(nn.Module):\ndef __init__(self, img_size=96, patch_size=16, num_hiddens=512):\n(continues on next page)\n\n468\nAttention Mechanisms and Transformers\n(continued from previous page)\nsuper().__init__()\ndef _make_tuple(x):\nif not isinstance(x, (list, tuple)):\nreturn (x, x)\nreturn x\nimg_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\nself.num_patches = (img_size[0] // patch_size[0]) * (\nimg_size[1] // patch_size[1])\nself.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\nstride=patch_size)\ndef forward(self, X):\n# Output shape: (batch size, no. of patches, no. of channels)\nreturn self.conv(X).flatten(2).transpose(1, 2)\nIn the following example, taking images with height and width of img_size as inputs, the\npatch embedding outputs (img_size//patch_size)**2 patches that are linearly projected\nto vectors of length num_hiddens.\nimg_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4\npatch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)\nX = torch.zeros(batch_size, 3, img_size, img_size)\nd2l.check_shape(patch_emb(X),\n(batch_size, (img_size//patch_size)**2, num_hiddens))\n11.8.3 Vision Transformer Encoder\nThe MLP of the vision Transformer encoder is slightly diﬀerent from the positionwise FFN\nof the original Transformer encoder (see Section 11.7.2). First, here the activation function\nuses the Gaussian error linear unit (GELU), which can be considered as a smoother version\nof the ReLU (Hendrycks and Gimpel, 2016). Second, dropout is applied to the output of\neach fully connected layer in the MLP for regularization.\nclass ViTMLP(nn.Module):\ndef __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\nsuper().__init__()\nself.dense1 = nn.LazyLinear(mlp_num_hiddens)\nself.gelu = nn.GELU()\nself.dropout1 = nn.Dropout(dropout)\nself.dense2 = nn.LazyLinear(mlp_num_outputs)\nself.dropout2 = nn.Dropout(dropout)\ndef forward(self, x):\nreturn self.dropout2(self.dense2(self.dropout1(self.gelu(\nself.dense1(x)))))\nThe vision Transformer encoder block implementation just follows the pre-normalization de-\nsign in Fig. 11.8.1, where normalization is applied right before multi-head attention or the\nMLP. In contrast to post-normalization (“add & norm” in Fig. 11.7.1), where normalization\n\n469\nTransformers for Vision\nis placed right after residual connections, pre-normalization leads to more eﬀective or ef-\nﬁcient training for Transformers (Baevski and Auli, 2018, Wang et al., 2019, Xiong et al.,\n2020).\nclass ViTBlock(nn.Module):\ndef __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\nnum_heads, dropout, use_bias=False):\nsuper().__init__()\nself.ln1 = nn.LayerNorm(norm_shape)\nself.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself.ln2 = nn.LayerNorm(norm_shape)\nself.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\ndef forward(self, X, valid_lens=None):\nX = X + self.attention(*([self.ln1(X)] * 3), valid_lens)\nreturn X + self.mlp(self.ln2(X))\nJust as in Section 11.7.4, no vision Transformer encoder block changes its input shape.\nX = torch.ones((2, 100, 24))\nencoder_blk = ViTBlock(24, 24, 48, 8, 0.5)\nencoder_blk.eval()\nd2l.check_shape(encoder_blk(X), X.shape)\n11.8.4 Putting It All Together\nThe forward pass of vision Transformers below is straightforward. First, input images are\nfed into an PatchEmbedding instance, whose output is concatenated with the “<cls>” token\nembedding. They are summed with learnable positional embeddings before dropout. Then\nthe output is fed into the Transformer encoder that stacks num_blks instances of the ViT-\nBlock class. Finally, the representation of the “<cls>” token is projected by the network\nhead.\nclass ViT(d2l.Classifier):\n\"\"\"Vision Transformer.\"\"\"\ndef __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens,\nnum_heads, num_blks, emb_dropout, blk_dropout, lr=0.1,\nuse_bias=False, num_classes=10):\nsuper().__init__()\nself.save_hyperparameters()\nself.patch_embedding = PatchEmbedding(\nimg_size, patch_size, num_hiddens)\nself.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\nnum_steps = self.patch_embedding.num_patches + 1\n# Add the cls token\n# Positional embeddings are learnable\nself.pos_embedding = nn.Parameter(\ntorch.randn(1, num_steps, num_hiddens))\nself.dropout = nn.Dropout(emb_dropout)\nself.blks = nn.Sequential()\n(continues on next page)\n\n470\nAttention Mechanisms and Transformers\n(continued from previous page)\nfor i in range(num_blks):\nself.blks.add_module(f\"{i}\", ViTBlock(\nnum_hiddens, num_hiddens, mlp_num_hiddens,\nnum_heads, blk_dropout, use_bias))\nself.head = nn.Sequential(nn.LayerNorm(num_hiddens),\nnn.Linear(num_hiddens, num_classes))\ndef forward(self, X):\nX = self.patch_embedding(X)\nX = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\nX = self.dropout(X + self.pos_embedding)\nfor blk in self.blks:\nX = blk(X)\nreturn self.head(X[:, 0])\n11.8.5 Training\nTraining a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were\ntrained in Chapter 8.\nimg_size, patch_size = 96, 16\nnum_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2\nemb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1\nmodel = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\nnum_blks, emb_dropout, blk_dropout, lr)\ntrainer = d2l.Trainer(max_epochs=10, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=128, resize=(img_size, img_size))\ntrainer.fit(model, data)\n11.8.6 Summary and Discussion\nYou may have noticed that for small datasets like Fashion-MNIST, our implemented vision\nTransformer does not outperform the ResNet in Section 8.6. Similar observations can be\nmade even on the ImageNet dataset (1.2 million images). This is because Transformers lack\n\n471\nLarge-Scale Pretraining with Transformers\n163\nthose useful principles in convolution, such as translation invariance and locality (Section\n7.1). However, the picture changes when training larger models on larger datasets (e.g., 300\nmillion images), where vision Transformers outperform ResNets by a large margin in image\nclassiﬁcation, demonstrating intrinsic superiority of Transformers in scalability (Dosovitskiy\net al., 2021). The introduction of vision Transformers has changed the landscape of network\ndesign for modeling image data. They were soon shown to be eﬀective on the ImageNet\ndataset with data-eﬃcient training strategies of DeiT (Touvron et al., 2021). However, the\nquadratic complexity of self-attention (Section 11.6) makes the Transformer architecture\nless suitable for higher-resolution images. Towards a general-purpose backbone network in\ncomputer vision, Swin Transformers addressed the quadratic computational complexity with\nrespect to image size (Section 11.6.2) and reinstated convolution-like priors, extending the\napplicability of Transformers to a range of computer vision tasks beyond image classiﬁcation\nwith state-of-the-art results (Liu et al., 2021).\n11.8.7 Exercises\n1. How does the value of img_size aﬀect training time?\n2. Instead of projecting the “<cls>” token representation to the output, how would you project\nthe averaged patch representations? Implement this change and see how it aﬀects the\naccuracy.\n3. Can you modify hyperparameters to improve the accuracy of the vision Transformer?\nDiscussions163.\n11.9 Large-Scale Pretraining with Transformers\nSo far in our image classiﬁcation and machine translation experiments, models have been\ntrained on datasets with input–output examples from scratch to perform speciﬁc tasks. For\nexample, a Transformer was trained with English–French pairs (Section 11.7) so that this\nmodel can translate input English text into French. As a result, each model becomes a speciﬁc\nexpert that is sensitive to even a slight shift in data distribution (Section 4.7). For better gen-\neralized models, or even more competent generalists that can perform multiple tasks with or\nwithout adaptation, pretraining models on large data has been increasingly common.\nGiven larger data for pretraining, the Transformer architecture performs better with an in-\ncreased model size and training compute, demonstrating superior scaling behavior. Specif-\nically, performance of Transformer-based language models scales as a power law with the\namount of model parameters, training tokens, and training compute (Kaplan et al., 2020).\nThe scalability of Transformers is also evidenced by the signiﬁcantly boosted performance\n\n472\nAttention Mechanisms and Transformers\nfrom larger vision Transformers trained on larger data (discussed in Section 11.8). More re-\ncent success stories include Gato, a generalist model that can play Atari, caption images, chat,\nand act as a robot (Reed et al., 2022). Gato is a single Transformer that scales well when\npretrained on diverse modalities, including text, images, joint torques, and button presses.\nNotably, all such multimodal data is serialized into a ﬂat sequence of tokens, which can be\nprocessed akin to text tokens (Section 11.7) or image patches (Section 11.8) by Transform-\ners.\nPrior to the compelling success of pretraining Transformers for multimodal data, Transform-\ners were extensively pretrained with a wealth of text. Originally proposed for machine trans-\nlation, the Transformer architecture in Fig. 11.7.1 consists of an encoder for representing\ninput sequences and a decoder for generating target sequences. Primarily, Transformers can\nbe used in three diﬀerent modes: encoder-only, encoder–decoder, and decoder-only. To con-\nclude this chapter, we will review these three modes and explain the scalability in pretraining\nTransformers.\n11.9.1 Encoder-Only\nWhen only the Transformer encoder is used, a sequence of input tokens is converted into the\nsame number of representations that can be further projected into output (e.g., classiﬁcation).\nA Transformer encoder consists of self-attention layers, where all input tokens attend to each\nother. For example, vision Transformers depicted in Fig. 11.8.1 are encoder-only, converting\na sequence of input image patches into the representation of a special “<cls>” token. Since\nthis representation depends on all input tokens, it is further projected into classiﬁcation labels.\nThis design was inspired by an earlier encoder-only Transformer pretrained on text: BERT\n(Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018).\nPretraining BERT\nBERT is pretrained on text sequences using masked language modeling: input text with ran-\ndomly masked tokens is fed into a Transformer encoder to predict the masked tokens. As il-\nlustrated in Fig. 11.9.1, an original text sequence “I”, “love”, “this”, “red”, “car” is prepended\nwith the “<cls>” token, and the “<mask>” token randomly replaces “love”; then the cross-\nentropy loss between the masked token “love” and its prediction is to be minimized during\npretraining. Note that there is no constraint in the attention pattern of Transformer encoders\n(right of Fig. 11.9.1) so all tokens can attend to each other. Thus, prediction of “love” depends\non input tokens before and after it in the sequence. This is why BERT is a “bidirectional en-\ncoder”. Without need for manual labeling, large-scale text data from books and Wikipedia\ncan be used for pretraining BERT.\n\n473\nLarge-Scale Pretraining with Transformers\nt\nFig. 11.9.1\nLeft: Pretraining BERT with masked language modeling. Prediction of the masked “love”\ntoken depends on all input tokens before and after “love”. Right: Attention pattern in the\nTransformer encoder. Each token along the vertical axis attends to all input tokens along\nthe horizontal axis.\nFine-Tuning BERT\nThe pretrained BERT can be ﬁne-tuned to downstream encoding tasks involving single text\nor text pairs. During ﬁne-tuning, additional layers can be added to BERT with randomized\nparameters: these parameters and those pretrained BERT parameters will be updated to ﬁt\ntraining data of downstream tasks.\nt\nFig. 11.9.2\nFine-tuning BERT for sentiment analysis.\nFig. 11.9.2 illustrates ﬁne-tuning of BERT for sentiment analysis. The Transformer encoder\nis a pretrained BERT, which takes a text sequence as input and feeds the “<cls>” represen-\ntation (global representation of the input) into an additional fully connected layer to predict\nthe sentiment. During ﬁne-tuning, the cross-entropy loss between the prediction and the la-\nbel on sentiment analysis data is minimized via gradient-based algorithms, where the addi-\ntional layer is trained from scratch while pretrained parameters of BERT are updated. BERT\ndoes more than sentiment analysis. The general language representations learned by the 350-\nmillion-parameter BERT from 250 billion training tokens advanced the state of the art for\nnatural language tasks such as single text classiﬁcation, text pair classiﬁcation or regression,\ntext tagging, and question answering.\n\n474\nAttention Mechanisms and Transformers\nYou may note that these downstream tasks include text pair understanding. BERT pretraining\nhas another loss for predicting whether one sentence immediately follows the other. How-\never, this loss was later found to be less useful when pretraining RoBERTa, a BERT variant\nof the same size, on 2000 billion tokens (Liu et al., 2019). Other derivatives of BERT im-\nproved model architectures or pretraining objectives, such as ALBERT (enforcing parame-\nter sharing) (Lan et al., 2019), SpanBERT (representing and predicting spans of text) (Joshi\net al., 2020), DistilBERT (lightweight via knowledge distillation) (Sanh et al., 2019), and\nELECTRA (replaced token detection) (Clark et al., 2020). Moreover, BERT inspired Trans-\nformer pretraining in computer vision, such as with vision Transformers (Dosovitskiy et al.,\n2021), Swin Transformers (Liu et al., 2021), and MAE (masked autoencoders) (He et al.,\n2022).\n11.9.2 Encoder–Decoder\nSince a Transformer encoder converts a sequence of input tokens into the same number of\noutput representations, the encoder-only mode cannot generate a sequence of arbitrary length\nas in machine translation. As originally proposed for machine translation, the Transformer\narchitecture can be outﬁtted with a decoder that autoregressively predicts the target sequence\nof arbitrary length, token by token, conditional on both encoder output and decoder output:\n(i) for conditioning on encoder output, encoder–decoder cross-attention (multi-head attention\nof decoder in Fig. 11.7.1) allows target tokens to attend to all input tokens; (ii) conditioning\non decoder output is achieved by a so-called causal attention (this name is common in the\nliterature but is misleading as it has little connection to the proper study of causality) pattern\n(masked multi-head attention of decoder in Fig. 11.7.1), where any target token can only\nattend to past and present tokens in the target sequence.\nTo pretrain encoder–decoder Transformers beyond human-labeled machine translation data,\nBART (Lewis et al., 2019) and T5 (Raﬀel et al., 2020) are two concurrently proposed\nencoder–decoder Transformers pretrained on large-scale text corpora. Both attempt to re-\nconstruct original text in their pretraining objectives, while the former emphasizes noising\ninput (e.g., masking, deletion, permutation, and rotation) and the latter highlights multitask\nuniﬁcation with comprehensive ablation studies.\nPretraining T5\nAs an example of the pretrained Transformer encoder–decoder, T5 (Text-to-Text Transfer\nTransformer) uniﬁes many tasks as the same text-to-text problem: for any task, the input\nof the encoder is a task description (e.g., “Summarize”, “:”) followed by task input (e.g., a\nsequence of tokens from an article), and the decoder predicts the task output (e.g., a sequence\nof tokens summarizing the input article). To perform as text-to-text, T5 is trained to generate\nsome target text conditional on input text.\nTo obtain input and output from any original text, T5 is pretrained to predict consecutive\nspans. Speciﬁcally, tokens from text are randomly replaced by special tokens where each\n\n475\nLarge-Scale Pretraining with Transformers\nt\nFig. 11.9.3\nLeft: Pretraining T5 by predicting consecutive spans. The original sentence is “I”, “love”,\n“this”, “red”, “car”, where “love” is replaced by a special “<X>” token, and consecutive\n“red”, “car” are replaced by a special “<Y>” token. The target sequence ends with a\nspecial “<Z>” token. Right: Attention pattern in the Transformer encoder–decoder. In the\nencoder self-attention (lower square), all input tokens attend to each other; In the\nencoder–decoder cross-attention (upper rectangle), each target token attends to all input\ntokens; In the decoder self-attention (upper triangle), each target token attends to present\nand past target tokens only (causal).\nconsecutive span is replaced by the same special token. Consider the example in Fig. 11.9.3,\nwhere the original text is “I”, “love”, “this”, “red”, “car”. Tokens “love”, “red”, “car” are ran-\ndomly replaced by special tokens. Since “red” and “car” are a consecutive span, they are\nreplaced by the same special token. As a result, the input sequence is “I”, “<X>”, “this”,\n“<Y>”, and the target sequence is “<X>”, “love”, “<Y>”, “red”, “car”, “<Z>”, where “<Z>”\nis another special token marking the end. As shown in Fig. 11.9.3, the decoder has a causal\nattention pattern to prevent itself from attending to future tokens during sequence predic-\ntion.\nIn T5, predicting consecutive span is also referred to as reconstructing corrupted text. With\nthis objective, T5 is pretrained with 1000 billion tokens from the C4 (Colossal Clean Crawled\nCorpus) data, which consists of clean English text from the web (Raﬀel et al., 2020).\nFine-Tuning T5\nSimilar to BERT, T5 needs to be ﬁne-tuned (updating T5 parameters) on task-speciﬁc train-\ning data to perform this task. Major diﬀerences from BERT ﬁne-tuning include: (i) T5 input\nincludes task descriptions; (ii) T5 can generate sequences with arbitrary length with its Trans-\nformer decoder; (iii) No additional layers are required.\nFig. 11.9.4 explains ﬁne-tuning T5 using text summarization as an example. In this down-\n\n476\nAttention Mechanisms and Transformers\nt\nFig. 11.9.4\nFine-tuning T5 for text summarization. Both the task description and article tokens are fed\ninto the Transformer encoder for predicting the summary.\nstream task, the task description tokens “Summarize”, “:” followed by the article tokens are\ninput to the encoder.\nAfter ﬁne-tuning, the 11-billion-parameter T5 (T5-11B) achieved state-of-the-art results on\nmultiple encoding (e.g., classiﬁcation) and generation (e.g., summarization) benchmarks.\nSince released, T5 has been extensively used in later research. For example, switch Trans-\nformers are designed based on T5 to activate a subset of the parameters for better compu-\ntational eﬃciency (Fedus et al., 2022). In a text-to-image model called Imagen, text is input\nto a frozen T5 encoder (T5-XXL) with 4.6 billion parameters (Saharia et al., 2022). The\nphotorealistic text-to-image examples in Fig. 11.9.5 suggest that the T5 encoder alone may\neﬀectively represent text even without ﬁne-tuning.\nt\nFig. 11.9.5\nText-to-image examples by the Imagen model, whose text encoder is from T5 (ﬁgures\ntaken from Saharia et al. (2022)).\n11.9.3 Decoder-Only\nWe have reviewed encoder-only and encoder–decoder Transformers. Alternatively, decoder-\nonly Transformers remove the entire encoder and the decoder sublayer with the encoder–\ndecoder cross-attention from the original encoder–decoder architecture depicted in Fig. 11.7.1.\n\n477\nLarge-Scale Pretraining with Transformers\nNowadays, decoder-only Transformers have been the de facto architecture in large-scale lan-\nguage modeling (Section 9.3), which leverages the world’s abundant unlabeled text corpora\nvia self-supervised learning.\nGPT and GPT-2\nUsing language modeling as the training objective, the GPT (generative pre-training) model\nchooses a Transformer decoder as its backbone (Radford et al., 2018).\nt\nFig. 11.9.6\nLeft: Pretraining GPT with language modeling. The target sequence is the input sequence\nshifted by one token. Both “<bos>” and “<eos>” are special tokens marking the beginning\nand end of sequences, respectively. Right: Attention pattern in the Transformer decoder.\nEach token along the vertical axis attends to only its past tokens along the horizontal axis\n(causal).\nFollowing the autoregressive language model training as described in Section 9.3.3, Fig.\n11.9.6 illustrates GPT pretraining with a Transformer encoder, where the target sequence\nis the input sequence shifted by one token. Note that the attention pattern in the Transformer\ndecoder enforces that each token can only attend to its past tokens (future tokens cannot be\nattended to because they have not yet been chosen).\nGPT has 100 million parameters and needs to be ﬁne-tuned for individual downstream tasks.\nA much larger Transformer-decoder language model, GPT-2, was introduced one year later\n(Radford et al., 2019). Compared with the original Transformer decoder in GPT, pre-normalization\n(discussed in Section 11.8.3) and improved initialization and weight-scaling were adopted in\nGPT-2. Pretrained on 40 GB of text, the 1.5-billion-parameter GPT-2 obtained the state-\nof-the-art results on language modeling benchmarks and promising results on multiple other\ntasks without updating the parameters or architecture.\n\n478\nAttention Mechanisms and Transformers\nGPT-3 and Beyond\nGPT-2 demonstrated potential of using the same language model for multiple tasks without\nupdating the model. This is more computationally eﬃcient than ﬁne-tuning, which requires\nmodel updates via gradient computation.\nt\nFig. 11.9.7\nZero-shot, one-shot, few-shot in-context learning with language models (Transformer\ndecoders). No parameter update is needed.\nBefore explaining the more computationally eﬃcient use of language models without pa-\nrameter update, recall Section 9.5 that a language model can be trained to generate a text\nsequence conditional on some preﬁx text sequence. Thus, a pretrained language model may\ngenerate the task output as a sequence without parameter update, conditional on an input\nsequence with the task description, task-speciﬁc input–output examples, and a prompt (task\ninput). This learning paradigm is called in-context learning (Brown et al., 2020), which can\nbe further categorized into zero-shot, one-shot, and few-shot, when there is no, one, and a few\ntask-speciﬁc input–output examples (Fig. 11.9.7).\nThese three settings were tested in GPT-3 (Brown et al., 2020), whose largest version uses\ndata and model size about two orders of magnitude larger than those in GPT-2. GPT-3 uses\nthe same Transformer decoder architecture as its direct predecessor GPT-2 except that atten-\ntion patterns (at the right in Fig. 11.9.6) are sparser at alternating layers. Pretrained with 300\nbillion tokens, GPT-3 performs better with larger model size, where few-shot performance\nincreases most rapidly (Fig. 11.9.8).\nThe subsequent GPT-4 model did not fully disclose technical details in its report (OpenAI,\n2023). By contrast with its predecessors, GPT-4 is a large-scale, multimodal model that can\ntake both text and images as input and generate text output.\n\n479\nLarge-Scale Pretraining with Transformers\nt\nFig. 11.9.8\nAggregate performance of GPT-3 for all 42 accuracy-denominated benchmarks (caption\nadapted and ﬁgure taken from Brown et al. (2020)).\n11.9.4 Scalability\nFig. 11.9.8 empirically demonstrates scalability of Transformers in the GPT-3 language\nmodel. For language modeling, more comprehensive empirical studies on the scalability of\nTransformers have led researchers to see promise in training larger Transformers with more\ndata and compute (Kaplan et al., 2020).\nt\nFig. 11.9.9\nTransformer language model performance improves smoothly as we increase the model\nsize, dataset size, and amount of compute used for training. For optimal performance all\nthree factors must be scaled up in tandem. Empirical performance has a power-law\nrelationship with each individual factor when not bottlenecked by the other two (caption\nadapted and ﬁgure taken from Kaplan et al. (2020)).\nAs shown in Fig. 11.9.9, power-law scaling can be observed in the performance with re-\nspect to the model size (number of parameters, excluding embedding layers), dataset size\n(number of training tokens), and amount of training compute (PetaFLOP/s-days, excluding\nembedding layers). In general, increasing all these three factors in tandem leads to better\n\n480\nAttention Mechanisms and Transformers\nperformance. However, how to increase them in tandem still remains a matter of debate\n(Hoﬀmann et al., 2022).\nt\nFig. 11.9.10\nTransformer language model training runs (ﬁgure taken from Kaplan et al. (2020)).\nAs well as increased performance, large models also enjoy better sample eﬃciency than small\nmodels. Fig. 11.9.10 shows that large models need fewer training samples (tokens processed)\nto perform at the same level achieved by small models, and performance is scaled smoothly\nwith compute.\nt\nFig. 11.9.11\nGPT-3 performance (cross-entropy validation loss) follows a power-law trend with the\namount of compute used for training. The power-law behavior observed in Kaplan et al.\n(2020) continues for an additional two orders of magnitude with only small deviations\nfrom the predicted curve. Embedding parameters are excluded from compute and\nparameter counts (caption adapted and ﬁgure taken from Brown et al. (2020)).\nThe empirical scaling behaviors in Kaplan et al. (2020) have been tested in subsequent large\nTransformer models. For example, GPT-3 supported this hypothesis with two more orders\nof magnitude in Fig. 11.9.11.\n11.9.5 Large Language Models\n\n481\nLarge-Scale Pretraining with Transformers\n164\nThe scalability of Transformers in the GPT series has inspired subsequent large language\nmodels. The GPT-2 Transformer decoder was used for training the 530-billion-parameter\nMegatron-Turing NLG (Smith et al., 2022) with 270 billion training tokens. Following the\nGPT-2 design, the 280-billion-parameter Gopher (Rae et al., 2021) pretrained with 300 bil-\nlion tokens, performed competitively across diverse tasks. Inheriting the same architecture\nand using the same compute budget of Gopher, Chinchilla (Hoﬀmann et al., 2022) is a sub-\nstantially smaller (70 billion parameters) model that trains for much longer (1.4 trillion train-\ning tokens), outperforming Gopher on many tasks and with more emphasis on the number\nof tokens than on the number of parameters. To continue the scaling line of language mod-\neling, PaLM (Pathway Language Model) (Chowdhery et al., 2022), a 540-billion-parameter\nTransformer decoder with modiﬁed designs pretrained on 780 billion tokens, outperformed\naverage human performance on the BIG-Bench benchmark (Srivastava et al., 2022). Its later\nversion, PaLM 2 (Anil et al., 2023), scaled data and model roughly 1:1 and improved multilin-\ngual and reasoning capabilities. Other large language models, such as Minerva (Lewkowycz\net al., 2022) that further trains a generalist (PaLM) and Galactica (Taylor et al., 2022) that is\nnot trained on a general corpus, have shown promising quantitative and scientiﬁc reasoning\ncapabilities.\nOpen-sourced releases, such as OPT (Open Pretrained Transformers) (Zhang et al., 2022),\nBLOOM (Scao et al., 2022), and FALCON (Penedo et al., 2023), democratized research\nand use of large language models. Focusing on computational eﬃciency at inference time, the\nopen-sourced Llama 1 (Touvron et al., 2023a) outperformed much larger models by training\non more tokens than had been typically used. The updated Llama 2 (Touvron et al., 2023b)\nfurther increased the pretraining corpus by 40%, leading to product models that may match\nthe performance of competitive close-sourced models.\nWei et al. (2022) discussed emergent abilities of large language models that are present in\nlarger models, but not in smaller models. However, simply increasing model size does not in-\nherently make models follow human instructions better. Sanh et al. (2021), Wei et al. (2021)\nhave found that ﬁne-tuning large language models on a range of datasets described via in-\nstructions can improve zero-shot performance on held-out tasks. Using reinforcement learning\nfrom human feedback, Ouyang et al. (2022) ﬁne-tuned GPT-3 to follow a diverse set of in-\nstructions. Following the resultant InstructGPT which aligns language models with human\nintent via ﬁne-tuning (Ouyang et al., 2022), ChatGPT164 can generate human-like responses\n(e.g., code debugging and creative writing) based on conversations with humans and can per-\nform many natural language processing tasks zero-shot (Qin et al., 2023). Bai et al. (2022)\nreplaced human inputs (e.g., human-labeled data) with model outputs to partially automate\nthe instruction tuning process, which is also known as reinforcement learning from AI feed-\nback.\nLarge language models oﬀer an exciting prospect of formulating text input to induce models\nto perform desired tasks via in-context learning, which is also known as prompting. Notably,\nchain-of-thought prompting (Wei et al., 2022), an in-context learning method with few-shot\n“question, intermediate reasoning steps, answer” demonstrations, elicits the complex rea-\nsoning capabilities of large language models in order to solve mathematical, commonsense,\n\n482\nAttention Mechanisms and Transformers\nand symbolic reasoning tasks. Sampling multiple reasoning paths (Wang et al., 2023), di-\nversifying few-shot demonstrations (Zhang et al., 2023), and reducing complex problems to\nsub-problems (Zhou et al., 2023) can all improve the reasoning accuracy. In fact, with sim-\nple prompts like “Let’s think step by step” just before each answer, large language models\ncan even perform zero-shot chain-of-thought reasoning with decent accuracy (Kojima et al.,\n2022). Even for multimodal inputs consisting of both text and images, language models can\nperform multimodal chain-of-thought reasoning with higher accuracy than using text input\nonly (Zhang et al., 2023).\n11.9.6 Summary and Discussion\nTransformers have been pretrained as encoder-only (e.g., BERT), encoder–decoder (e.g.,\nT5), and decoder-only (e.g., GPT series). Pretrained models may be adapted to perform dif-\nferent tasks with model update (e.g., ﬁne-tuning) or not (e.g., few-shot). Scalability of Trans-\nformers suggests that better performance beneﬁts from larger models, more training data, and\nmore training compute. Since Transformers were ﬁrst designed and pretrained for text data,\nthis section leans slightly towards natural language processing. Nonetheless, those models\ndiscussed above can be often found in more recent models across multiple modalities. For\nexample, (i) Chinchilla (Hoﬀmann et al., 2022) was further extended to Flamingo (Alayrac\net al., 2022), a visual language model for few-shot learning; (ii) GPT-2 (Radford et al., 2019)\nand the vision Transformer encode text and images in CLIP (Contrastive Language-Image\nPre-training) (Radford et al., 2021), whose image and text embeddings were later adopted in\nthe DALL-E 2 text-to-image system (Ramesh et al., 2022). Although there have been no sys-\ntematic studies on Transformer scalability in multimodal pretraining yet, an all-Transformer\ntext-to-image model called Parti (Yu et al., 2022) shows potential of scalability across modal-\nities: a larger Parti is more capable of high-ﬁdelity image generation and content-rich text\nunderstanding (Fig. 11.9.12).\nt\nFig. 11.9.12\nImage examples generated from the same text by the Parti model of increasing sizes\n(350M, 750M, 3B, 20B) (examples taken from Yu et al. (2022)).\n11.9.7 Exercises\n\n483\nLarge-Scale Pretraining with Transformers\n165\n1. Is it possible to ﬁne-tune T5 using a minibatch consisting of diﬀerent tasks? Why or why\nnot? How about for GPT-2?\n2. Given a powerful language model, what applications can you think of?\n3. Say that you are asked to ﬁne-tune a language model to perform text classiﬁcation by\nadding additional layers. Where will you add them? Why?\n4. Consider sequence-to-sequence problems (e.g., machine translation) where the input se-\nquence is always available throughout the target sequence prediction. What could be lim-\nitations of modeling with decoder-only Transformers? Why?\nDiscussions165.\n\n12\nOptimization Algorithms\nIf you read the book in sequence up to this point you already used a number of optimization\nalgorithms to train deep learning models. They were the tools that allowed us to continue up-\ndating model parameters and to minimize the value of the loss function, as evaluated on the\ntraining set. Indeed, anyone content with treating optimization as a black box device to min-\nimize objective functions in a simple setting might well content oneself with the knowledge\nthat there exists an array of incantations of such a procedure (with names such as “SGD” and\n“Adam”).\nTo do well, however, some deeper knowledge is required. Optimization algorithms are im-\nportant for deep learning. On the one hand, training a complex deep learning model can take\nhours, days, or even weeks. The performance of the optimization algorithm directly aﬀects\nthe model’s training eﬃciency. On the other hand, understanding the principles of diﬀer-\nent optimization algorithms and the role of their hyperparameters will enable us to tune the\nhyperparameters in a targeted manner to improve the performance of deep learning mod-\nels.\nIn this chapter, we explore common deep learning optimization algorithms in depth. Almost\nall optimization problems arising in deep learning are nonconvex. Nonetheless, the design and\nanalysis of algorithms in the context of convex problems have proven to be very instructive. It\nis for that reason that this chapter includes a primer on convex optimization and the proof for\na very simple stochastic gradient descent algorithm on a convex objective function.\n12.1 Optimization and Deep Learning\nIn this section, we will discuss the relationship between optimization and deep learning as well\nas the challenges of using optimization in deep learning. For a deep learning problem, we will\nusually deﬁne a loss function ﬁrst. Once we have the loss function, we can use an optimization\nalgorithm in attempt to minimize the loss. In optimization, a loss function is often referred\nto as the objective function of the optimization problem. By tradition and convention most\noptimization algorithms are concerned with minimization. If we ever need to maximize an\nobjective there is a simple solution: just ﬂip the sign on the objective.\n484\n\n485\nOptimization and Deep Learning\n12.1.1 Goal of Optimization\nAlthough optimization provides a way to minimize the loss function for deep learning, in\nessence, the goals of optimization and deep learning are fundamentally diﬀerent. The for-\nmer is primarily concerned with minimizing an objective whereas the latter is concerned\nwith ﬁnding a suitable model, given a ﬁnite amount of data. In Section 3.6, we discussed the\ndiﬀerence between these two goals in detail. For instance, training error and generalization\nerror generally diﬀer: since the objective function of the optimization algorithm is usually a\nloss function based on the training dataset, the goal of optimization is to reduce the training\nerror. However, the goal of deep learning (or more broadly, statistical inference) is to reduce\nthe generalization error. To accomplish the latter we need to pay attention to overﬁtting in\naddition to using the optimization algorithm to reduce the training error.\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch as d2l\nTo illustrate the aforementioned diﬀerent goals, let’s consider the empirical risk and the risk.\nAs described in Section 4.7.3, the empirical risk is an average loss on the training dataset\nwhile the risk is the expected loss on the entire population of data. Below we deﬁne two\nfunctions: the risk function f and the empirical risk function g. Suppose that we have only a\nﬁnite amount of training data. As a result, here g is less smooth than f.\ndef f(x):\nreturn x * torch.cos(np.pi * x)\ndef g(x):\nreturn f(x) + 0.2 * torch.cos(5 * np.pi * x)\nThe graph below illustrates that the minimum of the empirical risk on a training dataset may\nbe at a diﬀerent location from the minimum of the risk (generalization error).\ndef annotate(text, xy, xytext):\n#@save\nd2l.plt.gca().annotate(text, xy=xy, xytext=xytext,\narrowprops=dict(arrowstyle='->'))\nx = torch.arange(0.5, 1.5, 0.01)\nd2l.set_figsize((4.5, 2.5))\nd2l.plot(x, [f(x), g(x)], 'x', 'risk')\nannotate('min of\\nempirical risk', (1.0, -1.2), (0.5, -1.1))\nannotate('min of risk', (1.1, -1.05), (0.95, -0.5))\n12.1.2 Optimization Challenges in Deep Learning\nIn this chapter, we are going to focus speciﬁcally on the performance of optimization algo-\nrithms in minimizing the objective function, rather than a model’s generalization error. In\n\n486\nOptimization Algorithms\nSection 3.1 we distinguished between analytical solutions and numerical solutions in opti-\nmization problems. In deep learning, most objective functions are complicated and do not\nhave analytical solutions. Instead, we must use numerical optimization algorithms. The opti-\nmization algorithms in this chapter all fall into this category.\nThere are many challenges in deep learning optimization. Some of the most vexing ones are\nlocal minima, saddle points, and vanishing gradients. Let’s have a look at them.\nLocal Minima\nFor any objective function f (x), if the value of f (x) at x is smaller than the values of f (x)\nat any other points in the vicinity of x, then f (x) could be a local minimum. If the value of\nf (x) at x is the minimum of the objective function over the entire domain, then f (x) is the\nglobal minimum.\nFor example, given the function\nf (x) = x · cos(πx) for −1.0 ≤x ≤2.0,\n(12.1.1)\nwe can approximate the local minimum and global minimum of this function.\nx = torch.arange(-1.0, 2.0, 0.01)\nd2l.plot(x, [f(x), ], 'x', 'f(x)')\nannotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))\nannotate('global minimum', (1.1, -0.95), (0.6, 0.8))\nThe objective function of deep learning models usually has many local optima. When the nu-\nmerical solution of an optimization problem is near the local optimum, the numerical solution\nobtained by the ﬁnal iteration may only minimize the objective function locally, rather than\nglobally, as the gradient of the objective function’s solutions approaches or becomes zero.\nOnly some degree of noise might knock the parameter out of the local minimum. In fact,\nthis is one of the beneﬁcial properties of minibatch stochastic gradient descent where the\nnatural variation of gradients over minibatches is able to dislodge the parameters from local\nminima.\n\n487\nOptimization and Deep Learning\nSaddle Points\nBesides local minima, saddle points are another reason for gradients to vanish. A saddle point\nis any location where all gradients of a function vanish but which is neither a global nor a local\nminimum. Consider the function f (x) = x3. Its ﬁrst and second derivative vanish for x = 0.\nOptimization might stall at this point, even though it is not a minimum.\nx = torch.arange(-2.0, 2.0, 0.01)\nd2l.plot(x, [x**3], 'x', 'f(x)')\nannotate('saddle point', (0, -0.2), (-0.52, -5.0))\nSaddle points in higher dimensions are even more insidious, as the example below shows.\nConsider the function f (x, y) = x2 −y2. It has its saddle point at (0, 0). This is a maximum\nwith respect to y and a minimum with respect to x. Moreover, it looks like a saddle, which is\nwhere this mathematical property got its name.\nx, y = torch.meshgrid(\ntorch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))\nz = x**2 - y**2\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})\n(continues on next page)\n\n488\nOptimization Algorithms\n(continued from previous page)\nax.plot([0], [0], [0], 'rx')\nticks = [-1, 0, 1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y');\nWe assume that the input of a function is a k-dimensional vector and its output is a scalar,\nso its Hessian matrix will have k eigenvalues. The solution of the function could be a local\nminimum, a local maximum, or a saddle point at a position where the function gradient is\nzero:\n• When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are all\npositive, we have a local minimum for the function.\n• When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are all\nnegative, we have a local maximum for the function.\n• When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are\nnegative and positive, we have a saddle point for the function.\nFor high-dimensional problems the likelihood that at least some of the eigenvalues are negative\nis quite high. This makes saddle points more likely than local minima. We will discuss some\nexceptions to this situation in the next section when introducing convexity. In short, convex\nfunctions are those where the eigenvalues of the Hessian are never negative. Sadly, though,\nmost deep learning problems do not fall into this category. Nonetheless it is a great tool to\nstudy optimization algorithms.\nVanishing Gradients\nProbably the most insidious problem to encounter is the vanishing gradient. Recall our commonly-\nused activation functions and their derivatives in Section 5.1.2. For instance, assume that we\nwant to minimize the function f (x) = tanh(x) and we happen to get started at x = 4. As\nwe can see, the gradient of f is close to nil. More speciﬁcally, f ′(x) = 1 −tanh2(x) and\n\n489\nOptimization and Deep Learning\nthus f ′(4) = 0.0013. Consequently, optimization will get stuck for a long time before we\nmake progress. This turns out to be one of the reasons that training deep learning models was\nquite tricky prior to the introduction of the ReLU activation function.\nx = torch.arange(-2.0, 5.0, 0.01)\nd2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')\nannotate('vanishing gradient', (4, 1), (2, 0.0))\nAs we saw, optimization for deep learning is full of challenges. Fortunately there exists a\nrobust range of algorithms that perform well and that are easy to use even for beginners. Fur-\nthermore, it is not really necessary to ﬁnd the best solution. Local optima or even approximate\nsolutions thereof are still very useful.\n12.1.3 Summary\n• Minimizing the training error does not guarantee that we ﬁnd the best set of parameters to\nminimize the generalization error.\n• The optimization problems may have many local minima.\n• The problem may have even more saddle points, as generally the problems are not convex.\n• Vanishing gradients can cause optimization to stall. Often a reparametrization of the prob-\nlem helps. Good initialization of the parameters can be beneﬁcial, too.\n12.1.4 Exercises\n1. Consider a simple MLP with a single hidden layer of, say, d dimensions in the hidden\nlayer and a single output. Show that for any local minimum there are at least d! equivalent\nsolutions that behave identically.\n2. Assume that we have a symmetric random matrix M where the entries Mij = Mji are\neach drawn from some probability distribution pij. Furthermore assume that pij(x) =\npij(−x), i.e., that the distribution is symmetric (see e.g., Wigner (1958) for details).\n\n490\nOptimization Algorithms\n166\n1. Prove that the distribution over eigenvalues is also symmetric. That is, for any eigenvec-\ntor v the probability that the associated eigenvalue λ satisﬁes P(λ > 0) = P(λ < 0).\n2. Why does the above not imply P(λ > 0) = 0.5?\n3. What other challenges involved in deep learning optimization can you think of?\n4. Assume that you want to balance a (real) ball on a (real) saddle.\n1. Why is this hard?\n2. Can you exploit this eﬀect also for optimization algorithms?\nDiscussions166.\n12.2 Convexity\nConvexity plays a vital role in the design of optimization algorithms. This is largely due to\nthe fact that it is much easier to analyze and test algorithms in such a context. In other words,\nif the algorithm performs poorly even in the convex setting, typically we should not hope\nto see great results otherwise. Furthermore, even though the optimization problems in deep\nlearning are generally nonconvex, they often exhibit some properties of convex ones near\nlocal minima. This can lead to exciting new optimization variants such as (Izmailov et al.,\n2018).\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch as d2l\n12.2.1 Deﬁnitions\nBefore convex analysis, we need to deﬁne convex sets and convex functions. They lead to\nmathematical tools that are commonly applied to machine learning.\nConvex Sets\nSets are the basis of convexity. Simply put, a set X in a vector space is convex if for any\na, b ∈X the line segment connecting a and b is also in X. In mathematical terms this means\nthat for all λ ∈[0, 1] we have\nλa + (1 −λ)b ∈X whenever a, b ∈X.\n(12.2.1)\n\n491\nConvexity\nThis sounds a bit abstract. Consider Fig. 12.2.1. The ﬁrst set is not convex since there exist\nline segments that are not contained in it. The other two sets suﬀer no such problem.\nt\nFig. 12.2.1\nThe ﬁrst set is nonconvex and the other two are convex.\nDeﬁnitions on their own are not particularly useful unless you can do something with them.\nIn this case we can look at intersections as shown in Fig. 12.2.2. Assume that X and Y are\nconvex sets. Then X ∩Y is also convex. To see this, consider any a, b ∈X ∩Y. Since X and\nY are convex, the line segments connecting a and b are contained in both X and Y. Given\nthat, they also need to be contained in X ∩Y, thus proving our theorem.\nt\nFig. 12.2.2\nThe intersection between two convex sets is convex.\nWe can strengthen this result with little eﬀort: given convex sets Xi, their intersection ∩iXi\nis convex. To see that the converse is not true, consider two disjoint sets X ∩Y = ∅. Now\npick a ∈X and b ∈Y. The line segment in Fig. 12.2.3 connecting a and b needs to contain\nsome part that is neither in X nor in Y, since we assumed that X ∩Y = ∅. Hence the line\nsegment is not in X ∪Y either, thus proving that in general unions of convex sets need not\nbe convex.\nt\nFig. 12.2.3\nThe union of two convex sets need not be convex.\nTypically the problems in deep learning are deﬁned on convex sets. For instance, Rd, the set\nof d-dimensional vectors of real numbers, is a convex set (after all, the line between any two\n\n492\nOptimization Algorithms\npoints in Rd remains in Rd). In some cases we work with variables of bounded length, such\nas balls of radius r as deﬁned by {x|x ∈Rd and ∥x∥≤r}.\nConvex Functions\nNow that we have convex sets we can introduce convex functions f . Given a convex set X, a\nfunction f : X →R is convex if for all x, x′ ∈X and for all λ ∈[0, 1] we have\nλ f (x) + (1 −λ) f (x′) ≥f (λx + (1 −λ)x′).\n(12.2.2)\nTo illustrate this let’s plot a few functions and check which ones satisfy the requirement.\nBelow we deﬁne a few functions, both convex and nonconvex.\nf = lambda x: 0.5 * x**2\n# Convex\ng = lambda x: torch.cos(np.pi * x)\n# Nonconvex\nh = lambda x: torch.exp(0.5 * x)\n# Convex\nx, segment = torch.arange(-2, 2, 0.01), torch.tensor([-1.5, 1])\nd2l.use_svg_display()\n_, axes = d2l.plt.subplots(1, 3, figsize=(9, 3))\nfor ax, func in zip(axes, [f, g, h]):\nd2l.plot([x, segment], [func(x), func(segment)], axes=ax)\nAs expected, the cosine function is nonconvex, whereas the parabola and the exponential\nfunction are. Note that the requirement that X is a convex set is necessary for the condition\nto make sense. Otherwise the outcome of f (λx+(1−λ)x′) might not be well deﬁned.\nJensen’s Inequality\nGiven a convex function f , one of the most useful mathematical tools is Jensen’s inequality.\nIt amounts to a generalization of the deﬁnition of convexity:\n∑\ni\nαi f (xi) ≥f\n(∑\ni\nαixi\n)\nand EX[ f (X)] ≥f (EX[X]),\n(12.2.3)\n\n493\nConvexity\nwhere αi are nonnegative real numbers such that ∑\ni αi = 1 and X is a random variable. In\nother words, the expectation of a convex function is no less than the convex function of an\nexpectation, where the latter is usually a simpler expression. To prove the ﬁrst inequality we\nrepeatedly apply the deﬁnition of convexity to one term in the sum at a time.\nOne of the common applications of Jensen’s inequality is to bound a more complicated ex-\npression by a simpler one. For example, its application can be with regard to the log-likelihood\nof partially observed random variables. That is, we use\nEY∼P(Y)[−log P(X | Y)] ≥−log P(X),\n(12.2.4)\nsince\n∫\nP(Y)P(X | Y)dY = P(X). This can be used in variational methods. Here Y is typi-\ncally the unobserved random variable, P(Y) is the best guess of how it might be distributed,\nand P(X) is the distribution with Y integrated out. For instance, in clustering Y might be the\ncluster labels and P(X | Y) is the generative model when applying cluster labels.\n12.2.2 Properties\nConvex functions have many useful properties. We describe a few commonly-used ones be-\nlow.\nLocal Minima Are Global Minima\nFirst and foremost, the local minima of convex functions are also the global minima. We can\nprove it by contradiction as follows.\nConsider a convex function f deﬁned on a convex set X. Suppose that x∗∈X is a local\nminimum: there exists a small positive value p so that for x ∈X that satisﬁes 0 < |x−x∗| ≤p\nwe have f (x∗) < f (x).\nAssume that the local minimum x∗is not the global minimum of f : there exists x′ ∈X\nfor which f (x′) < f (x∗). There also exists λ ∈[0, 1) such as λ = 1 −\np\n|x∗−x′| so that\n0 < |λx∗+ (1 −λ)x′ −x∗| ≤p.\nHowever, according to the deﬁnition of convex functions, we have\nf (λx∗+ (1 −λ)x′) ≤λ f (x∗) + (1 −λ) f (x′)\n< λ f (x∗) + (1 −λ) f (x∗)\n= f (x∗),\n(12.2.5)\nwhich contradicts with our statement that x∗is a local minimum. Therefore, there does not\nexist x′ ∈X for which f (x′) < f (x∗). The local minimum x∗is also the global mini-\nmum.\nFor instance, the convex function f (x) = (x −1)2 has a local minimum at x = 1, which is\nalso the global minimum.\n\n494\nOptimization Algorithms\nf = lambda x: (x - 1) ** 2\nd2l.set_figsize()\nd2l.plot([x, segment], [f(x), f(segment)], 'x', 'f(x)')\nThe fact that the local minima for convex functions are also the global minima is very con-\nvenient. It means that if we minimize functions we cannot “get stuck”. Note, though, that\nthis does not mean that there cannot be more than one global minimum or that there might\neven exist one. For instance, the function f (x) = max(|x| −1, 0) attains its minimum value\nover the interval [−1, 1]. Conversely, the function f (x) = exp(x) does not attain a minimum\nvalue on R: for x →−∞it asymptotes to 0, but there is no x for which f (x) = 0.\nBelow Sets of Convex Functions Are Convex\nWe can conveniently deﬁne convex sets via below sets of convex functions. Concretely, given\na convex function f deﬁned on a convex set X, any below set\nSb\ndef\n= {x|x ∈X and f (x) ≤b}\n(12.2.6)\nis convex.\nLet’s prove this quickly. Recall that for any x, x′ ∈Sb we need to show that λx +(1−λ)x′ ∈\nSb as long as λ ∈[0, 1]. Since f (x) ≤b and f (x′) ≤b, by the deﬁnition of convexity we\nhave\nf (λx + (1 −λ)x′) ≤λ f (x) + (1 −λ) f (x′) ≤b.\n(12.2.7)\nConvexity and Second Derivatives\nWhenever the second derivative of a function f : Rn →R exists it is very easy to check\nwhether f is convex. All we need to do is check whether the Hessian of f is positive semidef-\ninite: ∇2 f ⪰0, i.e., denoting the Hessian matrix ∇2 f by H, x⊤Hx ≥0 for all x ∈Rn. For\n\n495\nConvexity\ninstance, the function f (x) = 1\n2 ∥x∥2 is convex since ∇2 f = 1, i.e., its Hessian is an identity\nmatrix.\nFormally, a twice-diﬀerentiable one-dimensional function f : R →R is convex if and only\nif its second derivative f ′′ ≥0. For any twice-diﬀerentiable multidimensional function f :\nRn →R, it is convex if and only if its Hessian ∇2 f ⪰0.\nFirst, we need to prove the one-dimensional case. To see that convexity of f implies f ′′ ≥0\nwe use the fact that\n1\n2 f (x + ϵ) + 1\n2 f (x −ϵ) ≥f\n( x + ϵ\n2\n+ x −ϵ\n2\n)\n= f (x).\n(12.2.8)\nSince the second derivative is given by the limit over ﬁnite diﬀerences it follows that\nf ′′(x) = lim\nϵ→0\nf (x + ϵ) + f (x −ϵ) −2 f (x)\nϵ2\n≥0.\n(12.2.9)\nTo see that f ′′ ≥0 implies that f is convex we use the fact that f ′′ ≥0 implies that\nf ′ is a monotonically nondecreasing function. Let a < x < b be three points in R, where\nx = (1−λ)a+λb and λ ∈(0, 1). According to the mean value theorem, there exist α ∈[a, x]\nand β ∈[x, b] such that\nf ′(α) = f (x) −f (a)\nx −a\nand f ′(β) = f (b) −f (x)\nb −x\n.\n(12.2.10)\nBy monotonicity f ′(β) ≥f ′(α), hence\nx −a\nb −a f (b) + b −x\nb −a f (a) ≥f (x).\n(12.2.11)\nSince x = (1 −λ)a + λb, we have\nλ f (b) + (1 −λ) f (a) ≥f ((1 −λ)a + λb),\n(12.2.12)\nthus proving convexity.\nSecond, we need a lemma before proving the multidimensional case: f : Rn →R is convex\nif and only if for all x, y ∈Rn\ng(z) def\n= f (zx + (1 −z)y) where z ∈[0, 1]\n(12.2.13)\nis convex.\nTo prove that convexity of f implies that g is convex, we can show that for all a, b, λ ∈[0, 1]\n(thus 0 ≤λa + (1 −λ)b ≤1)\ng(λa + (1 −λ)b)\n= f ((λa + (1 −λ)b) x + (1 −λa −(1 −λ)b) y)\n= f (λ (ax + (1 −a)y) + (1 −λ) (bx + (1 −b)y))\n≤λ f (ax + (1 −a)y) + (1 −λ) f (bx + (1 −b)y)\n=λg(a) + (1 −λ)g(b).\n(12.2.14)\n\n496\nOptimization Algorithms\nTo prove the converse, we can show that for all λ ∈[0, 1]\nf (λx + (1 −λ)y)\n=g(λ · 1 + (1 −λ) · 0)\n≤λg(1) + (1 −λ)g(0)\n=λ f (x) + (1 −λ) f (y).\n(12.2.15)\nFinally, using the lemma above and the result of the one-dimensional case, the multidimen-\nsional case can be proven as follows. A multidimensional function f : Rn →R is convex\nif and only if for all x, y ∈Rn g(z) def\n= f (zx + (1 −z)y), where z ∈[0, 1], is convex. Ac-\ncording to the one-dimensional case, this holds if and only if g′′ = (x −y)⊤H(x −y) ≥0\n(H def\n= ∇2 f ) for all x, y ∈Rn, which is equivalent to H ⪰0 per the deﬁnition of positive\nsemideﬁnite matrices.\n12.2.3 Constraints\nOne of the nice properties of convex optimization is that it allows us to handle constraints\neﬃciently. That is, it allows us to solve constrained optimization problems of the form:\nminimize\nx\nf (x)\nsubject to ci(x) ≤0 for all i ∈{1, . . ., n},\n(12.2.16)\nwhere f is the objective and the functions ci are constraint functions. To see what this does\nconsider the case where c1(x) = ∥x∥2 −1. In this case the parameters x are constrained to\nthe unit ball. If a second constraint is c2(x) = v⊤x + b, then this corresponds to all x lying\non a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of a\nball.\nLagrangian\nIn general, solving a constrained optimization problem is diﬃcult. One way of addressing it\nstems from physics with a rather simple intuition. Imagine a ball inside a box. The ball will\nroll to the place that is lowest and the forces of gravity will be balanced out with the forces\nthat the sides of the box can impose on the ball. In short, the gradient of the objective function\n(i.e., gravity) will be oﬀset by the gradient of the constraint function (the ball need to remain\ninside the box by virtue of the walls “pushing back”). Note that some constraints may not be\nactive: the walls that are not touched by the ball will not be able to exert any force on the\nball.\nSkipping over the derivation of the Lagrangian L, the above reasoning can be expressed via\nthe following saddle point optimization problem:\nL(x, α1, . . ., αn) = f (x) +\nn\n∑\ni=1\nαici(x) where αi ≥0.\n(12.2.17)\n\n497\nConvexity\nHere the variables αi (i = 1, . . ., n) are the so-called Lagrange multipliers that ensure that\nconstraints are properly enforced. They are chosen just large enough to ensure that ci(x) ≤0\nfor all i. For instance, for any x where ci(x) < 0 naturally, we’d end up picking αi = 0.\nMoreover, this is a saddle point optimization problem where one wants to maximize L with\nrespect to all αi and simultaneously minimize it with respect to x. There is a rich body of\nliterature explaining how to arrive at the function L(x, α1, . . ., αn). For our purposes it is\nsuﬃcient to know that the saddle point of L is where the original constrained optimization\nproblem is solved optimally.\nPenalties\nOne way of satisfying constrained optimization problems at least approximately is to adapt\nthe Lagrangian L. Rather than satisfying ci(x) ≤0 we simply add αici(x) to the objective\nfunction f (x). This ensures that the constraints will not be violated too badly.\nIn fact, we have been using this trick all along. Consider weight decay in Section 3.7. In it\nwe add λ\n2 ∥w∥2 to the objective function to ensure that w does not grow too large. From the\nconstrained optimization point of view we can see that this will ensure that ∥w∥2 −r2 ≤0\nfor some radius r. Adjusting the value of λ allows us to vary the size of w.\nIn general, adding penalties is a good way of ensuring approximate constraint satisfaction.\nIn practice this turns out to be much more robust than exact satisfaction. Furthermore, for\nnonconvex problems many of the properties that make the exact approach so appealing in the\nconvex case (e.g., optimality) no longer hold.\nProjections\nAn alternative strategy for satisfying constraints is projections. Again, we encountered them\nbefore, e.g., when dealing with gradient clipping in Section 9.5. There we ensured that a\ngradient has length bounded by θ via\ng ←g · min(1, θ/∥g∥).\n(12.2.18)\nThis turns out to be a projection of g onto the ball of radius θ. More generally, a projection\non a convex set X is deﬁned as\nProjX(x) = argmin\nx′∈X\n∥x −x′∥,\n(12.2.19)\nwhich is the closest point in X to x.\nThe mathematical deﬁnition of projections may sound a bit abstract. Fig. 12.2.4 explains it\nsomewhat more clearly. In it we have two convex sets, a circle and a diamond. Points inside\nboth sets (yellow) remain unchanged during projections. Points outside both sets (black) are\nprojected to the points inside the sets (red) that are closet to the original points (black). While\nfor ℓ2 balls this leaves the direction unchanged, this need not be the case in general, as can\nbe seen in the case of the diamond.\n\n498\nOptimization Algorithms\nt\nFig. 12.2.4\nConvex Projections.\nOne of the uses for convex projections is to compute sparse weight vectors. In this case we\nproject weight vectors onto an ℓ1 ball, which is a generalized version of the diamond case in\nFig. 12.2.4.\n12.2.4 Summary\nIn the context of deep learning the main purpose of convex functions is to motivate opti-\nmization algorithms and help us understand them in detail. In the following we will see how\ngradient descent and stochastic gradient descent can be derived accordingly.\n• Intersections of convex sets are convex. Unions are not.\n• The expectation of a convex function is no less than the convex function of an expectation\n(Jensen’s inequality).\n• A twice-diﬀerentiable function is convex if and only if its Hessian (a matrix of second\nderivatives) is positive semideﬁnite.\n• Convex constraints can be added via the Lagrangian. In practice we may simply add them\nwith a penalty to the objective function.\n• Projections map to points in the convex set closest to the original points.\n12.2.5 Exercises\n1. Assume that we want to verify convexity of a set by drawing all lines between points within\nthe set and checking whether the lines are contained.\n1. Prove that it is suﬃcient to check only the points on the boundary.\n2. Prove that it is suﬃcient to check only the vertices of the set.\n2. Denote by Bp[r] def\n= {x|x ∈Rd and ∥x∥p ≤r} the ball of radius r using the p-norm.\nProve that Bp[r] is convex for all p ≥1.\n3. Given convex functions f and g, show that max( f, g) is convex, too. Prove that min( f, g)\nis not convex.\n4. Prove that the normalization of the softmax function is convex. More speciﬁcally prove\nthe convexity of f (x) = log ∑\ni exp(xi).\n\n499\nGradient Descent\n167\n5. Prove that linear subspaces, i.e., X = {x|Wx = b}, are convex sets.\n6. Prove that in the case of linear subspaces with b = 0 the projection ProjX can be written\nas Mx for some matrix M.\n7. Show that for twice-diﬀerentiable convex functions f we can write f (x + ϵ) = f (x) +\nϵ f ′(x) + 1\n2ϵ2 f ′′(x + ξ) for some ξ ∈[0, ϵ].\n8. Given a convex set X and two vectors x and y, prove that projections never increase\ndistances, i.e., ∥x −y∥≥∥ProjX(x) −ProjX(y)∥.\nDiscussions167.\n12.3 Gradient Descent\nIn this section we are going to introduce the basic concepts underlying gradient descent. Al-\nthough it is rarely used directly in deep learning, an understanding of gradient descent is key\nto understanding stochastic gradient descent algorithms. For instance, the optimization prob-\nlem might diverge due to an overly large learning rate. This phenomenon can already be seen\nin gradient descent. Likewise, preconditioning is a common technique in gradient descent\nand carries over to more advanced algorithms. Let’s start with a simple special case.\n12.3.1 One-Dimensional Gradient Descent\nGradient descent in one dimension is an excellent example to explain why the gradient de-\nscent algorithm may reduce the value of the objective function. Consider some continuously\ndiﬀerentiable real-valued function f : R →R. Using a Taylor expansion we obtain\nf (x + ϵ) = f (x) + ϵ f ′(x) + O(ϵ2).\n(12.3.1)\nThat is, in ﬁrst-order approximation f (x +ϵ) is given by the function value f (x) and the ﬁrst\nderivative f ′(x) at x. It is not unreasonable to assume that for small ϵ moving in the direction\nof the negative gradient will decrease f . To keep things simple we pick a ﬁxed step size η > 0\nand choose ϵ = −η f ′(x). Plugging this into the Taylor expansion above we get\nf (x −η f ′(x)) = f (x) −η f ′2(x) + O(η2 f ′2(x)).\n(12.3.2)\nIf the derivative f ′(x) , 0 does not vanish we make progress since η f ′2(x) > 0. Moreover,\nwe can always choose η small enough for the higher-order terms to become irrelevant. Hence\nwe arrive at\nf (x −η f ′(x)) ⪅f (x).\n(12.3.3)\n\n500\nOptimization Algorithms\nThis means that, if we use\nx ←x −η f ′(x)\n(12.3.4)\nto iterate x, the value of function f (x) might decline. Therefore, in gradient descent we ﬁrst\nchoose an initial value x and a constant η > 0 and then use them to continuously iterate x\nuntil the stop condition is reached, for example, when the magnitude of the gradient | f ′(x)|\nis small enough or the number of iterations has reached a certain value.\nFor simplicity we choose the objective function f (x) = x2 to illustrate how to implement\ngradient descent. Although we know that x = 0 is the solution to minimize f (x), we still use\nthis simple function to observe how x changes.\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom d2l import torch as d2l\ndef f(x):\n# Objective function\nreturn x ** 2\ndef f_grad(x):\n# Gradient (derivative) of the objective function\nreturn 2 * x\nNext, we use x = 10 as the initial value and assume η = 0.2. Using gradient descent to\niterate x for 10 times we can see that, eventually, the value of x approaches the optimal\nsolution.\ndef gd(eta, f_grad):\nx = 10.0\nresults = [x]\nfor i in range(10):\nx -= eta * f_grad(x)\nresults.append(float(x))\nprint(f'epoch 10, x: {x:f}')\nreturn results\nresults = gd(0.2, f_grad)\nepoch 10, x: 0.060466\nThe progress of optimizing over x can be plotted as follows.\ndef show_trace(results, f):\nn = max(abs(min(results)), abs(max(results)))\nf_line = torch.arange(-n, n, 0.01)\nd2l.set_figsize()\nd2l.plot([f_line, results], [[f(x) for x in f_line], [\nf(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n(continues on next page)\n\n501\nGradient Descent\n(continued from previous page)\nshow_trace(results, f)\nLearning Rate\nThe learning rate η can be set by the algorithm designer. If we use a learning rate that is too\nsmall, it will cause x to update very slowly, requiring more iterations to get a better solution.\nTo show what happens in such a case, consider the progress in the same optimization problem\nfor η = 0.05. As we can see, even after 10 steps we are still very far from the optimal\nsolution.\nshow_trace(gd(0.05, f_grad), f)\nepoch 10, x: 3.486784\nConversely, if we use an excessively high learning rate,\n\f\fη f ′(x)\n\f\f might be too large for the\nﬁrst-order Taylor expansion formula. That is, the term O(η2 f ′2(x)) in (12.3.2) might become\nsigniﬁcant. In this case, we cannot guarantee that the iteration of x will be able to lower the\n\n502\nOptimization Algorithms\nvalue of f (x). For example, when we set the learning rate to η = 1.1, x overshoots the\noptimal solution x = 0 and gradually diverges.\nshow_trace(gd(1.1, f_grad), f)\nepoch 10, x: 61.917364\nLocal Minima\nTo illustrate what happens for nonconvex functions consider the case of f (x) = x·cos(cx) for\nsome constant c. This function has inﬁnitely many local minima. Depending on our choice of\nthe learning rate and depending on how well conditioned the problem is, we may end up with\none of many solutions. The example below illustrates how an (unrealistically) high learning\nrate will lead to a poor local minimum.\nc = torch.tensor(0.15 * np.pi)\ndef f(x):\n# Objective function\nreturn x * torch.cos(c * x)\ndef f_grad(x):\n# Gradient of the objective function\nreturn torch.cos(c * x) - c * x * torch.sin(c * x)\nshow_trace(gd(2, f_grad), f)\nepoch 10, x: -1.528166\n12.3.2 Multivariate Gradient Descent\nNow that we have a better intuition of the univariate case, let’s consider the situation where\nx = [x1, x2, . . ., xd]⊤. That is, the objective function f : Rd →R maps vectors into scalars.\n\n503\nGradient Descent\nCorrespondingly its gradient is multivariate, too. It is a vector consisting of d partial deriva-\ntives:\n∇f (x) =\n[ ∂f (x)\n∂x1\n, ∂f (x)\n∂x2\n, . . ., ∂f (x)\n∂xd\n]⊤\n.\n(12.3.5)\nEach partial derivative element ∂f (x)/∂xi in the gradient indicates the rate of change of f at\nx with respect to the input xi. As before in the univariate case we can use the corresponding\nTaylor approximation for multivariate functions to get some idea of what we should do. In\nparticular, we have that\nf (x + ϵ) = f (x) + ϵ⊤∇f (x) + O(∥ϵ∥2).\n(12.3.6)\nIn other words, up to second-order terms in ϵ the direction of steepest descent is given by the\nnegative gradient −∇f (x). Choosing a suitable learning rate η > 0 yields the prototypical\ngradient descent algorithm:\nx ←x −η∇f (x).\n(12.3.7)\nTo see how the algorithm behaves in practice let’s construct an objective function f (x) =\nx2\n1 + 2x2\n2 with a two-dimensional vector x = [x1, x2]⊤as input and a scalar as output. The\ngradient is given by ∇f (x) = [2x1, 4x2]⊤. We will observe the trajectory of x by gradient\ndescent from the initial position [−5, −2].\nTo begin with, we need two more helper functions. The ﬁrst uses an update function and\napplies it 20 times to the initial value. The second helper visualizes the trajectory of x.\ndef train_2d(trainer, steps=20, f_grad=None):\n#@save\n\"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n# `s1` and `s2` are internal state variables that will be used in Momentum,\n,→adagrad, RMSProp\nx1, x2, s1, s2 = -5, -2, 0, 0\nresults = [(x1, x2)]\nfor i in range(steps):\nif f_grad:\nx1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\nelse:\nx1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n(continues on next page)\n\n504\nOptimization Algorithms\n(continued from previous page)\nresults.append((x1, x2))\nprint(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\nreturn results\ndef show_trace_2d(f, results):\n#@save\n\"\"\"Show the trace of 2D variables during optimization.\"\"\"\nd2l.set_figsize()\nd2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\nx1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1),\ntorch.arange(-3.0, 1.0, 0.1), indexing='ij')\nd2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\nd2l.plt.xlabel('x1')\nd2l.plt.ylabel('x2')\nNext, we observe the trajectory of the optimization variable x for learning rate η = 0.1. We\ncan see that after 20 steps the value of x approaches its minimum at [0, 0]. Progress is fairly\nwell-behaved albeit rather slow.\ndef f_2d(x1, x2):\n# Objective function\nreturn x1 ** 2 + 2 * x2 ** 2\ndef f_2d_grad(x1, x2):\n# Gradient of the objective function\nreturn (2 * x1, 4 * x2)\ndef gd_2d(x1, x2, s1, s2, f_grad):\ng1, g2 = f_grad(x1, x2)\nreturn (x1 - eta * g1, x2 - eta * g2, 0, 0)\neta = 0.1\nshow_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))\nepoch 20, x1: -0.057646, x2: -0.000073\n12.3.3 Adaptive Methods\n\n505\nGradient Descent\nAs we could see in Section 12.3.1, getting the learning rate η “just right” is tricky. If we\npick it too small, we make little progress. If we pick it too large, the solution oscillates and\nin the worst case it might even diverge. What if we could determine η automatically or get\nrid of having to select a learning rate at all? Second-order methods that look not only at the\nvalue and gradient of the objective function but also at its curvature can help in this case.\nWhile these methods cannot be applied to deep learning directly due to the computational\ncost, they provide useful intuition into how to design advanced optimization algorithms that\nmimic many of the desirable properties of the algorithms outlined below.\nNewton’s Method\nReviewing the Taylor expansion of some function f : Rd →R there is no need to stop after\nthe ﬁrst term. In fact, we can write it as\nf (x + ϵ) = f (x) + ϵ⊤∇f (x) + 1\n2ϵ⊤∇2 f (x)ϵ + O(∥ϵ∥3).\n(12.3.8)\nTo avoid cumbersome notation we deﬁne H def\n= ∇2 f (x) to be the Hessian of f , which is\na d × d matrix. For small d and simple problems H is easy to compute. For deep neural\nnetworks, on the other hand, H may be prohibitively large, due to the cost of storing O(d2)\nentries. Furthermore it may be too expensive to compute via backpropagation. For now let’s\nignore such considerations and look at what algorithm we would get.\nAfter all, the minimum of f satisﬁes ∇f = 0. Following calculus rules in Section 2.4.3,\nby taking derivatives of (12.3.8) with regard to ϵ and ignoring higher-order terms we arrive\nat\n∇f (x) + Hϵ = 0 and hence ϵ = −H−1∇f (x).\n(12.3.9)\nThat is, we need to invert the Hessian H as part of the optimization problem.\nAs a simple example, for f (x) = 1\n2 x2 we have ∇f (x) = x and H = 1. Hence for any x\nwe obtain ϵ = −x. In other words, a single step is suﬃcient to converge perfectly without the\nneed for any adjustment! Alas, we got a bit lucky here: the Taylor expansion was exact since\nf (x + ϵ) = 1\n2 x2 + ϵx + 1\n2ϵ2.\nLet’s see what happens in other problems. Given a convex hyperbolic cosine function f (x) =\ncosh(cx) for some constant c, we can see that the global minimum at x = 0 is reached after\na few iterations.\nc = torch.tensor(0.5)\ndef f(x):\n# Objective function\nreturn torch.cosh(c * x)\ndef f_grad(x):\n# Gradient of the objective function\nreturn c * torch.sinh(c * x)\n(continues on next page)\n\n506\nOptimization Algorithms\n(continued from previous page)\ndef f_hess(x):\n# Hessian of the objective function\nreturn c**2 * torch.cosh(c * x)\ndef newton(eta=1):\nx = 10.0\nresults = [x]\nfor i in range(10):\nx -= eta * f_grad(x) / f_hess(x)\nresults.append(float(x))\nprint('epoch 10, x:', x)\nreturn results\nshow_trace(newton(), f)\nepoch 10, x: tensor(0.)\nNow let’s consider a nonconvex function, such as f (x) = x cos(cx) for some constant c.\nAfter all, note that in Newton’s method we end up dividing by the Hessian. This means that\nif the second derivative is negative we may walk into the direction of increasing the value of\nf . That is a fatal ﬂaw of the algorithm. Let’s see what happens in practice.\nc = torch.tensor(0.15 * np.pi)\ndef f(x):\n# Objective function\nreturn x * torch.cos(c * x)\ndef f_grad(x):\n# Gradient of the objective function\nreturn torch.cos(c * x) - c * x * torch.sin(c * x)\ndef f_hess(x):\n# Hessian of the objective function\nreturn - 2 * c * torch.sin(c * x) - x * c**2 * torch.cos(c * x)\nshow_trace(newton(), f)\n\n507\nGradient Descent\nepoch 10, x: tensor(26.8341)\nThis went spectacularly wrong. How can we ﬁx it? One way would be to “ﬁx” the Hessian\nby taking its absolute value instead. Another strategy is to bring back the learning rate. This\nseems to defeat the purpose, but not quite. Having second-order information allows us to\nbe cautious whenever the curvature is large and to take longer steps whenever the objective\nfunction is ﬂatter. Let’s see how this works with a slightly smaller learning rate, say η = 0.5.\nAs we can see, we have quite an eﬃcient algorithm.\nshow_trace(newton(0.5), f)\nepoch 10, x: tensor(7.2699)\nConvergence Analysis\nWe only analyze the convergence rate of Newton’s method for some convex and three times\ndiﬀerentiable objective function f , where the second derivative is nonzero, i.e., f ′′ > 0. The\n\n508\nOptimization Algorithms\nmultivariate proof is a straightforward extension of the one-dimensional argument below and\nomitted since it does not help us much in terms of intuition.\nDenote by x(k) the value of x at the kth iteration and let e(k) def\n= x(k)−x∗be the distance from\noptimality at the kth iteration. By Taylor expansion we have that the condition f ′(x∗) = 0\ncan be written as\n0 = f ′(x(k) −e(k)) = f ′(x(k)) −e(k) f ′′(x(k)) + 1\n2(e(k))2 f ′′′(ξ(k)),\n(12.3.10)\nwhich holds for some ξ(k) ∈[x(k) −e(k), x(k)]. Dividing the above expansion by f ′′(x(k))\nyields\ne(k) −f ′(x(k))\nf ′′(x(k)) = 1\n2(e(k))2 f ′′′(ξ(k))\nf ′′(x(k)) .\n(12.3.11)\nRecall that we have the update x(k+1) = x(k) −f ′(x(k))/ f ′′(x(k)). Plugging in this update\nequation and taking the absolute value of both sides, we have\n\f\f\fe(k+1)\f\f\f = 1\n2(e(k))2\n\f\f f ′′′(ξ(k))\n\f\f\nf ′′(x(k)) .\n(12.3.12)\nConsequently, whenever we are in a region of bounded\n\f\f f ′′′(ξ(k))\n\f\f /(2 f ′′(x(k))) ≤c, we\nhave a quadratically decreasing error\n\f\f\fe(k+1)\f\f\f ≤c(e(k))2.\n(12.3.13)\nAs an aside, optimization researchers call this linear convergence, whereas a condition such\nas\n\f\fe(k+1)\f\f ≤α\n\f\fe(k)\f\f would be called a constant rate of convergence. Note that this analysis\ncomes with a number of caveats. First, we do not really have much of a guarantee when\nwe will reach the region of rapid convergence. Instead, we only know that once we reach it,\nconvergence will be very quick. Second, this analysis requires that f is well-behaved up to\nhigher-order derivatives. It comes down to ensuring that f does not have any “surprising”\nproperties in terms of how it might change its values.\nPreconditioning\nQuite unsurprisingly computing and storing the full Hessian is very expensive. It is thus desir-\nable to ﬁnd alternatives. One way to improve matters is preconditioning. It avoids computing\nthe Hessian in its entirety but only computes the diagonal entries. This leads to update algo-\nrithms of the form\nx ←x −ηdiag(H)−1∇f (x).\n(12.3.14)\nWhile this is not quite as good as the full Newton’s method, it is still much better than not\nusing it. To see why this might be a good idea consider a situation where one variable de-\nnotes height in millimeters and the other one denotes height in kilometers. Assuming that for\nboth the natural scale is in meters, we have a terrible mismatch in parametrizations. Fortu-\nnately, using preconditioning removes this. Eﬀectively preconditioning with gradient descent\n\n509\nGradient Descent\namounts to selecting a diﬀerent learning rate for each variable (coordinate of vector x). As\nwe will see later, preconditioning drives some of the innovation in stochastic gradient descent\noptimization algorithms.\nGradient Descent with Line Search\nOne of the key problems in gradient descent is that we might overshoot the goal or make\ninsuﬃcient progress. A simple ﬁx for the problem is to use line search in conjunction with\ngradient descent. That is, we use the direction given by ∇f (x) and then perform binary search\nas to which learning rate η minimizes f (x −η∇f (x)).\nThis algorithm converges rapidly (for an analysis and proof see e.g., Boyd and Vandenberghe\n(2004)). However, for the purpose of deep learning this is not quite so feasible, since each\nstep of the line search would require us to evaluate the objective function on the entire dataset.\nThis is way too costly to accomplish.\n12.3.4 Summary\n• Learning rates matter. Too large and we diverge, too small and we do not make progress.\n• Gradient descent can get stuck in local minima.\n• In high dimensions adjusting the learning rate is complicated.\n• Preconditioning can help with scale adjustment.\n• Newton’s method is a lot faster once it has started working properly in convex problems.\n• Beware of using Newton’s method without any adjustments for nonconvex problems.\n12.3.5 Exercises\n1. Experiment with diﬀerent learning rates and objective functions for gradient descent.\n2. Implement line search to minimize a convex function in the interval [a, b].\n1. Do you need derivatives for binary search, i.e., to decide whether to pick [a, (a+b)/2]\nor [(a + b)/2, b].\n2. How rapid is the rate of convergence for the algorithm?\n3. Implement the algorithm and apply it to minimizing log(exp(x) + exp(−2x −3)).\n3. Design an objective function deﬁned on R2 where gradient descent is exceedingly slow.\nHint: scale diﬀerent coordinates diﬀerently.\n4. Implement the lightweight version of Newton’s method using preconditioning:\n1. Use diagonal Hessian as preconditioner.\n\n510\nOptimization Algorithms\n168\n2. Use the absolute values of that rather than the actual (possibly signed) values.\n3. Apply this to the problem above.\n5. Apply the algorithm above to a number of objective functions (convex or not). What\nhappens if you rotate coordinates by 45 degrees?\nDiscussions168.\n12.4 Stochastic Gradient Descent\nIn earlier chapters we kept using stochastic gradient descent in our training procedure, how-\never, without explaining why it works. To shed some light on it, we just described the basic\nprinciples of gradient descent in Section 12.3. In this section, we go on to discuss stochastic\ngradient descent in greater detail.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch as d2l\n12.4.1 Stochastic Gradient Updates\nIn deep learning, the objective function is usually the average of the loss functions for each\nexample in the training dataset. Given a training dataset of n examples, we assume that fi(x)\nis the loss function with respect to the training example of index i, where x is the parameter\nvector. Then we arrive at the objective function\nf (x) = 1\nn\nn\n∑\ni=1\nfi(x).\n(12.4.1)\nThe gradient of the objective function at x is computed as\n∇f (x) = 1\nn\nn\n∑\ni=1\n∇fi(x).\n(12.4.2)\nIf gradient descent is used, the computational cost for each independent variable iteration is\nO(n), which grows linearly with n. Therefore, when the training dataset is larger, the cost of\ngradient descent for each iteration will be higher.\nStochastic gradient descent (SGD) reduces computational cost at each iteration. At each it-\neration of stochastic gradient descent, we uniformly sample an index i ∈{1, . . ., n} for data\nexamples at random, and compute the gradient ∇fi(x) to update x:\nx ←x −η∇fi(x),\n(12.4.3)\n\n511\nStochastic Gradient Descent\nwhere η is the learning rate. We can see that the computational cost for each iteration drops\nfrom O(n) of the gradient descent to the constant O(1). Moreover, we want to emphasize\nthat the stochastic gradient ∇fi(x) is an unbiased estimate of the full gradient ∇f (x) be-\ncause\nEi∇fi(x) = 1\nn\nn\n∑\ni=1\n∇fi(x) = ∇f (x).\n(12.4.4)\nThis means that, on average, the stochastic gradient is a good estimate of the gradient.\nNow, we will compare it with gradient descent by adding random noise with a mean of 0 and\na variance of 1 to the gradient to simulate a stochastic gradient descent.\ndef f(x1, x2):\n# Objective function\nreturn x1 ** 2 + 2 * x2 ** 2\ndef f_grad(x1, x2):\n# Gradient of the objective function\nreturn 2 * x1, 4 * x2\ndef sgd(x1, x2, s1, s2, f_grad):\ng1, g2 = f_grad(x1, x2)\n# Simulate noisy gradient\ng1 += torch.normal(0.0, 1, (1,)).item()\ng2 += torch.normal(0.0, 1, (1,)).item()\neta_t = eta * lr()\nreturn (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)\ndef constant_lr():\nreturn 1\neta = 0.1\nlr = constant_lr\n# Constant learning rate\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))\nepoch 50, x1: -0.299320, x2: -0.024252\nAs we can see, the trajectory of the variables in the stochastic gradient descent is much\n\n512\nOptimization Algorithms\nmore noisy than the one we observed in gradient descent in Section 12.3. This is due to the\nstochastic nature of the gradient. That is, even when we arrive near the minimum, we are still\nsubject to the uncertainty injected by the instantaneous gradient via η∇fi(x). Even after 50\nsteps the quality is still not so good. Even worse, it will not improve after additional steps (we\nencourage you to experiment with a larger number of steps to conﬁrm this). This leaves us\nwith the only alternative: change the learning rate η. However, if we pick this too small, we\nwill not make any meaningful progress initially. On the other hand, if we pick it too large, we\nwill not get a good solution, as seen above. The only way to resolve these conﬂicting goals is\nto reduce the learning rate dynamically as optimization progresses.\nThis is also the reason for adding a learning rate function lr into the sgd step function. In\nthe example above any functionality for learning rate scheduling lies dormant as we set the\nassociated lr function to be constant.\n12.4.2 Dynamic Learning Rate\nReplacing η with a time-dependent learning rate η(t) adds to the complexity of controlling\nconvergence of an optimization algorithm. In particular, we need to ﬁgure out how rapidly\nη should decay. If it is too quick, we will stop optimizing prematurely. If we decrease it too\nslowly, we waste too much time on optimization. The following are a few basic strategies that\nare used in adjusting η over time (we will discuss more advanced strategies later):\nη(t) = ηi if ti ≤t ≤ti+1\npiecewise constant\nη(t) = η0 · e−λt\nexponential decay\nη(t) = η0 · (βt + 1)−α\npolynomial decay\n(12.4.5)\nIn the ﬁrst piecewise constant scenario we decrease the learning rate, e.g., whenever progress\nin optimization stalls. This is a common strategy for training deep networks. Alternatively\nwe could decrease it much more aggressively by an exponential decay. Unfortunately this\noften leads to premature stopping before the algorithm has converged. A popular choice is\npolynomial decay with α = 0.5. In the case of convex optimization there are a number of\nproofs that show that this rate is well behaved.\nLet’s see what the exponential decay looks like in practice.\ndef exponential_lr():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt += 1\nreturn math.exp(-0.1 * t)\nt = 1\nlr = exponential_lr\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))\nepoch 1000, x1: -0.896277, x2: 0.012123\n\n513\nStochastic Gradient Descent\nAs expected, the variance in the parameters is signiﬁcantly reduced. However, this comes\nat the expense of failing to converge to the optimal solution x = (0, 0). Even after 1000\niteration steps are we are still very far away from the optimal solution. Indeed, the algorithm\nfails to converge at all. On the other hand, if we use a polynomial decay where the learning\nrate decays with the inverse square root of the number of steps, convergence gets better after\nonly 50 steps.\ndef polynomial_lr():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt += 1\nreturn (1 + 0.1 * t) ** (-0.5)\nt = 1\nlr = polynomial_lr\nd2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))\nepoch 50, x1: 0.053842, x2: 0.059013\nThere exist many more choices for how to set the learning rate. For instance, we could start\nwith a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We\ncould even alternate between smaller and larger learning rates. There exists a large variety of\n\n514\nOptimization Algorithms\n169\nsuch schedules. For now let’s focus on learning rate schedules for which a comprehensive the-\noretical analysis is possible, i.e., on learning rates in a convex setting. For general nonconvex\nproblems it is very diﬃcult to obtain meaningful convergence guarantees, since in general\nminimizing nonlinear nonconvex problems is NP hard. For a survey see e.g., the excellent\nlecture notes169 of Tibshirani 2015.\n12.4.3 Convergence Analysis for Convex Objectives\nThe following convergence analysis of stochastic gradient descent for convex objective func-\ntions is optional and primarily serves to convey more intuition about the problem. We limit\nourselves to one of the simplest proofs (Nesterov and Vial, 2000). Signiﬁcantly more ad-\nvanced proof techniques exist, e.g., whenever the objective function is particularly well be-\nhaved.\nSuppose that the objective function f (ξ, x) is convex in x for all ξ. More concretely, we\nconsider the stochastic gradient descent update:\nxt+1 = xt −ηt∂x f (ξt, x),\n(12.4.6)\nwhere f (ξt, x) is the objective function with respect to the training example ξt drawn from\nsome distribution at step t and x is the model parameter. Denote by\nR(x) = Eξ[ f (ξ, x)]\n(12.4.7)\nthe expected risk and by R∗its minimum with regard to x. Last let x∗be the minimizer\n(we assume that it exists within the domain where x is deﬁned). In this case we can track\nthe distance between the current parameter xt at time t and the risk minimizer x∗and see\nwhether it improves over time:\n∥xt+1 −x∗∥2\n=∥xt −ηt∂x f (ξt, x) −x∗∥2\n=∥xt −x∗∥2 + η2\nt ∥∂x f (ξt, x)∥2 −2ηt\n⟨\nxt −x∗, ∂x f (ξt, x)\n⟩\n.\n(12.4.8)\nWe assume that the ℓ2 norm of stochastic gradient ∂x f (ξt, x) is bounded by some constant\nL, hence we have that\nη2\nt ∥∂x f (ξt, x)∥2 ≤η2\nt L2.\n(12.4.9)\nWe are mostly interested in how the distance between xt and x∗changes in expectation.\nIn fact, for any speciﬁc sequence of steps the distance might well increase, depending on\nwhichever ξt we encounter. Hence we need to bound the dot product. Since for any convex\nfunction f it holds that f (y) ≥f (x) + ⟨f ′(x), y −x⟩for all x and y, by convexity we\nhave\nf (ξt, x∗) ≥f (ξt, xt) +\n⟨\nx∗−xt, ∂x f (ξt, xt)\n⟩\n.\n(12.4.10)\nPlugging both inequalities (12.4.9) and (12.4.10) into (12.4.8) we obtain a bound on the\n\n515\nStochastic Gradient Descent\ndistance between parameters at time t + 1 as follows:\n∥xt −x∗∥2 −∥xt+1 −x∗∥2 ≥2ηt( f (ξt, xt) −f (ξt, x∗)) −η2\nt L2.\n(12.4.11)\nThis means that we make progress as long as the diﬀerence between current loss and the\noptimal loss outweighs ηt L2/2. Since this diﬀerence is bound to converge to zero it follows\nthat the learning rate ηt also needs to vanish.\nNext we take expectations over (12.4.11). This yields\nE\n[\n∥xt −x∗∥2]\n−E\n[\n∥xt+1 −x∗∥2]\n≥2ηt[E[R(xt)] −R∗] −η2\nt L2.\n(12.4.12)\nThe last step involves summing over the inequalities for t ∈{1, . . .,T}. Since the sum tele-\nscopes and by dropping the lower term we obtain\n∥x1 −x∗∥2 ≥2\n( T\n∑\nt=1\nηt\n)\n[E[R(xt)] −R∗] −L2\nT\n∑\nt=1\nη2\nt .\n(12.4.13)\nNote that we exploited that x1 is given and thus the expectation can be dropped. Last de-\nﬁne\n¯x def\n=\n∑T\nt=1 ηtxt\n∑T\nt=1 ηt\n.\n(12.4.14)\nSince\nE\n(∑T\nt=1 ηtR(xt)\n∑T\nt=1 ηt\n)\n=\n∑T\nt=1 ηtE[R(xt)]\n∑T\nt=1 ηt\n= E[R(xt)],\n(12.4.15)\nby Jensen’s inequality (setting i = t, αi = ηt/ ∑T\nt=1 ηt in (12.2.3)) and convexity of R it\nfollows that E[R(xt)] ≥E[R(¯x)], thus\nT\n∑\nt=1\nηtE[R(xt)] ≥\nT\n∑\nt=1\nηtE [R(¯x)] .\n(12.4.16)\nPlugging this into the inequality (12.4.13) yields the bound\n[E[¯x]] −R∗≤r2 + L2 ∑T\nt=1 η2\nt\n2 ∑T\nt=1 ηt\n,\n(12.4.17)\nwhere r2 def\n= ∥x1 −x∗∥2 is a bound on the distance between the initial choice of parame-\nters and the ﬁnal outcome. In short, the speed of convergence depends on how the norm of\nstochastic gradient is bounded (L) and how far away from optimality the initial parameter\nvalue is (r). Note that the bound is in terms of ¯x rather than xT. This is the case since ¯x is a\nsmoothed version of the optimization path. Whenever r, L, and T are known we can pick the\nlearning rate η = r/(L\n√\nT). This yields as upper bound rL/\n√\nT. That is, we converge with\nrate O(1/\n√\nT) to the optimal solution.\n\n516\nOptimization Algorithms\n12.4.4 Stochastic Gradients and Finite Samples\nSo far we have played a bit fast and loose when it comes to talking about stochastic gra-\ndient descent. We posited that we draw instances xi, typically with labels yi from some\ndistribution p(x, y) and that we use this to update the model parameters in some man-\nner. In particular, for a ﬁnite sample size we simply argued that the discrete distribution\np(x, y) = 1\nn\n∑n\ni=1 δxi(x)δyi(y) for some functions δxi and δyi allows us to perform stochas-\ntic gradient descent over it.\nHowever, this is not really what we did. In the toy examples in the current section we simply\nadded noise to an otherwise non-stochastic gradient, i.e., we pretended to have pairs (xi, yi).\nIt turns out that this is justiﬁed here (see the exercises for a detailed discussion). More trou-\nbling is that in all previous discussions we clearly did not do this. Instead we iterated over all\ninstances exactly once. To see why this is preferable consider the converse, namely that we\nare sampling n observations from the discrete distribution with replacement. The probability\nof choosing an element i at random is 1/n. Thus to choose it at least once is\nP(choose i) = 1 −P(omit i) = 1 −(1 −1/n)n ≈1 −e−1 ≈0.63.\n(12.4.18)\nA similar reasoning shows that the probability of picking some sample (i.e., training example)\nexactly once is given by\n(n\n1\n) 1\nn\n(\n1 −1\nn\n)n−1\n=\nn\nn −1\n(\n1 −1\nn\n)n\n≈e−1 ≈0.37.\n(12.4.19)\nSampling with replacement leads to an increased variance and decreased data eﬃciency rel-\native to sampling without replacement. Hence, in practice we perform the latter (and this is\nthe default choice throughout this book). Last note that repeated passes through the training\ndataset traverse it in a diﬀerent random order.\n12.4.5 Summary\n• For convex problems we can prove that for a wide choice of learning rates stochastic gra-\ndient descent will converge to the optimal solution.\n• For deep learning this is generally not the case. However, the analysis of convex problems\ngives us useful insight into how to approach optimization, namely to reduce the learning\nrate progressively, albeit not too quickly.\n• Problems occur when the learning rate is too small or too large. In practice a suitable\nlearning rate is often found only after multiple experiments.\n• When there are more examples in the training dataset, it costs more to compute each\niteration for gradient descent, so stochastic gradient descent is preferred in these cases.\n• Optimality guarantees for stochastic gradient descent are in general not available in non-\nconvex cases since the number of local minima that require checking might well be\nexponential.\n\n517\nMinibatch Stochastic Gradient Descent\n170\n12.4.6 Exercises\n1. Experiment with diﬀerent learning rate schedules for stochastic gradient descent and with\ndiﬀerent numbers of iterations. In particular, plot the distance from the optimal solution\n(0, 0) as a function of the number of iterations.\n2. Prove that for the function f (x1, x2) = x2\n1 + 2x2\n2 adding normal noise to the gradient is\nequivalent to minimizing a loss function f (x, w) = (x1 −w1)2 + 2(x2 −w2)2 where x\nis drawn from a normal distribution.\n3. Compare convergence of stochastic gradient descent when you sample from {(x1, y1), . . ., (xn, yn)}\nwith replacement and when you sample without replacement.\n4. How would you change the stochastic gradient descent solver if some gradient (or rather\nsome coordinate associated with it) was consistently larger than all the other gradients?\n5. Assume that f (x) = x2(1 + sin x). How many local minima does f have? Can you\nchange f in such a way that to minimize it one needs to evaluate all the local minima?\nDiscussions170.\n12.5 Minibatch Stochastic Gradient Descent\nSo far we encountered two extremes in the approach to gradient-based learning: Section 12.3\nuses the full dataset to compute gradients and to update parameters, one pass at a time.\nConversely Section 12.4 processes one training example at a time to make progress. Either\nof them has its own drawbacks. Gradient descent is not particularly data eﬃcient whenever\ndata is very similar. Stochastic gradient descent is not particularly computationally eﬃcient\nsince CPUs and GPUs cannot exploit the full power of vectorization. This suggests that there\nmight be something in between, and in fact, that is what we have been using so far in the\nexamples we discussed.\n12.5.1 Vectorization and Caches\nAt the heart of the decision to use minibatches is computational eﬃciency. This is most easily\nunderstood when considering parallelization to multiple GPUs and multiple servers. In this\ncase we need to send at least one image to each GPU. With 8 GPUs per server and 16 servers\nwe already arrive at a minibatch size no smaller than 128.\nThings are a bit more subtle when it comes to single GPUs or even CPUs. These devices\nhave multiple types of memory, often multiple types of computational units and diﬀerent\nbandwidth constraints between them. For instance, a CPU has a small number of registers\nand then the L1, L2, and in some cases even L3 cache (which is shared among diﬀerent\n\n518\nOptimization Algorithms\n171\nprocessor cores). These caches are of increasing size and latency (and at the same time they\nare of decreasing bandwidth). Suﬃce to say, the processor is capable of performing many\nmore operations than what the main memory interface is able to provide.\nFirst, a 2GHz CPU with 16 cores and AVX-512 vectorization can process up to 2 · 109 · 16 ·\n32 = 1012 bytes per second. The capability of GPUs easily exceeds this number by a factor\nof 100. On the other hand, a midrange server processor might not have much more than 100\nGB/s bandwidth, i.e., less than one tenth of what would be required to keep the processor\nfed. To make matters worse, not all memory access is created equal: memory interfaces are\ntypically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a single byte incurs\nthe cost of a much wider access.\nSecond, there is signiﬁcant overhead for the ﬁrst access whereas sequential access is relatively\ncheap (this is often called a burst read). There are many more things to keep in mind, such\nas caching when we have multiple sockets, chiplets, and other structures. See this Wikipedia\narticle171 for a more in-depth discussion.\nThe way to alleviate these constraints is to use a hierarchy of CPU caches that are actually fast\nenough to supply the processor with data. This is the driving force behind batching in deep\nlearning. To keep matters simple, consider matrix-matrix multiplication, say A = BC. We\nhave a number of options for calculating A. For instance, we could try the following:\n1. We could compute Aij = Bi,:C:,j, i.e., we could compute it elementwise by means of\ndot products.\n2. We could compute A:, j = BC:,j, i.e., we could compute it one column at a time. Likewise\nwe could compute A one row Ai,: at a time.\n3. We could simply compute A = BC.\n4. We could break B and C into smaller block matrices and compute A one block at a time.\nIf we follow the ﬁrst option, we will need to copy one row and one column vector into the\nCPU each time we want to compute an element Aij. Even worse, due to the fact that matrix\nelements are aligned sequentially we are thus required to access many disjoint locations for\none of the two vectors as we read them from memory. The second option is much more\nfavorable. In it, we are able to keep the column vector C:, j in the CPU cache while we keep on\ntraversing through B. This halves the memory bandwidth requirement with correspondingly\nfaster access. Of course, option 3 is most desirable. Unfortunately, most matrices might not\nentirely ﬁt into cache (this is what we are discussing after all). However, option 4 oﬀers a\npractically useful alternative: we can move blocks of the matrix into cache and multiply them\nlocally. Optimized libraries take care of this for us. Let’s have a look at how eﬃcient these\noperations are in practice.\nBeyond computational eﬃciency, the overhead introduced by Python and by the deep learn-\ning framework itself is considerable. Recall that each time we execute a command the Python\ninterpreter sends a command to the MXNet engine which needs to insert it into the compu-\ntational graph and deal with it during scheduling. Such overhead can be quite detrimental. In\nshort, it is highly advisable to use vectorization (and matrices) whenever possible.\n\n519\nMinibatch Stochastic Gradient Descent\n%matplotlib inline\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nA = torch.zeros(256, 256)\nB = torch.randn(256, 256)\nC = torch.randn(256, 256)\nSince we will benchmark the running time frequently in the rest of the book, let’s deﬁne a\ntimer.\nclass Timer:\n#@save\n\"\"\"Record multiple running times.\"\"\"\ndef __init__(self):\nself.times = []\nself.start()\ndef start(self):\n\"\"\"Start the timer.\"\"\"\nself.tik = time.time()\ndef stop(self):\n\"\"\"Stop the timer and record the time in a list.\"\"\"\nself.times.append(time.time() - self.tik)\nreturn self.times[-1]\ndef avg(self):\n\"\"\"Return the average time.\"\"\"\nreturn sum(self.times) / len(self.times)\ndef sum(self):\n\"\"\"Return the sum of time.\"\"\"\nreturn sum(self.times)\ndef cumsum(self):\n\"\"\"Return the accumulated time.\"\"\"\nreturn np.array(self.times).cumsum().tolist()\ntimer = Timer()\nElement-wise assignment simply iterates over all rows and columns of B and C respectively\nto assign the value to A.\n# Compute A = BC one element at a time\ntimer.start()\nfor i in range(256):\nfor j in range(256):\nA[i, j] = torch.dot(B[i, :], C[:, j])\ntimer.stop()\n\n520\nOptimization Algorithms\n2.3525452613830566\nA faster strategy is to perform column-wise assignment.\n# Compute A = BC one column at a time\ntimer.start()\nfor j in range(256):\nA[:, j] = torch.mv(B, C[:, j])\ntimer.stop()\n0.1891944408416748\nLast, the most eﬀective manner is to perform the entire operation in one block. Note that\nmultiplying any two matrices B ∈Rm×n and C ∈Rn×p takes approximately 2mnp ﬂoating\npoint operations, when scalar multiplication and addition are counted as separate operations\n(fused in practice). Thus, multiplying two 256×256 matrices takes 0.03 billion ﬂoating point\noperations. Let’s see what the respective speed of the operations is.\n# Compute A = BC in one go\ntimer.start()\nA = torch.mm(B, C)\ntimer.stop()\ngigaflops = [0.03 / i for i in timer.times]\nprint(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\nf'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')\nperformance in Gigaflops: element 0.013, column 0.159, full 3.200\n12.5.2 Minibatches\nIn the past we took it for granted that we would read minibatches of data rather than single\nobservations to update parameters. We now give a brief justiﬁcation for it. Processing single\nobservations requires us to perform many single matrix-vector (or even vector-vector) multi-\nplications, which is quite expensive and which incurs a signiﬁcant overhead on behalf of the\nunderlying deep learning framework. This applies both to evaluating a network when applied\nto data (often referred to as inference) and when computing gradients to update parameters.\nThat is, this applies whenever we perform w ←w −ηtgt where\ngt = ∂w f (xt, w)\n(12.5.1)\nWe can increase the computational eﬃciency of this operation by applying it to a minibatch\nof observations at a time. That is, we replace the gradient gt over a single observation by one\n\n521\nMinibatch Stochastic Gradient Descent\n172\nover a small batch\ngt = ∂w\n1\n|Bt|\n∑\ni∈Bt\nf (xi, w)\n(12.5.2)\nLet’s see what this does to the statistical properties of gt: since both xt and also all elements\nof the minibatch Bt are drawn uniformly at random from the training set, the expectation\nof the gradient remains unchanged. The variance, on the other hand, is reduced signiﬁcantly.\nSince the minibatch gradient is composed of b def\n= |Bt| independent gradients which are being\naveraged, its standard deviation is reduced by a factor of b−1\n2 . This, by itself, is a good thing,\nsince it means that the updates are more reliably aligned with the full gradient.\nNaively this would indicate that choosing a large minibatch Bt would be universally desir-\nable. Alas, after some point, the additional reduction in standard deviation is minimal when\ncompared to the linear increase in computational cost. In practice we pick a minibatch that\nis large enough to oﬀer good computational eﬃciency while still ﬁtting into the memory of\na GPU. To illustrate the savings let’s have a look at some code. In it we perform the same\nmatrix-matrix multiplication, but this time broken up into “minibatches” of 64 columns at a\ntime.\ntimer.start()\nfor j in range(0, 256, 64):\nA[:, j:j+64] = torch.mm(B, C[:, j:j+64])\ntimer.stop()\nprint(f'performance in Gigaflops: block {0.03 / timer.times[3]:.3f}')\nperformance in Gigaflops: block 1.258\nAs we can see, the computation on the minibatch is essentially as eﬃcient as on the full matrix.\nA word of caution is in order. In Section 8.5 we used a type of regularization that was heavily\ndependent on the amount of variance in a minibatch. As we increase the latter, the variance\ndecreases and with it the beneﬁt of the noise-injection due to batch normalization. See e.g.,\nIoﬀe (2017) for details on how to rescale and compute the appropriate terms.\n12.5.3 Reading the Dataset\nLet’s have a look at how minibatches are eﬃciently generated from data. In the following we\nuse a dataset developed by NASA to test the wing noise from diﬀerent aircraft172 to compare\nthese optimization algorithms. For convenience we only use the ﬁrst 1, 500 examples. The\ndata is whitened for preprocessing, i.e., we remove the mean and rescale the variance to 1\nper coordinate.\n#@save\nd2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n'76e5be1548fd8222e5074cf0faae75edff8cf93f')\n(continues on next page)\n\n522\nOptimization Algorithms\n(continued from previous page)\n#@save\ndef get_data_ch11(batch_size=10, n=1500):\ndata = np.genfromtxt(d2l.download('airfoil'),\ndtype=np.float32, delimiter='\\t')\ndata = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0))\ndata_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\nbatch_size, is_train=True)\nreturn data_iter, data.shape[1]-1\n12.5.4 Implementation from Scratch\nRecall the minibatch stochastic gradient descent implementation from Section 3.4. In the fol-\nlowing we provide a slightly more general implementation. For convenience it has the same\ncall signature as the other optimization algorithms introduced later in this chapter. Specif-\nically, we add the status input states and place the hyperparameter in dictionary hyper-\nparams. In addition, we will average the loss of each minibatch example in the training\nfunction, so the gradient in the optimization algorithm does not need to be divided by the\nbatch size.\ndef sgd(params, states, hyperparams):\nfor p in params:\np.data.sub_(hyperparams['lr'] * p.grad)\np.grad.data.zero_()\nNext, we implement a generic training function to facilitate the use of the other optimization\nalgorithms introduced later in this chapter. It initializes a linear regression model and can\nbe used to train the model with minibatch stochastic gradient descent and other algorithms\nintroduced subsequently.\n#@save\ndef train_ch11(trainer_fn, states, hyperparams, data_iter,\nfeature_dim, num_epochs=2):\n# Initialization\nw = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1),\nrequires_grad=True)\nb = torch.zeros((1), requires_grad=True)\nnet, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n# Train\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[0, num_epochs], ylim=[0.22, 0.35])\nn, timer = 0, d2l.Timer()\nfor _ in range(num_epochs):\nfor X, y in data_iter:\nl = loss(net(X), y).mean()\nl.backward()\ntrainer_fn([w, b], states, hyperparams)\nn += X.shape[0]\nif n % 200 == 0:\n(continues on next page)\n\n523\nMinibatch Stochastic Gradient Descent\n(continued from previous page)\ntimer.stop()\nanimator.add(n/X.shape[0]/len(data_iter),\n(d2l.evaluate_loss(net, data_iter, loss),))\ntimer.start()\nprint(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/\n,→epoch')\nreturn timer.cumsum(), animator.Y[0]\nLet’s see how optimization proceeds for batch gradient descent. This can be achieved by\nsetting the minibatch size to 1500 (i.e., to the total number of examples). As a result the\nmodel parameters are updated only once per epoch. There is little progress. In fact, after 6\nsteps progress stalls.\ndef train_sgd(lr, batch_size, num_epochs=2):\ndata_iter, feature_dim = get_data_ch11(batch_size)\nreturn train_ch11(\nsgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\ngd_res = train_sgd(1, 1500, 10)\nloss: 0.253, 0.028 sec/epoch\nWhen the batch size equals 1, we use stochastic gradient descent for optimization. For sim-\nplicity of implementation we picked a constant (albeit small) learning rate. In stochastic gra-\ndient descent, the model parameters are updated whenever an example is processed. In our\ncase this amounts to 1500 updates per epoch. As we can see, the decline in the value of\nthe objective function slows down after one epoch. Although both the procedures processed\n1500 examples within one epoch, stochastic gradient descent consumes more time than gra-\ndient descent in our experiment. This is because stochastic gradient descent updated the\nparameters more frequently and since it is less eﬃcient to process single observations one at\na time.\n\n524\nOptimization Algorithms\nsgd_res = train_sgd(0.005, 1)\nloss: 0.243, 0.954 sec/epoch\nFinally, when the batch size equals 100, we use minibatch stochastic gradient descent for\noptimization. The time required per epoch is shorter than the time needed for stochastic\ngradient descent and the time for batch gradient descent.\nmini1_res = train_sgd(.4, 100)\nloss: 0.246, 0.035 sec/epoch\nReducing the batch size to 10, the time for each epoch increases because the workload for\neach batch is less eﬃcient to execute.\nmini2_res = train_sgd(.05, 10)\n\n525\nMinibatch Stochastic Gradient Descent\nloss: 0.252, 0.109 sec/epoch\nNow we can compare the time vs. loss for the previous four experiments. As can be seen,\nalthough stochastic gradient descent converges faster than GD in terms of number of ex-\namples processed, it uses more time to reach the same loss than GD because computing the\ngradient example by example is not as eﬃcient. Minibatch stochastic gradient descent is able\nto trade-oﬀconvergence speed and computation eﬃciency. A minibatch size of 10 is more\neﬃcient than stochastic gradient descent; a minibatch size of 100 even outperforms GD in\nterms of runtime.\nd2l.set_figsize([6, 3])\nd2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),\n'time (sec)', 'loss', xlim=[1e-2, 10],\nlegend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\nd2l.plt.gca().set_xscale('log')\n12.5.5 Concise Implementation\nIn Gluon, we can use the Trainer class to call optimization algorithms. This is used to im-\nplement a generic training function. We will use this throughout the current chapter.\n\n526\nOptimization Algorithms\n#@save\ndef train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):\n# Initialization\nnet = nn.Sequential(nn.Linear(5, 1))\ndef init_weights(module):\nif type(module) == nn.Linear:\ntorch.nn.init.normal_(module.weight, std=0.01)\nnet.apply(init_weights)\noptimizer = trainer_fn(net.parameters(), **hyperparams)\nloss = nn.MSELoss(reduction='none')\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[0, num_epochs], ylim=[0.22, 0.35])\nn, timer = 0, d2l.Timer()\nfor _ in range(num_epochs):\nfor X, y in data_iter:\noptimizer.zero_grad()\nout = net(X)\ny = y.reshape(out.shape)\nl = loss(out, y)\nl.mean().backward()\noptimizer.step()\nn += X.shape[0]\nif n % 200 == 0:\ntimer.stop()\n# `MSELoss` computes squared error without the 1/2 factor\nanimator.add(n/X.shape[0]/len(data_iter),\n(d2l.evaluate_loss(net, data_iter, loss) / 2,))\ntimer.start()\nprint(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum()/num_epochs:.3f} sec/\n,→epoch')\nUsing Gluon to repeat the last experiment shows identical behavior.\ndata_iter, _ = get_data_ch11(10)\ntrainer = torch.optim.SGD\ntrain_concise_ch11(trainer, {'lr': 0.01}, data_iter)\nloss: 0.242, 0.128 sec/epoch\n12.5.6 Summary\n• Vectorization makes code more eﬃcient due to reduced overhead arising from the deep\nlearning framework and due to better memory locality and caching on CPUs and GPUs.\n• There is a trade-oﬀbetween statistical eﬃciency arising from stochastic gradient descent\nand computational eﬃciency arising from processing large batches of data at a time.\n• Minibatch stochastic gradient descent oﬀers the best of both worlds: computational and\nstatistical eﬃciency.\n\n527\nMinibatch Stochastic Gradient Descent\n173\n• In minibatch stochastic gradient descent we process batches of data obtained by a random\npermutation of the training data (i.e., each observation is processed only once per epoch,\nalbeit in random order).\n• It is advisable to decay the learning rates during training.\n• In general, minibatch stochastic gradient descent is faster than stochastic gradient descent\nand gradient descent for convergence to a smaller risk, when measured in terms of clock\ntime.\n12.5.7 Exercises\n1. Modify the batch size and learning rate and observe the rate of decline for the value of\nthe objective function and the time consumed in each epoch.\n2. Read the MXNet documentation and use the Trainer class set_learning_rate func-\ntion to reduce the learning rate of the minibatch stochastic gradient descent to 1/10 of its\nprevious value after each epoch.\n3. Compare minibatch stochastic gradient descent with a variant that actually samples with\nreplacement from the training set. What happens?\n4. An evil genie replicates your dataset without telling you (i.e., each observation occurs\ntwice and your dataset grows to twice its original size, but nobody told you). How does\nthe behavior of stochastic gradient descent, minibatch stochastic gradient descent and that\nof gradient descent change?\nDiscussions173.\n\n528\nOptimization Algorithms\n12.6 Momentum\nIn Section 12.4 we reviewed what happens when performing stochastic gradient descent,\ni.e., when performing optimization where only a noisy variant of the gradient is available. In\nparticular, we noticed that for noisy gradients we need to be extra cautious when it comes\nto choosing the learning rate in the face of noise. If we decrease it too rapidly, convergence\nstalls. If we are too lenient, we fail to converge to a good enough solution since noise keeps\non driving us away from optimality.\n12.6.1 Basics\nIn this section, we will explore more eﬀective optimization algorithms, especially for certain\ntypes of optimization problems that are common in practice.\nLeaky Averages\nThe previous section saw us discussing minibatch SGD as a means for accelerating computa-\ntion. It also had the nice side-eﬀect that averaging gradients reduced the amount of variance.\nThe minibatch stochastic gradient descent can be calculated by:\ngt,t−1 = ∂w\n1\n|Bt|\n∑\ni∈Bt\nf (xi, wt−1) =\n1\n|Bt|\n∑\ni∈Bt\nhi,t−1.\n(12.6.1)\nTo keep the notation simple, here we used hi,t−1 = ∂w f (xi, wt−1) as the stochastic gradient\ndescent for sample i using the weights updated at time t −1. It would be nice if we could\nbeneﬁt from the eﬀect of variance reduction even beyond averaging gradients on a mini-\nbatch. One option to accomplish this task is to replace the gradient computation by a “leaky\naverage”:\nvt = βvt−1 + gt,t−1\n(12.6.2)\nfor some β ∈(0, 1). This eﬀectively replaces the instantaneous gradient by one that is been\naveraged over multiple past gradients. v is called velocity. It accumulates past gradients sim-\nilar to how a heavy ball rolling down the objective function landscape integrates over past\nforces. To see what is happening in more detail let’s expand vt recursively into\nvt = β2vt−2 + βgt−1,t−2 + gt,t−1 = . . ., =\nt−1\n∑\nτ=0\nβτgt−τ,t−τ−1.\n(12.6.3)\nLarge β amounts to a long-range average, whereas small β amounts to only a slight correction\nrelative to a gradient method. The new gradient replacement no longer points into the direc-\ntion of steepest descent on a particular instance any longer but rather in the direction of a\nweighted average of past gradients. This allows us to realize most of the beneﬁts of averaging\n\n529\nMomentum\n174\nover a batch without the cost of actually computing the gradients on it. We will revisit this\naveraging procedure in more detail later.\nThe above reasoning formed the basis for what is now known as accelerated gradient methods,\nsuch as gradients with momentum. They enjoy the additional beneﬁt of being much more ef-\nfective in cases where the optimization problem is ill-conditioned (i.e., where there are some\ndirections where progress is much slower than in others, resembling a narrow canyon). Fur-\nthermore, they allow us to average over subsequent gradients to obtain more stable directions\nof descent. Indeed, the aspect of acceleration even for noise-free convex problems is one of\nthe key reasons why momentum works and why it works so well.\nAs one would expect, due to its eﬃcacy momentum is a well-studied subject in optimization\nfor deep learning and beyond. See e.g., the beautiful expository article174 by Goh (2017) for\nan in-depth analysis and interactive animation. It was proposed by Polyak (1964). Nesterov\n(2018) has a detailed theoretical discussion in the context of convex optimization. Momentum\nin deep learning has been known to be beneﬁcial for a long time. See e.g., the discussion by\nSutskever et al. (2013) for details.\nAn Ill-conditioned Problem\nTo get a better understanding of the geometric properties of the momentum method we revisit\ngradient descent, albeit with a signiﬁcantly less pleasant objective function. Recall that in\nSection 12.3 we used f (x) = x2\n1 + 2x2\n2, i.e., a moderately distorted ellipsoid objective. We\ndistort this function further by stretching it out in the x1 direction via\nf (x) = 0.1x2\n1 + 2x2\n2.\n(12.6.4)\nAs before f has its minimum at (0, 0). This function is very ﬂat in the direction of x1. Let’s\nsee what happens when we perform gradient descent as before on this new function. We pick\na learning rate of 0.4.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\neta = 0.4\ndef f_2d(x1, x2):\nreturn 0.1 * x1 ** 2 + 2 * x2 ** 2\ndef gd_2d(x1, x2, s1, s2):\nreturn (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\nepoch 20, x1: -0.943467, x2: -0.000073\nBy construction, the gradient in the x2 direction is much higher and changes much more\nrapidly than in the horizontal x1 direction. Thus we are stuck between two undesirable choices:\n\n530\nOptimization Algorithms\nif we pick a small learning rate we ensure that the solution does not diverge in the x2 direc-\ntion but we are saddled with slow convergence in the x1 direction. Conversely, with a large\nlearning rate we progress rapidly in the x1 direction but diverge in x2. The example below\nillustrates what happens even after a slight increase in learning rate from 0.4 to 0.6. Conver-\ngence in the x1 direction improves but the overall solution quality is much worse.\neta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))\nepoch 20, x1: -0.387814, x2: -1673.365109\nThe Momentum Method\nThe momentum method allows us to solve the gradient descent problem described above.\nLooking at the optimization trace above we might intuit that averaging gradients over the\npast would work well. After all, in the x1 direction this will aggregate well-aligned gradi-\nents, thus increasing the distance we cover with every step. Conversely, in the x2 direction\nwhere gradients oscillate, an aggregate gradient will reduce step size due to oscillations that\n\n531\nMomentum\ncancel each other out. Using vt instead of the gradient gt yields the following update equa-\ntions:\nvt ←βvt−1 + gt,t−1,\nxt ←xt−1 −ηtvt.\n(12.6.5)\nNote that for β = 0 we recover regular gradient descent. Before delving deeper into the math-\nematical properties let’s have a quick look at how the algorithm behaves in practice.\ndef momentum_2d(x1, x2, v1, v2):\nv1 = beta * v1 + 0.2 * x1\nv2 = beta * v2 + 4 * x2\nreturn x1 - eta * v1, x2 - eta * v2, v1, v2\neta, beta = 0.6, 0.5\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\nepoch 20, x1: 0.007188, x2: 0.002553\nAs we can see, even with the same learning rate that we used before, momentum still con-\nverges well. Let’s see what happens when we decrease the momentum parameter. Halving it\nto β = 0.25 leads to a trajectory that barely converges at all. Nonetheless, it is a lot better\nthan without momentum (when the solution diverges).\neta, beta = 0.6, 0.25\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))\nepoch 20, x1: -0.126340, x2: -0.186632\nNote that we can combine momentum with stochastic gradient descent and in particular,\nminibatch stochastic gradient descent. The only change is that in that case we replace the\ngradients gt,t−1 with gt. Last, for convenience we initialize v0 = 0 at time t = 0. Let’s look\nat what leaky averaging actually does to the updates.\n\n532\nOptimization Algorithms\nEﬀective Sample Weight\nRecall that vt = ∑t−1\nτ=0 βτgt−τ,t−τ−1. In the limit the terms add up to ∑∞\nτ=0 βτ =\n1\n1−β. In\nother words, rather than taking a step of size η in gradient descent or stochastic gradient\ndescent we take a step of size\nη\n1−β while at the same time, dealing with a potentially much\nbetter behaved descent direction. These are two beneﬁts in one. To illustrate how weighting\nbehaves for diﬀerent choices of β consider the diagram below.\nd2l.set_figsize()\nbetas = [0.95, 0.9, 0.6, 0]\nfor beta in betas:\nx = torch.arange(40).detach().numpy()\nd2l.plt.plot(x, beta ** x, label=f'beta = {beta:.2f}')\nd2l.plt.xlabel('time')\nd2l.plt.legend();\n12.6.2 Practical Experiments\nLet’s see how momentum works in practice, i.e., when used within the context of a proper\noptimizer. For this we need a somewhat more scalable implementation.\n\n533\nMomentum\nImplementation from Scratch\nCompared with (minibatch) stochastic gradient descent the momentum method needs to\nmaintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients (and\nvariables of the optimization problem). In the implementation below we call these variables\nstates.\ndef init_momentum_states(feature_dim):\nv_w = torch.zeros((feature_dim, 1))\nv_b = torch.zeros(1)\nreturn (v_w, v_b)\ndef sgd_momentum(params, states, hyperparams):\nfor p, v in zip(params, states):\nwith torch.no_grad():\nv[:] = hyperparams['momentum'] * v + p.grad\np[:] -= hyperparams['lr'] * v\np.grad.data.zero_()\nLet’s see how this works in practice.\ndef train_momentum(lr, momentum, num_epochs=2):\nd2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n{'lr': lr, 'momentum': momentum}, data_iter,\nfeature_dim, num_epochs)\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\ntrain_momentum(0.02, 0.5)\nloss: 0.246, 0.195 sec/epoch\nWhen we increase the momentum hyperparameter momentum to 0.9, it amounts to a signif-\nicantly larger eﬀective sample size of\n1\n1−0.9 = 10. We reduce the learning rate slightly to\n0.01 to keep matters under control.\n\n534\nOptimization Algorithms\ntrain_momentum(0.01, 0.9)\nloss: 0.254, 0.146 sec/epoch\nReducing the learning rate further addresses any issue of non-smooth optimization problems.\nSetting it to 0.005 yields good convergence properties.\ntrain_momentum(0.005, 0.9)\nloss: 0.247, 0.152 sec/epoch\nConcise Implementation\nThere is very little to do in Gluon since the standard sgd solver already had momentum built\nin. Setting matching parameters yields a very similar trajectory.\n\n535\nMomentum\ntrainer = torch.optim.SGD\nd2l.train_concise_ch11(trainer, {'lr': 0.005, 'momentum': 0.9}, data_iter)\nloss: 0.247, 0.142 sec/epoch\n12.6.3 Theoretical Analysis\nSo far the 2D example of f (x) = 0.1x2\n1 + 2x2\n2 seemed rather contrived. We will now see\nthat this is actually quite representative of the types of problem one might encounter, at least\nin the case of minimizing convex quadratic objective functions.\nQuadratic Convex Functions\nConsider the function\nh(x) = 1\n2x⊤Qx + x⊤c + b.\n(12.6.6)\nThis is a general quadratic function. For positive deﬁnite matrices Q ≻0, i.e., for matrices\nwith positive eigenvalues this has a minimizer at x∗= −Q−1c with minimum value b −\n1\n2c⊤Q−1c. Hence we can rewrite h as\nh(x) = 1\n2(x −Q−1c)⊤Q(x −Q−1c) + b −1\n2c⊤Q−1c.\n(12.6.7)\nThe gradient is given by ∂xh(x) = Q(x−Q−1c). That is, it is given by the distance between\nx and the minimizer, multiplied by Q. Consequently also the velocity is a linear combination\nof terms Q(xt −Q−1c).\nSince Q is positive deﬁnite it can be decomposed into its eigensystem via Q = O⊤ΛO for an\northogonal (rotation) matrix O and a diagonal matrix Λ of positive eigenvalues. This allows\n\n536\nOptimization Algorithms\nus to perform a change of variables from x to z def\n= O(x−Q−1c) to obtain a much simpliﬁed\nexpression:\nh(z) = 1\n2z⊤Λz + b′.\n(12.6.8)\nHere b′ = b −1\n2c⊤Q−1c. Since O is only an orthogonal matrix this does not perturb the\ngradients in a meaningful way. Expressed in terms of z gradient descent becomes\nzt = zt−1 −Λzt−1 = (I −Λ)zt−1.\n(12.6.9)\nThe important fact in this expression is that gradient descent does not mix between diﬀer-\nent eigenspaces. That is, when expressed in terms of the eigensystem of Q the optimization\nproblem proceeds in a coordinate-wise manner. This also holds for\nvt = βvt−1 + Λzt−1\nzt = zt−1 −η (βvt−1 + Λzt−1)\n= (I −ηΛ)zt−1 −ηβvt−1.\n(12.6.10)\nIn doing this we just proved the following theorem: gradient descent with and without mo-\nmentum for a convex quadratic function decomposes into coordinate-wise optimization in\nthe direction of the eigenvectors of the quadratic matrix.\nScalar Functions\nGiven the above result let’s see what happens when we minimize the function f (x) = λ\n2 x2.\nFor gradient descent we have\nxt+1 = xt −ηλxt = (1 −ηλ)xt.\n(12.6.11)\nWhenever |1 −ηλ| < 1 this optimization converges at an exponential rate since after t steps\nwe have xt = (1 −ηλ)t x0. This shows how the rate of convergence improves initially as we\nincrease the learning rate η until ηλ = 1. Beyond that things diverge and for ηλ > 2 the\noptimization problem diverges.\nlambdas = [0.1, 1, 10, 19]\neta = 0.1\nd2l.set_figsize((6, 4))\nfor lam in lambdas:\nt = torch.arange(20).detach().numpy()\nd2l.plt.plot(t, (1 - eta * lam) ** t, label=f'lambda = {lam:.2f}')\nd2l.plt.xlabel('time')\nd2l.plt.legend();\nTo analyze convergence in the case of momentum we begin by rewriting the update equations\nin terms of two scalars: one for x and one for velocity v. This yields:\n[vt+1\nxt+1\n]\n=\n[ β\nλ\n−ηβ\n(1 −ηλ)\n] [vt\nxt\n]\n= R(β, η, λ)\n[vt\nxt\n]\n.\n(12.6.12)\n\n537\nMomentum\n175\nWe used R to denote the 2×2 governing convergence behavior. After t steps the initial choice\n[v0, x0] becomes R(β, η, λ)t[v0, x0]. Hence, it is up to the eigenvalues of R to determine\nthe speed of convergence. See the Distill post 175 of Goh (2017) for a great animation and\nFlammarion and Bach (2015) for a detailed analysis. One can show that 0 < ηλ < 2 + 2β\nvelocity converges. This is a larger range of feasible parameters when compared to 0 < ηλ <\n2 for gradient descent. It also suggests that in general large values of β are desirable. Further\ndetails require a fair amount of technical detail and we suggest that the interested reader\nconsult the original publications.\n12.6.4 Summary\n• Momentum replaces gradients with a leaky average over past gradients. This accelerates\nconvergence signiﬁcantly.\n• It is desirable for both noise-free gradient descent and (noisy) stochastic gradient descent.\n• Momentum prevents stalling of the optimization process that is much more likely to occur\nfor stochastic gradient descent.\n• The eﬀective number of gradients is given by\n1\n1−β due to exponentiated downweighting of\npast data.\n• In the case of convex quadratic problems this can be analyzed explicitly in detail.\n• Implementation is quite straightforward but it requires us to store an additional state vector\n(velocity v).\n12.6.5 Exercises\n\n538\nOptimization Algorithms\n176\n1. Use other combinations of momentum hyperparameters and learning rates and observe\nand analyze the diﬀerent experimental results.\n2. Try out gradient descent and momentum for a quadratic problem where you have multiple\neigenvalues, i.e., f (x) = 1\n2\n∑\ni λix2\ni , e.g., λi = 2−i. Plot how the values of x decrease for\nthe initialization xi = 1.\n3. Derive minimum value and minimizer for h(x) = 1\n2x⊤Qx + x⊤c + b.\n4. What changes when we perform stochastic gradient descent with momentum? What hap-\npens when we use minibatch stochastic gradient descent with momentum? Experiment\nwith the parameters?\nDiscussions176.\n12.7 Adagrad\nLet’s begin by considering learning problems with features that occur infrequently.\n12.7.1 Sparse Features and Learning Rates\nImagine that we are training a language model. To get good accuracy we typically want to\ndecrease the learning rate as we keep on training, usually at a rate of O(t−1\n2 ) or slower. Now\nconsider a model training on sparse features, i.e., features that occur only infrequently. This\nis common for natural language, e.g., it is a lot less likely that we will see the word precon-\nditioning than learning. However, it is also common in other areas such as computational\nadvertising and personalized collaborative ﬁltering. After all, there are many things that are\nof interest only for a small number of people.\nParameters associated with infrequent features only receive meaningful updates whenever\nthese features occur. Given a decreasing learning rate we might end up in a situation where\nthe parameters for common features converge rather quickly to their optimal values, whereas\nfor infrequent features we are still short of observing them suﬃciently frequently before their\noptimal values can be determined. In other words, the learning rate either decreases too slowly\nfor frequent features or too quickly for infrequent ones.\nA possible hack to redress this issue would be to count the number of times we see a particular\nfeature and to use this as a clock for adjusting learning rates. That is, rather than choosing\na learning rate of the form η =\nη0\n√t+c we could use ηi =\nη0\n√\ns(i,t)+c. Here s(i, t) counts the\nnumber of nonzeros for feature i that we have observed up to time t. This is actually quite\neasy to implement at no meaningful overhead. However, it fails whenever we do not quite\nhave sparsity but rather just data where the gradients are often very small and only rarely\n\n539\nAdagrad\nlarge. After all, it is unclear where one would draw the line between something that qualiﬁes\nas an observed feature or not.\nAdagrad by Duchi et al. (2011) addresses this by replacing the rather crude counter s(i, t) by\nan aggregate of the squares of previously observed gradients. In particular, it uses s(i, t+1) =\ns(i, t) + (∂i f (x))2 as a means to adjust the learning rate. This has two beneﬁts: ﬁrst, we no\nlonger need to decide just when a gradient is large enough. Second, it scales automatically\nwith the magnitude of the gradients. Coordinates that routinely correspond to large gradients\nare scaled down signiﬁcantly, whereas others with small gradients receive a much more gentle\ntreatment. In practice this leads to a very eﬀective optimization procedure for computational\nadvertising and related problems. But this hides some of the additional beneﬁts inherent in\nAdagrad that are best understood in the context of preconditioning.\n12.7.2 Preconditioning\nConvex optimization problems are good for analyzing the characteristics of algorithms. After\nall, for most nonconvex problems it is diﬃcult to derive meaningful theoretical guarantees,\nbut intuition and insight often carry over. Let’s look at the problem of minimizing f (x) =\n1\n2x⊤Qx + c⊤x + b.\nAs we saw in Section 12.6, it is possible to rewrite this problem in terms of its eigendecom-\nposition Q = U⊤ΛU to arrive at a much simpliﬁed problem where each coordinate can be\nsolved individually:\nf (x) = ¯f (¯x) = 1\n2 ¯x⊤Λ¯x + ¯c⊤¯x + b.\n(12.7.1)\nHere we used ¯x = Ux and consequently ¯c = Uc. The modiﬁed problem has as its minimizer\n¯x = −Λ−1¯c and minimum value −1\n2¯c⊤Λ−1¯c + b. This is much easier to compute since Λ\nis a diagonal matrix containing the eigenvalues of Q.\nIf we perturb c slightly we would hope to ﬁnd only slight changes in the minimizer of f .\nUnfortunately this is not the case. While slight changes in c lead to equally slight changes in ¯c,\nthis is not the case for the minimizer of f (and of ¯f respectively). Whenever the eigenvalues\nΛi are large we will see only small changes in ¯xi and in the minimum of ¯f . Conversely,\nfor small Λi changes in ¯xi can be dramatic. The ratio between the largest and the smallest\neigenvalue is called the condition number of an optimization problem.\nκ = Λ1\nΛd\n.\n(12.7.2)\nIf the condition number κ is large, it is diﬃcult to solve the optimization problem accurately.\nWe need to ensure that we are careful in getting a large dynamic range of values right. Our\nanalysis leads to an obvious, albeit somewhat naive question: couldn’t we simply “ﬁx” the\nproblem by distorting the space such that all eigenvalues are 1. In theory this is quite easy:\nwe only need the eigenvalues and eigenvectors of Q to rescale the problem from x to one in\nz def\n= Λ\n1\n2 Ux. In the new coordinate system x⊤Qx could be simpliﬁed to ∥z∥2. Alas, this is\na rather impractical suggestion. Computing eigenvalues and eigenvectors is in general much\nmore expensive than solving the actual problem.\n\n540\nOptimization Algorithms\nWhile computing eigenvalues exactly might be expensive, guessing them and computing them\neven somewhat approximately may already be a lot better than not doing anything at all. In\nparticular, we could use the diagonal entries of Q and rescale it accordingly. This is much\ncheaper than computing eigenvalues.\n˜Q = diag−1\n2 (Q)Qdiag−1\n2 (Q).\n(12.7.3)\nIn this case we have ˜Qij = Qij/\n√\nQiiQj j and speciﬁcally ˜Qii = 1 for all i. In most cases this\nsimpliﬁes the condition number considerably. For instance, the cases we discussed previously,\nthis would entirely eliminate the problem at hand since the problem is axis aligned.\nUnfortunately we face yet another problem: in deep learning we typically do not even have\naccess to the second derivative of the objective function: for x ∈Rd the second derivative\neven on a minibatch may require O(d2) space and work to compute, thus making it practically\ninfeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal of the\nHessian that is both relatively cheap to compute and eﬀective—the magnitude of the gradient\nitself.\nIn order to see why this works, let’s look at ¯f (¯x). We have that\n∂¯x ¯f (¯x) = Λ¯x + ¯c = Λ (¯x −¯x0),\n(12.7.4)\nwhere ¯x0 is the minimizer of ¯f . Hence the magnitude of the gradient depends both on Λ and\nthe distance from optimality. If ¯x −¯x0 did not change, this would be all that is needed. After\nall, in this case the magnitude of the gradient ∂¯x ¯f (¯x) suﬃces. Since AdaGrad is a stochastic\ngradient descent algorithm, we will see gradients with nonzero variance even at optimality.\nAs a result we can safely use the variance of the gradients as a cheap proxy for the scale of the\nHessian. A thorough analysis is beyond the scope of this section (it would be several pages).\nWe refer the reader to (Duchi et al., 2011) for details.\n12.7.3 The Algorithm\nLet’s formalize the discussion from above. We use the variable st to accumulate past gradient\nvariance as follows.\ngt = ∂wl(yt, f (xt, w)),\nst = st−1 + g2\nt ,\nwt = wt−1 −\nη\n√st + ϵ · gt.\n(12.7.5)\nHere the operation are applied coordinate wise. That is, v2 has entries v2\ni . Likewise\n1\n√v has\nentries\n1\n√vi and u · v has entries uivi. As before η is the learning rate and ϵ is an additive\nconstant that ensures that we do not divide by 0. Last, we initialize s0 = 0.\nJust like in the case of momentum we need to keep track of an auxiliary variable, in this\ncase to allow for an individual learning rate per coordinate. This does not increase the cost\nof Adagrad signiﬁcantly relative to SGD, simply since the main cost is typically to compute\nl(yt, f (xt, w)) and its derivative.\n\n541\nAdagrad\nNote that accumulating squared gradients in st means that st grows essentially at linear rate\n(somewhat slower than linearly in practice, since the gradients initially diminish). This leads\nto an O(t−1\n2 ) learning rate, albeit adjusted on a per coordinate basis. For convex problems\nthis is perfectly adequate. In deep learning, though, we might want to decrease the learning\nrate rather more slowly. This led to a number of Adagrad variants that we will discuss in the\nsubsequent chapters. For now let’s see how it behaves in a quadratic convex problem. We use\nthe same problem as before:\nf (x) = 0.1x2\n1 + 2x2\n2.\n(12.7.6)\nWe are going to implement Adagrad using the same learning rate previously, i.e., η = 0.4. As\nwe can see, the iterative trajectory of the independent variable is smoother. However, due to\nthe cumulative eﬀect of st, the learning rate continuously decays, so the independent variable\ndoes not move as much during later stages of iteration.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch as d2l\ndef adagrad_2d(x1, x2, s1, s2):\neps = 1e-6\ng1, g2 = 0.2 * x1, 4 * x2\ns1 += g1 ** 2\ns2 += g2 ** 2\nx1 -= eta / math.sqrt(s1 + eps) * g1\nx2 -= eta / math.sqrt(s2 + eps) * g2\nreturn x1, x2, s1, s2\ndef f_2d(x1, x2):\nreturn 0.1 * x1 ** 2 + 2 * x2 ** 2\neta = 0.4\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))\nepoch 20, x1: -2.382563, x2: -0.158591\n\n542\nOptimization Algorithms\nAs we increase the learning rate to 2 we see much better behavior. This already indicates that\nthe decrease in learning rate might be rather aggressive, even in the noise-free case and we\nneed to ensure that parameters converge appropriately.\neta = 2\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))\nepoch 20, x1: -0.002295, x2: -0.000000\n12.7.4 Implementation from Scratch\nJust like the momentum method, Adagrad needs to maintain a state variable of the same\nshape as the parameters.\ndef init_adagrad_states(feature_dim):\ns_w = torch.zeros((feature_dim, 1))\ns_b = torch.zeros(1)\nreturn (s_w, s_b)\ndef adagrad(params, states, hyperparams):\neps = 1e-6\nfor p, s in zip(params, states):\nwith torch.no_grad():\ns[:] += torch.square(p.grad)\np[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)\np.grad.data.zero_()\nCompared to the experiment in Section 12.5 we use a larger learning rate to train the model.\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adagrad, init_adagrad_states(feature_dim),\n{'lr': 0.1}, data_iter, feature_dim);\n\n543\nAdagrad\nloss: 0.244, 0.232 sec/epoch\n12.7.5 Concise Implementation\nUsing the Trainer instance of the algorithm adagrad, we can invoke the Adagrad algorithm\nin Gluon.\ntrainer = torch.optim.Adagrad\nd2l.train_concise_ch11(trainer, {'lr': 0.1}, data_iter)\nloss: 0.242, 0.155 sec/epoch\n12.7.6 Summary\n• Adagrad decreases the learning rate dynamically on a per-coordinate basis.\n• It uses the magnitude of the gradient as a means of adjusting how quickly progress is\n\n544\nOptimization Algorithms\n177\n178\nachieved - coordinates with large gradients are compensated with a smaller learning\nrate.\n• Computing the exact second derivative is typically infeasible in deep learning problems\ndue to memory and computational constraints. The gradient can be a useful proxy.\n• If the optimization problem has a rather uneven structure Adagrad can help mitigate the\ndistortion.\n• Adagrad is particularly eﬀective for sparse features where the learning rate needs to de-\ncrease more slowly for infrequently occurring terms.\n• On deep learning problems Adagrad can sometimes be too aggressive in reducing learning\nrates. We will discuss strategies for mitigating this in the context of Section 12.10.\n12.7.7 Exercises\n1. Prove that for an orthogonal matrix U and a vector c the following holds: ∥c −ﬃ∥2 =\n∥Uc −Uﬃ∥2. Why does this mean that the magnitude of perturbations does not change\nafter an orthogonal change of variables?\n2. Try out Adagrad for f (x) = 0.1x2\n1 + 2x2\n2 and also for the objective function was rotated\nby 45 degrees, i.e., f (x) = 0.1(x1 + x2)2 + 2(x1 −x2)2. Does it behave diﬀerently?\n3. Prove Gerschgorin’s circle theorem 177 which states that eigenvalues λi of a matrix M\nsatisfy |λi −Mj j| ≤∑\nk,j |Mjk| for at least one choice of j.\n4. What does Gerschgorin’s theorem tell us about the eigenvalues of the diagonally precon-\nditioned matrix diag−1\n2 (M)Mdiag−1\n2 (M)?\n5. Try out Adagrad for a proper deep network, such as Section 7.6 when applied to Fashion-\nMNIST.\n6. How would you need to modify Adagrad to achieve a less aggressive decay in learning\nrate?\nDiscussions178.\n12.8 RMSProp\nOne of the key issues in Section 12.7 is that the learning rate decreases at a predeﬁned\nschedule of eﬀectively O(t−1\n2 ). While this is generally appropriate for convex problems,\nit might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet,\nthe coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\nTieleman and Hinton (2012) proposed the RMSProp algorithm as a simple ﬁx to decouple\n\n545\nRMSProp\nrate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accumu-\nlates the squares of the gradient gt into a state vector st = st−1 + g2\nt . As a result st keeps on\ngrowing without bound due to the lack of normalization, essentially linearly as the algorithm\nconverges.\nOne way of ﬁxing this problem would be to use st/t. For reasonable distributions of gt this\nwill converge. Unfortunately it might take a very long time until the limit behavior starts to\nmatter since the procedure remembers the full trajectory of values. An alternative is to use a\nleaky average in the same way we used in the momentum method, i.e., st ←γst−1+(1−γ)g2\nt\nfor some parameter γ > 0. Keeping all other parts unchanged yields RMSProp.\n12.8.1 The Algorithm\nLet’s write out the equations in detail.\nst ←γst−1 + (1 −γ)g2\nt ,\nxt ←xt−1 −\nη\n√st + ϵ ⊙gt.\n(12.8.1)\nThe constant ϵ > 0 is typically set to 10−6 to ensure that we do not suﬀer from division by\nzero or overly large step sizes. Given this expansion we are now free to control the learning\nrate η independently of the scaling that is applied on a per-coordinate basis. In terms of leaky\naverages we can apply the same reasoning as previously applied in the case of the momentum\nmethod. Expanding the deﬁnition of st yields\nst = (1 −γ)g2\nt + γst−1\n= (1 −γ) (g2\nt + γg2\nt−1 + γ2gt−2 + . . ., ) .\n(12.8.2)\nAs before in Section 12.6 we use 1 + γ + γ2 + . . ., =\n1\n1−γ. Hence the sum of weights is\nnormalized to 1 with a half-life time of an observation of γ−1. Let’s visualize the weights for\nthe past 40 time steps for various choices of γ.\nimport math\nimport torch\nfrom d2l import torch as d2l\nd2l.set_figsize()\ngammas = [0.95, 0.9, 0.8, 0.7]\nfor gamma in gammas:\nx = torch.arange(40).detach().numpy()\nd2l.plt.plot(x, (1-gamma) * gamma ** x, label=f'gamma = {gamma:.2f}')\nd2l.plt.xlabel('time');\n12.8.2 Implementation from Scratch\nAs before we use the quadratic function f (x) = 0.1x2\n1 + 2x2\n2 to observe the trajectory of\nRMSProp. Recall that in Section 12.7, when we used Adagrad with a learning rate of 0.4,\n\n546\nOptimization Algorithms\nthe variables moved only very slowly in the later stages of the algorithm since the learning\nrate decreased too quickly. Since η is controlled separately this does not happen with RM-\nSProp.\ndef rmsprop_2d(x1, x2, s1, s2):\ng1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\ns1 = gamma * s1 + (1 - gamma) * g1 ** 2\ns2 = gamma * s2 + (1 - gamma) * g2 ** 2\nx1 -= eta / math.sqrt(s1 + eps) * g1\nx2 -= eta / math.sqrt(s2 + eps) * g2\nreturn x1, x2, s1, s2\ndef f_2d(x1, x2):\nreturn 0.1 * x1 ** 2 + 2 * x2 ** 2\neta, gamma = 0.4, 0.9\nd2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))\nepoch 20, x1: -0.010599, x2: 0.000000\nNext, we implement RMSProp to be used in a deep network. This is equally straightfor-\nward.\n\n547\nRMSProp\ndef init_rmsprop_states(feature_dim):\ns_w = torch.zeros((feature_dim, 1))\ns_b = torch.zeros(1)\nreturn (s_w, s_b)\ndef rmsprop(params, states, hyperparams):\ngamma, eps = hyperparams['gamma'], 1e-6\nfor p, s in zip(params, states):\nwith torch.no_grad():\ns[:] = gamma * s + (1 - gamma) * torch.square(p.grad)\np[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)\np.grad.data.zero_()\nWe set the initial learning rate to 0.01 and the weighting term γ to 0.9. That is, s aggregates\non average over the past 1/(1 −γ) = 10 observations of the square gradient.\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),\n{'lr': 0.01, 'gamma': 0.9}, data_iter, feature_dim);\nloss: 0.243, 0.177 sec/epoch\n12.8.3 Concise Implementation\nSince RMSProp is a rather popular algorithm it is also available in the Trainer instance.\nAll we need to do is instantiate it using an algorithm named rmsprop, assigning γ to the\nparameter gamma1.\ntrainer = torch.optim.RMSprop\nd2l.train_concise_ch11(trainer, {'lr': 0.01, 'alpha': 0.9},\ndata_iter)\n\n548\nOptimization Algorithms\n179\nloss: 0.243, 0.138 sec/epoch\n12.8.4 Summary\n• RMSProp is very similar to Adagrad insofar as both use the square of the gradient to scale\ncoeﬃcients.\n• RMSProp shares with momentum the leaky averaging. However, RMSProp uses the tech-\nnique to adjust the coeﬃcient-wise preconditioner.\n• The learning rate needs to be scheduled by the experimenter in practice.\n• The coeﬃcient γ determines how long the history is when adjusting the per-coordinate\nscale.\n12.8.5 Exercises\n1. What happens experimentally if we set γ = 1? Why?\n2. Rotate the optimization problem to minimize f (x) = 0.1(x1 + x2)2 +2(x1 −x2)2. What\nhappens to the convergence?\n3. Try out what happens to RMSProp on a real machine learning problem, such as training\non Fashion-MNIST. Experiment with diﬀerent choices for adjusting the learning rate.\n4. Would you want to adjust γ as optimization progresses? How sensitive is RMSProp to\nthis?\nDiscussions179.\n\n549\nAdadelta\n12.9 Adadelta\nAdadelta is yet another variant of AdaGrad (Section 12.7). The main diﬀerence lies in the fact\nthat it decreases the amount by which the learning rate is adaptive to coordinates. Moreover,\ntraditionally it referred to as not having a learning rate since it uses the amount of change\nitself as calibration for future change. The algorithm was proposed in Zeiler (2012). It is\nfairly straightforward, given the discussion of previous algorithms so far.\n12.9.1 The Algorithm\nIn a nutshell, Adadelta uses two state variables, st to store a leaky average of the second\nmoment of the gradient and ∆xt to store a leaky average of the second moment of the change\nof parameters in the model itself. Note that we use the original notation and naming of the\nauthors for compatibility with other publications and implementations (there is no other real\nreason why one should use diﬀerent Greek variables to indicate a parameter serving the same\npurpose in momentum, Adagrad, RMSProp, and Adadelta).\nHere are the technical details of Adadelta. Given the parameter du jour is ρ, we obtain the\nfollowing leaky updates similarly to Section 12.8:\nst = ρst−1 + (1 −ρ)g2\nt .\n(12.9.1)\nThe diﬀerence to Section 12.8 is that we perform updates with the rescaled gradient g′\nt,\ni.e.,\nxt = xt−1 −g′\nt.\n(12.9.2)\nSo what is the rescaled gradient g′\nt? We can calculate it as follows:\ng′\nt =\n√∆xt−1 + ϵ\n√st + ϵ\n⊙gt,\n(12.9.3)\nwhere ∆xt−1 is the leaky average of the squared rescaled gradients g′\nt. We initialize ∆x0 to\nbe 0 and update it at each step with g′\nt, i.e.,\n∆xt = ρ∆xt−1 + (1 −ρ)g′\nt\n2,\n(12.9.4)\nand ϵ (a small value such as 10−5) is added to maintain numerical stability.\n12.9.2 Implementation\nAdadelta needs to maintain two state variables for each variable, st and ∆xt. This yields the\nfollowing implementation.\n\n550\nOptimization Algorithms\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\ndef init_adadelta_states(feature_dim):\ns_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\ndelta_w, delta_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\nreturn ((s_w, delta_w), (s_b, delta_b))\ndef adadelta(params, states, hyperparams):\nrho, eps = hyperparams['rho'], 1e-5\nfor p, (s, delta) in zip(params, states):\nwith torch.no_grad():\n# In-place updates via [:]\ns[:] = rho * s + (1 - rho) * torch.square(p.grad)\ng = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad\np[:] -= g\ndelta[:] = rho * delta + (1 - rho) * g * g\np.grad.data.zero_()\nChoosing ρ = 0.9 amounts to a half-life time of 10 for each parameter update. This tends\nto work quite well. We get the following behavior.\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adadelta, init_adadelta_states(feature_dim),\n{'rho': 0.9}, data_iter, feature_dim);\nloss: 0.244, 0.221 sec/epoch\nFor a concise implementation we simply use the Adadelta algorithm from high-level APIs.\nThis yields the following one-liner for a much more compact invocation.\ntrainer = torch.optim.Adadelta\nd2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)\n\n551\nAdadelta\n180\nloss: 0.243, 0.164 sec/epoch\n12.9.3 Summary\n• Adadelta has no learning rate parameter. Instead, it uses the rate of change in the param-\neters itself to adapt the learning rate.\n• Adadelta requires two state variables to store the second moments of gradient and the\nchange in parameters.\n• Adadelta uses leaky averages to keep a running estimate of the appropriate statistics.\n12.9.4 Exercises\n1. Adjust the value of ρ. What happens?\n2. Show how to implement the algorithm without the use of g′\nt. Why might this be a good\nidea?\n3. Is Adadelta really learning rate free? Could you ﬁnd optimization problems that break\nAdadelta?\n4. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.\nDiscussions180.\n\n552\nOptimization Algorithms\n12.10 Adam\nIn the discussions leading up to this section we encountered a number of techniques for eﬃ-\ncient optimization. Let’s recap them in detail here:\n• We saw that Section 12.4 is more eﬀective than Gradient Descent when solving optimiza-\ntion problems, e.g., due to its inherent resilience to redundant data.\n• We saw that Section 12.5 aﬀords signiﬁcant additional eﬃciency arising from vectorization,\nusing larger sets of observations in one minibatch. This is the key to eﬃcient multi-\nmachine, multi-GPU and overall parallel processing.\n• Section 12.6 added a mechanism for aggregating a history of past gradients to accelerate\nconvergence.\n• Section 12.7 used per-coordinate scaling to allow for a computationally eﬃcient precon-\nditioner.\n• Section 12.8 decoupled per-coordinate scaling from a learning rate adjustment.\nAdam (Kingma and Ba, 2014) combines all these techniques into one eﬃcient learning algo-\nrithm. As expected, this is an algorithm that has become rather popular as one of the more\nrobust and eﬀective optimization algorithms to use in deep learning. It is not without issues,\nthough. In particular, (Reddi et al., 2019) show that there are situations where Adam can\ndiverge due to poor variance control. In a follow-up work Zaheer et al. (2018) proposed a\nhotﬁx to Adam, called Yogi which addresses these issues. More on this later. For now let’s\nreview the Adam algorithm.\n12.10.1 The Algorithm\nOne of the key components of Adam is that it uses exponential weighted moving averages\n(also known as leaky averaging) to obtain an estimate of both the momentum and also the\nsecond moment of the gradient. That is, it uses the state variables\nvt ←β1vt−1 + (1 −β1)gt,\nst ←β2st−1 + (1 −β2)g2\nt .\n(12.10.1)\nHere β1 and β2 are nonnegative weighting parameters. Common choices for them are β1 =\n0.9 and β2 = 0.999. That is, the variance estimate moves much more slowly than the mo-\nmentum term. Note that if we initialize v0 = s0 = 0 we have a signiﬁcant amount of bias\ninitially towards smaller values. This can be addressed by using the fact that ∑t−1\ni=0 βi = 1−βt\n1−β\nto re-normalize terms. Correspondingly the normalized state variables are given by\nˆvt =\nvt\n1 −βt\n1\nand ˆst =\nst\n1 −βt\n2\n.\n(12.10.2)\n\n553\nAdam\nArmed with the proper estimates we can now write out the update equations. First, we rescale\nthe gradient in a manner very much akin to that of RMSProp to obtain\ng′\nt =\nηˆvt\n√ˆst + ϵ\n.\n(12.10.3)\nUnlike RMSProp our update uses the momentum ˆvt rather than the gradient itself. More-\nover, there is a slight cosmetic diﬀerence as the rescaling happens using\n1\n√ˆst +ϵ instead of\n1\n√ˆst +ϵ . The former works arguably slightly better in practice, hence the deviation from RM-\nSProp. Typically we pick ϵ = 10−6 for a good trade-oﬀbetween numerical stability and\nﬁdelity.\nNow we have all the pieces in place to compute updates. This is slightly anticlimactic and we\nhave a simple update of the form\nxt ←xt−1 −g′\nt.\n(12.10.4)\nReviewing the design of Adam its inspiration is clear. Momentum and scale are clearly visible\nin the state variables. Their rather peculiar deﬁnition forces us to debias terms (this could be\nﬁxed by a slightly diﬀerent initialization and update condition). Second, the combination of\nboth terms is pretty straightforward, given RMSProp. Last, the explicit learning rate η allows\nus to control the step length to address issues of convergence.\n12.10.2 Implementation\nImplementing Adam from scratch is not very daunting. For convenience we store the time\nstep counter t in the hyperparams dictionary. Beyond that all is straightforward.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\ndef init_adam_states(feature_dim):\nv_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\ns_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\nreturn ((v_w, s_w), (v_b, s_b))\ndef adam(params, states, hyperparams):\nbeta1, beta2, eps = 0.9, 0.999, 1e-6\nfor p, (v, s) in zip(params, states):\nwith torch.no_grad():\nv[:] = beta1 * v + (1 - beta1) * p.grad\ns[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)\nv_bias_corr = v / (1 - beta1 ** hyperparams['t'])\ns_bias_corr = s / (1 - beta2 ** hyperparams['t'])\np[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n+ eps)\np.grad.data.zero_()\nhyperparams['t'] += 1\n\n554\nOptimization Algorithms\nWe are ready to use Adam to train the model. We use a learning rate of η = 0.01.\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(adam, init_adam_states(feature_dim),\n{'lr': 0.01, 't': 1}, data_iter, feature_dim);\nloss: 0.246, 0.224 sec/epoch\nA more concise implementation is straightforward since adam is one of the algorithms pro-\nvided as part of the Gluon trainer optimization library. Hence we only need to pass con-\nﬁguration parameters for an implementation in Gluon.\ntrainer = torch.optim.Adam\nd2l.train_concise_ch11(trainer, {'lr': 0.01}, data_iter)\nloss: 0.243, 0.182 sec/epoch\n12.10.3 Yogi\n\n555\nAdam\nOne of the problems of Adam is that it can fail to converge even in convex settings when\nthe second moment estimate in st blows up. As a ﬁx Zaheer et al. (2018) proposed a reﬁned\nupdate (and initialization) for st. To understand what’s going on, let’s rewrite the Adam update\nas follows:\nst ←st−1 + (1 −β2) (g2\nt −st−1\n) .\n(12.10.5)\nWhenever g2\nt has high variance or updates are sparse, st might forget past values too quickly.\nA possible ﬁx for this is to replace g2\nt −st−1 by g2\nt ⊙sgn(g2\nt −st−1). Now the magnitude of the\nupdate no longer depends on the amount of deviation. This yields the Yogi updates\nst ←st−1 + (1 −β2)g2\nt ⊙sgn(g2\nt −st−1).\n(12.10.6)\nThe authors furthermore advise to initialize the momentum on a larger initial batch rather\nthan just initial pointwise estimate. We omit the details since they are not material to the\ndiscussion and since even without this convergence remains pretty good.\ndef yogi(params, states, hyperparams):\nbeta1, beta2, eps = 0.9, 0.999, 1e-3\nfor p, (v, s) in zip(params, states):\nwith torch.no_grad():\nv[:] = beta1 * v + (1 - beta1) * p.grad\ns[:] = s + (1 - beta2) * torch.sign(\ntorch.square(p.grad) - s) * torch.square(p.grad)\nv_bias_corr = v / (1 - beta1 ** hyperparams['t'])\ns_bias_corr = s / (1 - beta2 ** hyperparams['t'])\np[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)\n+ eps)\np.grad.data.zero_()\nhyperparams['t'] += 1\ndata_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n{'lr': 0.01, 't': 1}, data_iter, feature_dim);\nloss: 0.245, 0.207 sec/epoch\n\n556\nOptimization Algorithms\n181\n12.10.4 Summary\n• Adam combines features of many optimization algorithms into a fairly robust update rule.\n• Created on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic\ngradient.\n• Adam uses bias correction to adjust for a slow startup when estimating momentum and a\nsecond moment.\n• For gradients with signiﬁcant variance we may encounter issues with convergence. They\ncan be amended by using larger minibatches or by switching to an improved estimate\nfor st. Yogi oﬀers such an alternative.\n12.10.5 Exercises\n1. Adjust the learning rate and observe and analyze the experimental results.\n2. Can you rewrite momentum and second moment updates such that it does not require bias\ncorrection?\n3. Why do you need to reduce the learning rate η as we converge?\n4. Try to construct a case for which Adam diverges and Yogi converges?\nDiscussions181.\n12.11 Learning Rate Scheduling\nSo far we primarily focused on optimization algorithms for how to update the weight vectors\nrather than on the rate at which they are being updated. Nonetheless, adjusting the learning\nrate is often just as important as the actual algorithm. There are a number of aspects to\nconsider:\n• Most obviously the magnitude of the learning rate matters. If it is too large, optimization\ndiverges, if it is too small, it takes too long to train or we end up with a suboptimal result.\nWe saw previously that the condition number of the problem matters (see e.g., Section\n12.6 for details). Intuitively it is the ratio of the amount of change in the least sensitive\ndirection vs. the most sensitive one.\n• Secondly, the rate of decay is just as important. If the learning rate remains large we may\nsimply end up bouncing around the minimum and thus not reach optimality. Section\n12.5 discussed this in some detail and we analyzed performance guarantees in Section\n12.4. In short, we want the rate to decay, but probably more slowly than O(t−1\n2 ) which\nwould be a good choice for convex problems.\n\n557\nLearning Rate Scheduling\n• Another aspect that is equally important is initialization. This pertains both to how the\nparameters are set initially (review Section 5.4 for details) and also how they evolve\ninitially. This goes under the moniker of warmup, i.e., how rapidly we start moving\ntowards the solution initially. Large steps in the beginning might not be beneﬁcial, in\nparticular since the initial set of parameters is random. The initial update directions\nmight be quite meaningless, too.\n• Lastly, there are a number of optimization variants that perform cyclical learning rate ad-\njustment. This is beyond the scope of the current chapter. We recommend the reader to\nreview details in Izmailov et al. (2018), e.g., how to obtain better solutions by averaging\nover an entire path of parameters.\nGiven the fact that there is a lot of detail needed to manage learning rates, most deep learning\nframeworks have tools to deal with this automatically. In the current chapter we will review\nthe eﬀects that diﬀerent schedules have on accuracy and also show how this can be managed\neﬃciently via a learning rate scheduler.\n12.11.1 Toy Problem\nWe begin with a toy problem that is cheap enough to compute easily, yet suﬃciently nontrivial\nto illustrate some of the key aspects. For that we pick a slightly modernized version of LeNet\n(relu instead of sigmoid activation, MaxPooling rather than AveragePooling), as applied\nto Fashion-MNIST. Moreover, we hybridize the network for performance. Since most of the\ncode is standard we just introduce the basics without further detailed discussion. See Chapter\n7 for a refresher as needed.\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch.optim import lr_scheduler\nfrom d2l import torch as d2l\ndef net_fn():\nmodel = nn.Sequential(\nnn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),\nnn.MaxPool2d(kernel_size=2, stride=2),\nnn.Conv2d(6, 16, kernel_size=5), nn.ReLU(),\nnn.MaxPool2d(kernel_size=2, stride=2),\nnn.Flatten(),\nnn.Linear(16 * 5 * 5, 120), nn.ReLU(),\nnn.Linear(120, 84), nn.ReLU(),\nnn.Linear(84, 10))\nreturn model\nloss = nn.CrossEntropyLoss()\ndevice = d2l.try_gpu()\n(continues on next page)\n\n558\nOptimization Algorithms\n(continued from previous page)\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler=None):\nnet.to(device)\nanimator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\nlegend=['train loss', 'train acc', 'test acc'])\nfor epoch in range(num_epochs):\nmetric = d2l.Accumulator(3)\n# train_loss, train_acc, num_examples\nfor i, (X, y) in enumerate(train_iter):\nnet.train()\ntrainer.zero_grad()\nX, y = X.to(device), y.to(device)\ny_hat = net(X)\nl = loss(y_hat, y)\nl.backward()\ntrainer.step()\nwith torch.no_grad():\nmetric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\ntrain_loss = metric[0] / metric[2]\ntrain_acc = metric[1] / metric[2]\nif (i + 1) % 50 == 0:\nanimator.add(epoch + i / len(train_iter),\n(train_loss, train_acc, None))\ntest_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator.add(epoch+1, (None, None, test_acc))\nif scheduler:\nif scheduler.__module__ == lr_scheduler.__name__:\n# Using PyTorch In-Built scheduler\nscheduler.step()\nelse:\n# Using custom defined scheduler\nfor param_group in trainer.param_groups:\nparam_group['lr'] = scheduler(epoch)\nprint(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\nf'test acc {test_acc:.3f}')\nLet’s have a look at what happens if we invoke this algorithm with default settings, such as\na learning rate of 0.3 and train for 30 iterations. Note how the training accuracy keeps on\nincreasing while progress in terms of test accuracy stalls beyond a point. The gap between\nboth curves indicates overﬁtting.\nlr, num_epochs = 0.3, 30\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\n\n559\nLearning Rate Scheduling\ntrain loss 0.191, train acc 0.926, test acc 0.895\n12.11.2 Schedulers\nOne way of adjusting the learning rate is to set it explicitly at each step. This is conve-\nniently achieved by the set_learning_rate method. We could adjust it downward after\nevery epoch (or even after every minibatch), e.g., in a dynamic manner in response to how\noptimization is progressing.\nlr = 0.1\ntrainer.param_groups[0][\"lr\"] = lr\nprint(f'learning rate is now {trainer.param_groups[0][\"lr\"]:.2f}')\nlearning rate is now 0.10\nMore generally we want to deﬁne a scheduler. When invoked with the number of updates\nit returns the appropriate value of the learning rate. Let’s deﬁne a simple one that sets the\nlearning rate to η = η0(t + 1)−1\n2 .\nclass SquareRootScheduler:\ndef __init__(self, lr=0.1):\nself.lr = lr\ndef __call__(self, num_update):\nreturn self.lr * pow(num_update + 1.0, -0.5)\nLet’s plot its behavior over a range of values.\nscheduler = SquareRootScheduler(lr=0.1)\nd2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\nNow let’s see how this plays out for training on Fashion-MNIST. We simply provide the\nscheduler as an additional argument to the training algorithm.\n\n560\nOptimization Algorithms\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.272, train acc 0.901, test acc 0.883\nThis worked quite a bit better than previously. Two things stand out: the curve was rather\nmore smooth than previously. Secondly, there was less overﬁtting. Unfortunately it is not a\nwell-resolved question as to why certain strategies lead to less overﬁtting in theory. There is\nsome argument that a smaller stepsize will lead to parameters that are closer to zero and thus\nsimpler. However, this does not explain the phenomenon entirely since we do not really stop\nearly but simply reduce the learning rate gently.\n12.11.3 Policies\nWhile we cannot possibly cover the entire variety of learning rate schedulers, we attempt to\ngive a brief overview of popular policies below. Common choices are polynomial decay and\npiecewise constant schedules. Beyond that, cosine learning rate schedules have been found to\n\n561\nLearning Rate Scheduling\nwork well empirically on some problems. Lastly, on some problems it is beneﬁcial to warm\nup the optimizer prior to using large learning rates.\nFactor Scheduler\nOne alternative to a polynomial decay would be a multiplicative one, that is ηt+1 ←ηt · α\nfor α ∈(0, 1). To prevent the learning rate from decaying beyond a reasonable lower bound\nthe update equation is often modiﬁed to ηt+1 ←max(ηmin, ηt · α).\nclass FactorScheduler:\ndef __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):\nself.factor = factor\nself.stop_factor_lr = stop_factor_lr\nself.base_lr = base_lr\ndef __call__(self, num_update):\nself.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\nreturn self.base_lr\nscheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)\nd2l.plot(torch.arange(50), [scheduler(t) for t in range(50)])\nThis can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler.\nFactorScheduler object. It takes a few more parameters, such as warmup period, warmup\nmode (linear or constant), the maximum number of desired updates, etc.; Going forward we\nwill use the built-in schedulers as appropriate and only explain their functionality here. As\nillustrated, it is fairly straightforward to build your own scheduler if needed.\nMulti Factor Scheduler\nA common strategy for training deep networks is to keep the learning rate piecewise constant\nand to decrease it by a given amount every so often. That is, given a set of times when to\ndecrease the rate, such as s = {5, 10, 20} decrease ηt+1 ←ηt · α whenever t ∈s. Assuming\nthat the values are halved at each step we can implement this as follows.\n\n562\nOptimization Algorithms\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.5)\nscheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)\ndef get_lr(trainer, scheduler):\nlr = scheduler.get_last_lr()[0]\ntrainer.step()\nscheduler.step()\nreturn lr\nd2l.plot(torch.arange(num_epochs), [get_lr(trainer, scheduler)\nfor t in range(num_epochs)])\nThe intuition behind this piecewise constant learning rate schedule is that one lets optimiza-\ntion proceed until a stationary point has been reached in terms of the distribution of weight\nvectors. Then (and only then) do we decrease the rate such as to obtain a higher quality proxy\nto a good local minimum. The example below shows how this can produce ever slightly better\nsolutions.\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.196, train acc 0.926, test acc 0.889\n\n563\nLearning Rate Scheduling\nCosine Scheduler\nA rather perplexing heuristic was proposed by Loshchilov and Hutter (2016). It relies on\nthe observation that we might not want to decrease the learning rate too drastically in the\nbeginning and moreover, that we might want to “reﬁne” the solution in the end using a very\nsmall learning rate. This results in a cosine-like schedule with the following functional form\nfor learning rates in the range t ∈[0,T].\nηt = ηT + η0 −ηT\n2\n(1 + cos(πt/T))\n(12.11.1)\nHere η0 is the initial learning rate, ηT is the target rate at time T. Furthermore, for t > T we\nsimply pin the value to ηT without increasing it again. In the following example, we set the\nmax update step T = 20.\nclass CosineScheduler:\ndef __init__(self, max_update, base_lr=0.01, final_lr=0,\nwarmup_steps=0, warmup_begin_lr=0):\nself.base_lr_orig = base_lr\nself.max_update = max_update\nself.final_lr = final_lr\nself.warmup_steps = warmup_steps\nself.warmup_begin_lr = warmup_begin_lr\nself.max_steps = self.max_update - self.warmup_steps\ndef get_warmup_lr(self, epoch):\nincrease = (self.base_lr_orig - self.warmup_begin_lr) \\\n* float(epoch) / float(self.warmup_steps)\nreturn self.warmup_begin_lr + increase\ndef __call__(self, epoch):\nif epoch < self.warmup_steps:\nreturn self.get_warmup_lr(epoch)\nif epoch <= self.max_update:\nself.base_lr = self.final_lr + (\nself.base_lr_orig - self.final_lr) * (1 + math.cos(\nmath.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2\nreturn self.base_lr\nscheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)\nd2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\n\n564\nOptimization Algorithms\nIn the context of computer vision this schedule can lead to improved results. Note, though,\nthat such improvements are not guaranteed (as can be seen below).\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.186, train acc 0.932, test acc 0.901\nWarmup\nIn some cases initializing the parameters is not suﬃcient to guarantee a good solution. This is\nparticularly a problem for some advanced network designs that may lead to unstable optimiza-\ntion problems. We could address this by choosing a suﬃciently small learning rate to prevent\ndivergence in the beginning. Unfortunately this means that progress is slow. Conversely, a\nlarge learning rate initially leads to divergence.\nA rather simple ﬁx for this dilemma is to use a warmup period during which the learning rate\nincreases to its initial maximum and to cool down the rate until the end of the optimization\nprocess. For simplicity one typically uses a linear increase for this purpose. This leads to a\nschedule of the form indicated below.\nscheduler = CosineScheduler(20, warmup_steps=5, base_lr=0.3, final_lr=0.01)\nd2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])\nNote that the network converges better initially (in particular observe the performance during\nthe ﬁrst 5 epochs).\nnet = net_fn()\ntrainer = torch.optim.SGD(net.parameters(), lr=0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\n\n565\nLearning Rate Scheduling\n182\ntrain loss 0.200, train acc 0.925, test acc 0.897\nWarmup can be applied to any scheduler (not just cosine). For a more detailed discussion\nof learning rate schedules and many more experiments see also (Gotmare et al., 2018). In\nparticular they ﬁnd that a warmup phase limits the amount of divergence of parameters in\nvery deep networks. This makes intuitively sense since we would expect signiﬁcant divergence\ndue to random initialization in those parts of the network that take the most time to make\nprogress in the beginning.\n12.11.4 Summary\n• Decreasing the learning rate during training can lead to improved accuracy and (most\nperplexingly) reduced overﬁtting of the model.\n• A piecewise decrease of the learning rate whenever progress has plateaued is eﬀective in\npractice. Essentially this ensures that we converge eﬃciently to a suitable solution and\nonly then reduce the inherent variance of the parameters by reducing the learning rate.\n• Cosine schedulers are popular for some computer vision problems. See e.g., GluonCV182\nfor details of such a scheduler.\n• A warmup period before optimization can prevent divergence.\n\n566\nOptimization Algorithms\n183\n• Optimization serves multiple purposes in deep learning. Besides minimizing the training\nobjective, diﬀerent choices of optimization algorithms and learning rate scheduling can\nlead to rather diﬀerent amounts of generalization and overﬁtting on the test set (for the\nsame amount of training error).\n12.11.5 Exercises\n1. Experiment with the optimization behavior for a given ﬁxed learning rate. What is the\nbest model you can obtain this way?\n2. How does convergence change if you change the exponent of the decrease in the learning\nrate? Use PolyScheduler for your convenience in the experiments.\n3. Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet.\nHow does it aﬀect performance relative to other schedulers?\n4. How long should warmup last?\n5. Can you connect optimization and sampling? Start by using results from Welling and Teh\n(2011) on Stochastic Gradient Langevin Dynamics.\nDiscussions183.\n\n13\nComputational Performance\nIn deep learning, datasets and models are usually large, which involves heavy computation.\nTherefore, computational performance matters a lot. This chapter will focus on the ma-\njor factors that aﬀect computational performance: imperative programming, symbolic pro-\ngramming, asynchronous computing, automatic parallelism, and multi-GPU computation. By\nstudying this chapter, you may further improve computational performance of those mod-\nels implemented in the previous chapters, for example, by reducing training time without\naﬀecting accuracy.\n13.1 Compilers and Interpreters\nSo far, this book has focused on imperative programming, which makes use of statements\nsuch as print, +, and if to change a program’s state. Consider the following example of a\nsimple imperative program.\ndef add(a, b):\nreturn a + b\ndef fancy_func(a, b, c, d):\ne = add(a, b)\nf = add(c, d)\ng = add(e, f)\nreturn g\nprint(fancy_func(1, 2, 3, 4))\n10\nPython is an interpreted language. When evaluating the above fancy_func function it per-\nforms the operations making up the function’s body in sequence. That is, it will evaluate e =\nadd(a, b) and store the results as variable e, thereby changing the program’s state. The next\ntwo statements f = add(c, d) and g = add(e, f) will be executed similarly, performing\nadditions and storing the results as variables. Fig. 13.1.1 illustrates the ﬂow of data.\n567\n\n568\nComputational Performance\nt\nFig. 13.1.1\nData ﬂow in an imperative program.\nAlthough imperative programming is convenient, it may be ineﬃcient. On the one hand, even\nif the add function is repeatedly called throughout fancy_func, Python will execute the three\nfunction calls individually. If these are executed, say, on a GPU (or even on multiple GPUs),\nthe overhead arising from the Python interpreter can become overwhelming. Moreover, it\nwill need to save the variable values of e and f until all the statements in fancy_func have\nbeen executed. This is because we do not know whether the variables e and f will be used\nby other parts of the program after the statements e = add(a, b) and f = add(c, d) are\nexecuted.\n13.1.1 Symbolic Programming\nConsider the alternative, symbolic programming, where computation is usually performed\nonly once the process has been fully deﬁned. This strategy is used by multiple deep learning\nframeworks, including Theano and TensorFlow (the latter has acquired imperative exten-\nsions). It usually involves the following steps:\n1. Deﬁne the operations to be executed.\n2. Compile the operations into an executable program.\n3. Provide the required inputs and call the compiled program for execution.\nThis allows for a signiﬁcant amount of optimization. First, we can skip the Python inter-\npreter in many cases, thus removing a performance bottleneck that can become signiﬁcant\non multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler\nmight optimize and rewrite the above code into print((1 + 2) + (3 + 4)) or even\nprint(10). This is possible since a compiler gets to see the full code before turning it into\nmachine instructions. For instance, it can release memory (or never allocate it) whenever a\nvariable is no longer needed. Or it can transform the code entirely into an equivalent piece. To\nget a better idea, consider the following simulation of imperative programming (it is Python\nafter all) below.\ndef add_():\nreturn '''\ndef add(a, b):\nreturn a + b\n'''\n(continues on next page)\n\n569\nCompilers and Interpreters\n(continued from previous page)\ndef fancy_func_():\nreturn '''\ndef fancy_func(a, b, c, d):\ne = add(a, b)\nf = add(c, d)\ng = add(e, f)\nreturn g\n'''\ndef evoke_():\nreturn add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\nprog = evoke_()\nprint(prog)\ny = compile(prog, '', 'exec')\nexec(y)\ndef add(a, b):\nreturn a + b\ndef fancy_func(a, b, c, d):\ne = add(a, b)\nf = add(c, d)\ng = add(e, f)\nreturn g\nprint(fancy_func(1, 2, 3, 4))\n10\nThe diﬀerences between imperative (interpreted) programming and symbolic programming\nare as follows:\n• Imperative programming is easier. When imperative programming is used in Python, the\nmajority of the code is straightforward and easy to write. It is also easier to debug im-\nperative programming code. This is because it is easier to obtain and print all relevant\nintermediate variable values, or use Python’s built-in debugging tools.\n• Symbolic programming is more eﬃcient and easier to port. Symbolic programming makes\nit easier to optimize the code during compilation, while also having the ability to port\nthe program into a format independent of Python. This allows the program to be run\nin a non-Python environment, thus avoiding any potential performance issues related to\nthe Python interpreter.\n13.1.2 Hybrid Programming\nHistorically most deep learning frameworks choose between an imperative or a symbolic ap-\nproach. For example, Theano, TensorFlow (inspired by the former), Keras, and CNTK for-\nmulate models symbolically. Conversely, Chainer and PyTorch take an imperative approach.\nAn imperative mode was added to TensorFlow 2.0 and Keras in later revisions.\n\n570\nComputational Performance\nAs mentioned above, PyTorch is based on imperative programming and uses dynamic com-\nputation graphs. In an eﬀort to leverage the portability and eﬃciency of symbolic program-\nming, developers considered whether it would be possible to combine the beneﬁts of both\nprogramming paradigms. This led to a torchscript that lets users develop and debug using\npure imperative programming, while having the ability to convert most programs into sym-\nbolic programs to be run when product-level computing performance and deployment are\nrequired.\n13.1.3 Hybridizing the Sequential Class\nThe easiest way to get a feel for how hybridization works is to consider deep networks with\nmultiple layers. Conventionally the Python interpreter will need to execute the code for all\nlayers to generate an instruction that can then be forwarded to a CPU or a GPU. For a single\n(fast) computing device this does not cause any major issues. On the other hand, if we use\nan advanced 8-GPU server such as an AWS P3dn.24xlarge instance Python will struggle to\nkeep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here.\nLet’s see how we can address this for signiﬁcant parts of the code by replacing Sequential\nwith HybridSequential. We begin by deﬁning a simple MLP.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n# Factory for networks\ndef get_net():\nnet = nn.Sequential(nn.Linear(512, 256),\nnn.ReLU(),\nnn.Linear(256, 128),\nnn.ReLU(),\nnn.Linear(128, 2))\nreturn net\nx = torch.randn(size=(1, 512))\nnet = get_net()\nnet(x)\ntensor([[-0.1197,\n0.0413]], grad_fn=<AddmmBackward0>)\nBy converting the model using torch.jit.script function, we are able to compile and opti-\nmize the computation in the MLP. The model’s computation result remains unchanged.\nnet = torch.jit.script(net)\nnet(x)\ntensor([[-0.1197,\n0.0413]], grad_fn=<AddmmBackward0>)\n\n571\nCompilers and Interpreters\nThis seems almost too good to be true: write the same code as before and simply convert\nthe model using torch.jit.script. Once this happens the network is optimized (we will\nbenchmark the performance below).\nAcceleration by Hybridization\nTo demonstrate the performance improvement gained by compilation we compare the time\nneeded to evaluate net(x) before and after hybridization. Let’s deﬁne a class to measure this\ntime ﬁrst. It will come handy throughout the chapter as we set out to measure (and improve)\nperformance.\n#@save\nclass Benchmark:\n\"\"\"For measuring running time.\"\"\"\ndef __init__(self, description='Done'):\nself.description = description\ndef __enter__(self):\nself.timer = d2l.Timer()\nreturn self\ndef __exit__(self, *args):\nprint(f'{self.description}: {self.timer.stop():.4f} sec')\nNow we can invoke the network twice, once with and once without torchscript.\nnet = get_net()\nwith Benchmark('Without torchscript'):\nfor i in range(1000): net(x)\nnet = torch.jit.script(net)\nwith Benchmark('With torchscript'):\nfor i in range(1000): net(x)\nWithout torchscript: 18.1045 sec\nWith torchscript: 20.5523 sec\nAs is observed in the above results, after an nn.Sequential instance is scripted using the\ntorch.jit.script function, computing performance is improved through the use of sym-\nbolic programming.\nSerialization\nOne of the beneﬁts of compiling the models is that we can serialize (save) the model and\nits parameters to disk. This allows us to store a model in a manner that is independent of\nthe front-end language of choice. This allows us to deploy trained models to other devices\n\n572\nComputational Performance\n184\nand easily use other front-end programming languages. At the same time the code is often\nfaster than what can be achieved in imperative programming. Let’s see the save function in\naction.\nnet.save('my_mlp')\n!ls -lh my_mlp*\n-rw-rw-r-- 1 ubuntu ubuntu 652K Aug 18 22:58 my_mlp\n13.1.4 Summary\n• Imperative programming makes it easy to design new models since it is possible to write\ncode with control ﬂow and the ability to use a large amount of the Python software\necosystem.\n• Symbolic programming requires that we specify the program and compile it before exe-\ncuting it. The beneﬁt is improved performance.\n13.1.5 Exercises\n1. Review the models that interest you in the previous chapters. Can you improve their com-\nputational performance by reimplementing them?\nDiscussions184.\n13.2 Asynchronous Computation\nToday’s computers are highly parallel systems, consisting of multiple CPU cores (often mul-\ntiple threads per core), multiple processing elements per GPU, and often multiple GPUs per\ndevice. In short, we can process many diﬀerent things at the same time, often on diﬀer-\nent devices. Unfortunately Python is not a great way of writing parallel and asynchronous\ncode, at least not without some extra help. After all, Python is single-threaded and this is\nunlikely to change in the future. Deep learning frameworks such as MXNet and TensorFlow\nadopt an asynchronous programming model to improve performance, while PyTorch uses\nPython’s own scheduler leading to a diﬀerent performance trade-oﬀ. For PyTorch, by de-\nfault, GPU operations are asynchronous. When you call a function that uses the GPU, the\noperations are enqueued to the particular device, but not necessarily executed until later.\nThis allows us to execute more computations in parallel, including operations on the CPU or\nother GPUs.\n\n573\nAsynchronous Computation\nHence, understanding how asynchronous programming works helps us to develop more eﬃ-\ncient programs, by proactively reducing computational requirements and mutual dependen-\ncies. This allows us to reduce memory overhead and increase processor utilization.\nimport os\nimport subprocess\nimport numpy\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n13.2.1 Asynchrony via Backend\nFor a warmup consider the following toy problem: we want to generate a random matrix and\nmultiply it. Let’s do that both in NumPy and in PyTorch tensor to see the diﬀerence. Note\nthat PyTorch tensor is deﬁned on a GPU.\n# Warmup for GPU computation\ndevice = d2l.try_gpu()\na = torch.randn(size=(1000, 1000), device=device)\nb = torch.mm(a, a)\nwith d2l.Benchmark('numpy'):\nfor _ in range(10):\na = numpy.random.normal(size=(1000, 1000))\nb = numpy.dot(a, a)\nwith d2l.Benchmark('torch'):\nfor _ in range(10):\na = torch.randn(size=(1000, 1000), device=device)\nb = torch.mm(a, a)\nnumpy: 1.4568 sec\ntorch: 0.0009 sec\nThe benchmark output via PyTorch is orders of magnitude faster. NumPy dot product is\nexecuted on the CPU processor while PyTorch matrix multiplication is executed on GPU\nand hence the latter is expected to be much faster. But the huge time diﬀerence suggests\nsomething else must be going on. By default, GPU operations are asynchronous in PyTorch.\nForcing PyTorch to ﬁnish all computation prior to returning shows what happened previ-\nously: computation is being executed by the backend while the frontend returns control to\nPython.\nwith d2l.Benchmark():\nfor _ in range(10):\na = torch.randn(size=(1000, 1000), device=device)\nb = torch.mm(a, a)\ntorch.cuda.synchronize(device)\n\n574\nComputational Performance\nDone: 0.0031 sec\nBroadly speaking, PyTorch has a frontend for direct interaction with the users, e.g., via\nPython, as well as a backend used by the system to perform the computation. As shown in\nFig. 13.2.1, users can write PyTorch programs in various frontend languages, such as Python\nand C++. Regardless of the frontend programming language used, the execution of PyTorch\nprograms occurs primarily in the backend of C++ implementations. Operations issued by\nthe frontend language are passed on to the backend for execution. The backend manages its\nown threads that continuously collect and execute queued tasks. Note that for this to work\nthe backend must be able to keep track of the dependencies between various steps in the\ncomputational graph. Hence, it is not possible to parallelize operations that depend on each\nother.\nt\nFig. 13.2.1\nProgramming language frontends and deep learning framework backends.\nLet’s look at another toy example to understand the dependency graph a bit better.\nx = torch.ones((1, 2), device=device)\ny = torch.ones((1, 2), device=device)\nz = x * y + 2\nz\ntensor([[3., 3.]], device='cuda:0')\nt\nFig. 13.2.2\nThe backend tracks dependencies between various steps in the computational graph.\n\n575\nAsynchronous Computation\n185\nThe code snippet above is also illustrated in Fig. 13.2.2. Whenever the Python frontend thread\nexecutes one of the ﬁrst three statements, it simply returns the task to the backend queue.\nWhen the last statement’s results need to be printed, the Python frontend thread will wait\nfor the C++ backend thread to ﬁnish computing the result of the variable z. One beneﬁt of\nthis design is that the Python frontend thread does not need to perform actual computations.\nThus, there is little impact on the program’s overall performance, regardless of Python’s per-\nformance. Fig. 13.2.3 illustrates how frontend and backend interact.\nt\nFig. 13.2.3\nInteractions of the frontend and backend.\n13.2.2 Barriers and Blockers\n13.2.3 Improving Computation\n13.2.4 Summary\n• Deep learning frameworks may decouple the Python frontend from an execution backend.\nThis allows for fast asynchronous insertion of commands into the backend and associated\nparallelism.\n• Asynchrony leads to a rather responsive frontend. However, use caution not to overﬁll the\ntask queue since it may lead to excessive memory consumption. It is recommended to\nsynchronize for each minibatch to keep frontend and backend approximately synchro-\nnized.\n• Chip vendors oﬀer sophisticated performance analysis tools to obtain a much more ﬁne-\ngrained insight into the eﬃciency of deep learning.\n13.2.5 Exercises\n1. On the CPU, benchmark the same matrix multiplication operations in this section. Can\nyou still observe asynchrony via the backend?\nDiscussions185.\n\n576\nComputational Performance\n13.3 Automatic Parallelism\nDeep learning frameworks (e.g., MXNet and PyTorch) automatically construct computa-\ntional graphs at the backend. Using a computational graph, the system is aware of all the\ndependencies, and can selectively execute multiple non-interdependent tasks in parallel to\nimprove speed. For instance, Fig. 13.2.2 in Section 13.2 initializes two variables indepen-\ndently. Consequently the system can choose to execute them in parallel.\nTypically, a single operator will use all the computational resources on all CPUs or on a\nsingle GPU. For example, the dot operator will use all cores (and threads) on all CPUs,\neven if there are multiple CPU processors on a single machine. The same applies to a single\nGPU. Hence parallelization is not quite so useful for single-device computers. With multiple\ndevices things matter more. While parallelization is typically most relevant between multiple\nGPUs, adding the local CPU will increase performance slightly. For example, see Hadjis et\nal. (2016) that focuses on training computer vision models combining a GPU and a CPU.\nWith the convenience of an automatically parallelizing framework we can accomplish the\nsame goal in a few lines of Python code. More broadly, our discussion of automatic parallel\ncomputation focuses on parallel computation using both CPUs and GPUs, as well as the\nparallelization of computation and communication.\nNote that we need at least two GPUs to run the experiments in this section.\nimport torch\nfrom d2l import torch as d2l\n13.3.1 Parallel Computation on GPUs\nLet’s start by deﬁning a reference workload to test: the run function below performs 10\nmatrix-matrix multiplications on the device of our choice using data allocated into two vari-\nables: x_gpu1 and x_gpu2.\ndevices = d2l.try_all_gpus()\ndef run(x):\nreturn [x.mm(x) for _ in range(50)]\nx_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\nx_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])\nNow we apply the function to the data. To ensure that caching does not play a role in the\nresults we warm up the devices by performing a single pass on either of them prior to mea-\nsuring. torch.cuda.synchronize() waits for all kernels in all streams on a CUDA device\nto complete. It takes in a device argument, the device for which we need to synchronize. It\nuses the current device, given by current_device(), if the device argument is None (de-\nfault).\n\n577\nAutomatic Parallelism\nrun(x_gpu1)\nrun(x_gpu2)\n# Warm-up all devices\ntorch.cuda.synchronize(devices[0])\ntorch.cuda.synchronize(devices[1])\nwith d2l.Benchmark('GPU1 time'):\nrun(x_gpu1)\ntorch.cuda.synchronize(devices[0])\nwith d2l.Benchmark('GPU2 time'):\nrun(x_gpu2)\ntorch.cuda.synchronize(devices[1])\nGPU1 time: 0.4628 sec\nGPU2 time: 0.4522 sec\nIf we remove the synchronize statement between both tasks the system is free to parallelize\ncomputation on both devices automatically.\nwith d2l.Benchmark('GPU1 & GPU2'):\nrun(x_gpu1)\nrun(x_gpu2)\ntorch.cuda.synchronize()\nGPU1 & GPU2: 0.4630 sec\nIn the above case the total execution time is less than the sum of its parts, since the deep\nlearning framework automatically schedules computation on both GPU devices without the\nneed for sophisticated code on behalf of the user.\n13.3.2 Parallel Computation and Communication\nIn many cases we need to move data between diﬀerent devices, say between the CPU and\nGPU, or between diﬀerent GPUs. For instance, this occurs when we want to perform dis-\ntributed optimization where we need to aggregate the gradients over multiple accelerator\ncards. Let’s simulate this by computing on the GPU and then copying the results back to the\nCPU.\ndef copy_to_cpu(x, non_blocking=False):\nreturn [y.to('cpu', non_blocking=non_blocking) for y in x]\nwith d2l.Benchmark('Run on GPU1'):\ny = run(x_gpu1)\ntorch.cuda.synchronize()\nwith d2l.Benchmark('Copy to CPU'):\ny_cpu = copy_to_cpu(y)\ntorch.cuda.synchronize()\n\n578\nComputational Performance\nRun on GPU1: 0.4649 sec\nCopy to CPU: 2.5135 sec\nThis is somewhat ineﬃcient. Note that we could already start copying parts of y to the CPU\nwhile the remainder of the list is still being computed. This situation occurs, e.g., when we\ncompute the (backprop) gradient on a minibatch. The gradients of some of the parameters\nwill be available earlier than that of others. Hence it works to our advantage to start using\nPCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions\nsuch as to() and copy_() admit an explicit non_blocking argument, which lets the caller\nbypass synchronization when it is unnecessary. Setting non_blocking=True allows us to\nsimulate this scenario.\nwith d2l.Benchmark('Run on GPU1 and copy to CPU'):\ny = run(x_gpu1)\ny_cpu = copy_to_cpu(y, True)\ntorch.cuda.synchronize()\nRun on GPU1 and copy to CPU: 2.1902 sec\nThe total time required for both operations is (as expected) less than the sum of their parts.\nNote that this task is diﬀerent from parallel computation as it uses a diﬀerent resource: the\nbus between the CPU and GPUs. In fact, we could compute on both devices and communi-\ncate, all at the same time. As noted above, there is a dependency between computation and\ncommunication: y[i] must be computed before it can be copied to the CPU. Fortunately,\nthe system can copy y[i-1] while computing y[i] to reduce the total running time.\nWe conclude with an illustration of the computational graph and its dependencies for a simple\ntwo-layer MLP when training on a CPU and two GPUs, as depicted in Fig. 13.3.1. It would\nbe quite painful to schedule the parallel program resulting from this manually. This is where\nit is advantageous to have a graph-based computing backend for optimization.\n13.3.3 Summary\n• Modern systems have a variety of devices, such as multiple GPUs and CPUs. They can be\nused in parallel, asynchronously.\n• Modern systems also have a variety of resources for communication, such as PCI Express,\nstorage (typically solid-state drives or via networks), and network bandwidth. They can\nbe used in parallel for peak eﬃciency.\n• The backend can improve performance through automatic parallel computation and com-\nmunication.\n13.3.4 Exercises\n\n579\nAutomatic Parallelism\nt\nFig. 13.3.1\nThe computational graph and its dependencies of a two-layer MLP on a CPU and two\nGPUs.\n186\n187\n1. Eight operations were performed in the run function deﬁned in this section. There are no\ndependencies between them. Design an experiment to see if the deep learning framework\nwill automatically execute them in parallel.\n2. When the workload of an individual operator is suﬃciently small, parallelization can help\neven on a single CPU or GPU. Design an experiment to verify this.\n3. Design an experiment that uses parallel computation on CPUs, GPUs, and communication\nbetween both devices.\n4. Use a debugger such as NVIDIA’s Nsight186 to verify that your code is eﬃcient.\n5. Designing computation tasks that include more complex data dependencies, and run ex-\nperiments to see if you can obtain the correct results while improving performance.\nDiscussions187.\n\n580\nComputational Performance\n188\n189\n190\n13.4 Hardware\nBuilding systems with great performance requires a good understanding of the algorithms\nand models to capture the statistical aspects of the problem. At the same time it is also indis-\npensable to have at least a modicum of knowledge of the underlying hardware. The current\nsection is no substitute for a proper course on hardware and system design. Instead, it might\nserve as a starting point for understanding why some algorithms are more eﬃcient than oth-\ners and how to achieve good throughput. A good design can easily make a diﬀerence of an\norder of magnitude and, in turn, this can make the diﬀerence between being able to train a\nnetwork (e.g., in a week) and not at all (in 3 months, thus missing the deadline). We will start\nby looking at computers. Then we will zoom in to look more carefully at CPUs and GPUs.\nLastly we zoom out to review how multiple computers are connected in a server center or in\nthe cloud.\nt\nFig. 13.4.1\nLatency Numbers that every programmer should know.\nImpatient readers may be able to get by with Fig. 13.4.1. It is taken from Colin Scott’s inter-\nactive post188 that gives a good overview of the progress over the past decade. The original\nnumbers are due to JeﬀDean’s Stanford talk from 2010 189 . The discussion below explains\nsome of the rationale for these numbers and how they can guide us in designing algorithms.\nThe discussion below is very high level and cursory. It is clearly no substitute for a proper\ncourse but rather just meant to provide enough information for a statistical modeler to make\nsuitable design decisions. For an in-depth overview of computer architecture we refer the\nreader to (Hennessy and Patterson, 2011) or a recent course on the subject, such as the one\nby Arste Asanovic190.\n13.4.1 Computers\nMost deep learning researchers and practitioners have access to a computer with a fair amount\nof memory, computation, some form of an accelerator such as a GPU, or multiples thereof.\nA computer consists of the following key components:\n• A processor (also referred to as a CPU) that is able to execute the programs we give it (in\n\n581\nHardware\n191\naddition to running an operating system and many other things), typically consisting of\n8 or more cores.\n• Memory (RAM) to store and retrieve the results from computation, such as weight vectors\nand activations, and training data.\n• An Ethernet network connection (sometimes multiple) with speeds ranging from 1 GB/s\nto 100 GB/s. On high end servers more advanced interconnects can be found.\n• A high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers\nhave up to 8 accelerators, often connected in an advanced topology, while desktop sys-\ntems have 1 or 2, depending on the budget of the user and the size of the power supply.\n• Durable storage, such as a magnetic hard disk drive, a solid state drive, in many cases\nconnected using the PCIe bus. It provides eﬃcient transfer of training data to the system\nand storage of intermediate checkpoints as needed.\nt\nFig. 13.4.2\nConnectivity of components of a computer.\nAs Fig. 13.4.2 indicates, most components (network, GPU, and storage) are connected to\nthe CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the\nCPU. For instance AMD’s Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable\n16 Gbit/s data transfer in both directions. The memory is directly attached to the CPU with\na total bandwidth of up to 100 GB/s.\nWhen we run code on a computer we need to shuﬄe data to the processors (CPUs or GPUs),\nperform computation, and then move the results oﬀthe processor back to RAM and durable\nstorage. Hence, in order to get good performance we need to make sure that this works seam-\nlessly without any one of the systems becoming a major bottleneck. For instance, if we cannot\nload images quickly enough the processor will not have any work to do. Likewise, if we can-\nnot move matrices quickly enough to the CPU (or GPU), its processing elements will starve.\nFinally, if we want to synchronize multiple computers across the network, the latter should\nnot slow down computation. One option is to interleave communication and computation.\nLet’s have a look at the various components in more detail.\n13.4.2 Memory\nAt its most basic memory is used to store data that needs to be readily accessible. At present\nCPU RAM is typically of the DDR4191 variety, oﬀering 20–25 GB/s bandwidth per module.\nEach module has a 64-bit-wide bus. Typically pairs of memory modules are used to allow for\nmultiple channels. CPUs have between 2 and 4 memory channels, i.e., they have between 4\n\n582\nComputational Performance\n192\n193\n194\n0GB/s and 100 GB/s peak memory bandwidth. Often there are two banks per channel. For\ninstance AMD’s Zen 3 Threadripper has 8 slots.\nWhile these numbers are impressive, indeed, they only tell part of the story. When we want to\nread a portion from memory we ﬁrst need to tell the memory module where the information\ncan be found. That is, we ﬁrst need to send the address to RAM. Once this is accomplished\nwe can choose to read just a single 64 bit record or a long sequence of records. The latter\nis called burst read. In a nutshell, sending an address to memory and setting up the transfer\ntakes approximately 100 ns (details depend on the speciﬁc timing coeﬃcients of the memory\nchips used), every subsequent transfer takes only 0.2 ns. In short, the ﬁrst read is 500 times as\nexpensive as subsequent ones! Note that we could perform up to 10,000,000 random reads\nper second. This suggests that we avoid random memory access as far as possible and use\nburst reads (and writes) instead.\nMatters are a bit more complex when we take into account that we have multiple banks.\nEach bank can read memory largely independently. This means two things. On the one hand,\nthe eﬀective number of random reads is up to 4 times higher, provided that they are spread\nevenly across memory. It also means that it is still a bad idea to perform random reads since\nburst reads are 4 times faster, too. On the other hand, due to memory alignment to 64 bit\nboundaries it is a good idea to align any data structures with the same boundaries. Compilers\ndo this pretty much automatically192 when the appropriate ﬂags are set. Curious readers are\nencouraged to review a lecture on DRAMs such as the one by Zeshan Chishti193.\nGPU memory is subject to even higher bandwidth requirements since they have many more\nprocessing elements than CPUs. By and large there are two options to address them. The\nﬁrst is to make the memory bus signiﬁcantly wider. For instance, NVIDIA’s RTX 2080 Ti\nhas a 352-bit-wide bus. This allows for much more information to be transferred at the same\ntime. Second, GPUs use speciﬁc high-performance memory. Consumer-grade devices, such\nas NVIDIA’s RTX and Titan series typically use GDDR6 194 chips with over 500 GB/s\naggregate bandwidth. An alternative is to use HBM (high bandwidth memory) modules. They\nuse a very diﬀerent interface and connect directly with GPUs on a dedicated silicon wafer.\nThis makes them very expensive and their use is typically limited to high-end server chips,\nsuch as the NVIDIA Volta V100 series of accelerators. Quite unsurprisingly, GPU memory\nis generally much smaller than CPU memory due to the higher cost of the former. For our\npurposes, by and large their performance characteristics are similar, just a lot faster. We can\nsafely ignore the details for the purpose of this book. They only matter when tuning GPU\nkernels for high throughput.\n13.4.3 Storage\nWe saw that some of the key characteristics of RAM are bandwidth and latency. The same\nis true for storage devices, just that the diﬀerences can be even more extreme.\n\n583\nHardware\nHard Disk Drives\nHard disk drives (HDDs) have been in use for over half a century. In a nutshell they contain\na number of spinning platters with heads that can be positioned to read or write at any given\ntrack. High-end disks hold up to 16 TB on 9 platters. One of the key beneﬁts of HDDs is that\nthey are relatively inexpensive. One of their many downsides are their typically catastrophic\nfailure modes and their relatively high read latency.\nTo understand the latter, consider the fact that HDDs spin at around 7,200 RPM (revolutions\nper minute). If they were much faster they would shatter due to the centrifugal force exerted\non the platters. This has a major downside when it comes to accessing a speciﬁc sector on the\ndisk: we need to wait until the platter has rotated in position (we can move the heads but not\naccelerate the actual disks). Hence it can take over 8 ms until the requested data is available.\nA common way this is expressed is to say that HDDs can operate at approximately 100 IOPs\n(input/output operations per second). This number has essentially remained unchanged for\nthe past two decades. Worse still, it is equally diﬃcult to increase bandwidth (it is in the order\nof 100–200 MB/s). After all, each head reads a track of bits, hence the bit rate only scales\nwith the square root of the information density. As a result, HDDs are quickly becoming\nrelegated to archival storage and low-grade storage for very large datasets.\nSolid State Drives\nSolid state drives (SSDs) use ﬂash memory to store information persistently. This allows for\nmuch faster access to stored records. Modern SSDs can operate at 100,000 to 500,000 IOPs,\ni.e., up to 3 orders of magnitude faster than HDDs. Furthermore, their bandwidth can reach\n1–3GB/s, i.e., one order of magnitude faster than HDDs. These improvements sound almost\ntoo good to be true. Indeed, they come with the following caveats, due to the way SSDs are\ndesigned.\n• SSDs store information in blocks (256 KB or larger). They can only be written as a whole,\nwhich takes signiﬁcant time. Consequently bit-wise random writes on SSD have very\npoor performance. Likewise, writing data in general takes signiﬁcant time since the\nblock has to be read, erased and then rewritten with new information. By now SSD\ncontrollers and ﬁrmware have developed algorithms to mitigate this. Nonetheless, writes\ncan be much slower, in particular for QLC (quad level cell) SSDs. The key for improved\nperformance is to maintain a queue of operations, to prefer reads and to write in large\nblocks if possible.\n• The memory cells in SSDs wear out relatively quickly (often already after a few thou-\nsand writes). Wear-level protection algorithms are able to spread the degradation over\nmany cells. That said, it is not recommended to use SSDs for swapping ﬁles or for large\naggregations of log-ﬁles.\n• Lastly, the massive increase in bandwidth has forced computer designers to attach SSDs\ndirectly to the PCIe bus. The drives capable of handling this, referred to as NVMe (Non\n\n584\nComputational Performance\nVolatile Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to 8GB/s\non PCIe 4.0.\nCloud Storage\nCloud storage provides a conﬁgurable range of performance. That is, the assignment of stor-\nage to virtual machines is dynamic, both in terms of quantity and in terms of speed, as chosen\nby users. We recommend that users increase the provisioned number of IOPs whenever la-\ntency is too high, e.g., during training with many small records.\n13.4.4 CPUs\nCentral processing units (CPUs) are the centerpiece of any computer. They consist of a num-\nber of key components: processor cores that are able to execute machine code, a bus connect-\ning them (the speciﬁc topology diﬀers signiﬁcantly between processor models, generations,\nand vendors), and caches to allow for higher bandwidth and lower latency memory access\nthan what is possible by reads from main memory. Lastly, almost all modern CPUs contain\nvector processing units to aid with high performance linear algebra and convolutions, as they\nare common in media processing and machine learning.\nt\nFig. 13.4.3\nIntel Skylake consumer quad-core CPU.\nFig. 13.4.3 depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated\nGPU, caches, and a ringbus connecting the four cores. Peripherals, such as Ethernet, WiFi,\nBluetooth, SSD controller, and USB, are either part of the chipset or directly attached (PCIe)\nto the CPU.\n\n585\nHardware\n195\nMicroarchitecture\nEach of the processor cores consists of a rather sophisticated set of components. While details\ndiﬀer between generations and vendors, the basic functionality is pretty much standard. The\nfront-end loads instructions and tries to predict which path will be taken (e.g., for control\nﬂow). Instructions are then decoded from assembly code to microinstructions. Assembly code\nis often not the lowest level code that a processor executes. Instead, complex instructions may\nbe decoded into a set of more lower level operations. These are then processed by the actual\nexecution core. Often the latter is capable of performing many operations simultaneously.\nFor instance, the ARM Cortex A77 core of Fig. 13.4.4 is able to perform up to 8 operations\nsimultaneously.\nt\nFig. 13.4.4\nARM Cortex A77 Microarchitecture.\nThis means that eﬃcient programs might be able to perform more than one instruction per\nclock cycle, provided that they can be carried out independently. Not all units are created\nequal. Some specialize in integer instructions whereas others are optimized for ﬂoating point\nperformance. To increase throughput, the processor might also follow multiple code paths\nsimultaneously in a branching instruction and then discard the results of the branches not\ntaken. This is why branch prediction units matter (on the front-end) such that only the most\npromising paths are pursued.\nVectorization\nDeep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine\nlearning, one needs to perform many operations in one clock cycle. This is achieved via\nvector units. They have diﬀerent names: on ARM they are called NEON, on x86 they (a\nrecent generation) are referred to as AVX2195 units. A common aspect is that they are able\nto perform SIMD (single instruction multiple data) operations. Fig. 13.4.5 shows how 8 short\nintegers can be added in one clock cycle on ARM.\nDepending on architecture choices, such registers are up to 512 bits long, allowing for the\n\n586\nComputational Performance\nt\nFig. 13.4.5\n128 bit NEON vectorization.\n196\ncombination of up to 64 pairs of numbers. For instance, we might be multiplying two numbers\nand adding them to a third, which is also known as a fused multiply-add. Intel’s OpenVino\n196 uses these to achieve respectable throughput for deep learning on server-grade CPUs.\nNote, though, that this number is entirely dwarfed by what GPUs are capable of achieving.\nFor instance, NVIDIA’s RTX 2080 Ti has 4,352 CUDA cores, each of which is capable of\nprocessing such an operation at any time.\nCache\nConsider the following situation: we have a modest CPU core with 4 cores as depicted in\nFig. 13.4.3 above, running at 2 GHz frequency. Moreover, let’s assume that we have an IPC\n(instructions per clock) count of 1 and that the units have AVX2 with 256-bit width enabled.\nLet’s furthermore assume that at least one of the registers used for AVX2 operations needs\nto be retrieved from memory. This means that the CPU consumes 4 × 256 bit = 128 bytes\nof data per clock cycle. Unless we are able to transfer 2×109 ×128 = 256×109 bytes to the\nprocessor per second the processing elements are going to starve. Unfortunately the memory\ninterface of such a chip only supports 20–40 GB/s data transfer, i.e., one order of magnitude\nless. The ﬁx is to avoid loading new data from memory as far as possible and rather to cache\nit locally on the CPU. This is where caches come in handy. Commonly the following names\nor concepts are used:\n• Registers are strictly speaking not part of the cache. They help stage instructions. That\nsaid, CPU registers are memory locations that a CPU can access at clock speed without\nany delay penalty. CPUs have tens of registers. It is up to the compiler (or programmer)\nto use registers eﬃciently. For instance the C programming language has a register\nkeyword.\n• L1 caches are the ﬁrst line of defense against high memory bandwidth requirements. L1\ncaches are tiny (typical sizes might be 32–64 KB) and often split into data and instruc-\ntions caches. When data is found in the L1 cache, access is very fast. If they cannot be\nfound there, the search progresses down the cache hierarchy.\n• L2 caches are the next stop. Depending on architecture design and processor size they\nmight be exclusive. They might be accessible only by a given core or shared among\n\n587\nHardware\nmultiple cores. L2 caches are larger (typically 256–512 KB per core) and slower than\nL1. Furthermore, to access something in L2 we ﬁrst need to check to realize that the\ndata is not in L1, which adds a small amount of extra latency.\n• L3 caches are shared among multiple cores and can be quite large. AMD’s Epyc 3 server\nCPUs have a whopping 256 MB of cache spread across multiple chiplets. More typical\nnumbers are in the 4–8 MB range.\nPredicting which memory elements will be needed next is one of the key optimization param-\neters in chip design. For instance, it is advisable to traverse memory in a forward direction\nsince most caching algorithms will try to read ahead rather than backwards. Likewise, keep-\ning memory access patterns local is a good way of improving performance.\nAdding caches is a double-edge sword. On the one hand they ensure that the processor cores\ndo not starve of data. At the same time they increase chip size, using up area that otherwise\ncould have been spent on increasing processing power. Moreover, cache misses can be expen-\nsive. Consider the worst case scenario, false sharing, as depicted in Fig. 13.4.6. A memory\nlocation is cached on processor 0 when a thread on processor 1 requests the data. To obtain\nit, processor 0 needs to stop what it is doing, write the information back to main memory and\nthen let processor 1 read it from memory. During this operation both processors wait. Quite\npotentially such code runs more slowly on multiple processors when compared with an eﬃ-\ncient single-processor implementation. This is one more reason for why there is a practical\nlimit to cache sizes (besides their physical size).\nt\nFig. 13.4.6\nFalse sharing (image courtesy of Intel).\n13.4.5 GPUs and other Accelerators\nIt is not an exaggeration to claim that deep learning would not have been successful without\nGPUs. By the same token, it is quite reasonable to argue that GPU manufacturers’ fortunes\nhave increased signiﬁcantly due to deep learning. This co-evolution of hardware and algo-\nrithms has led to a situation where for better or worse deep learning is the preferable statistical\nmodeling paradigm. Hence it pays to understand the speciﬁc beneﬁts that GPUs and related\naccelerators such as the TPU (Jouppi et al., 2017).\nOf note is a distinction that is often made in practice: accelerators are optimized either for\n\n588\nComputational Performance\n197\ntraining or inference. For the latter we only need to compute the forward propagation in\na network. No storage of intermediate data is needed for backpropagation. Moreover, we\nmay not need very precise computation (FP16 or INT8 typically suﬃce). On the other hand,\nduring training all intermediate results need storage to compute gradients. Moreover, ac-\ncumulating gradients requires higher precision to avoid numerical underﬂow (or overﬂow).\nThis means that FP16 (or mixed precision with FP32) is the minimum requirement. All of\nthis necessitates faster and larger memory (HBM2 vs. GDDR6) and more processing power.\nFor instance, NVIDIA’s Turing197 T4 GPUs are optimized for inference whereas the V100\nGPUs are preferable for training.\nRecall vectorization as illustrated in Fig. 13.4.5. Adding vector units to a processor core\nallowed us to increase throughput signiﬁcantly. For example, in the example in Fig. 13.4.5\nwe were able to perform 16 operations simultaneously. First, what if we added operations\nthat optimized not just operations between vectors but also between matrices? This strategy\nled to tensor cores (to be covered shortly). Second, what if we added many more cores? In a\nnutshell, these two strategies summarize the design decisions in GPUs. Fig. 13.4.7 gives an\noverview of a basic processing block. It contains 16 integer and 16 ﬂoating point units. In\naddition to that, two tensor cores accelerate a narrow subset of additional operations relevant\nfor deep learning. Each streaming multiprocessor consists of four such blocks.\nt\nFig. 13.4.7\nNVIDIA Turing processing block (image courtesy of NVIDIA).\nNext, 12 streaming multiprocessors are grouped into graphics processing clusters which make\nup the high-end TU102 processors. Ample memory channels and an L2 cache complement\nthe setup. Fig. 13.4.8 has the relevant details. One of the reasons for designing such a device is\nthat individual blocks can be added or removed as needed to allow for more compact chips and\nto deal with yield issues (faulty modules might not be activated). Fortunately programming\nsuch devices is well hidden from the casual deep learning researcher beneath layers of CUDA\nand framework code. In particular, more than one of the programs might well be executed\nsimultaneously on the GPU, provided that there are available resources. Nonetheless it pays\nto be aware of the limitations of the devices to avoid picking models that do not ﬁt into device\nmemory.\nA last aspect that is worth mentioning in more detail are tensor cores. They are an example of a\n\n589\nHardware\nt\nFig. 13.4.8\nNVIDIA Turing architecture (image courtesy of NVIDIA)\nrecent trend of adding more optimized circuits that are speciﬁcally eﬀective for deep learning.\nFor instance, the TPU added a systolic array (Kung, 1988) for fast matrix multiplication.\nThere the design was to support a very small number (one for the ﬁrst generation of TPUs) of\nlarge operations. Tensor cores are at the other end. They are optimized for small operations\ninvolving between 4 × 4 and 16 × 16 matrices, depending on their numerical precision. Fig.\n13.4.9 gives an overview of the optimizations.\nt\nFig. 13.4.9\nNVIDIA tensor cores in Turing (image courtesy of NVIDIA).\nObviously when optimizing for computation we end up making certain compromises. One\nof them is that GPUs are not very good at handling interrupts and sparse data. While there\n\n590\nComputational Performance\n198\n199\n200\n201\n202\n203\nare notable exceptions, such as Gunrock198 (Wang et al., 2016), the access pattern of sparse\nmatrices and vectors do not go well with the high bandwidth burst read operations where\nGPUs excel. Matching both goals is an area of active research. See e.g., DGL 199 , a library\ntuned for deep learning on graphs.\n13.4.6 Networks and Buses\nWhenever a single device is insuﬃcient for optimization we need to transfer data to and from\nit to synchronize processing. This is where networks and buses come in handy. We have a\nnumber of design parameters: bandwidth, cost, distance, and ﬂexibility. On one end we have\nWiFi that has a pretty good range, is very easy to use (no wires, after all), cheap but it oﬀers\ncomparatively mediocre bandwidth and latency. No machine learning researcher within their\nright mind would use it to build a cluster of servers. In what follows we focus on interconnects\nthat are suitable for deep learning.\n• PCIe is a dedicated bus for very high bandwidth point-to-point connections (up to 32\nGB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit\nmicroseconds (5 μs). PCIe links are precious. Processors only have a limited number\nof them: AMD’s EPYC 3 has 128 lanes, Intel’s Xeon has up to 48 lanes per chip; on\ndesktop-grade CPUs the numbers are 20 (Ryzen 9) and 16 (Core i9) respectively. Since\nGPUs have typically 16 lanes, this limits the number of GPUs that can connect to the\nCPU at full bandwidth. After all, they need to share the links with other high bandwidth\nperipherals such as storage and Ethernet. Just like with RAM access, large bulk transfers\nare preferable due to reduced packet overhead.\n• Ethernet is the most commonly used way of connecting computers. While it is signiﬁ-\ncantly slower than PCIe, it is very cheap and resilient to install and covers much longer\ndistances. Typical bandwidth for low-grade servers is 1 GBit/s. Higher-end devices (e.g.,\nC5 instances200 in the cloud) oﬀer between 10 and 100 GBit/s bandwidth. As in all pre-\nvious cases data transmission has signiﬁcant overheads. Note that we almost never use\nraw Ethernet directly but rather a protocol that is executed on top of the physical inter-\nconnect (such as UDP or TCP/IP). This adds further overhead. Like PCIe, Ethernet is\ndesigned to connect two devices, e.g., a computer and a switch.\n• Switches allow us to connect multiple devices in a manner where any pair of them can\ncarry out a (typically full bandwidth) point-to-point connection simultaneously. For in-\nstance, Ethernet switches might connect 40 servers at high cross-sectional bandwidth.\nNote that switches are not unique to traditional computer networks. Even PCIe lanes\ncan be switched 201 . This occurs, e.g., to connect a large number of GPUs to a host\nprocessor, as is the case for the P2 instances202.\n• NVLink is an alternative to PCIe when it comes to very high bandwidth interconnects. It\noﬀers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have six\nlinks whereas consumer-grade GPUs (RTX 2080 Ti) have only one link, operating at a\nreduced 100 Gbit/s rate. We recommend to use NCCL203 to achieve high data transfer\nbetween GPUs.\n\n591\nHardware\n204\n205\n13.4.7 More Latency Numbers\nThe summary in Section 13.4.7 and Section 13.4.7 are from Eliot Eshelman204 who maintains\nan updated version of the numbers as a GitHub gist205.\nAction\nTime\nNotes\nL1 cache reference/hit\n1.5 ns\n4 cycles\nFloating-point add/mult/FMA\n1.5 ns\n4 cycles\nL2 cache reference/hit\n5 ns\n12 ~ 17 cycles\nBranch mispredict\n6 ns\n15 ~ 20 cycles\nL3 cache hit (unshared cache)\n16 ns\n42 cycles\nL3 cache hit (shared in another core)\n25 ns\n65 cycles\nMutex lock/unlock\n25 ns\nL3 cache hit (modiﬁed in another core)\n29 ns\n75 cycles\nL3 cache hit (on a remote CPU socket)\n40 ns\n100 ~ 300 cycles (40 ~ 116 ns)\nQPI hop to a another CPU (per hop)\n40 ns\n64MB memory ref. (local CPU)\n46 ns\nTinyMemBench on Broadwell E5-2690v4\n64MB memory ref. (remote CPU)\n70 ns\nTinyMemBench on Broadwell E5-2690v4\n256MB memory ref. (local CPU)\n75 ns\nTinyMemBench on Broadwell E5-2690v4\nIntel Optane random write\n94 ns\nUCSD Non-Volatile Systems Lab\n256MB memory ref. (remote CPU)\n120 ns\nTinyMemBench on Broadwell E5-2690v4\nIntel Optane random read\n305 ns\nUCSD Non-Volatile Systems Lab\nSend 4KB over 100 Gbps HPC fabric\n1 μs\nMVAPICH2 over Intel Omni-Path\nCompress 1KB with Google Snappy\n3 μs\nSend 4KB over 10 Gbps ethernet\n10 μs\nWrite 4KB randomly to NVMe SSD\n30 μs\nDC P3608 NVMe SSD (QOS 99% is 500μs)\nTransfer 1MB to/from NVLink GPU\n30 μs\n~33GB/s on NVIDIA 40GB NVLink\nTransfer 1MB to/from PCI-E GPU\n80 μs\n~12GB/s on PCIe 3.0 x16 link\nRead 4KB randomly from NVMe SSD\n120 μs\nDC P3608 NVMe SSD (QOS 99%)\nRead 1MB sequentially from NVMe SSD\n208 μs\n~4.8GB/s DC P3608 NVMe SSD\nWrite 4KB randomly to SATA SSD\n500 μs\nDC S3510 SATA SSD (QOS 99.9%)\nRead 4KB randomly from SATA SSD\n500 μs\nDC S3510 SATA SSD (QOS 99.9%)\nRound trip within same data center\n500 μs\nOne-way ping is ~250μs\nRead 1MB sequentially from SATA SSD\n2 ms\n~550MB/s DC S3510 SATA SSD\nRead 1MB sequentially from disk\n5 ms\n~200MB/s server HDD\nRandom Disk Access (seek+rotation)\n10 ms\nSend packet CA->Netherlands->CA\n150 ms\nTable: Common Latency Numbers.\n\n592\nComputational Performance\nAction\nTime\nNotes\nGPU Shared Memory access\n30 ns\n30~90 cycles (bank conﬂicts add latency)\nGPU Global Memory access\n200 ns\n200~800 cycles\nLaunch CUDA kernel on GPU\n10 μs\nHost CPU instructs GPU to start kernel\nTransfer 1MB to/from NVLink GPU\n30 μs\n~33GB/s on NVIDIA 40GB NVLink\nTransfer 1MB to/from PCI-E GPU\n80 μs\n~12GB/s on PCI-Express x16 link\nTable: Latency Numbers for NVIDIA Tesla GPUs.\n13.4.8 Summary\n• Devices have overheads for operations. Hence it is important to aim for a small number of\nlarge transfers rather than many small ones. This applies to RAM, SSDs, networks and\nGPUs.\n• Vectorization is key for performance. Make sure you are aware of the speciﬁc abilities\nof your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 op-\nerations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA\nTuring shines at FP16, INT8, and INT4 operations.\n• Numerical overﬂow due to small data types can be a problem during training (and to a\nlesser extent during inference).\n• Aliasing can signiﬁcantly degrade performance. For instance, memory alignment on 64 bit\nCPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea to\nkeep convolution sizes aligned, e.g., to tensor cores.\n• Match your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great\nspeedup (orders of magnitude) can be achieved when ﬁtting the parameters into caches.\n• We recommend that you sketch out the performance of a novel algorithm on paper before\nverifying the experimental results. Discrepancies of an order-of-magnitude or more are\nreasons for concern.\n• Use proﬁlers to debug performance bottlenecks.\n• Training and inference hardware have diﬀerent sweet spots in terms of price and perfor-\nmance.\n13.4.9 Exercises\n1. Write C code to test whether there is any diﬀerence in speed between accessing memory\naligned or misaligned relative to the external memory interface. Hint: be careful of caching\neﬀects.\n2. Test the diﬀerence in speed between accessing memory in sequence or with a given stride.\n\n593\nTraining on Multiple GPUs\n206\n3. How could you measure the cache sizes on a CPU?\n4. How would you lay out data across multiple memory channels for maximum bandwidth?\nHow would you lay it out if you had many small threads?\n5. An enterprise-class HDD is spinning at 10,000 rpm. What is the absolutely minimum time\nan HDD needs to spend worst case before it can read data (you can assume that heads move\nalmost instantaneously)? Why are 2.5” HDDs becoming popular for commercial servers\n(relative to 3.5” and 5.25” drives)?\n6. Assume that an HDD manufacturer increases the storage density from 1 Tbit per square\ninch to 5 Tbit per square inch. How much information can you store on a ring on a 2.5”\nHDD? Is there a diﬀerence between the inner and outer tracks?\n7. Going from 8 bit to 16 bit data types increases the amount of silicon approximately by four\ntimes. Why? Why might NVIDIA have added INT4 operations to their Turing GPUs?\n8. How much faster is it to read forward through memory vs. reading backwards? Does this\nnumber diﬀer between diﬀerent computers and CPU vendors? Why? Write C code and\nexperiment with it.\n9. Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs\nneed a cache?\n10. Measure the packet overhead when sending messages across the Ethernet. Look up the\ndiﬀerence between UDP and TCP/IP connections.\n11. Direct memory access allows devices other than the CPU to write (and read) directly to\n(from) memory. Why is this a good idea?\n12. Look at the performance numbers for the Turing T4 GPU. Why does the performance\n“only” double as you go from FP16 to INT8 and INT4?\n13. What is the shortest time it should take for a packet on a round trip between San Francisco\nand Amsterdam? Hint: you can assume that the distance is 10,000 km.\nDiscussions206.\n13.5 Training on Multiple GPUs\nSo far we discussed how to train models eﬃciently on CPUs and GPUs. We even showed\nhow deep learning frameworks allow one to parallelize computation and communication au-\ntomatically between them in Section 13.3. We also showed in Section 6.7 how to list all the\navailable GPUs on a computer using the nvidia-smi command. What we did not discuss\nis how to actually parallelize deep learning training. Instead, we implied in passing that one\nwould somehow split the data across multiple devices and make it work. The present section\n\n594\nComputational Performance\nﬁlls in the details and shows how to train a network in parallel when starting from scratch.\nDetails on how to take advantage of functionality in high-level APIs is relegated to Section\n13.6. We assume that you are familiar with minibatch stochastic gradient descent algorithms\nsuch as the ones described in Section 12.5.\n13.5.1 Splitting the Problem\nLet’s start with a simple computer vision problem and a slightly archaic network, e.g., with\nmultiple layers of convolutions, pooling, and possibly a few fully connected layers in the end.\nThat is, let’s start with a network that looks quite similar to LeNet (LeCun et al., 1998) or\nAlexNet (Krizhevsky et al., 2012). Given multiple GPUs (2 if it is a desktop server, 4 on\nan AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge), we want to\npartition training in a manner as to achieve good speedup while simultaneously beneﬁtting\nfrom simple and reproducible design choices. Multiple GPUs, after all, increase both memory\nand computation ability. In a nutshell, we have the following choices, given a minibatch of\ntraining data that we want to classify.\nFirst, we could partition the network across multiple GPUs. That is, each GPU takes as input\nthe data ﬂowing into a particular layer, processes data across a number of subsequent layers\nand then sends the data to the next GPU. This allows us to process data with larger networks\nwhen compared with what a single GPU could handle. Besides, memory footprint per GPU\ncan be well controlled (it is a fraction of the total network footprint).\nHowever, the interface between layers (and thus GPUs) requires tight synchronization. This\ncan be tricky, in particular if the computational workloads are not properly matched between\nlayers. The problem is exacerbated for large numbers of GPUs. The interface between lay-\ners also requires large amounts of data transfer, such as activations and gradients. This may\noverwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet sequential\noperations are nontrivial to partition. See e.g., Mirhoseini et al. (2017) for a best eﬀort in\nthis regard. It remains a diﬃcult problem and it is unclear whether it is possible to achieve\ngood (linear) scaling on nontrivial problems. We do not recommend it unless there is excellent\nframework or operating system support for chaining together multiple GPUs.\nSecond, we could split the work layerwise. For instance, rather than computing 64 channels\non a single GPU we could split up the problem across 4 GPUs, each of which generates data\nfor 16 channels. Likewise, for a fully connected layer we could split the number of output\nunits. Fig. 13.5.1 (taken from Krizhevsky et al. (2012)) illustrates this design, where this\nstrategy was used to deal with GPUs that had a very small memory footprint (2 GB at the\ntime). This allows for good scaling in terms of computation, provided that the number of\nchannels (or units) is not too small. Besides, multiple GPUs can process increasingly larger\nnetworks since the available memory scales linearly.\nHowever, we need a very large number of synchronization or barrier operations since each\nlayer depends on the results from all the other layers. Moreover, the amount of data that needs\nto be transferred is potentially even larger than when distributing layers across GPUs. Thus,\nwe do not recommend this approach due to its bandwidth cost and complexity.\n\n595\nTraining on Multiple GPUs\nt\nFig. 13.5.1\nModel parallelism in the original AlexNet design due to limited GPU memory.\nLast, we could partition data across multiple GPUs. This way all GPUs perform the same type\nof work, albeit on diﬀerent observations. Gradients are aggregated across GPUs after each\nminibatch of training data. This is the simplest approach and it can be applied in any situation.\nWe only need to synchronize after each minibatch. That said, it is highly desirable to start\nexchanging gradients parameters already while others are still being computed. Moreover,\nlarger numbers of GPUs lead to larger minibatch sizes, thus increasing training eﬃciency.\nHowever, adding more GPUs does not allow us to train larger models.\nt\nFig. 13.5.2\nParallelization on multiple GPUs. From left to right: original problem, network\npartitioning, layerwise partitioning, data parallelism.\nA comparison of diﬀerent ways of parallelization on multiple GPUs is depicted in Fig. 13.5.2.\nBy and large, data parallelism is the most convenient way to proceed, provided that we have\naccess to GPUs with suﬃciently large memory. See also (Li et al., 2014) for a detailed de-\nscription of partitioning for distributed training. GPU memory used to be a problem in the\nearly days of deep learning. By now this issue has been resolved for all but the most unusual\ncases. We focus on data parallelism in what follows.\n13.5.2 Data Parallelism\n\n596\nComputational Performance\nAssume that there are k GPUs on a machine. Given the model to be trained, each GPU will\nmaintain a complete set of model parameters independently though parameter values across\nthe GPUs are identical and synchronized. As an example, Fig. 13.5.3 illustrates training with\ndata parallelism when k = 2.\nt\nFig. 13.5.3\nCalculation of minibatch stochastic gradient descent using data parallelism on two GPUs.\nIn general, the training proceeds as follows:\n• In any iteration of training, given a random minibatch, we split the examples in the batch\ninto k portions and distribute them evenly across the GPUs.\n• Each GPU calculates loss and gradient of the model parameters based on the minibatch\nsubset it was assigned.\n• The local gradients of each of the k GPUs are aggregated to obtain the current minibatch\nstochastic gradient.\n• The aggregate gradient is re-distributed to each GPU.\n• Each GPU uses this minibatch stochastic gradient to update the complete set of model\nparameters that it maintains.\nNote that in practice we increase the minibatch size k-fold when training on k GPUs such\nthat each GPU has the same amount of work to do as if we were training on a single GPU\nonly. On a 16-GPU server this can increase the minibatch size considerably and we may have\nto increase the learning rate accordingly. Also note that batch normalization in Section 8.5\nneeds to be adjusted, e.g., by keeping a separate batch normalization coeﬃcient per GPU. In\nwhat follows we will use a toy network to illustrate multi-GPU training.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\n597\nTraining on Multiple GPUs\n13.5.3 A Toy Network\nWe use LeNet as introduced in Section 7.6 (with slight modiﬁcations). We deﬁne it from\nscratch to illustrate parameter exchange and synchronization in detail.\n# Initialize model parameters\nscale = 0.01\nW1 = torch.randn(size=(20, 1, 3, 3)) * scale\nb1 = torch.zeros(20)\nW2 = torch.randn(size=(50, 20, 5, 5)) * scale\nb2 = torch.zeros(50)\nW3 = torch.randn(size=(800, 128)) * scale\nb3 = torch.zeros(128)\nW4 = torch.randn(size=(128, 10)) * scale\nb4 = torch.zeros(10)\nparams = [W1, b1, W2, b2, W3, b3, W4, b4]\n# Define the model\ndef lenet(X, params):\nh1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\nh1_activation = F.relu(h1_conv)\nh1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\nh2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\nh2_activation = F.relu(h2_conv)\nh2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\nh2 = h2.reshape(h2.shape[0], -1)\nh3_linear = torch.mm(h2, params[4]) + params[5]\nh3 = F.relu(h3_linear)\ny_hat = torch.mm(h3, params[6]) + params[7]\nreturn y_hat\n# Cross-entropy loss function\nloss = nn.CrossEntropyLoss(reduction='none')\n13.5.4 Data Synchronization\nFor eﬃcient multi-GPU training we need two basic operations. First we need to have the abil-\nity to distribute a list of parameters to multiple devices and to attach gradients (get_params).\nWithout parameters it is impossible to evaluate the network on a GPU. Second, we need\nthe ability to sum parameters across multiple devices, i.e., we need an allreduce func-\ntion.\ndef get_params(params, device):\nnew_params = [p.to(device) for p in params]\nfor p in new_params:\np.requires_grad_()\nreturn new_params\nLet’s try it out by copying the model parameters to one GPU.\n\n598\nComputational Performance\nnew_params = get_params(params, d2l.try_gpu(0))\nprint('b1 weight:', new_params[1])\nprint('b1 grad:', new_params[1].grad)\nb1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,␣\n,→0., 0., 0., 0., 0.],\ndevice='cuda:0', requires_grad=True)\nb1 grad: None\nSince we did not perform any computation yet, the gradient with regard to the bias param-\neter is still zero. Now let’s assume that we have a vector distributed across multiple GPUs.\nThe following allreduce function adds up all vectors and broadcasts the result back to all\nGPUs. Note that for this to work we need to copy the data to the device accumulating the\nresults.\ndef allreduce(data):\nfor i in range(1, len(data)):\ndata[0][:] += data[i].to(data[0].device)\nfor i in range(1, len(data)):\ndata[i][:] = data[0].to(data[i].device)\nLet’s test this by creating vectors with diﬀerent values on diﬀerent devices and aggregate\nthem.\ndata = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\nprint('before allreduce:\\n', data[0], '\\n', data[1])\nallreduce(data)\nprint('after allreduce:\\n', data[0], '\\n', data[1])\nbefore allreduce:\ntensor([[1., 1.]], device='cuda:0')\ntensor([[2., 2.]], device='cuda:1')\nafter allreduce:\ntensor([[3., 3.]], device='cuda:0')\ntensor([[3., 3.]], device='cuda:1')\n13.5.5 Distributing Data\nWe need a simple utility function to distribute a minibatch evenly across multiple GPUs. For\ninstance, on two GPUs we would like to have half of the data to be copied to either of the\nGPUs. Since it is more convenient and more concise, we use the built-in function from the\ndeep learning framework to try it out on a 4 × 5 matrix.\ndata = torch.arange(20).reshape(4, 5)\ndevices = [torch.device('cuda:0'), torch.device('cuda:1')]\n(continues on next page)\n\n599\nTraining on Multiple GPUs\n(continued from previous page)\nsplit = nn.parallel.scatter(data, devices)\nprint('input :', data)\nprint('load into', devices)\nprint('output:', split)\ninput : tensor([[ 0,\n1,\n2,\n3,\n4],\n[ 5,\n6,\n7,\n8,\n9],\n[10, 11, 12, 13, 14],\n[15, 16, 17, 18, 19]])\nload into [device(type='cuda', index=0), device(type='cuda', index=1)]\noutput: (tensor([[0, 1, 2, 3, 4],\n[5, 6, 7, 8, 9]], device='cuda:0'), tensor([[10, 11, 12, 13, 14],\n[15, 16, 17, 18, 19]], device='cuda:1'))\nFor later reuse we deﬁne a split_batch function that splits both data and labels.\n#@save\ndef split_batch(X, y, devices):\n\"\"\"Split `X` and `y` into multiple devices.\"\"\"\nassert X.shape[0] == y.shape[0]\nreturn (nn.parallel.scatter(X, devices),\nnn.parallel.scatter(y, devices))\n13.5.6 Training\nNow we can implement multi-GPU training on a single minibatch. Its implementation is\nprimarily based on the data parallelism approach described in this section. We will use the\nauxiliary functions we just discussed, allreduce and split_and_load, to synchronize the\ndata among multiple GPUs. Note that we do not need to write any speciﬁc code to achieve\nparallelism. Since the computational graph does not have any dependencies across devices\nwithin a minibatch, it is executed in parallel automatically.\ndef train_batch(X, y, device_params, devices, lr):\nX_shards, y_shards = split_batch(X, y, devices)\n# Loss is calculated separately on each GPU\nls = [loss(lenet(X_shard, device_W), y_shard).sum()\nfor X_shard, y_shard, device_W in zip(\nX_shards, y_shards, device_params)]\nfor l in ls:\n# Backpropagation is performed separately on each GPU\nl.backward()\n# Sum all gradients from each GPU and broadcast them to all GPUs\nwith torch.no_grad():\nfor i in range(len(device_params[0])):\nallreduce([device_params[c][i].grad for c in range(len(devices))])\n# The model parameters are updated separately on each GPU\nfor param in device_params:\nd2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch\nNow, we can deﬁne the training function. It is slightly diﬀerent from the ones used in the\n\n600\nComputational Performance\nprevious chapters: we need to allocate the GPUs and copy all the model parameters to all\nthe devices. Obviously each batch is processed using the train_batch function to deal with\nmultiple GPUs. For convenience (and conciseness of code) we compute the accuracy on a\nsingle GPU, though this is ineﬃcient since the other GPUs are idle.\ndef train(num_gpus, batch_size, lr):\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ndevices = [d2l.try_gpu(i) for i in range(num_gpus)]\n# Copy model parameters to `num_gpus` GPUs\ndevice_params = [get_params(params, d) for d in devices]\nnum_epochs = 10\nanimator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\ntimer = d2l.Timer()\nfor epoch in range(num_epochs):\ntimer.start()\nfor X, y in train_iter:\n# Perform multi-GPU training for a single minibatch\ntrain_batch(X, y, device_params, devices, lr)\ntorch.cuda.synchronize()\ntimer.stop()\n# Evaluate the model on GPU 0\nanimator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\nlambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\nprint(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\nf'on {str(devices)}')\nLet’s see how well this works on a single GPU. We ﬁrst use a batch size of 256 and a learning\nrate of 0.2.\ntrain(num_gpus=1, batch_size=256, lr=0.2)\ntest acc: 0.83, 4.1 sec/epoch on [device(type='cuda', index=0)]\nBy keeping the batch size and learning rate unchanged and increasing the number of GPUs\nto 2, we can see that the test accuracy roughly stays the same compared with the previous\nexperiment. In terms of the optimization algorithms, they are identical. Unfortunately there\nis no meaningful speedup to be gained here: the model is simply too small; moreover we only\n\n601\nTraining on Multiple GPUs\nhave a small dataset, where our slightly unsophisticated approach to implementing multi-\nGPU training suﬀered from signiﬁcant Python overhead. We will encounter more complex\nmodels and more sophisticated ways of parallelization going forward. Let’s see what happens\nnonetheless for Fashion-MNIST.\ntrain(num_gpus=2, batch_size=256, lr=0.2)\ntest acc: 0.84, 4.3 sec/epoch on [device(type='cuda', index=0), device(type=\n,→'cuda', index=1)]\n13.5.7 Summary\n• There are multiple ways to split deep network training over multiple GPUs. We could\nsplit them between layers, across layers, or across data. The former two require tightly\nchoreographed data transfers. Data parallelism is the simplest strategy.\n• Data parallel training is straightforward. However, it increases the eﬀective minibatch size\nto be eﬃcient.\n• In data parallelism, data is split across multiple GPUs, where each GPU executes its own\nforward and backward operation and subsequently gradients are aggregated and results\nare broadcast back to the GPUs.\n• We may use slightly increased learning rates for larger minibatches.\n13.5.8 Exercises\n1. When training on k GPUs, change the minibatch size from b to k · b, i.e., scale it up by\nthe number of GPUs.\n2. Compare accuracy for diﬀerent learning rates. How does it scale with the number of\nGPUs?\n\n602\nComputational Performance\n207\n3. Implement a more eﬃcient allreduce function that aggregates diﬀerent parameters on\ndiﬀerent GPUs? Why is it more eﬃcient?\n4. Implement multi-GPU test accuracy computation.\nDiscussions207.\n13.6 Concise Implementation for Multiple GPUs\nImplementing parallelism from scratch for every new model is no fun. Moreover, there is\nsigniﬁcant beneﬁt in optimizing synchronization tools for high performance. In the following\nwe will show how to do this using high-level APIs of deep learning frameworks. The mathe-\nmatics and the algorithms are the same as in Section 13.5. Quite unsurprisingly you will need\nat least two GPUs to run code of this section.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n13.6.1 A Toy Network\nLet’s use a slightly more meaningful network than LeNet from Section 13.5 that is still suf-\nﬁciently easy and quick to train. We pick a ResNet-18 variant (He et al., 2016). Since the\ninput images are tiny we modify it slightly. In particular, the diﬀerence from Section 8.6 is\nthat we use a smaller convolution kernel, stride, and padding at the beginning. Moreover, we\nremove the max-pooling layer.\n#@save\ndef resnet18(num_classes, in_channels=1):\n\"\"\"A slightly modified ResNet-18 model.\"\"\"\ndef resnet_block(in_channels, out_channels, num_residuals,\nfirst_block=False):\nblk = []\nfor i in range(num_residuals):\nif i == 0 and not first_block:\nblk.append(d2l.Residual(out_channels, use_1x1conv=True,\nstrides=2))\nelse:\nblk.append(d2l.Residual(out_channels))\nreturn nn.Sequential(*blk)\n# This model uses a smaller convolution kernel, stride, and padding and\n# removes the max-pooling layer\nnet = nn.Sequential(\n(continues on next page)\n\n603\nConcise Implementation for Multiple GPUs\n(continued from previous page)\nnn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\nnn.BatchNorm2d(64),\nnn.ReLU())\nnet.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\nnet.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\nnet.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\nnet.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\nnet.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\nnet.add_module(\"fc\", nn.Sequential(nn.Flatten(),\nnn.Linear(512, num_classes)))\nreturn net\n13.6.2 Network Initialization\nWe will initialize the network inside the training loop. For a refresher on initialization methods\nsee Section 5.4.\nnet = resnet18(10)\n# Get a list of GPUs\ndevices = d2l.try_all_gpus()\n# We will initialize the network inside the training loop\n13.6.3 Training\nAs before, the training code needs to perform several basic functions for eﬃcient paral-\nlelism:\n• Network parameters need to be initialized across all devices.\n• While iterating over the dataset minibatches are to be divided across all devices.\n• We compute the loss and its gradient in parallel across devices.\n• Gradients are aggregated and parameters are updated accordingly.\nIn the end we compute the accuracy (again in parallel) to report the ﬁnal performance of the\nnetwork. The training routine is quite similar to implementations in previous chapters, except\nthat we need to split and aggregate data.\ndef train(net, num_gpus, batch_size, lr):\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ndevices = [d2l.try_gpu(i) for i in range(num_gpus)]\ndef init_weights(module):\nif type(module) in [nn.Linear, nn.Conv2d]:\nnn.init.normal_(module.weight, std=0.01)\nnet.apply(init_weights)\n# Set the model on multiple GPUs\nnet = nn.DataParallel(net, device_ids=devices)\n(continues on next page)\n\n604\nComputational Performance\n(continued from previous page)\ntrainer = torch.optim.SGD(net.parameters(), lr)\nloss = nn.CrossEntropyLoss()\ntimer, num_epochs = d2l.Timer(), 10\nanimator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\nfor epoch in range(num_epochs):\nnet.train()\ntimer.start()\nfor X, y in train_iter:\ntrainer.zero_grad()\nX, y = X.to(devices[0]), y.to(devices[0])\nl = loss(net(X), y)\nl.backward()\ntrainer.step()\ntimer.stop()\nanimator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\nprint(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\nf'on {str(devices)}')\nLet’s see how this works in practice. As a warm-up we train the network on a single GPU.\ntrain(net, num_gpus=1, batch_size=256, lr=0.1)\ntest acc: 0.92, 13.1 sec/epoch on [device(type='cuda', index=0)]\nNext we use 2 GPUs for training. Compared with LeNet evaluated in Section 13.5, the model\nfor ResNet-18 is considerably more complex. This is where parallelization shows its advan-\ntage. The time for computation is meaningfully larger than the time for synchronizing param-\neters. This improves scalability since the overhead for parallelization is less relevant.\ntrain(net, num_gpus=2, batch_size=512, lr=0.2)\ntest acc: 0.79, 10.6 sec/epoch on [device(type='cuda', index=0), device(type=\n,→'cuda', index=1)]\n\n605\nParameter Servers\n208\n13.6.4 Summary\n• Data is automatically evaluated on the devices where the data can be found.\n• Take care to initialize the networks on each device before trying to access the parameters\non that device. Otherwise you will encounter an error.\n• The optimization algorithms automatically aggregate over multiple GPUs.\n13.6.5 Exercises\n1. This section uses ResNet-18. Try diﬀerent epochs, batch sizes, and learning rates. Use\nmore GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an\nAWS p2.16xlarge instance)?\n2. Sometimes, diﬀerent devices provide diﬀerent computing power. We could use the GPUs\nand the CPU at the same time. How should we divide the work? Is it worth the eﬀort?\nWhy? Why not?\nDiscussions208.\n13.7 Parameter Servers\nAs we move from a single GPU to multiple GPUs and then to multiple servers containing\nmultiple GPUs, possibly all spread out across multiple racks and network switches, our algo-\nrithms for distributed and parallel training need to become much more sophisticated. Details\nmatter since diﬀerent interconnects have very diﬀerent bandwidth (e.g., NVLink can oﬀer\nup to 100 GB/s across 6 links in an appropriate setting, PCIe 4.0 (16-lane) oﬀers 32 GB/s,\nwhile even high speed 100GbE Ethernet only amounts to 10 GB/s). At the same time it is un-\nreasonable to expect that a statistical modeler be an expert in networking and systems.\n\n606\nComputational Performance\nThe core idea of the parameter server was introduced in Smola and Narayanamurthy (2010) in\nthe context of distributed latent variable models. A description of the push and pull semantics\nthen followed in Ahmed et al. (2012) and a description of the system and an open source\nlibrary followed in Li et al. (2014). In the following we will motivate the components needed\nfor eﬃciency.\n13.7.1 Data-Parallel Training\nLet’s review the data parallel training approach to distributed training. We will use this to the\nexclusion of all others in this section since it is signiﬁcantly simpler to implement in practice.\nThere are virtually no use cases (besides deep learning on graphs) where any other strategy for\nparallelism is preferred since GPUs have plenty of memory nowadays. Fig. 13.7.1 describes\nthe variant of data parallelism that we implemented in Section 13.5. The key aspect in it\nis that the aggregation of gradients occurs on one single GPU (GPU 0) before the updated\nparameters are rebroadcast to all GPUs.\nt\nFig. 13.7.1\nLeft: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss\nand gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens\nand the parameters are re-distributed to all GPUs.\nIn retrospect, the decision to aggregate on GPU 0 seems rather ad-hoc. After all, we might\njust as well aggregate on the CPU. In fact, we could even decide to aggregate some of the\nparameters on one GPU and some others on another. Provided that the optimization algorithm\n\n607\nParameter Servers\n209\n210\nsupports this, there is no real reason for why we could not. For instance, if we have four\nparameter vectors with associated gradients g1, . . ., g4 we could aggregate the gradients on\none GPU for each gi (i = 1, . . ., 4).\nThis reasoning seems arbitrary and frivolous. After all, the mathematics is the same through-\nout. However, we are dealing with real physical hardware where diﬀerent buses have diﬀer-\nent bandwidth as discussed in Section 13.4. Consider a real 4-way GPU server as described\nin Fig. 13.7.2. If it is particularly well connected, it might have a 100 GbE network card.\nMore typical numbers are in the 1–10 GbE range with an eﬀective bandwidth of 100 MB/s\nto 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g.,\nconsumer-grade Intel CPUs have 24 lanes) we need a multiplexer 209 . The bandwidth from\nthe CPU on a 16x Gen3 link is 16 GB/s. This is also the speed at which each of the GPUs\nis connected to the switch. This means that it is more eﬀective to communicate between the\ndevices.\nt\nFig. 13.7.2\nA 4-way GPU server.\nFor the sake of the argument let’s assume that the gradients are of 160 MB. In this case it\ntakes 30 ms to send the gradients from all 3 remaining GPUs to the fourth one (each transfer\ntakes 10 ms = 160 MB / 16 GB/s). Adding another 30 ms to transmit the weight vectors back\nwe arrive at a total of 60 ms. If we send all data to the CPU we incur a penalty of 40 ms\nsince each of the four GPUs needs to send the data to the CPU, yielding a total of 80 ms.\nLastly assume that we are able to split the gradients into 4 parts of 40 MB each. Now we can\naggregate each of the parts on a diﬀerent GPU simultaneously since the PCIe switch oﬀers\na full-bandwidth operation between all links. Instead of 30 ms this takes 7.5 ms, yielding a\ntotal of 15 ms for a synchronization operation. In short, depending on how we synchronize\nparameters the same operation can take anywhere from 15 ms to 80 ms. Fig. 13.7.3 depicts\nthe diﬀerent strategies for exchanging parameters.\nNote that we have yet another tool at our disposal when it comes to improving performance:\nin a deep network it takes some time to compute all gradients from the top to the bottom. We\ncan begin synchronizing gradients for some parameter groups even while we are still busy\ncomputing them for others. See e.g., Sergeev and Del Balso (2018) for details on how to do\nthis in Horovod210.\n\n608\nComputational Performance\nt\nFig. 13.7.3\nParameter synchronization strategies.\n13.7.2 Ring Synchronization\nWhen it comes to synchronization on modern deep learning hardware we often encounter sig-\nniﬁcantly bespoke network connectivity. For instance, the AWS p3.16xlarge and NVIDIA\nDGX-2 instances share the connectivity structure of Fig. 13.7.4. Each GPU connects to a\nhost CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also\nhas 6 NVLink connections, each of which is capable of transferring 300 Gbit/s bidirection-\nally. This amounts to around 18 GB/s per link per direction. In short, the aggregate NVLink\nbandwidth is signiﬁcantly higher than the PCIe bandwidth. The question is how to use it most\neﬃciently.\nt\nFig. 13.7.4\nNVLink connectivity on 8 V100 GPU servers (image courtesy of NVIDIA).\nIt turns out that the optimal synchronization strategy is to decompose the network into two\nrings and to use them to synchronize data directly (Wang et al., 2018). Fig. 13.7.5 illustrates\n\n609\nParameter Servers\nthat the network can be decomposed into one ring (1-2-3-4-5-6-7-8-1) with double NVLink\nbandwidth and into one (1-4-6-3-5-8-2-7-1) with regular bandwidth. Designing an eﬃcient\nsynchronization protocol in this case is nontrivial.\nt\nFig. 13.7.5\nDecomposition of the NVLink network into two rings.\nConsider the following thought experiment: given a ring of n computing nodes (or GPUs) we\ncan send gradients from the ﬁrst to the second node. There it is added to the local gradient and\nsent on to the third node, and so on. After n −1 steps the aggregate gradient can be found in\nthe last-visited node. That is, the time to aggregate gradients grows linearly with the number\nof nodes. But if we do this the algorithm is quite ineﬃcient. After all, at any time there is only\none of the nodes communicating. What if we broke the gradients into n chunks and started\nsynchronizing chunk i starting at node i? Since each chunk is of size 1/n the total time is\nnow (n −1)/n ≈1. In other words, the time spent to aggregate gradients does not grow as\nwe increase the size of the ring. This is quite an astonishing result. Fig. 13.7.6 illustrates the\nsequence of steps on n = 4 nodes.\nIf we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive at\napproximately 2 · 160MB/(3 · 18GB/s) ≈6ms. This is better than using the PCIe bus,\neven though we are now using 8 GPUs. Note that in practice these numbers are a bit worse,\nsince deep learning frameworks often fail to assemble communication into large burst trans-\nfers.\nNote that there is a common misconception that ring synchronization is fundamentally dif-\n\n610\nComputational Performance\nt\nFig. 13.7.6\nRing synchronization across 4 nodes. Each node starts transmitting parts of gradients to its\nleft neighbor until the assembled gradient can be found in its right neighbor.\nferent from other synchronization algorithms. The only diﬀerence is that the synchronization\npath is somewhat more elaborate when compared with a simple tree.\n13.7.3 Multi-Machine Training\nDistributed training on multiple machines adds a further challenge: we need to communicate\nwith servers that are only connected across a comparatively lower bandwidth fabric that can\nbe over an order of magnitude slower in some cases. Synchronization across devices is tricky.\nAfter all, diﬀerent machines running training code will have subtly diﬀerent speed. Hence we\nneed to synchronize them if we want to use synchronous distributed optimization. Fig. 13.7.7\nillustrates how distributed parallel training occurs.\n1. A (diﬀerent) batch of data is read on each machine, split across multiple GPUs and trans-\nferred to GPU memory. There predictions and gradients are computed on each GPU batch\nseparately.\n2. The gradients from all local GPUs are aggregated on one GPU (or parts of it are aggre-\ngated over diﬀerent GPUs).\n3. The gradients are sent to the CPUs.\n4. The CPUs send the gradients to a central parameter server which aggregates all the gra-\ndients.\n5. The aggregate gradients are then used to update the parameters and the updated param-\neters are broadcast back to the individual CPUs.\n\n611\nParameter Servers\n6. The information is sent to one (or multiple) GPUs.\n7. The updated parameters are spread across all GPUs.\nt\nFig. 13.7.7\nMulti-machine multi-GPU distributed parallel training.\nEach of these operations seems rather straightforward. And, indeed, they can be carried out\neﬃciently within a single machine. Once we look at multiple machines, though, we can see\nthat the central parameter server becomes the bottleneck. After all, the bandwidth per server\nis limited, hence for m workers the time it takes to send all gradients to the server is O(m).\nWe can break through this barrier by increasing the number of servers to n. At this point\neach server only needs to store O(1/n) of the parameters, hence the total time for updates\nand optimization becomes O(m/n). Matching both numbers yields constant scaling regard-\nless of how many workers we are dealing with. In practice we use the same machines both\nas workers and as servers. Fig. 13.7.8 illustrates the design (see also (Li et al., 2014) for\ndetails). In particular, ensuring that multiple machines work without unreasonable delays is\nnontrivial.\n13.7.4 Key–Value Stores\nImplementing the steps required for distributed multi-GPU training in practice is nontriv-\nial. This is why it pays to use a common abstraction, namely that of a key–value store with\nredeﬁned update semantics.\nAcross many workers and many GPUs the computation for gradienti can be deﬁned as\ngi =\n∑\nk∈workers\n∑\nj∈GPUs\ngijk,\n(13.7.1)\nwhere gijk is part of gradient i split on GPU j of worker k. The key aspect in this operation\n\n612\nComputational Performance\nt\nFig. 13.7.8\nTop: a single parameter server is a bottleneck since its bandwidth is ﬁnite. Bottom:\nmultiple parameter servers store parts of the parameters with aggregate bandwidth.\nis that it is a commutative reduction, that is, it turns many vectors into one and the order in\nwhich the operation is applied does not matter. This is great for our purposes since we do not\n(need to) have ﬁne grained control over when which gradient is received. Besides, note that\nthis operation is independent among diﬀerent i.\nThis allows us to deﬁne the following two operations: push, which accumulates gradients,\nand pull, which retrieves aggregate gradients. Since we have many diﬀerent sets of gradients\n(after all, we have many layers), we need to index the gradients with a key i. This similarity\nto key–value stores, such as the one introduced in Dynamo (DeCandia et al., 2007) is not by\ncoincidence. They, too, satisfy many similar characteristics, in particular when it comes to\ndistributing the parameters across multiple servers.\nThe push and pull operations for key-value stores are described as follows:\n• push(key, value) sends a particular gradient (the value) from a worker to a common stor-\nage. There the value is aggregated, e.g., by summing it up.\n• pull(key, value) retrieves an aggregate value from common storage, e.g., after combining\nthe gradients from all workers.\nBy hiding all the complexity about synchronization behind a simple push and pull operation\nwe can decouple the concerns of statistical modelers who want to be able to express opti-\n\n613\nParameter Servers\n211\nmization in simple terms and the system engineers who need to deal with the complexity\ninherent in distributed synchronization.\n13.7.5 Summary\n• Synchronization needs to be highly adaptive to speciﬁc network infrastructure and con-\nnectivity within a server. This can make a signiﬁcant diﬀerence to the time it takes to\nsynchronize.\n• Ring-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not\nso much.\n• A hierarchical synchronization strategy works well when adding multiple parameter servers\nfor increased bandwidth.\n13.7.6 Exercises\n1. Can you increase the ring synchronization even further? Hint: you can send messages in\nboth directions.\n2. Is it possible to allow asynchronous communication (while computation is still ongoing)?\nHow does it aﬀect performance?\n3. What if we lost a server during a long-running computation? How can we design a fault\ntolerance mechanism to avoid restarting the computation fully?\nDiscussions211.\n\n14\nComputer Vision\nWhether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart ﬁlters,\nmany applications in the ﬁeld of computer vision are closely related to our current and future\nlives. In recent years, deep learning has been the transformative power for advancing the\nperformance of computer vision systems. It can be said that the most advanced computer\nvision applications are almost inseparable from deep learning. In view of this, this chapter\nwill focus on the ﬁeld of computer vision, and investigate methods and applications that have\nrecently been inﬂuential in academia and industry.\nIn Chapter 7 and Chapter 8, we studied various convolutional neural networks that are com-\nmonly used in computer vision, and applied them to simple image classiﬁcation tasks. At the\nbeginning of this chapter, we will describe two methods that may improve model general-\nization, namely image augmentation and ﬁne-tuning, and apply them to image classiﬁcation.\nSince deep neural networks can eﬀectively represent images in multiple levels, such layer-\nwise representations have been successfully used in various computer vision tasks such as\nobject detection, semantic segmentation, and style transfer. Following the key idea of leverag-\ning layerwise representations in computer vision, we will begin with major components and\ntechniques for object detection. Next, we will show how to use fully convolutional networks\nfor semantic segmentation of images. Then we will explain how to use style transfer tech-\nniques to generate images like the cover of this book. In the end, we conclude this chapter by\napplying the materials of this chapter and several previous chapters on two popular computer\nvision benchmark datasets.\n14.1 Image Augmentation\nIn Section 8.1, we mentioned that large datasets are a prerequisite for the success of deep\nneural networks in various applications. Image augmentation generates similar but distinct\ntraining examples after a series of random changes to the training images, thereby expanding\nthe size of the training set. Alternatively, image augmentation can be motivated by the fact that\nrandom tweaks of training examples allow models to rely less on certain attributes, thereby\nimproving their generalization ability. For example, we can crop an image in diﬀerent ways\nto make the object of interest appear in diﬀerent positions, thereby reducing the dependence\nof a model on the position of the object. We can also adjust factors such as brightness and\n614\n\n615\nImage Augmentation\ncolor to reduce a model’s sensitivity to color. It is probably true that image augmentation\nwas indispensable for the success of AlexNet at that time. In this section we will discuss this\nwidely used technique in computer vision.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\n14.1.1 Common Image Augmentation Methods\nIn our investigation of common image augmentation methods, we will use the following 400×\n500 image an example.\nd2l.set_figsize()\nimg = d2l.Image.open('../img/cat1.jpg')\nd2l.plt.imshow(img);\nMost image augmentation methods have a certain degree of randomness. To make it easier for\nus to observe the eﬀect of image augmentation, next we deﬁne an auxiliary function apply.\nThis function runs the image augmentation method aug multiple times on the input image\nimg and shows all the results.\ndef apply(img, aug, num_rows=2, num_cols=4, scale=1.5):\nY = [aug(img) for _ in range(num_rows * num_cols)]\nd2l.show_images(Y, num_rows, num_cols, scale=scale)\nFlipping and Cropping\nFlipping the image left and right usually does not change the category of the object. This is\none of the earliest and most widely used methods of image augmentation. Next, we use the\n\n616\nComputer Vision\ntransforms module to create the RandomHorizontalFlip instance, which ﬂips an image\nleft and right with a 50% chance.\napply(img, torchvision.transforms.RandomHorizontalFlip())\nFlipping up and down is not as common as ﬂipping left and right. But at least for this example\nimage, ﬂipping up and down does not hinder recognition. Next, we create a RandomVerti-\ncalFlip instance to ﬂip an image up and down with a 50% chance.\napply(img, torchvision.transforms.RandomVerticalFlip())\nIn the example image we used, the cat is in the middle of the image, but this may not be the\ncase in general. In Section 7.5, we explained that the pooling layer can reduce the sensitivity\nof a convolutional layer to the target position. In addition, we can also randomly crop the\nimage to make objects appear in diﬀerent positions in the image at diﬀerent scales, which\ncan also reduce the sensitivity of a model to the target position.\nIn the code below, we randomly crop an area with an area of 10% ∼100% of the original area\neach time, and the ratio of width to height of this area is randomly selected from 0.5 ∼2.\nThen, the width and height of the region are both scaled to 200 pixels. Unless otherwise\nspeciﬁed, the random number between a and b in this section refers to a continuous value\nobtained by random and uniform sampling from the interval [a, b].\n\n617\nImage Augmentation\nshape_aug = torchvision.transforms.RandomResizedCrop(\n(200, 200), scale=(0.1, 1), ratio=(0.5, 2))\napply(img, shape_aug)\nChanging Colors\nAnother augmentation method is changing colors. We can change four aspects of the image\ncolor: brightness, contrast, saturation, and hue. In the example below, we randomly change\nthe brightness of the image to a value between 50% (1 −0.5) and 150% (1 + 0.5) of the\noriginal image.\napply(img, torchvision.transforms.ColorJitter(\nbrightness=0.5, contrast=0, saturation=0, hue=0))\nSimilarly, we can randomly change the hue of the image.\napply(img, torchvision.transforms.ColorJitter(\nbrightness=0, contrast=0, saturation=0, hue=0.5))\nWe can also create a RandomColorJitter instance and set how to randomly change the\nbrightness, contrast, saturation, and hue of the image at the same time.\n\n618\nComputer Vision\ncolor_aug = torchvision.transforms.ColorJitter(\nbrightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\napply(img, color_aug)\nCombining Multiple Image Augmentation Methods\nIn practice, we will combine multiple image augmentation methods. For example, we can\ncombine the diﬀerent image augmentation methods deﬁned above and apply them to each\nimage via a Compose instance.\naugs = torchvision.transforms.Compose([\ntorchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\napply(img, augs)\n14.1.2 Training with Image Augmentation\nLet’s train a model with image augmentation. Here we use the CIFAR-10 dataset instead\nof the Fashion-MNIST dataset that we used before. This is because the position and size of\nthe objects in the Fashion-MNIST dataset have been normalized, while the color and size of\nthe objects in the CIFAR-10 dataset have more signiﬁcant diﬀerences. The ﬁrst 32 training\nimages in the CIFAR-10 dataset are shown below.\n\n619\nImage Augmentation\nall_images = torchvision.datasets.CIFAR10(train=True, root=\"../data\",\ndownload=True)\nd2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);\nFiles already downloaded and verified\nIn order to obtain deﬁnitive results during prediction, we usually only apply image augmenta-\ntion to training examples, and do not use image augmentation with random operations during\nprediction. Here we only use the simplest random left-right ﬂipping method. In addition, we\nuse a ToTensor instance to convert a minibatch of images into the format required by the\ndeep learning framework, i.e., 32-bit ﬂoating point numbers between 0 and 1 with the shape\nof (batch size, number of channels, height, width).\ntrain_augs = torchvision.transforms.Compose([\ntorchvision.transforms.RandomHorizontalFlip(),\ntorchvision.transforms.ToTensor()])\ntest_augs = torchvision.transforms.Compose([\ntorchvision.transforms.ToTensor()])\nNext, we deﬁne an auxiliary function to facilitate reading the image and applying image aug-\n\n620\nComputer Vision\nmentation. The transform argument provided by PyTorch’s dataset applies augmentation\nto transform the images. For a detailed introduction to DataLoader, please refer to Section\n4.2.\ndef load_cifar10(is_train, augs, batch_size):\ndataset = torchvision.datasets.CIFAR10(root=\"../data\", train=is_train,\ntransform=augs, download=True)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\nshuffle=is_train, num_workers=d2l.get_dataloader_workers())\nreturn dataloader\nMulti-GPU Training\nWe train the ResNet-18 model from Section 8.6 on the CIFAR-10 dataset. Recall the intro-\nduction to multi-GPU training in Section 13.6. In the following, we deﬁne a function to train\nand evaluate the model using multiple GPUs.\n#@save\ndef train_batch_ch13(net, X, y, loss, trainer, devices):\n\"\"\"Train for a minibatch with multiple GPUs (defined in Chapter 13).\"\"\"\nif isinstance(X, list):\n# Required for BERT fine-tuning (to be covered later)\nX = [x.to(devices[0]) for x in X]\nelse:\nX = X.to(devices[0])\ny = y.to(devices[0])\nnet.train()\ntrainer.zero_grad()\npred = net(X)\nl = loss(pred, y)\nl.sum().backward()\ntrainer.step()\ntrain_loss_sum = l.sum()\ntrain_acc_sum = d2l.accuracy(pred, y)\nreturn train_loss_sum, train_acc_sum\n#@save\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices=d2l.try_all_gpus()):\n\"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\ntimer, num_batches = d2l.Timer(), len(train_iter)\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\nlegend=['train loss', 'train acc', 'test acc'])\nnet = nn.DataParallel(net, device_ids=devices).to(devices[0])\nfor epoch in range(num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples,\n# no. of predictions\nmetric = d2l.Accumulator(4)\nfor i, (features, labels) in enumerate(train_iter):\ntimer.start()\n(continues on next page)\n\n621\nImage Augmentation\n(continued from previous page)\nl, acc = train_batch_ch13(\nnet, features, labels, loss, trainer, devices)\nmetric.add(l, acc, labels.shape[0], labels.numel())\ntimer.stop()\nif (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\nanimator.add(epoch + (i + 1) / num_batches,\n(metric[0] / metric[2], metric[1] / metric[3],\nNone))\ntest_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator.add(epoch + 1, (None, None, test_acc))\nprint(f'loss {metric[0] / metric[2]:.3f}, train acc '\nf'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\nprint(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\nf'{str(devices)}')\nNow we can deﬁne the train_with_data_aug function to train the model with image aug-\nmentation. This function gets all available GPUs, uses Adam as the optimization algorithm,\napplies image augmentation to the training dataset, and ﬁnally calls the train_ch13 function\njust deﬁned to train and evaluate the model.\nbatch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)\nnet.apply(d2l.init_cnn)\ndef train_with_data_aug(train_augs, test_augs, net, lr=0.001):\ntrain_iter = load_cifar10(True, train_augs, batch_size)\ntest_iter = load_cifar10(False, test_augs, batch_size)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nnet(next(iter(train_iter))[0])\ntrain_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\nLet’s train the model using image augmentation based on random left-right ﬂipping.\ntrain_with_data_aug(train_augs, test_augs, net)\nloss 0.226, train acc 0.920, test acc 0.840\n3322.3 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n14.1.3 Summary\n• Image augmentation generates random images based on existing training data to improve\nthe generalization ability of models.\n• In order to obtain deﬁnitive results during prediction, we usually only apply image augmen-\ntation to training examples, and do not use image augmentation with random operations\nduring prediction.\n\n622\nComputer Vision\n212\n• Deep learning frameworks provide many diﬀerent image augmentation methods, which\ncan be applied simultaneously.\n14.1.4 Exercises\n1. Train the model without using image augmentation: train_with_data_aug(test_augs,\ntest_augs). Compare training and testing accuracy when using and not using image\naugmentation. Can this comparative experiment support the argument that image aug-\nmentation can mitigate overﬁtting? Why?\n2. Combine multiple diﬀerent image augmentation methods in model training on the CIFAR-\n10 dataset. Does it improve test accuracy?\n3. Refer to the online documentation of the deep learning framework. What other image\naugmentation methods does it also provide?\nDiscussions212.\n14.2 Fine-Tuning\nIn earlier chapters, we discussed how to train models on the Fashion-MNIST training dataset\nwith only 60000 images. We also described ImageNet, the most widely used large-scale image\ndataset in academia, which has more than 10 million images and 1000 objects. However, the\nsize of the dataset that we usually encounter is between those of the two datasets.\nSuppose that we want to recognize diﬀerent types of chairs from images, and then recommend\npurchase links to users. One possible method is to ﬁrst identify 100 common chairs, take\n1000 images of diﬀerent angles for each chair, and then train a classiﬁcation model on the\ncollected image dataset. Although this chair dataset may be larger than the Fashion-MNIST\ndataset, the number of examples is still less than one-tenth of that in ImageNet. This may\n\n623\nFine-Tuning\nlead to overﬁtting of complicated models that are suitable for ImageNet on this chair dataset.\nBesides, due to the limited amount of training examples, the accuracy of the trained model\nmay not meet practical requirements.\nIn order to address the above problems, an obvious solution is to collect more data. However,\ncollecting and labeling data can take a lot of time and money. For example, in order to col-\nlect the ImageNet dataset, researchers have spent millions of dollars from research funding.\nAlthough the current data collection cost has been signiﬁcantly reduced, this cost still cannot\nbe ignored.\nAnother solution is to apply transfer learning to transfer the knowledge learned from the\nsource dataset to the target dataset. For example, although most of the images in the Ima-\ngeNet dataset have nothing to do with chairs, the model trained on this dataset may extract\nmore general image features, which can help identify edges, textures, shapes, and object\ncomposition. These similar features may also be eﬀective for recognizing chairs.\n14.2.1 Steps\nIn this section, we will introduce a common technique in transfer learning: ﬁne-tuning. As\nshown in Fig. 14.2.1, ﬁne-tuning consists of the following four steps:\n1. Pretrain a neural network model, i.e., the source model, on a source dataset (e.g., the\nImageNet dataset).\n2. Create a new neural network model, i.e., the target model. This copies all model designs\nand their parameters on the source model except the output layer. We assume that these\nmodel parameters contain the knowledge learned from the source dataset and this knowl-\nedge will also be applicable to the target dataset. We also assume that the output layer of\nthe source model is closely related to the labels of the source dataset; thus it is not used\nin the target model.\n3. Add an output layer to the target model, whose number of outputs is the number of cate-\ngories in the target dataset. Then randomly initialize the model parameters of this layer.\n4. Train the target model on the target dataset, such as a chair dataset. The output layer will\nbe trained from scratch, while the parameters of all the other layers are ﬁne-tuned based\non the parameters of the source model.\nWhen target datasets are much smaller than source datasets, ﬁne-tuning helps to improve\nmodels’ generalization ability.\n14.2.2 Hot Dog Recognition\nLet’s demonstrate ﬁne-tuning via a concrete case: hot dog recognition. We will ﬁne-tune a\nResNet model on a small dataset, which was pretrained on the ImageNet dataset. This small\ndataset consists of thousands of images with and without hot dogs. We will use the ﬁne-tuned\nmodel to recognize hot dogs from images.\n\n624\nComputer Vision\nt\nFig. 14.2.1\nFine tuning.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\nReading the Dataset\nThe hot dog dataset we use was taken from online images. This dataset consists of 1400\npositive-class images containing hot dogs, and as many negative-class images containing other\nfoods. 1000 images of both classes are used for training and the rest are for testing.\nAfter unzipping the downloaded dataset, we obtain two folders hotdog/train and hotdog/\ntest. Both folders have hotdog and not-hotdog subfolders, either of which contains images\nof the corresponding class.\n#@save\nd2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',\n'fba480ffa8aa7e0febbb511d181409f899b9baa5')\ndata_dir = d2l.download_extract('hotdog')\nWe create two instances to read all the image ﬁles in the training and testing datasets, respec-\ntively.\ntrain_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\ntest_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))\n\n625\nFine-Tuning\nThe ﬁrst 8 positive examples and the last 8 negative images are shown below. As you can see,\nthe images vary in size and aspect ratio.\nhotdogs = [train_imgs[i][0] for i in range(8)]\nnot_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\nd2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);\nDuring training, we ﬁrst crop a random area of random size and random aspect ratio from the\nimage, and then scale this area to a 224 × 224 input image. During testing, we scale both the\nheight and width of an image to 256 pixels, and then crop a central 224×224 area as input. In\naddition, for the three RGB (red, green, and blue) color channels we standardize their values\nchannel by channel. Concretely, the mean value of a channel is subtracted from each value of\nthat channel and then the result is divided by the standard deviation of that channel.\n# Specify the means and standard deviations of the three RGB channels to\n# standardize each channel\nnormalize = torchvision.transforms.Normalize(\n[0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\ntrain_augs = torchvision.transforms.Compose([\ntorchvision.transforms.RandomResizedCrop(224),\ntorchvision.transforms.RandomHorizontalFlip(),\ntorchvision.transforms.ToTensor(),\nnormalize])\ntest_augs = torchvision.transforms.Compose([\ntorchvision.transforms.Resize([256, 256]),\ntorchvision.transforms.CenterCrop(224),\ntorchvision.transforms.ToTensor(),\nnormalize])\nDeﬁning and Initializing the Model\nWe use ResNet-18, which was pretrained on the ImageNet dataset, as the source model. Here,\nwe specify pretrained=True to automatically download the pretrained model parameters.\nIf this model is used for the ﬁrst time, Internet connection is required for download.\n\n626\nComputer Vision\npretrained_net = torchvision.models.resnet18(pretrained=True)\nThe pretrained source model instance contains a number of feature layers and an output layer\nfc. The main purpose of this division is to facilitate the ﬁne-tuning of model parameters of all\nlayers but the output layer. The member variable fc of source model is given below.\npretrained_net.fc\nLinear(in_features=512, out_features=1000, bias=True)\nAs a fully connected layer, it transforms ResNet’s ﬁnal global average pooling outputs into\n1000 class outputs of the ImageNet dataset. We then construct a new neural network as the\ntarget model. It is deﬁned in the same way as the pretrained source model except that its\nnumber of outputs in the ﬁnal layer is set to the number of classes in the target dataset (rather\nthan 1000).\nIn the code below, the model parameters before the output layer of the target model in-\nstance finetune_net are initialized to model parameters of the corresponding layers from\nthe source model. Since these model parameters were obtained via pretraining on ImageNet,\nthey are eﬀective. Therefore, we can only use a small learning rate to ﬁne-tune such pretrained\nparameters. In contrast, model parameters in the output layer are randomly initialized and\ngenerally require a larger learning rate to be learned from scratch. Letting the base learning\nrate be η, a learning rate of 10η will be used to iterate the model parameters in the output\nlayer.\nfinetune_net = torchvision.models.resnet18(pretrained=True)\nfinetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\nnn.init.xavier_uniform_(finetune_net.fc.weight);\nFine-Tuning the Model\nFirst, we deﬁne a training function train_fine_tuning that uses ﬁne-tuning so it can be\ncalled multiple times.\n# If `param_group=True`, the model parameters in the output layer will be\n# updated using a learning rate ten times greater\ndef train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\nparam_group=True):\ntrain_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'train'), transform=train_augs),\nbatch_size=batch_size, shuffle=True)\ntest_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'test'), transform=test_augs),\nbatch_size=batch_size)\n(continues on next page)\n\n627\nFine-Tuning\n(continued from previous page)\ndevices = d2l.try_all_gpus()\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nif param_group:\nparams_1x = [param for name, param in net.named_parameters()\nif name not in [\"fc.weight\", \"fc.bias\"]]\ntrainer = torch.optim.SGD([{'params': params_1x},\n{'params': net.fc.parameters(),\n'lr': learning_rate * 10}],\nlr=learning_rate, weight_decay=0.001)\nelse:\ntrainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\nweight_decay=0.001)\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices)\nWe set the base learning rate to a small value in order to ﬁne-tune the model parameters\nobtained via pretraining. Based on the previous settings, we will train the output layer pa-\nrameters of the target model from scratch using a learning rate ten times greater.\ntrain_fine_tuning(finetune_net, 5e-5)\nloss 0.204, train acc 0.918, test acc 0.926\n659.8 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nFor comparison, we deﬁne an identical model, but initialize all of its model parameters to\nrandom values. Since the entire model needs to be trained from scratch, we can use a larger\nlearning rate.\nscratch_net = torchvision.models.resnet18()\nscratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\ntrain_fine_tuning(scratch_net, 5e-4, param_group=False)\n\n628\nComputer Vision\nloss 0.459, train acc 0.819, test acc 0.871\n1284.3 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nAs we can see, the ﬁne-tuned model tends to perform better for the same epoch because its\ninitial parameter values are more eﬀective.\n14.2.3 Summary\n• Transfer learning transfers knowledge learned from the source dataset to the target dataset.\nFine-tuning is a common technique for transfer learning.\n• The target model copies all model designs with their parameters from the source model\nexcept the output layer, and ﬁne-tunes these parameters based on the target dataset. In\ncontrast, the output layer of the target model needs to be trained from scratch.\n• Generally, ﬁne-tuning parameters uses a smaller learning rate, while training the output\nlayer from scratch can use a larger learning rate.\n14.2.4 Exercises\n1. Keep increasing the learning rate of finetune_net. How does the accuracy of the model\nchange?\n2. Further adjust hyperparameters of finetune_net and scratch_net in the comparative\nexperiment. Do they still diﬀer in accuracy?\n3. Set the parameters before the output layer of finetune_net to those of the source model\nand do not update them during training. How does the accuracy of the model change?\nYou can use the following code.\nfor param in finetune_net.parameters():\nparam.requires_grad = False\n\n629\nObject Detection and Bounding Boxes\n213\n4. In fact, there is a “hotdog” class in the ImageNet dataset. Its corresponding weight pa-\nrameter in the output layer can be obtained via the following code. How can we leverage\nthis weight parameter?\nweight = pretrained_net.fc.weight\nhotdog_w = torch.split(weight.data, 1, dim=0)[934]\nhotdog_w.shape\ntorch.Size([1, 512])\nDiscussions213.\n14.3 Object Detection and Bounding Boxes\nIn earlier sections (e.g., Section 8.1–Section 8.4), we introduced various models for image\nclassiﬁcation. In image classiﬁcation tasks, we assume that there is only one major object in the\nimage and we only focus on how to recognize its category. However, there are often multiple\nobjects in the image of interest. We not only want to know their categories, but also their\nspeciﬁc positions in the image. In computer vision, we refer to such tasks as object detection\n(or object recognition).\nObject detection has been widely applied in many ﬁelds. For example, self-driving needs to\nplan traveling routes by detecting the positions of vehicles, pedestrians, roads, and obstacles\nin the captured video images. Besides, robots may use this technique to detect and localize\nobjects of interest throughout its navigation of an environment. Moreover, security systems\nmay need to detect abnormal objects, such as intruders or bombs.\nIn the next few sections, we will introduce several deep learning methods for object detection.\nWe will begin with an introduction to positions (or locations) of objects.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\nWe will load the sample image to be used in this section. We can see that there is a dog\non the left side of the image and a cat on the right. They are the two major objects in this\nimage.\nd2l.set_figsize()\nimg = d2l.plt.imread('../img/catdog.jpg')\nd2l.plt.imshow(img);\n\n630\nComputer Vision\n14.3.1 Bounding Boxes\nIn object detection, we usually use a bounding box to describe the spatial location of an object.\nThe bounding box is rectangular, which is determined by the x and y coordinates of the\nupper-left corner of the rectangle and the such coordinates of the lower-right corner. Another\ncommonly used bounding box representation is the (x, y)-axis coordinates of the bounding\nbox center, and the width and height of the box.\nHere we deﬁne functions to convert between these two representations: box_corner_to_center\nconverts from the two-corner representation to the center-width-height presentation, and\nbox_center_to_corner vice versa. The input argument boxes should be a two-dimensional\ntensor of shape (n, 4), where n is the number of bounding boxes.\n#@save\ndef box_corner_to_center(boxes):\n\"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\nx1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\ncx = (x1 + x2) / 2\ncy = (y1 + y2) / 2\nw = x2 - x1\nh = y2 - y1\nboxes = torch.stack((cx, cy, w, h), axis=-1)\nreturn boxes\n#@save\ndef box_center_to_corner(boxes):\n\"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\ncx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\nx1 = cx - 0.5 * w\ny1 = cy - 0.5 * h\nx2 = cx + 0.5 * w\ny2 = cy + 0.5 * h\nboxes = torch.stack((x1, y1, x2, y2), axis=-1)\nreturn boxes\nWe will deﬁne the bounding boxes of the dog and the cat in the image based on the coordinate\ninformation. The origin of the coordinates in the image is the upper-left corner of the image,\nand to the right and down are the positive directions of the x and y axes, respectively.\n\n631\nObject Detection and Bounding Boxes\n# Here `bbox` is the abbreviation for bounding box\ndog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]\nWe can verify the correctness of the two bounding box conversion functions by converting\ntwice.\nboxes = torch.tensor((dog_bbox, cat_bbox))\nbox_center_to_corner(box_corner_to_center(boxes)) == boxes\ntensor([[True, True, True, True],\n[True, True, True, True]])\nLet’s draw the bounding boxes in the image to check if they are accurate. Before drawing, we\nwill deﬁne a helper function bbox_to_rect. It represents the bounding box in the bounding\nbox format of the matplotlib package.\n#@save\ndef bbox_to_rect(bbox, color):\n\"\"\"Convert bounding box to matplotlib format.\"\"\"\n# Convert the bounding box (upper-left x, upper-left y, lower-right x,\n# lower-right y) format to the matplotlib format: ((upper-left x,\n# upper-left y), width, height)\nreturn d2l.plt.Rectangle(\nxy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\nfill=False, edgecolor=color, linewidth=2)\nAfter adding the bounding boxes on the image, we can see that the main outline of the two\nobjects are basically inside the two boxes.\nfig = d2l.plt.imshow(img)\nfig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\nfig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));\n14.3.2 Summary\n\n632\nComputer Vision\n214\n• Object detection not only recognizes all the objects of interest in the image, but also their\npositions. The position is generally represented by a rectangular bounding box.\n• We can convert between two commonly used bounding box representations.\n14.3.3 Exercises\n1. Find another image and try to label a bounding box that contains the object. Compare\nlabeling bounding boxes and categories: which usually takes longer?\n2. Why is the innermost dimension of the input argument boxes of box_corner_to_center\nand box_center_to_corner always 4?\nDiscussions214.\n14.4 Anchor Boxes\nObject detection algorithms usually sample a large number of regions in the input image,\ndetermine whether these regions contain objects of interest, and adjust the boundaries of\nthe regions so as to predict the ground-truth bounding boxes of the objects more accurately.\nDiﬀerent models may adopt diﬀerent region sampling schemes. Here we introduce one of\nsuch methods: it generates multiple bounding boxes with varying scales and aspect ratios\ncentered on each pixel. These bounding boxes are called anchor boxes. We will design an\nobject detection model based on anchor boxes in Section 14.7.\nFirst, let’s modify the printing accuracy just for more concise outputs.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\ntorch.set_printoptions(2)\n# Simplify printing accuracy\n14.4.1 Generating Multiple Anchor Boxes\nSuppose that the input image has a height of h and width of w. We generate anchor boxes\nwith diﬀerent shapes centered on each pixel of the image. Let the scale be s ∈(0, 1] and the\naspect ratio (ratio of width to height) is r > 0. Then the width and height of the anchor box\nare ws√r and hs/√r, respectively. Note that when the center position is given, an anchor\nbox with known width and height is determined.\nTo generate multiple anchor boxes with diﬀerent shapes, let’s set a series of scales s1, . . ., sn\n\n633\nAnchor Boxes\nand a series of aspect ratios r1, . . ., rm. When using all the combinations of these scales and\naspect ratios with each pixel as the center, the input image will have a total of whnm anchor\nboxes. Although these anchor boxes may cover all the ground-truth bounding boxes, the com-\nputational complexity is easily too high. In practice, we can only consider those combinations\ncontaining s1 or r1:\n(s1, r1), (s1, r2), . . ., (s1, rm), (s2, r1), (s3, r1), . . ., (sn, r1).\n(14.4.1)\nThat is to say, the number of anchor boxes centered on the same pixel is n + m −1. For the\nentire input image, we will generate a total of wh(n + m −1) anchor boxes.\nThe above method of generating anchor boxes is implemented in the following multi-\nbox_prior function. We specify the input image, a list of scales, and a list of aspect ratios,\nthen this function will return all the anchor boxes.\n#@save\ndef multibox_prior(data, sizes, ratios):\n\"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\nin_height, in_width = data.shape[-2:]\ndevice, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\nboxes_per_pixel = (num_sizes + num_ratios - 1)\nsize_tensor = torch.tensor(sizes, device=device)\nratio_tensor = torch.tensor(ratios, device=device)\n# Offsets are required to move the anchor to the center of a pixel. Since\n# a pixel has height=1 and width=1, we choose to offset our centers by 0.5\noffset_h, offset_w = 0.5, 0.5\nsteps_h = 1.0 / in_height\n# Scaled steps in y axis\nsteps_w = 1.0 / in_width\n# Scaled steps in x axis\n# Generate all center points for the anchor boxes\ncenter_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\ncenter_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\nshift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\nshift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n# Generate `boxes_per_pixel` number of heights and widths that are later\n# used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\nw = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\nsizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n* in_height / in_width\n# Handle rectangular inputs\nh = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\nsizes[0] / torch.sqrt(ratio_tensor[1:])))\n# Divide by 2 to get half height and half width\nanchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\nin_height * in_width, 1) / 2\n# Each center point will have `boxes_per_pixel` number of anchor boxes, so\n# generate a grid of all anchor box centers with `boxes_per_pixel` repeats\nout_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\ndim=1).repeat_interleave(boxes_per_pixel, dim=0)\noutput = out_grid + anchor_manipulations\nreturn output.unsqueeze(0)\n\n634\nComputer Vision\nWe can see that the shape of the returned anchor box variable Y is (batch size, number of\nanchor boxes, 4).\nimg = d2l.plt.imread('../img/catdog.jpg')\nh, w = img.shape[:2]\nprint(h, w)\nX = torch.rand(size=(1, 3, h, w))\n# Construct input data\nY = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\nY.shape\n561 728\ntorch.Size([1, 2042040, 4])\nAfter changing the shape of the anchor box variable Y to (image height, image width, number\nof anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes centered\non a speciﬁed pixel position. In the following, we access the ﬁrst anchor box centered on\n(250, 250). It has four elements: the (x, y)-axis coordinates at the upper-left corner and the\n(x, y)-axis coordinates at the lower-right corner of the anchor box. The coordinate values of\nboth axes are divided by the width and height of the image, respectively.\nboxes = Y.reshape(h, w, 5, 4)\nboxes[250, 250, 0, :]\ntensor([0.06, 0.07, 0.63, 0.82])\nIn order to show all the anchor boxes centered on one pixel in the image, we deﬁne the\nfollowing show_bboxes function to draw multiple bounding boxes on the image.\n#@save\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n\"\"\"Show bounding boxes.\"\"\"\ndef make_list(obj, default_values=None):\nif obj is None:\nobj = default_values\nelif not isinstance(obj, (list, tuple)):\nobj = [obj]\nreturn obj\nlabels = make_list(labels)\ncolors = make_list(colors, ['b', 'g', 'r', 'm', 'c'])\nfor i, bbox in enumerate(bboxes):\ncolor = colors[i % len(colors)]\nrect = d2l.bbox_to_rect(bbox.detach().numpy(), color)\naxes.add_patch(rect)\nif labels and len(labels) > i:\n(continues on next page)\n\n635\nAnchor Boxes\n(continued from previous page)\ntext_color = 'k' if color == 'w' else 'w'\naxes.text(rect.xy[0], rect.xy[1], labels[i],\nva='center', ha='center', fontsize=9, color=text_color,\nbbox=dict(facecolor=color, lw=0))\nAs we just saw, the coordinate values of the x and y axes in the variable boxes have been\ndivided by the width and height of the image, respectively. When drawing anchor boxes, we\nneed to restore their original coordinate values; thus, we deﬁne variable bbox_scale below.\nNow, we can draw all the anchor boxes centered on (250, 250) in the image. As you can see,\nthe blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds the dog in\nthe image.\nd2l.set_figsize()\nbbox_scale = torch.tensor((w, h, w, h))\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',\n's=0.75, r=0.5'])\n14.4.2 Intersection over Union (IoU)\nWe just mentioned that an anchor box “well” surrounds the dog in the image. If the ground-\ntruth bounding box of the object is known, how can “well” here be quantiﬁed? Intuitively, we\ncan measure the similarity between the anchor box and the ground-truth bounding box. We\nknow that the Jaccard index can measure the similarity between two sets. Given sets A and B,\ntheir Jaccard index is the size of their intersection divided by the size of their union:\nJ(A, B) = |A ∩B|\n|A ∪B| .\n(14.4.2)\nIn fact, we can consider the pixel area of any bounding box as a set of pixels. In this way,\nwe can measure the similarity of the two bounding boxes by the Jaccard index of their pixel\nsets. For two bounding boxes, we usually refer their Jaccard index as intersection over union\n\n636\nComputer Vision\n(IoU), which is the ratio of their intersection area to their union area, as shown in Fig. 14.4.1.\nThe range of an IoU is between 0 and 1: 0 means that two bounding boxes do not overlap at\nall, while 1 indicates that the two bounding boxes are equal.\nt\nFig. 14.4.1\nIoU is the ratio of the intersection area to the union area of two bounding boxes.\nFor the remainder of this section, we will use IoU to measure the similarity between anchor\nboxes and ground-truth bounding boxes, and between diﬀerent anchor boxes. Given two lists\nof anchor or bounding boxes, the following box_iou computes their pairwise IoU across\nthese two lists.\n#@save\ndef box_iou(boxes1, boxes2):\n\"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\nbox_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n(boxes[:, 3] - boxes[:, 1]))\n# Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n# (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\nareas1 = box_area(boxes1)\nareas2 = box_area(boxes2)\n# Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n# boxes1, no. of boxes2, 2)\ninter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\ninter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\ninters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n# Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\ninter_areas = inters[:, :, 0] * inters[:, :, 1]\nunion_areas = areas1[:, None] + areas2 - inter_areas\nreturn inter_areas / union_areas\n14.4.3 Labeling Anchor Boxes in Training Data\nIn a training dataset, we consider each anchor box as a training example. In order to train\nan object detection model, we need class and oﬀset labels for each anchor box, where the\nformer is the class of the object relevant to the anchor box and the latter is the oﬀset of the\nground-truth bounding box relative to the anchor box. During the prediction, for each image\nwe generate multiple anchor boxes, predict classes and oﬀsets for all the anchor boxes, adjust\ntheir positions according to the predicted oﬀsets to obtain the predicted bounding boxes, and\nﬁnally only output those predicted bounding boxes that satisfy certain criteria.\nAs we know, an object detection training set comes with labels for locations of ground-truth\n\n637\nAnchor Boxes\nbounding boxes and classes of their surrounded objects. To label any generated anchor box,\nwe refer to the labeled location and class of its assigned ground-truth bounding box that is\nclosest to the anchor box. In the following, we describe an algorithm for assigning closest\nground-truth bounding boxes to anchor boxes.\nAssigning Ground-Truth Bounding Boxes to Anchor Boxes\nGiven an image, suppose that the anchor boxes are A1, A2, . . ., Ana and the ground-truth\nbounding boxes are B1, B2, . . ., Bnb, where na ≥nb. Let’s deﬁne a matrix X ∈Rna×nb,\nwhose element xij in the ith row and jth column is the IoU of the anchor box Ai and the\nground-truth bounding box Bj. The algorithm consists of the following steps:\n1. Find the largest element in matrix X and denote its row and column indices as i1 and\nj1, respectively. Then the ground-truth bounding box Bj1 is assigned to the anchor box\nAi1. This is quite intuitive because Ai1 and Bj1 are the closest among all the pairs of\nanchor boxes and ground-truth bounding boxes. After the ﬁrst assignment, discard all the\nelements in the i1th row and the j1th column in matrix X.\n2. Find the largest of the remaining elements in matrix X and denote its row and column\nindices as i2 and j2, respectively. We assign ground-truth bounding box Bj2 to anchor box\nAi2 and discard all the elements in the i2th row and the j2th column in matrix X.\n3. At this point, elements in two rows and two columns in matrix X have been discarded.\nWe proceed until all elements in nb columns in matrix X are discarded. At this time, we\nhave assigned a ground-truth bounding box to each of nb anchor boxes.\n4. Only traverse through the remaining na −nb anchor boxes. For example, given any anchor\nbox Ai, ﬁnd the ground-truth bounding box Bj with the largest IoU with Ai throughout\nthe ith row of matrix X, and assign Bj to Ai only if this IoU is greater than a predeﬁned\nthreshold.\nLet’s illustrate the above algorithm using a concrete example. As shown in Fig. 14.4.2 (left),\nassuming that the maximum value in matrix X is x23, we assign the ground-truth bounding\nbox B3 to the anchor box A2. Then, we discard all the elements in row 2 and column 3 of the\nmatrix, ﬁnd the largest x71 in the remaining elements (shaded area), and assign the ground-\ntruth bounding box B1 to the anchor box A7. Next, as shown in Fig. 14.4.2 (middle), discard\nall the elements in row 7 and column 1 of the matrix, ﬁnd the largest x54 in the remaining\nelements (shaded area), and assign the ground-truth bounding box B4 to the anchor box A5.\nFinally, as shown in Fig. 14.4.2 (right), discard all the elements in row 5 and column 4 of the\nmatrix, ﬁnd the largest x92 in the remaining elements (shaded area), and assign the ground-\ntruth bounding box B2 to the anchor box A9. After that, we only need to traverse through the\nremaining anchor boxes A1, A3, A4, A6, A8 and determine whether to assign them ground-\ntruth bounding boxes according to the threshold.\nThis algorithm is implemented in the following assign_anchor_to_bbox function.\n\n638\nComputer Vision\nt\nFig. 14.4.2\nAssigning ground-truth bounding boxes to anchor boxes.\n#@save\ndef assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n\"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\nnum_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n# Element x_ij in the i-th row and j-th column is the IoU of the anchor\n# box i and the ground-truth bounding box j\njaccard = box_iou(anchors, ground_truth)\n# Initialize the tensor to hold the assigned ground-truth bounding box for\n# each anchor\nanchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\ndevice=device)\n# Assign ground-truth bounding boxes according to the threshold\nmax_ious, indices = torch.max(jaccard, dim=1)\nanc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\nbox_j = indices[max_ious >= iou_threshold]\nanchors_bbox_map[anc_i] = box_j\ncol_discard = torch.full((num_anchors,), -1)\nrow_discard = torch.full((num_gt_boxes,), -1)\nfor _ in range(num_gt_boxes):\nmax_idx = torch.argmax(jaccard)\n# Find the largest IoU\nbox_idx = (max_idx % num_gt_boxes).long()\nanc_idx = (max_idx / num_gt_boxes).long()\nanchors_bbox_map[anc_idx] = box_idx\njaccard[:, box_idx] = col_discard\njaccard[anc_idx, :] = row_discard\nreturn anchors_bbox_map\nLabeling Classes and Oﬀsets\nNow we can label the class and oﬀset for each anchor box. Suppose that an anchor box A\nis assigned a ground-truth bounding box B. On the one hand, the class of the anchor box A\n\n639\nAnchor Boxes\nwill be labeled as that of B. On the other hand, the oﬀset of the anchor box A will be labeled\naccording to the relative position between the central coordinates of B and A together with the\nrelative size between these two boxes. Given varying positions and sizes of diﬀerent boxes\nin the dataset, we can apply transformations to those relative positions and sizes that may\nlead to more uniformly distributed oﬀsets that are easier to ﬁt. Here we describe a common\ntransformation. Given the central coordinates of A and B as (xa, ya) and (xb, yb), their widths\nas wa and wb, and their heights as ha and hb, respectively. We may label the oﬀset of A\nas\n( xb−xa\nwa\n−µx\nσx\n,\nyb−ya\nha\n−µy\nσy\n,\nlog wb\nwa −µw\nσw\n,\nlog hb\nha −µh\nσh\n)\n,\n(14.4.3)\nwhere default values of the constants are µx = µy = µw = µh = 0, σx = σy = 0.1,\nand σw = σh = 0.2. This transformation is implemented below in the offset_boxes\nfunction.\n#@save\ndef offset_boxes(anchors, assigned_bb, eps=1e-6):\n\"\"\"Transform for anchor box offsets.\"\"\"\nc_anc = d2l.box_corner_to_center(anchors)\nc_assigned_bb = d2l.box_corner_to_center(assigned_bb)\noffset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\noffset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\noffset = torch.cat([offset_xy, offset_wh], axis=1)\nreturn offset\nIf an anchor box is not assigned a ground-truth bounding box, we just label the class of the\nanchor box as “background”. Anchor boxes whose classes are background are often referred\nto as negative anchor boxes, and the rest are called positive anchor boxes. We implement\nthe following multibox_target function to label classes and oﬀsets for anchor boxes (the\nanchors argument) using ground-truth bounding boxes (the labels argument). This func-\ntion sets the background class to zero and increments the integer index of a new class by\none.\n#@save\ndef multibox_target(anchors, labels):\n\"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\nbatch_size, anchors = labels.shape[0], anchors.squeeze(0)\nbatch_offset, batch_mask, batch_class_labels = [], [], []\ndevice, num_anchors = anchors.device, anchors.shape[0]\nfor i in range(batch_size):\nlabel = labels[i, :, :]\nanchors_bbox_map = assign_anchor_to_bbox(\nlabel[:, 1:], anchors, device)\nbbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n1, 4)\n# Initialize class labels and assigned bounding box coordinates with\n# zeros\nclass_labels = torch.zeros(num_anchors, dtype=torch.long,\ndevice=device)\n(continues on next page)\n\n640\nComputer Vision\n(continued from previous page)\nassigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\ndevice=device)\n# Label classes of anchor boxes using their assigned ground-truth\n# bounding boxes. If an anchor box is not assigned any, we label its\n# class as background (the value remains zero)\nindices_true = torch.nonzero(anchors_bbox_map >= 0)\nbb_idx = anchors_bbox_map[indices_true]\nclass_labels[indices_true] = label[bb_idx, 0].long() + 1\nassigned_bb[indices_true] = label[bb_idx, 1:]\n# Offset transformation\noffset = offset_boxes(anchors, assigned_bb) * bbox_mask\nbatch_offset.append(offset.reshape(-1))\nbatch_mask.append(bbox_mask.reshape(-1))\nbatch_class_labels.append(class_labels)\nbbox_offset = torch.stack(batch_offset)\nbbox_mask = torch.stack(batch_mask)\nclass_labels = torch.stack(batch_class_labels)\nreturn (bbox_offset, bbox_mask, class_labels)\nAn Example\nLet’s illustrate anchor box labeling via a concrete example. We deﬁne ground-truth bounding\nboxes for the dog and cat in the loaded image, where the ﬁrst element is the class (0 for dog\nand 1 for cat) and the remaining four elements are the (x, y)-axis coordinates at the upper-left\ncorner and the lower-right corner (range is between 0 and 1). We also construct ﬁve anchor\nboxes to be labeled using the coordinates of the upper-left corner and the lower-right corner:\nA0, . . ., A4 (the index starts from 0). Then we plot these ground-truth bounding boxes and\nanchor boxes in the image.\nground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n[1, 0.55, 0.2, 0.9, 0.88]])\nanchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n[0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n[0.57, 0.3, 0.92, 0.9]])\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\nshow_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);\nUsing the multibox_target function deﬁned above, we can label classes and oﬀsets of these\nanchor boxes based on the ground-truth bounding boxes for the dog and cat. In this example,\nindices of the background, dog, and cat classes are 0, 1, and 2, respectively. Below we add\nan dimension for examples of anchor boxes and ground-truth bounding boxes.\nlabels = multibox_target(anchors.unsqueeze(dim=0),\nground_truth.unsqueeze(dim=0))\n\n641\nAnchor Boxes\nThere are three items in the returned result, all of which are in the tensor format. The third\nitem contains the labeled classes of the input anchor boxes.\nLet’s analyze the returned class labels below based on anchor box and ground-truth bound-\ning box positions in the image. First, among all the pairs of anchor boxes and ground-truth\nbounding boxes, the IoU of the anchor box A4 and the ground-truth bounding box of the cat\nis the largest. Thus, the class of A4 is labeled as the cat. Taking out pairs containing A4 or the\nground-truth bounding box of the cat, among the rest the pair of the anchor box A1 and the\nground-truth bounding box of the dog has the largest IoU. So the class of A1 is labeled as the\ndog. Next, we need to traverse through the remaining three unlabeled anchor boxes: A0, A2,\nand A3. For A0, the class of the ground-truth bounding box with the largest IoU is the dog,\nbut the IoU is below the predeﬁned threshold (0.5), so the class is labeled as background; for\nA2, the class of the ground-truth bounding box with the largest IoU is the cat and the IoU\nexceeds the threshold, so the class is labeled as the cat; for A3, the class of the ground-truth\nbounding box with the largest IoU is the cat, but the value is below the threshold, so the class\nis labeled as background.\nlabels[2]\ntensor([[0, 1, 2, 0, 2]])\nThe second returned item is a mask variable of the shape (batch size, four times the number\nof anchor boxes). Every four elements in the mask variable correspond to the four oﬀset\nvalues of each anchor box. Since we do not care about background detection, oﬀsets of this\nnegative class should not aﬀect the objective function. Through elementwise multiplications,\nzeros in the mask variable will ﬁlter out negative class oﬀsets before calculating the objective\nfunction.\nlabels[1]\ntensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.\n,→,\n1., 1.]])\n\n642\nComputer Vision\nThe ﬁrst returned item contains the four oﬀset values labeled for each anchor box. Note that\nthe oﬀsets of negative-class anchor boxes are labeled as zeros.\nlabels[0]\ntensor([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00,\n1.40e+00,\n1.00e+01,\n2.59e+00,\n7.18e+00, -1.20e+00,\n2.69e-01,\n1.68e+00, -1.57e+00,\n-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00,\n4.17e-06,\n6.26e-01]])\n14.4.4 Predicting Bounding Boxes with Non-Maximum Suppression\nDuring prediction, we generate multiple anchor boxes for the image and predict classes and\noﬀsets for each of them. A predicted bounding box is thus obtained according to an anchor\nbox with its predicted oﬀset. Below we implement the offset_inverse function that takes\nin anchors and oﬀset predictions as inputs and applies inverse oﬀset transformations to return\nthe predicted bounding box coordinates.\n#@save\ndef offset_inverse(anchors, offset_preds):\n\"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\nanc = d2l.box_corner_to_center(anchors)\npred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\npred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\npred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)\npredicted_bbox = d2l.box_center_to_corner(pred_bbox)\nreturn predicted_bbox\nWhen there are many anchor boxes, many similar (with signiﬁcant overlap) predicted bound-\ning boxes can be potentially output for surrounding the same object. To simplify the output,\nwe can merge similar predicted bounding boxes that belong to the same object by using\nnon-maximum suppression (NMS).\nHere is how non-maximum suppression works. For a predicted bounding box B, the object\ndetection model calculates the predicted likelihood for each class. Denoting by p the largest\npredicted likelihood, the class corresponding to this probability is the predicted class for B.\nSpeciﬁcally, we refer to p as the conﬁdence (score) of the predicted bounding box B. On the\nsame image, all the predicted non-background bounding boxes are sorted by conﬁdence in\ndescending order to generate a list L. Then we manipulate the sorted list L in the following\nsteps:\n1. Select the predicted bounding box B1 with the highest conﬁdence from L as a basis and\nremove all non-basis predicted bounding boxes whose IoU with B1 exceeds a predeﬁned\nthreshold ϵ from L. At this point, L keeps the predicted bounding box with the highest con-\nﬁdence but drops others that are too similar to it. In a nutshell, those with non-maximum\nconﬁdence scores are suppressed.\n\n643\nAnchor Boxes\n2. Select the predicted bounding box B2 with the second highest conﬁdence from L as an-\nother basis and remove all non-basis predicted bounding boxes whose IoU with B2 exceeds\nϵ from L.\n3. Repeat the above process until all the predicted bounding boxes in L have been used as\na basis. At this time, the IoU of any pair of predicted bounding boxes in L is below the\nthreshold ϵ; thus, no pair is too similar with each other.\n4. Output all the predicted bounding boxes in the list L.\nThe following nms function sorts conﬁdence scores in descending order and returns their\nindices.\n#@save\ndef nms(boxes, scores, iou_threshold):\n\"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\nB = torch.argsort(scores, dim=-1, descending=True)\nkeep = []\n# Indices of predicted bounding boxes that will be kept\nwhile B.numel() > 0:\ni = B[0]\nkeep.append(i)\nif B.numel() == 1: break\niou = box_iou(boxes[i, :].reshape(-1, 4),\nboxes[B[1:], :].reshape(-1, 4)).reshape(-1)\ninds = torch.nonzero(iou <= iou_threshold).reshape(-1)\nB = B[inds + 1]\nreturn torch.tensor(keep, device=boxes.device)\nWe deﬁne the following multibox_detection to apply non-maximum suppression to pre-\ndicting bounding boxes. Do not worry if you ﬁnd the implementation a bit complicated: we\nwill show how it works with a concrete example right after the implementation.\n#@save\ndef multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\npos_threshold=0.009999999):\n\"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\ndevice, batch_size = cls_probs.device, cls_probs.shape[0]\nanchors = anchors.squeeze(0)\nnum_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\nout = []\nfor i in range(batch_size):\ncls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\nconf, class_id = torch.max(cls_prob[1:], 0)\npredicted_bb = offset_inverse(anchors, offset_pred)\nkeep = nms(predicted_bb, conf, nms_threshold)\n# Find all non-`keep` indices and set the class to background\nall_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\ncombined = torch.cat((keep, all_idx))\nuniques, counts = combined.unique(return_counts=True)\nnon_keep = uniques[counts == 1]\nall_id_sorted = torch.cat((keep, non_keep))\nclass_id[non_keep] = -1\nclass_id = class_id[all_id_sorted]\n(continues on next page)\n\n644\nComputer Vision\n(continued from previous page)\nconf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n# Here `pos_threshold` is a threshold for positive (non-background)\n# predictions\nbelow_min_idx = (conf < pos_threshold)\nclass_id[below_min_idx] = -1\nconf[below_min_idx] = 1 - conf[below_min_idx]\npred_info = torch.cat((class_id.unsqueeze(1),\nconf.unsqueeze(1),\npredicted_bb), dim=1)\nout.append(pred_info)\nreturn torch.stack(out)\nNow let’s apply the above implementations to a concrete example with four anchor boxes. For\nsimplicity, we assume that the predicted oﬀsets are all zeros. This means that the predicted\nbounding boxes are anchor boxes. For each class among the background, dog, and cat, we\nalso deﬁne its predicted likelihood.\nanchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n[0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\noffset_preds = torch.tensor([0] * anchors.numel())\ncls_probs = torch.tensor([[0] * 4,\n# Predicted background likelihood\n[0.9, 0.8, 0.7, 0.1],\n# Predicted dog likelihood\n[0.1, 0.2, 0.3, 0.9]])\n# Predicted cat likelihood\nWe can plot these predicted bounding boxes with their conﬁdence on the image.\nfig = d2l.plt.imshow(img)\nshow_bboxes(fig.axes, anchors * bbox_scale,\n['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])\nNow we can invoke the multibox_detection function to perform non-maximum suppres-\nsion, where the threshold is set to 0.5. Note that we add a dimension for examples in the\ntensor input.\nWe can see that the shape of the returned result is (batch size, number of anchor boxes, 6). The\nsix elements in the innermost dimension gives the output information for the same predicted\n\n645\nAnchor Boxes\nbounding box. The ﬁrst element is the predicted class index, which starts from 0 (0 is dog\nand 1 is cat). The value -1 indicates background or removal in non-maximum suppression.\nThe second element is the conﬁdence of the predicted bounding box. The remaining four\nelements are the (x, y)-axis coordinates of the upper-left corner and the lower-right corner\nof the predicted bounding box, respectively (range is between 0 and 1).\noutput = multibox_detection(cls_probs.unsqueeze(dim=0),\noffset_preds.unsqueeze(dim=0),\nanchors.unsqueeze(dim=0),\nnms_threshold=0.5)\noutput\ntensor([[[ 0.00,\n0.90,\n0.10,\n0.08,\n0.52,\n0.92],\n[ 1.00,\n0.90,\n0.55,\n0.20,\n0.90,\n0.88],\n[-1.00,\n0.80,\n0.08,\n0.20,\n0.56,\n0.95],\n[-1.00,\n0.70,\n0.15,\n0.30,\n0.62,\n0.91]]])\nAfter removing those predicted bounding boxes of class -1, we can output the ﬁnal predicted\nbounding box kept by non-maximum suppression.\nfig = d2l.plt.imshow(img)\nfor i in output[0].detach().numpy():\nif i[0] == -1:\ncontinue\nlabel = ('dog=', 'cat=')[int(i[0])] + str(i[1])\nshow_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)\nIn practice, we can remove predicted bounding boxes with lower conﬁdence even before\nperforming non-maximum suppression, thereby reducing computation in this algorithm. We\nmay also post-process the output of non-maximum suppression, for example, by only keeping\nresults with higher conﬁdence in the ﬁnal output.\n14.4.5 Summary\n• We generate anchor boxes with diﬀerent shapes centered on each pixel of the image.\n\n646\nComputer Vision\n215\n• Intersection over union (IoU), also known as Jaccard index, measures the similarity of two\nbounding boxes. It is the ratio of their intersection area to their union area.\n• In a training set, we need two types of labels for each anchor box. One is the class of the\nobject relevant to the anchor box and the other is the oﬀset of the ground-truth bounding\nbox relative to the anchor box.\n• During prediction, we can use non-maximum suppression (NMS) to remove similar pre-\ndicted bounding boxes, thereby simplifying the output.\n14.4.6 Exercises\n1. Change values of sizes and ratios in the multibox_prior function. What are the\nchanges to the generated anchor boxes?\n2. Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap with\neach other?\n3. Modify the variable anchors in Section 14.4.3 and Section 14.4.4. How do the results\nchange?\n4. Non-maximum suppression is a greedy algorithm that suppresses predicted bounding\nboxes by removing them. Is it possible that some of these removed ones are actually use-\nful? How can this algorithm be modiﬁed to suppress softly? You may refer to Soft-NMS\n(Bodla et al., 2017).\n5. Rather than being hand-crafted, can non-maximum suppression be learned?\nDiscussions215.\n14.5 Multiscale Object Detection\nIn Section 14.4, we generated multiple anchor boxes centered on each pixel of an input image.\nEssentially these anchor boxes represent samples of diﬀerent regions of the image. However,\nwe may end up with too many anchor boxes to compute if they are generated for every pixel.\nThink of a 561×728 input image. If ﬁve anchor boxes with varying shapes are generated for\neach pixel as their center, over two million anchor boxes (561 × 728 × 5) need to be labeled\nand predicted on the image.\n14.5.1 Multiscale Anchor Boxes\nYou may realize that it is not diﬃcult to reduce anchor boxes on an image. For instance, we\ncan just uniformly sample a small portion of pixels from the input image to generate anchor\n\n647\nMultiscale Object Detection\nboxes centered on them. In addition, at diﬀerent scales we can generate diﬀerent numbers of\nanchor boxes of diﬀerent sizes. Intuitively, smaller objects are more likely to appear on an\nimage than larger ones. As an example, 1 × 1, 1 × 2, and 2 × 2 objects can appear on a 2 × 2\nimage in 4, 2, and 1 possible ways, respectively. Therefore, when using smaller anchor boxes\nto detect smaller objects, we can sample more regions, while for larger objects we can sample\nfewer regions.\nTo demonstrate how to generate anchor boxes at multiple scales, let’s read an image. Its height\nand width are 561 and 728 pixels, respectively.\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\nimg = d2l.plt.imread('../img/catdog.jpg')\nh, w = img.shape[:2]\nh, w\n(561, 728)\nRecall that in Section 7.2 we call a two-dimensional array output of a convolutional layer\na feature map. By deﬁning the feature map shape, we can determine centers of uniformly\nsampled anchor boxes on any image.\nThe display_anchors function is deﬁned below. We generate anchor boxes (anchors) on\nthe feature map (fmap) with each unit (pixel) as the anchor box center. Since the (x, y)-axis\ncoordinate values in the anchor boxes (anchors) have been divided by the width and height\nof the feature map (fmap), these values are between 0 and 1, which indicate the relative\npositions of anchor boxes in the feature map.\nSince centers of the anchor boxes (anchors) are spread over all units on the feature map\n(fmap), these centers must be uniformly distributed on any input image in terms of their rela-\ntive spatial positions. More concretely, given the width and height of the feature map fmap_w\nand fmap_h, respectively, the following function will uniformly sample pixels in fmap_h rows\nand fmap_w columns on any input image. Centered on these uniformly sampled pixels, anchor\nboxes of scale s (assuming the length of the list s is 1) and diﬀerent aspect ratios (ratios)\nwill be generated.\ndef display_anchors(fmap_w, fmap_h, s):\nd2l.set_figsize()\n# Values on the first two dimensions do not affect the output\nfmap = torch.zeros((1, 10, fmap_h, fmap_w))\nanchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\nbbox_scale = torch.tensor((w, h, w, h))\nd2l.show_bboxes(d2l.plt.imshow(img).axes,\nanchors[0] * bbox_scale)\nFirst, let’s consider detection of small objects. In order to make it easier to distinguish when\ndisplayed, the anchor boxes with diﬀerent centers here do not overlap: the anchor box scale is\n\n648\nComputer Vision\nset to 0.15 and the height and width of the feature map are set to 4. We can see that the centers\nof the anchor boxes in 4 rows and 4 columns on the image are uniformly distributed.\ndisplay_anchors(fmap_w=4, fmap_h=4, s=[0.15])\nWe move on to reduce the height and width of the feature map by half and use larger anchor\nboxes to detect larger objects. When the scale is set to 0.4, some anchor boxes will overlap\nwith each other.\ndisplay_anchors(fmap_w=2, fmap_h=2, s=[0.4])\nFinally, we further reduce the height and width of the feature map by half and increase the\nanchor box scale to 0.8. Now the center of the anchor box is the center of the image.\ndisplay_anchors(fmap_w=1, fmap_h=1, s=[0.8])\n14.5.2 Multiscale Detection\nSince we have generated multiscale anchor boxes, we will use them to detect objects of var-\nious sizes at diﬀerent scales. In the following we introduce a CNN-based multiscale object\ndetection method that we will implement in Section 14.7.\n\n649\nMultiscale Object Detection\nAt some scale, say that we have c feature maps of shape h × w. Using the method in Section\n14.5.1, we generate hw sets of anchor boxes, where each set has a anchor boxes with the\nsame center. For example, at the ﬁrst scale in the experiments in Section 14.5.1, given ten\n(number of channels) 4×4 feature maps, we generated 16 sets of anchor boxes, where each set\ncontains 3 anchor boxes with the same center. Next, each anchor box is labeled with the class\nand oﬀset based on ground-truth bounding boxes. At the current scale, the object detection\nmodel needs to predict the classes and oﬀsets of hw sets of anchor boxes on the input image,\nwhere diﬀerent sets have diﬀerent centers.\nAssume that the c feature maps here are the intermediate outputs obtained by the CNN\nforward propagation based on the input image. Since there are hw diﬀerent spatial positions\non each feature map, the same spatial position can be thought of as having c units. According\nto the deﬁnition of receptive ﬁeld in Section 7.2, these c units at the same spatial position of\nthe feature maps have the same receptive ﬁeld on the input image: they represent the input\nimage information in the same receptive ﬁeld. Therefore, we can transform the c units of the\nfeature maps at the same spatial position into the classes and oﬀsets of the a anchor boxes\ngenerated using this spatial position. In essence, we use the information of the input image\nin a certain receptive ﬁeld to predict the classes and oﬀsets of the anchor boxes that are close\nto that receptive ﬁeld on the input image.\nWhen the feature maps at diﬀerent layers have varying-size receptive ﬁelds on the input im-\nage, they can be used to detect objects of diﬀerent sizes. For example, we can design a neural\nnetwork where units of feature maps that are closer to the output layer have wider receptive\nﬁelds, so they can detect larger objects from the input image.\nIn a nutshell, we can leverage layerwise representations of images at multiple levels by deep\nneural networks for multiscale object detection. We will show how this works through a con-\ncrete example in Section 14.7.\n14.5.3 Summary\n• At multiple scales, we can generate anchor boxes with diﬀerent sizes to detect objects with\ndiﬀerent sizes.\n\n650\nComputer Vision\n216\n• By deﬁning the shape of feature maps, we can determine centers of uniformly sampled\nanchor boxes on any image.\n• We use the information of the input image in a certain receptive ﬁeld to predict the classes\nand oﬀsets of the anchor boxes that are close to that receptive ﬁeld on the input image.\n• Through deep learning, we can leverage its layerwise representations of images at multiple\nlevels for multiscale object detection.\n14.5.4 Exercises\n1. According to our discussions in Section 8.1, deep neural networks learn hierarchical fea-\ntures with increasing levels of abstraction for images. In multiscale object detection, do\nfeature maps at diﬀerent scales correspond to diﬀerent levels of abstraction? Why or why\nnot?\n2. At the ﬁrst scale (fmap_w=4, fmap_h=4) in the experiments in Section 14.5.1, generate\nuniformly distributed anchor boxes that may overlap.\n3. Given a feature map variable with shape 1 × c × h × w, where c, h, and w are the number\nof channels, height, and width of the feature maps, respectively. How can you transform\nthis variable into the classes and oﬀsets of anchor boxes? What is the shape of the output?\nDiscussions216.\n14.6 The Object Detection Dataset\nThere is no small dataset such as MNIST and Fashion-MNIST in the ﬁeld of object detection.\nIn order to quickly demonstrate object detection models, we collected and labeled a small\ndataset. First, we took photos of free bananas from our oﬃce and generated 1000 banana\nimages with diﬀerent rotations and sizes. Then we placed each banana image at a random\nposition on some background image. In the end, we labeled bounding boxes for those bananas\non the images.\n14.6.1 Downloading the Dataset\nThe banana detection dataset with all the image and csv label ﬁles can be downloaded directly\nfrom the Internet.\n\n651\nThe Object Detection Dataset\n%matplotlib inline\nimport os\nimport pandas as pd\nimport torch\nimport torchvision\nfrom d2l import torch as d2l\n#@save\nd2l.DATA_HUB['banana-detection'] = (\nd2l.DATA_URL + 'banana-detection.zip',\n'5de26c8fce5ccdea9f91267273464dc968d20d72')\n14.6.2 Reading the Dataset\nWe are going to read the banana detection dataset in the read_data_bananas function be-\nlow. The dataset includes a csv ﬁle for object class labels and ground-truth bounding box\ncoordinates at the upper-left and lower-right corners.\n#@save\ndef read_data_bananas(is_train=True):\n\"\"\"Read the banana detection dataset images and labels.\"\"\"\ndata_dir = d2l.download_extract('banana-detection')\ncsv_fname = os.path.join(data_dir, 'bananas_train' if is_train\nelse 'bananas_val', 'label.csv')\ncsv_data = pd.read_csv(csv_fname)\ncsv_data = csv_data.set_index('img_name')\nimages, targets = [], []\nfor img_name, target in csv_data.iterrows():\nimages.append(torchvision.io.read_image(\nos.path.join(data_dir, 'bananas_train' if is_train else\n'bananas_val', 'images', f'{img_name}')))\n# Here `target` contains (class, upper-left x, upper-left y,\n# lower-right x, lower-right y), where all the images have the same\n# banana class (index 0)\ntargets.append(list(target))\nreturn images, torch.tensor(targets).unsqueeze(1) / 256\nBy using the read_data_bananas function to read images and labels, the following Ba-\nnanasDataset class will allow us to create a customized Dataset instance for loading the\nbanana detection dataset.\n#@save\nclass BananasDataset(torch.utils.data.Dataset):\n\"\"\"A customized dataset to load the banana detection dataset.\"\"\"\ndef __init__(self, is_train):\nself.features, self.labels = read_data_bananas(is_train)\nprint('read ' + str(len(self.features)) + (f' training examples' if\nis_train else f' validation examples'))\n(continues on next page)\n\n652\nComputer Vision\n(continued from previous page)\ndef __getitem__(self, idx):\nreturn (self.features[idx].float(), self.labels[idx])\ndef __len__(self):\nreturn len(self.features)\nFinally, we deﬁne the load_data_bananas function to return two data iterator instances for\nboth the training and test sets. For the test dataset, there is no need to read it in random\norder.\n#@save\ndef load_data_bananas(batch_size):\n\"\"\"Load the banana detection dataset.\"\"\"\ntrain_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),\nbatch_size, shuffle=True)\nval_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),\nbatch_size)\nreturn train_iter, val_iter\nLet’s read a minibatch and print the shapes of both images and labels in this minibatch. The\nshape of the image minibatch, (batch size, number of channels, height, width), looks familiar:\nit is the same as in our earlier image classiﬁcation tasks. The shape of the label minibatch is\n(batch size, m, 5), where m is the largest possible number of bounding boxes that any image\nhas in the dataset.\nAlthough computation in minibatches is more eﬃcient, it requires that all the image exam-\nples contain the same number of bounding boxes to form a minibatch via concatenation. In\ngeneral, images may have a varying number of bounding boxes; thus, images with fewer than\nm bounding boxes will be padded with illegal bounding boxes until m is reached. Then the\nlabel of each bounding box is represented by an array of length 5. The ﬁrst element in the\narray is the class of the object in the bounding box, where -1 indicates an illegal bounding\nbox for padding. The remaining four elements of the array are the (x, y)-coordinate values\nof the upper-left corner and the lower-right corner of the bounding box (the range is between\n0 and 1). For the banana dataset, since there is only one bounding box on each image, we\nhave m = 1.\nbatch_size, edge_size = 32, 256\ntrain_iter, _ = load_data_bananas(batch_size)\nbatch = next(iter(train_iter))\nbatch[0].shape, batch[1].shape\nDownloading ../data/banana-detection.zip from http://d2l-data.s3-accelerate.\n,→amazonaws.com/banana-detection.zip...\nread 1000 training examples\nread 100 validation examples\n\n653\nThe Object Detection Dataset\n(torch.Size([32, 3, 256, 256]), torch.Size([32, 1, 5]))\n14.6.3 Demonstration\nLet’s demonstrate ten images with their labeled ground-truth bounding boxes. We can see\nthat the rotations, sizes, and positions of bananas vary across all these images. Of course,\nthis is just a simple artiﬁcial dataset. In practice, real-world datasets are usually much more\ncomplicated.\nimgs = (batch[0][:10].permute(0, 2, 3, 1)) / 255\naxes = d2l.show_images(imgs, 2, 5, scale=2)\nfor ax, label in zip(axes, batch[1][:10]):\nd2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])\n14.6.4 Summary\n• The banana detection dataset we collected can be used to demonstrate object detection\nmodels.\n• The data loading for object detection is similar to that for image classiﬁcation. However,\nin object detection the labels also contain information of ground-truth bounding boxes,\nwhich is missing in image classiﬁcation.\n14.6.5 Exercises\n1. Demonstrate other images with ground-truth bounding boxes in the banana detection\ndataset. How do they diﬀer with respect to bounding boxes and objects?\n2. Say that we want to apply data augmentation, such as random cropping, to object detection.\nHow can it be diﬀerent from that in image classiﬁcation? Hint: what if a cropped image\nonly contains a small portion of an object?\n\n654\nComputer Vision\n217\nDiscussions217.\n14.7 Single Shot Multibox Detection\nIn Section 14.3–Section 14.6, we introduced bounding boxes, anchor boxes, multiscale object\ndetection, and the dataset for object detection. Now we are ready to use such background\nknowledge to design an object detection model: single shot multibox detection (SSD) (Liu\net al., 2016). This model is simple, fast, and widely used. Although this is just one of vast\namounts of object detection models, some of the design principles and implementation details\nin this section are also applicable to other models.\n14.7.1 Model\nFig. 14.7.1 provides an overview of the design of single-shot multibox detection. This model\nmainly consists of a base network followed by several multiscale feature map blocks. The\nbase network is for extracting features from the input image, so it can use a deep CNN. For\nexample, the original single-shot multibox detection paper adopts a VGG network truncated\nbefore the classiﬁcation layer (Liu et al., 2016), while ResNet has also been commonly used.\nThrough our design we can make the base network output larger feature maps so as to generate\nmore anchor boxes for detecting smaller objects. Subsequently, each multiscale feature map\nblock reduces (e.g., by half) the height and width of the feature maps from the previous\nblock, and enables each unit of the feature maps to increase its receptive ﬁeld on the input\nimage.\nRecall the design of multiscale object detection through layerwise representations of images\nby deep neural networks in Section 14.5. Since multiscale feature maps closer to the top of\nFig. 14.7.1 are smaller but have larger receptive ﬁelds, they are suitable for detecting fewer\nbut larger objects.\nIn a nutshell, via its base network and several multiscale feature map blocks, single-shot\nmultibox detection generates a varying number of anchor boxes with diﬀerent sizes, and de-\ntects varying-size objects by predicting classes and oﬀsets of these anchor boxes (thus the\nbounding boxes); thus, this is a multiscale object detection model.\nIn the following, we will describe the implementation details of diﬀerent blocks in Fig. 14.7.1.\nTo begin with, we discuss how to implement the class and bounding box prediction.\nClass Prediction Layer\nLet the number of object classes be q. Then anchor boxes have q + 1 classes, where class\n0 is background. At some scale, suppose that the height and width of feature maps are h\n\n655\nSingle Shot Multibox Detection\nt\nFig. 14.7.1\nAs a multiscale object detection model, single-shot multibox detection mainly consists of\na base network followed by several multiscale feature map blocks.\nand w, respectively. When a anchor boxes are generated with each spatial position of these\nfeature maps as their center, a total of hwa anchor boxes need to be classiﬁed. This often\nmakes classiﬁcation with fully connected layers infeasible due to likely heavy parametrization\ncosts. Recall how we used channels of convolutional layers to predict classes in Section 8.3.\nSingle-shot multibox detection uses the same technique to reduce model complexity.\nSpeciﬁcally, the class prediction layer uses a convolutional layer without altering width or\nheight of feature maps. In this way, there can be a one-to-one correspondence between out-\nputs and inputs at the same spatial dimensions (width and height) of feature maps. More\nconcretely, channels of the output feature maps at any spatial position (x, y) represent class\npredictions for all the anchor boxes centered on (x, y) of the input feature maps. To produce\nvalid predictions, there must be a(q+1) output channels, where for the same spatial position\nthe output channel with index i(q+1)+ j represents the prediction of the class j (0 ≤j ≤q)\nfor the anchor box i (0 ≤i < a).\nBelow we deﬁne such a class prediction layer, specifying a and q via arguments num_anchors\nand num_classes, respectively. This layer uses a 3 × 3 convolutional layer with a padding\nof 1. The width and height of the input and output of this convolutional layer remain un-\nchanged.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\ndef cls_predictor(num_inputs, num_anchors, num_classes):\n(continues on next page)\n\n656\nComputer Vision\n(continued from previous page)\nreturn nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\nkernel_size=3, padding=1)\nBounding Box Prediction Layer\nThe design of the bounding box prediction layer is similar to that of the class prediction layer.\nThe only diﬀerence lies in the number of outputs for each anchor box: here we need to predict\nfour oﬀsets rather than q + 1 classes.\ndef bbox_predictor(num_inputs, num_anchors):\nreturn nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)\nConcatenating Predictions for Multiple Scales\nAs we mentioned, single-shot multibox detection uses multiscale feature maps to generate\nanchor boxes and predict their classes and oﬀsets. At diﬀerent scales, the shapes of feature\nmaps or the numbers of anchor boxes centered on the same unit may vary. Therefore, shapes\nof the prediction outputs at diﬀerent scales may vary.\nIn the following example, we construct feature maps at two diﬀerent scales, Y1 and Y2, for\nthe same minibatch, where the height and width of Y2 are half of those of Y1. Let’s take class\nprediction as an example. Suppose that 5 and 3 anchor boxes are generated for every unit in Y1\nand Y2, respectively. Suppose further that the number of object classes is 10. For feature maps\nY1 and Y2 the numbers of channels in the class prediction outputs are 5 × (10 + 1) = 55 and\n3×(10+1) = 33, respectively, where either output shape is (batch size, number of channels,\nheight, width).\ndef forward(x, block):\nreturn block(x)\nY1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\nY2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\nY1.shape, Y2.shape\n(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))\nAs we can see, except for the batch size dimension, the other three dimensions all have dif-\nferent sizes. To concatenate these two prediction outputs for more eﬃcient computation, we\nwill transform these tensors into a more consistent format.\nNote that the channel dimension holds the predictions for anchor boxes with the same center.\nWe ﬁrst move this dimension to the innermost. Since the batch size remains the same for\ndiﬀerent scales, we can transform the prediction output into a two-dimensional tensor with\n\n657\nSingle Shot Multibox Detection\nshape (batch size, height × width × number of channels). Then we can concatenate such\noutputs at diﬀerent scales along dimension 1.\ndef flatten_pred(pred):\nreturn torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\ndef concat_preds(preds):\nreturn torch.cat([flatten_pred(p) for p in preds], dim=1)\nIn this way, even though Y1 and Y2 have diﬀerent sizes in channels, heights, and widths,\nwe can still concatenate these two prediction outputs at two diﬀerent scales for the same\nminibatch.\nconcat_preds([Y1, Y2]).shape\ntorch.Size([2, 25300])\nDownsampling Block\nIn order to detect objects at multiple scales, we deﬁne the following downsampling block\ndown_sample_blk that halves the height and width of input feature maps. In fact, this block\napplies the design of VGG blocks in Section 8.2.1. More concretely, each downsampling\nblock consists of two 3 × 3 convolutional layers with padding of 1 followed by a 2 × 2 max-\npooling layer with stride of 2. As we know, 3 × 3 convolutional layers with padding of 1 do\nnot change the shape of feature maps. However, the subsequent 2 × 2 max-pooling reduces\nthe height and width of input feature maps by half. For both input and output feature maps of\nthis downsampling block, because 1×2+(3−1)+(3−1) = 6, each unit in the output has a\n6 × 6 receptive ﬁeld on the input. Therefore, the downsampling block enlarges the receptive\nﬁeld of each unit in its output feature maps.\ndef down_sample_blk(in_channels, out_channels):\nblk = []\nfor _ in range(2):\nblk.append(nn.Conv2d(in_channels, out_channels,\nkernel_size=3, padding=1))\nblk.append(nn.BatchNorm2d(out_channels))\nblk.append(nn.ReLU())\nin_channels = out_channels\nblk.append(nn.MaxPool2d(2))\nreturn nn.Sequential(*blk)\nIn the following example, our constructed downsampling block changes the number of input\nchannels and halves the height and width of the input feature maps.\nforward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape\n\n658\nComputer Vision\ntorch.Size([2, 10, 10, 10])\nBase Network Block\nThe base network block is used to extract features from input images. For simplicity, we con-\nstruct a small base network consisting of three downsampling blocks that double the number\nof channels at each block. Given a 256 × 256 input image, this base network block outputs\n32 × 32 feature maps (256/23 = 32).\ndef base_net():\nblk = []\nnum_filters = [3, 16, 32, 64]\nfor i in range(len(num_filters) - 1):\nblk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\nreturn nn.Sequential(*blk)\nforward(torch.zeros((2, 3, 256, 256)), base_net()).shape\ntorch.Size([2, 64, 32, 32])\nThe Complete Model\nThe complete single shot multibox detection model consists of ﬁve blocks. The feature maps\nproduced by each block are used for both (i) generating anchor boxes and (ii) predicting\nclasses and oﬀsets of these anchor boxes. Among these ﬁve blocks, the ﬁrst one is the base\nnetwork block, the second to the fourth are downsampling blocks, and the last block uses\nglobal max-pooling to reduce both the height and width to 1. Technically, the second to the\nﬁfth blocks are all those multiscale feature map blocks in Fig. 14.7.1.\ndef get_blk(i):\nif i == 0:\nblk = base_net()\nelif i == 1:\nblk = down_sample_blk(64, 128)\nelif i == 4:\nblk = nn.AdaptiveMaxPool2d((1,1))\nelse:\nblk = down_sample_blk(128, 128)\nreturn blk\nNow we deﬁne the forward propagation for each block. Diﬀerent from in image classiﬁca-\ntion tasks, outputs here include (i) CNN feature maps Y, (ii) anchor boxes generated using\nY at the current scale, and (iii) classes and oﬀsets predicted (based on Y) for these anchor\nboxes.\n\n659\nSingle Shot Multibox Detection\ndef blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\nY = blk(X)\nanchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\ncls_preds = cls_predictor(Y)\nbbox_preds = bbox_predictor(Y)\nreturn (Y, anchors, cls_preds, bbox_preds)\nRecall that in Fig. 14.7.1 a multiscale feature map block that is closer to the top is for de-\ntecting larger objects; thus, it needs to generate larger anchor boxes. In the above forward\npropagation, at each multiscale feature map block we pass in a list of two scale values via the\nsizes argument of the invoked multibox_prior function (described in Section 14.4). In\nthe following, the interval between 0.2 and 1.05 is split evenly into ﬁve sections to determine\nthe smaller scale values at the ﬁve blocks: 0.2, 0.37, 0.54, 0.71, and 0.88. Then their larger\nscale values are given by\n√\n0.2 × 0.37 = 0.272,\n√\n0.37 × 0.54 = 0.447, and so on.\nsizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n[0.88, 0.961]]\nratios = [[1, 2, 0.5]] * 5\nnum_anchors = len(sizes[0]) + len(ratios[0]) - 1\nNow we can deﬁne the complete model TinySSD as follows.\nclass TinySSD(nn.Module):\ndef __init__(self, num_classes, **kwargs):\nsuper(TinySSD, self).__init__(**kwargs)\nself.num_classes = num_classes\nidx_to_in_channels = [64, 128, 128, 128, 128]\nfor i in range(5):\n# Equivalent to the assignment statement `self.blk_i = get_blk(i)`\nsetattr(self, f'blk_{i}', get_blk(i))\nsetattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\nnum_anchors, num_classes))\nsetattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\nnum_anchors))\ndef forward(self, X):\nanchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\nfor i in range(5):\n# Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\nX, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\nX, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\ngetattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\nanchors = torch.cat(anchors, dim=1)\ncls_preds = concat_preds(cls_preds)\ncls_preds = cls_preds.reshape(\ncls_preds.shape[0], -1, self.num_classes + 1)\nbbox_preds = concat_preds(bbox_preds)\nreturn anchors, cls_preds, bbox_preds\nWe create a model instance and use it to perform forward propagation on a minibatch of\n256 × 256 images X.\n\n660\nComputer Vision\nAs shown earlier in this section, the ﬁrst block outputs 32 × 32 feature maps. Recall that the\nsecond to fourth downsampling blocks halve the height and width and the ﬁfth block uses\nglobal pooling. Since 4 anchor boxes are generated for each unit along spatial dimensions of\nfeature maps, at all the ﬁve scales a total of (322 + 162 + 82 + 42 + 1) × 4 = 5444 anchor\nboxes are generated for each image.\nnet = TinySSD(num_classes=1)\nX = torch.zeros((32, 3, 256, 256))\nanchors, cls_preds, bbox_preds = net(X)\nprint('output anchors:', anchors.shape)\nprint('output class preds:', cls_preds.shape)\nprint('output bbox preds:', bbox_preds.shape)\noutput anchors: torch.Size([1, 5444, 4])\noutput class preds: torch.Size([32, 5444, 2])\noutput bbox preds: torch.Size([32, 21776])\n14.7.2 Training\nNow we will explain how to train the single shot multibox detection model for object detec-\ntion.\nReading the Dataset and Initializing the Model\nTo begin with, let’s read the banana detection dataset described in Section 14.6.\nbatch_size = 32\ntrain_iter, _ = d2l.load_data_bananas(batch_size)\nread 1000 training examples\nread 100 validation examples\nThere is only one class in the banana detection dataset. After deﬁning the model, we need to\ninitialize its parameters and deﬁne the optimization algorithm.\ndevice, net = d2l.try_gpu(), TinySSD(num_classes=1)\ntrainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)\nDeﬁning Loss and Evaluation Functions\nObject detection has two types of losses. The ﬁrst loss concerns classes of anchor boxes: its\ncomputation can simply reuse the cross-entropy loss function that we used for image classi-\n\n661\nSingle Shot Multibox Detection\nﬁcation. The second loss concerns oﬀsets of positive (non-background) anchor boxes: this is\na regression problem. For this regression problem, however, here we do not use the squared\nloss described in Section 3.1.3. Instead, we use the ℓ1 norm loss, the absolute value of the\ndiﬀerence between the prediction and the ground-truth. The mask variable bbox_masks ﬁl-\nters out negative anchor boxes and illegal (padded) anchor boxes in the loss calculation. In\nthe end, we sum up the anchor box class loss and the anchor box oﬀset loss to obtain the loss\nfunction for the model.\ncls_loss = nn.CrossEntropyLoss(reduction='none')\nbbox_loss = nn.L1Loss(reduction='none')\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\nbatch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\ncls = cls_loss(cls_preds.reshape(-1, num_classes),\ncls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\nbbox = bbox_loss(bbox_preds * bbox_masks,\nbbox_labels * bbox_masks).mean(dim=1)\nreturn cls + bbox\nWe can use accuracy to evaluate the classiﬁcation results. Due to the used ℓ1 norm loss for\nthe oﬀsets, we use the mean absolute error to evaluate the predicted bounding boxes. These\nprediction results are obtained from the generated anchor boxes and the predicted oﬀsets for\nthem.\ndef cls_eval(cls_preds, cls_labels):\n# Because the class prediction results are on the final dimension,\n# `argmax` needs to specify this dimension\nreturn float((cls_preds.argmax(dim=-1).type(\ncls_labels.dtype) == cls_labels).sum())\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\nreturn float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())\nTraining the Model\nWhen training the model, we need to generate multiscale anchor boxes (anchors) and predict\ntheir classes (cls_preds) and oﬀsets (bbox_preds) in the forward propagation. Then we\nlabel the classes (cls_labels) and oﬀsets (bbox_labels) of such generated anchor boxes\nbased on the label information Y. Finally, we calculate the loss function using the predicted\nand labeled values of the classes and oﬀsets. For concise implementations, evaluation of the\ntest dataset is omitted here.\nnum_epochs, timer = 20, d2l.Timer()\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\nlegend=['class error', 'bbox mae'])\nnet = net.to(device)\nfor epoch in range(num_epochs):\n(continues on next page)\n\n662\nComputer Vision\n(continued from previous page)\n# Sum of training accuracy, no. of examples in sum of training accuracy,\n# Sum of absolute error, no. of examples in sum of absolute error\nmetric = d2l.Accumulator(4)\nnet.train()\nfor features, target in train_iter:\ntimer.start()\ntrainer.zero_grad()\nX, Y = features.to(device), target.to(device)\n# Generate multiscale anchor boxes and predict their classes and\n# offsets\nanchors, cls_preds, bbox_preds = net(X)\n# Label the classes and offsets of these anchor boxes\nbbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n# Calculate the loss function using the predicted and labeled values\n# of the classes and offsets\nl = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\nbbox_masks)\nl.mean().backward()\ntrainer.step()\nmetric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\nbbox_eval(bbox_preds, bbox_labels, bbox_masks),\nbbox_labels.numel())\ncls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\nanimator.add(epoch + 1, (cls_err, bbox_mae))\nprint(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\nprint(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\nf'{str(device)}')\nclass err 3.31e-03, bbox mae 3.11e-03\n3535.8 examples/sec on cuda:0\n14.7.3 Prediction\nDuring prediction, the goal is to detect all the objects of interest on the image. Below we\nread and resize a test image, converting it to a four-dimensional tensor that is required by\nconvolutional layers.\n\n663\nSingle Shot Multibox Detection\nX = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()\nimg = X.squeeze(0).permute(1, 2, 0).long()\nUsing the multibox_detection function below, the predicted bounding boxes are obtained\nfrom the anchor boxes and their predicted oﬀsets. Then non-maximum suppression is used\nto remove similar predicted bounding boxes.\ndef predict(X):\nnet.eval()\nanchors, cls_preds, bbox_preds = net(X.to(device))\ncls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\noutput = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\nidx = [i for i, row in enumerate(output[0]) if row[0] != -1]\nreturn output[0, idx]\noutput = predict(X)\nFinally, we display all the predicted bounding boxes with conﬁdence 0.9 or above as out-\nput.\ndef display(img, output, threshold):\nd2l.set_figsize((5, 5))\nfig = d2l.plt.imshow(img)\nfor row in output:\nscore = float(row[1])\nif score < threshold:\ncontinue\nh, w = img.shape[:2]\nbbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\nd2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\ndisplay(img, output.cpu(), threshold=0.9)\n14.7.4 Summary\n• Single shot multibox detection is a multiscale object detection model. Via its base network\nand several multiscale feature map blocks, single-shot multibox detection generates a\nvarying number of anchor boxes with diﬀerent sizes, and detects varying-size objects by\npredicting classes and oﬀsets of these anchor boxes (thus the bounding boxes).\n• When training the single-shot multibox detection model, the loss function is calculated\nbased on the predicted and labeled values of the anchor box classes and oﬀsets.\n14.7.5 Exercises\n1. Can you improve the single-shot multibox detection by improving the loss function? For\nexample, replace ℓ1 norm loss with smooth ℓ1 norm loss for the predicted oﬀsets. This\n\n664\nComputer Vision\nloss function uses a square function around zero for smoothness, which is controlled by\nthe hyperparameter σ:\nf (x) =\n{\n(σx)2/2,\nif |x| < 1/σ2\n|x| −0.5/σ2,\notherwise\n(14.7.1)\nWhen σ is very large, this loss is similar to the ℓ1 norm loss. When its value is smaller, the\nloss function is smoother.\ndef smooth_l1(data, scalar):\nout = []\nfor i in data:\nif abs(i) < 1 / (scalar ** 2):\nout.append(((scalar * i) ** 2) / 2)\nelse:\nout.append(abs(i) - 0.5 / (scalar ** 2))\nreturn torch.tensor(out)\nsigmas = [10, 1, 0.5]\nlines = ['-', '--', '-.']\nx = torch.arange(-2, 2, 0.1)\nd2l.set_figsize()\nfor l, s in zip(lines, sigmas):\ny = smooth_l1(x, scalar=s)\nd2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\nd2l.plt.legend();\nBesides, in the experiment we used cross-entropy loss for class prediction: denoting by pj the\npredicted probability for the ground-truth class j, the cross-entropy loss is −log pj. We can\n\n665\nSingle Shot Multibox Detection\nalso use the focal loss (Lin et al., 2017): given hyperparameters γ > 0 and α > 0, this loss is\ndeﬁned as:\n−α(1 −pj)γ log pj.\n(14.7.2)\nAs we can see, increasing γ can eﬀectively reduce the relative loss for well-classiﬁed ex-\namples (e.g., pj > 0.5) so the training can focus more on those diﬃcult examples that are\nmisclassiﬁed.\ndef focal_loss(gamma, x):\nreturn -(1 - x) ** gamma * torch.log(x)\nx = torch.arange(0.01, 1, 0.01)\nfor l, gamma in zip(lines, [0, 1, 5]):\ny = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)\nd2l.plt.legend();\n2. Due to space limitations, we have omitted some implementation details of the single shot\nmultibox detection model in this section. Can you further improve the model in the fol-\nlowing aspects:\n1. When an object is much smaller compared with the image, the model could resize the\ninput image bigger.\n2. There are typically a vast number of negative anchor boxes. To make the class distri-\nbution more balanced, we could downsample negative anchor boxes.\n\n666\nComputer Vision\n218\n3. In the loss function, assign diﬀerent weight hyperparameters to the class loss and the\noﬀset loss.\n4. Use other methods to evaluate the object detection model, such as those in the single\nshot multibox detection paper (Liu et al., 2016).\nDiscussions218.\n14.8 Region-based CNNs (R-CNNs)\nBesides single shot multibox detection described in Section 14.7, region-based CNNs or re-\ngions with CNN features (R-CNNs) are also among many pioneering approaches of applying\ndeep learning to object detection (Girshick et al., 2014). In this section, we will introduce the\nR-CNN and its series of improvements: the fast R-CNN (Girshick, 2015), the faster R-CNN\n(Ren et al., 2015), and the mask R-CNN (He et al., 2017). Due to limited space, we will only\nfocus on the design of these models.\n14.8.1 R-CNNs\nThe R-CNN ﬁrst extracts many (e.g., 2000) region proposals from the input image (e.g., an-\nchor boxes can also be considered as region proposals), labeling their classes and bounding\nboxes (e.g., oﬀsets).\n(Girshick et al., 2014)\nThen a CNN is used to perform forward propagation on each region proposal to extract its\nfeatures. Next, features of each region proposal are used for predicting the class and bounding\nbox of this region proposal.\nt\nFig. 14.8.1\nThe R-CNN model.\nFig. 14.8.1 shows the R-CNN model. More concretely, the R-CNN consists of the following\nfour steps:\n1. Perform selective search to extract multiple high-quality region proposals on the input\nimage (Uijlings et al., 2013). These proposed regions are usually selected at multiple scales\n\n667\nRegion-based CNNs (R-CNNs)\nwith diﬀerent shapes and sizes. Each region proposal will be labeled with a class and a\nground-truth bounding box.\n2. Choose a pretrained CNN and truncate it before the output layer. Resize each region\nproposal to the input size required by the network, and output the extracted features for\nthe region proposal through forward propagation.\n3. Take the extracted features and labeled class of each region proposal as an example. Train\nmultiple support vector machines to classify objects, where each support vector machine\nindividually determines whether the example contains a speciﬁc class.\n4. Take the extracted features and labeled bounding box of each region proposal as an ex-\nample. Train a linear regression model to predict the ground-truth bounding box.\nAlthough the R-CNN model uses pretrained CNNs to eﬀectively extract image features, it\nis slow. Imagine that we select thousands of region proposals from a single input image: this\nrequires thousands of CNN forward propagations to perform object detection. This massive\ncomputing load makes it infeasible to widely use R-CNNs in real-world applications.\n14.8.2 Fast R-CNN\nThe main performance bottleneck of an R-CNN lies in the independent CNN forward prop-\nagation for each region proposal, without sharing computation. Since these regions usually\nhave overlaps, independent feature extractions lead to much repeated computation. One of\nthe major improvements of the fast R-CNN from the R-CNN is that the CNN forward prop-\nagation is only performed on the entire image (Girshick, 2015).\nt\nFig. 14.8.2\nThe fast R-CNN model.\nFig. 14.8.2 describes the fast R-CNN model. Its major computations are as follows:\n1. Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extrac-\ntion is the entire image, rather than individual region proposals. Moreover, this CNN is\ntrainable. Given an input image, let the shape of the CNN output be 1 × c × h1 × w1.\n\n668\nComputer Vision\n2. Suppose that selective search generates n region proposals. These region proposals (of\ndiﬀerent shapes) mark regions of interest (of diﬀerent shapes) on the CNN output. Then\nthese regions of interest further extract features of the same shape (say height h2 and\nwidth w2 are speciﬁed) in order to be easily concatenated. To achieve this, the fast R-CNN\nintroduces the region of interest (RoI) pooling layer: the CNN output and region proposals\nare input into this layer, outputting concatenated features of shape n × c × h2 × w2 that\nare further extracted for all the region proposals.\n3. Using a fully connected layer, transform the concatenated features into an output of shape\nn × d, where d depends on the model design.\n4. Predict the class and bounding box for each of the n region proposals. More concretely,\nin class and bounding box prediction, transform the fully connected layer output into an\noutput of shape n×q (q is the number of classes) and an output of shape n×4, respectively.\nThe class prediction uses softmax regression.\nThe region of interest pooling layer proposed in the fast R-CNN is diﬀerent from the pooling\nlayer introduced in Section 7.5. In the pooling layer, we indirectly control the output shape\nby specifying sizes of the pooling window, padding, and stride. In contrast, we can directly\nspecify the output shape in the region of interest pooling layer.\nFor example, let’s specify the output height and width for each region as h2 and w2, respec-\ntively. For any region of interest window of shape h×w, this window is divided into a h2×w2\ngrid of subwindows, where the shape of each subwindow is approximately (h/h2)×(w/w2).\nIn practice, the height and width of any subwindow shall be rounded up, and the largest el-\nement shall be used as output of the subwindow. Therefore, the region of interest pooling\nlayer can extract features of the same shape even when regions of interest have diﬀerent\nshapes.\nAs an illustrative example, in Fig. 14.8.3, the upper-left 3 × 3 region of interest is selected\non a 4 × 4 input. For this region of interest, we use a 2 × 2 region of interest pooling layer to\nobtain a 2 × 2 output. Note that each of the four divided subwindows contains elements 0, 1,\n4, and 5 (5 is the maximum); 2 and 6 (6 is the maximum); 8 and 9 (9 is the maximum); and\n10.\nt\nFig. 14.8.3\nA 2 × 2 region of interest pooling layer.\nBelow we demonstrate the computation of the region of interest pooling layer. Suppose that\nthe height and width of the CNN-extracted features X are both 4, and there is only a single\nchannel.\n\n669\nRegion-based CNNs (R-CNNs)\nimport torch\nimport torchvision\nX = torch.arange(16.).reshape(1, 1, 4, 4)\nX\ntensor([[[[ 0.,\n1.,\n2.,\n3.],\n[ 4.,\n5.,\n6.,\n7.],\n[ 8.,\n9., 10., 11.],\n[12., 13., 14., 15.]]]])\nLet’s further suppose that the height and width of the input image are both 40 pixels and\nthat selective search generates two region proposals on this image. Each region proposal is\nexpressed as ﬁve elements: its object class followed by the (x, y)-coordinates of its upper-left\nand lower-right corners.\nrois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])\nBecause the height and width of X are 1/10 of the height and width of the input image,\nthe coordinates of the two region proposals are multiplied by 0.1 according to the speciﬁed\nspatial_scale argument. Then the two regions of interest are marked on X as X[:, :,\n0:3, 0:3] and X[:, :, 1:4, 0:4], respectively. Finally in the 2 × 2 region of interest\npooling, each region of interest is divided into a grid of sub-windows to further extract features\nof the same shape 2 × 2.\ntorchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)\ntensor([[[[ 5.,\n6.],\n[ 9., 10.]]],\n[[[ 9., 11.],\n[13., 15.]]]])\n14.8.3 Faster R-CNN\nTo be more accurate in object detection, the fast R-CNN model usually has to generate a lot\nof region proposals in selective search. To reduce region proposals without loss of accuracy,\nthe faster R-CNN proposes to replace selective search with a region proposal network (Ren et\nal., 2015).\nFig. 14.8.4 shows the faster R-CNN model. Compared with the fast R-CNN, the faster R-\nCNN only changes the region proposal method from selective search to a region proposal\nnetwork. The rest of the model remain unchanged. The region proposal network works in the\nfollowing steps:\n\n670\nComputer Vision\nt\nFig. 14.8.4\nThe faster R-CNN model.\n1. Use a 3 × 3 convolutional layer with padding of 1 to transform the CNN output to a new\noutput with c channels. In this way, each unit along the spatial dimensions of the CNN-\nextracted feature maps gets a new feature vector of length c.\n2. Centered on each pixel of the feature maps, generate multiple anchor boxes of diﬀerent\nscales and aspect ratios and label them.\n3. Using the length-c feature vector at the center of each anchor box, predict the binary class\n(background or objects) and bounding box for this anchor box.\n4. Consider those predicted bounding boxes whose predicted classes are objects. Remove\noverlapped results using non-maximum suppression. The remaining predicted bounding\nboxes for objects are the region proposals required by the region of interest pooling layer.\nIt is worth noting that, as part of the faster R-CNN model, the region proposal network is\njointly trained with the rest of the model. In other words, the objective function of the faster R-\nCNN includes not only the class and bounding box prediction in object detection, but also the\nbinary class and bounding box prediction of anchor boxes in the region proposal network. As\na result of the end-to-end training, the region proposal network learns how to generate high-\nquality region proposals, so as to stay accurate in object detection with a reduced number of\nregion proposals that are learned from data.\n14.8.4 Mask R-CNN\nIn the training dataset, if pixel-level positions of object are also labeled on images, the mask\nR-CNN can eﬀectively leverage such detailed labels to further improve the accuracy of object\ndetection (He et al., 2017).\nAs shown in Fig. 14.8.5, the mask R-CNN is modiﬁed based on the faster R-CNN. Speciﬁ-\ncally, the mask R-CNN replaces the region of interest pooling layer with the region of interest\n(RoI) alignment layer. This region of interest alignment layer uses bilinear interpolation to\n\n671\nRegion-based CNNs (R-CNNs)\nt\nFig. 14.8.5\nThe mask R-CNN model.\npreserve the spatial information on the feature maps, which is more suitable for pixel-level\nprediction. The output of this layer contains feature maps of the same shape for all the re-\ngions of interest. They are used to predict not only the class and bounding box for each region\nof interest, but also the pixel-level position of the object through an additional fully convo-\nlutional network. More details on using a fully convolutional network to predict pixel-level\nsemantics of an image will be provided in subsequent sections of this chapter.\n14.8.5 Summary\n• The R-CNN extracts many region proposals from the input image, uses a CNN to perform\nforward propagation on each region proposal to extract its features, then uses these fea-\ntures to predict the class and bounding box of this region proposal.\n• One of the major improvements of the fast R-CNN from the R-CNN is that the CNN\nforward propagation is only performed on the entire image. It also introduces the region\nof interest pooling layer, so that features of the same shape can be further extracted for\nregions of interest that have diﬀerent shapes.\n• The faster R-CNN replaces the selective search used in the fast R-CNN with a jointly\ntrained region proposal network, so that the former can stay accurate in object detection\nwith a reduced number of region proposals.\n• Based on the faster R-CNN, the mask R-CNN additionally introduces a fully convolutional\nnetwork, so as to leverage pixel-level labels to further improve the accuracy of object\ndetection.\n14.8.6 Exercises\n\n672\nComputer Vision\n219\n1. Can we frame object detection as a single regression problem, such as predicting bounding\nboxes and class probabilities? You may refer to the design of the YOLO model (Redmon\net al., 2016).\n2. Compare single shot multibox detection with the methods introduced in this section. What\nare their major diﬀerences? You may refer to Figure 2 of Zhao et al. (2019).\nDiscussions219.\n14.9 Semantic Segmentation and the Dataset\nWhen discussing object detection tasks in Section 14.3–Section 14.8, rectangular bounding\nboxes are used to label and predict objects in images. This section will discuss the problem\nof semantic segmentation, which focuses on how to divide an image into regions belonging to\ndiﬀerent semantic classes. Diﬀerent from object detection, semantic segmentation recognizes\nand understands what are in images in pixel level: its labeling and prediction of semantic\nregions are in pixel level. Fig. 14.9.1 shows the labels of the dog, cat, and background of the\nimage in semantic segmentation. Compared with in object detection, the pixel-level borders\nlabeled in semantic segmentation are obviously more ﬁne-grained.\nt\nFig. 14.9.1\nLabels of the dog, cat, and background of the image in semantic segmentation.\n14.9.1 Image Segmentation and Instance Segmentation\nThere are also two important tasks in the ﬁeld of computer vision that are similar to semantic\nsegmentation, namely image segmentation and instance segmentation. We will brieﬂy distin-\nguish them from semantic segmentation as follows.\n• Image segmentation divides an image into several constituent regions. The methods for\nthis type of problem usually make use of the correlation between pixels in the image.\nIt does not need label information about image pixels during training, and it cannot\nguarantee that the segmented regions will have the semantics that we hope to obtain\nduring prediction. Taking the image in Fig. 14.9.1 as input, image segmentation may\n\n673\nSemantic Segmentation and the Dataset\n220\ndivide the dog into two regions: one covers the mouth and eyes which are mainly black,\nand the other covers the rest of the body which is mainly yellow.\n• Instance segmentation is also called simultaneous detection and segmentation. It studies how\nto recognize the pixel-level regions of each object instance in an image. Diﬀerent from\nsemantic segmentation, instance segmentation needs to distinguish not only semantics,\nbut also diﬀerent object instances. For example, if there are two dogs in the image,\ninstance segmentation needs to distinguish which of the two dogs a pixel belongs to.\n14.9.2 The Pascal VOC2012 Semantic Segmentation Dataset\nOn of the most important semantic segmentation dataset is Pascal VOC2012 220 . In the\nfollowing, we will take a look at this dataset.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom d2l import torch as d2l\nThe tar ﬁle of the dataset is about 2 GB, so it may take a while to download the ﬁle. The\nextracted dataset is located at ../data/VOCdevkit/VOC2012.\n#@save\nd2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n'4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\nvoc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\nAfter entering the path ../data/VOCdevkit/VOC2012, we can see the diﬀerent components\nof the dataset. The ImageSets/Segmentation path contains text ﬁles that specify training\nand test samples, while the JPEGImages and SegmentationClass paths store the input im-\nage and label for each example, respectively. The label here is also in the image format, with\nthe same size as its labeled input image. Besides, pixels with the same color in any label image\nbelong to the same semantic class. The following deﬁnes the read_voc_images function to\nread all the input images and labels into the memory.\n#@save\ndef read_voc_images(voc_dir, is_train=True):\n\"\"\"Read all VOC feature and label images.\"\"\"\ntxt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n'train.txt' if is_train else 'val.txt')\nmode = torchvision.io.image.ImageReadMode.RGB\nwith open(txt_fname, 'r') as f:\nimages = f.read().split()\nfeatures, labels = [], []\nfor i, fname in enumerate(images):\nfeatures.append(torchvision.io.read_image(os.path.join(\n(continues on next page)\n\n674\nComputer Vision\n(continued from previous page)\nvoc_dir, 'JPEGImages', f'{fname}.jpg')))\nlabels.append(torchvision.io.read_image(os.path.join(\nvoc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\nreturn features, labels\ntrain_features, train_labels = read_voc_images(voc_dir, True)\nWe draw the ﬁrst ﬁve input images and their labels. In the label images, white and black\nrepresent borders and background, respectively, while the other colors correspond to diﬀerent\nclasses.\nn = 5\nimgs = train_features[:n] + train_labels[:n]\nimgs = [img.permute(1,2,0) for img in imgs]\nd2l.show_images(imgs, 2, n);\nNext, we enumerate the RGB color values and class names for all the labels in this dataset.\n#@save\nVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n[0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n[64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n[64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n[0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n[0, 64, 128]]\n#@save\nVOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n'diningtable', 'dog', 'horse', 'motorbike', 'person',\n'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\nWith the two constants deﬁned above, we can conveniently ﬁnd the class index for each pixel\nin a label. We deﬁne the voc_colormap2label function to build the mapping from the above\nRGB color values to class indices, and the voc_label_indices function to map any RGB\nvalues to their class indices in this Pascal VOC2012 dataset.\n\n675\nSemantic Segmentation and the Dataset\n#@save\ndef voc_colormap2label():\n\"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\ncolormap2label = torch.zeros(256 ** 3, dtype=torch.long)\nfor i, colormap in enumerate(VOC_COLORMAP):\ncolormap2label[\n(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\nreturn colormap2label\n#@save\ndef voc_label_indices(colormap, colormap2label):\n\"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\ncolormap = colormap.permute(1, 2, 0).numpy().astype('int32')\nidx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n+ colormap[:, :, 2])\nreturn colormap2label[idx]\nFor example, in the ﬁrst example image, the class index for the front part of the airplane is\n1, while the background index is 0.\ny = voc_label_indices(train_labels[0], voc_colormap2label())\ny[105:115, 130:140], VOC_CLASSES[1]\n(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n[0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n[0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n[0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]),\n'aeroplane')\nData Preprocessing\nIn previous experiments such as in Section 8.1–Section 8.4, images are rescaled to ﬁt the\nmodel’s required input shape. However, in semantic segmentation, doing so requires rescaling\nthe predicted pixel classes back to the original shape of the input image. Such rescaling may\nbe inaccurate, especially for segmented regions with diﬀerent classes. To avoid this issue,\nwe crop the image to a ﬁxed shape instead of rescaling. Speciﬁcally, using random cropping\nfrom image augmentation, we crop the same area of the input image and the label.\n#@save\ndef voc_rand_crop(feature, label, height, width):\n\"\"\"Randomly crop both feature and label images.\"\"\"\nrect = torchvision.transforms.RandomCrop.get_params(\n(continues on next page)\n\n676\nComputer Vision\n(continued from previous page)\nfeature, (height, width))\nfeature = torchvision.transforms.functional.crop(feature, *rect)\nlabel = torchvision.transforms.functional.crop(label, *rect)\nreturn feature, label\nimgs = []\nfor _ in range(n):\nimgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\nimgs = [img.permute(1, 2, 0) for img in imgs]\nd2l.show_images(imgs[::2] + imgs[1::2], 2, n);\nCustom Semantic Segmentation Dataset Class\nWe deﬁne a custom semantic segmentation dataset class VOCSegDataset by inheriting the\nDataset class provided by high-level APIs. By implementing the __getitem__ function, we\ncan arbitrarily access the input image indexed as idx in the dataset and the class index of each\npixel in this image. Since some images in the dataset have a smaller size than the output size of\nrandom cropping, these examples are ﬁltered out by a custom filter function. In addition,\nwe also deﬁne the normalize_image function to standardize the values of the three RGB\nchannels of input images.\n#@save\nclass VOCSegDataset(torch.utils.data.Dataset):\n\"\"\"A customized dataset to load the VOC dataset.\"\"\"\ndef __init__(self, is_train, crop_size, voc_dir):\nself.transform = torchvision.transforms.Normalize(\nmean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nself.crop_size = crop_size\nfeatures, labels = read_voc_images(voc_dir, is_train=is_train)\nself.features = [self.normalize_image(feature)\nfor feature in self.filter(features)]\nself.labels = self.filter(labels)\nself.colormap2label = voc_colormap2label()\n(continues on next page)\n\n677\nSemantic Segmentation and the Dataset\n(continued from previous page)\nprint('read ' + str(len(self.features)) + ' examples')\ndef normalize_image(self, img):\nreturn self.transform(img.float() / 255)\ndef filter(self, imgs):\nreturn [img for img in imgs if (\nimg.shape[1] >= self.crop_size[0] and\nimg.shape[2] >= self.crop_size[1])]\ndef __getitem__(self, idx):\nfeature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n*self.crop_size)\nreturn (feature, voc_label_indices(label, self.colormap2label))\ndef __len__(self):\nreturn len(self.features)\nReading the Dataset\nWe use the custom VOCSegDataset class to create instances of the training set and test set,\nrespectively. Suppose that we specify that the output shape of randomly cropped images is\n320 × 480. Below we can view the number of examples that are retained in the training set\nand test set.\ncrop_size = (320, 480)\nvoc_train = VOCSegDataset(True, crop_size, voc_dir)\nvoc_test = VOCSegDataset(False, crop_size, voc_dir)\nread 1114 examples\nread 1078 examples\nSetting the batch size to 64, we deﬁne the data iterator for the training set. Let’s print the\nshape of the ﬁrst minibatch. Diﬀerent from in image classiﬁcation or object detection, labels\nhere are three-dimensional tensors.\nbatch_size = 64\ntrain_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\ndrop_last=True,\nnum_workers=d2l.get_dataloader_workers())\nfor X, Y in train_iter:\nprint(X.shape)\nprint(Y.shape)\nbreak\ntorch.Size([64, 3, 320, 480])\ntorch.Size([64, 320, 480])\n\n678\nComputer Vision\n221\nPutting It All Together\nFinally, we deﬁne the following load_data_voc function to download and read the Pascal\nVOC2012 semantic segmentation dataset. It returns data iterators for both the training and\ntest datasets.\n#@save\ndef load_data_voc(batch_size, crop_size):\n\"\"\"Load the VOC semantic segmentation dataset.\"\"\"\nvoc_dir = d2l.download_extract('voc2012', os.path.join(\n'VOCdevkit', 'VOC2012'))\nnum_workers = d2l.get_dataloader_workers()\ntrain_iter = torch.utils.data.DataLoader(\nVOCSegDataset(True, crop_size, voc_dir), batch_size,\nshuffle=True, drop_last=True, num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(\nVOCSegDataset(False, crop_size, voc_dir), batch_size,\ndrop_last=True, num_workers=num_workers)\nreturn train_iter, test_iter\n14.9.3 Summary\n• Semantic segmentation recognizes and understands what are in an image in pixel level by\ndividing the image into regions belonging to diﬀerent semantic classes.\n• One of the most important semantic segmentation dataset is Pascal VOC2012.\n• In semantic segmentation, since the input image and label correspond one-to-one on the\npixel, the input image is randomly cropped to a ﬁxed shape rather than rescaled.\n14.9.4 Exercises\n1. How can semantic segmentation be applied in autonomous vehicles and medical image\ndiagnostics? Can you think of other applications?\n2. Recall the descriptions of data augmentation in Section 14.1. Which of the image augmen-\ntation methods used in image classiﬁcation would be infeasible to be applied in semantic\nsegmentation?\nDiscussions221.\n\n679\nTransposed Convolution\n14.10 Transposed Convolution\nThe CNN layers we have seen so far, such as convolutional layers (Section 7.2) and pooling\nlayers (Section 7.5), typically reduce (downsample) the spatial dimensions (height and width)\nof the input, or keep them unchanged. In semantic segmentation that classiﬁes at pixel-level, it\nwill be convenient if the spatial dimensions of the input and output are the same. For example,\nthe channel dimension at one output pixel can hold the classiﬁcation results for the input pixel\nat the same spatial position.\nTo achieve this, especially after the spatial dimensions are reduced by CNN layers, we can\nuse another type of CNN layers that can increase (upsample) the spatial dimensions of in-\ntermediate feature maps. In this section, we will introduce transposed convolution, which is\nalso called fractionally-strided convolution (Dumoulin and Visin, 2016), for reversing down-\nsampling operations by the convolution.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n14.10.1 Basic Operation\nIgnoring channels for now, let’s begin with the basic transposed convolution operation with\nstride of 1 and no padding. Suppose that we are given a nh × nw input tensor and a kh × kw\nkernel. Sliding the kernel window with stride of 1 for nw times in each row and nh times\nin each column yields a total of nhnw intermediate results. Each intermediate result is a\n(nh+kh−1)×(nw+kw−1) tensor that are initialized as zeros. To compute each intermediate\ntensor, each element in the input tensor is multiplied by the kernel so that the resulting kh×kw\ntensor replaces a portion in each intermediate tensor. Note that the position of the replaced\nportion in each intermediate tensor corresponds to the position of the element in the input\ntensor used for the computation. In the end, all the intermediate results are summed over to\nproduce the output.\nAs an example, Fig. 14.10.1 illustrates how transposed convolution with a 2 × 2 kernel is\ncomputed for a 2 × 2 input tensor.\nWe can implement this basic transposed convolution operation trans_conv for a input ma-\ntrix X and a kernel matrix K.\ndef trans_conv(X, K):\nh, w = K.shape\nY = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\nfor i in range(X.shape[0]):\nfor j in range(X.shape[1]):\n(continues on next page)\n\n680\nComputer Vision\nt\nFig. 14.10.1\nTransposed convolution with a 2 × 2 kernel. The shaded portions are a portion of an\nintermediate tensor as well as the input and kernel tensor elements used for the\ncomputation.\n(continued from previous page)\nY[i: i + h, j: j + w] += X[i, j] * K\nreturn Y\nIn contrast to the regular convolution (in Section 7.2) that reduces input elements via the ker-\nnel, the transposed convolution broadcasts input elements via the kernel, thereby producing\nan output that is larger than the input. We can construct the input tensor X and the kernel\ntensor K from Fig. 14.10.1 to validate the output of the above implementation of the basic\ntwo-dimensional transposed convolution operation.\nX = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\nK = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\ntrans_conv(X, K)\ntensor([[ 0.,\n0.,\n1.],\n[ 0.,\n4.,\n6.],\n[ 4., 12.,\n9.]])\nAlternatively, when the input X and kernel K are both four-dimensional tensors, we can use\nhigh-level APIs to obtain the same results.\nX, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\ntconv.weight.data = K\ntconv(X)\ntensor([[[[ 0.,\n0.,\n1.],\n[ 0.,\n4.,\n6.],\n[ 4., 12.,\n9.]]]], grad_fn=<ConvolutionBackward0>)\n14.10.2 Padding, Strides, and Multiple Channels\n\n681\nTransposed Convolution\nDiﬀerent from in the regular convolution where padding is applied to input, it is applied to\noutput in the transposed convolution. For example, when specifying the padding number on\neither side of the height and width as 1, the ﬁrst and last rows and columns will be removed\nfrom the transposed convolution output.\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\ntconv.weight.data = K\ntconv(X)\ntensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)\nIn the transposed convolution, strides are speciﬁed for intermediate results (thus output), not\nfor input. Using the same input and kernel tensors from Fig. 14.10.1, changing the stride\nfrom 1 to 2 increases both the height and weight of intermediate tensors, hence the output\ntensor in Fig. 14.10.2.\nt\nFig. 14.10.2\nTransposed convolution with a 2 × 2 kernel with stride of 2. The shaded portions are a\nportion of an intermediate tensor as well as the input and kernel tensor elements used for\nthe computation.\nThe following code snippet can validate the transposed convolution output for stride of 2 in\nFig. 14.10.2.\ntconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\ntconv.weight.data = K\ntconv(X)\n\n682\nComputer Vision\ntensor([[[[0., 0., 0., 1.],\n[0., 0., 2., 3.],\n[0., 2., 0., 3.],\n[4., 6., 6., 9.]]]], grad_fn=<ConvolutionBackward0>)\nFor multiple input and output channels, the transposed convolution works in the same way\nas the regular convolution. Suppose that the input has ci channels, and that the transposed\nconvolution assigns a kh × kw kernel tensor to each input channel. When multiple output\nchannels are speciﬁed, we will have a ci × kh × kw kernel for each output channel.\nAs in all, if we feed X into a convolutional layer f to output Y = f (X) and create a transposed\nconvolutional layer g with the same hyperparameters as f except for the number of output\nchannels being the number of channels in X, then g(Y) will have the same shape as X. This\ncan be illustrated in the following example.\nX = torch.rand(size=(1, 10, 16, 16))\nconv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\ntconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\ntconv(conv(X)).shape == X.shape\nTrue\n14.10.3 Connection to Matrix Transposition\nThe transposed convolution is named after the matrix transposition. To explain, let’s ﬁrst see\nhow to implement convolutions using matrix multiplications. In the example below, we deﬁne\na 3×3 input X and a 2×2 convolution kernel K, and then use the corr2d function to compute\nthe convolution output Y.\nX = torch.arange(9.0).reshape(3, 3)\nK = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\nY = d2l.corr2d(X, K)\nY\ntensor([[27., 37.],\n[57., 67.]])\nNext, we rewrite the convolution kernel K as a sparse weight matrix W containing a lot of\nzeros. The shape of the weight matrix is (4, 9), where the non-zero elements come from the\nconvolution kernel K.\ndef kernel2matrix(K):\nk, W = torch.zeros(5), torch.zeros((4, 9))\nk[:2], k[3:5] = K[0, :], K[1, :]\n(continues on next page)\n\n683\nTransposed Convolution\n(continued from previous page)\nW[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\nreturn W\nW = kernel2matrix(K)\nW\ntensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],\n[0., 1., 2., 0., 3., 4., 0., 0., 0.],\n[0., 0., 0., 1., 2., 0., 3., 4., 0.],\n[0., 0., 0., 0., 1., 2., 0., 3., 4.]])\nConcatenate the input X row by row to get a vector of length 9. Then the matrix multiplication\nof W and the vectorized X gives a vector of length 4. After reshaping it, we can obtain the same\nresult Y from the original convolution operation above: we just implemented convolutions\nusing matrix multiplications.\nY == torch.matmul(W, X.reshape(-1)).reshape(2, 2)\ntensor([[True, True],\n[True, True]])\nLikewise, we can implement transposed convolutions using matrix multiplications. In the\nfollowing example, we take the 2 × 2 output Y from the above regular convolution as input\nto the transposed convolution. To implement this operation by multiplying matrices, we only\nneed to transpose the weight matrix W with the new shape (9, 4).\nZ = trans_conv(Y, K)\nZ == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)\ntensor([[True, True, True],\n[True, True, True],\n[True, True, True]])\nConsider implementing the convolution by multiplying matrices. Given an input vector x\nand a weight matrix W, the forward propagation function of the convolution can be imple-\nmented by multiplying its input with the weight matrix and outputting a vector y = Wx.\nSince backpropagation follows the chain rule and ∇xy = W⊤, the backpropagation function\nof the convolution can be implemented by multiplying its input with the transposed weight\nmatrix W⊤. Therefore, the transposed convolutional layer can just exchange the forward\npropagation function and the backpropagation function of the convolutional layer: its for-\nward propagation and backpropagation functions multiply their input vector with W⊤and\nW, respectively.\n14.10.4 Summary\n\n684\nComputer Vision\n222\n• In contrast to the regular convolution that reduces input elements via the kernel, the trans-\nposed convolution broadcasts input elements via the kernel, thereby producing an output\nthat is larger than the input.\n• If we feed X into a convolutional layer f to output Y = f (X) and create a transposed\nconvolutional layer g with the same hyperparameters as f except for the number of\noutput channels being the number of channels in X, then g(Y) will have the same shape\nas X.\n• We can implement convolutions using matrix multiplications. The transposed convolu-\ntional layer can just exchange the forward propagation function and the backpropagation\nfunction of the convolutional layer.\n14.10.5 Exercises\n1. In Section 14.10.3, the convolution input X and the transposed convolution output Z have\nthe same shape. Do they have the same value? Why?\n2. Is it eﬃcient to use matrix multiplications to implement convolutions? Why?\nDiscussions222.\n14.11 Fully Convolutional Networks\nAs discussed in Section 14.9, semantic segmentation classiﬁes images in pixel level. A fully\nconvolutional network (FCN) uses a convolutional neural network to transform image pixels\nto pixel classes (Long et al., 2015). Unlike the CNNs that we encountered earlier for image\nclassiﬁcation or object detection, a fully convolutional network transforms the height and\nwidth of intermediate feature maps back to those of the input image: this is achieved by\nthe transposed convolutional layer introduced in Section 14.10. As a result, the classiﬁcation\noutput and the input image have a one-to-one correspondence in pixel level: the channel\ndimension at any output pixel holds the classiﬁcation results for the input pixel at the same\nspatial position.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\n685\nFully Convolutional Networks\n14.11.1 The Model\nHere we describe the basic design of the fully convolutional network model. As shown in Fig.\n14.11.1, this model ﬁrst uses a CNN to extract image features, then transforms the number\nof channels into the number of classes via a 1 × 1 convolutional layer, and ﬁnally transforms\nthe height and width of the feature maps to those of the input image via the transposed\nconvolution introduced in Section 14.10. As a result, the model output has the same height\nand width as the input image, where the output channel contains the predicted classes for the\ninput pixel at the same spatial position.\nt\nFig. 14.11.1\nFully convolutional network.\nBelow, we use a ResNet-18 model pretrained on the ImageNet dataset to extract image fea-\ntures and denote the model instance as pretrained_net. The last few layers of this model\ninclude a global average pooling layer and a fully connected layer: they are not needed in the\nfully convolutional network.\npretrained_net = torchvision.models.resnet18(pretrained=True)\nlist(pretrained_net.children())[-3:]\n[Sequential(\n(0): BasicBlock(\n(conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1,␣\n,→1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_\n,→running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,␣\n,→1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_\n,→running_stats=True)\n(continues on next page)\n\n686\nComputer Vision\n(continued from previous page)\n(downsample): Sequential(\n(0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n(1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_\n,→running_stats=True)\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,␣\n,→1), bias=False)\n(bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_\n,→running_stats=True)\n(relu): ReLU(inplace=True)\n(conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,␣\n,→1), bias=False)\n(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_\n,→running_stats=True)\n)\n),\nAdaptiveAvgPool2d(output_size=(1, 1)),\nLinear(in_features=512, out_features=1000, bias=True)]\nNext, we create the fully convolutional network instance net. It copies all the pretrained layers\nin the ResNet-18 except for the ﬁnal global average pooling layer and the fully connected layer\nthat are closest to the output.\nnet = nn.Sequential(*list(pretrained_net.children())[:-2])\nGiven an input with height and width of 320 and 480 respectively, the forward propagation\nof net reduces the input height and width to 1/32 of the original, namely 10 and 15.\nX = torch.rand(size=(1, 3, 320, 480))\nnet(X).shape\ntorch.Size([1, 512, 10, 15])\nNext, we use a 1 × 1 convolutional layer to transform the number of output channels into the\nnumber of classes (21) of the Pascal VOC2012 dataset. Finally, we need to increase the height\nand width of the feature maps by 32 times to change them back to the height and width of the\ninput image. Recall how to calculate the output shape of a convolutional layer in Section 7.3.\nSince (320−64+16×2+32)/32 = 10 and (480−64+16×2+32)/32 = 15, we construct\na transposed convolutional layer with stride of 32, setting the height and width of the kernel\nto 64, the padding to 16. In general, we can see that for stride s, padding s/2 (assuming\ns/2 is an integer), and the height and width of the kernel 2s, the transposed convolution will\nincrease the height and width of the input by s times.\nnum_classes = 21\nnet.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n(continues on next page)\n\n687\nFully Convolutional Networks\n(continued from previous page)\nnet.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\nkernel_size=64, padding=16, stride=32))\n14.11.2 Initializing Transposed Convolutional Layers\nWe already know that transposed convolutional layers can increase the height and width of\nfeature maps. In image processing, we may need to scale up an image, i.e., upsampling. Bi-\nlinear interpolation is one of the commonly used upsampling techniques. It is also often used\nfor initializing transposed convolutional layers.\nTo explain bilinear interpolation, say that given an input image we want to calculate each\npixel of the upsampled output image. In order to calculate the pixel of the output image at\ncoordinate (x, y), ﬁrst map (x, y) to coordinate (x′, y′) on the input image, for example,\naccording to the ratio of the input size to the output size. Note that the mapped x′ and y′\nare real numbers. Then, ﬁnd the four pixels closest to coordinate (x′, y′) on the input image.\nFinally, the pixel of the output image at coordinate (x, y) is calculated based on these four\nclosest pixels on the input image and their relative distance from (x′, y′).\nUpsampling of bilinear interpolation can be implemented by the transposed convolutional\nlayer with the kernel constructed by the following bilinear_kernel function. Due to space\nlimitations, we only provide the implementation of the bilinear_kernel function below\nwithout discussions on its algorithm design.\ndef bilinear_kernel(in_channels, out_channels, kernel_size):\nfactor = (kernel_size + 1) // 2\nif kernel_size % 2 == 1:\ncenter = factor - 1\nelse:\ncenter = factor - 0.5\nog = (torch.arange(kernel_size).reshape(-1, 1),\ntorch.arange(kernel_size).reshape(1, -1))\nfilt = (1 - torch.abs(og[0] - center) / factor) * \\\n(1 - torch.abs(og[1] - center) / factor)\nweight = torch.zeros((in_channels, out_channels,\nkernel_size, kernel_size))\nweight[range(in_channels), range(out_channels), :, :] = filt\nreturn weight\nLet’s experiment with upsampling of bilinear interpolation that is implemented by a trans-\nposed convolutional layer. We construct a transposed convolutional layer that doubles the\nheight and weight, and initialize its kernel with the bilinear_kernel function.\nconv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\nbias=False)\nconv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));\n\n688\nComputer Vision\nRead the image X and assign the upsampling output to Y. In order to print the image, we need\nto adjust the position of the channel dimension.\nimg = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))\nX = img.unsqueeze(0)\nY = conv_trans(X)\nout_img = Y[0].permute(1, 2, 0).detach()\nAs we can see, the transposed convolutional layer increases both the height and width of the\nimage by a factor of two. Except for the diﬀerent scales in coordinates, the image scaled up\nby bilinear interpolation and the original image printed in Section 14.3 look the same.\nd2l.set_figsize()\nprint('input image shape:', img.permute(1, 2, 0).shape)\nd2l.plt.imshow(img.permute(1, 2, 0));\nprint('output image shape:', out_img.shape)\nd2l.plt.imshow(out_img);\ninput image shape: torch.Size([561, 728, 3])\noutput image shape: torch.Size([1122, 1456, 3])\nIn a fully convolutional network, we initialize the transposed convolutional layer with up-\nsampling of bilinear interpolation. For the 1 × 1 convolutional layer, we use Xavier initial-\nization.\nW = bilinear_kernel(num_classes, num_classes, 64)\nnet.transpose_conv.weight.data.copy_(W);\n14.11.3 Reading the Dataset\nWe read the semantic segmentation dataset as introduced in Section 14.9. The output image\nshape of random cropping is speciﬁed as 320 × 480: both the height and width are divisible\nby 32.\n\n689\nFully Convolutional Networks\nbatch_size, crop_size = 32, (320, 480)\ntrain_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)\nread 1114 examples\nread 1078 examples\n14.11.4 Training\nNow we can train our constructed fully convolutional network. The loss function and accu-\nracy calculation here are not essentially diﬀerent from those in image classiﬁcation of earlier\nchapters. Because we use the output channel of the transposed convolutional layer to predict\nthe class for each pixel, the channel dimension is speciﬁed in the loss calculation. In addition,\nthe accuracy is calculated based on correctness of the predicted class for all the pixels.\ndef loss(inputs, targets):\nreturn F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\nnum_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\ntrainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.448, train acc 0.862, test acc 0.852\n195.7 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n14.11.5 Prediction\nWhen predicting, we need to standardize the input image in each channel and transform the\nimage into the four-dimensional input format required by the CNN.\n\n690\nComputer Vision\ndef predict(img):\nX = test_iter.dataset.normalize_image(img).unsqueeze(0)\npred = net(X.to(devices[0])).argmax(dim=1)\nreturn pred.reshape(pred.shape[1], pred.shape[2])\nTo visualize the predicted class of each pixel, we map the predicted class back to its label\ncolor in the dataset.\ndef label2image(pred):\ncolormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\nX = pred.long()\nreturn colormap[X, :]\nImages in the test dataset vary in size and shape. Since the model uses a transposed convolu-\ntional layer with stride of 32, when the height or width of an input image is indivisible by 32,\nthe output height or width of the transposed convolutional layer will deviate from the shape\nof the input image. In order to address this issue, we can crop multiple rectangular areas with\nheight and width that are integer multiples of 32 in the image, and perform forward propa-\ngation on the pixels in these areas separately. Note that the union of these rectangular areas\nneeds to completely cover the input image. When a pixel is covered by multiple rectangular\nareas, the average of the transposed convolution outputs in separate areas for this same pixel\ncan be input to the softmax operation to predict the class.\nFor simplicity, we only read a few larger test images, and crop a 320×480 area for prediction\nstarting from the upper-left corner of an image. For these test images, we print their cropped\nareas, prediction results, and ground-truth row by row.\nvoc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\ntest_images, test_labels = d2l.read_voc_images(voc_dir, False)\nn, imgs = 4, []\nfor i in range(n):\ncrop_rect = (0, 0, 320, 480)\nX = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\npred = label2image(predict(X))\nimgs += [X.permute(1,2,0), pred.cpu(),\ntorchvision.transforms.functional.crop(\ntest_labels[i], *crop_rect).permute(1,2,0)]\nd2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);\n14.11.6 Summary\n• The fully convolutional network ﬁrst uses a CNN to extract image features, then transforms\nthe number of channels into the number of classes via a 1 × 1 convolutional layer, and\nﬁnally transforms the height and width of the feature maps to those of the input image\nvia the transposed convolution.\n• In a fully convolutional network, we can use upsampling of bilinear interpolation to ini-\ntialize the transposed convolutional layer.\n\n691\nNeural Style Transfer\n223\n14.11.7 Exercises\n1. If we use Xavier initialization for the transposed convolutional layer in the experiment,\nhow does the result change?\n2. Can you further improve the accuracy of the model by tuning the hyperparameters?\n3. Predict the classes of all pixels in test images.\n4. The original fully convolutional network paper also uses outputs of some intermediate\nCNN layers (Long et al., 2015). Try to implement this idea.\nDiscussions223.\n14.12 Neural Style Transfer\nIf you are a photography enthusiast, you may be familiar with the ﬁlter. It can change the color\nstyle of photos so that landscape photos become sharper or portrait photos have whitened\nskins. However, one ﬁlter usually only changes one aspect of the photo. To apply an ideal\nstyle to a photo, you probably need to try many diﬀerent ﬁlter combinations. This process is\nas complex as tuning the hyperparameters of a model.\nIn this section, we will leverage layerwise representations of a CNN to automatically apply\n\n692\nComputer Vision\nthe style of one image to another image, i.e., style transfer (Gatys et al., 2016). This task\nneeds two input images: one is the content image and the other is the style image. We will\nuse neural networks to modify the content image to make it close to the style image in style.\nFor example, the content image in Fig. 14.12.1 is a landscape photo taken by us in Mount\nRainier National Park in the suburbs of Seattle, while the style image is an oil painting with\nthe theme of autumn oak trees. In the output synthesized image, the oil brush strokes of the\nstyle image are applied, leading to more vivid colors, while preserving the main shape of the\nobjects in the content image.\nt\nFig. 14.12.1\nGiven content and style images, style transfer outputs a synthesized image.\n14.12.1 Method\nFig. 14.12.2 illustrates the CNN-based style transfer method with a simpliﬁed example. First,\nwe initialize the synthesized image, for example, into the content image. This synthesized\nimage is the only variable that needs to be updated during the style transfer process, i.e.,\nthe model parameters to be updated during training. Then we choose a pretrained CNN to\nextract image features and freeze its model parameters during training. This deep CNN uses\nmultiple layers to extract hierarchical features for images. We can choose the output of some\nof these layers as content features or style features. Take Fig. 14.12.2 as an example. The\npretrained neural network here has 3 convolutional layers, where the second layer outputs the\ncontent features, and the ﬁrst and third layers output the style features.\nNext, we calculate the loss function of style transfer through forward propagation (direction of\nsolid arrows), and update the model parameters (the synthesized image for output) through\nbackpropagation (direction of dashed arrows). The loss function commonly used in style\ntransfer consists of three parts: (i) content loss makes the synthesized image and the content\nimage close in content features; (ii) style loss makes the synthesized image and style image\nclose in style features; and (iii) total variation loss helps to reduce the noise in the synthesized\nimage. Finally, when the model training is over, we output the model parameters of the style\ntransfer to generate the ﬁnal synthesized image.\n\n693\nNeural Style Transfer\nt\nFig. 14.12.2\nCNN-based style transfer process. Solid lines show the direction of forward propagation\nand dotted lines show backward propagation.\nIn the following, we will explain the technical details of style transfer via a concrete experi-\nment.\n14.12.2 Reading the Content and Style Images\nFirst, we read the content and style images. From their printed coordinate axes, we can tell\nthat these images have diﬀerent sizes.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\nd2l.set_figsize()\ncontent_img = d2l.Image.open('../img/rainier.jpg')\nd2l.plt.imshow(content_img);\n\n694\nComputer Vision\nstyle_img = d2l.Image.open('../img/autumn-oak.jpg')\nd2l.plt.imshow(style_img);\n14.12.3 Preprocessing and Postprocessing\nBelow, we deﬁne two functions for preprocessing and postprocessing images. The prepro-\ncess function standardizes each of the three RGB channels of the input image and transforms\nthe results into the CNN input format. The postprocess function restores the pixel values\nin the output image to their original values before standardization. Since the image printing\nfunction requires that each pixel has a ﬂoating point value from 0 to 1, we replace any value\nsmaller than 0 or greater than 1 with 0 or 1, respectively.\nrgb_mean = torch.tensor([0.485, 0.456, 0.406])\nrgb_std = torch.tensor([0.229, 0.224, 0.225])\ndef preprocess(img, image_shape):\ntransforms = torchvision.transforms.Compose([\ntorchvision.transforms.Resize(image_shape),\ntorchvision.transforms.ToTensor(),\ntorchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\nreturn transforms(img).unsqueeze(0)\ndef postprocess(img):\nimg = img[0].to(rgb_std.device)\nimg = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\nreturn torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))\n14.12.4 Extracting Features\nWe use the VGG-19 model pretrained on the ImageNet dataset to extract image features\n(Gatys et al., 2016).\npretrained_net = torchvision.models.vgg19(pretrained=True)\n\n695\nNeural Style Transfer\nIn order to extract the content features and style features of the image, we can select the\noutput of certain layers in the VGG network. Generally speaking, the closer to the input\nlayer, the easier to extract details of the image, and vice versa, the easier to extract the global\ninformation of the image. In order to avoid excessively retaining the details of the content\nimage in the synthesized image, we choose a VGG layer that is closer to the output as the\ncontent layer to output the content features of the image. We also select the output of diﬀerent\nVGG layers for extracting local and global style features. These layers are also called style\nlayers. As mentioned in Section 8.2, the VGG network uses 5 convolutional blocks. In the\nexperiment, we choose the last convolutional layer of the fourth convolutional block as the\ncontent layer, and the ﬁrst convolutional layer of each convolutional block as the style layer.\nThe indices of these layers can be obtained by printing the pretrained_net instance.\nstyle_layers, content_layers = [0, 5, 10, 19, 28], [25]\nWhen extracting features using VGG layers, we only need to use all those from the input\nlayer to the content layer or style layer that is closest to the output layer. Let’s construct\na new network instance net, which only retains all the VGG layers to be used for feature\nextraction.\nnet = nn.Sequential(*[pretrained_net.features[i] for i in\nrange(max(content_layers + style_layers) + 1)])\nGiven the input X, if we simply invoke the forward propagation net(X), we can only get the\noutput of the last layer. Since we also need the outputs of intermediate layers, we need to\nperform layer-by-layer computation and keep the content and style layer outputs.\ndef extract_features(X, content_layers, style_layers):\ncontents = []\nstyles = []\nfor i in range(len(net)):\nX = net[i](X)\nif i in style_layers:\nstyles.append(X)\nif i in content_layers:\ncontents.append(X)\nreturn contents, styles\nTwo functions are deﬁned below: the get_contents function extracts content features from\nthe content image, and the get_styles function extracts style features from the style im-\nage. Since there is no need to update the model parameters of the pretrained VGG during\ntraining, we can extract the content and the style features even before the training starts.\nSince the synthesized image is a set of model parameters to be updated for style transfer,\nwe can only extract the content and style features of the synthesized image by calling the\nextract_features function during training.\ndef get_contents(image_shape, device):\ncontent_X = preprocess(content_img, image_shape).to(device)\n(continues on next page)\n\n696\nComputer Vision\n(continued from previous page)\ncontents_Y, _ = extract_features(content_X, content_layers, style_layers)\nreturn content_X, contents_Y\ndef get_styles(image_shape, device):\nstyle_X = preprocess(style_img, image_shape).to(device)\n_, styles_Y = extract_features(style_X, content_layers, style_layers)\nreturn style_X, styles_Y\n14.12.5 Deﬁning the Loss Function\nNow we will describe the loss function for style transfer. The loss function consists of the\ncontent loss, style loss, and total variation loss.\nContent Loss\nSimilar to the loss function in linear regression, the content loss measures the diﬀerence in\ncontent features between the synthesized image and the content image via the squared loss\nfunction. The two inputs of the squared loss function are both outputs of the content layer\ncomputed by the extract_features function.\ndef content_loss(Y_hat, Y):\n# We detach the target content from the tree used to dynamically compute\n# the gradient: this is a stated value, not a variable. Otherwise the loss\n# will throw an error.\nreturn torch.square(Y_hat - Y.detach()).mean()\nStyle Loss\nStyle loss, similar to content loss, also uses the squared loss function to measure the diﬀerence\nin style between the synthesized image and the style image. To express the style output of any\nstyle layer, we ﬁrst use the extract_features function to compute the style layer output.\nSuppose that the output has 1 example, c channels, height h, and width w, we can transform\nthis output into matrix X with c rows and hw columns. This matrix can be thought of as\nthe concatenation of c vectors x1, . . ., xc, each of which has a length of hw. Here, vector xi\nrepresents the style feature of channel i.\nIn the Gram matrix of these vectors XX⊤∈Rc×c, element xij in row i and column j is the\ndot product of vectors xi and xj. It represents the correlation of the style features of channels\ni and j. We use this Gram matrix to represent the style output of any style layer. Note that\nwhen the value of hw is larger, it likely leads to larger values in the Gram matrix. Note also\nthat the height and width of the Gram matrix are both the number of channels c. To allow\nstyle loss not to be aﬀected by these values, the gram function below divides the Gram matrix\nby the number of its elements, i.e., chw.\n\n697\nNeural Style Transfer\ndef gram(X):\nnum_channels, n = X.shape[1], X.numel() // X.shape[1]\nX = X.reshape((num_channels, n))\nreturn torch.matmul(X, X.T) / (num_channels * n)\nObviously, the two Gram matrix inputs of the squared loss function for style loss are based\non the style layer outputs for the synthesized image and the style image. It is assumed here\nthat the Gram matrix gram_Y based on the style image has been precomputed.\ndef style_loss(Y_hat, gram_Y):\nreturn torch.square(gram(Y_hat) - gram_Y.detach()).mean()\nTotal Variation Loss\nSometimes, the learned synthesized image has a lot of high-frequency noise, i.e., particu-\nlarly bright or dark pixels. One common noise reduction method is total variation denoising.\nDenote by xi,j the pixel value at coordinate (i, j). Reducing total variation loss\n∑\ni,j\n\f\fxi,j −xi+1, j\n\f\f +\n\f\fxi, j −xi,j+1\n\f\f\n(14.12.1)\nmakes values of neighboring pixels on the synthesized image closer.\ndef tv_loss(Y_hat):\nreturn 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\ntorch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())\nLoss Function\nThe loss function of style transfer is the weighted sum of content loss, style loss, and total\nvariation loss. By adjusting these weight hyperparameters, we can balance among content\nretention, style transfer, and noise reduction on the synthesized image.\ncontent_weight, style_weight, tv_weight = 1, 1e4, 10\ndef compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n# Calculate the content, style, and total variance losses respectively\ncontents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\ncontents_Y_hat, contents_Y)]\nstyles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\nstyles_Y_hat, styles_Y_gram)]\ntv_l = tv_loss(X) * tv_weight\n# Add up all the losses\nl = sum(styles_l + contents_l + [tv_l])\nreturn contents_l, styles_l, tv_l, l\n\n698\nComputer Vision\n14.12.6 Initializing the Synthesized Image\nIn style transfer, the synthesized image is the only variable that needs to be updated during\ntraining. Thus, we can deﬁne a simple model, SynthesizedImage, and treat the synthesized\nimage as the model parameters. In this model, forward propagation just returns the model\nparameters.\nclass SynthesizedImage(nn.Module):\ndef __init__(self, img_shape, **kwargs):\nsuper(SynthesizedImage, self).__init__(**kwargs)\nself.weight = nn.Parameter(torch.rand(*img_shape))\ndef forward(self):\nreturn self.weight\nNext, we deﬁne the get_inits function. This function creates a synthesized image model\ninstance and initializes it to the image X. Gram matrices for the style image at various style\nlayers, styles_Y_gram, are computed prior to training.\ndef get_inits(X, device, lr, styles_Y):\ngen_img = SynthesizedImage(X.shape).to(device)\ngen_img.weight.data.copy_(X.data)\ntrainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\nstyles_Y_gram = [gram(Y) for Y in styles_Y]\nreturn gen_img(), styles_Y_gram, trainer\n14.12.7 Training\nWhen training the model for style transfer, we continuously extract content features and style\nfeatures of the synthesized image, and calculate the loss function. Below deﬁnes the training\nloop.\ndef train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\nX, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\nscheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[10, num_epochs],\nlegend=['content', 'style', 'TV'],\nncols=2, figsize=(7, 2.5))\nfor epoch in range(num_epochs):\ntrainer.zero_grad()\ncontents_Y_hat, styles_Y_hat = extract_features(\nX, content_layers, style_layers)\ncontents_l, styles_l, tv_l, l = compute_loss(\nX, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\nl.backward()\ntrainer.step()\nscheduler.step()\nif (epoch + 1) % 10 == 0:\n(continues on next page)\n\n699\nNeural Style Transfer\n(continued from previous page)\nanimator.axes[1].imshow(postprocess(X))\nanimator.add(epoch + 1, [float(sum(contents_l)),\nfloat(sum(styles_l)), float(tv_l)])\nreturn X\nNow we start to train the model. We rescale the height and width of the content and style\nimages to 300 by 450 pixels. We use the content image to initialize the synthesized im-\nage.\ndevice, image_shape = d2l.try_gpu(), (300, 450)\n# PIL Image (h, w)\nnet = net.to(device)\ncontent_X, contents_Y = get_contents(image_shape, device)\n_, styles_Y = get_styles(image_shape, device)\noutput = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)\nWe can see that the synthesized image retains the scenery and objects of the content image,\nand transfers the color of the style image at the same time. For example, the synthesized\nimage has blocks of color like those in the style image. Some of these blocks even have the\nsubtle texture of brush strokes.\n14.12.8 Summary\n• The loss function commonly used in style transfer consists of three parts: (i) content loss\nmakes the synthesized image and the content image close in content features; (ii) style\nloss makes the synthesized image and style image close in style features; and (iii) total\nvariation loss helps to reduce the noise in the synthesized image.\n• We can use a pretrained CNN to extract image features and minimize the loss function to\ncontinuously update the synthesized image as model parameters during training.\n• We use Gram matrices to represent the style outputs from the style layers.\n14.12.9 Exercises\n\n700\nComputer Vision\n224\n1. How does the output change when you select diﬀerent content and style layers?\n2. Adjust the weight hyperparameters in the loss function. Does the output retain more con-\ntent or have less noise?\n3. Use diﬀerent content and style images. Can you create more interesting synthesized im-\nages?\n4. Can we apply style transfer for text? Hint: you may refer to the survey paper by Hu et al.\n(2022).\nDiscussions224.\n14.13 Image Classiﬁcation (CIFAR-10) on Kaggle\nSo far, we have been using high-level APIs of deep learning frameworks to directly obtain\nimage datasets in tensor format. However, custom image datasets often come in the form\nof image ﬁles. In this section, we will start from raw image ﬁles, and organize, read, then\ntransform them into tensor format step by step.\nWe experimented with the CIFAR-10 dataset in Section 14.1, which is an important dataset in\ncomputer vision. In this section, we will apply the knowledge we learned in previous sections\nto practice the Kaggle competition of CIFAR-10 image classiﬁcation. The web address of\nthe competition is https://www.kaggle.com/c/cifar-10\nFig. 14.13.1 shows the information on the competition’s webpage. In order to submit the\nresults, you need to register a Kaggle account.\nt\nFig. 14.13.1\nCIFAR-10 image classiﬁcation competition webpage information. The competition\ndataset can be obtained by clicking the “Data” tab.\n\n701\nImage Classiﬁcation (CIFAR-10) on Kaggle\nimport collections\nimport math\nimport os\nimport shutil\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\n14.13.1 Obtaining and Organizing the Dataset\nThe competition dataset is divided into a training set and a test set, which contain 50000 and\n300000 images, respectively. In the test set, 10000 images will be used for evaluation, while\nthe remaining 290000 images will not be evaluated: they are included just to make it hard to\ncheat with manually labeled results of the test set. The images in this dataset are all png color\n(RGB channels) image ﬁles, whose height and width are both 32 pixels. The images cover a\ntotal of 10 categories, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, boats, and\ntrucks. The upper-left corner of Fig. 14.13.1 shows some images of airplanes, cars, and birds\nin the dataset.\nDownloading the Dataset\nAfter logging in to Kaggle, we can click the “Data” tab on the CIFAR-10 image classiﬁcation\ncompetition webpage shown in Fig. 14.13.1 and download the dataset by clicking the “Down-\nload All” button. After unzipping the downloaded ﬁle in ../data, and unzipping train.7z\nand test.7z inside it, you will ﬁnd the entire dataset in the following paths:\n• ../data/cifar-10/train/[1-50000].png\n• ../data/cifar-10/test/[1-300000].png\n• ../data/cifar-10/trainLabels.csv\n• ../data/cifar-10/sampleSubmission.csv\nwhere the train and test directories contain the training and testing images, respectively,\ntrainLabels.csv provides labels for the training images, and sample_submission.csv is\na sample submission ﬁle.\nTo make it easier to get started, we provide a small-scale sample of the dataset that contains\nthe ﬁrst 1000 training images and 5 random testing images. To use the full dataset of the\nKaggle competition, you need to set the following demo variable to False.\n#@save\nd2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n(continues on next page)\n\n702\nComputer Vision\n(continued from previous page)\n'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n# If you use the full dataset downloaded for the Kaggle competition, set\n# `demo` to False\ndemo = True\nif demo:\ndata_dir = d2l.download_extract('cifar10_tiny')\nelse:\ndata_dir = '../data/cifar-10/'\nDownloading ../data/kaggle_cifar10_tiny.zip from http://d2l-data.s3-accelerate.\n,→amazonaws.com/kaggle_cifar10_tiny.zip...\nOrganizing the Dataset\nWe need to organize datasets to facilitate model training and testing. Let’s ﬁrst read the labels\nfrom the csv ﬁle. The following function returns a dictionary that maps the non-extension part\nof the ﬁlename to its label.\n#@save\ndef read_csv_labels(fname):\n\"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\nwith open(fname, 'r') as f:\n# Skip the file header line (column name)\nlines = f.readlines()[1:]\ntokens = [l.rstrip().split(',') for l in lines]\nreturn dict(((name, label) for name, label in tokens))\nlabels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\nprint('# training examples:', len(labels))\nprint('# classes:', len(set(labels.values())))\n# training examples: 1000\n# classes: 10\nNext, we deﬁne the reorg_train_valid function to split the validation set out of the orig-\ninal training set. The argument valid_ratio in this function is the ratio of the number\nof examples in the validation set to the number of examples in the original training set.\nMore concretely, let n be the number of images of the class with the least examples, and r\nbe the ratio. The validation set will split out max(⌊nr⌋, 1) images for each class. Let’s use\nvalid_ratio=0.1 as an example. Since the original training set has 50000 images, there will\nbe 45000 images used for training in the path train_valid_test/train, while the other\n5000 images will be split out as validation set in the path train_valid_test/valid. After\norganizing the dataset, images of the same class will be placed under the same folder.\n\n703\nImage Classiﬁcation (CIFAR-10) on Kaggle\n#@save\ndef copyfile(filename, target_dir):\n\"\"\"Copy a file into a target directory.\"\"\"\nos.makedirs(target_dir, exist_ok=True)\nshutil.copy(filename, target_dir)\n#@save\ndef reorg_train_valid(data_dir, labels, valid_ratio):\n\"\"\"Split the validation set out of the original training set.\"\"\"\n# The number of examples of the class that has the fewest examples in the\n# training dataset\nn = collections.Counter(labels.values()).most_common()[-1][1]\n# The number of examples per class for the validation set\nn_valid_per_label = max(1, math.floor(n * valid_ratio))\nlabel_count = {}\nfor train_file in os.listdir(os.path.join(data_dir, 'train')):\nlabel = labels[train_file.split('.')[0]]\nfname = os.path.join(data_dir, 'train', train_file)\ncopyfile(fname, os.path.join(data_dir, 'train_valid_test',\n'train_valid', label))\nif label not in label_count or label_count[label] < n_valid_per_label:\ncopyfile(fname, os.path.join(data_dir, 'train_valid_test',\n'valid', label))\nlabel_count[label] = label_count.get(label, 0) + 1\nelse:\ncopyfile(fname, os.path.join(data_dir, 'train_valid_test',\n'train', label))\nreturn n_valid_per_label\nThe reorg_test function below organizes the testing set for data loading during predic-\ntion.\n#@save\ndef reorg_test(data_dir):\n\"\"\"Organize the testing set for data loading during prediction.\"\"\"\nfor test_file in os.listdir(os.path.join(data_dir, 'test')):\ncopyfile(os.path.join(data_dir, 'test', test_file),\nos.path.join(data_dir, 'train_valid_test', 'test',\n'unknown'))\nFinally, we use a function to invoke the read_csv_labels, reorg_train_valid, and re-\norg_test functions deﬁned above.\ndef reorg_cifar10_data(data_dir, valid_ratio):\nlabels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\nreorg_train_valid(data_dir, labels, valid_ratio)\nreorg_test(data_dir)\nHere we only set the batch size to 32 for the small-scale sample of the dataset. When training\nand testing the complete dataset of the Kaggle competition, batch_size should be set to a\nlarger integer, such as 128. We split out 10% of the training examples as the validation set\nfor tuning hyperparameters.\n\n704\nComputer Vision\nbatch_size = 32 if demo else 128\nvalid_ratio = 0.1\nreorg_cifar10_data(data_dir, valid_ratio)\n14.13.2 Image Augmentation\nWe use image augmentation to address overﬁtting. For example, images can be ﬂipped hori-\nzontally at random during training. We can also perform standardization for the three RGB\nchannels of color images. Below lists some of these operations that you can tweak.\ntransform_train = torchvision.transforms.Compose([\n# Scale the image up to a square of 40 pixels in both height and width\ntorchvision.transforms.Resize(40),\n# Randomly crop a square image of 40 pixels in both height and width to\n# produce a small square of 0.64 to 1 times the area of the original\n# image, and then scale it to a square of 32 pixels in both height and\n# width\ntorchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\nratio=(1.0, 1.0)),\ntorchvision.transforms.RandomHorizontalFlip(),\ntorchvision.transforms.ToTensor(),\n# Standardize each channel of the image\ntorchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n[0.2023, 0.1994, 0.2010])])\nDuring testing, we only perform standardization on images so as to remove randomness in\nthe evaluation results.\ntransform_test = torchvision.transforms.Compose([\ntorchvision.transforms.ToTensor(),\ntorchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n[0.2023, 0.1994, 0.2010])])\n14.13.3 Reading the Dataset\nNext, we read the organized dataset consisting of raw image ﬁles. Each example includes an\nimage and a label.\ntrain_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'train_valid_test', folder),\ntransform=transform_train) for folder in ['train', 'train_valid']]\nvalid_ds, test_ds = [torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'train_valid_test', folder),\ntransform=transform_test) for folder in ['valid', 'test']]\nDuring training, we need to specify all the image augmentation operations deﬁned above.\n\n705\nImage Classiﬁcation (CIFAR-10) on Kaggle\nWhen the validation set is used for model evaluation during hyperparameter tuning, no ran-\ndomness from image augmentation should be introduced. Before ﬁnal prediction, we train\nthe model on the combined training set and validation set to make full use of all the labeled\ndata.\ntrain_iter, train_valid_iter = [torch.utils.data.DataLoader(\ndataset, batch_size, shuffle=True, drop_last=True)\nfor dataset in (train_ds, train_valid_ds)]\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\ndrop_last=True)\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\ndrop_last=False)\n14.13.4 Deﬁning the Model\nWe deﬁne the ResNet-18 model described in Section 8.6.\ndef get_net():\nnum_classes = 10\nnet = d2l.resnet18(num_classes, 3)\nreturn net\nloss = nn.CrossEntropyLoss(reduction=\"none\")\n14.13.5 Deﬁning the Training Function\nWe will select models and tune hyperparameters according to the model’s performance on\nthe validation set. In the following, we deﬁne the model training function train.\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\ntrainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\nweight_decay=wd)\nscheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer = len(train_iter), d2l.Timer()\nlegend = ['train loss', 'train acc']\nif valid_iter is not None:\nlegend.append('valid acc')\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\nlegend=legend)\nnet = nn.DataParallel(net, device_ids=devices).to(devices[0])\nfor epoch in range(num_epochs):\nnet.train()\nmetric = d2l.Accumulator(3)\nfor i, (features, labels) in enumerate(train_iter):\ntimer.start()\nl, acc = d2l.train_batch_ch13(net, features, labels,\n(continues on next page)\n\n706\nComputer Vision\n(continued from previous page)\nloss, trainer, devices)\nmetric.add(l, acc, labels.shape[0])\ntimer.stop()\nif (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\nanimator.add(epoch + (i + 1) / num_batches,\n(metric[0] / metric[2], metric[1] / metric[2],\nNone))\nif valid_iter is not None:\nvalid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\nanimator.add(epoch + 1, (None, None, valid_acc))\nscheduler.step()\nmeasures = (f'train loss {metric[0] / metric[2]:.3f}, '\nf'train acc {metric[1] / metric[2]:.3f}')\nif valid_iter is not None:\nmeasures += f', valid acc {valid_acc:.3f}'\nprint(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\nf' examples/sec on {str(devices)}')\n14.13.6 Training and Validating the Model\nNow, we can train and validate the model. All the following hyperparameters can be tuned.\nFor example, we can increase the number of epochs. When lr_period and lr_decay are set\nto 4 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by\n0.9 after every 4 epochs. Just for ease of demonstration, we only train 20 epochs here.\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\nlr_period, lr_decay, net = 4, 0.9, get_net()\nnet(next(iter(train_iter))[0])\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 0.727, train acc 0.738, valid acc 0.484\n613.0 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n\n707\nImage Classiﬁcation (CIFAR-10) on Kaggle\n14.13.7 Classifying the Testing Set and Submitting Results on Kaggle\nAfter obtaining a promising model with hyperparameters, we use all the labeled data (includ-\ning the validation set) to retrain the model and classify the testing set.\nnet, preds = get_net(), []\nnet(next(iter(train_valid_iter))[0])\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\nfor X, _ in test_iter:\ny_hat = net(X.to(devices[0]))\npreds.extend(y_hat.argmax(dim=1).type(torch.int32).cpu().numpy())\nsorted_ids = list(range(1, len(test_ds) + 1))\nsorted_ids.sort(key=lambda x: str(x))\ndf = pd.DataFrame({'id': sorted_ids, 'label': preds})\ndf['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])\ndf.to_csv('submission.csv', index=False)\ntrain loss 0.792, train acc 0.725\n806.6 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nThe above code will generate a submission.csv ﬁle, whose format meets the requirement\nof the Kaggle competition. The method for submitting results to Kaggle is similar to that in\nSection 5.7.\n14.13.8 Summary\n• We can read datasets containing raw image ﬁles after organizing them into the required\nformat.\n• We can use convolutional neural networks and image augmentation in an image classiﬁca-\ntion competition.\n\n708\nComputer Vision\n225\n14.13.9 Exercises\n1. Use the complete CIFAR-10 dataset for this Kaggle competition. Set hyperparameters as\nbatch_size = 128, num_epochs = 100, lr = 0.1, lr_period = 50, and lr_decay =\n0.1. See what accuracy and ranking you can achieve in this competition. Can you further\nimprove them?\n2. What accuracy can you get when not using image augmentation?\nDiscussions225.\n14.14 Dog Breed Identiﬁcation (ImageNet Dogs) on\nKaggle\nIn this section, we will practice the dog breed identiﬁcation problem on Kaggle. The web\naddress of this competition is https://www.kaggle.com/c/dog-breed-identification\nIn this competition, 120 diﬀerent breeds of dogs will be recognized. In fact, the dataset for\nthis competition is a subset of the ImageNet dataset. Unlike the images in the CIFAR-10\ndataset in Section 14.13, the images in the ImageNet dataset are both higher and wider in\nvarying dimensions. Fig. 14.14.1 shows the information on the competition’s webpage. You\nneed a Kaggle account to submit your results.\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\n14.14.1 Obtaining and Organizing the Dataset\nThe competition dataset is divided into a training set and a test set, which contain 10222 and\n10357 JPEG images of three RGB (color) channels, respectively. Among the training dataset,\nthere are 120 breeds of dogs such as Labradors, Poodles, Dachshunds, Samoyeds, Huskies,\nChihuahuas, and Yorkshire Terriers.\nDownloading the Dataset\nAfter logging into Kaggle, you can click on the “Data” tab on the competition webpage shown\nin Fig. 14.14.1 and download the dataset by clicking the “Download All” button. After un-\n\n709\nDog Breed Identiﬁcation (ImageNet Dogs) on Kaggle\nt\nFig. 14.14.1\nThe dog breed identiﬁcation competition website. The competition dataset can be\nobtained by clicking the “Data” tab.\nzipping the downloaded ﬁle in ../data, you will ﬁnd the entire dataset in the following\npaths:\n• ../data/dog-breed-identiﬁcation/labels.csv\n• ../data/dog-breed-identiﬁcation/sample_submission.csv\n• ../data/dog-breed-identiﬁcation/train\n• ../data/dog-breed-identiﬁcation/test\nYou may have noticed that the above structure is similar to that of the CIFAR-10 compe-\ntition in Section 14.13, where folders train/ and test/ contain training and testing dog\nimages, respectively, and labels.csv contains the labels for the training images. Similarly,\nto make it easier to get started, we provide a small sample of the dataset mentioned above:\ntrain_valid_test_tiny.zip. If you are going to use the full dataset for the Kaggle com-\npetition, you need to change the demo variable below to False.\n#@save\nd2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n# If you use the full dataset downloaded for the Kaggle competition, change\n# the variable below to `False`\n(continues on next page)\n\n710\nComputer Vision\n(continued from previous page)\ndemo = True\nif demo:\ndata_dir = d2l.download_extract('dog_tiny')\nelse:\ndata_dir = os.path.join('..', 'data', 'dog-breed-identification')\nOrganizing the Dataset\nWe can organize the dataset similarly to what we did in Section 14.13, namely splitting out\na validation set from the original training set, and moving images into subfolders grouped by\nlabels.\nThe reorg_dog_data function below reads the training data labels, splits out the validation\nset, and organizes the training set.\ndef reorg_dog_data(data_dir, valid_ratio):\nlabels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\nd2l.reorg_train_valid(data_dir, labels, valid_ratio)\nd2l.reorg_test(data_dir)\nbatch_size = 32 if demo else 128\nvalid_ratio = 0.1\nreorg_dog_data(data_dir, valid_ratio)\n14.14.2 Image Augmentation\nRecall that this dog breed dataset is a subset of the ImageNet dataset, whose images are\nlarger than those of the CIFAR-10 dataset in Section 14.13. The following lists a few image\naugmentation operations that might be useful for relatively larger images.\ntransform_train = torchvision.transforms.Compose([\n# Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n# the original area and height-to-width ratio between 3/4 and 4/3. Then,\n# scale the image to create a new 224 x 224 image\ntorchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\nratio=(3.0/4.0, 4.0/3.0)),\ntorchvision.transforms.RandomHorizontalFlip(),\n# Randomly change the brightness, contrast, and saturation\ntorchvision.transforms.ColorJitter(brightness=0.4,\ncontrast=0.4,\nsaturation=0.4),\n# Add random noise\ntorchvision.transforms.ToTensor(),\n# Standardize each channel of the image\ntorchvision.transforms.Normalize([0.485, 0.456, 0.406],\n[0.229, 0.224, 0.225])])\n\n711\nDog Breed Identiﬁcation (ImageNet Dogs) on Kaggle\nDuring prediction, we only use image preprocessing operations without randomness.\ntransform_test = torchvision.transforms.Compose([\ntorchvision.transforms.Resize(256),\n# Crop a 224 x 224 square area from the center of the image\ntorchvision.transforms.CenterCrop(224),\ntorchvision.transforms.ToTensor(),\ntorchvision.transforms.Normalize([0.485, 0.456, 0.406],\n[0.229, 0.224, 0.225])])\n14.14.3 Reading the Dataset\nAs in Section 14.13, we can read the organized dataset consisting of raw image ﬁles.\ntrain_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'train_valid_test', folder),\ntransform=transform_train) for folder in ['train', 'train_valid']]\nvalid_ds, test_ds = [torchvision.datasets.ImageFolder(\nos.path.join(data_dir, 'train_valid_test', folder),\ntransform=transform_test) for folder in ['valid', 'test']]\nBelow we create data iterator instances the same way as in Section 14.13.\ntrain_iter, train_valid_iter = [torch.utils.data.DataLoader(\ndataset, batch_size, shuffle=True, drop_last=True)\nfor dataset in (train_ds, train_valid_ds)]\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\ndrop_last=True)\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\ndrop_last=False)\n14.14.4 Fine-Tuning a Pretrained Model\nAgain, the dataset for this competition is a subset of the ImageNet dataset. Therefore, we can\nuse the approach discussed in Section 14.2 to select a model pretrained on the full ImageNet\ndataset and use it to extract image features to be fed into a custom small-scale output network.\nHigh-level APIs of deep learning frameworks provide a wide range of models pretrained on\nthe ImageNet dataset. Here, we choose a pretrained ResNet-34 model, where we simply reuse\nthe input of this model’s output layer (i.e., the extracted features). Then we can replace the\noriginal output layer with a small custom output network that can be trained, such as stacking\ntwo fully connected layers. Diﬀerent from the experiment in Section 14.2, the following does\nnot retrain the pretrained model used for feature extraction. This reduces training time and\nmemory for storing gradients.\nRecall that we standardized images using the means and standard deviations of the three RGB\n\n712\nComputer Vision\nchannels for the full ImageNet dataset. In fact, this is also consistent with the standardization\noperation by the pretrained model on ImageNet.\ndef get_net(devices):\nfinetune_net = nn.Sequential()\nfinetune_net.features = torchvision.models.resnet34(pretrained=True)\n# Define a new output network (there are 120 output categories)\nfinetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\nnn.ReLU(),\nnn.Linear(256, 120))\n# Move the model to devices\nfinetune_net = finetune_net.to(devices[0])\n# Freeze parameters of feature layers\nfor param in finetune_net.features.parameters():\nparam.requires_grad = False\nreturn finetune_net\nBefore calculating the loss, we ﬁrst obtain the input of the pretrained model’s output layer,\ni.e., the extracted feature. Then we use this feature as input for our small custom output\nnetwork to calculate the loss.\nloss = nn.CrossEntropyLoss(reduction='none')\ndef evaluate_loss(data_iter, net, devices):\nl_sum, n = 0.0, 0\nfor features, labels in data_iter:\nfeatures, labels = features.to(devices[0]), labels.to(devices[0])\noutputs = net(features)\nl = loss(outputs, labels)\nl_sum += l.sum()\nn += labels.numel()\nreturn l_sum / n\n14.14.5 Deﬁning the Training Function\nWe will select the model and tune hyperparameters according to the model’s performance on\nthe validation set. The model training function train only iterates parameters of the small\ncustom output network.\ndef train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\n# Only train the small custom output network\nnet = nn.DataParallel(net, device_ids=devices).to(devices[0])\ntrainer = torch.optim.SGD((param for param in net.parameters()\nif param.requires_grad), lr=lr,\nmomentum=0.9, weight_decay=wd)\nscheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer = len(train_iter), d2l.Timer()\nlegend = ['train loss']\nif valid_iter is not None:\n(continues on next page)\n\n713\nDog Breed Identiﬁcation (ImageNet Dogs) on Kaggle\n(continued from previous page)\nlegend.append('valid loss')\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\nlegend=legend)\nfor epoch in range(num_epochs):\nmetric = d2l.Accumulator(2)\nfor i, (features, labels) in enumerate(train_iter):\ntimer.start()\nfeatures, labels = features.to(devices[0]), labels.to(devices[0])\ntrainer.zero_grad()\noutput = net(features)\nl = loss(output, labels).sum()\nl.backward()\ntrainer.step()\nmetric.add(l, labels.shape[0])\ntimer.stop()\nif (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\nanimator.add(epoch + (i + 1) / num_batches,\n(metric[0] / metric[1], None))\nmeasures = f'train loss {metric[0] / metric[1]:.3f}'\nif valid_iter is not None:\nvalid_loss = evaluate_loss(valid_iter, net, devices)\nanimator.add(epoch + 1, (None, valid_loss.detach().cpu()))\nscheduler.step()\nif valid_iter is not None:\nmeasures += f', valid loss {valid_loss:.3f}'\nprint(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\nf' examples/sec on {str(devices)}')\n14.14.6 Training and Validating the Model\nNow we can train and validate the model. The following hyperparameters are all tunable. For\nexample, the number of epochs can be increased. Because lr_period and lr_decay are set\nto 2 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied\nby 0.9 after every 2 epochs.\ndevices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 1e-4, 1e-4\nlr_period, lr_decay, net = 2, 0.9, get_net(devices)\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 1.230, valid loss 1.313\n390.8 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n14.14.7 Classifying the Testing Set and Submitting Results on Kaggle\n\n714\nComputer Vision\nSimilar to the ﬁnal step in Section 14.13, in the end all the labeled data (including the val-\nidation set) are used for training the model and classifying the testing set. We will use the\ntrained custom output network for classiﬁcation.\nnet = get_net(devices)\ntrain(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\npreds = []\nfor data, label in test_iter:\noutput = torch.nn.functional.softmax(net(data.to(devices[0])), dim=1)\npreds.extend(output.cpu().detach().numpy())\nids = sorted(os.listdir(\nos.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\nwith open('submission.csv', 'w') as f:\nf.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\nfor i, output in zip(ids, preds):\nf.write(i.split('.')[0] + ',' + ','.join(\n[str(num) for num in output]) + '\\n')\ntrain loss 1.151\n582.9 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n\n715\nDog Breed Identiﬁcation (ImageNet Dogs) on Kaggle\n226\nThe above code will generate a submission.csv ﬁle to be submitted to Kaggle in the same\nway described in Section 5.7.\n14.14.8 Summary\n• Images in the ImageNet dataset are larger (with varying dimensions) than CIFAR-10 im-\nages. We may modify image augmentation operations for tasks on a diﬀerent dataset.\n• To classify a subset of the ImageNet dataset, we can leverage pre-trained models on the full\nImageNet dataset to extract features and only train a custom small-scale output network.\nThis will lead to less computational time and memory cost.\n14.14.9 Exercises\n1. When using the full Kaggle competition dataset, what results can you achieve when you in-\ncrease batch_size (batch size) and num_epochs (number of epochs) while setting some\nother hyperparameters as lr = 0.01, lr_period = 10, and lr_decay = 0.1?\n2. Do you get better results if you use a deeper pretrained model? How do you tune hyper-\nparameters? Can you further improve the results?\nDiscussions226.\n\n15\nNatural Language Processing: Pretraining\nHumans need to communicate. Out of this basic need of the human condition, a vast amount\nof written text has been generated on an everyday basis. Given rich text in social media, chat\napps, emails, product reviews, news articles, research papers, and books, it becomes vital to\nenable computers to understand them to oﬀer assistance or make decisions based on human\nlanguages.\nNatural language processing studies interactions between computers and humans using natural\nlanguages. In practice, it is very common to use natural language processing techniques to\nprocess and analyze text (human natural language) data, such as language models in Section\n9.3 and machine translation models in Section 10.5.\nTo understand text, we can begin by learning its representations. Leveraging the existing text\nsequences from large corpora, self-supervised learning has been extensively used to pretrain\ntext representations, such as by predicting some hidden part of the text using some other part\nof their surrounding text. In this way, models learn through supervision from massive text\ndata without expensive labeling eﬀorts!\nAs we will see in this chapter, when treating each word or subword as an individual token,\nthe representation of each token can be pretrained using word2vec, GloVe, or subword em-\nbedding models on large corpora. After pretraining, representation of each token can be a\nvector, however, it remains the same no matter what the context is. For instance, the vector\nrepresentation of “bank” is the same in both “go to the bank to deposit some money” and “go\nto the bank to sit down”. Thus, many more recent pretraining models adapt representation of\nthe same token to diﬀerent contexts. Among them is BERT, a much deeper self-supervised\nmodel based on the Transformer encoder. In this chapter, we will focus on how to pretrain\nsuch representations for text, as highlighted in Fig. 15.1.\nFor sight of the big picture, Fig. 15.1 shows that the pretrained text representations can be\nfed to a variety of deep learning architectures for diﬀerent downstream natural language\nprocessing applications. We will cover them in Chapter 16.\n716\n\n717\nWord Embedding (word2vec)\nt\nFig. 15.1\nPretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nthe upstream text representation pretraining.\n15.1 Word Embedding (word2vec)\nNatural language is a complex system used to express meanings. In this system, words are\nthe basic unit of the meaning. As the name implies, word vectors are vectors used to repre-\nsent words, and can also be considered as feature vectors or representations of words. The\ntechnique of mapping words to real vectors is called word embedding. In recent years, word\nembedding has gradually become the basic knowledge of natural language processing.\n15.1.1 One-Hot Vectors Are a Bad Choice\nWe used one-hot vectors to represent words (characters are words) in Section 9.5. Suppose\nthat the number of diﬀerent words in the dictionary (the dictionary size) is N, and each word\ncorresponds to a diﬀerent integer (index) from 0 to N −1. To obtain the one-hot vector\nrepresentation for any word with index i, we create a length-N vector with all 0s and set the\nelement at position i to 1. In this way, each word is represented as a vector of length N, and\nit can be used directly by neural networks.\nAlthough one-hot word vectors are easy to construct, they are usually not a good choice. A\nmain reason is that one-hot word vectors cannot accurately express the similarity between\ndiﬀerent words, such as the cosine similarity that we often use. For vectors x, y ∈Rd, their\ncosine similarity is the cosine of the angle between them:\nx⊤y\n∥x∥∥y∥∈[−1, 1].\n(15.1.1)\nSince the cosine similarity between one-hot vectors of any two diﬀerent words is 0, one-hot\nvectors cannot encode similarities among words.\n\n718\nNatural Language Processing: Pretraining\n227\n15.1.2 Self-Supervised word2vec\nThe word2vec227 tool was proposed to address the above issue. It maps each word to a ﬁxed-\nlength vector, and these vectors can better express the similarity and analogy relationship\namong diﬀerent words. The word2vec tool contains two models, namely skip-gram (Mikolov\net al., 2013) and continuous bag of words (CBOW) (Mikolov et al., 2013). For semanti-\ncally meaningful representations, their training relies on conditional probabilities that can be\nviewed as predicting some words using some of their surrounding words in corpora. Since\nsupervision comes from the data without labels, both skip-gram and continuous bag of words\nare self-supervised models.\nIn the following, we will introduce these two models and their training methods.\n15.1.3 The Skip-Gram Model\nThe skip-gram model assumes that a word can be used to generate its surrounding words in\na text sequence. Take the text sequence “the”, “man”, “loves”, “his”, “son” as an example.\nLet’s choose “loves” as the center word and set the context window size to 2. As shown in\nFig. 15.1.1, given the center word “loves”, the skip-gram model considers the conditional\nprobability for generating the context words: “the”, “man”, “his”, and “son”, which are no\nmore than 2 words away from the center word:\nP(\"the\", \"man\", \"his\", \"son\" | \"loves\").\n(15.1.2)\nAssume that the context words are independently generated given the center word (i.e.,\nconditional independence). In this case, the above conditional probability can be rewritten\nas\nP(\"the\" | \"loves\") · P(\"man\" | \"loves\") · P(\"his\" | \"loves\") · P(\"son\" | \"loves\").\n(15.1.3)\nt\nFig. 15.1.1\nThe skip-gram model considers the conditional probability of generating the surrounding\ncontext words given a center word.\nIn the skip-gram model, each word has two d-dimensional-vector representations for calcu-\nlating conditional probabilities. More concretely, for any word with index i in the dictionary,\ndenote by vi ∈Rd and ui ∈Rd its two vectors when used as a center word and a context\n\n719\nWord Embedding (word2vec)\nword, respectively. The conditional probability of generating any context word wo (with in-\ndex o in the dictionary) given the center word wc (with index c in the dictionary) can be\nmodeled by a softmax operation on vector dot products:\nP(wo | wc) =\nexp(u⊤\novc)\n∑\ni∈V exp(u⊤\ni vc),\n(15.1.4)\nwhere the vocabulary index set V = {0, 1, . . ., |V| −1}. Given a text sequence of length\nT, where the word at time step t is denoted as w(t). Assume that context words are indepen-\ndently generated given any center word. For context window size m, the likelihood function\nof the skip-gram model is the probability of generating all context words given any center\nword:\nT\n∏\nt=1\n∏\n−m≤j ≤m, j,0\nP(w(t+j) | w(t)),\n(15.1.5)\nwhere any time step that is less than 1 or greater than T can be omitted.\nTraining\nThe skip-gram model parameters are the center word vector and context word vector for\neach word in the vocabulary. In training, we learn the model parameters by maximizing the\nlikelihood function (i.e., maximum likelihood estimation). This is equivalent to minimizing\nthe following loss function:\n−\nT\n∑\nt=1\n∑\n−m≤j ≤m, j,0\nlog P(w(t+j) | w(t)).\n(15.1.6)\nWhen using stochastic gradient descent to minimize the loss, in each iteration we can ran-\ndomly sample a shorter subsequence to calculate the (stochastic) gradient for this subsequence\nto update the model parameters. To calculate this (stochastic) gradient, we need to obtain the\ngradients of the log conditional probability with respect to the center word vector and the con-\ntext word vector. In general, according to (15.1.4) the log conditional probability involving\nany pair of the center word wc and the context word wo is\nlog P(wo | wc) = u⊤\novc −log\n(∑\ni∈V\nexp(u⊤\ni vc)\n)\n.\n(15.1.7)\nThrough diﬀerentiation, we can obtain its gradient with respect to the center word vector vc\nas\n∂log P(wo | wc)\n∂vc\n= uo −\n∑\nj∈V exp(u⊤\nj vc)uj\n∑\ni∈V exp(u⊤\ni vc)\n= uo −\n∑\nj∈V\n(\nexp(u⊤\nj vc)\n∑\ni∈V exp(u⊤\ni vc)\n)\nuj\n= uo −\n∑\nj∈V\nP(wj | wc)uj.\n(15.1.8)\n\n720\nNatural Language Processing: Pretraining\nNote that the calculation in (15.1.8) requires the conditional probabilities of all words in\nthe dictionary with wc as the center word. The gradients for the other word vectors can be\nobtained in the same way.\nAfter training, for any word with index i in the dictionary, we obtain both word vectors vi (as\nthe center word) and ui (as the context word). In natural language processing applications,\nthe center word vectors of the skip-gram model are typically used as the word representa-\ntions.\n15.1.4 The Continuous Bag of Words (CBOW) Model\nThe continuous bag of words (CBOW) model is similar to the skip-gram model. The major\ndiﬀerence from the skip-gram model is that the continuous bag of words model assumes that\na center word is generated based on its surrounding context words in the text sequence. For\nexample, in the same text sequence “the”, “man”, “loves”, “his”, and “son”, with “loves” as\nthe center word and the context window size being 2, the continuous bag of words model con-\nsiders the conditional probability of generating the center word “loves” based on the context\nwords “the”, “man”, “his” and “son” (as shown in Fig. 15.1.2), which is\nP(\"loves\" | \"the\", \"man\", \"his\", \"son\").\n(15.1.9)\nt\nFig. 15.1.2\nThe continuous bag of words model considers the conditional probability of generating\nthe center word given its surrounding context words.\nSince there are multiple context words in the continuous bag of words model, these context\nword vectors are averaged in the calculation of the conditional probability. Speciﬁcally, for\nany word with index i in the dictionary, denote by vi ∈Rd and ui ∈Rd its two vectors when\nused as a context word and a center word (meanings are switched in the skip-gram model),\nrespectively. The conditional probability of generating any center word wc (with index c in\nthe dictionary) given its surrounding context words wo1, . . ., wo2m (with index o1, . . ., o2m\nin the dictionary) can be modeled by\nP(wc | wo1, . . ., wo2m) =\nexp ( 1\n2mu⊤\nc (vo1 + . . . + vo2m))\n∑\ni∈V exp ( 1\n2mu⊤\ni (vo1 + . . . + vo2m)) .\n(15.1.10)\nFor brevity, let Wo = {wo1, . . ., wo2m} and ¯vo = (vo1 + . . . + vo2m) /(2m). Then (15.1.10)\n\n721\nWord Embedding (word2vec)\ncan be simpliﬁed as\nP(wc | Wo) =\nexp (u⊤\nc ¯vo)\n∑\ni∈V exp (u⊤\ni ¯vo\n) .\n(15.1.11)\nGiven a text sequence of length T, where the word at time step t is denoted as w(t). For\ncontext window size m, the likelihood function of the continuous bag of words model is the\nprobability of generating all center words given their context words:\nT\n∏\nt=1\nP(w(t) | w(t−m), . . ., w(t−1), w(t+1), . . ., w(t+m)).\n(15.1.12)\nTraining\nTraining continuous bag of words models is almost the same as training skip-gram models.\nThe maximum likelihood estimation of the continuous bag of words model is equivalent to\nminimizing the following loss function:\n−\nT\n∑\nt=1\nlog P(w(t) | w(t−m), . . ., w(t−1), w(t+1), . . ., w(t+m)).\n(15.1.13)\nNotice that\nlog P(wc | Wo) = u⊤\nc ¯vo −log\n(∑\ni∈V\nexp (u⊤\ni ¯vo\n)\n)\n.\n(15.1.14)\nThrough diﬀerentiation, we can obtain its gradient with respect to any context word vector\nvoi(i = 1, . . ., 2m) as\n∂log P(wc | Wo)\n∂voi\n= 1\n2m\n©­\n«\nuc −\n∑\nj∈V\nexp(u⊤\nj ¯vo)uj\n∑\ni∈V exp(u⊤\ni ¯vo)\nª®\n¬\n= 1\n2m\n©­\n«\nuc −\n∑\nj∈V\nP(wj | Wo)ujª®\n¬\n.\n(15.1.15)\nThe gradients for the other word vectors can be obtained in the same way. Unlike the skip-\ngram model, the continuous bag of words model typically uses context word vectors as the\nword representations.\n15.1.5 Summary\n• Word vectors are vectors used to represent words, and can also be considered as feature\nvectors or representations of words. The technique of mapping words to real vectors is\ncalled word embedding.\n• The word2vec tool contains both the skip-gram and continuous bag of words models.\n• The skip-gram model assumes that a word can be used to generate its surrounding words\nin a text sequence; while the continuous bag of words model assumes that a center word\nis generated based on its surrounding context words.\n\n722\nNatural Language Processing: Pretraining\n228\n15.1.6 Exercises\n1. What is the computational complexity for calculating each gradient? What could be the\nissue if the dictionary size is huge?\n2. Some ﬁxed phrases in English consist of multiple words, such as “new york”. How to train\ntheir word vectors? Hint: see Section 4 in the word2vec paper (Mikolov et al., 2013).\n3. Let’s reﬂect on the word2vec design by taking the skip-gram model as an example. What\nis the relationship between the dot product of two word vectors in the skip-gram model\nand the cosine similarity? For a pair of words with similar semantics, why may the cosine\nsimilarity of their word vectors (trained by the skip-gram model) be high?\nDiscussions228.\n15.2 Approximate Training\nRecall our discussions in Section 15.1. The main idea of the skip-gram model is using softmax\noperations to calculate the conditional probability of generating a context word wo based on\nthe given center word wc in (15.1.4), whose corresponding logarithmic loss is given by the\nopposite of (15.1.7).\nDue to the nature of the softmax operation, since a context word may be anyone in the dictio-\nnary V, the opposite of (15.1.7) contains the summation of items as many as the entire size of\nthe vocabulary. Consequently, the gradient calculation for the skip-gram model in (15.1.8)\nand that for the continuous bag-of-words model in (15.1.15) both contain the summation.\nUnfortunately, the computational cost for such gradients that sum over a large dictionary\n(often with hundreds of thousands or millions of words) is huge!\nIn order to reduce the aforementioned computational complexity, this section will introduce\ntwo approximate training methods: negative sampling and hierarchical softmax. Due to the\nsimilarity between the skip-gram model and the continuous bag of words model, we will\njust take the skip-gram model as an example to describe these two approximate training\nmethods.\n15.2.1 Negative Sampling\nNegative sampling modiﬁes the original objective function. Given the context window of\na center word wc, the fact that any (context) word wo comes from this context window is\nconsidered as an event with the probability modeled by\nP(D = 1 | wc, wo) = σ(u⊤\novc),\n(15.2.1)\n\n723\nApproximate Training\nwhere σ uses the deﬁnition of the sigmoid activation function:\nσ(x) =\n1\n1 + exp(−x).\n(15.2.2)\nLet’s begin by maximizing the joint probability of all such events in text sequences to train\nword embeddings. Speciﬁcally, given a text sequence of length T, denote by w(t) the word\nat time step t and let the context window size be m, consider maximizing the joint probabil-\nity\nT\n∏\nt=1\n∏\n−m≤j ≤m, j,0\nP(D = 1 | w(t), w(t+j)).\n(15.2.3)\nHowever, (15.2.3) only considers those events that involve positive examples. As a result, the\njoint probability in (15.2.3) is maximized to 1 only if all the word vectors are equal to inﬁnity.\nOf course, such results are meaningless. To make the objective function more meaningful,\nnegative sampling adds negative examples sampled from a predeﬁned distribution.\nDenote by S the event that a context word wo comes from the context window of a center word\nwc. For this event involving wo, from a predeﬁned distribution P(w) sample K noise words\nthat are not from this context window. Denote by Nk the event that a noise word wk (k =\n1, . . ., K) does not come from the context window of wc. Assume that these events involving\nboth the positive example and negative examples S, N1, . . ., NK are mutually independent.\nNegative sampling rewrites the joint probability (involving only positive examples) in (15.2.3)\nas\nT\n∏\nt=1\n∏\n−m≤j ≤m, j,0\nP(w(t+j) | w(t)),\n(15.2.4)\nwhere the conditional probability is approximated through events S, N1, . . ., NK:\nP(w(t+j) | w(t)) = P(D = 1 | w(t), w(t+j))\nK\n∏\nk=1, wk∼P(w)\nP(D = 0 | w(t), wk). (15.2.5)\nDenote by it and hk the indices of a word w(t) at time step t of a text sequence and a noise\nword wk, respectively. The logarithmic loss with respect to the conditional probabilities in\n(15.2.5) is\n−log P(w(t+j) | w(t)) = −log P(D = 1 | w(t), w(t+j)) −\nK\n∑\nk=1, wk∼P(w)\nlog P(D = 0 | w(t), wk)\n= −log σ\n(\nu⊤\nit+j vit\n)\n−\nK\n∑\nk=1, wk∼P(w)\nlog\n(\n1 −σ\n(\nu⊤\nhk vit\n))\n= −log σ\n(\nu⊤\nit+j vit\n)\n−\nK\n∑\nk=1, wk∼P(w)\nlog σ\n(\n−u⊤\nhk vit\n)\n.\n(15.2.6)\nWe can see that now the computational cost for gradients at each training step has nothing\nto do with the dictionary size, but linearly depends on K. When setting the hyperparameter\n\n724\nNatural Language Processing: Pretraining\nK to a smaller value, the computational cost for gradients at each training step with negative\nsampling is smaller.\n15.2.2 Hierarchical Softmax\nAs an alternative approximate training method, hierarchical softmax uses the binary tree, a\ndata structure illustrated in Fig. 15.2.1, where each leaf node of the tree represents a word in\ndictionary V.\nt\nFig. 15.2.1\nHierarchical softmax for approximate training, where each leaf node of the tree represents\na word in the dictionary.\nDenote by L(w) the number of nodes (including both ends) on the path from the root node\nto the leaf node representing word w in the binary tree. Let n(w, j) be the jth node on this\npath, with its context word vector being un(w, j). For example, L(w3) = 4 in Fig. 15.2.1.\nHierarchical softmax approximates the conditional probability in (15.1.4) as\nP(wo | wc) =\nL(wo)−1\n∏\nj=1\nσ\n(\n[[n(wo, j + 1) = leftChild(n(wo, j))]] · u⊤\nn(wo,j)vc\n)\n,\n(15.2.7)\nwhere function σ is deﬁned in (15.2.2), and leftChild(n) is the left child node of node n: if\nx is true, [[x]] = 1; otherwise [[x]] = −1.\nTo illustrate, let’s calculate the conditional probability of generating word w3 given word wc\nin Fig. 15.2.1. This requires dot products between the word vector vc of wc and non-leaf node\nvectors on the path (the path in bold in Fig. 15.2.1) from the root to w3, which is traversed\nleft, right, then left:\nP(w3 | wc) = σ(u⊤\nn(w3,1)vc) · σ(−u⊤\nn(w3,2)vc) · σ(u⊤\nn(w3,3)vc).\n(15.2.8)\nSince σ(x) + σ(−x) = 1, it holds that the conditional probabilities of generating all the\nwords in dictionary V based on any word wc sum up to one:\n∑\nw∈V\nP(w | wc) = 1.\n(15.2.9)\n\n725\nThe Dataset for Pretraining Word Embeddings\n229\nFortunately, since L(wo) −1 is on the order of O(log2|V|) due to the binary tree struc-\nture, when the dictionary size V is huge, the computational cost for each training step using\nhierarchical softmax is signiﬁcantly reduced compared with that without approximate train-\ning.\n15.2.3 Summary\n• Negative sampling constructs the loss function by considering mutually independent events\nthat involve both positive and negative examples. The computational cost for training is\nlinearly dependent on the number of noise words at each step.\n• Hierarchical softmax constructs the loss function using the path from the root node to the\nleaf node in the binary tree. The computational cost for training is dependent on the\nlogarithm of the dictionary size at each step.\n15.2.4 Exercises\n1. How can we sample noise words in negative sampling?\n2. Verify that (15.2.9) holds.\n3. How to train the continuous bag of words model using negative sampling and hierarchical\nsoftmax, respectively?\nDiscussions229.\n15.3 The Dataset for Pretraining Word Embeddings\nNow that we know the technical details of the word2vec models and approximate training\nmethods, let’s walk through their implementations. Speciﬁcally, we will take the skip-gram\nmodel in Section 15.1 and negative sampling in Section 15.2 as an example. In this section,\nwe begin with the dataset for pretraining the word embedding model: the original format of\nthe data will be transformed into minibatches that can be iterated over during training.\nimport collections\nimport math\nimport os\nimport random\nimport torch\nfrom d2l import torch as d2l\n\n726\nNatural Language Processing: Pretraining\n230\n15.3.1 Reading the Dataset\nThe dataset that we use here is Penn Tree Bank (PTB)230. This corpus is sampled from Wall\nStreet Journal articles, split into training, validation, and test sets. In the original format, each\nline of the text ﬁle represents a sentence of words that are separated by spaces. Here we treat\neach word as a token.\n#@save\nd2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n'319d85e578af0cdc590547f26231e4e31cdf1e42')\n#@save\ndef read_ptb():\n\"\"\"Load the PTB dataset into a list of text lines.\"\"\"\ndata_dir = d2l.download_extract('ptb')\n# Read the training set\nwith open(os.path.join(data_dir, 'ptb.train.txt')) as f:\nraw_text = f.read()\nreturn [line.split() for line in raw_text.split('\\n')]\nsentences = read_ptb()\nf'# sentences: {len(sentences)}'\n'# sentences: 42069'\nAfter reading the training set, we build a vocabulary for the corpus, where any word that\nappears less than 10 times is replaced by the “<unk>” token. Note that the original dataset\nalso contains “<unk>” tokens that represent rare (unknown) words.\nvocab = d2l.Vocab(sentences, min_freq=10)\nf'vocab size: {len(vocab)}'\n'vocab size: 6719'\n15.3.2 Subsampling\nText data typically have high-frequency words such as “the”, “a”, and “in”: they may even\noccur billions of times in very large corpora. However, these words often co-occur with many\ndiﬀerent words in context windows, providing little useful signals. For instance, consider the\nword “chip” in a context window: intuitively its co-occurrence with a low-frequency word\n“intel” is more useful in training than the co-occurrence with a high-frequency word “a”.\nMoreover, training with vast amounts of (high-frequency) words is slow. Thus, when training\nword embedding models, high-frequency words can be subsampled (Mikolov et al., 2013).\nSpeciﬁcally, each indexed word wi in the dataset will be discarded with probability\nP(wi) = max\n(\n1 −\n√\nt\nf (wi), 0\n)\n,\n(15.3.1)\n\n727\nThe Dataset for Pretraining Word Embeddings\nwhere f (wi) is the ratio of the number of words wi to the total number of words in the\ndataset, and the constant t is a hyperparameter (10−4 in the experiment). We can see that\nonly when the relative frequency f (wi) > t can the (high-frequency) word wi be discarded,\nand the higher the relative frequency of the word, the greater the probability of being dis-\ncarded.\n#@save\ndef subsample(sentences, vocab):\n\"\"\"Subsample high-frequency words.\"\"\"\n# Exclude unknown tokens ('<unk>')\nsentences = [[token for token in line if vocab[token] != vocab.unk]\nfor line in sentences]\ncounter = collections.Counter([\ntoken for line in sentences for token in line])\nnum_tokens = sum(counter.values())\n# Return True if `token` is kept during subsampling\ndef keep(token):\nreturn(random.uniform(0, 1) <\nmath.sqrt(1e-4 / counter[token] * num_tokens))\nreturn ([[token for token in line if keep(token)] for line in sentences],\ncounter)\nsubsampled, counter = subsample(sentences, vocab)\nThe following code snippet plots the histogram of the number of tokens per sentence before\nand after subsampling. As expected, subsampling signiﬁcantly shortens sentences by dropping\nhigh-frequency words, which will lead to training speedup.\nd2l.show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence',\n'count', sentences, subsampled);\nFor individual tokens, the sampling rate of the high-frequency word “the” is less than 1/20.\ndef compare_counts(token):\nreturn (f'# of \"{token}\": '\n(continues on next page)\n\n728\nNatural Language Processing: Pretraining\n(continued from previous page)\nf'before={sum([l.count(token) for l in sentences])}, '\nf'after={sum([l.count(token) for l in subsampled])}')\ncompare_counts('the')\n'# of \"the\": before=50770, after=1992'\nIn contrast, low-frequency words “join” are completely kept.\ncompare_counts('join')\n'# of \"join\": before=45, after=45'\nAfter subsampling, we map tokens to their indices for the corpus.\ncorpus = [vocab[line] for line in subsampled]\ncorpus[:3]\n[[], [4127, 3228, 710, 1773, 4060], [3895, 3922, 1922, 4743]]\n15.3.3 Extracting Center Words and Context Words\nThe following get_centers_and_contexts function extracts all the center words and their\ncontext words from corpus. It uniformly samples an integer between 1 and max_window_size\nat random as the context window size. For any center word, those words whose distance from\nit does not exceed the sampled context window size are its context words.\n#@save\ndef get_centers_and_contexts(corpus, max_window_size):\n\"\"\"Return center words and context words in skip-gram.\"\"\"\ncenters, contexts = [], []\nfor line in corpus:\n# To form a \"center word--context word\" pair, each sentence needs to\n# have at least 2 words\nif len(line) < 2:\ncontinue\ncenters += line\nfor i in range(len(line)):\n# Context window centered at `i`\nwindow_size = random.randint(1, max_window_size)\nindices = list(range(max(0, i - window_size),\nmin(len(line), i + 1 + window_size)))\n# Exclude the center word from the context words\nindices.remove(i)\ncontexts.append([line[idx] for idx in indices])\nreturn centers, contexts\n\n729\nThe Dataset for Pretraining Word Embeddings\nNext, we create an artiﬁcial dataset containing two sentences of 7 and 3 words, respectively.\nLet the maximum context window size be 2 and print all the center words and their context\nwords.\ntiny_dataset = [list(range(7)), list(range(7, 10))]\nprint('dataset', tiny_dataset)\nfor center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\nprint('center', center, 'has contexts', context)\ndataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\ncenter 0 has contexts [1]\ncenter 1 has contexts [0, 2]\ncenter 2 has contexts [0, 1, 3, 4]\ncenter 3 has contexts [1, 2, 4, 5]\ncenter 4 has contexts [3, 5]\ncenter 5 has contexts [3, 4, 6]\ncenter 6 has contexts [5]\ncenter 7 has contexts [8, 9]\ncenter 8 has contexts [7, 9]\ncenter 9 has contexts [7, 8]\nWhen training on the PTB dataset, we set the maximum context window size to 5. The fol-\nlowing extracts all the center words and their context words in the dataset.\nall_centers, all_contexts = get_centers_and_contexts(corpus, 5)\nf'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'\n'# center-context pairs: 1503184'\n15.3.4 Negative Sampling\nWe use negative sampling for approximate training. To sample noise words according to a\npredeﬁned distribution, we deﬁne the following RandomGenerator class, where the (possibly\nunnormalized) sampling distribution is passed via the argument sampling_weights.\n#@save\nclass RandomGenerator:\n\"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\ndef __init__(self, sampling_weights):\n# Exclude\nself.population = list(range(1, len(sampling_weights) + 1))\nself.sampling_weights = sampling_weights\nself.candidates = []\nself.i = 0\ndef draw(self):\nif self.i == len(self.candidates):\n# Cache `k` random sampling results\n(continues on next page)\n\n730\nNatural Language Processing: Pretraining\n(continued from previous page)\nself.candidates = random.choices(\nself.population, self.sampling_weights, k=10000)\nself.i = 0\nself.i += 1\nreturn self.candidates[self.i - 1]\nFor example, we can draw 10 random variables X among indices 1, 2, and 3 with sampling\nprobabilities P(X = 1) = 2/9, P(X = 2) = 3/9, and P(X = 3) = 4/9 as follows.\nFor a pair of center word and context word, we randomly sample K (5 in the experiment)\nnoise words. According to the suggestions in the word2vec paper, the sampling probability\nP(w) of a noise word w is set to its relative frequency in the dictionary raised to the power\nof 0.75 (Mikolov et al., 2013).\n#@save\ndef get_negatives(all_contexts, vocab, counter, K):\n\"\"\"Return noise words in negative sampling.\"\"\"\n# Sampling weights for words with indices 1, 2, ... (index 0 is the\n# excluded unknown token) in the vocabulary\nsampling_weights = [counter[vocab.to_tokens(i)]**0.75\nfor i in range(1, len(vocab))]\nall_negatives, generator = [], RandomGenerator(sampling_weights)\nfor contexts in all_contexts:\nnegatives = []\nwhile len(negatives) < len(contexts) * K:\nneg = generator.draw()\n# Noise words cannot be context words\nif neg not in contexts:\nnegatives.append(neg)\nall_negatives.append(negatives)\nreturn all_negatives\nall_negatives = get_negatives(all_contexts, vocab, counter, 5)\n15.3.5 Loading Training Examples in Minibatches\nAfter all the center words together with their context words and sampled noise words are\nextracted, they will be transformed into minibatches of examples that can be iteratively loaded\nduring training.\nIn a minibatch, the ith example includes a center word and its ni context words and mi noise\nwords. Due to varying context window sizes, ni + mi varies for diﬀerent i. Thus, for each\nexample we concatenate its context words and noise words in the contexts_negatives vari-\nable, and pad zeros until the concatenation length reaches maxi ni + mi (max_len). To ex-\nclude paddings in the calculation of the loss, we deﬁne a mask variable masks. There is a one-\nto-one correspondence between elements in masks and elements in contexts_negatives,\nwhere zeros (otherwise ones) in masks correspond to paddings in contexts_negatives.\nTo distinguish between positive and negative examples, we separate context words from noise\n\n731\nThe Dataset for Pretraining Word Embeddings\nwords in contexts_negatives via a labels variable. Similar to masks, there is also a one-\nto-one correspondence between elements in labels and elements in contexts_negatives,\nwhere ones (otherwise zeros) in labels correspond to context words (positive examples) in\ncontexts_negatives.\nThe above idea is implemented in the following batchify function. Its input data is a list\nwith length equal to the batch size, where each element is an example consisting of the center\nword center, its context words context, and its noise words negative. This function returns\na minibatch that can be loaded for calculations during training, such as including the mask\nvariable.\n#@save\ndef batchify(data):\n\"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\nmax_len = max(len(c) + len(n) for _, c, n in data)\ncenters, contexts_negatives, masks, labels = [], [], [], []\nfor center, context, negative in data:\ncur_len = len(context) + len(negative)\ncenters += [center]\ncontexts_negatives += [context + negative + [0] * (max_len - cur_len)]\nmasks += [[1] * cur_len + [0] * (max_len - cur_len)]\nlabels += [[1] * len(context) + [0] * (max_len - len(context))]\nreturn (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\ncontexts_negatives), torch.tensor(masks), torch.tensor(labels))\nLet’s test this function using a minibatch of two examples.\nx_1 = (1, [2, 2], [3, 3, 3, 3])\nx_2 = (1, [2, 2, 2], [3, 3])\nbatch = batchify((x_1, x_2))\nnames = ['centers', 'contexts_negatives', 'masks', 'labels']\nfor name, data in zip(names, batch):\nprint(name, '=', data)\ncenters = tensor([[1],\n[1]])\ncontexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n[2, 2, 2, 3, 3, 0]])\nmasks = tensor([[1, 1, 1, 1, 1, 1],\n[1, 1, 1, 1, 1, 0]])\nlabels = tensor([[1, 1, 0, 0, 0, 0],\n[1, 1, 1, 0, 0, 0]])\n15.3.6 Putting It All Together\nLast, we deﬁne the load_data_ptb function that reads the PTB dataset and returns the data\niterator and the vocabulary.\n\n732\nNatural Language Processing: Pretraining\n#@save\ndef load_data_ptb(batch_size, max_window_size, num_noise_words):\n\"\"\"Download the PTB dataset and then load it into memory.\"\"\"\nnum_workers = d2l.get_dataloader_workers()\nsentences = read_ptb()\nvocab = d2l.Vocab(sentences, min_freq=10)\nsubsampled, counter = subsample(sentences, vocab)\ncorpus = [vocab[line] for line in subsampled]\nall_centers, all_contexts = get_centers_and_contexts(\ncorpus, max_window_size)\nall_negatives = get_negatives(\nall_contexts, vocab, counter, num_noise_words)\nclass PTBDataset(torch.utils.data.Dataset):\ndef __init__(self, centers, contexts, negatives):\nassert len(centers) == len(contexts) == len(negatives)\nself.centers = centers\nself.contexts = contexts\nself.negatives = negatives\ndef __getitem__(self, index):\nreturn (self.centers[index], self.contexts[index],\nself.negatives[index])\ndef __len__(self):\nreturn len(self.centers)\ndataset = PTBDataset(all_centers, all_contexts, all_negatives)\ndata_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\ncollate_fn=batchify,\nnum_workers=num_workers)\nreturn data_iter, vocab\nLet’s print the ﬁrst minibatch of the data iterator.\ndata_iter, vocab = load_data_ptb(512, 5, 5)\nfor batch in data_iter:\nfor name, data in zip(names, batch):\nprint(name, 'shape:', data.shape)\nbreak\ncenters shape: torch.Size([512, 1])\ncontexts_negatives shape: torch.Size([512, 60])\nmasks shape: torch.Size([512, 60])\nlabels shape: torch.Size([512, 60])\n15.3.7 Summary\n• High-frequency words may not be so useful in training. We can subsample them for speedup\nin training.\n\n733\nPretraining word2vec\n231\n• For computational eﬃciency, we load examples in minibatches. We can deﬁne other vari-\nables to distinguish paddings from non-paddings, and positive examples from negative\nones.\n15.3.8 Exercises\n1. How does the running time of code in this section changes if not using subsampling?\n2. The RandomGenerator class caches k random sampling results. Set k to other values and\nsee how it aﬀects the data loading speed.\n3. What other hyperparameters in the code of this section may aﬀect the data loading speed?\nDiscussions231.\n15.4 Pretraining word2vec\nWe go on to implement the skip-gram model deﬁned in Section 15.1. Then we will pretrain\nword2vec using negative sampling on the PTB dataset. First of all, let’s obtain the data iterator\nand the vocabulary for this dataset by calling the d2l.load_data_ptb function, which was\ndescribed in Section 15.3\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nbatch_size, max_window_size, num_noise_words = 512, 5, 5\ndata_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\nnum_noise_words)\n15.4.1 The Skip-Gram Model\nWe implement the skip-gram model by using embedding layers and batch matrix multipli-\ncations. First, let’s review how embedding layers work.\nEmbedding Layer\nAs described in Section 10.7, an embedding layer maps a token’s index to its feature vec-\ntor. The weight of this layer is a matrix whose number of rows equals to the dictionary\nsize (input_dim) and number of columns equals to the vector dimension for each token\n(output_dim). After a word embedding model is trained, this weight is what we need.\n\n734\nNatural Language Processing: Pretraining\nembed = nn.Embedding(num_embeddings=20, embedding_dim=4)\nprint(f'Parameter embedding_weight ({embed.weight.shape}, '\nf'dtype={embed.weight.dtype})')\nParameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\nThe input of an embedding layer is the index of a token (word). For any token index i, its\nvector representation can be obtained from the ith row of the weight matrix in the embedding\nlayer. Since the vector dimension (output_dim) was set to 4, the embedding layer returns\nvectors with shape (2, 3, 4) for a minibatch of token indices with shape (2, 3).\nx = torch.tensor([[1, 2, 3], [4, 5, 6]])\nembed(x)\ntensor([[[ 1.4289,\n0.6862,\n1.3745, -0.0402],\n[ 0.0612, -0.4935,\n0.7515,\n1.2910],\n[ 1.2522,\n0.1610, -1.4299, -0.7257]],\n[[ 0.1162,\n0.2630, -2.0102, -0.9138],\n[-0.9011,\n0.4052,\n0.4618,\n0.5271],\n[-1.8482,\n0.5460,\n0.0158, -0.2720]]], grad_fn=<EmbeddingBackward0>)\nDeﬁning the Forward Propagation\nIn the forward propagation, the input of the skip-gram model includes the center word in-\ndices center of shape (batch size, 1) and the concatenated context and noise word indices\ncontexts_and_negatives of shape (batch size, max_len), where max_len is deﬁned in\nSection 15.3.5. These two variables are ﬁrst transformed from the token indices into vectors\nvia the embedding layer, then their batch matrix multiplication (described in Section 11.3.2)\nreturns an output of shape (batch size, 1, max_len). Each element in the output is the dot\nproduct of a center word vector and a context or noise word vector.\ndef skip_gram(center, contexts_and_negatives, embed_v, embed_u):\nv = embed_v(center)\nu = embed_u(contexts_and_negatives)\npred = torch.bmm(v, u.permute(0, 2, 1))\nreturn pred\nLet’s print the output shape of this skip_gram function for some example inputs.\nskip_gram(torch.ones((2, 1), dtype=torch.long),\ntorch.ones((2, 4), dtype=torch.long), embed, embed).shape\n\n735\nPretraining word2vec\ntorch.Size([2, 1, 4])\n15.4.2 Training\nBefore training the skip-gram model with negative sampling, let’s ﬁrst deﬁne its loss func-\ntion.\nBinary Cross-Entropy Loss\nAccording to the deﬁnition of the loss function for negative sampling in Section 15.2.1, we\nwill use the binary cross-entropy loss.\nclass SigmoidBCELoss(nn.Module):\n# Binary cross-entropy loss with masking\ndef __init__(self):\nsuper().__init__()\ndef forward(self, inputs, target, mask=None):\nout = nn.functional.binary_cross_entropy_with_logits(\ninputs, target, weight=mask, reduction=\"none\")\nreturn out.mean(dim=1)\nloss = SigmoidBCELoss()\nRecall our descriptions of the mask variable and the label variable in Section 15.3.5. The\nfollowing calculates the binary cross-entropy loss for the given variables.\npred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\nlabel = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\nmask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\nloss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)\ntensor([0.9352, 1.8462])\nBelow shows how the above results are calculated (in a less eﬃcient way) using the sigmoid\nactivation function in the binary cross-entropy loss. We can consider the two outputs as two\nnormalized losses that are averaged over non-masked predictions.\ndef sigmd(x):\nreturn -math.log(1 / (1 + math.exp(-x)))\nprint(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\nprint(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')\n\n736\nNatural Language Processing: Pretraining\n0.9352\n1.8462\nInitializing Model Parameters\nWe deﬁne two embedding layers for all the words in the vocabulary when they are used as\ncenter words and context words, respectively. The word vector dimension embed_size is set\nto 100.\nembed_size = 100\nnet = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),\nembedding_dim=embed_size),\nnn.Embedding(num_embeddings=len(vocab),\nembedding_dim=embed_size))\nDeﬁning the Training Loop\nThe training loop is deﬁned below. Because of the existence of padding, the calculation of\nthe loss function is slightly diﬀerent compared to the previous training functions.\ndef train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\ndef init_weights(module):\nif type(module) == nn.Embedding:\nnn.init.xavier_uniform_(module.weight)\nnet.apply(init_weights)\nnet = net.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[1, num_epochs])\n# Sum of normalized losses, no. of normalized losses\nmetric = d2l.Accumulator(2)\nfor epoch in range(num_epochs):\ntimer, num_batches = d2l.Timer(), len(data_iter)\nfor i, batch in enumerate(data_iter):\noptimizer.zero_grad()\ncenter, context_negative, mask, label = [\ndata.to(device) for data in batch]\npred = skip_gram(center, context_negative, net[0], net[1])\nl = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n/ mask.sum(axis=1) * mask.shape[1])\nl.sum().backward()\noptimizer.step()\nmetric.add(l.sum(), l.numel())\nif (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\nanimator.add(epoch + (i + 1) / num_batches,\n(metric[0] / metric[1],))\n(continues on next page)\n\n737\nPretraining word2vec\n(continued from previous page)\nprint(f'loss {metric[0] / metric[1]:.3f}, '\nf'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')\nNow we can train a skip-gram model using negative sampling.\nlr, num_epochs = 0.002, 5\ntrain(net, data_iter, lr, num_epochs)\nloss 0.410, 190209.2 tokens/sec on cuda:0\n15.4.3 Applying Word Embeddings\nAfter training the word2vec model, we can use the cosine similarity of word vectors from\nthe trained model to ﬁnd words from the dictionary that are most semantically similar to an\ninput word.\ndef get_similar_tokens(query_token, k, embed):\nW = embed.weight.data\nx = W[vocab[query_token]]\n# Compute the cosine similarity. Add 1e-9 for numerical stability\ncos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\ntorch.sum(x * x) + 1e-9)\ntopk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\nfor i in topk[1:]:\n# Remove the input words\nprint(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\nget_similar_tokens('chip', 3, net[0])\ncosine sim=0.784: microprocessor\ncosine sim=0.743: intel\ncosine sim=0.653: chips\n\n738\nNatural Language Processing: Pretraining\n232\n15.4.4 Summary\n• We can train a skip-gram model with negative sampling using embedding layers and the\nbinary cross-entropy loss.\n• Applications of word embeddings include ﬁnding semantically similar words for a given\nword based on the cosine similarity of word vectors.\n15.4.5 Exercises\n1. Using the trained model, ﬁnd semantically similar words for other input words. Can you\nimprove the results by tuning hyperparameters?\n2. When a training corpus is huge, we often sample context words and noise words for the\ncenter words in the current minibatch when updating model parameters. In other words,\nthe same center word may have diﬀerent context words or noise words in diﬀerent training\nepochs. What are the beneﬁts of this method? Try to implement this training method.\nDiscussions232.\n15.5 Word Embedding with Global Vectors (GloVe)\nWord-word co-occurrences within context windows may carry rich semantic information. For\nexample, in a large corpus word “solid” is more likely to co-occur with “ice” than “steam”,\nbut word “gas” probably co-occurs with “steam” more frequently than “ice”. Besides, global\ncorpus statistics of such co-occurrences can be precomputed: this can lead to more eﬃcient\ntraining. To leverage statistical information in the entire corpus for word embedding, let’s ﬁrst\nrevisit the skip-gram model in Section 15.1.3, but interpreting it using global corpus statistics\nsuch as co-occurrence counts.\n15.5.1 Skip-Gram with Global Corpus Statistics\nDenoting by qij the conditional probability P(wj | wi) of word wj given word wi in the\nskip-gram model, we have\nqij =\nexp(u⊤\nj vi)\n∑\nk∈V exp(u⊤\nk vi),\n(15.5.1)\nwhere for any index i vectors vi and ui represent word wi as the center word and context\nword, respectively, and V = {0, 1, . . ., |V| −1} is the index set of the vocabulary.\nConsider word wi that may occur multiple times in the corpus. In the entire corpus, all the\n\n739\nWord Embedding with Global Vectors (GloVe)\ncontext words wherever wi is taken as their center word form a multiset Ci of word indices that\nallows for multiple instances of the same element. For any element, its number of instances\nis called its multiplicity. To illustrate with an example, suppose that word wi occurs twice\nin the corpus and indices of the context words that take wi as their center word in the two\ncontext windows are k, j, m, k and k, l, k, j. Thus, multiset Ci = {j, j, k, k, k, k, l, m}, where\nmultiplicities of elements j, k, l, m are 2, 4, 1, 1, respectively.\nNow let’s denote the multiplicity of element j in multiset Ci as xij. This is the global co-\noccurrence count of word wj (as the context word) and word wi (as the center word) in\nthe same context window in the entire corpus. Using such global corpus statistics, the loss\nfunction of the skip-gram model is equivalent to\n−\n∑\ni∈V\n∑\nj∈V\nxij log qij.\n(15.5.2)\nWe further denote by xi the number of all the context words in the context windows where wi\noccurs as their center word, which is equivalent to |Ci|. Letting pij be the conditional proba-\nbility xij/xi for generating context word wj given center word wi, (15.5.2) can be rewritten\nas\n−\n∑\ni∈V\nxi\n∑\nj∈V\npij log qij.\n(15.5.3)\nIn (15.5.3), −∑\nj∈V pij log qij calculates the cross-entropy of the conditional distribution\npij of global corpus statistics and the conditional distribution qij of model predictions. This\nloss is also weighted by xi as explained above. Minimizing the loss function in (15.5.3) will\nallow the predicted conditional distribution to get close to the conditional distribution from\nthe global corpus statistics.\nThough being commonly used for measuring the distance between probability distributions,\nthe cross-entropy loss function may not be a good choice here. On the one hand, as we men-\ntioned in Section 15.2, the cost of properly normalizing qij results in the sum over the entire\nvocabulary, which can be computationally expensive. On the other hand, a large number of\nrare events from a large corpus are often modeled by the cross-entropy loss to be assigned\nwith too much weight.\n15.5.2 The GloVe Model\nIn view of this, the GloVe model makes three changes to the skip-gram model based on\nsquared loss (Pennington et al., 2014):\n1. Use variables p′\nij = xij and q′\nij = exp(u⊤\nj vi) that are not probability distributions and\ntake the logarithm of both, so the squared loss term is\n(\nlog p′\nij −log q′\nij\n)2\n=\n(\nu⊤\nj vi −log xij\n)2\n.\n2. Add two scalar model parameters for each word wi: the center word bias bi and the context\nword bias ci.\n3. Replace the weight of each loss term with the weight function h(xij), where h(x) is in-\ncreasing in the interval of [0, 1].\n\n740\nNatural Language Processing: Pretraining\nPutting all things together, training GloVe is to minimize the following loss function:\n∑\ni∈V\n∑\nj∈V\nh(xij)\n(\nu⊤\nj vi + bi + cj −log xij\n)2\n.\n(15.5.4)\nFor the weight function, a suggested choice is: h(x) = (x/c)α (e.g α = 0.75) if x < c (e.g.,\nc = 100); otherwise h(x) = 1. In this case, because h(0) = 0, the squared loss term for\nany xij = 0 can be omitted for computational eﬃciency. For example, when using minibatch\nstochastic gradient descent for training, at each iteration we randomly sample a minibatch of\nnon-zero xij to calculate gradients and update the model parameters. Note that these non-\nzero xij are precomputed global corpus statistics; thus, the model is called GloVe for Global\nVectors.\nIt should be emphasized that if word wi appears in the context window of word wj, then vice\nversa. Therefore, xij = xji. Unlike word2vec that ﬁts the asymmetric conditional probability\npij, GloVe ﬁts the symmetric log xij. Therefore, the center word vector and the context word\nvector of any word are mathematically equivalent in the GloVe model. However in practice,\nowing to diﬀerent initialization values, the same word may still get diﬀerent values in these\ntwo vectors after training: GloVe sums them up as the output vector.\n15.5.3 Interpreting GloVe from the Ratio of Co-occurrence\nProbabilities\nWe can also interpret the GloVe model from another perspective. Using the same notation in\nSection 15.5.1, let pij\ndef\n= P(wj | wi) be the conditional probability of generating the context\nword wj given wi as the center word in the corpus. Table 15.5.1 lists several co-occurrence\nprobabilities given words “ice” and “steam” and their ratios based on statistics from a large\ncorpus.\nTable 15.5.1: Word-word co-occurrence probabilities and their ratios from a large corpus\n(adapted from Table 1 in Pennington et al. (2014))\nwk=\nsolid\ngas\nwater\nfashion\np1 = P(wk | ice)\n0.00019\n0.000066\n0.003\n0.000017\np2 = P(wk | steam)\n0.000022\n0.00078\n0.0022\n0.000018\np1/p2\n8.9\n0.085\n1.36\n0.96\nWe can observe the following from Table 15.5.1:\n• For a word wk that is related to “ice” but unrelated to “steam”, such as wk = solid, we\nexpect a larger ratio of co-occurence probabilities, such as 8.9.\n• For a word wk that is related to “steam” but unrelated to “ice”, such as wk = gas, we\nexpect a smaller ratio of co-occurence probabilities, such as 0.085.\n• For a word wk that is related to both “ice” and “steam”, such as wk = water, we expect a\nratio of co-occurence probabilities that is close to 1, such as 1.36.\n\n741\nWord Embedding with Global Vectors (GloVe)\n• For a word wk that is unrelated to both “ice” and “steam”, such as wk = fashion, we expect\na ratio of co-occurence probabilities that is close to 1, such as 0.96.\nIt can be seen that the ratio of co-occurrence probabilities can intuitively express the rela-\ntionship between words. Thus, we can design a function of three word vectors to ﬁt this ratio.\nFor the ratio of co-occurrence probabilities pij/pik with wi being the center word and wj\nand wk being the context words, we want to ﬁt this ratio using some function f :\nf (uj, uk, vi) ≈pij\npik\n.\n(15.5.5)\nAmong many possible designs for f , we only pick a reasonable choice in the following. Since\nthe ratio of co-occurrence probabilities is a scalar, we require that f be a scalar function,\nsuch as f (uj, uk, vi) = f ((uj −uk)⊤vi\n). Switching word indices j and k in (15.5.5), it\nmust hold that f (x) f (−x) = 1, so one possibility is f (x) = exp(x), i.e.,\nf (uj, uk, vi) =\nexp\n(\nu⊤\nj vi\n)\nexp\n(\nu⊤\nk vi\n) ≈pij\npik\n.\n(15.5.6)\nNow let’s pick exp\n(\nu⊤\nj vi\n)\n≈αpij, where α is a constant. Since pij = xij/xi, after taking\nthe logarithm on both sides we get u⊤\nj vi ≈log α + log xij −log xi. We may use additional\nbias terms to ﬁt −log α + log xi, such as the center word bias bi and the context word bias\ncj:\nu⊤\nj vi + bi + cj ≈log xij.\n(15.5.7)\nMeasuring the squared error of (15.5.7) with weights, the GloVe loss function in (15.5.4) is\nobtained.\n15.5.4 Summary\n• The skip-gram model can be interpreted using global corpus statistics such as word-word\nco-occurrence counts.\n• The cross-entropy loss may not be a good choice for measuring the diﬀerence of two\nprobability distributions, especially for a large corpus. GloVe uses squared loss to ﬁt\nprecomputed global corpus statistics.\n• The center word vector and the context word vector are mathematically equivalent for any\nword in GloVe.\n• GloVe can be interpreted from the ratio of word-word co-occurrence probabilities.\n15.5.5 Exercises\n1. If words wi and wj co-occur in the same context window, how can we use their distance\nin the text sequence to redesign the method for calculating the conditional probability pij?\nHint: see Section 4.2 of the GloVe paper (Pennington et al., 2014).\n\n742\nNatural Language Processing: Pretraining\n233\n2. For any word, are its center word bias and context word bias mathematically equivalent in\nGloVe? Why?\nDiscussions233.\n15.6 Subword Embedding\nIn English, words such as “helps”, “helped”, and “helping” are inﬂected forms of the same\nword “help”. The relationship between “dog” and “dogs” is the same as that between “cat” and\n“cats”, and the relationship between “boy” and “boyfriend” is the same as that between “girl”\nand “girlfriend”. In other languages such as French and Spanish, many verbs have over 40\ninﬂected forms, while in Finnish, a noun may have up to 15 cases. In linguistics, morphology\nstudies word formation and word relationships. However, the internal structure of words was\nneither explored in word2vec nor in GloVe.\n15.6.1 The fastText Model\nRecall how words are represented in word2vec. In both the skip-gram model and the continu-\nous bag-of-words model, diﬀerent inﬂected forms of the same word are directly represented\nby diﬀerent vectors without shared parameters. To use morphological information, the fast-\nText model proposed a subword embedding approach, where a subword is a character n-gram\n(Bojanowski et al., 2017). Instead of learning word-level vector representations, fastText can\nbe considered as the subword-level skip-gram, where each center word is represented by the\nsum of its subword vectors.\nLet’s illustrate how to obtain subwords for each center word in fastText using the word\n“where”. First, add special characters “<” and “>” at the beginning and end of the word\nto distinguish preﬁxes and suﬃxes from other subwords. Then, extract character n-grams\nfrom the word. For example, when n = 3, we obtain all subwords of length 3: “<wh”, “whe”,\n“her”, “ere”, “re>”, and the special subword “<where>”.\nIn fastText, for any word w, denote by Gw the union of all its subwords of length between\n3 and 6 and its special subword. The vocabulary is the union of the subwords of all words.\nLetting zg be the vector of subword g in the dictionary, the vector vw for word w as a center\nword in the skip-gram model is the sum of its subword vectors:\nvw =\n∑\ng∈Gw\nzg.\n(15.6.1)\nThe rest of fastText is the same as the skip-gram model. Compared with the skip-gram\nmodel, the vocabulary in fastText is larger, resulting in more model parameters. Besides,\nto calculate the representation of a word, all its subword vectors have to be summed, leading\nto higher computational complexity. However, thanks to shared parameters from subwords\n\n743\nSubword Embedding\namong words with similar structures, rare words and even out-of-vocabulary words may ob-\ntain better vector representations in fastText.\n15.6.2 Byte Pair Encoding\nIn fastText, all the extracted subwords have to be of the speciﬁed lengths, such as 3 to 6, thus\nthe vocabulary size cannot be predeﬁned. To allow for variable-length subwords in a ﬁxed-\nsize vocabulary, we can apply a compression algorithm called byte pair encoding (BPE) to\nextract subwords (Sennrich et al., 2015).\nByte pair encoding performs a statistical analysis of the training dataset to discover common\nsymbols within a word, such as consecutive characters of arbitrary length. Starting from sym-\nbols of length 1, byte pair encoding iteratively merges the most frequent pair of consecutive\nsymbols to produce new longer symbols. Note that for eﬃciency, pairs crossing word bound-\naries are not considered. In the end, we can use such symbols as subwords to segment words.\nByte pair encoding and its variants has been used for input representations in popular natural\nlanguage processing pretraining models such as GPT-2 (Radford et al., 2019) and RoBERTa\n(Liu et al., 2019). In the following, we will illustrate how byte pair encoding works.\nFirst, we initialize the vocabulary of symbols as all the English lowercase characters, a special\nend-of-word symbol '_', and a special unknown symbol '[UNK]'.\nimport collections\nsymbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n'_', '[UNK]']\nSince we do not consider symbol pairs that cross boundaries of words, we only need a dictio-\nnary raw_token_freqs that maps words to their frequencies (number of occurrences) in a\ndataset. Note that the special symbol '_' is appended to each word so that we can easily re-\ncover a word sequence (e.g., “a taller man”) from a sequence of output symbols ( e.g., “a_ tall\ner_ man”). Since we start the merging process from a vocabulary of only single characters and\nspecial symbols, space is inserted between every pair of consecutive characters within each\nword (keys of the dictionary token_freqs). In other words, space is the delimiter between\nsymbols within a word.\nraw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\ntoken_freqs = {}\nfor token, freq in raw_token_freqs.items():\ntoken_freqs[' '.join(list(token))] = raw_token_freqs[token]\ntoken_freqs\n{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\nWe deﬁne the following get_max_freq_pair function that returns the most frequent pair\n\n744\nNatural Language Processing: Pretraining\nof consecutive symbols within a word, where words come from keys of the input dictionary\ntoken_freqs.\ndef get_max_freq_pair(token_freqs):\npairs = collections.defaultdict(int)\nfor token, freq in token_freqs.items():\nsymbols = token.split()\nfor i in range(len(symbols) - 1):\n# Key of `pairs` is a tuple of two consecutive symbols\npairs[symbols[i], symbols[i + 1]] += freq\nreturn max(pairs, key=pairs.get)\n# Key of `pairs` with the max value\nAs a greedy approach based on frequency of consecutive symbols, byte pair encoding will\nuse the following merge_symbols function to merge the most frequent pair of consecutive\nsymbols to produce new symbols.\ndef merge_symbols(max_freq_pair, token_freqs, symbols):\nsymbols.append(''.join(max_freq_pair))\nnew_token_freqs = dict()\nfor token, freq in token_freqs.items():\nnew_token = token.replace(' '.join(max_freq_pair),\n''.join(max_freq_pair))\nnew_token_freqs[new_token] = token_freqs[token]\nreturn new_token_freqs\nNow we iteratively perform the byte pair encoding algorithm over the keys of the dictionary\ntoken_freqs. In the ﬁrst iteration, the most frequent pair of consecutive symbols are 't'\nand 'a', thus byte pair encoding merges them to produce a new symbol 'ta'. In the second\niteration, byte pair encoding continues to merge 'ta' and 'l' to result in another new symbol\n'tal'.\nnum_merges = 10\nfor i in range(num_merges):\nmax_freq_pair = get_max_freq_pair(token_freqs)\ntoken_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\nprint(f'merge #{i + 1}:', max_freq_pair)\nmerge #1: ('t', 'a')\nmerge #2: ('ta', 'l')\nmerge #3: ('tal', 'l')\nmerge #4: ('f', 'a')\nmerge #5: ('fa', 's')\nmerge #6: ('fas', 't')\nmerge #7: ('e', 'r')\nmerge #8: ('er', '_')\nmerge #9: ('tall', '_')\nmerge #10: ('fast', '_')\nAfter 10 iterations of byte pair encoding, we can see that list symbols now contains 10 more\nsymbols that are iteratively merged from other symbols.\n\n745\nSubword Embedding\nprint(symbols)\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p\n,→', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal\n,→', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\nFor the same dataset speciﬁed in the keys of the dictionary raw_token_freqs, each word\nin the dataset is now segmented by subwords “fast_”, “fast”, “er_”, “tall_”, and “tall” as a\nresult of the byte pair encoding algorithm. For instance, words “faster_” and “taller_” are\nsegmented as “fast er_” and “tall er_”, respectively.\nprint(list(token_freqs.keys()))\n['fast_', 'fast er_', 'tall_', 'tall er_']\nNote that the result of byte pair encoding depends on the dataset being used. We can also\nuse the subwords learned from one dataset to segment words of another dataset. As a greedy\napproach, the following segment_BPE function tries to break words into the longest possible\nsubwords from the input argument symbols.\ndef segment_BPE(tokens, symbols):\noutputs = []\nfor token in tokens:\nstart, end = 0, len(token)\ncur_output = []\n# Segment token with the longest possible subwords from symbols\nwhile start < len(token) and start < end:\nif token[start: end] in symbols:\ncur_output.append(token[start: end])\nstart = end\nend = len(token)\nelse:\nend -= 1\nif start < len(token):\ncur_output.append('[UNK]')\noutputs.append(' '.join(cur_output))\nreturn outputs\nIn the following, we use the subwords in list symbols, which is learned from the aforemen-\ntioned dataset, to segment tokens that represent another dataset.\ntokens = ['tallest_', 'fatter_']\nprint(segment_BPE(tokens, symbols))\n['tall e s t _', 'fa t t er_']\n\n746\nNatural Language Processing: Pretraining\n234\n15.6.3 Summary\n• The fastText model proposes a subword embedding approach. Based on the skip-gram\nmodel in word2vec, it represents a center word as the sum of its subword vectors.\n• Byte pair encoding performs a statistical analysis of the training dataset to discover com-\nmon symbols within a word. As a greedy approach, byte pair encoding iteratively merges\nthe most frequent pair of consecutive symbols.\n• Subword embedding may improve the quality of representations of rare words and out-of-\ndictionary words.\n15.6.4 Exercises\n1. As an example, there are about 3 × 108 possible 6-grams in English. What is the issue\nwhen there are too many subwords? How to address the issue? Hint: refer to the end of\nSection 3.2 of the fastText paper (Bojanowski et al., 2017).\n2. How to design a subword embedding model based on the continuous bag-of-words model?\n3. To get a vocabulary of size m, how many merging operations are needed when the initial\nsymbol vocabulary size is n?\n4. How to extend the idea of byte pair encoding to extract phrases?\nDiscussions234.\n15.7 Word Similarity and Analogy\nIn Section 15.4, we trained a word2vec model on a small dataset, and applied it to ﬁnd se-\nmantically similar words for an input word. In practice, word vectors that are pretrained on\nlarge corpora can be applied to downstream natural language processing tasks, which will\nbe covered later in Chapter 16. To demonstrate semantics of pretrained word vectors from\nlarge corpora in a straightforward way, let’s apply them in the word similarity and analogy\ntasks.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n747\nWord Similarity and Analogy\n235\n236\n15.7.1 Loading Pretrained Word Vectors\nBelow lists pretrained GloVe embeddings of dimension 50, 100, and 300, which can be\ndownloaded from the GloVe website 235 . The pretrained fastText embeddings are available\nin multiple languages. Here we consider one English version (300-dimensional “wiki.en”)\nthat can be downloaded from the fastText website236.\n#@save\nd2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n'0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n#@save\nd2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n#@save\nd2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n#@save\nd2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n'c1816da3821ae9f43899be655002f6c723e91b88')\nTo load these pretrained GloVe and fastText embeddings, we deﬁne the following TokenEm-\nbedding class.\n#@save\nclass TokenEmbedding:\n\"\"\"Token Embedding.\"\"\"\ndef __init__(self, embedding_name):\nself.idx_to_token, self.idx_to_vec = self._load_embedding(\nembedding_name)\nself.unknown_idx = 0\nself.token_to_idx = {token: idx for idx, token in\nenumerate(self.idx_to_token)}\ndef _load_embedding(self, embedding_name):\nidx_to_token, idx_to_vec = ['<unk>'], []\ndata_dir = d2l.download_extract(embedding_name)\n# GloVe website: https://nlp.stanford.edu/projects/glove/\n# fastText website: https://fasttext.cc/\nwith open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\nfor line in f:\nelems = line.rstrip().split(' ')\ntoken, elems = elems[0], [float(elem) for elem in elems[1:]]\n# Skip header information, such as the top row in fastText\nif len(elems) > 1:\nidx_to_token.append(token)\nidx_to_vec.append(elems)\nidx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\nreturn idx_to_token, torch.tensor(idx_to_vec)\ndef __getitem__(self, tokens):\n(continues on next page)\n\n748\nNatural Language Processing: Pretraining\n(continued from previous page)\nindices = [self.token_to_idx.get(token, self.unknown_idx)\nfor token in tokens]\nvecs = self.idx_to_vec[torch.tensor(indices)]\nreturn vecs\ndef __len__(self):\nreturn len(self.idx_to_token)\nBelow we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia subset).\nWhen creating the TokenEmbedding instance, the speciﬁed embedding ﬁle has to be down-\nloaded if it was not yet.\nglove_6b50d = TokenEmbedding('glove.6b.50d')\nOutput the vocabulary size. The vocabulary contains 400000 words (tokens) and a special\nunknown token.\nlen(glove_6b50d)\n400001\nWe can get the index of a word in the vocabulary, and vice versa.\nglove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]\n(3367, 'beautiful')\n15.7.2 Applying Pretrained Word Vectors\nUsing the loaded GloVe vectors, we will demonstrate their semantics by applying them in the\nfollowing word similarity and analogy tasks.\nWord Similarity\nSimilar to Section 15.4.3, in order to ﬁnd semantically similar words for an input word based\non cosine similarities between word vectors, we implement the following knn (k-nearest\nneighbors) function.\ndef knn(W, x, k):\n# Add 1e-9 for numerical stability\ncos = torch.mv(W, x.reshape(-1,)) / (\n(continues on next page)\n\n749\nWord Similarity and Analogy\n(continued from previous page)\ntorch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\ntorch.sqrt((x * x).sum()))\n_, topk = torch.topk(cos, k=k)\nreturn topk, [cos[int(i)] for i in topk]\nThen, we search for similar words using the pretrained word vectors from the TokenEmbed-\nding instance embed.\ndef get_similar_tokens(query_token, k, embed):\ntopk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\nfor i, c in zip(topk[1:], cos[1:]):\n# Exclude the input word\nprint(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')\nThe vocabulary of the pretrained word vectors in glove_6b50d contains 400000 words and a\nspecial unknown token. Excluding the input word and unknown token, among this vocabulary\nlet’s ﬁnd three most semantically similar words to word “chip”.\nget_similar_tokens('chip', 3, glove_6b50d)\ncosine sim=0.856: chips\ncosine sim=0.749: intel\ncosine sim=0.749: electronics\nBelow outputs similar words to “baby” and “beautiful”.\nget_similar_tokens('baby', 3, glove_6b50d)\ncosine sim=0.839: babies\ncosine sim=0.800: boy\ncosine sim=0.792: girl\nget_similar_tokens('beautiful', 3, glove_6b50d)\ncosine sim=0.921: lovely\ncosine sim=0.893: gorgeous\ncosine sim=0.830: wonderful\nWord Analogy\nBesides ﬁnding similar words, we can also apply word vectors to word analogy tasks. For ex-\nample, “man”:“woman”::“son”:“daughter” is the form of a word analogy: “man” is to “woman”\nas “son” is to “daughter”. Speciﬁcally, the word analogy completion task can be deﬁned as:\nfor a word analogy a : b :: c : d, given the ﬁrst three words a, b and c, ﬁnd d. Denote the\n\n750\nNatural Language Processing: Pretraining\nvector of word w by vec(w). To complete the analogy, we will ﬁnd the word whose vector is\nmost similar to the result of vec(c) + vec(b) −vec(a).\ndef get_analogy(token_a, token_b, token_c, embed):\nvecs = embed[[token_a, token_b, token_c]]\nx = vecs[1] - vecs[0] + vecs[2]\ntopk, cos = knn(embed.idx_to_vec, x, 1)\nreturn embed.idx_to_token[int(topk[0])]\n# Remove unknown words\nLet’s verify the “male-female” analogy using the loaded word vectors.\nget_analogy('man', 'woman', 'son', glove_6b50d)\n'daughter'\nBelow completes a “capital-country” analogy: “beijing”:“china”::“tokyo”:“japan”. This demon-\nstrates semantics in the pretrained word vectors.\nget_analogy('beijing', 'china', 'tokyo', glove_6b50d)\n'japan'\nFor the “adjective-superlative adjective” analogy such as “bad”:“worst”::“big”:“biggest”, we\ncan see that the pretrained word vectors may capture the syntactic information.\nget_analogy('bad', 'worst', 'big', glove_6b50d)\n'biggest'\nTo show the captured notion of past tense in the pretrained word vectors, we can test the\nsyntax using the “present tense-past tense” analogy: “do”:“did”::“go”:“went”.\nget_analogy('do', 'did', 'go', glove_6b50d)\n'went'\n15.7.3 Summary\n• In practice, word vectors that are pretrained on large corpora can be applied to downstream\nnatural language processing tasks.\n• Pretrained word vectors can be applied to the word similarity and analogy tasks.\n\n751\nBidirectional Encoder Representations from Transformers (BERT)\n237\n15.7.4 Exercises\n1. Test the fastText results using TokenEmbedding('wiki.en').\n2. When the vocabulary is extremely large, how can we ﬁnd similar words or complete a\nword analogy faster?\nDiscussions237.\n15.8 Bidirectional Encoder Representations from\nTransformers (BERT)\nWe have introduced several word embedding models for natural language understanding.\nAfter pretraining, the output can be thought of as a matrix where each row is a vector that\nrepresents a word of a predeﬁned vocabulary. In fact, these word embedding models are all\ncontext-independent. Let’s begin by illustrating this property.\n15.8.1 From Context-Independent to Context-Sensitive\nRecall the experiments in Section 15.4 and Section 15.7. For instance, word2vec and GloVe\nboth assign the same pretrained vector to the same word regardless of the context of the word\n(if any). Formally, a context-independent representation of any token x is a function f (x)\nthat only takes x as its input. Given the abundance of polysemy and complex semantics in nat-\nural languages, context-independent representations have obvious limitations. For instance,\nthe word “crane” in contexts “a crane is ﬂying” and “a crane driver came” has completely dif-\nferent meanings; thus, the same word may be assigned diﬀerent representations depending\non contexts.\nThis motivates the development of context-sensitive word representations, where representa-\ntions of words depend on their contexts. Hence, a context-sensitive representation of token x\nis a function f (x, c(x)) depending on both x and its context c(x). Popular context-sensitive\nrepresentations include TagLM (language-model-augmented sequence tagger) (Peters et al.,\n2017), CoVe (Context Vectors) (McCann et al., 2017), and ELMo (Embeddings from Lan-\nguage Models) (Peters et al., 2018).\nFor example, by taking the entire sequence as input, ELMo is a function that assigns a repre-\nsentation to each word from the input sequence. Speciﬁcally, ELMo combines all the inter-\nmediate layer representations from pretrained bidirectional LSTM as the output representa-\ntion. Then the ELMo representation will be added to a downstream task’s existing supervised\nmodel as additional features, such as by concatenating ELMo representation and the origi-\nnal representation (e.g., GloVe) of tokens in the existing model. On the one hand, all the\nweights in the pretrained bidirectional LSTM model are frozen after ELMo representations\n\n752\nNatural Language Processing: Pretraining\nare added. On the other hand, the existing supervised model is speciﬁcally customized for a\ngiven task. Leveraging diﬀerent best models for diﬀerent tasks at that time, adding ELMo\nimproved the state of the art across six natural language processing tasks: sentiment analy-\nsis, natural language inference, semantic role labeling, coreference resolution, named entity\nrecognition, and question answering.\n15.8.2 From Task-Speciﬁc to Task-Agnostic\nAlthough ELMo has signiﬁcantly improved solutions to a diverse set of natural language\nprocessing tasks, each solution still hinges on a task-speciﬁc architecture. However, it is prac-\ntically non-trivial to craft a speciﬁc architecture for every natural language processing task.\nThe GPT (Generative Pre-Training) model represents an eﬀort in designing a general task-\nagnostic model for context-sensitive representations (Radford et al., 2018). Built on a Trans-\nformer decoder, GPT pretrains a language model that will be used to represent text sequences.\nWhen applying GPT to a downstream task, the output of the language model will be fed into\nan added linear output layer to predict the label of the task. In sharp contrast to ELMo that\nfreezes parameters of the pretrained model, GPT ﬁne-tunes all the parameters in the pre-\ntrained Transformer decoder during supervised learning of the downstream task. GPT was\nevaluated on twelve tasks of natural language inference, question answering, sentence sim-\nilarity, and classiﬁcation, and improved the state of the art in nine of them with minimal\nchanges to the model architecture.\nHowever, due to the autoregressive nature of language models, GPT only looks forward (left-\nto-right). In contexts “i went to the bank to deposit cash” and “i went to the bank to sit down”,\nas “bank” is sensitive to the context to its left, GPT will return the same representation for\n“bank”, though it has diﬀerent meanings.\n15.8.3 BERT: Combining the Best of Both Worlds\nAs we have seen, ELMo encodes context bidirectionally but uses task-speciﬁc architectures;\nwhile GPT is task-agnostic but encodes context left-to-right. Combining the best of both\nworlds, BERT (Bidirectional Encoder Representations from Transformers) encodes context\nbidirectionally and requires minimal architecture changes for a wide range of natural language\nprocessing tasks (Devlin et al., 2018). Using a pretrained Transformer encoder, BERT is\nable to represent any token based on its bidirectional context. During supervised learning of\ndownstream tasks, BERT is similar to GPT in two aspects. First, BERT representations will\nbe fed into an added output layer, with minimal changes to the model architecture depending\non nature of tasks, such as predicting for every token vs. predicting for the entire sequence.\nSecond, all the parameters of the pretrained Transformer encoder are ﬁne-tuned, while the\nadditional output layer will be trained from scratch. Fig. 15.8.1 depicts the diﬀerences among\nELMo, GPT, and BERT.\nBERT further improved the state of the art on eleven natural language processing tasks un-\nder broad categories of (i) single text classiﬁcation (e.g., sentiment analysis), (ii) text pair\n\n753\nBidirectional Encoder Representations from Transformers (BERT)\nt\nFig. 15.8.1\nA comparison of ELMo, GPT, and BERT.\nclassiﬁcation (e.g., natural language inference), (iii) question answering, (iv) text tagging\n(e.g., named entity recognition). All proposed in 2018, from context-sensitive ELMo to task-\nagnostic GPT and BERT, conceptually simple yet empirically powerful pretraining of deep\nrepresentations for natural languages have revolutionized solutions to various natural lan-\nguage processing tasks.\nIn the rest of this chapter, we will dive into the pretraining of BERT. When natural language\nprocessing applications are explained in Chapter 16, we will illustrate ﬁne-tuning of BERT\nfor downstream applications.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n15.8.4 Input Representation\nIn natural language processing, some tasks (e.g., sentiment analysis) take single text as input,\nwhile in some other tasks (e.g., natural language inference), the input is a pair of text se-\nquences. The BERT input sequence unambiguously represents both single text and text pairs.\nIn the former, the BERT input sequence is the concatenation of the special classiﬁcation to-\nken “<cls>”, tokens of a text sequence, and the special separation token “<sep>”. In the latter,\nthe BERT input sequence is the concatenation of “<cls>”, tokens of the ﬁrst text sequence,\n“<sep>”, tokens of the second text sequence, and “<sep>”. We will consistently distinguish\nthe terminology “BERT input sequence” from other types of “sequences”. For instance, one\nBERT input sequence may include either one text sequence or two text sequences.\nTo distinguish text pairs, the learned segment embeddings eA and eB are added to the token\nembeddings of the ﬁrst sequence and the second sequence, respectively. For single text inputs,\nonly eA is used.\n\n754\nNatural Language Processing: Pretraining\nThe following get_tokens_and_segments takes either one sentence or two sentences as\ninput, then returns tokens of the BERT input sequence and their corresponding segment\nIDs.\n#@save\ndef get_tokens_and_segments(tokens_a, tokens_b=None):\n\"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\ntokens = ['<cls>'] + tokens_a + ['<sep>']\n# 0 and 1 are marking segment A and B, respectively\nsegments = [0] * (len(tokens_a) + 2)\nif tokens_b is not None:\ntokens += tokens_b + ['<sep>']\nsegments += [1] * (len(tokens_b) + 1)\nreturn tokens, segments\nBERT chooses the Transformer encoder as its bidirectional architecture. Common in the\nTransformer encoder, positional embeddings are added at every position of the BERT input\nsequence. However, diﬀerent from the original Transformer encoder, BERT uses learnable\npositional embeddings. To sum up, Fig. 15.8.2 shows that the embeddings of the BERT input\nsequence are the sum of the token embeddings, segment embeddings, and positional embed-\ndings.\nt\nFig. 15.8.2\nThe embeddings of the BERT input sequence are the sum of the token embeddings,\nsegment embeddings, and positional embeddings.\nThe following BERTEncoder class is similar to the TransformerEncoder class as imple-\nmented in Section 11.7. Diﬀerent from TransformerEncoder, BERTEncoder uses segment\nembeddings and learnable positional embeddings.\n#@save\nclass BERTEncoder(nn.Module):\n\"\"\"BERT encoder.\"\"\"\ndef __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout, max_len=1000, **kwargs):\nsuper(BERTEncoder, self).__init__(**kwargs)\nself.token_embedding = nn.Embedding(vocab_size, num_hiddens)\nself.segment_embedding = nn.Embedding(2, num_hiddens)\nself.blks = nn.Sequential()\nfor i in range(num_blks):\nself.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n(continues on next page)\n\n755\nBidirectional Encoder Representations from Transformers (BERT)\n(continued from previous page)\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n# In BERT, positional embeddings are learnable, thus we create a\n# parameter of positional embeddings that are long enough\nself.pos_embedding = nn.Parameter(torch.randn(1, max_len,\nnum_hiddens))\ndef forward(self, tokens, segments, valid_lens):\n# Shape of `X` remains unchanged in the following code snippet:\n# (batch size, max sequence length, `num_hiddens`)\nX = self.token_embedding(tokens) + self.segment_embedding(segments)\nX = X + self.pos_embedding[:, :X.shape[1], :]\nfor blk in self.blks:\nX = blk(X, valid_lens)\nreturn X\nSuppose that the vocabulary size is 10000. To demonstrate forward inference of BERTEn-\ncoder, let’s create an instance of it and initialize its parameters.\nvocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\nffn_num_input, num_blks, dropout = 768, 2, 0.2\nencoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nWe deﬁne tokens to be 2 BERT input sequences of length 8, where each token is an index\nof the vocabulary. The forward inference of BERTEncoder with the input tokens returns the\nencoded result where each token is represented by a vector whose length is predeﬁned by the\nhyperparameter num_hiddens. This hyperparameter is usually referred to as the hidden size\n(number of hidden units) of the Transformer encoder.\ntokens = torch.randint(0, vocab_size, (2, 8))\nsegments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\nencoded_X = encoder(tokens, segments, None)\nencoded_X.shape\ntorch.Size([2, 8, 768])\n15.8.5 Pretraining Tasks\nThe forward inference of BERTEncoder gives the BERT representation of each token of the\ninput text and the inserted special tokens “<cls>” and “<seq>”. Next, we will use these repre-\nsentations to compute the loss function for pretraining BERT. The pretraining is composed\nof the following two tasks: masked language modeling and next sentence prediction.\n\n756\nNatural Language Processing: Pretraining\nMasked Language Modeling\nAs illustrated in Section 9.3, a language model predicts a token using the context on its left. To\nencode context bidirectionally for representing each token, BERT randomly masks tokens and\nuses tokens from the bidirectional context to predict the masked tokens in a self-supervised\nfashion. This task is referred to as a masked language model.\nIn this pretraining task, 15% of tokens will be selected at random as the masked tokens for\nprediction. To predict a masked token without cheating by using the label, one straightforward\napproach is to always replace it with a special “<mask>” token in the BERT input sequence.\nHowever, the artiﬁcial special token “<mask>” will never appear in ﬁne-tuning. To avoid\nsuch a mismatch between pretraining and ﬁne-tuning, if a token is masked for prediction\n(e.g., “great” is selected to be masked and predicted in “this movie is great”), in the input it\nwill be replaced with:\n• a special “<mask>” token for 80% of the time (e.g., “this movie is great” becomes “this\nmovie is <mask>”);\n• a random token for 10% of the time (e.g., “this movie is great” becomes “this movie is\ndrink”);\n• the unchanged label token for 10% of the time (e.g., “this movie is great” becomes “this\nmovie is great”).\nNote that for 10% of 15% time a random token is inserted. This occasional noise encourages\nBERT to be less biased towards the masked token (especially when the label token remains\nunchanged) in its bidirectional context encoding.\nWe implement the following MaskLM class to predict masked tokens in the masked language\nmodel task of BERT pretraining. The prediction uses a one-hidden-layer MLP (self.mlp).\nIn forward inference, it takes two inputs: the encoded result of BERTEncoder and the token\npositions for prediction. The output is the prediction results at these positions.\n#@save\nclass MaskLM(nn.Module):\n\"\"\"The masked language model task of BERT.\"\"\"\ndef __init__(self, vocab_size, num_hiddens, **kwargs):\nsuper(MaskLM, self).__init__(**kwargs)\nself.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\nnn.ReLU(),\nnn.LayerNorm(num_hiddens),\nnn.LazyLinear(vocab_size))\ndef forward(self, X, pred_positions):\nnum_pred_positions = pred_positions.shape[1]\npred_positions = pred_positions.reshape(-1)\nbatch_size = X.shape[0]\nbatch_idx = torch.arange(0, batch_size)\n# Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n# `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\nbatch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n(continues on next page)\n\n757\nBidirectional Encoder Representations from Transformers (BERT)\n(continued from previous page)\nmasked_X = X[batch_idx, pred_positions]\nmasked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\nmlm_Y_hat = self.mlp(masked_X)\nreturn mlm_Y_hat\nTo demonstrate the forward inference of MaskLM, we create its instance mlm and initialize\nit. Recall that encoded_X from the forward inference of BERTEncoder represents 2 BERT\ninput sequences. We deﬁne mlm_positions as the 3 indices to predict in either BERT input\nsequence of encoded_X. The forward inference of mlm returns prediction results mlm_Y_hat\nat all the masked positions mlm_positions of encoded_X. For each prediction, the size of\nthe result is equal to the vocabulary size.\nmlm = MaskLM(vocab_size, num_hiddens)\nmlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\nmlm_Y_hat = mlm(encoded_X, mlm_positions)\nmlm_Y_hat.shape\ntorch.Size([2, 3, 10000])\nWith the ground truth labels mlm_Y of the predicted tokens mlm_Y_hat under masks, we\ncan calculate the cross-entropy loss of the masked language model task in BERT pretrain-\ning.\nmlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\nloss = nn.CrossEntropyLoss(reduction='none')\nmlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\nmlm_l.shape\ntorch.Size([6])\nNext Sentence Prediction\nAlthough masked language modeling is able to encode bidirectional context for representing\nwords, it does not explicitly model the logical relationship between text pairs. To help un-\nderstand the relationship between two text sequences, BERT considers a binary classiﬁcation\ntask, next sentence prediction, in its pretraining. When generating sentence pairs for pretrain-\ning, for half of the time they are indeed consecutive sentences with the label “True”; while\nfor the other half of the time the second sentence is randomly sampled from the corpus with\nthe label “False”.\nThe following NextSentencePred class uses a one-hidden-layer MLP to predict whether the\nsecond sentence is the next sentence of the ﬁrst in the BERT input sequence. Due to self-\nattention in the Transformer encoder, the BERT representation of the special token “<cls>”\n\n758\nNatural Language Processing: Pretraining\nencodes both the two sentences from the input. Hence, the output layer (self.output) of\nthe MLP classiﬁer takes X as input, where X is the output of the MLP hidden layer whose\ninput is the encoded “<cls>” token.\n#@save\nclass NextSentencePred(nn.Module):\n\"\"\"The next sentence prediction task of BERT.\"\"\"\ndef __init__(self, **kwargs):\nsuper(NextSentencePred, self).__init__(**kwargs)\nself.output = nn.LazyLinear(2)\ndef forward(self, X):\n# `X` shape: (batch size, `num_hiddens`)\nreturn self.output(X)\nWe can see that the forward inference of an NextSentencePred instance returns binary\npredictions for each BERT input sequence.\n# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n# flatten=True, all but the first axis of input data are collapsed together\nencoded_X = torch.flatten(encoded_X, start_dim=1)\n# input_shape for NSP: (batch size, `num_hiddens`)\nnsp = NextSentencePred()\nnsp_Y_hat = nsp(encoded_X)\nnsp_Y_hat.shape\ntorch.Size([2, 2])\nThe cross-entropy loss of the 2 binary classiﬁcations can also be computed.\nnsp_y = torch.tensor([0, 1])\nnsp_l = loss(nsp_Y_hat, nsp_y)\nnsp_l.shape\ntorch.Size([2])\nIt is noteworthy that all the labels in both the aforementioned pretraining tasks can be triv-\nially obtained from the pretraining corpus without manual labeling eﬀort. The original BERT\nhas been pretrained on the concatenation of BookCorpus (Zhu et al., 2015) and English\nWikipedia. These two text corpora are huge: they have 800 million words and 2.5 billion\nwords, respectively.\n15.8.6 Putting It All Together\nWhen pretraining BERT, the ﬁnal loss function is a linear combination of both the loss func-\ntions for masked language modeling and next sentence prediction. Now we can deﬁne the\n\n759\nBidirectional Encoder Representations from Transformers (BERT)\nBERTModel class by instantiating the three classes BERTEncoder, MaskLM, and NextSenten-\ncePred. The forward inference returns the encoded BERT representations encoded_X, pre-\ndictions of masked language modeling mlm_Y_hat, and next sentence predictions nsp_Y_hat.\n#@save\nclass BERTModel(nn.Module):\n\"\"\"The BERT model.\"\"\"\ndef __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len=1000):\nsuper(BERTModel, self).__init__()\nself.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout,\nmax_len=max_len)\nself.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\nnn.Tanh())\nself.mlm = MaskLM(vocab_size, num_hiddens)\nself.nsp = NextSentencePred()\ndef forward(self, tokens, segments, valid_lens=None, pred_positions=None):\nencoded_X = self.encoder(tokens, segments, valid_lens)\nif pred_positions is not None:\nmlm_Y_hat = self.mlm(encoded_X, pred_positions)\nelse:\nmlm_Y_hat = None\n# The hidden layer of the MLP classifier for next sentence prediction.\n# 0 is the index of the '<cls>' token\nnsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\nreturn encoded_X, mlm_Y_hat, nsp_Y_hat\n15.8.7 Summary\n• Word embedding models such as word2vec and GloVe are context-independent. They as-\nsign the same pretrained vector to the same word regardless of the context of the word\n(if any). It is hard for them to handle well polysemy or complex semantics in natural\nlanguages.\n• For context-sensitive word representations such as ELMo and GPT, representations of\nwords depend on their contexts.\n• ELMo encodes context bidirectionally but uses task-speciﬁc architectures (however, it is\npractically non-trivial to craft a speciﬁc architecture for every natural language process-\ning task); while GPT is task-agnostic but encodes context left-to-right.\n• BERT combines the best of both worlds: it encodes context bidirectionally and requires\nminimal architecture changes for a wide range of natural language processing tasks.\n• The embeddings of the BERT input sequence are the sum of the token embeddings, seg-\nment embeddings, and positional embeddings.\n• Pretraining BERT is composed of two tasks: masked language modeling and next sentence\nprediction. The former is able to encode bidirectional context for representing words,\nwhile the latter explicitly models the logical relationship between text pairs.\n\n760\nNatural Language Processing: Pretraining\n238\n15.8.8 Exercises\n1. All other things being equal, will a masked language model require more or fewer pre-\ntraining steps to converge than a left-to-right language model? Why?\n2. In the original implementation of BERT, the positionwise feed-forward network in BERTEn-\ncoder (via d2l.TransformerEncoderBlock) and the fully connected layer in MaskLM\nboth use the Gaussian error linear unit (GELU) (Hendrycks and Gimpel, 2016) as the\nactivation function. Research into the diﬀerence between GELU and ReLU.\nDiscussions238.\n15.9 The Dataset for Pretraining BERT\nTo pretrain the BERT model as implemented in Section 15.8, we need to generate the dataset\nin the ideal format to facilitate the two pretraining tasks: masked language modeling and next\nsentence prediction. On the one hand, the original BERT model is pretrained on the concate-\nnation of two huge corpora BookCorpus and English Wikipedia (see Section 15.8.5), making\nit hard to run for most readers of this book. On the other hand, the oﬀ-the-shelf pretrained\nBERT model may not ﬁt for applications from speciﬁc domains like medicine. Thus, it is\ngetting popular to pretrain BERT on a customized dataset. To facilitate the demonstration of\nBERT pretraining, we use a smaller corpus WikiText-2 (Merity et al., 2016).\nComparing with the PTB dataset used for pretraining word2vec in Section 15.3, WikiText-2\n(i) retains the original punctuation, making it suitable for next sentence prediction; (ii) retains\nthe original case and numbers; (iii) is over twice larger.\nimport os\nimport random\nimport torch\nfrom d2l import torch as d2l\nIn the WikiText-2 dataset, each line represents a paragraph where space is inserted between\nany punctuation and its preceding token. Paragraphs with at least two sentences are retained.\nTo split sentences, we only use the period as the delimiter for simplicity. We leave discus-\nsions of more complex sentence splitting techniques in the exercises at the end of this sec-\ntion.\n#@save\nd2l.DATA_HUB['wikitext-2'] = (\n'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n(continues on next page)\n\n761\nThe Dataset for Pretraining BERT\n(continued from previous page)\n#@save\ndef _read_wiki(data_dir):\nfile_name = os.path.join(data_dir, 'wiki.train.tokens')\nwith open(file_name, 'r') as f:\nlines = f.readlines()\n# Uppercase letters are converted to lowercase ones\nparagraphs = [line.strip().lower().split(' . ')\nfor line in lines if len(line.split(' . ')) >= 2]\nrandom.shuffle(paragraphs)\nreturn paragraphs\n15.9.1 Deﬁning Helper Functions for Pretraining Tasks\nIn the following, we begin by implementing helper functions for the two BERT pretraining\ntasks: next sentence prediction and masked language modeling. These helper functions will\nbe invoked later when transforming the raw text corpus into the dataset of the ideal format\nto pretrain BERT.\nGenerating the Next Sentence Prediction Task\nAccording to descriptions of Section 15.8.5, the _get_next_sentence function generates a\ntraining example for the binary classiﬁcation task.\n#@save\ndef _get_next_sentence(sentence, next_sentence, paragraphs):\nif random.random() < 0.5:\nis_next = True\nelse:\n# `paragraphs` is a list of lists of lists\nnext_sentence = random.choice(random.choice(paragraphs))\nis_next = False\nreturn sentence, next_sentence, is_next\nThe following function generates training examples for next sentence prediction from the\ninput paragraph by invoking the _get_next_sentence function. Here paragraph is a list\nof sentences, where each sentence is a list of tokens. The argument max_len speciﬁes the\nmaximum length of a BERT input sequence during pretraining.\n#@save\ndef _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\nnsp_data_from_paragraph = []\nfor i in range(len(paragraph) - 1):\ntokens_a, tokens_b, is_next = _get_next_sentence(\nparagraph[i], paragraph[i + 1], paragraphs)\n# Consider 1 '<cls>' token and 2 '<sep>' tokens\nif len(tokens_a) + len(tokens_b) + 3 > max_len:\n(continues on next page)\n\n762\nNatural Language Processing: Pretraining\n(continued from previous page)\ncontinue\ntokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\nnsp_data_from_paragraph.append((tokens, segments, is_next))\nreturn nsp_data_from_paragraph\nGenerating the Masked Language Modeling Task\nIn order to generate training examples for the masked language modeling task from a BERT\ninput sequence, we deﬁne the following _replace_mlm_tokens function. In its inputs, to-\nkens is a list of tokens representing a BERT input sequence, candidate_pred_positions\nis a list of token indices of the BERT input sequence excluding those of special tokens (spe-\ncial tokens are not predicted in the masked language modeling task), and num_mlm_preds\nindicates the number of predictions (recall 15% random tokens to predict). Following the\ndeﬁnition of the masked language modeling task in Section 15.8.5, at each prediction posi-\ntion, the input may be replaced by a special “<mask>” token or a random token, or remain\nunchanged. In the end, the function returns the input tokens after possible replacement, the\ntoken indices where predictions take place and labels for these predictions.\n#@save\ndef _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\nvocab):\n# For the input of a masked language model, make a new copy of tokens and\n# replace some of them by '<mask>' or random tokens\nmlm_input_tokens = [token for token in tokens]\npred_positions_and_labels = []\n# Shuffle for getting 15% random tokens for prediction in the masked\n# language modeling task\nrandom.shuffle(candidate_pred_positions)\nfor mlm_pred_position in candidate_pred_positions:\nif len(pred_positions_and_labels) >= num_mlm_preds:\nbreak\nmasked_token = None\n# 80% of the time: replace the word with the '<mask>' token\nif random.random() < 0.8:\nmasked_token = '<mask>'\nelse:\n# 10% of the time: keep the word unchanged\nif random.random() < 0.5:\nmasked_token = tokens[mlm_pred_position]\n# 10% of the time: replace the word with a random word\nelse:\nmasked_token = random.choice(vocab.idx_to_token)\nmlm_input_tokens[mlm_pred_position] = masked_token\npred_positions_and_labels.append(\n(mlm_pred_position, tokens[mlm_pred_position]))\nreturn mlm_input_tokens, pred_positions_and_labels\nBy invoking the aforementioned _replace_mlm_tokens function, the following function\ntakes a BERT input sequence (tokens) as an input and returns indices of the input tokens\n\n763\nThe Dataset for Pretraining BERT\n(after possible token replacement as described in Section 15.8.5), the token indices where\npredictions take place, and label indices for these predictions.\n#@save\ndef _get_mlm_data_from_tokens(tokens, vocab):\ncandidate_pred_positions = []\n# `tokens` is a list of strings\nfor i, token in enumerate(tokens):\n# Special tokens are not predicted in the masked language modeling\n# task\nif token in ['<cls>', '<sep>']:\ncontinue\ncandidate_pred_positions.append(i)\n# 15% of random tokens are predicted in the masked language modeling task\nnum_mlm_preds = max(1, round(len(tokens) * 0.15))\nmlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\ntokens, candidate_pred_positions, num_mlm_preds, vocab)\npred_positions_and_labels = sorted(pred_positions_and_labels,\nkey=lambda x: x[0])\npred_positions = [v[0] for v in pred_positions_and_labels]\nmlm_pred_labels = [v[1] for v in pred_positions_and_labels]\nreturn vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n15.9.2 Transforming Text into the Pretraining Dataset\nNow we are almost ready to customize a Dataset class for pretraining BERT. Before that,\nwe still need to deﬁne a helper function _pad_bert_inputs to append the special “<pad>”\ntokens to the inputs. Its argument examples contain the outputs from the helper functions\n_get_nsp_data_from_paragraph and _get_mlm_data_from_tokens for the two pretrain-\ning tasks.\n#@save\ndef _pad_bert_inputs(examples, max_len, vocab):\nmax_num_mlm_preds = round(max_len * 0.15)\nall_token_ids, all_segments, valid_lens,\n= [], [], []\nall_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\nnsp_labels = []\nfor (token_ids, pred_positions, mlm_pred_label_ids, segments,\nis_next) in examples:\nall_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\nmax_len - len(token_ids)), dtype=torch.long))\nall_segments.append(torch.tensor(segments + [0] * (\nmax_len - len(segments)), dtype=torch.long))\n# `valid_lens` excludes count of '<pad>' tokens\nvalid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\nall_pred_positions.append(torch.tensor(pred_positions + [0] * (\nmax_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n# Predictions of padded tokens will be filtered out in the loss via\n# multiplication of 0 weights\nall_mlm_weights.append(\ntorch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\nmax_num_mlm_preds - len(pred_positions)),\n(continues on next page)\n\n764\nNatural Language Processing: Pretraining\n(continued from previous page)\ndtype=torch.float32))\nall_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\nmax_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\nnsp_labels.append(torch.tensor(is_next, dtype=torch.long))\nreturn (all_token_ids, all_segments, valid_lens, all_pred_positions,\nall_mlm_weights, all_mlm_labels, nsp_labels)\nPutting the helper functions for generating training examples of the two pretraining tasks,\nand the helper function for padding inputs together, we customize the following _Wiki-\nTextDataset class as the WikiText-2 dataset for pretraining BERT. By implementing the\n__getitem__function, we can arbitrarily access the pretraining (masked language model-\ning and next sentence prediction) examples generated from a pair of sentences from the\nWikiText-2 corpus.\nThe original BERT model uses WordPiece embeddings whose vocabulary size is 30000\n(Wu et al., 2016). The tokenization method of WordPiece is a slight modiﬁcation of the\noriginal byte pair encoding algorithm in Section 15.6.2. For simplicity, we use the d2l.\ntokenize function for tokenization. Infrequent tokens that appear less than ﬁve times are\nﬁltered out.\n#@save\nclass _WikiTextDataset(torch.utils.data.Dataset):\ndef __init__(self, paragraphs, max_len):\n# Input `paragraphs[i]` is a list of sentence strings representing a\n# paragraph; while output `paragraphs[i]` is a list of sentences\n# representing a paragraph, where each sentence is a list of tokens\nparagraphs = [d2l.tokenize(\nparagraph, token='word') for paragraph in paragraphs]\nsentences = [sentence for paragraph in paragraphs\nfor sentence in paragraph]\nself.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n'<pad>', '<mask>', '<cls>', '<sep>'])\n# Get data for the next sentence prediction task\nexamples = []\nfor paragraph in paragraphs:\nexamples.extend(_get_nsp_data_from_paragraph(\nparagraph, paragraphs, self.vocab, max_len))\n# Get data for the masked language model task\nexamples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n+ (segments, is_next))\nfor tokens, segments, is_next in examples]\n# Pad inputs\n(self.all_token_ids, self.all_segments, self.valid_lens,\nself.all_pred_positions, self.all_mlm_weights,\nself.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\nexamples, max_len, self.vocab)\ndef __getitem__(self, idx):\nreturn (self.all_token_ids[idx], self.all_segments[idx],\nself.valid_lens[idx], self.all_pred_positions[idx],\nself.all_mlm_weights[idx], self.all_mlm_labels[idx],\n(continues on next page)\n\n765\nThe Dataset for Pretraining BERT\n(continued from previous page)\nself.nsp_labels[idx])\ndef __len__(self):\nreturn len(self.all_token_ids)\nBy using the _read_wiki function and the _WikiTextDataset class, we deﬁne the following\nload_data_wiki to download and WikiText-2 dataset and generate pretraining examples\nfrom it.\n#@save\ndef load_data_wiki(batch_size, max_len):\n\"\"\"Load the WikiText-2 dataset.\"\"\"\nnum_workers = d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\nparagraphs = _read_wiki(data_dir)\ntrain_set = _WikiTextDataset(paragraphs, max_len)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size,\nshuffle=True, num_workers=num_workers)\nreturn train_iter, train_set.vocab\nSetting the batch size to 512 and the maximum length of a BERT input sequence to be 64, we\nprint out the shapes of a minibatch of BERT pretraining examples. Note that in each BERT\ninput sequence, 10 (64 × 0.15) positions are predicted for the masked language modeling\ntask.\nbatch_size, max_len = 512, 64\ntrain_iter, vocab = load_data_wiki(batch_size, max_len)\nfor (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y) in train_iter:\nprint(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\npred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\nnsp_y.shape)\nbreak\ntorch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512,␣\n,→10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\nIn the end, let’s take a look at the vocabulary size. Even after ﬁltering out infrequent tokens,\nit is still over twice larger than that of the PTB dataset.\nlen(vocab)\n20256\n15.9.3 Summary\n\n766\nNatural Language Processing: Pretraining\n239\n• Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation,\ncase and numbers, and is over twice larger.\n• We can arbitrarily access the pretraining (masked language modeling and next sentence\nprediction) examples generated from a pair of sentences from the WikiText-2 corpus.\n15.9.4 Exercises\n1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other\nsentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an example.\nYou need to install NLTK ﬁrst: pip install nltk. In the code, ﬁrst import nltk. Then,\ndownload the Punkt sentence tokenizer: nltk.download('punkt'). To split sentences\nsuch as sentences = 'This is great ! Why not ?', invoking nltk.tokenize.\nsent_tokenize(sentences) will return a list of two sentence strings: ['This is great\n!', 'Why not ?'].\n2. What is the vocabulary size if we do not ﬁlter out any infrequent token?\nDiscussions239.\n15.10 Pretraining BERT\nWith the BERT model implemented in Section 15.8 and the pretraining examples generated\nfrom the WikiText-2 dataset in Section 15.9, we will pretrain BERT on the WikiText-2\ndataset in this section.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nTo start, we load the WikiText-2 dataset as minibatches of pretraining examples for masked\nlanguage modeling and next sentence prediction. The batch size is 512 and the maximum\nlength of a BERT input sequence is 64. Note that in the original BERT model, the maximum\nlength is 512.\nbatch_size, max_len = 512, 64\ntrain_iter, vocab = d2l.load_data_wiki(batch_size, max_len)\n15.10.1 Pretraining BERT\n\n767\nPretraining BERT\nThe original BERT has two versions of diﬀerent model sizes (Devlin et al., 2018). The base\nmodel (BERTBASE) uses 12 layers (Transformer encoder blocks) with 768 hidden units (hid-\nden size) and 12 self-attention heads. The large model (BERTLARGE) uses 24 layers with\n1024 hidden units and 16 self-attention heads. Notably, the former has 110 million param-\neters while the latter has 340 million parameters. For demonstration with ease, we deﬁne a\nsmall BERT, using 2 layers, 128 hidden units, and 2 self-attention heads.\nnet = d2l.BERTModel(len(vocab), num_hiddens=128,\nffn_num_hiddens=256, num_heads=2, num_blks=2, dropout=0.2)\ndevices = d2l.try_all_gpus()\nloss = nn.CrossEntropyLoss()\nBefore deﬁning the training loop, we deﬁne a helper function _get_batch_loss_bert.\nGiven the shard of training examples, this function computes the loss for both the masked\nlanguage modeling and next sentence prediction tasks. Note that the ﬁnal loss of BERT pre-\ntraining is just the sum of both the masked language modeling loss and the next sentence\nprediction loss.\n#@save\ndef _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\nsegments_X, valid_lens_x,\npred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y):\n# Forward pass\n_, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\nvalid_lens_x.reshape(-1),\npred_positions_X)\n# Compute masked language model loss\nmlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\nmlm_weights_X.reshape(-1, 1)\nmlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n# Compute next sentence prediction loss\nnsp_l = loss(nsp_Y_hat, nsp_y)\nl = mlm_l + nsp_l\nreturn mlm_l, nsp_l, l\nInvoking the two aforementioned helper functions, the following train_bert function de-\nﬁnes the procedure to pretrain BERT (net) on the WikiText-2 (train_iter) dataset. Train-\ning BERT can take very long. Instead of specifying the number of epochs for training as in\nthe train_ch13 function (see Section 14.1), the input num_steps of the following function\nspeciﬁes the number of iteration steps for training.\ndef train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\nnet(*next(iter(train_iter))[:4])\nnet = nn.DataParallel(net, device_ids=devices).to(devices[0])\ntrainer = torch.optim.Adam(net.parameters(), lr=0.01)\nstep, timer = 0, d2l.Timer()\nanimator = d2l.Animator(xlabel='step', ylabel='loss',\nxlim=[1, num_steps], legend=['mlm', 'nsp'])\n(continues on next page)\n\n768\nNatural Language Processing: Pretraining\n(continued from previous page)\n# Sum of masked language modeling losses, sum of next sentence prediction\n# losses, no. of sentence pairs, count\nmetric = d2l.Accumulator(4)\nnum_steps_reached = False\nwhile step < num_steps and not num_steps_reached:\nfor tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\nmlm_weights_X, mlm_Y, nsp_y in train_iter:\ntokens_X = tokens_X.to(devices[0])\nsegments_X = segments_X.to(devices[0])\nvalid_lens_x = valid_lens_x.to(devices[0])\npred_positions_X = pred_positions_X.to(devices[0])\nmlm_weights_X = mlm_weights_X.to(devices[0])\nmlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\ntrainer.zero_grad()\ntimer.start()\nmlm_l, nsp_l, l = _get_batch_loss_bert(\nnet, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\npred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\nl.backward()\ntrainer.step()\nmetric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\ntimer.stop()\nanimator.add(step + 1,\n(metric[0] / metric[3], metric[1] / metric[3]))\nstep += 1\nif step == num_steps:\nnum_steps_reached = True\nbreak\nprint(f'MLM loss {metric[0] / metric[3]:.3f}, '\nf'NSP loss {metric[1] / metric[3]:.3f}')\nprint(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\nf'{str(devices)}')\nWe can plot both the masked language modeling loss and the next sentence prediction loss\nduring BERT pretraining.\ntrain_bert(train_iter, net, loss, len(vocab), devices, 50)\nMLM loss 5.411, NSP loss 0.764\n2447.5 sentence pairs/sec on [device(type='cuda', index=0), device(type='cuda',\n,→index=1)]\n15.10.2 Representing Text with BERT\nAfter pretraining BERT, we can use it to represent single text, text pairs, or any token in them.\nThe following function returns the BERT (net) representations for all tokens in tokens_a\nand tokens_b.\n\n769\nPretraining BERT\ndef get_bert_encoding(net, tokens_a, tokens_b=None):\ntokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\ntoken_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\nsegments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\nvalid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\nencoded_X, _, _ = net(token_ids, segments, valid_len)\nreturn encoded_X\nConsider the sentence “a crane is ﬂying”. Recall the input representation of BERT as dis-\ncussed in Section 15.8.4. After inserting special tokens “<cls>” (used for classiﬁcation) and\n“<sep>” (used for separation), the BERT input sequence has a length of six. Since zero is\nthe index of the “<cls>” token, encoded_text[:, 0, :] is the BERT representation of\nthe entire input sentence. To evaluate the polysemy token “crane”, we also print out the ﬁrst\nthree elements of the BERT representation of the token.\ntokens_a = ['a', 'crane', 'is', 'flying']\nencoded_text = get_bert_encoding(net, tokens_a)\n# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\nencoded_text_cls = encoded_text[:, 0, :]\nencoded_text_crane = encoded_text[:, 2, :]\nencoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]\n(torch.Size([1, 6, 128]),\ntorch.Size([1, 128]),\ntensor([ 0.0282, -0.6125, -0.3779], device='cuda:0', grad_fn=<SliceBackward0>\n,→))\nNow consider a sentence pair “a crane driver came” and “he just left”. Similarly, encoded_pair[:,\n0, :] is the encoded result of the entire sentence pair from the pretrained BERT. Note that\nthe ﬁrst three elements of the polysemy token “crane” are diﬀerent from those when the\ncontext is diﬀerent. This supports that BERT representations are context-sensitive.\ntokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\nencoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n(continues on next page)\n\n770\nNatural Language Processing: Pretraining\n240\n(continued from previous page)\n# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n# 'left', '<sep>'\nencoded_pair_cls = encoded_pair[:, 0, :]\nencoded_pair_crane = encoded_pair[:, 2, :]\nencoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]\n(torch.Size([1, 10, 128]),\ntorch.Size([1, 128]),\ntensor([-1.3067, -0.3663,\n0.0759], device='cuda:0', grad_fn=<SliceBackward0>\n,→))\nIn Chapter 16, we will ﬁne-tune a pretrained BERT model for downstream natural language\nprocessing applications.\n15.10.3 Summary\n• The original BERT has two versions, where the base model has 110 million parameters\nand the large model has 340 million parameters.\n• After pretraining BERT, we can use it to represent single text, text pairs, or any token in\nthem.\n• In the experiment, the same token has diﬀerent BERT representation when their contexts\nare diﬀerent. This supports that BERT representations are context-sensitive.\n15.10.4 Exercises\n1. In the experiment, we can see that the masked language modeling loss is signiﬁcantly\nhigher than the next sentence prediction loss. Why?\n2. Set the maximum length of a BERT input sequence to be 512 (same as the original BERT\nmodel). Use the conﬁgurations of the original BERT model such as BERTLARGE. Do you\nencounter any error when running this section? Why?\nDiscussions240.\n\n16\nNatural Language Processing:\nApplications\nWe have seen how to represent tokens in text sequences and train their representations in\nChapter 15. Such pretrained text representations can be fed to various models for diﬀerent\ndownstream natural language processing tasks.\nIn fact, earlier chapters have already discussed some natural language processing applications\nwithout pretraining, just for explaining deep learning architectures. For instance, in Chapter\n9, we have relied on RNNs to design language models to generate novella-like text. In Chapter\n10 and Chapter 11, we have also designed models based on RNNs and attention mechanisms\nfor machine translation.\nHowever, this book does not intend to cover all such applications in a comprehensive manner.\nInstead, our focus is on how to apply (deep) representation learning of languages to addressing\nnatural language processing problems. Given pretrained text representations, this chapter will\nexplore two popular and representative downstream natural language processing tasks: senti-\nment analysis and natural language inference, which analyze single text and relationships of\ntext pairs, respectively.\nt\nFig. 16.1\nPretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nhow to design models for different downstream natural language processing applications.\nAs depicted in Fig. 16.1, this chapter focuses on describing the basic ideas of designing nat-\nural language processing models using diﬀerent types of deep learning architectures, such\nas MLPs, CNNs, RNNs, and attention. Though it is possible to combine any pretrained text\nrepresentations with any architecture for either application in Fig. 16.1, we select a few rep-\n771\n\n772\nNatural Language Processing: Applications\n241\nresentative combinations. Speciﬁcally, we will explore popular architectures based on RNNs\nand CNNs for sentiment analysis. For natural language inference, we choose attention and\nMLPs to demonstrate how to analyze text pairs. In the end, we introduce how to ﬁne-tune a\npretrained BERT model for a wide range of natural language processing applications, such\nas on a sequence level (single text classiﬁcation and text pair classiﬁcation) and a token level\n(text tagging and question answering). As a concrete empirical case, we will ﬁne-tune BERT\nfor natural language inference.\nAs we have introduced in Section 15.8, BERT requires minimal architecture changes for a\nwide range of natural language processing applications. However, this beneﬁt comes at the\ncost of ﬁne-tuning a huge number of BERT parameters for the downstream applications.\nWhen space or time is limited, those crafted models based on MLPs, CNNs, RNNs, and\nattention are more feasible. In the following, we start by the sentiment analysis application\nand illustrate the model design based on RNNs and CNNs, respectively.\n16.1 Sentiment Analysis and the Dataset\nWith the proliferation of online social media and review platforms, a plethora of opinionated\ndata has been logged, bearing great potential for supporting decision making processes. Senti-\nment analysis studies people’s sentiments in their produced text, such as product reviews, blog\ncomments, and forum discussions. It enjoys wide applications to ﬁelds as diverse as politics\n(e.g., analysis of public sentiments towards policies), ﬁnance (e.g., analysis of sentiments of\nthe market), and marketing (e.g., product research and brand management).\nSince sentiments can be categorized as discrete polarities or scales (e.g., positive and neg-\native), we can consider sentiment analysis as a text classiﬁcation task, which transforms a\nvarying-length text sequence into a ﬁxed-length text category. In this chapter, we will use\nStanford’s large movie review dataset 241 for sentiment analysis. It consists of a training set\nand a testing set, either containing 25000 movie reviews downloaded from IMDb. In both\ndatasets, there are equal number of “positive” and “negative” labels, indicating diﬀerent sen-\ntiment polarities.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n16.1.1 Reading the Dataset\nFirst, download and extract this IMDb review dataset in the path ../data/aclImdb.\n\n773\nSentiment Analysis and the Dataset\n#@save\nd2l.DATA_HUB['aclImdb'] = (d2l.DATA_URL + 'aclImdb_v1.tar.gz',\n'01ada507287d82875905620988597833ad4e0903')\ndata_dir = d2l.download_extract('aclImdb', 'aclImdb')\nNext, read the training and test datasets. Each example is a review and its label: 1 for “positive”\nand 0 for “negative”.\n#@save\ndef read_imdb(data_dir, is_train):\n\"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\ndata, labels = [], []\nfor label in ('pos', 'neg'):\nfolder_name = os.path.join(data_dir, 'train' if is_train else 'test',\nlabel)\nfor file in os.listdir(folder_name):\nwith open(os.path.join(folder_name, file), 'rb') as f:\nreview = f.read().decode('utf-8').replace('\\n', '')\ndata.append(review)\nlabels.append(1 if label == 'pos' else 0)\nreturn data, labels\ntrain_data = read_imdb(data_dir, is_train=True)\nprint('# trainings:', len(train_data[0]))\nfor x, y in zip(train_data[0][:3], train_data[1][:3]):\nprint('label:', y, 'review:', x[:60])\n# trainings: 25000\nlabel: 1 review: Shocking, well-made chiller is an undervalued tale of atroci\nlabel: 1 review: Fast-paced, funny, sexy, and spectacular. Cagney is always t\nlabel: 1 review: Especially for a time when not much science fiction was bein\n16.1.2 Preprocessing the Dataset\nTreating each word as a token and ﬁltering out words that appear less than 5 times, we create\na vocabulary out of the training dataset.\ntrain_tokens = d2l.tokenize(train_data[0], token='word')\nvocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])\nAfter tokenization, let’s plot the histogram of review lengths in tokens.\nd2l.set_figsize()\nd2l.plt.xlabel('# tokens per review')\nd2l.plt.ylabel('count')\nd2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));\nAs we expected, the reviews have varying lengths. To process a minibatch of such reviews\n\n774\nNatural Language Processing: Applications\nat each time, we set the length of each review to 500 with truncation and padding, which is\nsimilar to the preprocessing step for the machine translation dataset in Section 10.5.\nnum_steps = 500\n# sequence length\ntrain_features = torch.tensor([d2l.truncate_pad(\nvocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\nprint(train_features.shape)\ntorch.Size([25000, 500])\n16.1.3 Creating Data Iterators\nNow we can create data iterators. At each iteration, a minibatch of examples are returned.\ntrain_iter = d2l.load_array((train_features, torch.tensor(train_data[1])), 64)\nfor X, y in train_iter:\nprint('X:', X.shape, ', y:', y.shape)\nbreak\nprint('# batches:', len(train_iter))\nX: torch.Size([64, 500]) , y: torch.Size([64])\n# batches: 391\n16.1.4 Putting It All Together\nLast, we wrap up the above steps into the load_data_imdb function. It returns training and\ntest data iterators and the vocabulary of the IMDb review dataset.\n#@save\ndef load_data_imdb(batch_size, num_steps=500):\n(continues on next page)\n\n775\nSentiment Analysis: Using Recurrent Neural Networks\n242\n243\n(continued from previous page)\n\"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\ndata_dir = d2l.download_extract('aclImdb', 'aclImdb')\ntrain_data = read_imdb(data_dir, True)\ntest_data = read_imdb(data_dir, False)\ntrain_tokens = d2l.tokenize(train_data[0], token='word')\ntest_tokens = d2l.tokenize(test_data[0], token='word')\nvocab = d2l.Vocab(train_tokens, min_freq=5)\ntrain_features = torch.tensor([d2l.truncate_pad(\nvocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\ntest_features = torch.tensor([d2l.truncate_pad(\nvocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\ntrain_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),\nbatch_size)\ntest_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),\nbatch_size,\nis_train=False)\nreturn train_iter, test_iter, vocab\n16.1.5 Summary\n• Sentiment analysis studies people’s sentiments in their produced text, which is considered\nas a text classiﬁcation problem that transforms a varying-length text sequence into a\nﬁxed-length text category.\n• After preprocessing, we can load Stanford’s large movie review dataset (IMDb review\ndataset) into data iterators with a vocabulary.\n16.1.6 Exercises\n1. What hyperparameters in this section can we modify to accelerate training sentiment\nanalysis models?\n2. Can you implement a function to load the dataset of Amazon reviews242 into data iterators\nand labels for sentiment analysis?\nDiscussions243.\n16.2 Sentiment Analysis: Using Recurrent Neural\nNetworks\nLike word similarity and analogy tasks, we can also apply pretrained word vectors to senti-\nment analysis. Since the IMDb review dataset in Section 16.1 is not very big, using text rep-\nresentations that were pretrained on large-scale corpora may reduce overﬁtting of the model.\n\n776\nNatural Language Processing: Applications\nAs a speciﬁc example illustrated in Fig. 16.2.1, we will represent each token using the pre-\ntrained GloVe model, and feed these token representations into a multilayer bidirectional\nRNN to obtain the text sequence representation, which will be transformed into sentiment\nanalysis outputs (Maas et al., 2011). For the same downstream application, we will consider\na diﬀerent architectural choice later.\nt\nFig. 16.2.1\nThis section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\nDownloading ../data/aclImdb_v1.tar.gz from http://d2l-data.s3-accelerate.\n,→amazonaws.com/aclImdb_v1.tar.gz...\n16.2.1 Representing Single Text with RNNs\nIn text classiﬁcations tasks, such as sentiment analysis, a varying-length text sequence will be\ntransformed into ﬁxed-length categories. In the following BiRNN class, while each token of\na text sequence gets its individual pretrained GloVe representation via the embedding layer\n(self.embedding), the entire sequence is encoded by a bidirectional RNN (self.encoder).\nMore concretely, the hidden states (at the last layer) of the bidirectional LSTM at both the\ninitial and ﬁnal time steps are concatenated as the representation of the text sequence. This\nsingle text representation is then transformed into output categories by a fully connected layer\n(self.decoder) with two outputs (“positive” and “negative”).\nclass BiRNN(nn.Module):\ndef __init__(self, vocab_size, embed_size, num_hiddens,\nnum_layers, **kwargs):\nsuper(BiRNN, self).__init__(**kwargs)\n(continues on next page)\n\n777\nSentiment Analysis: Using Recurrent Neural Networks\n(continued from previous page)\nself.embedding = nn.Embedding(vocab_size, embed_size)\n# Set `bidirectional` to True to get a bidirectional RNN\nself.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\nbidirectional=True)\nself.decoder = nn.Linear(4 * num_hiddens, 2)\ndef forward(self, inputs):\n# The shape of `inputs` is (batch size, no. of time steps). Because\n# LSTM requires its input's first dimension to be the temporal\n# dimension, the input is transposed before obtaining token\n# representations. The output shape is (no. of time steps, batch size,\n# word vector dimension)\nembeddings = self.embedding(inputs.T)\nself.encoder.flatten_parameters()\n# Returns hidden states of the last hidden layer at different time\n# steps. The shape of `outputs` is (no. of time steps, batch size,\n# 2 * no. of hidden units)\noutputs, _ = self.encoder(embeddings)\n# Concatenate the hidden states at the initial and final time steps as\n# the input of the fully connected layer. Its shape is (batch size,\n# 4 * no. of hidden units)\nencoding = torch.cat((outputs[0], outputs[-1]), dim=1)\nouts = self.decoder(encoding)\nreturn outs\nLet’s construct a bidirectional RNN with two hidden layers to represent single text for senti-\nment analysis.\nembed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()\nnet = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)\ndef init_weights(module):\nif type(module) == nn.Linear:\nnn.init.xavier_uniform_(module.weight)\nif type(module) == nn.LSTM:\nfor param in module._flat_weights_names:\nif \"weight\" in param:\nnn.init.xavier_uniform_(module._parameters[param])\nnet.apply(init_weights);\n16.2.2 Loading Pretrained Word Vectors\nBelow we load the pretrained 100-dimensional (needs to be consistent with embed_size)\nGloVe embeddings for tokens in the vocabulary.\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n\n778\nNatural Language Processing: Applications\nDownloading ../data/glove.6B.100d.zip from http://d2l-data.s3-accelerate.\n,→amazonaws.com/glove.6B.100d.zip...\nPrint the shape of the vectors for all the tokens in the vocabulary.\nembeds = glove_embedding[vocab.idx_to_token]\nembeds.shape\ntorch.Size([49346, 100])\nWe use these pretrained word vectors to represent tokens in the reviews and will not update\nthese vectors during training.\nnet.embedding.weight.data.copy_(embeds)\nnet.embedding.weight.requires_grad = False\n16.2.3 Training and Evaluating the Model\nNow we can train the bidirectional RNN for sentiment analysis.\nlr, num_epochs = 0.01, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.693, train acc 0.502, test acc 0.500\n2089.4 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nWe deﬁne the following function to predict the sentiment of a text sequence using the trained\nmodel net.\n\n779\nSentiment Analysis: Using Recurrent Neural Networks\n244\n#@save\ndef predict_sentiment(net, vocab, sequence):\n\"\"\"Predict the sentiment of a text sequence.\"\"\"\nsequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())\nlabel = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\nreturn 'positive' if label == 1 else 'negative'\nFinally, let’s use the trained model to predict the sentiment for two simple sentences.\npredict_sentiment(net, vocab, 'this movie is so great')\n'positive'\npredict_sentiment(net, vocab, 'this movie is so bad')\n'positive'\n16.2.4 Summary\n• Pretrained word vectors can represent individual tokens in a text sequence.\n• Bidirectional RNNs can represent a text sequence, such as via the concatenation of its\nhidden states at the initial and ﬁnal time steps. This single text representation can be\ntransformed into categories using a fully connected layer.\n16.2.5 Exercises\n1. Increase the number of epochs. Can you improve the training and testing accuracies? How\nabout tuning other hyperparameters?\n2. Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does it\nimprove classiﬁcation accuracy?\n3. Can we improve the classiﬁcation accuracy by using the spaCy tokenization? You need to\ninstall spaCy (pip install spacy) and install the English package (python -m spacy\ndownload en). In the code, ﬁrst, import spaCy (import spacy). Then, load the spaCy\nEnglish package (spacy_en = spacy.load('en')). Finally, deﬁne the function def\ntokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]\nand replace the original tokenizer function. Note the diﬀerent forms of phrase tokens in\nGloVe and spaCy. For example, the phrase token “new york” takes the form of “new-york”\nin GloVe and the form of “new york” after the spaCy tokenization.\nDiscussions244.\n\n780\nNatural Language Processing: Applications\n16.3 Sentiment Analysis: Using Convolutional\nNeural Networks\nIn Chapter 7, we investigated mechanisms for processing two-dimensional image data with\ntwo-dimensional CNNs, which were applied to local features such as adjacent pixels. Though\noriginally designed for computer vision, CNNs are also widely used for natural language\nprocessing. Simply put, just think of any text sequence as a one-dimensional image. In this\nway, one-dimensional CNNs can process local features such as n-grams in text.\nIn this section, we will use the textCNN model to demonstrate how to design a CNN architec-\nture for representing single text (Kim, 2014). Compared with Fig. 16.2.1 that uses an RNN\narchitecture with GloVe pretraining for sentiment analysis, the only diﬀerence in Fig. 16.3.1\nlies in the choice of the architecture.\nt\nFig. 16.3.1\nThis section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nbatch_size = 64\ntrain_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)\n16.3.1 One-Dimensional Convolutions\nBefore introducing the model, let’s see how a one-dimensional convolution works. Bear in\nmind that it is just a special case of a two-dimensional convolution based on the cross-\ncorrelation operation.\nAs shown in Fig. 16.3.2, in the one-dimensional case, the convolution window slides from\nleft to right across the input tensor. During sliding, the input subtensor (e.g., 0 and 1 in Fig.\n\n781\nSentiment Analysis: Using Convolutional Neural Networks\nt\nFig. 16.3.2\nOne-dimensional cross-correlation operation. The shaded portions are the ﬁrst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0 × 1 + 1 × 2 = 2.\n16.3.2) contained in the convolution window at a certain position and the kernel tensor (e.g.,\n1 and 2 in Fig. 16.3.2) are multiplied elementwise. The sum of these multiplications gives\nthe single scalar value (e.g., 0 × 1 + 1 × 2 = 2 in Fig. 16.3.2) at the corresponding position\nof the output tensor.\nWe implement one-dimensional cross-correlation in the following corr1d function. Given an\ninput tensor X and a kernel tensor K, it returns the output tensor Y.\ndef corr1d(X, K):\nw = K.shape[0]\nY = torch.zeros((X.shape[0] - w + 1))\nfor i in range(Y.shape[0]):\nY[i] = (X[i: i + w] * K).sum()\nreturn Y\nWe can construct the input tensor X and the kernel tensor K from Fig. 16.3.2 to validate the\noutput of the above one-dimensional cross-correlation implementation.\nX, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])\ncorr1d(X, K)\ntensor([ 2.,\n5.,\n8., 11., 14., 17.])\nFor any one-dimensional input with multiple channels, the convolution kernel needs to have\nthe same number of input channels. Then for each channel, perform a cross-correlation op-\neration on the one-dimensional tensor of the input and the one-dimensional tensor of the\nconvolution kernel, summing the results over all the channels to produce the one-dimensional\noutput tensor. Fig. 16.3.3 shows a one-dimensional cross-correlation operation with 3 input\nchannels.\nWe can implement the one-dimensional cross-correlation operation for multiple input chan-\nnels and validate the results in Fig. 16.3.3.\ndef corr1d_multi_in(X, K):\n# First, iterate through the 0th dimension (channel dimension) of `X` and\n# `K`. Then, add them together\nreturn sum(corr1d(x, k) for x, k in zip(X, K))\nX = torch.tensor([[0, 1, 2, 3, 4, 5, 6],\n(continues on next page)\n\n782\nNatural Language Processing: Applications\nt\nFig. 16.3.3\nOne-dimensional cross-correlation operation with 3 input channels. The shaded portions\nare the ﬁrst output element as well as the input and kernel tensor elements used for the\noutput computation: 0 × 1 + 1 × 2 + 1 × 3 + 2 × 4 + 2 × (−1) + 3 × (−3) = 2.\n(continued from previous page)\n[1, 2, 3, 4, 5, 6, 7],\n[2, 3, 4, 5, 6, 7, 8]])\nK = torch.tensor([[1, 2], [3, 4], [-1, -3]])\ncorr1d_multi_in(X, K)\ntensor([ 2.,\n8., 14., 20., 26., 32.])\nNote that multi-input-channel one-dimensional cross-correlations are equivalent to single-\ninput-channel two-dimensional cross-correlations. To illustrate, an equivalent form of the\nmulti-input-channel one-dimensional cross-correlation in Fig. 16.3.3 is the single-input-channel\ntwo-dimensional cross-correlation in Fig. 16.3.4, where the height of the convolution kernel\nhas to be the same as that of the input tensor.\nt\nFig. 16.3.4\nTwo-dimensional cross-correlation operation with a single input channel. The shaded\nportions are the ﬁrst output element as well as the input and kernel tensor elements used\nfor the output computation: 2 × (−1) + 3 × (−3) + 1 × 3 + 2 × 4 + 0 × 1 + 1 × 2 = 2.\nBoth the outputs in Fig. 16.3.2 and Fig. 16.3.3 have only one channel. Same as two-dimensional\nconvolutions with multiple output channels described in Section 7.4.2, we can also specify\nmultiple output channels for one-dimensional convolutions.\n16.3.2 Max-Over-Time Pooling\nSimilarly, we can use pooling to extract the highest value from sequence representations as the\nmost important feature across time steps. The max-over-time pooling used in textCNN works\nlike the one-dimensional global max-pooling (Collobert et al., 2011). For a multi-channel\ninput where each channel stores values at diﬀerent time steps, the output at each channel is\n\n783\nSentiment Analysis: Using Convolutional Neural Networks\nthe maximum value for that channel. Note that the max-over-time pooling allows diﬀerent\nnumbers of time steps at diﬀerent channels.\n16.3.3 The textCNN Model\nUsing the one-dimensional convolution and max-over-time pooling, the textCNN model takes\nindividual pretrained token representations as input, then obtains and transforms sequence\nrepresentations for the downstream application.\nFor a single text sequence with n tokens represented by d-dimensional vectors, the width,\nheight, and number of channels of the input tensor are n, 1, and d, respectively. The textCNN\nmodel transforms the input into the output as follows:\n1. Deﬁne multiple one-dimensional convolution kernels and perform convolution operations\nseparately on the inputs. Convolution kernels with diﬀerent widths may capture local fea-\ntures among diﬀerent numbers of adjacent tokens.\n2. Perform max-over-time pooling on all the output channels, and then concatenate all the\nscalar pooling outputs as a vector.\n3. Transform the concatenated vector into the output categories using the fully connected\nlayer. Dropout can be used for reducing overﬁtting.\nt\nFig. 16.3.5\nThe model architecture of textCNN.\nFig. 16.3.5 illustrates the model architecture of textCNN with a concrete example. The input\nis a sentence with 11 tokens, where each token is represented by a 6-dimensional vectors. So\nwe have a 6-channel input with width 11. Deﬁne two one-dimensional convolution kernels of\n\n784\nNatural Language Processing: Applications\nwidths 2 and 4, with 4 and 5 output channels, respectively. They produce 4 output channels\nwith width 11−2+1 = 10 and 5 output channels with width 11−4+1 = 8. Despite diﬀerent\nwidths of these 9 channels, the max-over-time pooling gives a concatenated 9-dimensional\nvector, which is ﬁnally transformed into a 2-dimensional output vector for binary sentiment\npredictions.\nDeﬁning the Model\nWe implement the textCNN model in the following class. Compared with the bidirectional\nRNN model in Section 16.2, besides replacing recurrent layers with convolutional layers, we\nalso use two embedding layers: one with trainable weights and the other with ﬁxed weights.\nclass TextCNN(nn.Module):\ndef __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n**kwargs):\nsuper(TextCNN, self).__init__(**kwargs)\nself.embedding = nn.Embedding(vocab_size, embed_size)\n# The embedding layer not to be trained\nself.constant_embedding = nn.Embedding(vocab_size, embed_size)\nself.dropout = nn.Dropout(0.5)\nself.decoder = nn.Linear(sum(num_channels), 2)\n# The max-over-time pooling layer has no parameters, so this instance\n# can be shared\nself.pool = nn.AdaptiveAvgPool1d(1)\nself.relu = nn.ReLU()\n# Create multiple one-dimensional convolutional layers\nself.convs = nn.ModuleList()\nfor c, k in zip(num_channels, kernel_sizes):\nself.convs.append(nn.Conv1d(2 * embed_size, c, k))\ndef forward(self, inputs):\n# Concatenate two embedding layer outputs with shape (batch size, no.\n# of tokens, token vector dimension) along vectors\nembeddings = torch.cat((\nself.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n# Per the input format of one-dimensional convolutional layers,\n# rearrange the tensor so that the second dimension stores channels\nembeddings = embeddings.permute(0, 2, 1)\n# For each one-dimensional convolutional layer, after max-over-time\n# pooling, a tensor of shape (batch size, no. of channels, 1) is\n# obtained. Remove the last dimension and concatenate along channels\nencoding = torch.cat([\ntorch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\nfor conv in self.convs], dim=1)\noutputs = self.decoder(self.dropout(encoding))\nreturn outputs\nLet’s create a textCNN instance. It has 3 convolutional layers with kernel widths of 3, 4, and\n5, all with 100 output channels.\n\n785\nSentiment Analysis: Using Convolutional Neural Networks\nembed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\ndevices = d2l.try_all_gpus()\nnet = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\ndef init_weights(module):\nif type(module) in (nn.Linear, nn.Conv1d):\nnn.init.xavier_uniform_(module.weight)\nnet.apply(init_weights);\nLoading Pretrained Word Vectors\nSame as Section 16.2, we load pretrained 100-dimensional GloVe embeddings as the initial-\nized token representations. These token representations (embedding weights) will be trained\nin embedding and ﬁxed in constant_embedding.\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.data.copy_(embeds)\nnet.constant_embedding.weight.data.copy_(embeds)\nnet.constant_embedding.weight.requires_grad = False\nTraining and Evaluating the Model\nNow we can train the textCNN model for sentiment analysis.\nlr, num_epochs = 0.001, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.066, train acc 0.979, test acc 0.871\n2832.0 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nBelow we use the trained model to predict the sentiment for two simple sentences.\nd2l.predict_sentiment(net, vocab, 'this movie is so great')\n'positive'\nd2l.predict_sentiment(net, vocab, 'this movie is so bad')\n\n786\nNatural Language Processing: Applications\n245\n'negative'\n16.3.4 Summary\n• One-dimensional CNNs can process local features such as n-grams in text.\n• Multi-input-channel one-dimensional cross-correlations are equivalent to single-input-channel\ntwo-dimensional cross-correlations.\n• The max-over-time pooling allows diﬀerent numbers of time steps at diﬀerent channels.\n• The textCNN model transforms individual token representations into downstream appli-\ncation outputs using one-dimensional convolutional layers and max-over-time pooling\nlayers.\n16.3.5 Exercises\n1. Tune hyperparameters and compare the two architectures for sentiment analysis in Section\n16.2 and in this section, such as in classiﬁcation accuracy and computational eﬃciency.\n2. Can you further improve the classiﬁcation accuracy of the model by using the methods\nintroduced in the exercises of Section 16.2?\n3. Add positional encoding in the input representations. Does it improve the classiﬁcation\naccuracy?\nDiscussions245.\n\n787\nNatural Language Inference and the Dataset\n16.4 Natural Language Inference and the Dataset\nIn Section 16.1, we discussed the problem of sentiment analysis. This task aims to classify\na single text sequence into predeﬁned categories, such as a set of sentiment polarities. How-\never, when there is a need to decide whether one sentence can be inferred form another, or\neliminate redundancy by identifying sentences that are semantically equivalent, knowing how\nto classify one text sequence is insuﬃcient. Instead, we need to be able to reason over pairs\nof text sequences.\n16.4.1 Natural Language Inference\nNatural language inference studies whether a hypothesis can be inferred from a premise, where\nboth are a text sequence. In other words, natural language inference determines the logi-\ncal relationship between a pair of text sequences. Such relationships usually fall into three\ntypes:\n• Entailment: the hypothesis can be inferred from the premise.\n• Contradiction: the negation of the hypothesis can be inferred from the premise.\n• Neutral: all the other cases.\nNatural language inference is also known as the recognizing textual entailment task. For ex-\nample, the following pair will be labeled as entailment because “showing aﬀection” in the\nhypothesis can be inferred from “hugging one another” in the premise.\nPremise: Two women are hugging each other.\nHypothesis: Two women are showing aﬀection.\nThe following is an example of contradiction as “running the coding example” indicates “not\nsleeping” rather than “sleeping”.\nPremise: A man is running the coding example from Dive into Deep Learning.\nHypothesis: The man is sleeping.\nThe third example shows a neutrality relationship because neither “famous” nor “not famous”\ncan be inferred from the fact that “are performing for us”.\nPremise: The musicians are performing for us.\nHypothesis: The musicians are famous.\nNatural language inference has been a central topic for understanding natural language. It\nenjoys wide applications ranging from information retrieval to open-domain question an-\nswering. To study this problem, we will begin by investigating a popular natural language\ninference benchmark dataset.\n\n788\nNatural Language Processing: Applications\n16.4.2 The Stanford Natural Language Inference (SNLI) Dataset\nStanford Natural Language Inference (SNLI) Corpus is a collection of over 500000 labeled\nEnglish sentence pairs (Bowman et al., 2015). We download and store the extracted SNLI\ndataset in the path ../data/snli_1.0.\nimport os\nimport re\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n#@save\nd2l.DATA_HUB['SNLI'] = (\n'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n'9fcde07509c7e87ec61c640c1b2753d9041758e4')\ndata_dir = d2l.download_extract('SNLI')\nReading the Dataset\nThe original SNLI dataset contains much richer information than what we really need in our\nexperiments. Thus, we deﬁne a function read_snli to only extract part of the dataset, then\nreturn lists of premises, hypotheses, and their labels.\n#@save\ndef read_snli(data_dir, is_train):\n\"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\ndef extract_text(s):\n# Remove information that will not be used by us\ns = re.sub('\\\\(', '', s)\ns = re.sub('\\\\)', '', s)\n# Substitute two or more consecutive whitespace with space\ns = re.sub('\\\\s{2,}', ' ', s)\nreturn s.strip()\nlabel_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\nfile_name = os.path.join(data_dir, 'snli_1.0_train.txt'\nif is_train else 'snli_1.0_test.txt')\nwith open(file_name, 'r') as f:\nrows = [row.split('\\t') for row in f.readlines()[1:]]\npremises = [extract_text(row[1]) for row in rows if row[0] in label_set]\nhypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\nlabels = [label_set[row[0]] for row in rows if row[0] in label_set]\nreturn premises, hypotheses, labels\nNow let’s print the ﬁrst 3 pairs of premise and hypothesis, as well as their labels (“0”, “1”,\nand “2” correspond to “entailment”, “contradiction”, and “neutral”, respectively ).\ntrain_data = read_snli(data_dir, is_train=True)\n(continues on next page)\n\n789\nNatural Language Inference and the Dataset\n(continued from previous page)\nfor x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\nprint('premise:', x0)\nprint('hypothesis:', x1)\nprint('label:', y)\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person is training his horse for a competition .\nlabel: 2\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person is at a diner , ordering an omelette .\nlabel: 1\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person is outdoors , on a horse .\nlabel: 0\nThe training set has about 550000 pairs, and the testing set has about 10000 pairs. The fol-\nlowing shows that the three labels “entailment”, “contradiction”, and “neutral” are balanced\nin both the training set and the testing set.\ntest_data = read_snli(data_dir, is_train=False)\nfor data in [train_data, test_data]:\nprint([[row for row in data[2]].count(i) for i in range(3)])\n[183416, 183187, 182764]\n[3368, 3237, 3219]\nDeﬁning a Class for Loading the Dataset\nBelow we deﬁne a class for loading the SNLI dataset by inheriting from the Dataset class\nin Gluon. The argument num_steps in the class constructor speciﬁes the length of a text\nsequence so that each minibatch of sequences will have the same shape. In other words,\ntokens after the ﬁrst num_steps ones in longer sequence are trimmed, while special tokens\n“<pad>” will be appended to shorter sequences until their length becomes num_steps. By\nimplementing the __getitem__ function, we can arbitrarily access the premise, hypothesis,\nand label with the index idx.\n#@save\nclass SNLIDataset(torch.utils.data.Dataset):\n\"\"\"A customized dataset to load the SNLI dataset.\"\"\"\ndef __init__(self, dataset, num_steps, vocab=None):\nself.num_steps = num_steps\nall_premise_tokens = d2l.tokenize(dataset[0])\nall_hypothesis_tokens = d2l.tokenize(dataset[1])\nif vocab is None:\nself.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n(continues on next page)\n\n790\nNatural Language Processing: Applications\n(continued from previous page)\nmin_freq=5, reserved_tokens=['<pad>'])\nelse:\nself.vocab = vocab\nself.premises = self._pad(all_premise_tokens)\nself.hypotheses = self._pad(all_hypothesis_tokens)\nself.labels = torch.tensor(dataset[2])\nprint('read ' + str(len(self.premises)) + ' examples')\ndef _pad(self, lines):\nreturn torch.tensor([d2l.truncate_pad(\nself.vocab[line], self.num_steps, self.vocab['<pad>'])\nfor line in lines])\ndef __getitem__(self, idx):\nreturn (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\ndef __len__(self):\nreturn len(self.premises)\nPutting It All Together\nNow we can invoke the read_snli function and the SNLIDataset class to download the\nSNLI dataset and return DataLoader instances for both training and testing sets, together\nwith the vocabulary of the training set. It is noteworthy that we must use the vocabulary\nconstructed from the training set as that of the testing set. As a result, any new token from\nthe testing set will be unknown to the model trained on the training set.\n#@save\ndef load_data_snli(batch_size, num_steps=50):\n\"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\nnum_workers = d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_data = read_snli(data_dir, True)\ntest_data = read_snli(data_dir, False)\ntrain_set = SNLIDataset(train_data, num_steps)\ntest_set = SNLIDataset(test_data, num_steps, train_set.vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size,\nshuffle=True,\nnum_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\nshuffle=False,\nnum_workers=num_workers)\nreturn train_iter, test_iter, train_set.vocab\nHere we set the batch size to 128 and sequence length to 50, and invoke the load_data_snli\nfunction to get the data iterators and vocabulary. Then we print the vocabulary size.\ntrain_iter, test_iter, vocab = load_data_snli(128, 50)\nlen(vocab)\n\n791\nNatural Language Inference and the Dataset\n246\nread 549367 examples\nread 9824 examples\n18678\nNow we print the shape of the ﬁrst minibatch. Contrary to sentiment analysis, we have two\ninputs X[0] and X[1] representing pairs of premises and hypotheses.\nfor X, Y in train_iter:\nprint(X[0].shape)\nprint(X[1].shape)\nprint(Y.shape)\nbreak\ntorch.Size([128, 50])\ntorch.Size([128, 50])\ntorch.Size([128])\n16.4.3 Summary\n• Natural language inference studies whether a hypothesis can be inferred from a premise,\nwhere both are a text sequence.\n• In natural language inference, relationships between premises and hypotheses include en-\ntailment, contradiction, and neutral.\n• Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of\nnatural language inference.\n16.4.4 Exercises\n1. Machine translation has long been evaluated based on superﬁcial n-gram matching be-\ntween an output translation and a ground-truth translation. Can you design a measure for\nevaluating machine translation results by using natural language inference?\n2. How can we change hyperparameters to reduce the vocabulary size?\nDiscussions246.\n\n792\nNatural Language Processing: Applications\n16.5 Natural Language Inference: Using Attention\nWe introduced the natural language inference task and the SNLI dataset in Section 16.4. In\nview of many models that are based on complex and deep architectures, Parikh et al. (2016)\nproposed to address natural language inference with attention mechanisms and called it a\n“decomposable attention model”. This results in a model without recurrent or convolutional\nlayers, achieving the best result at the time on the SNLI dataset with much fewer parameters.\nIn this section, we will describe and implement this attention-based method (with MLPs) for\nnatural language inference, as depicted in Fig. 16.5.1.\nt\nFig. 16.5.1\nThis section feeds pretrained GloVe to an architecture based on attention and MLPs for\nnatural language inference.\n16.5.1 The Model\nSimpler than preserving the order of tokens in premises and hypotheses, we can just align to-\nkens in one text sequence to every token in the other, and vice versa, then compare and aggre-\ngate such information to predict the logical relationships between premises and hypotheses.\nSimilar to alignment of tokens between source and target sentences in machine translation,\nthe alignment of tokens between premises and hypotheses can be neatly accomplished by\nattention mechanisms.\nFig. 16.5.2 depicts the natural language inference method using attention mechanisms. At a\nhigh level, it consists of three jointly trained steps: attending, comparing, and aggregating.\nWe will illustrate them step by step in the following.\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\n793\nNatural Language Inference: Using Attention\nt\nFig. 16.5.2\nNatural language inference using attention mechanisms.\nAttending\nThe ﬁrst step is to align tokens in one text sequence to each token in the other sequence.\nSuppose that the premise is “i do need sleep” and the hypothesis is “i am tired”. Due to\nsemantical similarity, we may wish to align “i” in the hypothesis with “i” in the premise, and\nalign “tired” in the hypothesis with “sleep” in the premise. Likewise, we may wish to align\n“i” in the premise with “i” in the hypothesis, and align “need” and “sleep” in the premise\nwith “tired” in the hypothesis. Note that such alignment is soft using weighted average, where\nideally large weights are associated with the tokens to be aligned. For ease of demonstration,\nFig. 16.5.2 shows such alignment in a hard way.\nNow we describe the soft alignment using attention mechanisms in more detail. Denote by\nA = (a1, . . ., am) and B = (b1, . . ., bn) the premise and hypothesis, whose number of\ntokens are m and n, respectively, where ai, bj ∈Rd (i = 1, . . ., m, j = 1, . . ., n) is a d-\ndimensional word vector. For soft alignment, we compute the attention weights eij ∈R\nas\neij = f (ai)⊤f (bj),\n(16.5.1)\nwhere the function f is an MLP deﬁned in the following mlp function. The output dimension\nof f is speciﬁed by the num_hiddens argument of mlp.\ndef mlp(num_inputs, num_hiddens, flatten):\nnet = []\nnet.append(nn.Dropout(0.2))\nnet.append(nn.Linear(num_inputs, num_hiddens))\nnet.append(nn.ReLU())\nif flatten:\nnet.append(nn.Flatten(start_dim=1))\nnet.append(nn.Dropout(0.2))\n(continues on next page)\n\n794\nNatural Language Processing: Applications\n(continued from previous page)\nnet.append(nn.Linear(num_hiddens, num_hiddens))\nnet.append(nn.ReLU())\nif flatten:\nnet.append(nn.Flatten(start_dim=1))\nreturn nn.Sequential(*net)\nIt should be highlighted that, in (16.5.1) f takes inputs ai and bj separately rather than takes\na pair of them together as input. This decomposition trick leads to only m + n applications\n(linear complexity) of f rather than mn applications (quadratic complexity).\nNormalizing the attention weights in (16.5.1), we compute the weighted average of all the\ntoken vectors in the hypothesis to obtain representation of the hypothesis that is softly aligned\nwith the token indexed by i in the premise:\nβi =\nn\n∑\nj=1\nexp(eij)\n∑n\nk=1 exp(eik)bj.\n(16.5.2)\nLikewise, we compute soft alignment of premise tokens for each token indexed by j in the\nhypothesis:\nαj =\nm\n∑\ni=1\nexp(eij)\n∑m\nk=1 exp(ek j)ai.\n(16.5.3)\nBelow we deﬁne the Attend class to compute the soft alignment of hypotheses (beta) with\ninput premises A and soft alignment of premises (alpha) with input hypotheses B.\nclass Attend(nn.Module):\ndef __init__(self, num_inputs, num_hiddens, **kwargs):\nsuper(Attend, self).__init__(**kwargs)\nself.f = mlp(num_inputs, num_hiddens, flatten=False)\ndef forward(self, A, B):\n# Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n# `embed_size`)\n# Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n# `num_hiddens`)\nf_A = self.f(A)\nf_B = self.f(B)\n# Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n# no. of tokens in sequence B)\ne = torch.bmm(f_A, f_B.permute(0, 2, 1))\n# Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n# `embed_size`), where sequence B is softly aligned with each token\n# (axis 1 of `beta`) in sequence A\nbeta = torch.bmm(F.softmax(e, dim=-1), B)\n# Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n# `embed_size`), where sequence A is softly aligned with each token\n# (axis 1 of `alpha`) in sequence B\nalpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\nreturn beta, alpha\n\n795\nNatural Language Inference: Using Attention\nComparing\nIn the next step, we compare a token in one sequence with the other sequence that is softly\naligned with that token. Note that in soft alignment, all the tokens from one sequence, though\nwith probably diﬀerent attention weights, will be compared with a token in the other se-\nquence. For easy of demonstration, Fig. 16.5.2 pairs tokens with aligned tokens in a hard\nway. For example, suppose that the attending step determines that “need” and “sleep” in the\npremise are both aligned with “tired” in the hypothesis, the pair “tired–need sleep” will be\ncompared.\nIn the comparing step, we feed the concatenation (operator [·, ·]) of tokens from one sequence\nand aligned tokens from the other sequence into a function g (an MLP):\nvA,i = g([ai, βi]), i = 1, . . ., m\nvB,j = g([bj, αj]), j = 1, . . ., n.\n(16.5.4)\nIn (16.5.4), vA,i is the comparison between token i in the premise and all the hypothesis\ntokens that are softly aligned with token i; while vB,j is the comparison between token j in\nthe hypothesis and all the premise tokens that are softly aligned with token j. The following\nCompare class deﬁnes such as comparing step.\nclass Compare(nn.Module):\ndef __init__(self, num_inputs, num_hiddens, **kwargs):\nsuper(Compare, self).__init__(**kwargs)\nself.g = mlp(num_inputs, num_hiddens, flatten=False)\ndef forward(self, A, B, beta, alpha):\nV_A = self.g(torch.cat([A, beta], dim=2))\nV_B = self.g(torch.cat([B, alpha], dim=2))\nreturn V_A, V_B\nAggregating\nWith two sets of comparison vectors vA,i (i = 1, . . ., m) and vB, j (j = 1, . . ., n) on hand,\nin the last step we will aggregate such information to infer the logical relationship. We begin\nby summing up both sets:\nvA =\nm\n∑\ni=1\nvA,i,\nvB =\nn\n∑\nj=1\nvB,j.\n(16.5.5)\nNext we feed the concatenation of both summarization results into function h (an MLP) to\nobtain the classiﬁcation result of the logical relationship:\nˆy = h([vA, vB]).\n(16.5.6)\nThe aggregation step is deﬁned in the following Aggregate class.\n\n796\nNatural Language Processing: Applications\nclass Aggregate(nn.Module):\ndef __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\nsuper(Aggregate, self).__init__(**kwargs)\nself.h = mlp(num_inputs, num_hiddens, flatten=True)\nself.linear = nn.Linear(num_hiddens, num_outputs)\ndef forward(self, V_A, V_B):\n# Sum up both sets of comparison vectors\nV_A = V_A.sum(dim=1)\nV_B = V_B.sum(dim=1)\n# Feed the concatenation of both summarization results into an MLP\nY_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\nreturn Y_hat\nPutting It All Together\nBy putting the attending, comparing, and aggregating steps together, we deﬁne the decom-\nposable attention model to jointly train these three steps.\nclass DecomposableAttention(nn.Module):\ndef __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\nnum_inputs_compare=200, num_inputs_agg=400, **kwargs):\nsuper(DecomposableAttention, self).__init__(**kwargs)\nself.embedding = nn.Embedding(len(vocab), embed_size)\nself.attend = Attend(num_inputs_attend, num_hiddens)\nself.compare = Compare(num_inputs_compare, num_hiddens)\n# There are 3 possible outputs: entailment, contradiction, and neutral\nself.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\ndef forward(self, X):\npremises, hypotheses = X\nA = self.embedding(premises)\nB = self.embedding(hypotheses)\nbeta, alpha = self.attend(A, B)\nV_A, V_B = self.compare(A, B, beta, alpha)\nY_hat = self.aggregate(V_A, V_B)\nreturn Y_hat\n16.5.2 Training and Evaluating the Model\nNow we will train and evaluate the deﬁned decomposable attention model on the SNLI\ndataset. We begin by reading the dataset.\nReading the dataset\nWe download and read the SNLI dataset using the function deﬁned in Section 16.4. The batch\nsize and sequence length are set to 256 and 50, respectively.\n\n797\nNatural Language Inference: Using Attention\nbatch_size, num_steps = 256, 50\ntrain_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)\nread 549367 examples\nread 9824 examples\nCreating the Model\nWe use the pretrained 100-dimensional GloVe embedding to represent the input tokens.\nThus, we predeﬁne the dimension of vectors ai and bj in (16.5.1) as 100. The output di-\nmension of functions f in (16.5.1) and g in (16.5.4) is set to 200. Then we create a model\ninstance, initialize its parameters, and load the GloVe embedding to initialize vectors of input\ntokens.\nembed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\nnet = DecomposableAttention(vocab, embed_size, num_hiddens)\nglove_embedding = d2l.TokenEmbedding('glove.6b.100d')\nembeds = glove_embedding[vocab.idx_to_token]\nnet.embedding.weight.data.copy_(embeds);\nTraining and Evaluating the Model\nIn contrast to the split_batch function in Section 13.5 that takes single inputs such as text\nsequences (or images), we deﬁne a split_batch_multi_inputs function to take multiple\ninputs such as premises and hypotheses in minibatches.\nNow we can train and evaluate the model on the SNLI dataset.\nlr, num_epochs = 0.001, 4\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction=\"none\")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.496, train acc 0.805, test acc 0.823\n9660.5 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\nUsing the Model\nFinally, deﬁne the prediction function to output the logical relationship between a pair of\npremise and hypothesis.\n\n798\nNatural Language Processing: Applications\n#@save\ndef predict_snli(net, vocab, premise, hypothesis):\n\"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\nnet.eval()\npremise = torch.tensor(vocab[premise], device=d2l.try_gpu())\nhypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\nlabel = torch.argmax(net([premise.reshape((1, -1)),\nhypothesis.reshape((1, -1))]), dim=1)\nreturn 'entailment' if label == 0 else 'contradiction' if label == 1 \\\nelse 'neutral'\nWe can use the trained model to obtain the natural language inference result for a sample\npair of sentences.\npredict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])\n'contradiction'\n16.5.3 Summary\n• The decomposable attention model consists of three steps for predicting the logical rela-\ntionships between premises and hypotheses: attending, comparing, and aggregating.\n• With attention mechanisms, we can align tokens in one text sequence to every token in the\nother, and vice versa. Such alignment is soft using weighted average, where ideally large\nweights are associated with the tokens to be aligned.\n• The decomposition trick leads to a more desirable linear complexity than quadratic com-\nplexity when computing attention weights.\n• We can use pretrained word vectors as the input representation for downstream natural\nlanguage processing task such as natural language inference.\n\n799\nFine-Tuning BERT for Sequence-Level and Token-Level Applications\n247\n16.5.4 Exercises\n1. Train the model with other combinations of hyperparameters. Can you get better accuracy\non the test set?\n2. What are major drawbacks of the decomposable attention model for natural language\ninference?\n3. Suppose that we want to get the level of semantical similarity (e.g., a continuous value\nbetween 0 and 1) for any pair of sentences. How shall we collect and label the dataset?\nCan you design a model with attention mechanisms?\nDiscussions247.\n16.6 Fine-Tuning BERT for Sequence-Level and\nToken-Level Applications\nIn the previous sections of this chapter, we have designed diﬀerent models for natural lan-\nguage processing applications, such as based on RNNs, CNNs, attention, and MLPs. These\nmodels are helpful when there is space or time constraint, however, crafting a speciﬁc model\nfor every natural language processing task is practically infeasible. In Section 15.8, we in-\ntroduced a pretraining model, BERT, that requires minimal architecture changes for a wide\nrange of natural language processing tasks. On the one hand, at the time of its proposal,\nBERT improved the state of the art on various natural language processing tasks. On the\nother hand, as noted in Section 15.10, the two versions of the original BERT model come\nwith 110 million and 340 million parameters. Thus, when there are suﬃcient computational\nresources, we may consider ﬁne-tuning BERT for downstream natural language processing\napplications.\nIn the following, we generalize a subset of natural language processing applications as sequence-\nlevel and token-level. On the sequence level, we introduce how to transform the BERT repre-\nsentation of the text input to the output label in single text classiﬁcation and text pair classiﬁ-\ncation or regression. On the token level, we will brieﬂy introduce new applications such as text\ntagging and question answering and shed light on how BERT can represent their inputs and\nget transformed into output labels. During ﬁne-tuning, the “minimal architecture changes”\nrequired by BERT across diﬀerent applications are the extra fully connected layers. During\nsupervised learning of a downstream application, parameters of the extra layers are learned\nfrom scratch while all the parameters in the pretrained BERT model are ﬁne-tuned.\n16.6.1 Single Text Classiﬁcation\n\n800\nNatural Language Processing: Applications\nSingle text classiﬁcation takes a single text sequence as input and outputs its classiﬁcation\nresult. Besides sentiment analysis that we have studied in this chapter, the Corpus of Linguistic\nAcceptability (CoLA) is also a dataset for single text classiﬁcation, judging whether a given\nsentence is grammatically acceptable or not (Warstadt et al., 2019). For instance, “I should\nstudy.” is acceptable but “I should studying.” is not.\nt\nFig. 16.6.1\nFine-tuning BERT for single text classiﬁcation applications, such as sentiment analysis\nand testing linguistic acceptability. Suppose that the input single text has six tokens.\nSection 15.8 describes the input representation of BERT. The BERT input sequence un-\nambiguously represents both single text and text pairs, where the special classiﬁcation token\n“<cls>” is used for sequence classiﬁcation and the special classiﬁcation token “<sep>” marks\nthe end of single text or separates a pair of text. As shown in Fig. 16.6.1, in single text clas-\nsiﬁcation applications, the BERT representation of the special classiﬁcation token “<cls>”\nencodes the information of the entire input text sequence. As the representation of the input\nsingle text, it will be fed into a small MLP consisting of fully connected (dense) layers to\noutput the distribution of all the discrete label values.\n16.6.2 Text Pair Classiﬁcation or Regression\nWe have also examined natural language inference in this chapter. It belongs to text pair\nclassiﬁcation, a type of application classifying a pair of text.\nTaking a pair of text as input but outputting a continuous value, semantic textual similarity is a\npopular text pair regression task. This task measures semantic similarity of sentences. For in-\nstance, in the Semantic Textual Similarity Benchmark dataset, the similarity score of a pair of\nsentences is an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning equivalence)\n(Cer et al., 2017). The goal is to predict these scores. Examples from the Semantic Textual\nSimilarity Benchmark dataset include (sentence 1, sentence 2, similarity score):\n• “A plane is taking oﬀ.”, “An air plane is taking oﬀ.”, 5.000;\n• “A woman is eating something.”, “A woman is eating meat.”, 3.000;\n\n801\nFine-Tuning BERT for Sequence-Level and Token-Level Applications\n• “A woman is dancing.”, “A man is talking.”, 0.000.\nt\nFig. 16.6.2\nFine-tuning BERT for text pair classiﬁcation or regression applications, such as natural\nlanguage inference and semantic textual similarity. Suppose that the input text pair has\ntwo and three tokens.\nComparing with single text classiﬁcation in Fig. 16.6.1, ﬁne-tuning BERT for text pair clas-\nsiﬁcation in Fig. 16.6.2 is diﬀerent in the input representation. For text pair regression tasks\nsuch as semantic textual similarity, trivial changes can be applied such as outputting a con-\ntinuous label value and using the mean squared loss: they are common for regression.\n16.6.3 Text Tagging\nNow let’s consider token-level tasks, such as text tagging, where each token is assigned a\nlabel. Among text tagging tasks, part-of-speech tagging assigns each word a part-of-speech\ntag (e.g., adjective and determiner) according to the role of the word in the sentence. For\nexample, according to the Penn Treebank II tag set, the sentence “John Smith ‘s car is new”\nshould be tagged as “NNP (noun, proper singular) NNP POS (possessive ending) NN (noun,\nsingular or mass) VB (verb, base form) JJ (adjective)”.\nFine-tuning BERT for text tagging applications is illustrated in Fig. 16.6.3. Comparing with\nFig. 16.6.1, the only distinction lies in that in text tagging, the BERT representation of every\ntoken of the input text is fed into the same extra fully connected layers to output the label of\nthe token, such as a part-of-speech tag.\n16.6.4 Question Answering\nAs another token-level application, question answering reﬂects capabilities of reading com-\nprehension. For example, the Stanford Question Answering Dataset (SQuAD v1.1) consists\nof reading passages and questions, where the answer to every question is just a segment of text\n(text span) from the passage that the question is about (Rajpurkar et al., 2016). To explain,\nconsider a passage “Some experts report that a mask’s eﬃcacy is inconclusive. However,\n\n802\nNatural Language Processing: Applications\nt\nFig. 16.6.3\nFine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose\nthat the input single text has six tokens.\nmask makers insist that their products, such as N95 respirator masks, can guard against the\nvirus.” and a question “Who say that N95 respirator masks can guard against the virus?”. The\nanswer should be the text span “mask makers” in the passage. Thus, the goal in SQuAD v1.1\nis to predict the start and end of the text span in the passage given a pair of question and\npassage.\nt\nFig. 16.6.4\nFine-tuning BERT for question answering. Suppose that the input text pair has two and\nthree tokens.\nTo ﬁne-tune BERT for question answering, the question and passage are packed as the ﬁrst\nand second text sequence, respectively, in the input of BERT. To predict the position of the\nstart of the text span, the same additional fully connected layer will transform the BERT\nrepresentation of any token from the passage of position i into a scalar score si. Such scores\nof all the passage tokens are further transformed by the softmax operation into a probability\ndistribution, so that each token position i in the passage is assigned a probability pi of being\nthe start of the text span. Predicting the end of the text span is the same as above, except that\nparameters in its additional fully connected layer are independent from those for predicting\nthe start. When predicting the end, any passage token of position i is transformed by the same\n\n803\nNatural Language Inference: Fine-Tuning BERT\n248\nfully connected layer into a scalar score ei. Fig. 16.6.4 depicts ﬁne-tuning BERT for question\nanswering.\nFor question answering, the supervised learning’s training objective is as straightforward as\nmaximizing the log-likelihoods of the ground-truth start and end positions. When predicting\nthe span, we can compute the score si + ej for a valid span from position i to position j\n(i ≤j), and output the span with the highest score.\n16.6.5 Summary\n• BERT requires minimal architecture changes (extra fully connected layers) for sequence-\nlevel and token-level natural language processing applications, such as single text clas-\nsiﬁcation (e.g., sentiment analysis and testing linguistic acceptability), text pair classi-\nﬁcation or regression (e.g., natural language inference and semantic textual similarity),\ntext tagging (e.g., part-of-speech tagging), and question answering.\n• During supervised learning of a downstream application, parameters of the extra layers\nare learned from scratch while all the parameters in the pretrained BERT model are\nﬁne-tuned.\n16.6.6 Exercises\n1. Let’s design a search engine algorithm for news articles. When the system receives an\nquery (e.g., “oil industry during the coronavirus outbreak”), it should return a ranked list\nof news articles that are most relevant to the query. Suppose that we have a huge pool\nof news articles and a large number of queries. To simplify the problem, suppose that the\nmost relevant article has been labeled for each query. How can we apply negative sampling\n(see Section 15.2.1) and BERT in the algorithm design?\n2. How can we leverage BERT in training language models?\n3. Can we leverage BERT in machine translation?\nDiscussions248.\n16.7 Natural Language Inference: Fine-Tuning\nBERT\nIn earlier sections of this chapter, we have designed an attention-based architecture (in Section\n16.5) for the natural language inference task on the SNLI dataset (as described in Section\n16.4). Now we revisit this task by ﬁne-tuning BERT. As discussed in Section 16.6, natural\n\n804\nNatural Language Processing: Applications\nlanguage inference is a sequence-level text pair classiﬁcation problem, and ﬁne-tuning BERT\nonly requires an additional MLP-based architecture, as illustrated in Fig. 16.7.1.\nt\nFig. 16.7.1\nThis section feeds pretrained BERT to an MLP-based architecture for natural language\ninference.\nIn this section, we will download a pretrained small version of BERT, then ﬁne-tune it for\nnatural language inference on the SNLI dataset.\nimport json\nimport multiprocessing\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n16.7.1 Loading Pretrained BERT\nWe have explained how to pretrain BERT on the WikiText-2 dataset in Section 15.9 and\nSection 15.10 (note that the original BERT model is pretrained on much bigger corpora). As\ndiscussed in Section 15.10, the original BERT model has hundreds of millions of parameters.\nIn the following, we provide two versions of pretrained BERT: “bert.base” is about as big as\nthe original BERT base model that requires a lot of computational resources to ﬁne-tune,\nwhile “bert.small” is a small version to facilitate demonstration.\nd2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n'225d66f04cae318b841a13d32af3acc165f253ac')\nd2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n'c72329e68a732bef0452e4b96a1c341c8910f81f')\nEither pretrained BERT model contains a “vocab.json” ﬁle that deﬁnes the vocabulary set\nand a “pretrained.params” ﬁle of the pretrained parameters. We implement the following\nload_pretrained_model function to load pretrained BERT parameters.\n\n805\nNatural Language Inference: Fine-Tuning BERT\ndef load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len, devices):\ndata_dir = d2l.download_extract(pretrained_model)\n# Define an empty vocabulary to load the predefined vocabulary\nvocab = d2l.Vocab()\nvocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\nvocab.token_to_idx = {token: idx for idx, token in enumerate(\nvocab.idx_to_token)}\nbert = d2l.BERTModel(\nlen(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens, num_heads=4,\nnum_blks=2, dropout=0.2, max_len=max_len)\n# Load pretrained BERT parameters\nbert.load_state_dict(torch.load(os.path.join(data_dir,\n'pretrained.params')))\nreturn bert, vocab\nTo facilitate demonstration on most of machines, we will load and ﬁne-tune the small version\n(“bert.small”) of the pretrained BERT in this section. In the exercise, we will show how to\nﬁne-tune the much larger “bert.base” to signiﬁcantly improve the testing accuracy.\ndevices = d2l.try_all_gpus()\nbert, vocab = load_pretrained_model(\n'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\nnum_blks=2, dropout=0.1, max_len=512, devices=devices)\n16.7.2 The Dataset for Fine-Tuning BERT\nFor the downstream task natural language inference on the SNLI dataset, we deﬁne a cus-\ntomized dataset class SNLIBERTDataset. In each example, the premise and hypothesis form\na pair of text sequence and is packed into one BERT input sequence as depicted in Fig. 16.6.2.\nRecall Section 15.8.4 that segment IDs are used to distinguish the premise and the hypothesis\nin a BERT input sequence. With the predeﬁned maximum length of a BERT input sequence\n(max_len), the last token of the longer of the input text pair keeps getting removed until\nmax_len is met. To accelerate generation of the SNLI dataset for ﬁne-tuning BERT, we use\n4 worker processes to generate training or testing examples in parallel.\nclass SNLIBERTDataset(torch.utils.data.Dataset):\ndef __init__(self, dataset, max_len, vocab=None):\nall_premise_hypothesis_tokens = [[\np_tokens, h_tokens] for p_tokens, h_tokens in zip(\n*[d2l.tokenize([s.lower() for s in sentences])\nfor sentences in dataset[:2]])]\nself.labels = torch.tensor(dataset[2])\nself.vocab = vocab\nself.max_len = max_len\n(self.all_token_ids, self.all_segments,\nself.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\nprint('read ' + str(len(self.all_token_ids)) + ' examples')\n(continues on next page)\n\n806\nNatural Language Processing: Applications\n(continued from previous page)\ndef _preprocess(self, all_premise_hypothesis_tokens):\npool = multiprocessing.Pool(4)\n# Use 4 worker processes\nout = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\nall_token_ids = [\ntoken_ids for token_ids, segments, valid_len in out]\nall_segments = [segments for token_ids, segments, valid_len in out]\nvalid_lens = [valid_len for token_ids, segments, valid_len in out]\nreturn (torch.tensor(all_token_ids, dtype=torch.long),\ntorch.tensor(all_segments, dtype=torch.long),\ntorch.tensor(valid_lens))\ndef _mp_worker(self, premise_hypothesis_tokens):\np_tokens, h_tokens = premise_hypothesis_tokens\nself._truncate_pair_of_tokens(p_tokens, h_tokens)\ntokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\ntoken_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n* (self.max_len - len(tokens))\nsegments = segments + [0] * (self.max_len - len(segments))\nvalid_len = len(tokens)\nreturn token_ids, segments, valid_len\ndef _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n# Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n# input\nwhile len(p_tokens) + len(h_tokens) > self.max_len - 3:\nif len(p_tokens) > len(h_tokens):\np_tokens.pop()\nelse:\nh_tokens.pop()\ndef __getitem__(self, idx):\nreturn (self.all_token_ids[idx], self.all_segments[idx],\nself.valid_lens[idx]), self.labels[idx]\ndef __len__(self):\nreturn len(self.all_token_ids)\nAfter downloading the SNLI dataset, we generate training and testing examples by instanti-\nating the SNLIBERTDataset class. Such examples will be read in minibatches during training\nand testing of natural language inference.\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\ndata_dir = d2l.download_extract('SNLI')\ntrain_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\ntest_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\nnum_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\nnum_workers=num_workers)\n\n807\nNatural Language Inference: Fine-Tuning BERT\nread 549367 examples\nread 9824 examples\n16.7.3 Fine-Tuning BERT\nAs Fig. 16.6.2 indicates, ﬁne-tuning BERT for natural language inference requires only an\nextra MLP consisting of two fully connected layers (see self.hidden and self.output\nin the following BERTClassifier class). This MLP transforms the BERT representation\nof the special “<cls>” token, which encodes the information of both the premise and the\nhypothesis, into three outputs of natural language inference: entailment, contradiction, and\nneutral.\nclass BERTClassifier(nn.Module):\ndef __init__(self, bert):\nsuper(BERTClassifier, self).__init__()\nself.encoder = bert.encoder\nself.hidden = bert.hidden\nself.output = nn.LazyLinear(3)\ndef forward(self, inputs):\ntokens_X, segments_X, valid_lens_x = inputs\nencoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\nreturn self.output(self.hidden(encoded_X[:, 0, :]))\nIn the following, the pretrained BERT model bert is fed into the BERTClassifier instance\nnet for the downstream application. In common implementations of BERT ﬁne-tuning, only\nthe parameters of the output layer of the additional MLP (net.output) will be learned from\nscratch. All the parameters of the pretrained BERT encoder (net.encoder) and the hidden\nlayer of the additional MLP (net.hidden) will be ﬁne-tuned.\nnet = BERTClassifier(bert)\nRecall that in Section 15.8 both the MaskLM class and the NextSentencePred class have\nparameters in their employed MLPs. These parameters are part of those in the pretrained\nBERT model bert, and thus part of parameters in net. However, such parameters are only\nfor computing the masked language modeling loss and the next sentence prediction loss dur-\ning pretraining. These two loss functions are irrelevant to ﬁne-tuning downstream applica-\ntions, thus the parameters of the employed MLPs in MaskLM and NextSentencePred are not\nupdated (staled) when BERT is ﬁne-tuned.\nTo allow parameters with stale gradients, the ﬂag ignore_stale_grad=True is set in the\nstep function of d2l.train_batch_ch13. We use this function to train and evaluate the\nmodel net using the training set (train_iter) and the testing set (test_iter) of SNLI.\nDue to the limited computational resources, the training and testing accuracy can be further\nimproved: we leave its discussions in the exercises.\n\n808\nNatural Language Processing: Applications\nlr, num_epochs = 1e-4, 5\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction='none')\nnet(next(iter(train_iter))[0])\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.524, train acc 0.788, test acc 0.777\n8176.1 examples/sec on [device(type='cuda', index=0), device(type='cuda',␣\n,→index=1)]\n16.7.4 Summary\n• We can ﬁne-tune the pretrained BERT model for downstream applications, such as natural\nlanguage inference on the SNLI dataset.\n• During ﬁne-tuning, the BERT model becomes part of the model for the downstream ap-\nplication. Parameters that are only related to pretraining loss will not be updated during\nﬁne-tuning.\n16.7.5 Exercises\n1. Fine-tune a much larger pretrained BERT model that is about as big as the original BERT\nbase model if your computational resource allows. Set arguments in the load_pretrained_model\nfunction as: replacing ‘bert.small’ with ‘bert.base’, increasing values of num_hiddens=256,\nffn_num_hiddens=512, num_heads=4, and num_blks=2 to 768, 3072, 12, and 12, re-\nspectively. By increasing ﬁne-tuning epochs (and possibly tuning other hyperparameters),\ncan you get a testing accuracy higher than 0.86?\n2. How to truncate a pair of sequences according to their ratio of length? Compare this pair\ntruncation method and the one used in the SNLIBERTDataset class. What are their pros\nand cons?\nDiscussions249.\n\n250\n17\nReinforcement Learning\nPratik Chaudhari (University of Pennsylvania and Amazon), Rasool Fakoor (Amazon),\nand Kavosh Asadi (Amazon)\nReinforcement Learning (RL) is a suite of techniques that allows us to build machine learning\nsystems that take decisions sequentially. For example, a package containing new clothes that\nyou purchased from an online retailer arrives at your doorstep after a sequence of decisions,\ne.g., the retailer ﬁnding the clothes in the warehouse closest to your house, putting the clothes\nin a box, transporting the box via land or by air, and delivering it to your house within the\ncity. There are many variables that aﬀect the delivery of the package along the way, e.g.,\nwhether or not the clothes were available in the warehouse, how long it took to transport\nthe box, whether it arrived in your city before the daily delivery truck left, etc. The key idea\nis that at each stage these variables that we do not often control aﬀect the entire sequence\nof events in the future, e.g., if there were delays in packing the box in the warehouse the\nretailer may need to send the package via air instead of ground to ensure a timely delivery.\nReinforcement Learning methods allow us to take the appropriate action at each stage of a\nsequential decision making problem in order to maximize some utility eventually, e.g., the\ntimely delivery of the package to you.\nSuch sequential decision making problems are seen in numerous other places, e.g., while\nplaying Go 250 your current move determines the next moves and the opponent’s moves are\nthe variables that you cannot control… a sequence of moves eventually determines whether\nor not you win; the movies that Netﬂix recommends to you now determine what you watch,\nwhether you like the movie or not is unknown to Netﬂix, eventually a sequence of movie\nrecommendations determines how satisﬁed you are with Netﬂix. Reinforcement learning is\nbeing used today to develop eﬀective solutions to these problems (Mnih et al., 2013, Silver\net al., 2016). The key distinction between reinforcement learning and standard deep learning\nis that in standard deep learning the prediction of a trained model on one test datum does\nnot aﬀect the predictions on a future test datum; in reinforcement learning decisions at future\ninstants (in RL, decisions are also called actions) are aﬀected by what decisions were made\nin the past.\nIn this chapter, we will develop the fundamentals of reinforcement learning and obtain hands-\non experience in implementing some popular reinforcement learning methods. We will ﬁrst\ndevelop a concept called a Markov Decision Process (MDP) which allows us to think of\nsuch sequential decision making problems. An algorithm called Value Iteration will be our\nﬁrst insight into solving reinforcement learning problems under the assumption that we know\nhow the uncontrolled variables in an MDP (in RL, these controlled variables are called the\n809\n\n810\nReinforcement Learning\nenvironment) typically behave. Using the more general version of Value Iteration, an algo-\nrithm called Q-Learning, we will be able to take appropriate actions even when we do not\nnecessarily have full knowledge of the environment. We will then study how to use deep net-\nworks for reinforcement learning problems by imitating the actions of an expert. And ﬁnally,\nwe will develop a reinforcement learning method that uses a deep network to take actions in\nunknown environments. These techniques form the basis of more advanced RL algorithms\nthat are used today in a variety of real-world applications, some of which we will point to in\nthe chapter.\nt\nFig. 17.1\nReinforcement Learning Structure\n17.1 Markov Decision Process (MDP)\nIn this section, we will discuss how to formulate reinforcement learning problems using\nMarkov decision processes (MDPs) and describe various components of MDPs in detail.\n17.1.1 Deﬁnition of an MDP\nA Markov decision process (MDP) (Bellman, 1957) is a model for how the state of a system\nevolves as diﬀerent actions are applied to the system. A few diﬀerent quantities come together\nto form an MDP.\n• Let S be the set of states in the MDP. As a concrete example see Fig. 17.1.1, for a robot\nthat is navigating a gridworld. In this case, S corresponds to the set of locations that the\nrobot can be at any given timestep.\n\n811\nMarkov Decision Process (MDP)\nt\nFig. 17.1.1\nA simple gridworld navigation task where the robot not only has to ﬁnd its way to the goal\nlocation (shown as a green house) but also has to avoid trap locations (shown as red cross\nsigns).\n• Let A be the set of actions that the robot can take at each state, e.g., “go forward”, “turn\nright”, “turn left”, “stay at the same location”, etc. Actions can change the current state\nof the robot to some other state within the set S.\n• It may happen that we do not know how the robot moves exactly but only know it up to\nsome approximation. We model this situation in reinforcement learning as follows: if\nthe robot takes an action “go forward”, there might be a small probability that it stays\nat the current state, another small probability that it “turns left”, etc. Mathematically,\nthis amounts to deﬁning a “transition function” T : S × A × S →[0, 1] such that\nT(s, a, s′) = P(s′ | s, a) using the conditional probability of reaching a state s′ given\nthat the robot was at state s and took an action a. The transition function is a probability\ndistribution and we therefore have ∑\ns′∈S T(s, a, s′) = 1 for all s ∈S and a ∈A, i.e.,\nthe robot has to go to some state if it takes an action.\n• We now construct a notion of which actions are useful and which ones are not using the\nconcept of a “reward” r : S × A →R. We say that the robot gets a reward r(s, a) if the\nrobot takes an action a at state s. If the reward r(s, a) is large, this indicates that taking\nthe action a at state s is more useful to achieving the goal of the robot, i.e., going to\nthe green house. If the reward r(s, a) is small, then action a is less useful to achieving\nthis goal. It is important to note that the reward is designed by the user (the person who\ncreates the reinforcement learning algorithm) with the goal in mind.\n17.1.2 Return and Discount Factor\n\n812\nReinforcement Learning\nThe diﬀerent components above together form a Markov decision process (MDP)\nMDP : (S, A,T, r).\n(17.1.1)\nLet’s now consider the situation when the robot starts at a particular state s0 ∈S and continues\ntaking actions to result in a trajectory\nτ = (s0, a0, r0, s1, a1, r1, s2, a2, r2, . . .).\n(17.1.2)\nAt each time step t the robot is at a state st and takes an action at which results in a reward\nrt = r(st, at). The return of a trajectory is the total reward obtained by the robot along such\na trajectory\nR(τ) = r0 + r1 + r2 + · · · .\n(17.1.3)\nThe goal in reinforcement learning is to ﬁnd a trajectory that has the largest return.\nThink of the situation when the robot continues to travel in the gridworld without ever reach-\ning the goal location. The sequence of states and actions in a trajectory can be inﬁnitely long\nin this case and the return of any such inﬁnitely long trajectory will be inﬁnite. In order to keep\nthe reinforcement learning formulation meaningful even for such trajectories, we introduce\nthe notion of a discount factor γ < 1. We write the discounted return as\nR(τ) = r0 + γr1 + γ2r2 + · · · =\n∞\n∑\nt=0\nγtrt.\n(17.1.4)\nNote that if γ is very small, the rewards earned by the robot in the far future, say t = 1000, are\nheavily discounted by the factor γ1000. This encourages the robot to select short trajectories\nthat achieve its goal, namely that of going to the green house in the gridwold example (see\nFig. 17.1.1). For large values of the discount factor, say γ = 0.99, the robot is encouraged\nto explore and then ﬁnd the best trajectory to go to the goal location.\n17.1.3 Discussion of the Markov Assumption\nLet us think of a new robot where the state st is the location as above but the action at is\nthe acceleration that the robot applies to its wheels instead of an abstract command like “go\nforward”. If this robot has some non-zero velocity at state st, then the next location st+1 is\na function of the past location st, the acceleration at, also the velocity of the robot at time t\nwhich is proportional to st −st−1. This indicates that we should have\nst+1 = some function(st, at, st−1);\n(17.1.5)\nthe “some function” in our case would be Newton’s law of motion. This is quite diﬀerent from\nour transition function that simply depends upon st and at.\nMarkov systems are all systems where the next state st+1 is only a function of the current\nstate st and the action at taken at the current state. In Markov systems, the next state does\nnot depend on which actions were taken in the past or the states that the robot was at in the\n\n813\nValue Iteration\n251\n252\n253\npast. For example, the new robot that has acceleration as the action above is not Markovian\nbecause the next location st+1 depends upon the previous state st−1 through the velocity.\nIt may seem that Markovian nature of a system is a restrictive assumption, but it is not so.\nMarkov Decision Processes are still capable of modeling a very large class of real systems.\nFor example, for our new robot, if we chose our state st to the tuple (location, velocity) then\nthe system is Markovian because its next state (locationt+1, velocityt+1) depends only upon\nthe current state (locationt, velocityt) and the action at the current state at.\n17.1.4 Summary\nThe reinforcement learning problem is typically modeled using Markov Decision Processes.\nA Markov decision process (MDP) is deﬁned by a tuple of four entities (S, A,T, r) where\nS is the state space, A is the action space, T is the transition function that encodes the tran-\nsition probabilities of the MDP and r is the immediate reward obtained by taking action at a\nparticular state.\n17.1.5 Exercises\n1. Suppose that we want to design an MDP to model MountainCar251 problem.\n1. What would be the set of states?\n2. What would be the set of actions?\n3. What would be the possible reward functions?\n2. How would you design an MDP for an Atari game like Pong game252?\nDiscussions253.\n17.2 Value Iteration\nIn this section we will discuss how to pick the best action for the robot at each state to max-\nimize the return of the trajectory. We will describe an algorithm called Value Iteration and\nimplement it for a simulated robot that travels over a frozen lake.\n17.2.1 Stochastic Policy\nA stochastic policy denoted as π(a | s) (policy for short) is a conditional distribution over\nthe actions a ∈A given the state s ∈S, π(a | s) ≡P(a | s). As an example, if the robot has\nfour actions A = {go left, go down, go right, go up}. The policy at a state s ∈S for such a\n\n814\nReinforcement Learning\nset of actions A is a categorical distribution where the probabilities of the four actions could\nbe [0.4, 0.2, 0.1, 0.3]; at some other state s′ ∈S the probabilities π(a | s′) of the same four\nactions could be [0.1, 0.1, 0.2, 0.6]. Note that we should have ∑\na π(a | s) = 1 for any state s.\nA deterministic policy is a special case of a stochastic policy in that the distribution π(a | s)\nonly gives non-zero probability to one particular action, e.g., [1, 0, 0, 0] for our example with\nfour actions.\nTo make the notation less cumbersome, we will often write π(s) as the conditional distribution\ninstead of π(a | s).\n17.2.2 Value Function\nImagine now that the robot starts at a state s0 and at each time instant, it ﬁrst samples an action\nfrom the policy at ∼π(st) and takes this action to result in the next state st+1. The trajectory\nτ = (s0, a0, r0, s1, a1, r1, . . .), can be diﬀerent depending upon which particular action at is\nsampled at intermediate instants. We deﬁne the average return R(τ) = ∑∞\nt=0 γtr(st, at) of\nall such trajectories\nV π(s0) = Eat∼π(st )\n[\nR(τ)\n]\n= Eat∼π(st )\n[ ∞\n∑\nt=0\nγtr(st, at)\n]\n,\n(17.2.1)\nwhere st+1 ∼P(st+1 | st, at) is the next state of the robot and r(st, at) is the instantaneous\nreward obtained by taking action at in state st at time t. This is called the “value function”\nfor the policy π. In simple words, the value of a state s0 for a policy π, denoted by V π(s0),\nis the expected γ-discounted return obtained by the robot if it begins at state s0 and takes\nactions from the policy π at each time instant.\nWe next break down the trajectory into two stages (i) the ﬁrst stage which corresponds to\ns0 →s1 upon taking the action a0, and (ii) a second stage which is the trajectory τ′ =\n(s1, a1, r1, . . .) thereafter. The key idea behind all algorithms in reinforcement learning is that\nthe value of state s0 can be written as the average reward obtained in the ﬁrst stage and the\nvalue function averaged over all possible next states s1. This is quite intuitive and arises from\nour Markov assumption: the average return from the current state is the sum of the average\nreturn from the next state and the average reward of going to the next state. Mathematically,\nwe write the two stages as\nV π(s0) = r(s0, a0) + γ Ea0∼π(s0)\n[\nEs1∼P(s1 |s0,a0)\n[\nV π(s1)\n]]\n.\n(17.2.2)\nThis decomposition is very powerful: it is the foundation of the principle of dynamic pro-\ngramming upon which all reinforcement learning algorithms are based. Notice that the second\nstage gets two expectations, one over the choices of the action a0 taken in the ﬁrst stage using\nthe stochastic policy and another over the possible states s1 obtained from the chosen ac-\ntion. We can write (17.2.2) using the transition probabilities in the Markov decision process\n(MDP) as\nV π(s) =\n∑\na∈A\nπ(a | s)\n[\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)V π(s′)\n]\n; for all s ∈S.\n(17.2.3)\n\n815\nValue Iteration\nAn important thing to notice here is that the above identity holds for all states s ∈S because\nwe can think of any trajectory that begins at that state and break down the trajectory into two\nstages.\n17.2.3 Action-Value Function\nIn implementations, it is often useful to maintain a quantity called the “action value” function\nwhich is a closely related quantity to the value function. This is deﬁned to be the average\nreturn of a trajectory that begins at s0 but when the action of the ﬁrst stage is ﬁxed to be\na0\nQπ(s0, a0) = r(s0, a0) + Eat∼π(st )\n[ ∞\n∑\nt=1\nγtr(st, at)\n]\n,\n(17.2.4)\nnote that the summation inside the expectation is from t = 1, . . ., ∞because the reward of\nthe ﬁrst stage is ﬁxed in this case. We can again break down the trajectory into two parts and\nwrite\nQπ(s, a) = r(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)\n∑\na′∈A\nπ(a′ | s′) Qπ(s′, a′); for all s ∈S, a ∈A.\n(17.2.5)\nThis version is the analog of (17.2.3) for the action value function.\n17.2.4 Optimal Stochastic Policy\nBoth the value function and the action-value function depend upon the policy that the robot\nchooses. We will next think of the “optimal policy” that achieves the maximal average re-\nturn\nπ∗= argmax\nπ\nV π(s0).\n(17.2.6)\nOf all possible stochastic policies that the robot could have taken, the optimal policy π∗\nachieves the largest average discounted return for trajectories starting from state s0. Let us\ndenote the value function and the action-value function of the optimal policy as V∗≡V π∗\nand Q∗≡Qπ∗.\nLet us observe that for a deterministic policy where there is only one action that is possible\nunder the policy at any given state. This gives us\nπ∗(s) = argmax\na∈A\n[\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a) V∗(s′)\n]\n.\n(17.2.7)\nA good mnemonic to remember this is that the optimal action at state s (for a deterministic\npolicy) is the one that maximizes the sum of reward r(s, a) from the ﬁrst stage and the average\nreturn of the trajectories starting from the next sate s′, averaged over all possible next states\ns′ from the second stage.\n\n816\nReinforcement Learning\n17.2.5 Principle of Dynamic Programming\nOur developement in the previous section in (17.2.2) or (17.2.5) can be turned into an algo-\nrithm to compute the optimal value function V∗or the action-value function Q∗, respectively.\nObserve that\nV∗(s) =\n∑\na∈A\nπ∗(a | s)\n[\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)V∗(s′)\n]\n; for all s ∈S.\n(17.2.8)\nFor a deterministic optimal policy π∗, since there is only one action that can be taken at state\ns, we can also write\nV∗(s) = argmaxa∈A\n{\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)V∗(s′)\n}\n(17.2.9)\nfor all states s ∈S. This identity is called the “principle of dynamic programming” (Bellman,\n1952, Bellman, 1957). It was formulated by Richard Bellman in 1950s and we can remember\nit as “the remainder of an optimal trajectory is also optimal”.\n17.2.6 Value Iteration\nWe can turn the principle of dynamic programming into an algorithm for ﬁnding the optimal\nvalue function called value iteration. The key idea behind value iteration is to think of this\nidentity as a set of constraints that tie together V∗(s) at diﬀerent states s ∈S. We initialize\nthe value function to some arbitrary values V0(s) for all states s ∈S. At the kth iteration, the\nValue Iteration algorithm updates the value function as\nVk+1(s) = max\na∈A\n{\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)Vk(s′)\n}\n; for all s ∈S.\n(17.2.10)\nIt turns out that as k →∞the value function estimated by the Value Iteration algorithm\nconverges to the optimal value function irrespective of the initialization V0,\nV∗(s) = lim\nk→∞Vk(s); for all states s ∈S.\n(17.2.11)\nThe same Value Iteration algorithm can be equivalently written using the action-value func-\ntion as\nQk+1(s, a) = r(s, a) + γ max\na′∈A\n∑\ns′∈S\nP(s′ | s, a)Qk(s′, a′); for all s ∈S, a ∈A.\n(17.2.12)\nIn this case we initialize Q0(s, a) to some arbitrary values for all s ∈S and a ∈A. Again\nwe have Q∗(s, a) = limk→∞Qk(s, a) for all s ∈S and a ∈A.\n17.2.7 Policy Evaluation\nValue Iteration enables us to compute the optimal value function, i.e., V π∗of the optimal de-\nterministic policy π∗. We can also use similar iterative updates to compute the value function\n\n817\nValue Iteration\n254\nassociated with any other, potentially stochastic, policy π. We again initialize V π\n0 (s) to some\narbitrary values for all states s ∈S and at the kth iteration, perform the updates\nV π\nk+1(s) =\n∑\na∈A\nπ(a | s)\n[\nr(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a)V π\nk (s′)\n]\n; for all s ∈S.\n(17.2.13)\nThis algorithm is known as policy evaluation and is useful to compute the value function\ngiven the policy. Again, it turns out that as k →∞these updates converge to the correct\nvalue function irrespective of the initialization V0,\nV π(s) = lim\nk→∞V π\nk (s); for all states s ∈S.\n(17.2.14)\nThe algorithm for computing the action-value function Qπ(s, a) of a policy π is analogous.\n17.2.8 Implementation of Value Iteration\nWe next show how to implement Value Iteration for a navigation problem called FrozenLake\nfrom Open AI Gym 254 . We ﬁrst need to setup the enviroment as shown in the following\ncode.\n%matplotlib inline\nimport random\nimport numpy as np\nfrom d2l import torch as d2l\nseed = 0\n# Random number generator seed\ngamma = 0.95\n# Discount factor\nnum_iters = 10\n# Number of iterations\nrandom.seed(seed)\n# Set the random seed to ensure results can be reproduced\nnp.random.seed(seed)\n# Now set up the environment\nenv_info = d2l.make_env('FrozenLake-v1', seed=seed)\nIn the FrozenLake environment, the robot moves on a 4 × 4 grid (these are the states) with\nactions that are “up” (↑), “down” (→), “left” (←), and “right” (→). The environment contains\na number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are\nunknown to the robot. To keep the problem simple, we assume the robot has reliable actions,\ni.e. P(s′ | s, a) = 1 for all s ∈S, a ∈A. If the robot reaches the goal, the trial ends and the\nrobot receives a reward of 1 irrespective of the action; the reward at any other state is 0 for\nall actions. The objective of the robot is to learn a policy that reaches the goal location (G)\nfrom a given start location (S) (this is s0) to maximize the return.\nThe following function implements Value Iteration, where env_info contains MDP and en-\nvironment related information and gamma is the discount factor:\ndef value_iteration(env_info, gamma, num_iters):\nenv_desc = env_info['desc']\n# 2D array shows what each item means\nprob_idx = env_info['trans_prob_idx']\n(continues on next page)\n\n818\nReinforcement Learning\n(continued from previous page)\nnextstate_idx = env_info['nextstate_idx']\nreward_idx = env_info['reward_idx']\nnum_states = env_info['num_states']\nnum_actions = env_info['num_actions']\nmdp = env_info['mdp']\nV\n= np.zeros((num_iters + 1, num_states))\nQ\n= np.zeros((num_iters + 1, num_states, num_actions))\npi = np.zeros((num_iters + 1, num_states))\nfor k in range(1, num_iters + 1):\nfor s in range(num_states):\nfor a in range(num_actions):\n# Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\nfor pxrds in mdp[(s,a)]:\n# mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\npr = pxrds[prob_idx]\n# p(s'\\mid s,a)\nnextstate = pxrds[nextstate_idx]\n# Next state\nreward = pxrds[reward_idx]\n# Reward\nQ[k,s,a] += pr * (reward + gamma * V[k - 1, nextstate])\n# Record max value and max action\nV[k,s] = np.max(Q[k,s,:])\npi[k,s] = np.argmax(Q[k,s,:])\nd2l.show_value_function_progress(env_desc, V[:-1], pi[:-1])\nvalue_iteration(env_info=env_info, gamma=gamma, num_iters=num_iters)\n\n819\nQ-Learning\n255\nThe above pictures show the policy (the arrow indicates the action) and value function (the\nchange in color shows how the value function changes over time from the initial value shown\nby dark color to the optimal value shown by light colors.). As we see, Value Iteration ﬁnds the\noptimal value function after 10 iterations and the goal state (G) can be reached starting from\nany state as long as it is not an H cell. Another interesting aspect of the implementation is\nthat in addition to ﬁnding the optimal value function, we also automatically found the optimal\npolicy π∗corresponding to this value function.\n17.2.9 Summary\nThe main idea behind the Value Iteration algorithm is to use the principle of dynamic pro-\ngramming to ﬁnd the optimal average return obtained from a given state. Note that imple-\nmenting the Value Iteration algorithm requires that we know the Markov decision process\n(MDP), e.g., the transition and reward functions, completely.\n17.2.10 Exercises\n1. Try increasing the grid size to 8 × 8. Compared with 4 × 4 grid, how many iterations does\nit take to ﬁnd the optimal value function?\n2. What is the computational complexity of the Value Iteration algorithm?\n3. Run the Value Iteration algorithm again with γ (i.e. “gamma” in the above code) when it\nequals to 0, 0.5, and 1 and analyze its results.\n4. How does the value of γ aﬀect the number of iterations taken by Value Iteration to con-\nverge? What happens when γ = 1?\nDiscussions255.\n17.3 Q-Learning\nIn the previous section, we discussed the Value Iteration algorithm which requires accessing\nthe complete Markov decision process (MDP), e.g., the transition and reward functions. In\nthis section, we will look at Q-Learning (Watkins and Dayan, 1992) which is an algorithm\nto learn the value function without necessarily knowing the MDP. This algorithm embodies\nthe central idea behind reinforcement learning: it will enable the robot to obtain its own\ndata.\n\n820\nReinforcement Learning\n17.3.1 The Q-Learning Algorithm\nRecall that value iteration for the action-value function in Value Iteration (page 813) corre-\nsponds to the update\nQk+1(s, a) = r(s, a) + γ\n∑\ns′∈S\nP(s′ | s, a) max\na′∈A Qk(s′, a′); for all s ∈S and a ∈A.\n(17.3.1)\nAs we discussed, implementing this algorithm requires knowing the MDP, speciﬁcally the\ntransition function P(s′ | s, a). The key idea behind Q-Learning is to replace the summation\nover all s′ ∈S in the above expression by a summation over the states visited by the robot.\nThis allows us to subvert the need to know the transition function.\n17.3.2 An Optimization Problem Underlying Q-Learning\nLet us imagine that the robot uses a policy πe(a | s) to take actions. Just like the previous\nchapter, it collects a dataset of n trajectories of T timesteps each {(si\nt, ai\nt)t=0,...,T−1}i=1,...,n.\nRecall that value iteration is really a set of constraints that ties together the action-value\nQ∗(s, a) of diﬀerent states and actions to each other. We can implement an approximate\nversion of value iteration using the data that the robot has collected using πe as\nˆQ = min\nQ\n1\nnT\nn\n∑\ni=1\nT−1\n∑\nt=0\n(Q(si\nt, ai\nt) −r(si\nt, ai\nt) −γ max\na′ Q(si\nt+1, a′))2\n|                                                                   {z                                                                   }\ndef\n=ℓ(Q)\n.\n(17.3.2)\nLet us ﬁrst observe the similarities and diﬀerences between this expression and value iteration\nabove. If the robot’s policy πe were equal to the optimal policy π∗, and if it collected an\ninﬁnite amount of data, then this optimization problem would be identical to the optimization\nproblem underlying value iteration. But while value iteration requires us to know P(s′ | s, a),\nthe optimization objective does not have this term. We have not cheated: as the robot uses\nthe policy πe to take an action ai\nt at state si\nt, the next state si\nt+1 is a sample drawn from the\ntransition function. So the optimization objective also has access to the transition function,\nbut implicitly in terms of the data collected by the robot.\nThe variables of our optimization problem are Q(s, a) for all s ∈S and a ∈A. We can\nminimize the objective using gradient descent. For every pair (si\nt, ai\nt) in our dataset, we can\nwrite\nQ(si\nt, ai\nt) ←Q(si\nt, ai\nt) −α∇Q(si\nt,ai\nt )ℓ(Q)\n= (1 −α)Q(si\nt, ai\nt) −α\n(\nr(si\nt, ai\nt) + γ max\na′ Q(si\nt+1, a′)\n)\n,\n(17.3.3)\nwhere α is the learning rate. Typically in real problems, when the robot reaches the goal\nlocation, the trajectories end. The value of such a terminal state is zero because the robot\ndoes not take any further actions beyond this state. We should modify our update to handle\n\n821\nQ-Learning\nsuch states as\nQ(si\nt, ai\nt) = (1 −α)Q(si\nt, ai\nt) −α\n(\nr(si\nt, ai\nt) + γ(1 −⊮si\nt+1 is terminal) max\na′ Q(si\nt+1, a′)\n)\n.\n(17.3.4)\nwhere ⊮si\nt+1 is terminal is an indicator variable that is one if si\nt+1 is a terminal state and zero\notherwise. The value of state-action tuples (s, a) that are not a part of the dataset is set to\n−∞. This algorithm is known as Q-Learning.\nGiven the solution of these updates ˆQ, which is an approximation of the optimal value func-\ntion Q∗, we can obtain the optimal deterministic policy corresponding to this value function\neasily using\nˆπ(s) = argmaxa ˆQ(s, a).\n(17.3.5)\nThere can be situations when there are multiple deterministic policies that correspond to the\nsame optimal value function; such ties can be broken arbitrarily because they have the same\nvalue function.\n17.3.3 Exploration in Q-Learning\nThe policy used by the robot to collect data πe is critical to ensure that Q-Learning works well.\nAfterall, we have replaced the expectation over s′ using the transition function P(s′ | s, a)\nusing the data collected by the robot. If the policy πe does not reach diverse parts of the state-\naction space, then it is easy to imagine our estimate ˆQ will be a poor approximation of the\noptimal Q∗. It is also important to note that in such a situation, the estimate of Q∗at all states\ns ∈S will be bad, not just the ones visited by πe. This is because the Q-Learning objective\n(or value iteration) is a constraint that ties together the value of all state-action pairs. It is\ntherefore critical to pick the correct policy πe to collect data.\nWe can mitigate this concern by picking a completely random policy πe that samples actions\nuniformly randomly from A. Such a policy would visit all states, but it will take a large\nnumber of trajectories before it does so.\nWe thus arrive at the second key idea in Q-Learning, namely exploration. Typical implemen-\ntations of Q-Learning tie together the current estimate of Q and the policy πe to set\nπe(a | s) =\n{\nargmaxa′ ˆQ(s, a′)\nwith prob. 1 −ϵ\nuniform(A)\nwith prob. ϵ,\n(17.3.6)\nwhere ϵ is called the “exploration parameter” and is chosen by the user. The policy πe is called\nan exploration policy. This particular πe is called an ϵ-greedy exploration policy because it\nchooses the optimal action (under the current estimate ˆQ) with probability 1 −ϵ but explores\nrandomly with the remainder probability ϵ. We can also use the so-called softmax exploration\npolicy\nπe(a | s) =\ne ˆ\nQ(s,a)/T\n∑\na′ e ˆ\nQ(s,a′)/T ;\n(17.3.7)\n\n822\nReinforcement Learning\n256\nwhere the hyper-parameter T is called temperature. A large value of ϵ in ϵ-greedy policy\nfunctions similarly to a large value of temperature T for the softmax policy.\nIt is important to note that when we pick an exploration that depends upon the current estimate\nof the action-value function ˆQ, we need to resolve the optimization problem periodically.\nTypical implementations of Q-Learning make one mini-batch update using a few state-action\npairs in the collected dataset (typically the ones collected from the previous timestep of the\nrobot) after taking every action using πe.\n17.3.4 The “Self-correcting” Property of Q-Learning\nThe dataset collected by the robot during Q-Learning grows with time. Both the exploration\npolicy πe and the estimate ˆQ evolve as the robot collects more data. This gives us a key in-\nsight into why Q-Learning works well. Consider a state s: if a particular action a has a large\nvalue under the current estimate ˆQ(s, a), then both the ϵ-greedy and the softmax exploration\npolicies have a larger probability of picking this action. If this action actually is not the ideal\naction, then the future states that arise from this action will have poor rewards. The next\nupdate of the Q-Learning objective will therefore reduce the value ˆQ(s, a), which will re-\nduce the probability of picking this action the next time the robot visits state s. Bad actions,\ne.g., ones whose value is overestimated in ˆQ(s, a), are explored by the robot but their value\nis correct in the next update of the Q-Learning objective. Good actions, e.g., whose value\nˆQ(s, a) is large, are explored more often by the robot and thereby reinforced. This property\ncan be used to show that Q-Learning can converge to the optimal policy even if it begins with\na random policy πe (Watkins and Dayan, 1992).\nThis ability to not only collect new data but also collect the right kind of data is the central\nfeature of reinforcement learning algorithms, and this is what distinguishes them from su-\npervised learning. Q-Learning, using deep neural networks (which we will see in the DQN\nchapeter later), is responsible for the resurgence of reinforcement learning (Mnih et al.,\n2013).\n17.3.5 Implementation of Q-Learning\nWe now show how to implement Q-Learning on FrozenLake from Open AI Gym256 . Note\nthis is the same setup as we consider in Value Iteration (page 813) experiment.\n%matplotlib inline\nimport random\nimport numpy as np\nfrom d2l import torch as d2l\nseed = 0\n# Random number generator seed\ngamma = 0.95\n# Discount factor\nnum_iters = 256\n# Number of iterations\nalpha\n= 0.9\n# Learing rate\nepsilon = 0.9\n# Epsilon in epsilion gready algorithm\n(continues on next page)\n\n823\nQ-Learning\n(continued from previous page)\nrandom.seed(seed)\n# Set the random seed\nnp.random.seed(seed)\n# Now set up the environment\nenv_info = d2l.make_env('FrozenLake-v1', seed=seed)\nIn the FrozenLake environment, the robot moves on a 4 × 4 grid (these are the states) with\nactions that are “up” (↑), “down” (→), “left” (←), and “right” (→). The environment contains\na number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are\nunknown to the robot. To keep the problem simple, we assume the robot has reliable actions,\ni.e. P(s′ | s, a) = 1 for all s ∈S, a ∈A. If the robot reaches the goal, the trial ends and the\nrobot receives a reward of 1 irrespective of the action; the reward at any other state is 0 for\nall actions. The objective of the robot is to learn a policy that reaches the goal location (G)\nfrom a given start location (S) (this is s0) to maximize the return.\nWe ﬁrst implement ϵ-greedy method as follows:\ndef e_greedy(env, Q, s, epsilon):\nif random.random() < epsilon:\nreturn env.action_space.sample()\nelse:\nreturn np.argmax(Q[s,:])\nWe are now ready to implement Q-learning:\ndef q_learning(env_info, gamma, num_iters, alpha, epsilon):\nenv_desc = env_info['desc']\n# 2D array specifying what each grid item␣\n,→means\nenv = env_info['env']\n# 2D array specifying what each grid item means\nnum_states = env_info['num_states']\nnum_actions = env_info['num_actions']\nQ\n= np.zeros((num_states, num_actions))\nV\n= np.zeros((num_iters + 1, num_states))\npi = np.zeros((num_iters + 1, num_states))\nfor k in range(1, num_iters + 1):\n# Reset environment\nstate, done = env.reset(), False\nwhile not done:\n# Select an action for a given state and acts in env based on␣\n,→selected action\naction = e_greedy(env, Q, state, epsilon)\nnext_state, reward, done, _ = env.step(action)\n# Q-update:\ny = reward + gamma * np.max(Q[next_state,:])\nQ[state, action] = Q[state, action] + alpha * (y - Q[state,␣\n,→action])\n(continues on next page)\n\n824\nReinforcement Learning\n(continued from previous page)\n# Move to the next state\nstate = next_state\n# Record max value and max action for visualization purpose only\nfor s in range(num_states):\nV[k,s]\n= np.max(Q[s,:])\npi[k,s] = np.argmax(Q[s,:])\nd2l.show_Q_function_progress(env_desc, V[:-1], pi[:-1])\nq_learning(env_info=env_info, gamma=gamma, num_iters=num_iters, alpha=alpha,␣\n,→epsilon=epsilon)\nThis result shows that Q-learning can ﬁnd the optimal solution for this problem roughly after\n250 iterations. However, when we compare this result with the Value Iteration algorithm’s\nresult (see Implementation of Value Iteration (page 817)), we can see that the Value Iteration\nalgorithm needs way fewer iterations to ﬁnd the optimal solution for this problem. This hap-\npens because the Value Iteration algorithm has access to the full MDP whereas Q-learning\ndoes not.\n17.3.6 Summary\nQ-learning is one of the most fundamental reinforcement-learning algorithms. It has been\nat the epicenter of the recent success of reinforcement learning, most notably in learning\n\n825\nQ-Learning\n257\nto play video games (Mnih et al., 2013). Implementing Q-learning does not require that we\nknow the Markov decision process (MDP), e.g., the transition and reward functions, com-\npletely.\n17.3.7 Exercises\n1. Try increasing the grid size to 8 × 8. Compared with 4 × 4 grid, how many iterations does\nit take to ﬁnd the optimal value function?\n2. Run the Q-learning algorithm again with γ (i.e. “gamma” in the above code) when it equals\nto 0, 0.5, and 1 and analyze its results.\n3. Run the Q-learning algorithm again with ϵ (i.e. “epsilon” in the above code) when it equals\nto 0, 0.5, and 1 and analyze its results.\nDiscussions257.\n\n258\n18\nGaussian Processes\nAndrew Gordon Wilson (New York University and Amazon)\nGaussian processes (GPs) are ubitiquous. You have already encountered many examples of\nGPs without realizing it. Any model that is linear in its parameters with a Gaussian distribu-\ntion over the parameters is a Gaussian process. This class spans discrete models, including\nrandom walks, and autoregressive processes, as well as continuous models, including Bayesian\nlinear regression models, polynomials, Fourier series, radial basis functions, and even neural\nnetworks with an inﬁnite number of hidden units. There is a running joke that “everything is\na special case of a Gaussian process”.\nLearning about Gaussian processes is important for three reasons: (1) they provide a func-\ntion space perspective of modelling, which makes understanding a variety of model classes,\nincluding deep neural networks, much more approachable; (2) they have an extraordinary\nrange of applications where they are state-of-the-art, including active learning, hyperparam-\neter learning, auto-ML, and spatiotemporal regression; (3) over the last few years, algorithmic\nadvances have made Gaussian processes increasingly scalable and relevant, harmonizing with\ndeep learning through frameworks such as GPyTorch258 (Gardner et al., 2018). Indeed, GPs\nand and deep neural networks are not competing approaches, but highly complementary, and\ncan be combined to great eﬀect. These algorithmic advances are not just relevant to Gaus-\nsian processes, but provide a foundation in numerical methods that is broadly useful in deep\nlearning.\nIn this chapter, we introduce Gaussian processes. In the introductory notebook, we start by\nreasoning intuitively about what Gaussian processes are and how they directly model func-\ntions. In the priors notebook, we focus on how to specify Gaussian process priors. We di-\nrectly connect the tradiational weight-space approach to modelling to function space, which\nwill help us reason about constructing and understanding machine learning models, including\ndeep neural networks. We then introduce popular covariance functions, also known as ker-\nnels, which control the generalization properties of a Gaussian process. A GP with a given\nkernel deﬁnes a prior over functions. In the inference notebook, we will show how to use data\nto infer a posterior, in order to make predictions. This notebook contains from-scratch code\nfor making predictions with a Gaussian process, as well as an introduction to GPyTorch. In\nupcoming notebooks, we will introduce the numerics behind Gaussian processes, which is\nuseful for scaling Gaussian processes but also a powerful general foundation for deep learn-\ning, and advanced use-cases such as hyperparameter tuning in deep learning. Our examples\nwill make use of GPyTorch, which makes Gaussian processes scale, and is closely integrated\nwith deep learning functionality and PyTorch.\n826\n\n827\nIntroduction to Gaussian Processes\n18.1 Introduction to Gaussian Processes\nIn many cases, machine learning amounts to estimating parameters from data. These param-\neters are often numerous and relatively uninterpretable — such as the weights of a neural\nnetwork. Gaussian processes, by contrast, provide a mechanism for directly reasoning about\nthe high-level properties of functions that could ﬁt our data. For example, we may have a\nsense of whether these functions are quickly varying, periodic, involve conditional indepen-\ndencies, or translation invariance. Gaussian processes enable us to easily incorporate these\nproperties into our model, by directly specifying a Gaussian distribution over the function\nvalues that could ﬁt our data.\nLet’s get a feel for how Gaussian processes operate, by starting with some examples.\nSuppose we observe the following dataset, of regression targets (outputs), y, indexed by in-\nputs, x. As an example, the targets could be changes in carbon dioxide concentrations, and\nthe inputs could be the times at which these targets have been recorded. What are some fea-\ntures of the data? How quickly does it seem to varying? Do we have data points collected at\nregular intervals, or are there missing inputs? How would you imagine ﬁlling in the missing\nregions, or forecasting up until x = 25?\nt\nFig. 18.1.1\nObserved data.\nIn order to ﬁt the data with a Gaussian process, we start by specifying a prior distribution\nover what types of functions we might believe to be reasonable. Here we show several sam-\nple functions from a Gaussian process. Does this prior look reasonable? Note here we are\nnot looking for functions that ﬁt our dataset, but instead for specifying reasonable high-level\nproperties of the solutions, such as how quickly they vary with inputs. Note that we will see\ncode for reproducing all of the plots in this notebook, in the next notebooks on priors and\ninference.\nOnce we condition on data, we can use this prior to infer a posterior distribution over functions\nthat could ﬁt the data. Here we show sample posterior functions.\n\n828\nGaussian Processes\nt\nFig. 18.1.2\nSample prior functions that we may want to represent with our model.\nt\nFig. 18.1.3\nSample posterior functions, once we have observed the data.\nWe see that each of these functions are entirely consistent with our data, perfectly running\nthrough each observation. In order to use these posterior samples to make predictions, we can\naverage the values of every possible sample function from the posterior, to create the curve\nbelow, in thick blue. Note that we do not actually have to take an inﬁnite number of samples\nto compute this expectation; as we will see later, we can compute the expectation in closed\nform.\nt\nFig. 18.1.4\nPosterior samples, alongside posterior mean, which can be used for point predictions, in\nblue.\n\n829\nIntroduction to Gaussian Processes\nWe may also want a representation of uncertainty, so we know how conﬁdent we should be in\nour predictions. Intuitively, we should have more uncertainty where there is more variability\nin the sample posterior functions, as this tells us there are many more possible values the\ntrue function could take. This type of uncertainty is called epistemic uncertainty, which is\nthe reducible uncertainty associated with lack of information. As we acquire more data, this\ntype of uncertainty disappears, as there will be increasingly fewer solutions consistent with\nwhat we observe. Like with the posterior mean, we can compute the posterior variance (the\nvariability of these functions in the posterior) in closed form. With shade, we show two times\nthe posterior standard deviation on either side of the mean, creating a credible interval that\nhas a 95% probability of containing the true value of the function for any input x.\nt\nFig. 18.1.5\nPosterior samples, including 95% credible set.\nThe plot looks somewhat cleaner if we remove the posterior samples, simply visualizing the\ndata, posterior mean, and 95% credible set. Notice how the uncertainty grows away from the\ndata, a property of epistemic uncertainty.\nt\nFig. 18.1.6\nPoint predictions, and credible set.\nThe properties of the Gaussian process that we used to ﬁt the data are strongly controlled by\nwhat’s called a covariance function, also known as a kernel. The covariance function we used\nis called the RBF (Radial Basis Function) kernel, which has the form\nkRBF(x, x′) = Cov( f (x), f (x′)) = a2 exp\n(\n−1\n2ℓ2 ||x −x′||2\n)\n(18.1.1)\n\n830\nGaussian Processes\nThe hyperparameters of this kernel are interpretable. The amplitude parameter a controls the\nvertical scale over which the function is varying, and the length-scale parameter ℓcontrols\nthe rate of variation (the wiggliness) of the function. Larger a means larger function values,\nand larger ℓmeans more slowly varying functions. Let’s see what happens to our sample prior\nand posterior functions as we vary a and ℓ.\nThe length-scale has a particularly pronounced eﬀect on the predictions and uncertainty of a\nGP. At ||x −x′|| = ℓ, the covariance between a pair of function values is a2 exp(−0.5). At\nlarger distances than ℓ, the values of the function values becomes nearly uncorrelated. This\nmeans that if we want to make a prediction at a point x∗, then function values with inputs x\nsuch that ||x −x′|| > ℓwill not have a strong eﬀect on our predictions.\nLet’s see how changing the lengthscale aﬀects sample prior and posterior functions, and cred-\nible sets. The above ﬁts use a length-scale of 2. Let’s now consider ℓ= 0.1, 0.5, 2, 5, 10 . A\nlength-scale of 0.1 is very small relative to the range of the input domain we are considering,\n25. For example, the values of the function at x = 5 and x = 10 will have essentially no\ncorrelation at such a length-scale. On the other hand, for a length-scale of 10, the function\nvalues at these inputs will be highly correlated. Note that the vertical scale changes in the\nfollowing ﬁgures.\n\n831\nIntroduction to Gaussian Processes\n\n832\nGaussian Processes\nNotice as the length-scale increases the ‘wiggliness’ of the functions decrease, and our un-\ncertainty decreases. If the length-scale is small, the uncertainty will quickly increase as we\nmove away from the data, as the datapoints become less informative about the function val-\nues.\nNow, let’s vary the amplitude parameter, holding the length-scale ﬁxed at 2. Note the vertical\nscale is held ﬁxed for the prior samples, and varies for the posterior samples, so you can\nclearly see both the increasing scale of the function, and the ﬁts to the data.\n\n833\nIntroduction to Gaussian Processes\n\n834\nGaussian Processes\nWe see the amplitude parameter aﬀects the scale of the function, but not the rate of variation.\nAt this point, we also have the sense that the generalization performance of our procedure\nwill depend on having reasonable values for these hyperparameters. Values of ℓ= 2 and\na = 1 appeared to provide reasonable ﬁts, while some of the other values did not. Fortunately,\nthere is a robust and automatic way to specify these hyperparameters, using what is called\nthe marginal likelihood, which we will return to in the notebook on inference.\nSo what is a GP, really? As we started, a GP simply says that any collection of function val-\nues f (x1), . . ., f (xn), indexed by any collection of inputs x1, . . ., xn has a joint multivariate\nGaussian distribution. The mean vector µ of this distribution is given by a mean function,\n\n835\nIntroduction to Gaussian Processes\nwhich is typically taken to be a constant or zero. The covariance matrix of this distribution\nis given by the kernel evaluated at all pairs of the inputs x.\n\nf (x)\nf (x1)\n...\nf (xn)\n\n∼N\n©­­­­­\n«\nµ,\n\nk(x, x)\nk(x, x1)\n. . .\nk(x, xn)\nk(x1, x)\nk(x1, x1)\n. . .\nk(x1, xn)\n...\n...\n...\n...\nk(xn, x)\nk(xn, x1)\n. . .\nk(xn, xn)\n\nª®®®®®\n¬\n(18.1.2)\nEquation (18.1.2) speciﬁes a GP prior. We can compute the conditional distribution of f (x)\nfor any x given f (x1), . . ., f (xn), the function values we have observed. This conditional\ndistribution is called the posterior, and it is what we use to make predictions.\nIn particular,\nf (x)| f (x1), . . ., f (xn) ∼N(m, s2)\n(18.1.3)\nwhere\nm = k(x, x1:n)k(x1:n, x1:n)−1 f (x1:n)\n(18.1.4)\ns2 = k(x, x) −k(x, x1:n)k(x1:n, x1:n)−1k(x, x1:n)\n(18.1.5)\nwhere k(x, x1:n) is a 1×n vector formed by evaluating k(x, xi) fori = 1, . . ., n and k(x1:n, x1:n)\nis an n × n matrix formed by evaluating k(xi, xj) for i, j = 1, . . ., n. m is what we can use\nas a point predictor for any x, and s2 is what we use for uncertainty: if we want to create an\ninterval with a 95% probability that f (x) is in the interval, we would use m ± 2s. The pre-\ndictive means and uncertainties for all the above ﬁgures were created using these equations.\nThe observed data points were given by f (x1), . . ., f (xn) and chose a ﬁne grained set of x\npoints to make predictions.\nLet’s suppose we observe a single datapoint, f (x1), and we want to determine the value\nof f (x) at some x. Because f (x) is described by a Gaussian process, we know the joint\ndistribution over ( f (x), f (x1)) is Gaussian:\n[ f (x)\nf (x1)\n]\n∼N\n(\nµ,\n[ k(x, x)\nk(x, x1)\nk(x1, x)\nk(x1, x1)\n])\n(18.1.6)\nThe oﬀ-diagonal expression k(x, x1) = k(x1, x) tells us how correlated the function values\nwill be — how strongly determined f (x) will be from f (x1). We have seen already that if\nwe use a large length-scale, relative to the distance between x and x1, ||x −x1||, then the\nfunction values will be highly correlated. We can visualize the process of determining f (x)\nfrom f (x1) both in the space of functions, and in the joint distribution over f (x1), f (x).\nLet’s initially consider an x such that k(x, x1) = 0.9, and k(x, x) = 1, meaning that the\nvalue of f (x) is moderately correlated with the value of f (x1). In the joint distribution, the\ncontours of constant probability will be relatively narrow ellipses.\nSuppose we observe f (x1) = 1.2. To condition on this value of f (x1), we can draw a hori-\nzontal line at 1.2 on our plot of the density, and see that the value of f (x) is mostly constrained\nto [0.64, 1.52]. We have also drawn this plot in function space, showing the observed point\n\n836\nGaussian Processes\nf (x1) in orange, and 1 standard deviation of the Gaussian process predictive distribution for\nf (x) in blue, about the mean value of 1.08.\nNow suppose we have a stronger correlation, k(x, x1) = 0.95. Now the ellipses have nar-\nrowed further, and the value of f (x) is even more strongly determined by f (x1). Drawing a\nhorizontal line at 1.2, we see the contours for f (x) support values mostly within [0.83, 1.45].\nAgain, we also show the plot in function space, with one standard deviation about the mean\npredictive value of 1.14.\n\n837\nIntroduction to Gaussian Processes\nWe see that the posterior mean predictor of our Gaussian process is closer to 1.2, because\nthere is now a stronger correlation. We also see that our uncertainty (the error bars) have\nsomewhat decreased. Despite the strong correlation between these function values, our un-\ncertainty is still righly quite large, because we have only observed a single data point!\nThis procedure can give us a posterior on f (x) for any x, for any number of points we have\nobserved. Suppose we observe f (x1), f (x2). We now visualize the posterior for f (x) at a\nparticular x = x′ in function space. The exact distribution for f (x) is given by the above\nequations. f (x) is Gaussian distributed, with mean\nm = k(x, x1:3)k(x1:3, x1:3)−1 f (x1:3)\n(18.1.7)\nand variance\ns2 = k(x, x) −k(x, x1:3)k(x1:3, x1:3)−1k(x, x1:3)\n(18.1.8)\nIn this introductory notebook, we have been considering noise free observations. As we will\nsee, it is easy to include observation noise. If we assume that the data are generated from a\nlatent noise free function f (x) plus iid Gaussian noise ϵ(x) ∼N(0, σ2) with variance σ2,\nthen our covariance function simply becomes k(xi, xj) →k(xi, xj) + δijσ2, where δij = 1\nif i = j and 0 otherwise.\n\n838\nGaussian Processes\nWe have already started getting some intuition about how we can use a Gaussian process to\nspecify a prior and posterior over solutions, and how the kernel function aﬀects the proper-\nties of these solutions. In the following notebooks, we will precisely show how to specify a\nGaussian process prior, introduce and derive various kernel functions, and then go through\nthe mechanics of how to automatically learn kernel hyperparameters, and form a Gaussian\nprocess posterior to make predictions. While it takes time and practice to get used to concepts\nsuch as a “distributions over functions”, the actual mechanics of ﬁnding the GP predictive\nequations is actually quite simple — making it easy to get practice to form an intuitive un-\nderstanding of these concepts.\n18.1.1 Summary\nIn typical machine learning, we specify a function with some free parameters (such as a neu-\nral network and its weights), and we focus on estimating those parameters, which may not be\ninterpretable. With a Gaussian process, we instead reason about distributions over functions\ndirectly, which enables us to reason about the high-level properties of the solutions. These\nproperties are controlled by a covariance function (kernel), which often has a few highly in-\nterpretable hyperparameters. These hyperparameters include the length-scale, which controls\nhow rapidly (how wiggily) the functions are. Another hyperparameter is the amplitude, which\ncontrols the vertical scale over which our functions are varying. Representing many diﬀerent\nfunctions that can ﬁt the data, and combining them all together into a predictive distribution,\nis a distinctive feature of Bayesian methods. Because there is a greater amount of variability\nbetween possible solutions far away from the data, our uncertainty intuitively grows as we\nmove from the data.\nA Gaussian process represents a distribution over functions by specifying a multivariate nor-\nmal (Gaussian) distribution over all possible function values. It is possible to easily manipulate\nGaussian distributions to ﬁnd the distribution of one function value based on the values of\nany set of other values. In other words, if we observe a set of points, then we can condi-\ntion on these points and infer a distribution over what the value of the function might look\nlike at any other input. How we model the correlations between these points is determined\nby the covariance function and is what deﬁnes the generalization properties of the Gaussian\nprocess. While it takes time to get used to Gaussian processes, they are easy to work with,\nhave many applications, and help us understand and develop other model classes, like neural\nnetworks.\n18.1.2 Exercises\n1. What is the diﬀerence between epistemic uncertainty versus observation uncertainty?\n2. Besides rate of variation and amplitude, what other properties of functions might we want\nto consider, and what would be real-world examples of functions that have those proper-\nties?\n\n839\nGaussian Process Priors\n259\n3. The RBF covariance function we considered says that covariances (and correlations) be-\ntween observations decrease with their distance in the input space (times, spatial locations,\netc.). Is this a reasonable assumption? Why or why not?\n4. Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables\nGaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a\nGaussian?\n5. Repeat the exercise where we observe a data point at f (x1) = 1.2, but now suppose we\nadditionally observe f (x2) = 1.4. Let k(x, x1) = 0.9, and k(x, x2) = 0.8. Will we be\nmore or less certain about the value of f (x), than when we had only observed f (x1)?\nWhat is the mean and 95% credible set for our value of f (x) now?\n6. Do you think increasing our estimate of observation noise would increase or decrease our\nestimate of the length-scale of the ground truth function?\n7. As we move away from the data, suppose the uncertainty in our predictive distribution\nincreases to a point, then stops increasing. Why might that happen?\nDiscussions259.\n18.2 Gaussian Process Priors\nUnderstanding Gaussian processes (GPs) is important for reasoning about model construction\nand generalization, and for achieving state-of-the-art performance in a variety of applications,\nincluding active learning, and hyperparameter tuning in deep learning. GPs are everywhere,\nand it is in our interests to know what they are and how we can use them.\nIn this section, we introduce Gaussian process priors over functions. In the next notebook,\nwe show how to use these priors to do posterior inference and make predictions. The next\nsection can be viewed as “GPs in a nutshell”, quickly giving what you need to apply Gaussian\nprocesses in practice.\nimport numpy as np\nfrom scipy.spatial import distance_matrix\nfrom d2l import torch as d2l\nd2l.set_figsize()\n18.2.1 Deﬁnition\nA Gaussian process is deﬁned as a collection of random variables, any ﬁnite number of which\nhave a joint Gaussian distribution. If a function f (x) is a Gaussian process, with mean func-\ntion m(x) and covariance function or kernel k(x, x′), f (x) ∼GP(m, k), then any collec-\n\n840\nGaussian Processes\ntion of function values queried at any collection of input points x (times, spatial locations,\nimage pixels, etc.), has a joint multivariate Gaussian distribution with mean vector µ and\ncovariance matrix K: f (x1), . . ., f (xn) ∼N(µ, K), where µi = E[ f (xi)] = m(xi) and\nKij = Cov( f (xi), f (xj)) = k(xi, xj).\nThis deﬁnition may seem abstract and inaccessible, but Gaussian processes are in fact very\nsimple objects. Any function\nf (x) = w⊤ϕ(x) = ⟨w, ϕ(x)⟩,\n(18.2.1)\nwith w drawn from a Gaussian (normal) distribution, and ϕ being any vector of basis func-\ntions, for example ϕ(x) = (1, x, x2, ..., xd)⊤, is a Gaussian process. Moreover, any Gaussian\nprocess f(x) can be expressed in the form of equation (18.2.1). Let’s consider a few concrete\nexamples, to begin getting acquainted with Gaussian processes, after which we can appreciate\nhow simple and useful they really are.\n18.2.2 A Simple Gaussian Process\nSuppose f (x) = w0 + w1x, and w0, w1 ∼N(0, 1), with w0, w1, x all in one dimension. We\ncan equivalently write this function as the inner product f (x) = (w0, w1)(1, x)⊤. In (18.2.1)\nabove, w = (w0, w1)⊤and ϕ(x) = (1, x)⊤.\nFor any x, f (x) is a sum of two Gaussian random variables. Since Gaussians are closed under\naddition, f (x) is also a Gaussian random variable for any x. In fact, we can compute for any\nparticular x that f (x) is N(0, 1 + x2). Similarly, the joint distribution for any collection of\nfunction values, ( f (x1), . . ., f (xn)), for any collection of inputs x1, . . ., xn, is a multivariate\nGaussian distribution. Therefore f (x) is a Gaussian process.\nIn short, f (x) is a random function, or a distribution over functions. We can gain some in-\nsights into this distribution by repeatedly sampling values for w0, w1, and visualizing the\ncorresponding functions f (x), which are straight lines with slopes and diﬀerent intercepts,\nas follows:\ndef lin_func(x, n_sample):\npreds = np.zeros((n_sample, x.shape[0]))\nfor ii in range(n_sample):\nw = np.random.normal(0, 1, 2)\ny = w[0] + w[1] * x\npreds[ii, :] = y\nreturn preds\nx_points = np.linspace(-5, 5, 50)\nouts = lin_func(x_points, 10)\nlw_bd = -2 * np.sqrt((1 + x_points ** 2))\nup_bd = 2 * np.sqrt((1 + x_points ** 2))\nd2l.plt.fill_between(x_points, lw_bd, up_bd, alpha=0.25)\nd2l.plt.plot(x_points, np.zeros(len(x_points)), linewidth=4, color='black')\nd2l.plt.plot(x_points, outs.T)\n(continues on next page)\n\n841\nGaussian Process Priors\n(continued from previous page)\nd2l.plt.xlabel(\"x\", fontsize=20)\nd2l.plt.ylabel(\"f(x)\", fontsize=20)\nd2l.plt.show()\nIf w0 and w1 are instead drawn from N(0, α2), how do you imagine varying α aﬀects the\ndistribution over functions?\n18.2.3 From Weight Space to Function Space\nIn the plot above, we saw how a distribution over parameters in a model induces a distribution\nover functions. While we often have ideas about the functions we want to model — whether\nthey’re smooth, periodic, quickly varying, etc. — it is relatively tedious to reason about the\nparameters, which are largely uninterpretable. Fortunately, Gaussian processes provide an\neasy mechanism to reason directly about functions. Since a Gaussian distribution is entirely\ndeﬁned by its ﬁrst two moments, its mean and covariance matrix, a Gaussian process by\nextension is deﬁned by its mean function and covariance function.\nIn the above example, the mean function\nm(x) = E[ f (x)] = E[w0 + w1x] = E[w0] + E[w1]x = 0 + 0 = 0.\n(18.2.2)\nSimilarly, the covariance function is\nk(x, x′) = Cov( f (x), f (x′)) = E[ f (x) f (x′)] −E[ f (x)]E[ f (x′)] = E[w2\n0 + w0w1x′ + w1w0x + w2\n1xx′] = 1 + xx\n(18.2.3)\nOur distribution over functions can now be directly speciﬁed and sampled from, without\nneeding to sample from the distribution over parameters. For example, to draw from f (x),\nwe can simply form our multivariate Gaussian distribution associated with any collection of\nx we want to query, and sample from it directly. We will begin to see just how advantageous\nthis formulation will be.\nFirst, we note that essentially the same derivation for the simple straight line model above\n\n842\nGaussian Processes\ncan be applied to ﬁnd the mean and covariance function for any model of the form f (x) =\nw⊤ϕ(x), with w ∼N(u, S). In this case, the mean function m(x) = u⊤ϕ(x), and the covari-\nance function k(x, x′) = ϕ(x)⊤Sϕ(x′). Since ϕ(x) can represent a vector of any non-linear\nbasis functions, we are considering a very general model class, including models with an even\nan inﬁnite number of parameters.\n18.2.4 The Radial Basis Function (RBF) Kernel\nThe radial basis function (RBF) kernel is the most popular covariance function for Gaus-\nsian processes, and kernel machines in general. This kernel has the form kRBF(x, x′) =\na2 exp\n(\n−1\n2ℓ2 ||x −x′||2)\n, where a is an amplitude parameter, and ℓis a lengthscale hyper-\nparameter.\nLet’s derive this kernel starting from weight space. Consider the function\nf (x) =\nJ\n∑\ni=1\nwiϕi(x), wi ∼N\n(\n0, σ2\nJ\n)\n, ϕi(x) = exp\n(\n−(x −ci)2\n2ℓ2\n)\n.\n(18.2.4)\nf (x) is a sum of radial basis functions, with width ℓ, centred at the points ci, as shown in the\nfollowing ﬁgure.\nWe can recognize f (x) as having the form w⊤ϕ(x), where w = (w1, . . ., wJ)⊤and ϕ(x) is a\nvector containing each of the radial basis functions. The covariance function of this Gaussian\nprocess is then\nk(x, x′) = σ2\nJ\nJ\n∑\ni=1\nϕi(x)ϕi(x′).\n(18.2.5)\nNow let’s consider what happens as we take the number of parameters (and basis functions)\nto inﬁnity. Let cJ = log J, c1 = −log J, and ci+1 −ci = ∆c = 2 log J\nJ , and J →∞. The\ncovariance function becomes the Riemann sum:\nk(x, x′) = lim\nJ→∞\nσ2\nJ\nJ\n∑\ni=1\nϕi(x)ϕi(x′) =\n∫c∞\nc0\nϕc(x)ϕc(x′)dc.\n(18.2.6)\nBy setting c0 = −∞and c∞= ∞, we spread the inﬁnitely many basis functions across the\nwhole real line, each a distance ∆c →0 apart:\nk(x, x′) =\n∫∞\n−∞\nexp(−(x −c)2\n2ℓ2\n) exp(−(x′ −c)2\n2ℓ2\n)dc = √πℓσ2 exp(−(x −x′)2\n2(\n√\n2ℓ)2 ) ∝kRBF(x, x′).\n(18.2.7)\nIt is worth taking a moment to absorb what we have done here. By moving into the function\nspace representation, we have derived how to represent a model with an inﬁnite number of\nparameters, using a ﬁnite amount of computation. A Gaussian process with an RBF kernel is\na universal approximator, capable of representing any continuous function to arbitrary preci-\nsion. We can intuitively see why from the above derivation. We can collapse each radial basis\nfunction to a point mass taking ℓ→0, and give each point mass any height we wish.\n\n843\nGaussian Process Priors\nSo a Gaussian process with an RBF kernel is a model with an inﬁnite number of param-\neters and much more ﬂexibility than any ﬁnite neural network. Perhaps all the fuss about\noverparametrized neural networks is misplaced. As we will see, GPs with RBF kernels do\nnot overﬁt, and in fact provide especially compelling generalization performance on small\ndatasets. Moreover, the examples in (Zhang et al., 2021), such as the ability to ﬁt images\nwith random labels perfectly, but still generalize well on structured problems, (can be per-\nfectly reproduced using Gaussian processes) (Wilson and Izmailov, 2020). Neural networks\nare not as distinct as we make them out to be.\nWe can build further intuition about Gaussian processes with RBF kernels, and hyperparame-\nters such as length-scale, by sampling directly from the distribution over functions. As before,\nthis involves a simple procedure:\n1. Choose the input x points we want to query the GP: x1, . . ., xn.\n2. Evaluate m(xi), i = 1, . . ., n, and k(xi, xj) for i, j = 1, . . ., n to respectively form the\nmean vector and covariance matrix µ and K, where ( f (x1), . . ., f (xn)) ∼N(µ, K).\n3. Sample from this multivariate Gaussian distribution to obtain the sample function values.\n4. Sample more times to visualize more sample functions queried at those points.\nWe illustrate this process in the ﬁgure below.\ndef rbfkernel(x1, x2, ls=4.):\n#@save\ndist = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))\nreturn np.exp(-(1. / ls / 2) * (dist ** 2))\nx_points = np.linspace(0, 5, 50)\nmeanvec = np.zeros(len(x_points))\ncovmat = rbfkernel(x_points,x_points, 1)\nprior_samples= np.random.multivariate_normal(meanvec, covmat, size=5);\nd2l.plt.plot(x_points, prior_samples.T, alpha=0.5)\nd2l.plt.show()\n18.2.5 The Neural Network Kernel\n\n844\nGaussian Processes\nResearch on Gaussian processes in machine learning was triggered by research on neural net-\nworks. Radford Neal was pursuing ever larger Bayesian neural networks, ultimately showing\nin 1994 (later published in 1996, as it was one of the most infamous NeurIPS rejections)\nthat such networks with an inﬁnite number of hidden units become Gaussian processes with\nparticular kernel functions (Neal, 1996). Interest in this derivation has re-surfaced, with ideas\nlike the neural tangent kernel being used to investigate the generalization properties of neural\nnetworks (Matthews et al., 2018) (Novak et al., 2018). We can derive the neural network\nkernel as follows.\nConsider a neural network function f (x) with one hidden layer:\nf (x) = b +\nJ\n∑\ni=1\nvih(x; ui).\n(18.2.8)\nb is a bias, vi are the hidden to output weights, h is any bounded hidden unit transfer func-\ntion, ui are the input to hidden weights, and J is the number of hidden units. Let b and vi\nbe independent with zero mean and variances σ2\nb and σ2\nv /J, respectively, and let the ui have\nindependent identical distributions. We can then use the central limit theorem to show that\nany collection of function values f (x1), . . ., f (xn) has a joint multivariate Gaussian distri-\nbution.\nThe mean and covariance function of the corresponding Gaussian process are:\nm(x) = E[ f (x)] = 0\n(18.2.9)\nk(x, x′) = cov[ f (x), f (x′)] = E[ f (x) f (x′)] = σ2\nb + 1\nJ\nJ\n∑\ni=1\nσ2\nv E[hi(x; ui)hi(x′; ui)]\n(18.2.10)\nIn some cases, we can essentially evaluate this covariance function in closed form. Let h(x; u) =\nerf(u0 + ∑P\nj=1 uj xj), where erf(z) =\n2\n√π\n∫z\n0 e−t2dt, and u ∼N(0, Σ). Then k(x, x′) =\n2\nπ sin(\n2˜x⊤Σ˜x′\n√\n(1+2˜x⊤Σ˜x)(1+2˜x′⊤Σ˜x′)).\nThe RBF kernel is stationary, meaning that it is translation invariant, and therefore can be\nwritten as a function of τ = x−x′. Intuitively, stationarity means that the high-level properties\nof the function, such as rate of variation, do not change as we move in input space. The\nneural network kernel, however, is non-stationary. Below, we show sample functions from a\nGaussian process with this kernel. We can see that the function looks qualitatively diﬀerent\nnear the origin.\n18.2.6 Summary\nThe ﬁrst step in performing Bayesian inference involves specifying a prior. Gaussian processes\ncan be used to specify a whole prior over functions. Starting from a traditional “weight space”\nview of modelling, we can induce a prior over functions by starting with the functional form\nof a model, and introducing a distribution over its parameters. We can alternatively specify\n\n845\nGaussian Process Priors\n260\na prior distribution directly in function space, with properties controlled by a kernel. The\nfunction-space approach has many advantages. We can build models that actually correspond\nto an inﬁnite number of parameters, but use a ﬁnite amount of computation! Moreover, while\nthese models have a great amount of ﬂexibility, they also make strong assumptions about\nwhat types of functions are a priori likely, leading to relatively good generalization on small\ndatasets.\nThe assumptions of models in function space are intuitively controlled by kernels, which often\nencode higher level properties of functions, such as smoothness and periodicity. Many kernels\nare stationary, meaning that they are translation invariant. Functions drawn from a Gaussian\nprocess with a stationary kernel have roughly the same high-level properties (such as rate of\nvariation) regardless of where we look in the input space.\nGaussian processes are a relatively general model class, containing many examples of models\nwe are already familiar with, including polynomials, Fourier series, and so on, as long as\nwe have a Gaussian prior over the parameters. They also include neural networks with an\ninﬁnite number of parameters, even without Gaussian distributions over the parameters. This\nconnection, discovered by Radford Neal, triggered machine learning researchers to move\naway from neural networks, and towards Gaussian processes.\n18.2.7 Exercises\n1. Draw sample prior functions from a GP with an Ornstein-Uhlenbeck (OU) kernel, kOU(x, x′) =\nexp (−1\n2ℓ||x −x′|). If you ﬁx the lengthscale ℓto be the same, how do these functions\nlook diﬀerent than sample functions from a GP with an RBF kernel?\n2. How does changing the amplitude a2 of the RBF kernel aﬀect the distribution over func-\ntions?\n3. Suppose we form u(x) = f (x) + 2g(x), where f (x) ∼GP(m1, k1) and g(x) ∼\nGP(m2, k2). Is u(x) a Gaussian process, and if so, what is its mean and covariance func-\ntion?\n4. Suppose we form g(x) = a(x) f (x), where f (x) ∼GP(0, k) and a(x) = x2. Is g(x) a\nGaussian process, and if so, what is its mean and covariance function? What is the eﬀect\nof a(x)? What do sample functions drawn from g(x) look like?\n5. Suppose we form u(x) = f (x)g(x), where f (x) ∼GP(m1, k1) and g(x) ∼GP(m2, k2).\nIs u(x) a Gaussian process, and if so, what is its mean and covariance function?\nDiscussions260.\n\n846\nGaussian Processes\n261\n18.3 Gaussian Process Inference\nIn this section, we will show how to perform posterior inference and make predictions using\nthe GP priors we introduced in the last section. We will start with regression, where we can\nperform inference in closed form. This is a “GPs in a nutshell” section to quickly get up and\nrunning with Gaussian processes in practice. We’ll start coding all the basic operations from\nscratch, and then introduce GPyTorch 261 , which will make working with state-of-the-art\nGaussian processes and integration with deep neural networks much more convenient. We\nwill consider these more advanced topics in depth in the next section. In that section, we\nwill also consider settings where approximate inference is required — classiﬁcation, point\nprocesses, or any non-Gaussian likelihoods.\n18.3.1 Posterior Inference for Regression\nAn observation model relates the function we want to learn, f (x), to our observations y(x),\nboth indexed by some input x. In classiﬁcation, x could be the pixels of an image, and y could\nbe the associated class label. In regression, y typically represents a continuous output, such\nas a land surface temperature, a sea-level, a CO2 concentration, etc.\nIn regression, we often assume the outputs are given by a latent noise-free function f (x) plus\ni.i.d. Gaussian noise ϵ(x):\ny(x) = f (x) + ϵ(x),\n(18.3.1)\nwith ϵ(x) ∼N(0, σ2). Let y = y(X) = (y(x1), . . ., y(xn))⊤be a vector of our training\nobservations, and f = ( f (x1), . . ., f (xn))⊤be a vector of the latent noise-free function\nvalues, queried at the training inputs X = x1, . . ., xn.\nWe will assume f (x) ∼GP(m, k), which means that any collection of function values f has a\njoint multivariate Gaussian distribution, with mean vector µi = m(xi) and covariance matrix\nKij = k(xi, xj). The RBF kernel k(xi, xj) = a2 exp\n(\n−1\n2ℓ2 ||xi −xj||2)\nwould be a standard\nchoice of covariance function. For notational simplicity, we will assume the mean function\nm(x) = 0; our derivations can easily be generalized later on.\nSuppose we want to make predictions at a set of inputs\nX∗= x∗1, x∗2, . . ., x∗m.\n(18.3.2)\nThen we want to ﬁnd x2 and p(f∗|y, X). In the regression setting, we can conveniently ﬁnd this\ndistribution by using Gaussian identities, after ﬁnding the joint distribution over f∗= f (X∗)\nand y.\nIf we evaluate equation (18.3.1) at the training inputs X, we have y = f + ﬄ. By the\ndeﬁnition of a Gaussian process (see last section), f ∼N(0, K(X, X)) where K(X, X) is\nan n × n matrix formed by evaluating our covariance function (aka kernel) at all possible\n\n847\nGaussian Process Inference\npairs of inputs xi, xj ∈X. ﬄis simply a vector comprised of iid samples from N(0, σ2)\nand thus has distribution N(0, σ2I). y is therefore a sum of two independent multivariate\nGaussian variables, and thus has distribution N(0, K(X, X) + σ2I). One can also show that\ncov(f∗, y) = cov(y, f∗)⊤= K(X∗, X) where K(X∗, X) is an m × n matrix formed by evalu-\nating the kernel at all pairs of test and training inputs.\n[y\nf∗\n]\n∼N\n(\n0, A =\n[K(X, X) + σ2I\nK(X, X∗)\nK(X∗, X)\nK(X∗, X∗)\n])\n(18.3.3)\nWe can then use standard Gaussian identities to ﬁnd the conditional distribution from the joint\ndistribution (see, e.g., Bishop Chapter 2), f∗|y, X, X∗∼N(m∗, S∗), where m∗= K(X∗, X)[K(X, X)+\nσ2I]−1y, and S = K(X∗, X∗) −K(X∗, X)[K(X, X) + σ2I]−1K(X, X∗).\nTypically, we do not need to make use of the full predictive covariance matrix S, and instead\nuse the diagonal of S for uncertainty about each prediction. Often for this reason we write the\npredictive distribution for a single test point x∗, rather than a collection of test points.\nThe kernel matrix has parameters θ that we also wish to estimate, such the amplitude a and\nlengthscale ℓof the RBF kernel above. For these purposes we use the marginal likelihood,\np(y|θ, X), which we already derived in working out the marginal distributions to ﬁnd the joint\ndistribution over y, f∗. As we will see, the marginal likelihood compartmentalizes into model\nﬁt and model complexity terms, and automatically encodes a notion of Occam’s razor for\nlearning hyperparameters. For a full discussion, see MacKay Ch. 28 (MacKay, 2003), and\nRasmussen and Williams Ch. 5 (Rasmussen and Williams, 2006).\nimport math\nimport os\nimport gpytorch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom scipy import optimize\nfrom scipy.spatial import distance_matrix\nfrom d2l import torch as d2l\nd2l.set_figsize()\n18.3.2 Equations for Making Predictions and Learning Kernel\nHyperparameters in GP Regression\nWe list here the equations you will use for learning hyperparameters and making predictions\nin Gaussian process regression. Again, we assume a vector of regression targets y, indexed by\ninputs X = {x1, . . ., xn}, and we wish to make a prediction at a test input x∗. We assume i.i.d.\nadditive zero-mean Gaussian noise with variance σ2. We use a Gaussian process prior f (x) ∼\nGP(m, k) for the latent noise-free function, with mean function m and kernel function k. The\nkernel itself has parameters θ that we want to learn. For example, if we use an RBF kernel,\nk(xi, xj) = a2 exp\n(\n−1\n2ℓ2 ||x −x′||2)\n, we want to learn θ = {a2, ℓ2}. Let K(X, X) represent\nan n × n matrix corresponding to evaluating the kernel for all possible pairs of n training\n\n848\nGaussian Processes\ninputs. Let K(x∗, X) represent a 1 × n vector formed by evaluating k(x∗, xi), i = 1, . . ., n.\nLet µ be a mean vector formed by evaluating the mean function m(x) at every training points\nx.\nTypically in working with Gaussian processes, we follow a two-step procedure. 1. Learn\nkernel hyperparameters ˆθ by maximizing the marginal likelihood with respect to these hy-\nperparameters. 2. Use the predictive mean as a point predictor, and 2 times the predictive\nstandard deviation to form a 95% credible set, conditioning on these learned hyperparameters\nˆθ.\nThe log marginal likelihood is simply a log Gaussian density, which has the form:\nlog p(y|θ, X) = −1\n2y⊤[Kθ(X, X) + σ2I]−1y −1\n2 log |Kθ(X, X)| + c\n(18.3.4)\nThe predictive distribution has the form:\np(y∗|x∗, y, θ) = N(a∗, v∗)\n(18.3.5)\na∗= kθ(x∗, X)[Kθ(X, X) + σ2I]−1(y −µ) + µ\n(18.3.6)\nv∗= kθ(x∗, x∗) −Kθ(x∗, X)[Kθ(X, X) + σ2I]−1kθ(X, x∗)\n(18.3.7)\n18.3.3 Interpreting Equations for Learning and Predictions\nThere are some key points to note about the predictive distributions for Gaussian processes:\n• Despite the ﬂexibility of the model class, it is possible to do exact Bayesian inference for\nGP regression in closed form. Aside from learning the kernel hyperparameters, there\nis no training. We can write down exactly what equations we want to use to make pre-\ndictions. Gaussian processes are relatively exceptional in this respect, and it has greatly\ncontributed to their convenience, versatility, and continued popularity.\n• The predictive mean a∗is a linear combination of the training targets y, weighted by the\nkernel kθ(x∗, X)[Kθ(X, X) + σ2I]−1. As we will see, the kernel (and its hyperparame-\nters) thus plays a crucial role in the generalization properties of the model.\n• The predictive mean explicitly depends on the target values y but the predictive variance\ndoes not. The predictive uncertainty instead grows as the test input x∗moves away from\nthe target locations X, as governed by the kernel function. However, uncertainty will\nimplicitly depend on the values of the targets y through the kernel hyperparameters θ,\nwhich are learned from the data.\n• The marginal likelihood compartmentalizes into model ﬁt and model complexity (log de-\nterminant) terms. The marginal likelihood tends to select for hyperparameters that pro-\nvide the simplest ﬁts that are still consistent with the data.\n• The key computational bottlenecks come from solving a linear system and computing a\nlog determinant over an n × n symmetric positive deﬁnite matrix K(X, X) for n train-\ning points. Naively, these operations each incur O(n3) computations, as well as O(n2)\n\n849\nGaussian Process Inference\nstorage for each entry of the kernel (covariance) matrix, often starting with a Cholesky\ndecomposition. Historically, these bottlenecks have limited GPs to problems with fewer\nthan about 10,000 training points, and have given GPs a reputation for “being slow” that\nhas been inaccurate now for almost a decade. In advanced topics, we will discuss how\nGPs can be scaled to problems with millions of points.\n• For popular choices of kernel functions, K(X, X) is often close to singular, which can\ncause numerical issues when performing Cholesky decompositions or other operations\nintended to solve linear systems. Fortunately, in regression we are often working with\nKθ(X, X)+σ2I, such that the noise variance σ2 gets added to the diagonal of K(X, X),\nsigniﬁcantly improving its conditioning. If the noise variance is small, or we are doing\nnoise free regression, it is common practice to add a small amount of “jitter” to the\ndiagonal, on the order of 10−6, to improve conditioning.\n18.3.4 Worked Example from Scratch\nLet’s create some regression data, and then ﬁt the data with a GP, implementing every step\nfrom scratch. We’ll sample data from\ny(x) = sin(x) + 1\n2 sin(4x) + ϵ,\n(18.3.8)\nwith ϵ ∼N(0, σ2). The noise free function we wish to ﬁnd is f (x) = sin(x) + 1\n2 sin(4x).\nWe’ll start by using a noise standard deviation σ = 0.25.\ndef data_maker1(x, sig):\nreturn np.sin(x) + 0.5 * np.sin(4 * x) + np.random.randn(x.shape[0]) * sig\nsig = 0.25\ntrain_x, test_x = np.linspace(0, 5, 50), np.linspace(0, 5, 500)\ntrain_y, test_y = data_maker1(train_x, sig=sig), data_maker1(test_x, sig=0.)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y)\nd2l.plt.xlabel(\"x\", fontsize=20)\nd2l.plt.ylabel(\"Observations y\", fontsize=20)\nd2l.plt.show()\nHere we see the noisy observations as circles, and the noise-free function in blue that we wish\nto ﬁnd.\nNow, let’s specify a GP prior over the latent noise-free function, f (x) ∼GP(m, k). We’ll\nuse a mean function m(x) = 0, and an RBF covariance function (kernel)\nk(xi, xj) = a2 exp\n(\n−1\n2ℓ2 ||x −x′||2\n)\n.\n(18.3.9)\nmean = np.zeros(test_x.shape[0])\ncov = d2l.rbfkernel(test_x, test_x, ls=0.2)\n\n850\nGaussian Processes\nWe have started with a length-scale of 0.2. Before we ﬁt the data, it is important to consider\nwhether we have speciﬁed a reasonable prior. Let’s visualize some sample functions from this\nprior, as well as the 95% credible set (we believe there’s a 95% chance that the true function\nis within this region).\nprior_samples = np.random.multivariate_normal(mean=mean, cov=cov, size=5)\nd2l.plt.plot(test_x, prior_samples.T, color='black', alpha=0.5)\nd2l.plt.plot(test_x, mean, linewidth=2.)\nd2l.plt.fill_between(test_x, mean - 2 * np.diag(cov), mean + 2 * np.diag(cov),\nalpha=0.25)\nd2l.plt.show()\nDo these samples look reasonable? Are the high-level properties of the functions aligned with\nthe type of data we are trying to model?\nNow let’s form the mean and variance of the posterior predictive distribution at any arbitrary\ntest point x∗.\n¯f∗= K(x, x∗)T(K(x, x) + σ2I)−1y\n(18.3.10)\nV( f∗) = K(x∗, x∗) −K(x, x∗)T(K(x, x) + σ2I)−1K(x, x∗)\n(18.3.11)\nBefore we make predictions, we should learn our kernel hyperparameters θ and noise vari-\nance σ2. Let’s initialize our length-scale at 0.75, as our prior functions looked too quickly\n\n851\nGaussian Process Inference\nvarying compared to the data we are ﬁtting. We’ll also guess a noise standard deviation σ of\n0.75.\nIn order to learn these parameters, we will maximize the marginal likelihood with respect to\nthese parameters.\nlog p(y|X) = log\n∫\np(y| f, X)p( f |X)df\n(18.3.12)\nlog p(y|X) = −1\n2 yT(K(x, x) + σ2I)−1y −1\n2 log |K(x, x) + σ2I| −n\n2 log 2π\n(18.3.13)\nPerhaps our prior functions were too quickly varying. Let’s guess a length-scale of 0.4. We’ll\nalso guess a noise standard deviation of 0.75. These are simply hyperparameter initializations\n— we will learn these parameters from the marginal likelihood.\nell_est = 0.4\npost_sig_est = 0.5\ndef neg_MLL(pars):\nK = d2l.rbfkernel(train_x, train_x, ls=pars[0])\nkernel_term = -0.5 * train_y @ \\\nnp.linalg.inv(K + pars[1] ** 2 * np.eye(train_x.shape[0])) @ train_y\nlogdet = -0.5 * np.log(np.linalg.det(K + pars[1] ** 2 * \\\nnp.eye(train_x.shape[0])))\nconst = -train_x.shape[0] / 2. * np.log(2 * np.pi)\nreturn -(kernel_term + logdet + const)\nlearned_hypers = optimize.minimize(neg_MLL, x0=np.array([ell_est,post_sig_\n,→est]),\nbounds=((0.01, 10.), (0.01, 10.)))\nell = learned_hypers.x[0]\npost_sig_est = learned_hypers.x[1]\nIn this instance, we learn a length-scale of 0.299, and a noise standard deviation of 0.24. Note\nthat the learned noise is extremely close to the true noise, which helps indicate that our GP\nis a very well-speciﬁed to this problem.\nIn general, it is crucial to put careful thought into selecting the kernel and initializing the\nhyperparameters. While marginal likelihood optimization can be relatively robust to initial-\nization, it is not immune to poor initializations. Try running the above script with a variety\nof initializations and see what results you ﬁnd.\nNow, let’s make predictions with these learned hypers.\nK_x_xstar = d2l.rbfkernel(train_x, test_x, ls=ell)\nK_x_x = d2l.rbfkernel(train_x, train_x, ls=ell)\nK_xstar_xstar = d2l.rbfkernel(test_x, test_x, ls=ell)\npost_mean = K_x_xstar.T @ np.linalg.inv((K_x_x + \\\npost_sig_est ** 2 * np.eye(train_x.shape[0]))) @ train_y\n(continues on next page)\n\n852\nGaussian Processes\n(continued from previous page)\npost_cov = K_xstar_xstar - K_x_xstar.T @ np.linalg.inv((K_x_x + \\\npost_sig_est ** 2 * np.eye(train_x.shape[0]))) @ K_x_xstar\nlw_bd = post_mean - 2 * np.sqrt(np.diag(post_cov))\nup_bd = post_mean + 2 * np.sqrt(np.diag(post_cov))\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth=2.)\nd2l.plt.plot(test_x, post_mean, linewidth=2.)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)\nd2l.plt.legend(['Observed Data', 'True Function', 'Predictive Mean', '95% Set␣\n,→on True Func'])\nd2l.plt.show()\nWe see the posterior mean in orange almost perfectly matches the true noise free function!\nNote that the 95% credible set we are showing is for the latent noise free (true) function, and\nnot the data points. We see that this credible set entirely contains the true function, and does\nnot seem overly wide or narrow. We would not want nor expect it to contain the data points.\nIf we wish to have a credible set for the observations, we should compute\nlw_bd_observed = post_mean - 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)\nup_bd_observed = post_mean + 2 * np.sqrt(np.diag(post_cov) + post_sig_est ** 2)\nThere are two sources of uncertainty, epistemic uncertainty, representing reducible uncer-\ntainty, and aleatoric or irreducible uncertainty. The epistemic uncertainty here represents un-\ncertainty about the true values of the noise free function. This uncertainty should grow as we\nmove away from the data points, as away from the data there are a greater variety of function\nvalues consistent with our data. As we observe more and more data, our beliefs about the true\nfunction become more conﬁdent, and the epistemic uncertainty disappears. The aleatoric un-\ncertainty in this instance is the observation noise, since the data are given to us with this noise,\nand it cannot be reduced.\nThe epistemic uncertainty in the data is captured by variance of the latent noise free function\nnp.diag(post_cov). The aleatoric uncertainty is captured by the noise variance post_sig_est**2.\nUnfortunately, people are often careless about how they represent uncertainty, with many\n\n853\nGaussian Process Inference\npapers showing error bars that are completely undeﬁned, no clear sense of whether we are\nvisualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with\nnoise standard deviations, standard deviations with standard errors, conﬁdence intervals with\ncredible sets, and so on. Without being precise about what the uncertainty represents, it is\nessentially meaningless.\nIn the spirit of playing close attention to what our uncertainty represents, it is crucial to\nnote that we are taking two times the square root of our variance estimate for the noise free\nfunction. Since our predictive distribution is Gaussian, this quantity enables us to form a\n95% credible set, representing our beliefs about the interval which is 95% likely to contain\nthe ground truth function. The noise variance is living on a completely diﬀerent scale, and is\nmuch less interpretable.\nFinally, let’s take a look at 20 posterior samples. These samples tell us what types of functions\nwe believe might ﬁt our data, a posteriori.\npost_samples = np.random.multivariate_normal(post_mean, post_cov, size=20)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth=2.)\nd2l.plt.plot(test_x, post_mean, linewidth=2.)\nd2l.plt.plot(test_x, post_samples.T, color='gray', alpha=0.25)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha=0.25)\nplt.legend(['Observed Data', 'True Function', 'Predictive Mean', 'Posterior␣\n,→Samples'])\nd2l.plt.show()\nIn basic regression applications, it is most common to use the posterior predictive mean and\nstandard deviation as a point predictor and metric for uncertainty, respectively. In more ad-\nvanced applications, such as Bayesian optimization with Monte Carlo acquisition functions,\nor Gaussian processes for model-based RL, it often necessary to take posterior samples. How-\never, even if not strictly required in the basic applications, these samples give us more intuition\nabout the ﬁt we have for the data, and are often useful to include in visualizations.\n18.3.5 Making Life Easy with GPyTorch\n\n854\nGaussian Processes\n262\n263\nAs we have seen, it is actually pretty easy to implement basic Gaussian process regression\nentirely from scratch. However, as soon as we want to explore a variety of kernel choices,\nconsider approximate inference (which is needed even for classiﬁcation), combine GPs with\nneural networks, or even have a dataset larger than about 10,000 points, then an implementa-\ntion from scratch becomes unwieldy and cumbersome. Some of the most eﬀective methods\nfor scalable GP inference, such as SKI (also known as KISS-GP), can require hundreds of\nlines of code implementing advanced numerical linear algebra routines.\nIn these cases, the GPyTorch library will make our lives a lot easier. We’ll be discussing\nGPyTorch more in future notebooks on Gaussian process numerics, and advanced methods.\nThe GPyTorch library contains many examples 262 . To get a feel for the package, we will\nwalk through the simple regression example263, showing how it can be adapted to reproduce\nour above results using GPyTorch. This may seem like a lot of code to simply reproduce the\nbasic regression above, and in a sense, it is. But we can immediately use a variety of kernels,\nscalable inference techniques, and approximate inference, by only changing a few lines of\ncode from below, instead of writing potentially thousands of lines of new code.\n# First let's convert our data into tensors for use with PyTorch\ntrain_x = torch.tensor(train_x)\ntrain_y = torch.tensor(train_y)\ntest_y = torch.tensor(test_y)\n# We are using exact GP inference with a zero mean and RBF kernel\nclass ExactGPModel(gpytorch.models.ExactGP):\ndef __init__(self, train_x, train_y, likelihood):\nsuper(ExactGPModel, self).__init__(train_x, train_y, likelihood)\nself.mean_module = gpytorch.means.ZeroMean()\nself.covar_module = gpytorch.kernels.ScaleKernel(\ngpytorch.kernels.RBFKernel())\ndef forward(self, x):\nmean_x = self.mean_module(x)\ncovar_x = self.covar_module(x)\nreturn gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\nThis code block puts the data in the right format for GPyTorch, and speciﬁes that we are using\nexact inference, as well the mean function (zero) and kernel function (RBF) that we want to\nuse. We can use any other kernel very easily, by calling, for instance, gpytorch.kernels.matern_kernel(),\nor gpyotrch.kernels.spectral_mixture_kernel(). So far, we have only discussed exact infer-\nence, where it is possible to infer a predictive distribution without making any approxima-\ntions. For Gaussian processes, we can only perform exact inference when we have a Gaussian\nlikelihood; more speciﬁcally, when we assume that our observations are generated as a noise-\nfree function represented by a Gaussian process, plus Gaussian noise. In future notebooks,\nwe will consider other settings, such as classiﬁcation, where we cannot make these assump-\ntions.\n# Initialize Gaussian likelihood\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\n(continues on next page)\n\n855\nGaussian Process Inference\n(continued from previous page)\nmodel = ExactGPModel(train_x, train_y, likelihood)\ntraining_iter = 50\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n# Use the adam optimizer, includes GaussianLikelihood parameters\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n# Set our loss as the negative log GP marginal likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nHere, we explicitly specify the likelihood we want to use (Gaussian), the objective we will\nuse for training kernel hyperparameters (here, the marginal likelihood), and the procedure we\nwe want to use for optimizing that objective (in this case, Adam). We note that while we are\nusing Adam, which is a “stochastic” optimizer, in this case, it is full-batch Adam. Because the\nmarginal likelihood does not factorize over data instances, we cannot use an optimizer over\n“mini-batches” of data and be guaranteed convergence. Other optimizers, such as L-BFGS,\nare also supported by GPyTorch. Unlike in standard deep learning, doing a good job of op-\ntimizing the marginal likelihood corresponds strongly with good generalization, which often\ninclines us towards powerful optimizers like L-BFGS, assuming they are not prohibitively\nexpensive.\nfor i in range(training_iter):\n# Zero gradients from previous iteration\noptimizer.zero_grad()\n# Output from model\noutput = model(train_x)\n# Calc loss and backprop gradients\nloss = -mll(output, train_y)\nloss.backward()\nif i % 10 == 0:\nprint(f'Iter {i+1:d}/{training_iter:d} - Loss: {loss.item():.3f} '\nf'squared lengthscale: '\nf'{model.covar_module.base_kernel.lengthscale.item():.3f} '\nf'noise variance: {model.likelihood.noise.item():.3f}')\noptimizer.step()\nIter 1/50 - Loss: 1.013 squared lengthscale: 0.693 noise variance: 0.693\nIter 11/50 - Loss: 0.739 squared lengthscale: 0.508 noise variance: 0.312\nIter 21/50 - Loss: 0.522 squared lengthscale: 0.515 noise variance: 0.129\nIter 31/50 - Loss: 0.474 squared lengthscale: 0.511 noise variance: 0.064\nIter 41/50 - Loss: 0.481 squared lengthscale: 0.505 noise variance: 0.058\nHere we actually run the optimization procedure, outputting the values of the loss every 10\niterations.\n# Get into evaluation (predictive posterior) mode\ntest_x = torch.tensor(test_x)\nmodel.eval()\n(continues on next page)\n\n856\nGaussian Processes\n(continued from previous page)\nlikelihood.eval()\nobserved_pred = likelihood(model(test_x))\nThe above codeblock enables us to make predictions on our test inputs.\nwith torch.no_grad():\n# Initialize plot\nf, ax = d2l.plt.subplots(1, 1, figsize=(4, 3))\n# Get upper and lower bounds for 95\\% credible set (in this case, in\n# observation space)\nlower, upper = observed_pred.confidence_region()\nax.scatter(train_x.numpy(), train_y.numpy())\nax.plot(test_x.numpy(), test_y.numpy(), linewidth=2.)\nax.plot(test_x.numpy(), observed_pred.mean.numpy(), linewidth=2.)\nax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.25)\nax.set_ylim([-1.5, 1.5])\nax.legend(['True Function', 'Predictive Mean', 'Observed Data',\n'95% Credible Set'])\nFinally, we plot the ﬁt.\nWe see the ﬁts are virtually identical. A few things to note: GPyTorch is working with squared\nlength-scales and observation noise. For example, our learned noise standard deviation in the\nfor scratch code is about 0.283. The noise variance found by GPyTorch is 0.81 ≈0.2832.\nIn the GPyTorch plot, we also show the credible set in the observation space rather than the\nlatent function space, to demonstrate that they indeed cover the observed datapoints.\n18.3.6 Summary\nWe can combine a Gaussian process prior with data to form a posterior, which we use to make\npredictions. We can also form a marginal likelihood, which is useful for automatic learning\nof kernel hyperparameters, which control properties such as the rate of variation of the Gaus-\nsian process. The mechanics of forming the posterior and learning kernel hyperparameters\n\n857\nGaussian Process Inference\nfor regression are simple, involving about a dozen lines of code. This notebook is a good\nreference for any reader wanting to quickly get “up and running” with Gaussian processes.\nWe also introduced the GPyTorch library. Although the GPyTorch code for basic regression\nis relatively long, it can be trivially modiﬁed for other kernel functions, or more advanced\nfunctionality we will discuss in future notebooks, such as scalable inference, or non-Gaussian\nlikelihoods for classiﬁcation.\n18.3.7 Exercises\n1. We have emphasized the importance of learning kernel hyperparameters, and the eﬀect\nof hyperparameters and kernels on the generalization properties of Gaussian processes.\nTry skipping the step where we learn hypers, and instead guess a variety of length-scales\nand noise variances, and check their eﬀect on predictions. What happens when you use a\nlarge length-scale? A small length-scale? A large noise variance? A small noise variance?\n2. We have said that the marginal likelihood is not a convex objective, but that hyperpa-\nrameters like length-scale and noise variance can be reliably estimated in GP regression.\nThis is generally true — in fact, the marginal likelihood is much better at learning length-\nscale hyperparameters than conventional approaches in spatial statistics, which involve\nﬁtting empirical autocorrelation functions (“covariograms”). Arguably, the biggest con-\ntribution from machine learning to Gaussian process research, at least before recent work\non scalable inference, was the introduction of the marginal lkelihood for hyperparameter\nlearning.\nHowever, diﬀerent pairings of even these parameters provide interpretably diﬀerent plausible\nexplanations for many datasets, leading to local optima in our objective. If we use a large\nlength-scale, then we assume the true underlying function is slowly varying. If the observed\ndata are varying signiﬁcantly, then the only we can plausibly have a large length-scale is with\na large noise-variance. If we use a small length-scale, on the other hand, our ﬁt will be very\nsensitive to the variations in the data, leaving little room to explain variations with noise\n(aleatoric uncertainty).\nTry seeing if you can ﬁnd these local optima: initialize with very large length-scale with\nlarge noise, and small length-scales with small noise. Do you converge to diﬀerent solu-\ntions?\n3. We have said that a fundamental advantage of Bayesian methods is in naturally represent-\ning epistemic uncertainty. In the above example, we cannot fully see the eﬀects of epis-\ntemic uncertainty. Try instead to predict with test_x = np.linspace(0, 10, 1000).\nWhat happens to the 95% credible set as your predictions move beyond the data? Does\nit cover the true function in that interval? What happens if you only visualize aleatoric\nuncertainty in that region?\n4. Try running the above example, but instead with 10,000, 20,000 and 40,000 training\npoints, and measure the runtimes. How does the training time scale? Alternatively, how\ndo the runtimes scale with the number of test points? Is it diﬀerent for the predictive mean\nand the predictive variance? Answer this question both by theoretically working out the\n\n858\nGaussian Processes\n264\ntraining and testing time complexities, and by running the code above with a diﬀerent\nnumber of points.\n5. Try running the GPyTorch example with diﬀerent covariance functions, such as the Matern\nkernel. How do the results change? How about the spectral mixture kernel, found in the\nGPyTorch library? Are some easier to train the marginal likelihood than others? Are some\nmore valuable for long-range versus short-range predictions?\n6. In our GPyTorch example, we plotted the predictive distribution including observation\nnoise, while in our “from scratch” example, we only included epistemic uncertainty. Re-\ndo the GPyTorch example, but this time only plotting epistemic uncertainty, and compare\nto the from-scratch results. Do the predictive distributions now look the same? (They\nshould.)\nDiscussions264.\n\n19\nHyperparameter Optimization\nAaron Klein (Amazon), Matthias Seeger (Amazon), and Cedric Archambeau (Amazon)\nThe performance of every machine learning model depends on its hyperparameters. They\ncontrol the learning algorithm or the structure of the underlying statistical model. However,\nthere is no general way to choose hyperparameters in practice. Instead, hyperparameters are\noften set in a trial-and-error manner or sometimes left to their default values by practitioners,\nleading to suboptimal generalization.\nHyperparameter optimization provides a systematic approach to this problem, by casting it\nas an optimization problem: a good set of hyperparameters should (at least) minimize a val-\nidation error. Compared to most other optimization problems arising in machine learning,\nhyperparameter optimization is a nested one, where each iteration requires training and val-\nidating a machine learning model.\nIn this chapter, we will ﬁrst introduce the basics of hyperparameter optimization. We will\nalso present some recent advancements that improve the overall eﬃciency of hyperparameter\noptimization by exploiting cheap-to-evaluate proxies of the original objective function. At the\nend of this chapter, you should be able to apply state-of-the-art hyperparameter optimization\ntechniques to optimize the hyperparameter of your own machine learning algorithm.\n19.1 What Is Hyperparameter Optimization?\nAs we have seen in the previous chapters, deep neural networks come with a large number of\nparameters or weights that are learned during training. On top of these, every neural network\nhas additional hyperparameters that need to be conﬁgured by the user. For example, to ensure\nthat stochastic gradient descent converges to a local optimum of the training loss (see Chapter\n12), we have to adjust the learning rate and batch size. To avoid overﬁtting on training datasets,\nwe might have to set regularization parameters, such as weight decay (see Section 3.7) or\ndropout (see Section 5.6). We can deﬁne the capacity and inductive bias of the model by\nsetting the number of layers and number of units or ﬁlters per layer (i.e., the eﬀective number\nof weights).\nUnfortunately, we cannot simply adjust these hyperparameters by minimizing the training\n859\n\n860\nHyperparameter Optimization\nloss, because this would lead to overﬁtting on the training data. For example, setting regular-\nization parameters, such as dropout or weight decay to zero leads to a small training loss, but\nmight hurt the generalization performance.\nSet Hyperparameters\nTrain\nEvaluate\nDeploy\nLoop until validation \nperformance is maximised\nt\nFig. 19.1.1\nTypical workﬂow in machine learning that consists of training the model multiple times\nwith different hyperparameters.\nWithout a diﬀerent form of automation, hyperparameters have to be set manually in a trial-\nand-error fashion, in what amounts to a time-consuming and diﬃcult part of machine learning\nworkﬂows. For example, consider training a ResNet (see Section 8.6) on CIFAR-10, which\nrequires more than 2 hours on an Amazon Elastic Cloud Compute (EC2) g4dn.xlarge in-\nstance. Even just trying ten hyperparameter conﬁgurations in sequence, this would already\ntake us roughly one day. To make matters worse, hyperparameters are usually not directly\ntransferable across architectures and datasets (Bardenet et al., 2013, Feurer et al., 2022, Wis-\ntuba et al., 2018), and need to be re-optimized for every new task. Also, for most hyperpa-\nrameters, there are no rule-of-thumbs, and expert knowledge is required to ﬁnd sensible\nvalues.\nHyperparameter optimization (HPO) algorithms are designed to tackle this problem in a prin-\ncipled and automated fashion (Feurer and Hutter, 2018), by framing it as a global optimiza-\ntion problem. The default objective is the error on a hold-out validation dataset, but could in\nprinciple be any other business metric. It can be combined with or constrained by secondary\nobjectives, such as training time, inference time, or model complexity.\nRecently, hyperparameter optimization has been extended to neural architecture search (NAS)\n(Elsken et al., 2018, Wistuba et al., 2019), where the goal is to ﬁnd entirely new neural net-\nwork architectures. Compared to classical HPO, NAS is even more expensive in terms of\ncomputation and requires additional eﬀorts to remain feasible in practice. Both, HPO and\nNAS can be considered as sub-ﬁelds of AutoML (Hutter et al., 2019), which aims to automate\nthe entire ML pipeline.\nIn this section we will introduce HPO and show how we can automatically ﬁnd the best\nhyperparameters of the logistic regression example introduced in Section 4.5.\n19.1.1 The Optimization Problem\n\n861\nWhat Is Hyperparameter Optimization?\nWe will start with a simple toy problem: searching for the learning rate of the multi-class\nlogistic regression model SoftmaxRegression from Section 4.5 to minimize the validation\nerror on the Fashion MNIST dataset. While other hyperparameters like batch size or number\nof epochs are also worth tuning, we focus on learning rate alone for simplicity.\nimport numpy as np\nimport torch\nfrom scipy import stats\nfrom torch import nn\nfrom d2l import torch as d2l\nBefore we can run HPO, we ﬁrst need to deﬁne two ingredients: the objective function and\nthe conﬁguration space.\nThe Objective Function\nThe performance of a learning algorithm can be seen as a function f : X →R that maps\nfrom the hyperparameter space x ∈X to the validation loss. For every evaluation of f (x),\nwe have to train and validate our machine learning model, which can be time and compute\nintensive in the case of deep neural networks trained on large datasets. Given our criterion\nf (x) our goal is to ﬁnd x⋆∈argminx∈X f (x).\nThere is no simple way to compute gradients of f with respect to x, because it would require\nto propagate the gradient through the entire training process. While there is recent work\n(Franceschi et al., 2017, Maclaurin et al., 2015) to drive HPO by approximate “hypergradi-\nents”, none of the existing approaches are competitive with the state-of-the-art yet, and we\nwill not discuss them here. Furthermore, the computational burden of evaluating f requires\nHPO algorithms to approach the global optimum with as few samples as possible.\nThe training of neural networks is stochastic (e.g., weights are randomly initialized, mini-\nbatches are randomly sampled), so that our observations will be noisy: y ∼f (x) + ϵ, where\nwe usually assume that the ϵ ∼N(0, σ) observation noise is Gaussian distributed.\nFaced with all these challenges, we usually try to identify a small set of well performing\nhyperparameter conﬁgurations quickly, instead of hitting the global optima exactly. However,\ndue to large computational demands of most neural networks models, even this can take days\nor weeks of compute. We will explore in Section 19.4 how we can speed-up the optimization\nprocess by either distributing the search or using cheaper-to-evaluate approximations of the\nobjective function.\nWe begin with a method for computing the validation error of a model.\nclass HPOTrainer(d2l.Trainer):\n#@save\ndef validation_error(self):\nself.model.eval()\naccuracy = 0\n(continues on next page)\n\n862\nHyperparameter Optimization\n(continued from previous page)\nval_batch_idx = 0\nfor batch in self.val_dataloader:\nwith torch.no_grad():\nx, y = self.prepare_batch(batch)\ny_hat = self.model(x)\naccuracy += self.model.accuracy(y_hat, y)\nval_batch_idx += 1\nreturn 1 -\naccuracy / val_batch_idx\nWe optimize validation error with respect to the hyperparameter conﬁguration config, con-\nsisting of the learning_rate. For each evaluation, we train our model for max_epochs\nepochs, then compute and return its validation error:\ndef hpo_objective_softmax_classification(config, max_epochs=8):\nlearning_rate = config[\"learning_rate\"]\ntrainer = d2l.HPOTrainer(max_epochs=max_epochs)\ndata = d2l.FashionMNIST(batch_size=16)\nmodel = d2l.SoftmaxRegression(num_outputs=10, lr=learning_rate)\ntrainer.fit(model=model, data=data)\nreturn trainer.validation_error().detach().numpy()\nThe Conﬁguration Space\nAlong with the objective function f (x), we also need to deﬁne the feasible set x ∈X to opti-\nmize over, known as conﬁguration space or search space. For our logistic regression example,\nwe will use:\nconfig_space = {\"learning_rate\": stats.loguniform(1e-4, 1)}\nHere we use the use the loguniform object from SciPy, which represents a uniform distri-\nbution between -4 and -1 in the logarithmic space. This object allows us to sample random\nvariables from this distribution.\nEach hyperparameter has a data type, such as float for learning_rate, as well as a closed\nbounded range (i.e., lower and upper bounds). We usually assign a prior distribution (e.g,\nuniform or log-uniform) to each hyperparameter to sample from. Some positive parameters,\nsuch as learning_rate, are best represented on a logarithmic scale as optimal values can\ndiﬀer by several orders of magnitude, while others, such as momentum, come with linear\nscale.\nBelow we show a simple example of a conﬁguration space consisting of typical hyperparam-\neters of a multi-layer perceptron including their type and standard ranges.\nTable 19.1.1: Example conﬁguration space of multi-layer perceptron\n\n863\nWhat Is Hyperparameter Optimization?\nName\nType\nHyperparameter\nRanges\nlog-scale\nlearning rate\nﬂoat\n[10−6, 10−1]\nyes\nbatch size\ninteger\n[8, 256]\nyes\nmomentum\nﬂoat\n[0, 0.99]\nno\nactivation function\ncategorical\n{tanh, relu}\n•\nnumber of units\ninteger\n[32, 1024]\nyes\nnumber of layers\ninteger\n[1, 6]\nno\nIn general, the structure of the conﬁguration space X can be complex and it can be quite\ndiﬀerent from Rd. In practice, some hyperparameters may depend on the value of others.\nFor example, assume we try to tune the number of layers for a multi-layer perceptron, and\nfor each layer the number of units. The number of units of the l-th layer is relevant only if\nthe network has at least l + 1 layers. These advanced HPO problems are beyond the scope\nof this chapter. We refer the interested reader to (Baptista and Poloczek, 2018, Hutter et al.,\n2011, Jenatton et al., 2017).\nThe conﬁguration space plays an important role for hyperparameter optimization, since no\nalgorithms can ﬁnd something that is not included in the conﬁguration space. On the other\nhand, if the ranges are too large, the computation budget to ﬁnd well performing conﬁgura-\ntions might become infeasible.\n19.1.2 Random Search\nRandom search is the ﬁrst hyperparameter optimization algorithm we will consider. The main\nidea of random search is to independently sample from the conﬁguration space until a pre-\ndeﬁned budget (e.g maximum number of iterations) is exhausted, and to return the best ob-\nserved conﬁguration. All evaluations can be executed independently in parallel (see Section\n19.3), but here we use a sequential loop for simplicity.\nerrors, values = [], []\nnum_iterations = 5\nfor i in range(num_iterations):\nlearning_rate = config_space[\"learning_rate\"].rvs()\nprint(f\"Trial {i}: learning_rate = {learning_rate}\")\ny = hpo_objective_softmax_classification({\"learning_rate\": learning_rate})\nprint(f\"\nvalidation_error = {y}\")\nvalues.append(learning_rate)\nerrors.append(y)\nvalidation_error = 0.21079999208450317\nThe best learning rate is then simply the one with the lowest validation error.\n\n864\nHyperparameter Optimization\nbest_idx = np.argmin(errors)\nprint(f\"optimal learning rate = {values[best_idx]}\")\noptimal learning rate = 0.05458176607692195\nDue to its simplicity and generality, random search is one of the most frequently used HPO\nalgorithms. It does not require any sophisticated implementation and can be applied to any\n\n865\nWhat Is Hyperparameter Optimization?\nconﬁguration space as long as we can deﬁne some probability distribution for each hyperpa-\nrameter.\nUnfortunately random search also comes with a few shortcomings. First, it does not adapt\nthe sampling distribution based on the previous observations it collected so far. Hence, it is\nequally likely to sample a poorly performing conﬁguration than a better performing conﬁg-\nuration. Second, the same amount of resources are spent for all conﬁgurations, even though\nsome may show poor initial performance and are less likely to outperform previously seen\nconﬁgurations.\nIn the next sections we will look at more sample eﬃcient hyperparameter optimization al-\ngorithms that overcome the shortcomings of random search by using a model to guide the\nsearch. We will also look at algorithms that automatically stop the evaluation process of poorly\nperforming conﬁgurations to speed up the optimization process.\n19.1.3 Summary\nIn this section we introduced hyperparameter optimization (HPO) and how we can phrase it\nas a global optimization by deﬁning a conﬁguration space and an objective function. We also\nimplemented our ﬁrst HPO algorithm, random search, and applied it on a simple softmax\nclassiﬁcation problem.\n\n866\nHyperparameter Optimization\nWhile random search is very simple, it is the better alternative to grid search, which simply\nevaluates a ﬁxed set of hyperparameters. Random search somewhat mitigates the curse of\ndimensionality (Bellman, 1966), and can be far more eﬃcient than grid search if the criterion\nmost strongly depends on a small subset of the hyperparameters.\n19.1.4 Exercises\n1. In this chapter, we optimize the validation error of a model after training on a disjoint\ntraining set. For simplicity, our code uses Trainer.val_dataloader, which maps to a\nloader around FashionMNIST.val.\n1. Convince yourself (by looking at the code) that this means we use the original Fash-\nionMNIST training set (60000 examples) for training, and the original test set (10000\nexamples) for validation.\n2. Why could this practice be problematic? Hint: Re-read Section 3.6, especially about\nmodel selection.\n3. What should we have done instead?\n2. We stated above that hyperparameter optimization by gradient descent is very hard to do.\nConsider a small problem, such as training a two-layer perceptron on the FashionMNIST\ndataset (Section 5.2) with a batch size of 256. We would like to tune the learning rate of\nSGD in order to minimize a validation metric after one epoch of training.\n1. Why cannot we use validation error for this purpose? What metric on the validation\nset would you use?\n2. Sketch (roughly) the computational graph of the validation metric after training for\none epoch. You may assume that initial weights and hyperparameters (such as learning\nrate) are input nodes to this graph. Hint: Re-read about computational graphs in Section\n5.3.\n3. Give a rough estimate of the number of ﬂoating point values you need to store during\na forward pass on this graph. Hint: FashionMNIST has 60000 cases. Assume the re-\nquired memory is dominated by the activations after each layer, and look up the layer\nwidths in Section 5.2.\n4. Apart from the sheer amount of compute and storage required, what other issues would\ngradient-based hyperparameter optimization run into? Hint: Re-read about vanishing\nand exploding gradients in Section 5.4.\n5. Advanced: Read (Maclaurin et al., 2015) for an elegant (yet still somewhat unpractical)\napproach to gradient-based HPO.\n3. Grid search is another HPO baseline, where we deﬁne an equi-spaced grid for each hy-\nperparameter, then iterate over the (combinatorial) Cartesian product in order to suggest\nconﬁgurations.\n\n867\nHyperparameter Optimization API\n265\n1. We stated above that random search can be much more eﬃcient than grid search for\nHPO on a sizable number of hyperparameters, if the criterion most strongly depends\non a small subset of the hyperparameters. Why is this? Hint: Read (Bergstra et al.,\n2011).\nDiscussions265.\n19.2 Hyperparameter Optimization API\nBefore we dive into the methodology, we will ﬁrst discuss a basic code structure that allows us\nto eﬃciently implement various HPO algorithms. In general, all HPO algorithms considered\nhere need to implement two decision making primitives, searching and scheduling. First, they\nneed to sample new hyperparameter conﬁgurations, which often involves some kind of search\nover the conﬁguration space. Second, for each conﬁguration, an HPO algorithm needs to\nschedule its evaluation and decide how many resources to allocate for it. Once we start to\nevaluate a conﬁguration, we will refer to it as a trial. We map these decisions to two classes,\nHPOSearcher and HPOScheduler. On top of that, we also provide a HPOTuner class that\nexecutes the optimization process.\nThis concept of scheduler and searcher is also implemented in popular HPO libraries, such\nas Syne Tune (Salinas et al., 2022), Ray Tune (Liaw et al., 2018) or Optuna (Akiba et al.,\n2019).\nimport time\nfrom scipy import stats\nfrom d2l import torch as d2l\n19.2.1 Searcher\nBelow we deﬁne a base class for searchers, which provides a new candidate conﬁguration\nthrough the sample_configuration function. A simple way to implement this function\nwould be to sample conﬁgurations uniformly at random, as we did for random search in\nSection 19.1. More sophisticated algorithms, such as Bayesian optimization, will make these\ndecisions based on the performance of previous trials. As a result, these algorithms are able\nto sample more promising candidates over time. We add the update function in order to\nupdate the history of previous trials, which can then be exploited to improve our sampling\ndistribution.\nclass HPOSearcher(d2l.HyperParameters):\n#@save\ndef sample_configuration() -> dict:\nraise NotImplementedError\n(continues on next page)\n\n868\nHyperparameter Optimization\n(continued from previous page)\ndef update(self, config: dict, error: float, additional_info=None):\npass\nThe following code shows how to implement our random search optimizer from the previous\nsection in this API. As a slight extension, we allow the user to prescribe the ﬁrst conﬁguration\nto be evaluated via initial_config, while subsequent ones are drawn at random.\nclass RandomSearcher(HPOSearcher):\n#@save\ndef __init__(self, config_space: dict, initial_config=None):\nself.save_hyperparameters()\ndef sample_configuration(self) -> dict:\nif self.initial_config is not None:\nresult = self.initial_config\nself.initial_config = None\nelse:\nresult = {\nname: domain.rvs()\nfor name, domain in self.config_space.items()\n}\nreturn result\n19.2.2 Scheduler\nBeyond sampling conﬁgurations for new trials, we also need to decide when and for how long\nto run a trial. In practice, all these decisions are done by the HPOScheduler, which delegates\nthe choice of new conﬁgurations to a HPOSearcher. The suggest method is called whenever\nsome resource for training becomes available. Apart from invoking sample_configuration\nof a searcher, it may also decide upon parameters like max_epochs (i.e., how long to train the\nmodel for). The update method is called whenever a trial returns a new observation.\nclass HPOScheduler(d2l.HyperParameters):\n#@save\ndef suggest(self) -> dict:\nraise NotImplementedError\ndef update(self, config: dict, error: float, info=None):\nraise NotImplementedError\nTo implement random search, but also other HPO algorithms, we only need a basic scheduler\nthat schedules a new conﬁguration every time new resources become available.\nclass BasicScheduler(HPOScheduler):\n#@save\ndef __init__(self, searcher: HPOSearcher):\nself.save_hyperparameters()\ndef suggest(self) -> dict:\n(continues on next page)\n\n869\nHyperparameter Optimization API\n(continued from previous page)\nreturn self.searcher.sample_configuration()\ndef update(self, config: dict, error: float, info=None):\nself.searcher.update(config, error, additional_info=info)\n19.2.3 Tuner\nFinally, we need a component that runs the scheduler/searcher and does some book-keeping\nof the results. The following code implements a sequential execution of the HPO trials that\nevaluates one training job after the next and will serve as a basic example. We will later use\nSyne Tune for more scalable distributed HPO cases.\nclass HPOTuner(d2l.HyperParameters):\n#@save\ndef __init__(self, scheduler: HPOScheduler, objective: callable):\nself.save_hyperparameters()\n# Bookeeping results for plotting\nself.incumbent = None\nself.incumbent_error = None\nself.incumbent_trajectory = []\nself.cumulative_runtime = []\nself.current_runtime = 0\nself.records = []\ndef run(self, number_of_trials):\nfor i in range(number_of_trials):\nstart_time = time.time()\nconfig = self.scheduler.suggest()\nprint(f\"Trial {i}: config = {config}\")\nerror = self.objective(**config)\nerror = float(error.cpu().detach().numpy())\nself.scheduler.update(config, error)\nruntime = time.time() - start_time\nself.bookkeeping(config, error, runtime)\nprint(f\"\nerror = {error}, runtime = {runtime}\")\n19.2.4 Bookkeeping the Performance of HPO Algorithms\nWith any HPO algorithm, we are mostly interested in the best performing conﬁguration\n(called incumbent) and its validation error after a given wall-clock time. This is why we track\nruntime per iteration, which includes both the time to run an evaluation (call of objective)\nand the time to make a decision (call of scheduler.suggest). In the sequel, we will plot\ncumulative_runtime against incumbent_trajectory in order to visualize the any-time\nperformance of the HPO algorithm deﬁned in terms of scheduler (and searcher). This\nallows us to quantify not only how well the conﬁguration found by an optimizer works, but\nalso how quickly an optimizer is able to ﬁnd it.\n\n870\nHyperparameter Optimization\n@d2l.add_to_class(HPOTuner)\n#@save\ndef bookkeeping(self, config: dict, error: float, runtime: float):\nself.records.append({\"config\": config, \"error\": error, \"runtime\": runtime})\n# Check if the last hyperparameter configuration performs better\n# than the incumbent\nif self.incumbent is None or self.incumbent_error > error:\nself.incumbent = config\nself.incumbent_error = error\n# Add current best observed performance to the optimization trajectory\nself.incumbent_trajectory.append(self.incumbent_error)\n# Update runtime\nself.current_runtime += runtime\nself.cumulative_runtime.append(self.current_runtime)\n19.2.5 Example: Optimizing the Hyperparameters of a Convolutional\nNeural Network\nWe now use our new implementation of random search to optimize the batch size and learning\nrate of the LeNet convolutional neural network from Section 7.6. We being by deﬁning the\nobjective function, which will once more be validation error.\ndef hpo_objective_lenet(learning_rate, batch_size, max_epochs=10):\n#@save\nmodel = d2l.LeNet(lr=learning_rate, num_classes=10)\ntrainer = d2l.HPOTrainer(max_epochs=max_epochs, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=batch_size)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\ntrainer.fit(model=model, data=data)\nvalidation_error = trainer.validation_error()\nreturn validation_error\nWe also need to deﬁne the conﬁguration space. Moreover, the ﬁrst conﬁguration to be eval-\nuated is the default setting used in Section 7.6.\nconfig_space = {\n\"learning_rate\": stats.loguniform(1e-2, 1),\n\"batch_size\": stats.randint(32, 256),\n}\ninitial_config = {\n\"learning_rate\": 0.1,\n\"batch_size\": 128,\n}\nNow we can start our random search:\nsearcher = RandomSearcher(config_space, initial_config=initial_config)\nscheduler = BasicScheduler(searcher=searcher)\ntuner = HPOTuner(scheduler=scheduler, objective=hpo_objective_lenet)\ntuner.run(number_of_trials=5)\n\n871\nHyperparameter Optimization API\nerror = 0.47701597213745117, runtime = 90.8468370437622\nBelow we plot the optimization trajectory of the incumbent to get the any-time performance\nof random search:\nboard = d2l.ProgressBoard(xlabel=\"time\", ylabel=\"error\")\nfor time_stamp, error in zip(\ntuner.cumulative_runtime, tuner.incumbent_trajectory\n(continues on next page)\n\n872\nHyperparameter Optimization\n(continued from previous page)\n):\nboard.draw(time_stamp, error, \"random search\", every_n=1)\n19.2.6 Comparing HPO Algorithms\nJust as with training algorithms or model architectures, it is important to understand how to\nbest compare diﬀerent HPO algorithms. Each HPO run depends on two major sources of\n\n873\nHyperparameter Optimization API\nrandomness: the random eﬀects of the training process, such as random weight initialization\nor mini-batch ordering, and the intrinsic randomness of the HPO algorithm itself, such as\nthe random sampling of random search. Hence, when comparing diﬀerent algorithms, it is\ncrucial to run each experiment several times and report statistics, such as mean or median,\nacross a population of multiple repetitions of an algorithm based on diﬀerent seeds of the\nrandom number generator.\nTo illustrate this, we compare random search (see Section 19.1.2) and Bayesian optimiza-\ntion (Snoek et al., 2012) on tuning the hyperparameters of a feed-forward neural network.\nEach algorithm was evaluated 50 times with a diﬀerent random seed. The solid line indicates\nthe average performance of the incumbent across these 50 repetitions and the dashed line\nthe standard deviation. We can see that random search and Bayesian optimization perform\nroughly the same up to ~1000 seconds, but Bayesian optimization can make use of the past\nobservation to identify better conﬁgurations and thus quickly outperforms random search\nafterwards.\nt\nFig. 19.2.1\nExample any-time performance plot to compare two algorithms A and B.\n19.2.7 Summary\nThis section laid out a simple, yet ﬂexible interface to implement various HPO algorithms\nthat we will look at in this chapter. Similar interfaces can be found in popular open-source\nHPO frameworks. We also looked at how we can compare HPO algorithms, and potential\npitfall one needs to be aware.\n19.2.8 Exercises\n1. The goal of this exercise is to implement the objective function for a slightly more chal-\nlenging HPO problem, and to run more realistic experiments. We will use the two hidden\nlayer MLP DropoutMLP implemented in Section 5.6.\n1. Code up the objective function, which should depend on all hyperparameters of the\nmodel and batch_size. Use max_epochs=50. GPUs do not help here, so num_gpus=0.\nHint: Modify hpo_objective_lenet.\n\n874\nHyperparameter Optimization\n266\n2. Choose a sensible search space, where num_hiddens_1, num_hiddens_2 are integers\nin [8, 1024], and dropout values lie in [0, 0.95], while batch_size lies in [16, 384].\nProvide code for config_space, using sensible distributions from scipy.stats.\n3. Run random search on this example with number_of_trials=20 and plot the results.\nMake sure to ﬁrst evaluate the default conﬁguration of Section 5.6, which is ini-\ntial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256, 'dropout_1':\n0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256}.\n2. In this exercise, you will implement a new searcher (subclass of HPOSearcher) which\nmakes decisions based on past data. It depends on parameters probab_local, num_init_random.\nIts sample_configuration method works as follows. For the ﬁrst num_init_random\ncalls, do the same as RandomSearcher.sample_configuration. Otherwise, with prob-\nability 1 - probab_local, do the same as RandomSearcher.sample_configuration.\nOtherwise, pick the conﬁguration which attained the smallest validation error so far, select\none of its hyperparameters at random, and sample its value randomly like in RandomSearcher.\nsample_configuration, but leave all other values the same. Return this conﬁguration,\nwhich is identical to the best conﬁguration so far, except in this one hyperparameter.\n1. Code up this new LocalSearcher. Hint: Your searcher requires config_space as\nargument at construction. Feel free to use a member of type RandomSearcher. You\nwill also have to implement the update method.\n2. Re-run the experiment from the previous exercise, but using your new searcher instead\nof RandomSearcher. Experiment with diﬀerent values for probab_local, num_init_random.\nHowever, note that a proper comparison between diﬀerent HPO methods requires re-\npeating experiments several times, and ideally considering a number of benchmark\ntasks.\nDiscussions266.\n19.3 Asynchronous Random Search\nAs we have seen in the previous Section 19.2, we might have to wait hours or even days\nbefore random search returns a good hyperparameter conﬁguration, because of the expensive\nevaluation of hyperparameter conﬁgurations. In practice, we have often access to a pool of\nresources such as multiple GPUs on the same machine or multiple machines with a single\nGPU. This begs the question: How do we eﬃciently distribute random search?\nIn general, we distinguish between synchronous and asynchronous parallel hyperparameter\noptimization (see Fig. 19.3.1). In the synchronous setting, we wait for all concurrently run-\nning trials to ﬁnish, before we start the next batch. Consider conﬁguration spaces that contain\nhyperparameters such as the number of ﬁlters or number of layers of a deep neural network.\nHyperparameter conﬁgurations that contain a larger number of layers of ﬁlters will naturally\n\n875\nAsynchronous Random Search\ntake more time to ﬁnish, and all other trials in the same batch will have to wait at synchronisa-\ntion points (grey area in Fig. 19.3.1) before we can continue the optimization process.\nIn the asynchronous setting we immediately schedule a new trial as soon as resources become\navailable. This will optimally exploit our resources, since we can avoid any synchronisation\noverhead. For random search, each new hyperparameter conﬁguration is chosen indepen-\ndently of all others, and in particular without exploiting observations from any prior eval-\nuation. This means we can trivially parallelize random search asynchronously. This is not\nstraight-forward with more sophisticated methods that make decision based on previous ob-\nservations (see Section 19.5). While we need access to more resources than in the sequential\nsetting, asynchronous random search exhibits a linear speed-up, in that a certain performance\nis reached K times faster if K trials can be run in parallel.\nSequential\nTrial-0\nTrial-1\nTrial-2\nTrial-3\nTrial-4\nSynchronous\nAsynchronous\nTrial-0\nTrial-2\nTrial-3\nTrial-0\nTime\nTrial-5\nTrial-1\nTrial-1\nTrial-4\nTrial-5\nTrial-3\nTrial-2\nTrial-4\nTrial-5\nt\nFig. 19.3.1\nDistributing the hyperparameter optimization process either synchronously or\nasynchronously. Compared to the sequential setting, we can reduce the overall wall-clock\ntime while keep the total compute constant. Synchronous scheduling might lead to idling\nworkers in the case of stragglers.\nIn this notebook, we will look at asynchronous random search that, where trials are executed\nin multiple python processes on the same machine. Distributed job scheduling and execution\nis diﬃcult to implement from scratch. We will use Syne Tune (Salinas et al., 2022), which\nprovides us with a simple interface for asynchronous HPO. Syne Tune is designed to be run\nwith diﬀerent execution back-ends, and the interested reader is invited to study its simple\nAPIs in order to learn more about distributed HPO.\nimport logging\nfrom d2l import torch as d2l\nlogging.basicConfig(level=logging.INFO)\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune.backend.python_backend import PythonBackend\nfrom syne_tune.config_space import loguniform, randint\nfrom syne_tune.experiments import load_experiment\nfrom syne_tune.optimizer.baselines import RandomSearch\nINFO:root:SageMakerBackend is not imported since dependencies are missing. You␣\n(continues on next page)\n\n876\nHyperparameter Optimization\n(continued from previous page)\n,→can install them with\npip install 'syne-tune[extra]'\nAWS dependencies are not imported since dependencies are missing. You can␣\n,→install them with\npip install 'syne-tune[aws]'\nor (for everything)\npip install 'syne-tune[extra]'\nAWS dependencies are not imported since dependencies are missing. You can␣\n,→install them with\npip install 'syne-tune[aws]'\nor (for everything)\npip install 'syne-tune[extra]'\nINFO:root:Ray Tune schedulers and searchers are not imported since␣\n,→dependencies are missing. You can install them with\npip install 'syne-tune[raytune]'\nor (for everything)\npip install 'syne-tune[extra]'\n19.3.1 Objective Function\nFirst, we have to deﬁne a new objective function such that it now returns the performance\nback to Syne Tune via the report callback.\ndef hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\nfrom d2l import torch as d2l\nmodel = d2l.LeNet(lr=learning_rate, num_classes=10)\ntrainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=batch_size)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\nreport = Reporter()\nfor epoch in range(1, max_epochs + 1):\nif epoch == 1:\n# Initialize the state of Trainer\ntrainer.fit(model=model, data=data)\nelse:\ntrainer.fit_epoch()\nvalidation_error = trainer.validation_error().cpu().detach().numpy()\nreport(epoch=epoch, validation_error=float(validation_error))\nNote that the PythonBackend of Syne Tune requires dependencies to be imported inside the\nfunction deﬁnition.\n19.3.2 Asynchronous Scheduler\nFirst, we deﬁne the number of workers that evaluate trials concurrently. We also need to\nspecify how long we want to run random search, by deﬁning an upper limit on the total wall-\nclock time.\n\n877\nAsynchronous Random Search\nn_workers = 2\n# Needs to be <= the number of available GPUs\nmax_wallclock_time = 12 * 60\n# 12 minutes\nNext, we state which metric we want to optimize and whether we want to minimize or max-\nimize this metric. Namely, metric needs to correspond to the argument name passed to the\nreport callback.\nmode = \"min\"\nmetric = \"validation_error\"\nWe use the conﬁguration space from our previous example. In Syne Tune, this dictionary can\nalso be used to pass constant attributes to the training script. We make use of this feature\nin order to pass max_epochs. Moreover, we specify the ﬁrst conﬁguration to be evaluated in\ninitial_config.\nconfig_space = {\n\"learning_rate\": loguniform(1e-2, 1),\n\"batch_size\": randint(32, 256),\n\"max_epochs\": 10,\n}\ninitial_config = {\n\"learning_rate\": 0.1,\n\"batch_size\": 128,\n}\nNext, we need to specify the back-end for job executions. Here we just consider the distri-\nbution on a local machine where parallel jobs are executed as sub-processes. However, for\nlarge scale HPO, we could run this also on a cluster or cloud environment, where each trial\nconsumes a full instance.\ntrial_backend = PythonBackend(\ntune_function=hpo_objective_lenet_synetune,\nconfig_space=config_space,\n)\nWe can now create the scheduler for asynchronous random search, which is similar in be-\nhaviour to our BasicScheduler from Section 19.2.\nscheduler = RandomSearch(\nconfig_space,\nmetric=metric,\nmode=mode,\npoints_to_evaluate=[initial_config],\n)\nINFO:syne_tune.optimizer.schedulers.fifo:max_resource_level = 10, as inferred␣\n(continues on next page)\n\n878\nHyperparameter Optimization\n(continued from previous page)\n,→from config_space\nINFO:syne_tune.optimizer.schedulers.fifo:Master random_seed = 218748764\nSyne Tune also features a Tuner, where the main experiment loop and bookkeeping is cen-\ntralized, and interactions between scheduler and back-end are mediated.\nstop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)\ntuner = Tuner(\ntrial_backend=trial_backend,\nscheduler=scheduler,\nstop_criterion=stop_criterion,\nn_workers=n_workers,\nprint_update_interval=int(max_wallclock_time * 0.6),\n)\nLet us run our distributed HPO experiment. According to our stopping criterion, it will run\nfor about 12 minutes.\ntuner.run()\nINFO:syne_tune.tuner:results of trials will be saved on /home/ubuntu/syne-tune/\n,→python-entrypoint-2023-08-18-23-04-42-649\nINFO:root:Detected 8 GPUs\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.1 --batch_\n,→size 128 --max_epochs 10 --tune_function_root /home/ubuntu/syne-tune/python-\n,→entrypoint-2023-08-18-23-04-42-649/tune_function --tune_function_hash␣\n,→c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /home/ubuntu/syne-tune/\n,→python-entrypoint-2023-08-18-23-04-42-649/0/checkpoints\nINFO:syne_tune.tuner:(trial 0) - scheduled config {'learning_rate': 0.1,\n,→'batch_size': 128, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→11573821391982765 --batch_size 249 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/1/checkpoints\nINFO:syne_tune.tuner:(trial 1) - scheduled config {'learning_rate': 0.\n,→11573821391982765, 'batch_size': 249, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 0 completed.\nINFO:syne_tune.tuner:Trial trial_id 1 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→1270190537228491 --batch_size 180 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n(continues on next page)\n\n879\nAsynchronous Random Search\n(continued from previous page)\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/2/checkpoints\nINFO:syne_tune.tuner:(trial 2) - scheduled config {'learning_rate': 0.\n,→1270190537228491, 'batch_size': 180, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→013539985569129874 --batch_size 44 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/3/checkpoints\nINFO:syne_tune.tuner:(trial 3) - scheduled config {'learning_rate': 0.\n,→013539985569129874, 'batch_size': 44, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 2 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→5623908565974269 --batch_size 117 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/4/checkpoints\nINFO:syne_tune.tuner:(trial 4) - scheduled config {'learning_rate': 0.\n,→5623908565974269, 'batch_size': 117, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 3 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→4237481357205294 --batch_size 217 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/5/checkpoints\nINFO:syne_tune.tuner:(trial 5) - scheduled config {'learning_rate': 0.\n,→4237481357205294, 'batch_size': 217, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 4 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→14543999948294017 --batch_size 100 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/6/checkpoints\nINFO:syne_tune.tuner:(trial 6) - scheduled config {'learning_rate': 0.\n,→14543999948294017, 'batch_size': 100, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 5 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→013237796900392465 --batch_size 184 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/7/checkpoints\nINFO:syne_tune.tuner:(trial 7) - scheduled config {'learning_rate': 0.\n,→013237796900392465, 'batch_size': 184, 'max_epochs': 10}\n(continues on next page)\n\n880\nHyperparameter Optimization\n(continued from previous page)\nINFO:syne_tune.tuner:Trial trial_id 6 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→019419074864676922 --batch_size 147 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/8/checkpoints\nINFO:syne_tune.tuner:(trial 8) - scheduled config {'learning_rate': 0.\n,→019419074864676922, 'batch_size': 147, 'max_epochs': 10}\nINFO:syne_tune.tuner:tuning status (last metric is reported)\ntrial_id\nstatus\niter\nlearning_rate\nbatch_size\nmax_epochs\nepoch ␣\n,→validation_error\nworker-time\n0\nCompleted\n10\n0.100000\n128\n10\n10.0\n␣\n,→\n0.268987\n98.142261\n1\nCompleted\n10\n0.115738\n249\n10\n10.0\n␣\n,→\n0.380865\n94.529288\n2\nCompleted\n10\n0.127019\n180\n10\n10.0\n␣\n,→\n0.275060\n92.739022\n3\nCompleted\n10\n0.013540\n44\n10\n10.0\n␣\n,→\n0.421119\n128.757522\n4\nCompleted\n10\n0.562391\n117\n10\n10.0\n␣\n,→\n0.155812\n97.749136\n5\nCompleted\n10\n0.423748\n217\n10\n10.0\n␣\n,→\n0.234740\n95.035502\n6\nCompleted\n10\n0.145440\n100\n10\n10.0\n␣\n,→\n0.227300\n97.266276\n7 InProgress\n9\n0.013238\n184\n10\n9.0\n␣\n,→\n0.900074\n79.913983\n8 InProgress\n0\n0.019419\n147\n10\n-\n␣\n,→\n-\n-\n2 trials running, 7 finished (7 until the end), 436.49s wallclock-time\nINFO:syne_tune.tuner:Trial trial_id 7 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→10014301952595842 --batch_size 101 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/9/checkpoints\nINFO:syne_tune.tuner:(trial 9) - scheduled config {'learning_rate': 0.\n,→10014301952595842, 'batch_size': 101, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 8 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→011852303154416318 --batch_size 191 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_\n,→function --tune_function_hash c3e045478958babaa9af3aa66092328d --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-\n,→649/10/checkpoints\nINFO:syne_tune.tuner:(trial 10) - scheduled config {'learning_rate': 0.\n(continues on next page)\n\n881\nAsynchronous Random Search\n(continued from previous page)\n,→011852303154416318, 'batch_size': 191, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 9 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→04352474626996822 --batch_size 45 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/11/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 11) - scheduled config {'learning_rate': 0.\n,→04352474626996822, 'batch_size': 45, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 10 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→6523407101123465 --batch_size 240 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/12/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 12) - scheduled config {'learning_rate': 0.\n,→6523407101123465, 'batch_size': 240, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 12 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→0431019557345446 --batch_size 73 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/tune_function --\n,→tune_function_hash c3e045478958babaa9af3aa66092328d --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649/13/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 13) - scheduled config {'learning_rate': 0.\n,→0431019557345446, 'batch_size': 73, 'max_epochs': 10}\nINFO:syne_tune.stopping_criterion:reaching max wallclock time (720), stopping␣\n,→there.\nINFO:syne_tune.tuner:Stopping trials that may still be running.\nINFO:syne_tune.tuner:Tuning finished, results of trials can be found on /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-04-42-649\n--------------------\nResource summary (last result is reported):\ntrial_id\nstatus\niter\nlearning_rate\nbatch_size\nmax_epochs\nepoch ␣\n,→validation_error\nworker-time\n0\nCompleted\n10\n0.100000\n128\n10\n10.0\n␣\n,→\n0.268987\n98.142261\n1\nCompleted\n10\n0.115738\n249\n10\n10.0\n␣\n,→\n0.380865\n94.529288\n2\nCompleted\n10\n0.127019\n180\n10\n10.0\n␣\n,→\n0.275060\n92.739022\n3\nCompleted\n10\n0.013540\n44\n10\n10.0\n␣\n,→\n0.421119\n128.757522\n4\nCompleted\n10\n0.562391\n117\n10\n10.0\n␣\n,→\n0.155812\n97.749136\n5\nCompleted\n10\n0.423748\n217\n10\n10.0\n␣\n,→\n0.234740\n95.035502\n(continues on next page)\n\n882\nHyperparameter Optimization\n(continued from previous page)\n6\nCompleted\n10\n0.145440\n100\n10\n10.0\n␣\n,→\n0.227300\n97.266276\n7\nCompleted\n10\n0.013238\n184\n10\n10.0\n␣\n,→\n0.900086\n89.168077\n8\nCompleted\n10\n0.019419\n147\n10\n10.0\n␣\n,→\n0.901410\n96.518268\n9\nCompleted\n10\n0.100143\n101\n10\n10.0\n␣\n,→\n0.280792\n96.687997\n10\nCompleted\n10\n0.011852\n191\n10\n10.0\n␣\n,→\n0.900143\n90.441659\n11 InProgress\n9\n0.043525\n45\n10\n9.0\n␣\n,→\n0.264026\n144.953945\n12\nCompleted\n10\n0.652341\n240\n10\n10.0\n␣\n,→\n0.185565\n86.874975\n13 InProgress\n0\n0.043102\n73\n10\n-\n␣\n,→\n-\n-\n2 trials running, 12 finished (12 until the end), 722.77s wallclock-time\nvalidation_error: best 0.15581202507019043 for trial-id 4\n--------------------\nThe logs of all evaluated hyperparameter conﬁgurations are stored for further analysis. At any\ntime during the tuning job, we can easily get the results obtained so far and plot the incumbent\ntrajectory.\nd2l.set_figsize()\ntuning_experiment = load_experiment(tuner.name)\ntuning_experiment.plot()\nWARNING:matplotlib.legend:No artists with labels found to put in legend.\nNote␣\n,→that artists whose label start with an underscore are ignored when legend()␣\n,→is called with no argument.\n19.3.3 Visualize the Asynchronous Optimization Process\n\n883\nAsynchronous Random Search\nBelow we visualize how the learning curves of every trial (each color in the plot represents a\ntrial) evolve during the asynchronous optimization process. At any point in time, there are as\nmany trials running concurrently as we have workers. Once a trial ﬁnishes, we immediately\nstart the next trial, without waiting for the other trials to ﬁnish. Idle time of workers is reduced\nto a minimum with asynchronous scheduling.\nd2l.set_figsize([6, 2.5])\nresults = tuning_experiment.results\nfor trial_id in results.trial_id.unique():\ndf = results[results[\"trial_id\"] == trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time\"],\ndf[\"validation_error\"],\nmarker=\"o\"\n)\nd2l.plt.xlabel(\"wall-clock time\")\nd2l.plt.ylabel(\"objective function\")\nText(0, 0.5, 'objective function')\n19.3.4 Summary\nWe can reduce the waiting time for random search substantially by distribution trials across\nparallel resources. In general, we distinguish between synchronous scheduling and asyn-\nchronous scheduling. Synchronous scheduling means that we sample a new batch of hyper-\nparameter conﬁgurations once the previous batch ﬁnished. If we have a stragglers - trials\nthat takes more time to ﬁnish than other trials - our workers need to wait at synchronization\npoints. Asynchronous scheduling evaluates a new hyperparameter conﬁgurations as soon as\nresources become available, and, hence, ensures that all workers are busy at any point in time.\nWhile random search is easy to distribute asynchronously and does not require any change\nof the actual algorithm, other methods require some additional modiﬁcations.\n\n884\nHyperparameter Optimization\n267\n268\n269\n270\n271\n19.3.5 Exercises\n1. Consider the DropoutMLP model implemented in Section 5.6, and used in Exercise 1 of\nSection 19.2.\n1. Implement an objective function hpo_objective_dropoutmlp_synetune to be used\nwith Syne Tune. Make sure that your function reports the validation error after every\nepoch.\n2. Using the setup of Exercise 1 in Section 19.2, compare random search to Bayesian\noptimization. If you use SageMaker, feel free to use Syne Tune’s benchmarking facil-\nities in order to run experiments in parallel. Hint: Bayesian optimization is provided\nas syne_tune.optimizer.baselines.BayesianOptimization.\n3. For this exercise, you need to run on an instance with at least 4 CPU cores. For one\nof the methods used above (random search, Bayesian optimization), run experiments\nwith n_workers=1, n_workers=2, n_workers=4, and compare results (incumbent\ntrajectories). At least for random search, you should observe linear scaling with respect\nto the number of workers. Hint: For robust results, you may have to average over several\nrepetitions each.\n2. Advanced. The goal of this exercise is to implement a new scheduler in Syne Tune.\n1. Create a virtual environment containing both the d2lbook267 and syne-tune268 sources.\n2. Implement the LocalSearcher from Exercise 2 in Section 19.2 as a new searcher in\nSyne Tune. Hint: Read this tutorial269. Alternatively, you may follow this example270\n.\n3. Compare your new LocalSearcher with RandomSearch on the DropoutMLP bench-\nmark.\nDiscussions271.\n19.4 Multi-Fidelity Hyperparameter Optimization\nTraining neural networks can be expensive even on moderate size datasets. Depending on\nthe conﬁguration space (Section 19.1.1), hyperparameter optimization requires tens to hun-\ndreds of function evaluations to ﬁnd a well-performing hyperparameter conﬁguration. As\nwe have seen in Section 19.3, we can signiﬁcantly speed up the overall wall-clock time of\nHPO by exploiting parallel resources, but this does not reduce the total amount of compute\nrequired.\nIn this section, we will show how the evaluation of hyperparameter conﬁgurations can be sped\nup. Methods such as random search allocate the same amount of resources (e.g., number of\n\n885\nMulti-Fidelity Hyperparameter Optimization\nepochs, training data points) to each hyperparameter evaluation. Fig. 19.4.1 depicts learn-\ning curves of a set of neural networks trained with diﬀerent hyperparameter conﬁgurations.\nAfter a few epochs we are already able to visually distinguish between well-performing and\nsuboptimal conﬁgurations. However, the learning curves are noisy, and we might still require\nthe full amount of 100 epochs to identify the best performing one.\nt\nFig. 19.4.1\nLearning curves of random hyperparameter conﬁgurations\nMulti-ﬁdelity hyperparameter optimization allocates more resources to promising conﬁgura-\ntions and stop evaluations of poorly performing ones early. This speeds up the optimization\nprocess, since we can try a larger number of conﬁgurations for the same total amount of\nresources.\nMore formally, we expand our deﬁnition in Section 19.1.1, such that our objective function\nf (x, r) gets an additional input r ∈[rmin, rmax], specifying the amount of resources that we\nare willing to spend for the evaluation of conﬁguration x. We assume that the error f (x, r)\ndecreases with r, whereas the computational cost c(x, r) increases. Typically, r represents\nthe number of epochs for training the neural network, but it could also be the training subset\nsize or the number of cross-validation folds.\nfrom collections import defaultdict\nimport numpy as np\nfrom scipy import stats\nfrom d2l import torch as d2l\nd2l.set_figsize()\n\n886\nHyperparameter Optimization\n19.4.1 Successive Halving\nOne of the simplest ways to adapt random search to the multi-ﬁdelity setting is successive\nhalving (Jamieson and Talwalkar, 2016, Karnin et al., 2013). The basic idea is to start with\nN conﬁgurations, for example randomly sampled from the conﬁguration space, and to train\neach of them for rmin epochs only. We then discard a fraction of the worst performing trials\nand train the remaining ones for longer. Iterating this process, fewer trials run for longer, until\nat least one trial reaches rmax epochs.\nMore formally, consider a minimum budget rmin (for example 1 epoch), a maximum bud-\nget rmax, for example max_epochs in our previous example, and a halving constant η ∈\n{2, 3, . . . }. For simplicity, assume that rmax = rminηK, with K ∈I . The number of initial\nconﬁgurations is then N = ηK. Let us deﬁne the set of rungs R = {rmin, rminη, rminη2, . . ., rmax}.\nOne round of successive halving proceeds as follows. We start with running N trials until the\nﬁrst rung rmin. Sorting the validation errors, we keep the top 1/η fraction (which amounts to\nηK−1 conﬁgurations) and discard all the rest. The surviving trials are trained for the next rung\n(rminη epochs), and the process is repeated. At each rung, a 1/η fraction of trials survives\nand their training continues with a η times larger budget. With this particular choice of N,\nonly a single trial will be trained to the full budget rmax. Once such a round of successive\nhalving is done, we start the next one with a new set of initial conﬁgurations, iterating until\nthe total budget is spent.\nt\nFig. 19.4.2\nLearning curves of random hyperparameter conﬁgurations.\n\n887\nMulti-Fidelity Hyperparameter Optimization\nWe subclass the HPOScheduler base class from Section 19.2 in order to implement successive\nhalving, allowing for a generic HPOSearcher object to sample conﬁgurations (which, in our\nexample below, will be a RandomSearcher). Additionally, the user has to pass the minimum\nresource rmin, the maximum resource rmax and η as input. Inside our scheduler, we maintain\na queue of conﬁgurations that still need to be evaluated for the current rung ri. We update\nthe queue every time we jump to the next rung.\nclass SuccessiveHalvingScheduler(d2l.HPOScheduler):\n#@save\ndef __init__(self, searcher, eta, r_min, r_max, prefact=1):\nself.save_hyperparameters()\n# Compute K, which is later used to determine the number of␣\n,→configurations\nself.K = int(np.log(r_max / r_min) / np.log(eta))\n# Define the rungs\nself.rung_levels = [r_min * eta ** k for k in range(self.K + 1)]\nif r_max not in self.rung_levels:\n# The final rung should be r_max\nself.rung_levels.append(r_max)\nself.K += 1\n# Bookkeeping\nself.observed_error_at_rungs = defaultdict(list)\nself.all_observed_error_at_rungs = defaultdict(list)\n# Our processing queue\nself.queue = []\nIn the beginning our queue is empty, and we ﬁll it with n = prefact·ηK conﬁgurations, which\nare ﬁrst evaluated on the smallest rung rmin. Here, prefact allows us to reuse our code in a\ndiﬀerent context. For the purpose of this section, we ﬁx prefact = 1. Every time resources\nbecome available and the HPOTuner object queries the suggest function, we return an el-\nement from the queue. Once we ﬁnish one round of successive halving, which means that\nwe evaluated all surviving conﬁgurations on the highest resource level rmax and our queue\nis empty, we start the entire process again with a new, randomly sampled set of conﬁgura-\ntions.\n@d2l.add_to_class(SuccessiveHalvingScheduler)\n#@save\ndef suggest(self):\nif len(self.queue) == 0:\n# Start a new round of successive halving\n# Number of configurations for the first rung:\nn0 = int(self.prefact * self.eta ** self.K)\nfor _ in range(n0):\nconfig = self.searcher.sample_configuration()\nconfig[\"max_epochs\"] = self.r_min\n# Set r = r_min\nself.queue.append(config)\n# Return an element from the queue\nreturn self.queue.pop()\nWhen we collected a new data point, we ﬁrst update the searcher module. Afterwards we\ncheck if we already collect all data points on the current rung. If so, we sort all conﬁgurations\nand push the top 1\nη conﬁgurations into the queue.\n\n888\nHyperparameter Optimization\n@d2l.add_to_class(SuccessiveHalvingScheduler)\n#@save\ndef update(self, config: dict, error: float, info=None):\nri = int(config[\"max_epochs\"])\n# Rung r_i\n# Update our searcher, e.g if we use Bayesian optimization later\nself.searcher.update(config, error, additional_info=info)\nself.all_observed_error_at_rungs[ri].append((config, error))\nif ri < self.r_max:\n# Bookkeeping\nself.observed_error_at_rungs[ri].append((config, error))\n# Determine how many configurations should be evaluated on this rung\nki = self.K - self.rung_levels.index(ri)\nni = int(self.prefact * self.eta ** ki)\n# If we observed all configuration on this rung r_i, we estimate the\n# top 1 / eta configuration, add them to queue and promote them for\n# the next rung r_{i+1}\nif len(self.observed_error_at_rungs[ri]) >= ni:\nkiplus1 = ki - 1\nniplus1 = int(self.prefact * self.eta ** kiplus1)\nbest_performing_configurations = self.get_top_n_configurations(\nrung_level=ri, n=niplus1\n)\nriplus1 = self.rung_levels[self.K - kiplus1]\n# r_{i+1}\n# Queue may not be empty: insert new entries at the beginning\nself.queue = [\ndict(config, max_epochs=riplus1)\nfor config in best_performing_configurations\n] + self.queue\nself.observed_error_at_rungs[ri] = []\n# Reset\nConﬁgurations are sorted based on their observed performance on the current rung.\n@d2l.add_to_class(SuccessiveHalvingScheduler)\n#@save\ndef get_top_n_configurations(self, rung_level, n):\nrung = self.observed_error_at_rungs[rung_level]\nif not rung:\nreturn []\nsorted_rung = sorted(rung, key=lambda x: x[1])\nreturn [x[0] for x in sorted_rung[:n]]\nLet us see how successive halving is doing on our neural network example. We will use\nrmin = 2, η = 2, rmax = 10, so that rung levels are 2, 4, 8, 10.\nmin_number_of_epochs = 2\nmax_number_of_epochs = 10\neta = 2\nnum_gpus=1\nconfig_space = {\n\"learning_rate\": stats.loguniform(1e-2, 1),\n\"batch_size\": stats.randint(32, 256),\n}\ninitial_config = {\n\"learning_rate\": 0.1,\n(continues on next page)\n\n889\nMulti-Fidelity Hyperparameter Optimization\n(continued from previous page)\n\"batch_size\": 128,\n}\nWe just replace the scheduler with our new SuccessiveHalvingScheduler.\nsearcher = d2l.RandomSearcher(config_space, initial_config=initial_config)\nscheduler = SuccessiveHalvingScheduler(\nsearcher=searcher,\neta=eta,\nr_min=min_number_of_epochs,\nr_max=max_number_of_epochs,\n)\ntuner = d2l.HPOTuner(\nscheduler=scheduler,\nobjective=d2l.hpo_objective_lenet,\n)\ntuner.run(number_of_trials=30)\nerror = 0.2517373561859131, runtime = 56.05348062515259\nWe can visualize the learning curves of all conﬁgurations that we evaluated. Most of the\nconﬁgurations are stopped early and only the better performing conﬁgurations survive until\n\n890\nHyperparameter Optimization\nrmax. Compare this to vanilla random search, which would allocate rmax to every conﬁgura-\ntion.\nfor rung_index, rung in scheduler.all_observed_error_at_rungs.items():\nerrors = [xi[1] for xi in rung]\nd2l.plt.scatter([rung_index] * len(errors), errors)\nd2l.plt.xlim(min_number_of_epochs - 0.5, max_number_of_epochs + 0.5)\nd2l.plt.xticks(\nnp.arange(min_number_of_epochs, max_number_of_epochs + 1),\n(continues on next page)\n\n891\nMulti-Fidelity Hyperparameter Optimization\n(continued from previous page)\nnp.arange(min_number_of_epochs, max_number_of_epochs + 1)\n)\nd2l.plt.ylabel(\"validation error\")\nd2l.plt.xlabel(\"epochs\")\nText(0.5, 0, 'epochs')\n\n892\nHyperparameter Optimization\nFinally, note some slight complexity in our implementation of SuccessiveHalvingSched-\nuler. Say that a worker is free to run a job, and suggest is called when the current rung has\nalmost been completely ﬁlled, but another worker is still busy with an evaluation. Since we\nlack the metric value from this worker, we cannot determine the top 1/η fraction to open up\nthe next rung. On the other hand, we want to assign a job to our free worker, so it does not\nremain idle. Our solution is to start a new round of successive halving and assign our worker\nto the ﬁrst trial there. However, once a rung is completed in update, we make sure to insert\nnew conﬁgurations at the beginning of the queue, so they take precedence over conﬁgurations\nfrom the next round.\n\n893\nMulti-Fidelity Hyperparameter Optimization\n19.4.2 Summary\nIn this section, we introduced the concept of multi-ﬁdelity hyperparameter optimization,\nwhere we assume to have access to cheap-to-evaluate approximations of the objective func-\ntion, such as validation error after a certain number of epochs of training as proxy to val-\nidation error after the full number of epochs. Multi-ﬁdelity hyperparameter optimization\nallows to reduce the overall computation of the HPO instead of just reducing the wall-clock\ntime.\n\n894\nHyperparameter Optimization\n272\nWe implemented and evaluated successive halving, a simple yet eﬃcient multi-ﬁdelity HPO\nalgorithm.\nDiscussions272.\n\n895\nAsynchronous Successive Halving\n19.5 Asynchronous Successive Halving\nAs we have seen in Section 19.3, we can accelerate HPO by distributing the evaluation of\nhyperparameter conﬁgurations across either multiple instances or multiples CPUs / GPUs\non a single instance. However, compared to random search, it is not straightforward to run\nsuccessive halving (SH) asynchronously in a distributed setting. Before we can decide which\n\n896\nHyperparameter Optimization\nconﬁguration to run next, we ﬁrst have to collect all observations at the current rung level.\nThis requires to synchronize workers at each rung level. For example, for the lowest rung\nlevel rmin, we ﬁrst have to evaluate all N = ηK conﬁgurations, before we can promote the 1\nη\nof them to the next rung level.\nIn any distributed system, synchronization typically implies idle time for workers. First, we\noften observe high variations in training time across hyperparameter conﬁgurations. For ex-\nample, assuming the number of ﬁlters per layer is a hyperparameter, then networks with less\nﬁlters ﬁnish training faster than networks with more ﬁlters, which implies idle worker time\n\n897\nAsynchronous Successive Halving\ndue to stragglers. Moreover, the number of slots in a rung level is not always a multiple of the\nnumber of workers, in which case some workers may even sit idle for a full batch.\nFigure Fig. 19.5.1 shows the scheduling of synchronous SH with η = 2 for four diﬀerent trials\nwith two workers. We start with evaluating Trial-0 and Trial-1 for one epoch and immediately\ncontinue with the next two trials once they are ﬁnished. We ﬁrst have to wait until Trial-2\nﬁnishes, which takes substantially more time than the other trials, before we can promote\nthe best two trials, i.e., Trial-0 and Trial-3 to the next rung level. This causes idle time for\nWorker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer than Trial-0, which\n\n898\nHyperparameter Optimization\nleads to an additional ideling time of Worker-0. Once, we reach Rung-2, only the best trial,\nTrial-0, remains which occupies only one worker. To avoid that Worker-1 idles during that\ntime, most implementaitons of SH continue already with the next round, and start evaluating\nnew trials (e.g Trial-4) on the ﬁrst rung.\nAsynchronous successive halving (ASHA) (Li et al., 2018) adapts SH to the asynchronous\nparallel scenario. The main idea of ASHA is to promote conﬁgurations to the next rung level\nas soon as we collected at least η observations on the current rung level. This decision rule\nmay lead to suboptimal promotions: conﬁgurations can be promoted to the next rung level,\n\n899\nAsynchronous Successive Halving\nwhich in hindsight do not compare favourably against most others at the same rung level. On\nthe other hand, we get rid of all synchronization points this way. In practice, such suboptimal\ninitial promotions have only a modest impact on performance, not only because the ranking\nof hyperparameter conﬁgurations is often fairly consistent across rung levels, but also because\nrungs grow over time and reﬂect the distribution of metric values at this level better and better.\nIf a worker is free, but no conﬁguration can be promoted, we start a new conﬁguration with\nr = rmin, i.e the ﬁrst rung level.\nFig. 19.5.2 shows the scheduling of the same conﬁgurations for ASHA. Once Trial-1 ﬁnishes,\nTrial-0\nTrial-1\nTrial-2\nTrial-3\nTrial-0\nTrial-3\nSynchronous Successive Halving\nWorker-0\nWorker-1\nRung 0\nTrial-0\nRung 1\nRung 2\nTrial-4\nt\nFig. 19.5.1\nSynchronous successive halving with two workers.\n\n900\nHyperparameter Optimization\nwe collect the results of two trials (i.e Trial-0 and Trial-1) and immediately promote the better\nof them (Trial-0) to the next rung level. After Trial-0 ﬁnishes on rung 1, there are too few\ntrials there in order to support a further promotion. Hence, we continue with rung 0 and\nevaluate Trial-3. Once Trial-3 ﬁnishes, Trial-2 is still pending. At this point we have 3 trials\nevaluated on rung 0 and one trial evaluated already on rung 1. Since Trial-3 performs worse\nthan Trial-0 at rung 0, and η = 2, we cannot promote any new trial yet, and Worker-1 starts\nTrial-4 from scratch instead. However, once Trial-2 ﬁnishes and scores worse than Trial-3,\nthe latter is promoted towards rung 1. Afterwards, we collected 2 evaluations on rung 1, which\nmeans we can now promote Trial-0 towards rung 2. At the same time, Worker-1 continues\nwith evaluating new trials (i.e., Trial-5) on rung 0.\nTrial-0\nTrial-1\nTrial-2\nTrial-0\nTrial-3\nAsynchronous Successive Halving\nWorker-0\nWorker-1\nTrial-3\nTrial-4\nTrial-0\nPromotion to Rung 1\nPromotion to Rung 1\nStart new trial on Rung 0\nPromotion to Rung 2\nTrial-5\nStart new trial on Rung 0\nt\nFig. 19.5.2\nAsynchronous successive halving (ASHA) with two workers.\nimport logging\nfrom d2l import torch as d2l\nlogging.basicConfig(level=logging.INFO)\nimport matplotlib.pyplot as plt\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune.backend.python_backend import PythonBackend\nfrom syne_tune.config_space import loguniform, randint\nfrom syne_tune.experiments import load_experiment\nfrom syne_tune.optimizer.baselines import ASHA\nINFO:root:SageMakerBackend is not imported since dependencies are missing. You␣\n,→can install them with\npip install 'syne-tune[extra]'\nAWS dependencies are not imported since dependencies are missing. You can␣\n,→install them with\npip install 'syne-tune[aws]'\nor (for everything)\npip install 'syne-tune[extra]'\nAWS dependencies are not imported since dependencies are missing. You can␣\n,→install them with\npip install 'syne-tune[aws]'\nor (for everything)\npip install 'syne-tune[extra]'\nINFO:root:Ray Tune schedulers and searchers are not imported since␣\n,→dependencies are missing. You can install them with\n(continues on next page)\n\n901\nAsynchronous Successive Halving\n(continued from previous page)\npip install 'syne-tune[raytune]'\nor (for everything)\npip install 'syne-tune[extra]'\n19.5.1 Objective Function\nWe will use Syne Tune with the same objective function as in Section 19.3.\ndef hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\nfrom d2l import torch as d2l\nmodel = d2l.LeNet(lr=learning_rate, num_classes=10)\ntrainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)\ndata = d2l.FashionMNIST(batch_size=batch_size)\nmodel.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\nreport = Reporter()\nfor epoch in range(1, max_epochs + 1):\nif epoch == 1:\n# Initialize the state of Trainer\ntrainer.fit(model=model, data=data)\nelse:\ntrainer.fit_epoch()\nvalidation_error = trainer.validation_error().cpu().detach().numpy()\nreport(epoch=epoch, validation_error=float(validation_error))\nWe will also use the same conﬁguration space as before:\nmin_number_of_epochs = 2\nmax_number_of_epochs = 10\neta = 2\nconfig_space = {\n\"learning_rate\": loguniform(1e-2, 1),\n\"batch_size\": randint(32, 256),\n\"max_epochs\": max_number_of_epochs,\n}\ninitial_config = {\n\"learning_rate\": 0.1,\n\"batch_size\": 128,\n}\n19.5.2 Asynchronous Scheduler\nFirst, we deﬁne the number of workers that evaluate trials concurrently. We also need to\nspecify how long we want to run random search, by deﬁning an upper limit on the total wall-\nclock time.\n\n902\nHyperparameter Optimization\nn_workers = 2\n# Needs to be <= the number of available GPUs\nmax_wallclock_time = 12 * 60\n# 12 minutes\nThe code for running ASHA is a simple variation of what we did for asynchronous random\nsearch.\nmode = \"min\"\nmetric = \"validation_error\"\nresource_attr = \"epoch\"\nscheduler = ASHA(\nconfig_space,\nmetric=metric,\nmode=mode,\npoints_to_evaluate=[initial_config],\nmax_resource_attr=\"max_epochs\",\nresource_attr=resource_attr,\ngrace_period=min_number_of_epochs,\nreduction_factor=eta,\n)\nINFO:syne_tune.optimizer.schedulers.fifo:max_resource_level = 10, as inferred␣\n,→from config_space\nINFO:syne_tune.optimizer.schedulers.fifo:Master random_seed = 3239342404\nHere, metric and resource_attr specify the key names used with the report callback,\nand max_resource_attr denotes which input to the objective function corresponds to rmax.\nMoreover, grace_period provides rmin, and reduction_factor is η. We can run Syne\nTune as before (this will take about 12 minutes):\ntrial_backend = PythonBackend(\ntune_function=hpo_objective_lenet_synetune,\nconfig_space=config_space,\n)\nstop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)\ntuner = Tuner(\ntrial_backend=trial_backend,\nscheduler=scheduler,\nstop_criterion=stop_criterion,\nn_workers=n_workers,\nprint_update_interval=int(max_wallclock_time * 0.6),\n)\ntuner.run()\nINFO:syne_tune.tuner:results of trials will be saved on /home/ubuntu/syne-tune/\n,→python-entrypoint-2023-08-18-23-02-26-487\nINFO:root:Detected 8 GPUs\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n(continues on next page)\n\n903\nAsynchronous Successive Halving\n(continued from previous page)\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.1 --batch_\n,→size 128 --max_epochs 10 --tune_function_root /home/ubuntu/syne-tune/python-\n,→entrypoint-2023-08-18-23-02-26-487/tune_function --tune_function_hash␣\n,→836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /home/ubuntu/syne-tune/\n,→python-entrypoint-2023-08-18-23-02-26-487/0/checkpoints\nINFO:syne_tune.tuner:(trial 0) - scheduled config {'learning_rate': 0.1,\n,→'batch_size': 128, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→13853005130955554 --batch_size 121 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/1/checkpoints\nINFO:syne_tune.tuner:(trial 1) - scheduled config {'learning_rate': 0.\n,→13853005130955554, 'batch_size': 121, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→04888009860727095 --batch_size 226 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/2/checkpoints\nINFO:syne_tune.tuner:(trial 2) - scheduled config {'learning_rate': 0.\n,→04888009860727095, 'batch_size': 226, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→9214410391140626 --batch_size 246 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/3/checkpoints\nINFO:syne_tune.tuner:(trial 3) - scheduled config {'learning_rate': 0.\n,→9214410391140626, 'batch_size': 246, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 1 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→018248573629167826 --batch_size 139 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/4/checkpoints\nINFO:syne_tune.tuner:(trial 4) - scheduled config {'learning_rate': 0.\n,→018248573629167826, 'batch_size': 139, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→02520991452721714 --batch_size 77 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/5/checkpoints\nINFO:syne_tune.tuner:(trial 5) - scheduled config {'learning_rate': 0.\n(continues on next page)\n\n904\nHyperparameter Optimization\n(continued from previous page)\n,→02520991452721714, 'batch_size': 77, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→023767502921144167 --batch_size 235 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/6/checkpoints\nINFO:syne_tune.tuner:(trial 6) - scheduled config {'learning_rate': 0.\n,→023767502921144167, 'batch_size': 235, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 3 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→5128649019485186 --batch_size 133 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/7/checkpoints\nINFO:syne_tune.tuner:(trial 7) - scheduled config {'learning_rate': 0.\n,→5128649019485186, 'batch_size': 133, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→0820355191393112 --batch_size 66 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/8/checkpoints\nINFO:syne_tune.tuner:(trial 8) - scheduled config {'learning_rate': 0.\n,→0820355191393112, 'batch_size': 66, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→015979339095164306 --batch_size 236 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/9/checkpoints\nINFO:syne_tune.tuner:(trial 9) - scheduled config {'learning_rate': 0.\n,→015979339095164306, 'batch_size': 236, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 7 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→011287864128690532 --batch_size 128 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/10/checkpoints\nINFO:syne_tune.tuner:(trial 10) - scheduled config {'learning_rate': 0.\n,→011287864128690532, 'batch_size': 128, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→022375895483915522 --batch_size 80 --max_epochs 10 --tune_function_root /\n(continues on next page)\n\n905\nAsynchronous Successive Halving\n(continued from previous page)\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/11/checkpoints\nINFO:syne_tune.tuner:(trial 11) - scheduled config {'learning_rate': 0.\n,→022375895483915522, 'batch_size': 80, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→10869856278853329 --batch_size 246 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/12/checkpoints\nINFO:syne_tune.tuner:(trial 12) - scheduled config {'learning_rate': 0.\n,→10869856278853329, 'batch_size': 246, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→6321793222955815 --batch_size 142 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/13/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 13) - scheduled config {'learning_rate': 0.\n,→6321793222955815, 'batch_size': 142, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→2536414155118574 --batch_size 249 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/14/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 14) - scheduled config {'learning_rate': 0.\n,→2536414155118574, 'batch_size': 249, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→017930844776160398 --batch_size 152 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/15/checkpoints\nINFO:syne_tune.tuner:(trial 15) - scheduled config {'learning_rate': 0.\n,→017930844776160398, 'batch_size': 152, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→011532901021050378 --batch_size 207 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/16/checkpoints\nINFO:syne_tune.tuner:(trial 16) - scheduled config {'learning_rate': 0.\n(continues on next page)\n\n906\nHyperparameter Optimization\n(continued from previous page)\n,→011532901021050378, 'batch_size': 207, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→042449174869797324 --batch_size 210 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/17/checkpoints\nINFO:syne_tune.tuner:(trial 17) - scheduled config {'learning_rate': 0.\n,→042449174869797324, 'batch_size': 210, 'max_epochs': 10}\nINFO:syne_tune.tuner:tuning status (last metric is reported)\ntrial_id\nstatus\niter\nlearning_rate\nbatch_size\nmax_epochs\nepoch ␣\n,→validation_error\nworker-time\n0\nStopped\n4\n0.100000\n128\n10\n4.0\n␣\n,→\n0.464695\n37.212292\n1\nCompleted\n10\n0.138530\n121\n10\n10.0\n␣\n,→\n0.253368\n91.954761\n2\nStopped\n2\n0.048880\n226\n10\n2.0\n␣\n,→\n0.901075\n18.409374\n3\nCompleted\n10\n0.921441\n246\n10\n10.0\n␣\n,→\n0.191130\n91.572764\n4\nStopped\n2\n0.018249\n139\n10\n2.0\n␣\n,→\n0.899994\n19.555009\n5\nStopped\n2\n0.025210\n77\n10\n2.0\n␣\n,→\n0.899966\n26.743767\n6\nStopped\n4\n0.023768\n235\n10\n4.0\n␣\n,→\n0.899760\n38.330475\n7\nCompleted\n10\n0.512865\n133\n10\n10.0\n␣\n,→\n0.231725\n95.423449\n8\nStopped\n4\n0.082036\n66\n10\n4.0\n␣\n,→\n0.376454\n43.781020\n9\nStopped\n2\n0.015979\n236\n10\n2.0\n␣\n,→\n0.899967\n18.806864\n10\nStopped\n4\n0.011288\n128\n10\n4.0\n␣\n,→\n0.899031\n35.672628\n11\nStopped\n2\n0.022376\n80\n10\n2.0\n␣\n,→\n0.900000\n20.890393\n12\nStopped\n2\n0.108699\n246\n10\n2.0\n␣\n,→\n0.900107\n18.380533\n13\nStopped\n10\n0.632179\n142\n10\n10.0\n␣\n,→\n0.180695\n90.497290\n14\nStopped\n4\n0.253641\n249\n10\n4.0\n␣\n,→\n0.425554\n34.445888\n15\nStopped\n2\n0.017931\n152\n10\n2.0\n␣\n,→\n0.899920\n20.064630\n16 InProgress\n1\n0.011533\n207\n10\n1.0\n␣\n,→\n0.899648\n11.990970\n17 InProgress\n0\n0.042449\n210\n10\n-\n␣\n,→\n-\n-\n2 trials running, 16 finished (3 until the end), 437.06s wallclock-time\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n(continues on next page)\n\n907\nAsynchronous Successive Halving\n(continued from previous page)\n,→5484553814219248 --batch_size 170 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/18/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 18) - scheduled config {'learning_rate': 0.\n,→5484553814219248, 'batch_size': 170, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→011944500922210607 --batch_size 235 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/19/checkpoints\nINFO:syne_tune.tuner:(trial 19) - scheduled config {'learning_rate': 0.\n,→011944500922210607, 'batch_size': 235, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→08643278498726796 --batch_size 251 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/20/checkpoints\nINFO:syne_tune.tuner:(trial 20) - scheduled config {'learning_rate': 0.\n,→08643278498726796, 'batch_size': 251, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→7678647823352986 --batch_size 112 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/21/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 21) - scheduled config {'learning_rate': 0.\n,→7678647823352986, 'batch_size': 112, 'max_epochs': 10}\nINFO:syne_tune.tuner:Trial trial_id 18 completed.\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→25750421499886467 --batch_size 143 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/22/checkpoints\nINFO:syne_tune.tuner:(trial 22) - scheduled config {'learning_rate': 0.\n,→25750421499886467, 'batch_size': 143, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→04460413473978858 --batch_size 229 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n(continues on next page)\n\n908\nHyperparameter Optimization\n(continued from previous page)\n,→487/23/checkpoints\nINFO:syne_tune.tuner:(trial 23) - scheduled config {'learning_rate': 0.\n,→04460413473978858, 'batch_size': 229, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→03511449170254089 --batch_size 131 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/24/checkpoints\nINFO:syne_tune.tuner:(trial 24) - scheduled config {'learning_rate': 0.\n,→03511449170254089, 'batch_size': 131, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→013061175753480661 --batch_size 38 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/25/checkpoints\nINFO:syne_tune.tuner:(trial 25) - scheduled config {'learning_rate': 0.\n,→013061175753480661, 'batch_size': 38, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→028889846591178023 --batch_size 78 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/26/checkpoints\nINFO:syne_tune.tuner:(trial 26) - scheduled config {'learning_rate': 0.\n,→028889846591178023, 'batch_size': 78, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→06457802087000711 --batch_size 253 --max_epochs 10 --tune_function_root /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_\n,→function --tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_\n,→checkpoint_dir /home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-\n,→487/27/checkpoints\nINFO:syne_tune.tuner:(trial 27) - scheduled config {'learning_rate': 0.\n,→06457802087000711, 'batch_size': 253, 'max_epochs': 10}\nINFO:root:running subprocess with command: /home/ubuntu/miniconda3/envs/np3/\n,→bin/python /home/ubuntu/miniconda3/envs/np3/lib/python3.9/site-packages/syne_\n,→tune/backend/python_backend/python_entrypoint.py --learning_rate 0.\n,→7405331110191096 --batch_size 40 --max_epochs 10 --tune_function_root /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/tune_function --\n,→tune_function_hash 836d63ee48b563fe3f22bde2d210ad89 --st_checkpoint_dir /\n,→home/ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487/28/\n,→checkpoints\nINFO:syne_tune.tuner:(trial 28) - scheduled config {'learning_rate': 0.\n,→7405331110191096, 'batch_size': 40, 'max_epochs': 10}\nINFO:syne_tune.stopping_criterion:reaching max wallclock time (720), stopping␣\n,→there.\n(continues on next page)\n\n909\nAsynchronous Successive Halving\n(continued from previous page)\nINFO:syne_tune.tuner:Stopping trials that may still be running.\nINFO:syne_tune.tuner:Tuning finished, results of trials can be found on /home/\n,→ubuntu/syne-tune/python-entrypoint-2023-08-18-23-02-26-487\n--------------------\nResource summary (last result is reported):\ntrial_id\nstatus\niter\nlearning_rate\nbatch_size\nmax_epochs\nepoch ␣\n,→validation_error\nworker-time\n0\nStopped\n4\n0.100000\n128\n10\n4.0\n␣\n,→\n0.464695\n37.212292\n1\nCompleted\n10\n0.138530\n121\n10\n10.0\n␣\n,→\n0.253368\n91.954761\n2\nStopped\n2\n0.048880\n226\n10\n2.0\n␣\n,→\n0.901075\n18.409374\n3\nCompleted\n10\n0.921441\n246\n10\n10.0\n␣\n,→\n0.191130\n91.572764\n4\nStopped\n2\n0.018249\n139\n10\n2.0\n␣\n,→\n0.899994\n19.555009\n5\nStopped\n2\n0.025210\n77\n10\n2.0\n␣\n,→\n0.899966\n26.743767\n6\nStopped\n4\n0.023768\n235\n10\n4.0\n␣\n,→\n0.899760\n38.330475\n7\nCompleted\n10\n0.512865\n133\n10\n10.0\n␣\n,→\n0.231725\n95.423449\n8\nStopped\n4\n0.082036\n66\n10\n4.0\n␣\n,→\n0.376454\n43.781020\n9\nStopped\n2\n0.015979\n236\n10\n2.0\n␣\n,→\n0.899967\n18.806864\n10\nStopped\n4\n0.011288\n128\n10\n4.0\n␣\n,→\n0.899031\n35.672628\n11\nStopped\n2\n0.022376\n80\n10\n2.0\n␣\n,→\n0.900000\n20.890393\n12\nStopped\n2\n0.108699\n246\n10\n2.0\n␣\n,→\n0.900107\n18.380533\n13\nStopped\n10\n0.632179\n142\n10\n10.0\n␣\n,→\n0.180695\n90.497290\n14\nStopped\n4\n0.253641\n249\n10\n4.0\n␣\n,→\n0.425554\n34.445888\n15\nStopped\n2\n0.017931\n152\n10\n2.0\n␣\n,→\n0.899920\n20.064630\n16\nStopped\n2\n0.011533\n207\n10\n2.0\n␣\n,→\n0.900529\n21.072846\n17\nStopped\n4\n0.042449\n210\n10\n4.0\n␣\n,→\n0.899878\n34.671282\n18\nCompleted\n10\n0.548455\n170\n10\n10.0\n␣\n,→\n0.178600\n92.981846\n19\nStopped\n4\n0.011945\n235\n10\n4.0\n␣\n,→\n0.870940\n35.437183\n20\nStopped\n2\n0.086433\n251\n10\n2.0\n␣\n,→\n0.900077\n18.594752\n21\nStopped\n10\n0.767865\n112\n10\n10.0\n␣\n,→\n0.152976\n95.257922\n22\nStopped\n8\n0.257504\n143\n10\n8.0\n␣\n,→\n0.235383\n75.945771\n23\nStopped\n2\n0.044604\n229\n10\n2.0\n␣\n,→\n0.899916\n19.311652\n(continues on next page)\n\n910\nHyperparameter Optimization\n(continued from previous page)\n24\nStopped\n2\n0.035114\n131\n10\n2.0\n␣\n,→\n0.900274\n21.667166\n25\nStopped\n4\n0.013061\n38\n10\n4.0\n␣\n,→\n0.900319\n58.163195\n26\nStopped\n4\n0.028890\n78\n10\n4.0\n␣\n,→\n0.900231\n41.910056\n27 InProgress\n0\n0.064578\n253\n10\n-\n␣\n,→\n-\n-\n28 InProgress\n0\n0.740533\n40\n10\n-\n␣\n,→\n-\n-\n2 trials running, 27 finished (4 until the end), 723.76s wallclock-time\nvalidation_error: best 0.15019840002059937 for trial-id 21\n--------------------\nNote that we are running a variant of ASHA where underperforming trials are stopped early.\nThis is diﬀerent to our implementation in Section 19.4.1, where each training job is started\nwith a ﬁxed max_epochs. In the latter case, a well-performing trial which reaches the full 10\nepochs, ﬁrst needs to train 1, then 2, then 4, then 8 epochs, each time starting from scratch.\nThis type of pause-and-resume scheduling can be implemented eﬃciently by checkpoint-\ning the training state after each epoch, but we avoid this extra complexity here. After the\nexperiment has ﬁnished, we can retrieve and plot results.\nd2l.set_figsize()\ne = load_experiment(tuner.name)\ne.plot()\nWARNING:matplotlib.legend:No artists with labels found to put in legend.\nNote␣\n,→that artists whose label start with an underscore are ignored when legend()␣\n,→is called with no argument.\n19.5.3 Visualize the Optimization Process\n\n911\nAsynchronous Successive Halving\n273\nOnce more, we visualize the learning curves of every trial (each color in the plot represents\na trial). Compare this to asynchronous random search in Section 19.3. As we have seen for\nsuccessive halving in Section 19.4, most of the trials are stopped at 1 or 2 epochs (rmin or\nη∗rmin). However, trials do not stop at the same point, because they require diﬀerent amount\nof time per epoch. If we ran standard successive halving instead of ASHA, we would need to\nsynchronize our workers, before we can promote conﬁgurations to the next rung level.\nd2l.set_figsize([6, 2.5])\nresults = e.results\nfor trial_id in results.trial_id.unique():\ndf = results[results[\"trial_id\"] == trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time\"],\ndf[\"validation_error\"],\nmarker=\"o\"\n)\nd2l.plt.xlabel(\"wall-clock time\")\nd2l.plt.ylabel(\"objective function\")\nText(0, 0.5, 'objective function')\n19.5.4 Summary\nCompared to random search, successive halving is not quite as trivial to run in an asyn-\nchronous distributed setting. To avoid synchronisation points, we promote conﬁgurations as\nquickly as possible to the next rung level, even if this means promoting some wrong ones. In\npractice, this usually does not hurt much, and the gains of asynchronous versus synchronous\nscheduling are usually much higher than the loss of the suboptimal decision making.\nDiscussions273.\n\n20\nGenerative Adversarial Networks\n20.1 Generative Adversarial Networks\nThroughout most of this book, we have talked about how to make predictions. In some form\nor another, we used deep neural networks to learn mappings from data examples to labels.\nThis kind of learning is called discriminative learning, as in, we’d like to be able to discrimi-\nnate between photos of cats and photos of dogs. Classiﬁers and regressors are both examples\nof discriminative learning. And neural networks trained by backpropagation have upended\neverything we thought we knew about discriminative learning on large complicated datasets.\nClassiﬁcation accuracies on high-res images have gone from useless to human-level (with\nsome caveats) in just 5-6 years. We will spare you another spiel about all the other discrimi-\nnative tasks where deep neural networks do astoundingly well.\nBut there is more to machine learning than just solving discriminative tasks. For example,\ngiven a large dataset, without any labels, we might want to learn a model that concisely cap-\ntures the characteristics of this data. Given such a model, we could sample synthetic data\nexamples that resemble the distribution of the training data. For example, given a large cor-\npus of photographs of faces, we might want to be able to generate a new photorealistic image\nthat looks like it might plausibly have come from the same dataset. This kind of learning is\ncalled generative modeling.\nUntil recently, we had no method that could synthesize novel photorealistic images. But the\nsuccess of deep neural networks for discriminative learning opened up new possibilities. One\nbig trend over the last three years has been the application of discriminative deep nets to\novercome challenges in problems that we do not generally think of as supervised learning\nproblems. The recurrent neural network language models are one example of using a dis-\ncriminative network (trained to predict the next character) that once trained can act as a\ngenerative model.\nIn 2014, a breakthrough paper introduced Generative adversarial networks (GANs) (Good-\nfellow et al., 2014), a clever new way to leverage the power of discriminative models to get\ngood generative models. At their heart, GANs rely on the idea that a data generator is good if\nwe cannot tell fake data apart from real data. In statistics, this is called a two-sample test - a\ntest to answer the question whether datasets X = {x1, . . ., xn} and X′ = {x′\n1, . . ., x′\nn} were\ndrawn from the same distribution. The main diﬀerence between most statistics papers and\n912\n\n913\nGenerative Adversarial Networks\n274\nGANs is that the latter use this idea in a constructive way. In other words, rather than just\ntraining a model to say “hey, these two datasets do not look like they came from the same dis-\ntribution”, they use the two-sample test274 to provide training signals to a generative model.\nThis allows us to improve the data generator until it generates something that resembles the\nreal data. At the very least, it needs to fool the classiﬁer even if our classiﬁer is a state of the\nart deep neural network.\nt\nFig. 20.1.1\nGenerative Adversarial Networks\nThe GAN architecture is illustrated in Fig. 20.1.1. As you can see, there are two pieces\nin GAN architecture - ﬁrst oﬀ, we need a device (say, a deep network but it really could\nbe anything, such as a game rendering engine) that might potentially be able to generate\ndata that looks just like the real thing. If we are dealing with images, this needs to generate\nimages. If we are dealing with speech, it needs to generate audio sequences, and so on. We call\nthis the generator network. The second component is the discriminator network. It attempts\nto distinguish fake and real data from each other. Both networks are in competition with\neach other. The generator network attempts to fool the discriminator network. At that point,\nthe discriminator network adapts to the new fake data. This information, in turn is used to\nimprove the generator network, and so on.\nThe discriminator is a binary classiﬁer to distinguish if the input x is real (from real data) or\nfake (from the generator). Typically, the discriminator outputs a scalar prediction o ∈R for\ninput x, such as using a fully connected layer with hidden size 1, and then applies sigmoid\nfunction to obtain the predicted probability D(x) = 1/(1+ e−o). Assume the label y for the\ntrue data is 1 and 0 for the fake data. We train the discriminator to minimize the cross-entropy\nloss, i.e.,\nmin\nD {−y log D(x) −(1 −y) log(1 −D(x))},\n(20.1.1)\nFor the generator, it ﬁrst draws some parameter z ∈Rd from a source of randomness, e.g.,\na normal distribution z ∼N(0, 1). We often call z as the latent variable. It then applies\na function to generate x′ = G(z). The goal of the generator is to fool the discriminator\nto classify x′ = G(z) as true data, i.e., we want D(G(z)) ≈1. In other words, for a given\ndiscriminator D, we update the parameters of the generator G to maximize the cross-entropy\nloss when y = 0, i.e.,\nmax\nG {−(1 −y) log(1 −D(G(z)))} = max\nG {−log(1 −D(G(z)))}.\n(20.1.2)\n\n914\nGenerative Adversarial Networks\nIf the generator does a perfect job, then D(x′) ≈1, so the above loss is near 0, which results\nin the gradients that are too small to make good progress for the discriminator. So commonly,\nwe minimize the following loss:\nmin\nG {−y log(D(G(z)))} = min\nG {−log(D(G(z)))},\n(20.1.3)\nwhich is just feeding x′ = G(z) into the discriminator but giving label y = 1.\nTo sum up, D and G are playing a “minimax” game with the comprehensive objective func-\ntion:\nmin\nD max\nG {−Ex∼Data log D(x) −Ez∼Noise log(1 −D(G(z)))}.\n(20.1.4)\nMany of the GANs applications are in the context of images. As a demonstration purpose, we\nare going to content ourselves with ﬁtting a much simpler distribution ﬁrst. We will illustrate\nwhat happens if we use GANs to build the world’s most ineﬃcient estimator of parameters\nfor a Gaussian. Let’s get started.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n20.1.1 Generate Some “Real” Data\nSince this is going to be the world’s lamest example, we simply generate data drawn from a\nGaussian.\nX = torch.normal(0.0, 1, (1000, 2))\nA = torch.tensor([[1, 2], [-0.1, 0.5]])\nb = torch.tensor([1, 2])\ndata = torch.matmul(X, A) + b\nLet’s see what we got. This should be a Gaussian shifted in some rather arbitrary way with\nmean b and covariance matrix AT A.\nd2l.set_figsize()\nd2l.plt.scatter(data[:100, 0].detach().numpy(), data[:100, 1].detach().\n,→numpy());\nprint(f'The covariance matrix is\\n{torch.matmul(A.T, A)}')\nThe covariance matrix is\ntensor([[1.0100, 1.9500],\n[1.9500, 4.2500]])\nbatch_size = 8\ndata_iter = d2l.load_array((data,), batch_size)\n\n915\nGenerative Adversarial Networks\n20.1.2 Generator\nOur generator network will be the simplest network possible - a single layer linear model.\nThis is since we will be driving that linear network with a Gaussian data generator. Hence, it\nliterally only needs to learn the parameters to fake things perfectly.\nnet_G = nn.Sequential(nn.Linear(2, 2))\n20.1.3 Discriminator\nFor the discriminator we will be a bit more discriminating: we will use an MLP with 3 layers\nto make things a bit more interesting.\nnet_D = nn.Sequential(\nnn.Linear(2, 5), nn.Tanh(),\nnn.Linear(5, 3), nn.Tanh(),\nnn.Linear(3, 1))\n20.1.4 Training\nFirst we deﬁne a function to update the discriminator.\n#@save\ndef update_D(X, Z, net_D, net_G, loss, trainer_D):\n\"\"\"Update discriminator.\"\"\"\nbatch_size = X.shape[0]\nones = torch.ones((batch_size,), device=X.device)\nzeros = torch.zeros((batch_size,), device=X.device)\ntrainer_D.zero_grad()\nreal_Y = net_D(X)\nfake_X = net_G(Z)\n# Do not need to compute gradient for `net_G`, detach it from\n# computing gradients.\n(continues on next page)\n\n916\nGenerative Adversarial Networks\n(continued from previous page)\nfake_Y = net_D(fake_X.detach())\nloss_D = (loss(real_Y, ones.reshape(real_Y.shape)) +\nloss(fake_Y, zeros.reshape(fake_Y.shape))) / 2\nloss_D.backward()\ntrainer_D.step()\nreturn loss_D\nThe generator is updated similarly. Here we reuse the cross-entropy loss but change the label\nof the fake data from 0 to 1.\n#@save\ndef update_G(Z, net_D, net_G, loss, trainer_G):\n\"\"\"Update generator.\"\"\"\nbatch_size = Z.shape[0]\nones = torch.ones((batch_size,), device=Z.device)\ntrainer_G.zero_grad()\n# We could reuse `fake_X` from `update_D` to save computation\nfake_X = net_G(Z)\n# Recomputing `fake_Y` is needed since `net_D` is changed\nfake_Y = net_D(fake_X)\nloss_G = loss(fake_Y, ones.reshape(fake_Y.shape))\nloss_G.backward()\ntrainer_G.step()\nreturn loss_G\nBoth the discriminator and the generator performs a binary logistic regression with the cross-\nentropy loss. We use Adam to smooth the training process. In each iteration, we ﬁrst up-\ndate the discriminator and then the generator. We visualize both losses and generated exam-\nples.\ndef train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\nloss = nn.BCEWithLogitsLoss(reduction='sum')\nfor w in net_D.parameters():\nnn.init.normal_(w, 0, 0.02)\nfor w in net_G.parameters():\nnn.init.normal_(w, 0, 0.02)\ntrainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)\ntrainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[1, num_epochs], nrows=2, figsize=(5, 5),\nlegend=['discriminator', 'generator'])\nanimator.fig.subplots_adjust(hspace=0.3)\nfor epoch in range(num_epochs):\n# Train one epoch\ntimer = d2l.Timer()\nmetric = d2l.Accumulator(3)\n# loss_D, loss_G, num_examples\nfor (X,) in data_iter:\nbatch_size = X.shape[0]\nZ = torch.normal(0, 1, size=(batch_size, latent_dim))\nmetric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\nupdate_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n(continues on next page)\n\n917\nGenerative Adversarial Networks\n(continued from previous page)\n# Visualize generated examples\nZ = torch.normal(0, 1, size=(100, latent_dim))\nfake_X = net_G(Z).detach().numpy()\nanimator.axes[1].cla()\nanimator.axes[1].scatter(data[:, 0], data[:, 1])\nanimator.axes[1].scatter(fake_X[:, 0], fake_X[:, 1])\nanimator.axes[1].legend(['real', 'generated'])\n# Show the losses\nloss_D, loss_G = metric[0]/metric[2], metric[1]/metric[2]\nanimator.add(epoch + 1, (loss_D, loss_G))\nprint(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\nf'{metric[2] / timer.stop():.1f} examples/sec')\nNow we specify the hyperparameters to ﬁt the Gaussian distribution.\nlr_D, lr_G, latent_dim, num_epochs = 0.05, 0.005, 2, 20\ntrain(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\nlatent_dim, data[:100].detach().numpy())\nloss_D 0.693, loss_G 0.693, 985.2 examples/sec\n20.1.5 Summary\n• Generative adversarial networks (GANs) composes of two deep networks, the generator\nand the discriminator.\n\n918\nGenerative Adversarial Networks\n275\n276\n• The generator generates the image as much closer to the true image as possible to fool the\ndiscriminator, via maximizing the cross-entropy loss, i.e., max log(D(x′)).\n• The discriminator tries to distinguish the generated images from the true images, via min-\nimizing the cross-entropy loss, i.e., min −y log D(x) −(1 −y) log(1 −D(x)).\n20.1.6 Exercises\n• Does an equilibrium exist where the generator wins, i.e. the discriminator ends up unable\nto distinguish the two distributions on ﬁnite samples?\nDiscussions275.\n20.2 Deep Convolutional Generative Adversarial\nNetworks\nIn Section 20.1, we introduced the basic ideas behind how GANs work. We showed that they\ncan draw samples from some simple, easy-to-sample distribution, like a uniform or normal\ndistribution, and transform them into samples that appear to match the distribution of some\ndataset. And while our example of matching a 2D Gaussian distribution got the point across,\nit is not especially exciting.\nIn this section, we will demonstrate how you can use GANs to generate photorealistic images.\nWe will be basing our models on the deep convolutional GANs (DCGAN) introduced in\nRadford et al. (2015). We will borrow the convolutional architecture that have proven so\nsuccessful for discriminative computer vision problems and show how via GANs, they can\nbe leveraged to generate photorealistic images.\nimport warnings\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch as d2l\n20.2.1 The Pokemon Dataset\nThe dataset we will use is a collection of Pokemon sprites obtained from pokemondb 276 .\nFirst download, extract and load this dataset.\n\n919\nDeep Convolutional Generative Adversarial Networks\n#@save\nd2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n'c065c0e2593b8b161a2d7873e42418bf6a21106c')\ndata_dir = d2l.download_extract('pokemon')\npokemon = torchvision.datasets.ImageFolder(data_dir)\nWe resize each image into 64×64. The ToTensor transformation will project the pixel value\ninto [0, 1], while our generator will use the tanh function to obtain outputs in [−1, 1]. There-\nfore we normalize the data with 0.5 mean and 0.5 standard deviation to match the value\nrange.\nbatch_size = 256\ntransformer = torchvision.transforms.Compose([\ntorchvision.transforms.Resize((64, 64)),\ntorchvision.transforms.ToTensor(),\ntorchvision.transforms.Normalize(0.5, 0.5)\n])\npokemon.transform = transformer\ndata_iter = torch.utils.data.DataLoader(\npokemon, batch_size=batch_size,\nshuffle=True, num_workers=d2l.get_dataloader_workers())\nLet’s visualize the ﬁrst 20 images.\nwarnings.filterwarnings('ignore')\nd2l.set_figsize((4, 4))\nfor X, y in data_iter:\nimgs = X[:20,:,:,:].permute(0, 2, 3, 1)/2+0.5\nd2l.show_images(imgs, num_rows=4, num_cols=5)\nbreak\n20.2.2 The Generator\nThe generator needs to map the noise variable z ∈Rd, a length-d vector, to a RGB image\nwith width and height to be 64 × 64 . In Section 14.11 we introduced the fully convolutional\nnetwork that uses transposed convolution layer (refer to Section 14.10) to enlarge input size.\nThe basic block of the generator contains a transposed convolution layer followed by the\nbatch normalization and ReLU activation.\nclass G_block(nn.Module):\ndef __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,\npadding=1, **kwargs):\nsuper(G_block, self).__init__(**kwargs)\nself.conv2d_trans = nn.ConvTranspose2d(in_channels, out_channels,\nkernel_size, strides, padding, bias=False)\nself.batch_norm = nn.BatchNorm2d(out_channels)\nself.activation = nn.ReLU()\n(continues on next page)\n\n920\nGenerative Adversarial Networks\n(continued from previous page)\ndef forward(self, X):\nreturn self.activation(self.batch_norm(self.conv2d_trans(X)))\nIn default, the transposed convolution layer uses a kh = kw = 4 kernel, a sh = sw = 2\nstrides, and a ph = pw = 1 padding. With a input shape of n\n′\nh ×n\n′\nw = 16×16, the generator\nblock will double input’s width and height.\nn\n′\nh × n\n′\nw = [(nhkh −(nh −1)(kh −sh) −2ph] × [(nwkw −(nw −1)(kw −sw) −2pw]\n= [(kh + sh(nh −1) −2ph] × [(kw + sw(nw −1) −2pw]\n= [(4 + 2 × (16 −1) −2 × 1] × [(4 + 2 × (16 −1) −2 × 1]\n= 32 × 32.\n(20.2.1)\nx = torch.zeros((2, 3, 16, 16))\ng_blk = G_block(20)\ng_blk(x).shape\ntorch.Size([2, 20, 32, 32])\nIf changing the transposed convolution layer to a 4×4 kernel, 1×1 strides and zero padding.\n\n921\nDeep Convolutional Generative Adversarial Networks\nWith a input size of 1 × 1, the output will have its width and height increased by 3 respec-\ntively.\nx = torch.zeros((2, 3, 1, 1))\ng_blk = G_block(20, strides=1, padding=0)\ng_blk(x).shape\ntorch.Size([2, 20, 4, 4])\nThe generator consists of four basic blocks that increase input’s both width and height from 1\nto 32. At the same time, it ﬁrst projects the latent variable into 64×8 channels, and then halve\nthe channels each time. At last, a transposed convolution layer is used to generate the output.\nIt further doubles the width and height to match the desired 64 × 64 shape, and reduces the\nchannel size to 3. The tanh activation function is applied to project output values into the\n(−1, 1) range.\nn_G = 64\nnet_G = nn.Sequential(\nG_block(in_channels=100, out_channels=n_G*8,\nstrides=1, padding=0),\n# Output: (64 * 8, 4, 4)\nG_block(in_channels=n_G*8, out_channels=n_G*4), # Output: (64 * 4, 8, 8)\nG_block(in_channels=n_G*4, out_channels=n_G*2), # Output: (64 * 2, 16, 16)\nG_block(in_channels=n_G*2, out_channels=n_G),\n# Output: (64, 32, 32)\nnn.ConvTranspose2d(in_channels=n_G, out_channels=3,\nkernel_size=4, stride=2, padding=1, bias=False),\nnn.Tanh())\n# Output: (3, 64, 64)\nGenerate a 100 dimensional latent variable to verify the generator’s output shape.\nx = torch.zeros((1, 100, 1, 1))\nnet_G(x).shape\ntorch.Size([1, 3, 64, 64])\n20.2.3 Discriminator\nThe discriminator is a normal convolutional network network except that it uses a leaky ReLU\nas its activation function. Given α ∈[0, 1], its deﬁnition is\nleaky ReLU(x) =\n{\nx\nif x > 0\nαx\notherwise\n.\n(20.2.2)\nAs it can be seen, it is normal ReLU if α = 0, and an identity function if α = 1. For\nα ∈(0, 1), leaky ReLU is a nonlinear function that give a non-zero output for a negative\ninput. It aims to ﬁx the “dying ReLU” problem that a neuron might always output a negative\nvalue and therefore cannot make any progress since the gradient of ReLU is 0.\n\n922\nGenerative Adversarial Networks\nalphas = [0, .2, .4, .6, .8, 1]\nx = torch.arange(-2, 1, 0.1)\nY = [nn.LeakyReLU(alpha)(x).detach().numpy() for alpha in alphas]\nd2l.plot(x.detach().numpy(), Y, 'x', 'y', alphas)\nThe basic block of the discriminator is a convolution layer followed by a batch normalization\nlayer and a leaky ReLU activation. The hyperparameters of the convolution layer are similar\nto the transpose convolution layer in the generator block.\nclass D_block(nn.Module):\ndef __init__(self, out_channels, in_channels=3, kernel_size=4, strides=2,\npadding=1, alpha=0.2, **kwargs):\nsuper(D_block, self).__init__(**kwargs)\nself.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size,\nstrides, padding, bias=False)\nself.batch_norm = nn.BatchNorm2d(out_channels)\nself.activation = nn.LeakyReLU(alpha, inplace=True)\ndef forward(self, X):\nreturn self.activation(self.batch_norm(self.conv2d(X)))\nA basic block with default settings will halve the width and height of the inputs, as we demon-\nstrated in Section 7.3. For example, given a input shape nh = nw = 16, with a kernel shape\nkh = kw = 4, a stride shape sh = sw = 2, and a padding shape ph = pw = 1, the output\nshape will be:\nn\n′\nh × n\n′\nw = ⌊(nh −kh + 2ph + sh)/sh⌋× ⌊(nw −kw + 2pw + sw)/sw⌋\n= ⌊(16 −4 + 2 × 1 + 2)/2⌋× ⌊(16 −4 + 2 × 1 + 2)/2⌋\n= 8 × 8.\n(20.2.3)\nx = torch.zeros((2, 3, 16, 16))\nd_blk = D_block(20)\nd_blk(x).shape\n\n923\nDeep Convolutional Generative Adversarial Networks\ntorch.Size([2, 20, 8, 8])\nThe discriminator is a mirror of the generator.\nn_D = 64\nnet_D = nn.Sequential(\nD_block(n_D),\n# Output: (64, 32, 32)\nD_block(in_channels=n_D, out_channels=n_D*2),\n# Output: (64 * 2, 16, 16)\nD_block(in_channels=n_D*2, out_channels=n_D*4),\n# Output: (64 * 4, 8, 8)\nD_block(in_channels=n_D*4, out_channels=n_D*8),\n# Output: (64 * 8, 4, 4)\nnn.Conv2d(in_channels=n_D*8, out_channels=1,\nkernel_size=4, bias=False))\n# Output: (1, 1, 1)\nIt uses a convolution layer with output channel 1 as the last layer to obtain a single prediction\nvalue.\nx = torch.zeros((1, 3, 64, 64))\nnet_D(x).shape\ntorch.Size([1, 1, 1, 1])\n20.2.4 Training\nCompared to the basic GAN in Section 20.1, we use the same learning rate for both generator\nand discriminator since they are similar to each other. In addition, we change β1 in Adam\n(Section 12.10) from 0.9 to 0.5. It decreases the smoothness of the momentum, the exponen-\ntially weighted moving average of past gradients, to take care of the rapid changing gradients\nbecause the generator and the discriminator ﬁght with each other. Besides, the random gen-\nerated noise Z, is a 4-D tensor and we are using GPU to accelerate the computation.\ndef train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\ndevice=d2l.try_gpu()):\nloss = nn.BCEWithLogitsLoss(reduction='sum')\nfor w in net_D.parameters():\nnn.init.normal_(w, 0, 0.02)\nfor w in net_G.parameters():\nnn.init.normal_(w, 0, 0.02)\nnet_D, net_G = net_D.to(device), net_G.to(device)\ntrainer_hp = {'lr': lr, 'betas': [0.5,0.999]}\ntrainer_D = torch.optim.Adam(net_D.parameters(), **trainer_hp)\ntrainer_G = torch.optim.Adam(net_G.parameters(), **trainer_hp)\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[1, num_epochs], nrows=2, figsize=(5, 5),\nlegend=['discriminator', 'generator'])\nanimator.fig.subplots_adjust(hspace=0.3)\nfor epoch in range(1, num_epochs + 1):\n# Train one epoch\n(continues on next page)\n\n924\nGenerative Adversarial Networks\n(continued from previous page)\ntimer = d2l.Timer()\nmetric = d2l.Accumulator(3)\n# loss_D, loss_G, num_examples\nfor X, _ in data_iter:\nbatch_size = X.shape[0]\nZ = torch.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\nX, Z = X.to(device), Z.to(device)\nmetric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),\nd2l.update_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n# Show generated examples\nZ = torch.normal(0, 1, size=(21, latent_dim, 1, 1), device=device)\n# Normalize the synthetic data to N(0, 1)\nfake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5\nimgs = torch.cat(\n[torch.cat([\nfake_x[i * 7 + j].cpu().detach() for j in range(7)], dim=1)\nfor i in range(len(fake_x)//7)], dim=0)\nanimator.axes[1].cla()\nanimator.axes[1].imshow(imgs)\n# Show the losses\nloss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\nanimator.add(epoch, (loss_D, loss_G))\nprint(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\nf'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')\nWe train the model with a small number of epochs just for demonstration. For better perfor-\nmance, the variable num_epochs can be set to a larger number.\nlatent_dim, lr, num_epochs = 100, 0.005, 20\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\nloss_D 0.085, loss_G 7.452, 1912.6 examples/sec on cuda:0\n20.2.5 Summary\n• DCGAN architecture has four convolutional layers for the Discriminator and four “fractionally-\nstrided” convolutional layers for the Generator.\n• The Discriminator is a 4-layer strided convolutions with batch normalization (except its\ninput layer) and leaky ReLU activations.\n• Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It\naims to ﬁx the “dying ReLU” problem and helps the gradients ﬂow easier through the\narchitecture.\n20.2.6 Exercises\n1. What will happen if we use standard ReLU activation rather than leaky ReLU?\n\n925\nDeep Convolutional Generative Adversarial Networks\n277\n2. Apply DCGAN on Fashion-MNIST and see which category works well and which does\nnot.\nDiscussions277.\n\nA\nMathematics for Deep\nLearning\nBrent Werness (Amazon), Rachel Hu (Amazon), and authors of this book\nOne of the wonderful parts of modern deep learning is the fact that much of it can be un-\nderstood and used without a full understanding of the mathematics below it. This is a sign\nthat the ﬁeld is maturing. Just as most software developers no longer need to worry about\nthe theory of computable functions, neither should deep learning practitioners need to worry\nabout the theoretical foundations of maximum likelihood learning.\nBut, we are not quite there yet.\nIn practice, you will sometimes need to understand how architectural choices inﬂuence gra-\ndient ﬂow, or the implicit assumptions you make by training with a certain loss function. You\nmight need to know what in the world entropy measures, and how it can help you understand\nexactly what bits-per-character means in your model. These all require deeper mathematical\nunderstanding.\nThis appendix aims to provide you the mathematical background you need to understand\nthe core theory of modern deep learning, but it is not exhaustive. We will begin with ex-\namining linear algebra in greater depth. We develop a geometric understanding of all the\ncommon linear algebraic objects and operations that will enable us to visualize the eﬀects\nof various transformations on our data. A key element is the development of the basics of\neigen-decompositions.\nWe next develop the theory of diﬀerential calculus to the point that we can fully understand\nwhy the gradient is the direction of steepest descent, and why back-propagation takes the\nform it does. Integral calculus is then discussed to the degree needed to support our next\ntopic, probability theory.\nProblems encountered in practice frequently are not certain, and thus we need a language to\nspeak about uncertain things. We review the theory of random variables and the most com-\nmonly encountered distributions so we may discuss models probabilistically. This provides\nthe foundation for the naive Bayes classiﬁer, a probabilistic classiﬁcation technique.\nClosely related to probability theory is the study of statistics. While statistics is far too large a\nﬁeld to do justice in a short section, we will introduce fundamental concepts that all machine\nlearning practitioners should be aware of, in particular: evaluating and comparing estimators,\nconducting hypothesis tests, and constructing conﬁdence intervals.\nLast, we turn to the topic of information theory, which is the mathematical study of infor-\n926\n\n927\nGeometry and Linear Algebraic Operations\nmation storage and transmission. This provides the core language by which we may discuss\nquantitatively how much information a model holds on a domain of discourse.\nTaken together, these form the core of the mathematical concepts needed to begin down the\npath towards a deep understanding of deep learning.\nA.1 Geometry and Linear Algebraic Operations\nIn Section 2.3, we encountered the basics of linear algebra and saw how it could be used\nto express common operations for transforming our data. Linear algebra is one of the key\nmathematical pillars underlying much of the work that we do in deep learning and in ma-\nchine learning more broadly. While Section 2.3 contained enough machinery to communi-\ncate the mechanics of modern deep learning models, there is a lot more to the subject. In\nthis section, we will go deeper, highlighting some geometric interpretations of linear alge-\nbra operations, and introducing a few fundamental concepts, including of eigenvalues and\neigenvectors.\nA.1.1 Geometry of Vectors\nFirst, we need to discuss the two common geometric interpretations of vectors, as either\npoints or directions in space. Fundamentally, a vector is a list of numbers such as the Python\nlist below.\nv = [1, 7, 0, 1]\nMathematicians most often write this as either a column or row vector, which is to say either\nas\nx =\n\n1\n7\n0\n1\n\n,\n(A.1)\nor\nx⊤=\n[\n1\n7\n0\n1\n]\n.\n(A.2)\nThese often have diﬀerent interpretations, where data examples are column vectors and\nweights used to form weighted sums are row vectors. However, it can be beneﬁcial to be\nﬂexible. As we have described in Section 2.3, though a single vector’s default orientation is a\ncolumn vector, for any matrix representing a tabular dataset, treating each data example as a\nrow vector in the matrix is more conventional.\nGiven a vector, the ﬁrst interpretation that we should give it is as a point in space. In two or\n\n928\nMathematics for Deep Learning\nthree dimensions, we can visualize these points by using the components of the vectors to\ndeﬁne the location of the points in space compared to a ﬁxed reference called the origin. This\ncan be seen in Fig. A.1.\nt\nFig. A.1\nAn illustration of visualizing vectors as points in the plane. The ﬁrst component of the\nvector gives the x-coordinate, the second component gives the y-coordinate. Higher\ndimensions are analogous, although much harder to visualize.\nThis geometric point of view allows us to consider the problem on a more abstract level. No\nlonger faced with some insurmountable seeming problem like classifying pictures as either\ncats or dogs, we can start considering tasks abstractly as collections of points in space and\npicturing the task as discovering how to separate two distinct clusters of points.\nIn parallel, there is a second point of view that people often take of vectors: as directions in\nspace. Not only can we think of the vector v = [3, 2]⊤as the location 3 units to the right\nand 2 units up from the origin, we can also think of it as the direction itself to take 3 steps\nto the right and 2 steps up. In this way, we consider all the vectors in ﬁgure Fig. A.2 the\nsame.\nt\nFig. A.2\nAny vector can be visualized as an arrow in the plane. In this case, every vector drawn is a\nrepresentation of the vector (3, 2)⊤.\nOne of the beneﬁts of this shift is that we can make visual sense of the act of vector addition.\nIn particular, we follow the directions given by one vector, and then follow the directions\ngiven by the other, as is seen in Fig. A.3.\nVector subtraction has a similar interpretation. By considering the identity that u = v+(u−\n\n929\nGeometry and Linear Algebraic Operations\nt\nFig. A.3\nWe can visualize vector addition by ﬁrst following one vector, and then another.\nv), we see that the vector u −v is the direction that takes us from the point v to the point\nu.\nA.1.2 Dot Products and Angles\nAs we saw in Section 2.3, if we take two column vectors u and v, we can form their dot\nproduct by computing:\nu⊤v =\n∑\ni\nui · vi.\n(A.3)\nBecause (A.3) is symmetric, we will mirror the notation of classical multiplication and write\nu · v = u⊤v = v⊤u,\n(A.4)\nto highlight the fact that exchanging the order of the vectors will yield the same answer.\nThe dot product (A.3) also admits a geometric interpretation: it is closely related to the angle\nbetween two vectors. Consider the angle shown in Fig. A.4.\nt\nFig. A.4\nBetween any two vectors in the plane there is a well deﬁned angle θ. We will see this\nangle is intimately tied to the dot product.\nTo start, let’s consider two speciﬁc vectors:\nv = (r, 0) and w = (s cos(θ), s sin(θ)).\n(A.5)\n\n930\nMathematics for Deep Learning\nThe vector v is length r and runs parallel to the x-axis, and the vector w is of length s\nand at angle θ with the x-axis. If we compute the dot product of these two vectors, we see\nthat\nv · w = rs cos(θ) = ∥v∥∥w∥cos(θ).\n(A.6)\nWith some simple algebraic manipulation, we can rearrange terms to obtain\nθ = arccos\n( v · w\n∥v∥∥w∥\n)\n.\n(A.7)\nIn short, for these two speciﬁc vectors, the dot product combined with the norms tell us\nthe angle between the two vectors. This same fact is true in general. We will not derive\nthe expression here, however, if we consider writing ∥v −w∥2 in two ways: one with the\ndot product, and the other geometrically using the law of cosines, we can obtain the full\nrelationship. Indeed, for any two vectors v and w, the angle between the two vectors is\nθ = arccos\n( v · w\n∥v∥∥w∥\n)\n.\n(A.8)\nThis is a nice result since nothing in the computation references two-dimensions. Indeed, we\ncan use this in three or three million dimensions without issue.\nAs a simple example, let’s see how to compute the angle between a pair of vectors:\n%matplotlib inline\nimport torch\nimport torchvision\nfrom IPython import display\nfrom torchvision import transforms\nfrom d2l import torch as d2l\ndef angle(v, w):\nreturn torch.acos(v.dot(w) / (torch.norm(v) * torch.norm(w)))\nangle(torch.tensor([0, 1, 2], dtype=torch.float32), torch.tensor([2.0, 3, 4]))\ntensor(0.4190)\nWe will not use it right now, but it is useful to know that we will refer to vectors for which\nthe angle is π/2 (or equivalently 90◦) as being orthogonal. By examining the equation above,\nwe see that this happens when θ = π/2, which is the same thing as cos(θ) = 0. The only\nway this can happen is if the dot product itself is zero, and two vectors are orthogonal if\nand only if v · w = 0. This will prove to be a helpful formula when understanding objects\ngeometrically.\nIt is reasonable to ask: why is computing the angle useful? The answer comes in the kind of\ninvariance we expect data to have. Consider an image, and a duplicate image, where every\npixel value is the same but 10% the brightness. The values of the individual pixels are in\ngeneral far from the original values. Thus, if one computed the distance between the original\n\n931\nGeometry and Linear Algebraic Operations\nimage and the darker one, the distance can be large. However, for most ML applications, the\ncontent is the same—it is still an image of a cat as far as a cat/dog classiﬁer is concerned.\nHowever, if we consider the angle, it is not hard to see that for any vector v, the angle between\nv and 0.1·v is zero. This corresponds to the fact that scaling vectors keeps the same direction\nand just changes the length. The angle considers the darker image identical.\nExamples like this are everywhere. In text, we might want the topic being discussed to not\nchange if we write twice as long of document that says the same thing. For some encoding\n(such as counting the number of occurrences of words in some vocabulary), this corresponds\nto a doubling of the vector encoding the document, so again we can use the angle.\nCosine Similarity\nIn ML contexts where the angle is employed to measure the closeness of two vectors, prac-\ntitioners adopt the term cosine similarity to refer to the portion\ncos(θ) =\nv · w\n∥v∥∥w∥.\n(A.9)\nThe cosine takes a maximum value of 1 when the two vectors point in the same direction, a\nminimum value of −1 when they point in opposite directions, and a value of 0 when the two\nvectors are orthogonal. Note that if the components of high-dimensional vectors are sampled\nrandomly with mean 0, their cosine will nearly always be close to 0.\nA.1.3 Hyperplanes\nIn addition to working with vectors, another key object that you must understand to go far in\nlinear algebra is the hyperplane, a generalization to higher dimensions of a line (two dimen-\nsions) or of a plane (three dimensions). In an d-dimensional vector space, a hyperplane has\nd −1 dimensions and divides the space into two half-spaces.\nLet’s start with an example. Suppose that we have a column vector w = [2, 1]⊤. We want\nto know, “what are the points v with w · v = 1?” By recalling the connection between dot\nproducts and angles above (A.8), we can see that this is equivalent to\n∥v∥∥w∥cos(θ) = 1\n⇐⇒\n∥v∥cos(θ) =\n1\n∥w∥= 1\n√\n5\n.\n(A.10)\nIf we consider the geometric meaning of this expression, we see that this is equivalent to\nsaying that the length of the projection of v onto the direction of w is exactly 1/∥w∥, as is\nshown in Fig. A.5. The set of all points where this is true is a line at right angles to the vector\nw. If we wanted, we could ﬁnd the equation for this line and see that it is 2x + y = 1 or\nequivalently y = 1 −2x.\nIf we now look at what happens when we ask about the set of points with w · v > 1 or\nw · v < 1, we can see that these are cases where the projections are longer or shorter than\n\n932\nMathematics for Deep Learning\nt\nFig. A.5\nRecalling trigonometry, we see the formula ∥v∥cos(θ) is the length of the projection of\nthe vector v onto the direction of w\n1/∥w∥, respectively. Thus, those two inequalities deﬁne either side of the line. In this way,\nwe have found a way to cut our space into two halves, where all the points on one side have\ndot product below a threshold, and the other side above as we see in Fig. A.6.\nt\nFig. A.6\nIf we now consider the inequality version of the expression, we see that our hyperplane (in\nthis case: just a line) separates the space into two halves.\nThe story in higher dimension is much the same. If we now take w = [1, 2, 3]⊤and ask about\nthe points in three dimensions with w · v = 1, we obtain a plane at right angles to the given\nvector w. The two inequalities again deﬁne the two sides of the plane as is shown in Fig.\nA.7.\nt\nFig. A.7\nHyperplanes in any dimension separate the space into two halves.\nWhile our ability to visualize runs out at this point, nothing stops us from doing this in tens,\nhundreds, or billions of dimensions. This occurs often when thinking about machine learned\n\n933\nGeometry and Linear Algebraic Operations\nmodels. For instance, we can understand linear classiﬁcation models like those from Section\n4.1, as methods to ﬁnd hyperplanes that separate the diﬀerent target classes. In this context,\nsuch hyperplanes are often referred to as decision planes. The majority of deep learned clas-\nsiﬁcation models end with a linear layer fed into a softmax, so one can interpret the role of\nthe deep neural network to be to ﬁnd a non-linear embedding such that the target classes can\nbe separated cleanly by hyperplanes.\nTo give a hand-built example, notice that we can produce a reasonable model to classify tiny\nimages of t-shirts and trousers from the Fashion-MNIST dataset (seen in Section 4.2) by\njust taking the vector between their means to deﬁne the decision plane and eyeball a crude\nthreshold. First we will load the data and compute the averages.\n# Load in the dataset\ntrans = []\ntrans.append(transforms.ToTensor())\ntrans = transforms.Compose(trans)\ntrain = torchvision.datasets.FashionMNIST(root=\"../data\", transform=trans,\ntrain=True, download=True)\ntest = torchvision.datasets.FashionMNIST(root=\"../data\", transform=trans,\ntrain=False, download=True)\nX_train_0 = torch.stack(\n[x[0] * 256 for x in train if x[1] == 0]).type(torch.float32)\nX_train_1 = torch.stack(\n[x[0] * 256 for x in train if x[1] == 1]).type(torch.float32)\nX_test = torch.stack(\n[x[0] * 256 for x in test if x[1] == 0 or x[1] == 1]).type(torch.float32)\ny_test = torch.stack([torch.tensor(x[1]) for x in test\nif x[1] == 0 or x[1] == 1]).type(torch.float32)\n# Compute averages\nave_0 = torch.mean(X_train_0, axis=0)\nave_1 = torch.mean(X_train_1, axis=0)\nIt can be informative to examine these averages in detail, so let’s plot what they look like. In\nthis case, we see that the average indeed resembles a blurry image of a t-shirt.\n# Plot average t-shirt\nd2l.set_figsize()\nd2l.plt.imshow(ave_0.reshape(28, 28).tolist(), cmap='Greys')\nd2l.plt.show()\nIn the second case, we again see that the average resembles a blurry image of trousers.\n# Plot average trousers\nd2l.plt.imshow(ave_1.reshape(28, 28).tolist(), cmap='Greys')\nd2l.plt.show()\nIn a fully machine learned solution, we would learn the threshold from the dataset. In this\ncase, I simply eyeballed a threshold that looked good on the training data by hand.\n\n934\nMathematics for Deep Learning\n# Print test set accuracy with eyeballed threshold\nw = (ave_1 - ave_0).T\n# '@' is Matrix Multiplication operator in pytorch.\npredictions = X_test.reshape(2000, -1) @ (w.flatten()) > -1500000\n# Accuracy\ntorch.mean((predictions.type(y_test.dtype) == y_test).float(), dtype=torch.\n,→float64)\ntensor(0.7870, dtype=torch.float64)\nA.1.4 Geometry of Linear Transformations\nThrough Section 2.3 and the above discussions, we have a solid understanding of the geom-\netry of vectors, lengths, and angles. However, there is one important object we have omitted\ndiscussing, and that is a geometric understanding of linear transformations represented by\nmatrices. Fully internalizing what matrices can do to transform data between two potentially\ndiﬀerent high dimensional spaces takes signiﬁcant practice, and is beyond the scope of this\nappendix. However, we can start building up intuition in two dimensions.\n\n935\nGeometry and Linear Algebraic Operations\nSuppose that we have some matrix:\nA =\n[a\nb\nc\nd\n]\n.\n(A.11)\nIf we want to apply this to an arbitrary vector v = [x, y]⊤, we multiply and see that\nAv =\n[a\nb\nc\nd\n] [x\ny\n]\n=\n[ax + by\ncx + dy\n]\n= x\n[a\nc\n]\n+ y\n[b\nd\n]\n= x\n{\nA\n[1\n0\n]}\n+ y\n{\nA\n[0\n1\n]}\n.\n(A.12)\nThis may seem like an odd computation, where something clear became somewhat impene-\ntrable. However, it tells us that we can write the way that a matrix transforms any vector in\nterms of how it transforms two speciﬁc vectors: [1, 0]⊤and [0, 1]⊤. This is worth considering\nfor a moment. We have essentially reduced an inﬁnite problem (what happens to any pair of\nreal numbers) to a ﬁnite one (what happens to these speciﬁc vectors). These vectors are an\nexample a basis, where we can write any vector in our space as a weighted sum of these basis\nvectors.\nLet’s draw what happens when we use the speciﬁc matrix\nA =\n[ 1\n2\n−1\n3\n]\n.\n(A.13)\nIf we look at the speciﬁc vector v = [2, −1]⊤, we see this is 2·[1, 0]⊤+−1·[0, 1]⊤, and thus we\nknow that the matrix A will send this to 2(A[1, 0]⊤)+−1(A[0, 1])⊤= 2[1, −1]⊤−[2, 3]⊤=\n[0, −5]⊤. If we follow this logic through carefully, say by considering the grid of all integer\npairs of points, we see that what happens is that the matrix multiplication can skew, rotate,\nand scale the grid, but the grid structure must remain as you see in Fig. A.8.\nt\nFig. A.8\nThe matrix A acting on the given basis vectors. Notice how the entire grid is transported\nalong with it.\n\n936\nMathematics for Deep Learning\nThis is the most important intuitive point to internalize about linear transformations repre-\nsented by matrices. Matrices are incapable of distorting some parts of space diﬀerently than\nothers. All they can do is take the original coordinates on our space and skew, rotate, and\nscale them.\nSome distortions can be severe. For instance the matrix\nB =\n[2\n−1\n4\n−2\n]\n,\n(A.14)\ncompresses the entire two-dimensional plane down to a single line. Identifying and working\nwith such transformations are the topic of a later section, but geometrically we can see that\nthis is fundamentally diﬀerent from the types of transformations we saw above. For instance,\nthe result from matrix A can be “bent back” to the original grid. The results from matrix B\ncannot because we will never know where the vector [1, 2]⊤came from—was it [1, 1]⊤or\n[0, −1]⊤?\nWhile this picture was for a 2×2 matrix, nothing prevents us from taking the lessons learned\ninto higher dimensions. If we take similar basis vectors like [1, 0, . . ., 0] and see where our\nmatrix sends them, we can start to get a feeling for how the matrix multiplication distorts the\nentire space in whatever dimension space we are dealing with.\nA.1.5 Linear Dependence\nConsider again the matrix\nB =\n[2\n−1\n4\n−2\n]\n.\n(A.15)\nThis compresses the entire plane down to live on the single line y = 2x. The question now\narises: is there some way we can detect this just looking at the matrix itself? The answer is\nthat indeed we can. Let’s take b1 = [2, 4]⊤and b2 = [−1, −2]⊤be the two columns of B.\nRemember that we can write everything transformed by the matrix B as a weighted sum of\nthe columns of the matrix: like a1b1 + a2b2. We call this a linear combination. The fact that\nb1 = −2 · b2 means that we can write any linear combination of those two columns entirely\nin terms of say b2 since\na1b1 + a2b2 = −2a1b2 + a2b2 = (a2 −2a1)b2.\n(A.16)\nThis means that one of the columns is, in a sense, redundant because it does not deﬁne a\nunique direction in space. This should not surprise us too much since we already saw that\nthis matrix collapses the entire plane down into a single line. Moreover, we see that the linear\ndependence b1 = −2 · b2 captures this. To make this more symmetrical between the two\nvectors, we will write this as\nb1 + 2 · b2 = 0.\n(A.17)\n\n937\nGeometry and Linear Algebraic Operations\nIn general, we will say that a collection of vectors v1, . . ., vk are linearly dependent if there\nexist coeﬃcients a1, . . ., ak not all equal to zero so that\nk\n∑\ni=1\naivi = 0.\n(A.18)\nIn this case, we can solve for one of the vectors in terms of some combination of the others,\nand eﬀectively render it redundant. Thus, a linear dependence in the columns of a matrix\nis a witness to the fact that our matrix is compressing the space down to some lower di-\nmension. If there is no linear dependence we say the vectors are linearly independent. If the\ncolumns of a matrix are linearly independent, no compression occurs and the operation can\nbe undone.\nA.1.6 Rank\nIf we have a general n × m matrix, it is reasonable to ask what dimension space the matrix\nmaps into. A concept known as the rank will be our answer. In the previous section, we noted\nthat a linear dependence bears witness to compression of space into a lower dimension and\nso we will be able to use this to deﬁne the notion of rank. In particular, the rank of a matrix\nA is the largest number of linearly independent columns amongst all subsets of columns. For\nexample, the matrix\nB =\n[ 2\n4\n−1\n−2\n]\n,\n(A.19)\nhas rank(B) = 1, since the two columns are linearly dependent, but either column by itself\nis not linearly dependent. For a more challenging example, we can consider\nC =\n\n1\n3\n0\n−1\n0\n−1\n0\n1\n1\n−1\n0\n3\n1\n0\n−1\n2\n3\n−1\n−2\n1\n\n,\n(A.20)\nand show that C has rank two since, for instance, the ﬁrst two columns are linearly indepen-\ndent, however any of the four collections of three columns are dependent.\nThis procedure, as described, is very ineﬃcient. It requires looking at every subset of the\ncolumns of our given matrix, and thus is potentially exponential in the number of columns.\nLater we will see a more computationally eﬃcient way to compute the rank of a matrix, but\nfor now, this is suﬃcient to see that the concept is well deﬁned and understand the mean-\ning.\nA.1.7 Invertibility\nWe have seen above that multiplication by a matrix with linearly dependent columns cannot\nbe undone, i.e., there is no inverse operation that can always recover the input. However,\n\n938\nMathematics for Deep Learning\nmultiplication by a full-rank matrix (i.e., some A that is n×n matrix with rank n), we should\nalways be able to undo it. Consider the matrix\nI =\n\n1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n1\n\n.\n(A.21)\nwhich is the matrix with ones along the diagonal, and zeros elsewhere. We call this the identity\nmatrix. It is the matrix which leaves our data unchanged when applied. To ﬁnd a matrix which\nundoes what our matrix A has done, we want to ﬁnd a matrix A−1 such that\nA−1A = AA−1 = I.\n(A.22)\nIf we look at this as a system, we have n×n unknowns (the entries of A−1) and n×n equations\n(the equality that needs to hold between every entry of the product A−1A and every entry of\nI) so we should generically expect a solution to exist. Indeed, in the next section we will see a\nquantity called the determinant, which has the property that as long as the determinant is not\nzero, we can ﬁnd a solution. We call such a matrix A−1 the inverse matrix. As an example,\nif A is the general 2 × 2 matrix\nA =\n[a\nb\nc\nd\n]\n,\n(A.23)\nthen we can see that the inverse is\n1\nad −bc\n[ d\n−b\n−c\na\n]\n.\n(A.24)\nWe can test to see this by seeing that multiplying by the inverse given by the formula above\nworks in practice.\nM = torch.tensor([[1, 2], [1, 4]], dtype=torch.float32)\nM_inv = torch.tensor([[2, -1], [-0.5, 0.5]])\nM_inv @ M\ntensor([[1., 0.],\n[0., 1.]])\nNumerical Issues\nWhile the inverse of a matrix is useful in theory, we must say that most of the time we do not\nwish to use the matrix inverse to solve a problem in practice. In general, there are far more\nnumerically stable algorithms for solving linear equations like\nAx = b,\n(A.25)\n\n939\nGeometry and Linear Algebraic Operations\nthan computing the inverse and multiplying to get\nx = A−1b.\n(A.26)\nJust as division by a small number can lead to numerical instability, so can inversion of a\nmatrix which is close to having low rank.\nMoreover, it is common that the matrix A is sparse, which is to say that it contains only a\nsmall number of non-zero values. If we were to explore examples, we would see that this\ndoes not mean the inverse is sparse. Even if A was a 1 million by 1 million matrix with\nonly 5 million non-zero entries (and thus we need only store those 5 million), the inverse will\ntypically have almost every entry non-negative, requiring us to store all 1M2 entries—that is\n1 trillion entries!\nWhile we do not have time to dive all the way into the thorny numerical issues frequently\nencountered when working with linear algebra, we want to provide you with some intuition\nabout when to proceed with caution, and generally avoiding inversion in practice is a good\nrule of thumb.\nA.1.8 Determinant\nThe geometric view of linear algebra gives an intuitive way to interpret a fundamental quantity\nknown as the determinant. Consider the grid image from before, but now with a highlighted\nregion (Fig. A.9).\nt\nFig. A.9\nThe matrix A again distorting the grid. This time, I want to draw particular attention to\nwhat happens to the highlighted square.\nLook at the highlighted square. This is a square with edges given by (0, 1) and (1, 0) and thus\nit has area one. After A transforms this square, we see that it becomes a parallelogram. There\nis no reason this parallelogram should have the same area that we started with, and indeed in\nthe speciﬁc case shown here of\nA =\n[ 1\n2\n−1\n3\n]\n,\n(A.27)\nit is an exercise in coordinate geometry to compute the area of this parallelogram and obtain\nthat the area is 5.\n\n940\nMathematics for Deep Learning\nIn general, if we have a matrix\nA =\n[a\nb\nc\nd\n]\n,\n(A.28)\nwe can see with some computation that the area of the resulting parallelogram is ad −bc.\nThis area is referred to as the determinant.\nLet’s check this quickly with some example code.\ntorch.det(torch.tensor([[1, -1], [2, 3]], dtype=torch.float32))\ntensor(5.)\nThe eagle-eyed amongst us will notice that this expression can be zero or even negative. For\nthe negative term, this is a matter of convention taken generally in mathematics: if the matrix\nﬂips the ﬁgure, we say the area is negated. Let’s see now that when the determinant is zero,\nwe learn more.\nLet’s consider\nB =\n[ 2\n4\n−1\n−2\n]\n.\n(A.29)\nIf we compute the determinant of this matrix, we get 2 · (−2) −4 · (−1) = 0. Given our\nunderstanding above, this makes sense. B compresses the square from the original image\ndown to a line segment, which has zero area. And indeed, being compressed into a lower\ndimensional space is the only way to have zero area after the transformation. Thus we see the\nfollowing result is true: a matrix A is invertible if and only if the determinant is not equal to\nzero.\nAs a ﬁnal comment, imagine that we have any ﬁgure drawn on the plane. Thinking like com-\nputer scientists, we can decompose that ﬁgure into a collection of little squares so that the\narea of the ﬁgure is in essence just the number of squares in the decomposition. If we now\ntransform that ﬁgure by a matrix, we send each of these squares to parallelograms, each one\nof which has area given by the determinant. We see that for any ﬁgure, the determinant gives\nthe (signed) number that a matrix scales the area of any ﬁgure.\nComputing determinants for larger matrices can be laborious, but the intuition is the same.\nThe determinant remains the factor that n × n matrices scale n-dimensional volumes.\nA.1.9 Tensors and Common Linear Algebra Operations\nIn Section 2.3 the concept of tensors was introduced. In this section, we will dive more deeply\ninto tensor contractions (the tensor equivalent of matrix multiplication), and see how it can\nprovide a uniﬁed view on a number of matrix and vector operations.\nWith matrices and vectors we knew how to multiply them to transform data. We need to have\n\n941\nGeometry and Linear Algebraic Operations\na similar deﬁnition for tensors if they are to be useful to us. Think about matrix multiplica-\ntion:\nC = AB,\n(A.30)\nor equivalently\nci,j =\n∑\nk\nai,kbk,j.\n(A.31)\nThis pattern is one we can repeat for tensors. For tensors, there is no one case of what to sum\nover that can be universally chosen, so we need specify exactly which indices we want to sum\nover. For instance we could consider\nyil =\n∑\njk\nxijklajk.\n(A.32)\nSuch a transformation is called a tensor contraction. It can represent a far more ﬂexible family\nof transformations that matrix multiplication alone.\nAs a often-used notational simpliﬁcation, we can notice that the sum is over exactly those\nindices that occur more than once in the expression, thus people often work with Einstein\nnotation, where the summation is implicitly taken over all repeated indices. This gives the\ncompact expression:\nyil = xijklajk.\n(A.33)\nCommon Examples from Linear Algebra\nLet’s see how many of the linear algebraic deﬁnitions we have seen before can be expressed\nin this compressed tensor notation:\n• v · w = ∑\ni viwi\n• ∥v∥2\n2 = ∑\ni vivi\n• (Av)i = ∑\nj aijvj\n• (AB)ik = ∑\nj aijbjk\n• tr(A) = ∑\ni aii\nIn this way, we can replace a myriad of specialized notations with short tensor expressions.\nExpressing in Code\nTensors may ﬂexibly be operated on in code as well. As seen in Section 2.3, we can create\ntensors as is shown below.\n\n942\nMathematics for Deep Learning\n# Define tensors\nB = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\nA = torch.tensor([[1, 2], [3, 4]])\nv = torch.tensor([1, 2])\n# Print out the shapes\nA.shape, B.shape, v.shape\n(torch.Size([2, 2]), torch.Size([2, 2, 3]), torch.Size([2]))\nEinstein summation has been implemented directly. The indices that occurs in the Einstein\nsummation can be passed as a string, followed by the tensors that are being acted upon. For\ninstance, to implement matrix multiplication, we can consider the Einstein summation seen\nabove (Av = aijvj) and strip out the indices themselves to get the implementation:\n# Reimplement matrix multiplication\ntorch.einsum(\"ij, j -> i\", A, v), A@v\n(tensor([ 5, 11]), tensor([ 5, 11]))\nThis is a highly ﬂexible notation. For instance if we want to compute what would be tradi-\ntionally written as\nckl =\n∑\nij\nbijkailvj.\n(A.34)\nit can be implemented via Einstein summation as:\ntorch.einsum(\"ijk, il, j -> kl\", B, A, v)\ntensor([[ 90, 126],\n[102, 144],\n[114, 162]])\nThis notation is readable and eﬃcient for humans, however bulky if for whatever reason we\nneed to generate a tensor contraction programmatically. For this reason, einsum provides\nan alternative notation by providing integer indices for each tensor. For example, the same\ntensor contraction can also be written as:\n# PyTorch does not support this type of notation.\nEither notation allows for concise and eﬃcient representation of tensor contractions in code.\nA.1.10 Summary\n• Vectors can be interpreted geometrically as either points or directions in space.\n\n943\nGeometry and Linear Algebraic Operations\n• Dot products deﬁne the notion of angle to arbitrarily high-dimensional spaces.\n• Hyperplanes are high-dimensional generalizations of lines and planes. They can be used\nto deﬁne decision planes that are often used as the last step in a classiﬁcation task.\n• Matrix multiplication can be geometrically interpreted as uniform distortions of the un-\nderlying coordinates. They represent a very restricted, but mathematically clean, way to\ntransform vectors.\n• Linear dependence is a way to tell when a collection of vectors are in a lower dimensional\nspace than we would expect (say you have 3 vectors living in a 2-dimensional space).\nThe rank of a matrix is the size of the largest subset of its columns that are linearly\nindependent.\n• When a matrix’s inverse is deﬁned, matrix inversion allows us to ﬁnd another matrix that\nundoes the action of the ﬁrst. Matrix inversion is useful in theory, but requires care in\npractice owing to numerical instability.\n• Determinants allow us to measure how much a matrix expands or contracts a space. A\nnonzero determinant implies an invertible (non-singular) matrix and a zero-valued de-\nterminant means that the matrix is non-invertible (singular).\n• Tensor contractions and Einstein summation provide for a neat and clean notation for ex-\npressing many of the computations that are seen in machine learning.\nA.1.11 Exercises\n1. What is the angle between\n®v1 =\n\n1\n0\n−1\n2\n\n,\n®v2 =\n\n3\n1\n0\n1\n\n?\n(A.35)\n2. True or false:\n[1\n2\n0\n1\n]\nand\n[1\n−2\n0\n1\n]\nare inverses of one another?\n3. Suppose that we draw a shape in the plane with area 100m2. What is the area after trans-\nforming the ﬁgure by the matrix\n[2\n3\n1\n2\n]\n.\n(A.36)\n4. Which of the following sets of vectors are linearly independent?\n•\n\n\n©­­\n«\n1\n0\n−1\nª®®\n¬\n,\n©­­\n«\n2\n1\n−1\nª®®\n¬\n,\n©­­\n«\n3\n1\n1\nª®®\n¬\n\n\n\n944\nMathematics for Deep Learning\n278\n•\n\n\n©­­\n«\n3\n1\n1\nª®®\n¬\n,\n©­­\n«\n1\n1\n1\nª®®\n¬\n,\n©­­\n«\n0\n0\n0\nª®®\n¬\n\n\n•\n\n\n©­­\n«\n1\n1\n0\nª®®\n¬\n,\n©­­\n«\n0\n1\n−1\nª®®\n¬\n,\n©­­\n«\n1\n0\n1\nª®®\n¬\n\n\n5. Suppose that you have a matrix written as A =\n[c\nd\n]\n·\n[\na\nb\n]\nfor some choice of values\na, b, c, and d. True or false: the determinant of such a matrix is always 0?\n6. The vectors e1 =\n[1\n0\n]\nand e2 =\n[0\n1\n]\nare orthogonal. What is the condition on a matrix A\nso that Ae1 and Ae2 are orthogonal?\n7. How can you write tr(A4) in Einstein notation for an arbitrary matrix A?\nDiscussions278.\nA.2 Eigendecompositions\nEigenvalues are often one of the most useful notions we will encounter when studying linear\nalgebra, however, as a beginner, it is easy to overlook their importance. Below, we introduce\neigendecomposition and try to convey some sense of just why it is so important.\nSuppose that we have a matrix A with the following entries:\nA =\n[2\n0\n0\n−1\n]\n.\n(A.1)\nIf we apply A to any vector v = [x, y]⊤, we obtain a vector Av = [2x, −y]⊤. This has an\nintuitive interpretation: stretch the vector to be twice as wide in the x-direction, and then ﬂip\nit in the y-direction.\nHowever, there are some vectors for which something remains unchanged. Namely [1, 0]⊤\ngets sent to [2, 0]⊤and [0, 1]⊤gets sent to [0, −1]⊤. These vectors are still in the same line,\nand the only modiﬁcation is that the matrix stretches them by a factor of 2 and −1 respectively.\nWe call such vectors eigenvectors and the factor they are stretched by eigenvalues.\nIn general, if we can ﬁnd a number λ and a vector v such that\nAv = λv.\n(A.2)\nWe say that v is an eigenvector for A and λ is an eigenvalue.\n\n945\nEigendecompositions\nA.2.1 Finding Eigenvalues\nLet’s ﬁgure out how to ﬁnd them. By subtracting oﬀthe λv from both sides, and then factoring\nout the vector, we see the above is equivalent to:\n(A −λI)v = 0.\n(A.3)\nFor (A.3) to happen, we see that (A −λI) must compress some direction down to zero,\nhence it is not invertible, and thus the determinant is zero. Thus, we can ﬁnd the eigenvalues\nby ﬁnding for what λ is det(A −λI) = 0. Once we ﬁnd the eigenvalues, we can solve\nAv = λv to ﬁnd the associated eigenvector(s).\nAn Example\nLet’s see this with a more challenging matrix\nA =\n[2\n1\n2\n3\n]\n.\n(A.4)\nIf we consider det(A −λI) = 0, we see this is equivalent to the polynomial equation 0 =\n(2−λ)(3−λ)−2 = (4−λ)(1−λ). Thus, two eigenvalues are 4 and 1. To ﬁnd the associated\nvectors, we then need to solve\n[2\n1\n2\n3\n] [x\ny\n]\n=\n[x\ny\n]\nand\n[2\n1\n2\n3\n] [x\ny\n]\n=\n[4x\n4y\n]\n.\n(A.5)\nWe can solve this with the vectors [1, −1]⊤and [1, 2]⊤respectively.\nWe can check this in code using the built-in numpy.linalg.eig routine.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\ntorch.linalg.eig(torch.tensor([[2, 1], [2, 3]], dtype=torch.float64))\ntorch.return_types.linalg_eig(\neigenvalues=tensor([1.+0.j, 4.+0.j], dtype=torch.complex128),\neigenvectors=tensor([[-0.7071+0.j, -0.4472+0.j],\n[ 0.7071+0.j, -0.8944+0.j]], dtype=torch.complex128))\nNote that numpy normalizes the eigenvectors to be of length one, whereas we took ours to\nbe of arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors\ncomputed are parallel to the ones we found by hand with the same eigenvalues.\n\n946\nMathematics for Deep Learning\nA.2.2 Decomposing Matrices\nLet’s continue the previous example one step further. Let\nW =\n[ 1\n1\n−1\n2\n]\n,\n(A.6)\nbe the matrix where the columns are the eigenvectors of the matrix A. Let\nΣ =\n[1\n0\n0\n4\n]\n,\n(A.7)\nbe the matrix with the associated eigenvalues on the diagonal. Then the deﬁnition of eigen-\nvalues and eigenvectors tells us that\nAW = WΣ.\n(A.8)\nThe matrix W is invertible, so we may multiply both sides by W−1 on the right, we see that\nwe may write\nA = WΣW−1.\n(A.9)\nIn the next section we will see some nice consequences of this, but for now we need only\nknow that such a decomposition will exist as long as we can ﬁnd a full collection of linearly\nindependent eigenvectors (so that W is invertible).\nA.2.3 Operations on Eigendecompositions\nOne nice thing about eigendecompositions (A.9) is that we can write many operations we usu-\nally encounter cleanly in terms of the eigendecomposition. As a ﬁrst example, consider:\nAn =\nn times\nz   }|   {\nA · · · A =\nn times\nz                               }|                               {\n(WΣW−1) · · · (WΣW−1) = W\nn times\nz   }|   {\nΣ · · · Σ W−1 = WΣnW−1.\n(A.10)\nThis tells us that for any positive power of a matrix, the eigendecomposition is obtained by\njust raising the eigenvalues to the same power. The same can be shown for negative powers,\nso if we want to invert a matrix we need only consider\nA−1 = WΣ−1W−1,\n(A.11)\nor in other words, just invert each eigenvalue. This will work as long as each eigenvalue is\nnon-zero, so we see that invertible is the same as having no zero eigenvalues.\nIndeed, additional work can show that if λ1, . . ., λn are the eigenvalues of a matrix, then the\ndeterminant of that matrix is\ndet(A) = λ1 · · · λn,\n(A.12)\n\n947\nEigendecompositions\nor the product of all the eigenvalues. This makes sense intuitively because whatever stretch-\ning W does, W−1 undoes it, so in the end the only stretching that happens is by multipli-\ncation by the diagonal matrix Σ, which stretches volumes by the product of the diagonal\nelements.\nFinally, recall that the rank was the maximum number of linearly independent columns of\nyour matrix. By examining the eigendecomposition closely, we can see that the rank is the\nsame as the number of non-zero eigenvalues of A.\nThe examples could continue, but hopefully the point is clear: eigendecomposition can sim-\nplify many linear-algebraic computations and is a fundamental operation underlying many\nnumerical algorithms and much of the analysis that we do in linear algebra.\nA.2.4 Eigendecompositions of Symmetric Matrices\nIt is not always possible to ﬁnd enough linearly independent eigenvectors for the above process\nto work. For instance the matrix\nA =\n[1\n1\n0\n1\n]\n,\n(A.13)\nhas only a single eigenvector, namely (1, 0)⊤. To handle such matrices, we require more\nadvanced techniques than we can cover (such as the Jordan Normal Form, or Singular Value\nDecomposition). We will often need to restrict our attention to those matrices where we can\nguarantee the existence of a full set of eigenvectors.\nThe most commonly encountered family are the symmetric matrices, which are those matrices\nwhere A = A⊤. In this case, we may take W to be an orthogonal matrix—a matrix whose\ncolumns are all length one vectors that are at right angles to one another, where W⊤=\nW−1—and all the eigenvalues will be real. Thus, in this special case, we can write (A.9)\nas\nA = WΣW⊤.\n(A.14)\nA.2.5 Gershgorin Circle Theorem\nEigenvalues are often diﬃcult to reason with intuitively. If presented an arbitrary matrix,\nthere is little that can be said about what the eigenvalues are without computing them. There\nis, however, one theorem that can make it easy to approximate well if the largest values are\non the diagonal.\nLet A = (aij) be any square matrix (n × n). We will deﬁne ri = ∑\nj,i |aij|. Let Di repre-\nsent the disc in the complex plane with center aii radius ri. Then, every eigenvalue of A is\ncontained in one of the Di.\n\n948\nMathematics for Deep Learning\nThis can be a bit to unpack, so let’s look at an example. Consider the matrix:\nA =\n\n1.0\n0.1\n0.1\n0.1\n0.1\n3.0\n0.2\n0.3\n0.1\n0.2\n5.0\n0.5\n0.1\n0.3\n0.5\n9.0\n\n.\n(A.15)\nWe have r1 = 0.3, r2 = 0.6, r3 = 0.8 and r4 = 0.9. The matrix is symmetric, so all eigen-\nvalues are real. This means that all of our eigenvalues will be in one of the ranges of\n[a11 −r1, a11 + r1] = [0.7, 1.3],\n(A.16)\n[a22 −r2, a22 + r2] = [2.4, 3.6],\n(A.17)\n[a33 −r3, a33 + r3] = [4.2, 5.8],\n(A.18)\n[a44 −r4, a44 + r4] = [8.1, 9.9].\n(A.19)\nPerforming the numerical computation shows that the eigenvalues are approximately 0.99,\n2.97, 4.95, 9.08, all comfortably inside the ranges provided.\nA = torch.tensor([[1.0, 0.1, 0.1, 0.1],\n[0.1, 3.0, 0.2, 0.3],\n[0.1, 0.2, 5.0, 0.5],\n[0.1, 0.3, 0.5, 9.0]])\nv, _ = torch.linalg.eig(A)\nv\ntensor([0.9923+0.j, 9.0803+0.j, 4.9539+0.j, 2.9734+0.j])\nIn this way, eigenvalues can be approximated, and the approximations will be fairly accurate\nin the case that the diagonal is signiﬁcantly larger than all the other elements.\nIt is a small thing, but with a complex and subtle topic like eigendecomposition, it is good to\nget any intuitive grasp we can.\nA.2.6 A Useful Application: The Growth of Iterated Maps\nNow that we understand what eigenvectors are in principle, let’s see how they can be used to\nprovide a deep understanding of a problem central to neural network behavior: proper weight\ninitialization.\nEigenvectors as Long Term Behavior\nThe full mathematical investigation of the initialization of deep neural networks is beyond the\nscope of the text, but we can see a toy version here to understand how eigenvalues can help\n\n949\nEigendecompositions\nus see how these models work. As we know, neural networks operate by interspersing layers\nof linear transformations with non-linear operations. For simplicity here, we will assume that\nthere is no non-linearity, and that the transformation is a single repeated matrix operation A,\nso that the output of our model is\nvout = A · A · · · Avin = ANvin.\n(A.20)\nWhen these models are initialized, A is taken to be a random matrix with Gaussian entries,\nso let’s make one of those. To be concrete, we start with a mean zero, variance one Gaussian\ndistributed 5 × 5 matrix.\ntorch.manual_seed(42)\nk = 5\nA = torch.randn(k, k, dtype=torch.float64)\nA\ntensor([[ 0.2996,\n0.2424,\n0.2832, -0.2329,\n0.6712],\n[ 0.7818, -1.7903, -1.7484,\n0.1735, -0.1182],\n[-1.7446, -0.4695,\n0.4573,\n0.5177, -0.2771],\n[-0.6641,\n0.6551,\n0.2616, -1.5265, -0.3311],\n[-0.6378,\n0.1072,\n0.7096,\n0.3009, -0.2869]], dtype=torch.float64)\nBehavior on Random Data\nFor simplicity in our toy model, we will assume that the data vector we feed in vin is a\nrandom ﬁve dimensional Gaussian vector. Let’s think about what we want to have happen.\nFor context, lets think of a generic ML problem, where we are trying to turn input data, like\nan image, into a prediction, like the probability the image is a picture of a cat. If repeated\napplication of A stretches a random vector out to be very long, then small changes in input\nwill be ampliﬁed into large changes in output—tiny modiﬁcations of the input image would\nlead to vastly diﬀerent predictions. This does not seem right!\nOn the ﬂip side, if A shrinks random vectors to be shorter, then after running through many\nlayers, the vector will essentially shrink to nothing, and the output will not depend on the\ninput. This is also clearly not right either!\nWe need to walk the narrow line between growth and decay to make sure that our output\nchanges depending on our input, but not much!\nLet’s see what happens when we repeatedly multiply our matrix A against a random input\nvector, and keep track of the norm.\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in = torch.randn(k, 1, dtype=torch.float64)\nnorm_list = [torch.norm(v_in).item()]\n(continues on next page)\n\n950\nMathematics for Deep Learning\n(continued from previous page)\nfor i in range(1, 100):\nv_in = A @ v_in\nnorm_list.append(torch.norm(v_in).item())\nd2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')\nThe norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a\npattern.\n# Compute the scaling factor of the norms\nnorm_ratio_list = []\nfor i in range(1, 100):\nnorm_ratio_list.append(norm_list[i]/norm_list[i - 1])\nd2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\nIf we look at the last portion of the above computation, we see that the random vector is\nstretched by a factor of 1.974459321485[...], where the portion at the end shifts a little,\nbut the stretching factor is stable.\n\n951\nEigendecompositions\nRelating Back to Eigenvectors\nWe have seen that eigenvectors and eigenvalues correspond to the amount something is\nstretched, but that was for speciﬁc vectors, and speciﬁc stretches. Let’s take a look at what\nthey are for A. A bit of a caveat here: it turns out that to see them all, we will need to go to\ncomplex numbers. You can think of these as stretches and rotations. By taking the norm of\nthe complex number (square root of the sums of squares of real and imaginary parts) we can\nmeasure that stretching factor. Let’s also sort them.\n# Compute the eigenvalues\neigs = torch.linalg.eig(A).eigenvalues.tolist()\nnorm_eigs = [torch.abs(torch.tensor(x)) for x in eigs]\nnorm_eigs.sort()\nprint(f'norms of eigenvalues: {norm_eigs}')\nnorms of eigenvalues: [tensor(0.3490), tensor(1.1296), tensor(1.1296),␣\n,→tensor(1.1828), tensor(2.4532)]\nAn Observation\nWe see something a bit unexpected happening here: that number we identiﬁed before for\nthe long term stretching of our matrix A applied to a random vector is exactly (accurate to\nthirteen decimal places!) the largest eigenvalue of A. This is clearly not a coincidence!\nBut, if we now think about what is happening geometrically, this starts to make sense. Con-\nsider a random vector. This random vector points a little in every direction, so in particular,\nit points at least a little bit in the same direction as the eigenvector of A associated with the\nlargest eigenvalue. This is so important that it is called the principle eigenvalue and principle\neigenvector. After applying A, our random vector gets stretched in every possible direction,\nas is associated with every possible eigenvector, but it is stretched most of all in the direc-\ntion associated with this principle eigenvector. What this means is that after apply in A, our\nrandom vector is longer, and points in a direction closer to being aligned with the principle\neigenvector. After applying the matrix many times, the alignment with the principle eigen-\nvector becomes closer and closer until, for all practical purposes, our random vector has\nbeen transformed into the principle eigenvector! Indeed this algorithm is the basis for what\nis known as the power iteration for ﬁnding the largest eigenvalue and eigenvector of a matrix.\nFor details see, for example, (Golub and Van Loan, 1996).\nFixing the Normalization\nNow, from above discussions, we concluded that we do not want a random vector to be\nstretched or squished at all, we would like random vectors to stay about the same size through-\nout the entire process. To do so, we now rescale our matrix by this principle eigenvalue so\nthat the largest eigenvalue is instead now just one. Let’s see what happens in this case.\n\n952\nMathematics for Deep Learning\n# Rescale the matrix `A`\nA /= norm_eigs[-1]\n# Do the same experiment again\nv_in = torch.randn(k, 1, dtype=torch.float64)\nnorm_list = [torch.norm(v_in).item()]\nfor i in range(1, 100):\nv_in = A @ v_in\nnorm_list.append(torch.norm(v_in).item())\nd2l.plot(torch.arange(0, 100), norm_list, 'Iteration', 'Value')\nWe can also plot the ratio between consecutive norms as before and see that indeed it stabi-\nlizes.\n# Also plot the ratio\nnorm_ratio_list = []\nfor i in range(1, 100):\nnorm_ratio_list.append(norm_list[i]/norm_list[i-1])\nd2l.plot(torch.arange(1, 100), norm_ratio_list, 'Iteration', 'Ratio')\n\n953\nEigendecompositions\nA.2.7 Discussion\nWe now see exactly what we hoped for! After normalizing the matrices by the principal\neigenvalue, we see that the random data does not explode as before, but rather eventually\nequilibrates to a speciﬁc value. It would be nice to be able to do these things from ﬁrst prin-\nciples, and it turns out that if we look deeply at the mathematics of it, we can see that the\nlargest eigenvalue of a large random matrix with independent mean zero, variance one Gaus-\nsian entries is on average about √n, or in our case\n√\n5 ≈2.2, due to a fascinating fact known\nas the circular law (Ginibre, 1965). The relationship between the eigenvalues (and a related\nobject called singular values) of random matrices has been shown to have deep connections\nto proper initialization of neural networks as was discussed in Pennington et al. (2017) and\nsubsequent works.\nA.2.8 Summary\n• Eigenvectors are vectors which are stretched by a matrix without changing direction.\n• Eigenvalues are the amount that the eigenvectors are stretched by the application of the\nmatrix.\n• The eigendecomposition of a matrix can allow for many operations to be reduced to oper-\nations on the eigenvalues.\n• The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of a\nmatrix.\n• The behavior of iterated matrix powers depends primarily on the size of the largest eigen-\nvalue. This understanding has many applications in the theory of neural network initial-\nization.\nA.2.9 Exercises\n1. What are the eigenvalues and eigenvectors of\nA =\n[2\n1\n1\n2\n]\n?\n(A.21)\n2. What are the eigenvalues and eigenvectors of the following matrix, and what is strange\nabout this example compared to the previous one?\nA =\n[2\n1\n0\n2\n]\n.\n(A.22)\n3. Without computing the eigenvalues, is it possible that the smallest eigenvalue of the fol-\n\n954\nMathematics for Deep Learning\n279\nlowing matrix is less that 0.5? Note: this problem can be done in your head.\nA =\n\n3.0\n0.1\n0.3\n1.0\n0.1\n1.0\n0.1\n0.2\n0.3\n0.1\n5.0\n0.0\n1.0\n0.2\n0.0\n1.8\n\n.\n(A.23)\nDiscussions279.\nA.3 Single Variable Calculus\nIn Section 2.4, we saw the basic elements of diﬀerential calculus. This section takes a deeper\ndive into the fundamentals of calculus and how we can understand and apply it in the context\nof machine learning.\nA.3.1 Diﬀerential Calculus\nDiﬀerential calculus is fundamentally the study of how functions behave under small changes.\nTo see why this is so core to deep learning, let’s consider an example.\nSuppose that we have a deep neural network where the weights are, for convenience, con-\ncatenated into a single vector w = (w1, . . ., wn). Given a training dataset, we consider the\nloss of our neural network on this dataset, which we will write as L(w).\nThis function is extraordinarily complex, encoding the performance of all possible models of\nthe given architecture on this dataset, so it is nearly impossible to tell what set of weights w\nwill minimize the loss. Thus, in practice, we often start by initializing our weights randomly,\nand then iteratively take small steps in the direction which makes the loss decrease as rapidly\nas possible.\nThe question then becomes something that on the surface is no easier: how do we ﬁnd the\ndirection which makes the weights decrease as quickly as possible? To dig into this, let’s ﬁrst\nexamine the case with only a single weight: L(w) = L(x) for a single real value x.\nLet’s take x and try to understand what happens when we change it by a small amount to\nx + ϵ. If you wish to be concrete, think a number like ϵ = 0.0000001. To help us visualize\nwhat happens, let’s graph an example function, f (x) = sin(xx), over the [0, 3].\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\n(continues on next page)\n\n955\nSingle Variable Calculus\n(continued from previous page)\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2\n# Define pi in torch\n# Plot a function in a normal range\nx_big = torch.arange(0.01, 3.01, 0.01)\nys = torch.sin(x_big**x_big)\nd2l.plot(x_big, ys, 'x', 'f(x)')\nAt this large scale, the function’s behavior is not simple. However, if we reduce our range to\nsomething smaller like [1.75, 2.25], we see that the graph becomes much simpler.\n# Plot a the same function in a tiny range\nx_med = torch.arange(1.75, 2.25, 0.001)\nys = torch.sin(x_med**x_med)\nd2l.plot(x_med, ys, 'x', 'f(x)')\nTaking this to an extreme, if we zoom into a tiny segment, the behavior becomes far simpler:\nit is just a straight line.\n# Plot a the same function in a tiny range\nx_small = torch.arange(2.0, 2.01, 0.0001)\nys = torch.sin(x_small**x_small)\nd2l.plot(x_small, ys, 'x', 'f(x)')\n\n956\nMathematics for Deep Learning\nThis is the key observation of single variable calculus: the behavior of familiar functions\ncan be modeled by a line in a small enough range. This means that for most functions, it is\nreasonable to expect that as we shift the x value of the function by a little bit, the output\nf (x) will also be shifted by a little bit. The only question we need to answer is, “How large\nis the change in the output compared to the change in the input? Is it half as large? Twice as\nlarge?”\nThus, we can consider the ratio of the change in the output of a function for a small change\nin the input of the function. We can write this formally as\nL(x + ϵ) −L(x)\n(x + ϵ) −x\n= L(x + ϵ) −L(x)\nϵ\n.\n(A.1)\nThis is already enough to start to play around with in code. For instance, suppose that we\nknow that L(x) = x2 + 1701(x −4)3, then we can see how large this value is at the point\nx = 4 as follows.\n# Define our function\ndef L(x):\nreturn x**2 + 1701*(x-4)**3\n# Print the difference divided by epsilon for several epsilon\nfor epsilon in [0.1, 0.001, 0.0001, 0.00001]:\nprint(f'epsilon = {epsilon:.5f} -> {(L(4+epsilon) - L(4)) / epsilon:.5f}')\nepsilon = 0.10000 -> 25.11000\nepsilon = 0.00100 -> 8.00270\nepsilon = 0.00010 -> 8.00012\nepsilon = 0.00001 -> 8.00001\nNow, if we are observant, we will notice that the output of this number is suspiciously close\nto 8. Indeed, if we decrease ϵ, we will see value becomes progressively closer to 8. Thus we\nmay conclude, correctly, that the value we seek (the degree a change in the input changes\nthe output) should be 8 at the point x = 4. The way that a mathematician encodes this fact\n\n957\nSingle Variable Calculus\nis\nlim\nϵ→0\nL(4 + ϵ) −L(4)\nϵ\n= 8.\n(A.2)\nAs a bit of a historical digression: in the ﬁrst few decades of neural network research, sci-\nentists used this algorithm (the method of ﬁnite diﬀerences) to evaluate how a loss function\nchanged under small perturbation: just change the weights and see how the loss changed.\nThis is computationally ineﬃcient, requiring two evaluations of the loss function to see how\na single change of one variable inﬂuenced the loss. If we tried to do this with even a paltry few\nthousand parameters, it would require several thousand evaluations of the network over the\nentire dataset! It was not solved until 1986 that the backpropagation algorithm introduced in\nRumelhart et al. (1988) provided a way to calculate how any change of the weights together\nwould change the loss in the same computation time as a single prediction of the network\nover the dataset.\nBack in our example, this value 8 is diﬀerent for diﬀerent values of x, so it makes sense to\ndeﬁne it as a function of x. More formally, this value dependent rate of change is referred to\nas the derivative which is written as\ndf\ndx (x) = lim\nϵ→0\nf (x + ϵ) −f (x)\nϵ\n.\n(A.3)\nDiﬀerent texts will use diﬀerent notations for the derivative. For instance, all of the below\nnotations indicate the same thing:\ndf\ndx = d\ndx f = f ′ = ∇x f = Dx f = fx.\n(A.4)\nMost authors will pick a single notation and stick with it, however even that is not guaranteed.\nIt is best to be familiar with all of these. We will use the notation df\ndx throughout this text,\nunless we want to take the derivative of a complex expression, in which case we will use d\ndx f\nto write expressions like\nd\ndx\n[\nx4 + cos\n( x2 + 1\n2x −1\n)]\n.\n(A.5)\nOftentimes, it is intuitively useful to unravel the deﬁnition of derivative (A.3) again to see\nhow a function changes when we make a small change of x:\ndf\ndx (x) = lim\nϵ→0\nf (x + ϵ) −f (x)\nϵ\n=⇒df\ndx (x) ≈f (x + ϵ) −f (x)\nϵ\n=⇒ϵ df\ndx (x) ≈f (x + ϵ) −f (x)\n=⇒f (x + ϵ) ≈f (x) + ϵ df\ndx (x).\n(A.6)\nThe last equation is worth explicitly calling out. It tells us that if you take any function and\nchange the input by a small amount, the output would change by that small amount scaled by\nthe derivative.\nIn this way, we can understand the derivative as the scaling factor that tells us how large of\nchange we get in the output from a change in the input.\n\n958\nMathematics for Deep Learning\nA.3.2 Rules of Calculus\nWe now turn to the task of understanding how to compute the derivative of an explicit func-\ntion. A full formal treatment of calculus would derive everything from ﬁrst principles. We\nwill not indulge in this temptation here, but rather provide an understanding of the common\nrules encountered.\nCommon Derivatives\nAs was seen in Section 2.4, when computing derivatives one can oftentimes use a series of\nrules to reduce the computation to a few core functions. We repeat them here for ease of\nreference.\n• Derivative of constants.\nd\ndx c = 0.\n• Derivative of linear functions.\nd\ndx (ax) = a.\n• Power rule.\nd\ndx xn = nxn−1.\n• Derivative of exponentials.\nd\ndx ex = ex.\n• Derivative of the logarithm.\nd\ndx log(x) = 1\nx .\nDerivative Rules\nIf every derivative needed to be separately computed and stored in a table, diﬀerential cal-\nculus would be near impossible. It is a gift of mathematics that we can generalize the above\nderivatives and compute more complex derivatives like ﬁnding the derivative of f (x) =\nlog (1 + (x −1)10). As was mentioned in Section 2.4, the key to doing so is to codify what\nhappens when we take functions and combine them in various ways, most importantly: sums,\nproducts, and compositions.\n• Sum rule.\nd\ndx (g(x) + h(x)) = dg\ndx (x) + dh\ndx (x).\n• Product rule.\nd\ndx (g(x) · h(x)) = g(x) dh\ndx (x) + dg\ndx (x)h(x).\n• Chain rule.\nd\ndx g(h(x)) = dg\ndh(h(x)) · dh\ndx (x).\nLet’s see how we may use (A.6) to understand these rules. For the sum rule, consider following\nchain of reasoning:\nf (x + ϵ) = g(x + ϵ) + h(x + ϵ)\n≈g(x) + ϵ dg\ndx (x) + h(x) + ϵ dh\ndx (x)\n= g(x) + h(x) + ϵ\n( dg\ndx (x) + dh\ndx (x)\n)\n= f (x) + ϵ\n( dg\ndx (x) + dh\ndx (x)\n)\n.\n(A.7)\n\n959\nSingle Variable Calculus\nBy comparing this result with the fact that f (x + ϵ) ≈f (x) + ϵ df\ndx (x), we see that df\ndx (x) =\ndg\ndx (x) + dh\ndx (x) as desired. The intuition here is: when we change the input x, g and h jointly\ncontribute to the change of the output by dg\ndx (x) and dh\ndx (x).\nThe product is more subtle, and will require a new observation about how to work with these\nexpressions. We will begin as before using (A.6):\nf (x + ϵ) = g(x + ϵ) · h(x + ϵ)\n≈\n(\ng(x) + ϵ dg\ndx (x)\n)\n·\n(\nh(x) + ϵ dh\ndx (x)\n)\n= g(x) · h(x) + ϵ\n(\ng(x)dh\ndx (x) + dg\ndx (x)h(x)\n)\n+ ϵ2 dg\ndx (x)dh\ndx (x)\n= f (x) + ϵ\n(\ng(x)dh\ndx (x) + dg\ndx (x)h(x)\n)\n+ ϵ2 dg\ndx (x)dh\ndx (x).\n(A.8)\nThis resembles the computation done above, and indeed we see our answer ( df\ndx (x) = g(x) dh\ndx (x)+\ndg\ndx (x)h(x)) sitting next to ϵ, but there is the issue of that term of size ϵ2. We will refer to\nthis as a higher-order term, since the power of ϵ2 is higher than the power of ϵ1. We will\nsee in a later section that we will sometimes want to keep track of these, however for now\nobserve that if ϵ = 0.0000001, then ϵ2 = 0.0000000000001, which is vastly smaller. As\nwe send ϵ →0, we may safely ignore the higher order terms. As a general convention in this\nappendix, we will use “≈” to denote that the two terms are equal up to higher order terms.\nHowever, if we wish to be more formal we may examine the diﬀerence quotient\nf (x + ϵ) −f (x)\nϵ\n= g(x)dh\ndx (x) + dg\ndx (x)h(x) + ϵ dg\ndx (x)dh\ndx (x),\n(A.9)\nand see that as we send ϵ →0, the right hand term goes to zero as well.\nFinally, with the chain rule, we can again progress as before using (A.6) and see that\nf (x + ϵ) = g(h(x + ϵ))\n≈g\n(\nh(x) + ϵ dh\ndx (x)\n)\n≈g(h(x)) + ϵ dh\ndx (x) dg\ndh(h(x))\n= f (x) + ϵ dg\ndh(h(x))dh\ndx (x),\n(A.10)\nwhere in the second line we view the function g as having its input (h(x)) shifted by the tiny\nquantity ϵ dh\ndx (x).\nThese rule provide us with a ﬂexible set of tools to compute essentially any expression desired.\n\n960\nMathematics for Deep Learning\nFor instance,\nd\ndx\n[\nlog (1 + (x −1)10)]\n= (1 + (x −1)10)−1 d\ndx\n[\n1 + (x −1)10]\n= (1 + (x −1)10)−1\n( d\ndx [1] + d\ndx [(x −1)10]\n)\n= (1 + (x −1)10)−1\n(\n0 + 10(x −1)9 d\ndx [x −1]\n)\n= 10 (1 + (x −1)10)−1 (x −1)9\n=\n10(x −1)9\n1 + (x −1)10 .\n(A.11)\nWhere each line has used the following rules:\n1. The chain rule and derivative of logarithm.\n2. The sum rule.\n3. The derivative of constants, chain rule, and power rule.\n4. The sum rule, derivative of linear functions, derivative of constants.\nTwo things should be clear after doing this example:\n1. Any function we can write down using sums, products, constants, powers, exponentials,\nand logarithms can have its derivate computed mechanically by following these rules.\n2. Having a human follow these rules can be tedious and error prone!\nThankfully, these two facts together hint towards a way forward: this is a perfect candidate for\nmechanization! Indeed backpropagation, which we will revisit later in this section, is exactly\nthat.\nLinear Approximation\nWhen working with derivatives, it is often useful to geometrically interpret the approximation\nused above. In particular, note that the equation\nf (x + ϵ) ≈f (x) + ϵ df\ndx (x),\n(A.12)\napproximates the value of f by a line which passes through the point (x, f (x)) and has slope\ndf\ndx (x). In this way we say that the derivative gives a linear approximation to the function f ,\nas illustrated below:\n# Compute sin\nxs = torch.arange(-torch.pi, torch.pi, 0.01)\nplots = [torch.sin(xs)]\n# Compute some linear approximations. Use d(sin(x))/dx = cos(x)\n(continues on next page)\n\n961\nSingle Variable Calculus\n(continued from previous page)\nfor x0 in [-1.5, 0.0, 2.0]:\nplots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *\ntorch.cos(torch.tensor(x0)))\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\nHigher Order Derivatives\nLet’s now do something that may on the surface seem strange. Take a function f and compute\nthe derivative df\ndx . This gives us the rate of change of f at any point.\nHowever, the derivative, df\ndx , can be viewed as a function itself, so nothing stops us from\ncomputing the derivative of df\ndx to get d2 f\ndx2 = df\ndx\n(\ndf\ndx\n)\n. We will call this the second derivative\nof f . This function is the rate of change of the rate of change of f , or in other words, how the\nrate of change is changing. We may apply the derivative any number of times to obtain what\nis called the n-th derivative. To keep the notation clean, we will denote the n-th derivative\nas\nf (n)(x) = dn f\ndxn =\n( d\ndx\n)n\nf .\n(A.13)\nLet’s try to understand why this is a useful notion. Below, we visualize f (2)(x), f (1)(x), and\nf (x).\nFirst, consider the case that the second derivative f (2)(x) is a positive constant. This means\nthat the slope of the ﬁrst derivative is positive. As a result, the ﬁrst derivative f (1)(x) may\nstart out negative, becomes zero at a point, and then becomes positive in the end. This tells\nus the slope of our original function f and therefore, the function f itself decreases, ﬂattens\nout, then increases. In other words, the function f curves up, and has a single minimum as is\nshown in Fig. A.1.\nSecond, if the second derivative is a negative constant, that means that the ﬁrst derivative is\ndecreasing. This implies the ﬁrst derivative may start out positive, becomes zero at a point,\n\n962\nMathematics for Deep Learning\nt\nFig. A.1\nIf we assume the second derivative is a positive constant, then the ﬁst derivative in\nincreasing, which implies the function itself has a minimum.\nand then becomes negative. Hence, the function f itself increases, ﬂattens out, then decreases.\nIn other words, the function f curves down, and has a single maximum as is shown in Fig.\nA.2.\nt\nFig. A.2\nIf we assume the second derivative is a negative constant, then the ﬁst derivative in\ndecreasing, which implies the function itself has a maximum.\nThird, if the second derivative is a always zero, then the ﬁrst derivative will never change—it\nis constant! This means that f increases (or decreases) at a ﬁxed rate, and f is itself a straight\nline as is shown in Fig. A.3.\nt\nFig. A.3\nIf we assume the second derivative is zero, then the ﬁst derivative is constant, which\nimplies the function itself is a straight line.\nTo summarize, the second derivative can be interpreted as describing the way that the function\nf curves. A positive second derivative leads to a upwards curve, while a negative second\nderivative means that f curves downwards, and a zero second derivative means that f does\nnot curve at all.\nLet’s take this one step further. Consider the function g(x) = ax2 + bx + c. We can then\n\n963\nSingle Variable Calculus\ncompute that\ndg\ndx (x) = 2ax + b\nd2g\ndx2 (x) = 2a.\n(A.14)\nIf we have some original function f (x) in mind, we may compute the ﬁrst two derivatives\nand ﬁnd the values for a, b, and c that make them match this computation. Similarly to the\nprevious section where we saw that the ﬁrst derivative gave the best approximation with a\nstraight line, this construction provides the best approximation by a quadratic. Let’s visualize\nthis for f (x) = sin(x).\n# Compute sin\nxs = torch.arange(-torch.pi, torch.pi, 0.01)\nplots = [torch.sin(xs)]\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0 in [-1.5, 0.0, 2.0]:\nplots.append(torch.sin(torch.tensor(x0)) + (xs - x0) *\ntorch.cos(torch.tensor(x0)) - (xs - x0)**2 *\ntorch.sin(torch.tensor(x0)) / 2)\nd2l.plot(xs, plots, 'x', 'f(x)', ylim=[-1.5, 1.5])\nWe will extend this idea to the idea of a Taylor series in the next section.\nTaylor Series\nThe Taylor series provides a method to approximate the function f (x) if we are given values\nfor the ﬁrst n derivatives at a point x0, i.e.,\n{\nf (x0), f (1)(x0), f (2)(x0), . . ., f (n)(x0)\n}\n. The\nidea will be to ﬁnd a degree n polynomial that matches all the given derivatives at x0.\nWe saw the case of n = 2 in the previous section and a little algebra shows this is\nf (x) ≈1\n2\nd2 f\ndx2 (x0)(x −x0)2 + df\ndx (x0)(x −x0) + f (x0).\n(A.15)\n\n964\nMathematics for Deep Learning\nAs we can see above, the denominator of 2 is there to cancel out the 2 we get when we\ntake two derivatives of x2, while the other terms are all zero. Same logic applies for the ﬁrst\nderivative and the value itself.\nIf we push the logic further to n = 3, we will conclude that\nf (x) ≈\nd3 f\ndx3 (x0)\n6\n(x −x0)3 +\nd2 f\ndx2 (x0)\n2\n(x −x0)2 + df\ndx (x0)(x −x0) + f (x0).\n(A.16)\nwhere the 6 = 3×2 = 3! comes from the constant we get in front if we take three derivatives\nof x3.\nFurthermore, we can get a degree n polynomial by\nPn(x) =\nn\n∑\ni=0\nf (i)(x0)\ni!\n(x −x0)i.\n(A.17)\nwhere the notation\nf (n)(x) = dn f\ndxn =\n( d\ndx\n)n\nf .\n(A.18)\nIndeed, Pn(x) can be viewed as the best n-th degree polynomial approximation to our func-\ntion f (x).\nWhile we are not going to dive all the way into the error of the above approximations, it\nis worth mentioning the inﬁnite limit. In this case, for well behaved functions (known as\nreal analytic functions) like cos(x) or ex, we can write out the inﬁnite number of terms and\napproximate the exactly same function\nf (x) =\n∞\n∑\nn=0\nf (n)(x0)\nn!\n(x −x0)n.\n(A.19)\nTake f (x) = ex as am example. Since ex is its own derivative, we know that f (n)(x) = ex.\nTherefore, ex can be reconstructed by taking the Taylor series at x0 = 0, i.e.,\nex =\n∞\n∑\nn=0\nxn\nn! = 1 + x + x2\n2 + x3\n6 + · · · .\n(A.20)\nLet’s see how this works in code and observe how increasing the degree of the Taylor ap-\nproximation brings us closer to the desired function ex.\n# Compute the exponential function\nxs = torch.arange(0, 3, 0.01)\nys = torch.exp(xs)\n# Compute a few Taylor series approximations\nP1 = 1 + xs\nP2 = 1 + xs + xs**2 / 2\nP5 = 1 + xs + xs**2 / 2 + xs**3 / 6 + xs**4 / 24 + xs**5 / 120\nd2l.plot(xs, [ys, P1, P2, P5], 'x', 'f(x)', legend=[\n\"Exponential\", \"Degree 1 Taylor Series\", \"Degree 2 Taylor Series\",\n\"Degree 5 Taylor Series\"])\n\n965\nSingle Variable Calculus\n280\nTaylor series have two primary applications:\n1. Theoretical applications: Often when we try to understand a too complex function, using\nTaylor series enables us to turn it into a polynomial that we can work with directly.\n2. Numerical applications: Some functions like ex or cos(x) are diﬃcult for machines to\ncompute. They can store tables of values at a ﬁxed precision (and this is often done), but\nit still leaves open questions like “What is the 1000-th digit of cos(1)?” Taylor series are\noften helpful to answer such questions.\nA.3.3 Summary\n• Derivatives can be used to express how functions change when we change the input by a\nsmall amount.\n• Elementary derivatives can be combined using derivative rules to create arbitrarily complex\nderivatives.\n• Derivatives can be iterated to get second or higher order derivatives. Each increase in order\nprovides more ﬁne grained information on the behavior of the function.\n• Using information in the derivatives of a single data example, we can approximate well\nbehaved functions by polynomials obtained from the Taylor series.\nA.3.4 Exercises\n1. What is the derivative of x3 −4x + 1?\n2. What is the derivative of log( 1\nx )?\n3. True or False: If f ′(x) = 0 then f has a maximum or minimum at x?\n4. Where is the minimum of f (x) = x log(x) for x ≥0 (where we assume that f takes the\nlimiting value of 0 at f (0))?\nDiscussions280.\n\n966\nMathematics for Deep Learning\nA.4 Multivariable Calculus\nNow that we have a fairly strong understanding of derivatives of a function of a single variable,\nlet’s return to our original question where we were considering a loss function of potentially\nbillions of weights.\nA.4.1 Higher-Dimensional Diﬀerentiation\nWhat Section A.3 tells us is that if we change a single one of these billions of weights leaving\nevery other one ﬁxed, we know what will happen! This is nothing more than a function of a\nsingle variable, so we can write\nL(w1 + ϵ1, w2, . . ., wN) ≈L(w1, w2, . . ., wN) + ϵ1\nd\ndw1\nL(w1, w2, . . ., wN).\n(A.1)\nWe will call the derivative in one variable while ﬁxing the other variables the partial derivative,\nand we will use the notation\n∂\n∂w1 for the derivative in (A.1).\nNow, let’s take this and change w2 a little bit to w2 + ϵ2:\nL(w1 + ϵ1, w2 + ϵ2, . . ., wN) ≈L(w1, w2 + ϵ2, . . ., wN) + ϵ1\n∂\n∂w1\nL(w1, w2 + ϵ2, . . ., wN + ϵN)\n≈L(w1, w2, . . ., wN)\n+ ϵ2\n∂\n∂w2\nL(w1, w2, . . ., wN)\n+ ϵ1\n∂\n∂w1\nL(w1, w2, . . ., wN)\n+ ϵ1ϵ2\n∂\n∂w2\n∂\n∂w1\nL(w1, w2, . . ., wN)\n≈L(w1, w2, . . ., wN)\n+ ϵ2\n∂\n∂w2\nL(w1, w2, . . ., wN)\n+ ϵ1\n∂\n∂w1\nL(w1, w2, . . ., wN).\n(A.2)\nWe have again used the idea that ϵ1ϵ2 is a higher order term that we can discard in the\nsame way we could discard ϵ2 in the previous section, along with what we saw in (A.1). By\ncontinuing in this manner, we may write that\nL(w1 + ϵ1, w2 + ϵ2, . . ., wN + ϵN) ≈L(w1, w2, . . ., wN) +\n∑\ni\nϵi\n∂\n∂wi\nL(w1, w2, . . ., wN).\n(A.3)\nThis may look like a mess, but we can make this more familiar by noting that the sum on the\n\n967\nMultivariable Calculus\nright looks exactly like a dot product, so if we let\nϵ = [ϵ1, . . ., ϵN]⊤and ∇xL =\n[ ∂L\n∂x1\n, . . ., ∂L\n∂xN\n]⊤\n,\n(A.4)\nthen\nL(w + ϵ) ≈L(w) + ϵ · ∇wL(w).\n(A.5)\nWe will call the vector ∇wL the gradient of L.\nEquation (A.5) is worth pondering for a moment. It has exactly the format that we encoun-\ntered in one dimension, just we have converted everything to vectors and dot products. It\nallows us to tell approximately how the function L will change given any perturbation to the\ninput. As we will see in the next section, this will provide us with an important tool in under-\nstanding geometrically how we can learn using information contained in the gradient.\nBut ﬁrst, let’s see this approximation at work with an example. Suppose that we are working\nwith the function\nf (x, y) = log(ex + ey) with gradient ∇f (x, y) =\n[\nex\nex + ey,\ney\nex + ey\n]\n.\n(A.6)\nIf we look at a point like (0, log(2)), we see that\nf (x, y) = log(3) with gradient ∇f (x, y) =\n[1\n3, 2\n3\n]\n.\n(A.7)\nThus, if we want to approximate f at (ϵ1, log(2)+ϵ2), we see that we should have the speciﬁc\ninstance of (A.5):\nf (ϵ1, log(2) + ϵ2) ≈log(3) + 1\n3ϵ1 + 2\n3ϵ2.\n(A.8)\nWe can test this in code to see how good the approximation is.\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch as d2l\ndef f(x, y):\nreturn torch.log(torch.exp(x) + torch.exp(y))\ndef grad_f(x, y):\nreturn torch.tensor([torch.exp(x) / (torch.exp(x) + torch.exp(y)),\ntorch.exp(y) / (torch.exp(x) + torch.exp(y))])\nepsilon = torch.tensor([0.01, -0.03])\ngrad_approx = f(torch.tensor([0.]), torch.log(\ntorch.tensor([2.]))) + epsilon.dot(\ngrad_f(torch.tensor([0.]), torch.log(torch.tensor(2.))))\ntrue_value = f(torch.tensor([0.]) + epsilon[0], torch.log(\ntorch.tensor([2.])) + epsilon[1])\nf'approximation: {grad_approx}, true Value: {true_value}'\n\n968\nMathematics for Deep Learning\n'approximation: tensor([1.0819]), true Value: tensor([1.0821])'\nA.4.2 Geometry of Gradients and Gradient Descent\nConsider the expression from (A.5) again:\nL(w + ϵ) ≈L(w) + ϵ · ∇wL(w).\n(A.9)\nLet’s suppose that I want to use this to help minimize our loss L. Let’s understand geometri-\ncally the algorithm of gradient descent ﬁrst described in Section 2.5. What we will do is the\nfollowing:\n1. Start with a random choice for the initial parameters w.\n2. Find the direction v that makes L decrease the most rapidly at w.\n3. Take a small step in that direction: w →w + ϵv.\n4. Repeat.\nThe only thing we do not know exactly how to do is to compute the vector v in the sec-\nond step. We will call such a direction the direction of steepest descent. Using the geometric\nunderstanding of dot products from Section A.1, we see that we can rewrite (A.5) as\nL(w + v) ≈L(w) + v · ∇wL(w) = L(w) + ∥∇wL(w)∥cos(θ).\n(A.10)\nNote that we have taken our direction to have length one for convenience, and used θ for the\nangle between v and ∇wL(w). If we want to ﬁnd the direction that decreases L as rapidly as\npossible, we want to make this expression as negative as possible. The only way the direction\nwe pick enters into this equation is through cos(θ), and thus we wish to make this cosine\nas negative as possible. Now, recalling the shape of cosine, we can make this as negative as\npossible by making cos(θ) = −1 or equivalently making the angle between the gradient and\nour chosen direction to be π radians, or equivalently 180 degrees. The only way to achieve\nthis is to head in the exact opposite direction: pick v to point in the exact opposite direction\nto ∇wL(w)!\nThis brings us to one of the most important mathematical concepts in machine learning: the\ndirection of steepest decent points in the direction of −∇wL(w). Thus our informal algorithm\ncan be rewritten as follows.\n1. Start with a random choice for the initial parameters w.\n2. Compute ∇wL(w).\n3. Take a small step in the opposite of that direction: w ←w −ϵ∇wL(w).\n4. Repeat.\nThis basic algorithm has been modiﬁed and adapted many ways by many researchers, but\nthe core concept remains the same in all of them. Use the gradient to ﬁnd the direction that\n\n969\nMultivariable Calculus\ndecreases the loss as rapidly as possible, and update the parameters to take a step in that\ndirection.\nA.4.3 A Note on Mathematical Optimization\nThroughout this book, we focus squarely on numerical optimization techniques for the prac-\ntical reason that all functions we encounter in the deep learning setting are too complex to\nminimize explicitly.\nHowever, it is a useful exercise to consider what the geometric understanding we obtained\nabove tells us about optimizing functions directly.\nSuppose that we wish to ﬁnd the value of x0 which minimizes some function L(x). Let’s\nsuppose that moreover someone gives us a value and tells us that it is the value that minimizes\nL. Is there anything we can check to see if their answer is even plausible?\nAgain consider (A.5):\nL(x0 + ϵ) ≈L(x0) + ϵ · ∇xL(x0).\n(A.11)\nIf the gradient is not zero, we know that we can take a step in the direction −ϵ∇xL(x0) to ﬁnd\na value of L that is smaller. Thus, if we truly are at a minimum, this cannot be the case! We\ncan conclude that if x0 is a minimum, then ∇xL(x0) = 0. We call points with ∇xL(x0) = 0\ncritical points.\nThis is nice, because in some rare settings, we can explicitly ﬁnd all the points where the\ngradient is zero, and ﬁnd the one with the smallest value.\nFor a concrete example, consider the function\nf (x) = 3x4 −4x3 −12x2.\n(A.12)\nThis function has derivative\ndf\ndx = 12x3 −12x2 −24x = 12x(x −2)(x + 1).\n(A.13)\nThe only possible location of minima are at x = −1, 0, 2, where the function takes the values\n−5, 0, −32 respectively, and thus we can conclude that we minimize our function when x = 2.\nA quick plot conﬁrms this.\nx = torch.arange(-2, 3, 0.01)\nf = (3 * x**4) - (4 * x**3) - (12 * x**2)\nd2l.plot(x, f, 'x', 'f(x)')\nThis highlights an important fact to know when working either theoretically or numerically:\nthe only possible points where we can minimize (or maximize) a function will have gradient\nequal to zero, however, not every point with gradient zero is the true global minimum (or\nmaximum).\n\n970\nMathematics for Deep Learning\nA.4.4 Multivariate Chain Rule\nLet’s suppose that we have a function of four variables (w, x, y, and z) which we can make\nby composing many terms:\nf (u, v) = (u + v)2\nu(a, b) = (a + b)2,\nv(a, b) = (a −b)2,\na(w, x, y, z) = (w + x + y + z)2,\nb(w, x, y, z) = (w + x −y −z)2.\n(A.14)\nSuch chains of equations are common when working with neural networks, so trying to un-\nderstand how to compute gradients of such functions is key. We can start to see visual hints\nof this connection in Fig. A.1 if we take a look at what variables directly relate to one an-\nother.\nt\nFig. A.1\nThe function relations above where nodes represent values and edges show functional\ndependence.\nNothing stops us from just composing everything from (A.14) and writing out that\nf (w, x, y, z) =\n(((w + x + y + z)2 + (w + x −y −z)2)2 + ((w + x + y + z)2 −(w + x −y −z)2)2)2\n.\n(A.15)\nWe may then take the derivative by just using single variable derivatives, but if we did that\nwe would quickly ﬁnd ourself swamped with terms, many of which are repeats! Indeed, one\n\n971\nMultivariable Calculus\ncan see that, for instance:\n∂f\n∂w = 2 (2 (2(w + x + y + z) −2(w + x −y −z)) ((w + x + y + z)2 −(w + x −y −z)2) +\n2 (2(w + x −y −z) + 2(w + x + y + z)) ((w + x −y −z)2 + (w + x + y + z)2)) ×\n(((w + x + y + z)2 −(w + x −y −z)2)2 + ((w + x −y −z)2 + (w + x + y + z)2)2)\n.\n(A.16)\nIf we then also wanted to compute ∂f\n∂x , we would end up with a similar equation again with\nmany repeated terms, and many shared repeated terms between the two derivatives. This\nrepresents a massive quantity of wasted work, and if we needed to compute derivatives this\nway, the whole deep learning revolution would have stalled out before it began!\nLet’s break up the problem. We will start by trying to understand how f changes when we\nchange a, essentially assuming that w, x, y, and z all do not exist. We will reason as we did\nback when we worked with the gradient for the ﬁrst time. Let’s take a and add a small amount\nϵ to it.\nf (u(a + ϵ, b), v(a + ϵ, b))\n≈f\n(\nu(a, b) + ϵ ∂u\n∂a(a, b), v(a, b) + ϵ ∂v\n∂a(a, b)\n)\n≈f (u(a, b), v(a, b)) + ϵ\n[ ∂f\n∂u (u(a, b), v(a, b))∂u\n∂a(a, b) + ∂f\n∂v (u(a, b), v(a, b)) ∂v\n∂a(a, b)\n]\n.\n(A.17)\nThe ﬁrst line follows from the deﬁnition of partial derivative, and the second follows from the\ndeﬁnition of gradient. It is notationally burdensome to track exactly where we evaluate every\nderivative, as in the expression ∂f\n∂u(u(a, b), v(a, b)), so we often abbreviate this to the much\nmore memorable\n∂f\n∂a = ∂f\n∂u\n∂u\n∂a + ∂f\n∂v\n∂v\n∂a .\n(A.18)\nIt is useful to think about the meaning of the process. We are trying to understand how a\nfunction of the form f (u(a, b), v(a, b)) changes its value with a change in a. There are two\npathways this can occur: there is the pathway where a →u →f and where a →v →f . We\ncan compute both of these contributions via the chain rule: ∂w\n∂u · ∂u\n∂x and ∂w\n∂v · ∂v\n∂x respectively,\nand added up.\nImagine we have a diﬀerent network of functions where the functions on the right depend on\nthose that are connected to on the left as is shown in Fig. A.2.\nt\nFig. A.2\nAnother more subtle example of the chain rule.\nTo compute something like ∂f\n∂y , we need to sum over all (in this case 3) paths from y to f\n\n972\nMathematics for Deep Learning\ngiving\n∂f\n∂y = ∂f\n∂a\n∂a\n∂u\n∂u\n∂y + ∂f\n∂u\n∂u\n∂y + ∂f\n∂b\n∂b\n∂v\n∂v\n∂y .\n(A.19)\nUnderstanding the chain rule in this way will pay great dividends when trying to understand\nhow gradients ﬂow through networks, and why various architectural choices like those in\nLSTMs (Section 10.1) or residual layers (Section 8.6) can help shape the learning process by\ncontrolling gradient ﬂow.\nA.4.5 The Backpropagation Algorithm\nLet’s return to the example of (A.14) the previous section where\nf (u, v) = (u + v)2\nu(a, b) = (a + b)2,\nv(a, b) = (a −b)2,\na(w, x, y, z) = (w + x + y + z)2,\nb(w, x, y, z) = (w + x −y −z)2.\n(A.20)\nIf we want to compute say ∂f\n∂w we may apply the multi-variate chain rule to see:\n∂f\n∂w = ∂f\n∂u\n∂u\n∂w + ∂f\n∂v\n∂v\n∂w,\n∂u\n∂w = ∂u\n∂a\n∂a\n∂w + ∂u\n∂b\n∂b\n∂w,\n∂v\n∂w = ∂v\n∂a\n∂a\n∂w + ∂v\n∂b\n∂b\n∂w .\n(A.21)\nLet’s try using this decomposition to compute ∂f\n∂w . Notice that all we need here are the various\nsingle step partials:\n∂f\n∂u = 2(u + v),\n∂f\n∂v = 2(u + v),\n∂u\n∂a = 2(a + b),\n∂u\n∂b = 2(a + b),\n∂v\n∂a = 2(a −b),\n∂v\n∂b = −2(a −b),\n∂a\n∂w = 2(w + x + y + z),\n∂b\n∂w = 2(w + x −y −z).\n(A.22)\nIf we write this out into code this becomes a fairly manageable expression.\n# Compute the value of the function from inputs to outputs\nw, x, y, z = -1, 0, -2, 1\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\nprint(f'\nf at {w}, {x}, {y}, {z} is {f}')\n# Compute the single step partials\ndf_du, df_dv = 2*(u + v), 2*(u + v)\ndu_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\n(continues on next page)\n\n973\nMultivariable Calculus\n(continued from previous page)\nda_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\n# Compute the final result from inputs to outputs\ndu_dw, dv_dw = du_da*da_dw + du_db*db_dw, dv_da*da_dw + dv_db*db_dw\ndf_dw = df_du*du_dw + df_dv*dv_dw\nprint(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\nf at -1, 0, -2, 1 is 1024\ndf/dw at -1, 0, -2, 1 is -4096\nHowever, note that this still does not make it easy to compute something like ∂f\n∂x . The reason\nfor that is the way we chose to apply the chain rule. If we look at what we did above, we\nalways kept ∂w in the denominator when we could. In this way, we chose to apply the chain\nrule seeing how w changed every other variable. If that is what we wanted, this would be a\ngood idea. However, think back to our motivation from deep learning: we want to see how\nevery parameter changes the loss. In essence, we want to apply the chain rule keeping ∂f in\nthe numerator whenever we can!\nTo be more explicit, note that we can write\n∂f\n∂w = ∂f\n∂a\n∂a\n∂w + ∂f\n∂b\n∂b\n∂w,\n∂f\n∂a = ∂f\n∂u\n∂u\n∂a + ∂f\n∂v\n∂v\n∂a,\n∂f\n∂b = ∂f\n∂u\n∂u\n∂b + ∂f\n∂v\n∂v\n∂b.\n(A.23)\nNote that this application of the chain rule has us explicitly compute ∂f\n∂u, ∂f\n∂v, ∂f\n∂a, ∂f\n∂b, and ∂f\n∂w .\nNothing stops us from also including the equations:\n∂f\n∂x = ∂f\n∂a\n∂a\n∂x + ∂f\n∂b\n∂b\n∂x,\n∂f\n∂y = ∂f\n∂a\n∂a\n∂y + ∂f\n∂b\n∂b\n∂y,\n∂f\n∂z = ∂f\n∂a\n∂a\n∂z + ∂f\n∂b\n∂b\n∂z .\n(A.24)\nand then keeping track of how f changes when we change any node in the entire network.\nLet’s implement it.\n# Compute the value of the function from inputs to outputs\nw, x, y, z = -1, 0, -2, 1\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\nprint(f'f at {w}, {x}, {y}, {z} is {f}')\n# Compute the derivative using the decomposition above\n# First compute the single step partials\n(continues on next page)\n\n974\nMathematics for Deep Learning\n(continued from previous page)\ndf_du, df_dv = 2*(u + v), 2*(u + v)\ndu_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\nda_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\nda_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)\nda_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)\nda_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)\n# Now compute how f changes when we change any value from output to input\ndf_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db\ndf_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx\ndf_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz\nprint(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\nprint(f'df/dx at {w}, {x}, {y}, {z} is {df_dx}')\nprint(f'df/dy at {w}, {x}, {y}, {z} is {df_dy}')\nprint(f'df/dz at {w}, {x}, {y}, {z} is {df_dz}')\nf at -1, 0, -2, 1 is 1024\ndf/dw at -1, 0, -2, 1 is -4096\ndf/dx at -1, 0, -2, 1 is -4096\ndf/dy at -1, 0, -2, 1 is -4096\ndf/dz at -1, 0, -2, 1 is -4096\nThe fact that we compute derivatives from f back towards the inputs rather than from the\ninputs forward to the outputs (as we did in the ﬁrst code snippet above) is what gives this\nalgorithm its name: backpropagation. Note that there are two steps: 1. Compute the value of\nthe function, and the single step partials from front to back. While not done above, this can\nbe combined into a single forward pass. 2. Compute the gradient of f from back to front.\nWe call this the backwards pass.\nThis is precisely what every deep learning algorithm implements to allow the computation\nof the gradient of the loss with respect to every weight in the network at one pass. It is an\nastonishing fact that we have such a decomposition.\nTo see how to encapsulated this, let’s take a quick look at this example.\n# Initialize as ndarrays, then attach gradients\nw = torch.tensor([-1.], requires_grad=True)\nx = torch.tensor([0.], requires_grad=True)\ny = torch.tensor([-2.], requires_grad=True)\nz = torch.tensor([1.], requires_grad=True)\n# Do the computation like usual, tracking gradients\na, b = (w + x + y + z)**2, (w + x - y - z)**2\nu, v = (a + b)**2, (a - b)**2\nf = (u + v)**2\n# Execute backward pass\nf.backward()\nprint(f'df/dw at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\nf'{z.data.item()} is {w.grad.data.item()}')\n(continues on next page)\n\n975\nMultivariable Calculus\n(continued from previous page)\nprint(f'df/dx at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\nf'{z.data.item()} is {x.grad.data.item()}')\nprint(f'df/dy at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\nf'{z.data.item()} is {y.grad.data.item()}')\nprint(f'df/dz at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\nf'{z.data.item()} is {z.grad.data.item()}')\ndf/dw at -1.0, 0.0, -2.0, 1.0 is -4096.0\ndf/dx at -1.0, 0.0, -2.0, 1.0 is -4096.0\ndf/dy at -1.0, 0.0, -2.0, 1.0 is -4096.0\ndf/dz at -1.0, 0.0, -2.0, 1.0 is -4096.0\nAll of what we did above can be done automatically by calling f.backwards().\nA.4.6 Hessians\nAs with single variable calculus, it is useful to consider higher-order derivatives in order to get\na handle on how we can obtain a better approximation to a function than using the gradient\nalone.\nThere is one immediate problem one encounters when working with higher order derivatives\nof functions of several variables, and that is there are a large number of them. If we have a\nfunction f (x1, . . ., xn) of n variables, then we can take n2 many second derivatives, namely\nfor any choice of i and j:\nd2 f\ndxidxj\n=\nd\ndxi\n( d\ndxj\nf\n)\n.\n(A.25)\nThis is traditionally assembled into a matrix called the Hessian:\nHf =\n\nd2 f\ndx1dx1\n· · ·\nd2 f\ndx1dxn\n...\n...\n...\nd2 f\ndxndx1\n· · ·\nd2 f\ndxndxn\n\n.\n(A.26)\nNot every entry of this matrix is independent. Indeed, we can show that as long as both mixed\npartials (partial derivatives with respect to more than one variable) exist and are continuous,\nwe can say that for any i, and j,\nd2 f\ndxidxj\n=\nd2 f\ndxjdxi\n.\n(A.27)\nThis follows by considering ﬁrst perturbing a function in the direction of xi, and then per-\nturbing it in xj and then comparing the result of that with what happens if we perturb ﬁrst\nxj and then xi, with the knowledge that both of these orders lead to the same ﬁnal change in\nthe output of f .\nAs with single variables, we can use these derivatives to get a far better idea of how the\n\n976\nMathematics for Deep Learning\nfunction behaves near a point. In particular, we can use it to ﬁnd the best ﬁtting quadratic\nnear a point x0, as we saw in a single variable.\nLet’s see an example. Suppose that f (x1, x2) = a+b1x1 +b2x2 +c11x2\n1 +c12x1x2 +c22x2\n2.\nThis is the general form for a quadratic in two variables. If we look at the value of the function,\nits gradient, and its Hessian (A.26), all at the point zero:\nf (0, 0) = a,\n∇f (0, 0) =\n[b1\nb2\n]\n,\nH f (0, 0) =\n[2c11\nc12\nc12\n2c22\n]\n,\n(A.28)\nwe can get our original polynomial back by saying\nf (x) = f (0) + ∇f (0) · x + 1\n2x⊤H f (0)x.\n(A.29)\nIn general, if we computed this expansion any point x0, we see that\nf (x) = f (x0) + ∇f (x0) · (x −x0) + 1\n2(x −x0)⊤H f (x0)(x −x0).\n(A.30)\nThis works for any dimensional input, and provides the best approximating quadratic to any\nfunction at a point. To give an example, let’s plot the function\nf (x, y) = xe−x2−y2.\n(A.31)\nOne can compute that the gradient and Hessian are\n∇f (x, y) = e−x2−y2 (1 −2x2\n−2xy\n)\nand H f (x, y) = e−x2−y2 ( 4x3 −6x\n4x2y −2y\n4x2y −2y\n4xy2 −2x\n)\n.\n(A.32)\nAnd thus, with a little algebra, see that the approximating quadratic at [−1, 0]⊤is\nf (x, y) ≈e−1 (−1 −(x + 1) + (x + 1)2 + y2) .\n(A.33)\n# Construct grid and compute function\nx, y = torch.meshgrid(torch.linspace(-2, 2, 101),\ntorch.linspace(-2, 2, 101))\nz = x*torch.exp(- x**2 - y**2)\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw = torch.exp(torch.tensor([-1.]))*(-1 - (x + 1) + 2 * (x + 1)**2 + 2 * y**2)\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x.numpy(), y.numpy(), z.numpy(),\n**{'rstride': 10, 'cstride': 10})\nax.plot_wireframe(x.numpy(), y.numpy(), w.numpy(),\n**{'rstride': 10, 'cstride': 10}, color='purple')\n(continues on next page)\n\n977\nMultivariable Calculus\n(continued from previous page)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(-1, 1)\nax.dist = 12\nThis forms the basis for Newton’s Algorithm discussed in Section 12.3, where we perform\nnumerical optimization iteratively ﬁnding the best ﬁtting quadratic, and then exactly mini-\nmizing that quadratic.\nA.4.7 A Little Matrix Calculus\nDerivatives of functions involving matrices turn out to be particularly nice. This section can\nbecome notationally heavy, so may be skipped in a ﬁrst reading, but it is useful to know how\nderivatives of functions involving common matrix operations are often much cleaner than\none might initially anticipate, particularly given how central matrix operations are to deep\nlearning applications.\nLet’s begin with an example. Suppose that we have some ﬁxed column vector β, and we want\nto take the product function f (x) = β⊤x, and understand how the dot product changes when\nwe change x.\nA bit of notation that will be useful when working with matrix derivatives in ML is called\nthe denominator layout matrix derivative where we assemble our partial derivatives into the\nshape of whatever vector, matrix, or tensor is in the denominator of the diﬀerential. In this\ncase, we will write\ndf\ndx =\n\ndf\ndx1...\ndf\ndxn\n\n,\n(A.34)\nwhere we matched the shape of the column vector x.\n\n978\nMathematics for Deep Learning\nIf we write out our function into components this is\nf (x) =\nn\n∑\ni=1\nβixi = β1x1 + · · · + βnxn.\n(A.35)\nIf we now take the partial derivative with respect to say β1, note that everything is zero but\nthe ﬁrst term, which is just x1 multiplied by β1, so we obtain that\ndf\ndx1\n= β1,\n(A.36)\nor more generally that\ndf\ndxi\n= βi.\n(A.37)\nWe can now reassemble this into a matrix to see\ndf\ndx =\n\ndf\ndx1...\ndf\ndxn\n\n=\n\nβ1\n...\nβn\n\n= β.\n(A.38)\nThis illustrates a few factors about matrix calculus that we will often counter throughout this\nsection:\n• First, The computations will get rather involved.\n• Second, The ﬁnal results are much cleaner than the intermediate process, and will al-\nways look similar to the single variable case. In this case, note that\nd\ndx (bx) = b and\nd\ndx(β⊤x) = β are both similar.\n• Third, transposes can often appear seemingly from nowhere. The core reason for this is the\nconvention that we match the shape of the denominator, thus when we multiply matrices,\nwe will need to take transposes to match back to the shape of the original term.\nTo keep building intuition, let’s try a computation that is a little harder. Suppose that we have\na column vector x, and a square matrix A and we want to compute\nd\ndx(x⊤Ax).\n(A.39)\nTo drive towards easier to manipulate notation, let’s consider this problem using Einstein\nnotation. In this case we can write the function as\nx⊤Ax = xiaij xj.\n(A.40)\nTo compute our derivative, we need to understand for every k, what is the value of\nd\ndxk\n(x⊤Ax) =\nd\ndxk\nxiaij xj.\n(A.41)\nBy the product rule, this is\nd\ndxk\nxiaij xj = dxi\ndxk\naij xj + xiaij\ndxj\ndxk\n.\n(A.42)\n\n979\nMultivariable Calculus\nFor a term like dxi\ndxk , it is not hard to see that this is one when i = k and zero otherwise. This\nmeans that every term where i and k are diﬀerent vanish from this sum, so the only terms that\nremain in that ﬁrst sum are the ones where i = k. The same reasoning holds for the second\nterm where we need j = k. This gives\nd\ndxk\nxiaij xj = ak j xj + xiaik.\n(A.43)\nNow, the names of the indices in Einstein notation are arbitrary—the fact that i and j are\ndiﬀerent is immaterial to this computation at this point, so we can re-index so that they both\nuse i to see that\nd\ndxk\nxiaij xj = akixi + xiaik = (aki + aik)xi.\n(A.44)\nNow, here is where we start to need some practice to go further. Let’s try and identify this\noutcome in terms of matrix operations. aki + aik is the k, i-th component of A + A⊤. This\ngives\nd\ndxk\nxiaij xj = [A + A⊤]kixi.\n(A.45)\nSimilarly, this term is now the product of the matrix A + A⊤by the vector x, so we see\nthat\n[ d\ndx(x⊤Ax)\n]\nk\n=\nd\ndxk\nxiaij xj = [(A + A⊤)x]k.\n(A.46)\nThus, we see that the k-th entry of the desired derivative from (A.39) is just the k-th entry\nof the vector on the right, and thus the two are the same. Thus yields\nd\ndx(x⊤Ax) = (A + A⊤)x.\n(A.47)\nThis required signiﬁcantly more work than our last one, but the ﬁnal result is small. More than\nthat, consider the following computation for traditional single variable derivatives:\nd\ndx (xax) = dx\ndx ax + xa dx\ndx = (a + a)x.\n(A.48)\nEquivalently\nd\ndx (ax2) = 2ax = (a + a)x. Again, we get a result that looks rather like the\nsingle variable result but with a transpose tossed in.\nAt this point, the pattern should be looking rather suspicious, so let’s try to ﬁgure out why.\nWhen we take matrix derivatives like this, let’s ﬁrst assume that the expression we get will\nbe another matrix expression: an expression we can write it in terms of products and sums\nof matrices and their transposes. If such an expression exists, it will need to be true for all\nmatrices. In particular, it will need to be true of 1 × 1 matrices, in which case the matrix\nproduct is just the product of the numbers, the matrix sum is just the sum, and the transpose\ndoes nothing at all! In other words, whatever expression we get must match the single variable\nexpression. This means that, with some practice, one can often guess matrix derivatives just\nby knowing what the associated single variable expression must look like!\n\n980\nMathematics for Deep Learning\nLet’s try this out. Suppose that X is a n × m matrix, U is an n × r and V is an r × m. Let’s\ntry to compute\nd\ndV ∥X −UV∥2\n2 = ?\n(A.49)\nThis computation is important in an area called matrix factorization. For us, however, it is\njust a derivative to compute. Let’s try to imagine what this would be for 1 × 1 matrices. In\nthat case, we get the expression\nd\ndv (x −uv)2 = −2(x −uv)u,\n(A.50)\nwhere, the derivative is rather standard. If we try to convert this back into a matrix expression\nwe get\nd\ndV ∥X −UV∥2\n2 = −2(X −UV)U.\n(A.51)\nHowever, if we look at this it does not quite work. Recall that X is n × m, as is UV, so the\nmatrix 2(X −UV) is n × m. On the other hand U is n × r, and we cannot multiply a n × m\nand a n × r matrix since the dimensions do not match!\nWe want to get\nd\ndV, which is the same shape as V, which is r × m. So somehow we need to\ntake a n×m matrix and a n×r matrix, multiply them together (perhaps with some transposes)\nto get a r × m. We can do this by multiplying U⊤by (X −UV). Thus, we can guess the\nsolution to (A.49) is\nd\ndV ∥X −UV∥2\n2 = −2U⊤(X −UV).\n(A.52)\nTo show that this works, we would be remiss to not provide a detailed computation. If we\nalready believe that this rule-of-thumb works, feel free to skip past this derivation. To com-\npute\nd\ndV ∥X −UV∥2\n2,\n(A.53)\nwe must ﬁnd for every a, and b\nd\ndvab\n∥X −UV∥2\n2 =\nd\ndvab\n∑\ni, j\n(\nxij −\n∑\nk\nuikvk j\n)2\n.\n(A.54)\nRecalling that all entries of X and U are constants as far as\nd\ndvab is concerned, we may push\nthe derivative inside the sum, and apply the chain rule to the square to get\nd\ndvab\n∥X −UV∥2\n2 =\n∑\ni,j\n2\n(\nxij −\n∑\nk\nuikvk j\n) (\n−\n∑\nk\nuik\ndvk j\ndvab\n)\n.\n(A.55)\nAs in the previous derivation, we may note that dvk j\ndvab is only non-zero if the k = a and\nj = b. If either of those conditions do not hold, the term in the sum is zero, and we may\nfreely discard it. We see that\nd\ndvab\n∥X −UV∥2\n2 = −2\n∑\ni\n(\nxib −\n∑\nk\nuikvkb\n)\nuia.\n(A.56)\n\n981\nMultivariable Calculus\nAn important subtlety here is that the requirement that k = a does not occur inside the inner\nsum since that k is a dummy variable which we are summing over inside the inner term. For\na notationally cleaner example, consider why\nd\ndx1\n(∑\ni\nxi\n)2\n= 2\n(∑\ni\nxi\n)\n.\n(A.57)\nFrom this point, we may start identifying components of the sum. First,\n∑\nk\nuikvkb = [UV]ib.\n(A.58)\nSo the entire expression in the inside of the sum is\nxib −\n∑\nk\nuikvkb = [X −UV]ib.\n(A.59)\nThis means we may now write our derivative as\nd\ndvab\n∥X −UV∥2\n2 = −2\n∑\ni\n[X −UV]ibuia.\n(A.60)\nWe want this to look like the a, b element of a matrix so we can use the technique as in the\nprevious example to arrive at a matrix expression, which means that we need to exchange the\norder of the indices on uia. If we notice that uia = [U⊤]ai, we can then write\nd\ndvab\n∥X −UV∥2\n2 = −2\n∑\ni\n[U⊤]ai[X −UV]ib.\n(A.61)\nThis is a matrix product, and thus we can conclude that\nd\ndvab\n∥X −UV∥2\n2 = −2[U⊤(X −UV)]ab.\n(A.62)\nand thus we may write the solution to (A.49)\nd\ndV ∥X −UV∥2\n2 = −2U⊤(X −UV).\n(A.63)\nThis matches the solution we guessed above!\nIt is reasonable to ask at this point, “Why can I not just write down matrix versions of all\nthe calculus rules I have learned? It is clear this is still mechanical. Why do we not just get it\nover with!” And indeed there are such rules and (Petersen and Pedersen, 2008) provides an\nexcellent summary. However, due to the plethora of ways matrix operations can be combined\ncompared to single values, there are many more matrix derivative rules than single variable\nones. It is often the case that it is best to work with the indices, or leave it up to automatic\ndiﬀerentiation when appropriate.\nA.4.8 Summary\n• In higher dimensions, we can deﬁne gradients which serve the same purpose as derivatives\nin one dimension. These allow us to see how a multi-variable function changes when we\nmake an arbitrary small change to the inputs.\n\n982\nMathematics for Deep Learning\n281\n• The backpropagation algorithm can be seen to be a method of organizing the multi-variable\nchain rule to allow for the eﬃcient computation of many partial derivatives.\n• Matrix calculus allows us to write the derivatives of matrix expressions in concise ways.\nA.4.9 Exercises\n1. Given a column vector β, compute the derivatives of both f (x) = β⊤x and g(x) = x⊤β.\nWhy do you get the same answer?\n2. Let v be an n dimension vector. What is\n∂\n∂v ∥v∥2?\n3. Let L(x, y) = log(ex + ey). Compute the gradient. What is the sum of the components\nof the gradient?\n4. Let f (x, y) = x2y + xy2. Show that the only critical point is (0, 0). By considering\nf (x, x), determine if (0, 0) is a maximum, minimum, or neither.\n5. Suppose that we are minimizing a function f (x) = g(x) + h(x). How can we geomet-\nrically interpret the condition of ∇f = 0 in terms of g and h?\nDiscussions281.\nA.5 Integral Calculus\nDiﬀerentiation only makes up half of the content of a traditional calculus education. The\nother pillar, integration, starts out seeming a rather disjoint question, “What is the area un-\nderneath this curve?” While seemingly unrelated, integration is tightly intertwined with the\ndiﬀerentiation via what is known as the fundamental theorem of calculus.\nAt the level of machine learning we discuss in this book, we will not need a deep understand-\ning of integration. However, we will provide a brief introduction to lay the groundwork for\nany further applications we will encounter later on.\nA.5.1 Geometric Interpretation\nSuppose that we have a function f (x). For simplicity, let’s assume that f (x) is non-negative\n(never takes a value less than zero). What we want to try and understand is: what is the area\ncontained between f (x) and the x-axis?\n\n983\nIntegral Calculus\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch as d2l\nx = torch.arange(-2, 2, 0.01)\nf = torch.exp(-x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist(), f.tolist())\nd2l.plt.show()\nIn most cases, this area will be inﬁnite or undeﬁned (consider the area under f (x) = x2), so\npeople will often talk about the area between a pair of ends, say a and b.\nx = torch.arange(-2, 2, 0.01)\nf = torch.exp(-x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color='black')\nd2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250])\nd2l.plt.show()\n\n984\nMathematics for Deep Learning\nWe will denote this area by the integral symbol below:\nArea(A) =\n∫b\na\nf (x) dx.\n(A.1)\nThe inner variable is a dummy variable, much like the index of a sum in a ∑, and so this can\nbe equivalently written with any inner value we like:\n∫b\na\nf (x) dx =\n∫b\na\nf (z) dz.\n(A.2)\nThere is a traditional way to try and understand how we might try to approximate such inte-\ngrals: we can imagine taking the region in-between a and b and chopping it into N vertical\nslices. If N is large, we can approximate the area of each slice by a rectangle, and then add\nup the areas to get the total area under the curve. Let’s take a look at an example doing this\nin code. We will see how to get the true value in a later section.\nepsilon = 0.05\na = 0\nb = 2\nx = torch.arange(a, b, epsilon)\nf = x / (1 + x**2)\napprox = torch.sum(epsilon*f)\ntrue = torch.log(torch.tensor([5.])) / 2\nd2l.set_figsize()\nd2l.plt.bar(x, f, width=epsilon, align='edge')\nd2l.plt.plot(x, f, color='black')\nd2l.plt.ylim([0, 1])\nd2l.plt.show()\nf'approximation: {approx}, truth: {true}'\n'approximation: 0.7944855690002441, truth: tensor([0.8047])'\nThe issue is that while it can be done numerically, we can do this approach analytically for\n\n985\nIntegral Calculus\nonly the simplest functions like\n∫b\na\nx dx.\n(A.3)\nAnything somewhat more complex like our example from the code above\n∫b\na\nx\n1 + x2 dx.\n(A.4)\nis beyond what we can solve with such a direct method.\nWe will instead take a diﬀerent approach. We will work intuitively with the notion of the\narea, and learn the main computational tool used to ﬁnd integrals: the fundamental theorem\nof calculus. This will be the basis for our study of integration.\nA.5.2 The Fundamental Theorem of Calculus\nTo dive deeper into the theory of integration, let’s introduce a function\nF(x) =\n∫x\n0\nf (y)dy.\n(A.5)\nThis function measures the area between 0 and x depending on how we change x. Notice that\nthis is everything we need since\n∫b\na\nf (x) dx = F(b) −F(a).\n(A.6)\nThis is a mathematical encoding of the fact that we can measure the area out to the far end-\npoint and then subtract oﬀthe area to the near end point as indicated in Fig. A.1.\nt\nFig. A.1\nVisualizing why we may reduce the problem of computing the area under a curve between\ntwo points to computing the area to the left of a point.\nThus, we can ﬁgure out what the integral over any interval is by ﬁguring out what F(x)\nis.\nTo do so, let’s consider an experiment. As we often do in calculus, let’s imagine what happens\nwhen we shift the value by a tiny bit. From the comment above, we know that\nF(x + ϵ) −F(x) =\n∫x+ϵ\nx\nf (y) dy.\n(A.7)\nThis tells us that the function changes by the area under a tiny sliver of a function.\nThis is the point at which we make an approximation. If we look at a tiny sliver of area like\nthis, it looks like this area is close to the rectangular area with height the value of f (x) and\n\n986\nMathematics for Deep Learning\nthe base width ϵ. Indeed, one can show that as ϵ →0 this approximation becomes better and\nbetter. Thus we can conclude:\nF(x + ϵ) −F(x) ≈ϵ f (x).\n(A.8)\nHowever, we can now notice: this is exactly the pattern we expect if we were computing the\nderivative of F! Thus we see the following rather surprising fact:\ndF\ndx (x) = f (x).\n(A.9)\nThis is the fundamental theorem of calculus. We may write it in expanded form as\nd\ndx\n∫x\n0\nf (y) dy = f (x).\n(A.10)\nIt takes the concept of ﬁnding areas (a priori rather hard), and reduces it to a statement\nderivatives (something much more completely understood). One last comment that we must\nmake is that this does not tell us exactly what F(x) is. Indeed F(x) + C for any C has the\nsame derivative. This is a fact-of-life in the theory of integration. Thankfully, notice that\nwhen working with deﬁnite integrals, the constants drop out, and thus are irrelevant to the\noutcome.\n∫b\na\nf (x) dx = (F(b) + C) −(F(a) + C) = F(b) −F(a).\n(A.11)\nThis may seem like abstract non-sense, but let’s take a moment to appreciate that it has given\nus a whole new perspective on computing integrals. Our goal is no-longer to do some sort\nof chop-and-sum process to try and recover the area, rather we need only ﬁnd a function\nwhose derivative is the function we have! This is incredible since we can now list many rather\ndiﬃcult integrals by just reversing the table from Section A.3.2. For instance, we know that the\nderivative of xn is nxn−1. Thus, we can say using the fundamental theorem (A.10) that\n∫x\n0\nnyn−1 dy = xn −0n = xn.\n(A.12)\nSimilarly, we know that the derivative of ex is itself, so that means\n∫x\n0\nex dx = ex −e0 = ex −1.\n(A.13)\nIn this way, we can develop the entire theory of integration leveraging ideas from diﬀerential\ncalculus freely. Every integration rule derives from this one fact.\nA.5.3 Change of Variables\nJust as with diﬀerentiation, there are a number of rules which make the computation of in-\ntegrals more tractable. In fact, every rule of diﬀerential calculus (like the product rule, sum\nrule, and chain rule) has a corresponding rule for integral calculus (integration by parts, lin-\nearity of integration, and the change of variables formula respectively). In this section, we\nwill dive into what is arguably the most important from the list: the change of variables for-\nmula.\n\n987\nIntegral Calculus\nFirst, suppose that we have a function which is itself an integral:\nF(x) =\n∫x\n0\nf (y) dy.\n(A.14)\nLet’s suppose that we want to know how this function looks when we compose it with another\nto obtain F(u(x)). By the chain rule, we know\nd\ndx F(u(x)) = dF\ndu (u(x)) · du\ndx .\n(A.15)\nWe can turn this into a statement about integration by using the fundamental theorem (A.10)\nas above. This gives\nF(u(x)) −F(u(0)) =\n∫x\n0\ndF\ndu (u(y)) · du\ndy dy.\n(A.16)\nRecalling that F is itself an integral gives that the left hand side may be rewritten to be\n∫u(x)\nu(0)\nf (y) dy =\n∫x\n0\ndF\ndu (u(y)) · du\ndy dy.\n(A.17)\nSimilarly, recalling that F is an integral allows us to recognize that dF\ndx = f using the funda-\nmental theorem (A.10), and thus we may conclude\n∫u(x)\nu(0)\nf (y) dy =\n∫x\n0\nf (u(y)) · du\ndy dy.\n(A.18)\nThis is the change of variables formula.\nFor a more intuitive derivation, consider what happens when we take an integral of f (u(x))\nbetween x and x + ϵ. For a small ϵ, this integral is approximately ϵ f (u(x)), the area of the\nassociated rectangle. Now, let’s compare this with the integral of f (y) from u(x) to u(x+ϵ).\nWe know that u(x + ϵ) ≈u(x) + ϵ du\ndx (x), so the area of this rectangle is approximately\nϵ du\ndx (x) f (u(x)). Thus, to make the area of these two rectangles to agree, we need to multiply\nthe ﬁrst one by du\ndx (x) as is illustrated in Fig. A.2.\nt\nFig. A.2\nVisualizing the transformation of a single thin rectangle under the change of variables.\nThis tells us that\n∫x+ϵ\nx\nf (u(y))du\ndy (y) dy =\n∫u(x+ϵ)\nu(x)\nf (y) dy.\n(A.19)\nThis is the change of variables formula expressed for a single small rectangle.\n\n988\nMathematics for Deep Learning\nIf u(x) and f (x) are properly chosen, this can allow for the computation of incredibly com-\nplex integrals. For instance, if we even chose f (y) = 1 and u(x) = e−x2 (which means\ndu\ndx (x) = −2xe−x2), this can show for instance that\ne−1 −1 =\n∫e−1\ne−0 1 dy = −2\n∫1\n0\nye−y2 dy,\n(A.20)\nand thus by rearranging that\n∫1\n0\nye−y2 dy = 1 −e−1\n2\n.\n(A.21)\nA.5.4 A Comment on Sign Conventions\nKeen-eyed readers will observe something strange about the computations above. Namely,\ncomputations like\n∫e−1\ne−0 1 dy = e−1 −1 < 0,\n(A.22)\ncan produce negative numbers. When thinking about areas, it can be strange to see a negative\nvalue, and so it is worth digging into what the convention is.\nMathematicians take the notion of signed areas. This manifests itself in two ways. First, if\nwe consider a function f (x) which is sometimes less than zero, then the area will also be\nnegative. So for instance\n∫1\n0\n(−1) dx = −1.\n(A.23)\nSimilarly, integrals which progress from right to left, rather than left to right are also taken\nto be negative areas\n∫−1\n0\n1 dx = −1.\n(A.24)\nThe standard area (from left to right of a positive function) is always positive. Anything ob-\ntained by ﬂipping it (say ﬂipping over the x-axis to get the integral of a negative number, or\nﬂipping over the y-axis to get an integral in the wrong order) will produce a negative area.\nAnd indeed, ﬂipping twice will give a pair of negative signs that cancel out to have positive\narea\n∫−1\n0\n(−1) dx = 1.\n(A.25)\nIf this discussion sounds familiar, it is! In Section A.1 we discussed how the determinant\nrepresented the signed area in much the same way.\n\n989\nIntegral Calculus\nA.5.5 Multiple Integrals\nIn some cases, we will need to work in higher dimensions. For instance, suppose that we have\na function of two variables, like f (x, y) and we want to know the volume under f when x\nranges over [a, b] and y ranges over [c, d].\n# Construct grid and compute function\nx, y = torch.meshgrid(torch.linspace(-2, 2, 101), torch.linspace(-2, 2, 101))\nz = torch.exp(- x**2 - y**2)\n# Plot function\nax = d2l.plt.figure().add_subplot(111, projection='3d')\nax.plot_wireframe(x, y, z)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('y')\nd2l.plt.xticks([-2, -1, 0, 1, 2])\nd2l.plt.yticks([-2, -1, 0, 1, 2])\nd2l.set_figsize()\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_zlim(0, 1)\nax.dist = 12\nWe write this as\n∫\n[a,b]×[c,d]\nf (x, y) dx dy.\n(A.26)\nSuppose that we wish to compute this integral. My claim is that we can do this by iteratively\ncomputing ﬁrst the integral in x and then shifting to the integral in y, that is to say\n∫\n[a,b]×[c,d]\nf (x, y) dx dy =\n∫d\nc\n(∫b\na\nf (x, y) dx\n)\ndy.\n(A.27)\nLet’s see why this is.\nConsider the ﬁgure above where we have split the function into ϵ × ϵ squares which we will\nindex with integer coordinates i, j. In this case, our integral is approximately\n∑\ni, j\nϵ2 f (ϵi, ϵ j).\n(A.28)\n\n990\nMathematics for Deep Learning\nOnce we discretize the problem, we may add up the values on these squares in whatever order\nwe like, and not worry about changing the values. This is illustrated in Fig. A.3. In particular,\nwe can say that\n∑\nj\nϵ\n(∑\ni\nϵ f (ϵi, ϵ j)\n)\n.\n(A.29)\nt\nFig. A.3\nIllustrating how to decompose a sum over many squares as a sum over ﬁrst the columns\n(1), then adding the column sums together (2).\nThe sum on the inside is precisely the discretization of the integral\nG(ϵ j) =\n∫b\na\nf (x, ϵ j) dx.\n(A.30)\nFinally, notice that if we combine these two expressions we get\n∑\nj\nϵG(ϵ j) ≈\n∫d\nc\nG(y) dy =\n∫\n[a,b]×[c,d]\nf (x, y) dx dy.\n(A.31)\nThus putting it all together, we have that\n∫\n[a,b]×[c,d]\nf (x, y) dx dy =\n∫d\nc\n(∫b\na\nf (x, y) dx\n)\ndy.\n(A.32)\nNotice that, once discretized, all we did was rearrange the order in which we added a list\nof numbers. This may make it seem like it is nothing, however this result (called Fubini’s\nTheorem) is not always true! For the type of mathematics encountered when doing machine\nlearning (continuous functions), there is no concern, however it is possible to create examples\nwhere it fails (for example the function f (x, y) = xy(x2 −y2)/(x2 + y2)3 over the rectangle\n[0, 2] × [0, 1]).\nNote that the choice to do the integral in x ﬁrst, and then the integral in y was arbitrary. We\ncould have equally well chosen to do y ﬁrst and then x to see\n∫\n[a,b]×[c,d]\nf (x, y) dx dy =\n∫b\na\n(∫d\nc\nf (x, y) dy\n)\ndx.\n(A.33)\nOften times, we will condense down to vector notation, and say that for U = [a, b] × [c, d]\nthis is\n∫\nU\nf (x) dx.\n(A.34)\n\n991\nIntegral Calculus\nA.5.6 Change of Variables in Multiple Integrals\nAs with single variables in (A.18), the ability to change variables inside a higher dimensional\nintegral is a key tool. Let’s summarize the result without derivation.\nWe need a function that reparametrizes our domain of integration. We can take this to be\nϕ : Rn →Rn, that is any function which takes in n real variables and returns another n. To\nkeep the expressions clean, we will assume that ϕ is injective which is to say it never folds\nover itself (ϕ(x) = ϕ(y) =⇒x = y).\nIn this case, we can say that\n∫\nϕ(U)\nf (x) dx =\n∫\nU\nf (ϕ(x))\n\f\fdet(Dϕ(x))\n\f\f dx.\n(A.35)\nwhere Dϕ is the Jacobian of ϕ, which is the matrix of partial derivatives of ϕ = (ϕ1(x1, . . ., xn), . . ., ϕn(x1, . . ., xn)),\nDϕ =\n\n∂ϕ1\n∂x1\n· · ·\n∂ϕ1\n∂xn\n...\n...\n...\n∂ϕn\n∂x1\n· · ·\n∂ϕn\n∂xn\n\n.\n(A.36)\nLooking closely, we see that this is similar to the single variable chain rule (A.18), except\nwe have replaced the term du\ndx (x) with\n\f\fdet(Dϕ(x))\n\f\f. Let’s see how we can to interpret this\nterm. Recall that the du\ndx (x) term existed to say how much we stretched our x-axis by applying\nu. The same process in higher dimensions is to determine how much we stretch the area\n(or volume, or hyper-volume) of a little square (or little hyper-cube) by applying ϕ. If ϕ\nwas the multiplication by a matrix, then we know how the determinant already gives the\nanswer.\nWith some work, one can show that the Jacobian provides the best approximation to a mul-\ntivariable function ϕ at a point by a matrix in the same way we could approximate by lines or\nplanes with derivatives and gradients. Thus the determinant of the Jacobian exactly mirrors\nthe scaling factor we identiﬁed in one dimension.\nIt takes some work to ﬁll in the details to this, so do not worry if they are not clear now. Let’s\nsee at least one example we will make use of later on. Consider the integral\n∫∞\n−∞\n∫∞\n−∞\ne−x2−y2 dx dy.\n(A.37)\nPlaying with this integral directly will get us no-where, but if we change variables, we can\nmake signiﬁcant progress. If we let ϕ(r, θ) = (r cos(θ), r sin(θ)) (which is to say that x =\nr cos(θ), y = r sin(θ)), then we can apply the change of variable formula to see that this is\nthe same thing as\n∫∞\n0\n∫2π\n0\ne−r2 \f\fdet(DŒ(x))\n\f\f dθ dr,\n(A.38)\nwhere\n\f\fdet(DŒ(x))\n\f\f =\n\f\f\f\fdet\n[cos(θ)\n−r sin(θ)\nsin(θ)\nr cos(θ)\n]\f\f\f\f = r(cos2(θ) + sin2(θ)) = r.\n(A.39)\n\n992\nMathematics for Deep Learning\n282\nThus, the integral is\n∫∞\n0\n∫2π\n0\nre−r2 dθ dr = 2π\n∫∞\n0\nre−r2 dr = π,\n(A.40)\nwhere the ﬁnal equality follows by the same computation that we used in section Section\nA.5.3.\nWe will meet this integral again when we study continuous random variables in Section\nA.6.\nA.5.7 Summary\n• The theory of integration allows us to answer questions about areas or volumes.\n• The fundamental theorem of calculus allows us to leverage knowledge about derivatives\nto compute areas via the observation that the derivative of the area up to some point is\ngiven by the value of the function being integrated.\n• Integrals in higher dimensions can be computed by iterating single variable integrals.\nA.5.8 Exercises\n1. What is\n∫2\n1\n1\nx dx?\n2. Use the change of variables formula to integrate\n∫√π\n0\nx sin(x2) dx.\n3. What is\n∫\n[0,1]2 xy dx dy?\n4. Use the change of variables formula to compute\n∫2\n0\n∫1\n0 xy(x2 −y2)/(x2 + y2)3 dy dx\nand\n∫1\n0\n∫2\n0 f (x, y) = xy(x2 −y2)/(x2 + y2)3 dx dy to see they are diﬀerent.\nDiscussions282.\nA.6 Random Variables\nIn Section 2.6 we saw the basics of how to work with discrete random variables, which in\nour case refer to those random variables which take either a ﬁnite set of possible values, or\nthe integers. In this section, we develop the theory of continuous random variables, which are\nrandom variables which can take on any real value.\n\n993\nRandom Variables\nA.6.1 Continuous Random Variables\nContinuous random variables are a signiﬁcantly more subtle topic than discrete random vari-\nables. A fair analogy to make is that the technical jump is comparable to the jump between\nadding lists of numbers and integrating functions. As such, we will need to take some time\nto develop the theory.\nFrom Discrete to Continuous\nTo understand the additional technical challenges encountered when working with continuous\nrandom variables, let’s perform a thought experiment. Suppose that we are throwing a dart at\nthe dart board, and we want to know the probability that it hits exactly 2cm from the center\nof the board.\nTo start with, we imagine measuring a single digit of accuracy, that is to say with bins for\n0cm, 1cm, 2cm, and so on. We throw say 100 darts at the dart board, and if 20 of them fall\ninto the bin for 2cm we conclude that 20% of the darts we throw hit the board 2cm away\nfrom the center.\nHowever, when we look closer, this does not match our question! We wanted exact equality,\nwhereas these bins hold all that fell between say 1.5cm and 2.5cm.\nUndeterred, we continue further. We measure even more precisely, say 1.9cm, 2.0cm, 2.1cm,\nand now see that perhaps 3 of the 100 darts hit the board in the 2.0cm bucket. Thus we\nconclude the probability is 3%.\nHowever, this does not solve anything! We have just pushed the issue down one digit fur-\nther. Let’s abstract a bit. Imagine we know the probability that the ﬁrst k digits match with\n2.00000 . . . and we want to know the probability it matches for the ﬁrst k + 1 digits. It is\nfairly reasonable to assume that the k + 1th digit is essentially a random choice from the set\n{0, 1, 2, . . ., 9}. At least, we cannot conceive of a physically meaningful process which would\nforce the number of micrometers away form the center to prefer to end in a 7 vs a 3.\nWhat this means is that in essence each additional digit of accuracy we require should decrease\nprobability of matching by a factor of 10. Or put another way, we would expect that\nP(distance is 2.00 . . ., to k digits) ≈p · 10−k.\n(A.1)\nThe value p essentially encodes what happens with the ﬁrst few digits, and the 10−k handles\nthe rest.\nNotice that if we know the position accurate to k = 4 digits after the decimal, that means we\nknow the value falls within the interval say [1.99995, 2.00005] which is an interval of length\n2.00005 −1.99995 = 10−4. Thus, if we call the length of this interval ϵ, we can say\nP(distance is in an ϵ-sized interval around 2) ≈ϵ · p.\n(A.2)\nLet’s take this one ﬁnal step further. We have been thinking about the point 2 the entire time,\n\n994\nMathematics for Deep Learning\nbut never thinking about other points. Nothing is diﬀerent there fundamentally, but it is the\ncase that the value p will likely be diﬀerent. We would at least hope that a dart thrower was\nmore likely to hit a point near the center, like 2cm rather than 20cm. Thus, the value p is not\nﬁxed, but rather should depend on the point x. This tells us that we should expect\nP(distance is in an ϵ-sized interval around x) ≈ϵ · p(x).\n(A.3)\nIndeed, (A.3) precisely deﬁnes the probability density function. It is a function p(x) which\nencodes the relative probability of hitting near one point vs. another. Let’s visualize what such\na function might look like.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2\n# Define pi in torch\n# Plot the probability density function for some random variable\nx = torch.arange(-5, 5, 0.01)\np = 0.2*torch.exp(-(x - 3)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi)) + \\\n0.8*torch.exp(-(x + 1)**2 / 2)/torch.sqrt(2 * torch.tensor(torch.pi))\nd2l.plot(x, p, 'x', 'Density')\nThe locations where the function value is large indicates regions where we are more likely to\nﬁnd the random value. The low portions are areas where we are unlikely to ﬁnd the random\nvalue.\nProbability Density Functions\nLet’s now investigate this further. We have already seen what a probability density func-\ntion is intuitively for a random variable X, namely the density function is a function p(x) so\nthat\nP(X is in an ϵ-sized interval around x) ≈ϵ · p(x).\n(A.4)\n\n995\nRandom Variables\nBut what does this imply for the properties of p(x)?\nFirst, probabilities are never negative, thus we should expect that p(x) ≥0 as well.\nSecond, let’s imagine that we slice up the R into an inﬁnite number of slices which are ϵ wide,\nsay with slices (ϵ · i, ϵ · (i + 1)]. For each of these, we know from (A.4) the probability is\napproximately\nP(X is in an ϵ-sized interval around x) ≈ϵ · p(ϵ · i),\n(A.5)\nso summed over all of them it should be\nP(X ∈R) ≈\n∑\ni\nϵ · p(ϵ · i).\n(A.6)\nThis is nothing more than the approximation of an integral discussed in Section A.5, thus we\ncan say that\nP(X ∈R) =\n∫∞\n−∞\np(x) dx.\n(A.7)\nWe know that P(X ∈R) = 1, since the random variable must take on some number, we can\nconclude that for any density\n∫∞\n−∞\np(x) dx = 1.\n(A.8)\nIndeed, digging into this further shows that for any a, and b, we see that\nP(X ∈(a, b]) =\n∫b\na\np(x) dx.\n(A.9)\nWe may approximate this in code by using the same discrete approximation methods as\nbefore. In this case we can approximate the probability of falling in the blue region.\n# Approximate probability using numerical integration\nepsilon = 0.01\nx = torch.arange(-5, 5, 0.01)\np = 0.2*torch.exp(-(x - 3)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi)) +\\\n0.8*torch.exp(-(x + 1)**2 / 2) / torch.sqrt(2 * torch.tensor(torch.pi))\nd2l.set_figsize()\nd2l.plt.plot(x, p, color='black')\nd2l.plt.fill_between(x.tolist()[300:800], p.tolist()[300:800])\nd2l.plt.show()\nf'approximate Probability: {torch.sum(epsilon*p[300:800])}'\n'approximate Probability: 0.773617148399353'\nIt turns out that these two properties describe exactly the space of possible probability density\nfunctions (or p.d.f.’s for the commonly encountered abbreviation). They are non-negative\nfunctions p(x) ≥0 such that\n∫∞\n−∞\np(x) dx = 1.\n(A.10)\n\n996\nMathematics for Deep Learning\nWe interpret this function by using integration to obtain the probability our random variable\nis in a speciﬁc interval:\nP(X ∈(a, b]) =\n∫b\na\np(x) dx.\n(A.11)\nIn Section A.8 we will see a number of common distributions, but let’s continue working in\nthe abstract.\nCumulative Distribution Functions\nIn the previous section, we saw the notion of the p.d.f. In practice, this is a commonly en-\ncountered method to discuss continuous random variables, but it has one signiﬁcant pitfall:\nthat the values of the p.d.f. are not themselves probabilities, but rather a function that we\nmust integrate to yield probabilities. There is nothing wrong with a density being larger than\n10, as long as it is not larger than 10 for more than an interval of length 1/10. This can be\ncounter-intuitive, so people often also think in terms of the cumulative distribution function,\nor c.d.f., which is a probability.\nIn particular, by using (A.11), we deﬁne the c.d.f. for a random variable X with density p(x)\nby\nF(x) =\n∫x\n−∞\np(x) dx = P(X ≤x).\n(A.12)\nLet’s observe a few properties.\n• F(x) →0 as x →−∞.\n• F(x) →1 as x →∞.\n• F(x) is non-decreasing (y > x =⇒F(y) ≥F(x)).\n• F(x) is continuous (has no jumps) if X is a continuous random variable.\nWith the fourth bullet point, note that this would not be true if X were discrete, say taking\n\n997\nRandom Variables\nthe values 0 and 1 both with probability 1/2. In that case\nF(x) =\n\n\n0\nx < 0,\n1\n2\nx < 1,\n1\nx ≥1.\n(A.13)\nIn this example, we see one of the beneﬁts of working with the c.d.f., the ability to deal with\ncontinuous or discrete random variables in the same framework, or indeed mixtures of the\ntwo (ﬂip a coin: if heads return the roll of a die, if tails return the distance of a dart throw\nfrom the center of a dart board).\nMeans\nSuppose that we are dealing with a random variables X. The distribution itself can be hard\nto interpret. It is often useful to be able to summarize the behavior of a random variable\nconcisely. Numbers that help us capture the behavior of a random variable are called summary\nstatistics. The most commonly encountered ones are the mean, the variance, and the standard\ndeviation.\nThe mean encodes the average value of a random variable. If we have a discrete random\nvariable X, which takes the values xi with probabilities pi, then the mean is given by the\nweighted average: sum the values times the probability that the random variable takes on that\nvalue:\nµX = E[X] =\n∑\ni\nxipi.\n(A.14)\nThe way we should interpret the mean (albeit with caution) is that it tells us essentially where\nthe random variable tends to be located.\nAs a minimalistic example that we will examine throughout this section, let’s take X to be\nthe random variable which takes the value a −2 with probability p, a + 2 with probability p\nand a with probability 1 −2p. We can compute using (A.14) that, for any possible choice of\na and p, the mean is\nµX = E[X] =\n∑\ni\nxipi = (a −2)p + a(1 −2p) + (a + 2)p = a.\n(A.15)\nThus we see that the mean is a. This matches the intuition since a is the location around\nwhich we centered our random variable.\nBecause they are helpful, let’s summarize a few properties.\n• For any random variable X and numbers a and b, we have that µaX+b = aµX + b.\n• If we have two random variables X and Y, we have µX+Y = µX + µY.\nMeans are useful for understanding the average behavior of a random variable, however the\nmean is not suﬃcient to even have a full intuitive understanding. Making a proﬁt of $10 ± $1\nper sale is very diﬀerent from making $10 ± $15 per sale despite having the same average\n\n998\nMathematics for Deep Learning\nvalue. The second one has a much larger degree of ﬂuctuation, and thus represents a much\nlarger risk. Thus, to understand the behavior of a random variable, we will need at minimum\none more measure: some measure of how widely a random variable ﬂuctuates.\nVariances\nThis leads us to consider the variance of a random variable. This is a quantitative measure of\nhow far a random variable deviates from the mean. Consider the expression X −µX. This is\nthe deviation of the random variable from its mean. This value can be positive or negative,\nso we need to do something to make it positive so that we are measuring the magnitude of\nthe deviation.\nA reasonable thing to try is to look at |X −µX|, and indeed this leads to a useful quantity\ncalled the mean absolute deviation, however due to connections with other areas of mathe-\nmatics and statistics, people often use a diﬀerent solution.\nIn particular, they look at (X −µX)2. If we look at the typical size of this quantity by taking\nthe mean, we arrive at the variance\nσ2\nX = Var(X) = E\n[\n(X −µX)2]\n= E[X2] −µ2\nX.\n(A.16)\nThe last equality in (A.16) holds by expanding out the deﬁnition in the middle, and applying\nthe properties of expectation.\nLet’s look at our example where X is the random variable which takes the value a −2 with\nprobability p, a + 2 with probability p and a with probability 1 −2p. In this case µX = a,\nso all we need to compute is E\n[\nX2]\n. This can readily be done:\nE\n[\nX2]\n= (a −2)2p + a2(1 −2p) + (a + 2)2p = a2 + 8p.\n(A.17)\nThus, we see that by (A.16) our variance is\nσ2\nX = Var(X) = E[X2] −µ2\nX = a2 + 8p −a2 = 8p.\n(A.18)\nThis result again makes sense. The largest p can be is 1/2 which corresponds to picking a−2\nor a + 2 with a coin ﬂip. The variance of this being 4 corresponds to the fact that both a −2\nand a + 2 are 2 units away from the mean, and 22 = 4. On the other end of the spectrum, if\np = 0, this random variable always takes the value 0 and so it has no variance at all.\nWe will list a few properties of variance below:\n• For any random variable X, Var(X) ≥0, with Var(X) = 0 if and only if X is a constant.\n• For any random variable X and numbers a and b, we have that Var(aX + b) = a2Var(X).\n• If we have two independent random variables X and Y, we have Var(X +Y) = Var(X) +\nVar(Y).\nWhen interpreting these values, there can be a bit of a hiccup. In particular, let’s try imagining\nwhat happens if we keep track of units through this computation. Suppose that we are working\n\n999\nRandom Variables\nwith the star rating assigned to a product on the web page. Then a, a −2, and a + 2 are all\nmeasured in units of stars. Similarly, the mean µX is then also measured in stars (being a\nweighted average). However, if we get to the variance, we immediately encounter an issue,\nwhich is we want to look at (X −µX)2, which is in units of squared stars. This means that\nthe variance itself is not comparable to the original measurements. To make it interpretable,\nwe will need to return to our original units.\nStandard Deviations\nThis summary statistics can always be deduced from the variance by taking the square root!\nThus we deﬁne the standard deviation to be\nσX =\n√\nVar(X).\n(A.19)\nIn our example, this means we now have the standard deviation is σX = 2√2p. If we are\ndealing with units of stars for our review example, σX is again in units of stars.\nThe properties we had for the variance can be restated for the standard deviation.\n• For any random variable X, σX ≥0.\n• For any random variable X and numbers a and b, we have that σaX+b = |a|σX\n• If we have two independent random variables X and Y, we have σX+Y =\n√\nσ2\nX + σ2\nY.\nIt is natural at this moment to ask, “If the standard deviation is in the units of our original ran-\ndom variable, does it represent something we can draw with regards to that random variable?”\nThe answer is a resounding yes! Indeed much like the mean told us the typical location of our\nrandom variable, the standard deviation gives the typical range of variation of that random\nvariable. We can make this rigorous with what is known as Chebyshev’s inequality:\nP (X < [µX −ασX, µX + ασX]) ≤1\nα2 .\n(A.20)\nOr to state it verbally in the case of α = 10, 99% of the samples from any random variable\nfall within 10 standard deviations of the mean. This gives an immediate interpretation to our\nstandard summary statistics.\nTo see how this statement is rather subtle, let’s take a look at our running example again\nwhere X is the random variable which takes the value a −2 with probability p, a + 2 with\nprobability p and a with probability 1 −2p. We saw that the mean was a and the standard\ndeviation was 2√2p. This means, if we take Chebyshev’s inequality (A.20) with α = 2, we\nsee that the expression is\nP\n(\nX < [a −4\n√\n2p, a + 4\n√\n2p]\n)\n≤1\n4.\n(A.21)\nThis means that 75% of the time, this random variable will fall within this interval for any\nvalue of p. Now, notice that as p →0, this interval also converges to the single point a. But\nwe know that our random variable takes the values a −2, a, and a + 2 only so eventually we\n\n1000\nMathematics for Deep Learning\ncan be certain a −2 and a + 2 will fall outside the interval! The question is, at what p does\nthat happen. So we want to solve: for what p does a + 4√2p = a + 2, which is solved when\np = 1/8, which is exactly the ﬁrst p where it could possibly happen without violating our\nclaim that no more than 1/4 of samples from the distribution would fall outside the interval\n(1/8 to the left, and 1/8 to the right).\nLet’s visualize this. We will show the probability of getting the three values as three vertical\nbars with height proportional to the probability. The interval will be drawn as a horizontal\nline in the middle. The ﬁrst plot shows what happens for p > 1/8 where the interval safely\ncontains all points.\n# Define a helper to plot these figures\ndef plot_chebyshev(a, p):\nd2l.set_figsize()\nd2l.plt.stem([a-2, a, a+2], [p, 1-2*p, p], use_line_collection=True)\nd2l.plt.xlim([-4, 4])\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.hlines(0.5, a - 4 * torch.sqrt(2 * p),\na + 4 * torch.sqrt(2 * p), 'black', lw=4)\nd2l.plt.vlines(a - 4 * torch.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\nd2l.plt.vlines(a + 4 * torch.sqrt(2 * p), 0.53, 0.47, 'black', lw=1)\nd2l.plt.title(f'p = {p:.3f}')\nd2l.plt.show()\n# Plot interval when p > 1/8\nplot_chebyshev(0.0, torch.tensor(0.2))\nThe second shows that at p = 1/8, the interval exactly touches the two points. This shows that\nthe inequality is sharp, since no smaller interval could be taken while keeping the inequality\ntrue.\n# Plot interval when p = 1/8\nplot_chebyshev(0.0, torch.tensor(0.125))\n\n1001\nRandom Variables\nThe third shows that for p < 1/8 the interval only contains the center. This does not invalidate\nthe inequality since we only needed to ensure that no more than 1/4 of the probability falls\noutside the interval, which means that once p < 1/8, the two points at a −2 and a + 2 can\nbe discarded.\n# Plot interval when p < 1/8\nplot_chebyshev(0.0, torch.tensor(0.05))\nMeans and Variances in the Continuum\nThis has all been in terms of discrete random variables, but the case of continuous random\nvariables is similar. To intuitively understand how this works, imagine that we split the real\nnumber line into intervals of length ϵ given by (ϵi, ϵ(i +1)]. Once we do this, our continuous\nrandom variable has been made discrete and we can use (A.14) say that\nµX ≈\n∑\ni\n(ϵi)P(X ∈(ϵi, ϵ(i + 1)])\n≈\n∑\ni\n(ϵi)pX(ϵi)ϵ,\n(A.22)\n\n1002\nMathematics for Deep Learning\nwhere pX is the density of X. This is an approximation to the integral of xpX(x), so we can\nconclude that\nµX =\n∫∞\n−∞\nxpX(x) dx.\n(A.23)\nSimilarly, using (A.16) the variance can be written as\nσ2\nX = E[X2] −µ2\nX =\n∫∞\n−∞\nx2pX(x) dx −\n(∫∞\n−∞\nxpX(x) dx\n)2\n.\n(A.24)\nEverything stated above about the mean, the variance, and the standard deviation still applies\nin this case. For instance, if we consider the random variable with density\np(x) =\n{\n1\nx ∈[0, 1],\n0\notherwise.\n(A.25)\nwe can compute\nµX =\n∫∞\n−∞\nxp(x) dx =\n∫1\n0\nx dx = 1\n2.\n(A.26)\nand\nσ2\nX =\n∫∞\n−∞\nx2p(x) dx −\n(1\n2\n)2\n= 1\n3 −1\n4 = 1\n12.\n(A.27)\nAs a warning, let’s examine one more example, known as the Cauchy distribution. This is the\ndistribution with p.d.f. given by\np(x) =\n1\n1 + x2 .\n(A.28)\n# Plot the Cauchy distribution p.d.f.\nx = torch.arange(-5, 5, 0.01)\np = 1 / (1 + x**2)\nd2l.plot(x, p, 'x', 'p.d.f.')\nThis function looks innocent, and indeed consulting a table of integrals will show it has area\none under it, and thus it deﬁnes a continuous random variable.\n\n1003\nRandom Variables\nTo see what goes astray, let’s try to compute the variance of this. This would involve using\n(A.16) computing\n∫∞\n−∞\nx2\n1 + x2 dx.\n(A.29)\nThe function on the inside looks like this:\n# Plot the integrand needed to compute the variance\nx = torch.arange(-20, 20, 0.01)\np = x**2 / (1 + x**2)\nd2l.plot(x, p, 'x', 'integrand')\nThis function clearly has inﬁnite area under it since it is essentially the constant one with a\nsmall dip near zero, and indeed we could show that\n∫∞\n−∞\nx2\n1 + x2 dx = ∞.\n(A.30)\nThis means it does not have a well-deﬁned ﬁnite variance.\nHowever, looking deeper shows an even more disturbing result. Let’s try to compute the mean\nusing (A.14). Using the change of variables formula, we see\nµX =\n∫∞\n−∞\nx\n1 + x2 dx = 1\n2\n∫∞\n1\n1\nu du.\n(A.31)\nThe integral inside is the deﬁnition of the logarithm, so this is in essence log(∞) = ∞, so\nthere is no well-deﬁned average value either!\nMachine learning scientists deﬁne their models so that we most often do not need to deal\nwith these issues, and will in the vast majority of cases deal with random variables with well-\ndeﬁned means and variances. However, every so often random variables with heavy tails (that\nis those random variables where the probabilities of getting large values are large enough to\nmake things like the mean or variance undeﬁned) are helpful in modeling physical systems,\nthus it is worth knowing that they exist.\n\n1004\nMathematics for Deep Learning\nJoint Density Functions\nThe above work all assumes we are working with a single real valued random variable. But\nwhat if we are dealing with two or more potentially highly correlated random variables? This\ncircumstance is the norm in machine learning: imagine random variables like Ri, j which\nencode the red value of the pixel at the (i, j) coordinate in an image, or Pt which is a random\nvariable given by a stock price at time t. Nearby pixels tend to have similar color, and nearby\ntimes tend to have similar prices. We cannot treat them as separate random variables, and\nexpect to create a successful model (we will see in Section A.9 a model that under-performs\ndue to such an assumption). We need to develop the mathematical language to handle these\ncorrelated continuous random variables.\nThankfully, with the multiple integrals in Section A.5 we can develop such a language. Sup-\npose that we have, for simplicity, two random variables X,Y which can be correlated. Then,\nsimilar to the case of a single variable, we can ask the question:\nP(X is in an ϵ-sized interval around x and Y is in an ϵ-sized interval around y).\n(A.32)\nSimilar reasoning to the single variable case shows that this should be approximately\nP(X is in an ϵ-sized interval around x and Y is in an ϵ-sized interval around y) ≈ϵ2p(x, y),\n(A.33)\nfor some function p(x, y). This is referred to as the joint density of X andY. Similar properties\nare true for this as we saw in the single variable case. Namely:\n• p(x, y) ≥0;\n•\n∫\nR2 p(x, y) dx dy = 1;\n• P((X,Y) ∈D) =\n∫\nD p(x, y) dx dy.\nIn this way, we can deal with multiple, potentially correlated random variables. If we wish\nto work with more than two random variables, we can extend the multivariate density to as\nmany coordinates as desired by considering p(x) = p(x1, . . ., xn). The same properties of\nbeing non-negative, and having total integral of one still hold.\nMarginal Distributions\nWhen dealing with multiple variables, we oftentimes want to be able to ignore the relation-\nships and ask, “how is this one variable distributed?” Such a distribution is called a marginal\ndistribution.\nTo be concrete, let’s suppose that we have two random variables X,Y with joint density given\nby pX,Y(x, y). We will be using the subscript to indicate what random variables the density\nis for. The question of ﬁnding the marginal distribution is taking this function, and using it\nto ﬁnd pX(x).\n\n1005\nRandom Variables\nAs with most things, it is best to return to the intuitive picture to ﬁgure out what should be\ntrue. Recall that the density is the function pX so that\nP(X ∈[x, x + ϵ]) ≈ϵ · pX(x).\n(A.34)\nThere is no mention of Y, but if all we are given is pX,Y, we need to include Y somehow. We\ncan ﬁrst observe that this is the same as\nP(X ∈[x, x + ϵ], and Y ∈R) ≈ϵ · pX(x).\n(A.35)\nOur density does not directly tell us about what happens in this case, we need to split into\nsmall intervals in y as well, so we can write this as\nϵ · pX(x) ≈\n∑\ni\nP(X ∈[x, x + ϵ], and Y ∈[ϵ · i, ϵ · (i + 1)])\n≈\n∑\ni\nϵ2pX,Y(x, ϵ · i).\n(A.36)\nt\nFig. A.1\nBy summing along the columns of our array of probabilities, we are able to obtain the\nmarginal distribution for just the random variable represented along the x-axis.\nThis tells us to add up the value of the density along a series of squares in a line as is shown\nin Fig. A.1. Indeed, after canceling one factor of epsilon from both sides, and recognizing\nthe sum on the right is the integral over y, we can conclude that\npX(x) ≈\n∑\ni\nϵpX,Y(x, ϵ · i)\n≈\n∫∞\n−∞\npX,Y(x, y) dy.\n(A.37)\nThus we see\npX(x) =\n∫∞\n−∞\npX,Y(x, y) dy.\n(A.38)\nThis tells us that to get a marginal distribution, we integrate over the variables we do not care\nabout. This process is often referred to as integrating out or marginalized out the unneeded\nvariables.\n\n1006\nMathematics for Deep Learning\nCovariance\nWhen dealing with multiple random variables, there is one additional summary statistic which\nis helpful to know: the covariance. This measures the degree that two random variable ﬂuc-\ntuate together.\nSuppose that we have two random variables X and Y, to begin with, let’s suppose they are\ndiscrete, taking on values (xi, yj) with probability pij. In this case, the covariance is deﬁned\nas\nσXY = Cov(X,Y) =\n∑\ni,j\n(xi −µX)(yj −µY)pij. = E[XY] −E[X]E[Y].\n(A.39)\nTo think about this intuitively: consider the following pair of random variables. Suppose that\nX takes the values 1 and 3, and Y takes the values −1 and 3. Suppose that we have the\nfollowing probabilities\nP(X = 1 and Y = −1) = p\n2,\nP(X = 1 and Y = 3) = 1 −p\n2\n,\nP(X = 3 and Y = −1) = 1 −p\n2\n,\nP(X = 3 and Y = 3) = p\n2,\n(A.40)\nwhere p is a parameter in [0, 1] we get to pick. Notice that if p = 1 then they are both always\ntheir minimum or maximum values simultaneously, and if p = 0 they are guaranteed to\ntake their ﬂipped values simultaneously (one is large when the other is small and vice versa).\nIf p = 1/2, then the four possibilities are all equally likely, and neither should be related.\nLet’s compute the covariance. First, note µX = 2 and µY = 1, so we may compute using\n(A.39):\nCov(X,Y) =\n∑\ni,j\n(xi −µX)(yj −µY)pij\n= (1 −2)(−1 −1) p\n2 + (1 −2)(3 −1)1 −p\n2\n+ (3 −2)(−1 −1)1 −p\n2\n+ (3 −2)(3 −1) p\n2\n= 4p −2.\n(A.41)\nWhen p = 1 (the case where they are both maximally positive or negative at the same time)\nhas a covariance of 2. When p = 0 (the case where they are ﬂipped) the covariance is −2.\nFinally, when p = 1/2 (the case where they are unrelated), the covariance is 0. Thus we see\nthat the covariance measures how these two random variables are related.\nA quick note on the covariance is that it only measures these linear relationships. More com-\nplex relationships like X = Y 2 where Y is randomly chosen from {−2, −1, 0, 1, 2} with equal\nprobability can be missed. Indeed a quick computation shows that these random variables\nhave covariance zero, despite one being a deterministic function of the other.\nFor continuous random variables, much the same story holds. At this point, we are pretty\n\n1007\nRandom Variables\ncomfortable with doing the transition between discrete and continuous, so we will provide\nthe continuous analogue of (A.39) without any derivation.\nσXY =\n∫\nR2(x −µX)(y −µY)p(x, y) dx dy.\n(A.42)\nFor visualization, let’s take a look at a collection of random variables with tunable covari-\nance.\n# Plot a few random variables adjustable covariance\ncovs = [-0.9, 0.0, 1.2]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\nX = torch.randn(500)\nY = covs[i]*X + torch.randn(500)\nd2l.plt.subplot(1, 4, i+1)\nd2l.plt.scatter(X.numpy(), Y.numpy())\nd2l.plt.xlabel('X')\nd2l.plt.ylabel('Y')\nd2l.plt.title(f'cov = {covs[i]}')\nd2l.plt.show()\nLet’s see some properties of covariances:\n• For any random variable X, Cov(X, X) = Var(X).\n• For any random variables X,Y and numbers a and b, Cov(aX +b,Y) = Cov(X, aY +b) =\naCov(X,Y).\n• If X and Y are independent then Cov(X,Y) = 0.\nIn addition, we can use the covariance to expand a relationship we saw before. Recall that is\nX and Y are two independent random variables then\nVar(X + Y) = Var(X) + Var(Y).\n(A.43)\nWith knowledge of covariances, we can expand this relationship. Indeed, some algebra can\n\n1008\nMathematics for Deep Learning\nshow that in general,\nVar(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y).\n(A.44)\nThis allows us to generalize the variance summation rule for correlated random variables.\nCorrelation\nAs we did in the case of means and variances, let’s now consider units. If X is measured in\none unit (say inches), and Y is measured in another (say dollars), the covariance is measured\nin the product of these two units inches × dollars. These units can be hard to interpret. What\nwe will often want in this case is a unit-less measurement of relatedness. Indeed, often we do\nnot care about exact quantitative correlation, but rather ask if the correlation is in the same\ndirection, and how strong the relationship is.\nTo see what makes sense, let’s perform a thought experiment. Suppose that we convert our\nrandom variables in inches and dollars to be in inches and cents. In this case the random\nvariable Y is multiplied by 100. If we work through the deﬁnition, this means that Cov(X,Y)\nwill be multiplied by 100. Thus we see that in this case a change of units change the covariance\nby a factor of 100. Thus, to ﬁnd our unit-invariant measure of correlation, we will need to\ndivide by something else that also gets scaled by 100. Indeed we have a clear candidate, the\nstandard deviation! Indeed if we deﬁne the correlation coeﬃcient to be\nρ(X,Y) = Cov(X,Y)\nσXσY\n,\n(A.45)\nwe see that this is a unit-less value. A little mathematics can show that this number is between\n−1 and 1 with 1 meaning maximally positively correlated, whereas −1 means maximally\nnegatively correlated.\nReturning to our explicit discrete example above, we can see that σX = 1 and σY = 2,\nso we can compute the correlation between the two random variables using (A.45) to see\nthat\nρ(X,Y) = 4p −2\n1 · 2\n= 2p −1.\n(A.46)\nThis now ranges between −1 and 1 with the expected behavior of 1 meaning most correlated,\nand −1 meaning minimally correlated.\nAs another example, consider X as any random variable, and Y = aX + b as any linear\ndeterministic function of X. Then, one can compute that\nσY = σaX+b = |a|σX,\n(A.47)\nCov(X,Y) = Cov(X, aX + b) = aCov(X, X) = aVar(X),\n(A.48)\nand thus by (A.45) that\nρ(X,Y) = aVar(X)\n|a|σ2\nX\n= a\n|a| = sign(a).\n(A.49)\n\n1009\nRandom Variables\nThus we see that the correlation is +1 for any a > 0, and −1 for any a < 0 illustrating that\ncorrelation measures the degree and directionality the two random variables are related, not\nthe scale that the variation takes.\nLet’s again plot a collection of random variables with tunable correlation.\n# Plot a few random variables adjustable correlations\ncors = [-0.9, 0.0, 1.0]\nd2l.plt.figure(figsize=(12, 3))\nfor i in range(3):\nX = torch.randn(500)\nY = cors[i] * X + torch.sqrt(torch.tensor(1) -\ncors[i]**2) * torch.randn(500)\nd2l.plt.subplot(1, 4, i + 1)\nd2l.plt.scatter(X.numpy(), Y.numpy())\nd2l.plt.xlabel('X')\nd2l.plt.ylabel('Y')\nd2l.plt.title(f'cor = {cors[i]}')\nd2l.plt.show()\nLet’s list a few properties of the correlation below.\n• For any random variable X, ρ(X, X) = 1.\n• For any random variables X,Y and numbers a and b, ρ(aX + b,Y) = ρ(X, aY + b) =\nρ(X,Y).\n• If X and Y are independent with non-zero variance then ρ(X,Y) = 0.\nAs a ﬁnal note, you may feel like some of these formulae are familiar. Indeed, if we expand\neverything out assuming that µX = µY = 0, we see that this is\nρ(X,Y) =\n∑\ni,j xiyipij\n√∑\ni, j x2\ni pij\n√∑\ni,j y2\nj pij\n.\n(A.50)\nThis looks like a sum of a product of terms divided by the square root of sums of terms. This\n\n1010\nMathematics for Deep Learning\nis exactly the formula for the cosine of the angle between two vectors v, w with the diﬀerent\ncoordinates weighted by pij:\ncos(θ) =\nv · w\n∥v∥∥w∥=\n∑\ni viwi\n√∑\ni v2\ni\n√∑\ni w2\ni\n.\n(A.51)\nIndeed if we think of norms as being related to standard deviations, and correlations as being\ncosines of angles, much of the intuition we have from geometry can be applied to thinking\nabout random variables.\nA.6.2 Summary\n• Continuous random variables are random variables that can take on a continuum of values.\nThey have some technical diﬃculties that make them more challenging to work with\ncompared to discrete random variables.\n• The probability density function allows us to work with continuous random variables by\ngiving a function where the area under the curve on some interval gives the probability\nof ﬁnding a sample point in that interval.\n• The cumulative distribution function is the probability of observing the random variable to\nbe less than a given threshold. It can provide a useful alternate viewpoint which uniﬁes\ndiscrete and continuous variables.\n• The mean is the average value of a random variable.\n• The variance is the expected square of the diﬀerence between the random variable and its\nmean.\n• The standard deviation is the square root of the variance. It can be thought of as measuring\nthe range of values the random variable may take.\n• Chebyshev’s inequality allows us to make this intuition rigorous by giving an explicit in-\nterval that contains the random variable most of the time.\n• Joint densities allow us to work with correlated random variables. We may marginalize\njoint densities by integrating over unwanted random variables to get the distribution of\nthe desired random variable.\n• The covariance and correlation coeﬃcient provide a way to measure any linear relationship\nbetween two correlated random variables.\nA.6.3 Exercises\n1. Suppose that we have the random variable with density given by p(x) =\n1\nx2 for x ≥1\nand p(x) = 0 otherwise. What is P(X > 2)?\n\n1011\nMaximum Likelihood\n283\n2. The Laplace distribution is a random variable whose density is given by p(x = 1\n2e−|x|.\nWhat is the mean and the standard deviation of this function? As a hint,\n∫∞\n0 xe−x dx = 1\nand\n∫∞\n0 x2e−x dx = 2.\n3. I walk up to you on the street and say “I have a random variable with mean 1, standard\ndeviation 2, and I observed 25% of my samples taking a value larger than 9.” Do you\nbelieve me? Why or why not?\n4. Suppose that you have two random variables X,Y, with joint density given by pXY(x, y) =\n4xy for x, y ∈[0, 1] and pXY(x, y) = 0 otherwise. What is the covariance of X and Y?\nDiscussions283.\nA.7 Maximum Likelihood\nOne of the most commonly encountered way of thinking in machine learning is the maximum\nlikelihood point of view. This is the concept that when working with a probabilistic model\nwith unknown parameters, the parameters which make the data have the highest probability\nare the most likely ones.\nA.7.1 The Maximum Likelihood Principle\nThis has a Bayesian interpretation which can be helpful to think about. Suppose that we have\na model with parameters θ and a collection of data examples X. For concreteness, we can\nimagine that θ is a single value representing the probability that a coin comes up heads when\nﬂipped, and X is a sequence of independent coin ﬂips. We will look at this example in depth\nlater.\nIf we want to ﬁnd the most likely value for the parameters of our model, that means we want\nto ﬁnd\nargmax P(θ | X).\n(A.1)\nBy Bayes’ rule, this is the same thing as\nargmax P(X | θ)P(θ)\nP(X)\n.\n(A.2)\nThe expression P(X), a parameter agnostic probability of generating the data, does not de-\npend on θ at all, and so can be dropped without changing the best choice of θ. Similarly, we\nmay now posit that we have no prior assumption on which set of parameters are better than\nany others, so we may declare that P(θ) does not depend on theta either! This, for instance,\nmakes sense in our coin ﬂipping example where the probability it comes up heads could be\n\n1012\nMathematics for Deep Learning\nany value in [0, 1] without any prior belief it is fair or not (often referred to as an uninforma-\ntive prior). Thus we see that our application of Bayes’ rule shows that our best choice of θ is\nthe maximum likelihood estimate for θ:\nˆθ = argmax\nθ\nP(X | θ).\n(A.3)\nAs a matter of common terminology, the probability of the data given the parameters (P(X |\nθ)) is referred to as the likelihood.\nA Concrete Example\nLet’s see how this works in a concrete example. Suppose that we have a single parameter θ\nrepresenting the probability that a coin ﬂip is heads. Then the probability of getting a tails is\n1 −θ, and so if our observed data X is a sequence with nH heads and nT tails, we can use the\nfact that independent probabilities multiply to see that\nP(X | θ) = θnH (1 −θ)nT .\n(A.4)\nIf we ﬂip 13 coins and get the sequence “HHHTHTTHHHHHT”, which has nH = 9 and\nnT = 4, we see that this is\nP(X | θ) = θ9(1 −θ)4.\n(A.5)\nOne nice thing about this example will be that we know the answer going in. Indeed, if we said\nverbally, “I ﬂipped 13 coins, and 9 came up heads, what is our best guess for the probability\nthat the coin comes us heads?, ” everyone would correctly guess 9/13. What this maximum\nlikelihood method will give us is a way to get that number from ﬁrst principals in a way that\nwill generalize to vastly more complex situations.\nFor our example, the plot of P(X | θ) is as follows:\n%matplotlib inline\nimport torch\nfrom d2l import torch as d2l\ntheta = torch.arange(0, 1, 0.001)\np = theta**9 * (1 - theta)**4.\nd2l.plot(theta, p, 'theta', 'likelihood')\nThis has its maximum value somewhere near our expected 9/13 ≈0.7 . . .. To see if it\nis exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the\nfunction is ﬂat. Thus, we could ﬁnd the maximum likelihood estimate (A.1) by ﬁnding the\nvalues of θ where the derivative is zero, and ﬁnding the one that gives the highest probability.\n\n1013\nMaximum Likelihood\nWe compute:\n0 = d\ndθ P(X | θ)\n= d\ndθ θ9(1 −θ)4\n= 9θ8(1 −θ)4 −4θ9(1 −θ)3\n= θ8(1 −θ)3(9 −13θ).\n(A.6)\nThis has three solutions: 0, 1 and 9/13. The ﬁrst two are clearly minima, not maxima as they\nassign probability 0 to our sequence. The ﬁnal value does not assign zero probability to our\nsequence, and thus must be the maximum likelihood estimate ˆθ = 9/13.\nA.7.2 Numerical Optimization and the Negative Log-Likelihood\nThe previous example is nice, but what if we have billions of parameters and data exam-\nples?\nFirst, notice that if we make the assumption that all the data examples are independent, we\ncan no longer practically consider the likelihood itself as it is a product of many probabilities.\nIndeed, each probability is in [0, 1], say typically of value about 1/2, and the product of\n(1/2)1000000000 is far below machine precision. We cannot work with that directly.\nHowever, recall that the logarithm turns products to sums, in which case\nlog((1/2)1000000000) = 1000000000 · log(1/2) ≈−301029995.6 . . .\n(A.7)\nThis number ﬁts perfectly within even a single precision 32-bit ﬂoat. Thus, we should consider\nthe log-likelihood, which is\nlog(P(X | θ)).\n(A.8)\nSince the function x 7→log(x) is increasing, maximizing the likelihood is the same thing\nas maximizing the log-likelihood. Indeed in Section A.9 we will see this reasoning applied\nwhen working with the speciﬁc example of the naive Bayes classiﬁer.\n\n1014\nMathematics for Deep Learning\nWe often work with loss functions, where we wish to minimize the loss. We may turn max-\nimum likelihood into the minimization of a loss by taking −log(P(X | θ)), which is the\nnegative log-likelihood.\nTo illustrate this, consider the coin ﬂipping problem from before, and pretend that we do not\nknow the closed form solution. We may compute that\n−log(P(X | θ)) = −log(θnH (1 −θ)nT ) = −(nH log(θ) + nT log(1 −θ)).\n(A.9)\nThis can be written into code, and freely optimized even for billions of coin ﬂips.\n# Set up our data\nn_H = 8675309\nn_T = 256245\n# Initialize our paramteres\ntheta = torch.tensor(0.5, requires_grad=True)\n# Perform gradient descent\nlr = 1e-9\nfor iter in range(100):\nloss = -(n_H * torch.log(theta) + n_T * torch.log(1 - theta))\nloss.backward()\nwith torch.no_grad():\ntheta -= lr * theta.grad\ntheta.grad.zero_()\n# Check output\ntheta, n_H / (n_H + n_T)\n(tensor(0.9713, requires_grad=True), 0.9713101437890875)\nNumerical convenience is not the only reason why people like to use negative log-likelihoods.\nThere are several other reasons why it is preferable.\nThe second reason we consider the log-likelihood is the simpliﬁed application of calculus\nrules. As discussed above, due to independence assumptions, most probabilities we encounter\nin machine learning are products of individual probabilities.\nP(X | θ) = p(x1 | θ) · p(x2 | θ) · · · p(xn | θ).\n(A.10)\nThis means that if we directly apply the product rule to compute a derivative we get\n∂\n∂θ P(X | θ) =\n( ∂\n∂θ P(x1 | θ)\n)\n· P(x2 | θ) · · · P(xn | θ)\n+ P(x1 | θ) ·\n( ∂\n∂θ P(x2 | θ)\n)\n· · · P(xn | θ)\n...\n+ P(x1 | θ) · P(x2 | θ) · · ·\n( ∂\n∂θ P(xn | θ)\n)\n.\n(A.11)\n\n1015\nMaximum Likelihood\nThis requires n(n −1) multiplications, along with (n −1) additions, so it is proportional to\nquadratic time in the inputs! Suﬃcient cleverness in grouping terms will reduce this to linear\ntime, but it requires some thought. For the negative log-likelihood we have instead\n−log (P(X | θ)) = −log(P(x1 | θ)) −log(P(x2 | θ)) · · · −log(P(xn | θ)),\n(A.12)\nwhich then gives\n−∂\n∂θ log (P(X | θ)) =\n1\nP(x1 | θ)\n( ∂\n∂θ P(x1 | θ)\n)\n+ · · · +\n1\nP(xn | θ)\n( ∂\n∂θ P(xn | θ)\n)\n.\n(A.13)\nThis requires only n divides and n −1 sums, and thus is linear time in the inputs.\nThe third and ﬁnal reason to consider the negative log-likelihood is the relationship to infor-\nmation theory, which we will discuss in detail in Section A.11. This is a rigorous mathematical\ntheory which gives a way to measure the degree of information or randomness in a random\nvariable. The key object of study in that ﬁeld is the entropy which is\nH(p) = −\n∑\ni\npi log2(pi),\n(A.14)\nwhich measures the randomness of a source. Notice that this is nothing more than the average\n−log probability, and thus if we take our negative log-likelihood and divide by the number\nof data examples, we get a relative of entropy known as cross-entropy. This theoretical inter-\npretation alone would be suﬃciently compelling to motivate reporting the average negative\nlog-likelihood over the dataset as a way of measuring model performance.\nA.7.3 Maximum Likelihood for Continuous Variables\nEverything that we have done so far assumes we are working with discrete random variables,\nbut what if we want to work with continuous ones?\nThe short summary is that nothing at all changes, except we replace all the instances of the\nprobability with the probability density. Recalling that we write densities with lower case p,\nthis means that for example we now say\n−log (p(X | θ)) = −log(p(x1 | θ)) −log(p(x2 | θ)) · · · −log(p(xn | θ)) = −\n∑\ni\nlog(p(xi | θ)).\n(A.15)\nThe question becomes, “Why is this OK?” After all, the reason we introduced densities was\nbecause probabilities of getting speciﬁc outcomes themselves was zero, and thus is not the\nprobability of generating our data for any set of parameters zero?\nIndeed, this is the case, and understanding why we can shift to densities is an exercise in\ntracing what happens to the epsilons.\nLet’s ﬁrst re-deﬁne our goal. Suppose that for continuous random variables we no longer want\nto compute the probability of getting exactly the right value, but instead matching to within\nsome range ϵ. For simplicity, we assume our data is repeated observations x1, . . ., xN of\n\n1016\nMathematics for Deep Learning\n284\nidentically distributed random variables X1, . . ., XN. As we have seen previously, this can be\nwritten as\nP(X1 ∈[x1, x1 + ϵ], X2 ∈[x2, x2 + ϵ], . . ., XN ∈[xN, xN + ϵ] | θ)\n≈ϵ N p(x1 | θ) · p(x2 | θ) · · · p(xn | θ).\n(A.16)\nThus, if we take negative logarithms of this we obtain\n−log(P(X1 ∈[x1, x1 + ϵ], X2 ∈[x2, x2 + ϵ], . . ., XN ∈[xN, xN + ϵ] | θ))\n≈−N log(ϵ) −\n∑\ni\nlog(p(xi | θ)).\n(A.17)\nIf we examine this expression, the only place that the ϵ occurs is in the additive constant\n−N log(ϵ). This does not depend on the parameters θ at all, so the optimal choice of θ does\nnot depend on our choice of ϵ! If we demand four digits or four-hundred, the best choice of\nθ remains the same, thus we may freely drop the epsilon to see that what we want to optimize\nis\n−\n∑\ni\nlog(p(xi | θ)).\n(A.18)\nThus, we see that the maximum likelihood point of view can operate with continuous ran-\ndom variables as easily as with discrete ones by replacing the probabilities with probability\ndensities.\nA.7.4 Summary\n• The maximum likelihood principle tells us that the best ﬁt model for a given dataset is the\none that generates the data with the highest probability.\n• Often people work with the negative log-likelihood instead for a variety of reasons: nu-\nmerical stability, conversion of products to sums (and the resulting simpliﬁcation of\ngradient computations), and theoretical ties to information theory.\n• While simplest to motivate in the discrete setting, it may be freely generalized to the con-\ntinuous setting as well by maximizing the probability density assigned to the datapoints.\nA.7.5 Exercises\n1. Suppose that you know that a non-negative random variable has density αe−αx for some\nvalue α > 0. You obtain a single observation from the random variable which is the\nnumber 3. What is the maximum likelihood estimate for α?\n2. Suppose that you have a dataset of samples {xi}N\ni=1 drawn from a Gaussian with unknown\nmean, but variance 1. What is the maximum likelihood estimate for the mean?\nDiscussions284.\n\n1017\nDistributions\nA.8 Distributions\nNow that we have learned how to work with probability in both the discrete and the continuous\nsetting, let’s get to know some of the common distributions encountered. Depending on the\narea of machine learning, we may need to be familiar with vastly more of these, or for some\nareas of deep learning potentially none at all. This is, however, a good basic list to be familiar\nwith. Let’s ﬁrst import some common libraries.\n%matplotlib inline\nfrom math import erf, factorial\nimport torch\nfrom IPython import display\nfrom d2l import torch as d2l\ntorch.pi = torch.acos(torch.zeros(1)) * 2\n# Define pi in torch\nA.8.1 Bernoulli\nThis is the simplest random variable usually encountered. This random variable encodes a\ncoin ﬂip which comes up 1 with probability p and 0 with probability 1 −p. If we have a\nrandom variable X with this distribution, we will write\nX ∼Bernoulli(p).\n(A.1)\nThe cumulative distribution function is\nF(x) =\n\n\n0\nx < 0,\n1 −p\n0 ≤x < 1,\n1\nx >= 1.\n(A.2)\nThe probability mass function is plotted below.\np = 0.3\nd2l.set_figsize()\nd2l.plt.stem([0, 1], [1 - p, p], use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\nNow, let’s plot the cumulative distribution function (A.2).\nx = torch.arange(-1, 2, 0.01)\ndef F(x):\n(continues on next page)\n\n1018\nMathematics for Deep Learning\n(continued from previous page)\nreturn 0 if x < 0 else 1 if x > 1 else 1 - p\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\nIf X ∼Bernoulli(p), then:\n• µX = p,\n• σ2\nX = p(1 −p).\nWe can sample an array of arbitrary shape from a Bernoulli random variable as follows.\n1*(torch.rand(10, 10) < p)\ntensor([[0, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n[1, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n[1, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n[1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n[1, 0, 1, 1, 1, 0, 0, 0, 1, 0],\n[1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n[1, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n(continues on next page)\n\n1019\nDistributions\n(continued from previous page)\n[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n[1, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\nA.8.2 Discrete Uniform\nThe next commonly encountered random variable is a discrete uniform. For our discussion\nhere, we will assume that it is supported on the integers {1, 2, . . ., n}, however any other set\nof values can be freely chosen. The meaning of the word uniform in this context is that every\npossible value is equally likely. The probability for each value i ∈{1, 2, 3, . . ., n} is pi = 1\nn.\nWe will denote a random variable X with this distribution as\nX ∼U(n).\n(A.3)\nThe cumulative distribution function is\nF(x) =\n\n\n0\nx < 1,\nk\nn\nk ≤x < k + 1 with 1 ≤k < n,\n1\nx >= n.\n(A.4)\nLet’s ﬁrst plot the probability mass function.\nn = 5\nd2l.plt.stem([i+1 for i in range(n)], n*[1 / n], use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\nNow, let’s plot the cumulative distribution function (A.4).\n\n1020\nMathematics for Deep Learning\nx = torch.arange(-1, 6, 0.01)\ndef F(x):\nreturn 0 if x < 1 else 1 if x > n else torch.floor(x) / n\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\nIf X ∼U(n), then:\n• µX = 1+n\n2 ,\n• σ2\nX = n2−1\n12 .\nWe can sample an array of arbitrary shape from a discrete uniform random variable as fol-\nlows.\ntorch.randint(1, n, size=(10, 10))\ntensor([[1, 3, 4, 1, 1, 1, 2, 2, 1, 3],\n[1, 1, 1, 2, 2, 1, 1, 2, 2, 3],\n[2, 3, 2, 2, 4, 3, 2, 2, 3, 2],\n[1, 2, 1, 4, 3, 2, 3, 2, 4, 3],\n[1, 2, 2, 3, 2, 1, 2, 2, 3, 3],\n[2, 4, 1, 1, 2, 1, 1, 2, 3, 4],\n[3, 1, 3, 1, 1, 3, 4, 4, 1, 4],\n[1, 1, 3, 1, 1, 4, 3, 3, 4, 1],\n[1, 2, 3, 2, 4, 4, 3, 4, 3, 3],\n[2, 1, 1, 2, 1, 3, 2, 3, 1, 4]])\nA.8.3 Continuous Uniform\nNext, let’s discuss the continuous uniform distribution. The idea behind this random variable\nis that if we increase the n in the discrete uniform distribution, and then scale it to ﬁt within\nthe interval [a, b], we will approach a continuous random variable that just picks an arbitrary\n\n1021\nDistributions\nvalue in [a, b] all with equal probability. We will denote this distribution as\nX ∼U(a, b).\n(A.5)\nThe probability density function is\np(x) =\n{\n1\nb−a\nx ∈[a, b],\n0\nx < [a, b].\n(A.6)\nThe cumulative distribution function is\nF(x) =\n\n\n0\nx < a,\nx−a\nb−a\nx ∈[a, b],\n1\nx >= b.\n(A.7)\nLet’s ﬁrst plot the probability density function (A.6).\na, b = 1, 3\nx = torch.arange(0, 4, 0.01)\np = (x > a).type(torch.float32)*(x < b).type(torch.float32)/(b-a)\nd2l.plot(x, p, 'x', 'p.d.f.')\nNow, let’s plot the cumulative distribution function (A.7).\ndef F(x):\nreturn 0 if x < a else 1 if x > b else (x - a) / (b - a)\nd2l.plot(x, torch.tensor([F(y) for y in x]), 'x', 'c.d.f.')\nIf X ∼U(a, b), then:\n• µX = a+b\n2 ,\n• σ2\nX = (b−a)2\n12\n.\nWe can sample an array of arbitrary shape from a uniform random variable as follows. Note\nthat it by default samples from a U(0, 1), so if we want a diﬀerent range we need to scale\nit.\n\n1022\nMathematics for Deep Learning\n(b - a) * torch.rand(10, 10) + a\ntensor([[1.7669, 1.1279, 1.4392, 2.4289, 1.6923, 1.3447, 2.7770, 1.3319, 2.\n,→2326,\n1.3150],\n[2.7011, 2.8355, 1.5303, 2.0331, 2.8806, 1.6149, 2.5169, 2.9279, 1.\n,→3378,\n2.5879],\n[1.9821, 1.5423, 2.6820, 1.2674, 1.7578, 1.6478, 2.7660, 1.4664, 1.\n,→1517,\n1.6246],\n[2.1729, 1.9362, 1.2715, 2.9132, 1.3799, 1.8364, 1.5225, 2.3158, 1.\n,→9384,\n2.3300],\n[2.3944, 1.8187, 2.3988, 1.6223, 1.2805, 2.8716, 2.8287, 1.2452, 1.\n,→8449,\n2.4097],\n[1.3478, 2.8731, 2.4662, 2.0276, 1.1287, 2.8155, 1.5838, 1.2206, 2.\n,→2059,\n2.8454],\n[2.4784, 2.5221, 1.1688, 2.7545, 2.6526, 2.9883, 2.1063, 1.6344, 2.\n,→4662,\n1.8148],\n[1.6195, 2.8807, 1.6211, 1.2088, 1.9361, 2.0862, 2.3551, 1.7576, 1.\n,→6081,\n1.5808],\n[2.6168, 2.6342, 2.3619, 2.5629, 2.0102, 2.0918, 1.6956, 1.6710, 1.\n,→4272,\n1.7900],\n[2.2539, 2.6632, 2.0519, 1.5298, 2.2536, 2.0400, 1.8634, 1.5086, 2.\n,→0670,\n1.6122]])\nA.8.4 Binomial\nLet’s make things a little more complex and examine the binomial random variable. This\nrandom variable originates from performing a sequence of n independent experiments, each\n\n1023\nDistributions\nof which has probability p of succeeding, and asking how many successes we expect to\nsee.\nLet’s express this mathematically. Each experiment is an independent random variable Xi\nwhere we will use 1 to encode success, and 0 to encode failure. Since each is an independent\ncoin ﬂip which is successful with probability p, we can say that Xi ∼Bernoulli(p). Then, the\nbinomial random variable is\nX =\nn\n∑\ni=1\nXi.\n(A.8)\nIn this case, we will write\nX ∼Binomial(n, p).\n(A.9)\nTo get the cumulative distribution function, we need to notice that getting exactly k successes\ncan occur in (n\nk\n) =\nn!\nk!(n−k)! ways each of which has a probability of pk(1−p)n−k of occurring.\nThus the cumulative distribution function is\nF(x) =\n\n\n0\nx < 0,\n∑\nm≤k\n( n\nm\n)pm(1 −p)n−m\nk ≤x < k + 1 with 0 ≤k < n,\n1\nx >= n.\n(A.10)\nLet’s ﬁrst plot the probability mass function.\nn, p = 10, 0.2\n# Compute binomial coefficient\ndef binom(n, k):\ncomb = 1\nfor i in range(min(k, n - k)):\ncomb = comb * (n - i) // (i + 1)\nreturn comb\npmf = torch.tensor([p**i * (1-p)**(n - i) * binom(n, i) for i in range(n + 1)])\nd2l.plt.stem([i for i in range(n + 1)], pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\nNow, let’s plot the cumulative distribution function (A.10).\nx = torch.arange(-1, 11, 0.01)\ncmf = torch.cumsum(pmf, dim=0)\ndef F(x):\nreturn 0 if x < 0 else 1 if x > n else cmf[int(x)]\nd2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\nIf X ∼Binomial(n, p), then:\n\n1024\nMathematics for Deep Learning\n• µX = np,\n• σ2\nX = np(1 −p).\nThis follows from the linearity of expected value over the sum of n Bernoulli random vari-\nables, and the fact that the variance of the sum of independent random variables is the sum\nof the variances. This can be sampled as follows.\nm = torch.distributions.binomial.Binomial(n, p)\nm.sample(sample_shape=(10, 10))\ntensor([[3., 3., 3., 3., 0., 2., 1., 3., 2., 1.],\n[4., 0., 0., 4., 2., 1., 0., 3., 0., 1.],\n[3., 2., 3., 0., 1., 2., 3., 1., 1., 2.],\n[4., 1., 1., 4., 3., 2., 1., 2., 1., 3.],\n[2., 2., 1., 1., 3., 3., 2., 1., 2., 2.],\n[3., 1., 4., 5., 2., 1., 2., 2., 0., 0.],\n[3., 2., 0., 2., 0., 1., 2., 5., 2., 1.],\n[2., 2., 5., 0., 1., 2., 1., 4., 3., 4.],\n[0., 3., 2., 0., 1., 0., 2., 2., 1., 3.],\n[2., 3., 1., 1., 1., 2., 2., 2., 2., 2.]])\nA.8.5 Poisson\n\n1025\nDistributions\nLet’s now perform a thought experiment. We are standing at a bus stop and we want to know\nhow many buses will arrive in the next minute. Let’s start by considering X(1) ∼Bernoulli(p)\nwhich is simply the probability that a bus arrives in the one minute window. For bus stops far\nfrom an urban center, this might be a pretty good approximation. We may never see more\nthan one bus in a minute.\nHowever, if we are in a busy area, it is possible or even likely that two buses will arrive. We\ncan model this by splitting our random variable into two parts for the ﬁrst 30 seconds, or the\nsecond 30 seconds. In this case we can write\nX(2) ∼X(2)\n1\n+ X(2)\n2 ,\n(A.11)\nwhere X(2) is the total sum, and X(2)\ni\n∼Bernoulli(p/2). The total distribution is then X(2) ∼\nBinomial(2, p/2).\nWhy stop here? Let’s continue to split that minute into n parts. By the same reasoning as\nabove, we see that\nX(n) ∼Binomial(n, p/n).\n(A.12)\nConsider these random variables. By the previous section, we know that (A.12) has mean\nµX(n) = n(p/n) = p, and variance σ2\nX(n) = n(p/n)(1 −(p/n)) = p(1 −p/n). If we take\nn →∞, we can see that these numbers stabilize to µX(∞) = p, and variance σ2\nX(∞) = p. This\nindicates that there could be some random variable we can deﬁne in this inﬁnite subdivision\nlimit.\nThis should not come as too much of a surprise, since in the real world we can just count the\nnumber of bus arrivals, however it is nice to see that our mathematical model is well deﬁned.\nThis discussion can be made formal as the law of rare events.\nFollowing through this reasoning carefully, we can arrive at the following model. We will\nsay that X ∼Poisson(λ) if it is a random variable which takes the values {0, 1, 2, . . .} with\nprobability\npk = λke−λ\nk!\n.\n(A.13)\nThe value λ > 0 is known as the rate (or the shape parameter), and denotes the average\nnumber of arrivals we expect in one unit of time.\nWe may sum this probability mass function to get the cumulative distribution function.\nF(x) =\n{\n0\nx < 0,\ne−λ ∑k\nm=0\nλm\nm!\nk ≤x < k + 1 with 0 ≤k.\n(A.14)\nLet’s ﬁrst plot the probability mass function (A.13).\nlam = 5.0\nxs = [i for i in range(20)]\n(continues on next page)\n\n1026\nMathematics for Deep Learning\n(continued from previous page)\npmf = torch.tensor([torch.exp(torch.tensor(-lam)) * lam**k\n/ factorial(k) for k in xs])\nd2l.plt.stem(xs, pmf, use_line_collection=True)\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.show()\nNow, let’s plot the cumulative distribution function (A.14).\nx = torch.arange(-1, 21, 0.01)\ncmf = torch.cumsum(pmf, dim=0)\ndef F(x):\nreturn 0 if x < 0 else 1 if x > n else cmf[int(x)]\nd2l.plot(x, torch.tensor([F(y) for y in x.tolist()]), 'x', 'c.d.f.')\nAs we saw above, the means and variances are particularly concise. If X ∼Poisson(λ),\nthen:\n• µX = λ,\n• σ2\nX = λ.\n\n1027\nDistributions\nThis can be sampled as follows.\nm = torch.distributions.poisson.Poisson(lam)\nm.sample((10, 10))\ntensor([[ 7.,\n2.,\n1.,\n6.,\n2., 10., 10.,\n6.,\n2.,\n4.],\n[ 5.,\n2.,\n6.,\n4.,\n2.,\n4.,\n5.,\n3.,\n2.,\n9.],\n[ 3.,\n6.,\n6.,\n5.,\n3.,\n9., 12.,\n7.,\n4.,\n6.],\n[ 8., 10.,\n9.,\n4.,\n4.,\n7.,\n5.,\n4.,\n3.,\n8.],\n[ 2.,\n2., 10.,\n6.,\n7.,\n5.,\n9.,\n1.,\n7.,\n7.],\n[ 6.,\n4.,\n3.,\n8.,\n4.,\n2.,\n3.,\n5.,\n3.,\n5.],\n[ 0.,\n3.,\n3.,\n4.,\n7.,\n6.,\n2.,\n3.,\n4.,\n4.],\n[ 8.,\n5.,\n4.,\n2.,\n5.,\n5.,\n9.,\n7.,\n8.,\n4.],\n[ 5.,\n4.,\n7.,\n1.,\n2., 11.,\n2.,\n9.,\n3.,\n5.],\n[ 2.,\n9.,\n7.,\n8.,\n8.,\n6.,\n1.,\n2.,\n6.,\n7.]])\nA.8.6 Gaussian\nNow Let’s try a diﬀerent, but related experiment. Let’s say we again are performing n in-\ndependent Bernoulli(p) measurements Xi. The distribution of the sum of these is X(n) ∼\nBinomial(n, p). Rather than taking a limit as n increases and p decreases, Let’s ﬁx p, and\nthen send n →∞. In this case µX(n) = np →∞and σ2\nX(n) = np(1 −p) →∞, so there is\nno reason to think this limit should be well deﬁned.\nHowever, not all hope is lost! Let’s just make the mean and variance be well behaved by\ndeﬁning\nY (n) = X(n) −µX(n)\nσX(n)\n.\n(A.15)\nThis can be seen to have mean zero and variance one, and so it is plausible to believe that it\nwill converge to some limiting distribution. If we plot what these distributions look like, we\nwill become even more convinced that it will work.\np = 0.2\nns = [1, 10, 100, 1000]\nd2l.plt.figure(figsize=(10, 3))\nfor i in range(4):\nn = ns[i]\npmf = torch.tensor([p**i * (1-p)**(n-i) * binom(n, i)\nfor i in range(n + 1)])\nd2l.plt.subplot(1, 4, i + 1)\nd2l.plt.stem([(i - n*p)/torch.sqrt(torch.tensor(n*p*(1 - p)))\nfor i in range(n + 1)], pmf,\nuse_line_collection=True)\nd2l.plt.xlim([-4, 4])\nd2l.plt.xlabel('x')\nd2l.plt.ylabel('p.m.f.')\nd2l.plt.title(\"n = {}\".format(n))\nd2l.plt.show()\n\n1028\nMathematics for Deep Learning\nOne thing to note: compared to the Poisson case, we are now dividing by the standard devia-\ntion which means that we are squeezing the possible outcomes into smaller and smaller areas.\nThis is an indication that our limit will no longer be discrete, but rather continuous.\nA derivation of what occurs is beyond the scope of this document, but the central limit the-\norem states that as n →∞, this will yield the Gaussian Distribution (or sometimes normal\ndistribution). More explicitly, for any a, b:\nlim\nn→∞P(Y (n) ∈[a, b]) = P(N(0, 1) ∈[a, b]),\n(A.16)\nwhere we say a random variable is normally distributed with given mean µ and variance σ2,\nwritten X ∼N(µ, σ2) if X has density\npX(x) =\n1\n√\n2πσ2 e−(x−µ)2\n2σ2 .\n(A.17)\nLet’s ﬁrst plot the probability density function (A.17).\nmu, sigma = 0, 1\nx = torch.arange(-3, 3, 0.01)\np = 1 / torch.sqrt(2 * torch.pi * sigma**2) * torch.exp(\n-(x - mu)**2 / (2 * sigma**2))\nd2l.plot(x, p, 'x', 'p.d.f.')\n\n1029\nDistributions\nNow, let’s plot the cumulative distribution function. It is beyond the scope of this appendix,\nbut the Gaussian c.d.f. does not have a closed-form formula in terms of more elementary\nfunctions. We will use erf which provides a way to compute this integral numerically.\ndef phi(x):\nreturn (1.0 + erf((x - mu) / (sigma * torch.sqrt(torch.tensor(2.))))) / 2.0\nd2l.plot(x, torch.tensor([phi(y) for y in x.tolist()]), 'x', 'c.d.f.')\nKeen-eyed readers will recognize some of these terms. Indeed, we encountered this integral\nin Section A.5. Indeed we need exactly that computation to see that this pX(x) has total area\none and is thus a valid density.\nOur choice of working with coin ﬂips made computations shorter, but nothing about that\nchoice was fundamental. Indeed, if we take any collection of independent identically dis-\ntributed random variables Xi, and form\nX(N) =\nN\n∑\ni=1\nXi.\n(A.18)\nThen\nX(N) −µX(N)\nσX(N)\n(A.19)\nwill be approximately Gaussian. There are additional requirements needed to make it work,\nmost commonly E[X4] < ∞, but the philosophy is clear.\nThe central limit theorem is the reason why the Gaussian is fundamental to probability, statis-\ntics, and machine learning. Whenever we can say that something we measured is a sum of\nmany small independent contributions, we can assume that the thing being measured will be\nclose to Gaussian.\nThere are many more fascinating properties of Gaussians, and we would like to discuss one\nmore here. The Gaussian is what is known as a maximum entropy distribution. We will get\ninto entropy more deeply in Section A.11, however all we need to know at this point is that it\nis a measure of randomness. In a rigorous mathematical sense, we can think of the Gaussian\n\n1030\nMathematics for Deep Learning\nas the most random choice of random variable with ﬁxed mean and variance. Thus, if we\nknow that our random variable has some mean and variance, the Gaussian is in a sense the\nmost conservative choice of distribution we can make.\nTo close the section, let’s recall that if X ∼N(µ, σ2), then:\n• µX = µ,\n• σ2\nX = σ2.\nWe can sample from the Gaussian (or standard normal) distribution as shown below.\ntorch.normal(mu, sigma, size=(10, 10))\ntensor([[ 0.1100,\n1.2204, -0.0183, -0.0438,\n0.1343, -1.7406, -1.0956, -1.\n,→0103,\n-0.8703,\n0.9853],\n[-1.7443, -0.8289,\n1.8382, -1.2046,\n0.4454, -0.1467, -1.2068, -1.\n,→1904,\n0.7402, -0.9666],\n[ 0.4668, -0.5373, -0.6894, -0.5451,\n0.5279,\n1.7936, -0.3843,\n1.\n,→6206,\n0.3415, -0.6788],\n[-0.4203,\n1.5654,\n0.5024,\n0.5272,\n2.4095, -1.4125,\n0.0776, -0.\n,→7675,\n-2.1896, -0.4801],\n[-0.4477,\n1.2597, -1.7383,\n0.2176,\n2.0329, -0.0337, -0.3814,\n1.\n,→8359,\n0.3762, -1.0739],\n[-0.5160,\n2.0706, -0.0820,\n0.5688, -0.3288,\n1.8007,\n0.4799, -0.\n,→6264,\n0.3517, -0.4274],\n[-1.1953,\n1.4043, -0.8325, -0.2550,\n0.2850,\n0.0788, -1.6121, -1.\n,→0295,\n-0.5723, -0.5142],\n[-0.0106, -0.6287,\n0.6475,\n1.5597,\n0.3415,\n0.8483,\n1.5621,\n0.\n,→0976,\n1.8702, -1.8762],\n[-1.0095, -0.2200,\n0.1529, -0.3789,\n0.1588,\n0.6434,\n0.5891,\n2.\n,→1004,\n-1.4606, -0.9920],\n[-0.2421,\n0.4331,\n0.5397, -1.2819,\n0.7858, -0.5514,\n2.0629,\n0.\n,→3418,\n-2.1320, -0.5923]])\nA.8.7 Exponential Family\nOne shared property for all the distributions listed above is that they all belong to which is\nknown as the exponential family. The exponential family is a set of distributions whose density\ncan be expressed in the following form:\np(x | η) = h(x) · exp (η⊤· T(x) −A(η))\n(A.20)\n\n1031\nDistributions\nAs this deﬁnition can be a little subtle, let’s examine it closely.\nFirst, h(x) is known as the underlying measure or the base measure. This can be viewed as\nan original choice of measure we are modifying with our exponential weight.\nSecond, we have the vector η = (η1, η2, ..., ηl) ∈Rl called the natural parameters or canon-\nical parameters. These deﬁne how the base measure will be modiﬁed. The natural param-\neters enter into the new measure by taking the dot product of these parameters against\nsome function T(·) of x = (x1, x2, ..., xn) ∈Rn and exponentiated. The vector T(x) =\n(T1(x),T2(x), ...,Tl(x)) is called the suﬃcient statistics for η. This name is used since the\ninformation represented by T(x) is suﬃcient to calculate the probability density and no other\ninformation from the sample x’s are required.\nThird, we have A(η), which is referred to as the cumulant function, which ensures that the\nabove distribution (A.20) integrates to one, i.e.,\nA(η) = log\n[∫\nh(x) · exp (η⊤· T(x)) dx\n]\n.\n(A.21)\nTo be concrete, let’s consider the Gaussian. Assuming that x is an univariate variable, we saw\nthat it had a density of\np(x | µ, σ) =\n1\n√\n2πσ2 · exp\n{−(x −µ)2\n2σ2\n}\n=\n1\n√\n2π\n· exp\n{ µ\nσ2 x −\n1\n2σ2 x2 −\n( 1\n2σ2 µ2 + log(σ)\n)}\n.\n(A.22)\nThis matches the deﬁnition of the exponential family with:\n• underlying measure: h(x) =\n1\n√\n2π ,\n• natural parameters: η =\n[η1\nη2\n]\n=\n[\nµ\nσ2\n1\n2σ2\n]\n,\n• suﬃcient statistics: T(x) =\n[ x\n−x2\n]\n, and\n• cumulant function: A(η) =\n1\n2σ2 µ2 + log(σ) = η2\n1\n4η2 −1\n2 log(2η2).\nIt is worth noting that the exact choice of each of above terms is somewhat arbitrary. Indeed,\nthe important feature is that the distribution can be expressed in this form, not the exact form\nitself.\nAs we allude to in Section 4.1.2, a widely used technique is to assume that the ﬁnal output y\nfollows an exponential family distribution. The exponential family is a common and powerful\nfamily of distributions encountered frequently in machine learning.\nA.8.8 Summary\n• Bernoulli random variables can be used to model events with a yes/no outcome.\n\n1032\nMathematics for Deep Learning\n285\n• Discrete uniform distributions model selects from a ﬁnite set of possibilities.\n• Continuous uniform distributions select from an interval.\n• Binomial distributions model a series of Bernoulli random variables, and count the number\nof successes.\n• Poisson random variables model the arrival of rare events.\n• Gaussian random variables model the result of adding a large number of independent ran-\ndom variables together.\n• All the above distributions belong to exponential family.\nA.8.9 Exercises\n1. What is the standard deviation of a random variable that is the diﬀerence X −Y of two\nindependent binomial random variables X,Y ∼Binomial(16, 1/2).\n2. If we take a Poisson random variable X ∼Poisson(λ) and consider (X−λ)/\n√\nλ as λ →∞,\nwe can show that this becomes approximately Gaussian. Why does this make sense?\n3. What is the probability mass function for a sum of two discrete uniform random variables\non n elements?\nDiscussions285.\nA.9 Naive Bayes\nThroughout the previous sections, we learned about the theory of probability and random\nvariables. To put this theory to work, let’s introduce the naive Bayes classiﬁer. This uses\nnothing but probabilistic fundamentals to allow us to perform classiﬁcation of digits.\nLearning is all about making assumptions. If we want to classify a new data example that we\nhave never seen before we have to make some assumptions about which data examples are\nsimilar to each other. The naive Bayes classiﬁer, a popular and remarkably clear algorithm,\nassumes all features are independent from each other to simplify the computation. In this\nsection, we will apply this model to recognize characters in images.\n%matplotlib inline\nimport math\nimport torch\nimport torchvision\nfrom d2l import torch as d2l\nd2l.use_svg_display()\n\n1033\nNaive Bayes\nA.9.1 Optical Character Recognition\nMNIST (LeCun et al., 1998) is one of widely used datasets. It contains 60,000 images for\ntraining and 10,000 images for validation. Each image contains a handwritten digit from 0 to\n9. The task is classifying each image into the corresponding digit.\nGluon provides a MNIST class in the data.vision module to automatically retrieve the\ndataset from the Internet. Subsequently, Gluon will use the already-downloaded local copy.\nWe specify whether we are requesting the training set or the test set by setting the value of the\nparameter train to True or False, respectively. Each image is a grayscale image with both\nwidth and height of 28 with shape (28,28,1). We use a customized transformation to remove\nthe last channel dimension. In addition, the dataset represents each pixel by an unsigned 8-bit\ninteger. We quantize them into binary features to simplify the problem.\ndata_transform = torchvision.transforms.Compose([\ntorchvision.transforms.ToTensor(),\nlambda x: torch.floor(x * 255 / 128).squeeze(dim=0)\n])\nmnist_train = torchvision.datasets.MNIST(\nroot='./temp', train=True, transform=data_transform, download=True)\nmnist_test = torchvision.datasets.MNIST(\nroot='./temp', train=False, transform=data_transform, download=True)\nWe can access a particular example, which contains the image and the corresponding la-\nbel.\nimage, label = mnist_train[2]\nimage.shape, label\n(torch.Size([28, 28]), 4)\nOur example, stored here in the variable image, corresponds to an image with a height and\nwidth of 28 pixels.\nimage.shape, image.dtype\n(torch.Size([28, 28]), torch.float32)\nOur code stores the label of each image as a scalar. Its type is a 32-bit integer.\nlabel, type(label)\n(4, int)\nWe can also access multiple examples at the same time.\n\n1034\nMathematics for Deep Learning\nimages = torch.stack([mnist_train[i][0] for i in range(10, 38)], dim=0)\nlabels = torch.tensor([mnist_train[i][1] for i in range(10, 38)])\nimages.shape, labels.shape\n(torch.Size([28, 28, 28]), torch.Size([28]))\nLet’s visualize these examples.\nd2l.show_images(images, 2, 9);\nA.9.2 The Probabilistic Model for Classiﬁcation\nIn a classiﬁcation task, we map an example into a category. Here an example is a grayscale\n28×28 image, and a category is a digit. (Refer to Section 4.1 for a more detailed explanation.)\nOne natural way to express the classiﬁcation task is via the probabilistic question: what is the\nmost likely label given the features (i.e., image pixels)? Denote by x ∈Rd the features of\nthe example and y ∈R the label. Here features are image pixels, where we can reshape a 2-\ndimensional image to a vector so that d = 282 = 784, and labels are digits. The probability\nof the label given the features is p(y | x). If we are able to compute these probabilities, which\nare p(y | x) for y = 0, . . ., 9 in our example, then the classiﬁer will output the prediction ˆy\ngiven by the expression:\nˆy = argmax p(y | x).\n(A.1)\nUnfortunately, this requires that we estimate p(y | x) for every value of x = x1, ..., xd.\nImagine that each feature could take one of 2 values. For example, the feature x1 = 1 might\nsignify that the word apple appears in a given document and x1 = 0 would signify that it\ndoes not. If we had 30 such binary features, that would mean that we need to be prepared to\nclassify any of 230 (over 1 billion!) possible values of the input vector x.\nMoreover, where is the learning? If we need to see every single possible example in order to\npredict the corresponding label then we are not really learning a pattern but just memorizing\nthe dataset.\nA.9.3 The Naive Bayes Classiﬁer\n\n1035\nNaive Bayes\nFortunately, by making some assumptions about conditional independence, we can introduce\nsome inductive bias and build a model capable of generalizing from a comparatively modest\nselection of training examples. To begin, let’s use Bayes theorem, to express the classiﬁer\nas\nˆy = argmaxy p(y | x) = argmaxy\np(x | y)p(y)\np(x)\n.\n(A.2)\nNote that the denominator is the normalizing term p(x) which does not depend on the value\nof the label y. As a result, we only need to worry about comparing the numerator across\ndiﬀerent values of y. Even if calculating the denominator turned out to be intractable, we\ncould get away with ignoring it, so long as we could evaluate the numerator. Fortunately,\neven if we wanted to recover the normalizing constant, we could. We can always recover the\nnormalization term since ∑\ny p(y | x) = 1.\nNow, let’s focus on p(x | y). Using the chain rule of probability, we can express the term\np(x | y) as\np(x1 | y) · p(x2 | x1, y) · ... · p(xd | x1, ..., xd−1, y).\n(A.3)\nBy itself, this expression does not get us any further. We still must estimate roughly 2d\nparameters. However, if we assume that the features are conditionally independent of each\nother, given the label, then suddenly we are in much better shape, as this term simpliﬁes to\n∏\ni p(xi | y), giving us the predictor\nˆy = argmaxy\nd\n∏\ni=1\np(xi | y)p(y).\n(A.4)\nIf we can estimate p(xi = 1 | y) for every i and y, and save its value in Pxy[i, y], here Pxy is\na d × n matrix with n being the number of classes and y ∈{1, . . ., n}, then we can also use\nthis to estimate p(xi = 0 | y), i.e.,\np(xi = ti | y) =\n{\nPxy[i, y]\nfor ti = 1;\n1 −Pxy[i, y]\nfor ti = 0.\n(A.5)\nIn addition, we estimate p(y) for every y and save it in Py[y], with Py a n-length vector.\nThen, for any new example t = (t1, t2, . . ., td), we could compute\nˆy = argmaxy p(y)\nd\n∏\ni=1\np(xt = ti | y)\n= argmaxy Py[y]\nd\n∏\ni=1\nPxy[i, y]ti (1 −Pxy[i, y])1−ti\n(A.6)\nfor any y. So our assumption of conditional independence has taken the complexity of our\nmodel from an exponential dependence on the number of features O(2dn) to a linear depen-\ndence, which is O(dn).\n\n1036\nMathematics for Deep Learning\nA.9.4 Training\nThe problem now is that we do not know Pxy and Py. So we need to estimate their values\ngiven some training data ﬁrst. This is training the model. Estimating Py is not too hard.\nSince we are only dealing with 10 classes, we may count the number of occurrences ny for\neach of the digits and divide it by the total amount of data n. For instance, if digit 8 occurs\nn8 = 5, 800 times and we have a total of n = 60, 000 images, the probability estimate is\np(y = 8) = 0.0967.\nX = torch.stack([mnist_train[i][0] for i in range(len(mnist_train))], dim=0)\nY = torch.tensor([mnist_train[i][1] for i in range(len(mnist_train))])\nn_y = torch.zeros(10)\nfor y in range(10):\nn_y[y] = (Y == y).sum()\nP_y = n_y / n_y.sum()\nP_y\ntensor([0.0987, 0.1124, 0.0993, 0.1022, 0.0974, 0.0904, 0.0986, 0.1044, 0.0975,\n0.0992])\nNow on to slightly more diﬃcult things Pxy. Since we picked black and white images, p(xi |\ny) denotes the probability that pixel i is switched on for class y. Just like before we can\ngo and count the number of times niy such that an event occurs and divide it by the total\nnumber of occurrences of y, i.e., ny. But there is something slightly troubling: certain pixels\nmay never be black (e.g., for well cropped images the corner pixels might always be white).\nA convenient way for statisticians to deal with this problem is to add pseudo counts to all\noccurrences. Hence, rather than niy we use niy + 1 and instead of ny we use ny + 2 (since\nthere are two possible values pixel i can take - it can either be black or white). This is also\ncalled Laplace Smoothing. It may seem ad-hoc, however it can be motivated from a Bayesian\npoint-of-view by a Beta-binomial model.\nn_x = torch.zeros((10, 28, 28))\nfor y in range(10):\nn_x[y] = torch.tensor(X.numpy()[Y.numpy() == y].sum(axis=0))\nP_xy = (n_x + 1) / (n_y + 2).reshape(10, 1, 1)\nd2l.show_images(P_xy, 2, 5);\nBy visualizing these 10 × 28 × 28 probabilities (for each pixel for each class) we could get\nsome mean looking digits.\nNow we can use (A.6) to predict a new image. Given x, the following functions computes\np(x | y)p(y) for every y.\ndef bayes_pred(x):\nx = x.unsqueeze(0)\n# (28, 28) -> (1, 28, 28)\n(continues on next page)\n\n1037\nNaive Bayes\n(continued from previous page)\np_xy = P_xy * x + (1 - P_xy)*(1 - x)\np_xy = p_xy.reshape(10, -1).prod(dim=1)\n# p(x|y)\nreturn p_xy * P_y\nimage, label = mnist_test[0]\nbayes_pred(image)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\nThis went horribly wrong! To ﬁnd out why, let’s look at the per pixel probabilities. They\nare typically numbers between 0.001 and 1. We are multiplying 784 of them. At this point\nit is worth mentioning that we are calculating these numbers on a computer, hence with a\nﬁxed range for the exponent. What happens is that we experience numerical underﬂow, i.e.,\nmultiplying all the small numbers leads to something even smaller until it is rounded down\nto zero. We discussed this as a theoretical issue in Section A.7, but we see the phenomena\nclearly here in practice.\nAs discussed in that section, we ﬁx this by use the fact that log ab = log a + log b, i.e., we\nswitch to summing logarithms. Even if both a and b are small numbers, the logarithm values\nshould be in a proper range.\na = 0.1\nprint('underflow:', a**784)\nprint('logarithm is normal:', 784*math.log(a))\nunderflow: 0.0\nlogarithm is normal: -1805.2267129073316\nSince the logarithm is an increasing function, we can rewrite (A.6) as\nˆy = argmaxy log Py[y] +\nd\n∑\ni=1\n[\nti log Pxy[xi, y] + (1 −ti) log(1 −Pxy[xi, y])\n]\n.\n(A.7)\nWe can implement the following stable version:\n\n1038\nMathematics for Deep Learning\nlog_P_xy = torch.log(P_xy)\nlog_P_xy_neg = torch.log(1 - P_xy)\nlog_P_y = torch.log(P_y)\ndef bayes_pred_stable(x):\nx = x.unsqueeze(0)\n# (28, 28) -> (1, 28, 28)\np_xy = log_P_xy * x + log_P_xy_neg * (1 - x)\np_xy = p_xy.reshape(10, -1).sum(axis=1)\n# p(x|y)\nreturn p_xy + log_P_y\npy = bayes_pred_stable(image)\npy\ntensor([-268.9725, -301.7044, -245.1951, -218.8738, -193.4570, -206.0909,\n-292.5226, -114.6257, -220.3313, -163.1784])\nWe may now check if the prediction is correct.\npy.argmax(dim=0) == label\ntensor(True)\nIf we now predict a few validation examples, we can see the Bayes classiﬁer works pretty\nwell.\ndef predict(X):\nreturn [bayes_pred_stable(x).argmax(dim=0).type(torch.int32).item()\nfor x in X]\nX = torch.stack([mnist_test[i][0] for i in range(18)], dim=0)\ny = torch.tensor([mnist_test[i][1] for i in range(18)])\npreds = predict(X)\nd2l.show_images(X, 2, 9, titles=[str(d) for d in preds]);\nFinally, let’s compute the overall accuracy of the classiﬁer.\nX = torch.stack([mnist_test[i][0] for i in range(len(mnist_test))], dim=0)\ny = torch.tensor([mnist_test[i][1] for i in range(len(mnist_test))])\npreds = torch.tensor(predict(X), dtype=torch.int32)\nfloat((preds == y).sum()) / len(y)\n# Validation accuracy\n\n1039\nStatistics\n286\n0.8427\nModern deep networks achieve error rates of less than 0.01. The relatively poor performance\nis due to the incorrect statistical assumptions that we made in our model: we assumed that\neach and every pixel are independently generated, depending only on the label. This is clearly\nnot how humans write digits, and this wrong assumption led to the downfall of our overly\nnaive (Bayes) classiﬁer.\nA.9.5 Summary\n• Using Bayes’ rule, a classiﬁer can be made by assuming all observed features are indepen-\ndent.\n• This classiﬁer can be trained on a dataset by counting the number of occurrences of com-\nbinations of labels and pixel values.\n• This classiﬁer was the gold standard for decades for tasks such as spam detection.\nA.9.6 Exercises\n1. Consider the dataset [[0, 0], [0, 1], [1, 0], [1, 1]] with labels given by the XOR of the two\nelements [0, 1, 1, 0]. What are the probabilities for a Naive Bayes classiﬁer built on this\ndataset. Does it successfully classify our points? If not, what assumptions are violated?\n2. Suppose that we did not use Laplace smoothing when estimating probabilities and a data\nexample arrived at testing time which contained a value never observed in training. What\nwould the model output?\n3. The naive Bayes classiﬁer is a speciﬁc example of a Bayesian network, where the depen-\ndence of random variables are encoded with a graph structure. While the full theory is\nbeyond the scope of this section (see Koller and Friedman (2009) for full details), explain\nwhy allowing explicit dependence between the two input variables in the XOR model\nallows for the creation of a successful classiﬁer.\nDiscussions286.\nA.10 Statistics\nUndoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art\nand high accurate models is crucial. However, it is often unclear when improvements are\n\n1040\nMathematics for Deep Learning\nsigniﬁcant, or only the result of random ﬂuctuations in the training process. To be able to\ndiscuss uncertainty in estimated values, we must learn some statistics.\nThe earliest reference of statistics can be traced back to an Arab scholar Al-Kindi in the 9th-\ncentury, who gave a detailed description of how to use statistics and frequency analysis to\ndecipher encrypted messages. After 800 years, the modern statistics arose from Germany\nin 1700s, when the researchers focused on the demographic and economic data collection\nand analysis. Today, statistics is the science subject that concerns the collection, processing,\nanalysis, interpretation and visualization of data. What is more, the core theory of statistics\nhas been widely used in the research within academia, industry, and government.\nMore speciﬁcally, statistics can be divided to descriptive statistics and statistical inference. The\nformer focus on summarizing and illustrating the features of a collection of observed data,\nwhich is referred to as a sample. The sample is drawn from a population, denotes the total set\nof similar individuals, items, or events of our experiment interests. Contrary to descriptive\nstatistics, statistical inference further deduces the characteristics of a population from the given\nsamples, based on the assumptions that the sample distribution can replicate the population\ndistribution at some degree.\nYou may wonder: “What is the essential diﬀerence between machine learning and statistics?”\nFundamentally speaking, statistics focuses on the inference problem. This type of problems\nincludes modeling the relationship between the variables, such as causal inference, and testing\nthe statistically signiﬁcance of model parameters, such as A/B testing. In contrast, machine\nlearning emphasizes on making accurate predictions, without explicitly programming and\nunderstanding each parameter’s functionality.\nIn this section, we will introduce three types of statistics inference methods: evaluating and\ncomparing estimators, conducting hypothesis tests, and constructing conﬁdence intervals.\nThese methods can help us infer the characteristics of a given population, i.e., the true pa-\nrameter θ. For brevity, we assume that the true parameter θ of a given population is a scalar\nvalue. It is straightforward to extend to the case where θ is a vector or a tensor, thus we omit\nit in our discussion.\nA.10.1 Evaluating and Comparing Estimators\nIn statistics, an estimator is a function of given samples used to estimate the true parame-\nter θ. We will write ˆθn = ˆf (x1, . . ., xn) for the estimate of θ after observing the samples\n{x1, x2, . . ., xn}.\nWe have seen simple examples of estimators before in section Section A.7. If you have a\nnumber of samples from a Bernoulli random variable, then the maximum likelihood estimate\nfor the probability the random variable is one can be obtained by counting the number of ones\nobserved and dividing by the total number of samples. Similarly, an exercise asked you to\nshow that the maximum likelihood estimate of the mean of a Gaussian given a number of\nsamples is given by the average value of all the samples. These estimators will almost never\ngive the true value of the parameter, but ideally for a large number of samples the estimate\nwill be close.\n\n1041\nStatistics\nAs an example, we show below the true density of a Gaussian random variable with mean zero\nand variance one, along with a collection samples from that Gaussian. We constructed the y\ncoordinate so every point is visible and the relationship to the original density is clearer.\nimport torch\nfrom d2l import torch as d2l\ntorch.pi = torch.acos(torch.zeros(1)) * 2\n#define pi in torch\n# Sample datapoints and create y coordinate\nepsilon = 0.1\ntorch.manual_seed(8675309)\nxs = torch.randn(size=(300,))\nys = torch.tensor(\n[torch.sum(torch.exp(-(xs[:i] - xs[i])**2 / (2 * epsilon**2))\\\n/ torch.sqrt(2*torch.pi*epsilon**2)) / len(xs)\\\nfor i in range(len(xs))])\n# Compute true density\nxd = torch.arange(torch.min(xs), torch.max(xs), 0.01)\nyd = torch.exp(-xd**2/2) / torch.sqrt(2 * torch.pi)\n# Plot the results\nd2l.plot(xd, yd, 'x', 'density')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x=0)\nd2l.plt.axvline(x=torch.mean(xs), linestyle='--', color='purple')\nd2l.plt.title(f'sample mean: {float(torch.mean(xs).item()):.2f}')\nd2l.plt.show()\nThere can be many ways to compute an estimator of a parameter ˆθn. In this section, we\nintroduce three common methods to evaluate and compare estimators: the mean squared\nerror, the standard deviation, and statistical bias.\n\n1042\nMathematics for Deep Learning\nMean Squared Error\nPerhaps the simplest metric used to evaluate estimators is the mean squared error (MSE) (or\nl2 loss) estimator which can be deﬁned as\nMSE(ˆθn, θ) = E[(ˆθn −θ)2].\n(A.1)\nThis allows us to quantify the average squared deviation from the true value. MSE is always\nnon-negative. If you have read Section 3.1, you will recognize it as the most commonly used\nregression loss function. As a measure to evaluate an estimator, the closer its value to zero,\nthe closer the estimator is close to the true parameter θ.\nStatistical Bias\nThe MSE provides a natural metric, but we can easily imagine multiple diﬀerent phenomena\nthat might make it large. Two fundamentally important are ﬂuctuation in the estimator due\nto randomness in the dataset, and systematic error in the estimator due to the estimation\nprocedure.\nFirst, let’s measure the systematic error. For an estimator ˆθn, the mathematical illustration of\nstatistical bias can be deﬁned as\nbias(ˆθn) = E(ˆθn −θ) = E(ˆθn) −θ.\n(A.2)\nNote that when bias(ˆθn) = 0, the expectation of the estimator ˆθn is equal to the true value of\nparameter. In this case, we say ˆθn is an unbiased estimator. In general, an unbiased estimator\nis better than a biased estimator since its expectation is the same as the true parameter.\nIt is worth being aware, however, that biased estimators are frequently used in practice.\nThere are cases where unbiased estimators do not exist without further assumptions, or\nare intractable to compute. This may seem like a signiﬁcant ﬂaw in an estimator, however\nthe majority of estimators encountered in practice are at least asymptotically unbiased in\nthe sense that the bias tends to zero as the number of available samples tends to inﬁnity:\nlimn→∞bias(ˆθn) = 0.\nVariance and Standard Deviation\nSecond, let’s measure the randomness in the estimator. Recall from Section A.6, the standard\ndeviation (or standard error) is deﬁned as the squared root of the variance. We may measure\nthe degree of ﬂuctuation of an estimator by measuring the standard deviation or variance of\nthat estimator.\nσˆθn =\n√\nVar(ˆθn) =\n√\nE[(ˆθn −E(ˆθn))2].\n(A.3)\n\n1043\nStatistics\nIt is important to compare (A.3) to (A.1). In this equation we do not compare to the true\npopulation value θ, but instead to E(ˆθn), the expected sample mean. Thus we are not mea-\nsuring how far the estimator tends to be from the true value, but instead we measuring the\nﬂuctuation of the estimator itself.\nThe Bias-Variance Trade-oﬀ\nIt is intuitively clear that these two main components contribute to the mean squared error.\nWhat is somewhat shocking is that we can show that this is actually a decomposition of the\nmean squared error into these two contributions plus a third one. That is to say that we can\nwrite the mean squared error as the sum of the square of the bias, the variance and the\nirreducible error.\nMSE(ˆθn, θ) = E[(ˆθn −θ)2]\n= E[(ˆθn)2] + E[θ2] −2E[ˆθnθ]\n= Var[ˆθn] + E[ˆθn]2 + Var[θ] + E[θ]2 −2E[ˆθn]E[θ]\n= (E[ˆθn] −E[θ])2 + Var[ˆθn] + Var[θ]\n= (E[ˆθn −θ])2 + Var[ˆθn] + Var[θ]\n= (bias[ˆθn])2 + Var(ˆθn) + Var[θ].\n(A.4)\nWe refer the above formula as bias-variance trade-oﬀ. The mean squared error can be di-\nvided into three sources of error: the error from high bias, the error from high variance and\nthe irreducible error. The bias error is commonly seen in a simple model (such as a linear\nregression model), which cannot extract high dimensional relations between the features and\nthe outputs. If a model suﬀers from high bias error, we often say it is underﬁtting or lack of\nﬂexibilty as introduced in (Section 3.6). The high variance usually results from a too complex\nmodel, which overﬁts the training data. As a result, an overﬁtting model is sensitive to small\nﬂuctuations in the data. If a model suﬀers from high variance, we often say it is overﬁtting\nand lack of generalization as introduced in (Section 3.6). The irreducible error is the result\nfrom noise in the θ itself.\nEvaluating Estimators in Code\nSince the standard deviation of an estimator has been implementing by simply calling a.\nstd() for a tensor a, we will skip it but implement the statistical bias and the mean squared\nerror.\n# Statistical bias\ndef stat_bias(true_theta, est_theta):\nreturn(torch.mean(est_theta) - true_theta)\n# Mean squared error\ndef mse(data, true_theta):\nreturn(torch.mean(torch.square(data - true_theta)))\n\n1044\nMathematics for Deep Learning\nTo illustrate the equation of the bias-variance trade-oﬀ, let’s simulate of normal distribution\nN(θ, σ2) with 10, 000 samples. Here, we use a θ = 1 and σ = 4. As the estimator is a\nfunction of the given samples, here we use the mean of the samples as an estimator for true\nθ in this normal distribution N(θ, σ2) .\ntheta_true = 1\nsigma = 4\nsample_len = 10000\nsamples = torch.normal(theta_true, sigma, size=(sample_len, 1))\ntheta_est = torch.mean(samples)\ntheta_est\ntensor(1.0170)\nLet’s validate the trade-oﬀequation by calculating the summation of the squared bias and the\nvariance of our estimator. First, calculate the MSE of our estimator.\nmse(samples, theta_true)\ntensor(16.0298)\nNext, we calculate Var(ˆθn) + [bias(ˆθn)]2 as below. As you can see, the two values agree to\nnumerical precision.\nbias = stat_bias(theta_true, theta_est)\ntorch.square(samples.std(unbiased=False)) + torch.square(bias)\ntensor(16.0298)\nA.10.2 Conducting Hypothesis Tests\nThe most commonly encountered topic in statistical inference is hypothesis testing. While\nhypothesis testing was popularized in the early 20th century, the ﬁrst use can be traced back\nto John Arbuthnot in the 1700s. John tracked 80-year birth records in London and concluded\nthat more men were born than women each year. Following that, the modern signiﬁcance\ntesting is the intelligence heritage by Karl Pearson who invented p-value and Pearson’s chi-\nsquared test, William Gosset who is the father of Student’s t-distribution, and Ronald Fisher\nwho initialed the null hypothesis and the signiﬁcance test.\nA hypothesis test is a way of evaluating some evidence against the default statement about a\npopulation. We refer the default statement as the null hypothesis H0, which we try to reject\nusing the observed data. Here, we use H0 as a starting point for the statistical signiﬁcance\ntesting. The alternative hypothesis HA (or H1) is a statement that is contrary to the null hy-\npothesis. A null hypothesis is often stated in a declarative form which posits a relationship\n\n1045\nStatistics\nbetween variables. It should reﬂect the brief as explicit as possible, and be testable by statistics\ntheory.\nImagine you are a chemist. After spending thousands of hours in the lab, you develop a new\nmedicine which can dramatically improve one’s ability to understand math. To show its magic\npower, you need to test it. Naturally, you may need some volunteers to take the medicine and\nsee whether it can help them learn mathematics better. How do you get started?\nFirst, you will need carefully random selected two groups of volunteers, so that there is no\ndiﬀerence between their mathematical understanding ability measured by some metrics. The\ntwo groups are commonly referred to as the test group and the control group. The test group\n(or treatment group) is a group of individuals who will experience the medicine, while the\ncontrol group represents the group of users who are set aside as a benchmark, i.e., identical\nenvironment setups except taking this medicine. In this way, the inﬂuence of all the variables\nare minimized, except the impact of the independent variable in the treatment.\nSecond, after a period of taking the medicine, you will need to measure the two groups’\nmathematical understanding by the same metrics, such as letting the volunteers do the same\ntests after learning a new mathematical formula. Then, you can collect their performance\nand compare the results. In this case, our null hypothesis will be that there is no diﬀerence\nbetween the two groups, and our alternate will be that there is.\nThis is still not fully formal. There are many details you have to think of carefully. For exam-\nple, what is the suitable metrics to test their mathematical understanding ability? How many\nvolunteers for your test so you can be conﬁdent to claim the eﬀectiveness of your medicine?\nHow long should you run the test? How do you decide if there is a diﬀerence between the\ntwo groups? Do you care about the average performance only, or also the range of variation\nof the scores? And so on.\nIn this way, hypothesis testing provides a framework for experimental design and reasoning\nabout certainty in observed results. If we can now show that the null hypothesis is very unlikely\nto be true, we may reject it with conﬁdence.\nTo complete the story of how to work with hypothesis testing, we need to now introduce\nsome additional terminology and make some of our concepts above formal.\nStatistical Signiﬁcance\nThe statistical signiﬁcance measures the probability of erroneously rejecting the null hypoth-\nesis, H0, when it should not be rejected, i.e.,\nstatistical signiﬁcance = 1 −α = 1 −P(reject H0 | H0 is true).\n(A.5)\nIt is also referred to as the type I error or false positive. The α, is called as the signiﬁcance level\nand its commonly used value is 5%, i.e., 1−α = 95%. The signiﬁcance level can be explained\nas the level of risk that we are willing to take, when we reject a true null hypothesis.\nFig. A.1 shows the observations’ values and probability of a given normal distribution in\n\n1046\nMathematics for Deep Learning\na two-sample hypothesis test. If the observation data example is located outsides the 95%\nthreshold, it will be a very unlikely observation under the null hypothesis assumption. Hence,\nthere might be something wrong with the null hypothesis and we will reject it.\nt\nFig. A.1\nStatistical signiﬁcance.\nStatistical Power\nThe statistical power (or sensitivity) measures the probability of reject the null hypothesis, H0,\nwhen it should be rejected, i.e.,\nstatistical power = 1 −β = 1 −P( fail to reject H0 | H0 is false).\n(A.6)\nRecall that a type I error is error caused by rejecting the null hypothesis when it is true,\nwhereas a type II error is resulted from failing to reject the null hypothesis when it is false.\nA type II error is usually denoted as β, and hence the corresponding statistical power is\n1 −β.\nIntuitively, statistical power can be interpreted as how likely our test will detect a real dis-\ncrepancy of some minimum magnitude at a desired statistical signiﬁcance level. 80% is a\ncommonly used statistical power threshold. The higher the statistical power, the more likely\nwe are to detect true diﬀerences.\nOne of the most common uses of statistical power is in determining the number of samples\nneeded. The probability you reject the null hypothesis when it is false depends on the degree\nto which it is false (known as the eﬀect size) and the number of samples you have. As you\nmight expect, small eﬀect sizes will require a very large number of samples to be detectable\nwith high probability. While beyond the scope of this brief appendix to derive in detail, as an\nexample, want to be able to reject a null hypothesis that our sample came from a mean zero\nvariance one Gaussian, and we believe that our sample’s mean is actually close to one, we\ncan do so with acceptable error rates with a sample size of only 8. However, if we think our\nsample population true mean is close to 0.01, then we’d need a sample size of nearly 80000\nto detect the diﬀerence.\nWe can imagine the power as a water ﬁlter. In this analogy, a high power hypothesis test is\n\n1047\nStatistics\nlike a high quality water ﬁltration system that will reduce harmful substances in the water\nas much as possible. On the other hand, a smaller discrepancy is like a low quality water\nﬁlter, where some relative small substances may easily escape from the gaps. Similarly, if\nthe statistical power is not of enough high power, then the test may not catch the smaller\ndiscrepancy.\nTest Statistic\nA test statistic T(x) is a scalar which summarizes some characteristic of the sample data.\nThe goal of deﬁning such a statistic is that it should allow us to distinguish between diﬀerent\ndistributions and conduct our hypothesis test. Thinking back to our chemist example, if we\nwish to show that one population performs better than the other, it could be reasonable to\ntake the mean as the test statistic. Diﬀerent choices of test statistic can lead to statistical test\nwith drastically diﬀerent statistical power.\nOften, T(X) (the distribution of the test statistic under our null hypothesis) will follow, at\nleast approximately, a common probability distribution such as a normal distribution when\nconsidered under the null hypothesis. If we can derive explicitly such a distribution, and then\nmeasure our test statistic on our dataset, we can safely reject the null hypothesis if our statistic\nis far outside the range that we would expect. Making this quantitative leads us to the notion\nof p-values.\np-value\nThe p-value (or the probability value) is the probability that T(X) is at least as extreme as the\nobserved test statistic T(x) assuming that the null hypothesis is true, i.e.,\np-value = PH0(T(X) ≥T(x)).\n(A.7)\nIf the p-value is smaller than or equal to a predeﬁned and ﬁxed statistical signiﬁcance level α,\nwe may reject the null hypothesis. Otherwise, we will conclude that we are lack of evidence\nto reject the null hypothesis. For a given population distribution, the region of rejection will\nbe the interval contained of all the points which has a p-value smaller than the statistical\nsigniﬁcance level α.\nOne-side Test and Two-sided Test\nNormally there are two kinds of signiﬁcance test: the one-sided test and the two-sided test.\nThe one-sided test (or one-tailed test) is applicable when the null hypothesis and the alternative\nhypothesis only have one direction. For example, the null hypothesis may state that the true\nparameter θ is less than or equal to a value c. The alternative hypothesis would be that θ is\ngreater than c. That is, the region of rejection is on only one side of the sampling distribution.\nContrary to the one-sided test, the two-sided test (or two-tailed test) is applicable when the\n\n1048\nMathematics for Deep Learning\nregion of rejection is on both sides of the sampling distribution. An example in this case may\nhave a null hypothesis state that the true parameter θ is equal to a value c. The alternative\nhypothesis would be that θ is not equal to c.\nGeneral Steps of Hypothesis Testing\nAfter getting familiar with the above concepts, let’s go through the general steps of hypothesis\ntesting.\n1. State the question and establish a null hypotheses H0.\n2. Set the statistical signiﬁcance level α and a statistical power (1 −β).\n3. Obtain samples through experiments. The number of samples needed will depend on the\nstatistical power, and the expected eﬀect size.\n4. Calculate the test statistic and the p-value.\n5. Make the decision to keep or reject the null hypothesis based on the p-value and the\nstatistical signiﬁcance level α.\nTo conduct a hypothesis test, we start by deﬁning a null hypothesis and a level of risk that we\nare willing to take. Then we calculate the test statistic of the sample, taking an extreme value\nof the test statistic as evidence against the null hypothesis. If the test statistic falls within the\nreject region, we may reject the null hypothesis in favor of the alternative.\nHypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B\ntesting.\nA.10.3 Constructing Conﬁdence Intervals\nWhen estimating the value of a parameter θ, point estimators like ˆθ are of limited utility since\nthey contain no notion of uncertainty. Rather, it would be far better if we could produce an\ninterval that would contain the true parameter θ with high probability. If you were interested\nin such ideas a century ago, then you would have been excited to read “Outline of a Theory\nof Statistical Estimation Based on the Classical Theory of Probability” by Jerzy Neyman\n(Neyman, 1937), who ﬁrst introduced the concept of conﬁdence interval in 1937.\nTo be useful, a conﬁdence interval should be as small as possible for a given degree of cer-\ntainty. Let’s see how to derive it.\nDeﬁnition\nMathematically, a conﬁdence interval for the true parameter θ is an interval Cn that computed\nfrom the sample data such that\nPθ(Cn ∋θ) ≥1 −α, ∀θ.\n(A.8)\n\n1049\nStatistics\nHere α ∈(0, 1), and 1 −α is called the conﬁdence level or coverage of the interval. This is\nthe same α as the signiﬁcance level as we discussed about above.\nNote that (A.8) is about variable Cn, not about the ﬁxed θ. To emphasize this, we write\nPθ(Cn ∋θ) rather than Pθ(θ ∈Cn).\nInterpretation\nIt is very tempting to interpret a 95% conﬁdence interval as an interval where you can be\n95% sure the true parameter lies, however this is sadly not true. The true parameter is ﬁxed,\nand it is the interval that is random. Thus a better interpretation would be to say that if you\ngenerated a large number of conﬁdence intervals by this procedure, 95% of the generated\nintervals would contain the true parameter.\nThis may seem pedantic, but it can have real implications for the interpretation of the results.\nIn particular, we may satisfy (A.8) by constructing intervals that we are almost certain do\nnot contain the true value, as long as we only do so rarely enough. We close this section by\nproviding three tempting but false statements. An in-depth discussion of these points can be\nfound in Morey et al. (2016).\n• Fallacy 1. Narrow conﬁdence intervals mean we can estimate the parameter precisely.\n• Fallacy 2. The values inside the conﬁdence interval are more likely to be the true value\nthan those outside the interval.\n• Fallacy 3. The probability that a particular observed 95% conﬁdence interval contains the\ntrue value is 95%.\nSuﬃced to say, conﬁdence intervals are subtle objects. However, if you keep the interpretation\nclear, they can be powerful tools.\nA Gaussian Example\nLet’s discuss the most classical example, the conﬁdence interval for the mean of a Gaussian\nof unknown mean and variance. Suppose we collect n samples {xi}n\ni=1 from our Gaussian\nN(µ, σ2). We can compute estimators for the mean and variance by taking\nˆµn = 1\nn\nn\n∑\ni=1\nxi and ˆσ2\nn =\n1\nn −1\nn\n∑\ni=1\n(xi −ˆµ)2.\n(A.9)\nIf we now consider the random variable\nT = ˆµn −µ\nˆσn/√n,\n(A.10)\nwe obtain a random variable following a well-known distribution called the Student’s t-distribution\non n −1 degrees of freedom.\nThis distribution is very well studied, and it is known, for instance, that as n →∞, it is\n\n1050\nMathematics for Deep Learning\napproximately a standard Gaussian, and thus by looking up values of the Gaussian c.d.f. in a\ntable, we may conclude that the value of T is in the interval [−1.96, 1.96] at least 95% of the\ntime. For ﬁnite values of n, the interval needs to be somewhat larger, but are well known and\nprecomputed in tables.\nThus, we may conclude that for large n,\nP\n( ˆµn −µ\nˆσn/√n ∈[−1.96, 1.96]\n)\n≥0.95.\n(A.11)\nRearranging this by multiplying both sides by ˆσn/√n and then adding ˆµn, we obtain\nP\n(\nµ ∈\n[\nˆµn −1.96 ˆσn\n√n, ˆµn + 1.96 ˆσn\n√n\n])\n≥0.95.\n(A.12)\nThus we know that we have found our 95% conﬁdence interval:\n[\nˆµn −1.96 ˆσn\n√n, ˆµn + 1.96 ˆσn\n√n\n]\n.\n(A.13)\nIt is safe to say that (A.13) is one of the most used formula in statistics. Let’s close our\ndiscussion of statistics by implementing it. For simplicity, we assume we are in the asymp-\ntotic regime. Small values of N should include the correct value of t_star obtained either\nprogrammatically or from a t-table.\n# PyTorch uses Bessel's correction by default, which means the use of ddof=1\n# instead of default ddof=0 in numpy. We can use unbiased=False to imitate\n# ddof=0.\n# Number of samples\nN = 1000\n# Sample dataset\nsamples = torch.normal(0, 1, size=(N,))\n# Lookup Students's t-distribution c.d.f.\nt_star = 1.96\n# Construct interval\nmu_hat = torch.mean(samples)\nsigma_hat = samples.std(unbiased=True)\n(mu_hat - t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)),\\\nmu_hat + t_star*sigma_hat/torch.sqrt(torch.tensor(N, dtype=torch.float32)))\n(tensor(-0.0568), tensor(0.0704))\nA.10.4 Summary\n• Statistics focuses on inference problems, whereas deep learning emphasizes on making\naccurate predictions without explicitly programming and understanding.\n\n1051\nInformation Theory\n287\n• There are three common statistics inference methods: evaluating and comparing estima-\ntors, conducting hypothesis tests, and constructing conﬁdence intervals.\n• There are three most common estimators: statistical bias, standard deviation, and mean\nsquare error.\n• A conﬁdence interval is an estimated range of a true population parameter that we can\nconstruct by given the samples.\n• Hypothesis testing is a way of evaluating some evidence against the default statement about\na population.\nA.10.5 Exercises\n1. Let X1, X2, . . ., Xn\niid∼Unif(0, θ), where “iid” stands for independent and identically dis-\ntributed. Consider the following estimators of θ:\nˆθ = max{X1, X2, . . ., Xn};\n(A.14)\n˜θ = 2 ¯Xn = 2\nn\nn\n∑\ni=1\nXi.\n(A.15)\n• Find the statistical bias, standard deviation, and mean square error of ˆθ.\n• Find the statistical bias, standard deviation, and mean square error of ˜θ.\n• Which estimator is better?\n2. For our chemist example in introduction, can you derive the 5 steps to conduct a two-sided\nhypothesis testing? Given the statistical signiﬁcance level α = 0.05 and the statistical\npower 1 −β = 0.8.\n3. Run the conﬁdence interval code with N = 2 and α = 0.5 for 100 independently gen-\nerated dataset, and plot the resulting intervals (in this case t_star = 1.0). You will see\nseveral very short intervals which are very far from containing the true mean 0. Does this\ncontradict the interpretation of the conﬁdence interval? Do you feel comfortable using\nshort intervals to indicate high precision estimates?\nDiscussions287.\nA.11 Information Theory\nThe universe is overﬂowing with information. Information provides a common language\nacross disciplinary rifts: from Shakespeare’s Sonnet to researchers’ paper on Cornell ArXiv,\nfrom Van Gogh’s printing Starry Night to Beethoven’s music Symphony No. 5, from the\n\n1052\nMathematics for Deep Learning\nﬁrst programming language Plankalkül to the state-of-the-art machine learning algorithms.\nEverything must follow the rules of information theory, no matter the format. With infor-\nmation theory, we can measure and compare how much information is present in diﬀerent\nsignals. In this section, we will investigate the fundamental concepts of information theory\nand applications of information theory in machine learning.\nBefore we get started, let’s outline the relationship between machine learning and information\ntheory. Machine learning aims to extract interesting signals from data and make critical pre-\ndictions. On the other hand, information theory studies encoding, decoding, transmitting, and\nmanipulating information. As a result, information theory provides fundamental language for\ndiscussing the information processing in machine learned systems. For example, many ma-\nchine learning applications use the cross-entropy loss as described in Section 4.1. This loss\ncan be directly derived from information theoretic considerations.\nA.11.1 Information\nLet’s start with the “soul” of information theory: information. Information can be encoded\nin anything with a particular sequence of one or more encoding formats. Suppose that we\ntask ourselves with trying to deﬁne a notion of information. What could be our starting\npoint?\nConsider the following thought experiment. We have a friend with a deck of cards. They will\nshuﬄe the deck, ﬂip over some cards, and tell us statements about the cards. We will try to\nassess the information content of each statement.\nFirst, they ﬂip over a card and tell us, “I see a card.” This provides us with no information\nat all. We were already certain that this was the case so we hope the information should be\nzero.\nNext, they ﬂip over a card and say, “I see a heart.” This provides us some information, but\nin reality there are only 4 diﬀerent suits that were possible, each equally likely, so we are\nnot surprised by this outcome. We hope that whatever the measure of information, this event\nshould have low information content.\nNext, they ﬂip over a card and say, “This is the 3 of spades.” This is more information. Indeed\nthere were 52 equally likely possible outcomes, and our friend told us which one it was. This\nshould be a medium amount of information.\nLet’s take this to the logical extreme. Suppose that ﬁnally they ﬂip over every card from the\ndeck and read oﬀthe entire sequence of the shuﬄed deck. There are 52! diﬀerent orders\nto the deck, again all equally likely, so we need a lot of information to know which one it\nis.\nAny notion of information we develop must conform to this intuition. Indeed, in the next\nsections we will learn how to compute that these events have 0 bits, 2 bits, 5.7 bits, and\n225.6 bits of information respectively.\nIf we read through these thought experiments, we see a natural idea. As a starting point, rather\n\n1053\nInformation Theory\nthan caring about the knowledge, we may build oﬀthe idea that information represents the\ndegree of surprise or the abstract possibility of the event. For example, if we want to describe\nan unusual event, we need a lot information. For a common event, we may not need much\ninformation.\nIn 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shannon,\n1948) establishing the theory of information. In his article, Shannon introduced the concept\nof information entropy for the ﬁrst time. We will begin our journey here.\nSelf-information\nSince information embodies the abstract possibility of an event, how do we map the possibility\nto the number of bits? Shannon introduced the terminology bit as the unit of information,\nwhich was originally created by John Tukey. So what is a “bit” and why do we use it to measure\ninformation? Historically, an antique transmitter can only send or receive two types of code:\n0 and 1. Indeed, binary encoding is still in common use on all modern digital computers. In\nthis way, any information is encoded by a series of 0 and 1. And hence, a series of binary\ndigits of length n contains n bits of information.\nNow, suppose that for any series of codes, each 0 or 1 occurs with a probability of 1\n2. Hence,\nan event X with a series of codes of length n, occurs with a probability of\n1\n2n . At the same\ntime, as we mentioned before, this series contains n bits of information. So, can we gener-\nalize to a mathematical function which can transfer the probability p to the number of bits?\nShannon gave the answer by deﬁning self-information\nI(X) = −log2(p),\n(A.1)\nas the bits of information we have received for this event X. Note that we will always use\nbase-2 logarithms in this section. For the sake of simplicity, the rest of this section will omit\nthe subscript 2 in the logarithm notation, i.e., log(.) always refers to log2(.). For example,\nthe code “0010” has a self-information\nI(\"0010\") = −log(p(\"0010\")) = −log\n( 1\n24\n)\n= 4 bits.\n(A.2)\nWe can calculate self information as shown below. Before that, let’s ﬁrst import all the nec-\nessary packages in this section.\nimport torch\nfrom torch.nn import NLLLoss\ndef nansum(x):\n# Define nansum, as pytorch does not offer it inbuilt.\nreturn x[~torch.isnan(x)].sum()\ndef self_information(p):\nreturn -torch.log2(torch.tensor(p)).item()\n(continues on next page)\n\n1054\nMathematics for Deep Learning\n(continued from previous page)\nself_information(1 / 64)\n6.0\nA.11.2 Entropy\nAs self-information only measures the information of a single discrete event, we need a\nmore generalized measure for any random variable of either discrete or continuous distri-\nbution.\nMotivating Entropy\nLet’s try to get speciﬁc about what we want. This will be an informal statement of what are\nknown as the axioms of Shannon entropy. It will turn out that the following collection of\ncommon-sense statements force us to a unique deﬁnition of information. A formal version of\nthese axioms, along with several others may be found in Csiszár (2008).\n1. The information we gain by observing a random variable does not depend on what we call\nthe elements, or the presence of additional elements which have probability zero.\n2. The information we gain by observing two random variables is no more than the sum of\nthe information we gain by observing them separately. If they are independent, then it is\nexactly the sum.\n3. The information gained when observing (nearly) certain events is (nearly) zero.\nWhile proving this fact is beyond the scope of our text, it is important to know that this\nuniquely determines the form that entropy must take. The only ambiguity that these allow is\nin the choice of fundamental units, which is most often normalized by making the choice we\nsaw before that the information provided by a single fair coin ﬂip is one bit.\nDeﬁnition\nFor any random variable X that follows a probability distribution P with a probability den-\nsity function (p.d.f.) or a probability mass function (p.m.f.) p(x), we measure the expected\namount of information through entropy (or Shannon entropy)\nH(X) = −Ex∼P[log p(x)].\n(A.3)\nTo be speciﬁc, if X is discrete,\nH(X) = −\n∑\ni\npi log pi, where pi = P(Xi).\n(A.4)\n\n1055\nInformation Theory\nOtherwise, if X is continuous, we also refer entropy as diﬀerential entropy\nH(X) = −\n∫\nx\np(x) log p(x) dx.\n(A.5)\nWe can deﬁne entropy as below.\ndef entropy(p):\nentropy = - p * torch.log2(p)\n# Operator `nansum` will sum up the non-nan number\nout = nansum(entropy)\nreturn out\nentropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))\ntensor(1.6855)\nInterpretations\nYou may be curious: in the entropy deﬁnition (A.3), why do we use an expectation of a\nnegative logarithm? Here are some intuitions.\nFirst, why do we use a logarithm function log? Suppose that p(x) = f1(x) f2(x) . . ., fn(x),\nwhere each component function fi(x) is independent from each other. This means that each\nfi(x) contributes independently to the total information obtained from p(x). As discussed\nabove, we want the entropy formula to be additive over independent random variables. Luck-\nily, log can naturally turn a product of probability distributions to a summation of the indi-\nvidual terms.\nNext, why do we use a negative log? Intuitively, more frequent events should contain less\ninformation than less common events, since we often gain more information from an unusual\ncase than from an ordinary one. However, log is monotonically increasing with the prob-\nabilities, and indeed negative for all values in [0, 1]. We need to construct a monotonically\ndecreasing relationship between the probability of events and their entropy, which will ideally\nbe always positive (for nothing we observe should force us to forget what we have known).\nHence, we add a negative sign in front of log function.\nLast, where does the expectation function come from? Consider a random variable X. We\ncan interpret the self-information (−log(p)) as the amount of surprise we have at seeing a\nparticular outcome. Indeed, as the probability approaches zero, the surprise becomes inﬁnite.\nSimilarly, we can interpret the entropy as the average amount of surprise from observing\nX. For example, imagine that a slot machine system emits statistical independently symbols\ns1, . . ., sk with probabilities p1, . . ., pk respectively. Then the entropy of this system equals\nto the average self-information from observing each output, i.e.,\nH(S) =\n∑\ni\npi · I(si) = −\n∑\ni\npi · log pi.\n(A.6)\n\n1056\nMathematics for Deep Learning\nProperties of Entropy\nBy the above examples and interpretations, we can derive the following properties of entropy\n(A.3). Here, we refer to X as an event and P as the probability distribution of X.\n• H(X) ≥0 for all discrete X (entropy can be negative for continuous X).\n• If X ∼P with a p.d.f. or a p.m.f. p(x), and we try to estimate P by a new probability\ndistribution Q with a p.d.f. or a p.m.f. q(x), then\nH(X) = −Ex∼P[log p(x)] ≤−Ex∼P[log q(x)], with equality if and only if P = Q.\n(A.7)\nAlternatively, H(X) gives a lower bound of the average number of bits needed to encode\nsymbols drawn from P.\n• If X ∼P, then x conveys the maximum amount of information if it spreads evenly among\nall possible outcomes. Speciﬁcally, if the probability distribution P is discrete with k-\nclass {p1, . . ., pk}, then\nH(X) ≤log(k), with equality if and only if pi = 1\nk, ∀i.\n(A.8)\nIf P is a continuous random variable, then the story becomes much more complicated.\nHowever, if we additionally impose that P is supported on a ﬁnite interval (with all\nvalues between 0 and 1), then P has the highest entropy if it is the uniform distribution\non that interval.\nA.11.3 Mutual Information\nPreviously we deﬁned entropy of a single random variable X, how about the entropy of a pair\nrandom variables (X,Y)? We can think of these techniques as trying to answer the following\ntype of question, “What information is contained in X and Y together compared to each\nseparately? Is there redundant information, or is it all unique?”\nFor the following discussion, we always use (X,Y) as a pair of random variables that follows\na joint probability distribution P with a p.d.f. or a p.m.f. pX,Y(x, y), while X and Y follow\nprobability distribution pX(x) and pY(y), respectively.\nJoint Entropy\nSimilar to entropy of a single random variable (A.3), we deﬁne the joint entropy H(X,Y) of\na pair random variables (X,Y) as\nH(X,Y) = −E(x,y)∼P[log pX,Y(x, y)].\n(A.9)\nPrecisely, on the one hand, if (X,Y) is a pair of discrete random variables, then\nH(X,Y) = −\n∑\nx\n∑\ny\npX,Y(x, y) log pX,Y(x, y).\n(A.10)\n\n1057\nInformation Theory\nOn the other hand, if (X,Y) is a pair of continuous random variables, then we deﬁne the\ndiﬀerential joint entropy as\nH(X,Y) = −\n∫\nx,y\npX,Y(x, y) log pX,Y(x, y) dx dy.\n(A.11)\nWe can think of (A.9) as telling us the total randomness in the pair of random variables. As\na pair of extremes, if X = Y are two identical random variables, then the information in\nthe pair is exactly the information in one and we have H(X,Y) = H(X) = H(Y). On the\nother extreme, if X and Y are independent then H(X,Y) = H(X) + H(Y). Indeed we will\nalways have that the information contained in a pair of random variables is no smaller than\nthe entropy of either random variable and no more than the sum of both.\nH(X), H(Y) ≤H(X,Y) ≤H(X) + H(Y).\n(A.12)\nLet’s implement joint entropy from scratch.\ndef joint_entropy(p_xy):\njoint_ent = -p_xy * torch.log2(p_xy)\n# Operator `nansum` will sum up the non-nan number\nout = nansum(joint_ent)\nreturn out\njoint_entropy(torch.tensor([[0.1, 0.5], [0.1, 0.3]]))\ntensor(1.6855)\nNotice that this is the same code as before, but now we interpret it diﬀerently as working on\nthe joint distribution of the two random variables.\nConditional Entropy\nThe joint entropy deﬁned above the amount of information contained in a pair of random\nvariables. This is useful, but oftentimes it is not what we care about. Consider the setting of\nmachine learning. Let’s take X to be the random variable (or vector of random variables) that\ndescribes the pixel values of an image, andY to be the random variable which is the class label.\nX should contain substantial information—a natural image is a complex thing. However, the\ninformation contained inY once the image has been show should be low. Indeed, the image of\na digit should already contain the information about what digit it is unless the digit is illegible.\nThus, to continue to extend our vocabulary of information theory, we need to be able to reason\nabout the information content in a random variable conditional on another.\nIn the probability theory, we saw the deﬁnition of the conditional probability to measure the\nrelationship between variables. We now want to analogously deﬁne the conditional entropy\nH(Y | X). We can write this as\nH(Y | X) = −E(x,y)∼P[log p(y | x)],\n(A.13)\n\n1058\nMathematics for Deep Learning\nwhere p(y | x) = pX,Y (x,y)\npX(x)\nis the conditional probability. Speciﬁcally, if (X,Y) is a pair of\ndiscrete random variables, then\nH(Y | X) = −\n∑\nx\n∑\ny\np(x, y) log p(y | x).\n(A.14)\nIf (X,Y) is a pair of continuous random variables, then the diﬀerential conditional entropy is\nsimilarly deﬁned as\nH(Y | X) = −\n∫\nx\n∫\ny\np(x, y) log p(y | x) dx dy.\n(A.15)\nIt is now natural to ask, how does the conditional entropy H(Y | X) relate to the entropy H(X)\nand the joint entropy H(X,Y)? Using the deﬁnitions above, we can express this cleanly:\nH(Y | X) = H(X,Y) −H(X).\n(A.16)\nThis has an intuitive interpretation: the information in Y given X (H(Y | X)) is the same as\nthe information in both X and Y together (H(X,Y)) minus the information already contained\nin X. This gives us the information in Y which is not also represented in X.\nNow, let’s implement conditional entropy (A.13) from scratch.\ndef conditional_entropy(p_xy, p_x):\np_y_given_x = p_xy/p_x\ncond_ent = -p_xy * torch.log2(p_y_given_x)\n# Operator `nansum` will sum up the non-nan number\nout = nansum(cond_ent)\nreturn out\nconditional_entropy(torch.tensor([[0.1, 0.5], [0.2, 0.3]]),\ntorch.tensor([0.2, 0.8]))\ntensor(0.8635)\nMutual Information\nGiven the previous setting of random variables (X,Y), you may wonder: “Now that we know\nhow much information is contained in Y but not in X, can we similarly ask how much infor-\nmation is shared between X and Y?” The answer will be the mutual information of (X,Y),\nwhich we will write as I(X,Y).\nRather than diving straight into the formal deﬁnition, let’s practice our intuition by ﬁrst try-\ning to derive an expression for the mutual information entirely based on terms we have con-\nstructed before. We wish to ﬁnd the information shared between two random variables. One\nway we could try to do this is to start with all the information contained in both X and Y\ntogether, and then we take oﬀthe parts that are not shared. The information contained in\nboth X and Y together is written as H(X,Y). We want to subtract from this the information\ncontained in X but not in Y, and the information contained in Y but not in X. As we saw in\n\n1059\nInformation Theory\nthe previous section, this is given by H(X | Y) and H(Y | X) respectively. Thus, we have\nthat the mutual information should be\nI(X,Y) = H(X,Y) −H(Y | X) −H(X | Y).\n(A.17)\nIndeed, this is a valid deﬁnition for the mutual information. If we expand out the deﬁnitions\nof these terms and combine them, a little algebra shows that this is the same as\nI(X,Y) = ExEy\n{\npX,Y(x, y) log pX,Y(x, y)\npX(x)pY(y)\n}\n.\n(A.18)\nWe can summarize all of these relationships in image Fig. A.1. It is an excellent test of\nintuition to see why the following statements are all also equivalent to I(X,Y).\n• H(X) −H(X | Y)\n• H(Y) −H(Y | X)\n• H(X) + H(Y) −H(X,Y)\nt\nFig. A.1\nMutual information’s relationship with joint entropy and conditional entropy.\nIn many ways we can think of the mutual information (A.18) as principled extension of corre-\nlation coeﬃcient we saw in Section A.6. This allows us to ask not only for linear relationships\nbetween variables, but for the maximum information shared between the two random vari-\nables of any kind.\nNow, let’s implement mutual information from scratch.\ndef mutual_information(p_xy, p_x, p_y):\np = p_xy / (p_x * p_y)\nmutual = p_xy * torch.log2(p)\n# Operator `nansum` will sum up the non-nan number\nout = nansum(mutual)\nreturn out\nmutual_information(torch.tensor([[0.1, 0.5], [0.1, 0.3]]),\ntorch.tensor([0.2, 0.8]), torch.tensor([[0.75, 0.25]]))\n\n1060\nMathematics for Deep Learning\ntensor(0.7195)\nProperties of Mutual Information\nRather than memorizing the deﬁnition of mutual information (A.18), you only need to keep\nin mind its notable properties:\n• Mutual information is symmetric, i.e., I(X,Y) = I(Y, X).\n• Mutual information is non-negative, i.e., I(X,Y) ≥0.\n• I(X,Y) = 0 if and only if X and Y are independent. For example, if X and Y are inde-\npendent, then knowing Y does not give any information about X and vice versa, so their\nmutual information is zero.\n• Alternatively, if X is an invertible function of Y, then Y and X share all information and\nI(X,Y) = H(Y) = H(X).\n(A.19)\nPointwise Mutual Information\nWhen we worked with entropy at the beginning of this chapter, we were able to provide an\ninterpretation of −log(pX(x)) as how surprised we were with the particular outcome. We\nmay give a similar interpretation to the logarithmic term in the mutual information, which is\noften referred to as the pointwise mutual information:\npmi(x, y) = log pX,Y(x, y)\npX(x)pY(y).\n(A.20)\nWe can think of (A.20) as measuring how much more or less likely the speciﬁc combina-\ntion of outcomes x and y are compared to what we would expect for independent random\noutcomes. If it is large and positive, then these two speciﬁc outcomes occur much more fre-\nquently than they would compared to random chance (note: the denominator is pX(x)pY(y)\nwhich is the probability of the two outcomes were independent), whereas if it is large and\nnegative it represents the two outcomes happening far less than we would expect by random\nchance.\nThis allows us to interpret the mutual information (A.18) as the average amount that we were\nsurprised to see two outcomes occurring together compared to what we would expect if they\nwere independent.\nApplications of Mutual Information\nMutual information may be a little abstract in it pure deﬁnition, so how does it related to\nmachine learning? In natural language processing, one of the most diﬃcult problems is the\n\n1061\nInformation Theory\nambiguity resolution, or the issue of the meaning of a word being unclear from context. For\nexample, recently a headline in the news reported that “Amazon is on ﬁre”. You may won-\nder whether the company Amazon has a building on ﬁre, or the Amazon rain forest is on\nﬁre.\nIn this case, mutual information can help us resolve this ambiguity. We ﬁrst ﬁnd the group\nof words that each has a relatively large mutual information with the company Amazon, such\nas e-commerce, technology, and online. Second, we ﬁnd another group of words that each\nhas a relatively large mutual information with the Amazon rain forest, such as rain, forest,\nand tropical. When we need to disambiguate “Amazon”, we can compare which group has\nmore occurrence in the context of the word Amazon. In this case the article would go on to\ndescribe the forest, and make the context clear.\nA.11.4 Kullback–Leibler Divergence\nAs what we have discussed in Section 2.3, we can use norms to measure distance between\ntwo points in space of any dimensionality. We would like to be able to do a similar task\nwith probability distributions. There are many ways to go about this, but information theory\nprovides one of the nicest. We now explore the Kullback–Leibler (KL) divergence, which\nprovides a way to measure if two distributions are close together or not.\nDeﬁnition\nGiven a random variable X that follows the probability distribution P with a p.d.f. or a p.m.f.\np(x), and we estimate P by another probability distribution Q with a p.d.f. or a p.m.f. q(x).\nThen the Kullback–Leibler (KL) divergence (or relative entropy) between P and Q is\nDKL(P∥Q) = Ex∼P\n[\nlog p(x)\nq(x)\n]\n.\n(A.21)\nAs with the pointwise mutual information (A.20), we can again provide an interpretation of\nthe logarithmic term: −log q(x)\np(x) = −log(q(x)) −(−log(p(x))) will be large and positive\nif we see x far more often under P than we would expect for Q, and large and negative if\nwe see the outcome far less than expected. In this way, we can interpret it as our relative\nsurprise at observing the outcome compared to how surprised we would be observing it from\nour reference distribution.\nLet’s implement the KL divergence from Scratch.\ndef kl_divergence(p, q):\nkl = p * torch.log2(p / q)\nout = nansum(kl)\nreturn out.abs().item()\n\n1062\nMathematics for Deep Learning\nKL Divergence Properties\nLet’s take a look at some properties of the KL divergence (A.21).\n• KL divergence is non-symmetric, i.e., there are P, Q such that\nDKL(P∥Q) , DKL(Q∥P).\n(A.22)\n• KL divergence is non-negative, i.e.,\nDKL(P∥Q) ≥0.\n(A.23)\nNote that the equality holds only when P = Q.\n• If there exists an x such that p(x) > 0 and q(x) = 0, then DKL(P∥Q) = ∞.\n• There is a close relationship between KL divergence and mutual information. Besides the\nrelationship shown in Fig. A.1, I(X,Y) is also numerically equivalent with the following\nterms:\n1. DKL(P(X,Y) ∥P(X)P(Y));\n2. EY{DKL(P(X | Y) ∥P(X))};\n3. EX{DKL(P(Y | X) ∥P(Y))}.\nFor the ﬁrst term, we interpret mutual information as the KL divergence between P(X,Y) and\nthe product of P(X) and P(Y), and thus is a measure of how diﬀerent the joint distribution\nis from the distribution if they were independent. For the second term, mutual information\ntells us the average reduction in uncertainty about Y that results from learning the value of\nthe X’s distribution. Similarly to the third term.\nExample\nLet’s go through a toy example to see the non-symmetry explicitly.\nFirst, let’s generate and sort three tensors of length 10, 000: an objective tensor p which\nfollows a normal distribution N(0, 1), and two candidate tensors q1 and q2 which follow\nnormal distributions N(−1, 1) and N(1, 1) respectively.\ntorch.manual_seed(1)\ntensor_len = 10000\np = torch.normal(0, 1, (tensor_len, ))\nq1 = torch.normal(-1, 1, (tensor_len, ))\nq2 = torch.normal(1, 1, (tensor_len, ))\np = torch.sort(p)[0]\nq1 = torch.sort(q1)[0]\nq2 = torch.sort(q2)[0]\n\n1063\nInformation Theory\nSince q1 and q2 are symmetric with respect to the y-axis (i.e., x = 0), we expect a similar\nvalue of KL divergence between DKL(p∥q1) and DKL(p∥q2). As you can see below, there is\nonly a less than 3% oﬀbetween DKL(p∥q1) and DKL(p∥q2).\nkl_pq1 = kl_divergence(p, q1)\nkl_pq2 = kl_divergence(p, q2)\nsimilar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100\nkl_pq1, kl_pq2, similar_percentage\n(8582.0341796875, 8828.3095703125, 2.8290698237936858)\nIn contrast, you may ﬁnd that DKL(q2∥p) and DKL(p∥q2) are oﬀa lot, with around 40% oﬀ\nas shown below.\nkl_q2p = kl_divergence(q2, p)\ndiffer_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100\nkl_q2p, differ_percentage\n(14130.125, 46.18621024399691)\nA.11.5 Cross-Entropy\nIf you are curious about applications of information theory in deep learning, here is a quick\nexample. We deﬁne the true distribution P with probability distribution p(x), and the esti-\nmated distribution Q with probability distribution q(x), and we will use them in the rest of\nthis section.\nSay we need to solve a binary classiﬁcation problem based on given n data examples {x1, . . ., xn}.\nAssume that we encode 1 and 0 as the positive and negative class label yi respectively, and\nour neural network is parametrized by θ. If we aim to ﬁnd a best θ so that ˆyi = pθ(yi | xi), it\nis natural to apply the maximum log-likelihood approach as was seen in Section A.7. To be\nspeciﬁc, for true labels yi and predictions ˆyi = pθ(yi | xi), the probability to be classiﬁed as\npositive is πi = pθ(yi = 1 | xi). Hence, the log-likelihood function would be\nl(θ) = log L(θ)\n= log\nn\n∏\ni=1\nπyi\ni (1 −πi)1−yi\n=\nn\n∑\ni=1\nyi log(πi) + (1 −yi) log(1 −πi).\n(A.24)\nMaximizing the log-likelihood function l(θ) is identical to minimizing −l(θ), and hence we\ncan ﬁnd the best θ from here. To generalize the above loss to any distributions, we also called\n\n1064\nMathematics for Deep Learning\n−l(θ) the cross-entropy loss CE(y, ˆy), where y follows the true distribution P and ˆy follows\nthe estimated distribution Q.\nThis was all derived by working from the maximum likelihood point of view. However, if we\nlook closely we can see that terms like log(πi) have entered into our computation which is\na solid indication that we can understand the expression from an information theoretic point\nof view.\nFormal Deﬁnition\nLike KL divergence, for a random variable X, we can also measure the divergence between\nthe estimating distribution Q and the true distribution P via cross-entropy,\nCE(P, Q) = −Ex∼P[log(q(x))].\n(A.25)\nBy using properties of entropy discussed above, we can also interpret it as the summation of\nthe entropy H(P) and the KL divergence between P and Q, i.e.,\nCE(P, Q) = H(P) + DKL(P∥Q).\n(A.26)\nWe can implement the cross-entropy loss as below.\ndef cross_entropy(y_hat, y):\nce = -torch.log(y_hat[range(len(y_hat)), y])\nreturn ce.mean()\nNow deﬁne two tensors for the labels and predictions, and calculate the cross-entropy loss of\nthem.\nlabels = torch.tensor([0, 2])\npreds = torch.tensor([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\ncross_entropy(preds, labels)\ntensor(0.9486)\nProperties\nAs alluded in the beginning of this section, cross-entropy (A.25) can be used to deﬁne a loss\nfunction in the optimization problem. It turns out that the following are equivalent:\n1. Maximizing predictive probability of Q for distribution P, (i.e., Ex∼P[log(q(x))]);\n2. Minimizing cross-entropy CE(P, Q);\n3. Minimizing the KL divergence DKL(P∥Q).\n\n1065\nInformation Theory\nThe deﬁnition of cross-entropy indirectly proves the equivalent relationship between objective\n2 and objective 3, as long as the entropy of true data H(P) is constant.\nCross-Entropy as An Objective Function of Multi-class Classiﬁcation\nIf we dive deep into the classiﬁcation objective function with cross-entropy loss CE, we will\nﬁnd minimizing CE is equivalent to maximizing the log-likelihood function L.\nTo begin with, suppose that we are given a dataset with n examples, and it can be classiﬁed\ninto k-classes. For each data example i, we represent any k-class label yi = (yi1, . . ., yik)\nby one-hot encoding. To be speciﬁc, if the example i belongs to class j, then we set the j-th\nentry to 1, and all other components to 0, i.e.,\nyij =\n{\n1\nj ∈J;\n0\notherwise.\n(A.27)\nFor instance, if a multi-class classiﬁcation problem contains three classes A, B, and C, then\nthe labels yi can be encoded in {A : (1, 0, 0); B : (0, 1, 0); C : (0, 0, 1)}.\nAssume that our neural network is parametrized by θ. For true label vectors yi and predic-\ntions\nˆyi = pθ(yi | xi) =\nk\n∑\nj=1\nyijpθ(yij | xi).\n(A.28)\nHence, the cross-entropy loss would be\nCE(y, ˆy) = −\nn\n∑\ni=1\nyi log ˆyi = −\nn\n∑\ni=1\nk\n∑\nj=1\nyij log pθ(yij | xi).\n(A.29)\nOn the other side, we can also approach the problem through maximum likelihood estima-\ntion. To begin with, let’s quickly introduce a k-class multinoulli distribution. It is an ex-\ntension of the Bernoulli distribution from binary class to multi-class. If a random variable\nz = (z1, . . ., zk) follows a k-class multinoulli distribution with probabilities p = (p1, . . ., pk),\ni.e.,\np(z) = p(z1, . . ., zk) = Multi(p1, . . ., pk), where\nk\n∑\ni=1\npi = 1,\n(A.30)\nthen the joint probability mass function(p.m.f.) of z is\npz =\nk\n∏\nj=1\npzj\nj .\n(A.31)\nIt can be seen that the label of each data example, yi, is following a k-class multinoulli\ndistribution with probabilities π = (π1, . . ., πk). Therefore, the joint p.m.f. of each data\n\n1066\nMathematics for Deep Learning\nexample yi is ßyi = ∏k\nj=1 πyi j\nj . Hence, the log-likelihood function would be\nl(θ) = log L(θ) = log\nn\n∏\ni=1\nπyi = log\nn\n∏\ni=1\nk\n∏\nj=1\nπyi j\nj\n=\nn\n∑\ni=1\nk\n∑\nj=1\nyij log πj.\n(A.32)\nSince in maximum likelihood estimation, we maximizing the objective function l(θ) by hav-\ning πj = pθ(yij | xi). Therefore, for any multi-class classiﬁcation, maximizing the above\nlog-likelihood function l(θ) is equivalent to minimizing the CE loss CE(y, ˆy).\nTo test the above proof, let’s apply the built-in measure NegativeLogLikelihood. Using\nthe same labels and preds as in the earlier example, we will get the same numerical loss as\nthe previous example up to the 5 decimal place.\n# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\n# and `nn.NLLLoss()`\nnll_loss = NLLLoss()\nloss = nll_loss(torch.log(preds), labels)\nloss\ntensor(0.9486)\nA.11.6 Summary\n• Information theory is a ﬁeld of study about encoding, decoding, transmitting, and manip-\nulating information.\n• Entropy is the unit to measure how much information is presented in diﬀerent signals.\n• KL divergence can also measure the divergence between two distributions.\n• Cross-entropy can be viewed as an objective function of multi-class classiﬁcation. Mini-\nmizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\nA.11.7 Exercises\n1. Verify that the card examples from the ﬁrst section indeed have the claimed entropy.\n2. Show that the KL divergence D(p∥q) is nonnegative for all distributions p and q. Hint:\nuse Jensen’s inequality, i.e., use the fact that −log x is a convex function.\n3. Let’s compute the entropy from a few data sources:\n• Assume that you are watching the output generated by a monkey at a typewriter. The\nmonkey presses any of the 44 keys of the typewriter at random (you can assume\nthat it has not discovered any special keys or the shift key yet). How many bits of\nrandomness per character do you observe?\n\n1067\nInformation Theory\n288\n• Being unhappy with the monkey, you replaced it by a drunk typesetter. It is able to\ngenerate words, albeit not coherently. Instead, it picks a random word out of a vo-\ncabulary of 2, 000 words. Let’s assume that the average length of a word is 4.5 letters\nin English. How many bits of randomness per character do you observe now?\n• Still being unhappy with the result, you replace the typesetter by a high quality lan-\nguage model. The language model can currently obtain a perplexity as low as 15\npoints per word. The character perplexity of a language model is deﬁned as the in-\nverse of the geometric mean of a set of probabilities, each probability is correspond-\ning to a character in the word. To be speciﬁc, if the length of a given word is l,\nthen PPL(word) = [∏\ni p(characteri)]−1\nl = exp\n[\n−1\nl\n∑\ni log p(characteri)\n]\n. As-\nsume that the test word has 4.5 letters, how many bits of randomness per character\ndo you observe now?\n4. Explain intuitively why I(X,Y) = H(X)−H(X | Y). Then, show this is true by expressing\nboth sides as an expectation with respect to the joint distribution.\n5. What is the KL Divergence between the two Gaussian distributions N(µ1, σ2\n1) and N(µ2, σ2\n2)?\nDiscussions288.\n\n289\nB\nTools for Deep Learning\nTo get the most out of Dive into Deep Learning, we will talk you through diﬀerent tools in this\nappendix, such as for running and contributing to this interactive open-source book.\nB.1 Using Jupyter Notebooks\nThis section describes how to edit and run the code in each section of this book using the\nJupyter Notebook. Make sure you have installed Jupyter and downloaded the code as de-\nscribed in Installation (page xxxiv). If you want to know more about Jupyter see the excellent\ntutorial in their documentation289.\nB.1.1 Editing and Running the Code Locally\nSuppose that the local path of the book’s code is xx/yy/d2l-en/. Use the shell to change\nthe directory to this path (cd xx/yy/d2l-en) and run the command jupyter notebook.\nIf your browser does not do this automatically, open http://localhost:8888 and you will see\nthe interface of Jupyter and all the folders containing the code of the book, as shown in Fig.\nB.1.\nt\nFig. B.1\nThe folders containing the code of this book.\nYou can access the notebook ﬁles by clicking on the folder displayed on the webpage. They\n1068\n\n1069\nUsing Jupyter Notebooks\nusually have the suﬃx “.ipynb”. For the sake of brevity, we create a temporary “test.ipynb”\nﬁle. The content displayed after you click it is shown in Fig. B.2. This notebook includes a\nmarkdown cell and a code cell. The content in the markdown cell includes “This Is a Title”\nand “This is text.”. The code cell contains two lines of Python code.\nt\nFig. B.2\nMarkdown and code cells in the “text.ipynb” ﬁle.\nDouble click on the markdown cell to enter edit mode. Add a new text string “Hello world.”\nat the end of the cell, as shown in Fig. B.3.\nt\nFig. B.3\nEdit the markdown cell.\nAs demonstrated in Fig. B.4, click “Cell” →“Run Cells” in the menu bar to run the edited\ncell.\nAfter running, the markdown cell is shown in Fig. B.5.\nNext, click on the code cell. Multiply the elements by 2 after the last line of code, as shown\nin Fig. B.6.\nYou can also run the cell with a shortcut (“Ctrl + Enter” by default) and obtain the output\nresult from Fig. B.7.\nWhen a notebook contains more cells, we can click “Kernel” →“Restart & Run All” in the\n\n1070\nTools for Deep Learning\nt\nFig. B.4\nRun the cell.\nt\nFig. B.5\nThe markdown cell after running.\nt\nFig. B.6\nEdit the code cell.\n\n1071\nUsing Jupyter Notebooks\nt\nFig. B.7\nRun the code cell to obtain the output.\nmenu bar to run all the cells in the entire notebook. By clicking “Help” →“Edit Keyboard\nShortcuts” in the menu bar, you can edit the shortcuts according to your preferences.\nB.1.2 Advanced Options\nBeyond local editing two things are quite important: editing the notebooks in the markdown\nformat and running Jupyter remotely. The latter matters when we want to run the code on a\nfaster server. The former matters since Jupyter’s native ipynb format stores a lot of auxiliary\ndata that is irrelevant to the content, mostly related to how and where the code is run. This\nis confusing for Git, making reviewing contributions very diﬃcult. Fortunately there is an\nalternative—native editing in the markdown format.\nMarkdown Files in Jupyter\nIf you wish to contribute to the content of this book, you need to modify the source ﬁle (md\nﬁle, not ipynb ﬁle) on GitHub. Using the notedown plugin we can modify notebooks in the\nmd format directly in Jupyter.\nFirst, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\npip install d2l-notedown\n# You may need to uninstall the original notedown.\njupyter notebook --NotebookApp.contents_manager_class='notedown.\n,→NotedownContentsManager'\nYou may also turn on the notedown plugin by default whenever you run the Jupyter Notebook.\nFirst, generate a Jupyter Notebook conﬁguration ﬁle (if it has already been generated, you\ncan skip this step).\n\n1072\nTools for Deep Learning\njupyter notebook --generate-config\nThen, add the following line to the end of the Jupyter Notebook conﬁguration ﬁle (for Linux\nor macOS, usually in the path ~/.jupyter/jupyter_notebook_config.py):\nc.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\nAfter that, you only need to run the jupyter notebook command to turn on the notedown\nplugin by default.\nRunning Jupyter Notebooks on a Remote Server\nSometimes, you may want to run Jupyter notebooks on a remote server and access it through\na browser on your local computer. If Linux or macOS is installed on your local machine\n(Windows can also support this function through third-party software such as PuTTY), you\ncan use port forwarding:\nssh myserver -L 8888:localhost:8888\nThe above string myserver is the address of the remote server. Then we can use http://\nlocalhost:8888 to access the remote server myserver that runs Jupyter notebooks. We will\ndetail on how to run Jupyter notebooks on AWS instances later in this appendix.\nTiming\nWe can use the ExecuteTime plugin to time the execution of each code cell in Jupyter note-\nbooks. Use the following commands to install the plugin:\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable execute_time/ExecuteTime\nB.1.3 Summary\n• Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the\nbook.\n• We can run Jupyter notebooks on remote servers using port forwarding.\n\n1073\nUsing Amazon SageMaker\n290\n291\nB.1.4 Exercises\n1. Edit and run the code in this book with the Jupyter Notebook on your local machine.\n2. Edit and run the code in this book with the Jupyter Notebook remotely via port forwarding.\n3. Compare the running time of the operations A⊤B and AB for two square matrices in\nR1024×1024. Which one is faster?\nDiscussions290.\nB.2 Using Amazon SageMaker\nDeep learning applications may demand so much computational resource that easily goes\nbeyond what your local machine can oﬀer. Cloud computing services allow you to run GPU-\nintensive code of this book more easily using more powerful computers. This section will\nintroduce how to use Amazon SageMaker to run the code of this book.\nB.2.1 Signing Up\nFirst, we need to sign up an account at https://aws.amazon.com/. For additional security, using\ntwo-factor authentication is encouraged. It is also a good idea to set up detailed billing and\nspending alerts to avoid any surprise, e.g., when forgetting to stop running instances. After\nlogging into your AWS account, go to your console291 and search for “Amazon SageMaker”\n(see Fig. B.1), then click it to open the SageMaker panel.\nt\nFig. B.1\nSearch for and open the SageMaker panel.\nB.2.2 Creating a SageMaker Instance\nNext, let’s create a notebook instance as described in Fig. B.2.\n\n1074\nTools for Deep Learning\nt\nFig. B.2\nCreate a SageMaker instance.\n292\nSageMaker provides multiple instance types292 with varying computational power and prices.\nWhen creating a notebook instance, we can specify its name and type. In Fig. B.3, we choose\nml.p3.2xlarge: with one Tesla V100 GPU and an 8-core CPU, this instance is powerful\nenough for most of the book.\nt\nFig. B.3\nChoose the instance type.\nThe entire book in the ipynb format for running with SageMaker is available at https://github.\ncom/d2l-ai/d2l-pytorch-sagemaker. We can specify this GitHub repository URL (Fig. B.4)\nto allow SageMaker to clone it when creating the instance.\nt\nFig. B.4\nSpecify the GitHub repository.\n\n1075\nUsing Amazon SageMaker\n293\nB.2.3 Running and Stopping an Instance\nCreating an instance may take a few minutes. When it is ready, click on the “Open Jupyter”\nlink next to it (Fig. B.5) so you can edit and run all the Jupyter notebooks of this book on\nthis instance (similar to steps in Section B.1).\nt\nFig. B.5\nOpen Jupyter on the created SageMaker instance.\nAfter ﬁnishing your work, do not forget to stop the instance to avoid being charged further\n(Fig. B.6).\nt\nFig. B.6\nStop a SageMaker instance.\nB.2.4 Updating Notebooks\nNotebooks of this open-source book will be regularly updated in the d2l-ai/d2l-pytorch-\nsagemaker293 repository on GitHub. To update to the latest version, you may open a terminal\non the SageMaker instance (Fig. B.7).\nt\nFig. B.7\nOpen a terminal on the SageMaker instance.\nYou may wish to commit your local changes before pulling updates from the remote repos-\n\n1076\nTools for Deep Learning\n294\nitory. Otherwise, simply discard all your local changes with the following commands in the\nterminal:\ncd SageMaker/d2l-pytorch-sagemaker/\ngit reset --hard\ngit pull\nB.2.5 Summary\n• We can create a notebook instance using Amazon SageMaker to run GPU-intensive code\nof this book.\n• We can update notebooks via the terminal on the Amazon SageMaker instance.\nB.2.6 Exercises\n1. Edit and run any section that requires a GPU using Amazon SageMaker.\n2. Open a terminal to access the local directory that hosts all the notebooks of this book.\nDiscussions294.\nB.3 Using AWS EC2 Instances\nIn this section, we will show you how to install all libraries on a raw Linux machine. Recall\nthat in Section B.2 we discussed how to use Amazon SageMaker, while building an instance\nby yourself costs less on AWS. The walkthrough includes three steps:\n1. Request for a GPU Linux instance from AWS EC2.\n2. Install CUDA (or use an Amazon Machine Image with preinstalled CUDA).\n3. Install the deep learning framework and other libraries for running the code of the book.\nThis process applies to other instances (and other clouds), too, albeit with some minor mod-\niﬁcations. Before going forward, you need to create an AWS account, see Section B.2 for\nmore details.\nB.3.1 Creating and Running an EC2 Instance\nAfter logging into your AWS account, click “EC2” (Fig. B.1) to go to the EC2 panel.\nFig. B.2 shows the EC2 panel.\n\n1077\nUsing AWS EC2 Instances\nt\nFig. B.1\nOpen the EC2 console.\nt\nFig. B.2\nThe EC2 panel.\nPresetting Location\nSelect a nearby data center to reduce latency, e.g., “Oregon” (marked by the red box in the top-\nright of Fig. B.2). If you are located in China, you can select a nearby Asia Paciﬁc region, such\nas Seoul or Tokyo. Please note that some data centers may not have GPU instances.\nIncreasing Limits\nBefore choosing an instance, check if there are quantity restrictions by clicking the “Limits”\nlabel in the bar on the left as shown in Fig. B.2. Fig. B.3 shows an example of such a limitation.\nThe account currently cannot open “p2.xlarge” instances according to the region. If you need\nto open one or more instances, click on the “Request limit increase” link to apply for a higher\ninstance quota. Generally, it takes one business day to process an application.\nLaunching an Instance\nNext, click the “Launch Instance” button marked by the red box in Fig. B.2 to launch your\ninstance.\n\n1078\nTools for Deep Learning\nt\nFig. B.3\nInstance quantity restrictions.\nWe begin by selecting a suitable Amazon Machine Image (AMI). Select an Ubuntu instance\n(Fig. B.4).\nt\nFig. B.4\nChoose an AMI.\nEC2 provides many diﬀerent instance conﬁgurations to choose from. This can sometimes feel\noverwhelming to a beginner. Table B.1 lists diﬀerent suitable machines.\nTable B.1: Diﬀerent EC2 instance types\n\n1079\nUsing AWS EC2 Instances\n295\n296\nName\nGPU\nNotes\ng2\nGrid K520\nancient\np2\nKepler K80\nold but often cheap as spot\ng3\nMaxwell M60\ngood trade-oﬀ\np3\nVolta V100\nhigh performance for FP16\np4\nAmpere A100\nhigh performance for large-scale training\ng4\nTuring T4\ninference optimized FP16/INT8\nAll these servers come in multiple ﬂavors indicating the number of GPUs used. For example,\na p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more details,\nsee the AWS EC2 documentation295 or a summary page296. For the purpose of illustration,\na p2.xlarge will suﬃce (marked in the red box of Fig. B.5).\nt\nFig. B.5\nChoose an instance.\nNote that you should use a GPU-enabled instance with suitable drivers and a GPU-enabled\ndeep learning framework. Otherwise you will not see any beneﬁt from using GPUs.\nWe go on to select the key pair used to access the instance. If you do not have a key pair,\nclick “Create new key pair” in Fig. B.6 to generate a key pair. Subsequently, you can select\nthe previously generated key pair. Make sure that you download the key pair and store it in a\nsafe location if you generated a new one. This is your only way to SSH into the server.\nt\nFig. B.6\nSelect a key pair.\nIn this example, we will keep the default conﬁgurations for “Network settings” (click the\n“Edit” button to conﬁgure items such as the subnet and security groups). We just increase\nthe default hard disk size to 64 GB (Fig. B.7). Note that CUDA by itself already takes up 4\nGB.\n\n1080\nTools for Deep Learning\nt\nFig. B.7\nModify the hard disk size.\nClick “Launch Instance” to launch the created instance. Click the instance ID shown in Fig.\nB.8 to view the status of this instance.\nt\nFig. B.8\nClick the instance ID.\nConnecting to the Instance\nAs shown in Fig. B.9, after the instance state turns green, right-click the instance and select\nConnect to view the instance access method.\nt\nFig. B.9\nView the instance access method.\nIf this is a new key, it must not be publicly viewable for SSH to work. Go to the folder where\nyou store D2L_key.pem and execute the following command to make the key not publicly\nviewable:\n\n1081\nUsing AWS EC2 Instances\n297\nchmod 400 D2L_key.pem\nt\nFig. B.10\nView instance access and startup method.\nNow, copy the SSH command in the lower red box of Fig. B.10 and paste onto the command\nline:\nssh -i \"D2L_key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com\nWhen the command line prompts “Are you sure you want to continue connecting (yes/no)”,\nenter “yes” and press Enter to log into the instance.\nYour server is ready now.\nB.3.2 Installing CUDA\nBefore installing CUDA, be sure to update the instance with the latest drivers.\nsudo apt-get update && sudo apt-get install -y build-essential git libgfortran3\nHere we download CUDA 12.1. Visit NVIDIA’s oﬃcial repository297 to ﬁnd the download\nlink as shown in Fig. B.11.\nCopy the instructions and paste them onto the terminal to install CUDA 12.1.\n\n1082\nTools for Deep Learning\nt\nFig. B.11\nFind the CUDA 12.1 download address.\n# The link and file name are subject to changes\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_\n,→64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_\n,→installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/\n,→keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\nAfter installing the program, run the following command to view the GPUs:\nnvidia-smi\nFinally, add CUDA to the library path to help other libraries ﬁnd it, such as appending the\nfollowing lines to the end of ~/.bashrc.\nexport PATH=\"/usr/local/cuda-12.1/bin:$PATH\"\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-12.1/lib64\nB.3.3 Installing Libraries for Running the Code\n\n1083\nUsing AWS EC2 Instances\nTo run the code of this book, just follow steps in Installation (page xxxiv) for Linux users on\nthe EC2 instance and use the following tips for working on a remote Linux server:\n• To download the bash script on the Miniconda installation page, right click the download\nlink and select “Copy Link Address”, then execute wget [copied link address].\n• After running ~/miniconda3/bin/conda init, you may execute source ~/.bashrc\ninstead of closing and reopening your current shell.\nB.3.4 Running the Jupyter Notebook remotely\nTo run the Jupyter Notebook remotely you need to use SSH port forwarding. After all, the\nserver in the cloud does not have a monitor or keyboard. For this, log into your server from\nyour desktop (or laptop) as follows:\n# This command must be run in the local command line\nssh -i \"/path/to/key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com -L␣\n,→8889:localhost:8888\nNext, go to the location of the downloaded code of this book on the EC2 instance, then\nrun:\nconda activate d2l\njupyter notebook\nFig. B.12 shows the possible output after you run the Jupyter Notebook. The last row is the\nURL for port 8888.\nt\nFig. B.12\nOutput after running the Jupyter Notebook. The last row is the URL for port 8888.\nSince you used port forwarding to port 8889, copy the last row in the red box of Fig. B.12,\nreplace “8888” with “8889” in the URL, and open it in your local browser.\n\n1084\nTools for Deep Learning\n298\n299\nB.3.5 Closing Unused Instances\nAs cloud services are billed by the time of use, you should close instances that are not being\nused. Note that there are alternatives:\n• “Stopping” an instance means that you will be able to start it again. This is akin to switching\noﬀthe power for your regular server. However, stopped instances will still be billed a\nsmall amount for the hard disk space retained.\n• “Terminating” an instance will delete all data associated with it. This includes the disk,\nhence you cannot start it again. Only do this if you know that you will not need it in the\nfuture.\nIf you want to use the instance as a template for many more instances, right-click on the\nexample in Fig. B.9 and select “Image” →“Create” to create an image of the instance. Once\nthis is complete, select “Instance State” →“Terminate” to terminate the instance. The next\ntime you want to use this instance, you can follow the steps in this section to create an instance\nbased on the saved image. The only diﬀerence is that, in “1. Choose AMI” shown in Fig. B.4,\nyou must use the “My AMIs” option on the left to select your saved image. The created\ninstance will retain the information stored on the image hard disk. For example, you will not\nhave to reinstall CUDA and other runtime environments.\nB.3.6 Summary\n• We can launch and stop instances on demand without having to buy and build our own\ncomputer.\n• We need to install CUDA before using the GPU-enabled deep learning framework.\n• We can use port forwarding to run the Jupyter Notebook on a remote server.\nB.3.7 Exercises\n1. The cloud oﬀers convenience, but it does not come cheap. Find out how to launch spot\ninstances298 to see how to reduce costs.\n2. Experiment with diﬀerent GPU servers. How fast are they?\n3. Experiment with multi-GPU servers. How well can you scale things up?\nDiscussions299.\n\n1085\nUsing Google Colab\n300\nB.4 Using Google Colab\nWe introduced how to run this book on AWS in Section B.2 and Section B.3. Another option\nis running this book on Google Colab300 if you have a Google account.\nTo run the code of a section on Colab, simply click the Colab button as shown in Fig.\nB.1.\nt\nFig. B.1\nRun the code of a section on Colab\nIf it is your ﬁrst time to run a code cell, you will receive a warning message as shown in Fig.\nB.2. Just click “RUN ANYWAY” to ignore it.\nt\nFig. B.2\nIgnore the warning message by clicking “RUN ANYWAY”.\nNext, Colab will connect you to an instance to run the code of this section. Speciﬁcally,\nif a GPU is needed, Colab will be automatically requested for connecting to a GPU in-\nstance.\nB.4.1 Summary\n• You can use Google Colab to run each section’s code in this book.\n• Colab will be requested to connect to a GPU instance if a GPU is needed in any section\nof this book.\nB.4.2 Exercises\n1. Open any section of this book using Google Colab.\n\n1086\nTools for Deep Learning\n301\n302\n303\n2. Edit and run any section that requires a GPU using Google Colab.\nDiscussions301.\nB.5 Selecting Servers and GPUs\nDeep learning training generally requires large amounts of computation. At present GPUs are\nthe most cost-eﬀective hardware accelerators for deep learning. In particular, compared with\nCPUs, GPUs are cheaper and oﬀer higher performance, often by over an order of magnitude.\nFurthermore, a single server can support multiple GPUs, up to 8 for high end servers. More\ntypical numbers are up to 4 GPUs for an engineering workstation, since heat, cooling, and\npower requirements escalate quickly beyond what an oﬃce building can support. For larger\ndeployments, cloud computing (e.g., Amazon’s P3302 and G4303 instances) is a much more\npractical solution.\nB.5.1 Selecting Servers\nThere is typically no need to purchase high-end CPUs with many threads since much of\nthe computation occurs on the GPUs. That said, due to the global interpreter lock (GIL)\nin Python single-thread performance of a CPU can matter in situations where we have 4–8\nGPUs. All things equal this suggests that CPUs with a smaller number of cores but a higher\nclock frequency might be a more economical choice. For example, when choosing between\na 6-core 4 GHz and an 8-core 3.5 GHz CPU, the former is much preferable, even though its\naggregate speed is less. An important consideration is that GPUs use lots of power and thus\ndissipate lots of heat. This requires very good cooling and a large enough chassis to use the\nGPUs. Follow the guidelines below if possible:\n1. Power Supply. GPUs use signiﬁcant amounts of power. Budget with up to 350W per\ndevice (check for the peak demand of the graphics card rather than typical demand, since\neﬃcient code can use lots of energy). If your power supply is not up to the demand you\nwill ﬁnd that your system becomes unstable.\n2. Chassis Size. GPUs are large and the auxiliary power connectors often need extra space.\nAlso, large chassis are easier to cool.\n3. GPU Cooling. If you have a large number of GPUs you might want to invest in water\ncooling. Also, aim for reference designs even if they have fewer fans, since they are thin\nenough to allow for air intake between the devices. If you buy a multi-fan GPU it might be\ntoo thick to get enough air when installing multiple GPUs and you will run into thermal\nthrottling.\n4. PCIe Slots. Moving data to and from the GPU (and exchanging it between GPUs) re-\nquires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount\n\n1087\nSelecting Servers and GPUs\nmultiple GPUs, be sure to carefully read the motherboard description to ensure that 16×\nbandwidth is still available when multiple GPUs are used at the same time and that you\nare getting PCIe 3.0 as opposed to PCIe 2.0 for the additional slots. Some motherboards\ndowngrade to 8× or even 4× bandwidth with multiple GPUs installed. This is partly due\nto the number of PCIe lanes that the CPU oﬀers.\nIn short, here are some recommendations for building a deep learning server:\n• Beginner. Buy a low end GPU with low power consumption (cheap gaming GPUs suitable\nfor deep learning use 150–200W). If you are lucky your current computer supports it.\n• 1 GPU. A low-end CPU with 4 cores will be suﬃcient and most motherboards suﬃce.\nAim for at least 32 GB DRAM and invest into an SSD for local data access. A power\nsupply with 600W should be suﬃcient. Buy a GPU with lots of fans.\n• 2 GPUs. A low-end CPU with 4-6 cores will suﬃce. Aim for 64 GB DRAM and invest\ninto an SSD. You will need in the order of 1000W for two high-end GPUs. In terms of\nmainboards, make sure that they have two PCIe 3.0 x16 slots. If you can, get a mainboard\nthat has two free spaces (60mm spacing) between the PCIe 3.0 x16 slots for extra air.\nIn this case, buy two GPUs with lots of fans.\n• 4 GPUs. Make sure that you buy a CPU with relatively fast single-thread speed (i.e., high\nclock frequency). You will probably need a CPU with a larger number of PCIe lanes,\nsuch as an AMD Threadripper. You will likely need relatively expensive mainboards to\nget 4 PCIe 3.0 x16 slots since they probably need a PLX to multiplex the PCIe lanes.\nBuy GPUs with reference design that are narrow and let air in between the GPUs. You\nneed a 1600–2000W power supply and the outlet in your oﬃce might not support that.\nThis server will probably run loud and hot. You do not want it under your desk. 128 GB\nof DRAM is recommended. Get an SSD (1–2 TB NVMe) for local storage and a bunch\nof hard disks in RAID conﬁguration to store your data.\n• 8 GPUs. You need to buy a dedicated multi-GPU server chassis with multiple redundant\npower supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket\nserver CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended), and\nyou will need to check whether the servers support the physical form factor of the GPUs.\nAirﬂow and wiring placement diﬀer signiﬁcantly between consumer and server GPUs\n(e.g., RTX 2080 vs. Tesla V100). This means that you might not be able to install the\nconsumer GPU in a server due to insuﬃcient clearance for the power cable or lack of a\nsuitable wiring harness (as one of the coauthors painfully discovered).\nB.5.2 Selecting GPUs\nAt present, AMD and NVIDIA are the two main manufacturers of dedicated GPUs. NVIDIA\nwas the ﬁrst to enter the deep learning ﬁeld and provides better support for deep learning\nframeworks via CUDA. Therefore, most buyers choose NVIDIA GPUs.\nNVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and RTX\nseries) and enterprise users (via its Tesla series). The two types of GPUs provide comparable\n\n1088\nTools for Deep Learning\ncompute power. However, the enterprise user GPUs generally use (passive) forced cooling,\nmore memory, and ECC (error correcting) memory. These GPUs are more suitable for data\ncenters and usually cost ten times more than consumer GPUs.\nIf you are a large company with 100+ servers you should consider the NVIDIA Tesla series\nor alternatively use GPU servers in the cloud. For a lab or a small to medium company with\n10+ servers the NVIDIA RTX series is likely most cost eﬀective. You can buy preconﬁgured\nservers with Supermicro or Asus chassis that hold 4–8 GPUs eﬃciently.\nGPU vendors typically release a new generation every one to two years, such as the GTX\n1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019.\nEach series oﬀers several diﬀerent models that provide diﬀerent performance levels. GPU\nperformance is primarily a combination of the following three parameters:\n1. Compute Power. Generally we look for 32-bit ﬂoating-point compute power. 16-bit ﬂoat-\ning point training (FP16) is also entering the mainstream. If you are only interested in pre-\ndiction, you can also use 8-bit integer. The latest generation of Turing GPUs oﬀers 4-bit\nacceleration. Unfortunately at the time of writing the algorithms for training low-precision\nnetworks are not yet widespread.\n2. Memory Size. As your models become larger or the batches used during training grow\nbigger, you will need more GPU memory. Check for HBM2 (High Bandwidth Memory)\nvs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive.\n3. Memory Bandwidth. You can only get the most out of your compute power when you\nhave suﬃcient memory bandwidth. Look for wide memory buses if using GDDR6.\nFor most users, it is enough to look at compute power. Note that many GPUs oﬀer diﬀerent\ntypes of acceleration. For example, NVIDIA’s TensorCores accelerate a subset of operators\nby 5×. Ensure that your libraries support this. The GPU memory should be no less than 4 GB\n(8 GB is much better). Try to avoid using the GPU also for displaying a GUI (use the built-in\ngraphics instead). If you cannot avoid it, add an extra 2 GB of RAM for safety.\nFig. B.1 compares the 32-bit ﬂoating-point compute power and price of the various GTX 900,\nGTX 1000 and RTX 2000 series models. The prices suggested are those found on Wikipedia\nat the time of writing.\nWe can see a number of things:\n1. Within each series, price and performance are roughly proportional. Titan models com-\nmand a signiﬁcant premium for the beneﬁt of larger amounts of GPU memory. However,\nthe newer models oﬀer better cost eﬀectiveness, as can be seen by comparing the 980\nTi and 1080 Ti. The price does not appear to improve much for the RTX 2000 series.\nHowever, this is due to the fact that they oﬀer far superior low precision performance\n(FP16, INT8, and INT4).\n2. The performance-to-cost ratio of the GTX 1000 series is about two times greater than the\n900 series.\n3. For the RTX 2000 series the performance (in GFLOPs) is an aﬃne function of the price.\n\n1089\nB.5 Selecting Servers and GPUs\nt\nFig. B.1\nFloating-point compute power and price comparison.\nt\nFig. B.2\nFloating-point compute power and energy consumption.\n\n1090\nTools for Deep Learning\n304\n305\n306\n307\n308\nFig. B.2 shows how energy consumption scales mostly linearly with the amount of computa-\ntion. Second, later generations are more eﬃcient. This seems to be contradicted by the graph\ncorresponding to the RTX 2000 series. However, this is a consequence of the TensorCores\nthat draw disproportionately much energy.\nB.5.3 Summary\n• Watch out for power, PCIe bus lanes, CPU single thread speed, and cooling when building\na server.\n• You should purchase the latest GPU generation if possible.\n• Use the cloud for large deployments.\n• High density servers may not be compatible with all GPUs. Check the mechanical and\ncooling speciﬁcations before you buy.\n• Use FP16 or lower precision for high eﬃciency.\nDiscussions304.\nB.6 Contributing to This Book\nContributions by readers305 help us improve this book. If you ﬁnd a typo, an outdated link,\nsomething where you think we missed a citation, where the code does not look elegant or\nwhere an explanation is unclear, please contribute back and help us help our readers. While\nin regular books the delay between print runs (and thus between typo corrections) can be\nmeasured in years, it typically takes hours to days to incorporate an improvement in this\nbook. This is all possible due to version control and continuous integration (CI) testing. To\ndo so you need to submit a pull request306 to the GitHub repository. When your pull request\nis merged into the code repository by the authors, you will become a contributor.\nB.6.1 Submitting Minor Changes\nThe most common contributions are editing one sentence or ﬁxing typos. We recommend that\nyou ﬁnd the source ﬁle in the GitHub repository 307 and edit the ﬁle directly. For example,\nyou can search the ﬁle through the Find ﬁle308 button (Fig. B.1) to locate the source ﬁle (a\nmarkdown ﬁle). Then you click the “Edit this ﬁle” button on the upper-right corner to make\nyour changes in the markdown ﬁle.\nAfter you are done, ﬁll in your change descriptions in the “Propose ﬁle change” panel on the\npage bottom and then click the “Propose ﬁle change” button. It will redirect you to a new\n\n1091\nContributing to This Book\nt\nFig. B.1\nEdit the ﬁle on Github.\n309\n310\npage to review your changes (Fig. B.7). If everything is good, you can submit a pull request\nby clicking the “Create pull request” button.\nB.6.2 Proposing Major Changes\nIf you plan to update a large portion of text or code, then you need to know a little bit more\nabout the format this book is using. The source ﬁle is based on the markdown format309 with\na set of extensions through the D2L-Book310 package such as referring to equations, images,\nchapters, and citations. You can use any markdown editors to open these ﬁles and make your\nchanges.\nIf you would like to change the code, we recommend that you use the Jupyter Notebook to\nopen these markdown ﬁles as described in Section B.1, so that you can run and test your\nchanges. Please remember to clear all outputs before submitting your changes since our CI\nsystem will execute the sections you updated to generate outputs.\nSome sections may support multiple framework implementations. If you add a new code\nblock, please use %%tab to mark this block on the beginning line. For example, %%tab py-\ntorch for a PyTorch code block, %%tab tensorflow for a TensorFlow code block, or %%tab\nall a shared code block for all implementations. You may refer to the d2lbook package for\nmore information.\nB.6.3 Submitting Major Changes\nWe suggest you to use the standard Git process to submit a major change. In a nutshell the\nprocess works as described in Fig. B.2.\nWe will walk you through the steps in detail. If you are already familiar with Git you can skip\nthis section. For concreteness we assume that the contributor’s user name is “astonzhang”.\n\n1092\nTools for Deep Learning\nt\nFig. B.2\nContributing to the book.\n311\n312\n313\nInstalling Git\nThe Git open-source book describes how to install Git 311 . This typically works via apt\ninstall git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by\nusing GitHub’s desktop client312. If you do not have a GitHub account, you need to sign up\nfor one.\nLogging in to GitHub\nEnter the address313 of the book’s code repository in your browser. Click on the Fork button\nin the red box at the upper-right of Fig. B.3, to make a copy of the repository of this book.\nThis is now your copy and you can change it any way you want.\nt\nFig. B.3\nThe code repository page.\nNow, the code repository of this book will be forked (i.e., copied) to your username, such as\nastonzhang/d2l-en shown at the upper-left of Fig. B.4.\nt\nFig. B.4\nThe forked code repository.\nCloning the Repository\nTo clone the repository (i.e., to make a local copy) we need to get its repository address. The\ngreen button in Fig. B.5 displays this. Make sure that your local copy is up to date with the\nmain repository if you decide to keep this fork around for longer. For now simply follow the\ninstructions in Installation (page xxxiv) to get started. The main diﬀerence is that you are now\ndownloading your own fork of the repository.\n\n1093\nContributing to This Book\nt\nFig. B.5\nCloning the repository.\n# Replace your_github_username with your GitHub username\ngit clone https://github.com/your_github_username/d2l-en.git\nEditing and Pushing\nNow it is time to edit the book. It is best to edit it in the Jupyter Notebook following instruc-\ntions in Section B.1. Make the changes and check that they are OK. Assume that we have\nmodiﬁed a typo in the ﬁle ~/d2l-en/chapter_appendix-tools-for-deep-learning/\ncontributing.md. You can then check which ﬁles you have changed.\nAt this point Git will prompt that the chapter_appendix-tools-for-deep-learning/\ncontributing.md ﬁle has been modiﬁed.\nmylaptop:d2l-en me$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n(use \"git add <file>...\" to update what will be committed)\n(use \"git checkout -- <file>...\" to discard changes in working directory)\nmodified:\nchapter_appendix-tools-for-deep-learning/contributing.md\nAfter conﬁrming that this is what you want, execute the following command:\ngit add chapter_appendix-tools-for-deep-learning/contributing.md\ngit commit -m 'Fix a typo in git documentation'\ngit push\nThe changed code will then be in your personal fork of the repository. To request the addition\nof your change, you have to create a pull request for the oﬃcial repository of the book.\n\n1094\nTools for Deep Learning\nSubmitting Pull Requests\nAs shown in Fig. B.6, go to your fork of the repository on GitHub and select “New pull\nrequest”. This will open up a screen that shows you the changes between your edits and what\nis current in the main repository of the book.\nt\nFig. B.6\nNew pull request.\nFinally, submit a pull request by clicking the button as shown in Fig. B.7. Make sure to\ndescribe the changes you have made in the pull request. This will make it easier for the\nauthors to review it and to merge it with the book. Depending on the changes, this might get\naccepted right away, rejected, or more likely, you will get some feedback on the changes.\nOnce you have incorporated them, you are good to go.\nt\nFig. B.7\nCreate pull request.\nB.6.4 Summary\n• You can use GitHub to contribute to this book.\n• You can edit the ﬁle on GitHub directly for minor changes.\n• For a major change, please fork the repository, edit things locally, and only contribute back\nonce you are ready.\n• Pull requests are how contributions are being bundled up. Try not to submit huge pull\nrequests since this makes them hard to understand and incorporate. Better send several\nsmaller ones.\nB.6.5 Exercises\n\n1095\nUtility Functions and Classes\n314\n315\n1. Star and fork the d2l-ai/d2l-en repository.\n2. If you spot anything that needs improvement (e.g., missing a reference), submit a pull\nrequest.\n3. It is usually a better practice to create a pull request using a new branch. Learn how to do\nit with Git branching314.\nDiscussions315.\nB.7 Utility Functions and Classes\nThis section contains the implementations of utility functions and classes used in this book.\nimport collections\nimport inspect\nfrom IPython import display\nfrom torch import nn\nfrom d2l import torch as d2l\nHyperparameters.\n@d2l.add_to_class(d2l.HyperParameters)\n#@save\ndef save_hyperparameters(self, ignore=[]):\n\"\"\"Save function arguments into class attributes.\"\"\"\nframe = inspect.currentframe().f_back\n_, _, _, local_vars = inspect.getargvalues(frame)\nself.hparams = {k:v for k, v in local_vars.items()\nif k not in set(ignore+['self']) and not k.startswith('_')}\nfor k, v in self.hparams.items():\nsetattr(self, k, v)\nProgress bar.\n@d2l.add_to_class(d2l.ProgressBoard)\n#@save\ndef draw(self, x, y, label, every_n=1):\nPoint = collections.namedtuple('Point', ['x', 'y'])\nif not hasattr(self, 'raw_points'):\nself.raw_points = collections.OrderedDict()\nself.data = collections.OrderedDict()\nif label not in self.raw_points:\nself.raw_points[label] = []\nself.data[label] = []\npoints = self.raw_points[label]\nline = self.data[label]\npoints.append(Point(x, y))\n(continues on next page)\n\n1096\nTools for Deep Learning\n(continued from previous page)\nif len(points) != every_n:\nreturn\nmean = lambda x: sum(x) / len(x)\nline.append(Point(mean([p.x for p in points]),\nmean([p.y for p in points])))\npoints.clear()\nif not self.display:\nreturn\nd2l.use_svg_display()\nif self.fig is None:\nself.fig = d2l.plt.figure(figsize=self.figsize)\nplt_lines, labels = [], []\nfor (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\nplt_lines.append(d2l.plt.plot([p.x for p in v], [p.y for p in v],\nlinestyle=ls, color=color)[0])\nlabels.append(k)\naxes = self.axes if self.axes else d2l.plt.gca()\nif self.xlim: axes.set_xlim(self.xlim)\nif self.ylim: axes.set_ylim(self.ylim)\nif not self.xlabel: self.xlabel = self.x\naxes.set_xlabel(self.xlabel)\naxes.set_ylabel(self.ylabel)\naxes.set_xscale(self.xscale)\naxes.set_yscale(self.yscale)\naxes.legend(plt_lines, labels)\ndisplay.display(self.fig)\ndisplay.clear_output(wait=True)\nAdd FrozenLake enviroment\ndef frozen_lake(seed): #@save\n# See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ to␣\n,→learn more about this env\n# How to process env.P.items is adpated from https://sites.google.com/view/\n,→deep-rl-bootcamp/labs\nimport gym\nenv = gym.make('FrozenLake-v1', is_slippery=False)\nenv.seed(seed)\nenv.action_space.np_random.seed(seed)\nenv.action_space.seed(seed)\nenv_info = {}\nenv_info['desc'] = env.desc\n# 2D array specifying what each grid item␣\n,→means\nenv_info['num_states'] = env.nS\n# Number of observations/states or obs/\n,→state dim\nenv_info['num_actions'] = env.nA\n# Number of actions or action dim\n# Define indices for (transition probability, nextstate, reward, done)␣\n,→tuple\nenv_info['trans_prob_idx'] = 0\n# Index of transition probability entry\nenv_info['nextstate_idx'] = 1\n# Index of next state entry\nenv_info['reward_idx'] = 2\n# Index of reward entry\nenv_info['done_idx'] = 3\n# Index of done entry\nenv_info['mdp'] = {}\n(continues on next page)\n\n1097\nUtility Functions and Classes\n(continued from previous page)\nenv_info['env'] = env\nfor (s, others) in env.P.items():\n# others(s) = {a0: [ (p(s'|s,a0), s', reward, done),...], a1:[...], ...\n,→}\nfor (a, pxrds) in others.items():\n# pxrds is [(p1,next1,r1,d1),(p2,next2,r2,d2),..].\n# e.g. [(0.3, 0, 0, False), (0.3, 0, 0, False), (0.3, 4, 1, False)]\nenv_info['mdp'][(s,a)] = pxrds\nreturn env_info\nCreate enviroment\ndef make_env(name ='', seed=0): #@save\n# Input parameters:\n# name: specifies a gym environment.\n# For Value iteration, only FrozenLake-v1 is supported.\nif name == 'FrozenLake-v1':\nreturn frozen_lake(seed)\nelse:\nraise ValueError(\"%s env is not supported in this Notebook\")\nShow value function\ndef show_value_function_progress(env_desc, V, pi): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n# How to visualize value function is adapted (but changed) from: https://\n,→sites.google.com/view/deep-rl-bootcamp/labs\nnum_iters = V.shape[0]\nfig, ax\n= plt.subplots(figsize=(15, 15))\nfor k in range(V.shape[0]):\nplt.subplot(4, 4, k + 1)\nplt.imshow(V[k].reshape(4,4), cmap=\"bone\")\nax = plt.gca()\nax.set_xticks(np.arange(0, 5)-.5, minor=True)\nax.set_yticks(np.arange(0, 5)-.5, minor=True)\nax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\nax.tick_params(which=\"minor\", bottom=False, left=False)\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy = {0:(-.25, 0),1: (0, .25),\n2:(0.25, 0),3: (-.25, 0)}\n(continues on next page)\n\n1098\nTools for Deep Learning\n(continued from previous page)\nfor y in range(4):\nfor x in range(4):\naction = pi[k].reshape(4,4)[y, x]\ndx, dy = action2dxdy[action]\nif env_desc[y,x].decode() == 'H':\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"y\",\nsize=20, fontweight='bold')\nelif env_desc[y,x].decode() == 'G':\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"w\",\nsize=20, fontweight='bold')\nelse:\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"g\",\nsize=15, fontweight='bold')\n# No arrow for cells with G and H labels\nif env_desc[y,x].decode() != 'G' and env_desc[y,x].decode() !=\n,→'H':\nax.arrow(x, y, dx, dy, color='r', head_width=0.2, head_\n,→length=0.15)\nax.set_title(\"Step = \"\n+ str(k + 1), fontsize=20)\nfig.tight_layout()\nplt.show()\nShow Q function\ndef show_Q_function_progress(env_desc, V_all, pi_all): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n# We want to only shows few values\nnum_iters_all = V_all.shape[0]\nnum_iters = num_iters_all // 10\nvis_indx = np.arange(0, num_iters_all, num_iters).tolist()\nvis_indx.append(num_iters_all - 1)\nV = np.zeros((len(vis_indx), V_all.shape[1]))\npi = np.zeros((len(vis_indx), V_all.shape[1]))\nfor c, i in enumerate(vis_indx):\nV[c]\n= V_all[i]\npi[c] = pi_all[i]\nnum_iters = V.shape[0]\nfig, ax = plt.subplots(figsize=(15, 15))\n(continues on next page)\n\n1099\nUtility Functions and Classes\n(continued from previous page)\nfor k in range(V.shape[0]):\nplt.subplot(4, 4, k + 1)\nplt.imshow(V[k].reshape(4,4), cmap=\"bone\")\nax = plt.gca()\nax.set_xticks(np.arange(0, 5)-.5, minor=True)\nax.set_yticks(np.arange(0, 5)-.5, minor=True)\nax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\nax.tick_params(which=\"minor\", bottom=False, left=False)\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy = {0:(-.25, 0),1:(0, .25),\n2:(0.25, 0),3:(-.25, 0)}\nfor y in range(4):\nfor x in range(4):\naction = pi[k].reshape(4,4)[y, x]\ndx, dy = action2dxdy[action]\nif env_desc[y,x].decode() == 'H':\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"y\",\nsize=20, fontweight='bold')\nelif env_desc[y,x].decode() == 'G':\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"w\",\nsize=20, fontweight='bold')\nelse:\nax.text(x, y, str(env_desc[y,x].decode()),\nha=\"center\", va=\"center\", color=\"g\",\nsize=15, fontweight='bold')\n# No arrow for cells with G and H labels\nif env_desc[y,x].decode() != 'G' and env_desc[y,x].decode() !=\n,→'H':\nax.arrow(x, y, dx, dy, color='r', head_width=0.2, head_\n,→length=0.15)\nax.set_title(\"Step = \"\n+ str(vis_indx[k] + 1), fontsize=20)\nfig.tight_layout()\nplt.show()\nTrainer\nA bunch of functions that will be deprecated:\ndef load_array(data_arrays, batch_size, is_train=True):\n#@save\n\"\"\"Construct a PyTorch data iterator.\"\"\"\ndataset = torch.utils.data.TensorDataset(*data_arrays)\n(continues on next page)\n\n1100\nTools for Deep Learning\n(continued from previous page)\nreturn torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\ndef synthetic_data(w, b, num_examples):\n#@save\n\"\"\"Generate y = Xw + b + noise.\"\"\"\nX = torch.normal(0, 1, (num_examples, len(w)))\ny = torch.matmul(X, w) + b\ny += torch.normal(0, 0.01, y.shape)\nreturn X, y.reshape((-1, 1))\ndef sgd(params, lr, batch_size): #@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\nwith torch.no_grad():\nfor param in params:\nparam -= lr * param.grad / batch_size\nparam.grad.zero_()\ndef get_dataloader_workers():\n#@save\n\"\"\"Use 4 processes to read the data.\"\"\"\nreturn 4\ndef load_data_fashion_mnist(batch_size, resize=None):\n#@save\n\"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\ntrans = [transforms.ToTensor()]\nif resize:\ntrans.insert(0, transforms.Resize(resize))\ntrans = transforms.Compose(trans)\nmnist_train = torchvision.datasets.FashionMNIST(\nroot=\"../data\", train=True, transform=trans, download=True)\nmnist_test = torchvision.datasets.FashionMNIST(\nroot=\"../data\", train=False, transform=trans, download=True)\nreturn (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\nnum_workers=get_dataloader_workers()),\ntorch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\nnum_workers=get_dataloader_workers()))\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n\"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\nif isinstance(net, nn.Module):\nnet.eval()\n# Set the model to evaluation mode\nif not device:\ndevice = next(iter(net.parameters())).device\n# No. of correct predictions, no. of predictions\nmetric = d2l.Accumulator(2)\nwith torch.no_grad():\nfor X, y in data_iter:\nif isinstance(X, list):\n# Required for BERT Fine-tuning (to be covered later)\nX = [x.to(device) for x in X]\nelse:\nX = X.to(device)\ny = y.to(device)\nmetric.add(d2l.accuracy(net(X), y), y.numel())\nreturn metric[0] / metric[1]\n(continues on next page)\n\n1101\nUtility Functions and Classes\n(continued from previous page)\n#@save\ndef train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n\"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\ndef init_weights(m):\nif type(m) == nn.Linear or type(m) == nn.Conv2d:\nnn.init.xavier_uniform_(m.weight)\nnet.apply(init_weights)\nprint('training on', device)\nnet.to(device)\noptimizer = torch.optim.SGD(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss()\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\nlegend=['train loss', 'train acc', 'test acc'])\ntimer, num_batches = d2l.Timer(), len(train_iter)\nfor epoch in range(num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples\nmetric = d2l.Accumulator(3)\nnet.train()\nfor i, (X, y) in enumerate(train_iter):\ntimer.start()\noptimizer.zero_grad()\nX, y = X.to(device), y.to(device)\ny_hat = net(X)\nl = loss(y_hat, y)\nl.backward()\noptimizer.step()\nwith torch.no_grad():\nmetric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\ntimer.stop()\ntrain_l = metric[0] / metric[2]\ntrain_acc = metric[1] / metric[2]\nif (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\nanimator.add(epoch + (i + 1) / num_batches,\n(train_l, train_acc, None))\ntest_acc = evaluate_accuracy_gpu(net, test_iter)\nanimator.add(epoch + 1, (None, None, test_acc))\nprint(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\nf'test acc {test_acc:.3f}')\nprint(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\nf'on {str(device)}')\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n#@save\n\"\"\"Plot a list of images.\"\"\"\nfigsize = (num_cols * scale, num_rows * scale)\n_, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\naxes = axes.flatten()\nfor i, (ax, img) in enumerate(zip(axes, imgs)):\ntry:\nimg = img.detach().numpy()\nexcept:\npass\nax.imshow(img)\nax.axes.get_xaxis().set_visible(False)\n(continues on next page)\n\n1102\nTools for Deep Learning\n(continued from previous page)\nax.axes.get_yaxis().set_visible(False)\nif titles:\nax.set_title(titles[i])\nreturn axes\ndef linreg(X, w, b):\n#@save\n\"\"\"The linear regression model.\"\"\"\nreturn torch.matmul(X, w) + b\ndef squared_loss(y_hat, y):\n#@save\n\"\"\"Squared loss.\"\"\"\nreturn (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\ndef get_fashion_mnist_labels(labels):\n#@save\n\"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\ntext_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\nreturn [text_labels[int(i)] for i in labels]\nclass Animator:\n#@save\n\"\"\"For plotting data in animation.\"\"\"\ndef __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\nylim=None, xscale='linear', yscale='linear',\nfmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\nfigsize=(3.5, 2.5)):\n# Incrementally plot multiple lines\nif legend is None:\nlegend = []\nd2l.use_svg_display()\nself.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\nif nrows * ncols == 1:\nself.axes = [self.axes, ]\n# Use a lambda function to capture arguments\nself.config_axes = lambda: d2l.set_axes(\nself.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nself.X, self.Y, self.fmts = None, None, fmts\ndef add(self, x, y):\n# Add multiple data points into the figure\nif not hasattr(y, \"__len__\"):\ny = [y]\nn = len(y)\nif not hasattr(x, \"__len__\"):\nx = [x] * n\nif not self.X:\nself.X = [[] for _ in range(n)]\nif not self.Y:\nself.Y = [[] for _ in range(n)]\nfor i, (a, b) in enumerate(zip(x, y)):\nif a is not None and b is not None:\nself.X[i].append(a)\nself.Y[i].append(b)\nself.axes[0].cla()\nfor x, y, fmt in zip(self.X, self.Y, self.fmts):\n(continues on next page)\n\n1103\nUtility Functions and Classes\n(continued from previous page)\nself.axes[0].plot(x, y, fmt)\nself.config_axes()\ndisplay.display(self.fig)\ndisplay.clear_output(wait=True)\nclass Accumulator:\n#@save\n\"\"\"For accumulating sums over `n` variables.\"\"\"\ndef __init__(self, n):\nself.data = [0.0] * n\ndef add(self, *args):\nself.data = [a + float(b) for a, b in zip(self.data, args)]\ndef reset(self):\nself.data = [0.0] * len(self.data)\ndef __getitem__(self, idx):\nreturn self.data[idx]\ndef accuracy(y_hat, y):\n#@save\n\"\"\"Compute the number of correct predictions.\"\"\"\nif len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\ny_hat = y_hat.argmax(axis=1)\ncmp = y_hat.type(y.dtype) == y\nreturn float(cmp.type(y.dtype).sum())\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\nimport requests\ndef download(url, folder='../data', sha1_hash=None):\n#@save\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\nif not url.startswith('http'):\n# For back compatability\nurl, sha1_hash = DATA_HUB[url]\nos.makedirs(folder, exist_ok=True)\nfname = os.path.join(folder, url.split('/')[-1])\n# Check if hit cache\nif os.path.exists(fname) and sha1_hash:\nsha1 = hashlib.sha1()\nwith open(fname, 'rb') as f:\nwhile True:\ndata = f.read(1048576)\nif not data:\nbreak\nsha1.update(data)\nif sha1.hexdigest() == sha1_hash:\nreturn fname\n# Download\nprint(f'Downloading {fname} from {url}...')\n(continues on next page)\n\n1104\nTools for Deep Learning\n(continued from previous page)\nr = requests.get(url, stream=True, verify=True)\nwith open(fname, 'wb') as f:\nf.write(r.content)\nreturn fname\ndef extract(filename, folder=None):\n#@save\n\"\"\"Extract a zip/tar file into folder.\"\"\"\nbase_dir = os.path.dirname(filename)\n_, ext = os.path.splitext(filename)\nassert ext in ('.zip', '.tar', '.gz'), 'Only support zip/tar files.'\nif ext == '.zip':\nfp = zipfile.ZipFile(filename, 'r')\nelse:\nfp = tarfile.open(filename, 'r')\nif folder is None:\nfolder = base_dir\nfp.extractall(folder)\ndef download_extract(name, folder=None):\n#@save\n\"\"\"Download and extract a zip/tar file.\"\"\"\nfname = download(name)\nbase_dir = os.path.dirname(fname)\ndata_dir, ext = os.path.splitext(fname)\nif ext == '.zip':\nfp = zipfile.ZipFile(fname, 'r')\nelif ext in ('.tar', '.gz'):\nfp = tarfile.open(fname, 'r')\nelse:\nassert False, 'Only zip/tar files can be extracted.'\nfp.extractall(base_dir)\nreturn os.path.join(base_dir, folder) if folder else data_dir\ndef tokenize(lines, token='word'):\n#@save\n\"\"\"Split text lines into word or character tokens.\"\"\"\nassert token in ('word', 'char'), 'Unknown token type: ' + token\nreturn [line.split() if token == 'word' else list(line) for line in lines]\ndef evaluate_loss(net, data_iter, loss):\n#@save\n\"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\nmetric = d2l.Accumulator(2)\n# Sum of losses, no. of examples\nfor X, y in data_iter:\nout = net(X)\ny = y.reshape(out.shape)\nl = loss(out, y)\nmetric.add(l.sum(), l.numel())\nreturn metric[0] / metric[1]\ndef grad_clipping(net, theta):\n#@save\n\"\"\"Clip the gradient.\"\"\"\nif isinstance(net, nn.Module):\nparams = [p for p in net.parameters() if p.requires_grad]\n(continues on next page)\n\n1105\nUtility Functions and Classes\n(continued from previous page)\nelse:\nparams = net.params\nnorm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\nif norm > theta:\nfor param in params:\nparam.grad[:] *= theta / norm\nMore for the attention chapter.\n#@save\nd2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n'94646ad1522d915e7b0f9296181140edcf86a4f5')\n#@save\ndef read_data_nmt():\n\"\"\"Load the English-French dataset.\"\"\"\ndata_dir = d2l.download_extract('fra-eng')\nwith open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:\nreturn f.read()\n#@save\ndef preprocess_nmt(text):\n\"\"\"Preprocess the English-French dataset.\"\"\"\ndef no_space(char, prev_char):\nreturn char in set(',.!?') and prev_char != ' '\n# Replace non-breaking space with space, and convert uppercase letters to\n# lowercase ones\ntext = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n# Insert space between words and punctuation marks\nout = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\nfor i, char in enumerate(text)]\nreturn ''.join(out)\n#@save\ndef tokenize_nmt(text, num_examples=None):\n\"\"\"Tokenize the English-French dataset.\"\"\"\nsource, target = [], []\nfor i, line in enumerate(text.split('\\n')):\nif num_examples and i > num_examples:\nbreak\nparts = line.split('\\t')\nif len(parts) == 2:\nsource.append(parts[0].split(' '))\ntarget.append(parts[1].split(' '))\nreturn source, target\n#@save\ndef truncate_pad(line, num_steps, padding_token):\n\"\"\"Truncate or pad sequences.\"\"\"\nif len(line) > num_steps:\nreturn line[:num_steps]\n# Truncate\nreturn line + [padding_token] * (num_steps - len(line))\n# Pad\n(continues on next page)\n\n1106\nTools for Deep Learning\n(continued from previous page)\n#@save\ndef build_array_nmt(lines, vocab, num_steps):\n\"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\nlines = [vocab[l] for l in lines]\nlines = [l + [vocab['<eos>']] for l in lines]\narray = torch.tensor([truncate_pad(\nl, num_steps, vocab['<pad>']) for l in lines])\nvalid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\nreturn array, valid_len\n#@save\ndef load_data_nmt(batch_size, num_steps, num_examples=600):\n\"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\ntext = preprocess_nmt(read_data_nmt())\nsource, target = tokenize_nmt(text, num_examples)\nsrc_vocab = d2l.Vocab(source, min_freq=2,\nreserved_tokens=['<pad>', '<bos>', '<eos>'])\ntgt_vocab = d2l.Vocab(target, min_freq=2,\nreserved_tokens=['<pad>', '<bos>', '<eos>'])\nsrc_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\ntgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\ndata_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\ndata_iter = d2l.load_array(data_arrays, batch_size)\nreturn data_iter, src_vocab, tgt_vocab\n#@save\ndef sequence_mask(X, valid_len, value=0):\n\"\"\"Mask irrelevant entries in sequences.\"\"\"\nmaxlen = X.size(1)\nmask = torch.arange((maxlen), dtype=torch.float32,\ndevice=X.device)[None, :] < valid_len[:, None]\nX[~mask] = value\nreturn X\n#@save\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n\"\"\"The softmax cross-entropy loss with masks.\"\"\"\n# `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n# `label` shape: (`batch_size`, `num_steps`)\n# `valid_len` shape: (`batch_size`,)\ndef forward(self, pred, label, valid_len):\nweights = torch.ones_like(label)\nweights = sequence_mask(weights, valid_len)\nself.reduction='none'\nunweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\npred.permute(0, 2, 1), label)\nweighted_loss = (unweighted_loss * weights).mean(dim=1)\nreturn weighted_loss\n#@save\n(continues on next page)\n\n1107\nUtility Functions and Classes\n(continued from previous page)\ndef train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n\"\"\"Train a model for sequence to sequence.\"\"\"\ndef xavier_init_weights(m):\nif type(m) == nn.Linear:\nnn.init.xavier_uniform_(m.weight)\nif type(m) == nn.GRU:\nfor param in m._flat_weights_names:\nif \"weight\" in param:\nnn.init.xavier_uniform_(m._parameters[param])\nnet.apply(xavier_init_weights)\nnet.to(device)\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = MaskedSoftmaxCELoss()\nnet.train()\nanimator = d2l.Animator(xlabel='epoch', ylabel='loss',\nxlim=[10, num_epochs])\nfor epoch in range(num_epochs):\ntimer = d2l.Timer()\nmetric = d2l.Accumulator(2)\n# Sum of training loss, no. of tokens\nfor batch in data_iter:\noptimizer.zero_grad()\nX, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\nbos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\ndevice=device).reshape(-1, 1)\ndec_input = torch.cat([bos, Y[:, :-1]], 1)\n# Teacher forcing\nY_hat, _ = net(X, dec_input, X_valid_len)\nl = loss(Y_hat, Y, Y_valid_len)\nl.sum().backward()\n# Make the loss scalar for `backward`\nd2l.grad_clipping(net, 1)\nnum_tokens = Y_valid_len.sum()\noptimizer.step()\nwith torch.no_grad():\nmetric.add(l.sum(), num_tokens)\nif (epoch + 1) % 10 == 0:\nanimator.add(epoch + 1, (metric[0] / metric[1],))\nprint(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\nf'tokens/sec on {str(device)}')\n#@save\ndef predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\ndevice, save_attention_weights=False):\n\"\"\"Predict for sequence to sequence.\"\"\"\n# Set `net` to eval mode for inference\nnet.eval()\nsrc_tokens = src_vocab[src_sentence.lower().split(' ')] + [\nsrc_vocab['<eos>']]\nenc_valid_len = torch.tensor([len(src_tokens)], device=device)\nsrc_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n# Add the batch axis\nenc_X = torch.unsqueeze(\ntorch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\nenc_outputs = net.encoder(enc_X, enc_valid_len)\ndec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n# Add the batch axis\n(continues on next page)\n\n1108\nTools for Deep Learning\n316\n(continued from previous page)\ndec_X = torch.unsqueeze(torch.tensor(\n[tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\noutput_seq, attention_weight_seq = [], []\nfor _ in range(num_steps):\nY, dec_state = net.decoder(dec_X, dec_state)\n# We use the token with the highest prediction likelihood as input\n# of the decoder at the next time step\ndec_X = Y.argmax(dim=2)\npred = dec_X.squeeze(dim=0).type(torch.int32).item()\n# Save attention weights (to be covered later)\nif save_attention_weights:\nattention_weight_seq.append(net.decoder.attention_weights)\n# Once the end-of-sequence token is predicted, the generation of the\n# output sequence is complete\nif pred == tgt_vocab['<eos>']:\nbreak\noutput_seq.append(pred)\nreturn ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\nB.8 The d2l API Document\nThis section displays classes and functions (sorted alphabetically) in the d2l package, show-\ning where they are deﬁned in the book so you can ﬁnd more detailed implementations and\nexplanations. See also the source code on the GitHub repository316.\nB.8.1 Classes\nclass d2l.torch.AdditiveAttention(num_hiddens, dropout, **kwargs)\nBases: Module\nAdditive attention.\nDeﬁned in Section 11.3.2\nforward(queries, keys, values, valid_lens)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\n\n1109\nThe d2l API Document\ntraining: bool\nclass d2l.torch.AddNorm(norm_shape, dropout)\nBases: Module\nThe residual connection followed by layer normalization.\nDeﬁned in Section 11.7.2\nforward(X, Y)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.AttentionDecoder\nBases: Decoder (page 1110)\nThe base attention-based decoder interface.\nDeﬁned in Section 11.4\nproperty attention_weights\ntraining: bool\nclass d2l.torch.Classifier(plot_train_per_epoch=2, plot_valid_per_epoch=1)\nBases: Module (page 1113)\nThe base class of classiﬁcation models.\nDeﬁned in Section 4.3\naccuracy(Y_hat, Y, averaged=True)\nCompute the number of correct predictions.\nDeﬁned in Section 4.3\nlayer_summary(X_shape)\nDeﬁned in Section 7.6\nloss(Y_hat, Y, averaged=True)\nDeﬁned in Section 4.5\n\n1110\nTools for Deep Learning\ntraining: bool\nvalidation_step(batch)\nclass d2l.torch.DataModule(root='../data', num_workers=4)\nBases: HyperParameters (page 1112)\nThe base class of data.\nDeﬁned in Section 3.2.2\nget_dataloader(train)\nget_tensorloader(tensors, train, indices=slice(0, None, None))\nDeﬁned in Section 3.3\ntrain_dataloader()\nval_dataloader()\nclass d2l.torch.Decoder\nBases: Module\nThe base decoder interface for the encoder–decoder architecture.\nDeﬁned in Section 10.6\nforward(X, state)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ninit_state(enc_all_outputs, *args)\ntraining: bool\nclass d2l.torch.DotProductAttention(dropout)\nBases: Module\nScaled dot product attention.\nDeﬁned in Section 11.3.2\n\n1111\nThe d2l API Document\nforward(queries, keys, values, valid_lens=None)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.Encoder\nBases: Module\nThe base encoder interface for the encoder–decoder architecture.\nDeﬁned in Section 10.6\nforward(X, *args)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.EncoderDecoder(encoder, decoder)\nBases: Classifier (page 1109)\nThe base class for the encoder–decoder architecture.\nDeﬁned in Section 10.6\nforward(enc_X, dec_X, *args)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\n\n1112\nTools for Deep Learning\npredict_step(batch, device, num_steps, save_attention_weights=False)\nDeﬁned in Section 10.7.6\ntraining: bool\nclass d2l.torch.FashionMNIST(batch_size=64, resize=(28, 28))\nBases: DataModule (page 1110)\nThe Fashion-MNIST dataset.\nDeﬁned in Section 4.2\nget_dataloader(train)\nDeﬁned in Section 4.2\ntext_labels(indices)\nReturn text labels.\nDeﬁned in Section 4.2\nvisualize(batch, nrows=1, ncols=8, labels=[])\nDeﬁned in Section 4.2\nclass d2l.torch.GRU(num_inputs, num_hiddens, num_layers, dropout=0)\nBases: RNN (page 1117)\nThe multilayer GRU model.\nDeﬁned in Section 10.3\ntraining: bool\nclass d2l.torch.HyperParameters\nBases: object\nThe base class of hyperparameters.\nsave_hyperparameters(ignore=[])\nSave function arguments into class attributes.\nDeﬁned in Section B.7\nclass d2l.torch.LeNet(lr=0.1, num_classes=10)\nBases: Classifier (page 1109)\nThe LeNet-5 model.\nDeﬁned in Section 7.6\ntraining: bool\n\n1113\nThe d2l API Document\nclass d2l.torch.LinearRegression(lr)\nBases: Module (page 1113)\nThe linear regression model implemented with high-level APIs.\nDeﬁned in Section 3.5\nconfigure_optimizers()\nDeﬁned in Section 3.5\nforward(X)\nDeﬁned in Section 3.5\nget_w_b()\nDeﬁned in Section 3.5\nloss(y_hat, y)\nDeﬁned in Section 3.5\ntraining: bool\nclass d2l.torch.LinearRegressionScratch(num_inputs, lr, sigma=0.01)\nBases: Module (page 1113)\nThe linear regression model implemented from scratch.\nDeﬁned in Section 3.4\nconfigure_optimizers()\nDeﬁned in Section 3.4\nforward(X)\nDeﬁned in Section 3.4\nloss(y_hat, y)\nDeﬁned in Section 3.4\ntraining: bool\nclass d2l.torch.Module(plot_train_per_epoch=2, plot_valid_per_epoch=1)\nBases: Module, HyperParameters (page 1112)\nThe base class of models.\nDeﬁned in Section 3.2\napply_init(inputs, init=None)\nDeﬁned in Section 6.4\nconfigure_optimizers()\nDeﬁned in Section 4.3\n\n1114\nTools for Deep Learning\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\nloss(y_hat, y)\nplot(key, value, train)\nPlot a point in animation.\ntraining: bool\ntraining_step(batch)\nvalidation_step(batch)\nclass d2l.torch.MTFraEng(batch_size, num_steps=9, num_train=512, num_val=128)\nBases: DataModule (page 1110)\nThe English-French dataset.\nDeﬁned in Section 10.5\nbuild(src_sentences, tgt_sentences)\nDeﬁned in Section 10.5.3\nget_dataloader(train)\nDeﬁned in Section 10.5.3\nclass d2l.torch.MultiHeadAttention(num_hiddens, num_heads, dropout,\nbias=False, **kwargs)\nBases: Module (page 1113)\nMulti-head attention.\nDeﬁned in Section 11.5\nforward(queries, keys, values, valid_lens)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\n\n1115\nThe d2l API Document\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\ntranspose_output(X)\nReverse the operation of transpose_qkv.\nDeﬁned in Section 11.5\ntranspose_qkv(X)\nTransposition for parallel computation of multiple attention heads.\nDeﬁned in Section 11.5\nclass d2l.torch.PositionalEncoding(num_hiddens, dropout, max_len=1000)\nBases: Module\nPositional encoding.\nDeﬁned in Section 11.6\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.PositionWiseFFN(ﬀn_num_hiddens, ﬀn_num_outputs)\nBases: Module\nThe positionwise feed-forward network.\nDeﬁned in Section 11.7\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\n\n1116\nTools for Deep Learning\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.ProgressBoard(xlabel=None, ylabel=None, xlim=None, ylim=None,\nxscale='linear', yscale='linear', ls=['-', '--', '-.', ':'],\ncolors=['C0', 'C1', 'C2', 'C3'], ﬁg=None, axes=None,\nﬁgsize=(3.5, 2.5), display=True)\nBases: HyperParameters (page 1112)\nThe board that plots data points in animation.\nDeﬁned in Section 3.2\ndraw(x, y, label, every_n=1)\nDeﬁned in Section B.7\nclass d2l.torch.Residual(num_channels, use_1x1conv=False, strides=1)\nBases: Module\nThe Residual block of ResNet models.\nDeﬁned in Section 8.6\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.ResNeXtBlock(num_channels, groups, bot_mul, use_1x1conv=False,\nstrides=1)\nBases: Module\nThe ResNeXt block.\nDeﬁned in Section 8.6.2\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\n\n1117\nThe d2l API Document\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.RNN(num_inputs, num_hiddens)\nBases: Module (page 1113)\nThe RNN model implemented with high-level APIs.\nDeﬁned in Section 9.6\nforward(inputs, H=None)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.RNNLM(rnn, vocab_size, lr=0.01)\nBases: RNNLMScratch (page 1117)\nThe RNN-based language model implemented with high-level APIs.\nDeﬁned in Section 9.6\ninit_params()\noutput_layer(hiddens)\nDeﬁned in Section 9.5\ntraining: bool\nclass d2l.torch.RNNLMScratch(rnn, vocab_size, lr=0.01)\nBases: Classifier (page 1109)\nThe RNN-based language model implemented from scratch.\nDeﬁned in Section 9.5\nforward(X, state=None)\nDeﬁned in Section 9.5\n\n1118\nTools for Deep Learning\ninit_params()\none_hot(X)\nDeﬁned in Section 9.5\noutput_layer(rnn_outputs)\nDeﬁned in Section 9.5\npredict(preﬁx, num_preds, vocab, device=None)\nDeﬁned in Section 9.5\ntraining: bool\ntraining_step(batch)\nvalidation_step(batch)\nclass d2l.torch.RNNScratch(num_inputs, num_hiddens, sigma=0.01)\nBases: Module (page 1113)\nThe RNN model implemented from scratch.\nDeﬁned in Section 9.5\nforward(inputs, state=None)\nDeﬁned in Section 9.5\ntraining: bool\nclass d2l.torch.Seq2Seq(encoder, decoder, tgt_pad, lr)\nBases: EncoderDecoder (page 1111)\nThe RNN encoder–decoder for sequence to sequence learning.\nDeﬁned in Section 10.7.3\nconfigure_optimizers()\nDeﬁned in Section 4.3\ntraining: bool\nvalidation_step(batch)\nclass d2l.torch.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers,\ndropout=0)\nBases: Encoder (page 1111)\nThe RNN encoder for sequence-to-sequence learning.\nDeﬁned in Section 10.7\n\n1119\nThe d2l API Document\nforward(X, *args)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.SGD(params, lr)\nBases: HyperParameters (page 1112)\nMinibatch stochastic gradient descent.\nDeﬁned in Section 3.4\nstep()\nzero_grad()\nclass d2l.torch.SoftmaxRegression(num_outputs, lr)\nBases: Classifier (page 1109)\nThe softmax regression model.\nDeﬁned in Section 4.5\nforward(X)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.SyntheticRegressionData(w, b, noise=0.01, num_train=1000,\nnum_val=1000, batch_size=32)\nBases: DataModule (page 1110)\nSynthetic data for linear regression.\nDeﬁned in Section 3.3\n\n1120\nTools for Deep Learning\nget_dataloader(train)\nDeﬁned in Section 3.3\nclass d2l.torch.TimeMachine(batch_size, num_steps, num_train=10000,\nnum_val=5000)\nBases: DataModule (page 1110)\nThe Time Machine dataset.\nDeﬁned in Section 9.2\nbuild(raw_text, vocab=None)\nDeﬁned in Section 9.2\nget_dataloader(train)\nDeﬁned in Section 9.3.3\nclass d2l.torch.Trainer(max_epochs, num_gpus=0, gradient_clip_val=0)\nBases: HyperParameters (page 1112)\nThe base class for training models with data.\nDeﬁned in Section 3.2.2\nclip_gradients(grad_clip_val, model)\nDeﬁned in Section 9.5\nfit(model, data)\nfit_epoch()\nDeﬁned in Section 3.4\nprepare_batch(batch)\nDeﬁned in Section 6.7\nprepare_data(data)\nprepare_model(model)\nDeﬁned in Section 6.7\nclass d2l.torch.TransformerEncoder(vocab_size, num_hiddens, ﬀn_num_hiddens,\nnum_heads, num_blks, dropout,\nuse_bias=False)\nBases: Encoder (page 1111)\nThe Transformer encoder.\nDeﬁned in Section 11.7.4\n\n1121\nThe d2l API Document\nforward(X, valid_lens)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.TransformerEncoderBlock(num_hiddens, ﬀn_num_hiddens,\nnum_heads, dropout, use_bias=False)\nBases: Module\nThe Transformer encoder block.\nDeﬁned in Section 11.7.2\nforward(X, valid_lens)\nDeﬁnes the computation performed at every call.\nShould be overridden by all subclasses.\nNote: Although the recipe for forward pass needs to be deﬁned within this function,\none should call the Module (page 1113) instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently ignores them.\ntraining: bool\nclass d2l.torch.Vocab(tokens=[], min_freq=0, reserved_tokens=[])\nBases: object\nVocabulary for text.\nto_tokens(indices)\nproperty unk\nB.8.2 Functions\nd2l.torch.add_to_class(Class)\nRegister functions as methods in created class.\nDeﬁned in Section 3.2\n\n1122\nTools for Deep Learning\nd2l.torch.bleu(pred_seq, label_seq, k)\nCompute the BLEU.\nDeﬁned in Section 10.7.6\nd2l.torch.check_len(a, n)\nCheck the length of a list.\nDeﬁned in Section 9.5\nd2l.torch.check_shape(a, shape)\nCheck the shape of a tensor.\nDeﬁned in Section 9.5\nd2l.torch.corr2d(X, K)\nCompute 2D cross-correlation.\nDeﬁned in Section 7.2\nd2l.torch.cpu()\nGet the CPU device.\nDeﬁned in Section 6.7\nd2l.torch.gpu(i=0)\nGet a GPU device.\nDeﬁned in Section 6.7\nd2l.torch.init_cnn(module)\nInitialize weights for CNNs.\nDeﬁned in Section 7.6\nd2l.torch.init_seq2seq(module)\nInitialize weights for sequence-to-sequence learning.\nDeﬁned in Section 10.7\nd2l.torch.masked_softmax(X, valid_lens)\nPerform softmax operation by masking elements on the last axis.\nDeﬁned in Section 11.3\nd2l.torch.num_gpus()\nGet the number of available GPUs.\nDeﬁned in Section 6.7\n\n1123\nThe d2l API Document\nd2l.torch.plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\nylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'),\nﬁgsize=(3.5, 2.5), axes=None)\nPlot data points.\nDeﬁned in Section 2.4\nd2l.torch.set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nSet the axes for matplotlib.\nDeﬁned in Section 2.4\nd2l.torch.set_figsize(ﬁgsize=(3.5, 2.5))\nSet the ﬁgure size for matplotlib.\nDeﬁned in Section 2.4\nd2l.torch.show_heatmaps(matrices, xlabel, ylabel, titles=None, ﬁgsize=(2.5, 2.5),\ncmap='Reds')\nShow heatmaps of matrices.\nDeﬁned in Section 11.1\nd2l.torch.show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist)\nPlot the histogram for list length pairs.\nDeﬁned in Section 10.5\nd2l.torch.try_all_gpus()\nReturn all available GPUs, or [cpu(),] if no GPU exists.\nDeﬁned in Section 6.7\nd2l.torch.try_gpu(i=0)\nReturn gpu(i) if exists, otherwise return cpu().\nDeﬁned in Section 6.7\nd2l.torch.use_svg_display()\nUse the svg format to display a plot in Jupyter.\nDeﬁned in Section 2.4\n\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … et al. (2016). TensorFlow:\na system for large-scale machine learning. 12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 16) (pp. 265–283).\nAbdel-Hamid, O., Mohamed, A.-R., Jiang, H., Deng, L., Penn, G., & Yu, D. (2014). Convo-\nlutional neural networks for speech recognition. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 22(10), 1533–1545.\nAhmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable\ninference in latent variable models. Proceedings of the Fifth ACM International Conference\non Web Search and Data Mining (pp. 123–132).\nAkiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: a next-generation\nhyperparameter optimization framework. Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., … et al. (2022).\nFlamingo: a visual language model for few-shot learning. ArXiv:2204.14198.\nAlsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., & Reblitz-Richardson, O. (2020). Mind\nthe PAD – CNNs can develop blind spots. ArXiv:2010.02178.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., … et al. (2023). PaLM\n2 Technical Report. ArXiv:2305.10403.\nAnil, R., Gupta, V., Koren, T., Regan, K., & Singer, Y. (2020). Scalable second-order opti-\nmization for deep learning. ArXiv:2002.09018.\nAronszajn, N. (1950). Theory of reproducing kernels. Transactions of the American Mathe-\nmatical Society, 68(3), 337–404.\nBa, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. ArXiv:1607.06450.\nBaevski, A., & Auli, M. (2018). Adaptive input representations for neural language modeling.\nInternational Conference on Learning Representations.\nBahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning\nto align and translate. ArXiv:1409.0473.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., … et al. (2022). Consti-\ntutional AI: harmlessness from AI feedback. ArXiv:2212.08073.\nBaptista, R., & Poloczek, M. (2018). Bayesian optimization of combinatorial structures. Pro-\nceedings of the 35th International Conference on Machine Learning.\nBardenet, R., Brendel, M., Kégl, B., & Sebag, M. (2013). Collaborative hyperparameter tun-\ning. Proceedings of the 30th International Conference on Machine Learning (ICML'13).\nBay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded up robust features. European\nConference on Computer Vision (pp. 404–417).\nBellman, R. (1966). Dynamic programming. Science, 153, 34–37.\n1124\n\n1125\nREFERENCES\nBellman, R. (1952). On the theory of dynamic programming. Proceedings of the National\nAcademy of Sciences, 38(8), 716–719.\nBellman, R. (1957). A Markovian decision process. Journal of Mathematics and Mechanics,\n6(5), 679–684. URL: http://www.jstor.org/stable/24900506\nBellman, R. (1957). Dynamic Programming. Dover Publications.\nBeltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: the long-document transformer.\nArXiv:2004.05150.\nBengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language\nmodel. Journal of Machine Learning Research, 3(Feb), 1137–1155.\nBengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gra-\ndient descent is diﬃcult. IEEE Transactions on Neural Networks, 5(2), 157–166.\nBergstra, J., Bardenet, R., Bengio, Y., & Kégl, B. (2011). Algorithms for hyper-parameter\noptimization. Advances in Neural Information Processing Systems, 24.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., … Bengio,\nY. (2010). Theano: a CPU and GPU math compiler in Python. Proc. 9th Python in Science\nConference (pp. 3–10).\nBeutel, A., Murray, K., Faloutsos, C., & Smola, A. J. (2014). CoBaFi: collaborative Bayesian\nﬁltering. Proceedings of the 23rd International Conference on World Wide Web (pp. 97–\n108).\nBishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. Neural\nComputation, 7(1), 108–116.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nBlack, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of\nPolitical Economy, 81, 637–654.\nBodla, N., Singh, B., Chellappa, R., & Davis, L. S. (2017). Soft-NMS-improving object de-\ntection with one line of code. Proceedings of the IEEE International Conference on Com-\nputer Vision (pp. 5561–5569).\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics, 5,\n135–146.\nBollobás, B. (1999). Linear Analysis. Cambridge University Press.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., … et al. (2021).\nOn the opportunities and risks of foundation models. ArXiv:2108.07258.\nBottou, L. (2010). Large-scale machine learning with stochastic gradient descent. Proceed-\nings of COMPSTAT'2010 (pp. 177–186). Springer.\nBottou, L., & Le Cun, Y. (1988). SN: a simulator for connectionist models. Proceedings\nof NeuroNimes 88 (pp. 371–382). Nimes, France. URL: http://leon.bottou.org/papers/\nbottou-lecun-88\nBoucheron, S., Bousquet, O., & Lugosi, G. (2005). Theory of classiﬁcation: a survey of some\nrecent advances. ESAIM: Probability and Statistics, 9, 323–375.\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus\nfor learning natural language inference. ArXiv:1508.05326.\nBoyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge, England: Cambridge\nUniversity Press.\n\n1126\nREFERENCES\nBradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The\nmethod of paired comparisons. Biometrika, 39(3/4), 324–345.\nBrown, N., & Sandholm, T. (2017). Libratus: the superhuman AI for no-limit poker. IJCAI\n(pp. 5226–5228).\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Laﬀerty, J.,\n… Roossin, P. S. (1990). A statistical approach to machine translation. Computational\nLinguistics, 16(2), 79–85.\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L., &\nRoossin, P. (1988). A statistical approach to language translation. COLING Budapest 1988\nVolume 1: International Conference on Computational Linguistics.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., … et al. (2020).\nLanguage models are few-shot learners. Advances in Neural Information Processing Sys-\ntems, 33, 1877–1901.\nBuslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., Druzhinin, M., & Kalinin, A. A.\n(2020). Albumentations: Fast and ﬂexible image augmentations. Information, 11(2), 125.\nCampbell, M., Hoane Jr, A. J., & Hsu, F.-h. (2002). Deep blue. Artiﬁcial Intelligence, 134(1-\n2), 57–83.\nCanny, J. (1987). A computational approach to edge detection. Readings in Computer Vision\n(pp. 184–203). Elsevier.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). SemEval-2017 Task 1:\nsemantic textual similarity multilingual and crosslingual focused evaluation. Proceedings\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017) (pp. 1–14).\nChan, W., Jaitly, N., Le, Q. V., & Vinyals, O. (2015). Listen, attend and spell.\nArXiv:1508.01211.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., … Mordatch, I. (2021).\nDecision transformer: reinforcement learning via sequence modeling. Advances in Neural\nInformation Processing Systems, 34, 15084–15097.\nChen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., … Zhang, Z. (2015). MXNET:\na ﬂexible and eﬃcient machine learning library for heterogeneous distributed systems.\nArXiv:1512.01274.\nCheng, J., Dong, L., & Lapata, M. (2016). Long short-term memory-networks for machine\nreading. Proceedings of the 2016 Conference on Empirical Methods in Natural Language\nProcessing (pp. 551–561).\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., & Shelhamer,\nE. (2014). CuDNN: Eﬃcient primitives for deep learning. ArXiv:1410.0759.\nCho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of\nneural machine translation: Encoder–decoder approaches. ArXiv:1409.1259.\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &\nBengio, Y. (2014). Learning phrase representations using RNN encoder–decoder for sta-\ntistical machine translation. ArXiv:1406.1078.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … et al. (2022).\nPaLM: scaling language modeling with pathways. ArXiv:2204.02311.\nChung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. ArXiv:1412.3555.\n\n1127\nREFERENCES\nClark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: pre-training text\nencoders as discriminators rather than generators. International Conference on Learning\nRepresentations.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).\nNatural language processing (almost) from scratch. Journal of Machine Learning Research,\n12, 2493–2537.\nCordonnier, J.-B., Loukas, A., & Jaggi, M. (2020). On the relationship between self-attention\nand convolutional layers. International Conference on Learning Representations.\nCover, T., & Thomas, J. (1999). Elements of Information Theory. John Wiley & Sons.\nCsiszár, I. (2008). Axiomatic characterizations of information measures. Entropy, 10(3),\n261–273.\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics\nof Control, Signals and Systems, 2(4), 303–314.\nDalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. 2005\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)\n(pp. 886–893).\nDe Cock, D. (2011). Ames, Iowa: alternative to the Boston housing data as an end of semester\nregression project. Journal of Statistics Education, 19(3).\nDean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., … et al. (2012). Large\nscale distributed deep networks. Proceedings of the 25th International Conference on Neural\nInformation Processing Systems, Volume 1 (pp. 1223–1231).\nDeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., …\nVogels, W. (2007). Dynamo: Amazon's highly available key-value store. ACM SIGOPS\nOperating Systems Review (pp. 205–220).\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: a large-\nscale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern\nRecognition (pp. 248–255).\nDer Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? does it matter? Structural\nSafety, 31(2), 105–112.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep\nbidirectional transformers for language understanding. ArXiv:1810.04805.\nDinh, L., Krueger, D., & Bengio, Y. (2014). NICE: non-linear independent components es-\ntimation. ArXiv:1410.8516.\nDinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real NVP. Inter-\nnational Conference on Learning Representations.\nDoersch, C., Gupta, A., & Efros, A. A. (2015). Unsupervised visual representation learning\nby context prediction. Proceedings of the IEEE International Conference on Computer Vision\n(pp. 1422–1430).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., …\net al. (2021). An image is worth 16 x 16 words: transformers for image recognition at scale.\nInternational Conference on Learning Representations.\nDuchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine Learning Research, 12, 2121–2159.\nDumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning.\nArXiv:1603.07285.\n\n1128\nREFERENCES\nDwivedi, V. P., & Bresson, X. (2020). A generalization of transformer networks to graphs.\nArXiv:2012.09699.\nDwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., & Roth, A. L. (2015). Pre-\nserving statistical validity in adaptive data analysis. Proceedings of the 47th Annual ACM\nSymposium on Theory of Computing (pp. 117–126).\nElman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2), 179–211.\nElsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: a ssurvey.\nArXiv:1808.05377 [stat.ML].\nFechner, G. T. (1860). Elemente der Psychophysik. Vol. 2. Breitkopf u. Härtel.\nFedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: scaling to trillion parameter\nmodels with simple and eﬃcient sparsity. Journal of Machine Learning Research, 23(120),\n1–39.\nFernando, R. (2004). GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time\nGraphics. Addison-Wesley.\nFeurer, M., & Hutter, F. (2018). Hyperparameter ptimization. Automatic Machine Learning:\nMethods, Systems, Challenges. Springer.\nFeurer, M., Letham, B., Hutter, F., & Bakshy, E. (2022). Practical transfer learning for\nBayesian optimization. ArXiv:1802.02219 [stat.ML].\nField, D. J. (1987). Relations between the statistics of natural images and the response prop-\nerties of cortical cells. JOSA A, 4(12), 2379–2394.\nFisher, R. A. (1925). Statistical Methods for Research Workers. Oliver & Boyd.\nFlammarion, N., & Bach, F. (2015). From averaging to acceleration, there is only a step-size.\nConference on Learning Theory (pp. 658–695).\nForrester, A. I., Sóbester, A., & Keane, A. J. (2007). Multi-ﬁdelity optimization via surrogate\nmodelling. Proceedings of the Royal Society A: Mathematical, Physical and Engineering\nSciences, 463(2088), 3251–3269.\nFranceschi, L., Donini, M., Frasconi, P., & Pontil, M. (2017). Forward and reverse gradient-\nbased hyperparameter optimization. Proceedings of the 34th International Conference on\nMachine Learning (ICML'17).\nFrankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: ﬁnding sparse, trainable neu-\nral networks. ArXiv:1803.03635.\nFrazier, P. I. (2018). A tutorial on Bayesian optimization. ArXiv:1807.02811.\nFreund, Y., & Schapire, R. E. (1996). Experiments with a new boosting algorithm. Proceed-\nings of the International Conference on Machine Learning (pp. 148–156).\nFriedman, J. H. (1987). Exploratory projection pursuit. Journal of the American Statistical\nAssociation, 82(397), 249–266.\nFrostig, R., Johnson, M. J., & Leary, C. (2018). Compiling machine learning programs via\nhigh-level tracing. Proceedings of Systems for Machine Learning.\nFukushima, K. (1982). Neocognitron: a self-organizing neural network model for a mecha-\nnism of visual pattern recognition. Competition and Cooperation in Neural Nets (pp. 267–\n285). Springer.\nGardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., & Wilson, A. G. (2018). GPyTorch:\nblackbox matrix–matrix Gaussian process inference with GPU acceleration. Advances in\nNeural Information Processing Systems.\n\n1129\nREFERENCES\nGarg, S., Balakrishnan, S., Kolter, Z., & Lipton, Z. (2021). RATT: leveraging unlabeled\ndata to guarantee generalization. International Conference on Machine Learning (pp. 3598–\n3609).\nGatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional neu-\nral networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition (pp. 2414–2423).\nGauss, C. F. (1809). Theoria motus corporum coelestum. Werke. Königlich Preussische\nAkademie der Wissenschaften.\nGibbs, J. W. (1902). Elementary Principles of Statistical Mhanics. Scribner's.\nGinibre, J. (1965). Statistical ensembles of complex, quaternion, and real matrices. Journal\nof Mathematical Physics, 6(3), 440–449.\nGirshick, R. (2015). Fast R-CNN. Proceedings of the IEEE International Conference on Com-\nputer Vision (pp. 1440–1448).\nGirshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (pp. 580–587).\nGlorot, X., & Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward\nneural networks. Proceedings of the 13th International Conference on Artiﬁcial Intelligence\nand Statistics (pp. 249–256).\nGoh, G. (2017). Why momentum really works. Distill. URL: http://distill.pub/2017/\nmomentum\nGoldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative ﬁltering to\nweave an information tapestry. Communications of the ACM, 35(12), 61–71.\nGolub, G. H., & Van Loan, C. F. (1996). Matrix Computations. Johns Hopkins University\nPress.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. http://www.\ndeeplearningbook.org.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio,\nY. (2014). Generative adversarial nets. Advances in Neural Information Processing Systems\n(pp. 2672–2680).\nGotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A closer look at deep learning\nheuristics: learning rate restarts, warmup and distillation. ArXiv:1810.13243.\nGoyal, A., Bochkovskiy, A., Deng, J., & Koltun, V. (2021). Non-deep networks.\nArXiv:2110.07641.\nGraham, B. (2014). Fractional max-pooling. ArXiv:1412.6071.\nGraves, A. (2013). Generating sequences with recurrent neural networks. ArXiv:1308.0850.\nGraves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., & Schmidhuber, J. (2008).\nA novel connectionist system for unconstrained handwriting recognition. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 31(5), 855–868.\nGraves, A., & Schmidhuber, J. (2005). Framewise phoneme classiﬁcation with bidirectional\nLSTM and other neural network architectures. Neural Networks, 18(5-6), 602–610.\nGriewank, A. (1989). On automatic diﬀerentiation. Mathematical Programming: Recent De-\nvelopments and Applications (pp. 83–107). Kluwer.\n\n1130\nREFERENCES\nGulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., … et al. (2020). Con-\nformer: convolution-augmented transformer for speech recognition. Proc. Interspeech\n2020, pp. 5036–5040.\nGuyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. A. (2008). Feature Extraction: Foundations\nand Applications. Springer.\nHadjis, S., Zhang, C., Mitliagkas, I., Iter, D., & Ré, C. (2016). Omnivore: an optimizer for\nmulti-device deep learning on CPUs and GPUs. ArXiv:1606.04487.\nHartley, R., & Zisserman, A. (2000). Multiple View Geometry in Computer Vision. Cambridge\nUniversity Press.\nHartley, R. I., & Kahl, F. (2009). Global optimization through rotation space search. Inter-\nnational Journal of Computer Vision, 82(1), 64–79.\nHe, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). Masked autoencoders\nare scalable vision learners. Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp. 16000–16009).\nHe, K., Gkioxari, G., Dollár, P., & Girshick, R. (2017). Mask R-CNN. Proceedings of the\nIEEE International Conference on Computer Vision (pp. 2961–2969).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectiﬁers: surpassing human-\nlevel performance on ImageNet classiﬁcation. Proceedings of the IEEE International Con-\nference on Computer Vision (pp. 1026–1034).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition.\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 770–\n778).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks.\nEuropean Conference on Computer Vision (pp. 630–645).\nHebb, D. O. (1949). The Organization of Behavior. Wiley.\nHendrycks,\nD.,\n&\nGimpel,\nK.\n(2016).\nGaussian\nerror\nlinear\nunits\n(GELUs).\nArXiv:1606.08415.\nHennessy, J. L., & Patterson, D. A. (2011). Computer Architecture: A Quantitative Approach.\nElsevier.\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diﬀusion probabilistic models. Advances in\nNeural Information Processing Systems, 33, 6840–6851.\nHochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient ﬂow in recur-\nrent nets: the diﬃculty of learning long-term dependencies. A Field Guide to Dynamical\nRecurrent Neural Networks. IEEE Press.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation,\n9(8), 1735–1780.\nHoﬀmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … et al.\n(2022). Training compute-optimal large language models. ArXiv:2203.15556.\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., … Adam, H. (2019).\nSearching for MobileNetV3. Proceedings of the IEEE/CVF International Conference on\nComputer Vision (pp. 1314–1324).\nHoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., & Schölkopf, B. (2009). Nonlinear causal\ndiscovery with additive noise models. Advances in Neural Information Processing Systems\n(pp. 689–696).\n\n1131\nREFERENCES\nHu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (pp. 7132–7141).\nHu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative ﬁltering for implicit feedback\ndatasets. 2008 8th IEEE International Conference on Data Mining (pp. 263–272).\nHu, Z., Lee, R. K.-W., Aggarwal, C. C., & Zhang, A. (2022). Text style transfer: a review and\nexperimental evaluation. SIGKDD Explor. Newsl., 24(1). URL: https://doi.org/10.1145/\n3544903.3544906\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., … Eck,\nD. (2018). Music transformer: generating music with long-term structure. International\nConference on Learning Representations.\nHuang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected\nconvolutional networks. Proceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition (pp. 4700–4708).\nHuang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM–CRF models for sequence tagging.\nArXiv:1508.01991.\nHubel, D. H., & Wiesel, T. N. (1959). Receptive ﬁelds of single neurones in the cat's striate\ncortex. Journal of Physiology, 148(3), 574–591.\nHubel, D. H., & Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and functional\narchitecture in the cat's visual cortex. Journal of Physiology, 160(1), 106–154.\nHubel, D. H., & Wiesel, T. N. (1968). Receptive ﬁelds and functional architecture of monkey\nstriate cortex. Journal of Physiology, 195(1), 215–243.\nHutter, F., Hoos, H., & Leyton-Brown, K. (2011). Sequential model-based optimization\nfor general algorithm conﬁguration. Proceedings of the Fifth International Conference on\nLearning and Intelligent Optimization (LION'11).\nHutter, F., Kotthoﬀ, L., & Vanschoren, J. (Eds.) (2019). Automated Machine Learning: Meth-\nods, Systems, Challenges. Springer.\nIoﬀe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batch-\nnormalized models. Advances in Neural Information Processing Systems (pp. 1945–1953).\nIoﬀe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training by\nreducing internal covariate shift. ArXiv:1502.03167.\nIzmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging\nweights leads to wider optima and better generalization. ArXiv:1803.05407.\nJacot, A., Gabriel, F., & Hongler, C. (2018). Neural tangent kernel: convergence and gener-\nalization in neural networks. Advances in Neural Information Processing Systems.\nJaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL,\nEKF and the “echo state network” approach. GMD-Forschungszentrum Informationstech-\nnik Bonn.\nJamieson, K., & Talwalkar, A. (2016). Non-stochastic best arm identiﬁcation and hyperpa-\nrameter optimization. Proceedings of the 17th International Conference on Artiﬁcial Intel-\nligence and Statistics.\nJenatton, R., Archambeau, C., González, J., & Seeger, M. (2017). Bayesian optimization with\ntree-structured dependencies. Proceedings of the 34th International Conference on Machine\nLearning (ICML'17).\n\n1132\nREFERENCES\nJia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., … et al. (2018). Highly scalable\ndeep learning training system with mixed-precision: training ImageNet in four minutes.\nArXiv:1807.11205.\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., … Darrell, T. (2014).\nCaﬀe: convolutional architecture for fast feature embedding. Proceedings of the 22nd ACM\nInternational Conference on Multimedia (pp. 675–678).\nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., & Levy, O. (2020). SpanBERT:\nimproving pre-training by representing and predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8, 64–77.\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., … et al. (2017). In-\ndatacenter performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th Annual\nInternational Symposium on Computer Architecture (ISCA) (pp. 1–12).\nKalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A convolutional neural network\nfor modelling sentences. ArXiv:1404.2188.\nKalman, B. L., & Kwasny, S. C. (1992). Why tanh: choosing a sigmoidal function. Proceed-\nings of the International Joint Conference on Neural Networks (IJCNN) (pp. 578–581).\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., … Amodei, D.\n(2020). Scaling laws for neural language models. ArXiv:2001.08361.\nKarnin, Z., Koren, T., & Somekh, O. (2013). Almost optimal exploration in multi-armed\nbandits. Proceedings of the 30th International Conference on Machine Learning (ICML'13).\nKarras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of GANs for\nimproved quality, stability, and variation. ArXiv:1710.10196.\nKim, J., El-Khamy, M., & Lee, J. (2017). Residual LSTM: design of a deep recurrent archi-\ntecture for distant speech recognition. ArXiv:1701.03360.\nKim, Y. (2014). Convolutional neural networks for sentence classiﬁcation. ArXiv:1408.5882.\nKimeldorf, G. S., & Wahba, G. (1971). Some results on Tchebycheﬃan spline functions. J.\nMath. Anal. Appl., 33, 82–95.\nKingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization.\nArXiv:1412.6980.\nKingma, D. P., & Welling, M. (2014). Auto-encoding variational Bayes. International Con-\nference on Learning Representations (ICLR).\nKipf, T. N., & Welling, M. (2016). Semi-supervised classiﬁcation with graph convolutional\nnetworks. ArXiv:1609.02907.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models\nare zero-shot reasoners. arxiv.org/abs/2205.11916.\nKoller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Tech-\nniques. MIT Press.\nKolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Inst.\nItal. Attuari, Giorn., 4, 83–91.\nKolter,\nZ.\n(2008).\nLinear\nalgebra\nreview\nand\nreference.\nAvailable\nonline:\nhttp://cs229.stanford.edu/section/cs229-linalg.pdf.\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classiﬁcation with deep con-\nvolutional neural networks. Advances in Neural Information Processing Systems (pp. 1097–\n1105).\nKung, S. Y. (1988). VLSI Array Processors. Prentice Hall.\n\n1133\nREFERENCES\nKuzovkin, I., Vicente, R., Petton, M., Lachaux, J.-P., Baciu, M., Kahane, P., … Aru, J.\n(2018). Activations of deep convolutional neural networks are aligned with gamma band\nactivity of human visual cortex. Communications Biology, 1(1), 1–12.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). ALBERT:\na lite BERT for self-supervised learning of language representations. ArXiv:1909.11942.\nLavin, A., & Gray, S. (2016). Fast algorithms for convolutional neural networks. Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4013–4021).\nLe, Q. V. (2013). Building high-level features using large scale unsupervised learning. Pro-\nceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing\n(pp. 8595–8598).\nLeCun, Y., Bengio, Y., & et al. (1995). Convolutional networks for images, speech, and time\nseries. The Handbook of Brain Theory and Neural Networks (p. 3361). MIT Press.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel,\nL. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural Com-\nputation, 1(4), 541–551.\nLeCun, Y., Bottou, L., Orr, G., & Muller, K.-R. (1998). Eﬃcient backprop. Neural Networks:\nTricks of the Trade. Springer.\nLeCun, Y., Bottou, L., Bengio, Y., & Haﬀner, P. (1998). Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11), 2278–2324.\nLeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., … et al. (1995). Com-\nparison of learning algorithms for handwritten digit recognition. International Conference\non Artiﬁcial Neural Networks (pp. 53–60).\nLegendre, A. M. (1805). Mémoire sur les Opérations Trigonométriques: dont les Résultats\nDépendent de la Figure de la Terre. F. Didot.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., … Zettlemoyer,\nL. (2019). BART: denoising sequence-to-sequence pre-training for natural language gen-\neration, translation, and comprehension. ArXiv:1910.13461.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh,\nV., … et al. (2022). Solving quantitative reasoning problems with language models.\nArXiv:2206.14858.\nLi, L., Jamieson, K., Rostamizadeh, A., Gonina, K., Hardt, M., Recht, B., & Talwalkar, A.\n(2018). Massively parallel hyperparameter tuning. ArXiv:1810.05934.\nLi, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design\n(Doctoral dissertation). PhD Thesis, CMU.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., … Su, B.-Y.\n(2014). Scaling distributed machine learning with the parameter server. 11th Symposium\non Operating Systems Design and Implementation (OSDI 14) (pp. 583–598).\nLi, M., Zhang, T., Chen, Y., & Smola, A. J. (2014). Eﬃcient mini-batch training for\nstochastic optimization. Proceedings of the 20th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining (pp. 661–670).\nLiaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J., & Stoica, I. (2018). Tune: a\nresearch platform for distributed model selection and training. ArXiv:1807.05118.\nLin, M., Chen, Q., & Yan, S. (2013). Network in network. ArXiv:1312.4400.\n\n1134\nREFERENCES\nLin, T.-Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object\ndetection. Proceedings of the IEEE International Conference on Computer Vision (pp. 2980–\n2988).\nLin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., … others. (2010). ImageNet classiﬁca-\ntion: fast descriptor coding and large-scale SVM training. Large Scale Visual Recognition\nChallenge.\nLin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A\nstructured self-attentive sentence embedding. ArXiv:1703.03130.\nLipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural net-\nworks for sequence learning. ArXiv:1506.00019.\nLipton, Z. C., Kale, D. C., Elkan, C., & Wetzel, R. (2016). Learning to diagnose with LSTM\nrecurrent neural networks. International Conference on Learning Representations (ICLR).\nLipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship.\nCommunications of the ACM, 17, 45–77.\nLiu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale\noptimization. Mathematical Programming, 45(1), 503–528.\nLiu, H., Simonyan, K., & Yang, Y. (2018). DARTS: diﬀerentiable architecture search.\nArXiv:1806.09055.\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016).\nSSD: single shot multibox detector. European Conference on Computer Vision (pp. 21–37).\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … Stoyanov, V. (2019). RoBERTa:\na robustly optimized BERT pretraining approach. ArXiv:1907.11692.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … Guo, B. (2021). Swin transformer:\nhierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (pp. 10012–10022).\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convNet for\nthe 2020s. ArXiv:2201.03545.\nLong, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic\nsegmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition (pp. 3431–3440).\nLoshchilov, I., & Hutter, F. (2016). SGDR: stochastic gradient descent with warm restarts.\nArXiv:1608.03983.\nLowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International\nJournal of Computer Vision, 60(2), 91–110.\nLuo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in\nbatch normalization. ArXiv:1809.00846.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learning\nword vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Language Technologies, Volume 1 (pp. 142–\n150).\nMack, Y.-P., & Silverman, B. W. (1982). Weak and strong uniform consistency of kernel re-\ngression estimates. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 61(3),\n405–415.\nMacKay, D. J. (2003). Information Theory, Inference and Learning Algorithms. Cambridge\nUniversity Press.\n\n1135\nREFERENCES\nMaclaurin, D., Duvenaud, D., & Adams, R. (2015). Gradient-based hyperparameter opti-\nmization through reversible learning. Proceedings of the 32nd International Conference on\nMachine Learning (ICML'15).\nMangasarian, O. L. (1965). Linear and nonlinear separation of patterns by linear program-\nming. Oper. Res., 13, 444-452.\nMangram, M. E. (2013). A simpliﬁed perspective of the Markowitz portfolio theory. Global\nJournal of Business Research, 7(1), 59–70.\nMatthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., & Ghahramani, Z. (2018).\nGaussian process behaviour in wide deep neural networks. ArXiv:1804.11271.\nMcCann, B., Bradbury, J., Xiong, C., & Socher, R. (2017). Learned in translation: Contextu-\nalized word vectors. Advances in Neural Information Processing Systems (pp. 6294–6305).\nMcCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous\nactivity. Bulletin of Mathematical Biophysics, 5(4), 115–133.\nMead, C. (1980). Introduction to VLSI systems. IEE Proceedings I-Solid-State and Electron\nDevices, 128(1), 18.\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models.\nArXiv:1609.07843.\nMicchelli, C. A. (1984). Interpolation of scattered data: distance matrices and conditionally\npositive deﬁnite functions. Approximation Theory and Spline Functions (pp. 143–145).\nSpringer.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Eﬃcient estimation of word repre-\nsentations in vector space. ArXiv:1301.3781.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed represen-\ntations of words and phrases and their compositionality. Advances in Neural Information\nProcessing Systems (pp. 3111–3119).\nMiller, G. A. (1995). WordNet: a lexical database for English. Communications of the ACM,\n38(11), 39–41.\nMirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., … Dean, J. (2017).\nDevice placement optimization with reinforcement learning. Proceedings of the 34th In-\nternational Conference on Machine Learning (pp. 2430–2439).\nMnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention.\nAdvances in Neural Information Processing Systems (pp. 2204–2212).\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Ried-\nmiller, M. (2013). Playing Atari with deep reinforcement learning. ArXiv:1312.5602.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … et al.\n(2015). Human-level control through deep reinforcement learning. Nature, 518(7540),\n529–533.\nMoon, T., Smola, A., Chang, Y., & Zheng, Z. (2010). Intervalrank: isotonic regression with\nlistwise and pairwise constraints. Proceedings of the 3rd ACM International Conference on\nWeb Search and Data Mining (pp. 151–160).\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016).\nThe fallacy of placing conﬁdence in conﬁdence intervals. Psychonomic Bulletin & Review,\n23(1), 103–123.\nMorozov, V. A. (1984). Methods for Solving Incorrectly Posed Problems. Springer.\n\n1136\nREFERENCES\nNadaraya, E. A. (1964). On estimating regression. Theory of Probability & its Applications,\n9(1), 141–142.\nNair, V., & Hinton, G. E. (2010). Rectiﬁed linear units improve restricted Boltzmann ma-\nchines. ICML.\nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., & Sutskever, I. (2021). Deep\ndouble descent: where bigger models and more data hurt. Journal of Statistical Mechanics:\nTheory and Experiment, 2021(12), 124003.\nNaor, M., & Reingold, O. (1999). On the construction of pseudorandom permutations: Luby–\nRackoﬀrevisited. Journal of Cryptology, 12(1), 29–66.\nNeal, R. M. (1996). Bayesian Learning for Neural Networks. Springer.\nNesterov, Y. (2018). Lectures on Convex Optimization. Springer.\nNesterov, Y., & Vial, J.-P. (2000). Conﬁdence level solutions for stochastic programming.\nAutomatica, 44(6), 1559–1568.\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical the-\nory of probability. Philosophical Transactions of the Royal Society of London. Series A,\nMathematical and Physical Sciences, 236(767), 333–380.\nNorelli, A., Fumero, M., Maiorca, V., Moschella, L., Rodolà, E., & Locatello, F.\n(2022). ASIF: coupled data turns unimodal models to multimodal without training.\nArXiv:2210.01738.\nNovak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., … Sohl-Dickstein, J. (2018).\nBayesian deep convolutional networks with many channels are Gaussian processes.\nArXiv:1810.05148.\nNovikoﬀ, A. B. J. (1962). On convergence proofs on perceptrons. Proceedings of the Sym-\nposium on the Mathematical Theory of Automata (pp. 615–622).\nOlshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties\nby learning a sparse code for natural images. Nature, 381(6583), 607–609.\nOng, C. S., Smola, A., & Williamson, R. (2005). Learning the kernel with hyperkernels.\nJournal of Machine Learning Research, 6, 1043–1071.\nOpenAI. (2023). GPT-4 Technical Report. ArXiv:2303.08774.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., … et al. (2022).\nTraining language models to follow instructions with human feedback. ArXiv:2203.02155.\nPapineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic\nevaluation of machine translation. Proceedings of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics (pp. 311–318).\nParikh, A. P., Täckström, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention\nmodel for natural language inference. ArXiv:1606.01933.\nPark, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with\nspatially-adaptive normalization. Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (pp. 2337–2346).\nParzen, E. (1957). On consistent estimates of the spectrum of a stationary time series. Annals\nof Mathematical Statistics, 28, 329–348.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., … et al. (2019). Py-\nTorch: an imperative style, high-performance deep learning library. Advances in Neural\nInformation Processing Systems, 32, 8026–8037.\n\n1137\nREFERENCES\nPaulus, R., Xiong, C., & Socher, R. (2017). A deep reinforced model for abstractive sum-\nmarization. ArXiv:1705.04304.\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., … Launay,\nJ. (2023). The ReﬁnedWeb dataset for Falcon LLM: outperforming curated corpora with\nweb data, and web data only. ArXiv:2306.01116.\nPennington, J., Schoenholz, S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep learn-\ning through dynamical isometry: theory and practice. Advances in Neural Information Pro-\ncessing Systems (pp. 4785–4795).\nPennington, J., Socher, R., & Manning, C. (2014). GloVe: global vectors for word repre-\nsentation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) (pp. 1532–1543).\nPeters, J., Janzing, D., & Schölkopf, B. (2017). Elements of Causal Inference: Foundations\nand Learning Algorithms. MIT Press.\nPeters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence\ntagging with bidirectional language models. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics, Volume 1 (pp. 1756–1765).\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.\n(2018). Deep contextualized word representations. Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (pp. 2227–2237).\nPetersen, K. B., & Pedersen, M. S. (2008). The Matrix Cookbook. Technical University of\nDenmark.\nPleiss, G., Chen, D., Huang, G., Li, T., Van Der Maaten, L., & Weinberger, K. Q. (2017).\nMemory-eﬃcient implementation of densenets. ArXiv:1707.06990.\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSR Computational Mathematics and Mathematical Physics, 4(5), 1–17.\nPrakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016). Neural\nparaphrase generation with stacked residual LSTM networks. ArXiv:1610.03098.\nQin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. (2023). Is ChatGPT a\ngeneral-purpose natural language processing task solver? ArXiv:2302.06476.\nQuadrana, M., Cremonesi, P., & Jannach, D. (2018). Sequence-aware recommender systems.\nACM Computing Surveys, 51(4), 66.\nQuinlan, J. R. (1993). C4.5: Programs for Machine Learning. Elsevier.\nRabiner, L., & Juang, B.-H. (1993). Fundamentals of Speech Recognition. Prentice-Hall.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … et al. (2021).\nLearning transferable visual models from natural language supervision. International Con-\nference on Machine Learning (pp. 8748–8763).\nRadford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep\nconvolutional generative adversarial networks. ArXiv:1511.06434.\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language un-\nderstanding by generative pre-training. OpenAI.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 9.\n\n1138\nREFERENCES\nRadosavovic, I., Johnson, J., Xie, S., Lo, W.-Y., & Dollár, P. (2019). On network design\nspaces for visual recognition. Proceedings of the IEEE/CVF International Conference on\nComputer Vision (pp. 1882–1890).\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Dollár, P. (2020). Designing network\ndesign spaces. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (pp. 10428–10436).\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoﬀmann, J., Song, F., … et al.\n(2021). Scaling language models: methods, analysis & insights from training gopher.\nArXiv:2112.11446.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … Liu, P. J. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of\nMachine Learning Research, 21, 1–67.\nRajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ questions for\nmachine comprehension of text. ArXiv:1606.05250.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019).\nStand-alone self-attention in vision models. Advances in Neural Information Processing\nSystems, 32.\nRamachandran, P., Zoph, B., & Le, Q. V. (2017). Searching for activation functions.\nArXiv:1710.05941.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-\nconditional image generation with clip latents. ArXiv:2204.06125.\nRamón y Cajal, Santiago, & Azoulay, L. (1894). Les Nouvelles Idées sur la Structure du Sys-\ntème Nerveux chez l'Homme et chez les Vertébrés. Paris, C. Reinwald & Cie.\nRanzato, M.-A., Boureau, Y.-L., Chopra, S., & LeCun, Y. (2007). A uniﬁed energy-based\nframework for unsupervised learning. Artiﬁcial Intelligence and Statistics (pp. 371–379).\nRasmussen, C. E., & Williams, C. K. (2006). Gaussian Processes for Machine Learning. MIT\nPress.\nReddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of Adam and beyond.\nArXiv:1904.09237.\nRedmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look once: uniﬁed,\nreal-time object detection. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 779–788).\nRedmon,\nJ.,\n&\nFarhadi,\nA.\n(2018).\nYOLOv3:\nan\nincremental\nimprovement.\nArXiv:1804.02767.\nReed, S., & De Freitas, N. (2015). Neural programmer-interpreters. ArXiv:1511.06279.\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., …\net al. (2022). A generalist agent. ArXiv:2205.06175.\nRen, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: towards real-time object de-\ntection with region proposal networks. Advances in Neural Information Processing Systems\n(pp. 91–99).\nRevels, J., Lubin, M., & Papamarkou, T. (2016). Forward-mode automatic diﬀerentiation in\nJulia. ArXiv:1607.07892.\nRezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and ap-\nproximate inference in deep generative models. International Conference on Machine\nLearning (pp. 1278–1286).\n\n1139\nREFERENCES\nRiesenhuber, M., & Poggio, T. (1999). Hierarchical models of object recognition in cortex.\nNature Neuroscience, 2(11), 1019–1025.\nRockafellar, R. T. (1970). Convex Analysis. Princeton University Press.\nRolnick, D., Veit, A., Belongie, S., & Shavit, N. (2017). Deep learning is robust to massive\nlabel noise. ArXiv:1705.10694.\nRudin, W. (1973). Functional Analysis. McGraw-Hill.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by back-\npropagating errors. Cognitive Modeling, 5(3), 1.\nRussakovsky, O., Deng, J., Huang, Z., Berg, A. C., & Fei-Fei, L. (2013). Detecting avocados\nto zucchinis: what have we done, and where are we going? International Conference on\nComputer Vision (ICCV).\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., … et al. (2015). ImageNet\nlarge scale visual recognition challenge. International Journal of Computer Vision, 115(3),\n211–252.\nRussell, S. J., & Norvig, P. (2016). Artiﬁcial Intelligence: A Modern Approach. Pearson Ed-\nucation Limited.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … et al. (2022). Photorealis-\ntic text-to-image diﬀusion models with deep language understanding. ArXiv:2205.11487.\nSalinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., & Archambeau, C. (2022).\nSyne Tune: a library for large scale hyperparameter tuning and reproducible research. First\nConference on Automated Machine Learning.\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. ArXiv:1910.01108.\nSanh, V., Webson, A., Raﬀel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., … et al. (2021).\nMultitask prompted training enables zero-shot task generalization. ArXiv:2110.08207.\nSanturkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help\noptimization? Advances in Neural Information Processing Systems (pp. 2483–2493).\nSarwar, B. M., Karypis, G., Konstan, J. A., & Riedl, J. (2001). Item-based collaborative ﬁl-\ntering recommendation algorithms. Proceedings of 10th International Conference on World\nWide Web (pp. 285–295).\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., … et al. (2022). BLOOM:\na 176B-parameter open-access multilingual language model. ArXiv:2211.05100.\nSchein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M. (2002). Methods and metrics\nfor cold-start recommendations. Proceedings of the 25th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (pp. 253–260).\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., … et al.\n(2022). LAION-5B: an open large-scale dataset for training next generation image-text\nmodels. ArXiv:2210.08402.\nSchuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Trans-\nactions on Signal Processing, 45(11), 2673–2681.\nSchölkopf, B., Herbrich, R., & Smola, A. J. (2001). Helmbold, D. P., & Williamson, B.\n(Eds.). A generalized representer theorem. Proceedings of the Annual Conference on Com-\nputational Learning Theory (pp. 416–426). Springer-Verlag.\nSchölkopf, B., Burges, C., & Vapnik, V. (1996). Incorporating invariances in support vector\nlearning machines. International Conference on Artiﬁcial Neural Networks (pp. 47–52).\n\n1140\nREFERENCES\nSchölkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector Machines, Reg-\nularization, Optimization, and Beyond. MIT Press.\nSennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with\nsubword units. ArXiv:1508.07909.\nSergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in\nTensorFlow. ArXiv:1802.05799.\nShannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical\nJournal, 27(3), 379–423.\nShao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., … Abdelzaher, T. (2020). Con-\ntrolVAE: controllable variational autoencoder. Proceedings of the 37th International Con-\nference on Machine Learning.\nShaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position represen-\ntations. ArXiv:1803.02155.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019).\nMegatron-LM: training multi-billion parameter language models using model parallelism.\nArXiv:1909.08053.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … et al.\n(2016). Mastering the game of Go with deep neural networks and tree search. Nature,\n529(7587), 484.\nSilverman, B. W. (1986). Density Estimation for Statistical and Data Analysis. Chapman and\nHall.\nSimard, P. Y., LeCun, Y. A., Denker, J. S., & Victorri, B. (1998). Transformation invariance\nin pattern recognition – tangent distance and tangent propagation. Neural Networks: Tricks\nof the Trade (pp. 239–274). Springer.\nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale\nimage recognition. ArXiv:1409.1556.\nSindhwani, V., Sainath, T. N., & Kumar, S. (2015). Structured transforms for small-footprint\ndeep learning. ArXiv:1510.01722.\nSivic, J., & Zisserman, A. (2003). Video Google: a text retrieval approach to object matching\nin videos. Proceedings of the IEEE International Conference on Computer Vision (pp. 1470–\n1470).\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., … et al.\n(2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-\nscale generative language model. ArXiv:2201.11990.\nSmola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Pro-\nceedings of the VLDB Endowment, 3(1-2), 703–710.\nSnoek, J., Larochelle, H., & Adams, R. (2012). Practical Bayesian optimization of machine\nlearning algorithms. Advances in Neural Information Processing Systems 25 (pp. 2951–\n2959).\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsuper-\nvised learning using nonequilibrium thermodynamics. International Conference on Ma-\nchine Learning (pp. 2256–2265).\nSong, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data dis-\ntribution. Advances in Neural Information Processing Systems, 32.\n\n1141\nREFERENCES\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021).\nScore-based generative modeling through stochastic diﬀerential equations. International\nConference on Learning Representations.\nSpeelpenning, B. (1980). Compiling fast partial derivatives of functions given by algorithms\n(Doctoral dissertation). University of Illinois at Urbana-Champaign.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., … et al. (2022).\nBeyond the imitation game: quantifying and extrapolating the capabilities of language\nmodels. ArXiv:2206.04615.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).\nDropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\nLearning Research, 15(1), 1929–1958.\nSrivastava,\nR.\nK.,\nGreﬀ,\nK.,\n&\nSchmidhuber,\nJ.\n(2015).\nHighway\nnetworks.\nArXiv:1505.00387.\nStrang, G. (1993). Introduction to Linear Algebra. Wellesley–Cambridge Press.\nSu, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative ﬁltering techniques. Ad-\nvances in Artiﬁcial Intelligence, 2009.\nSukhbaatar, S., Weston, J., & Fergus, R. (2015). End-to-end memory networks. Advances in\nNeural Information Processing Systems (pp. 2440–2448).\nSutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initial-\nization and momentum in deep learning. International Conference on Machine Learning\n(pp. 1139–1147).\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advances in Neural Information Processing Systems (pp. 3104–3112).\nSzegedy, C., Ioﬀe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, Inception-\nResNet and the impact of residual connections on learning. 31st AAAI Conference on\nArtiﬁcial Intelligence.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … Rabinovich, A.\n(2015). Going deeper with convolutions. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (pp. 1–9).\nSzegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the Incep-\ntion architecture for computer vision. Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (pp. 2818–2826).\nTallec, C., & Ollivier, Y. (2017). Unbiasing truncated backpropagation through time.\nArXiv:1705.08209.\nTan, M., & Le, Q. (2019). EﬃcientNet: rethinking model scaling for convolutional neural\nnetworks. International Conference on Machine Learning (pp. 6105–6114).\nTaskar, B., Guestrin, C., & Koller, D. (2004). Max-margin Markov networks. Advances in\nNeural Information Processing Systems, 16, 25.\nTay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). Eﬃcient transformers: a survey.\nArXiv:2009.06732.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., … Stojnic, R.\n(2022). Galactica: a large language model for science. ArXiv:2211.09085.\nTeye, M., Azizpour, H., & Smith, K. (2018). Bayesian uncertainty estimation for batch nor-\nmalized deep networks. ArXiv:1802.06455.\n\n1142\nREFERENCES\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., … Li, L.-J.\n(2016). Yfcc100m: the new data in multimedia research. Communications of the ACM,\n59(2), 64–73.\nTieleman, T., & Hinton, G. (2012). Divide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural Networks for Machine Learning, Lecture 6.5-rmsprop.\nTikhonov, A. N., & Arsenin, V. Y. (1977). Solutions of Ill-Posed Problems. W.H. Winston.\nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., … et al.\n(2021). MLP-mixer: an all-MLP architecture for vision. Advances in Neural Information\nProcessing Systems, 34.\nTorralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: a large data set\nfor nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 30(11), 1958–1970.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021). Training\ndata-eﬃcient image transformers & distillation through attention. International Conference\non Machine Learning (pp. 10347–10357).\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., … et al.\n(2023a). LLaMA: open and eﬃcient foundation language models. ArXiv:2302.13971.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., … et al. (2023b).\nLLaMA 2: open foundation and ﬁne-tuned chat models. ArXiv:2307.09288.\nTsoumakas, G., & Katakis, I. (2007). Multi-label classiﬁcation: an overview. International\nJournal of Data Warehousing and Mining, 3(3), 1–13.\nTuring, A. (1950). Computing machinery and intelligence. Mind, 59(236), 433.\nUijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective search\nfor object recognition. International Journal of Computer Vision, 104(2), 154–171.\nVapnik, V. (1995). The Nature of Statistical Learning Theory. New York: Springer.\nVapnik, V. (1998). Statistical Learning Theory. New York: John Wiley and Sons.\nVapnik, V., & Chervonenkis, A. (1964). A note on one class of perceptrons. Automation and\nRemote Control, 25.\nVapnik, V., & Chervonenkis, A. (1968). Uniform convergence of frequencies of occurence\nof events to their probabilities. Dokl. Akad. Nauk SSSR, 181, 915-918.\nVapnik, V., & Chervonenkis, A. (1971). On the uniform convergence of relative frequencies\nof events to their probabilities. Theory Probab. Appl., 16(2), 264-281.\nVapnik, V., & Chervonenkis, A. (1981). The necessary and suﬃcient conditions for the uni-\nform convergence of averages to their expected values. Teoriya Veroyatnostei i Ee Prime-\nneniya, 26(3), 543-564.\nVapnik, V., & Chervonenkis, A. (1991). The necessary and suﬃcient conditions for consis-\ntency in the empirical risk minimization method. Pattern Recognition and Image Analysis,\n1(3), 283-305.\nVapnik, V. N., & Chervonenkis, A. Y. (1974). Ordered risk minimization. Automation and\nRemote Control, 35, 1226–1235, 1403–1412.\nVapnik, V. (1992). Principles of risk minimization for learning theory. Advances in Neural\nInformation Processing Systems (pp. 831–838).\nVapnik, V., Levin, E., & Le Cun, Y. (1994). Measuring the VC-dimension of a learning\nmachine. Neural Computation, 6(5), 851–876.\n\n1143\nREFERENCES\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin,\nI. (2017). Attention is all you need. Advances in Neural Information Processing Systems\n(pp. 5998–6008).\nWahba, G. (1990). Spline Models for Observational Data. SIAM.\nWaibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. J. (1989). Phoneme recogni-\ntion using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal\nProcessing, 37(3), 328–339.\nWang, H., Zhang, A., Zheng, S., Shi, X., Li, M., & Wang, Z. (2022). Removing batch\nnormalization boosts adversarial training. International Conference on Machine Learning\n(pp. 23433–23445).\nWang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for ag-\ngregation. Networks, 2(3), 2–3.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S. (2019). Learning\ndeep transformer models for machine translation. Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics (pp. 1810–1822).\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., & Zhou, D. (2023). Self-consistency im-\nproves chain of thought reasoning in language models. International Conference on Learn-\ning Representations.\nWang, Y., Davidson, A., Pan, Y., Wu, Y., Riﬀel, A., & Owens, J. D. (2016). Gunrock: a\nhigh-performance graph processing library on the GPU. ACM SIGPLAN Notices (p. 11).\nWarstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7, 625–641.\nWasserman, L. (2013). All of Statistics: A Concise Course in Statistical Inference. Springer.\nWatkins, C. J., & Dayan, P. (1992). Q-learning. Machine Learning, 8(3–4), 279–292.\nWatson, G. S. (1964). Smooth regression analysis. Sankhyā: The Indian Journal of Statistics,\nSeries A, pp. 359–372.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … Le, Q. V. (2021).\nFinetuned language models are zero-shot learners. ArXiv:2109.01652.\nWei, J., Tay, Y., Bommasani, R., Raﬀel, C., Zoph, B., Borgeaud, S., … et al. (2022). Emer-\ngent abilities of large language models. ArXiv:2206.07682.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain\nof thought prompting elicits reasoning in large language models. ArXiv:2201.11903.\nWelling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dy-\nnamics. Proceedings of the 28th International Conference on Machine Learning (ICML-11)\n(pp. 681–688).\nWengert, R. E. (1964). A simple automatic derivative evaluation program. Communications\nof the ACM, 7(8), 463–464.\nWerbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceed-\nings of the IEEE, 78(10), 1550–1560.\nWigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann.\nMath. (pp. 325–327).\nWilson, A. G., & Izmailov, P. (2020). Bayesian deep learning and a probabilistic perspective\nof generalization. Advances in Neural Information Processing Systems, 33, 4697–4708.\nWistuba, M., Rawat, A., & Pedapati, T. (2019). A survey on neural architecture search.\nArXiv:1905.01392 [cs.LG].\n\n1144\nREFERENCES\nWistuba, M., Schilling, N., & Schmidt-Thieme, L. (2018). Scalable Gaussian process-based\ntransfer surrogates for hyperparameter optimization. Machine Learning, 108, 43–78.\nWolpert, D. H., & Macready, W. G. (1995). No free lunch theorems for search. Technical\nReport SFI-TR-95-02-010, Santa Fe Institute.\nWood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence\nmemoizer. Communications of the ACM, 54(2), 91–98.\nWu, B., Wan, A., Yue, X., Jin, P., Zhao, S., Golmant, N., … Keutzer, K. (2018). Shift: a zero\nﬂop, zero parameter alternative to spatial convolutions. Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (pp. 9127–9135).\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … et al. (2016).\nGoogle's neural machine translation system: bridging the gap between human and machine\ntranslation. ArXiv:1609.08144.\nXiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for bench-\nmarking machine learning algorithms. ArXiv:1708.07747.\nXiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., & Pennington, J. (2018). Dynamical\nisometry and a mean ﬁeld theory of CNNs: how to train 10,000-layer vanilla convolutional\nneural networks. International Conference on Machine Learning (pp. 5393–5402).\nXie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. (2017). Aggregated residual transformations\nfor deep neural networks. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 1492–1500).\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., … Liu, T. (2020). On layer nor-\nmalization in the transformer architecture. International Conference on Machine Learning\n(pp. 10524–10533).\nXiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., & Stolcke, A. (2018). The Microsoft\n2017 conversational speech recognition system. 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) (pp. 5934–5938).\nYamaguchi, K., Sakamoto, K., Akabane, T., & Fujimoto, Y. (1990). A neural network for\nspeaker-independent isolated word recognition. First International Conference on Spoken\nLanguage Processing.\nYang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016). Neural machine translation with\nrecurrent attention modeling. ArXiv:1607.05108.\nYang, Z., Moczulski, M., Denil, M., De Freitas, N., Smola, A., Song, L., & Wang, Z. (2015).\nDeep fried convnets. Proceedings of the IEEE International Conference on Computer Vision\n(pp. 1476–1483).\nYe, M., Yin, P., Lee, W.-C., & Lee, D.-L. (2011). Exploiting geographical inﬂuence for col-\nlaborative point-of-interest recommendation. Proceedings of the 34th International ACM\nSIGIR Conference on Research and Development in Information Retrieval (pp. 325–334).\nYou, Y., Gitman, I., & Ginsburg, B. (2017). Large batch training of convolutional networks.\nArXiv:1708.03888.\nYu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., … Wu, Y. (2022). Scaling autore-\ngressive models for content-rich text-to-image generation. ArXiv:2206.10789.\nZaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for\nnonconvex optimization. Advances in Neural Information Processing Systems (pp. 9793–\n9803).\nZeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. ArXiv:1212.5701.\n\n1145\nREFERENCES\nZeiler, M. D., & Fergus, R. (2013). Stochastic pooling for regularization of deep convolu-\ntional neural networks. ArXiv:1301.3557.\nZhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., & Fu, J. (2021). Beyond\nfully-connected layers with quaternions: parameterization of hypercomplex multiplications\nwith 1/n parameters. International Conference on Learning Representations.\nZhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2021). Understanding deep learn-\ning (still) requires rethinking generalization. Communications of the ACM, 64(3), 107–115.\nZhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system: a\nsurvey and new perspectives. ACM Computing Surveys, 52(1), 5.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., … et al. (2022). OPT:\nopen pre-trained transformer language models. ArXiv:2205.01068.\nZhang, W., Tanida, J., Itoh, K., & Ichioka, Y. (1988). Shift-invariant pattern recognition\nneural network and its optical architecture. Proceedings of Annual Conference of the Japan\nSociety of Applied Physics.\nZhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., … Wang, X. (2021). ByteTrack:\nmulti-object tracking by associating every detection box. ArXiv:2110.06864.\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting\nin large language models. International Conference on Learning Representations.\nZhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2023). Multimodal chain-\nof-thought reasoning in language models. ArXiv:2302.00923.\nZhao, Z.-Q., Zheng, P., Xu, S.-t., & Wu, X. (2019). Object detection with deep learning: a\nreview. IEEE Transactions on Neural Networks and Learning Systems, 30(11), 3212–3232.\nZhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., … Chi, E. (2023). Least-to-most\nprompting enables complex reasoning in large language models. International Conference\non Learning Representations.\nZhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. Proceedings of the IEEE International Confer-\nence on Computer Vision (pp. 2223–2232).\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.\n(2015). Aligning books and movies: towards story-like visual explanations by watching\nmovies and reading books. Proceedings of the IEEE International Conference on Computer\nVision (pp. 19–27).\nZoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning.\nArXiv:1611.01578.",
    "pdf_filename": "Dive into Deep Learning.pdf"
}