{
    "title": "CLIP Unreasonable Potential in Single-Shot Face",
    "abstract": "sion, designed to identify and authenticate individuals by an- other classification task is subjectivity to false-positive result, alyzing facial patterns and features. This field intersects with misunderstanding someone face with others. Upon testing, we artificial intelligence, image processing, and machine learning, realizethatbyusingthevision-languagecorrespondencefrom withapplicationsinsecurity,authentication,andpersonalization. Traditional approaches in facial recognition focus on capturing CLIP features, model only require a single shot finetuning facial features like the eyes, nose, and mouth and matching while reducing false-positive results significantly without any these against a database to verify identities. However, chal- state-of-the-art methods for extracting facial features from lenges such as high false-positive rates have persisted, often massive datasets. due to the similarity among individuals’ facial features. Re- cently,ContrastiveLanguage-ImagePretraining(CLIP),amodel II. BACKGROUND developed by OpenAI, has shown promising advancements by linking natural language processing with vision tasks, allowing A. Face recognition it to generalize across modalities. Using CLIP’s vision-language correspondenceandsingle-shotfinetuning,themodelcanachieve Facerecognitionhasemergedasapowerfultoolincomputer lower false-positive rates upon deployment without the need of vision,enablingtheidentificationorverificationofindividuals mass facial features extraction. This integration demonstrating based on their facial features. This technology has applica- CLIP’s potential to address persistent issues in face recognition tions in various fields such as security, law enforcement, and modelperformancewithoutcomplicatingourtrainingparadigm. personal device authentication. Early approaches focused on Index Terms—Face recognition, image classification, multi- geometrical models and eigenface techniques [5]. With the modal learning advent of machine learning, methods such as Local Binary Patterns (LBP) [6] and Fisherfaces [7] improved robustness I. INTRODUCTION under various lighting conditions and facial expressions. Face recognition [1]–[3] is a pivotal task within the realm The rise of deep learning has further revolutionized the of computer vision, wherein algorithms are designed to iden- field, with convolutional neural networks (CNNs) becoming tify and authenticate individuals by analyzing and comparing the backbone of face recognition systems. Groundbreaking patterns in facial features. It’s a multifaceted field that inter- models like DeepFace [8] and FaceNet [9] introduced face sects with various disciplines like artificial intelligence, image embeddingsthatprovidedremarkableaccuracyandefficiency, processing and machine learning. even in large-scale applications. More recent advancements At its core, facial recognition involves capturing facial leverage deep residual networks and novel loss functions to images and videos, extracting unique characteristics such as achieve even higher accuracy [10], [11]. These advancements the arrangement of eyes, nose, and mouth, and then matching underscore the rapid development of face recognition and its these features against a database of known faces to make growing importance across technology sectors. identifications or verifications. This technology has found B. CLIP and its applications widespread applications across diverse sectors, ranging from securityandsurveillancetoauthenticationandpersonalization. CLIP [4] model has made significant strides in connecting CLIP [4], or Contrastive Language-Image Pretraining, is a visionandlanguagemodalities,enablingzero-shotcapabilities ground breaking model developed by OpenAI that transcends acrossvariouscomputervisiontasks[12]–[14].CLIP[4]lever- traditional boundaries between natural language processing ages a contrastive learning framework, aligning text descrip- andcomputervision.Unlikeconventionalmodelsthatspecial- tionsandimagesinasharedembeddingspace.Thisalignment izeineithertextorimagemodeling,CLIPlearnstounderstand allows CLIP [4] to generalize across tasks without the need both modalities simultaneously. This means it is possible that for task-specific training. CLIP [4] has shown versatility in CLIP can perform a wide range of tasks across different object detection [15], [16], image generation [17], [18], and domains, including those related to facial recognition. scene understanding [19]. 4202 voN 91 ]VC.sc[ 1v91321.1142:viXra",
    "body": "CLIP Unreasonable Potential in Single-Shot Face\nRecognition\nNhan T. Luu\nDepartment of Computer Science and Engineering, The University of Aizu,\nAizuwakamatsu, Japan\nltnhan0902@gmail.com\nAbstract—Face recognition is a core task in computer vi- One of the big problem distinct facial recognition with\nsion, designed to identify and authenticate individuals by an- other classification task is subjectivity to false-positive result,\nalyzing facial patterns and features. This field intersects with\nmisunderstanding someone face with others. Upon testing, we\nartificial intelligence, image processing, and machine learning,\nrealizethatbyusingthevision-languagecorrespondencefrom\nwithapplicationsinsecurity,authentication,andpersonalization.\nTraditional approaches in facial recognition focus on capturing CLIP features, model only require a single shot finetuning\nfacial features like the eyes, nose, and mouth and matching while reducing false-positive results significantly without any\nthese against a database to verify identities. However, chal- state-of-the-art methods for extracting facial features from\nlenges such as high false-positive rates have persisted, often\nmassive datasets.\ndue to the similarity among individuals’ facial features. Re-\ncently,ContrastiveLanguage-ImagePretraining(CLIP),amodel\nII. BACKGROUND\ndeveloped by OpenAI, has shown promising advancements by\nlinking natural language processing with vision tasks, allowing A. Face recognition\nit to generalize across modalities. Using CLIP’s vision-language\ncorrespondenceandsingle-shotfinetuning,themodelcanachieve Facerecognitionhasemergedasapowerfultoolincomputer\nlower false-positive rates upon deployment without the need of vision,enablingtheidentificationorverificationofindividuals\nmass facial features extraction. This integration demonstrating based on their facial features. This technology has applica-\nCLIP’s potential to address persistent issues in face recognition\ntions in various fields such as security, law enforcement, and\nmodelperformancewithoutcomplicatingourtrainingparadigm.\npersonal device authentication. Early approaches focused on\nIndex Terms—Face recognition, image classification, multi- geometrical models and eigenface techniques [5]. With the\nmodal learning advent of machine learning, methods such as Local Binary\nPatterns (LBP) [6] and Fisherfaces [7] improved robustness\nI. INTRODUCTION under various lighting conditions and facial expressions.\nFace recognition [1]–[3] is a pivotal task within the realm The rise of deep learning has further revolutionized the\nof computer vision, wherein algorithms are designed to iden- field, with convolutional neural networks (CNNs) becoming\ntify and authenticate individuals by analyzing and comparing the backbone of face recognition systems. Groundbreaking\npatterns in facial features. It’s a multifaceted field that inter- models like DeepFace [8] and FaceNet [9] introduced face\nsects with various disciplines like artificial intelligence, image embeddingsthatprovidedremarkableaccuracyandefficiency,\nprocessing and machine learning. even in large-scale applications. More recent advancements\nAt its core, facial recognition involves capturing facial leverage deep residual networks and novel loss functions to\nimages and videos, extracting unique characteristics such as achieve even higher accuracy [10], [11]. These advancements\nthe arrangement of eyes, nose, and mouth, and then matching underscore the rapid development of face recognition and its\nthese features against a database of known faces to make growing importance across technology sectors.\nidentifications or verifications. This technology has found\nB. CLIP and its applications\nwidespread applications across diverse sectors, ranging from\nsecurityandsurveillancetoauthenticationandpersonalization. CLIP [4] model has made significant strides in connecting\nCLIP [4], or Contrastive Language-Image Pretraining, is a visionandlanguagemodalities,enablingzero-shotcapabilities\nground breaking model developed by OpenAI that transcends acrossvariouscomputervisiontasks[12]–[14].CLIP[4]lever-\ntraditional boundaries between natural language processing ages a contrastive learning framework, aligning text descrip-\nandcomputervision.Unlikeconventionalmodelsthatspecial- tionsandimagesinasharedembeddingspace.Thisalignment\nizeineithertextorimagemodeling,CLIPlearnstounderstand allows CLIP [4] to generalize across tasks without the need\nboth modalities simultaneously. This means it is possible that for task-specific training. CLIP [4] has shown versatility in\nCLIP can perform a wide range of tasks across different object detection [15], [16], image generation [17], [18], and\ndomains, including those related to facial recognition. scene understanding [19].\n4202\nvoN\n91\n]VC.sc[\n1v91321.1142:viXra\nFig. 1: A diagram illustrate our dataset acquisition and preprocessing method.\nCLIP [4] has also been used to improve the quality and rel- accurately.Forfurtheralignmentandstandardization,weused\nevanceofimagesproducedbydecoder-likegenerativemodels. the model to extract essential facial keypoints, specifically\nIts capability to evaluate semantic similarity has enabled inte- the positions of the eyes, nose, and mouth. These keypoints\ngrationwithautoregressivemodelanddiffusionmodelsuchas facilitated the vertical alignment of faces across different\nunCLIP [17] as well as GAN in the case of StyleCLIP [18], images, ensuring consistency in the data representation. The\nwhere CLIP-guided latent manipulation achieves fine-grained cropped and aligned face images were then quality assured\ncontrolovergeneratedimages.Recentresearchhasshownthat manually by a person before being saved into a separate\nCLIP-guided methods achieve better alignment with desired folderstructure,labeledwitheachparticipant’sname,allowing\ntextual descriptions, as seen in text-to-image generation [17], organized and efficient access for subsequent experimentation\n[20], [21] and image captioning [22], [23] tasks. Aside from and analysis.\nthat,therearealsosomeresearchesdivertintoimprovingCLIP\nperformance by alternating its architecture, such as MaPLe IV. EXPERIMENTDESIGN\n[24], [25], CoOp [26] and Co-CoOp [19].\nA. Model training\nIn face recognition, CLIP’s multimodal capabilities provide\nunique advantages. While traditional face recognition relies After completing the dataset creation and preprocessing\non supervised learning with large labeled datasets [9], [11], stages, we designed a face recognition pipeline that diverges\nrecent studies have explored using CLIP to develop face from traditional approaches by replacing the standard facial\nrecognition systems by leveraging its generalized embeddings featureencoderandclassifierwithCLIP[4]model,specifically\n[27], [28]. Still, CLIP face recognition potentials are still the CLIP-RN50 variant (as shown in Figure 2).\nremain underexplored. In this setup, we treated face recognition as an image\nclassificationtask,usingasingle-shotfinetuningmethodonthe\nIII. DATAACQUISITIONANDPROCESSING processed images of participants. Instead of tuning the entire\nIn this research, we used an image dataset consists of model, we froze the image encoder parameters to leverage\nhigh resolution images of volunteers, contains faces sorted CLIP’s pretrained vision features and performed backpropa-\nby individual names, ensuring a clear and organized structure gation only on the text encoder. Each image was associated\nfor face recognition experiments. Images were captured using withatextpromptformattedas”Thisistheimageofaperson\na 3 megapixels camera within a 2 meters range to obtain named ...” which allows the CLIP model to align specific\nvarious perspectives of each participant, including views from participant identities with their visual features.\nabove, below, left, right, and the front (as shown in Figure Initially, approaching face recognition as an image classifi-\n1). These multiple angles simulate real-world scenarios where cation problem may seem counterintuitive. Conventional ap-\nfaces may appear in diverse orientations. Each participant proachesthattreatfacerecognitionthiswayoftenachievehigh\ncontributed around 30 images, resulting in a dataset of 300 accuracy in training and validation phases. However, these\nimages covering 10 distinct individuals. methods typically suffer from high false-positive rates in real-\nDue to the sensitive nature of this dataset (comprising world applications due to factors such as environmental noise\nunique facial features of each participant), only the dataset and limited data variation. Unlike these traditional models,\ngathering methodology will be publicized to safeguard the CLIP demonstrates robustness against false positives when\nprivacy of the individuals involved. deployed in real-world settings, likely due to its multimodal\nOnce the images were collected, we processed them using design and pretraining on diverse, real-world data.\nthe SCRFD [29] face detection model. This model enabled Learning from previous researches related to CLIP finetun-\nus to detect and crop out the face region from each image ing[30],[31],themodelwasthensingle-shottrainedusingthe\nFig. 2: A graph comparing traditional face recognition pipeline and our method using single-shot finetunned CLIP model.\nAdamWoptimizer[32]withaweightdecayregularizationpa- VRAM.Thisconfigurationoptimizedthemodel’sperformance\nrametersetto10−3.Wesetaninitiallearningrateof5×10−6, efficientlywhileleveragingthecomputationalpoweravailable.\nutilizingacosineannealingschedule[33]toadjustthelearning\nratedynamicallythroughouttraining.Cross-EntropyLosswith B. Deployment evaluation\nSoftmax function and mean reduction L(x,y) [34] across N\nmini-batch dimension was also widely employed as the loss To evaluate the false positive rate of our face recognition\nfunction: system in real-world scenarios, we conducted a deployment\ntest of the finetuned CLIP [4] model on the same device\nL(x,y)=\n(cid:80)N\nn=1l n, and camera setup used during the training and data extraction\nN phases. This consistency ensured that any environmental vari-\n(1)\nl\n=−(cid:88)C\nw log\nexp{x n,c}\ny\nables or device-specific nuances affecting model performance\nn c (cid:80)C\nexp{x }\nn,c would closely match those encountered during initial data\nc=1 i=1 n,i collection.\nwhere l is loss of each mini-batch, x is the input, y is the In this test, we instructed each of the 10 volunteer partici-\nn\ntarget, w is the weight, C is the total amount of class where pants included in the training dataset, as well as 2 additional\ni∈C and c is the mini-classes. participants who were not in the dataset, to stand individually\nThe training process was carried out for our dataset where in front of the camera for a duration of 5 seconds. Each\n80% of the images were selected with deterministic random participant took turns, ensuring no overlap in presence before\nseedingtoformthetrainingset,whiletheremaining20%were moving to the next individual. This process allowed us to\nallocated for testing. Finetunning and inference are performed observe the model’s real-time response to both known and\nwith a resized minibatch size of 16 224x224 pixels images unknown faces, assessing its ability to correctly identify or\non a single NVIDIA GeForce RTX 3090 GPU with 24GB of reject individuals.\nTrainingAccuracy(in DeploymentAccuracy FPR(in%,loweris FNR(in%,loweris\nModels\n%,higherisbetter) (in%,higherisbetter) better) better)\nVGG-Face(o)[10] 96.85 8.33 90.00 100.00\nVGG-Face(c)[10] 92.72 0.00 100.00 100.00\nArcface(o)[35] 95.26 16.67 90.00 50.00\nArcface(c)[35] 94.13 8.33 90.00 100.00\nCLIPRN-50(Ours)[4] 92.18 75.00 20.00 50.00\nTABLE I: Comparison of evaluation metrics among tested models, best performance are highlighted with bold. Models denoted\nwith ”o” is trained from scratch using original settings mentioned in their paper and ”c” is trained from scratch using normal\nimage classification settings.\nFollowing these observations, we perform evaluation the techniques or traditional feature extraction methods used in\nmodel’s deployment accuracy, false positive rate (FPR) and face recognition tasks.\nfalse negative rate (FNR) as: Although we observed a significant degradation in per-\nTP +TN formance across multiple variants of VGG-Face [10] and\nAccuracy = (2) ArcFace [35], this outcome was somewhat expected, as these\nTP +TN +FP +FN\nmodels were originally designed for large-scale facial feature\nFP\nFPR= (3) extraction. When applied to an image recognition scenario\nFP +TN\nor when only a limited set of facial features are used, their\nFN performance diminishes drastically, highlighting the models’\nFNR= (4)\nFN +TP dependency on detailed facial feature analysis for optimal\nwhere we abbreviated the values as: results.\n• TP: True positives The results highlight CLIP’s robustness and effectiveness\n• TN: True negatives in real-world deployment scenarios, making it a competitive\n• FP: False positives choiceforfacerecognitiontaskswithminimaltrainingadjust-\n• FN: False negatives ments.\nandallpredictionswithaconfidencevaluelowerthan80%are\nVI. DISCUSSION\nconsideredunrecognized(classifiedaseitherfalsenegativesor\ntrue negatives). A. Prompting choices\nV. RESULTS While there has been considerable research on prompting\noptimization for CLIP in various computer vision tasks, there\nIn addition to the finetuned CLIP [4] model, we employed\nis a noticeable lack of studies addressing this specifically\nseveral well-known face recognition models, including VGG-\nwithin the context of face recognition. As part of our experi-\nFace [10] and ArcFace [35], for comparison under multiple\nments,weexploredtheeffectofdifferentpromptformulations\nsettings. These settings included a traditional image recogni-\non the CLIP model’s performance in face recognition tasks.\ntionconfiguration(denoted”c”)andthespecificconfigurations\nIn addition to the standard prompt ”This is the image\noriginallydescribedineachrespectivepaper(denoted”o”),as\nof a person named ...”, we tested other variations, such as\ndetailed in Table I.\nsimply stating ”This is the image of ...” and directly feeding\nTo ensure consistency in training and comparison, all addi-\nthe name of the person to the text encoder without any\ntional models were trained using the Adam optimizer [36],\nwith a learning rate of lr = 1 × 10−3 and parameters additional context. Despite these variations, the fine-tuning\nresults exhibited only minimal fluctuations in performance,\nβ = (0.9,0.999). Training was conducted over 100 epochs\nwith a difference of approximately 1%. This suggests that, in\nwith identical batch sizes, resolution settings, and device\nthe context of face recognition, the choice of prompt and the\nconfigurations to CLIP [4] to maintain comparability across\ninclusion of additional context may not significantly impact\nmodels.\nthe model’s performance.\nTheresultsshowthat,whiletheCLIP[4]experiencedlower\nperformance during the training phase in comparison to the\nB. Distinctive features between faces\nVGG-Face [10] and ArcFace [35] variants, it demonstrated\nsignificantly improved performance during deployment infer- WhileCLIPachievesalowerFPRcomparedtothemajority\nence. Notably, the finetuned CLIP [4] achieved a markedly oftestedmodels,itsperformanceremainsfarfromperfect.As\nlower FPR and a reduced FNR compared to the other models. observed from the results, although both the FPR and FNR\nThis enhanced performance was achieved even though CLIP are reduced relative to other tested models, this level of per-\n[4] was fine-tuned with a single-shot approach as an image formance is still inadequate for security-sensitive applications\nrecognition model, without employing any advanced training of face recognition.\nDuring testing, we noticed a relatively large cosine simi- datasets and further optimizing its performance for diverse\nlarity between face features produced by CLIP’s image en- and dynamic real-world applications. Overall, this research\ncoder (approximately 80%). We believe this high similarity contributes to advancing face recognition technology, show-\ncontributes significantly to CLIP’s suboptimal performance casing CLIP’s potential for more efficient and accurate facial\nwhenappliedaspartofafacerecognitionpipeline.Toaddress authentication systems.\nthis limitation, we propose that future researches could focus\non improving the model by training with a triplet sampling\nREFERENCES\nmethod, as utilized in FaceNet [9], or by incorporating Arc- [1] X.Wang,J.Peng,S.Zhang,B.Chen,Y.Wang,andY.Guo,“Asurvey\nFace loss [35] during the training process. offacerecognition,”arXivpreprintarXiv:2212.13038,2022.\n[2] Y.Kortli,M.Jridi,A.AlFalou,andM.Atri,“Facerecognitionsystems:\nAsurvey,”Sensors,vol.20,no.2,p.342,2020.\nC. Training gradient\n[3] M.WangandW.Deng,“Deepfacerecognition:Asurvey,”Neurocom-\nDuring the hypothesis formulation and experiment design puting,vol.429,pp.215–244,2021.\n[4] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nphase, we initially planned to test on a significantly larger\nG.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable\nnumber of classes, typical of traditional large-scale face veri- visual models from natural language supervision,” in International\nficationproblemswherethenumberofclassesgreatlyexceeds conferenceonmachinelearning. PMLR,2021,pp.8748–8763.\n[5] M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,”\nthe number of images per class. This would have allowed\nin Proceedings. 1991 IEEE computer society conference on computer\nus to explore the scalability and robustness of CLIP in more vision and pattern recognition. IEEE Computer Society, 1991, pp.\ncomplex, real-world face recognition tasks. 586–587.\n[6] T.Ahonen,A.Hadid,andM.Pietikainen,“Facedescriptionwithlocal\nHowever, during testing, we encountered a significant lim-\nbinarypatterns:Applicationtofacerecognition,”IEEEtransactionson\nitation. We realized that to handle such a large number of pattern analysis and machine intelligence, vol. 28, no. 12, pp. 2037–\nclasses, a substantial amount of GPU memory would be 2041,2006.\n[7] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\nrequired, particularly because the gradients of CLIP’s text\nfisherfaces: Recognition using class specific linear projection,” IEEE\nencoder grow very large as the number of classes increases. Transactionsonpatternanalysisandmachineintelligence,vol.19,no.7,\nGiven our limited computing resources, we were unable to pp.711–720,1997.\n[8] Y.Taigman,M.Yang,M.Ranzato,andL.Wolf,“Deepface:Closingthe\nscale the problem as initially planned and were forced to\ngaptohuman-levelperformanceinfaceverification,”inProceedingsof\ndownscale the experiment to just 10 classes (equivalent to theIEEEconferenceoncomputervisionandpatternrecognition,2014,\n10 persons). This reduction in scale highlights a potential pp.1701–1708.\n[9] F.Schroff,D.Kalenichenko,andJ.Philbin,“Facenet:Aunifiedembed-\nchallengewhenattemptingtoapplyCLIPtolarge-scaleimage\nding for face recognition and clustering,” in Proceedings of the IEEE\ndatabases, as the computational demands may become pro- conferenceoncomputervisionandpatternrecognition,2015,pp.815–\nhibitive without access to high-performance hardware. This 823.\n[10] O. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”\nissue may need to be addressed in future work to enable the\nin BMVC 2015-Proceedings of the British Machine Vision Conference\nuse of CLIP in more expansive and computationally intensive 2015. BritishMachineVisionAssociation,2015.\nface recognition scenarios. [11] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW.Liu,“Cosface:Largemargincosinelossfordeepfacerecognition,”\nVII. CONCLUSION inProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition,2018,pp.5265–5274.\nIn this paper, we explored the application of Contrastive [12] Z. Zhou, Y. Lei, B. Zhang, L. Liu, and Y. Liu, “Zegclip: Towards\nLanguage-ImagePretraining(CLIP)forfacerecognition,high- adapting clip for zero-shot semantic segmentation,” in Proceedings of\ntheIEEE/CVFConferenceonComputerVisionandPatternRecognition,\nlighting its potential to address key challenges in the field.\n2023,pp.11175–11185.\nTraditionalfacerecognitionmodelstypicallyrelyonextracting [13] H.Wang,Y.Li,H.Yao,andX.Li,“Clipnforzero-shotooddetection:\nand matching detailed facial features, but they often struggle Teachingcliptosayno,”inProceedingsoftheIEEE/CVFInternational\nConferenceonComputerVision,2023,pp.1802–1812.\nwith high false-positive rates due to the inherent similarity\n[14] A.Sanghi,H.Chu,J.G.Lambourne,Y.Wang,C.-Y.Cheng,M.Fumero,\namong individuals’ facial structures. By leveraging CLIP’s and K. R. Malekshan, “Clip-forge: Towards zero-shot text-to-shape\nvision-language correspondence and employing a single-shot generation,”inProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition,2022,pp.18603–18613.\nfinetuning approach, we demonstrated that the model can\n[15] V. Vidit, M. Engilberge, and M. Salzmann, “Clip the gap: A single\nachieve significantly lower false-positive rates during deploy- domaingeneralizationapproachforobjectdetection,”inProceedingsof\nment while eliminating the need for complex facial feature theIEEE/CVFconferenceoncomputervisionandpatternrecognition,\n2023,pp.3219–3229.\nextraction techniques.\n[16] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, “Open-vocabulary object\nOurfindingsindicatethatCLIP,withitsabilitytogeneralize detectionviavisionandlanguageknowledgedistillation,”arXivpreprint\nacross modalities, offers a promising solution to persistent arXiv:2104.13921,2021.\n[17] A.Ramesh,P.Dhariwal,A.Nichol,C.Chu,andM.Chen,“Hierarchical\nissues in face recognition systems. The model’s robustness in\ntext-conditional image generation with clip latents,” arXiv preprint\nreal-world deployments, without the need for extensive train- arXiv:2204.06125,vol.1,no.2,p.3,2022.\ning data or traditional feature extraction methods, positions [18] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski,\n“Styleclip: Text-driven manipulation of stylegan imagery,” in Proceed-\nit as a powerful alternative to conventional face recognition\nings of the IEEE/CVF international conference on computer vision,\nmodels. Future work may focus on scaling CLIP to larger 2021,pp.2085–2094.\n[19] K.Zhou,J.Yang,C.C.Loy,andZ.Liu,“Learningtopromptforvision-\nlanguagemodels,”InternationalJournalofComputerVision,vol.130,\nno.9,pp.2337–2348,2022.\n[20] M.Tao,B.-K.Bao,H.Tang,andC.Xu,“Galip:Generativeadversarial\nclips for text-to-image synthesis,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n14214–14223.\n[21] Z. Wang, W. Liu, Q. He, X. Wu, and Z. Yi, “Clip-gen: Language-\nfree training of a text-to-image generator with clip,” arXiv preprint\narXiv:2203.00386,2022.\n[22] M. Barraco, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,\n“The unreasonable effectiveness of clip features for image captioning:\nanexperimentalanalysis,”inproceedingsoftheIEEE/CVFconference\noncomputervisionandpatternrecognition,2022,pp.4662–4670.\n[23] R. Mokady, A. Hertz, and A. H. Bermano, “Clipcap: Clip prefix for\nimagecaptioning,”arXivpreprintarXiv:2111.09734,2021.\n[24] N.T.Luu,C.Onuoha,andT.C.Thang,“Blindimagequalityassessment\nwithmultimodalpromptlearning,”in2023IEEE15thInternationalCon-\nference on Computational Intelligence and Communication Networks\n(CICN). IEEE,2023,pp.614–618.\n[25] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan,\n“Maple:Multi-modalpromptlearning,”inProceedingsoftheIEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n19113–19122.\n[26] K.Zhou,J.Yang,C.C.Loy,andZ.Liu,“Conditionalpromptlearning\nfor vision-language models,” in Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2022, pp. 16816–\n16825.\n[27] A.BhatandS.Jain,“Facerecognitionintheageofclip&billionimage\ndatasets,”arXivpreprintarXiv:2301.07315,2023.\n[28] S. Shen, W. Li, X. Wang, D. Zhang, Z. Jin, J. Zhou, and J. Lu,\n“Clip-cluster:Clip-guidedattributehallucinationforfaceclustering,”in\nProceedings of the IEEE/CVF International Conference on Computer\nVision,2023,pp.20786–20795.\n[29] J. Guo, J. Deng, A. Lattas, and S. Zafeiriou, “Sample and com-\nputation redistribution for efficient face detection,” arXiv preprint\narXiv:2105.04714,2021.\n[30] J.Wang,K.C.Chan,andC.C.Loy,“Exploringclipforassessingthe\nlook and feel of images,” in Proceedings of the AAAI Conference on\nArtificialIntelligence,vol.37,no.2,2023,pp.2555–2563.\n[31] W.Zhang,G.Zhai,Y.Wei,X.Yang,andK.Ma,“Blindimagequality\nassessment via vision-language correspondence: A multitask learning\nperspective,”inProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,2023,pp.14071–14081.\n[32] I.Loshchilov,“Decoupledweightdecayregularization,”arXivpreprint\narXiv:1711.05101,2017.\n[33] I. Loshchilov and F. Hutter, “Stochastic gradient descent with warm\nrestarts,”inProceedingsofthe5thInternationalConferenceonLearning\nRepresentations,pp.1–16.\n[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneuralinformationprocessingsystems,vol.32,2019.\n[35] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular\nmarginlossfordeepfacerecognition,”inProceedingsoftheIEEE/CVF\nconferenceoncomputervisionandpatternrecognition,2019,pp.4690–\n4699.\n[36] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,”\narXivpreprintarXiv:1412.6980,2014.",
    "pdf_filename": "CLIP_Unreasonable_Potential_in_Single-Shot_Face_Recognition.pdf"
}