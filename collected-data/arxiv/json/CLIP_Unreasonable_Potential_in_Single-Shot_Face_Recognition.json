{
    "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
    "abstract": "sion, designed to identify and authenticate individuals by an- alyzing facial patterns and features. This field intersects with artificial intelligence, image processing, and machine learning, with applications in security, authentication, and personalization. Traditional approaches in facial recognition focus on capturing facial features like the eyes, nose, and mouth and matching these against a database to verify identities. However, chal- lenges such as high false-positive rates have persisted, often due to the similarity among individuals’ facial features. Re- cently, Contrastive Language-Image Pretraining (CLIP), a model developed by OpenAI, has shown promising advancements by linking natural language processing with vision tasks, allowing it to generalize across modalities. Using CLIP’s vision-language correspondence and single-shot finetuning, the model can achieve lower false-positive rates upon deployment without the need of mass facial features extraction. This integration demonstrating CLIP’s potential to address persistent issues in face recognition model performance without complicating our training paradigm. Index Terms—Face recognition, image classification, multi- modal learning I. INTRODUCTION Face recognition [1]–[3] is a pivotal task within the realm of computer vision, wherein algorithms are designed to iden- tify and authenticate individuals by analyzing and comparing patterns in facial features. It’s a multifaceted field that inter- sects with various disciplines like artificial intelligence, image processing and machine learning. At its core, facial recognition involves capturing facial images and videos, extracting unique characteristics such as the arrangement of eyes, nose, and mouth, and then matching these features against a database of known faces to make identifications or verifications. This technology has found widespread applications across diverse sectors, ranging from security and surveillance to authentication and personalization. CLIP [4], or Contrastive Language-Image Pretraining, is a ground breaking model developed by OpenAI that transcends traditional boundaries between natural language processing and computer vision. Unlike conventional models that special- ize in either text or image modeling, CLIP learns to understand both modalities simultaneously. This means it is possible that CLIP can perform a wide range of tasks across different domains, including those related to facial recognition. One of the big problem distinct facial recognition with other classification task is subjectivity to false-positive result, misunderstanding someone face with others. Upon testing, we realize that by using the vision-language correspondence from CLIP features, model only require a single shot finetuning while reducing false-positive results significantly without any state-of-the-art methods for extracting facial features from massive datasets. II. BACKGROUND A. Face recognition Face recognition has emerged as a powerful tool in computer vision, enabling the identification or verification of individuals based on their facial features. This technology has applica- tions in various fields such as security, law enforcement, and personal device authentication. Early approaches focused on geometrical models and eigenface techniques [5]. With the advent of machine learning, methods such as Local Binary Patterns (LBP) [6] and Fisherfaces [7] improved robustness under various lighting conditions and facial expressions. The rise of deep learning has further revolutionized the field, with convolutional neural networks (CNNs) becoming the backbone of face recognition systems. Groundbreaking models like DeepFace [8] and FaceNet [9] introduced face embeddings that provided remarkable accuracy and efficiency, even in large-scale applications. More recent advancements leverage deep residual networks and novel loss functions to achieve even higher accuracy [10], [11]. These advancements underscore the rapid development of face recognition and its growing importance across technology sectors. B. CLIP and its applications CLIP [4] model has made significant strides in connecting vision and language modalities, enabling zero-shot capabilities across various computer vision tasks [12]–[14]. CLIP [4] lever- ages a contrastive learning framework, aligning text descrip- tions and images in a shared embedding space. This alignment allows CLIP [4] to generalize across tasks without the need for task-specific training. CLIP [4] has shown versatility in object detection [15], [16], image generation [17], [18], and scene understanding [19]. arXiv:2411.12319v1  [cs.CV]  19 Nov 2024",
    "body": "CLIP Unreasonable Potential in Single-Shot Face\nRecognition\nNhan T. Luu\nDepartment of Computer Science and Engineering, The University of Aizu,\nAizuwakamatsu, Japan\nltnhan0902@gmail.com\nAbstract—Face recognition is a core task in computer vi-\nsion, designed to identify and authenticate individuals by an-\nalyzing facial patterns and features. This field intersects with\nartificial intelligence, image processing, and machine learning,\nwith applications in security, authentication, and personalization.\nTraditional approaches in facial recognition focus on capturing\nfacial features like the eyes, nose, and mouth and matching\nthese against a database to verify identities. However, chal-\nlenges such as high false-positive rates have persisted, often\ndue to the similarity among individuals’ facial features. Re-\ncently, Contrastive Language-Image Pretraining (CLIP), a model\ndeveloped by OpenAI, has shown promising advancements by\nlinking natural language processing with vision tasks, allowing\nit to generalize across modalities. Using CLIP’s vision-language\ncorrespondence and single-shot finetuning, the model can achieve\nlower false-positive rates upon deployment without the need of\nmass facial features extraction. This integration demonstrating\nCLIP’s potential to address persistent issues in face recognition\nmodel performance without complicating our training paradigm.\nIndex Terms—Face recognition, image classification, multi-\nmodal learning\nI. INTRODUCTION\nFace recognition [1]–[3] is a pivotal task within the realm\nof computer vision, wherein algorithms are designed to iden-\ntify and authenticate individuals by analyzing and comparing\npatterns in facial features. It’s a multifaceted field that inter-\nsects with various disciplines like artificial intelligence, image\nprocessing and machine learning.\nAt its core, facial recognition involves capturing facial\nimages and videos, extracting unique characteristics such as\nthe arrangement of eyes, nose, and mouth, and then matching\nthese features against a database of known faces to make\nidentifications or verifications. This technology has found\nwidespread applications across diverse sectors, ranging from\nsecurity and surveillance to authentication and personalization.\nCLIP [4], or Contrastive Language-Image Pretraining, is a\nground breaking model developed by OpenAI that transcends\ntraditional boundaries between natural language processing\nand computer vision. Unlike conventional models that special-\nize in either text or image modeling, CLIP learns to understand\nboth modalities simultaneously. This means it is possible that\nCLIP can perform a wide range of tasks across different\ndomains, including those related to facial recognition.\nOne of the big problem distinct facial recognition with\nother classification task is subjectivity to false-positive result,\nmisunderstanding someone face with others. Upon testing, we\nrealize that by using the vision-language correspondence from\nCLIP features, model only require a single shot finetuning\nwhile reducing false-positive results significantly without any\nstate-of-the-art methods for extracting facial features from\nmassive datasets.\nII. BACKGROUND\nA. Face recognition\nFace recognition has emerged as a powerful tool in computer\nvision, enabling the identification or verification of individuals\nbased on their facial features. This technology has applica-\ntions in various fields such as security, law enforcement, and\npersonal device authentication. Early approaches focused on\ngeometrical models and eigenface techniques [5]. With the\nadvent of machine learning, methods such as Local Binary\nPatterns (LBP) [6] and Fisherfaces [7] improved robustness\nunder various lighting conditions and facial expressions.\nThe rise of deep learning has further revolutionized the\nfield, with convolutional neural networks (CNNs) becoming\nthe backbone of face recognition systems. Groundbreaking\nmodels like DeepFace [8] and FaceNet [9] introduced face\nembeddings that provided remarkable accuracy and efficiency,\neven in large-scale applications. More recent advancements\nleverage deep residual networks and novel loss functions to\nachieve even higher accuracy [10], [11]. These advancements\nunderscore the rapid development of face recognition and its\ngrowing importance across technology sectors.\nB. CLIP and its applications\nCLIP [4] model has made significant strides in connecting\nvision and language modalities, enabling zero-shot capabilities\nacross various computer vision tasks [12]–[14]. CLIP [4] lever-\nages a contrastive learning framework, aligning text descrip-\ntions and images in a shared embedding space. This alignment\nallows CLIP [4] to generalize across tasks without the need\nfor task-specific training. CLIP [4] has shown versatility in\nobject detection [15], [16], image generation [17], [18], and\nscene understanding [19].\narXiv:2411.12319v1  [cs.CV]  19 Nov 2024\n\nFig. 1: A diagram illustrate our dataset acquisition and preprocessing method.\nCLIP [4] has also been used to improve the quality and rel-\nevance of images produced by decoder-like generative models.\nIts capability to evaluate semantic similarity has enabled inte-\ngration with autoregressive model and diffusion model such as\nunCLIP [17] as well as GAN in the case of StyleCLIP [18],\nwhere CLIP-guided latent manipulation achieves fine-grained\ncontrol over generated images. Recent research has shown that\nCLIP-guided methods achieve better alignment with desired\ntextual descriptions, as seen in text-to-image generation [17],\n[20], [21] and image captioning [22], [23] tasks. Aside from\nthat, there are also some researches divert into improving CLIP\nperformance by alternating its architecture, such as MaPLe\n[24], [25], CoOp [26] and Co-CoOp [19].\nIn face recognition, CLIP’s multimodal capabilities provide\nunique advantages. While traditional face recognition relies\non supervised learning with large labeled datasets [9], [11],\nrecent studies have explored using CLIP to develop face\nrecognition systems by leveraging its generalized embeddings\n[27], [28]. Still, CLIP face recognition potentials are still\nremain underexplored.\nIII. DATA ACQUISITION AND PROCESSING\nIn this research, we used an image dataset consists of\nhigh resolution images of volunteers, contains faces sorted\nby individual names, ensuring a clear and organized structure\nfor face recognition experiments. Images were captured using\na 3 megapixels camera within a 2 meters range to obtain\nvarious perspectives of each participant, including views from\nabove, below, left, right, and the front (as shown in Figure\n1). These multiple angles simulate real-world scenarios where\nfaces may appear in diverse orientations. Each participant\ncontributed around 30 images, resulting in a dataset of 300\nimages covering 10 distinct individuals.\nDue to the sensitive nature of this dataset (comprising\nunique facial features of each participant), only the dataset\ngathering methodology will be publicized to safeguard the\nprivacy of the individuals involved.\nOnce the images were collected, we processed them using\nthe SCRFD [29] face detection model. This model enabled\nus to detect and crop out the face region from each image\naccurately. For further alignment and standardization, we used\nthe model to extract essential facial keypoints, specifically\nthe positions of the eyes, nose, and mouth. These keypoints\nfacilitated the vertical alignment of faces across different\nimages, ensuring consistency in the data representation. The\ncropped and aligned face images were then quality assured\nmanually by a person before being saved into a separate\nfolder structure, labeled with each participant’s name, allowing\norganized and efficient access for subsequent experimentation\nand analysis.\nIV. EXPERIMENT DESIGN\nA. Model training\nAfter completing the dataset creation and preprocessing\nstages, we designed a face recognition pipeline that diverges\nfrom traditional approaches by replacing the standard facial\nfeature encoder and classifier with CLIP [4] model, specifically\nthe CLIP-RN50 variant (as shown in Figure 2).\nIn this setup, we treated face recognition as an image\nclassification task, using a single-shot finetuning method on the\nprocessed images of participants. Instead of tuning the entire\nmodel, we froze the image encoder parameters to leverage\nCLIP’s pretrained vision features and performed backpropa-\ngation only on the text encoder. Each image was associated\nwith a text prompt formatted as ”This is the image of a person\nnamed ...” which allows the CLIP model to align specific\nparticipant identities with their visual features.\nInitially, approaching face recognition as an image classifi-\ncation problem may seem counterintuitive. Conventional ap-\nproaches that treat face recognition this way often achieve high\naccuracy in training and validation phases. However, these\nmethods typically suffer from high false-positive rates in real-\nworld applications due to factors such as environmental noise\nand limited data variation. Unlike these traditional models,\nCLIP demonstrates robustness against false positives when\ndeployed in real-world settings, likely due to its multimodal\ndesign and pretraining on diverse, real-world data.\nLearning from previous researches related to CLIP finetun-\ning [30], [31], the model was then single-shot trained using the\n\nFig. 2: A graph comparing traditional face recognition pipeline and our method using single-shot finetunned CLIP model.\nAdamW optimizer [32] with a weight decay regularization pa-\nrameter set to 10−3. We set an initial learning rate of 5×10−6,\nutilizing a cosine annealing schedule [33] to adjust the learning\nrate dynamically throughout training. Cross-Entropy Loss with\nSoftmax function and mean reduction L(x, y) [34] across N\nmini-batch dimension was also widely employed as the loss\nfunction:\nL(x, y) =\nPN\nn=1 ln\nN\n,\nln = −\nC\nX\nc=1\nwc log\nexp{xn,c}\nPC\ni=1 exp{xn,i}\nyn,c\n(1)\nwhere ln is loss of each mini-batch, x is the input, y is the\ntarget, w is the weight, C is the total amount of class where\ni ∈C and c is the mini-classes.\nThe training process was carried out for our dataset where\n80% of the images were selected with deterministic random\nseeding to form the training set, while the remaining 20% were\nallocated for testing. Finetunning and inference are performed\nwith a resized minibatch size of 16 224x224 pixels images\non a single NVIDIA GeForce RTX 3090 GPU with 24GB of\nVRAM. This configuration optimized the model’s performance\nefficiently while leveraging the computational power available.\nB. Deployment evaluation\nTo evaluate the false positive rate of our face recognition\nsystem in real-world scenarios, we conducted a deployment\ntest of the finetuned CLIP [4] model on the same device\nand camera setup used during the training and data extraction\nphases. This consistency ensured that any environmental vari-\nables or device-specific nuances affecting model performance\nwould closely match those encountered during initial data\ncollection.\nIn this test, we instructed each of the 10 volunteer partici-\npants included in the training dataset, as well as 2 additional\nparticipants who were not in the dataset, to stand individually\nin front of the camera for a duration of 5 seconds. Each\nparticipant took turns, ensuring no overlap in presence before\nmoving to the next individual. This process allowed us to\nobserve the model’s real-time response to both known and\nunknown faces, assessing its ability to correctly identify or\nreject individuals.\n\nModels\nTraining Accuracy (in\n%, higher is better)\nDeployment Accuracy\n(in %, higher is better)\nFPR (in %, lower is\nbetter)\nFNR (in %, lower is\nbetter)\nVGG-Face (o) [10]\n96.85\n8.33\n90.00\n100.00\nVGG-Face (c) [10]\n92.72\n0.00\n100.00\n100.00\nArcface (o) [35]\n95.26\n16.67\n90.00\n50.00\nArcface (c) [35]\n94.13\n8.33\n90.00\n100.00\nCLIP RN-50 (Ours) [4]\n92.18\n75.00\n20.00\n50.00\nTABLE I: Comparison of evaluation metrics among tested models, best performance are highlighted with bold. Models denoted\nwith ”o” is trained from scratch using original settings mentioned in their paper and ”c” is trained from scratch using normal\nimage classification settings.\nFollowing these observations, we perform evaluation the\nmodel’s deployment accuracy, false positive rate (FPR) and\nfalse negative rate (FNR) as:\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(2)\nFPR =\nFP\nFP + TN\n(3)\nFNR =\nFN\nFN + TP\n(4)\nwhere we abbreviated the values as:\n• TP: True positives\n• TN: True negatives\n• FP: False positives\n• FN: False negatives\nand all predictions with a confidence value lower than 80% are\nconsidered unrecognized (classified as either false negatives or\ntrue negatives).\nV. RESULTS\nIn addition to the finetuned CLIP [4] model, we employed\nseveral well-known face recognition models, including VGG-\nFace [10] and ArcFace [35], for comparison under multiple\nsettings. These settings included a traditional image recogni-\ntion configuration (denoted ”c”) and the specific configurations\noriginally described in each respective paper (denoted ”o”), as\ndetailed in Table I.\nTo ensure consistency in training and comparison, all addi-\ntional models were trained using the Adam optimizer [36],\nwith a learning rate of lr = 1 × 10−3 and parameters\nβ = (0.9, 0.999). Training was conducted over 100 epochs\nwith identical batch sizes, resolution settings, and device\nconfigurations to CLIP [4] to maintain comparability across\nmodels.\nThe results show that, while the CLIP [4] experienced lower\nperformance during the training phase in comparison to the\nVGG-Face [10] and ArcFace [35] variants, it demonstrated\nsignificantly improved performance during deployment infer-\nence. Notably, the finetuned CLIP [4] achieved a markedly\nlower FPR and a reduced FNR compared to the other models.\nThis enhanced performance was achieved even though CLIP\n[4] was fine-tuned with a single-shot approach as an image\nrecognition model, without employing any advanced training\ntechniques or traditional feature extraction methods used in\nface recognition tasks.\nAlthough we observed a significant degradation in per-\nformance across multiple variants of VGG-Face [10] and\nArcFace [35], this outcome was somewhat expected, as these\nmodels were originally designed for large-scale facial feature\nextraction. When applied to an image recognition scenario\nor when only a limited set of facial features are used, their\nperformance diminishes drastically, highlighting the models’\ndependency on detailed facial feature analysis for optimal\nresults.\nThe results highlight CLIP’s robustness and effectiveness\nin real-world deployment scenarios, making it a competitive\nchoice for face recognition tasks with minimal training adjust-\nments.\nVI. DISCUSSION\nA. Prompting choices\nWhile there has been considerable research on prompting\noptimization for CLIP in various computer vision tasks, there\nis a noticeable lack of studies addressing this specifically\nwithin the context of face recognition. As part of our experi-\nments, we explored the effect of different prompt formulations\non the CLIP model’s performance in face recognition tasks.\nIn addition to the standard prompt ”This is the image\nof a person named ...”, we tested other variations, such as\nsimply stating ”This is the image of ...” and directly feeding\nthe name of the person to the text encoder without any\nadditional context. Despite these variations, the fine-tuning\nresults exhibited only minimal fluctuations in performance,\nwith a difference of approximately 1%. This suggests that, in\nthe context of face recognition, the choice of prompt and the\ninclusion of additional context may not significantly impact\nthe model’s performance.\nB. Distinctive features between faces\nWhile CLIP achieves a lower FPR compared to the majority\nof tested models, its performance remains far from perfect. As\nobserved from the results, although both the FPR and FNR\nare reduced relative to other tested models, this level of per-\nformance is still inadequate for security-sensitive applications\nof face recognition.\n\nDuring testing, we noticed a relatively large cosine simi-\nlarity between face features produced by CLIP’s image en-\ncoder (approximately 80%). We believe this high similarity\ncontributes significantly to CLIP’s suboptimal performance\nwhen applied as part of a face recognition pipeline. To address\nthis limitation, we propose that future researches could focus\non improving the model by training with a triplet sampling\nmethod, as utilized in FaceNet [9], or by incorporating Arc-\nFace loss [35] during the training process.\nC. Training gradient\nDuring the hypothesis formulation and experiment design\nphase, we initially planned to test on a significantly larger\nnumber of classes, typical of traditional large-scale face veri-\nfication problems where the number of classes greatly exceeds\nthe number of images per class. This would have allowed\nus to explore the scalability and robustness of CLIP in more\ncomplex, real-world face recognition tasks.\nHowever, during testing, we encountered a significant lim-\nitation. We realized that to handle such a large number of\nclasses, a substantial amount of GPU memory would be\nrequired, particularly because the gradients of CLIP’s text\nencoder grow very large as the number of classes increases.\nGiven our limited computing resources, we were unable to\nscale the problem as initially planned and were forced to\ndownscale the experiment to just 10 classes (equivalent to\n10 persons). This reduction in scale highlights a potential\nchallenge when attempting to apply CLIP to large-scale image\ndatabases, as the computational demands may become pro-\nhibitive without access to high-performance hardware. This\nissue may need to be addressed in future work to enable the\nuse of CLIP in more expansive and computationally intensive\nface recognition scenarios.\nVII. CONCLUSION\nIn this paper, we explored the application of Contrastive\nLanguage-Image Pretraining (CLIP) for face recognition, high-\nlighting its potential to address key challenges in the field.\nTraditional face recognition models typically rely on extracting\nand matching detailed facial features, but they often struggle\nwith high false-positive rates due to the inherent similarity\namong individuals’ facial structures. By leveraging CLIP’s\nvision-language correspondence and employing a single-shot\nfinetuning approach, we demonstrated that the model can\nachieve significantly lower false-positive rates during deploy-\nment while eliminating the need for complex facial feature\nextraction techniques.\nOur findings indicate that CLIP, with its ability to generalize\nacross modalities, offers a promising solution to persistent\nissues in face recognition systems. The model’s robustness in\nreal-world deployments, without the need for extensive train-\ning data or traditional feature extraction methods, positions\nit as a powerful alternative to conventional face recognition\nmodels. Future work may focus on scaling CLIP to larger\ndatasets and further optimizing its performance for diverse\nand dynamic real-world applications. Overall, this research\ncontributes to advancing face recognition technology, show-\ncasing CLIP’s potential for more efficient and accurate facial\nauthentication systems.\nREFERENCES\n[1] X. Wang, J. Peng, S. Zhang, B. Chen, Y. Wang, and Y. Guo, “A survey\nof face recognition,” arXiv preprint arXiv:2212.13038, 2022.\n[2] Y. Kortli, M. Jridi, A. Al Falou, and M. Atri, “Face recognition systems:\nA survey,” Sensors, vol. 20, no. 2, p. 342, 2020.\n[3] M. Wang and W. Deng, “Deep face recognition: A survey,” Neurocom-\nputing, vol. 429, pp. 215–244, 2021.\n[4] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning.\nPMLR, 2021, pp. 8748–8763.\n[5] M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,”\nin Proceedings. 1991 IEEE computer society conference on computer\nvision and pattern recognition.\nIEEE Computer Society, 1991, pp.\n586–587.\n[6] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local\nbinary patterns: Application to face recognition,” IEEE transactions on\npattern analysis and machine intelligence, vol. 28, no. 12, pp. 2037–\n2041, 2006.\n[7] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\nfisherfaces: Recognition using class specific linear projection,” IEEE\nTransactions on pattern analysis and machine intelligence, vol. 19, no. 7,\npp. 711–720, 1997.\n[8] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing the\ngap to human-level performance in face verification,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014,\npp. 1701–1708.\n[9] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embed-\nding for face recognition and clustering,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp. 815–\n823.\n[10] O. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”\nin BMVC 2015-Proceedings of the British Machine Vision Conference\n2015.\nBritish Machine Vision Association, 2015.\n[11] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW. Liu, “Cosface: Large margin cosine loss for deep face recognition,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 5265–5274.\n[12] Z. Zhou, Y. Lei, B. Zhang, L. Liu, and Y. Liu, “Zegclip: Towards\nadapting clip for zero-shot semantic segmentation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 11 175–11 185.\n[13] H. Wang, Y. Li, H. Yao, and X. Li, “Clipn for zero-shot ood detection:\nTeaching clip to say no,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2023, pp. 1802–1812.\n[14] A. Sanghi, H. Chu, J. G. Lambourne, Y. Wang, C.-Y. Cheng, M. Fumero,\nand K. R. Malekshan, “Clip-forge: Towards zero-shot text-to-shape\ngeneration,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 18 603–18 613.\n[15] V. Vidit, M. Engilberge, and M. Salzmann, “Clip the gap: A single\ndomain generalization approach for object detection,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2023, pp. 3219–3229.\n[16] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, “Open-vocabulary object\ndetection via vision and language knowledge distillation,” arXiv preprint\narXiv:2104.13921, 2021.\n[17] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical\ntext-conditional image generation with clip latents,” arXiv preprint\narXiv:2204.06125, vol. 1, no. 2, p. 3, 2022.\n[18] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski,\n“Styleclip: Text-driven manipulation of stylegan imagery,” in Proceed-\nings of the IEEE/CVF international conference on computer vision,\n2021, pp. 2085–2094.\n\n[19] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for vision-\nlanguage models,” International Journal of Computer Vision, vol. 130,\nno. 9, pp. 2337–2348, 2022.\n[20] M. Tao, B.-K. Bao, H. Tang, and C. Xu, “Galip: Generative adversarial\nclips for text-to-image synthesis,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n14 214–14 223.\n[21] Z. Wang, W. Liu, Q. He, X. Wu, and Z. Yi, “Clip-gen: Language-\nfree training of a text-to-image generator with clip,” arXiv preprint\narXiv:2203.00386, 2022.\n[22] M. Barraco, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,\n“The unreasonable effectiveness of clip features for image captioning:\nan experimental analysis,” in proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2022, pp. 4662–4670.\n[23] R. Mokady, A. Hertz, and A. H. Bermano, “Clipcap: Clip prefix for\nimage captioning,” arXiv preprint arXiv:2111.09734, 2021.\n[24] N. T. Luu, C. Onuoha, and T. C. Thang, “Blind image quality assessment\nwith multimodal prompt learning,” in 2023 IEEE 15th International Con-\nference on Computational Intelligence and Communication Networks\n(CICN).\nIEEE, 2023, pp. 614–618.\n[25] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan,\n“Maple: Multi-modal prompt learning,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n19 113–19 122.\n[26] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional prompt learning\nfor vision-language models,” in Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2022, pp. 16 816–\n16 825.\n[27] A. Bhat and S. Jain, “Face recognition in the age of clip & billion image\ndatasets,” arXiv preprint arXiv:2301.07315, 2023.\n[28] S. Shen, W. Li, X. Wang, D. Zhang, Z. Jin, J. Zhou, and J. Lu,\n“Clip-cluster: Clip-guided attribute hallucination for face clustering,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 20 786–20 795.\n[29] J. Guo, J. Deng, A. Lattas, and S. Zafeiriou, “Sample and com-\nputation redistribution for efficient face detection,” arXiv preprint\narXiv:2105.04714, 2021.\n[30] J. Wang, K. C. Chan, and C. C. Loy, “Exploring clip for assessing the\nlook and feel of images,” in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 37, no. 2, 2023, pp. 2555–2563.\n[31] W. Zhang, G. Zhai, Y. Wei, X. Yang, and K. Ma, “Blind image quality\nassessment via vision-language correspondence: A multitask learning\nperspective,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2023, pp. 14 071–14 081.\n[32] I. Loshchilov, “Decoupled weight decay regularization,” arXiv preprint\narXiv:1711.05101, 2017.\n[33] I. Loshchilov and F. Hutter, “Stochastic gradient descent with warm\nrestarts,” in Proceedings of the 5th International Conference on Learning\nRepresentations, pp. 1–16.\n[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneural information processing systems, vol. 32, 2019.\n[35] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular\nmargin loss for deep face recognition,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2019, pp. 4690–\n4699.\n[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.",
    "pdf_filename": "CLIP_Unreasonable_Potential_in_Single-Shot_Face_Recognition.pdf"
}