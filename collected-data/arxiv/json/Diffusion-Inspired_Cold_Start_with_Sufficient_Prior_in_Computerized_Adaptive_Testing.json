{
    "title": "Diffusion-Inspired Cold Start with Sufficient Prior in",
    "abstract": "ComputerizedAdaptiveTesting(CAT)aimstoselectthemostap- ables,therebyblockingbackdoorpathsthathindercausaldiscov- propriatequestionsbasedontheexaminee‚Äôsabilityandiswidely ery.Giventhatexcessiveuncertaintycanaffecttheapplicability usedinonlineeducation.However,existingCATsystemsoften ofgeneratedresultstotheCATsystem,weproposeconsistency lackinitialunderstandingoftheexaminee‚Äôsability,requiringran- constraintandtask-orientedconstrainttocontroltherandomness dom probing questions. This can lead to poorly matched ques- ofthegeneratedresultsandtheirrelevancetotheCATtask,re- tions,extendingthetestdurationandnegativelyimpactingthe spectively.OurDCSRcanseamlesslyapplythegeneratedinitial examinee‚Äôsmindset,aphenomenonreferredtoastheColdStart abilitystatesinthetargetdomaintoexistingquestionselectional- withInsufficientPrior(CSIP)task.ThisissueoccursbecauseCAT gorithms,thusimprovingthecoldstartperformanceoftheCATsys- systemsdonoteffectivelyutilizetheabundantpriorinformation tem.Extensiveexperimentsconductedonfivereal-worlddatasets abouttheexamineeavailablefromothercoursesononlineplat- demonstratethatDCSRsignificantlyoutperformsexistingbase- forms.Theseresponserecords,duetothecommonalityofcogni- linemethodsinaddressingtheCSIPtask.Thecodeisavailableat: tivestatesacrossdifferentknowledgedomains,canprovidevalu- https://github.com/BIMK/Intelligent-Education/tree/main/DCSR. ablepriorinformationforthetargetdomain.However,noprior CCSConcepts work has explored solutions for the CSIP task. In response to thisgap,weproposeDiffusionCognitiveStatesTransfeRFrame- ‚Ä¢Appliedcomputing‚ÜíComputer-assistedinstruction. work(DCSR),anoveldomaintransferframeworkbasedonDiffu-",
    "body": "Diffusion-Inspired Cold Start with Sufficient Prior in\nComputerized Adaptive Testing\nHaipingMa AoqingXia ChangqianWang\nInformationMaterialsandIntelligent TheInstitutesofPhysicalScienceand TheInstitutesofPhysicalScienceand\nSensingLaboratoryofAnhui InformationTechnology,Anhui InformationTechnology,Anhui\nProvince,theInstitutesofPhysical University University\nScienceandInformationTechnology, Hefei,Anhui,China Hefei,Anhui,China\nAnhuiUniversity q23301252@stu.ahu.edu.cn changqian.wang.dl@gmail.com\nHefei,Anhui,China\nhpma@ahu.edu.cn\nHaiWang XingyiZhang‚àó\nTheInstitutesofPhysicalScienceand SchoolofComputerScienceand\nInformationTechnology,Anhui Technology,AnhuiUniversity\nUniversity Hefei,Anhui,China\nHefei,Anhui,China xyzhanghust@gmail.com\nq22201135@stu.ahu.edu.cn\nAbstract designedthreedecouplingstrategiestocontrolconfoundingvari-\nComputerizedAdaptiveTesting(CAT)aimstoselectthemostap- ables,therebyblockingbackdoorpathsthathindercausaldiscov-\npropriatequestionsbasedontheexaminee‚Äôsabilityandiswidely ery.Giventhatexcessiveuncertaintycanaffecttheapplicability\nusedinonlineeducation.However,existingCATsystemsoften ofgeneratedresultstotheCATsystem,weproposeconsistency\nlackinitialunderstandingoftheexaminee‚Äôsability,requiringran- constraintandtask-orientedconstrainttocontroltherandomness\ndom probing questions. This can lead to poorly matched ques- ofthegeneratedresultsandtheirrelevancetotheCATtask,re-\ntions,extendingthetestdurationandnegativelyimpactingthe spectively.OurDCSRcanseamlesslyapplythegeneratedinitial\nexaminee‚Äôsmindset,aphenomenonreferredtoastheColdStart abilitystatesinthetargetdomaintoexistingquestionselectional-\nwithInsufficientPrior(CSIP)task.ThisissueoccursbecauseCAT gorithms,thusimprovingthecoldstartperformanceoftheCATsys-\nsystemsdonoteffectivelyutilizetheabundantpriorinformation tem.Extensiveexperimentsconductedonfivereal-worlddatasets\nabouttheexamineeavailablefromothercoursesononlineplat- demonstratethatDCSRsignificantlyoutperformsexistingbase-\nforms.Theseresponserecords,duetothecommonalityofcogni- linemethodsinaddressingtheCSIPtask.Thecodeisavailableat:\ntivestatesacrossdifferentknowledgedomains,canprovidevalu- https://github.com/BIMK/Intelligent-Education/tree/main/DCSR.\nablepriorinformationforthetargetdomain.However,noprior\nCCSConcepts\nwork has explored solutions for the CSIP task. In response to\nthisgap,weproposeDiffusionCognitiveStatesTransfeRFrame- ‚Ä¢Appliedcomputing‚ÜíComputer-assistedinstruction.\nwork(DCSR),anoveldomaintransferframeworkbasedonDiffu-\nsionModels(DMs)toaddresstheCSIPtask.Specifically,wecon- Keywords\nstructacognitivestatetransitionbridgebetweendomains,guided ComputerizedAdaptiveTesting,IntellegentEducation\nbythecommoncognitivestatesofexaminees,encouragingthe\nACMReferenceFormat:\nmodeltoreconstructtheinitialabilitystateinthetargetdomain.\nHaipingMa,AoqingXia,ChangqianWang,HaiWang,andXingyiZhang.\nToenrichtheexpressivepowerofthegenerateddata,weanalyze\n2025.Diffusion-InspiredColdStartwithSufficientPriorinComputerized\nthecausalrelationshipsinthegenerationprocessfromacausal\nAdaptiveTesting.InProceedingsofMakesuretoenterthecorrectconference\nperspective.Redundantandextraneouscognitivestatescanlead\ntitlefromyourrightsconfirmationemai(KDD‚Äô25).ACM,NewYork,NY,\nto limited transfer and negative transfer effects. Therefore, we USA,10pages.https://doi.org/XXXXXXX.XXXXXXX\n‚àóCorrespondingauthor.\n1 Introduction\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nAsartificialintelligenceempowereducation,computerizedadap-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation tivetesting(CAT)ononlineeducationplatformshavegarnered\nonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe extensiveattention[38,39,42,45].CATaimstoprovideexaminees\nauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or\nwithasmallnumberofappropriatequestionstoprogressivelyas-\nrepublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/orafee.Requestpermissionsfrompermissions@acm.org. sesstheircognitivestatesinspecificdomains[19,29,30].Typically,\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada CATconsistsoftwoiterativecomponents:thecognitivediagnostic\n¬©2025Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\nmodel(CDM)andthequestionselectionalgorithm.Asshownin\nACMISBN978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX Figure1(a),theCDMestimatesexaminee‚Äôsabilitybasedonher\n4202\nvoN\n91\n]GL.sc[\n1v28121.1142:viXra\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada HaipingMaetal.\ntransferableattributes.Thesepre-diagnosedabilitiescanprovide\nrichpriorinformationforthetarget(coldstart)course,allowingthe\nCATsystemtograsptheexaminee‚Äôsabilityrangeinadvance.For\nexample,ifanexamineeperformswellinmathematics,especiallyin\ncomplexalgebraandgeometryquestions,theCATsystemcaninfer\nthathealsopossessstronglogicalthinkingandproblem-solving\nskillsinphysics,therebyassigningahigherinitialability.Toour\nknowledge,noresearchhasyetutilizedthesecross-coursedatato\nsolvetheCSIPtask,despiteitssignificantandpracticalimportance.\nTofullyleveragethesecross-courseresponserecordsandprovide\npriorinformationaboutexamineestoCATsystem,weintegrate\nFigure1:Illustrationof(a)typicalCATprocess,and(b)the andtransferthepre-diagnosisresultsofexamineestothetarget\ndilemmaofCATundercoldstart. domainbasedontheconceptofDiffusionModels(DMs)[3,11].\nDMsaddnoiseincrementallyandthenreconstructthecorrupted\ndatastepbystepinreverse,whichnotonlyaddressesnoisyre-\nresponse at stepùë° ‚àí1 [26, 43, 46, 47], after which the question sponserecordsinthesourcedomainbutalsoaptlymeetstheneeds\nselectionalgorithmprovidessuitablequestionforthenextstep, ofsolvingtheCSIPtask.ThisisduetotheabilityofDMstograd-\nthusenablingamoreaccurateassessmentofexaminee‚Äôsability. uallymergecomplexmulti-distributioncross-domaindataintoa\nExistingresearchonthequestionselectionalgorithm,acritical unifiedlatentspaceduringdenoising,generatingaunifiedlatent\ncomponentofCAT,canbecategorizedintopolicy-based[1,50]and representation,whichreconstructstheinitialabilityofthetarget\nlearnable-based[34,49]approaches.Theformergenerallyconverts domainwithspecifictransferattributesfromthenoise.However,\nthequestionselectionprocessintomodelexpectation,whilethe despitethegreatpotentialofusingDMstoaddresstheCSIPtask,\nlatterdefinestheCATprocessasabi-leveloptimizationproblem. weuncoverchallengesintermsofwhatandhow:(1)Whatkindof\nDespitetheadvancementsmadebyvariousquestionselection sourcedomaininformationcanbeinjectedintothereversedenois-\nalgorithms,theystillfacechallengesrelatedtothecoldstartprob- ingprocessasguidance.And(2)Howtoconstraintheoutputto\nlemintheCATprocess.AsillustratedinFigure1(b),onanonline matchtheCATsystem,implyingthatexcessiveuncertaintyinthe\neducationplatform,thesysteminitiallyhasnoknowledgeofthe generatedresultscanleadtodataunsuitableforCATtask.\nexaminee‚Äôsability,causingtheCATsystemtotentativelyselect Toaddressthesechallenges,wedesignanoveldomaintransfer\nquestionsrandomlytodeterminethestartingpointofthetest.The frameworknamedtheDiffusionCognitiveStatesTransfeRFrame-\ndifficulty and content of the question might not align with the work(DCSR)fortheCSIPtask.GuidedbytheprincipleofDMs,\nexaminee‚Äôsactualability,leadingtoconfusionorfrustrationand wegenerateinitialabilitiesinthetargetdomainforexamineesto\nincreasingthetimecostoftheassessment.Additionally,theques- enhancethecold-startperformanceofCATsystem.Specifically,\ntionselectionprocessisaniterativeMarkovchain.Thismeans topreventthelossofpersonalizedinformationinthediffusion\nthatifthequestionselectedintheinitialstagesdonotmatchthe generationresults,weconditionthedenoiseronexaminees‚Äôprior\nexaminee‚Äôsability,itmayleadtobiasedquestionselectionsinsub- cognitiveabilitiestoincorporatepersonalizedtargetdomainabil-\nsequentstages.Mostquestionselectionalgorithmsbasedongreedy ities.Toavoidredundantinformationandnegativetransfer,we\nstrategiestendtoamplifythisbias,harmingtheperformanceofthe analyzethecausalrelationshipsbetweendifferentvariablesfrom\nCATsystem.Thesealgorithms,althoughcapableofquicklyfinding thecausalperspectiveofmodelanddesignthreedecouplingstrate-\nlocaloptimaincertainscenarios,caneasilygettrappedinlocal giestoadjustconfoundingvariablesinthebackdoorpath,including\noptimaovermultipleroundsofquestionselection.Inourpaper, domain-sharedcognition,domain-specificcognition,andorthogo-\nwerefertothistaskasColdStartwithInsufficientPriors(CSIP), nalregularization-basedgradientseparation,blockingpathsthat\nwhicharisesfromtheinsufficientunderstandingoftheexaminee obscuretruecausalrelationships.Tomitigatetheexcessiveuncer-\nbythequestionselectionalgorithmattheinitialstageofthetest, taintyofDMsandenhancetheadaptabilitytoCATtask,wedesign\nleadingtouncertainstartingpointsforselectionandsubsequently consistencyconstraintandtask-orientedconstrainttocontrolran-\naffectingtheadaptivetestingprocess. domnessandmatchtheinputrequirementsofCATsystem.The\nWiththewidespreadapplicationofeducationalplatforms,vast resultsgeneratedbyDCSRseamlesslyintegratewithexistingques-\namountsofexamineedataarecollectedandstored[14],providing tionselectionalgorithms,improvingthecold-startperformance\nvaluableresourcestoenrichthepriorinformationforCATsystem. ofCATsystem.Extensiveexperimentalresultsonfivereal-world\nNotably,historicalresponserecordsofexamineesinothercourses datasetsdemonstratethatDCSReffectivelyaddressestheCSIPtask\ncanserveascrucialreferencesforCATsystemtopreliminarily andsignificantlyoutperformsbaselines.\nunderstandexamineeabilities.Therearecertaincommonalities\namongknowledgeconceptsacrossdifferentcourses.Forexample, 2 RelatedWork\nmanyphysicsquestions,suchasthoseinkinematicsanddynam-\n2.1 ComputerizedAdaptiveTesting\nics,requirealgebraicequationsolving,whichisfundamentalin\nmathematics.Therefore,anexaminee‚Äôsresponserecordsacross ComputerizedAdaptiveTesting(CAT)aimstoevaluateanexam-\nmultiplecoursescanreflecttheirfoundationalabilitiesandcogni- inee‚Äôsabilityprogressivelywithinashortertestlength[12,27].\ntivecommonalitiesinmultipleinterdisciplinaryfields,whichare CATsystemsprimarilyconsistoftwocomponents:(1)Cognitive\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada\nDiagnosticModels(CDMs)fordiagnosingexaminees‚Äôabilities Model(CDM)M,whichassessestheexaminee‚Äôsabilitystateafter\nbasedontheirresponsestoselectedquestions[20,23,24,37,41]. respondingtotheselectedquestions,and(2)aquestionselection\nItemResponseTheory(IRT)[5],awidelyusedCDM,assumesuni- algorithmŒ†,whichchoosesthenextmostinformativequestion\ndimensionalindependenceandusescontinuouslatentvariablesto basedontheexaminee‚Äôscurrentabilitystatetomeasuretheirabil-\nassessexaminees‚Äôlatentabilities.Withthewidespreadapplication itymoreprecisely.Thesetwocomponentsalternatelyiterateuntil\nofdeepneuralnetworks,neuralCDMssuchasNCD[32],RCD[7], aterminationconditionismet.\nandKaNCD[33]utilizeneuralnetworkstocapturecomplexinter- Specifically,givenaexamineeùëí ùëñ andhis/hercandidatequestion\nactionsbetweenexamineesandquestions,enablingfine-grained setQùëñ,theCATprocesscanberepresentedas:\nd adia ag pn tio vs et li yc cm ho od oe selin thg e.( n2 e) xQ tau pe ps rt oio prn ias te el qe uc eti so tin ona blg ao ser dit oh nm ths eai Cm Dt Mo ùëûùë° ‚ÜêŒ†(ùëÑ\nùëñ\n|ùúÉ ùëñùë°‚àí1),\n(1)\nfeedback.Earlyquestionselectionalgorithmsweremodel-specific, ùúÉ ùëñùë° ‚ÜêM(ùëü ùëñùëûùë° |ùëûùë°,ùúÉ ùëñùë°‚àí1),\nsuchasIRT-specificMaximumFisherInformation[21],Kullback- whereùëûùë° denotesthequestionselectedbyŒ†basedontheexam-\nL the eib sele sr imIn pf lo er am lga ot ri io tn hmIn sd ce ox ul[ d4 n], oa tn md eeM tta hx eE evn otr lvo ip ny gn[2 e8 e] d. sH ofo Cw Dev Me sr,\n,\ninee‚ÄôsabilityùúÉ ùëñùë°‚àí1 attimeùë° ‚àí1fromthecandidatequestionset\nQùëñ,whichonlyincludesquestionsthattheexamineeùëí ùëñ hasnotyet\nleadingtoperformancelimitations.Therefore,amodel-agnostic\nquestionselectionalgorithm,MAAT[1]wasproposed,leverag-\nrespondedto.Thevariableùëü ùëñùëûùë° representstheresponseresultof\ning active learning to transform the question selection process examineeùëí ùëñ toquestionùëûùë° .Notethatùëá isthemaximumnumberof\nstepsforterminatingthetest.\nintochoosingquestionswiththegreatestexpectedmodelchange.\nSimilarly,BECAT[50]employsanexpectedgradientdifference 3.1.2 Coldstartwithinsufficientpriors. IntheColdStartwithIn-\napproach,treatingthequestionselectionprocessasasubsetselec- sufficientPriors(CSIP)scenario,thequestionselectionalgorithm\ntionproblemguidedbytheoreticalestimatesofexaminees‚Äôtrue considersonlytheinformationfromthetargetdomainwhileig-\nabilities.Anotherresearchdirectionfocusesondata-drivenques- noringtherichdatafromthesourcedomains.Toalleviatethislack\ntionselectionalgorithms.Forexample,BOBCAT[9],NCAT[49], ofpriorinformation,wedefineùëÄsourcedomainsS 1,S 2,...,SùëÄ\nandGMOCAT[34]definetheCATtaskasabi-leveloptimization andonetargetdomainT intheDiffusionCognitiveStateTransfer\nproblem,usingreinforcementlearningtolearnquestionselection Framework(DCSR).Eachdomainincludesthreegroupsofenti-\nalgorithmfromlarge-scaleresponsedata.However,fromamodel ties.WeuseE Sùëö,Q Sùëö andC Sùëö todenotethesetsofexaminees,\nperspective,thecrucialCDMcomponentincurrentCATsystems questions,andknowledgeconceptsintheùëö‚ààùëÄsourcedomain,\nrandomlyinitializesexaminees‚Äôabilitiesatthetest‚Äôsoutset.This respectively.Similarly,E T,Q T andC T denotethethreeentitiesin\nrandominitializationnecessitatesmorequestionselectionsteps thetargetdomain.Theoverlappingexamineesbetweendomains\nforgreedyalgorithmstounderstandtheexaminee‚Äôsabilityrange, aredefinedasE O ‚àãE Oùëö =E Sùëö ‚à©E T,whiletheothertwoentity\ntherebyexacerbatingthecoldstartprobleminCATsystem. groupsgenerallydonothaveoverlappingelementsacrossdomains.\n2.2 DiffusionModel 3.1.3 Training and Testing Phases. In the given CAT testing\nplatform, both warm-start and cold-start, meaning the CAT\nDiffusionModels(DMs)haveachievedimpressiveresultsinimage\nsystem has never encountered before, are included in the\ngeneration[3,11].Totransferthesesuccessestootherdomains,re-\ntarget domain. Their response records can be represented as\nwce in thtw oo thrk ers[ fi1 e5 l, d2 s5 ., C4 O8] Dh IGav Ee Matt [e 3m 1]p wte ad sto thb eri fid rg se tt th oe ei xm ta eg ne dd to hm ea di en - R T =(cid:110) Rùë§ Tùëéùëüùëö,Rùëê Tùëúùëôùëë(cid:111) =(cid:110) (ùëí ùëñ,ùëû ùëó,ùëü ùëñùëó) |ùëí ùëñ ‚ààEùë§ Tùëéùëüùëö‚à™Eùëê Tùëúùëôùëë(cid:111) ,\nwhere ùëü ùëñùëó = 1 indicates that examinee ùëí ùëñ answered question\nnoisingmoduleinDMstorecommendationsystems.DiffuRec[17],\nDreamRec[44]andCF-Diff[13]modelthelatentrepresentations ùëû ùëó ‚àà Q T correctly, and ùëü ùëñùëó = 0 otherwise. Similarly, the re-\nsponse records in theùëö source domain can be represented as\no e nrf oai ttt ee om np les yra s rn o ed n duau l cs i eze er gdp er nre e ef p re r ar e te isn oec nne t cs a, otg i so tu snid s b.i un D tg i afft lh sRe oecd ae c[n h3o 5 ie]i vs ai en n tg d em mDo piffd ouu raAle lS mt Ro o[g d1e e8n l] -- rR eS cùëö ord= s(cid:8) a( rùëí eùëñ,ùëû deùëó, nùëü oùëñùëó t) ed|ùëí aùëñ s‚àà RE SOùëö =, (cid:8)ùëû Rùëó S‚àà 1,Q RQ Sùëö 2,(cid:9) ., .a .n ,Rda Sl ùëöls (cid:9)o .u Tr oce pd ro em vea nin\nt\ndata leakage, the aforementioned response sets are uniformly\ni Mng aro kf oi vnt ce hr aa ic nti mon odse eq liu ne gn cc he as. raM co ter re io sv tie cr s, om fo Dre Mr ses te oa er xc ph ll oe rv ee sr ea qg ues enth cee d Div ùë°ùëíid ùë†ùë°ed =i Rnt ùëêo ùëúùëôùëëa .trainingsetDùë°ùëüùëéùëñùëõ ={Rùë§ Tùëéùëüùëö,R S}andatestset\nmodeling,withfewworksutilizingdiffusionfeaturestostudycross- T\nOurDCSRconsistsoftwophases:trainingtheinitialabilities\ndomainproblems.AlthoughDiffCDR[36]hasmadepreliminary\nofexamineesinthesourcedomainbasedonDùë°ùëüùëéùëñùëõ andtesting\nexplorationsinthisarea,itintroducesredundantinformationand\ntheperformanceofquestionselectionintheCATsystemwithout\nfaceslimitationsincross-domainrepresentationcapacitydueto\nlearning,basedonDùë°ùëíùë†ùë°.\nthediverseentitiesinCATsystem.\n3.1.4 Pre-EstablishCognitiveStates. Thedatainthetrainingset\n3 Preliminary Dùë°ùëüùëéùëñùëõcomesfromtheresponserecordsofoverlappingexaminees\n3.1 ComputerizedAdaptiveTesting E O betweendomains.Wepre-traintheseexamineestoestablish\ntheircognitivestatesinboththesourceandtargetdomains.The\n3.1.1 TaskIntroduction. Inanonlineeducationplatform,theCom-\npre-trainingprocessisexpressedas:\nputerizedAdaptiveTesting(CAT)systemadaptivelyselectsques-\ntionsforexamineestoaccuratelyrevealtheircognitiveabilities.The ùúÉ‚àó=argmin ‚àëÔ∏Å L(cid:0)ùëü ùëñùëó,MŒ®(ùëû ùëó|ùúÉ ùëñ)(cid:1), (2)\nCATsystemcomprisestwocomponents:(1)aCognitiveDiagnosis ùúÉ‚ààŒò (ùëíùëñ,ùëûùëó,ùëüùëñùëó)‚ààDùë°ùëüùëéùëñùëõ\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada HaipingMaetal.\nFigure2:TheoverviewofDCSR:Theleftsidecorrespondstopre-training.Whiletheblue,yellow,andgreenmodulesare\nusedfortraining,andtheorangemodulescorrespondtotheapplicationinCAT.Additionally,therightsidedepictsthecausal\ndiscoveryinthegenerationprocess.\nwhere MŒ® is the CDM used for pre-training, and different do- isbuiltupontheDiffusionModuleandissupportedbytwokey\nmain data correspond to different CDM parameters, i.e., Œ® = components:theCognitiveStateUnificationModule(CSUM)and\n{ùúì S 1,ùúì S 2,...,ùúì SùëÄ,ùúì T}. The abilities of overlapping examinees theHarmonizationandCalibrationModule(HCM).Specifically,the\naredenotedasŒò S = {ùúÉ ùëñ |ùëñ ‚ààE O},andsimilarly,theabilitiesin DiffusionModuleusespriorabilitiesfromthesourcedomainto\nthetargetdomainaredenotedasŒò T. reconstructthecognitivestateofexamineesinthetargetdomain\nfromnoise.Fromacausalmodelperspective,thegeneratedabili-\n3.2 DiffusionModel tiesareinfluencedbyredundantknowledge,anddomain-specific\ncognition may cause negative transfer. Therefore, the CSUM is\nDiffusion Models (DMs) have demonstrated exceptional perfor-\nemployedtocontrolconfoundingvariablesinthebackdoorpath,\nmanceinfieldssuchascomputervision[11].Typically,DMsconsist\ntherebyuncoveringthetruecausalrelationships.Additionally,to\noftwoparts:theforwardprocessandthereverseprocess.\nmitigateuncertaintyduringthediffusionprocess,wedesignedcon-\n3.2.1 ForwardProcess. Givenadatapointùë• 0‚àºùëû(ùë•)sampledfrom sistencyandtask-orientedconstraintsinHCM,aimingtoensure\nthetruedatadistribution,theforwardprocessgraduallydegrades thatthegeneratedresultsadheretothetruedistributionandmeet\nùë• 0intostandardGaussiannoiseùë• ùëá ‚àºN(0,1)byinjectingGaussian therequirementsofCATtask.Itisworthnotingthatourframe-\nnoiseoverùëá steps.Specifically,theprocessofconvertingùë• ùë°‚àí1to workdemonstratessignificantscalability,seamlesslyintegrating\nùë• ùë° inDMsisrepresentedasùëû(ùë• ùë° |ùë• ùë°‚àí1)=N(ùë• ùë°;‚àöÔ∏Å 1‚àíùõΩ ùë°ùë• ùë°‚àí1,ùõΩ ùë°I), thegeneratedinitialabilitiesinthetargetdomainintoexistingCAT,\nwhereùë° ‚àà {1,...,ùëá} represents the diffusion steps, ùõΩ ùë° ‚àà (0,1) therebyimprovingitscold-startperformance.\nisthepredefinednoiseschedulingcoefficient,andN denotesthe\n4.1 DiffusionModule\nGaussiandistribution.\nWeemploytheconceptofdiffusionasthebackboneofourmodel,\n3.2.2 ReverseProcess. Inthereverseprocess,DMslearntoremove\nestablishingabridgeforcognitivestatetransformationbetweenthe\ntheaddednoise,therebyrecoveringtheoriginaldatadistribution\nsourceandtargetdomains.ThepurposeoftheDiffusionModuleis\nùë• 0frompurenoise,aimingtointroduceminoruncertaintiesinthe\ntoincorporatepriorinformationfromthesourcedomainintothe\ngenerationprocess.Thisprocesslearnsaparameterizednetwork\ninitialabilityestimationprocessinthetargetdomain.Therefore,\nùëù ùõø(ùë• ùë°‚àí1 | ùë• ùë°) toapproximatethereverseprocess,whichcanbe\nitinherentlyinvolvestwodistinctprocesses:theforwardnoise\nformalizedasùëù ùõø(ùë• ùë°‚àí1 |ùë• ùë°) =N(ùë• ùë°‚àí1;ùúá ùõø(ùë• ùë°,ùë°),Œ£ ùõø(ùë• ùë°,ùë°)),where\nadditionprocessandthereversedenoisingprocess.\nùúá ùõø andŒ£ ùõø arethemeanandvarianceoftheGaussiandistribution Duringtraining,wegraduallyinjectGaussiannoiseintothe\npredictedbyaneuralnetworkwithparametersùõø.\nabilityvectorsŒò T ofexamineesinthetargetdomainoverùëá steps.\n4 Method Foraspecificexamineeùëí ùëñ ‚ààE O,hisabilityvectorùúÉ ùëñT 0 =ùúÉ ùëñT ‚ààŒò T\nOverview. Thecoreideaofthisworkistotransferpriordiagnos-\niscorruptedintoùúÉ ùëñT 1:ùëá,whichismodeledasaGaussiantransition\nMarkovchain:\nticresultsfromthesourcedomaintothetargetdomain,thereby\nùëá\ng inen the era tt ain rgg etpe dr os mon aa inliz ce od uri sn ei .ti Aal sa ilb li uli st ti re as tefo dr inco Fl id g- us rta er 2t ,e ox ua rm Din Ce Se Rs ùëû(ùúÉ ùëñT\n1:ùëá\n|ùúÉ ùëñT 0)=(cid:214) ùë°=1N(ùúÉ ùëñT ùë°;‚àöÔ∏Å 1‚àíùõΩ ùë°ùúÉ ùëñT ùë°‚àí1,ùõΩ ùë°I) (3)\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada\nwhereùõΩ ùë° ‚àà (0,1) controlsthescaleofnoiseaddedatthe$t$-th and(¬∑ ‚à• ¬∑)denotestheconcatoperation.Wethenencouragethisde-\nstep,andN denotesaGaussiandistribution. coupleddomain-sharedcognitiontoassistinpredictingresponses\nInthereversedenoisingprocess,thetraditionaldenoisingmeth- acrossalldomains:\no trd as ns(a fes rin bt er co ad uu sc ee td hein ds ee nc ot ii so in ng3. p2 r.2 o) cea sr se min oa dd ee lq eu dat lae cf ko sr ge uff ie dc at niv cee L 1= ‚àëÔ∏Å ‚à•ùëü ùëñT\nùëó\n‚àíMùúôùëá(ùúÉ ùëñùë†‚Ñéùëéùëüùëí,ùëû ùëóT)‚à•\nfromsourcedomaininformation,resultinginthelossofpersonal-\n(ùëíùëñ,ùëûT ùëó,ùëü ùëñT ùëó)‚ààR Tùë§ùëéùëüùëö\nviz ea ct ti oo rn s,in wt ehe prg oe pn oe sr eat ue td ila izb ii nli gtie ps r. ioT ro ig ne fn oe rmra ate tiop ner fs ro on mali tz he ed sa ob uil ri cty\ne\n+ ‚àëÔ∏ÅùëÄ ‚àëÔ∏Å ‚à•ùëü ùëñS ùëòùëö ‚àíMùúôùëÜùëö(ùúÉ ùëñùë†‚Ñéùëéùëüùëí,ùëû ùëòSùëö)‚à•, (7)\ndomaintoguidethedenoisingprocess.Specifically,forexamineeùëí ùëñ, Sùëö=1(ùëíùëñ,ùëûS ùëóùëö,ùëü ùëñS ùëóùëö)‚ààRSùëö\nweusethepretrainedsourcedomainabilityùúÉ ùëñS ‚ààŒò S asguidance: whereùúÉùë†‚Ñéùëéùëüùëí\nisencouragedtopredictresponsesacrossallscenarios,\nùëù ùõø(ùúÉ ùëñT ùë°ÀÜ\n‚àí1\n|ùúÉ ùëñT ùë°,ùúÉ ùëñS)=N(ùúÉ ùëñT ùë°ÀÜ ‚àí1;ùúá ùõø(ùúÉ ùëñT ùë°,ùúÉ ùëñS,ùë°),Œ£ ùõø(ùúÉ ùëñT ùë°,ùúÉ ùëñS,ùë°)), (4) i rn esc plu od ni sn eg dt ah te at ia nrg the etd seo cm onai dn ti en rmth .e Nfi er xs tt ,tt her em spa en cd ifia cll coso gu nr ic tie ondo om fta hin\ne\nw wih te hr le eaùúá rùõø na an bld eŒ£ pùõø ara ar me et th ee rsp ùõøa .rametersoutputbyneuralnetworks ùêµta 1rg ‚Üíet ùëådo am ndai ùêµn 1ùêµ ‚Üê1h {a ùëãs ,t ùêµw }o ‚Üípat ùëåh .s Win efl au de jun sc tin thg et ch oe ng foe un ner da inte gd vr ae rs iu ablt ls e:\n{ùëã,ùêµ}inthesecondconfoundingpathtoblockthebackdoorpath.\n4.2 CognitiveStateUnificationModule Specifically,weextractthespecificcognitionofthetargetdomain,\nAlthoughusingunifiedsourcedomainabilitiesobtainedthrough whichwillbeusedasinputinthesection4.1alongsideobtaining\nCDM to guide the reverse denoising process is a promising ap- theoverallcognitivestateofthetargetdomain(correspondingto\nproach, from the causal perspective of the model, as shown in\neventùêµinthecausalgraph):\nt bh ye dr oi mgh at ins -i sd he aro ef dF cig ou gnre iti2 o, nth ùëãe ,dg oe mne ar ia nt -e sd pea cb ifiil city coùëå gni is tioin nfl ùê¥ue 1,n ac ned\nd\nùúÉ ùëñTùë†ùëùùëíùëêùëñùëìùëñùëê =ùëì\nùúë\n4(ùúé(ùëì\nùúë\n3(ùúÉT))),\n(8)\ntargetdomainabilityùêµduringthetrainingphase.Thisisbecause ùúÉ ùëñùëêùëúùëõùëêùëéùë° =ùëì\nùúë\n5(ùúé(ùëì\nùúë\n6(ùúÉ ùëñTùë†ùëùùëíùëêùëñùëìùëñùëê ‚à•ùúÉ ùëñùë†‚Ñéùëéùëüùëí ))),\nweuserandomsamplingoftimestepsfortraining,whichretains\nsomepersonalizedinformationinthetargetdomainabilityvector whereùëì {ùúë 3,ùúë 4,ùúë 5,ùúë 6}arefourlinearlayerswithdifferentparameters.\nevenwhileintroducingnoise.Inotherwords,ùúÉ ùëñT\n1:ùëá\ndoesnotapprox- T ao nole va er ln dt eh ce ousp pe lic ni gfic stc ro ag ten git yi ,o wn ho if chth ee nt ca org ue rat gd eo sm thai en p, rw ee dip cr tio op nos oe\nf\nimatestandardGaussiannoise,whichlacksextensivepersonalized\nfeatures.However,theinclusionofdomain-specificcognitionùê¥\n1\nwithin-domainresponseresultsbydomain-specificcognitionwhile\ndegradingperformanceinotherdomains:\nincreasesthecomplexityofthemodelandreducesitsgeneraliza-\ntionability,makingthemodelpronetooverfitting.Therefore,we L 2= ‚àëÔ∏Å (cid:16) ‚à•ùëü ùëñT\nùëó\n‚àíMùúôùëá(ùúÉ ùëñùëêùëúùëõùëêùëéùë°,ùëû ùëóT)‚à•\nproposetoexplorethecausalrelationshipbetweenthegenerated\nresultùëå andthedomain-sharedcognitionùëã andthepretrained\n(ùëíùëñ,ùëûT ùëó,ùëü ùëñT ùëó)‚ààR Tùë§ùëéùëüùëö\nabilityùêµofthetargetdomain,whichcanalsobefurtherdecoupled +‚à•ùëü ùëñT\nùëó\n‚àíMùúôùëá(ùúÉ ùëñTùë†ùëùùëíùëêùëñùëìùëñùëê,ùëû ùëóT)‚à•(cid:17)\nintodomain-specificcognitionùêµ 1anddomain-sharedcognitionùëã.\nùëÄ\nN the ex gt, ew nee rw ati ell ds rp ee sc ui lfi tc .allyanalyzetheimpactofdifferentfactorson ‚àí ‚àëÔ∏Å ‚àëÔ∏Å (cid:16) ‚à•ùëü ùëñS ùëòùëö ‚àíMùúôùëÜùëö(ùúÉ ùëñùëêùëúùëõùëêùëéùë°,ùëû ùëòSùëö)‚à•\nFirst,therearethreepathsbetweendomain-sharedcognitionùëã Sùëö=1(ùëíùëñ,ùëûS ùëóùëö,ùëü ùëñS ùëóùëö)‚ààRSùëö\na {ùêµnd ,ùêµt 1h }e ‚Üígen ùëåe ,r wat he ed rr ee ts hu elt ls atùëå te: rùëã tw‚Üí opùëå a, tùëã hs‚Üê are{ cùê¥ o, nùê¥ fo1 u} n‚Üí dinùëå g, pa an td hsùëã .W‚Üê\ne\n+‚à•ùëü ùëñS ùëòùëö ‚àíMùúôùëÜùëö(ùúÉ ùëñTùë†ùëùùëíùëêùëñùëìùëñùëê,ùëû ùëòSùëö)‚à•(cid:17)\nwillapplythebackdoorcriteriontoexplorethecausalrelationship +E ùëíùëñ‚ààEOùëö‚à•ùúÉ ùëñT‚àíùúÉ ùëñùëêùëúùëõùëêùëéùë° ‚à•.\nof ùëã ‚Üí ùëå, which means we need to control the confounding (9)\nvariablesùê∂ ={ùê¥,ùê¥ 1}‚à™{ùêµ,ùêµ 1}toblockthebackdoorpaths: Here,thefirsttermencouragesthetargetdomain-specificcognition\n‚àëÔ∏Å toassistinpredictingtheresponserecordsintargetdomain,bothin-\nùëÉ(ùëå |ùëëùëú(ùëã))= ùëÉ(ùëå |ùëã,ùê∂ =ùëê)ùëÉ(ùê∂ =ùëê), (5)\ndependentlyandinconjunctionwithdomain-sharedcognition.The\nùê∂\nsecondtermaimstointentionallydegradepredictionperformance\nwherethefirsttermrepresentstheeffectofùëãonùëåwhilecontrolling\ninthesourcedomains,indicatingthatthetargetdomain-specific\nfortheconfoundingvariablesùê∂,andthesecondtermrepresents\ncognitionisnotapplicabletothesourcedomains.Thethirdterm\nthejointprobabilitydistributionoftheconfoundingvariables.To\nconstrainsthediagnosticabilitiesobtainedfrompre-training.Addi-\ncontroltheconfoundingvariables,inspiredby[8],wedecouplethe\ntionally,tofurthercontroltheinfluenceofconfoundingvariableson\ndomain-sharedcognitionùúÉ ùëñùë†‚Ñéùëéùëüùëí fromallthepriorabilitiesinthe\nthegeneratedresults,weapplygradient-basedorthogonalregular-\nsourceandtargetdomains:\nization[6]toensuretheindependenceoftheaboverepresentations\nùúÉ ùëñùë†‚Ñéùëéùëüùëí =ùëì ùúë 2(ùúé(ùëì ùúë 1(ùúÉ ùëñT ‚à•E ùëö‚àºSùëÄWùëöùúÉ ùëñSùëö))), (6) inthefeaturespace:\nw abh ile ir tie eùëä sfro‚àà mR mùëÄ u√ó lùëë tipis lea dw oe mig ah int sm ina tt ori tx hu es se ad mt eo fm eaa tup rt eh se pp acre et ,r wai hn ee rd\ne\nL 3=(cid:13) (cid:13) (cid:13)‚à•‚àá‚àáùúë ùúë1 1, ,ùúëùúë 2 2L L1\n1‚à•\n¬∑ ‚à•‚àá ‚àáùúë ùúë3 3, ,ùúë ùúë4 4L L2 2‚à•(cid:13) (cid:13)\n(cid:13)\n2. (10)\nùëë isthefeaturedimensionrelatedtoCDM,ùëì ùúë andùëì ùúë arelinear Thisapproachminimizestheinfluenceofdomain-specificcognition\n1 2\nlayerswithdifferentparameters,ùúéreferstotheactivationfunction, whenlearningdomain-sharedcognition,andviceversa.\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada HaipingMaetal.\nTherefore, the parameters in the forward and reverse Statistics C C++ DS Java Python\nùëùp ùõøro (ùúÉce ùëñT ùë°s ùë† ‚àíùëùs 1ùëíÀÜe ùëês ùëñùëìùëñùëêa |r ùúÉe ùëñT ùë°ùë†ùëùùëíu ùëêùëñp ùëìùëñd ùëêa ,t ùúÉe ùëñùë†d ‚Ñéùëéùëüùëít )o ,resùëû pe(ùúÉ cùëñ tT ùë° iùë† vùëù eùëíùëê lyùëñùëì .ùëñùëê |ùúÉ ùëñT ùë°ùë† ‚àíùëù 1ùëíùëêùëñùëìùëñùëê) and # #E Qx ua em sti in oe ne 3 1, 18 55 ,6\n335\n3 1, 08 ,5 26\n41\n3 1, 48 ,5 26\n07\n3 1, 08 ,5 06\n43\n3 1, 18 ,5 66\n00\n#Concept 654 472 560 541 480\n4.3 HarmonizationandCalibrationModule #Log 2,163,624 1,332,005 1,398,444 1,086,906 923,879\nTolearntheparametersùõø ofthedenoisingnetwork,DCSRaims Table1:Thestatisticsofthedatasets.\ntomaximizetheEvidenceLowerBound(ELBO)oftheobserved\nex loa gm ùëù(i ùúÉn ùëñTe ùë†ùëùe ùëíùëêùëña ùëìùëñb ùëê)il ‚â•it E (cid:124)y ùëû (cid:32)(cid:32)(cid:32)(cid:32)ùúÉ ( (cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)ùëñ ùëñT (cid:32)T (cid:32)0ùë† (cid:32)(cid:32)ùë† ùëù (cid:32)(cid:32)(cid:32)ùëí (cid:32)ùëù (cid:32)ùëê (cid:32)(cid:32)ùëí (cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)ùëê (cid:32)ùëñ (cid:32)(cid:32)ùëñ ùëê (cid:32)(cid:32)(cid:32)ùëì | (cid:32)(cid:32)ùúÉ (cid:32)ùëê (cid:32)(cid:32)ùëñT (cid:32)1(cid:32)ùë† (cid:32): (cid:32)ùëù (cid:32)(cid:32)(cid:32)ùëí (cid:32)(cid:32)ùëê (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)(cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëê (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:104) (cid:32)(cid:32)l (cid:32)(cid:32)o (cid:32) (cid:123)g (cid:122)ùëù (cid:32)(cid:32)ùõø (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)ùëñ (cid:32)T (cid:32)0 (cid:32)ùë† (cid:32)(cid:32)(cid:32)ùëù (cid:32)(cid:32)ùëí (cid:32)(cid:32)(cid:32)ùëê (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëê (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)ùëñ (cid:32)T (cid:32)1 (cid:32)ùë† (cid:32)(cid:32)(cid:32)ùëù (cid:32)(cid:32)ùëí (cid:32)(cid:32)(cid:32)ùëê (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëê (cid:32)(cid:32)(cid:32)) (cid:32) (cid:125)(cid:105) cognitiv ‚àáe ùúÉÀÜf Te Lat ùë°u ùëêr =es ‚àíùúÉ ùëñÀÜT with ‚àëÔ∏ÅtheCDM: (cid:16) ùëü ùëñùëólog(cid:0)Mùúì T(ùúÉ ùëñÀÜT,ùëû ùëó)(cid:1)\nùëá\n:=L0 (ùëíùëñ,ùëûùëó,ùëüùëñùëó)‚ààR Tùë§ùëéùëüùëö\n(15)\n‚àí‚àëÔ∏Å ùë°=2E (cid:32)ùëû (cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)ùëñ(cid:32)T (cid:32)ùë°(cid:32)ùë† (cid:32)(cid:32)ùëù (cid:32)(cid:32)ùëí (cid:32)(cid:32)(cid:32)ùëê (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)(cid:32)(cid:32)ùëñ (cid:32)ùëê (cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)ùúÉ (cid:32)(cid:32)(cid:32)ùëñ(cid:32)T (cid:32)0ùë† (cid:32)(cid:32)ùëù (cid:32)(cid:32)(cid:32)ùëí (cid:32)(cid:32)ùëê (cid:32)(cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëì (cid:32)(cid:32)ùëñ (cid:32)(cid:32)ùëê (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:104) (cid:32)(cid:32)K (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)L (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)q (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T (cid:32)t (cid:32)‚àís (cid:32)(cid:32)p (cid:32)1 (cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)ùúÉ iT ts (cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)(cid:32)c (cid:32)(cid:32), (cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T 0 (cid:32)(cid:32)s (cid:32)(cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)‚à• (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)p (cid:32)(cid:32)(cid:32)ùõø (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T t (cid:32)(cid:32)‚àís (cid:32)(cid:32)p (cid:32)1 (cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)ùúÉ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T t (cid:32)(cid:32)s (cid:32)(cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)) (cid:32)(cid:105) . +(1‚àíùëü ùëñùëó)log(cid:0) 1‚àíMùúì T(ùúÉ ùëñÀÜT,ùëû ùëó)(cid:1)(cid:17) ,\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=Lùë°‚àí1 whereMùúì (¬∑)denotesthepre-trainedCDMforthetargetdomain,\n(11) T\nHere,thefirsttermrepresentsthereconstructionterm,whichre-\nwithitsparametersfrozen,andonlythenetworkparametersùúá ùõø(¬∑)\ncoverstheprobabilityofùúÉ ùëñTùë†ùëùùëíùëêùëñùëìùëñùëê.Thesecondtermisthedenoising ofthedenoisingmoduleareupdated.\n0\nmatchingterm,whichalignstheintractableposteriorprobability 4.4 DCSR-CAT:ImplementationofDCSR\nùëù ùõø(¬∑)withthetractabledistributionùëû(¬∑).Tomaintaintrainingsta-\nbilityandsimplifycomputation,weignorethelearningofŒ£ ùõø(¬∑) Inthissection,weintroduceDCSR-CATasanimplementationof\nandsetittoafixedvalueùõΩ ùë° asintheforwardprocess[11,35].The DCSRtodemonstrateitsapplicabilitytoCAT.\nDuringthisstage,DCSRnolongerinvolvestheforwardprocess\ndenoisingmatchingtermcanthenbefurthercomputedas:\nLùë°‚àí1=E\nùëû(ùúÉ ùëñT 0ùë†ùëùùëíùëêùëñùëìùëñùëê |ùúÉ ùëñT 1ùë†ùëùùëíùëêùëñùëìùëñùëê )\n(cid:20) 21\nùõΩ ùë°\n(cid:13) (cid:13) (cid:13)ùúá ùõø(ùúÉ ùëñT ùë°ùë†ùëùùëíùëêùëñùëìùëñùëê,ùúÉùë†‚Ñéùëéùëüùëí,ùë°) b c thou elt d sd - osir t uae rrc ctt el ey x dat oa m mk ie ans inep e ,u wùëír ùëñ ee ‚àà un R so ei ùëê Ts ùëú te ùëô hùëëùúñ e0 w ph‚àº reo -N th ra a( s0 in, r1 ee d) spa Cos Dni sn Mep ru te ot c. o oF r bo d tr s aia o nng tli hyv ee i in n\nr\n‚àíùúá(ùúÉTùë†ùëùùëíùëêùëñùëìùëñùëê,ùúÉTùë†ùëùùëíùëêùëñùëìùëñùëê)(cid:13) (cid:13)(cid:105) , priorabilityandcalculatetheirdomain-sharedcognitivefeatures\n(cid:101) ùëñùë° ùëñ 0 (cid:13)\n(12)\nùúÉ ùëñùë†‚Ñéùëéùëüùëí .ItisnoteworthythatsincetheoriginalùúÉ ùëñT isunknown,the\naveragecognitivestateofthetargetdomainisusedasasubstitute:\nwhere ùúá(¬∑) satisfies the tractable distribution\nùëû(ùúÉ ùëñT ùë°ùë† ‚àíùëù 1ùëíùëêùëñùëìùëñùëê(cid:101) |ùúÉ ùëñT ùë°ùë†ùëùùëíùëêùëñùëìùëñùëê,ùúÉ ùëñT 0ùë†ùëùùëíùëêùëñùëìùëñùëê)=N(ùúÉ ùëñT ùë°ùë† ‚àíùëù 1ùëíùëêùëñùëìùëñùëê; (cid:101)ùúá(¬∑),ùõΩ ùë°I). ùúÉ ùëñTùëêùëúùëôùëë =E ùëíùëó‚ààR Tùë§ùëéùëüùëöMùúì T‚àáùúÉ(ùëí ùëó), (16)\nAlthoughtheabovetrainingobjectiveensuresdiversityingen-\nwhichretainsdomaininformationtosomeextent.Therefore,sub-\neratedsamples,excessiverandomnessintroducedbythediffusion\nstitutingbackintoequation(6),weobtainthesharedcognitive\nmodelduringtrainingandinferencestagesmayresultingenerated abilityùúÉ ùëñùë†‚Ñéùëéùëüùëí ofthecold-startexamineeùëí ùëñ.Toimproveinference\noutcomesthatdonotmatchtherequirementsoftheCATsystem.\nefficiency,weuseDPM-Solver[22]asafastsolvertoefficiently\nTherefore, we constrain the generated ability vectors from two\nobtaintheinitialabilityofcold-startexaminee:\naspects:ConsistencyConstraintandTask-orientedConstraint.\nùúÉ ùëñ0=Solver(ùúá ùõø(¬∑),ùúÉ ùëñùë†‚Ñéùëéùëüùëí,ùúñ 0), (17)\n4.3.1 ConsistencyConstraint. aimstominimizethedifferencebe- whichiscrucialforreal-worldapplications.OurDCSRseamlessly\ntweengeneratedvectorsandrealdatasamplesinthefeaturespace, integrateswithanyexistingquestionselectionalgorithmŒ†.Atthe\nthuslimitinguncertaintywithinacontrollablerange.Specifically, beginningofthetest,theselectionprocesscanberepresentedas:\nthe Diffusion Module diffuses the domain-shared cognitive fea- ùëû1‚ÜêŒ†(ùëÑ ùëñ |ùúÉ ùëñ0), (18)\nturesintodomain-specificcognitivefeatures.Firstly,thegeneral\ncognitivefeaturesofthetargetdomainaregeneratedasùúÉ ùëñÀÜT: w aph pi rc oh pi rn iad ti eca itt ee ms t ùëûh 1at inth ste epite ùë°m =1se fl re oc mtio tn heal cg ao nr dit idh am teŒ† ites mele pc ot os lt Qhe\nùëñ\nùúÉ ùëñÀÜT =ùúá ùõø(ùúÉ ùëñT ùëáùë† :ùëù 1ùëíùëêùëñùëìùëñùëê,ùúÉ ùëñùë†‚Ñéùëéùëüùëí,ùë° ùëá:1)+ùõΩ ùë°ùúñ,ùúñ ‚àà (0,1). (13) Tfo hr ee sx ua bm sein qe ue enùëí ùëñ tb sta es pe sd fo on llot whe ti hn eit gia el na eb rail lit Cy AùúÉ Tùëñ0 sin yi st ti ea mliz wed orb ky flD owC .SR.\nDuetoinherentrandomness,thegeneratedabilityvectorsarenoisy,\nwhichisnotconducivetoreflectingtheexaminee‚Äôstrueability. 5 Experiments\nThus,theconsistencyconstraintaimstominimizethedifference\nInthissection,weconductexperimentswiththeaimofaddressing\nbetweenùúÉ ùëñÀÜT andthetargetdomainabilityfeaturesùúÉ ùëñT obtained thefollowingquestions:\nthroughpre-training,whichcanbemodeledas:\n‚Ä¢ RQ1:CanDCSRutilizepriorinformationfromasingledo-\nLùëêùëê =E ùëíùëñ‚ààEO(ùúÉ ùëñT‚àíùúÉ ùëñÀÜT)2. (14) mainormultipledomainstoimprovethecoldstartperfor-\nmanceofexistingCATsystems?\n4.3.2 Task-orientedConstraint. ensuresthatthegeneratedability ‚Ä¢ RQ2:HoweffectivearethekeycomponentsoftheDCSR\nfeaturesmeettherequirementsoftheCATtask.Specifically,we framework?\nsamplesomequestionsùëû ùëó ‚àà Rùë§ùëéùëüùëö fromthetrainingsetofthe ‚Ä¢ RQ3:DoesDCSRalleviatetheissueofthequestionselection\nT\ntarget domain and match the generated target domain general algorithmfallingintolocaloptima?\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada\nCDM IRT NCD\nCAT Fisher MAAT MAAT BECAT NCAT\nMetrics AUC(%)@1/ACC(%)‚Üë@1\nBaselines Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle*\nC++ 71.6/75.0 78.5/77.5 77.9/76.3 79.5/75.4 71.2/74.9 78.5/77.4 77.7/76.0 79.6/75.7 67.4/62.8 74.7/74.6 79.2/77.3 88.0/83.1 67.4/62.7 74.6/74.6 79.2/77.3 88.0/83.0 67.4/62.8 74.5/74.6 79.3/77.3 88.0/83.1\nC\nDS 71.8/75.4 75.5/77.0 75.4/75.9 77.2/78.2 71.1/75.2 74.9/76.8 75.0/75.7 77.2/78.2 67.5/64.9 77.7/76.6 79.4/77.8 89.3/84.3 67.6/65.0 77.7/76.6 79.4/77.8 89.3/84.3 67.6/65.0 77.7/76.6 79.4/77.8 89.3/84.4\nC 67.4/74.8 71.6/75.5 72.5/75.9 76.8/77.2 67.1/74.8 71.5/75.4 72.4/75.8 76.8/77.2 66.1/67.3 71.6/74.8 71.7/74.1 85.8/82.0 66.0/67.3 71.6/74.9 71.7/74.1 85.8/82.0 66.1/67.3 71.6/74.9 71.7/74.1 85.8/82.0\nDS\nC++ 71.6/75.0 73.8/75.0 74.5/75.5 79.5/75.4 71.2/74.9 73.5/75.0 74.1/75.3 79.6/75.7 67.4/62.8 72.4/73.7 74.4/74.9 88.0/83.1 67.4/62.7 72.4/73.7 74.4/74.9 88.0/83.0 67.4/62.8 72.4/73.7 74.4/74.9 88.0/83.1\nC 67.4/74.8 74.2/76.0 74.7/76.8 76.8/77.2 67.1/74.8 74.2/76.1 74.6/76.6 76.8/77.2 66.1/67.3 72.8/75.3 73.4/75.5 85.8/82.0 66.0/67.3 72.8/75.3 73.4/75.5 85.8/82.0 66.1/67.3 72.8/75.2 73.1/75.3 85.8/82.0\nC++\nDS 71.8/75.4 71.9/76.5 73.3/76.6 77.2/78.2 71.1/75.2 71.4/76.3 72.7/76.4 77.2/78.2 67.5/64.9 76.4/76.5 77.0/76.9 89.3/84.3 67.6/65.0 76.4/76.5 77.0/76.9 89.3/84.3 67.6/65.0 76.4/76.5 77.0/76.9 89.3/84.4\nMetrics AUC(%)‚Üë@5/ACC(%)‚Üë@5\nC++ 74.2/75.6 78.9/77.9 78.8/76.9 79.2/74.7 72.0/75.0 78.2/77.3 77.5/76.1 79.7/76.3 67.5/63.1 74.8/74.6 79.3/77.4 88.0/83.0 67.5/62.9 74.7/74.6 79.3/77.4 88.0/83.1 67.6/63.0 74.7/74.7 79.3/77.4 88.0/83.1\nC\nDS 74.2/76.0 76.9/77.7 76.6/76.5 77.4/78.3 71.5/75.4 75.0/76.9 75.0/75.7 77.2/78.1 67.9/65.3 77.9/76.7 79.5/77.9 89.2/84.2 67.7/65.1 77.8/76.7 79.5.77.9 89.3/84.4 67.8/65.2 77.8/76.7 79.5/77.9 89.3/84.4\nC 68.5/75.0 72.4/75.7 73.1/76.1 76.8/77.3 67.4/74.8 71.8/75.6 72.6/75.9 76.7/77.2 66.3/67.5 71.6/74.8 71.8/74.1 85.9/82.1 66.1/67.4 71.6/74.9 71.7/74.1 85.8/82.0 66.2/67.4 71.7/74.9 71.7/74.1 85.8/82.1\nDS\nC++ 74.2/75.6 75.3/75.7 76.3/76.4 79.2/74.7 72.0/75.0 74.0/75.2 74.9/75.8 79.7/76.3 67.5/63.1 72.6/73.8 74.5/75.0 88.0/83.0 67.5/62.9 72.5/73.8 74.5/75.0 88.0/83.1 67.6/63.0 72.5/73.8 74.5/75.0 88.0/83.1\nC 68.5/75.0 74.4/75.9 74.8/76.9 76.8/77.3 67.4/74.8 74.4/76.2 74.7/76.7 76.7/77.2 66.3/67.5 72.9/75.4 73.5/75.5 85.9/82.1 66.1/67.4 72.9/75.4 73.4/75.5 85.8/82.0 66.2/67.4 72.9/75.3 73.2/75.3 85.8/82.1\nC++\nDS 74.2/76.0 73.4/77.2 75.1/77.3 77.4/78.3 71.5/75.4 71.8/76.4 73.1/76.5 77.2/78.1 67.9/65.3 76.6/76.7 77.1/77.0 89.2/84.2 67.7/65.1 76.5/76.6 77.0/77.0 89.3/84.4 67.8/65.2 76.5/76.6 77.0/77.0 89.3/84.4\nTable2:TheAUC/ACCperformanceinsixscenarios.Thebestresultsarehighlightedinbold,while‚àódenotestheupperbounds.\n‚Ä¢ RQ4:CanDCSRenhancethecoldstarteffectofcognitive ‚Ä¢ MLCCM:Across-coursemethodbasedonmeta-learning,\ndiagnosismodels? applyingtheideaofmeta-learningtolearncross-domain\n‚Ä¢ RQ5:IstheinitialabilityassignedbyDCSRtocoldstart mappingfunctionsfromthetrainingset.\nexamineesreasonable? ‚Ä¢ Oracle:ThismethodusesCDMtodirectlytrainthetarget\ndomainabilityfromtheresponserecordsofexamineesin\n5.1 ExperimentalSettings thetargetdomain(testset).\nInthissection,weintroducethedatasets,theselectedbaselines, 5.1.3 EvaluationMetrics. TheperformanceofourDCSRwillbe\nandtheapplicationofCAT. validatedduringthetestingphaseoftheCATsystem.Weevaluate\ntheaccuracyofthefinalabilityestimationbypredictingexaminees‚Äô\n5.1.1 Dataset Description. We conducted experiments on five\nbinary responses to the test question set. For this purpose, we\nreal-worlddatasetscollectedfromthepubliclyavailablePTADisc\nusetheareaundertheAreaUnderROCCurve(AUC)[2,40]and\ndataset [14], covering courses in Data Structures (DS), C, C++,\nAccuracy(ACC)asevaluationmetrics.\nPython, and Java programming languages. These datasets are\nsourcedfromthePTAplatform,whichrecordsthelearningperfor- 5.1.4 ParameterSettings. Inthepre-trainingphase,forIRT,weset\nmanceofexamineesacrossaseriesofcourses.Eachdatasetprovides thelatentfeaturedimensionofbothexamineesandquestionsto1,\ntheresponserecordsoftheexamineesandquestion-conceptrela- whileforNCD,itissettothenumberofknowledgeconceptsinthe\ntion,witheachdatasetconsideredasadistinctdomain.Wefirst correspondingdomain.Additionally,weuniformlysetthebatchsize\nexcludedexamineeswithfewerthan100responserecordsfrom andlearningrateto32and0.002,respectively,forthisphase.Inthe\neachdataset.Thestatisticsoftheprocesseddatasetsareshown DSCRtrainingphase,theforwardprocessofthediffusionmodule\ninTable1.Forfairnessintesting,werandomlysplitthefiltered issetto1000stepsofnoiseaddition,andDPM-solver[22]isused\nexamineerecordsintoatrainingsetandatestsetinan80%:20% toacceleratesampling,whichisperformedin30steps.Meanwhile,\nratio.Examineesinthetestsetareconsideredcold-startexaminees thebatchsizeandlearningratearefixedat256and0,respectively,\ninthecurrent(target)domain,meaningtheirresponserecordsin inthisphase.WeinitializeallparametersusingXavier[10],anduse\notherdomainsareincludedinthetrainingset.Thetrainingset theAdam[16]optimizer.IntheCATtestingphase,thequestion\nisonlyusedfortrainingDSCRandthelearning-basedquestion selectionalgorithmsfollowthesettingsintheoriginalpapers,with\nselectionalgorithm,preventingdataleakage. thetestlengthsetto1and5,andtherandomseedinalltheabove\nprocessesissetto0.AllexperimentsareconductedonanNVIDIA\n5.1.2 BaselineMethods. Todemonstratetheeffectivenessandcom- RTX4090GPU.\npatibilityofourframework,weappliedittofourwidelyusedCAT\nsystems,includingthestrategy-basedFisher[21],MAAT[1],BE- 5.2 OverallPerformance(RQ1)\nCAT[50],andthedata-drivenNCAT[49].Weselectcross-domain\nToverifytheeffectivenessoftheproposedframeworkinaddressing\nbaselinesforcomparison,wheretheRandomandOraclemethods\ntheCSIPchallenge,wecomparedDCSRwithothercross-domain\nrepresentthelowerandupperboundsofCATcold-startperfor-\nbaselinesusingbothsingleandmultidomainaspriorknowledge,\nmance,respectively.\nsettingthequestionselectionstepsto1and5.First,weexplored\n‚Ä¢ Random:Thismethodrandomlypredictstheinitialability leveragingasingledomainasthesourcedomain,encompassingsix\nof examinees in the target domain from aùëàùëõùëñùëìùëúùëüùëö(0,1) cross-domainscenarios.Werotatedeachdatasettoplaytherole\ndistribution,whichisthemostcommonmethodinexisting ofthetargetdomain,withotherdatasetsservingasthesourcedo-\nCATsystem. main.TheexperimentalresultspresentedinTable2showthatour\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada HaipingMaetal.\nFigure3:Comparisonofthepriorinformationprovidedby\nmultiandsingledomainintheNCD-MAATCATsystem.\nFigure5:Resultsunderextendedtestingproceduresinthe\nIRT-FisherCATsystem.\nHarmonizationandCalibrationModule(HCM)insection4.3(de-\nnotedasw/oHCM).CSUMprovidesdiffusionguidanceforthe\nDiffusionModule(DM),whileHCMconstrainstheDM‚Äôsoutputto\nmatchtheCATtask.AsshowninFigure4,wevalidatedtheimpact\nofdifferentcomponentsonDCSRintheNCD-MAATCATsystem,\nwithCsetasthetargetdomainandotherdomainsasthesource\ndomains.Specifically,whenCSUMisremovedfromDSCR,perfor-\nmancedropssignificantly,indicatingthatthespecificcognition\nofthesourcedomain,asaconfoundingvariable,hindersthetrue\ncausaleffect,leadingtonegativetransfer.Similarly,whenHCM\nFigure4:Ablationstudyonkeycomponents. isremoved,thereisalsoacertaindegreeofperformancedecline,\nsuggestingthatexcessiveuncertaintyaffectsthealignmentofthe\ngeneratedresultswiththeCATtask.Therefore,allcomponents\nproposedDCSRnotonlyoutperformsallbaselinesintheCATcold- contributetoDCSRtovaryingdegrees.\nstarttaskacrossallsixscenariosbutalsoadaptswelltostrategy-\nbasedandlearning-basedselectionalgorithms.Additionally,we 5.4 Performanceunderlongteststeps(RQ3)\nobservedthefollowing:(1)ComparedtothecommonlyusedRan- ToexploretheimpactofDCSRoncold-startperformanceundera\ndommethod,DCSRprovidesmoreaccurateinitialabilityestimates greedyalgorithm,asshowninFigure5,weextendedthetestlength\nforcold-startexaminees,especiallywhenthereisasignificantcor- intheIRT-FisherCATsystemandusedsingle-domaininformation\nrelationbetweenthesourceandtargetdomains,suchasCandC++, asthepriorinformationprovider,withthecommonlyusedRandom\nwhereDCSRclearlyoutperformsotherbaselinesandapproaches methodasthebaseline.Weobservedthatintheearlystagesof\ntheperformanceoftheOraclemethod.Thisdemonstratestheef- testing,regardlessofthesourcedomain,cold-startperformance\nfectivenessofusingexaminees‚Äôdomain-sharedcognitionasthe improvessignificantly.Notably,whenC++isusedasthesource\ntransfercondition.(2)ComparedtoMLCCM,DCSRdoesnotrely domain,theinitialeffectalreadysurpassestheRandommethod\nheavilyonsupervisedtraining,therebyavoidingoverfittingtothe after60roundsofquestionselectionduetothecorrelationbetween\nlabels.Moreover,inmostscenarios,theabilityassignedbyDCSR courses.ThisdemonstratesthatDCSRnotonlyalleviatestheissue\nattheinitialstageshowsbetterperformancethantheresultsdiag- ofselectionalgorithmsfallingintolocaloptimabutalsoguides\nnosedbyothermethodsaftermultipleroundsofselection. theselectionalgorithmbyprovidinganaccuratestartingpointfor\nNext,weexploredusingmulti-domaininformationasthesource testing.Additionally,evenwhenaweaklycorrelatedcourseisused\ndomain,coveringthreescenarioswhereC,C++,andDSwereset asthesourcedomain,theoptimalresultofthetraditionalRandom\nasthetargetdomains,withothercoursesservingasthesource methodcanbeachievedwithinashortertestlength.\ndomain.AsshowninFigure3,comparedtosingle-domaininfor-\nmation,themulti-domainenrichesthecommoncognition,andthe 5.5 Cold-StartinCognitiveDiagnosis(RQ4)\ncorrelationbetweencoursessignificantlyhelpsresolvetheCSIP.\nToexploretheperformanceofDCSRinaddressingthecold-start\nTheperformanceinthefirsttwoscenariosisparticularlyeffective\nproblemincognitivediagnosistask,wedirectlyappliedDCSRto\nduetotheoverlapofconceptsbetweenprogramminglanguages.\nCDM,includingunidimensionalIRTandmultidimensionalNCD.\nAsshowninFigure6,DCSRdemonstratessuperiorperformance\n5.3 AblationStudy(RQ2)\ninCDMacrossdifferentdimensions,approachingOracleperfor-\nThissectionprovidesanin-depthanalysisofhowkeycomponents manceinsomescenarios.Moreover,inhigh-dimensionalCDM,i.e.,\ninDCSRcontributetoaddressingtheCSIPchallenge.Weconducted NCD,DCSRfurtherenhancescold-startperformanceinCDtask.\nexperimentsbyindividuallyremovingtheCognitiveStateUnifica- ItsignificantlyoutperformsthecommonlyusedRandommethod.\ntionModule(CSUM)insection4.2(denotedasw/oCSUM)andthe Additionally, in scenarios where the correlation is not obvious,\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada\nFigure8:Caseanalysisofanindividualexaminee.\nFigure6:Performanceoncognitivediagnosiscoldstarttask.\napproachbetterservestheCATsystem,asunderestimatingabil-\nitycanguidetheselectionalgorithmtochooserelativelysimpler\nquestion,avoidingthenegativeemotionsassociatedwithoverly\ndifficultquestion.\n6 Conclusion\nInthispaper,weproposetheDiffusionCognitiveStatesTransfer\nFrameworktoaddresstheColdStartwithInsufficientPrior(CSIP)\ninComputerizedAdaptiveTesting(CAT).Thischallengecompels\nCATsystemstouseadditionalselectionstepstomitigatethecold\nstart issue. To address this, we first reconstruct the examinee‚Äôs\ninitialabilityinthetargetdomain,guidedbydiffusionprinciple.\nConcurrently, we analyze the causal relationships in the gener-\natedoutcomesfromamodel-basedcausalperspective,proposing\nthreedecouplingstrategiestoblockthetwobackdoorpathsthat\nFigure7:T-SNEvisualizationofabilitiesinthetargetdomain. hindercausaldiscovery.Subsequently,weintroduceconstraints\nfromtheperspectivesofconsistencyandtask-orientedtoenforce\nalignmentofthegeneratedoutcomeswiththeCATsystem.There-\nsuchasusingDSasthesourcedomainandC++asthetargetdo- fore,thisframeworkcanbeappliedtoexistingmainstreamCAT\nmain,DCSRshowsbetterperformancethanMLCCM,significantly systems.Finally,extensiveexperimentshighlighttheeffectiveness\noutperformingbaselines. andapplicabilityofourframework.\n5.6 AnalysisofInitializedAbility(RQ5) References\n[1] HaoyangBi,HaipingMa,ZhenyaHuang,YuYin,QiLiu,EnhongChen,YuSu,\nWefurtherexploredwhethertheabilitiesassignedbyDCSRto\nandShijinWang.2020.Qualitymeetsdiversity:Amodel-agnosticframework\nexamineesinthetargetdomainarereasonable.Weconductedthe forcomputerizedadaptivetesting.In2020IEEEInternationalConferenceonData\nanalysisfromtwoaspects.First,usinghigh-dimensionalNCDasthe Mining(ICDM).IEEE,42‚Äì51.\n[2] AndrewPBradley.1997. TheuseoftheareaundertheROCcurveinthe\nbackbone,wevisualizethedistributionofinitialabilitiesassigned\nevaluationofmachinelearningalgorithms. Patternrecognition30,7(1997),\ntoexamineesbydifferentmethodsacross12scenarios.Asshown 1145‚Äì1159.\ninFigure7,theabilitydistributionassignedbytheRandommethod [3] ShangChai,LianshengZhuang,andFengyingYan.2023.Layoutdm:Transformer-\nbaseddiffusionmodelforlayoutgeneration.InProceedingsoftheIEEE/CVF\nisclearlyspherical,whichisnotconducivetodistinguishingbe- ConferenceonComputerVisionandPatternRecognition.18349‚Äì18358.\ntweenexaminees‚Äôabilities.Incontrasttothelocalizedclustering [4] Hua-HuaChangandZhiliangYing.1996. Aglobalinformationapproachto\ncomputerizedadaptivetesting.AppliedPsychologicalMeasurement20,3(1996),\npresentedbyMLCCM,DCSRisclosertothatexhibitedbyOracle,\n213‚Äì229.\nshowingcleardifferentiation.Thisdemonstratesthatintroducing [5] SusanEEmbretsonandStevenPReise.2013.Itemresponsetheory.Psychology\nexaminees‚Äôcross-domaincommoncognitioncangeneratemore Press.\n[6] MehrdadFarajtabar,NavidAzizan,AlexMott,andAngLi.2020. Orthogonal\npersonalizedinitialabilities.Secondly,werandomlyselectedan\ngradientdescentforcontinuallearning.InInternationalConferenceonArtificial\nexaminee,obtainedhisdiagnosticfeedbackusingNCD,andran- IntelligenceandStatistics.PMLR,3762‚Äì3773.\ndomlyselectedtenknowledgeconceptstocalculatethedifference [7] WeiboGao,QiLiu,ZhenyaHuang,YuYin,HaoyangBi,Mu-ChunWang,Jianhui\nMa,ShijinWang,andYuSu.2021.RCD:Relationmapdrivencognitivediagnosis\nbetweenthediagnosisresultsofthecorrespondingdimensionsand forintelligenteducationsystems.InProceedingsofthe44thinternationalACM\ntheresultsofOracle.AsshowninFigure8,DCSRshowssignif- SIGIRconferenceonresearchanddevelopmentininformationretrieval.501‚Äì510.\n[8] WeiboGao,QiLiu,HaoWang,LinanYue,HaoyangBi,YinGu,FangzhouYao,\nicantlylowerfluctuationandtendstounderestimatetheability\nZhengZhang,XinLi,andYuanjingHe.2024.Zero-1-to-3:Domain-LevelZero-\nofexamineeratherthanoverestimatehimasMLCCMdoes.This ShotCognitiveDiagnosisviaOneBatchofEarly-BirdStudentstowardsThree\nKDD‚Äô25,August03‚Äì07,2025,Toronto,ON,Canada HaipingMaetal.\nDiagnosticObjectives.InProceedingsoftheAAAIConferenceonArtificialIntelli- [32] FeiWang,QiLiu,EnhongChen,ZhenyaHuang,YuyingChen,YuYin,ZaiHuang,\ngence,Vol.38.8417‚Äì8426. andShijinWang.2020. Neuralcognitivediagnosisforintelligenteducation\n[9] AritraGhoshandAndrewLan.2021.Bobcat:Bileveloptimization-basedcom- systems.InProceedingsoftheAAAIconferenceonartificialintelligence,Vol.34.\nputerizedadaptivetesting.arXivpreprintarXiv:2108.07386(2021). 6153‚Äì6161.\n[10] XavierGlorotandYoshuaBengio.2010.Understandingthedifficultyoftraining [33] FeiWang,QiLiu,EnhongChen,ZhenyaHuang,YuYin,ShijinWang,andYuSu.\ndeepfeedforwardneuralnetworks.InProceedingsofthethirteenthinternational 2022.NeuralCD:ageneralframeworkforcognitivediagnosis.IEEETransactions\nconferenceonartificialintelligenceandstatistics.JMLRWorkshopandConference onKnowledgeandDataEngineering35,8(2022),8312‚Äì8327.\nProceedings,249‚Äì256. [34] HangyuWang,TingLong,LiangYin,WeinanZhang,WeiXia,QichenHong,\n[11] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic DingyinXia,RuimingTang,andYongYu.2023.GMOCAT:AGraph-Enhanced\nmodels.Advancesinneuralinformationprocessingsystems33(2020),6840‚Äì6851. Multi-ObjectiveMethodforComputerizedAdaptiveTesting.InProceedingsof\n[12] YutingHong,ShiweiTong,WeiHuang,YanZhuang,QiLiu,EnhongChen,Xin the29thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining.\nLi,andYuanjingHe.2023. Search-EfficientComputerizedAdaptiveTesting. 2279‚Äì2289.\nInProceedingsofthe32ndACMInternationalConferenceonInformationand [35] WenjieWang,YiyanXu,FuliFeng,XinyuLin,XiangnanHe,andTat-SengChua.\nKnowledgeManagement.773‚Äì782. 2023.Diffusionrecommendermodel.InProceedingsofthe46thInternationalACM\n[13] YuHou,Jin-DukPark,andWon-YongShin.2024.CollaborativeFilteringBased SIGIRConferenceonResearchandDevelopmentinInformationRetrieval.832‚Äì841.\nonDiffusionModels:UnveilingthePotentialofHigh-OrderConnectivity.In [36] YunerXuan.2024. DiffusionCross-domainRecommendation. arXivpreprint\nProceedingsofthe47thInternationalACMSIGIRConferenceonResearchand arXiv:2402.02182(2024).\nDevelopmentinInformationRetrieval.1360‚Äì1369. [37] ShangshangYang,MingyangChen,ZiwenWang,XiaoshanYu,PanpanZhang,\n[14] LiyaHu,ZhiangDong,JingyuanChen,GuifengWang,ZhihuaWang,ZhouZhao, HaipingMa,andXingyiZhang.2024.DisenGCD:AMetaMultigraph-assisted\nandFeiWu.2023.PTADisc:across-coursedatasetsupportingpersonalizedlearn- DisentangledGraphLearningFrameworkforCognitiveDiagnosis.arXivpreprint\ningincold-startscenarios.AdvancesinNeuralInformationProcessingSystems36 arXiv:2410.17564(2024).\n(2023),44976‚Äì44996. [38] ShangshangYang,HaipingMa,YingBi,YeTian,LimiaoZhang,YaochuJin,\n[15] YangqinJiang,YuhaoYang,LianghaoXia,andChaoHuang.2024.Diffkg:Knowl- andXingyiZhang.2024.Anevolutionarymulti-objectiveneuralarchitecture\nedgegraphdiffusionmodelforrecommendation.InProceedingsofthe17thACM searchapproachtoadvancingcognitivediagnosisinintelligenteducation.IEEE\nInternationalConferenceonWebSearchandDataMining.313‚Äì321. TransactionsonEvolutionaryComputation(2024).\n[16] DiederikKingmaandJimmyBa.2014.Adam:AMethodforStochasticOptimiza- [39] ShangshangYang,LinruiQin,andXiaoshanYu.2024.EndowingInterpretability\ntion.ComputerScience(2014). forNeuralCognitiveDiagnosisbyEfficientKolmogorov-ArnoldNetworks.arXiv\n[17] ZihaoLi,AixinSun,andChenliangLi.2023. Diffurec:Adiffusionmodelfor preprintarXiv:2405.14399(2024).\nsequentialrecommendation. ACMTransactionsonInformationSystems42,3 [40] ShangshangYang,YeTian,ChengHe,XingyiZhang,KayChenTan,andYaochu\n(2023),1‚Äì28. Jin.2021. Agradient-guidedevolutionaryapproachtotrainingdeepneural\n[18] QidongLiu,FanYan,XiangyuZhao,ZhaochengDu,HuifengGuo,RuimingTang, networks. IEEETransactionsonNeuralNetworksandLearningSystems33,9\nandFengTian.2023.Diffusionaugmentationforsequentialrecommendation. (2021),4861‚Äì4875.\nInProceedingsofthe32ndACMInternationalConferenceonInformationand [41] ShangshangYang,HaoyuWei,HaipingMa,YeTian,XingyiZhang,YunboCao,\nKnowledgeManagement.1576‚Äì1586. andYaochuJin.2023. Cognitivediagnosis-basedpersonalizedexercisegroup\n[19] QiLiu,YanZhuang,HaoyangBi,ZhenyaHuang,WeizheHuang,JiatongLi, assemblyviaamulti-objectiveevolutionaryalgorithm. IEEETransactionson\nJunhaoYu,ZiruiLiu,ZiruiHu,YutingHong,etal.2024. SurveyofCom- EmergingTopicsinComputationalIntelligence7,3(2023),829‚Äì844.\nputerizedAdaptiveTesting:AMachineLearningPerspective. arXivpreprint [42] ShangshangYang,XiaoshanYu,YeTian,XuemingYan,HaipingMa,andXingyi\narXiv:2404.00712(2024). Zhang.2024.Evolutionaryneuralarchitecturesearchfortransformerinknowl-\n[20] ShuhuanLiu,XiaoshanYu,HaipingMa,ZiwenWang,ChuanQin,andXingyi edgetracing.AdvancesinNeuralInformationProcessingSystems36(2024).\nZhang.2023.HomogeneousCohort-AwareGroupCognitiveDiagnosis:AMulti- [43] ShangshangYang,ChengZhen,YeTian,HaipingMa,YuanchaoLiu,Panpan\ngrainedModelingPerspective.InProceedingsofthe32ndACMInternational Zhang,andXingyiZhang.2023.Evolutionarymulti-objectiveneuralarchitec-\nConferenceonInformationandKnowledgeManagement.4094‚Äì4098. turesearchforgeneralizedcognitivediagnosismodels.In20235thInternational\n[21] FredericMLord.2012. Applicationsofitemresponsetheorytopracticaltesting ConferenceonData-drivenOptimizationofComplexSystems(DOCS).IEEE,1‚Äì10.\nproblems.Routledge. [44] ZhengyiYang,JiancanWu,ZhicaiWang,XiangWang,YanchengYuan,and\n[22] ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. XiangnanHe.2024.Generatewhatyouprefer:Reshapingsequentialrecommen-\n2022.Dpm-solver:Afastodesolverfordiffusionprobabilisticmodelsamplingin dationviaguideddiffusion.AdvancesinNeuralInformationProcessingSystems\naround10steps.AdvancesinNeuralInformationProcessingSystems35(2022), 36(2024).\n5775‚Äì5787. [45] XiaoshanYu,ChuanQin,DazhongShen,HaipingMa,LeZhang,XingyiZhang,\n[23] HaipingMa,SiyuSong,ChuanQin,XiaoshanYu,LimiaoZhang,XingyiZhang, HengshuZhu,andHuiXiong.2024.Rdgt:enhancinggroupcognitivediagnosis\nandHengshuZhu.2024.DGCD:AnAdaptiveDenoisingGNNforGroup-level withrelation-guideddual-sidegraphtransformer.IEEETransactionsonKnowledge\nCognitiveDiagnosis.InThe33rdInternationalJointConferenceonArtificialIntel- andDataEngineering(2024).\nligence(IJCAI-24). [46] XiaoshanYu,ChuanQin,DazhongShen,ShangshangYang,HaipingMa,Hengshu\n[24] HaipingMa,ChangqianWang,HengshuZhu,ShangshangYang,Xiaoming Zhu,andXingyiZhang.2024. Rigl:Aunifiedreciprocalapproachfortracing\nZhang,andXingyiZhang.2024. Enhancingcognitivediagnosisusingun- theindependentandgrouplearningprocesses.InProceedingsofthe30thACM\ninteractedexercises:Acollaboration-awaremixedsamplingapproach.InPro- SIGKDDConferenceonKnowledgeDiscoveryandDataMining.4047‚Äì4058.\nceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.8877‚Äì8885. [47] XiaoshanYu,ChuanQin,QiZhang,ChenZhu,HaipingMa,XingyiZhang,\n[25] HaokaiMa,RuobingXie,LeiMeng,XinChen,XuZhang,LeyuLin,andZhan- andHengshuZhu.2024. DISCO:AHierarchicalDisentangledCognitiveDi-\nhuiKang.2024. Plug-indiffusionmodelforsequentialrecommendation.In agnosisFrameworkforInterpretableJobRecommendation. arXivpreprint\nProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.8886‚Äì8894. arXiv:2410.07671(2024).\n[26] HaipingMa,YongYang,ChuanQin,XiaoshanYu,ShangshangYang,Xingyi [48] JujiaZhao,WangWenjie,YiyanXu,TengSun,FuliFeng,andTat-SengChua.2024.\nZhang,andHengshuZhu.2024.HD-KT:AdvancingRobustKnowledgeTracing Denoisingdiffusionrecommendermodel.InProceedingsofthe47thInternational\nviaAnomalousLearningInteractionDetection.InProceedingsoftheACMonWeb ACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.\nConference2024.4479‚Äì4488. 1370‚Äì1379.\n[27] HaipingMa,YiZeng,ShangshangYang,ChuanQin,XingyiZhang,andLimiao [49] YanZhuang,QiLiu,ZhenyaHuang,ZhiLi,ShuanghongShen,andHaipingMa.\nZhang.2023.Anovelcomputerizedadaptivetestingframeworkwithdecoupled 2022.Fullyadaptiveframework:Neuralcomputerizedadaptivetestingforonline\nlearningselector.Complex&IntelligentSystems9,5(2023),5555‚Äì5566. education.InProceedingsoftheAAAIconferenceonartificialintelligence,Vol.36.\n[28] WimJVanderLindenandCeesAWGlas.2000.Computerizedadaptivetesting: 4734‚Äì4742.\nTheoryandpractice.Springer. [50] YanZhuang,QiLiu,GuanHaoZhao,ZhenyaHuang,WeizheHuang,Zachary\n[29] Jill-J√™nnVie,FabricePopineau,√âricBruillard,andYolaineBourda.2017.Areview Pardos,EnhongChen,JinzeWu,andXinLi.2024.Aboundedabilityestimation\nofrecentadvancesinadaptiveassessment. Learninganalytics:Fundaments, forcomputerizedadaptivetesting. AdvancesinNeuralInformationProcessing\napplications,andtrends:Aviewofthecurrentstateofthearttoenhancee-learning Systems36(2024).\n(2017),113‚Äì142.\n[30] HowardWainer,NeilJDorans,RonaldFlaugher,BertFGreen,andRobertJ\nMislevy.2000.Computerizedadaptivetesting:Aprimer.Routledge.\n[31] JoojoWalker,TingZhong,FengliZhang,QiangGao,andFanZhou.2022.Rec-\nommendationviacollaborativediffusiongenerativemodel.InInternationalCon-\nferenceonKnowledgeScience,EngineeringandManagement.Springer,593‚Äì605.",
    "pdf_filename": "Diffusion-Inspired_Cold_Start_with_Sufficient_Prior_in_Computerized_Adaptive_Testing.pdf"
}