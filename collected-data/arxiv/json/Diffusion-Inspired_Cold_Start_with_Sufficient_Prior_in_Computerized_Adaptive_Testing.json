{
    "title": "Diffusion-Inspired Cold Start with Sufficient Prior in",
    "abstract": "ComputerizedAdaptiveTesting(CAT)aimstoselectthemostap- ables,therebyblockingbackdoorpathsthathindercausaldiscov- propriatequestionsbasedontheexamineeâ€™sabilityandiswidely ery.Giventhatexcessiveuncertaintycanaffecttheapplicability usedinonlineeducation.However,existingCATsystemsoften ofgeneratedresultstotheCATsystem,weproposeconsistency lackinitialunderstandingoftheexamineeâ€™sability,requiringran- constraintandtask-orientedconstrainttocontroltherandomness dom probing questions. This can lead to poorly matched ques- ofthegeneratedresultsandtheirrelevancetotheCATtask,re- tions,extendingthetestdurationandnegativelyimpactingthe spectively.OurDCSRcanseamlesslyapplythegeneratedinitial examineeâ€™smindset,aphenomenonreferredtoastheColdStart abilitystatesinthetargetdomaintoexistingquestionselectional- withInsufficientPrior(CSIP)task.ThisissueoccursbecauseCAT gorithms,thusimprovingthecoldstartperformanceoftheCATsys- systemsdonoteffectivelyutilizetheabundantpriorinformation tem.Extensiveexperimentsconductedonfivereal-worlddatasets abouttheexamineeavailablefromothercoursesononlineplat- demonstratethatDCSRsignificantlyoutperformsexistingbase- forms.Theseresponserecords,duetothecommonalityofcogni- linemethodsinaddressingtheCSIPtask.Thecodeisavailableat: tivestatesacrossdifferentknowledgedomains,canprovidevalu- https://github.com/BIMK/Intelligent-Education/tree/main/DCSR. ablepriorinformationforthetargetdomain.However,noprior CCSConcepts work has explored solutions for the CSIP task. In response to thisgap,weproposeDiffusionCognitiveStatesTransfeRFrame- â€¢Appliedcomputingâ†’Computer-assistedinstruction. work(DCSR),anoveldomaintransferframeworkbasedonDiffu-",
    "body": "Diffusion-Inspired Cold Start with Sufficient Prior in\nComputerized Adaptive Testing\nHaipingMa AoqingXia ChangqianWang\nInformationMaterialsandIntelligent TheInstitutesofPhysicalScienceand TheInstitutesofPhysicalScienceand\nSensingLaboratoryofAnhui InformationTechnology,Anhui InformationTechnology,Anhui\nProvince,theInstitutesofPhysical University University\nScienceandInformationTechnology, Hefei,Anhui,China Hefei,Anhui,China\nAnhuiUniversity q23301252@stu.ahu.edu.cn changqian.wang.dl@gmail.com\nHefei,Anhui,China\nhpma@ahu.edu.cn\nHaiWang XingyiZhangâˆ—\nTheInstitutesofPhysicalScienceand SchoolofComputerScienceand\nInformationTechnology,Anhui Technology,AnhuiUniversity\nUniversity Hefei,Anhui,China\nHefei,Anhui,China xyzhanghust@gmail.com\nq22201135@stu.ahu.edu.cn\nAbstract designedthreedecouplingstrategiestocontrolconfoundingvari-\nComputerizedAdaptiveTesting(CAT)aimstoselectthemostap- ables,therebyblockingbackdoorpathsthathindercausaldiscov-\npropriatequestionsbasedontheexamineeâ€™sabilityandiswidely ery.Giventhatexcessiveuncertaintycanaffecttheapplicability\nusedinonlineeducation.However,existingCATsystemsoften ofgeneratedresultstotheCATsystem,weproposeconsistency\nlackinitialunderstandingoftheexamineeâ€™sability,requiringran- constraintandtask-orientedconstrainttocontroltherandomness\ndom probing questions. This can lead to poorly matched ques- ofthegeneratedresultsandtheirrelevancetotheCATtask,re-\ntions,extendingthetestdurationandnegativelyimpactingthe spectively.OurDCSRcanseamlesslyapplythegeneratedinitial\nexamineeâ€™smindset,aphenomenonreferredtoastheColdStart abilitystatesinthetargetdomaintoexistingquestionselectional-\nwithInsufficientPrior(CSIP)task.ThisissueoccursbecauseCAT gorithms,thusimprovingthecoldstartperformanceoftheCATsys-\nsystemsdonoteffectivelyutilizetheabundantpriorinformation tem.Extensiveexperimentsconductedonfivereal-worlddatasets\nabouttheexamineeavailablefromothercoursesononlineplat- demonstratethatDCSRsignificantlyoutperformsexistingbase-\nforms.Theseresponserecords,duetothecommonalityofcogni- linemethodsinaddressingtheCSIPtask.Thecodeisavailableat:\ntivestatesacrossdifferentknowledgedomains,canprovidevalu- https://github.com/BIMK/Intelligent-Education/tree/main/DCSR.\nablepriorinformationforthetargetdomain.However,noprior\nCCSConcepts\nwork has explored solutions for the CSIP task. In response to\nthisgap,weproposeDiffusionCognitiveStatesTransfeRFrame- â€¢Appliedcomputingâ†’Computer-assistedinstruction.\nwork(DCSR),anoveldomaintransferframeworkbasedonDiffu-\nsionModels(DMs)toaddresstheCSIPtask.Specifically,wecon- Keywords\nstructacognitivestatetransitionbridgebetweendomains,guided ComputerizedAdaptiveTesting,IntellegentEducation\nbythecommoncognitivestatesofexaminees,encouragingthe\nACMReferenceFormat:\nmodeltoreconstructtheinitialabilitystateinthetargetdomain.\nHaipingMa,AoqingXia,ChangqianWang,HaiWang,andXingyiZhang.\nToenrichtheexpressivepowerofthegenerateddata,weanalyze\n2025.Diffusion-InspiredColdStartwithSufficientPriorinComputerized\nthecausalrelationshipsinthegenerationprocessfromacausal\nAdaptiveTesting.InProceedingsofMakesuretoenterthecorrectconference\nperspective.Redundantandextraneouscognitivestatescanlead\ntitlefromyourrightsconfirmationemai(KDDâ€™25).ACM,NewYork,NY,\nto limited transfer and negative transfer effects. Therefore, we USA,10pages.https://doi.org/XXXXXXX.XXXXXXX\nâˆ—Correspondingauthor.\n1 Introduction\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nAsartificialintelligenceempowereducation,computerizedadap-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation tivetesting(CAT)ononlineeducationplatformshavegarnered\nonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe extensiveattention[38,39,42,45].CATaimstoprovideexaminees\nauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or\nwithasmallnumberofappropriatequestionstoprogressivelyas-\nrepublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/orafee.Requestpermissionsfrompermissions@acm.org. sesstheircognitivestatesinspecificdomains[19,29,30].Typically,\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada CATconsistsoftwoiterativecomponents:thecognitivediagnostic\nÂ©2025Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\nmodel(CDM)andthequestionselectionalgorithm.Asshownin\nACMISBN978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX Figure1(a),theCDMestimatesexamineeâ€™sabilitybasedonher\n4202\nvoN\n91\n]GL.sc[\n1v28121.1142:viXra\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada HaipingMaetal.\ntransferableattributes.Thesepre-diagnosedabilitiescanprovide\nrichpriorinformationforthetarget(coldstart)course,allowingthe\nCATsystemtograsptheexamineeâ€™sabilityrangeinadvance.For\nexample,ifanexamineeperformswellinmathematics,especiallyin\ncomplexalgebraandgeometryquestions,theCATsystemcaninfer\nthathealsopossessstronglogicalthinkingandproblem-solving\nskillsinphysics,therebyassigningahigherinitialability.Toour\nknowledge,noresearchhasyetutilizedthesecross-coursedatato\nsolvetheCSIPtask,despiteitssignificantandpracticalimportance.\nTofullyleveragethesecross-courseresponserecordsandprovide\npriorinformationaboutexamineestoCATsystem,weintegrate\nFigure1:Illustrationof(a)typicalCATprocess,and(b)the andtransferthepre-diagnosisresultsofexamineestothetarget\ndilemmaofCATundercoldstart. domainbasedontheconceptofDiffusionModels(DMs)[3,11].\nDMsaddnoiseincrementallyandthenreconstructthecorrupted\ndatastepbystepinreverse,whichnotonlyaddressesnoisyre-\nresponse at stepğ‘¡ âˆ’1 [26, 43, 46, 47], after which the question sponserecordsinthesourcedomainbutalsoaptlymeetstheneeds\nselectionalgorithmprovidessuitablequestionforthenextstep, ofsolvingtheCSIPtask.ThisisduetotheabilityofDMstograd-\nthusenablingamoreaccurateassessmentofexamineeâ€™sability. uallymergecomplexmulti-distributioncross-domaindataintoa\nExistingresearchonthequestionselectionalgorithm,acritical unifiedlatentspaceduringdenoising,generatingaunifiedlatent\ncomponentofCAT,canbecategorizedintopolicy-based[1,50]and representation,whichreconstructstheinitialabilityofthetarget\nlearnable-based[34,49]approaches.Theformergenerallyconverts domainwithspecifictransferattributesfromthenoise.However,\nthequestionselectionprocessintomodelexpectation,whilethe despitethegreatpotentialofusingDMstoaddresstheCSIPtask,\nlatterdefinestheCATprocessasabi-leveloptimizationproblem. weuncoverchallengesintermsofwhatandhow:(1)Whatkindof\nDespitetheadvancementsmadebyvariousquestionselection sourcedomaininformationcanbeinjectedintothereversedenois-\nalgorithms,theystillfacechallengesrelatedtothecoldstartprob- ingprocessasguidance.And(2)Howtoconstraintheoutputto\nlemintheCATprocess.AsillustratedinFigure1(b),onanonline matchtheCATsystem,implyingthatexcessiveuncertaintyinthe\neducationplatform,thesysteminitiallyhasnoknowledgeofthe generatedresultscanleadtodataunsuitableforCATtask.\nexamineeâ€™sability,causingtheCATsystemtotentativelyselect Toaddressthesechallenges,wedesignanoveldomaintransfer\nquestionsrandomlytodeterminethestartingpointofthetest.The frameworknamedtheDiffusionCognitiveStatesTransfeRFrame-\ndifficulty and content of the question might not align with the work(DCSR)fortheCSIPtask.GuidedbytheprincipleofDMs,\nexamineeâ€™sactualability,leadingtoconfusionorfrustrationand wegenerateinitialabilitiesinthetargetdomainforexamineesto\nincreasingthetimecostoftheassessment.Additionally,theques- enhancethecold-startperformanceofCATsystem.Specifically,\ntionselectionprocessisaniterativeMarkovchain.Thismeans topreventthelossofpersonalizedinformationinthediffusion\nthatifthequestionselectedintheinitialstagesdonotmatchthe generationresults,weconditionthedenoiseronexamineesâ€™prior\nexamineeâ€™sability,itmayleadtobiasedquestionselectionsinsub- cognitiveabilitiestoincorporatepersonalizedtargetdomainabil-\nsequentstages.Mostquestionselectionalgorithmsbasedongreedy ities.Toavoidredundantinformationandnegativetransfer,we\nstrategiestendtoamplifythisbias,harmingtheperformanceofthe analyzethecausalrelationshipsbetweendifferentvariablesfrom\nCATsystem.Thesealgorithms,althoughcapableofquicklyfinding thecausalperspectiveofmodelanddesignthreedecouplingstrate-\nlocaloptimaincertainscenarios,caneasilygettrappedinlocal giestoadjustconfoundingvariablesinthebackdoorpath,including\noptimaovermultipleroundsofquestionselection.Inourpaper, domain-sharedcognition,domain-specificcognition,andorthogo-\nwerefertothistaskasColdStartwithInsufficientPriors(CSIP), nalregularization-basedgradientseparation,blockingpathsthat\nwhicharisesfromtheinsufficientunderstandingoftheexaminee obscuretruecausalrelationships.Tomitigatetheexcessiveuncer-\nbythequestionselectionalgorithmattheinitialstageofthetest, taintyofDMsandenhancetheadaptabilitytoCATtask,wedesign\nleadingtouncertainstartingpointsforselectionandsubsequently consistencyconstraintandtask-orientedconstrainttocontrolran-\naffectingtheadaptivetestingprocess. domnessandmatchtheinputrequirementsofCATsystem.The\nWiththewidespreadapplicationofeducationalplatforms,vast resultsgeneratedbyDCSRseamlesslyintegratewithexistingques-\namountsofexamineedataarecollectedandstored[14],providing tionselectionalgorithms,improvingthecold-startperformance\nvaluableresourcestoenrichthepriorinformationforCATsystem. ofCATsystem.Extensiveexperimentalresultsonfivereal-world\nNotably,historicalresponserecordsofexamineesinothercourses datasetsdemonstratethatDCSReffectivelyaddressestheCSIPtask\ncanserveascrucialreferencesforCATsystemtopreliminarily andsignificantlyoutperformsbaselines.\nunderstandexamineeabilities.Therearecertaincommonalities\namongknowledgeconceptsacrossdifferentcourses.Forexample, 2 RelatedWork\nmanyphysicsquestions,suchasthoseinkinematicsanddynam-\n2.1 ComputerizedAdaptiveTesting\nics,requirealgebraicequationsolving,whichisfundamentalin\nmathematics.Therefore,anexamineeâ€™sresponserecordsacross ComputerizedAdaptiveTesting(CAT)aimstoevaluateanexam-\nmultiplecoursescanreflecttheirfoundationalabilitiesandcogni- ineeâ€™sabilityprogressivelywithinashortertestlength[12,27].\ntivecommonalitiesinmultipleinterdisciplinaryfields,whichare CATsystemsprimarilyconsistoftwocomponents:(1)Cognitive\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDDâ€™25,August03â€“07,2025,Toronto,ON,Canada\nDiagnosticModels(CDMs)fordiagnosingexamineesâ€™abilities Model(CDM)M,whichassessestheexamineeâ€™sabilitystateafter\nbasedontheirresponsestoselectedquestions[20,23,24,37,41]. respondingtotheselectedquestions,and(2)aquestionselection\nItemResponseTheory(IRT)[5],awidelyusedCDM,assumesuni- algorithmÎ ,whichchoosesthenextmostinformativequestion\ndimensionalindependenceandusescontinuouslatentvariablesto basedontheexamineeâ€™scurrentabilitystatetomeasuretheirabil-\nassessexamineesâ€™latentabilities.Withthewidespreadapplication itymoreprecisely.Thesetwocomponentsalternatelyiterateuntil\nofdeepneuralnetworks,neuralCDMssuchasNCD[32],RCD[7], aterminationconditionismet.\nandKaNCD[33]utilizeneuralnetworkstocapturecomplexinter- Specifically,givenaexamineeğ‘’ ğ‘– andhis/hercandidatequestion\nactionsbetweenexamineesandquestions,enablingfine-grained setQğ‘–,theCATprocesscanberepresentedas:\nd adia ag pn tio vs et li yc cm ho od oe selin thg e.( n2 e) xQ tau pe ps rt oio prn ias te el qe uc eti so tin ona blg ao ser dit oh nm ths eai Cm Dt Mo ğ‘ğ‘¡ â†Î (ğ‘„\nğ‘–\n|ğœƒ ğ‘–ğ‘¡âˆ’1),\n(1)\nfeedback.Earlyquestionselectionalgorithmsweremodel-specific, ğœƒ ğ‘–ğ‘¡ â†M(ğ‘Ÿ ğ‘–ğ‘ğ‘¡ |ğ‘ğ‘¡,ğœƒ ğ‘–ğ‘¡âˆ’1),\nsuchasIRT-specificMaximumFisherInformation[21],Kullback- whereğ‘ğ‘¡ denotesthequestionselectedbyÎ basedontheexam-\nL the eib sele sr imIn pf lo er am lga ot ri io tn hmIn sd ce ox ul[ d4 n], oa tn md eeM tta hx eE evn otr lvo ip ny gn[2 e8 e] d. sH ofo Cw Dev Me sr,\n,\nineeâ€™sabilityğœƒ ğ‘–ğ‘¡âˆ’1 attimeğ‘¡ âˆ’1fromthecandidatequestionset\nQğ‘–,whichonlyincludesquestionsthattheexamineeğ‘’ ğ‘– hasnotyet\nleadingtoperformancelimitations.Therefore,amodel-agnostic\nquestionselectionalgorithm,MAAT[1]wasproposed,leverag-\nrespondedto.Thevariableğ‘Ÿ ğ‘–ğ‘ğ‘¡ representstheresponseresultof\ning active learning to transform the question selection process examineeğ‘’ ğ‘– toquestionğ‘ğ‘¡ .Notethatğ‘‡ isthemaximumnumberof\nstepsforterminatingthetest.\nintochoosingquestionswiththegreatestexpectedmodelchange.\nSimilarly,BECAT[50]employsanexpectedgradientdifference 3.1.2 Coldstartwithinsufficientpriors. IntheColdStartwithIn-\napproach,treatingthequestionselectionprocessasasubsetselec- sufficientPriors(CSIP)scenario,thequestionselectionalgorithm\ntionproblemguidedbytheoreticalestimatesofexamineesâ€™true considersonlytheinformationfromthetargetdomainwhileig-\nabilities.Anotherresearchdirectionfocusesondata-drivenques- noringtherichdatafromthesourcedomains.Toalleviatethislack\ntionselectionalgorithms.Forexample,BOBCAT[9],NCAT[49], ofpriorinformation,wedefineğ‘€sourcedomainsS 1,S 2,...,Sğ‘€\nandGMOCAT[34]definetheCATtaskasabi-leveloptimization andonetargetdomainT intheDiffusionCognitiveStateTransfer\nproblem,usingreinforcementlearningtolearnquestionselection Framework(DCSR).Eachdomainincludesthreegroupsofenti-\nalgorithmfromlarge-scaleresponsedata.However,fromamodel ties.WeuseE Sğ‘š,Q Sğ‘š andC Sğ‘š todenotethesetsofexaminees,\nperspective,thecrucialCDMcomponentincurrentCATsystems questions,andknowledgeconceptsintheğ‘šâˆˆğ‘€sourcedomain,\nrandomlyinitializesexamineesâ€™abilitiesatthetestâ€™soutset.This respectively.Similarly,E T,Q T andC T denotethethreeentitiesin\nrandominitializationnecessitatesmorequestionselectionsteps thetargetdomain.Theoverlappingexamineesbetweendomains\nforgreedyalgorithmstounderstandtheexamineeâ€™sabilityrange, aredefinedasE O âˆ‹E Oğ‘š =E Sğ‘š âˆ©E T,whiletheothertwoentity\ntherebyexacerbatingthecoldstartprobleminCATsystem. groupsgenerallydonothaveoverlappingelementsacrossdomains.\n2.2 DiffusionModel 3.1.3 Training and Testing Phases. In the given CAT testing\nplatform, both warm-start and cold-start, meaning the CAT\nDiffusionModels(DMs)haveachievedimpressiveresultsinimage\nsystem has never encountered before, are included in the\ngeneration[3,11].Totransferthesesuccessestootherdomains,re-\ntarget domain. Their response records can be represented as\nwce in thtw oo thrk ers[ fi1 e5 l, d2 s5 ., C4 O8] Dh IGav Ee Matt [e 3m 1]p wte ad sto thb eri fid rg se tt th oe ei xm ta eg ne dd to hm ea di en - R T =(cid:110) Rğ‘¤ Tğ‘ğ‘Ÿğ‘š,Rğ‘ Tğ‘œğ‘™ğ‘‘(cid:111) =(cid:110) (ğ‘’ ğ‘–,ğ‘ ğ‘—,ğ‘Ÿ ğ‘–ğ‘—) |ğ‘’ ğ‘– âˆˆEğ‘¤ Tğ‘ğ‘Ÿğ‘šâˆªEğ‘ Tğ‘œğ‘™ğ‘‘(cid:111) ,\nwhere ğ‘Ÿ ğ‘–ğ‘— = 1 indicates that examinee ğ‘’ ğ‘– answered question\nnoisingmoduleinDMstorecommendationsystems.DiffuRec[17],\nDreamRec[44]andCF-Diff[13]modelthelatentrepresentations ğ‘ ğ‘— âˆˆ Q T correctly, and ğ‘Ÿ ğ‘–ğ‘— = 0 otherwise. Similarly, the re-\nsponse records in theğ‘š source domain can be represented as\no e nrf oai ttt ee om np les yra s rn o ed n duau l cs i eze er gdp er nre e ef p re r ar e te isn oec nne t cs a, otg i so tu snid s b.i un D tg i afft lh sRe oecd ae c[n h3o 5 ie]i vs ai en n tg d em mDo piffd ouu raAle lS mt Ro o[g d1e e8n l] -- rR eS cğ‘š ord= s(cid:8) a( rğ‘’ eğ‘–,ğ‘ değ‘—, nğ‘Ÿ oğ‘–ğ‘— t) ed|ğ‘’ ağ‘– sâˆˆ RE SOğ‘š =, (cid:8)ğ‘ Rğ‘— Sâˆˆ 1,Q RQ Sğ‘š 2,(cid:9) ., .a .n ,Rda Sl ğ‘šls (cid:9)o .u Tr oce pd ro em vea nin\nt\ndata leakage, the aforementioned response sets are uniformly\ni Mng aro kf oi vnt ce hr aa ic nti mon odse eq liu ne gn cc he as. raM co ter re io sv tie cr s, om fo Dre Mr ses te oa er xc ph ll oe rv ee sr ea qg ues enth cee d Div ğ‘¡ğ‘’id ğ‘ ğ‘¡ed =i Rnt ğ‘o ğ‘œğ‘™ğ‘‘a .trainingsetDğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ={Rğ‘¤ Tğ‘ğ‘Ÿğ‘š,R S}andatestset\nmodeling,withfewworksutilizingdiffusionfeaturestostudycross- T\nOurDCSRconsistsoftwophases:trainingtheinitialabilities\ndomainproblems.AlthoughDiffCDR[36]hasmadepreliminary\nofexamineesinthesourcedomainbasedonDğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› andtesting\nexplorationsinthisarea,itintroducesredundantinformationand\ntheperformanceofquestionselectionintheCATsystemwithout\nfaceslimitationsincross-domainrepresentationcapacitydueto\nlearning,basedonDğ‘¡ğ‘’ğ‘ ğ‘¡.\nthediverseentitiesinCATsystem.\n3.1.4 Pre-EstablishCognitiveStates. Thedatainthetrainingset\n3 Preliminary Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›comesfromtheresponserecordsofoverlappingexaminees\n3.1 ComputerizedAdaptiveTesting E O betweendomains.Wepre-traintheseexamineestoestablish\ntheircognitivestatesinboththesourceandtargetdomains.The\n3.1.1 TaskIntroduction. Inanonlineeducationplatform,theCom-\npre-trainingprocessisexpressedas:\nputerizedAdaptiveTesting(CAT)systemadaptivelyselectsques-\ntionsforexamineestoaccuratelyrevealtheircognitiveabilities.The ğœƒâˆ—=argmin âˆ‘ï¸ L(cid:0)ğ‘Ÿ ğ‘–ğ‘—,MÎ¨(ğ‘ ğ‘—|ğœƒ ğ‘–)(cid:1), (2)\nCATsystemcomprisestwocomponents:(1)aCognitiveDiagnosis ğœƒâˆˆÎ˜ (ğ‘’ğ‘–,ğ‘ğ‘—,ğ‘Ÿğ‘–ğ‘—)âˆˆDğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada HaipingMaetal.\nFigure2:TheoverviewofDCSR:Theleftsidecorrespondstopre-training.Whiletheblue,yellow,andgreenmodulesare\nusedfortraining,andtheorangemodulescorrespondtotheapplicationinCAT.Additionally,therightsidedepictsthecausal\ndiscoveryinthegenerationprocess.\nwhere MÎ¨ is the CDM used for pre-training, and different do- isbuiltupontheDiffusionModuleandissupportedbytwokey\nmain data correspond to different CDM parameters, i.e., Î¨ = components:theCognitiveStateUnificationModule(CSUM)and\n{ğœ“ S 1,ğœ“ S 2,...,ğœ“ Sğ‘€,ğœ“ T}. The abilities of overlapping examinees theHarmonizationandCalibrationModule(HCM).Specifically,the\naredenotedasÎ˜ S = {ğœƒ ğ‘– |ğ‘– âˆˆE O},andsimilarly,theabilitiesin DiffusionModuleusespriorabilitiesfromthesourcedomainto\nthetargetdomainaredenotedasÎ˜ T. reconstructthecognitivestateofexamineesinthetargetdomain\nfromnoise.Fromacausalmodelperspective,thegeneratedabili-\n3.2 DiffusionModel tiesareinfluencedbyredundantknowledge,anddomain-specific\ncognition may cause negative transfer. Therefore, the CSUM is\nDiffusion Models (DMs) have demonstrated exceptional perfor-\nemployedtocontrolconfoundingvariablesinthebackdoorpath,\nmanceinfieldssuchascomputervision[11].Typically,DMsconsist\ntherebyuncoveringthetruecausalrelationships.Additionally,to\noftwoparts:theforwardprocessandthereverseprocess.\nmitigateuncertaintyduringthediffusionprocess,wedesignedcon-\n3.2.1 ForwardProcess. Givenadatapointğ‘¥ 0âˆ¼ğ‘(ğ‘¥)sampledfrom sistencyandtask-orientedconstraintsinHCM,aimingtoensure\nthetruedatadistribution,theforwardprocessgraduallydegrades thatthegeneratedresultsadheretothetruedistributionandmeet\nğ‘¥ 0intostandardGaussiannoiseğ‘¥ ğ‘‡ âˆ¼N(0,1)byinjectingGaussian therequirementsofCATtask.Itisworthnotingthatourframe-\nnoiseoverğ‘‡ steps.Specifically,theprocessofconvertingğ‘¥ ğ‘¡âˆ’1to workdemonstratessignificantscalability,seamlesslyintegrating\nğ‘¥ ğ‘¡ inDMsisrepresentedasğ‘(ğ‘¥ ğ‘¡ |ğ‘¥ ğ‘¡âˆ’1)=N(ğ‘¥ ğ‘¡;âˆšï¸ 1âˆ’ğ›½ ğ‘¡ğ‘¥ ğ‘¡âˆ’1,ğ›½ ğ‘¡I), thegeneratedinitialabilitiesinthetargetdomainintoexistingCAT,\nwhereğ‘¡ âˆˆ {1,...,ğ‘‡} represents the diffusion steps, ğ›½ ğ‘¡ âˆˆ (0,1) therebyimprovingitscold-startperformance.\nisthepredefinednoiseschedulingcoefficient,andN denotesthe\n4.1 DiffusionModule\nGaussiandistribution.\nWeemploytheconceptofdiffusionasthebackboneofourmodel,\n3.2.2 ReverseProcess. Inthereverseprocess,DMslearntoremove\nestablishingabridgeforcognitivestatetransformationbetweenthe\ntheaddednoise,therebyrecoveringtheoriginaldatadistribution\nsourceandtargetdomains.ThepurposeoftheDiffusionModuleis\nğ‘¥ 0frompurenoise,aimingtointroduceminoruncertaintiesinthe\ntoincorporatepriorinformationfromthesourcedomainintothe\ngenerationprocess.Thisprocesslearnsaparameterizednetwork\ninitialabilityestimationprocessinthetargetdomain.Therefore,\nğ‘ ğ›¿(ğ‘¥ ğ‘¡âˆ’1 | ğ‘¥ ğ‘¡) toapproximatethereverseprocess,whichcanbe\nitinherentlyinvolvestwodistinctprocesses:theforwardnoise\nformalizedasğ‘ ğ›¿(ğ‘¥ ğ‘¡âˆ’1 |ğ‘¥ ğ‘¡) =N(ğ‘¥ ğ‘¡âˆ’1;ğœ‡ ğ›¿(ğ‘¥ ğ‘¡,ğ‘¡),Î£ ğ›¿(ğ‘¥ ğ‘¡,ğ‘¡)),where\nadditionprocessandthereversedenoisingprocess.\nğœ‡ ğ›¿ andÎ£ ğ›¿ arethemeanandvarianceoftheGaussiandistribution Duringtraining,wegraduallyinjectGaussiannoiseintothe\npredictedbyaneuralnetworkwithparametersğ›¿.\nabilityvectorsÎ˜ T ofexamineesinthetargetdomainoverğ‘‡ steps.\n4 Method Foraspecificexamineeğ‘’ ğ‘– âˆˆE O,hisabilityvectorğœƒ ğ‘–T 0 =ğœƒ ğ‘–T âˆˆÎ˜ T\nOverview. Thecoreideaofthisworkistotransferpriordiagnos-\niscorruptedintoğœƒ ğ‘–T 1:ğ‘‡,whichismodeledasaGaussiantransition\nMarkovchain:\nticresultsfromthesourcedomaintothetargetdomain,thereby\nğ‘‡\ng inen the era tt ain rgg etpe dr os mon aa inliz ce od uri sn ei .ti Aal sa ilb li uli st ti re as tefo dr inco Fl id g- us rta er 2t ,e ox ua rm Din Ce Se Rs ğ‘(ğœƒ ğ‘–T\n1:ğ‘‡\n|ğœƒ ğ‘–T 0)=(cid:214) ğ‘¡=1N(ğœƒ ğ‘–T ğ‘¡;âˆšï¸ 1âˆ’ğ›½ ğ‘¡ğœƒ ğ‘–T ğ‘¡âˆ’1,ğ›½ ğ‘¡I) (3)\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDDâ€™25,August03â€“07,2025,Toronto,ON,Canada\nwhereğ›½ ğ‘¡ âˆˆ (0,1) controlsthescaleofnoiseaddedatthe$t$-th and(Â· âˆ¥ Â·)denotestheconcatoperation.Wethenencouragethisde-\nstep,andN denotesaGaussiandistribution. coupleddomain-sharedcognitiontoassistinpredictingresponses\nInthereversedenoisingprocess,thetraditionaldenoisingmeth- acrossalldomains:\no trd as ns(a fes rin bt er co ad uu sc ee td hein ds ee nc ot ii so in ng3. p2 r.2 o) cea sr se min oa dd ee lq eu dat lae cf ko sr ge uff ie dc at niv cee L 1= âˆ‘ï¸ âˆ¥ğ‘Ÿ ğ‘–T\nğ‘—\nâˆ’Mğœ™ğ‘‡(ğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’,ğ‘ ğ‘—T)âˆ¥\nfromsourcedomaininformation,resultinginthelossofpersonal-\n(ğ‘’ğ‘–,ğ‘T ğ‘—,ğ‘Ÿ ğ‘–T ğ‘—)âˆˆR Tğ‘¤ğ‘ğ‘Ÿğ‘š\nviz ea ct ti oo rn s,in wt ehe prg oe pn oe sr eat ue td ila izb ii nli gtie ps r. ioT ro ig ne fn oe rmra ate tiop ner fs ro on mali tz he ed sa ob uil ri cty\ne\n+ âˆ‘ï¸ğ‘€ âˆ‘ï¸ âˆ¥ğ‘Ÿ ğ‘–S ğ‘˜ğ‘š âˆ’Mğœ™ğ‘†ğ‘š(ğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’,ğ‘ ğ‘˜Sğ‘š)âˆ¥, (7)\ndomaintoguidethedenoisingprocess.Specifically,forexamineeğ‘’ ğ‘–, Sğ‘š=1(ğ‘’ğ‘–,ğ‘S ğ‘—ğ‘š,ğ‘Ÿ ğ‘–S ğ‘—ğ‘š)âˆˆRSğ‘š\nweusethepretrainedsourcedomainabilityğœƒ ğ‘–S âˆˆÎ˜ S asguidance: whereğœƒğ‘ â„ğ‘ğ‘Ÿğ‘’\nisencouragedtopredictresponsesacrossallscenarios,\nğ‘ ğ›¿(ğœƒ ğ‘–T ğ‘¡Ë†\nâˆ’1\n|ğœƒ ğ‘–T ğ‘¡,ğœƒ ğ‘–S)=N(ğœƒ ğ‘–T ğ‘¡Ë† âˆ’1;ğœ‡ ğ›¿(ğœƒ ğ‘–T ğ‘¡,ğœƒ ğ‘–S,ğ‘¡),Î£ ğ›¿(ğœƒ ğ‘–T ğ‘¡,ğœƒ ğ‘–S,ğ‘¡)), (4) i rn esc plu od ni sn eg dt ah te at ia nrg the etd seo cm onai dn ti en rmth .e Nfi er xs tt ,tt her em spa en cd ifia cll coso gu nr ic tie ondo om fta hin\ne\nw wih te hr le eağœ‡ rğ›¿ na an bld eÎ£ pğ›¿ ara ar me et th ee rsp ğ›¿a .rametersoutputbyneuralnetworks ğµta 1rg â†’et ğ‘Œdo am ndai ğµn 1ğµ â†1h {a ğ‘‹s ,t ğµw }o â†’pat ğ‘Œh .s Win efl au de jun sc tin thg et ch oe ng foe un ner da inte gd vr ae rs iu ablt ls e:\n{ğ‘‹,ğµ}inthesecondconfoundingpathtoblockthebackdoorpath.\n4.2 CognitiveStateUnificationModule Specifically,weextractthespecificcognitionofthetargetdomain,\nAlthoughusingunifiedsourcedomainabilitiesobtainedthrough whichwillbeusedasinputinthesection4.1alongsideobtaining\nCDM to guide the reverse denoising process is a promising ap- theoverallcognitivestateofthetargetdomain(correspondingto\nproach, from the causal perspective of the model, as shown in\neventğµinthecausalgraph):\nt bh ye dr oi mgh at ins -i sd he aro ef dF cig ou gnre iti2 o, nth ğ‘‹e ,dg oe mne ar ia nt -e sd pea cb ifiil city coğ‘Œ gni is tioin nfl ğ´ue 1,n ac ned\nd\nğœƒ ğ‘–Tğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ =ğ‘“\nğœ‘\n4(ğœ(ğ‘“\nğœ‘\n3(ğœƒT))),\n(8)\ntargetdomainabilityğµduringthetrainingphase.Thisisbecause ğœƒ ğ‘–ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ =ğ‘“\nğœ‘\n5(ğœ(ğ‘“\nğœ‘\n6(ğœƒ ğ‘–Tğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ âˆ¥ğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ ))),\nweuserandomsamplingoftimestepsfortraining,whichretains\nsomepersonalizedinformationinthetargetdomainabilityvector whereğ‘“ {ğœ‘ 3,ğœ‘ 4,ğœ‘ 5,ğœ‘ 6}arefourlinearlayerswithdifferentparameters.\nevenwhileintroducingnoise.Inotherwords,ğœƒ ğ‘–T\n1:ğ‘‡\ndoesnotapprox- T ao nole va er ln dt eh ce ousp pe lic ni gfic stc ro ag ten git yi ,o wn ho if chth ee nt ca org ue rat gd eo sm thai en p, rw ee dip cr tio op nos oe\nf\nimatestandardGaussiannoise,whichlacksextensivepersonalized\nfeatures.However,theinclusionofdomain-specificcognitionğ´\n1\nwithin-domainresponseresultsbydomain-specificcognitionwhile\ndegradingperformanceinotherdomains:\nincreasesthecomplexityofthemodelandreducesitsgeneraliza-\ntionability,makingthemodelpronetooverfitting.Therefore,we L 2= âˆ‘ï¸ (cid:16) âˆ¥ğ‘Ÿ ğ‘–T\nğ‘—\nâˆ’Mğœ™ğ‘‡(ğœƒ ğ‘–ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡,ğ‘ ğ‘—T)âˆ¥\nproposetoexplorethecausalrelationshipbetweenthegenerated\nresultğ‘Œ andthedomain-sharedcognitionğ‘‹ andthepretrained\n(ğ‘’ğ‘–,ğ‘T ğ‘—,ğ‘Ÿ ğ‘–T ğ‘—)âˆˆR Tğ‘¤ğ‘ğ‘Ÿğ‘š\nabilityğµofthetargetdomain,whichcanalsobefurtherdecoupled +âˆ¥ğ‘Ÿ ğ‘–T\nğ‘—\nâˆ’Mğœ™ğ‘‡(ğœƒ ğ‘–Tğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğ‘ ğ‘—T)âˆ¥(cid:17)\nintodomain-specificcognitionğµ 1anddomain-sharedcognitionğ‘‹.\nğ‘€\nN the ex gt, ew nee rw ati ell ds rp ee sc ui lfi tc .allyanalyzetheimpactofdifferentfactorson âˆ’ âˆ‘ï¸ âˆ‘ï¸ (cid:16) âˆ¥ğ‘Ÿ ğ‘–S ğ‘˜ğ‘š âˆ’Mğœ™ğ‘†ğ‘š(ğœƒ ğ‘–ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡,ğ‘ ğ‘˜Sğ‘š)âˆ¥\nFirst,therearethreepathsbetweendomain-sharedcognitionğ‘‹ Sğ‘š=1(ğ‘’ğ‘–,ğ‘S ğ‘—ğ‘š,ğ‘Ÿ ğ‘–S ğ‘—ğ‘š)âˆˆRSğ‘š\na {ğµnd ,ğµt 1h }e â†’gen ğ‘Œe ,r wat he ed rr ee ts hu elt ls atğ‘Œ te: rğ‘‹ twâ†’ opğ‘Œ a, tğ‘‹ hsâ† are{ cğ´ o, nğ´ fo1 u} nâ†’ dinğ‘Œ g, pa an td hsğ‘‹ .Wâ†\ne\n+âˆ¥ğ‘Ÿ ğ‘–S ğ‘˜ğ‘š âˆ’Mğœ™ğ‘†ğ‘š(ğœƒ ğ‘–Tğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğ‘ ğ‘˜Sğ‘š)âˆ¥(cid:17)\nwillapplythebackdoorcriteriontoexplorethecausalrelationship +E ğ‘’ğ‘–âˆˆEOğ‘šâˆ¥ğœƒ ğ‘–Tâˆ’ğœƒ ğ‘–ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ âˆ¥.\nof ğ‘‹ â†’ ğ‘Œ, which means we need to control the confounding (9)\nvariablesğ¶ ={ğ´,ğ´ 1}âˆª{ğµ,ğµ 1}toblockthebackdoorpaths: Here,thefirsttermencouragesthetargetdomain-specificcognition\nâˆ‘ï¸ toassistinpredictingtheresponserecordsintargetdomain,bothin-\nğ‘ƒ(ğ‘Œ |ğ‘‘ğ‘œ(ğ‘‹))= ğ‘ƒ(ğ‘Œ |ğ‘‹,ğ¶ =ğ‘)ğ‘ƒ(ğ¶ =ğ‘), (5)\ndependentlyandinconjunctionwithdomain-sharedcognition.The\nğ¶\nsecondtermaimstointentionallydegradepredictionperformance\nwherethefirsttermrepresentstheeffectofğ‘‹onğ‘Œwhilecontrolling\ninthesourcedomains,indicatingthatthetargetdomain-specific\nfortheconfoundingvariablesğ¶,andthesecondtermrepresents\ncognitionisnotapplicabletothesourcedomains.Thethirdterm\nthejointprobabilitydistributionoftheconfoundingvariables.To\nconstrainsthediagnosticabilitiesobtainedfrompre-training.Addi-\ncontroltheconfoundingvariables,inspiredby[8],wedecouplethe\ntionally,tofurthercontroltheinfluenceofconfoundingvariableson\ndomain-sharedcognitionğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ fromallthepriorabilitiesinthe\nthegeneratedresults,weapplygradient-basedorthogonalregular-\nsourceandtargetdomains:\nization[6]toensuretheindependenceoftheaboverepresentations\nğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ =ğ‘“ ğœ‘ 2(ğœ(ğ‘“ ğœ‘ 1(ğœƒ ğ‘–T âˆ¥E ğ‘šâˆ¼Sğ‘€Wğ‘šğœƒ ğ‘–Sğ‘š))), (6) inthefeaturespace:\nw abh ile ir tie eğ‘Š sfroâˆˆ mR mğ‘€ uÃ— lğ‘‘ tipis lea dw oe mig ah int sm ina tt ori tx hu es se ad mt eo fm eaa tup rt eh se pp acre et ,r wai hn ee rd\ne\nL 3=(cid:13) (cid:13) (cid:13)âˆ¥âˆ‡âˆ‡ğœ‘ ğœ‘1 1, ,ğœ‘ğœ‘ 2 2L L1\n1âˆ¥\nÂ· âˆ¥âˆ‡ âˆ‡ğœ‘ ğœ‘3 3, ,ğœ‘ ğœ‘4 4L L2 2âˆ¥(cid:13) (cid:13)\n(cid:13)\n2. (10)\nğ‘‘ isthefeaturedimensionrelatedtoCDM,ğ‘“ ğœ‘ andğ‘“ ğœ‘ arelinear Thisapproachminimizestheinfluenceofdomain-specificcognition\n1 2\nlayerswithdifferentparameters,ğœreferstotheactivationfunction, whenlearningdomain-sharedcognition,andviceversa.\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada HaipingMaetal.\nTherefore, the parameters in the forward and reverse Statistics C C++ DS Java Python\nğ‘p ğ›¿ro (ğœƒce ğ‘–T ğ‘¡s ğ‘  âˆ’ğ‘s 1ğ‘’Ë†e ğ‘s ğ‘–ğ‘“ğ‘–ğ‘a |r ğœƒe ğ‘–T ğ‘¡ğ‘ ğ‘ğ‘’u ğ‘ğ‘–p ğ‘“ğ‘–d ğ‘a ,t ğœƒe ğ‘–ğ‘ d â„ğ‘ğ‘Ÿğ‘’t )o ,resğ‘ pe(ğœƒ cğ‘– tT ğ‘¡ iğ‘  vğ‘ eğ‘’ğ‘ lyğ‘–ğ‘“ .ğ‘–ğ‘ |ğœƒ ğ‘–T ğ‘¡ğ‘  âˆ’ğ‘ 1ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘) and # #E Qx ua em sti in oe ne 3 1, 18 55 ,6\n335\n3 1, 08 ,5 26\n41\n3 1, 48 ,5 26\n07\n3 1, 08 ,5 06\n43\n3 1, 18 ,5 66\n00\n#Concept 654 472 560 541 480\n4.3 HarmonizationandCalibrationModule #Log 2,163,624 1,332,005 1,398,444 1,086,906 923,879\nTolearntheparametersğ›¿ ofthedenoisingnetwork,DCSRaims Table1:Thestatisticsofthedatasets.\ntomaximizetheEvidenceLowerBound(ELBO)oftheobserved\nex loa gm ğ‘(i ğœƒn ğ‘–Te ğ‘ ğ‘e ğ‘’ğ‘ğ‘–a ğ‘“ğ‘–b ğ‘)il â‰¥it E (cid:124)y ğ‘ (cid:32)(cid:32)(cid:32)(cid:32)ğœƒ ( (cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)ğ‘– ğ‘–T (cid:32)T (cid:32)0ğ‘  (cid:32)(cid:32)ğ‘  ğ‘ (cid:32)(cid:32)(cid:32)ğ‘’ (cid:32)ğ‘ (cid:32)ğ‘ (cid:32)(cid:32)ğ‘’ (cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)ğ‘ (cid:32)ğ‘– (cid:32)(cid:32)ğ‘– ğ‘ (cid:32)(cid:32)(cid:32)ğ‘“ | (cid:32)(cid:32)ğœƒ (cid:32)ğ‘ (cid:32)(cid:32)ğ‘–T (cid:32)1(cid:32)ğ‘  (cid:32): (cid:32)ğ‘ (cid:32)(cid:32)(cid:32)ğ‘’ (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)(cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:104) (cid:32)(cid:32)l (cid:32)(cid:32)o (cid:32) (cid:123)g (cid:122)ğ‘ (cid:32)(cid:32)ğ›¿ (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)ğ‘– (cid:32)T (cid:32)0 (cid:32)ğ‘  (cid:32)(cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘’ (cid:32)(cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)ğ‘– (cid:32)T (cid:32)1 (cid:32)ğ‘  (cid:32)(cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘’ (cid:32)(cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)) (cid:32) (cid:125)(cid:105) cognitiv âˆ‡e ğœƒË†f Te Lat ğ‘¡u ğ‘r =es âˆ’ğœƒ ğ‘–Ë†T with âˆ‘ï¸theCDM: (cid:16) ğ‘Ÿ ğ‘–ğ‘—log(cid:0)Mğœ“ T(ğœƒ ğ‘–Ë†T,ğ‘ ğ‘—)(cid:1)\nğ‘‡\n:=L0 (ğ‘’ğ‘–,ğ‘ğ‘—,ğ‘Ÿğ‘–ğ‘—)âˆˆR Tğ‘¤ğ‘ğ‘Ÿğ‘š\n(15)\nâˆ’âˆ‘ï¸ ğ‘¡=2E (cid:32)ğ‘ (cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)ğ‘–(cid:32)T (cid:32)ğ‘¡(cid:32)ğ‘  (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘’ (cid:32)(cid:32)(cid:32)ğ‘ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)(cid:32)(cid:32)ğ‘– (cid:32)ğ‘ (cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)ğœƒ (cid:32)(cid:32)(cid:32)ğ‘–(cid:32)T (cid:32)0ğ‘  (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)ğ‘’ (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘“ (cid:32)(cid:32)ğ‘– (cid:32)(cid:32)ğ‘ (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:104) (cid:32)(cid:32)K (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)L (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)q (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T (cid:32)t (cid:32)âˆ’s (cid:32)(cid:32)p (cid:32)1 (cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)ğœƒ iT ts (cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)(cid:32)c (cid:32)(cid:32), (cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T 0 (cid:32)(cid:32)s (cid:32)(cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)âˆ¥ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)p (cid:32)(cid:32)(cid:32)ğ›¿ (cid:32)(cid:32)(cid:32)(cid:32)( (cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T t (cid:32)(cid:32)âˆ’s (cid:32)(cid:32)p (cid:32)1 (cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)| (cid:32)(cid:32)(cid:32)ğœƒ (cid:32)(cid:32)(cid:32)(cid:32)i (cid:32)T t (cid:32)(cid:32)s (cid:32)(cid:32)p (cid:32)(cid:32)e (cid:32)(cid:32)c (cid:32)(cid:32)i (cid:32)fi (cid:32)(cid:32)c (cid:32)(cid:32)(cid:32)) (cid:32)(cid:105) . +(1âˆ’ğ‘Ÿ ğ‘–ğ‘—)log(cid:0) 1âˆ’Mğœ“ T(ğœƒ ğ‘–Ë†T,ğ‘ ğ‘—)(cid:1)(cid:17) ,\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=Lğ‘¡âˆ’1 whereMğœ“ (Â·)denotesthepre-trainedCDMforthetargetdomain,\n(11) T\nHere,thefirsttermrepresentsthereconstructionterm,whichre-\nwithitsparametersfrozen,andonlythenetworkparametersğœ‡ ğ›¿(Â·)\ncoverstheprobabilityofğœƒ ğ‘–Tğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘.Thesecondtermisthedenoising ofthedenoisingmoduleareupdated.\n0\nmatchingterm,whichalignstheintractableposteriorprobability 4.4 DCSR-CAT:ImplementationofDCSR\nğ‘ ğ›¿(Â·)withthetractabledistributionğ‘(Â·).Tomaintaintrainingsta-\nbilityandsimplifycomputation,weignorethelearningofÎ£ ğ›¿(Â·) Inthissection,weintroduceDCSR-CATasanimplementationof\nandsetittoafixedvalueğ›½ ğ‘¡ asintheforwardprocess[11,35].The DCSRtodemonstrateitsapplicabilitytoCAT.\nDuringthisstage,DCSRnolongerinvolvestheforwardprocess\ndenoisingmatchingtermcanthenbefurthercomputedas:\nLğ‘¡âˆ’1=E\nğ‘(ğœƒ ğ‘–T 0ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ |ğœƒ ğ‘–T 1ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ )\n(cid:20) 21\nğ›½ ğ‘¡\n(cid:13) (cid:13) (cid:13)ğœ‡ ğ›¿(ğœƒ ğ‘–T ğ‘¡ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğœƒğ‘ â„ğ‘ğ‘Ÿğ‘’,ğ‘¡) b c thou elt d sd - osir t uae rrc ctt el ey x dat oa m mk ie ans inep e ,u wğ‘’r ğ‘– ee âˆˆ un R so ei ğ‘ Ts ğ‘œ te ğ‘™ hğ‘‘ğœ– e0 w phâˆ¼ reo -N th ra a( s0 in, r1 ee d) spa Cos Dni sn Mep ru te ot c. o oF r bo d tr s aia o nng tli hyv ee i in n\nr\nâˆ’ğœ‡(ğœƒTğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğœƒTğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘)(cid:13) (cid:13)(cid:105) , priorabilityandcalculatetheirdomain-sharedcognitivefeatures\n(cid:101) ğ‘–ğ‘¡ ğ‘– 0 (cid:13)\n(12)\nğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ .Itisnoteworthythatsincetheoriginalğœƒ ğ‘–T isunknown,the\naveragecognitivestateofthetargetdomainisusedasasubstitute:\nwhere ğœ‡(Â·) satisfies the tractable distribution\nğ‘(ğœƒ ğ‘–T ğ‘¡ğ‘  âˆ’ğ‘ 1ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘(cid:101) |ğœƒ ğ‘–T ğ‘¡ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğœƒ ğ‘–T 0ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘)=N(ğœƒ ğ‘–T ğ‘¡ğ‘  âˆ’ğ‘ 1ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘; (cid:101)ğœ‡(Â·),ğ›½ ğ‘¡I). ğœƒ ğ‘–Tğ‘ğ‘œğ‘™ğ‘‘ =E ğ‘’ğ‘—âˆˆR Tğ‘¤ğ‘ğ‘Ÿğ‘šMğœ“ Tâˆ‡ğœƒ(ğ‘’ ğ‘—), (16)\nAlthoughtheabovetrainingobjectiveensuresdiversityingen-\nwhichretainsdomaininformationtosomeextent.Therefore,sub-\neratedsamples,excessiverandomnessintroducedbythediffusion\nstitutingbackintoequation(6),weobtainthesharedcognitive\nmodelduringtrainingandinferencestagesmayresultingenerated abilityğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ ofthecold-startexamineeğ‘’ ğ‘–.Toimproveinference\noutcomesthatdonotmatchtherequirementsoftheCATsystem.\nefficiency,weuseDPM-Solver[22]asafastsolvertoefficiently\nTherefore, we constrain the generated ability vectors from two\nobtaintheinitialabilityofcold-startexaminee:\naspects:ConsistencyConstraintandTask-orientedConstraint.\nğœƒ ğ‘–0=Solver(ğœ‡ ğ›¿(Â·),ğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’,ğœ– 0), (17)\n4.3.1 ConsistencyConstraint. aimstominimizethedifferencebe- whichiscrucialforreal-worldapplications.OurDCSRseamlessly\ntweengeneratedvectorsandrealdatasamplesinthefeaturespace, integrateswithanyexistingquestionselectionalgorithmÎ .Atthe\nthuslimitinguncertaintywithinacontrollablerange.Specifically, beginningofthetest,theselectionprocesscanberepresentedas:\nthe Diffusion Module diffuses the domain-shared cognitive fea- ğ‘1â†Î (ğ‘„ ğ‘– |ğœƒ ğ‘–0), (18)\nturesintodomain-specificcognitivefeatures.Firstly,thegeneral\ncognitivefeaturesofthetargetdomainaregeneratedasğœƒ ğ‘–Ë†T: w aph pi rc oh pi rn iad ti eca itt ee ms t ğ‘h 1at inth ste epite ğ‘¡m =1se fl re oc mtio tn heal cg ao nr dit idh am teÎ  ites mele pc ot os lt Qhe\nğ‘–\nğœƒ ğ‘–Ë†T =ğœ‡ ğ›¿(ğœƒ ğ‘–T ğ‘‡ğ‘  :ğ‘ 1ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘,ğœƒ ğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’,ğ‘¡ ğ‘‡:1)+ğ›½ ğ‘¡ğœ–,ğœ– âˆˆ (0,1). (13) Tfo hr ee sx ua bm sein qe ue enğ‘’ ğ‘– tb sta es pe sd fo on llot whe ti hn eit gia el na eb rail lit Cy Ağœƒ Tğ‘–0 sin yi st ti ea mliz wed orb ky flD owC .SR.\nDuetoinherentrandomness,thegeneratedabilityvectorsarenoisy,\nwhichisnotconducivetoreflectingtheexamineeâ€™strueability. 5 Experiments\nThus,theconsistencyconstraintaimstominimizethedifference\nInthissection,weconductexperimentswiththeaimofaddressing\nbetweenğœƒ ğ‘–Ë†T andthetargetdomainabilityfeaturesğœƒ ğ‘–T obtained thefollowingquestions:\nthroughpre-training,whichcanbemodeledas:\nâ€¢ RQ1:CanDCSRutilizepriorinformationfromasingledo-\nLğ‘ğ‘ =E ğ‘’ğ‘–âˆˆEO(ğœƒ ğ‘–Tâˆ’ğœƒ ğ‘–Ë†T)2. (14) mainormultipledomainstoimprovethecoldstartperfor-\nmanceofexistingCATsystems?\n4.3.2 Task-orientedConstraint. ensuresthatthegeneratedability â€¢ RQ2:HoweffectivearethekeycomponentsoftheDCSR\nfeaturesmeettherequirementsoftheCATtask.Specifically,we framework?\nsamplesomequestionsğ‘ ğ‘— âˆˆ Rğ‘¤ğ‘ğ‘Ÿğ‘š fromthetrainingsetofthe â€¢ RQ3:DoesDCSRalleviatetheissueofthequestionselection\nT\ntarget domain and match the generated target domain general algorithmfallingintolocaloptima?\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDDâ€™25,August03â€“07,2025,Toronto,ON,Canada\nCDM IRT NCD\nCAT Fisher MAAT MAAT BECAT NCAT\nMetrics AUC(%)@1/ACC(%)â†‘@1\nBaselines Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle* Random MLCCM DCSR Oracle*\nC++ 71.6/75.0 78.5/77.5 77.9/76.3 79.5/75.4 71.2/74.9 78.5/77.4 77.7/76.0 79.6/75.7 67.4/62.8 74.7/74.6 79.2/77.3 88.0/83.1 67.4/62.7 74.6/74.6 79.2/77.3 88.0/83.0 67.4/62.8 74.5/74.6 79.3/77.3 88.0/83.1\nC\nDS 71.8/75.4 75.5/77.0 75.4/75.9 77.2/78.2 71.1/75.2 74.9/76.8 75.0/75.7 77.2/78.2 67.5/64.9 77.7/76.6 79.4/77.8 89.3/84.3 67.6/65.0 77.7/76.6 79.4/77.8 89.3/84.3 67.6/65.0 77.7/76.6 79.4/77.8 89.3/84.4\nC 67.4/74.8 71.6/75.5 72.5/75.9 76.8/77.2 67.1/74.8 71.5/75.4 72.4/75.8 76.8/77.2 66.1/67.3 71.6/74.8 71.7/74.1 85.8/82.0 66.0/67.3 71.6/74.9 71.7/74.1 85.8/82.0 66.1/67.3 71.6/74.9 71.7/74.1 85.8/82.0\nDS\nC++ 71.6/75.0 73.8/75.0 74.5/75.5 79.5/75.4 71.2/74.9 73.5/75.0 74.1/75.3 79.6/75.7 67.4/62.8 72.4/73.7 74.4/74.9 88.0/83.1 67.4/62.7 72.4/73.7 74.4/74.9 88.0/83.0 67.4/62.8 72.4/73.7 74.4/74.9 88.0/83.1\nC 67.4/74.8 74.2/76.0 74.7/76.8 76.8/77.2 67.1/74.8 74.2/76.1 74.6/76.6 76.8/77.2 66.1/67.3 72.8/75.3 73.4/75.5 85.8/82.0 66.0/67.3 72.8/75.3 73.4/75.5 85.8/82.0 66.1/67.3 72.8/75.2 73.1/75.3 85.8/82.0\nC++\nDS 71.8/75.4 71.9/76.5 73.3/76.6 77.2/78.2 71.1/75.2 71.4/76.3 72.7/76.4 77.2/78.2 67.5/64.9 76.4/76.5 77.0/76.9 89.3/84.3 67.6/65.0 76.4/76.5 77.0/76.9 89.3/84.3 67.6/65.0 76.4/76.5 77.0/76.9 89.3/84.4\nMetrics AUC(%)â†‘@5/ACC(%)â†‘@5\nC++ 74.2/75.6 78.9/77.9 78.8/76.9 79.2/74.7 72.0/75.0 78.2/77.3 77.5/76.1 79.7/76.3 67.5/63.1 74.8/74.6 79.3/77.4 88.0/83.0 67.5/62.9 74.7/74.6 79.3/77.4 88.0/83.1 67.6/63.0 74.7/74.7 79.3/77.4 88.0/83.1\nC\nDS 74.2/76.0 76.9/77.7 76.6/76.5 77.4/78.3 71.5/75.4 75.0/76.9 75.0/75.7 77.2/78.1 67.9/65.3 77.9/76.7 79.5/77.9 89.2/84.2 67.7/65.1 77.8/76.7 79.5.77.9 89.3/84.4 67.8/65.2 77.8/76.7 79.5/77.9 89.3/84.4\nC 68.5/75.0 72.4/75.7 73.1/76.1 76.8/77.3 67.4/74.8 71.8/75.6 72.6/75.9 76.7/77.2 66.3/67.5 71.6/74.8 71.8/74.1 85.9/82.1 66.1/67.4 71.6/74.9 71.7/74.1 85.8/82.0 66.2/67.4 71.7/74.9 71.7/74.1 85.8/82.1\nDS\nC++ 74.2/75.6 75.3/75.7 76.3/76.4 79.2/74.7 72.0/75.0 74.0/75.2 74.9/75.8 79.7/76.3 67.5/63.1 72.6/73.8 74.5/75.0 88.0/83.0 67.5/62.9 72.5/73.8 74.5/75.0 88.0/83.1 67.6/63.0 72.5/73.8 74.5/75.0 88.0/83.1\nC 68.5/75.0 74.4/75.9 74.8/76.9 76.8/77.3 67.4/74.8 74.4/76.2 74.7/76.7 76.7/77.2 66.3/67.5 72.9/75.4 73.5/75.5 85.9/82.1 66.1/67.4 72.9/75.4 73.4/75.5 85.8/82.0 66.2/67.4 72.9/75.3 73.2/75.3 85.8/82.1\nC++\nDS 74.2/76.0 73.4/77.2 75.1/77.3 77.4/78.3 71.5/75.4 71.8/76.4 73.1/76.5 77.2/78.1 67.9/65.3 76.6/76.7 77.1/77.0 89.2/84.2 67.7/65.1 76.5/76.6 77.0/77.0 89.3/84.4 67.8/65.2 76.5/76.6 77.0/77.0 89.3/84.4\nTable2:TheAUC/ACCperformanceinsixscenarios.Thebestresultsarehighlightedinbold,whileâˆ—denotestheupperbounds.\nâ€¢ RQ4:CanDCSRenhancethecoldstarteffectofcognitive â€¢ MLCCM:Across-coursemethodbasedonmeta-learning,\ndiagnosismodels? applyingtheideaofmeta-learningtolearncross-domain\nâ€¢ RQ5:IstheinitialabilityassignedbyDCSRtocoldstart mappingfunctionsfromthetrainingset.\nexamineesreasonable? â€¢ Oracle:ThismethodusesCDMtodirectlytrainthetarget\ndomainabilityfromtheresponserecordsofexamineesin\n5.1 ExperimentalSettings thetargetdomain(testset).\nInthissection,weintroducethedatasets,theselectedbaselines, 5.1.3 EvaluationMetrics. TheperformanceofourDCSRwillbe\nandtheapplicationofCAT. validatedduringthetestingphaseoftheCATsystem.Weevaluate\ntheaccuracyofthefinalabilityestimationbypredictingexamineesâ€™\n5.1.1 Dataset Description. We conducted experiments on five\nbinary responses to the test question set. For this purpose, we\nreal-worlddatasetscollectedfromthepubliclyavailablePTADisc\nusetheareaundertheAreaUnderROCCurve(AUC)[2,40]and\ndataset [14], covering courses in Data Structures (DS), C, C++,\nAccuracy(ACC)asevaluationmetrics.\nPython, and Java programming languages. These datasets are\nsourcedfromthePTAplatform,whichrecordsthelearningperfor- 5.1.4 ParameterSettings. Inthepre-trainingphase,forIRT,weset\nmanceofexamineesacrossaseriesofcourses.Eachdatasetprovides thelatentfeaturedimensionofbothexamineesandquestionsto1,\ntheresponserecordsoftheexamineesandquestion-conceptrela- whileforNCD,itissettothenumberofknowledgeconceptsinthe\ntion,witheachdatasetconsideredasadistinctdomain.Wefirst correspondingdomain.Additionally,weuniformlysetthebatchsize\nexcludedexamineeswithfewerthan100responserecordsfrom andlearningrateto32and0.002,respectively,forthisphase.Inthe\neachdataset.Thestatisticsoftheprocesseddatasetsareshown DSCRtrainingphase,theforwardprocessofthediffusionmodule\ninTable1.Forfairnessintesting,werandomlysplitthefiltered issetto1000stepsofnoiseaddition,andDPM-solver[22]isused\nexamineerecordsintoatrainingsetandatestsetinan80%:20% toacceleratesampling,whichisperformedin30steps.Meanwhile,\nratio.Examineesinthetestsetareconsideredcold-startexaminees thebatchsizeandlearningratearefixedat256and0,respectively,\ninthecurrent(target)domain,meaningtheirresponserecordsin inthisphase.WeinitializeallparametersusingXavier[10],anduse\notherdomainsareincludedinthetrainingset.Thetrainingset theAdam[16]optimizer.IntheCATtestingphase,thequestion\nisonlyusedfortrainingDSCRandthelearning-basedquestion selectionalgorithmsfollowthesettingsintheoriginalpapers,with\nselectionalgorithm,preventingdataleakage. thetestlengthsetto1and5,andtherandomseedinalltheabove\nprocessesissetto0.AllexperimentsareconductedonanNVIDIA\n5.1.2 BaselineMethods. Todemonstratetheeffectivenessandcom- RTX4090GPU.\npatibilityofourframework,weappliedittofourwidelyusedCAT\nsystems,includingthestrategy-basedFisher[21],MAAT[1],BE- 5.2 OverallPerformance(RQ1)\nCAT[50],andthedata-drivenNCAT[49].Weselectcross-domain\nToverifytheeffectivenessoftheproposedframeworkinaddressing\nbaselinesforcomparison,wheretheRandomandOraclemethods\ntheCSIPchallenge,wecomparedDCSRwithothercross-domain\nrepresentthelowerandupperboundsofCATcold-startperfor-\nbaselinesusingbothsingleandmultidomainaspriorknowledge,\nmance,respectively.\nsettingthequestionselectionstepsto1and5.First,weexplored\nâ€¢ Random:Thismethodrandomlypredictstheinitialability leveragingasingledomainasthesourcedomain,encompassingsix\nof examinees in the target domain from ağ‘ˆğ‘›ğ‘–ğ‘“ğ‘œğ‘Ÿğ‘š(0,1) cross-domainscenarios.Werotatedeachdatasettoplaytherole\ndistribution,whichisthemostcommonmethodinexisting ofthetargetdomain,withotherdatasetsservingasthesourcedo-\nCATsystem. main.TheexperimentalresultspresentedinTable2showthatour\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada HaipingMaetal.\nFigure3:Comparisonofthepriorinformationprovidedby\nmultiandsingledomainintheNCD-MAATCATsystem.\nFigure5:Resultsunderextendedtestingproceduresinthe\nIRT-FisherCATsystem.\nHarmonizationandCalibrationModule(HCM)insection4.3(de-\nnotedasw/oHCM).CSUMprovidesdiffusionguidanceforthe\nDiffusionModule(DM),whileHCMconstrainstheDMâ€™soutputto\nmatchtheCATtask.AsshowninFigure4,wevalidatedtheimpact\nofdifferentcomponentsonDCSRintheNCD-MAATCATsystem,\nwithCsetasthetargetdomainandotherdomainsasthesource\ndomains.Specifically,whenCSUMisremovedfromDSCR,perfor-\nmancedropssignificantly,indicatingthatthespecificcognition\nofthesourcedomain,asaconfoundingvariable,hindersthetrue\ncausaleffect,leadingtonegativetransfer.Similarly,whenHCM\nFigure4:Ablationstudyonkeycomponents. isremoved,thereisalsoacertaindegreeofperformancedecline,\nsuggestingthatexcessiveuncertaintyaffectsthealignmentofthe\ngeneratedresultswiththeCATtask.Therefore,allcomponents\nproposedDCSRnotonlyoutperformsallbaselinesintheCATcold- contributetoDCSRtovaryingdegrees.\nstarttaskacrossallsixscenariosbutalsoadaptswelltostrategy-\nbasedandlearning-basedselectionalgorithms.Additionally,we 5.4 Performanceunderlongteststeps(RQ3)\nobservedthefollowing:(1)ComparedtothecommonlyusedRan- ToexploretheimpactofDCSRoncold-startperformanceundera\ndommethod,DCSRprovidesmoreaccurateinitialabilityestimates greedyalgorithm,asshowninFigure5,weextendedthetestlength\nforcold-startexaminees,especiallywhenthereisasignificantcor- intheIRT-FisherCATsystemandusedsingle-domaininformation\nrelationbetweenthesourceandtargetdomains,suchasCandC++, asthepriorinformationprovider,withthecommonlyusedRandom\nwhereDCSRclearlyoutperformsotherbaselinesandapproaches methodasthebaseline.Weobservedthatintheearlystagesof\ntheperformanceoftheOraclemethod.Thisdemonstratestheef- testing,regardlessofthesourcedomain,cold-startperformance\nfectivenessofusingexamineesâ€™domain-sharedcognitionasthe improvessignificantly.Notably,whenC++isusedasthesource\ntransfercondition.(2)ComparedtoMLCCM,DCSRdoesnotrely domain,theinitialeffectalreadysurpassestheRandommethod\nheavilyonsupervisedtraining,therebyavoidingoverfittingtothe after60roundsofquestionselectionduetothecorrelationbetween\nlabels.Moreover,inmostscenarios,theabilityassignedbyDCSR courses.ThisdemonstratesthatDCSRnotonlyalleviatestheissue\nattheinitialstageshowsbetterperformancethantheresultsdiag- ofselectionalgorithmsfallingintolocaloptimabutalsoguides\nnosedbyothermethodsaftermultipleroundsofselection. theselectionalgorithmbyprovidinganaccuratestartingpointfor\nNext,weexploredusingmulti-domaininformationasthesource testing.Additionally,evenwhenaweaklycorrelatedcourseisused\ndomain,coveringthreescenarioswhereC,C++,andDSwereset asthesourcedomain,theoptimalresultofthetraditionalRandom\nasthetargetdomains,withothercoursesservingasthesource methodcanbeachievedwithinashortertestlength.\ndomain.AsshowninFigure3,comparedtosingle-domaininfor-\nmation,themulti-domainenrichesthecommoncognition,andthe 5.5 Cold-StartinCognitiveDiagnosis(RQ4)\ncorrelationbetweencoursessignificantlyhelpsresolvetheCSIP.\nToexploretheperformanceofDCSRinaddressingthecold-start\nTheperformanceinthefirsttwoscenariosisparticularlyeffective\nproblemincognitivediagnosistask,wedirectlyappliedDCSRto\nduetotheoverlapofconceptsbetweenprogramminglanguages.\nCDM,includingunidimensionalIRTandmultidimensionalNCD.\nAsshowninFigure6,DCSRdemonstratessuperiorperformance\n5.3 AblationStudy(RQ2)\ninCDMacrossdifferentdimensions,approachingOracleperfor-\nThissectionprovidesanin-depthanalysisofhowkeycomponents manceinsomescenarios.Moreover,inhigh-dimensionalCDM,i.e.,\ninDCSRcontributetoaddressingtheCSIPchallenge.Weconducted NCD,DCSRfurtherenhancescold-startperformanceinCDtask.\nexperimentsbyindividuallyremovingtheCognitiveStateUnifica- ItsignificantlyoutperformsthecommonlyusedRandommethod.\ntionModule(CSUM)insection4.2(denotedasw/oCSUM)andthe Additionally, in scenarios where the correlation is not obvious,\nDiffusion-InspiredColdStartwithSufficientPriorinComputerizedAdaptiveTesting KDDâ€™25,August03â€“07,2025,Toronto,ON,Canada\nFigure8:Caseanalysisofanindividualexaminee.\nFigure6:Performanceoncognitivediagnosiscoldstarttask.\napproachbetterservestheCATsystem,asunderestimatingabil-\nitycanguidetheselectionalgorithmtochooserelativelysimpler\nquestion,avoidingthenegativeemotionsassociatedwithoverly\ndifficultquestion.\n6 Conclusion\nInthispaper,weproposetheDiffusionCognitiveStatesTransfer\nFrameworktoaddresstheColdStartwithInsufficientPrior(CSIP)\ninComputerizedAdaptiveTesting(CAT).Thischallengecompels\nCATsystemstouseadditionalselectionstepstomitigatethecold\nstart issue. To address this, we first reconstruct the examineeâ€™s\ninitialabilityinthetargetdomain,guidedbydiffusionprinciple.\nConcurrently, we analyze the causal relationships in the gener-\natedoutcomesfromamodel-basedcausalperspective,proposing\nthreedecouplingstrategiestoblockthetwobackdoorpathsthat\nFigure7:T-SNEvisualizationofabilitiesinthetargetdomain. hindercausaldiscovery.Subsequently,weintroduceconstraints\nfromtheperspectivesofconsistencyandtask-orientedtoenforce\nalignmentofthegeneratedoutcomeswiththeCATsystem.There-\nsuchasusingDSasthesourcedomainandC++asthetargetdo- fore,thisframeworkcanbeappliedtoexistingmainstreamCAT\nmain,DCSRshowsbetterperformancethanMLCCM,significantly systems.Finally,extensiveexperimentshighlighttheeffectiveness\noutperformingbaselines. andapplicabilityofourframework.\n5.6 AnalysisofInitializedAbility(RQ5) References\n[1] HaoyangBi,HaipingMa,ZhenyaHuang,YuYin,QiLiu,EnhongChen,YuSu,\nWefurtherexploredwhethertheabilitiesassignedbyDCSRto\nandShijinWang.2020.Qualitymeetsdiversity:Amodel-agnosticframework\nexamineesinthetargetdomainarereasonable.Weconductedthe forcomputerizedadaptivetesting.In2020IEEEInternationalConferenceonData\nanalysisfromtwoaspects.First,usinghigh-dimensionalNCDasthe Mining(ICDM).IEEE,42â€“51.\n[2] AndrewPBradley.1997. TheuseoftheareaundertheROCcurveinthe\nbackbone,wevisualizethedistributionofinitialabilitiesassigned\nevaluationofmachinelearningalgorithms. Patternrecognition30,7(1997),\ntoexamineesbydifferentmethodsacross12scenarios.Asshown 1145â€“1159.\ninFigure7,theabilitydistributionassignedbytheRandommethod [3] ShangChai,LianshengZhuang,andFengyingYan.2023.Layoutdm:Transformer-\nbaseddiffusionmodelforlayoutgeneration.InProceedingsoftheIEEE/CVF\nisclearlyspherical,whichisnotconducivetodistinguishingbe- ConferenceonComputerVisionandPatternRecognition.18349â€“18358.\ntweenexamineesâ€™abilities.Incontrasttothelocalizedclustering [4] Hua-HuaChangandZhiliangYing.1996. Aglobalinformationapproachto\ncomputerizedadaptivetesting.AppliedPsychologicalMeasurement20,3(1996),\npresentedbyMLCCM,DCSRisclosertothatexhibitedbyOracle,\n213â€“229.\nshowingcleardifferentiation.Thisdemonstratesthatintroducing [5] SusanEEmbretsonandStevenPReise.2013.Itemresponsetheory.Psychology\nexamineesâ€™cross-domaincommoncognitioncangeneratemore Press.\n[6] MehrdadFarajtabar,NavidAzizan,AlexMott,andAngLi.2020. Orthogonal\npersonalizedinitialabilities.Secondly,werandomlyselectedan\ngradientdescentforcontinuallearning.InInternationalConferenceonArtificial\nexaminee,obtainedhisdiagnosticfeedbackusingNCD,andran- IntelligenceandStatistics.PMLR,3762â€“3773.\ndomlyselectedtenknowledgeconceptstocalculatethedifference [7] WeiboGao,QiLiu,ZhenyaHuang,YuYin,HaoyangBi,Mu-ChunWang,Jianhui\nMa,ShijinWang,andYuSu.2021.RCD:Relationmapdrivencognitivediagnosis\nbetweenthediagnosisresultsofthecorrespondingdimensionsand forintelligenteducationsystems.InProceedingsofthe44thinternationalACM\ntheresultsofOracle.AsshowninFigure8,DCSRshowssignif- SIGIRconferenceonresearchanddevelopmentininformationretrieval.501â€“510.\n[8] WeiboGao,QiLiu,HaoWang,LinanYue,HaoyangBi,YinGu,FangzhouYao,\nicantlylowerfluctuationandtendstounderestimatetheability\nZhengZhang,XinLi,andYuanjingHe.2024.Zero-1-to-3:Domain-LevelZero-\nofexamineeratherthanoverestimatehimasMLCCMdoes.This ShotCognitiveDiagnosisviaOneBatchofEarly-BirdStudentstowardsThree\nKDDâ€™25,August03â€“07,2025,Toronto,ON,Canada HaipingMaetal.\nDiagnosticObjectives.InProceedingsoftheAAAIConferenceonArtificialIntelli- [32] FeiWang,QiLiu,EnhongChen,ZhenyaHuang,YuyingChen,YuYin,ZaiHuang,\ngence,Vol.38.8417â€“8426. andShijinWang.2020. Neuralcognitivediagnosisforintelligenteducation\n[9] AritraGhoshandAndrewLan.2021.Bobcat:Bileveloptimization-basedcom- systems.InProceedingsoftheAAAIconferenceonartificialintelligence,Vol.34.\nputerizedadaptivetesting.arXivpreprintarXiv:2108.07386(2021). 6153â€“6161.\n[10] XavierGlorotandYoshuaBengio.2010.Understandingthedifficultyoftraining [33] FeiWang,QiLiu,EnhongChen,ZhenyaHuang,YuYin,ShijinWang,andYuSu.\ndeepfeedforwardneuralnetworks.InProceedingsofthethirteenthinternational 2022.NeuralCD:ageneralframeworkforcognitivediagnosis.IEEETransactions\nconferenceonartificialintelligenceandstatistics.JMLRWorkshopandConference onKnowledgeandDataEngineering35,8(2022),8312â€“8327.\nProceedings,249â€“256. [34] HangyuWang,TingLong,LiangYin,WeinanZhang,WeiXia,QichenHong,\n[11] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic DingyinXia,RuimingTang,andYongYu.2023.GMOCAT:AGraph-Enhanced\nmodels.Advancesinneuralinformationprocessingsystems33(2020),6840â€“6851. Multi-ObjectiveMethodforComputerizedAdaptiveTesting.InProceedingsof\n[12] YutingHong,ShiweiTong,WeiHuang,YanZhuang,QiLiu,EnhongChen,Xin the29thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining.\nLi,andYuanjingHe.2023. Search-EfficientComputerizedAdaptiveTesting. 2279â€“2289.\nInProceedingsofthe32ndACMInternationalConferenceonInformationand [35] WenjieWang,YiyanXu,FuliFeng,XinyuLin,XiangnanHe,andTat-SengChua.\nKnowledgeManagement.773â€“782. 2023.Diffusionrecommendermodel.InProceedingsofthe46thInternationalACM\n[13] YuHou,Jin-DukPark,andWon-YongShin.2024.CollaborativeFilteringBased SIGIRConferenceonResearchandDevelopmentinInformationRetrieval.832â€“841.\nonDiffusionModels:UnveilingthePotentialofHigh-OrderConnectivity.In [36] YunerXuan.2024. DiffusionCross-domainRecommendation. arXivpreprint\nProceedingsofthe47thInternationalACMSIGIRConferenceonResearchand arXiv:2402.02182(2024).\nDevelopmentinInformationRetrieval.1360â€“1369. [37] ShangshangYang,MingyangChen,ZiwenWang,XiaoshanYu,PanpanZhang,\n[14] LiyaHu,ZhiangDong,JingyuanChen,GuifengWang,ZhihuaWang,ZhouZhao, HaipingMa,andXingyiZhang.2024.DisenGCD:AMetaMultigraph-assisted\nandFeiWu.2023.PTADisc:across-coursedatasetsupportingpersonalizedlearn- DisentangledGraphLearningFrameworkforCognitiveDiagnosis.arXivpreprint\ningincold-startscenarios.AdvancesinNeuralInformationProcessingSystems36 arXiv:2410.17564(2024).\n(2023),44976â€“44996. [38] ShangshangYang,HaipingMa,YingBi,YeTian,LimiaoZhang,YaochuJin,\n[15] YangqinJiang,YuhaoYang,LianghaoXia,andChaoHuang.2024.Diffkg:Knowl- andXingyiZhang.2024.Anevolutionarymulti-objectiveneuralarchitecture\nedgegraphdiffusionmodelforrecommendation.InProceedingsofthe17thACM searchapproachtoadvancingcognitivediagnosisinintelligenteducation.IEEE\nInternationalConferenceonWebSearchandDataMining.313â€“321. TransactionsonEvolutionaryComputation(2024).\n[16] DiederikKingmaandJimmyBa.2014.Adam:AMethodforStochasticOptimiza- [39] ShangshangYang,LinruiQin,andXiaoshanYu.2024.EndowingInterpretability\ntion.ComputerScience(2014). forNeuralCognitiveDiagnosisbyEfficientKolmogorov-ArnoldNetworks.arXiv\n[17] ZihaoLi,AixinSun,andChenliangLi.2023. Diffurec:Adiffusionmodelfor preprintarXiv:2405.14399(2024).\nsequentialrecommendation. ACMTransactionsonInformationSystems42,3 [40] ShangshangYang,YeTian,ChengHe,XingyiZhang,KayChenTan,andYaochu\n(2023),1â€“28. Jin.2021. Agradient-guidedevolutionaryapproachtotrainingdeepneural\n[18] QidongLiu,FanYan,XiangyuZhao,ZhaochengDu,HuifengGuo,RuimingTang, networks. IEEETransactionsonNeuralNetworksandLearningSystems33,9\nandFengTian.2023.Diffusionaugmentationforsequentialrecommendation. (2021),4861â€“4875.\nInProceedingsofthe32ndACMInternationalConferenceonInformationand [41] ShangshangYang,HaoyuWei,HaipingMa,YeTian,XingyiZhang,YunboCao,\nKnowledgeManagement.1576â€“1586. andYaochuJin.2023. Cognitivediagnosis-basedpersonalizedexercisegroup\n[19] QiLiu,YanZhuang,HaoyangBi,ZhenyaHuang,WeizheHuang,JiatongLi, assemblyviaamulti-objectiveevolutionaryalgorithm. IEEETransactionson\nJunhaoYu,ZiruiLiu,ZiruiHu,YutingHong,etal.2024. SurveyofCom- EmergingTopicsinComputationalIntelligence7,3(2023),829â€“844.\nputerizedAdaptiveTesting:AMachineLearningPerspective. arXivpreprint [42] ShangshangYang,XiaoshanYu,YeTian,XuemingYan,HaipingMa,andXingyi\narXiv:2404.00712(2024). Zhang.2024.Evolutionaryneuralarchitecturesearchfortransformerinknowl-\n[20] ShuhuanLiu,XiaoshanYu,HaipingMa,ZiwenWang,ChuanQin,andXingyi edgetracing.AdvancesinNeuralInformationProcessingSystems36(2024).\nZhang.2023.HomogeneousCohort-AwareGroupCognitiveDiagnosis:AMulti- [43] ShangshangYang,ChengZhen,YeTian,HaipingMa,YuanchaoLiu,Panpan\ngrainedModelingPerspective.InProceedingsofthe32ndACMInternational Zhang,andXingyiZhang.2023.Evolutionarymulti-objectiveneuralarchitec-\nConferenceonInformationandKnowledgeManagement.4094â€“4098. turesearchforgeneralizedcognitivediagnosismodels.In20235thInternational\n[21] FredericMLord.2012. Applicationsofitemresponsetheorytopracticaltesting ConferenceonData-drivenOptimizationofComplexSystems(DOCS).IEEE,1â€“10.\nproblems.Routledge. [44] ZhengyiYang,JiancanWu,ZhicaiWang,XiangWang,YanchengYuan,and\n[22] ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. XiangnanHe.2024.Generatewhatyouprefer:Reshapingsequentialrecommen-\n2022.Dpm-solver:Afastodesolverfordiffusionprobabilisticmodelsamplingin dationviaguideddiffusion.AdvancesinNeuralInformationProcessingSystems\naround10steps.AdvancesinNeuralInformationProcessingSystems35(2022), 36(2024).\n5775â€“5787. [45] XiaoshanYu,ChuanQin,DazhongShen,HaipingMa,LeZhang,XingyiZhang,\n[23] HaipingMa,SiyuSong,ChuanQin,XiaoshanYu,LimiaoZhang,XingyiZhang, HengshuZhu,andHuiXiong.2024.Rdgt:enhancinggroupcognitivediagnosis\nandHengshuZhu.2024.DGCD:AnAdaptiveDenoisingGNNforGroup-level withrelation-guideddual-sidegraphtransformer.IEEETransactionsonKnowledge\nCognitiveDiagnosis.InThe33rdInternationalJointConferenceonArtificialIntel- andDataEngineering(2024).\nligence(IJCAI-24). [46] XiaoshanYu,ChuanQin,DazhongShen,ShangshangYang,HaipingMa,Hengshu\n[24] HaipingMa,ChangqianWang,HengshuZhu,ShangshangYang,Xiaoming Zhu,andXingyiZhang.2024. Rigl:Aunifiedreciprocalapproachfortracing\nZhang,andXingyiZhang.2024. Enhancingcognitivediagnosisusingun- theindependentandgrouplearningprocesses.InProceedingsofthe30thACM\ninteractedexercises:Acollaboration-awaremixedsamplingapproach.InPro- SIGKDDConferenceonKnowledgeDiscoveryandDataMining.4047â€“4058.\nceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.8877â€“8885. [47] XiaoshanYu,ChuanQin,QiZhang,ChenZhu,HaipingMa,XingyiZhang,\n[25] HaokaiMa,RuobingXie,LeiMeng,XinChen,XuZhang,LeyuLin,andZhan- andHengshuZhu.2024. DISCO:AHierarchicalDisentangledCognitiveDi-\nhuiKang.2024. Plug-indiffusionmodelforsequentialrecommendation.In agnosisFrameworkforInterpretableJobRecommendation. arXivpreprint\nProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.8886â€“8894. arXiv:2410.07671(2024).\n[26] HaipingMa,YongYang,ChuanQin,XiaoshanYu,ShangshangYang,Xingyi [48] JujiaZhao,WangWenjie,YiyanXu,TengSun,FuliFeng,andTat-SengChua.2024.\nZhang,andHengshuZhu.2024.HD-KT:AdvancingRobustKnowledgeTracing Denoisingdiffusionrecommendermodel.InProceedingsofthe47thInternational\nviaAnomalousLearningInteractionDetection.InProceedingsoftheACMonWeb ACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.\nConference2024.4479â€“4488. 1370â€“1379.\n[27] HaipingMa,YiZeng,ShangshangYang,ChuanQin,XingyiZhang,andLimiao [49] YanZhuang,QiLiu,ZhenyaHuang,ZhiLi,ShuanghongShen,andHaipingMa.\nZhang.2023.Anovelcomputerizedadaptivetestingframeworkwithdecoupled 2022.Fullyadaptiveframework:Neuralcomputerizedadaptivetestingforonline\nlearningselector.Complex&IntelligentSystems9,5(2023),5555â€“5566. education.InProceedingsoftheAAAIconferenceonartificialintelligence,Vol.36.\n[28] WimJVanderLindenandCeesAWGlas.2000.Computerizedadaptivetesting: 4734â€“4742.\nTheoryandpractice.Springer. [50] YanZhuang,QiLiu,GuanHaoZhao,ZhenyaHuang,WeizheHuang,Zachary\n[29] Jill-JÃªnnVie,FabricePopineau,Ã‰ricBruillard,andYolaineBourda.2017.Areview Pardos,EnhongChen,JinzeWu,andXinLi.2024.Aboundedabilityestimation\nofrecentadvancesinadaptiveassessment. Learninganalytics:Fundaments, forcomputerizedadaptivetesting. AdvancesinNeuralInformationProcessing\napplications,andtrends:Aviewofthecurrentstateofthearttoenhancee-learning Systems36(2024).\n(2017),113â€“142.\n[30] HowardWainer,NeilJDorans,RonaldFlaugher,BertFGreen,andRobertJ\nMislevy.2000.Computerizedadaptivetesting:Aprimer.Routledge.\n[31] JoojoWalker,TingZhong,FengliZhang,QiangGao,andFanZhou.2022.Rec-\nommendationviacollaborativediffusiongenerativemodel.InInternationalCon-\nferenceonKnowledgeScience,EngineeringandManagement.Springer,593â€“605.",
    "pdf_filename": "Diffusion-Inspired_Cold_Start_with_Sufficient_Prior_in_Computerized_Adaptive_Testing.pdf"
}