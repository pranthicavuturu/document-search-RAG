{
    "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
    "abstract": "Recent advances in fine-tuning Vision- Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code is available at https://github.com/minglllli/CLIPFit. 1 Introduction Large pre-trained Visual-Language Models (VLMs) have been developed a lot in recent years. For example, CLIP (Radford et al., 2021) and ALIG (Jia et al., 2021) demonstrated remarkable performance for various tasks, e.g., image recog- nition in a zero-shot fashion. To further improve the performance on the specific downstream tasks, prompt tuning (Lester et al., 2021; Yao et al., 2023; Zhu et al., 2023; Zhou et al., 2022a) and adapter tuning (Gao et al., 2023; Zhang et al., 2021) methods have been proposed. As shown in Fig. 1, prompt tuning methods proposed to introduce a set of learnable prompt vectors as the input of the text encoder while adapter tuning approaches adopted an additional bottleneck layer to learn new features. During the fine-tuning procedure, both of these two strategies keep CLIP‚Äôs parameters fixed. The performance of prompt tuning and adapter tuning methods are superior on various tasks (Zhou et al., 2022b; Gao et al., 2023), so research on fine-tuning the inherent parameters of VLMs has been barely touched. For language models, fully fine-tuning with downstream data can achieve promising results (Zaken et al., 2021; Liu et al., 2022). Moreover, recent works in language model fine-tuning (e.g., BitFit (Zaken et al., 2021)) have demonstrated that, without introducing any external parameters, fine- tuning only the bias terms in a pre-trained model can perform competitively on downstream tasks compared with fine-tuning the entire model. For VLMs, however, it is believed that fine-tuning the parameters of VLMs corrupts the inherent pre- trained knowledge as fully fine-tuning degrades performance (Zhou et al., 2022b). In this paper, we revisit this viewpoint and ask if, without in- troducing any external parameters, fine-tuning the inherent parameters of VLMs can achieve competi- tive performance compared with prompt tuning. We start with directly applying BitFit to fine- tuning the CLIP model. We explore two strategies: (i) applying BitFit to the text encoder alone, and (ii) applying BitFit to both the text and image encoder. We found that both two strategies can acquire task- specific knowledge but their performance to unseen class data can be poor (more discussed in Sec. 4.4), implying that directly fine-tuning the bias terms of a text or image encoder may harm the model‚Äôs generalization ability. These findings motivate us to develop more effective and efficient fine-tuning techniques for VLMs. In light of this, we propose CLIPFit, a simple yet arXiv:2409.16718v2  [cs.CV]  19 Nov 2024",
    "body": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\nModification\nMing Li 1, Jike Zhong2, Chenxin Li4, Liuzhuozheng Li1,\nNie Lin1, Masashi Sugiyama3,1\n1The University of Tokyo, 2University of Southern California,\n3RIKEN Center for Advanced Intelligence Project,\n4 The Chinese University of Hong Kong\nli-ming948@g.ecc.u-tokyo.ac.jp\njikezhon@usc.edu\nchenxinli@stu.xmu.edu.cn\nl.li@ms.k.u-tokyo.ac.jp\nnielin@iis.u-tokyo.ac.jp\nsugi@k.u-tokyo.ac.jp\nAbstract\nRecent\nadvances\nin\nfine-tuning\nVision-\nLanguage Models (VLMs) have witnessed\nthe success of prompt tuning and adapter\ntuning, while the classic model fine-tuning on\ninherent parameters seems to be overlooked.\nIt is believed that fine-tuning the parameters\nof VLMs with few-shot samples corrupts\nthe pre-trained knowledge since fine-tuning\nthe CLIP model even degrades performance.\nIn this paper, we revisit this viewpoint, and\npropose a new perspective: fine-tuning the\nspecific parameters instead of all will uncover\nthe power of classic model fine-tuning on\nVLMs.\nThrough our meticulous study, we\npropose ClipFit, a simple yet effective method\nto fine-tune CLIP without introducing any\noverhead of extra parameters. We demonstrate\nthat by only fine-tuning the specific bias terms\nand normalization layers, ClipFit can improve\nthe performance of zero-shot CLIP by 7.27%\naverage harmonic mean accuracy.\nLastly,\nto understand how fine-tuning in CLIPFit\naffects the pre-trained models, we conducted\nextensive experimental analyses w.r.t. changes\nin internal parameters and representations. We\nfound that low-level text bias layers and the\nfirst layer normalization layer change much\nmore than other layers. The code is available at\nhttps://github.com/minglllli/CLIPFit.\n1\nIntroduction\nLarge\npre-trained\nVisual-Language\nModels\n(VLMs) have been developed a lot in recent years.\nFor example, CLIP (Radford et al., 2021) and\nALIG (Jia et al., 2021) demonstrated remarkable\nperformance for various tasks, e.g., image recog-\nnition in a zero-shot fashion. To further improve\nthe performance on the specific downstream tasks,\nprompt tuning (Lester et al., 2021; Yao et al., 2023;\nZhu et al., 2023; Zhou et al., 2022a) and adapter\ntuning (Gao et al., 2023; Zhang et al., 2021)\nmethods have been proposed. As shown in Fig.\n1, prompt tuning methods proposed to introduce\na set of learnable prompt vectors as the input of\nthe text encoder while adapter tuning approaches\nadopted an additional bottleneck layer to learn new\nfeatures. During the fine-tuning procedure, both\nof these two strategies keep CLIP‚Äôs parameters\nfixed.\nThe performance of prompt tuning and\nadapter tuning methods are superior on various\ntasks (Zhou et al., 2022b; Gao et al., 2023), so\nresearch on fine-tuning the inherent parameters of\nVLMs has been barely touched.\nFor language models, fully fine-tuning with\ndownstream data can achieve promising results\n(Zaken et al., 2021; Liu et al., 2022). Moreover,\nrecent works in language model fine-tuning (e.g.,\nBitFit (Zaken et al., 2021)) have demonstrated that,\nwithout introducing any external parameters, fine-\ntuning only the bias terms in a pre-trained model\ncan perform competitively on downstream tasks\ncompared with fine-tuning the entire model. For\nVLMs, however, it is believed that fine-tuning the\nparameters of VLMs corrupts the inherent pre-\ntrained knowledge as fully fine-tuning degrades\nperformance (Zhou et al., 2022b). In this paper,\nwe revisit this viewpoint and ask if, without in-\ntroducing any external parameters, fine-tuning the\ninherent parameters of VLMs can achieve competi-\ntive performance compared with prompt tuning.\nWe start with directly applying BitFit to fine-\ntuning the CLIP model. We explore two strategies:\n(i) applying BitFit to the text encoder alone, and (ii)\napplying BitFit to both the text and image encoder.\nWe found that both two strategies can acquire task-\nspecific knowledge but their performance to unseen\nclass data can be poor (more discussed in Sec. 4.4),\nimplying that directly fine-tuning the bias terms\nof a text or image encoder may harm the model‚Äôs\ngeneralization ability. These findings motivate us\nto develop more effective and efficient fine-tuning\ntechniques for VLMs.\nIn light of this, we propose CLIPFit, a simple yet\narXiv:2409.16718v2  [cs.CV]  19 Nov 2024\n\nPrompt\nüî•\n[class]\nText Encoder\nImage Encoder\n‚ùÑ\n‚ùÑ\ncos\n(a) Prompt Tuning \nA photo of a [class].\nText Encoder\nImage Encoder\n‚ùÑ\n‚ùÑ\ncos\n(b) Adapter Tuning \nAdapter\nüî•\nText Encoder\nImage Encoder\ncos\n(c) CLIPFit ‚Äì Our Method \nA photo of a [class].\nüî•\n/\n‚ùÑ\nüî•\n/\n‚ùÑ\nFigure 1: Comparison of (a) prompt tuning methods, (b) adapter tuning methods, and (c) our proposed CLIPFit\nmethod. Prompt tuning methods introduce a set of learnable external parameters as input to learn task-specific\nknowledge. Adapter tuning methods introduce extra learnable networks following the image encoder to learn\ntask-specific features. Unlike these two methods, our CLIPFit does not introduce external parameters and fine-tunes\nonly a small portion of the CLIP model.\neffective method for efficiently fine-tuning VLMs.\nCLIPFit is orthogonal to previous prompt tuning\nand adapter tuning methods, as shown in Fig. 1 (c).\nFor the text encoder, instead of fine-tuning all the\nbias terms, CLIPFit proposes to tune only the bias\nterms of projection linear layers in feed-forward\nnetworks (FFNs). Fine-tuning only the bias terms\nof projection linear layers in FFNs will reduce the\nnumber of training parameters compared with fine-\ntuning all the bias terms. Moreover, empirically,\nwe discovered that our bias term tuning strategy can\ngeneralize better than BitFit (Zaken et al., 2021),\nas shown in Sec. 4.4. For the image encoder, as\nmentioned before, it may harm the model‚Äôs per-\nformance if directly applying BitFit. In the im-\nage encoder, layer normalization (LayerNorm) (Ba\net al., 2016) aims to normalize the distributions of\nintermediate layers. Since the distributions of pre-\ntraining and downstream data might be divergent,\npre-trained LayerNorm might lead to sub-optimal\nperformance for downstream data inference. There-\nfore, CLIPFit proposes to further update only the\nparameters of the image encoder‚Äôs LayerNorm. Up-\ndating LayerNorm can yield a better image encoder\nfor downstream data. Lastly, previous studies (Yao\net al., 2023) have shown that generic pre-trained\nknowledge is easily forgotten in the fine-tuning\nstage. Therefore, we explored two different regu-\nlarization strategies for alleviating forgetting: (i)\nusing the knowledge distillation (KD) loss (Hinton\net al., 2015) to guide CLIPFit to learn from the\nzero-shot CLIP; (ii) using the mean squared error\n(MSE) loss in bias terms to penalize changes in\ntext encoder. We empirically found that both two\nstrategies can alleviate forgetting problems and the\nKD loss performs better, thus we used the KD loss\nas the final solution for CLIPFit.\nFine-tuning is an empirical and black-box pro-\ncess. So, understanding how fine-tuning affects\nthe pre-trained models is important for uncovering\nthe black-box fine-tuning process. Previous works\n(Zhou and Srikumar, 2022; De Vries et al., 2020;\nMerchant et al., 2020) explored this for language\nmodels fine-tuning. However, very little work ex-\nplored the internal black-box fine-tuning process\nfor VLMs. In this paper, we conducted an initial\nexploration to analyze VLM fine-tuning process of\nCLIPFit, focusing on changes in internal param-\neters and representations. We found that for bias\nterms in the FNN of the text encoder, as the number\nof layers increases, the change in bias decreases,\nwhich means that during the fine-tuning process,\nlow-level features in the text encoder change more\nthan high-level features. For LayerNorm in the\nimage encoder, we found that the first layer (patch\nembedding) changes much more than other lay-\ners. Experimentally, we showed that more changed\nlayers play a more important role in adapting down-\nstream knowledge than less changed layers. More-\nover, we explored how KD loss affects the fine-\ntuning process for alleviating forgetting. We found\nthat KD loss will reduce the changes for the more-\nchanged low-level bias terms and enhance changes\nin less-changed high-level layers, which implies\nthat penalizing changes for low-level bias terms is\nimportant for avoiding overfitting. Lastly, we found\nthat tuning LayerNorm will form a better image\nfeature space compared with zero-shot CLIP.\nWe conducted extensive experiments on 11\ndatasets in 4 different settings to show the effective-\nness of the proposed CLIPFit. Overall, our main\ncontributions can be summarized as follows:\n‚Ä¢ We propose a CLIPFit method for efficiently\nfine-tuning the CLIP model to uncover the\n\npower of classic model fine-tuning on VLMs.\nUnlike existing prompt tuning or adapter tun-\ning methods, CLIPFit does not introduce any\nexternal parameters and only fine-tunes a\nsmall specific subset of CLIP‚Äôs inherent pa-\nrameters.\n‚Ä¢ To analyze how CLIPFit affects the pre-\ntrained models, we conducted extensive analy-\nses during the fine-tuning process, focusing on\nthe changes in parameters and representations.\nThese analyses help us better understand the\nblack-box fine-tuning process.\n‚Ä¢ We conducted extensive experiments on 11\ndatasets. Results show that CLIPFit brings a\n7.33% improvement in harmonic mean accu-\nracy compared with zero-shot CLIP on the 16-\nshot base-to-new setting, demonstrating that\nCLIPFit is a promising alternative to prompt\ntuning and adapter tuning.\n2\nRelated Works\nVisual-Language Models (VLMs). With large-\nscale available web-crawled image-text pairs\n(Schuhmann et al., 2022), pre-training VLMs have\nbeen developed fast in recent years (Xu et al., 2021;\nRadford et al., 2021; Jia et al., 2021; Wang et al.,\n2022a) and achieved remarkable zero-shot perfor-\nmance in the downstream tasks, e.g., image clas-\nsification. Despite the remarkable transfer ability,\nthe potential of VLMs can be further stimulated\nby fine-tuning it with few-shot downstream data\n(Song et al., 2022; Zhang et al., 2021; Shen et al.,\n2021; Wang et al., 2022c,b; Chen et al., 2023a).\nParameter-efficient Fine-tuning (PEFT) on\nVLMs. There are mainly two categories of VLM\nparameter-efficient fine-tuning methods: prompt\ntuning (Zhou et al., 2022b,a; Chen et al., 2022; Yao\net al., 2023; Zhu et al., 2023; Zhang et al., 2023;\nKhattak et al., 2023) and adapter tuning (Gao et al.,\n2023; Zhang et al., 2021). Prompt tuning methods\nfor VLMs introduced a few learnable parameters\n(prompts) as input, which were inspired by lan-\nguage prompt tuning (Lester et al., 2021). Adapter\ntuning methods set an additional bottleneck layer\nfollowing the text or image encoder to learn better\nfeatures by a residual way. Both prompt tuning and\nadapter tuning methods boost CLIP‚Äôs performance,\nso research on fine-tuning the inherent parameters\nof CLIP seems to be overlooked. To explore classic\nmodel fine-tuning on VLMs, our CLIPFit proposes\nto fine-tune CLIP by modifying a small portion\nof the CLIP model‚Äôs inherent parameters without\nintroducing any external learnable parameters.\nPEFT on Large Language Models. Fully fine-\ntuning language models (Radford et al., 2018; De-\nvlin et al., 2018) can achieve promising results but\nis expensive. To efficiently fine-tune pre-trained\nlanguage models, a lot of approaches have sought\nto fine-tune only a small number of parameters.\nFor example, adapter methods (Bapna et al., 2019;\nHoulsby et al., 2019; Pfeiffer et al., 2020) and\nprompt tuning methods (Liu et al., 2023; Lester\net al., 2021; Brown et al., 2020; Gao et al., 2020)\nintroduce a set of learnable external parameters\nfor adaptation to downstream tasks. Recently, Bit-\nFit (Zaken et al., 2021) demonstrated that, without\nintroducing any new parameters, fine-tuning only\nthe bias terms in pre-trained language models can\nperform competitively compared with fully fine-\ntuning. However, BitFit is designed for LLM fine-\ntuning, and our experiments in Sec. 4 shows that\ndirectly applying BitFit to VLM fine-tuning may\nharm the model‚Äôs generalization ability. Thus, our\nCLIPFit proposes to only fine-tune the LayerNorm\nof image encoder motivated by distribution shift.\nOur method is different to BitFit Moreover, to un-\nderstand how fine-tuning affects pre-trained mod-\nels, various works (Zhou and Srikumar, 2022; Mos-\nbach et al., 2020; De Vries et al., 2020; Merchant\net al., 2020) have explored this with LLM fine-\ntuning. However, very little work was attempted\non the VLM side. In this paper, we attempt to\nbridge this gap by conducting an initial exploration\nto analyze the fine-tuning process in CLIPFit for\nVLMs, focusing on changes in internal parameters\nand representations.\n3\nMethodolgy\nIn this section, we introduce CLIPFit. We first\nbriefly review CLIP and then illustrate CLIPFit.\n3.1\nReview of CLIP\nWe first briefly review CLIP (Radford et al., 2021).\nDuring pre-training, CLIP aims to align image fea-\ntures and text features in the joint embedding space\nto capture the relationship between images and\ntexts. Let D = {(xi, ti)}b\ni=1 be the sampled batch,\nwhere xi is the input image, ti is the input text and\nb is the batch size. A CLIP model is comprised\nof two types of encoders: visual encoder EI(¬∑, Œ∏I)\nand text encoder ET(¬∑, Œ∏T). The visual encoder\n\nA photo of a [class].\nWord Embed \nAttn\nLayer Norm\nFFN\nText Encoder\nFFN\nImage Encoder\nEmbeding\nEmbeding\nLinear\nLayer\n√óN\n√óN\nüî•\n‚ùÑ\nLayer Norm\n‚ùÑ\nx1\n‚Ä¶\nx1¬∑y1 x1¬∑y2 x1¬∑y3\nx1¬∑yn\n‚Ä¶\nx2\nx3\nxn\ny1\ny2\ny3\nyn\nx2¬∑y1 x2¬∑y2 x2¬∑y3\nx2¬∑yn\nx3¬∑y1 x3¬∑y2 x3¬∑y3\nx3¬∑yn\nxn¬∑y1 xn¬∑y2 xn¬∑y3\nxn¬∑yn\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚ùÑ\nAttn\n‚ùÑ\n‚ùÑ\nLayer Norm\nüî•\nLayer Norm\nüî•\nBiasüî•\n‚ùÑ\nEmbeding\nFFN\nImage\n‚ùÑ\nWeight\nPatch Embed \ncat\ndog\nbird\n‚Ä¶\nüî•\n‚ùÑ\nTuned\nFrozen\nFigure 2: An overview of our CLIPFit. Unlike existing prompt tuning methods or adapter tuning methods, CLIPFit\ndoes not introduce any external parameters and fine-tunes specific inherent parameters of CLIP. For the text\nencoder, as shown in the upper part of the figure, CLIPFit fine-tunes only the bias terms of projection linear layers in\nfeed-forward networks. For the image encoder, as shown in the lower part of the figure, CLIPFit updates LayerNorm.\nencodes image xi into f i and text ti into gi, i.e.,\nf i = EI(xi, Œ∏I),\ngj = ET(ti, Œ∏T).\n(1)\nThen, a contrastive learning loss is applied to them\nfor alignment.\nAfter pre-training, CLIP can perform zero-shot\nimage recognition by comparing the image features\nwith class weights {wi}K\ni=1, where K is the num-\nber of classes. The class weight wi is generated\nby text encoder ET (¬∑, Œ∏T ) which takes the class\ndescriptions (prompts) as input. These prompts\nusually take the form ‚Äúa photo of a [CLASS].‚Äù,\nwhere the class token will be replaced by the i-th\nclass name (e.g., cat) for weight wi. Formally, for\nan image feature f, the probability that it belongs\nto class i is calculated by\np(y = i | x) =\nexp (cos (wi, f) /œÑ)\nPK\nj=1 exp (cos (wj, f) /œÑ)\n, (2)\nwhere œÑ is a temperature parameter learned by\nCLIP during pre-training and cos(¬∑, ¬∑) denotes the\ncosine similarity function.\n3.2\nCLIPFit\nThe overall pipeline of the proposed CLIPFit is\nshown in Fig. 2. Without introducing any external\nparameters, CLIPFit involves fine-tuning only the\nbias terms of projection linear layers in FNNs of\nthe text encoder and updating LayerNorm (Ba et al.,\n2016) in the image encoder.\nText Encoder. For the text encoder, instead\nof fine-tuning all bias terms, CLIPFit fine-tunes\nonly the bias terms of projection linear layers (i.e.,\nsecond layers) in the FFNs of the text encoder.\nFine-tuning only part of bias terms will reduce\nthe number of training parameters compared with\nfine-tuning all bias terms. Moreover, Sec. 4.4 will\nempirically show that our bias tuning method can\nachieve better performance compared with fine-\ntuning all bias terms (Zaken et al., 2021).\nImage Encoder. As mentioned in Sec. 1, di-\nrectly applying BitFit (Zaken et al., 2021) to the\nimage encoder may cause a negative impact on\nthe model‚Äôs performance. Instead of fine-tuning\nthe bias terms of the image encoder, CLIPFit pro-\nposes to fine-tune LayerNorm. In LayerNorm, the\ntwo learniable parameters gain g and bias b are ap-\nplied for affine transformation on normalized input\nvectors x for re-centering and re-scaling, which\nare expected to enhance the expressive power by\nre-shaping the distribution (Ba et al., 2016). Dif-\nferent data distributions should produce different\ngains and biases in LayerNorm for distribution re-\nshaping during the training process. Prior work has\nalso found norm layers to be an influential com-\nponent in the fine-tuning process (Giannou et al.,\n2023; Qi et al., 2022; Tu et al., 2023; Zhong et al.,\n2024). So, if shifted gains and biases in Layer-\nNorm are applied during inference, it may lead to a\nsub-optimal solution. Therefore, CLIPFit proposes\n\nto fine-tune LayerNorm in the image encoder.\nLoss function. Previous works (Yao et al., 2023;\nXuhong et al., 2018) have verified that during the\nfine-tuning stage, generic pre-trained knowledge is\neasily forgotten. Therefore, we explore two differ-\nent strategies for alleviating such forgetting. The\nfirst one is to use the knowledge distillation (Hinton\net al., 2015; Yao et al., 2023) loss to guide CLIP-\nFit to learn from the original zero-shot CLIP. Let\n{wclip\ni\n}K\ni=1 and {wi}K\ni=1be the text features from\noriginal CLIP and text features from CLIPFit. The\ntraining loss and KD loss of CLIPFit are defined\nby\nL = Lce + Œ≤Lkg,\n(3)\nLkg = 1\nK\nK\nX\ni=1\ncos(wclip\ni\n, wi),\n(4)\nwhere Lce is the cross entropy loss for classification\n(Zhou et al., 2022b,a) and Œ≤ is a hyperparameter.\nThe second strategy is using the MSE loss in\nbias terms to penalize changes in the text encoder.\nLet {bclip\ni\n}L\ni=1 and {bi}L\ni=1 be the unfixed text bias\nterms from pre-trained CLIP and unfixed text bias\nterms from CLIPFit, where L is the number of\nunfixed bias layers. The MSE loss is defined as\nLmse = 1\nL\nL\nX\ni=1\n||bclip\ni\n‚àíbi||2.\n(5)\nWe found that both strategies can alleviate the for-\ngetting problems and the KD loss performs better\n(as discussed in Sec. 4.3), thus we adopted the KD\nloss as the final solution for CLIPFit.\n4\nExperiments\nIn this section, we show and discuss the experi-\nmental results. To evaluate the effectiveness of our\nproposed method, we conducted extensive experi-\nments and analyses on 11 datasets.\n4.1\nExperimental Setup\nDatasets. Following CoOp, we conducted exten-\nsive experiments on 11 public classification bench-\nmark datasets to evaluate CLIPFit. The datasets\nare ImageNet (Deng et al., 2009), Caltech101 (Fei-\nFei et al., 2004), OxfordPets (Parkhi et al., 2012),\nStanfordCars (Krause et al., 2013), Flowers102\n(Nilsback and Zisserman, 2008), Food101 (Bossard\net al., 2014), FGVCAircraft (Maji et al., 2013),\nSUN397 (Xiao et al., 2010), DTD (Cimpoi et al.,\n2014), EuroSAT (Helber et al., 2019), and UCF101\n(Soomro et al., 2012). Implementation details.\nDataset\nCLIP CoOp CoCoOp Adapter KgCoOp MaPLe CLIPFit\nAverage\nBase 69.34 82.69\n80.47\n82.23\n80.73\n82.28\n83.72\nNew 74.22 63.22\n71.69\n70.61\n73.60\n75.14\n74.84\nHM 71.70 71.66\n75.83\n75.98\n77.00\n78.55\n79.03\nImageNet\nBase 72.43 76.47\n75.98\n76.13\n75.83\n76.66\n76.2\nNew 68.14 67.88\n70.43\n67.17\n69.96\n70.54\n70.17\nHM 70.22 71.92\n73.10\n71.37\n72.78\n73.47\n73.06\nCaltech101\nBase 96.84 98.00\n97.96\n97.40\n97.72\n97.74\n98.3\nNew 94.00 89.81\n93.81\n93.23\n93.70\n94.36\n93.7\nHM 95.40 93.73\n95.84\n95.51\n96.03\n96.02\n95.94\nOxfordPets\nBase 91.17 93.67\n95.20\n94.33\n94.65\n95.43\n95.23\nNew 97.26 95.29\n97.69\n97.10\n97.76\n97.76\n97.13\nHM 94.12 94.47\n96.43\n95.69\n96.18\n96.58\n96.17\nStanford\nCars\nBase 63.37 78.12\n70.49\n76.10\n71.76\n72.94\n78.80\nNew 74.89 60.40\n68.87\n71.20\n75.04\n74.00\n73.87\nHM 68.65 68.13\n72.01\n72.30\n73.36\n73.47\n76.26\nFlowers102\nBase 72.08 97.60\n94.87\n97.23\n95.00\n95.92\n96.83\nNew 77.80 59.67\n71.75\n69.27\n74.73\n72.46\n73.53\nHM 74.83 74.06\n81.71\n80.90\n83.65\n82.56\n83.59\nFood101\nBase 90.10 88.33\n90.70\n90.37\n90.50\n90.71\n90.6\nNew 91.22 82.26\n91.29\n90.83\n91.70\n92.05\n91.33\nHM 90.66 85.19\n90.99\n90.6\n91.09\n91.38\n90.96\nFGVC\nAircraft\nBase 27.19 40.44\n33.41\n38.70\n36.21\n37.44\n42.47\nNew 36.29 22.30\n23.71\n32.27\n33.55\n35.61\n33.47\nHM 31.09 28.75\n27.74\n35.19\n34.83\n36.50\n37.43\nSUN397\nBase 69.36 80.60\n79.74\n81.57\n80.29\n80.82\n81.97\nNew 75.35 65.89\n76.86\n74.03\n76.53\n78.70\n78.17\nHM 72.23 72.51\n78.27\n77.62\n78.36\n79.75\n80.02\nDTD\nBase 53.24 79.44\n77.01\n79.53\n77.55\n80.36\n81.97\nNew 59.90 41.18\n56.00\n51.67\n54.99\n59.18\n63.5\nHM 56.37 54.24\n64.85\n62.64\n64.35\n68.16\n71.56\nEuroSAT\nBase 56.48 92.19\n87.49\n87.70\n85.64\n94.07\n93.33\nNew 64.05 54.74\n60.04\n58.83\n64.34\n73.23\n71.07\nHM 60.03 68.69\n71.21\n70.42\n73.48\n82.35\n80.69\nUCF101\nBase 70.53 84.69\n82.33\n85.47\n82.89\n83.00\n85.23\nNew 77.50 56.05\n73.45\n72.97\n76.67\n78.66\n77.3\nHM 73.85 67.46\n77.64\n78.73\n79.65\n80.77\n81.07\nTable 1: Accuracy comparison on Base-to-new generalization\nof CLIPFit with previous methods. Adapter: CLIP-Adapter.\nWe implemented our method with PyTorch (Paszke\net al., 2019). The experiments were based on the\nvision backbone with Vit-B/16 (Dosovitskiy et al.,\n2020). We followed CoOp to preprocess input\nimages. We used a single text prompt for all ex-\nperiments for a fair comparison. We used SGD\noptimizer with batch size set as 32, and set the\nlearning rate as 0.002 (Zhou et al., 2022b). All\nresults reported below are the average of three runs\nwith different random seeds. The training epoch\nwas set to 100 for all datasets except ImageNet and\nFood101. Œ≤ was set to 8 for all datasets on the\nbase-to-new and cross-dataset setting, and 2 for the\ndistribution shift setting. For the few-shot setting,\nwe set Œ≤ to 2 for all datasets except SUN397 and\nDTD. More implementation details are provided in\nappendix B.\nComparisons.\nWe compared our method\nagainst state-of-the-art methods: zero-shot CLIP,\nprompt tuning methods: CoOp, CoCoOp (Zhou\net al., 2022a), ProGrad (Zhu et al., 2023), KgCoOp\n\n(Yao et al., 2023), MaPLe (Khattak et al., 2023) and\nadapter tuning methods: CLIP-adapter, Tip-adapter\n(Zhang et al., 2021). Detailed introductions to these\nmethods can be found in appendix C.\n4.2\nComparisons with State-of-the-arts\nResults on base-to-new generalization setting\nFollowing Zhou et al. (2022b), we split each date-\nset into two disjoint groups: the base class dataset\nand the new class dataset. All compared meth-\nods and the proposed CLIPFit were trained on the\nbase class dataset and evaluated on the new class\ndataset. We conducted 4/8/16-shot experiments,\nfollowing Yao et al. (2023). We reported base and\nnew class accuracies (Base and New) and their har-\nmonic mean accuracy (HM). The 16-shot results\nare shown in Table 1, and 4/8-shot results are pro-\nvided in appendix G. As shown in Table 1, CLIPFit\nachieves 6 best HM accuracies among 11 datasets\nand the best average HM accuracy, which demon-\nstrates that CLIPFit can not only learn well on seen\nbase class data but also can generalize well to data\nfrom unseen new classes. A notable issue with pre-\nvious methods like CoOp, CoCoOp, ProGrad, and\nKgCoOp is that they usually perform well only on\neither the base or new class. To alleviate this issue,\nMaPLe proposes a multi-modal prompt learning\nstrategy for CLIP tuning which improves a lot over\nHM compared to previous methods. Compared\nto MaPLe, our CLIPFit achieves better HM accu-\nracy and average performance on the base class,\nwith slightly lower average performance on the\nnew class. It is important to note that CLIPFit\nonly needs to tune nearly 46K parameters while\nMaPLe needs to tune nearly 3.55M parameters for\neach task, which is 77 times more than CLIPFit,\nmeaning that CLIPFit fine-tunes significantly fewer\nparameters and is much more efficient.\nResults on few-shot learning setting. To ver-\nify whether the proposed CLIPFit can learn task-\nspecific knowledge, we also compared CLIPFit\nwith other existing methods on the few-shot learn-\ning setting. Following Zhou et al. (2022a), we used\n1, 2, 4, 8, and 16-shot sets for training and reported\naccuracy performance. We report the results of the\naverage accuracy of 11 datasets in Table. 2, and\nreport all results on each dataset in appendix F. As\nshown in Table 2, compared with other methods,\nCLIPFit shows overall consistent improvements\namong all 1/2/4/8/16-shot settings. This demon-\nstrates that CLIPFit can successfully learn task-\nspecific knowledge. It is worth noting that CLIPFit\nTable 2: Comparison of CLIPFit and other methods on\nthe few-shot learning setting. We report average accu-\nracy on 11 datasets for the 1/2/4/8/16-shot setting.\nMethod\nshot\n1\n2\n4\n8\n16\nCoOP\n68.09\n70.13\n73.59\n76.45\n79.01\nCLIP-adapter\n67.87\n70.20\n72.65\n76.92\n79.86\nCoCoOp\n66.95\n67.63\n71.98\n72.92\n75.02\nProGrad\n68.2\n71.78\n74.21\n77.93\n79.2\nKgCoOp\n69.51\n71.57\n74.48\n75.82\n77.26\nTip-adapter\n70.62\n73.08\n75.75\n78.51\n81.15\nCLIPFit\n72.32\n74.39\n77.18\n79.03\n81.27\nTable 3: Comparison of our method against other meth-\nods on robustness to distribution shift.\nMethod\nSouce\nTarget\nAverage\nImageNet\n-V2\n-Sketch\nCLIP\n66.73\n60.83\n46.15\n57.90\nCoOP\n71.51\n64.20\n47.99\n61.23\nCLIP-adapter\n71.60\n63.67\n46.52\n60.60\nTip-adapter\n73.10\n64.82\n46.73\n61.55\nCoCoOp\n71.02\n64.07\n48.75\n61.28\nProGrad\n72.24\n64.73\n47.61\n61.53\nKgCoOp\n71.20\n64.10\n48.97\n61.42\nCLIPFit\n71.53\n64.83\n48.87\n61.74\noutperforms other methods by a large margin in\n1/2/4-shot settings, demonstrating CLIPFit‚Äôs ro-\nbust ability to learn with extremely few samples.\nResults on the robustness to distribution shift\nsetting. Following Zhang et al. (2021), we evalu-\nated the robustness under distribution shift of CLIP-\nFit and other methods by first training models on\nthe 16-shot ImageNet dataset and then evaluating\non ImageNet-V2 (Recht et al., 2019) and ImageNet-\nSketch (Wang et al., 2019). The label sets of two\nevaluating datasets are subsets of the label set of\nImageNet. Although the label sets are compati-\nble, the distributions of these three datasets are\ndifferent from each other. The results are shown in\nTable 3. As shown in Table 3, while TIP-adapter\nachieves the best performance on the ImageNet\ndataset, CLIPFit can achieve better average perfor-\nmance compared to existing methods, effectively\nunderlining the robustness of CLIPFit.\nResults on cross-dataset transfer setting is pro-\nvided in appendix D.\n4.3\nFine-tuning Analysis\nAnalyzing parameter change. To understand the\nblack-box fine-tuning process in CLIPFit, we first\nanalyzed changes in the parameters of both the\ntext encoder and image encoder. We computed\nthe squared difference ||ppre ‚àíp||2 for each layer,\nwhere ppre is the pre-trained parameter vector and\np is the fine-tuned parameter vector. We conduct\nexperiments on the DTD dataset. The results are\nshown in Fig. 3. As observed Fig. 3 (a), for bias\n\n(a) Text encoder\n(b) Image encoder\nFigure 3: Visualization of changes in different layers.\nterms in the FNN of the text encoder, when the\nnumber of layers increases, the change in bias de-\ncreases, which implies that low-level features in the\ntext encoder change more than high-level features\nduring the fine-tuning process of CLIPFit. From\nFig. 3 (b), we found that for LayerNorm in the im-\nage encoder, the first layer (i.e., patch embedding\nlayer) changes much more compared with other\nlayers for both bias and gain, showing that tuning\npatch embedding LayerNorm is crucial for shifted\ndownstream tasks. Moreover, the gain of the last\nseveral LayerNorm layers has much changed and\nthe most intermediate layers change much less. The\ndifference in change between different layers may\nbe caused by gradient difference. We visualize the\nsquared sum of gradient from each text bias layer\nin Fig. 4 (a). As observed, the curve of the gradient\nsum is very similar to changes in parameters.\nTo verify whether more changed layers are more\nimportant in fine-tuning, we conducted experi-\nments by freezing less (or more) changed Layer-\nNorm layers on the 4-shot setting. We found that\nwhen only updating the first LayerNorm layer and\nfreezing other LayerNorm layers, the average ac-\ncuracy is 76.22%. For comparison, the average\naccuracy is 74.93% when only updating the twelfth\nLayerNorm, and 75.06% when updating the last\nLayerNorm. Both are much less than the first layer\nand these two layers change much less than the first\nlayer, as shown in Fig. 3 (b). Moreover, when up-\ndating the top 6 most changed LayerNorm layers,\nthe average accuracy is 77.03%, which is only a\n0.15% drop, while only tuning 23% parameters of\nCLIPFit. The phenomenon for text encoder is simi-\nlar and can be found in appendix H. These results\ndemonstrate that the more changed layers are more\nimportant for knowledge adapting.\nAnalyzing regularization loss. We then ana-\nlyze the two regularization losses: KD loss and\nbias term MSE loss. We found that both two losses\ncan avoid overfitting and boost performance dur-\ning fine-tuning. For the 4-shot learning task, fine-\ntuning w/ KD loss leads to a 77.18% average accu-\n(b) Change w/regulariza2on loss\n(a) Gradient sum\nFigure 4: Left: visualization of squared gradient sum.\nRight: visualization of change w/ regularization loss.\n(a) zero-shot CLIP\n(b) CLIPFit\nFigure 5: Visualization of learned image feature space\nfrom zero-shot CLIP and CLIPFit via t-SNE.\nracy, and fine-tuning w/ KD loss leads to a 76.23%\naverage accuracy. Both two performances are bet-\nter than fine-tuning w/o regularization loss (76.13%\naverage accuracy) and KD loss performs better. We\nthen analyze how these two losses affect changes\nin parameters during fine-tuning of CLIPFit. The\nresults are shown in Fig. 4 (b). As observed, KD\nloss will reduce the changes for the more-changed\nlow-level bias terms and enhance changes in less-\nchanged high-level layers, which implies that pe-\nnalizing changes for low-level bias terms is impor-\ntant in avoiding overfitting. Compared with KD\nloss, MSE loss directly applying to text bias terms\nreduces more changes in low-level layers.\nAnalysing image encoder representations. We\nused t-SNE (Van der Maaten and Hinton, 2008) to\nvisualize the image representation space of zero-\nshot CLIP and CLIPFit to analyze image encoder\nrepresentations. We visualize the data from Eu-\nroSAT dataset. The visualization results are pre-\nsented in Fig. 5. As observed, in high-dimensional\nclassification feature space, CLIPFit has a much\nclearer separation of different class image fea-\ntures compared with zero-shot CLIP, which demon-\nstrates that CLIPFit can better detect the similar-\nities among images. These results verify that up-\ndating LayerNorm in the image encoder during\nfine-tuning will lead to a more separated and better\nsimilarity-detected image feature space.\nUpdating LayerNorm can also benefit other\n\nTable 4: Comparison of prompt tuning and adapter tun-\ning methods w/ and w/o updating LayerNorm on the 16-\nshot base-to-new setting. +UL means training updating\nLayerNorm.\nMethod\nBase\nNew\nH\nCoOp\n82.63\n67.99\n74.60\n+UL\n82.96(+0.33)\n69.09(+1.10)\n75.39(+0.79)\nKgCoOp\n80.73\n73.60\n77.00\n+UL\n82.13(+1.40)\n74.96(+1.36)\n78.38(+1.38)\nCLIP-adapter\n82.23\n70.61\n75.98\n+UL\n83.63(+1.40)\n71.87(+1.26)\n77.31(+1.33)\nTable 5: Comparison of different strategies of fine-\ntuning bias terms in CLIP.\nStrategy\nBase\nNew\nH\n# param.\n(a) Text+Image bias\n84.15\n64.35\n72.93\n0.17M\n(b) Text bias\n83.33\n64.43\n72.67\n67.6K\n(c) FFNs bias (Text)\n83.25\n67.60\n74.61\n30.7K\n(d) Projection bias (Text)\n83.23\n67.58\n74.59\n6.1K\nmethods. We show that updating LayerNorm can\nalso benefit prompt tuning methods and adapter tun-\ning methods. We re-implemented CoOp, KgCoOp,\nand CLIP-adapter with updating LayerNorm. The\nresults are shown in Table 4. Table 4 shows that\ntraining with LayerNorm updating can boost the\nbase class, new class, and harmonic mean perfor-\nmance for all three methods. For example, train-\ning KgCoOp with updating LayerNorm can bring\n1.4%, 1.36%, and 1.38% improvements in the base\nclass, new class, and Harmonic Mean (HM) ac-\ncuracy, which demonstrates the effectiveness and\nwide validity of the proposed updating LayerNorm.\nMore detailed analyses about other datasets and\nother aspects are provided in appendix H.\n4.4\nAblation Study\nComparison of different strategies of fine-tuning\nbias terms. We give an in-depth exploration of\nhow to apply BitFit to fine-tune the CLIP model.\nOriginal BitFit fine-tunes all bias terms in language\nmodels. We conduct 4 strategies for fine-tuning\nbias terms of CLIP: (a) fine-tuning all bias terms\nof the text and image encoder; (b) fine-tuning all\nbias terms of the text encoder; (c) fine-tuning bias\nterms of FFNs of the text encoder; (d) fine-tuning\nbias terms of projection linear layers in FFNs of\nthe text encoder. We trained these four strategies\non the 16-shot base-to-new setting with Lce and\nreported average accuracy. The results are shown\nin Table 5. As shown in Table 5, both strategy (a)\nand strategy (b) can boost seen base class perfor-\nmance but will decrease significantly unseen new\nclass performance, which implies that directly ap-\nplying BitFit to CLIP may be harmful to model‚Äôs\ngeneralization ability. Moreover, strategy (c) and\nConfigurations\nAccuracy(%)\n1-shot\n4-shot\n16-shot\nPorjection Bias\n68.86\n74.72\n79.53\nw/ LayerNorm\n70.75\n76.13\n81.04\nw/ KD loss\n71.21\n75.94\n79.62\nw/ KD loss + LayerNorm\n72.32\n77.18\n81.27\nTable 6: Ablation study of CLIPFit on the few-shot set-\nting with 1/4/16-shot. Projection Bias: fine-tuning bias\nterms of projection layer in FNNs of the text encoder.\nLayerNorm: updating LayerNorm in the image encoder.\nMethod\nParams\ntime\nBase\nNew\nHM\nCoOp\n2048\n0.44ms\n82.69\n63.22\n71.66\nCoCoOp\n35k\n25.59ms\n80.47\n71.69\n75.83\nKgCoOp\n2048\n0.44ms\n80.73\n73.60\n77.00\nMaPLe\n3.55M\n2.1ds\n82.28\n75.14\n78.55\nCLIPFit\n44k\n0.96ms\n83.72\n74.84\n79.03\nTable 7: Comparison of training efficiency with other\nmethods over 11 datasets.\nstrategy (d) can have similar performance among\nboth the base and new class data, but strategy (d)\nfine-tunes only one-fifth of parameters compared\nwith strategy (c), which speeds up training.\nEffectiveness of proposed components. We\nvalidated the effects of updating LayerNorm and\nKD loss by ablating them. The results are shown\nin Table 6. Fine-tuning bias terms with KD loss\nbrings 2.35%, 1.22%, and 0.09% improvements\nfor 1/4/16-shot setting, respectively. Fine-tuning\nbias terms in the text encoder and LayerNorm\nin the image encoder brings 1.89%, 1.41%, and\n1.51% improvements for 1/4/16-shot setting, re-\nspectively. Together, CLIPFit brings 3.46%, 2.46%\nand 1.74% improvements for 1/4/16-shot setting,\nrespectively. These results demonstrate the effec-\ntiveness of each CLIPFit components.\nTraining efficiency. We compare the training\nefficiency of CLIPFit and other methods w.r.t. pa-\nrameters and training time per image (Yao et al.,\n2023). The results are shown in Table 7. It is\nnoticed that CoOp and KgCoOp have the lowest\nnumber of training parameters and time. However,\nthe performance of these two methods is not sat-\nisfactory. MaPLe improves accuracy performance\ncompared with other methods but also increases\nthe required tuning parameters to 3.55M, which is\nvery time-consuming. CLIPFit achieves the best\nharmonic mean accuracy with only 44k parameters,\nwhich is much less than MaPLe. Also, the training\ntime of CLIPFit is slightly higher than CoOp and\nKgCoOp. Given the large improvement of CLIPFit,\na slight increase in training time is acceptable.\n\n4.5\nDiscussion\nAlthough our method is designed for contrastive en-\ncoder VLMs (CLIP), the core idea of CLIPFit and\nour model analysis may still provide insights for\nother large multimodal model (e.g., LLaVA (Liu\net al., 2024)) fine-tuning and many furthur appli-\ncations (Touvron et al., 2023; Mizrahi et al., 2023;\nTeam et al., 2024a,b; Li et al., 2024b,a, 2023a;\nZhang et al., 2022; Chen et al., 2023b; Li et al.,\n2023b). For example, the idea of tuning Layer-\nNorm could be used when distributions of down-\nstream and pretraining QA image data are diver-\ngent, and parameter change and importance anal-\nysis (Sec. 4.3) could provide insights for how to\nselect fine-tuning parameters. We hope our work\ncan provide insights for a broader range of VLM\nfine-tuning.\n5\nConclusion\nIn this paper, we presented CLIPFit for fine-tuning\nvisual-language models. Unlike existing prompt\ntuning and adapter tuning methods, CLIPFit does\nnot introduce any external parameters and fine-\ntunes CLIP by updating only bias terms of pro-\njection layers in FFNs of the text encoder and the\nimage encoder‚Äôs LayerNorm. To understand the\neffect of CLIPFit fine-tuning on the pre-trained\nmodel, we conducted various analyses focusing\non changes in internal parameters and representa-\ntions. We conducted extensive experiments and\nanalysis to evaluate CLIPFit on 11 datasets, whose\nperformances show the superiority of our method.\n6\nLimitations\nIn this paper, we presented CLIPFit for VLM fine-\ntuning and conducted an exploration of how CLIP-\nFit affects the pre-trained CLIP model. Our analy-\nses found some interesting phenomena after fine-\ntuning, i.e., low-level bias terms in the text encoder\nchange much more than high-level bias terms and\nthe change in the first LayerNorm layer is much\nbigger than other LayerNorm layers in the image\nencoders. Moreover, we found that this may be\ncaused by the difference in the magnitude of the\ngradient. Nevertheless, our analysis does not reveal\nwhy the difference in the magnitude of the gradi-\nent happens during fine-tuning. A deeper analysis\nof gradient back-propagation during fine-tuning is\nneeded to understand this for future work.\nFurthermore, following previous works (Zhou\net al., 2022a; Yao et al., 2023; Zhou et al., 2022b;\nKhattak et al., 2023), this paper focused on im-\nage classification for VLMs, so our study was con-\nstrained to classification tasks. Expanding CLIPFit\nfor VLM fine-tuning to a broader range of tasks\n(e.g., image retrieval) could be the future work.\nAcknowledgement\nMS was partially supported by Institute for AI and\nBeyond, UTokyo.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat.\n2019. Simple, scalable adaptation for neural machine\ntranslation. arXiv preprint arXiv:1909.08478.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101‚Äìmining discriminative components\nwith random forests.\nIn Computer Vision‚ÄìECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part VI 13,\npages 446‚Äì461. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nGuangyi Chen, Weiran Yao, Xiangchen Song, Xinyue\nLi, Yongming Rao, and Kun Zhang. 2022. Prompt\nlearning with optimal transport for vision-language\nmodels. arXiv preprint arXiv:2210.01253.\nGuanhua Chen, Lu Hou, Yun Chen, Wenliang Dai,\nLifeng Shang, Xin Jiang, Qun Liu, Jia Pan, and Wen-\nping Wang. 2023a. mCLIP: Multilingual CLIP via\ncross-lingual transfer. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13028‚Äì\n13043, Toronto, Canada. Association for Computa-\ntional Linguistics.\nHong-You Chen, Jike Zhong, Mingda Zhang, Xuhui\nJia, Hang Qi, Boqing Gong, Wei-Lun Chao, and\nLi Zhang. 2023b. Federated learning of shareable\nbases for personalization-friendly image classifica-\ntion. Preprint, arXiv:2304.07882.\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nSammy Mohamed, and Andrea Vedaldi. 2014. De-\nscribing textures in the wild. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 3606‚Äì3613.\nWietse De Vries, Andreas Van Cranenburgh, and Malv-\nina Nissim. 2020. What‚Äôs so special about bert‚Äôs\n\nlayers? a closer look at the nlp pipeline in mono-\nlingual and multilingual models.\narXiv preprint\narXiv:2004.06499.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248‚Äì255. Ieee.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk Weissenborn,\nXiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale.\narXiv preprint\narXiv:2010.11929.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-\ning generative visual models from few training ex-\namples: An incremental bayesian approach tested on\n101 object categories. In 2004 conference on com-\nputer vision and pattern recognition workshop, pages\n178‚Äì178. IEEE.\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nRongyao Fang, Yongfeng Zhang, Hongsheng Li, and\nYu Qiao. 2023. Clip-adapter: Better vision-language\nmodels with feature adapters. International Journal\nof Computer Vision, pages 1‚Äì15.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nAngeliki Giannou, Shashank Rajput, and Dimitris\nPapailiopoulos. 2023.\nThe expressive power of\ntuning only the normalization layers.\nPreprint,\narXiv:2302.07937.\nPatrick Helber, Benjamin Bischke, Andreas Dengel,\nand Damian Borth. 2019. Eurosat: A novel dataset\nand deep learning benchmark for land use and land\ncover classification. IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing,\n12(7):2217‚Äì2226.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790‚Äì2799. PMLR.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International conference on ma-\nchine learning, pages 4904‚Äì4916. PMLR.\nMuhammad Uzair Khattak, Hanoona Rasheed, Muham-\nmad Maaz, Salman Khan, and Fahad Shahbaz Khan.\n2023. Maple: Multi-modal prompt learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 19113‚Äì19122.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 2013. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE inter-\nnational conference on computer vision workshops,\npages 554‚Äì561.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nLi Li, You Qin, Wei Ji, Yuxiao Zhou, and Roger Zim-\nmermann. 2024a. Domain-wise invariant learning for\npanoptic scene graph generation. In ICASSP 2024\n- 2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 3165‚Äì\n3169.\nLi Li, Chenwei Wang, You Qin, Wei Ji, and Renjie\nLiang. 2023a. Biased-predicate annotation identifica-\ntion via unbiased visual predicate representation. In\nProceedings of the 31st ACM International Confer-\nence on Multimedia, MM ‚Äô23, page 4410‚Äì4420.\nMing Li, Qingli Li, and Yan Wang. 2023b.\nClass\nbalanced adaptive pseudo labeling for federated\nsemi-supervised learning.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 16292‚Äì16301.\nShawn Li, Huixian Gong, Hao Dong, Tiankai Yang,\nZhengzhong Tu, and Yue Zhao. 2024b. Dpu: Dy-\nnamic prototype updating for multimodal out-of-\ndistribution detection. Preprint, arXiv:2411.08227.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2024. Visual instruction tuning. Advances in\nneural information processing systems, 36.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1‚Äì35.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61‚Äì68.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. 2013. Fine-grained\nvisual classification of aircraft.\narXiv preprint\narXiv:1306.5151.\n\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020.\nWhat happens to bert\nembeddings during fine-tuning?\narXiv preprint\narXiv:2004.14448.\nDavid Mizrahi, Roman Bachmann, OÀòguzhan Fatih Kar,\nTeresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir\nZamir. 2023. 4m: Massively multimodal masked\nmodeling. Preprint, arXiv:2312.06647.\nMarius Mosbach, Anna Khokhlova, Michael A Hed-\nderich, and Dietrich Klakow. 2020. On the inter-\nplay between fine-tuning and sentence-level probing\nfor linguistic knowledge in pre-trained transformers.\narXiv preprint arXiv:2010.02616.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomated flower classification over a large number\nof classes. In 2008 Sixth Indian conference on com-\nputer vision, graphics & image processing, pages\n722‚Äì729. IEEE.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand CV Jawahar. 2012. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern\nrecognition, pages 3498‚Äì3505. IEEE.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nJonas Pfeiffer, Ivan Vuli¬¥c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer.\narXiv\npreprint arXiv:2005.00052.\nWang Qi, Yu-Ping Ruan, Yuan Zuo, and Taihao Li.\n2022. Parameter-efficient tuning on layer normal-\nization for pre-trained language models. Preprint,\narXiv:2211.08682.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748‚Äì8763. PMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt,\nand Vaishaal Shankar. 2019. Do imagenet classifiers\ngeneralize to imagenet? In International conference\non machine learning, pages 5389‚Äì5400. PMLR.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. Advances in Neural Information Pro-\ncessing Systems, 35:25278‚Äì25294.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit\nBansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. 2021. How much can clip\nbenefit vision-and-language tasks? arXiv preprint\narXiv:2107.06383.\nHaoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and\nFuru Wei. 2022. Clip models are few-shot learners:\nEmpirical studies on vqa and visual entailment. arXiv\npreprint arXiv:2203.07190.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak\nShah. 2012. Ucf101: A dataset of 101 human ac-\ntions classes from videos in the wild. arXiv preprint\narXiv:1212.0402.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M. Dai, Anja Hauth, Katie\nMillican, David Silver, Melvin Johnson, Ioannis\nAntonoglou, Julian Schrittwieser, Amelia Glaese,\nJilin Chen, Emily Pitler, Timothy Lillicrap, Ange-\nliki Lazaridou, Orhan Firat, James Molloy, Michael\nIsard, Paul R. Barham, Tom Hennigan, Benjamin\nLee, Fabio Viola, Malcolm Reynolds, Yuanzhong\nXu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza\nRutherford, Erica Moreira, Kareem Ayoub, Megha\nGoel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-\nTze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty\nChan, Manaal Faruqui, Aliaksei Severyn, Hanzhao\nLin, YaGuang Li, Yong Cheng, Abe Ittycheriah,\nMahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran,\nSumit Bagri, Balaji Lakshminarayanan, Jeremiah\nLiu, Andras Orban, Fabian G√ºra, Hao Zhou, Xiny-\ning Song, Aurelien Boffy, Harish Ganapathy, Steven\nZheng, HyunJeong Choe, √Ågoston Weisz, Tao Zhu,\nYifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej\nKula, Jeff Pitman, Rushin Shah, Emanuel Taropa,\nMajd Al Merey, Martin Baeuml, Zhifeng Chen, Lau-\nrent El Shafey, Yujing Zhang, Olcan Sercinoglu,\nGeorge Tucker, Enrique Piqueras, Maxim Krikun,\nIain Barr, Nikolay Savinov, Ivo Danihelka, Becca\nRoelofs, Ana√Øs White, Anders Andreassen, Tamara\nvon Glehn, Lakshman Yagati, Mehran Kazemi, Lu-\ncas Gonzalez, Misha Khalman, Jakub Sygnowski,\nAlexandre Frechette, Charlotte Smith, Laura Culp,\nLev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan\nSchucher, Federico Lebron, Alban Rrustemi, Na-\ntalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao,\nBartek Perz, Dian Yu, Heidi Howard, Adam Blo-\nniarz, Jack W. Rae, Han Lu, Laurent Sifre, Mar-\ncello Maggioni, Fred Alcober, Dan Garrette, Megan\nBarnes, Shantanu Thakoor, Jacob Austin, Gabriel\nBarth-Maron, William Wong, Rishabh Joshi, Rahma\nChaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh\nTomar, Evan Senter, Martin Chadwick, Ilya Kor-\nnakov, Nithya Attaluri, I√±aki Iturrate, Ruibo Liu,\nYunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia,\nChenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse\nHartman, Xavier Garcia, Thanumalayan Sankara-\nnarayana Pillai, Jacob Devlin, Michael Laskin, Diego\nde Las Casas, Dasha Valter, Connie Tao, Lorenzo\nBlanco, Adri√† Puigdom√®nech Badia, David Reitter,\nMianna Chen, Jenny Brennan, Clara Rivera, Sergey\n\nBrin, Shariq Iqbal, Gabriela Surita, Jane Labanowski,\nAbhi Rao, Stephanie Winkler, Emilio Parisotto, Yim-\ning Gu, Kate Olszewska, Ravi Addanki, Antoine\nMiech, Annie Louis, Denis Teplyashin, Geoff Brown,\nElliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang,\nZoe Ashwood, Anton Briukhov, Albert Webson, San-\njay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-\nWei Chang, Axel Stjerngren, Josip Djolonga, Yut-\ning Sun, Ankur Bapna, Matthew Aitchison, Pedram\nPejman, Henryk Michalewski, Tianhe Yu, Cindy\nWang, Juliette Love, Junwhan Ahn, Dawn Bloxwich,\nKehang Han, Peter Humphreys, Thibault Sellam,\nJames Bradbury, Varun Godbole, Sina Samangooei,\nBogdan Damoc, Alex Kaskasoli, S√©bastien M. R.\nArnold, Vijay Vasudevan, Shubham Agrawal, Jason\nRiesa, Dmitry Lepikhin, Richard Tanburn, Srivat-\nsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson,\nPranav Shyam, Johan Ferret, Steven Hand, Ankush\nGarg, Tom Le Paine, Jian Li, Yujia Li, Minh Gi-\nang, Alexander Neitz, Zaheer Abbas, Sarah York,\nMachel Reid, Elizabeth Cole, Aakanksha Chowdh-\nery, Dipanjan Das, Dominika Rogozi¬¥nska, Vitaliy\nNikolaev, Pablo Sprechmann, Zachary Nado, Lukas\nZilka, Flavien Prost, Luheng He, Marianne Mon-\nteiro, Gaurav Mishra, Chris Welty, Josh Newlan,\nDawei Jia, Miltiadis Allamanis, Clara Huiyi Hu,\nRaoul de Liedekerke, Justin Gilmer, Carl Saroufim,\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava,\nAnirudh Baddepudi, Alex Goldin, Adnan Ozturel,\nAlbin Cassirer, Yunhan Xu, Daniel Sohn, Deven-\ndra Sachan, Reinald Kim Amplayo, Craig Swan-\nson, Dessie Petrova, Shashi Narayan, Arthur Guez,\nSiddhartha Brahma, Jessica Landon, Miteyan Pa-\ntel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wen-\nhao Jia, Matthew Rahtz, Mai Gim√©nez, Legg Yeung,\nJames Keeling, Petko Georgiev, Diana Mincu, Boxi\nWu, Salem Haykal, Rachel Saputro, Kiran Vodra-\nhalli, James Qin, Zeynep Cankara, Abhanshu Sharma,\nNick Fernando, Will Hawkins, Behnam Neyshabur,\nSolomon Kim, Adrian Hutter, Priyanka Agrawal,\nAlex Castro-Ros, George van den Driessche, Tao\nWang, Fan Yang, Shuo yiin Chang, Paul Komarek,\nRoss McIlroy, Mario LuÀáci¬¥c, Guodong Zhang, Wael\nFarhan, Michael Sharman, Paul Natsev, Paul Michel,\nYamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shak-\neri, Christina Butterfield, Justin Chung, Paul Kishan\nRubenstein, Shivani Agrawal, Arthur Mensch, Kedar\nSoparkar, Karel Lenc, Timothy Chung, Aedan Pope,\nLoren Maggiore, Jackie Kay, Priya Jhakra, Shibo\nWang, Joshua Maynez, Mary Phuong, Taylor Tobin,\nAndrea Tacchetti, Maja Trebacz, Kevin Robinson,\nYash Katariya, Sebastian Riedel, Paige Bailey, Kefan\nXiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone,\nNeil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gri-\nbovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music\nLi, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers,\nAnna Bortsova, Sanjay Ghemawat, Zafarali Ahmed,\nTianqi Liu, Richard Powell, Vijay Bolina, Mariko\nIinuma, Polina Zablotskaia, James Besley, Da-Woon\nChung, Timothy Dozat, Ramona Comanescu, Xi-\nance Si, Jeremy Greer, Guolong Su, Martin Polacek,\nRapha√´l Lopez Kaufman, Simon Tokumine, Hexiang\nHu, Elena Buchatskaya, Yingjie Miao, Mohamed\nElhawaty, Aditya Siddhant, Nenad Tomasev, Jin-\nwei Xing, Christina Greer, Helen Miller, Shereen\nAshraf, Aurko Roy, Zizhao Zhang, Ada Ma, Ange-\nlos Filos, Milos Besta, Rory Blevins, Ted Klimenko,\nChih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Os-\ncar Chang, Mantas Pajarskas, Carrie Muir, Vered\nCohen, Charline Le Lan, Krishna Haridasan, Amit\nMarathe, Steven Hansen, Sholto Douglas, Rajku-\nmar Samuel, Mingqiu Wang, Sophia Austin, Chang\nLan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo,\nLars Lowe Sj√∂sund, S√©bastien Cevey, Zach Gle-\nicher, Thi Avrahami, Anudhyan Boral, Hansa Srini-\nvasan, Vittorio Selo, Rhys May, Konstantinos Aiso-\npos, L√©onard Hussenot, Livio Baldini Soares, Kate\nBaumli, Michael B. Chang, Adri√† Recasens, Ben\nCaine, Alexander Pritzel, Filip Pavetic, Fabio Pardo,\nAnita Gergely, Justin Frye, Vinay Ramasesh, Dan\nHorgan, Kartikeya Badola, Nora Kassner, Subhra-\njit Roy, Ethan Dyer, V√≠ctor Campos Campos, Alex\nTomala, Yunhao Tang, Dalia El Badawy, Elspeth\nWhite, Basil Mustafa, Oran Lang, Abhishek Jin-\ndal, Sharad Vikram, Zhitao Gong, Sergi Caelles,\nRoss Hemsley, Gregory Thornton, Fangxiaoyu Feng,\nWojciech Stokowiec, Ce Zheng, Phoebe Thacker,\n√áaÀòglar √únl√º, Zhishuai Zhang, Mohammad Saleh,\nJames Svensson, Max Bileschi, Piyush Patil, Ankesh\nAnand, Roman Ring, Katerina Tsihlas, Arpi Vezer,\nMarco Selvi, Toby Shevlane, Mikel Rodriguez, Tom\nKwiatkowski, Samira Daruki, Keran Rong, Allan\nDafoe, Nicholas FitzGerald, Keren Gu-Lemberg,\nMina Khan, Lisa Anne Hendricks, Marie Pellat,\nVladimir Feinberg, James Cobon-Kerr, Tara Sainath,\nMaribeth Rauh, Sayed Hadi Hashemi, Richard Ives,\nYana Hasson, Eric Noland, Yuan Cao, Nathan Byrd,\nLe Hou, Qingze Wang, Thibault Sottiaux, Michela\nPaganini, Jean-Baptiste Lespiau, Alexandre Mou-\nfarek, Samer Hassan, Kaushik Shivakumar, Joost van\nAmersfoort, Amol Mandhane, Pratik Joshi, Anirudh\nGoyal, Matthew Tung, Andrew Brock, Hannah Shea-\nhan, Vedant Misra, Cheng Li, Nemanja Raki¬¥cevi¬¥c,\nMostafa Dehghani, Fangyu Liu, Sid Mittal, Jun-\nhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot,\nMatthew Lamm, Nicola De Cao, Charlie Chen, Sid-\nharth Mudgal, Romina Stella, Kevin Brooks, Gau-\ntam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita\nMelinkeri, Aaron Cohen, Venus Wang, Kristie Sey-\nmore, Sergey Zubkov, Rahul Goel, Summer Yue,\nSai Krishnakumaran, Brian Albert, Nate Hurley,\nMotoki Sano, Anhad Mohananey, Jonah Joughin,\nEgor Filonov, Tomasz KÀõepa, Yomna Eldawy, Jiaw-\nern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor\nBos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara\nPadmanabhan, Subha Puttagunta, Kalpesh Krishna,\nLeslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam\nKurzrok, Shuntong Lei, Anthony Yu, Oren Litvin,\nXiang Zhou, Zhichun Wu, Sam Sobell, Andrea Si-\nciliano, Alan Papir, Robby Neale, Jonas Bragagnolo,\nTej Toor, Tina Chen, Valentin Anklin, Feiran Wang,\nRichie Feng, Milad Gholami, Kevin Ling, Lijuan\nLiu, Jules Walter, Hamid Moghaddam, Arun Kishore,\nJakub Adamek, Tyler Mercado, Jonathan Mallinson,\nSiddhinita Wandekar, Stephen Cagle, Eran Ofek,\nGuillermo Garrido, Clemens Lombriser, Maksim\nMukha, Botu Sun, Hafeezul Rahman Mohammad,\nJosip Matak, Yadi Qian, Vikas Peswani, Pawel Janus,\n\nQuan Yuan, Leif Schelin, Oana David, Ankur Garg,\nYifan He, Oleksii Duzhyi, Anton √Ñlgmyr, Timo-\nth√©e Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex\nChinien, Rakesh Shivanna, Aleksandr Chuklin, Josie\nLi, Carrie Spadine, Travis Wolfe, Kareem Mohamed,\nSubhabrata Das, Zihang Dai, Kyle He, Daniel von\nDincklage, Shyam Upadhyay, Akanksha Maurya,\nLuyan Chi, Sebastian Krause, Khalid Salama, Pam G\nRabinovitch, Pavan Kumar Reddy M, Aarush Sel-\nvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Gu-\nven, Himanshu Gupta, Boyi Liu, Deepak Sharma,\nIdan Heimlich Shtacher, Shachi Paul, Oscar Aker-\nlund, Fran√ßois-Xavier Aubet, Terry Huang, Chen\nZhu, Eric Zhu, Elico Teixeira, Matthew Fritze,\nFrancesco Bertolini, Liana-Eleonora Marinescu, Mar-\ntin B√∂lle, Dominik Paulus, Khyatti Gupta, Tejasi\nLatkar, Max Chang, Jason Sanders, Roopa Wil-\nson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet,\nTulsee Doshi, Sid Lall, Swaroop Mishra, Wanming\nChen, Thang Luong, Seth Benjamin, Jasmine Lee,\nEwa Andrejczuk, Dominik Rabiej, Vipul Ranjan,\nKrzysztof Styrc, Pengcheng Yin, Jon Simon, Mal-\ncolm Rose Harriott, Mudit Bansal, Alexei Robsky,\nGeoff Bacon, David Greene, Daniil Mirylenka, Chen\nZhou, Obaid Sarvana, Abhimanyu Goyal, Samuel\nAndermatt, Patrick Siegler, Ben Horn, Assaf Is-\nrael, Francesco Pongetti, Chih-Wei \"Louis\" Chen,\nMarco Selvatici, Pedro Silva, Kathie Wang, Jack-\nson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai,\nAlessandro Agostini, Maulik Shah, Hung Nguyen,\nNoah √ì Donnaile, S√©bastien Pereira, Linda Friso,\nAdam Stambler, Adam Kurzrok, Chenkai Kuang,\nYan Romanikhin, Mark Geller, ZJ Yan, Kane Jang,\nCheng-Chun Lee, Wojciech Fica, Eric Malmi, Qi-\njun Tan, Dan Banica, Daniel Balle, Ryan Pham,\nYanping Huang, Diana Avram, Hongzhi Shi, Jasjot\nSingh, Chris Hidey, Niharika Ahuja, Pranab Sax-\nena, Dan Dooley, Srividya Pranavi Potharaju, Eileen\nO‚ÄôNeill, Anand Gokulchandran, Ryan Foley, Kai\nZhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta,\nRagha Kotikalapudi, Chalence Safranek-Shrader, An-\ndrew Goodman, Joshua Kessinger, Eran Globen, Pra-\nteek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang\nSong, Ali Eichenbaum, Thomas Brovelli, Sahitya\nPotluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani,\nCharles Chen, Andy Crawford, Shalini Pal, Mukund\nSridhar, Petru Gurita, Asier Mujika, Igor Petrovski,\nPierre-Louis Cedoz, Chenmei Li, Shiyuan Chen,\nNiccol√≤ Dal Santo, Siddharth Goyal, Jitesh Pun-\njabi, Karthik Kappaganthu, Chester Kwak, Pallavi\nLV, Sarmishta Velury, Himadri Choudhury, Jamie\nHall, Premal Shah, Ricardo Figueira, Matt Thomas,\nMinjie Lu, Ting Zhou, Chintu Kumar, Thomas Ju-\nrdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo\nKwak, Victor √Ñhdel, Sujeevan Rajayogam, Travis\nChoma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho\nPark, Vincent Hellendoorn, Alex Bailey, Taylan Bi-\nlal, Huanjie Zhou, Mehrdad Khatir, Charles Sut-\nton, Wojciech Rzadkowski, Fiona Macintosh, Kon-\nstantin Shagin, Paul Medina, Chen Liang, Jinjing\nZhou, Pararth Shah, Yingying Bi, Attila Dankovics,\nShipra Banga, Sabine Lehmann, Marissa Bredesen,\nZifan Lin, John Eric Hoffmann, Jonathan Lai, Ray-\nnald Chung, Kai Yang, Nihal Balani, Arthur Bra≈æin-\nskas, Andrei Sozanschi, Matthew Hayes, H√©ctor Fer-\nn√°ndez Alcalde, Peter Makarov, Will Chen, Anto-\nnio Stella, Liselotte Snijders, Michael Mandl, Ante\nK√§rrman, Pawe≈Ç Nowak, Xinyi Wu, Alex Dyck, Kr-\nishnan Vaidyanathan, Raghavender R, Jessica Mal-\nlet, Mitch Rudominer, Eric Johnston, Sushil Mit-\ntal, Akhil Udathu, Janara Christensen, Vishal Verma,\nZach Irving, Andreas Santucci, Gamaleldin Elsayed,\nElnaz Davoodi, Marin Georgiev, Ian Tenney, Nan\nHua, Geoffrey Cideron, Edouard Leurent, Mah-\nmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy\nZheng, Dylan Scandinaro, Heinrich Jiang, Jasper\nSnoek, Mukund Sundararajan, Xuezhi Wang, Zack\nOntiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar,\nLara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan\nUesato, Romina Datta, Oskar Bunyan, Shimu Wu,\nJohn Zhang, Piotr Stanczyk, Ye Zhang, David Steiner,\nSubhajit Naskar, Michael Azzam, Matthew Johnson,\nAdam Paszke, Chung-Cheng Chiu, Jaume Sanchez\nElias, Afroz Mohiuddin, Faizan Muhammad, Jin\nMiao, Andrew Lee, Nino Vieillard, Jane Park, Ji-\nageng Zhang, Jeff Stanway, Drew Garmon, Abhijit\nKarmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Lu-\nowei Zhou, Jonathan Evens, William Isaac, Geoffrey\nIrving, Edward Loper, Michael Fink, Isha Arkatkar,\nNanxin Chen, Izhak Shafran, Ivan Petrychenko,\nZhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai\nZhu, Peter Grabowski, Yu Mao, Alberto Magni,\nKaisheng Yao, Javier Snaider, Norman Casagrande,\nEvan Palmer, Paul Suganthan, Alfonso Casta√±o,\nIrene Giannoumis, Wooyeol Kim, Miko≈Çaj Rybi¬¥nski,\nAshwin Sreevatsa, Jennifer Prendki, David Soergel,\nAdrian Goedeckemeyer, Willi Gierke, Mohsen Jafari,\nMeenu Gaba, Jeremy Wiesner, Diana Gage Wright,\nYawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay\nHoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu,\nLu Liu, Kevin Ramirez, Andrey Khorlin, Albert\nCui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith\nPallo, Abhishek Chakladar, Ginger Perng, Elena Al-\nlica Abellan, Mingyang Zhang, Ishita Dasgupta,\nNate Kushman, Ivo Penchev, Alena Repina, Xihui\nWu, Tom van der Weide, Priya Ponnapalli, Car-\noline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier\nDousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pa-\nsumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel\nAndor, Pedro Valenzuela, Minnie Lui, Cosmin Padu-\nraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang,\nSomer Greene, Duc Dung Nguyen, Paula Kurylow-\nicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam\nChoo, Ziqiang Feng, Biao Zhang, Achintya Sing-\nhal, Dayou Du, Dan McKinnon, Natasha Antropova,\nTolga Bolukbasi, Orgad Keller, David Reid, Daniel\nFinchelstein, Maria Abi Raad, Remi Crocker, Pe-\nter Hawkins, Robert Dadashi, Colin Gaffney, Ken\nFranko, Anna Bulanova, R√©mi Leblond, Shirley\nChung, Harry Askham, Luis C. Cobo, Kelvin Xu,\nFelix Fischer, Jun Xu, Christina Sorokin, Chris Al-\nberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev,\nHannah Forbes, Dylan Banarse, Zora Tung, Mark\nOmernick, Colton Bishop, Rachel Sterneck, Rohan\nJain, Jiawei Xia, Ehsan Amid, Francesco Piccinno,\nXingyu Wang, Praseem Banzal, Daniel J. Mankowitz,\nAlex Polozov, Victoria Krakovna, Sasha Brown, Mo-\n\nhammadHossein Bateni, Dennis Duan, Vlad Firoiu,\nMeghana Thotakuri, Tom Natan, Matthieu Geist,\nSer tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko\nTojo, Michael Kwong, James Lee-Thorp, Christo-\npher Yew, Danila Sinopalnikov, Sabela Ramos, John\nMellor, Abhishek Sharma, Kathy Wu, David Miller,\nNicolas Sonnerat, Denis Vnukov, Rory Greig, Jen-\nnifer Beattie, Emily Caveness, Libin Bai, Julian\nEisenschlos, Alex Korchemniy, Tomy Tsai, Mimi\nJasarevic, Weize Kong, Phuong Dao, Zeyu Zheng,\nFrederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh,\nJason Sanmiya, Evgeny Gladchenko, Nejc Trdin,\nDaniel Toyama, Evan Rosen, Sasan Tavakkol, Lint-\ning Xue, Chen Elkind, Oliver Woodman, John Car-\npenter, George Papamakarios, Rupert Kemp, Sushant\nKafle, Tanya Grunina, Rishika Sinha, Alice Tal-\nbert, Diane Wu, Denese Owusu-Afriyie, Cosmo\nDu, Chloe Thornton, Jordi Pont-Tuset, Pradyumna\nNarayana, Jing Li, Saaber Fatehi, John Wieting,\nOmar Ajmeri, Benigno Uria, Yeongil Ko, Laura\nKnight, Am√©lie H√©liou, Ning Niu, Shane Gu, Chenxi\nPang, Yeqing Li, Nir Levine, Ariel Stolovich, Re-\nbeca Santamaria-Fernandez, Sonam Goenka, Wenny\nYustalim, Robin Strudel, Ali Elqursh, Charlie Deck,\nHyo Lee, Zonglin Li, Kyle Levin, Raphael Hoff-\nmann, Dan Holtmann-Rice, Olivier Bachem, Sho\nArora, Christy Koh, Soheil Hassas Yeganeh, Siim\nP√µder, Mukarram Tariq, Yanhua Sun, Lucian Ionita,\nMojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, An-\nmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz,\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown,\nShreya Singh, Wei Fan, Aaron Parisi, Joe Stan-\nton, Vinod Koverkathu, Christopher A. Choquette-\nChoo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash\nShroff, Mani Varadarajan, Sanaz Bahargam, Rob\nWilloughby, David Gaddy, Guillaume Desjardins,\nMarco Cornero, Brona Robenek, Bhavishya Mit-\ntal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev,\nHenrik Jacobsson, Alireza Ghaffarkhah, Morgane\nRivi√®re, Alanna Walton, Cl√©ment Crepy, Alicia Par-\nrish, Zongwei Zhou, Clement Farabet, Carey Rade-\nbaugh, Praveen Srinivasan, Claudia van der Salm,\nAndreas Fidjeland, Salvatore Scellato, Eri Latorre-\nChimoto, Hanna Klimczak-Pluci¬¥nska, David Bridson,\nDario de Cesare, Tom Hudson, Piermaria Mendolic-\nchio, Lexi Walker, Alex Morris, Matthew Mauger,\nAlexey Guseynov, Alison Reid, Seth Odoom, Lu-\ncia Loher, Victor Cotruta, Madhavi Yenugula, Do-\nminik Grewe, Anastasia Petrushkina, Tom Duerig,\nAntonio Sanchez, Steve Yadlowsky, Amy Shen,\nAmir Globerson, Lynette Webb, Sahil Dua, Dong\nLi, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi,\nAnanth Agarwal, Tomer Shani, Matan Eyal, Anuj\nKhare, Shreyas Rammohan Belle, Lei Wang, Chetan\nTekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin\nSang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao\nZhao, Stephan Lee, Pandu Nayak, Doug Fritz, Man-\nish Reddy Vuyyuru, John Aslanides, Nidhi Vyas,\nMartin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Mar-\ntin, Hardie Cate, James Manyika, Keyvan Amiri,\nYelin Kim, Xi Xiong, Kai Kang, Florian Luisier,\nNilesh Tripuraneni, David Madras, Mandy Guo,\nAustin Waters, Oliver Wang, Joshua Ainslie, Jason\nBaldridge, Han Zhang, Garima Pruthi, Jakob Bauer,\nFeng Yang, Riham Mansour, Jason Gelman, Yang Xu,\nGeorge Polovets, Ji Liu, Honglong Cai, Warren Chen,\nXiangHai Sheng, Emily Xue, Sherjil Ozair, Christof\nAngermueller, Xiaowei Li, Anoop Sinha, Weiren\nWang, Julia Wiesinger, Emmanouil Koukoumidis,\nYuan Tian, Anand Iyer, Madhu Gurumurthy, Mark\nGoldenson, Parashar Shah, MK Blake, Hongkun Yu,\nAnthony Urbanowicz, Jennimaria Palomaki, Chrisan-\ntha Fernando, Ken Durden, Harsh Mehta, Nikola\nMomchev, Elahe Rahimtoroghi, Maria Georgaki,\nAmit Raul, Sebastian Ruder, Morgan Redshaw, Jin-\nhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li,\nBlake Hechtman, Parker Schuh, Milad Nasr, Kieran\nMilan, Vladimir Mikulik, Juliana Franco, Tim Green,\nNam Nguyen, Joe Kelley, Aroma Mahendru, Andrea\nHu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshi-\ntij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang,\nKe Ye, Jean Michel Sarr, Melanie Moranski Preston,\nMadeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta,\nIce Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi\nM., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric\nChu, Xuanyi Dong, Amruta Muthal, Senaka Buth-\npitiya, Sarthak Jauhari, Nan Hua, Urvashi Khan-\ndelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Sha-\nhar Drath, Avigail Dabush, Nan-Jiang Jiang, Har-\nshal Godhia, Uli Sachs, Anthony Chen, Yicheng\nFan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai,\nJames Wang, Chen Liang, Jenny Hamer, Chun-Sung\nFerng, Chenel Elkind, Aviel Atias, Paulina Lee, V√≠t\nList√≠k, Mathias Carlen, Jan van de Kerkhof, Marcin\nPikus, Krunoslav Zaher, Paul M√ºller, Sasha Zykova,\nRichard Stefanec, Vitaly Gatsko, Christoph Hirn-\nschall, Ashwin Sethi, Xingyu Federico Xu, Chetan\nAhuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Ke-\nshav Dhandhania, Manish Katyal, Akshay Gupta,\nAtharva Parulekar, Divya Pitta, Jing Zhao, Vivaan\nBhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin\nLi, Peter Danenberg, Dennis Tu, Alex Pine, Vera\nFilippova, Abhipso Ghosh, Ben Limonchik, Bhar-\ngava Urala, Chaitanya Krishna Lanka, Derik Clive,\nYi Sun, Edward Li, Hao Wu, Kevin Hongtongsak,\nIanna Li, Kalind Thakkar, Kuanysh Omarov, Kushal\nMajmundar, Michael Alverson, Michael Kucharski,\nMohak Patel, Mudit Jain, Maksim Zabelin, Paolo\nPelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim,\nSwetha Sankar, Vineet Shah, Lakshmi Ramachan-\ndruni, Xiangkai Zeng, Ben Bariach, Laura Wei-\ndinger, Tu Vu, Alek Andreev, Antoine He, Kevin\nHui, Sheleem Kashem, Amar Subramanya, Sissie\nHsiao, Demis Hassabis, Koray Kavukcuoglu, Adam\nSadovsky, Quoc Le, Trevor Strohman, Yonghui Wu,\nSlav Petrov, Jeffrey Dean, and Oriol Vinyals. 2024a.\nGemini: A family of highly capable multimodal mod-\nels. Preprint, arXiv:2312.11805.\nGemini Team, Petko Georgiev, Ving Ian Lei, Ryan\nBurnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nDamien Vincent, Zhufeng Pan, Shibo Wang, Soroosh\nMariooryad, Yifan Ding, Xinyang Geng, Fred Al-\ncober, Roy Frostig, Mark Omernick, Lexi Walker,\nCosmin Paduraru, Christina Sorokin, Andrea Tac-\nchetti, Colin Gaffney, Samira Daruki, Olcan Ser-\ncinoglu, Zach Gleicher, Juliette Love, Paul Voigt-\nlaender, Rohan Jain, Gabriela Surita, Kareem Mo-\n\nhamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Korn-\nraphop Kawintiranon, Orhan Firat, Yiming Gu, Yu-\njing Zhang, Matthew Rahtz, Manaal Faruqui, Natalie\nClay, Justin Gilmer, JD Co-Reyes, Ivo Penchev, Rui\nZhu, Nobuyuki Morioka, Kevin Hui, Krishna Hari-\ndasan, Victor Campos, Mahdis Mahdieh, Mandy Guo,\nSamer Hassan, Kevin Kilgour, Arpi Vezer, Heng-\nTze Cheng, Raoul de Liedekerke, Siddharth Goyal,\nPaul Barham, DJ Strouse, Seb Noury, Jonas Adler,\nMukund Sundararajan, Sharad Vikram, Dmitry Lep-\nikhin, Michela Paganini, Xavier Garcia, Fan Yang,\nDasha Valter, Maja Trebacz, Kiran Vodrahalli, Chu-\nlayuth Asawaroengchai, Roman Ring, Norbert Kalb,\nLivio Baldini Soares, Siddhartha Brahma, David\nSteiner, Tianhe Yu, Fabian Mentzer, Antoine He,\nLucas Gonzalez, Bibo Xu, Raphael Lopez Kauf-\nman, Laurent El Shafey, Junhyuk Oh, Tom Hennigan,\nGeorge van den Driessche, Seth Odoom, Mario Lucic,\nBecca Roelofs, Sid Lall, Amit Marathe, Betty Chan,\nSantiago Ontanon, Luheng He, Denis Teplyashin,\nJonathan Lai, Phil Crone, Bogdan Damoc, Lewis\nHo, Sebastian Riedel, Karel Lenc, Chih-Kuan Yeh,\nAakanksha Chowdhery, Yang Xu, Mehran Kazemi,\nEhsan Amid, Anastasia Petrushkina, Kevin Swersky,\nAli Khodaei, Gowoon Chen, Chris Larkin, Mario\nPinto, Geng Yan, Adria Puigdomenech Badia, Piyush\nPatil, Steven Hansen, Dave Orr, Sebastien M. R.\nArnold, Jordan Grimstad, Andrew Dai, Sholto Dou-\nglas, Rishika Sinha, Vikas Yadav, Xi Chen, Elena Gri-\nbovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel,\nPaul Komarek, Sophia Austin, Sebastian Borgeaud,\nLinda Friso, Abhimanyu Goyal, Ben Caine, Kris\nCao, Da-Woon Chung, Matthew Lamm, Gabe Barth-\nMaron, Thais Kagohara, Kate Olszewska, Mia Chen,\nKaushik Shivakumar, Rishabh Agarwal, Harshal\nGodhia, Ravi Rajwar, Javier Snaider, Xerxes Doti-\nwalla, Yuan Liu, Aditya Barua, Victor Ungureanu,\nYuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth,\nJames Qin, Ivo Danihelka, Tulsee Doshi, Martin\nChadwick, Jilin Chen, Sanil Jain, Quoc Le, Ar-\njun Kar, Madhu Gurumurthy, Cheng Li, Ruoxin\nSang, Fangyu Liu, Lampros Lamprou, Rich Munoz,\nNathan Lintz, Harsh Mehta, Heidi Howard, Mal-\ncolm Reynolds, Lora Aroyo, Quan Wang, Lorenzo\nBlanco, Albin Cassirer, Jordan Griffith, Dipanjan\nDas, Stephan Lee, Jakub Sygnowski, Zach Fisher,\nJames Besley, Richard Powell, Zafarali Ahmed, Do-\nminik Paulus, David Reitter, Zalan Borsos, Rishabh\nJoshi, Aedan Pope, Steven Hand, Vittorio Selo, Vi-\nhan Jain, Nikhil Sethi, Megha Goel, Takaki Makino,\nRhys May, Zhen Yang, Johan Schalkwyk, Christina\nButterfield, Anja Hauth, Alex Goldin, Will Hawkins,\nEvan Senter, Sergey Brin, Oliver Woodman, Mar-\nvin Ritter, Eric Noland, Minh Giang, Vijay Bolina,\nLisa Lee, Tim Blyth, Ian Mackinnon, Machel Reid,\nObaid Sarvana, David Silver, Alexander Chen, Lily\nWang, Loren Maggiore, Oscar Chang, Nithya At-\ntaluri, Gregory Thornton, Chung-Cheng Chiu, Os-\nkar Bunyan, Nir Levine, Timothy Chung, Evgenii\nEltyshev, Xiance Si, Timothy Lillicrap, Demetra\nBrady, Vaibhav Aggarwal, Boxi Wu, Yuanzhong Xu,\nRoss McIlroy, Kartikeya Badola, Paramjit Sandhu,\nErica Moreira, Wojciech Stokowiec, Ross Hems-\nley, Dong Li, Alex Tudor, Pranav Shyam, Elahe\nRahimtoroghi, Salem Haykal, Pablo Sprechmann,\nXiang Zhou, Diana Mincu, Yujia Li, Ravi Addanki,\nKalpesh Krishna, Xiao Wu, Alexandre Frechette,\nMatan Eyal, Allan Dafoe, Dave Lacey, Jay Whang,\nThi Avrahami, Ye Zhang, Emanuel Taropa, Hanzhao\nLin, Daniel Toyama, Eliza Rutherford, Motoki Sano,\nHyunJeong Choe, Alex Tomala, Chalence Safranek-\nShrader, Nora Kassner, Mantas Pajarskas, Matt\nHarvey, Sean Sechrist, Meire Fortunato, Christina\nLyu, Gamaleldin Elsayed, Chenkai Kuang, James\nLottes, Eric Chu, Chao Jia, Chih-Wei Chen, Pe-\nter Humphreys, Kate Baumli, Connie Tao, Rajku-\nmar Samuel, Cicero Nogueira dos Santos, Anders\nAndreassen, Nemanja Raki¬¥cevi¬¥c, Dominik Grewe,\nAviral Kumar, Stephanie Winkler, Jonathan Caton,\nAndrew Brock, Sid Dalmia, Hannah Sheahan, Iain\nBarr, Yingjie Miao, Paul Natsev, Jacob Devlin, Fer-\nyal Behbahani, Flavien Prost, Yanhua Sun, Artiom\nMyaskovsky, Thanumalayan Sankaranarayana Pillai,\nDan Hurt, Angeliki Lazaridou, Xi Xiong, Ce Zheng,\nFabio Pardo, Xiaowei Li, Dan Horgan, Joe Stanton,\nMoran Ambar, Fei Xia, Alejandro Lince, Mingqiu\nWang, Basil Mustafa, Albert Webson, Hyo Lee, Ro-\nhan Anil, Martin Wicke, Timothy Dozat, Abhishek\nSinha, Enrique Piqueras, Elahe Dabir, Shyam Upad-\nhyay, Anudhyan Boral, Lisa Anne Hendricks, Corey\nFry, Josip Djolonga, Yi Su, Jake Walker, Jane La-\nbanowski, Ronny Huang, Vedant Misra, Jeremy\nChen, RJ Skerry-Ryan, Avi Singh, Shruti Rijh-\nwani, Dian Yu, Alex Castro-Ros, Beer Changpinyo,\nRomina Datta, Sumit Bagri, Arnar Mar Hrafnkels-\nson, Marcello Maggioni, Daniel Zheng, Yury Sul-\nsky, Shaobo Hou, Tom Le Paine, Antoine Yang,\nJason Riesa, Dominika Rogozinska, Dror Marcus,\nDalia El Badawy, Qiao Zhang, Luyu Wang, Helen\nMiller, Jeremy Greer, Lars Lowe Sjos, Azade Nova,\nHeiga Zen, Rahma Chaabouni, Mihaela Rosca, Jiepu\nJiang, Charlie Chen, Ruibo Liu, Tara Sainath, Maxim\nKrikun, Alex Polozov, Jean-Baptiste Lespiau, Josh\nNewlan, Zeyncep Cankara, Soo Kwak, Yunhan Xu,\nPhil Chen, Andy Coenen, Clemens Meyer, Katerina\nTsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, Chen-\njie Gu, Jin Miao, Christian Frank, Zeynep Cankara,\nSanjay Ganapathy, Ishita Dasgupta, Steph Hughes-\nFitt, Heng Chen, David Reid, Keran Rong, Hongmin\nFan, Joost van Amersfoort, Vincent Zhuang, Aaron\nCohen, Shixiang Shane Gu, Anhad Mohananey,\nAnastasija Ilic, Taylor Tobin, John Wieting, Anna\nBortsova, Phoebe Thacker, Emma Wang, Emily\nCaveness, Justin Chiu, Eren Sezener, Alex Kaskasoli,\nSteven Baker, Katie Millican, Mohamed Elhawaty,\nKostas Aisopos, Carl Lebsack, Nathan Byrd, Hanjun\nDai, Wenhao Jia, Matthew Wiethoff, Elnaz Davoodi,\nAlbert Weston, Lakshman Yagati, Arun Ahuja, Isabel\nGao, Golan Pundak, Susan Zhang, Michael Azzam,\nKhe Chai Sim, Sergi Caelles, James Keeling, Ab-\nhanshu Sharma, Andy Swing, YaGuang Li, Chenxi\nLiu, Carrie Grimes Bostock, Yamini Bansal, Zachary\nNado, Ankesh Anand, Josh Lipschultz, Abhijit Kar-\nmarkar, Lev Proleev, Abe Ittycheriah, Soheil Has-\nsas Yeganeh, George Polovets, Aleksandra Faust,\nJiao Sun, Alban Rrustemi, Pen Li, Rakesh Shivanna,\nJeremiah Liu, Chris Welty, Federico Lebron, Anirudh\nBaddepudi, Sebastian Krause, Emilio Parisotto, Radu\n\nSoricut, Zheng Xu, Dawn Bloxwich, Melvin John-\nson, Behnam Neyshabur, Justin Mao-Jones, Ren-\nshen Wang, Vinay Ramasesh, Zaheer Abbas, Arthur\nGuez, Constant Segal, Duc Dung Nguyen, James\nSvensson, Le Hou, Sarah York, Kieran Milan, So-\nphie Bridgers, Wiktor Gworek, Marco Tagliasacchi,\nJames Lee-Thorp, Michael Chang, Alexey Guseynov,\nAle Jakse Hartman, Michael Kwong, Ruizhe Zhao,\nSheleem Kashem, Elizabeth Cole, Antoine Miech,\nRichard Tanburn, Mary Phuong, Filip Pavetic, Se-\nbastien Cevey, Ramona Comanescu, Richard Ives,\nSherry Yang, Cosmo Du, Bo Li, Zizhao Zhang,\nMariko Iinuma, Clara Huiyi Hu, Aurko Roy, Shaan\nBijwadia, Zhenkai Zhu, Danilo Martins, Rachel\nSaputro, Anita Gergely, Steven Zheng, Dawei Jia,\nIoannis Antonoglou, Adam Sadovsky, Shane Gu,\nYingying Bi, Alek Andreev, Sina Samangooei, Mina\nKhan, Tomas Kocisky, Angelos Filos, Chintu Ku-\nmar, Colton Bishop, Adams Yu, Sarah Hodkin-\nson, Sid Mittal, Premal Shah, Alexandre Moufarek,\nYong Cheng, Adam Bloniarz, Jaehoon Lee, Pedram\nPejman, Paul Michel, Stephen Spencer, Vladimir\nFeinberg, Xuehan Xiong, Nikolay Savinov, Char-\nlotte Smith, Siamak Shakeri, Dustin Tran, Mary\nChesus, Bernd Bohnet, George Tucker, Tamara von\nGlehn, Carrie Muir, Yiran Mao, Hideto Kazawa,\nAmbrose Slone, Kedar Soparkar, Disha Shrivastava,\nJames Cobon-Kerr, Michael Sharman, Jay Pavagadhi,\nCarlos Araya, Karolis Misiunas, Nimesh Ghelani,\nMichael Laskin, David Barker, Qiujia Li, Anton\nBriukhov, Neil Houlsby, Mia Glaese, Balaji Laksh-\nminarayanan, Nathan Schucher, Yunhao Tang, Eli\nCollins, Hyeontaek Lim, Fangxiaoyu Feng, Adria\nRecasens, Guangda Lai, Alberto Magni, Nicola De\nCao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay,\nMostafa Dehghani, Jenny Brennan, Yifan He, Kelvin\nXu, Yang Gao, Carl Saroufim, James Molloy, Xinyi\nWu, Seb Arnold, Solomon Chang, Julian Schrit-\ntwieser, Elena Buchatskaya, Soroush Radpour, Mar-\ntin Polacek, Skye Giordano, Ankur Bapna, Simon\nTokumine, Vincent Hellendoorn, Thibault Sottiaux,\nSarah Cogan, Aliaksei Severyn, Mohammad Saleh,\nShantanu Thakoor, Laurent Shefey, Siyuan Qiao,\nMeenu Gaba, Shuo yiin Chang, Craig Swanson, Biao\nZhang, Benjamin Lee, Paul Kishan Rubenstein, Gan\nSong, Tom Kwiatkowski, Anna Koop, Ajay Kan-\nnan, David Kao, Parker Schuh, Axel Stjerngren, Gol-\nnaz Ghiasi, Gena Gibson, Luke Vilnis, Ye Yuan, Fe-\nlipe Tiengo Ferreira, Aishwarya Kamath, Ted Kli-\nmenko, Ken Franko, Kefan Xiao, Indro Bhattacharya,\nMiteyan Patel, Rui Wang, Alex Morris, Robin\nStrudel, Vivek Sharma, Peter Choy, Sayed Hadi\nHashemi, Jessica Landon, Mara Finkelstein, Priya\nJhakra, Justin Frye, Megan Barnes, Matthew Mauger,\nDennis Daun, Khuslen Baatarsukh, Matthew Tung,\nWael Farhan, Henryk Michalewski, Fabio Viola, Fe-\nlix de Chaumont Quitry, Charline Le Lan, Tom Hud-\nson, Qingze Wang, Felix Fischer, Ivy Zheng, Elspeth\nWhite, Anca Dragan, Jean baptiste Alayrac, Eric Ni,\nAlexander Pritzel, Adam Iwanicki, Michael Isard,\nAnna Bulanova, Lukas Zilka, Ethan Dyer, Deven-\ndra Sachan, Srivatsan Srinivasan, Hannah Mucken-\nhirn, Honglong Cai, Amol Mandhane, Mukarram\nTariq, Jack W. Rae, Gary Wang, Kareem Ayoub,\nNicholas FitzGerald, Yao Zhao, Woohyun Han, Chris\nAlberti, Dan Garrette, Kashyap Krishnakumar, Mai\nGimenez, Anselm Levskaya, Daniel Sohn, Josip\nMatak, Inaki Iturrate, Michael B. Chang, Jackie Xi-\nang, Yuan Cao, Nishant Ranka, Geoff Brown, Adrian\nHutter, Vahab Mirrokni, Nanxin Chen, Kaisheng\nYao, Zoltan Egyed, Francois Galilee, Tyler Liechty,\nPraveen Kallakuri, Evan Palmer, Sanjay Ghemawat,\nJasmine Liu, David Tao, Chloe Thornton, Tim Green,\nMimi Jasarevic, Sharon Lin, Victor Cotruta, Yi-Xuan\nTan, Noah Fiedel, Hongkun Yu, Ed Chi, Alexan-\nder Neitz, Jens Heitkaemper, Anu Sinha, Denny\nZhou, Yi Sun, Charbel Kaed, Brice Hulse, Swa-\nroop Mishra, Maria Georgaki, Sneha Kudugunta,\nClement Farabet, Izhak Shafran, Daniel Vlasic, An-\nton Tsitsulin, Rajagopal Ananthanarayanan, Alen\nCarin, Guolong Su, Pei Sun, Shashank V, Gabriel\nCarvajal, Josef Broder, Iulia Comsa, Alena Repina,\nWilliam Wong, Warren Weilun Chen, Peter Hawkins,\nEgor Filonov, Lucia Loher, Christoph Hirnschall,\nWeiyi Wang, Jingchen Ye, Andrea Burns, Hardie\nCate, Diana Gage Wright, Federico Piccinini, Lei\nZhang, Chu-Cheng Lin, Ionel Gog, Yana Kulizh-\nskaya, Ashwin Sreevatsa, Shuang Song, Luis C.\nCobo, Anand Iyer, Chetan Tekur, Guillermo Gar-\nrido, Zhuyun Xiao, Rupert Kemp, Huaixiu Steven\nZheng, Hui Li, Ananth Agarwal, Christel Ngani,\nKati Goshvadi, Rebeca Santamaria-Fernandez, Woj-\nciech Fica, Xinyun Chen, Chris Gorgolewski, Sean\nSun, Roopal Garg, Xinyu Ye, S. M. Ali Eslami,\nNan Hua, Jon Simon, Pratik Joshi, Yelin Kim, Ian\nTenney, Sahitya Potluri, Lam Nguyen Thiet, Quan\nYuan, Florian Luisier, Alexandra Chronopoulou, Sal-\nvatore Scellato, Praveen Srinivasan, Minmin Chen,\nVinod Koverkathu, Valentin Dalibard, Yaming Xu,\nBrennan Saeta, Keith Anderson, Thibault Sellam,\nNick Fernando, Fantine Huot, Junehyuk Jung, Mani\nVaradarajan, Michael Quinn, Amit Raul, Maigo Le,\nRuslan Habalov, Jon Clark, Komal Jalan, Kalesha\nBullard, Achintya Singhal, Thang Luong, Boyu\nWang, Sujeevan Rajayogam, Julian Eisenschlos,\nJohnson Jia, Daniel Finchelstein, Alex Yakubovich,\nDaniel Balle, Michael Fink, Sameer Agarwal, Jing\nLi, Dj Dvijotham, Shalini Pal, Kai Kang, Jaclyn\nKonzelmann, Jennifer Beattie, Olivier Dousse, Diane\nWu, Remi Crocker, Chen Elkind, Siddhartha Reddy\nJonnalagadda, Jong Lee, Dan Holtmann-Rice, Krys-\ntal Kallarackal, Rosanne Liu, Denis Vnukov, Neera\nVats, Luca Invernizzi, Mohsen Jafari, Huanjie Zhou,\nLilly Taylor, Jennifer Prendki, Marcus Wu, Tom\nEccles, Tianqi Liu, Kavya Kopparapu, Francoise\nBeaufays, Christof Angermueller, Andreea Marzoca,\nShourya Sarcar, Hilal Dib, Jeff Stanway, Frank Per-\nbet, Nejc Trdin, Rachel Sterneck, Andrey Khor-\nlin, Dinghua Li, Xihui Wu, Sonam Goenka, David\nMadras, Sasha Goldshtein, Willi Gierke, Tong Zhou,\nYaxin Liu, Yannie Liang, Anais White, Yunjie Li,\nShreya Singh, Sanaz Bahargam, Mark Epstein, Su-\njoy Basu, Li Lao, Adnan Ozturel, Carl Crous, Alex\nZhai, Han Lu, Zora Tung, Neeraj Gaur, Alanna\nWalton, Lucas Dixon, Ming Zhang, Amir Glober-\nson, Grant Uy, Andrew Bolt, Olivia Wiles, Milad\nNasr, Ilia Shumailov, Marco Selvi, Francesco Pic-\ncinno, Ricardo Aguilar, Sara McCarthy, Misha Khal-\n\nman, Mrinal Shukla, Vlado Galic, John Carpen-\nter, Kevin Villela, Haibin Zhang, Harry Richard-\nson, James Martens, Matko Bosnjak, Shreyas Ram-\nmohan Belle, Jeff Seibert, Mahmoud Alnahlawi,\nBrian McWilliams, Sankalp Singh, Annie Louis,\nWen Ding, Dan Popovici, Lenin Simicich, Laura\nKnight, Pulkit Mehta, Nishesh Gupta, Chongyang\nShi, Saaber Fatehi, Jovana Mitrovic, Alex Grills,\nJoseph Pagadora, Dessie Petrova, Danielle Eisenbud,\nZhishuai Zhang, Damion Yates, Bhavishya Mittal,\nNilesh Tripuraneni, Yannis Assael, Thomas Brovelli,\nPrateek Jain, Mihajlo Velimirovic, Canfer Akbulut,\nJiaqi Mu, Wolfgang Macherey, Ravin Kumar, Jun\nXu, Haroon Qureshi, Gheorghe Comanici, Jeremy\nWiesner, Zhitao Gong, Anton Ruddock, Matthias\nBauer, Nick Felt, Anirudh GP, Anurag Arnab, Dustin\nZelle, Jonas Rothfuss, Bill Rosgen, Ashish Shenoy,\nBryan Seybold, Xinjian Li, Jayaram Mudigonda,\nGoker Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi,\nYi Yao, Christopher Yew, Steven Kan, Isaac Caswell,\nCarey Radebaugh, Andre Elisseeff, Pedro Valen-\nzuela, Kay McKinney, Kim Paterson, Albert Cui, Eri\nLatorre-Chimoto, Solomon Kim, William Zeng, Ken\nDurden, Priya Ponnapalli, Tiberiu Sosea, Christo-\npher A. Choquette-Choo, James Manyika, Brona\nRobenek, Harsha Vashisht, Sebastien Pereira, Hoi\nLam, Marko Velic, Denese Owusu-Afriyie, Kather-\nine Lee, Tolga Bolukbasi, Alicia Parrish, Shawn Lu,\nJane Park, Balaji Venkatraman, Alice Talbert, Lam-\nbert Rosique, Yuchung Cheng, Andrei Sozanschi,\nAdam Paszke, Praveen Kumar, Jessica Austin, Lu Li,\nKhalid Salama, Wooyeol Kim, Nandita Dukkipati,\nAnthony Baryshnikov, Christos Kaplanis, Xiang-\nHai Sheng, Yuri Chervonyi, Caglar Unlu, Diego\nde Las Casas, Harry Askham, Kathryn Tunyasuvu-\nnakool, Felix Gimeno, Siim Poder, Chester Kwak,\nMatt Miecnikowski, Vahab Mirrokni, Alek Dimitriev,\nAaron Parisi, Dangyi Liu, Tomy Tsai, Toby Shevlane,\nChristina Kouridi, Drew Garmon, Adrian Goedeck-\nemeyer, Adam R. Brown, Anitha Vijayakumar, Ali\nElqursh, Sadegh Jazayeri, Jin Huang, Sara Mc Carthy,\nJay Hoover, Lucy Kim, Sandeep Kumar, Wei Chen,\nCourtney Biles, Garrett Bingham, Evan Rosen, Lisa\nWang, Qijun Tan, David Engel, Francesco Pongetti,\nDario de Cesare, Dongseong Hwang, Lily Yu, Jen-\nnifer Pullman, Srini Narayanan, Kyle Levin, Sid-\ndharth Gopal, Megan Li, Asaf Aharoni, Trieu Trinh,\nJessica Lo, Norman Casagrande, Roopali Vij, Loic\nMatthey, Bramandia Ramadhana, Austin Matthews,\nCJ Carey, Matthew Johnson, Kremena Goranova, Ro-\nhin Shah, Shereen Ashraf, Kingshuk Dasgupta, Ras-\nmus Larsen, Yicheng Wang, Manish Reddy Vuyyuru,\nChong Jiang, Joana Ijazi, Kazuki Osawa, Celine\nSmith, Ramya Sree Boppana, Taylan Bilal, Yuma\nKoizumi, Ying Xu, Yasemin Altun, Nir Shabat,\nBen Bariach, Alex Korchemniy, Kiam Choo, Olaf\nRonneberger, Chimezie Iwuanyanwu, Shubin Zhao,\nDavid Soergel, Cho-Jui Hsieh, Irene Cai, Shariq\nIqbal, Martin Sundermeyer, Zhe Chen, Elie Bursztein,\nChaitanya Malaviya, Fadi Biadsy, Prakash Shroff, In-\nderjit Dhillon, Tejasi Latkar, Chris Dyer, Hannah\nForbes, Massimo Nicosia, Vitaly Nikolaev, Somer\nGreene, Marin Georgiev, Pidong Wang, Nina Mar-\ntin, Hanie Sedghi, John Zhang, Praseem Banzal,\nDoug Fritz, Vikram Rao, Xuezhi Wang, Jiageng\nZhang, Viorica Patraucean, Dayou Du, Igor Mor-\ndatch, Ivan Jurin, Lewis Liu, Ayush Dubey, Abhi\nMohan, Janek Nowakowski, Vlad-Doru Ion, Nan\nWei, Reiko Tojo, Maria Abi Raad, Drew A. Hud-\nson, Vaishakh Keshava, Shubham Agrawal, Kevin\nRamirez, Zhichun Wu, Hoang Nguyen, Ji Liu, Mad-\nhavi Sewak, Bryce Petrini, DongHyun Choi, Ivan\nPhilips, Ziyue Wang, Ioana Bica, Ankush Garg,\nJarek Wilkiewicz, Priyanka Agrawal, Xiaowei Li,\nDanhao Guo, Emily Xue, Naseer Shaik, Andrew\nLeach, Sadh MNM Khan, Julia Wiesinger, Sammy\nJerome, Abhishek Chakladar, Alek Wenjiao Wang,\nTina Ornduff, Folake Abu, Alireza Ghaffarkhah, Mar-\ncus Wainwright, Mario Cortes, Frederick Liu, Joshua\nMaynez, Andreas Terzis, Pouya Samangouei, Ri-\nham Mansour, Tomasz KÀõepa, Fran√ßois-Xavier Aubet,\nAnton Algymr, Dan Banica, Agoston Weisz, An-\ndras Orban, Alexandre Senges, Ewa Andrejczuk,\nMark Geller, Niccolo Dal Santo, Valentin Anklin,\nMajd Al Merey, Martin Baeuml, Trevor Strohman,\nJunwen Bai, Slav Petrov, Yonghui Wu, Demis Has-\nsabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol\nVinyals. 2024b. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context.\nPreprint, arXiv:2403.05530.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint,\narXiv:2302.13971.\nCheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike\nZhong, Vardaan Pahuja, Tanya Berger-Wolf, Song\nGao, Charles Stewart, Yu Su, and Wei-Lun Chao.\n2023.\nHolistic transfer: Towards non-disruptive\nfine-tuning with partial target data.\nPreprint,\narXiv:2311.01420.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. 2019. Learning robust global representations\nby penalizing local predictive power. Advances in\nNeural Information Processing Systems, 32.\nZifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Ji-\nmeng Sun. 2022a. Medclip: Contrastive learning\nfrom unpaired medical images and text. In 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022.\nZifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi\nSun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong\nSu, Vincent Perot, Jennifer Dy, et al. 2022b. Dual-\nprompt: Complementary prompting for rehearsal-\nfree continual learning. In European Conference on\nComputer Vision, pages 631‚Äì648. Springer.\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\nRuoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,\n\nJennifer Dy, and Tomas Pfister. 2022c. Learning to\nprompt for continual learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 139‚Äì149.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. 2010. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\n2010 IEEE computer society conference on computer\nvision and pattern recognition, pages 3485‚Äì3492.\nIEEE.\nHu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettle-\nmoyer, and Christoph Feichtenhofer. 2021. Video-\nclip: Contrastive pre-training for zero-shot video-text\nunderstanding. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 6787‚Äì6800.\nLI Xuhong, Yves Grandvalet, and Franck Davoine.\n2018. Explicit inductive bias for transfer learning\nwith convolutional networks. In International Con-\nference on Machine Learning, pages 2825‚Äì2834.\nPMLR.\nHantao Yao, Rui Zhang, and Changsheng Xu. 2023.\nVisual-language prompt tuning with knowledge-\nguided context optimization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6757‚Äì6767.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021.\nBitfit:\nSimple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nCheng Zhang, Tai-Yu Pan, Tianle Chen, Jike Zhong,\nWenjin Fu, and Wei-Lun Chao. 2022. Learning with\nfree object segments for long-tailed instance segmen-\ntation. Preprint, arXiv:2202.11124.\nRenrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng\nLi. 2021. Tip-adapter: Training-free clip-adapter\nfor better vision-language modeling. arXiv preprint\narXiv:2111.03930.\nRenrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang,\nHanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng\nLi. 2023. Prompt, generate, then cache: Cascade\nof foundation models makes strong few-shot learn-\ners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n15211‚Äì15222.\nJike Zhong, Hong-You Chen, and Wei-Lun Chao. 2024.\nMaking batch normalization great in federated deep\nlearning. Preprint, arXiv:2303.06530.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022a.\nConditional prompt learning\nfor vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16816‚Äì16825.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022b. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision, 130(9):2337‚Äì2348.\nYichu Zhou and Vivek Srikumar. 2022. A closer look\nat how fine-tuning changes bert.\nIn Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1046‚Äì1061.\nBeier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Han-\nwang Zhang. 2023.\nPrompt-aligned gradient for\nprompt tuning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n15659‚Äì15669.\n\nA\nDataset Statistics\nFollowing CoOp (Zhou et al., 2022b), we con-\nducted extensive experiments on 11 public clas-\nsification benchmark datasets to evaluate the ef-\nfectiveness of the proposed CLIPFit. The datasets\nare ImageNet (Deng et al., 2009), Caltech101 (Fei-\nFei et al., 2004), OxfordPets (Parkhi et al., 2012),\nStanfordCars (Krause et al., 2013), Flowers102\n(Nilsback and Zisserman, 2008), Food101 (Bossard\net al., 2014), FGVCAircraft (Maji et al., 2013),\nSUN397 (Xiao et al., 2010), DTD (Cimpoi et al.,\n2014), EuroSAT (Helber et al., 2019), and UCF101\n(Soomro et al., 2012). In distribution shift exper-\niments, we also used ImageNet-V2 (Recht et al.,\n2019), and ImageNet-Sketch (Wang et al., 2019) as\nthe target dataset. The statistics of these datasets\ncan be found in Table 8.\nB\nImplementation Details\nWe implemented our method with PyTorch (Paszke\net al., 2019). All experiments were based on the\nvision backbone with Vit-B/16 (Dosovitskiy et al.,\n2020) of CLIP (Radford et al., 2021). We followed\nCoOp (Zhou et al., 2022b) to preprocess input im-\nages: we resized all images to 224√ó224 and used\nrandom cropping, resizing, and random horizontal\nflipping for data augmentation. Following Radford\net al. (2021), we used a single hand-craft prompt\nas text input for all methods except prompt tuning\nmethods for a fair comparison. The prompt for\neach dataset can be found in Table 8. We used\nSGD optimizer with batch size set as 32, and set\nthe learning rate as 0.002 (Zhou et al., 2022b). All\nresults reported below are the average of three runs\nwith different random seeds. The training epoch\nwas set to 100 for all datasets except ImageNet\nand Food101. The training epoch for ImageNet\nand Food101 datasets was set to 10. Smoothing\nparameter Œ± was set to 0.99 for all experiments. Œ≤\nwas set to 8 for all datasets on the base-to-new and\ncross-dataset setting, and 2 for the distribution shift\nsetting. For the few-shot setting, we set Œ≤ to 2 for\nall datasets except SUN397 and DTD. Œ≤ was set to\n8 for SUN397 and DTD datasets. All experiments\nwere run on one single NVIDIA A100 GPU.\nC\nDetailed Introduction to Baseline\nMethods.\nWe compared our method against state-of-the-art\nmethods: zero-shot CLIP (Radford et al., 2021),\nprompt tuning methods: CoOp (Zhou et al., 2022b),\nùõΩ\nFigure 6: Performance changes of harmonic mean on\n16 shot base-to-new setting by varying hyperparameter\nŒ≤.\n.\nCoCoOp (Zhou et al., 2022a), ProGrad (Zhu et al.,\n2023), KgCoOp (Yao et al., 2023) , MaPLe (Khat-\ntak et al., 2023) and adapter tuning methods: CLIP-\nddapter (Gao et al., 2023), Tip-adapter (Zhang\net al., 2021).\nZero-shot CLIP (Radford et al., 2021) uses the\nhand-crafted template ‚Äúa photo of a []‚Äù to gener-\nate the prompts and then applies these prompts to\npredict the class of given images.\nCoOp (Zhou et al., 2022b) introduces learnable\ntext prompts instead of hand-crafted prompts to\nadapt the CLIP model to downstream image recog-\nnition tasks.\nCoCoOp (Zhou et al., 2022a) proposes to gener-\nate an input-conditional token for each image with\na lightweight learnable neural network.\nKgCoOp (Yao et al., 2023) proposes to use con-\ntext knowledge distillation to learn from the origi-\nnal CLIP model to avoid overfitting and forgetting.\nMaPLe (Khattak et al., 2023) proposes a multi-\nmodal prompt learning strategy to introduce learn-\nable text and image prompts.\nCLIP-Adapter (Gao et al., 2023) sets an addi-\ntional bottleneck layer following the text or image\nencoder to learn better features by a residual way.\nTip-Adapter(Zhang et al., 2021) does not need\ntraining but creates the weights by a key-value\ncache model constructed from the few-shot training\nset and then uses this cache model for inference.\n\nTable 8: Statistics and prompts for each Dataset.\nDataset\nClasses\nTrain\nVal\nTest\nHand-crafted prompt\nImageNet\n1,000\n1.28M\nN/A\n50,000\n‚Äúa photo of a [CLASS].‚Äù\nCaltech101\n100\n4,128\n1,649\n2,465\n‚Äúa photo of a [CLASS].‚Äù\nOxfordPets\n37\n2,944\n736\n3,669\n‚Äúa photo of a [CLASS], a type of pet.‚Äù\nStanfordCars\n196\n6,509\n1,635\n8,041\n‚Äúa photo of a [CLASS].‚Äù\nFlowers\n102\n4,093\n1,633\n2,463\n‚Äúa photo of a [CLASS], a type of flower.‚Äù\nFood101\n101\n50,500\n20,200\n30,300\n‚Äúa photo of [CLASS], a type of food.‚Äù\nFGVCAircraft\n100\n3,334\n3,333\n3,333\n‚Äúa photo of a [CLASS], a type of aircraft.‚Äù\nSUN397\n397\n15,880\n3,970\n19,850\n‚Äúa photo of a [CLASS].‚Äù\nDTD\n47\n2,820\n1,128\n1,692\n‚Äú[CLASS] texture.‚Äù\nEuroSAT\n10\n13,500\n5,400\n8,100\n‚Äúa centered satellite photo of [CLASS].‚Äù\nUCF101\n101\n7,639\n1,898\n3,783\n‚Äúa photo of a person doing [CLASS].‚Äù\nImageNetV2\n1,000\nN/A\nN/A\n10,000\n‚Äúa photo of a [CLASS].‚Äù\nImageNet-Sketch\n1,000\nN/A\nN/A\n50,889\n‚Äúa photo of a [CLASS].‚Äù\nTable 9: Comparison of CLIPFit and other methods in the cross-dataset transfer setting. S.C.: StanfordCars dataset.\nF.A.: FGVCAircraft dataset.\nSource\nTarget\nImageNet\nCaltech101\nOxfordPets\nS.C.\nFlowers102\nFood101\nF.A.\nSUN397\nDTD\nEuroSAT\nUCF101\nAverage\nCoOp\n71.51\n93.70\n89.14\n64.51\n68.71\n85.30\n18.47\n64.15\n41.92\n46.39\n66.55\n63.88\nCoCoOp\n71.02\n94.43\n90.14\n65.32\n71.88\n86.06\n22.94\n67.36\n45.73\n45.37\n68.21\n65.74\nProGrad\n72.24\n91.52\n89.64\n62.39\n67.87\n85.40\n20.61\n62.47\n39.42\n43.46\n64.29\n62.71\nKgCoOp\n70.66\n93.92\n89.83\n65.41\n70.01\n86.36\n22.51\n66.16\n46.35\n46.04\n68.50\n65.51\nCLIPFit\n71.10\n93.77\n90.36\n64.56\n71.43\n85.76\n24.46\n67.43\n45.20\n46.40\n69.17\n65.85\nD\nResults on cross-dataset transfer\nsetting\nFollowing Zhou et al. (2022b,a), We also evaluated\nthe cross-dataset generalization ability of CLIP-\nFit and other methods. Models were trained on\nthe 16-shot Imagenet dataset and then tested on\nother datasets. The results are shown in Table 9.\nAs shown in Table 9, the average performance of\nCLIPFit is also better than existing methods, which\nshows that CLIPFit has a good generalization abil-\nity.\nE\nParameter Analysis\nIn this section, we aim to discuss hyper-parameters\nŒ≤. Œ± is the coefficient parameter to control the\nweight of knowledge distillation loss. Experiments\nwere conducted on the 16-shot base-to-new setting,\nand we report harmonic mean accuracy in Fig. 6.\nAs shown in Fig. 6, performances are not sensitive\nwithin certain ranges.\nF\nMore Few-shot Learning Results\nFollowing Zhou et al. (2022a), we used 1, 2, 4, 8,\nand 16-shot sets for training and reported accuracy\nperformance to test whether our proposed method\ncan learn task-specific knowledge. The results are\nreported in Table 10. As shown in Table 10, CLIP-\nFit can bring a consistent improvement in terms of\naverage accuracy on all settings.\nG\nMore Results of base-to-new setting\nIn this section, we give more detailed results on the\nbase-to-new setting. The detailed results for each\ndataset on 4-shot and 8-shot settings are shown\nin Table 11 and Table 12. Since MaPLe (Khattak\net al., 2023) did not conduct experiments on 4/8-\nshot setting, we do not report results from MaPLe.\nAs shown in Table 11 and Table 12, the proposed\nCLIPFit brings consistent improvement compared\nwith other methods.\n\n(a) Text Encoder\n(b) MSE loss\nFigure 7: Visualization of changes in different layers on\nthe EuroSAT dataset.\n(a) LayerNorm gain\n(b) LayerNorm bias\nFigure 8: Visualization of LayerNorm changes w/ and\nw/o regularization loss in the DTD dataset.\nH\nMore Fine-tuning Analysis\nSec. 4.3 discussed the changes in unfixed parame-\nters after fine-tuning the DTD dataset and the im-\nportance of more changed LayerNorm. In this sub-\nsection, we give more detailed analyses of other\ndatasets and other aspects.\nImportance of low-level bias terms in text en-\ncoder. Sec. 4.3 presented that after the fine-tuning\nof CLIPFit, for bias terms in the FNN of the text en-\ncoder, as the number of layers increases, the change\nin bias decreases. In this subsection, we conducted\nexperiments to verify whether more changed layers\nin the text encoder are more important. Similar to\nSec. 4.3, we freeze less (or more) changed Layer-\nNorm bias layers in the text encoder on the 4-shot\nsetting. When updating only the first bias layer\nand freezing other layers, the average accuracy is\n74.69%. For comparison, the average accuracy is\n73.33% when only updating the sixth bias layer\nand 70.62% when only updating the last bias layer.\nBoth are much lower than updating the first layer.\nWe also found that when only updating the top-3\nbias layers (changed more) and freezing other bias\nterm layers, the average accuracy is 76.13%. For\ncomparison, when only updating the last 3 bias\nlayers (changed less) and freezing other bias term\nlayers, the average accuracy is 70.86%, which is\nmuch lower than updating the top 3 bias layers.\nThese results demonstrate that the more changed\nparameters are crucial for fine-tuning.\nAnalyzing LayerNorm with regularization\nloss. Sec. 4.3 analyzed the difference of changes\n(a) KD loss\n(b) MSE loss\nFigure 9: Visualization of bias changes w/ and w/o\nregularization loss in the EuroSAT dataset.\n(a) LayerNorm gain\n(b) LayerNorm bias\nFigure 10: Visualization of LayerNorm changes w/ and\nw/o regularization loss in the EuroSAT dataset.\n(b) Change in each itera/on\n(a) LayerNorm gradient sum\nFigure 11: Left: visualization of squared gradient sum\nin LayerNorm layers. Right: change of the first text bias\nlayer and first LayerNorm layer at each iteration.\nin the text encoder bias terms between w/ and w.o\nregularization loss. In this subsection, we will ana-\nlyze LayerNorm in the image encoder bias terms\nbetween w/ and w.o regularization loss. Noted that\nalthough the two regularization losses are applied\nto text features or text encoder, the image encoder\nor image features will also be affected since these\ntwo encoders are fine-tuned simultaneously. The\nresults on the DTD dataset are shown in Fig. 8.\nWhen fine-tuning w/ KD loss, unlike in text en-\ncoder, changes in gain and bias increase compared\nwith w/o KD loss. This phenomenon implies that\nimage features will change more w/ KD loss com-\npared with fine-tuning w/o KD loss. Moreover, we\nalso found that the increases are almost in the more\nchanged LayerNorm layers. When fine-tuning w/\nMSE loss, changes in gain and bias are equal or\nslightly higher than fine-tuning w/o KD loss.\nLayerNorm gradient. We visualize the squared\nsum of gradient from each LayerNorm layer in the\nimage encoder in Fig. 11 (a). As observed, the\nmagnitude of gradient in the first LayerNorm layer\n\nTable 10: Comparison with existing methods in the few-shot learning setting. S.C.: StandfordCars dataset. F.A.:\nFGVCAircraft dataset.\nshot\nMethod\nImageNet\nCaltech101\nOxfordPets\nS.C.\nFlowers\nFood101\nF.A.\nSun397\nDTD\nEuroSAT\nUCF101\nAVG\n1\nCoOp\n65.77\n92.37\n92.2\n67.1\n82.2\n82.07\n26.73\n64.7\n49.0\n54.8\n72.0\n68.09\nCoCoOp\n69.51\n93.8\n91.17\n67.92\n71.98\n86.1\n13.2\n68.19\n48.51\n55.71\n70.35\n66.95\nCLIP-adapter\n67.93\n93.3\n89.03\n67.1\n72.03\n85.9\n27.6\n67.1\n45.2\n61.7\n69.67\n67.87\nTIP-adapter\n67.43\n93.56\n90.72\n67.88\n86.63\n86.01\n29.58\n64.49\n53.25\n63.95\n73.27\n70.62\nProGrad\n64.33\n90.96\n89.01\n67.11\n83.81\n82.75\n27.97\n64.54\n52.79\n55.1\n71.91\n68.2\nKgCoOp\n69.03\n94.13\n91.97\n67.03\n74.63\n86.27\n26.9\n68.43\n52.5\n60.83\n72.93\n69.51\nCLIPFit\n69.37\n93.67\n91.63\n69.33\n82.83\n86.17\n27.73\n69.07\n54.63\n76.87\n74.27\n72.32\n2\nCoOp\n68.17\n92.83\n89.2\n69.37\n88.47\n80.8\n29.57\n66.4\n51.7\n61.2\n73.67\n70.13\nCoCoOp\n69.84\n94.92\n92.14\n68.77\n76.12\n86.21\n15.03\n69.11\n52.02\n46.24\n73.58\n67.63\nCLIP-adapter\n68.6\n93.67\n89.73\n68.97\n78.53\n86.1\n29.6\n69.0\n47.87\n66.07\n74.1\n70.2\nTIP-adapter\n68.6\n94.22\n91.1\n71.39\n90.21\n86.26\n32.51\n66.74\n56.32\n70.38\n76.11\n73.08\nProGrad\n66.12\n93.21\n90.55\n71.94\n88.62\n84.81\n30.84\n68.51\n54.35\n66.19\n74.39\n71.78\nKgCoOp\n69.63\n94.2\n92.13\n68.13\n79.47\n86.6\n28.07\n69.53\n55.73\n68.97\n74.83\n71.57\nCLIPFit\n69.93\n94.47\n92.03\n72.7\n87.77\n86.63\n30.7\n70.87\n57.7\n78.83\n76.7\n74.39\n4\nCoOp\n69.38\n94.44\n91.3\n72.73\n91.14\n82.58\n33.18\n70.13\n58.57\n68.62\n77.41\n73.59\nCoCoOp\n70.55\n94.98\n93.01\n69.1\n82.56\n86.64\n30.87\n70.5\n54.79\n63.83\n74.99\n71.98\nCLIP-adapter\n69.56\n94.0\n90.87\n71.13\n86.77\n86.47\n31.1\n71.3\n53.83\n66.8\n77.3\n72.65\nTIP-adapter\n69.86\n95.06\n91.58\n74.59\n91.5\n86.48\n35.15\n70.29\n62.09\n76.43\n80.24\n75.75\nProGrad\n70.21\n94.93\n93.21\n71.75\n89.98\n85.77\n32.93\n71.17\n57.72\n70.84\n77.82\n74.21\nKgCoOp\n70.19\n94.65\n93.2\n71.98\n90.69\n86.59\n32.47\n71.79\n58.31\n71.06\n78.4\n74.48\nCLIPFit\n70.4\n95.0\n93.07\n76.43\n92.03\n86.73\n35.8\n73.0\n63.2\n83.17\n80.13\n77.18\n8\nCoOp\n70.83\n94.1\n90.83\n76.57\n94.37\n83.37\n37.63\n72.5\n64.7\n75.53\n80.57\n76.45\nCoCoOp\n70.77\n95.11\n93.44\n70.19\n84.17\n86.92\n26.53\n70.62\n58.92\n68.26\n77.19\n72.92\nCLIP-adapter\n70.2\n94.27\n91.77\n76.47\n94.9\n86.67\n37.2\n73.13\n66.4\n73.23\n81.87\n76.92\nTIP-adapter\n71.4\n95.2\n92.09\n78.34\n94.98\n86.74\n40.61\n73.6\n67.28\n81.11\n82.24\n78.51\nProGrad\n71.1\n94.92\n92.18\n78.78\n93.51\n85.91\n37.89\n72.91\n62.13\n79.22\n88.64\n77.93\nKgCoOp\n70.23\n94.97\n93.1\n73.53\n89.53\n86.9\n34.97\n72.5\n65.87\n72.37\n80.03\n75.82\nCLIPFit\n71.0\n95.43\n93.13\n79.57\n94.7\n87.0\n39.93\n74.27\n67.17\n84.87\n82.17\n79.03\n16\nCoOp\n71.51\n95.5\n91.8\n78.89\n96.1\n85.17\n40.93\n74.5\n68.63\n83.6\n82.43\n79.01\nCoCoOp\n71.02\n95.19\n93.25\n71.68\n87.64\n87.19\n31.29\n72.05\n63.78\n73.82\n78.34\n75.02\nCLIP-adapter\n71.6\n94.57\n92.03\n80.9\n97.0\n86.83\n42.67\n75.3\n71.17\n81.87\n84.53\n79.86\nTIP-adapter\n73.1\n95.79\n92.7\n83.09\n96.18\n87.24\n45.59\n74.99\n72.05\n87.46\n84.5\n81.15\nProGrad\n72.68\n95.8\n92.13\n81.46\n94.87\n87.01\n40.39\n75.0\n65.92\n84.38\n81.59\n79.2\nKgCoOp\n71.2\n95.03\n93.23\n74.87\n92.9\n87.03\n36.27\n73.4\n69.37\n74.93\n81.43\n77.26\nCLIPFit\n71.53\n96.13\n93.5\n82.43\n96.37\n87.37\n45.47\n75.67\n71.57\n90.13\n83.83\n81.27\nis much bigger than other layers. So the differ-\nence in change may be caused by the difference in\ngradient.\nChange in each iteration. We visualize the\nchange in first-layer text bias terms, first-layer Lay-\nerNorm gain, and first-layer LayerNorm bias for\neach iteration in Fig. 11 (b). As observed, the\nchange will increase smoothly and converge to\nsome values.\nAnalyses on other datasets. We also conducted\nanalyses on other datasets. The results for the Eu-\nroSAT dataset are shown in Fig. 7, Fig. 9, and Fig.\n10. The phenomena in the EuroSAT dataset are\nvery similar to the DTD dataset.\n\nTable 11: Comparison with existing methods in the base-to-new generalization based on the 4-shot settings. H:\nHarmonic mean.\nDatasets\nmetric\nCoOp\nCLIP-adapter\nCoCoOp\nProGrad\nKgCoOp\nCLIPFit\nImageNet\nBase\n73.6\n74.23\n75.46\n74.24\n74.87\n75.03\nNew\n63.29\n67.93\n69.58\n65.47\n69.09\n69.87\nH\n68.06\n70.94\n72.4\n69.58\n71.86\n72.36\nCaltech101\nBase\n97.27\n97.23\n97.25\n97.37\n97.53\n97.57\nNew\n93.01\n94.17\n94.9\n93.92\n94.43\n94.23\nH\n95.09\n95.68\n96.06\n95.61\n95.95\n95.87\nOxfordPets\nBase\n93.33\n93.8\n94.59\n94.08\n94.68\n94.93\nNew\n95.69\n97.0\n96.75\n97.63\n97.58\n96.97\nH\n94.5\n95.37\n95.66\n95.82\n96.11\n95.94\nStandfordCars\nBase\n70.92\n69.43\n67.71\n72.69\n69.25\n73.77\nNew\n69.38\n73.0\n75.37\n69.88\n74.98\n73.77\nH\n70.14\n71.17\n71.33\n71.26\n72.0\n73.77\nFlowers\nBase\n92.5\n87.93\n84.75\n92.46\n91.3\n91.03\nNew\n70.12\n71.9\n73.85\n72.69\n75.34\n74.47\nH\n79.77\n79.11\n78.93\n81.39\n82.56\n81.92\nFood101\nBase\n86.79\n90.2\n89.79\n88.91\n90.3\n90.2\nNew\n89.06\n90.97\n90.99\n90.18\n91.39\n91.23\nH\n87.91\n90.58\n90.39\n89.54\n90.84\n90.71\nFGVCAircraft\nBase\n33.21\n32.43\n32.07\n33.73\n34.21\n34.53\nNew\n28.57\n33.77\n33.93\n30.09\n32.81\n31.47\nH\n30.72\n33.09\n32.97\n31.81\n33.5\n32.93\nSun397\nBase\n76.49\n77.7\n77.57\n77.72\n78.87\n79.5\nNew\n64.56\n75.67\n76.96\n71.93\n75.64\n77.77\nH\n70.02\n76.67\n77.26\n74.71\n77.22\n78.63\nDTD\nBase\n71.26\n67.43\n67.44\n71.06\n73.65\n74.37\nNew\n50.93\n55.43\n56.0\n52.58\n57.21\n64.1\nH\n59.4\n60.84\n61.19\n60.44\n64.4\n68.85\nEuroSAT\nBase\n82.56\n81.9\n79.27\n82.48\n82.63\n88.57\nNew\n53.04\n59.67\n65.44\n56.43\n59.98\n76.7\nH\n64.59\n69.04\n71.69\n67.01\n69.51\n82.21\nUCF101\nBase\n79.97\n80.4\n78.01\n81.3\n80.8\n82.77\nNew\n65.98\n76.17\n73.07\n76.02\n75.77\n76.43\nH\n72.3\n78.23\n75.46\n78.57\n78.2\n79.47\nAVG\nBase\n78.43\n77.52\n76.72\n79.18\n78.92\n80.21\nNew\n68.03\n72.33\n73.35\n71.14\n73.11\n75.18\nH\n72.44\n74.84\n74.85\n74.62\n75.9\n77.61\n\nTable 12: Comparison with existing methods in the base-to-new generalization based on the 8-shot settings. H:\nHarmonic mean.\nDatasets\nmetric\nCoOp\nCLIP-adapter\nCoCoOp\nProGrad\nKgCoOp\nCLIPFit\nImageNet\nBase\n75.22\n75.07\n75.52\n75.72\n75.84\n75.73\nNew\n65.91\n67.6\n70.28\n66.76\n69.33\n70.07\nH\n70.26\n71.14\n72.81\n70.96\n72.44\n72.79\nCaltech101\nBase\n97.81\n97.3\n97.76\n98.0\n97.68\n97.83\nNew\n92.58\n93.83\n93.63\n93.38\n94.1\n93.8\nH\n95.12\n95.53\n95.65\n95.63\n95.86\n95.77\nOxfordPets\nBase\n94.19\n94.33\n95.5\n94.47\n94.81\n94.83\nNew\n96.11\n96.83\n97.69\n97.03\n97.58\n97.03\nH\n95.14\n95.56\n96.58\n95.73\n96.18\n95.92\nStandfordCars\nBase\n73.2\n72.13\n69.7\n75.08\n69.66\n76.63\nNew\n67.44\n71.37\n74.13\n70.63\n75.4\n74.23\nH\n70.2\n71.75\n71.85\n72.79\n72.42\n75.41\nFlowers\nBase\n96.17\n94.27\n92.24\n93.8\n87.72\n94.17\nNew\n69.41\n70.67\n72.77\n72.2\n74.75\n74.47\nH\n80.63\n80.78\n81.36\n81.59\n80.72\n83.17\nFood101\nBase\n87.27\n90.27\n89.6\n89.48\n90.46\n90.33\nNew\n86.96\n90.7\n90.79\n89.9\n91.63\n91.5\nH\n87.11\n90.48\n90.19\n89.69\n91.04\n90.91\nFGVCAircraft\nBase\n37.01\n35.47\n33.71\n36.89\n34.53\n38.9\nNew\n38.45\n33.03\n32.15\n31.67\n34.95\n32.43\nH\n37.72\n34.2\n32.91\n34.08\n34.74\n35.37\nSun397\nBase\n78.61\n79.53\n78.05\n79.21\n79.37\n80.57\nNew\n66.25\n74.9\n76.29\n70.77\n76.85\n77.77\nH\n71.9\n77.1\n77.16\n74.75\n78.09\n79.15\nDTD\nBase\n76.97\n74.43\n73.03\n74.42\n69.72\n77.87\nNew\n51.81\n52.77\n57.24\n52.38\n56.44\n62.63\nH\n61.93\n61.75\n64.18\n61.48\n62.38\n69.42\nEuroSAT\nBase\n83.27\n80.23\n78.68\n82.27\n81.07\n90.3\nNew\n50.59\n59.87\n56.03\n58.52\n63.13\n73.0\nH\n62.94\n68.57\n65.45\n68.39\n70.98\n80.73\nUCF101\nBase\n82.85\n82.83\n80.4\n82.61\n81.16\n84.4\nNew\n64.32\n74.53\n71.68\n73.75\n78.65\n77.57\nH\n72.42\n78.46\n75.79\n77.93\n79.89\n80.84\nAVG\nBase\n80.74\n79.62\n78.56\n80.62\n78.37\n81.96\nNew\n68.39\n71.46\n72.06\n71.02\n73.75\n74.95\nH\n73.51\n75.32\n74.9\n75.21\n76.06\n78.3",
    "pdf_filename": "Vision-Language_Model_Fine-Tuning_via_Simple_Parameter-Efficient_Modification.pdf"
}