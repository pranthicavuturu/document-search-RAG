{
    "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient",
    "abstract": "a set of learnable prompt vectors as the input of Recent advances in fine-tuning Vision- thetextencoderwhileadaptertuningapproaches Language Models (VLMs) have witnessed adoptedanadditionalbottlenecklayertolearnnew the success of prompt tuning and adapter features. During the fine-tuning procedure, both tuning,whiletheclassicmodelfine-tuningon of these two strategies keep CLIP‚Äôs parameters inherent parameters seems to be overlooked. fixed. The performance of prompt tuning and It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts adapter tuning methods are superior on various the pre-trained knowledge since fine-tuning tasks (Zhou et al., 2022b; Gao et al., 2023), so the CLIP model even degrades performance. researchonfine-tuningtheinherentparametersof In this paper, we revisit this viewpoint, and VLMshasbeenbarelytouched. propose a new perspective: fine-tuning the For language models, fully fine-tuning with specificparametersinsteadofallwilluncover downstream data can achieve promising results the power of classic model fine-tuning on (Zaken et al., 2021; Liu et al., 2022). Moreover, VLMs. Through our meticulous study, we proposeClipFit,asimpleyeteffectivemethod recentworksinlanguagemodelfine-tuning(e.g., to fine-tune CLIP without introducing any BitFit(Zakenetal.,2021))havedemonstratedthat, overheadofextraparameters. Wedemonstrate withoutintroducinganyexternalparameters,fine- thatbyonlyfine-tuningthespecificbiasterms tuningonlythebiastermsinapre-trainedmodel andnormalizationlayers,ClipFitcanimprove can perform competitively on downstream tasks theperformanceofzero-shotCLIPby7.27% compared with fine-tuning the entire model. For average harmonic mean accuracy. Lastly, VLMs,however,itisbelievedthatfine-tuningthe to understand how fine-tuning in CLIPFit parameters of VLMs corrupts the inherent pre- affectsthepre-trainedmodels, weconducted extensiveexperimentalanalysesw.r.t. changes trained knowledge as fully fine-tuning degrades ininternalparametersandrepresentations. We performance (Zhou et al., 2022b). In this paper, found that low-level text bias layers and the we revisit this viewpoint and ask if, without in- first layer normalization layer change much troducinganyexternalparameters,fine-tuningthe morethanotherlayers. Thecodeisavailableat inherentparametersofVLMscanachievecompeti- https://github.com/minglllli/CLIPFit. tiveperformancecomparedwithprompttuning. We start with directly applying BitFit to fine- 1 Introduction tuningtheCLIPmodel. Weexploretwostrategies: Large pre-trained Visual-Language Models (i)applyingBitFittothetextencoderalone,and(ii) (VLMs)havebeendevelopedalotinrecentyears. applyingBitFittoboththetextandimageencoder. For example, CLIP (Radford et al., 2021) and Wefoundthatbothtwostrategiescanacquiretask- ALIG (Jia et al., 2021) demonstrated remarkable specificknowledgebuttheirperformancetounseen performance for various tasks, e.g., image recog- classdatacanbepoor(morediscussedinSec. 4.4), nition in a zero-shot fashion. To further improve implying that directly fine-tuning the bias terms theperformanceonthespecificdownstreamtasks, ofatextorimageencodermayharmthemodel‚Äôs prompttuning(Lesteretal.,2021;Yaoetal.,2023; generalizationability. Thesefindingsmotivateus Zhu et al., 2023; Zhou et al., 2022a) and adapter todevelopmoreeffectiveandefficientfine-tuning tuning (Gao et al., 2023; Zhang et al., 2021) techniquesforVLMs. methods have been proposed. As shown in Fig. Inlightofthis,weproposeCLIPFit,asimpleyet 4202 voN 91 ]VC.sc[ 2v81761.9042:viXra",
    "body": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\nModification\nMingLi1,JikeZhong2,ChenxinLi4,LiuzhuozhengLi1,\nNieLin1,MasashiSugiyama3,1\n1TheUniversityofTokyo,2UniversityofSouthernCalifornia,\n3RIKENCenterforAdvancedIntelligenceProject,\n4 TheChineseUniversityofHongKong\nli-ming948@g.ecc.u-tokyo.ac.jp jikezhon@usc.edu\nchenxinli@stu.xmu.edu.cn l.li@ms.k.u-tokyo.ac.jp\nnielin@iis.u-tokyo.ac.jp sugi@k.u-tokyo.ac.jp\nAbstract 1, prompt tuning methods proposed to introduce\na set of learnable prompt vectors as the input of\nRecent advances in fine-tuning Vision- thetextencoderwhileadaptertuningapproaches\nLanguage Models (VLMs) have witnessed\nadoptedanadditionalbottlenecklayertolearnnew\nthe success of prompt tuning and adapter\nfeatures. During the fine-tuning procedure, both\ntuning,whiletheclassicmodelfine-tuningon\nof these two strategies keep CLIP‚Äôs parameters\ninherent parameters seems to be overlooked.\nfixed. The performance of prompt tuning and\nIt is believed that fine-tuning the parameters\nof VLMs with few-shot samples corrupts adapter tuning methods are superior on various\nthe pre-trained knowledge since fine-tuning tasks (Zhou et al., 2022b; Gao et al., 2023), so\nthe CLIP model even degrades performance. researchonfine-tuningtheinherentparametersof\nIn this paper, we revisit this viewpoint, and\nVLMshasbeenbarelytouched.\npropose a new perspective: fine-tuning the\nFor language models, fully fine-tuning with\nspecificparametersinsteadofallwilluncover\ndownstream data can achieve promising results\nthe power of classic model fine-tuning on\n(Zaken et al., 2021; Liu et al., 2022). Moreover,\nVLMs. Through our meticulous study, we\nproposeClipFit,asimpleyeteffectivemethod recentworksinlanguagemodelfine-tuning(e.g.,\nto fine-tune CLIP without introducing any BitFit(Zakenetal.,2021))havedemonstratedthat,\noverheadofextraparameters. Wedemonstrate withoutintroducinganyexternalparameters,fine-\nthatbyonlyfine-tuningthespecificbiasterms tuningonlythebiastermsinapre-trainedmodel\nandnormalizationlayers,ClipFitcanimprove\ncan perform competitively on downstream tasks\ntheperformanceofzero-shotCLIPby7.27%\ncompared with fine-tuning the entire model. For\naverage harmonic mean accuracy. Lastly,\nVLMs,however,itisbelievedthatfine-tuningthe\nto understand how fine-tuning in CLIPFit\nparameters of VLMs corrupts the inherent pre-\naffectsthepre-trainedmodels, weconducted\nextensiveexperimentalanalysesw.r.t. changes trained knowledge as fully fine-tuning degrades\nininternalparametersandrepresentations. We performance (Zhou et al., 2022b). In this paper,\nfound that low-level text bias layers and the we revisit this viewpoint and ask if, without in-\nfirst layer normalization layer change much\ntroducinganyexternalparameters,fine-tuningthe\nmorethanotherlayers. Thecodeisavailableat\ninherentparametersofVLMscanachievecompeti-\nhttps://github.com/minglllli/CLIPFit.\ntiveperformancecomparedwithprompttuning.\nWe start with directly applying BitFit to fine-\n1 Introduction\ntuningtheCLIPmodel. Weexploretwostrategies:\nLarge pre-trained Visual-Language Models (i)applyingBitFittothetextencoderalone,and(ii)\n(VLMs)havebeendevelopedalotinrecentyears. applyingBitFittoboththetextandimageencoder.\nFor example, CLIP (Radford et al., 2021) and Wefoundthatbothtwostrategiescanacquiretask-\nALIG (Jia et al., 2021) demonstrated remarkable specificknowledgebuttheirperformancetounseen\nperformance for various tasks, e.g., image recog- classdatacanbepoor(morediscussedinSec. 4.4),\nnition in a zero-shot fashion. To further improve implying that directly fine-tuning the bias terms\ntheperformanceonthespecificdownstreamtasks, ofatextorimageencodermayharmthemodel‚Äôs\nprompttuning(Lesteretal.,2021;Yaoetal.,2023; generalizationability. Thesefindingsmotivateus\nZhu et al., 2023; Zhou et al., 2022a) and adapter todevelopmoreeffectiveandefficientfine-tuning\ntuning (Gao et al., 2023; Zhang et al., 2021) techniquesforVLMs.\nmethods have been proposed. As shown in Fig. Inlightofthis,weproposeCLIPFit,asimpleyet\n4202\nvoN\n91\n]VC.sc[\n2v81761.9042:viXra\nüî•\nPrompt [class] A photo of a [class]. A photo of a [class].\n‚ùÑ\nImage Encoder\n‚ùÑ ‚ùÑ‚ùÑ ‚ùÑ/üî• ‚ùÑ/üî•\nüî•\nText Encoder Image Encoder Text Encoder Text Encoder Image Encoder\nAdapter\ncos cos cos\n(a) Prompt Tuning (b) Adapter Tuning (c) CLIPFit‚ÄìOur Method\nFigure1: Comparisonof(a)prompttuningmethods,(b)adaptertuningmethods,and(c)ourproposedCLIPFit\nmethod. Prompttuningmethodsintroduceasetoflearnableexternalparametersasinputtolearntask-specific\nknowledge. Adapter tuning methods introduce extra learnable networks following the image encoder to learn\ntask-specificfeatures. Unlikethesetwomethods,ourCLIPFitdoesnotintroduceexternalparametersandfine-tunes\nonlyasmallportionoftheCLIPmodel.\neffectivemethodforefficientlyfine-tuningVLMs. Fine-tuning is an empirical and black-box pro-\nCLIPFit is orthogonal to previous prompt tuning cess. So, understanding how fine-tuning affects\nandadaptertuningmethods,asshowninFig. 1(c). thepre-trainedmodelsisimportantforuncovering\nForthetextencoder,insteadoffine-tuningallthe theblack-boxfine-tuningprocess. Previousworks\nbiasterms,CLIPFitproposestotuneonlythebias (ZhouandSrikumar,2022;DeVriesetal.,2020;\nterms of projection linear layers in feed-forward Merchantetal.,2020)exploredthisforlanguage\nnetworks(FFNs). Fine-tuningonlythebiasterms modelsfine-tuning. However,verylittleworkex-\nofprojectionlinearlayersinFFNswillreducethe plored the internal black-box fine-tuning process\nnumberoftrainingparameterscomparedwithfine- for VLMs. In this paper, we conducted an initial\ntuning all the bias terms. Moreover, empirically, explorationtoanalyzeVLMfine-tuningprocessof\nwediscoveredthatourbiastermtuningstrategycan CLIPFit, focusing on changes in internal param-\ngeneralize better than BitFit (Zaken et al., 2021), eters and representations. We found that for bias\nas shown in Sec. 4.4. For the image encoder, as termsintheFNNofthetextencoder,asthenumber\nmentioned before, it may harm the model‚Äôs per- of layers increases, the change in bias decreases,\nformance if directly applying BitFit. In the im- which means that during the fine-tuning process,\nageencoder,layernormalization(LayerNorm)(Ba low-levelfeaturesinthetextencoderchangemore\netal.,2016)aimstonormalizethedistributionsof than high-level features. For LayerNorm in the\nintermediatelayers. Sincethedistributionsofpre- imageencoder,wefoundthatthefirstlayer(patch\ntraininganddownstreamdatamightbedivergent, embedding) changes much more than other lay-\npre-trainedLayerNormmightleadtosub-optimal ers. Experimentally,weshowedthatmorechanged\nperformancefordownstreamdatainference. There- layersplayamoreimportantroleinadaptingdown-\nfore,CLIPFitproposestofurtherupdateonlythe streamknowledgethanlesschangedlayers. More-\nparametersoftheimageencoder‚ÄôsLayerNorm. Up- over, we explored how KD loss affects the fine-\ndatingLayerNormcanyieldabetterimageencoder tuningprocessforalleviatingforgetting. Wefound\nfordownstreamdata. Lastly,previousstudies(Yao thatKDlosswillreducethechangesforthemore-\net al., 2023) have shown that generic pre-trained changedlow-levelbiastermsandenhancechanges\nknowledge is easily forgotten in the fine-tuning in less-changed high-level layers, which implies\nstage. Therefore,weexploredtwodifferentregu- thatpenalizingchangesforlow-levelbiastermsis\nlarization strategies for alleviating forgetting: (i) importantforavoidingoverfitting. Lastly,wefound\nusingtheknowledgedistillation(KD)loss(Hinton that tuning LayerNorm will form a better image\net al., 2015) to guide CLIPFit to learn from the featurespacecomparedwithzero-shotCLIP.\nzero-shotCLIP;(ii)usingthemeansquarederror We conducted extensive experiments on 11\n(MSE) loss in bias terms to penalize changes in datasetsin4differentsettingstoshowtheeffective-\ntextencoder. Weempiricallyfoundthatbothtwo ness of the proposed CLIPFit. Overall, our main\nstrategiescanalleviateforgettingproblemsandthe contributionscanbesummarizedasfollows:\nKDlossperformsbetter,thusweusedtheKDloss ‚Ä¢ WeproposeaCLIPFitmethodforefficiently\nasthefinalsolutionforCLIPFit. fine-tuning the CLIP model to uncover the\npowerofclassicmodelfine-tuningonVLMs. to fine-tune CLIP by modifying a small portion\nUnlikeexistingprompttuningoradaptertun- oftheCLIPmodel‚Äôsinherentparameterswithout\ningmethods,CLIPFitdoesnotintroduceany introducinganyexternallearnableparameters.\nexternal parameters and only fine-tunes a PEFTonLargeLanguageModels. Fullyfine-\nsmall specific subset of CLIP‚Äôs inherent pa- tuninglanguagemodels(Radfordetal.,2018;De-\nrameters. vlinetal.,2018)canachievepromisingresultsbut\nis expensive. To efficiently fine-tune pre-trained\n‚Ä¢ To analyze how CLIPFit affects the pre-\nlanguagemodels,alotofapproacheshavesought\ntrainedmodels,weconductedextensiveanaly-\nto fine-tune only a small number of parameters.\nsesduringthefine-tuningprocess,focusingon\nForexample,adaptermethods(Bapnaetal.,2019;\nthechangesinparametersandrepresentations.\nHoulsby et al., 2019; Pfeiffer et al., 2020) and\nTheseanalyseshelpusbetterunderstandthe\nprompt tuning methods (Liu et al., 2023; Lester\nblack-boxfine-tuningprocess.\netal.,2021;Brownetal.,2020;Gaoetal.,2020)\nintroduce a set of learnable external parameters\n‚Ä¢ We conducted extensive experiments on 11\nforadaptationtodownstreamtasks. Recently,Bit-\ndatasets. ResultsshowthatCLIPFitbringsa\nFit(Zakenetal.,2021)demonstratedthat,without\n7.33%improvementinharmonicmeanaccu-\nintroducinganynewparameters,fine-tuningonly\nracycomparedwithzero-shotCLIPonthe16-\nthebiastermsinpre-trainedlanguagemodelscan\nshotbase-to-newsetting,demonstratingthat\nperform competitively compared with fully fine-\nCLIPFitisapromisingalternativetoprompt\ntuning. However,BitFitisdesignedforLLMfine-\ntuningandadaptertuning.\ntuning,andourexperimentsinSec. 4showsthat\ndirectly applying BitFit to VLM fine-tuning may\n2 RelatedWorks\nharmthemodel‚Äôsgeneralizationability. Thus,our\nVisual-Language Models (VLMs). With large- CLIPFitproposestoonlyfine-tunetheLayerNorm\nscale available web-crawled image-text pairs of image encoder motivated by distribution shift.\n(Schuhmannetal.,2022),pre-trainingVLMshave OurmethodisdifferenttoBitFitMoreover,toun-\nbeendevelopedfastinrecentyears(Xuetal.,2021; derstandhowfine-tuningaffectspre-trainedmod-\nRadfordetal.,2021;Jiaetal.,2021;Wangetal., els,variousworks(ZhouandSrikumar,2022;Mos-\n2022a)andachievedremarkablezero-shotperfor- bachetal.,2020;DeVriesetal.,2020;Merchant\nmance in the downstream tasks, e.g., image clas- et al., 2020) have explored this with LLM fine-\nsification. Despitetheremarkabletransferability, tuning. However, very little work was attempted\nthe potential of VLMs can be further stimulated on the VLM side. In this paper, we attempt to\nby fine-tuning it with few-shot downstream data bridgethisgapbyconductinganinitialexploration\n(Songetal.,2022;Zhangetal.,2021;Shenetal., to analyze the fine-tuning process in CLIPFit for\n2021;Wangetal.,2022c,b;Chenetal.,2023a). VLMs,focusingonchangesininternalparameters\nParameter-efficient Fine-tuning (PEFT) on andrepresentations.\nVLMs. TherearemainlytwocategoriesofVLM\nparameter-efficient fine-tuning methods: prompt 3 Methodolgy\ntuning(Zhouetal.,2022b,a;Chenetal.,2022;Yao\nIn this section, we introduce CLIPFit. We first\net al., 2023; Zhu et al., 2023; Zhang et al., 2023;\nbrieflyreviewCLIPandthenillustrateCLIPFit.\nKhattaketal.,2023)andadaptertuning(Gaoetal.,\n2023;Zhangetal.,2021). Prompttuningmethods\n3.1 ReviewofCLIP\nforVLMsintroducedafewlearnableparameters\n(prompts) as input, which were inspired by lan- WefirstbrieflyreviewCLIP(Radfordetal.,2021).\nguageprompttuning(Lesteretal.,2021). Adapter Duringpre-training,CLIPaimstoalignimagefea-\ntuningmethodssetanadditionalbottlenecklayer turesandtextfeaturesinthejointembeddingspace\nfollowingthetextorimageencodertolearnbetter to capture the relationship between images and\nfeaturesbyaresidualway. Bothprompttuningand texts. LetD = {(x ,t )}b bethesampledbatch,\ni i i=1\nadaptertuningmethodsboostCLIP‚Äôsperformance, wherex istheinputimage,t istheinputtextand\ni i\nsoresearchonfine-tuningtheinherentparameters b is the batch size. A CLIP model is comprised\nofCLIPseemstobeoverlooked. Toexploreclassic oftwotypesofencoders: visualencoderE (¬∑,Œ∏ )\nI I\nmodelfine-tuningonVLMs,ourCLIPFitproposes and text encoder E (¬∑,Œ∏ ). The visual encoder\nT T\nText Encoder\n‚ùÑ ‚ùÑ ‚ùÑ üî• üî• Tuned\n‚ùÑFrozen\n√óN A photo of a [class].\ncat dog ‚Ä¶ bird\nFFN\n‚ùÑ\n‚ùÑ\nWeight\nBias\nüî• y1 y2 y3 ‚Ä¶ yn\nx1 x1¬∑y1x1¬∑y2x1¬∑y3 ‚Ä¶ x1¬∑yn\nüî• ‚ùÑ üî• ‚ùÑ\nx2 x2¬∑y1x2¬∑y2x2¬∑y3 ‚Ä¶ x2¬∑yn\n√óN x3 x3¬∑y1x3¬∑y2x3¬∑y3 ‚Ä¶ x3¬∑yn\n‚Ä¶\nImage\nxn xn¬∑y1xn¬∑y2xn¬∑y3 ‚Ä¶ xn¬∑yn\nImage Encoder\nFigure2: AnoverviewofourCLIPFit. Unlikeexistingprompttuningmethodsoradaptertuningmethods,CLIPFit\ndoes not introduce any external parameters and fine-tunes specific inherent parameters of CLIP. For the text\nencoder,asshownintheupperpartofthefigure,CLIPFitfine-tunesonlythebiastermsofprojectionlinearlayersin\nfeed-forwardnetworks.Fortheimageencoder,asshowninthelowerpartofthefigure,CLIPFitupdatesLayerNorm.\nencodesimagex intof andtextt intog ,i.e., Text Encoder. For the text encoder, instead\ni i i i\nof fine-tuning all bias terms, CLIPFit fine-tunes\nf = E (x ,Œ∏ ), g = E (t ,Œ∏ ). (1)\ni I i I j T i T onlythebiastermsofprojectionlinearlayers(i.e.,\nThen,acontrastivelearninglossisappliedtothem second layers) in the FFNs of the text encoder.\nforalignment. Fine-tuning only part of bias terms will reduce\nAfterpre-training,CLIPcanperformzero-shot thenumberoftrainingparameterscomparedwith\nimagerecognitionbycomparingtheimagefeatures fine-tuningallbiasterms. Moreover,Sec. 4.4will\nwithclassweights{w }K ,whereK isthenum- empiricallyshowthatourbiastuningmethodcan\ni i=1\nber of classes. The class weight w is generated achieve better performance compared with fine-\ni\nby text encoder E (¬∑,Œ∏ ) which takes the class tuningallbiasterms(Zakenetal.,2021).\nT T\ndescriptions (prompts) as input. These prompts Image Encoder. As mentioned in Sec. 1, di-\nusually take the form ‚Äúa photo of a [CLASS].‚Äù, rectly applying BitFit (Zaken et al., 2021) to the\nwheretheclasstokenwillbereplacedbythei-th image encoder may cause a negative impact on\nclassname(e.g.,cat)forweightw i. Formally,for the model‚Äôs performance. Instead of fine-tuning\nanimagefeaturef,theprobabilitythatitbelongs thebiastermsoftheimageencoder,CLIPFitpro-\ntoclassiiscalculatedby posestofine-tuneLayerNorm. InLayerNorm,the\ntwolearniableparametersgaing andbiasbareap-\nexp(cos(w ,f)/œÑ)\ni\np(y = i | x) = , (2) pliedforaffinetransformationonnormalizedinput\n(cid:80)K\nexp(cos(w ,f)/œÑ)\nj=1 j vectors x for re-centering and re-scaling, which\nwhere œÑ is a temperature parameter learned by are expected to enhance the expressive power by\nCLIPduringpre-trainingandcos(¬∑,¬∑)denotesthe re-shaping the distribution (Ba et al., 2016). Dif-\ncosinesimilarityfunction. ferentdatadistributionsshouldproducedifferent\ngainsandbiasesinLayerNormfordistributionre-\n3.2 CLIPFit\nshapingduringthetrainingprocess. Priorworkhas\nThe overall pipeline of the proposed CLIPFit is also found norm layers to be an influential com-\nshowninFig. 2. Withoutintroducinganyexternal ponent in the fine-tuning process (Giannou et al.,\nparameters,CLIPFitinvolvesfine-tuningonlythe 2023;Qietal.,2022;Tuetal.,2023;Zhongetal.,\nbias terms of projection linear layers in FNNs of 2024). So, if shifted gains and biases in Layer-\nthetextencoderandupdatingLayerNorm(Baetal., Normareappliedduringinference,itmayleadtoa\n2016)intheimageencoder. sub-optimalsolution. Therefore,CLIPFitproposes\nWord\nEmbed\nPatch\nEmbed\nEmbeding\nLayer\nNorm\nLayer\nNorm\nLayer Linear\nAttn\nAttn\nEmbeding\nLayer\nNorm\nLayer\nNorm\nFFN\nFFN\nEmbeding\n‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\ntofine-tuneLayerNormintheimageencoder. Dataset CLIP CoOp CoCoOp Adapter KgCoOp MaPLe CLIPFit\nLossfunction. Previousworks(Yaoetal.,2023; Base 69.34 82.69 80.47 82.23 80.73 82.28 83.72\nAverage New 74.22 63.22 71.69 70.61 73.60 75.14 74.84\nXuhongetal.,2018)haveverifiedthatduringthe\nHM 71.70 71.66 75.83 75.98 77.00 78.55 79.03\nfine-tuningstage,genericpre-trainedknowledgeis Base 72.43 76.47 75.98 76.13 75.83 76.66 76.2\neasilyforgotten. Therefore,weexploretwodiffer- ImageNet New 68.14 67.88 70.43 67.17 69.96 70.54 70.17\nHM 70.22 71.92 73.10 71.37 72.78 73.47 73.06\nentstrategiesforalleviatingsuchforgetting. The\nBase 96.84 98.00 97.96 97.40 97.72 97.74 98.3\nfirstoneistousetheknowledgedistillation(Hinton Caltech101 New 94.00 89.81 93.81 93.23 93.70 94.36 93.7\nHM 95.40 93.73 95.84 95.51 96.03 96.02 95.94\netal.,2015;Yaoetal.,2023)losstoguideCLIP-\nBase 91.17 93.67 95.20 94.33 94.65 95.43 95.23\nFittolearnfromtheoriginalzero-shotCLIP.Let OxfordPets New 97.26 95.29 97.69 97.10 97.76 97.76 97.13\n{wclip}K and {w }K be the text features from HM 94.12 94.47 96.43 95.69 96.18 96.58 96.17\ni i=1 i i=1\nBase 63.37 78.12 70.49 76.10 71.76 72.94 78.80\noriginalCLIPandtextfeaturesfromCLIPFit. The Stanford\nNew 74.89 60.40 68.87 71.20 75.04 74.00 73.87\nCars\ntraining loss and KD loss of CLIPFit are defined HM 68.65 68.13 72.01 72.30 73.36 73.47 76.26\nby Base 72.08 97.60 94.87 97.23 95.00 95.92 96.83\nFlowers102 New 77.80 59.67 71.75 69.27 74.73 72.46 73.53\nL = L ce+Œ≤L kg, (3) HM 74.83 74.06 81.71 80.90 83.65 82.56 83.59\nBase 90.10 88.33 90.70 90.37 90.50 90.71 90.6\nL =\n1 (cid:88)K\ncos(wclip,w ), (4)\nFood101 N He Mw 9 91 0. .2 62\n6\n8 82 5. .2 16\n9\n9 91 0. .2 99\n9\n9 90 0.8 .63 9 91 1. .7 00\n9\n9 92 1. .0 35\n8\n9 91 0. .3 93\n6\nkg K i i\nBase 27.19 40.44 33.41 38.70 36.21 37.44 42.47\ni=1 FGVC\nNew 36.29 22.30 23.71 32.27 33.55 35.61 33.47\nAircraft\nwhereL isthecrossentropylossforclassification HM 31.09 28.75 27.74 35.19 34.83 36.50 37.43\nce\n(Zhouetal.,2022b,a)andŒ≤ isahyperparameter. Base 69.36 80.60 79.74 81.57 80.29 80.82 81.97\nSUN397 New 75.35 65.89 76.86 74.03 76.53 78.70 78.17\nThe second strategy is using the MSE loss in HM 72.23 72.51 78.27 77.62 78.36 79.75 80.02\nbiastermstopenalizechangesinthetextencoder. Base 53.24 79.44 77.01 79.53 77.55 80.36 81.97\nLet{bclip}L and{b }L betheunfixedtextbias DTD New 59.90 41.18 56.00 51.67 54.99 59.18 63.5\ni i=1 i i=1 HM 56.37 54.24 64.85 62.64 64.35 68.16 71.56\ntermsfrompre-trainedCLIPandunfixedtextbias\nBase 56.48 92.19 87.49 87.70 85.64 94.07 93.33\nterms from CLIPFit, where L is the number of EuroSAT New 64.05 54.74 60.04 58.83 64.34 73.23 71.07\nHM 60.03 68.69 71.21 70.42 73.48 82.35 80.69\nunfixedbiaslayers. TheMSElossisdefinedas\nBase 70.53 84.69 82.33 85.47 82.89 83.00 85.23\nUCF101 New 77.50 56.05 73.45 72.97 76.67 78.66 77.3\nL\nL = 1 (cid:88) ||bclip‚àíb ||2. (5) HM 73.85 67.46 77.64 78.73 79.65 80.77 81.07\nmse L i i\ni=1 Table1:AccuracycomparisononBase-to-newgeneralization\nWefoundthatbothstrategiescanalleviatethefor- ofCLIPFitwithpreviousmethods.Adapter:CLIP-Adapter.\ngettingproblemsandtheKDlossperformsbetter\n(asdiscussedinSec. 4.3),thusweadoptedtheKD WeimplementedourmethodwithPyTorch(Paszke\nlossasthefinalsolutionforCLIPFit. etal.,2019). Theexperimentswerebasedonthe\nvisionbackbonewithVit-B/16(Dosovitskiyetal.,\n4 Experiments\n2020). We followed CoOp to preprocess input\nimages. We used a single text prompt for all ex-\nIn this section, we show and discuss the experi-\nperiments for a fair comparison. We used SGD\nmentalresults. Toevaluatetheeffectivenessofour\noptimizer with batch size set as 32, and set the\nproposedmethod,weconductedextensiveexperi-\nlearning rate as 0.002 (Zhou et al., 2022b). All\nmentsandanalyseson11datasets.\nresultsreportedbelowaretheaverageofthreeruns\n4.1 ExperimentalSetup with different random seeds. The training epoch\nDatasets. FollowingCoOp, weconductedexten- wassetto100foralldatasetsexceptImageNetand\nFood101. Œ≤ was set to 8 for all datasets on the\nsiveexperimentson11publicclassificationbench-\nmark datasets to evaluate CLIPFit. The datasets base-to-newandcross-datasetsetting,and2forthe\nareImageNet(Dengetal.,2009),Caltech101(Fei- distributionshiftsetting. Forthefew-shotsetting,\nFeietal.,2004),OxfordPets(Parkhietal.,2012), wesetŒ≤ to2foralldatasetsexceptSUN397and\nStanfordCars (Krause et al., 2013), Flowers102 DTD.Moreimplementationdetailsareprovidedin\nappendixB.\n(NilsbackandZisserman,2008),Food101(Bossard\net al., 2014), FGVCAircraft (Maji et al., 2013), Comparisons. We compared our method\nSUN397(Xiaoetal.,2010),DTD(Cimpoietal., against state-of-the-art methods: zero-shot CLIP,\n2014),EuroSAT(Helberetal.,2019),andUCF101 prompt tuning methods: CoOp, CoCoOp (Zhou\n(Soomro et al., 2012). Implementation details. etal.,2022a),ProGrad(Zhuetal.,2023),KgCoOp\n(Yaoetal.,2023),MaPLe(Khattaketal.,2023)and Table2: ComparisonofCLIPFitandothermethodson\nthefew-shotlearningsetting. Wereportaverageaccu-\nadaptertuningmethods: CLIP-adapter,Tip-adapter\nracyon11datasetsforthe1/2/4/8/16-shotsetting.\n(Zhangetal.,2021). Detailedintroductionstothese\nmethodscanbefoundinappendixC. shot\nMethod\n1 2 4 8 16\nCoOP 68.09 70.13 73.59 76.45 79.01\n4.2 ComparisonswithState-of-the-arts\nCLIP-adapter 67.87 70.20 72.65 76.92 79.86\nResults on base-to-new generalization setting CoCoOp 66.95 67.63 71.98 72.92 75.02\nProGrad 68.2 71.78 74.21 77.93 79.2\nFollowingZhouetal.(2022b),wespliteachdate- KgCoOp 69.51 71.57 74.48 75.82 77.26\nsetintotwodisjointgroups: thebaseclassdataset Tip-adapter 70.62 73.08 75.75 78.51 81.15\nCLIPFit 72.32 74.39 77.18 79.03 81.27\nand the new class dataset. All compared meth-\nTable3: Comparisonofourmethodagainstothermeth-\nodsandtheproposedCLIPFitweretrainedonthe\nodsonrobustnesstodistributionshift.\nbaseclassdatasetandevaluatedonthenewclass\ndataset. We conducted 4/8/16-shot experiments, Souce Target\nMethod Average\nImageNet -V2 -Sketch\nfollowingYaoetal.(2023). Wereportedbaseand\nCLIP 66.73 60.83 46.15 57.90\nnewclassaccuracies(BaseandNew)andtheirhar- CoOP 71.51 64.20 47.99 61.23\nCLIP-adapter 71.60 63.67 46.52 60.60\nmonic mean accuracy (HM). The 16-shot results\nTip-adapter 73.10 64.82 46.73 61.55\nareshowninTable1,and4/8-shotresultsarepro- CoCoOp 71.02 64.07 48.75 61.28\nProGrad 72.24 64.73 47.61 61.53\nvidedinappendixG.AsshowninTable1,CLIPFit\nKgCoOp 71.20 64.10 48.97 61.42\nachieves6bestHMaccuraciesamong11datasets CLIPFit 71.53 64.83 48.87 61.74\nandthebestaverageHMaccuracy,whichdemon-\nstratesthatCLIPFitcannotonlylearnwellonseen outperforms other methods by a large margin in\nbaseclassdatabutalsocangeneralizewelltodata 1/2/4-shot settings, demonstrating CLIPFit‚Äôs ro-\nfromunseennewclasses. Anotableissuewithpre- bustabilitytolearnwithextremelyfewsamples.\nviousmethodslikeCoOp,CoCoOp,ProGrad,and Resultsontherobustnesstodistributionshift\nKgCoOpisthattheyusuallyperformwellonlyon setting. FollowingZhangetal.(2021),weevalu-\neitherthebaseornewclass. Toalleviatethisissue, atedtherobustnessunderdistributionshiftofCLIP-\nMaPLe proposes a multi-modal prompt learning Fitandothermethodsbyfirsttrainingmodelson\nstrategyforCLIPtuningwhichimprovesalotover the16-shotImageNetdatasetandthenevaluating\nHM compared to previous methods. Compared onImageNet-V2(Rechtetal.,2019)andImageNet-\ntoMaPLe,ourCLIPFitachievesbetterHMaccu- Sketch(Wangetal.,2019). Thelabelsetsoftwo\nracy and average performance on the base class, evaluating datasets are subsets of the label set of\nwith slightly lower average performance on the ImageNet. Although the label sets are compati-\nnew class. It is important to note that CLIPFit ble, the distributions of these three datasets are\nonly needs to tune nearly 46K parameters while differentfromeachother. Theresultsareshownin\nMaPLeneedstotunenearly3.55Mparametersfor Table 3. As shown in Table 3, while TIP-adapter\neach task, which is 77 times more than CLIPFit, achieves the best performance on the ImageNet\nmeaningthatCLIPFitfine-tunessignificantlyfewer dataset,CLIPFitcanachievebetteraverageperfor-\nparametersandismuchmoreefficient. mancecomparedtoexistingmethods,effectively\nResults on few-shot learning setting. To ver- underliningtherobustnessofCLIPFit.\nify whether the proposed CLIPFit can learn task- Results on cross-dataset transfer setting is pro-\nspecific knowledge, we also compared CLIPFit videdinappendixD.\nwithotherexistingmethodsonthefew-shotlearn-\n4.3 Fine-tuningAnalysis\ningsetting. FollowingZhouetal.(2022a),weused\n1,2,4,8,and16-shotsetsfortrainingandreported Analyzingparameterchange. Tounderstandthe\naccuracyperformance. Wereporttheresultsofthe black-boxfine-tuningprocessinCLIPFit,wefirst\naverage accuracy of 11 datasets in Table. 2, and analyzed changes in the parameters of both the\nreportallresultsoneachdatasetinappendixF.As text encoder and image encoder. We computed\nshown in Table 2, compared with other methods, thesquareddifference||ppre‚àíp||2 foreachlayer,\nCLIPFit shows overall consistent improvements whereppre isthepre-trainedparametervectorand\namongall1/2/4/8/16-shotsettings. Thisdemon- pisthefine-tunedparametervector. Weconduct\nstrates that CLIPFit can successfully learn task- experiments on the DTD dataset. The results are\nspecificknowledge. ItisworthnotingthatCLIPFit showninFig. 3. AsobservedFig. 3(a), forbias\n(a) Text encoder (b) Image encoder (a) Gradient sum (b) Change w/regulariza2on loss\nFigure3: Visualizationofchangesindifferentlayers. Figure4: Left: visualizationofsquaredgradientsum.\nRight: visualizationofchangew/regularizationloss.\nterms in the FNN of the text encoder, when the\nnumberoflayersincreases,thechangeinbiasde-\ncreases,whichimpliesthatlow-levelfeaturesinthe\ntextencoderchangemorethanhigh-levelfeatures\nduring the fine-tuning process of CLIPFit. From\nFig. 3(b),wefoundthatforLayerNormintheim-\nageencoder,thefirstlayer(i.e.,patchembedding\nlayer) changes much more compared with other\nlayersforbothbiasandgain,showingthattuning (a) zero-shot CLIP (b) CLIPFit\npatchembeddingLayerNormiscrucialforshifted\nFigure5: Visualizationoflearnedimagefeaturespace\ndownstreamtasks. Moreover, thegainofthelast\nfromzero-shotCLIPandCLIPFitviat-SNE.\nseveralLayerNormlayershasmuchchangedand\nthemostintermediatelayerschangemuchless. The\nracy,andfine-tuningw/KDlossleadstoa76.23%\ndifferenceinchangebetweendifferentlayersmay\naverageaccuracy. Bothtwoperformancesarebet-\nbecausedbygradientdifference. Wevisualizethe\nterthanfine-tuningw/oregularizationloss(76.13%\nsquaredsumofgradientfromeachtextbiaslayer\naverageaccuracy)andKDlossperformsbetter. We\ninFig. 4(a). Asobserved,thecurveofthegradient\nthenanalyzehowthesetwolossesaffectchanges\nsumisverysimilartochangesinparameters.\ninparametersduringfine-tuningofCLIPFit. The\nToverifywhethermorechangedlayersaremore\nresultsareshowninFig. 4(b). Asobserved,KD\nimportant in fine-tuning, we conducted experi-\nlosswillreducethechangesforthemore-changed\nments by freezing less (or more) changed Layer-\nlow-levelbiastermsandenhancechangesinless-\nNormlayersonthe4-shotsetting. Wefoundthat\nchanged high-level layers, which implies that pe-\nwhenonlyupdatingthefirstLayerNormlayerand\nnalizingchangesforlow-levelbiastermsisimpor-\nfreezing other LayerNorm layers, the average ac-\ntant in avoiding overfitting. Compared with KD\ncuracy is 76.22%. For comparison, the average\nloss,MSElossdirectlyapplyingtotextbiasterms\naccuracyis74.93%whenonlyupdatingthetwelfth\nreducesmorechangesinlow-levellayers.\nLayerNorm, and 75.06% when updating the last\nLayerNorm. Botharemuchlessthanthefirstlayer Analysingimageencoderrepresentations. We\nandthesetwolayerschangemuchlessthanthefirst usedt-SNE(VanderMaatenandHinton,2008)to\nlayer,asshowninFig. 3(b). Moreover,whenup- visualize the image representation space of zero-\ndatingthetop6mostchangedLayerNormlayers, shotCLIPandCLIPFittoanalyzeimageencoder\nthe average accuracy is 77.03%, which is only a representations. We visualize the data from Eu-\n0.15%drop,whileonlytuning23%parametersof roSAT dataset. The visualization results are pre-\nCLIPFit. Thephenomenonfortextencoderissimi- sentedinFig. 5. Asobserved,inhigh-dimensional\nlarandcanbefoundinappendixH.Theseresults classification feature space, CLIPFit has a much\ndemonstratethatthemorechangedlayersaremore clearer separation of different class image fea-\nimportantforknowledgeadapting. turescomparedwithzero-shotCLIP,whichdemon-\nAnalyzing regularization loss. We then ana- strates that CLIPFit can better detect the similar-\nlyze the two regularization losses: KD loss and ities among images. These results verify that up-\nbiastermMSEloss. Wefoundthatbothtwolosses dating LayerNorm in the image encoder during\ncan avoid overfitting and boost performance dur- fine-tuningwillleadtoamoreseparatedandbetter\ningfine-tuning. Forthe4-shotlearningtask,fine- similarity-detectedimagefeaturespace.\ntuningw/KDlossleadstoa77.18%averageaccu- UpdatingLayerNormcanalsobenefitother\nTable4: Comparisonofprompttuningandadaptertun- Accuracy(%)\nConfigurations\n1-shot 4-shot 16-shot\ningmethodsw/andw/oupdatingLayerNormonthe16-\nPorjectionBias 68.86 74.72 79.53\nshotbase-to-newsetting. +ULmeanstrainingupdating w/LayerNorm 70.75 76.13 81.04\nLayerNorm. w/KDloss 71.21 75.94 79.62\nw/KDloss+LayerNorm 72.32 77.18 81.27\nMethod Base New H\nCoOp 82.63 67.99 74.60 Table6: AblationstudyofCLIPFitonthefew-shotset-\n+UL 82.96(+0.33) 69.09(+1.10) 75.39(+0.79) tingwith1/4/16-shot. ProjectionBias:fine-tuningbias\nKgCoOp 80.73 73.60 77.00\ntermsofprojectionlayerinFNNsofthetextencoder.\n+UL 82.13(+1.40) 74.96(+1.36) 78.38(+1.38)\nCLIP-adapter 82.23 70.61 75.98 LayerNorm:updatingLayerNormintheimageencoder.\n+UL 83.63(+1.40) 71.87(+1.26) 77.31(+1.33)\nTable 5: Comparison of different strategies of fine-\nMethod Params time Base New HM\ntuningbiastermsinCLIP.\nCoOp 2048 0.44ms 82.69 63.22 71.66\nCoCoOp 35k 25.59ms 80.47 71.69 75.83\nStrategy Base New H #param. KgCoOp 2048 0.44ms 80.73 73.60 77.00\n(a)Text+Imagebias 84.15 64.35 72.93 0.17M MaPLe 3.55M 2.1ds 82.28 75.14 78.55\n(b)Textbias 83.33 64.43 72.67 67.6K CLIPFit 44k 0.96ms 83.72 74.84 79.03\n(c)FFNsbias(Text) 83.25 67.60 74.61 30.7K\n(d)Projectionbias(Text) 83.23 67.58 74.59 6.1K\nTable7: Comparisonoftrainingefficiencywithother\nmethodsover11datasets.\nmethods. WeshowthatupdatingLayerNormcan\nalsobenefitprompttuningmethodsandadaptertun-\ningmethods. Were-implementedCoOp,KgCoOp,\nstrategy(d)canhavesimilarperformanceamong\nandCLIP-adapterwithupdatingLayerNorm. The\nboththebaseandnewclassdata,butstrategy(d)\nresults are shown in Table 4. Table 4 shows that\nfine-tunesonlyone-fifthofparameterscompared\ntraining with LayerNorm updating can boost the\nwithstrategy(c),whichspeedsuptraining.\nbaseclass,newclass,andharmonicmeanperfor-\nmance for all three methods. For example, train- Effectiveness of proposed components. We\ningKgCoOpwithupdatingLayerNormcanbring validated the effects of updating LayerNorm and\n1.4%,1.36%,and1.38%improvementsinthebase KDlossbyablatingthem. Theresultsareshown\nclass, new class, and Harmonic Mean (HM) ac- in Table 6. Fine-tuning bias terms with KD loss\ncuracy,whichdemonstratestheeffectivenessand brings 2.35%, 1.22%, and 0.09% improvements\nwidevalidityoftheproposedupdatingLayerNorm. for1/4/16-shotsetting,respectively. Fine-tuning\nMoredetailedanalysesaboutotherdatasetsand bias terms in the text encoder and LayerNorm\notheraspectsareprovidedinappendixH. in the image encoder brings 1.89%, 1.41%, and\n1.51% improvements for 1/4/16-shot setting, re-\n4.4 AblationStudy spectively. Together,CLIPFitbrings3.46%,2.46%\nand1.74%improvementsfor1/4/16-shotsetting,\nComparisonofdifferentstrategiesoffine-tuning\nrespectively. Theseresultsdemonstratetheeffec-\nbias terms. We give an in-depth exploration of\ntivenessofeachCLIPFitcomponents.\nhow to apply BitFit to fine-tune the CLIP model.\nOriginalBitFitfine-tunesallbiastermsinlanguage Training efficiency. We compare the training\nmodels. We conduct 4 strategies for fine-tuning efficiencyofCLIPFitandothermethodsw.r.t. pa-\nbias terms of CLIP: (a) fine-tuning all bias terms rameters and training time per image (Yao et al.,\nof the text and image encoder; (b) fine-tuning all 2023). The results are shown in Table 7. It is\nbiastermsofthetextencoder;(c)fine-tuningbias noticed that CoOp and KgCoOp have the lowest\ntermsofFFNsofthetextencoder;(d)fine-tuning numberoftrainingparametersandtime. However,\nbias terms of projection linear layers in FFNs of the performance of these two methods is not sat-\nthetextencoder. Wetrainedthesefourstrategies isfactory. MaPLeimprovesaccuracyperformance\non the 16-shot base-to-new setting with L and compared with other methods but also increases\nce\nreportedaverageaccuracy. Theresultsareshown therequiredtuningparametersto3.55M,whichis\ninTable5. AsshowninTable5,bothstrategy(a) very time-consuming. CLIPFit achieves the best\nand strategy (b) can boost seen base class perfor- harmonicmeanaccuracywithonly44kparameters,\nmancebutwilldecreasesignificantlyunseennew whichismuchlessthanMaPLe. Also,thetraining\nclassperformance,whichimpliesthatdirectlyap- timeofCLIPFitisslightlyhigherthanCoOpand\nplyingBitFittoCLIPmaybeharmfultomodel‚Äôs KgCoOp. GiventhelargeimprovementofCLIPFit,\ngeneralizationability. Moreover,strategy(c)and aslightincreaseintrainingtimeisacceptable.\n4.5 Discussion Khattak et al., 2023), this paper focused on im-\nageclassificationforVLMs,soourstudywascon-\nAlthoughourmethodisdesignedforcontrastiveen-\nstrainedtoclassificationtasks. ExpandingCLIPFit\ncoderVLMs(CLIP),thecoreideaofCLIPFitand\nfor VLM fine-tuning to a broader range of tasks\nour model analysis may still provide insights for\n(e.g.,imageretrieval)couldbethefuturework.\nother large multimodal model (e.g., LLaVA (Liu\net al., 2024)) fine-tuning and many furthur appli-\nAcknowledgement\ncations(Touvronetal.,2023;Mizrahietal.,2023;\nTeam et al., 2024a,b; Li et al., 2024b,a, 2023a; MSwaspartiallysupportedbyInstituteforAIand\nZhang et al., 2022; Chen et al., 2023b; Li et al., Beyond,UTokyo.\n2023b). For example, the idea of tuning Layer-\nNorm could be used when distributions of down-\nReferences\nstream and pretraining QA image data are diver-\ngent,andparameterchangeandimportanceanal- JimmyLeiBa,JamieRyanKiros,andGeoffreyEHin-\nysis (Sec. 4.3) could provide insights for how to ton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nselectfine-tuningparameters. Wehopeourwork\ncanprovideinsightsforabroaderrangeofVLM Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.\nfine-tuning. 2019. Simple,scalableadaptationforneuralmachine\ntranslation. arXivpreprintarXiv:1909.08478.\n5 Conclusion\nLukasBossard,MatthieuGuillaumin,andLucVanGool.\nInthispaper,wepresentedCLIPFitforfine-tuning 2014. Food-101‚Äìminingdiscriminativecomponents\nwith random forests. In Computer Vision‚ÄìECCV\nvisual-language models. Unlike existing prompt\n2014: 13thEuropeanConference, Zurich, Switzer-\ntuningandadaptertuningmethods,CLIPFitdoes land,September6-12,2014,Proceedings,PartVI13,\nnot introduce any external parameters and fine- pages446‚Äì461.Springer.\ntunes CLIP by updating only bias terms of pro-\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\njectionlayersinFFNsofthetextencoderandthe\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\nimage encoder‚Äôs LayerNorm. To understand the Neelakantan,PranavShyam,GirishSastry,Amanda\neffect of CLIPFit fine-tuning on the pre-trained Askell,etal.2020. Languagemodelsarefew-shot\nlearners. Advancesinneuralinformationprocessing\nmodel, we conducted various analyses focusing\nsystems,33:1877‚Äì1901.\nonchangesininternalparametersandrepresenta-\ntions. We conducted extensive experiments and GuangyiChen,WeiranYao,XiangchenSong,Xinyue\nanalysistoevaluateCLIPFiton11datasets,whose Li,YongmingRao,andKunZhang.2022. Prompt\nlearningwithoptimaltransportforvision-language\nperformancesshowthesuperiorityofourmethod.\nmodels. arXivpreprintarXiv:2210.01253.\n6 Limitations\nGuanhua Chen, Lu Hou, Yun Chen, Wenliang Dai,\nLifengShang,XinJiang,QunLiu,JiaPan,andWen-\nInthispaper,wepresentedCLIPFitforVLMfine-\npingWang.2023a. mCLIP:MultilingualCLIPvia\ntuningandconductedanexplorationofhowCLIP- cross-lingualtransfer. InProceedingsofthe61stAn-\nFitaffectsthepre-trainedCLIPmodel. Ouranaly- nualMeetingoftheAssociationforComputational\nLinguistics(Volume1: LongPapers),pages13028‚Äì\nsesfoundsome interestingphenomenaafterfine-\n13043,Toronto,Canada.AssociationforComputa-\ntuning,i.e.,low-levelbiastermsinthetextencoder\ntionalLinguistics.\nchangemuchmorethanhigh-levelbiastermsand\nthe change in the first LayerNorm layer is much Hong-You Chen, Jike Zhong, Mingda Zhang, Xuhui\nJia, Hang Qi, Boqing Gong, Wei-Lun Chao, and\nbiggerthanotherLayerNormlayersintheimage\nLi Zhang. 2023b. Federated learning of shareable\nencoders. Moreover, we found that this may be\nbases for personalization-friendly image classifica-\ncaused by the difference in the magnitude of the tion. Preprint,arXiv:2304.07882.\ngradient. Nevertheless,ouranalysisdoesnotreveal\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nwhy the difference in the magnitude of the gradi-\nSammyMohamed,andAndreaVedaldi.2014. De-\nenthappensduringfine-tuning. Adeeperanalysis scribingtexturesinthewild. InProceedingsofthe\nofgradientback-propagationduringfine-tuningis IEEE conference on computer vision and pattern\nrecognition,pages3606‚Äì3613.\nneededtounderstandthisforfuturework.\nFurthermore, following previous works (Zhou\nWietseDeVries,AndreasVanCranenburgh,andMalv-\netal.,2022a;Yaoetal.,2023;Zhouetal.,2022b; ina Nissim. 2020. What‚Äôs so special about bert‚Äôs\nlayers? a closer look at the nlp pipeline in mono- vision-language representation learning with noisy\nlingual and multilingual models. arXiv preprint textsupervision. InInternationalconferenceonma-\narXiv:2004.06499. chinelearning,pages4904‚Äì4916.PMLR.\nJiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi, MuhammadUzairKhattak,HanoonaRasheed,Muham-\nandLiFei-Fei.2009. Imagenet: Alarge-scalehier- madMaaz,SalmanKhan,andFahadShahbazKhan.\narchicalimagedatabase. In2009IEEEconference 2023. Maple: Multi-modalpromptlearning. InPro-\noncomputervisionandpatternrecognition, pages ceedingsoftheIEEE/CVFConferenceonComputer\n248‚Äì255.Ieee. VisionandPatternRecognition,pages19113‚Äì19122.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and JonathanKrause,MichaelStark,JiaDeng,andLiFei-\nKristinaToutanova.2018. Bert: Pre-trainingofdeep Fei.2013. 3dobjectrepresentationsforfine-grained\nbidirectionaltransformersforlanguageunderstand- categorization. In Proceedings of the IEEE inter-\ning. arXivpreprintarXiv:1810.04805. nationalconferenceoncomputervisionworkshops,\npages554‚Äì561.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, BrianLester,RamiAl-Rfou,andNoahConstant.2021.\nThomas Unterthiner, Mostafa Dehghani, Matthias The power of scale for parameter-efficient prompt\nMinderer,GeorgHeigold,SylvainGelly,etal.2020. tuning. arXivpreprintarXiv:2104.08691.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint LiLi,YouQin,WeiJi,YuxiaoZhou,andRogerZim-\narXiv:2010.11929. mermann.2024a. Domain-wiseinvariantlearningfor\npanopticscenegraphgeneration. InICASSP2024\nLiFei-Fei,RobFergus,andPietroPerona.2004. Learn-\n-2024IEEEInternationalConferenceonAcoustics,\ning generative visual models from few training ex-\nSpeechandSignalProcessing(ICASSP),pages3165‚Äì\namples: Anincrementalbayesianapproachtestedon\n3169.\n101objectcategories. In2004conferenceoncom-\nputervisionandpatternrecognitionworkshop,pages\nLi Li, Chenwei Wang, You Qin, Wei Ji, and Renjie\n178‚Äì178.IEEE.\nLiang.2023a. Biased-predicateannotationidentifica-\ntionviaunbiasedvisualpredicaterepresentation. In\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nProceedingsofthe31stACMInternationalConfer-\nRongyaoFang,YongfengZhang,HongshengLi,and\nenceonMultimedia,MM‚Äô23,page4410‚Äì4420.\nYuQiao.2023. Clip-adapter: Bettervision-language\nmodelswithfeatureadapters. InternationalJournal\nMing Li, Qingli Li, and Yan Wang. 2023b. Class\nofComputerVision,pages1‚Äì15.\nbalanced adaptive pseudo labeling for federated\nsemi-supervised learning. In Proceedings of the\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nIEEE/CVFConferenceonComputerVisionandPat-\nMakingpre-trainedlanguagemodelsbetterfew-shot\nternRecognition(CVPR),pages16292‚Äì16301.\nlearners. arXivpreprintarXiv:2012.15723.\nShawn Li, Huixian Gong, Hao Dong, Tiankai Yang,\nAngeliki Giannou, Shashank Rajput, and Dimitris\nZhengzhong Tu, and Yue Zhao. 2024b. Dpu: Dy-\nPapailiopoulos. 2023. The expressive power of\nnamic prototype updating for multimodal out-of-\ntuning only the normalization layers. Preprint,\ndistributiondetection. Preprint,arXiv:2411.08227.\narXiv:2302.07937.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, HaotianLiu,ChunyuanLi,QingyangWu,andYongJae\nandDamianBorth.2019. Eurosat: Anoveldataset Lee.2024. Visualinstructiontuning. Advancesin\nanddeeplearningbenchmarkforlanduseandland\nneuralinformationprocessingsystems,36.\ncoverclassification. IEEEJournalofSelectedTopics\nPengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,\ninAppliedEarthObservationsandRemoteSensing,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\n12(7):2217‚Äì2226.\ntrain, prompt, and predict: A systematic survey of\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. promptingmethodsinnaturallanguageprocessing.\nDistillingtheknowledgeinaneuralnetwork. arXiv ACMComputingSurveys,55(9):1‚Äì35.\npreprintarXiv:1503.02531.\nXiaoLiu,KaixuanJi,YichengFu,WengTam,Zhengx-\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, iaoDu,ZhilinYang,andJieTang.2022. P-tuning:\nBruna Morrone, Quentin De Laroussilhe, Andrea Prompt tuning can be comparable to fine-tuning\nGesmundo,MonaAttariyan,andSylvainGelly.2019. acrossscalesandtasks. InProceedingsofthe60th\nParameter-efficienttransferlearningfornlp. InIn- AnnualMeetingoftheAssociationforComputational\nternationalConferenceonMachineLearning,pages Linguistics(Volume2: ShortPapers),pages61‚Äì68.\n2790‚Äì2799.PMLR.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nChaoJia,YinfeiYang,YeXia,Yi-TingChen,Zarana Blaschko,andAndreaVedaldi.2013. Fine-grained\nParekh,HieuPham,QuocLe,Yun-HsuanSung,Zhen visual classification of aircraft. arXiv preprint\nLi, and Tom Duerig. 2021. Scaling up visual and arXiv:1306.5151.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, Sheng Shen, Liunian Harold Li, Hao Tan, Mohit\nand Ian Tenney. 2020. What happens to bert Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nembeddings during fine-tuning? arXiv preprint Yao, and Kurt Keutzer. 2021. How much can clip\narXiv:2004.14448. benefit vision-and-language tasks? arXiv preprint\narXiv:2107.06383.\nDavidMizrahi,RomanBachmann,OgÀòuzhanFatihKar,\nTeresaYeo,MingfeiGao,AfshinDehghan,andAmir HaoyuSong,LiDong,Wei-NanZhang,TingLiu,and\nZamir. 2023. 4m: Massively multimodal masked FuruWei.2022. Clipmodelsarefew-shotlearners:\nmodeling. Preprint,arXiv:2312.06647. Empiricalstudiesonvqaandvisualentailment. arXiv\npreprintarXiv:2203.07190.\nMarius Mosbach, Anna Khokhlova, Michael A Hed-\nderich, and Dietrich Klakow. 2020. On the inter-\nKhurramSoomro,AmirRoshanZamir,andMubarak\nplaybetweenfine-tuningandsentence-levelprobing\nShah. 2012. Ucf101: A dataset of 101 human ac-\nforlinguisticknowledgeinpre-trainedtransformers.\ntionsclassesfromvideosinthewild. arXivpreprint\narXivpreprintarXiv:2010.02616.\narXiv:1212.0402.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomatedflowerclassificationoveralargenumber GeminiTeam,RohanAnil,SebastianBorgeaud,Jean-\nofclasses. In2008SixthIndianconferenceoncom- Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nputer vision, graphics & image processing, pages Schalkwyk, Andrew M. Dai, Anja Hauth, Katie\n722‚Äì729.IEEE. Millican, David Silver, Melvin Johnson, Ioannis\nAntonoglou, Julian Schrittwieser, Amelia Glaese,\nOmkarMParkhi,AndreaVedaldi,AndrewZisserman, Jilin Chen, Emily Pitler, Timothy Lillicrap, Ange-\nand CV Jawahar. 2012. Cats and dogs. In 2012 likiLazaridou,OrhanFirat,JamesMolloy,Michael\nIEEE conference on computer vision and pattern Isard, Paul R. Barham, Tom Hennigan, Benjamin\nrecognition,pages3498‚Äì3505.IEEE. Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong\nXu,RyanDoherty,EliCollins,ClemensMeyer,Eliza\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nRutherford,EricaMoreira,KareemAyoub,Megha\nLerer, James Bradbury, Gregory Chanan, Trevor\nGoel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nTzeCheng,EricNi,PurviShah,PatrickKane,Betty\nAntiga, et al. 2019. Pytorch: An imperative style,\nChan,ManaalFaruqui,AliakseiSeveryn,Hanzhao\nhigh-performancedeeplearninglibrary. Advancesin\nLin, YaGuang Li, Yong Cheng, Abe Ittycheriah,\nneuralinformationprocessingsystems,32.\nMahdisMahdieh,MiaChen,PeiSun,DustinTran,\nSumit Bagri, Balaji Lakshminarayanan, Jeremiah\nJonasPfeiffer,IvanVulic¬¥,IrynaGurevych,andSebas-\nLiu,AndrasOrban,FabianG√ºra,HaoZhou,Xiny-\ntianRuder.2020. Mad-x: Anadapter-basedframe-\ningSong,AurelienBoffy,HarishGanapathy,Steven\nwork for multi-task cross-lingual transfer. arXiv\nZheng,HyunJeongChoe,√ÅgostonWeisz,TaoZhu,\npreprintarXiv:2005.00052.\nYifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej\nWang Qi, Yu-Ping Ruan, Yuan Zuo, and Taihao Li. Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa,\n2022. Parameter-efficient tuning on layer normal- MajdAlMerey,MartinBaeuml,ZhifengChen,Lau-\nization for pre-trained language models. Preprint, rent El Shafey, Yujing Zhang, Olcan Sercinoglu,\narXiv:2211.08682. George Tucker, Enrique Piqueras, Maxim Krikun,\nIain Barr, Nikolay Savinov, Ivo Danihelka, Becca\nAlecRadford,JongWookKim,ChrisHallacy,Aditya Roelofs,Ana√ØsWhite,AndersAndreassen,Tamara\nRamesh,GabrielGoh,SandhiniAgarwal,GirishSas- vonGlehn,LakshmanYagati,MehranKazemi,Lu-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, cas Gonzalez, Misha Khalman, Jakub Sygnowski,\netal.2021. Learningtransferablevisualmodelsfrom Alexandre Frechette, Charlotte Smith, Laura Culp,\nnaturallanguagesupervision. InInternationalconfer- LevProleev,YiLuan,XiChen,JamesLottes,Nathan\nenceonmachinelearning,pages8748‚Äì8763.PMLR. Schucher, Federico Lebron, Alban Rrustemi, Na-\ntalieClay,PhilCrone,TomasKocisky,JeffreyZhao,\nAlecRadford,KarthikNarasimhan,TimSalimans,Ilya\nBartek Perz, Dian Yu, Heidi Howard, Adam Blo-\nSutskever, et al. 2018. Improving language under-\nniarz, Jack W. Rae, Han Lu, Laurent Sifre, Mar-\nstandingbygenerativepre-training.\ncelloMaggioni,FredAlcober,DanGarrette,Megan\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Barnes, Shantanu Thakoor, Jacob Austin, Gabriel\nandVaishaalShankar.2019. Doimagenetclassifiers Barth-Maron,WilliamWong,RishabhJoshi,Rahma\ngeneralizetoimagenet? InInternationalconference Chaabouni,DeeniFatiha,ArunAhuja,GauravSingh\nonmachinelearning,pages5389‚Äì5400.PMLR. Tomar, Evan Senter, Martin Chadwick, Ilya Kor-\nnakov, Nithya Attaluri, I√±aki Iturrate, Ruibo Liu,\nChristoph Schuhmann, Romain Beaumont, Richard YunxuanLi, SarahCogan, JeremyChen, ChaoJia,\nVencu,CadeGordon,RossWightman,MehdiCherti, ChenjieGu,QiaoZhang,JordanGrimstad,AleJakse\nTheo Coombes, Aarush Katta, Clayton Mullis, Hartman, Xavier Garcia, Thanumalayan Sankara-\nMitchellWortsman,etal.2022. Laion-5b: Anopen narayanaPillai,JacobDevlin,MichaelLaskin,Diego\nlarge-scaledatasetfortrainingnextgenerationimage- de Las Casas, Dasha Valter, Connie Tao, Lorenzo\ntext models. Advances in Neural Information Pro- Blanco,Adri√†Puigdom√®nechBadia,DavidReitter,\ncessingSystems,35:25278‚Äì25294. MiannaChen,JennyBrennan,ClaraRivera,Sergey\nBrin,ShariqIqbal,GabrielaSurita,JaneLabanowski, wei Xing, Christina Greer, Helen Miller, Shereen\nAbhiRao,StephanieWinkler,EmilioParisotto,Yim- Ashraf,AurkoRoy,ZizhaoZhang,AdaMa,Ange-\ning Gu, Kate Olszewska, Ravi Addanki, Antoine losFilos,MilosBesta,RoryBlevins,TedKlimenko,\nMiech,AnnieLouis,DenisTeplyashin,GeoffBrown, Chih-KuanYeh,SoravitChangpinyo,JiaqiMu,Os-\nElliotCatt,JanBalaguer,JackieXiang,PidongWang, car Chang, Mantas Pajarskas, Carrie Muir, Vered\nZoeAshwood,AntonBriukhov,AlbertWebson,San- Cohen,CharlineLeLan,KrishnaHaridasan,Amit\njayGanapathy,SmitSanghavi,AjayKannan,Ming- Marathe, Steven Hansen, Sholto Douglas, Rajku-\nWei Chang, Axel Stjerngren, Josip Djolonga, Yut- marSamuel,MingqiuWang,SophiaAustin,Chang\ningSun,AnkurBapna,MatthewAitchison,Pedram Lan,JiepuJiang,JustinChiu,JaimeAlonsoLorenzo,\nPejman, Henryk Michalewski, Tianhe Yu, Cindy Lars Lowe Sj√∂sund, S√©bastien Cevey, Zach Gle-\nWang,JulietteLove,JunwhanAhn,DawnBloxwich, icher,ThiAvrahami,AnudhyanBoral,HansaSrini-\nKehang Han, Peter Humphreys, Thibault Sellam, vasan,VittorioSelo,RhysMay,KonstantinosAiso-\nJamesBradbury,VarunGodbole,SinaSamangooei, pos,L√©onardHussenot,LivioBaldiniSoares,Kate\nBogdan Damoc, Alex Kaskasoli, S√©bastien M. R. Baumli, Michael B. Chang, Adri√† Recasens, Ben\nArnold,VijayVasudevan,ShubhamAgrawal,Jason Caine,AlexanderPritzel,FilipPavetic,FabioPardo,\nRiesa, Dmitry Lepikhin, Richard Tanburn, Srivat- Anita Gergely, Justin Frye, Vinay Ramasesh, Dan\nsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Horgan, Kartikeya Badola, Nora Kassner, Subhra-\nPranavShyam,JohanFerret,StevenHand,Ankush jitRoy,EthanDyer,V√≠ctorCamposCampos,Alex\nGarg, Tom Le Paine, Jian Li, Yujia Li, Minh Gi- Tomala, Yunhao Tang, Dalia El Badawy, Elspeth\nang, Alexander Neitz, Zaheer Abbas, Sarah York, White, Basil Mustafa, Oran Lang, Abhishek Jin-\nMachelReid,ElizabethCole,AakankshaChowdh- dal, Sharad Vikram, Zhitao Gong, Sergi Caelles,\nery, Dipanjan Das, Dominika Rogozin¬¥ska, Vitaliy RossHemsley,GregoryThornton,FangxiaoyuFeng,\nNikolaev,PabloSprechmann,ZacharyNado,Lukas Wojciech Stokowiec, Ce Zheng, Phoebe Thacker,\nZilka, Flavien Prost, Luheng He, Marianne Mon- √áagÀòlar √únl√º, Zhishuai Zhang, Mohammad Saleh,\nteiro, Gaurav Mishra, Chris Welty, Josh Newlan, JamesSvensson,MaxBileschi,PiyushPatil,Ankesh\nDawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Anand,RomanRing,KaterinaTsihlas,ArpiVezer,\nRaouldeLiedekerke,JustinGilmer,CarlSaroufim, MarcoSelvi,TobyShevlane,MikelRodriguez,Tom\nShruti Rijhwani, Shaobo Hou, Disha Shrivastava, Kwiatkowski, Samira Daruki, Keran Rong, Allan\nAnirudh Baddepudi, Alex Goldin, Adnan Ozturel, Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg,\nAlbin Cassirer, Yunhan Xu, Daniel Sohn, Deven- Mina Khan, Lisa Anne Hendricks, Marie Pellat,\ndra Sachan, Reinald Kim Amplayo, Craig Swan- VladimirFeinberg,JamesCobon-Kerr,TaraSainath,\nson,DessiePetrova,ShashiNarayan,ArthurGuez, MaribethRauh,SayedHadiHashemi,RichardIves,\nSiddhartha Brahma, Jessica Landon, Miteyan Pa- YanaHasson,EricNoland,YuanCao,NathanByrd,\ntel, RuizheZhao, KevinVillela, LuyuWang, Wen- LeHou,QingzeWang,ThibaultSottiaux,Michela\nhaoJia,MatthewRahtz,MaiGim√©nez,LeggYeung, Paganini, Jean-Baptiste Lespiau, Alexandre Mou-\nJamesKeeling,PetkoGeorgiev,DianaMincu,Boxi farek,SamerHassan,KaushikShivakumar,Joostvan\nWu, Salem Haykal, Rachel Saputro, Kiran Vodra- Amersfoort,AmolMandhane,PratikJoshi,Anirudh\nhalli,JamesQin,ZeynepCankara,AbhanshuSharma, Goyal,MatthewTung,AndrewBrock,HannahShea-\nNickFernando,WillHawkins,BehnamNeyshabur, han, Vedant Misra, Cheng Li, Nemanja Rakic¬¥evic¬¥,\nSolomon Kim, Adrian Hutter, Priyanka Agrawal, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Jun-\nAlex Castro-Ros, George van den Driessche, Tao hyuk Oh, Seb Noury, Eren Sezener, Fantine Huot,\nWang, FanYang, ShuoyiinChang, PaulKomarek, MatthewLamm,NicolaDeCao,CharlieChen,Sid-\nRossMcIlroy,MarioLucÀáic¬¥,GuodongZhang,Wael harth Mudgal, Romina Stella, Kevin Brooks, Gau-\nFarhan,MichaelSharman,PaulNatsev,PaulMichel, tamVasudevan,ChenxiLiu,MainakChain,Nivedita\nYaminiBansal,SiyuanQiao,KrisCao,SiamakShak- Melinkeri,AaronCohen,VenusWang,KristieSey-\neri,ChristinaButterfield,JustinChung,PaulKishan more, Sergey Zubkov, Rahul Goel, Summer Yue,\nRubenstein,ShivaniAgrawal,ArthurMensch,Kedar Sai Krishnakumaran, Brian Albert, Nate Hurley,\nSoparkar,KarelLenc,TimothyChung,AedanPope, Motoki Sano, Anhad Mohananey, Jonah Joughin,\nLoren Maggiore, Jackie Kay, Priya Jhakra, Shibo EgorFilonov,TomaszKeÀõpa,YomnaEldawy,Jiaw-\nWang,JoshuaMaynez,MaryPhuong,TaylorTobin, ernLim,RahulRishi,ShirinBadiezadegan,Taylor\nAndrea Tacchetti, Maja Trebacz, Kevin Robinson, Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara\nYashKatariya,SebastianRiedel,PaigeBailey,Kefan Padmanabhan,SubhaPuttagunta,KalpeshKrishna,\nXiao,NimeshGhelani,LoraAroyo,AmbroseSlone, LeslieBaker,NorbertKalb,VamsiBedapudi,Adam\nNeilHoulsby,XuehanXiong,ZhenYang,ElenaGri- Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin,\nbovskaya,JonasAdler,MateoWirth,LisaLee,Music Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Si-\nLi,ThaisKagohara,JayPavagadhi,SophieBridgers, ciliano,AlanPapir,RobbyNeale,JonasBragagnolo,\nAnnaBortsova,SanjayGhemawat,ZafaraliAhmed, TejToor,TinaChen,ValentinAnklin,FeiranWang,\nTianqi Liu, Richard Powell, Vijay Bolina, Mariko Richie Feng, Milad Gholami, Kevin Ling, Lijuan\nIinuma,PolinaZablotskaia,JamesBesley,Da-Woon Liu,JulesWalter,HamidMoghaddam,ArunKishore,\nChung, Timothy Dozat, Ramona Comanescu, Xi- JakubAdamek,TylerMercado,JonathanMallinson,\nanceSi,JeremyGreer,GuolongSu,MartinPolacek, Siddhinita Wandekar, Stephen Cagle, Eran Ofek,\nRapha√´lLopezKaufman,SimonTokumine,Hexiang Guillermo Garrido, Clemens Lombriser, Maksim\nHu, Elena Buchatskaya, Yingjie Miao, Mohamed Mukha, Botu Sun, Hafeezul Rahman Mohammad,\nElhawaty, Aditya Siddhant, Nenad Tomasev, Jin- JosipMatak,YadiQian,VikasPeswani,PawelJanus,\nQuanYuan,LeifSchelin,OanaDavid,AnkurGarg, naldChung,KaiYang,NihalBalani,ArthurBra≈æin-\nYifan He, Oleksii Duzhyi, Anton √Ñlgmyr, Timo- skas,AndreiSozanschi,MatthewHayes,H√©ctorFer-\nth√©e Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex n√°ndez Alcalde, Peter Makarov, Will Chen, Anto-\nChinien,RakeshShivanna,AleksandrChuklin,Josie nioStella,LiselotteSnijders,MichaelMandl,Ante\nLi,CarrieSpadine,TravisWolfe,KareemMohamed, K√§rrman,Pawe≈ÇNowak,XinyiWu,AlexDyck,Kr-\nSubhabrataDas, ZihangDai, KyleHe, Danielvon ishnanVaidyanathan,RaghavenderR,JessicaMal-\nDincklage, Shyam Upadhyay, Akanksha Maurya, let, Mitch Rudominer, Eric Johnston, Sushil Mit-\nLuyanChi,SebastianKrause,KhalidSalama,PamG tal,AkhilUdathu,JanaraChristensen,VishalVerma,\nRabinovitch, Pavan Kumar Reddy M, Aarush Sel- ZachIrving,AndreasSantucci,GamaleldinElsayed,\nvan,MikhailDektiarev,GolnazGhiasi,ErdemGu- Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan\nven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Hua, Geoffrey Cideron, Edouard Leurent, Mah-\nIdan Heimlich Shtacher, Shachi Paul, Oscar Aker- moud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy\nlund, Fran√ßois-Xavier Aubet, Terry Huang, Chen Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper\nZhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Snoek,MukundSundararajan,XuezhiWang,Zack\nFrancescoBertolini,Liana-EleonoraMarinescu,Mar- Ontiveros,ItayKaro,JeremyCole,VinuRajashekhar,\ntin B√∂lle, Dominik Paulus, Khyatti Gupta, Tejasi LaraTumeh,EyalBen-David,RishubJain,Jonathan\nLatkar, Max Chang, Jason Sanders, Roopa Wil- Uesato, Romina Datta, Oskar Bunyan, Shimu Wu,\nson,XueweiWu,Yi-XuanTan,LamNguyenThiet, JohnZhang,PiotrStanczyk,YeZhang,DavidSteiner,\nTulseeDoshi,SidLall,SwaroopMishra,Wanming SubhajitNaskar,MichaelAzzam,MatthewJohnson,\nChen, Thang Luong, Seth Benjamin, Jasmine Lee, AdamPaszke,Chung-ChengChiu,JaumeSanchez\nEwa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Elias, Afroz Mohiuddin, Faizan Muhammad, Jin\nKrzysztof Styrc, Pengcheng Yin, Jon Simon, Mal- Miao, Andrew Lee, Nino Vieillard, Jane Park, Ji-\ncolm Rose Harriott, Mudit Bansal, Alexei Robsky, agengZhang,JeffStanway,DrewGarmon,Abhijit\nGeoffBacon,DavidGreene,DaniilMirylenka,Chen Karmarkar,ZheDong,JongLee,AviralKumar,Lu-\nZhou, Obaid Sarvana, Abhimanyu Goyal, Samuel oweiZhou,JonathanEvens,WilliamIsaac,Geoffrey\nAndermatt, Patrick Siegler, Ben Horn, Assaf Is- Irving,EdwardLoper,MichaelFink,IshaArkatkar,\nrael, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Nanxin Chen, Izhak Shafran, Ivan Petrychenko,\nMarco Selvatici, Pedro Silva, Kathie Wang, Jack- ZheChen,JohnsonJia,AnselmLevskaya,Zhenkai\nsonTolins,KelvinGuu,RoeyYogev,XiaochenCai, Zhu, Peter Grabowski, Yu Mao, Alberto Magni,\nAlessandro Agostini, Maulik Shah, Hung Nguyen, KaishengYao,JavierSnaider,NormanCasagrande,\nNoah √ì Donnaile, S√©bastien Pereira, Linda Friso, Evan Palmer, Paul Suganthan, Alfonso Casta√±o,\nAdam Stambler, Adam Kurzrok, Chenkai Kuang, IreneGiannoumis,WooyeolKim,Miko≈ÇajRybin¬¥ski,\nYanRomanikhin,MarkGeller,ZJYan,KaneJang, AshwinSreevatsa,JenniferPrendki,DavidSoergel,\nCheng-Chun Lee, Wojciech Fica, Eric Malmi, Qi- AdrianGoedeckemeyer,WilliGierke,MohsenJafari,\njun Tan, Dan Banica, Daniel Balle, Ryan Pham, MeenuGaba,JeremyWiesner,DianaGageWright,\nYanpingHuang,DianaAvram,HongzhiShi,Jasjot YawenWei,HarshaVashisht,YanaKulizhskaya,Jay\nSingh, Chris Hidey, Niharika Ahuja, Pranab Sax- Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu,\nena,DanDooley,SrividyaPranaviPotharaju,Eileen Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert\nO‚ÄôNeill, Anand Gokulchandran, Ryan Foley, Kai Cui,TianLIN,MarcusWu,RicardoAguilar,Keith\nZhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Pallo,AbhishekChakladar,GingerPerng,ElenaAl-\nRaghaKotikalapudi,ChalenceSafranek-Shrader,An- lica Abellan, Mingyang Zhang, Ishita Dasgupta,\ndrewGoodman,JoshuaKessinger,EranGloben,Pra- Nate Kushman, Ivo Penchev, Alena Repina, Xihui\nteekKolhar,ChrisGorgolewski,AliIbrahim,Yang Wu, Tom van der Weide, Priya Ponnapalli, Car-\nSong, Ali Eichenbaum, Thomas Brovelli, Sahitya oline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier\nPotluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Dousse,FanYang,JeffPiper,NathanIe,RamaPa-\nCharlesChen,AndyCrawford,ShaliniPal,Mukund sumarthi,NathanLintz,AnithaVijayakumar,Daniel\nSridhar,PetruGurita,AsierMujika,IgorPetrovski, Andor,PedroValenzuela,MinnieLui,CosminPadu-\nPierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, raru, Daiyi Peng, Katherine Lee, Shuyuan Zhang,\nNiccol√≤ Dal Santo, Siddharth Goyal, Jitesh Pun- SomerGreene,DucDungNguyen,PaulaKurylow-\njabi, Karthik Kappaganthu, Chester Kwak, Pallavi icz,CassidyHardin,LucasDixon,LiliJanzer,Kiam\nLV, Sarmishta Velury, Himadri Choudhury, Jamie Choo, Ziqiang Feng, Biao Zhang, Achintya Sing-\nHall,PremalShah,RicardoFigueira,MattThomas, hal,DayouDu,DanMcKinnon,NatashaAntropova,\nMinjie Lu, Ting Zhou, Chintu Kumar, Thomas Ju- TolgaBolukbasi,OrgadKeller,DavidReid,Daniel\nrdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Finchelstein, Maria Abi Raad, Remi Crocker, Pe-\nKwak, Victor √Ñhdel, Sujeevan Rajayogam, Travis ter Hawkins, Robert Dadashi, Colin Gaffney, Ken\nChoma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Franko, Anna Bulanova, R√©mi Leblond, Shirley\nPark,VincentHellendoorn,AlexBailey,TaylanBi- Chung, Harry Askham, Luis C. Cobo, Kelvin Xu,\nlal, Huanjie Zhou, Mehrdad Khatir, Charles Sut- FelixFischer,JunXu,ChristinaSorokin,ChrisAl-\nton,WojciechRzadkowski,FionaMacintosh,Kon- berti,Chu-ChengLin,ColinEvans,AlekDimitriev,\nstantin Shagin, Paul Medina, Chen Liang, Jinjing Hannah Forbes, Dylan Banarse, Zora Tung, Mark\nZhou,PararthShah,YingyingBi,AttilaDankovics, Omernick,ColtonBishop,RachelSterneck,Rohan\nShipraBanga,SabineLehmann,MarissaBredesen, Jain,JiaweiXia,EhsanAmid,FrancescoPiccinno,\nZifanLin,JohnEricHoffmann,JonathanLai,Ray- XingyuWang,PraseemBanzal,DanielJ.Mankowitz,\nAlexPolozov,VictoriaKrakovna,SashaBrown,Mo-\nhammadHosseinBateni,DennisDuan,VladFiroiu, FengYang,RihamMansour,JasonGelman,YangXu,\nMeghana Thotakuri, Tom Natan, Matthieu Geist, GeorgePolovets,JiLiu,HonglongCai,WarrenChen,\nSertanGirgin,HuiLi,JiayuYe,OfirRoval,Reiko XiangHaiSheng,EmilyXue,SherjilOzair,Christof\nTojo, Michael Kwong, James Lee-Thorp, Christo- Angermueller, Xiaowei Li, Anoop Sinha, Weiren\npherYew,DanilaSinopalnikov,SabelaRamos,John Wang, Julia Wiesinger, Emmanouil Koukoumidis,\nMellor,AbhishekSharma,KathyWu,DavidMiller, YuanTian, AnandIyer, MadhuGurumurthy, Mark\nNicolasSonnerat, DenisVnukov, RoryGreig, Jen- Goldenson,ParasharShah,MKBlake,HongkunYu,\nnifer Beattie, Emily Caveness, Libin Bai, Julian AnthonyUrbanowicz,JennimariaPalomaki,Chrisan-\nEisenschlos, Alex Korchemniy, Tomy Tsai, Mimi tha Fernando, Ken Durden, Harsh Mehta, Nikola\nJasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Momchev, Elahe Rahimtoroghi, Maria Georgaki,\nFrederickLiu,FanYang,RuiZhu,TianHueyTeh, AmitRaul,SebastianRuder,MorganRedshaw,Jin-\nJason Sanmiya, Evgeny Gladchenko, Nejc Trdin, hyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li,\nDanielToyama,EvanRosen,SasanTavakkol,Lint- BlakeHechtman,ParkerSchuh,MiladNasr,Kieran\ningXue,ChenElkind,OliverWoodman,JohnCar- Milan,VladimirMikulik,JulianaFranco,TimGreen,\npenter,GeorgePapamakarios,RupertKemp,Sushant NamNguyen,JoeKelley,AromaMahendru,Andrea\nKafle, Tanya Grunina, Rishika Sinha, Alice Tal- Hu,JoshuaHowland,BenVargas,JeffreyHui,Kshi-\nbert, Diane Wu, Denese Owusu-Afriyie, Cosmo tijBansal,VikramRao,RakeshGhiya,EmmaWang,\nDu,ChloeThornton,JordiPont-Tuset,Pradyumna KeYe,JeanMichelSarr,MelanieMoranskiPreston,\nNarayana, Jing Li, Saaber Fatehi, John Wieting, MadeleineElish,SteveLi,AakashKaku,JigarGupta,\nOmar Ajmeri, Benigno Uria, Yeongil Ko, Laura IcePasupat,Da-ChengJuan,MilanSomeswar,Tejvi\nKnight,Am√©lieH√©liou,NingNiu,ShaneGu,Chenxi M.,XinyunChen,AidaAmini,AlexFabrikant,Eric\nPang, Yeqing Li, Nir Levine, Ariel Stolovich, Re- Chu, Xuanyi Dong, Amruta Muthal, Senaka Buth-\nbecaSantamaria-Fernandez,SonamGoenka,Wenny pitiya, Sarthak Jauhari, Nan Hua, Urvashi Khan-\nYustalim,RobinStrudel,AliElqursh,CharlieDeck, delwal, AyalHitron, JieRen, LarissaRinaldi, Sha-\nHyo Lee, Zonglin Li, Kyle Levin, Raphael Hoff- har Drath, Avigail Dabush, Nan-Jiang Jiang, Har-\nmann, Dan Holtmann-Rice, Olivier Bachem, Sho shal Godhia, Uli Sachs, Anthony Chen, Yicheng\nArora, Christy Koh, Soheil Hassas Yeganeh, Siim Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai,\nP√µder,MukarramTariq,YanhuaSun,LucianIonita, JamesWang,ChenLiang,JennyHamer,Chun-Sung\nMojtabaSeyedhosseini,PouyaTafti,ZhiyuLiu,An- Ferng,ChenelElkind,AvielAtias,PaulinaLee,V√≠t\nmolGulati,JasmineLiu,XinyuYe,BartChrzaszcz, List√≠k,MathiasCarlen,JanvandeKerkhof,Marcin\nLily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Pikus,KrunoslavZaher,PaulM√ºller,SashaZykova,\nShreya Singh, Wei Fan, Aaron Parisi, Joe Stan- Richard Stefanec, Vitaly Gatsko, Christoph Hirn-\nton, Vinod Koverkathu, Christopher A. Choquette- schall,AshwinSethi,XingyuFedericoXu,Chetan\nChoo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Ke-\nShroff, Mani Varadarajan, Sanaz Bahargam, Rob shav Dhandhania, Manish Katyal, Akshay Gupta,\nWilloughby, David Gaddy, Guillaume Desjardins, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan\nMarco Cornero, Brona Robenek, Bhavishya Mit- Bhatia,YashodhaBhavnani,OmarAlhadlaq,Xiaolin\ntal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera\nHenrik Jacobsson, Alireza Ghaffarkhah, Morgane Filippova, Abhipso Ghosh, Ben Limonchik, Bhar-\nRivi√®re,AlannaWalton,Cl√©mentCrepy,AliciaPar- gavaUrala,ChaitanyaKrishnaLanka,DerikClive,\nrish,ZongweiZhou,ClementFarabet,CareyRade- Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak,\nbaugh, Praveen Srinivasan, Claudia van der Salm, IannaLi,KalindThakkar,KuanyshOmarov,Kushal\nAndreas Fidjeland, Salvatore Scellato, Eri Latorre- Majmundar,MichaelAlverson,MichaelKucharski,\nChimoto,HannaKlimczak-Plucin¬¥ska,DavidBridson, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo\nDariodeCesare,TomHudson,PiermariaMendolic- Pelagatti,RohanKohli,SaurabhKumar,JosephKim,\nchio, Lexi Walker, Alex Morris, Matthew Mauger, Swetha Sankar, Vineet Shah, Lakshmi Ramachan-\nAlexey Guseynov, Alison Reid, Seth Odoom, Lu- druni, Xiangkai Zeng, Ben Bariach, Laura Wei-\ncia Loher, Victor Cotruta, Madhavi Yenugula, Do- dinger, Tu Vu, Alek Andreev, Antoine He, Kevin\nminik Grewe, Anastasia Petrushkina, Tom Duerig, Hui, Sheleem Kashem, Amar Subramanya, Sissie\nAntonio Sanchez, Steve Yadlowsky, Amy Shen, Hsiao,DemisHassabis,KorayKavukcuoglu,Adam\nAmir Globerson, Lynette Webb, Sahil Dua, Dong Sadovsky,QuocLe,TrevorStrohman,YonghuiWu,\nLi, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, SlavPetrov,JeffreyDean,andOriolVinyals.2024a.\nAnanth Agarwal, Tomer Shani, Matan Eyal, Anuj Gemini:Afamilyofhighlycapablemultimodalmod-\nKhare,ShreyasRammohanBelle,LeiWang,Chetan els. Preprint,arXiv:2312.11805.\nTekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin\nSang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan\nZhao,StephanLee,PanduNayak,DougFritz,Man- Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,\nish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, DamienVincent,ZhufengPan,ShiboWang,Soroosh\nMartinWicke,XiaoMa,EvgeniiEltyshev,NinaMar- Mariooryad, Yifan Ding, Xinyang Geng, Fred Al-\ntin, Hardie Cate, James Manyika, Keyvan Amiri, cober, Roy Frostig, Mark Omernick, Lexi Walker,\nYelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Cosmin Paduraru, Christina Sorokin, Andrea Tac-\nNilesh Tripuraneni, David Madras, Mandy Guo, chetti, Colin Gaffney, Samira Daruki, Olcan Ser-\nAustinWaters,OliverWang,JoshuaAinslie,Jason cinoglu, Zach Gleicher, Juliette Love, Paul Voigt-\nBaldridge,HanZhang,GarimaPruthi,JakobBauer, laender, Rohan Jain, Gabriela Surita, Kareem Mo-\nhamed,RoryBlevins,JunwhanAhn,TaoZhu,Korn- Rahimtoroghi, Salem Haykal, Pablo Sprechmann,\nraphopKawintiranon,OrhanFirat,YimingGu,Yu- XiangZhou,DianaMincu,YujiaLi,RaviAddanki,\njingZhang,MatthewRahtz,ManaalFaruqui,Natalie Kalpesh Krishna, Xiao Wu, Alexandre Frechette,\nClay,JustinGilmer,JDCo-Reyes,IvoPenchev,Rui MatanEyal,AllanDafoe,DaveLacey,JayWhang,\nZhu,NobuyukiMorioka,KevinHui,KrishnaHari- ThiAvrahami,YeZhang,EmanuelTaropa,Hanzhao\ndasan,VictorCampos,MahdisMahdieh,MandyGuo, Lin,DanielToyama,ElizaRutherford,MotokiSano,\nSamer Hassan, Kevin Kilgour, Arpi Vezer, Heng- HyunJeongChoe,AlexTomala,ChalenceSafranek-\nTzeCheng,RaouldeLiedekerke,SiddharthGoyal, Shrader, Nora Kassner, Mantas Pajarskas, Matt\nPaulBarham, DJStrouse, SebNoury, JonasAdler, Harvey, Sean Sechrist, Meire Fortunato, Christina\nMukundSundararajan,SharadVikram,DmitryLep- Lyu, Gamaleldin Elsayed, Chenkai Kuang, James\nikhin, MichelaPaganini, XavierGarcia, FanYang, Lottes, Eric Chu, Chao Jia, Chih-Wei Chen, Pe-\nDashaValter,MajaTrebacz,KiranVodrahalli,Chu- ter Humphreys, Kate Baumli, Connie Tao, Rajku-\nlayuthAsawaroengchai,RomanRing,NorbertKalb, mar Samuel, Cicero Nogueira dos Santos, Anders\nLivio Baldini Soares, Siddhartha Brahma, David Andreassen, Nemanja Rakic¬¥evic¬¥, Dominik Grewe,\nSteiner, Tianhe Yu, Fabian Mentzer, Antoine He, Aviral Kumar, Stephanie Winkler, Jonathan Caton,\nLucas Gonzalez, Bibo Xu, Raphael Lopez Kauf- AndrewBrock,SidDalmia,HannahSheahan,Iain\nman,LaurentElShafey,JunhyukOh,TomHennigan, Barr,YingjieMiao,PaulNatsev,JacobDevlin,Fer-\nGeorgevandenDriessche,SethOdoom,MarioLucic, yalBehbahani,FlavienProst,YanhuaSun,Artiom\nBeccaRoelofs,SidLall,AmitMarathe,BettyChan, Myaskovsky,ThanumalayanSankaranarayanaPillai,\nSantiago Ontanon, Luheng He, Denis Teplyashin, DanHurt,AngelikiLazaridou,XiXiong,CeZheng,\nJonathan Lai, Phil Crone, Bogdan Damoc, Lewis FabioPardo,XiaoweiLi,DanHorgan,JoeStanton,\nHo,SebastianRiedel,KarelLenc,Chih-KuanYeh, MoranAmbar, FeiXia, AlejandroLince, Mingqiu\nAakankshaChowdhery,YangXu,MehranKazemi, Wang,BasilMustafa,AlbertWebson,HyoLee,Ro-\nEhsanAmid,AnastasiaPetrushkina,KevinSwersky, hanAnil,MartinWicke,TimothyDozat,Abhishek\nAli Khodaei, Gowoon Chen, Chris Larkin, Mario Sinha,EnriquePiqueras,ElaheDabir,ShyamUpad-\nPinto,GengYan,AdriaPuigdomenechBadia,Piyush hyay,AnudhyanBoral,LisaAnneHendricks,Corey\nPatil, Steven Hansen, Dave Orr, Sebastien M. R. Fry, Josip Djolonga, Yi Su, Jake Walker, Jane La-\nArnold,JordanGrimstad,AndrewDai,SholtoDou- banowski, Ronny Huang, Vedant Misra, Jeremy\nglas,RishikaSinha,VikasYadav,XiChen,ElenaGri- Chen, RJ Skerry-Ryan, Avi Singh, Shruti Rijh-\nbovskaya,JacobAustin,JeffreyZhao,KaushalPatel, wani,DianYu,AlexCastro-Ros,BeerChangpinyo,\nPaulKomarek,SophiaAustin,SebastianBorgeaud, Romina Datta, Sumit Bagri, Arnar Mar Hrafnkels-\nLinda Friso, Abhimanyu Goyal, Ben Caine, Kris son, Marcello Maggioni, Daniel Zheng, Yury Sul-\nCao,Da-WoonChung,MatthewLamm,GabeBarth- sky, Shaobo Hou, Tom Le Paine, Antoine Yang,\nMaron,ThaisKagohara,KateOlszewska,MiaChen, Jason Riesa, Dominika Rogozinska, Dror Marcus,\nKaushik Shivakumar, Rishabh Agarwal, Harshal DaliaElBadawy, QiaoZhang, LuyuWang, Helen\nGodhia, Ravi Rajwar, Javier Snaider, Xerxes Doti- Miller,JeremyGreer,LarsLoweSjos,AzadeNova,\nwalla, Yuan Liu, Aditya Barua, Victor Ungureanu, HeigaZen,RahmaChaabouni,MihaelaRosca,Jiepu\nYuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth, Jiang,CharlieChen,RuiboLiu,TaraSainath,Maxim\nJames Qin, Ivo Danihelka, Tulsee Doshi, Martin Krikun,AlexPolozov,Jean-BaptisteLespiau,Josh\nChadwick, Jilin Chen, Sanil Jain, Quoc Le, Ar- Newlan,ZeyncepCankara,SooKwak,YunhanXu,\njun Kar, Madhu Gurumurthy, Cheng Li, Ruoxin PhilChen,AndyCoenen,ClemensMeyer,Katerina\nSang,FangyuLiu,LamprosLamprou,RichMunoz, Tsihlas,AdaMa,JurajGottweis,JinweiXing,Chen-\nNathan Lintz, Harsh Mehta, Heidi Howard, Mal- jieGu,JinMiao,ChristianFrank,ZeynepCankara,\ncolmReynolds,LoraAroyo,QuanWang,Lorenzo SanjayGanapathy,IshitaDasgupta,StephHughes-\nBlanco, Albin Cassirer, Jordan Griffith, Dipanjan Fitt,HengChen,DavidReid,KeranRong,Hongmin\nDas, Stephan Lee, Jakub Sygnowski, Zach Fisher, Fan,JoostvanAmersfoort,VincentZhuang,Aaron\nJamesBesley,RichardPowell,ZafaraliAhmed,Do- Cohen, Shixiang Shane Gu, Anhad Mohananey,\nminikPaulus,DavidReitter,ZalanBorsos,Rishabh Anastasija Ilic, Taylor Tobin, John Wieting, Anna\nJoshi,AedanPope,StevenHand,VittorioSelo,Vi- Bortsova, Phoebe Thacker, Emma Wang, Emily\nhanJain,NikhilSethi,MeghaGoel,TakakiMakino, Caveness,JustinChiu,ErenSezener,AlexKaskasoli,\nRhysMay,ZhenYang,JohanSchalkwyk,Christina StevenBaker, KatieMillican, MohamedElhawaty,\nButterfield,AnjaHauth,AlexGoldin,WillHawkins, KostasAisopos,CarlLebsack,NathanByrd,Hanjun\nEvan Senter, Sergey Brin, Oliver Woodman, Mar- Dai,WenhaoJia,MatthewWiethoff,ElnazDavoodi,\nvinRitter,EricNoland,MinhGiang,VijayBolina, AlbertWeston,LakshmanYagati,ArunAhuja,Isabel\nLisaLee,TimBlyth,IanMackinnon,MachelReid, Gao,GolanPundak,SusanZhang,MichaelAzzam,\nObaidSarvana,DavidSilver,AlexanderChen,Lily Khe Chai Sim, Sergi Caelles, James Keeling, Ab-\nWang, Loren Maggiore, Oscar Chang, Nithya At- hanshuSharma,AndySwing,YaGuangLi,Chenxi\ntaluri, Gregory Thornton, Chung-Cheng Chiu, Os- Liu,CarrieGrimesBostock,YaminiBansal,Zachary\nkar Bunyan, Nir Levine, Timothy Chung, Evgenii Nado,AnkeshAnand,JoshLipschultz,AbhijitKar-\nEltyshev, Xiance Si, Timothy Lillicrap, Demetra markar, Lev Proleev, Abe Ittycheriah, Soheil Has-\nBrady,VaibhavAggarwal,BoxiWu,YuanzhongXu, sas Yeganeh, George Polovets, Aleksandra Faust,\nRoss McIlroy, Kartikeya Badola, Paramjit Sandhu, JiaoSun,AlbanRrustemi,PenLi,RakeshShivanna,\nErica Moreira, Wojciech Stokowiec, Ross Hems- JeremiahLiu,ChrisWelty,FedericoLebron,Anirudh\nley, Dong Li, Alex Tudor, Pranav Shyam, Elahe Baddepudi,SebastianKrause,EmilioParisotto,Radu\nSoricut, ZhengXu, DawnBloxwich, MelvinJohn- NicholasFitzGerald,YaoZhao,WoohyunHan,Chris\nson, Behnam Neyshabur, Justin Mao-Jones, Ren- Alberti,DanGarrette,KashyapKrishnakumar,Mai\nshenWang,VinayRamasesh,ZaheerAbbas,Arthur Gimenez, Anselm Levskaya, Daniel Sohn, Josip\nGuez, Constant Segal, Duc Dung Nguyen, James Matak,InakiIturrate,MichaelB.Chang,JackieXi-\nSvensson, Le Hou, Sarah York, Kieran Milan, So- ang,YuanCao,NishantRanka,GeoffBrown,Adrian\nphieBridgers,WiktorGworek,MarcoTagliasacchi, Hutter, Vahab Mirrokni, Nanxin Chen, Kaisheng\nJamesLee-Thorp,MichaelChang,AlexeyGuseynov, Yao,ZoltanEgyed,FrancoisGalilee,TylerLiechty,\nAleJakseHartman,MichaelKwong,RuizheZhao, PraveenKallakuri,EvanPalmer,SanjayGhemawat,\nSheleem Kashem, Elizabeth Cole, Antoine Miech, JasmineLiu,DavidTao,ChloeThornton,TimGreen,\nRichard Tanburn, Mary Phuong, Filip Pavetic, Se- MimiJasarevic,SharonLin,VictorCotruta,Yi-Xuan\nbastien Cevey, Ramona Comanescu, Richard Ives, Tan, Noah Fiedel, Hongkun Yu, Ed Chi, Alexan-\nSherry Yang, Cosmo Du, Bo Li, Zizhao Zhang, der Neitz, Jens Heitkaemper, Anu Sinha, Denny\nMarikoIinuma,ClaraHuiyiHu,AurkoRoy,Shaan Zhou, Yi Sun, Charbel Kaed, Brice Hulse, Swa-\nBijwadia, Zhenkai Zhu, Danilo Martins, Rachel roop Mishra, Maria Georgaki, Sneha Kudugunta,\nSaputro, Anita Gergely, Steven Zheng, Dawei Jia, ClementFarabet,IzhakShafran,DanielVlasic,An-\nIoannis Antonoglou, Adam Sadovsky, Shane Gu, ton Tsitsulin, Rajagopal Ananthanarayanan, Alen\nYingyingBi,AlekAndreev,SinaSamangooei,Mina Carin, Guolong Su, Pei Sun, Shashank V, Gabriel\nKhan, Tomas Kocisky, Angelos Filos, Chintu Ku- Carvajal,JosefBroder,IuliaComsa,AlenaRepina,\nmar, Colton Bishop, Adams Yu, Sarah Hodkin- WilliamWong,WarrenWeilunChen,PeterHawkins,\nson,SidMittal,PremalShah,AlexandreMoufarek, Egor Filonov, Lucia Loher, Christoph Hirnschall,\nYongCheng,AdamBloniarz,JaehoonLee,Pedram Weiyi Wang, Jingchen Ye, Andrea Burns, Hardie\nPejman, Paul Michel, Stephen Spencer, Vladimir Cate, Diana Gage Wright, Federico Piccinini, Lei\nFeinberg, Xuehan Xiong, Nikolay Savinov, Char- Zhang, Chu-Cheng Lin, Ionel Gog, Yana Kulizh-\nlotte Smith, Siamak Shakeri, Dustin Tran, Mary skaya, Ashwin Sreevatsa, Shuang Song, Luis C.\nChesus,BerndBohnet,GeorgeTucker,Tamaravon Cobo, Anand Iyer, Chetan Tekur, Guillermo Gar-\nGlehn, Carrie Muir, Yiran Mao, Hideto Kazawa, rido, Zhuyun Xiao, Rupert Kemp, Huaixiu Steven\nAmbroseSlone,KedarSoparkar,DishaShrivastava, Zheng, Hui Li, Ananth Agarwal, Christel Ngani,\nJamesCobon-Kerr,MichaelSharman,JayPavagadhi, KatiGoshvadi,RebecaSantamaria-Fernandez,Woj-\nCarlos Araya, Karolis Misiunas, Nimesh Ghelani, ciechFica,XinyunChen,ChrisGorgolewski,Sean\nMichael Laskin, David Barker, Qiujia Li, Anton Sun, Roopal Garg, Xinyu Ye, S. M. Ali Eslami,\nBriukhov,NeilHoulsby,MiaGlaese,BalajiLaksh- Nan Hua, Jon Simon, Pratik Joshi, Yelin Kim, Ian\nminarayanan, Nathan Schucher, Yunhao Tang, Eli Tenney, Sahitya Potluri, Lam Nguyen Thiet, Quan\nCollins, Hyeontaek Lim, Fangxiaoyu Feng, Adria Yuan,FlorianLuisier,AlexandraChronopoulou,Sal-\nRecasens,GuangdaLai,AlbertoMagni,NicolaDe vatoreScellato,PraveenSrinivasan,MinminChen,\nCao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay, Vinod Koverkathu, Valentin Dalibard, Yaming Xu,\nMostafaDehghani,JennyBrennan,YifanHe,Kelvin Brennan Saeta, Keith Anderson, Thibault Sellam,\nXu,YangGao,CarlSaroufim,JamesMolloy,Xinyi NickFernando,FantineHuot,JunehyukJung,Mani\nWu, Seb Arnold, Solomon Chang, Julian Schrit- Varadarajan,MichaelQuinn,AmitRaul,MaigoLe,\ntwieser,ElenaBuchatskaya,SoroushRadpour,Mar- Ruslan Habalov, Jon Clark, Komal Jalan, Kalesha\ntin Polacek, Skye Giordano, Ankur Bapna, Simon Bullard, Achintya Singhal, Thang Luong, Boyu\nTokumine,VincentHellendoorn,ThibaultSottiaux, Wang, Sujeevan Rajayogam, Julian Eisenschlos,\nSarahCogan,AliakseiSeveryn,MohammadSaleh, JohnsonJia,DanielFinchelstein,AlexYakubovich,\nShantanu Thakoor, Laurent Shefey, Siyuan Qiao, DanielBalle,MichaelFink,SameerAgarwal,Jing\nMeenuGaba,ShuoyiinChang,CraigSwanson,Biao Li, Dj Dvijotham, Shalini Pal, Kai Kang, Jaclyn\nZhang,BenjaminLee,PaulKishanRubenstein,Gan Konzelmann,JenniferBeattie,OlivierDousse,Diane\nSong, Tom Kwiatkowski, Anna Koop, Ajay Kan- Wu,RemiCrocker,ChenElkind,SiddharthaReddy\nnan,DavidKao,ParkerSchuh,AxelStjerngren,Gol- Jonnalagadda,JongLee,DanHoltmann-Rice,Krys-\nnazGhiasi,GenaGibson,LukeVilnis,YeYuan,Fe- talKallarackal,RosanneLiu,DenisVnukov,Neera\nlipe Tiengo Ferreira, Aishwarya Kamath, Ted Kli- Vats,LucaInvernizzi,MohsenJafari,HuanjieZhou,\nmenko,KenFranko,KefanXiao,IndroBhattacharya, Lilly Taylor, Jennifer Prendki, Marcus Wu, Tom\nMiteyan Patel, Rui Wang, Alex Morris, Robin Eccles, Tianqi Liu, Kavya Kopparapu, Francoise\nStrudel, Vivek Sharma, Peter Choy, Sayed Hadi Beaufays,ChristofAngermueller,AndreeaMarzoca,\nHashemi, Jessica Landon, Mara Finkelstein, Priya ShouryaSarcar,HilalDib,JeffStanway,FrankPer-\nJhakra,JustinFrye,MeganBarnes,MatthewMauger, bet, Nejc Trdin, Rachel Sterneck, Andrey Khor-\nDennisDaun,KhuslenBaatarsukh,MatthewTung, lin,DinghuaLi,XihuiWu,SonamGoenka,David\nWaelFarhan,HenrykMichalewski,FabioViola,Fe- Madras,SashaGoldshtein,WilliGierke,TongZhou,\nlixdeChaumontQuitry,CharlineLeLan,TomHud- Yaxin Liu, Yannie Liang, Anais White, Yunjie Li,\nson,QingzeWang,FelixFischer,IvyZheng,Elspeth ShreyaSingh,SanazBahargam,MarkEpstein,Su-\nWhite,AncaDragan,JeanbaptisteAlayrac,EricNi, joyBasu,LiLao,AdnanOzturel,CarlCrous,Alex\nAlexander Pritzel, Adam Iwanicki, Michael Isard, Zhai, Han Lu, Zora Tung, Neeraj Gaur, Alanna\nAnna Bulanova, Lukas Zilka, Ethan Dyer, Deven- Walton, Lucas Dixon, Ming Zhang, Amir Glober-\ndraSachan,SrivatsanSrinivasan,HannahMucken- son, Grant Uy, Andrew Bolt, Olivia Wiles, Milad\nhirn, Honglong Cai, Amol Mandhane, Mukarram Nasr, Ilia Shumailov, Marco Selvi, Francesco Pic-\nTariq, Jack W. Rae, Gary Wang, Kareem Ayoub, cinno,RicardoAguilar,SaraMcCarthy,MishaKhal-\nman, Mrinal Shukla, Vlado Galic, John Carpen- Doug Fritz, Vikram Rao, Xuezhi Wang, Jiageng\nter, Kevin Villela, Haibin Zhang, Harry Richard- Zhang, Viorica Patraucean, Dayou Du, Igor Mor-\nson,JamesMartens,MatkoBosnjak,ShreyasRam- datch, Ivan Jurin, Lewis Liu, Ayush Dubey, Abhi\nmohan Belle, Jeff Seibert, Mahmoud Alnahlawi, Mohan, Janek Nowakowski, Vlad-Doru Ion, Nan\nBrian McWilliams, Sankalp Singh, Annie Louis, Wei, Reiko Tojo, Maria Abi Raad, Drew A. Hud-\nWen Ding, Dan Popovici, Lenin Simicich, Laura son, Vaishakh Keshava, Shubham Agrawal, Kevin\nKnight, Pulkit Mehta, Nishesh Gupta, Chongyang Ramirez,ZhichunWu,HoangNguyen,JiLiu,Mad-\nShi, Saaber Fatehi, Jovana Mitrovic, Alex Grills, havi Sewak, Bryce Petrini, DongHyun Choi, Ivan\nJosephPagadora,DessiePetrova,DanielleEisenbud, Philips, Ziyue Wang, Ioana Bica, Ankush Garg,\nZhishuai Zhang, Damion Yates, Bhavishya Mittal, Jarek Wilkiewicz, Priyanka Agrawal, Xiaowei Li,\nNileshTripuraneni,YannisAssael,ThomasBrovelli, Danhao Guo, Emily Xue, Naseer Shaik, Andrew\nPrateekJain,MihajloVelimirovic,CanferAkbulut, Leach,SadhMNMKhan,JuliaWiesinger,Sammy\nJiaqi Mu, Wolfgang Macherey, Ravin Kumar, Jun Jerome,AbhishekChakladar,AlekWenjiaoWang,\nXu, Haroon Qureshi, Gheorghe Comanici, Jeremy TinaOrnduff,FolakeAbu,AlirezaGhaffarkhah,Mar-\nWiesner, Zhitao Gong, Anton Ruddock, Matthias cusWainwright,MarioCortes,FrederickLiu,Joshua\nBauer,NickFelt,AnirudhGP,AnuragArnab,Dustin Maynez, Andreas Terzis, Pouya Samangouei, Ri-\nZelle,JonasRothfuss,BillRosgen,AshishShenoy, hamMansour,TomaszKeÀõpa,Fran√ßois-XavierAubet,\nBryan Seybold, Xinjian Li, Jayaram Mudigonda, Anton Algymr, Dan Banica, Agoston Weisz, An-\nGokerErdogan,JiaweiXia,JiriSimsa,AndreaMichi, dras Orban, Alexandre Senges, Ewa Andrejczuk,\nYiYao,ChristopherYew,StevenKan,IsaacCaswell, Mark Geller, Niccolo Dal Santo, Valentin Anklin,\nCarey Radebaugh, Andre Elisseeff, Pedro Valen- MajdAlMerey, MartinBaeuml, TrevorStrohman,\nzuela,KayMcKinney,KimPaterson,AlbertCui,Eri JunwenBai,SlavPetrov,YonghuiWu,DemisHas-\nLatorre-Chimoto,SolomonKim,WilliamZeng,Ken sabis,KorayKavukcuoglu,JeffreyDean,andOriol\nDurden, Priya Ponnapalli, Tiberiu Sosea, Christo- Vinyals.2024b. Gemini1.5: Unlockingmultimodal\npher A. Choquette-Choo, James Manyika, Brona understandingacrossmillionsoftokensofcontext.\nRobenek, Harsha Vashisht, Sebastien Pereira, Hoi Preprint,arXiv:2403.05530.\nLam,MarkoVelic,DeneseOwusu-Afriyie,Kather-\nHugoTouvron,ThibautLavril,GautierIzacard,Xavier\nineLee,TolgaBolukbasi,AliciaParrish,ShawnLu,\nMartinet,Marie-AnneLachaux,Timoth√©eLacroix,\nJanePark,BalajiVenkatraman,AliceTalbert,Lam-\nBaptisteRozi√®re,NamanGoyal,EricHambro,Faisal\nbert Rosique, Yuchung Cheng, Andrei Sozanschi,\nAzhar,AurelienRodriguez,ArmandJoulin,Edouard\nAdamPaszke,PraveenKumar,JessicaAustin,LuLi,\nGrave,andGuillaumeLample.2023. Llama: Open\nKhalid Salama, Wooyeol Kim, Nandita Dukkipati,\nandefficientfoundationlanguagemodels. Preprint,\nAnthony Baryshnikov, Christos Kaplanis, Xiang-\narXiv:2302.13971.\nHai Sheng, Yuri Chervonyi, Caglar Unlu, Diego\ndeLasCasas,HarryAskham,KathrynTunyasuvu-\nCheng-Hao Tu, Hong-You Chen, Zheda Mai, Jike\nnakool, Felix Gimeno, Siim Poder, Chester Kwak,\nZhong, Vardaan Pahuja, Tanya Berger-Wolf, Song\nMattMiecnikowski,VahabMirrokni,AlekDimitriev,\nGao, Charles Stewart, Yu Su, and Wei-Lun Chao.\nAaronParisi,DangyiLiu,TomyTsai,TobyShevlane,\n2023. Holistic transfer: Towards non-disruptive\nChristinaKouridi,DrewGarmon,AdrianGoedeck-\nfine-tuning with partial target data. Preprint,\nemeyer,AdamR.Brown,AnithaVijayakumar,Ali\narXiv:2311.01420.\nElqursh,SadeghJazayeri,JinHuang,SaraMcCarthy,\nJayHoover,LucyKim,SandeepKumar,WeiChen, Laurens Van der Maaten and Geoffrey Hinton. 2008.\nCourtneyBiles,GarrettBingham,EvanRosen,Lisa Visualizing data using t-sne. Journal of machine\nWang,QijunTan,DavidEngel,FrancescoPongetti, learningresearch,9(11).\nDariodeCesare,DongseongHwang,LilyYu,Jen-\nnifer Pullman, Srini Narayanan, Kyle Levin, Sid- HaohanWang,SongweiGe,ZacharyLipton,andEricP\ndharthGopal,MeganLi,AsafAharoni,TrieuTrinh, Xing.2019. Learningrobustglobalrepresentations\nJessicaLo,NormanCasagrande,RoopaliVij,Loic by penalizing local predictive power. Advances in\nMatthey,BramandiaRamadhana,AustinMatthews, NeuralInformationProcessingSystems,32.\nCJCarey,MatthewJohnson,KremenaGoranova,Ro-\nZifengWang,ZhenbangWu,DineshAgarwal,andJi-\nhinShah,ShereenAshraf,KingshukDasgupta,Ras-\nmeng Sun. 2022a. Medclip: Contrastive learning\nmusLarsen,YichengWang,ManishReddyVuyyuru,\nfromunpairedmedicalimagesandtext. In2022Con-\nChong Jiang, Joana Ijazi, Kazuki Osawa, Celine\nferenceonEmpiricalMethodsinNaturalLanguage\nSmith, Ramya Sree Boppana, Taylan Bilal, Yuma\nProcessing,EMNLP2022.\nKoizumi, Ying Xu, Yasemin Altun, Nir Shabat,\nBen Bariach, Alex Korchemniy, Kiam Choo, Olaf\nZifengWang, Zizhao Zhang, SaynaEbrahimi, Ruoxi\nRonneberger,ChimezieIwuanyanwu,ShubinZhao,\nSun,HanZhang,Chen-YuLee,XiaoqiRen,Guolong\nDavid Soergel, Cho-Jui Hsieh, Irene Cai, Shariq\nSu,VincentPerot,JenniferDy,etal.2022b. Dual-\nIqbal,MartinSundermeyer,ZheChen,ElieBursztein,\nprompt: Complementary prompting for rehearsal-\nChaitanyaMalaviya,FadiBiadsy,PrakashShroff,In-\nfreecontinuallearning. InEuropeanConferenceon\nderjit Dhillon, Tejasi Latkar, Chris Dyer, Hannah\nComputerVision,pages631‚Äì648.Springer.\nForbes, Massimo Nicosia, Vitaly Nikolaev, Somer\nGreene, MarinGeorgiev, PidongWang, NinaMar- ZifengWang,ZizhaoZhang,Chen-YuLee,HanZhang,\ntin, Hanie Sedghi, John Zhang, Praseem Banzal, RuoxiSun,XiaoqiRen,GuolongSu,VincentPerot,\nJenniferDy,andTomasPfister.2022c. Learningto KaiyangZhou,JingkangYang,ChenChangeLoy,and\nprompt for continual learning. In Proceedings of Ziwei Liu. 2022b. Learning to prompt for vision-\ntheIEEE/CVFConferenceonComputerVisionand languagemodels. InternationalJournalofComputer\nPatternRecognition,pages139‚Äì149. Vision,130(9):2337‚Äì2348.\nJianxiongXiao,JamesHays,KristaAEhinger,Aude YichuZhouandVivekSrikumar.2022. Acloserlook\nOliva, and Antonio Torralba. 2010. Sun database: at how fine-tuning changes bert. In Proceedings\nLarge-scalescenerecognitionfromabbeytozoo. In of the 60th Annual Meeting of the Association for\n2010IEEEcomputersocietyconferenceoncomputer ComputationalLinguistics(Volume1: LongPapers),\nvision and pattern recognition, pages 3485‚Äì3492. pages1046‚Äì1061.\nIEEE.\nBeierZhu,YuleiNiu,YuchengHan,YueWu,andHan-\nHuXu,GargiGhosh,Po-YaoHuang,DmytroOkhonko, wang Zhang. 2023. Prompt-aligned gradient for\nArmen Aghajanyan, Florian Metze, Luke Zettle- prompt tuning. In Proceedings of the IEEE/CVF\nmoyer, andChristophFeichtenhofer.2021. Video- InternationalConferenceonComputerVision,pages\nclip:Contrastivepre-trainingforzero-shotvideo-text 15659‚Äì15669.\nunderstanding. InProceedingsofthe2021Confer-\nence on Empirical Methods in Natural Language\nProcessing,pages6787‚Äì6800.\nLI Xuhong, Yves Grandvalet, and Franck Davoine.\n2018. Explicit inductive bias for transfer learning\nwithconvolutionalnetworks. InInternationalCon-\nference on Machine Learning, pages 2825‚Äì2834.\nPMLR.\nHantao Yao, Rui Zhang, and Changsheng Xu. 2023.\nVisual-language prompt tuning with knowledge-\nguidedcontextoptimization. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPat-\nternRecognition,pages6757‚Äì6767.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitfit: Simple parameter-efficient\nfine-tuningfortransformer-basedmaskedlanguage-\nmodels. arXivpreprintarXiv:2106.10199.\nCheng Zhang, Tai-Yu Pan, Tianle Chen, Jike Zhong,\nWenjinFu,andWei-LunChao.2022. Learningwith\nfreeobjectsegmentsforlong-tailedinstancesegmen-\ntation. Preprint,arXiv:2202.11124.\nRenruiZhang,RongyaoFang,WeiZhang,PengGao,\nKunchangLi,JifengDai,YuQiao,andHongsheng\nLi. 2021. Tip-adapter: Training-free clip-adapter\nforbettervision-languagemodeling. arXivpreprint\narXiv:2111.03930.\nRenruiZhang,XiangfeiHu,BohaoLi,SiyuanHuang,\nHanqiuDeng,YuQiao,PengGao,andHongsheng\nLi. 2023. Prompt, generate, then cache: Cascade\noffoundationmodelsmakesstrongfew-shotlearn-\ners. In Proceedings of the IEEE/CVF Conference\nonComputerVisionandPatternRecognition,pages\n15211‚Äì15222.\nJikeZhong,Hong-YouChen,andWei-LunChao.2024.\nMakingbatchnormalizationgreatinfederateddeep\nlearning. Preprint,arXiv:2303.06530.\nKaiyangZhou,JingkangYang,ChenChangeLoy,and\nZiwei Liu. 2022a. Conditional prompt learning\nforvision-languagemodels. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPat-\nternRecognition,pages16816‚Äì16825.\nA DatasetStatistics\nFollowing CoOp (Zhou et al., 2022b), we con-\nducted extensive experiments on 11 public clas-\nsification benchmark datasets to evaluate the ef-\nfectivenessoftheproposedCLIPFit. Thedatasets\nareImageNet(Dengetal.,2009),Caltech101(Fei-\nFeietal.,2004),OxfordPets(Parkhietal.,2012),\nStanfordCars (Krause et al., 2013), Flowers102\n(NilsbackandZisserman,2008),Food101(Bossard\net al., 2014), FGVCAircraft (Maji et al., 2013),\nSUN397(Xiaoetal.,2010),DTD(Cimpoietal.,\n2014),EuroSAT(Helberetal.,2019),andUCF101\n(Soomro et al., 2012). In distribution shift exper- ùõΩ\niments, we also used ImageNet-V2 (Recht et al.,\nFigure6: Performancechangesofharmonicmeanon\n2019),andImageNet-Sketch(Wangetal.,2019)as\n16shotbase-to-newsettingbyvaryinghyperparameter\nthe target dataset. The statistics of these datasets\nŒ≤.\ncanbefoundinTable8.\n.\nB ImplementationDetails\nWeimplementedourmethodwithPyTorch(Paszke\nCoCoOp(Zhouetal.,2022a),ProGrad(Zhuetal.,\net al., 2019). All experiments were based on the\n2023),KgCoOp(Yaoetal.,2023),MaPLe(Khat-\nvisionbackbonewithVit-B/16(Dosovitskiyetal.,\ntaketal.,2023)andadaptertuningmethods: CLIP-\n2020)ofCLIP(Radfordetal.,2021). Wefollowed\nddapter (Gao et al., 2023), Tip-adapter (Zhang\nCoOp(Zhouetal.,2022b)topreprocessinputim-\netal.,2021).\nages: weresizedallimagesto224√ó224andused\nZero-shotCLIP(Radfordetal.,2021)usesthe\nrandomcropping,resizing,andrandomhorizontal\nhand-crafted template ‚Äúa photo of a []‚Äù to gener-\nflippingfordataaugmentation. FollowingRadford\natethepromptsandthenappliesthesepromptsto\net al. (2021), we used a single hand-craft prompt\npredicttheclassofgivenimages.\nastextinputforallmethodsexceptprompttuning\nmethods for a fair comparison. The prompt for CoOp(Zhouetal.,2022b)introduceslearnable\neach dataset can be found in Table 8. We used text prompts instead of hand-crafted prompts to\nSGD optimizer with batch size set as 32, and set adapttheCLIPmodeltodownstreamimagerecog-\nthelearningrateas0.002(Zhouetal.,2022b). All nitiontasks.\nresultsreportedbelowaretheaverageofthreeruns\nCoCoOp(Zhouetal.,2022a)proposestogener-\nwith different random seeds. The training epoch\nateaninput-conditionaltokenforeachimagewith\nwas set to 100 for all datasets except ImageNet\nalightweightlearnableneuralnetwork.\nand Food101. The training epoch for ImageNet\nand Food101 datasets was set to 10. Smoothing KgCoOp(Yaoetal.,2023)proposestousecon-\nparameterŒ±wassetto0.99forallexperiments. Œ≤ textknowledgedistillationtolearnfromtheorigi-\nwassetto8foralldatasetsonthebase-to-newand nalCLIPmodeltoavoidoverfittingandforgetting.\ncross-datasetsetting,and2forthedistributionshift\nMaPLe(Khattaketal.,2023)proposesamulti-\nsetting. Forthefew-shotsetting,wesetŒ≤ to2for\nmodalpromptlearningstrategytointroducelearn-\nalldatasetsexceptSUN397andDTD.Œ≤ wassetto\nabletextandimageprompts.\n8forSUN397andDTDdatasets. Allexperiments\nwererunononesingleNVIDIAA100GPU. CLIP-Adapter (Gao et al., 2023) sets an addi-\ntionalbottlenecklayerfollowingthetextorimage\nC DetailedIntroductiontoBaseline encodertolearnbetterfeaturesbyaresidualway.\nMethods.\nTip-Adapter(Zhangetal.,2021)doesnotneed\nWecomparedourmethodagainststate-of-the-art training but creates the weights by a key-value\nmethods: zero-shot CLIP (Radford et al., 2021), cachemodelconstructedfromthefew-shottraining\nprompttuningmethods: CoOp(Zhouetal.,2022b), setandthenusesthiscachemodelforinference.\nTable8: StatisticsandpromptsforeachDataset.\nDataset Classes Train Val Test Hand-craftedprompt\nImageNet 1,000 1.28M N/A 50,000 ‚Äúaphotoofa[CLASS].‚Äù\nCaltech101 100 4,128 1,649 2,465 ‚Äúaphotoofa[CLASS].‚Äù\nOxfordPets 37 2,944 736 3,669 ‚Äúaphotoofa[CLASS],atypeofpet.‚Äù\nStanfordCars 196 6,509 1,635 8,041 ‚Äúaphotoofa[CLASS].‚Äù\nFlowers 102 4,093 1,633 2,463 ‚Äúaphotoofa[CLASS],atypeofflower.‚Äù\nFood101 101 50,500 20,200 30,300 ‚Äúaphotoof[CLASS],atypeoffood.‚Äù\nFGVCAircraft 100 3,334 3,333 3,333 ‚Äúaphotoofa[CLASS],atypeofaircraft.‚Äù\nSUN397 397 15,880 3,970 19,850 ‚Äúaphotoofa[CLASS].‚Äù\nDTD 47 2,820 1,128 1,692 ‚Äú[CLASS]texture.‚Äù\nEuroSAT 10 13,500 5,400 8,100 ‚Äúacenteredsatellitephotoof[CLASS].‚Äù\nUCF101 101 7,639 1,898 3,783 ‚Äúaphotoofapersondoing[CLASS].‚Äù\nImageNetV2 1,000 N/A N/A 10,000 ‚Äúaphotoofa[CLASS].‚Äù\nImageNet-Sketch 1,000 N/A N/A 50,889 ‚Äúaphotoofa[CLASS].‚Äù\nTable9: ComparisonofCLIPFitandothermethodsinthecross-datasettransfersetting. S.C.: StanfordCarsdataset.\nF.A.: FGVCAircraftdataset.\nSource Target\nImageNet\nCaltech101 OxfordPets\nS.C.\nFlowers102\nFood101\nF.A.\nSUN397\nDTD\nEuroSAT UCF101 Average\nCoOp 71.51 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88\nCoCoOp 71.02 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 45.37 68.21 65.74\nProGrad 72.24 91.52 89.64 62.39 67.87 85.40 20.61 62.47 39.42 43.46 64.29 62.71\nKgCoOp 70.66 93.92 89.83 65.41 70.01 86.36 22.51 66.16 46.35 46.04 68.50 65.51\nCLIPFit 71.10 93.77 90.36 64.56 71.43 85.76 24.46 67.43 45.20 46.40 69.17 65.85\nD Resultsoncross-datasettransfer F MoreFew-shotLearningResults\nsetting\nFollowingZhouetal.(2022a),weused1,2,4,8,\nFollowingZhouetal.(2022b,a),Wealsoevaluated\nand16-shotsetsfortrainingandreportedaccuracy\nthe cross-dataset generalization ability of CLIP-\nperformancetotestwhetherourproposedmethod\nFit and other methods. Models were trained on\ncanlearntask-specificknowledge. Theresultsare\nthe 16-shot Imagenet dataset and then tested on\nreportedinTable10. AsshowninTable10,CLIP-\nother datasets. The results are shown in Table 9.\nFitcanbringaconsistentimprovementintermsof\nAsshowninTable9,theaverageperformanceof\naverageaccuracyonallsettings.\nCLIPFitisalsobetterthanexistingmethods,which\nshowsthatCLIPFithasagoodgeneralizationabil-\nG MoreResultsofbase-to-newsetting\nity.\nE ParameterAnalysis Inthissection,wegivemoredetailedresultsonthe\nbase-to-newsetting. Thedetailedresultsforeach\nInthissection,weaimtodiscusshyper-parameters dataset on 4-shot and 8-shot settings are shown\nŒ≤. Œ± is the coefficient parameter to control the inTable11andTable12. SinceMaPLe(Khattak\nweightofknowledgedistillationloss. Experiments etal.,2023)didnotconductexperimentson4/8-\nwereconductedonthe16-shotbase-to-newsetting, shotsetting,wedonotreportresultsfromMaPLe.\nandwereportharmonicmeanaccuracyinFig. 6. AsshowninTable11andTable12,theproposed\nAsshowninFig.6,performancesarenotsensitive CLIPFitbringsconsistentimprovementcompared\nwithincertainranges. withothermethods.\n(a) Text Encoder (b) MSE loss (a) KD loss (b) MSE loss\nFigure7: Visualizationofchangesindifferentlayerson Figure 9: Visualization of bias changes w/ and w/o\ntheEuroSATdataset. regularizationlossintheEuroSATdataset.\n(a) LayerNorm gain (b) LayerNorm bias\n(a) LayerNorm gain (b) LayerNorm bias\nFigure8: VisualizationofLayerNormchangesw/and\nFigure10: VisualizationofLayerNormchangesw/and\nw/oregularizationlossintheDTDdataset.\nw/oregularizationlossintheEuroSATdataset.\nH MoreFine-tuningAnalysis\nSec. 4.3discussedthechangesinunfixedparame-\ntersafterfine-tuningtheDTDdatasetandtheim-\nportanceofmorechangedLayerNorm. Inthissub-\nsection, we give more detailed analyses of other\n(a) LayerNorm gradient sum (b) Change in each itera/on\ndatasetsandotheraspects.\nImportanceoflow-levelbiastermsintexten- Figure11: Left: visualizationofsquaredgradientsum\ncoder. Sec. 4.3presentedthatafterthefine-tuning inLayerNormlayers. Right:changeofthefirsttextbias\nlayerandfirstLayerNormlayerateachiteration.\nofCLIPFit,forbiastermsintheFNNofthetexten-\ncoder,asthenumberoflayersincreases,thechange\ninbiasdecreases. Inthissubsection,weconducted inthetextencoderbiastermsbetweenw/andw.o\nexperimentstoverifywhethermorechangedlayers regularizationloss. Inthissubsection,wewillana-\ninthetextencoderaremoreimportant. Similarto lyzeLayerNormintheimageencoderbiasterms\nSec. 4.3,wefreezeless(ormore)changedLayer- betweenw/andw.oregularizationloss. Notedthat\nNormbiaslayersinthetextencoderonthe4-shot althoughthetworegularizationlossesareapplied\nsetting. When updating only the first bias layer totextfeaturesortextencoder,theimageencoder\nandfreezingotherlayers,theaverageaccuracyis orimagefeatureswillalsobeaffectedsincethese\n74.69%. Forcomparison,theaverageaccuracyis two encoders are fine-tuned simultaneously. The\n73.33% when only updating the sixth bias layer results on the DTD dataset are shown in Fig. 8.\nand70.62%whenonlyupdatingthelastbiaslayer. When fine-tuning w/ KD loss, unlike in text en-\nBotharemuchlowerthanupdatingthefirstlayer. coder,changesingainandbiasincreasecompared\nWealsofoundthatwhenonlyupdatingthetop-3 withw/oKDloss. Thisphenomenonimpliesthat\nbiaslayers(changedmore)andfreezingotherbias imagefeatureswillchangemorew/KDlosscom-\ntermlayers,theaverageaccuracyis76.13%. For paredwithfine-tuningw/oKDloss. Moreover,we\ncomparison, when only updating the last 3 bias alsofoundthattheincreasesarealmostinthemore\nlayers(changedless)andfreezingotherbiasterm changedLayerNormlayers. Whenfine-tuningw/\nlayers, the average accuracy is 70.86%, which is MSE loss, changes in gain and bias are equal or\nmuch lower than updating the top 3 bias layers. slightlyhigherthanfine-tuningw/oKDloss.\nThese results demonstrate that the more changed LayerNormgradient. Wevisualizethesquared\nparametersarecrucialforfine-tuning. sumofgradientfromeachLayerNormlayerinthe\nAnalyzing LayerNorm with regularization image encoder in Fig. 11 (a). As observed, the\nloss. Sec. 4.3analyzedthedifferenceofchanges magnitudeofgradientinthefirstLayerNormlayer\nTable10: Comparisonwithexistingmethodsinthefew-shotlearningsetting. S.C.: StandfordCarsdataset. F.A.:\nFGVCAircraftdataset.\nshot Method ImageNet Caltech101 OxfordPets S.C. Flowers Food101 F.A. Sun397 DTD EuroSAT UCF101 AVG\nCoOp 65.77 92.37 92.2 67.1 82.2 82.07 26.73 64.7 49.0 54.8 72.0 68.09\nCoCoOp 69.51 93.8 91.17 67.92 71.98 86.1 13.2 68.19 48.51 55.71 70.35 66.95\nCLIP-adapter 67.93 93.3 89.03 67.1 72.03 85.9 27.6 67.1 45.2 61.7 69.67 67.87\nTIP-adapter 67.43 93.56 90.72 67.88 86.63 86.01 29.58 64.49 53.25 63.95 73.27 70.62\n1\nProGrad 64.33 90.96 89.01 67.11 83.81 82.75 27.97 64.54 52.79 55.1 71.91 68.2\nKgCoOp 69.03 94.13 91.97 67.03 74.63 86.27 26.9 68.43 52.5 60.83 72.93 69.51\nCLIPFit 69.37 93.67 91.63 69.33 82.83 86.17 27.73 69.07 54.63 76.87 74.27 72.32\nCoOp 68.17 92.83 89.2 69.37 88.47 80.8 29.57 66.4 51.7 61.2 73.67 70.13\nCoCoOp 69.84 94.92 92.14 68.77 76.12 86.21 15.03 69.11 52.02 46.24 73.58 67.63\nCLIP-adapter 68.6 93.67 89.73 68.97 78.53 86.1 29.6 69.0 47.87 66.07 74.1 70.2\nTIP-adapter 68.6 94.22 91.1 71.39 90.21 86.26 32.51 66.74 56.32 70.38 76.11 73.08\n2\nProGrad 66.12 93.21 90.55 71.94 88.62 84.81 30.84 68.51 54.35 66.19 74.39 71.78\nKgCoOp 69.63 94.2 92.13 68.13 79.47 86.6 28.07 69.53 55.73 68.97 74.83 71.57\nCLIPFit 69.93 94.47 92.03 72.7 87.77 86.63 30.7 70.87 57.7 78.83 76.7 74.39\nCoOp 69.38 94.44 91.3 72.73 91.14 82.58 33.18 70.13 58.57 68.62 77.41 73.59\nCoCoOp 70.55 94.98 93.01 69.1 82.56 86.64 30.87 70.5 54.79 63.83 74.99 71.98\nCLIP-adapter 69.56 94.0 90.87 71.13 86.77 86.47 31.1 71.3 53.83 66.8 77.3 72.65\nTIP-adapter 69.86 95.06 91.58 74.59 91.5 86.48 35.15 70.29 62.09 76.43 80.24 75.75\n4\nProGrad 70.21 94.93 93.21 71.75 89.98 85.77 32.93 71.17 57.72 70.84 77.82 74.21\nKgCoOp 70.19 94.65 93.2 71.98 90.69 86.59 32.47 71.79 58.31 71.06 78.4 74.48\nCLIPFit 70.4 95.0 93.07 76.43 92.03 86.73 35.8 73.0 63.2 83.17 80.13 77.18\nCoOp 70.83 94.1 90.83 76.57 94.37 83.37 37.63 72.5 64.7 75.53 80.57 76.45\nCoCoOp 70.77 95.11 93.44 70.19 84.17 86.92 26.53 70.62 58.92 68.26 77.19 72.92\nCLIP-adapter 70.2 94.27 91.77 76.47 94.9 86.67 37.2 73.13 66.4 73.23 81.87 76.92\nTIP-adapter 71.4 95.2 92.09 78.34 94.98 86.74 40.61 73.6 67.28 81.11 82.24 78.51\n8\nProGrad 71.1 94.92 92.18 78.78 93.51 85.91 37.89 72.91 62.13 79.22 88.64 77.93\nKgCoOp 70.23 94.97 93.1 73.53 89.53 86.9 34.97 72.5 65.87 72.37 80.03 75.82\nCLIPFit 71.0 95.43 93.13 79.57 94.7 87.0 39.93 74.27 67.17 84.87 82.17 79.03\nCoOp 71.51 95.5 91.8 78.89 96.1 85.17 40.93 74.5 68.63 83.6 82.43 79.01\nCoCoOp 71.02 95.19 93.25 71.68 87.64 87.19 31.29 72.05 63.78 73.82 78.34 75.02\nCLIP-adapter 71.6 94.57 92.03 80.9 97.0 86.83 42.67 75.3 71.17 81.87 84.53 79.86\nTIP-adapter 73.1 95.79 92.7 83.09 96.18 87.24 45.59 74.99 72.05 87.46 84.5 81.15\n16\nProGrad 72.68 95.8 92.13 81.46 94.87 87.01 40.39 75.0 65.92 84.38 81.59 79.2\nKgCoOp 71.2 95.03 93.23 74.87 92.9 87.03 36.27 73.4 69.37 74.93 81.43 77.26\nCLIPFit 71.53 96.13 93.5 82.43 96.37 87.37 45.47 75.67 71.57 90.13 83.83 81.27\nis much bigger than other layers. So the differ-\nenceinchangemaybecausedbythedifferencein\ngradient.\nChange in each iteration. We visualize the\nchangeinfirst-layertextbiasterms,first-layerLay-\nerNorm gain, and first-layer LayerNorm bias for\neach iteration in Fig. 11 (b). As observed, the\nchange will increase smoothly and converge to\nsomevalues.\nAnalysesonotherdatasets. Wealsoconducted\nanalysesonotherdatasets. TheresultsfortheEu-\nroSATdatasetareshowninFig. 7,Fig. 9,andFig.\n10. The phenomena in the EuroSAT dataset are\nverysimilartotheDTDdataset.\nTable11: Comparisonwithexistingmethodsinthebase-to-newgeneralizationbasedonthe4-shotsettings. H:\nHarmonicmean.\nDatasets metric CoOp CLIP-adapter CoCoOp ProGrad KgCoOp CLIPFit\nBase 73.6 74.23 75.46 74.24 74.87 75.03\nImageNet New 63.29 67.93 69.58 65.47 69.09 69.87\nH 68.06 70.94 72.4 69.58 71.86 72.36\nBase 97.27 97.23 97.25 97.37 97.53 97.57\nCaltech101 New 93.01 94.17 94.9 93.92 94.43 94.23\nH 95.09 95.68 96.06 95.61 95.95 95.87\nBase 93.33 93.8 94.59 94.08 94.68 94.93\nOxfordPets New 95.69 97.0 96.75 97.63 97.58 96.97\nH 94.5 95.37 95.66 95.82 96.11 95.94\nBase 70.92 69.43 67.71 72.69 69.25 73.77\nStandfordCars New 69.38 73.0 75.37 69.88 74.98 73.77\nH 70.14 71.17 71.33 71.26 72.0 73.77\nBase 92.5 87.93 84.75 92.46 91.3 91.03\nFlowers New 70.12 71.9 73.85 72.69 75.34 74.47\nH 79.77 79.11 78.93 81.39 82.56 81.92\nBase 86.79 90.2 89.79 88.91 90.3 90.2\nFood101 New 89.06 90.97 90.99 90.18 91.39 91.23\nH 87.91 90.58 90.39 89.54 90.84 90.71\nBase 33.21 32.43 32.07 33.73 34.21 34.53\nFGVCAircraft New 28.57 33.77 33.93 30.09 32.81 31.47\nH 30.72 33.09 32.97 31.81 33.5 32.93\nBase 76.49 77.7 77.57 77.72 78.87 79.5\nSun397 New 64.56 75.67 76.96 71.93 75.64 77.77\nH 70.02 76.67 77.26 74.71 77.22 78.63\nBase 71.26 67.43 67.44 71.06 73.65 74.37\nDTD New 50.93 55.43 56.0 52.58 57.21 64.1\nH 59.4 60.84 61.19 60.44 64.4 68.85\nBase 82.56 81.9 79.27 82.48 82.63 88.57\nEuroSAT New 53.04 59.67 65.44 56.43 59.98 76.7\nH 64.59 69.04 71.69 67.01 69.51 82.21\nBase 79.97 80.4 78.01 81.3 80.8 82.77\nUCF101 New 65.98 76.17 73.07 76.02 75.77 76.43\nH 72.3 78.23 75.46 78.57 78.2 79.47\nBase 78.43 77.52 76.72 79.18 78.92 80.21\nAVG New 68.03 72.33 73.35 71.14 73.11 75.18\nH 72.44 74.84 74.85 74.62 75.9 77.61\nTable12: Comparisonwithexistingmethodsinthebase-to-newgeneralizationbasedonthe8-shotsettings. H:\nHarmonicmean.\nDatasets metric CoOp CLIP-adapter CoCoOp ProGrad KgCoOp CLIPFit\nBase 75.22 75.07 75.52 75.72 75.84 75.73\nImageNet New 65.91 67.6 70.28 66.76 69.33 70.07\nH 70.26 71.14 72.81 70.96 72.44 72.79\nBase 97.81 97.3 97.76 98.0 97.68 97.83\nCaltech101 New 92.58 93.83 93.63 93.38 94.1 93.8\nH 95.12 95.53 95.65 95.63 95.86 95.77\nBase 94.19 94.33 95.5 94.47 94.81 94.83\nOxfordPets New 96.11 96.83 97.69 97.03 97.58 97.03\nH 95.14 95.56 96.58 95.73 96.18 95.92\nBase 73.2 72.13 69.7 75.08 69.66 76.63\nStandfordCars New 67.44 71.37 74.13 70.63 75.4 74.23\nH 70.2 71.75 71.85 72.79 72.42 75.41\nBase 96.17 94.27 92.24 93.8 87.72 94.17\nFlowers New 69.41 70.67 72.77 72.2 74.75 74.47\nH 80.63 80.78 81.36 81.59 80.72 83.17\nBase 87.27 90.27 89.6 89.48 90.46 90.33\nFood101 New 86.96 90.7 90.79 89.9 91.63 91.5\nH 87.11 90.48 90.19 89.69 91.04 90.91\nBase 37.01 35.47 33.71 36.89 34.53 38.9\nFGVCAircraft New 38.45 33.03 32.15 31.67 34.95 32.43\nH 37.72 34.2 32.91 34.08 34.74 35.37\nBase 78.61 79.53 78.05 79.21 79.37 80.57\nSun397 New 66.25 74.9 76.29 70.77 76.85 77.77\nH 71.9 77.1 77.16 74.75 78.09 79.15\nBase 76.97 74.43 73.03 74.42 69.72 77.87\nDTD New 51.81 52.77 57.24 52.38 56.44 62.63\nH 61.93 61.75 64.18 61.48 62.38 69.42\nBase 83.27 80.23 78.68 82.27 81.07 90.3\nEuroSAT New 50.59 59.87 56.03 58.52 63.13 73.0\nH 62.94 68.57 65.45 68.39 70.98 80.73\nBase 82.85 82.83 80.4 82.61 81.16 84.4\nUCF101 New 64.32 74.53 71.68 73.75 78.65 77.57\nH 72.42 78.46 75.79 77.93 79.89 80.84\nBase 80.74 79.62 78.56 80.62 78.37 81.96\nAVG New 68.39 71.46 72.06 71.02 73.75 74.95\nH 73.51 75.32 74.9 75.21 76.06 78.3",
    "pdf_filename": "Vision-Language_Model_Fine-Tuning_via_Simple_Parameter-Efficient_Modification.pdf"
}