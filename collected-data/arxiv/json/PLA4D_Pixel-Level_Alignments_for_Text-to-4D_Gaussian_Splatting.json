{
    "title": "PLA4D Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
    "context": "Previous text-to-4D methods have leveraged multiple Score Distillation Sampling (SDS) techniques, combining motion priors from video-based diffusion models (DMs) with geometric priors from multiview DMs to implicitly guide 4D renderings. However, differences in these priors result in conflicting gradient directions during optimization, causing trade-offs between motion fidelity and geometry ac- curacy, and requiring substantial optimization time to rec- oncile the models. In this paper, we introduce Pixel-Level Alignment for text-driven 4D Gaussian splatting (PLA4D) to resolve this motion-geometry conflict. PLA4D provides an anchor reference, i.e., text-generated video, to align the rendering process conditioned by different DMs in pixel space. For static alignment, our approach introduces a focal alignment method and Gaussian-Mesh contrastive learning to iteratively adjust focal lengths and provide ex- plicit geometric priors at each timestep. At the dynamic level, a motion alignment technique and T-MV refinement method are employed to enforce both pose alignment and motion continuity across unknown viewpoints, ensuring in- trinsic geometric consistency across views. With such pixel- level multi-DM alignment, our PLA4D framework is able to generate 4D objects with superior geometric, motion, and semantic consistency. Fully implemented with open-source tools, PLA4D offers an efficient and accessible solution for high-quality 4D digital content creation with significantly reduced generation time. Text-to-4D content generation has significant potential in applications ranging from game production to autonomous driving. However, this task remains challenging due to the need to generate high-quality geometry and textures, arXiv:2405.19957v4  [cs.CV]  19 Nov 2024",
    "body": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting\nQiaowei Miao\nZhejiang University\nHangzhou, China\nqiaoweimiao@zju.edu.cn\nJinsheng Quan\nZhejiang University\nHangzhou, China\njinshengquancv@gmail.com\nKehan Li\nZhejiang University\nHangzhou, China\nkehanli@zju.edu.cn\nYawei Luo*\nZhejiang University\nHangzhou, China\nyaweiluo@zju.edu.cn\nA tiger is sleepy, 4K, high quality.\nMultiview\nA monkey is playing guitar.\nMotion\nAn polar bear wearing sunglasses and drinking a coca cola.\nA clown is crying at a childrens' birthday party.\nFigure 1. 4D objects generated by PLA4D. PLA4D produces 4D content with geometric consistency and smooth, video-like motion that\naligns precisely with the text prompt, within a rapid 15-minute processing time.\nAbstract\nPrevious text-to-4D methods have leveraged multiple\nScore Distillation Sampling (SDS) techniques, combining\nmotion priors from video-based diffusion models (DMs)\nwith geometric priors from multiview DMs to implicitly\nguide 4D renderings. However, differences in these priors\nresult in conflicting gradient directions during optimization,\ncausing trade-offs between motion fidelity and geometry ac-\ncuracy, and requiring substantial optimization time to rec-\noncile the models. In this paper, we introduce Pixel-Level\nAlignment for text-driven 4D Gaussian splatting (PLA4D)\nto resolve this motion-geometry conflict. PLA4D provides\nan anchor reference, i.e., text-generated video, to align the\nrendering process conditioned by different DMs in pixel\nspace.\nFor static alignment, our approach introduces a\nfocal alignment method and Gaussian-Mesh contrastive\nlearning to iteratively adjust focal lengths and provide ex-\nplicit geometric priors at each timestep. At the dynamic\nlevel, a motion alignment technique and T-MV refinement\nmethod are employed to enforce both pose alignment and\nmotion continuity across unknown viewpoints, ensuring in-\ntrinsic geometric consistency across views. With such pixel-\nlevel multi-DM alignment, our PLA4D framework is able to\ngenerate 4D objects with superior geometric, motion, and\nsemantic consistency. Fully implemented with open-source\ntools, PLA4D offers an efficient and accessible solution for\nhigh-quality 4D digital content creation with significantly\nreduced generation time.\n1. Introduction\nText-to-4D content generation has significant potential in\napplications ranging from game production to autonomous\ndriving.\nHowever, this task remains challenging due to\nthe need to generate high-quality geometry and textures,\narXiv:2405.19957v4  [cs.CV]  19 Nov 2024\n\nùíû\nùíû\nùíû\nVS\nùíû\nùíû\nùíû\nPixel-level Alignment\nw/o Pixel-level Alignment\nUnknown View \nRenderings\nPixel-level \nAlignment Anchor\nùíûCondition\nImage DM\nImage DM\nVideo DM\nMultiview DM\nVideo DM\nMultiview DM\nOptimization  Direction\nFigure 2. Without offering an anchor reference in pixel space,\nmultiple SDS align each rendering to their respective priors, which\nmay not be consistent across different diffusion model priors, re-\nquiring significant time for reconciliation to generate a 4D result.\nWith the anchor reference in pixel space, however, each SDS can\noptimize the 4D geometry and motion representation according to\nits respective prior more effectively.\nalongside coherent object animations aligned with textual\nprompts. Existing methods in text-to-4D synthesis, such\nas MAV3D [37] and 4D-fy [2], often employ Neural Radi-\nance Fields (NeRF) [29]. MAV3D achieves text-to-4D gen-\neration by distilling text-to-video diffusion models (DMs)\nonto a Hexplane[5], while 4D-fy utilizes multiple pre-\ntrained DMs with hybrid score distillation sampling (SDS)\nto generate compelling 4D content. Recent approaches, like\nAYG [24], leverage 3D Gaussians deformed by a neural net-\nwork and incorporate multiple SDS modules from text-to-\nimage, text-to-multiview, and text-to-video DMs [4, 34] to\nguide geometry and motion generation.\nA commonality among the above methods is their heavy\nreliance on the SDS of multiple DMs to provide priors\nfor guiding the generation of geometry and motion. How-\never, the reliance on SDS-based methods brings consider-\nable challenges. As shown in Fig. 2, the goal of SDS can\nbe viewed as leveraging the priors from DMs to implicitly\nalign rendered images with conditions C.\nHowever, due\nto the different source datasets each DM is pre-trained on,\neven with the same condition, the results generated by dif-\nferent DMs vary. This discrepancy can lead to conflicts\nwhen multiple DMs are jointly optimized using SDS, re-\nsulting in two primary issues: (1) Motion-geometry trade-\noff. When video DMs and multiview DMs have conflicting\noptimization targets, it becomes challenging to generate 4D\noutputs that balance both motion and geometry. Since the\nSDS implicitly aligns rendered images with the condition,\nwe cannot easily adjust the scale of their losses for a bet-\nter motion or a better geometry. (2) Excessive optimization\ntime. When conflicts arise between multiple SDS losses, a\nsubstantial amount of time is required to balance these con-\nflicting objectives, which is one of the main reasons for the\ntime-consuming nature of current methods.\nIn this paper, we introduce a novel framework for\ntext-to-4D content creation, dubbed PLA4D (Pixel-Level\nAlignments for Text-to-4D Gaussian Splatting), which gen-\nerates 4D objects with video-like smooth motion from text\nin exceptionally short time. Our core idea is to shift from\nimplicit latent-level alignments to explicit pixel-level align-\nment. By using text-generated video as an anchor, we en-\nsure that rendered images are simultaneously aligned with\nboth prompt and pixel representations across the priors of\nmultiple DMs. To achieve this, we approach the problem at\nstatic and dynamic levels, with each level incorporating sev-\neral novel modules. In the static alignment module, we in-\ntroduce the Focal Alignment module to estimate the corre-\nsponding focal length of each generated frame, which gen-\nerates a reference mesh corresponding to the video frame\nby an image-to-mesh diffusion model.\nIt then estimates\nthe focal length of each generated frame by calculating the\nsimilarity between mesh renderings and video frames at\ndifferent focal lengths. With the correct focal length, the\ncurrent frame can accurately supervise the primary view-\npoint rendering of 4D at the corresponding timestep. Con-\nsequently, we introduce Gaussian-Mesh Contrastive Learn-\ning, which utilizes the mesh during the focal length align-\nment to provide geometric supervision, thus maintaining\ngeometric consistency for unknown views.\nIn the dynamic alignment module, we need to consider\nboth temporal and multiview consistency. We guide the mo-\ntion of 4D outputs to align with the anchor video, transfer-\nring the motion guidance from the video to the 4D target.\nSimultaneously, we ensure coherent motion across different\nviewpoints. To achieve motion continuity, we randomly se-\nlect a viewpoint for rendering multiple timesteps and align\nthis with the text conditions under the guidance of a video\nDM that generates the anchor video. This approach en-\nables the geometry and texture learned from static align-\nment to smoothly extend across the temporal dimension.\nBesides, the motion performance of 4D objects in unknown\nviews can align with the anchor video. To further reinforce\nconsistency in unseen viewpoints, we randomly choose a\ntimestep and render images across multiple views, enhanc-\ning their consistency with the corresponding frame of an-\nchor video under the guidance of a multiview DM. Through\nthe combined effects of static alignment and dynamic align-\nment modules, PLA4D enables text-driven-generated 4D\nobjects to have geometric consistency, smooth and seman-\ntically aligned motion, and minimal time overhead.\nPLA4D can generate a wide range of dynamic objects\nrapidly, producing diverse, vivid, and intricate details while\nmaintaining geometry consistency, as shown in Fig. 1. In\nsummary, our contributions are as follows:\n‚Ä¢ We present a novel text-driven 4D generation frame-\nwork that leverages explicit anchor reference, i.e., text-\ngenerated video, to align the rendering process condi-\ntioned by different DMs in pixel space, eliminating the\n\noptimization conflicts of different DMs.\n‚Ä¢ We propose focal alignment and Gaussian-Mesh con-\ntrastive learning, which automatically finds the best focal\nparameters corresponding to reference pixels and explic-\nitly provides geometry guidance for 4D.\n‚Ä¢ We propose a motion alignment method and Time-\nMultiview refinement modules to optimize 4D, ensuring\nvideo-like, large motions aligned with textual semantics.\n‚Ä¢ PLA4D achieves remarkable performance, generating 4D\nobjects with fine textures, accurate geometry, and coher-\nent motion in significantly less time.\n2. Related work\n3D Generation. Recent advancements in DMs within 2D\ndomains have sparked significant interest in exploring 3D\ngenerative modeling [6‚Äì8, 10, 11, 17, 19, 22, 23, 28, 38,\n40, 42] for content generation. Under given control condi-\ntions (e.g., text prompt or single image), some efforts [25‚Äì\n27, 33, 34] are made to extend 2D DMs from single-\nview images to multiview images to seamlessly integrate\nwith different 3D representation methods (e.g, Nerf [29],\nMesh [16], and 3D Gaussian [20]). However, due to the un-\ncertainty of the diffusion model‚Äôs denoise process, the mul-\ntiview consistency and corresponding camera poses of gen-\nerated images are not guaranteed, leading to artifacts and\ntexture ambiguity in the generated 3D object. Further, some\nworks [30, 43, 44] apply SDS [39] in latent space to extend\nthe 2D DMs to guide 3D generation. Although such SDS-\nbased methods can improve the textural of 3D representa-\ntion, they frequently suffer from Janus-face problems due\nto the lack of comprehensive multiview knowledge. Re-\ncently, some methods [8, 20, 23, 28] have integrated the\nabove two approaches, which use pre-trained multiview dif-\nfusion for SDS. The comprehensive multiview knowledge\nor 3D awareness hidden in the pre-trained model enhances\nthe consistency of 3D representation, yet such SDS-based\nmethods are time-consuming, needing hours to train.\nVideo Generation. Video generation [1, 3, 4, 12‚Äì15, 21],\nincluding text-to-video and image-to-video generation, has\nbeen getting more and more attention recently. The for-\nmer, such as MAV [36] and AYL [4], rely on large amounts\nof high-quality text-to-video data for training to deepen\ntheir understanding of verbs, enabling them to generate rich\nand creative sequences of coherent video frames. The lat-\nter [3, 14] infers subsequent actions of the target object\nbased solely on a given initial frame image, which does not\nsupport flexible control over actions.\n4D Generation. At the current stage, 4D generation is in-\nfluenced by various factors. (1) Representation Methods:\nPrevious methods have mainly been based on NeRF [2, 37,\n50], where its multi-layer MLP architecture facilitates the\ngeneration of smooth 4D surfaces, but requires a signif-\nicant amount of time for training. Recently, some meth-\nods [24, 47] based on 4D GS have emerged. While training\nspeeds have improved, guiding the motion of each Gaus-\nsian point to drive the 4D target raises higher requirements\nfor motion guidance. (2) Motion Guidance Methods: Some\nprevious methods [31, 46, 47] used image-to-video mod-\nels to accomplish the image-to-4D task. However, the gen-\nerated motions do not support user manipulation, signifi-\ncantly limiting usability.\nUsing text-to-video models for\nguidance is a better approach. But current methods, such\nas MAV3D [37] and AYG [24], rely on closed-source video\nmodels [4, 36]. 4D-fy [2] attempts to use the open-source\nvideo model and SDS [39] to distill motion priors, but our\nexperimental results show that this can only provide very\nlimited motion. (3) Training Duration: Current text-to-4D\nmethods are trained directly from a random initialization\nstate based on SDS. Due to inconsistent optimization objec-\ntives for each SDS, a substantial amount of time is required\nfor compromise, leading to generation times that often take\nhours. To address these challenges, we propose PLA4D,\nwhich is based on 4D GS. It uses a text-to-video model to\nprovide pixel-level motion guidance and generates 4D ob-\njects quickly with mesh geometry priors.\n3. Methodology\n3.1. Preliminaries\n4D Gaussian Splatting is derived from 3D GS [20] by ex-\ntending it along the time dimension via another model, such\nas the deformation network. 3D Gaussian involves a collec-\ntion of N Gaussian points, each defined by four attributes:\npositions ¬µi, covariances Œ£i, colors ‚Ñìi, and opacities Œ±i. A\ncommon approach to incorporating time is to add a defor-\nmation network that predicts the attributes of each Gaussian\npoint at each timestep. To render novel views images at time\nœÑ, 4D Gaussians fix time parameter and reproject the 3D\nGaussians onto a 2D image space, obtaining their projection\npositions ¬µ and corresponding covariances ÀÜŒ£i. Point-based\nŒ±-blending rendering [51] is then applied to determine the\ncolor C(p) of image pixel p along a ray r:\nC(p) =\nX\ni‚ààN\n‚ÑìiŒ∑i\ni=1\nY\nj=1\n(1 ‚àíŒ∑j),\n(1)\nŒ∑i = Œ±iexp\n\u0014\n‚àí1\n2(p ‚àíÀÜ¬µi)T ÀÜŒ£i(p ‚àíÀÜ¬µi)\n\u0015\n,\n(2)\nwhere j iterates over the points traversed by the ray r, ‚Ñìi\nand Œ±i donate the color and opacity of the i-th Gaussian.\nÀÜ¬µi is the projection of ¬µi on 2D image plane. Within each\nmoment, the deformation network predicts a variable for\neach Gaussian point‚Äôs attributes and adds it on them, thus\ndriving the 4D object‚Äôs motion across multiple times.\nScore Distillation Sampling (SDS) is widely used in 3D\ngeneration methods [30, 31, 34, 34, 44], which aligns the\n\nùíû\nùíû\nùíû\nVS\nùíû\nùíû\nùíû\nPixel-level Alignment\nOur \nMethods\nRandom\nView\nFront\nView\nFront View\nRendering\nRandom View\nRendering\n4D/3D\nRepresentation\nData\nFlow\nGradient\nFlow\nOur \nModules\nw/o Pixel-level Alignment\nUnknown View \nRenderings\nPixel-level \nAlignment Anchor\nùíûCondition\nImage DM\nImage DM\nVideo DM\nMultiview DM\nVideo DM\nMultiview DM\n‚ÄúA monkey is \nplaying a guitar.‚Äù\n4D GS\nAnchor Video\nStatic \nAlignment\nDynamic \nAlignment\nPLA4D\nT2V Diffusion\nImage-to-Mesh\nDiffusion Model\n4DGS\nMesh\nFocal \nAlignment\nGenerate \nMesh\nGeometry \nAlignment\nTexture \nAlignment\nFront \nView\nRandom \nView\nRendering Focal\nRender\n(a) Static Alignment\nGaussian-Mesh\nContrastive \nLearning\nRender\nFrame of Anchor Video\nT-MV \nRefinemt\nT-MV \nRefinemt\n4DGS\nTime\nTime\nView\nT\nAnchor Video\nMotion \nAlignment\nT\nV\nRender\n(b) Dynamic Alignment\nCondition\nMV Diffusion\nT2V Diffusion\nCondition\n‚ÄúA monkey is \nplaying a guitar.‚Äù\n‚ÄúA monkey is \nplaying a guitar.‚Äù\nT2I SDS\nT2I Diffusion\nCondition\nOptimization  Direction\nFigure 3. Pipeline of PLA4D, which leverages text as the condition and text-generated video as an anchor for 4D generation. (a) Static\nalignment: We propose focal alignment to search for the best focal length for 4D automatically. We also introduce Gaussian-Mesh\nContrastive Learning to provide geometric information for 4D Gaussian in unknown views, explicitly leveraging the geometric priors of\nthe mesh. (b) Dynamic alignment: Across multiple frames, we introduce motion alignment to guide the 4D object‚Äôs motion following\nthe anchor video. Furthermore, we propose Time-Multiview (T-MV) refinement to optimize the motion and quality of the 4D object‚Äôs\nunknown viewpoints, using the prior and the condition of the model that generates the video.\n3D generation process to the 2D DMs training process. In\nthe training of 2D DMs, sample noise œµ from q(x) and add\nit on the data x (e.g., images and videos) with t times un-\ntil q(xt) converges to a Gaussian prior distribution N(0, I).\nThe network œï is trained to predict the removal noise ÀÜœµ for\ndenoising and reconstructing data x. 3D generation meth-\nods set the renderings got from 3D scene representation as\ndata x, and calculate the Mean Squared Error (MSE) to get\nSDS gradient [30]:\n‚àáŒ∏LSDS(x = g(Œ∏)) = Et,œµ\n\u0014\nw(t)(ÀÜœµœï(z, v, t) ‚àíœµ)‚àÇx\n‚àÇŒ∏\n\u0015\n,\n(3)\nwhere t is the timestamp of denoising process, and w(t) is\ntime-dependent weights. z represents the latent of x need-\ning to denoise. v indicates the given conditions, such as text\nprompts and images. The SDS gradients are then backprop-\nagated through the differentiable rendering process g into\nthe 3D representation and update its parameters Œ∏.\nPrevious 4D generation tasks use multiple DMs to ob-\ntain prior information about multiview and motion via SDS.\nFor example, in the absence of pixel-level alignment targets,\nthe SDS gradient directions of each DM might conflict with\neach other. Consider a simple case where only video and\nMV DMs are used, the SDS gradient is:\n‚àáŒ∏LSDS(x = g(Œ∏)) =\nEt,œµ\n\u0014\nw(t)(ÀÜœµœïV (z, v, t) + ÀÜœµœïMV (z, v, t) ‚àí2œµ)‚àÇx\n‚àÇŒ∏\n\u0015\n,\n(4)\nthe overall SDS gradient descent direction should be the\nvector sum of the multi-view DM SDS and video DM SDS\ngradient descents. However, this is not stable. When the\ntwo gradient directions are opposite, the 4D model will be\ncaught in a dilemma, requiring extensive optimization time\nto find a local optimum. As the number of DMs increases,\nthis issue becomes more pronounced.\n3.2. Pipeline of PLA4D\nPLA4D introduces static alignment and dynamic alignment\nmodules to achieve text-driven 4D generation, leveraging\nmultiple DMs, including T2V DM, I2MV DM, and T2I\nDM, as illustrated in Fig. 3. T2V DM is used to generate an\nanchor video and refine the motion of the 4D object, while\nI2MV DM refines the geometry of the 4D object from un-\nknown views. T2I DM bridges the gap between the anchor\nvideo and the pixel-level geometry priors provided by the\nmesh. Inspired by DreamGaussian4D [31], we combine 3D\n\nGaussians with a deformation network to support 4D gener-\nation. Initially, we use an open-source T2V DM to generate\nan anchor video and use Eq.11 of the static alignment to get\n3D Gaussian as initialization for 4D Gaussian. Next, we ap-\nply Eq.11 of the static alignment and Eq. 15 of the dynamic\nalignment modules to optimize the deformation network.\n3.3. Static Alignment Module\nMesh\n(a) Focal Alignment\nrender\nrender\nMSE \nloss\nMask \nloss\nLPIPS\nloss\nMesh\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n4D GS\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n+‚àÜùëì\n(b) Gaussian-Mesh Contrastive Learning\nFigure 4. Focal alignment and Gaussian-Mesh contrastive learn-\ning. (a) We render multiple front-view images and calculate the\nMSE with the first frame for searching the matched focal. (b) We\ncollect two sets of images: one of 4D Gaussians and another of\nmesh renderings, both captured using the same random camera\nposes. We include the first frame of the anchor video and the\nfront-view renderings in these two sets. Then, we calculate the\nMSE loss, Mask loss, and LPIPS loss between the corresponding\nimages.\nFocal Alignment for Texture Alignment. PLA4D aims\nto use text-generated video as the pixel-level alignment an-\nchor for 4D generation, which needs a matched focal for\nframes.\nHowever, anchor frames‚Äô focals are unknown.\nTherefore, we propose focal alignment to search for the\nmatched focal f. Specifically, we start with the video syn-\nthesis. Given the text prompt v, PLA4D applies a text-to-\nvideo DM Gvid to create a video {It\nvid}T = Gvid(œµ; v) with\nT frames. œµ is a random noise. Because the view angles in\nthe anchor frames are relatively fixed, we set the video‚Äôs\nview c as the 4D object‚Äôs front perspective. Next, at the\nbeginning of each timestep t, we need to fix the time pa-\nrameter of 4D Gaussian and compare its front view render-\ning and It\nvid to search f ‚Ä≤, as shown in Fig. 4(a). Hence, we\nintroduce CRM [45], an image-to-mesh feed-forward 3D\ngeneration model, to generate a mesh œàt based on It\nvid. We\nrender œàt‚Äôs front-view images {xœàt}M with M different fo-\ncals iterated from f ‚Ä≤ + ‚àÜfmin to f ‚Ä≤ + ‚àÜfmax, where f ‚Ä≤ is\nan initial focal length. We calculate the Mean Squared Error\n(MSE) between It\nvid and {xœàt}M for searching the matched\nfocal f ‚Ä≤:\nf = arg min\nf ‚Ä≤\nX\nH,W\n||xf ‚Ä≤\nœàt ‚àíIt\nvid||2\n2.\n(5)\nAt each timestep, with the corresponding focal f, we\npropose Gaussian-Mesh contrastive learning to align the\nfront-view 4D Gaussian renderings to the frames to achieve\ntexture alignment, which is composed of three losses: (I)\nLMSE for aligning the pixel-level similarity, (II) LMask for\nreducing the floaters, and (III) LLPIPS for enhancing the vi-\nsual perceptual perception. In particular, we use the MSE\nloss between front view c rendering xŒ∏ of 4D Gaussians and\nIt\nvid as follows:\nLMSE(xc\nŒ∏t, It\nvid) =\nX\nH,W\n||xc\nŒ∏t ‚àíIt\nvid||2\n2.\n(6)\nBesides, to reduce the floaters, we also use the transpar-\nent output Œ± of 4D Gaussians as the mask and calculate the\nmask loss:\nLMask(xc\nŒ∏t, It\nvid) =\nX\nH,W\n||Œ±c\nŒ∏t ‚àíŒ±t\nvid||2\n2,\n(7)\nwhere Œ±t\nvid is the alpha channel of It\nvid.\nBesides,\nwe introduce Learned Perceptual Image Patch Similarity\n(LPIPS) [48], which is a metric used to measure percep-\ntual differences between images. We apply LPIPS loss be-\ntween xc\nŒ∏t and It\nvid to enhance the visual quality of textures.\nLLPIPS needs an encoder (i.e., VGG [35]) to extract feature\nstack from l layers and unit-normalize in the channel di-\nmension, and calculate the MSE between features extracted\nfrom each layer:\nLLPIPS(xc\nŒ∏t, It\nvid) =\nX\nl\n1\nHlWl\nX\nHlWl\n||zc\nŒ∏t ‚àízt\nvid||2\n2.\n(8)\nNow, we can get the texture alignment loss LTA:\nLTA = LMSE(xc\nŒ∏t, It\nvid)\n+ LMask(xc\nŒ∏t, It\nvid) + ŒªLLPIPS(xc\nŒ∏t, It\nvid),\n(9)\nwhere Œª is the scaling weight for balance.\nGaussian-Mesh Contrastive Learning for Geometry\nAlignment. Thanks to our focal alignment method, we ob-\ntain accurate focal lengths, enabling us to leverage video\nfor primary viewpoint texture information and mesh œàt got\nbefore for geometric information from other viewpoints.\nThus, we propose Gaussian-Mesh Contrastive Learning, as\nshown in Fig. 4(b). We randomly choose Nc‚Ä≤ camera poses\n{c‚Ä≤\ni}Nc‚Ä≤ , and each corresponding focal is f + ‚àÜf, ‚àÜf is a\nslight and random perturbation. Different from multiview\nDMs‚Äô productions, the rendered images of mesh œàt are ob-\ntained from one entity, that naturally has multiview consis-\ntency. Besides, this method can provide references from\nany number of different viewpoints for training 4D Gaus-\nsian Œ∏, such density data can avoid artifacts in renderings.\n\n‚ÄúA baby panda eating \nice cream.‚Äù\n‚ÄúA squirrel riding a \nmotorcycle.‚Äù\n‚ÄúA humanoid robot \nplaying the violin.‚Äù\n‚ÄúA panda is dancing.‚Äù\n‚ÄúA goat is laughing.‚Äù\n‚ÄúA human skeleton \ndrinking wine.‚Äù\n‚ÄúA silver humanoid \nrobot.‚Äù\n‚ÄúAn alien playing \nthe piano.‚Äù\nFigure 5. Visualization results of PLA4D. The 4D objects generated by PLA4D not only rigorously follow the semantics but also feature-\nrich dynamics and excellent geometric consistency. More importantly, PLA4D generates each sample in approximately 15 minutes.\nThe geometry alignment loss LGA can be summarized as:\nLGA =\nNc‚Ä≤\nX\ni=1\n(LMSE(xc‚Ä≤\ni\nŒ∏t, xc‚Ä≤\ni\nœàt)\n+ LMask(xc‚Ä≤\ni\nŒ∏t, xc‚Ä≤\ni\nœàt) + ŒªLLPIPS(xc‚Ä≤\ni\nŒ∏txc‚Ä≤\ni\nœàt)).\n(10)\nBesides, we additionally introduce a T2I DM using the SDS\nmethod to enhance the control of the text prompt over the\ncurrent object. Overall, the static alignment loss Lstatic is\ndenoted as:\nLstatic = LTA + LGA + LT2I.\n(11)\n3.4. Dynamic Alignment Module\nMotion Alignment. With our focal alignment method, we\ncan directly use the anchor video as the pixel-level align-\nment targets to provide motion guidance. Thus, we mini-\nmize the motion alignment loss LMA to inject dynamics:\nLMA = 1\nT\nT\nX\nt=1\nX\nH,W\n||xc\nŒ∏t ‚àíIt\nvid||2\n2,\n(12)\nwhere xc\nŒ∏t is the front-view renderings of 4D Gaussian at\ntime t. It\nvid is the corresponding frame of anchor video.\nTime and Multiview Refinement. Despite following the\n\nGSGEN\n4\nSD+4DGen Drea\nSD+DGS4D\n‚ÄúA panda \ndancing.\nMAV3D\nA\n‚Äú3D rendering of a fox \nplaying videogame.‚Äù\nOurs\n4D-fy\nDream-in-4D\nSD+4DGen\nSD+DGS4D\nOurs\n4D-fy\nSD+4DGen Dream-in-4D\nSD+DGS4D\n‚ÄúA panda is \ndancing.‚Äù\nMAV3D\nAYG\nMAV3D\nAYG\nFigure 6. Comparison of PLA4D with text-to-4D and image-to-4D methods. Top: The pixel-level geometric priors provided by Gaussian-\nMesh effectively help PLA4D avoid multi-face artifacts. The addition of the focal alignment module corrects the erroneous primary\nviewpoint projection relationships observed in image-to-4D methods. Bottom: With pixel-level alignment, PLA4D achieves the maximum\nmotion range across 8-frame intervals, producing semantically coherent motion rather than pixel jittering.\naforementioned technical steps to obtain a dynamic and ge-\nometrically reasonable 4D target, surface splitting may still\noccur. The Gaussian points with predicted locations are too\nfar apart, and the scale cannot bridge the gap between these\npoints. This indicates that some unfamiliar viewpoints still\nlack temporal continuity and geometric consistency. Thus,\nwe propose the Time-Multiview (T-MV) Refinement, which\nuses the text prompt as a condition to optimize motion via\nvideo DM œïV , and the anchor video as a condition to op-\ntimize geometry via multiview DM œïMV , ensuring stable\nperformance across multiple timestamps and random view-\npoints. The LT-MV includes LTime and LMV:\nLTime = 1\nT\nT\nX\nt=1\nX\nH,W\nw(œÑ)||œµœïV (Œ±œÑxc‚Ä≤\nŒ∏t + œÉœÑœµ; C; œÑ) ‚àíœµ||2\n2,\n(13)\nLMV =\n1\nNc‚Ä≤\nNc‚Ä≤\nX\ni=1\nX\nH,W\nw(œÑ)||œµœïMV (Œ±œÑxc‚Ä≤\ni\nŒ∏t+œÉœÑœµ; It\nvid; œÑ)‚àíœµ||2\n2,\n(14)\nwhere œÑ is the timestep of DM, w(œÑ), Œ±œÑ and œÉœÑ are pa-\nrameters depends on the timestep œÑ.\nHere, we can get\nLT-MV = LTime +LMV. In summary, we ultimately derive\nthe dynamic alignment loss Ldynamic:\nLdynamic = LMA + LT-MV.\n(15)\n4. Experiments\nBaselines. For a comprehensive comparison, we evaluate\nour method alongside both text-to-4D methods [2, 24, 37,\n46, 50] and image-to-4D method [31]. For the image-to-\n4D methods, we use Stable Diffusion 2.1 [32] with identical\nprompts to generate images, which are then used to generate\n4D objects. Additionally, we compare methods based on\nboth NeRF and Gaussian representations. For the closed-\nsource methods MAV3D [37] and AYG [24], we perform\ncomparisons using overlapping examples.\nComparative Studies.\nWe present a large number of\nPLA4D-generated results in Fig. 5. Thanks to the pixel-\nlevel alignment methods, the 4D objects move beyond the\nrigid rendering style of previous 4D generation methods,\nexhibiting a stronger photorealistic style.\nAdditionally,\ndue to explicit motion guidance provided by the reference\nvideo, the target demonstrates detailed motion differences at\neach timestamp. Furthermore, with our proposed Gaussian-\nMesh contrastive learning method, PLA4D‚Äôs products also\nexhibit excellent geometric consistency. More importantly,\neach sample can be generated in just 15 minutes with 0.6K\niterations.\nCompared with other 4D generation methods, PLA4D\ndemonstrates superior geometric structure, smooth motion,\nand semantic consistency, as shown in Fig. 6. (1) Geom-\netry: due to the implicit distillation of geometric priors in\nDream-in-4D, the generated object suffers from the Janus-\nface problem. The same phenomenon can also be observed\nin the samples from MAV3D. (2) Motion: due to the im-\nplicit distillation of motion priors, even when comparing\nthe first and tenth frames, previous methods exhibit only\nsmall motion amplitudes, making it difficult to align with\nthe motion described in the prompt. (3) Semantic consis-\ntency: Although 4D-fy does not suffer from the Janus-face\n\nùúè= 1\nùúè = 10\nùê∂=0¬∞\nùê∂=180¬∞\nPLA4D\nùúè= 1\nùúè = 10\nw/o T-MV\nRefinement\nùúè= 1\nùúè = 10\nw/o Motion\nAlignment\nùúè= 1\nùúè = 10\nw/o Focal\nAlignment\nùúè= 1\nùúè = 10\nw/o Gaussian-Mesh \nContrastive Learning\nFigure 7. Ablation studies. If no focal alignment or Gaussian-Mesh contrastive learning, the 4D object loses its detailed texture and correct\ngeometry. Without motion alignment, a 4D object degenerates into a static object. Absent T-MV refinement, the displacement of Gaussians\ncauses surface tearing.\nMethods\nRepresentation\nGeneration Time\nIterations\nAnimate124 [49]\nNeRF\n-\n20K\n4DGen [46]\nNeRF\n3.0 hr\n3K\nConsistent4D [18]\nNeRF\n2.5 hr\n10K\nDreamGaussian4D [31]\nGaussians\n6.5 min\n0.7K\n4D-fy [2]\nNeRF\n23 hr\n120K\nDream-in-4D [50]\nNeRF\n10.5 hr\n20K\nMAV3D [37]\nNeRF\n6.5 hr\n12K\nAYG [24]\nGaussians\n-\n20K\nPLA4D (ours)\nGaussians\n15 min\n0.6K\nTable 1. Speed comparison. The upper part presents image-to-4D\nmethods, while the lower part collects text-to-4D methods..\nModel\nMotion\nGeometry\nSemantic consistency\n4D-fy [2]\n14.19 %\n21.10 %\n11.76 %\nDream-in-4D [50]\n34.95 %\n27.68 %\n32.18 %\nPLA4D\n50.86 %\n51.22 %\n56.06 %\nTable 2. User study. PLA4D receives the most praise from users\nfor its consistency in motion, geometry, and semantics.\nproblem, its generated outputs exhibit semantic inconsis-\ntencies. Due to the conflicts arising from the simultane-\nous optimization of multiple SDS objectives, where the op-\ntimization directions for geometry, motion, and semantics\ncompete with each other, balancing these factors becomes\nchallenging. PLA4D effectively alleviates this issue by em-\nploying pixel-level alignment.\nThe unified structure of NeRF with its MLP structure is\nnot sensitive to each optimization step, allowing for better\ntexture generation. In contrast, the Gaussian model opti-\nmizes each Gaussian point independently, making it more\nsensitive to each optimization step [9, 39, 41]. This struc-\ntural difference introduces greater challenges in optimizing\ntexture. However, PLA4D can still maintain high-quality\ntexture by leveraging the T-MV refinement.\nAblation Study. In Fig. 7, we demonstrate the role of each\nmodule in PLA4D for text-to-4D generation. Without the\nfocal alignment method, using unmatched focal f, Gaus-\nsians can not learn the correct attributes of points to align\nto the generated frames. Both the geometry and motion of\n4D objects are compromised. Without Gaussian-Mesh con-\ntrastive learning, the geometry structure and texture in un-\nknown views can not learned from multiview DM prior in\nsuch a short training time. Without motion alignment, the\n4D object degrades into a static 3D object. Without T-MV\nrefinement, dynamic multiview renderings of 4D objects re-\nsult in surface cracks.\nEfficiency Study. We compare the time overhead of mul-\ntiple 4D generation methods proposed for image-to-4d and\ntext-to-4d tasks, as shown in Tab. 1. It can be observed that\nprevious 4D generation tasks overly rely on SDS, which re-\nquires extensive training (over 10K iterations) by implicitly\naligning various diffusion models to generate 4D objects. In\ncontrast, PLA4D uses explicit pixel-level alignment, result-\ning in better textures, geometry, and motion for 4D targets\nwith significantly lower time overhead.\nUser Study. To further evaluate the quality of our 4D gen-\neration objects, we conducted a user study on 30 partici-\npants. Specifically, we investigated users‚Äô preference of 4D-\nfy [2], Dream-in-4D [50], and our PLA4D in terms of mo-\ntion, geometry, and semantic consistency. We didn‚Äôt include\nMAV3D [37] and AYG [24] because they are closed-source.\nAs shown in Tab. 2, our PLA4D surpasses other comparison\nmethods in all perspectives, indicating our superior perfor-\nmance on motion, geometry, and semantic consistency.\nLimitation. PLA4D uses video as an anchor, relying on the\nperformance of T2V DM. As the motion range of the text-\ndriven generated video increases and the video duration ex-\ntends, PLA4D will produce improved motion performance.\n5. Conclusion\nIn this paper, we introduce PLA4D, a framework that lever-\nages text-driven generated video as explicit pixel alignment\ntargets for 4D generation, anchoring the rendering process\nconditioned by different DMs. We propose various mod-\n\nules to achieve such anchoring: we propose Gaussian-Mesh\ncontrastive learning and focal alignment to ensure geometry\nconsistency from the mesh and produce textures as detailed\nas those in the generated video frames. Additionally, we\nhave developed a novel motion alignment method and T-\nMV refinement technology to optimize dynamic surfaces.\nCompared to existing methods, PLA4D effectively avoids\nJanus-face problem and generates 4D targets with accu-\nrate geometry and smooth motion in significantly less time.\nFurthermore, PLA4D is constructed entirely using existing\nopen-source models, eliminating the need for pre-training\nany DMs. This flexible architecture allows the community\nto freely replace or upgrade components to achieve state-\nof-the-art performance. We aim for PLA4D to become an\naccessible, user-friendly, and promising tool for 4D digital\ncontent creation.\n\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023.\n[2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon\nWetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov,\nJeong Joon Park, Andrea Tagliasacchi, and David B Lindell.\n4d-fy: Text-to-4d generation using hybrid score distillation\nsampling. arXiv preprint arXiv:2311.17984, 2023.\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023.\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563‚Äì22575, 2023.\n[5] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 130‚Äì141, 2023.\n[6] Guikun Chen and Wenguan Wang. A survey on 3d gaussian\nsplatting. arXiv preprint arXiv:2401.03890, 2024.\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 22246‚Äì22256, 2023.\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. IT3D: improved\ntext-to-3d generation with explicit view synthesis. In Thirty-\nEighth AAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2024, Fourteenth Symposium on\nEducational Advances in Artificial Intelligence, EAAI 2014,\nFebruary 20-27, 2024, Vancouver, Canada, pages 1237‚Äì\n1244. AAAI Press, 2024.\n[9] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu.\nText-to-3d using gaussian splatting.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 21401‚Äì21412, 2024.\n[10] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637‚Äì20647, 2023.\n[11] Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, and\nXiaoli Liu.\nMetadreamer:\nEfficient text-to-3d creation\nwith disentangling geometry and texture.\narXiv preprint\narXiv:2311.10123, 2023.\n[12] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 22930‚Äì22941, 2023.\n[13] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709, 2023.\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023.\n[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022.\n[16] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDon-\nald, and Werner Stuetzle. Mesh optimization. In Proceedings\nof the 20th annual conference on Computer graphics and in-\nteractive techniques, pages 19‚Äì26, 1993.\n[17] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023.\n[18] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao\nYao.\nConsistent4d: Consistent 360 {\\deg} dynamic ob-\nject generation from monocular video.\narXiv preprint\narXiv:2311.02848, 2023.\n[19] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski.\nNoise-free score distillation.\narXiv preprint\narXiv:2310.17590, 2023.\n[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk¬®uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4):1‚Äì14, 2023.\n[21] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 15954‚Äì15964, 2023.\n[22] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiao-\ngang Xu, and Yingcong Chen.\nLuciddreamer: Towards\nhigh-fidelity text-to-3d generation via interval score match-\ning. arXiv preprint arXiv:2311.11284, 2023.\n[23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300‚Äì309, 2023.\n[24] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fi-\ndler, and Karsten Kreis. Align your gaussians: Text-to-4d\nwith dynamic 3d gaussians and composed diffusion models.\narXiv preprint arXiv:2312.13763, 2023.\n\n[25] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund\nVarma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single\nimage to 3d mesh in 45 seconds without per-shape optimiza-\ntion. Advances in Neural Information Processing Systems,\n36, 2024.\n[26] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298‚Äì9309, 2023.\n[27] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023.\n[28] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\ntext-to-3d object synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 17946‚Äì\n17956, 2023.\n[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99‚Äì106, 2021.\n[30] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022.\n[31] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao,\nGang Zeng, and Ziwei Liu.\nDreamgaussian4d: Genera-\ntive 4d gaussian splatting. arXiv preprint arXiv:2312.17142,\n2023.\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¬®orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684‚Äì10695, 2022.\n[33] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n[34] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023.\n[35] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[36] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022.\n[37] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv preprint arXiv:2301.11280,\n2023.\n[38] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023.\n[39] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023.\n[40] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023.\n[41] Trapoom Ukarapol and Kevin Pruvost. Gradeadreamer: En-\nhanced text-to-3d generation using gaussian splatting and\nmulti-view diffusion.\narXiv preprint arXiv:2406.09850,\n2024.\n[42] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619‚Äì12629, 2023.\n[43] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619‚Äì12629, 2023.\n[44] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. Advances in Neural Information Processing Systems,\n36, 2024.\n[45] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xi-\nang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su,\nand Jun Zhu.\nCrm: Single image to 3d textured mesh\nwith convolutional reconstruction model.\narXiv preprint\narXiv:2403.05034, 2024.\n[46] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao,\nand Yunchao Wei.\n4dgen:\nGrounded 4d content gen-\neration with spatial-temporal consistency.\narXiv preprint\narXiv:2312.17225, 2023.\n[47] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian\nLin, Hao Zhu, Weiming Hu, Xun Cao, and Yao Yao. Stag4d:\nSpatial-temporal anchored generative 4d gaussians. In Eu-\nropean Conference on Computer Vision, pages 163‚Äì179.\nSpringer, 2025.\n[48] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586‚Äì595, 2018.\n[49] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhen-\nguo Li, and Gim Hee Lee. Animate124: Animating one im-\nage to 4d dynamic scene. arXiv preprint arXiv:2311.14603,\n2023.\n[50] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar\nHilliges, and Shalini De Mello.\nA unified approach for\ntext-and image-guided 4d scene generation. arXiv preprint\narXiv:2311.16854, 2023.\n\n[51] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa volume splatting. In Proceedings Visu-\nalization, 2001. VIS‚Äô01., pages 29‚Äì538. IEEE, 2001.",
    "pdf_filename": "PLA4D_Pixel-Level_Alignments_for_Text-to-4D_Gaussian_Splatting.pdf"
}