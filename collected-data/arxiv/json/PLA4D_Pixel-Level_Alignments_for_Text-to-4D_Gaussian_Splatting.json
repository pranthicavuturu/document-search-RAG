{
    "title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
    "abstract": "level, a motion alignment technique and T-MV refinement method are employed to enforce both pose alignment and Previous text-to-4D methods have leveraged multiple motioncontinuityacrossunknownviewpoints,ensuringin- Score Distillation Sampling (SDS) techniques, combining trinsicgeometricconsistencyacrossviews.Withsuchpixel- motion priors from video-based diffusion models (DMs) levelmulti-DMalignment,ourPLA4Dframeworkisableto with geometric priors from multiview DMs to implicitly generate 4D objects with superior geometric, motion, and guide 4D renderings. However, differences in these priors semanticconsistency. Fullyimplementedwithopen-source resultinconflictinggradientdirectionsduringoptimization, tools,PLA4Doffersanefficientandaccessiblesolutionfor causingtrade-offsbetweenmotionfidelityandgeometryac- high-quality 4D digital content creation with significantly curacy, andrequiringsubstantialoptimizationtimetorec- reducedgenerationtime. oncile the models. In this paper, we introduce Pixel-Level Alignment for text-driven 4D Gaussian splatting (PLA4D) to resolve this motion-geometry conflict. PLA4D provides 1.Introduction ananchorreference,i.e.,text-generatedvideo,toalignthe rendering process conditioned by different DMs in pixel Text-to-4D content generation has significant potential in space. For static alignment, our approach introduces a applicationsrangingfromgameproductiontoautonomous focal alignment method and Gaussian-Mesh contrastive driving. However, this task remains challenging due to learning to iteratively adjust focal lengths and provide ex- the need to generate high-quality geometry and textures, 4202 voN 91 ]VC.sc[ 4v75991.5042:viXra noitoM weivitluM",
    "body": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting\nQiaoweiMiao JinshengQuan KehanLi\nZhejiangUniversity ZhejiangUniversity ZhejiangUniversity\nHangzhou,China Hangzhou,China Hangzhou,China\nqiaoweimiao@zju.edu.cn jinshengquancv@gmail.com kehanli@zju.edu.cn\nYaweiLuo*\nZhejiangUniversity\nHangzhou,China\nyaweiluo@zju.edu.cn\nA monkey is playing guitar. A clown is crying at a childrens' birthday party.\nA tiger is sleepy, 4K, high quality. An polar bear wearing sunglasses and drinking a coca cola.\nFigure1.4DobjectsgeneratedbyPLA4D.PLA4Dproduces4Dcontentwithgeometricconsistencyandsmooth,video-likemotionthat\nalignspreciselywiththetextprompt,withinarapid15-minuteprocessingtime.\nAbstract plicit geometric priors at each timestep. At the dynamic\nlevel, a motion alignment technique and T-MV refinement\nmethod are employed to enforce both pose alignment and\nPrevious text-to-4D methods have leveraged multiple\nmotioncontinuityacrossunknownviewpoints,ensuringin-\nScore Distillation Sampling (SDS) techniques, combining\ntrinsicgeometricconsistencyacrossviews.Withsuchpixel-\nmotion priors from video-based diffusion models (DMs)\nlevelmulti-DMalignment,ourPLA4Dframeworkisableto\nwith geometric priors from multiview DMs to implicitly\ngenerate 4D objects with superior geometric, motion, and\nguide 4D renderings. However, differences in these priors\nsemanticconsistency. Fullyimplementedwithopen-source\nresultinconflictinggradientdirectionsduringoptimization,\ntools,PLA4Doffersanefficientandaccessiblesolutionfor\ncausingtrade-offsbetweenmotionfidelityandgeometryac-\nhigh-quality 4D digital content creation with significantly\ncuracy, andrequiringsubstantialoptimizationtimetorec-\nreducedgenerationtime.\noncile the models. In this paper, we introduce Pixel-Level\nAlignment for text-driven 4D Gaussian splatting (PLA4D)\nto resolve this motion-geometry conflict. PLA4D provides\n1.Introduction\nananchorreference,i.e.,text-generatedvideo,toalignthe\nrendering process conditioned by different DMs in pixel Text-to-4D content generation has significant potential in\nspace. For static alignment, our approach introduces a applicationsrangingfromgameproductiontoautonomous\nfocal alignment method and Gaussian-Mesh contrastive driving. However, this task remains challenging due to\nlearning to iteratively adjust focal lengths and provide ex- the need to generate high-quality geometry and textures,\n4202\nvoN\n91\n]VC.sc[\n4v75991.5042:viXra\nnoitoM\nweivitluM\npla‚Äú yA i nm go an k ge uy i ti as r .‚Äù FIn ratr ma- e FIn rate mr- e\nT2V Diffusion Anchor Video PLA4D 4D GS\npla‚Äú yA i nm go an k ge uy i ti as r .‚Äù Condition T2I SDS pla‚Äú yA i nm go an k ge uy i ti as r .‚Äù Condition\nFrame of Anchor Video Texture T2I Diffusion RT ef- iM neV m t\nAlignment Render\nTime T T2V Diffusion\nFront\nAliF go nc mal e nt RendeV rinie gw Focal Ga Cu Los4 ensD ai ta r rG a nn s i-S ntM i gve es h tnemngilA yrtemoeG Time Render\nT\nAM ligo nt mio Cen n ot\nndition Anchor Video\nRender View 4DGS\nG Men ee sr hate RT ef- iM neV\nm t\nRandom\nDIm iffa ug se i- ot no - MM oe dsh el (a) Static Alignment Mesh View (b) Dynamic Alignment V MV Diffusion\nOur Our Random Front FrontView RandomView 4D/3D Data Gradient\nModules Methods View View Rendering Rendering Representation Flow Flow\nw/o Pixel-level Alignment Pixel-level Alignment text-to-4D content creation, dubbed PLA4D (Pixel-Level\nImage DM Image DM AlignmentsforText-to-4DGaussianSplatting),whichgen-\nùíû ùíû erates4Dobjectswithvideo-likesmoothmotionfromtext\nVS in exceptionally short time. Our core idea is to shift from\nimplicitlatent-levelalignmentstoexplicitpixel-levelalign-\nment. By using text-generated video as an anchor, we en-\nùíû ùíû ùíû ùíû\nsure that rendered images are simultaneously aligned with\nMultiview DM Video DM Multiview DM Video DM both prompt and pixel representations across the priors of\nUnknown View ùíûCondition Pixel-level multipleDMs. Toachievethis,weapproachtheproblemat\nRenderings Optimization Direction Alignment Anchor staticanddynamiclevels,witheachlevelincorporatingsev-\neralnovelmodules. Inthestaticalignmentmodule,wein-\nFigure 2. Without offering an anchor reference in pixel space,\ntroducetheFocalAlignmentmoduletoestimatethecorre-\nmultipleSDSaligneachrenderingtotheirrespectivepriors,which\nmaynotbeconsistentacrossdifferentdiffusionmodelpriors,re- spondingfocallengthofeachgeneratedframe,whichgen-\nquiringsignificanttimeforreconciliationtogeneratea4Dresult. erates a reference mesh corresponding to the video frame\nWiththeanchorreferenceinpixelspace,however,eachSDScan by an image-to-mesh diffusion model. It then estimates\noptimizethe4Dgeometryandmotionrepresentationaccordingto thefocallengthofeachgeneratedframebycalculatingthe\nitsrespectivepriormoreeffectively. similarity between mesh renderings and video frames at\ndifferent focal lengths. With the correct focal length, the\nalongside coherent object animations aligned with textual current frame can accurately supervise the primary view-\nprompts. Existing methods in text-to-4D synthesis, such pointrenderingof4Datthecorrespondingtimestep. Con-\nasMAV3D[37]and4D-fy[2],oftenemployNeuralRadi- sequently,weintroduceGaussian-MeshContrastiveLearn-\nanceFields(NeRF)[29]. MAV3Dachievestext-to-4Dgen- ing, which utilizes the mesh during the focal length align-\neration by distilling text-to-video diffusion models (DMs) ment to provide geometric supervision, thus maintaining\nonto a Hexplane[5], while 4D-fy utilizes multiple pre- geometricconsistencyforunknownviews.\ntrainedDMswithhybridscoredistillationsampling(SDS) In the dynamic alignment module, we need to consider\ntogeneratecompelling4Dcontent.Recentapproaches,like bothtemporalandmultiviewconsistency.Weguidethemo-\nAYG[24],leverage3DGaussiansdeformedbyaneuralnet- tionof4Doutputstoalignwiththeanchorvideo,transfer-\nwork and incorporate multiple SDS modules from text-to- ring the motion guidance from the video to the 4D target.\nimage, text-to-multiview, andtext-to-videoDMs[4,34]to Simultaneously,weensurecoherentmotionacrossdifferent\nguidegeometryandmotiongeneration. viewpoints. Toachievemotioncontinuity,werandomlyse-\nAcommonalityamongtheabovemethodsistheirheavy lectaviewpointforrenderingmultipletimestepsandalign\nreliance on the SDS of multiple DMs to provide priors thiswiththetextconditionsundertheguidanceofavideo\nfor guiding the generation of geometry and motion. How- DM that generates the anchor video. This approach en-\never, the reliance on SDS-based methods brings consider- ables the geometry and texture learned from static align-\nable challenges. As shown in Fig. 2, the goal of SDS can ment to smoothly extend across the temporal dimension.\nbe viewed as leveraging the priors from DMs to implicitly Besides,themotionperformanceof4Dobjectsinunknown\nalign rendered images with conditions C. However, due viewscanalignwiththeanchorvideo. Tofurtherreinforce\nto the different source datasets each DM is pre-trained on, consistency in unseen viewpoints, we randomly choose a\nevenwiththesamecondition,theresultsgeneratedbydif- timestepandrenderimagesacrossmultipleviews,enhanc-\nferent DMs vary. This discrepancy can lead to conflicts ing their consistency with the corresponding frame of an-\nwhen multiple DMs are jointly optimized using SDS, re- chorvideoundertheguidanceofamultiviewDM.Through\nsulting in two primary issues: (1) Motion-geometry trade- thecombinedeffectsofstaticalignmentanddynamicalign-\noff. WhenvideoDMsandmultiviewDMshaveconflicting ment modules, PLA4D enables text-driven-generated 4D\noptimizationtargets,itbecomeschallengingtogenerate4D objects to have geometric consistency, smooth and seman-\noutputs that balance both motion and geometry. Since the ticallyalignedmotion,andminimaltimeoverhead.\nSDS implicitly aligns rendered images with the condition, PLA4D can generate a wide range of dynamic objects\nwe cannot easily adjust the scale of their losses for a bet- rapidly,producingdiverse,vivid,andintricatedetailswhile\ntermotionorabettergeometry. (2)Excessiveoptimization maintaining geometry consistency, as shown in Fig. 1. In\ntime. WhenconflictsarisebetweenmultipleSDSlosses,a summary,ourcontributionsareasfollows:\nsubstantialamountoftimeisrequiredtobalancethesecon- ‚Ä¢ We present a novel text-driven 4D generation frame-\nflictingobjectives,whichisoneofthemainreasonsforthe work that leverages explicit anchor reference, i.e., text-\ntime-consumingnatureofcurrentmethods. generated video, to align the rendering process condi-\nIn this paper, we introduce a novel framework for tioned by different DMs in pixel space, eliminating the\noptimizationconflictsofdifferentDMs. ods[24,47]basedon4DGShaveemerged. Whiletraining\n‚Ä¢ We propose focal alignment and Gaussian-Mesh con- speeds have improved, guiding the motion of each Gaus-\ntrastivelearning,whichautomaticallyfindsthebestfocal sianpointtodrivethe4Dtargetraiseshigherrequirements\nparameterscorrespondingtoreferencepixelsandexplic- formotionguidance. (2)MotionGuidanceMethods: Some\nitlyprovidesgeometryguidancefor4D. previous methods [31, 46, 47] used image-to-video mod-\n‚Ä¢ We propose a motion alignment method and Time- elstoaccomplishtheimage-to-4Dtask. However,thegen-\nMultiview refinement modules to optimize 4D, ensuring erated motions do not support user manipulation, signifi-\nvideo-like,largemotionsalignedwithtextualsemantics. cantly limiting usability. Using text-to-video models for\n‚Ä¢ PLA4Dachievesremarkableperformance,generating4D guidance is a better approach. But current methods, such\nobjectswithfinetextures, accurategeometry, andcoher- asMAV3D[37]andAYG[24],relyonclosed-sourcevideo\nentmotioninsignificantlylesstime. models [4, 36]. 4D-fy [2] attempts to use the open-source\nvideomodelandSDS[39]todistillmotionpriors, butour\n2.Relatedwork experimental results show that this can only provide very\nlimited motion. (3) Training Duration: Current text-to-4D\n3D Generation. Recent advancements in DMs within 2D\nmethods are trained directly from a random initialization\ndomains have sparked significant interest in exploring 3D\nstatebasedonSDS.Duetoinconsistentoptimizationobjec-\ngenerative modeling [6‚Äì8, 10, 11, 17, 19, 22, 23, 28, 38,\ntivesforeachSDS,asubstantialamountoftimeisrequired\n40,42]forcontentgeneration. Undergivencontrolcondi-\nforcompromise,leadingtogenerationtimesthatoftentake\ntions(e.g.,textpromptorsingleimage),someefforts[25‚Äì\nhours. To address these challenges, we propose PLA4D,\n27, 33, 34] are made to extend 2D DMs from single-\nwhich is based on 4D GS. It uses a text-to-video model to\nview images to multiview images to seamlessly integrate\nprovide pixel-level motion guidance and generates 4D ob-\nwith different 3D representation methods (e.g, Nerf [29],\njectsquicklywithmeshgeometrypriors.\nMesh[16],and3DGaussian[20]). However,duetotheun-\ncertaintyofthediffusionmodel‚Äôsdenoiseprocess,themul- 3.Methodology\ntiviewconsistencyandcorrespondingcameraposesofgen-\nerated images are not guaranteed, leading to artifacts and 3.1.Preliminaries\ntextureambiguityinthegenerated3Dobject.Further,some\n4DGaussianSplattingisderivedfrom3DGS[20]byex-\nworks[30,43,44]applySDS [39]inlatentspacetoextend\ntendingitalongthetimedimensionviaanothermodel,such\nthe2DDMstoguide3Dgeneration. AlthoughsuchSDS-\nasthedeformationnetwork. 3DGaussianinvolvesacollec-\nbased methods can improve the textural of 3D representa-\ntionofN Gaussianpoints,eachdefinedbyfourattributes:\ntion, they frequently suffer from Janus-face problems due\npositions¬µ ,covariancesŒ£ ,colors‚Ñì ,andopacitiesŒ± . A\nto the lack of comprehensive multiview knowledge. Re- i i i i\ncommon approach to incorporating time is to add a defor-\ncently, some methods [8, 20, 23, 28] have integrated the\nmationnetworkthatpredictstheattributesofeachGaussian\nabovetwoapproaches,whichusepre-trainedmultiviewdif-\npointateachtimestep.Torendernovelviewsimagesattime\nfusion for SDS. The comprehensive multiview knowledge\nœÑ, 4D Gaussians fix time parameter and reproject the 3D\nor3Dawarenesshiddeninthepre-trainedmodelenhances\nGaussiansontoa2Dimagespace,obtainingtheirprojection\nthe consistency of 3D representation, yet such SDS-based positions¬µandcorrespondingcovariancesŒ£ÀÜ . Point-based\nmethodsaretime-consuming,needinghourstotrain. i\nŒ±-blendingrendering[51]isthenappliedtodeterminethe\nVideo Generation. Video generation [1, 3, 4, 12‚Äì15, 21],\ncolorC(p)ofimagepixelpalongarayr:\nincludingtext-to-videoandimage-to-videogeneration,has\nbeen getting more and more attention recently. The for- i=1\n(cid:88) (cid:89)\nmer,suchasMAV[36]andAYL[4],relyonlargeamounts C(p)= ‚Ñì Œ∑ (1‚àíŒ∑ ), (1)\ni i j\nof high-quality text-to-video data for training to deepen i‚ààN j=1\ntheirunderstandingofverbs,enablingthemtogeneraterich (cid:20) 1 (cid:21)\nand creative sequences of coherent video frames. The lat- Œ∑ i =Œ± iexp ‚àí 2(p‚àí¬µÀÜ i)TŒ£ÀÜ i(p‚àí¬µÀÜ i) , (2)\nter [3, 14] infers subsequent actions of the target object\nbasedsolelyonagiveninitialframeimage,whichdoesnot where j iterates over the points traversed by the ray r, ‚Ñì\ni\nsupportflexiblecontroloveractions. and Œ± donate the color and opacity of the i-th Gaussian.\ni\n4DGeneration. Atthecurrentstage, 4Dgenerationisin- ¬µÀÜ istheprojectionof¬µ on2Dimageplane. Withineach\ni i\nfluenced by various factors. (1) Representation Methods: moment, the deformation network predicts a variable for\nPreviousmethodshavemainlybeenbasedonNeRF[2,37, each Gaussian point‚Äôs attributes and adds it on them, thus\n50], where its multi-layer MLP architecture facilitates the drivingthe4Dobject‚Äôsmotionacrossmultipletimes.\ngeneration of smooth 4D surfaces, but requires a signif- Score Distillation Sampling (SDS) is widely used in 3D\nicant amount of time for training. Recently, some meth- generation methods [30, 31, 34, 34, 44], which aligns the\nStatic Dynamic\n‚ÄúA monkey is\nAlignment Alignment\nplaying a guitar.‚Äù\nT2V Diffusion Anchor Video PLA4D 4D GS\n‚ÄúA monkey is Condition T2I SDS ‚ÄúA monkey is Condition\nplaying a guitar.‚Äù playing a guitar.‚Äù\nT2I Diffusion\nFrame of Anchor Video T-MV\nRefinemt\nTexture\nAlignment Render\nTime T T2V Diffusion\nFront\nView 4DGS Render Motion\nAliF go nc mal\ne nt Rendering Focal\nGa Cu Los ens ai ta\nr ra\nnn\ns\ni- ntM\ni\ngve es h\ntnemngilA yrtemoeG\nTime\nT\nAlignment\nAnchor Video\nCondition\n4DGS\nRender View\nGenerate T-MV\nMesh Refinemt\nRandom\nMesh View\nDIm iffa ug se i- ot no - MM oe dsh el (a) Static Alignment (b) Dynamic Alignment V MV Diffusion\nOur Our Random Front FrontView RandomView 4D/3D Data Gradient\nModules Methods View View Rendering Rendering Representation Flow Flow\nFigure3. PipelineofPLA4D,whichleveragestextastheconditionandtext-generatedvideoasananchorfor4Dgeneration. (a)Static\nalignment: We propose focal alignment to search for the best focal length for 4D automatically. We also introduce Gaussian-Mesh\nContrastiveLearningtoprovidegeometricinformationfor4DGaussianinunknownviews,explicitlyleveragingthegeometricpriorsof\nthemesh. (b)Dynamicalignment: Acrossmultipleframes, weintroducemotionalignmenttoguidethe4Dobject‚Äôsmotionfollowing\nthe anchor video. Furthermore, we propose Time-Multiview (T-MV) refinement to optimize the motion and quality of the 4D object‚Äôs\nunknownviewpoints,usingthepriorandtheconditionofthemodelthatgeneratesthevideo.\n3Dgenerationprocesstothe2DDMstrainingprocess. In MVDMsareused,theSDSgradientis:\nthetrainingof2DDMs,samplenoiseœµfromq(x)andadd\nit on the data x (e.g., images and videos) with t times un- ‚àá L (x=g(Œ∏))=\nŒ∏ SDS\ntilq(x t)convergestoaGaussianpriordistributionN(0,I).\n(cid:20) ‚àÇx(cid:21)\nThenetworkœïistrainedtopredicttheremovalnoiseœµÀÜfor E w(t)(œµÀÜ (z,v,t)+œµÀÜ (z,v,t)‚àí2œµ) , (4)\nw/o Pixel-level Alignment Pixel-level At,lœµignmenœïtV œïMV ‚àÇŒ∏\ndenoising and reconstructing data x. 3D generation meth-\nodssettherenderingsgotfrom3Dscenerepresentationas\nImage DM Imtahgeeo DveMrall SDS gradient descent direction should be the\ndatax,andcalculatetheMeanSquaredError(MSE)toget\nvectorsumofthemulti-viewDMSDSandvideoDMSDS\nSDSgradient[30]:\nùíû graùíûdient descents. However, this is not stable. When the\ntwogradientdirectionsareopposite, the4Dmodelwillbe\n(cid:20) VS ‚àÇx(cid:21) caughtinadilemma,requiringextensiveoptimizationtime\n‚àá L (x=g(Œ∏))=E w(t)(œµÀÜ (z,v,t)‚àíœµ) ,\nŒ∏ SDS t,œµ œï ‚àÇŒ∏ tofindalocaloptimum. AsthenumberofDMsincreases,\n(3) thisissuebecomesmorepronounced.\nwheretisthetimestampofdenoisingprocess,andw(t)is\ntime-depenùíûdent weights. z repreùíûsents the latent of x neeùíûd- 3.2.PipelineùíûofPLA4D\ningtodenoise.vindicatesthegivenconditions,suchastext\nPLA4Dintroducesstaticalignmentanddynamicalignment\npromptsandimages.TheSDSgradientsarethenbackprop-\nmodules to achieve text-driven 4D generation, leveraging\nagateM du thl rt oiv ugie hw\nth\nD\ne\ndM ifferV eni td iae bo\nle\nD reM\nndering\nproceM ssu gl it niv toiew DM Video DM\nmultiple DMs, including T2V DM, I2MV DM, and T2I\nthe3DrepresentationandupdateitsparametersŒ∏.\nDM,asillustratedinFig.3. T2VDMisusedtogeneratean\nùíû Condition\nPreUvinouksn4oDwnge Vneireawtio n tasks use multiple DMs to ob- anchorvidPeioxaenld-lreevfienel themotionofthe4Dobject,while\ntainprRioernindfeorrminagtisonaboutmultiOvipewtimanidzmatoitoionn vDiairSeDcSt.ion I2MVDMArleifignnemstehnetg Aeonmcehtroyrofthe4Dobjectfromun-\nForexample,intheabsenceofpixel-levelalignmenttargets, knownviews. T2IDMbridgesthegapbetweentheanchor\ntheSDSgradientdirectionsofeachDMmightconflictwith video and the pixel-level geometry priors provided by the\neach other. Consider a simple case where only video and mesh. InspiredbyDreamGaussian4D[31],wecombine3D\nGaussianswithadeformationnetworktosupport4Dgener- propose Gaussian-Mesh contrastive learning to align the\nation. Initially,weuseanopen-sourceT2VDMtogenerate front-view4DGaussianrenderingstotheframestoachieve\nananchorvideoanduseEq.11ofthestaticalignmenttoget texture alignment, which is composed of three losses: (I)\n3DGaussianasinitializationfor4DGaussian.Next,weap- L foraligningthepixel-levelsimilarity,(II)L for\nMSE Mask\nplyEq.11ofthestaticalignmentandEq.15ofthedynamic reducingthefloaters,and(III)L forenhancingthevi-\nLPIPS\nalignmentmodulestooptimizethedeformationnetwork. sual perceptual perception. In particular, we use the MSE\nlossbetweenfrontviewcrenderingx of4DGaussiansand\n3.3.StaticAlignmentModule Œ∏\nIt asfollows:\nvid\n(cid:88)\nL (xc ,It )= ||xc ‚àíIt ||2. (6)\n+‚àÜùëì +‚àÜùëì MSE Œ∏t vid Œ∏t vid 2\nH,W\n+‚àÜùëì\nrender\n+‚àÜùëì Besides, to reduce the floaters, we also use the transpar-\n+‚àÜùëì\nMesh MSE Mask LPIPS entoutputŒ±of4DGaussiansasthemaskandcalculatethe\nloss loss loss\nMesh +‚àÜùëì +‚àÜùëì maskloss:\n+‚àÜùëì\nrender\n(cid:88)\n+‚àÜùëì L (xc ,It )= ||Œ±c ‚àíŒ±t ||2, (7)\n+‚àÜùëì\nMask Œ∏t vid Œ∏t vid 2\n4D GS H,W\n(a)FocalAlignment (b)Gaussian-MeshContrastiveLearning\nwhere Œ±t is the alpha channel of It . Besides,\nFigure4. FocalalignmentandGaussian-Meshcontrastivelearn- vid vid\ning. (a) We render multiple front-view images and calculate the we introduce Learned Perceptual Image Patch Similarity\nMSEwiththefirstframeforsearchingthematchedfocal. (b)We (LPIPS) [48], which is a metric used to measure percep-\ncollect two sets of images: one of 4D Gaussians and another of tualdifferencesbetweenimages. WeapplyLPIPSlossbe-\nmesh renderings, both captured using the same random camera tweenxc andIt toenhancethevisualqualityoftextures.\nŒ∏t vid\nposes. We include the first frame of the anchor video and the L needsanencoder(i.e.,VGG[35])toextractfeature\nLPIPS\nfront-view renderings in these two sets. Then, we calculate the stack from l layers and unit-normalize in the channel di-\nMSEloss,Maskloss,andLPIPSlossbetweenthecorresponding\nmension,andcalculatetheMSEbetweenfeaturesextracted\nimages.\nfromeachlayer:\nFocalAlignmentforTextureAlignment. PLA4Daims\ntousetext-generatedvideoasthepixel-levelalignmentan- (cid:88) 1 (cid:88)\nL (xc ,It )= ||zc ‚àízt ||2. (8)\nchor for 4D generation, which needs a matched focal for LPIPS Œ∏t vid H W Œ∏t vid 2\nl l\nframes. However, anchor frames‚Äô focals are unknown. l HlWl\nTherefore, we propose focal alignment to search for the\nNow,wecangetthetexturealignmentlossL :\nmatchedfocalf. Specifically,westartwiththevideosyn- TA\nthesis. Given the text prompt v, PLA4D applies a text-to-\nvideoDMG vidtocreateavideo{I vt id}\nT\n=G vid(œµ;v)with L TA =L MSE(xc Œ∏t,I vt id)\nT frames. œµisarandomnoise. Becausetheviewanglesin +L (xc ,It )+ŒªL (xc ,It ), (9)\nMask Œ∏t vid LPIPS Œ∏t vid\nthe anchor frames are relatively fixed, we set the video‚Äôs\nview c as the 4D object‚Äôs front perspective. Next, at the whereŒªisthescalingweightforbalance.\nbeginning of each timestep t, we need to fix the time pa-\nGaussian-Mesh Contrastive Learning for Geometry\nrameterof4DGaussianandcompareitsfrontviewrender-\nAlignment. Thankstoourfocalalignmentmethod,weob-\ningandIt tosearchf‚Ä≤,asshowninFig.4(a). Hence,we\nvid tain accurate focal lengths, enabling us to leverage video\nintroduce CRM [45], an image-to-mesh feed-forward 3D\nforprimaryviewpointtextureinformationandmeshœà got\ngenerationmodel,togenerateameshœà basedonIt . We t\nt vid before for geometric information from other viewpoints.\nrenderœà ‚Äôsfront-viewimages{x } withM differentfo-\nt œàt M Thus,weproposeGaussian-MeshContrastiveLearning,as\ncalsiteratedfromf‚Ä≤+‚àÜf tof‚Ä≤+‚àÜf ,wheref‚Ä≤ is\nmin max showninFig.4(b). WerandomlychooseN cameraposes\nc‚Ä≤\naninitialfocallength.WecalculatetheMeanSquaredError {c‚Ä≤} ,andeachcorrespondingfocalisf +‚àÜf,‚àÜf isa\n(MSE)betweenI vt idand{x œàt}\nM\nforsearchingthematched slii ghN tc a‚Ä≤\nnd random perturbation. Different from multiview\nfocalf‚Ä≤:\nDMs‚Äôproductions,therenderedimagesofmeshœà areob-\nt\nf =argmin (cid:88) ||xf‚Ä≤ ‚àíIt ||2. (5) tainedfromoneentity,thatnaturallyhasmultiviewconsis-\nf‚Ä≤\nœàt vid 2 tency. Besides, this method can provide references from\nH,W\nany number of different viewpoints for training 4D Gaus-\nAt each timestep, with the corresponding focal f, we sian Œ∏, such density data can avoid artifacts in renderings.\nto\nb.n\nd\nio\no rilo n\na\nd io n\na m u\nhiv e h\nt g n iy\nm u h\nr e v\nlis.to b\no r\nAa lp A\ng n iy\na lp n e.o n a ip\nn o te\nle k s n\na.e\nn iw g n\nila\nn\ne\nh\nt\nm\nu\nhik\nn\nir\nA Ad\ng n .g\nita\ne a.m\nn\nic n\na\nda d\nn ae\nr\ns\ni\np yc\ne\na\nd\nbc n\nai a\nb p\nA A\na\ng\n.g\nn\nn\nid.e\nih\ng\nir\nle\nr r\niulc\ny c\nr o\nto\nu\na l\ns i\nta\nqm o\ns\nA\ng\nA\nFigure5. VisualizationresultsofPLA4D.The4DobjectsgeneratedbyPLA4Dnotonlyrigorouslyfollowthesemanticsbutalsofeature-\nrichdynamicsandexcellentgeometricconsistency.Moreimportantly,PLA4Dgenerateseachsampleinapproximately15minutes.\nThegeometryalignmentlossL canbesummarizedas: 3.4.DynamicAlignmentModule\nGA\nMotionAlignment. Withourfocalalignmentmethod,we\nL\n=(cid:88)N c‚Ä≤\n(L (xc‚Ä≤ i,xc‚Ä≤ i) can directly use the anchor video as the pixel-level align-\nGA MSE Œ∏t œàt\nment targets to provide motion guidance. Thus, we mini-\ni=1\n+L\n(xc‚Ä≤ i,xc‚Ä≤\ni)+ŒªL\n(xc‚Ä≤ ixc‚Ä≤\ni)). (10)\nmizethemotionalignmentlossL MAtoinjectdynamics:\nMask Œ∏t œàt LPIPS Œ∏t œàt\nT\n1 (cid:88) (cid:88)\nBesides,weadditionallyintroduceaT2IDMusingtheSDS L = ||xc ‚àíIt ||2, (12)\nmethod to enhance the control of the text prompt over the\nMA T Œ∏t vid 2\nt=1H,W\ncurrent object. Overall, the static alignment loss L is\nstatic\ndenotedas: where xc is the front-view renderings of 4D Gaussian at\ntimet.\nIŒ∏ tt\nisthecorrespondingframeofanchorvideo.\nvid\nL =L +L +L . (11) Time and Multiview Refinement. Despite following the\nstatic TA GA T2I\n‚Äú\n‚Äú\n‚Äú\n‚Äú\n‚Äù\n‚Äù\n‚Äù\n‚Äù\n‚Äú\n‚Äú\n‚Äù\n‚Äú\n‚Äù\n‚Äú\n‚Äù\n‚Äù\ns ru O Dy 4-f m\naD\nn4 -- i G Y A x o f a fo.e m a\ng\ne rD g no e\nN E G S G D 3 V A M n e G D 4 + D\nS\nD 4 S G D + D\nS\nir\ne d n e r D\n3d\niv g n iy a lp\nD\n4\ns ru O Dy 4-f m a en -- i G Y A s i a.g\nrD dn\nN E G S G D 3 V A M n e G D 4 + D D 4 S G D + D\nn\na p\nAic\nn a d\nS S\nD\n4\ns ru O Dy 4-f m an -- i G Y A\ne\nrD\nn D\n‚Äú3D rendering of a fox D 3 V e G D 4 S G\nplaying videogame.‚Äù A M 4 + D D + D\nS S\nD\n4\ns ru\nO\nDy 4-f\nm\nan -- i G Y\nA\ne\nrD\nn D\n‚ÄúA panda is D 3 V e G D 4 S G\ndancing.‚Äù A 4 D\nM + D + D\nS S\nFigure6.ComparisonofPLA4Dwithtext-to-4Dandimage-to-4Dmethods.Top:Thepixel-levelgeometricpriorsprovidedbyGaussian-\nMesh effectively help PLA4D avoid multi-face artifacts. The addition of the focal alignment module corrects the erroneous primary\nviewpointprojectionrelationshipsobservedinimage-to-4Dmethods.Bottom:Withpixel-levelalignment,PLA4Dachievesthemaximum\nmotionrangeacross8-frameintervals,producingsemanticallycoherentmotionratherthanpixeljittering.\naforementionedtechnicalstepstoobtainadynamicandge- 46, 50] and image-to-4D method [31]. For the image-to-\nometricallyreasonable4Dtarget,surfacesplittingmaystill 4Dmethods,weuseStableDiffusion2.1[32]withidentical\noccur. TheGaussianpointswithpredictedlocationsaretoo promptstogenerateimages,whicharethenusedtogenerate\nfarapart,andthescalecannotbridgethegapbetweenthese 4D objects. Additionally, we compare methods based on\npoints. Thisindicatesthatsomeunfamiliarviewpointsstill both NeRF and Gaussian representations. For the closed-\nlacktemporalcontinuityandgeometricconsistency. Thus, source methods MAV3D [37] and AYG [24], we perform\nweproposetheTime-Multiview(T-MV)Refinement,which comparisonsusingoverlappingexamples.\nusesthetextpromptasaconditiontooptimizemotionvia Comparative Studies. We present a large number of\nvideo DM œï V, and the anchor video as a condition to op- PLA4D-generated results in Fig. 5. Thanks to the pixel-\ntimize geometry via multiview DM œï MV, ensuring stable level alignment methods, the 4D objects move beyond the\nperformanceacrossmultipletimestampsandrandomview- rigid rendering style of previous 4D generation methods,\npoints. TheL T-MV includesL TimeandL MV: exhibiting a stronger photorealistic style. Additionally,\ndue to explicit motion guidance provided by the reference\nL = 1 (cid:88)T (cid:88) w(œÑ)||œµ (Œ± xc‚Ä≤ +œÉ œµ;C;œÑ)‚àíœµ||2, video,thetargetdemonstratesdetailedmotiondifferencesat\nTime T œïV œÑ Œ∏t œÑ 2 eachtimestamp. Furthermore,withourproposedGaussian-\nt=1H,W Meshcontrastivelearningmethod, PLA4D‚Äôsproductsalso\n(13)\nexhibitexcellentgeometricconsistency. Moreimportantly,\nL = 1 (cid:88)N c‚Ä≤ (cid:88) w(œÑ)||œµ (Œ± xc‚Ä≤ i+œÉ œµ;It ;œÑ)‚àíœµ||2, eachsamplecanbegeneratedinjust15minuteswith0.6K\nMV N\nc‚Ä≤\nœïMV œÑ Œ∏t œÑ vid 2 iterations.\ni=1H,W\n(14) Compared with other 4D generation methods, PLA4D\nwhere œÑ is the timestep of DM, w(œÑ), Œ± and œÉ are pa- demonstratessuperiorgeometricstructure,smoothmotion,\nœÑ œÑ\nrameters depends on the timestep œÑ. Here, we can get and semantic consistency, as shown in Fig. 6. (1) Geom-\nL =L +L .Insummary,weultimatelyderive etry: due to the implicit distillation of geometric priors in\nT-MV Time MV\nthedynamicalignmentlossL : Dream-in-4D,thegeneratedobjectsuffersfromtheJanus-\ndynamic\nfaceproblem. Thesamephenomenoncanalsobeobserved\nL =L +L . (15) in the samples from MAV3D. (2) Motion: due to the im-\ndynamic MA T-MV\nplicit distillation of motion priors, even when comparing\n4.Experiments the first and tenth frames, previous methods exhibit only\nsmall motion amplitudes, making it difficult to align with\nBaselines. For a comprehensive comparison, we evaluate the motion described in the prompt. (3) Semantic consis-\nour method alongside both text-to-4D methods [2, 24, 37, tency: Although4D-fydoesnotsufferfromtheJanus-face\n‚Äú\n‚Äú\n‚Äù\n‚Äù\nw/o Focal w/o Gaussian-Mesh w/o Motion w/o T-MV\nPLA4D Alignment Contrastive Learning Alignment Refinement\nùúè=1 ùúè=1 ùúè=1 ùúè=1 ùúè=1\nFigure7.Ablationùúès =tu d1i0es.Ifnofocalalignmeùúèn =to 1r0Gaussian-Meshcontraùúès =tiv 1e0learning,the4Dobjecùúèt =lo s1e0sitsdetailedtextureaùúèn d=c 1or0rect\ngeometry.Withoutmotionalignment,a4Dobjectdegeneratesintoastaticobject.AbsentT-MVrefinement,thedisplacementofGaussians\ncausessurfacetearing.\nMethods Representation GenerationTime Iterations sians can not learn the correct attributes of points to align\nAnimate124[49] NeRF - 20K tothegeneratedframes. Boththegeometryandmotionof\n4DGen[46] NeRF 3.0hr 3K 4Dobjectsarecompromised. WithoutGaussian-Meshcon-\nConsistent4D[18] NeRF 2.5hr 10K\ntrastive learning, the geometry structure and texture in un-\nDreamGaussian4D[31] Gaussians 6.5min 0.7K\nknown views can not learned from multiview DM prior in\n4D-fy[2] NeRF 23hr 120K\nsuch a short training time. Without motion alignment, the\nDream-in-4D[50] NeRF 10.5hr 20K\nMAV3D[37] NeRF 6.5hr 12K 4Dobjectdegradesintoastatic3Dobject. WithoutT-MV\nAYG[24] Gaussians - 20K refinement,dynamicmultiviewrenderingsof4Dobjectsre-\nPLA4D(ours) Gaussians 15min 0.6K sultinsurfacecracks.\nEfficiency Study. We compare the time overhead of mul-\nTable1. Speedcomparison. Theupperpartpresentsimage-to-4D\ntiple4Dgenerationmethodsproposedforimage-to-4dand\nmethods,whilethelowerpartcollectstext-to-4Dmethods..\ntext-to-4dtasks,asshowninTab.1. Itcanbeobservedthat\nModel Motion Geometry Semanticconsistency previous4DgenerationtasksoverlyrelyonSDS,whichre-\nquiresextensivetraining(over10Kiterations)byimplicitly\n4D-fy[2] 14.19% 21.10% 11.76%\nDream-in-4D[50] 34.95% 27.68% 32.18% aligningvariousdiffusionmodelstogenerate4Dobjects.In\ncontrast,PLA4Dusesexplicitpixel-levelalignment,result-\nPLA4D 50.86% 51.22% 56.06%\ninginbettertextures, geometry, andmotionfor4Dtargets\nTable2. Userstudy. PLA4Dreceivesthemostpraisefromusers withsignificantlylowertimeoverhead.\nforitsconsistencyinmotion,geometry,andsemantics. UserStudy. Tofurtherevaluatethequalityofour4Dgen-\neration objects, we conducted a user study on 30 partici-\nproblem, its generated outputs exhibit semantic inconsis-\npants.Specifically,weinvestigatedusers‚Äôpreferenceof4D-\ntencies. Due to the conflicts arising from the simultane-\nfy[2],Dream-in-4D[50],andourPLA4Dintermsofmo-\nousoptimizationofmultipleSDSobjectives,wheretheop-\ntion,geometry,andsemanticconsistency.Wedidn‚Äôtinclude\ntimization directions for geometry, motion, and semantics\nMAV3D[37]andAYG[24]becausetheyareclosed-source.\ncompete with each other, balancing these factors becomes\nAsshowninTab.2,ourPLA4Dsurpassesothercomparison\nchallenging. PLA4Deffectivelyalleviatesthisissuebyem-\nmethodsinallperspectives,indicatingoursuperiorperfor-\nployingpixel-levelalignment.\nmanceonmotion,geometry,andsemanticconsistency.\nTheunifiedstructureofNeRFwithitsMLPstructureis\nLimitation.PLA4Dusesvideoasananchor,relyingonthe\nnot sensitive to each optimization step, allowing for better\nperformanceofT2VDM.Asthemotionrangeofthetext-\ntexture generation. In contrast, the Gaussian model opti-\ndrivengeneratedvideoincreasesandthevideodurationex-\nmizes each Gaussian point independently, making it more\ntends,PLA4Dwillproduceimprovedmotionperformance.\nsensitive to each optimization step [9, 39, 41]. This struc-\nturaldifferenceintroducesgreaterchallengesinoptimizing\n5.Conclusion\ntexture. However, PLA4D can still maintain high-quality\ntexturebyleveragingtheT-MVrefinement. Inthispaper,weintroducePLA4D,aframeworkthatlever-\nAblationStudy. InFig.7,wedemonstratetheroleofeach agestext-drivengeneratedvideoasexplicitpixelalignment\nmodule in PLA4D for text-to-4D generation. Without the targets for 4D generation, anchoring the rendering process\nfocal alignment method, using unmatched focal f, Gaus- conditioned by different DMs. We propose various mod-\n¬∞081=ùê∂\n¬∞0=ùê∂\nulestoachievesuchanchoring:weproposeGaussian-Mesh\ncontrastivelearningandfocalalignmenttoensuregeometry\nconsistencyfromthemeshandproducetexturesasdetailed\nas those in the generated video frames. Additionally, we\nhave developed a novel motion alignment method and T-\nMV refinement technology to optimize dynamic surfaces.\nCompared to existing methods, PLA4D effectively avoids\nJanus-face problem and generates 4D targets with accu-\nrategeometryandsmoothmotioninsignificantlylesstime.\nFurthermore,PLA4Disconstructedentirelyusingexisting\nopen-source models, eliminating the need for pre-training\nanyDMs. Thisflexiblearchitectureallowsthecommunity\nto freely replace or upgrade components to achieve state-\nof-the-art performance. We aim for PLA4D to become an\naccessible, user-friendly, andpromisingtoolfor4Ddigital\ncontentcreation.\nReferences YuLiu,andYogeshBalaji. Preserveyourowncorrelation:\nA noise prior for video diffusion models. In Proceedings\n[1] JieAn,SongyangZhang,HarryYang,SonalGupta,Jia-Bin\noftheIEEE/CVFInternationalConferenceonComputerVi-\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion,pages22930‚Äì22941,2023.\nsion with temporal shift for efficient text-to-video genera-\n[13] RohitGirdhar,MannatSingh,AndrewBrown,QuentinDu-\ntion. arXivpreprintarXiv:2304.08477,2023.\nval,SamanehAzadi,SaiSakethRambhatla,AkbarShah,Xi\n[2] SherwinBahmani,IvanSkorokhodov,VictorRong,Gordon\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\nWetzstein,LeonidasGuibas,PeterWonka,SergeyTulyakov,\ningtext-to-videogenerationbyexplicitimageconditioning.\nJeongJoonPark,AndreaTagliasacchi,andDavidBLindell.\narXivpreprintarXiv:2311.10709,2023.\n4d-fy: Text-to-4dgenerationusinghybridscoredistillation\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nsampling. arXivpreprintarXiv:2311.17984,2023.\nQiao, DahuaLin, andBoDai. Animatediff: Animateyour\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\npersonalizedtext-to-imagediffusionmodelswithoutspecific\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\ntuning. arXivpreprintarXiv:2307.04725,2023.\nZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideo\n[15] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,\ndiffusion: Scaling latent video diffusion models to large\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\ndatasets. arXivpreprintarXiv:2311.15127,2023.\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\n[4] AndreasBlattmann,RobinRombach,HuanLing,TimDock-\nvideo:Highdefinitionvideogenerationwithdiffusionmod-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nels. arXivpreprintarXiv:2210.02303,2022.\nAlignyourlatents: High-resolutionvideosynthesiswithla-\n[16] HuguesHoppe,TonyDeRose,TomDuchamp,JohnMcDon-\ntentdiffusionmodels.InProceedingsoftheIEEE/CVFCon-\nald,andWernerStuetzle.Meshoptimization.InProceedings\nferenceonComputerVisionandPatternRecognition,pages\nofthe20thannualconferenceonComputergraphicsandin-\n22563‚Äì22575,2023.\nteractivetechniques,pages19‚Äì26,1993.\n[5] AngCaoandJustinJohnson. Hexplane: Afastrepresenta-\n[17] YukunHuang,JiananWang,YukaiShi,XianbiaoQi,Zheng-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nJunZha,andLeiZhang.Dreamtime:Animprovedoptimiza-\nConference on Computer Vision and Pattern Recognition,\ntionstrategyfortext-to-3dcontentcreation. arXivpreprint\npages130‚Äì141,2023.\narXiv:2306.12422,2023.\n[6] GuikunChenandWenguanWang. Asurveyon3dgaussian\nsplatting. arXivpreprintarXiv:2401.03890,2024. [18] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao\nYao. Consistent4d: Consistent 360 {\\deg} dynamic ob-\n[7] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan-\nject generation from monocular video. arXiv preprint\ntasia3d: Disentangling geometry and appearance for high-\narXiv:2311.02848,2023.\nquality text-to-3d content creation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, [19] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\npages22246‚Äì22256,2023. Lischinski. Noise-free score distillation. arXiv preprint\narXiv:2310.17590,2023.\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. IT3D: improved [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¬®hler,\ntext-to-3dgenerationwithexplicitviewsynthesis. InThirty- and George Drettakis. 3d gaussian splatting for real-time\nEighth AAAI Conference on Artificial Intelligence, AAAI radiancefieldrendering.ACMTransactionsonGraphics,42\n2024,Thirty-SixthConferenceonInnovativeApplicationsof (4):1‚Äì14,2023.\nArtificialIntelligence,IAAI2024,FourteenthSymposiumon [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nEducationalAdvancesinArtificialIntelligence,EAAI2014, vosyan, Roberto Henschel, Zhangyang Wang, Shant\nFebruary 20-27, 2024, Vancouver, Canada, pages 1237‚Äì Navasardyan, and Humphrey Shi. Text2video-zero: Text-\n1244.AAAIPress,2024. to-imagediffusionmodelsarezero-shotvideogenerators.In\n[9] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. ProceedingsoftheIEEE/CVFInternationalConferenceon\nText-to-3d using gaussian splatting. In Proceedings of ComputerVision,pages15954‚Äì15964,2023.\ntheIEEE/CVFConferenceonComputerVisionandPattern [22] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiao-\nRecognition,pages21401‚Äì21412,2024. gang Xu, and Yingcong Chen. Luciddreamer: Towards\n[10] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, high-fidelitytext-to-3dgenerationviaintervalscorematch-\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. ing. arXivpreprintarXiv:2311.11284,2023.\nNerdi:Single-viewnerfsynthesiswithlanguage-guideddif- [23] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,\nfusion as general image priors. In Proceedings of the Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nIEEE/CVF Conference on Computer Vision and Pattern Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution\nRecognition,pages20637‚Äì20647,2023. text-to-3dcontentcreation.InProceedingsoftheIEEE/CVF\n[11] Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, and Conference on Computer Vision and Pattern Recognition,\nXiaoli Liu. Metadreamer: Efficient text-to-3d creation pages300‚Äì309,2023.\nwith disentangling geometry and texture. arXiv preprint [24] HuanLing, SeungWookKim, AntonioTorralba, SanjaFi-\narXiv:2311.10123,2023. dler, and Karsten Kreis. Align your gaussians: Text-to-4d\n[12] SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,Andrew withdynamic3dgaussiansandcomposeddiffusionmodels.\nTao,BryanCatanzaro,DavidJacobs,Jia-BinHuang,Ming- arXivpreprintarXiv:2312.13763,2023.\n[25] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,Mukund [38] JingxiangSun,BoZhang,RuizhiShao,LizhenWang,Wen\nVarmaT,ZexiangXu,andHaoSu. One-2-3-45:Anysingle Liu,ZhendaXie,andYebinLiu. Dreamcraft3d: Hierarchi-\nimageto3dmeshin45secondswithoutper-shapeoptimiza- cal 3d generation with bootstrapped diffusion prior. arXiv\ntion. Advances in Neural Information Processing Systems, preprintarXiv:2310.16818,2023.\n36,2024. [39] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGang\n[26] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok- Zeng.Dreamgaussian:Generativegaussiansplattingforeffi-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: cient3dcontentcreation. arXivpreprintarXiv:2309.16653,\nZero-shot one image to 3d object. In Proceedings of the 2023.\nIEEE/CVF International Conference on Computer Vision, [40] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\npages9298‚Äì9309,2023. MichaelNiemeyer,andFedericoTombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts. arXiv\n[27] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie\npreprintarXiv:2304.12439,2023.\nLiu,TakuKomura,andWenpingWang. Syncdreamer:Gen-\n[41] TrapoomUkarapolandKevinPruvost. Gradeadreamer:En-\neratingmultiview-consistentimagesfromasingle-viewim-\nhanced text-to-3d generation using gaussian splatting and\nage. arXivpreprintarXiv:2309.03453,2023.\nmulti-view diffusion. arXiv preprint arXiv:2406.09850,\n[28] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\n2024.\nLin,TowakiTakikawa,NicholasSharp,Tsung-YiLin,Ming-\n[42] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\nandGregShakhnarovich. Scorejacobianchaining: Lifting\ntext-to-3dobjectsynthesis.InProceedingsoftheIEEE/CVF\npretrained 2d diffusion models for 3d generation. In Pro-\nInternationalConferenceonComputerVision,pages17946‚Äì\nceedingsoftheIEEE/CVFConferenceonComputerVision\n17956,2023.\nandPatternRecognition,pages12619‚Äì12629,2023.\n[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\n[43] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nJonathanTBarron,RaviRamamoorthi,andRenNg. Nerf:\nandGregShakhnarovich. Scorejacobianchaining: Lifting\nRepresentingscenesasneuralradiancefieldsforviewsyn-\npretrained 2d diffusion models for 3d generation. In Pro-\nthesis. CommunicationsoftheACM,65(1):99‚Äì106,2021.\nceedingsoftheIEEE/CVFConferenceonComputerVision\n[30] BenPoole,AjayJain,JonathanTBarron,andBenMilden- andPatternRecognition,pages12619‚Äì12629,2023.\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv [44] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan\npreprintarXiv:2209.14988,2022. Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand\n[31] JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao, diversetext-to-3dgenerationwithvariationalscoredistilla-\nGang Zeng, and Ziwei Liu. Dreamgaussian4d: Genera- tion. Advances in Neural Information Processing Systems,\ntive4dgaussiansplatting.arXivpreprintarXiv:2312.17142, 36,2024.\n2023. [45] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xi-\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, ang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su,\nPatrick Esser, and Bjo¬®rn Ommer. High-resolution image and Jun Zhu. Crm: Single image to 3d textured mesh\nsynthesis with latent diffusion models. In Proceedings of with convolutional reconstruction model. arXiv preprint\ntheIEEE/CVFConferenceonComputerVisionandPattern arXiv:2403.05034,2024.\nRecognition(CVPR),pages10684‚Äì10695,2022. [46] Yuyang Yin, Dejia Xu, Zhangyang Wang, Yao Zhao,\nand Yunchao Wei. 4dgen: Grounded 4d content gen-\n[33] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,\neration with spatial-temporal consistency. arXiv preprint\nChaoXu,XinyueWei,LinghaoChen,ChongZeng,andHao\narXiv:2312.17225,2023.\nSu. Zero123++:asingleimagetoconsistentmulti-viewdif-\n[47] Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian\nfusionbasemodel. arXivpreprintarXiv:2310.15110,2023.\nLin,HaoZhu,WeimingHu,XunCao,andYaoYao.Stag4d:\n[34] YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,\nSpatial-temporal anchored generative 4d gaussians. In Eu-\nandXiaoYang. Mvdream:Multi-viewdiffusionfor3dgen-\nropean Conference on Computer Vision, pages 163‚Äì179.\neration. arXivpreprintarXiv:2308.16512,2023.\nSpringer,2025.\n[35] KarenSimonyanandAndrewZisserman. Verydeepconvo-\n[48] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nlutional networks for large-scale image recognition. arXiv\nman, and Oliver Wang. The unreasonable effectiveness of\npreprintarXiv:1409.1556,2014.\ndeepfeaturesasaperceptualmetric. InProceedingsofthe\n[36] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, IEEE conference on computer vision and pattern recogni-\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, tion,pages586‚Äì595,2018.\nOranGafni, etal. Make-a-video: Text-to-videogeneration [49] YuyangZhao,ZhiwenYan,EnzeXie,LanqingHong,Zhen-\nwithout text-video data. arXiv preprint arXiv:2209.14792, guoLi,andGimHeeLee. Animate124: Animatingoneim-\n2022. ageto4ddynamicscene. arXivpreprintarXiv:2311.14603,\n[37] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, 2023.\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea [50] YufengZheng, XuetingLi, KokiNagano, SifeiLiu, Otmar\nVedaldi, DeviParikh, JustinJohnson, etal. Text-to-4ddy- Hilliges, and Shalini De Mello. A unified approach for\nnamic scene generation. arXiv preprint arXiv:2301.11280, text-andimage-guided4dscenegeneration. arXivpreprint\n2023. arXiv:2311.16854,2023.\n[51] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkusGross. Ewavolumesplatting. InProceedingsVisu-\nalization,2001.VIS‚Äô01.,pages29‚Äì538.IEEE,2001.",
    "pdf_filename": "PLA4D_Pixel-Level_Alignments_for_Text-to-4D_Gaussian_Splatting.pdf"
}