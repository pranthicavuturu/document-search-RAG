{
    "title": "Predicting Customer Satisfaction by Replicating the Survey Response Distribution",
    "context": "For many call centers, customer satisfaction (CSAT) is a key performance indicator (KPI). However, only a fraction of customers take the CSAT survey after the call, leading to a biased and inaccurate average CSAT value, and missed opportunities for coaching, follow-up, and rec- tification. Therefore, call centers can benefit from a model predicting customer satisfaction on calls where the customer did not complete the survey. Given that CSAT is a closely moni- tored KPI, it is critical to minimize any bias in the average predicted CSAT (pCSAT). In this paper, we introduce a method such that pre- dicted CSAT (pCSAT) scores accurately repli- cate the distribution of survey CSAT responses for every call center with sufficient data in a live production environment. The method can be applied to many multiclass classification prob- lems to improve the class balance and minimize its changes upon model updates. 1 Many machine learning applications use classifiers updated periodically by developers. Without spe- cial control mechanisms, these updates can shift the relative balance of output classes, causing un- intended effects. For the case of predicting CSAT, we developed a control mechanism to address this issue, taking care to mitigate the risks posed by sampling noise. This paper explains our method and strategies for handling sampling noise, and aims to help developers seeking to replicate one or more target class distribution(s). Customer satisfaction (CSAT) is critical for call center performance assessment, yet often measured through surveys completed by a small subset of cus- tomers—averaging 8% in our dataset. This limited response rate can skew perceived performance, as non-responding customers’ satisfaction remains un- known. Predicting CSAT for all calls, even those without survey responses, can mitigate this bias Manderscheid and Lee (2023). Ensuring these pCSAT scores do not introduce further bias is crucial. This paper introduces a method to more accurately replicate the distribu- tion of survey CSAT responses in a live production environment, addressing limitations identified in prior work and providing more accurate metrics for call center performance. 2 Related Work Predicting CSAT using machine learning models has gained attention, especially in call center con- versations. The challenge is not only predicting accurate scores but also ensuring these predictions replicate the true distribution of survey responses. This section reviews relevant studies and method- ologies applied to similar problems, focusing on distribution replication and ordinal classification (since CSAT is measured on a 1-5 scale). 2.1 Predicting Customer Satisfaction Previous research explored various approaches to predict CSAT from contact center conversations. Bockhorst et al. (2017) developed a system using ASR-generated call transcripts, non-textual data, and sentiment scores to predict a Representative Satisfaction Index (RSI) with rank scoring and iso- tonic regression models. Similarly, Auguste et al. (2019) used the Net Promoter Score (NPS) with binary classification (promoters vs. detractors) for predicting customer satisfaction in chat conversa- tions, achieving moderate improvements with a macro F1 score of 53.8%. Other studies examined predicting CSAT from raw audio signal features such as acoustic, emo- tional, and prosodic features (Park and Gates, 2009; Zweig et al., 2006; Vaudable and Devillers, 2012; Devillers et al., 2010). This work builds on a previously developed method for predicting CSAT scores using ASR- generated (Automated Speech Recognition) call arXiv:2411.12539v1  [cs.LG]  19 Nov 2024",
    "body": "Predicting Customer Satisfaction by Replicating the Survey Response\nDistribution\nEtienne Manderscheid, Matthias Lee\nDialpad Canada Inc.\n1100 Melville St #400\nVancouver, BC, Canada, V6E 4A6\n{etienne,matthias.lee}@dialpad.com\nAbstract\nFor many call centers, customer satisfaction\n(CSAT) is a key performance indicator (KPI).\nHowever, only a fraction of customers take the\nCSAT survey after the call, leading to a biased\nand inaccurate average CSAT value, and missed\nopportunities for coaching, follow-up, and rec-\ntification. Therefore, call centers can benefit\nfrom a model predicting customer satisfaction\non calls where the customer did not complete\nthe survey. Given that CSAT is a closely moni-\ntored KPI, it is critical to minimize any bias in\nthe average predicted CSAT (pCSAT). In this\npaper, we introduce a method such that pre-\ndicted CSAT (pCSAT) scores accurately repli-\ncate the distribution of survey CSAT responses\nfor every call center with sufficient data in a live\nproduction environment. The method can be\napplied to many multiclass classification prob-\nlems to improve the class balance and minimize\nits changes upon model updates.\n1\nIntroduction\nMany machine learning applications use classifiers\nupdated periodically by developers. Without spe-\ncial control mechanisms, these updates can shift\nthe relative balance of output classes, causing un-\nintended effects. For the case of predicting CSAT,\nwe developed a control mechanism to address this\nissue, taking care to mitigate the risks posed by\nsampling noise. This paper explains our method\nand strategies for handling sampling noise, and\naims to help developers seeking to replicate one or\nmore target class distribution(s).\nCustomer satisfaction (CSAT) is critical for call\ncenter performance assessment, yet often measured\nthrough surveys completed by a small subset of cus-\ntomers—averaging 8% in our dataset. This limited\nresponse rate can skew perceived performance, as\nnon-responding customers’ satisfaction remains un-\nknown. Predicting CSAT for all calls, even those\nwithout survey responses, can mitigate this bias\nManderscheid and Lee (2023).\nEnsuring these pCSAT scores do not introduce\nfurther bias is crucial. This paper introduces a\nmethod to more accurately replicate the distribu-\ntion of survey CSAT responses in a live production\nenvironment, addressing limitations identified in\nprior work and providing more accurate metrics for\ncall center performance.\n2\nRelated Work\nPredicting CSAT using machine learning models\nhas gained attention, especially in call center con-\nversations. The challenge is not only predicting\naccurate scores but also ensuring these predictions\nreplicate the true distribution of survey responses.\nThis section reviews relevant studies and method-\nologies applied to similar problems, focusing on\ndistribution replication and ordinal classification\n(since CSAT is measured on a 1-5 scale).\n2.1\nPredicting Customer Satisfaction\nPrevious research explored various approaches to\npredict CSAT from contact center conversations.\nBockhorst et al. (2017) developed a system using\nASR-generated call transcripts, non-textual data,\nand sentiment scores to predict a Representative\nSatisfaction Index (RSI) with rank scoring and iso-\ntonic regression models. Similarly, Auguste et al.\n(2019) used the Net Promoter Score (NPS) with\nbinary classification (promoters vs. detractors) for\npredicting customer satisfaction in chat conversa-\ntions, achieving moderate improvements with a\nmacro F1 score of 53.8%.\nOther studies examined predicting CSAT from\nraw audio signal features such as acoustic, emo-\ntional, and prosodic features (Park and Gates, 2009;\nZweig et al., 2006; Vaudable and Devillers, 2012;\nDevillers et al., 2010).\nThis work builds on a previously developed\nmethod for predicting CSAT scores using ASR-\ngenerated (Automated Speech Recognition) call\narXiv:2411.12539v1  [cs.LG]  19 Nov 2024\n\ntranscripts (Manderscheid and Lee, 2023). This\nmethod improved prediction accuracy with a map-\nping function from binary model outputs to 5 CSAT\nclasses (Figure 1). The mapping function was pa-\nrameterized by 4 decision thresholds. The binary\nmodel itself was a trained Big bird, a transformer\nwith sparse attention optimized to handle long in-\nput sequences (such as call transcripts) with a linear\nmemory requirement (Zaheer et al., 2020).\n2.2\nReplicating Class Distribution\nOur threshold fitting approach replicates the survey\nCSAT distribution, crucial for maintaining class\nproportions in predictions. Research on maintain-\ning class distribution intersects with imbalanced\nlearning and ordinal regression, using techniques\nlike resampling, re-weighting, and threshold adjust-\nment to handle class imbalances. Model calibration\ncan be a helpful addition to these methods, but is\nnot a replacement, as model calibration focuses\non adjusting predicted probabilities to better re-\nflect true likelihoods, which does not imply that the\nclass distribution will be faithfully replicated if the\ndecision thresholds are incorrect.\nRe-sampling and Re-weighting: These tech-\nniques adjust training processes to account for class\nimbalances, ensuring minority classes are repre-\nsented. However, they are not well suited to repli-\ncating an exact class balance, as the effects of these\ntraining set adjustments are difficult to predict.\nThreshold Optimization in Multiclass and Or-\ndinal Classification: Threshold optimization is\ncritical in contexts requiring precise class predic-\ntions, such as multiclass and ordinal classification\ntasks. Kotsiantis et al. (2006) discuss methods to\nadjust decision thresholds for imbalanced datasets,\nbalancing sensitivity and specificity to represent\nminority classes. Ferri et al., 2002 introduce meth-\nods to optimize decision thresholds to minimize\nmisclassification costs. Their work is relevant in\ncontexts where different misclassifications have dif-\nferent costs, making threshold adjustment crucial.\nWhile these approaches are closely related to ours\nby adjusting model thresholds to reflect true class\ndistributions, they focus on binary and multiclass\nclassification without emphasizing ordinal classes\nas ours does.\nCardoso and da Costa (2007) proposed a data\nreplication method for ordinal classification, han-\ndling ordinal data by replicating instances to indi-\nrectly optimize thresholds for ordinal predictions.\nThis study aligns with our work, emphasizing main-\ntaining the natural order of classes, but our method\ndirectly optimizes thresholds to replicate survey\nresponses, rather than using data replication.\nIn \"A simple approach to ordinal classification,\"\nFrank and Hall, 2001 propose a threshold-based\nmethod for ordinal classification problems. Their\napproach involves training a series of binary clas-\nsifiers to predict whether an instance belongs to\na class above a certain threshold. This method is\nclosely related to our approach, as both aim to pre-\ndict ordinal classes by optimizing thresholds. How-\never, we use a single classifier, and our method\ngoes further by ensuring that the predicted class\ndistribution matches the training class distribution,\na step beyond the basic ordinal classification task.\nChu and Keerthi (2007) explored ordinal regres-\nsion using support vector machines (SVMs), opti-\nmizing thresholds within the SVM framework to\nrespect the ordinal nature of data. Similar to our\nwork, this study focuses on ordinal data and thresh-\nold optimization, but our method is model-agnostic,\npost-processing outputs of any classifier to match\ndesired distributions.\nThese studies provide valuable insights into\nthreshold optimization for multiclass and ordinal\nclassification. Our work distinguishes itself by:\n1. Optimizing specific decision thresholds to\nalign pCSAT scores with CSAT survey re-\nsponses, ensuring calibration and class dis-\ntribution replication.\n2. Creating a custom loss function reflecting our\nunique product goals and user suggestions.\n3. Applying our method to a large language\nmodel (LLM) predicting CSAT scores from\ncall center conversations, integrating thresh-\nold optimization into a broader machine learn-\ning pipeline to address practical challenges in\nreal-world settings.\n3\nData & Methods\n3.0.1\nTranscripts\nWe used conversational transcripts generated from\nour Automatic Speech Recognition engine. The\naccuracy (1 - Word Error Rate) was > 85%.\n3.0.2\nCalls\nWe used approximately 892K call center calls with\na CSAT survey score and a model-assigned pCSAT\nprobability ranging from June 24, 2023 to June 17,\n2024.\n\nSurvey Responses\n# of Call Centers\n1-50\n401\n51-200\n908\n201-500\n425\n501-1000\n199\n> 1000\n197\nTable 1: Number of Call Centers by Survey Responses\nVolume\n3.0.3\nTrials\nTo rule out effects due to chance or periodicity, we\nran the experiment 7 times using different training\nand test periods. The last of those trials corresponds\nto a production deployment of the model, and the\nother trials were simulated for the purposes of this\nanalysis. Each trial consists of a 60 day training\nperiod and 120 day test period. A 30 day period\nseparates the start of one trial from the start of\nthe next (thus trials overlap). We expected and\nobserved no differences between the deployed and\nsimulated trials since the pipeline is the same.\n3.0.4\nCall Centers\nWe excluded call centers with fewer than 5 high\nand 5 low CSAT calls over the 60-day training pe-\nriod to avoid very high sampling noise. To better\nunderstand the impact of sampling noise, we fur-\nther categorized call centers heuristically based on\nthe number of survey CSAT responses in the 60-\nday training period. Table 1 shows the number of\ncall centers in each survey response bin, summed\nover the 7 trials.\n3.1\nThreshold Optimization Procedure\n3.1.1\nModel and Mapping\nThe model is a large language model (LLM) that\npredicts CSAT with binary outputs: high or low\nCSAT. Details on the model and how it was trained\nare provided in Manderscheid and Lee, 2023. The\nmodel also provides the probability of both classes,\nreferred to as \"proba\" for the low CSAT class. The\nmapping function (Figure 1) uses this probability to\noutput a pCSAT score on a 1-5 scale. The mapping\nhas four parameters representing decision thresh-\nolds (i.e. class boundaries): t1,2, t2,3, t3,4, and\nt4,5. For example, t3,4 is the probability threshold\nseparating a pCSAT of 3 from 4.\n3.1.2\nProduct Requirements\nOur approach is based on meeting product require-\nments, ranked by importance:\nFinetuned\nLLM \nmodel \nCall transcript \nclass 1\nFan-out \nclassification\nLow CSAT \nModel \nProbability\n(“proba”)\n[0.93]\nt1,2=0.92, t2,3=0.45\nt3,4=0.09, t4,5=0.03\nHigh CSAT \nModel \nProbability\nSoftmax\n[0.07]\nif proba > t1,2\nclass 2\nclass 3\nclass 4\nclass 5\nelif proba > t2,3\nelif proba > t3,4\nelif proba > t4,5\nelse\nFigure 1: The mapping function that takes low CSAT\nprobability (\"proba\") as input and outputs 1-5 pCSAT.\nIn this example, the \"proba\" is 0.93, which is larger\nthan t1,2 so the model emits a pCSAT of 1.\n1. The average pCSAT should equal the average\nsurvey CSAT over the same set of calls. By\ndefault, the displayed average pCSAT (and\naverage CSAT) is the % of satisfied calls, i.e.,\ncalls with pCSAT ≥4.\n2. These averages should also match when tog-\ngled to use a 1-5 scale.\n3. The distribution of pCSAT and survey CSAT\nshould match as closely as possible\nBased on customer feedback we set 1% and 0.1 as\nthe maximum differences to target for requirements\n1 and 2 respectively.\n3.1.3\nParameter Estimation\nJointly estimate the four thresholds:\nTo find the\noptimal parameters, our process iterates through\ndifferent combinations of thresholds to find the set\nthat minimizes the loss.\nTo meet all three product requirements, we cre-\nated the following loss function:\nLoss = ∆%p,c + ∆avgp,c + MSEp,c\nwhere p and c are short for pCSAT and CSAT, re-\nspectively, and:\n∆%p,c = | (% of pCSAT ≥4)−(% of CSAT ≥4) |\n∆avgp,c = |avg_pcsat −avg_csat|\nMSEp,c = MSE(\n⃗\npcsat, ⃗\ncsat)\nWe preferred a random search to a grid search\nto save computation time. We used 5000 iterations\nfor random search, but found 98.7% convergence\nby 500 iterations.\n\n3.1.4\nOptimization Steps:\n1. Compute the Number of Calls for Each Survey\nCSAT Level:\nncsati = number of calls with survey CSAT\nwhere class i ∈(1, 2, 3, 4, 5)\n2. Calculate the Average Survey CSAT:\navg_csat =\nP5\ni=1(ncsati · i)\nP5\ni=1 ncsati\n3. Initialize the loss:\nbest_loss = 1000.0\n4. Random Search for Optimal Thresholds: Per-\nform a random search through the possible\nthresholds to find the set that minimizes the\nloss. For j in range(5000):\n(a) Generate 4 uniform random values and\nsort them:\nt12 > t23 > t34 > t45 ∼U(0, 1)\n(b) Compute the Number of Calls for Each\npCSAT Level:\nnpcsati = number of calls with pCSAT\nwhere class i ∈(1, 2, 3, 4, 5)\n(c) Calculate the Average pCSAT:\navg_pcsat =\nP5\ni=1(npcsati · i)\nP5\ni=1 npcsati\n(d) Compute the Delta Between Average pC-\nSAT and CSAT:\n∆pcsat_csat = avg_pcsat −avg_csat\n(e) Compute ∆%p,c\n(f) Normalize both CSAT Vectors to unit\nlength:\n⃗\npcsat = normalized([npcsat1, . . . , npcsat5])\n⃗\ncsat = normalized([ncsat1, . . . , ncsat5])\n(g) Calculate the Mean Squared Error Be-\ntween the Normalized Vectors (MSEp,c)\n(h) Compute the loss:\nLoss = ∆%p,c + ∆avgp,c + MSEp,c\n(i) Update the loss and best thresholds if the\ncurrent loss is lower.\n3.2\nExperimental Conditions\nWe evaluated the loss under five conditions:\n1. Baseline: Naive model output (evaluated over\ntest period)\n2. Global Threshold: Thresholds are fitted on a\nsingle, global pool (evaluated over test period)\n3. Call Center Threshold: Individual threshold-\ning for each call center (evaluated over test\nperiod)\n4. Train Period: We apply the same call center-\nspecific parameters as used in the Call Center\nThreshold condition, but apply them to the\ntraining period instead of the test period. As\nwe expect a near-zero difference of means,\nthis serves to validate our parameter estima-\ntion method.\n5. Bootstrap (Train Period): This approach\nis similar to the \"train\" condition, but the\nkey difference is that we repeatedly resam-\nple the training set and measure the difference\nof means over these samples. This method\nhelps us estimate how much of the loss in the\n\"Call center thresholding\" condition is due to\nsampling noise, and attribute the rest to differ-\nences between train and test distributions, i.e.\nmodel drift.\nWe note that the first 3 conditions are test condi-\ntions, i.e. evaluated on the test set, whereas the\nlast 2 are train conditions which help us understand\nsources of error.\n4\nResults\nThe effectiveness of different methods for predict-\ning customer satisfaction (CSAT) scores was evalu-\nated through various experimental conditions. The\nresults are summarized in Figures 2-5, and detailed\nobservations are as follows:\n4.1\nLoss\nOverall loss, depicted in Figure 2, combines the\ndifference of means, difference of percent satisfied,\nand MSE. We see that the Baseline method consis-\ntently has the highest loss, and the Train condition\nhas the lowest. The train condition does not have\n0 error because it is not usually possible to find 4\nparameters to zero all 3 terms that make up the loss\nsimultaneously.\n\nCount of Survey Responses \nLoss\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1-50\n51-200\n201-500\n501-1000\n> 1000\nBaseline\nGlobal\nCall Center\nTrain\nBootstrap\nFigure 2: The average loss for each of the five exper-\nimental conditions, binned by the call center’s CSAT\nsurvey responses.\nNow we compare the test conditions. For call\ncenters with the smallest response volumes, the\nGlobal Threshold method performs best. On the\nother hand, the Call Center Threshold method per-\nformed best for call centers with the largest re-\nsponse volumes. Overall, we see a gradual trend\nof this method improving as the response volume\nincreases. This makes sense since the Call Cen-\nter Threshold method is limited by sampling noise,\nwhich is greatest for small response volumes. In-\ndeed, we can see the effect of the sampling noise\ndirectly by looking at how much more loss the\nBootstrap condition has relative to the Train con-\ndition at low (< 200) response volumes. As we\nget to higher (> 500) response volumes, we ob-\nserved that the Bootstrap condition catches up with\nthe Train condition, which indicates that sampling\nerror ceases to be significant at those volumes.\n4.2\nDifference in % of Satisfied Calls\nCount of Survey Responses \nDifference of %\n0.00%\n2.00%\n4.00%\n6.00%\n8.00%\n10.00%\n1-50\n51-200\n201-500\n501-1000\n> 1000\nBaseline\nGlobal\nCall Center\nTrain\nBootstrap\nFigure 3: The average difference in percentage of satis-\nfied calls between pCSAT and CSAT, broken down by\nthe call center’s count of survey responses.\nFigure 3 examines the first component of the loss\nfunction, the difference in percentage of satisfied\ncalls between pCSAT and CSAT, averaged over the\ncall centers in each bin. The Baseline method was\nonly outperformed for call centers with the largest\nresponse volume (> 1000). For these call centers,\nthe Call Center Threshold method performs best,\nfollowed by Global Threshold. We also note an\nunexpected uptick in error for all 3 test conditions,\ncompared to smaller response volumes.\n4.3\nDifference of Means\nCount of Survey Responses \nDifference of Averages\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n1-50\n51-200\n201-500\n501-1000\n> 1000\nBaseline\nGlobal\nCall Center\nTrain\nBootstrap\nFigure 4: The average absolute difference between mean\nPredicted CSAT and survey CSAT for each of the five\nexperimental conditions, binned by the call center’s\nCSAT survey responses.\nFigure 4 focuses on the average absolute differ-\nence between mean Predicted CSAT and survey\nCSAT. The Baseline method consistently lags other\nmethods, showing our methods create a substan-\ntial improvement. The Global Threshold method\nperforms best at the lowest response volumes (<\n200), whereas the Call Center Threshold method\noutperforms other methods from 200 calls onwards,\nconsistent with its requirement of small sampling\nnoise.\nAs expected, the Train and Bootstrap conditions\nshow very low percentages, further validating the\nparameter estimation and highlighting the minimal\nimpact of sampling noise after 500 calls.\n4.4\nMSE\nThe MSE, shown in 5, measures the vector simi-\nlarity between the pCSAT and CSAT distributions.\nThe Baseline method exhibits by far the highest\nMSE values across all call volumes, so much so\nthe figure requires a logarithmic scale. It exceeds\n0.1 for all survey response volumes. The Global\nThreshold method is consistently best at lowering\nthe MSE, though the gap with Call Center Thresh-\nold narrows as survey responses increase.\n\nCount of Survey Responses \nMSE\n0.001\n0.005\n0.01\n0.05\n0.1\n1-50\n51-200\n201-500\n501-1000\n> 1000\nBaseline\nGlobal\nCall Center\nTrain\nBootstrap\nFigure 5: The Mean Squared Error measures the vector\nalignment between the normalized Predicted CSAT and\nsurvey CSAT distributions. Shown for each of the five\nexperimental conditions, binned by the call center’s\nCSAT survey responses.\n5\nDiscussion\nSome but not all initial targets were achieved:\n• Difference in Percentage of Satisfied Calls:\nNot achieved. The methods were not able to\nimprove over Baseline or meet the target of\nless than 1%. The fact the fitting thresholds\ndoes not improve the output class distribution\nsuggests that the baseline classifier outputs\nmay be already well distributed. This is a\nlikely explanation because this metric treats\nCSAT as binary (call satisfied if CSAT >= 4)\nand the baseline classifier is trained on binary\nCSAT data. Further work may be required to\nyield improvements.\n• Difference of Means:\nThe Call Center\nThreshold method consistently achieved the\ntarget (difference of means less than 0.1) for\ncall centers with survey responses greater than\n500-1000. This method also achieved signifi-\ncant improvements over Baseline for all call\ncenter bins.\n• Mean Squared Error (MSE): Both the\nGlobal Threshold and Call Center Threshold\nmethods significantly improved the MSE over\nBaseline, indicating improved alignment be-\ntween pCSAT and actual CSAT distributions.\nIn this case, there was no quantitative target,\nbut the improvement is over 10X.\nHaving learned from the varying performance\nof the methods across different survey response\nvolumes, we are now considering implementing\na hybrid approach. Specifically, using the Global\nThreshold method for call centers with fewer than\n200 survey responses and Call Center Threshold\nmethod for larger ones. This hybrid strategy lever-\nages the strengths of both methods, ensuring more\naccurate and reliable pCSAT predictions across di-\nverse operational contexts.\nWe recommend a similar approach for multi-\nclass classification problems where a consistent\nclass balance is important across model updates.\nOur approach can be used whether there is a single\npool of inputs, or subgroups analogous to our call\ncenters. Developers should be cautious of the sam-\npling noise in their datasets and use a data-driven\napproach to determine the minimum sample sizes\nfor their specific application.\n6\nLimitations\nWhile our thresholding method demonstrates sub-\nstantial improvements, several limitations must be\nacknowledged:\n• Sampling Noise: As highlighted, small call\ncenters with low survey response volumes suf-\nfer from high sampling noise, limiting the ef-\nfectiveness of our approach for sample sizes\nunder 500, especially call center thresholding.\n• Temporal Stability: Although our method\nshows promise in maintaining low loss over\nat least 4 months, we did not examine the\ntimecourse of errors over those 4 months or\nbeyond. Long-term drift could be a concern\nand warrants further investigation.\n7\nEthics Statement\nIn developing and implementing this method, we\nhave adhered to ethical standards to ensure fairness,\ntransparency, and accountability:\n• Bias Mitigation: Previously, we have sam-\npled subpopulations of users and evaluated\ninternally to ensure the pCSAT is not biased\nagainst specific groups. This approach takes\na further step to reduce bias in pCSAT scores\nby ensuring a more accurate reflection of cus-\ntomer satisfaction across different call centers.\nHowever, continuous evaluation and improve-\nment are necessary to address any emergent bi-\nases, and our near-term plans include quantifi-\nable and verifiable explainability for AI CSAT\nwhich will help our users pinpoint the causes\nof low pCSAT, including any bias.\n\n• Transparency: We have documented our\nmethods and findings comprehensively to pro-\nvide clear insights into our process and its\nimpact on prediction accuracy.\n• Data Privacy: All customer data used in this\nstudy has been anonymized and handled in\ncompliance with data privacy regulations to\nprotect individual privacy. We follow the data\nprivacy measures in place at Dialpad which\ninclude scrubbing personal identifiable infor-\nmation (PII) from customer data and restrict-\ning our use of customer data to improvements\nto the services we provide them. We did not\nrely on any external annotations.\n• Stakeholder Impact: The improved accuracy\nin CSAT predictions enables better decision-\nmaking for coaching, follow-up, and ser-\nvice improvements, ultimately benefiting cus-\ntomers and call center performance.\n8\nAcknowledgements\nWe would like to acknowledge the contributions of\nour colleagues, the support from Dialpad, and the\nfeedback from call center managers who helped\nrefine our approach. In particular, we thank person\nDoug Mackenzie for maintaining essential database\ntables.\nReferences\nJeremy Auguste, Delphine Charlet, Geraldine Damnati,\nFrederic Bechet, and Benoit Favre. 2019. Can we\npredict self-reported customer satisfaction from inter-\nactions? In Proceedings of the 2019 Conference on\nNeural Information Processing Systems (NeurIPS),\npages 7385–7389.\nJoseph Bockhorst, Shi Yu, Luisa Polania, and Glenn\nFung. 2017. Predicting self-reported customer sat-\nisfaction of interactions with a corporate call center.\nIn Machine Learning and Knowledge Discovery in\nDatabases, pages 179–190, Cham. Springer Interna-\ntional Publishing.\nJ. S. Cardoso and J. F. Pinto da Costa. 2007. Learning\nto classify ordinal data: The data replication method.\nJournal of Machine Learning Research, 8:1393–1429.\nRetrieved from jmlr.org (SpringerLink).\nW. Chu and S. S. Keerthi. 2007. Support vector ordi-\nnal regression. Neural Computation, 19(3):792–815.\nRetrieved from doi.org (SpringerLink).\nLaurence Devillers, Caroline Vaudable, and Chan-\ntal Chastagnol. 2010.\nReal-life emotion-related\nstates detection in call centers:\na cross-corpora\nstudy. In Eleventh Annual Conference of the Interna-\ntional Speech Communication Association (INTER-\nSPEECH), volume 10, pages 2350–2353.\nC. Ferri, J. Hernández-Orallo, and M. A. Salido. 2002.\nLearning decision trees using the area under the roc\ncurve. In Proceedings of the 19th International Con-\nference on Machine Learning, pages 139–146. Re-\ntrieved from researchgate.net.\nE. Frank and M. Hall. 2001. A simple approach to\nordinal classification. In European Conference on\nMachine Learning, pages 145–156. Retrieved from\nspringer.com.\nS. B. Kotsiantis, D. Kanellopoulos, and P. E. Pinte-\nlas. 2006. Handling imbalanced datasets: A review.\nGESTS International Transactions on Computer Sci-\nence and Engineering, 30(1):25–36. Retrieved from\ngests-intl.com.\nE. Manderscheid and M. Lee. 2023. Predicting cus-\ntomer satisfaction with soft labels for ordinal classifi-\ncation. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 5: Industry Track), pages 652–659, Toronto,\nCanada. Association for Computational Linguistics.\nAnonymized for review.\nYoung-Bum Park and Stephen C. Gates. 2009. Towards\nreal-time measurement of customer satisfaction using\nautomatically generated call transcripts. In Proceed-\nings of the 18th ACM Conference on Information and\nKnowledge Management, pages 1387–1396. ACM.\nCaroline Vaudable and Laurence Devillers. 2012. Neg-\native emotions detection as an indicator of dialogs\nquality in call centers. In 2012 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 5109–5112. IEEE.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big bird: Transformers for\nlonger sequences. arXiv preprint arXiv:2007.14062.\nSubmitted on 28 Jul 2020 (v1), last revised 8 Jan\n2021 (v2).\nGeoffrey Zweig, Olivier Siohan, George Saon, Bhu-\nvana Ramabhadran, Daniel Povey, Lidia Mangu, and\nBrian Kingsbury. 2006. Automated quality monitor-\ning for call centers using speech and nlp technolo-\ngies. In Proceedings of the 2006 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics on Human Language Technol-\nogy: Companion Volume: Demonstrations, pages\n292–295. Association for Computational Linguistics.",
    "pdf_filename": "Predicting_Customer_Satisfaction_by_Replicating_the_Survey_Response_Distribution.pdf"
}