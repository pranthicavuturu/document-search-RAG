{
    "title": "Adapting Amidst Degradation Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Ti",
    "context": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and efficient energy management and carries significant socio- economic implications. Although Machine Learning (ML)-based State of Health (SOH) estimation methods have made significant progress in accuracy, the scarcity of high-quality LIB data remains a major obstacle. Although existing transfer learning methods for cross-domain LIB SOH estimation have significantly alleviated the labeling burden of target LIB data, they still require sufficient unla- beled target data (UTD) for effective adaptation to the target domain. Collecting this UTD is challenging due to the time-consuming na- ture of degradation experiments. To address this issue, we introduce a practical Test-Time Training framework, BatteryTTT, which adapts the model continually using each UTD collected amidst degra- dation, thereby significantly reducing data collection time. To fully utilize each UTD, BatteryTTT integrates the inherent physical laws of modern LIBs into self-supervised learning, termed Physcics- Guided Test-Time Training. Additionally, we explore the poten- tial of large language models (LLMs) in battery sequence model- ing by evaluating their performance in SOH estimation through model reprogramming and prefix prompt adaptation. The combi- nation of BatteryTTT and LLM modeling, termed GPT4Battery, achieves state-of-the-art generalization results across current LIB benchmarks. Furthermore, we demonstrate the practical value and scalability of our approach by deploying it in our real-world battery management system (BMS) for 300Ah large-scale energy storage LIBs. CCS Concepts â€¢ Test Time Training â†’Battery Health Estimation. Keywords Battery Health Estimation, Test Time Training, Data Scarcity, Large Language Model Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018. Adapting Amidst Degradation: Cross Domain Li-ion Battery Health Es- timation via Physics-Guided Test-Time Training. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym â€™XX). ACM, New York, NY, USA, 12 pages. https: //doi.org/XXXXXXX.XXXXXXX 1 One Unlabeled Data Physics Guided Self-Supervised Learning Step3 TTA Source Domain BatteryML, LLMs Source LIB Real-world Deployment Step2 Pre-training Target Domain Step1 Step2 Step3 Step4 Prefix-Prompt Adaptation Step4 Figure 1: Overview of BatteryTTT framework, which consists of three major components: (Step 1) pre-training on experimental datasets; (Step 2) incremental data collection after deployment; and (Steps 3-4) test-time adaptation. Steps 2, 3, and 4 iterate until the LIB retires. The rapid advancements in rechargeable Li-ion batteries (LIBs) have facilitated their widespread use across various sectors, includ- ing portable electronics, medical devices, renewable energy systems, and electric vehicles [11]. This ubiquity, however, introduces critical challenges associated with capacity degradation and performance evaluation. As an inherently interdisciplinary subject, battery aging modeling has emerged as a fundamental issue at the intersection of battery science and machine learning (ML) [25, 32, 33, 51]. Accurate State of Health (SOH) estimation for LIBs is crucial not only for ensuring safe and efficient energy management but also for opti- mizing the design and performance of next-generation batteries, thus having significant socio-economic implications. With the rapid advancement of ML technology, data-driven SOH estimation models have achieved significant progress in both ac- curacy and computational efficiency [32, 51]. However, obtaining sufficient training data for LIBs is challenging due to the time- consuming nature of degradation experiments, which typically arXiv:2402.00068v3  [cs.LG]  19 Nov 2024",
    "body": "Adapting Amidst Degradation: Cross Domain Li-ion Battery\nHealth Estimation via Physics-Guided Test-Time Training\nYuyuan Feng\nfengyuyuan01@gmail.com\nXiamen University\nXiamen, China\nGuosheng Hu\nUniversity of Bristol\nBristol, England\nhuguosheng100@gmail.com\nXiaodong Li\nHong Kong University\nHong Kong, China\nZhihong Zhang*\nXiamen University\nXiamen, China\nAbstract\nHealth modeling of lithium-ion batteries (LIBs) is crucial for safe\nand efficient energy management and carries significant socio-\neconomic implications. Although Machine Learning (ML)-based\nState of Health (SOH) estimation methods have made significant\nprogress in accuracy, the scarcity of high-quality LIB data remains\na major obstacle. Although existing transfer learning methods for\ncross-domain LIB SOH estimation have significantly alleviated the\nlabeling burden of target LIB data, they still require sufficient unla-\nbeled target data (UTD) for effective adaptation to the target domain.\nCollecting this UTD is challenging due to the time-consuming na-\nture of degradation experiments. To address this issue, we introduce\na practical Test-Time Training framework, BatteryTTT, which\nadapts the model continually using each UTD collected amidst degra-\ndation, thereby significantly reducing data collection time. To fully\nutilize each UTD, BatteryTTT integrates the inherent physical laws\nof modern LIBs into self-supervised learning, termed Physcics-\nGuided Test-Time Training. Additionally, we explore the poten-\ntial of large language models (LLMs) in battery sequence model-\ning by evaluating their performance in SOH estimation through\nmodel reprogramming and prefix prompt adaptation. The combi-\nnation of BatteryTTT and LLM modeling, termed GPT4Battery,\nachieves state-of-the-art generalization results across current LIB\nbenchmarks. Furthermore, we demonstrate the practical value and\nscalability of our approach by deploying it in our real-world battery\nmanagement system (BMS) for 300Ah large-scale energy storage\nLIBs.\nCCS Concepts\nâ€¢ Test Time Training â†’Battery Health Estimation.\nKeywords\nBattery Health Estimation, Test Time Training, Data Scarcity, Large\nLanguage Model\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nÂ© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nYuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018.\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Es-\ntimation via Physics-Guided Test-Time Training. In Proceedings of Make\nsure to enter the correct conference title from your rights confirmation emai\n(Conference acronym â€™XX). ACM, New York, NY, USA, 12 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1\nIntroduction\nOne Unlabeled Data\nPhysics Guided \nSelf-Supervised \nLearning\nStep3\nTTA\nSource Domain\nBatteryML, LLMs\nSource LIB\nReal-world Deployment\nStep2\nPre-training\nTarget Domain\nStep1\nStep2\nStep3\nStep4\nPrefix-Prompt \nAdaptation\nStep4\nFigure 1: Overview of BatteryTTT framework, which consists of\nthree major components: (Step 1) pre-training on experimental\ndatasets; (Step 2) incremental data collection after deployment; and\n(Steps 3-4) test-time adaptation. Steps 2, 3, and 4 iterate until the LIB\nretires.\nThe rapid advancements in rechargeable Li-ion batteries (LIBs)\nhave facilitated their widespread use across various sectors, includ-\ning portable electronics, medical devices, renewable energy systems,\nand electric vehicles [11]. This ubiquity, however, introduces critical\nchallenges associated with capacity degradation and performance\nevaluation. As an inherently interdisciplinary subject, battery aging\nmodeling has emerged as a fundamental issue at the intersection of\nbattery science and machine learning (ML) [25, 32, 33, 51]. Accurate\nState of Health (SOH) estimation for LIBs is crucial not only for\nensuring safe and efficient energy management but also for opti-\nmizing the design and performance of next-generation batteries,\nthus having significant socio-economic implications.\nWith the rapid advancement of ML technology, data-driven SOH\nestimation models have achieved significant progress in both ac-\ncuracy and computational efficiency [32, 51]. However, obtaining\nsufficient training data for LIBs is challenging due to the time-\nconsuming nature of degradation experiments, which typically\narXiv:2402.00068v3  [cs.LG]  19 Nov 2024\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nTable 1: Comparison between BatteryTTT and other transfer learn-\ning methods regarding the target LIB dataset prerequisites. Note: the\ntime estimates are based on the KOKAM dataset [3].\nMethods\nwoDegrad\nSSF-WL\nBatteryTTT(Ours)\nUnlabeled Data\n100%\n30%\n1 amidst degradation\nLabeling\n0%\n30%\n0%\nCollection Time\n8473 hours\n2542 hours\n0 hours\nspan months to years. Additionally, precisely labeling this data\nnecessitates additional cycles under controlled temperature and\ncurrent conditions in the laboratory. Consequently, the scarcity of\nhigh-quality LIB data continues to present a major obstacle obstacle\nin battery aging modeling [22, 33, 51].\nTo address the challenge of data scarcity, existing studies have\nextensively explored transfer learning methods for cross-domain\nLIB SOH estimation [22, 35, 39, 41, 46]. For instance, SSF-WL [46]\nintroduced a self-supervised approach that pre-trains on unlabeled\nLIB data from a source domain and fine-tunes on a different type of\nLIB (target domain) using only a small portion of labeled data. This\nmethod achieves comparable results using just 30% of the labeled\ntarget data. Another notable study, woDegrad [22], proposed mini-\nmizing the domain gap between source and target LIBs by aligning\ntheir features in the latent space, eliminating the need for additional\nlabeled target data to estimate SOH.\nUnfortunately, while existing studies have significantly allevi-\nated the burden of data labeling, they often overlook the challenge of\ncollecting unlabeled target data (UTD). Due to the time-consuming\nnature of degradation experiments, gathering sufficient UTD for\na typical LIB can take months to years, depending on the battery\ntype. As a result, preparing sufficient UTD for previous algorithms\nto function effectively also demands a considerable amount of time.\nFor example, migrating a pre-trained woDegrad model [22] to the\nKOKAM dataset [3], a widely used LIB dataset, would require a\nminimum of 8,473 hours to collect enough UTD from KOKAM\nfor effective domain gap alignment, which is hardly practical in\nreal-world applications. To the best of our knowledge, few studies\nhave successfully estimated SOH without relying on a substantial\namount of UTD.\nThe purpose of this study is to develop a practical transfer learn-\ning framework for cross-domain LIB SOH estimation that minimizes\nthe reliance on UTDs. Inspired by the Test-Time Training (TTT) 1\ntechnique from computer vision [10, 38], we propose BatteryTTT.\nUnlike previous methods that require a substantial amount of UTDs\ncollected over time, BatteryTTT adapts the model continually using\neach individual UTD collected amidst degradation. In Table 1, we\ncompare the prerequisites of the target LIB dataset for BatteryTTT\nwith those of other transfer learning methods. BatteryTTT signifi-\ncantly reduces the amount of required UTD and labeling, offering a\nmore efficient approach compared to existing methods. To achieve\nthis, BatteryTTT employs a proxy unsupervised task to utilize the\nUTD for gradient-based model updates. Although some unsuper-\nvised methods from existing TTT literature can be applied, such as\nself-prediction, they are not specifically designed for battery-related\n1In the following sections, the terms Test-Time Training (TTT) and Test-Time Adapta-\ntion (TTA) may be used interchangeably.\ntasks, leading to suboptimal performance. To address this, we ex-\nplore the integration of the inherent physical laws of modern LIBs\ninto self-supervised learning within BatteryTTT, a framework we\nterm Physics-Guided Test-Time Training. This approach lever-\nages the 1-RC Equivalent Circuit Model (ECM) equations to guide\nthe pre-trained model in making accurate self-predictions, leading\nto improved results. The details of our method and experimental\nresults are discussed in the following sections.\nOn the other hand, with the rapid advancement of large language\nmodels (LLMs) [8, 28, 29, 42] there has been growing interest in\nexploring their potential for processing cross-disciplinary sequence\ndata beyond natural language, including protein sequence predic-\ntion [23] and time series analysis [19, 54]. Pioneering research\nhas validated the efficacy of this paradigm and highlighted the\nunderlying zero-shot generalization capabilities of LLMs across\nvarious datasets. However, the application of LLMs to battery se-\nquence modeling remains undiscovered. To address this gap, we\nevaluate the performance of LLMs for processing cross-domain\nLIB sequences in this study. Specifically, We employ the concept\nof model reprogramming to bridge the gap between language and\nbattery modalities. Additionally, we develop a prefix prompt adap-\ntation strategy to efficiently integrate an LLM into our BatteryTTT\nframework. This combination of strategies, termed GPT4Battery,\nachieves state-of-the-art generalization results among current LIB\nbenchmarks.\nBy introducing the BatteryTTT framework and GPT4Battery\nmodel, we hope this study will inspire the research community to\nfully leverage advanced AI techniques, such as Test-Time Training\n(TTT) and large language models (LLMs), for more scenario-fitting\nAI4Science problems, thereby saving enormous time and acceler-\nating scientific discovery. For the rest of this paper, Sec.2 presents\nbackground and related works, Sec. 3 introduces the preliminar-\nies, Sec. 4 describes the methodology of BatteryTTT framework\nand GPT4Battery model, Sec. 5 conducts experiments, and Sec. 6\nconcludes.\n2\nRelated Work\nIn this section, we introduce the related works in Data-Driven\nBattery SOH Estimation (Sec. 2.1), Test-Time Adaptation (Sec. 2.2) and\nLLM for Cross-disciplinary Sequence Modeling (Sec. 2.3), respectively.\n2.1\nData-Driven Battery SOH Estimation\nData-driven battery SOH estimation as ascended as a pivotal topic\nin industrial artificial intelligence and data mining with the wide-\nspread adoption of modern LIBs in various applications, bringing a\nsurge in demand for safe and efficient battery management[25, 32].\nWith the evolution of model architectures within the AI community,\nalgorithms for battery state estimation have also progressed from\nstatistical machine learning methods, such as Random Forest [4]\nand Gaussian Process Regression [30], to high-performance deep\nneural networks, including MLP [14], LSTM [16], and Transformer\n[51].\nHowever, to obtain both battery training data and ground-truth\nlabels requires time- and resource-consuming degradation experi-\nments, posing a persistent hurdle in battery aging modeling [22, 32,\n33, 51]. Noticing this significant issue, researchers in the battery\n\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time TrainingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nfield have tried to use transfer learning methods for generalizable\nSOH estimation [22, 35, 39, 41, 46]. However, there is a notable gap\nbetween current methodsâ€™ assumptions about having access to the\ntarget LIB dataset and the real-world situation, especially regard-\ning the unlabeled target data (UTD). while existing studies have\nsignificantly alleviated the burden of data labeling, they often over-\nlook the challenge of collecting UTDs. Due to the time-consuming\nnature of degradation experiments, gathering sufficient UTD for\na typical LIB can take months to years, depending on the battery\ntype. This cost of time is unacceptable in real-world deployment\n(as shown in Table 1). Conversely, BatteryTTT fully utilizes each\nindividual UTD collected amidst degradation for adaptation to the\ntarget domain, thereby significantly reducing data collection time\n2.2\nTest-Time Training\nTest-time Training (TTT)/Adaptation (TTA), also known as one-\nsample unsupervised domain adaptation, aims to adapt a model\ntrained on the source domain to the target domain as every un-\nlabeled test sample arrives [20, 38]. The process of TTA usually\ninvolves a self-supervised loss to extract information from the sin-\ngle target domain sample, such as rotation [38], mask [10] and en-\ntropy minimization [45]. Despite the study of various TTA methods,\nmost are designed for (image) classification and cannot be applied\nto time series regression. For instance, Test-time Entropy Mini-\nmization (Tent) [45] found that the entropy of prediction strongly\ncorrelates with accuracy on the target domain, BACS [53], MEMO\n[52], and EATA [26] follow Tentâ€™s approach and improve adaptation\nperformance, making them the most representative TTA methods.\nAlthough some unsupervised methods from existing TTT literature\ncan be applied, such as self-prediction, they are not specifically de-\nsigned for battery-related tasks, leading to suboptimal performance.\nIn this paper, we explore how to incorporate the inherent physics\nof LIBs into self-supervised learning, resulting in a more natural\nand powerful approach.\n2.3\nLLM for Cross-Disciplinary Sequence\nModeling\nWith the rapid advancement of large language models (LLMs)\n[8, 28, 29, 42] there has been growing interest in exploring their\npotential for processing cross-disciplinary sequence data beyond\nnatural language, including protein sequence prediction [23] and\ntime series analysis [6, 13, 36, 54]. For protein sequence prediction,\nLLM have showed powerful cross-domain potential against pro-\ntein language models by the design of vocabulary [9, 31]. For time\nseries, these efforts have evolved from the initial direct applica-\ntion of large language models (LLMs) to sequence tasks [54], to\ndesigning a learned dictionary of prompts to guide inference [5],\nto attempting to align the semantic spaces between language and\ntime series modalities [18, 27]. Although the effectiveness of large\nlanguage models (LLMs) in handling cross-disciplinary sequence\ndata has been demonstrated, the field of battery research has yet to\nbenefit from this advancement. In this paper, we address this gap by\nrepurposing an LLM for State of Health (SOH) estimation through\nmodel reprogramming. Additionally, we evaluate the generalization\nimprovements achieved by adapting an LLM for cross-battery SOH\nestimation.\n3\nPreliminaries\n3.1\nBattery SOH Definition\nAs batteries undergo repeated charge and discharge cycles, their ca-\npacity gradually declines due to aging, which leads to performance\ndegradation and potential safety issues. The State of Health (SOH)\nquantifies the batteryâ€™s remaining capacity relative to its initial\ncapacity when new. Specifically, if we denote the nominal capacity\nof the LIB as ğ¶ğ‘›ğ‘œğ‘Ÿğ‘šand the full discharge capacity in the current\ncycle as ğ¶ğ‘“ğ‘¢ğ‘™ğ‘™, the SOH is defined as the ratio of these two values,\nexpressed as a percentage:\nSOH = ğ¶full\nğ¶nom\nÃ— 100%.\n(1)\nLIBs are typically considered to have reached the end of their\nlife cycle when their SOH drops to approximately 75%.\n3.2\nFeature Engineering\nIn this study, we utilize QdLinear [33], a degradation feature derived\nfrom the linear interpolation of the voltage-capacity curve during\ncharge cycles, to map the relationship with SOH. This feature is\nwidely recognized and employed by mainstream SOH estimation\nalgorithms [22, 25, 33, 51].\n3.3\nProblem Definition\nWe formalize the problem of cross-domain LIB SOH estimation\nas follows. Given a well-curated battery dataset from the source\ndomain, denoted as S = {(ğ‘¥1,ğ‘¦1), (ğ‘¥2,ğ‘¦2), . . . , (ğ‘¥ğ‘†,ğ‘¦ğ‘†)}, where\nX âˆˆR1Ã—ğ‘‡represents the extracted QdLinear feature with ğ‘‡time\nsteps, and ğ‘¦denotes the corresponding SOH label. This source\nset contains ğ‘†labeled lifelong samples. In contrast, for a different\nbattery type in the target domain, we can acquire only one unla-\nbeled feature at a time after real-world deployment, represented as\nT = {ğ‘¥1,ğ‘¥2, . . . ,ğ‘¥ğ‘‡}. Our objective is to estimate each correspond-\ning target label ğ‘¦ğ‘¡, with ğ‘¦1 being considered as SOH = 100% for a\nnew battery.\n4\nMethodology\nIn this section, we present the methodology of our framework. Af-\nter providing a systematic overview in Section 4.1, we focus on\ntwo key innovations: Physics-Guided Self-Supervised Learning (PG-\nSSL) and Prefix Prompt Adaptation (PPA), which are detailed in\nSections 4.2 and 4.3. In Section 4.4, we explain how existing State of\nHealth (SOH) estimation models are integrated into the BatteryTTT\nframework for cross-domain transfer learning, alongside the ex-\nploration of Large Language Models (LLMs) for battery sequence\nmodeling.\n4.1\nSystem Overview\nFigure 1 depicts an overview of the BatteryTTT framework, which is\ncomposed of three major components: pre-training on experimental\ndatasets, incremental data collection in real-world deployment, and\ntest-time adaptation. Firstly, we utilize experimental datasets (source\ndomain) to train a pre-trained model for SOH estimation. We then\ndeploy this pre-trained model to real-world devices, such as the\nBattery Management System (BMS) of an electric car or a mobile\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nphone. The BMS individually collects unlabeled test data during\nusage, and upon receiving a sample, we conduct a test-time training\nprocess to adapt the pre-trained model to this different type of\nLIB (target domain). Specifically, the test-time adaptation process\nconsists of two steps: PG-SSL to construct an unsupervised loss from\nthe unlabeled sample, and PPA to adapt the pre-trained model to this\nnew domain in a parameter-efficient manner. After learning from\nincoming data, the adapted model is ready to make a prediction.\nThis process operates continually until the LIB retires.\n4.2\nPhysics-Guided Self-supervised Learning\nğ¶à¯£\nğ‘…à¯£\nğ‘…\nğ‘‚ğ¶ğ‘‰\nğ¼\nğ‘¢à¯£\nğ‘¢\nPhysics-Guided  \nLoss\n1-RC ECM Equation\nUnlabeled Qdlinear \nFeature\nFigure 2: Transform an arbitrary unlabeled Qdlinear feature\ninto a Physics-Guided loss.\nIn this subsection, we elaborate on how to fully exploit the in-\nherent physical laws of a single LIB sequence to improve TTA\nperformance through the design of physics-guided self-supervised\nlearning (PG-SSL). Specifically, we first describe how to use the\n1-RC ECM equation to transform an arbitrary unlabeled Qdlinear\nfeature into a physics-guided loss (PGLoss). We will then explain\nhow minimizing this PGloss facilitates the estimation of the LIB\nSOH.\n4.2.1\nTheveninâ€™s Equivalent Circuit Model (ECM) of LIB. The equiv-\nalent circuit model (ECM) is widely used battery model to describe\nthe electrical behavior of the battery in terms of voltages, currents,\nresistances and capacitances [48] [43]. The first order Thevenin\nmodel is thought to be accurate and adequate to model the condi-\ntion of the battery, and at the same time simple and computationally\nefficient [49]. The ğ‘‚ğ¶ğ‘‰is represented by an ideal voltage source\nof the battery. ğ‘…accounts for the internal ohmic resistance. The\nparallel ğ‘…ğ¶-branch, comprising ğ‘…ğ‘ƒand ğ¶ğ‘ƒ, is used to model battery\npolarization effect. ğ‘¢and ğ¼denotes the terminal voltage and current\nthat can be collected in use. Based on Kirchhoffâ€™s law, the electrical\nbehavior of the battery can be characterized as physical equations\nas:\nğ‘¢ğ‘‚ğ¶ğ‘‰= ğ‘¢ğ‘…+ ğ‘¢ğ‘+ ğ‘¢\n(2)\nğ‘…+ ğ‘…ğ‘\nğ¶ğ‘ğ‘…ğ‘\nğ¼+\n1\nğ¶ğ‘ğ‘…ğ‘\nğ‘¢+ Â¤ğ‘¢= 0\n(3)\nWe defineğœƒ1 = ğ‘…+ğ‘…ğ‘\nğ¶ğ‘ğ‘…ğ‘andğœƒ2 =\n1\nğ¶ğ‘ğ‘…ğ‘. Following recent works, the\ncoeffients are functions of temperature. The state function becomes:\nğœƒ1(ğ‘‡)ğ¼+ ğœƒ2(ğ‘‡)ğ‘¢+ Â¤ğ‘¢= 0\n(4)\nTime/Capacity\n \nRandom Mask\n(b) Generated complete Qdlinear feature\nGradients \n& Value\nModel\nTime/Capacity\nVoltage\nVoltage\nMaximum capacity (SOH)\n(a) Partial Qdlinear feature\nPGLoss\nFigure 3: Guide the pre-trained model to generate a complete\nQdlinear feature curve and supervise it with Physics-Guided\nloss.\nThis implies that the terminal voltage ğ‘¢, current ğ¼and tempera-\nture should, in principle, follow the ordinary differential equation\n(ODE) functions representing the batteryâ€™s physical state, as de-\nscribed in Equation 4. By querying the real-time current and temper-\nature value from the BMS, we can transform an arbitrary unlabeled\nQdlinear feature into a PGLoss in an unsupervised manner.\n4.2.2\nGenerate a complete Qdlinear feature and supervise it with\nPGLoss. In the filed of battery research, a complete Qdlinear curve\nspanning from the lower to the upper voltage limits can describe\nLIBâ€™s aging mode and therefore can theoretically identify the accu-\nrate state of health [50] [7] [40] according to its definition2. Inspired\nby this, we want to design the objective by guiding the pre-trained\nmodel to generate a complete Qdlinear feature from the given par-\ntial one, which means to generate Figure 3 (b) from (a).\nSpecifically, given the input (partial) Qdlinear feature curve x âˆˆ\nR1Ã—ğ‘‡, we use the pre-trained model to generate a complete one Ë†x âˆˆ\nR1Ã—ğ‘‡â€², where ğ‘‡â€² > ğ‘‡. This complete voltage-capacity curve is then\nsupervised with PGLoss (as shown in Figure 3). Additionally, we\nrandomly mask a portion of x to promote representation learning.\nIn general, our objective can be formulated in the form of:\nLPGâˆ’SSL = âˆ¥Ë†x[0 : ğ‘‡] âˆ’xâˆ¥2\nğ¹+ ğœ†âˆ¥ğœƒ1ğ¼+ ğœƒ2 Ë†ğ‘¢+ Ë†Â¤ğ‘¢= 0âˆ¥2\nğ¹\n(5)\nHere, Ë†x[0 : ğ‘‡] represents the overlap between the generated\ncomplete voltage-capacity curve and the given x. Ë†ğ‘¢denotes the\ngenerated complete voltage-time curve. The parameters ğœƒ1 and ğœƒ2\nare associated with temperature, and ğ¼denotes the current; all of\nthese values can be queried through the BMS during deployment.\nWe will empirically demonstrate that the design of Physics-Guided\nPG-SSL is highly suitable for our SOH estimation task and leads\nto better performance than simple self-prediction, as shown in the\nexperimental section.\n2This complete Qdlinear (voltage-capacity) curve requires additional cycles under\nconstrained temperature and current conditions in the laboratory and infeasible at\ndeployment. The unlabeled Qdlinear feature we obtained at use is always a partial of\nit.\n\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time TrainingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\n4.3\nPrefix Prompt Adaptation\nIn this subsection, we introduce Prefix Prompt Adaptation (PPA), a\nmethod that reduces the dimension of the solution space for easier\noptimization of test-time adaptation. Specifically, inspired by the\ndemonstrated effectiveness of continuous prompt learning in the\nfield of deep model fine-tuning [2, 17]. We add a small number of\nsoft prompts as a prefix to the embedded input context for test-\ntime updating, while keeping all other model parameters frozen.\nIn this way, the dimension of learnable model parameters shall be\nsignificantly reduced and thus enabling the practical deployment of\nTTA on real-world edge devices such as a CPU when the pre-trained\nmodel is large (such as an LLM). Formally, given a test sample ğ‘‹ğ‘¡ğ‘’ğ‘ ğ‘¡,\nour goal is to find an optimal prompt ğ’‘âˆ—:\nğ’‘âˆ—= arg min\nğ’‘LPGTPT(F, ğ’‘,ğ‘‹test)\n(6)\nusing the physics-guided self-supervised loss LPGTPT from Equa-\ntion 4. Here, F denotes the pre-trained model. We will also demon-\nstrate experimentally that PPA significantly reduces the cost of\nTTA while only marginally reducing its performance.\n4.4\nIntegrete Existing SOH Estimation Models\ninto BatteryTTT\nIn this subsection, we demonstrate how to incorporate existing SOH\nestimation algorithms [51] into our test-time adaptation framework\nto complete the whole cross-domain SOH estimation process de-\npicted in Figure 1. Existing SOH estimation studies [32, 51] exten-\nsively use statistical models and high-performance neural networks\nfor LIB SOH estimation, such as Gaussian Process Regression [47],\nRandom Forest [12], MLP [14], RNNs[16] and Transformer [44] for\nSOH estimation. Only deep learning methods can be incorporated\ninto the TTA framework, as well as other transfer learning settings.\nSpecifically, our architecture follows a Y-shaped design, as de-\nscribed in [10, 34]: a feature extractor ğ‘“is simultaneously followed\nby a self-supervised head ğ‘”and a main task head â„. Here, we sub-\nstitute ğ‘“with the encoder of existing neural networks for SOH\nestimation, such as GRU, LSTM, and Transformer, and ğ‘”with the\ndecoder. For the main regression task headâ„for SOH estimation, we\nuse a linear projection from the dimension of the encoder features\nto 1, which is primarily a historic artifact [15].\nDuring pre-training, we first train ğ‘”â—¦ğ‘“using the PG-SSL loss\nin Equation 5 in an unsupervised manner with the features of the\nsource LIB dataset. Then we perform linear probing by combining\nthe encoder ğ‘“with the main task headâ„, keeping ğ‘“frozen. Formally:\nğ‘“0, ğ’ˆ0 = arg min\nğ’‡,ğ’ˆ\n1\nğ‘›\nğ‘›\nâˆ‘ï¸\nğ‘–=1\nLPGâˆ’SSL(ğ‘¥ğ‘–;ğ’‡, ğ’ˆ)\n(7)\nFollowed by:\nâ„0 = arg min\nâ„\n1\nğ‘›\nğ‘›\nâˆ‘ï¸\nğ‘–=1\nLMSE(â„â—¦ğ‘“0(ğ‘¥ğ‘–),ğ‘¦ğ‘–)\n(8)\nThe summation is over the training set with ğ‘›samples, each\nconsisting of input ğ‘¥ğ‘–and label ğ‘¦ğ‘–. During test-time adaptation, we\noptimize ğ‘”â—¦ğ‘“before making a prediction each time a test input ğ‘¥\narrives. After optimization, we make a prediction on ğ‘¥as â„â—¦ğ‘“ğ‘¥(ğ‘¥),\nformally as:\nğ‘“ğ‘¥,ğ‘”ğ‘¥= arg min\nğ‘“,ğ‘”LPGâˆ’SSL(ğ‘¥; ğ‘“0,ğ‘”0)\n(9)\nğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›= â„0 â—¦ğ‘“ğ‘¥(ğ‘¥)\n(10)\nIn particular, we argue for the use of our proposed prefix prompt\nadaptation in Equation 6 to avoid the substantial computation\nbrought by fine-tuning Equation 9 at inference time.\n4.5\nIntegrate an LLM into BatteryTTT\n(GPT4Battery)\nIn this subsection, we explore the potential of LLM in battery se-\nquence modeling. Inspired by recent studies which utilize an LLM\nfor protein sequence prediction [23] and time series analysis [18],\nwe primarily employ the idea of model reprogramming to effectively\nalign the modalities of battery data and natural language, leverag-\ning the reasoning and generalization abilities of LLMs for battery\ntasks.\nSpecifically, we first tokenize and map the input Qdlinear features\ninto a high-dimensional space with the same dimensionality as the\nword space of the language model. Then, we fuse the information\nof battery sequence modality and language modality using a cross-\nattention layer.\nHowever, for a word embedding space of an LLM E âˆˆRğ‘‰Ã—ğ·,\nwhere ğ‘‰is the vocabulary size. The vocabulary size can be in-\nevitably large (for example, GPT2 has a V of 50257 [29]). Simply\nleveraging E will result in large and potentially dense reprogram-\nming space, increasing the computation complexity and difficulty\nof catching the relevant source tokens. Following [37] and [18],\nwe maintain only a small collection of text prototypes by linearly\nprobing E, denoted as Eâ€² âˆˆRğ‘‰â€²Ã—ğ·, where ğ‘‰â€² â‰ªğ‘‰. Then, we align\nthe tokenized input patches and text prototypes with a multi-head\ncross-attention layer. Specifically:\nZ(ğ‘–)\nğ‘˜\n= attention(Q(ğ‘–)\nğ‘˜, K(ğ‘–)\nğ‘˜, V(ğ‘–)\nğ‘˜)\n= softmax(\nQ(ğ‘–)\nğ‘˜K(ğ‘–)âŠ¤\nğ‘˜\nâˆšï¸\nğ‘‘ğ‘˜\n)V(ğ‘–)\nğ‘˜\nBy aggregating each Z(ğ‘–)\nğ‘˜\nâˆˆRğ‘ƒÃ—ğ‘‘in every head, we obtain\nZ(ğ‘–) âˆˆRğ‘ƒÃ—ğ·. This way, the text prototypes can learn cues in\nlanguage which can then represent the relevant local patch infor-\nmation. We will experimentally demonstrate the improvement in\ngeneralizability achieved by incorporating LLMs and the effective-\nness of model reprogramming.\n5\nExperiments\nIn this section, we empirically evaluate the proposed approach\non six real-world LIB datasets, including five publicly available\ndatasets for daily applications (with capacities ranging from a few\nAh) and our own collected 300Ah large LIB dataset for energy\nstorage. Specifically, we focus on (1) the overall improved gener-\nalization performance of TTA and the superior performance by\ncombining GPT-2 and TTA (GPT4Battery), (2) an efficacy study\nof the two proposed designs, PG-SSL and PPA, (3) the ablation re-\nsults of GPT4Battery, (4) inference efficiency of involving TTA, and\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nTable 2: Main specifications of selected LIB datasets.\nDataset\nElectrode Material\nNominal Capacity\nVoltage Range\nSamples\nEstimated Collect\nTime\nCollector\nCALCE\nLCO\n1.1 (Ah)\n2.7-4.2 (V)\n2807\n1397 (hour)\nUniversity of Maryland\nSANYO\nNMC\n1.85 (Ah)\n3.0-4.1 (V)\n415\n644 (hour)\nRWTH Aachen University\nKOKAM\nLCO/NCO\n0.74 (Ah)\n2.7-4.2 (V)\n503\n8473 (hour)\nUniversity of Oxford\nPANASONIC\nNCA\n3.03 (Ah)\n2.5-4.29 (V)\n2770\n1801 (hour)\nBeijing Institude of Technology\nGOTION\nLFP\n27 (Ah)\n2.0-3.65 (V)\n4262\n2238 (hour)\nBeijing Institude of Technology\nNHRY\nLFP\n300 (Ah)\n2.5-3.5 (V)\n808\n1200 (hour)\nOurs\n(5) the scalability and deployment of our method on large energy\nstorage LIBs.\n5.1\nExperiment Settings\n5.1.1\nDataset preparation. We conducted experiments using five\npublicly available lithium-ion battery (LIB) datasets intended for\ndaily commercial use, with capacities ranging from 0.74 Ah to 27\nAh. Additionally, we utilized four of our own LIB datasets collected\nfor industrial-level energy storage, specifically with a capacity of\n300 Ah, which we will also make publicly available for academic\npurposes. These datasets encompass a variety of widely used cath-\node active materials, capacities, and manufacturers. A summary of\nthe dataset statistics is presented in Table 2.\n5.1.2\nBaselines. We compare our method with four types of base-\nlines to demonstrate the efficacy of the proposed BatteryTTT frame-\nwork and the GPT4Battery model: (1) Existing non-transfer learn-\ning machine learning (ML) methods for LIB State of Health (SOH)\nestimation, including Gaussian Process Regression [47], Random\nForest [12], Multi-Layer Perceptron (MLP) [14], Recurrent Neural\nNetworks (RNNs) [16], and Transformer [44]3; (2) Integration of ex-\nisting models into our BatteryTTT framework; (3) State-of-the-art\ntransfer learning methods for cross-domain LIB SOH estimation,\nsuch as woDegrad [22] and SSF-WL [46]4; and (4) Integration of\nlarge language models (LLMs) into the BatteryTTT framework\n(GPT4Battery).\nWe reproduce the results of BatteryML and woDegrad based on\nthe provided code and follow the approach details for SSF-WL. For\na fair comparison, we adhere to the data pre-processing methods\noutlined in [51] and use QdLinear [1] as the unified feature set.\nWe adopt the standard evaluation metrics of mean absolute error\n(MAE) and root mean squared error (RMSE).\n5.1.3\nImplementation details. Our models are implemented using\nPytorch and trained on a single 3070Ti GPU. We utilize the AdamW\noptimizer [21] with a fixed learning rate of 1e-3 for pre-training\nand linear probing until convergence. The mask ratio is set to\n30% during this phase. TTA is conducted using stochastic gradient\ndescent (SGD) with a momentum of 0.9 due to its consistency in\nimproving performance on distribution shifts [10]. Typically, we set\na fixed learning rate of 1e-2 and iterate for 10 steps, as more steps\nonly marginally improve performance based on our observations.\nThe mask ratio during TTA will be specifically analyzed later. For\n3BatteryML [51] provides a comprehensive platform summarizing these models\n4These methods rely on sufficient unlabeled target data (UTDs) to operate effectively,\nwhich requires impractical data collection time, as summarized in Table 1.\nfurther details, we have made the relevant code and data available at\nthe following link: https://anonymous.4open.science/r/gpt4battery-\n55FC.\n5.2\nMain Performance\nIn this section, we report the main improvements in cross-domain\ngeneralization performance of our proposed TTA methods (e.g.\nPG-SSL and PPA), on five commercial LIB datasets for daily usage.\nAdditionally, we demonstrate that by leveraging an LLM as the back-\nbone, GPT4Battery achieves superior generalization performance\ncompared to all baseline methods.\nTable 3: Improved performance of TTA on existing methods\nand comparison with current transfer learning methods.\nModels\nCALCE\nSANYO\nKOKAM\nPANASONIC\nGOTION\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nRandom Forest\n5.76\n4.9\n7.87\n6.73\n5.76\n4.88\n4.96\n4\n0.62\n0.54\nLight GBM\n6.8\n5.42\n6.31\n5.62\n6.52\n5.31\n6.06\n4.97\n0.55\n0.47\nMLP\n3.93\n3.52\n6.1\n5.93\n13.1\n10.9\n5.08\n4.38\n0.54\n0.44\nGRU\n2.18\n2.86\n9.17\n9.19\n3.07\n3.44\n2.53\n3.5\n0.74\n0.84\nLSTM\n2.52\n2.78\n7.58\n7.84\n3.07\n3.93\n1.55\n2.19\n1.65\n1.68\nTransformer\n2.27\n2.57\n8.1\n8.24\n15.3\n17.1\n1.9\n2.35\n1.28\n1.4\nGRU+ (TTA)\n1.9\n2.25\n7.8\n8.01\n2.4\n2.74\n1.74\n2.73\n0.67\n0.71\nLSTM+\n2.08\n2.36\n6.71\n7.04\n2.93\n3.67\n1.31\n2\n0.65\n0.77\nTransformer +\n1.83\n1.97\n7.03\n7.18\n13.2\n14.6\n1.32\n1.85\n0.34\n0.43\nGPT-2+\n1.52\n1.89\n1.35\n1.71\n7.95\n8.01\n1.28\n1.95\n0.38\n0.47\nBert+\n2.01\n2.33\n1.46\n1.82\n7.77\n8.04\n1.52\n2.2\n0.41\n0.52\nLlama-7b+\n1.57\n1.94\n1.35\n1.69\n8.58\n9 .66\n1.3\n2.01\n0.4\n0.49\nwoDegrad\n1.76\n1.96\n1.21\n1.54\n1.76\n3.01\n2.09\n2.56\n0.45\n0.58\nSSF-WL\n1.55\n1.93\n1.08\n1.24\n6.21\n5.1\n1.44\n2.06\n0.51\n0.72\n5.2.1\nImprovement of Generalization Ability of TTA. Table 3 shows\nthe main improvements in the generalization performance of our\nproposed TTA methods. We use the GOTION dataset as the source\ndataset for its extensive label coverage and then include its own\ntest set along with the remaining four datasets (CALCE, SANYO,\nKOKAM and PANASONIC) as the target datasets for generaliz-\nability testing. Overall, we observe that the TTA method shows a\nsignificant performance improvement of about 50% compared to\nthe no-TTA method. Some models (such as Transformer, GPT2)\nequipped with TTA achieve a performance that rivals or even ex-\nceeds the performance of current transfer learning methods that\nrequire additional access to the target data in CALCE, PANASONIC\nand GOTION. Specifically, within all the methods that use TTA,\nthe utilization of large language models also has made quite a dif-\nference in generalization performance improvement. For instance,\nGPT2+ achieves first or second rank performance on the CALCE,\n\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time TrainingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nGOTION and PANASONIC datasets compared to all baseline meth-\nods. LLama+ also obtained the best and second-best performance\non the SANYO and PANASONIC dataset, respectively, compared to\nthe other TTA methods. We also observe that some model architec-\ntures dominate the performance on certain datasets over TTA, e.g.,\nthe RNN family (GRU, LSTM) outperforms the Transformer family\non KOKAM as a whole, and the TTA-enhanced GRU+ achieves\nperformance comparable to that of woDegrad.\n2.1\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n108\nMean Absolute Error (MAE)\n(Test Time) Trainable Params\nGRU+\nLSTM+\nTransformer+\nGPT2+\nSSF-WL (2024)\nGPT2+(PPA)\nwoDegrad (2023)\n107\n106\n105\n104\n103\n   \n0\nFigure 4: Superior Performance of GPT4Battery (on CALCE\ndataset).\n5.2.2\nSuperior Performance of GPT4Battery over all Baseline Meth-\nods. In this section, we provide a detailed comparison of the gener-\nalizability gains and computation trade-offs achieved by incorpo-\nrating LLMs. Figure 4 demonstrates that GPT2 equipped with TTA\n(which we name GPT4Battery) obtains the lowest MAE results on\nthe CALCE dataset than all other models including current transfer\nlearning methods (woDegrad and SSF-WL) that require additional\nassumptions to access the target dataset. GPT4battery also gets\nvery competitive results in other datasets as do other large models\n(e.g. Llama and Bert).\nThe results of applying Prefix Prompt Adaptation (PPA) to large\nlanguage models (LLMs), as illustrated in Figure 4, are particularly\nnoteworthy. Conventional fine-tuning of GPT-2â€™s positional encod-\ning and layer normalization significantly increased the number of\ntrained parameters at test time, by a factor of 10 to 100 compared to\nTransformer+ and RNNs+, while only (relatively) slightly reducing\nthe mean absolute error (MAE) from 1.83 to 1.71. In contrast, our\nPrefix Prompt Adaptation (PPA) approach not only reduces the\nnumber of adjustable parameters by nearly 10,000 times, making\nGPT4Battery ten times more parameter-efficient than the RNN\nseries, but also further reduces the MAE from 1.71 to 1.52, an im-\nprovement of approximately 10.7%. However, we should note the\nlimitation that involving an LLM does increase the inference time\neven with PPA, which is a trade-off between accuracy and compu-\ntation efficiency.\nIt is important to note that while applying PPA to regular Trans-\nformer+ and RNNs+ also reduces the number of trainable parame-\nters to the order of 103, it can impair their generalization perfor-\nmance, resulting in a slightly inferior performance compared to\nfull-parameter fine-tuning. Therefore, we overall report the best\nperformance for their full parameter fine-tuning, while reporting\nthe best performance for LLMs using PPA in Table 3 and Figure 4.\nThe performance of other models using PPA is thoroughly ablated\nin Sec. 5.3.1.\n5.3\nEfficacy Analysis\nIn this section, we analyze the effectiveness and important design\nchoices of each component. Specifically, we evaluate the two TTA\ndesigns: Physical-Guided Self-supervised Learning (PG-SSL) and\nPrefix Prompt Adaptation (PPA).\n5.3.1\nEffect and Computation Trade-offs of Prefix Prompt Adapta-\ntion. By adjusting only a small number of soft prompts prefix to the\ninput context, this design can significantly reduce the adjustable\nparameters to 103 orders of magnitude regardless of model size, as\nillustrated in Figure 5 on the GPT2, Transformer and LSTM mod-\nels. Applying PPA to Transformer and LSTM slightly impact the\naccuracy, as shown in Figure 5. Applying PPA to LLMs however, sig-\nnificantly reduces the Mean Absolute Error (MAE) from 3.89 to 1.52,\na reduction of approximately 60.9%. Moreover, PPA achieves better\ngeneralization performance than traditional parameter-efficient\nfine-tuning of the positional encoding and layer normalization lay-\ners [54] of the LLM. We attribute this to the language-agnostic\npattern recognition and inference capabilities acquired through\npre-training on text corpora [13, 24]. Guided by learnable prefix\nprompts, these capabilities can also be generalized to battery se-\nquence data.\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50\nTrainable Params\nMAE/RMSE\n MAE\n RMSE\n Params\n0\n103\n104\n105\n106\n107\n108\n1.5\n2.0\n2.5\n3.0\nMAE/RMSE\n MAE\n RMSE\n Params\n10\n2\n103\n104\n105\n106\nTrainable Params\n1.5\n2.0\n2.5\n3.0\nMAE/RMSE\n MAE\n RMSE\n Params\n102\n103\n104\n105\n106\nTrainable Params\n(a) GPT2 + TTA\n(b) Transformer + TTA\n(c) LSTM + TTA\nNo TTA\nTTA-full\nTTA-PPA\nPPA\nTTA-\nTTA-peft\nTTA-full\nNo TTA\nNo TTA\nTTA-full\nTTA-PPA\nFigure 5: Efficacy and computation trade-offs of Prefix\nPrompt Adaptation (PPA) on different models.\n5.3.2\nEfficacy and Parameter Sensitivity of Physical-Guided Self-\nsupervised Learning. In this section, we analyze the effectiveness\nof PG-SSL and conduct a sensitivity analysis of another important\nparameter affecting the self-supervised loss, the mask ratio. We\nperformed ablation studies on losses using physical constraints and\npure mask reconstruction. Additionally, we analyzed the effect of\ndifferent masking rates on the MAE reduction of TTAs. We employ\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nthe transformer model and full-parameter tuning for TTA, conduct-\ning experiments on the adaptation to the CALCE and PANASONIC\ndatasets.\nFigure 6 shows that the inclusion of the physical-guided loss\nseamlessly enhances the performance in both MAE and RMSE\nreduction. Comparatively, the MAE reduction of using PG-SSL is\nslightly more significant in the CALCE data set (Fig. 6 (a)) than in\nthe PANASONIC data set (Figure 6 (b)). This is because the former\nis a much more nonlinear dataset, making pure reconstruction loss\nless effective in capturing representations. We also observe from\nboth (a) and (b) of Figure 6 that a larger mask ratio of 0.7 to 0.9\npromotes learning a better representation with or without PG-SSL,\nwhile a small mask ratio of 0.5 to 0.6 fails. This is consistent with the\nMAE-based self-supervised learning observations in the computer\nvision domain, where masking a high proportion of the input image,\ne.g., 75%, yields a non-trivial and meaningful self-supervisory task.\nIn summary, the combination of masking input and PG-SSL results\nin the best TTA results.\n1.0\n0.9\n0.8\n0.7\n0.6\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMAE Reduction\nMask Ratio\n wo PG\n PG\n1.0\n0.9\n0.8\n0.7\n0.6\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nRMSE Reduction\nMask Ratio\n wo PG\n PG\n1.0\n0.9\n0.8\n0.7\n0.6\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nMAE Reduction\nMask Ratio\n wo PG\n PG\n1.0\n0.9\n0.8\n0.7\n0.6\n-0.6\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRMSE Reduction\nMask Ratio\n wo PG\n PG\n(b) Effect on PANASONIC dataset.\n(a) Effect on CALCE dataset.\nFigure 6: Effect on MAE/RMSE Reduction of Physical-Guided\nSelf-supervised Learning and Sensitivity of Mask Ratio.\n5.4\nAblation Study of GPT4Battery\nIn this section, we consider the best performing GPT2+TTA as a\nself-contained model (GPT4Battery) and provide ablation study on\nthe effects of different components or design choices. Our results in\nTable 4 indicate that ablating either model reprogramming or any\nother designs in test-time adaptation hurts the generalization per-\nformance on unseen LIBs. In the absence of physical guidance, we\nobserve a notable average performance degradation of 55.1%, which\nbecomes more pronounced (i.e., exceeding 70%) when discarding\nthe TTA strategy completely. The act of model reprogramming also\nstands as a pivotal element in cross-modality alignment, enabling\nthe LLM to understand the LIBâ€™s sequence data with the help of\ntext prototypes. Ablation of reprogramming results leads to over\n20% degradation on average performance.\nTable 4: Ablating the different components of GPT4Battery.\nRed: the best.\nMethod\nCALCE\nSANYO\nKOKAM\nPANAS.\nGOTION\nGPT4Battery\n1.52\n1.35\n7.95\n1.28\n0.25\nw/o PG-SSL\n2.11\n1.12\n8.34\n3.11\n0.265\nw/o Masked TTA\n2.05\n0.97\n8.44\n1.22\n0.255\nw/o TTA\n2.13\n1.34\n9.51\n3.44\n0.297\nw/o model reprogramming\n4.13\n2.78\n10.56\n4.57\n0.31\n5.5\nTTA Efficiency\nWe present the average running time across five datasets for three\nrepresentative models equipped with TTA and two current transfer\nlearning methods. Our focus is primarily on model inference time,\nas TTA does not significantly impact training duration. Table 5\ndemonstrates that although TTA-equipped methods achieve sub-\nstantially higher accuracy, they are 10 to 100 times slower than\nexisting methods. This trade-off highlights that our approach sacri-\nfices some speed for enhanced accuracy. However, the resulting 102\nspeed level remains sufficient for modern BMS requirements, where\nindividual response times typically need to be under 500ms in our\ndeployed system. More importantly, our framework can eliminate\nthe need for 644 to 8,473 hours of degradation experiments for data\ncollection, as summarized in Tables 1 and 2.\nTable 5: Efficiency Analysis of TTA.\nMethod\nGPT2+\nTransformer+\nLSTM+\nwoDegrad\nSSF-WL\nOverall Inference Time (s)\n32.6\n6.1\n4.3\n0.35\n0.3\nOne Inference Time (ms)\n51.17\n9.57\n6.75\n0.55\n0.47\nModel parameters\n7680\n1280\n1280\n68774000\n2601369\n5.6\nScalability on Large Energy Storage LIBs\nIn this section, we report the scalability of the proposed framework\nthrough deployment on our battery management system (BMS) for\nindustry-level energy storage LIBs (300Ah).\n5.6.1\nLIB Dataset. We collect the NHRY dataset by performing\ndegradation experiments on 8 industry-level energy storage LIBs\n(300Ah) from 4 brands (Ningde, Haichen, Ruipu, Yiwei). Test envi-\nronment and procedures comply with the China Standard BMS for\nEnergy Storage GB/T 34131-2023. A total of 800 degradation cycles\nwere experienced ranging over two months from December 2022\nto January 2023.\n5.6.2\nOnline Performance. For four different brands of LIBs with\nthe same capacity, we mix two of the brands as the source domain\nand evaluate the online performance of the LIBs from the remaining\ntwo brands, resulting in a total of two combinations for the cross-\nbattery setting. We report the MAE/RMSE metric along with the\ninference time (ms) using representative baselines.\n6\nConclusion\nIn this paper, we propose a novel test-time adaptation (TTA) frame-\nwork for cross-domain LIB state of health (SOH) estimation. This\none sample adaptation setting allows the model to continually\n\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time TrainingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTable 6: Online performance on Large Energy Storage LIBs.\nCombination 1\nCombination 2\nMAE\nRMSE\nPer Infer. Time\nMAE\nRMSE\nPer Infer. Time\nLSTM\n4.33\n4.59\n4.23\n2.44\n2.69\n3.23\nTransformer\n5.62\n5.89\n7.61\n3.48\n3.78\n6.32\nLSTM+\n0.775\n0.788\n54.53\n0.55\n0.62\n44.85\nTransformer+\n1.25\n1.38\n88.57\n1.02\n1.23\n77.83\nGPT4Battery\n0.734\n0.761\n103.47\n0.314\n0.418\n105.68\nadapt to the target domain with every single unlabeled test sample,\nperfectly aligning with the nature of battery degradation features,\nwhich can only be obtained one by one during the long aging pro-\ncess. This setting also addresses the limitations of existing transfer\nlearning methods, which assume additional access to the target LIB\ndataset, thereby saving months to years of labor in data collection.\nBy introducing GPT4Battery, we hope this work will inspire the\nresearch community to fully leverage advanced AI techniques, such\nas Test-Time Adaptation (TTA) and large language models (LLMs),\nfor more AI4Science problems, thereby saving enormous time and\naccelerating scientific discovery.\nReferences\n[1] Peter M Attia, Kristen A Severson, and Jeremy D Witmer. 2021. Statistical\nlearning for accurate and interpretable battery lifetime prediction. Journal of The\nElectrochemical Society 168, 9 (2021), 090547.\n[2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022.\nExploring visual prompts for adapting large-scale models.\narXiv preprint\narXiv:2203.17274 (2022).\n[3] Christoph Birkl. 2017. Oxford battery degradation dataset 1. (2017).\n[4] Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 5â€“32.\n[5] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and\nYan Liu. 2023. Tempo: Prompt-based generative pre-trained transformer for time\nseries forecasting. arXiv preprint arXiv:2310.04948 (2023).\n[6] Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. 2024.\nLLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecast-\ners. arXiv:2308.08469 [cs.LG]\n[7] Cheng Chen, Rui Xiong, Ruixin Yang, and Hailong Li. 2022. A novel data-driven\nmethod for mining battery open-circuit voltage characterization. Green Energy\nand Intelligent Transportation 1, 1 (2022), 100001.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\n[9] Noelia Ferruz, Steffen Schmidt, and Birte HÃ¶cker. 2022. ProtGPT2 is a deep\nunsupervised language model for protein design. Nature communications 13, 1\n(2022), 4348.\n[10] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. 2022. Test-time training\nwith masked autoencoders. Advances in Neural Information Processing Systems\n35 (2022), 29374â€“29385.\n[11] Konstantinos N. Genikomsakis, Nikolaos-Fivos Galatoulas, and Christos S.\nIoakimidis. 2021. Towards the development of a hotel-based e-bike rental service:\nResults from a stated preference survey and techno-economic analysis. Energy\n215 (2021), 119052. https://doi.org/10.1016/j.energy.2020.119052\n[12] Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized\ntrees. Machine learning 63 (2006), 3â€“42.\n[13] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson. 2024. Large language\nmodels are zero-shot time series forecasters. Advances in Neural Information\nProcessing Systems 36 (2024).\n[14] Simon Haykin. 1998. Neural networks: a comprehensive foundation. Prentice Hall\nPTR.\n[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick.\n2022. Masked autoencoders are scalable vision learners. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. 16000â€“16009.\n[16] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[17] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie,\nBharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In Euro-\npean Conference on Computer Vision. Springer, 709â€“727.\n[18] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi,\nPin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. 2023. Time-llm:\nTime series forecasting by reprogramming large language models. arXiv preprint\narXiv:2310.01728 (2023).\n[19] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong\nWang, Shirui Pan, and Qingsong Wen. 2024. Position Paper: What Can Large Lan-\nguage Models Tell Us about Time Series Analysis. arXiv preprint arXiv:2402.02713\n(2024).\n[20] Jian Liang, Ran He, and Tieniu Tan. 2024. A comprehensive survey on test-time\nadaptation under distribution shifts. International Journal of Computer Vision\n(2024), 1â€“34.\n[21] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\narXiv preprint arXiv:1711.05101 (2017).\n[22] Jiahuan Lu, Rui Xiong, Jinpeng Tian, Chenxu Wang, and Fengchun Sun. 2023.\nDeep learning to estimate lithium-ion battery state of health without additional\ndegradation experiments. Nature Communications 14, 1 (2023), 2760.\n[23] Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian\nChen, Li Yuan, and Yonghong Tian. 2024. Prollama: A protein large language\nmodel for multi-task protein language processing. arXiv preprint arXiv:2402.16445\n(2024).\n[24] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montser-\nrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. 2023. Large\nLanguage Models as General Pattern Machines. arXiv:2307.04721 [cs.AI]\n[25] Man-Fai Ng, Jin Zhao, Qingyu Yan, Gareth J Conduit, and Zhi Wei Seh. 2020.\nPredicting the state of charge and health of batteries using data-driven machine\nlearning. Nature Machine Intelligence 2, 3 (2020), 161â€“170.\n[26] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin\nZhao, and Mingkui Tan. 2022. Efficient test-time model adaptation without\nforgetting. In International conference on machine learning. PMLR, 16888â€“16905.\n[27] Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and\nDongjin Song. 2024. IP-LLM: Semantic Space Informed Prompt Learning with\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nLLM for Time Series Forecasting. arXiv preprint arXiv:2403.05798 (2024).\n[28] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.\nImproving language understanding by generative pre-training. (2018).\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[30] Carl Edward Rasmussen and Hannes Nickisch. 2010. Gaussian processes for\nmachine learning (GPML) toolbox. The Journal of Machine Learning Research 11\n(2010), 3011â€“3015.\n[31] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Ja-\nson Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\n2021. Biological structure and function emerge from scaling unsupervised learn-\ning to 250 million protein sequences. Proceedings of the National Academy of\nSciences 118, 15 (2021), e2016239118. https://doi.org/10.1073/pnas.2016239118\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118\n[32] Darius Roman, Saurabh Saxena, Valentin Robu, Michael Pecht, and David Flynn.\n2021. Machine learning pipeline for battery state-of-health estimation. Nature\nMachine Intelligence 3, 5 (2021), 447â€“456.\n[33] Kristen A Severson, Peter M Attia, Norman Jin, Nicholas Perkins, Benben Jiang,\nZi Yang, Michael H Chen, Muratahan Aykol, Patrick K Herring, Dimitrios\nFraggedakis, et al. 2019. Data-driven prediction of battery cycle life before\ncapacity degradation. Nature Energy 4, 5 (2019), 383â€“391.\n[34] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anand-\nkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for zero-shot gener-\nalization in vision-language models. Advances in Neural Information Processing\nSystems 35 (2022), 14274â€“14289.\n[35] Xing Shu, Jiangwei Shen, Guang Li, Yuanjian Zhang, Zheng Chen, and Yonggang\nLiu. 2021. A Flexible State-of-Health Prediction Scheme for Lithium-Ion Battery\nPacks With Long Short-Term Memory Network and Transfer Learning. IEEE\nTransactions on Transportation Electrification 7, 4 (2021), 2238â€“2248.\nhttps:\n//doi.org/10.1109/TTE.2021.3074638\n[36] Dimitris Spathis and Fahim Kawsar. 2023. The first step is the hardest: Pitfalls\nof Representing and Tokenizing Temporal Data for Large Language Models.\narXiv:2309.06236 [cs.LG]\n[37] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. 2023. TEST: Text proto-\ntype aligned embedding to activate LLMâ€™s ability for time series. arXiv preprint\narXiv:2308.08241 (2023).\n[38] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz\nHardt. 2020. Test-time training with self-supervision for generalization under\ndistribution shifts. In International conference on machine learning. PMLR, 9229â€“\n9248.\n[39] Yandan Tan and Guangcai Zhao. 2020. Transfer Learning With Long Short-Term\nMemory Network for State-of-Health Prediction of Lithium-Ion Batteries. IEEE\nTransactions on Industrial Electronics 67, 10 (2020), 8723â€“8731. https://doi.org/10.\n1109/TIE.2019.2946551\n[40] Jinpeng Tian, Rui Xiong, Weixiang Shen, Jiahuan Lu, and Xiao-Guang Yang. 2021.\nDeep neural network battery charging curve prediction using 30 points collected\nin 10 min. Joule 5, 6 (2021), 1521â€“1534.\n[41] Jinpeng Tian, Rui Xiong, Weixiang Shen, Jiahuan Lu, and Xiao-Guang Yang. 2021.\nDeep neural network battery charging curve prediction using 30 points collected\nin 10 min. Joule 5, 6 (2021), 1521â€“1534. https://doi.org/10.1016/j.joule.2021.05.012\n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[43] Manh-Kien Tran, Manoj Mathew, Stefan Janhunen, Satyam Panchal, Kaamran\nRaahemifar, Roydon Fraser, and Michael Fowler. 2021. A comprehensive equiva-\nlent circuit model for lithium-ion batteries, incorporating the effects of state of\nhealth, state of charge, and temperature on model parameters. Journal of Energy\nStorage 43 (2021), 103252.\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[45] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor\nDarrell. 2020. Tent: Fully test-time adaptation by entropy minimization. arXiv\npreprint arXiv:2006.10726 (2020).\n[46] Tianyu Wang, Zhongjing Ma, Suli Zou, Zhan Chen, and Peng Wang. 2024.\nLithium-ion battery state-of-health estimation: A self-supervised framework\nincorporating weak labels. Applied Energy 355 (2024), 122332.\n[47] Christopher KI Williams and Carl Edward Rasmussen. 2006. Gaussian processes\nfor machine learning. Vol. 2. MIT press Cambridge, MA.\n[48] Rui Xiong, Linlin Li, and Jinpeng Tian. 2018. Towards a smarter battery manage-\nment system: A critical review on battery state of health monitoring methods.\nJournal of Power Sources 405 (2018), 18â€“29.\n[49] Zhaoyi Xu, Yanjie Guo, and Joseph Homer Saleh. 2022. A physics-informed\ndynamic deep autoencoder for accurate state-of-health prediction of lithium-ion\nbattery. Neural Computing and Applications 34, 18 (2022), 15997â€“16017.\n[50] Sijia Yang, Caiping Zhang, Jiuchun Jiang, Weige Zhang, Yang Gao, and Linjing\nZhang. 2021. A voltage reconstruction model based on partial charging curve\nfor state-of-health estimation of lithium-ion batteries. Journal of Energy Storage\n35 (2021), 102271.\n[51] Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, and Jiang Bian. 2024.\nBatteryML: An Open-source Platform for Machine Learning on Battery Degrada-\ntion. In The Twelfth International Conference on Learning Representations.\n[52] Marvin Zhang, Sergey Levine, and Chelsea Finn. 2022. Memo: Test time robust-\nness via adaptation and augmentation. Advances in neural information processing\nsystems 35 (2022), 38629â€“38642.\n[53] Aurick Zhou and Sergey Levine. 2021. Bayesian adaptation for covariate shift.\nAdvances in neural information processing systems 34 (2021), 914â€“927.\n[54] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One Fits\nAll: Power General Time Series Analysis by Pretrained LM. arXiv preprint\narXiv:2302.11939 (2023).\n\nAdapting Amidst Degradation: Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Time TrainingConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nA\nAppendix: Pseudo Code\nAlgorithm 1 Pretraining and Test-Time Adaptation of BatteryTTT\n1: Input: Source LIB dataset (ğ‘¥ğ‘–,ğ‘¦ğ‘–) for ğ‘–= 1, . . . ,ğ‘›\n2: Output: Prediction for test input ğ‘¥\n3: Pre-training Phase:\n4: for ğ‘–= 1 to ğ‘›do\n5:\nCompute LPGâˆ’SSL(ğ‘¥ğ‘–;ğ’‡, ğ’ˆ) using Equation 5\n6: end for\n7: Train: ğ‘“0,ğ‘”0 = arg minğ’‡,ğ’ˆ1\nğ‘›\nÃğ‘›\nğ‘–=1 LPGâˆ’SSL(ğ‘¥ğ‘–;ğ’‡, ğ’ˆ) (Equation\n1)\n8: Linear Probing:\n9: for ğ‘–= 1 to ğ‘›do\n10:\nCompute LMSE(â„â—¦ğ‘“0(ğ‘¥ğ‘–),ğ‘¦ğ‘–)\n11: end for\n12: Train: â„0 = arg minâ„1\nğ‘›\nÃğ‘›\nğ‘–=1 LMSE(â„â—¦ğ‘“0(ğ‘¥ğ‘–),ğ‘¦ğ‘–) (Equation 2)\n13: Test-Time Adaptation Phase:\n14: for each test input ğ‘¥do\n15:\nOptimize ğ‘“ğ‘¥,ğ‘”ğ‘¥= arg minğ‘“,ğ‘”LPGâˆ’SSL(ğ‘¥; ğ‘“0,ğ‘”0) (Equation 9)\n16:\nCompute ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›= â„0 â—¦ğ‘“ğ‘¥(ğ‘¥) (Equation 4)\n17: end for\n18: Prefix Prompt Adaptation:\n19: Use proposed prefix prompt adaptation to avoid computation\nduring inference\nB\nAppendix: Degradation Conditions of\nSelected Datasets\nTable 7 provides the specific degradation conditions of the selected\nlithium-ion batteries (LIBs) used in this work, including detailed\ncell information and the protocols for charging and discharging.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n\nConference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY\nTrovato et al.\nTable 7: Degradation Conditions of the Selected LIB Datasets\nDataset\nCell Information\nTest Conditions\nCALCE\n3 cells termed\nCS2-35,\nCS2-\n36, CS2-37\nâ€¢ Charged at a constant current rate of 0.5C until the voltage reached 4.2V and then 4.2V was\nsustained until the charging current dropped to below 0.05A.\nâ€¢ Discharged at a constant current rate of 1C.\nâ€¢ Ambient temperature is not mentioned.\nSANYO\n48 commercial\nUR18650E\ncylindrical\ncells\nâ€¢ Charged at a constant current rate of 1C until the voltage reached 4.1V and then 4.1V was sustained\nuntil the charging current dropped to below 0.04A.\nâ€¢ Discharged at a constant current rate of 1C.\nâ€¢ 25Â°C.\nKOKAM\n8\nKOKAM\nSLPB533459H\n4 pouch cells\nâ€¢ Charged at a constant current rate of 1C until the voltage reached 4.2V.\nâ€¢ Discharged at a constant current rate of 1C.\nâ€¢ 40Â°C.\nPANASONIC\n3 commercial\nNCR18650BD\ncylindrical\ncells\nâ€¢ Charged at a constant current rate of 0.3C until the voltage reached 4.2V and then 4.2V was\nsustained until the charging current dropped to below 0.03A.\nâ€¢ Discharged at a constant current rate of 2C.\nâ€¢ 20Â°C.\nGOTION\n3 commercial\nIFP20100140A\ncells\nCharged at a constant current rate of 1C until the voltage reached 3.65V and then 3.65V was sustained\nuntil the charging current dropped to below 1.35A.\nâ€¢ Discharged at a constant current rate of 1C.\nâ€¢ 45Â°C.",
    "pdf_filename": "Adapting_Amidst_Degradation_Cross_Domain_Li-ion_Battery_Health_Estimation_via_Physics-Guided_Test-Ti.pdf"
}