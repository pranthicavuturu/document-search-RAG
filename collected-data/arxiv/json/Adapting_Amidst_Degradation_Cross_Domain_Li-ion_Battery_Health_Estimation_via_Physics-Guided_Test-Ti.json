{
    "title": "Adapting Amidst Degradation: Cross Domain Li-ion Battery",
    "abstract": "Healthmodelingoflithium-ionbatteries(LIBs)iscrucialforsafe Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018. AdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEs- and efficient energy management and carries significant socio- timationviaPhysics-GuidedTest-TimeTraining.InProceedingsofMake economicimplications.AlthoughMachineLearning(ML)-based suretoenterthecorrectconferencetitlefromyourrightsconfirmationemai StateofHealth(SOH)estimationmethodshavemadesignificant (Conference acronym ’XX). ACM, New York, NY, USA, 12 pages. https: progressinaccuracy,thescarcityofhigh-qualityLIBdataremains //doi.org/XXXXXXX.XXXXXXX amajorobstacle.Althoughexistingtransferlearningmethodsfor cross-domainLIBSOHestimationhavesignificantlyalleviatedthe 1 Introduction labelingburdenoftargetLIBdata,theystillrequiresufficientunla- beledtargetdata(UTD)foreffectiveadaptationtothetargetdomain. Step2 Step1 CollectingthisUTDischallengingduetothetime-consumingna- tureofdegradationexperiments.Toaddressthisissue,weintroduce Pre-training Source LIB Source Domain BatteryML, LLMs Real-world Deployment apractical Test-TimeTrainingframework,BatteryTTT,which adaptsthemodelcontinuallyusingeachUTDcollectedamidstdegra- Step4 Step4 Step2 dation,therebysignificantlyreducingdatacollectiontime.Tofully Prefix-Prompt TTA utilizeeachUTD,BatteryTTTintegratestheinherentphysicallaws Adaptation Target Domain Step3 ofmodernLIBsintoself-supervisedlearning,termedPhyscics- Step3 GuidedTest-TimeTraining.Additionally,weexplorethepoten- Physics Guided Self-Supervised One Unlabeled Data tialoflargelanguagemodels(LLMs)inbatterysequencemodel- Learning ingbyevaluatingtheirperformanceinSOHestimationthrough modelreprogrammingandprefixpromptadaptation.Thecombi- Figure1:OverviewofBatteryTTTframework,whichconsistsof nationofBatteryTTTandLLMmodeling,termedGPT4Battery, three major components: (Step 1) pre-training on experimental achievesstate-of-the-artgeneralizationresultsacrosscurrentLIB datasets;(Step2)incrementaldatacollectionafterdeployment;and benchmarks.Furthermore,wedemonstratethepracticalvalueand (Steps3-4)test-timeadaptation.Steps2,3,and4iterateuntiltheLIB scalabilityofourapproachbydeployingitinourreal-worldbattery retires. managementsystem(BMS)for300Ahlarge-scaleenergystorage LIBs. TherapidadvancementsinrechargeableLi-ionbatteries(LIBs) havefacilitatedtheirwidespreaduseacrossvarioussectors,includ- CCSConcepts ingportableelectronics,medicaldevices,renewableenergysystems, •TestTimeTraining→BatteryHealthEstimation. andelectricvehicles[11].Thisubiquity,however,introducescritical challengesassociatedwithcapacitydegradationandperformance",
    "body": "Adapting Amidst Degradation: Cross Domain Li-ion Battery\nHealth Estimation via Physics-Guided Test-Time Training\nYuyuanFeng GuoshengHu\nfengyuyuan01@gmail.com UniversityofBristol\nXiamenUniversity Bristol,England\nXiamen,China huguosheng100@gmail.com\nXiaodongLi ZhihongZhang*\nHongKongUniversity XiamenUniversity\nHongKong,China Xiamen,China\nAbstract ACMReferenceFormat:\nHealthmodelingoflithium-ionbatteries(LIBs)iscrucialforsafe Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018.\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEs-\nand efficient energy management and carries significant socio-\ntimationviaPhysics-GuidedTest-TimeTraining.InProceedingsofMake\neconomicimplications.AlthoughMachineLearning(ML)-based\nsuretoenterthecorrectconferencetitlefromyourrightsconfirmationemai\nStateofHealth(SOH)estimationmethodshavemadesignificant\n(Conference acronym ’XX). ACM, New York, NY, USA, 12 pages. https:\nprogressinaccuracy,thescarcityofhigh-qualityLIBdataremains\n//doi.org/XXXXXXX.XXXXXXX\namajorobstacle.Althoughexistingtransferlearningmethodsfor\ncross-domainLIBSOHestimationhavesignificantlyalleviatedthe 1 Introduction\nlabelingburdenoftargetLIBdata,theystillrequiresufficientunla-\nbeledtargetdata(UTD)foreffectiveadaptationtothetargetdomain. Step2\nStep1\nCollectingthisUTDischallengingduetothetime-consumingna-\ntureofdegradationexperiments.Toaddressthisissue,weintroduce Pre-training Source LIB\nSource Domain BatteryML, LLMs Real-world Deployment\napractical Test-TimeTrainingframework,BatteryTTT,which\nadaptsthemodelcontinuallyusingeachUTDcollectedamidstdegra- Step4 Step4 Step2\ndation,therebysignificantlyreducingdatacollectiontime.Tofully Prefix-Prompt TTA\nutilizeeachUTD,BatteryTTTintegratestheinherentphysicallaws Adaptation\nTarget Domain Step3\nofmodernLIBsintoself-supervisedlearning,termedPhyscics- Step3\nGuidedTest-TimeTraining.Additionally,weexplorethepoten- Physics Guided\nSelf-Supervised One Unlabeled Data\ntialoflargelanguagemodels(LLMs)inbatterysequencemodel-\nLearning\ningbyevaluatingtheirperformanceinSOHestimationthrough\nmodelreprogrammingandprefixpromptadaptation.Thecombi-\nFigure1:OverviewofBatteryTTTframework,whichconsistsof\nnationofBatteryTTTandLLMmodeling,termedGPT4Battery,\nthree major components: (Step 1) pre-training on experimental\nachievesstate-of-the-artgeneralizationresultsacrosscurrentLIB datasets;(Step2)incrementaldatacollectionafterdeployment;and\nbenchmarks.Furthermore,wedemonstratethepracticalvalueand (Steps3-4)test-timeadaptation.Steps2,3,and4iterateuntiltheLIB\nscalabilityofourapproachbydeployingitinourreal-worldbattery retires.\nmanagementsystem(BMS)for300Ahlarge-scaleenergystorage\nLIBs. TherapidadvancementsinrechargeableLi-ionbatteries(LIBs)\nhavefacilitatedtheirwidespreaduseacrossvarioussectors,includ-\nCCSConcepts\ningportableelectronics,medicaldevices,renewableenergysystems,\n•TestTimeTraining→BatteryHealthEstimation. andelectricvehicles[11].Thisubiquity,however,introducescritical\nchallengesassociatedwithcapacitydegradationandperformance\nKeywords evaluation.Asaninherentlyinterdisciplinarysubject,batteryaging\nBatteryHealthEstimation,TestTimeTraining,DataScarcity,Large modelinghasemergedasafundamentalissueattheintersectionof\nLanguageModel batteryscienceandmachinelearning(ML)[25,32,33,51].Accurate\nStateofHealth(SOH)estimationforLIBsiscrucialnotonlyfor\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nensuringsafeandefficientenergymanagementbutalsoforopti-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation mizingthedesignandperformanceofnext-generationbatteries,\nonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe thushavingsignificantsocio-economicimplications.\nauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or\nWiththerapidadvancementofMLtechnology,data-drivenSOH\nrepublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/orafee.Requestpermissionsfrompermissions@acm.org. estimationmodelshaveachievedsignificantprogressinbothac-\nConferenceacronym’XX,June03–05,2018,Woodstock,NY curacyandcomputationalefficiency[32,51].However,obtaining\n©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\nsufficient training data for LIBs is challenging due to the time-\nACMISBN978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX consuming nature of degradation experiments, which typically\n4202\nvoN\n91\n]GL.sc[\n3v86000.2042:viXra\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nTable1:ComparisonbetweenBatteryTTTandothertransferlearn- tasks,leadingtosuboptimalperformance.Toaddressthis,weex-\ningmethodsregardingthetargetLIBdatasetprerequisites.Note:the ploretheintegrationoftheinherentphysicallawsofmodernLIBs\ntimeestimatesarebasedontheKOKAMdataset[3].\nintoself-supervisedlearningwithinBatteryTTT,aframeworkwe\ntermPhysics-GuidedTest-TimeTraining.Thisapproachlever-\nMethods woDegrad SSF-WL BatteryTTT(Ours) agesthe1-RCEquivalentCircuitModel(ECM)equationstoguide\nUnlabeledData 100% 30% 1amidstdegradation thepre-trainedmodelinmakingaccurateself-predictions,leading\nLabeling 0% 30% 0% toimprovedresults.Thedetailsofourmethodandexperimental\nCollectionTime 8473hours 2542hours 0hours resultsarediscussedinthefollowingsections.\nOntheotherhand,withtherapidadvancementoflargelanguage\nmodels(LLMs)[8,28,29,42]therehasbeengrowinginterestin\nspan months to years. Additionally, precisely labeling this data\nexploringtheirpotentialforprocessingcross-disciplinarysequence\nnecessitatesadditionalcyclesundercontrolledtemperatureand\ndatabeyondnaturallanguage,includingproteinsequencepredic-\ncurrentconditionsinthelaboratory.Consequently,thescarcityof\ntion [23] and time series analysis [19, 54]. Pioneering research\nhigh-qualityLIBdatacontinuestopresentamajorobstacleobstacle\nhas validated the efficacy of this paradigm and highlighted the\ninbatteryagingmodeling[22,33,51].\nunderlyingzero-shotgeneralizationcapabilitiesofLLMsacross\nToaddressthechallengeofdatascarcity,existingstudieshave\nvariousdatasets.However,theapplicationofLLMstobatteryse-\nextensivelyexploredtransferlearningmethodsforcross-domain\nquencemodelingremainsundiscovered.Toaddressthisgap,we\nLIBSOHestimation[22,35,39,41,46].Forinstance,SSF-WL[46]\nevaluatetheperformanceofLLMsforprocessingcross-domain\nintroducedaself-supervisedapproachthatpre-trainsonunlabeled\nLIBsequencesinthisstudy.Specifically,Weemploytheconcept\nLIBdatafromasourcedomainandfine-tunesonadifferenttypeof\nofmodelreprogrammingtobridgethegapbetweenlanguageand\nLIB(targetdomain)usingonlyasmallportionoflabeleddata.This\nbatterymodalities.Additionally,wedevelopaprefixpromptadap-\nmethodachievescomparableresultsusingjust30%ofthelabeled\ntationstrategytoefficientlyintegrateanLLMintoourBatteryTTT\ntargetdata.Anothernotablestudy,woDegrad[22],proposedmini-\nframework.Thiscombinationofstrategies,termedGPT4Battery,\nmizingthedomaingapbetweensourceandtargetLIBsbyaligning\nachievesstate-of-the-artgeneralizationresultsamongcurrentLIB\ntheirfeaturesinthelatentspace,eliminatingtheneedforadditional\nbenchmarks.\nlabeledtargetdatatoestimateSOH.\nByintroducingtheBatteryTTTframeworkandGPT4Battery\nUnfortunately,whileexistingstudieshavesignificantlyallevi-\nmodel,wehopethisstudywillinspiretheresearchcommunityto\natedtheburdenofdatalabeling,theyoftenoverlookthechallengeof\nfullyleverageadvancedAItechniques,suchasTest-TimeTraining\ncollectingunlabeledtargetdata(UTD).Duetothetime-consuming\n(TTT)andlargelanguagemodels(LLMs),formorescenario-fitting\nnatureofdegradationexperiments,gatheringsufficientUTDfor\nAI4Scienceproblems,therebysavingenormoustimeandacceler-\natypicalLIBcantakemonthstoyears,dependingonthebattery\natingscientificdiscovery.Fortherestofthispaper,Sec.2presents\ntype.Asaresult,preparingsufficientUTDforpreviousalgorithms\nbackgroundandrelatedworks,Sec.3introducesthepreliminar-\ntofunctioneffectivelyalsodemandsaconsiderableamountoftime.\nies,Sec.4describesthemethodologyofBatteryTTTframework\nForexample,migratingapre-trainedwoDegradmodel[22]tothe\nandGPT4Batterymodel,Sec.5conductsexperiments,andSec.6\nKOKAMdataset[3],awidelyusedLIBdataset,wouldrequirea\nconcludes.\nminimum of 8,473 hours to collect enough UTD from KOKAM\nforeffectivedomaingapalignment,whichishardlypracticalin\n2 RelatedWork\nreal-worldapplications.Tothebestofourknowledge,fewstudies\nhavesuccessfullyestimatedSOHwithoutrelyingonasubstantial In this section, we introduce the related works in Data-Driven\namountofUTD. BatterySOHEstimation(Sec.2.1),Test-TimeAdaptation(Sec.2.2)and\nThepurposeofthisstudyistodevelopapracticaltransferlearn- LLMforCross-disciplinarySequenceModeling(Sec.2.3),respectively.\ningframeworkforcross-domainLIBSOHestimationthatminimizes\ntherelianceonUTDs.InspiredbytheTest-TimeTraining(TTT)1 2.1 Data-DrivenBatterySOHEstimation\ntechniquefromcomputervision[10,38],weproposeBatteryTTT.\nData-drivenbatterySOHestimationasascendedasapivotaltopic\nUnlikepreviousmethodsthatrequireasubstantialamountofUTDs\ninindustrialartificialintelligenceanddataminingwiththewide-\ncollectedovertime,BatteryTTTadaptsthemodelcontinuallyusing\nspreadadoptionofmodernLIBsinvariousapplications,bringinga\neachindividualUTDcollectedamidstdegradation.InTable1,we\nsurgeindemandforsafeandefficientbatterymanagement[25,32].\ncomparetheprerequisitesofthetargetLIBdatasetforBatteryTTT\nWiththeevolutionofmodelarchitectureswithintheAIcommunity,\nwiththoseofothertransferlearningmethods.BatteryTTTsignifi-\nalgorithmsforbatterystateestimationhavealsoprogressedfrom\ncantlyreducestheamountofrequiredUTDandlabeling,offeringa\nstatisticalmachinelearningmethods,suchasRandomForest[4]\nmoreefficientapproachcomparedtoexistingmethods.Toachieve\nandGaussianProcessRegression[30],tohigh-performancedeep\nthis,BatteryTTTemploysaproxyunsupervisedtasktoutilizethe\nneuralnetworks,includingMLP[14],LSTM[16],andTransformer\nUTDforgradient-basedmodelupdates.Althoughsomeunsuper-\n[51].\nvisedmethodsfromexistingTTTliteraturecanbeapplied,suchas\nHowever,toobtainbothbatterytrainingdataandground-truth\nself-prediction,theyarenotspecificallydesignedforbattery-related\nlabelsrequirestime-andresource-consumingdegradationexperi-\n1Inthefollowingsections,thetermsTest-TimeTraining(TTT)andTest-TimeAdapta- ments,posingapersistenthurdleinbatteryagingmodeling[22,32,\ntion(TTA)maybeusedinterchangeably. 33,51].Noticingthissignificantissue,researchersinthebattery\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronym’XX,June03–05,2018,Woodstock,NY\nfieldhavetriedtousetransferlearningmethodsforgeneralizable 3 Preliminaries\nSOHestimation[22,35,39,41,46].However,thereisanotablegap\n3.1 BatterySOHDefinition\nbetweencurrentmethods’assumptionsabouthavingaccesstothe\nAsbatteriesundergorepeatedchargeanddischargecycles,theirca-\ntargetLIBdatasetandthereal-worldsituation,especiallyregard-\npacitygraduallydeclinesduetoaging,whichleadstoperformance\ningtheunlabeledtargetdata(UTD).whileexistingstudieshave\ndegradationandpotentialsafetyissues.TheStateofHealth(SOH)\nsignificantlyalleviatedtheburdenofdatalabeling,theyoftenover-\nquantifiesthebattery’sremainingcapacityrelativetoitsinitial\nlookthechallengeofcollectingUTDs.Duetothetime-consuming\ncapacitywhennew.Specifically,ifwedenotethenominalcapacity\nnatureofdegradationexperiments,gatheringsufficientUTDfor\natypicalLIBcantakemonthstoyears,dependingonthebattery oftheLIBas𝐶 𝑛𝑜𝑟𝑚 andthefulldischargecapacityinthecurrent\ntype.Thiscostoftimeisunacceptableinreal-worlddeployment cycleas𝐶 𝑓𝑢𝑙𝑙 ,theSOHisdefinedastheratioofthesetwovalues,\nexpressedasapercentage:\n(asshowninTable1).Conversely,BatteryTTTfullyutilizeseach\nindividualUTDcollectedamidstdegradationforadaptationtothe\n𝐶\ntargetdomain,therebysignificantlyreducingdatacollectiontime SOH= full ×100%. (1)\n𝐶\nnom\n2.2 Test-TimeTraining LIBsaretypicallyconsideredtohavereachedtheendoftheir\nlifecyclewhentheirSOHdropstoapproximately75%.\nTest-timeTraining(TTT)/Adaptation(TTA),alsoknownasone-\nsampleunsuperviseddomainadaptation,aimstoadaptamodel\n3.2 FeatureEngineering\ntrainedonthesourcedomaintothetargetdomainaseveryun-\nInthisstudy,weutilizeQdLinear[33],adegradationfeaturederived\nlabeledtestsamplearrives[20,38].TheprocessofTTAusually\nfromthelinearinterpolationofthevoltage-capacitycurveduring\ninvolvesaself-supervisedlosstoextractinformationfromthesin-\nchargecycles,tomaptherelationshipwithSOH.Thisfeatureis\ngletargetdomainsample,suchasrotation[38],mask[10]anden-\nwidelyrecognizedandemployedbymainstreamSOHestimation\ntropyminimization[45].DespitethestudyofvariousTTAmethods,\nalgorithms[22,25,33,51].\nmostaredesignedfor(image)classificationandcannotbeapplied\nto time series regression. For instance, Test-time Entropy Mini-\n3.3 ProblemDefinition\nmization(Tent)[45]foundthattheentropyofpredictionstrongly\ncorrelateswithaccuracyonthetargetdomain,BACS[53],MEMO Weformalizetheproblemofcross-domainLIBSOHestimation\n[52],andEATA[26]followTent’sapproachandimproveadaptation asfollows.Givenawell-curatedbatterydatasetfromthesource\nperformance,makingthemthemostrepresentativeTTAmethods. domain, denoted as S = {(𝑥 1,𝑦 1),(𝑥 2,𝑦 2),...,(𝑥 𝑆,𝑦 𝑆)}, where\nAlthoughsomeunsupervisedmethodsfromexistingTTTliterature X∈R1×𝑇 representstheextractedQdLinearfeaturewith𝑇 time\ncanbeapplied,suchasself-prediction,theyarenotspecificallyde- steps, and𝑦 denotes the corresponding SOH label. This source\nsignedforbattery-relatedtasks,leadingtosuboptimalperformance. setcontains𝑆labeledlifelongsamples.Incontrast,foradifferent\nInthispaper,weexplorehowtoincorporatetheinherentphysics batterytypeinthetargetdomain,wecanacquireonlyoneunla-\nofLIBsintoself-supervisedlearning,resultinginamorenatural beledfeatureatatimeafterreal-worlddeployment,representedas\nandpowerfulapproach. T ={𝑥 1,𝑥 2,...,𝑥 𝑇}.Ourobjectiveistoestimateeachcorrespond-\ningtargetlabel𝑦 𝑡,with𝑦 1beingconsideredasSOH=100%fora\n2.3 LLMforCross-DisciplinarySequence newbattery.\nModeling\n4 Methodology\nWith the rapid advancement of large language models (LLMs)\nInthissection,wepresentthemethodologyofourframework.Af-\n[8,28,29,42]therehasbeengrowinginterestinexploringtheir\nterprovidingasystematicoverviewinSection4.1,wefocuson\npotentialforprocessingcross-disciplinarysequencedatabeyond\ntwokeyinnovations:Physics-GuidedSelf-SupervisedLearning(PG-\nnaturallanguage,includingproteinsequenceprediction[23]and\nSSL)andPrefixPromptAdaptation(PPA),whicharedetailedin\ntimeseriesanalysis[6,13,36,54].Forproteinsequenceprediction,\nSections4.2and4.3.InSection4.4,weexplainhowexistingStateof\nLLMhaveshowedpowerfulcross-domainpotentialagainstpro-\nHealth(SOH)estimationmodelsareintegratedintotheBatteryTTT\nteinlanguagemodelsbythedesignofvocabulary[9,31].Fortime\nframeworkforcross-domaintransferlearning,alongsidetheex-\nseries,theseeffortshaveevolvedfromtheinitialdirectapplica-\nplorationofLargeLanguageModels(LLMs)forbatterysequence\ntionoflargelanguagemodels(LLMs)tosequencetasks[54],to\nmodeling.\ndesigningalearneddictionaryofpromptstoguideinference[5],\ntoattemptingtoalignthesemanticspacesbetweenlanguageand\n4.1 SystemOverview\ntimeseriesmodalities[18,27].Althoughtheeffectivenessoflarge\nlanguagemodels(LLMs)inhandlingcross-disciplinarysequence Figure1depictsanoverviewoftheBatteryTTTframework,whichis\ndatahasbeendemonstrated,thefieldofbatteryresearchhasyetto composedofthreemajorcomponents:pre-trainingonexperimental\nbenefitfromthisadvancement.Inthispaper,weaddressthisgapby datasets,incrementaldatacollectioninreal-worlddeployment,and\nrepurposinganLLMforStateofHealth(SOH)estimationthrough test-timeadaptation.Firstly,weutilizeexperimentaldatasets(source\nmodelreprogramming.Additionally,weevaluatethegeneralization domain)totrainapre-trainedmodelforSOHestimation.Wethen\nimprovementsachievedbyadaptinganLLMforcross-batterySOH deploythispre-trainedmodeltoreal-worlddevices,suchasthe\nestimation. BatteryManagementSystem(BMS)ofanelectriccaroramobile\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nphone.TheBMSindividuallycollectsunlabeledtestdataduring\nusage,anduponreceivingasample,weconductatest-timetraining PGLoss\nprocess to adapt the pre-trained model to this different type of\nLIB(targetdomain).Specifically,thetest-timeadaptationprocess\nconsistsoftwosteps:PG-SSLtoconstructanunsupervisedlossfrom\nG\n&\nr Va ad li ue ents\ntheunlabeledsample,andPPAtoadaptthepre-trainedmodeltothis Random Mask\nnewdomaininaparameter-efficientmanner.Afterlearningfrom Model\nincomingdata,theadaptedmodelisreadytomakeaprediction.\nThisprocessoperatescontinuallyuntiltheLIBretires. Maximum capacity (SOH)\n4.2 Physics-GuidedSelf-supervisedLearning Time/Capacity Time/Capacity\n(a) P artial Qdlinear feature (b) Generated complete Qdlinear feature\nUnlabeled Qdlinear 1-RC ECM Equation Physics-Guided Figure3:Guidethepre-trainedmodeltogenerateacomplete\nFeature Loss QdlinearfeaturecurveandsuperviseitwithPhysics-Guided\n𝐶(cid:3043) loss.\n𝑅\n𝑅(cid:3043)\n𝐼 𝑂𝐶𝑉 𝑢(cid:3043) 𝑢 Thisimpliesthattheterminalvoltage𝑢,current𝐼 andtempera-\ntureshould,inprinciple,followtheordinarydifferentialequation\n(ODE)functionsrepresentingthebattery’sphysicalstate,asde-\nFigure2:TransformanarbitraryunlabeledQdlinearfeature scribedinEquation4.Byqueryingthereal-timecurrentandtemper-\nintoaPhysics-Guidedloss. aturevaluefromtheBMS,wecantransformanarbitraryunlabeled\nQdlinearfeatureintoaPGLossinanunsupervisedmanner.\nInthissubsection,weelaborateonhowtofullyexploitthein-\n4.2.2 GenerateacompleteQdlinearfeatureandsuperviseitwith\nherent physical laws of a single LIB sequence to improve TTA\nPGLoss. Inthefiledofbatteryresearch,acompleteQdlinearcurve\nperformancethroughthedesignofphysics-guidedself-supervised\nspanningfromthelowertotheuppervoltagelimitscandescribe\nlearning(PG-SSL).Specifically,wefirstdescribehowtousethe\nLIB’sagingmodeandthereforecantheoreticallyidentifytheaccu-\n1-RCECMequationtotransformanarbitraryunlabeledQdlinear ratestateofhealth[50][7][40]accordingtoitsdefinition2.Inspired\nfeatureintoaphysics-guidedloss(PGLoss).Wewillthenexplain\nbythis,wewanttodesigntheobjectivebyguidingthepre-trained\nhowminimizingthisPGlossfacilitatestheestimationoftheLIB\nmodeltogenerateacompleteQdlinearfeaturefromthegivenpar-\nSOH.\ntialone,whichmeanstogenerateFigure3(b)from(a).\n4.2.1 Thevenin’sEquivalentCircuitModel(ECM)ofLIB. Theequiv- Specifically,giventheinput(partial)Qdlinearfeaturecurvex∈\nalentcircuitmodel(ECM)iswidelyusedbatterymodeltodescribe R1×𝑇 ,weusethepre-trainedmodeltogenerateacompleteonexˆ ∈\ntheelectricalbehaviorofthebatteryintermsofvoltages,currents,\nR1×𝑇′ ,where𝑇′ >𝑇.Thiscompletevoltage-capacitycurveisthen\nresistancesandcapacitances[48][43].ThefirstorderThevenin supervisedwithPGLoss(asshowninFigure3).Additionally,we\nmodelisthoughttobeaccurateandadequatetomodelthecondi- randomlymaskaportionofxtopromoterepresentationlearning.\ntionofthebattery,andatthesametimesimpleandcomputationally Ingeneral,ourobjectivecanbeformulatedintheformof:\nefficient[49].The𝑂𝐶𝑉 isrepresentedbyanidealvoltagesource\nofthebattery.𝑅accountsfortheinternalohmicresistance.The\nparallel𝑅𝐶-branch,comprising𝑅 𝑃 and𝐶 𝑃,isusedtomodelbattery L PG−SSL=∥xˆ[0:𝑇]−x∥2 𝐹 +𝜆∥𝜃 1𝐼+𝜃 2𝑢ˆ+𝑢ˆ(cid:164)=0∥2 𝐹 (5)\npolarizationeffect.𝑢and𝐼denotestheterminalvoltageandcurrent\nHere,xˆ[0 : 𝑇] representstheoverlapbetweenthegenerated\nthatcanbecollectedinuse.BasedonKirchhoff’slaw,theelectrical completevoltage-capacitycurveandthegivenx.𝑢ˆ denotesthe\nbehaviorofthebatterycanbecharacterizedasphysicalequations generatedcompletevoltage-timecurve.Theparameters𝜃 1and𝜃\n2\nas: areassociatedwithtemperature,and𝐼 denotesthecurrent;allof\nthesevaluescanbequeriedthroughtheBMSduringdeployment.\n𝑢 𝑂𝐶𝑉 =𝑢 𝑅+𝑢 𝑝 +𝑢 (2) WewillempiricallydemonstratethatthedesignofPhysics-Guided\nPG-SSLishighlysuitableforourSOHestimationtaskandleads\n𝑅+𝑅\n𝑝 𝐼+ 1 𝑢+𝑢(cid:164)=0 (3) tobetterperformancethansimpleself-prediction,asshowninthe\n𝐶 𝑅 𝐶 𝑅\n𝑝 𝑝 𝑝 𝑝 experimentalsection.\nWedefine𝜃 1= 𝐶𝑅 𝑝+𝑅 𝑅𝑝\n𝑝\nand𝜃 2= 𝐶𝑝1 𝑅𝑝.Followingrecentworks,the\ncoeffientsarefunctionsoftemperature.Thestatefunctionbecomes: 2ThiscompleteQdlinear(voltage-capacity)curverequiresadditionalcyclesunder\nconstrainedtemperatureandcurrentconditionsinthelaboratoryandinfeasibleat\ndeployment.TheunlabeledQdlinearfeatureweobtainedatuseisalwaysapartialof\n𝜃 1(𝑇)𝐼+𝜃 2(𝑇)𝑢+𝑢(cid:164)=0 (4) it.\negatloV egatloV\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronym’XX,June03–05,2018,Woodstock,NY\n4.3 PrefixPromptAdaptation\nInthissubsection,weintroducePrefixPromptAdaptation(PPA),a 𝑓 𝑥,𝑔 𝑥 =argminL PG−SSL(𝑥;𝑓 0,𝑔 0) (9)\n𝑓,𝑔\nmethodthatreducesthedimensionofthesolutionspaceforeasier\noptimizationoftest-timeadaptation.Specifically,inspiredbythe\ndemonstratedeffectivenessofcontinuouspromptlearninginthe\n𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛=ℎ 0◦𝑓 𝑥(𝑥) (10)\nfieldofdeepmodelfine-tuning[2,17].Weaddasmallnumberof Inparticular,wearguefortheuseofourproposedprefixprompt\nsoftpromptsasaprefixtotheembeddedinputcontextfortest- adaptation in Equation 6 to avoid the substantial computation\ntimeupdating,whilekeepingallothermodelparametersfrozen. broughtbyfine-tuningEquation9atinferencetime.\nInthisway,thedimensionoflearnablemodelparametersshallbe\nsignificantlyreducedandthusenablingthepracticaldeploymentof 4.5 IntegrateanLLMintoBatteryTTT\nTTAonreal-worldedgedevicessuchasaCPUwhenthepre-trained (GPT4Battery)\nmodelislarge(suchasanLLM).Formally,givenatestsample𝑋 𝑡𝑒𝑠𝑡,\nInthissubsection,weexplorethepotentialofLLMinbatteryse-\nourgoalistofindanoptimalprompt𝒑∗:\nquencemodeling.InspiredbyrecentstudieswhichutilizeanLLM\nforproteinsequenceprediction[23]andtimeseriesanalysis[18],\n𝒑∗=argminL PGTPT(F,𝒑,𝑋 test) (6)\nweprimarilyemploytheideaofmodelreprogrammingtoeffectively\n𝒑\nusingthephysics-guidedself-supervisedlossL PGTPTfromEqua- alignthemodalitiesofbatterydataandnaturallanguage,leverag-\ntion4.Here,F denotesthepre-trainedmodel.Wewillalsodemon- ingthereasoningandgeneralizationabilitiesofLLMsforbattery\nstrateexperimentallythatPPAsignificantlyreducesthecostof tasks.\nTTAwhileonlymarginallyreducingitsperformance. Specifically,wefirsttokenizeandmaptheinputQdlinearfeatures\nintoahigh-dimensionalspacewiththesamedimensionalityasthe\n4.4 IntegreteExistingSOHEstimationModels wordspaceofthelanguagemodel.Then,wefusetheinformation\nintoBatteryTTT ofbatterysequencemodalityandlanguagemodalityusingacross-\nattentionlayer.\nInthissubsection,wedemonstratehowtoincorporateexistingSOH However,forawordembeddingspaceofanLLME ∈ R𝑉×𝐷 ,\nestimationalgorithms[51]intoourtest-timeadaptationframework where𝑉 is the vocabulary size. The vocabulary size can be in-\ntocompletethewholecross-domainSOHestimationprocessde-\nevitablylarge(forexample,GPT2hasaVof50257[29]).Simply\npictedinFigure1.ExistingSOHestimationstudies[32,51]exten-\nleveragingEwillresultinlargeandpotentiallydensereprogram-\nsivelyusestatisticalmodelsandhigh-performanceneuralnetworks\nmingspace,increasingthecomputationcomplexityanddifficulty\nforLIBSOHestimation,suchasGaussianProcessRegression[47],\nofcatchingtherelevantsourcetokens.Following[37]and[18],\nRandomForest[12],MLP[14],RNNs[16]andTransformer[44]for\nwemaintainonlyasmallcollectionoftextprototypesbylinearly\nSOHestimation.Onlydeeplearningmethodscanbeincorporated probingE,denotedasE′ ∈R𝑉′×𝐷 ,where𝑉′ ≪𝑉.Then,wealign\nintotheTTAframework,aswellasothertransferlearningsettings.\nthetokenizedinputpatchesandtextprototypeswithamulti-head\nSpecifically,ourarchitecturefollowsaY-shapeddesign,asde-\ncross-attentionlayer.Specifically:\nscribedin[10,34]:afeatureextractor𝑓 issimultaneouslyfollowed\nb sty ita uts eel 𝑓f-s wu ip te hrv this eed enh ce oa dd e𝑔 ra on fd exa ism tia ni gn nta es uk rah lea nd etℎ w. oH re kr se, fow re Ss Ou Hb-\nZ\n𝑘(𝑖)\n=attention(Q\n𝑘(𝑖),K 𝑘(𝑖),V 𝑘(𝑖)\n)\nestimation,suchasGRU,LSTM,andTransformer,and𝑔withthe Q(𝑖) K(𝑖)⊤\ndecoder.ForthemainregressiontaskheadℎforSOHestimation,we =softmax(\n𝑘 √︁𝑑𝑘\n)V\n𝑘(𝑖)\nusealinearprojectionfromthedimensionoftheencoderfeatures 𝑘\nto1,whichisprimarilyahistoricartifact[15]. By aggregating each Z(𝑖) ∈ R𝑃×𝑑 in every head, we obtain\nDuringpre-training,wefirsttrain𝑔◦𝑓 usingthePG-SSLloss Z(𝑖) ∈ R𝑃×𝐷 . This way,𝑘 the text prototypes can learn cues in\ninEquation5inanunsupervisedmannerwiththefeaturesofthe\nlanguagewhichcanthenrepresenttherelevantlocalpatchinfor-\nsourceLIBdataset.Thenweperformlinearprobingbycombining\nmation.Wewillexperimentallydemonstratetheimprovementin\ntheencoder𝑓 withthemaintaskheadℎ,keeping𝑓 frozen.Formally:\ngeneralizabilityachievedbyincorporatingLLMsandtheeffective-\n𝑛 nessofmodelreprogramming.\n1∑︁\n𝑓 0,𝒈0=argm 𝒇,i 𝒈n\n𝑛\n𝑖=1L PG−SSL(𝑥 𝑖;𝒇,𝒈) (7)\n5 Experiments\nFollowedby:\nIn this section, we empirically evaluate the proposed approach\n𝑛 on six real-world LIB datasets, including five publicly available\n1∑︁\nℎ 0=argm ℎin\n𝑛\nL MSE(ℎ◦𝑓 0(𝑥 𝑖),𝑦 𝑖) (8) datasetsfordailyapplications(withcapacitiesrangingfromafew\n𝑖=1 Ah) and our own collected 300Ah large LIB dataset for energy\nThesummationisoverthetrainingsetwith𝑛 samples,each storage.Specifically,wefocuson(1)theoverallimprovedgener-\nconsistingofinput𝑥 𝑖 andlabel𝑦 𝑖.Duringtest-timeadaptation,we alizationperformanceofTTAandthesuperiorperformanceby\noptimize𝑔◦𝑓 beforemakingapredictioneachtimeatestinput𝑥 combiningGPT-2andTTA(GPT4Battery),(2)anefficacystudy\narrives.Afteroptimization,wemakeapredictionon𝑥asℎ◦𝑓 𝑥(𝑥), ofthetwoproposeddesigns,PG-SSLandPPA,(3)theablationre-\nformallyas: sultsofGPT4Battery,(4)inferenceefficiencyofinvolvingTTA,and\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nTable2:MainspecificationsofselectedLIBdatasets.\nDataset VoltageRange Samples EstimatedCollect Collector\nElectrodeMaterNiaolminalCapacity Time\nCALCE LCO 1.1(Ah) 2.7-4.2(V) 2807 1397(hour) UniversityofMaryland\nSANYO NMC 1.85(Ah) 3.0-4.1(V) 415 644(hour) RWTHAachenUniversity\nKOKAM LCO/NCO 0.74(Ah) 2.7-4.2(V) 503 8473(hour) UniversityofOxford\nPANASONIC NCA 3.03(Ah) 2.5-4.29(V) 2770 1801(hour) BeijingInstitudeofTechnology\nGOTION LFP 27(Ah) 2.0-3.65(V) 4262 2238(hour) BeijingInstitudeofTechnology\nNHRY LFP 300(Ah) 2.5-3.5(V) 808 1200(hour) Ours\n(5)thescalabilityanddeploymentofourmethodonlargeenergy furtherdetails,wehavemadetherelevantcodeanddataavailableat\nstorageLIBs. thefollowinglink:https://anonymous.4open.science/r/gpt4battery-\n55FC.\n5.1 ExperimentSettings\n5.2 MainPerformance\n5.1.1 Datasetpreparation. Weconductedexperimentsusingfive\npubliclyavailablelithium-ionbattery(LIB)datasetsintendedfor Inthissection,wereportthemainimprovementsincross-domain\ndailycommercialuse,withcapacitiesrangingfrom0.74Ahto27 generalization performance of our proposed TTA methods (e.g.\nAh.Additionally,weutilizedfourofourownLIBdatasetscollected PG-SSLandPPA),onfivecommercialLIBdatasetsfordailyusage.\nforindustrial-levelenergystorage,specificallywithacapacityof Additionally,wedemonstratethatbyleveraginganLLMastheback-\n300Ah,whichwewillalsomakepubliclyavailableforacademic bone,GPT4Batteryachievessuperiorgeneralizationperformance\npurposes.Thesedatasetsencompassavarietyofwidelyusedcath- comparedtoallbaselinemethods.\nodeactivematerials,capacities,andmanufacturers.Asummaryof\nthedatasetstatisticsispresentedinTable2.\nTable3:ImprovedperformanceofTTAonexistingmethods\n5.1.2 Baselines. Wecompareourmethodwithfourtypesofbase-\nandcomparisonwithcurrenttransferlearningmethods.\nlinestodemonstratetheefficacyoftheproposedBatteryTTTframe-\nworkandtheGPT4Batterymodel:(1)Existingnon-transferlearn-\nCALCE SANYO KOKAM PANASONIC GOTION\ningmachinelearning(ML)methodsforLIBStateofHealth(SOH)\nModels MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nestimation,includingGaussianProcessRegression[47],Random\nRandomForest 5.76 4.9 7.87 6.73 5.76 4.88 4.96 4 0.62 0.54\nForest[12],Multi-LayerPerceptron(MLP)[14],RecurrentNeural LightGBM 6.8 5.42 6.31 5.62 6.52 5.31 6.06 4.97 0.55 0.47\nNetworks(RNNs)[16],andTransformer[44]3;(2)Integrationofex- MLP 3.93 3.52 6.1 5.93 13.1 10.9 5.08 4.38 0.54 0.44\nGRU 2.18 2.86 9.17 9.19 3.07 3.44 2.53 3.5 0.74 0.84\nistingmodelsintoourBatteryTTTframework;(3)State-of-the-art LSTM 2.52 2.78 7.58 7.84 3.07 3.93 1.55 2.19 1.65 1.68\nTransformer 2.27 2.57 8.1 8.24 15.3 17.1 1.9 2.35 1.28 1.4\ntransferlearningmethodsforcross-domainLIBSOHestimation,\nsuchaswoDegrad[22]andSSF-WL[46]4;and(4)Integrationof GRU+(TTA) 1.9 2.25 7.8 8.01 2.4 2.74 1.74 2.73 0.67 0.71\nLSTM+ 2.08 2.36 6.71 7.04 2.93 3.67 1.31 2 0.65 0.77\nlarge language models (LLMs) into the BatteryTTT framework Transformer+ 1.83 1.97 7.03 7.18 13.2 14.6 1.32 1.85 0.34 0.43\nGPT-2+ 1.52 1.89 1.35 1.71 7.95 8.01 1.28 1.95 0.38 0.47\n(GPT4Battery). Bert+ 2.01 2.33 1.46 1.82 7.77 8.04 1.52 2.2 0.41 0.52\nWereproducetheresultsofBatteryMLandwoDegradbasedon Llama-7b+ 1.57 1.94 1.35 1.69 8.58 9.66 1.3 2.01 0.4 0.49\ntheprovidedcodeandfollowtheapproachdetailsforSSF-WL.For woDegrad 1.76 1.96 1.21 1.54 1.76 3.01 2.09 2.56 0.45 0.58\nSSF-WL 1.55 1.93 1.08 1.24 6.21 5.1 1.44 2.06 0.51 0.72\nafaircomparison,weadheretothedatapre-processingmethods\noutlinedin[51]anduseQdLinear [1]astheunifiedfeatureset.\nWeadoptthestandardevaluationmetricsofmeanabsoluteerror 5.2.1 ImprovementofGeneralizationAbilityofTTA. Table3shows\n(MAE)androotmeansquarederror(RMSE). themainimprovementsinthegeneralizationperformanceofour\nproposedTTAmethods.WeusetheGOTIONdatasetasthesource\n5.1.3 Implementationdetails. Ourmodelsareimplementedusing\ndatasetforitsextensivelabelcoverageandthenincludeitsown\nPytorchandtrainedonasingle3070TiGPU.WeutilizetheAdamW\ntestsetalongwiththeremainingfourdatasets(CALCE,SANYO,\noptimizer[21]withafixedlearningrateof1e-3forpre-training\nKOKAM and PANASONIC) as the target datasets for generaliz-\nand linear probing until convergence. The mask ratio is set to\nabilitytesting.Overall,weobservethattheTTAmethodshowsa\n30%duringthisphase.TTAisconductedusingstochasticgradient\nsignificantperformanceimprovementofabout50%comparedto\ndescent(SGD)withamomentumof0.9duetoitsconsistencyin\ntheno-TTAmethod.Somemodels(suchasTransformer,GPT2)\nimprovingperformanceondistributionshifts[10].Typically,weset\nequippedwithTTAachieveaperformancethatrivalsorevenex-\nafixedlearningrateof1e-2anditeratefor10steps,asmoresteps\nceedstheperformanceofcurrenttransferlearningmethodsthat\nonlymarginallyimproveperformancebasedonourobservations.\nrequireadditionalaccesstothetargetdatainCALCE,PANASONIC\nThemaskratioduringTTAwillbespecificallyanalyzedlater.For\nandGOTION.Specifically,withinallthemethodsthatuseTTA,\ntheutilizationoflargelanguagemodelsalsohasmadequiteadif-\n3BatteryML[51]providesacomprehensiveplatformsummarizingthesemodels\n4Thesemethodsrelyonsufficientunlabeledtargetdata(UTDs)tooperateeffectively, ferenceingeneralizationperformanceimprovement.Forinstance,\nwhichrequiresimpracticaldatacollectiontime,assummarizedinTable1. GPT2+achievesfirstorsecondrankperformanceontheCALCE,\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronym’XX,June03–05,2018,Woodstock,NY\nGOTIONandPANASONICdatasetscomparedtoallbaselinemeth- full-parameterfine-tuning.Therefore,weoverallreportthebest\nods.LLama+alsoobtainedthebestandsecond-bestperformance performancefortheirfullparameterfine-tuning,whilereporting\nontheSANYOandPANASONICdataset,respectively,comparedto thebestperformanceforLLMsusingPPAinTable3andFigure4.\ntheotherTTAmethods.Wealsoobservethatsomemodelarchitec- TheperformanceofothermodelsusingPPAisthoroughlyablated\nturesdominatetheperformanceoncertaindatasetsoverTTA,e.g., inSec.5.3.1.\ntheRNNfamily(GRU,LSTM)outperformstheTransformerfamily\nonKOKAMasawhole,andtheTTA-enhancedGRU+achieves 5.3 EfficacyAnalysis\nperformancecomparabletothatofwoDegrad.\nInthissection,weanalyzetheeffectivenessandimportantdesign\nchoicesofeachcomponent.Specifically,weevaluatethetwoTTA\ndesigns:Physical-GuidedSelf-supervisedLearning(PG-SSL)and\nGRU+ PrefixPromptAdaptation(PPA).\n108 LSTM+\nTransformer+ 5.3.1 EffectandComputationTrade-offsofPrefixPromptAdapta-\n107 GPT2+ tion. Byadjustingonlyasmallnumberofsoftpromptsprefixtothe\nSSF-WL (2024) inputcontext,thisdesigncansignificantlyreducetheadjustable\n106 woDegrad (2023) parametersto103ordersofmagnituderegardlessofmodelsize,as\n105 GPT2+(PPA) illustratedinFigure5ontheGPT2,TransformerandLSTMmod-\nels.ApplyingPPAtoTransformerandLSTMslightlyimpactthe\n104 accuracy,asshowninFigure5.ApplyingPPAtoLLMshowever,sig-\nnificantlyreducestheMeanAbsoluteError(MAE)from3.89to1.52,\n103\nareductionofapproximately60.9%.Moreover,PPAachievesbetter\n0 generalization performance than traditional parameter-efficient\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 fine-tuningofthepositionalencodingandlayernormalizationlay-\nMean Absolute Error (MAE)\ners [54] ofthe LLM. We attribute this tothe language-agnostic\npatternrecognitionandinferencecapabilitiesacquiredthrough\nFigure4:SuperiorPerformanceofGPT4Battery(onCALCE\npre-trainingontextcorpora[13,24].Guidedbylearnableprefix\ndataset).\nprompts,thesecapabilitiescanalsobegeneralizedtobatteryse-\nquencedata.\n5.2.2 SuperiorPerformanceofGPT4BatteryoverallBaselineMeth-\nods. Inthissection,weprovideadetailedcomparisonofthegener-\nalizabilitygainsandcomputationtrade-offsachievedbyincorpo- 3.50 MAE 108\n3.25 RMSE\nratingLLMs.Figure4demonstratesthatGPT2equippedwithTTA 3.00 P arams 107\n(whichwenameGPT4Battery)obtainsthelowestMAEresultson 2.75 106\n2.50 105\ntheCALCEdatasetthanallothermodelsincludingcurrenttransfer 2.25 104\nlearningmethods(woDegradandSSF-WL)thatrequireadditional 12 .. 70 50 103\nassumptionstoaccessthetargetdataset.GPT4batteryalsogets 1.50 0\nNo TTA TTA-full TTA-peft TTA-PPA\nverycompetitiveresultsinotherdatasetsasdootherlargemodels (a) GPT2 + TTA\n(e.g.LlamaandBert).\nTheresultsofapplyingPrefixPromptAdaptation(PPA)tolarge 3.0 R PM aMA rS aE mE s106 3.0 M R PaMA rE S amE s 106\nlanguagemodels(LLMs),asillustratedinFigure4,areparticularly 2.5 105 2.5 105\nnoteworthy.Conventionalfine-tuningofGPT-2’spositionalencod- 104 104\ningandlayernormalizationsignificantlyincreasedthenumberof 2.0 2.0\n103 103\ntrainedparametersattesttime,byafactorof10to100comparedto\nTransformer+andRNNs+,whileonly(relatively)slightlyreducing 1.5 No TTA TTA-full TTA-PPA 102 1.5 No TTA TTA-full TTA-PPA 102\n(b) Transformer + TTA (c) LSTM + TTA\nthemeanabsoluteerror(MAE)from1.83to1.71.Incontrast,our\nPrefixPromptAdaptation(PPA)approachnotonlyreducesthe\nFigure 5: Efficacy and computation trade-offs of Prefix\nnumberofadjustableparametersbynearly10,000times,making\nPromptAdaptation(PPA)ondifferentmodels.\nGPT4Battery ten times more parameter-efficient than the RNN\nseries,butalsofurtherreducestheMAEfrom1.71to1.52,anim-\nprovementofapproximately10.7%.However,weshouldnotethe\nlimitationthatinvolvinganLLMdoesincreasetheinferencetime 5.3.2 EfficacyandParameterSensitivityofPhysical-GuidedSelf-\nevenwithPPA,whichisatrade-offbetweenaccuracyandcompu- supervisedLearning. Inthissection,weanalyzetheeffectiveness\ntationefficiency. ofPG-SSLandconductasensitivityanalysisofanotherimportant\nItisimportanttonotethatwhileapplyingPPAtoregularTrans- parameteraffectingtheself-supervisedloss,themaskratio.We\nformer+andRNNs+alsoreducesthenumberoftrainableparame- performedablationstudiesonlossesusingphysicalconstraintsand\nterstotheorderof103,itcanimpairtheirgeneralizationperfor- puremaskreconstruction.Additionally,weanalyzedtheeffectof\nmance,resultinginaslightlyinferiorperformancecomparedto differentmaskingratesontheMAEreductionofTTAs.Weemploy\nsmaraP\nelbaniarT\n)emiT\ntseT(\nESMR/EAM\nESMR/EAM\nsmaraP\nelbaniarT\nESMR/EAM\nsmaraP\nelbaniarT\nsmaraP\nelbaniarT\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nthetransformermodelandfull-parametertuningforTTA,conduct- Table4:AblatingthedifferentcomponentsofGPT4Battery.\ningexperimentsontheadaptationtotheCALCEandPANASONIC Red:thebest.\ndatasets.\nFigure6showsthattheinclusionofthephysical-guidedloss Method CALCE SANYO KOKAM PANAS. GOTION\nseamlessly enhances the performance in both MAE and RMSE\nGPT4Battery 1.52 1.35 7.95 1.28 0.25\nreduction.Comparatively,theMAEreductionofusingPG-SSLis w/oPG-SSL 2.11 1.12 8.34 3.11 0.265\nslightlymoresignificantintheCALCEdataset(Fig.6(a))thanin w/oMaskedTTA 2.05 0.97 8.44 1.22 0.255\nw/oTTA 2.13 1.34 9.51 3.44 0.297\nthePANASONICdataset(Figure6(b)).Thisisbecausetheformer\nw/omodelreprogramming 4.13 2.78 10.56 4.57 0.31\nisamuchmorenonlineardataset,makingpurereconstructionloss\nlesseffectiveincapturingrepresentations.Wealsoobservefrom\nboth(a)and(b)ofFigure6thatalargermaskratioof0.7to0.9 5.5 TTAEfficiency\npromoteslearningabetterrepresentationwithorwithoutPG-SSL,\nWepresenttheaveragerunningtimeacrossfivedatasetsforthree\nwhileasmallmaskratioof0.5to0.6fails.Thisisconsistentwiththe\nrepresentativemodelsequippedwithTTAandtwocurrenttransfer\nMAE-basedself-supervisedlearningobservationsinthecomputer\nlearningmethods.Ourfocusisprimarilyonmodelinferencetime,\nvisiondomain,wheremaskingahighproportionoftheinputimage,\nasTTAdoesnotsignificantlyimpacttrainingduration.Table5\ne.g.,75%,yieldsanon-trivialandmeaningfulself-supervisorytask.\ndemonstratesthatalthoughTTA-equippedmethodsachievesub-\nInsummary,thecombinationofmaskinginputandPG-SSLresults\nstantiallyhigheraccuracy,theyare10to100timesslowerthan\ninthebestTTAresults.\nexistingmethods.Thistrade-offhighlightsthatourapproachsacri-\nficessomespeedforenhancedaccuracy.However,theresulting102\nspeedlevelremainssufficientformodernBMSrequirements,where\n0.5 wo PG 0.8 wo PG individualresponsetimestypicallyneedtobeunder500msinour\n0.4 PG 0.7 PG\n0.3 0.6 deployedsystem.Moreimportantly,ourframeworkcaneliminate\n0.5\n0.2 0.4 theneedfor644to8,473hoursofdegradationexperimentsfordata\n0.1 00 .. 23 collection,assummarizedinTables1and2.\n0.0 0.1\n-0.1 0.0\n-0.2 -0.1 Table5:EfficiencyAnalysisofTTA.\n0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0\nMask Ratio Mask Ratio\n(a) Effect on CALCE dataset.\nMethod GPT2+ Transformer+ LSTM+ woDegrad SSF-WL\n00 .. 56 w PGo PG 00 .. 45 w PGo PG\nOverallInferenceTime(s) 32.6 6.1 4.3 0.35 0.3 000 ... 234 0000 .... 0123 One MIn of de ere ln pc ae raT mim ete er( sms) 5 71 6. 81 07 19 2.5 87\n0\n16 2.7 85\n0\n6870 7.5 45\n000\n260 0. 14 37\n69\n0.1 -0.1 0.0 -0.2\n-0.1 -- 00 .. 43\n-0.2 -0.5\n-0.3 -0.6 5.6 ScalabilityonLargeEnergyStorageLIBs\n0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0\nMask Ratio Mask Ratio\n(b) Effect on PANASONIC dataset. Inthissection,wereportthescalabilityoftheproposedframework\nthroughdeploymentonourbatterymanagementsystem(BMS)for\nFigure6:EffectonMAE/RMSEReductionofPhysical-Guided industry-levelenergystorageLIBs(300Ah).\nSelf-supervisedLearningandSensitivityofMaskRatio.\n5.6.1 LIBDataset. WecollecttheNHRYdatasetbyperforming\ndegradationexperimentson8industry-levelenergystorageLIBs\n(300Ah)from4brands(Ningde,Haichen,Ruipu,Yiwei).Testenvi-\nronmentandprocedurescomplywiththeChinaStandardBMSfor\nEnergyStorageGB/T34131-2023.Atotalof800degradationcycles\n5.4 AblationStudyofGPT4Battery\nwereexperiencedrangingovertwomonthsfromDecember2022\nInthissection,weconsiderthebestperformingGPT2+TTAasa\ntoJanuary2023.\nself-containedmodel(GPT4Battery)andprovideablationstudyon\ntheeffectsofdifferentcomponentsordesignchoices.Ourresultsin 5.6.2 OnlinePerformance. ForfourdifferentbrandsofLIBswith\nTable4indicatethatablatingeithermodelreprogrammingorany thesamecapacity,wemixtwoofthebrandsasthesourcedomain\notherdesignsintest-timeadaptationhurtsthegeneralizationper- andevaluatetheonlineperformanceoftheLIBsfromtheremaining\nformanceonunseenLIBs.Intheabsenceofphysicalguidance,we twobrands,resultinginatotaloftwocombinationsforthecross-\nobserveanotableaverageperformancedegradationof55.1%,which batterysetting.WereporttheMAE/RMSEmetricalongwiththe\nbecomesmorepronounced(i.e.,exceeding70%)whendiscarding inferencetime(ms)usingrepresentativebaselines.\ntheTTAstrategycompletely.Theactofmodelreprogrammingalso\n6 Conclusion\nstandsasapivotalelementincross-modalityalignment,enabling\ntheLLMtounderstandtheLIB’ssequencedatawiththehelpof Inthispaper,weproposeanoveltest-timeadaptation(TTA)frame-\ntextprototypes.Ablationofreprogrammingresultsleadstoover workforcross-domainLIBstateofhealth(SOH)estimation.This\n20%degradationonaverageperformance. one sample adaptation setting allows the model to continually\nnoitcudeR\nEAM\nnoitcudeR\nEAM\nnoitcudeR\nESMR\nnoitcudeR\nESMR\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronym’XX,June03–05,2018,Woodstock,NY\nTable6:OnlineperformanceonLargeEnergyStorageLIBs. References\n[1] PeterMAttia,KristenASeverson,andJeremyDWitmer.2021. Statistical\nCombination1 Combination2 learningforaccurateandinterpretablebatterylifetimeprediction.JournalofThe\nElectrochemicalSociety168,9(2021),090547.\nMAE RMSE PerInfer.Time MAE RMSE PerInfer.Time [2] HyojinBahng,AliJahanian,SwamiSankaranarayanan,andPhillipIsola.2022.\nLSTM 4.33 4.59 4.23 2.44 2.69 3.23 Exploring visual prompts for adapting large-scale models. arXiv preprint\nTransformer 5.62 5.89 7.61 3.48 3.78 6.32 arXiv:2203.17274(2022).\n[3] ChristophBirkl.2017.Oxfordbatterydegradationdataset1.(2017).\nLSTM+ 0.775 0.788 54.53 0.55 0.62 44.85\n[4] LeoBreiman.2001.Randomforests.Machinelearning45(2001),5–32.\nTransformer+ 1.25 1.38 88.57 1.02 1.23 77.83\n[5] DefuCao,FurongJia,SercanOArik,TomasPfister,YixiangZheng,WenYe,and\nGPT4Battery 0.734 0.761 103.47 0.314 0.418 105.68\nYanLiu.2023.Tempo:Prompt-basedgenerativepre-trainedtransformerfortime\nseriesforecasting.arXivpreprintarXiv:2310.04948(2023).\n[6] Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. 2024.\nadapttothetargetdomainwitheverysingleunlabeledtestsample, LLM4TS:AligningPre-TrainedLLMsasData-EfficientTime-SeriesForecast-\ners. arXiv:2308.08469[cs.LG]\nperfectlyaligningwiththenatureofbatterydegradationfeatures,\n[7] ChengChen,RuiXiong,RuixinYang,andHailongLi.2022.Anoveldata-driven\nwhichcanonlybeobtainedonebyoneduringthelongagingpro- methodforminingbatteryopen-circuitvoltagecharacterization.GreenEnergy\ncess.Thissettingalsoaddressesthelimitationsofexistingtransfer andIntelligentTransportation1,1(2022),100001.\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:\nlearningmethods,whichassumeadditionalaccesstothetargetLIB\nPre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv\ndataset,therebysavingmonthstoyearsoflaborindatacollection. preprintarXiv:1810.04805(2018).\nByintroducingGPT4Battery,wehopethisworkwillinspirethe [9] NoeliaFerruz,SteffenSchmidt,andBirteHöcker.2022. ProtGPT2isadeep\nunsupervisedlanguagemodelforproteindesign.Naturecommunications13,1\nresearchcommunitytofullyleverageadvancedAItechniques,such (2022),4348.\nasTest-TimeAdaptation(TTA)andlargelanguagemodels(LLMs), [10] YossiGandelsman,YuSun,XinleiChen,andAlexeiEfros.2022.Test-timetraining\nwithmaskedautoencoders.AdvancesinNeuralInformationProcessingSystems\nformoreAI4Scienceproblems,therebysavingenormoustimeand\n35(2022),29374–29385.\nacceleratingscientificdiscovery. [11] Konstantinos N. Genikomsakis, Nikolaos-Fivos Galatoulas, and Christos S.\nIoakimidis.2021.Towardsthedevelopmentofahotel-basede-bikerentalservice:\nResultsfromastatedpreferencesurveyandtechno-economicanalysis.Energy\n215(2021),119052. https://doi.org/10.1016/j.energy.2020.119052\n[12] PierreGeurts,DamienErnst,andLouisWehenkel.2006.Extremelyrandomized\ntrees.Machinelearning63(2006),3–42.\n[13] NateGruver,MarcFinzi,ShikaiQiu,andAndrewGWilson.2024.Largelanguage\nmodelsarezero-shottimeseriesforecasters. AdvancesinNeuralInformation\nProcessingSystems36(2024).\n[14] SimonHaykin.1998.Neuralnetworks:acomprehensivefoundation.PrenticeHall\nPTR.\n[15] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.\n2022. Maskedautoencodersarescalablevisionlearners.InProceedingsofthe\nIEEE/CVFconferenceoncomputervisionandpatternrecognition.16000–16009.\n[16] SeppHochreiterandJürgenSchmidhuber.1997.Longshort-termmemory.Neural\ncomputation9,8(1997),1735–1780.\n[17] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,\nBharathHariharan,andSer-NamLim.2022. Visualprompttuning.InEuro-\npeanConferenceonComputerVision.Springer,709–727.\n[18] MingJin,ShiyuWang,LintaoMa,ZhixuanChu,JamesYZhang,XiaomingShi,\nPin-YuChen,YuxuanLiang,Yuan-FangLi,ShiruiPan,etal.2023. Time-llm:\nTimeseriesforecastingbyreprogramminglargelanguagemodels.arXivpreprint\narXiv:2310.01728(2023).\n[19] MingJin,YifanZhang,WeiChen,KexinZhang,YuxuanLiang,BinYang,Jindong\nWang,ShiruiPan,andQingsongWen.2024.PositionPaper:WhatCanLargeLan-\nguageModelsTellUsaboutTimeSeriesAnalysis.arXivpreprintarXiv:2402.02713\n(2024).\n[20] JianLiang,RanHe,andTieniuTan.2024.Acomprehensivesurveyontest-time\nadaptationunderdistributionshifts. InternationalJournalofComputerVision\n(2024),1–34.\n[21] IlyaLoshchilovandFrankHutter.2017.Decoupledweightdecayregularization.\narXivpreprintarXiv:1711.05101(2017).\n[22] JiahuanLu,RuiXiong,JinpengTian,ChenxuWang,andFengchunSun.2023.\nDeeplearningtoestimatelithium-ionbatterystateofhealthwithoutadditional\ndegradationexperiments.NatureCommunications14,1(2023),2760.\n[23] LiuzhenghaoLv,ZongyingLin,HaoLi,YuyangLiu,JiaxiCui,CalvinYu-Chian\nChen,LiYuan,andYonghongTian.2024.Prollama:Aproteinlargelanguage\nmodelformulti-taskproteinlanguageprocessing.arXivpreprintarXiv:2402.16445\n(2024).\n[24] SuvirMirchandani,FeiXia,PeteFlorence,BrianIchter,DannyDriess,Montser-\nratGonzalezArenas,KanishkaRao,DorsaSadigh,andAndyZeng.2023.Large\nLanguageModelsasGeneralPatternMachines. arXiv:2307.04721[cs.AI]\n[25] Man-FaiNg,JinZhao,QingyuYan,GarethJConduit,andZhiWeiSeh.2020.\nPredictingthestateofchargeandhealthofbatteriesusingdata-drivenmachine\nlearning.NatureMachineIntelligence2,3(2020),161–170.\n[26] ShuaichengNiu,JiaxiangWu,YifanZhang,YaofoChen,ShijianZheng,Peilin\nZhao,andMingkuiTan.2022. Efficienttest-timemodeladaptationwithout\nforgetting.InInternationalconferenceonmachinelearning.PMLR,16888–16905.\n[27] ZijiePan,YushanJiang,SahilGarg,AndersonSchneider,YuriyNevmyvaka,and\nDongjinSong.2024.IP-LLM:SemanticSpaceInformedPromptLearningwith\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nLLMforTimeSeriesForecasting.arXivpreprintarXiv:2403.05798(2024). [40] JinpengTian,RuiXiong,WeixiangShen,JiahuanLu,andXiao-GuangYang.2021.\n[28] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.2018. Deepneuralnetworkbatterychargingcurvepredictionusing30pointscollected\nImprovinglanguageunderstandingbygenerativepre-training.(2018). in10min.Joule5,6(2021),1521–1534.\n[29] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever, [41] JinpengTian,RuiXiong,WeixiangShen,JiahuanLu,andXiao-GuangYang.2021.\netal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog Deepneuralnetworkbatterychargingcurvepredictionusing30pointscollected\n1,8(2019),9. in10min.Joule5,6(2021),1521–1534. https://doi.org/10.1016/j.joule.2021.05.012\n[30] CarlEdwardRasmussenandHannesNickisch.2010. Gaussianprocessesfor [42] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne\nmachinelearning(GPML)toolbox.TheJournalofMachineLearningResearch11 Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal\n(2010),3011–3015. Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv\n[31] AlexanderRives,JoshuaMeier,TomSercu,SiddharthGoyal,ZemingLin,Ja- preprintarXiv:2302.13971(2023).\nsonLiu,DemiGuo,MyleOtt,C.LawrenceZitnick,JerryMa,andRobFergus. [43] Manh-KienTran,ManojMathew,StefanJanhunen,SatyamPanchal,Kaamran\n2021.Biologicalstructureandfunctionemergefromscalingunsupervisedlearn- Raahemifar,RoydonFraser,andMichaelFowler.2021.Acomprehensiveequiva-\ningto250millionproteinsequences. ProceedingsoftheNationalAcademyof lentcircuitmodelforlithium-ionbatteries,incorporatingtheeffectsofstateof\nSciences118,15(2021),e2016239118. https://doi.org/10.1073/pnas.2016239118 health,stateofcharge,andtemperatureonmodelparameters.JournalofEnergy\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118 Storage43(2021),103252.\n[32] DariusRoman,SaurabhSaxena,ValentinRobu,MichaelPecht,andDavidFlynn. [44] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\n2021.Machinelearningpipelineforbatterystate-of-healthestimation.Nature AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall\nMachineIntelligence3,5(2021),447–456. youneed.Advancesinneuralinformationprocessingsystems30(2017).\n[33] KristenASeverson,PeterMAttia,NormanJin,NicholasPerkins,BenbenJiang, [45] DequanWang,EvanShelhamer,ShaotengLiu,BrunoOlshausen,andTrevor\nZiYang,MichaelHChen,MuratahanAykol,PatrickKHerring,Dimitrios Darrell.2020.Tent:Fullytest-timeadaptationbyentropyminimization.arXiv\nFraggedakis,etal.2019. Data-drivenpredictionofbatterycyclelifebefore preprintarXiv:2006.10726(2020).\ncapacitydegradation.NatureEnergy4,5(2019),383–391. [46] TianyuWang,ZhongjingMa,SuliZou,ZhanChen,andPengWang.2024.\n[34] ManliShu,WeiliNie,De-AnHuang,ZhidingYu,TomGoldstein,AnimaAnand- Lithium-ionbatterystate-of-healthestimation:Aself-supervisedframework\nkumar,andChaoweiXiao.2022.Test-timeprompttuningforzero-shotgener- incorporatingweaklabels.AppliedEnergy355(2024),122332.\nalizationinvision-languagemodels.AdvancesinNeuralInformationProcessing [47] ChristopherKIWilliamsandCarlEdwardRasmussen.2006.Gaussianprocesses\nSystems35(2022),14274–14289. formachinelearning.Vol.2.MITpressCambridge,MA.\n[35] XingShu,JiangweiShen,GuangLi,YuanjianZhang,ZhengChen,andYonggang [48] RuiXiong,LinlinLi,andJinpengTian.2018.Towardsasmarterbatterymanage-\nLiu.2021.AFlexibleState-of-HealthPredictionSchemeforLithium-IonBattery mentsystem:Acriticalreviewonbatterystateofhealthmonitoringmethods.\nPacksWithLongShort-TermMemoryNetworkandTransferLearning. IEEE JournalofPowerSources405(2018),18–29.\nTransactionsonTransportationElectrification7,4(2021),2238–2248. https: [49] ZhaoyiXu,YanjieGuo,andJosephHomerSaleh.2022. Aphysics-informed\n//doi.org/10.1109/TTE.2021.3074638 dynamicdeepautoencoderforaccuratestate-of-healthpredictionoflithium-ion\n[36] DimitrisSpathisandFahimKawsar.2023.Thefirststepisthehardest:Pitfalls battery.NeuralComputingandApplications34,18(2022),15997–16017.\nofRepresentingandTokenizingTemporalDataforLargeLanguageModels. [50] SijiaYang,CaipingZhang,JiuchunJiang,WeigeZhang,YangGao,andLinjing\narXiv:2309.06236[cs.LG] Zhang.2021.Avoltagereconstructionmodelbasedonpartialchargingcurve\n[37] ChenxiSun,YaliangLi,HongyanLi,andShendaHong.2023.TEST:Textproto- forstate-of-healthestimationoflithium-ionbatteries.JournalofEnergyStorage\ntypealignedembeddingtoactivateLLM’sabilityfortimeseries.arXivpreprint 35(2021),102271.\narXiv:2308.08241(2023). [51] HanZhang,XiaofanGui,ShunZheng,ZihengLu,YuqiLi,andJiangBian.2024.\n[38] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritz BatteryML:AnOpen-sourcePlatformforMachineLearningonBatteryDegrada-\nHardt.2020.Test-timetrainingwithself-supervisionforgeneralizationunder tion.InTheTwelfthInternationalConferenceonLearningRepresentations.\ndistributionshifts.InInternationalconferenceonmachinelearning.PMLR,9229– [52] MarvinZhang,SergeyLevine,andChelseaFinn.2022.Memo:Testtimerobust-\n9248. nessviaadaptationandaugmentation.Advancesinneuralinformationprocessing\n[39] YandanTanandGuangcaiZhao.2020.TransferLearningWithLongShort-Term systems35(2022),38629–38642.\nMemoryNetworkforState-of-HealthPredictionofLithium-IonBatteries.IEEE [53] AurickZhouandSergeyLevine.2021.Bayesianadaptationforcovariateshift.\nTransactionsonIndustrialElectronics67,10(2020),8723–8731. https://doi.org/10. Advancesinneuralinformationprocessingsystems34(2021),914–927.\n1109/TIE.2019.2946551 [54] TianZhou,PeisongNiu,XueWang,LiangSun,andRongJin.2023. OneFits\nAll:PowerGeneralTimeSeriesAnalysisbyPretrainedLM. arXivpreprint\narXiv:2302.11939(2023).\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronym’XX,June03–05,2018,Woodstock,NY\nA Appendix:PseudoCode B Appendix:DegradationConditionsof\nSelectedDatasets\nAlgorithm1PretrainingandTest-TimeAdaptationofBatteryTTT\nTable7providesthespecificdegradationconditionsoftheselected\n1: Input:SourceLIBdataset(𝑥 𝑖,𝑦 𝑖)for𝑖 =1,...,𝑛 lithium-ionbatteries(LIBs)usedinthiswork,includingdetailed\n2: Output:Predictionfortestinput𝑥 cellinformationandtheprotocolsforcharginganddischarging.\n3: Pre-trainingPhase:\n4:\nfor𝑖 =1to𝑛do Received20February2007;revised12March2009;accepted5June2009\n5: ComputeL PG−SSL(𝑥 𝑖;𝒇,𝒈)usingEquation5\n6: endfor\n7: Train:𝑓 0,𝑔 0=argmin 𝒇,𝒈 𝑛1 (cid:205)𝑛 𝑖=1L PG−SSL(𝑥 𝑖;𝒇,𝒈)(Equation\n1)\n8: LinearProbing:\n9:\nfor𝑖 =1to𝑛do\n10: ComputeL MSE(ℎ◦𝑓 0(𝑥 𝑖),𝑦 𝑖)\n11: endfor\n12: Train:ℎ 0=argminℎ 𝑛1 (cid:205)𝑛 𝑖=1L MSE(ℎ◦𝑓 0(𝑥 𝑖),𝑦 𝑖)(Equation2)\n13: Test-TimeAdaptationPhase:\n14: foreachtestinput𝑥 do\n15: Optimize𝑓 𝑥,𝑔 𝑥 =argmin𝑓,𝑔L PG−SSL(𝑥;𝑓 0,𝑔 0)(Equation9)\n16:\nCompute𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛=ℎ 0◦𝑓 𝑥(𝑥)(Equation4)\n17: endfor\n18: PrefixPromptAdaptation:\n19: Useproposedprefixpromptadaptationtoavoidcomputation\nduringinference\nConferenceacronym’XX,June03–05,2018,Woodstock,NY Trovatoetal.\nTable7:DegradationConditionsoftheSelectedLIBDatasets\nDataset CellInformation TestConditions\nCALCE 3 cells termed •Chargedataconstantcurrentrateof0.5Cuntilthevoltagereached4.2Vandthen4.2Vwas\nCS2-35, CS2- sustaineduntilthechargingcurrentdroppedtobelow0.05A.\n36,CS2-37 •Dischargedataconstantcurrentrateof1C.\n•Ambienttemperatureisnotmentioned.\nSANYO 48commercial •Chargedataconstantcurrentrateof1Cuntilthevoltagereached4.1Vandthen4.1Vwassustained\nUR18650E untilthechargingcurrentdroppedtobelow0.04A.\ncylindrical •Dischargedataconstantcurrentrateof1C.\ncells •25°C.\nKOKAM 8 KOKAM •Chargedataconstantcurrentrateof1Cuntilthevoltagereached4.2V.\nSLPB533459H •Dischargedataconstantcurrentrateof1C.\n4pouchcells •40°C.\nPANASONIC 3 commercial •Chargedataconstantcurrentrateof0.3Cuntilthevoltagereached4.2Vandthen4.2Vwas\nNCR18650BD sustaineduntilthechargingcurrentdroppedtobelow0.03A.\ncylindrical •Dischargedataconstantcurrentrateof2C.\ncells •20°C.\nGOTION 3 commercial Chargedataconstantcurrentrateof1Cuntilthevoltagereached3.65Vandthen3.65Vwassustained\nIFP20100140A untilthechargingcurrentdroppedtobelow1.35A.\ncells •Dischargedataconstantcurrentrateof1C.\n•45°C.",
    "pdf_filename": "Adapting_Amidst_Degradation_Cross_Domain_Li-ion_Battery_Health_Estimation_via_Physics-Guided_Test-Ti.pdf"
}