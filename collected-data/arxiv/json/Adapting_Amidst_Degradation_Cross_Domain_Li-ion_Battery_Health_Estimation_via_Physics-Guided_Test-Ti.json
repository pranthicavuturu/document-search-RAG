{
    "title": "Adapting Amidst Degradation: Cross Domain Li-ion Battery",
    "abstract": "Healthmodelingoflithium-ionbatteries(LIBs)iscrucialforsafe Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018. AdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEs- and efficient energy management and carries significant socio- timationviaPhysics-GuidedTest-TimeTraining.InProceedingsofMake economicimplications.AlthoughMachineLearning(ML)-based suretoenterthecorrectconferencetitlefromyourrightsconfirmationemai StateofHealth(SOH)estimationmethodshavemadesignificant (Conference acronym â€™XX). ACM, New York, NY, USA, 12 pages. https: progressinaccuracy,thescarcityofhigh-qualityLIBdataremains //doi.org/XXXXXXX.XXXXXXX amajorobstacle.Althoughexistingtransferlearningmethodsfor cross-domainLIBSOHestimationhavesignificantlyalleviatedthe 1 Introduction labelingburdenoftargetLIBdata,theystillrequiresufficientunla- beledtargetdata(UTD)foreffectiveadaptationtothetargetdomain. Step2 Step1 CollectingthisUTDischallengingduetothetime-consumingna- tureofdegradationexperiments.Toaddressthisissue,weintroduce Pre-training Source LIB Source Domain BatteryML, LLMs Real-world Deployment apractical Test-TimeTrainingframework,BatteryTTT,which adaptsthemodelcontinuallyusingeachUTDcollectedamidstdegra- Step4 Step4 Step2 dation,therebysignificantlyreducingdatacollectiontime.Tofully Prefix-Prompt TTA utilizeeachUTD,BatteryTTTintegratestheinherentphysicallaws Adaptation Target Domain Step3 ofmodernLIBsintoself-supervisedlearning,termedPhyscics- Step3 GuidedTest-TimeTraining.Additionally,weexplorethepoten- Physics Guided Self-Supervised One Unlabeled Data tialoflargelanguagemodels(LLMs)inbatterysequencemodel- Learning ingbyevaluatingtheirperformanceinSOHestimationthrough modelreprogrammingandprefixpromptadaptation.Thecombi- Figure1:OverviewofBatteryTTTframework,whichconsistsof nationofBatteryTTTandLLMmodeling,termedGPT4Battery, three major components: (Step 1) pre-training on experimental achievesstate-of-the-artgeneralizationresultsacrosscurrentLIB datasets;(Step2)incrementaldatacollectionafterdeployment;and benchmarks.Furthermore,wedemonstratethepracticalvalueand (Steps3-4)test-timeadaptation.Steps2,3,and4iterateuntiltheLIB scalabilityofourapproachbydeployingitinourreal-worldbattery retires. managementsystem(BMS)for300Ahlarge-scaleenergystorage LIBs. TherapidadvancementsinrechargeableLi-ionbatteries(LIBs) havefacilitatedtheirwidespreaduseacrossvarioussectors,includ- CCSConcepts ingportableelectronics,medicaldevices,renewableenergysystems, â€¢TestTimeTrainingâ†’BatteryHealthEstimation. andelectricvehicles[11].Thisubiquity,however,introducescritical challengesassociatedwithcapacitydegradationandperformance",
    "body": "Adapting Amidst Degradation: Cross Domain Li-ion Battery\nHealth Estimation via Physics-Guided Test-Time Training\nYuyuanFeng GuoshengHu\nfengyuyuan01@gmail.com UniversityofBristol\nXiamenUniversity Bristol,England\nXiamen,China huguosheng100@gmail.com\nXiaodongLi ZhihongZhang*\nHongKongUniversity XiamenUniversity\nHongKong,China Xiamen,China\nAbstract ACMReferenceFormat:\nHealthmodelingoflithium-ionbatteries(LIBs)iscrucialforsafe Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018.\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEs-\nand efficient energy management and carries significant socio-\ntimationviaPhysics-GuidedTest-TimeTraining.InProceedingsofMake\neconomicimplications.AlthoughMachineLearning(ML)-based\nsuretoenterthecorrectconferencetitlefromyourrightsconfirmationemai\nStateofHealth(SOH)estimationmethodshavemadesignificant\n(Conference acronym â€™XX). ACM, New York, NY, USA, 12 pages. https:\nprogressinaccuracy,thescarcityofhigh-qualityLIBdataremains\n//doi.org/XXXXXXX.XXXXXXX\namajorobstacle.Althoughexistingtransferlearningmethodsfor\ncross-domainLIBSOHestimationhavesignificantlyalleviatedthe 1 Introduction\nlabelingburdenoftargetLIBdata,theystillrequiresufficientunla-\nbeledtargetdata(UTD)foreffectiveadaptationtothetargetdomain. Step2\nStep1\nCollectingthisUTDischallengingduetothetime-consumingna-\ntureofdegradationexperiments.Toaddressthisissue,weintroduce Pre-training Source LIB\nSource Domain BatteryML, LLMs Real-world Deployment\napractical Test-TimeTrainingframework,BatteryTTT,which\nadaptsthemodelcontinuallyusingeachUTDcollectedamidstdegra- Step4 Step4 Step2\ndation,therebysignificantlyreducingdatacollectiontime.Tofully Prefix-Prompt TTA\nutilizeeachUTD,BatteryTTTintegratestheinherentphysicallaws Adaptation\nTarget Domain Step3\nofmodernLIBsintoself-supervisedlearning,termedPhyscics- Step3\nGuidedTest-TimeTraining.Additionally,weexplorethepoten- Physics Guided\nSelf-Supervised One Unlabeled Data\ntialoflargelanguagemodels(LLMs)inbatterysequencemodel-\nLearning\ningbyevaluatingtheirperformanceinSOHestimationthrough\nmodelreprogrammingandprefixpromptadaptation.Thecombi-\nFigure1:OverviewofBatteryTTTframework,whichconsistsof\nnationofBatteryTTTandLLMmodeling,termedGPT4Battery,\nthree major components: (Step 1) pre-training on experimental\nachievesstate-of-the-artgeneralizationresultsacrosscurrentLIB datasets;(Step2)incrementaldatacollectionafterdeployment;and\nbenchmarks.Furthermore,wedemonstratethepracticalvalueand (Steps3-4)test-timeadaptation.Steps2,3,and4iterateuntiltheLIB\nscalabilityofourapproachbydeployingitinourreal-worldbattery retires.\nmanagementsystem(BMS)for300Ahlarge-scaleenergystorage\nLIBs. TherapidadvancementsinrechargeableLi-ionbatteries(LIBs)\nhavefacilitatedtheirwidespreaduseacrossvarioussectors,includ-\nCCSConcepts\ningportableelectronics,medicaldevices,renewableenergysystems,\nâ€¢TestTimeTrainingâ†’BatteryHealthEstimation. andelectricvehicles[11].Thisubiquity,however,introducescritical\nchallengesassociatedwithcapacitydegradationandperformance\nKeywords evaluation.Asaninherentlyinterdisciplinarysubject,batteryaging\nBatteryHealthEstimation,TestTimeTraining,DataScarcity,Large modelinghasemergedasafundamentalissueattheintersectionof\nLanguageModel batteryscienceandmachinelearning(ML)[25,32,33,51].Accurate\nStateofHealth(SOH)estimationforLIBsiscrucialnotonlyfor\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor\nensuringsafeandefficientenergymanagementbutalsoforopti-\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation mizingthedesignandperformanceofnext-generationbatteries,\nonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe thushavingsignificantsocio-economicimplications.\nauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or\nWiththerapidadvancementofMLtechnology,data-drivenSOH\nrepublish,topostonserversortoredistributetolists,requirespriorspecificpermission\nand/orafee.Requestpermissionsfrompermissions@acm.org. estimationmodelshaveachievedsignificantprogressinbothac-\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY curacyandcomputationalefficiency[32,51].However,obtaining\nÂ©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\nsufficient training data for LIBs is challenging due to the time-\nACMISBN978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX consuming nature of degradation experiments, which typically\n4202\nvoN\n91\n]GL.sc[\n3v86000.2042:viXra\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nTable1:ComparisonbetweenBatteryTTTandothertransferlearn- tasks,leadingtosuboptimalperformance.Toaddressthis,weex-\ningmethodsregardingthetargetLIBdatasetprerequisites.Note:the ploretheintegrationoftheinherentphysicallawsofmodernLIBs\ntimeestimatesarebasedontheKOKAMdataset[3].\nintoself-supervisedlearningwithinBatteryTTT,aframeworkwe\ntermPhysics-GuidedTest-TimeTraining.Thisapproachlever-\nMethods woDegrad SSF-WL BatteryTTT(Ours) agesthe1-RCEquivalentCircuitModel(ECM)equationstoguide\nUnlabeledData 100% 30% 1amidstdegradation thepre-trainedmodelinmakingaccurateself-predictions,leading\nLabeling 0% 30% 0% toimprovedresults.Thedetailsofourmethodandexperimental\nCollectionTime 8473hours 2542hours 0hours resultsarediscussedinthefollowingsections.\nOntheotherhand,withtherapidadvancementoflargelanguage\nmodels(LLMs)[8,28,29,42]therehasbeengrowinginterestin\nspan months to years. Additionally, precisely labeling this data\nexploringtheirpotentialforprocessingcross-disciplinarysequence\nnecessitatesadditionalcyclesundercontrolledtemperatureand\ndatabeyondnaturallanguage,includingproteinsequencepredic-\ncurrentconditionsinthelaboratory.Consequently,thescarcityof\ntion [23] and time series analysis [19, 54]. Pioneering research\nhigh-qualityLIBdatacontinuestopresentamajorobstacleobstacle\nhas validated the efficacy of this paradigm and highlighted the\ninbatteryagingmodeling[22,33,51].\nunderlyingzero-shotgeneralizationcapabilitiesofLLMsacross\nToaddressthechallengeofdatascarcity,existingstudieshave\nvariousdatasets.However,theapplicationofLLMstobatteryse-\nextensivelyexploredtransferlearningmethodsforcross-domain\nquencemodelingremainsundiscovered.Toaddressthisgap,we\nLIBSOHestimation[22,35,39,41,46].Forinstance,SSF-WL[46]\nevaluatetheperformanceofLLMsforprocessingcross-domain\nintroducedaself-supervisedapproachthatpre-trainsonunlabeled\nLIBsequencesinthisstudy.Specifically,Weemploytheconcept\nLIBdatafromasourcedomainandfine-tunesonadifferenttypeof\nofmodelreprogrammingtobridgethegapbetweenlanguageand\nLIB(targetdomain)usingonlyasmallportionoflabeleddata.This\nbatterymodalities.Additionally,wedevelopaprefixpromptadap-\nmethodachievescomparableresultsusingjust30%ofthelabeled\ntationstrategytoefficientlyintegrateanLLMintoourBatteryTTT\ntargetdata.Anothernotablestudy,woDegrad[22],proposedmini-\nframework.Thiscombinationofstrategies,termedGPT4Battery,\nmizingthedomaingapbetweensourceandtargetLIBsbyaligning\nachievesstate-of-the-artgeneralizationresultsamongcurrentLIB\ntheirfeaturesinthelatentspace,eliminatingtheneedforadditional\nbenchmarks.\nlabeledtargetdatatoestimateSOH.\nByintroducingtheBatteryTTTframeworkandGPT4Battery\nUnfortunately,whileexistingstudieshavesignificantlyallevi-\nmodel,wehopethisstudywillinspiretheresearchcommunityto\natedtheburdenofdatalabeling,theyoftenoverlookthechallengeof\nfullyleverageadvancedAItechniques,suchasTest-TimeTraining\ncollectingunlabeledtargetdata(UTD).Duetothetime-consuming\n(TTT)andlargelanguagemodels(LLMs),formorescenario-fitting\nnatureofdegradationexperiments,gatheringsufficientUTDfor\nAI4Scienceproblems,therebysavingenormoustimeandacceler-\natypicalLIBcantakemonthstoyears,dependingonthebattery\natingscientificdiscovery.Fortherestofthispaper,Sec.2presents\ntype.Asaresult,preparingsufficientUTDforpreviousalgorithms\nbackgroundandrelatedworks,Sec.3introducesthepreliminar-\ntofunctioneffectivelyalsodemandsaconsiderableamountoftime.\nies,Sec.4describesthemethodologyofBatteryTTTframework\nForexample,migratingapre-trainedwoDegradmodel[22]tothe\nandGPT4Batterymodel,Sec.5conductsexperiments,andSec.6\nKOKAMdataset[3],awidelyusedLIBdataset,wouldrequirea\nconcludes.\nminimum of 8,473 hours to collect enough UTD from KOKAM\nforeffectivedomaingapalignment,whichishardlypracticalin\n2 RelatedWork\nreal-worldapplications.Tothebestofourknowledge,fewstudies\nhavesuccessfullyestimatedSOHwithoutrelyingonasubstantial In this section, we introduce the related works in Data-Driven\namountofUTD. BatterySOHEstimation(Sec.2.1),Test-TimeAdaptation(Sec.2.2)and\nThepurposeofthisstudyistodevelopapracticaltransferlearn- LLMforCross-disciplinarySequenceModeling(Sec.2.3),respectively.\ningframeworkforcross-domainLIBSOHestimationthatminimizes\ntherelianceonUTDs.InspiredbytheTest-TimeTraining(TTT)1 2.1 Data-DrivenBatterySOHEstimation\ntechniquefromcomputervision[10,38],weproposeBatteryTTT.\nData-drivenbatterySOHestimationasascendedasapivotaltopic\nUnlikepreviousmethodsthatrequireasubstantialamountofUTDs\ninindustrialartificialintelligenceanddataminingwiththewide-\ncollectedovertime,BatteryTTTadaptsthemodelcontinuallyusing\nspreadadoptionofmodernLIBsinvariousapplications,bringinga\neachindividualUTDcollectedamidstdegradation.InTable1,we\nsurgeindemandforsafeandefficientbatterymanagement[25,32].\ncomparetheprerequisitesofthetargetLIBdatasetforBatteryTTT\nWiththeevolutionofmodelarchitectureswithintheAIcommunity,\nwiththoseofothertransferlearningmethods.BatteryTTTsignifi-\nalgorithmsforbatterystateestimationhavealsoprogressedfrom\ncantlyreducestheamountofrequiredUTDandlabeling,offeringa\nstatisticalmachinelearningmethods,suchasRandomForest[4]\nmoreefficientapproachcomparedtoexistingmethods.Toachieve\nandGaussianProcessRegression[30],tohigh-performancedeep\nthis,BatteryTTTemploysaproxyunsupervisedtasktoutilizethe\nneuralnetworks,includingMLP[14],LSTM[16],andTransformer\nUTDforgradient-basedmodelupdates.Althoughsomeunsuper-\n[51].\nvisedmethodsfromexistingTTTliteraturecanbeapplied,suchas\nHowever,toobtainbothbatterytrainingdataandground-truth\nself-prediction,theyarenotspecificallydesignedforbattery-related\nlabelsrequirestime-andresource-consumingdegradationexperi-\n1Inthefollowingsections,thetermsTest-TimeTraining(TTT)andTest-TimeAdapta- ments,posingapersistenthurdleinbatteryagingmodeling[22,32,\ntion(TTA)maybeusedinterchangeably. 33,51].Noticingthissignificantissue,researchersinthebattery\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY\nfieldhavetriedtousetransferlearningmethodsforgeneralizable 3 Preliminaries\nSOHestimation[22,35,39,41,46].However,thereisanotablegap\n3.1 BatterySOHDefinition\nbetweencurrentmethodsâ€™assumptionsabouthavingaccesstothe\nAsbatteriesundergorepeatedchargeanddischargecycles,theirca-\ntargetLIBdatasetandthereal-worldsituation,especiallyregard-\npacitygraduallydeclinesduetoaging,whichleadstoperformance\ningtheunlabeledtargetdata(UTD).whileexistingstudieshave\ndegradationandpotentialsafetyissues.TheStateofHealth(SOH)\nsignificantlyalleviatedtheburdenofdatalabeling,theyoftenover-\nquantifiesthebatteryâ€™sremainingcapacityrelativetoitsinitial\nlookthechallengeofcollectingUTDs.Duetothetime-consuming\ncapacitywhennew.Specifically,ifwedenotethenominalcapacity\nnatureofdegradationexperiments,gatheringsufficientUTDfor\natypicalLIBcantakemonthstoyears,dependingonthebattery oftheLIBasğ¶ ğ‘›ğ‘œğ‘Ÿğ‘š andthefulldischargecapacityinthecurrent\ntype.Thiscostoftimeisunacceptableinreal-worlddeployment cycleasğ¶ ğ‘“ğ‘¢ğ‘™ğ‘™ ,theSOHisdefinedastheratioofthesetwovalues,\nexpressedasapercentage:\n(asshowninTable1).Conversely,BatteryTTTfullyutilizeseach\nindividualUTDcollectedamidstdegradationforadaptationtothe\nğ¶\ntargetdomain,therebysignificantlyreducingdatacollectiontime SOH= full Ã—100%. (1)\nğ¶\nnom\n2.2 Test-TimeTraining LIBsaretypicallyconsideredtohavereachedtheendoftheir\nlifecyclewhentheirSOHdropstoapproximately75%.\nTest-timeTraining(TTT)/Adaptation(TTA),alsoknownasone-\nsampleunsuperviseddomainadaptation,aimstoadaptamodel\n3.2 FeatureEngineering\ntrainedonthesourcedomaintothetargetdomainaseveryun-\nInthisstudy,weutilizeQdLinear[33],adegradationfeaturederived\nlabeledtestsamplearrives[20,38].TheprocessofTTAusually\nfromthelinearinterpolationofthevoltage-capacitycurveduring\ninvolvesaself-supervisedlosstoextractinformationfromthesin-\nchargecycles,tomaptherelationshipwithSOH.Thisfeatureis\ngletargetdomainsample,suchasrotation[38],mask[10]anden-\nwidelyrecognizedandemployedbymainstreamSOHestimation\ntropyminimization[45].DespitethestudyofvariousTTAmethods,\nalgorithms[22,25,33,51].\nmostaredesignedfor(image)classificationandcannotbeapplied\nto time series regression. For instance, Test-time Entropy Mini-\n3.3 ProblemDefinition\nmization(Tent)[45]foundthattheentropyofpredictionstrongly\ncorrelateswithaccuracyonthetargetdomain,BACS[53],MEMO Weformalizetheproblemofcross-domainLIBSOHestimation\n[52],andEATA[26]followTentâ€™sapproachandimproveadaptation asfollows.Givenawell-curatedbatterydatasetfromthesource\nperformance,makingthemthemostrepresentativeTTAmethods. domain, denoted as S = {(ğ‘¥ 1,ğ‘¦ 1),(ğ‘¥ 2,ğ‘¦ 2),...,(ğ‘¥ ğ‘†,ğ‘¦ ğ‘†)}, where\nAlthoughsomeunsupervisedmethodsfromexistingTTTliterature XâˆˆR1Ã—ğ‘‡ representstheextractedQdLinearfeaturewithğ‘‡ time\ncanbeapplied,suchasself-prediction,theyarenotspecificallyde- steps, andğ‘¦ denotes the corresponding SOH label. This source\nsignedforbattery-relatedtasks,leadingtosuboptimalperformance. setcontainsğ‘†labeledlifelongsamples.Incontrast,foradifferent\nInthispaper,weexplorehowtoincorporatetheinherentphysics batterytypeinthetargetdomain,wecanacquireonlyoneunla-\nofLIBsintoself-supervisedlearning,resultinginamorenatural beledfeatureatatimeafterreal-worlddeployment,representedas\nandpowerfulapproach. T ={ğ‘¥ 1,ğ‘¥ 2,...,ğ‘¥ ğ‘‡}.Ourobjectiveistoestimateeachcorrespond-\ningtargetlabelğ‘¦ ğ‘¡,withğ‘¦ 1beingconsideredasSOH=100%fora\n2.3 LLMforCross-DisciplinarySequence newbattery.\nModeling\n4 Methodology\nWith the rapid advancement of large language models (LLMs)\nInthissection,wepresentthemethodologyofourframework.Af-\n[8,28,29,42]therehasbeengrowinginterestinexploringtheir\nterprovidingasystematicoverviewinSection4.1,wefocuson\npotentialforprocessingcross-disciplinarysequencedatabeyond\ntwokeyinnovations:Physics-GuidedSelf-SupervisedLearning(PG-\nnaturallanguage,includingproteinsequenceprediction[23]and\nSSL)andPrefixPromptAdaptation(PPA),whicharedetailedin\ntimeseriesanalysis[6,13,36,54].Forproteinsequenceprediction,\nSections4.2and4.3.InSection4.4,weexplainhowexistingStateof\nLLMhaveshowedpowerfulcross-domainpotentialagainstpro-\nHealth(SOH)estimationmodelsareintegratedintotheBatteryTTT\nteinlanguagemodelsbythedesignofvocabulary[9,31].Fortime\nframeworkforcross-domaintransferlearning,alongsidetheex-\nseries,theseeffortshaveevolvedfromtheinitialdirectapplica-\nplorationofLargeLanguageModels(LLMs)forbatterysequence\ntionoflargelanguagemodels(LLMs)tosequencetasks[54],to\nmodeling.\ndesigningalearneddictionaryofpromptstoguideinference[5],\ntoattemptingtoalignthesemanticspacesbetweenlanguageand\n4.1 SystemOverview\ntimeseriesmodalities[18,27].Althoughtheeffectivenessoflarge\nlanguagemodels(LLMs)inhandlingcross-disciplinarysequence Figure1depictsanoverviewoftheBatteryTTTframework,whichis\ndatahasbeendemonstrated,thefieldofbatteryresearchhasyetto composedofthreemajorcomponents:pre-trainingonexperimental\nbenefitfromthisadvancement.Inthispaper,weaddressthisgapby datasets,incrementaldatacollectioninreal-worlddeployment,and\nrepurposinganLLMforStateofHealth(SOH)estimationthrough test-timeadaptation.Firstly,weutilizeexperimentaldatasets(source\nmodelreprogramming.Additionally,weevaluatethegeneralization domain)totrainapre-trainedmodelforSOHestimation.Wethen\nimprovementsachievedbyadaptinganLLMforcross-batterySOH deploythispre-trainedmodeltoreal-worlddevices,suchasthe\nestimation. BatteryManagementSystem(BMS)ofanelectriccaroramobile\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nphone.TheBMSindividuallycollectsunlabeledtestdataduring\nusage,anduponreceivingasample,weconductatest-timetraining PGLoss\nprocess to adapt the pre-trained model to this different type of\nLIB(targetdomain).Specifically,thetest-timeadaptationprocess\nconsistsoftwosteps:PG-SSLtoconstructanunsupervisedlossfrom\nG\n&\nr Va ad li ue ents\ntheunlabeledsample,andPPAtoadaptthepre-trainedmodeltothis Random Mask\nnewdomaininaparameter-efficientmanner.Afterlearningfrom Model\nincomingdata,theadaptedmodelisreadytomakeaprediction.\nThisprocessoperatescontinuallyuntiltheLIBretires. Maximum capacity (SOH)\n4.2 Physics-GuidedSelf-supervisedLearning Time/Capacity Time/Capacity\n(a) P artial Qdlinear feature (b) Generated complete Qdlinear feature\nUnlabeled Qdlinear 1-RC ECM Equation Physics-Guided Figure3:Guidethepre-trainedmodeltogenerateacomplete\nFeature Loss QdlinearfeaturecurveandsuperviseitwithPhysics-Guided\nğ¶(cid:3043) loss.\nğ‘…\nğ‘…(cid:3043)\nğ¼ ğ‘‚ğ¶ğ‘‰ ğ‘¢(cid:3043) ğ‘¢ Thisimpliesthattheterminalvoltageğ‘¢,currentğ¼ andtempera-\ntureshould,inprinciple,followtheordinarydifferentialequation\n(ODE)functionsrepresentingthebatteryâ€™sphysicalstate,asde-\nFigure2:TransformanarbitraryunlabeledQdlinearfeature scribedinEquation4.Byqueryingthereal-timecurrentandtemper-\nintoaPhysics-Guidedloss. aturevaluefromtheBMS,wecantransformanarbitraryunlabeled\nQdlinearfeatureintoaPGLossinanunsupervisedmanner.\nInthissubsection,weelaborateonhowtofullyexploitthein-\n4.2.2 GenerateacompleteQdlinearfeatureandsuperviseitwith\nherent physical laws of a single LIB sequence to improve TTA\nPGLoss. Inthefiledofbatteryresearch,acompleteQdlinearcurve\nperformancethroughthedesignofphysics-guidedself-supervised\nspanningfromthelowertotheuppervoltagelimitscandescribe\nlearning(PG-SSL).Specifically,wefirstdescribehowtousethe\nLIBâ€™sagingmodeandthereforecantheoreticallyidentifytheaccu-\n1-RCECMequationtotransformanarbitraryunlabeledQdlinear ratestateofhealth[50][7][40]accordingtoitsdefinition2.Inspired\nfeatureintoaphysics-guidedloss(PGLoss).Wewillthenexplain\nbythis,wewanttodesigntheobjectivebyguidingthepre-trained\nhowminimizingthisPGlossfacilitatestheestimationoftheLIB\nmodeltogenerateacompleteQdlinearfeaturefromthegivenpar-\nSOH.\ntialone,whichmeanstogenerateFigure3(b)from(a).\n4.2.1 Theveninâ€™sEquivalentCircuitModel(ECM)ofLIB. Theequiv- Specifically,giventheinput(partial)Qdlinearfeaturecurvexâˆˆ\nalentcircuitmodel(ECM)iswidelyusedbatterymodeltodescribe R1Ã—ğ‘‡ ,weusethepre-trainedmodeltogenerateacompleteonexË† âˆˆ\ntheelectricalbehaviorofthebatteryintermsofvoltages,currents,\nR1Ã—ğ‘‡â€² ,whereğ‘‡â€² >ğ‘‡.Thiscompletevoltage-capacitycurveisthen\nresistancesandcapacitances[48][43].ThefirstorderThevenin supervisedwithPGLoss(asshowninFigure3).Additionally,we\nmodelisthoughttobeaccurateandadequatetomodelthecondi- randomlymaskaportionofxtopromoterepresentationlearning.\ntionofthebattery,andatthesametimesimpleandcomputationally Ingeneral,ourobjectivecanbeformulatedintheformof:\nefficient[49].Theğ‘‚ğ¶ğ‘‰ isrepresentedbyanidealvoltagesource\nofthebattery.ğ‘…accountsfortheinternalohmicresistance.The\nparallelğ‘…ğ¶-branch,comprisingğ‘… ğ‘ƒ andğ¶ ğ‘ƒ,isusedtomodelbattery L PGâˆ’SSL=âˆ¥xË†[0:ğ‘‡]âˆ’xâˆ¥2 ğ¹ +ğœ†âˆ¥ğœƒ 1ğ¼+ğœƒ 2ğ‘¢Ë†+ğ‘¢Ë†(cid:164)=0âˆ¥2 ğ¹ (5)\npolarizationeffect.ğ‘¢andğ¼denotestheterminalvoltageandcurrent\nHere,xË†[0 : ğ‘‡] representstheoverlapbetweenthegenerated\nthatcanbecollectedinuse.BasedonKirchhoffâ€™slaw,theelectrical completevoltage-capacitycurveandthegivenx.ğ‘¢Ë† denotesthe\nbehaviorofthebatterycanbecharacterizedasphysicalequations generatedcompletevoltage-timecurve.Theparametersğœƒ 1andğœƒ\n2\nas: areassociatedwithtemperature,andğ¼ denotesthecurrent;allof\nthesevaluescanbequeriedthroughtheBMSduringdeployment.\nğ‘¢ ğ‘‚ğ¶ğ‘‰ =ğ‘¢ ğ‘…+ğ‘¢ ğ‘ +ğ‘¢ (2) WewillempiricallydemonstratethatthedesignofPhysics-Guided\nPG-SSLishighlysuitableforourSOHestimationtaskandleads\nğ‘…+ğ‘…\nğ‘ ğ¼+ 1 ğ‘¢+ğ‘¢(cid:164)=0 (3) tobetterperformancethansimpleself-prediction,asshowninthe\nğ¶ ğ‘… ğ¶ ğ‘…\nğ‘ ğ‘ ğ‘ ğ‘ experimentalsection.\nWedefineğœƒ 1= ğ¶ğ‘… ğ‘+ğ‘… ğ‘…ğ‘\nğ‘\nandğœƒ 2= ğ¶ğ‘1 ğ‘…ğ‘.Followingrecentworks,the\ncoeffientsarefunctionsoftemperature.Thestatefunctionbecomes: 2ThiscompleteQdlinear(voltage-capacity)curverequiresadditionalcyclesunder\nconstrainedtemperatureandcurrentconditionsinthelaboratoryandinfeasibleat\ndeployment.TheunlabeledQdlinearfeatureweobtainedatuseisalwaysapartialof\nğœƒ 1(ğ‘‡)ğ¼+ğœƒ 2(ğ‘‡)ğ‘¢+ğ‘¢(cid:164)=0 (4) it.\negatloV egatloV\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY\n4.3 PrefixPromptAdaptation\nInthissubsection,weintroducePrefixPromptAdaptation(PPA),a ğ‘“ ğ‘¥,ğ‘” ğ‘¥ =argminL PGâˆ’SSL(ğ‘¥;ğ‘“ 0,ğ‘” 0) (9)\nğ‘“,ğ‘”\nmethodthatreducesthedimensionofthesolutionspaceforeasier\noptimizationoftest-timeadaptation.Specifically,inspiredbythe\ndemonstratedeffectivenessofcontinuouspromptlearninginthe\nğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›=â„ 0â—¦ğ‘“ ğ‘¥(ğ‘¥) (10)\nfieldofdeepmodelfine-tuning[2,17].Weaddasmallnumberof Inparticular,wearguefortheuseofourproposedprefixprompt\nsoftpromptsasaprefixtotheembeddedinputcontextfortest- adaptation in Equation 6 to avoid the substantial computation\ntimeupdating,whilekeepingallothermodelparametersfrozen. broughtbyfine-tuningEquation9atinferencetime.\nInthisway,thedimensionoflearnablemodelparametersshallbe\nsignificantlyreducedandthusenablingthepracticaldeploymentof 4.5 IntegrateanLLMintoBatteryTTT\nTTAonreal-worldedgedevicessuchasaCPUwhenthepre-trained (GPT4Battery)\nmodelislarge(suchasanLLM).Formally,givenatestsampleğ‘‹ ğ‘¡ğ‘’ğ‘ ğ‘¡,\nInthissubsection,weexplorethepotentialofLLMinbatteryse-\nourgoalistofindanoptimalpromptğ’‘âˆ—:\nquencemodeling.InspiredbyrecentstudieswhichutilizeanLLM\nforproteinsequenceprediction[23]andtimeseriesanalysis[18],\nğ’‘âˆ—=argminL PGTPT(F,ğ’‘,ğ‘‹ test) (6)\nweprimarilyemploytheideaofmodelreprogrammingtoeffectively\nğ’‘\nusingthephysics-guidedself-supervisedlossL PGTPTfromEqua- alignthemodalitiesofbatterydataandnaturallanguage,leverag-\ntion4.Here,F denotesthepre-trainedmodel.Wewillalsodemon- ingthereasoningandgeneralizationabilitiesofLLMsforbattery\nstrateexperimentallythatPPAsignificantlyreducesthecostof tasks.\nTTAwhileonlymarginallyreducingitsperformance. Specifically,wefirsttokenizeandmaptheinputQdlinearfeatures\nintoahigh-dimensionalspacewiththesamedimensionalityasthe\n4.4 IntegreteExistingSOHEstimationModels wordspaceofthelanguagemodel.Then,wefusetheinformation\nintoBatteryTTT ofbatterysequencemodalityandlanguagemodalityusingacross-\nattentionlayer.\nInthissubsection,wedemonstratehowtoincorporateexistingSOH However,forawordembeddingspaceofanLLME âˆˆ Rğ‘‰Ã—ğ· ,\nestimationalgorithms[51]intoourtest-timeadaptationframework whereğ‘‰ is the vocabulary size. The vocabulary size can be in-\ntocompletethewholecross-domainSOHestimationprocessde-\nevitablylarge(forexample,GPT2hasaVof50257[29]).Simply\npictedinFigure1.ExistingSOHestimationstudies[32,51]exten-\nleveragingEwillresultinlargeandpotentiallydensereprogram-\nsivelyusestatisticalmodelsandhigh-performanceneuralnetworks\nmingspace,increasingthecomputationcomplexityanddifficulty\nforLIBSOHestimation,suchasGaussianProcessRegression[47],\nofcatchingtherelevantsourcetokens.Following[37]and[18],\nRandomForest[12],MLP[14],RNNs[16]andTransformer[44]for\nwemaintainonlyasmallcollectionoftextprototypesbylinearly\nSOHestimation.Onlydeeplearningmethodscanbeincorporated probingE,denotedasEâ€² âˆˆRğ‘‰â€²Ã—ğ· ,whereğ‘‰â€² â‰ªğ‘‰.Then,wealign\nintotheTTAframework,aswellasothertransferlearningsettings.\nthetokenizedinputpatchesandtextprototypeswithamulti-head\nSpecifically,ourarchitecturefollowsaY-shapeddesign,asde-\ncross-attentionlayer.Specifically:\nscribedin[10,34]:afeatureextractorğ‘“ issimultaneouslyfollowed\nb sty ita uts eel ğ‘“f-s wu ip te hrv this eed enh ce oa dd eğ‘” ra on fd exa ism tia ni gn nta es uk rah lea nd etâ„ w. oH re kr se, fow re Ss Ou Hb-\nZ\nğ‘˜(ğ‘–)\n=attention(Q\nğ‘˜(ğ‘–),K ğ‘˜(ğ‘–),V ğ‘˜(ğ‘–)\n)\nestimation,suchasGRU,LSTM,andTransformer,andğ‘”withthe Q(ğ‘–) K(ğ‘–)âŠ¤\ndecoder.Forthemainregressiontaskheadâ„forSOHestimation,we =softmax(\nğ‘˜ âˆšï¸ğ‘‘ğ‘˜\n)V\nğ‘˜(ğ‘–)\nusealinearprojectionfromthedimensionoftheencoderfeatures ğ‘˜\nto1,whichisprimarilyahistoricartifact[15]. By aggregating each Z(ğ‘–) âˆˆ Rğ‘ƒÃ—ğ‘‘ in every head, we obtain\nDuringpre-training,wefirsttrainğ‘”â—¦ğ‘“ usingthePG-SSLloss Z(ğ‘–) âˆˆ Rğ‘ƒÃ—ğ· . This way,ğ‘˜ the text prototypes can learn cues in\ninEquation5inanunsupervisedmannerwiththefeaturesofthe\nlanguagewhichcanthenrepresenttherelevantlocalpatchinfor-\nsourceLIBdataset.Thenweperformlinearprobingbycombining\nmation.Wewillexperimentallydemonstratetheimprovementin\ntheencoderğ‘“ withthemaintaskheadâ„,keepingğ‘“ frozen.Formally:\ngeneralizabilityachievedbyincorporatingLLMsandtheeffective-\nğ‘› nessofmodelreprogramming.\n1âˆ‘ï¸\nğ‘“ 0,ğ’ˆ0=argm ğ’‡,i ğ’ˆn\nğ‘›\nğ‘–=1L PGâˆ’SSL(ğ‘¥ ğ‘–;ğ’‡,ğ’ˆ) (7)\n5 Experiments\nFollowedby:\nIn this section, we empirically evaluate the proposed approach\nğ‘› on six real-world LIB datasets, including five publicly available\n1âˆ‘ï¸\nâ„ 0=argm â„in\nğ‘›\nL MSE(â„â—¦ğ‘“ 0(ğ‘¥ ğ‘–),ğ‘¦ ğ‘–) (8) datasetsfordailyapplications(withcapacitiesrangingfromafew\nğ‘–=1 Ah) and our own collected 300Ah large LIB dataset for energy\nThesummationisoverthetrainingsetwithğ‘› samples,each storage.Specifically,wefocuson(1)theoverallimprovedgener-\nconsistingofinputğ‘¥ ğ‘– andlabelğ‘¦ ğ‘–.Duringtest-timeadaptation,we alizationperformanceofTTAandthesuperiorperformanceby\noptimizeğ‘”â—¦ğ‘“ beforemakingapredictioneachtimeatestinputğ‘¥ combiningGPT-2andTTA(GPT4Battery),(2)anefficacystudy\narrives.Afteroptimization,wemakeapredictiononğ‘¥asâ„â—¦ğ‘“ ğ‘¥(ğ‘¥), ofthetwoproposeddesigns,PG-SSLandPPA,(3)theablationre-\nformallyas: sultsofGPT4Battery,(4)inferenceefficiencyofinvolvingTTA,and\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nTable2:MainspecificationsofselectedLIBdatasets.\nDataset VoltageRange Samples EstimatedCollect Collector\nElectrodeMaterNiaolminalCapacity Time\nCALCE LCO 1.1(Ah) 2.7-4.2(V) 2807 1397(hour) UniversityofMaryland\nSANYO NMC 1.85(Ah) 3.0-4.1(V) 415 644(hour) RWTHAachenUniversity\nKOKAM LCO/NCO 0.74(Ah) 2.7-4.2(V) 503 8473(hour) UniversityofOxford\nPANASONIC NCA 3.03(Ah) 2.5-4.29(V) 2770 1801(hour) BeijingInstitudeofTechnology\nGOTION LFP 27(Ah) 2.0-3.65(V) 4262 2238(hour) BeijingInstitudeofTechnology\nNHRY LFP 300(Ah) 2.5-3.5(V) 808 1200(hour) Ours\n(5)thescalabilityanddeploymentofourmethodonlargeenergy furtherdetails,wehavemadetherelevantcodeanddataavailableat\nstorageLIBs. thefollowinglink:https://anonymous.4open.science/r/gpt4battery-\n55FC.\n5.1 ExperimentSettings\n5.2 MainPerformance\n5.1.1 Datasetpreparation. Weconductedexperimentsusingfive\npubliclyavailablelithium-ionbattery(LIB)datasetsintendedfor Inthissection,wereportthemainimprovementsincross-domain\ndailycommercialuse,withcapacitiesrangingfrom0.74Ahto27 generalization performance of our proposed TTA methods (e.g.\nAh.Additionally,weutilizedfourofourownLIBdatasetscollected PG-SSLandPPA),onfivecommercialLIBdatasetsfordailyusage.\nforindustrial-levelenergystorage,specificallywithacapacityof Additionally,wedemonstratethatbyleveraginganLLMastheback-\n300Ah,whichwewillalsomakepubliclyavailableforacademic bone,GPT4Batteryachievessuperiorgeneralizationperformance\npurposes.Thesedatasetsencompassavarietyofwidelyusedcath- comparedtoallbaselinemethods.\nodeactivematerials,capacities,andmanufacturers.Asummaryof\nthedatasetstatisticsispresentedinTable2.\nTable3:ImprovedperformanceofTTAonexistingmethods\n5.1.2 Baselines. Wecompareourmethodwithfourtypesofbase-\nandcomparisonwithcurrenttransferlearningmethods.\nlinestodemonstratetheefficacyoftheproposedBatteryTTTframe-\nworkandtheGPT4Batterymodel:(1)Existingnon-transferlearn-\nCALCE SANYO KOKAM PANASONIC GOTION\ningmachinelearning(ML)methodsforLIBStateofHealth(SOH)\nModels MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nestimation,includingGaussianProcessRegression[47],Random\nRandomForest 5.76 4.9 7.87 6.73 5.76 4.88 4.96 4 0.62 0.54\nForest[12],Multi-LayerPerceptron(MLP)[14],RecurrentNeural LightGBM 6.8 5.42 6.31 5.62 6.52 5.31 6.06 4.97 0.55 0.47\nNetworks(RNNs)[16],andTransformer[44]3;(2)Integrationofex- MLP 3.93 3.52 6.1 5.93 13.1 10.9 5.08 4.38 0.54 0.44\nGRU 2.18 2.86 9.17 9.19 3.07 3.44 2.53 3.5 0.74 0.84\nistingmodelsintoourBatteryTTTframework;(3)State-of-the-art LSTM 2.52 2.78 7.58 7.84 3.07 3.93 1.55 2.19 1.65 1.68\nTransformer 2.27 2.57 8.1 8.24 15.3 17.1 1.9 2.35 1.28 1.4\ntransferlearningmethodsforcross-domainLIBSOHestimation,\nsuchaswoDegrad[22]andSSF-WL[46]4;and(4)Integrationof GRU+(TTA) 1.9 2.25 7.8 8.01 2.4 2.74 1.74 2.73 0.67 0.71\nLSTM+ 2.08 2.36 6.71 7.04 2.93 3.67 1.31 2 0.65 0.77\nlarge language models (LLMs) into the BatteryTTT framework Transformer+ 1.83 1.97 7.03 7.18 13.2 14.6 1.32 1.85 0.34 0.43\nGPT-2+ 1.52 1.89 1.35 1.71 7.95 8.01 1.28 1.95 0.38 0.47\n(GPT4Battery). Bert+ 2.01 2.33 1.46 1.82 7.77 8.04 1.52 2.2 0.41 0.52\nWereproducetheresultsofBatteryMLandwoDegradbasedon Llama-7b+ 1.57 1.94 1.35 1.69 8.58 9.66 1.3 2.01 0.4 0.49\ntheprovidedcodeandfollowtheapproachdetailsforSSF-WL.For woDegrad 1.76 1.96 1.21 1.54 1.76 3.01 2.09 2.56 0.45 0.58\nSSF-WL 1.55 1.93 1.08 1.24 6.21 5.1 1.44 2.06 0.51 0.72\nafaircomparison,weadheretothedatapre-processingmethods\noutlinedin[51]anduseQdLinear [1]astheunifiedfeatureset.\nWeadoptthestandardevaluationmetricsofmeanabsoluteerror 5.2.1 ImprovementofGeneralizationAbilityofTTA. Table3shows\n(MAE)androotmeansquarederror(RMSE). themainimprovementsinthegeneralizationperformanceofour\nproposedTTAmethods.WeusetheGOTIONdatasetasthesource\n5.1.3 Implementationdetails. Ourmodelsareimplementedusing\ndatasetforitsextensivelabelcoverageandthenincludeitsown\nPytorchandtrainedonasingle3070TiGPU.WeutilizetheAdamW\ntestsetalongwiththeremainingfourdatasets(CALCE,SANYO,\noptimizer[21]withafixedlearningrateof1e-3forpre-training\nKOKAM and PANASONIC) as the target datasets for generaliz-\nand linear probing until convergence. The mask ratio is set to\nabilitytesting.Overall,weobservethattheTTAmethodshowsa\n30%duringthisphase.TTAisconductedusingstochasticgradient\nsignificantperformanceimprovementofabout50%comparedto\ndescent(SGD)withamomentumof0.9duetoitsconsistencyin\ntheno-TTAmethod.Somemodels(suchasTransformer,GPT2)\nimprovingperformanceondistributionshifts[10].Typically,weset\nequippedwithTTAachieveaperformancethatrivalsorevenex-\nafixedlearningrateof1e-2anditeratefor10steps,asmoresteps\nceedstheperformanceofcurrenttransferlearningmethodsthat\nonlymarginallyimproveperformancebasedonourobservations.\nrequireadditionalaccesstothetargetdatainCALCE,PANASONIC\nThemaskratioduringTTAwillbespecificallyanalyzedlater.For\nandGOTION.Specifically,withinallthemethodsthatuseTTA,\ntheutilizationoflargelanguagemodelsalsohasmadequiteadif-\n3BatteryML[51]providesacomprehensiveplatformsummarizingthesemodels\n4Thesemethodsrelyonsufficientunlabeledtargetdata(UTDs)tooperateeffectively, ferenceingeneralizationperformanceimprovement.Forinstance,\nwhichrequiresimpracticaldatacollectiontime,assummarizedinTable1. GPT2+achievesfirstorsecondrankperformanceontheCALCE,\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY\nGOTIONandPANASONICdatasetscomparedtoallbaselinemeth- full-parameterfine-tuning.Therefore,weoverallreportthebest\nods.LLama+alsoobtainedthebestandsecond-bestperformance performancefortheirfullparameterfine-tuning,whilereporting\nontheSANYOandPANASONICdataset,respectively,comparedto thebestperformanceforLLMsusingPPAinTable3andFigure4.\ntheotherTTAmethods.Wealsoobservethatsomemodelarchitec- TheperformanceofothermodelsusingPPAisthoroughlyablated\nturesdominatetheperformanceoncertaindatasetsoverTTA,e.g., inSec.5.3.1.\ntheRNNfamily(GRU,LSTM)outperformstheTransformerfamily\nonKOKAMasawhole,andtheTTA-enhancedGRU+achieves 5.3 EfficacyAnalysis\nperformancecomparabletothatofwoDegrad.\nInthissection,weanalyzetheeffectivenessandimportantdesign\nchoicesofeachcomponent.Specifically,weevaluatethetwoTTA\ndesigns:Physical-GuidedSelf-supervisedLearning(PG-SSL)and\nGRU+ PrefixPromptAdaptation(PPA).\n108 LSTM+\nTransformer+ 5.3.1 EffectandComputationTrade-offsofPrefixPromptAdapta-\n107 GPT2+ tion. Byadjustingonlyasmallnumberofsoftpromptsprefixtothe\nSSF-WL (2024) inputcontext,thisdesigncansignificantlyreducetheadjustable\n106 woDegrad (2023) parametersto103ordersofmagnituderegardlessofmodelsize,as\n105 GPT2+(PPA) illustratedinFigure5ontheGPT2,TransformerandLSTMmod-\nels.ApplyingPPAtoTransformerandLSTMslightlyimpactthe\n104 accuracy,asshowninFigure5.ApplyingPPAtoLLMshowever,sig-\nnificantlyreducestheMeanAbsoluteError(MAE)from3.89to1.52,\n103\nareductionofapproximately60.9%.Moreover,PPAachievesbetter\n0 generalization performance than traditional parameter-efficient\n1.5 1.6 1.7 1.8 1.9 2.0 2.1 fine-tuningofthepositionalencodingandlayernormalizationlay-\nMean Absolute Error (MAE)\ners [54] ofthe LLM. We attribute this tothe language-agnostic\npatternrecognitionandinferencecapabilitiesacquiredthrough\nFigure4:SuperiorPerformanceofGPT4Battery(onCALCE\npre-trainingontextcorpora[13,24].Guidedbylearnableprefix\ndataset).\nprompts,thesecapabilitiescanalsobegeneralizedtobatteryse-\nquencedata.\n5.2.2 SuperiorPerformanceofGPT4BatteryoverallBaselineMeth-\nods. Inthissection,weprovideadetailedcomparisonofthegener-\nalizabilitygainsandcomputationtrade-offsachievedbyincorpo- 3.50 MAE 108\n3.25 RMSE\nratingLLMs.Figure4demonstratesthatGPT2equippedwithTTA 3.00 P arams 107\n(whichwenameGPT4Battery)obtainsthelowestMAEresultson 2.75 106\n2.50 105\ntheCALCEdatasetthanallothermodelsincludingcurrenttransfer 2.25 104\nlearningmethods(woDegradandSSF-WL)thatrequireadditional 12 .. 70 50 103\nassumptionstoaccessthetargetdataset.GPT4batteryalsogets 1.50 0\nNo TTA TTA-full TTA-peft TTA-PPA\nverycompetitiveresultsinotherdatasetsasdootherlargemodels (a) GPT2 + TTA\n(e.g.LlamaandBert).\nTheresultsofapplyingPrefixPromptAdaptation(PPA)tolarge 3.0 R PM aMA rS aE mE s106 3.0 M R PaMA rE S amE s 106\nlanguagemodels(LLMs),asillustratedinFigure4,areparticularly 2.5 105 2.5 105\nnoteworthy.Conventionalfine-tuningofGPT-2â€™spositionalencod- 104 104\ningandlayernormalizationsignificantlyincreasedthenumberof 2.0 2.0\n103 103\ntrainedparametersattesttime,byafactorof10to100comparedto\nTransformer+andRNNs+,whileonly(relatively)slightlyreducing 1.5 No TTA TTA-full TTA-PPA 102 1.5 No TTA TTA-full TTA-PPA 102\n(b) Transformer + TTA (c) LSTM + TTA\nthemeanabsoluteerror(MAE)from1.83to1.71.Incontrast,our\nPrefixPromptAdaptation(PPA)approachnotonlyreducesthe\nFigure 5: Efficacy and computation trade-offs of Prefix\nnumberofadjustableparametersbynearly10,000times,making\nPromptAdaptation(PPA)ondifferentmodels.\nGPT4Battery ten times more parameter-efficient than the RNN\nseries,butalsofurtherreducestheMAEfrom1.71to1.52,anim-\nprovementofapproximately10.7%.However,weshouldnotethe\nlimitationthatinvolvinganLLMdoesincreasetheinferencetime 5.3.2 EfficacyandParameterSensitivityofPhysical-GuidedSelf-\nevenwithPPA,whichisatrade-offbetweenaccuracyandcompu- supervisedLearning. Inthissection,weanalyzetheeffectiveness\ntationefficiency. ofPG-SSLandconductasensitivityanalysisofanotherimportant\nItisimportanttonotethatwhileapplyingPPAtoregularTrans- parameteraffectingtheself-supervisedloss,themaskratio.We\nformer+andRNNs+alsoreducesthenumberoftrainableparame- performedablationstudiesonlossesusingphysicalconstraintsand\nterstotheorderof103,itcanimpairtheirgeneralizationperfor- puremaskreconstruction.Additionally,weanalyzedtheeffectof\nmance,resultinginaslightlyinferiorperformancecomparedto differentmaskingratesontheMAEreductionofTTAs.Weemploy\nsmaraP\nelbaniarT\n)emiT\ntseT(\nESMR/EAM\nESMR/EAM\nsmaraP\nelbaniarT\nESMR/EAM\nsmaraP\nelbaniarT\nsmaraP\nelbaniarT\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nthetransformermodelandfull-parametertuningforTTA,conduct- Table4:AblatingthedifferentcomponentsofGPT4Battery.\ningexperimentsontheadaptationtotheCALCEandPANASONIC Red:thebest.\ndatasets.\nFigure6showsthattheinclusionofthephysical-guidedloss Method CALCE SANYO KOKAM PANAS. GOTION\nseamlessly enhances the performance in both MAE and RMSE\nGPT4Battery 1.52 1.35 7.95 1.28 0.25\nreduction.Comparatively,theMAEreductionofusingPG-SSLis w/oPG-SSL 2.11 1.12 8.34 3.11 0.265\nslightlymoresignificantintheCALCEdataset(Fig.6(a))thanin w/oMaskedTTA 2.05 0.97 8.44 1.22 0.255\nw/oTTA 2.13 1.34 9.51 3.44 0.297\nthePANASONICdataset(Figure6(b)).Thisisbecausetheformer\nw/omodelreprogramming 4.13 2.78 10.56 4.57 0.31\nisamuchmorenonlineardataset,makingpurereconstructionloss\nlesseffectiveincapturingrepresentations.Wealsoobservefrom\nboth(a)and(b)ofFigure6thatalargermaskratioof0.7to0.9 5.5 TTAEfficiency\npromoteslearningabetterrepresentationwithorwithoutPG-SSL,\nWepresenttheaveragerunningtimeacrossfivedatasetsforthree\nwhileasmallmaskratioof0.5to0.6fails.Thisisconsistentwiththe\nrepresentativemodelsequippedwithTTAandtwocurrenttransfer\nMAE-basedself-supervisedlearningobservationsinthecomputer\nlearningmethods.Ourfocusisprimarilyonmodelinferencetime,\nvisiondomain,wheremaskingahighproportionoftheinputimage,\nasTTAdoesnotsignificantlyimpacttrainingduration.Table5\ne.g.,75%,yieldsanon-trivialandmeaningfulself-supervisorytask.\ndemonstratesthatalthoughTTA-equippedmethodsachievesub-\nInsummary,thecombinationofmaskinginputandPG-SSLresults\nstantiallyhigheraccuracy,theyare10to100timesslowerthan\ninthebestTTAresults.\nexistingmethods.Thistrade-offhighlightsthatourapproachsacri-\nficessomespeedforenhancedaccuracy.However,theresulting102\nspeedlevelremainssufficientformodernBMSrequirements,where\n0.5 wo PG 0.8 wo PG individualresponsetimestypicallyneedtobeunder500msinour\n0.4 PG 0.7 PG\n0.3 0.6 deployedsystem.Moreimportantly,ourframeworkcaneliminate\n0.5\n0.2 0.4 theneedfor644to8,473hoursofdegradationexperimentsfordata\n0.1 00 .. 23 collection,assummarizedinTables1and2.\n0.0 0.1\n-0.1 0.0\n-0.2 -0.1 Table5:EfficiencyAnalysisofTTA.\n0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0\nMask Ratio Mask Ratio\n(a) Effect on CALCE dataset.\nMethod GPT2+ Transformer+ LSTM+ woDegrad SSF-WL\n00 .. 56 w PGo PG 00 .. 45 w PGo PG\nOverallInferenceTime(s) 32.6 6.1 4.3 0.35 0.3 000 ... 234 0000 .... 0123 One MIn of de ere ln pc ae raT mim ete er( sms) 5 71 6. 81 07 19 2.5 87\n0\n16 2.7 85\n0\n6870 7.5 45\n000\n260 0. 14 37\n69\n0.1 -0.1 0.0 -0.2\n-0.1 -- 00 .. 43\n-0.2 -0.5\n-0.3 -0.6 5.6 ScalabilityonLargeEnergyStorageLIBs\n0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0\nMask Ratio Mask Ratio\n(b) Effect on PANASONIC dataset. Inthissection,wereportthescalabilityoftheproposedframework\nthroughdeploymentonourbatterymanagementsystem(BMS)for\nFigure6:EffectonMAE/RMSEReductionofPhysical-Guided industry-levelenergystorageLIBs(300Ah).\nSelf-supervisedLearningandSensitivityofMaskRatio.\n5.6.1 LIBDataset. WecollecttheNHRYdatasetbyperforming\ndegradationexperimentson8industry-levelenergystorageLIBs\n(300Ah)from4brands(Ningde,Haichen,Ruipu,Yiwei).Testenvi-\nronmentandprocedurescomplywiththeChinaStandardBMSfor\nEnergyStorageGB/T34131-2023.Atotalof800degradationcycles\n5.4 AblationStudyofGPT4Battery\nwereexperiencedrangingovertwomonthsfromDecember2022\nInthissection,weconsiderthebestperformingGPT2+TTAasa\ntoJanuary2023.\nself-containedmodel(GPT4Battery)andprovideablationstudyon\ntheeffectsofdifferentcomponentsordesignchoices.Ourresultsin 5.6.2 OnlinePerformance. ForfourdifferentbrandsofLIBswith\nTable4indicatethatablatingeithermodelreprogrammingorany thesamecapacity,wemixtwoofthebrandsasthesourcedomain\notherdesignsintest-timeadaptationhurtsthegeneralizationper- andevaluatetheonlineperformanceoftheLIBsfromtheremaining\nformanceonunseenLIBs.Intheabsenceofphysicalguidance,we twobrands,resultinginatotaloftwocombinationsforthecross-\nobserveanotableaverageperformancedegradationof55.1%,which batterysetting.WereporttheMAE/RMSEmetricalongwiththe\nbecomesmorepronounced(i.e.,exceeding70%)whendiscarding inferencetime(ms)usingrepresentativebaselines.\ntheTTAstrategycompletely.Theactofmodelreprogrammingalso\n6 Conclusion\nstandsasapivotalelementincross-modalityalignment,enabling\ntheLLMtounderstandtheLIBâ€™ssequencedatawiththehelpof Inthispaper,weproposeanoveltest-timeadaptation(TTA)frame-\ntextprototypes.Ablationofreprogrammingresultsleadstoover workforcross-domainLIBstateofhealth(SOH)estimation.This\n20%degradationonaverageperformance. one sample adaptation setting allows the model to continually\nnoitcudeR\nEAM\nnoitcudeR\nEAM\nnoitcudeR\nESMR\nnoitcudeR\nESMR\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY\nTable6:OnlineperformanceonLargeEnergyStorageLIBs. References\n[1] PeterMAttia,KristenASeverson,andJeremyDWitmer.2021. Statistical\nCombination1 Combination2 learningforaccurateandinterpretablebatterylifetimeprediction.JournalofThe\nElectrochemicalSociety168,9(2021),090547.\nMAE RMSE PerInfer.Time MAE RMSE PerInfer.Time [2] HyojinBahng,AliJahanian,SwamiSankaranarayanan,andPhillipIsola.2022.\nLSTM 4.33 4.59 4.23 2.44 2.69 3.23 Exploring visual prompts for adapting large-scale models. arXiv preprint\nTransformer 5.62 5.89 7.61 3.48 3.78 6.32 arXiv:2203.17274(2022).\n[3] ChristophBirkl.2017.Oxfordbatterydegradationdataset1.(2017).\nLSTM+ 0.775 0.788 54.53 0.55 0.62 44.85\n[4] LeoBreiman.2001.Randomforests.Machinelearning45(2001),5â€“32.\nTransformer+ 1.25 1.38 88.57 1.02 1.23 77.83\n[5] DefuCao,FurongJia,SercanOArik,TomasPfister,YixiangZheng,WenYe,and\nGPT4Battery 0.734 0.761 103.47 0.314 0.418 105.68\nYanLiu.2023.Tempo:Prompt-basedgenerativepre-trainedtransformerfortime\nseriesforecasting.arXivpreprintarXiv:2310.04948(2023).\n[6] Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. 2024.\nadapttothetargetdomainwitheverysingleunlabeledtestsample, LLM4TS:AligningPre-TrainedLLMsasData-EfficientTime-SeriesForecast-\ners. arXiv:2308.08469[cs.LG]\nperfectlyaligningwiththenatureofbatterydegradationfeatures,\n[7] ChengChen,RuiXiong,RuixinYang,andHailongLi.2022.Anoveldata-driven\nwhichcanonlybeobtainedonebyoneduringthelongagingpro- methodforminingbatteryopen-circuitvoltagecharacterization.GreenEnergy\ncess.Thissettingalsoaddressesthelimitationsofexistingtransfer andIntelligentTransportation1,1(2022),100001.\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:\nlearningmethods,whichassumeadditionalaccesstothetargetLIB\nPre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv\ndataset,therebysavingmonthstoyearsoflaborindatacollection. preprintarXiv:1810.04805(2018).\nByintroducingGPT4Battery,wehopethisworkwillinspirethe [9] NoeliaFerruz,SteffenSchmidt,andBirteHÃ¶cker.2022. ProtGPT2isadeep\nunsupervisedlanguagemodelforproteindesign.Naturecommunications13,1\nresearchcommunitytofullyleverageadvancedAItechniques,such (2022),4348.\nasTest-TimeAdaptation(TTA)andlargelanguagemodels(LLMs), [10] YossiGandelsman,YuSun,XinleiChen,andAlexeiEfros.2022.Test-timetraining\nwithmaskedautoencoders.AdvancesinNeuralInformationProcessingSystems\nformoreAI4Scienceproblems,therebysavingenormoustimeand\n35(2022),29374â€“29385.\nacceleratingscientificdiscovery. [11] Konstantinos N. Genikomsakis, Nikolaos-Fivos Galatoulas, and Christos S.\nIoakimidis.2021.Towardsthedevelopmentofahotel-basede-bikerentalservice:\nResultsfromastatedpreferencesurveyandtechno-economicanalysis.Energy\n215(2021),119052. https://doi.org/10.1016/j.energy.2020.119052\n[12] PierreGeurts,DamienErnst,andLouisWehenkel.2006.Extremelyrandomized\ntrees.Machinelearning63(2006),3â€“42.\n[13] NateGruver,MarcFinzi,ShikaiQiu,andAndrewGWilson.2024.Largelanguage\nmodelsarezero-shottimeseriesforecasters. AdvancesinNeuralInformation\nProcessingSystems36(2024).\n[14] SimonHaykin.1998.Neuralnetworks:acomprehensivefoundation.PrenticeHall\nPTR.\n[15] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollÃ¡r,andRossGirshick.\n2022. Maskedautoencodersarescalablevisionlearners.InProceedingsofthe\nIEEE/CVFconferenceoncomputervisionandpatternrecognition.16000â€“16009.\n[16] SeppHochreiterandJÃ¼rgenSchmidhuber.1997.Longshort-termmemory.Neural\ncomputation9,8(1997),1735â€“1780.\n[17] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,\nBharathHariharan,andSer-NamLim.2022. Visualprompttuning.InEuro-\npeanConferenceonComputerVision.Springer,709â€“727.\n[18] MingJin,ShiyuWang,LintaoMa,ZhixuanChu,JamesYZhang,XiaomingShi,\nPin-YuChen,YuxuanLiang,Yuan-FangLi,ShiruiPan,etal.2023. Time-llm:\nTimeseriesforecastingbyreprogramminglargelanguagemodels.arXivpreprint\narXiv:2310.01728(2023).\n[19] MingJin,YifanZhang,WeiChen,KexinZhang,YuxuanLiang,BinYang,Jindong\nWang,ShiruiPan,andQingsongWen.2024.PositionPaper:WhatCanLargeLan-\nguageModelsTellUsaboutTimeSeriesAnalysis.arXivpreprintarXiv:2402.02713\n(2024).\n[20] JianLiang,RanHe,andTieniuTan.2024.Acomprehensivesurveyontest-time\nadaptationunderdistributionshifts. InternationalJournalofComputerVision\n(2024),1â€“34.\n[21] IlyaLoshchilovandFrankHutter.2017.Decoupledweightdecayregularization.\narXivpreprintarXiv:1711.05101(2017).\n[22] JiahuanLu,RuiXiong,JinpengTian,ChenxuWang,andFengchunSun.2023.\nDeeplearningtoestimatelithium-ionbatterystateofhealthwithoutadditional\ndegradationexperiments.NatureCommunications14,1(2023),2760.\n[23] LiuzhenghaoLv,ZongyingLin,HaoLi,YuyangLiu,JiaxiCui,CalvinYu-Chian\nChen,LiYuan,andYonghongTian.2024.Prollama:Aproteinlargelanguage\nmodelformulti-taskproteinlanguageprocessing.arXivpreprintarXiv:2402.16445\n(2024).\n[24] SuvirMirchandani,FeiXia,PeteFlorence,BrianIchter,DannyDriess,Montser-\nratGonzalezArenas,KanishkaRao,DorsaSadigh,andAndyZeng.2023.Large\nLanguageModelsasGeneralPatternMachines. arXiv:2307.04721[cs.AI]\n[25] Man-FaiNg,JinZhao,QingyuYan,GarethJConduit,andZhiWeiSeh.2020.\nPredictingthestateofchargeandhealthofbatteriesusingdata-drivenmachine\nlearning.NatureMachineIntelligence2,3(2020),161â€“170.\n[26] ShuaichengNiu,JiaxiangWu,YifanZhang,YaofoChen,ShijianZheng,Peilin\nZhao,andMingkuiTan.2022. Efficienttest-timemodeladaptationwithout\nforgetting.InInternationalconferenceonmachinelearning.PMLR,16888â€“16905.\n[27] ZijiePan,YushanJiang,SahilGarg,AndersonSchneider,YuriyNevmyvaka,and\nDongjinSong.2024.IP-LLM:SemanticSpaceInformedPromptLearningwith\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nLLMforTimeSeriesForecasting.arXivpreprintarXiv:2403.05798(2024). [40] JinpengTian,RuiXiong,WeixiangShen,JiahuanLu,andXiao-GuangYang.2021.\n[28] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.2018. Deepneuralnetworkbatterychargingcurvepredictionusing30pointscollected\nImprovinglanguageunderstandingbygenerativepre-training.(2018). in10min.Joule5,6(2021),1521â€“1534.\n[29] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever, [41] JinpengTian,RuiXiong,WeixiangShen,JiahuanLu,andXiao-GuangYang.2021.\netal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog Deepneuralnetworkbatterychargingcurvepredictionusing30pointscollected\n1,8(2019),9. in10min.Joule5,6(2021),1521â€“1534. https://doi.org/10.1016/j.joule.2021.05.012\n[30] CarlEdwardRasmussenandHannesNickisch.2010. Gaussianprocessesfor [42] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne\nmachinelearning(GPML)toolbox.TheJournalofMachineLearningResearch11 Lachaux,TimothÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,Faisal\n(2010),3011â€“3015. Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv\n[31] AlexanderRives,JoshuaMeier,TomSercu,SiddharthGoyal,ZemingLin,Ja- preprintarXiv:2302.13971(2023).\nsonLiu,DemiGuo,MyleOtt,C.LawrenceZitnick,JerryMa,andRobFergus. [43] Manh-KienTran,ManojMathew,StefanJanhunen,SatyamPanchal,Kaamran\n2021.Biologicalstructureandfunctionemergefromscalingunsupervisedlearn- Raahemifar,RoydonFraser,andMichaelFowler.2021.Acomprehensiveequiva-\ningto250millionproteinsequences. ProceedingsoftheNationalAcademyof lentcircuitmodelforlithium-ionbatteries,incorporatingtheeffectsofstateof\nSciences118,15(2021),e2016239118. https://doi.org/10.1073/pnas.2016239118 health,stateofcharge,andtemperatureonmodelparameters.JournalofEnergy\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118 Storage43(2021),103252.\n[32] DariusRoman,SaurabhSaxena,ValentinRobu,MichaelPecht,andDavidFlynn. [44] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\n2021.Machinelearningpipelineforbatterystate-of-healthestimation.Nature AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall\nMachineIntelligence3,5(2021),447â€“456. youneed.Advancesinneuralinformationprocessingsystems30(2017).\n[33] KristenASeverson,PeterMAttia,NormanJin,NicholasPerkins,BenbenJiang, [45] DequanWang,EvanShelhamer,ShaotengLiu,BrunoOlshausen,andTrevor\nZiYang,MichaelHChen,MuratahanAykol,PatrickKHerring,Dimitrios Darrell.2020.Tent:Fullytest-timeadaptationbyentropyminimization.arXiv\nFraggedakis,etal.2019. Data-drivenpredictionofbatterycyclelifebefore preprintarXiv:2006.10726(2020).\ncapacitydegradation.NatureEnergy4,5(2019),383â€“391. [46] TianyuWang,ZhongjingMa,SuliZou,ZhanChen,andPengWang.2024.\n[34] ManliShu,WeiliNie,De-AnHuang,ZhidingYu,TomGoldstein,AnimaAnand- Lithium-ionbatterystate-of-healthestimation:Aself-supervisedframework\nkumar,andChaoweiXiao.2022.Test-timeprompttuningforzero-shotgener- incorporatingweaklabels.AppliedEnergy355(2024),122332.\nalizationinvision-languagemodels.AdvancesinNeuralInformationProcessing [47] ChristopherKIWilliamsandCarlEdwardRasmussen.2006.Gaussianprocesses\nSystems35(2022),14274â€“14289. formachinelearning.Vol.2.MITpressCambridge,MA.\n[35] XingShu,JiangweiShen,GuangLi,YuanjianZhang,ZhengChen,andYonggang [48] RuiXiong,LinlinLi,andJinpengTian.2018.Towardsasmarterbatterymanage-\nLiu.2021.AFlexibleState-of-HealthPredictionSchemeforLithium-IonBattery mentsystem:Acriticalreviewonbatterystateofhealthmonitoringmethods.\nPacksWithLongShort-TermMemoryNetworkandTransferLearning. IEEE JournalofPowerSources405(2018),18â€“29.\nTransactionsonTransportationElectrification7,4(2021),2238â€“2248. https: [49] ZhaoyiXu,YanjieGuo,andJosephHomerSaleh.2022. Aphysics-informed\n//doi.org/10.1109/TTE.2021.3074638 dynamicdeepautoencoderforaccuratestate-of-healthpredictionoflithium-ion\n[36] DimitrisSpathisandFahimKawsar.2023.Thefirststepisthehardest:Pitfalls battery.NeuralComputingandApplications34,18(2022),15997â€“16017.\nofRepresentingandTokenizingTemporalDataforLargeLanguageModels. [50] SijiaYang,CaipingZhang,JiuchunJiang,WeigeZhang,YangGao,andLinjing\narXiv:2309.06236[cs.LG] Zhang.2021.Avoltagereconstructionmodelbasedonpartialchargingcurve\n[37] ChenxiSun,YaliangLi,HongyanLi,andShendaHong.2023.TEST:Textproto- forstate-of-healthestimationoflithium-ionbatteries.JournalofEnergyStorage\ntypealignedembeddingtoactivateLLMâ€™sabilityfortimeseries.arXivpreprint 35(2021),102271.\narXiv:2308.08241(2023). [51] HanZhang,XiaofanGui,ShunZheng,ZihengLu,YuqiLi,andJiangBian.2024.\n[38] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritz BatteryML:AnOpen-sourcePlatformforMachineLearningonBatteryDegrada-\nHardt.2020.Test-timetrainingwithself-supervisionforgeneralizationunder tion.InTheTwelfthInternationalConferenceonLearningRepresentations.\ndistributionshifts.InInternationalconferenceonmachinelearning.PMLR,9229â€“ [52] MarvinZhang,SergeyLevine,andChelseaFinn.2022.Memo:Testtimerobust-\n9248. nessviaadaptationandaugmentation.Advancesinneuralinformationprocessing\n[39] YandanTanandGuangcaiZhao.2020.TransferLearningWithLongShort-Term systems35(2022),38629â€“38642.\nMemoryNetworkforState-of-HealthPredictionofLithium-IonBatteries.IEEE [53] AurickZhouandSergeyLevine.2021.Bayesianadaptationforcovariateshift.\nTransactionsonIndustrialElectronics67,10(2020),8723â€“8731. https://doi.org/10. Advancesinneuralinformationprocessingsystems34(2021),914â€“927.\n1109/TIE.2019.2946551 [54] TianZhou,PeisongNiu,XueWang,LiangSun,andRongJin.2023. OneFits\nAll:PowerGeneralTimeSeriesAnalysisbyPretrainedLM. arXivpreprint\narXiv:2302.11939(2023).\nAdaptingAmidstDegradation:CrossDomainLi-ionBatteryHealthEstimationviaPhysics-GuidedTest-TimeTraininCgonferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY\nA Appendix:PseudoCode B Appendix:DegradationConditionsof\nSelectedDatasets\nAlgorithm1PretrainingandTest-TimeAdaptationofBatteryTTT\nTable7providesthespecificdegradationconditionsoftheselected\n1: Input:SourceLIBdataset(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–)forğ‘– =1,...,ğ‘› lithium-ionbatteries(LIBs)usedinthiswork,includingdetailed\n2: Output:Predictionfortestinputğ‘¥ cellinformationandtheprotocolsforcharginganddischarging.\n3: Pre-trainingPhase:\n4:\nforğ‘– =1toğ‘›do Received20February2007;revised12March2009;accepted5June2009\n5: ComputeL PGâˆ’SSL(ğ‘¥ ğ‘–;ğ’‡,ğ’ˆ)usingEquation5\n6: endfor\n7: Train:ğ‘“ 0,ğ‘” 0=argmin ğ’‡,ğ’ˆ ğ‘›1 (cid:205)ğ‘› ğ‘–=1L PGâˆ’SSL(ğ‘¥ ğ‘–;ğ’‡,ğ’ˆ)(Equation\n1)\n8: LinearProbing:\n9:\nforğ‘– =1toğ‘›do\n10: ComputeL MSE(â„â—¦ğ‘“ 0(ğ‘¥ ğ‘–),ğ‘¦ ğ‘–)\n11: endfor\n12: Train:â„ 0=argminâ„ ğ‘›1 (cid:205)ğ‘› ğ‘–=1L MSE(â„â—¦ğ‘“ 0(ğ‘¥ ğ‘–),ğ‘¦ ğ‘–)(Equation2)\n13: Test-TimeAdaptationPhase:\n14: foreachtestinputğ‘¥ do\n15: Optimizeğ‘“ ğ‘¥,ğ‘” ğ‘¥ =argminğ‘“,ğ‘”L PGâˆ’SSL(ğ‘¥;ğ‘“ 0,ğ‘” 0)(Equation9)\n16:\nComputeğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›=â„ 0â—¦ğ‘“ ğ‘¥(ğ‘¥)(Equation4)\n17: endfor\n18: PrefixPromptAdaptation:\n19: Useproposedprefixpromptadaptationtoavoidcomputation\nduringinference\nConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Trovatoetal.\nTable7:DegradationConditionsoftheSelectedLIBDatasets\nDataset CellInformation TestConditions\nCALCE 3 cells termed â€¢Chargedataconstantcurrentrateof0.5Cuntilthevoltagereached4.2Vandthen4.2Vwas\nCS2-35, CS2- sustaineduntilthechargingcurrentdroppedtobelow0.05A.\n36,CS2-37 â€¢Dischargedataconstantcurrentrateof1C.\nâ€¢Ambienttemperatureisnotmentioned.\nSANYO 48commercial â€¢Chargedataconstantcurrentrateof1Cuntilthevoltagereached4.1Vandthen4.1Vwassustained\nUR18650E untilthechargingcurrentdroppedtobelow0.04A.\ncylindrical â€¢Dischargedataconstantcurrentrateof1C.\ncells â€¢25Â°C.\nKOKAM 8 KOKAM â€¢Chargedataconstantcurrentrateof1Cuntilthevoltagereached4.2V.\nSLPB533459H â€¢Dischargedataconstantcurrentrateof1C.\n4pouchcells â€¢40Â°C.\nPANASONIC 3 commercial â€¢Chargedataconstantcurrentrateof0.3Cuntilthevoltagereached4.2Vandthen4.2Vwas\nNCR18650BD sustaineduntilthechargingcurrentdroppedtobelow0.03A.\ncylindrical â€¢Dischargedataconstantcurrentrateof2C.\ncells â€¢20Â°C.\nGOTION 3 commercial Chargedataconstantcurrentrateof1Cuntilthevoltagereached3.65Vandthen3.65Vwassustained\nIFP20100140A untilthechargingcurrentdroppedtobelow1.35A.\ncells â€¢Dischargedataconstantcurrentrateof1C.\nâ€¢45Â°C.",
    "pdf_filename": "Adapting_Amidst_Degradation_Cross_Domain_Li-ion_Battery_Health_Estimation_via_Physics-Guided_Test-Ti.pdf"
}