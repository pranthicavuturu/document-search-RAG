{
    "title": "Combining Induction and Transduction for Abstract Reasoning",
    "abstract": "Wen-Ding Li*1 Keya Hu*2 Carter Larsen1 Yuqing Wu1 Simon Alford1 Caleb Woo1 Spencer M. Dunn1 Hao Tang1 Michelangelo Naim3 Dat Nguyen3 Wei-Long Zheng2 Zenna Tavares†3 Yewen Pu†4 Kevin Ellis†1 1Cornell 2Shanghai Jiao Tong University 3Basis 4Autodesk *co-leads †co-advising correspondence: {wl678,kellis}@cornell.edu, hu_keya@sjtu.edu.cn When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predict new test outputs, e.g. using a neural network? We study this question on ARC-AGI by training neural models for induction (inferring latent functions) and transduction (directly predicting the test output for a given test input). We train on synthetically generated variations of Python programs that solve ARC-AGI train- ing tasks. We find inductive and transductive models solve different kinds of test problems, despite having the same training problems and sharing the same neural architecture: Inductive program synthesis excels at precise computations, and at composing multiple concepts, while transduction succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level performance on ARC-AGI. 1 INTRODUCTION Robust generalization from few examples remains one of the most important ways in which human ing: Given just a few training input-outputs xtrain, ytrain, together with a test input xtest, the idea is to predict the corresponding test output ytest using reasoning strategies such as analogical reasoning, chain-of-thought, inductive program synthesis, or transductive prediction (Thoms et al., 2023; Wang et al., 2024; Witt et al., 2023; Lee et al., 2024; Tang et al., 2024a; Hocquette & Cropper, 2024; Butt few-shot learning benchmark that tests the ability to rapidly learn a diverse range of new skills, and apply them to new situations. Each ARC-AGI task is presented as input-outputs over colored grids, but can engage concepts such as occlusion, pathfinding, collision, symmetry, gravity, bouncing, counting, etc., making ARC-AGI essentially a composite of many reasoning datasets, and one of the more interesting unsolved benchmarks that stresses broad-coverage few-shot learning (Figure 1). object contact gravity symmetry, occlusion jumping bouncing mazes pathfinding growth task typically has 2-5 input-output examples. Here we show just one input-output example per task. Here we study neural methods for induction and transduction, using few-shot learning problems from ARC-AGI as our testbed. Induction means first finding a function f where f(xtrain) ≈ytrain, and then predicting ytest = f(xtest). Transduction instead outputs ytest without explicit construction 1 arXiv:2411.02272v3  [cs.LG]  19 Nov 2024",
    "body": "Preprint.\nCOMBINING INDUCTION AND TRANSDUCTION FOR\nABSTRACT REASONING\nWen-Ding Li*1 Keya Hu*2 Carter Larsen1 Yuqing Wu1 Simon Alford1 Caleb Woo1\nSpencer M. Dunn1 Hao Tang1 Michelangelo Naim3 Dat Nguyen3 Wei-Long Zheng2\nZenna Tavares†3 Yewen Pu†4 Kevin Ellis†1\n1Cornell\n2Shanghai Jiao Tong University\n3Basis\n4Autodesk\n*co-leads †co-advising\ncorrespondence: {wl678,kellis}@cornell.edu, hu_keya@sjtu.edu.cn\nABSTRACT\nWhen learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to directly\npredict new test outputs, e.g. using a neural network? We study this question on\nARC-AGI by training neural models for induction (inferring latent functions) and\ntransduction (directly predicting the test output for a given test input). We train on\nsynthetically generated variations of Python programs that solve ARC-AGI train-\ning tasks. We find inductive and transductive models solve different kinds of test\nproblems, despite having the same training problems and sharing the same neural\narchitecture: Inductive program synthesis excels at precise computations, and at\ncomposing multiple concepts, while transduction succeeds on fuzzier perceptual\nconcepts. Ensembling them approaches human-level performance on ARC-AGI.\n1\nINTRODUCTION\nRobust generalization from few examples remains one of the most important ways in which human\nintelligence surpasses AI. Much recent work views this generalization as a form of abstract reason-\ning: Given just a few training input-outputs xtrain, ytrain, together with a test input xtest, the idea is\nto predict the corresponding test output ytest using reasoning strategies such as analogical reasoning,\nchain-of-thought, inductive program synthesis, or transductive prediction (Thoms et al., 2023; Wang\net al., 2024; Witt et al., 2023; Lee et al., 2024; Tang et al., 2024a; Hocquette & Cropper, 2024; Butt\net al., 2024). The Abstraction and Reasoning Corpus (Chollet (2019), henceforth ARC-AGI) is a\nfew-shot learning benchmark that tests the ability to rapidly learn a diverse range of new skills, and\napply them to new situations. Each ARC-AGI task is presented as input-outputs over colored grids,\nbut can engage concepts such as occlusion, pathfinding, collision, symmetry, gravity, bouncing,\ncounting, etc., making ARC-AGI essentially a composite of many reasoning datasets, and one of the\nmore interesting unsolved benchmarks that stresses broad-coverage few-shot learning (Figure 1).\nobject contact\ngravity\nsymmetry, occlusion\njumping\nbouncing\nmazes\npathfinding\ngrowth\nFigure 1: Few-shot learning tasks from the Abstraction and Reasoning Corpus (ARC-AGI). Each\ntask typically has 2-5 input-output examples. Here we show just one input-output example per task.\nHere we study neural methods for induction and transduction, using few-shot learning problems\nfrom ARC-AGI as our testbed. Induction means first finding a function f where f(xtrain) ≈ytrain,\nand then predicting ytest = f(xtest). Transduction instead outputs ytest without explicit construction\n1\narXiv:2411.02272v3  [cs.LG]  19 Nov 2024\n\nPreprint.\nof an intermediate function f. Intuitively, induction captures the notion that a learner should first\nexplain the training data, then use that explanation to make predictions. Inductive learners can per-\nform better by spending more time optimizing or searching for better explanations, using the training\nexamples xtrain, ytrain to score candidate functions. Transduction instead captures the intuition that\nthe training examples themselves should play a direct role in generating new predictions, and that\nsuccessful prediction need not require an explicit explanation. See Figure 2.\nf(•)\nf(xtest )\ninduction\ntransduction\nxtrain \nytrain \nxtest\nytest\nxtrai\nn\nytrai\nn\nxtest\nytest\nxtrain \nytrain \nxtest\nFigure 2: Induction generates an intermediate function f to explain training input-outputs. Trans-\nduction directly predicts the test output, for example using a neural network.\nWe train neural networks for both induction and transduction by generating a large corpus of syn-\nthetic problems. We discover that neural models for induction and transduction are strongly comple-\nmentary. We believe this is surprising: Although any pair of models would generally solve somewhat\ndifferent problems, usually this can be attributed to different priors, data, or architecture. Instead,\nwe find that, even controlling for priors, data, and architecture, most problems solved by induc-\ntion were not solved by transduction, and vice versa. Moreover, induction and transduction can be\ntrivially ensembled by using induction to generate candidate functions f until either a satisfactory\nfunction is found (e.g. f(xtrain) = ytrain) or until a test-time compute budget is reached, at which\npoint, transduction kicks in as a fallback: That they are complementary has practical implications.\nOur study is tightly linked to program synthesis. We represent functions f as Python code, mean-\ning that induction synthesizes programs. We train transduction models on LLM-produced Python\nscripts, meaning that transduction is trained on the input-outputs of symbolic code. Although pro-\ngram learning has long been a popular vision of how general AI could work (Solomonoff, 1964;\nSchmidhuber, 2004; Hutter, 2004), the dominant theory has always been one of explicit code gen-\neration (induction), rather than implicitly teaching neural networks to imitate code (transduction).\nOur work puts this assumption to the test.\nTesting these neural methods requires a large dataset of function-learning problems, which is chal-\nlenging to generate because not only must we make novel functions, but we must also make good\ninputs to those functions. Consider the range of transformations in Figure 1: What counts as a good\ninput for one function is unlikely to work for another. To address this challenge, our data generator\nfirst produces a deterministic Python function for f, and then a probabilistic program for sampling\ninputs to f, finally executing those programs to produce input-outputs. This helps generate func-\ntion inputs that are appropriate for the underlying transformation, and constrains xtrain, ytest to be\nexplainable by a deterministic mapping.\nWe contribute the following:\n1. A study finding that neural models for induction and transduction are strongly complementary,\neven when trained on the same problems. This contradicts seminal neural program synthesis\nwork (Devlin et al. (2017), which found induction superior), and contradicts the findings of the\nleading ARC team (Cole et al. (2024), which advocates transduction with test-time training).\n2. An automated data generation methodology that starts with 100-160 program solutions for ARC\ntraining tasks, and expands them to make 400k new problems paired with Python solutions.\n3. A study of how these methods scale. We find performance saturates quickly when increasing\nmanually-labelled data, but scales with compute, both at training and testing time.\n4. Analysis of families of problems solved by each approach, and how they compare to humans.\n2\n\nPreprint.\n2\nNEURAL MODELS FOR INDUCTION AND TRANSDUCTION\nWe consider few-shot supervised learning problems where the learner is trained to map members\nof an input space X to output space Y. For K-shot learning, we receive K training input-outputs\n(xtrain, ytrain) ∈X K × YK, together with a single test input from xtest ∈X, and predict ytest ∈Y.\nOur neural models for K-shot learning are meta-learned (Mishra et al., 2017, inter alia.) using\nmeta-learning data further annotated with a ground-truth function f : X →Y, which supervises the\ninduction model. Below we define the training and use of these models.\nDefinition: Neural networks for induction and transduction. A neural network for transduction\nis a function t that maps (xtrain, ytrain, xtest) to a distribution over ytest, and which has learnable pa-\nrameters θ. In other words, tθ : X K ×YK ×X →∆(Y), where the notation ∆(S) means the set of\ndistributions over S. We can also write this as a conditional distribution, tθ(ytest|xtrain, ytrain, xtest).\nA neural network for induction is a function i that maps (xtrain, ytrain, xtest) to a distribution over\nfunctions f that map X to Y, with learnable parameters θ. In other words, iθ : X K × YK × X →\n∆(X →Y), which we can write as a conditional distribution iθ(f|xtrain, ytrain, xtest).\nTraining induction and transduction. Both types of models are trained via meta-learning. We\nassume a meta-learning dataset D of few-shot learning problems, each equipped with a ground-\ntruth function f such that f(x) = y for every x, y in (xtrain, ytrain) and (xtest, ytest). Inductive and\ntransductive models are meta-trained to minimize the following losses:\nTRANSDUCTION LOSS = E(xtrain,ytrain,xtest,ytest,f)∼D [−log tθ(ytest|xtrain, ytrain, xtest)]\n(1)\nINDUCTION LOSS = E(xtrain,ytrain,xtest,ytest,f)∼D [−log iθ(f|xtrain, ytrain, xtest)]\n(2)\nTesting induction and transduction.\nAfter meta-learning the models encounter a test-time few-\nshot learning task (xtrain, ytrain, xtest).\nTransductive models predict their most likely output for\nytest (approximated via beam search). Inductive models sample a test-time budget of B functions\nf1 · · · fB, which are filtered by (xtrain, ytrain), and finally used to predict ytest = f(xtest). Writing\nˆytest for the predicted test output:\nTRANSDUCTION: ˆytest = arg max\ny∈Y\ntθ(y|xtrain, ytrain, xtest)\n(3)\nINDUCTION: ˆytest ∼Uniform (F)\n(4)\nwhere F = {fb(xtest) : for 1 ≤b ≤B if fb(xtrain) = ytrain}\nfb ∼iθ(f|xtrain, ytrain, xtest)\nCombining induction and transduction. Induction allows checking candidate hypotheses against\nthe training examples. Therefore, we know when induction has found a plausible solution–but\nsometimes it fails to find any solution. Transduction has the opposite property: We can’t check\nif its predictions match the training examples, but it always offers a candidate answer. Therefore\nwe ensemble by attempting induction first, then transduction if none of the candidate hypotheses\nexplained the examples:\nENSEMBLE: ˆytest ∼Uniform (F) if F ̸= ∅\nˆytest = arg max\ny∈Y\ntθ(y|xtrain, ytrain, xtest) if F = ∅\n(5)\nInstantiating the framework for ARC-AGI.\nEvery input from X and output from Y is a 2D grid\nranging from 1–30 pixels per side, with each pixel containing one of ten colors. Because ARC-\nAGI tasks are highly diverse yet typically have an abstract program-like structure, we represent\nthe underlying function f as Python code, which is computationally universal, and so possible in\nprinciple of solving any ARC-AGI task. Therefore the induction model must generate Python code,\nso we initialize our models with Llama3.1-8B-instruct (Dubey et al., 2024) because it was pretrained\non source code.1 We encode 2D colored grids as strings using 1 token per pixel, and use newlines to\ndelimit rows (Appendix B.1). We then meta-learn by further fine-tuning Llama3.1-8B-instruct for\ninduction or transduction using a synthetically-generated corpus of problems, described next.\n1Our preliminary experiments suggested Llama3.1-8B-instruct was better than Mistral-7B-v0.3, Qwen2-\n7B-Instruct, and deepseek-coder-6.7b-instruct\n3\n\nPreprint.\n3\nGENERATING DATASETS FOR INDUCTION AND TRANSDUCTION\nGenerating ARC-style tasks is challenging because of the diversity of concepts that can occur in\nARC-AGI. It is also challenging because we need to generate not just a function, and also inputs\nthat serve as good examples for that function.\nAt a high level, our dataset grows out of 100 manually-written Python programs, each of which both\nsolves a given task (function f), and also randomly generates new input grids. We call these 100\nmanually-written programs seeds. Each seed is commented with natural language describing the\ncore intuitions behind the problem. We then use a large language model to mutate and recombine\nthe seeds, producing many thousands of programs (Figure 3).\nmanual problem solving\nsynthetic augmentation\nremix\nsolve\n100 seed problems\n100 seed solutions\n100k seed problems with solutions\nconcepts: cropping\ndescription:\nIn the input you will see a single colored \nshape, around 4x6 in size, floating in a \n12x12 grid of black.\nTo make the output, crop the background \nout of the image - so the output grid has \nthe same dimensions as the shape.\nlanguage\ncode\nexamples\nconcepts:\ndescription:\nruntime check\nlanguage\ncode\nexamples\nFigure 3: Synthetic data generation pipeline, starting with human-written programs (seeds).\nThe structure of seeds.\nEach seed consists of three parts:\n1. A natural language description of its specific ARC task—including how to solve that\ntask—represented as a Python comment at the top of the seed.\n2. A Python function transform_grid corresponding to the function f in the manuscript,\nwhich maps each input grid of a specific ARC task to its corresponding output grid.\n3. A Python function generate_input, which takes no arguments, and which randomly\ngenerates new inputs to f (new inputs to transform_grid).\nPrior knowledge.\nThe seeds impart a prior upon the system by demonstrating good programs for\nsolving training tasks. We further codified much of this prior into a Python library containing code\nthat we found useful across many seeds, such as subroutines for generating random sprites, detecting\nsymmetries, or extracting objects (Appendix A.2). Synthetic problems can use that same library.\nHowever, this prior knowledge is different from previous Domain Specific Languages for ARC-\nAGI (Butt et al., 2024; Wind, 2020; Ainooson et al., 2023). Domain Specific Languages restrict the\nclass of allowed programs by only allowing stereotyped combinations of domain-specific primitives.\nWe still allow arbitrary Python code, which helps cover the long tail of diverse tasks.\nRemixing the seeds.\nTo generate a larger synthetic dataset we “remix” the seeds using LLMs.\nEach new synthetic ARC problem is generated by a three stage pipeline (Figure 11):\n1. A new natural language description is sampled by prompting an LLM with seed natural lan-\nguage descriptions, effectively using in-context learning to recombine and mutate elements\nof different problems, in the spirit of self-instruct (Wang et al., 2023).\n2. Code is generated for that new description via Retrieval Augmented Generation\n(RAG: Lewis et al. (2020)). Our RAG pipeline retrieves seeds with similar descriptions,\nand prompts an LLM to generate code for the new description, given the retrieved seeds.\n3. The newly created generate_input is executed to make inputs, which are passed to\ntransform_grid to produce input-output grids.\n4\n\nPreprint.\nFigure 4 illustrates example problems generated by our pipeline, with further examples visualized at\nthis link. Unless otherwise mentioned, we create synthetic datasets with GPT4o-mini and ada-002.\nconcepts: \ncolor blending, \nradial expansion\nconcepts: \nshape detection, contour \nextraction, bounding box\nconcepts: \npattern detection, copying, \npositioning\nFigure 4: Example synthetic ARC problems generated by our pipeline. Concepts are generated in a\ncomment near the top of the Python script as part of the natural language description of the seed.\n4\nEMPIRICAL STUDY OF INDUCTION AND TRANSDUCTION\nWe train inductive and transductive models with the goal of understanding (1) how the methods\ncompare; (2) how performance scales with train-time effort; and (3) how performance scales with\ntest-time compute (for induction only, as it allows drawing more samples at test time to improve\nperformance). We report performance on the 400-problem public validation split of ARC-AGI,\nwhich is harder than the training split. The systems described in this section learn from a 100-\nproblem subset of the training split, specifically problems for which we created seeds.\nInduction and Transduction are strongly complementary.\nDespite training on the exact same\nproblems, inductive and transductive models solve different tasks, and neither approach is dramat-\nically more effective than the other. And although these methods have a similar overall solve rate,\nmost problems solved by induction are not solved by transduction, and vice versa (Figure 5A).\nAn alternative explanation is that induction and transduction are not actually complementary, but\ninstead that, having trained two neural networks with different random initializations, they simply\nsolved different problems due to randomness at train or test time. To test this alternative explanation,\nwe trained many models with different random initializations. We find that the problems solved by\ninduction/transduction are surprisingly stable across these different runs (Figure 5B). In other words,\nsome problems are friendlier to induction, and others friendlier to transduction (Figure 6).\n0.0\n1.0\ntransduction\ntransduction\ninduction\ninduction\np<.004\n= 1 problem solved\n(A) complementary methods \n(B) finding is stable across random seeds, and (C) statistically significant\nFigure 5: (A) Induction and transduction solve different problems, where solve means predicting the\nright output given 2 tries. Venn diagram for models trained on 100k synthetic problems generated\nusing gpt4o-mini. (B) Training many models with different random seeds, and then measuring the\ncorrelation between solved tasks by different models. Solved tasks strongly correlates with other\nmodels of the same class but not the other class. (C) Statistical significance test evaluating the null\nhypothesis that correlation is independent of whether a model is inductive/transductive.\n5\n\nPreprint.\nsolved only by induction\nsolved only by transduction\nsolved by both\nFigure 6: Example tasks solved by induction/transduction/both. See also Appendix C.\nPerformance scales with dataset size, but quickly saturates with increasing number of seeds.\nWe trained models while systematically varying the number of human-created seeds we use, and\nvarying the amount of synthetic data generated from those seeds (Figure 7). Performance improves\nwith increasing training data for fine-tuning (increasing synthetic data), but saturates for increas-\ning quantity of human-created seeds. We conjecture that this saturation occurs because each seed\nserves to introduce a few core concepts, and that after enough seeds, essentially all of the important\nconcepts have been demonstrated. This suggests that, beyond a critical threshold number of seeds,\nthe method can scale with increasing amounts of compute without demanding further human data\nlabeling. Looking beyond ARC-AGI, this means that our methodology could probably be applied\nto other few-shot function-learning problems using a modest amount of manual data labeling.\n20\n40\n60\n80\n100\nnum synthetic problems (10^3)\n0.050\n0.075\n0.100\n0.125\n0.150\nValidation Accuracy\nincreasing synthetic data, 100 seeds\nInduction\nTransduction\n25\n50\n100\nnum seeds\nincreasing seeds, 40k synthetic problems\nInduction\nTransduction\nFigure 7: Increased manual human effort (# seeds) does not significantly increase performance, but\nincreasing compute spent generating synthetic data increases performance of the final model.\n6\n\nPreprint.\nInduction performance scales with test-time compute.\nWe vary the test-time sampling budget\nfor induction, finding an almost monotonic increase in solve rate (Figure 8, left). In principle,\ndrawing additional samples runs the risk of discovering solutions that are “false-positives,” meaning\nthey fit the training examples without correctly predicting the test output. In practice, about 9% of\nsamples that fit the training examples are false-positives. Figure 8 (right) shows that about half of\nthis 9% corresponds to problems where the majority of the probability mass is still placed on the\ncorrect output, meaning that a simple majority vote scheme would squash any false positives (e.g.\nclustering in AlphaCode Li et al. (2022)). Appendix D shows example false positives.\n0\n500\n1000\n1500\n2000\nnumber of samples\n0%\n5%\n10%\n15%\n20%\nSolve Rate\nARC Val Accuracy\nSample + Oracle\nSample + Filter\nCodeIt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0\n20\n40\n60\n# validation problems\nFigure 8: Left: Sample+Oracle assumes an oracle that selects one of the sampled programs. It\nupper-bounds the accuracy of randomly selecting one program consistent with the training exam-\nples (Sample+Filter). Induction model trained with 100k gpt4-description-gpt4omini-codegen data.\nCodeIt (Butt et al., 2024) is a recent neural program induction model for ARC-AGI. Right: His-\ntogram of false positive rate. Of the problems that have false positives, about half have a false\npositive rate less than 0.5, meaning that most (filtered) samples predict the right test output.\nStronger LLMs make better synthetic data, and induction is more sensitive to data quality.\nTo save costs, the previous results all used GPT4o-mini to generate synthetic data. To understand\nthe value of stronger models we generated 100k synthetic problems using GPT4 to generate natural\nlanguage problem descriptions (but GPT4o-mini still generated the code). The richer and more\ndiverse synthetic problems elicited from GPT4 significantly improved performance, but primarily\nfor induction, while transduction was less sensitive to data quality (Table 1).\nTable 1: Val Acc: % validation tasks that are correctly solved in 2 tries.\nSystem\nVal Acc.\nFinetuning Data\nEnsemble\n26.50%\nGPT-4 for generating descriptions, GPT-4o-mini for code\nInduction, 2048 samples\n18.78%\nTransduction, beam size 20\n15.25%\nEnsemble\n19.50%\nGPT-4o-mini for generating descriptions and code\nInduction, 2048 samples\n11.07%\nTransduction, beam size 20\n13.50%\n5\nSCALING OUR METHOD\nMotivated by our findings so far, we scaled up our method by producing two larger datasets:\nARC-Heavy: 200k problems from 160 seeds. The purpose of ARC-Heavy is to scale our method\nin an easily reproducible way, while also filling any gaps in its mastery of the training split. We first\nran models from Section 4 on the training split to identify 60 problems that they still struggled with,\nfor which we produced 60 new seeds, giving 160 seeds in total. From those seeds we produced 200k\nsynthetic problems, with GPT4 generating natural language descriptions and GPT4o-mini generat-\ning the corresponding Python code.\n7\n\nPreprint.\nARC-Potpourri: 400k problems from heterogeneous sources. The purpose of ARC-Potpourri is\nto assemble the biggest dataset that we could, even if it comes from a messy mixture of sources.\nStarting with ARC-Heavy we added all synthetic data from Section 4. We further added 100k\ntransduction-only training examples from ReARC (Hodel, 2024).\nTest-time improvements. We improve transduction with test-time training (abbreviated TTT; Sun\net al. (2020)) and a reranking scheme that augments each problem and predicts the most likely output\nunder multiple augmentations (Appendix E- F). We expand our sampling budget to 20k programs.\nWe call our resulting systems BARC. Table 2 shows the performance of various BARC models.\nBoth transduction and induction are effective, with induction solving slightly more problems, un-\ntil adding test-time training/reranking, after which transduction does slightly better. An ensem-\nble scores 56.75%, surpassing previously published methods. Releasing this work later allowed\nAkyürek et al. (2024) to improve that score to 61.9% via better test time training of our transduc-\ntion model, while using our induction model as-is. But absent our models—including the program\nsynthesizer—they instead score lower, suggesting that sophisticated test time training does not fully\nsubstitute for program synthesis. Our best model performs nearly as well as the average human\n(56.75% vs. 60.2%) but much worse than the best humans (98%). Model outputs visualized here.\nTable 2: % validation tasks correctly solved in 2 tries. Human results from LeGris et al. (2024).\nSystem\nVal Acc.\nFair comparison?\nARC-Heavy: BARC models\nInduction, 10k samples, majority vote\n30.50%\n—\nTransduction (no TTT)\n19.25%\n—\nEnsemble (no TTT)\n37.50%\n—\nTransduction (TTT)\n29.75%\n—\nEnsemble (TTT)\n43.25%\n—\nARC-Potpourri: BARC models\nInduction, 20k samples, majority vote\n38.00%\n—\nTransduction (no TTT, no reranking )\n29.125%\n—\nTransduction (reranking, no TTT)\n35.25%\n—\nTransduction (TTT, no reranking )\n39.25%\n—\nTransduction (TTT + reranking)\n43.00%\n—\nEnsemble (TTT + reranking)\n56.75%\n—\nCodeIt (Butt et al., 2024)\n15%\nYes, only trains on training set\nClaude-3.5 / Greenblatt (2024)\n21% / 42%\nYes, but closed LLMs at test time\nWind (2020)\n39%\nNo, designed by looking at val set\nAkyürek et al. (2024), w/o our models\n47.1%\nYes\n+ensembled with & using both our models\n61.9%\nNo, builds on our models\nAvg/Best Human\n60.2% / 97.8%\nYes\nScaling down our method. Our flagship model is too expensive to run on the private test set hosted\nby Kaggle. We scale down by omitting test-time training, only sampling 336 programs, and reducing\nthe transduction beam size to 3. This scores 19% on Kaggle and 36.5% on validation. Table 3 shows\nthat program synthesis is less effective given this smaller search budget. Given the large-compute\neffectiveness of program synthesis, this suggests a strong payoff for smarter neural program search.\nTable 3: Smaller version of our model evaluated on the private test and public validation splits\nPrivate Test Set\nPublic Validation Set\nTransduction (no TTT, beam size 3)\n18%\n32.25%\nInduction, 384 samples\n4%\n14%\nEnsemble\n19%\n36.5%\n8\n\nPreprint.\n6\nWHICH PROBLEMS ARE EASIER FOR THE MODELS, AND FOR HUMANS?\n0.2\n0.4\n0.6\n0.8\nhuman % solved\n0.2\n0.4\n0.6\n0.8\nmodel % solved\nhardest 20%\neasiest 20%\ntransduction+TTT\ninduction\nensemble\nFigure 9: Human vs model performance across 5\ndifficulty levels. The easiest difficulty level con-\ntains problems in the top 20% of human accuracy,\nand the hardest difficulty level contains the 20%\nof problems with the lowest human accuracy.\nDo problems that challenge humans also\nchallenge the model, and vice-versa?\nWe\nsort ARC-AGI validation problems into 5\nequally-sized\ndifficulty\nclasses\nusing\ndata\nfrom LeGris et al. (2024).\nFigure 9 illus-\ntrates a peculiar relationship between human\nand model accuracy: All models surpass hu-\nman performance on the hardest problems, but\nunderperform on the easiest problems. Because\nour models train on simple Python programs,\nthis suggests some problems are simple in code\nand learnable by transformers, but very hard\nfor people—and conversely that people pos-\nsess priors allowing effortless solution of prob-\nlems beyond what our Python program gener-\nator produces. For problems of typical diffi-\nculty, the model roughly tracks human perfor-\nmance, and across all difficulty levels, transduc-\ntion and induction serve complementary roles,\neven when augmented with test time training.\nWhich concepts are easier for the models?\nWe test on ConceptARC (Moskvichev et al., 2023),\nan alternative ARC-style test-set which classifies its tasks into “concept groups” each exemplifying\na single isolated high-level concept such as “sameness” or “above vs below.” We use models trained\non ARC-Potpourri, finding that specific concept categories are easier for induction or transduction\n(Figure 10). We find an intuitive division of labor between the two approaches: Concept groups\nsuch as counting are best solved with symbolic code, while transduction better handles perceptual\nprocesses such as judging whether a shape is more horizontal or more vertical, or more top/bottom.\nConceptARC reveals another dimension along which transduction and induction differ: Because\nConceptARC illustrates one concept per problem, there is no need to compose many concepts to-\ngether. Therefore the induction model, which is uniquely equipped for symbolic composition, loses\na key advantage. Transduction has more limited composition capabilities but can instantiate indi-\nvidual concepts in flexible subsymbolic ways, which could explain why it excels on ConceptARC.\ninduction\ntransduction\npass@1\nFigure 10: ConceptARC accuracy by concept group. Concept groups sorted left-to-right by ratio of\ninductive to transductive performance. IceCube is the original ARC Kaggle winner (Wind, 2020).\nWe report pass@3 because Moskvichev et al. (2023) report accuracy given 3 attempts.\n9\n\nPreprint.\n7\nRELATED WORK\nARC-AGI was originally designed to challenge conventional deep learning and spur progress on al-\nternative paradigms (Chollet, 2019). The first wave of successful approaches used discrete program\nsearch over domain-specific programming languages, including the original Kaggle winner (Wind,\n2020). These symbolic approaches held their own against GPT-4 (Wang et al., 2024), but have\nrecently been surpassed by transductive architectures using test-time training (Cole et al., 2024),\nand by LLM-guided program generation (Greenblatt, 2024). ARC-AGI has so far resisted con-\nventional neural and symbolic approaches, but is solvable for adult humans, and to some extent,\nchildren (LeGris et al., 2024; Opielka et al., 2024).\nCode generation via LLMs is done in many recent works (Li et al., 2022; Gao et al., 2023; Chen\net al., 2021; Austin et al., 2021). We most directly build on Li & Ellis (2024) and Greenblatt (2024).\nThe former fine-tunes LLMs for inductive program synthesis using LLM-generated variations of\nhuman-written programs. While there are many technical differences, a key factor is that we gen-\nerate function inputs by synthesizing an input_generator function, rather than have an LLM\ndirectly generate possible inputs. This matters because an LLM alone could not generate complex,\nprecisely-correct inputs such as ARC grids. This potentially makes our work applicable to other\nfew-shot generalization problems with complex input-spaces such as webpages, robot planning,\netc. Greenblatt (2024) samples many Python programs from GPT4o: Comparable to our induction\nmodel, but instead of fine-tuning, it uses prompting. Fine-tuning forced us to create a dataset of new\nproblems, which created the opportunity for exploring transductive models.\nClassic work in neural program synthesis has previously compared induction and transduction (Ro-\nbustFill: Devlin et al. (2017)). We explore here a richer space of functions, reaching a qualitatively\ndifferent conclusion than RobustFill: Instead of finding transduction inferior to induction, we find\nthem complementary. More broadly, the transductive-inductive divide lies near the heart of super-\nvised learning. Inductive approaches, such as linear regression, first construct a function f where\nf(xtrain) ≈ytrain, and then predict ytest = f(xtest). Transductive approaches, such as Support Vector\nMachines and In-Context Learning, instead output their predictions by performing direct compar-\nisons with the training data. We use the same neural network architecture and dataset to perform\nboth tasks, allowing a controlled comparison between these paradigms.\nDatasets for ARC-AGI. ReARC (Hodel, 2024) is a dataset of handwritten programs that solve\nall ARC-AGI training tasks, and which generates new inputs for them. ReARC is implemented\nin a domain-specific language, and lacks natural language annotations, making it difficult to remix\nwith LLMs. Other works annotate ARC-AGI using either natural language (Acquaviva et al., 2022)\nor Python programs (Huang et al., 2024), which could potentially serve as seed programs for our\nwork. Acquaviva et al. (2022) inspired our natural-language descriptions and Huang et al. (2024)\ninfluenced our seed program format. Our new seeds were a better fit for this approach because they\nencode shared priors in a Python library (Appendix A.2), and have an explicit input_generator\ndescribing a precisely-structured infinite space of valid inputs.\n8\nDISCUSSION\nWhat we learn about robust sample-efficient generalization.\nNeither explicit symbolic hy-\npotheses nor implicit neural representations suffice to solve all problems: each has their own do-\nmain of applicability, and simply ensembling models specialized in each does not cover all cases.\nEngineering a more clever neural program search, or training transductive predictors on more data,\nis unlikely to be fruitful. Instead we need representations irreducible to a purely neural or symbolic\nform, which intertwine inductive and transductive reasoning. One way of implementing this idea is\nto do program synthesis within a language whose atomic primitives are non-symbolic, and to pre-\ntrain those primitives to encapsulate the basic atoms of core knowledge. While work has taken steps\nin related directions (Reed & De Freitas, 2015; Alet et al., 2018; Tang & Ellis, 2023; Li et al., 2024),\nhow to engineer and scale this idea remains open.\nTo what extent is this methodology applicable beyond ARC-AGI?\nFew-shot function learning\nis a very flexible framework, but our particular method is most applicable when the target general-\nization can be described in symbolic code. As an immediately tangible example, web scraping and\n10\n\nPreprint.\nother forms of data-munging could fit within our framework. As a more ambitious goal, symbolic\ncode is an especially good medium for defining precise models of how the world works. This is\ntrue both within the natural sciences (Schmidt & Lipson, 2009) and also within AI, with examples\nsuch as robotic policies (Liang et al., 2023), planners (Wong et al., 2023), and world models more\nbroadly (Das et al., 2023; Tang et al., 2024b; Evans et al., 2021; Liang et al., 2024). These are not\nthe kinds of programs that occur often in LLM pretraining data—so merely prompting is unlikely\nto perform well—but it is nonetheless feasible to curate around 100 seeds demonstrating what the\nsystem should learn.\nTheoretically, induction and transduction should not be so complementary.\nEquivalences be-\ntween induction and transduction are well-know, such as the ‘kernel trick’ which allows translating\nparametric function fitting into a transductive problem. Our metalearning models, given infinite\nmetatraining data, should similarly converge because transformers are universal function approxi-\nmators. That there remains a difference is interesting precisely because it deviates from what one\nwould expect theoretically.\nAre we cheating by training on 400k synthetic problems?\nThe spirit of ARC-AGI is to gener-\nalize from few examples. Yet we fine-tune on many examples. In our view, the true training data is\n160 seeds, not the 400k ‘remixes,’ which are instead analogous to ‘dream data’ in amortized infer-\nence or wake-sleep (Le et al., 2017; Ritchie et al., 2016; Hinton et al., 1995). In other words, our\nsystem inputs 160 annotated solutions to training set problems, and does up-front computation to\nconvert that data into a neural network capable of solving new problems. From that perspective, it\nis a sample efficient way of learning to solve ARC-AGI—although it is not compute efficient.\nImpact on ARC-AGI efforts.\nReleasing our code and data helped Akyürek et al. (2024) achieve\nthe first open-source system that performs at the level of an average human. The 2nd place ARC-\nAGI ’24 team (Franzen et al., 2024) also benefited from our data. We hope others will continue\nbuilding on our work. We also intend our findings to encourage research on discrete program search\nas an alternative to the test-time training currently dominating in the community, and have geared\nour experiments toward showing the value in this other pathway.\nFrom domain-specific languages to domain-specific libraries.\nMany works that perform pro-\ngram search rely on carefully tuned domain-specific languages instead of using a general purpose\nlanguage such as Python (Butt et al., 2024; Wind, 2020; Alford et al., 2022; Ainooson et al., 2023).\nHowever, we believe general-purpose programming languages can give much broader coverage, and\nthat attempts to engineer restricted languages inevitably sacrifice regions of program-space which\ncould plausibly occur in open-ended learning domains such as ARC. Instead we advocate here for\ndomain-specific libraries, which equip a general-purpose language with extra priors, but do not\nrestrict it to using only those priors.\nHow to represent input-output mappings.\nOur seeds include: (1) a grid transformation program,\n(2) an input generator, and (3) natural language descriptions for both (1) and (2). Practically, this\nrepresentation allows us to sample consistent input-output example pairs for training, while the\nnatural language descriptions help LLMs to remix seeds into novel problems. This further captures\na latent natural language description of both inputs and outputs, from which the function and its\npreimage are derived.\nNext steps suggested by biological intelligence & wake-sleep.\nOur work has a straightforward\nanalogy to dual-process models in psychology, which categorizes human thinking according to\nwhether it relies on fast nonverbal intuitions or deliberative conscious thought (Smith & DeCoster,\n2000; Kahneman, 2011). Although preliminary, our results could suggest that this partitioning is\nactually normative, and emerges from properties of the problems being solved, not properties of the\nsolver itself. Human thinking is however more sophisticated in how it combines these modes: Fast\nintuitions can be further reprocessed by deliberative symbolic processing, which can trigger further\nintuitions, and so on. Our method has no analogous way of interleaving these two strategies, but\nmore deeply integrating induction and transduction is a natural next step.\nOur approach can also be seen as a form of wake-sleep or dream learning (Hinton et al., 1995;\nFosse et al., 2003; Rasch & Born, 2013) where samples from a generative model train an inference\n11\n\nPreprint.\nnetwork. Here the generative model is a prompt with the seeds, and the inference network is our\nfine-tuned models. Drawing the analogy to wake-sleep suggests two directions. First, we could learn\nfrom recently-solved test problems (during ‘waking’) by generating fresh synthetic data using those\nproblems as seeds (during ‘dreaming/sleeping’). Second, we could also implement a form of ab-\nstraction learning that automatically expands our custom ARC library (Appendix A.2), or adds new\nneural primitives to that library, in the spirit of library learning and modular metalearning (Bowers\net al., 2023; Ellis et al., 2021; Alet et al., 2018).\nLimitations.\nOur system does not grow more competent at few-shot learning by solving new prob-\nlems: Instead, it bootstraps from manually encoded knowledge in the seeds, which is transformed\ninto a few-shot learner via an LLM training/inference pipeline. A more compelling approach would\nbe to have the system discover for itself the knowledge that we compiled for it within the seeds, for\ninstance by practicing on training tasks, without supervising on ground truth solutions.\nOur work is only evaluated on ARC-AGI. However, ARC-AGI is designed to contain many concepts\nand problems embedded within it, so can be viewed as an open-ended composite of different learning\nproblems. Owing to this diversity, it is also notoriously challenging, and has resisted solution despite\na series of high-profile competitions. We therefore believe that although evaluating on multiple\nbenchmarks is desirable, ARC-AGI is an appropriate benchmark to use as the centerpiece of an\nexperimental evaluation.\nCode & Data Availability. Our code, data, and model weights are freely available at https:\n//github.com/xu3kev/BARC. Interactive visualizations of our dataset and model outputs are\navailable at this link.\nAuthor Contributions. Neural network experiments were engineered by Wen-Ding Li and Keya\nHu. Data generation was engineered by Carter Larsen, Wen-Ding Li, Keya Hu, and Kevin Ellis.\nKevin Ellis, Yewen Pu, Zenna Tavares, Wei-Long Zheng, and Hao Tang provided high-level advisory\nguidance. Zenna Tavares, Michelangelo Naim, Dat Nguyen, and Keya Hu analyzed the transduction\nmodel. Seeds were written by Keya Hu, Kevin Ellis, Carter Larsen, Yuqing Wu, Simon Alford,\nCaleb Woo, Spencer M. Dunn, and Yewen Pu. The paper was written by Yewen Pu, Keya Hu,\nZenna Tavares, Wen-Ding Li, and Kevin Ellis.\nAcknowledgements. We are grateful for advice from Robert Hawkins regarding the statistical anal-\nysis in Figure 5C and for discussions with Weinan Sun about biological learning. This work was\npartly supported by an NSF CAREER award to K.E.\nREFERENCES\nSam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle\nEcanow, Maxwell Nye, Michael Henry Tessler, and Joshua B. Tenenbaum. Communicating nat-\nural programs to humans and machines. In Thirty-sixth Conference on Neural Information Pro-\ncessing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/\nforum?id=OxFoLTKDcNm.\nJames Ainooson, Deepayan Sanyal, Joel P. Michelson, Yuan Yang, and Maithilee Kunda.\nA\nneurodiversity-inspired solver for the abstraction & reasoning corpus (arc) using visual imagery\nand program synthesis, 2023. URL https://arxiv.org/abs/2302.09425.\nEkin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising\neffectiveness of test-time training for abstract reasoning, 2024. Preprint.\nFerran Alet, Tomás Lozano-Pérez, and Leslie P Kaelbling. Modular meta-learning. In Conference\non robot learning, pp. 856–868. PMLR, 2018.\nSimon Alford, Anshula Gandhi, Akshay Rangamani, Andrzej Banburski, Tony Wang, Sylee Dan-\ndekar, John Chin, Tomaso Poggio, and Peter Chin. Neural-guided, bidirectional program search\nfor abstraction and reasoning. In Complex Networks & Their Applications X: Volume 1, Pro-\nceedings of the Tenth International Conference on Complex Networks and Their Applications\nCOMPLEX NETWORKS 2021 10, pp. 657–668. Springer, 2022.\n12\n\nPreprint.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nMatthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin\nEllis, and Armando Solar-Lezama. Top-down synthesis for library learning. POPL, 2023. doi:\n10.1145/3571234. URL https://doi.org/10.1145/3571234.\nNatasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David W Zhang, Michaël Deffer-\nrard, and Taco Cohen. Codeit: Self-improving language models with prioritized hindsight replay.\nIn International Conference on Machine Learning, 2024.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-\nGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\nFrançois Chollet. On the measure of intelligence, 2019.\nJack Cole, Mohamed Osman, Michael Hodel, Keith Duggar, and Tim Scarfe. Machine learning\nstreet talk, June 2024.\nRia Das, Joshua B Tenenbaum, Armando Solar-Lezama, and Zenna Tavares. Combining functional\nand automata synthesis to discover causal reactive programs. Proceedings of the ACM on Pro-\ngramming Languages, 7(POPL):1628–1658, 2023.\nJacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and\nPushmeet Kohli. Robustfill: Neural program learning under noisy i/o. ICML, 2017.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\nArun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere,\nBethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\nMarra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\nLivshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\nSmith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Ander-\nson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah\nKorevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy\nFu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak,\nJongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Al-\nwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini,\nKrithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo,\nLukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Man-\nnat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova,\nMelanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,\nNarjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur\nÇelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhar-\ngava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\n13\n\nPreprint.\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRoberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sum-\nbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,\nSanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang,\nSharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende,\nSoumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney\nBorodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom,\nTobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petro-\nvic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang,\nXiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur,\nYasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre\nCoudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha\nJain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay\nMenon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda\nKallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew\nGu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita\nSaraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh\nYazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De\nPaola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Bran-\ndon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina\nMejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai,\nChris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li,\nDanny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Ar-\ncaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco\nCaggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella\nSchwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory\nSizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang,\nHanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Gold-\nman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman,\nJames Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer\nChan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie\nWang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun\nZand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal\nChawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva,\nLee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian\nKhabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson,\nMatthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Ke-\nneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel\nSamvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mo-\nhammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy-\nata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong,\nNing Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli,\nParkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux,\nPiotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao,\nRachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li,\nRebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott,\nSai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Sa-\ntadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind-\nsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang\nZhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen\nChen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho,\nSunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser,\nTamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Tim-\nothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,\nVinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu\n14\n\nPreprint.\nMihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Con-\nstable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu,\nXinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,\nYoungjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef\nRosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024.\nURL https://arxiv.org/abs/2407.21783.\nKevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke He-\nwitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrap-\nping inductive program synthesis with wake-sleep library learning.\nIn PLDI, 2021.\ndoi:\n10.1145/3453483.3454080. URL https://doi.org/10.1145/3453483.3454080.\nRichard Evans, Matko Bošnjak, Lars Buesing, Kevin Ellis, David Pfau, Pushmeet Kohli, and Marek\nSergot. Making sense of raw input. Artificial Intelligence, 299:103521, 2021. ISSN 0004-3702.\ndoi: https://doi.org/10.1016/j.artint.2021.103521.\nURL https://www.sciencedirect.\ncom/science/article/pii/S0004370221000722.\nMagdalena J Fosse, Roar Fosse, J Allan Hobson, and Robert J Stickgold. Dreaming and episodic\nmemory: a functional dissociation? Journal of cognitive neuroscience, 15(1):1–9, 2003.\nDaniel Franzen, Jan Disselhoff, and David Hartmann. The llm architect: Solving the arc challenge is\na matter of perspective, 2024. URL https://github.com/da-fr/arc-prize-2024/\nblob/main/the_architects.pdf. Preprint.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. In International Conference on Machine\nLearning, pp. 10764–10799. PMLR, 2023.\nRyan Greenblatt. Draw more samples. \"https://redwoodresearch.substack.com/p/\ngetting-50-sota-on-arc-agi-with-gpt\", 2024. Accuracy from ARCPrize Leader-\nboard.\nGeoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The \"wake-sleep\" algorithm\nfor unsupervised neural networks. Science, 268(5214):1158–1161, 1995.\nCéline Hocquette and Andrew Cropper. Relational decomposition for program synthesis. arXiv\npreprint arXiv:2408.12212, 2024.\nMichael Hodel. Rearc. https://github.com/michaelhodel/re-arc, 2024. [Online\nGitHub repository].\nDi Huang, Ziyuan Nan, Xing Hu, Pengwei Jin, Shaohui Peng, Yuanbo Wen, Rui Zhang, Zidong Du,\nQi Guo, Yewen Pu, et al. Anpl: towards natural programming with interactive decomposition.\nAdvances in Neural Information Processing Systems, 36, 2024.\nMarcus Hutter. Universal artificial intelligence: Sequential decisions based on algorithmic proba-\nbility. Springer Science & Business Media, 2004.\nDaniel Kahneman. Thinking, fast and slow. macmillan, 2011.\nTuan Anh Le, Atilim Gunes Baydin, and Frank Wood. Inference Compilation and Universal Proba-\nbilistic Programming. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International\nConference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learn-\ning Research, pp. 1338–1348. PMLR, 20–22 Apr 2017. URL https://proceedings.mlr.\npress/v54/le17a.html.\nSeungpil Lee, Woochang Sim, Donghyeon Shin, Wongyu Seo, Jiwon Park, Seokki Lee, Sanha\nHwang, Sejin Kim, and Sundong Kim. Reasoning abilities of large language models: In-depth\nanalysis on the abstraction and reasoning corpus, 2024. URL https://arxiv.org/abs/\n2403.11793.\nSolim LeGris, Wai Keen Vong, Brenden M. Lake, and Todd M. Gureckis. H-arc: A robust estimate\nof human performance on the abstraction and reasoning corpus benchmark, 2024. URL https:\n//arxiv.org/abs/2409.01374.\n15\n\nPreprint.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-\ntion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:\n9459–9474, 2020.\nChengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey\nLevine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-\naugmented code emulator. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller,\nNuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st Interna-\ntional Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Re-\nsearch, pp. 28259–28277. PMLR, 21–27 Jul 2024.\nURL https://proceedings.mlr.\npress/v235/li24ar.html.\nWen-Ding Li and Kevin Ellis. Is programming by example solved by llms?, 2024.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science, 378(6624):1092–1097, 2022.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 9493–9500. IEEE, 2023.\nYichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver,\nJoão F. Henriques, and Kevin Ellis. Visualpredicator: Learning abstract world models with neuro-\nsymbolic predicates for robot planning, 2024.\nURL https://arxiv.org/abs/2410.\n23156.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. arXiv preprint arXiv:1707.03141, 2017.\nArsenii Kirillovich Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptARC\nbenchmark: Evaluating understanding and generalization in the ARC domain. Transactions on\nMachine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/\nforum?id=8ykyGbtt2q.\nGustaw Opielka, Hannes Rosenbusch, Veerle Vijverberg, and Claire E. Stevenson. Do large lan-\nguage models solve arc visual analogies like people do?, 2024. URL https://arxiv.org/\nabs/2403.09734.\nBjörn Rasch and Jan Born. About sleep’s role in memory. Physiological reviews, 93(2):681–766,\n2013.\nScott Reed and Nando De Freitas.\nNeural programmer-interpreters.\narXiv preprint\narXiv:1511.06279, 2015.\nDaniel Ritchie, Paul Horsfall, and Noah D Goodman. Deep amortized inference for probabilistic\nprograms. arXiv preprint arXiv:1610.05735, 2016.\nJürgen Schmidhuber. Optimal ordered problem solver. Machine Learning, 54(3):211–254, 2004.\nMichael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. science,\n324(5923):81–85, 2009.\nEliot R Smith and Jamie DeCoster. Dual-process models in social and cognitive psychology: Con-\nceptual integration and links to underlying memory systems. Personality and social psychology\nreview, 4(2):108–131, 2000.\nRay J Solomonoff. A formal theory of inductive inference. Information and control, 7(1):1–22,\n1964.\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train-\ning with self-supervision for generalization under distribution shifts. In International conference\non machine learning, pp. 9229–9248. PMLR, 2020.\n16\n\nPreprint.\nHao Tang and Kevin Ellis. From perception to programs: Regularize, overparameterize, and amor-\ntize. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learn-\ning, volume 202 of Proceedings of Machine Learning Research, pp. 33616–33631. PMLR, 23–29\nJul 2023. URL https://proceedings.mlr.press/v202/tang23c.html.\nHao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin Ellis.\nCode repair with llms gives an exploration-exploitation tradeoff. NeurIPS, 2024a.\nHao Tang, Darren Key, and Kevin Ellis. Worldcoder, a model-based llm agent: Building world\nmodels by writing code and interacting with the environment. NeurIPS, 2024b.\nLuca H. Thoms, Karel A. Veldkamp, Hannes Rosenbusch, and Claire E. Stevenson.\nSolv-\ning arc visual analogies with neural embeddings and vector arithmetic:\nA generalized\nmethod.\nArXiv, abs/2311.08083, 2023.\nURL https://api.semanticscholar.org/\nCorpusID:265158110.\nRuocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman.\nHypothesis search: Inductive reasoning with language models. ICLR, 2024.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.\nIn The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.\nJohan Sokrates Wind.\n1st place 2020 arc kaggle.\nhttps://github.com/top-quarks/\nARC-solution, 2020. [Online GitHub repository].\nJonas Witt, Stef Rasing, Sebastijan Dumanˇci´c, Tias Guns, and Claus-Christian Carbon. A divide-\nalign-conquer strategy for program synthesis, 2023.\nURL https://arxiv.org/abs/\n2301.03094.\nLionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S Siegel, Jiahai Feng, Noa Korneev,\nJoshua B Tenenbaum, and Jacob Andreas. Learning adaptive planning representations with natu-\nral language guidance. ICLR, 2023.\n17\n\nPreprint.\nA\nDATA GENERATION TECHNICAL DETAILS\n100 seed \nproblems\nget four codes \nwith the most \nsimilar language \npart with the \nnew description\nconcepts:\ndescription:\nnew concepts:\nnew description:\ncalculate cosine \nsimilarity with \nthe seeds \nlanguage part\nlanguage\ncode\nnew \nlanguage\n100 seed \nproblems\nrandomly sample\nconcepts:\ndescription:\nlanguage\nPrompt example\nPrompt example\nnew concepts:\nnew description:\nnew language\nLLM\nLLM\nnew code\nStep1: Random remix to generate new language descriptions\nStep2: RAG to generate new codes\nnew concepts:... new description:...\nnew language\nFigure 11: A new natural language description is sampled by prompting an LLM with seed natural\nlanguage descriptions, effectively using in-context learning to recombine and mutate elements of\ndifferent problems. Code is generated for that new description via Retrieval Augmented Generation\n(RAG). Our RAG pipeline retrieves seeds with similar descriptions, and prompts an LLM to generate\ncode for the new description, given the retrieved seeds.\nThe prompt template for generating natural language descriptions by randomly sampling language\ndescriptions from seed problems is as follows:\nYou’ve generated these on previous requests:\n{examples}\nBrainstorm {num_generations} more, using similar thinking:\n‘‘‘python\n# concepts:\n# <concepts in your new generation>\n# description:\n# <description of your new generation>\n‘‘‘\nThe prompt template for generating Python code from the natural-language descriptions (and similar\nexample code retrieved from the seeds, via RAG) is as follows:\nYou are a puzzle maker designing geometric, physical, and topological\npuzzles for curious middle-schoolers.\nEach puzzle consists of uncovering a deterministic rule, pattern, procedure,\nalgorithm, or transformation law that maps inputs to outputs.\nBoth the inputs and outputs are 2D grids of colored pixels. There are 10\ncolors, but the order of the colors is never relevant to the puzzle.\n18\n\nPreprint.\nThe middle schoolers are trying to discover this deterministic\ntransformation, which can be implemented as a Python function called ‘main‘.\nDesigning a puzzle involves also creating example inputs, which can be\nimplemented as a Python function called ‘generate_input‘. Unlike ‘main‘, the\n‘generate_input‘ function should be stochastic, so that every time you run\nit, you get another good example of what the transformation can be applied\nto.\nHere is a overview of the puzzle you are designing:\n{description}\nPlease implement the puzzle by writing code containing the ‘generate_input‘\nand ‘main‘ functions. Use the following standard library (‘common.py‘):\n‘‘‘python\n{common_lib}\n‘‘‘\nHere are some examples from puzzles with similar descriptions to show you\nhow to use functions in ‘common.py‘:\n{examples}\nYour task is to implement the puzzle, following these steps:\n1. Inspect the example puzzle implementations, making note of the functions\nused and the physical/geometric/topological/logical details\n2. Inspect the new puzzle’s description\n3. Brainstorm a possible implementation for the new puzzle\n4. Generate a code block formatted like the earlier examples with a comment\nstarting ‘# concepts:‘ listing the concepts and ‘# description:‘ describing\nthe inputs and transformation from the given description.\nBe sure to make the transformation ‘main‘ deterministic. Follow the\ndescription closely.\nExecution and Filtering of Generated Problems\nWe heuristically filter problems to improve the\nquality of data based on the following criteria:\n• The generator and transformation functions can be executed, producing at least 4 input-\noutput grids examples.\n• Transformation being deterministic: We check for consistency by running the functions\nwith different random seeds and filter out those with non-deterministic outputs.\n• Appropriate grid sizes: We remove input-output grids with height or width larger than 30,\naligning with grid sizes in ARC\n• Color permutation check: Since we use numpy arrays with integers 0-9 to represent colors,\nwe want to ensure transformations don’t rely on arithmetic operations of these integers. We\nfilter this by checking if input-output remains consistent when permuting the underlying\ncolor-number mapping.\n• Removal of problems with all trivial identity input-output examples.\n19\n\nPreprint.\nA.1\nSEED EXAMPLES\nExample seed 0d3d703e\nExample seed 1b2d62fb\nExample seed 0dfd9992\nFigure 12: Three seed examples\n\"\"\"============ problem id: 0d3d703e ============\"\"\"\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color mapping\n# description:\n# The input is a grid where each column is of the same color.\n# To make the output, change each color according to the following\nmapping:\n# green -> yellow, blue -> gray, red -> pink, teal -> maroon, yellow ->\ngreen, gray -> blue, pink -> red,\nmaroon -> teal\ndef transform_grid(input_grid):\n# Initialize output grid\noutput_grid = input_grid.copy()\n# Performs color mapping\noutput_grid = np.vectorize(lambda color: color_map.get(color, color))\n(output_grid)\nreturn output_grid\n# Constructing the color map\ncolor_map = {Color.GREEN : Color.YELLOW,\nColor.BLUE : Color.GRAY,\nColor.RED : Color.PINK,\nColor.TEAL : Color.MAROON,\nColor.YELLOW : Color.GREEN,\nColor.GRAY : Color.BLUE,\nColor.PINK : Color.RED,\nColor.MAROON : Color.TEAL\n}\ndef generate_input():\ngrid = np.full((3, 3), Color.BLACK)\nfor x in range(grid.shape[0]):\ngrid[x, :] = random.choice(list(color_map.keys()))\nreturn grid\n\"\"\"============ problem id: 1b2d62fb ============\"\"\"\nimport numpy as np\nfrom typing import *\nfrom common import *\n20\n\nPreprint.\n# concepts:\n# boolean logical operations, bitmasks with separator\n# description:\n# In the input you will see two maroon bitmasks separated by a blue\nvertical bar\n# To make the output, color teal the pixels that are not set in either\nbitmasks (logical NOR)\ndef transform_grid(input_grid: np.ndarray) -> np.ndarray:\n# Find the blue vertical bar. Vertical means constant X\nfor x_bar in range(input_grid.shape[0]):\nif np.all(input_grid[x_bar, :] == Color.BLUE):\nbreak\nleft_mask = input_grid[:x_bar, :]\nright_mask = input_grid[x_bar+1:, :]\noutput_grid = np.zeros_like(left_mask)\noutput_grid[(left_mask != Color.MAROON) & (right_mask != Color.MAROON\n)] = Color.TEAL\nreturn output_grid\ndef generate_input() -> np.ndarray:\n# create a pair of equally sized maroon bitmasks\nwidth, height = np.random.randint(2, 10), np.random.randint(2, 10)\ngrid1 = np.zeros((width, height), dtype=int)\ngrid2 = np.zeros((width, height), dtype=int)\nfor x in range(width):\nfor y in range(height):\ngrid1[x, y] = np.random.choice([Color.MAROON, Color.BLACK])\ngrid2[x, y] = np.random.choice([Color.MAROON, Color.BLACK])\n# create a blue vertical bar\nbar = np.zeros((1, height), dtype=int)\nbar[0, :] = Color.BLUE\ngrid = np.concatenate((grid1, bar, grid2), axis=0)\nreturn grid\n\"\"\"============ problem id: 0dfd9992 ============\"\"\"\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# occlusion, translational symmetry\n# description:\n# In the input you will see a translationally symmetric pattern randomly\noccluded by black pixels.\n# To make the output, remove the occluding black pixels to reveal the\ntranslationally symmetric pattern.\ndef transform_grid(input_grid):\n# Plan:\n# 1. Find the translational symmetries\n21\n\nPreprint.\n# 2. Reconstruct the sprite by ignoring the black pixels and\nexploiting the symmetry\nw, h = input_grid.shape\n# Identify the translational symmetries\ntranslations = detect_translational_symmetry(input_grid,\nignore_colors=[Color.BLACK])\nassert len(translations) > 0, \"No translational symmetry found\"\n# Reconstruct the occluded black pixels by replacing them with colors\nfound in the orbit of the\nsymmetries\noutput_grid = np.copy(input_grid)\nfor x in range(w):\nfor y in range(h):\nif output_grid[x, y] == Color.BLACK:\n# Use the translational symmetry to fill in the occluded\npixels\n# to do this we compute the ORBIT of the current pixel\nunder the\ntranslations\n# and take the most common non-black color in the orbit\n# Compute the orbit into the output\norbit_pixels = orbit(output_grid, x, y, translations)\norbit_colors = {input_grid[transformed_x, transformed_y]\nfor transformed_x, transformed_y in\norbit_pixels}\n# occluded by black, so whatever color it is, black doesn\n’t count\norbit_colors = orbit_colors - {Color.BLACK}\n# Copy the color\nassert len(orbit_colors) == 1, \"Ambiguity: multiple\ncolors in the orbit\"\noutput_grid[x, y] = orbit_colors.pop()\nreturn output_grid\ndef generate_input():\n# Make a random large canvas\ngrid = np.full((np.random.randint(15, 30), np.random.randint(15, 30))\n, Color.BLACK)\n# Make the basic sprite\nw, h = random.randint(3, 8), random.randint(3, 8)\nsprite = random_sprite(w, h, density=1, color_palette=Color.NOT_BLACK\n)\n# Place the sprite in the canvas\nfor x in range(0, grid.shape[0], w):\nfor y in range(0, grid.shape[1], h):\nblit_sprite(grid, sprite, x, y)\n# Create random occluders\nn_occluders = random.randint(1, 5)\nfor _ in range(n_occluders):\nx, y = random.randint(0, grid.shape[0]), random.randint(0, grid.\nshape[1])\nw, h = random.randint(3, 7), random.randint(3, 7)\noccluder_sprite = np.full((w, h), Color.BLACK)\nblit_sprite(grid, occluder_sprite, x, y)\n22\n\nPreprint.\nreturn grid\nA.2\nCOMMON LIBRARY\n23\n\nPreprint.\n\"\"\"Common library for ARC\"\"\"\nimport numpy as np\nimport random\nclass Color:\n\"\"\"\nEnum for colors\nColor.BLACK, Color.BLUE, Color.RED, Color.GREEN, Color.YELLOW,\nColor.GREY, Color.PINK, Color.ORANGE, Color.TEAL, Color.MAROON\nUse Color.ALL_COLORS for ‘set‘ of all possible colors\nUse Color.NOT_BLACK for ‘set‘ of all colors except black\nColors are strings (NOT integers),\nso you CAN’T do math/arithmetic/indexing on them.\n(The exception is Color.BLACK, which is 0)\n\"\"\"\ndef flood_fill(grid, x, y, color, connectivity=4):\n\"\"\"\nFill the connected region that contains the point (x, y) with\nthe specified color.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\n8-way counts diagonals as connected,\n4-way only counts cardinal directions as connected.\n\"\"\"\ndef draw_line(grid, x, y, end_x=None, end_y=None, length=None, direction=\nNone,\ncolor=None, stop_at_color=[]):\n\"\"\"\nDraws a line starting at (x, y) extending to (end_x, end_y) or\nof the specified length in the specified direction\nDirection should be a vector with elements -1, 0, or 1.\nIf length is None, then the line will continue until it hits\nthe edge of the grid.\nstop_at_color: optional list of colors that the line should stop at.\nIf the line hits a pixel of one of these colors, it will stop.\nExample:\n# blue diagonal line from (0, 0) to (2, 2)\ndraw_line(grid, 0, 0, length=3, color=blue, direction=(1, 1))\ndraw_line(grid, 0, 0, end_x=2, end_y=2, color=blue)\n\"\"\"\ndef find_connected_components(grid, background=Color.BLACK, connectivity=\n4,\nmonochromatic=True):\n\"\"\"\nFind the connected components in the grid.\nReturns a list of connected\ncomponents, where each connected component is a numpy array.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\nmonochromatic: if True, each connected component is assumed to have\nonly one color.\nIf False, each connected component can include multiple colors.\n\"\"\"\n24\n\nPreprint.\ndef random_scatter_points(grid, color, density=0.5,\nbackground=Color.BLACK):\n\"\"\"\nRandomly scatter points of the specified color in the grid with\nspecified density.\n\"\"\"\ndef scale_pattern(pattern, scale_factor):\n\"\"\"\nScales the pattern by the specified factor.\n\"\"\"\ndef blit_object(grid, obj, background=Color.BLACK):\n\"\"\"\nDraws an object onto the grid using its current location.\nExample usage:\nblit_object(output_grid, an_object, background=background_color)\n\"\"\"\ndef blit_sprite(grid, sprite, x, y, background=Color.BLACK):\n\"\"\"\nDraws a sprite onto the grid at the specified location.\nExample usage:\nblit_sprite(output_grid, the_sprite, x=x, y=y,\nbackground=background_color)\n\"\"\"\ndef bounding_box(grid, background=Color.BLACK):\n\"\"\"\nFind the bounding box of the non-background pixels in the grid.\nReturns a tuple (x, y, width, height) of the bounding box.\nExample usage:\nobjects = find_connected_components(input_grid, monochromatic=True,\nbackground=Color.BLACK, connectivity=8)\nteal_object=[obj for obj in objects if np.any(obj == Color.TEAL)][0]\nteal_x, teal_y, teal_w, teal_h = bounding_box(teal_object)\n\"\"\"\ndef object_position(obj, background=Color.BLACK, anchor=\"upper left\"):\n\"\"\"\n(x,y) position of the provided object.\nBy default, the upper left corner.\nanchor: \"upper left\", \"upper right\", \"lower left\", \"lower right\",\n\"center\", \"upper center\", \"lower center\", \"left center\", \"right\ncenter\"\nExample usage:\nx, y = object_position(obj, background=background_color,\nanchor=\"upper left\")\nmiddle_x, middle_y = object_position(obj, background=background_color\n,\nanchor=\"center\")\n\"\"\"\ndef crop(grid, background=Color.BLACK):\n\"\"\"\nCrop the grid to the smallest bounding box that contains all\nnon-background pixels.\nExample usage:\n25\n\nPreprint.\n# Extract a sprite from an object\nsprite = crop(an_object, background=background_color)\n\"\"\"\ndef translate(obj, x, y, background=Color.BLACK):\n\"\"\"\nTranslate by the vector (x, y). Fills in the new pixels with the\nbackground color.\nExample usage:\nred_object = ... # extract some object\nshifted_red_object = translate(red_object, x=1, y=1)\nblit_object(output_grid, shifted_red_object,\nbackground=background_color)\n\"\"\"\ndef collision(_=None, object1=None, object2=None, x1=0, y1=0, x2=0, y2=0,\nbackground=Color.BLACK):\n\"\"\"\nCheck if object1 and object2 collide when object1 is at (x1, y1) and\nobject2 is at (x2, y2).\nExample usage:\n# Check if a sprite can be placed onto a grid at (X,Y)\ncollision(object1=output_grid, object2=a_sprite, x2=X, y2=Y)\n# Check if two objects collide\ncollision(object1=object1, object2=object2,\nx1=X1, y1=Y1, x2=X2, y2=Y2)\n\"\"\"\ndef contact(_=None, object1=None, object2=None, x1=0, y1=0, x2=0, y2=0,\nbackground=Color.BLACK, connectivity=4,):\n\"\"\"\nCheck if object1 and object2 touch each other (have contact)\nwhen object1 is at (x1, y1) and object2 is at (x2, y2).\nThey are touching each other if they share a border, or if they\noverlap.\nCollision implies contact, but contact does not imply collision.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\n(8-way counts diagonals as touching,\n4-way only counts cardinal directions as touching)\nExample usage:\n# Check if a sprite touches anything if it were to be placed at (X,Y)\ncontact(object1=output_grid, object2=a_sprite, x2=X, y2=Y)\n# Check if two objects touch each other\ncontact(object1=object1, object2=object2)\n\"\"\"\ndef generate_position_has_interval(max_len, position_num, if_padding=\nFalse):\n\"\"\"\nGenerate the position of the lines with random interval.\n\"\"\"\ndef random_free_location_for_sprite(grid, sprite, background=Color.BLACK,\nborder_size=0, padding=0,\npadding_connectivity=8):\n\"\"\"\nFind a random free location for the sprite in the grid\n26\n\nPreprint.\nReturns a tuple (x, y) of the top-left corner of the sprite in the\ngrid, which can be passed to ‘blit_sprite‘\nborder_size: minimum distance from the edge of the grid\nbackground: color treated as transparent\npadding: if non-zero, the sprite will be padded with a non-background\ncolor before checking for collision\npadding_connectivity: 4 or 8, for 4-way or 8-way connectivity when\npadding the sprite\nExample usage:\nx, y = random_free_location_for_sprite(grid, sprite, padding=1,\npadding_connectivity=8, border_size=1, background=Color.BLACK)\n# find the location, using generous padding\nassert not collision(object1=grid, object2=sprite, x2=x, y2=y)\nblit_sprite(grid, sprite, x, y)\n\"\"\"\ndef object_interior(grid, background=Color.BLACK):\n\"\"\"\nComputes the interior of the object (including edges)\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s interior.\nExample usage:\ninterior = object_interior(obj, background=Color.BLACK)\nfor x, y in np.argwhere(interior):\n# x,y is either inside the object or at least on its edge\n\"\"\"\ndef object_boundary(grid, background=Color.BLACK):\n\"\"\"\nComputes the boundary of the object (excluding interior)\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s boundary.\nExample usage:\nboundary = object_boundary(obj, background=Color.BLACK)\nassert np.all(obj[boundary] != Color.BLACK)\n\"\"\"\ndef object_neighbors(grid, background=Color.BLACK, connectivity=4):\n\"\"\"\nComputes a mask of the points that neighbor or border the object, but\nare not part of the object.\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s border neighbors5.\nExample usage:\nneighbors = object_neighbors(obj, background=Color.BLACK)\nassert np.all(obj[neighbors] == Color.BLACK)\n\"\"\"\nclass Symmetry:\n\"\"\"\nSymmetry transformations, which transformed the 2D grid in ways that\npreserve visual structure.\nReturned by ‘detect_rotational_symmetry‘,\n‘detect_translational_symmetry‘, ‘detect_mirror_symmetry‘.\n\"\"\"\ndef apply(self, x, y, iters=1):\n27\n\nPreprint.\n\"\"\"\nApply the symmetry transformation to the point (x, y) ‘iters‘\ntimes.\nReturns the transformed point (x’,y’)\n\"\"\"\ndef orbit(grid, x, y, symmetries):\n\"\"\"\nCompute the orbit of the point (x, y) under the symmetry\ntransformations ‘symmetries‘.\nThe orbit is the set of points that the point (x, y) maps to after\napplying the symmetry transformations different numbers of times.\nReturns a list of points in the orbit.\nExample:\nsymmetries = detect_rotational_symmetry(input_grid)\nfor x, y in np.argwhere(input_grid != Color.BLACK):\n# Compute orbit on to the target grid, which is typically the\noutput\nsymmetric_points = orbit(output_grid, x, y, symmetries)\n# ... now we do something with them like copy colors or infer\nmissing colors\n\"\"\"\ndef detect_translational_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nFinds translational symmetries in a grid.\nSatisfies: grid[x, y] == grid[x + translate_x, y + translate_y] for\nall x, y, as long as neither pixel is in ‘ignore_colors‘.\nReturns a list of Symmetry objects, each representing a different\ntranslational symmetry.\nExample:\nsymmetries = detect_translational_symmetry(grid, ignore_colors=[\noccluder_color])\nfor x, y in np.argwhere(grid != occluder_color):\n# Compute orbit on to the target grid\n# When copying to an output, this is usually the output grid\nsymmetric_points = orbit(grid, x, y, symmetries)\nfor x, y in symmetric_points:\nassert grid[x, y] == grid[x, y] or grid[x, y] ==\noccluder_color\n\"\"\"\ndef detect_mirror_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nReturns list of mirror symmetries.\nSatisfies: grid[x, y] == grid[2*mirror_x - x, 2*mirror_y - y]\nfor all x, y, as long as neither pixel is in ‘ignore_colors‘\nExample:\nsymmetries = detect_mirror_symmetry(grid,ignore_colors=[Color.BLACK])\n# ignore_color: In case parts of the object have been removed and\n# occluded by black\nfor x, y in np.argwhere(grid != Color.BLACK):\nfor sym in symmetries:\nsymmetric_x, symmetric_y = sym.apply(x, y)\nassert grid[symmetric_x, symmetric_y] == grid[x, y]\nor grid[symmetric_x, symmetric_y] == Color.BLACK\nIf the grid has both horizontal and vertical mirror symmetries,\nthe returned list will contain two elements.\n\"\"\"\n28\n\nPreprint.\ndef detect_rotational_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nFinds rotational symmetry in a grid, or returns None if no symmetry\nis possible.\nSatisfies: grid[x, y] == grid[y - rotate_center_y + rotate_center_x,\n-x + rotate_center_y + rotate_center_x]\n# clockwise\ngrid[x, y] == grid[-y + rotate_center_y + rotate_center_x,\nx - rotate_center_y + rotate_center_x]\n# counterclockwise\nfor all x,y, as long as neither pixel is in ignore_colors\nExample:\nsym = detect_rotational_symmetry(grid, ignore_colors=[Color.BLACK])\n# ignore_color: In case parts of the object have been removed and\n# occluded by black\nfor x, y in np.argwhere(grid != Color.BLACK):\nrotated_x, rotated_y = sym.apply(x, y, iters=1) # +1 clockwise,\n-1 counterclockwise\nassert grid[rotated_x, rotated_y] == grid[x, y] or\ngrid[rotated_x, rotated_y] == Color.BLACK\nprint(sym.center_x, sym.center_y) # In case these are needed, they\nare floats\n\"\"\"\ndef is_contiguous(bitmask, background=Color.BLACK, connectivity=4):\n\"\"\"\nCheck if an array is contiguous.\nbackground: Color that counts as transparent (default: Color.BLACK)\nconnectivity: 4 or 8, for 4-way (only cardinal directions) or\n8-way connectivity (also diagonals) (default: 4)\nReturns True/False\n\"\"\"\ndef random_sprite(n, m, density=0.5, symmetry=None, color_palette=None,\nconnectivity=4, background=Color.BLACK):\n\"\"\"\nGenerate a sprite (an object), represented as a numpy array.\nn, m: dimensions of the sprite. If these are lists, then a random\nvalue will be chosen from the list.\nsymmetry: optional type of symmetry to apply to the sprite. Can be\n’horizontal’, ’vertical’, ’diagonal’, ’radial’, ’not_symmetric’. If\nNone, a random symmetry type will be chosen.\ncolor_palette: optional list of colors to use in the sprite. If None,\na random color palette will be chosen.\nReturns an (n,m) NumPy array representing the sprite.\n\"\"\"\ndef detect_objects(grid, _=None, predicate=None, background=Color.BLACK,\nmonochromatic=False, connectivity=None,\nallowed_dimensions=None,\ncolors=None, can_overlap=False):\n\"\"\"\nDetects and extracts objects from the grid that satisfy custom\nspecification.\npredicate:\na function that takes a candidate object as input and\nreturns True if it counts as an object\nbackground:\n29\n\nPreprint.\ncolor treated as transparent\nmonochromatic:\nif True, each object is assumed to have only one color\nIf False, each object can include multiple colors.\nconnectivity:\n4 or 8, for 4-way or 8-way connectivity.\nIf None, the connectivity is determined automatically.\nallowed_dimensions:\na list of tuples (n, m) specifying the allowed dimensions of the\nobjects.\nIf None, objects of any size are allowed.\ncolors:\na list of colors that the objects are allowed to have.\nIf None, objects of any color are allowed.\ncan_overlap: if True, objects can overlap.\nIf False, objects cannot overlap.\nReturns a list of objects, where each object is a numpy array.\n\"\"\"\n30\n\nPreprint.\nconcepts: \ncolor inversion, \nobjecxt transformation\nconcepts: \npixel sorting, frequency counting, \ncolor arrangement\nconcepts: \nshape completion, color filling\nconcepts: \nsymmetry detection, \nreflection\nconcepts: \ncolor blending, \nradial expansion\nconcepts: \nobject detection, shape expansion, \ncolor filling\nconcepts: \npattern detection, copying, \npositioning\nconcepts: \ninverse patterns, scaling\nconcepts: \ncolor blending, \nshape merging\nFigure 13: Nine example problems generated automatically by our pipeline.\n31\n\nPreprint.\nA.3\nGENERATED ARC EXAMPLES\nB\nFINE TUNING TRAINING DETAILS\nB.1\nPROMPTING THE MODELS\nWe must include in our prompts for our fine-tuned models the input/output 2D colored grids of\neach problem. To do this we represent the problem textually by naming the colors one-by-one. We\nrenamed certain colors which were more than one token (e.g., maroon→brown saves 1 token/pixel),\nand presented the grid as a whitespace-delimited 2D array with newlines delimiting rows. Please\nsee below.\nTrain examples\nTest example\nExample 1\nExample 2\nExample 3\nFigure 14: Prompt example illustration\nTransduction example:\n-----\nRole: system\n--------------------\nYou are a world-class puzzle solver with exceptional pattern recognition\nskills. Your task is to analyze puzzles, spot patterns, and provide direct\nsolutions.\n-----\nRole: user\n--------------------\nGiven input-output grid pairs as reference examples, carefully observe the\npatterns to predict the output grid for new test input. Each pair follows\nthe same transformation rule. Grids are 2D arrays represented as strings,\nwith cells (colors) separated by spaces and rows by newlines.\nHere are the input and output grids for the reference examples:\nExample 1\nInput:\nBlack Gray Black\nGray Gray Gray\nBlack Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nExample 2\nInput:\nGray Black Black\nBlack Gray Black\nBlack Black Gray\nOutput:\nBlue Red Black Black Black Black\nRed Blue Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\n32\n\nPreprint.\nBlack Black Black Black Blue Red\nBlack Black Black Black Red Blue\nExample 3\nInput:\nBlack Gray Black\nBlack Gray Gray\nGray Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Blue Red Blue Red\nBlack Black Red Blue Red Blue\nBlue Red Blue Red Black Black\nRed Blue Red Blue Black Black\nHere is the input grid for the test example:\nInput:\nBlack Black Black\nBlack Gray Black\nGray Gray Gray\nDirectly provide the output grids corresponding to the given test input\ngrids, based on the patterns observed in the reference examples.\n-----\nRole: assistant\n--------------------\nThe output grid for the test input grid is:\n‘‘‘\nBlack Black Black Black Black Black\nBlack Black Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\n‘‘‘\nInduction example:\n-----\nRole: system\n--------------------\nYou are a world-class puzzle solver with exceptional pattern recognition\nskills and expertise in Python programming. Your task is to analyze puzzles\nand provide Python solutions.\n-----\nRole: user\n--------------------\nGiven input-output grid pairs as reference examples, carefully observe the\npatterns to predict the output grid for new test input. Each pair follows\nthe same transformation rule. Grids are 2D arrays represented as strings,\nwith cells (colors) separated by spaces and rows by newlines.\nHere are the input and output grids for the reference examples:\nExample 1\nInput:\nBlack Gray Black\nGray Gray Gray\nBlack Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\n33\n\nPreprint.\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nExample 2\nInput:\nGray Black Black\nBlack Gray Black\nBlack Black Gray\nOutput:\nBlue Red Black Black Black Black\nRed Blue Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Black Black Blue Red\nBlack Black Black Black Red Blue\nExample 3\nInput:\nBlack Gray Black\nBlack Gray Gray\nGray Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Blue Red Blue Red\nBlack Black Red Blue Red Blue\nBlue Red Blue Red Black Black\nRed Blue Red Blue Black Black\nHere is the input grid for the test example:\nInput:\nBlack Black Black\nBlack Gray Black\nGray Gray Gray\nWrite a Python function ‘transform‘ that can convert any given input grid to\nits corresponding output grid based on the pattern observed in the\nreference examples.\n-----\nRole: assistant\n--------------------\nLet’s solve this puzzle using Python code with the common library functions.\nWe’ll first reason about the problem and then write the code to solve it.\nThe ‘transform‘ function will take the input grid and return the output grid\n. Here is the Python code with the comments describing how to solve the\nproblem:\n‘‘‘python\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# repetition, diagonal lines, color change\n# description:\n# In the input you will see a 3x3 grid with a specific diagonal pattern of\ngray pixels.\n# The output should be the result of repeating this diagonal pattern\nhorizontally and vertically\n34\n\nPreprint.\n# to fill a 6x6 grid, changing the color from gray to blue and red in an\nalternating manner.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a new 6x6 output grid filled with the background color (black)\noutput_grid = np.full((6, 6), Color.BLACK)\n# Check for the diagonal pattern in the input grid\nfor i in range(3):\nfor j in range(3):\nif input_grid[i, j] == Color.GRAY:\n# Fill the same position in the output grid with alternating\ncolors\noutput_grid[i * 2, j * 2] = Color.BLUE\noutput_grid[i * 2, j * 2 + 1] = Color.RED\noutput_grid[i * 2 + 1, j * 2] = Color.RED\noutput_grid[i * 2 + 1, j * 2 + 1] = Color.BLUE\nreturn output_grid\n‘‘‘\nC\nEXAMPLES OF INDUCTION SOLUTIONS\nC.1\nARC PROBLEM F3CDC58F\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color counting, grid transformation\n# description:\n# In the input, you will see a grid with several colored squares.\n# To make the output, count the number of squares of each color.\n# Fill the output grid with bars of corresponding colors.\n# The height of each bar corresponds to the number of squares of that\ncolor.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a blank output grid\noutput_grid = np.zeros_like(input_grid)\n# Count the number of squares of each color\ncolor_counts = {color: 0 for color in Color.NOT_BLACK}\nfor row in input_grid:\nfor cell in row:\nif cell in color_counts:\ncolor_counts[cell] += 1\n# Define the height of the bars based on the counts\nmax_height = 10\n# Maximum height of the bars\nfor color, count in color_counts.items():\nif count > 0:\nfor h in range(min(count, max_height)):\noutput_grid[-(h + 1), (color - 1) %\nreturn output_grid\n35\n\nPreprint.\nC.2\nARC PROBLEM BE03B35F\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color detection, pattern extraction, rotation transformation\n# description:\n# In the input, you will see a 5x5 grid consisting of a colored pattern\nin the top left corner and a 2x2\nsquare of red pixels in the bottom\nright corner.\n# Depending on the color of the top left corner (blue or green), rotate\nthe pattern by 90 degrees clockwise\nor counterclockwise.\n# The output grid should be the transformed pattern placed in the same\nposition as the original top left\npattern.\ndef transform(input_grid):\n# Extract the top left 5x5 pattern\npattern = input_grid[0:5, 0:5]\n# Identify the color in the top left corner\ncorner_color = pattern[0, 0]\n# Define rotation based on the corner color\nif corner_color == Color.BLUE:\ntransformed_pattern = np.rot90(pattern, k=-1)\n# Rotate 90\ndegrees counterclockwise\nelif corner_color == Color.GREEN:\ntransformed_pattern = np.rot90(pattern, k=1)\n# Rotate 90\ndegrees clockwise\nelse:\nraise ValueError(\"The corner color must be either BLUE or GREEN.\"\n)\n# Create the output grid\noutput_grid = np.full((2, 2), Color.BLACK)\n# Initialize a small grid\noutput_grid = blit_sprite(output_grid, transformed_pattern, x=0, y=0,\nbackground=Color.BLACK)\nreturn output_grid\nC.3\nARC PROBLEM 2072ABA6\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color mapping, pattern replication\n# description:\n# In the input you will see a 3x3 grid of colored pixels.\n# The colors are either black or gray. The output should be a grid where:\n# 1. If the pixel is black, it remains black in the output.\n# 2. If the pixel is gray, it should be replaced with a 2x2 block of blue\nand red pixels in a checkerboard\npattern.\n36\n\nPreprint.\n# The blue pixel should occupy the top-left and bottom-right of the 2x2\nblock, while the red pixel occupies\nthe top-right and bottom-left.\ndef transform(input_grid):\n# Create an output grid that is larger than the input grid\noutput_grid = np.zeros((input_grid.shape[0] * 2, input_grid.shape[1]\n* 2), dtype=int)\n# Fill the output grid based on the input grid\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] == Color.BLACK:\n# Black stays black in the output\ncontinue\nelse:\n# Replace gray with a checkerboard pattern of blue and\nred\noutput_grid[2 * x:2 * x + 2, 2 * y:2 * y + 2] = [\n[Color.BLUE, Color.RED],\n[Color.RED, Color.BLUE]\n]\nreturn output_grid\nC.4\nARC PROBLEM EF26CBF6\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color transformation, grid sections, boundary detection\n# description:\n# In the input, you will see a grid with a pattern of yellow and blue\npixels with a black background,\n# and multiple colored circles (not yellow or blue) placed randomly\nwithin the grid.\n# The goal is to transform the output grid by replacing all the blue\npixels with the color of the closest\ncircle\n# and keeping the yellow pixels unchanged.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a copy of the input grid to modify\noutput_grid = np.copy(input_grid)\n# Find the coordinates of the colored circles\ncircle_coordinates = np.argwhere((input_grid != Color.BLACK) & (\ninput_grid != Color.YELLOW) & (\ninput_grid != Color.BLUE))\n# Iterate over the grid and replace blue pixels with the nearest\ncircle color\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] == Color.BLUE:\n# Find the closest circle\nclosest_color = None\nclosest_distance = float(’inf’)\nfor circle_x, circle_y in circle_coordinates:\n37\n\nPreprint.\ndistance = abs(circle_x - x) + abs(circle_y - y)\n#\nManhattan\ndistance\nif distance < closest_distance:\nclosest_distance = distance\nclosest_color = input_grid[circle_x, circle_y]\n# Replace the blue pixel with the closest circle color\noutput_grid[x, y] = closest_color\nreturn output_grid\nC.5\nARC PROBLEM E7639916\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# boundary detection, color filling\n# description:\n# In the input grid, you will see a black background with several purple\npixels forming a boundary.\n# The task is to fill the area enclosed by the purple boundary with blue\npixels.\ndef transform(input_grid):\n# Create an output grid that starts as a copy of the input grid\noutput_grid = np.copy(input_grid)\n# Find the coordinates of the purple pixels\npurple_coords = np.argwhere(input_grid == Color.PURPLE)\n# If no purple pixels are found, return the original grid\nif len(purple_coords) == 0:\nreturn output_grid\n# Get the boundaries of the purple pixels\nmin_x, min_y = np.min(purple_coords, axis=0)\nmax_x, max_y = np.max(purple_coords, axis=0)\n# Fill the area enclosed by the purple boundary\nfor x in range(min_x, max_x + 1):\nfor y in range(min_y, max_y + 1):\n# Check if the current position is outside the purple\nboundary\nif (x == min_x or x == max_x or y == min_y or y == max_y) and\noutput_grid[x, y] ==\nColor.BLACK:\noutput_grid[x, y] = Color.BLUE\nreturn output_grid\nC.6\nARC PROBLEM C074846D\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n38\n\nPreprint.\n# rotation, color change, symmetry\n# description:\n# In the input, you will see a colored object with a single gray pixel.\n# To make the output, rotate the object 90 degrees clockwise around the\ngray pixel,\n# and color the newly exposed pixels green.\ndef transform(input_grid):\n# Find the gray pixel location\ngray_pixel_locations = np.argwhere(input_grid == Color.GRAY)\nassert len(gray_pixel_locations) == 1\ngray_x, gray_y = gray_pixel_locations[0]\n# Create an output grid\noutput_grid = np.full(input_grid.shape, Color.BLACK)\n# Rotate the object around the gray pixel\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] != Color.BLACK and input_grid[x, y] !=\nColor.GRAY:\n# Calculate new position after 90 degrees clockwise\nrotation\nnew_x = gray_x + (y - gray_y)\nnew_y = gray_y - (x - gray_x)\n# Check if the new position is within bounds\nif 0 <= new_x < output_grid.shape[0] and 0 <= new_y <\noutput_grid.shape[1]\n:\n# Place the rotated pixel in the output grid\noutput_grid[new_x, new_y] = input_grid[x, y]\n# Color newly exposed pixels green\nif output_grid[x, y] == Color.BLACK:\noutput_grid[x, y] = Color.GREEN\n# Place the gray pixel back in the center\noutput_grid[gray_x, gray_y] = Color.GRAY\nreturn output_grid\nC.7\nARC PROBLEM AE58858E\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# object detection, color change, size comparison\n# description:\n# In the input, you will see a grid with red objects of various sizes.\n# To make the output, change all objects larger than a certain size (3\npixels) to pink.\ndef transform(input_grid):\n# Create a copy of the input grid to produce the output\noutput_grid = np.copy(input_grid)\n# Find all connected components (red objects) in the input grid\n39\n\nPreprint.\nred_objects = find_connected_components(input_grid, background=Color.\nBLACK, monochromatic=True)\n# Iterate through the red objects and change their color if they are\nlarger than 3 pixels\nfor obj in red_objects:\nsize = np.sum(obj == Color.RED)\nif size > 3:\noutput_grid[obj == Color.RED] = Color.PINK\nreturn output_grid\nC.8\nARC PROBLEM 21F83797\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# vertical and horizontal lines, intersection\n# description:\n# In the input, you will see two red pixels.\n# To make the output, draw a blue square with the red pixels as corners.\n# Additionally, draw a vertical and horizontal line that intersect at\neach red pixel.\ndef transform(input_grid):\n# Copy the input grid to the output grid\noutput_grid = np.copy(input_grid)\n# Find the positions of the red pixels\nred_pixels = np.argwhere(input_grid == Color.RED)\n# Ensure there are exactly two red pixels\nassert len(red_pixels) == 2\n(x1, y1), (x2, y2) = red_pixels\n# Sort the red pixels’ positions to determine which is top-left and\nbottom-right\ntop_left = (min(x1, x2), min(y1, y2))\nbottom_right = (max(x1, x2), max(y1, y2))\n# Draw a blue square from the top-left to the bottom-right corner\noutput_grid[top_left[0]:bottom_right[0]+1, top_left[1]:bottom_right[1\n]+1] = Color.BLUE\n# Draw horizontal and vertical lines through the red pixels\nfor x, y in [(x1, y1), (x2, y2)]:\noutput_grid[x, :] = Color.RED\n# Horizontal line through red\npixels\noutput_grid[:, y] = Color.RED\n# Vertical line through red pixels\nreturn output_grid\nC.9\nARC PROBLEM 33B52DE3\nfrom common import *\nimport numpy as np\nfrom typing import *\n40\n\nPreprint.\n# concepts:\n# object detection, color mapping, grid transformation\n# description:\n# In the input, you will see a 20x20 grid containing a 4x4 pattern made\nof colored pixels (not necessarily\ncontiguous) and a series of 3x3 gray\nsquares.\n# The task is to extract the colors from the 4x4 pattern and color the\ncorresponding 3x3 gray squares with\nthose colors in a specific order.\n# The first gray square should be colored with the first color from the\npattern, the second gray square with\nthe second color, and so on.\ndef transform(input_grid):\n# Step 1: Detect the 4x4 color pattern in the input grid.\ncolor_pattern = detect_objects(input_grid, monochromatic=False,\nconnectivity=8)\n# Step 2: Extract the colors from the detected pattern.\ncolors = []\nfor obj in color_pattern:\ncropped_obj = crop(grid=obj, background=Color.BLACK)\ncolors.extend(cropped_obj.flatten())\n# Step 3: Identify the locations of the gray squares.\ngray_squares = detect_objects(input_grid, colors=[Color.GRAY],\nmonochromatic=True, connectivity\n=8)\n# Step 4: Color the gray squares with the corresponding colors from\nthe pattern.\noutput_grid = np.copy(input_grid)\nfor i, gray_square in enumerate(gray_squares):\nif i < len(colors):\n# Color the gray square with the corresponding color\nx, y = np.where(gray_square == Color.GRAY)\nflood_fill(output_grid, x[0], y[0], colors[i])\nreturn output_grid\nC.10\nARC PROBLEM 332EFDB3\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# borders, alternating patterns\n# description:\n# In the input, you will see an empty black grid. To make the output,\ndraw a blue border around the grid,\nbut only on the edges that are\nmultiples of 2 (i.e., the top and\nbottom edges, and the left and right\nedges at every even column/index).\ndef transform(input_grid):\n# Get the dimensions of the input grid\n41\n\nPreprint.\nheight, width = input_grid.shape\n# Create the output grid, initially a copy of the input grid\noutput_grid = input_grid.copy()\n# Draw the blue border on edges that are multiples of 2\nfor x in range(0, height, 2):\n# Vertical edges: multiples of 2\ndraw_line(output_grid, x, 0, direction=(0, 1), color=Color.BLUE)\n# Top edge\ndraw_line(output_grid, x, width - 1, direction=(0, 1), color=\nColor.BLUE)\n# Bottom edge\nfor y in range(0, width, 2):\n# Horizontal edges: multiples of 2\ndraw_line(output_grid, 0, y, direction=(1, 0), color=Color.BLUE)\n# Left edge\ndraw_line(output_grid, height - 1, y, direction=(1, 0), color=\nColor.BLUE)\n# Right edge\nreturn output_grid\nD\nINDUCTION FALSE POSITIVE SOLUTIONS\nD.1\nARC PROBLEM 25094A63\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# grid transformation, color replacement, pattern detection\n# description:\n# In the input, you will see a grid with colored regions. If a 5x5 square\nin the grid consists entirely of a\nsingle color,\n# replace that square with a 5x5 yellow square in the output grid. If the\nsquare is not fully filled with a\nsingle color, leave it as is.\ndef transform(input_grid):\n# Plan:\n# 1. Iterate over the grid to find 5x5 squares.\n# 2. For each 5x5 square, check if it is monochromatic (all pixels\nare the same color).\n# 3. If it is, replace it with a 5x5 yellow square in the output grid\n.\noutput_grid = np.copy(input_grid)\n# Dimensions\nsquare_size = 5\nfor x in range(len(input_grid) - square_size + 1):\nfor y in range(len(input_grid[0]) - square_size + 1):\n# Extract the 5x5 region\nregion = input_grid[x:x + square_size, y:y + square_size]\n# Check if the region is monochromatic\nif np.all(region == region[0, 0]):\n# All pixels should be\nthe same color\n# Replace the region with a yellow square\noutput_grid[x:x + square_size, y:y + square_size] = Color\n.YELLOW\n42\n\nPreprint.\nreturn output_grid\nD.2\nARC PROBLEM 009D5C81\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# pattern recognition, color transformation, grid manipulation\n# description:\n# In the input, you will see a 15x15 grid with a pattern of purple pixels\nthat forms a shape and a blue\ncircle in the center of the grid.\n# To create the output, you should color the shape formed by the purple\npixels with a specific color based\non the number of purple pixels:\n# - If there are 1-10 purple pixels, color the shape yellow.\n# - If there are 11-20 purple pixels, color the shape green.\n# - If there are 21-30 purple pixels, color the shape red.\n# - If there are more than 30 purple pixels, color the shape orange.\n# The blue circle should be replaced by black.\ndef transform(input_grid):\n# Create an output grid initialized to black\noutput_grid = np.zeros_like(input_grid)\n# Count the number of purple pixels\npurple_count = np.count_nonzero(input_grid == Color.PURPLE)\n# Determine the color based on the number of purple pixels\nif 1 <= purple_count <= 10:\nshape_color = Color.YELLOW\nelif 11 <= purple_count <= 20:\nshape_color = Color.GREEN\nelif 21 <= purple_count <= 30:\nshape_color = Color.RED\nelse:\nshape_color = Color.ORANGE\n# Find the purple shape and color it accordingly\noutput_grid[input_grid == Color.PURPLE] = shape_color\n# Replace the blue circle with black\noutput_grid[input_grid == Color.BLUE] = Color.BLACK\nreturn output_grid\nD.3\nARC PROBLEM E95E3D8E\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# occlusion, symmetry detection, reconstruction\n# description:\n43\n\nPreprint.\n# In the input you will see a grid containing a repeated pattern that has\nbeen partially occluded by black\nsquares.\n# To make the output, remove the occluding black squares and restore the\npattern based on the detected\nsymmetry.\ndef transform(input_grid):\n# Plan:\n# 1. Extract the occluded pattern from the input grid.\n# 2. Detect the symmetries in the pattern.\n# 3. Fill in the missing parts of the pattern using the detected\nsymmetries.\n# Step 1: Identify non-black pixels to find the pattern\nocclusion_color = Color.BLACK\nnon_black_mask = input_grid != occlusion_color\nobjects = detect_objects(input_grid, background=occlusion_color,\nmonochromatic=False,\nconnectivity=8)\n# Step 2: Find the symmetries in the detected objects\nsymmetries = detect_translational_symmetry(input_grid, ignore_colors=\n[occlusion_color])\n# Step 3: Fill in the missing parts using the symmetries\noutput_grid = input_grid.copy()\nfor x, y in np.argwhere(non_black_mask):\nfor symmetric_x, symmetric_y in orbit(input_grid, x, y,\nsymmetries):\nif output_grid[symmetric_x, symmetric_y] == occlusion_color:\noutput_grid[symmetric_x, symmetric_y] = input_grid[x, y]\nreturn output_grid\nD.4\nARC PROBLEM 73CCF9C2\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# pattern extraction, rotation, centering\n# description:\n# In the input, you will see a grid with a pattern repeated several times\n, but with random rotations.\n# To make the output, extract one instance of the pattern and rotate it\nback to its original orientation.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a copy of the input grid for the output\noutput_grid = np.copy(input_grid)\n# Find connected components to extract patterns\ncomponents = find_connected_components(output_grid, background=Color.\nBLACK, connectivity=8)\n# For each component, check if it’s a valid pattern\nfor component in components:\n# Crop the component to get the pattern\npattern = crop(component, background=Color.BLACK)\n44\n\nPreprint.\n# Check if the pattern can be rotated back to its original\norientation\n# We can rotate the pattern and compare with the original\noriginal_pattern = pattern.copy()\n# Rotate the pattern in 90-degree increments and check if the\noriginal pattern matches\nrotated_patterns = [np.rot90(original_pattern, k) for k in range(\n4)]\n# We assume the first valid pattern is the original orientation\nfor rotated in rotated_patterns:\nif np.array_equal(rotated, original_pattern):\n# If it matches, we’ve found the original orientation\noutput_grid = pattern\nbreak\nreturn output_grid\nD.5\nARC PROBLEM D56F2372\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# object detection, symmetry detection, cropping\n# description:\n# In the input you will see a grid filled with different colored shapes.\n# The output should be the smallest symmetric shape (with respect to the\ncenter) that can fit all the others\ninside it.\n# If there are no symmetric shapes, the output should be a grid of the\nsame size filled with the background\ncolor.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Find all connected components (shapes) in the grid\ncomponents = find_connected_components(input_grid, background=Color.\nBLACK, connectivity=8,\nmonochromatic=False)\n# Initialize a variable to track the largest symmetric shape found\nlargest_symmetric_shape = None\nmax_area = 0\n# Check each component for symmetry and area\nfor component in components:\n# Crop the component to get its shape\ncropped_shape = crop(component, background=Color.BLACK)\n# Check for horizontal and vertical mirror symmetry\nis_symmetric = np.array_equal(cropped_shape, np.flip(\ncropped_shape, axis=0)) or \\\nnp.array_equal(cropped_shape, np.flip(\ncropped_shape\n, axis=1))\nif is_symmetric:\narea = np.count_nonzero(cropped_shape != Color.BLACK)\nif area > max_area:\nmax_area = area\n45\n\nPreprint.\nlargest_symmetric_shape = cropped_shape\n# If we found a symmetric shape, return it; otherwise return a grid\nof the same size filled with the\nbackground color\nif largest_symmetric_shape is not None:\nreturn largest_symmetric_shape\nelse:\nreturn np.full(input_grid.shape, Color.BLACK)\nE\nDATA AUGMENTATION AND RERANKING\nWe improve the performance of the transductive model through data augmentation and output\nreranking. The key insight is that any invertible transformation T can be applied to both train-\ning and test inputs, allowing us to generate multiple diverse predictions that are then aggregated and\nreranked. We consider two transformations of the training examples and test input in addition to\nthe original task: (i) transpositions, Tt(x) = xT ; (ii) color permutation, implemented as a random\npermutation of the integers 0-9 representing colors. For each transformation T, we apply it to all\ntraining examples and the test input, run a beam size 10 decoding on the transformed data, and then\napply T −1 to the predictions to return to the original space. The transposition transformation ad-\ndresses potential biases in the pretrained model regarding horizontal vs. vertical processing, while\ncolor permutations ensure the model isn’t relying on specific color values. For each transformation\nT, each candidate output y receives a beam search score sT (y) computed as:\nsT (y) = tθ\n\u0000T(y) | T(xtrain), T(xtrain), T(ytrain)\n\u0001\n(6)\nwhere tθ(·|·) is the transduction model. For ranking, we aggregate candidates across all transfor-\nmations. For each unique candidate y, we track both its frequency of appearance freq(y) across\ndifferent transformations and its average beam search score E [sT (y)]. These are then ranked with\nfrequency taking precedence over average score.\nF\nTEST-TIME TRAINING\nTest time training is an approach for updating model parameters at test time, which we apply\nto our transduction model.\nWe assume that we are given test problems D comprising triples\n(xtrain, ytrain, xtest) and a data augmentation procedure AUG(x, y) which constructs variations of\nan ARC problem, for example by permuting colors and rotating grids. Then model parameters are\noptimized to maximize the likelihood of augmented test tasks where a random training input-output\nis selected to serve as a fake test example:\nE\n(xtrain,ytrain,xtest)∼D\nk∼1..len(xtrain)\n(x′\ntrain,y′\ntrain)∼AUG(xtrain,ytrain)\n[tθ (ytrain[k] | xtrain[: k], xtrain[k + 1 :], ytrain[: k], ytrain[k + 1 :])]\n(7)\nwhere len(xtrain) is the number of example training inputs, and k is the index of the training input-\noutput which is randomly selected to serve as a fake testcase. Note that this procedure does not rely\non access to ground truth test predictions.\nWe choose each training example as a fake test example and do 10 times different randomly com-\nbined data augmentation for each fake task, which gives us 12k pseudo training dataset from the\nevaluation dataset. We also randomly include 5k RE-ARC examples and 5k ARC-Heavy examples,\nwhich we speculated would have a regularizing effect. The dataset size for test time training is 22k\ntotal problems.\nG\nEXPERIMENT PARAMETERS\nG.1\nINDUCTION\nFine-tuning Hyperparameters:\n46\n\nPreprint.\ntraining type\nlora rank\nlora alpha\nlearning rate\ngradient accumulate steps\nlora finetune\n64\n64\n2e-4\n2\nper device batch size\ndevice\nepcoh\nweight decay\nlearning rate scheduler type\n8\n8xA100\n3\n0\ncosine\nFor the last 230k data finetune we used full finetuning instead of LoRA:\ntraining type\nlearning rate\nweight decay\ngradient accumulate steps\nfull finetune\n1e-5\n0.05\n1\nepoch\nper device train batch\nlearning rate scheduler type\ndevices\n2\n16\ncosine\n8xA100\nInference Hyperparameters:\n• temperature: 0.8 (1.0 for the full-data fine-tuned model)\n• top-p: 1.0\nOutput selection: For experiments in section 4, when allowing one or two attempts, we filter the\nsample programs using train input-output examples and then randomly select one or two distinct\nprograms uniformly. We report the expected value in our results. For experiments in section 5, we\ntake the execution results of test output from the programs that can pass all the train examples and\nuse majority vote to select the top 2, and in the case of concept arc, the top 3 test outputs.\nG.2\nTRANSDUCTION\nFine-tune Hyperparameters:\ntraining type\nlearning rate\nweight decay\ngradient accumulate steps\ndevice\nfull finetune\n1e-5\n1e-2\n2\n8xA100\nengineer epoch\nother epoch\nper device train batch\nlearning rate scheduler type\n3\n2\n8\ncosine\nFor the final engineering results, we train for 3 epochs. For all other experiment results, we train for\n2 epochs.\nInference Hyperparameters:\n• temperature: 0\n• use beam search: True\n• beam width:\n1. engineer results: 40\n2. 100k data scale: 20\n3. all other experiment results: 3\n• top-p: 1.0\nTest-time Fine-tuning Hyperparameters\ntraining type\nlora rank\nlora alpha\nlearning rate\ngradient accumulate steps\nlora finetune\n64\n64\n2e-4\n2\nper device batch size\ndevice\nepcoh\nweight decay\nlearning rate scheduler type\n2\n4xA6000\n3\n0\ncosine\n47",
    "pdf_filename": "Combining_Induction_and_Transduction_for_Abstract_Reasoning.pdf"
}