{
    "title": "Preprint.",
    "abstract": "Wen-DingLi*1 KeyaHu*2 CarterLarsen1 YuqingWu1 SimonAlford1 CalebWoo1 SpencerM.Dunn1 HaoTang1 MichelangeloNaim3 DatNguyen3 Wei-LongZheng2 ZennaTavares†3 YewenPu†4KevinEllis†1 1Cornell 2ShanghaiJiaoTongUniversity 3Basis 4Autodesk *co-leads †co-advising correspondence: {wl678,kellis}@cornell.edu, hu_keya@sjtu.edu.cn When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predictnewtestoutputs,e.g. usinganeuralnetwork? Westudythisquestionon ARC-AGIbytrainingneuralmodelsforinduction(inferringlatentfunctions)and transduction(directlypredictingthetestoutputforagiventestinput).Wetrainon syntheticallygeneratedvariationsofPythonprogramsthatsolveARC-AGItrain- ingtasks. Wefindinductiveandtransductivemodelssolvedifferentkindsoftest problems,despitehavingthesametrainingproblemsandsharingthesameneural architecture: Inductive program synthesis excels at precise computations, and at composingmultipleconcepts, whiletransductionsucceedsonfuzzierperceptual concepts. Ensemblingthemapproacheshuman-levelperformanceonARC-AGI. 1 INTRODUCTION Robustgeneralizationfromfewexamplesremainsoneofthemostimportantwaysinwhichhuman ing: Given just a few training input-outputs x ,y , together with a test input x , the idea is train train test topredictthecorrespondingtestoutputy usingreasoningstrategiessuchasanalogicalreasoning, test chain-of-thought,inductiveprogramsynthesis,ortransductiveprediction(Thomsetal.,2023;Wang etal.,2024;Wittetal.,2023;Leeetal.,2024;Tangetal.,2024a;Hocquette&Cropper,2024;Butt few-shotlearningbenchmarkthatteststheabilitytorapidlylearnadiverserangeofnewskills,and applythemtonewsituations. EachARC-AGItaskispresentedasinput-outputsovercoloredgrids, but can engage concepts such as occlusion, pathfinding, collision, symmetry, gravity, bouncing, counting,etc.,makingARC-AGIessentiallyacompositeofmanyreasoningdatasets,andoneofthe moreinterestingunsolvedbenchmarksthatstressesbroad-coveragefew-shotlearning(Figure1). object contact gravity symmetry, occlusion jumping bouncing mazes pathfinding growth tasktypicallyhas2-5input-outputexamples. Hereweshowjustoneinput-outputexamplepertask. Here we study neural methods for induction and transduction, using few-shot learning problems fromARC-AGIasourtestbed. Inductionmeansfirstfindingafunctionf wheref(x ) ≈ y , train train andthenpredictingy = f(x ). Transductioninsteadoutputsy withoutexplicitconstruction test test test 1 4202 voN 91 ]GL.sc[ 3v27220.1142:viXra",
    "body": "Preprint.\nCOMBINING INDUCTION AND TRANSDUCTION FOR\nABSTRACT REASONING\nWen-DingLi*1 KeyaHu*2 CarterLarsen1 YuqingWu1 SimonAlford1 CalebWoo1\nSpencerM.Dunn1 HaoTang1 MichelangeloNaim3 DatNguyen3 Wei-LongZheng2\nZennaTavares†3 YewenPu†4KevinEllis†1\n1Cornell 2ShanghaiJiaoTongUniversity 3Basis 4Autodesk *co-leads †co-advising\ncorrespondence: {wl678,kellis}@cornell.edu, hu_keya@sjtu.edu.cn\nABSTRACT\nWhen learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to directly\npredictnewtestoutputs,e.g. usinganeuralnetwork? Westudythisquestionon\nARC-AGIbytrainingneuralmodelsforinduction(inferringlatentfunctions)and\ntransduction(directlypredictingthetestoutputforagiventestinput).Wetrainon\nsyntheticallygeneratedvariationsofPythonprogramsthatsolveARC-AGItrain-\ningtasks. Wefindinductiveandtransductivemodelssolvedifferentkindsoftest\nproblems,despitehavingthesametrainingproblemsandsharingthesameneural\narchitecture: Inductive program synthesis excels at precise computations, and at\ncomposingmultipleconcepts, whiletransductionsucceedsonfuzzierperceptual\nconcepts. Ensemblingthemapproacheshuman-levelperformanceonARC-AGI.\n1 INTRODUCTION\nRobustgeneralizationfromfewexamplesremainsoneofthemostimportantwaysinwhichhuman\nintelligencesurpassesAI.Muchrecentworkviewsthisgeneralizationasaformofabstractreason-\ning: Given just a few training input-outputs x ,y , together with a test input x , the idea is\ntrain train test\ntopredictthecorrespondingtestoutputy usingreasoningstrategiessuchasanalogicalreasoning,\ntest\nchain-of-thought,inductiveprogramsynthesis,ortransductiveprediction(Thomsetal.,2023;Wang\netal.,2024;Wittetal.,2023;Leeetal.,2024;Tangetal.,2024a;Hocquette&Cropper,2024;Butt\net al., 2024). The Abstraction and Reasoning Corpus (Chollet (2019), henceforth ARC-AGI) is a\nfew-shotlearningbenchmarkthatteststheabilitytorapidlylearnadiverserangeofnewskills,and\napplythemtonewsituations. EachARC-AGItaskispresentedasinput-outputsovercoloredgrids,\nbut can engage concepts such as occlusion, pathfinding, collision, symmetry, gravity, bouncing,\ncounting,etc.,makingARC-AGIessentiallyacompositeofmanyreasoningdatasets,andoneofthe\nmoreinterestingunsolvedbenchmarksthatstressesbroad-coveragefew-shotlearning(Figure1).\nobject contact gravity symmetry, occlusion jumping\nbouncing mazes pathfinding growth\nFigure 1: Few-shot learning tasks from the Abstraction and Reasoning Corpus (ARC-AGI). Each\ntasktypicallyhas2-5input-outputexamples. Hereweshowjustoneinput-outputexamplepertask.\nHere we study neural methods for induction and transduction, using few-shot learning problems\nfromARC-AGIasourtestbed. Inductionmeansfirstfindingafunctionf wheref(x ) ≈ y ,\ntrain train\nandthenpredictingy = f(x ). Transductioninsteadoutputsy withoutexplicitconstruction\ntest test test\n1\n4202\nvoN\n91\n]GL.sc[\n3v27220.1142:viXra\nPreprint.\nof an intermediate function f. Intuitively, induction captures the notion that a learner should first\nexplainthetrainingdata,thenusethatexplanationtomakepredictions. Inductivelearnerscanper-\nformbetterbyspendingmoretimeoptimizingorsearchingforbetterexplanations,usingthetraining\nexamplesx ,y toscorecandidatefunctions. Transductioninsteadcapturestheintuitionthat\ntrain train\nthe training examples themselves should play a direct role in generating new predictions, and that\nsuccessfulpredictionneednotrequireanexplicitexplanation. SeeFigure2.\ninduction transduction\nx y\nxxttrraaiin y tyrtaraini\ntrain train\nn n\nf(•)\ny\ntest y\nf(x ) test\ntest\nx test xx tteesstt\nFigure 2: Induction generates an intermediate function f to explain training input-outputs. Trans-\nductiondirectlypredictsthetestoutput,forexampleusinganeuralnetwork.\nWetrainneuralnetworksforbothinductionandtransductionbygeneratingalargecorpusofsyn-\ntheticproblems.Wediscoverthatneuralmodelsforinductionandtransductionarestronglycomple-\nmentary.Webelievethisissurprising:Althoughanypairofmodelswouldgenerallysolvesomewhat\ndifferent problems, usually this can be attributed to different priors, data, or architecture. Instead,\nwe find that, even controlling for priors, data, and architecture, most problems solved by induc-\ntionwerenotsolvedbytransduction,andviceversa. Moreover,inductionandtransductioncanbe\ntriviallyensembledbyusinginductiontogeneratecandidatefunctions f untileitherasatisfactory\nfunction is found (e.g. f(x ) = y ) or until a test-time compute budget is reached, at which\ntrain train\npoint,transductionkicksinasafallback: Thattheyarecomplementaryhaspracticalimplications.\nOurstudyistightlylinkedtoprogramsynthesis. Werepresentfunctionsf asPythoncode, mean-\ning that induction synthesizes programs. We train transduction models on LLM-produced Python\nscripts,meaningthattransductionistrainedontheinput-outputsofsymboliccode. Althoughpro-\ngram learning has long been a popular vision of how general AI could work (Solomonoff, 1964;\nSchmidhuber,2004;Hutter,2004),thedominanttheoryhasalwaysbeenoneofexplicitcodegen-\neration (induction), rather than implicitly teaching neural networks to imitate code (transduction).\nOurworkputsthisassumptiontothetest.\nTestingtheseneuralmethodsrequiresalargedatasetoffunction-learningproblems,whichischal-\nlengingtogeneratebecausenotonlymustwemakenovelfunctions, butwemustalsomakegood\ninputstothosefunctions. ConsidertherangeoftransformationsinFigure1: Whatcountsasagood\ninputforonefunctionisunlikelytoworkforanother. Toaddressthischallenge,ourdatagenerator\nfirstproducesadeterministicPythonfunctionforf,andthenaprobabilisticprogramforsampling\ninputs to f, finally executing those programs to produce input-outputs. This helps generate func-\ntion inputs that are appropriate for the underlying transformation, and constrains x ,y to be\ntrain test\nexplainablebyadeterministicmapping.\nWecontributethefollowing:\n1. A study finding that neural models for induction and transduction are strongly complementary,\neven when trained on the same problems. This contradicts seminal neural program synthesis\nwork(Devlinetal.(2017),whichfoundinductionsuperior),andcontradictsthefindingsofthe\nleadingARCteam(Coleetal.(2024),whichadvocatestransductionwithtest-timetraining).\n2. Anautomateddatagenerationmethodologythatstartswith100-160programsolutionsforARC\ntrainingtasks,andexpandsthemtomake400knewproblemspairedwithPythonsolutions.\n3. A study of how these methods scale. We find performance saturates quickly when increasing\nmanually-labelleddata,butscaleswithcompute,bothattrainingandtestingtime.\n4. Analysisoffamiliesofproblemssolvedbyeachapproach,andhowtheycomparetohumans.\n2\nPreprint.\n2 NEURAL MODELS FOR INDUCTION AND TRANSDUCTION\nWe consider few-shot supervised learning problems where the learner is trained to map members\nof an input space X to output space Y. For K-shot learning, we receive K training input-outputs\n(x ,y ) ∈ XK ×YK,togetherwithasingletestinputfromx ∈ X,andpredicty ∈ Y.\ntrain train test test\nOur neural models for K-shot learning are meta-learned (Mishra et al., 2017, inter alia.) using\nmeta-learningdatafurtherannotatedwithaground-truthfunctionf :X →Y,whichsupervisesthe\ninductionmodel. Belowwedefinethetraininganduseofthesemodels.\nDefinition: Neuralnetworksforinductionandtransduction. Aneuralnetworkfortransduction\nisafunctiontthatmaps(x ,y ,x )toadistributionovery ,andwhichhaslearnablepa-\ntrain train test test\nrametersθ.Inotherwords,t :XK×YK×X →∆(Y),wherethenotation∆(S)meansthesetof\nθ\ndistributionsoverS. Wecanalsowritethisasaconditionaldistribution,t (y |x ,y ,x ).\nθ test train train test\nA neural network for induction is a function i that maps (x ,y ,x ) to a distribution over\ntrain train test\nfunctionsf thatmapX toY,withlearnableparametersθ. Inotherwords,i :XK ×YK ×X →\nθ\n∆(X →Y),whichwecanwriteasaconditionaldistributioni (f|x ,y ,x ).\nθ train train test\nTraining induction and transduction. Both types of models are trained via meta-learning. We\nassume a meta-learning dataset D of few-shot learning problems, each equipped with a ground-\ntruth function f such that f(x) = y for every x,y in (x ,y ) and (x ,y ). Inductive and\ntrain train test test\ntransductivemodelsaremeta-trainedtominimizethefollowinglosses:\nTRANSDUCTIONLOSS =E (xtrain,ytrain,xtest,ytest,f)∼D[−logt θ(y test|x train,y train,x test)] (1)\nINDUCTIONLOSS =E (xtrain,ytrain,xtest,ytest,f)∼D[−logi θ(f|x train,y train,x test)] (2)\nTestinginductionandtransduction. Aftermeta-learningthemodelsencounteratest-timefew-\nshot learning task (x ,y ,x ). Transductive models predict their most likely output for\ntrain train test\ny (approximated via beam search). Inductive models sample a test-time budget of B functions\ntest\nf ···f , which are filtered by (x ,y ), and finally used to predict y = f(x ). Writing\n1 B train train test test\nyˆ forthepredictedtestoutput:\ntest\nTRANSDUCTION: yˆ\ntest\n=argmaxt θ(y|x train,y train,x test) (3)\ny∈Y\nINDUCTION: yˆ\ntest\n∼Uniform(F) (4)\nwhereF ={f (x ) : for1≤b≤Biff (x )=y }\nb test b train train\nf ∼i (f|x ,y ,x )\nb θ train train test\nCombininginductionandtransduction. Inductionallowscheckingcandidatehypothesesagainst\nthe training examples. Therefore, we know when induction has found a plausible solution–but\nsometimes it fails to find any solution. Transduction has the opposite property: We can’t check\nif its predictions match the training examples, but it always offers a candidate answer. Therefore\nwe ensemble by attempting induction first, then transduction if none of the candidate hypotheses\nexplainedtheexamples:\nENSEMBLE: yˆ\ntest\n∼Uniform(F) ifF ̸=∅\nyˆ =argmaxt (y|x ,y ,x )ifF =∅ (5)\ntest θ train train test\ny∈Y\nInstantiatingtheframeworkforARC-AGI. EveryinputfromX andoutputfromY isa2Dgrid\nranging from 1–30 pixels per side, with each pixel containing one of ten colors. Because ARC-\nAGI tasks are highly diverse yet typically have an abstract program-like structure, we represent\nthe underlying function f as Python code, which is computationally universal, and so possible in\nprincipleofsolvinganyARC-AGItask. ThereforetheinductionmodelmustgeneratePythoncode,\nsoweinitializeourmodelswithLlama3.1-8B-instruct(Dubeyetal.,2024)becauseitwaspretrained\nonsourcecode.1 Weencode2Dcoloredgridsasstringsusing1tokenperpixel,andusenewlinesto\ndelimit rows (Appendix B.1). We then meta-learn by further fine-tuning Llama3.1-8B-instruct for\ninductionortransductionusingasynthetically-generatedcorpusofproblems,describednext.\n1Our preliminary experiments suggested Llama3.1-8B-instruct was better than Mistral-7B-v0.3, Qwen2-\n7B-Instruct,anddeepseek-coder-6.7b-instruct\n3\nPreprint.\n3 GENERATING DATASETS FOR INDUCTION AND TRANSDUCTION\nGenerating ARC-style tasks is challenging because of the diversity of concepts that can occur in\nARC-AGI. It is also challenging because we need to generate not just a function, and also inputs\nthatserveasgoodexamplesforthatfunction.\nAtahighlevel,ourdatasetgrowsoutof100manually-writtenPythonprograms,eachofwhichboth\nsolves a given task (function f), and also randomly generates new input grids. We call these 100\nmanually-written programs seeds. Each seed is commented with natural language describing the\ncoreintuitionsbehindtheproblem. Wethenusealargelanguagemodeltomutateandrecombine\ntheseeds,producingmanythousandsofprograms(Figure3).\n100seedsolutions 100kseedproblemswithsolutions\n100seedproblems language language\nconcepts:cropping concepts:\nexamples description: description:\nIn the input you will see a single colored code\nshape, around 4x6 in size, floating in a\n12x12 grid of black.\nsolve To make the output, crop the background remix\nout of the image - so the output grid has runtimecheck\nthe same dimensions as the shape.\nexamples\ncode\nmanualproblemsolving syntheticaugmentation\nFigure3: Syntheticdatagenerationpipeline,startingwithhuman-writtenprograms(seeds).\nThestructureofseeds. Eachseedconsistsofthreeparts:\n1. A natural language description of its specific ARC task—including how to solve that\ntask—representedasaPythoncommentatthetopoftheseed.\n2. APythonfunctiontransform_gridcorrespondingtothefunctionf inthemanuscript,\nwhichmapseachinputgridofaspecificARCtasktoitscorrespondingoutputgrid.\n3. APythonfunctiongenerate_input, whichtakesnoarguments, andwhichrandomly\ngeneratesnewinputstof (newinputstotransform_grid).\nPriorknowledge. Theseedsimpartaprioruponthesystembydemonstratinggoodprogramsfor\nsolvingtrainingtasks. WefurthercodifiedmuchofthispriorintoaPythonlibrarycontainingcode\nthatwefoundusefulacrossmanyseeds,suchassubroutinesforgeneratingrandomsprites,detecting\nsymmetries,orextractingobjects(AppendixA.2). Syntheticproblemscanusethatsamelibrary.\nHowever, this prior knowledge is different from previous Domain Specific Languages for ARC-\nAGI(Buttetal.,2024;Wind,2020;Ainoosonetal.,2023). DomainSpecificLanguagesrestrictthe\nclassofallowedprogramsbyonlyallowingstereotypedcombinationsofdomain-specificprimitives.\nWestillallowarbitraryPythoncode,whichhelpscoverthelongtailofdiversetasks.\nRemixing the seeds. To generate a larger synthetic dataset we “remix” the seeds using LLMs.\nEachnewsyntheticARCproblemisgeneratedbyathreestagepipeline(Figure11):\n1. AnewnaturallanguagedescriptionissampledbypromptinganLLMwithseednaturallan-\nguagedescriptions,effectivelyusingin-contextlearningtorecombineandmutateelements\nofdifferentproblems,inthespiritofself-instruct(Wangetal.,2023).\n2. Code is generated for that new description via Retrieval Augmented Generation\n(RAG: Lewis et al. (2020)). Our RAG pipeline retrieves seeds with similar descriptions,\nandpromptsanLLMtogeneratecodeforthenewdescription,giventheretrievedseeds.\n3. The newly created generate_input is executed to make inputs, which are passed to\ntransform_gridtoproduceinput-outputgrids.\n4\nPreprint.\nFigure4illustratesexampleproblemsgeneratedbyourpipeline,withfurtherexamplesvisualizedat\nthislink. Unlessotherwisementioned,wecreatesyntheticdatasetswithGPT4o-miniandada-002.\nconcepts: concepts: concepts:\nshape detection, contour color blending, pattern detection, copying,\nextraction, bounding box radial expansion positioning\nFigure4: ExamplesyntheticARCproblemsgeneratedbyourpipeline. Conceptsaregeneratedina\ncommentnearthetopofthePythonscriptaspartofthenaturallanguagedescriptionoftheseed.\n4 EMPIRICAL STUDY OF INDUCTION AND TRANSDUCTION\nWe train inductive and transductive models with the goal of understanding (1) how the methods\ncompare; (2) how performance scales with train-time effort; and (3) how performance scales with\ntest-time compute (for induction only, as it allows drawing more samples at test time to improve\nperformance). We report performance on the 400-problem public validation split of ARC-AGI,\nwhich is harder than the training split. The systems described in this section learn from a 100-\nproblemsubsetofthetrainingsplit,specificallyproblemsforwhichwecreatedseeds.\nInduction and Transduction are strongly complementary. Despite training on the exact same\nproblems,inductiveandtransductivemodelssolvedifferenttasks,andneitherapproachisdramat-\nicallymoreeffectivethantheother. Andalthoughthesemethodshaveasimilaroverallsolverate,\nmostproblemssolvedbyinductionarenotsolvedbytransduction,andviceversa(Figure5A).\nAn alternative explanation is that induction and transduction are not actually complementary, but\ninstead that, having trained two neural networks with different random initializations, they simply\nsolveddifferentproblemsduetorandomnessattrainortesttime.Totestthisalternativeexplanation,\nwetrainedmanymodelswithdifferentrandominitializations. Wefindthattheproblemssolvedby\ninduction/transductionaresurprisinglystableacrossthesedifferentruns(Figure5B).Inotherwords,\nsomeproblemsarefriendliertoinduction,andothersfriendliertotransduction(Figure6).\n(A) complementary methods (B) finding is stable across random seeds, and (C) statistically significant\ntransduction induction\n1.0\np<.004\n0.0\n= 1 problem solved\nFigure5:(A)Inductionandtransductionsolvedifferentproblems,wheresolvemeanspredictingthe\nrightoutputgiven2tries. Venndiagramformodelstrainedon100ksyntheticproblemsgenerated\nusinggpt4o-mini. (B)Trainingmanymodelswithdifferentrandomseeds,andthenmeasuringthe\ncorrelation between solved tasks by different models. Solved tasks strongly correlates with other\nmodelsofthesameclassbutnottheotherclass. (C)Statisticalsignificancetestevaluatingthenull\nhypothesisthatcorrelationisindependentofwhetheramodelisinductive/transductive.\n5\nnoitcudsnart\nnoitcudni\nPreprint.\nsolved only by induction\nsolved only by transduction\nsolved by both\nFigure6: Exampletaskssolvedbyinduction/transduction/both. SeealsoAppendixC.\nPerformancescaleswithdatasetsize, butquicklysaturateswithincreasingnumberofseeds.\nWe trained models while systematically varying the number of human-created seeds we use, and\nvaryingtheamountofsyntheticdatageneratedfromthoseseeds(Figure7). Performanceimproves\nwith increasing training data for fine-tuning (increasing synthetic data), but saturates for increas-\ning quantity of human-created seeds. We conjecture that this saturation occurs because each seed\nservestointroduceafewcoreconcepts,andthatafterenoughseeds,essentiallyalloftheimportant\nconceptshavebeendemonstrated. Thissuggeststhat,beyondacriticalthresholdnumberofseeds,\nthe method can scale with increasing amounts of compute without demanding further human data\nlabeling. Looking beyond ARC-AGI, this means that our methodology could probably be applied\ntootherfew-shotfunction-learningproblemsusingamodestamountofmanualdatalabeling.\nincreasing synthetic data, 100 seeds increasing seeds, 40k synthetic problems\n0.150 Induction Induction\nTransduction Transduction\n0.125\n0.100\n0.075\n0.050\n20 40 60 80 100 25 50 100\nnum synthetic problems (10^3) num seeds\nFigure7: Increasedmanualhumaneffort(#seeds)doesnotsignificantlyincreaseperformance,but\nincreasingcomputespentgeneratingsyntheticdataincreasesperformanceofthefinalmodel.\n6\nycaruccA\nnoitadilaV\nPreprint.\nInductionperformancescaleswithtest-timecompute. Wevarythetest-timesamplingbudget\nfor induction, finding an almost monotonic increase in solve rate (Figure 8, left). In principle,\ndrawingadditionalsamplesrunstheriskofdiscoveringsolutionsthatare“false-positives,”meaning\ntheyfitthetrainingexampleswithoutcorrectlypredictingthetestoutput. Inpractice,about9%of\nsamplesthatfitthetrainingexamplesarefalse-positives. Figure8(right)showsthatabouthalfof\nthis 9% corresponds to problems where the majority of the probability mass is still placed on the\ncorrectoutput, meaningthatasimplemajorityvoteschemewouldsquashanyfalsepositives(e.g.\nclusteringinAlphaCodeLietal.(2022)). AppendixDshowsexamplefalsepositives.\nARC Val Accuracy\n20%\n15%\n60\n10% 40\nSample + Oracle\n5%\nSample + Filter 20\nCodeIt\n0%\n0\n0 500 1000 1500 2000 0.0 0.2 0.4 0.6 0.8 1.0\nnumber of samples False Positive Rate\nFigure 8: Left: Sample+Oracle assumes an oracle that selects one of the sampled programs. It\nupper-bounds the accuracy of randomly selecting one program consistent with the training exam-\nples(Sample+Filter). Inductionmodeltrainedwith100kgpt4-description-gpt4omini-codegendata.\nCodeIt (Butt et al., 2024) is a recent neural program induction model for ARC-AGI. Right: His-\ntogram of false positive rate. Of the problems that have false positives, about half have a false\npositiveratelessthan0.5,meaningthatmost(filtered)samplespredicttherighttestoutput.\nStronger LLMs make better synthetic data, and induction is more sensitive to data quality.\nTo save costs, the previous results all used GPT4o-mini to generate synthetic data. To understand\nthevalueofstrongermodelswegenerated100ksyntheticproblemsusingGPT4togeneratenatural\nlanguage problem descriptions (but GPT4o-mini still generated the code). The richer and more\ndiverse synthetic problems elicited from GPT4 significantly improved performance, but primarily\nforinduction,whiletransductionwaslesssensitivetodataquality(Table1).\nTable1: ValAcc: %validationtasksthatarecorrectlysolvedin2tries.\nSystem ValAcc. FinetuningData\nEnsemble 26.50%\nInduction,2048samples 18.78% GPT-4forgeneratingdescriptions,GPT-4o-miniforcode\nTransduction,beamsize20 15.25%\nEnsemble 19.50%\nInduction,2048samples 11.07% GPT-4o-miniforgeneratingdescriptionsandcode\nTransduction,beamsize20 13.50%\n5 SCALING OUR METHOD\nMotivatedbyourfindingssofar,wescaledupourmethodbyproducingtwolargerdatasets:\nARC-Heavy: 200kproblemsfrom160seeds. ThepurposeofARC-Heavyistoscaleourmethod\ninaneasilyreproducibleway,whilealsofillinganygapsinitsmasteryofthetrainingsplit. Wefirst\nranmodelsfromSection4onthetrainingsplittoidentify60problemsthattheystillstruggledwith,\nforwhichweproduced60newseeds,giving160seedsintotal.Fromthoseseedsweproduced200k\nsyntheticproblems,withGPT4generatingnaturallanguagedescriptionsandGPT4o-minigenerat-\ningthecorrespondingPythoncode.\n7\netaR\nevloS\nsmelborp\nnoitadilav\n#\nPreprint.\nARC-Potpourri: 400kproblemsfromheterogeneoussources. ThepurposeofARC-Potpourriis\nto assemble the biggest dataset that we could, even if it comes from a messy mixture of sources.\nStarting with ARC-Heavy we added all synthetic data from Section 4. We further added 100k\ntransduction-onlytrainingexamplesfromReARC(Hodel,2024).\nTest-timeimprovements. Weimprovetransductionwithtest-timetraining(abbreviatedTTT;Sun\netal.(2020))andarerankingschemethataugmentseachproblemandpredictsthemostlikelyoutput\nundermultipleaugmentations(AppendixE-F).Weexpandoursamplingbudgetto20kprograms.\nWe call our resulting systems BARC. Table 2 shows the performance of various BARC models.\nBoth transduction and induction are effective, with induction solving slightly more problems, un-\ntil adding test-time training/reranking, after which transduction does slightly better. An ensem-\nble scores 56.75%, surpassing previously published methods. Releasing this work later allowed\nAkyürek et al. (2024) to improve that score to 61.9% via better test time training of our transduc-\ntionmodel,whileusingourinductionmodelas-is. Butabsentourmodels—includingtheprogram\nsynthesizer—theyinsteadscorelower,suggestingthatsophisticatedtesttimetrainingdoesnotfully\nsubstitute for program synthesis. Our best model performs nearly as well as the average human\n(56.75%vs. 60.2%)butmuchworsethanthebesthumans(98%). Modeloutputsvisualizedhere.\nTable2: %validationtaskscorrectlysolvedin2tries. HumanresultsfromLeGrisetal.(2024).\nSystem ValAcc. Faircomparison?\nARC-Heavy: BARCmodels\nInduction,10ksamples,majorityvote 30.50% —\nTransduction(noTTT) 19.25% —\nEnsemble(noTTT) 37.50% —\nTransduction(TTT) 29.75% —\nEnsemble(TTT) 43.25% —\nARC-Potpourri: BARCmodels\nInduction,20ksamples,majorityvote 38.00% —\nTransduction(noTTT,noreranking) 29.125% —\nTransduction(reranking,noTTT) 35.25% —\nTransduction(TTT,noreranking) 39.25% —\nTransduction(TTT+reranking) 43.00% —\nEnsemble(TTT+reranking) 56.75% —\nCodeIt(Buttetal.,2024) 15% Yes,onlytrainsontrainingset\nClaude-3.5/Greenblatt(2024) 21%/42% Yes,butclosedLLMsattesttime\nWind(2020) 39% No,designedbylookingatvalset\nAkyüreketal.(2024),w/oourmodels 47.1% Yes\n+ensembledwith&usingbothourmodels 61.9% No,buildsonourmodels\nAvg/BestHuman 60.2%/97.8% Yes\nScalingdownourmethod. Ourflagshipmodelistooexpensivetorunontheprivatetestsethosted\nbyKaggle.Wescaledownbyomittingtest-timetraining,onlysampling336programs,andreducing\nthetransductionbeamsizeto3.Thisscores19%onKaggleand36.5%onvalidation.Table3shows\nthat program synthesis is less effective given this smaller search budget. Given the large-compute\neffectivenessofprogramsynthesis,thissuggestsastrongpayoffforsmarterneuralprogramsearch.\nTable3: Smallerversionofourmodelevaluatedontheprivatetestandpublicvalidationsplits\nPrivateTestSet PublicValidationSet\nTransduction(noTTT,beamsize3) 18% 32.25%\nInduction,384samples 4% 14%\nEnsemble 19% 36.5%\n8\nPreprint.\n6 WHICH PROBLEMS ARE EASIER FOR THE MODELS, AND FOR HUMANS?\nDo problems that challenge humans also\nchallenge the model, and vice-versa? We\nsort ARC-AGI validation problems into 5 transduction+TTT\n0.8 induction easiest 20%\nequally-sized difficulty classes using data\nensemble\nfrom LeGris et al. (2024). Figure 9 illus-\ntrates a peculiar relationship between human\nand model accuracy: All models surpass hu- 0.6 hardest 20%\nmanperformanceonthehardestproblems, but\nunderperformontheeasiestproblems.Because\nour models train on simple Python programs, 0.4\nthissuggestssomeproblemsaresimpleincode\nand learnable by transformers, but very hard\nfor people—and conversely that people pos- 0.2\n0.2 0.4 0.6 0.8\nsesspriorsallowingeffortlesssolutionofprob-\nhuman % solved\nlems beyond what our Python program gener-\nFigure9: Humanvsmodelperformanceacross5\nator produces. For problems of typical diffi-\ndifficulty levels. The easiest difficulty level con-\nculty, the model roughly tracks human perfor-\ntainsproblemsinthetop20%ofhumanaccuracy,\nmance,andacrossalldifficultylevels,transduc-\nand the hardest difficulty level contains the 20%\ntion and induction serve complementary roles,\nofproblemswiththelowesthumanaccuracy.\nevenwhenaugmentedwithtesttimetraining.\nWhichconceptsareeasierforthemodels? WetestonConceptARC(Moskvichevetal.,2023),\nanalternativeARC-styletest-setwhichclassifiesitstasksinto“conceptgroups”eachexemplifying\nasingleisolatedhigh-levelconceptsuchas“sameness”or“abovevsbelow.” Weusemodelstrained\nonARC-Potpourri, findingthatspecificconceptcategoriesareeasierforinductionortransduction\n(Figure 10). We find an intuitive division of labor between the two approaches: Concept groups\nsuchascountingarebestsolvedwithsymboliccode, whiletransductionbetterhandlesperceptual\nprocessessuchasjudgingwhetherashapeismorehorizontalormorevertical,ormoretop/bottom.\nConceptARC reveals another dimension along which transduction and induction differ: Because\nConceptARC illustrates one concept per problem, there is no need to compose many concepts to-\ngether. Thereforetheinductionmodel,whichisuniquelyequippedforsymboliccomposition,loses\na key advantage. Transduction has more limited composition capabilities but can instantiate indi-\nvidualconceptsinflexiblesubsymbolicways,whichcouldexplainwhyitexcelsonConceptARC.\ninduction pass@1 transduction\nFigure10: ConceptARCaccuracybyconceptgroup. Conceptgroupssortedleft-to-rightbyratioof\ninductivetotransductiveperformance. IceCubeistheoriginalARCKagglewinner(Wind,2020).\nWereportpass@3becauseMoskvichevetal.(2023)reportaccuracygiven3attempts.\n9\ndevlos\n%\nledom\nPreprint.\n7 RELATED WORK\nARC-AGI wasoriginallydesignedtochallengeconventionaldeeplearningandspurprogressonal-\nternativeparadigms(Chollet,2019). Thefirstwaveofsuccessfulapproachesuseddiscreteprogram\nsearchoverdomain-specificprogramminglanguages,includingtheoriginalKagglewinner(Wind,\n2020). These symbolic approaches held their own against GPT-4 (Wang et al., 2024), but have\nrecently been surpassed by transductive architectures using test-time training (Cole et al., 2024),\nand by LLM-guided program generation (Greenblatt, 2024). ARC-AGI has so far resisted con-\nventional neural and symbolic approaches, but is solvable for adult humans, and to some extent,\nchildren(LeGrisetal.,2024;Opielkaetal.,2024).\nCodegenerationviaLLMsisdoneinmanyrecentworks(Lietal.,2022;Gaoetal.,2023;Chen\netal.,2021;Austinetal.,2021). WemostdirectlybuildonLi&Ellis(2024)andGreenblatt(2024).\nThe former fine-tunes LLMs for inductive program synthesis using LLM-generated variations of\nhuman-writtenprograms. While therearemanytechnicaldifferences, akeyfactoristhat wegen-\neratefunctioninputsbysynthesizinganinput_generatorfunction, ratherthanhaveanLLM\ndirectlygeneratepossibleinputs. ThismattersbecauseanLLMalonecouldnotgeneratecomplex,\nprecisely-correct inputs such as ARC grids. This potentially makes our work applicable to other\nfew-shot generalization problems with complex input-spaces such as webpages, robot planning,\netc. Greenblatt(2024)samplesmanyPythonprogramsfromGPT4o: Comparabletoourinduction\nmodel,butinsteadoffine-tuning,itusesprompting. Fine-tuningforcedustocreateadatasetofnew\nproblems,whichcreatedtheopportunityforexploringtransductivemodels.\nClassicworkinneuralprogramsynthesishaspreviouslycomparedinductionandtransduction(Ro-\nbustFill:Devlinetal.(2017)). Weexploreherearicherspaceoffunctions,reachingaqualitatively\ndifferent conclusion than RobustFill: Instead of finding transduction inferior to induction, we find\nthemcomplementary. Morebroadly, thetransductive-inductivedivideliesneartheheartofsuper-\nvised learning. Inductive approaches, such as linear regression, first construct a function f where\nf(x )≈y ,andthenpredicty =f(x ). Transductiveapproaches,suchasSupportVector\ntrain train test test\nMachines and In-Context Learning, instead output their predictions by performing direct compar-\nisons with the training data. We use the same neural network architecture and dataset to perform\nbothtasks,allowingacontrolledcomparisonbetweentheseparadigms.\nDatasets for ARC-AGI. ReARC (Hodel, 2024) is a dataset of handwritten programs that solve\nall ARC-AGI training tasks, and which generates new inputs for them. ReARC is implemented\ninadomain-specificlanguage,andlacksnaturallanguageannotations,makingitdifficulttoremix\nwithLLMs. OtherworksannotateARC-AGIusingeithernaturallanguage(Acquavivaetal.,2022)\nor Python programs (Huang et al., 2024), which could potentially serve as seed programs for our\nwork. Acquaviva et al. (2022) inspired our natural-language descriptions and Huang et al. (2024)\ninfluencedourseedprogramformat. Ournewseedswereabetterfitforthisapproachbecausethey\nencodesharedpriorsinaPythonlibrary(AppendixA.2),andhaveanexplicitinput_generator\ndescribingaprecisely-structuredinfinitespaceofvalidinputs.\n8 DISCUSSION\nWhat we learn about robust sample-efficient generalization. Neither explicit symbolic hy-\npotheses nor implicit neural representations suffice to solve all problems: each has their own do-\nmain of applicability, and simply ensembling models specialized in each does not cover all cases.\nEngineeringamorecleverneuralprogramsearch,ortrainingtransductivepredictorsonmoredata,\nisunlikelytobefruitful. Insteadweneedrepresentationsirreducibletoapurelyneuralorsymbolic\nform,whichintertwineinductiveandtransductivereasoning. Onewayofimplementingthisideais\ntodoprogramsynthesiswithinalanguagewhoseatomicprimitivesarenon-symbolic, andtopre-\ntrainthoseprimitivestoencapsulatethebasicatomsofcoreknowledge. Whileworkhastakensteps\ninrelateddirections(Reed&DeFreitas,2015;Aletetal.,2018;Tang&Ellis,2023;Lietal.,2024),\nhowtoengineerandscalethisidearemainsopen.\nTowhatextentisthismethodologyapplicablebeyondARC-AGI? Few-shotfunctionlearning\nisaveryflexibleframework,butourparticularmethodismostapplicablewhenthetargetgeneral-\nizationcanbedescribedinsymboliccode. Asanimmediatelytangibleexample,webscrapingand\n10\nPreprint.\notherformsofdata-mungingcouldfitwithinourframework. Asamoreambitiousgoal, symbolic\ncode is an especially good medium for defining precise models of how the world works. This is\ntruebothwithinthenaturalsciences(Schmidt&Lipson,2009)andalsowithinAI,withexamples\nsuchasroboticpolicies(Liangetal.,2023),planners(Wongetal.,2023),andworldmodelsmore\nbroadly(Dasetal.,2023;Tangetal.,2024b;Evansetal.,2021;Liangetal.,2024). Thesearenot\nthe kinds of programs that occur often in LLM pretraining data—so merely prompting is unlikely\ntoperform well—butitis nonethelessfeasibleto curatearound100 seedsdemonstratingwhat the\nsystemshouldlearn.\nTheoretically,inductionandtransductionshouldnotbesocomplementary. Equivalencesbe-\ntweeninductionandtransductionarewell-know,suchasthe‘kerneltrick’whichallowstranslating\nparametric function fitting into a transductive problem. Our metalearning models, given infinite\nmetatraining data, should similarly converge because transformers are universal function approxi-\nmators. That there remains a difference is interesting precisely because it deviates from what one\nwouldexpecttheoretically.\nArewecheatingbytrainingon400ksyntheticproblems? ThespiritofARC-AGIistogener-\nalizefromfewexamples. Yetwefine-tuneonmanyexamples. Inourview,thetruetrainingdatais\n160seeds,notthe400k‘remixes,’whichareinsteadanalogousto‘dreamdata’inamortizedinfer-\nence or wake-sleep (Le et al., 2017; Ritchie et al., 2016; Hinton et al., 1995). In other words, our\nsystem inputs 160 annotated solutions to training set problems, and does up-front computation to\nconvertthatdataintoaneuralnetworkcapableofsolvingnewproblems. Fromthatperspective,it\nisasampleefficientwayoflearningtosolveARC-AGI—althoughitisnotcomputeefficient.\nImpactonARC-AGIefforts. ReleasingourcodeanddatahelpedAkyüreketal.(2024)achieve\nthefirstopen-sourcesystemthatperformsatthelevelofanaveragehuman. The2ndplaceARC-\nAGI ’24 team (Franzen et al., 2024) also benefited from our data. We hope others will continue\nbuildingonourwork. Wealsointendourfindingstoencourageresearchondiscreteprogramsearch\nas an alternative to the test-time training currently dominating in the community, and have geared\nourexperimentstowardshowingthevalueinthisotherpathway.\nFrom domain-specific languages to domain-specific libraries. Many works that perform pro-\ngram search rely on carefully tuned domain-specific languages instead of using a general purpose\nlanguagesuchasPython(Buttetal.,2024;Wind,2020;Alfordetal.,2022;Ainoosonetal.,2023).\nHowever,webelievegeneral-purposeprogramminglanguagescangivemuchbroadercoverage,and\nthat attempts to engineer restricted languages inevitably sacrifice regions of program-space which\ncouldplausiblyoccurinopen-endedlearningdomainssuchasARC.Insteadweadvocateherefor\ndomain-specific libraries, which equip a general-purpose language with extra priors, but do not\nrestrictittousingonlythosepriors.\nHowtorepresentinput-outputmappings. Ourseedsinclude:(1)agridtransformationprogram,\n(2) an input generator, and (3) natural language descriptions for both (1) and (2). Practically, this\nrepresentation allows us to sample consistent input-output example pairs for training, while the\nnaturallanguagedescriptionshelpLLMstoremixseedsintonovelproblems. Thisfurthercaptures\na latent natural language description of both inputs and outputs, from which the function and its\npreimagearederived.\nNextstepssuggestedbybiologicalintelligence&wake-sleep. Ourworkhasastraightforward\nanalogy to dual-process models in psychology, which categorizes human thinking according to\nwhetheritreliesonfastnonverbalintuitionsordeliberativeconsciousthought(Smith&DeCoster,\n2000; Kahneman, 2011). Although preliminary, our results could suggest that this partitioning is\nactuallynormative,andemergesfrompropertiesoftheproblemsbeingsolved,notpropertiesofthe\nsolveritself. Humanthinkingishowevermoresophisticatedinhowitcombinesthesemodes: Fast\nintuitionscanbefurtherreprocessedbydeliberativesymbolicprocessing,whichcantriggerfurther\nintuitions, and so on. Our method has no analogous way of interleaving these two strategies, but\nmoredeeplyintegratinginductionandtransductionisanaturalnextstep.\nOur approach can also be seen as a form of wake-sleep or dream learning (Hinton et al., 1995;\nFosseetal.,2003;Rasch&Born,2013)wheresamplesfromagenerativemodeltrainaninference\n11\nPreprint.\nnetwork. Here the generative model is a prompt with the seeds, and the inference network is our\nfine-tunedmodels.Drawingtheanalogytowake-sleepsuggeststwodirections.First,wecouldlearn\nfromrecently-solvedtestproblems(during‘waking’)bygeneratingfreshsyntheticdatausingthose\nproblems as seeds (during ‘dreaming/sleeping’). Second, we could also implement a form of ab-\nstractionlearningthatautomaticallyexpandsourcustomARClibrary(AppendixA.2),oraddsnew\nneuralprimitivestothatlibrary,inthespiritoflibrarylearningandmodularmetalearning(Bowers\netal.,2023;Ellisetal.,2021;Aletetal.,2018).\nLimitations. Oursystemdoesnotgrowmorecompetentatfew-shotlearningbysolvingnewprob-\nlems: Instead, it bootstrapsfrom manuallyencoded knowledgein theseeds, whichis transformed\nintoafew-shotlearnerviaanLLMtraining/inferencepipeline. Amorecompellingapproachwould\nbetohavethesystemdiscoverforitselftheknowledgethatwecompiledforitwithintheseeds,for\ninstancebypracticingontrainingtasks,withoutsupervisingongroundtruthsolutions.\nOurworkisonlyevaluatedonARC-AGI.However,ARC-AGIisdesignedtocontainmanyconcepts\nandproblemsembeddedwithinit,socanbeviewedasanopen-endedcompositeofdifferentlearning\nproblems.Owingtothisdiversity,itisalsonotoriouslychallenging,andhasresistedsolutiondespite\na series of high-profile competitions. We therefore believe that although evaluating on multiple\nbenchmarks is desirable, ARC-AGI is an appropriate benchmark to use as the centerpiece of an\nexperimentalevaluation.\nCode & Data Availability. Our code, data, and model weights are freely available at https:\n//github.com/xu3kev/BARC. Interactivevisualizationsofourdatasetandmodeloutputsare\navailableatthislink.\nAuthor Contributions. Neural network experiments were engineered by Wen-Ding Li and Keya\nHu. Data generation was engineered by Carter Larsen, Wen-Ding Li, Keya Hu, and Kevin Ellis.\nKevinEllis,YewenPu,ZennaTavares,Wei-LongZheng,andHaoTangprovidedhigh-leveladvisory\nguidance. ZennaTavares,MichelangeloNaim,DatNguyen,andKeyaHuanalyzedthetransduction\nmodel. Seeds were written by Keya Hu, Kevin Ellis, Carter Larsen, Yuqing Wu, Simon Alford,\nCaleb Woo, Spencer M. Dunn, and Yewen Pu. The paper was written by Yewen Pu, Keya Hu,\nZennaTavares,Wen-DingLi,andKevinEllis.\nAcknowledgements. WearegratefulforadvicefromRobertHawkinsregardingthestatisticalanal-\nysis in Figure 5C and for discussions with Weinan Sun about biological learning. This work was\npartlysupportedbyanNSFCAREERawardtoK.E.\nREFERENCES\nSam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle\nEcanow,MaxwellNye,MichaelHenryTessler,andJoshuaB.Tenenbaum. Communicatingnat-\nuralprogramstohumansandmachines. InThirty-sixthConferenceonNeuralInformationPro-\ncessingSystemsDatasetsandBenchmarksTrack,2022. URLhttps://openreview.net/\nforum?id=OxFoLTKDcNm.\nJames Ainooson, Deepayan Sanyal, Joel P. Michelson, Yuan Yang, and Maithilee Kunda. A\nneurodiversity-inspiredsolverfortheabstraction&reasoningcorpus(arc)usingvisualimagery\nandprogramsynthesis,2023. URLhttps://arxiv.org/abs/2302.09425.\nEkinAkyürek,MehulDamani,LinluQiu,HanGuo,YoonKim,andJacobAndreas. Thesurprising\neffectivenessoftest-timetrainingforabstractreasoning,2024. Preprint.\nFerranAlet,TomásLozano-Pérez,andLesliePKaelbling. Modularmeta-learning. InConference\nonrobotlearning,pp.856–868.PMLR,2018.\nSimon Alford, Anshula Gandhi, Akshay Rangamani, Andrzej Banburski, Tony Wang, Sylee Dan-\ndekar,JohnChin,TomasoPoggio,andPeterChin. Neural-guided,bidirectionalprogramsearch\nfor abstraction and reasoning. In Complex Networks & Their Applications X: Volume 1, Pro-\nceedings of the Tenth International Conference on Complex Networks and Their Applications\nCOMPLEXNETWORKS202110,pp.657–668.Springer,2022.\n12\nPreprint.\nJacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXivpreprintarXiv:2108.07732,2021.\nMatthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin\nEllis, andArmandoSolar-Lezama. Top-downsynthesisforlibrarylearning. POPL,2023. doi:\n10.1145/3571234. URLhttps://doi.org/10.1145/3571234.\nNatashaButt,BlazejManczak,AukeWiggers,CorradoRainone,DavidWZhang,MichaëlDeffer-\nrard,andTacoCohen. Codeit: Self-improvinglanguagemodelswithprioritizedhindsightreplay.\nInInternationalConferenceonMachineLearning,2024.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\nPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,WilliamSaunders,\nChristopherHesse,AndrewN.Carr,JanLeike,JoshAchiam,VedantMisra,EvanMorikawa,Alec\nRadford,MatthewKnight,MilesBrundage,MiraMurati,KatieMayer,PeterWelinder,BobMc-\nGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciechZaremba. Evaluatinglarge\nlanguagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.\nFrançoisChollet. Onthemeasureofintelligence,2019.\nJack Cole, Mohamed Osman, Michael Hodel, Keith Duggar, and Tim Scarfe. Machine learning\nstreettalk,June2024.\nRiaDas,JoshuaBTenenbaum,ArmandoSolar-Lezama,andZennaTavares. Combiningfunctional\nand automata synthesis to discover causal reactive programs. Proceedings of the ACM on Pro-\ngrammingLanguages,7(POPL):1628–1658,2023.\nJacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and\nPushmeetKohli. Robustfill: Neuralprogramlearningundernoisyi/o. ICML,2017.\nAbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\nArunRao,AstonZhang,AurelienRodriguez,AustenGregerson,AvaSpataru,BaptisteRoziere,\nBethanyBiron,BinhTang,BobbieChern,CharlotteCaucheteux,ChayaNayak,ChloeBi,Chris\nMarra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\nCristianCantonFerrer,CyrusNikolaidis,DamienAllonsius,DanielSong,DaniellePintz,Danny\nLivshits, DavidEsiobu, DhruvChoudhary, DhruvMahajan, DiegoGarcia-Olano, DiegoPerino,\nDieuwkeHupkes,EgorLakomkin,EhabAlBadawy,ElinaLobanova,EmilyDinan,EricMichael\nSmith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Ander-\nson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah\nKorevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-\nhadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy\nFu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak,\nJongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Al-\nwala,KartikeyaUpasani,KatePlawiak,KeLi,KennethHeafield,KevinStone,KhalidEl-Arini,\nKrithikaIyer,KshitizMalik,KuenleyChiu,KunalBhalla,LaurenRantala-Yeary,Laurensvander\nMaaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo,\nLukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Man-\nnat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova,\nMelanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,\nNarjesTorabi,NikolayBashlykov,NikolayBogoychev,NiladriChatterji,OlivierDuchenne,Onur\nÇelebi,PatrickAlrassy,PengchuanZhang,PengweiLi,PetarVasic,PeterWeng,PrajjwalBhar-\ngava, PratikDubal, PraveenKrishnan, PunitSinghKoura, PuxinXu, QingHe, QingxiaoDong,\n13\nPreprint.\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic,\nRobertaRaileanu,RohitGirdhar,RohitPatel,RomainSauvestre,RonniePolidoro,RoshanSum-\nbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa,\nSanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang,\nSharathRaparthy,ShengShen,ShengyeWan,ShrutiBhosale,ShunZhang,SimonVandenhende,\nSoumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney\nBorodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom,\nTobiasSpeckbacher,TodorMihaylov,TongXiao,UjjwalKarn,VedanujGoswami,VibhorGupta,\nVigneshRamanathan,ViktorKerkez,VincentGonguet,VirginieDo,VishVogeti,VladanPetro-\nvic,WeiweiChu,WenhanXiong,WenyinFu,WhitneyMeers,XavierMartinet,XiaodongWang,\nXiaoqingEllenTan,XinfengXie,XuchaoJia,XueweiWang,YaelleGoldschlag,YasheshGaur,\nYasmineBabaei,YiWen,YiwenSong,YuchenZhang,YueLi,YuningMao,ZacharieDelpierre\nCoudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha\nJain,AdamKelsey,AdamShajnfeld,AdithyaGangidi,AdolfoVictoria,AhuvaGoldstand,Ajay\nMenon,AjaySharma,AlexBoesenberg,AlexVaughan,AlexeiBaevski,AllieFeinstein,Amanda\nKallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew\nGu, AndrewHo, AndrewPoulton, AndrewRyan, AnkitRamchandani, AnnieFranco, Aparajita\nSaraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh\nYazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De\nPaola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Bran-\ndonSpence,BraniStojkovic,BrianGamido,BrittMontalvo,CarlParker,CarlyBurton,Catalina\nMejia,ChanghanWang,ChangkyuKim,ChaoZhou,ChesterHu,Ching-HsiangChu,ChrisCai,\nChris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li,\nDanny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich,DidemFoss,DingkangWang,DucLe,DustinHolland,EdwardDowling,EissaJamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Ar-\ncaute,EvanDunbar,EvanSmothers,FeiSun,FelixKreuk,FengTian,FiratOzgenel,Francesco\nCaggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella\nSchwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory\nSizov,Guangyi,Zhang,GunaLakshminarayanan,HamidShojanazeri,HanZou,HannahWang,\nHanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Gold-\nman,IbrahimDamlaj,IgorMolybog,IgorTufanov,Irina-ElenaVeliche,ItaiGat,JakeWeissman,\nJamesGeboski,JamesKohli,JaphetAsher,Jean-BaptisteGaya,JeffMarcus,JeffTang,Jennifer\nChan,JennyZhen,JeremyReizenstein,JeremyTeboul,JessicaZhong,JianJin,JingyiYang,Joe\nCummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie\nWang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun\nZand,KathyMatosich,KaushikVeeraraghavan,KellyMichelena,KeqianLi,KunHuang,Kunal\nChawla,KushalLakhotia,KyleHuang,LailinChen,LakshyaGarg,LavenderA,LeandroSilva,\nLeeBell,LeiZhang,LiangpengGuo,LichengYu,LironMoshkovich,LucaWehrstedt,Madian\nKhabsa, ManavAvalani, ManishBhatt, MariaTsimpoukelli, MartynasMankus, MatanHasson,\nMatthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Ke-\nneally,MichaelL.Seltzer,MichalValko,MichelleRestrepo,MihirPatel,MikVyatskov,Mikayel\nSamvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mo-\nhammadRastegari,MunishBansal,NandhiniSanthanam,NataschaParks,NatashaWhite,Navy-\nataBawa,NayanSinghal,NickEgebo,NicolasUsunier,NikolayPavlovichLaptev,NingDong,\nNing Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli,\nParkinKent,ParthParekh,PaulSaab,PavanBalaji,PedroRittner,PhilipBontrager,PierreRoux,\nPiotrDollar,PolinaZvyagina,PrashantRatanchandani,PritishYuvraj,QianLiang,RachadAlao,\nRachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li,\nRebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott,\nSai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Sa-\ntadruPan,SaurabhVerma,SeijiYamamoto,SharadhRamaswamy,ShaunLindsay,ShaunLind-\nsay,ShengFeng,ShenghaoLin,ShengxinCindyZha,ShivaShankar,ShuqiangZhang,Shuqiang\nZhang,SinongWang,SnehaAgarwal,SojiSajuyigbe,SoumithChintala,StephanieMax,Stephen\nChen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho,\nSunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser,\nTamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Tim-\nothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,\nVinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu\n14\nPreprint.\nMihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Con-\nstable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu,\nXinboGao,YanjunChen,YeHu,YeJia,YeQi,YendaLi,YilinZhang,YingZhang,YossiAdi,\nYoungjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef\nRosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024.\nURLhttps://arxiv.org/abs/2407.21783.\nKevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke He-\nwitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrap-\nping inductive program synthesis with wake-sleep library learning. In PLDI, 2021. doi:\n10.1145/3453483.3454080. URLhttps://doi.org/10.1145/3453483.3454080.\nRichardEvans,MatkoBošnjak,LarsBuesing,KevinEllis,DavidPfau,PushmeetKohli,andMarek\nSergot. Makingsenseofrawinput. ArtificialIntelligence,299:103521,2021. ISSN0004-3702.\ndoi: https://doi.org/10.1016/j.artint.2021.103521. URL https://www.sciencedirect.\ncom/science/article/pii/S0004370221000722.\nMagdalena J Fosse, Roar Fosse, J Allan Hobson, and Robert J Stickgold. Dreaming and episodic\nmemory: afunctionaldissociation? Journalofcognitiveneuroscience,15(1):1–9,2003.\nDanielFranzen,JanDisselhoff,andDavidHartmann.Thellmarchitect:Solvingthearcchallengeis\namatterofperspective,2024. URLhttps://github.com/da-fr/arc-prize-2024/\nblob/main/the_architects.pdf. Preprint.\nLuyuGao, AmanMadaan, ShuyanZhou, UriAlon, PengfeiLiu, YimingYang, JamieCallan, and\nGrahamNeubig. Pal: Program-aidedlanguagemodels. InInternationalConferenceonMachine\nLearning,pp.10764–10799.PMLR,2023.\nRyanGreenblatt. Drawmoresamples. \"https://redwoodresearch.substack.com/p/\ngetting-50-sota-on-arc-agi-with-gpt\",2024. AccuracyfromARCPrizeLeader-\nboard.\nGeoffreyEHinton,PeterDayan,BrendanJFrey,andRadfordMNeal. The\"wake-sleep\"algorithm\nforunsupervisedneuralnetworks. Science,268(5214):1158–1161,1995.\nCéline Hocquette and Andrew Cropper. Relational decomposition for program synthesis. arXiv\npreprintarXiv:2408.12212,2024.\nMichael Hodel. Rearc. https://github.com/michaelhodel/re-arc, 2024. [Online\nGitHubrepository].\nDiHuang,ZiyuanNan,XingHu,PengweiJin,ShaohuiPeng,YuanboWen,RuiZhang,ZidongDu,\nQi Guo, Yewen Pu, et al. Anpl: towards natural programming with interactive decomposition.\nAdvancesinNeuralInformationProcessingSystems,36,2024.\nMarcusHutter. Universalartificialintelligence: Sequentialdecisionsbasedonalgorithmicproba-\nbility. SpringerScience&BusinessMedia,2004.\nDanielKahneman. Thinking,fastandslow. macmillan,2011.\nTuanAnhLe,AtilimGunesBaydin,andFrankWood. InferenceCompilationandUniversalProba-\nbilisticProgramming. InAartiSinghandJerryZhu(eds.),Proceedingsofthe20thInternational\nConferenceonArtificialIntelligenceandStatistics,volume54ofProceedingsofMachineLearn-\ningResearch,pp.1338–1348.PMLR,20–22Apr2017.URLhttps://proceedings.mlr.\npress/v54/le17a.html.\nSeungpil Lee, Woochang Sim, Donghyeon Shin, Wongyu Seo, Jiwon Park, Seokki Lee, Sanha\nHwang, Sejin Kim, and Sundong Kim. Reasoning abilities of large language models: In-depth\nanalysis on the abstraction and reasoning corpus, 2024. URL https://arxiv.org/abs/\n2403.11793.\nSolimLeGris,WaiKeenVong,BrendenM.Lake,andToddM.Gureckis. H-arc: Arobustestimate\nofhumanperformanceontheabstractionandreasoningcorpusbenchmark,2024. URLhttps:\n//arxiv.org/abs/2409.01374.\n15\nPreprint.\nPatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,\nHeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,etal. Retrieval-augmentedgenera-\ntionforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems,33:\n9459–9474,2020.\nChengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey\nLevine,LiFei-Fei,FeiXia,andBrianIchter. Chainofcode: Reasoningwithalanguagemodel-\naugmentedcodeemulator.InRuslanSalakhutdinov,ZicoKolter,KatherineHeller,AdrianWeller,\nNuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st Interna-\ntional Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Re-\nsearch, pp. 28259–28277. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.\npress/v235/li24ar.html.\nWen-DingLiandKevinEllis. Isprogrammingbyexamplesolvedbyllms?,2024.\nYujia Li, David Choi, JunyoungChung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom\nEccles,JamesKeeling,FelixGimeno,AgustinDalLago,etal.Competition-levelcodegeneration\nwithalphacode. Science,378(6624):1092–1097,2022.\nJackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,and\nAndyZeng. Code aspolicies: Languagemodel programsfor embodiedcontrol. In 2023IEEE\nInternationalConferenceonRoboticsandAutomation(ICRA),pp.9493–9500.IEEE,2023.\nYichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver,\nJoãoF.Henriques,andKevinEllis.Visualpredicator:Learningabstractworldmodelswithneuro-\nsymbolic predicates for robot planning, 2024. URL https://arxiv.org/abs/2410.\n23156.\nNikhilMishra,MostafaRohaninejad,XiChen,andPieterAbbeel. Asimpleneuralattentivemeta-\nlearner. arXivpreprintarXiv:1707.03141,2017.\nArseniiKirillovichMoskvichev,VictorVikramOdouard,andMelanieMitchell. TheconceptARC\nbenchmark: Evaluating understanding and generalization in the ARC domain. Transactions on\nMachine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/\nforum?id=8ykyGbtt2q.\nGustaw Opielka, Hannes Rosenbusch, Veerle Vijverberg, and Claire E. Stevenson. Do large lan-\nguagemodelssolvearcvisualanalogieslikepeopledo?,2024. URLhttps://arxiv.org/\nabs/2403.09734.\nBjörnRaschandJanBorn. Aboutsleep’sroleinmemory. Physiologicalreviews, 93(2):681–766,\n2013.\nScott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint\narXiv:1511.06279,2015.\nDaniel Ritchie, Paul Horsfall, and Noah D Goodman. Deep amortized inference for probabilistic\nprograms. arXivpreprintarXiv:1610.05735,2016.\nJürgenSchmidhuber. Optimalorderedproblemsolver. MachineLearning,54(3):211–254,2004.\nMichaelSchmidtandHodLipson.Distillingfree-formnaturallawsfromexperimentaldata.science,\n324(5923):81–85,2009.\nEliotRSmithandJamieDeCoster. Dual-processmodelsinsocialandcognitivepsychology: Con-\nceptualintegrationandlinkstounderlyingmemorysystems. Personalityandsocialpsychology\nreview,4(2):108–131,2000.\nRay J Solomonoff. A formal theory of inductive inference. Information and control, 7(1):1–22,\n1964.\nYuSun,XiaolongWang,ZhuangLiu,JohnMiller,AlexeiEfros,andMoritzHardt. Test-timetrain-\ningwithself-supervisionforgeneralizationunderdistributionshifts. InInternationalconference\nonmachinelearning,pp.9229–9248.PMLR,2020.\n16\nPreprint.\nHaoTangandKevinEllis. Fromperceptiontoprograms: Regularize,overparameterize,andamor-\ntize. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nandJonathanScarlett(eds.),Proceedingsofthe40thInternationalConferenceonMachineLearn-\ning,volume202ofProceedingsofMachineLearningResearch,pp.33616–33631.PMLR,23–29\nJul2023. URLhttps://proceedings.mlr.press/v202/tang23c.html.\nHao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin Ellis.\nCoderepairwithllmsgivesanexploration-exploitationtradeoff. NeurIPS,2024a.\nHao Tang, Darren Key, and Kevin Ellis. Worldcoder, a model-based llm agent: Building world\nmodelsbywritingcodeandinteractingwiththeenvironment. NeurIPS,2024b.\nLuca H. Thoms, Karel A. Veldkamp, Hannes Rosenbusch, and Claire E. Stevenson. Solv-\ning arc visual analogies with neural embeddings and vector arithmetic: A generalized\nmethod. ArXiv, abs/2311.08083, 2023. URL https://api.semanticscholar.org/\nCorpusID:265158110.\nRuocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman.\nHypothesissearch: Inductivereasoningwithlanguagemodels. ICLR,2024.\nYizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,DanielKhashabi,and\nHannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.\nInThe61stAnnualMeetingOfTheAssociationForComputationalLinguistics,2023.\nJohan Sokrates Wind. 1st place 2020 arc kaggle. https://github.com/top-quarks/\nARC-solution,2020. [OnlineGitHubrepository].\nJonas Witt, Stef Rasing, Sebastijan Dumancˇic´, Tias Guns, and Claus-Christian Carbon. A divide-\nalign-conquer strategy for program synthesis, 2023. URL https://arxiv.org/abs/\n2301.03094.\nLionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S Siegel, Jiahai Feng, Noa Korneev,\nJoshuaBTenenbaum,andJacobAndreas. Learningadaptiveplanningrepresentationswithnatu-\nrallanguageguidance. ICLR,2023.\n17\nPreprint.\nA DATA GENERATION TECHNICAL DETAILS\nStep1: Random remix to generate new language descriptions\nPrompt example\nrandomly sample\nLLM new language\n100 seed language\nproblems concepts: new concepts:\nnew description:\ndescription:\nStep2: RAG to generate new codes Prompt example\ncalculate cosine get four codes\nsimilarity with with the most language\nthe seeds similar language concepts:\nlanguage part part with the description:\nlann ge uw a ge new description code LLM new code\nnew concepts: 100 seed\nnew description: problems\nnew language\nnew concepts:... new description:...\nFigure11: AnewnaturallanguagedescriptionissampledbypromptinganLLMwithseednatural\nlanguage descriptions, effectively using in-context learning to recombine and mutate elements of\ndifferentproblems. CodeisgeneratedforthatnewdescriptionviaRetrievalAugmentedGeneration\n(RAG).OurRAGpipelineretrievesseedswithsimilardescriptions,andpromptsanLLMtogenerate\ncodeforthenewdescription,giventheretrievedseeds.\nTheprompttemplateforgeneratingnaturallanguagedescriptionsbyrandomlysamplinglanguage\ndescriptionsfromseedproblemsisasfollows:\nYou’ve generated these on previous requests:\n{examples}\nBrainstorm {num_generations} more, using similar thinking:\n‘‘‘python\n# concepts:\n# <concepts in your new generation>\n# description:\n# <description of your new generation>\n‘‘‘\nTheprompttemplateforgeneratingPythoncodefromthenatural-languagedescriptions(andsimilar\nexamplecoderetrievedfromtheseeds,viaRAG)isasfollows:\nYou are a puzzle maker designing geometric, physical, and topological\npuzzles for curious middle-schoolers.\nEach puzzle consists of uncovering a deterministic rule, pattern, procedure,\nalgorithm, or transformation law that maps inputs to outputs.\nBoth the inputs and outputs are 2D grids of colored pixels. There are 10\ncolors, but the order of the colors is never relevant to the puzzle.\n18\nPreprint.\nThe middle schoolers are trying to discover this deterministic\ntransformation, which can be implemented as a Python function called ‘main‘.\nDesigning a puzzle involves also creating example inputs, which can be\nimplemented as a Python function called ‘generate_input‘. Unlike ‘main‘, the\n‘generate_input‘ function should be stochastic, so that every time you run\nit, you get another good example of what the transformation can be applied\nto.\nHere is a overview of the puzzle you are designing:\n{description}\nPlease implement the puzzle by writing code containing the ‘generate_input‘\nand ‘main‘ functions. Use the following standard library (‘common.py‘):\n‘‘‘python\n{common_lib}\n‘‘‘\nHere are some examples from puzzles with similar descriptions to show you\nhow to use functions in ‘common.py‘:\n{examples}\nYour task is to implement the puzzle, following these steps:\n1. Inspect the example puzzle implementations, making note of the functions\nused and the physical/geometric/topological/logical details\n2. Inspect the new puzzle’s description\n3. Brainstorm a possible implementation for the new puzzle\n4. Generate a code block formatted like the earlier examples with a comment\nstarting ‘# concepts:‘ listing the concepts and ‘# description:‘ describing\nthe inputs and transformation from the given description.\nBe sure to make the transformation ‘main‘ deterministic. Follow the\ndescription closely.\nExecutionandFilteringofGeneratedProblems Weheuristicallyfilterproblemstoimprovethe\nqualityofdatabasedonthefollowingcriteria:\n• The generator and transformation functions can be executed, producing at least 4 input-\noutputgridsexamples.\n• Transformation being deterministic: We check for consistency by running the functions\nwithdifferentrandomseedsandfilteroutthosewithnon-deterministicoutputs.\n• Appropriategridsizes: Weremoveinput-outputgridswithheightorwidthlargerthan30,\naligningwithgridsizesinARC\n• Colorpermutationcheck:Sinceweusenumpyarrayswithintegers0-9torepresentcolors,\nwewanttoensuretransformationsdon’trelyonarithmeticoperationsoftheseintegers.We\nfilter this by checking if input-output remains consistent when permuting the underlying\ncolor-numbermapping.\n• Removalofproblemswithalltrivialidentityinput-outputexamples.\n19\nPreprint.\nA.1 SEEDEXAMPLES\nExampleseed0d3d703e Exampleseed1b2d62fb Exampleseed0dfd9992\nFigure12: Threeseedexamples\n\"\"\"============ problem id: 0d3d703e ============\"\"\"\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color mapping\n# description:\n# The input is a grid where each column is of the same color.\n# To make the output, change each color according to the following\nmapping:\n# green -> yellow, blue -> gray, red -> pink, teal -> maroon, yellow ->\ngreen, gray -> blue, pink -> red,\nmaroon -> teal\ndef transform_grid(input_grid):\n# Initialize output grid\noutput_grid = input_grid.copy()\n# Performs color mapping\noutput_grid = np.vectorize(lambda color: color_map.get(color, color))\n(output_grid)\nreturn output_grid\n# Constructing the color map\ncolor_map = {Color.GREEN : Color.YELLOW,\nColor.BLUE : Color.GRAY,\nColor.RED : Color.PINK,\nColor.TEAL : Color.MAROON,\nColor.YELLOW : Color.GREEN,\nColor.GRAY : Color.BLUE,\nColor.PINK : Color.RED,\nColor.MAROON : Color.TEAL\n}\ndef generate_input():\ngrid = np.full((3, 3), Color.BLACK)\nfor x in range(grid.shape[0]):\ngrid[x, :] = random.choice(list(color_map.keys()))\nreturn grid\n\"\"\"============ problem id: 1b2d62fb ============\"\"\"\nimport numpy as np\nfrom typing import *\nfrom common import *\n20\nPreprint.\n# concepts:\n# boolean logical operations, bitmasks with separator\n# description:\n# In the input you will see two maroon bitmasks separated by a blue\nvertical bar\n# To make the output, color teal the pixels that are not set in either\nbitmasks (logical NOR)\ndef transform_grid(input_grid: np.ndarray) -> np.ndarray:\n# Find the blue vertical bar. Vertical means constant X\nfor x_bar in range(input_grid.shape[0]):\nif np.all(input_grid[x_bar, :] == Color.BLUE):\nbreak\nleft_mask = input_grid[:x_bar, :]\nright_mask = input_grid[x_bar+1:, :]\noutput_grid = np.zeros_like(left_mask)\noutput_grid[(left_mask != Color.MAROON) & (right_mask != Color.MAROON\n)] = Color.TEAL\nreturn output_grid\ndef generate_input() -> np.ndarray:\n# create a pair of equally sized maroon bitmasks\nwidth, height = np.random.randint(2, 10), np.random.randint(2, 10)\ngrid1 = np.zeros((width, height), dtype=int)\ngrid2 = np.zeros((width, height), dtype=int)\nfor x in range(width):\nfor y in range(height):\ngrid1[x, y] = np.random.choice([Color.MAROON, Color.BLACK])\ngrid2[x, y] = np.random.choice([Color.MAROON, Color.BLACK])\n# create a blue vertical bar\nbar = np.zeros((1, height), dtype=int)\nbar[0, :] = Color.BLUE\ngrid = np.concatenate((grid1, bar, grid2), axis=0)\nreturn grid\n\"\"\"============ problem id: 0dfd9992 ============\"\"\"\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# occlusion, translational symmetry\n# description:\n# In the input you will see a translationally symmetric pattern randomly\noccluded by black pixels.\n# To make the output, remove the occluding black pixels to reveal the\ntranslationally symmetric pattern.\ndef transform_grid(input_grid):\n# Plan:\n# 1. Find the translational symmetries\n21\nPreprint.\n# 2. Reconstruct the sprite by ignoring the black pixels and\nexploiting the symmetry\nw, h = input_grid.shape\n# Identify the translational symmetries\ntranslations = detect_translational_symmetry(input_grid,\nignore_colors=[Color.BLACK])\nassert len(translations) > 0, \"No translational symmetry found\"\n# Reconstruct the occluded black pixels by replacing them with colors\nfound in the orbit of the\nsymmetries\noutput_grid = np.copy(input_grid)\nfor x in range(w):\nfor y in range(h):\nif output_grid[x, y] == Color.BLACK:\n# Use the translational symmetry to fill in the occluded\npixels\n# to do this we compute the ORBIT of the current pixel\nunder the\ntranslations\n# and take the most common non-black color in the orbit\n# Compute the orbit into the output\norbit_pixels = orbit(output_grid, x, y, translations)\norbit_colors = {input_grid[transformed_x, transformed_y]\nfor transformed_x, transformed_y in\norbit_pixels}\n# occluded by black, so whatever color it is, black doesn\n’t count\norbit_colors = orbit_colors - {Color.BLACK}\n# Copy the color\nassert len(orbit_colors) == 1, \"Ambiguity: multiple\ncolors in the orbit\"\noutput_grid[x, y] = orbit_colors.pop()\nreturn output_grid\ndef generate_input():\n# Make a random large canvas\ngrid = np.full((np.random.randint(15, 30), np.random.randint(15, 30))\n, Color.BLACK)\n# Make the basic sprite\nw, h = random.randint(3, 8), random.randint(3, 8)\nsprite = random_sprite(w, h, density=1, color_palette=Color.NOT_BLACK\n)\n# Place the sprite in the canvas\nfor x in range(0, grid.shape[0], w):\nfor y in range(0, grid.shape[1], h):\nblit_sprite(grid, sprite, x, y)\n# Create random occluders\nn_occluders = random.randint(1, 5)\nfor _ in range(n_occluders):\nx, y = random.randint(0, grid.shape[0]), random.randint(0, grid.\nshape[1])\nw, h = random.randint(3, 7), random.randint(3, 7)\noccluder_sprite = np.full((w, h), Color.BLACK)\nblit_sprite(grid, occluder_sprite, x, y)\n22\nPreprint.\nreturn grid\nA.2 COMMONLIBRARY\n23\nPreprint.\n\"\"\"Common library for ARC\"\"\"\nimport numpy as np\nimport random\nclass Color:\n\"\"\"\nEnum for colors\nColor.BLACK, Color.BLUE, Color.RED, Color.GREEN, Color.YELLOW,\nColor.GREY, Color.PINK, Color.ORANGE, Color.TEAL, Color.MAROON\nUse Color.ALL_COLORS for ‘set‘ of all possible colors\nUse Color.NOT_BLACK for ‘set‘ of all colors except black\nColors are strings (NOT integers),\nso you CAN’T do math/arithmetic/indexing on them.\n(The exception is Color.BLACK, which is 0)\n\"\"\"\ndef flood_fill(grid, x, y, color, connectivity=4):\n\"\"\"\nFill the connected region that contains the point (x, y) with\nthe specified color.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\n8-way counts diagonals as connected,\n4-way only counts cardinal directions as connected.\n\"\"\"\ndef draw_line(grid, x, y, end_x=None, end_y=None, length=None, direction=\nNone,\ncolor=None, stop_at_color=[]):\n\"\"\"\nDraws a line starting at (x, y) extending to (end_x, end_y) or\nof the specified length in the specified direction\nDirection should be a vector with elements -1, 0, or 1.\nIf length is None, then the line will continue until it hits\nthe edge of the grid.\nstop_at_color: optional list of colors that the line should stop at.\nIf the line hits a pixel of one of these colors, it will stop.\nExample:\n# blue diagonal line from (0, 0) to (2, 2)\ndraw_line(grid, 0, 0, length=3, color=blue, direction=(1, 1))\ndraw_line(grid, 0, 0, end_x=2, end_y=2, color=blue)\n\"\"\"\ndef find_connected_components(grid, background=Color.BLACK, connectivity=\n4,\nmonochromatic=True):\n\"\"\"\nFind the connected components in the grid.\nReturns a list of connected\ncomponents, where each connected component is a numpy array.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\nmonochromatic: if True, each connected component is assumed to have\nonly one color.\nIf False, each connected component can include multiple colors.\n\"\"\"\n24\nPreprint.\ndef random_scatter_points(grid, color, density=0.5,\nbackground=Color.BLACK):\n\"\"\"\nRandomly scatter points of the specified color in the grid with\nspecified density.\n\"\"\"\ndef scale_pattern(pattern, scale_factor):\n\"\"\"\nScales the pattern by the specified factor.\n\"\"\"\ndef blit_object(grid, obj, background=Color.BLACK):\n\"\"\"\nDraws an object onto the grid using its current location.\nExample usage:\nblit_object(output_grid, an_object, background=background_color)\n\"\"\"\ndef blit_sprite(grid, sprite, x, y, background=Color.BLACK):\n\"\"\"\nDraws a sprite onto the grid at the specified location.\nExample usage:\nblit_sprite(output_grid, the_sprite, x=x, y=y,\nbackground=background_color)\n\"\"\"\ndef bounding_box(grid, background=Color.BLACK):\n\"\"\"\nFind the bounding box of the non-background pixels in the grid.\nReturns a tuple (x, y, width, height) of the bounding box.\nExample usage:\nobjects = find_connected_components(input_grid, monochromatic=True,\nbackground=Color.BLACK, connectivity=8)\nteal_object=[obj for obj in objects if np.any(obj == Color.TEAL)][0]\nteal_x, teal_y, teal_w, teal_h = bounding_box(teal_object)\n\"\"\"\ndef object_position(obj, background=Color.BLACK, anchor=\"upper left\"):\n\"\"\"\n(x,y) position of the provided object.\nBy default, the upper left corner.\nanchor: \"upper left\", \"upper right\", \"lower left\", \"lower right\",\n\"center\", \"upper center\", \"lower center\", \"left center\", \"right\ncenter\"\nExample usage:\nx, y = object_position(obj, background=background_color,\nanchor=\"upper left\")\nmiddle_x, middle_y = object_position(obj, background=background_color\n,\nanchor=\"center\")\n\"\"\"\ndef crop(grid, background=Color.BLACK):\n\"\"\"\nCrop the grid to the smallest bounding box that contains all\nnon-background pixels.\nExample usage:\n25\nPreprint.\n# Extract a sprite from an object\nsprite = crop(an_object, background=background_color)\n\"\"\"\ndef translate(obj, x, y, background=Color.BLACK):\n\"\"\"\nTranslate by the vector (x, y). Fills in the new pixels with the\nbackground color.\nExample usage:\nred_object = ... # extract some object\nshifted_red_object = translate(red_object, x=1, y=1)\nblit_object(output_grid, shifted_red_object,\nbackground=background_color)\n\"\"\"\ndef collision(_=None, object1=None, object2=None, x1=0, y1=0, x2=0, y2=0,\nbackground=Color.BLACK):\n\"\"\"\nCheck if object1 and object2 collide when object1 is at (x1, y1) and\nobject2 is at (x2, y2).\nExample usage:\n# Check if a sprite can be placed onto a grid at (X,Y)\ncollision(object1=output_grid, object2=a_sprite, x2=X, y2=Y)\n# Check if two objects collide\ncollision(object1=object1, object2=object2,\nx1=X1, y1=Y1, x2=X2, y2=Y2)\n\"\"\"\ndef contact(_=None, object1=None, object2=None, x1=0, y1=0, x2=0, y2=0,\nbackground=Color.BLACK, connectivity=4,):\n\"\"\"\nCheck if object1 and object2 touch each other (have contact)\nwhen object1 is at (x1, y1) and object2 is at (x2, y2).\nThey are touching each other if they share a border, or if they\noverlap.\nCollision implies contact, but contact does not imply collision.\nconnectivity: 4 or 8, for 4-way or 8-way connectivity.\n(8-way counts diagonals as touching,\n4-way only counts cardinal directions as touching)\nExample usage:\n# Check if a sprite touches anything if it were to be placed at (X,Y)\ncontact(object1=output_grid, object2=a_sprite, x2=X, y2=Y)\n# Check if two objects touch each other\ncontact(object1=object1, object2=object2)\n\"\"\"\ndef generate_position_has_interval(max_len, position_num, if_padding=\nFalse):\n\"\"\"\nGenerate the position of the lines with random interval.\n\"\"\"\ndef random_free_location_for_sprite(grid, sprite, background=Color.BLACK,\nborder_size=0, padding=0,\npadding_connectivity=8):\n\"\"\"\nFind a random free location for the sprite in the grid\n26\nPreprint.\nReturns a tuple (x, y) of the top-left corner of the sprite in the\ngrid, which can be passed to ‘blit_sprite‘\nborder_size: minimum distance from the edge of the grid\nbackground: color treated as transparent\npadding: if non-zero, the sprite will be padded with a non-background\ncolor before checking for collision\npadding_connectivity: 4 or 8, for 4-way or 8-way connectivity when\npadding the sprite\nExample usage:\nx, y = random_free_location_for_sprite(grid, sprite, padding=1,\npadding_connectivity=8, border_size=1, background=Color.BLACK)\n# find the location, using generous padding\nassert not collision(object1=grid, object2=sprite, x2=x, y2=y)\nblit_sprite(grid, sprite, x, y)\n\"\"\"\ndef object_interior(grid, background=Color.BLACK):\n\"\"\"\nComputes the interior of the object (including edges)\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s interior.\nExample usage:\ninterior = object_interior(obj, background=Color.BLACK)\nfor x, y in np.argwhere(interior):\n# x,y is either inside the object or at least on its edge\n\"\"\"\ndef object_boundary(grid, background=Color.BLACK):\n\"\"\"\nComputes the boundary of the object (excluding interior)\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s boundary.\nExample usage:\nboundary = object_boundary(obj, background=Color.BLACK)\nassert np.all(obj[boundary] != Color.BLACK)\n\"\"\"\ndef object_neighbors(grid, background=Color.BLACK, connectivity=4):\n\"\"\"\nComputes a mask of the points that neighbor or border the object, but\nare not part of the object.\nreturns a new grid of ‘bool‘ where True indicates that the pixel is\npart of the object’s border neighbors5.\nExample usage:\nneighbors = object_neighbors(obj, background=Color.BLACK)\nassert np.all(obj[neighbors] == Color.BLACK)\n\"\"\"\nclass Symmetry:\n\"\"\"\nSymmetry transformations, which transformed the 2D grid in ways that\npreserve visual structure.\nReturned by ‘detect_rotational_symmetry‘,\n‘detect_translational_symmetry‘, ‘detect_mirror_symmetry‘.\n\"\"\"\ndef apply(self, x, y, iters=1):\n27\nPreprint.\n\"\"\"\nApply the symmetry transformation to the point (x, y) ‘iters‘\ntimes.\nReturns the transformed point (x’,y’)\n\"\"\"\ndef orbit(grid, x, y, symmetries):\n\"\"\"\nCompute the orbit of the point (x, y) under the symmetry\ntransformations ‘symmetries‘.\nThe orbit is the set of points that the point (x, y) maps to after\napplying the symmetry transformations different numbers of times.\nReturns a list of points in the orbit.\nExample:\nsymmetries = detect_rotational_symmetry(input_grid)\nfor x, y in np.argwhere(input_grid != Color.BLACK):\n# Compute orbit on to the target grid, which is typically the\noutput\nsymmetric_points = orbit(output_grid, x, y, symmetries)\n# ... now we do something with them like copy colors or infer\nmissing colors\n\"\"\"\ndef detect_translational_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nFinds translational symmetries in a grid.\nSatisfies: grid[x, y] == grid[x + translate_x, y + translate_y] for\nall x, y, as long as neither pixel is in ‘ignore_colors‘.\nReturns a list of Symmetry objects, each representing a different\ntranslational symmetry.\nExample:\nsymmetries = detect_translational_symmetry(grid, ignore_colors=[\noccluder_color])\nfor x, y in np.argwhere(grid != occluder_color):\n# Compute orbit on to the target grid\n# When copying to an output, this is usually the output grid\nsymmetric_points = orbit(grid, x, y, symmetries)\nfor x, y in symmetric_points:\nassert grid[x, y] == grid[x, y] or grid[x, y] ==\noccluder_color\n\"\"\"\ndef detect_mirror_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nReturns list of mirror symmetries.\nSatisfies: grid[x, y] == grid[2*mirror_x - x, 2*mirror_y - y]\nfor all x, y, as long as neither pixel is in ‘ignore_colors‘\nExample:\nsymmetries = detect_mirror_symmetry(grid,ignore_colors=[Color.BLACK])\n# ignore_color: In case parts of the object have been removed and\n# occluded by black\nfor x, y in np.argwhere(grid != Color.BLACK):\nfor sym in symmetries:\nsymmetric_x, symmetric_y = sym.apply(x, y)\nassert grid[symmetric_x, symmetric_y] == grid[x, y]\nor grid[symmetric_x, symmetric_y] == Color.BLACK\nIf the grid has both horizontal and vertical mirror symmetries,\nthe returned list will contain two elements.\n\"\"\"\n28\nPreprint.\ndef detect_rotational_symmetry(grid, ignore_colors=[Color.BLACK]):\n\"\"\"\nFinds rotational symmetry in a grid, or returns None if no symmetry\nis possible.\nSatisfies: grid[x, y] == grid[y - rotate_center_y + rotate_center_x,\n-x + rotate_center_y + rotate_center_x]\n# clockwise\ngrid[x, y] == grid[-y + rotate_center_y + rotate_center_x,\nx - rotate_center_y + rotate_center_x]\n# counterclockwise\nfor all x,y, as long as neither pixel is in ignore_colors\nExample:\nsym = detect_rotational_symmetry(grid, ignore_colors=[Color.BLACK])\n# ignore_color: In case parts of the object have been removed and\n# occluded by black\nfor x, y in np.argwhere(grid != Color.BLACK):\nrotated_x, rotated_y = sym.apply(x, y, iters=1) # +1 clockwise,\n-1 counterclockwise\nassert grid[rotated_x, rotated_y] == grid[x, y] or\ngrid[rotated_x, rotated_y] == Color.BLACK\nprint(sym.center_x, sym.center_y) # In case these are needed, they\nare floats\n\"\"\"\ndef is_contiguous(bitmask, background=Color.BLACK, connectivity=4):\n\"\"\"\nCheck if an array is contiguous.\nbackground: Color that counts as transparent (default: Color.BLACK)\nconnectivity: 4 or 8, for 4-way (only cardinal directions) or\n8-way connectivity (also diagonals) (default: 4)\nReturns True/False\n\"\"\"\ndef random_sprite(n, m, density=0.5, symmetry=None, color_palette=None,\nconnectivity=4, background=Color.BLACK):\n\"\"\"\nGenerate a sprite (an object), represented as a numpy array.\nn, m: dimensions of the sprite. If these are lists, then a random\nvalue will be chosen from the list.\nsymmetry: optional type of symmetry to apply to the sprite. Can be\n’horizontal’, ’vertical’, ’diagonal’, ’radial’, ’not_symmetric’. If\nNone, a random symmetry type will be chosen.\ncolor_palette: optional list of colors to use in the sprite. If None,\na random color palette will be chosen.\nReturns an (n,m) NumPy array representing the sprite.\n\"\"\"\ndef detect_objects(grid, _=None, predicate=None, background=Color.BLACK,\nmonochromatic=False, connectivity=None,\nallowed_dimensions=None,\ncolors=None, can_overlap=False):\n\"\"\"\nDetects and extracts objects from the grid that satisfy custom\nspecification.\npredicate:\na function that takes a candidate object as input and\nreturns True if it counts as an object\nbackground:\n29\nPreprint.\ncolor treated as transparent\nmonochromatic:\nif True, each object is assumed to have only one color\nIf False, each object can include multiple colors.\nconnectivity:\n4 or 8, for 4-way or 8-way connectivity.\nIf None, the connectivity is determined automatically.\nallowed_dimensions:\na list of tuples (n, m) specifying the allowed dimensions of the\nobjects.\nIf None, objects of any size are allowed.\ncolors:\na list of colors that the objects are allowed to have.\nIf None, objects of any color are allowed.\ncan_overlap: if True, objects can overlap.\nIf False, objects cannot overlap.\nReturns a list of objects, where each object is a numpy array.\n\"\"\"\n30\nPreprint.\nconcepts: concepts: concepts:\ncolor inversion, pixel sorting, frequency counting, shape completion, color filling\nobjecxt transformation color arrangement\nconcepts: concepts: concepts:\nsymmetry detection, color blending, object detection, shape expansion,\nreflection radial expansion color filling\nconcepts: concepts: concepts:\npattern detection, copying, inverse patterns, scaling color blending,\npositioning shape merging\nFigure13: Nineexampleproblemsgeneratedautomaticallybyourpipeline.\n31\nPreprint.\nA.3 GENERATEDARCEXAMPLES\nB FINE TUNING TRAINING DETAILS\nB.1 PROMPTINGTHEMODELS\nWe must include in our prompts for our fine-tuned models the input/output 2D colored grids of\neachproblem. Todothiswerepresenttheproblemtextuallybynamingthecolorsone-by-one. We\nrenamedcertaincolorswhichweremorethanonetoken(e.g.,maroon→brownsaves1token/pixel),\nand presented the grid as a whitespace-delimited 2D array with newlines delimiting rows. Please\nseebelow.\nTrainexamples Testexample\nExample1 Example2 Example3\nFigure14: Promptexampleillustration\nTransductionexample:\n----- Role: system --------------------\nYou are a world-class puzzle solver with exceptional pattern recognition\nskills. Your task is to analyze puzzles, spot patterns, and provide direct\nsolutions.\n----- Role: user --------------------\nGiven input-output grid pairs as reference examples, carefully observe the\npatterns to predict the output grid for new test input. Each pair follows\nthe same transformation rule. Grids are 2D arrays represented as strings,\nwith cells (colors) separated by spaces and rows by newlines.\nHere are the input and output grids for the reference examples:\nExample 1\nInput:\nBlack Gray Black\nGray Gray Gray\nBlack Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nExample 2\nInput:\nGray Black Black\nBlack Gray Black\nBlack Black Gray\nOutput:\nBlue Red Black Black Black Black\nRed Blue Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\n32\nPreprint.\nBlack Black Black Black Blue Red\nBlack Black Black Black Red Blue\nExample 3\nInput:\nBlack Gray Black\nBlack Gray Gray\nGray Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Blue Red Blue Red\nBlack Black Red Blue Red Blue\nBlue Red Blue Red Black Black\nRed Blue Red Blue Black Black\nHere is the input grid for the test example:\nInput:\nBlack Black Black\nBlack Gray Black\nGray Gray Gray\nDirectly provide the output grids corresponding to the given test input\ngrids, based on the patterns observed in the reference examples.\n----- Role: assistant --------------------\nThe output grid for the test input grid is:\n‘‘‘\nBlack Black Black Black Black Black\nBlack Black Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\n‘‘‘\nInductionexample:\n----- Role: system --------------------\nYou are a world-class puzzle solver with exceptional pattern recognition\nskills and expertise in Python programming. Your task is to analyze puzzles\nand provide Python solutions.\n----- Role: user --------------------\nGiven input-output grid pairs as reference examples, carefully observe the\npatterns to predict the output grid for new test input. Each pair follows\nthe same transformation rule. Grids are 2D arrays represented as strings,\nwith cells (colors) separated by spaces and rows by newlines.\nHere are the input and output grids for the reference examples:\nExample 1\nInput:\nBlack Gray Black\nGray Gray Gray\nBlack Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlue Red Blue Red Blue Red\nRed Blue Red Blue Red Blue\n33\nPreprint.\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nExample 2\nInput:\nGray Black Black\nBlack Gray Black\nBlack Black Gray\nOutput:\nBlue Red Black Black Black Black\nRed Blue Black Black Black Black\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Black Black Blue Red\nBlack Black Black Black Red Blue\nExample 3\nInput:\nBlack Gray Black\nBlack Gray Gray\nGray Gray Black\nOutput:\nBlack Black Blue Red Black Black\nBlack Black Red Blue Black Black\nBlack Black Blue Red Blue Red\nBlack Black Red Blue Red Blue\nBlue Red Blue Red Black Black\nRed Blue Red Blue Black Black\nHere is the input grid for the test example:\nInput:\nBlack Black Black\nBlack Gray Black\nGray Gray Gray\nWrite a Python function ‘transform‘ that can convert any given input grid to\nits corresponding output grid based on the pattern observed in the\nreference examples.\n----- Role: assistant --------------------\nLet’s solve this puzzle using Python code with the common library functions.\nWe’ll first reason about the problem and then write the code to solve it.\nThe ‘transform‘ function will take the input grid and return the output grid\n. Here is the Python code with the comments describing how to solve the\nproblem:\n‘‘‘python\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# repetition, diagonal lines, color change\n# description:\n# In the input you will see a 3x3 grid with a specific diagonal pattern of\ngray pixels.\n# The output should be the result of repeating this diagonal pattern\nhorizontally and vertically\n34\nPreprint.\n# to fill a 6x6 grid, changing the color from gray to blue and red in an\nalternating manner.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a new 6x6 output grid filled with the background color (black)\noutput_grid = np.full((6, 6), Color.BLACK)\n# Check for the diagonal pattern in the input grid\nfor i in range(3):\nfor j in range(3):\nif input_grid[i, j] == Color.GRAY:\n# Fill the same position in the output grid with alternating\ncolors\noutput_grid[i * 2, j * 2] = Color.BLUE\noutput_grid[i * 2, j * 2 + 1] = Color.RED\noutput_grid[i * 2 + 1, j * 2] = Color.RED\noutput_grid[i * 2 + 1, j * 2 + 1] = Color.BLUE\nreturn output_grid\n‘‘‘\nC EXAMPLES OF INDUCTION SOLUTIONS\nC.1 ARCPROBLEMF3CDC58F\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color counting, grid transformation\n# description:\n# In the input, you will see a grid with several colored squares.\n# To make the output, count the number of squares of each color.\n# Fill the output grid with bars of corresponding colors.\n# The height of each bar corresponds to the number of squares of that\ncolor.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a blank output grid\noutput_grid = np.zeros_like(input_grid)\n# Count the number of squares of each color\ncolor_counts = {color: 0 for color in Color.NOT_BLACK}\nfor row in input_grid:\nfor cell in row:\nif cell in color_counts:\ncolor_counts[cell] += 1\n# Define the height of the bars based on the counts\nmax_height = 10 # Maximum height of the bars\nfor color, count in color_counts.items():\nif count > 0:\nfor h in range(min(count, max_height)):\noutput_grid[-(h + 1), (color - 1) %\nreturn output_grid\n35\nPreprint.\nC.2 ARCPROBLEMBE03B35F\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color detection, pattern extraction, rotation transformation\n# description:\n# In the input, you will see a 5x5 grid consisting of a colored pattern\nin the top left corner and a 2x2\nsquare of red pixels in the bottom\nright corner.\n# Depending on the color of the top left corner (blue or green), rotate\nthe pattern by 90 degrees clockwise\nor counterclockwise.\n# The output grid should be the transformed pattern placed in the same\nposition as the original top left\npattern.\ndef transform(input_grid):\n# Extract the top left 5x5 pattern\npattern = input_grid[0:5, 0:5]\n# Identify the color in the top left corner\ncorner_color = pattern[0, 0]\n# Define rotation based on the corner color\nif corner_color == Color.BLUE:\ntransformed_pattern = np.rot90(pattern, k=-1) # Rotate 90\ndegrees counterclockwise\nelif corner_color == Color.GREEN:\ntransformed_pattern = np.rot90(pattern, k=1) # Rotate 90\ndegrees clockwise\nelse:\nraise ValueError(\"The corner color must be either BLUE or GREEN.\"\n)\n# Create the output grid\noutput_grid = np.full((2, 2), Color.BLACK) # Initialize a small grid\noutput_grid = blit_sprite(output_grid, transformed_pattern, x=0, y=0,\nbackground=Color.BLACK)\nreturn output_grid\nC.3 ARCPROBLEM2072ABA6\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color mapping, pattern replication\n# description:\n# In the input you will see a 3x3 grid of colored pixels.\n# The colors are either black or gray. The output should be a grid where:\n# 1. If the pixel is black, it remains black in the output.\n# 2. If the pixel is gray, it should be replaced with a 2x2 block of blue\nand red pixels in a checkerboard\npattern.\n36\nPreprint.\n# The blue pixel should occupy the top-left and bottom-right of the 2x2\nblock, while the red pixel occupies\nthe top-right and bottom-left.\ndef transform(input_grid):\n# Create an output grid that is larger than the input grid\noutput_grid = np.zeros((input_grid.shape[0] * 2, input_grid.shape[1]\n* 2), dtype=int)\n# Fill the output grid based on the input grid\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] == Color.BLACK:\n# Black stays black in the output\ncontinue\nelse:\n# Replace gray with a checkerboard pattern of blue and\nred\noutput_grid[2 * x:2 * x + 2, 2 * y:2 * y + 2] = [\n[Color.BLUE, Color.RED],\n[Color.RED, Color.BLUE]\n]\nreturn output_grid\nC.4 ARCPROBLEMEF26CBF6\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# color transformation, grid sections, boundary detection\n# description:\n# In the input, you will see a grid with a pattern of yellow and blue\npixels with a black background,\n# and multiple colored circles (not yellow or blue) placed randomly\nwithin the grid.\n# The goal is to transform the output grid by replacing all the blue\npixels with the color of the closest\ncircle\n# and keeping the yellow pixels unchanged.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a copy of the input grid to modify\noutput_grid = np.copy(input_grid)\n# Find the coordinates of the colored circles\ncircle_coordinates = np.argwhere((input_grid != Color.BLACK) & (\ninput_grid != Color.YELLOW) & (\ninput_grid != Color.BLUE))\n# Iterate over the grid and replace blue pixels with the nearest\ncircle color\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] == Color.BLUE:\n# Find the closest circle\nclosest_color = None\nclosest_distance = float(’inf’)\nfor circle_x, circle_y in circle_coordinates:\n37\nPreprint.\ndistance = abs(circle_x - x) + abs(circle_y - y) #\nManhattan\ndistance\nif distance < closest_distance:\nclosest_distance = distance\nclosest_color = input_grid[circle_x, circle_y]\n# Replace the blue pixel with the closest circle color\noutput_grid[x, y] = closest_color\nreturn output_grid\nC.5 ARCPROBLEME7639916\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# boundary detection, color filling\n# description:\n# In the input grid, you will see a black background with several purple\npixels forming a boundary.\n# The task is to fill the area enclosed by the purple boundary with blue\npixels.\ndef transform(input_grid):\n# Create an output grid that starts as a copy of the input grid\noutput_grid = np.copy(input_grid)\n# Find the coordinates of the purple pixels\npurple_coords = np.argwhere(input_grid == Color.PURPLE)\n# If no purple pixels are found, return the original grid\nif len(purple_coords) == 0:\nreturn output_grid\n# Get the boundaries of the purple pixels\nmin_x, min_y = np.min(purple_coords, axis=0)\nmax_x, max_y = np.max(purple_coords, axis=0)\n# Fill the area enclosed by the purple boundary\nfor x in range(min_x, max_x + 1):\nfor y in range(min_y, max_y + 1):\n# Check if the current position is outside the purple\nboundary\nif (x == min_x or x == max_x or y == min_y or y == max_y) and\noutput_grid[x, y] ==\nColor.BLACK:\noutput_grid[x, y] = Color.BLUE\nreturn output_grid\nC.6 ARCPROBLEMC074846D\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n38\nPreprint.\n# rotation, color change, symmetry\n# description:\n# In the input, you will see a colored object with a single gray pixel.\n# To make the output, rotate the object 90 degrees clockwise around the\ngray pixel,\n# and color the newly exposed pixels green.\ndef transform(input_grid):\n# Find the gray pixel location\ngray_pixel_locations = np.argwhere(input_grid == Color.GRAY)\nassert len(gray_pixel_locations) == 1\ngray_x, gray_y = gray_pixel_locations[0]\n# Create an output grid\noutput_grid = np.full(input_grid.shape, Color.BLACK)\n# Rotate the object around the gray pixel\nfor x in range(input_grid.shape[0]):\nfor y in range(input_grid.shape[1]):\nif input_grid[x, y] != Color.BLACK and input_grid[x, y] !=\nColor.GRAY:\n# Calculate new position after 90 degrees clockwise\nrotation\nnew_x = gray_x + (y - gray_y)\nnew_y = gray_y - (x - gray_x)\n# Check if the new position is within bounds\nif 0 <= new_x < output_grid.shape[0] and 0 <= new_y <\noutput_grid.shape[1]\n:\n# Place the rotated pixel in the output grid\noutput_grid[new_x, new_y] = input_grid[x, y]\n# Color newly exposed pixels green\nif output_grid[x, y] == Color.BLACK:\noutput_grid[x, y] = Color.GREEN\n# Place the gray pixel back in the center\noutput_grid[gray_x, gray_y] = Color.GRAY\nreturn output_grid\nC.7 ARCPROBLEMAE58858E\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# object detection, color change, size comparison\n# description:\n# In the input, you will see a grid with red objects of various sizes.\n# To make the output, change all objects larger than a certain size (3\npixels) to pink.\ndef transform(input_grid):\n# Create a copy of the input grid to produce the output\noutput_grid = np.copy(input_grid)\n# Find all connected components (red objects) in the input grid\n39\nPreprint.\nred_objects = find_connected_components(input_grid, background=Color.\nBLACK, monochromatic=True)\n# Iterate through the red objects and change their color if they are\nlarger than 3 pixels\nfor obj in red_objects:\nsize = np.sum(obj == Color.RED)\nif size > 3:\noutput_grid[obj == Color.RED] = Color.PINK\nreturn output_grid\nC.8 ARCPROBLEM21F83797\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# vertical and horizontal lines, intersection\n# description:\n# In the input, you will see two red pixels.\n# To make the output, draw a blue square with the red pixels as corners.\n# Additionally, draw a vertical and horizontal line that intersect at\neach red pixel.\ndef transform(input_grid):\n# Copy the input grid to the output grid\noutput_grid = np.copy(input_grid)\n# Find the positions of the red pixels\nred_pixels = np.argwhere(input_grid == Color.RED)\n# Ensure there are exactly two red pixels\nassert len(red_pixels) == 2\n(x1, y1), (x2, y2) = red_pixels\n# Sort the red pixels’ positions to determine which is top-left and\nbottom-right\ntop_left = (min(x1, x2), min(y1, y2))\nbottom_right = (max(x1, x2), max(y1, y2))\n# Draw a blue square from the top-left to the bottom-right corner\noutput_grid[top_left[0]:bottom_right[0]+1, top_left[1]:bottom_right[1\n]+1] = Color.BLUE\n# Draw horizontal and vertical lines through the red pixels\nfor x, y in [(x1, y1), (x2, y2)]:\noutput_grid[x, :] = Color.RED # Horizontal line through red\npixels\noutput_grid[:, y] = Color.RED # Vertical line through red pixels\nreturn output_grid\nC.9 ARCPROBLEM33B52DE3\nfrom common import *\nimport numpy as np\nfrom typing import *\n40\nPreprint.\n# concepts:\n# object detection, color mapping, grid transformation\n# description:\n# In the input, you will see a 20x20 grid containing a 4x4 pattern made\nof colored pixels (not necessarily\ncontiguous) and a series of 3x3 gray\nsquares.\n# The task is to extract the colors from the 4x4 pattern and color the\ncorresponding 3x3 gray squares with\nthose colors in a specific order.\n# The first gray square should be colored with the first color from the\npattern, the second gray square with\nthe second color, and so on.\ndef transform(input_grid):\n# Step 1: Detect the 4x4 color pattern in the input grid.\ncolor_pattern = detect_objects(input_grid, monochromatic=False,\nconnectivity=8)\n# Step 2: Extract the colors from the detected pattern.\ncolors = []\nfor obj in color_pattern:\ncropped_obj = crop(grid=obj, background=Color.BLACK)\ncolors.extend(cropped_obj.flatten())\n# Step 3: Identify the locations of the gray squares.\ngray_squares = detect_objects(input_grid, colors=[Color.GRAY],\nmonochromatic=True, connectivity\n=8)\n# Step 4: Color the gray squares with the corresponding colors from\nthe pattern.\noutput_grid = np.copy(input_grid)\nfor i, gray_square in enumerate(gray_squares):\nif i < len(colors):\n# Color the gray square with the corresponding color\nx, y = np.where(gray_square == Color.GRAY)\nflood_fill(output_grid, x[0], y[0], colors[i])\nreturn output_grid\nC.10 ARCPROBLEM332EFDB3\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# borders, alternating patterns\n# description:\n# In the input, you will see an empty black grid. To make the output,\ndraw a blue border around the grid,\nbut only on the edges that are\nmultiples of 2 (i.e., the top and\nbottom edges, and the left and right\nedges at every even column/index).\ndef transform(input_grid):\n# Get the dimensions of the input grid\n41\nPreprint.\nheight, width = input_grid.shape\n# Create the output grid, initially a copy of the input grid\noutput_grid = input_grid.copy()\n# Draw the blue border on edges that are multiples of 2\nfor x in range(0, height, 2): # Vertical edges: multiples of 2\ndraw_line(output_grid, x, 0, direction=(0, 1), color=Color.BLUE)\n# Top edge\ndraw_line(output_grid, x, width - 1, direction=(0, 1), color=\nColor.BLUE) # Bottom edge\nfor y in range(0, width, 2): # Horizontal edges: multiples of 2\ndraw_line(output_grid, 0, y, direction=(1, 0), color=Color.BLUE)\n# Left edge\ndraw_line(output_grid, height - 1, y, direction=(1, 0), color=\nColor.BLUE) # Right edge\nreturn output_grid\nD INDUCTION FALSE POSITIVE SOLUTIONS\nD.1 ARCPROBLEM25094A63\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# grid transformation, color replacement, pattern detection\n# description:\n# In the input, you will see a grid with colored regions. If a 5x5 square\nin the grid consists entirely of a\nsingle color,\n# replace that square with a 5x5 yellow square in the output grid. If the\nsquare is not fully filled with a\nsingle color, leave it as is.\ndef transform(input_grid):\n# Plan:\n# 1. Iterate over the grid to find 5x5 squares.\n# 2. For each 5x5 square, check if it is monochromatic (all pixels\nare the same color).\n# 3. If it is, replace it with a 5x5 yellow square in the output grid\n.\noutput_grid = np.copy(input_grid)\n# Dimensions\nsquare_size = 5\nfor x in range(len(input_grid) - square_size + 1):\nfor y in range(len(input_grid[0]) - square_size + 1):\n# Extract the 5x5 region\nregion = input_grid[x:x + square_size, y:y + square_size]\n# Check if the region is monochromatic\nif np.all(region == region[0, 0]): # All pixels should be\nthe same color\n# Replace the region with a yellow square\noutput_grid[x:x + square_size, y:y + square_size] = Color\n.YELLOW\n42\nPreprint.\nreturn output_grid\nD.2 ARCPROBLEM009D5C81\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# pattern recognition, color transformation, grid manipulation\n# description:\n# In the input, you will see a 15x15 grid with a pattern of purple pixels\nthat forms a shape and a blue\ncircle in the center of the grid.\n# To create the output, you should color the shape formed by the purple\npixels with a specific color based\non the number of purple pixels:\n# - If there are 1-10 purple pixels, color the shape yellow.\n# - If there are 11-20 purple pixels, color the shape green.\n# - If there are 21-30 purple pixels, color the shape red.\n# - If there are more than 30 purple pixels, color the shape orange.\n# The blue circle should be replaced by black.\ndef transform(input_grid):\n# Create an output grid initialized to black\noutput_grid = np.zeros_like(input_grid)\n# Count the number of purple pixels\npurple_count = np.count_nonzero(input_grid == Color.PURPLE)\n# Determine the color based on the number of purple pixels\nif 1 <= purple_count <= 10:\nshape_color = Color.YELLOW\nelif 11 <= purple_count <= 20:\nshape_color = Color.GREEN\nelif 21 <= purple_count <= 30:\nshape_color = Color.RED\nelse:\nshape_color = Color.ORANGE\n# Find the purple shape and color it accordingly\noutput_grid[input_grid == Color.PURPLE] = shape_color\n# Replace the blue circle with black\noutput_grid[input_grid == Color.BLUE] = Color.BLACK\nreturn output_grid\nD.3 ARCPROBLEME95E3D8E\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# occlusion, symmetry detection, reconstruction\n# description:\n43\nPreprint.\n# In the input you will see a grid containing a repeated pattern that has\nbeen partially occluded by black\nsquares.\n# To make the output, remove the occluding black squares and restore the\npattern based on the detected\nsymmetry.\ndef transform(input_grid):\n# Plan:\n# 1. Extract the occluded pattern from the input grid.\n# 2. Detect the symmetries in the pattern.\n# 3. Fill in the missing parts of the pattern using the detected\nsymmetries.\n# Step 1: Identify non-black pixels to find the pattern\nocclusion_color = Color.BLACK\nnon_black_mask = input_grid != occlusion_color\nobjects = detect_objects(input_grid, background=occlusion_color,\nmonochromatic=False,\nconnectivity=8)\n# Step 2: Find the symmetries in the detected objects\nsymmetries = detect_translational_symmetry(input_grid, ignore_colors=\n[occlusion_color])\n# Step 3: Fill in the missing parts using the symmetries\noutput_grid = input_grid.copy()\nfor x, y in np.argwhere(non_black_mask):\nfor symmetric_x, symmetric_y in orbit(input_grid, x, y,\nsymmetries):\nif output_grid[symmetric_x, symmetric_y] == occlusion_color:\noutput_grid[symmetric_x, symmetric_y] = input_grid[x, y]\nreturn output_grid\nD.4 ARCPROBLEM73CCF9C2\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# pattern extraction, rotation, centering\n# description:\n# In the input, you will see a grid with a pattern repeated several times\n, but with random rotations.\n# To make the output, extract one instance of the pattern and rotate it\nback to its original orientation.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Create a copy of the input grid for the output\noutput_grid = np.copy(input_grid)\n# Find connected components to extract patterns\ncomponents = find_connected_components(output_grid, background=Color.\nBLACK, connectivity=8)\n# For each component, check if it’s a valid pattern\nfor component in components:\n# Crop the component to get the pattern\npattern = crop(component, background=Color.BLACK)\n44\nPreprint.\n# Check if the pattern can be rotated back to its original\norientation\n# We can rotate the pattern and compare with the original\noriginal_pattern = pattern.copy()\n# Rotate the pattern in 90-degree increments and check if the\noriginal pattern matches\nrotated_patterns = [np.rot90(original_pattern, k) for k in range(\n4)]\n# We assume the first valid pattern is the original orientation\nfor rotated in rotated_patterns:\nif np.array_equal(rotated, original_pattern):\n# If it matches, we’ve found the original orientation\noutput_grid = pattern\nbreak\nreturn output_grid\nD.5 ARCPROBLEMD56F2372\nfrom common import *\nimport numpy as np\nfrom typing import *\n# concepts:\n# object detection, symmetry detection, cropping\n# description:\n# In the input you will see a grid filled with different colored shapes.\n# The output should be the smallest symmetric shape (with respect to the\ncenter) that can fit all the others\ninside it.\n# If there are no symmetric shapes, the output should be a grid of the\nsame size filled with the background\ncolor.\ndef transform(input_grid: np.ndarray) -> np.ndarray:\n# Find all connected components (shapes) in the grid\ncomponents = find_connected_components(input_grid, background=Color.\nBLACK, connectivity=8,\nmonochromatic=False)\n# Initialize a variable to track the largest symmetric shape found\nlargest_symmetric_shape = None\nmax_area = 0\n# Check each component for symmetry and area\nfor component in components:\n# Crop the component to get its shape\ncropped_shape = crop(component, background=Color.BLACK)\n# Check for horizontal and vertical mirror symmetry\nis_symmetric = np.array_equal(cropped_shape, np.flip(\ncropped_shape, axis=0)) or \\\nnp.array_equal(cropped_shape, np.flip(\ncropped_shape\n, axis=1))\nif is_symmetric:\narea = np.count_nonzero(cropped_shape != Color.BLACK)\nif area > max_area:\nmax_area = area\n45\nPreprint.\nlargest_symmetric_shape = cropped_shape\n# If we found a symmetric shape, return it; otherwise return a grid\nof the same size filled with the\nbackground color\nif largest_symmetric_shape is not None:\nreturn largest_symmetric_shape\nelse:\nreturn np.full(input_grid.shape, Color.BLACK)\nE DATA AUGMENTATION AND RERANKING\nWe improve the performance of the transductive model through data augmentation and output\nreranking. The key insight is that any invertible transformation T can be applied to both train-\ningandtestinputs,allowingustogeneratemultiplediversepredictionsthatarethenaggregatedand\nreranked. We consider two transformations of the training examples and test input in addition to\ntheoriginaltask: (i)transpositions,T (x) = xT; (ii)colorpermutation,implementedasarandom\nt\npermutation of the integers 0-9 representing colors. For each transformation T, we apply it to all\ntrainingexamplesandthetestinput,runabeamsize10decodingonthetransformeddata,andthen\napply T−1 to the predictions to return to the original space. The transposition transformation ad-\ndressespotentialbiasesinthepretrainedmodelregardinghorizontalvs. verticalprocessing,while\ncolorpermutationsensurethemodelisn’trelyingonspecificcolorvalues. Foreachtransformation\nT,eachcandidateoutputyreceivesabeamsearchscores (y)computedas:\nT\n(cid:0) (cid:1)\ns (y)=t T(y)|T(x ),T(x ),T(y ) (6)\nT θ train train train\nwhere t (·|·) is the transduction model. For ranking, we aggregate candidates across all transfor-\nθ\nmations. For each unique candidate y, we track both its frequency of appearance freq(y) across\ndifferenttransformationsanditsaveragebeamsearchscoreE[s (y)]. Thesearethenrankedwith\nT\nfrequencytakingprecedenceoveraveragescore.\nF TEST-TIME TRAINING\nTest time training is an approach for updating model parameters at test time, which we apply\nto our transduction model. We assume that we are given test problems D comprising triples\n(x train,y train,x test) and a data augmentation procedure AUG(x,y) which constructs variations of\nanARCproblem,forexamplebypermutingcolorsandrotatinggrids. Thenmodelparametersare\noptimizedtomaximizethelikelihoodofaugmentedtesttaskswherearandomtraininginput-output\nisselectedtoserveasafaketestexample:\nE [t (y [k]|x [:k],x [k+1:],y [:k],y [k+1:])] (7)\nθ train train train train train\n(xtrain,ytrain,xtest)∼D\nk∼1..len(xtrain)\n(x′ train,y t′ rain)∼AUG(xtrain,ytrain)\nwherelen(x )isthenumberofexampletraininginputs,andk istheindexofthetraininginput-\ntrain\noutputwhichisrandomlyselectedtoserveasafaketestcase. Notethatthisproceduredoesnotrely\nonaccesstogroundtruthtestpredictions.\nWechooseeachtrainingexampleasafaketestexampleanddo10timesdifferentrandomlycom-\nbined data augmentation for each fake task, which gives us 12k pseudo training dataset from the\nevaluationdataset. Wealsorandomlyinclude5kRE-ARCexamplesand5kARC-Heavyexamples,\nwhichwespeculatedwouldhavearegularizingeffect. Thedatasetsizefortesttimetrainingis22k\ntotalproblems.\nG EXPERIMENT PARAMETERS\nG.1 INDUCTION\nFine-tuningHyperparameters:\n46\nPreprint.\ntrainingtype lorarank loraalpha learningrate gradientaccumulatesteps\nlorafinetune 64 64 2e-4 2\nperdevicebatchsize device epcoh weightdecay learningrateschedulertype\n8 8xA100 3 0 cosine\nForthelast230kdatafinetuneweusedfullfinetuninginsteadofLoRA:\ntrainingtype learningrate weightdecay gradientaccumulatesteps\nfullfinetune 1e-5 0.05 1\nepoch perdevicetrainbatch learningrateschedulertype devices\n2 16 cosine 8xA100\nInferenceHyperparameters:\n• temperature: 0.8(1.0forthefull-datafine-tunedmodel)\n• top-p: 1.0\nOutput selection: For experiments in section 4, when allowing one or two attempts, we filter the\nsample programs using train input-output examples and then randomly select one or two distinct\nprogramsuniformly. Wereporttheexpectedvalueinourresults. Forexperimentsinsection5,we\ntaketheexecutionresultsoftestoutputfromtheprogramsthatcanpassallthetrainexamplesand\nusemajorityvotetoselectthetop2,andinthecaseofconceptarc,thetop3testoutputs.\nG.2 TRANSDUCTION\nFine-tuneHyperparameters:\ntrainingtype learningrate weightdecay gradientaccumulatesteps device\nfullfinetune 1e-5 1e-2 2 8xA100\nengineerepoch otherepoch perdevicetrainbatch learningrateschedulertype\n3 2 8 cosine\nForthefinalengineeringresults,wetrainfor3epochs. Forallotherexperimentresults,wetrainfor\n2epochs.\nInferenceHyperparameters:\n• temperature: 0\n• usebeamsearch: True\n• beamwidth:\n1. engineerresults: 40\n2. 100kdatascale: 20\n3. allotherexperimentresults: 3\n• top-p: 1.0\nTest-timeFine-tuningHyperparameters\ntrainingtype lorarank loraalpha learningrate gradientaccumulatesteps\nlorafinetune 64 64 2e-4 2\nperdevicebatchsize device epcoh weightdecay learningrateschedulertype\n2 4xA6000 3 0 cosine\n47",
    "pdf_filename": "Combining_Induction_and_Transduction_for_Abstract_Reasoning.pdf"
}