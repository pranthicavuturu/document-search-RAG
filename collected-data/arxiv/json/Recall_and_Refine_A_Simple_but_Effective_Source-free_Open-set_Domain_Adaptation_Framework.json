{
    "title": "Recall and Refine A Simple but Effective Source-free Open-set Domain Adaptation Framework",
    "abstract": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where novel classes — also referred to as target-private un- known classes — are present. Source-free Open-set Domain Adaptation (SF-OSDA) methods address OSDA without ac- cessing labeled source data, making them particularly rele- vant under privacy constraints. However, SF-OSDA presents significant challenges due to distribution shifts and the intro- duction of novel classes. Existing SF-OSDA methods typi- cally rely on thresholding the prediction entropy of a sample to identify it as either a known or unknown class but fail to explicitly learn discriminative features for the target-private unknown classes. We propose Recall and Refine (RRDA), a novel SF-OSDA framework designed to address these lim- itations by explicitly learning features for target-private un- known classes. RRDA employs a two-step process. First, we enhance the model’s capacity to recognize unknown classes by training a target classifier with an additional decision boundary, guided by synthetic samples generated from tar- get domain features. This enables the classifier to effectively separate known and unknown classes. In the second step, we adapt the entire model to the target domain, address- ing both domain shifts and improving generalization to un- known classes. Any off-the-shelf source-free domain adapta- tion method (e.g., SHOT, AaD) can be seamlessly integrated into our framework at this stage. Extensive experiments on three benchmark datasets demonstrate that RRDA signifi- cantly outperforms existing SF-OSDA and OSDA methods. The source code is publicly available1. Introduction Unsupervised Domain Adaptation (UDA) (Ben-David et al. 2010; Ganin and Lempitsky 2015; Long et al. 2015) adapts a model from a labeled source domain to an unlabeled target domain (Oza et al. 2023), effectively addressing the issue of domain shift where the source and target distributions differ. UDA strategies typically align feature distributions between domains using metric learning techniques (Long et al. 2015; Kang et al. 2019) or adversarial training (Ganin and Lempit- sky 2015; Tzeng et al. 2017; Luo et al. 2019), and more re- cently, self-training approaches (Sun et al. 2022; Hoyer et al. 2023; Zhu, Bai, and Wang 2023). Despite their success, most 1https://github.com/ismailnejjar/RRDA current domain adaptation approaches operate under the as- sumption of a shared label set between the source and target domains (i.e., Cs = Ct), referred to as Closed-set Domain Adaptation (Saenko et al. 2010). However, this assumption is often impractical in real-world scenarios. In contrast, Open-set Domain Adaptation (OSDA) ex- tends the target label space beyond that of the source do- main (i.e., Cs ⊂Ct) (Saito et al. 2018; Liu et al. 2019), thereby adding complexity to the DA task. OSDA aims to align target samples from known classes with those from the source domain while effectively identifying target sam- ples belonging to categories not observed in the source do- main, referred to as unknown classes (Panareda Busto and Gall 2017; Bucci, Loghmani, and Tommasi 2020; Jang et al. 2022). Various criteria based on instance-level predictions have been proposed, including entropy-based (Feng, Xu, and Tao 2021; Saito et al. 2020) and confidence-based (Saito and Saenko 2021; Fu et al. 2020) methods. Additionally, privacy and legal considerations increas- ingly limit access to labeled source data for adaptation pur- poses. To address this, source-free adaptation methods (Fang et al. 2024) have emerged, enabling adaptation without re- liance on labeled source data (Kim et al. 2021; Kundu et al. 2020a; Li et al. 2020). In this paper, we focus on Source- free Open-set Domain Adaptation (SF-OSDA), where only a pre-trained source model is available for knowledge transfer, without access to labeled source data. While some Source- free Domain Adaptation (SF-DA) methods have demon- strated effectiveness in addressing SF-OSDA for classifica- tion tasks (Liang, Hu, and Feng 2020; Yang et al. 2022; Wan et al. 2024), semantic segmentation (Choe et al. 2024), and graph applications (Wang et al. 2024), they primarily focus on the semantics of known classes in the source domain, often overlooking the crucial aspect of novel-class seman- tics. These methods focus on segregating target samples with low entropy, categorizing them as known classes, and subse- quently optimizing specific objectives such as entropy mini- mization or clustering. In this process, data points associated with known classes are prioritized, while those with high en- tropy are typically excluded from training, leading to a se- mantic disparity between the known and unknown classes. To effectively adapt a pre-trained source model to a target domain facing both category and distribution shifts, we pro- pose Recall and Refine for Domain Adaptation (RRDA) for arXiv:2411.12558v1  [cs.CV]  19 Nov 2024",
    "body": "Recall and Refine: A Simple but Effective Source-free Open-set Domain\nAdaptation Framework\nIsmail Nejjar 1, Hao Dong 2, Olga Fink 1\n1 ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), 2ETH Z¨urich\nismail.nejjar@epfl.ch, hao.dong@ibk.baug.ethz.ch, olga.fink@epfl.ch\nAbstract\nOpen-set Domain Adaptation (OSDA) aims to adapt a model\nfrom a labeled source domain to an unlabeled target domain,\nwhere novel classes — also referred to as target-private un-\nknown classes — are present. Source-free Open-set Domain\nAdaptation (SF-OSDA) methods address OSDA without ac-\ncessing labeled source data, making them particularly rele-\nvant under privacy constraints. However, SF-OSDA presents\nsignificant challenges due to distribution shifts and the intro-\nduction of novel classes. Existing SF-OSDA methods typi-\ncally rely on thresholding the prediction entropy of a sample\nto identify it as either a known or unknown class but fail to\nexplicitly learn discriminative features for the target-private\nunknown classes. We propose Recall and Refine (RRDA), a\nnovel SF-OSDA framework designed to address these lim-\nitations by explicitly learning features for target-private un-\nknown classes. RRDA employs a two-step process. First, we\nenhance the model’s capacity to recognize unknown classes\nby training a target classifier with an additional decision\nboundary, guided by synthetic samples generated from tar-\nget domain features. This enables the classifier to effectively\nseparate known and unknown classes. In the second step,\nwe adapt the entire model to the target domain, address-\ning both domain shifts and improving generalization to un-\nknown classes. Any off-the-shelf source-free domain adapta-\ntion method (e.g., SHOT, AaD) can be seamlessly integrated\ninto our framework at this stage. Extensive experiments on\nthree benchmark datasets demonstrate that RRDA signifi-\ncantly outperforms existing SF-OSDA and OSDA methods.\nThe source code is publicly available1.\nIntroduction\nUnsupervised Domain Adaptation (UDA) (Ben-David et al.\n2010; Ganin and Lempitsky 2015; Long et al. 2015) adapts\na model from a labeled source domain to an unlabeled target\ndomain (Oza et al. 2023), effectively addressing the issue of\ndomain shift where the source and target distributions differ.\nUDA strategies typically align feature distributions between\ndomains using metric learning techniques (Long et al. 2015;\nKang et al. 2019) or adversarial training (Ganin and Lempit-\nsky 2015; Tzeng et al. 2017; Luo et al. 2019), and more re-\ncently, self-training approaches (Sun et al. 2022; Hoyer et al.\n2023; Zhu, Bai, and Wang 2023). Despite their success, most\n1https://github.com/ismailnejjar/RRDA\ncurrent domain adaptation approaches operate under the as-\nsumption of a shared label set between the source and target\ndomains (i.e., Cs = Ct), referred to as Closed-set Domain\nAdaptation (Saenko et al. 2010). However, this assumption\nis often impractical in real-world scenarios.\nIn contrast, Open-set Domain Adaptation (OSDA) ex-\ntends the target label space beyond that of the source do-\nmain (i.e., Cs ⊂Ct) (Saito et al. 2018; Liu et al. 2019),\nthereby adding complexity to the DA task. OSDA aims to\nalign target samples from known classes with those from\nthe source domain while effectively identifying target sam-\nples belonging to categories not observed in the source do-\nmain, referred to as unknown classes (Panareda Busto and\nGall 2017; Bucci, Loghmani, and Tommasi 2020; Jang et al.\n2022). Various criteria based on instance-level predictions\nhave been proposed, including entropy-based (Feng, Xu, and\nTao 2021; Saito et al. 2020) and confidence-based (Saito and\nSaenko 2021; Fu et al. 2020) methods.\nAdditionally, privacy and legal considerations increas-\ningly limit access to labeled source data for adaptation pur-\nposes. To address this, source-free adaptation methods (Fang\net al. 2024) have emerged, enabling adaptation without re-\nliance on labeled source data (Kim et al. 2021; Kundu et al.\n2020a; Li et al. 2020). In this paper, we focus on Source-\nfree Open-set Domain Adaptation (SF-OSDA), where only a\npre-trained source model is available for knowledge transfer,\nwithout access to labeled source data. While some Source-\nfree Domain Adaptation (SF-DA) methods have demon-\nstrated effectiveness in addressing SF-OSDA for classifica-\ntion tasks (Liang, Hu, and Feng 2020; Yang et al. 2022; Wan\net al. 2024), semantic segmentation (Choe et al. 2024), and\ngraph applications (Wang et al. 2024), they primarily focus\non the semantics of known classes in the source domain,\noften overlooking the crucial aspect of novel-class seman-\ntics. These methods focus on segregating target samples with\nlow entropy, categorizing them as known classes, and subse-\nquently optimizing specific objectives such as entropy mini-\nmization or clustering. In this process, data points associated\nwith known classes are prioritized, while those with high en-\ntropy are typically excluded from training, leading to a se-\nmantic disparity between the known and unknown classes.\nTo effectively adapt a pre-trained source model to a target\ndomain facing both category and distribution shifts, we pro-\npose Recall and Refine for Domain Adaptation (RRDA) for\narXiv:2411.12558v1  [cs.CV]  19 Nov 2024\n\nrobust SF-OSDA. RRDA employs a two-step strategy. First,\nwe propose to leverage the semantics of the unknown classes\nby introducing a novel target classifier with K +K′ decision\nboundaries. These boundaries extend the K classes from the\nsource domain with K′ additional classes for the unknown\ncategories. To achieve this, synthetic samples are generated\nin the feature space from target domain features. These syn-\nthetic points are optimized to exhibit low entropy for known\nclasses and high entropy for unknown classes, which are\nthen clustered into K′ categories. The synthetic data are\nused to refine the decision boundaries of the source clas-\nsifier, enabling the target classifier to accommodate the un-\nknown classes. In the second step, any off-the-shelf source-\nfree domain adaptation method (e.g., SHOT (Liang, Hu,\nand Feng 2020), AaD (Yang et al. 2022)) can be integrated\ninto our framework to adapt the entire model to the target\ndomain. RRDA directly learns to classify target unknown\nclasses. The framework introduces K′ as a hyper-parameter,\nwhich we set to K′ = K for simplicity. Sensitivity analysis\nshows that performance improves with higher values of K′,\nthough results remain robust across a range of settings. Ex-\ntensive experiments on three SF-OSDA benchmark datasets\ndemonstrate the effectiveness of our approach, significantly\noutperforming existing methods.\nRelated Work\nUnsupervised Domain Adaptation (UDA) aims to adapt a\nmodel originally trained on a labeled source domain to per-\nform effectively in an unlabeled target domain. This adapta-\ntion process assumes access to data from both the source and\ntarget domains during training (Oza et al. 2023). UDA strate-\ngies often align feature distributions between domains using\nmetric learning techniques (Long et al. 2015; Kang et al.\n2019; Nejjar, Wang, and Fink 2023) or adversarial training\nacross various spaces, including image input space (Murez\net al. 2018; Pizzati et al. 2020), feature space (Ganin and\nLempitsky 2015), and output space (Luo et al. 2019; Vu et al.\n2019). Additionally, various techniques incorporate pseudo-\nlabeling or self-training algorithms (Sun et al. 2022; Dong\net al. 2023; Yue, Sun, and Zhang 2024), which generate\npseudo-labels for unlabeled samples in the target domain.\nHowever, existing approaches assume that label spaces are\nidentical across both domains, limiting their applicability in\nreal-world scenarios.\nOpen-set Domain Adaptation (OSDA) addresses scenar-\nios where the target domain may contain classes not present\nin the source domain (Panareda Busto and Gall 2017; Dong,\nChatzi, and Fink 2024; Dong et al. 2024; Li et al. 2021).\nVarious approaches have been proposed to tackle this chal-\nlenge, including assigning target domain images to source\ncategories while discarding unrelated target domain im-\nages (Panareda Busto and Gall 2017), and using adver-\nsarial training to separate unknown target samples (Saito\net al. 2018; Jang et al. 2022). Separate to Adapt (STA) ap-\nproach (Liu et al. 2019) progressively separates unknown\nand known class samples using a coarse-to-fine weighting\nmechanism and proposes evaluating OSDA on diverse lev-\nels of openness. Rotation-based Open Set (ROS) (Bucci,\nLoghmani, and Tommasi 2020) explores the use of self-\nsupervised tasks such as rotation recognition for unknown\nclass detection. (Jing et al. 2021) project features to a hy-\nperspherical latent space to reject known samples based on\nangular distance. Adjustment and Alignment for Unbiased\nOpen Set Domain Adaptation (ANNA) (Li et al. 2023) ad-\ndresses semantic-level bias in OSDA by designing Front-\nDoor Adjustment and Decoupled Causal Alignment mod-\nules. However, these approaches all assume the availability\nof labeled source data, which can pose challenges due to pri-\nvacy concerns in real applications.\nSource-free Domain Adaptation (SFDA) leverages only a\nsource-trained model and unlabeled target data for adapta-\ntion to the target domain. SFDA approaches can be catego-\nrized into data-based and model-based methods (Yu et al.\n2023). One of the data-driven methods, SHOT, was intro-\nduced by Liang et al. (Liang, Hu, and Feng 2020). It adapts a\npre-trained source model via information maximization with\nself-supervised pseudo-labeling to implicitly align target do-\nmain representations to the source hypothesis. Building on\nthis approach, subsequent works(Chu et al. 2022; Lee et al.\n2022; Qu et al. 2022) refine the adaptation through self-\ntraining techniques. Other works explore different training\nprocedures. For example, historical Contrastive Learning\n(HCL) (Huang et al. 2021) compensates for the absence of\nsource data by leveraging historical models and contrasting\ncurrent and historical embeddings of target samples. Some\nmethods (Yang et al. 2021, 2022) enforce consistency be-\ntween local neighbors by considering local feature density,\nwith Attract and Disperse (AaD) (Yang et al. 2022) treat-\ning SFDA as an unsupervised clustering problem. Addition-\nally, Zhang, Wang, and He (2023) explore leveraging source\nmodel classifier weights as class prototypes to embed class\nrelationships into a similarity measure for a target sample.\nSource-free Open-set Domain Adaptation (SF-OSDA)\nextends SFDA to scenarios where the target domain contains\nnovel classes not present in the source domain. While meth-\nods like SHOT (Liang, Hu, and Feng 2020), AaD (Yang et al.\n2022), and Uncertainty-guided Source-free Domain Adapta-\ntion (U-SFAN) (Roy et al. 2022) have been adapted for SF-\nOSDA, they primarily focus on the known class semantics in\nthe source domain, which can lead to suboptimal handling\nof target-private unknown classes. Universal Domain Adap-\ntation (UniDA) aims to handle domain shifts and label set\ndifferences between source and target domains, encompass-\ning open, partial, and open-partial set scenarios (Liang et al.\n2021; Qu et al. 2023, 2024). Recent SF-UniDA methods\nproposed one-vs-all clustering approaches (Qu et al. 2023)\nand subspace decomposition (Qu et al. 2024) to separate\nand identify common and private target classes in a source-\nfree setup. Similarly, Progressive Graph Learning (Luo et al.\n2023) decomposes the target hypothesis space into shared\nand unknown subspaces for SF-OSDA. However, current\nmethods either require specific training for the source model\nto incorporate the unknown classes (Kundu et al. 2020b,c),\nwhich is usually impractical, or rely on thresholding a met-\nric to distinguish known classes from unknown ones dur-\ning training and inference, making the prediction sensitive\nto different thresholds.\n\n(1) Source Domain Pre-training\nSource Domain\nTarget Domain\nUnlabelled target domain (known\nand unknown classes)\nSynthetic data points\n(2) Target adapation\n(b) Optimisation of synthetic points to show\nlow/high entropy\n(a) Projected Target feature on the source\nclassifier\n(c) Train a new target classifier with the synthetic\npoints\nWeight initlization\nFigure 1: Overview of RRDA for Source-free Open-set Adaptation. Unlike conventional methods that overlook unknown class\nsemantics, RRDA explicitly incorporates this information by generating synthetic points for both known and unknown classes\nfrom projected target features, enabling the training of a new target classifier that captures the semantics of all classes. The\nadaptation is then achieved using standard closed-set domain adaptation methods.\nMethodology\nPreliminary\nFor SF-OSDA, we are given a source pre-trained model f s\nθ\nand an unlabeled target domain with nt samples, denoted\nas Dt = {(xt\ni)}nt\ni=1, where xt\ni ∈X ⊂RX. The target do-\nmain follows a distinct data distribution (P t ̸= P s) from the\nsource domain, reflecting both distribution and label shifts.\nLet Cs and Ct ⊂Y represent the label sets for the source\nand target domains, respectively, where Cs ⊂Ct. Both do-\nmains share K common classes referred to as known classes\n(Ct\nk = Cs). Additionally, the target domain includes target-\nprivate novel classes, jointly considered as a single unknown\nclass (Ct\nunk = Ct \\ Cs).\nThe primary objective of SF-OSDA is to classify both un-\nknown and known classes, relying exclusively on the tar-\nget domain data and a pre-trained source model. The pre-\ntrained model can be decomposed as f s\nθ = hs\nθ ◦gs\nθ, where\nhs\nθ : RX →RD is a feature extractor and gs\nθ : RD →RK\nis the source classifier. Unlike previous works, which freeze\nthe source classifier (e.g., SHOT) during adaptation, we pro-\npose training a new target classifier gt\nθ to explicitly account\nfor target-private unknown classes.\nOne of the challenges in open-set scenarios is the abil-\nity to distinguish known from unknown classes in the target\ndomain. Different approaches have been proposed for dis-\ntinguishing between known and unknown classes, including\nhand-crafted thresholding criteria and clustering strategies.\nHowever, paradigms such as vendor-to-client (Kundu et al.\n2020c) are more effective, as they incorporate an auxiliary\nout-of-distribution classifier during source training, enabling\nbetter handling of unknown classes in the target domain.\nIn this paper, we propose a novel approach to address this\nlimitation by adapting the source classifier post hoc to in-\nclude new decision boundaries for unknown classes. Our\nmethod enables the seamless adaptation of any off-the-shelf\nsource pre-trained model to a target domain, even in the\npresence of novel classes. Motivated by the idea that learn-\ning from unknown class samples can improve performance\nin open-set scenarios, our objective is to simplify adaptation\nand eliminate the dependency on threshold-based methods\nduring inference.\nRRDA\nOur proposed Recall and Refine framework for SF-OSDA\nconsists of three main steps:\n1. Synthetic Data Generation: Referring to step (b) in Fig-\nure 1, synthetic feature points are generated for both\nknown and unknown classes. This involves optimizing\ntarget feature representations using entropy objectives.\n2. Target Classifier Training: Referring to step (c) in Fig-\nure 1, the synthetic feature points are used to train a new\ntarget classifier gt\nθ with extended decision boundaries to\naccommodate unknown classes.\n3. Target Domain Adaptation: The entire model is\nadapted using any off-the-shelf source-free domain adap-\ntation methods (e.g., SHOT, AaD) on target domain data.\nThis allows the model to (1) learn the semantics of both\nknown and unknown classes in the target domain, (2) treat\nOSDA as a simple closed-set scenario, and (3) directly out-\nput predictions for unknown classes.\nSynthetic Data Generation.\nThe first step of our pro-\nposed approach involves generating synthetic features for\nboth known and unknown classes using the source classifier\ngs\nθ. Specifically, we optimize the target feature representa-\ntion zt = hs\nθ(xt) to generate synthetic samples that exhibit\nlow entropy for known classes and high entropy for the un-\nknown class. We denote these optimized synthetic features\nas z∗t\nk and z∗t\nunk. The unknown features are then clustered\nin K′ classes, and a new target classifier gt\nθ is introduced\n\nwith K +K′ classes. In this section, we describe the process\nfor obtaining feature representations for both known and un-\nknown classes. We use standard gradient descent optimiza-\ntion to generate the desired feature representations.\nSynthetic Unknown Classes Generation: To effectively\nidentify points near the source classifier’s decision bound-\nary, we aim to find z∗t\nunk that maximizes entropy while en-\nsuring diverse feature representations, thereby reducing the\nrisk of collapsing to a single-point representation. To prevent\nfeature collapse, we introduce a variance regularization term\nin the form of a hinge function applied to the standard devia-\ntion of features across the batch dimension. Specifically, we\ninitialize the optimization with a noisy version of the origi-\nnal features zt. This process is formulated as follows:\nmin\nzt\n−H(σ(gs\nθ(zt))) + λ · max(0, 1 −\np\nVar(zt)), (1)\nwhere H(p) = −PK\nk=1 pk log(pk) represents the entropy,\nand σ is the softmax activation function, and λ was set\nto 1 for all the experiments. After optimization, only the\npoints satisfying H(gs\nθ(z)) > 0.75 · log(K) (see Ablation\nsection for threshold discussion) are considered as z∗t\nunk.\nThe selected features z∗t\nunk are then clustered into K′ un-\nknown classes using K-means. Each cluster is assigned a\npseudo-label corresponding to a new class index, ˆy∗t\nunk ∈\n{K + 1, ..., K + K′}, representing the specific unknown\nclass assigned to the synthetic features. These synthetic fea-\ntures and their associated pseudo-labels (z∗t\nunk, ˆy∗t\nunk) will\nbe used in the subsequent training of the target classifier.\nThis approach is motivated by the observation in the litera-\nture (Lampert, Nickisch, and Harmeling 2009) that it is pos-\nsible to generate meaningful semantics for novel classes us-\ning known classes.\nSynthetic Known Classes Generation: A similar opti-\nmization approach is employed to generate synthetic data\npoints for the known classes. The optimization is performed\niteratively K times, once for each known class k (where\nk ∈[1, ..., K]). The objective is to minimize the cross-\nentropy for each class directly from zt. The optimization\nproblem for generating a sample for class k is defined as:\nmin\nzt\nLCE(gs\nθ(zt), Ik) + λ · max(0, 1 −\np\nVar(zt)), (2)\nwhere Ik is the identity function for the k-th class (i.e., a\none-hot vector), and λ controls the regularization term, set\nto 1 in all experiments. After optimization, only the points\nsatisfying LCE(gs\nθ(z), Ik) < 0.25 · log(K) (see Ablation\nsection for threshold discussion) are considered as z∗t\nk . Each\nselected synthetic feature z∗t\nk is assigned the pseudo-label\nˆy∗t\nk\n= k, forming the pairs (z∗t\nk ,ˆy∗t\nk ). These synthetic data\npoints and their corresponding pseudo-labels are then used\nto train the target classifier. By iteratively generating fea-\nture points for each known class, our method enhances the\ndecision boundaries without requiring access to the original\nsource data or labels.\nTarget Classifier Training.\nIn the second step, we intro-\nduce a new target classifier gt\nθ with K + K′ classes, where\nK is the number of known classes and K′ is the number of\nunknown classes. The weights for the known classes gt\nθ[1:K]\nare initialized using the source classifier’s weights gs\nθ, while\nthe weights for the unknown classes gt\nθ[K+1:K+K′] are ran-\ndomly initialized. The target classifier gt\nθ is trained using the\nsynthetic feature-label pairs for the known classes (z∗t\nk ,ˆy∗t\nk )\nfor k ∈{1, ..., K}, and the unknown classes (z∗t\nunk, ˆy∗t\nunk).\nThe supervised training objective is defined as:\nmin\nθ\nLCE(gt\nθ(z∗t), ˆ\ny∗t),\n(3)\nwhere z∗t represents the combined synthetic features for\nboth known and unknown classes, and ˆy∗t represents their\ncorresponding labels. The results of the previous steps, in-\ncluding the refined decision boundaries achieved by RRDA,\nare illustrated in Figure 2.\nTarget Domain Adaptation.\nAny source-free unsuper-\nvised domain adaptation method (originally designed for\nclosed-set scenarios) can be integrated into our approach\nto address open-set scenarios, provided it incorporates a\ndiversity loss or a similar mechanism to facilitate self-\nlearning of unknown classes. To empirically validate this\nhypothesis, we consider SHOT (Liang, Hu, and Feng\n2020) and AaD (Yang et al. 2022), using their respective\ntraining objectives for adaptation. SHOT (Liang, Hu, and\nFeng 2020) employs information maximization and self-\nsupervised pseudo-labeling to adapt the source model to the\ntarget domain. Its objective function can be expressed as:\nLshot = −λent\nnt\nnt\nX\ni=1\nK+K′\nX\nk=1\npk,i log pk,i + λdiv ·\nK+K′\nX\nk=1\n¯pk log ¯pk\n+ λps · Lpseudo,\nwhere ¯pk =\n1\nnt\nPnt\ni=1 pk(xi; θ), and Lpseudo is the pseudo-\nlabeling loss function from (Liang, Hu, and Feng 2020).\nDuring adaptation, only the feature encoder is updated while\nthe classifier remains frozen. AaD (Yang et al. 2022) lever-\nages local consistency and global dispersion. The objective\nfunction for feature i is formulated as:\nLAaD,i = −\nX\nj∈Ci\npT\ni pj + λ\nX\nm∈Bi\npT\ni pm,\nwhere Ci represents the local neighborhood of feature i and\nBi is the mini-batch feature not in Ci. Unlike SHOT, AaD\nupdates the entire model weights during adaptation.\nExperiments\nExperimental Setup\nDatasets. Office-Home (Venkateswara et al. 2017) com-\nprises 65 labeled image categories from four distinct do-\nmains: Art (Ar), Clipart (Cl), Product (Pr), and Real World\n(Rw). We designate the first 25 alphabetically ordered cate-\ngories as known classes, with the remaining 40 as unknown.\nOffice-31 (Saenko et al. 2010) consists of 31 classes across\nthree domains: Amazon (A), Dslr (D), and Webcam (W). We\nassign the first 10 as known and the last 10 classes as un-\nknown. VisDA (Peng et al. 2017) have 12 categories across\n\n6\n4\n2\n0\n2\n4\n6\nFeature 1\n6\n4\n2\n0\n2\n4\n6\nFeature 2\nTarget-shared classes 0\nTarget-shared classes 1\nTarget-shared classes 2\nTarget-private classes\n(a) Unlabeled target domain projected\nonto the decision boundary of the source\nclassifier.\n6\n4\n2\n0\n2\n4\n6\nFeature 1\n6\n4\n2\n0\n2\n4\n6\nFeature 2\nSynthetic point Class 0\nSynthetic point Class 1\nSynthetic point Class 2\nOutlier Class 0\nOutlier Class 1\nOutlier Class 2\n(b) Optimized synthetic points for known\nand unknown classes.\n6\n4\n2\n0\n2\n4\n6\nFeature 1\n6\n4\n2\n0\n2\n4\n6\nFeature 2\nTarget-shared classes 0\nTarget-shared classes 1\nTarget-shared classes 2\nTarget-private classes\n(c) Unlabeled target domain projected\nonto the new decision boundary of the tar-\nget classifier.\nFigure 2: Visualization of the synthetic data generation process and the resulting target classifier boundary on a toy example\nwith K = K′ = 3 classes.\ntwo domains: Real (R) and Synthetic (S). The first 6 classes\nare categorized as known and the remaining 6 as unknown.\nEvaluation Metrics. To assess model performance, we\nadopt standard evaluation metrics widely used in previous\nOSDA studies (Bucci, Loghmani, and Tommasi 2020; Liu\net al. 2019; Li et al. 2023). The Harmonic Open-set (HOS)\naccuracy balances performance on known and unknown\nclasses and can be calculated as HOS =\n2×OS∗×UNK\nOS∗+UNK ,\nwhere OS∗represents the accuracy of known classes,\nand UNK denotes the accuracy of unknown classes. The\nHOS metric provides a comprehensive measure, by equally\nweighting the model’s ability to classify known classes and\ndetect unknown classes.\nImplementation Details. All experiments are conducted on\na single A100 GPU using PyTorch. For synthetic data gen-\neration, we employ the Adam optimizer with a learning rate\nof 0.001 for 1000 steps, for both known (z∗t\nk ) and unknown\n(z∗t\nunk) classes. In all main experiments, we set K′ = K. To\nmaintain class balance, we cap the sample size at 1000 for\nknown classes in Office-Home and Office-31, and 10, 000\nfor VisDA. The target classifier is trained for 50 epochs us-\ning SGD with a learning rate of 0.01, momentum of 0.9,\nweight decay of 0.001, and a fixed batch size of 128. During\ntarget model adaptation, we use SGD with momentum 0.9,\na batch size of 64, and train for 50 epochs. The learning rate\nis set to 0.001 for Office-31 and Office-Home, and 0.0001\nfor VisDA, when using Resnet-50 (He et al. 2016) as back-\nbone. When using ViT-B (Wu et al. 2020) as the backbone,\nwe set the learning rate to 0.0001 for all experiments. For\nSHOT, we freeze the target classifier and train only the fea-\nture extractor and backbone. For AaD, all model parameters\nare trained, with the feature extractor’s learning rate set to\n10 times lower. During inference, samples belonging to the\nnew K′ classes are considered unknown target samples. λps\nwas set to 0.1, 0.3, and 0.4 for Office-Home, Office-31, and\nVisDA respectively. The hyper-parameters λdiv and λ were\nset to 1 and λent was set to 0.5 in all our experiments.\nDuring inference, samples assigned to any of the new\nK′ classes are treated as unknown target samples. Follow-\ning the standard open-set protocol, predictions in the range\n[1, K] correspond to known classes, while predictions in\nMethods\nSF\nOffice-31\nA2D A2W D2A D2W W2A W2D Avg\nCMU\n✗\n52.6\n55.7\n76.5\n75.9\n65.8\n64.7 65.2\nDANCE\n✗\n84.9\n78.8\n79.1\n78.8\n68.3\n78.8 79.8\nOSLPP\n✗\n91.5\n89.0\n79.3\n92.3\n78.7\n9..6\n87.4\nGATE\n✗\n88.4\n86.5\n84.2\n95.0\n86.1\n96.7 89.5\nANNA\n✗\n83.8\n85.5\n82.5\n99.5\n81.6\n98.4 88.6\nSource-only\n✓\n78.2\n72.1\n44.2\n82.2\n52.1\n88.8 69.6\nUMAD\n✓\n88.5\n84.4\n86.8\n95.0\n88.2\n95.9 89.8\nLEAD\n✓\n84.9\n85.1\n90.9\n94.8\n90.3\n96.5 90.3\nGLC\n✓\n82.6\n74.6\n92.6\n96.0\n91.8\n96.1 89.0\nAaD-O\n✓\n82.3\n79.0\n84.3\n93.1\n84.8\n95.0 86.4\nAaD + RRDA\n✓\n91.1\n94.3\n94.1\n96.6\n94.0\n96.2 94.4\n+8.8 +15.3 +9.8 +3.5\n+9.2\n+1.2 +8.0\nSHOT-O\n✓\n89.5\n83.0\n85.9\n91.4\n84.0\n95.2 88.2\nSHOT+ RRDA ✓\n90.0\n92.2\n92.6\n98.2\n91.6\n98.2 93.8\n+0.5\n+9.2\n+6.7 +6.8\n+7.6\n+3.0 +5.6\nTable 1: HOS (%) results on Office-31 (ResNet-50). SF\ndenotes source-free methods. AaD-O and SHOT-O are the\nadapted open-set methods of AaD and SHOT. RRDA uses\nstandard AaD and SHOT versions (closed-set scenario).\n[K + 1, K + K′] are aggregated into a single K + 1 class,\nrepresenting the unknown class.\nBaselines. We compare our method against open-set domain\nadaptation approaches, including both non-source-free and\nsource-free methods. The non-source-free methods include\nOSBP (Saito et al. 2018), CMU (Fu et al. 2020), STA (Liu\net al. 2019), DANCE (Saito et al. 2020), GATE (Chen et al.\n2022), ANNA (Li et al. 2023), and OSLPP (Wang, Meng,\nand Breckon 2024). For source-free methods, we consider\nUMAD (Liang et al. 2021), GLC (Qu et al. 2023), SF-PGL\n(Luo et al. 2023), and LEAD (Qu et al. 2024).\nExperimental Results\nFrom Table 1 to 3, we compare our method against state-\nof-the-art (SOTA) OSDA methods in both source-free and\nnon-source-free setups. We include non-SF methods to pro-\nvide a comprehensive performance benchmark, despite our\nfocus on SF scenarios. We use RRDA alongside AaD and\n\nMethods\nSF\nOffice-Home\nAr2Cl Ar2Pr Ar2Rw Cl2Ar Cl2Pr Cl2Rw Pr2Ar Pr2Cl Pr2Rw Rw2Ar Rw2Cl Rw2Pr Avg\nCMU\n✗\n55.0\n57.0\n59.0\n59.3\n58.2\n60.6\n59.2\n51.3\n61.2\n61.9\n53.5\n55.3\n57.6\nResNet-50\nDANCE\n✗\n6.5\n9.0\n9.9\n20.4\n10.1\n9.2\n28.1\n15.8\n12.6\n14.2\n7.9\n13.7\n12.9\nOSLPP\n✗\n61.0\n72.8\n74.3\n60.9\n66.9\n70.4\n63.6\n59.3\n74.0\n67.2\n59.0\n74.4\n67.0\nGATE\n✗\n63.8\n70.5\n75.8\n66.4\n67.9\n71.7\n67.3\n61.3\n76.0\n70.4\n61.8\n75.4\n69.0\nANNA\n✗\n69.0\n73.7\n76.8\n64.7\n68.6\n73.0\n66.5\n63.1\n76.6\n71.3\n65.7\n78.7\n70.7\nSource-only\n✓\n46.1\n63.3\n72.9\n42.8\n54.0\n58.7\n47.8\n36.1\n66.2\n60.8\n45.3\n68.2\n55.2\nUMAD\n✓\n59.2\n71.8\n76.6\n63.5\n69.0\n71.9\n62.5\n54.6\n72.8\n66.5\n57.9\n70.7\n66.4\nLEAD\n✓\n60.7\n70.8\n76.5\n61.0\n68.6\n70.8\n65.3\n59.8\n74.2\n64.8\n57.7\n75.6\n67.2\nGLC\n✓\n65.3\n74.2\n79.0\n60.4\n71.6\n74.7\n63.7\n63.2\n75.8\n67.1\n64.3\n77.8\n69.8\nAaD-O\n✓\n58.0\n68.2\n75.4\n58.8\n65.7\n69.0\n54.6\n52.9\n72.3\n65.8\n56.3\n72.2\n64.1\nAaD + RRDA\n✓\n61.7\n72.8\n73.5\n59.0\n74.9\n69.9\n59.5\n58.3\n71.2\n64.5\n64.8\n73.2\n66.9\n+3.7\n+4.6\n-1.9\n+0.2\n+9.2\n+0.9\n+4.9\n+5.4\n-1.1\n-1.3\n+7.7\n+1.0\n+2.8\nSHOT-O\n✓\n57.2\n65.4\n69.9\n58.1\n62.6\n64.3\n60.5\n52.8\n71.1\n64.4\n53.5\n40.6\n61.9\nSHOT + RRDA\n✓\n64.6\n74.2\n77.2\n63.1\n71.4\n71.3\n67.7\n59.1\n76.7\n70.2\n67.4\n76.7\n70.0\n+7.4\n+8.8\n+7.3\n+5.0\n+8.8\n+7.0\n+7.2\n+6.3\n+5.6\n+5.8\n+13.9\n+36.1\n+8.1\nSource-only\n✓\n57.1\n69.5\n79.9\n50.2\n62.5\n66.0\n52.2\n45.7\n75.1\n69.3\n56.4\n73.7\n63.1\nViT\nLEAD\n✓\n58.6\n74.7\n82.7\n58.9\n74.6\n74.3\n59.0\n47.1\n78.3\n71.9\n58.7\n77.4\n68.0\nAaD-O\n✓\n57.8\n74.9\n82.7\n53.9\n68.6\n70.8\n52.5\n45.8\n76.8\n70.6\n58.2\n77.7\n65.9\nAaD + RRDA\n✓\n67.4\n77.2\n81.2\n71.4\n71.5\n76.1\n73.9\n63.8\n78.5\n74.8\n67.5\n75.0\n73.2\n+9.6\n+1.5\n-1.5\n+17.5\n+2.9\n+5.3\n+21.4 +18.0\n+1.7\n+4.2\n+9.3\n-2.7\n+7.3\nSHOT-O\n✓\n63.6\n73.5\n81.7\n66.7\n69.8\n75.5\n66.5\n56.2\n79.0\n73.5\n62.6\n74.6\n70.3\nSHOT + RRDA\n✓\n68.9\n75.0\n81.4\n71.2\n73.8\n73.6\n71.9\n60.4\n79.2\n76.5\n66.2\n77.5\n73.0\n+5.3\n+1.5\n-0.3\n+4.5\n+4.0\n-2.1\n+5.4\n+4.2\n+0.2\n+3.0\n+3.6\n+2.9\n+2.7\nTable 2: HOS (%) results on Office-Home (ResNet-50 and ViT). |Cs| = 25, |Ct| = 65. SF denotes source-free methods.\nMethods\nVisDA\nBic\nBus\nCar\nMot\nTra\nTru\nUNK HOS\nOSBP\n35.6 59.8 48.3 76.8 55.5 29.8\n81.7\n62.7\nResNet-50\nSTA\n50.1 69.1 59.7 85.7 84.7 25.1\n82.4\n71.0\nSource-only\n16.3\n7.9\n24.9 48.0\n6.1\n0.0\n72.7\n27.9\nLEAD\n83.5 65.2 57.7 35.7 82.1 79.5\n82.7\n74.2\nSF-PGL\n91.5 90.1 74.1 90.3 81.9 74.8\n72.0\n77.4\nAaD-O\n86.8 69.8 51.5 38.7 84.3 26.0\n65.5\n62.4\nAaD + RRDA 96.0 85.7 34.5 37.6 92.2 46.4\n71.7\n68.4\n+9.2 +15.9 -17.0 -1.1 +7.9 +20.4 +6.2\n+6.0\nShot-O\n82.1 67.0 78.6 57.3 72.2 17.9\n50.7\n56.0\nShot + RRDA 88.6 82.2 66.8 47.3 87.2 74.3\n83.5\n78.7\n+6.5 +15.2 -11.8 -10.0 +15.0 +56.4 +32.8 +22.7\nSource-only\n62.3 17.9 17.7 50.7\n0.0\n0.6\n90.8\n39.1\nViT\nLEAD\n87.6 65.3 49.8 30.5 70.9 54.4\n98.2\n74.3\nAaD-O\n87.9 77.6 47.7 36.8 61.2 16.6\n67.6\n60.4\nAaD + RRDA 98.2 91.1 84.8 39.4 94.2 97.5\n81.7\n82.9\n+10.3 +13.5 +37.1 +2.6 +33.0 +80.9 +14.1 +22.5\nShot-O\n96.7 77.0 80.4 75.9\n3.0\n4.7\n80.1\n66.1\nShot + RRDA 95.4 84.8 74.1 48.7 85.1 82.3\n79.3\n78.9\n-1.3\n+7.8\n-6.3 -27.2 +82.1 +77.6 -0.8 + 12.8\nTable 3: Accuracy for each class (%) and HOS (%) results\non VisDA (ResNet-50 and ViT), with |Cs| = 6, |Ct| = 12.\nSHOT (vanilla methods for closed-set scenarios), as well as\ntheir open-set variants denoted as AaD-O and SHOT-O that\nrely on entropy-thresholding during training and inference.\nResults for comparison methods are sourced from (Qu et al.\n2024; Li et al. 2023), and the mean HOS is reported.\nOffice-31. Table 1 presents results on the Office-31 dataset,\nwhere RRDA demonstrates significant improvements over\nthreshold-based methods. AaD+RRDA achieves an average\nHOS of 94.4%, which is an 8.0% increase over AaD-O. Sim-\nilarly, SHOT+RRDA reaches 93.8%, representing a 5.6%\nimprovement over SHOT-O. These results surpass all com-\npared source-free and non-source-free SOTA methods.\nOffice-Home. On the Office-Home dataset (Table 2), RRDA\nconsistently enhances the performance of both AaD-O and\nSHOT-O across most domain adaptation tasks. AaD+RRDA\nand SHOT+RRDA show average HOS improvements of\n+2.8% and +8.1% respectively. SHOT+RRDA achieves\na competitive 70.0% average HOS, outperforming most\nmethods, including both source-free and non-source-free\napproaches, while falling just slightly short of ANNA a\nnon-source-free adaptation method. A similar observation\ncan be made when using ViT as backbone, where RRDA\nconsistently improves previous methods AaD+RRDA and\nSHOT+RRDA show average HOS improvements of +7.3%\nand +2.7% respectively, and surpass the other baselines.\nVisDA. On the challenging VisDA dataset (Table 3), RRDA\ncontinues to demonstrate its effectiveness. AaD+RRDA im-\nproves upon AaD-O by +6.0% in HOS, significantly im-\nproving unknown sample recognition and overall class accu-\nracy. Significant improvements are observed in classes such\nas ”Bus” (+15.9%) and ”Truck” (+20.4%). Similar improve-\nments can be observed when applying RRDA to SHOT with\nan improvement of +22.7% in HOS. We observe that both\nmethods improve over the same class and degrade the per-\nformances of the ”car” and ”motorcycles” classes.\nThese results demonstrate RRDA’s consistent superior-\nity across various domain adaptation scenarios. Our method\nsignificantly improves existing SF-OSDA techniques, as\nevidenced by the consistent performance gains across all\ndatasets. The key advantage of RRDA lies in its novel ap-\nproach to handling unknown classes. Unlike previous meth-\nods that rely on thresholding and discard unknown class\n\nEntropy\nDiversity\nOS*\nUNK\nHOS\n95.6\n79.8\n86.5\n✓\n95.0\n91.8\n93.4\n✓\n✓\n95.8\n91.9\n93.8\nTable 4: Ablation on the optimization objective to generate\nsynthetic points. Results using SHOT+RRDA on Office-31.\nMethods\n0.1/0.9\n0.2/0.8\n0.25/0.75\n0.3/0.7\n0.4/0.6\n0.5/0.5\nSHOT\n93.3\n91.3\n91.1\n91.1\n91.1\n91.9\nAaD\n90.9\n89.5\n90.0\n89.7\n90.1\n90.0\nTable 5: Ablation on the optimization objective to generate\nsynthetic points. Results using SHOT+RRDA on Office-31.\ndata during adaptation, RRDA actively learns the semantics\nof unknown classes through our adaptive target classifier,\nwhich evolves to accommodate the unknown class distri-\nbution. Furthermore, the consistent performance gains with\nboth AaD and SHOT demonstrate RRDA’s versatility. These\nresults underscore the importance of explicitly modeling un-\nknown classes in open-set domain adaptation, rather than\ntreating them as outliers to be discarded.\nAblation Study and Sensitivity Analysis\nOptimization Process. We conducted ablation studies on\nOffice-31 with three different settings to train the new clas-\nsifier. The results are shown in Table 4. We compare the fol-\nlowing scenarios: (1) selecting target features based on en-\ntropy threshold without optimization, (2) optimizing entropy\nwithout hinge loss for diversity, and (3) the full proposed\nmethod optimizing feature points based on entropy and di-\nversity. Our findings are as follows: (1) Using target features\nbased on entropy directly to train the target classifier leads\nto the worst results in terms of HOS. SHOT-O achieves an\nHOS of 88.2 %, while using features directly without op-\ntimization achieves an HOS of 86.5%. (2) Optimizing the\npoints significantly improves performance. There is a slight\nadditional improvement when using hinge loss during opti-\nmization to promote diversity. (3) The full proposed method,\nwhich optimizes feature points based on both entropy and\ndiversity, yields the best performance. We used SHOT for\nadaptation in the experiment as it keeps the classifier frozen,\nallowing for a direct performance comparison with the new\nclassifier.\nThreshold Sensitivity Analysis. To further analyze the hy-\nperparameter sensitivity and its impact on performance, we\nexamined the effect of varying the entropy threshold used\nfor feature selection during the optimization process. The\nthresholds were evaluated on the A2D task (ref Table 5).\nWe observe that the best-performing threshold on this\ntask is T = 0.1. However, the HOS score remains consis-\ntent across different thresholds. For larger datasets, such as\nVisDA, where the domain shifts are more significant, lower\nthresholds (e.g., T = 0.1) can result in highly imbalanced\ndatasets, with some classes being excluded entirely. For ex-\nample, under such thresholds, a subset of classes may not\nmeet the selection criteria. To ensure consistency across all\ndatasets while maintaining a balanced feature distribution,\nwe report results using T = 0.25 throughout the experi-\nments. This threshold provides a balance between maintain-\ning sufficient class representation and achieving competitive\nperformance, particularly in scenarios with significant do-\nmain shifts.\nVarying Unknown Classes. We investigated the robustness\nof our framework against an increased number of unknown\nprivate classes, which complicates the distinction between\nknown and unknown classes. We compared our method to\nLEAD, SHOT-O, and AaD-O on the Office-31 dataset. As\nshown in Figure 3a, our RRDA method in combination\nwith SHOT and AaD achieves stable results and consistently\noutperforms existing approaches. For consistency with our\nmain results, we kept K′ fixed at 10.\nSensitivity to K′. Figure 3b shows adaptation performance\nfor different K′ values of the target classifier on Office-31\ndataset. The performance improves as K′ increases, validat-\ning the benefit of inheriting class separability knowledge,\nbefore eventually reaching a plateau. In fact, K′ = 15\nyields the best results. For the main experiments, we re-\nported Office-31 results using K′ = K = 10.\nTraining Stability. Figure 3c illustrates the training curves\nfor the A2W task on the Office-31 dataset. Our method\nshows consistent HOS improvement on the test set, with\nsteadily increasing before plateauing. In contrast, AaD-O\nexhibits unstable training, with noticeable performance fluc-\ntuations throughout the training process.\nFeature Space Visualization. Figure 4 shows t-SNE em-\nbeddings of pre-classifier features for the source-only\nmodel, AaD-O, and our method on the A2W task on the\nOffice-31 dataset. The source-only model (Figure 4a) ex-\nhibits well-separated known class clusters but mixes un-\nknown samples with known classes. AaD-O (Figure 4b)\nslightly improves known-unknown separation, but class\noverlap remains. Our method (Figure 4c) achieves supe-\nrior separation of known and unknown classes, maintaining\ntight, well-defined known class clusters while isolating un-\nknown samples. This demonstrates our method’s effective-\nness in inheriting class separability during adaptation.\nConclusion\nIn this work, we introduce Recall and Refine for Domain\nAdaptation (RRDA), a simple but effective framework for\nSF-OSDA. RRDA enables the successful adaptation of off-\nthe-shelf source pre-trained models to target domains, effec-\ntively addressing both distribution and category shift prob-\nlems. RRDA achieves this by introducing a new target clas-\nsifier that aids in classifying and learning the semantics of\nboth known and unknown classes. This approach enables the\ndirect use of source-free adaptation methods designed for\nclosed-set scenarios in open-set contexts. Extensive exper-\niments on three challenging benchmarks demonstrate that\nRRDA significantly outperforms existing SF-OSDA meth-\nods and even surpasses OSDA methods that have access to\nthe source domain. Future work could explore its potential\nfor continuous adaptation in the setup where new classes ap-\npear over time.\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNumber of Unknown Private Target Classes\n0.80\n0.83\n0.85\n0.88\n0.90\n0.93\n0.95\n0.98\nHOS\nLEAD\nAaD-O\nAaD + RRDA (Ours)\nSHOT-O\nSHOT + RRDA (Ours)\n(a) Various openness settings\n1 2 3 4 5 6 7 8 9 10 11 12131415161718192021222324\nNumber of target classes (K′) for the classifier\n0.78\n0.80\n0.82\n0.85\n0.88\n0.90\n0.93\n0.95\nHOS\nSOTA (LEAD)\n(b) Sensitivity to K′\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nEpoch\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nHOS\n0.72\n0.94\n0.76\n0.78\n0.75\n0.85\nAaD + RRDA (Ours)\nAaD-O\n SOTA (LEAD)\n(c) HOS convergence\nFigure 3: Sensitivity analysis on Office-31. (a) Adaptation performance across different openness levels (average across all\ntransfer tasks). (b) Sensitivity to K′ target classifier classes (average across all transfer tasks). (c) HOS curves for the A2W\ntask.\n(a) Source Only\n(b) AaD-O\n(c) AaD + RRDA (Ours)\nFigure 4: T-SNE visualization of the pre-classifier feature space for the A2W task on Office-31 dataset.\nReferences\nBen-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.;\nPereira, F.; and Vaughan, J. W. 2010. A theory of learning\nfrom different domains. Machine learning, 79: 151–175.\nBucci, S.; Loghmani, M. R.; and Tommasi, T. 2020. On the\neffectiveness of image rotation for open set domain adapta-\ntion. In European conference on computer vision, 422–438.\nSpringer.\nChen, L.; Lou, Y.; He, J.; Bai, T.; and Deng, M. 2022. Geo-\nmetric anchor correspondence mining with uncertainty mod-\neling for universal domain adaptation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 16134–16143.\nChoe, S.-A.; Shin, A.-H.; Park, K.-H.; Choi, J.; and Park, G.-\nM. 2024. Open-Set Domain Adaptation for Semantic Seg-\nmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 23943–23953.\nChu, T.; Liu, Y.; Deng, J.; Li, W.; and Duan, L. 2022. De-\nnoised Maximum Classifier Discrepancy for Source-Free\nUnsupervised Domain Adaptation. In Proceedings of the\nAAAI conference on artificial intelligence, volume 36, 472–\n480.\nDong, H.; Chatzi, E.; and Fink, O. 2024.\nTowards Mul-\ntimodal Open-Set Domain Generalization and Adaptation\nthrough Self-supervision. In European Conference on Com-\nputer Vision.\nDong, H.; Nejjar, I.; Sun, H.; Chatzi, E.; and Fink, O. 2023.\nSimMMDG: A Simple and Effective Framework for Multi-\nmodal Domain Generalization. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS).\nDong, H.; Zhao, Y.; Chatzi, E.; and Fink, O. 2024. Mul-\ntiOOD: Scaling Out-of-Distribution Detection for Multiple\nModalities. arXiv preprint arXiv:2405.17419.\nFang, Y.; Yap, P.-T.; Lin, W.; Zhu, H.; and Liu, M. 2024.\nSource-free unsupervised domain adaptation: A survey.\nNeural Networks, 106230.\nFeng, Z.; Xu, C.; and Tao, D. 2021. Open-set hypothesis\ntransfer with semantic consistency. IEEE Transactions on\nImage Processing, 30: 6473–6484.\nFu, B.; Cao, Z.; Long, M.; and Wang, J. 2020. Learning\nto detect open classes for universal domain adaptation. In\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XV\n16, 567–583. Springer.\nGanin, Y.; and Lempitsky, V. 2015. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, 1180–1189. PMLR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\n\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHoyer, L.; Dai, D.; Wang, H.; and Van Gool, L. 2023. MIC:\nMasked Image Consistency for Context-Enhanced Domain\nAdaptation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 11721–\n11732.\nHuang, J.; Guan, D.; Xiao, A.; and Lu, S. 2021. Model adap-\ntation: Historical contrastive learning for unsupervised do-\nmain adaptation without source data. Advances in Neural\nInformation Processing Systems, 34: 3635–3649.\nJang, J.; Na, B.; Shin, D. H.; Ji, M.; Song, K.; and Moon,\nI.-C. 2022. Unknown-aware domain adversarial learning for\nopen-set domain adaptation. Advances in Neural Informa-\ntion Processing Systems, 35: 16755–16767.\nJing, M.; Li, J.; Zhu, L.; Ding, Z.; Lu, K.; and Yang, Y.\n2021.\nBalanced open set domain adaptation via centroid\nalignment. In Proceedings of the AAAI conference on ar-\ntificial intelligence, volume 35, 8013–8020.\nKang, G.; Jiang, L.; Yang, Y.; and Hauptmann, A. G. 2019.\nContrastive adaptation network for unsupervised domain\nadaptation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 4893–4902.\nKim, Y.; Cho, D.; Han, K.; Panda, P.; and Hong, S. 2021.\nDomain adaptation without source data. IEEE Transactions\non Artificial Intelligence, 2(6): 508–518.\nKundu, J. N.; Venkat, N.; Babu, R. V.; et al. 2020a. Uni-\nversal source-free domain adaptation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 4544–4553.\nKundu, J. N.; Venkat, N.; M V, R.; and Babu, R. V. 2020b.\nUniversal Source-Free Domain Adaptation.\nIn The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nKundu, J. N.; Venkat, N.; Revanur, A.; Babu, R. V.; et al.\n2020c.\nTowards inheritable models for open-set domain\nadaptation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 12376–12385.\nLampert, C. H.; Nickisch, H.; and Harmeling, S. 2009.\nLearning to detect unseen object classes by between-class\nattribute transfer. In 2009 IEEE conference on computer vi-\nsion and pattern recognition, 951–958. IEEE.\nLee, J.; Jung, D.; Yim, J.; and Yoon, S. 2022. Confidence\nscore for source-free unsupervised domain adaptation. In In-\nternational conference on machine learning, 12365–12377.\nPMLR.\nLi, G.; Kang, G.; Zhu, Y.; Wei, Y.; and Yang, Y. 2021. Do-\nmain consensus clustering for universal domain adaptation.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 9757–9766.\nLi, R.; Jiao, Q.; Cao, W.; Wong, H.-S.; and Wu, S. 2020.\nModel adaptation: Unsupervised domain adaptation without\nsource data. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 9641–9650.\nLi, W.; Liu, J.; Han, B.; and Yuan, Y. 2023. Adjustment\nand Alignment for Unbiased Open Set Domain Adaptation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 24110–24119.\nLiang, J.; Hu, D.; and Feng, J. 2020. Do we really need to\naccess the source data? source hypothesis transfer for unsu-\npervised domain adaptation. In International conference on\nmachine learning, 6028–6039. PMLR.\nLiang, J.; Hu, D.; Feng, J.; and He, R. 2021. Umad: Univer-\nsal model adaptation under domain and category shift. arXiv\npreprint arXiv:2112.08553.\nLiu, H.; Cao, Z.; Long, M.; Wang, J.; and Yang, Q. 2019.\nSeparate to adapt: Open set domain adaptation via progres-\nsive separation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2927–2936.\nLong, M.; Cao, Y.; Wang, J.; and Jordan, M. 2015. Learn-\ning transferable features with deep adaptation networks.\nIn International conference on machine learning, 97–105.\nPMLR.\nLuo, Y.; Wang, Z.; Chen, Z.; Huang, Z.; and Baktashmot-\nlagh, M. 2023. Source-free progressive graph learning for\nopen-set domain adaptation. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence.\nLuo, Y.; Zheng, L.; Guan, T.; Yu, J.; and Yang, Y. 2019. Tak-\ning a closer look at domain shift: Category-level adversaries\nfor semantics consistent domain adaptation. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 2507–2516.\nMurez, Z.; Kolouri, S.; Kriegman, D.; Ramamoorthi, R.; and\nKim, K. 2018. Image to image translation for domain adap-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 4500–4509.\nNejjar, I.; Wang, Q.; and Fink, O. 2023. DARE-GRAM: Un-\nsupervised domain adaptation regression by aligning inverse\ngram matrices. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 11744–11754.\nOza, P.; Sindagi, V. A.; Sharmini, V. V.; and Patel, V. M.\n2023. Unsupervised domain adaptation of object detectors:\nA survey. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence.\nPanareda Busto, P.; and Gall, J. 2017.\nOpen set domain\nadaptation. In Proceedings of the IEEE international con-\nference on computer vision, 754–763.\nPeng, X.; Usman, B.; Kaushik, N.; Hoffman, J.; Wang, D.;\nand Saenko, K. 2017. Visda: The visual domain adaptation\nchallenge. arXiv preprint arXiv:1710.06924.\nPizzati, F.; Charette, R. d.; Zaccaria, M.; and Cerri, P. 2020.\nDomain bridge for unpaired image-to-image translation and\nunsupervised domain adaptation.\nIn Proceedings of the\nIEEE/CVF winter conference on applications of computer\nvision, 2990–2998.\nQu, S.; Chen, G.; Zhang, J.; Li, Z.; He, W.; and Tao, D. 2022.\nBmd: A general class-balanced multicentric dynamic proto-\ntype strategy for source-free domain adaptation. In Euro-\npean conference on computer vision, 165–182. Springer.\nQu, S.; Zou, T.; He, L.; R¨ohrbein, F.; Knoll, A.; Chen,\nG.; and Jiang, C. 2024. Lead: Learning decomposition for\nsource-free universal domain adaptation. In Proceedings of\n\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 23334–23343.\nQu, S.; Zou, T.; R¨ohrbein, F.; Lu, C.; Chen, G.; Tao, D.;\nand Jiang, C. 2023. Upcycling models under domain and\ncategory shift. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 20019–20028.\nRoy, S.; Trapp, M.; Pilzer, A.; Kannala, J.; Sebe, N.; Ricci,\nE.; and Solin, A. 2022. Uncertainty-guided source-free do-\nmain adaptation. In European Conference on Computer Vi-\nsion, 537–555. Springer.\nSaenko, K.; Kulis, B.; Fritz, M.; and Darrell, T. 2010. Adapt-\ning visual category models to new domains. In Computer\nVision–ECCV 2010: 11th European Conference on Com-\nputer Vision, Heraklion, Crete, Greece, September 5-11,\n2010, Proceedings, Part IV 11, 213–226. Springer.\nSaito, K.; Kim, D.; Sclaroff, S.; and Saenko, K. 2020.\nUniversal domain adaptation through self supervision.\nAdvances in neural information processing systems, 33:\n16282–16292.\nSaito, K.; and Saenko, K. 2021.\nOvanet: One-vs-all net-\nwork for universal domain adaptation. In Proceedings of the\nieee/cvf international conference on computer vision, 9000–\n9009.\nSaito, K.; Yamamoto, S.; Ushiku, Y.; and Harada, T. 2018.\nOpen set domain adaptation by backpropagation. In Pro-\nceedings of the European conference on computer vision\n(ECCV), 153–168.\nSun, T.; Lu, C.; Zhang, T.; and Ling, H. 2022. Safe self-\nrefinement for transformer-based domain adaptation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 7191–7200.\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\nAdversarial discriminative domain adaptation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 7167–7176.\nVenkateswara, H.; Eusebio, J.; Chakraborty, S.; and Pan-\nchanathan, S. 2017. Deep hashing network for unsupervised\ndomain adaptation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 5018–5027.\nVu, T.-H.; Jain, H.; Bucher, M.; Cord, M.; and P´erez, P.\n2019. Dada: Depth-aware domain adaptation in semantic\nsegmentation.\nIn Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 7364–7373.\nWan, F.; Zhao, H.; Yang, X.; and Deng, C. 2024. Unveiling\nthe Unknown: Unleashing the Power of Unknown to Known\nin Open-Set Source-Free Domain Adaptation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 24015–24024.\nWang, Q.; Meng, F.; and Breckon, T. P. 2024. Progressively\nselect and reject pseudo-labelled samples for open-set do-\nmain adaptation.\nIEEE Transactions on Artificial Intelli-\ngence.\nWang, Y.; Zhu, R.; Ji, P.; and Li, S. 2024. Open-Set Graph\nDomain Adaptation via Separate Domain Alignment.\nIn\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, 9142–9150.\nWu, B.; Xu, C.; Dai, X.; Wan, A.; Zhang, P.; Yan, Z.;\nTomizuka, M.; Gonzalez, J.; Keutzer, K.; and Vajda, P. 2020.\nVisual Transformers: Token-based Image Representation\nand Processing for Computer Vision. arXiv:2006.03677.\nYang, S.; Jui, S.; van de Weijer, J.; et al. 2022. Attracting and\ndispersing: A simple approach for source-free domain adap-\ntation. Advances in Neural Information Processing Systems,\n35: 5802–5815.\nYang, S.; Wang, Y.; Van De Weijer, J.; Herranz, L.; and Jui,\nS. 2021.\nGeneralized source-free domain adaptation.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 8978–8987.\nYu, Z.; Li, J.; Du, Z.; Zhu, L.; and Shen, H. T. 2023. A\nComprehensive Survey on Source-free Domain Adaptation.\narXiv preprint arXiv:2302.11803.\nYue, Z.; Sun, Q.; and Zhang, H. 2024. Make the u in uda\nmatter: Invariant consistency learning for unsupervised do-\nmain adaptation. Advances in Neural Information Process-\ning Systems, 36.\nZhang, Y.; Wang, Z.; and He, W. 2023.\nClass relation-\nship embedded learning for source-free unsupervised do-\nmain adaptation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 7619–\n7629.\nZhu, J.; Bai, H.; and Wang, L. 2023. Patch-Mix Transformer\nfor Unsupervised Domain Adaptation: A Game Perspective.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 3561–3571.",
    "pdf_filename": "Recall_and_Refine_A_Simple_but_Effective_Source-free_Open-set_Domain_Adaptation_Framework.pdf"
}