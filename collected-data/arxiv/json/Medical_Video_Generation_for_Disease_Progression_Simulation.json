{
    "title": "Medical Video Generation for Disease Progression Simulation",
    "abstract": "Modelingdiseaseprogressioniscrucialforimprovingthe MVG qualityandefficacyofclinicaldiagnosisandprognosis,but itisoftenhinderedbyalackoflongitudinalmedicalimage Input Fundus RegionGuide monitoring for individual patients. To address this chal- Retinal Image lenge,weproposethefirstMedicalVideoGeneration(MVG) MVG frameworkthatenablescontrolledmanipulationofdisease- relatedimageandvideofeatures,allowingprecise,realistic, andpersonalizedsimulationsofdiseaseprogression.Ourap- Chest X-ray RegionGuide Image proachbeginsbyleveraginglargelanguagemodels(LLMs) to recaption prompt for disease trajectory. Next, a con- MVG trollablemulti-rounddiffusionmodelsimulatesthedisease progressionstateforeachpatient,creatingrealisticinterme- diatediseasestatesequence.Finally,adiffusion-basedvideo Skin Image RegionGuide transitiongenerationmodelinterpolatesdiseaseprogression Prompt-controlled Disease Progression Generation between these states. We validate our framework across Figure 1. Illustrative examples of video-based disease progres- threemedicalimagingdomains: chestX-ray,fundusphotog- sionsimulation(6-8s)usingpredefinedmedicalreportsandour raphy,andskinimage. OurresultsdemonstratethatMVG proposedmethod. Thetopsequencedepictsapatient’sDiabetic significantlyoutperformsbaselinemodelsingeneratingco- Retinopathy. ThemiddlesequencedemonstratestheEdemaina herentandclinicallyplausiblediseasetrajectories.Twouser patient’slung.ThebottomsequencedemonstratestheBenignSkin studiesbyveteranphysicians,providefurthervalidationand Lesioninapatient’sskin. insightsintotheclinicalutilityofthegeneratedsequences. MVGhasthepotentialtoassisthealthcareprovidersinmod- ofcontinuousmonitoringofindividualpatientsovertime, elingdiseasetrajectories,interpolatingmissingmedicalim- aswellasthehighcostandrisksassociatedwithcollecting agedata,andenhancingmedicaleducationthroughrealistic, longitudinal imaging data [13, 38, 61]. The intricate and dynamicvisualizationsofdiseaseprogression. multifaceted dynamics of disease progression, combined with the lack of comprehensive and continuous image or video data of individual patients, result in the absence of 1.Introduction establishedmethodologiesformedicalimagingtrajectories Diseaseprogressionreferstothewayanillnessevolvesin simulation[34]. anindividualovertime. Understandingthisprogressionen- Recentadvancementsinimageandvideogenerationmod- ableshealthcareprofessionalstodevelopeffectivetreatment elspresentpromisingopportunitiesforsimulatingrealistic strategies, anticipate complications, and adjust care plans medicalvideos,potentiallyenrichingexistingdatabasesand accordingly. Diseaseprogressionmodelingcanalsobeseen addressingdatalimitations. Toincorporategenerativemod- asaformofhumandigitaltwin,layingthefoundationfor elsintodiseaseprogressionsimulations,weestablishthree futureprecisionmedicine[37,68,70]. However,modeling keycriteriathatmedicalvideogenerationmodelsmustmeet: diseaseprogressiononmedicalimagespresentssignificant (i)Themodelshouldgeneratevideospresentinglongdisease challenges. Thesechallengesariseprimarilyfromthelack progressionunderzero-shotsetting,astherearenoexisting 4202 voN 81 ]VC.sc[ 1v34911.1142:viXra",
    "body": "Medical Video Generation for Disease Progression Simulation\nXuCao1,KaizhaoLiang2,3,Kuei-DaLiao4,TianrenGao5,WenqianYe6,JintaiChen1,\nZhiguangDing7,JianguoCao8,JamesM.Rehg1,JimengSun1\n1UniversityofIllinoisUrbana-Champaign 2UniversityofTexasatAustin\n3SambaNovaSystem,Inc 4Objective,Inc 5Microsoft 6UniversityofVirginia\n7ShenzhenNanshanPeople’sHospital 8ShenzhenChildren’sHospital\nAbstract\nModelingdiseaseprogressioniscrucialforimprovingthe MVG\nqualityandefficacyofclinicaldiagnosisandprognosis,but\nitisoftenhinderedbyalackoflongitudinalmedicalimage\nInput Fundus RegionGuide\nmonitoring for individual patients. To address this chal- Retinal Image\nlenge,weproposethefirstMedicalVideoGeneration(MVG)\nMVG\nframeworkthatenablescontrolledmanipulationofdisease-\nrelatedimageandvideofeatures,allowingprecise,realistic,\nandpersonalizedsimulationsofdiseaseprogression.Ourap- Chest X-ray RegionGuide\nImage\nproachbeginsbyleveraginglargelanguagemodels(LLMs)\nto recaption prompt for disease trajectory. Next, a con- MVG\ntrollablemulti-rounddiffusionmodelsimulatesthedisease\nprogressionstateforeachpatient,creatingrealisticinterme-\ndiatediseasestatesequence.Finally,adiffusion-basedvideo Skin Image RegionGuide\ntransitiongenerationmodelinterpolatesdiseaseprogression Prompt-controlled Disease Progression Generation\nbetween these states. We validate our framework across\nFigure 1. Illustrative examples of video-based disease progres-\nthreemedicalimagingdomains: chestX-ray,fundusphotog-\nsionsimulation(6-8s)usingpredefinedmedicalreportsandour\nraphy,andskinimage. OurresultsdemonstratethatMVG\nproposedmethod. Thetopsequencedepictsapatient’sDiabetic\nsignificantlyoutperformsbaselinemodelsingeneratingco-\nRetinopathy. ThemiddlesequencedemonstratestheEdemaina\nherentandclinicallyplausiblediseasetrajectories.Twouser patient’slung.ThebottomsequencedemonstratestheBenignSkin\nstudiesbyveteranphysicians,providefurthervalidationand Lesioninapatient’sskin.\ninsightsintotheclinicalutilityofthegeneratedsequences.\nMVGhasthepotentialtoassisthealthcareprovidersinmod-\nofcontinuousmonitoringofindividualpatientsovertime,\nelingdiseasetrajectories,interpolatingmissingmedicalim-\naswellasthehighcostandrisksassociatedwithcollecting\nagedata,andenhancingmedicaleducationthroughrealistic,\nlongitudinal imaging data [13, 38, 61]. The intricate and\ndynamicvisualizationsofdiseaseprogression.\nmultifaceted dynamics of disease progression, combined\nwith the lack of comprehensive and continuous image or\nvideo data of individual patients, result in the absence of\n1.Introduction\nestablishedmethodologiesformedicalimagingtrajectories\nDiseaseprogressionreferstothewayanillnessevolvesin simulation[34].\nanindividualovertime. Understandingthisprogressionen- Recentadvancementsinimageandvideogenerationmod-\nableshealthcareprofessionalstodevelopeffectivetreatment elspresentpromisingopportunitiesforsimulatingrealistic\nstrategies, anticipate complications, and adjust care plans medicalvideos,potentiallyenrichingexistingdatabasesand\naccordingly. Diseaseprogressionmodelingcanalsobeseen addressingdatalimitations. Toincorporategenerativemod-\nasaformofhumandigitaltwin,layingthefoundationfor elsintodiseaseprogressionsimulations,weestablishthree\nfutureprecisionmedicine[37,68,70]. However,modeling keycriteriathatmedicalvideogenerationmodelsmustmeet:\ndiseaseprogressiononmedicalimagespresentssignificant (i)Themodelshouldgeneratevideospresentinglongdisease\nchallenges. Thesechallengesariseprimarilyfromthelack progressionunderzero-shotsetting,astherearenoexisting\n4202\nvoN\n81\n]VC.sc[\n1v34911.1142:viXra\ndatasetsforimageandvideo-baseddiseaseprogression;(ii) 2.RelatedWorks\nThegenerateddiseasestatesmustbesemanticallyrelevant\nDisease Progression Simulation. Longitudinal disease\nto the initial input image; (iii) The generated progression\nprogressiondataderivedfromindividualelectronichealth\nshouldbeclinicallyverifiedandconsistentwiththecorre-\nrecordsofferanexcitingavenuetoinvestigatethenuanced\nspondingtextdescriptions.\ndifferencesintheprogressionofdiseasesovertime[46,60,\nInthiswork,weproposeMVG,avideogenerationframe- 65]. However, most of the previous works are based on\nworkforsimulatingdiseaseprogressionthatintegratestext HMM[38,71]ordeepprobabilisticmodels[3]withoutus-\ninference,progressiveimagegeneration,andvideocliptran- ingdatafromimagingspace.Somerecentworkshavestarted\nsitiongeneration. Specifically,ourapproachusesGPT-4[2] to resolve image disease progression simulation by using\ntosummarizeclinicalreportsandgenerateprompts,which deep-generationmodels. [25,56]utilizedtheGenerativeAd-\nare then used to progressively control disease-related fea- versarialNetworks(GANs)basedmodelandlinearregressor\nturesextractedbyatextencoder. Thisapproachallowsus withindividualsequentialmonitoringdataforAlzheimer’s\ntoconditionallysimulatediseaseprogressioninthevisual diseaseprogressionsimulationinMRIimagingspace. All\ndomain without significantly altering the core features of thesemethodshavetousefullsequentialimagesastraining\ntheinitialimage(seeFigure1). Ourframeworkisbuilton sets and are hard to adapt to the general medical imaging\nthe invertibility of denoising diffusion probabilistic mod- domain.\nels[21,64], thevisual-languagealignmentcapabilitiesof\ncontextencoders[16],andframe-levelsynthesis.\nGenerativeModels. Recently,DenoisingDiffusionMod-\nOurtheoreticalanalysisdemonstratesthatthemulti-step els[21,26,58,64]havebecomeincreasinglypopulardueto\ndiseasestatesimulationmoduleofMVGcanbeunderstood theirabilitytocreatehigh-resolutionrealisticimagesfrom\nas a gradient descent process toward maximizing the log- textualdescriptions. Onemajoradvantageofthesemodels\nlikelihoodofthegiventextconditioning. Thelearningrate istheycanuseCLIP[54]embeddingtoguideimageediting\ninthisiterativeprocessdecaysexponentiallywitheachfor- based on contextual prompts. Among the various text-to-\nwardstep,allowingthealgorithmtoeffectivelyexplorethe imagemodels,latentdiffusionmodel(LDM)[16,58]and\nsolutionspacewhilebalancingconvergencespeedandsta- itsfollow-upimage-to-imageeditingworks[9,49,51]has\nbility. This guarantees that our framework moves toward receivedconsiderableattentionbecauseofitsimpressiveper-\nthetargetdiseasemanifold,ensuringthatthemodifications formanceingeneratinghigh-qualityimagesanditsabilityto\nmadeareclinicallyplausibleandremainboundedformedi- editscenariosacrossmultiplemodalities.\ncalconcepts. Finally,aftergeneratingasequenceofdisease Whileimagegenerationhasseensubstantialprogressin\nstateimages,weutilizeavideotransitiongenerationmodel, generaldomains,itsapplicationinthemedicalfieldremains\nguidedbyconditionalmasks,tointerpolatebetweensucces- lessexplored[76]. EarlierworkusingVariationalAutoen-\nsivediseasestates,therebycreatingarealisticsimulationof coders(VAEs)[29]andGANs[18]focusedongenerating\ndiseaseprogression. medicalimageslikeX-raysandMRIstoaddresstheissue\noflimitedtrainingdata[14,43,48,78]. Theintroduction\nThecontributionsaresummarizedasfollows:\nof LDMs significantly improved the quality of these im-\nages[27,47,50],evenextendingto3Dsynthesis[15,28].\n• Weproposethefirstmedicalvideosimulationframework\nRecently, efforts have been made to unify medical report\nMVG,whichallowsforapreciseunderstandingofdisease-\ngenerationwithimagesynthesis[7,35],anddesignimage\nrelatedimagefeaturesandleadstoaccurateandindividu-\nediting pipeline for counterfactual medical image genera-\nalizedlongitudinaldiseaseprogressionsimulation.\ntion[19,33].\n• Weprovidetheoreticalevidencethatouriterativerefine-\nment process is equivalent to gradient descent with an\nexponentiallydecayinglearningrate,whichhelpstoestab- Text-to-VideoGeneration. Text-to-imagemodelshaveat-\nlishadeeperunderstandingofapplyingdiffusion-based tractedsignificantattentionfrombothacademiaandindustry,\ngenerativemodelsinhealthcareresearch. asevidencedbyadvancementslikeDALL·E[4],Midjour-\n• WedemonstratethesuperiorperformanceofMVGover ney [44], and Stable Diffusion 3 [16]. These innovations\nbaselinesindiseaseprogressionpredictionwiththreemed- havesignificantlyimpactedthetext-to-videodomain[66],\nical domains using CLIP-I score, disease classification leading to the development of models such as Sora [45],\nconfidencescore,andphysicianuserpreferencestudy. Pika[52],andStableDiffusionVideo[5]. Thecoreofthese\n• Inthefollow-upuserstudy,35physiciansagreethat76.2% text-to-videomodelsofteninvolvesfine-tuningorintegrat-\nof disease state sequences simulated by MVG closely ing additional modules or priors into pre-trained text-to-\nmatchedphysicians’expectations,indicatingourgenera- imagediffusionmodelsusingvideodata,asseeninMake-\ntionresultsarehighrelatedtotheclinicalcontext. A-Video [63], PYoCo [17], and LaVie [72], SEINE [11],\nAnyV2V[31]. However,applyingvideogenerationmodels\nin the healthcare domain presents challenges, particularly\nbecausetime-seriesmedicalimagingdatafordiseasepro- xˆ i =g ϕ(x i,x i+1) (2)\ngression is difficult to collect. While some studies have\nThe output videos {xˆ,xˆ,xˆ,...,xˆ } finally con-\nexploredvideogenerationinmedicalimaging[36,67],they 0 1 2 N\ncatenate into the disease progression video X ∈\nhavenotfocusedonsimulatingdiseaseprogression.\nRKN×H×W×C.\n3.ProblemStatement\n4.MedicalVideoGeneration(MVG)\nTraditionalimagetovideogenerationmodelsneedtotrain\nAsshowninFigure3,MVGcontainstwomaincomponents:\nwithalargeamountoftext-to-videoorimage-to-videodata.\n(i)Progressivediseaseimageediting(PIE)withmedical\nHowever,itisalmostimpossibletoobtainlarge-scalelongi-\ndomain-specificdiffusionmodeland(ii)TransitionGener-\ntudemedicalimagingdata(canbealsoconsideredasatype\nationProcessbetweengenerateddiseasestateswithvideo\nofmedicalvideodata)asmostpatientsmaynotgotothe\nlatentdiffusionmodel.\nsamehospitalforfollow-uptreatmentandthehospitalsalso\nThefirstcomponentPIEisalong-sequencemedicalim-\nlackmedicalimagingandclinicalreportsintheearlystages\nageeditingframeworkproposedtorefineandenhanceim-\nofdiseases.\nagesiterativelyanddiscretely,allowingclinicalreport-based\nInourpaper,wereconsiderthisprobleminanotherway.\npromptsforpreciseadjustmentstosimulatediseasedevelop-\nGiven an input medical image x , and clinical report and\n0\nmentwhilekeepingrealism.Unliketraditionalimageediting\nmedicalhistorylabely . Experiencedmedicaldoctorscan\n0\ntechniques,PIEinvolvesamulti-stageprocesswhereeach\npredictthediseaseprogressionofthepatientbasedontheir\nstep builds upon the previous one, intending to achieve a\nclinicalpriorknowledge,denotedasy ,whereN+1isthe\nN\nfinalresultthatismorerefinedthanifallchangesweremade\ntotalnumberofstatesofthepredicteddisease.Thepredicted\natonce. Transitiongenerationisusedinthelongvideogen-\ndisease progression is a video sequence X, which can be\nerationmodeltoconnectsdifferentnarrativemoments. Once\nseparatedbyasetofshortvideoclips{xˆ,xˆ,xˆ,...,x ˆ },\n0 1 2 N−1\nwhere xˆ ∈ RK×H×W×C isa videoclipbetweendisease theframe-levelsequenceisgeneratedbyPIE,wewillpro-\ni\nvideeachpairofadjacentframesandusetransitionprompts\nimagestatex andx . K,H,W,C denotethenumberof\ni i+1\nanddiseaseregionmasktocontrolthestyleandcontent,cre-\nframes,height,width,andchannelsofthevideoclip. K isa\natingintermediateframesthatfurtherillustratethetransition\nverysmallnumbertocontrolthediseaseprogressionchange\norprogressionwithinthemedicalvideosequence.\ninalimitedmedicalimagingspace. Inxˆ,thestartingframe\ni\nx i ∈ RH×W×C is the initial disease state and end frame 4.1.ProgressiveImageEditing(PIE)\nx ∈RH×W×C istheenddiseasestate.\ni+1\nProcedure. TheinputstoPIEareadiscretemedicalimage\nx0 depicting any start or middle stage of a disease and a\n0\ncorrespondingterminalstageclinicalreporty inferredby\nN\nmedicaldoctorandthenre-captionedbyGPT-4[2],provid-\nx0 x1 x2 x4 x10 xN ing the potential hint of the patient’s disease progression.\nTheLatenty willbethetextconditioningofthediffusion\nFigure2. Visualizationforcardiomegarlydiseasestateabsolute\nmodel[58]. y isgeneratedfromapretrainedtextencoder\ndifference heatmap. The highlighted red portion illustrates the\nfromCLIP[54](clip-vit-large-patch14),wherethetextinput\nprogressionofthepathologyateachstep.\nisy . TheoutputgeneratedbyPIEisasequenceofimages\nN\nWeseparatethediseaseprogressionvideogenerationinto presentingthediseaseprogression,{x0,x0,··· ,x0 }. The\n0 1 N\natwostagestrategy.Inthefirststage,thekeyideaistogener- iterativePIEprocedureisdefinedasfollows:\natediscretediseaseprogressivestates{x ,x ,x ,...,x }:\n0 1 2 N\nProposition 1. * Let x0 ∼ χ, where χ is distribution\nn\nofphoto-realisticmedicalimages, y bethetextcondition-\nx =f (x ,y ) (1) ing,runningPIE (·,·)recursivelyisdenotedasfollowing,\n1:T θ 0 T n\nwheren={N,N −1,···1},\nIn the training phase of the first stage, f is a denois-\nθ\ning diffusion model learned from independent identically x0 n =PIE n(x0 n−1,y) (3)\ndistributed(x,y)fromdifferentpatients.\nx0 =PIE ◦PIE ◦···◦PIE (x0,y) (4)\nInthesecondstage,weadoptvideolatentdiffusionmod- N N N−1 1 0\n(cid:124) (cid:123)(cid:122) (cid:125)\nelsfinetunedwithvideodatainthegeneraldomain. Indoing Ntimes\nso,weconvertthediseaseprogressionvideogenerationtask\n*TheproofofProposition1andProposition2areshowninthesupple-\nintoaframe-leveltransitiongenerationproblem: mentarymaterial.\n𝑥0 Randomly perturbed 𝑥𝑘 𝑥𝑘−1 𝑥0\n𝑛−1 𝑛−1 𝑛−1 𝑛\nUsers\nR\n𝑃\n𝜃\n𝑃\n𝜃\nmenife\ntne\n𝑘−1\nLLM Re-captioning After disease progressed, heart size mildly Text\nenlarged exaggerated by low lung volumes. Encoder\nInitial Report ROI Extraction\n𝒙 𝟎𝟎\nPIE 0\n𝒙 𝟏𝟎\nPIE 𝑛 𝒙 𝑵𝟎\nSource Mid\nState State 1 𝑁−1 Target\nState\n… …\nSource State Mid State 1 Target State\nTransition Generation (N-1) * Transition Generation\nFigure3.OverviewoftheMVGinferencepipeline.TheabovebluepartdenotesthesinglestepofPIE.ForanygivenstepninPIE,wefirst\nutilizeinversionofdiffusionmodeltoprocureaninvertednoisemap.Subsequently,wedenoiseitusingGPT-4re-captionedclinicalreports\nfromthefuturestateandusetheROImasktorefinetheeditingafterthelaststepofdenoising.TheoutputofasinglestepofPIEistheinput\nforthenextstepn+1,thusensuringagradualandcontrollablediseaseprogressionsimulation. AftersimulatingN steps,theimageis\nconvergedtothefinalstate. Thebelowgreenpartshowsthetransitiongenerationprocessbetweendiseasestates. WeuseROImaskto\ncontrolthemaskrecoveryofSEINEandfinallyoutputthelongsequenceofvideo-baseddiseaseprogression.\nThen,theresultingfinaloutputx0 maximizestheposterior generated by pretrained Med-SAM [42] and then slightly\nN\nprobabilityp(x0 |x0,y). editbyhumantocontrolandrefinethefinaloutput:\nN 0\nToruntheinferencepipelineofPIEtogenerateadiscrete\ndisease progression image sequence, we use the original x′ ←(β ·(x′−x0)+x0)·(1−M )+\n1 0 0 ROI\ninputimagex0 0asthestartpoint. Thehyperparametersare (β ·(x′−x0)+x0)·M (6)\nthenumberofprogressionstageN,thenumberofdiffusion 2 0 0 ROI\nstepsT,textconditionalvectory,noisestrengthγ,diffusion where β , β are hyperparameter to control the interpola-\n1 2\nparameterized denoiser ϵ θ, and a region of interest (ROI) tionbetweengeneratedresultandtheinputimage. Thelast\nmaskM ROI,whereeachpixelinM Ri, Oj\nI\n∈[0,1]. outputimagex′ isxT n−1,whichisalsotheinputx0\nn\nofthe\nSincePIEisarecursiveprocess,atprogressionstagen, nextstep(n+1step)diseasestategeneration. Equation6\ntheinputimageisx0 n−1. Fromdiffusionstepkto1, guaranteestheeditingisregionalbasedandavoidstheim-\nagedistortioncausedbymultipletimesimageediting. Itis\n√ x′−√ 1−α ϵ(t)(x′,y) worthnotingthatEquation6cangeneralizetoarbitrarydif-\nx′ ← α ( √ t θ )+ fusionbackbonesincludingStableDiffusion-1.4[58],Stable\nt−1 α t (5) Diffusion3[16].\n(cid:112) 1−α ·ϵ(t)(x′,y) Witheachroundofeditingasshowninthemiddlepart\nt−1 θ\nofFigure3,theimagegetsclosertotheobjectivebymov-\nwherex′instepkisx0 ,kisγ·T,ϵ(t)(x′,y)isthenoise inginthedirectionof−∇logp(x|y). Thestepsizewould\nn−1 θ\npredictionbyUNetorTransformer,whereθistheparameter graduallydecreasewithaconstantfactor. Theiterativecon-\ninthedenoiser. Afterthelaststep,weusetheM initially vergenceanalysisisasfollows:\nROI\nProposition2. Assuming∥x0∥≤C and∥ϵ (x,y)∥≤C , Datasets ImagingType Instances inputsize\n0 1 θ 2\n(x,y)∈(χ,Γ),foranyδ >0,if CheXpertPlus[10] ChestX-ray 223,462 512×512\nMIMIC-CXR[24] ChestX-ray 227,835 512×512\nDiabeticRetinopathyDetection[1] Retinopathy 35,126 1024×1024\n2\nn> ·(log(δ)−C) (7) ISIC2024[32] Skin 40,1059 128×128\nlog(α ) ISIC2018[12] Skin 10,015 128×128\n0\nthen, Table1.DatasetsusedtotrainPIEofMVG.\n∥x0 −x0∥<δ (8)\nn+1 n\n5.1.ExperimentalSetups\n√ √\nwhere, λ = α0−α0α √1− α1−α0α1, χ is the image dis-\nα1 ImplementationDetails. ForexperimentsinTable2,PIE\ntribution, Γ is the text condition distribution , and C =\nandthebaselinesareusingpubliclyavailableStableDiffu-\nlog((√1\nα0\n−1)·C 1+λ·C 2)\nsioncheckpoints(CompVis/stable-diffusion-v1-4)andthen\nwe further finetune on the training sets of each of the tar-\nProposition2showsasngrowsbigger,thechangesbe-\ngetdatasets. Thisisbecausethepipelineoftheothertwo\ntweenstepswouldgrowsmaller. Eventually,thedifference\nbaselinesonlysupportthemodelweightfromoriginalSta-\nbetweenstepswillgetarbitrarilysmall. Theconvergenceof\nble Diffusion 1.4 version. For user study in Table 3, we\nPIEisguaranteed,andmodificationstoanymedicalimaging\nadoptStableDiffusion3mediumasthemodelweightand\ninputsareboundedbyaconstant. TheproofofProposition\nfinetuneitwiththreemedicaldomain. Theweightfortran-\n2isshowninthesupplementarymaterial.\nsitiongenerationmodelisfromSEINE[11]. Ourcodeand\n4.2.TransitionGenerationProcess checkpointswillbepubliclyavailableuponpublication. All\nexperimentsareconductedon4NVIDIAH100GPUs.\nTheconceptofscenetransitiongenerationisfirstproposed\nby SEINE [11], which is a short-to-long video diffusion\nmodel. InMVG,weuseM tocontrolSEINEtoconnect DatasetsforDiseaseProgression. Weevaluatethepre-\nROI\nthediseaseprogressionbetweeneachstepgeneratedbyPIE, traineddomain-specificstablediffusionmodelonthreedif-\nferenttypesofdiseasedatasetsfromdifferenttasks: CheX-\nxˆ ′ =Concat(x0 , ϵ,··· ,ϵ,x0) (9) pertPlus[10]andMIMIC-CXR[24]forchestX-rayclassi-\nn n−1 n\n(cid:124) (cid:123)(cid:122) (cid:125) ficationandreportgeneration [10,23,24],ISIC2024and\nrandomnoise\nISIC2018[12,32,69]forskincancerprediction,andKag-\ngleDiabeticRetinopathyDetectionChallenge [1]. Eachof\nx0 +x0 these datasets presents unique challenges and all of them\nxˆ = n−1 n ·(1−M )+g(xˆ ′)·M (10)\nn 2 ROI n ROI havinglarge-scaleofdata,makingthemsuitablefortesting\nthe robustness and versatility of MVG. We also collected\n,where xˆ n is a video clip with the first and last frames over50dataamongthetestsetfromthesedatasetsasinitial\naretheinputx0 andoutputx0 fromprogressionstagen inputdatafordiseaseprogressionvideogeneration. These\nn−1 n\nin PIE. Between x0 and x0, all frames are masks with data were used for disease progression simulation. Three\nn−1 n\nrandom noise. By predicting and modeling the noise, the groupsofprogressionvisualizationresultscanbefoundin\ntransition generation process g(·) aims to extend realistic, Figure5.\nvisuallycoherenttransitionframesthatseamlesslyintegrate\nthevisibleframeswiththeunmaskedones.\nEvaluationMetrics. Theevaluationofgenerateddisease\nprogressionimagesfocusesontwokeyaspects: alignment\n5.ExperimentsandResults\nwiththeintendeddiseasefeaturesandpreservationofpatient\nInthissection,wepresentexperimentsonvariousdisease identity. To assess these aspects, we employ two primary\nprogression tasks. Experiments results demonstrate that metrics:theCLIP-Iscoreandclassificationconfidencescore,\nMVGcansimulatethedisease-changingtrajectorythatis allowing us to compare the baselines and PIE (stage 1 of\ninfluencedbydifferentmedicalconditions. Notably,MVG MVG)underconsistentconditions.\nalso preserves unrelated visual features from the original TheCLIP-Iscore(theoreticallyrangingfrom[0,1])rep-\nmedical imaging report, even as it progressively edits the resentstheaveragepairwisecosinesimilaritybetweenthe\ndiseaserepresentation. Figure5showcasesasetofdisease CLIPembeddingsofthegeneratedmedicalimagesequence\nprogressionsimulationexamplesacrossthreedistincttypes andtheinitialrealmedicalimages[54,59]. AhighCLIP-I\nofmedicalimaging. DetailsforStableDiffusionfine-tuning, scoreindicatesstrongpatientidentityconsistencybutalso\npretrainingmodelforconfidencemetricssettingsareavail- meansminimalchangesbetweentheeditedsequenceand\nableintheSupplementary. theoriginalinput. Therefore,anidealdiseaseprogression\nChestX-ray FundusRetinalImage SkinImage\nMethod\nConf(↑) CLIP-I(↑) Conf(↑) CLIP-I(↑) Conf(↑) CLIP-I(↑)\nExtrapolationMethods[20] 0.054 0.972 0.074 0.991 0.226 0.951\nSableVideoDiffusion(SVD)[57] 0.389 0.923 0.121 0.892 0.201 0.886\nPIE(Stage1ofMVG) 0.712 0.978 0.807 0.992 0.453 0.958\nTable2.Comparisonswithcommercialimageeditingtoolswithotherfinetunedmulti-stepmedicalimageeditingsimulations.Thebackbone\nofPIEandallbaselineapproaches[5,20]areusedthesamefinetunedStableDiffusionv1.4weightoneachdataset.\nX-ray Skin Retinal\nMethodA MethodB\nHumanEval(↑) HumanEval(↑) HumanEval(↑)\nPixVerse[53] 0.42 0.50 0.54\nPika[52] CogVideoX[75] 0.46 0.42 0.67\nMVG(Our) 0.20 0.33 0.33\nPika[52] 0.58 0.50 0.46\nPixVerse[53] CogVideoX[75] 0.58 0.58 0.58 PIE SVD Extrapolation\nMVG(Our) 0.23 0.37 0.37\nPika[52] 0.54 0.58 0.33 Figure 4. Editing path of PIE, SVD, and Extrapolation in the\nCogVideoX[75] PixVerse[53] 0.42 0.42 0.42\nMVG(Our) 0.17 0.33 0.20 manifold.\nPika[52] 0.80 0.67 0.63\nMVG(Our) PixVerse[53] 0.77 0.63 0.67\nWalk[57]forshortvideogeneration. SVDisthebasicof\nCogVideoX[75] 0.83 0.67 0.80\nthelatent-basedvideogenerationmethodslikeStableDif-\nTable 3. User preference A/B test from 30 verified clinicians, fusionVideo[6, 73], but itdo notneed anytraining from\nradiologistsofthegenerateddiseaseprogressionvideosfromMVG videodatasets. AnotheroneistheStyle-BasedManifoldEx-\nandthreeSOTAimage-to-videogenerationmodels.\ntrapolation(Extrapolation)[20]forgeneratingprogressive\nmedicalimagingwithGAN,asitdon’tneeddiagnosisla-\nsequenceshouldbalancethedegreeofeditingwithidentity beleddata[20,55],whichissimilartoourdefinitionsetting\npreservation. butitneedplentyofprogressioninferenceprior. InFigure4,\nTheclassificationconfidencescoreisderivedfromasu- weshowcasehowthesemodeledittheimagewithmulti-step\nperviseddeepnetworktrainedforbinaryclassificationbe- bypromptguidanceinthemanifold. Duringthecomparison,\ntweennegative(healthy)andpositive(disease)samples. It alltrainablebaselinemethodsareusingthesameStableDif-\nisdefinedasConf = Sigmoid(f (x))andmeasureshow fusionfinetunedweightsinspecificdatasetandalsoapplied\nθ\nwellthegeneratedimagesalignwiththetargetdiseasestate. M ROIforregionguidance.\nFor our experiments, we utilize the DeepAUC maximiza-\ntionmethod[77]—recognizedforitsSOTAperformanceon\nCheXpertandISIC2018task3—usingDenseNet121[22]as\nthebackbonetocomputetheclassificationconfidencescore.\nHowever,thesemetricsaloneareinsufficientforevaluat-\ningtheclinicalrelevanceofthegeneratedvideosequences.\nTherefore,inspiredbyImageReward[74],wealsoconducted\naclinicianpreferenceevaluationtocompareMVGwithsev-\neralSOTAimage-to-videogenerationmodels. Weusedtwo\nsetsofdatafromthreemedicaldomainsandengaged30clin-\niciansandradiologists(verifiedbyco-authorsfromclinical\ninstitutions)torankthesevideosthroughA/Btesting.\nInputImage Step1 Step4 Step10\nBaselines. Toourknowledge,therearenoexistinggenera- Figure5.DiseaseProgressionSimulationofMVG.Thetopprogres-\nsionisforCardiomegarly.ThemiddleprogressionisforDiabetic\ntionmodelsspecificallydesignedforsimulatingdiscretedis-\nRetinopathy.ThebottomprogressionisforMelanocyticNevus.\neaseprogressionsequencesorvideosunderthenotrainable\nsequentialdatasetting. Tounderscoretheuniquestrengths\n5.2.DiseaseStateSimulation\nofMVG,wecompareitagainstwithrelatedbaselinemulti-\nstage diffusion generation strategy. One of them is Sta- InordertodemonstratethesuperiorperformanceofMVGin\nble Video Diffusion (SVD), also called Stable Diffusion diseaseprogressionsimulationoverothersingle-stepediting\nyar-XtsehC\nsudnuF\nnikS\nmethods,weperformexperimentsonthreedatasetsprevi- MVGisavailableintheSupplementarymaterial.\nouslymentioned. Foreachdiseaseinthesedatasets,weused\n5.4.AblationStudy\n50healthysamplesinthetestsetassimulationstartpoint\nand run MVG, SVD, Extrapolation with 5 random seeds. DuringtheMVGsimulation,theregionguidemasksplay\nWeobtainatleast50diseaseimagingtrajectoriesforeach abigroleaspriorinformation. Unlikeotherrandomlyin-\npatient. Table2showcasesthatMVGconsistentlysurpasses paintingtasks[40],ROImaskformedicalimagingcanbe\nbothSVDandExtrapolationintermsofdiseaseconfidence extractedfromclinicalreports[8,39]usingdomain-specific\nscoreswhilemaintaininghighCLIP-Iscores. ForChexpert SegmentAnythingmodels[30,41]. Ithelpskeepunrelated\ndataset,the0.712finalconfidencescoreistheaveragescore regions consistent through the progressive changes using\namong5classes. ForDiabeticRetinopathyandISIC2018 MVGorbaselinemodels. Inordertogeneratesequential\ndatasets, we compare MVG with SVD, Extrapolation for diseaseimagingdata,MVGusesnoisestrengthγ tocontrol\nediting image to the most common seen class since these theinfluencefromthepatient’sclinicallyreportedandex-\ndatasets are highly imbalanced. We observe that MVG is pected treatment regimen at time n. N is used to control\nabletoproducemorefaithfulandrealisticprogressiveedit- thedurationofthediseaseoccurrenceortreatmentregimen.\ningcomparedtotheothertwobaselines. Interestingly,while MVGallowstheusertomakesuchcontrolsovertheiterative\ntheCLIP-IscoreofExtrapolationiscomparabletothatof process,andrunningPIE multipletimescanimprovethe\nn\nMVG,itfailstoeffectivelyeditthekeydiseasefeaturesof accuracyofdiseaseimagingtrackingandreducethelikeli-\ntheinputimagesasitsimageneverchangeduringtheinfer- hoodofmissedormisinterpretedchanges. Weshowedabla-\nence and its classification confidence scores are also very tionstudyforM inTable4,γ inTable5,N inTable6,\nROI\nlow. β andβ inTable7. Theexperimentalresultsdemonstrate\n1 2\nFigure 6 showcases a group of progression simulation that M is a good controller to balance the alignment\nROI\nqualitativeresultsforEdemainchestX-rayswithCheXpert withtheintendeddiseasefeaturesandpreservationofpatient\nclinicalreportprompt. Itisevidentfromourobservations identity. Fromtheseexperiments,wealsofinalizethebest\nthatwhileSVDcansignificantlyaltertheinputimageinthe hyperparameter(N = 10,γ = 0.6,β = 0.01,β = 0.75)\n1 2\ninitial step, it fails to identify the proper direction of pro- forthemainexperiment.\ngressioninthemanifoldafterafewstepsandwouldeasily\ncreateuncontrollablenoise. Conversely,Extrapolationonly 5.5. Compare with Real Longitude Medical Imag-\nbrightenstheChestX-raywithoutmakingsubstantialmod- ingSequence.\nifications. MVG,ontheotherhand,notonlyconvincingly\nDue to the spread of COVID, part of the latest released\nsimulatesthediseasetrajectorybutalsomanagestopreserve\ndatasetcontainslimitedlongitudinaldata. Inordertovali-\nunrelated visual features from the original medical imag-\ndatethediseasesequencemodelingthatMVGcanmatchreal\ning. Furthervisualcomparisonsamongdifferentdatasetsare\ndiseasetrajectories,weconductanablationstudyongener-\npresentedinSupplementary.\natingEdemadiseaseprogressionfrom10patientsinBrixIA\nCOVID-19 Dataset [62] who’s radiology report showed\n5.3.DiseaseProgressionVideoSimulation\nEdema. The input image is the day 1 image, and we use\nTable 3 shows the comparison results between MVG and MVGtogeneratefuturediseaseprogressionbasedonreal\nthreeimage-to-videogenerationbaselines. Wedidnotcom- clinicalreportsforedema. Experimentalresultsshowthat\npareourmethodwithtext-to-videogenerationmodelslike after the disease state seqeuence simulation of MVG, the\nStable Diffusion Video [5], as these models do not sup- meanabsoluteerrorbetweenMVG’ssimulatedimageand\nportvideogenerationfromaninitialmedicalimage. Com- realdiseaseprogressionimagefromthesamepatientisap-\nparedtoPixVerse[53],CogVideoX[75],andPika[52],our proximately 0.0658. Figure 7 shows an example of the\nmethod demonstrates significantly higher clinician prefer- comparison.\nence, achievingaveragewinratesof79%, 70%, and66%\n5.6.UserStudy\nforCardiomegalyinchestX-ray,diabeticretinopathy,and\nbenignskinlesiondiseaseprogressionsimulations,respec- Tofurtherassessthequalityofourgenerateddiseasestate\ntively. In contrast, for the A/B tests comparing the other sequences,weconductedanothercomprehensiveuserstudy\nvideo generation methods, clinicians were generally un- from35physiciansandradiologistswith14.4yearsofexpe-\nabletodifferentiatebetweenthem,withwinratesaveraging rienceonaveragetoansweraquestionnaireonchestX-rays.\naround50%forbothAmethodandBmethod,indicatingno Thequestionnaireincludesdiseaseclassificationsonthegen-\nclearpreference.Theresultsoftheclinicianpreferencestudy eratedandrealX-rayimagesandevaluationsoftherealism\nindicate that MVG is capable of generating high-fidelity of generated disease progression video of Cardiomegaly,\ndiseaseprogressionsequencesthatalignwellwithclinical Edema,andPleuralEffusion. Moredetailsofthequestion-\ncontext. Thediseaseprogressionvideodatageneratedby naire and the calculation of the statistics are presented in\nChestX-ray FundusRetinalImage SkinLesionImage\nMethod\nConf(↑) CLIP-I(↑) Conf(↑) CLIP-I(↑) Conf(↑) CLIP-I(↑)\nMVGw/omask 0.729 0.933 0.163 0.968 0.666 0.852\nMVGwithmask 0.712 0.978 0.807 0.992 0.453 0.958\nTable4.Ablationstudyformask,w/omaskguidancecomparisons.\nStrength Conf(↑) CLIP-I(↑) KID(↓)\n0.1 0.120 0.969 0.0638\n0.2 0.273 0.969 0.0885\n0.4 0.746 0.965 0.1142\n0.6 0.995 0.956 0.1549\n0.8 0.999 0.951 0.1629\nTable5.AblationstudyonStrengthγselectionforN =10.\nStep(N) Conf(↑) CLIP-I(↑) KID(↓)\nStep1 Step2 Step4 Step10\n1 0.491 0.965 0.094\nFigure 6. Using MVG, SVD, Extrapolation to simulate Edema\n5 0.881 0.963 0.121\nprogressionwithclinicalreportsasinputprompt.\n10 0.978 0.962 0.142\n50 0.975 0.962 0.130\n100 0.959 0.962 0.115\nTable 6. Ablation study on simulation steps N selection with\nγ =0.5. RealDay7 MVG\nInputimage Data Simulated ConfidenceScore\nFigure7.EvaluatingtheconfidencescoresofPIE(stage1ofMVG)\nβ β Conf(↑) CLIP-I(↑) KID(↓)\n1 2\nprogressiontrajectorieshighlightsthealignmentwithrealisticpro-\n0.01 1.0 0.954 0.946 0.133 gression.\n0.01 0.75 0.977 0.948 0.140\n0.01 0.5 0.554 0.965 0.090\n6.ConclusionandOutlook\n0.1 1.0 0.960 0.965 0.126\n0.1 0.75 0.976 0.962 0.140\nInconclusion,ourproposedframework,MedicalVideoGen-\n0.1 0.5 0.554 0.962 0.089\neration (MVG) for disease progression simulation, holds\n0.2 1.0 0.963 0.947 0.134 great potential as a tool for medical research and clinical\n0.2 0.75 0.977 0.964 0.137 practiceinsimulatingdiseaseprogressiontoaugmentlack-\n0.2 0.5 0.556 0.962 0.089 inglongitudedata. Theoreticalanalysisalsoshowsthatthe\niterativerefiningprocessinthestage1ofMVGisequivalent\nTable7.Ablationstudyonβ 1andβ 2selection. togradientdescentwithanexponentiallydecayedlearning\nrate, and practical experiments on three medical imaging\ndatasetsdemonstratethatMVGsurpassesbaselinemethods.\nSupplementary. The participating physicians have agreed Theclinicianhumanpreferencestudyfrom30medicaldoc-\nwith a confidence of 76.2% that MVG simulated disease torsalsoshowsthatthediseaseprogressionvideosequences\nstate progressions on the targeted diseases fit their expec- generated by MVG are both real and consistent with the\ntations. One plausible explanation is due to the nature of correspondingclinicaltextdescriptions. Despitecurrentlim-\nMVG,theresultofrunningprogressiveimageeditingmakes itationsduetothelackoflargeamountsoflongitudemedical\npathologicalfeaturesmoreevident. Theaggregatedresults imagingdata,ourframeworkhasvastpotentialinrestoring\nfromtheuserstudydemonstrateourframework’sabilityto missingdatafrompreviouselectronichealthrecords(EHRs),\nsimulatediseaseprogressiontomeetreal-worldstandards. improvingclinicaleducation. Movingforward,weaimto\nGVM\nDVS\nnoitalopartxE\nincorporatemoretypesofmedicalimagingdatawithricher modelforgenerativetransitionandprediction.InTheTwelfth\nclinicaldescriptionsintomedicalvideogeneration,enabling InternationalConferenceonLearningRepresentations,2023.\nourframeworktomoreprecisecontroloverdiseasesimula- 2,5\ntionthroughtextconditioning. [12] Noel Codella, Veronica Rotemberg, Philipp Tschandl,\nMEmreCelebi,StephenDusza,DavidGutman,BrianHelba,\nReferences AadiKalloo,KonstantinosLiopyris,MichaelMarchetti,etal.\nSkinlesionanalysistowardmelanomadetection2018:Achal-\n[1] Diabetic Retinopathy Detection, howpublished= https: lengehostedbytheinternationalskinimagingcollaboration\n//www.kaggle.com/c/diabetic-retinopathy- (isic). arXivpreprintarXiv:1902.03368,2019. 5\ndetection,2015. 5 [13] SarahFCookandRobertRBies. Diseaseprogressionmodel-\n[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad, ing:keyconceptsandrecentdevelopments. Currentpharma-\nIlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,Janko cologyreports,2:221–230,2016. 1\nAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4 [14] PedroCosta,AdrianGaldran,MariaInesMeyer,Meindert\ntechnicalreport.arXivpreprintarXiv:2303.08774,2023.2,3 Niemeijer, MichaelAbràmoff, AnaMariaMendonça, and\n[3] AhmedMAlaaandMihaelavanderSchaar. Attentivestate- AurélioCampilho. End-to-endadversarialretinalimagesyn-\nspacemodelingofdiseaseprogression. Advancesinneural thesis.IEEEtransactionsonmedicalimaging,37(3):781–791,\ninformationprocessingsystems,32,2019. 2 2017. 2\n[4] JamesBetker,GabrielGoh,LiJing,TimBrooks,Jianfeng [15] SalmanUlHassanDar,ArmanGhanaat,JannikKahmann,\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Isabelle Ayx, Theano Papavassiliu, Stefan O Schoenberg,\nLee, Yufei Guo, et al. Improving image generation with andSandyEngelhardt. Investigatingdatamemorizationin\nbetter captions. Computer Science. https://cdn. openai. 3dlatentdiffusionmodelsformedicalimagesynthesis. In\ncom/papers/dall-e-3.pdf,2(3):8,2023. 2 InternationalConferenceonMedicalImageComputingand\n[5] AndreasBlattmann, TimDockhorn, SumithKulal, Daniel Computer-AssistedIntervention,pages56–65.Springer,2023.\nMendelevitch, MaciejKilian, DominikLorenz, YamLevi, 2\nZion English, Vikram Voleti, Adam Letts, et al. Stable [16] PatrickEsser,SumithKulal,AndreasBlattmann,RahimEn-\nvideo diffusion: Scaling latent video diffusion models to tezari,JonasMüller,HarrySaini,YamLevi,DominikLorenz,\nlargedatasets. arXivpreprintarXiv:2311.15127,2023. 2,6, AxelSauer,FredericBoesel,etal.Scalingrectifiedflowtrans-\n7 formersforhigh-resolutionimagesynthesis. InForty-first\n[6] AndreasBlattmann,RobinRombach,HuanLing,TimDock- InternationalConferenceonMachineLearning,2024. 2,4\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. [17] SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,Andrew\nAlignyourlatents:High-resolutionvideosynthesiswithla- Tao,BryanCatanzaro,DavidJacobs,Jia-BinHuang,Ming-\ntentdiffusionmodels. InProceedingsoftheIEEE/CVFCon- YuLiu,andYogeshBalaji. Preserveyourowncorrelation:\nferenceonComputerVisionandPatternRecognition,pages Anoisepriorforvideodiffusionmodels. InProceedingsof\n22563–22575,2023. 6 theIEEE/CVFInternationalConferenceonComputerVision,\n[7] Christian Bluethgen, Pierre Chambon, Jean-Benoit Del- pages22930–22941,2023. 2\nbrouck, Rogier van der Sluijs, Małgorzata Połacin, [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nJuanManuelZambranoChaves,TanishqMathewAbraham, Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and\nShivanshuPurohit,CurtisPLanglotz,andAkshaySChaud- YoshuaBengio. Generativeadversarialnetworks. Communi-\nhari. Avision–languagefoundationmodelforthegeneration cationsoftheACM,63(11):139–144,2020. 2\nofrealisticchestx-rayimages. NatureBiomedicalEngineer- [19] YuGu,JianweiYang,NaotoUsuyama,ChunyuanLi,Sheng\ning,pages1–13,2024. 2 Zhang,MatthewPLungren,JianfengGao,andHoifungPoon.\n[8] WilliamBoag,Tzu-MingHarryHsu,MatthewMcDermott, Biomedjourney: Counterfactual biomedical image genera-\nGabrielaBerner,EmilyAlesentzer,andPeterSzolovits.Base- tionbyinstruction-learningfrommultimodalpatientjourneys.\nlinesforchestx-rayreportgeneration. InMachinelearning arXivpreprintarXiv:2310.10765,2023. 2\nforhealthworkshop,pages126–140.PMLR,2020. 7 [20] Tianyu Han, Jakob Nikolas Kather, Federico Pedersoli,\n[9] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In- MarkusZimmermann,SebastianKeil,MaximilianSchulze-\nstructpix2pix:Learningtofollowimageeditinginstructions. Hagen,MarcTerwoelbeck,PeterIsfort,ChristophHaarburger,\narXivpreprintarXiv:2211.09800,2022. 2 FabianKiessling,etal. Imagepredictionofdiseaseprogres-\n[10] PierreChambon,Jean-BenoitDelbrouck,ThomasSounack, sionforosteoarthritisbystyle-basedmanifoldextrapolation.\nShih-ChengHuang,ZhihongChen,MayaVarma,StevenQH NatureMachineIntelligence,pages1–11,2022. 6\nTruong,ChuTheChuong,andCurtisPLanglotz. Chexpert [21] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-\nplus:Hundredsofthousandsofalignedradiologytexts,im- sionprobabilisticmodels. AdvancesinNeuralInformation\nagesandpatients. arXivpreprintarXiv:2405.19538,2024. ProcessingSystems,33:6840–6851,2020. 2\n5 [22] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKil-\n[11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin ianQWeinberger.Denselyconnectedconvolutionalnetworks.\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu InProceedingsoftheIEEEconferenceoncomputervision\nQiao,andZiweiLiu. Seine: Short-to-longvideodiffusion andpatternrecognition,pages4700–4708,2017. 6\n[23] JeremyIrvin,PranavRajpurkar,MichaelKo,YifanYu,Sil- [36] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Y Feng,\nvianaCiurea-Ilcus,ChrisChute,HenrikMarklund,Behzad WuyangLi,XinyuLiu,ZhenChen,JingShao,andYixuan\nHaghgoo,RobynBall,KatieShpanskaya,etal. Chexpert: Yuan. Endora:Videogenerationmodelsasendoscopysimu-\nAlargechestradiographdatasetwithuncertaintylabelsand lators. arXivpreprintarXiv:2403.11050,2024. 3\nexpertcomparison. InProceedingsoftheAAAIconference [37] Linyuan Li, Jianing Qiu, Anujit Saha, Lin Li, Poyuan Li,\nonartificialintelligence,pages590–597,2019. 5 MengxianHe,ZiyuGuo,andWuYuan.Artificialintelligence\n[24] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, forbiomedicalvideogeneration,2024. 1\nNathaniel R Greenbaum, Matthew P Lungren, Chih-ying\n[38] Yu-YingLiu,ShuangLi,FuxinLi,LeSong,andJamesM\nDeng,RogerGMark,andStevenHorng. Mimic-cxr,ade-\nRehg. Efficientlearningofcontinuous-timehiddenmarkov\nidentified publicly available database of chest radiographs modelsfordiseaseprogression. Advancesinneuralinforma-\nwithfree-textreports. Scientificdata,6(1):317,2019. 5 tionprocessingsystems,28,2015. 1,2\n[25] EuijinJung,MiguelLuna,andSangHyunPark. Conditional\n[39] JustinLovelaceandBobakMortazavi. Learningtogenerate\nganwith3ddiscriminatorformrigenerationofalzheimer’s\nclinically coherent chest x-ray reports. In Findings of the\ndiseaseprogression. PatternRecognition,133:109061,2023.\nAssociationforComputationalLinguistics: EMNLP2020,\n2\npages1235–1243,2020. 7\n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\n[40] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher\nElucidatingthedesignspaceofdiffusion-basedgenerative\nYu,RaduTimofte,andLucVanGool. Repaint: Inpainting\nmodels. arXivpreprintarXiv:2206.00364,2022. 2\nusingdenoisingdiffusionprobabilisticmodels. InProceed-\n[27] AmirhosseinKazerouni,EhsanKhodapanahAghdam,Moein\ningsoftheIEEE/CVFConferenceonComputerVisionand\nHeidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu,\nPatternRecognition,pages11461–11471,2022. 7\nandDoritMerhof. Diffusionmodelsinmedicalimaging:A\n[41] JunMaandBoWang. Segmentanythinginmedicalimages.\ncomprehensivesurvey. MedicalImageAnalysis,88:102846,\narXivpreprintarXiv:2304.12306,2023. 7\n2023. 2\n[42] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and\n[28] Firas Khader, Gustav Müller-Franzes, Soroosh\nBo Wang. Segment anything in medical images. Nature\nTayebi Arasteh, Tianyu Han, Christoph Haarburger,\nCommunications,15(1):654,2024. 4\nMaximilian Schulze-Hagen, Philipp Schad, Sandy Engel-\nhardt,BettinaBaeßler,SebastianFoersch,etal. Denoising [43] AliMadani,MehdiMoradi,AlexandrosKarargyris,andTan-\ndiffusion probabilistic models for 3d medical image veerSyeda-Mahmood. Chestx-raygenerationanddataaug-\ngeneration. ScientificReports,13(1):7303,2023. 2 mentationforcardiovascularabnormalityclassification. In\nMedicalimaging2018: Imageprocessing,pages415–420.\n[29] DiederikPKingmaandMaxWelling. Auto-encodingvaria-\nSPIE,2018. 2\ntionalbayes. arXivpreprintarXiv:1312.6114,2013. 2\n[30] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, [44] Midjourney. Midjourney – home. https://www.\nChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- midjourney.com/home,2024. Accessed: 2024-07-30.\nhead,AlexanderCBerg,Wan-YenLo,etal. Segmentany- 2\nthing. arXivpreprintarXiv:2304.02643,2023. 7 [45] Midjourney. Videogenerationmodelsasworldsimulators.\n[31] MaxKu,CongWei,WeimingRen,HuanYang,andWenhu https://www.midjourney.com/home, 2024. Ac-\nChen. Anyv2v:Aplug-and-playframeworkforanyvideo-to- cessed:2024-07-30. 2\nvideoeditingtasks. arXivpreprintarXiv:2403.14468,2024. [46] Peter G Mikhael, Jeremy Wohlwend, Adam Yala, Ludvig\n3 Karstens,JustinXiang,AngeloKTakigami,PatrickPBour-\n[32] Nicholas R Kurtansky, Brian M D’Alessandro, Maura C gouin,PuiYeeChan,SofianeMrah,WaelAmayri,etal.Sybil:\nGillis,BrigidBetz-Stablein,SaraECerminara,RafaelGar- avalidateddeeplearningmodeltopredictfuturelungcan-\ncia,MarcelaAlvesGirundi,ElisabethVictoriaGoessinger, cerriskfromasinglelow-dosechestcomputedtomography.\nPhilippeGottfrois,PascaleGuitera,etal.Theslice-3ddataset: JournalofClinicalOncology,pagesJCO–22,2023. 2\n400,000skinlesionimagecropsextractedfrom3dtbpfor [47] GustavMüller-Franzes,JanMoritzNiehues,FirasKhader,\nskincancerdetection. ScientificData,11(1):884,2024. 5 SorooshTayebiArasteh,ChristophHaarburger,Christiane\n[33] DaeunKyung,JunuKim,TackeunKim,andEdwardChoi. Kuhl, Tianci Wang, Tianyu Han, Teresa Nolte, Sven\nTowardspredictingtemporalchangesinapatient’schestx-ray Nebelung,etal. Amultimodalcomparisonoflatentdenois-\nimagesbasedonelectronichealthrecords. arXivpreprint ingdiffusionprobabilisticmodelsandgenerativeadversarial\narXiv:2409.07012,2024. 2 networksformedicalimagesynthesis. ScientificReports,13\n[34] Garam Lee, Kwangsik Nho, Byungkon Kang, Kyung-Ah (1):12098,2023. 2\nSohn,andDokyoonKim. Predictingalzheimer’sdiseasepro- [48] Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su\ngressionusingmulti-modaldeeplearningapproach.Scientific Ruan, Qian Wang, and Dinggang Shen. Medical im-\nreports,9(1):1952,2019. 1 agesynthesiswithcontext-awaregenerativeadversarialnet-\n[35] SuhyeonLee,WonJunKim,JinhoChang,andJongChulYe. works. InMedicalImageComputingandComputerAssisted\nLlm-cxr:Instruction-finetunedllmforcxrimageunderstand- Intervention-MICCAI2017:20thInternationalConference,\ningandgeneration. arXivpreprintarXiv:2305.11490,2023. QuebecCity,QC,Canada,September11-13,2017,Proceed-\n2 ings,PartIII20,pages417–425.Springer,2017. 2\n[49] HadasOrgad,BahjatKawar,andYonatanBelinkov. Editing Bs-net: Learning covid-19 pneumonia severity on a large\nimplicitassumptionsintext-to-imagediffusionmodels.arXiv chest x-ray dataset. Medical Image Analysis, 71:102046,\npreprintarXiv:2303.08084,2023. 2 2021. 7\n[50] KaiPackhäuser,LukasFolle,FlorianThamm,andAndreas [63] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,\nMaier. Generationofanonymouschestradiographsusing Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nlatentdiffusionmodelsfortrainingthoracicabnormalityclas- OranGafni,etal. Make-a-video: Text-to-videogeneration\nsificationsystems. In2023IEEE20thInternationalSympo- withouttext-videodata. arXivpreprintarXiv:2209.14792,\nsiumonBiomedicalImaging(ISBI),pages1–5.IEEE,2023. 2022. 2\n2 [64] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising\n[51] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun diffusionimplicitmodels. arXivpreprintarXiv:2010.02502,\nLi,JingwanLu,andJun-YanZhu. Zero-shotimage-to-image 2020. 2\ntranslation. arXivpreprintarXiv:2302.03027,2023. 2 [65] KamileStankeviciute,AhmedMAlaa,andMihaelavander\n[52] Pika.Pikaart–home.https://pika.art/home,2024. Schaar. Conformal time-series forecasting. Advances in\nAccessed:2024-07-30. 2,6,7 NeuralInformationProcessingSystems,34:6216–6228,2021.\n2\n[53] PixVerse. Pixverse. https://app.pixverse.ai/,\n[66] RuiSun,YuminZhang,TejalShah,JiahaoSun,Shuoying\n2024. Accessed:2024-11-10. 6,7\nZhang, Wenqi Li, Haoran Duan, Bo Wei, and Rajiv Ran-\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\njan. Fromsorawhatwecansee: Asurveyoftext-to-video\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\ngeneration. arXivpreprintarXiv:2405.10674,2024. 2\nAmandaAskell,PamelaMishkin,JackClark,etal. Learning\n[67] WeixiangSun,XiaocaoYou,RuizheZheng,ZhengqingYuan,\ntransferable visual models from natural language supervi-\nXiangLi,LifangHe,QuanzhengLi,andLichaoSun. Bora:\nsion. InInternationalconferenceonmachinelearning,pages\nBiomedicalgeneralistvideogenerationmodel.arXivpreprint\n8748–8763.PMLR,2021. 2,3,5\narXiv:2407.08944,2024. 3\n[55] Daniele Ravi, Daniel C Alexander, Neil P Oxtoby, and\n[68] ChenyuTang,WentianYi,EdoardoOcchipinti,YanningDai,\nAlzheimer’sDiseaseNeuroimagingInitiative. Degenerative\nShuoGao,andLuigiGOcchipinti. Aroadmapforthede-\nadversarialneuroimagenets:generatingimagesthatmimic\nvelopment of human body digital twins. Nature Reviews\ndiseaseprogression. InInternationalConferenceonMedical\nElectricalEngineering,1(3):199–207,2024. 1\nImageComputingandComputer-AssistedIntervention,pages\n[69] PhilippTschandl,CliffRosendahl,andHaraldKittler. The\n164–172.Springer,2019. 6\nham10000dataset,alargecollectionofmulti-sourcedermato-\n[56] DanieleRavi,StefanoBBlumberg,SilviaIngala,Frederik\nscopicimagesofcommonpigmentedskinlesions. Scientific\nBarkhof,DanielCAlexander,NeilPOxtoby,Alzheimer’s\ndata,5(1):1–9,2018. 5\nDiseaseNeuroimagingInitiative,etal.Degenerativeadversar-\n[70] Alexandre Vallée. Envisioning the future of personalized\nialneuroimagenetsforbrainscansimulations:Application\nmedicine: Role and realities of digital twins. Journal of\ninageinganddementia. MedicalImageAnalysis,75:102257,\nMedicalInternetResearch,26:e50204,2024. 1\n2022. 2\n[71] XiangWang,DavidSontag,andFeiWang. Unsupervised\n[57] NathanRaw. Stablediffusionvideos. https://github.\nlearningofdiseaseprogressionmodels.InProceedingsofthe\ncom/nateraw/stable-diffusion-videos, 2023.\n20thACMSIGKDDinternationalconferenceonKnowledge\n6\ndiscoveryanddatamining,pages85–94,2014. 2\n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\n[72] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nPatrick Esser, and Björn Ommer. High-resolution image\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nsynthesis with latent diffusion models. In Proceedings of\nYu, PeiqingYang, etal. Lavie: High-qualityvideogener-\ntheIEEE/CVFConferenceonComputerVisionandPattern\nationwithcascadedlatentdiffusionmodels. arXivpreprint\nRecognition,pages10684–10695,2022. 2,3,4\narXiv:2309.15103,2023. 2\n[59] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, [73] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nMichaelRubinstein,andKfirAberman. Dreambooth:Fine Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\ntuningtext-to-imagediffusionmodelsforsubject-drivengen- MikeZhengShou. Tune-a-video:One-shottuningofimage\neration. arXivpreprintarXiv:2208.12242,2022. 5 diffusionmodelsfortext-to-videogeneration. arXivpreprint\n[60] PeterSchulamandRamanArora. Diseasetrajectorymaps. arXiv:2212.11565,2022. 6\nAdvancesinneuralinformationprocessingsystems,29,2016. [74] JiazhengXu,XiaoLiu,YuchenWu,YuxuanTong,Qinkai\n2 Li,MingDing,JieTang,andYuxiaoDong. Imagereward:\n[61] KristenASeverson,LanaMChahine,LubaSmolensky,Ken- Learningandevaluatinghumanpreferencesfortext-to-image\nney Ng, Jianying Hu, and Soumya Ghosh. Personalized generation. AdvancesinNeuralInformationProcessingSys-\ninput-outputhiddenmarkovmodelsfordiseaseprogression tems,36,2024. 6\nmodeling. InMachineLearningforHealthcareConference, [75] ZhuoyiYang,JiayanTeng,WendiZheng,MingDing,Shiyu\npages309–330.PMLR,2020. 1 Huang,JiazhengXu,YuanmingYang,WenyiHong,Xiao-\n[62] Alberto Signoroni, Mattia Savardi, Sergio Benini, Nicola hanZhang,GuanyuFeng,etal. Cogvideox: Text-to-video\nAdami,RiccardoLeonardi,PaoloGibellini,FilippoVaccher, diffusionmodelswithanexperttransformer. arXivpreprint\nMarcoRavanelli,AndreaBorghesi,RobertoMaroldi,etal. arXiv:2408.06072,2024. 6,7\n[76] XinYi,EktaWalia,andPaulBabyn. Generativeadversar-\nialnetworkinmedicalimaging: Areview. Medicalimage\nanalysis,58:101552,2019. 2\n[77] ZhuoningYuan,YanYan,MilanSonka,andTianbaoYang.\nLarge-scalerobustdeepaucmaximization:Anewsurrogate\nlossandempiricalstudiesonmedicalimageclassification. In\nProceedingsoftheIEEE/CVFInternationalConferenceon\nComputerVision,pages3040–3049,2021. 6\n[78] Zizhao Zhang, Lin Yang, and Yefeng Zheng. Translating\nandsegmentingmultimodalmedicalvolumeswithcycle-and\nshape-consistencygenerativeadversarialnetwork.InProceed-\ningsoftheIEEEconferenceoncomputervisionandpattern\nRecognition,pages9242–9251,2018. 2",
    "pdf_filename": "Medical_Video_Generation_for_Disease_Progression_Simulation.pdf"
}