{
    "title": "Medical Video Generation for Disease Progression Simulation",
    "context": "Modeling disease progression is crucial for improving the quality and efficacy of clinical diagnosis and prognosis, but it is often hindered by a lack of longitudinal medical image monitoring for individual patients. To address this chal- lenge, we propose the first Medical Video Generation (MVG) framework that enables controlled manipulation of disease- related image and video features, allowing precise, realistic, and personalized simulations of disease progression. Our ap- proach begins by leveraging large language models (LLMs) to recaption prompt for disease trajectory. Next, a con- trollable multi-round diffusion model simulates the disease progression state for each patient, creating realistic interme- diate disease state sequence. Finally, a diffusion-based video transition generation model interpolates disease progression between these states. We validate our framework across three medical imaging domains: chest X-ray, fundus photog- raphy, and skin image. Our results demonstrate that MVG significantly outperforms baseline models in generating co- herent and clinically plausible disease trajectories. Two user studies by veteran physicians, provide further validation and insights into the clinical utility of the generated sequences. MVG has the potential to assist healthcare providers in mod- eling disease trajectories, interpolating missing medical im- age data, and enhancing medical education through realistic, dynamic visualizations of disease progression. Disease progression refers to the way an illness evolves in an individual over time. Understanding this progression en- ables healthcare professionals to develop effective treatment strategies, anticipate complications, and adjust care plans accordingly. Disease progression modeling can also be seen as a form of human digital twin, laying the foundation for future precision medicine [37, 68, 70]. However, modeling disease progression on medical images presents significant challenges. These challenges arise primarily from the lack Region Guide Input Fundus Retinal Image MVG Region Guide Chest X-ray Image MVG Region Guide Skin Image MVG Prompt-controlled Disease Progression Generation Figure 1. Illustrative examples of video-based disease progres- sion simulation (6-8s) using predefined medical reports and our proposed method. The top sequence depicts a patient’s Diabetic Retinopathy. The middle sequence demonstrates the Edema in a patient’s lung. The bottom sequence demonstrates the Benign Skin Lesion in a patient’s skin. of continuous monitoring of individual patients over time, as well as the high cost and risks associated with collecting longitudinal imaging data [13, 38, 61]. The intricate and multifaceted dynamics of disease progression, combined with the lack of comprehensive and continuous image or video data of individual patients, result in the absence of established methodologies for medical imaging trajectories simulation [34]. Recent advancements in image and video generation mod- els present promising opportunities for simulating realistic medical videos, potentially enriching existing databases and addressing data limitations. To incorporate generative mod- els into disease progression simulations, we establish three key criteria that medical video generation models must meet: (i) The model should generate videos presenting long disease progression under zero-shot setting, as there are no existing arXiv:2411.11943v1  [cs.CV]  18 Nov 2024",
    "body": "Medical Video Generation for Disease Progression Simulation\nXu Cao1, Kaizhao Liang2,3, Kuei-Da Liao4, Tianren Gao5, Wenqian Ye6, Jintai Chen1,\nZhiguang Ding7, Jianguo Cao8, James M. Rehg1, Jimeng Sun1\n1University of Illinois Urbana-Champaign\n2University of Texas at Austin\n3SambaNova System, Inc\n4Objective, Inc\n5Microsoft\n6University of Virginia\n7Shenzhen Nanshan People’s Hospital\n8Shenzhen Children’s Hospital\nAbstract\nModeling disease progression is crucial for improving the\nquality and efficacy of clinical diagnosis and prognosis, but\nit is often hindered by a lack of longitudinal medical image\nmonitoring for individual patients. To address this chal-\nlenge, we propose the first Medical Video Generation (MVG)\nframework that enables controlled manipulation of disease-\nrelated image and video features, allowing precise, realistic,\nand personalized simulations of disease progression. Our ap-\nproach begins by leveraging large language models (LLMs)\nto recaption prompt for disease trajectory. Next, a con-\ntrollable multi-round diffusion model simulates the disease\nprogression state for each patient, creating realistic interme-\ndiate disease state sequence. Finally, a diffusion-based video\ntransition generation model interpolates disease progression\nbetween these states. We validate our framework across\nthree medical imaging domains: chest X-ray, fundus photog-\nraphy, and skin image. Our results demonstrate that MVG\nsignificantly outperforms baseline models in generating co-\nherent and clinically plausible disease trajectories. Two user\nstudies by veteran physicians, provide further validation and\ninsights into the clinical utility of the generated sequences.\nMVG has the potential to assist healthcare providers in mod-\neling disease trajectories, interpolating missing medical im-\nage data, and enhancing medical education through realistic,\ndynamic visualizations of disease progression.\n1. Introduction\nDisease progression refers to the way an illness evolves in\nan individual over time. Understanding this progression en-\nables healthcare professionals to develop effective treatment\nstrategies, anticipate complications, and adjust care plans\naccordingly. Disease progression modeling can also be seen\nas a form of human digital twin, laying the foundation for\nfuture precision medicine [37, 68, 70]. However, modeling\ndisease progression on medical images presents significant\nchallenges. These challenges arise primarily from the lack\nRegion Guide\nInput Fundus \nRetinal Image\nMVG\nRegion Guide\nChest X-ray \nImage\nMVG\nRegion Guide\nSkin Image\nMVG\nPrompt-controlled Disease Progression Generation\nFigure 1. Illustrative examples of video-based disease progres-\nsion simulation (6-8s) using predefined medical reports and our\nproposed method. The top sequence depicts a patient’s Diabetic\nRetinopathy. The middle sequence demonstrates the Edema in a\npatient’s lung. The bottom sequence demonstrates the Benign Skin\nLesion in a patient’s skin.\nof continuous monitoring of individual patients over time,\nas well as the high cost and risks associated with collecting\nlongitudinal imaging data [13, 38, 61]. The intricate and\nmultifaceted dynamics of disease progression, combined\nwith the lack of comprehensive and continuous image or\nvideo data of individual patients, result in the absence of\nestablished methodologies for medical imaging trajectories\nsimulation [34].\nRecent advancements in image and video generation mod-\nels present promising opportunities for simulating realistic\nmedical videos, potentially enriching existing databases and\naddressing data limitations. To incorporate generative mod-\nels into disease progression simulations, we establish three\nkey criteria that medical video generation models must meet:\n(i) The model should generate videos presenting long disease\nprogression under zero-shot setting, as there are no existing\narXiv:2411.11943v1  [cs.CV]  18 Nov 2024\n\ndatasets for image and video-based disease progression; (ii)\nThe generated disease states must be semantically relevant\nto the initial input image; (iii) The generated progression\nshould be clinically verified and consistent with the corre-\nsponding text descriptions.\nIn this work, we propose MVG, a video generation frame-\nwork for simulating disease progression that integrates text\ninference, progressive image generation, and video clip tran-\nsition generation. Specifically, our approach uses GPT-4 [2]\nto summarize clinical reports and generate prompts, which\nare then used to progressively control disease-related fea-\ntures extracted by a text encoder. This approach allows us\nto conditionally simulate disease progression in the visual\ndomain without significantly altering the core features of\nthe initial image (see Figure 1). Our framework is built on\nthe invertibility of denoising diffusion probabilistic mod-\nels [21, 64], the visual-language alignment capabilities of\ncontext encoders [16], and frame-level synthesis.\nOur theoretical analysis demonstrates that the multi-step\ndisease state simulation module of MVG can be understood\nas a gradient descent process toward maximizing the log-\nlikelihood of the given text conditioning. The learning rate\nin this iterative process decays exponentially with each for-\nward step, allowing the algorithm to effectively explore the\nsolution space while balancing convergence speed and sta-\nbility. This guarantees that our framework moves toward\nthe target disease manifold, ensuring that the modifications\nmade are clinically plausible and remain bounded for medi-\ncal concepts. Finally, after generating a sequence of disease\nstate images, we utilize a video transition generation model,\nguided by conditional masks, to interpolate between succes-\nsive disease states, thereby creating a realistic simulation of\ndisease progression.\nThe contributions are summarized as follows:\n• We propose the first medical video simulation framework\nMVG, which allows for a precise understanding of disease-\nrelated image features and leads to accurate and individu-\nalized longitudinal disease progression simulation.\n• We provide theoretical evidence that our iterative refine-\nment process is equivalent to gradient descent with an\nexponentially decaying learning rate, which helps to estab-\nlish a deeper understanding of applying diffusion-based\ngenerative models in healthcare research.\n• We demonstrate the superior performance of MVG over\nbaselines in disease progression prediction with three med-\nical domains using CLIP-I score, disease classification\nconfidence score, and physician user preference study.\n• In the follow-up user study, 35 physicians agree that 76.2%\nof disease state sequences simulated by MVG closely\nmatched physicians’ expectations, indicating our genera-\ntion results are high related to the clinical context.\n2. Related Works\nDisease Progression Simulation.\nLongitudinal disease\nprogression data derived from individual electronic health\nrecords offer an exciting avenue to investigate the nuanced\ndifferences in the progression of diseases over time [46, 60,\n65]. However, most of the previous works are based on\nHMM [38, 71] or deep probabilistic models [3] without us-\ning data from imaging space. Some recent works have started\nto resolve image disease progression simulation by using\ndeep-generation models. [25, 56] utilized the Generative Ad-\nversarial Networks (GANs) based model and linear regressor\nwith individual sequential monitoring data for Alzheimer’s\ndisease progression simulation in MRI imaging space. All\nthese methods have to use full sequential images as training\nsets and are hard to adapt to the general medical imaging\ndomain.\nGenerative Models.\nRecently, Denoising Diffusion Mod-\nels [21, 26, 58, 64] have become increasingly popular due to\ntheir ability to create high-resolution realistic images from\ntextual descriptions. One major advantage of these models\nis they can use CLIP [54] embedding to guide image editing\nbased on contextual prompts. Among the various text-to-\nimage models, latent diffusion model (LDM) [16, 58] and\nits follow-up image-to-image editing works [9, 49, 51] has\nreceived considerable attention because of its impressive per-\nformance in generating high-quality images and its ability to\nedit scenarios across multiple modalities.\nWhile image generation has seen substantial progress in\ngeneral domains, its application in the medical field remains\nless explored [76]. Earlier work using Variational Autoen-\ncoders (VAEs) [29] and GANs [18] focused on generating\nmedical images like X-rays and MRIs to address the issue\nof limited training data [14, 43, 48, 78]. The introduction\nof LDMs significantly improved the quality of these im-\nages [27, 47, 50], even extending to 3D synthesis [15, 28].\nRecently, efforts have been made to unify medical report\ngeneration with image synthesis [7, 35], and design image\nediting pipeline for counterfactual medical image genera-\ntion [19, 33].\nText-to-Video Generation.\nText-to-image models have at-\ntracted significant attention from both academia and industry,\nas evidenced by advancements like DALL·E [4], Midjour-\nney [44], and Stable Diffusion 3 [16]. These innovations\nhave significantly impacted the text-to-video domain [66],\nleading to the development of models such as Sora [45],\nPika [52], and Stable Diffusion Video [5]. The core of these\ntext-to-video models often involves fine-tuning or integrat-\ning additional modules or priors into pre-trained text-to-\nimage diffusion models using video data, as seen in Make-\nA-Video [63], PYoCo [17], and LaVie [72], SEINE [11],\n\nAnyV2V [31]. However, applying video generation models\nin the healthcare domain presents challenges, particularly\nbecause time-series medical imaging data for disease pro-\ngression is difficult to collect. While some studies have\nexplored video generation in medical imaging [36, 67], they\nhave not focused on simulating disease progression.\n3. Problem Statement\nTraditional image to video generation models need to train\nwith a large amount of text-to-video or image-to-video data.\nHowever, it is almost impossible to obtain large-scale longi-\ntude medical imaging data (can be also considered as a type\nof medical video data) as most patients may not go to the\nsame hospital for follow-up treatment and the hospitals also\nlack medical imaging and clinical reports in the early stages\nof diseases.\nIn our paper, we reconsider this problem in another way.\nGiven an input medical image x0, and clinical report and\nmedical history label y0. Experienced medical doctors can\npredict the disease progression of the patient based on their\nclinical prior knowledge, denoted as yN, where N + 1 is the\ntotal number of states of the predicted disease. The predicted\ndisease progression is a video sequence X, which can be\nseparated by a set of short video clips { ˆx0, ˆx1, ˆx2, ...,\nˆ\nxN−1},\nwhere ˆxi ∈RK×H×W ×C is a video clip between disease\nimage state xi and xi+1. K, H, W, C denote the number of\nframes, height, width, and channels of the video clip. K is a\nvery small number to control the disease progression change\nin a limited medical imaging space. In ˆxi, the starting frame\nxi ∈RH×W ×C is the initial disease state and end frame\nxi+1 ∈RH×W ×C is the end disease state.\nx0\nx1\nx2\nx4\nx10\nxN\nFigure 2. Visualization for cardiomegarly disease state absolute\ndifference heatmap. The highlighted red portion illustrates the\nprogression of the pathology at each step.\nWe separate the disease progression video generation into\na two stage strategy. In the first stage, the key idea is to gener-\nate discrete disease progressive states {x0, x1, x2, ..., xN}:\nx1:T = fθ(x0, yT )\n(1)\nIn the training phase of the first stage, fθ is a denois-\ning diffusion model learned from independent identically\ndistributed (x, y) from different patients.\nIn the second stage, we adopt video latent diffusion mod-\nels finetuned with video data in the general domain. In doing\nso, we convert the disease progression video generation task\ninto a frame-level transition generation problem:\nˆxi = gϕ(xi, xi+1)\n(2)\nThe output videos { ˆx0, ˆx1, ˆx2, ..., ˆ\nxN} finally con-\ncatenate into the disease progression video X\n∈\nRKN×H×W ×C.\n4. Medical Video Generation (MVG)\nAs shown in Figure 3, MVG contains two main components:\n(i) Progressive disease image editing (PIE) with medical\ndomain-specific diffusion model and (ii) Transition Gener-\nation Process between generated disease states with video\nlatent diffusion model.\nThe first component PIE is a long-sequence medical im-\nage editing framework proposed to refine and enhance im-\nages iteratively and discretely, allowing clinical report-based\nprompts for precise adjustments to simulate disease develop-\nment while keeping realism. Unlike traditional image editing\ntechniques, PIE involves a multi-stage process where each\nstep builds upon the previous one, intending to achieve a\nfinal result that is more refined than if all changes were made\nat once. Transition generation is used in the long video gen-\neration model to connects different narrative moments. Once\nthe frame-level sequence is generated by PIE, we will pro-\nvide each pair of adjacent frames and use transition prompts\nand disease region mask to control the style and content, cre-\nating intermediate frames that further illustrate the transition\nor progression within the medical video sequence.\n4.1. Progressive Image Editing (PIE)\nProcedure.\nThe inputs to PIE are a discrete medical image\nx0\n0 depicting any start or middle stage of a disease and a\ncorresponding terminal stage clinical report yN inferred by\nmedical doctor and then re-captioned by GPT-4 [2], provid-\ning the potential hint of the patient’s disease progression.\nThe Latent y will be the text conditioning of the diffusion\nmodel [58]. y is generated from a pretrained text encoder\nfrom CLIP [54] (clip-vit-large-patch14), where the text input\nis yN. The output generated by PIE is a sequence of images\npresenting the disease progression, {x0\n0, x0\n1, · · · , x0\nN}. The\niterative PIE procedure is defined as follows:\nProposition 1.\n*\nLet x0\nn ∼χ, where χ is distribution\nof photo-realistic medical images, y be the text condition-\ning, running PIEn(·, ·) recursively is denoted as following,\nwhere n = {N, N −1, · · · 1},\nx0\nn = PIEn(x0\nn−1, y)\n(3)\nx0\nN = PIEN ◦PIEN−1 ◦· · · ◦PIE1\n|\n{z\n}\nN times\n(x0\n0, y)\n(4)\n*The proof of Proposition 1 and Proposition 2 are shown in the supple-\nmentary material.\n\nRandomly perturbed\nText \nEncoder\nAfter disease progressed, heart size mildly \nenlarged exaggerated by low lung volumes.\nUsers\nInitial Report\nPIE0\nPIE𝑛\n𝑃𝜃\n𝑃𝜃\n𝑥𝑛−1\n0\n𝑥𝑛−1\n𝑘\n𝑥𝑛−1\n𝑘−1\n𝑥𝑛0\nRefinement\n𝑘−1\n𝒙𝟎\n𝟎\n𝒙𝟏\n𝟎\n𝒙𝑵\n𝟎\n𝑁−1\nROI Extraction\n…\n…\nLLM Re-captioning\nSource \nState\nTarget \nState\nSource State\nTarget State\nMid State 1\nMid \nState 1\nTransition Generation\n(N-1) * Transition Generation\nFigure 3. Overview of the MVG inference pipeline. The above blue part denotes the single step of PIE. For any given step n in PIE, we first\nutilize inversion of diffusion model to procure an inverted noise map. Subsequently, we denoise it using GPT-4 re-captioned clinical reports\nfrom the future state and use the ROI mask to refine the editing after the last step of denoising. The output of a single step of PIE is the input\nfor the next step n + 1, thus ensuring a gradual and controllable disease progression simulation. After simulating N steps, the image is\nconverged to the final state. The below green part shows the transition generation process between disease states. We use ROI mask to\ncontrol the mask recovery of SEINE and finally output the long sequence of video-based disease progression.\nThen, the resulting final output x0\nN maximizes the posterior\nprobability p(x0\nN| x0\n0, y).\nTo run the inference pipeline of PIE to generate a discrete\ndisease progression image sequence, we use the original\ninput image x0\n0 as the start point. The hyperparameters are\nthe number of progression stage N, the number of diffusion\nsteps T, text conditional vector y, noise strength γ, diffusion\nparameterized denoiser ϵθ, and a region of interest (ROI)\nmask MROI, where each pixel in M i,j\nROI ∈[0, 1].\nSince PIE is a recursive process, at progression stage n,\nthe input image is x0\nn−1. From diffusion step k to 1,\nx′ ←√αt−1(x′ −√1 −αtϵ(t)\nθ (x′, y)\n√αt\n)+\np\n1 −αt−1 · ϵ(t)\nθ (x′, y)\n(5)\nwhere x′ in step k is x0\nn−1, k is γ · T, ϵ(t)\nθ (x′, y) is the noise\nprediction by UNet or Transformer, where θ is the parameter\nin the denoiser. After the last step, we use the MROI initially\ngenerated by pretrained Med-SAM [42] and then slightly\nedit by human to control and refine the final output:\nx′ ←(β1 · (x′ −x0\n0) + x0\n0) · (1 −MROI)+\n(β2 · (x′ −x0\n0) + x0\n0) · MROI\n(6)\nwhere β1, β2 are hyperparameter to control the interpola-\ntion between generated result and the input image. The last\noutput image x′ is xT\nn−1, which is also the input x0\nn of the\nnext step (n + 1 step) disease state generation. Equation 6\nguarantees the editing is regional based and avoids the im-\nage distortion caused by multiple times image editing. It is\nworth noting that Equation 6 can generalize to arbitrary dif-\nfusion backbones including Stable Diffusion-1.4 [58], Stable\nDiffusion 3 [16].\nWith each round of editing as shown in the middle part\nof Figure 3, the image gets closer to the objective by mov-\ning in the direction of −∇log p(x|y). The step size would\ngradually decrease with a constant factor. The iterative con-\nvergence analysis is as follows:\n\nProposition 2. Assuming ∥x0\n0∥≤C1 and ∥ϵθ(x, y)∥≤C2,\n(x, y) ∈(χ, Γ), for any δ > 0, if\nn >\n2\nlog(α0) · (log(δ) −C)\n(7)\nthen,\n∥x0\nn+1 −x0\nn∥< δ\n(8)\nwhere, λ =\n√α0−α0α1−√α1−α0α1\n√α1\n, χ is the image dis-\ntribution, Γ is the text condition distribution , and C =\nlog((\n1\n√α0 −1) · C1 + λ · C2)\nProposition 2 shows as n grows bigger, the changes be-\ntween steps would grow smaller. Eventually, the difference\nbetween steps will get arbitrarily small. The convergence of\nPIE is guaranteed, and modifications to any medical imaging\ninputs are bounded by a constant. The proof of Proposition\n2 is shown in the supplementary material.\n4.2. Transition Generation Process\nThe concept of scene transition generation is first proposed\nby SEINE [11], which is a short-to-long video diffusion\nmodel. In MVG, we use MROI to control SEINE to connect\nthe disease progression between each step generated by PIE,\nˆ\nxn\n′ = Concat(x0\nn−1, ϵ, · · · , ϵ\n| {z }\nrandom noise\n, x0\nn)\n(9)\nˆ\nxn = x0\nn−1 + x0\nn\n2\n· (1 −MROI) + g( ˆ\nxn\n′) · MROI\n(10)\n,where ˆ\nxn is a video clip with the first and last frames\nare the input x0\nn−1 and output x0\nn from progression stage n\nin PIE. Between x0\nn−1 and x0\nn, all frames are masks with\nrandom noise. By predicting and modeling the noise, the\ntransition generation process g(·) aims to extend realistic,\nvisually coherent transition frames that seamlessly integrate\nthe visible frames with the unmasked ones.\n5. Experiments and Results\nIn this section, we present experiments on various disease\nprogression tasks. Experiments results demonstrate that\nMVG can simulate the disease-changing trajectory that is\ninfluenced by different medical conditions. Notably, MVG\nalso preserves unrelated visual features from the original\nmedical imaging report, even as it progressively edits the\ndisease representation. Figure 5 showcases a set of disease\nprogression simulation examples across three distinct types\nof medical imaging. Details for Stable Diffusion fine-tuning,\npretraining model for confidence metrics settings are avail-\nable in the Supplementary.\nDatasets\nImaging Type\nInstances\ninput size\nCheXpert Plus [10]\nChest X-ray\n223,462\n512 × 512\nMIMIC-CXR [24]\nChest X-ray\n227,835\n512 × 512\nDiabetic Retinopathy Detection [1]\nRetinopathy\n35,126\n1024 × 1024\nISIC 2024 [32]\nSkin\n40,1059\n128 × 128\nISIC 2018 [12]\nSkin\n10,015\n128 × 128\nTable 1. Datasets used to train PIE of MVG.\n5.1. Experimental Setups\nImplementation Details. For experiments in Table 2, PIE\nand the baselines are using publicly available Stable Diffu-\nsion checkpoints (CompVis/stable-diffusion-v1-4) and then\nwe further finetune on the training sets of each of the tar-\nget datasets. This is because the pipeline of the other two\nbaselines only support the model weight from original Sta-\nble Diffusion 1.4 version. For user study in Table 3, we\nadopt Stable Diffusion 3 medium as the model weight and\nfinetune it with three medical domain. The weight for tran-\nsition generation model is from SEINE [11]. Our code and\ncheckpoints will be publicly available upon publication. All\nexperiments are conducted on 4 NVIDIA H100 GPUs.\nDatasets for Disease Progression.\nWe evaluate the pre-\ntrained domain-specific stable diffusion model on three dif-\nferent types of disease datasets from different tasks: CheX-\npert Plus [10] and MIMIC-CXR [24] for chest X-ray classi-\nfication and report generation [10, 23, 24], ISIC 2024 and\nISIC 2018 [12, 32, 69] for skin cancer prediction, and Kag-\ngle Diabetic Retinopathy Detection Challenge [1]. Each of\nthese datasets presents unique challenges and all of them\nhaving large-scale of data, making them suitable for testing\nthe robustness and versatility of MVG. We also collected\nover 50 data among the test set from these datasets as initial\ninput data for disease progression video generation. These\ndata were used for disease progression simulation. Three\ngroups of progression visualization results can be found in\nFigure 5.\nEvaluation Metrics.\nThe evaluation of generated disease\nprogression images focuses on two key aspects: alignment\nwith the intended disease features and preservation of patient\nidentity. To assess these aspects, we employ two primary\nmetrics: the CLIP-I score and classification confidence score,\nallowing us to compare the baselines and PIE (stage 1 of\nMVG) under consistent conditions.\nThe CLIP-I score (theoretically ranging from [0, 1]) rep-\nresents the average pairwise cosine similarity between the\nCLIP embeddings of the generated medical image sequence\nand the initial real medical images [54, 59]. A high CLIP-I\nscore indicates strong patient identity consistency but also\nmeans minimal changes between the edited sequence and\nthe original input. Therefore, an ideal disease progression\n\nMethod\nChest X-ray\nFundus Retinal Image\nSkin Image\nConf (↑)\nCLIP-I (↑)\nConf (↑)\nCLIP-I (↑)\nConf (↑)\nCLIP-I (↑)\nExtrapolation Methods [20]\n0.054\n0.972\n0.074\n0.991\n0.226\n0.951\nSable Video Diffusion (SVD) [57]\n0.389\n0.923\n0.121\n0.892\n0.201\n0.886\nPIE (Stage 1 of MVG)\n0.712\n0.978\n0.807\n0.992\n0.453\n0.958\nTable 2. Comparisons with commercial image editing tools with other finetuned multi-step medical image editing simulations. The backbone\nof PIE and all baseline approaches [5, 20] are used the same finetuned Stable Diffusion v1.4 weight on each dataset.\nMethod A\nMethod B\nX-ray\nSkin\nRetinal\nHumanEval (↑)\nHumanEval (↑)\nHumanEval (↑)\nPika [52]\nPixVerse [53]\n0.42\n0.50\n0.54\nCogVideoX [75]\n0.46\n0.42\n0.67\nMVG (Our)\n0.20\n0.33\n0.33\nPixVerse [53]\nPika [52]\n0.58\n0.50\n0.46\nCogVideoX [75]\n0.58\n0.58\n0.58\nMVG (Our)\n0.23\n0.37\n0.37\nCogVideoX [75]\nPika [52]\n0.54\n0.58\n0.33\nPixVerse [53]\n0.42\n0.42\n0.42\nMVG (Our)\n0.17\n0.33\n0.20\nMVG (Our)\nPika [52]\n0.80\n0.67\n0.63\nPixVerse [53]\n0.77\n0.63\n0.67\nCogVideoX [75]\n0.83\n0.67\n0.80\nTable 3. User preference A/B test from 30 verified clinicians,\nradiologists of the generated disease progression videos from MVG\nand three SOTA image-to-video generation models.\nsequence should balance the degree of editing with identity\npreservation.\nThe classification confidence score is derived from a su-\npervised deep network trained for binary classification be-\ntween negative (healthy) and positive (disease) samples. It\nis defined as Conf = Sigmoid(fθ(x)) and measures how\nwell the generated images align with the target disease state.\nFor our experiments, we utilize the DeepAUC maximiza-\ntion method [77]—recognized for its SOTA performance on\nCheXpert and ISIC 2018 task 3—using DenseNet121 [22] as\nthe backbone to compute the classification confidence score.\nHowever, these metrics alone are insufficient for evaluat-\ning the clinical relevance of the generated video sequences.\nTherefore, inspired by ImageReward [74], we also conducted\na clinician preference evaluation to compare MVG with sev-\neral SOTA image-to-video generation models. We used two\nsets of data from three medical domains and engaged 30 clin-\nicians and radiologists (verified by co-authors from clinical\ninstitutions) to rank these videos through A/B testing.\nBaselines.\nTo our knowledge, there are no existing genera-\ntion models specifically designed for simulating discrete dis-\nease progression sequences or videos under the no trainable\nsequential data setting. To underscore the unique strengths\nof MVG, we compare it against with related baseline multi-\nstage diffusion generation strategy. One of them is Sta-\nble Video Diffusion (SVD), also called Stable Diffusion\nPIE\nSVD\nExtrapolation\nFigure 4. Editing path of PIE, SVD, and Extrapolation in the\nmanifold.\nWalk [57] for short video generation. SVD is the basic of\nthe latent-based video generation methods like Stable Dif-\nfusion Video [6, 73], but it do not need any training from\nvideo datasets. Another one is the Style-Based Manifold Ex-\ntrapolation (Extrapolation) [20] for generating progressive\nmedical imaging with GAN, as it don’t need diagnosis la-\nbeled data [20, 55], which is similar to our definition setting\nbut it need plenty of progression inference prior. In Figure 4,\nwe showcase how these model edit the image with multi-step\nby prompt guidance in the manifold. During the comparison,\nall trainable baseline methods are using the same Stable Dif-\nfusion finetuned weights in specific dataset and also applied\nMROI for region guidance.\nChest X-ray\nFundus\nSkin\nInput Image\nStep 1\nStep 4\nStep 10\nFigure 5. Disease Progression Simulation of MVG. The top progres-\nsion is for Cardiomegarly. The middle progression is for Diabetic\nRetinopathy. The bottom progression is for Melanocytic Nevus.\n5.2. Disease State Simulation\nIn order to demonstrate the superior performance of MVG in\ndisease progression simulation over other single-step editing\n\nmethods, we perform experiments on three datasets previ-\nously mentioned. For each disease in these datasets, we used\n50 healthy samples in the test set as simulation start point\nand run MVG, SVD, Extrapolation with 5 random seeds.\nWe obtain at least 50 disease imaging trajectories for each\npatient. Table 2 showcases that MVG consistently surpasses\nboth SVD and Extrapolation in terms of disease confidence\nscores while maintaining high CLIP-I scores. For Chexpert\ndataset, the 0.712 final confidence score is the average score\namong 5 classes. For Diabetic Retinopathy and ISIC 2018\ndatasets, we compare MVG with SVD, Extrapolation for\nediting image to the most common seen class since these\ndatasets are highly imbalanced. We observe that MVG is\nable to produce more faithful and realistic progressive edit-\ning compared to the other two baselines. Interestingly, while\nthe CLIP-I score of Extrapolation is comparable to that of\nMVG, it fails to effectively edit the key disease features of\nthe input images as its image never change during the infer-\nence and its classification confidence scores are also very\nlow.\nFigure 6 showcases a group of progression simulation\nqualitative results for Edema in chest X-rays with CheXpert\nclinical report prompt. It is evident from our observations\nthat while SVD can significantly alter the input image in the\ninitial step, it fails to identify the proper direction of pro-\ngression in the manifold after a few steps and would easily\ncreate uncontrollable noise. Conversely, Extrapolation only\nbrightens the Chest X-ray without making substantial mod-\nifications. MVG, on the other hand, not only convincingly\nsimulates the disease trajectory but also manages to preserve\nunrelated visual features from the original medical imag-\ning. Further visual comparisons among different datasets are\npresented in Supplementary.\n5.3. Disease Progression Video Simulation\nTable 3 shows the comparison results between MVG and\nthree image-to-video generation baselines. We did not com-\npare our method with text-to-video generation models like\nStable Diffusion Video [5], as these models do not sup-\nport video generation from an initial medical image. Com-\npared to PixVerse [53], CogVideoX [75], and Pika [52], our\nmethod demonstrates significantly higher clinician prefer-\nence, achieving average win rates of 79%, 70%, and 66%\nfor Cardiomegaly in chest X-ray, diabetic retinopathy, and\nbenign skin lesion disease progression simulations, respec-\ntively. In contrast, for the A/B tests comparing the other\nvideo generation methods, clinicians were generally un-\nable to differentiate between them, with win rates averaging\naround 50% for both A method and B method, indicating no\nclear preference. The results of the clinician preference study\nindicate that MVG is capable of generating high-fidelity\ndisease progression sequences that align well with clinical\ncontext. The disease progression video data generated by\nMVG is available in the Supplementary material.\n5.4. Ablation Study\nDuring the MVG simulation, the region guide masks play\na big role as prior information. Unlike other randomly in-\npainting tasks [40], ROI mask for medical imaging can be\nextracted from clinical reports [8, 39] using domain-specific\nSegment Anything models [30, 41]. It helps keep unrelated\nregions consistent through the progressive changes using\nMVG or baseline models. In order to generate sequential\ndisease imaging data, MVG uses noise strength γ to control\nthe influence from the patient’s clinically reported and ex-\npected treatment regimen at time n. N is used to control\nthe duration of the disease occurrence or treatment regimen.\nMVG allows the user to make such controls over the iterative\nprocess, and running PIEn multiple times can improve the\naccuracy of disease imaging tracking and reduce the likeli-\nhood of missed or misinterpreted changes. We showed abla-\ntion study for MROI in Table 4, γ in Table 5, N in Table 6,\nβ1 and β2 in Table 7. The experimental results demonstrate\nthat MROI is a good controller to balance the alignment\nwith the intended disease features and preservation of patient\nidentity. From these experiments, we also finalize the best\nhyperparameter (N = 10, γ = 0.6, β1 = 0.01, β2 = 0.75)\nfor the main experiment.\n5.5. Compare with Real Longitude Medical Imag-\ning Sequence.\nDue to the spread of COVID, part of the latest released\ndataset contains limited longitudinal data. In order to vali-\ndate the disease sequence modeling that MVG can match real\ndisease trajectories, we conduct an ablation study on gener-\nating Edema disease progression from 10 patients in BrixIA\nCOVID-19 Dataset [62] who’s radiology report showed\nEdema. The input image is the day 1 image, and we use\nMVG to generate future disease progression based on real\nclinical reports for edema. Experimental results show that\nafter the disease state seqeuence simulation of MVG, the\nmean absolute error between MVG’s simulated image and\nreal disease progression image from the same patient is ap-\nproximately 0.0658. Figure 7 shows an example of the\ncomparison.\n5.6. User Study\nTo further assess the quality of our generated disease state\nsequences, we conducted another comprehensive user study\nfrom 35 physicians and radiologists with 14.4 years of expe-\nrience on average to answer a questionnaire on chest X-rays.\nThe questionnaire includes disease classifications on the gen-\nerated and real X-ray images and evaluations of the realism\nof generated disease progression video of Cardiomegaly,\nEdema, and Pleural Effusion. More details of the question-\nnaire and the calculation of the statistics are presented in\n\nMethod\nChest X-ray\nFundus Retinal Image\nSkin Lesion Image\nConf (↑)\nCLIP-I (↑)\nConf (↑)\nCLIP-I (↑)\nConf (↑)\nCLIP-I (↑)\nMVG w/o mask\n0.729\n0.933\n0.163\n0.968\n0.666\n0.852\nMVG with mask\n0.712\n0.978\n0.807\n0.992\n0.453\n0.958\nTable 4. Ablation study for mask, w/o mask guidance comparisons.\nStrength\nConf (↑)\nCLIP-I (↑)\nKID (↓)\n0.1\n0.120\n0.969\n0.0638\n0.2\n0.273\n0.969\n0.0885\n0.4\n0.746\n0.965\n0.1142\n0.6\n0.995\n0.956\n0.1549\n0.8\n0.999\n0.951\n0.1629\nTable 5. Ablation study on Strength γ selection for N = 10.\nStep (N)\nConf (↑)\nCLIP-I (↑)\nKID (↓)\n1\n0.491\n0.965\n0.094\n5\n0.881\n0.963\n0.121\n10\n0.978\n0.962\n0.142\n50\n0.975\n0.962\n0.130\n100\n0.959\n0.962\n0.115\nTable 6. Ablation study on simulation steps N selection with\nγ = 0.5.\nβ1\nβ2\nConf (↑)\nCLIP-I (↑)\nKID (↓)\n0.01\n1.0\n0.954\n0.946\n0.133\n0.01\n0.75\n0.977\n0.948\n0.140\n0.01\n0.5\n0.554\n0.965\n0.090\n0.1\n1.0\n0.960\n0.965\n0.126\n0.1\n0.75\n0.976\n0.962\n0.140\n0.1\n0.5\n0.554\n0.962\n0.089\n0.2\n1.0\n0.963\n0.947\n0.134\n0.2\n0.75\n0.977\n0.964\n0.137\n0.2\n0.5\n0.556\n0.962\n0.089\nTable 7. Ablation study on β1 and β2 selection.\nSupplementary. The participating physicians have agreed\nwith a confidence of 76.2% that MVG simulated disease\nstate progressions on the targeted diseases fit their expec-\ntations. One plausible explanation is due to the nature of\nMVG, the result of running progressive image editing makes\npathological features more evident. The aggregated results\nfrom the user study demonstrate our framework’s ability to\nsimulate disease progression to meet real-world standards.\nMVG\nSVD\nExtrapolation\nStep 1\nStep 2\nStep 4\nStep 10\nFigure 6. Using MVG, SVD, Extrapolation to simulate Edema\nprogression with clinical reports as input prompt.\nInput image\nReal Day 7\nData\nMVG\nSimulated\nConfidence Score\nFigure 7. Evaluating the confidence scores of PIE (stage 1 of MVG)\nprogression trajectories highlights the alignment with realistic pro-\ngression.\n6. Conclusion and Outlook\nIn conclusion, our proposed framework, Medical Video Gen-\neration (MVG) for disease progression simulation, holds\ngreat potential as a tool for medical research and clinical\npractice in simulating disease progression to augment lack-\ning longitude data. Theoretical analysis also shows that the\niterative refining process in the stage 1 of MVG is equivalent\nto gradient descent with an exponentially decayed learning\nrate, and practical experiments on three medical imaging\ndatasets demonstrate that MVG surpasses baseline methods.\nThe clinician human preference study from 30 medical doc-\ntors also shows that the disease progression video sequences\ngenerated by MVG are both real and consistent with the\ncorresponding clinical text descriptions. Despite current lim-\nitations due to the lack of large amounts of longitude medical\nimaging data, our framework has vast potential in restoring\nmissing data from previous electronic health records (EHRs),\nimproving clinical education. Moving forward, we aim to\n\nincorporate more types of medical imaging data with richer\nclinical descriptions into medical video generation, enabling\nour framework to more precise control over disease simula-\ntion through text conditioning.\nReferences\n[1] Diabetic Retinopathy Detection, howpublished= https:\n//www.kaggle.com/c/diabetic-retinopathy-\ndetection, 2015. 5\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3\n[3] Ahmed M Alaa and Mihaela van der Schaar. Attentive state-\nspace modeling of disease progression. Advances in neural\ninformation processing systems, 32, 2019. 2\n[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023. 2\n[5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al.\nStable\nvideo diffusion: Scaling latent video diffusion models to\nlarge datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 6,\n7\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023. 6\n[7] Christian Bluethgen, Pierre Chambon, Jean-Benoit Del-\nbrouck,\nRogier van der Sluijs,\nMałgorzata Połacin,\nJuan Manuel Zambrano Chaves, Tanishq Mathew Abraham,\nShivanshu Purohit, Curtis P Langlotz, and Akshay S Chaud-\nhari. A vision–language foundation model for the generation\nof realistic chest x-ray images. Nature Biomedical Engineer-\ning, pages 1–13, 2024. 2\n[8] William Boag, Tzu-Ming Harry Hsu, Matthew McDermott,\nGabriela Berner, Emily Alesentzer, and Peter Szolovits. Base-\nlines for chest x-ray report generation. In Machine learning\nfor health workshop, pages 126–140. PMLR, 2020. 7\n[9] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 2\n[10] Pierre Chambon, Jean-Benoit Delbrouck, Thomas Sounack,\nShih-Cheng Huang, Zhihong Chen, Maya Varma, Steven QH\nTruong, Chu The Chuong, and Curtis P Langlotz. Chexpert\nplus: Hundreds of thousands of aligned radiology texts, im-\nages and patients. arXiv preprint arXiv:2405.19538, 2024.\n5\n[11] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu. Seine: Short-to-long video diffusion\nmodel for generative transition and prediction. In The Twelfth\nInternational Conference on Learning Representations, 2023.\n2, 5\n[12] Noel Codella, Veronica Rotemberg, Philipp Tschandl,\nM Emre Celebi, Stephen Dusza, David Gutman, Brian Helba,\nAadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al.\nSkin lesion analysis toward melanoma detection 2018: A chal-\nlenge hosted by the international skin imaging collaboration\n(isic). arXiv preprint arXiv:1902.03368, 2019. 5\n[13] Sarah F Cook and Robert R Bies. Disease progression model-\ning: key concepts and recent developments. Current pharma-\ncology reports, 2:221–230, 2016. 1\n[14] Pedro Costa, Adrian Galdran, Maria Ines Meyer, Meindert\nNiemeijer, Michael Abràmoff, Ana Maria Mendonça, and\nAurélio Campilho. End-to-end adversarial retinal image syn-\nthesis. IEEE transactions on medical imaging, 37(3):781–791,\n2017. 2\n[15] Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann,\nIsabelle Ayx, Theano Papavassiliu, Stefan O Schoenberg,\nand Sandy Engelhardt. Investigating data memorization in\n3d latent diffusion models for medical image synthesis. In\nInternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 56–65. Springer, 2023.\n2\n[16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim En-\ntezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,\nAxel Sauer, Frederic Boesel, et al. Scaling rectified flow trans-\nformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning, 2024. 2, 4\n[17] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 22930–22941, 2023. 2\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Communi-\ncations of the ACM, 63(11):139–144, 2020. 2\n[19] Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng\nZhang, Matthew P Lungren, Jianfeng Gao, and Hoifung Poon.\nBiomedjourney: Counterfactual biomedical image genera-\ntion by instruction-learning from multimodal patient journeys.\narXiv preprint arXiv:2310.10765, 2023. 2\n[20] Tianyu Han, Jakob Nikolas Kather, Federico Pedersoli,\nMarkus Zimmermann, Sebastian Keil, Maximilian Schulze-\nHagen, Marc Terwoelbeck, Peter Isfort, Christoph Haarburger,\nFabian Kiessling, et al. Image prediction of disease progres-\nsion for osteoarthritis by style-based manifold extrapolation.\nNature Machine Intelligence, pages 1–11, 2022. 6\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840–6851, 2020. 2\n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4700–4708, 2017. 6\n\n[23] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Sil-\nviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad\nHaghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:\nA large chest radiograph dataset with uncertainty labels and\nexpert comparison. In Proceedings of the AAAI conference\non artificial intelligence, pages 590–597, 2019. 5\n[24] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-ying\nDeng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-\nidentified publicly available database of chest radiographs\nwith free-text reports. Scientific data, 6(1):317, 2019. 5\n[25] Euijin Jung, Miguel Luna, and Sang Hyun Park. Conditional\ngan with 3d discriminator for mri generation of alzheimer’s\ndisease progression. Pattern Recognition, 133:109061, 2023.\n2\n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. arXiv preprint arXiv:2206.00364, 2022. 2\n[27] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein\nHeidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu,\nand Dorit Merhof. Diffusion models in medical imaging: A\ncomprehensive survey. Medical Image Analysis, 88:102846,\n2023. 2\n[28] Firas\nKhader,\nGustav\nMüller-Franzes,\nSoroosh\nTayebi\nArasteh,\nTianyu\nHan,\nChristoph\nHaarburger,\nMaximilian Schulze-Hagen, Philipp Schad, Sandy Engel-\nhardt, Bettina Baeßler, Sebastian Foersch, et al. Denoising\ndiffusion probabilistic models for 3d medical image\ngeneration. Scientific Reports, 13(1):7303, 2023. 2\n[29] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 7\n[31] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu\nChen. Anyv2v: A plug-and-play framework for any video-to-\nvideo editing tasks. arXiv preprint arXiv:2403.14468, 2024.\n3\n[32] Nicholas R Kurtansky, Brian M D’Alessandro, Maura C\nGillis, Brigid Betz-Stablein, Sara E Cerminara, Rafael Gar-\ncia, Marcela Alves Girundi, Elisabeth Victoria Goessinger,\nPhilippe Gottfrois, Pascale Guitera, et al. The slice-3d dataset:\n400,000 skin lesion image crops extracted from 3d tbp for\nskin cancer detection. Scientific Data, 11(1):884, 2024. 5\n[33] Daeun Kyung, Junu Kim, Tackeun Kim, and Edward Choi.\nTowards predicting temporal changes in a patient’s chest x-ray\nimages based on electronic health records. arXiv preprint\narXiv:2409.07012, 2024. 2\n[34] Garam Lee, Kwangsik Nho, Byungkon Kang, Kyung-Ah\nSohn, and Dokyoon Kim. Predicting alzheimer’s disease pro-\ngression using multi-modal deep learning approach. Scientific\nreports, 9(1):1952, 2019. 1\n[35] Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye.\nLlm-cxr: Instruction-finetuned llm for cxr image understand-\ning and generation. arXiv preprint arXiv:2305.11490, 2023.\n2\n[36] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Y Feng,\nWuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, and Yixuan\nYuan. Endora: Video generation models as endoscopy simu-\nlators. arXiv preprint arXiv:2403.11050, 2024. 3\n[37] Linyuan Li, Jianing Qiu, Anujit Saha, Lin Li, Poyuan Li,\nMengxian He, Ziyu Guo, and Wu Yuan. Artificial intelligence\nfor biomedical video generation, 2024. 1\n[38] Yu-Ying Liu, Shuang Li, Fuxin Li, Le Song, and James M\nRehg. Efficient learning of continuous-time hidden markov\nmodels for disease progression. Advances in neural informa-\ntion processing systems, 28, 2015. 1, 2\n[39] Justin Lovelace and Bobak Mortazavi. Learning to generate\nclinically coherent chest x-ray reports. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020,\npages 1235–1243, 2020. 7\n[40] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11461–11471, 2022. 7\n[41] Jun Ma and Bo Wang. Segment anything in medical images.\narXiv preprint arXiv:2304.12306, 2023. 7\n[42] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and\nBo Wang. Segment anything in medical images. Nature\nCommunications, 15(1):654, 2024. 4\n[43] Ali Madani, Mehdi Moradi, Alexandros Karargyris, and Tan-\nveer Syeda-Mahmood. Chest x-ray generation and data aug-\nmentation for cardiovascular abnormality classification. In\nMedical imaging 2018: Image processing, pages 415–420.\nSPIE, 2018. 2\n[44] Midjourney.\nMidjourney – home.\nhttps://www.\nmidjourney.com/home, 2024. Accessed: 2024-07-30.\n2\n[45] Midjourney. Video generation models as world simulators.\nhttps://www.midjourney.com/home, 2024. Ac-\ncessed: 2024-07-30. 2\n[46] Peter G Mikhael, Jeremy Wohlwend, Adam Yala, Ludvig\nKarstens, Justin Xiang, Angelo K Takigami, Patrick P Bour-\ngouin, PuiYee Chan, Sofiane Mrah, Wael Amayri, et al. Sybil:\na validated deep learning model to predict future lung can-\ncer risk from a single low-dose chest computed tomography.\nJournal of Clinical Oncology, pages JCO–22, 2023. 2\n[47] Gustav Müller-Franzes, Jan Moritz Niehues, Firas Khader,\nSoroosh Tayebi Arasteh, Christoph Haarburger, Christiane\nKuhl, Tianci Wang, Tianyu Han, Teresa Nolte, Sven\nNebelung, et al. A multimodal comparison of latent denois-\ning diffusion probabilistic models and generative adversarial\nnetworks for medical image synthesis. Scientific Reports, 13\n(1):12098, 2023. 2\n[48] Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su\nRuan, Qian Wang, and Dinggang Shen.\nMedical im-\nage synthesis with context-aware generative adversarial net-\nworks. In Medical Image Computing and Computer Assisted\nIntervention- MICCAI 2017: 20th International Conference,\nQuebec City, QC, Canada, September 11-13, 2017, Proceed-\nings, Part III 20, pages 417–425. Springer, 2017. 2\n\n[49] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing\nimplicit assumptions in text-to-image diffusion models. arXiv\npreprint arXiv:2303.08084, 2023. 2\n[50] Kai Packhäuser, Lukas Folle, Florian Thamm, and Andreas\nMaier. Generation of anonymous chest radiographs using\nlatent diffusion models for training thoracic abnormality clas-\nsification systems. In 2023 IEEE 20th International Sympo-\nsium on Biomedical Imaging (ISBI), pages 1–5. IEEE, 2023.\n2\n[51] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. arXiv preprint arXiv:2302.03027, 2023. 2\n[52] Pika. Pika art – home. https://pika.art/home, 2024.\nAccessed: 2024-07-30. 2, 6, 7\n[53] PixVerse. Pixverse. https://app.pixverse.ai/,\n2024. Accessed: 2024-11-10. 6, 7\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2, 3, 5\n[55] Daniele Ravi, Daniel C Alexander, Neil P Oxtoby, and\nAlzheimer’s Disease Neuroimaging Initiative. Degenerative\nadversarial neuroimage nets: generating images that mimic\ndisease progression. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages\n164–172. Springer, 2019. 6\n[56] Daniele Ravi, Stefano B Blumberg, Silvia Ingala, Frederik\nBarkhof, Daniel C Alexander, Neil P Oxtoby, Alzheimer’s\nDisease Neuroimaging Initiative, et al. Degenerative adversar-\nial neuroimage nets for brain scan simulations: Application\nin ageing and dementia. Medical Image Analysis, 75:102257,\n2022. 2\n[57] Nathan Raw. Stable diffusion videos. https://github.\ncom/nateraw/stable-diffusion-videos, 2023.\n6\n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684–10695, 2022. 2, 3, 4\n[59] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. arXiv preprint arXiv:2208.12242, 2022. 5\n[60] Peter Schulam and Raman Arora. Disease trajectory maps.\nAdvances in neural information processing systems, 29, 2016.\n2\n[61] Kristen A Severson, Lana M Chahine, Luba Smolensky, Ken-\nney Ng, Jianying Hu, and Soumya Ghosh.\nPersonalized\ninput-output hidden markov models for disease progression\nmodeling. In Machine Learning for Healthcare Conference,\npages 309–330. PMLR, 2020. 1\n[62] Alberto Signoroni, Mattia Savardi, Sergio Benini, Nicola\nAdami, Riccardo Leonardi, Paolo Gibellini, Filippo Vaccher,\nMarco Ravanelli, Andrea Borghesi, Roberto Maroldi, et al.\nBs-net: Learning covid-19 pneumonia severity on a large\nchest x-ray dataset. Medical Image Analysis, 71:102046,\n2021. 7\n[63] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2\n[64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020. 2\n[65] Kamile Stankeviciute, Ahmed M Alaa, and Mihaela van der\nSchaar. Conformal time-series forecasting. Advances in\nNeural Information Processing Systems, 34:6216–6228, 2021.\n2\n[66] Rui Sun, Yumin Zhang, Tejal Shah, Jiahao Sun, Shuoying\nZhang, Wenqi Li, Haoran Duan, Bo Wei, and Rajiv Ran-\njan. From sora what we can see: A survey of text-to-video\ngeneration. arXiv preprint arXiv:2405.10674, 2024. 2\n[67] Weixiang Sun, Xiaocao You, Ruizhe Zheng, Zhengqing Yuan,\nXiang Li, Lifang He, Quanzheng Li, and Lichao Sun. Bora:\nBiomedical generalist video generation model. arXiv preprint\narXiv:2407.08944, 2024. 3\n[68] Chenyu Tang, Wentian Yi, Edoardo Occhipinti, Yanning Dai,\nShuo Gao, and Luigi G Occhipinti. A roadmap for the de-\nvelopment of human body digital twins. Nature Reviews\nElectrical Engineering, 1(3):199–207, 2024. 1\n[69] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The\nham10000 dataset, a large collection of multi-source dermato-\nscopic images of common pigmented skin lesions. Scientific\ndata, 5(1):1–9, 2018. 5\n[70] Alexandre Vallée. Envisioning the future of personalized\nmedicine: Role and realities of digital twins. Journal of\nMedical Internet Research, 26:e50204, 2024. 1\n[71] Xiang Wang, David Sontag, and Fei Wang. Unsupervised\nlearning of disease progression models. In Proceedings of the\n20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 85–94, 2014. 2\n[72] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 2\n[73] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 6\n[74] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:\nLearning and evaluating human preferences for text-to-image\ngeneration. Advances in Neural Information Processing Sys-\ntems, 36, 2024. 6\n[75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 6, 7\n\n[76] Xin Yi, Ekta Walia, and Paul Babyn. Generative adversar-\nial network in medical imaging: A review. Medical image\nanalysis, 58:101552, 2019. 2\n[77] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang.\nLarge-scale robust deep auc maximization: A new surrogate\nloss and empirical studies on medical image classification. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3040–3049, 2021. 6\n[78] Zizhao Zhang, Lin Yang, and Yefeng Zheng. Translating\nand segmenting multimodal medical volumes with cycle-and\nshape-consistency generative adversarial network. In Proceed-\nings of the IEEE conference on computer vision and pattern\nRecognition, pages 9242–9251, 2018. 2",
    "pdf_filename": "Medical_Video_Generation_for_Disease_Progression_Simulation.pdf"
}