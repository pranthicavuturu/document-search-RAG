{
    "title": "Efficient Training in Multi-Agent Reinforcement",
    "abstract": "Self-organizing systems consists of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on re- inforcement learning to enable agents to gain the skills needed for task completion, such as in thebox-pushingenvironment. However,whenagentspushfromopposingdirectionsduringexplo- ration,theytendtoexertequalandoppositeforcesonthebox,resultinginminimaldisplacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and faciliate coordinate and reduce force conflicts among agents, thus enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents’ collaborative effectiveness. 1 Introduction Self-organizing systems can address scalability, bottlenecks, and reliability issues that are common in systems with a central controller [1]. However, in practice, agents that learn purely by themselves without the help of a central entity can result in the agent having unbalanced data [2]. Agents can combat the issue through implicit observations or through direct communication ([3], [4], [5]). To add the ability to communicate would require additional overhead to the model. The solution proposed in this paper requires no communication or significant overhead. Cooperation and coordination in reinforcement learning has been a difficult yet crucial problem to solve. There are many studies that aim to find an efficient solution that will allow agents in a multi-agent environment to coordinate. For example, Yexin et al. proposed a contextual cooperative reinforcement learning model to address the lack of cooperations between couriers while also consid- ering system context [6]. Weirong et al. proposed a cooperative reinforcement learning algorithm for distributedeconomicdispatchinmicrogrids[7]. Similartopreviousstudies,thispaperaimstoaddress the agent coordination issue in the box-pushing game. Intheboxpushinggame,agentscollaboratetopushaboxtowardsthegoalwhileavoidingobstacles. The minimalist agents have a small set of possible actions. They also choose their action based on limitedobservationcapacities. Theagentscanonlyobservetheenvironmentusingaboxsensorplaced inthemiddleoftheboxthatisbeingpushed. Thesensorgiveslimitedenvironmentalinformationsuch as the direction of the goal and nearby obstacles (see Sec. 3.2). The agents are unable to sense one another – all agents are unaware of one another. They can only act through their learned experience and their observation of the current environment. Because of this fact, agents often push against one other, especially towards the beginning of training. This leads to undesirable exploration. ∗Correspondingauthor 1 4202 voN 91 ]IA.sc[ 1v64221.1142:viXra",
    "body": "Efficient Training in Multi-Agent Reinforcement\nLearning: A Communication-Free Framework for\nthe Box-Pushing Problem\nDavid Ge Hao Ji∗\nDepartment of Computer Science Center for Advanced Research\nUniversity of California, Berkeley Computing\n2301 Durant Ave, Berkeley, CA 94704 University of Southern California\nlwg0320@berkeley.edu 3434 South Grand Avenue, Building\nCAL, Los Angeles, CA 90089\nhaoji@usc.edu\nAbstract\nSelf-organizing systems consists of autonomous agents that can perform complex tasks and\nadapt to dynamic environments without a central controller. Prior research often relies on re-\ninforcement learning to enable agents to gain the skills needed for task completion, such as in\nthebox-pushingenvironment. However,whenagentspushfromopposingdirectionsduringexplo-\nration,theytendtoexertequalandoppositeforcesonthebox,resultinginminimaldisplacement\nand inefficient training. This paper proposes a model called Shared Pool of Information (SPI),\nwhich enables information to be accessible to all agents and faciliate coordinate and reduce force\nconflicts among agents, thus enhancing exploration efficiency. Through computer simulations, we\ndemonstrate that SPI not only expedites the training process but also requires fewer steps per\nepisode, significantly improving the agents’ collaborative effectiveness.\n1 Introduction\nSelf-organizing systems can address scalability, bottlenecks, and reliability issues that are common in\nsystems with a central controller [1]. However, in practice, agents that learn purely by themselves\nwithout the help of a central entity can result in the agent having unbalanced data [2]. Agents can\ncombat the issue through implicit observations or through direct communication ([3], [4], [5]). To add\nthe ability to communicate would require additional overhead to the model. The solution proposed in\nthis paper requires no communication or significant overhead.\nCooperation and coordination in reinforcement learning has been a difficult yet crucial problem\nto solve. There are many studies that aim to find an efficient solution that will allow agents in a\nmulti-agent environment to coordinate. For example, Yexin et al. proposed a contextual cooperative\nreinforcement learning model to address the lack of cooperations between couriers while also consid-\nering system context [6]. Weirong et al. proposed a cooperative reinforcement learning algorithm for\ndistributedeconomicdispatchinmicrogrids[7]. Similartopreviousstudies,thispaperaimstoaddress\nthe agent coordination issue in the box-pushing game.\nIntheboxpushinggame,agentscollaboratetopushaboxtowardsthegoalwhileavoidingobstacles.\nThe minimalist agents have a small set of possible actions. They also choose their action based on\nlimitedobservationcapacities. Theagentscanonlyobservetheenvironmentusingaboxsensorplaced\ninthemiddleoftheboxthatisbeingpushed. Thesensorgiveslimitedenvironmentalinformationsuch\nas the direction of the goal and nearby obstacles (see Sec. 3.2). The agents are unable to sense one\nanother – all agents are unaware of one another. They can only act through their learned experience\nand their observation of the current environment. Because of this fact, agents often push against one\nother, especially towards the beginning of training. This leads to undesirable exploration.\n∗Correspondingauthor\n1\n4202\nvoN\n91\n]IA.sc[\n1v64221.1142:viXra\nIn this paper, we introduce the idea of a shared pool of information (SPI), which provides addi-\ntional information generated and given to all agents at initialization. This idea is similar to common\nknowledge, in which agents use deduction as the basis for their action [8]. SPI aims to make up for\na lack of communication between the agents by providing a common framework for all agents to base\ntheir exploration on. To generate an SPI that would enable such collaboration, it must pass a fitness\ntest, which consists of an origin avoidance test and an angular spread test. The goal of the tests is to\nmaximize total box displacement and the box’s range of motion (see Sec 3.7). We tested the efficiency\nof training in various challenging environments to observe the difference between SPI and random\nexploration.\nUsing the box-pushing game, this paper demonstrates how SPI is able to improve the ability\nfor agents to coordinate during training. Furthermore, it adds minimal computation and overhead.\nIn the experiment, SPI reduces instances in which agents cancel out one another’s actions, making\neach agents’ exploration more meaningful. This idea can be applied and used in tandem with other\nalgorithms to help address the issue of coordination and cooperation for multi-agent problems in\nreinforcement learning.\n2 Related Works\n2.1 The Box pushing problem\nThe box pushing problem has been experimented on with single-agent and multi-agent RL. Studies\nfind that both algorithms are equally as effective in simple environments. However, single agent RL\nprovestobemoreefficientandconsistentincomplexenvironments([9],[10]). Inrouteplanning,Ezra,\nJos´e, and Gregorio proposed a new path planning algorithm that allows for the minimum number of\nreconfigurations, which can aid robots in planning and optimizing routes for the box pushing problem\n[11]. For the box pushing problem, researchers also devised RL algorithms that are more efficient\nthan the standard q-learning algorithm used for multi-agent RL ([12], [13]). Toshiyuki, Kazuhiro, and\nKazuaki proposed a technique that aims to organize what agents learn into clusters that represent\nthe environment or situations that they might encounter. Each cluster contains its own set of rules\nto dictate what actions are optimal for agents in that specific scenario/environment [13]. Kao-Shing,\nJ.L., and Wei-Han proposed adaptive state aggregation q-learning that aids coordination by splitting\nthe state space into chunks [12]. Yang and Clarence used genetic algorithms alongside reinforcement\nlearning, with an arbitrator evaluating the output of both to determine the agent’s actions [14].\n2.2 Multiagent Communication\nMany multiagent reinforcement learning tasks require agents to coordinate in order to accomplish. In\nMarkovDecisionProcesseach”action”isajointactionforallagentsandtherewardistheircombined\nreward. However, this creates a large action space [15]. Local optimization for the different agents\nvia reward and value sharing ([16], [17]), direct policy search [18], and Factored MDPs ([19], [20])\nare approaches to address this problem. Agents can also observe one another’s actions and efforts\nimplicitly rather than communicating directly to determine their own actions ([21], [22]). However,\nimplicitcommunicationprotocolsforcomplexcoordinationproblemstypicallyrequiremultipletraining\nsessions for different components, severely increasing the training time and training complexity ([23],\n[24]). Sheng et al. developed coordinate graphs that are automatically and dynamically created for\nthe agents to share information and decide on actions together [25].\n2.3 Centralized vs Decentralized Learning\nCentralizedtrainingfordecentralizedexecution(CLDE)isanapproachtoaddresstheissueofcoordi-\nnation [26]. Under the CLDE framework, Wenzhe et al. attempts to address the issue of coordination\nusing f-MAT, which enables efficient collaboration through graph modeling [27]. Centralized learning\nis useful to give agents information during training that they normally wouldn’t have access to during\nexecution [28]. There are two main ways of accomplishing this - a centralized critic for all agents or a\ncentralized critic for each agent ([29], [30]). Daewoo et al. proposed SchedNet where the centralized\ncritic gives feedback which consists of message encoders, action selectors, and weight generators to\neach agent [31]. Gang proposed Centralized Training and Exploration with Decentralized Execution\n2\nvia policy Distillation as an example of a critic for each agent [32]. In addition, Alexander et al.\nproposed a Bayesian mode to further optimize the exploration phase of CLDE training. It weighs the\ncosts of exploration against its expected benefit gained from exploring [33]. Decentralized learning is\ngenerally more computationally demanding or can encounter problems. But, in the real world, it is\noften impractical for a centralized model to access all agent’s actions and observations [34]. In DL,\nnodes communicate and share models with their immediate neighbors, converging to a global model\nthrough local interactions [35]. CHOCO-SGD proposed by Anastasia et al. allows the communication\nand gradient computation steps to run in parallel [36]. Although less research has been done on DL\ncompared CL, Akash et al. created the DecentralizePy, hoping that others can use it to develop new\nDL systems [37].\n3 Methodology\nIn a simulated environment designed for agents to collaborate on the box-pushing task, agents rely on\nan implicit coordination framework called the Shared Pool of Information (SPI), which consists of a\nmap and key. This framework provides agents with a shared reference that fosters aligned movements\nand minimizes counteractive actions. It aims to address the inefficiencies of random exploration in\nmulti-agent reinforcement learning.\n3.1 Environment\nFigure 1: Illustration of the box environment.\nFig. 1 shows the environment, which consists of patches, obstacles, agents, boxes, and the goal.\nTheenvironmentisbuiltusingthePygamelibrary. Thegraphicalillustrationdemonstratesapossible\nstateofthebox-pushingtaskinwhichagents(greenboxes)pushthebox(blackbox). Agentsnavigate\naround obstacles (represented by red circle) and walls (represented by black lines) to reach the goal\n(represented by blue diamond). Success occurs when the outer perimeter of the box collides with any\narea of the goal and failure happens when the outer perimeter of the box touches the obstacle or any\nwall. Failure can also happen when the box is unable to reach the goal in under 300 steps.\n3\nFigure 2: Illustration of the action space.\nFigure 3: Illustration of the state space.\n3.2 Action Space and State Space\nAt each time step, each agent chooses one region within the six available regions in the box neigh-\nborhood to apply force (see Fig. 2), directing the box’s movement accordingly. This information is\nsubsequently converted into the action space A, represented as follows:\nA={a ,a ,a ,a ,a ,a }\n1 2 3 4 5 6\nIn the neighborhood surrounding the box, six distinct regions are represented as a ,a ,...,a (see\n1 2 6\nFig. 2). For each region, a value of 1 indicates that the agent has selected that region, while a value\nof 0 indicates that it has not. Each agent chooses exactly one region; therefore, {1,1,0,0,0,0} and\n{0,0,0,0,0,0}arenotvalidactionspaces. Infig.2,theagentselectsregion1oftheboxneighborhood.\nTherefore, the action space A is defined as {1,0,0,0,0,0}.\nAt each time step, the box’s sensor detects relevant environmental information within a 150-pixel\nradius as well as the angle between the box and the goal. This information is then converted into the\nstate space S, represented as follows:\nS ={s ,s ,s ,s ,s ,s ,s ,s ,s }\n1 2 3 4 5 6 7 8 9\ns ,s ,...,s representtheeightoctantsinfig.3. Avalueof1indicatesthepresenceofanobstacle\n1 2 8\nor wall in the octant, whereas a value of 0 indicates its absence. s represents the angle between\n9\nthe box and the goal, scaled into the range [−1,1]. In fig. 3, an obstacle is present in states s and\n3\ns , and walls are present in states s and s . Consequently, s ,s ,s , and s = 0, while s ,s ,s ,\n6 4 5 1 2 7 8 3 4 5\nand s = 1. The angle θ = 30◦ translates to s = θ−180 = −0.83. This gives the state space\n6 9 180\nS ={0,0,1,1,1,1,0,0,−0.83}.\n3.3 Reward\nDistance reward(R )istherewardformovingtheboxclosertothegoal. D andD represent\ndis old new\nthe distance in pixels between the box’s sensor and the goal at the previous and current timesteps,\nrespectively.\nR =(D −D )·2.5\ndis old new\nRotation reward (R ) is the reward designed to discourage excessive rotation of the box. α\nrot old\nand α represent the angles between the box’s x-axis and the goal at the previous and current\nnew\ntimesteps, respectively.\nR =cos(α −α )−0.98\nrot new old\nCollision reward, (R ), is the negative reward applied if the box collides with an obstacle or\ncol\nwall.\n(cid:40)\n0 if there is no collision\nR =\ncol\n−900 if there is a collision\n4\nGoal reward, (R ), is the positive reward assigned when the box successfully reaches the goal.\ngoal\n(cid:40)\n900 if goal is reached\nR =\ngoal\n0 if goal is not reached\nThetotalrewardisformulatedasaweightedsumofdistance, rotation, collision, andgoalrewards.\nAll reward weights are set to fixed values, with the distance reward assigned the highest weight. The\nvalues are defined in [38].\nR =w R +w R +w R +w R\nTotal 1 dis 2 rot 3 col 4 goal\n3.4 Speed Factor\nSpeed factor controls how much force agents are able to exert. This impacts how much the agents can\nboth displace and rotate the box. In this experiment, the speed factor is reduced to one-third and\none-half of its original value, effectively limiting the agents’ pushing strength. With reduced force,\nindividual agents can no longer move the box as efficiently on their own. Cooperation becomes even\nmore critical as failure to do so results in slower progress. This shift in dynamics is designed to test\nhow the agents adapt to a more challenging environment, where collaborative effort is essential to\nachieving the desired outcome.\n3.5 Implicit Coordination Through Shared Pool of Information\nThe primary goal of the introduction of a shared pool of information (SPI) is to encourage implied\ncoordination – the ability for agents to work constructively without specifically needing to exchange\ninformation between one another. In order to accomplish this, the scenario must have the following\nkey characteristics:\n1. All agents have access to the same shared information at instantiation.\n2. Agents can only observe or read from the shared information.\n3. The shared information cannot be changed by the agents.\n4. The shared information is simple - needs little to no training and should have a small time and\nspace complexity.\nThe following ruleset ensures that agents, at instantiation, must be identical (1) and cannot com-\nmunicate with one another in any way by any means (2, 3). The addition of SPI is efficient and\nlightweight – adds no significant overhead (4).\nThe inclusion of SPI reduces the likelihood that agents push the box from different sides, resulting\nin minimal movement. SPI should act like a silent guide that allows the agents to better align their\nefforts in order to improve the speed and efficiency of training.\n3.6 Proposed Shared Pool of Information\nInsteadofagentsrandomlyexploring,theagentswouldnowpseudo-randomlyexplorebasedonshared\ninformation. This is to improve the agents’ cooperation during training. The proposed SPI has two\nportions, map and key. The map refers to an idealized distribution outlining the potential outcomes\nand distribution of those outcomes when all the agents push the box simultaneously. In other words,\nwe want to simulate the movement of the box under the assumption that the agents act as one or\nare controlled by a centralized entity. It is important to note that simply having such a distribution\nand enacting on its implementation would change the experiment from using multiagent reinforced\nlearning(RL)approachtoasinglecentralizedagentRLapproachbydefinition. Therefore,itiscrucial\nto include a key, in which all agents can use to decipher the distribution, allowing for cooperation\nwithout the need for communication between agents.\nThe key must be able toberandomized andthe randomizationmusthave sufficient range– larger\nthan the number of agents in the experiment. The distribution of the randomized key must also be\nuniform to ensure no agents will have no bias to any distribution outlined in the map. Although there\n5\nFigure 4: This figure illustrates the procedure by which agents select actions during\ntraining.\nare numerous possible implementations of the key, in the experiment, it is simply generated with a\nrandom number generator using the random library in Python. The key will be generated every step\nof training and all agents will be able to observe and read from it.\nThemapofSPIcontainsalargenumberofprobabilitydistributionlists(PDL)–theexactnumber\nis discussed in section 4.1.6. As there are six different actions the agents can take, each PDL will have\nsix entries, representing the probability that an agent should choose each action. The summation of\nthe entries of each PDL should be exactly 1. Examples of valid and invalid PDLs are in Table 1.\nDuring exploration, agents use the map by first reading the generated key. Then, the agent must\nfind the PDL associated with the key. Given the PDL, the agent will subsequently use it to determine\nwhat action to take. Fig. 4 is a drawn example of the process.\nPDL Type Examples\nValid PDL (cid:2)1,1,1,1,1,1(cid:3) , [1,0,0,0,0,0], (cid:2)1,1,1,1, 1 , 1 (cid:3)\n6 6 6 6 6 6 6 6 4 4 12 12\nInvalid PDL\n(cid:2)1,1,1,1,1,1(cid:3)\n,\n(cid:2)1,1,1,1,1,1(cid:3)\n,\n(cid:2)1,1,1,1(cid:3)\n7 7 7 7 7 7 5 5 5 5 5 5 4 4 4 4\nTable 1: Examples of Valid and Invalid PDL\n3.7 Fitness Test using BMD\nAt each step, agents will simultaneously push the box after determining what action to take. In one\nsimulation, the displacement of the box in both the x and y axis in which the box is pushed from\nits starting point is recorded. Each simulation is exactly one step long. After a significant amount\nof simulations, we can plot out the resulting displacement distribution of the box. We call that box\nmovement distribution (BMD). Figure 4 shows an example of such a graph. When using the map,\nagents should be able to collaborate efficiently. To be considered effective, the map needs to pass a\nfitness test, which consists of an origin avoidance test and a uniformity of angular spread test. We\n6\nmustensurethatBMDminimizestheamountofconflictingforcesexertedbytheagentswhileallowing\nagents to push the box in all directions with equal likelihoods.\nFigure 4: Graph\n3.7.1 Origin Avoidance\nThe first component of the fitness test evaluates origin avoidance, which measures the percentage\nof time the agents can displace the box a sufficient distance. The origin avoidance fitness score is\ncalculated as follows:\n(cid:18) (cid:19)\nnear origin count\norigin avoidance fitness=1−\ntotal count\nIn the tests, the box is considered near the origin if its total displacement is three times less than\nits maximum potential displacement. This ensures the agents cooperate so that the box is sufficiently\ndisplaced from its original position. The count of instances where the box remains near the origin is\nrecorded as near origin count, while total count denotes the overall count of test instances. A low\noriginavoidancefitnessscorewouldindicatethattheagentsconstantlyexertconflictingforces, unable\ntoconsistentlydisplacetheboxfarfromtheorigin. Ontheotherhand, ahighoriginavoidancefitness\nscore would indicate that the agents are generally able to collaborate, as they rarely exert conflicting\nforces.\n3.7.2 Uniformity of Angular Spread\nThe second component of the fitness test evaluates the uniformity of the box’s angular spread, which\ntests if the box can be pushed in any direction with equal likelihood. To calculate the uniformity\nof angular spread, the angle of each box from the origin is computed. Then, we define bin edges\nfor n equally sized bins, each representing 360 degrees, where n is the number of agents. Next, we\nn\ncompute the histogram of the angles (see fig. 7) and utilize the mean and standard deviation of the\nhistogram to determine the coefficient of variation (CV). A lower CV indicates higher uniformity in\nangular distribution. Therefore, we assess the uniformity of the angular spread by inverting the CV\nand normalizing it to a [0,1] range, as shown in the following equation:\n(cid:18) (cid:19)\n1−CV\nuniformity of angular spread=\nCV\nA low fitness score indicates insufficient diversification in the box’s movement directions, which\ncan hinder the training process. Conversely, a higher score reflects that, throughout the simulations,\nthe agents have successfully aligned themselves to enable multi-directional pushing of the box. Such\nflexibility in angular spread is essential for maximizing the range of the box’s potential movement.\n4 Results and Discussion\n4.1 Generating PDL\nThere are 6 potential actions for each agent (see fig. 2). In terms of box displacement, action 2 and\n3 along with action 5 and 6 are effectively the same. For calculating the ideal distribution, we can\nsimplify the state actions to the 4 actions shown. The instruction to generate one PDL is as follows.\n4.1.1 Generating the Initial Values\nFirst generates a random float, value1, from a uniform distribution in range [0,cap]. This will serve\nas the 0th element in the first PDL:\nvalue1∼U(0,cap)\nThe remaining sum represents the total the remaining three elements need to sum up to:\nremaining sum=1−value1\n7\nThen, three additional random values are generated.\nvalue2∼U(0,1), value3∼U(0,1), value4∼U(0,1)\nThethreegeneratedvaluesarenormalizedtosumto1. Then,thesevaluesarescaledbyremaining sum\nto ensure that the total sum of all three elements equals 1:\nvalue2 = value2 value2 =value2 ×remaining sum\nnorm value2+value3+value4 scaled norm\nvalue3 = value3 value3 =value3 ×remaining sum\nnorm value2+value3+value4 scaled norm\nvalue4 = value4 value4 =value4 ×remaining sum\nnorm value2+value3+value4 scaled norm\nThecompletelistofvalues,all values,iscreatedbyconcatenatingvalue1withthescaledvalues:\nall values=[value1,value2 ,value3 ,value4 ]\nscaled scaled scaled\n4.1.2 Condition Check for Validity\nThe difference between value1 and value3 must be minimum margin value. If this condition is\nscaled\nnot met, the generation process restarts:\n|value −value3 |≥margin\n1 scaled\n4.1.3 Generating the PDL 4x4 Matrix\nThe next step is to create a 4×4 matrix, PDL 4x4. Each row i is a cyclic permutation (shift) of\nall values by i positions:\n \nvalue value2 value3 value4\n1 scaled scaled scaled\n \n \nvalue4 value value2 value3 \n scaled 1 scaled scaled\nPDL 4×4 = \n \nvalue3 scaled value4 scaled value 1 value2 scaled\n \n \nvalue2 value3 value4 value\nscaled scaled scaled 1\nIn general, each element in row i and column j of PDL is defined as:\n4×4\nPDL [i,j]=value\n4×4 ((j+i)mod4)+1\n4.1.4 Generating the PDL 4x6 Matrix\nA4×6matrix,PDL 4x6,iscreatedusingspecifictransformationsof PDL 4x4. Thecolumnsof PDL 4x6\nare populated as follows:\n• The 0th column of PDL 4x6 matches the 0th column of PDL 4x4:\nPDL 4x6[i,0]=PDL 4x4[i,0].\n• The 1st and 2nd columns are each half of the 1st column of PDL 4x4:\n1 1\nPDL 4x6[i,1]= ·PDL 4x4[i,1], PDL 4x6[i,2]= ·PDL 4x4[i,1].\n2 2\n• The 3rd column matches the 2nd column of PDL 4x4:\nPDL 4x6[i,3]=PDL 4x4[i,2].\n• The 4th and 5th columns are each half of the 3rd column of PDL 4x4:\n1 1\nPDL 4x6[i,4]= ·PDL 4x4[i,3], PDL 4x6[i,5]= ·PDL 4x4[i,3].\n2 2\n8\n4.1.5 Normalizing Rows in PDL 4x6\nEach row in PDL 4x6 is normalized to ensure the values sum to 1.\n(cid:88)\nrow sums[i]= PDL 4x6[i,j].\nj\nPDL 4x6[i,j]\nPDL 4x6[i,j]= .\nrow sums[i]\nFour valid PDLs are generated: PDL 4x6\n4.1.6 Number of PDLs Generated\nWhen there are 15 agents, the angular avoidance fitness score increases with more PDL distributions.\nAfterabout250PDLs,additionalPDLsseemtohavelittletonoeffectontheangularavoidancefitness\nscore. This trend also holds true given any margin and cap values (see Fig. 5). In the experiment, we\nwill use 4000 PDLs to ensure a high angular avoidance fitness score.\nFigure 5: The angular avoidance fitness Figure 6: The angular avoidance fitness\nscore is tested for different margin and cap score is calculated for different numbers of\nvalues. The lighter lines represent a high PDLsusingamarginvalueof0.3andacap\nmargin and cap value. The darker lines value of 0.1.\nrepresent a low margin and cap value.\n4.1.7 Generating Cap and Margin Values\nIn the experiment, 15 agents and 4000 PDLs are used. The optimal margin value and cap value for\nthose parameters are tested. 100000 total simulations are run and the results are plotted in a line\ngraph.\nAs margin value increases, the origin avoidance fitness score also increases. A smaller cap value\nleads to a higher origin avoidance fitness score. However, with a higher margin value (ie. 0.5), the\nsignificance of cap value decreases.\nUntilamarginvalueof0.30,byincreasingmarginvalue,angularspreadfitnessscorealsoincreases.\nAfterwards, an increase in margin value drastically decreases the angular spread fitness score. Cap\nvalue does not have a meaningful impact on angular spread fitness score.\nThe cap chosen for the experiment is 0.1 while the margin value is 0.3. This set of values is chosen\nbecause it maximizes angular spread fitness score while maintaining a high origin avoidance fitness\nscore. There exist many values of margin value and fitness value in which both origin avoidance and\nangularspreadtestaresufficientlypassed. Itisuptotheexperimentertodetermineiforiginavoidance\nis more valuable or angular spread.\n4.1.8 Fitness Test - New vs Random Explore\nThe old method of random exploration has a origin avoidance score of 0.215 and an angular spread\nscore of 0.714. The new method has a origin score of 0.899 and an angular spread score of 0.911.\n9\nBy comparing the differences between both methods using the box movement graph and histogram\nof point directions, it is evident that by using SPI, the box not only has access to a wider range of\nmovement, but also can explore all directions more uniformly (see Fig. 7).\nSPI Random Exploration\nKernel density estimate (KDE) plot of box movement from origin.\nHistogram of box displacement.\nHistogram of directional box movement.\nFigure 7: Comparison of SPI and random exploration for stuffs\n4.2 Experiment Results\nTwoexperimentsareconductedtotesttheeffectivenessofSPI.Eachexperimentisrunwithadifferent\nspeed factor (1 and 1). In each experiment, we plotted, compared, and evaluated the performance\n2 3\nmeasures (step distance, runtime, reward, and success rate) of the training phase between random\nexploration and the proposed SPI model.\n4.2.1 Success Rate\nSuccessisdefinedasthescenarioinwhichtheboxreachesthegoalwithouthittinganobstacle. Failure\nis defined as the scenario in which the box hits an obstacle. Success rate measures the frequency of\nsuccess in each training episode. For both SPI and random exploration, the success rate typically\n10\nincreases throughout training. It rises quickly at first, but it plateaus once the success rate becomes\nhigh. The episodic success rate for both algorithms at the beginning of training is nearly identical.\nHowever, especially for a speed factor of 1, SPI has a higher success rate towards the end (see fig. 8).\n3\n(a) 1/3 speed factor (b) 1/2 speed factor\nFigure8: Thisplotillustratesthesuccessrateoftheboxreachingthegoalacrosstraining\nepisodes.\n4.2.2 Steps\nWe define a step as every instance in which all agents select an action simultaneously. During each\nstep, agents will collectively push the box, resulting in a single displacement of the box. As shown in\nFigures ?, the trend of step count for speed factors of 1 and 1 are very similar. However, there is a\n3 2\nslightly larger discrepancy between SPI and random exploration for a speed factor of 1.\n3\n(a) 1/3 speed factor (b) 1/2 speed factor\nFigure 9: These figures illustrates and compares the step count between SPI and random\nexploration at two different speed factors: 1/3 and 1/2.\nWedefinesuccess stepsandfailure stepsasthenumberofstepsinasuccessfulorfailedtraining\nepisode,respectively. ThedifferenceinstepefficiencybetweenSPIandrandomexplorationcanlargely\nbe attributed to the success steps (see Fig. 10). Early in training, agents using SPI are able to find\nmoreefficientsuccessfulroutesthanagentsusingrandomexploration(seeFig.11). Towardstheendof\ntraining,agentsusingSPIconsistentlyuseoptimalroutestoreachthegoal,whileagentsusingrandom\nexploration struggle to learn to reach the goal quickly (see Fig. 12).\n11\n1/3 Speed Factor 1/2 Speed Factor\nFigure 10: These figures illustrate and compare the success steps taken during all of\ntraining at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor 1/2 Speed Factor\nFigure 11: These figures illustrate and compare the success steps taken during the first\nhalf of training at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor 1/2 Speed Factor\nFigure 12: These figures illustrate and compare the success steps taken during the second\nhalf of training at two different speed factors: 1/3 and 1/2.\nFor failure steps, SPI generally takes slightly fewer steps than random exploration (see Fig. 13).\n12\nHowever, the comparison changes when examining failure steps at the beginning versus the end of\ntraining. Early on, the distribution of SPI and random exploration failure steps is similar to the\noverall failure step distribution (see Fig. 14). However, towards the end of training, when the success\nrateofeachepisodeishigh,thefailurestepdistributionofSPIandrandomexplorationdiverges. When\nSPI fails during the second half of training, it consistently does so with a small step count, whereas\nrandom exploration failures during the second half of training are distributed over a wider range of\nsteps and exhibit a higher average step count (see Fig. 15).\n1/3 Speed Factor 1/2 Speed Factor\nFigure 13: These figures illustrate and compare the failure steps taken during all of\ntraining at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor 1/2 Speed Factor\nFigure 14: These figures illustrate and compare the failure steps taken during the first\nhalf of training at two different speed factors: 1/3 and 1/2.\n13\n1/3 Speed Factor 1/2 Speed Factor\nFigure 15: These figures illustrate and compare the failure steps taken during the second\nhalf of training at two different speed factors: 1/3 and 1/2.\n4.2.3 Reward\nFor both speed factors 1 and 1, agents receive similar rewards in the first few episodes. As training\n2 3\nprogresses, SPI consistently yields higher rewards at a speed factor of 1. A similar trend is observed\n3\nforthe 1 speedfactor,thoughthedifferenceinrewardsbetweenthetwoapproachesislesspronounced.\n2\n1/3 Speed Factor 1/2 Speed Factor\nFigure 16: These figures illustrates and compares the reward obtained during training\nby agents using SPI or random exploration at two different speed factors: 1/3 and 1/2.\n4.3 Discussion\nTheresultsindicatethatSPIcanserveasaneffectivealternativetorandomexplorationinagent-based\nself-organizing systems. Future research could explore SPI’s adaptability to scenarios discouraging\ncollaboration, as well as enhancements in SPI’s data structure for more complex environments. For\nexample, if there was a solution to find the optimal number of agents needed at any step, then we can\nupdate SPI to allow agents to adapt to different environments. But, this solution might take a lot\nmore computational power and can be inaccurate. Future studies can expand on both the action and\nstatespaces, asthisstudyemployssimplifiedversionsthatmaylimittheagents’accesstoinformation\nduringtrainingandexecution. Broaderactionandstatespacescouldprovideagentswitharichersetof\ninputsandactions,potentiallyenhancingtheiradaptabilityandperformanceincomplexenvironments.\nSPI will eventually converge with random exploration given enough episodes. However, it is able\nto find a more efficient route faster than random exploration in the experiment. This is applicable for\n14\nsituationslikeswarmroboticswheretimeefficiencyandrapidlearningisvaluable. SPIisalsoscalable\nduetoitsdecentralizednature. Agentsfollowasetglobalpolicywhichallowsforquickdeploymentfor\nany number of agents. Furthermore, SPI requires no communication, implicit or explicit. This makes\nit valuable in systems with limited or no access to communication bandwidths during training.\nThe effectiveness of SPI depends on the environment. If many micro movements are needed to\navoidobstacles–whenthemostoptimalactionforagentsistonotpushthebox–. thentheproposed\nSPI will be ineffective. In that scenario, the inefficiency arises from the proposed SPI’s emphasis on\nmaximizing box displacement every step. However, we have shown that the proposed SPI is very\neffective in an environment in which collaboration – where each agent’s effort in pushing the box\ncontributes positively - is essential for optimal success. Therefore, knowledge of the environment is\ncritical to determine the structure and implementation of SPI.\n5 Conclusion and Future Work\nIn this study, we tested the training effectiveness of agents in the box-pushing game. Our original RL\ntechnology SPI replaces the random exploration in MARL models to aid the coordination between\nagents in the box-pushing environment. This study presents SPI as a streamlined method for coor-\ndinating agent actions in multi-agent systems. After running computer simulations, we found that\nthe effectiveness of the proposed SPI depends on the environment. In an environment that highly\nencourages all agents to apply force collaboratively to push the box, the proposed SPI trained faster\nand found a more efficient route than random exploration. By minimizing inter-agent conflicts, SPI\nfacilitates efficient learning, enabling the system to achieve complex tasks more effectively.\nIn the future, we plan to test different SPI algorithms across randomized environments, defined\nonly by a set of rules and underlying physics of the environment. This approach will allow us to\nevaluate the adaptability and robustness of SPI under diverse and unpredictable conditions.\nReferences\n[1] V. I. Gorodetskii. Self-organization and multiagent systems: I. models of multiagent self-\norganization. Journal of Computer and Systems Sciences International, 51(2):256–281, 2012.\n[2] PeterKairouz,H.BrendanMcMahan,BrendanAvent,Aur´elienBellet,MehdiBennis,ArjunNitin\nBhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-\nvances and Open Problems in Federated Learning, volume 14 of Foundations and Trends in Ma-\nchine Learning. Now Publishers, 2021.\n[3] CarolineClausandCraigBoutilier. Thedynamicsofreinforcementlearningincooperativemulti-\nagentsystems. InProceedings of the National Conference on Artificial Intelligence (AAAI),pages\n746–752. AAAI Press, 1998.\n[4] Luis A. Hurtado, Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu, and Ren´e I. G. Kam-\nphuis. Enablingcooperativebehaviorforbuildingdemandresponsebasedonextendedjointaction\nlearning. IEEE Transactions on Industrial Informatics, 14(1):127–136, 2018.\n[5] AmirOroojlooyandDezhabHajinezhad. Areviewofcooperativemulti-agentdeepreinforcement\nlearning. Applied Intelligence, 53:13677–13722, 2023.\n[6] Yexin Li, Yu Zheng, and Qiang Yang. Efficient and effective express via contextual cooperative\nreinforcement learning. In Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 510–519, 2019.\n[7] Weirong Liu, Peng Zhuang, Hao Liang, Jun Peng, and Zhiwu Huang. Distributed economic\ndispatchinmicrogridsbasedoncooperativereinforcementlearning. IEEETransactionsonNeural\nNetworks and Learning Systems, 29(6):2192–2203, 2018.\n[8] ChristianA.SchroederdeWitt,JakobN.Foerster,GregoryFarquhar,PhilipH.S.Torr,Wendelin\nB¨ohmer,andShimonWhiteson. Multi-agentcommonknowledgereinforcementlearning,page890.\nCurran Associates Inc., Red Hook, NY, USA, 2019.\n15\n[9] YingWangandClarenceW.DeSilva. Multi-robotbox-pushing: Single-agentq-learningvs.team\nq-learning. In2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,pages\n3694–3699, 2006.\n[10] Mehdi Rahimi, Spencer Gibb, Yantao Shen, and Hung Manh La. A comparison of various ap-\nproaches to reinforcement learning algorithms for multi-robot box pushing. In Hamido Fujita,\nDuy Cuong Nguyen, Ngoc Pi Vu, Tien Long Banh, and Hermann Horst Puta, editors, Advances\nin Engineering Research and Application, pages 16–30, Cham, 2019. Springer International Pub-\nlishing.\n[11] EzraFedericoParra-Gonzalez,Jos´eGabrielRamirez-Torres,andGregorioToscano-Pulido. Anew\nobject path planner for the box pushing problem. In 2009 Electronics, Robotics and Automotive\nMechanics Conference (CERMA), pages 119–124, 2009.\n[12] K. S. Hwang, J. L. Ling, and Wei-Han Wang. Adaptive reinforcement learning in box-pushing\nrobots. In2014 IEEE International Conference on Automation Science and Engineering (CASE),\npages 1182–1187, 2014.\n[13] Toshiyuki Yasuda, Kazuhiro Ohkura, and Kazuaki Yamada. Multi-robot cooperation based on\ncontinuous reinforcement learning with two state space representations. In 2013 IEEE Interna-\ntional Conference on Systems, Man, and Cybernetics, pages 4470–4475, 2013.\n[14] YangWangandC.W.deSilva. Anobjecttransportationsystemwithmultiplerobotsandmachine\nlearning. In Proceedings of the 2005, American Control Conference, 2005., pages 1371–1376 vol.\n2, 2005.\n[15] Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics,\n6(5):679–684, 1957.\n[16] La¨etitia Matignon, Laurent Jeanpierre, and Abdel-Illah Mouaddib. Distributed value functions\nformulti-robotexploration. In2012 IEEE International Conference on Robotics and Automation,\npages 1544–1550, 2012.\n[17] David Wolpert, Kevin Wheeler, and Kagan Tumer. General principles of learning-based multi-\nagent systems. arXiv: Multiagent Systems, 06 1999.\n[18] Leonid Peshkin, Kee-Eung Kim, Nicolas Meuleau, and Leslie Pack Kaelbling. Learning to coop-\nerate via policy search. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial\nIntelligence,UAI’00,page489–496,SanFrancisco,CA,USA,2000.MorganKaufmannPublishers\nInc.\n[19] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps.\nIn Proceedings of the 14th International Conference on Neural Information Processing Systems:\nNatural and Synthetic, NIPS’01, page 1523–1530, Cambridge, MA, USA, 2001. MIT Press.\n[20] D. Szer and F. Charpillet. Improving coordination with communication in multi-agent reinforce-\nmentlearning. In16th IEEE International Conference on Tools with Artificial Intelligence, pages\n436–440, 2004.\n[21] Amir Rasouli, Iuliia Kotseruba, and John K. Tsotsos. Agreeing to cross: How drivers and pedes-\ntrians communicate. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 264–269, 2017.\n[22] F. Stulp, M. Isik, and M. Beetz. Implicit coordination in robotic teams using learned prediction\nmodels. In Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006.\nICRA 2006., pages 1330–1335, 2006.\n[23] AlexanderSashaVezhnevets,SimonOsindero,TomSchaul,NicolasHeess,MaxJaderberg,David\nSilver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In\nProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,\npage 3540–3549. JMLR.org, 2017.\n16\n[24] Zheng Tian, Shihao Zou, Ian Davies, Tim Warr, Lisheng Wu, Haitham Bou Ammar, and Jun\nWang. Learning to communicate implicitly by actions. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34:7261–7268, 04 2020.\n[25] ShengLi,JayeshK.Gupta,PeterMorales,RossAllen,andMykelJ.Kochenderfer. Deepimplicit\ncoordination graphs for multi-agent reinforcement learning, 2021.\n[26] Christopher Amato. An introduction to centralized training for decentralized execution in coop-\nerative multi-agent reinforcement learning, 2024.\n[27] WenzheFan,ZishunYu,ChengdongMa,ChangyeLi,YaodongYang,andXinhuaZhang.Towards\nefficient collaboration via graph modeling in reinforcement learning, 2024.\n[28] Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized\nexecution: Multi-agent conditional policy factorization, 2023.\n[29] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent\nactor-critic for mixed cooperative-competitive environments, 2020.\n[30] JakobN.Foerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhite-\nson. Counterfactual multi-agent policy gradients. In Proceedings of the Thirty-Second AAAI\nConference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelli-\ngenceConferenceandEighthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,\nAAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018.\n[31] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son,\nand Yung Yi. Learning to schedule communication in multi-agent reinforcement learning, 2019.\n[32] Gang Chen. A new framework for multi-agent reinforcement learning – centralized training and\nexplorationwithdecentralizedexecutionviapolicydistillation. InProceedingsofthe19thInterna-\ntional Conference on Autonomous Agents and MultiAgent Systems,AAMAS’20,page1801–1803,\nRichland, SC, 2020. International Foundation for Autonomous Agents and Multiagent Systems.\n[33] AlexanderSashaVezhnevets,SimonOsindero,TomSchaul,NicolasHeess,MaxJaderberg,David\nSilver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In\nProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,\npage 3540–3549. JMLR.org, 2017.\n[34] Younghwan Shin, Seungwoo Seo, Byunghyun Yoo, Hyunwoo Kim, Hwajeon Song, and Sungwon\nLee. Survey on recent advances in multiagent reinforcement learning focusing on decentralized\ntraining with decentralized execution framework. Electronics and Telecommunications Trends,\n38(4):95–103, Aug 2023.\n[35] AnastasiaKoloskova, NicolasLoizou, SadraBoreiri, MartinJaggi, andSebastianStich. Aunified\ntheory of decentralized SGD with changing topology and local updates. In Hal Daum´e III and\nAarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning Research, pages 5381–5393. PMLR, 13–18 Jul\n2020.\n[36] AnastasiaKoloskova*,TaoLin*,SebastianUStich,andMartinJaggi.Decentralizeddeeplearning\nwith arbitrary communication compression. In International Conference on Learning Represen-\ntations, 2020.\n[37] Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Rishi Sharma, and Milos Vujasinovic. De-\ncentralized learning made easy with decentralizepy. In Proceedings of the 3rd Workshop on Ma-\nchine Learning and Systems,EuroMLSys’23,page34–41,NewYork,NY,USA,2023.Association\nfor Computing Machinery.\n[38] Hao Ji and Yan Jin. Knowledge acquisition of self-organizing systems with deep multiagent\nreinforcement learning. Journal of Computing and Information Science in Engineering, 22:1–47,\n10 2021.\n17",
    "pdf_filename": "Efficient_Training_in_Multi-Agent_Reinforcement_Learning_A_Communication-Free_Framework_for_the_Box-.pdf"
}