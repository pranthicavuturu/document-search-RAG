{
    "title": "Efficient Training in Multi-Agent Reinforcement Learning A Communication-Free Framework for the Box-",
    "abstract": "Self-organizing systems consists of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on re- inforcement learning to enable agents to gain the skills needed for task completion, such as in the box-pushing environment. However, when agents push from opposing directions during explo- ration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and faciliate coordinate and reduce force conflicts among agents, thus enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents’ collaborative effectiveness. 1 Introduction Self-organizing systems can address scalability, bottlenecks, and reliability issues that are common in systems with a central controller [1]. However, in practice, agents that learn purely by themselves without the help of a central entity can result in the agent having unbalanced data [2]. Agents can combat the issue through implicit observations or through direct communication ([3], [4], [5]). To add the ability to communicate would require additional overhead to the model. The solution proposed in this paper requires no communication or significant overhead. Cooperation and coordination in reinforcement learning has been a difficult yet crucial problem to solve. There are many studies that aim to find an efficient solution that will allow agents in a multi-agent environment to coordinate. For example, Yexin et al. proposed a contextual cooperative reinforcement learning model to address the lack of cooperations between couriers while also consid- ering system context [6]. Weirong et al. proposed a cooperative reinforcement learning algorithm for distributed economic dispatch in microgrids [7]. Similar to previous studies, this paper aims to address the agent coordination issue in the box-pushing game. In the box pushing game, agents collaborate to push a box towards the goal while avoiding obstacles. The minimalist agents have a small set of possible actions. They also choose their action based on limited observation capacities. The agents can only observe the environment using a box sensor placed in the middle of the box that is being pushed. The sensor gives limited environmental information such as the direction of the goal and nearby obstacles (see Sec. 3.2). The agents are unable to sense one another – all agents are unaware of one another. They can only act through their learned experience and their observation of the current environment. Because of this fact, agents often push against one other, especially towards the beginning of training. This leads to undesirable exploration. ∗Corresponding author 1 arXiv:2411.12246v1  [cs.AI]  19 Nov 2024",
    "body": "Efficient Training in Multi-Agent Reinforcement\nLearning: A Communication-Free Framework for\nthe Box-Pushing Problem\nDavid Ge\nDepartment of Computer Science\nUniversity of California, Berkeley\n2301 Durant Ave, Berkeley, CA 94704\nlwg0320@berkeley.edu\nHao Ji∗\nCenter for Advanced Research\nComputing\nUniversity of Southern California\n3434 South Grand Avenue, Building\nCAL, Los Angeles, CA 90089\nhaoji@usc.edu\nAbstract\nSelf-organizing systems consists of autonomous agents that can perform complex tasks and\nadapt to dynamic environments without a central controller. Prior research often relies on re-\ninforcement learning to enable agents to gain the skills needed for task completion, such as in\nthe box-pushing environment. However, when agents push from opposing directions during explo-\nration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement\nand inefficient training. This paper proposes a model called Shared Pool of Information (SPI),\nwhich enables information to be accessible to all agents and faciliate coordinate and reduce force\nconflicts among agents, thus enhancing exploration efficiency. Through computer simulations, we\ndemonstrate that SPI not only expedites the training process but also requires fewer steps per\nepisode, significantly improving the agents’ collaborative effectiveness.\n1\nIntroduction\nSelf-organizing systems can address scalability, bottlenecks, and reliability issues that are common in\nsystems with a central controller [1]. However, in practice, agents that learn purely by themselves\nwithout the help of a central entity can result in the agent having unbalanced data [2]. Agents can\ncombat the issue through implicit observations or through direct communication ([3], [4], [5]). To add\nthe ability to communicate would require additional overhead to the model. The solution proposed in\nthis paper requires no communication or significant overhead.\nCooperation and coordination in reinforcement learning has been a difficult yet crucial problem\nto solve. There are many studies that aim to find an efficient solution that will allow agents in a\nmulti-agent environment to coordinate. For example, Yexin et al. proposed a contextual cooperative\nreinforcement learning model to address the lack of cooperations between couriers while also consid-\nering system context [6]. Weirong et al. proposed a cooperative reinforcement learning algorithm for\ndistributed economic dispatch in microgrids [7]. Similar to previous studies, this paper aims to address\nthe agent coordination issue in the box-pushing game.\nIn the box pushing game, agents collaborate to push a box towards the goal while avoiding obstacles.\nThe minimalist agents have a small set of possible actions. They also choose their action based on\nlimited observation capacities. The agents can only observe the environment using a box sensor placed\nin the middle of the box that is being pushed. The sensor gives limited environmental information such\nas the direction of the goal and nearby obstacles (see Sec. 3.2). The agents are unable to sense one\nanother – all agents are unaware of one another. They can only act through their learned experience\nand their observation of the current environment. Because of this fact, agents often push against one\nother, especially towards the beginning of training. This leads to undesirable exploration.\n∗Corresponding author\n1\narXiv:2411.12246v1  [cs.AI]  19 Nov 2024\n\nIn this paper, we introduce the idea of a shared pool of information (SPI), which provides addi-\ntional information generated and given to all agents at initialization. This idea is similar to common\nknowledge, in which agents use deduction as the basis for their action [8]. SPI aims to make up for\na lack of communication between the agents by providing a common framework for all agents to base\ntheir exploration on. To generate an SPI that would enable such collaboration, it must pass a fitness\ntest, which consists of an origin avoidance test and an angular spread test. The goal of the tests is to\nmaximize total box displacement and the box’s range of motion (see Sec 3.7). We tested the efficiency\nof training in various challenging environments to observe the difference between SPI and random\nexploration.\nUsing the box-pushing game, this paper demonstrates how SPI is able to improve the ability\nfor agents to coordinate during training. Furthermore, it adds minimal computation and overhead.\nIn the experiment, SPI reduces instances in which agents cancel out one another’s actions, making\neach agents’ exploration more meaningful. This idea can be applied and used in tandem with other\nalgorithms to help address the issue of coordination and cooperation for multi-agent problems in\nreinforcement learning.\n2\nRelated Works\n2.1\nThe Box pushing problem\nThe box pushing problem has been experimented on with single-agent and multi-agent RL. Studies\nfind that both algorithms are equally as effective in simple environments. However, single agent RL\nproves to be more efficient and consistent in complex environments ([9], [10]). In route planning, Ezra,\nJos´e, and Gregorio proposed a new path planning algorithm that allows for the minimum number of\nreconfigurations, which can aid robots in planning and optimizing routes for the box pushing problem\n[11]. For the box pushing problem, researchers also devised RL algorithms that are more efficient\nthan the standard q-learning algorithm used for multi-agent RL ([12], [13]). Toshiyuki, Kazuhiro, and\nKazuaki proposed a technique that aims to organize what agents learn into clusters that represent\nthe environment or situations that they might encounter. Each cluster contains its own set of rules\nto dictate what actions are optimal for agents in that specific scenario/environment [13]. Kao-Shing,\nJ.L., and Wei-Han proposed adaptive state aggregation q-learning that aids coordination by splitting\nthe state space into chunks [12]. Yang and Clarence used genetic algorithms alongside reinforcement\nlearning, with an arbitrator evaluating the output of both to determine the agent’s actions [14].\n2.2\nMultiagent Communication\nMany multiagent reinforcement learning tasks require agents to coordinate in order to accomplish. In\nMarkov Decision Process each ”action” is a joint action for all agents and the reward is their combined\nreward. However, this creates a large action space [15]. Local optimization for the different agents\nvia reward and value sharing ([16], [17]), direct policy search [18], and Factored MDPs ([19], [20])\nare approaches to address this problem. Agents can also observe one another’s actions and efforts\nimplicitly rather than communicating directly to determine their own actions ([21], [22]). However,\nimplicit communication protocols for complex coordination problems typically require multiple training\nsessions for different components, severely increasing the training time and training complexity ([23],\n[24]). Sheng et al. developed coordinate graphs that are automatically and dynamically created for\nthe agents to share information and decide on actions together [25].\n2.3\nCentralized vs Decentralized Learning\nCentralized training for decentralized execution (CLDE) is an approach to address the issue of coordi-\nnation [26]. Under the CLDE framework, Wenzhe et al. attempts to address the issue of coordination\nusing f-MAT, which enables efficient collaboration through graph modeling [27]. Centralized learning\nis useful to give agents information during training that they normally wouldn’t have access to during\nexecution [28]. There are two main ways of accomplishing this - a centralized critic for all agents or a\ncentralized critic for each agent ([29], [30]). Daewoo et al. proposed SchedNet where the centralized\ncritic gives feedback which consists of message encoders, action selectors, and weight generators to\neach agent [31]. Gang proposed Centralized Training and Exploration with Decentralized Execution\n2\n\nvia policy Distillation as an example of a critic for each agent [32]. In addition, Alexander et al.\nproposed a Bayesian mode to further optimize the exploration phase of CLDE training. It weighs the\ncosts of exploration against its expected benefit gained from exploring [33]. Decentralized learning is\ngenerally more computationally demanding or can encounter problems. But, in the real world, it is\noften impractical for a centralized model to access all agent’s actions and observations [34]. In DL,\nnodes communicate and share models with their immediate neighbors, converging to a global model\nthrough local interactions [35]. CHOCO-SGD proposed by Anastasia et al. allows the communication\nand gradient computation steps to run in parallel [36]. Although less research has been done on DL\ncompared CL, Akash et al. created the DecentralizePy, hoping that others can use it to develop new\nDL systems [37].\n3\nMethodology\nIn a simulated environment designed for agents to collaborate on the box-pushing task, agents rely on\nan implicit coordination framework called the Shared Pool of Information (SPI), which consists of a\nmap and key. This framework provides agents with a shared reference that fosters aligned movements\nand minimizes counteractive actions. It aims to address the inefficiencies of random exploration in\nmulti-agent reinforcement learning.\n3.1\nEnvironment\nFigure 1: Illustration of the box environment.\nFig. 1 shows the environment, which consists of patches, obstacles, agents, boxes, and the goal.\nThe environment is built using the Pygame library. The graphical illustration demonstrates a possible\nstate of the box-pushing task in which agents (green boxes) push the box (black box). Agents navigate\naround obstacles (represented by red circle) and walls (represented by black lines) to reach the goal\n(represented by blue diamond). Success occurs when the outer perimeter of the box collides with any\narea of the goal and failure happens when the outer perimeter of the box touches the obstacle or any\nwall. Failure can also happen when the box is unable to reach the goal in under 300 steps.\n3\n\nFigure 2: Illustration of the action space.\nFigure 3: Illustration of the state space.\n3.2\nAction Space and State Space\nAt each time step, each agent chooses one region within the six available regions in the box neigh-\nborhood to apply force (see Fig. 2), directing the box’s movement accordingly. This information is\nsubsequently converted into the action space A, represented as follows:\nA = {a1, a2, a3, a4, a5, a6}\nIn the neighborhood surrounding the box, six distinct regions are represented as a1, a2, . . . , a6 (see\nFig. 2). For each region, a value of 1 indicates that the agent has selected that region, while a value\nof 0 indicates that it has not. Each agent chooses exactly one region; therefore, {1, 1, 0, 0, 0, 0} and\n{0, 0, 0, 0, 0, 0} are not valid action spaces. In fig. 2, the agent selects region 1 of the box neighborhood.\nTherefore, the action space A is defined as {1, 0, 0, 0, 0, 0}.\nAt each time step, the box’s sensor detects relevant environmental information within a 150-pixel\nradius as well as the angle between the box and the goal. This information is then converted into the\nstate space S, represented as follows:\nS = {s1, s2, s3, s4, s5, s6, s7, s8, s9}\ns1, s2, . . . , s8 represent the eight octants in fig. 3. A value of 1 indicates the presence of an obstacle\nor wall in the octant, whereas a value of 0 indicates its absence. s9 represents the angle between\nthe box and the goal, scaled into the range [−1, 1]. In fig. 3, an obstacle is present in states s3 and\ns6, and walls are present in states s4 and s5. Consequently, s1, s2, s7, and s8 = 0, while s3, s4, s5,\nand s6 = 1.\nThe angle θ = 30◦translates to s9 =\nθ−180\n180\n= −0.83.\nThis gives the state space\nS = {0, 0, 1, 1, 1, 1, 0, 0, −0.83}.\n3.3\nReward\nDistance reward (Rdis) is the reward for moving the box closer to the goal. Dold and Dnew represent\nthe distance in pixels between the box’s sensor and the goal at the previous and current timesteps,\nrespectively.\nRdis = (Dold −Dnew) · 2.5\nRotation reward (Rrot) is the reward designed to discourage excessive rotation of the box. αold\nand αnew represent the angles between the box’s x-axis and the goal at the previous and current\ntimesteps, respectively.\nRrot = cos(αnew −αold) −0.98\nCollision reward, (Rcol), is the negative reward applied if the box collides with an obstacle or\nwall.\nRcol =\n(\n0\nif there is no collision\n−900\nif there is a collision\n4\n\nGoal reward, (Rgoal), is the positive reward assigned when the box successfully reaches the goal.\nRgoal =\n(\n900\nif goal is reached\n0\nif goal is not reached\nThe total reward is formulated as a weighted sum of distance, rotation, collision, and goal rewards.\nAll reward weights are set to fixed values, with the distance reward assigned the highest weight. The\nvalues are defined in [38].\nRTotal = w1Rdis + w2Rrot + w3Rcol + w4Rgoal\n3.4\nSpeed Factor\nSpeed factor controls how much force agents are able to exert. This impacts how much the agents can\nboth displace and rotate the box. In this experiment, the speed factor is reduced to one-third and\none-half of its original value, effectively limiting the agents’ pushing strength. With reduced force,\nindividual agents can no longer move the box as efficiently on their own. Cooperation becomes even\nmore critical as failure to do so results in slower progress. This shift in dynamics is designed to test\nhow the agents adapt to a more challenging environment, where collaborative effort is essential to\nachieving the desired outcome.\n3.5\nImplicit Coordination Through Shared Pool of Information\nThe primary goal of the introduction of a shared pool of information (SPI) is to encourage implied\ncoordination – the ability for agents to work constructively without specifically needing to exchange\ninformation between one another. In order to accomplish this, the scenario must have the following\nkey characteristics:\n1. All agents have access to the same shared information at instantiation.\n2. Agents can only observe or read from the shared information.\n3. The shared information cannot be changed by the agents.\n4. The shared information is simple - needs little to no training and should have a small time and\nspace complexity.\nThe following ruleset ensures that agents, at instantiation, must be identical (1) and cannot com-\nmunicate with one another in any way by any means (2, 3). The addition of SPI is efficient and\nlightweight – adds no significant overhead (4).\nThe inclusion of SPI reduces the likelihood that agents push the box from different sides, resulting\nin minimal movement. SPI should act like a silent guide that allows the agents to better align their\nefforts in order to improve the speed and efficiency of training.\n3.6\nProposed Shared Pool of Information\nInstead of agents randomly exploring, the agents would now pseudo-randomly explore based on shared\ninformation. This is to improve the agents’ cooperation during training. The proposed SPI has two\nportions, map and key. The map refers to an idealized distribution outlining the potential outcomes\nand distribution of those outcomes when all the agents push the box simultaneously. In other words,\nwe want to simulate the movement of the box under the assumption that the agents act as one or\nare controlled by a centralized entity. It is important to note that simply having such a distribution\nand enacting on its implementation would change the experiment from using multiagent reinforced\nlearning (RL) approach to a single centralized agent RL approach by definition. Therefore, it is crucial\nto include a key, in which all agents can use to decipher the distribution, allowing for cooperation\nwithout the need for communication between agents.\nThe key must be able to be randomized and the randomization must have sufficient range – larger\nthan the number of agents in the experiment. The distribution of the randomized key must also be\nuniform to ensure no agents will have no bias to any distribution outlined in the map. Although there\n5\n\nFigure 4:\nThis figure illustrates the procedure by which agents select actions during\ntraining.\nare numerous possible implementations of the key, in the experiment, it is simply generated with a\nrandom number generator using the random library in Python. The key will be generated every step\nof training and all agents will be able to observe and read from it.\nThe map of SPI contains a large number of probability distribution lists (PDL) – the exact number\nis discussed in section 4.1.6. As there are six different actions the agents can take, each PDL will have\nsix entries, representing the probability that an agent should choose each action. The summation of\nthe entries of each PDL should be exactly 1. Examples of valid and invalid PDLs are in Table 1.\nDuring exploration, agents use the map by first reading the generated key. Then, the agent must\nfind the PDL associated with the key. Given the PDL, the agent will subsequently use it to determine\nwhat action to take. Fig. 4 is a drawn example of the process.\nPDL Type\nExamples\nValid PDL\n\u0002 1\n6, 1\n6, 1\n6, 1\n6, 1\n6, 1\n6\n\u0003\n, [1, 0, 0, 0, 0, 0],\n\u0002 1\n6, 1\n6, 1\n4, 1\n4, 1\n12, 1\n12\n\u0003\nInvalid PDL\n\u0002 1\n7, 1\n7, 1\n7, 1\n7, 1\n7, 1\n7\n\u0003\n,\n\u0002 1\n5, 1\n5, 1\n5, 1\n5, 1\n5, 1\n5\n\u0003\n,\n\u0002 1\n4, 1\n4, 1\n4, 1\n4\n\u0003\nTable 1: Examples of Valid and Invalid PDL\n3.7\nFitness Test using BMD\nAt each step, agents will simultaneously push the box after determining what action to take. In one\nsimulation, the displacement of the box in both the x and y axis in which the box is pushed from\nits starting point is recorded. Each simulation is exactly one step long. After a significant amount\nof simulations, we can plot out the resulting displacement distribution of the box. We call that box\nmovement distribution (BMD). Figure 4 shows an example of such a graph. When using the map,\nagents should be able to collaborate efficiently. To be considered effective, the map needs to pass a\nfitness test, which consists of an origin avoidance test and a uniformity of angular spread test. We\n6\n\nmust ensure that BMD minimizes the amount of conflicting forces exerted by the agents while allowing\nagents to push the box in all directions with equal likelihoods.\nFigure 4: Graph\n3.7.1\nOrigin Avoidance\nThe first component of the fitness test evaluates origin avoidance, which measures the percentage\nof time the agents can displace the box a sufficient distance. The origin avoidance fitness score is\ncalculated as follows:\norigin avoidance fitness = 1 −\n\u0012near origin count\ntotal count\n\u0013\nIn the tests, the box is considered near the origin if its total displacement is three times less than\nits maximum potential displacement. This ensures the agents cooperate so that the box is sufficiently\ndisplaced from its original position. The count of instances where the box remains near the origin is\nrecorded as near origin count, while total count denotes the overall count of test instances. A low\norigin avoidance fitness score would indicate that the agents constantly exert conflicting forces, unable\nto consistently displace the box far from the origin. On the other hand, a high origin avoidance fitness\nscore would indicate that the agents are generally able to collaborate, as they rarely exert conflicting\nforces.\n3.7.2\nUniformity of Angular Spread\nThe second component of the fitness test evaluates the uniformity of the box’s angular spread, which\ntests if the box can be pushed in any direction with equal likelihood. To calculate the uniformity\nof angular spread, the angle of each box from the origin is computed. Then, we define bin edges\nfor n equally sized bins, each representing 360\nn\ndegrees, where n is the number of agents. Next, we\ncompute the histogram of the angles (see fig. 7) and utilize the mean and standard deviation of the\nhistogram to determine the coefficient of variation (CV). A lower CV indicates higher uniformity in\nangular distribution. Therefore, we assess the uniformity of the angular spread by inverting the CV\nand normalizing it to a [0, 1] range, as shown in the following equation:\nuniformity of angular spread =\n\u00121 −CV\nCV\n\u0013\nA low fitness score indicates insufficient diversification in the box’s movement directions, which\ncan hinder the training process. Conversely, a higher score reflects that, throughout the simulations,\nthe agents have successfully aligned themselves to enable multi-directional pushing of the box. Such\nflexibility in angular spread is essential for maximizing the range of the box’s potential movement.\n4\nResults and Discussion\n4.1\nGenerating PDL\nThere are 6 potential actions for each agent (see fig. 2). In terms of box displacement, action 2 and\n3 along with action 5 and 6 are effectively the same. For calculating the ideal distribution, we can\nsimplify the state actions to the 4 actions shown. The instruction to generate one PDL is as follows.\n4.1.1\nGenerating the Initial Values\nFirst generates a random float, value1, from a uniform distribution in range [0, cap]. This will serve\nas the 0th element in the first PDL:\nvalue1 ∼U(0, cap)\nThe remaining sum represents the total the remaining three elements need to sum up to:\nremaining sum = 1 −value1\n7\n\nThen, three additional random values are generated.\nvalue2 ∼U(0, 1),\nvalue3 ∼U(0, 1),\nvalue4 ∼U(0, 1)\nThe three generated values are normalized to sum to 1. Then, these values are scaled by remaining sum\nto ensure that the total sum of all three elements equals 1:\nvalue2norm =\nvalue2\nvalue2+value3+value4\nvalue2scaled = value2norm × remaining sum\nvalue3norm =\nvalue3\nvalue2+value3+value4\nvalue3scaled = value3norm × remaining sum\nvalue4norm =\nvalue4\nvalue2+value3+value4\nvalue4scaled = value4norm × remaining sum\nThe complete list of values, all values, is created by concatenating value1 with the scaled values:\nall values = [value1, value2scaled, value3scaled, value4scaled]\n4.1.2\nCondition Check for Validity\nThe difference between value1 and value3scaled must be minimum margin value. If this condition is\nnot met, the generation process restarts:\n|value1 −value3scaled| ≥margin\n4.1.3\nGenerating the PDL 4x4 Matrix\nThe next step is to create a 4 × 4 matrix, PDL 4x4. Each row i is a cyclic permutation (shift) of\nall values by i positions:\nPDL4×4 =\n\n\nvalue1\nvalue2scaled\nvalue3scaled\nvalue4scaled\nvalue4scaled\nvalue1\nvalue2scaled\nvalue3scaled\nvalue3scaled\nvalue4scaled\nvalue1\nvalue2scaled\nvalue2scaled\nvalue3scaled\nvalue4scaled\nvalue1\n\n\nIn general, each element in row i and column j of PDL4×4 is defined as:\nPDL4×4[i, j] = value((j+i) mod 4)+1\n4.1.4\nGenerating the PDL 4x6 Matrix\nA 4×6 matrix, PDL 4x6, is created using specific transformations of PDL 4x4. The columns of PDL 4x6\nare populated as follows:\n• The 0th column of PDL 4x6 matches the 0th column of PDL 4x4:\nPDL 4x6[i, 0] = PDL 4x4[i, 0].\n• The 1st and 2nd columns are each half of the 1st column of PDL 4x4:\nPDL 4x6[i, 1] = 1\n2 · PDL 4x4[i, 1],\nPDL 4x6[i, 2] = 1\n2 · PDL 4x4[i, 1].\n• The 3rd column matches the 2nd column of PDL 4x4:\nPDL 4x6[i, 3] = PDL 4x4[i, 2].\n• The 4th and 5th columns are each half of the 3rd column of PDL 4x4:\nPDL 4x6[i, 4] = 1\n2 · PDL 4x4[i, 3],\nPDL 4x6[i, 5] = 1\n2 · PDL 4x4[i, 3].\n8\n\n4.1.5\nNormalizing Rows in PDL 4x6\nEach row in PDL 4x6 is normalized to ensure the values sum to 1.\nrow sums[i] =\nX\nj\nPDL 4x6[i, j].\nPDL 4x6[i, j] = PDL 4x6[i, j]\nrow sums[i] .\nFour valid PDLs are generated: PDL 4x6\n4.1.6\nNumber of PDLs Generated\nWhen there are 15 agents, the angular avoidance fitness score increases with more PDL distributions.\nAfter about 250 PDLs, additional PDLs seem to have little to no effect on the angular avoidance fitness\nscore. This trend also holds true given any margin and cap values (see Fig. 5). In the experiment, we\nwill use 4000 PDLs to ensure a high angular avoidance fitness score.\nFigure 5:\nThe angular avoidance fitness\nscore is tested for different margin and cap\nvalues. The lighter lines represent a high\nmargin and cap value.\nThe darker lines\nrepresent a low margin and cap value.\nFigure 6:\nThe angular avoidance fitness\nscore is calculated for different numbers of\nPDLs using a margin value of 0.3 and a cap\nvalue of 0.1.\n4.1.7\nGenerating Cap and Margin Values\nIn the experiment, 15 agents and 4000 PDLs are used. The optimal margin value and cap value for\nthose parameters are tested. 100000 total simulations are run and the results are plotted in a line\ngraph.\nAs margin value increases, the origin avoidance fitness score also increases. A smaller cap value\nleads to a higher origin avoidance fitness score. However, with a higher margin value (ie. 0.5), the\nsignificance of cap value decreases.\nUntil a margin value of 0.30, by increasing margin value, angular spread fitness score also increases.\nAfterwards, an increase in margin value drastically decreases the angular spread fitness score. Cap\nvalue does not have a meaningful impact on angular spread fitness score.\nThe cap chosen for the experiment is 0.1 while the margin value is 0.3. This set of values is chosen\nbecause it maximizes angular spread fitness score while maintaining a high origin avoidance fitness\nscore. There exist many values of margin value and fitness value in which both origin avoidance and\nangular spread test are sufficiently passed. It is up to the experimenter to determine if origin avoidance\nis more valuable or angular spread.\n4.1.8\nFitness Test - New vs Random Explore\nThe old method of random exploration has a origin avoidance score of 0.215 and an angular spread\nscore of 0.714. The new method has a origin score of 0.899 and an angular spread score of 0.911.\n9\n\nBy comparing the differences between both methods using the box movement graph and histogram\nof point directions, it is evident that by using SPI, the box not only has access to a wider range of\nmovement, but also can explore all directions more uniformly (see Fig. 7).\nSPI\nRandom Exploration\nKernel density estimate (KDE) plot of box movement from origin.\nHistogram of box displacement.\nHistogram of directional box movement.\nFigure 7: Comparison of SPI and random exploration for stuffs\n4.2\nExperiment Results\nTwo experiments are conducted to test the effectiveness of SPI. Each experiment is run with a different\nspeed factor ( 1\n2 and 1\n3). In each experiment, we plotted, compared, and evaluated the performance\nmeasures (step distance, runtime, reward, and success rate) of the training phase between random\nexploration and the proposed SPI model.\n4.2.1\nSuccess Rate\nSuccess is defined as the scenario in which the box reaches the goal without hitting an obstacle. Failure\nis defined as the scenario in which the box hits an obstacle. Success rate measures the frequency of\nsuccess in each training episode. For both SPI and random exploration, the success rate typically\n10\n\nincreases throughout training. It rises quickly at first, but it plateaus once the success rate becomes\nhigh. The episodic success rate for both algorithms at the beginning of training is nearly identical.\nHowever, especially for a speed factor of 1\n3, SPI has a higher success rate towards the end (see fig. 8).\n(a) 1/3 speed factor\n(b) 1/2 speed factor\nFigure 8: This plot illustrates the success rate of the box reaching the goal across training\nepisodes.\n4.2.2\nSteps\nWe define a step as every instance in which all agents select an action simultaneously. During each\nstep, agents will collectively push the box, resulting in a single displacement of the box. As shown in\nFigures ?, the trend of step count for speed factors of 1\n3 and 1\n2 are very similar. However, there is a\nslightly larger discrepancy between SPI and random exploration for a speed factor of 1\n3.\n(a) 1/3 speed factor\n(b) 1/2 speed factor\nFigure 9: These figures illustrates and compares the step count between SPI and random\nexploration at two different speed factors: 1/3 and 1/2.\nWe define success steps and failure steps as the number of steps in a successful or failed training\nepisode, respectively. The difference in step efficiency between SPI and random exploration can largely\nbe attributed to the success steps (see Fig. 10). Early in training, agents using SPI are able to find\nmore efficient successful routes than agents using random exploration (see Fig. 11). Towards the end of\ntraining, agents using SPI consistently use optimal routes to reach the goal, while agents using random\nexploration struggle to learn to reach the goal quickly (see Fig. 12).\n11\n\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 10: These figures illustrate and compare the success steps taken during all of\ntraining at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 11: These figures illustrate and compare the success steps taken during the first\nhalf of training at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 12: These figures illustrate and compare the success steps taken during the second\nhalf of training at two different speed factors: 1/3 and 1/2.\nFor failure steps, SPI generally takes slightly fewer steps than random exploration (see Fig. 13).\n12\n\nHowever, the comparison changes when examining failure steps at the beginning versus the end of\ntraining.\nEarly on, the distribution of SPI and random exploration failure steps is similar to the\noverall failure step distribution (see Fig. 14). However, towards the end of training, when the success\nrate of each episode is high, the failure step distribution of SPI and random exploration diverges. When\nSPI fails during the second half of training, it consistently does so with a small step count, whereas\nrandom exploration failures during the second half of training are distributed over a wider range of\nsteps and exhibit a higher average step count (see Fig. 15).\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 13:\nThese figures illustrate and compare the failure steps taken during all of\ntraining at two different speed factors: 1/3 and 1/2.\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 14: These figures illustrate and compare the failure steps taken during the first\nhalf of training at two different speed factors: 1/3 and 1/2.\n13\n\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 15: These figures illustrate and compare the failure steps taken during the second\nhalf of training at two different speed factors: 1/3 and 1/2.\n4.2.3\nReward\nFor both speed factors 1\n2 and 1\n3, agents receive similar rewards in the first few episodes. As training\nprogresses, SPI consistently yields higher rewards at a speed factor of 1\n3. A similar trend is observed\nfor the 1\n2 speed factor, though the difference in rewards between the two approaches is less pronounced.\n1/3 Speed Factor\n1/2 Speed Factor\nFigure 16: These figures illustrates and compares the reward obtained during training\nby agents using SPI or random exploration at two different speed factors: 1/3 and 1/2.\n4.3\nDiscussion\nThe results indicate that SPI can serve as an effective alternative to random exploration in agent-based\nself-organizing systems. Future research could explore SPI’s adaptability to scenarios discouraging\ncollaboration, as well as enhancements in SPI’s data structure for more complex environments. For\nexample, if there was a solution to find the optimal number of agents needed at any step, then we can\nupdate SPI to allow agents to adapt to different environments. But, this solution might take a lot\nmore computational power and can be inaccurate. Future studies can expand on both the action and\nstate spaces, as this study employs simplified versions that may limit the agents’ access to information\nduring training and execution. Broader action and state spaces could provide agents with a richer set of\ninputs and actions, potentially enhancing their adaptability and performance in complex environments.\nSPI will eventually converge with random exploration given enough episodes. However, it is able\nto find a more efficient route faster than random exploration in the experiment. This is applicable for\n14\n\nsituations like swarm robotics where time efficiency and rapid learning is valuable. SPI is also scalable\ndue to its decentralized nature. Agents follow a set global policy which allows for quick deployment for\nany number of agents. Furthermore, SPI requires no communication, implicit or explicit. This makes\nit valuable in systems with limited or no access to communication bandwidths during training.\nThe effectiveness of SPI depends on the environment. If many micro movements are needed to\navoid obstacles – when the most optimal action for agents is to not push the box –. then the proposed\nSPI will be ineffective. In that scenario, the inefficiency arises from the proposed SPI’s emphasis on\nmaximizing box displacement every step. However, we have shown that the proposed SPI is very\neffective in an environment in which collaboration – where each agent’s effort in pushing the box\ncontributes positively - is essential for optimal success. Therefore, knowledge of the environment is\ncritical to determine the structure and implementation of SPI.\n5\nConclusion and Future Work\nIn this study, we tested the training effectiveness of agents in the box-pushing game. Our original RL\ntechnology SPI replaces the random exploration in MARL models to aid the coordination between\nagents in the box-pushing environment. This study presents SPI as a streamlined method for coor-\ndinating agent actions in multi-agent systems. After running computer simulations, we found that\nthe effectiveness of the proposed SPI depends on the environment. In an environment that highly\nencourages all agents to apply force collaboratively to push the box, the proposed SPI trained faster\nand found a more efficient route than random exploration. By minimizing inter-agent conflicts, SPI\nfacilitates efficient learning, enabling the system to achieve complex tasks more effectively.\nIn the future, we plan to test different SPI algorithms across randomized environments, defined\nonly by a set of rules and underlying physics of the environment. This approach will allow us to\nevaluate the adaptability and robustness of SPI under diverse and unpredictable conditions.\nReferences\n[1] V. I. Gorodetskii.\nSelf-organization and multiagent systems:\nI. models of multiagent self-\norganization. Journal of Computer and Systems Sciences International, 51(2):256–281, 2012.\n[2] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin\nBhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-\nvances and Open Problems in Federated Learning, volume 14 of Foundations and Trends in Ma-\nchine Learning. Now Publishers, 2021.\n[3] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multi-\nagent systems. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages\n746–752. AAAI Press, 1998.\n[4] Luis A. Hurtado, Elena Mocanu, Phuong H. Nguyen, Madeleine Gibescu, and Ren´e I. G. Kam-\nphuis. Enabling cooperative behavior for building demand response based on extended joint action\nlearning. IEEE Transactions on Industrial Informatics, 14(1):127–136, 2018.\n[5] Amir Oroojlooy and Dezhab Hajinezhad. A review of cooperative multi-agent deep reinforcement\nlearning. Applied Intelligence, 53:13677–13722, 2023.\n[6] Yexin Li, Yu Zheng, and Qiang Yang. Efficient and effective express via contextual cooperative\nreinforcement learning. In Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 510–519, 2019.\n[7] Weirong Liu, Peng Zhuang, Hao Liang, Jun Peng, and Zhiwu Huang.\nDistributed economic\ndispatch in microgrids based on cooperative reinforcement learning. IEEE Transactions on Neural\nNetworks and Learning Systems, 29(6):2192–2203, 2018.\n[8] Christian A. Schroeder de Witt, Jakob N. Foerster, Gregory Farquhar, Philip H. S. Torr, Wendelin\nB¨ohmer, and Shimon Whiteson. Multi-agent common knowledge reinforcement learning, page 890.\nCurran Associates Inc., Red Hook, NY, USA, 2019.\n15\n\n[9] Ying Wang and Clarence W. De Silva. Multi-robot box-pushing: Single-agent q-learning vs. team\nq-learning. In 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n3694–3699, 2006.\n[10] Mehdi Rahimi, Spencer Gibb, Yantao Shen, and Hung Manh La. A comparison of various ap-\nproaches to reinforcement learning algorithms for multi-robot box pushing. In Hamido Fujita,\nDuy Cuong Nguyen, Ngoc Pi Vu, Tien Long Banh, and Hermann Horst Puta, editors, Advances\nin Engineering Research and Application, pages 16–30, Cham, 2019. Springer International Pub-\nlishing.\n[11] Ezra Federico Parra-Gonzalez, Jos´e Gabriel Ramirez-Torres, and Gregorio Toscano-Pulido. A new\nobject path planner for the box pushing problem. In 2009 Electronics, Robotics and Automotive\nMechanics Conference (CERMA), pages 119–124, 2009.\n[12] K. S. Hwang, J. L. Ling, and Wei-Han Wang. Adaptive reinforcement learning in box-pushing\nrobots. In 2014 IEEE International Conference on Automation Science and Engineering (CASE),\npages 1182–1187, 2014.\n[13] Toshiyuki Yasuda, Kazuhiro Ohkura, and Kazuaki Yamada. Multi-robot cooperation based on\ncontinuous reinforcement learning with two state space representations. In 2013 IEEE Interna-\ntional Conference on Systems, Man, and Cybernetics, pages 4470–4475, 2013.\n[14] Yang Wang and C.W. de Silva. An object transportation system with multiple robots and machine\nlearning. In Proceedings of the 2005, American Control Conference, 2005., pages 1371–1376 vol.\n2, 2005.\n[15] Richard Bellman.\nA markovian decision process.\nJournal of Mathematics and Mechanics,\n6(5):679–684, 1957.\n[16] La¨etitia Matignon, Laurent Jeanpierre, and Abdel-Illah Mouaddib. Distributed value functions\nfor multi-robot exploration. In 2012 IEEE International Conference on Robotics and Automation,\npages 1544–1550, 2012.\n[17] David Wolpert, Kevin Wheeler, and Kagan Tumer. General principles of learning-based multi-\nagent systems. arXiv: Multiagent Systems, 06 1999.\n[18] Leonid Peshkin, Kee-Eung Kim, Nicolas Meuleau, and Leslie Pack Kaelbling. Learning to coop-\nerate via policy search. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial\nIntelligence, UAI’00, page 489–496, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers\nInc.\n[19] Carlos Guestrin, Daphne Koller, and Ronald Parr.\nMultiagent planning with factored mdps.\nIn Proceedings of the 14th International Conference on Neural Information Processing Systems:\nNatural and Synthetic, NIPS’01, page 1523–1530, Cambridge, MA, USA, 2001. MIT Press.\n[20] D. Szer and F. Charpillet. Improving coordination with communication in multi-agent reinforce-\nment learning. In 16th IEEE International Conference on Tools with Artificial Intelligence, pages\n436–440, 2004.\n[21] Amir Rasouli, Iuliia Kotseruba, and John K. Tsotsos. Agreeing to cross: How drivers and pedes-\ntrians communicate. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 264–269, 2017.\n[22] F. Stulp, M. Isik, and M. Beetz. Implicit coordination in robotic teams using learned prediction\nmodels. In Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006.\nICRA 2006., pages 1330–1335, 2006.\n[23] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David\nSilver, and Koray Kavukcuoglu.\nFeudal networks for hierarchical reinforcement learning.\nIn\nProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,\npage 3540–3549. JMLR.org, 2017.\n16\n\n[24] Zheng Tian, Shihao Zou, Ian Davies, Tim Warr, Lisheng Wu, Haitham Bou Ammar, and Jun\nWang. Learning to communicate implicitly by actions. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34:7261–7268, 04 2020.\n[25] Sheng Li, Jayesh K. Gupta, Peter Morales, Ross Allen, and Mykel J. Kochenderfer. Deep implicit\ncoordination graphs for multi-agent reinforcement learning, 2021.\n[26] Christopher Amato. An introduction to centralized training for decentralized execution in coop-\nerative multi-agent reinforcement learning, 2024.\n[27] Wenzhe Fan, Zishun Yu, Chengdong Ma, Changye Li, Yaodong Yang, and Xinhua Zhang. Towards\nefficient collaboration via graph modeling in reinforcement learning, 2024.\n[28] Jiangxing Wang, Deheng Ye, and Zongqing Lu.\nMore centralized training, still decentralized\nexecution: Multi-agent conditional policy factorization, 2023.\n[29] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent\nactor-critic for mixed cooperative-competitive environments, 2020.\n[30] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-\nson.\nCounterfactual multi-agent policy gradients.\nIn Proceedings of the Thirty-Second AAAI\nConference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelli-\ngence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence,\nAAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018.\n[31] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son,\nand Yung Yi. Learning to schedule communication in multi-agent reinforcement learning, 2019.\n[32] Gang Chen. A new framework for multi-agent reinforcement learning – centralized training and\nexploration with decentralized execution via policy distillation. In Proceedings of the 19th Interna-\ntional Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’20, page 1801–1803,\nRichland, SC, 2020. International Foundation for Autonomous Agents and Multiagent Systems.\n[33] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David\nSilver, and Koray Kavukcuoglu.\nFeudal networks for hierarchical reinforcement learning.\nIn\nProceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,\npage 3540–3549. JMLR.org, 2017.\n[34] Younghwan Shin, Seungwoo Seo, Byunghyun Yoo, Hyunwoo Kim, Hwajeon Song, and Sungwon\nLee. Survey on recent advances in multiagent reinforcement learning focusing on decentralized\ntraining with decentralized execution framework. Electronics and Telecommunications Trends,\n38(4):95–103, Aug 2023.\n[35] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified\ntheory of decentralized SGD with changing topology and local updates. In Hal Daum´e III and\nAarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning Research, pages 5381–5393. PMLR, 13–18 Jul\n2020.\n[36] Anastasia Koloskova*, Tao Lin*, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning\nwith arbitrary communication compression. In International Conference on Learning Represen-\ntations, 2020.\n[37] Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Rishi Sharma, and Milos Vujasinovic. De-\ncentralized learning made easy with decentralizepy. In Proceedings of the 3rd Workshop on Ma-\nchine Learning and Systems, EuroMLSys ’23, page 34–41, New York, NY, USA, 2023. Association\nfor Computing Machinery.\n[38] Hao Ji and Yan Jin.\nKnowledge acquisition of self-organizing systems with deep multiagent\nreinforcement learning. Journal of Computing and Information Science in Engineering, 22:1–47,\n10 2021.\n17",
    "pdf_filename": "Efficient_Training_in_Multi-Agent_Reinforcement_Learning_A_Communication-Free_Framework_for_the_Box-.pdf"
}