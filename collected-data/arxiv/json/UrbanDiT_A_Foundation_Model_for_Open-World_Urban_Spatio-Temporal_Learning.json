{
    "title": "Preprint",
    "abstract": "The urban environment is characterized by complex spatio-temporal dynamics arisingfromdiversehumanactivitiesandinteractions. Effectivelymodelingthese dynamics is essential for understanding and optimizing urban systems. In this work, weintroduceUrbanDiT,afoundationmodelforopen-worldurbanspatio- temporal learning that successfully scale up diffusion transformers in this field. UrbanDiT pioneers a unified model that integrates diverse spatio-temporal data sources and types while learning universal spatio-temporal patterns across dif- ferent cities and scenarios. This allows the model to unify both multi-data and multi-task learning, and effectively support a wide range of spatio-temporal ap- plications. Its key innovation lies in the elaborated prompt learning framework, which adaptively generates both data-driven and task-specific prompts, guiding themodeltodeliversuperiorperformanceacrossvariousurbanapplications. UrbanDiT offers three primary advantages: 1) It unifies diverse data types, such as grid-based and graph-based data, into a sequential format, allowing to cap- turespatio-temporaldynamicsacrossdiversescenariosofdifferentcities;2)With masking strategies and task-specific prompts, it supports a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spa- tial extrapolation, and spatio-temporal imputation; and 3) It generalizes effec- tively to open-world scenarios, with its powerful zero-shot capabilities outper- formingnearlyallbaselineswithtrainingdata. ThesefeaturesallowUrbanDiTto achievesstate-of-the-artperformanceindifferentdomainssuchastransportation traffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multi- ple cities and tasks. UrbanDiT sets up a new benchmark for foundation models intheurbanspatio-temporaldomain. Codeanddatasetsarepubliclyavailableat https://github.com/YuanYuan98/UrbanDiT. 1 INTRODUCTION Diverse Urban Spatio-Temporal Data Multiple tasks Data Task Crowd Flow Prompt Prompt Bi-directional Prediction Taxi Demand Temporal Interpolation Grid data Bike Usage Graph data Transportation UrbanDiT Spatial Extrapolation Cellular Traffic ST Imputation Figure1: AdiagramofourproposedUrbanDiTutilizingdataandtaskprompts. Itisafoundation modelthatintegratesdiversedatasourcesandtypeswhilesimultaneouslyperformingmultipletasks. Theurbanenvironmentischaracterizedbycomplexspatio-temporaldynamicsarisingfromdiverse humanactivitiesandinteractionswithinthecity. Thesedynamicsarereflectedindifferenttypesof 1 4202 voN 91 ]GL.sc[ 1v46121.1142:viXra",
    "body": "Preprint\nURBANDIT: A FOUNDATION MODEL FOR OPEN-\nWORLD URBAN SPATIO-TEMPORAL LEARNING\nYuanYuan,ChonghuaHan,JingtaoDing,DepengJin,YongLi\nDepartmentofElectronicEngineering\nTsinghuaUniversity\nBeijing,China\ny-yuan20@mails.tsinghua.edu.cn, liyong07@tsinghua.edu.cn\nABSTRACT\nThe urban environment is characterized by complex spatio-temporal dynamics\narisingfromdiversehumanactivitiesandinteractions. Effectivelymodelingthese\ndynamics is essential for understanding and optimizing urban systems. In this\nwork, weintroduceUrbanDiT,afoundationmodelforopen-worldurbanspatio-\ntemporal learning that successfully scale up diffusion transformers in this field.\nUrbanDiT pioneers a unified model that integrates diverse spatio-temporal data\nsources and types while learning universal spatio-temporal patterns across dif-\nferent cities and scenarios. This allows the model to unify both multi-data and\nmulti-task learning, and effectively support a wide range of spatio-temporal ap-\nplications. Its key innovation lies in the elaborated prompt learning framework,\nwhich adaptively generates both data-driven and task-specific prompts, guiding\nthemodeltodeliversuperiorperformanceacrossvariousurbanapplications.\nUrbanDiT offers three primary advantages: 1) It unifies diverse data types, such\nas grid-based and graph-based data, into a sequential format, allowing to cap-\nturespatio-temporaldynamicsacrossdiversescenariosofdifferentcities;2)With\nmasking strategies and task-specific prompts, it supports a wide range of tasks,\nincluding bi-directional spatio-temporal prediction, temporal interpolation, spa-\ntial extrapolation, and spatio-temporal imputation; and 3) It generalizes effec-\ntively to open-world scenarios, with its powerful zero-shot capabilities outper-\nformingnearlyallbaselineswithtrainingdata. ThesefeaturesallowUrbanDiTto\nachievesstate-of-the-artperformanceindifferentdomainssuchastransportation\ntraffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multi-\nple cities and tasks. UrbanDiT sets up a new benchmark for foundation models\nintheurbanspatio-temporaldomain. Codeanddatasetsarepubliclyavailableat\nhttps://github.com/YuanYuan98/UrbanDiT.\n1 INTRODUCTION\nDiverse Urban Spatio-Temporal Data Multiple tasks\nData Task\nCrowd Flow Prompt Prompt Bi-directional Prediction\nTaxi Demand\nTemporal Interpolation\nGrid data\nBike Usage\nGraph data\nTransportation UrbanDiT Spatial Extrapolation\nCellular Traffic ST Imputation\nFigure1: AdiagramofourproposedUrbanDiTutilizingdataandtaskprompts. Itisafoundation\nmodelthatintegratesdiversedatasourcesandtypeswhilesimultaneouslyperformingmultipletasks.\nTheurbanenvironmentischaracterizedbycomplexspatio-temporaldynamicsarisingfromdiverse\nhumanactivitiesandinteractionswithinthecity. Thesedynamicsarereflectedindifferenttypesof\n1\n4202\nvoN\n91\n]GL.sc[\n1v46121.1142:viXra\nPreprint\nTable1: ComparisonbetweenexistingmodelsandUrbanDiTacrossfiveaspects.\nMethod ModelInit. DataType DataSource[1] TaskFlexibility Zero-shot\nGPD(Yuanetal.,2024b) Scratch Graph × × ×\nUniST(Yuanetal.,2024a) Scratch Grid ✓ × ✓\nUrbanGPT(Lietal.,2024) LLMs Grid ✓ × ✓\nCityGPT(Fengetal.,2024a) LLMs Languages × ✓ ×\nUrbanDiT Scratch Graph/Grid ✓ ✓ ✓\n[1]:Whetherleveragediversedatasources.\ndata. Forexample,grid-baseddatadividesurbanspaceintoregularcells,oftenusedtotrackcrowd\nflows. In contrast, graph-based data represents spatial structures like road networks as nodes and\nedges,suchastrafficspeedsonroads. Thesedatasourcesusuallycomefromdifferentcities,each\nwith unique layouts, infrastructures, and planning strategies. Effectively modeling these diverse\nspatio-temporal dynamics is crucial for optimizing urban services and understanding how cities\nfunctionandevolve.Therefore,itraisesanessentialresearchquestion:canwedevelopafoundation\nmodel, similar to those in natural language processing (Touvron et al., 2023; Brown et al., 2020)\nandcomputervision(Brooksetal.,2024;Liuetal.,2023a;Esseretal.,2024),thatlearnsuniversal\nspatio-temporalpatternsandservesasageneral-purposemodelforvariousurbanapplications?\nIn the context of urban spatio-temporal modeling, recent advancements such as GPD (Yuan et al.,\n2024b),UrbanGPT(Lietal.,2024),andUniST(Yuanetal.,2024a)haveopenedexcitingavenues\nfor understanding complex urban dynamics. As compared in Table 1, these models either utilize\nLLMs(Lietal.,2024)ordevelopunifiedmodelsfromscratch(Yuanetal.,2024a;b)tailoredforur-\nbanspatio-temporalpredictions. Bytrainingonmultipledatasets,theyhaveshowcasedimpressive\ngeneralization capabilities. However, their focus remains largely on prediction tasks, and they are\noftenrestrictedtospecificdatatypes—suchasgrid-baseddata(Lietal.,2024;Yuanetal.,2024a)\norgraph-basedtrafficdata(Yuanetal.,2024b).Thus,realizingthefullpotentialoffoundationmod-\nels capable of seamlessly handling diverse data types, sources, and tasks in open-world scenarios\nremainsanopenandlargelyunexploredareaofresearch.\nUrbanspatio-temporaldataistypicallydefinedbydiverseproperties,includingvaryingspatialres-\nolutions,temporaldynamics,andcomplexinteractionsamongentities. Buildinganeffectivefoun-\ndationmodelrequiresascalablearchitecturecapableofaccommodatingthesecomplexities. More-\nover,theintricatenatureofurbanspatio-temporaldynamicsnecessitatesamodelthatcanlearnfrom\ncomplexdatadistributions. DiffusionTransformers,exemplifiedbymodelslikeSora(Brooksetal.,\n2024), offer a compelling solution for this purpose. By combining the generative power of diffu-\nsionprocesseswiththescalabilityandflexibilityoftransformerarchitectures,diffusiontransformers\npresentapromisingbackbone.\nIn this work, we introduce UrbanDiT, which unifies training across diverse urban scenarios and\ntasks,effectivelyscalingupdiffusiontransformersforcomprehensiveurbanspatio-temporallearn-\ning. Itoffersthreeappealingbenefits:1)Itunifiesdiversedatatypesintoasequentialformat,allow-\ningittocapturespatio-temporalpatternsacrossvariouscitiesanddomains, guidedbydata-driven\nprompts that highlight critical patterns. 2) It supports a wide range of tasks with a single model,\nusingmaskingstrategiesandtask-specificprompts,withouttheneedforre-trainingacrossdifferent\ntasks.3)Itgeneralizeswelltoopen-worldscenarios,exhibitingpowerfulzero-shotperformance.To\nachievethis,wefirstunifydifferentinputdatabyconvertingitintothesequentialformat. Webuild\nthedenoisingnetworkusingtransformerblocks,equippedwithbothtemporalandspatialattention\nmodules.Tointegratediversedatatypesandtasks,weproposeaunifiedpromptlearningframework\nthat enhances the denoising process. This framework maintains memory pools to capture learned\nspatio-temporalpatternsandgeneratedata-drivenprompts, whilealsocreatetask-specificprompts\nforvariousspatio-temporaltasks. Thesepromptsareconcatenatedintotheunifiedsequentialinput\nbefore being fed into the transformer modules. The design of prompt learning serves as a flexible\nintermediary,adaptabletoawiderangeofscenarios.\nUrbanDiT, built on the DiT backbone with a prompt learning framework, is a pioneering open-\nworldfoundationmodel. Itexcelsathandlingdiverseurbanspatio-temporaldataandawiderange\noftasks,includingbi-directionalspatio-temporalprediction,temporalinterpolation,spatialextrapo-\nlation,andspatio-temporalimputation. ThismakesUrbanDiTapowerfulanduniversalsolutionfor\nvariousurbanspatio-temporalapplications. Wesummarizeourcontributionsasfollows:\n2\nPreprint\n• Tothebestofourknowledge,wearethefirsttoexploreafoundationmodelforgeneral-purpose\nurbanspatio-temporallearning,integratingdiversespatio-temporaldatatypesandmultipleurabn\ntaskswithinasingleunifiedmodel.\n• WepresentUrbanDiT,anopen-worldfoundationmodelbuiltondiffusiontransformers. Through\nour proposed prompt learning, UrbanDiT effectively brings together heterogeneous spatio-\ntemporaldataandtasks,usingdata-drivenandtask-specificpromptstoenhanceperformance.\n• Extensive experiments demonstrate that UrbanDiT effectively captures complex urban spatio-\ntemporal dynamics, achieving state-of-the-art performance across multiple datasets and tasks.\nIt also exhibits powerful zero-shot capabilities, proving its applicability in open-world settings.\nUrbanDiTmarksasignificantstepforwardintheadvancementofurbanfoundationmodels.\n2 RELATED WORK\n2.1 URBANSPATIO-TEMPORALLEARNING\nUrbanspatio-temporallearningencompassavarietyoftaskssuchasprediction(Tanetal.,2023b;\nBaietal.,2020;Yuanetal.,2023;Lietal.,2018;Zhangetal.,2017),interpolation(Aumondetal.,\n2018;Gra¨leretal.,2016),extrapolation(Milleretal.,2004;Maetal.,2019),andimputation(Tashiro\net al., 2021; Hu et al., 2023), addressing how urban systems evolve across space and time. Deep\nlearning has achieved significant progress in these areas, with techniques ranging from CNNs (Li\net al., 2018; Zhang et al., 2017), RNNs (Wang et al., 2017; 2018; Lin et al., 2020), MLPs (Shao\net al., 2022a), GNNs (Bai et al., 2020; Geng et al., 2019), and Transformers (Chen et al., 2022;\nJiang et al., 2023), to the more recent use of diffusion models (Yuan et al., 2023; 2024b; Tashiro\netal.,2021;Wenetal.,2023). Eachoftheseapproacheshasbeenemployedtomodelcomplicated\nspatio-temporal relationships inherent to urban environments. However, most existing models are\ntailoredtospecificdatasetsandtasks. Incontrast,ourapproachisdesignedtohandlemultipletasks\nandgeneralizeacrossdiverseurbanscenarioswithouttheneedforre-trainingonnewdatasets.\n2.2 URBANFOUNDATIONMODELS\nFoundationmodelshavemadesignificantprogressinlanguagemodels(Touvronetal.,2023;Brown\net al., 2020) and image generation (Brooks et al., 2024; Liu et al., 2023a; Esser et al., 2024). Re-\ncently,researchershaveextendedtheconceptoffoundationmodelstourbanenvironments,aiming\ntoaddressuniquechallengesofurbanspatio-temporaldata. Somerepresentativeworksinthisarea\nincludeUrbanGPT(Lietal.,2024),UniST(Yuanetal.,2024a),andCityGPT(Fengetal.,2024b).\nUrbanGPTintroducesLLMsdesignedforspatio-temporalpredictionswithinurbancontexts.UniST\ndevelops a foundation model from scratch specifically for urban prediction tasks, demonstrating\nzero-shotcapabilitiesthatallowthemodeltogeneralizetonewscenarioswithoutadditionaltrain-\ning. CityGPT,ontheotherhand,focusesonenhancingtheLLM’sabilitytocomprehendandsolve\nurbantasksbyimprovingitsunderstandingofurbanspaces. Table1providesacomparisonofkey\nabilities across existing urban foundation models and UrbanDiT. As shown, UrbanDiT is trained\nfrom scratch, allowing it to fully leverage data diversity while offering flexibility across a wide\nrangeoftasks. Additionally,itdemonstratesemergentzero-shotcapabilities. Comparedtoprevious\nefforts,UrbanDiTrepresentsasignificantadvancementindevelopingurbanfoundationmodels.\n2.3 DIFFUSIONMODELSFORSPATIO-TEMPORALDATA\nDiffusionmodels,originallypopularizedinimagegeneration,haverecentlygainedattentioninhan-\ndlingspatio-temporaldataandtimeseries. Theyiterativelyaddandremovenoisefromdata,allow-\ningthemtocapturecomplexpatternsacrossbothtemporalandspatialdimensions(Yangetal.,2024;\nYuanetal.,2023;Huetal.,2023;Wenetal.,2023;Rasuletal.,2021). Inthecontextoftimeseries,\ndiffusionmodelshavebeenappliedtotaskssuchasforecasting(Kolloviehetal.,2024;Rasuletal.,\n2021) and imputation (Xiao et al., 2023; Tashiro et al., 2021), outperforming traditional methods\nby generating more accurate and coherent sequences. For spatio-temporal data, diffusion models\nhave proven useful in a variety of tasks, including traffic prediction (Wen et al., 2023), environ-\nmentalmonitoring(Yuanetal.,2023),andhumanmobilitygeneration(Zhuetal.,2024;2023). By\neffectivelymodelingspatio-temporaldependencies,thesemodelscancaptureboththespatialcorre-\nlationsandtemporaldynamicsinherentinurbansystems. UrbanDiTleveragesthegenerativepower\n3\nPreprint\nOutput\nFrequency pattern\nReshape\nTemporal pattern ST\nData\nSpatial pattern\nData\nmemory Data-driven prompt\nUrban Diffusion Transformer\nDifferent Sequential\nA\ndata types format ett\nConcat\nTask\nnoitn Mask\nSpatio-temporal Embedding memory Task-specific prompt\npatching layer\n(d)Unified Prompt Learning\nNoisy Timestep\nMultiple Domains\nST data 𝑡\nMultiple Cities\n(a)Datasets (b) Urban Diffusion Transformer (c)Masking Strategies\nFigure 2: Illustration of the whole framework of UrbanDiT, including four key components: a)\nUnifyingdifferenturbanspatio-temporaldatatypes;b)ThediffusionpipelineofourUrbanDiT;c)\nDifferentmaskingstrategiestospecifydifferenttasks;d)Unifiedpromptlearningwithdata-driven\nandtask-specificpromptstoenhancethedenoisingprocess.\nofdiffusionmodelstocapturecomplexurbanspatio-temporalpatterns,whileitsflexiblecondition-\ningmechanismsallowittoaddressawiderangeofspatio-temporaltasks. ThismakesUrbanDiTa\nsignificantadvancementinapplyingdiffusionmodelstourbanspatio-temporalchallenges.\n3 METHOD\n3.1 PRELIMINARY\nUrbanSpatio-TemporalData.Urbanspatio-temporaldatatypicallyfallsintotwocategories:grid-\nbased andgraph-baseddata. Grid-baseddataisstructuredinauniformgridlayout. Graph-based\ndata, ontheotherhand, highlightsconnectivity, capturingtherelationshipsbetweenvariousurban\nentitieslikestreetsandintersections. Forbothdifferentspatialorganizations, thetemporaldimen-\nsionischaracterizedastimeseriesdata. ThedatacanbedenotedasXN×T,whereN denotesthe\nnumberofspatialpartitions.Forgraph-baseddata,N correspondstothenumberofnodes,whilefor\ngrid-baseddata,itisdefinedastheproductoftheheightandwidthofthegrid(N =H×W). This\nenablesaunifiedrepresentationofurbanspatio-temporaldatawithdifferentspatialorganizations.\nUrban Spatio-Temporal Tasks. In addition to the commonly recognized (1) forward prediction\ntask,urbanspatio-temporalanalysisencompassesseveralothercriticaltasks. (2)BackwardPredic-\ntioninvolvesestimatingpaststatesbasedoncurrentorfuturedata. Itisessentialforunderstanding\nhistoricaltrendsandvalidatingpredictivemodels. (3)TemporalInterpolationaimstoestimateval-\nues at unobserved time points within a known temporal range. (4) Spatial Extrapolation involves\npredictingvaluesbeyondtheobservedspatialdomain.Itisimportantforassessingpotentialchanges\ninurbanenvironmentsandplanningforfuturedevelopments.(5)Spatio-TemporalImputationrefers\ntotheprocessoffillinginmissingvaluesinspatio-temporaldatasets.\n3.2 OVERALLFRAMEWORK\nFigure 2 illustrates the overall framework of our proposed UrbanDiT, which is based on diffusion\ntransformers. This framework seamlessly integrates various data types and tasks into a cohesive\nmodel.\n4\nPreprint\nForward Prediction Backward Prediction Temporal Imputation\nUrbanDiT\nTemporal Interpolation Spatial Extrapolation Spatio-Temporal Imputation\nFigure3: Maskingstrategiestospecifyvariousurbanspatio-temporaltasks.\nUnificationofDataandTasks.Weconvertdata,characterizedbyathree-dimensionalstructure(2D\nspatialand1Dtemporaldimensions),intoaunifiedsequentialformat. Forthetemporaldimension,\nweemploypatchingtechniquescommonlyusedinfoundationalmodelsfortimeseries(Nieetal.,\n2022). For grid-based data, we apply 2D patching methods, which are widely utilized in image\nprocessing,toorganizethedata. Thisallowsustorearrangethethree-dimensionalgriddataintoa\none-dimensional sequential format. For graph-based data, we use Graph Convolutional Networks\n(GCN) (Zhang et al., 2019) to process each node and integrate it with the temporal dimension to\nreshape the data into a one-dimensional format as well. More details of data unification can be\nfoundinAppendixB.1\nTo adapt to various tasks, we employ a unified masking strategy. As illustrated in Figure 3, these\ntasks can be framed as reconstructing missing parts of the data, with distinct masking strategies\ntailored to each task. For Forward Prediction, we mask future time steps while utilizing past and\npresent data points to predict the missing values. Conversely, for Backward Prediction, we mask\npast time steps to estimate historical values based on current and future observations. In the case\noftemporalinterpolationtasks,weapplymaskstospecifictimepointswithinacontinuousseries,\nallowingthemodeltofillinthesegaps.Forspatio-temporalimputation,werandomlymaskmissing\nvalues across both spatial and temporal dimensions, enabling the model to leverage surrounding\ncontextforaccurateestimations. Finally, inspatialextrapolationtasks, wemaskareasoutsidethe\nobservedspatialdomaintopredictvaluesforunobservedregionsbasedonexistingspatialpatterns.\nConsequently, the input of the denoising network Xt is represented as the concatenation of noise\nfeaturesandunmaskedspatio-temporaldata(conditionalobservations):\nXt =Xt∗(1−M)+X0∗M\nwhere Xt denotes the noise features, M is the mask that controls the availability of values for\ndownstreamtasks,andX0 representsthecleanvaluesofthespatio-temporaldata. Inthisway,we\ncanmodulatedifferentmasksM tofacilitatevariousurbanspatio-temporalapplications.\nSequential Input of Spatio-Temporal Data. We first apply temporal patching to process time\nseriesdataateachspatiallocation,representedasXN×T′×D = CONV(XN×T×D),whereT′ = T\npt\nand p is the temporal patch size. Next, for grid-based data, we implement 2D spatial patching,\nt\nresultinginX\np\n= CONV2D(XH×W×T′×D),whereX\np\n∈ RL×D,L = pH s×× pW s×× pT t. Inthisway,we\neffectivelyreorganizethedataintoaformatthatisconducivetotransformerarchitectures.\nSpatio-TemporalTransformerBlock. Theoverallmodeliscomposedofmultiplespatio-temporal\ntransformerblocks. Eachblockfeaturesbothtemporalmulti-headattentionandspatialmulti-head\nattention, with spatial and temporal attention mechanisms operating independently. This design\nchoiceismadetoenhancecomputationalefficiency, asthecomplexityofattentionscaleswiththe\nsquareofthesequencelength.\nDiffusion Transformer. We adopt the diffusion transformer model, which integrates a denoising\nnetworkdesignedtoprocesscomplexinputseffectively.Theinputstothedenoisingnetworkconsist\nof three key components: the noisy spatio-temporal data, the timestep, and the prompt. For the\ntimestep t, we utilize them for layer normalization following previous practices (Peebles & Xie,\n2023; Lu et al.), which helps stabilize and standardize the input features at each timestep. The\nprompt,whichprovidescontextualinformationorguidanceforthemodel,isconcatenatedwiththe\n5\nPreprint\ninputdatatoenhancethemodel’sunderstandingofthedataandtaskathand. Thisconcatenationis\nstraightforwardduetothetransformer’scapabilitytomanagevariablesequencelengths,providing\nflexibilityinprocessingdiverseinputs. Byincorporatingtheseelements, thediffusiontransformer\nmodeleffectivelylearnstodenoiseandgeneraterobustdesiredresultsinspatio-temporalcontexts.\n3.3 UNIFIEDPROMPTLEARNING\nWeproposeaunifiedpromptlearningframeworktoenhancethediffusiontransformers’universality\nacrossvariousdatatypesandtasks.\nData-DrivenPrompt. Thedata-drivenpromptiscru-\nMemory Pool\ncialfortrainingaunifiedmodelwithmultipleanddi-\nverse datasets, as such datasets often exhibit signifi- Key Value\nc ca on nt tev xa t,ri ta ht eio pn rs omin ptp aa ct tt ser an ss aa gn ud idd inis gtr mib eu ct hio an ns i. smI ,n heth lpis\n-\nettaP K …ey V …alue morP\nr p\ningthemodeltoeffectivelynavigatethesedifferences n t\nKey Value\nand generate accurate results. Similar to retrieval-\nCosinesimilarity Weightedsum\naugmentedgeneration,promptsretrievethemostrele-\nvantinformation,enhancingthemodel’sabilitytocon-\ntextualizeandinterpretspatio-temporaldata.Byalign- Figure 4: Key-value structure of memory\ningthemodel’slearningprocesswiththespecificchar- pools.\nacteristics of various spatio-temporal patterns, data prompts ensure that UrbanDiT can adaptively\nrespondtoawiderangeofurbanspatio-temporalscenarios.\nTo achieve this goal, we employ memory networks, specifically utilizing three memory pools de-\nsignedtocapturethetime-domain,frequency-domainandspatialpatternsofspatio-temporaldynam-\nics. Fordifferentinputdata,thepromptnetworkretrievespromptsfromthesememorypoolsbased\nontherespectivetime-domain,frequency-domain,andspatialpatterns. AsshowninFigure4,each\nmemory pool is structured as a key-value store (K ,V ) = {(k1,v1),...,(kN,vN)}, (K ,V ) =\nt t t t t t f f\n{(k1,v1),...,(kN,vN)}, (K ,V ) = {(k1,v1),...,(kN,vN)}, where both keys and values are\nf f f f s s s s s s\nlearnableembeddingsandrandomlyinitialized. Thedata-drivenpromptsaregeneratedasfollows:\n(cid:88)\nα\nt\n= SOFTMAX(X t,K t), P\nt\n= α t·V t,\n(cid:88)\nα\nf\n= SOFTMAX(X f,K f), P\nf\n= α\nf\n·V f,\n(cid:88)\nα\nf\n= SOFTMAX(X s,K s), P\ns\n= α s·V s,\nX = CONCAT(P t,P f,X).\nTask-SpecificPrompt. Wealsodesigntask-specificpromptstoenhancethemodel’sperformance\nacrossdifferenttasks. Thesepromptsaregeneratedfromthemask,andweemployattentionmech-\nanismstoobtainthemaskpromptP\nm\nfromthemaskmapasP\nm\n= ATTENTION(FLATTEN(M)).\nThe learned pattern P is then concatenated with the input sequence, resulting in X =\nm\nCONCAT(P m,X). Thisenablesthemodeltoeffectivelyincorporatetask-specificinformation.\nWeprovidemoredetailsofdata-driventask-specificpromptsinAppendixB.2\n3.4 TRAININGANDINFERENCE\nThetrainingprocessalternatesbetweenmultipledatasetsandtasks. Ineachiteration,werandomly\nselect a dataset and a corresponding task to perform gradient descent training. This approach en-\nhances the model’s robustness by exposing it to diverse scenarios and helps prevent overfitting by\nensuringthemodellearnsfromawiderangeofinputsandobjectives. LetD ={D ,D ,...,D }\n1 2 m\nrepresentthesetofdatasets,andT = {T ,T ,...,T }denotethesetoftasks. LetL(d ,t )bethe\n1 2 k i i\nlossfunctionforthechosendatasetd andtaskt ,withthemodelparametersdenotedasθ. Overall,\ni i\nthetrainingprocesscanbesummarizedasfollows:\nFori=1toN : d ∼Uniform(D), t ∼Uniform(T) ⇒ θ ←θ−η∇L(d ,t ;θ)\ni i i i\nwhereN isthetotalnumberoftrainingiterationsandηisthelearningrate.\n6\nPreprint\nFor the training of the UrbanDiT model, we adopt a novel diffusion training approach proposed\nbytheInstaFlow(Liuetal.,2023a),whichsignificantlyimprovestheefficiencyofspatio-temporal\ndata generation. By employing rectified flow, it is an ordinary differential equation (ODE)-based\nframeworkthatalignsthenoiseanddatadistributionsthroughastraightenedtrajectory,asopposed\ntothecurvedpathsoftenseenintraditionalmodels.\n4 PERFORMANCE EVALUATIONS\n4.1 EXPERIMENTALSETTINGS\nDatasets. We utilize a diverse set of datasets from multiple domains and cities to evaluate urban\nspatio-temporal applications. These domains include taxi demand, cellular network traffic, crowd\nflows,transportationtraffic,anddynamicpopulation,reflectingabroadspectrumofurbanactivities.\nThedatasetsaresourcedfromdifferentcitiessuchasNewYorkCity(USA),Beijing,Shanghai,and\nNanjing (China), each representing unique urban characteristics. These datasets vary significantly\nin their spatial structures (e.g., grid or graph formats), the number of locations, and their spatial\nand temporal resolutions. These variations are influenced by differences in city structures, urban\nplanning strategies, and data collection methodologies across regions. For a detailed summary of\nthedatasets,pleaserefertoTable5andTable6inAppendixA.\nWesplitthedatasetsintotraining,validation,andtestingsetsalongthetemporaldimension,using\na6:2:2ratio. Toensurenooverlapbetweenthesesets,wecarefullyremoveanyoverlappingpoints,\nensuringclearseparationacrossthetemporalsplitsforevaluation.\nBaselines. To evaluate the performance of UrbanDiT, we establish a comprehensive benchmark,\ncomparing it against state-of-the-art models across different urban tasks. For prediction tasks,\nwe include both traditional time series models such as Historical Average (HA) and ARIMA, as\nwellasadvanceddeeplearning-basedspatio-temporalmodelslikeSTResNet(Zhangetal.,2017),\nACFM(Liuetal.,2018),STNorm(Dengetal.,2021),STGSP(Zhaoetal.,2022),MC-STL(Zhang\netal.,2023a),PromptST(Zhangetal.,2023b),STID(Shaoetal.,2022a),andUniST(Yuanetal.,\n2024a). Additionally,wecompareagainstleadingvideopredictionmodels,includingSimVP(Gao\net al., 2022), TAU (Tan et al., 2023a), MAU (Chang et al., 2021), and MIM (Wang et al., 2019),\nas well as recent time series forecasting approaches such as PatchTST (Nie et al., 2022), iTrans-\nformer (Liu et al., 2023b), Time-LLM (Jin et al.), and the diffusion-based model CSDI (Tashiro\netal.,2021). Forgraph-baseddatasets,weevaluateUrbanDiTagainstcutting-edgespatio-temporal\ngraph models, including STGCN (Yu et al., 2018), DCRNN (Li et al., 2018), GWN (Wu et al.,\n2019), MTGNN (Wu et al., 2020), AGCRN (Bai et al., 2020), GTS (Shang & Chen, 2021), and\nSTEP (Shao et al., 2022b). Furthermore, for spatio-temporal imputation tasks, we compare our\nmodel with state-of-the-art baselines such as CSDI, ImputeFormer (Nie et al., 2024), Grin (Cini\net al., 2022), and BriTS (Cao et al., 2018), adapting these methods for temporal interpolation and\nspatialextrapolationtaskswhereapplicable. WeprovidemoredetailsofbaselinesinAppendixC.1\n4.2 COMPARISONTOTHESTATE-OF-THE-ART\nBi-directional Spatio-Temporal Prediction. For this task, we set both the historical input win-\ndow and prediction horizon to 12 time steps. Depending on the dataset, the temporal granularity\nvaries—12stepsmaycorrespondto1hourfordatasetswith5-minuteintervals,6hoursfordatasets\nwith 30-minute intervals, and 12 hours for those with 1-hour intervals. For baselines that cannot\nhandledatasetswithdifferentshapes,wetrainindividualmodelsforeachdataset.Formoreflexible\nmodelslikeUniSTandPatchTST,wetrainasingleunifiedmodelacrossmultipledatasets.\nTable 2 provides a comprehensive benchmark for forward prediction on grid-based data. As ob-\nserved, traditional deep learning models such as STResNet, ACFM, and MC-STL, do not deliver\ncompetitive performance. Similarly, video prediction models, such as MAU, MIM, and SimVP,\nreveallimitations, suggestingthedifferencebetweenurbanspatio-temporaldynamicsandthosein\nconventionalvideodata. UniSTdemonstratesrelativelystrongperformance, suggestingthattrain-\ning a universal model across different datasets holds potential for improving prediction accuracy.\nHowever, time-series forecasting models struggled to capture the complex spatial interactions in-\nherent in urban environments, indicating that precisely modeling these interactions is critical for\nachieving better results in urban spatio-temporal prediction. Notably, CSDI ranks second in most\n7\nPreprint\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nHA 53.03 91.55 13.43 38.92 26.49 77.10 0.48 0.93 0.232 0.343\nARIMA 57.5 291 9.15 26.70 23.91 99.22 0.443 0.989 0.236 0.404\nSTResNet 26.55 37.96 45.63 59.82 14.81 26.88 0.511 0.718 0.546 0.751\nACFM 19.87 30.95 24.95 46.92 9.85 20.82 0.284 0.468 0.141 0.200\nSTNorm 19.00 31.21 11.88 28.46 10.43 26.94 0.231 0.384 0.132 0.198\nSTGSP 17.54 27.31 17.54 38.77 10.52 25.94 0.263 0.410 0.157 0.229\nMC-STL 28.51 38.50 33.83 46.06 26.01 36.75 0.727 0.504 0.235 0.311\nMAU 46.37 71.07 21.38 45.04 21.79 49.15 0.402 0.648 0.166 0.256\nMIM 42.40 68.18 22.49 47.29 9.151 24.53 0.399 0.715 0.214 0.298\nSimVP 21.67 35.58 15.87 28.59 9.08 19.69 0.191 0.282 0.148 0.213\nTAU 15.86 26.43 15.22 26.04 9.08 19.46 0.219 0.326 0.135 0.196\nPromptST 16.12 27.42 9.37 23.01 8.24 22.82 0.161 0.306 0.099 0.171\nUniST(unified) 14.04 23.67 9.10 19.95 5.85 17.55 0.119 0.191 0.106 0.172\nSTID 16.36 25.55 12.92 21.19 8.32 18.49 0.160 0.234 0.203 0.262\nPatchTST 30.55 53.36 10.69 28.17 17.03 50.45 0.223 0.465 0.189 0.291\nPatchTST(unified) 33.62 60.55 12.16 31.79 21.27 58.61 0.403 0.811 0.176 0.279\niTransformer 24.05 42.17 10.19 25.91 45.19 45.19 0.216 0.466 0.154 0.249\nTime-LLM 29.55 51.20 10.57 28.19 17.65 52.94 0.210 0.405 0.115 0.195\nCSDI 14.76 25.87 8.77 23.37 5.05 16.37 0.094 0.168 0.078 0.136\nUrbanDiT 12.61 21.09 5.61 14.44 5.58 15.53 0.092 0.166 0.077 0.129\nTable 2: Performance comparison for grid-based forward prediction evaluated using MAE and\nRMSE. The results represent the average prediction errors across different prediction steps. The\nbestperformanceishighlightedinbold,andthesecond-bestisindicatedwithunderlining.\ncases, showing the effectiveness of diffusion-based models in capturing complex patterns within\nurban spatio-temporal data. Our proposed model, UrbanDiT, delivers the best performance across\ndifferentdatasetsusingasingleunifiedmodel,achievingarelativeimprovementof11.3%.\nWe also compare the backward\nSpeedBJ SpeedSH SpeedNJ\nprediction performance of Urban-\nDiT with the second-best baseline, Model MAE RMSE MAE RMSE MAE RMSE\nCSDI, as shown in Appendix Ta- HA 1.35 2.13 0.92 1.46 1.94 3.01\nSTGCN 1.81 2.44 0.99 1.35 1.63 2.31\nble 7. Notably, CSDI is specifi-\nCRNN 1.37 1.98 0.89 1.28 1.53 2.38\ncally trained for backward predic- GWN 1.69 2.32 0.93 1.32 1.50 2.16\ntiontasks. However,UrbanDiTnot MTGNN 1.15 1.70 0.86 1.33 1.57 2.42\nonly excels in forward prediction AGCRN 1.66 2.29 1.14 1.56 1.77 2.46\nGTS 1.76 2.36 1.31 1.74 2.04 2.68\nbutalsosurpassesspecializedmod-\nSTEP 1.45 2.04 0.93 1.32 1.58 2.42\nels like CSDI in backward predic-\nSTID 1.08 1.69 0.83 1.26 1.56 2.38\ntionby30.4%. Thisresultdemon- PatchTST 1.27 1.99 0.87 1.37 1.83 2.74\nstrates UrbanDiT’s ability to cap- PatchTST(unified) 1.55 2.44 1.08 1.70 2.19 3.34\niTransformer 1.26 1.97 0.90 1.40 1.70 2.62\nture complex spatio-temporal pat-\nTime-LLM 1.28 2.00 0.87 1.36 1.82 2.76\nternsmoreeffectively.\nUrbanDiT 1.02 1.66 0.78 1.20 1.51 2.30\nTemporal Interpolation.1 In this\nTable 3: Comparison of forward prediction performance\ntask, we set the missing ratio to\nacrossthreegraph-structuredtrafficspeeddatasets.\n0.5,meaningthatweonlyknowthe\neven-numbered time steps (e.g., 0,\n2, 4, ..., 2n), and the model is required to predict the odd-numbered time steps (e.g., 1, 3, 5, ...,\n2n-1). Thegoalistoevaluatehowwellthemodelcaninferthemissingtemporalvaluesbylever-\nagingtheobserveddatapointsbeforeandafterthemissingsteps. AppendixTable9demonstrates\nthat UrbanDiT, employing a unified model, outperforms baselines trained separately for different\ndatasetsinmostcases.\nSpatialExtrapolation. Weevaluatethemodels’abilitytopredictmissingvaluesinspecificspatial\nregions by masking 50% of of spatial locations across the temporal sequence. The objective is to\ndetermine how effectively models extrapolate unobserved spatial information from the remaining\nvisibledata. AsshowninTable4,UrbanDiTachievesthebestperformanceinmostcases.\n8\nPreprint\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nCSDI 36.66 75.89 15.53 34.77 19.56 69.10 0.34 0.74 0.18 0.32\nImputeformer 37.13 77.53 17.67 38.96 20.28 49.85 0.39 0.71 0.21 0.34\nGrin 41.73 92.61 22.56 47.76 22.44 58.15 0.51 0.71 0.23 0.38\nBriTS 59.94 112.34 33.74 59.10 23.39 58.47 0.50 0.70 0.54 0.75\nUrbanDiT(ours) 8.10 12.23 5.44 10.17 4.91 12.52 0.099 0.155 0.084 0.146\nTable 4: Performance comparison for spatial extrapolation evaluated using MAE and RMSE. The\nresultsrepresenttheaverageerrorsacrossdifferentextrapolationsteps.\n(a) PopSH-5% few-shot (b) PopSH-10% few-shot\nFigure 5: Evaluation of UrbanDiT and baseline models in 5% and 1% few-shot scenarios on the\nPopSHdataset. ThereddashedlineindicatesUrbanDiT’szero-shotperformance\nSpatio-Temporal Imputation. This task assesses the models’ capacity to impute missing values\nacrossbothspatialandtemporaldimensions. Werandomlymask50%ofpositionsinthe3Dspatio-\ntemporaldata, simulatingreal-worldscenarioswhereurbandatamaybeincompleteduetosensor\nfailuresorirregularitiesindatacollection. AsshowninAppendixTable10,UrbanDiTachievesthe\nbestperformanceinmostcases.\nTheseresultssubstantiatethatUrbanDiTconsistentlydeliverssuperiorperformanceacrossdiverse\ntasksanddatasetsusingasingle, unifiedmodel. ThiscapabilitypositionsUrbanDiTasageneral-\npurposefoundationmodel,enablingpractitionerstoleverageoptimizedparametersdirectly,thereby\nsimplifyingdeploymentandenhancingapplicabilityinurbanspatio-temporalapplications.\n4.3 FEW-SHOTANDZERO-SHOTPERFORMANCE.\nA key strength of foundation models is their ability to generalize easily. Therefore, we perform\nexperiments in both few-shot and zero-shot scenarios, testing its adaptability to new datasets with\nlittle or no additional training. In the few-shot scenario, we train UrbanDiT on a small portion of\nthe target dataset—specifically using only 5% and 10% of the available data—and then evaluate\nits performance on the corresponding test set. This setup challenges the model to generalize well\nfromsparsedata. Inthezero-shotscenario,nodatafromthetargetdatasetisprovidedfortraining.\nInstead, we directly evaluate UrbanDiT’s performance on the target dataset, relying solely on its\npretrainedknowledgetohandleunseendatawithoutanyfine-tuning.\nFigure5demonstratesthefew-shotandzero-shotperformanceofUrbanDiTincomparisontobase-\nlinemodels. Inthefew-shotsetting(with5%and1%ofthetrainingdata),UrbanDiTconsistently\noutperforms the baselines, showcasing its strong ability to learn from minimal data. Even more\nstriking,inthezero-shotscenario,UrbanDiTexhibitsexceptionalinferencecapabilities,surpassing\nnearlyallbaselinemodelsthathadaccesstotrainingdata. Thishighlightsitsexceptionalgeneral-\nizationabilitywithoutfine-tuning,reinforcingitseffectivenessasanopen-worldfoundationmodel.\n4.4 ABLATIONSTUDIES.\nPrompt. Unified prompt learning is a key design in UrbanDiT. To investigate the contribution of\neachprompttothefinalperformance,weconductablationstudiesbysystematicallyremovingeach\n9\nPreprint\nFull w/o F w/o M\n25 21\nw/o T w/o S w/o P\n25\n24\n20\n24\n23\n19\n23\n22\n18\n22\n21\n5 10 20 50 100\n21 #Inference Steps\nFigure 6: Ablation study on the prompt Figure 7: Performance evaluation (RMSE)\ndesign using RMSE on the TaxiBJ with varying numbers of inference steps on\ndataset. TaxiBJandTaxiNYCdatasets.\ntype of prompt. Specifically, we identify four types of prompts: F for frequency-domain prompt,\nT for time-domain prompt, S for spatial prompt, and M for task-specific prompt. We denote the\nremovalofapromptasw/o{F,T,S,M}andindicatetheabsenceofanypromptasw/oP.\nFigure6presentstheresultsofablationstudies.Thefindingsrevealthatremovinganysingleprompt\nsignificantly degrades the model’s performance. In the absence of prompt design altogether, the\nmodel exhibits the poorest performance. Among the four types of prompts, the removal of the\nfrequency-domainprompthasthemostpronouncednegativeimpactontheoverallperformance.\nInference Steps of Diffusion Models. We further investigate the effect of inference steps on the\nperformance of diffusion models. The number of inference steps is a critical factor in balancing\nthe model’s accuracy and efficiency. Figure 7 illustrates the performance of the diffusion model\nacrossdifferentnumbersofinferencestepsfortwodatasets,TaxiBJandTaxiNYC,measuredusing\nRMSE. Notably, we observe that around 20 inference steps provide the optimal balance between\ncomputational efficiency and model performance for both datasets. By setting the diffusion steps\nto500andtheinferencestepsto20, weachievea25ximprovementinefficiencycomparedtothe\noriginalDDPM(Hoetal.,2020),withoutsacrificingaccuracy.\n4.5 SCALABILITY.\nModel Performance vs. Datasize Scale\nAs a foundation model, it is crucial to un- for Different Model Sizes\nderstand how model performance evolves as 0.080 UrbanDiT-S\nthedatasizescalevariesacrossdifferentmodel UrbanDiT-M\nsizes. This information is valuable for prac- 0.075 UrbanDiT-L\ntitioners to train and fine-tune the founda-\n0.070\ntion model effectively. In Figure 8, we ex-\nplore the relationship between model perfor- 0.065\nmanceanddatasizescaleforthreemodelsizes:\nUrbanDiT-S (small), UrbanDiT-M (medium), 0.060\nandUrbanDiT-L(large). Asobserved,allthree\n0.055\nmodels demonstrate improved performance as\nthe data size increases. However, when the\n0.2 0.4 0.6 0.8 1.0\ndataset size increases from 0.8 to 1, the large Datasize Scale\nmodel, UrbanDiT-L, shows a notably steeper Figure8: ThescalabilityofUrbanDiT.\nimprovement(withaslopeof0.011),compared\ntothemedium(slopeof0.0015)andsmallmodels(slopeof0.0019).Thispronouncedscalingeffect\nfor the large model indicates its potential to further enhance performance as more data becomes\navailable. These results highlight the promising scalability of UrbanDiT-L, suggesting that it can\neffectivelyhandlelargerdatasetsandachieveevenbetteroutcomeswithincreaseddatasize.\n10\nESMR\necnamrofreP\nledoM\nJBixaT\nCYNixaT\nPreprint\n5 CONCLUSION\nInthispaper,wepresentUrbanDiT,anopen-worldfoundationmodelbuiltonadiffusiontransform-\nersandaunifiedpromptlearningframework.UrbanDiTenablesseamlessadaptationtoawiderange\nofurbanspatio-temporaltasksacrossdiversedatasetsfromurbanenvironments. Ourextensiveex-\nperimentshighlightthemodel’sexceptionalpotentialinadvancingthefieldofurbanspatio-temporal\nmodeling. Webelievethisworknotonlypushestheboundariesofurbanspatio-temporalmodeling\nbutalsoservesasaninspirefutureresearchintherapidlyevolvingfieldoffoundationmodels.\nREFERENCES\nPierreAumond, ArnaudCan, VivienMallet, BertDeCoensel, CarlosRibeiro, DickBotteldooren,\nandCatherineLavandier. Kriging-basedspatialinterpolationfrommeasurementsforsoundlevel\nmapping in urban areas. The journal of the acoustical society of America, 143(5):2847–2857,\n2018.\nLeiBai,LinaYao,CanLi,XianzhiWang,andCanWang. Adaptivegraphconvolutionalrecurrent\nnetwork for traffic forecasting. Advances in neural information processing systems, 33:17804–\n17815,2020.\nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe\nTaylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024.\nURLhttps://openai.com/research/video-generation-models-as-world-simulators,3,2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare\nfew-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.\nWei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent\nimputationfortimeseries. Advancesinneuralinformationprocessingsystems,31,2018.\nZhengChang,XinfengZhang,ShansheWang,SiweiMa,YanYe,XiangXinguang,andWenGao.\nMau: A motion-aware unit for video prediction and beyond. Advances in Neural Information\nProcessingSystems,34:26950–26962,2021.\nChanglu Chen, Yanbin Liu, Ling Chen, and Chengqi Zhang. Bidirectional spatial-temporal adap-\ntive transformer for urban traffic flow forecasting. IEEE Transactions on Neural Networks and\nLearningSystems,2022.\nAndreaCini,IvanMarisca,andCesareAlippi.Fillingtheg ap s:Multivariatetimeseriesimputation\nbygraphneuralnetworks. InInternationalConferenceonLearningRepresentations,2022. URL\nhttps://openreview.net/forum?id=kOu3-S3wJ7.\nJinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. St-norm: Spatial and\ntemporalnormalizationformulti-variatetimeseriesforecasting. InProceedingsofthe27thACM\nSIGKDDconferenceonknowledgediscovery&datamining,pp.269–278,2021.\nPatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¨ller, HarrySaini, Yam\nLevi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor\nhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n2024.\nJieFeng,YuweiDu,TianhuiLiu,SiqiGuo,YumingLin,andYongLi. Citygpt: Empoweringurban\nspatialcognitionoflargelanguagemodels. arXivpreprintarXiv:2406.13948,2024a.\nJieFeng,YuweiDu,TianhuiLiu,SiqiGuo,YumingLin,andYongLi. Citygpt: Empoweringurban\nspatialcognitionoflargelanguagemodels. arXivpreprintarXiv:2406.13948,2024b.\nZhangyangGao,ChengTan,LirongWu,andStanZLi. Simvp:Simpleryetbettervideoprediction.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n3170–3180,2022.\n11\nPreprint\nXuGeng,YaguangLi,LeyeWang,LingyuZhang,QiangYang,JiepingYe,andYanLiu.Spatiotem-\nporalmulti-graphconvolutionnetworkforride-hailingdemandforecasting.InProceedingsofthe\nAAAIconferenceonartificialintelligence,volume33,pp.3656–3663,2019.\nBenediktGra¨ler,EdzerJPebesma,andGerardBMHeuvelink. Spatio-temporalinterpolationusing\ngstat. RJ.,8(1):204,2016.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,33:6840–6851,2020.\nJunfengHu,XuLiu,ZhenchengFan,YuxuanLiang,andRogerZimmermann.Towardsunifyingdif-\nfusionmodelsforprobabilisticspatio-temporalgraphlearning. arXivpreprintarXiv:2310.17360,\n2023.\nJiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. Pdformer: Propaga-\ntion delay-aware dynamic long-range transformer for traffic flow prediction. arXiv preprint\narXiv:2301.07945,2023.\nMingJin,ShiyuWang,LintaoMa,ZhixuanChu,JamesYZhang,XiaomingShi,Pin-YuChen,Yux-\nuanLiang,Yuan-FangLi,ShiruiPan,etal. Time-llm: Timeseriesforecastingbyreprogramming\nlargelanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations.\nMarcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang,\nand Yuyang Bernie Wang. Predict, refine, synthesize: Self-guiding diffusion models for proba-\nbilistictimeseriesforecasting. AdvancesinNeuralInformationProcessingSystems,36,2024.\nYaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-\nwork: Data-driventrafficforecasting. InInternationalConferenceonLearningRepresentations,\n2018.\nZhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao\nHuang. Urbangpt: Spatio-temporallargelanguagemodels,2024.\nZhihuiLin,MaomaoLi,ZhuobinZheng,YangyangCheng,andChunYuan.Self-attentionconvlstm\nfor spatiotemporal prediction. In Proceedings of the AAAI conference on artificial intelligence,\nvolume34,pp.11531–11538,2020.\nLingboLiu,RuimaoZhang,JiefengPeng,GuanbinLi,BowenDu,andLiangLin. Attentivecrowd\nflow machines. In Proceedings of the 26th ACM international conference on Multimedia, pp.\n1553–1561,2018.\nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for\nhigh-quality diffusion-based text-to-image generation. In The Twelfth International Conference\nonLearningRepresentations,2023a.\nYong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.\nitransformer: Inverted transformers are effective for time series forecasting. arXiv preprint\narXiv:2310.06625,2023b.\nHaoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt:\nGeneral-purpose video diffusion transformers via mask modeling. In The Twelfth International\nConferenceonLearningRepresentations.\nJunMa,YuexiongDing,JackCPCheng,FeifengJiang,andZhiweiWan. Atemporal-spatialinter-\npolationandextrapolationmethodbasedongeographiclongshort-termmemoryneuralnetwork\nforpm2.5. JournalofCleanerProduction,237:117729,2019.\nJamesRMiller,MonicaGTurner,EricaAHSmithwick,CLisaDent,andEmilyHStanley. Spatial\nextrapolation: the science of predicting ecological patterns and processes. BioScience, 54(4):\n310–320,2004.\nTong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, and Jian Sun. Imputeformer: Low rankness-\ninduced transformers for generalizable spatiotemporal imputation. In Proceedings of the 30th\nACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining,pp.2260–2271,2024.\n12\nPreprint\nYuqiNie,NamHNguyen,PhanwadeeSinthong,andJayantKalagnanam. Atimeseriesisworth64\nwords: Long-termforecastingwithtransformers. arXivpreprintarXiv:2211.14730,2022.\nWilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof\ntheIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.\nKashifRasul,CalvinSeward,IngmarSchuster,andRolandVollgraf. Autoregressivedenoisingdif-\nfusionmodelsformultivariateprobabilistictimeseriesforecasting. InInternationalConference\nonMachineLearning,pp.8857–8868.PMLR,2021.\nChaoShangandJieChen. Discretegraphstructurelearningforforecastingmultipletimeseries. In\nProceedingsofInternationalConferenceonLearningRepresentations,2021.\nZezhiShao,ZhaoZhang,FeiWang,WeiWei,andYongjunXu. Spatial-temporalidentity:Asimple\nyet effective baseline for multivariate time series forecasting. In Proceedings of the 31st ACM\nInternationalConferenceonInformation&KnowledgeManagement,pp.4454–4458,2022a.\nZezhiShao,ZhaoZhang,FeiWang,andYongjunXu. Pre-trainingenhancedspatial-temporalgraph\nneuralnetworkformultivariatetimeseriesforecasting.InProceedingsofthe28thACMSIGKDD\nconferenceonknowledgediscoveryanddatamining,pp.1567–1577,2022b.\nCheng Tan, Zhangyang Gao, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li, and Stan Z Li. Tem-\nporalattentionunit: Towardsefficientspatiotemporalpredictivelearning. InProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.18770–18782,2023a.\nCheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu,\nand Stan Z Li. Openstl: A comprehensive benchmark of spatio-temporal predictive learning.\nAdvancesinNeuralInformationProcessingSystems,36:69819–69831,2023b.\nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based\ndiffusionmodelsforprobabilistictimeseriesimputation. AdvancesinNeuralInformationPro-\ncessingSystems,34:24804–24816,2021.\nHugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-\nlayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-\ntionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.\nYunboWang,MingshengLong,JianminWang,ZhifengGao,andPhilipSYu. Predrnn: Recurrent\nneuralnetworksforpredictivelearningusingspatiotemporallstms. Advancesinneuralinforma-\ntionprocessingsystems,30,2017.\nYunboWang,ZhifengGao,MingshengLong,JianminWang,andSYuPhilip. Predrnn++: Towards\naresolutionofthedeep-in-timedilemmainspatiotemporalpredictivelearning. InInternational\nConferenceonMachineLearning,pp.5123–5132.PMLR,2018.\nYunboWang,JianjinZhang,HongyuZhu,MingshengLong,JianminWang,andPhilipSYu. Mem-\noryinmemory: Apredictiveneuralnetworkforlearninghigher-ordernon-stationarityfromspa-\ntiotemporaldynamics. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpat-\nternrecognition,pp.9154–9162,2019.\nHaomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger Zimmermann, and\nYuxuanLiang. Diffstg: Probabilisticspatio-temporalgraphforecastingwithdenoisingdiffusion\nmodels. In Proceedings of the 31st ACM International Conference on Advances in Geographic\nInformationSystems,pp.1–12,2023.\nZonghanWu,ShiruiPan,GuodongLong,JingJiang,andChengqiZhang. Graphwavenetfordeep\nspatial-temporal graph modeling. In Proceedings of the 28th International Joint Conference on\nArtificialIntelligence,pp.1907–1913,2019.\nZonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Con-\nnectingthedots: Multivariatetimeseriesforecastingwithgraphneuralnetworks. InProceedings\nofthe26thACMSIGKDDinternationalconferenceonknowledgediscovery&datamining,pp.\n753–763,2020.\n13\nPreprint\nChunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, and Fan Zhou. Imputation-based time-\nseriesanomalydetectionwithconditionalweight-incrementaldiffusionmodels.InProceedingsof\nthe29thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining,pp.2742–2751,\n2023.\nYiyuanYang,MingJin,HaominWen,ChaoliZhang,YuxuanLiang,LintaoMa,YiWang,Chenghao\nLiu,BinYang,ZenglinXu,etal.Asurveyondiffusionmodelsfortimeseriesandspatio-temporal\ndata. arXivpreprintarXiv:2404.18886,2024.\nBingYu,HaotengYin,andZhanxingZhu. Spatio-temporalgraphconvolutionalnetworks: adeep\nlearningframeworkfortrafficforecasting. InProceedingsofthe27thInternationalJointConfer-\nenceonArtificialIntelligence,pp.3634–3640,2018.\nYuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, and Yong Li. Spatio-temporal diffusion\npointprocesses. InProceedingsofthe29thACMSIGKDDConferenceonKnowledgeDiscovery\nandDataMining,pp.3173–3184,2023.\nYuanYuan,JingtaoDing,JieFeng,DepengJin,andYongLi.Unist:aprompt-empowereduniversal\nmodelforurbanspatio-temporalprediction.InProceedingsofthe30thACMSIGKDDConference\nonKnowledgeDiscoveryandDataMining,pp.4095–4106,2024a.\nYuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, and Yong Li. Spatio-temporal few-shot\nlearning via diffusive neural network generation. In The Twelfth International Conference on\nLearningRepresentations,2024b.\nJunboZhang,YuZheng,andDekangQi.Deepspatio-temporalresidualnetworksforcitywidecrowd\nflows prediction. In Proceedings of the AAAI conference on artificial intelligence, volume 31,\n2017.\nSi Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a\ncomprehensivereview. ComputationalSocialNetworks,6(1):1–23,2019.\nXu Zhang, Yongshun Gong, Xinxin Zhang, Xiaoming Wu, Chengqi Zhang, and Xiangjun Dong.\nMask-andcontrast-enhancedspatio-temporallearningforurbanflowprediction. InProceedings\nof the 32nd ACM International Conference on Information and Knowledge Management, pp.\n3298–3307,2023a.\nZijianZhang,XiangyuZhao,QidongLiu,ChunxuZhang,QianMa,WanyuWang,HongweiZhao,\nYiqi Wang, and Zitao Liu. Promptst: Prompt-enhanced spatio-temporal multi-attribute predic-\ntion. InProceedingsofthe32ndACMInternationalConferenceonInformationandKnowledge\nManagement,pp.3195–3205,2023b.\nLiangZhao,MinGao,andZongweiWang. St-gsp: Spatial-temporalglobalsemanticrepresentation\nlearningforurbanflowprediction.InProceedingsoftheFifteenthACMInternationalConference\nonWebSearchandDataMining,pp.1443–1451,2022.\nYuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. Difftraj: Generating\ngps trajectory with diffusion probabilistic model. Advances in Neural Information Processing\nSystems,36:65168–65188,2023.\nYuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian\nZhang, Xuetao Wei, and Yuxuan Liang. Controltraj: Controllable trajectory generation with\ntopology-constraineddiffusionmodel. InProceedingsofthe30thACMSIGKDDConferenceon\nKnowledgeDiscoveryandDataMining,pp.4676–4687,2024.\n14\nPreprint\nTable5: Basicstatisticsofgrid-baseddata.\nDataset City Type TemporalPeriod Spatialpartition Interval Mean Std\nFlowSH Shanghai Mobilityflow 2016/04/25-2016/05/01 20×20 15min 31.935 137.926\nPopBJ Beijing Crowdflow 2021/10/25-2021/11/21 28×24 Onehour 0.367 0.411\nTaxiBJ Beijing Taxiflow 2013/06/01-2013/10/30 32×32 Halfanhour 97.543 122.174\nCrowdNJ Nanjing Crowdflow 2021/02/02-2021/03/01 20×28 Onehour 0.872 1.345\nTaxiNYC NewYorkCity Taxiflow 2015/01/01-2015/03/01 10×20 Halfanhour 38.801 103.924\nPopSH Shanghai Dynamicpopulation 2014/08/01-2014/08/28 32×28 Onehour 0.175 0.212\nTable6: BasicstatisticsofGraph-baseddata.\nDataset City Type TemporalPeriod Interval #Nodes #Edges Mean Std\nSpeedSH Shanghai Trafficspeed 2022/01/27-2022/02/27 15min 21099 39065 7.815 4.044\nSpeedBJ Beijing Trafficspeed 2022/03/05-2022/04/05 15min 13675 24444 6.837 3.412\nSpeedNJ Nanjing Trafficspeed 2022/03/05-2022/04/05 15min 13419 25100 6.699 4.253\nA DATASETS\nWe provide a detailed overview of the datasets utilized in our study to support future research in\nthe field of urban spatio-temporal modeling. The datasets are categorized into two distinct types:\ngrid-basedand graph-basedspatio-temporal data. Eachtype ofdata reflects differentspatial orga-\nnizationsanddynamics, enablingacomprehensiveevaluationofmodelperformanceacrossvaried\nurbanscenarios.\nGrid-baseddatarepresentspatialinformationinastructured,uniformgridlayout,whereeachgrid\ncellcorrespondstoaspecificgeographicalarea. Table5outlinestheessentialdetailsandstatistics\nfor the grid-based datasets, including spatial resolution, temporal resolution, temporal period, and\nthesizeofeachdataset.\nGraph-baseddata,ontheotherhand,captureurbanspatialrelationshipsthroughanetworkofnodes\nand edges, where nodes typically represent points of interest (e.g., intersections or key locations),\nand edges represent the connections between them (e.g., roads or transit lines). This type of data\nis well-suited for modeling scenarios that involve irregular spatial structures, such as transporta-\ntion networks. Table 6 provides a comprehensive summary of the graph-based datasets, including\ninformationonthenumberofnodes,edges,temporalresolution,temporalperiod,anddatasetsize.\nB METHODOLOGY DETAILS\nB.1 SEQUENTIALFORMATOFINPUTDATA\nWeprovideadetaileddescriptionofthedataunificationprocessforbothgrid-basedandgraph-based\nspatio-temporaldata. Thekeygoalistotransformthedataintoaunifiedsequentialformatsuitable\nforthetransformer’sinput.\nGrid-baseddataisstructuredinauniformgridlayout,typicallyrepresentedinathree-dimensional\nform X ∈ RT×H×W with two spatial dimensions (height H and width W) and one temporal\ngrid\ndimensionT. Toprocessthisdata,weutilize3DConvolutionalNeuralNetworks(3DCNN),which\narewidelyusedforcapturingbothspatialandtemporaldependenciesinspatio-temporaltasks. The\nprocessisformulatedasfollows:\nX′ = CONV3D(X grid,kernelsize=(p t,p s,p s))\nX\np\n= RESHAPE(X′,[N])\nwhere N = T × H × W represents the total number of spatio-temporal partitions, effectively\npt ps ps\nconvertingthedataintoaone-dimensionalsequenceforfurtherprocessingbythetransformermodel.\nGraph-baseddataisinherentlynon-Euclidean,capturingrelationshipsbetweenurbanentities(e.g.,\nstreets and intersections). The spatial dimension is represented by a graph structure with nodes\n15\nPreprint\nand edges, and the temporal dimension is still captured as a time series at each node. The graph-\nbased data can be represented as a tensor X ∈ RN×T, where N is the number of nodes in\ngraph\nthe graph, and T is the number of time steps. To handle the temporal dimension, we first apply a\n1D convolutional network (1D CNN) along the time axis to capture local temporal dependencies.\nNext, to capture spatial relationships, we apply a Graph Convolutional Network (GCN) (Zhang\net al., 2019) on the graph structure. For each temporal patch, the GCN aggregates information\nfrom neighboring nodes using the graph’s adjacency matrix A ∈ RN×N. Finally, we reshape the\ngraph-baseddataintoasequentialformat. Theoperationsareformulatedasfollows:\nX′ = CONV1D(X graph,kernelsize=p t)\nX′ = GCN(X′,A,W)\nX\np\n= RESHAPE(X′,[M])\nwhere M represents the number of spatio-temporal patches, ensuring that the graph-based data is\ntransformedintoaone-dimensionalsequence,similartothegrid-baseddata. Thisunifiedsequential\nrepresentationallowsbothdatatypestobeprocessedconsistentlybythetransformermodel.\nB.2 UNIFIEDPROMPTLEARNING\nWeprovidedetailsofhowtoobtainthedata-drivenandtask-specificprompts.\nTime-domain patterns. Suppose the patched spatio-temporal data is denoted as X ∈ RT′×N′,\nwhere T′ = T and N′ = H × W. we extract time-domain patterns by applying an attention\npt ps ps\nmechanism along the temporal dimension. This is done independently for each spatial location,\nallowingustocapturetemporaldependenciesacrossdifferentspatialpatchesasfollows:\nX\nt\n= ATTENTION(XT),XT ∈RN′×T′ ,X\nt\n∈RN′×1×D\nwhereDistheembeddingsize.\nFrequency-domainpatterns. Inourwork,weemployfourdistinctapproachestocomputefeatures\ninthefrequencydomain, dependingontheconfigurationoftheFastFourierTransform(FFT)and\nthresholdingmechanisms:\n• Without FFT Threshold: we directly compute the FFT of the input tensor. The tensor is per-\nmutedalongtheappropriatedimensions, andtherealandimaginarycomponentsoftheFFTare\nconcatenated along the last dimension. This results in a frequency domain representation of the\ndata. Itisformulatedasfollows:\nX =FFT(X),\nFFT\nX =[ℜ(X ),ℑ(X )],\nfreq FFT FFT\nwhereℜ(X )representstherealpartoftheFFT,andℑ(X )representstheimaginarypart.\nFFT FFT\n• Basic FFT Threshold: we apply a basic threshold technique by computing the amplitude of\nthe FFT and creating a binary mask. The mask retains frequency components whose amplitude\nis greater than the mean amplitude, filtering out low-frequency noise and preserving significant\nfrequencycomponents. Theprocessisformulatedasfollow:\nX =FFT(X),\nFFT\n1 (cid:88)\nA=|X |, µ = A,\nFFT A H ×W ×T\nM =I(A>µ ), X =X ,⊙M,\nA FFT,filtered FFT\nX =[ℜ(X ),ℑ(X )].\nfreq FFT,filtered FFT,filtered\n• Quantile-basedFFTThreshold: Wefurtherrefinethefrequencyselectionbyapplyingathresh-\nold based on the 80t% of the amplitude distribution. This approach retains the most prominent\n16\nPreprint\nfrequencycomponents,allowingformoreflexiblefilteringcomparedtothemean-basedthreshold.\nTheselectionprocesscanbeformulatedasfollows:\nX =FFT(X),\nFFT\nA=|X |, q =Quantile(A,0.8),\nFFT 80\nM =I(A>q ), X =X ⊙M,\n80 FFT,filtered FFT\nX =[ℜ(X ),ℑ(X )].\nfreq FFT,filtered FFT,filtered\n• Top-kFrequencyFiltering:Weretainonlythetopkfrequencycomponents(e.g.,thefirstthree).\nWe generate a mask to preserve only these dominant components, filtering out the rest. It is\nformulatedasfollows:\nX =FFT(X), A=|X |,\nFFT FFT\nindices=argsort(A,descending)[:k],\nM =mask(indices), X =X ⊙M,\nFFT,filtered FFT\nX =[ℜ(X ),ℑ(X )].\nfreq FFT,filtered FFT,filtered\nSpatial patterns. For the same patched spatio-temporal data X ∈ RT′×N′, we extract spatial\npatterns by applying an attention mechanism along the spatial dimension, independently on each\ntemporal patch. This process allows us to model spatial dependencies within each time patch as\nfollows:\nX\ns\n= ATTENTION(X),X\n∈RT′×N′\n,X\nt\n∈RT′×1×D\nC EXPERIMENT DETAILS\nC.1 BASELINES\n• HA:HistoryAverageisaforecastingmethodthatpredictsfuturevaluesbycalculatingthemean\nofhistoricaldatafromthesametimeperiods.\n• MIM(Wangetal.,2019):Thismodelutilizesthedifferenceindatabetweenconsecutiverecurring\nstatestoaddressnon-stationarycharacteristics. BystackingmultipleMIMblocks,itcancapture\nhigher-ordernon-stationarityinthedata.\n• MAU(Changetal.,2021):TheMotion-awareUnitextendsthetemporalscopeofpredictionunits\nto seize correlations in motion between frames. It encompasses an attention mechanism and a\nfusionmechanism,whichareintegraltovideopredictiontasks.\n• SimVP(Gaoetal.,2022):Asimpleyeteffectivevideopredictionmodelisentirelybasedoncon-\nvolutionalneuralnetworksandemploysMSElossasitsperformancemetric,providingareliable\nbenchmarkforcomparativestudiesinvideoprediction.\n• TAU (Tan et al., 2023a): The Temporal Attention Module breaks down temporal attention into\ntwoparts: within-frameandbetween-frames, andemploysdifferentialdivergenceregularization\ntomanagevariationsacrossframes.\n• STResNet(Zhangetal.,2017): STResNetemploysresidualneuralnetworkstodetectproximity,\nperiodicity,andtrendsinthetemporaldata.\n• ACFM(Liuetal.,2018): TheAttentiveCrowdFlowMachinemodelforecastscrowdmovements\nbyusinganattentionmechanismtodynamicallyintegratesequentialandcyclicalpatterns.\n• STGSP(Zhaoetal.,2022): Thismodelhighlightsthesignificanceofglobalandpositionaltem-\nporal data for spatio-temporal forecasting. It incorporates a semantic flow encoder to capture\ntemporalpositioncuesandanattentionmechanismtohandlemulti-scaletemporalinteractions.\n• MC-STL (Zhang et al., 2023a): MC-STL utilizes mask-enhanced contrastive learning to effi-\ncientlyidentifyspatio-temporalrelationships.\n• STNorm(Dengetal.,2021):Itintroducestwodistinctnormalizationmodules:spatialnormaliza-\ntion for handling high-frequency elements and temporal normalization for managing local com-\nponents.\n17\nPreprint\n• STID(Shaoetal.,2022a):ThisMLP-basedspatio-temporalforecastingmodeldiscernssubtleties\nwithinthespatialandtemporalaxes,showcasingitsdesign’sefficiencyandefficacy.\n• PromptST(Zhangetal.,2023b): Anadvancedpre-trainingandprompt-tuningmethodologytai-\nloredforspatio-temporalforecasting.\n• UniST (Yuan et al., 2024a): A versatile urban spatio-temporal prediction model that uses grid-\nbased data. It employs various spatio-temporal masking techniques for pre-training and fine-\ntuningwithspatio-temporalknowledge-basedprompts.\n• STGCN(Yuetal.,2018): TheSpatio-TemporalGraphConvolutionalNetworkisadeeplearning\narchitecture for predicting traffic patterns, harnessing both spatial and temporal correlations. It\nintegratesgraphconvolutionaloperationswithconvolutionalsequencelearningtocapturemulti-\nscaledynamicswithintrafficnetworks.\n• GWN(Wuetal.,2019): GraphWaveNetisatechniquecraftedtoovercometheshortcomingsof\ncurrentspatial-temporalgraphmodelingmethods. Itintroducesaself-adjustingadjacencymatrix\nandutilizesstackeddilatedcausalconvolutionstoefficientlycapturetemporalrelationships.\n• MTGNN (Wu et al., 2020): MTGNN is a framework tailored for multivariate time series anal-\nysis. It autonomously identifies directional relationships between variables via a graph learning\ncomponentandincorporatesadditionalinformationsuchasvariableattributes.\n• GTS(Shang&Chen,2021): GTSisanapproachthatconcurrentlylearnsthetopologyofagraph\nalongsideaGraphNeuralNetwork(GNN)forpredictingmultipletimeseries. Itmodelsthegraph\nstructureusinganeuralnetwork,allowingforthegenerationofdistinctgraphsamples,andaims\ntooptimizetheaverageperformanceacrossthedistributionofgraphs.\n• DCRNN (Li et al., 2018): The Diffusion Convolutional Recurrent Neural Network is a deep\nlearningframeworkforspatiotemporalprediction. Ittreatstrafficflowasadiffusionphenomenon\nonadirectedgraph,securingspatialinterdependenciesviatwo-wayrandomwalksandtemporal\ninterdependenciesthroughanencoder-decodersetupwithscheduledsampling.\n• STEP(Shaoetal.,2022b):Spatial-temporalGraphNeuralNetworkEnhancedbyPre-trainingis\naframeworkthatusesapre-trainedmodeltoenhancespatial-temporalgraphneuralnetworksfor\nbetterforecastingofmultivariatetimeseriesdata.\n• AGCRN (Bai et al., 2020): The AGCRN framework improves upon Graph Convolutional Net-\nworksbyincorporatingtwoadaptivecomponents: NodeAdaptiveParameterLearningandData\nAdaptive Graph Generation. This approach effectively captures nuanced spatial and temporal\nrelationshipswithintrafficdata,functioningindependentlyofpre-setgraphstructures.\n• PatchTST (Nie et al., 2022): It employs patching and self-supervised learning techniques for\nforecasting multivariate time series. By dividing the time series into segments, it captures long-\ntermdependenciesandanalyzeseachdatachannelseparatelyusingaunifiednetworkarchitecture.\n• iTransformer(Liuetal.,2023b): Thisstate-of-the-artmodelformultivariatetimeseriesutilizes\nattentionmechanismsandfeed-forwardneuralnetworklayersoninverteddimensionstoempha-\nsizetherelationshipsamongmultiplevariables.\n• Time-LLM (Jin et al.): TIME-LLM represents an advanced approach in applying large-scale\nlanguagemodelstotimeseriesprediction.ItemploysareprogrammingstrategythatadaptsLLMs\nforforecastingtaskswithoutalteringtheunderlyinglanguagemodelarchitecture.\n• CSDI (Tashiro et al., 2021): CSDI is explicitly trained for imputation and can exploit correla-\ntionsbetweenobservedvalues,leadingtosignificantimprovementsinperformanceoverexisting\nprobabilisticimputationmethods.\n• Imputeformer (Nie et al., 2024): It introduces a low-rank inductive bias into the Transformer\nframeworktobalancestronginductivepriorswithhighmodelexpressivity,makingitsuitablefor\nawiderangeofimputationtasks.\n• Grin(Cinietal.,2022): GRINintroducesanovelgraphneuralnetworkarchitecturedesignedto\nreconstruct missing data in different channels of a multivariate time series, outperforming state-\nof-the-artmethodsinimputationtasks.\n• BriTS (Cao et al., 2018): BRITS is a method for imputing missing values in time series data,\nutilizing a bidirectional recurrent neural network (RNN) without imposing assumptions on the\ndata’sunderlyingdynamics.\n18\nPreprint\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nCSDI 17.40 33.98 10.65 31.88 4.83 15.43 0.094 0.16 0.082 0.14\nUrbanDiT 11.57 20.08 5.996 14.37 4.71 15.07 0.16 0.099 0.071 0.117\nTable 7: Performance comparison for grid-based backward prediction evaluated using MAE and\nRMSE.\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nCSDI 11.20 18.42 5.71 13.14 3.86 11.59 0.055 0.092 0.044 0.077\nImputeformer 11.99 19.83 6.72 15.69 5.61 16.72 0.079 0.16 0.066 0.11\nGrin 13.69 23.45 9.61 26.28 8.10 21.32 0.10 0.18 0.083 0.16\nBriTS 17.57 27.63 15.24 28.40 19.41 50.25 0.19 0.28 0.16 0.25\nUrbanDiT(ours) 9.09 14.54 4.90 10.308 4.50 11.46 0.077 0.121 0.056 0.094\nTable8: PerformancecomparisonfortemporalinterpolationevaluatedusingMAEandRMSE.The\nresultsrepresenttheaverageerrorsacrossdifferentinterpolationsteps.\nItisworthnotingthatthebaselines,includingUniST(Yuanetal.,2024a)andPatchTST(Nieetal.,\n2022), can also be trained using multiple datasets. In our comparison experiments, we train these\nmodels in a unified manner using the same diverse datasets to ensure a fair comparison. This ap-\nproachensuresthattheperformancegainsofUrbanDiTarenotmerelyduetodatasetdiversity,but\nreflectthemodel’strueadvantage.\nC.2 EXPERIMENTCONFIGURATION\nForUrbanDiT-S(small),themodelconsistsof4transformerlayerswithahiddensizeof256. Both\nthespatialandtemporalpatchsizesaresetto2,andthenumberofattentionheadsis4. UrbanDiT-\nM(medium)iscomposedof6transformerlayerswithahiddensizeof384,maintainingthesame\nspatialandtemporalpatchsizesof2,and6attentionheads. UrbanDiT-L(large)includes12trans-\nformer layers, a hidden size of 384, spatial and temporal patch sizes of 2, and 12 attention heads.\nEachmemorypoolcontains512embeddings,withtheembeddingdimensionmatchingthemodel’s\nhidden size. The learning rate is set to 1e-4, and the maximum number of training epochs is 500,\nwith early stopping applied to prevent overfitting. The batch size is tailored for each dataset to\nmaintainasimilarnumberoftrainingiterationsacrossthem.\nC.3 METRICS.\nTo assess the performance of UrbanDiT in urban spatio-temporal applications, we employ widely\nrecognizedevaluationmetrics: RootMeanSquareError(RMSE)andMeanAbsoluteError(MAE).\nGiven that UrbanDiT operates as a probabilistic model, we conduct 20 inference runs and use the\naverageresultforcomparisonagainstthegroundtruth. Weapplythesameevaluationframeworkto\ntheprobabilisticbaselines,ensuringaconsistentandfairassessmentofallmodels.\nD ADDITIONAL RESULTS\nD.1 RESULTSOFMULTIPLETASKS\nTable7toTable10illustrateadditionalresultsofmultipletasks.\nD.2 FEW-SHOTANDZERO-SHOTPERFORMANCE\nFigure9demonstratesUrbanDiT’sfew-shotandzero-shotcapabilitiesontheTaxiBJdataset.\n19\nPreprint\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nCSDI 12.29 22.07 7.94 21.86 4.33 13.09 0.071 0.12 0.055 0.094\nImputeformer 13.65 23.18 9.22 19.97 5.95 16.36 0.093 0.16 0.069 0.12\nGrin 16.83 27.61 9.70 23.52 9.15 21.43 0.16 0.30 0.096 0.18\nBriTS 22.57 38.39 17.14 38.82 19.93 50.47 0.26 0.41 0.18 0.29\nUrbanDiT(ours) 9.38 15.19 5.03 11.52 4.62 12.16 0.083 0.13 0.061 0.101\nTable 9: Performance comparison for temporal imputation evaluated using MAE and RMSE. The\nresultsrepresenttheaverageerrorsacrossdifferentimputationsteps.\nTaxiBJ FlowSH TaxiNYC CrowdNJ PopBJ\nModel MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE\nCSDI 7.92 12.42 4.28 8.62 3.86 11.54 0.057 0.091 0.046 0.083\nImputeformer 9.70 13.80 5.50 10.30 4.79 15.35 0.076 0.12 0.061 0.11\nGrin 11.96 19.62 9.21 19.68 9.62 20.77 0.11 0.19 0.080 0.14\nBriTS 13.99 23.53 17.95 38.57 19.17 50.15 0.21 0.44 0.13 0.19\nUrbanDiT(ours) 7.83 12.13 5.07 9.79 3.63 11.44 0.057 0.090 0.049 0.092\nTable10:Performancecomparisonforgrid-basedspatio-temporalimputationevaluatedusingMAE\nandRMSE.Theresultsrepresenttheaveragepredictionerrorsacrossdifferentpredictionsteps.\n(a) TaxiBJ-5% few-shot (b) TaxiBJ-10% few-shot\nFigure 9: Evaluation of UrbanDiT and baseline models in 5% and 1% few-shot scenarios on the\nTaxiBJdataset. ThereddashedlineindicatesUrbanDiT’szero-shotperformance\n20",
    "pdf_filename": "UrbanDiT_A_Foundation_Model_for_Open-World_Urban_Spatio-Temporal_Learning.pdf"
}