{
    "title": "UrbanDiT A Foundation Model for Open-World Urban Spatio-Temporal Learning",
    "context": "The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions. Effectively modeling these dynamics is essential for understanding and optimizing urban systems. In this work, we introduce UrbanDiT, a foundation model for open-world urban spatio- temporal learning that successfully scale up diffusion transformers in this field. UrbanDiT pioneers a unified model that integrates diverse spatio-temporal data sources and types while learning universal spatio-temporal patterns across dif- ferent cities and scenarios. This allows the model to unify both multi-data and multi-task learning, and effectively support a wide range of spatio-temporal ap- plications. Its key innovation lies in the elaborated prompt learning framework, which adaptively generates both data-driven and task-specific prompts, guiding the model to deliver superior performance across various urban applications. UrbanDiT offers three primary advantages: 1) It unifies diverse data types, such as grid-based and graph-based data, into a sequential format, allowing to cap- ture spatio-temporal dynamics across diverse scenarios of different cities; 2) With masking strategies and task-specific prompts, it supports a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spa- tial extrapolation, and spatio-temporal imputation; and 3) It generalizes effec- tively to open-world scenarios, with its powerful zero-shot capabilities outper- forming nearly all baselines with training data. These features allow UrbanDiT to achieves state-of-the-art performance in different domains such as transportation traffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multi- ple cities and tasks. UrbanDiT sets up a new benchmark for foundation models in the urban spatio-temporal domain. Code and datasets are publicly available at https://github.com/YuanYuan98/UrbanDiT. 1 Crowd Flow Taxi Demand Bike Usage Transportation Cellular Traffic Grid data Graph data UrbanDiT Data Prompt Task Prompt Bi-directional Prediction Temporal Interpolation Spatial Extrapolation ST Imputation Diverse Urban Spatio-Temporal Data Multiple tasks Figure 1: A diagram of our proposed UrbanDiT utilizing data and task prompts. It is a foundation model that integrates diverse data sources and types while simultaneously performing multiple tasks. The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions within the city. These dynamics are reflected in different types of 1 arXiv:2411.12164v1  [cs.LG]  19 Nov 2024",
    "body": "Preprint\nURBANDIT: A FOUNDATION MODEL\nFOR OPEN-\nWORLD URBAN SPATIO-TEMPORAL LEARNING\nYuan Yuan, Chonghua Han, Jingtao Ding, Depeng Jin, Yong Li\nDepartment of Electronic Engineering\nTsinghua University\nBeijing, China\ny-yuan20@mails.tsinghua.edu.cn, liyong07@tsinghua.edu.cn\nABSTRACT\nThe urban environment is characterized by complex spatio-temporal dynamics\narising from diverse human activities and interactions. Effectively modeling these\ndynamics is essential for understanding and optimizing urban systems. In this\nwork, we introduce UrbanDiT, a foundation model for open-world urban spatio-\ntemporal learning that successfully scale up diffusion transformers in this field.\nUrbanDiT pioneers a unified model that integrates diverse spatio-temporal data\nsources and types while learning universal spatio-temporal patterns across dif-\nferent cities and scenarios. This allows the model to unify both multi-data and\nmulti-task learning, and effectively support a wide range of spatio-temporal ap-\nplications. Its key innovation lies in the elaborated prompt learning framework,\nwhich adaptively generates both data-driven and task-specific prompts, guiding\nthe model to deliver superior performance across various urban applications.\nUrbanDiT offers three primary advantages: 1) It unifies diverse data types, such\nas grid-based and graph-based data, into a sequential format, allowing to cap-\nture spatio-temporal dynamics across diverse scenarios of different cities; 2) With\nmasking strategies and task-specific prompts, it supports a wide range of tasks,\nincluding bi-directional spatio-temporal prediction, temporal interpolation, spa-\ntial extrapolation, and spatio-temporal imputation; and 3) It generalizes effec-\ntively to open-world scenarios, with its powerful zero-shot capabilities outper-\nforming nearly all baselines with training data. These features allow UrbanDiT to\nachieves state-of-the-art performance in different domains such as transportation\ntraffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multi-\nple cities and tasks. UrbanDiT sets up a new benchmark for foundation models\nin the urban spatio-temporal domain. Code and datasets are publicly available at\nhttps://github.com/YuanYuan98/UrbanDiT.\n1\nINTRODUCTION\nCrowd Flow\nTaxi Demand\nBike Usage\nTransportation\nCellular Traffic\nGrid data\nGraph data\nUrbanDiT\nData\nPrompt\nTask\nPrompt\nBi-directional Prediction\nTemporal Interpolation\nSpatial Extrapolation\nST Imputation\nDiverse Urban Spatio-Temporal Data\nMultiple tasks\nFigure 1: A diagram of our proposed UrbanDiT utilizing data and task prompts. It is a foundation\nmodel that integrates diverse data sources and types while simultaneously performing multiple tasks.\nThe urban environment is characterized by complex spatio-temporal dynamics arising from diverse\nhuman activities and interactions within the city. These dynamics are reflected in different types of\n1\narXiv:2411.12164v1  [cs.LG]  19 Nov 2024\n\nPreprint\nTable 1: Comparison between existing models and UrbanDiT across five aspects.\nMethod\nModel Init.\nData Type\nData Source[1]\nTask Flexibility\nZero-shot\nGPD (Yuan et al., 2024b)\nScratch\nGraph\n×\n×\n×\nUniST (Yuan et al., 2024a)\nScratch\nGrid\n✓\n×\n✓\nUrbanGPT (Li et al., 2024)\nLLMs\nGrid\n✓\n×\n✓\nCityGPT (Feng et al., 2024a)\nLLMs\nLanguages\n×\n✓\n×\nUrbanDiT\nScratch\nGraph/Grid\n✓\n✓\n✓\n[1]: Whether leverage diverse data sources.\ndata. For example, grid-based data divides urban space into regular cells, often used to track crowd\nflows. In contrast, graph-based data represents spatial structures like road networks as nodes and\nedges, such as traffic speeds on roads. These data sources usually come from different cities, each\nwith unique layouts, infrastructures, and planning strategies. Effectively modeling these diverse\nspatio-temporal dynamics is crucial for optimizing urban services and understanding how cities\nfunction and evolve. Therefore, it raises an essential research question: can we develop a foundation\nmodel, similar to those in natural language processing (Touvron et al., 2023; Brown et al., 2020)\nand computer vision (Brooks et al., 2024; Liu et al., 2023a; Esser et al., 2024), that learns universal\nspatio-temporal patterns and serves as a general-purpose model for various urban applications?\nIn the context of urban spatio-temporal modeling, recent advancements such as GPD (Yuan et al.,\n2024b), UrbanGPT (Li et al., 2024), and UniST (Yuan et al., 2024a) have opened exciting avenues\nfor understanding complex urban dynamics. As compared in Table 1, these models either utilize\nLLMs (Li et al., 2024) or develop unified models from scratch (Yuan et al., 2024a;b) tailored for ur-\nban spatio-temporal predictions. By training on multiple datasets, they have showcased impressive\ngeneralization capabilities. However, their focus remains largely on prediction tasks, and they are\noften restricted to specific data types—such as grid-based data (Li et al., 2024; Yuan et al., 2024a)\nor graph-based traffic data (Yuan et al., 2024b). Thus, realizing the full potential of foundation mod-\nels capable of seamlessly handling diverse data types, sources, and tasks in open-world scenarios\nremains an open and largely unexplored area of research.\nUrban spatio-temporal data is typically defined by diverse properties, including varying spatial res-\nolutions, temporal dynamics, and complex interactions among entities. Building an effective foun-\ndation model requires a scalable architecture capable of accommodating these complexities. More-\nover, the intricate nature of urban spatio-temporal dynamics necessitates a model that can learn from\ncomplex data distributions. Diffusion Transformers, exemplified by models like Sora (Brooks et al.,\n2024), offer a compelling solution for this purpose. By combining the generative power of diffu-\nsion processes with the scalability and flexibility of transformer architectures, diffusion transformers\npresent a promising backbone.\nIn this work, we introduce UrbanDiT, which unifies training across diverse urban scenarios and\ntasks, effectively scaling up diffusion transformers for comprehensive urban spatio-temporal learn-\ning. It offers three appealing benefits: 1) It unifies diverse data types into a sequential format, allow-\ning it to capture spatio-temporal patterns across various cities and domains, guided by data-driven\nprompts that highlight critical patterns. 2) It supports a wide range of tasks with a single model,\nusing masking strategies and task-specific prompts, without the need for re-training across different\ntasks. 3) It generalizes well to open-world scenarios, exhibiting powerful zero-shot performance. To\nachieve this, we first unify different input data by converting it into the sequential format. We build\nthe denoising network using transformer blocks, equipped with both temporal and spatial attention\nmodules. To integrate diverse data types and tasks, we propose a unified prompt learning framework\nthat enhances the denoising process. This framework maintains memory pools to capture learned\nspatio-temporal patterns and generate data-driven prompts, while also create task-specific prompts\nfor various spatio-temporal tasks. These prompts are concatenated into the unified sequential input\nbefore being fed into the transformer modules. The design of prompt learning serves as a flexible\nintermediary, adaptable to a wide range of scenarios.\nUrbanDiT, built on the DiT backbone with a prompt learning framework, is a pioneering open-\nworld foundation model. It excels at handling diverse urban spatio-temporal data and a wide range\nof tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spatial extrapo-\nlation, and spatio-temporal imputation. This makes UrbanDiT a powerful and universal solution for\nvarious urban spatio-temporal applications. We summarize our contributions as follows:\n2\n\nPreprint\n• To the best of our knowledge, we are the first to explore a foundation model for general-purpose\nurban spatio-temporal learning, integrating diverse spatio-temporal data types and multiple urabn\ntasks within a single unified model.\n• We present UrbanDiT, an open-world foundation model built on diffusion transformers. Through\nour proposed prompt learning, UrbanDiT effectively brings together heterogeneous spatio-\ntemporal data and tasks, using data-driven and task-specific prompts to enhance performance.\n• Extensive experiments demonstrate that UrbanDiT effectively captures complex urban spatio-\ntemporal dynamics, achieving state-of-the-art performance across multiple datasets and tasks.\nIt also exhibits powerful zero-shot capabilities, proving its applicability in open-world settings.\nUrbanDiT marks a significant step forward in the advancement of urban foundation models.\n2\nRELATED WORK\n2.1\nURBAN SPATIO-TEMPORAL LEARNING\nUrban spatio-temporal learning encompass a variety of tasks such as prediction (Tan et al., 2023b;\nBai et al., 2020; Yuan et al., 2023; Li et al., 2018; Zhang et al., 2017), interpolation (Aumond et al.,\n2018; Gr¨aler et al., 2016), extrapolation (Miller et al., 2004; Ma et al., 2019), and imputation (Tashiro\net al., 2021; Hu et al., 2023), addressing how urban systems evolve across space and time. Deep\nlearning has achieved significant progress in these areas, with techniques ranging from CNNs (Li\net al., 2018; Zhang et al., 2017), RNNs (Wang et al., 2017; 2018; Lin et al., 2020), MLPs (Shao\net al., 2022a), GNNs (Bai et al., 2020; Geng et al., 2019), and Transformers (Chen et al., 2022;\nJiang et al., 2023), to the more recent use of diffusion models (Yuan et al., 2023; 2024b; Tashiro\net al., 2021; Wen et al., 2023). Each of these approaches has been employed to model complicated\nspatio-temporal relationships inherent to urban environments. However, most existing models are\ntailored to specific datasets and tasks. In contrast, our approach is designed to handle multiple tasks\nand generalize across diverse urban scenarios without the need for re-training on new datasets.\n2.2\nURBAN FOUNDATION MODELS\nFoundation models have made significant progress in language models (Touvron et al., 2023; Brown\net al., 2020) and image generation (Brooks et al., 2024; Liu et al., 2023a; Esser et al., 2024). Re-\ncently, researchers have extended the concept of foundation models to urban environments, aiming\nto address unique challenges of urban spatio-temporal data. Some representative works in this area\ninclude UrbanGPT (Li et al., 2024), UniST (Yuan et al., 2024a), and CityGPT (Feng et al., 2024b).\nUrbanGPT introduces LLMs designed for spatio-temporal predictions within urban contexts. UniST\ndevelops a foundation model from scratch specifically for urban prediction tasks, demonstrating\nzero-shot capabilities that allow the model to generalize to new scenarios without additional train-\ning. CityGPT, on the other hand, focuses on enhancing the LLM’s ability to comprehend and solve\nurban tasks by improving its understanding of urban spaces. Table 1 provides a comparison of key\nabilities across existing urban foundation models and UrbanDiT. As shown, UrbanDiT is trained\nfrom scratch, allowing it to fully leverage data diversity while offering flexibility across a wide\nrange of tasks. Additionally, it demonstrates emergent zero-shot capabilities. Compared to previous\nefforts, UrbanDiT represents a significant advancement in developing urban foundation models.\n2.3\nDIFFUSION MODELS FOR SPATIO-TEMPORAL DATA\nDiffusion models, originally popularized in image generation, have recently gained attention in han-\ndling spatio-temporal data and time series. They iteratively add and remove noise from data, allow-\ning them to capture complex patterns across both temporal and spatial dimensions (Yang et al., 2024;\nYuan et al., 2023; Hu et al., 2023; Wen et al., 2023; Rasul et al., 2021). In the context of time series,\ndiffusion models have been applied to tasks such as forecasting (Kollovieh et al., 2024; Rasul et al.,\n2021) and imputation (Xiao et al., 2023; Tashiro et al., 2021), outperforming traditional methods\nby generating more accurate and coherent sequences. For spatio-temporal data, diffusion models\nhave proven useful in a variety of tasks, including traffic prediction (Wen et al., 2023), environ-\nmental monitoring (Yuan et al., 2023), and human mobility generation (Zhu et al., 2024; 2023). By\neffectively modeling spatio-temporal dependencies, these models can capture both the spatial corre-\nlations and temporal dynamics inherent in urban systems. UrbanDiT leverages the generative power\n3\n\nPreprint\nSpatio-temporal \npatching\nEmbedding\nlayer\nNoisy\nST data\nTimestep\n𝑡\nReshape\nUrban Diffusion Transformer\nMultiple Domains\nMultiple Cities\n(a) Datasets                      (b) Urban Diffusion Transformer                     (c) Masking Strategies\n(d) Unified Prompt Learning\nMask\nST\nData\nFrequency pattern\nTemporal pattern\nData \nmemory\nConcat\nTask\nmemory\nAttention\nOutput\nData-driven prompt\nTask-specific prompt\nSequential\nformat\nDifferent \ndata types\nSpatial pattern\nFigure 2: Illustration of the whole framework of UrbanDiT, including four key components: a)\nUnifying different urban spatio-temporal data types; b) The diffusion pipeline of our UrbanDiT; c)\nDifferent masking strategies to specify different tasks; d) Unified prompt learning with data-driven\nand task-specific prompts to enhance the denoising process.\nof diffusion models to capture complex urban spatio-temporal patterns, while its flexible condition-\ning mechanisms allow it to address a wide range of spatio-temporal tasks. This makes UrbanDiT a\nsignificant advancement in applying diffusion models to urban spatio-temporal challenges.\n3\nMETHOD\n3.1\nPRELIMINARY\nUrban Spatio-Temporal Data. Urban spatio-temporal data typically falls into two categories: grid-\nbased and graph-based data. Grid-based data is structured in a uniform grid layout. Graph-based\ndata, on the other hand, highlights connectivity, capturing the relationships between various urban\nentities like streets and intersections. For both different spatial organizations, the temporal dimen-\nsion is characterized as time series data. The data can be denoted as XN×T , where N denotes the\nnumber of spatial partitions. For graph-based data, N corresponds to the number of nodes, while for\ngrid-based data, it is defined as the product of the height and width of the grid (N = H × W). This\nenables a unified representation of urban spatio-temporal data with different spatial organizations.\nUrban Spatio-Temporal Tasks. In addition to the commonly recognized (1) forward prediction\ntask, urban spatio-temporal analysis encompasses several other critical tasks. (2) Backward Predic-\ntion involves estimating past states based on current or future data. It is essential for understanding\nhistorical trends and validating predictive models. (3) Temporal Interpolation aims to estimate val-\nues at unobserved time points within a known temporal range. (4) Spatial Extrapolation involves\npredicting values beyond the observed spatial domain. It is important for assessing potential changes\nin urban environments and planning for future developments. (5) Spatio-Temporal Imputation refers\nto the process of filling in missing values in spatio-temporal datasets.\n3.2\nOVERALL FRAMEWORK\nFigure 2 illustrates the overall framework of our proposed UrbanDiT, which is based on diffusion\ntransformers. This framework seamlessly integrates various data types and tasks into a cohesive\nmodel.\n4\n\nPreprint\nForward Prediction\nBackward Prediction\nTemporal Imputation\nTemporal Interpolation\nSpatial Extrapolation\nSpatio-Temporal Imputation\nUrbanDiT\nFigure 3: Masking strategies to specify various urban spatio-temporal tasks.\nUnification of Data and Tasks. We convert data, characterized by a three-dimensional structure (2D\nspatial and 1D temporal dimensions), into a unified sequential format. For the temporal dimension,\nwe employ patching techniques commonly used in foundational models for time series (Nie et al.,\n2022). For grid-based data, we apply 2D patching methods, which are widely utilized in image\nprocessing, to organize the data. This allows us to rearrange the three-dimensional grid data into a\none-dimensional sequential format. For graph-based data, we use Graph Convolutional Networks\n(GCN) (Zhang et al., 2019) to process each node and integrate it with the temporal dimension to\nreshape the data into a one-dimensional format as well. More details of data unification can be\nfound in Appendix B.1\nTo adapt to various tasks, we employ a unified masking strategy. As illustrated in Figure 3, these\ntasks can be framed as reconstructing missing parts of the data, with distinct masking strategies\ntailored to each task. For Forward Prediction, we mask future time steps while utilizing past and\npresent data points to predict the missing values. Conversely, for Backward Prediction, we mask\npast time steps to estimate historical values based on current and future observations. In the case\nof temporal interpolation tasks, we apply masks to specific time points within a continuous series,\nallowing the model to fill in these gaps. For spatio-temporal imputation, we randomly mask missing\nvalues across both spatial and temporal dimensions, enabling the model to leverage surrounding\ncontext for accurate estimations. Finally, in spatial extrapolation tasks, we mask areas outside the\nobserved spatial domain to predict values for unobserved regions based on existing spatial patterns.\nConsequently, the input of the denoising network Xt is represented as the concatenation of noise\nfeatures and unmasked spatio-temporal data (conditional observations):\nXt = Xt ∗(1 −M) + X0 ∗M\nwhere Xt denotes the noise features, M is the mask that controls the availability of values for\ndownstream tasks, and X0 represents the clean values of the spatio-temporal data. In this way, we\ncan modulate different masks M to facilitate various urban spatio-temporal applications.\nSequential Input of Spatio-Temporal Data. We first apply temporal patching to process time\nseries data at each spatial location, represented as XN×T ′×D = CONV(XN×T ×D), where T ′ = T\npt\nand pt is the temporal patch size. Next, for grid-based data, we implement 2D spatial patching,\nresulting in Xp = CONV2D(XH×W ×T ′×D), where Xp ∈RL×D, L = H×W ×T\nps×ps×pt . In this way, we\neffectively reorganize the data into a format that is conducive to transformer architectures.\nSpatio-Temporal Transformer Block. The overall model is composed of multiple spatio-temporal\ntransformer blocks. Each block features both temporal multi-head attention and spatial multi-head\nattention, with spatial and temporal attention mechanisms operating independently. This design\nchoice is made to enhance computational efficiency, as the complexity of attention scales with the\nsquare of the sequence length.\nDiffusion Transformer. We adopt the diffusion transformer model, which integrates a denoising\nnetwork designed to process complex inputs effectively. The inputs to the denoising network consist\nof three key components: the noisy spatio-temporal data, the timestep, and the prompt. For the\ntimestep t, we utilize them for layer normalization following previous practices (Peebles & Xie,\n2023; Lu et al.), which helps stabilize and standardize the input features at each timestep. The\nprompt, which provides contextual information or guidance for the model, is concatenated with the\n5\n\nPreprint\ninput data to enhance the model’s understanding of the data and task at hand. This concatenation is\nstraightforward due to the transformer’s capability to manage variable sequence lengths, providing\nflexibility in processing diverse inputs. By incorporating these elements, the diffusion transformer\nmodel effectively learns to denoise and generate robust desired results in spatio-temporal contexts.\n3.3\nUNIFIED PROMPT LEARNING\nWe propose a unified prompt learning framework to enhance the diffusion transformers’ universality\nacross various data types and tasks.\nPrompt\n…\nKey\nKey\nKey\nMemory Pool\n…\nValue\nValue\nValue\nCosine similarity\nWeighted sum\nPattern\nFigure 4: Key-value structure of memory\npools.\nData-Driven Prompt. The data-driven prompt is cru-\ncial for training a unified model with multiple and di-\nverse datasets, as such datasets often exhibit signifi-\ncant variations in patterns and distributions. In this\ncontext, the prompt acts as a guiding mechanism, help-\ning the model to effectively navigate these differences\nand generate accurate results.\nSimilar to retrieval-\naugmented generation, prompts retrieve the most rele-\nvant information, enhancing the model’s ability to con-\ntextualize and interpret spatio-temporal data. By align-\ning the model’s learning process with the specific char-\nacteristics of various spatio-temporal patterns, data prompts ensure that UrbanDiT can adaptively\nrespond to a wide range of urban spatio-temporal scenarios.\nTo achieve this goal, we employ memory networks, specifically utilizing three memory pools de-\nsigned to capture the time-domain, frequency-domain and spatial patterns of spatio-temporal dynam-\nics. For different input data, the prompt network retrieves prompts from these memory pools based\non the respective time-domain, frequency-domain, and spatial patterns. As shown in Figure 4, each\nmemory pool is structured as a key-value store (Kt, Vt) = {(k1\nt , v1\nt ), ..., (kN\nt , vN\nt )}, (Kf, Vf) =\n{(k1\nf, v1\nf), ..., (kN\nf , vN\nf )}, (Ks, Vs) = {(k1\ns, v1\ns), ..., (kN\ns , vN\ns )}, where both keys and values are\nlearnable embeddings and randomly initialized. The data-driven prompts are generated as follows:\nαt = SOFTMAX(Xt, Kt),\nPt =\nX\nαt · Vt,\nαf = SOFTMAX(Xf, Kf),\nPf =\nX\nαf · Vf,\nαf = SOFTMAX(Xs, Ks),\nPs =\nX\nαs · Vs,\nX = CONCAT(Pt, Pf, X).\nTask-Specific Prompt. We also design task-specific prompts to enhance the model’s performance\nacross different tasks. These prompts are generated from the mask, and we employ attention mech-\nanisms to obtain the mask prompt Pm from the mask map as Pm = ATTENTION(FLATTEN(M)).\nThe learned pattern Pm is then concatenated with the input sequence, resulting in X\n=\nCONCAT(Pm, X). This enables the model to effectively incorporate task-specific information.\nWe provide more details of data-driven task-specific prompts in Appendix B.2\n3.4\nTRAINING AND INFERENCE\nThe training process alternates between multiple datasets and tasks. In each iteration, we randomly\nselect a dataset and a corresponding task to perform gradient descent training. This approach en-\nhances the model’s robustness by exposing it to diverse scenarios and helps prevent overfitting by\nensuring the model learns from a wide range of inputs and objectives. Let D = {D1, D2, . . . , Dm}\nrepresent the set of datasets, and T = {T1, T2, . . . , Tk} denote the set of tasks. Let L(di, ti) be the\nloss function for the chosen dataset di and task ti, with the model parameters denoted as θ. Overall,\nthe training process can be summarized as follows:\nFor i = 1 to N :\ndi ∼Uniform(D),\nti ∼Uniform(T)\n⇒\nθ ←θ −η∇L(di, ti; θ)\nwhere N is the total number of training iterations and η is the learning rate.\n6\n\nPreprint\nFor the training of the UrbanDiT model, we adopt a novel diffusion training approach proposed\nby the InstaFlow (Liu et al., 2023a), which significantly improves the efficiency of spatio-temporal\ndata generation. By employing rectified flow, it is an ordinary differential equation (ODE)-based\nframework that aligns the noise and data distributions through a straightened trajectory, as opposed\nto the curved paths often seen in traditional models.\n4\nPERFORMANCE EVALUATIONS\n4.1\nEXPERIMENTAL SETTINGS\nDatasets. We utilize a diverse set of datasets from multiple domains and cities to evaluate urban\nspatio-temporal applications. These domains include taxi demand, cellular network traffic, crowd\nflows, transportation traffic, and dynamic population, reflecting a broad spectrum of urban activities.\nThe datasets are sourced from different cities such as New York City (USA), Beijing, Shanghai, and\nNanjing (China), each representing unique urban characteristics. These datasets vary significantly\nin their spatial structures (e.g., grid or graph formats), the number of locations, and their spatial\nand temporal resolutions. These variations are influenced by differences in city structures, urban\nplanning strategies, and data collection methodologies across regions. For a detailed summary of\nthe datasets, please refer to Table 5 and Table 6 in Appendix A.\nWe split the datasets into training, validation, and testing sets along the temporal dimension, using\na 6:2:2 ratio. To ensure no overlap between these sets, we carefully remove any overlapping points,\nensuring clear separation across the temporal splits for evaluation.\nBaselines. To evaluate the performance of UrbanDiT, we establish a comprehensive benchmark,\ncomparing it against state-of-the-art models across different urban tasks.\nFor prediction tasks,\nwe include both traditional time series models such as Historical Average (HA) and ARIMA, as\nwell as advanced deep learning-based spatio-temporal models like STResNet (Zhang et al., 2017),\nACFM (Liu et al., 2018), STNorm (Deng et al., 2021), STGSP (Zhao et al., 2022), MC-STL (Zhang\net al., 2023a), PromptST (Zhang et al., 2023b), STID (Shao et al., 2022a), and UniST (Yuan et al.,\n2024a). Additionally, we compare against leading video prediction models, including SimVP (Gao\net al., 2022), TAU (Tan et al., 2023a), MAU (Chang et al., 2021), and MIM (Wang et al., 2019),\nas well as recent time series forecasting approaches such as PatchTST (Nie et al., 2022), iTrans-\nformer (Liu et al., 2023b), Time-LLM (Jin et al.), and the diffusion-based model CSDI (Tashiro\net al., 2021). For graph-based datasets, we evaluate UrbanDiT against cutting-edge spatio-temporal\ngraph models, including STGCN (Yu et al., 2018), DCRNN (Li et al., 2018), GWN (Wu et al.,\n2019), MTGNN (Wu et al., 2020), AGCRN (Bai et al., 2020), GTS (Shang & Chen, 2021), and\nSTEP (Shao et al., 2022b). Furthermore, for spatio-temporal imputation tasks, we compare our\nmodel with state-of-the-art baselines such as CSDI, ImputeFormer (Nie et al., 2024), Grin (Cini\net al., 2022), and BriTS (Cao et al., 2018), adapting these methods for temporal interpolation and\nspatial extrapolation tasks where applicable. We provide more details of baselines in Appendix C.1\n4.2\nCOMPARISON TO THE STATE-OF-THE-ART\nBi-directional Spatio-Temporal Prediction. For this task, we set both the historical input win-\ndow and prediction horizon to 12 time steps. Depending on the dataset, the temporal granularity\nvaries—12 steps may correspond to 1 hour for datasets with 5-minute intervals, 6 hours for datasets\nwith 30-minute intervals, and 12 hours for those with 1-hour intervals. For baselines that cannot\nhandle datasets with different shapes, we train individual models for each dataset.For more flexible\nmodels like UniST and PatchTST, we train a single unified model across multiple datasets.\nTable 2 provides a comprehensive benchmark for forward prediction on grid-based data. As ob-\nserved, traditional deep learning models such as STResNet, ACFM, and MC-STL, do not deliver\ncompetitive performance. Similarly, video prediction models, such as MAU, MIM, and SimVP,\nreveal limitations, suggesting the difference between urban spatio-temporal dynamics and those in\nconventional video data. UniST demonstrates relatively strong performance, suggesting that train-\ning a universal model across different datasets holds potential for improving prediction accuracy.\nHowever, time-series forecasting models struggled to capture the complex spatial interactions in-\nherent in urban environments, indicating that precisely modeling these interactions is critical for\nachieving better results in urban spatio-temporal prediction. Notably, CSDI ranks second in most\n7\n\nPreprint\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nHA\n53.03\n91.55\n13.43\n38.92\n26.49\n77.10\n0.48\n0.93\n0.232\n0.343\nARIMA\n57.5\n291\n9.15\n26.70\n23.91\n99.22\n0.443\n0.989\n0.236\n0.404\nSTResNet\n26.55\n37.96\n45.63\n59.82\n14.81\n26.88\n0.511\n0.718\n0.546\n0.751\nACFM\n19.87\n30.95\n24.95\n46.92\n9.85\n20.82\n0.284\n0.468\n0.141\n0.200\nSTNorm\n19.00\n31.21\n11.88\n28.46\n10.43\n26.94\n0.231\n0.384\n0.132\n0.198\nSTGSP\n17.54\n27.31\n17.54\n38.77\n10.52\n25.94\n0.263\n0.410\n0.157\n0.229\nMC-STL\n28.51\n38.50\n33.83\n46.06\n26.01\n36.75\n0.727\n0.504\n0.235\n0.311\nMAU\n46.37\n71.07\n21.38\n45.04\n21.79\n49.15\n0.402\n0.648\n0.166\n0.256\nMIM\n42.40\n68.18\n22.49\n47.29\n9.151\n24.53\n0.399\n0.715\n0.214\n0.298\nSimVP\n21.67\n35.58\n15.87\n28.59\n9.08\n19.69\n0.191\n0.282\n0.148\n0.213\nTAU\n15.86\n26.43\n15.22\n26.04\n9.08\n19.46\n0.219\n0.326\n0.135\n0.196\nPromptST\n16.12\n27.42\n9.37\n23.01\n8.24\n22.82\n0.161\n0.306\n0.099\n0.171\nUniST (unified)\n14.04\n23.67\n9.10\n19.95\n5.85\n17.55\n0.119\n0.191\n0.106\n0.172\nSTID\n16.36\n25.55\n12.92\n21.19\n8.32\n18.49\n0.160\n0.234\n0.203\n0.262\nPatchTST\n30.55\n53.36\n10.69\n28.17\n17.03\n50.45\n0.223\n0.465\n0.189\n0.291\nPatchTST (unified)\n33.62\n60.55\n12.16\n31.79\n21.27\n58.61\n0.403\n0.811\n0.176\n0.279\niTransformer\n24.05\n42.17\n10.19\n25.91\n45.19\n45.19\n0.216\n0.466\n0.154\n0.249\nTime-LLM\n29.55\n51.20\n10.57\n28.19\n17.65\n52.94\n0.210\n0.405\n0.115\n0.195\nCSDI\n14.76\n25.87\n8.77\n23.37\n5.05\n16.37\n0.094\n0.168\n0.078\n0.136\nUrbanDiT\n12.61\n21.09\n5.61\n14.44\n5.58\n15.53\n0.092\n0.166\n0.077\n0.129\nTable 2: Performance comparison for grid-based forward prediction evaluated using MAE and\nRMSE. The results represent the average prediction errors across different prediction steps. The\nbest performance is highlighted in bold, and the second-best is indicated with underlining.\ncases, showing the effectiveness of diffusion-based models in capturing complex patterns within\nurban spatio-temporal data. Our proposed model, UrbanDiT, delivers the best performance across\ndifferent datasets using a single unified model, achieving a relative improvement of 11.3%.\nSpeedBJ\nSpeedSH\nSpeedNJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nHA\n1.35\n2.13\n0.92\n1.46\n1.94\n3.01\nSTGCN\n1.81\n2.44\n0.99\n1.35\n1.63\n2.31\nCRNN\n1.37\n1.98\n0.89\n1.28\n1.53\n2.38\nGWN\n1.69\n2.32\n0.93\n1.32\n1.50\n2.16\nMTGNN\n1.15\n1.70\n0.86\n1.33\n1.57\n2.42\nAGCRN\n1.66\n2.29\n1.14\n1.56\n1.77\n2.46\nGTS\n1.76\n2.36\n1.31\n1.74\n2.04\n2.68\nSTEP\n1.45\n2.04\n0.93\n1.32\n1.58\n2.42\nSTID\n1.08\n1.69\n0.83\n1.26\n1.56\n2.38\nPatchTST\n1.27\n1.99\n0.87\n1.37\n1.83\n2.74\nPatchTST (unified)\n1.55\n2.44\n1.08\n1.70\n2.19\n3.34\niTransformer\n1.26\n1.97\n0.90\n1.40\n1.70\n2.62\nTime-LLM\n1.28\n2.00\n0.87\n1.36\n1.82\n2.76\nUrbanDiT\n1.02\n1.66\n0.78\n1.20\n1.51\n2.30\nTable 3:\nComparison of forward prediction performance\nacross three graph-structured traffic speed datasets.\nWe also compare the backward\nprediction performance of Urban-\nDiT with the second-best baseline,\nCSDI, as shown in Appendix Ta-\nble 7.\nNotably, CSDI is specifi-\ncally trained for backward predic-\ntion tasks. However, UrbanDiT not\nonly excels in forward prediction\nbut also surpasses specialized mod-\nels like CSDI in backward predic-\ntion by 30.4%. This result demon-\nstrates UrbanDiT’s ability to cap-\nture complex spatio-temporal pat-\nterns more effectively.\nTemporal Interpolation.1 In this\ntask, we set the missing ratio to\n0.5, meaning that we only know the\neven-numbered time steps (e.g., 0,\n2, 4, ..., 2n), and the model is required to predict the odd-numbered time steps (e.g., 1, 3, 5, ...,\n2n-1). The goal is to evaluate how well the model can infer the missing temporal values by lever-\naging the observed data points before and after the missing steps. Appendix Table 9 demonstrates\nthat UrbanDiT, employing a unified model, outperforms baselines trained separately for different\ndatasets in most cases.\nSpatial Extrapolation. We evaluate the models’ ability to predict missing values in specific spatial\nregions by masking 50% of of spatial locations across the temporal sequence. The objective is to\ndetermine how effectively models extrapolate unobserved spatial information from the remaining\nvisible data. As shown in Table 4, UrbanDiT achieves the best performance in most cases.\n8\n\nPreprint\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nCSDI\n36.66\n75.89\n15.53\n34.77\n19.56\n69.10\n0.34\n0.74\n0.18\n0.32\nImputeformer\n37.13\n77.53\n17.67\n38.96\n20.28\n49.85\n0.39\n0.71\n0.21\n0.34\nGrin\n41.73\n92.61\n22.56\n47.76\n22.44\n58.15\n0.51\n0.71\n0.23\n0.38\nBriTS\n59.94\n112.34\n33.74\n59.10\n23.39\n58.47\n0.50\n0.70\n0.54\n0.75\nUrbanDiT (ours)\n8.10\n12.23\n5.44\n10.17\n4.91\n12.52\n0.099\n0.155\n0.084\n0.146\nTable 4: Performance comparison for spatial extrapolation evaluated using MAE and RMSE. The\nresults represent the average errors across different extrapolation steps.\n(a) PopSH - 5% few-shot                                            (b) PopSH - 10% few-shot\nFigure 5: Evaluation of UrbanDiT and baseline models in 5% and 1% few-shot scenarios on the\nPopSH dataset. The red dashed line indicates UrbanDiT’s zero-shot performance\nSpatio-Temporal Imputation. This task assesses the models’ capacity to impute missing values\nacross both spatial and temporal dimensions. We randomly mask 50% of positions in the 3D spatio-\ntemporal data, simulating real-world scenarios where urban data may be incomplete due to sensor\nfailures or irregularities in data collection. As shown in Appendix Table 10, UrbanDiT achieves the\nbest performance in most cases.\nThese results substantiate that UrbanDiT consistently delivers superior performance across diverse\ntasks and datasets using a single, unified model. This capability positions UrbanDiT as a general-\npurpose foundation model, enabling practitioners to leverage optimized parameters directly, thereby\nsimplifying deployment and enhancing applicability in urban spatio-temporal applications.\n4.3\nFEW-SHOT AND ZERO-SHOT PERFORMANCE.\nA key strength of foundation models is their ability to generalize easily. Therefore, we perform\nexperiments in both few-shot and zero-shot scenarios, testing its adaptability to new datasets with\nlittle or no additional training. In the few-shot scenario, we train UrbanDiT on a small portion of\nthe target dataset—specifically using only 5% and 10% of the available data—and then evaluate\nits performance on the corresponding test set. This setup challenges the model to generalize well\nfrom sparse data. In the zero-shot scenario, no data from the target dataset is provided for training.\nInstead, we directly evaluate UrbanDiT’s performance on the target dataset, relying solely on its\npretrained knowledge to handle unseen data without any fine-tuning.\nFigure 5 demonstrates the few-shot and zero-shot performance of UrbanDiT in comparison to base-\nline models. In the few-shot setting (with 5% and 1% of the training data), UrbanDiT consistently\noutperforms the baselines, showcasing its strong ability to learn from minimal data. Even more\nstriking, in the zero-shot scenario, UrbanDiT exhibits exceptional inference capabilities, surpassing\nnearly all baseline models that had access to training data. This highlights its exceptional general-\nization ability without fine-tuning, reinforcing its effectiveness as an open-world foundation model.\n4.4\nABLATION STUDIES.\nPrompt. Unified prompt learning is a key design in UrbanDiT. To investigate the contribution of\neach prompt to the final performance, we conduct ablation studies by systematically removing each\n9\n\nPreprint\n21\n22\n23\n24\n25\nRMSE\nFull\nw/o T\nw/o F\nw/o S\nw/o M\nw/o P\nFigure 6: Ablation study on the prompt\ndesign\nusing\nRMSE\non\nthe\nTaxiBJ\ndataset.\n5\n10\n20\n50\n100\n#Inference Steps\n21\n22\n23\n24\n25\nTaxiBJ\n18\n19\n20\n21\nTaxiNYC\nFigure 7: Performance evaluation (RMSE)\nwith varying numbers of inference steps on\nTaxiBJ and TaxiNYC datasets.\ntype of prompt. Specifically, we identify four types of prompts: F for frequency-domain prompt,\nT for time-domain prompt, S for spatial prompt, and M for task-specific prompt. We denote the\nremoval of a prompt as w/o {F, T, S, M} and indicate the absence of any prompt as w/o P.\nFigure 6 presents the results of ablation studies. The findings reveal that removing any single prompt\nsignificantly degrades the model’s performance. In the absence of prompt design altogether, the\nmodel exhibits the poorest performance. Among the four types of prompts, the removal of the\nfrequency-domain prompt has the most pronounced negative impact on the overall performance.\nInference Steps of Diffusion Models. We further investigate the effect of inference steps on the\nperformance of diffusion models. The number of inference steps is a critical factor in balancing\nthe model’s accuracy and efficiency. Figure 7 illustrates the performance of the diffusion model\nacross different numbers of inference steps for two datasets, TaxiBJ and TaxiNYC, measured using\nRMSE. Notably, we observe that around 20 inference steps provide the optimal balance between\ncomputational efficiency and model performance for both datasets. By setting the diffusion steps\nto 500 and the inference steps to 20, we achieve a 25x improvement in efficiency compared to the\noriginal DDPM (Ho et al., 2020), without sacrificing accuracy.\n4.5\nSCALABILITY.\n0.2\n0.4\n0.6\n0.8\n1.0\nDatasize Scale\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\nModel Performance\nModel Performance vs. Datasize Scale \n for Different Model Sizes\nUrbanDiT-S\nUrbanDiT-M\nUrbanDiT-L\nFigure 8: The scalability of UrbanDiT.\nAs a foundation model, it is crucial to un-\nderstand how model performance evolves as\nthe datasize scale varies across different model\nsizes.\nThis information is valuable for prac-\ntitioners to train and fine-tune the founda-\ntion model effectively.\nIn Figure 8, we ex-\nplore the relationship between model perfor-\nmance and datasize scale for three model sizes:\nUrbanDiT-S (small), UrbanDiT-M (medium),\nand UrbanDiT-L (large). As observed, all three\nmodels demonstrate improved performance as\nthe data size increases.\nHowever, when the\ndataset size increases from 0.8 to 1, the large\nmodel, UrbanDiT-L, shows a notably steeper\nimprovement (with a slope of 0.011), compared\nto the medium (slope of 0.0015) and small models (slope of 0.0019). This pronounced scaling effect\nfor the large model indicates its potential to further enhance performance as more data becomes\navailable. These results highlight the promising scalability of UrbanDiT-L, suggesting that it can\neffectively handle larger datasets and achieve even better outcomes with increased data size.\n10\n\nPreprint\n5\nCONCLUSION\nIn this paper, we present UrbanDiT, an open-world foundation model built on a diffusion transform-\ners and a unified prompt learning framework. UrbanDiT enables seamless adaptation to a wide range\nof urban spatio-temporal tasks across diverse datasets from urban environments. Our extensive ex-\nperiments highlight the model’s exceptional potential in advancing the field of urban spatio-temporal\nmodeling. We believe this work not only pushes the boundaries of urban spatio-temporal modeling\nbut also serves as an inspire future research in the rapidly evolving field of foundation models.\nREFERENCES\nPierre Aumond, Arnaud Can, Vivien Mallet, Bert De Coensel, Carlos Ribeiro, Dick Botteldooren,\nand Catherine Lavandier. Kriging-based spatial interpolation from measurements for sound level\nmapping in urban areas. The journal of the acoustical society of America, 143(5):2847–2857,\n2018.\nLei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent\nnetwork for traffic forecasting. Advances in neural information processing systems, 33:17804–\n17815, 2020.\nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe\nTaylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024.\nURL https://openai. com/research/video-generation-models-as-world-simulators, 3, 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nWei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent\nimputation for time series. Advances in neural information processing systems, 31, 2018.\nZheng Chang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Yan Ye, Xiang Xinguang, and Wen Gao.\nMau: A motion-aware unit for video prediction and beyond. Advances in Neural Information\nProcessing Systems, 34:26950–26962, 2021.\nChanglu Chen, Yanbin Liu, Ling Chen, and Chengqi Zhang. Bidirectional spatial-temporal adap-\ntive transformer for urban traffic flow forecasting. IEEE Transactions on Neural Networks and\nLearning Systems, 2022.\nAndrea Cini, Ivan Marisca, and Cesare Alippi. Filling the g ap s: Multivariate time series imputation\nby graph neural networks. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=kOu3-S3wJ7.\nJinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. St-norm: Spatial and\ntemporal normalization for multi-variate time series forecasting. In Proceedings of the 27th ACM\nSIGKDD conference on knowledge discovery & data mining, pp. 269–278, 2021.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first International Conference on Machine Learning,\n2024.\nJie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, and Yong Li. Citygpt: Empowering urban\nspatial cognition of large language models. arXiv preprint arXiv:2406.13948, 2024a.\nJie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, and Yong Li. Citygpt: Empowering urban\nspatial cognition of large language models. arXiv preprint arXiv:2406.13948, 2024b.\nZhangyang Gao, Cheng Tan, Lirong Wu, and Stan Z Li. Simvp: Simpler yet better video prediction.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n3170–3180, 2022.\n11\n\nPreprint\nXu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping Ye, and Yan Liu. Spatiotem-\nporal multi-graph convolution network for ride-hailing demand forecasting. In Proceedings of the\nAAAI conference on artificial intelligence, volume 33, pp. 3656–3663, 2019.\nBenedikt Gr¨aler, Edzer J Pebesma, and Gerard BM Heuvelink. Spatio-temporal interpolation using\ngstat. R J., 8(1):204, 2016.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840–6851, 2020.\nJunfeng Hu, Xu Liu, Zhencheng Fan, Yuxuan Liang, and Roger Zimmermann. Towards unifying dif-\nfusion models for probabilistic spatio-temporal graph learning. arXiv preprint arXiv:2310.17360,\n2023.\nJiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang.\nPdformer:\nPropaga-\ntion delay-aware dynamic long-range transformer for traffic flow prediction.\narXiv preprint\narXiv:2301.07945, 2023.\nMing Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yux-\nuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming\nlarge language models. In The Twelfth International Conference on Learning Representations.\nMarcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang,\nand Yuyang Bernie Wang. Predict, refine, synthesize: Self-guiding diffusion models for proba-\nbilistic time series forecasting. Advances in Neural Information Processing Systems, 36, 2024.\nYaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-\nwork: Data-driven traffic forecasting. In International Conference on Learning Representations,\n2018.\nZhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao\nHuang. Urbangpt: Spatio-temporal large language models, 2024.\nZhihui Lin, Maomao Li, Zhuobin Zheng, Yangyang Cheng, and Chun Yuan. Self-attention convlstm\nfor spatiotemporal prediction. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 11531–11538, 2020.\nLingbo Liu, Ruimao Zhang, Jiefeng Peng, Guanbin Li, Bowen Du, and Liang Lin. Attentive crowd\nflow machines. In Proceedings of the 26th ACM international conference on Multimedia, pp.\n1553–1561, 2018.\nXingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for\nhigh-quality diffusion-based text-to-image generation. In The Twelfth International Conference\non Learning Representations, 2023a.\nYong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.\nitransformer: Inverted transformers are effective for time series forecasting.\narXiv preprint\narXiv:2310.06625, 2023b.\nHaoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt:\nGeneral-purpose video diffusion transformers via mask modeling. In The Twelfth International\nConference on Learning Representations.\nJun Ma, Yuexiong Ding, Jack CP Cheng, Feifeng Jiang, and Zhiwei Wan. A temporal-spatial inter-\npolation and extrapolation method based on geographic long short-term memory neural network\nfor pm2. 5. Journal of Cleaner Production, 237:117729, 2019.\nJames R Miller, Monica G Turner, Erica AH Smithwick, C Lisa Dent, and Emily H Stanley. Spatial\nextrapolation: the science of predicting ecological patterns and processes. BioScience, 54(4):\n310–320, 2004.\nTong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, and Jian Sun.\nImputeformer: Low rankness-\ninduced transformers for generalizable spatiotemporal imputation. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2260–2271, 2024.\n12\n\nPreprint\nYuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64\nwords: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 4195–4205, 2023.\nKashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising dif-\nfusion models for multivariate probabilistic time series forecasting. In International Conference\non Machine Learning, pp. 8857–8868. PMLR, 2021.\nChao Shang and Jie Chen. Discrete graph structure learning for forecasting multiple time series. In\nProceedings of International Conference on Learning Representations, 2021.\nZezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. Spatial-temporal identity: A simple\nyet effective baseline for multivariate time series forecasting. In Proceedings of the 31st ACM\nInternational Conference on Information & Knowledge Management, pp. 4454–4458, 2022a.\nZezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. Pre-training enhanced spatial-temporal graph\nneural network for multivariate time series forecasting. In Proceedings of the 28th ACM SIGKDD\nconference on knowledge discovery and data mining, pp. 1567–1577, 2022b.\nCheng Tan, Zhangyang Gao, Lirong Wu, Yongjie Xu, Jun Xia, Siyuan Li, and Stan Z Li. Tem-\nporal attention unit: Towards efficient spatiotemporal predictive learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18770–18782, 2023a.\nCheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, Zicheng Liu, Lirong Wu,\nand Stan Z Li. Openstl: A comprehensive benchmark of spatio-temporal predictive learning.\nAdvances in Neural Information Processing Systems, 36:69819–69831, 2023b.\nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based\ndiffusion models for probabilistic time series imputation. Advances in Neural Information Pro-\ncessing Systems, 34:24804–24816, 2021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nYunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and Philip S Yu. Predrnn: Recurrent\nneural networks for predictive learning using spatiotemporal lstms. Advances in neural informa-\ntion processing systems, 30, 2017.\nYunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and S Yu Philip. Predrnn++: Towards\na resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In International\nConference on Machine Learning, pp. 5123–5132. PMLR, 2018.\nYunbo Wang, Jianjin Zhang, Hongyu Zhu, Mingsheng Long, Jianmin Wang, and Philip S Yu. Mem-\nory in memory: A predictive neural network for learning higher-order non-stationarity from spa-\ntiotemporal dynamics. In Proceedings of the IEEE/CVF conference on computer vision and pat-\ntern recognition, pp. 9154–9162, 2019.\nHaomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger Zimmermann, and\nYuxuan Liang. Diffstg: Probabilistic spatio-temporal graph forecasting with denoising diffusion\nmodels. In Proceedings of the 31st ACM International Conference on Advances in Geographic\nInformation Systems, pp. 1–12, 2023.\nZonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for deep\nspatial-temporal graph modeling. In Proceedings of the 28th International Joint Conference on\nArtificial Intelligence, pp. 1907–1913, 2019.\nZonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Con-\nnecting the dots: Multivariate time series forecasting with graph neural networks. In Proceedings\nof the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp.\n753–763, 2020.\n13\n\nPreprint\nChunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, and Fan Zhou. Imputation-based time-\nseries anomaly detection with conditional weight-incremental diffusion models. In Proceedings of\nthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2742–2751,\n2023.\nYiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao\nLiu, Bin Yang, Zenglin Xu, et al. A survey on diffusion models for time series and spatio-temporal\ndata. arXiv preprint arXiv:2404.18886, 2024.\nBing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: a deep\nlearning framework for traffic forecasting. In Proceedings of the 27th International Joint Confer-\nence on Artificial Intelligence, pp. 3634–3640, 2018.\nYuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, and Yong Li. Spatio-temporal diffusion\npoint processes. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pp. 3173–3184, 2023.\nYuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. Unist: a prompt-empowered universal\nmodel for urban spatio-temporal prediction. In Proceedings of the 30th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, pp. 4095–4106, 2024a.\nYuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, and Yong Li. Spatio-temporal few-shot\nlearning via diffusive neural network generation. In The Twelfth International Conference on\nLearning Representations, 2024b.\nJunbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd\nflows prediction. In Proceedings of the AAAI conference on artificial intelligence, volume 31,\n2017.\nSi Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a\ncomprehensive review. Computational Social Networks, 6(1):1–23, 2019.\nXu Zhang, Yongshun Gong, Xinxin Zhang, Xiaoming Wu, Chengqi Zhang, and Xiangjun Dong.\nMask-and contrast-enhanced spatio-temporal learning for urban flow prediction. In Proceedings\nof the 32nd ACM International Conference on Information and Knowledge Management, pp.\n3298–3307, 2023a.\nZijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang, Hongwei Zhao,\nYiqi Wang, and Zitao Liu. Promptst: Prompt-enhanced spatio-temporal multi-attribute predic-\ntion. In Proceedings of the 32nd ACM International Conference on Information and Knowledge\nManagement, pp. 3195–3205, 2023b.\nLiang Zhao, Min Gao, and Zongwei Wang. St-gsp: Spatial-temporal global semantic representation\nlearning for urban flow prediction. In Proceedings of the Fifteenth ACM International Conference\non Web Search and Data Mining, pp. 1443–1451, 2022.\nYuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. Difftraj: Generating\ngps trajectory with diffusion probabilistic model. Advances in Neural Information Processing\nSystems, 36:65168–65188, 2023.\nYuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian\nZhang, Xuetao Wei, and Yuxuan Liang.\nControltraj: Controllable trajectory generation with\ntopology-constrained diffusion model. In Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pp. 4676–4687, 2024.\n14\n\nPreprint\nTable 5: Basic statistics of grid-based data.\nDataset\nCity\nType\nTemporal Period\nSpatial partition\nInterval\nMean\nStd\nFlowSH\nShanghai\nMobility flow\n2016/04/25 - 2016/05/01\n20 × 20\n15min\n31.935\n137.926\nPopBJ\nBeijing\nCrowd flow\n2021/10/25 - 2021/11/21\n28 × 24\nOne hour\n0.367\n0.411\nTaxiBJ\nBeijing\nTaxi flow\n2013/06/01 - 2013/10/30\n32 × 32\nHalf an hour\n97.543\n122.174\nCrowdNJ\nNanjing\nCrowd flow\n2021/02/02 - 2021/03/01\n20 × 28\nOne hour\n0.872\n1.345\nTaxiNYC\nNew York City\nTaxi flow\n2015/01/01 - 2015/03/01\n10 × 20\nHalf an hour\n38.801\n103.924\nPopSH\nShanghai\nDynamic population\n2014/08/01 - 2014/08/28\n32 × 28\nOne hour\n0.175\n0.212\nTable 6: Basic statistics of Graph-based data.\nDataset\nCity\nType\nTemporal Period\nInterval\n#Nodes\n#Edges\nMean\nStd\nSpeedSH\nShanghai\nTraffic speed\n2022/01/27 - 2022/02/27\n15min\n21099\n39065\n7.815\n4.044\nSpeedBJ\nBeijing\nTraffic speed\n2022/03/05 - 2022/04/05\n15min\n13675\n24444\n6.837\n3.412\nSpeedNJ\nNanjing\nTraffic speed\n2022/03/05 - 2022/04/05\n15min\n13419\n25100\n6.699\n4.253\nA\nDATASETS\nWe provide a detailed overview of the datasets utilized in our study to support future research in\nthe field of urban spatio-temporal modeling. The datasets are categorized into two distinct types:\ngrid-based and graph-based spatio-temporal data. Each type of data reflects different spatial orga-\nnizations and dynamics, enabling a comprehensive evaluation of model performance across varied\nurban scenarios.\nGrid-based data represent spatial information in a structured, uniform grid layout, where each grid\ncell corresponds to a specific geographical area. Table 5 outlines the essential details and statistics\nfor the grid-based datasets, including spatial resolution, temporal resolution, temporal period, and\nthe size of each dataset.\nGraph-based data, on the other hand, capture urban spatial relationships through a network of nodes\nand edges, where nodes typically represent points of interest (e.g., intersections or key locations),\nand edges represent the connections between them (e.g., roads or transit lines). This type of data\nis well-suited for modeling scenarios that involve irregular spatial structures, such as transporta-\ntion networks. Table 6 provides a comprehensive summary of the graph-based datasets, including\ninformation on the number of nodes, edges, temporal resolution, temporal period, and dataset size.\nB\nMETHODOLOGY DETAILS\nB.1\nSEQUENTIAL FORMAT OF INPUT DATA\nWe provide a detailed description of the data unification process for both grid-based and graph-based\nspatio-temporal data. The key goal is to transform the data into a unified sequential format suitable\nfor the transformer’s input.\nGrid-based data is structured in a uniform grid layout, typically represented in a three-dimensional\nform Xgrid ∈RT ×H×W with two spatial dimensions (height H and width W) and one temporal\ndimension T. To process this data, we utilize 3D Convolutional Neural Networks (3D CNN), which\nare widely used for capturing both spatial and temporal dependencies in spatio-temporal tasks. The\nprocess is formulated as follows:\nX′ = CONV3D(Xgrid, kernel size = (pt, ps, ps))\nXp = RESHAPE(X′, [N])\nwhere N =\nT\npt × H\nps × W\nps represents the total number of spatio-temporal partitions, effectively\nconverting the data into a one-dimensional sequence for further processing by the transformer model.\nGraph-based data is inherently non-Euclidean, capturing relationships between urban entities (e.g.,\nstreets and intersections). The spatial dimension is represented by a graph structure with nodes\n15\n\nPreprint\nand edges, and the temporal dimension is still captured as a time series at each node. The graph-\nbased data can be represented as a tensor Xgraph ∈RN×T , where N is the number of nodes in\nthe graph, and T is the number of time steps. To handle the temporal dimension, we first apply a\n1D convolutional network (1D CNN) along the time axis to capture local temporal dependencies.\nNext, to capture spatial relationships, we apply a Graph Convolutional Network (GCN) (Zhang\net al., 2019) on the graph structure. For each temporal patch, the GCN aggregates information\nfrom neighboring nodes using the graph’s adjacency matrix A ∈RN×N. Finally, we reshape the\ngraph-based data into a sequential format. The operations are formulated as follows:\nX′ = CONV1D(Xgraph, kernel size = pt)\nX′ = GCN(X′, A, W)\nXp = RESHAPE(X′, [M])\nwhere M represents the number of spatio-temporal patches, ensuring that the graph-based data is\ntransformed into a one-dimensional sequence, similar to the grid-based data. This unified sequential\nrepresentation allows both data types to be processed consistently by the transformer model.\nB.2\nUNIFIED PROMPT LEARNING\nWe provide details of how to obtain the data-driven and task-specific prompts.\nTime-domain patterns. Suppose the patched spatio-temporal data is denoted as X ∈RT ′×N ′,\nwhere T ′ =\nT\npt and N ′ =\nH\nps × W\nps . we extract time-domain patterns by applying an attention\nmechanism along the temporal dimension. This is done independently for each spatial location,\nallowing us to capture temporal dependencies across different spatial patches as follows:\nXt = ATTENTION(XT ), XT ∈RN ′×T ′, Xt ∈RN ′×1×D\nwhere D is the embedding size.\nFrequency-domain patterns. In our work, we employ four distinct approaches to compute features\nin the frequency domain, depending on the configuration of the Fast Fourier Transform (FFT) and\nthresholding mechanisms:\n• Without FFT Threshold: we directly compute the FFT of the input tensor. The tensor is per-\nmuted along the appropriate dimensions, and the real and imaginary components of the FFT are\nconcatenated along the last dimension. This results in a frequency domain representation of the\ndata. It is formulated as follows:\nXFFT = FFT(X),\nXfreq = [ℜ(XFFT), ℑ(XFFT)] ,\nwhere ℜ(XFFT) represents the real part of the FFT, and ℑ(XFFT) represents the imaginary part.\n• Basic FFT Threshold: we apply a basic threshold technique by computing the amplitude of\nthe FFT and creating a binary mask. The mask retains frequency components whose amplitude\nis greater than the mean amplitude, filtering out low-frequency noise and preserving significant\nfrequency components. The process is formulated as follow:\nXFFT = FFT(X),\nA = |XFFT|, µA =\n1\nH × W × T\nX\nA,\nM = I(A > µA), XFFT,filtered = XFFT, ⊙M,\nXfreq = [ℜ(XFFT,filtered), ℑ(XFFT,filtered)] .\n• Quantile-based FFT Threshold: We further refine the frequency selection by applying a thresh-\nold based on the 80t% of the amplitude distribution. This approach retains the most prominent\n16\n\nPreprint\nfrequency components, allowing for more flexible filtering compared to the mean-based threshold.\nThe selection process can be formulated as follows:\nXFFT = FFT(X),\nA = |XFFT|, q80 = Quantile(A, 0.8),\nM = I(A > q80), XFFT,filtered = XFFT ⊙M,\nXfreq = [ℜ(XFFT,filtered), ℑ(XFFT,filtered)] .\n• Top-k Frequency Filtering: We retain only the top k frequency components (e.g., the first three).\nWe generate a mask to preserve only these dominant components, filtering out the rest. It is\nformulated as follows:\nXFFT = FFT(X),\nA = |XFFT|,\nindices = argsort(A, descending)[: k],\nM = mask(indices), XFFT,filtered = XFFT ⊙M,\nXfreq = [ℜ(XFFT,filtered), ℑ(XFFT,filtered)] .\nSpatial patterns. For the same patched spatio-temporal data X ∈RT ′×N ′, we extract spatial\npatterns by applying an attention mechanism along the spatial dimension, independently on each\ntemporal patch. This process allows us to model spatial dependencies within each time patch as\nfollows:\nXs = ATTENTION(X), X ∈RT ′×N ′, Xt ∈RT ′×1×D\nC\nEXPERIMENT DETAILS\nC.1\nBASELINES\n• HA: History Average is a forecasting method that predicts future values by calculating the mean\nof historical data from the same time periods.\n• MIM (Wang et al., 2019): This model utilizes the difference in data between consecutive recurring\nstates to address non-stationary characteristics. By stacking multiple MIM blocks, it can capture\nhigher-order non-stationarity in the data.\n• MAU (Chang et al., 2021): The Motion-aware Unit extends the temporal scope of prediction units\nto seize correlations in motion between frames. It encompasses an attention mechanism and a\nfusion mechanism, which are integral to video prediction tasks.\n• SimVP (Gao et al., 2022): A simple yet effective video prediction model is entirely based on con-\nvolutional neural networks and employs MSE loss as its performance metric, providing a reliable\nbenchmark for comparative studies in video prediction.\n• TAU (Tan et al., 2023a): The Temporal Attention Module breaks down temporal attention into\ntwo parts: within-frame and between-frames, and employs differential divergence regularization\nto manage variations across frames.\n• STResNet (Zhang et al., 2017): STResNet employs residual neural networks to detect proximity,\nperiodicity, and trends in the temporal data.\n• ACFM (Liu et al., 2018): The Attentive Crowd Flow Machine model forecasts crowd movements\nby using an attention mechanism to dynamically integrate sequential and cyclical patterns.\n• STGSP (Zhao et al., 2022): This model highlights the significance of global and positional tem-\nporal data for spatio-temporal forecasting. It incorporates a semantic flow encoder to capture\ntemporal position cues and an attention mechanism to handle multi-scale temporal interactions.\n• MC-STL (Zhang et al., 2023a): MC-STL utilizes mask-enhanced contrastive learning to effi-\nciently identify spatio-temporal relationships.\n• STNorm (Deng et al., 2021): It introduces two distinct normalization modules: spatial normaliza-\ntion for handling high-frequency elements and temporal normalization for managing local com-\nponents.\n17\n\nPreprint\n• STID (Shao et al., 2022a): This MLP-based spatio-temporal forecasting model discerns subtleties\nwithin the spatial and temporal axes, showcasing its design’s efficiency and efficacy.\n• PromptST (Zhang et al., 2023b): An advanced pre-training and prompt-tuning methodology tai-\nlored for spatio-temporal forecasting.\n• UniST (Yuan et al., 2024a): A versatile urban spatio-temporal prediction model that uses grid-\nbased data. It employs various spatio-temporal masking techniques for pre-training and fine-\ntuning with spatio-temporal knowledge-based prompts.\n• STGCN (Yu et al., 2018): The Spatio-Temporal Graph Convolutional Network is a deep learning\narchitecture for predicting traffic patterns, harnessing both spatial and temporal correlations. It\nintegrates graph convolutional operations with convolutional sequence learning to capture multi-\nscale dynamics within traffic networks.\n• GWN (Wu et al., 2019): Graph WaveNet is a technique crafted to overcome the shortcomings of\ncurrent spatial-temporal graph modeling methods. It introduces a self-adjusting adjacency matrix\nand utilizes stacked dilated causal convolutions to efficiently capture temporal relationships.\n• MTGNN (Wu et al., 2020): MTGNN is a framework tailored for multivariate time series anal-\nysis. It autonomously identifies directional relationships between variables via a graph learning\ncomponent and incorporates additional information such as variable attributes.\n• GTS (Shang & Chen, 2021): GTS is an approach that concurrently learns the topology of a graph\nalongside a Graph Neural Network (GNN) for predicting multiple time series. It models the graph\nstructure using a neural network, allowing for the generation of distinct graph samples, and aims\nto optimize the average performance across the distribution of graphs.\n• DCRNN (Li et al., 2018): The Diffusion Convolutional Recurrent Neural Network is a deep\nlearning framework for spatiotemporal prediction. It treats traffic flow as a diffusion phenomenon\non a directed graph, securing spatial interdependencies via two-way random walks and temporal\ninterdependencies through an encoder-decoder setup with scheduled sampling.\n• STEP (Shao et al., 2022b):Spatial-temporal Graph Neural Network Enhanced by Pre-training is\na framework that uses a pre-trained model to enhance spatial-temporal graph neural networks for\nbetter forecasting of multivariate time series data.\n• AGCRN (Bai et al., 2020): The AGCRN framework improves upon Graph Convolutional Net-\nworks by incorporating two adaptive components: Node Adaptive Parameter Learning and Data\nAdaptive Graph Generation. This approach effectively captures nuanced spatial and temporal\nrelationships within traffic data, functioning independently of pre-set graph structures.\n• PatchTST (Nie et al., 2022): It employs patching and self-supervised learning techniques for\nforecasting multivariate time series. By dividing the time series into segments, it captures long-\nterm dependencies and analyzes each data channel separately using a unified network architecture.\n• iTransformer (Liu et al., 2023b): This state-of-the-art model for multivariate time series utilizes\nattention mechanisms and feed-forward neural network layers on inverted dimensions to empha-\nsize the relationships among multiple variables.\n• Time-LLM (Jin et al.): TIME-LLM represents an advanced approach in applying large-scale\nlanguage models to time series prediction. It employs a reprogramming strategy that adapts LLMs\nfor forecasting tasks without altering the underlying language model architecture.\n• CSDI (Tashiro et al., 2021): CSDI is explicitly trained for imputation and can exploit correla-\ntions between observed values, leading to significant improvements in performance over existing\nprobabilistic imputation methods.\n• Imputeformer (Nie et al., 2024): It introduces a low-rank inductive bias into the Transformer\nframework to balance strong inductive priors with high model expressivity, making it suitable for\na wide range of imputation tasks.\n• Grin (Cini et al., 2022): GRIN introduces a novel graph neural network architecture designed to\nreconstruct missing data in different channels of a multivariate time series, outperforming state-\nof-the-art methods in imputation tasks.\n• BriTS (Cao et al., 2018): BRITS is a method for imputing missing values in time series data,\nutilizing a bidirectional recurrent neural network (RNN) without imposing assumptions on the\ndata’s underlying dynamics.\n18\n\nPreprint\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nCSDI\n17.40\n33.98\n10.65\n31.88\n4.83\n15.43\n0.094\n0.16\n0.082\n0.14\nUrbanDiT\n11.57\n20.08\n5.996\n14.37\n4.71\n15.07\n0.16\n0.099\n0.071\n0.117\nTable 7: Performance comparison for grid-based backward prediction evaluated using MAE and\nRMSE.\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nCSDI\n11.20\n18.42\n5.71\n13.14\n3.86\n11.59\n0.055\n0.092\n0.044\n0.077\nImputeformer\n11.99\n19.83\n6.72\n15.69\n5.61\n16.72\n0.079\n0.16\n0.066\n0.11\nGrin\n13.69\n23.45\n9.61\n26.28\n8.10\n21.32\n0.10\n0.18\n0.083\n0.16\nBriTS\n17.57\n27.63\n15.24\n28.40\n19.41\n50.25\n0.19\n0.28\n0.16\n0.25\nUrbanDiT (ours)\n9.09\n14.54\n4.90\n10.308\n4.50\n11.46\n0.077\n0.121\n0.056\n0.094\nTable 8: Performance comparison for temporal interpolation evaluated using MAE and RMSE. The\nresults represent the average errors across different interpolation steps.\nIt is worth noting that the baselines, including UniST (Yuan et al., 2024a) and PatchTST (Nie et al.,\n2022), can also be trained using multiple datasets. In our comparison experiments, we train these\nmodels in a unified manner using the same diverse datasets to ensure a fair comparison. This ap-\nproach ensures that the performance gains of UrbanDiT are not merely due to dataset diversity, but\nreflect the model’s true advantage.\nC.2\nEXPERIMENT CONFIGURATION\nFor UrbanDiT-S (small), the model consists of 4 transformer layers with a hidden size of 256. Both\nthe spatial and temporal patch sizes are set to 2, and the number of attention heads is 4. UrbanDiT-\nM (medium) is composed of 6 transformer layers with a hidden size of 384, maintaining the same\nspatial and temporal patch sizes of 2, and 6 attention heads. UrbanDiT-L (large) includes 12 trans-\nformer layers, a hidden size of 384, spatial and temporal patch sizes of 2, and 12 attention heads.\nEach memory pool contains 512 embeddings, with the embedding dimension matching the model’s\nhidden size. The learning rate is set to 1e-4, and the maximum number of training epochs is 500,\nwith early stopping applied to prevent overfitting. The batch size is tailored for each dataset to\nmaintain a similar number of training iterations across them.\nC.3\nMETRICS.\nTo assess the performance of UrbanDiT in urban spatio-temporal applications, we employ widely\nrecognized evaluation metrics: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).\nGiven that UrbanDiT operates as a probabilistic model, we conduct 20 inference runs and use the\naverage result for comparison against the ground truth. We apply the same evaluation framework to\nthe probabilistic baselines, ensuring a consistent and fair assessment of all models.\nD\nADDITIONAL RESULTS\nD.1\nRESULTS OF MULTIPLE TASKS\nTable 7 to Table 10 illustrate additional results of multiple tasks.\nD.2\nFEW-SHOT AND ZERO-SHOT PERFORMANCE\nFigure 9 demonstrates UrbanDiT’s few-shot and zero-shot capabilities on the TaxiBJ dataset.\n19\n\nPreprint\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nCSDI\n12.29\n22.07\n7.94\n21.86\n4.33\n13.09\n0.071\n0.12\n0.055\n0.094\nImputeformer\n13.65\n23.18\n9.22\n19.97\n5.95\n16.36\n0.093\n0.16\n0.069\n0.12\nGrin\n16.83\n27.61\n9.70\n23.52\n9.15\n21.43\n0.16\n0.30\n0.096\n0.18\nBriTS\n22.57\n38.39\n17.14\n38.82\n19.93\n50.47\n0.26\n0.41\n0.18\n0.29\nUrbanDiT (ours)\n9.38\n15.19\n5.03\n11.52\n4.62\n12.16\n0.083\n0.13\n0.061\n0.101\nTable 9: Performance comparison for temporal imputation evaluated using MAE and RMSE. The\nresults represent the average errors across different imputation steps.\nTaxiBJ\nFlowSH\nTaxiNYC\nCrowdNJ\nPopBJ\nModel\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nCSDI\n7.92\n12.42\n4.28\n8.62\n3.86\n11.54\n0.057\n0.091\n0.046\n0.083\nImputeformer\n9.70\n13.80\n5.50\n10.30\n4.79\n15.35\n0.076\n0.12\n0.061\n0.11\nGrin\n11.96\n19.62\n9.21\n19.68\n9.62\n20.77\n0.11\n0.19\n0.080\n0.14\nBriTS\n13.99\n23.53\n17.95\n38.57\n19.17\n50.15\n0.21\n0.44\n0.13\n0.19\nUrbanDiT (ours)\n7.83\n12.13\n5.07\n9.79\n3.63\n11.44\n0.057\n0.090\n0.049\n0.092\nTable 10: Performance comparison for grid-based spatio-temporal imputation evaluated using MAE\nand RMSE. The results represent the average prediction errors across different prediction steps.\n(a) TaxiBJ - 5% few-shot                                            (b) TaxiBJ - 10% few-shot\nFigure 9: Evaluation of UrbanDiT and baseline models in 5% and 1% few-shot scenarios on the\nTaxiBJ dataset. The red dashed line indicates UrbanDiT’s zero-shot performance\n20",
    "pdf_filename": "UrbanDiT_A_Foundation_Model_for_Open-World_Urban_Spatio-Temporal_Learning.pdf"
}