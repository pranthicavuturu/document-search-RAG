{
    "title": "VidComposition Can MLLMs Analyze Compositions in Compiled Videos",
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal un- derstanding, expanding their capacity to analyze video con- tent. However, existing evaluation benchmarks for MLLMs a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual el- ements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition un- derstanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations. VidCom- position includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs re- veals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compo- sitions and offers insights into areas for further improve- ment. The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/.",
    "body": "VIDCOMPOSITION: Can MLLMs Analyze Compositions in Compiled Videos?\nYunlong Tang1,∗, Junjia Guo1,∗, Hang Hua1, Susan Liang1, Mingqian Feng1, Xinyang Li1,\nRui Mao1, Chao Huang1, Jing Bi1, Zeliang Zhang1, Pooyan Fazli2, Chenliang Xu1†\n1University of Rochester, 2Arizona State University\n{yunlong.tang, mingqian.feng, jing.bi, chenliang.xu}@rochester.edu, pooyan@asu.edu\n{jguo40, xli190, rmao6, zzh136}@ur.rochester.edu, {hhua2, sliang22, chuang65}@cs.rochester.edu\nAbstract\nThe advancement of Multimodal Large Language Models\n(MLLMs) has enabled significant progress in multimodal un-\nderstanding, expanding their capacity to analyze video con-\ntent. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking\na detailed assessment of their ability to understand video\ncompositions, the nuanced interpretation of how visual el-\nements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark\nspecifically designed to evaluate the video composition un-\nderstanding capabilities of MLLMs using carefully curated\ncompiled videos and cinematic-level annotations. VidCom-\nposition includes 982 videos with 1706 multiple-choice\nquestions, covering various compositional aspects such as\ncamera movement, angle, shot size, narrative structure,\ncharacter actions and emotions, etc. Our comprehensive\nevaluation of 33 open-source and proprietary MLLMs re-\nveals a significant performance gap between human and\nmodel capabilities. This highlights the limitations of current\nMLLMs in understanding complex, compiled video compo-\nsitions and offers insights into areas for further improve-\nment. The leaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.\n1. Introduction\nRecent advancements in Multimodal Large Language Mod-\nels (MLLMs) [1, 3, 8, 15, 27, 43, 47] have greatly enhanced\ncapabilities in understanding multimodality. However, while\ncurrent benchmarks [10, 12, 25, 33] for evaluating MLLMs\nassess general image or video comprehension, they lack a\ndetailed focus on video composition, the nuanced interpre-\ntation of how visual elements combine and interact within\ncompiled videos. Compiled videos refer to those created by\nFigure 1. Top MLLMs’ performance on VIDCOMPOSITION, across\n15 tasks of 5 aspects of video composition understanding: Cine-\nmatography Analysis, Character Understanding, Narrative Under-\nstanding, Scene Perception, and Making Analysis.\nediting and integrating multiple clips, scenes, or sequences,\neither from various sources or from different segments of a\nsingle recording, e.g. films, TV series, documentaries, ani-\nmations, vlogs, etc. These videos are carefully constructed\nto create a seamless flow and include richer compositions,\nrequiring shot-by-shot analysis to interpret.\nShot-by-shot analysis, a technique where creators meticu-\nlously break down the elements of a video, serves as a vital\ntool for understanding video composition in depth. This\nlevel of understanding, essential in film analysis and video\nproduction, goes beyond general scene or action recognition,\nrequiring an in-depth grasp of compositional elements such\n1\narXiv:2411.10979v2  [cs.CV]  19 Nov 2024\n\nFigure 2. VIDCOMPOSITION comprises 15 categories of high-quality QA pairs, focusing on five aspects of compositions in compiled videos:\ncinematography, character, narrative, scene, and making. The correct answers are highlighted .\nas camera movements, shot sizes, narrative structures, and\ncharacter dynamics. This analysis also captures the intricate\nlayers of visual storytelling by deconstructing how technical\nand artistic choices shape the viewing experience. However,\nachieving this fine-grained level of composition understand-\ning remains a significant challenge for existing MLLMs,\nwhich primarily operate on broader, more coarse-grained\ninterpretations of video content.\nThrough investigating existing benchmarks, we identified\ntheir limitations in evaluating MLLM in video composition\nunderstanding. As shown in Table 1, the benchmarks in\nthe first group [10, 16, 31] primarily focus on static images\nand overlook the dynamic aspects of visual content. Among\nthese, Winoground [48] and MMComposition [16] attempts\nto assess the compositionality of MLLMs, though it is lim-\nited to image-based evaluations. The second group consists\nof traditional video benchmarks [21, 53, 54, 58], which are\nless effective at addressing the specific limitations of modern\nMLLMs. While TVQA [21] includes a compositional video\nQA component, its compositionality is relatively coarse-\ngrained, limited to basic question types like “who,” “when,”\n“where,” “how,” and “what.” The third group highlights recent\nbenchmarks [5, 12, 25, 32, 40, 44, 50] developed to assess\nMLLMs’ video comprehension capabilities. Although these\nbenchmarks incorporate tasks that touch on compositional\nunderstanding, their evaluations of compositionality remain\nlimited. Additionally, most videos in these benchmarks are\nnatural-shot rather than compiled, posing a challenge for\nmodels trained on natural footage to effectively interpret the\nincreasingly prevalent edited and compiled videos seen on\nmodern online video platforms.\nRecognizing the gap in existing evaluation methods, we\nintroduce VIDCOMPOSITION, a new benchmark designed\nto assess MLLMs on understanding video composition at\na cinematic level. VIDCOMPOSITION includes 982 care-\nfully curated videos and 1,706 multiple-choice questions,\nfeaturing meticulously annotated clips from films, TV series,\nanimations, commentary videos, etc. These questions in-\nclude five key areas of video composition: Cinematography\nAnalysis, Character Understanding, Narrative Understand-\ning, Scene Perception, and Making Analysis, spanning 15\ndistinct tasks. Each area captures critical aspects of compo-\nsitional understanding, e.g. camera movements, angles, shot\nsizes, narrative structures, characters, scenes, cuts, special\neffects, etc., providing a extensive framework for evaluating\nthe nuanced comprehension required in cinematic contexts.\nWe evaluate 33 state-of-the-art MLLMs on VIDCOMPO-\nSITION, including 27 open-source and 6 proprietary models,\nrevealing a substantial performance gap between MLLMs\nand human-level comprehension in video composition un-\n2\n\nTable 1. A comparative overview of various benchmarks across\nseveral dimensions, such as data format (image I or video V), the\nsize of dataset for evaluation (#Data), the number of tasks covered\n(#Task), whether the dataset supports compositional question an-\nswering (Compositional QA), the presence of Compiled Videos\nand Fine-Grained sub-tasks, and the annotation method (manual\nor automatic/manual, indicated by Anno.).\nBenchmark\nI/V #Data #Task Composi-\ntional QA\nCompiled\nVideos\nFine-\nGrained Anno.\nWinoground [48]\nI\n400\n8\n✓\n-\n✗\nM\nMME [10]\nI\n1.1k\n14\n✗\n-\n✓\nM\nMMBench [33]\nI\n1.7k\n20\n✗\n-\n✓\nA+M\nMMComposition [16]\nI\n4.3k\n13\n✓\n-\n✓\nM\nMSVD-QA [54]\nV\n504\n5\n✗\n✗\n✗\nA\nMSRVTT-QA [54]\nV\n2.9k\n5\n✗\n✗\n✗\nA\nTGIF-QA [18]\nV\n9.6k\n4\n✗\n✗\n✗\nA\nTVQA [21]\nV\n2.2k\n8\n✓\n✓\n✗\nA+M\nActivityNet-QA [58]\nV\n5.8k\n4\n✗\n✗\n✗\nM\nNExT-QA [53]\nV\n1k\n8\n✗\n✗\n✗\nA\nAutoEval-Video [5]\nV\n327\n9\n✗\n✗\n✗\nA+M\nVideo-Bench [40]\nV\n5.9k\n10\n✗\n✗\n✗\nA+M\nLVBench [50]\nV\n500\n6\n✗\n✗\n✗\nM\nMVBench [25]\nV\n3.6k\n20\n✗\n✗\n✓\nA+M\nMovieChat-1k [44]\nV\n100\n8\n✗\n✓\n✓\nM\nTempCompass [32]\nV\n410\n4\n✗\n✗\n✓\nM\nVideo-MME [12]\nV\n900\n12\n✗\n✗\n✓\nM\nVIDCOMPOSITION\nV\n982\n15\n✓\n✓\n✓\nM\nderstanding.\nAs shown in Figure 1, although top mod-\nels [1, 7, 8, 23, 28, 43, 49] perform well on basic perception\ntasks (e.g. action perception), they fall short in comprehend-\ning complex video compositions, particularly in cinematog-\nraphy. This performance disparity underscores current mod-\nels’ limitations in capturing intricate, multi-layered video\nstructures. Additional experiments further analyze factors\ninfluencing MLLM performance, such as the number of\nframes provided as input, the resolution of visual encoders,\nthe size of language decoders, and the data volume for fine-\ntuning, yielding insights for future advancements in model\ndesign. Overall, our benchmark offers valuable insights for\nenhancing MLLMs and also suggests applications in video\ngeneration where MLLMs could assist in automatically eval-\nuating the compositional quality of generated videos.\nIn summary, our contribution is three-fold:\n• We introduce VIDCOMPOSITION, a novel, human-\nannotated, high-quality benchmark for evaluating fine-\ngrained video composition understanding in MLLMs.\n• We comprehensively evaluate 33 MLLMs for video un-\nderstanding with VIDCOMPOSITION. The results show\nthe challenging nature of VIDCOMPOSITION and the sub-\nstantial gap between MLLMs’ and humans’ capabilities\nin video composition understanding.\n• We analyze the critical factors that influence the perfor-\nmance of MLLMs systematically, providing potential di-\nrections for model improvement and future advancements.\n2. VIDCOMPOSITION\n2.1. Overview and Terminology\nVIDCOMPOSITION contains 982 compiled videos with 1706\nhuman-annotated multiple-choice questions for video com-\nposition understanding, including 5 main categories: Cine-\nmatography Analysis (CA), Character Understanding (CU),\nNarrative Understanding (NU), Scene Perception (SP), and\nMaking Analysis (MA); and 15 sub-tasks: Camera Move-\nment Perception (CamM-P), Shot Size Perception (SS-P),\nCamera Angle Perception (CamA-P), Emotion Perception\n(E-P), Action Perception (A-P), Costume, Makeup and\nProps Perception (CMP-P), Character Counting (Cha-C),\nScript Matching (S-M), Plot Ordering (P-O), Background\nPerception (B-P), Scene Counting (S-C), Lighting Percep-\ntion (L-P), Art Style Perception (AS-P), Cut Counting (Cut-\nC), and Special Effect Perception (SE-P). Examples of each\ntask can be found in Figure 2. The detailed definitions of\neach task are provided in Supplementary.\n2.2. Dataset Curation Process\nVideo Collection and Filtering. Our dataset comprises\nvideos sourced from the Internet, focusing on compiled\nvideos primarily derived from commentary videos for\nmovies, TV series, and animations, which have no copy-\nright concerns. These videos typically include subtitles and\nscripts uploaded by users, which assist in later annotation\nstages. We further refine the collected videos by filtering out\ninappropriate content, such as clips that may cause psycho-\nlogical distress or those flagged as sensitive by API-based\nmodels. The average duration of the collected videos is\nabout 20 minutes. For videos with subtitles or scripts, we\nextract the timestamps marking the start and end of each\nsubtitle. With this information, we further segment the video\ninto coherent sections whose average length is 794 frames.\nTo avoid models from predicting answers relying on speech,\nwe removed the audio of the videos.\nHuman Annotation. To ensure the quality and reliability of\nthe dataset, we engage multiple human annotators, assign-\ning each video segment to several annotators to minimize\npotential biases. All questions are meticulously designed to\naddress specific tasks. For perception tasks such as A-P, E-P,\nCMP-P, B-P, and for counting tasks such as Cha-C, S-C,\nand Cut-C, annotators watch the video segment and write\nthe correct answer alongside several incorrect (distractor)\noptions. For tasks such as CamM-P, SS-P, CamA-P, L-P,\nSE-P, and AS-P, we provide a predefined set of selectable\nlabels. For example, for CamM-P, the labels include zoom\nin, zoom out, pan left, pan right, pan up, pan down, and\nstatic shot. Annotators choose appropriate labels for each\nvideo segment, which are then used as correct options, while\ndistractors are randomly selected from the remaining labels,\nensuring they differ from the correct options. For S-M, we\n3\n\nFigure 3. (Left) Task statistics in VIDCOMPOSITION, organized into five main categories: Cinematography Analysis (CA), Character\nUnderstanding (CU), Narrative Understanding (NU), Scene Perception (SP), and Making Analysis (MA), comprising a total of 15 sub-tasks.\nThe number of QA pairs is shown in parentheses below each task. (Right) The difficulty distribution across these five categories. If a\nquestion is answered correctly by more than 60% of MLLMs, it will be labeled as “Easy.” Conversely, if a question is answered correctly by\nfewer than 10% of MLLMs, it will be labeled as “Super Hard.”\nuse the video’s commentary script, extracted from the subti-\ntle file, as the correct option, with distractor options sourced\nfrom nearby segments’ scripts to create plausible alternatives.\nFor P-O, we segment the commentary script into multiple\nparts, shuffling and inserting them into the question with se-\nquence numbers. The correct answer is the original order of\nthe script, while other options are generated by randomizing\nthese sequence numbers.\nQuality Control. To ensure the quality of our benchmark,\neach video and corresponding QA pair undergoes multiple\nrounds of review. We implemented an annotation review\nsystem (see the user interface in Supplementary), which\ndisplays the annotated video, question, and answer options\nalongside an additional feedback option for reviewers to pro-\nvide corrections or comments. Reviewers are required to\nattempt each question themselves; if they identify errors in\nthe question or options, they can select the feedback option\nto either suggest improvements to the question or specify\nwhat they believe is the correct answer. After each round\nof review, all feedback submitted through the annotation re-\nview system is analyzed and used to enhance the annotation\nquality further. This iterative quality control process ensures\naccuracy and consistency across annotations, minimizes er-\nrors, and refines question clarity for each task.\n2.3. Evaluation Metrics\nTo obtain model predictions, each question is structured\nwithin a predefined prompt template P that includes the\nquestion text and associated options Oq (A, B, C, D, along\nwith descriptive texts). This prompt Iq is then fed into the\nmodel M, which is expected to output a single character rep-\nresenting its predicted answer (one of {A, B, C, D}). The\nprediction process is formalized in Algorithm 1. The prompt\ntemplate P can be found in Supplementary.\nAlgorithm 1 Model Prediction\n1: Input: Question q, Options Oq\n2: Output: Prediction A\n3: Iq ←P(q, Oq)\n4: Rq ←M(Iq)\n5: Aq ←\n\n\n\n\n\nRq\nif Rq ∈{A, B, C, D},\nAM(Rq)\nif more letters in Rq,\nRS({A, B, C, D})\notherwise.\nThe model output Rq is first checked for validity as a\nsingle character from {A, B, C, D}. An Answer-Matching\n(AM) function identifies a valid option if the output includes\nmultiple characters. The implementation of the AM function\ncan be found in the Supplementary Materials. A random se-\nlection (RS) function from {A, B, C, D} is used to generate\nAq if no valid match is found.\nOnce Aq is obtained, we evaluate its accuracy based on\nwhether it matches the correct answer for each question. Let\nS = {Si = {Qj}Ni\nj=1}|S|\ni=1 represent our dataset, where each\nsub-task Si contains a set of questions Qj across a total of\n|S| sub-tasks. For each question q ∈S, let Gq represent the\ncorrect answer, and Aq represent the model’s answer. The\nscore for question q, denoted as sq, is calculated as follows:\nsq =\n(\n1,\nif Aq = Gq,\n0,\nif Aq ̸= Gq.\n(1)\nA score of 1 is assigned if the model’s prediction Aq matches\nthe correct answer Gq; otherwise, the score is 0. Each sub-\ntask’s accuracy ACCi is calculated as the average score\nacross its Ni questions: ACCi = Ni\n−1 PNi\nj=1 sqj, where\nNi is the total number of questions in sub-task Si. The over-\nall accuracy is computed as the ratio of total correct answers\n4\n\nTable 2. The comprehensive evaluation of 30 MLLMs on VIDCOMPOSITION, including open source models and API-based models . The\nbest results are in bold, and second best results are in underlined, respectively.\nMethod\nCinematography Analysis\nCharacter Understanding\nNarrative Underst.\nScene Perception\nMaking Analysis\nOverall\nCamM-P\nSS-P\nCamA-P\nE-P\nA-P\nCMP-P\nCha-C\nS-M\nP-O\nB-P\nS-C\nL-P\nAS-P\nCut-C\nSE-P\nHuman\n84.1\n85.4\n80.0\n82.6\n92.3\n92.9\n94.1\n97.0\n97.5\n94.4\n80.2\n81.8\n85.7\n87.5\n94.7\n86.26\nLLaVA-OneVision-72B [23]\n57.1\n60.5\n66.3\n63.3\n90.0\n90.0\n74.6\n84.7\n72.4\n76.9\n12.0\n90.3\n89.5\n34.5\n74.1\n63.31\nInternVL2-40B [6]\n46.6\n60.0\n58.9\n51.0\n90.0\n83.3\n67.6\n79.6\n51.9\n80.0\n47.4\n64.5\n68.4\n44.8\n85.2\n60.73\nInternVL2-76B [6]\n46.6\n63.9\n45.5\n51.0\n86.7\n86.7\n76.1\n81.3\n49.0\n80.0\n44.5\n51.6\n76.3\n24.1\n75.9\n58.73\nQwen2-VL-72B [49]\n37.9\n56.6\n57.9\n34.7\n76.7\n76.7\n63.4\n76.2\n66.5\n73.8\n53.6\n74.2\n65.8\n3.4\n55.6\n58.68\nVideo-LLaMA2-72B [8]\n47.5\n56.1\n59.4\n63.3\n76.7\n80.0\n71.8\n68.5\n63.2\n73.8\n36.8\n74.2\n60.5\n34.5\n72.2\n58.62\nInternVL2-8B [6]\n55.3\n56.6\n59.4\n44.9\n80.0\n83.3\n59.2\n67.2\n40.6\n78.5\n32.5\n64.5\n52.6\n31.0\n72.2\n54.63\nGPT-4o [1]\n40.2\n37.1\n59.4\n51.0\n73.3\n90.0\n40.8\n90.6\n41.4\n72.3\n27.8\n51.6\n81.6\n34.5\n77.8\n52.93\nGemini-1.5-Flash [43]\n43.4\n32.7\n52.0\n55.1\n70.0\n48.4\n62.0\n78.7\n47.3\n83.1\n26.3\n71.0\n78.9\n44.8\n70.4\n52.40\nVILA-1.5-40B [28]\n32.0\n56.6\n54.5\n42.9\n83.3\n86.7\n57.7\n67.7\n44.4\n75.4\n22.5\n74.2\n65.8\n48.3\n77.8\n51.23\nGPT-4o mini [1]\n33.8\n49.8\n50.5\n49.0\n80.0\n90.0\n31.0\n79.6\n41.4\n66.2\n26.8\n61.3\n76.3\n20.7\n79.6\n50.23\nGemini-1.5-Pro [43]\n33.8\n51.7\n51.5\n51.0\n73.3\n80.0\n70.4\n47.7\n36.4\n73.8\n37.3\n71.0\n84.2\n58.6\n75.9\n49.36\nQwen2-VL-7B [49]\n20.1\n46.8\n37.1\n38.8\n70.0\n76.7\n54.9\n73.2\n49.4\n72.3\n52.2\n61.3\n42.1\n17.2\n70.4\n49.30\nOryx-7B [34]\n34.7\n54.1\n57.4\n57.1\n80.0\n73.3\n66.2\n48.5\n34.7\n73.8\n41.6\n61.3\n39.5\n20.7\n66.7\n48.77\nGemini-1.5-Flash-8B [43]\n43.4\n45.9\n56.9\n36.7\n70.0\n76.7\n35.2\n69.8\n36.0\n73.8\n26.8\n48.4\n71.1\n24.1\n64.8\n48.59\nVideo-LLaMA2.1 [8]\n44.3\n35.6\n39.6\n51.0\n76.7\n83.3\n50.7\n60.9\n45.2\n75.4\n35.4\n58.1\n36.8\n20.7\n81.5\n47.77\nVideoChat2 [25]\n24.2\n58.0\n42.1\n44.9\n66.7\n83.3\n60.6\n62.6\n27.6\n73.8\n55.0\n35.5\n50.0\n10.3\n59.3\n47.36\nInternVL2-26B [6]\n33.3\n47.8\n39.6\n55.1\n76.7\n83.3\n56.3\n68.9\n33.9\n76.9\n25.4\n45.2\n34.2\n41.4\n75.9\n46.42\nLongVA [61]\n24.7\n41.0\n48.0\n40.8\n70.0\n73.3\n42.3\n51.9\n32.2\n72.3\n42.6\n48.4\n52.6\n34.5\n70.4\n43.73\nMiniCPM-V2.6 [55]\n28.3\n43.4\n43.6\n53.1\n73.3\n80.0\n50.7\n59.1\n23.0\n75.4\n22.0\n71.0\n57.9\n20.7\n72.2\n42.49\nInternVL2-4B [6]\n27.4\n42.9\n26.2\n32.7\n66.7\n73.3\n49.3\n60.4\n28.0\n78.5\n41.6\n35.5\n44.7\n10.3\n72.2\n41.68\nVideo-LLaMA2.1-AV [8]\n27.4\n45.9\n38.6\n55.1\n73.3\n76.7\n47.9\n46.4\n30.1\n81.5\n25.8\n45.2\n34.2\n34.5\n83.3\n41.50\nVILA-1.5-8B [28]\n31.5\n40.0\n37.6\n51.0\n63.3\n66.7\n40.8\n40.9\n26.8\n70.8\n37.8\n41.9\n60.5\n44.8\n59.3\n40.21\nGPT-4-turbo [1]\n23.7\n37.1\n35.1\n46.9\n63.3\n80.0\n25.4\n54.9\n36.4\n50.8\n29.7\n64.5\n39.5\n44.8\n70.4\n39.85\nLongLLaVA [52]\n28.3\n37.1\n27.2\n24.5\n60.0\n56.7\n54.9\n48.1\n32.6\n61.5\n38.3\n41.9\n26.3\n24.1\n66.7\n38.45\nKangaroo [30]\n29.2\n42.0\n24.3\n30.6\n56.7\n66.7\n57.7\n31.5\n26.8\n67.7\n47.8\n61.3\n21.1\n6.9\n55.6\n37.10\nInternVL2-2B [6]\n23.7\n24.4\n24.8\n36.7\n76.7\n63.3\n53.5\n48.9\n21.8\n80.0\n40.2\n29.0\n47.4\n6.9\n83.3\n36.75\nLongVILA [28]\n25.1\n35.6\n40.6\n40.8\n80.0\n60.0\n38.0\n32.8\n25.1\n76.9\n20.6\n64.5\n50.0\n37.9\n79.6\n36.46\nQwen2-VL-2B [49]\n21.0\n29.3\n25.2\n30.6\n63.3\n70.0\n42.3\n50.6\n23.8\n67.7\n37.3\n74.2\n34.2\n24.1\n63.0\n36.16\nVideo-LLaMA2-7B [8]\n25.1\n29.3\n23.3\n30.6\n70.0\n66.7\n40.8\n31.5\n26.4\n72.3\n40.2\n29.0\n44.7\n27.6\n66.7\n34.35\nVILA-1.5-3B [28]\n20.1\n32.7\n38.1\n51.0\n53.3\n46.7\n31.0\n26.4\n10.5\n72.3\n35.4\n32.3\n36.8\n10.3\n83.3\n31.95\nVideo-LLaVA [27]\n26.5\n25.9\n38.1\n32.7\n53.3\n40.0\n25.4\n26.8\n23.0\n55.4\n30.1\n38.7\n21.1\n51.7\n51.9\n31.07\nChat-UniVi [19]\n25.1\n31.7\n30.2\n30.6\n53.3\n30.0\n22.5\n26.4\n24.3\n29.2\n21.1\n32.3\n34.2\n44.8\n40.7\n28.02\nInternVL2-1B [6]\n24.7\n24.9\n22.8\n22.4\n46.7\n33.3\n22.5\n26.4\n26.8\n30.8\n30.6\n22.6\n23.7\n34.5\n29.6\n26.61\nRANDOM\n26.0\n25.8\n25.3\n24.3\n23.7\n23.7\n24.8\n25.0\n25.3\n27.4\n24.4\n20.3\n24.7\n25.2\n28.7\n25.33\nto the total questions across all sub-tasks:\nOverall ACC =\n1\nP\ni Ni\nX\ni\nNi\nX\nj=1\nsqj.\n(2)\n3. Main Results\nIn this section, we analyze and quantify the video composi-\ntion understanding capabilities of state-of-the-art MLLMs,\nproviding a comprehensive evaluation of these models. For\nall experiments, we use a standardized prompt template and\nthe default hyperparameters specified for each model.\nOverall Performance. As shown in Table 2, the overall per-\nformance on the VIDCOMPOSITION benchmark reveals that\nunderstanding intricate video compositions remains chal-\nlenging for MLLMs. While humans achieve exceptionally\nhigh scores (86.26), the leading models, such as LLaVA-\nOneVision-72B [23] (63.31), InternVL2-40B [6] (60.73),\nInternVL2-76B [6] (58.7) and Qwen2-VL-72B [49] (58.78),\ndemonstrate only moderate success, underscoring the com-\nplexity of the tasks and the current limitations in video\ncomposition understanding. This gap between human and\nmodel performance highlights the benchmark’s rigor and the\nneed for advancements in fine-grained video-based composi-\ntional learning. Open-source models with advanced vision\ncomponents, particularly InternVL2 variants, outperform\nAPI-based models like GPT-4o [1] (52.93) and Genmini-\n1.5-Flash [43] (52.40). The mean overall accuracy of these\nMLLMs is 43.44. For reference, the random-choice base-\nline has a score of 25.33. While the models exceed it, they\nstill face considerable obstacles in approaching human-level\nvideo composition understanding.\nStrengths & Weaknesses Analysis. From Table 2, we see\nthat MLLMs generally perform better in CU tasks, particu-\nlarly A-P and CMP-P. For example, top models like LLaVA-\nOneVision-72B [23], GPT-4o [1] and GPT-4o mini [1]\nachieve high scores in CMP-P; LLaVA-OneVision-72B [23]\nand InternVL2-40B [6] get 90.0 on A-P. This indicates that\nstate-of-the-art MLLMs can effectively recognize and in-\nterpret actions and visual details of characters in a scene.\nModels also show strong performance on SP tasks such\nas B-P and L-P, with models like Gemini-1.5-Flash [43]\nachieves 83.1 on B-P and LLaVA-OneVision-72B [23] gets\n90.3 on L-P. Additionally, these models achieve competitive\nscores on some MA tasks, such as AS-P and SE-P, with\ntop scores reaching 89.5 and 85.2, respectively. This strong\nperformance may be attributed to the fact that these tasks\n5\n\nTable 3. Resolution Analysis. Models are compared based on #frm, LLM size, and Res.. CA, CU, NU, SP, MA, and Overall are averaged\non models in each row. The results indicate that higher Res. leads to improved performance in most cases, with highlighted relative gains.\nModels\n#frm\nLLM\nsize\nRes.\nCA\nCU\nNU\nSP\nMA\nOverall\nChat-UniVi [19]; Video-LLaVA [27]; VideoChat2 [25]\n8\n7B\n224\n31.68\n42.22\n31.02\n40.11\n42.98\n34.94\nChat-UniVi-v1.5 [19]; LongVA [61]; Video-LLaMA2 [8]\n336\n33.6+1.92\n48.89+6.67\n32.42+1.4\n44.26+4.15\n50.96+7.98\n38.04+3.1\nVideo-LLaMA2.1 [8]; Video-LLaMA2.1-AV [8]\n384\n36.9+3.3\n55.28+6.39\n46.0+13.58\n43.11−1.15\n53.72+2.76\n43.7+5.66\nChat-UniVi [19]; Video-LLaVA [27]; VideoChat2 [25]\n16\n7B\n224\n29.66\n39.81\n31.86\n28.52\n34.71\n31.52\nChat-UniVi-v1.5 [19]; LongVA [61]; Video-LLaMA2 [8]\n336\n32.75+3.09\n49.07+9.26\n32.21+0.35\n44.92+16.4\n49.04+14.33\n37.67+6.15\nVideo-LLaMA2.1 [8]; Video-LLaMA2.1-AV [8]\n384\n37.06+4.31\n57.22+8.15\n45.68+13.47\n43.44−1.48\n54.13+5.09\n43.96+6.29\nLongVA [61]; Video-LLaMA2 [8]\n32\n7B\n336\n31.39\n46.67\n35.26\n44.26\n53.72\n37.98\nVideo-LLaMA2.1 [8]; Video-LLaMA2.1-AV [8]\n384\n38.5+7.11\n59.72+13.05\n45.47+10.21\n42.95−1.31\n54.55+0.83\n44.64+6.66\nFigure 4. #frm analysis. We compare the overall accuracy of\nmodels from the same series with the same LLM size and Res.. The\nresults indicate a counterintuitive irrelevance between the overall\naccuracy and input #frm.\nrely on some expert knowledge about video-making tech-\nniques, which MLLMs can acquire from massive corpora.\nConversely, the models encounter significant difficulties\nin more complex compositional tasks, especially CA. For\nexample, CamM-P and SS-P yield only modest scores, with\ntop models reaching 57.1 and 63.9, respectively, reflect-\ning a significant gap in understanding cinematic techniques.\nNU tasks, such as S-M and P-O, also present challenges,\nwith model performance substantially trailing behind human\nbenchmarks because there is often a gap between scripts\nand actual video presentation. Unlike humans, who can in-\ntuitively bridge this gap, MLLMs are fine-tuned on closely\nmatched vision-text pairs, limiting their ability to interpret\nsubtle or implied connections in narrative tasks. Additionally,\ncounting tasks, such as Cha-C, S-C, and Cut-C, remain par-\nticularly problematic for most models, further underscoring\nthe limitations in visual counting abilities and understanding\nscene transitions across multiple frames across all models.\n4. Diagnostic Analysis of Factors Affecting\nMLLMs’ Performance\nIn this section, we analyze the factors that may affect the\nMLLM’s understanding of video composition. We focus on\nfour factors: the number of input frames (#frm), the reso-\nTable 4. Overall accuracy of LongVA [61] and LongVILA [28] on\ndifferent #frm ranging from 4 to 128.\nModel\nRes.\nLLM\nsize\n#frm\n4\n8\n16\n32\n64\n128\nLongVA [61]\n336\n7B\n40.74\n43.73\n43.32\n41.62\n39.98\n39.39\nLongVILA [28]\nDynamic\n8B\n33.00\n35.87\n35.11\n36.46\n36.17\n36.11\nlution of the visual encoder (Res.), the size of the language\ndecoder (LLM size), and the volume of training data (Data\nvolume) in the SFT stage. We provide full analysis tables\nand figures of each factor in Supplementary.\nThe Number of Input Frames. Across all the models, we\nconsistently observe that the input frames don’t contribute\nto the performance. As shown in the Figure 4 and Table 4,\nthe overall accuracy is either stable or fluctuates randomly.\nWe couldn’t see any clear trends, although intuitively, extra\nframes would provide more information to help the model\nmake decisions. We suspect that while more frames provide\nmore information, this small amount of useful information\nis mixed in with a large amount of duplicate information,\nand the model cannot effectively extract it. This is against\nour expectation that it would bring benefits to counting tasks\n(Cha-C, S-C, Cut-C).\nThe Resolution of Visual Encoder.\nWe observe that\nMLLMs with higher-resolution visual encoders perform sig-\nnificantly better. While the resolution is unchangeable for\none specific model, we calculate the mean performance of all\nmodels with the same LLM size and video frames. As shown\nin Table 3, as resolution increases, performance on all five\nmain categories increases consistently. However, it is worth\nnoting that it is impossible to determine how much of this\nimprovement is due to the higher-resolution visual encoder\nand how much is due to the different models themselves.\nThe Size of Language Decoder. To analyze this relationship\nmore accurately, we compare models with different decoder\nsizes while keeping the encoder and training data constant.\nFrom Table 5, we observe that models with larger decoders\ndemonstrate stronger performance, and the gains are mainly\nfrom NU, which requires the model not only to recognize\nindividual frames but also to establish logical and causal\n6\n\nTable 5. LLM size analysis. We compare models from the same series with the same #frm and Res. but different LLM sizes. The results\nindicate that larger LLM sizes lead to improved performance in most cases, with highlighted relative gains.\nModel\nRes.\n#frm\nLLM size\nCA\nCU\nNU\nSP\nMA\nOverall\n2B\n25.08\n47.22\n37.05\n47.54\n44.63\n36.17\n7B\n34.35+9.27\n56.67+9.45\n61.05+24.0\n57.38+9.84\n48.76+4.13\n49.3+13.13\nQwen2-VL [49]\nDynamic\n2 fps\n72B\n50.48+16.13\n60.0+3.33\n71.16+10.11\n60.0+2.62\n46.28−2.48\n58.68+9.38\n3B\n26.84\n43.89\n19.16\n37.38\n47.11\n29.84\n8\n8B\n35.94+9.1\n52.78+8.89\n34.74+15.58\n42.62+5.24\n56.2+9.09\n40.04+10.2\n3B\n26.04\n41.67\n23.37\n34.75\n48.76\n30.13\nVILA-1.5 [28]\n384\n16\n8B\n35.78+9.74\n51.11+9.44\n36.42+13.05\n39.34+4.59\n60.33+11.57\n39.98+9.85\n7B\n25.88\n47.78\n28.84\n45.9\n50.41\n34.35\nVideo-LLaMA2 [8]\n336\n32\n72B\n54.15+28.27\n71.67+23.89\n65.68+36.84\n48.52+2.62\n59.5+9.09\n58.62+24.27\n0.5B\n24.12\n28.33\n26.53\n29.84\n28.93\n26.61\n1.8B\n24.28+0.16\n54.44+26.11\n35.16+8.63\n47.54+17.7\n53.72+24.79\n36.75+10.14\n3.8B\n32.11+7.83\n51.67−2.77\n44.0+8.84\n48.85+1.31\n48.76−4.96\n41.68+4.93\n8B\n57.03+24.92\n62.78+11.11\n53.68+9.68\n45.57−3.28\n56.2+7.44\n54.63+12.95\n20B\n40.1−16.93\n63.89+1.11\n51.16−2.52\n38.36−7.21\n54.55−1.65\n46.42−8.21\n34B\n54.95+14.85\n69.44+5.55\n65.47+14.31\n56.07+17.71\n70.25+15.7\n60.73+14.31\nInternVL2 [7]\n448\n16\n70B\n51.92−3.03\n72.78+3.34\n64.84−0.63\n52.79−3.28\n63.64−6.61\n58.73−2.0\nTable 6. Data volume analysis. We compare models with the same #frm, Res., and LLM sizes but using different Data volume in the SFT\nstage. The results indicate that larger Data volumes lead to improved performance in most cases, with highlighted relative gains.\nModel\n#frm\nRes.\nLLM size\nData volume\nCA\nCU\nNU\nSP\nMA\nOverall\nChat-UniVi [19]\n0.65M\n25.56\n34.44\n23.37\n25.9\n36.36\n26.73\nVideoChat2 [25]\n8\n224\n7B\n2M\n39.46+13.9\n57.78+23.34\n44.84+21.47\n58.03+32.13\n50.41+14.05\n47.01+20.28\nChat-UniVi-v1.5 [19]\n1.27M\n37.38\n47.78\n26.53\n37.38\n46.28\n36.11\nLongVA [61]\n8\n336\n7B\n1.32M\n37.54+0.16\n51.67+3.89\n41.89+15.36\n49.51+12.13\n56.2+9.92\n43.73+7.62\nVILA-1.5 [28]\n8B\n1.21M\n35.94\n52.78\n34.74\n42.62\n56.2\n40.04\nVideo-LLaMA2.1-AV [8]\n7B\n3.35M\n35.46−0.48\n52.78+0\n37.05+3.31\n39.34−3.28\n58.68+2.48\n40.09+0.05\nVideo-LLaMA2.1 [8]\n8\n384\n7B\n3.35M\n38.34+2.88\n57.78+5.0\n54.95+17.9\n46.89+7.75\n48.76−9.92\n47.3+7.22\nChat-UniVi [19]\n0.65M\n24.92\n32.22\n23.37\n24.92\n38.84\n26.26\nVideoChat2 [25]\n16\n224\n7B\n2M\n39.3+14.38\n60.0+27.78\n44.84+21.47\n31.8+6.88\n26.45−12.39\n40.8+14.54\nChat-UniVi-v1.5 [19]\n1.27M\n34.5\n48.89\n25.89\n40.98\n42.98\n35.4\nLongVA [61]\n16\n336\n7B\n1.32M\n37.86+3.36\n51.11+2.22\n41.89+16.0\n47.87+6.89\n53.72+10.74\n43.32+7.92\nVILA-1.5 [28]\n8B\n1.21M\n35.78\n51.11\n36.42\n39.34\n60.33\n39.98\nVideo-LLaMA2.1-AV [8]\n7B\n3.35M\n35.78+0\n56.67+5.56\n36.42+0\n40.0+0.66\n59.5−0.83\n40.62+0.64\nVideo-LLaMA2.1 [8]\n16\n384\n7B\n3.35M\n38.34+2.56\n57.78+1.11\n54.95+18.53\n46.89+6.89\n48.76−10.74\n47.3+6.68\nKangaroo [30]\n2.94M\n31.79\n51.67\n29.05\n53.44\n33.06\n37.1\nMiniCPM-V [55]\n64\n448\n8B\n8.32M\n38.18+6.39\n60.0+8.33\n40.84+11.79\n38.36−15.08\n55.37+22.31\n42.5+5.4\nrelationships across sequences, a capability that benefits\nfrom a more powerful language decoder. Tasks in MA also\nbenefit from the external knowledge acquired by LLMs.\nThe Volume of Training Data. We can observe the perfor-\nmance influence brought by fine-tuning the MLLMs with\nmore data from Table 6. We compare models with the same\nconfiguration, e.g. the same number of input frames, the\nsame resolution of the vision encoder, and the same or simi-\nlar size of LLM adopted. The results indicate that larger data\nvolumes lead to improved performance of video composition\nunderstanding in most cases.\nQualitative Analysis. We perform an error analysis to gain\ndeeper insights into the models’ shortcomings in fine-grained\nvideo composition understanding. In this analysis, the mod-\nels are required to answer questions and provide explanations\nin a dialogue format. Figure 5 shows the examples where\ntop models fail to predict correct answers. For example,\nwhile humans easily use visual context to distinguish cam-\nera movements and angles like “pan left” and “zoom in” or\nangles like “eye level” and “low angle,” models like LLaVA-\nOneVision-72B [23] and GPT-4 [1] often struggle due to\nscene transitions and subtle perspective changes.\n5. Related Work\nMLLMs for Video Understanding. Equipping LLMs with\nadapted video encoders has led to the creation of several\nmultimodal models tailored for video understanding [46].\nFor instance, GPT-4-turbo, GPT-4o, and GPT-4o-mini [1]\nare GPT-based models with integrated video comprehension\ncapabilities. InternVL2 [7], with parameter counts ranging\nfrom 1B to 76B, is based on the InternLM framework [4]\nand supports video processing at multiple scales. Addi-\ntionally, models derived from the LLaMA backbone—such\nas LLaVA-OneVision [23], VILA [28], VideoLLaMA [8],\nand LongLLaVA [52]—have been adapted for video input.\n7\n\nFigure 5. Qualitative analysis. Green represents correct answers, while red indicates wrong prediction or explanation. More cases can be\nfound in Supplementary.\nGemini has also been extended to include video process-\ning capabilities [43]. Other models, including Qwen [49],\nMiniCPM [55], Kangaroo [30], and Chat-UniVi [19], ex-\nhibit strong video understanding abilities. In our work, we\nthoroughly evaluate these models’ capabilities in video com-\nposition understanding and provide detailed analysis.\nEvaluation Benchmarks for MLLMs. Numerous bench-\nmarks for MLLMs have recently emerged to evaluate diverse\nmodel capabilities, with image captioning, Visual Question\nAnswering (VQA), and visual reasoning among the most\nfrequently assessed tasks. Image captioning [9, 29, 38, 42]\nmeasures an MLLM’s ability to generate text descriptions\nof visual content. VQA [2, 37, 39] assesses the model’s\nproficiency in answering questions based on visual inputs\nby integrating visual perception with language understand-\ning and external knowledge. Visual reasoning [17, 20, 45]\nevaluates a model’s spatial awareness and logical reasoning\nin processing visual information. Moreover, the comprehen-\nsive abilities of MLLMs are gauged using advanced bench-\nmarks [11, 13, 22, 31, 35, 36, 56, 57, 59]. For video MLLMs,\nsimilar efforts leverage existing benchmarks [26, 51] to eval-\nuate video understanding [24, 41]. However, a notable gap re-\nmains in evaluating models on the video-composition under-\n8\n\nstanding. This composition understanding is crucial for ac-\ncurately processing and correlating multiple elements within\na visual scene [14, 60]. While existing benchmarks assess\ncompositionally in images [16, 48], few comprehensively ad-\ndress the specific challenges of compositional understanding\nin video, where many MLLMs still show limitations.\n6. Conclusion\nWe introduce VIDCOMPOSITION, a novel and high-quality\nbenchmark designed to evaluate MLLMs in understanding\nvideo compositions. Our benchmark incorporates various\nvideo types and QA categories, covering various aspects of\nvideo composition, e.g. camera movement, shot size, nar-\nrative structure, and character actions. Through VIDCOM-\nPOSITION, we comprehensively assess MLLMs’ abilities\nto understand complex video compositions. The evaluation\nreveals a significant performance gap between humans and\nmodels, shedding light on the limitations of current MLLMs\nand providing valuable insights for future improvements.\nAcknowledgement\nThis work was supported in part by the National Eye Insti-\ntute of the National Institutes of Health under award number\nR01EY034562 and the Defense Advance Research Projects\nAgency under contract number HR00112220003. The con-\ntent is solely the responsibility of the authors and does not\nnecessarily represent the official views of the funding agen-\ncies; no official endorsement should be inferred.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 1,\n3, 5, 7\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425–\n2433, 2015. 8\n[3] Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen\nNguyen, and Chenliang Xu. Eagle: Egocentric aggregated\nlanguage-video engine. In Proceedings of the 32nd ACM In-\nternational Conference on Multimedia, page 1682–1691, New\nYork, NY, USA, 2024. Association for Computing Machinery.\n1\n[4] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, et al.\nInternlm2 technical report.\narXiv preprint\narXiv:2403.17297, 2024. 7\n[5] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang.\nAutoeval-video: An automatic benchmark for assessing large\nvision language models in open-ended video question answer-\ning. arXiv preprint arXiv:2311.14906, 2023. 2, 3\n[6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei\nGao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo,\nZheng Ma, et al. How far are we to gpt-4v? closing the gap\nto commercial multimodal models with open-source suites.\narXiv preprint arXiv:2404.16821, 2024. 5\n[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, et al.\nInternvl: Scaling up vision foundation models and align-\ning for generic visual-linguistic tasks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 24185–24198, 2024. 3, 7\n[8] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin\nLi, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang\nLuo, Deli Zhao, et al. Videollama 2: Advancing spatial-\ntemporal modeling and audio understanding in video-llms.\narXiv preprint arXiv:2406.07476, 2024. 1, 3, 5, 6, 7\n[9] Mingqian Feng, Yunlong Tang, Zeliang Zhang, and Chen-\nliang Xu. Do more details always introduce more halluci-\nnations in lvlm-based image captioning?\narXiv preprint\narXiv:2406.12663, 2024. 8\n[10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Meng-\ndan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, et al. Mme: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint\narXiv:2306.13394, 2023. 1, 2, 3\n[11] Chaoyou Fu, Peixian Chen, et al. Mme: A comprehensive\nevaluation benchmark for multimodal large language models.\nArXiv, abs/2306.13394, 2023. 8\n[12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai\nRen, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang\nShen, Mengdan Zhang, et al. Video-mme: The first-ever\ncomprehensive evaluation benchmark of multi-modal llms in\nvideo analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2,\n3\n[13] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia\nLi, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,\nYaser Yacoob, et al.\nHallusionbench: an advanced diag-\nnostic suite for entangled language hallucination and visual\nillusion in large vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14375–14385, 2024. 8\n[14] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kem-\nbhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable\nbenchmarks for vision-language compositionality. Advances\nin Neural Information Processing Systems, 36, 2024. 9\n[15] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo.\nV2xum-llm: Cross-modal video summarization with temporal\nprompt instruction tuning. arXiv preprint arXiv:2404.12353,\n2024. 1\n[16] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao,\nZhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo\nLuo.\nMmcomposition: Revisiting the compositionality\nof pre-trained vision-language models.\narXiv preprint\narXiv:2410.09733, 2024. 2, 3, 9\n[17] Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan\nZhang, John Collomosse, Scott Cohen, and Jiebo Luo. Fine-\nmatch: Aspect-based fine-grained image and text mismatch\ndetection and correction. In European Conference on Com-\nputer Vision, pages 474–491. Springer, 2025. 8\n9\n\n[18] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and\nGunhee Kim. Tgif-qa: Toward spatio-temporal reasoning\nin visual question answering. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n2758–2766, 2017. 3\n[19] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao,\nand Li Yuan. Chat-univi: Unified visual representation em-\npowers large language models with image and video un-\nderstanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13700–\n13710, 2024. 5, 6, 7, 8\n[20] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten,\nLi Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A\ndiagnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017. 8\n[21] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\nTvqa: Localized, compositional video question answering. In\nEMNLP, 2018. 2, 3\n[22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal llms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 8\n[23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li,\nHao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Zi-\nwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task\ntransfer, 2024. 3, 5, 7\n[24] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi\nLiu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang,\nand Yu Qiao. Mvbench: A comprehensive multi-modal video\nunderstanding benchmark, 2024. 8\n[25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\nMvbench: A comprehensive multi-modal video understand-\ning benchmark. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22195–\n22206, 2024. 1, 2, 3, 5, 6, 7\n[26] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,\nRohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,\nWilliam Yang Wang, Tamara Lee Berg, Mohit Bansal,\nJingjing Liu, Lijuan Wang, and Zicheng Liu. Value: A multi-\ntask benchmark for video-and-language understanding evalu-\nation, 2021. 8\n[27] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng\nJin, and Li Yuan. Video-llava: Learning united visual rep-\nresentation by alignment before projection. arXiv preprint\narXiv:2311.10122, 2023. 1, 5, 6\n[28] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad\nShoeybi, and Song Han. Vila: On pre-training for visual\nlanguage models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n26689–26699, 2024. 3, 5, 6, 7\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV. Springer, 2024. 8\n[30] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xi-\naoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie\nHu. Kangaroo: A powerful video-language model supporting\nlong-context video input. arXiv preprint arXiv:2408.15542,\n2024. 5, 7, 8\n[31] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n2, 8\n[32] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai\nRen, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcom-\npass: Do video llms really understand videos? arXiv preprint\narXiv:2403.00476, 2024. 2, 3\n[33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? In European Conference on Computer\nVision, pages 216–233. Springer, 2025. 1, 3\n[34] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu,\nand Yongming Rao. Oryx mllm: On-demand spatial-temporal\nunderstanding at arbitrary resolution, 2024. 5\n[35] Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha,\nManoj Acharya, Kushal Kafle, and Christopher Kanan.\nRevisiting multi-modal llm evaluation.\narXiv preprint\narXiv:2408.05334, 2024. 8\n[36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemati-\ncal reasoning of foundation models in visual contexts. arXiv\npreprint arXiv:2310.02255, 2023. 8\n[37] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195–3204, 2019. 8\n[38] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning.\nArXiv, abs/2203.10244, 2022. 8\n[39] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and\nC. V. Jawahar. Docvqa: A dataset for vqa on document\nimages. 2021 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 2199–2208, 2020. 8\n[40] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan,\nDongdong Chen, and Li Yuan. Video-bench: A comprehen-\nsive benchmark and toolkit for evaluating video-based large\nlanguage models. arXiv preprint arXiv:2311.16103, 2023. 2,\n3\n[41] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan,\nDongdong Chen, and Li Yuan. Video-bench: A comprehen-\nsive benchmark and toolkit for evaluating video-based large\nlanguage models, 2023. 8\n[42] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan\nBitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana\nParekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason\nBaldridge. DOCCI: Descriptions of Connected and Contrast-\ning Images. In arXiv:2404.19753, 2024. 8\n10\n\n[43] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry\nLepikhin, Timothy Lillicrap, Alayrac, et al. Gemini 1.5: Un-\nlocking multimodal understanding across millions of tokens\nof context. arXiv preprint arXiv:2403.05530, 2024. 1, 3, 5, 8\n[44] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang,\nHaoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian\nYe, Yanting Zhang, et al. Moviechat: From dense token to\nsparse memory for long video understanding. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18221–18232, 2024. 2, 3\n[45] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A\ncorpus of natural language for visual reasoning. In Annual\nMeeting of the Association for Computational Linguistics,\n2017. 8\n[46] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan\nLiang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin,\nRongyi Zhu, et al. Video understanding with large language\nmodels: A survey. arXiv preprint arXiv:2312.17432, 2023. 7\n[47] Yunlong Tang, Daiki Shimada, Jing Bi, and Chenliang Xu.\nAvicuna: Audio-visual llm with interleaver and context-\nboundary alignment for temporal referential dialogue. arXiv\npreprint arXiv:2403.16276, 2024. 1\n[48] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nWinoground: Probing vision and language models for visio-\nlinguistic compositionality. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5238–5248, 2022. 2, 3, 9\n[49] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan,\nJinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution. arXiv preprint\narXiv:2409.12191, 2024. 3, 5, 7, 8\n[50] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan\nZhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming\nDing, et al. Lvbench: An extreme long video understanding\nbenchmark. arXiv preprint arXiv:2406.08035, 2024. 2, 3\n[51] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang,\nand William Yang Wang. Vatex: A large-scale, high-quality\nmultilingual dataset for video-and-language research, 2020. 8\n[52] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang,\nand Benyou Wang. Longllava: Scaling multi-modal llms to\n1000 images efficiently via hybrid architecture. arXiv preprint\narXiv:2409.02889, 2024. 5, 7\n[53] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.\nNext-qa: Next phase of question-answering to explaining tem-\nporal actions. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n9777–9786, 2021. 2, 3\n[54] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video question answering\nvia gradually refined attention over appearance and motion.\nIn Proceedings of the 25th ACM international conference on\nMultimedia, pages 1645–1653, 2017. 2, 3\n[55] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui,\nHongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He,\net al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv\npreprint arXiv:2408.01800, 2024. 5, 7, 8\n[56] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. In International conference on machine learning.\nPMLR, 2024. 8\n[57] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and\nJiebo Luo. Promptfix: You prompt and we fix the photo.\narXiv preprint arXiv:2405.16785, 2024. 8\n[58] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting\nZhuang, and Dacheng Tao. Activitynet-qa: A dataset for\nunderstanding complex web videos via question answering. In\nProceedings of the AAAI Conference on Artificial Intelligence,\npages 9127–9134, 2019. 2, 3\n[59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, et al.\nMmmu: A massive multi-discipline multimodal understand-\ning and reasoning benchmark for expert agi. arXiv preprint\narXiv:2311.16502, 2023. 8\n[60] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan\nJurafsky, and James Zou. When and why vision-language\nmodels behave like bags-of-words, and what to do about\nit? In The Eleventh International Conference on Learning\nRepresentations, 2022. 9\n[61] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,\nJingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan,\nChunyuan Li, and Ziwei Liu. Long context transfer from\nlanguage to vision. arXiv preprint arXiv:2406.16852, 2024.\n5, 6, 7\n11",
    "pdf_filename": "VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos.pdf"
}