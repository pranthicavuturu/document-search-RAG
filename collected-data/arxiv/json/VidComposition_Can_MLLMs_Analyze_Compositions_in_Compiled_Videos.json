{
    "title": "VIDCOMPOSITION: Can MLLMs Analyze Compositions in Compiled Videos?",
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs)hasenabledsignificantprogressinmultimodalun- derstanding,expandingtheircapacitytoanalyzevideocon- tent. However,existingevaluationbenchmarksforMLLMs a detailed assessment of their ability to understand video compositions,thenuancedinterpretationofhowvisualel- ementscombineandinteractwithinhighlycompiledvideo contexts. WeintroduceVidComposition,anewbenchmark specificallydesignedtoevaluatethevideocompositionun- derstandingcapabilitiesofMLLMsusingcarefullycurated compiledvideosandcinematic-levelannotations. VidCom- position includes 982 videos with 1706 multiple-choice questions,coveringvariouscompositionalaspectssuchas camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs re- veals a significant performance gap between human and Figure1.TopMLLMs’performanceonVIDCOMPOSITION,across modelcapabilities. Thishighlightsthelimitationsofcurrent 15tasksof5aspectsofvideocompositionunderstanding: Cine- MLLMsinunderstandingcomplex,compiledvideocompo- matographyAnalysis,CharacterUnderstanding,NarrativeUnder- standing,ScenePerception,andMakingAnalysis. sitions and offers insights into areas for further improve- ment. Theleaderboardandevaluationcodeareavailableat https://yunlong10.github.io/VidComposition/. editingandintegratingmultipleclips,scenes,orsequences, eitherfromvarioussourcesorfromdifferentsegmentsofa singlerecording,e.g.films,TVseries,documentaries,ani- 1.Introduction mations,vlogs,etc. Thesevideosarecarefullyconstructed RecentadvancementsinMultimodalLargeLanguageMod- tocreateaseamlessflowandincluderichercompositions, els(MLLMs)[1,3,8,15,27,43,47]havegreatlyenhanced requiringshot-by-shotanalysistointerpret. capabilitiesinunderstandingmultimodality. However,while Shot-by-shotanalysis,atechniquewherecreatorsmeticu- currentbenchmarks[10,12,25,33]forevaluatingMLLMs louslybreakdowntheelementsofavideo,servesasavital assessgeneralimageorvideocomprehension,theylacka tool for understanding video composition in depth. This detailedfocusonvideocomposition,thenuancedinterpre- levelofunderstanding,essentialinfilmanalysisandvideo tationofhowvisualelementscombineandinteractwithin production,goesbeyondgeneralsceneoractionrecognition, compiledvideos. Compiledvideosrefertothosecreatedby requiringanin-depthgraspofcompositionalelementssuch 1 4202 voN 91 ]VC.sc[ 2v97901.1142:viXra",
    "body": "VIDCOMPOSITION: Can MLLMs Analyze Compositions in Compiled Videos?\nYunlongTang1,∗,JunjiaGuo1,∗,HangHua1,SusanLiang1,MingqianFeng1,XinyangLi1,\nRuiMao1,ChaoHuang1,JingBi1,ZeliangZhang1,PooyanFazli2,ChenliangXu1†\n1UniversityofRochester,2ArizonaStateUniversity\n{yunlong.tang, mingqian.feng, jing.bi, chenliang.xu}@rochester.edu, pooyan@asu.edu\n{jguo40, xli190, rmao6, zzh136}@ur.rochester.edu, {hhua2, sliang22, chuang65}@cs.rochester.edu\nAbstract\nThe advancement of Multimodal Large Language Models\n(MLLMs)hasenabledsignificantprogressinmultimodalun-\nderstanding,expandingtheircapacitytoanalyzevideocon-\ntent. However,existingevaluationbenchmarksforMLLMs\nprimarilyfocusonabstractvideocomprehension,lacking\na detailed assessment of their ability to understand video\ncompositions,thenuancedinterpretationofhowvisualel-\nementscombineandinteractwithinhighlycompiledvideo\ncontexts. WeintroduceVidComposition,anewbenchmark\nspecificallydesignedtoevaluatethevideocompositionun-\nderstandingcapabilitiesofMLLMsusingcarefullycurated\ncompiledvideosandcinematic-levelannotations. VidCom-\nposition includes 982 videos with 1706 multiple-choice\nquestions,coveringvariouscompositionalaspectssuchas\ncamera movement, angle, shot size, narrative structure,\ncharacter actions and emotions, etc. Our comprehensive\nevaluation of 33 open-source and proprietary MLLMs re-\nveals a significant performance gap between human and Figure1.TopMLLMs’performanceonVIDCOMPOSITION,across\nmodelcapabilities. Thishighlightsthelimitationsofcurrent 15tasksof5aspectsofvideocompositionunderstanding: Cine-\nMLLMsinunderstandingcomplex,compiledvideocompo- matographyAnalysis,CharacterUnderstanding,NarrativeUnder-\nstanding,ScenePerception,andMakingAnalysis.\nsitions and offers insights into areas for further improve-\nment. Theleaderboardandevaluationcodeareavailableat\nhttps://yunlong10.github.io/VidComposition/.\neditingandintegratingmultipleclips,scenes,orsequences,\neitherfromvarioussourcesorfromdifferentsegmentsofa\nsinglerecording,e.g.films,TVseries,documentaries,ani-\n1.Introduction mations,vlogs,etc. Thesevideosarecarefullyconstructed\nRecentadvancementsinMultimodalLargeLanguageMod- tocreateaseamlessflowandincluderichercompositions,\nels(MLLMs)[1,3,8,15,27,43,47]havegreatlyenhanced requiringshot-by-shotanalysistointerpret.\ncapabilitiesinunderstandingmultimodality. However,while Shot-by-shotanalysis,atechniquewherecreatorsmeticu-\ncurrentbenchmarks[10,12,25,33]forevaluatingMLLMs louslybreakdowntheelementsofavideo,servesasavital\nassessgeneralimageorvideocomprehension,theylacka tool for understanding video composition in depth. This\ndetailedfocusonvideocomposition,thenuancedinterpre- levelofunderstanding,essentialinfilmanalysisandvideo\ntationofhowvisualelementscombineandinteractwithin production,goesbeyondgeneralsceneoractionrecognition,\ncompiledvideos. Compiledvideosrefertothosecreatedby requiringanin-depthgraspofcompositionalelementssuch\n1\n4202\nvoN\n91\n]VC.sc[\n2v97901.1142:viXra\nFigure2.VIDCOMPOSITIONcomprises15categoriesofhigh-qualityQApairs,focusingonfiveaspectsofcompositionsincompiledvideos:\ncinematography,character,narrative,scene,andmaking.Thecorrectanswersare highlighted.\nascameramovements,shotsizes,narrativestructures,and understanding,theirevaluationsofcompositionalityremain\ncharacterdynamics. Thisanalysisalsocapturestheintricate limited. Additionally,mostvideosinthesebenchmarksare\nlayersofvisualstorytellingbydeconstructinghowtechnical natural-shot rather than compiled, posing a challenge for\nandartisticchoicesshapetheviewingexperience. However, modelstrainedonnaturalfootagetoeffectivelyinterpretthe\nachievingthisfine-grainedlevelofcompositionunderstand- increasinglyprevalenteditedandcompiledvideosseenon\ning remains a significant challenge for existing MLLMs, modernonlinevideoplatforms.\nwhich primarily operate on broader, more coarse-grained Recognizingthegapinexistingevaluationmethods,we\ninterpretationsofvideocontent. introduceVIDCOMPOSITION,anewbenchmarkdesigned\nThroughinvestigatingexistingbenchmarks,weidentified to assess MLLMs on understanding video composition at\ntheirlimitationsinevaluatingMLLMinvideocomposition a cinematic level. VIDCOMPOSITION includes 982 care-\nunderstanding. As shown in Table 1, the benchmarks in fully curated videos and 1,706 multiple-choice questions,\nthefirstgroup[10,16,31]primarilyfocusonstaticimages featuringmeticulouslyannotatedclipsfromfilms,TVseries,\nandoverlookthedynamicaspectsofvisualcontent. Among animations, commentary videos, etc. These questions in-\nthese,Winoground[48]andMMComposition[16]attempts cludefivekeyareasofvideocomposition: Cinematography\ntoassessthecompositionalityofMLLMs,thoughitislim- Analysis,CharacterUnderstanding,NarrativeUnderstand-\nitedtoimage-basedevaluations. Thesecondgroupconsists ing,ScenePerception,andMakingAnalysis,spanning15\noftraditionalvideobenchmarks[21,53,54,58],whichare distincttasks. Eachareacapturescriticalaspectsofcompo-\nlesseffectiveataddressingthespecificlimitationsofmodern sitionalunderstanding,e.g.cameramovements,angles,shot\nMLLMs. WhileTVQA[21]includesacompositionalvideo sizes,narrativestructures,characters,scenes,cuts,special\nQA component, its compositionality is relatively coarse- effects,etc.,providingaextensiveframeworkforevaluating\ngrained,limitedtobasicquestiontypeslike“who,”“when,” thenuancedcomprehensionrequiredincinematiccontexts.\n“where,”“how,”and“what.”Thethirdgrouphighlightsrecent Weevaluate33state-of-the-artMLLMsonVIDCOMPO-\nbenchmarks[5,12,25,32,40,44,50]developedtoassess SITION,including27open-sourceand6proprietarymodels,\nMLLMs’videocomprehensioncapabilities. Althoughthese revealingasubstantialperformancegapbetweenMLLMs\nbenchmarksincorporatetasksthattouchoncompositional andhuman-levelcomprehensioninvideocompositionun-\n2\nTable1. Acomparativeoverviewofvariousbenchmarksacross 2. VIDCOMPOSITION\nseveraldimensions,suchasdataformat(imageIorvideoV),the\nsizeofdatasetforevaluation(#Data),thenumberoftaskscovered 2.1.OverviewandTerminology\n(#Task),whetherthedatasetsupportscompositionalquestionan- VIDCOMPOSITIONcontains982compiledvideoswith1706\nswering(CompositionalQA),thepresenceofCompiledVideos human-annotatedmultiple-choicequestionsforvideocom-\nandFine-Grainedsub-tasks,andtheannotationmethod(manual\npositionunderstanding,including5maincategories: Cine-\norautomatic/manual,indicatedbyAnno.).\nmatographyAnalysis(CA),CharacterUnderstanding(CU),\nComposi- Compiled Fine- NarrativeUnderstanding(NU),ScenePerception(SP),and\nBenchmark I/V #Data #Task Anno.\ntionalQA Videos Grained MakingAnalysis(MA);and15sub-tasks: CameraMove-\nWinoground[48] I 400 8 ✓ - ✗ M mentPerception(CamM-P),ShotSizePerception(SS-P),\nMME[10] I 1.1k 14 ✗ - ✓ M\nMMBench[33] I 1.7k 20 ✗ - ✓ A+M CameraAnglePerception(CamA-P),EmotionPerception\nMMComposition[16] I 4.3k 13 ✓ - ✓ M (E-P), Action Perception (A-P), Costume, Makeup and\nMSVD-QA[54] V 504 5 ✗ ✗ ✗ A PropsPerception(CMP-P),CharacterCounting(Cha-C),\nMSRVTT-QA[54] V 2.9k 5 ✗ ✗ ✗ A\nTGIF-QA[18] V 9.6k 4 ✗ ✗ ✗ A ScriptMatching(S-M),PlotOrdering(P-O),Background\nTVQA[21] V 2.2k 8 ✓ ✓ ✗ A+M Perception(B-P),SceneCounting(S-C),LightingPercep-\nActivityNet-QA[58] V 5.8k 4 ✗ ✗ ✗ M\ntion(L-P),ArtStylePerception(AS-P),CutCounting(Cut-\nNExT-QA[53] V 1k 8 ✗ ✗ ✗ A\nC),andSpecialEffectPerception(SE-P).Examplesofeach\nAutoEval-Video[5] V 327 9 ✗ ✗ ✗ A+M\nVideo-Bench[40] V 5.9k 10 ✗ ✗ ✗ A+M taskcanbefoundin Figure2. Thedetaileddefinitionsof\nLVBench[50] V 500 6 ✗ ✗ ✗ M\neachtaskareprovidedinSupplementary.\nMVBench[25] V 3.6k 20 ✗ ✗ ✓ A+M\nMovieChat-1k[44] V 100 8 ✗ ✓ ✓ M\nTempCompass[32] V 410 4 ✗ ✗ ✓ M 2.2.DatasetCurationProcess\nVideo-MME[12] V 900 12 ✗ ✗ ✓ M\nVIDCOMPOSITION V 982 15 ✓ ✓ ✓ M Video Collection and Filtering. Our dataset comprises\nvideos sourced from the Internet, focusing on compiled\nvideos primarily derived from commentary videos for\nmovies, TV series, and animations, which have no copy-\nderstanding. As shown in Figure 1, although top mod-\nrightconcerns. Thesevideostypicallyincludesubtitlesand\nels[1,7,8,23,28,43,49]performwellonbasicperception\nscriptsuploadedbyusers,whichassistinlaterannotation\ntasks(e.g.actionperception),theyfallshortincomprehend-\nstages. Wefurtherrefinethecollectedvideosbyfilteringout\ningcomplexvideocompositions,particularlyincinematog-\ninappropriatecontent,suchasclipsthatmaycausepsycho-\nraphy. Thisperformancedisparityunderscorescurrentmod-\nlogicaldistressorthoseflaggedassensitivebyAPI-based\nels’ limitations in capturing intricate, multi-layered video\nmodels. The average duration of the collected videos is\nstructures. Additionalexperimentsfurtheranalyzefactors\nabout20minutes. Forvideoswithsubtitlesorscripts,we\ninfluencing MLLM performance, such as the number of\nextract the timestamps marking the start and end of each\nframesprovidedasinput,theresolutionofvisualencoders,\nsubtitle. Withthisinformation,wefurthersegmentthevideo\nthesizeoflanguagedecoders,andthedatavolumeforfine-\nintocoherentsectionswhoseaveragelengthis794frames.\ntuning,yieldinginsightsforfutureadvancementsinmodel\nToavoidmodelsfrompredictinganswersrelyingonspeech,\ndesign. Overall,ourbenchmarkoffersvaluableinsightsfor\nweremovedtheaudioofthevideos.\nenhancingMLLMsandalsosuggestsapplicationsinvideo\ngenerationwhereMLLMscouldassistinautomaticallyeval- HumanAnnotation. Toensurethequalityandreliabilityof\nuatingthecompositionalqualityofgeneratedvideos. thedataset,weengagemultiplehumanannotators,assign-\ningeachvideosegmenttoseveralannotatorstominimize\nInsummary,ourcontributionisthree-fold: potentialbiases. Allquestionsaremeticulouslydesignedto\naddressspecifictasks.ForperceptiontaskssuchasA-P,E-P,\n• We introduce VIDCOMPOSITION, a novel, human- CMP-P,B-P,andforcountingtaskssuchasCha-C,S-C,\nannotated, high-quality benchmark for evaluating fine- andCut-C,annotatorswatchthevideosegmentandwrite\ngrainedvideocompositionunderstandinginMLLMs. the correct answer alongside several incorrect (distractor)\n• We comprehensively evaluate 33 MLLMs for video un- options. FortaskssuchasCamM-P,SS-P,CamA-P,L-P,\nderstanding with VIDCOMPOSITION. The results show SE-P,andAS-P,weprovideapredefinedsetofselectable\nthechallengingnatureofVIDCOMPOSITIONandthesub- labels. Forexample,forCamM-P,thelabelsincludezoom\nstantialgapbetweenMLLMs’andhumans’capabilities in, zoom out, pan left, pan right, pan up, pan down, and\ninvideocompositionunderstanding. staticshot. Annotatorschooseappropriatelabelsforeach\n• We analyze the critical factors that influence the perfor- videosegment,whicharethenusedascorrectoptions,while\nmanceofMLLMssystematically,providingpotentialdi- distractorsarerandomlyselectedfromtheremaininglabels,\nrectionsformodelimprovementandfutureadvancements. ensuringtheydifferfromthecorrectoptions. ForS-M,we\n3\nFigure3. (Left)TaskstatisticsinVIDCOMPOSITION,organizedintofivemaincategories: CinematographyAnalysis(CA),Character\nUnderstanding(CU),NarrativeUnderstanding(NU),ScenePerception(SP),andMakingAnalysis(MA),comprisingatotalof15sub-tasks.\nThenumberofQApairsisshowninparenthesesbeloweachtask. (Right)Thedifficultydistributionacrossthesefivecategories. Ifa\nquestionisansweredcorrectlybymorethan60%ofMLLMs,itwillbelabeledas“Easy.”Conversely,ifaquestionisansweredcorrectlyby\nfewerthan10%ofMLLMs,itwillbelabeledas“SuperHard.”\nusethevideo’scommentaryscript,extractedfromthesubti- templateP canbefoundinSupplementary.\ntlefile,asthecorrectoption,withdistractoroptionssourced\nfromnearbysegments’scriptstocreateplausiblealternatives. Algorithm1ModelPrediction\nForP-O,wesegmentthecommentaryscriptintomultiple\n1: Input:Questionq,OptionsO q\nparts,shufflingandinsertingthemintothequestionwithse- 2: Output:PredictionA\nquencenumbers. Thecorrectansweristheoriginalorderof 3: I q←P(q,O q)\nthescript,whileotheroptionsaregeneratedbyrandomizing 4: R q← M(I q)\nthesesequencenumbers.\nR\nq\nifR q∈{A,B,C,D},\n5: A q← AM(R q) ifmorelettersinR q,\nQualityControl. Toensurethequalityofourbenchmark,\nRS({A,B,C,D})\notherwise.\neachvideoandcorrespondingQApairundergoesmultiple\nrounds of review. We implemented an annotation review The model output R is first checked for validity as a\nq\nsystem (see the user interface in Supplementary), which singlecharacterfrom{A,B,C,D}. AnAnswer-Matching\ndisplaystheannotatedvideo,question,andansweroptions (AM)functionidentifiesavalidoptioniftheoutputincludes\nalongsideanadditionalfeedbackoptionforreviewerstopro- multiplecharacters. TheimplementationoftheAMfunction\nvide corrections or comments. Reviewers are required to canbefoundintheSupplementaryMaterials. Arandomse-\nattempteachquestionthemselves;iftheyidentifyerrorsin lection(RS)functionfrom{A,B,C,D}isusedtogenerate\nthequestionoroptions,theycanselectthefeedbackoption A ifnovalidmatchisfound.\nq\nto either suggest improvements to the question or specify OnceA isobtained,weevaluateitsaccuracybasedon\nq\nwhattheybelieve isthecorrectanswer. Aftereachround whetheritmatchesthecorrectanswerforeachquestion. Let\nofreview,allfeedbacksubmittedthroughtheannotationre- S ={S ={Q }Ni }|S| representourdataset,whereeach\ni j j=1 i=1\nviewsystemisanalyzedandusedtoenhancetheannotation\nsub-taskS containsasetofquestionsQ acrossatotalof\ni j\nqualityfurther. Thisiterativequalitycontrolprocessensures\n|S|sub-tasks. Foreachquestionq ∈S,letG representthe\nq\naccuracyandconsistencyacrossannotations,minimizeser-\ncorrectanswer,andA representthemodel’sanswer. The\nq\nrors,andrefinesquestionclarityforeachtask.\nscoreforquestionq,denotedass ,iscalculatedasfollows:\nq\n(cid:40)\n1, ifA =G ,\n2.3.EvaluationMetrics s = q q (1)\nq\n0, ifA ̸=G .\nTo obtain model predictions, each question is structured q q\nwithin a predefined prompt template P that includes the Ascoreof1isassignedifthemodel’spredictionA matches\nq\nquestiontextandassociatedoptionsO (A,B,C,D,along thecorrectanswerG ;otherwise,thescoreis0. Eachsub-\nq q\nwithdescriptivetexts). ThispromptI isthenfedintothe task’s accuracy ACC is calculated as the average score\nq i\nmodelM,whichisexpectedtooutputasinglecharacterrep- across its N questions: ACC = N\n−1(cid:80)Ni\ns , where\ni i i j=1 qj\nresentingitspredictedanswer(oneof{A,B,C,D}). The N isthetotalnumberofquestionsinsub-taskS . Theover-\ni i\npredictionprocessisformalizedinAlgorithm1. Theprompt allaccuracyiscomputedastheratiooftotalcorrectanswers\n4\nTable2.Thecomprehensiveevaluationof30MLLMsonVIDCOMPOSITION,includingopensourcemodelsand API-basedmodels.The\nbestresultsareinbold,andsecondbestresultsareinunderlined,respectively.\nCinematographyAnalysis CharacterUnderstanding NarrativeUnderst. ScenePerception MakingAnalysis\nMethod Overall\nCamM-P SS-P CamA-P E-P A-P CMP-P Cha-C S-M P-O B-P S-C L-P AS-P Cut-C SE-P\nHuman 84.1 85.4 80.0 82.6 92.3 92.9 94.1 97.0 97.5 94.4 80.2 81.8 85.7 87.5 94.7 86.26\nLLaVA-OneVision-72B[23] 57.1 60.5 66.3 63.3 90.0 90.0 74.6 84.7 72.4 76.9 12.0 90.3 89.5 34.5 74.1 63.31\nInternVL2-40B[6] 46.6 60.0 58.9 51.0 90.0 83.3 67.6 79.6 51.9 80.0 47.4 64.5 68.4 44.8 85.2 60.73\nInternVL2-76B[6] 46.6 63.9 45.5 51.0 86.7 86.7 76.1 81.3 49.0 80.0 44.5 51.6 76.3 24.1 75.9 58.73\nQwen2-VL-72B[49] 37.9 56.6 57.9 34.7 76.7 76.7 63.4 76.2 66.5 73.8 53.6 74.2 65.8 3.4 55.6 58.68\nVideo-LLaMA2-72B[8] 47.5 56.1 59.4 63.3 76.7 80.0 71.8 68.5 63.2 73.8 36.8 74.2 60.5 34.5 72.2 58.62\nInternVL2-8B[6] 55.3 56.6 59.4 44.9 80.0 83.3 59.2 67.2 40.6 78.5 32.5 64.5 52.6 31.0 72.2 54.63\nGPT-4o[1] 40.2 37.1 59.4 51.0 73.3 90.0 40.8 90.6 41.4 72.3 27.8 51.6 81.6 34.5 77.8 52.93\nGemini-1.5-Flash[43] 43.4 32.7 52.0 55.1 70.0 48.4 62.0 78.7 47.3 83.1 26.3 71.0 78.9 44.8 70.4 52.40\nVILA-1.5-40B[28] 32.0 56.6 54.5 42.9 83.3 86.7 57.7 67.7 44.4 75.4 22.5 74.2 65.8 48.3 77.8 51.23\nGPT-4omini[1] 33.8 49.8 50.5 49.0 80.0 90.0 31.0 79.6 41.4 66.2 26.8 61.3 76.3 20.7 79.6 50.23\nGemini-1.5-Pro[43] 33.8 51.7 51.5 51.0 73.3 80.0 70.4 47.7 36.4 73.8 37.3 71.0 84.2 58.6 75.9 49.36\nQwen2-VL-7B[49] 20.1 46.8 37.1 38.8 70.0 76.7 54.9 73.2 49.4 72.3 52.2 61.3 42.1 17.2 70.4 49.30\nOryx-7B[34] 34.7 54.1 57.4 57.1 80.0 73.3 66.2 48.5 34.7 73.8 41.6 61.3 39.5 20.7 66.7 48.77\nGemini-1.5-Flash-8B[43] 43.4 45.9 56.9 36.7 70.0 76.7 35.2 69.8 36.0 73.8 26.8 48.4 71.1 24.1 64.8 48.59\nVideo-LLaMA2.1[8] 44.3 35.6 39.6 51.0 76.7 83.3 50.7 60.9 45.2 75.4 35.4 58.1 36.8 20.7 81.5 47.77\nVideoChat2[25] 24.2 58.0 42.1 44.9 66.7 83.3 60.6 62.6 27.6 73.8 55.0 35.5 50.0 10.3 59.3 47.36\nInternVL2-26B[6] 33.3 47.8 39.6 55.1 76.7 83.3 56.3 68.9 33.9 76.9 25.4 45.2 34.2 41.4 75.9 46.42\nLongVA[61] 24.7 41.0 48.0 40.8 70.0 73.3 42.3 51.9 32.2 72.3 42.6 48.4 52.6 34.5 70.4 43.73\nMiniCPM-V2.6[55] 28.3 43.4 43.6 53.1 73.3 80.0 50.7 59.1 23.0 75.4 22.0 71.0 57.9 20.7 72.2 42.49\nInternVL2-4B[6] 27.4 42.9 26.2 32.7 66.7 73.3 49.3 60.4 28.0 78.5 41.6 35.5 44.7 10.3 72.2 41.68\nVideo-LLaMA2.1-AV[8] 27.4 45.9 38.6 55.1 73.3 76.7 47.9 46.4 30.1 81.5 25.8 45.2 34.2 34.5 83.3 41.50\nVILA-1.5-8B[28] 31.5 40.0 37.6 51.0 63.3 66.7 40.8 40.9 26.8 70.8 37.8 41.9 60.5 44.8 59.3 40.21\nGPT-4-turbo[1] 23.7 37.1 35.1 46.9 63.3 80.0 25.4 54.9 36.4 50.8 29.7 64.5 39.5 44.8 70.4 39.85\nLongLLaVA[52] 28.3 37.1 27.2 24.5 60.0 56.7 54.9 48.1 32.6 61.5 38.3 41.9 26.3 24.1 66.7 38.45\nKangaroo[30] 29.2 42.0 24.3 30.6 56.7 66.7 57.7 31.5 26.8 67.7 47.8 61.3 21.1 6.9 55.6 37.10\nInternVL2-2B[6] 23.7 24.4 24.8 36.7 76.7 63.3 53.5 48.9 21.8 80.0 40.2 29.0 47.4 6.9 83.3 36.75\nLongVILA[28] 25.1 35.6 40.6 40.8 80.0 60.0 38.0 32.8 25.1 76.9 20.6 64.5 50.0 37.9 79.6 36.46\nQwen2-VL-2B[49] 21.0 29.3 25.2 30.6 63.3 70.0 42.3 50.6 23.8 67.7 37.3 74.2 34.2 24.1 63.0 36.16\nVideo-LLaMA2-7B[8] 25.1 29.3 23.3 30.6 70.0 66.7 40.8 31.5 26.4 72.3 40.2 29.0 44.7 27.6 66.7 34.35\nVILA-1.5-3B[28] 20.1 32.7 38.1 51.0 53.3 46.7 31.0 26.4 10.5 72.3 35.4 32.3 36.8 10.3 83.3 31.95\nVideo-LLaVA[27] 26.5 25.9 38.1 32.7 53.3 40.0 25.4 26.8 23.0 55.4 30.1 38.7 21.1 51.7 51.9 31.07\nChat-UniVi[19] 25.1 31.7 30.2 30.6 53.3 30.0 22.5 26.4 24.3 29.2 21.1 32.3 34.2 44.8 40.7 28.02\nInternVL2-1B[6] 24.7 24.9 22.8 22.4 46.7 33.3 22.5 26.4 26.8 30.8 30.6 22.6 23.7 34.5 29.6 26.61\nRANDOM 26.0 25.8 25.3 24.3 23.7 23.7 24.8 25.0 25.3 27.4 24.4 20.3 24.7 25.2 28.7 25.33\ntothetotalquestionsacrossallsub-tasks: tionallearning. Open-sourcemodelswithadvancedvision\n1\n(cid:88)(cid:88)Ni components, particularly InternVL2 variants, outperform\nOverallACC= s . (2) API-based models like GPT-4o [1] (52.93) and Genmini-\n(cid:80) N qj\ni i i j=1 1.5-Flash[43](52.40). Themeanoverallaccuracyofthese\nMLLMsis43.44. Forreference, therandom-choicebase-\n3.MainResults linehasascoreof25.33. Whilethemodelsexceedit,they\nstillfaceconsiderableobstaclesinapproachinghuman-level\nInthissection,weanalyzeandquantifythevideocomposi-\nvideocompositionunderstanding.\ntionunderstandingcapabilitiesofstate-of-the-artMLLMs,\nprovidingacomprehensiveevaluationofthesemodels. For\nStrengths&WeaknessesAnalysis. FromTable2,wesee\nallexperiments,weuseastandardizedprompttemplateand\nthatMLLMsgenerallyperformbetterinCUtasks,particu-\nthedefaulthyperparametersspecifiedforeachmodel.\nlarlyA-PandCMP-P.Forexample,topmodelslikeLLaVA-\nOverallPerformance. AsshowninTable2,theoverallper- OneVision-72B [23], GPT-4o [1] and GPT-4o mini [1]\nformanceontheVIDCOMPOSITIONbenchmarkrevealsthat achievehighscoresinCMP-P;LLaVA-OneVision-72B[23]\nunderstanding intricate video compositions remains chal- andInternVL2-40B[6]get90.0onA-P.Thisindicatesthat\nlengingforMLLMs. Whilehumansachieveexceptionally state-of-the-art MLLMs can effectively recognize and in-\nhigh scores (86.26), the leading models, such as LLaVA- terpret actions and visual details of characters in a scene.\nOneVision-72B [23] (63.31), InternVL2-40B [6] (60.73), Models also show strong performance on SP tasks such\nInternVL2-76B[6](58.7)andQwen2-VL-72B[49](58.78), as B-P and L-P, with models like Gemini-1.5-Flash [43]\ndemonstrateonlymoderatesuccess,underscoringthecom- achieves83.1onB-PandLLaVA-OneVision-72B[23]gets\nplexity of the tasks and the current limitations in video 90.3onL-P.Additionally,thesemodelsachievecompetitive\ncompositionunderstanding. Thisgapbetweenhumanand scores on some MA tasks, such as AS-P and SE-P, with\nmodelperformancehighlightsthebenchmark’srigorandthe topscoresreaching89.5and85.2,respectively. Thisstrong\nneedforadvancementsinfine-grainedvideo-basedcomposi- performance may be attributed to the fact that these tasks\n5\nTable3.ResolutionAnalysis.Modelsarecomparedbasedon#frm,LLMsize,andRes..CA,CU,NU,SP,MA,andOverallareaveraged\nonmodelsineachrow.TheresultsindicatethathigherRes.leadstoimprovedperformanceinmostcases,withhighlightedrelativegains.\nLLM\nModels #frm Res. CA CU NU SP MA Overall\nsize\nChat-UniVi[19];Video-LLaVA[27];VideoChat2[25] 224 31.68 42.22 31.02 40.11 42.98 34.94\nChat-UniVi-v1.5[19];LongVA[61];Video-LLaMA2[8] 8 7B 336 33.6+1.92 48.89+6.67 32.42+1.4 44.26+4.15 50.96+7.98 38.04+3.1\nVideo-LLaMA2.1[8];Video-LLaMA2.1-AV[8] 384 36.9+3.3 55.28+6.39 46.0+13.58 43.11−1.15 53.72+2.76 43.7+5.66\nChat-UniVi[19];Video-LLaVA[27];VideoChat2[25] 224 29.66 39.81 31.86 28.52 34.71 31.52\nChat-UniVi-v1.5[19];LongVA[61];Video-LLaMA2[8] 16 7B 336 32.75+3.09 49.07+9.26 32.21+0.35 44.92+16.4 49.04+14.33 37.67+6.15\nVideo-LLaMA2.1[8];Video-LLaMA2.1-AV[8] 384 37.06+4.31 57.22+8.15 45.68+13.47 43.44−1.48 54.13+5.09 43.96+6.29\nLongVA[61];Video-LLaMA2[8] 336 31.39 46.67 35.26 44.26 53.72 37.98\n32 7B\nVideo-LLaMA2.1[8];Video-LLaMA2.1-AV[8] 384 38.5+7.11 59.72+13.05 45.47+10.21 42.95−1.31 54.55+0.83 44.64+6.66\nTable4.OverallaccuracyofLongVA[61]andLongVILA[28]on\ndifferent#frmrangingfrom4to128.\nLLM #frm\nModel Res.\nsize 4 8 16 32 64 128\nLongVA[61] 336 7B 40.74 43.73 43.32 41.62 39.98 39.39\nLongVILA[28] Dynamic 8B 33.00 35.87 35.11 36.46 36.17 36.11\nlutionofthevisualencoder(Res.),thesizeofthelanguage\ndecoder(LLMsize),andthevolumeoftrainingdata(Data\nvolume)intheSFTstage. Weprovidefullanalysistables\nandfiguresofeachfactorinSupplementary.\nTheNumberofInputFrames. Acrossallthemodels,we\nconsistentlyobservethattheinputframesdon’tcontribute\nFigure 4. #frm analysis. We compare the overall accuracy of\nmodelsfromthesameserieswiththesameLLMsizeandRes..The totheperformance. AsshownintheFigure4andTable4,\nresultsindicateacounterintuitiveirrelevancebetweentheoverall theoverallaccuracyiseitherstableorfluctuatesrandomly.\naccuracyandinput#frm. Wecouldn’tseeanycleartrends,althoughintuitively,extra\nframeswouldprovidemoreinformationtohelpthemodel\nrely on some expert knowledge about video-making tech- makedecisions. Wesuspectthatwhilemoreframesprovide\nniques,whichMLLMscanacquirefrommassivecorpora. moreinformation,thissmallamountofusefulinformation\nConversely,themodelsencountersignificantdifficulties is mixed in with a large amount of duplicate information,\ninmorecomplexcompositionaltasks,especiallyCA.For andthemodelcannoteffectivelyextractit. Thisisagainst\nexample,CamM-PandSS-Pyieldonlymodestscores,with ourexpectationthatitwouldbringbenefitstocountingtasks\ntop models reaching 57.1 and 63.9, respectively, reflect- (Cha-C,S-C,Cut-C).\ningasignificantgapinunderstandingcinematictechniques.\nNU tasks, such as S-M and P-O, also present challenges, The Resolution of Visual Encoder. We observe that\nwithmodelperformancesubstantiallytrailingbehindhuman MLLMswithhigher-resolutionvisualencodersperformsig-\nbenchmarks because there is often a gap between scripts nificantlybetter. Whiletheresolutionisunchangeablefor\nandactualvideopresentation. Unlikehumans,whocanin- onespecificmodel,wecalculatethemeanperformanceofall\ntuitivelybridgethisgap,MLLMsarefine-tunedonclosely modelswiththesameLLMsizeandvideoframes.Asshown\nmatchedvision-textpairs,limitingtheirabilitytointerpret inTable3,asresolutionincreases,performanceonallfive\nsubtleorimpliedconnectionsinnarrativetasks.Additionally, maincategoriesincreasesconsistently. However,itisworth\ncountingtasks,suchasCha-C,S-C,andCut-C,remainpar- notingthatitisimpossibletodeterminehowmuchofthis\nticularlyproblematicformostmodels,furtherunderscoring improvementisduetothehigher-resolutionvisualencoder\nthelimitationsinvisualcountingabilitiesandunderstanding andhowmuchisduetothedifferentmodelsthemselves.\nscenetransitionsacrossmultipleframesacrossallmodels.\nTheSizeofLanguageDecoder.Toanalyzethisrelationship\nmoreaccurately,wecomparemodelswithdifferentdecoder\n4. Diagnostic Analysis of Factors Affecting\nsizeswhilekeepingtheencoderandtrainingdataconstant.\nMLLMs’Performance\nFromTable5,weobservethatmodelswithlargerdecoders\nIn this section, we analyze the factors that may affect the demonstratestrongerperformance,andthegainsaremainly\nMLLM’sunderstandingofvideocomposition. Wefocuson fromNU,whichrequiresthemodelnotonlytorecognize\nfourfactors: thenumberofinputframes(#frm),thereso- individual frames but also to establish logical and causal\n6\nTable5.LLMsizeanalysis.Wecomparemodelsfromthesameserieswiththesame#frmandRes.butdifferentLLMsizes.Theresults\nindicatethatlargerLLMsizesleadtoimprovedperformanceinmostcases,withhighlightedrelativegains.\nModel Res. #frm LLMsize CA CU NU SP MA Overall\n2B 25.08 47.22 37.05 47.54 44.63 36.17\nQwen2-VL[49] Dynamic 2fps 7B 34.35+9.27 56.67+9.45 61.05+24.0 57.38+9.84 48.76+4.13 49.3+13.13\n72B 50.48+16.13 60.0+3.33 71.16+10.11 60.0+2.62 46.28−2.48 58.68+9.38\n3B 26.84 43.89 19.16 37.38 47.11 29.84\n8\n8B 35.94+9.1 52.78+8.89 34.74+15.58 42.62+5.24 56.2+9.09 40.04+10.2\nVILA-1.5[28] 384\n3B 26.04 41.67 23.37 34.75 48.76 30.13\n16\n8B 35.78+9.74 51.11+9.44 36.42+13.05 39.34+4.59 60.33+11.57 39.98+9.85\n7B 25.88 47.78 28.84 45.9 50.41 34.35\nVideo-LLaMA2[8] 336 32\n72B 54.15+28.27 71.67+23.89 65.68+36.84 48.52+2.62 59.5+9.09 58.62+24.27\n0.5B 24.12 28.33 26.53 29.84 28.93 26.61\n1.8B 24.28+0.16 54.44+26.11 35.16+8.63 47.54+17.7 53.72+24.79 36.75+10.14\n3.8B 32.11+7.83 51.67−2.77 44.0+8.84 48.85+1.31 48.76−4.96 41.68+4.93\nInternVL2[7] 448 16 8B 57.03+24.92 62.78+11.11 53.68+9.68 45.57−3.28 56.2+7.44 54.63+12.95\n20B 40.1−16.93 63.89+1.11 51.16−2.52 38.36−7.21 54.55−1.65 46.42−8.21\n34B 54.95+14.85 69.44+5.55 65.47+14.31 56.07+17.71 70.25+15.7 60.73+14.31\n70B 51.92−3.03 72.78+3.34 64.84−0.63 52.79−3.28 63.64−6.61 58.73−2.0\nTable6.Datavolumeanalysis.Wecomparemodelswiththesame#frm,Res.,andLLMsizesbutusingdifferentDatavolumeintheSFT\nstage.TheresultsindicatethatlargerDatavolumesleadtoimprovedperformanceinmostcases,withhighlightedrelativegains.\nModel #frm Res. LLMsize Datavolume CA CU NU SP MA Overall\nChat-UniVi[19] 0.65M 25.56 34.44 23.37 25.9 36.36 26.73\n8 224 7B\nVideoChat2[25] 2M 39.46+13.9 57.78+23.34 44.84+21.47 58.03+32.13 50.41+14.05 47.01+20.28\nChat-UniVi-v1.5[19] 1.27M 37.38 47.78 26.53 37.38 46.28 36.11\n8 336 7B\nLongVA[61] 1.32M 37.54+0.16 51.67+3.89 41.89+15.36 49.51+12.13 56.2+9.92 43.73+7.62\nVILA-1.5[28] 8B 1.21M 35.94 52.78 34.74 42.62 56.2 40.04\nVideo-LLaMA2.1-AV[8] 8 384 7B 3.35M 35.46−0.48 52.78+0 37.05+3.31 39.34−3.28 58.68+2.48 40.09+0.05\nVideo-LLaMA2.1[8] 7B 3.35M 38.34+2.88 57.78+5.0 54.95+17.9 46.89+7.75 48.76−9.92 47.3+7.22\nChat-UniVi[19] 0.65M 24.92 32.22 23.37 24.92 38.84 26.26\n16 224 7B\nVideoChat2[25] 2M 39.3+14.38 60.0+27.78 44.84+21.47 31.8+6.88 26.45−12.39 40.8+14.54\nChat-UniVi-v1.5[19] 1.27M 34.5 48.89 25.89 40.98 42.98 35.4\n16 336 7B\nLongVA[61] 1.32M 37.86+3.36 51.11+2.22 41.89+16.0 47.87+6.89 53.72+10.74 43.32+7.92\nVILA-1.5[28] 8B 1.21M 35.78 51.11 36.42 39.34 60.33 39.98\nVideo-LLaMA2.1-AV[8] 16 384 7B 3.35M 35.78+0 56.67+5.56 36.42+0 40.0+0.66 59.5−0.83 40.62+0.64\nVideo-LLaMA2.1[8] 7B 3.35M 38.34+2.56 57.78+1.11 54.95+18.53 46.89+6.89 48.76−10.74 47.3+6.68\nKangaroo[30] 2.94M 31.79 51.67 29.05 53.44 33.06 37.1\n64 448 8B\nMiniCPM-V[55] 8.32M 38.18+6.39 60.0+8.33 40.84+11.79 38.36−15.08 55.37+22.31 42.5+5.4\nrelationships across sequences, a capability that benefits whilehumanseasilyusevisualcontexttodistinguishcam-\nfromamorepowerfullanguagedecoder. TasksinMAalso eramovementsandangleslike“panleft”and“zoomin”or\nbenefitfromtheexternalknowledgeacquiredbyLLMs. angleslike“eyelevel”and“lowangle,”modelslikeLLaVA-\nOneVision-72B [23] and GPT-4 [1] often struggle due to\nTheVolumeofTrainingData. Wecanobservetheperfor- scenetransitionsandsubtleperspectivechanges.\nmance influence brought by fine-tuning the MLLMs with\nmoredatafromTable6. Wecomparemodelswiththesame\n5.RelatedWork\nconfiguration, e.g. the same number of input frames, the\nsameresolutionofthevisionencoder,andthesameorsimi- MLLMsforVideoUnderstanding. EquippingLLMswith\nlarsizeofLLMadopted. Theresultsindicatethatlargerdata adapted video encoders has led to the creation of several\nvolumesleadtoimprovedperformanceofvideocomposition multimodal models tailored for video understanding [46].\nunderstandinginmostcases. Forinstance, GPT-4-turbo,GPT-4o,andGPT-4o-mini[1]\nareGPT-basedmodelswithintegratedvideocomprehension\nQualitativeAnalysis. Weperformanerroranalysistogain capabilities. InternVL2[7],withparametercountsranging\ndeeperinsightsintothemodels’shortcomingsinfine-grained from1Bto76B,isbasedontheInternLMframework[4]\nvideocompositionunderstanding. Inthisanalysis,themod- and supports video processing at multiple scales. Addi-\nelsarerequiredtoanswerquestionsandprovideexplanations tionally,modelsderivedfromtheLLaMAbackbone—such\ninadialogueformat. Figure5showstheexampleswhere asLLaVA-OneVision[23],VILA[28],VideoLLaMA[8],\ntop models fail to predict correct answers. For example, andLongLLaVA[52]—havebeenadaptedforvideoinput.\n7\nFigure5.Qualitativeanalysis.Greenrepresentscorrectanswers,whileredindicateswrongpredictionorexplanation.Morecasescanbe\nfoundinSupplementary.\nGemini has also been extended to include video process- measuresanMLLM’sabilitytogeneratetextdescriptions\ningcapabilities[43]. Othermodels, includingQwen[49], of visual content. VQA [2, 37, 39] assesses the model’s\nMiniCPM [55], Kangaroo [30], and Chat-UniVi [19], ex- proficiencyinansweringquestionsbasedonvisualinputs\nhibitstrongvideounderstandingabilities. Inourwork,we byintegratingvisualperceptionwithlanguageunderstand-\nthoroughlyevaluatethesemodels’capabilitiesinvideocom- ingandexternalknowledge. Visualreasoning[17,20,45]\npositionunderstandingandprovidedetailedanalysis. evaluatesamodel’sspatialawarenessandlogicalreasoning\ninprocessingvisualinformation. Moreover,thecomprehen-\nEvaluationBenchmarksforMLLMs. Numerousbench- siveabilitiesofMLLMsaregaugedusingadvancedbench-\nmarksforMLLMshaverecentlyemergedtoevaluatediverse marks[11,13,22,31,35,36,56,57,59].ForvideoMLLMs,\nmodelcapabilities,withimagecaptioning,VisualQuestion similareffortsleverageexistingbenchmarks[26,51]toeval-\nAnswering (VQA), and visual reasoning among the most uatevideounderstanding[24,41].However,anotablegapre-\nfrequentlyassessedtasks. Imagecaptioning[9,29,38,42] mainsinevaluatingmodelsonthevideo-compositionunder-\n8\nstanding. Thiscompositionunderstandingiscrucialforac- [6] ZheChen,WeiyunWang,HaoTian,ShenglongYe,Zhangwei\ncuratelyprocessingandcorrelatingmultipleelementswithin Gao, ErfeiCui, WenwenTong, KongzhiHu, JiapengLuo,\navisualscene[14,60]. Whileexistingbenchmarksassess ZhengMa,etal. Howfararewetogpt-4v?closingthegap\ncompositionallyinimages[16,48],fewcomprehensivelyad- tocommercialmultimodalmodelswithopen-sourcesuites.\narXivpreprintarXiv:2404.16821,2024. 5\ndressthespecificchallengesofcompositionalunderstanding\n[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, et al.\ninvideo,wheremanyMLLMsstillshowlimitations.\nInternvl: Scaling up vision foundation models and align-\ning for generic visual-linguistic tasks. In Proceedings of\n6.Conclusion\ntheIEEE/CVFConferenceonComputerVisionandPattern\nWeintroduceVIDCOMPOSITION,anovelandhigh-quality Recognition,pages24185–24198,2024. 3,7\nbenchmarkdesignedtoevaluateMLLMsinunderstanding [8] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin\nvideocompositions. Ourbenchmarkincorporatesvarious Li,GuanzhengChen,YongxinZhu,WenqiZhang,Ziyang\nvideotypesandQAcategories,coveringvariousaspectsof Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-\nvideo composition, e.g. camera movement, shot size, nar- temporalmodelingandaudiounderstandinginvideo-llms.\nrativestructure,andcharacteractions. ThroughVIDCOM- arXivpreprintarXiv:2406.07476,2024. 1,3,5,6,7\nPOSITION, we comprehensively assess MLLMs’ abilities [9] MingqianFeng, YunlongTang, ZeliangZhang, andChen-\nliangXu. Domoredetailsalwaysintroducemorehalluci-\ntounderstandcomplexvideocompositions. Theevaluation\nnations in lvlm-based image captioning? arXiv preprint\nrevealsasignificantperformancegapbetweenhumansand\narXiv:2406.12663,2024. 8\nmodels,sheddinglightonthelimitationsofcurrentMLLMs\n[10] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,Meng-\nandprovidingvaluableinsightsforfutureimprovements.\ndan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXingSun,etal. Mme: Acomprehensiveevaluationbench-\nAcknowledgement\nmarkformultimodallargelanguagemodels. arXivpreprint\narXiv:2306.13394,2023. 1,2,3\nThisworkwassupportedinpartbytheNationalEyeInsti-\n[11] ChaoyouFu,PeixianChen,etal. Mme: Acomprehensive\ntuteoftheNationalInstitutesofHealthunderawardnumber\nevaluationbenchmarkformultimodallargelanguagemodels.\nR01EY034562andtheDefenseAdvanceResearchProjects\nArXiv,abs/2306.13394,2023. 8\nAgencyundercontractnumberHR00112220003. Thecon-\n[12] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai\ntentissolelytheresponsibilityoftheauthorsanddoesnot Ren, RenruiZhang, ZihanWang, ChenyuZhou, Yunhang\nnecessarilyrepresenttheofficialviewsofthefundingagen- Shen, Mengdan Zhang, et al. Video-mme: The first-ever\ncies;noofficialendorsementshouldbeinferred. comprehensiveevaluationbenchmarkofmulti-modalllmsin\nvideoanalysis. arXivpreprintarXiv:2405.21075,2024. 1,2,\nReferences 3\n[13] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,Zongxia\n[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad, Li,XiaoyuLiu,XijunWang,LichangChen,FurongHuang,\nIlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,Janko Yaser Yacoob, et al. Hallusionbench: an advanced diag-\nAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4 nosticsuiteforentangledlanguagehallucinationandvisual\ntechnicalreport. arXivpreprintarXiv:2303.08774,2023. 1, illusioninlargevision-languagemodels. InProceedingsof\n3,5,7 theIEEE/CVFConferenceonComputerVisionandPattern\n[2] StanislawAntol,AishwaryaAgrawal,JiasenLu,Margaret Recognition,pages14375–14385,2024. 8\nMitchell,DhruvBatra,CLawrenceZitnick,andDeviParikh. [14] Cheng-YuHsieh,JieyuZhang,ZixianMa,AniruddhaKem-\nVqa:Visualquestionanswering. InProceedingsoftheIEEE bhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable\ninternational conference on computer vision, pages 2425– benchmarksforvision-languagecompositionality. Advances\n2433,2015. 8 inNeuralInformationProcessingSystems,36,2024. 9\n[3] JingBi,YunlongTang,LuchuanSong,AliVosoughi,Nguyen [15] Hang Hua, Yunlong Tang, Chenliang Xu, and Jiebo Luo.\nNguyen,andChenliangXu. Eagle: Egocentricaggregated V2xum-llm:Cross-modalvideosummarizationwithtemporal\nlanguage-videoengine. InProceedingsofthe32ndACMIn- promptinstructiontuning. arXivpreprintarXiv:2404.12353,\nternationalConferenceonMultimedia,page1682–1691,New 2024. 1\nYork,NY,USA,2024.AssociationforComputingMachinery. [16] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao,\n1 ZhengyuanYang, HangfengHe, ChenliangXu, andJiebo\n[4] ZhengCai,MaosongCao,HaojiongChen,KaiChen,Keyu Luo. Mmcomposition: Revisiting the compositionality\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei of pre-trained vision-language models. arXiv preprint\nChu, et al. Internlm2 technical report. arXiv preprint arXiv:2410.09733,2024. 2,3,9\narXiv:2403.17297,2024. 7 [17] Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan\n[5] XiuyuanChen,YuanLin,YuchenZhang,andWeiranHuang. Zhang,JohnCollomosse,ScottCohen,andJieboLuo. Fine-\nAutoeval-video:Anautomaticbenchmarkforassessinglarge match:Aspect-basedfine-grainedimageandtextmismatch\nvisionlanguagemodelsinopen-endedvideoquestionanswer- detectionandcorrection. InEuropeanConferenceonCom-\ning. arXivpreprintarXiv:2311.14906,2023. 2,3 puterVision,pages474–491.Springer,2025. 8\n9\n[18] YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,and Hu. Kangaroo:Apowerfulvideo-languagemodelsupporting\nGunhee Kim. Tgif-qa: Towardspatio-temporal reasoning long-contextvideoinput. arXivpreprintarXiv:2408.15542,\ninvisualquestionanswering. InProceedingsoftheIEEE 2024. 5,7,8\nconferenceoncomputervisionandpatternrecognition,pages [31] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang\n2758–2766,2017. 3 Zhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe,\n[19] PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao, ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan\nandLiYuan. Chat-univi: Unifiedvisualrepresentationem- all-aroundplayer? arXivpreprintarXiv:2307.06281,2023.\npowers large language models with image and video un- 2,8\nderstanding. InProceedingsoftheIEEE/CVFConference\n[32] YuanxinLiu,ShichengLi,YiLiu,YuxiangWang,Shuhuai\nonComputerVisionandPatternRecognition,pages13700–\nRen,LeiLi,SishuoChen,XuSun,andLuHou. Tempcom-\n13710,2024. 5,6,7,8\npass:Dovideollmsreallyunderstandvideos? arXivpreprint\n[20] JustinJohnson,BharathHariharan,LaurensVanDerMaaten,\narXiv:2403.00476,2024. 2,3\nLiFei-Fei,CLawrenceZitnick,andRossGirshick. Clevr:A\n[33] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang\ndiagnosticdatasetforcompositionallanguageandelementary\nZhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe,\nvisualreasoning. InProceedingsoftheIEEEconferenceon\nZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan\ncomputervisionandpatternrecognition,2017. 8\nall-aroundplayer? InEuropeanConferenceonComputer\n[21] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.\nVision,pages216–233.Springer,2025. 1,3\nTvqa:Localized,compositionalvideoquestionanswering. In\n[34] ZuyanLiu,YuhaoDong,ZiweiLiu,WinstonHu,JiwenLu,\nEMNLP,2018. 2,3\nandYongmingRao.Oryxmllm:On-demandspatial-temporal\n[22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\nunderstandingatarbitraryresolution,2024. 5\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodalllmswithgenerativecomprehension. arXivpreprint [35] Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha,\narXiv:2307.16125,2023. 8 Manoj Acharya, Kushal Kafle, and Christopher Kanan.\n[23] BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi, Revisiting multi-modal llm evaluation. arXiv preprint\nHaoZhang,KaichenZhang,PeiyuanZhang,YanweiLi,Zi- arXiv:2408.05334,2024. 8\nweiLiu,andChunyuanLi. Llava-onevision:Easyvisualtask [36] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,\ntransfer,2024. 3,5,7 HannanehHajishirzi,HaoCheng,Kai-WeiChang,Michel\n[24] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,Yi Galley,andJianfengGao. Mathvista:Evaluatingmathemati-\nLiu,ZunWang,JilanXu,GuoChen,PingLuo,LiminWang, calreasoningoffoundationmodelsinvisualcontexts. arXiv\nandYuQiao. Mvbench:Acomprehensivemulti-modalvideo preprintarXiv:2310.02255,2023. 8\nunderstandingbenchmark,2024. 8 [37] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\n[25] KunchangLi, YaliWang, YinanHe, YizhuoLi, YiWang, RoozbehMottaghi. Ok-vqa: Avisualquestionanswering\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. benchmark requiring external knowledge. In Proceedings\nMvbench:Acomprehensivemulti-modalvideounderstand- oftheIEEE/cvfconferenceoncomputervisionandpattern\ningbenchmark. InProceedingsoftheIEEE/CVFConference recognition,pages3195–3204,2019. 8\nonComputerVisionandPatternRecognition,pages22195–\n[38] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqR.Joty,\n22206,2024. 1,2,3,5,6,7\nand Enamul Hoque. Chartqa: A benchmark for question\n[26] LinjieLi, JieLei, ZheGan, LichengYu, Yen-ChunChen,\nanswering about charts with visual and logical reasoning.\nRohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,\nArXiv,abs/2203.10244,2022. 8\nWilliam Yang Wang, Tamara Lee Berg, Mohit Bansal,\n[39] MineshMathew,DimosthenisKaratzas,R.Manmatha,and\nJingjingLiu,LijuanWang,andZichengLiu. Value:Amulti-\nC. V. Jawahar. Docvqa: A dataset for vqa on document\ntaskbenchmarkforvideo-and-languageunderstandingevalu-\nimages. 2021IEEEWinterConferenceonApplicationsof\nation,2021. 8\nComputerVision(WACV),pages2199–2208,2020. 8\n[27] BinLin, YangYe, BinZhu, JiaxiCui, MunanNing, Peng\n[40] MunanNing,BinZhu,YujiaXie,BinLin,JiaxiCui,LuYuan,\nJin,andLiYuan. Video-llava: Learningunitedvisualrep-\nDongdongChen,andLiYuan. Video-bench:Acomprehen-\nresentationbyalignmentbeforeprojection. arXivpreprint\nsivebenchmarkandtoolkitforevaluatingvideo-basedlarge\narXiv:2311.10122,2023. 1,5,6\nlanguagemodels. arXivpreprintarXiv:2311.16103,2023. 2,\n[28] JiLin,HongxuYin,WeiPing,PavloMolchanov,Mohammad\n3\nShoeybi, and Song Han. Vila: On pre-training for visual\nlanguagemodels. InProceedingsoftheIEEE/CVFConfer- [41] MunanNing,BinZhu,YujiaXie,BinLin,JiaxiCui,LuYuan,\nence on Computer Vision and Pattern Recognition, pages DongdongChen,andLiYuan. Video-bench:Acomprehen-\n26689–26699,2024. 3,5,6,7 sivebenchmarkandtoolkitforevaluatingvideo-basedlarge\n[29] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, languagemodels,2023. 8\nPietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence [42] YasumasaOnoe,SunayanaRane,ZacharyBerger,Yonatan\nZitnick. Microsoft coco: Common objects in context. In Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana\nECCV.Springer,2024. 8 Parekh,JordiPont-Tuset,GarrettTanzer,SuWang,andJason\n[30] JiajunLiu,YibingWang,HanghangMa,XiaopingWu,Xi- Baldridge. DOCCI:DescriptionsofConnectedandContrast-\naoqiMa, XiaomingWei, JianbinJiao, EnhuaWu, andJie ingImages. InarXiv:2404.19753,2024. 8\n10\n[43] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry [56] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nLepikhin,TimothyLillicrap,Alayrac,etal. Gemini1.5:Un- KevinLin,ZichengLiu,XinchaoWang,andLijuanWang.\nlockingmultimodalunderstandingacrossmillionsoftokens Mm-vet:Evaluatinglargemultimodalmodelsforintegrated\nofcontext. arXivpreprintarXiv:2403.05530,2024. 1,3,5,8 capabilities.InInternationalconferenceonmachinelearning.\n[44] EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang, PMLR,2024. 8\nHaoyangZhou, FeiyangWu, HaozheChi, XunGuo, Tian [57] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and\nYe,YantingZhang,etal. Moviechat: Fromdensetokento Jiebo Luo. Promptfix: You prompt and we fix the photo.\nsparsememoryforlongvideounderstanding. InProceedings arXivpreprintarXiv:2405.16785,2024. 8\noftheIEEE/CVFConferenceonComputerVisionandPattern [58] ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,Yueting\nRecognition,pages18221–18232,2024. 2,3 Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for\n[45] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A understandingcomplexwebvideosviaquestionanswering.In\ncorpusofnaturallanguageforvisualreasoning. InAnnual ProceedingsoftheAAAIConferenceonArtificialIntelligence,\nMeeting of the Association for Computational Linguistics, pages9127–9134,2019. 2,3\n2017. 8 [59] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,etal.\n[46] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Mmmu:Amassivemulti-disciplinemultimodalunderstand-\nLiang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, ingandreasoningbenchmarkforexpertagi. arXivpreprint\nRongyiZhu,etal. Videounderstandingwithlargelanguage arXiv:2311.16502,2023. 8\nmodels:Asurvey. arXivpreprintarXiv:2312.17432,2023. 7 [60] MertYuksekgonul,FedericoBianchi,PratyushaKalluri,Dan\n[47] YunlongTang,DaikiShimada,JingBi,andChenliangXu. Jurafsky,andJamesZou. Whenandwhyvision-language\nAvicuna: Audio-visual llm with interleaver and context- models behave like bags-of-words, and what to do about\nboundaryalignmentfortemporalreferentialdialogue. arXiv it? InTheEleventhInternationalConferenceonLearning\npreprintarXiv:2403.16276,2024. 1 Representations,2022. 9\n[48] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\n[61] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,\nSingh, Adina Williams, Douwe Kiela, and Candace Ross.\nJingkangYang,YuanhanZhang,ZiyueWang,HaoranTan,\nWinoground:Probingvisionandlanguagemodelsforvisio-\nChunyuan Li, and Ziwei Liu. Long context transfer from\nlinguisticcompositionality. InProceedingsoftheIEEE/CVF languagetovision. arXivpreprintarXiv:2406.16852,2024.\nConference on Computer Vision and Pattern Recognition,\n5,6,7\npages5238–5248,2022. 2,3,9\n[49] PengWang,ShuaiBai,SinanTan,ShijieWang,ZhihaoFan,\nJinzeBai,KeqinChen,XuejingLiu,JialinWang,Wenbin\nGe, et al. Qwen2-vl: Enhancing vision-language model’s\nperception of the world at any resolution. arXiv preprint\narXiv:2409.12191,2024. 3,5,7,8\n[50] WeihanWang,ZehaiHe,WenyiHong,YeanCheng,Xiaohan\nZhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming\nDing,etal. Lvbench:Anextremelongvideounderstanding\nbenchmark. arXivpreprintarXiv:2406.08035,2024. 2,3\n[51] XinWang,JiaweiWu,JunkunChen,LeiLi,Yuan-FangWang,\nandWilliamYangWang. Vatex:Alarge-scale,high-quality\nmultilingualdatasetforvideo-and-languageresearch,2020.8\n[52] XidongWang, DingjieSong, ShunianChen, ChenZhang,\nandBenyouWang. Longllava:Scalingmulti-modalllmsto\n1000imagesefficientlyviahybridarchitecture.arXivpreprint\narXiv:2409.02889,2024. 5,7\n[53] JunbinXiao,XindiShang,AngelaYao,andTat-SengChua.\nNext-qa:Nextphaseofquestion-answeringtoexplainingtem-\nporalactions. InProceedingsoftheIEEE/CVFConference\nonComputerVisionandPatternRecognition(CVPR),pages\n9777–9786,2021. 2,3\n[54] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,\nXiangnanHe,andYuetingZhuang.Videoquestionanswering\nviagraduallyrefinedattentionoverappearanceandmotion.\nInProceedingsofthe25thACMinternationalconferenceon\nMultimedia,pages1645–1653,2017. 2,3\n[55] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,\nHongjiZhu,TianchiCai,HaoyuLi,WeilinZhao,ZhihuiHe,\netal. Minicpm-v:Agpt-4vlevelmllmonyourphone. arXiv\npreprintarXiv:2408.01800,2024. 5,7,8\n11",
    "pdf_filename": "VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos.pdf"
}