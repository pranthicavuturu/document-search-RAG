{
    "title": "Improving Multi-task Learning via Seeking Task-based Flat Regions",
    "abstract": "Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory. 1 Introduction Over the last few years, deep learning has emerged as a powerful tool for functional approximation by exhibiting superior performance and even exceeding human ability on a wide range of applications. In spite of the appealing performance, training massive independent neural networks to handle individual tasks requires not only expensive computational and storage resources but also long runtime. Therefore, multi-task learning is a more preferable approach in many situations [1, 2, 3] as they can: (i) avoid redundant features calculation for each task through their inherently shared architecture; and (ii) reduce the number of total trainable parameters by hard parameter sharing [4, 5] or soft parameter sharing [6, 7]. However, existing state-of-the-art methods following the veins of gradient-based multi-task learning [8, 9, 10, 11, 12, 13] tend to neglect geometrical properties of the loss landscape yet solely focus on minimizing the empirical error in the optimization process, which can be easily prone to the overfitting problem [14, 15]. ∗Equal contributions. 1 arXiv:2211.13723v3  [cs.LG]  19 Nov 2024",
    "body": "Improving Multi-task Learning\nvia Seeking Task-based Flat Regions\nHoang Phan∗1\nLam Tran∗2\nQuyen Tran2\nNgoc N. Tran3\nTuan Truong2\nNhat Ho4\nDinh Phung5\nTrung Le5\nNew York University1,\nVinAI Research2,\nVanderbilt University3,\nUniversity of Texas, Austin4,\nMonash University, Australia 5\nNovember 20, 2024\nAbstract\nMulti-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep\nneural networks that allows learning more than one objective by a single backbone. Compared to\ntraining tasks separately, MTL significantly reduces computational costs, improves data efficiency,\nand potentially enhances model performance by leveraging knowledge across tasks. Hence, it\nhas been adopted in a variety of applications, ranging from computer vision to natural language\nprocessing and speech recognition. Among them, there is an emerging line of work in MTL that\nfocuses on manipulating the task gradient to derive an ultimate gradient descent direction to\nbenefit all tasks. Despite achieving impressive results on many benchmarks, directly applying\nthese approaches without using appropriate regularization techniques might lead to suboptimal\nsolutions to real-world problems. In particular, standard training that minimizes the empirical\nloss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by\nnoisy-labeled ones, which can cause negative transfer between tasks and overall performance drop.\nTo alleviate such problems, we propose to leverage a recently introduced training method, named\nSharpness-aware Minimization, which can enhance model generalization ability on single-task\nlearning. Accordingly, we present a novel MTL training methodology, encouraging the model to\nfind task-based flat minima for coherently improving its generalization capability on all tasks.\nFinally, we conduct comprehensive experiments on a variety of applications to demonstrate the\nmerit of our proposed approach to existing gradient-based MTL methods, as suggested by our\ndeveloped theory.\n1\nIntroduction\nOver the last few years, deep learning has emerged as a powerful tool for functional approximation\nby exhibiting superior performance and even exceeding human ability on a wide range of applications.\nIn spite of the appealing performance, training massive independent neural networks to handle\nindividual tasks requires not only expensive computational and storage resources but also long\nruntime. Therefore, multi-task learning is a more preferable approach in many situations [1, 2, 3]\nas they can: (i) avoid redundant features calculation for each task through their inherently shared\narchitecture; and (ii) reduce the number of total trainable parameters by hard parameter sharing [4, 5]\nor soft parameter sharing [6, 7]. However, existing state-of-the-art methods following the veins of\ngradient-based multi-task learning [8, 9, 10, 11, 12, 13] tend to neglect geometrical properties of the\nloss landscape yet solely focus on minimizing the empirical error in the optimization process, which\ncan be easily prone to the overfitting problem [14, 15].\n∗Equal contributions.\n1\narXiv:2211.13723v3  [cs.LG]  19 Nov 2024\n\nMeanwhile, the overfitting problem of modern neural networks is often attributed to high-\ndimensional and non-convex loss functions, which result in complex loss landscapes containing\nmultiple local optima. Hence, understanding the loss surface plays a crucial role in training robust\nmodels, and developing flat minimizers remains one of the most effective approaches [16, 14, 17, 18].\nTo be more specific, recent studies [19, 20] show that the obtained loss landscape from directly\nminimizing the empirical risk can consist of many sharp minimums, thus yielding poor generalization\ncapacity when being exposed to unseen data. Moreover, this issue is apparently exacerbated in\noptimizing multiple objectives simultaneously, as in the context of multi-task learning. Certainly,\nsharp minima of each constituent objective might appear at different locations, which potentially\nresults in large generalization errors on the associated task. To this end, finding a common flat\nand low-loss valued region for all tasks is desirable for improving the current methods of multi-task\nlearning.\nContribution. To further address the above desideratum, we propose a novel MTL training\nmethod, incorporating the recently introduced optimization sharpness-aware minimization (SAM) [21]\ninto existing gradient manipulation strategies in multi-task learning to further boost their performance.\nGuiding by the generalization error in Theorem 1, the proposed approach not only orients the model\nto the joint low empirical loss value across tasks but also encourages the model to reach the task-based\nflat regions. Importantly, our approach is model-agnostic and compatible with current gradient-based\nMTL approaches (see Figure 1 for the overview of our approach). By using our proposed framework,\nthe gradient conflict across tasks is mitigated significantly, which is the goal of recent gradient-based\nMTL studies in alleviating negative transfer between tasks. Finally, we conduct comprehensive\nexperiments on a variety of applications to demonstrate the merit of our approach for improving not\nonly task performance but also model robustness and calibration. Last but not least, to the best of\nour knowledge, ours is the first work to improve multi-task learning by investigating the geometrical\nproperties of the model loss landscape.\n2\nRelated work\n2.1\nMulti-task learning\nIn multi-task learning (MTL), we often aim to jointly train one model to tackle multiple different\nbut correlated tasks. It has been proven in prior work [22, 2, 23, 24] that it is not only able to\nenhance the overall performance but also reduce the memory footprint and fasten the inference\nprocess. Previous studies on MTL often employ a hard parameter-sharing mechanism along with\nlight-weight task-specific modules to handle multiple tasks.\nPareto multi-task learning. Originated from Multiple-gradient descent algorithm (MGDA),\na popular line of gradient-based MTL methods aim to find Pareto stationary solutions, from\nwhich we can not further improve model performance on any particular task without diminishing\nanother [8]. Moreover, recent studies suggest exploring the whole Pareto front by learning diverse\nsolutions [25, 26, 27, 28], or profiling the entire Pareto front with hyper-network [29, 30]. While\nthese methods are theoretically grounded and guaranteed to converge to Pareto-stationary points,\nthe experimental results are often limited and lack comparisons under practical settings.\nLoss and gradient balancing. Another branch of preliminary work in MTL capitalizes\non the idea of dynamically reweighting loss functions based on gradient magnitudes [31], task\nhomoscedastic uncertainty [32], or difficulty prioritization [33] to balance the gradients across tasks.\nMore recently, PCGrad [9] developed a gradient manipulation procedure to avoid conflicts among\n2\n\nInput\nTask 1\nTask 2\nShared\nencoder\nFinal gradient\nTask flat gradient\nCombined flat gradient\nTask loss gradient\n Combined loss gradient\nLow loss regions\nFigure 1: We demonstrate our framework in a 2-task problem. For the shared part, task-based flat\ngradients (red dashed arrows) steer the model to escape sharp areas, while task-based loss gradients\n(orange dashed arrows) lead the model into their corresponding low-loss regions. In our method, we\naggregate them to find the combined flat gradient gflat\nsh\nand combined loss gradient gloss\nsh , respectively.\nFinally, we add those two output gradients to target the joint low-loss and flat regions across the\ntasks. Conversely, updating task-specific non-shared parts is straightforward and much easier since\nthere is the involvement of one objective only.\ntasks by projecting random task gradients on the normal plane of the other. Similarly, [10] proposes a\nprovably convergent method to minimize the average loss, and [11] calculates loss scaling coefficients\nsuch that the combined gradient has equal-length projections onto individual task gradients.\n2.2\nFlat minima\nFlat minimizer has been found to improve generalization ability of neural networks because it enables\nmodels to find wider local minima, by which they will be more robust against shifts between train\nand test losses [34, 35, 36]. This relationship between generalization ability and the width of minima\nis theoretically and empirically studied in many studies [37, 38, 39, 40], and subsequently, a variety\nof methods seeking flat minima have been proposed [41, 42, 43, 44]. For example, [43, 45, 46] analyze\nthe impacts of different training factors, such as batch-size, learning rate, covariance of gradient,\ndropout, on the flatness of found minima. Additionally, several schemes pursue wide local minima by\nadding regularization terms to the loss function [41, 47, 48, 42], e.g., softmax output’s low entropy\npenalty, [41], distillation losses [47, 48].\nRecently, SAM [21], which seeks flat regions by explicitly minimizing the worst-case loss around\nthe current model, has received significant attention due to its effectiveness and scalability compared\nto previous methods. Particularly, it has been exploited in a variety of tasks and domains [49, 50, 51,\n52, 53, 54, 55]. A notable example is the improvement that SAM brings to meta-learning bi-level\noptimization in [50]. Another application of SAM is in federated learning (FL) [51] in which the\nauthors achieved tighter convergence rates than existing FL works, and proposed a generalization\nbound for the global model. In addition, SAM shows its generalization ability in vision models [54],\nlanguage models [53] and domain generalization [49]. However, existing studies have only focused\n3\n\non single task problems. In this work, we leverage SAM’s principle to develop theory and devise\npractical methods, allowing seeking flat minima in gradient-based multi-task learning models.\n3\nPreliminaries\nConventional training methods that focus on minimizing the empirical loss can be easily prone\nto overfitting problems (i.e., the validation error no longer decreases, but the training loss keeps\ndeclining), thus, restricting model generalization performance. In an attempt to alleviate such\nphenomenons, [21] proposed to minimize the worst-case loss in a neighborhood of the current model\nparameter given by:\nmin\nθ\nmax\n||ϵ||2≤ρL (θ + ϵ) ,\n(1)\nwhere || · ||2 denotes the l2 norm and ρ represents the radius of the neighborhood. We assume L is\ndifferentiable up to the first order with respect to θ. The optimization problem (1) is referred to as\nsharpness aware minimization (SAM).\nTo solve problem (1), [21] proposed to first find the solution for the inner maximization by\napproximating L(θ + ϵ) via a first-order Taylor expansion w.r.t ϵ around 0, which is as follows:\nϵ∗= arg max\n||ϵ||2≤ρ\nL(θ + ϵ) ≈arg max\n||ϵ||2≤ρ\nϵ⊤∇θL(θ) ≈ρ\n∇θL(θ)\n||∇θL(θ)||2\n.\nPutting into words, the worst-case perturbation is approximated as the scaled gradient of the loss\nw.r.t the current parameter θ. Then, the gradient w.r.t this perturbed model is computed to update\nθ:\ngSAM := ∇θ max\n||ϵ||2≤ρL (θ + ϵ) ≈∇θL(θ + ϵ)|θ+ϵ∗\n(2)\n4\nOur Proposed Framework\nThis section describes our proposed framework to improve existing methods on gradient-based MTL.\nWe first recall the goal of multi-task learning, then establish the upper bounds for the general loss\nof each task. Subsequently, we rely on these upper bounds to devise the proposed framework for\nimproving the model generalization ability by guiding it to a flatter region of each task.\n4.1\nMulti-task learning setting\nIn multi-task learning, we are given a data-label distribution D from which we can sample a training\nset S = {(xi, y1\ni , ..., ym\ni )n\ni=1}, where xi is a data example and y1\ni , ..., ym\ni\nare the labels of the tasks\n1, 2, ..., m respectively.\nThe model for each task θi = [θsh, θi\nns] consists of the shared part θsh and the individual\nnon-shared part θi\nns. We denote the general loss for task i as Li\nD(θi), while its empirical loss over\nthe training set S as Li\nS(θi). Existing works in MTL, typically MGDA [8], PCGrad [9], CAGrad [10],\nand IMTL [11], aim to find a model that simultaneously minimizes the empirical losses for all tasks:\nmin\nθsh,θ1:m\nns\n\u0002\nL1\nS\n\u0000θ1\u0001\n, ..., Lm\nS (θm)\n\u0003\n,\n(3)\n4\n\nby calculating gradient gi for i-th task (i ∈[m]). The current model parameter is then updated by the\nunified gradient g = gradient_aggregate(g1, g2, . . . , gm), with the generic operation gradient_aggregate\nis to combine multiple task gradients, as proposed in gradient-based MTL studies.\nAdditionally, prior works only focus on minimizing the empirical losses and do not concern the\ngeneral losses which combat overfitting. Inspired by SAM [21], it is desirable to develop sharpness-\naware MTL approaches wherein the task models simultaneously seek low loss and flat regions.\nHowever, this is challenging since we have multiple objective functions in (3) and each task model\nconsists of a shared and an individual non-shared parts. To address the above challenge, in Theorem\n1, we develop upper bounds for the task general losses in the context of MTL which signifies the\nconcepts of sharpness for the shared part and non-shared parts and then rely on these new concepts\nto devise a novel MTL framework via seeking the task-based flat regions.\n4.2\nTheoretical development\nWe first state our main theorem that bounds the generalization performance of individual tasks by\nthe empirical error on the training set:\nTheorem 1. (Informally stated) For any perturbation radius ρsh, ρns > 0, under some mild\nassumptions, with probability 1 −δ (over the choice of training set S ∼D) we obtain\n\u0002\nLi\nD\n\u0000θi\u0001\u0003m\ni=1\n≤\nmax\n∥ϵsh∥2≤ρsh\n\u0014\nmax\n∥ϵins∥2≤ρns\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\n+\nfi \u0000∥θi∥2\n2\n\u0001\u0015m\ni=1\n, (4)\nwhere fi : R+ →R+, i ∈[m] are strictly increasing functions.\nTheorem 1 establishes the connection between the generalization error of each task with its\nempirical training error via worst-case perturbation on the parameter space. The formally stated\ntheorem and proof are provided in the appendix. Here we note that the worst-case shared perturbation\nϵsh is commonly learned for all tasks, while the worst-case non-shared perturbation ϵi\nns is tailored\nfor each task i. Theorem 1 directly hints us an initial and direct approach.\nAdditionally, [21] invokes the [56] PAC-Bayesian generalization bound , hence is only applicable\nto the 0-1 loss in the binary classification setting. In terms of the theory contribution, we employ\na more general PAC-Bayesian generalization bound [57] to tackle more general losses in MTL.\nMoreover, our theory development requires us to handle multiple objectives, each of which consists\nof the non-shared and shared parts, which is certainly non-trivial.\n4.3\nInitial and direct approach\nA straight-forward approach guided by Theorem 1 is to find the non-shared perturbations ϵi\nns, i ∈[m]\nindependently for the non-shared parts and a common shared perturbation for the shared part.\nDriven by this theoretical guidance, we propose the following updates.\nUpdate the non-shared parts. Based on the upper bounds in Theorem 1, because the non-\nshared perturbations ϵi\nns, i ∈[m] are independent to each task, for task i, we update its non-shared\n5\n\npart θi\nns:\nϵi\nns = ρns\n∇θi\nnsLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥∇θi\nnsLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥2\n,\ngi,SAM\nns\n= ∇θi\nnsLi\nS\n\u0000θsh, θi\nns + ϵi\nns\n\u0001\n,\nθi\nns = θi\nns −ηgi,SAM\nns\n,\nwhere η > 0 is the learning rate.\n(5)\nUpdate the shared part. Updating the shared part θsh is more challenging because its worst-cased\nperturbation ϵsh is shared among the tasks. To derive how to update θsh w.r.t. all tasks, we first\ndiscuss the case when we update this w.r.t. task i without caring about other tasks. Specifically,\nthis task’s SAM shared gradient is computed as:\nϵi\nsh = ρsh\n∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥2\n,\ngi,SAM\nsh\n= ∇θshLi\nS\n\u0000θsh + ϵi\nsh, θi\nns\n\u0001\n,\nthen we have a straight-forward updating strategy:\ngSAM\nsh\n= gradient_aggregate(g1,SAM\nsh\n, . . . , gm,SAM\nsh\n),\nθsh = θsh −ηgSAM\nsh\n.\nAccording to our analysis in Section 4.4, each gi,SAM\nsh\n= gi,loss\nsh\n+gi,flat\nsh\nis constituted by two components:\n(i) gi,loss\nsh\nto navigate to the task low-loss region and (ii) gi,flat\nsh\nto navigate to the task-based flat\nregion. However, a direct gradient aggregation of gi,SAM\nsh\n, i ∈[m] can be negatively affected by the\ngradient cancelation or conflict because it aims to combine many individual elements with different\nobjectives. In this paper, we go beyond this initial approach by deriving an updating formula to\ndecompose SAM gradient into two components, each serving its own purpose, and then combining\ntheir corresponding task gradients simultaneously. We also compare our method against the naive\napproach in Section 5.3.\n4.4\nOur proposed approach\nThe non-shared parts are updated normally as in Equation (5). It is more crucial to investigate how\nto update the shared part more efficiently. To better understand the SAM’s gradients, we analyze\ntheir characteristics by deriving them as follows:\ngi,SAM\nsh\n= ∇θshLi\nS\n\u0000θsh + ϵi\nsh, θi\nns\n\u0001 (1)\n≈∇θsh\n\u0002\nLi\nS\n\u0000θsh, θi\nns\n\u0001\u0003\n+\n\nϵi\nsh, ∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\u000b\n= ∇θsh\n\u0014\nLi\nS\n\u0000θsh, θi\nns\n\u0001\n+ ρsh\n\n∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥2\n, ∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\u000b\u0015\n= ∇θsh\n\u0002\nLi\nS\n\u0000θsh, θi\nns\n\u0001\n+ ρsh∥∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥2\n\u0003\n(6)\nwhere in\n(1)\n≈, we apply the first-order Taylor expansion and ⟨·, ·⟩represents the dot product.\nIt is obvious that following the negative direction of gi,SAM\nsh\nwill minimize the loss Li\nS\n\u0000θsh, θi\nns\n\u0001\nand the gradient norm ∥∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\n∥2 of task i, hence leading the model to the low-valued\nregion for the loss of this task and its flatter region with a lower gradient norm magnitude.\n6\n\nMoreover, inspired from the derivation in Equation (6), we decompose the gradient gi,SAM\nsh\n=\ngi,loss\nsh\n+ gi,flat\nsh\nwhere we define gi,loss\nsh\n:= ∇θshLi\nS\n\u0000θsh, θi\nns\n\u0001\nand gi,flat\nsh\n:= gi,SAM\nsh\n−gi,loss\nsh\n.\nAs\naforementioned, the purpose of the negative gradient −gi,loss\nsh\nis to orient the model to minimize the\nloss of the task i, while −gi,flat\nsh\nnavigates the model to the task i’s flatter region.\nTherefore, the SAM gradients gi,SAM\nsh\n, i ∈[m] constitute two components with different purposes.\nTo mitigate the possible confliction and interference of the two components when aggregating, we\npropose to aggregate the low-loss components solely and then the flat components solely. Specifically,\nto find a common direction that leads the joint low-valued losses for all tasks and the joint flatter\nregion for them, we first combine the gradients gi,loss\nsh\n, i ∈[m] and the gradients gi,flat\nsh\n, i ∈[m], then\nadd the two aggregated gradients, and finally update the shared part as:\ngloss\nsh = gradient_aggregate(g1,loss\nsh\n, . . . , gm,loss\nsh\n),\ngflat\nsh = gradient_aggregate(g1,flat\nsh\n, . . . , gm,flat\nsh\n),\ngSAM\nsh\n= gloss\nsh + gflat\nsh ; θsh = θsh −ηgSAM\nsh\n,\nFinally, the key steps of our proposed framework are summarized in Algorithm 1 and the overall\nschema of our proposed method is demonstrated in Figure 1.\nAlgorithm 1 Sharpness minimization for multi-task learning\nInput: Model parameter θ = [θsh, θ1:m\nns ], perturbation radius ρ = [ρsh, ρns], step size η and a list of\nm differentiable loss functions\n\b\nLi\tm\ni=1.\nOutput: Updated parameter θ∗\n1: for task i ∈[m] do\n2:\nCompute gradient gi,loss\nsh\n, gi\nns ←∇θLi(θ)\n3:\nWorst-case perturbation direction\nϵi\nsh = ρsh · gi,loss\nsh\n/\n\r\r\rgi,loss\nsh\n\r\r\r and ϵi\nns = ρns · gi\nns/\n\r\rgi\nns\n\r\r\n4:\nApproximate SAM’s gradient\ngi,SAM\nsh\n= ∇θshLi(θsh + ϵi\nsh, θi\nns) and gi,SAM\nns\n= ∇θi\nnsLi(θsh, θi\nns + ϵi\nns)\n5:\nCompute flat gradient\ngi,flat\nsh\n= gi,SAM\nsh\n−gi,loss\nsh\n6: end for\n7: Calculate combined update gradients:\ngloss\nsh = gradient_aggregate(g1,loss\nsh\n, g2,loss\nsh\n, . . . , gm,loss\nsh\n)\ngflat\nsh = gradient_aggregate(g1,flat\nsh\n, g2,flat\nsh\n, . . . , gm,flat\nsh\n)\n8: Calculate shared gradient update gSAM\nsh\n= gloss\nsh + gflat\nsh\n9: Update model parameter\nθ∗= [θsh, θ1:m\nns ] −η[gSAM\nsh\n, g1:m,SAM\nns\n]\n7\n\n5\nExperiments\nIn this section, we compare our proposed method against other state-of-the-art methods of multi-task\nlearning in different scenarios, ranging from image classification to scene understanding problems.\nRefer to the appendix for the detailed settings used for each dataset and additional experiments.\nDatasets and Baselines. Our proposed method is evaluated on four MTL benchmarks including\nMulti-MNIST [25], CelebA [58] for visual classification, and NYUv2 [59], CityScapes [60] for scene\nunderstanding. We show how our framework can boost the performance of gradient-based MTL\nmethods by comparing vanilla MGDA [8], PCGrad [9], CAGrad [10] and IMTL [11] to their flat-based\nversions F-MGDA, F-PCGrad, F-CAGrad and F-IMTL. We also add single task learning (STL)\nbaseline for each dataset.\n5.1\nImage classification\nMulti-MNIST. Following the protocol of [8], we set up three Multi-MNIST experiments with the\nResNet18 [61] backbone, namely: MultiFashion, MultiMNIST and MultiFashion+MNIST. In each\ndataset, two images are sampled uniformly from the MNIST [62] or Fashion-MNIST [63], then one\nis placed on the top left and the other is on the bottom right. We thus obtain a two-task learning\nthat requires predicting the categories of the digits or fashion items on the top left (task 1) and on\nthe bottom right (task 2) respectively.\nTable 1: Evaluation of different methods on three Multi-MNIST datasets. Rows with flat-based\nminimizers are shaded. Bold numbers denote higher accuracy between flat-based methods and their\nbaselines. ∗denotes the highest accuracy (except for STL, since it unfairly exploits multiple neural\nnetworks). We also use arrows to indicate that the higher is the better (↑) or vice-versa (↓).\nMultiFashion\nMultiMNIST\nMultiFashion+MNIST\nMethod\nTask 1 ↑\nTask 2 ↑\nAverage ↑\nTask 1 ↑\nTask 2 ↑\nAverage ↑\nTask 1 ↑\nTask 2\nAverage ↑\nSTL\n87.10 ± 0.09\n86.20 ± 0.06\n86.65 ± 0.02\n95.33 ± 0.08\n94.16 ± 0.04\n94.74 ± 0.06\n98.40 ± 0.02\n89.42 ± 0.03\n93.91 ± 0.02\nMGDA\n86.76 ± 0.09\n85.78 ± 0.36\n86.27 ± 0.22\n95.62 ± 0.02\n94.49 ± 0.10\n95.05 ± 0.06\n97.24 ± 0.04\n88.19 ± 0.13\n92.72 ± 0.07\nF-MGDA\n88.12 ± 0.11\n87.35 ± 0.11\n87.73 ± 0.09\n96.37 ± 0.06\n94.99 ± 0.06\n95.68 ± 0.00\n97.30 ± 0.09\n89.26 ± 0.14\n93.28 ± 0.03\nPCGrad\n86.93 ± 0.17\n86.20 ± 0.14\n86.57 ± 0.12\n95.71 ± 0.03\n94.41 ± 0.02\n95.06 ± 0.02\n97.12 ± 0.16\n88.45 ± 0.08\n92.78 ± 0.11\nF-PCGrad\n88.17 ± 0.14\n87.35 ± 0.27\n87.76 ± 0.07\n96.49 ± 0.05\n95.34 ± 0.10\n95.92 ± 0.07\n97.65 ± 0.06\n89.35 ± 0.07∗\n93.50 ± 0.01\nCAGrad\n86.99 ± 0.17\n86.04 ± 0.15\n86.51 ± 0.16\n95.62 ± 0.05\n94.39 ± 0.04\n95.01 ± 0.04\n97.19 ± 0.06\n88.18 ± 0.14\n92.68 ±0.04\nF-CAGrad\n88.19 ± 0.19∗\n87.45 ± 0.13\n87.82 ± 0.10∗\n96.54 ± 0.02\n95.36 ± 0.04∗\n95.95 ± 0.01∗\n97.82 ± 0.05∗\n89.26 ± 0.22\n93.54 ± 0.13∗\nIMTL\n87.35 ± 0.22\n86.45 ± 0.09\n86.90 ± 0.15\n95.93 ± 0.09\n94.63 ± 0.13\n95.28 ± 0.02\n97.47 ± 0.06\n88.46 ± 0.11\n92.97 ± 0.03\nF-IMTL\n88.1 ± 0.10\n87.5 ± 0.04∗\n87.80 ± 0.06\n96.55 ± 0.07∗\n95.16 ± 0.05\n95.85 ± 0.05\n97.59 ± 0.12\n88.99 ± 0.08\n93.29 ± 0.02\nAs summarized in Table 1, we can see that seeking flatter regions for all tasks can improve the\nperformance of all the baselines across all three datasets. Especially, flat-based methods achieve the\nhighest score for each task and for the average, outperforming STL by 1.2% on MultiFashion and\nMultiMNIST. We conjecture that the discrepancy between gradient update trajectories to classify\ndigits from MNIST and fashion items from FashionMNIST has resulted in the fruitless performance\nof baselines, compared to STL on MultiFashion+MNIST. Even if there exists dissimilarity between\ntasks, our best obtained average accuracy when applying our method to CAGrad is just slightly\nlower than STL (< 0.4%) while employing a single model only.\nInterestingly, our proposed MTL training method also helps improve model calibration performance\nby mitigating the over-confident phenomenon of deep neural networks. As can be seen from Figure\n8\n\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nOur\nERM\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nOur\nERM\nFigure 2: Entropy distributions of ResNet18 on in-domain set (left) and out-of-domain set (right).\n2, our method produces high-entropy predictions that represent its uncertainty, ERM-based method\noutputs high confident predictions on both in and out-of-domain data. More details about model\ncalibration improvement can be found in the appendix.\nTable 2: Mean of error per\ncategory of MTL algorithms\nin multi-label classification\non CelebA dataset.\nMethod\nAverage error ↓\nSTL\n8.77\nLS\n9.99\nUW\n9.66\nMGDA\n9.96\nF-MGDA\n9.22\nPCGrad\n8.69\nF-PCGrad\n8.23\nCAGrad\n8.52\nF-CAGrad\n8.22∗\nIMTL\n8.88\nF-IMTL\n8.24\nCelebA. CelebA [64] is a face dataset, which consists of 200K\ncelebrity facial photos with 40 attributes. Similar to [8], each attribute\nforms a binary classification problem, thus a 40-class multi-label\nclassification problem is constructed.\nTable 2 shows the average errors over 40 tasks of the methods\nwith linear scalarization (LS) and Uncertainty weighting (UW) [32]\nbeing included to serve as comparative baselines. The best results\nin each pair and among all are highlighted using bold font and ∗,\nrespectively. When the number of tasks is large, flat region seeking\nstill consistently shows its advantages and the lowest average accuracy\nerror is achieved by F-CAGrad. Interestingly, when the optimizer is\naware of flat minima, the gaps between PCGrad, IMTL and CAGrad,\n(8.23, 8.24 vs 8.22), are smaller than those using conventional ERM\ntraining, (8.69, 8.88 and 8.52). This might be due to the better\naggregation of tasks’ gradients, which means that the conflict between\nthese gradients is likely to be reduced when the shared parameters\napproach the common flat region of all tasks.\n5.2\nScene Understanding\nTwo datasets used in this sub-section are NYUv2 [59] and CityScapes [60]. NYUv2 is an indoor\nscene dataset that contains 3 tasks: 13-class semantic segmentation, depth estimation, and surface\nnormal prediction. In CityScapes, there are 19 classes of street-view images, which are coarsened\ninto 7 categories to create two tasks: semantic segmentation and depth estimation. For these two\nexperiments, we additionally include several recent MTL methods, namely, scale-invariant (SI),\nrandom loss weighting (RLW), Dynamic Weight Average (DWA) [2], GradDrop [65], and Nash-MTL\n[13] whose results are taken from [13]. Details of each baseline can be found in the appendix. Also\nfollowing the standard protocol used in [2, 10, 13], Multi-Task Attention Network [2] is employed on\ntop of the SegNet architecture [66], our presented results are averaged over the last 10 epochs to\nalign with previous work.\nEvaluation metric. In this experiment, we have to deal with different task types rather than\none only as in the case of image classification. Since each of them has its own set of metrics. We thus\n9\n\nmark the overall performance of comparative methods by reporting their relative task improvement\n[67] throughout this section. Let Mi and Si be the metrics obtained by the main and the\nTable 3: Test performance for two-task CityScapes: semantic segmentation and depth estimation.∗\ndenotes the best score for each task’s metrics.\nSegmentation\nDepth\nMethod\nmIoU ↑Pix Acc ↑Abs Err ↓Rel Err↓∆m% ↓\nSTL\n74.01\n93.16\n0.0125\n27.77\nLS\n75.18\n93.49\n0.0155\n46.77\n22.60\nSI\n70.95\n91.73\n0.0161\n33.83\n14.11\nRLW\n74.57\n93.41\n0.0158\n47.79\n24.38\nDWA\n75.24\n93.52\n0.0160\n44.37\n21.45\nUW\n72.02\n92.85\n0.0140\n30.13∗\n5.89\nGradDrop\n75.27\n93.53\n0.0157\n47.54\n23.73\nNash-MTL\n75.41\n93.66\n0.0129\n35.02\n6.82\nMGDA\n68.84\n91.54\n0.0309\n33.50\n44.14\nF-MGDA\n73.77\n93.12\n0.0129\n27.44∗\n0.67∗\nPCGrad\n75.13\n93.48\n0.0154\n42.07\n18.29\nF-PCGrad\n75.77\n93.67\n0.0144\n39.60\n13.65\nCAGrad\n75.16\n93.48\n0.0141\n37.60\n11.64\nF-CAGrad\n76.02\n93.72\n0.0134\n34.64\n7.25\nIMTL\n75.33\n93.49\n0.0135\n38.41\n11.10\nF-IMTL\n76.63∗\n93.76∗\n0.0124∗\n31.17\n1.87\nsingle-task learning (STL) model, respectively, the relative task improvement on i-th task is\nmathematically given by: ∆i := 100 · (−1)li (Mi −Si)/Si, where li = 1 if a lower value for the i-th\ncriterion is better and 0 otherwise. We depict our results by the average relative task improvement\n∆m% = 1\nm\nPm\ni=1 ∆i.\nCityScapes. In Table 3, the positive effect of seeking flat regions is consistently observed in all\nmetrics and baselines. In particular, the relative improvements of MGDA and IMTL are significantly\nboosted, achieving the highest and second-best ∆m% scores, respectively. The segmentation scores\nof PCGrad, CAGrad and IMTL even surpass STL. Intriguingly, MGDA biases to the depth estimation\nobjective, leading to the predominant performance on that task, similar patterns appear in [11] and\nthe below NYUv2 experiment.\nNYUv2. Table 4 shows each task’s results and the relative improvements over STL of different\nmethods. Generally, the flat-based versions obtain comparable or higher results on most of the\nmetrics, except for MGDA at the segmentation task, in which F-MGDA notably decreases the mIoU\nscore. However, it does significantly help other tasks, which contributes to the overall MGDA’s\nrelative improvement, from 1.38% being worse than STL to 0.33% being higher. Remarkably,\nF-CAGrad and F-IMTL outperform their competitors by large margins across all tasks, resulting in\nthe top two relative improvements, 3.78% and 4.77% .\n5.3\nAblation study\nSo far, our proposed technique has shown state-of-the-art performances under different settings, we\nnow investigate in more detailed how it affects conventional training by inspecting loss surfaces and\nmodel robustness. Similar patterns are observed in other experiments and given in the appendix.\nTask conflict. To empirically confirm that tasks’ gradients are less conflicted when the model\nis driven to the flat regions, we measure the gradient conflict and present the result in Figure 3.\n10\n\nTable 4: Test performance for three-task NYUv2 of Segnet [66]: semantic segmentation, depth\nestimation, and surface normal. Using the proposed procedure in conjunction with gradient-based\nmulti-task learning methods consistently advances their overall performance.\nSegmentation\nDepth\nSurface Normal\nmIoU ↑Pix Acc ↑Abs Err ↓Rel Err ↓Angle Distance ↓\nWithin t◦↑\n∆m% ↓\nMean\nMedian\n11.25\n22.5\n30\nSTL\n38.30\n63.76\n0.6754\n0.2780\n25.01\n19.21\n30.14\n57.20\n69.15\n0.00\nLS\n39.29\n65.33\n0.5493\n0.2263\n28.15\n23.96\n22.09\n47.50\n61.08\n5.59\nSI\n38.45\n64.27\n0.5354\n0.2201\n27.60\n23.37\n22.53\n48.57\n62.32\n4.39\nRLW\n37.17\n63.77\n0.5759\n0.2410\n28.27\n24.18\n22.26\n47.05\n60.62\n7.78\nDWA\n39.11\n65.31\n0.5510\n0.2285\n27.61\n23.18\n24.17\n50.18\n62.39\n3.57\nUW\n36.87\n63.17\n0.5446\n0.2260\n27.04\n22.61\n23.54\n49.05\n63.65\n4.05\nGradDrop\n39.39\n65.12\n0.5455\n0.2279\n27.48\n22.96\n23.38\n49.44\n62.87\n3.58\nNash-MTL\n40.13\n65.93\n0.5261∗\n0.2171\n25.26\n20.08\n28.4\n55.47\n68.15\n−4.04\nMGDA\n30.47\n59.90\n0.6070\n0.2555\n24.88\n19.45\n29.18\n56.88\n69.36\n1.38\nF-MGDA\n26.42\n58.78\n0.6078\n0.2353\n24.34∗\n18.45∗\n31.64∗58.86∗70.50∗\n−0.33\nPCGrad\n38.06\n64.64\n0.5550\n0.2325\n27.41\n22.80\n23.86\n49.83\n63.14\n3.97\nF-PCGrad\n40.05\n65.42\n0.5429\n0.2243\n27.38\n23.00\n23.47\n49.35\n62.74\n3.14\nCAGrad\n39.79\n65.49\n0.5486\n0.2250\n26.31\n21.58\n25.61\n52.36\n65.58\n0.20\nF-CAGrad 40.93∗\n66.68∗\n0.5285\n0.2162\n25.43\n20.39\n27.99\n54.82\n67.56\n−3.78\nIMTL\n39.35\n65.60\n0.5426\n0.2256\n26.02\n21.19\n26.2\n53.13\n66.24\n−0.76\nF-IMTL\n40.42\n65.61\n0.5389\n0.2121∗\n25.03\n19.75\n28.90\n56.19\n68.72 −4.77∗\nWhile the percentage of gradient conflict of ERM increases to more than 50%, ours decreases and\napproaches 0%. This reduction in gradient conflict is also the goal of recent gradient-based MTL\nmethods in mitigating negative transfer between tasks [9, 68, 3].\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n10\n20\n30\n40\n50\n60\nGradient conflict (%)\nOurs\nERM\nFigure 3:\nProportion of conflict between per-task gradients (g1,loss · g2,loss\n<\n0) on\nMultiFashion+MNIST dataset.\nModel robustness against noise. To verify that SAM can orient the model to the common\nflat and low-loss region of all tasks, we measure the model performance within a r-radius Euclidean\nball. To be more specific, we perturb parameters of two converged models by ϵ, which lies in a\nr-radius ball and plot the accuracy of the perturbed models of each task as we increase r from 0 to\n11\n\n1000. At each value of r, 10 different models around the r−radius ball of the converged model are\nsampled.\n0\n200\n400\n600\n800\nr\n20\n40\n60\n80\n100\nAccuracy\nOurs\nERM\nFigure 4: Accuracy within r−radius ball. Solid/dashed lines denote performance on train/test sets,\nrespectively.\nIn Figure 4, the accuracy of the model trained using our method remains at a high level when\nnoise keeps increasing until r = 800. This also gives evidence that our model found a region that\nchanges slowly in loss. By contrast, the naively trained model loses its predictive capabilities as\nsoon as the noise appears and becomes a dummy classifier that attains 10% accuracy in a 10-way\nclassification.\nAggregation strategies comparison. Table 5 provides a comparison between the direct\naggregation on {gi,SAM\nsh\n}m\ni=1 and individual aggregation on {gi,flat\nsh\n}m\ni=1 and {gi,loss\nsh\n}m\ni=1 (our method).\nTable 5: Two aggregation strategies on CityScapes.\nSegmentation\nDepth\nMethod\nmIoU ↑\nPix Acc ↑\nAbs Err ↓\nRel Err↓\n∆m% ↓\nERM\n68.84\n91.54\n0.0309\n33.50\n44.14\nOurs (direct)\n68.93\n91.41\n0.0130\n31.37\n6.43\nOurs (individual)\n73.77\n93.12\n0.0129\n27.44∗\n0.67\nCompared to the naive approach, in which per-task SAM gradients are directly aggregated, our\ndecomposition approach consistently improves performance by a large margin across all tasks. This\nresult reinforces the rationale behind separately aggregating low-loss directions and flat directions.\nVisualization of the loss landscapes. Following [69], we plot the loss surfaces at convergence\nafter training Resnet18 from scratch on the MultiMNIST dataset. Test loss surfaces of checkpoints\nthat have the highest validation accuracy scores are shown in Figure 5.\nWe can clearly see that the solution found by our proposed method not only mitigates the test\nloss sharpness for both tasks but also can intentionally reduce the test loss value itself, in comparison\nto traditional ERM. This is a common behavior when using flat minimizers as the gap between train\nand test performance has been narrowed [44, 14].\n12\n\nFigure 5: Visualization of test loss surfaces with standard ERM training and when applying our\nmethod. The coordinate plane axes are two random sampled orthogonal Gaussian perturbations.\n6\nConclusion\nIn this work, we have presented a general framework that can be incorporated into current multi-task\nlearning methods following the gradient balancing mechanism. The core ideas of our proposed\nmethod are the employment of flat minimizers in the context of MTL and proving that they can help\nenhance previous works both theoretically and empirically. Concretely, our method goes beyond\noptimizing per-task objectives solely to yield models that have both low errors and high generalization\ncapabilities. On the experimental side, the efficacy of our method is demonstrated on a wide range\nof commonly used MTL benchmarks, in which ours consistently outperforms comparative methods.\nReferences\n[1] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in\nEuropean conference on computer vision, pp. 94–108, Springer, 2014.\n[2] S. Liu, E. Johns, and A. J. Davison, “End-to-end multi-task learning with attention,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pp. 1871–1880, 2019.\n[3] Z. Wang, Y. Tsvetkov, O. Firat, and Y. Cao, “Gradient vaccine: Investigating and improving multi-task\noptimization in massively multilingual models,” in International Conference on Learning Representations,\n2020.\n[4] I. Kokkinos, “Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level\nvision using diverse datasets and limited memory,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 6129–6138, 2017.\n[5] F. Heuer, S. Mantowsky, S. Bukhari, and G. Schneider, “Multitask-centernet (mcn): Efficient and diverse\nmultitask learning using an anchor free approach,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 997–1005, 2021.\n[6] Y. Gao, J. Ma, M. Zhao, W. Liu, and A. L. Yuille, “Nddr-cnn: Layerwise feature fusing in multi-task\ncnns by neural discriminative dimensionality reduction,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 3205–3214, 2019.\n[7] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard, “Latent multi-task architecture learning,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 4822–4829, 2019.\n[8] O. Sener and V. Koltun, “Multi-task learning as multi-objective optimization,” Advances in neural\ninformation processing systems, vol. 31, 2018.\n13\n\n[9] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-task\nlearning,” Advances in Neural Information Processing Systems, vol. 33, pp. 5824–5836, 2020.\n[10] B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient descent for multi-task learning,”\nAdvances in Neural Information Processing Systems, vol. 34, pp. 18878–18890, 2021.\n[11] L. Liu, Y. Li, Z. Kuang, J.-H. Xue, Y. Chen, W. Yang, Q. Liao, and W. Zhang, “Towards impartial\nmulti-task learning,” in International Conference on Learning Representations, 2020.\n[12] A. Javaloy and I. Valera, “Rotograd: Gradient homogenization in multitask learning,” in International\nConference on Learning Representations, 2021.\n[13] A. Navon, A. Shamsian, I. Achituve, H. Maron, K. Kawaguchi, G. Chechik, and E. Fetaya, “Multi-task\nlearning as a bargaining game,” arXiv preprint arXiv:2202.01017, 2022.\n[14] J. Kaddour, L. Liu, R. Silva, and M. J. Kusner, “A fair comparison of two popular flat minima optimizers:\nStochastic weight averaging vs. sharpness-aware minimization,” arXiv preprint arXiv:2202.00661, vol. 1,\n2022.\n[15] Y. Zhao, H. Zhang, and X. Hu, “Penalizing gradient norm for efficiently improving generalization in\ndeep learning,” arXiv preprint arXiv:2202.03599, 2022.\n[16] N. S. Keskar, J. Nocedal, P. T. P. Tang, D. Mudigere, and M. Smelyanskiy, “On large-batch training\nfor deep learning: Generalization gap and sharp minima,” in 5th International Conference on Learning\nRepresentations, ICLR 2017, 2017.\n[17] Z. Li, Z. Wang, and J. Li, “Analyzing sharpness along gd trajectory: Progressive sharpening and edge of\nstability,” arXiv preprint arXiv:2207.12678, 2022.\n[18] K. Lyu, Z. Li, and S. Arora, “Understanding the generalization benefit of normalization layers: Sharpness\nreduction,” arXiv preprint arXiv:2206.07085, 2022.\n[19] H. He, G. Huang, and Y. Yuan, “Asymmetric valleys: Beyond sharp and flat local minima,” Advances in\nneural information processing systems, vol. 32, 2019.\n[20] Y. Zheng, R. Zhang, and Y. Mao, “Regularizing neural networks via adversarial model perturbation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8156–8165,\n2021.\n[21] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware minimization for efficiently\nimproving generalization,” in International Conference on Learning Representations, 2021.\n[22] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, pp. 41–75, 1997.\n[23] X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks for natural language understanding,”\nin Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4487–\n4496, Association for Computational Linguistics, 2019.\n[24] S. Ruder, “An overview of multi-task learning in deep neural networks,” arXiv preprint arXiv:1706.05098,\n2017.\n[25] X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong, “Pareto multi-task learning,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[26] X. Liu, X. Tong, and Q. Liu, “Profiling pareto front with multi-objective stein variational gradient\ndescent,” Advances in Neural Information Processing Systems, vol. 34, pp. 14721–14733, 2021.\n[27] D. Mahapatra and V. Rajan, “Multi-task learning with user preferences: Gradient descent with controlled\nascent in pareto optimization,” in International Conference on Machine Learning, pp. 6597–6607, PMLR,\n2020.\n14\n\n[28] D. Mahapatra and V. Rajan, “Exact pareto optimal search for multi-task learning: Touring the pareto\nfront,” arXiv preprint arXiv:2108.00597, 2021.\n[29] X. Lin, Z. Yang, Q. Zhang, and S. Kwong, “Controllable pareto multi-task learning,” arXiv preprint\narXiv:2010.06313, 2020.\n[30] A. Navon, A. Shamsian, G. Chechik, and E. Fetaya, “Learning the pareto front with hypernetworks,” in\nInternational Conference on Learning Representations, 2021.\n[31] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Gradnorm: Gradient normalization for\nadaptive loss balancing in deep multitask networks,” in International conference on machine learning,\npp. 794–803, PMLR, 2018.\n[32] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for scene\ngeometry and semantics,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7482–7491, 2018.\n[33] M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-Fei, “Dynamic task prioritization for multitask\nlearning,” in Proceedings of the European conference on computer vision (ECCV), pp. 270–287, 2018.\n[34] Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio, “Fantastic generalization measures and\nwhere to find them,” in ICLR, OpenReview.net, 2020.\n[35] H. Petzka, M. Kamp, L. Adilova, C. Sminchisescu, and M. Boley, “Relative flatness and generalization,”\nin NeurIPS, pp. 18420–18432, 2021.\n[36] G. K. Dziugaite and D. M. Roy, “Computing nonvacuous generalization bounds for deep (stochastic)\nneural networks with many more parameters than training data,” in UAI, AUAI Press, 2017.\n[37] S. Hochreiter and J. Schmidhuber, “Simplifying neural nets by discovering flat minima,” in NIPS,\npp. 529–536, MIT Press, 1994.\n[38] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro, “Exploring generalization in deep learning,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[39] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio, “Sharp minima can generalize for deep nets,” in\nInternational Conference on Machine Learning, pp. 1019–1028, PMLR, 2017.\n[40] S. Fort and S. Ganguli, “Emergent properties of the local geometry of neural loss landscapes,” arXiv\npreprint arXiv:1910.05929, 2019.\n[41] G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. E. Hinton, “Regularizing neural networks by\npenalizing confident output distributions,” in ICLR (Workshop), OpenReview.net, 2017.\n[42] P. Chaudhari, A. Choromańska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. T. Chayes, L. Sagun, and\nR. Zecchina, “Entropy-sgd: biasing gradient descent into wide valleys,” Journal of Statistical Mechanics:\nTheory and Experiment, vol. 2019, 2017.\n[43] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On large-batch training for\ndeep learning: Generalization gap and sharp minima,” in ICLR, OpenReview.net, 2017.\n[44] P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson, “Averaging weights leads to\nwider optima and better generalization,” in UAI, pp. 876–885, AUAI Press, 2018.\n[45] S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. J. Storkey, “Three factors\ninfluencing minima in sgd,” ArXiv, vol. abs/1711.04623, 2017.\n[46] C. Wei, S. Kakade, and T. Ma, “The implicit and explicit regularization effects of dropout,” in International\nconference on machine learning, pp. 10181–10192, PMLR, 2020.\n[47] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual learning,” 2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 4320–4328, 2018.\n15\n\n[48] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma, “Be your own teacher: Improve the performance\nof convolutional neural networks via self distillation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 3713–3722, 2019.\n[49] J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y. Lee, and S. Park, “Swad: Domain generalization by\nseeking flat minima,” Advances in Neural Information Processing Systems, vol. 34, pp. 22405–22418,\n2021.\n[50] M. Abbas, Q. Xiao, L. Chen, P.-Y. Chen, and T. Chen, “Sharp-maml: Sharpness-aware model-agnostic\nmeta learning,” arXiv preprint arXiv:2206.03996, 2022.\n[51] Z. Qu, X. Li, R. Duan, Y. Liu, B. Tang, and Z. Lu, “Generalized federated learning via sharpness aware\nminimization,” arXiv preprint arXiv:2206.02618, 2022.\n[52] D. Caldarola, B. Caputo, and M. Ciccone, “Improving generalization in federated learning by seeking\nflat minima,” in European Conference on Computer Vision, pp. 654–672, Springer, 2022.\n[53] D. Bahri, H. Mobahi, and Y. Tay, “Sharpness-aware minimization improves language model generalization,”\nin Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), (Dublin, Ireland), pp. 7360–7371, Association for Computational Linguistics, May 2022.\n[54] X. Chen, C.-J. Hsieh, and B. Gong, “When vision transformers outperform resnets without pre-training\nor strong data augmentations,” arXiv preprint arXiv:2106.01548, 2021.\n[55] V.-A. Nguyen, T.-L. Vuong, H. Phan, T.-T. Do, D. Phung, and T. Le, “Flat seeking bayesian neural\nnetworks,” Advances in Neural Information Processing Systems, 2023.\n[56] D. A. McAllester, “Pac-bayesian model averaging,” in Proceedings of the twelfth annual conference on\nComputational learning theory, pp. 164–170, 1999.\n[57] P. Alquier, J. Ridgway, and N. Chopin, “On the properties of variational approximations of gibbs\nposteriors,” Journal of Machine Learning Research, vol. 17, no. 236, pp. 1–41, 2016.\n[58] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of the\nIEEE international conference on computer vision, pp. 3730–3738, 2015.\n[59] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation and support inference from rgbd\nimages,” in European conference on computer vision, pp. 746–760, Springer, 2012.\n[60] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3213–3223, 2016.\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n[62] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document\nrecognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[63] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine\nlearning algorithms,” arXiv preprint arXiv:1708.07747, 2017.\n[64] Z. Liu, P. Luo, X. Wang, and X. Tang, “Large-scale celebfaces attributes (celeba) dataset,” Retrieved\nAugust, vol. 15, no. 2018, p. 11, 2018.\n[65] Z. Chen, J. Ngiam, Y. Huang, T. Luong, H. Kretzschmar, Y. Chai, and D. Anguelov, “Just pick a\nsign: Optimizing deep multitask models with gradient sign dropout,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 2039–2050, 2020.\n[66] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-decoder\narchitecture for image segmentation,” IEEE transactions on pattern analysis and machine intelligence,\nvol. 39, no. 12, pp. 2481–2495, 2017.\n16\n\n[67] K.-K. Maninis, I. Radosavovic, and I. Kokkinos, “Attentive single-tasking of multiple tasks,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1851–1860, 2019.\n[68] S. Zhu, H. Zhao, P. Wang, H. Deng, J. Xu, and B. Zheng, “Gradient deconfliction via orthogonal\nprojections onto subspaces for multi-task learning,” 2022.\n[69] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the loss landscape of neural nets,”\nAdvances in neural information processing systems, vol. 31, 2018.\n[70] B. Laurent and P. Massart, “Adaptive estimation of a quadratic functional by model selection,” Annals\nof Statistics, pp. 1302–1338, 2000.\n[71] J.-A. Désidéri, “Multiple-gradient descent algorithm (mgda) for multiobjective optimization,” Comptes\nRendus Mathematique, vol. 350, no. 5-6, pp. 313–318, 2012.\n[72] B. Lin, F. Ye, Y. Zhang, and I. W. Tsang, “Reasonable effectiveness of random weighting: A litmus test\nfor multi-task learning,” arXiv preprint arXiv:2111.10603, 2021.\n[73] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980,\n2014.\n[74] J. Kwon, J. Kim, H. Park, and I. K. Choi, “Asam: Adaptive sharpness-aware minimization for scale-\ninvariant learning of deep neural networks,” 18–24 Jul 2021.\n[75] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in\nInternational conference on machine learning, pp. 1321–1330, PMLR, 2017.\n[76] G. W. Brier et al., “Verification of forecasts expressed in terms of probability,” Monthly weather review,\nvol. 78, no. 1, pp. 1–3, 1950.\n[77] M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtaining well calibrated probabilities using bayesian\nbinning,” in Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n[78] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and\nJ. Snoek, “Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset\nshift,” Advances in neural information processing systems, vol. 32, 2019.\n[79] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive uncertainty estimation\nusing deep ensembles,” Advances in neural information processing systems, vol. 30, 2017.\n[80] A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,” Advances in neural\ninformation processing systems, vol. 31, 2018.\n17\n\nSupplement to “Improving Multi-task Learning\nvia Seeking Task-based Flat Regions\"\nDue to space constraints, some details were omitted from the main paper. We therefore include\nadditional theoretical developments (section A) and experimental results (section C) in this appendix.\nA\nOur Theory Development\nThis section contains the proofs and derivations of our theory development to support the main\nsubmission.\nWe first start with the following theorem, which is inspired by the general PAC-Bayes in [57].\nTheorem 2. With the assumption that adding Gaussian perturbation will raise the test error:\nLD(θ) ≤Eϵ∼N(0,σ2I) [LD(θ + ϵ)]. Let T be the number of parameter θ, and N be the cardinality of\nS, then the following inequality is true with the probability 1 −δ:\nLD (θ) ≤Eϵ∼N(0,σ2I) [LS(θ + ϵ)] +\n1\n√\nN\n\"\n1\n2 + T\n2 log\n\u0010\n1 + ||θ||2\nTσ2\n\u0011\n+ log 1\nδ + 6 log(N + T) + L2\n8\n#\nwhere L is the upper-bound of the loss function.\nProof. We use the PAC-Bayes theory for P = N(0, σ2\nP IT ) and Q = N(θ, σ2IT ) are the prior and\nposterior distributions, respectively.\nBy using the bound in [57], with probability at least 1 −δ and for all β > 0, we have:\nEθ∼Q [LD(θ)] ≤Eθ∼Q [LS(θ)] + 1\nβ\nh\nKL(Q∥P) + log 1\nδ + Ψ(β, N)\ni\n,\nwhere we have defined:\nΨ(β, N) = log EP ES\nh\nexp\nn\nβ\n\u0000LD(θ) −LS(θ)\n\u0001oi\nNote that the loss function is bounded by L, according to Hoeffding’s lemma, we have:\nΨ(β, N) ≤β2L2\n8N .\nBy Cauchy inequality:\n1\n√\nN\n\"\nT\n2 log\n\u0010\n1 + ||θ||2\nTσ2\n\u0011\n+ L2\n8\n#\n≥\nL\n2\n√\nN\nr\nT log\n\u0010\n1 + ||θ||2\nTσ2\n\u0011\n≥L,\nwhich means that the theorem is proved since the loss function is upper bounded by L, following\nassumptions.\nNow, we only need to prove the theorem under the case: ||θ||2 ≤Tσ2h\nexp 4N\nT −1\ni\n.\nWe need to specify P in advance since it is a prior distribution. However, we do not know in\nadvance the value of θ that affects the KL divergence term. Hence, we build a family of distribution\nP as follows:\nP =\nn\nPj = N(0, σ2\nPjIT ) : σ2\nPj = c exp\n\u00001 −j\nT\n\u0001\n, c = σ2\u00001 + exp 4N\nT\n\u0001\n, j = 1, 2, . . .\no\n.\n18\n\nSet δj =\n6δ\nπ2j2 , the below inequality holds with probability at least 1 −δj:\nEθ∼Q [LD(θ)] ≤Eθ∼Q [LS(θ)] + 1\nβ\nh\nKL(Q∥Pj) + log 1\nδj\n+ β2L2\n8N\ni\n.\nOr it can be written as:\nEϵ∼N(0,σ2I) [LD(θ + ϵ)] ≤Eϵ∼N(0,σ2I) [LS(θ + ϵ)] + 1\nβ\nh\nKL(Q∥Pj) + log 1\nδj\n+ β2L2\n8N\ni\n.\nThus, with probability 1 −δ the above inequalities hold for all Pj. We choose:\nj∗=\n$\n1 + T log\n \nσ2\u00001 + exp{4N/T}\n\u0001\nσ2 + ∥θ∥2/T\n!%\n.\nSince ∥θ∥2\nT\n≤σ2\u0002\nexp 4N\nT −1\n\u0003\n, we get σ2 + ∥θ∥2\nT\n≤σ2 exp 4N\nT , thus j∗is well-defined. We also have:\nT log\nc\nσ2 + ∥θ∥2/T\n≤j∗\n≤1 + T log\nc\nσ2 + ∥θ∥2/T\n⇒\nlog\nc\nσ2 + ∥θ∥2/T\n≤j∗\nT\n≤1\nT + log\nc\nσ2 + ∥θ∥2/T\n⇒\n−1\nT + log σ2 + ∥θ∥2/T\nc\n≤−j∗\nT\n≤log σ2 + ∥θ∥2/T\nc\n⇒\ne−1/T σ2 + ∥θ∥2/T\nc\n≤e−j∗/T ≤σ2 + ∥θ∥2/T\nc\n⇒\nσ2 + ∥θ∥2\nT\n≤ce\n1−j∗\nT\n≤e\n1\nT\n\u0010\nσ2 + ∥θ∥2\nT\n\u0011\n⇒\nσ2 + ∥θ∥2\nT\n≤σ2\nPj∗\n≤e\n1\nT\n\u0010\nσ2 + ∥θ∥2\nT\n\u0011\n.\nHence, we have:\nKL(Q∥Pj∗) = 1\n2\nhTσ2 + ∥θ∥2\nσ2\nPj∗\n−T + T log\nσ2\nPj∗\nσ2\ni\n≤1\n2\nh Tσ2 + ∥θ∥2\nσ2 + ∥θ∥2/T −T + T log e1/T \u0000σ2 + ∥θ∥2/T\n\u0001\nσ2\ni\n≤1\n2\nh\n1 + T log\n\u00001 + ∥θ∥2\nTσ2\n\u0001i\n.\n19\n\nFor the term log\n1\nδj∗, use the inequality log(1 + et) ≤1 + t for t > 0:\nlog 1\nδj∗= log (j∗)2π2\n6δ\n= log 1\nδ + log\n\u0010π2\n6\n\u0011\n+ 2 log(j∗)\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T log σ2\u00001 + exp(4N/T)\n\u0001\nσ2 + ∥θ∥2/T\n\u0011\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T log\n\u00001 + exp(4N/T)\n\u0001\u0011\n≤log 1\nδ + log π2\n6 + 2 log\n\u0010\n1 + T\n\u00001 + 4N\nT\n\u0001\u0011\n≤log 1\nδ + log π2\n6 + log(1 + T + 4N).\nChoosing β =\n√\nN, with probability at least 1 −δ we get:\n1\nβ\nh\nKL(Q∥Pj∗) + log 1\nδj∗+ β2L2\n8N\ni\n≤\n1\n√\nN\nh1\n2 + T\n2 log\n\u0010\n1 + ∥θ∥2\nTσ2\n\u0011\n+ log 1\nδ + 6 log(N + T)\ni\n+\nL2\n8\n√\nN\n.\nThus the theorem is proved.\nBack to our context of multi-task learning in which we have m tasks with each task model:\nθi = [θsh, θi\nns], we can prove the following theorem.\nTheorem 3. With the assumption that adding Gaussian perturbation will rise the test error:\nLD(θi) ≤Eϵ∼N(0,σ2I)\n\u0002\nLD(θi + ϵ)\n\u0003\n. Let Ti be the number of parameter θi and N be the cardinality\nof S. We have the following inequality holds with probability 1 −δ (over the choice of training set\nS ∼D):\n\u0002\nLi\nD\n\u0000θi\u0001\u0003m\ni=1 ≤\n\u0002\nEϵ∼N(0,σ2I)\n\u0002\nLS(θi + ϵ)\n\u0003\n+ fi \u0000∥θi∥2\n2\n\u0001 \u0003m\ni=1,\n(7)\nwhere\nfi \u0000∥θi∥2\n2\n\u0001\n=\n1\n√\nN\n\"\n1\n2 + Ti\n2 log\n\u0010\n1 + ||θ||2\nTiσ2\n\u0011\n+ log 1\nδ + 6 log(N + Ti) + L2\n8\n#\n.\nProof. The result for the base case m = 1 can be achieved by using Theorem 2 where ξ = δ and\nf1 is defined accordingly. We proceed by induction, suppose that Theorem 3 is true for all i ∈[n]\nwith probability 1 −δ/2, which also means:\n\u0002\nLi\nD\n\u0000θi\u0001\u0003n\ni=1 ≤\n\u0002\nEϵ∼N(0,σI)\n\u0002\nLS(θi + ϵ)\n\u0003\n+ fi \u0000∥θi∥2\n2\n\u0001 \u0003n\ni=1.\nUsing Theorem 2 for θn+1 and ξ = δ/2, with probability 1 −δ/2, we have:\nLn+1\nD\n\u0000θn+1\u0001\n≤Eϵ∼N(0,σI)\n\u0002\nLS(θn+1 + ϵ)\n\u0003\n+ fn+1 \u0000∥θn+1∥2\n2\n\u0001\n.\nUsing the inclusion–exclusion principle, with probability at least 1 −δ, we reach the conclusion\nfor m = n + 1.\nWe next prove the result in the main paper. Let us begin by formally restating the main theorem\nas follows:\n20\n\nTheorem 4. For any perturbation radius ρsh, ρns > 0, with probability 1 −δ (over the choice of\ntraining set S ∼D) we obtain:\n\u0002\nLi\nD\n\u0000θi\u0001\u0003m\ni=1 ≤\nmax\n∥ϵsh∥2≤ρsh\n\"\nmax\n∥ϵins∥2≤ρns\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\n+ fi \u0000∥θi∥2\n2\n\u0001\n#m\ni=1\n,\n(8)\nwhere fi \u0000∥θi∥2\n2\n\u0001\nis defined the same as in Theorem 3.\nProof. Theorem 3 gives us\nh\nLi\nD\n\u0010\nθi\u0011im\ni=1 ≤\nh\nEϵ∼N(0,σ2I)\nh\nLi\nS\n\u0010\nθi + ϵ\n\u0011i\n+ f i \u0010\n∥θi∥2\n\u0011im\ni=1\n=\n\u0014Z\nEϵins\nh\nLi\nS\n\u0010\nθsh + ϵsh, θi\nns + ϵi\nns\n\u0011i\np (ϵsh) dϵsh + f i \u0010\n∥θi∥2\n\u0011\u0015m\ni=1\n= Eϵsh\nh\nEϵins\nh\nLi\nS\n\u0010\nθsh + ϵsh, θi\nns + ϵi\nns\n\u0011i\n+ f i \u0010\n∥θi∥2\n\u0011im\ni=1 ,\nwhere p(ϵsh) is the density function of Gaussian distribution; ϵsh and ϵi\nns are drawn from their\ncorresponding Gaussian distributions.\nWe have ϵi\nns ∼N(0, σ2Ins) with the dimension Ti,ns, therefore ∥ϵi\nns∥follows the Chi-square\ndistribution. As proven in [70], we have for all i:\nP\n\u0010\n∥ϵi\nns∥2\n2 ≥Ti,nsσ2 + 2σ2p\nTi,nst + 2tσ2\u0011\n≤e−t, ∀t > 0\nP\n\u0010\n∥ϵi\nns∥2\n2 < Ti,nsσ2 + 2σ2p\nTi,nst + 2tσ2\u0011\n> 1 −e−t\nfor all t > 0.\nSelect t = ln(\n√\nN), we derive the following bound for the noise magnitude in terms of the\nperturbation radius ρns for all i:\nP\n\u0012\n∥ϵi\nns∥2\n2 ≤σ2(2 ln(\n√\nN) + Ti,ns + 2\nq\nTi,ns ln(\n√\nN))\n\u0013\n> 1 −\n1\n√\nN\n.\n(9)\nMoreover, we have ϵsh ∼N(0, σ2Ish) with the dimension Tsh, therefore ∥ϵsh∥follows the Chi-\nsquare distribution. As proven in [70], we have:\nP\n\u0010\n∥ϵsh∥2\n2 ≥Tshσ2 + 2σ2p\nTsht + 2tσ2\u0011\n≤e−t, ∀t > 0\nP\n\u0010\n∥ϵsh∥2\n2 < Tshσ2 + 2σ2p\nTsht + 2tσ2\u0011\n> 1 −e−t\nfor all t > 0.\nSelect t = ln(\n√\nN), we derive the following bound for the noise magnitude in terms of the\nperturbation radius ρsh:\nP\n\u0012\n∥ϵsh∥2\n2 ≤σ2(2 ln(\n√\nN) + Tsh + 2\nq\nTsh ln(\n√\nN))\n\u0013\n> 1 −\n1\n√\nN\n.\n(10)\n21\n\nBy choosing σ less than\nρsh\nq\n2 ln N1/2+Tsh+2√\nTsh ln N1/2 and mini\nρns\nq\n2 ln N1/2+Ti,ns+2√\nTi,ns ln N1/2 , and\nreferring to (9,10), we achieve both:\nP\n\u0000∥ϵi\nns∥< ρns\n\u0001\n> 1 −\n1\nN1/2 , ∀i,\nP (∥ϵsh∥< ρsh) > 1 −\n1\nN1/2 .\nFinally, we finish the proof as:\n\u0002\nLi\nD\n\u0000θi\u0001\u0003m\ni=1 ≤Eϵsh\n\u0002\nEϵins\n\u0002\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\u0003\n+ fi \u0000∥θi∥2\n\u0001\u0003m\ni=1\n≤max||ϵsh||<ρsh\n\u0014\nmax||ϵins||<ρnsLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\n+\n2\n√\nN −1\nN + fi \u0000∥θi∥2\n\u0001\u0015m\ni=1\nTo reach the final conclusion, we redefine:\nfi \u0000∥θi∥2\n\u0001\n=\n2\n√\nN\n−1\nN + fi \u0000∥θi∥2\n\u0001\n.\nHere we note that we reach the final inequality due to the following derivations:\nEϵsh\n\u0002\nEϵins\n\u0002\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\u0003\u0003m\ni=1\n≤\nZ\nBsh\n\"Z\nBins\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\ndϵi\nns +\n1\n√\nN\n#m\ni=1\ndϵsh\n+\nZ\nBc\nsh\n\"Z\nBins\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\ndϵi\nns +\n1\n√\nN\n#m\ni=1\ndϵsh\n≤\nZ\nBsh\n\"Z\nBins\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\ndϵi\nns\n#m\ni=1\ndϵsh +\n\u0012\n1 −\n1\n√\nN\n\u0013\n1\n√\nN\n+\n1\n√\nN\n≤max||ϵsh||<ρsh\n\u0014\nmax||ϵins||<ρns\n\u0002\nLi\nS\n\u0000θsh + ϵsh, θi\nns + ϵi\nns\n\u0001\u0003 \u0015m\ni=1\n+ 2\n√\nN\n−1\nN ,\nwhere Bsh = {ϵsh : ||ϵsh|| ≤ρsh}, Bc\nsh is the compliment set, and Bi\nns =\n\b\nϵi\nns : ||ϵi\nns|| ≤ρns\n\t\n.\nB\nGradient aggregation strategies overview\nThis section details how the gradient_aggregate operation is defined according to recent gradient-\nbased multi-task learning methods that we employed as baselines in the main paper, including MGDA\n[8], PCGrad [9], CAGrad [10] and IMTL [11]. Assume that we are given m vectors g1, g2, . . . , gm\nrepresent task gradients. Typically, we aim to find a combined gradient vector as:\ng = gradient_aggregate(g1, g2, . . . , gm)\n.\n22\n\nB.1\nMultiple-gradient descent algorithm - MGDA\n[8] apply MGDA [71] to find the minimum-norm gradient vector that lies in the convex hull composed\nby task gradients g1, g2, . . . , gm:\ng = argmin||\nm\nX\ni=1\nwigi||2, s.t.\nm\nX\ni=1\nwi = 1\nand\n, wi ≥0∀i.\nThis approach can guarantee that the obtained solutions lie on the Pareto front of task objective\nfunctions.\nB.2\nProjecting conflicting gradients - PCGrad\nPCgrad resolves the disagreement between tasks by projecting gradients that conflict with each\nother, i.e. ⟨gi, gj⟩< 0, to the orthogonal direction of each other. Specifically, gi is replaced by its\nprojection on the normal plane of gj:\ngi\nPC = gi −gi · gj\n||gj||2 gj.\nThen compute the aggregated gradient based on these deconflict vectors g = Pm\ni gi\nPC.\nB.3\nConflict Averse Gradient Descent - CAGrad\nCAGrad [10] seeks a worst-case direction in a local ball around the average gradient of all tasks, g0,\nthat minimizes conflict with all of the gradients. The updated vector is obtained by optimizing the\nfollowing problem:\nmax\ng∈R min\ni∈[m]⟨gi, g⟩\ns.t.\n||g −g0|| ≤c|||g0|,\nwhere g0 = 1\nm\nPm\ni gi is the averaged gradient and c is a hyper-parameter.\nB.4\nImpartial multi-task learning - IMTL\nIMTL [11] proposes to balance per-task gradients by finding the combined vector g, whose projections\nonto {gi}m\ni=1 are equal. Following this, they obtain the closed-form solution for the simplex vector\nw for reweighting task gradients:\nw = g1U ⊤\u0010\nDU ⊤\u0011−1\nwhere ui = gi/\n\r\rgi\r\r, U =\n\u0002\nu1 −u2, · · · , u1 −um\u0003\n, and D =\n\u0002\ng1 −g2, · · · , g1 −gm\u0003\nThe aggregated\nvector is then calculated as g = Pm\ni wigi.\nC\nImplementation Details\nIn this part, we provide implementation details regarding the empirical evaluation in the main paper\nalong with additional comparison experiments.\n23\n\nC.1\nBaselines\nIn this subsection, we briefly introduce some of the comparative methods that appeared in the main\ntext:\n• Linear scalarization (LS) minimizes the unweighted sum of task objectives Pm\ni Li(θ).\n• Scale-invariant (SI) aims toward obtaining similar convergent solutions even if losses are scaled\nwith different coefficients via minimizing Pm\ni log Li(θ).\n• Random loss weighting (RLW) [72] is a simple yet effective method for balancing task losses or\ngradients by random weights.\n• Dynamic Weight Average (DWA) [2] simply adjusts the weighting coefficients by taking the\nrate of change of loss for each task into account.\n• GradDrop [65] presents a probabilistic masking process that algorithmically eliminates all\ngradient values having the opposite sign w.r.t a predefined direction.\nC.2\nImage classification\nNetwork Architectures. For two datasets in this problem, Multi-MNIST and CelebA, we replicate\nexperiments from [8, 25] by respectively using the Resnet18 (11M parameters) and Resnet50 (23M\nparameters) [61] with the last output layer removed as the shared encoders and constructing linear\nclassifiers as the task-specific heads, i.e. 2 heads for Multi-MNIST and 40 for CelebA, respectively.\nTraining Details. We train the all the models under our proposed framework and baselines\nusing:\n• Multi-MNIST: Adam optimizer [73] with a learning rate of 0.001 for 200 epochs using a batch\nsize of 256. Images from the three datasets are resized to 36 × 36.\n• CelebA: Batch-size of 256 and images are resized to 64 × 64 × 3. Adam [73] is used again with\na learning rate of 0.0005, which is decayed by 0.85 for every 10 epochs, our model is trained\nfor 50 epochs in total.\nRegarding the hyperparameter for SAM [21], we use their adaptive version [74] where both ρsh\nand ρns are set equally and extensively tuned from 0.005 to 5.\nC.3\nScene understanding\nTwo datasets used in this problem are NYUv2 and CityScapes. Similar to [13], all images in the\nNYUv2 dataset are resized to 288 × 384 while all images in the CityScapes dataset are resized to\n128 × 256 to speed up the training process. We follow the exact protocol in [13] for implementation.\nSpecifically, SegNet [66] is adopted as the architecture for the backbone and Multi-Task Attention\nNetwork MTAN [2] is applied on top of it. We train each method for 200 epochs using Adam\noptimizer [73] with an initial learning rate of 1e −4 and reduced it to 5e −5 after 100 epochs. We\nuse a batch size of 2 for NYUv2 and 8 for CityScapes. The last 10 epochs are averaged to get the\nfinal results, and all experiments are run with three random seeds.\n24\n\nD\nAdditional Results\nTo further show the improvement of our proposed training framework over the conventional one, this\nsection provides additional comparison results in terms of qualitative results, predictive performance,\nconvergent behavior, loss landscape, model sharpness, and gradient norm. Please note that in\nthese experiments, we choose IMTL and F-IMTL as two examples for standard and flat-aware\ngradient-based MTL training respectively. We also complete the ablation study in the main paper\nby providing results on all three datasets in the Multi-MNIST dataset.\nD.1\nImage segmentation qualitative result\nIn this section, we provide qualitative results of our method of the CityScapes experiment. We\ncompare our proposed method against its main baseline by highlighting typical cases where our\nmethod excels in generalization performance. Figure 6 shows some visual examples of segmentation\noutputs on the test set. Note that in the CityScapes dataset, the “void\" class is identified as unclear\nand pixels labeled as void do not contribute to either objective or score [60].\n(a) A training sample (after augmentation)\n(b) Corresponding original image (before augmentation)\n(c) Predictions on an unseen image\n(d) Predictions on an unseen image\nFigure 6: Semantic segmentation prediction comparison on CityScapes . From left to right are\ninput images, ground truth, and segmentation outputs from SegNet [66] using ERM training and\nsharpness-aware training. Regions that are represented in gray color are ignored during training.\n(Best viewed in color).\n25\n\nWhile there is only a small gap between the segmentation performance of IMTL and F-IMTL,\nwe found that a small area, which is the car hood and located at the bottom of images, is often\nincorrectly classified. For example, in Figure 6, the third and fourth rows compare the prediction\nof SegNet [66] with ERM training and with our proposed method. It can be seen that both of\nthem could not detect this area correctly, this is because this unclear “void\" class did not appear\nduring training. Even worse, the currently employed data augmentation technique in the codebase\nof Nash-MTL and other recent multi-task learning methods [13, 10] consists of RandomCrop, which\noften unintentionally excludes edge regions. For example, Figure 6a shows an example fed to the\nneural network for training, which excludes the car hood and its logo, compared to the original\nimage (Figure 6b). Therefore, we can consider this \"void\" class as a novel class in this experiment,\nsince its appearance is ignored in both training and evaluation. Even though, in Figures 6c and 6d\nour training method is still able to distinguish between this unknown area and other nearby known\nclasses, which empirically shows the robustness and generalization ability of our method over ERM.\nD.2\nPredictive performance\nIn this part, we provide experimental justification for an intriguing insight into the connection\nbetween model sharpness and model calibration. Empirically, we found that when a model converges\nto flatter minima, it tends to be more calibrated. We start by giving the formal definition of a\nwell-calibrated classification model and three metrics to measure the calibration of a model, then we\nanalyze our empirical results.\nConsider a C-class classification problem with a test set of N samples given in the form (xi, yi)N\ni=1\nwhere yi is the true label for the sample xi. Model outputs the predicted probability for a given\nsample xi to fall into C classes, is given by\nˆp(xi) = [ˆp(y = 1|xi), . . . , ˆp(y = C|xi)].\nˆp(y = c|xi) is also the confidence of the model when assigning the sample xi to class c. The predicted\nlabel ˆyi is the class with the highest predicted value, ˆp(xi) := maxc ˆp(y = c|xi). We refer to ˆp(xi) as\nthe confidence score of a sample xi.\nModel calibration is a desideratum of modern deep neural networks, which indicates that the\npredicted probability of a model should match its true probability. This means that the classification\nnetwork should be not only accurate but also confident about its prediction, i.e. being aware of\nwhen it is likely to be incorrect. Formally stated, the perfect calibration [75] is:\nP(ˆy = y|ˆp = q) = q, ∀q ∈[0, 1].\n(11)\nMetric. The exact computation of Equation 11 is infeasible, thus we need to define some metrics\nto evaluate how well-calibrated a model is.\n• Brier score ↓(BS) [76] assesses the accuracy of a model’s predicted probability by taking into\naccount the absolute difference between its confidence for a sample to fall into a class and the\ntrue label of that sample. Formally,\nBS = 1\nN\nN\nX\ni=1\nC\nX\nc=1\n(ˆp(y = c|xi) −1[yi = c])2 .\n26\n\n• Expected calibration error ↓(ECE) compares the predicted probability (or confidence) of a\nmodel to its accuracy [77, 75]. To compute this error, we first bin the confidence interval [0, 1]\ninto M equal bins, then categorize data samples into these bins according to their confidence\nscores. We finally compute the absolute value of the difference between the average confidence\nand the average accuracy within each bin, and report the average value over all bins as the\nECE. Specifically, let Bm denote the set of indices of samples having their confidence scores\nbelonging to the mth bin. The average accuracy and the average confidence within this bin are:\nacc(Bm) =\n1\n|Bm|\nX\ni∈Bm\n1[ ˆyi = yi],\nconf(Bm) =\n1\n|Bm|\nX\ni∈Bm\nˆp(xi).\nThen the ECE of the model is defined as:\nECE =\nM\nX\nm=1\n|Bm|\nN |acc(Bm) −conf(Bm)|.\nIn short, the lower ECE neural networks obtain, the more calibrated they are.\n• Predictive entropy (PE) is a widely-used measure of uncertainty [78, 79, 80] via the predictive\nprobability of the model output. When encountering an unseen sample, a well-calibrated model\nis expected to yield a high PE, representing its uncertainty in predicting out-of-domain (OOD)\ndata.\nPE = 1\nC\nC\nX\nc=1\n−ˆp(y = c|xi) log ˆp(y = c|xi).\nFigures 7 and 8 plot the distribution of the model’s predicted entropy in the case of in-domain\nand out-domain testing, respectively. We can see when considering the flatness of minima, the model\nshows higher predictive entropy on both in-domain and out-of-domain, compared to ERM. This also\nmeans that our model outputs high uncertainty prediction when it is exposed to a sample from a\ndifferent domain.\n27\n\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nOurs\nERM\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n0\n5\n10\n15\n20\n25\n30\nOurs\nERM\n0.0\n0.5\n1.0\n1.5\n2.0\n0\n2\n4\n6\n8\n10\n12\n14\n16\nOurs\nERM\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n0\n5\n10\n15\n20\n25\nOurs\nERM\nFigure 7: Histograms of predictive entropy of ResNet18 [61] on in domain dataset, train and test on\nMultiMNIST (left) and MultiFashion (right). We use the orange lines to denote ERM training while\nblue lines indicate our proposed method.\n0.0\n0.5\n1.0\n1.5\n2.0\n0\n1\n2\n3\n4\n5\n6\nOurs\nERM\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nOurs\nERM\n0.0\n0.5\n1.0\n1.5\n2.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nOurs\nERM\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0\n1\n2\n3\n4\nOurs\nERM\nFigure 8: Out of domain: model is trained on MultiMNIST, then tested on MultiFashion (left)\nand vice versa (right). Models trained with ERM give over-confident predictions as their predictive\nentropy concentrates around 0.\n28\n\nHere, we calculate the results for both tasks 1 and 2 as a whole and plot their ECE in Figure 9.\nWhen we look at the in-domain prediction in more detail, our model still outperforms ERM in terms\nof expected calibration error. We hypothesize that considering flat minima optimizer not only lowers\nerrors across tasks but also improves the predictive performance of the model.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Accuracy\nECE=4.47\nERM\nGap\nAccuracy\nECE=2.49\nOurs\nGap\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Accuracy\nECE=11.72\nGap\nAccuracy\nECE=3.28\nGap\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nExpected Accuracy\nECE=6.54\nGap\nAccuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\nECE=4.91\nGap\nAccuracy\nFigure 9: The predictive performance (measured by the expected calibration error) of neural networks\nhas been enhanced by using our proposed training method (right column).\nWe also report the Brier score and ECE for each task in Table 6 and Table 7. As can be observed\n29\n\nTable 6: Brier score on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST datasets. We use\nthe bold font to highlight the best results.\n.\nDataset\nTask\nMulti-Fashion\nMulti-Fashion+MNIST\nMultiMNIST\nERM\nTop left\n0.237\n0.055\n0.082\nBottom right\n0.254\n0.217\n0.106\nAverage\n0.246\n0.136\n0.094\nOurs\nTop left\n0.172\n0.037\n0.059\nBottom right\n0.186\n0.189\n0.075\nAverage\n0.179\n0.113\n0.067\nfrom these tables, our method shows consistent improvement in the model calibration when both\nscores decrease over all scenarios.\nTable 7: Expected calibration error on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST\ndatasets. Here we set the number of bins equal to 10.\nDataset\nTask\nMulti-Fashion\nMulti-Fashion+MNIST\nMultiMNIST\nERM\nTop left\n0.113\n0.027\n0.039\nBottom right\n0.121\n0.104\n0.050\nAverage\n0.117\n0.066\n0.045\nOurs\nTop left\n0.034\n0.015\n0.022\nBottom right\n0.032\n0.083\n0.028\nAverage\n0.033\n0.049\n0.025\nD.3\nEffect of choosing perturbation radius ρ.\nThe experimental results analyzing the sensitivity of model w.r.t ρ are given in Figure 10. We evenly\npicked ρ from 0 to 3.0 to run F-CAGrad on three Multi-MNIST datasets.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n88\n90\n92\n94\n96\nAccuracy\nMultiFashion\nMultiMnist\nMultiFashion+MNIST\nFigure 10: Average accuracy when varying ρ from 0 to 3.0 (with error bar from three independent\nruns).\nWe find that the average accuracy of each task is rather stable from ρ = 0.5, which means the\n30\n\neffect of different values of ρ in a reasonably small range is similar. It can also easy to notice that\nthe improvement tends to saturate when ρ ≥1.5.\nD.4\nGradient conflict.\nIn the main paper, we measure the percentage of gradient conflict on the MultiFashion+MNIST\ndataset. Here, we provide the full results on three different datasets. As can be seen from Figure 11,\nthere is about half of the mini-batches lead to the conflict between task 1 and task 2 when using\ntraditional training. Conversely, our proposed method significantly reduces such confliction (less\nthan 5%) via updating the parameter toward flat regions.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n10\n20\n30\n40\n50\nGradient conflict (%)\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n10\n20\n30\n40\n50\n60\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n10\n20\n30\n40\n50\nFigure 11: Task gradient conflict proportion of models trained with our proposed method and\nERM across MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nD.5\nLoss landscape\nThirdly, we provide additional visual comparisons of the loss landscapes trained with standard\ntraining and with our framework across two tasks of three datasets of Multi-MNIST. As parts of the\nobtained visualizations have been presented in the main paper, we provide the rest of them in this\nsubsection. The results in Figures 12 and 13 consistently show that our method obtains significantly\nflatter minima on both two tasks, encouraging the model to generalize well.\nFigure 12: Loss landscapes of task 1 and task 2 on MultiFashion\n31\n\nFigure 13: Loss landscapes of task 1 and task 2 on MultiFashion+MNIST\nD.6\nTraining curves\nSecondly, we compare the test accuracy of trained models under the two settings in Fig. 15. It can\nbe seen that from the early epochs (20-th epoch), the flat-based method outperforms the ERM-based\nmethod on all tasks and datasets. . Although the ERM training model is overfitted after such a\nlong training, our model retains a high generalizability, as discussed throughout previous sections.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAccuracy\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\nAccuracy\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.80\n0.85\n0.90\n0.95\n1.00\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nFigure 14: Train accuracy of models trained with our proposed method and ERM across 2 tasks\n(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nFurthermore, we also plot the training accuracy curves across experiments in Figure 14 to\nshow that training accuracy scores of both ERM and our proposed method are similar and reach\n≈100% from 50-th epoch, which illustrates that the improvement is associated with generalization\nenhancement, not better training.\n32\n\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n90\n91\n92\n93\n94\n95\n96\nAccuracy\n0\n25\n50\n75\n100\n125\n150\n175\n200\n90\n92\n94\n96\n98\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n81\n82\n83\n84\n85\n86\n87\n88\n0\n25\n50\n75\n100\n125\n150\n175\n200\n88\n89\n90\n91\n92\n93\n94\n95\nAccuracy\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n82\n83\n84\n85\n86\n87\n88\n89\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n80\n82\n84\n86\nFigure 15: Test accuracy of models trained with our proposed method and ERM across 2 tasks\n(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nD.7\nModel sharpness\nFourthly, Figure 16 displays the evolution of ρ-sharpness of models along training epochs under\nconventional loss function (ERM) and worst-case loss function (ours) on training sets of three\ndatasets from Multi-MNIST, with multiple values of ρ. We can clearly see that under our framework,\nfor both tasks, the model can guarantee uniformly low loss value in the ρ-ball neighborhood of\nparameter across training process. In contrast, ERM suffers from sharp minima from certain epochs\nwhen the model witnesses a large gap between the loss of worst-case perturbed model and current\nmodel. This is the evidence for the benefit that our framework brings to gradient-based methods,\nwhich is all tasks can concurrently find flat minima thus achieving better generalization.\nD.8\nGradient norm\nFinally, we demonstrate the gradient norm of the loss function w.r.t the worst-case perturbed\nparameter of each task. On the implementation side, we calculate the magnitude of the flat gradient\ngi,flat for each task at different values of ρ in Figure 17. As analyzed by equation (6) from the main\npaper, following the negative direction of gi,SAM\nsh\nwill lower the L2 norm of the gradient, which\norients the model towards flat regions. This is empirically verified in Figure 17. In contrast, as the\nnumber of epochs increases, gradnorm of the model trained with ERM tends to increase or fluctuate\naround a value higher than that of model trained with SAM.\n33\n\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n(a) ρ = 0.005\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n1\n2\n3\n4\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.00\n0.05\n0.10\n0.15\n0.20\n(b) ρ = 0.05\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n2\n4\n6\n8\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n20\n40\n60\n80\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n5\n10\n15\n20\n25\n30\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n1\n2\n3\n4\n5\n6\n7\nSharpness\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n10\n20\n30\n40\n50\n60\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n5\n10\n15\n20\n(c) ρ = 0.5\nFigure 16: Sharpness of models trained with our proposed method and ERM with different values\nof ρ. For each ρ, the top and bottom row respectively represents the first and second task, and each\ncolumn respectively represents each dataset in Multi-MNIST: from left to right are MultiFashion,\nMultiFashion+MNIST, MultiMNIST.\n34\n\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.00\n0.05\n0.10\n0.15\n0.20\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.00\n0.02\n0.04\n0.06\n0.08\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(a) ρ = 0.005\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(b) ρ = 0.05\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n1\n2\n3\n4\n5\n6\n7\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n2\n4\n6\n8\n10\n12\n14\nOurs\nERM\n0\n25\n50\n75\n100\n125\n150\n175\n200\n0\n1\n2\n3\n4\n5\n6\n7\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n1\n2\n3\n4\n5\n6\n7\nGradient norm\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n5\n10\n15\n20\n0\n25\n50\n75\n100\n125\n150\n175\n200\nEpochs\n0\n1\n2\n3\n4\n5\n6\n7\n(c) ρ = 0.5\nFigure 17: Gradient magnitude at the worst-case perturbations of models trained with our\nproposed method and ERM with different values of ρ.\nFor each ρ, the top and bottom row\nrespectively represents the first and second task, and each column respectively represents each\ndataset in Multi-MNIST: from left to right are MultiFashion, MultiFashion+MNIST, MultiMNIST.\n35",
    "pdf_filename": "Improving_Multi-task_Learning_via_Seeking_Task-based_Flat_Regions.pdf"
}