{
    "title": "Improving Multi-task Learning",
    "abstract": "Multi-TaskLearning(MTL)isawidelyusedandpowerfullearningparadigmfortrainingdeep neuralnetworksthat allows learningmore thanone objectivebya singlebackbone. Comparedto trainingtasksseparately,MTLsignificantlyreducescomputationalcosts,improvesdataefficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeledones,whichcancausenegativetransferbetweentasksandoverallperformancedrop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory. 1 Introduction Over the last few years, deep learning has emerged as a powerful tool for functional approximation by exhibiting superior performance and even exceeding human ability on a wide range of applications. In spite of the appealing performance, training massive independent neural networks to handle individual tasks requires not only expensive computational and storage resources but also long runtime. Therefore, multi-task learning is a more preferable approach in many situations [1, 2, 3] as they can: (i) avoid redundant features calculation for each task through their inherently shared architecture; and(ii)reducethenumberoftotaltrainableparametersbyhardparametersharing[4,5] or soft parameter sharing [6, 7]. However, existing state-of-the-art methods following the veins of gradient-based multi-task learning [8, 9, 10, 11, 12, 13] tend to neglect geometrical properties of the loss landscape yet solely focus on minimizing the empirical error in the optimization process, which can be easily prone to the overfitting problem [14, 15]. ∗ Equal contributions. 1 4202 voN 91 ]GL.sc[ 3v32731.1122:viXra",
    "body": "Improving Multi-task Learning\nvia Seeking Task-based Flat Regions\nHoang Phan∗1 Lam Tran∗2 Quyen Tran2 Ngoc N. Tran3\nTuan Truong2 Nhat Ho4 Dinh Phung5 Trung Le5\nNew York University1, VinAI Research2, Vanderbilt University3,\nUniversity of Texas, Austin4, Monash University, Australia 5\nNovember 20, 2024\nAbstract\nMulti-TaskLearning(MTL)isawidelyusedandpowerfullearningparadigmfortrainingdeep\nneuralnetworksthat allows learningmore thanone objectivebya singlebackbone. Comparedto\ntrainingtasksseparately,MTLsignificantlyreducescomputationalcosts,improvesdataefficiency,\nand potentially enhances model performance by leveraging knowledge across tasks. Hence, it\nhas been adopted in a variety of applications, ranging from computer vision to natural language\nprocessing and speech recognition. Among them, there is an emerging line of work in MTL that\nfocuses on manipulating the task gradient to derive an ultimate gradient descent direction to\nbenefit all tasks. Despite achieving impressive results on many benchmarks, directly applying\nthese approaches without using appropriate regularization techniques might lead to suboptimal\nsolutions to real-world problems. In particular, standard training that minimizes the empirical\nloss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by\nnoisy-labeledones,whichcancausenegativetransferbetweentasksandoverallperformancedrop.\nTo alleviate such problems, we propose to leverage a recently introduced training method, named\nSharpness-aware Minimization, which can enhance model generalization ability on single-task\nlearning. Accordingly, we present a novel MTL training methodology, encouraging the model to\nfind task-based flat minima for coherently improving its generalization capability on all tasks.\nFinally, we conduct comprehensive experiments on a variety of applications to demonstrate the\nmerit of our proposed approach to existing gradient-based MTL methods, as suggested by our\ndeveloped theory.\n1 Introduction\nOver the last few years, deep learning has emerged as a powerful tool for functional approximation\nby exhibiting superior performance and even exceeding human ability on a wide range of applications.\nIn spite of the appealing performance, training massive independent neural networks to handle\nindividual tasks requires not only expensive computational and storage resources but also long\nruntime. Therefore, multi-task learning is a more preferable approach in many situations [1, 2, 3]\nas they can: (i) avoid redundant features calculation for each task through their inherently shared\narchitecture; and(ii)reducethenumberoftotaltrainableparametersbyhardparametersharing[4,5]\nor soft parameter sharing [6, 7]. However, existing state-of-the-art methods following the veins of\ngradient-based multi-task learning [8, 9, 10, 11, 12, 13] tend to neglect geometrical properties of the\nloss landscape yet solely focus on minimizing the empirical error in the optimization process, which\ncan be easily prone to the overfitting problem [14, 15].\n∗\nEqual contributions.\n1\n4202\nvoN\n91\n]GL.sc[\n3v32731.1122:viXra\nMeanwhile, the overfitting problem of modern neural networks is often attributed to high-\ndimensional and non-convex loss functions, which result in complex loss landscapes containing\nmultiple local optima. Hence, understanding the loss surface plays a crucial role in training robust\nmodels, and developing flat minimizers remains one of the most effective approaches [16, 14, 17, 18].\nTo be more specific, recent studies [19, 20] show that the obtained loss landscape from directly\nminimizing the empirical risk can consist of many sharp minimums, thus yielding poor generalization\ncapacity when being exposed to unseen data. Moreover, this issue is apparently exacerbated in\noptimizing multiple objectives simultaneously, as in the context of multi-task learning. Certainly,\nsharp minima of each constituent objective might appear at different locations, which potentially\nresults in large generalization errors on the associated task. To this end, finding a common flat\nand low-loss valued region for all tasks is desirable for improving the current methods of multi-task\nlearning.\nContribution. To further address the above desideratum, we propose a novel MTL training\nmethod,incorporatingtherecentlyintroducedoptimizationsharpness-awareminimization(SAM)[21]\nintoexistinggradientmanipulationstrategiesinmulti-tasklearningtofurtherboosttheirperformance.\nGuiding by the generalization error in Theorem 1, the proposed approach not only orients the model\ntothejointlowempiricallossvalueacrosstasksbutalsoencouragesthemodeltoreachthetask-based\nflat regions. Importantly, our approach is model-agnostic and compatible with current gradient-based\nMTL approaches (see Figure 1 for the overview of our approach). By using our proposed framework,\nthe gradient conflict across tasks is mitigated significantly, which is the goal of recent gradient-based\nMTL studies in alleviating negative transfer between tasks. Finally, we conduct comprehensive\nexperiments on a variety of applications to demonstrate the merit of our approach for improving not\nonly task performance but also model robustness and calibration. Last but not least, to the best of\nour knowledge, ours is the first work to improve multi-task learning by investigating the geometrical\nproperties of the model loss landscape.\n2 Related work\n2.1 Multi-task learning\nIn multi-task learning (MTL), we often aim to jointly train one model to tackle multiple different\nbut correlated tasks. It has been proven in prior work [22, 2, 23, 24] that it is not only able to\nenhance the overall performance but also reduce the memory footprint and fasten the inference\nprocess. Previous studies on MTL often employ a hard parameter-sharing mechanism along with\nlight-weight task-specific modules to handle multiple tasks.\nPareto multi-task learning. Originated from Multiple-gradient descent algorithm (MGDA),\na popular line of gradient-based MTL methods aim to find Pareto stationary solutions, from\nwhich we can not further improve model performance on any particular task without diminishing\nanother [8]. Moreover, recent studies suggest exploring the whole Pareto front by learning diverse\nsolutions [25, 26, 27, 28], or profiling the entire Pareto front with hyper-network [29, 30]. While\nthese methods are theoretically grounded and guaranteed to converge to Pareto-stationary points,\nthe experimental results are often limited and lack comparisons under practical settings.\nLoss and gradient balancing. Another branch of preliminary work in MTL capitalizes\non the idea of dynamically reweighting loss functions based on gradient magnitudes [31], task\nhomoscedastic uncertainty [32], or difficulty prioritization [33] to balance the gradients across tasks.\nMore recently, PCGrad [9] developed a gradient manipulation procedure to avoid conflicts among\n2\nTask 1 Task 2\nShared\nencoder\nTask flat gradient\nCombined flat gradient\nTask loss gradient\nCombined loss gradient\nFinal gradient\nLow loss regions\nInput\nFigure 1: We demonstrate our framework in a 2-task problem. For the shared part, task-based flat\ngradients (red dashed arrows) steer the model to escape sharp areas, while task-based loss gradients\n(orange dashed arrows) lead the model into their corresponding low-loss regions. In our method, we\naggregate them to find the combined flat gradient gflat and combined loss gradient gloss, respectively.\nsh sh\nFinally, we add those two output gradients to target the joint low-loss and flat regions across the\ntasks. Conversely, updating task-specific non-shared parts is straightforward and much easier since\nthere is the involvement of one objective only.\ntasks by projecting random task gradients on the normal plane of the other. Similarly, [10] proposes a\nprovably convergent method to minimize the average loss, and [11] calculates loss scaling coefficients\nsuch that the combined gradient has equal-length projections onto individual task gradients.\n2.2 Flat minima\nFlat minimizer has been found to improve generalization ability of neural networks because it enables\nmodels to find wider local minima, by which they will be more robust against shifts between train\nand test losses [34, 35, 36]. This relationship between generalization ability and the width of minima\nis theoretically and empirically studied in many studies [37, 38, 39, 40], and subsequently, a variety\nof methods seeking flat minima have been proposed [41, 42, 43, 44]. For example, [43, 45, 46] analyze\nthe impacts of different training factors, such as batch-size, learning rate, covariance of gradient,\ndropout, on the flatness of found minima. Additionally, several schemes pursue wide local minima by\nadding regularization terms to the loss function [41, 47, 48, 42], e.g., softmax output’s low entropy\npenalty, [41], distillation losses [47, 48].\nRecently, SAM [21], which seeks flat regions by explicitly minimizing the worst-case loss around\nthe current model, has received significant attention due to its effectiveness and scalability compared\nto previous methods. Particularly, it has been exploited in a variety of tasks and domains [49, 50, 51,\n52, 53, 54, 55]. A notable example is the improvement that SAM brings to meta-learning bi-level\noptimization in [50]. Another application of SAM is in federated learning (FL) [51] in which the\nauthors achieved tighter convergence rates than existing FL works, and proposed a generalization\nbound for the global model. In addition, SAM shows its generalization ability in vision models [54],\nlanguage models [53] and domain generalization [49]. However, existing studies have only focused\n3\non single task problems. In this work, we leverage SAM’s principle to develop theory and devise\npractical methods, allowing seeking flat minima in gradient-based multi-task learning models.\n3 Preliminaries\nConventional training methods that focus on minimizing the empirical loss can be easily prone\nto overfitting problems (i.e., the validation error no longer decreases, but the training loss keeps\ndeclining), thus, restricting model generalization performance. In an attempt to alleviate such\nphenomenons, [21] proposed to minimize the worst-case loss in a neighborhood of the current model\nparameter given by:\nmin max L(θ+ϵ), (1)\nθ ||ϵ||2≤ρ\nwhere ||·|| denotes the l norm and ρ represents the radius of the neighborhood. We assume L is\n2 2\ndifferentiable up to the first order with respect to θ. The optimization problem (1) is referred to as\nsharpness aware minimization (SAM).\nTo solve problem (1), [21] proposed to first find the solution for the inner maximization by\napproximating L(θ+ϵ) via a first-order Taylor expansion w.r.t ϵ around 0, which is as follows:\n∇ L(θ)\nϵ∗ = argmax L(θ+ϵ) ≈ argmax ϵ⊤∇ L(θ) ≈ ρ θ .\nθ\n||∇ L(θ)||\n||ϵ||2≤ρ ||ϵ||2≤ρ θ 2\nPutting into words, the worst-case perturbation is approximated as the scaled gradient of the loss\nw.r.t the current parameter θ. Then, the gradient w.r.t this perturbed model is computed to update\nθ:\ngSAM := ∇ max L(θ+ϵ) ≈ ∇ L(θ+ϵ)| (2)\nθ θ θ+ϵ∗\n||ϵ||2≤ρ\n4 Our Proposed Framework\nThis section describes our proposed framework to improve existing methods on gradient-based MTL.\nWe first recall the goal of multi-task learning, then establish the upper bounds for the general loss\nof each task. Subsequently, we rely on these upper bounds to devise the proposed framework for\nimproving the model generalization ability by guiding it to a flatter region of each task.\n4.1 Multi-task learning setting\nIn multi-task learning, we are given a data-label distribution D from which we can sample a training\nset S = {(x ,y1,...,ym)n }, where x is a data example and y1,...,ym are the labels of the tasks\ni i i i=1 i i i\n1,2,...,m respectively.\nThe model for each task θi = [θ ,θi ] consists of the shared part θ and the individual\nsh ns sh\nnon-shared part θi . We denote the general loss for task i as Li (θi), while its empirical loss over\nns D\nthe training set S as Li (θi). Existing works in MTL, typically MGDA [8], PCGrad [9], CAGrad [10],\nS\nand IMTL [11], aim to find a model that simultaneously minimizes the empirical losses for all tasks:\nmin (cid:2) L1 (cid:0) θ1(cid:1) ,...,Lm(θm)(cid:3) , (3)\nS S\nθ ,θ1:m\nsh ns\n4\nbycalculatinggradientgi fori-thtask(i ∈ [m]). Thecurrentmodelparameteristhenupdatedbythe\nunifiedgradientg = gradient_aggregate(g1,g2,...,gm),withthegenericoperationgradient_aggregate\nis to combine multiple task gradients, as proposed in gradient-based MTL studies.\nAdditionally, prior works only focus on minimizing the empirical losses and do not concern the\ngeneral losses which combat overfitting. Inspired by SAM [21], it is desirable to develop sharpness-\naware MTL approaches wherein the task models simultaneously seek low loss and flat regions.\nHowever, this is challenging since we have multiple objective functions in (3) and each task model\nconsists of a shared and an individual non-shared parts. To address the above challenge, in Theorem\n1, we develop upper bounds for the task general losses in the context of MTL which signifies the\nconcepts of sharpness for the shared part and non-shared parts and then rely on these new concepts\nto devise a novel MTL framework via seeking the task-based flat regions.\n4.2 Theoretical development\nWe first state our main theorem that bounds the generalization performance of individual tasks by\nthe empirical error on the training set:\nTheorem 1. (Informally stated) For any perturbation radius ρ ,ρ > 0, under some mild\nsh ns\nassumptions, with probability 1−δ (over the choice of training set S ∼ D) we obtain\n(cid:20) (cid:21)m\n(cid:2) Li (cid:0) θi(cid:1)(cid:3)m ≤ max max Li (cid:0) θ +ϵ ,θi +ϵi (cid:1) + fi(cid:0) ∥θi∥2(cid:1) , (4)\nD i=1 S sh sh ns ns 2\n∥ϵ sh∥2≤ρ sh ∥ϵi ns∥2≤ρns i=1\nwhere fi : R → R ,i ∈ [m] are strictly increasing functions.\n+ +\nTheorem 1 establishes the connection between the generalization error of each task with its\nempirical training error via worst-case perturbation on the parameter space. The formally stated\ntheoremandproofareprovidedintheappendix. Herewenotethattheworst-casesharedperturbation\nϵ is commonly learned for all tasks, while the worst-case non-shared perturbation ϵi is tailored\nsh ns\nfor each task i. Theorem 1 directly hints us an initial and direct approach.\nAdditionally, [21] invokes the [56] PAC-Bayesian generalization bound , hence is only applicable\nto the 0-1 loss in the binary classification setting. In terms of the theory contribution, we employ\na more general PAC-Bayesian generalization bound [57] to tackle more general losses in MTL.\nMoreover, our theory development requires us to handle multiple objectives, each of which consists\nof the non-shared and shared parts, which is certainly non-trivial.\n4.3 Initial and direct approach\nA straight-forward approach guided by Theorem 1 is to find the non-shared perturbations ϵi ,i ∈ [m]\nns\nindependently for the non-shared parts and a common shared perturbation for the shared part.\nDriven by this theoretical guidance, we propose the following updates.\nUpdate the non-shared parts. Based on the upper bounds in Theorem 1, because the non-\nshared perturbations ϵi ,i ∈ [m] are independent to each task, for task i, we update its non-shared\nns\n5\npart θi :\nns\n∇ Li (cid:0) θ ,θi (cid:1)\nϵi = ρ θi ns S sh ns ,\nns ns ∥∇ Li (cid:0) θ ,θi (cid:1) ∥\nθi S sh ns 2\nns\ngi,SAM = ∇ Li (cid:0) θ ,θi +ϵi (cid:1) ,\nns θi S sh ns ns\nns\nθi = θi −ηgi,SAM, where η > 0 is the learning rate. (5)\nns ns ns\nUpdate the shared part. Updatingthesharedpartθ ismorechallengingbecauseitsworst-cased\nsh\nperturbation ϵ is shared among the tasks. To derive how to update θ w.r.t. all tasks, we first\nsh sh\ndiscuss the case when we update this w.r.t. task i without caring about other tasks. Specifically,\nthis task’s SAM shared gradient is computed as:\n∇ Li (cid:0) θ ,θi (cid:1)\nϵi = ρ θ sh S sh ns ,\nsh sh ∥∇ Li (cid:0) θ ,θi (cid:1) ∥\nθ sh S sh ns 2\ngi,SAM = ∇ Li (cid:0) θ +ϵi ,θi (cid:1) ,\nsh θ sh S sh sh ns\nthen we have a straight-forward updating strategy:\ngSAM = gradient_aggregate(g1,SAM,...,gm,SAM),\nsh sh sh\nθ = θ −ηgSAM.\nsh sh sh\nAccordingtoouranalysisinSection4.4,eachgi,SAM = gi,loss+gi,flat isconstitutedbytwocomponents:\nsh sh sh\n(i) gi,loss to navigate to the task low-loss region and (ii) gi,flat to navigate to the task-based flat\nsh sh\nregion. However, a direct gradient aggregation of gi,SAM,i ∈ [m] can be negatively affected by the\nsh\ngradient cancelation or conflict because it aims to combine many individual elements with different\nobjectives. In this paper, we go beyond this initial approach by deriving an updating formula to\ndecompose SAM gradient into two components, each serving its own purpose, and then combining\ntheir corresponding task gradients simultaneously. We also compare our method against the naive\napproach in Section 5.3.\n4.4 Our proposed approach\nThe non-shared parts are updated normally as in Equation (5). It is more crucial to investigate how\nto update the shared part more efficiently. To better understand the SAM’s gradients, we analyze\ntheir characteristics by deriving them as follows:\ngi,SAM = ∇ Li (cid:0) θ +ϵi ,θi (cid:1) ( ≈1) ∇ (cid:2) Li (cid:0) θ ,θi (cid:1)(cid:3) +(cid:10) ϵi ,∇ Li (cid:0) θ ,θi (cid:1)(cid:11)\nsh θ sh S sh sh ns θ sh S sh ns sh θ sh S sh ns\n(cid:20) ∇ Li (cid:0) θ ,θi (cid:1) (cid:21)\n= ∇ Li (cid:0) θ ,θi (cid:1) +ρ (cid:10) θ sh S sh ns ,∇ Li (cid:0) θ ,θi (cid:1)(cid:11)\nθ sh S sh ns sh ∥∇ Li (cid:0) θ ,θi (cid:1) ∥ θ sh S sh ns\nθ sh S sh ns 2\n= ∇ (cid:2) Li (cid:0) θ ,θi (cid:1) +ρ ∥∇ Li (cid:0) θ ,θi (cid:1) ∥ (cid:3) (6)\nθ sh S sh ns sh θ sh S sh ns 2\n(1)\nwhere in ≈, we apply the first-order Taylor expansion and ⟨·,·⟩ represents the dot product.\nIt is obvious that following the negative direction of gi,SAM will minimize the loss Li (cid:0) θ ,θi (cid:1)\nsh S sh ns\nand the gradient norm ∥∇ Li (cid:0) θ ,θi (cid:1) ∥ of task i, hence leading the model to the low-valued\nθ sh S sh ns 2\nregion for the loss of this task and its flatter region with a lower gradient norm magnitude.\n6\nMoreover, inspired from the derivation in Equation (6), we decompose the gradient gi,SAM =\nsh\ngi,loss + gi,flat where we define gi,loss := ∇ Li (cid:0) θ ,θi (cid:1) and gi,flat := gi,SAM − gi,loss. As\nsh sh sh θ sh S sh ns sh sh sh\naforementioned, the purpose of the negative gradient −gi,loss is to orient the model to minimize the\nsh\nloss of the task i, while −gi,flat navigates the model to the task i’s flatter region.\nsh\nTherefore, the SAM gradients gi,SAM,i ∈ [m] constitute two components with different purposes.\nsh\nTo mitigate the possible confliction and interference of the two components when aggregating, we\npropose to aggregate the low-loss components solely and then the flat components solely. Specifically,\nto find a common direction that leads the joint low-valued losses for all tasks and the joint flatter\nregion for them, we first combine the gradients gi,loss,i ∈ [m] and the gradients gi,flat,i ∈ [m], then\nsh sh\nadd the two aggregated gradients, and finally update the shared part as:\ngloss = gradient_aggregate(g1,loss,...,gm,loss),\nsh sh sh\ngflat = gradient_aggregate(g1,flat,...,gm,flat),\nsh sh sh\ngSAM = gloss+gflat; θ = θ −ηgSAM,\nsh sh sh sh sh sh\nFinally, the key steps of our proposed framework are summarized in Algorithm 1 and the overall\nschema of our proposed method is demonstrated in Figure 1.\nAlgorithm 1 Sharpness minimization for multi-task learning\nInput: Model parameter θ = [θ ,θ1:m], perturbation radius ρ = [ρ ,ρ ], step size η and a list of\nsh ns sh ns\nm differentiable loss functions (cid:8) Li(cid:9)m .\ni=1\nOutput: Updated parameter θ∗\n1: for task i ∈ [m] do\n2: Compute gradient gi s, hloss,gi\nns\n← ∇ θLi(θ)\nWorst-case perturbation direction\n3:\n(cid:13) (cid:13)\nϵi\nsh\n= ρ sh·gi s, hloss/(cid:13) (cid:13)gi s, hloss(cid:13)\n(cid:13)\nand ϵi\nns\n= ρ ns·gi ns/(cid:13) (cid:13)gi ns(cid:13) (cid:13)\nApproximate SAM’s gradient\n4:\ngi,SAM = ∇ Li(θ +ϵi ,θi ) and gi,SAM = ∇ Li(θ ,θi +ϵi )\nComputs eh flat gradiθ es nh t sh sh ns ns θi ns sh ns ns\n5:\ngi,flat = gi,SAM−gi,loss\nsh sh sh\n6: end for\nCalculate combined update gradients:\n7:\ngloss = gradient_aggregate(g1,loss,g2,loss,...,gm,loss)\nsh sh sh sh\ngflat = gradient_aggregate(g1,flat,g2,flat,...,gm,flat)\nsh sh sh sh\n8: Calculate shared gradient update gSAM = gloss+gflat\nsh sh sh\nUpdate model parameter\n9:\nθ∗ = [θ ,θ1:m]−η[gSAM,g1:m,SAM]\nsh ns sh ns\n7\n5 Experiments\nIn this section, we compare our proposed method against other state-of-the-art methods of multi-task\nlearning in different scenarios, ranging from image classification to scene understanding problems.\nRefer to the appendix for the detailed settings used for each dataset and additional experiments.\nDatasets and Baselines. OurproposedmethodisevaluatedonfourMTLbenchmarksincluding\nMulti-MNIST [25], CelebA [58] for visual classification, and NYUv2 [59], CityScapes [60] for scene\nunderstanding. We show how our framework can boost the performance of gradient-based MTL\nmethodsbycomparingvanilla MGDA[8],PCGrad[9],CAGrad[10]andIMTL[11]totheirflat-based\nversions F-MGDA, F-PCGrad, F-CAGrad and F-IMTL. We also add single task learning (STL)\nbaseline for each dataset.\n5.1 Image classification\nMulti-MNIST. Following the protocol of [8], we set up three Multi-MNIST experiments with the\nResNet18 [61] backbone, namely: MultiFashion, MultiMNIST and MultiFashion+MNIST. In each\ndataset, two images are sampled uniformly from the MNIST [62] or Fashion-MNIST [63], then one\nis placed on the top left and the other is on the bottom right. We thus obtain a two-task learning\nthat requires predicting the categories of the digits or fashion items on the top left (task 1) and on\nthe bottom right (task 2) respectively.\nTable 1: Evaluation of different methods on three Multi-MNIST datasets. Rows with flat-based\nminimizers are shaded. Bold numbers denote higher accuracy between flat-based methods and their\nbaselines. ∗ denotes the highest accuracy (except for STL, since it unfairly exploits multiple neural\nnetworks). We also use arrows to indicate that the higher is the better (↑) or vice-versa (↓).\nMultiFashion MultiMNIST MultiFashion+MNIST\nMethod\nTask1↑ Task2↑ Average↑ Task1↑ Task2↑ Average↑ Task1↑ Task2 Average↑\nSTL 87.10±0.09 86.20±0.06 86.65±0.02 95.33±0.08 94.16±0.04 94.74±0.06 98.40±0.02 89.42±0.03 93.91±0.02\nMGDA 86.76±0.09 85.78±0.36 86.27±0.22 95.62±0.02 94.49±0.10 95.05±0.06 97.24±0.04 88.19±0.13 92.72±0.07\nF-MGDA 88.12±0.11 87.35±0.11 87.73±0.09 96.37±0.06 94.99±0.06 95.68±0.00 97.30±0.09 89.26±0.14 93.28±0.03\nPCGrad 86.93±0.17 86.20±0.14 86.57±0.12 95.71±0.03 94.41±0.02 95.06±0.02 97.12±0.16 88.45±0.08 92.78±0.11\nF-PCGrad 88.17±0.14 87.35±0.27 87.76±0.07 96.49±0.05 95.34±0.10 95.92±0.07 97.65±0.06 89.35±0.07∗ 93.50±0.01\nCAGrad 86.99±0.17 86.04±0.15 86.51±0.16 95.62±0.05 94.39±0.04 95.01±0.04 97.19±0.06 88.18±0.14 92.68±0.04\nF-CAGrad 88.19±0.19∗ 87.45±0.13 87.82±0.10∗ 96.54±0.02 95.36±0.04∗ 95.95±0.01∗ 97.82±0.05∗ 89.26±0.22 93.54±0.13∗\nIMTL 87.35±0.22 86.45±0.09 86.90±0.15 95.93±0.09 94.63±0.13 95.28±0.02 97.47±0.06 88.46±0.11 92.97±0.03\nF-IMTL 88.1±0.10 87.5±0.04∗ 87.80±0.06 96.55±0.07∗ 95.16±0.05 95.85±0.05 97.59±0.12 88.99±0.08 93.29±0.02\nAs summarized in Table 1, we can see that seeking flatter regions for all tasks can improve the\nperformance of all the baselines across all three datasets. Especially, flat-based methods achieve the\nhighest score for each task and for the average, outperforming STL by 1.2% on MultiFashion and\nMultiMNIST. We conjecture that the discrepancy between gradient update trajectories to classify\ndigits from MNIST and fashion items from FashionMNIST has resulted in the fruitless performance\nof baselines, compared to STL on MultiFashion+MNIST. Even if there exists dissimilarity between\ntasks, our best obtained average accuracy when applying our method to CAGrad is just slightly\nlower than STL (< 0.4%) while employing a single model only.\nInterestingly,ourproposedMTLtrainingmethodalsohelpsimprovemodelcalibrationperformance\nby mitigating the over-confident phenomenon of deep neural networks. As can be seen from Figure\n8\n17.5 Our 3.5 Our\nERM ERM\n15.0 3.0\n12.5 2.5\n10.0 2.0\n7.5 1.5\n5.0 1.0\n2.5 0.5\n0.0 0.0\n0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0\nFigure 2: Entropy distributions of ResNet18 on in-domain set (left) and out-of-domain set (right).\n2, our method produces high-entropy predictions that represent its uncertainty, ERM-based method\noutputs high confident predictions on both in and out-of-domain data. More details about model\ncalibration improvement can be found in the appendix.\nCelebA. CelebA [64] is a face dataset, which consists of 200K Table 2: Mean of error per\ncelebrityfacialphotoswith40attributes. Similarto[8],eachattribute category of MTL algorithms\nforms a binary classification problem, thus a 40-class multi-label in multi-label classification\nclassification problem is constructed. on CelebA dataset.\nTable 2 shows the average errors over 40 tasks of the methods\nwith linear scalarization (LS) and Uncertainty weighting (UW) [32] Method Average error ↓\nbeing included to serve as comparative baselines. The best results STL 8.77\nLS 9.99\nin each pair and among all are highlighted using bold font and ∗,\nUW 9.66\nrespectively. When the number of tasks is large, flat region seeking\nMGDA 9.96\nstillconsistentlyshowsitsadvantagesandthelowestaverageaccuracy\nF-MGDA 9.22\nerror is achieved by F-CAGrad. Interestingly, when the optimizer is\nPCGrad 8.69\naware of flat minima, the gaps between PCGrad, IMTL and CAGrad,\nF-PCGrad 8.23\n(8.23, 8.24 vs 8.22), are smaller than those using conventional ERM\nCAGrad 8.52\ntraining, (8.69, 8.88 and 8.52). This might be due to the better\nF-CAGrad 8.22∗\naggregationoftasks’gradients, whichmeans that theconflictbetween\nIMTL 8.88\nthese gradients is likely to be reduced when the shared parameters\nF-IMTL 8.24\napproach the common flat region of all tasks.\n5.2 Scene Understanding\nTwo datasets used in this sub-section are NYUv2 [59] and CityScapes [60]. NYUv2 is an indoor\nscene dataset that contains 3 tasks: 13-class semantic segmentation, depth estimation, and surface\nnormal prediction. In CityScapes, there are 19 classes of street-view images, which are coarsened\ninto 7 categories to create two tasks: semantic segmentation and depth estimation. For these two\nexperiments, we additionally include several recent MTL methods, namely, scale-invariant (SI),\nrandom loss weighting (RLW), Dynamic Weight Average (DWA) [2], GradDrop [65], and Nash-MTL\n[13] whose results are taken from [13]. Details of each baseline can be found in the appendix. Also\nfollowing the standard protocol used in [2, 10, 13], Multi-Task Attention Network [2] is employed on\ntop of the SegNet architecture [66], our presented results are averaged over the last 10 epochs to\nalign with previous work.\nEvaluation metric. In this experiment, we have to deal with different task types rather than\none only as in the case of image classification. Since each of them has its own set of metrics. We thus\n9\nmark the overall performance of comparative methods by reporting their relative task improvement\n[67] throughout this section. Let M and S be the metrics obtained by the main and the\ni i\nTable 3: Test performance for two-task CityScapes: semantic segmentation and depth estimation.∗\ndenotes the best score for each task’s metrics.\nSegmentation Depth\nMethod mIoU↑ PixAcc↑ AbsErr↓ RelErr↓ ∆m%↓\nSTL 74.01 93.16 0.0125 27.77\nLS 75.18 93.49 0.0155 46.77 22.60\nSI 70.95 91.73 0.0161 33.83 14.11\nRLW 74.57 93.41 0.0158 47.79 24.38\nDWA 75.24 93.52 0.0160 44.37 21.45\nUW 72.02 92.85 0.0140 30.13∗ 5.89\nGradDrop 75.27 93.53 0.0157 47.54 23.73\nNash-MTL 75.41 93.66 0.0129 35.02 6.82\nMGDA 68.84 91.54 0.0309 33.50 44.14\nF-MGDA 73.77 93.12 0.0129 27.44∗ 0.67∗\nPCGrad 75.13 93.48 0.0154 42.07 18.29\nF-PCGrad 75.77 93.67 0.0144 39.60 13.65\nCAGrad 75.16 93.48 0.0141 37.60 11.64\nF-CAGrad 76.02 93.72 0.0134 34.64 7.25\nIMTL 75.33 93.49 0.0135 38.41 11.10\nF-IMTL 76.63∗ 93.76∗ 0.0124∗ 31.17 1.87\nsingle-task learning (STL) model, respectively, the relative task improvement on i-th task is\nmathematically given by: ∆\ni\n:= 100·(−1)li (M i−S i)/S i, where l\ni\n= 1 if a lower value for the i-th\ncriterion is better and 0 otherwise. We depict our results by the average relative task improvement\n∆m% = 1 (cid:80)m ∆ .\nm i=1 i\nCityScapes. In Table 3, the positive effect of seeking flat regions is consistently observed in all\nmetrics and baselines. In particular, the relative improvements of MGDA and IMTL are significantly\nboosted, achieving the highest and second-best ∆m% scores, respectively. The segmentation scores\nofPCGrad,CAGradandIMTLevensurpassSTL.Intriguingly,MGDAbiasestothedepthestimation\nobjective, leading to the predominant performance on that task, similar patterns appear in [11] and\nthe below NYUv2 experiment.\nNYUv2. Table 4 shows each task’s results and the relative improvements over STL of different\nmethods. Generally, the flat-based versions obtain comparable or higher results on most of the\nmetrics, except for MGDA at the segmentation task, in which F-MGDA notably decreases the mIoU\nscore. However, it does significantly help other tasks, which contributes to the overall MGDA’s\nrelative improvement, from 1.38% being worse than STL to 0.33% being higher. Remarkably,\nF-CAGrad and F-IMTL outperform their competitors by large margins across all tasks, resulting in\nthe top two relative improvements, 3.78% and 4.77% .\n5.3 Ablation study\nSo far, our proposed technique has shown state-of-the-art performances under different settings, we\nnow investigate in more detailed how it affects conventional training by inspecting loss surfaces and\nmodel robustness. Similar patterns are observed in other experiments and given in the appendix.\nTask conflict. To empirically confirm that tasks’ gradients are less conflicted when the model\nis driven to the flat regions, we measure the gradient conflict and present the result in Figure 3.\n10\nTable 4: Test performance for three-task NYUv2 of Segnet [66]: semantic segmentation, depth\nestimation, and surface normal. Using the proposed procedure in conjunction with gradient-based\nmulti-task learning methods consistently advances their overall performance.\nSegmentation Depth SurfaceNormal\nAngleDistance↓ Withint◦ ↑ ∆m%↓\nmIoU↑ PixAcc↑ AbsErr↓ RelErr↓\nMean Median 11.25 22.5 30\nSTL 38.30 63.76 0.6754 0.2780 25.01 19.21 30.14 57.20 69.15 0.00\nLS 39.29 65.33 0.5493 0.2263 28.15 23.96 22.09 47.50 61.08 5.59\nSI 38.45 64.27 0.5354 0.2201 27.60 23.37 22.53 48.57 62.32 4.39\nRLW 37.17 63.77 0.5759 0.2410 28.27 24.18 22.26 47.05 60.62 7.78\nDWA 39.11 65.31 0.5510 0.2285 27.61 23.18 24.17 50.18 62.39 3.57\nUW 36.87 63.17 0.5446 0.2260 27.04 22.61 23.54 49.05 63.65 4.05\nGradDrop 39.39 65.12 0.5455 0.2279 27.48 22.96 23.38 49.44 62.87 3.58\nNash-MTL 40.13 65.93 0.5261∗ 0.2171 25.26 20.08 28.4 55.47 68.15 −4.04\nMGDA 30.47 59.90 0.6070 0.2555 24.88 19.45 29.18 56.88 69.36 1.38\nF-MGDA 26.42 58.78 0.6078 0.2353 24.34∗ 18.45∗ 31.64∗ 58.86∗ 70.50∗ −0.33\nPCGrad 38.06 64.64 0.5550 0.2325 27.41 22.80 23.86 49.83 63.14 3.97\nF-PCGrad 40.05 65.42 0.5429 0.2243 27.38 23.00 23.47 49.35 62.74 3.14\nCAGrad 39.79 65.49 0.5486 0.2250 26.31 21.58 25.61 52.36 65.58 0.20\nF-CAGrad 40.93∗ 66.68∗ 0.5285 0.2162 25.43 20.39 27.99 54.82 67.56 −3.78\nIMTL 39.35 65.60 0.5426 0.2256 26.02 21.19 26.2 53.13 66.24 −0.76\nF-IMTL 40.42 65.61 0.5389 0.2121∗ 25.03 19.75 28.90 56.19 68.72 −4.77∗\nWhile the percentage of gradient conflict of ERM increases to more than 50%, ours decreases and\napproaches 0%. This reduction in gradient conflict is also the goal of recent gradient-based MTL\nmethods in mitigating negative transfer between tasks [9, 68, 3].\n60\n50\n40\n30\n20\n10\nOurs ERM\n0\n0 25 50 75 100 125 150 175 200\nEpochs\nFigure 3: Proportion of conflict between per-task gradients (g1,loss · g2,loss < 0) on\nMultiFashion+MNIST dataset.\nModel robustness against noise. To verify that SAM can orient the model to the common\nflat and low-loss region of all tasks, we measure the model performance within a r-radius Euclidean\nball. To be more specific, we perturb parameters of two converged models by ϵ, which lies in a\nr-radius ball and plot the accuracy of the perturbed models of each task as we increase r from 0 to\n11\n)%(\ntcilfnoc\ntneidarG\n1000. At each value of r, 10 different models around the r−radius ball of the converged model are\nsampled.\n100\n80\n60\n40\n20\nOurs ERM\n0 200 400 600 800\nr\nFigure 4: Accuracy within r−radius ball. Solid/dashed lines denote performance on train/test sets,\nrespectively.\nIn Figure 4, the accuracy of the model trained using our method remains at a high level when\nnoise keeps increasing until r = 800. This also gives evidence that our model found a region that\nchanges slowly in loss. By contrast, the naively trained model loses its predictive capabilities as\nsoon as the noise appears and becomes a dummy classifier that attains 10% accuracy in a 10-way\nclassification.\nAggregation strategies comparison. Table 5 provides a comparison between the direct\naggregation on {gi,SAM}m and individual aggregation on {gi,flat}m and {gi,loss}m (our method).\nsh i=1 sh i=1 sh i=1\nTable 5: Two aggregation strategies on CityScapes.\nSegmentation Depth\nMethod mIoU ↑ Pix Acc ↑ Abs Err ↓ Rel Err↓ ∆m% ↓\nERM 68.84 91.54 0.0309 33.50 44.14\nOurs (direct) 68.93 91.41 0.0130 31.37 6.43\nOurs (individual) 73.77 93.12 0.0129 27.44∗ 0.67\nCompared to the naive approach, in which per-task SAM gradients are directly aggregated, our\ndecomposition approach consistently improves performance by a large margin across all tasks. This\nresult reinforces the rationale behind separately aggregating low-loss directions and flat directions.\nVisualization of the loss landscapes. Following [69], we plot the loss surfaces at convergence\nafter training Resnet18 from scratch on the MultiMNIST dataset. Test loss surfaces of checkpoints\nthat have the highest validation accuracy scores are shown in Figure 5.\nWe can clearly see that the solution found by our proposed method not only mitigates the test\nloss sharpness for both tasks but also can intentionally reduce the test loss value itself, in comparison\nto traditional ERM. This is a common behavior when using flat minimizers as the gap between train\nand test performance has been narrowed [44, 14].\n12\nycaruccA\nFigure 5: Visualization of test loss surfaces with standard ERM training and when applying our\nmethod. The coordinate plane axes are two random sampled orthogonal Gaussian perturbations.\n6 Conclusion\nIn this work, we have presented a general framework that can be incorporated into current multi-task\nlearning methods following the gradient balancing mechanism. The core ideas of our proposed\nmethod are the employment of flat minimizers in the context of MTL and proving that they can help\nenhance previous works both theoretically and empirically. Concretely, our method goes beyond\noptimizingper-taskobjectivessolelytoyieldmodelsthathavebothlowerrorsandhighgeneralization\ncapabilities. On the experimental side, the efficacy of our method is demonstrated on a wide range\nof commonly used MTL benchmarks, in which ours consistently outperforms comparative methods.\nReferences\n[1] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection by deep multi-task learning,” in\nEuropean conference on computer vision, pp. 94–108, Springer, 2014.\n[2] S. Liu, E. Johns, and A. J. Davison, “End-to-end multi-task learning with attention,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pp. 1871–1880, 2019.\n[3] Z. Wang, Y. Tsvetkov, O. Firat, and Y. Cao, “Gradient vaccine: Investigating and improving multi-task\noptimizationinmassivelymultilingualmodels,” inInternational Conference on Learning Representations,\n2020.\n[4] I. Kokkinos, “Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level\nvision using diverse datasets and limited memory,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 6129–6138, 2017.\n[5] F. Heuer, S. Mantowsky, S. Bukhari, and G. Schneider, “Multitask-centernet (mcn): Efficient and diverse\nmultitask learning using an anchor free approach,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 997–1005, 2021.\n[6] Y. Gao, J. Ma, M. Zhao, W. Liu, and A. L. Yuille, “Nddr-cnn: Layerwise feature fusing in multi-task\ncnns by neural discriminative dimensionality reduction,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 3205–3214, 2019.\n[7] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard, “Latent multi-task architecture learning,” in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 4822–4829, 2019.\n[8] O. Sener and V. Koltun, “Multi-task learning as multi-objective optimization,” Advances in neural\ninformation processing systems, vol. 31, 2018.\n13\n[9] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, “Gradient surgery for multi-task\nlearning,” Advances in Neural Information Processing Systems, vol. 33, pp. 5824–5836, 2020.\n[10] B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, “Conflict-averse gradient descent for multi-task learning,”\nAdvances in Neural Information Processing Systems, vol. 34, pp. 18878–18890, 2021.\n[11] L. Liu, Y. Li, Z. Kuang, J.-H. Xue, Y. Chen, W. Yang, Q. Liao, and W. Zhang, “Towards impartial\nmulti-task learning,” in International Conference on Learning Representations, 2020.\n[12] A. Javaloy and I. Valera, “Rotograd: Gradient homogenization in multitask learning,” in International\nConference on Learning Representations, 2021.\n[13] A. Navon, A. Shamsian, I. Achituve, H. Maron, K. Kawaguchi, G. Chechik, and E. Fetaya, “Multi-task\nlearning as a bargaining game,” arXiv preprint arXiv:2202.01017, 2022.\n[14] J.Kaddour,L.Liu,R.Silva,andM.J.Kusner,“Afaircomparisonoftwopopularflatminimaoptimizers:\nStochastic weight averaging vs. sharpness-aware minimization,” arXiv preprint arXiv:2202.00661, vol. 1,\n2022.\n[15] Y. Zhao, H. Zhang, and X. Hu, “Penalizing gradient norm for efficiently improving generalization in\ndeep learning,” arXiv preprint arXiv:2202.03599, 2022.\n[16] N. S. Keskar, J. Nocedal, P. T. P. Tang, D. Mudigere, and M. Smelyanskiy, “On large-batch training\nfor deep learning: Generalization gap and sharp minima,” in 5th International Conference on Learning\nRepresentations, ICLR 2017, 2017.\n[17] Z. Li, Z. Wang, and J. Li, “Analyzing sharpness along gd trajectory: Progressive sharpening and edge of\nstability,” arXiv preprint arXiv:2207.12678, 2022.\n[18] K.Lyu,Z.Li,andS.Arora,“Understandingthegeneralizationbenefitofnormalizationlayers: Sharpness\nreduction,” arXiv preprint arXiv:2206.07085, 2022.\n[19] H. He, G. Huang, and Y. Yuan, “Asymmetric valleys: Beyond sharp and flat local minima,” Advances in\nneural information processing systems, vol. 32, 2019.\n[20] Y. Zheng, R. Zhang, and Y. Mao, “Regularizing neural networks via adversarial model perturbation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8156–8165,\n2021.\n[21] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware minimization for efficiently\nimproving generalization,” in International Conference on Learning Representations, 2021.\n[22] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, pp. 41–75, 1997.\n[23] X.Liu,P.He,W.Chen,andJ.Gao,“Multi-taskdeepneuralnetworksfornaturallanguageunderstanding,”\nin Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4487–\n4496, Association for Computational Linguistics, 2019.\n[24] S.Ruder, “Anoverviewofmulti-tasklearningindeepneuralnetworks,” arXiv preprint arXiv:1706.05098,\n2017.\n[25] X. Lin, H.-L. Zhen, Z. Li, Q.-F. Zhang, and S. Kwong, “Pareto multi-task learning,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[26] X. Liu, X. Tong, and Q. Liu, “Profiling pareto front with multi-objective stein variational gradient\ndescent,” Advances in Neural Information Processing Systems, vol. 34, pp. 14721–14733, 2021.\n[27] D.MahapatraandV.Rajan,“Multi-tasklearningwithuserpreferences: Gradientdescentwithcontrolled\nascentinparetooptimization,” inInternational Conference on Machine Learning, pp.6597–6607, PMLR,\n2020.\n14\n[28] D. Mahapatra and V. Rajan, “Exact pareto optimal search for multi-task learning: Touring the pareto\nfront,” arXiv preprint arXiv:2108.00597, 2021.\n[29] X. Lin, Z. Yang, Q. Zhang, and S. Kwong, “Controllable pareto multi-task learning,” arXiv preprint\narXiv:2010.06313, 2020.\n[30] A. Navon, A. Shamsian, G. Chechik, and E. Fetaya, “Learning the pareto front with hypernetworks,” in\nInternational Conference on Learning Representations, 2021.\n[31] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, “Gradnorm: Gradient normalization for\nadaptive loss balancing in deep multitask networks,” in International conference on machine learning,\npp. 794–803, PMLR, 2018.\n[32] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using uncertainty to weigh losses for scene\ngeometry and semantics,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 7482–7491, 2018.\n[33] M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-Fei, “Dynamic task prioritization for multitask\nlearning,” in Proceedings of the European conference on computer vision (ECCV), pp. 270–287, 2018.\n[34] Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio, “Fantastic generalization measures and\nwhere to find them,” in ICLR, OpenReview.net, 2020.\n[35] H. Petzka, M. Kamp, L. Adilova, C. Sminchisescu, and M. Boley, “Relative flatness and generalization,”\nin NeurIPS, pp. 18420–18432, 2021.\n[36] G. K. Dziugaite and D. M. Roy, “Computing nonvacuous generalization bounds for deep (stochastic)\nneural networks with many more parameters than training data,” in UAI, AUAI Press, 2017.\n[37] S. Hochreiter and J. Schmidhuber, “Simplifying neural nets by discovering flat minima,” in NIPS,\npp. 529–536, MIT Press, 1994.\n[38] B.Neyshabur,S.Bhojanapalli,D.McAllester,andN.Srebro,“Exploringgeneralizationindeeplearning,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[39] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio, “Sharp minima can generalize for deep nets,” in\nInternational Conference on Machine Learning, pp. 1019–1028, PMLR, 2017.\n[40] S. Fort and S. Ganguli, “Emergent properties of the local geometry of neural loss landscapes,” arXiv\npreprint arXiv:1910.05929, 2019.\n[41] G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. E. Hinton, “Regularizing neural networks by\npenalizing confident output distributions,” in ICLR (Workshop), OpenReview.net, 2017.\n[42] P. Chaudhari, A. Choromańska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. T. Chayes, L. Sagun, and\nR. Zecchina, “Entropy-sgd: biasing gradient descent into wide valleys,” Journal of Statistical Mechanics:\nTheory and Experiment, vol. 2019, 2017.\n[43] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On large-batch training for\ndeep learning: Generalization gap and sharp minima,” in ICLR, OpenReview.net, 2017.\n[44] P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson, “Averaging weights leads to\nwider optima and better generalization,” in UAI, pp. 876–885, AUAI Press, 2018.\n[45] S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, Y. Bengio, and A. J. Storkey, “Three factors\ninfluencing minima in sgd,” ArXiv, vol. abs/1711.04623, 2017.\n[46] C.Wei,S.Kakade,andT.Ma,“Theimplicitandexplicitregularizationeffectsofdropout,” inInternational\nconference on machine learning, pp. 10181–10192, PMLR, 2020.\n[47] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual learning,” 2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 4320–4328, 2018.\n15\n[48] L.Zhang, J.Song, A.Gao, J.Chen, C.Bao, andK.Ma, “Beyourownteacher: Improvetheperformance\nof convolutional neural networks via self distillation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 3713–3722, 2019.\n[49] J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y. Lee, and S. Park, “Swad: Domain generalization by\nseeking flat minima,” Advances in Neural Information Processing Systems, vol. 34, pp. 22405–22418,\n2021.\n[50] M. Abbas, Q. Xiao, L. Chen, P.-Y. Chen, and T. Chen, “Sharp-maml: Sharpness-aware model-agnostic\nmeta learning,” arXiv preprint arXiv:2206.03996, 2022.\n[51] Z. Qu, X. Li, R. Duan, Y. Liu, B. Tang, and Z. Lu, “Generalized federated learning via sharpness aware\nminimization,” arXiv preprint arXiv:2206.02618, 2022.\n[52] D. Caldarola, B. Caputo, and M. Ciccone, “Improving generalization in federated learning by seeking\nflat minima,” in European Conference on Computer Vision, pp. 654–672, Springer, 2022.\n[53] D.Bahri,H.Mobahi,andY.Tay,“Sharpness-awareminimizationimproveslanguagemodelgeneralization,”\nin Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), (Dublin, Ireland), pp. 7360–7371, Association for Computational Linguistics, May 2022.\n[54] X. Chen, C.-J. Hsieh, and B. Gong, “When vision transformers outperform resnets without pre-training\nor strong data augmentations,” arXiv preprint arXiv:2106.01548, 2021.\n[55] V.-A. Nguyen, T.-L. Vuong, H. Phan, T.-T. Do, D. Phung, and T. Le, “Flat seeking bayesian neural\nnetworks,” Advances in Neural Information Processing Systems, 2023.\n[56] D. A. McAllester, “Pac-bayesian model averaging,” in Proceedings of the twelfth annual conference on\nComputational learning theory, pp. 164–170, 1999.\n[57] P. Alquier, J. Ridgway, and N. Chopin, “On the properties of variational approximations of gibbs\nposteriors,” Journal of Machine Learning Research, vol. 17, no. 236, pp. 1–41, 2016.\n[58] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of the\nIEEE international conference on computer vision, pp. 3730–3738, 2015.\n[59] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation and support inference from rgbd\nimages,” in European conference on computer vision, pp. 746–760, Springer, 2012.\n[60] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3213–3223, 2016.\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n[62] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document\nrecognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.\n[63] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine\nlearning algorithms,” arXiv preprint arXiv:1708.07747, 2017.\n[64] Z. Liu, P. Luo, X. Wang, and X. Tang, “Large-scale celebfaces attributes (celeba) dataset,” Retrieved\nAugust, vol. 15, no. 2018, p. 11, 2018.\n[65] Z. Chen, J. Ngiam, Y. Huang, T. Luong, H. Kretzschmar, Y. Chai, and D. Anguelov, “Just pick a\nsign: Optimizing deep multitask models with gradient sign dropout,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 2039–2050, 2020.\n[66] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-decoder\narchitecture for image segmentation,” IEEE transactions on pattern analysis and machine intelligence,\nvol. 39, no. 12, pp. 2481–2495, 2017.\n16\n[67] K.-K.Maninis,I.Radosavovic,andI.Kokkinos,“Attentivesingle-taskingofmultipletasks,” inProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1851–1860, 2019.\n[68] S. Zhu, H. Zhao, P. Wang, H. Deng, J. Xu, and B. Zheng, “Gradient deconfliction via orthogonal\nprojections onto subspaces for multi-task learning,” 2022.\n[69] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the loss landscape of neural nets,”\nAdvances in neural information processing systems, vol. 31, 2018.\n[70] B. Laurent and P. Massart, “Adaptive estimation of a quadratic functional by model selection,” Annals\nof Statistics, pp. 1302–1338, 2000.\n[71] J.-A. Désidéri, “Multiple-gradient descent algorithm (mgda) for multiobjective optimization,” Comptes\nRendus Mathematique, vol. 350, no. 5-6, pp. 313–318, 2012.\n[72] B. Lin, F. Ye, Y. Zhang, and I. W. Tsang, “Reasonable effectiveness of random weighting: A litmus test\nfor multi-task learning,” arXiv preprint arXiv:2111.10603, 2021.\n[73] D.P.KingmaandJ.Ba,“Adam: Amethodforstochasticoptimization,” arXiv preprint arXiv:1412.6980,\n2014.\n[74] J. Kwon, J. Kim, H. Park, and I. K. Choi, “Asam: Adaptive sharpness-aware minimization for scale-\ninvariant learning of deep neural networks,” 18–24 Jul 2021.\n[75] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in\nInternational conference on machine learning, pp. 1321–1330, PMLR, 2017.\n[76] G. W. Brier et al., “Verification of forecasts expressed in terms of probability,” Monthly weather review,\nvol. 78, no. 1, pp. 1–3, 1950.\n[77] M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtaining well calibrated probabilities using bayesian\nbinning,” in Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.\n[78] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon, B. Lakshminarayanan, and\nJ. Snoek, “Can you trust your model’s uncertainty? evaluating predictive uncertainty under dataset\nshift,” Advances in neural information processing systems, vol. 32, 2019.\n[79] B.Lakshminarayanan,A.Pritzel,andC.Blundell,“Simpleandscalablepredictiveuncertaintyestimation\nusing deep ensembles,” Advances in neural information processing systems, vol. 30, 2017.\n[80] A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,” Advances in neural\ninformation processing systems, vol. 31, 2018.\n17\nSupplement to “Improving Multi-task Learning\nvia Seeking Task-based Flat Regions\"\nDue to space constraints, some details were omitted from the main paper. We therefore include\nadditional theoretical developments (section A) and experimental results (section C) in this appendix.\nA Our Theory Development\nThis section contains the proofs and derivations of our theory development to support the main\nsubmission.\nWe first start with the following theorem, which is inspired by the general PAC-Bayes in [57].\nTheorem 2. With the assumption that adding Gaussian perturbation will raise the test error:\nL (θ) ≤ E [L (θ+ϵ)]. Let T be the number of parameter θ, and N be the cardinality of\nD ϵ∼N(0,σ2I) D\nS, then the following inequality is true with the probability 1−δ:\n(cid:34) (cid:35)\n1 1 T (cid:16) ||θ||2(cid:17) 1 L2\nL (θ) ≤ E [L (θ+ϵ)]+ √ + log 1+ +log +6log(N +T)+\nD ϵ∼N(0,σ2I) S N 2 2 Tσ2 δ 8\nwhere L is the upper-bound of the loss function.\nProof. We use the PAC-Bayes theory for P = N(0,σ2I ) and Q = N(θ,σ2I ) are the prior and\nP T T\nposterior distributions, respectively.\nBy using the bound in [57], with probability at least 1−δ and for all β > 0, we have:\n1(cid:104) 1 (cid:105)\nE [L (θ)] ≤ E [L (θ)]+ KL(Q∥P)+log +Ψ(β,N) ,\nθ∼Q D θ∼Q S\nβ δ\nwhere we have defined:\n(cid:104) (cid:110) (cid:111)(cid:105)\nΨ(β,N) = logE E exp β(cid:0) L (θ)−L (θ)(cid:1)\nP S D S\nNote that the loss function is bounded by L, according to Hoeffding’s lemma, we have:\nβ2L2\nΨ(β,N) ≤ .\n8N\nBy Cauchy inequality:\n(cid:34) (cid:35) (cid:114)\n1 T (cid:16) ||θ||2(cid:17) L2 L (cid:16) ||θ||2(cid:17)\n√ log 1+ + ≥ √ T log 1+ ≥ L,\nN 2 Tσ2 8 2 N Tσ2\nwhich means that the theorem is proved since the loss function is upper bounded by L, following\nassumptions.\n(cid:104) (cid:105)\nNow, we only need to prove the theorem under the case: ||θ||2 ≤ Tσ2 exp 4N −1 .\nT\nWe need to specify P in advance since it is a prior distribution. However, we do not know in\nadvance the value of θ that affects the KL divergence term. Hence, we build a family of distribution\nP as follows:\nP = (cid:110) P = N(0,σ2 I ) : σ2 = cexp(cid:0)1−j(cid:1) ,c = σ2(cid:0) 1+exp 4N(cid:1) ,j = 1,2,...(cid:111) .\nj Pj T Pj T T\n18\nSet δ = 6δ , the below inequality holds with probability at least 1−δ :\nj π2j2 j\n1(cid:104) 1 β2L2(cid:105)\nE [L (θ)] ≤ E [L (θ)]+ KL(Q∥P )+log + .\nθ∼Q D θ∼Q S j\nβ δ 8N\nj\nOr it can be written as:\n1(cid:104) 1 β2L2(cid:105)\nE [L (θ+ϵ)] ≤ E [L (θ+ϵ)]+ KL(Q∥P )+log + .\nϵ∼N(0,σ2I) D ϵ∼N(0,σ2I) S\nβ\nj\nδ 8N\nj\nThus, with probability 1−δ the above inequalities hold for all P . We choose:\nj\n(cid:36) (cid:32) σ2(cid:0) 1+exp{4N/T}(cid:1)(cid:33)(cid:37)\nj∗ = 1+T log .\nσ2+∥θ∥2/T\nSince ∥θ∥2 ≤ σ2(cid:2) exp 4N −1(cid:3), we get σ2+ ∥θ∥2 ≤ σ2exp 4N, thus j∗ is well-defined. We also have:\nT T T T\nc c\nT log ≤ j∗ ≤ 1+T log\nσ2+∥θ∥2/T σ2+∥θ∥2/T\nc j∗ 1 c\n⇒ log ≤ ≤ +log\nσ2+∥θ∥2/T T T σ2+∥θ∥2/T\n1 σ2+∥θ∥2/T −j∗ σ2+∥θ∥2/T\n⇒ − +log ≤ ≤ log\nT c T c\nσ2+∥θ∥2/T σ2+∥θ∥2/T\n⇒ e−1/T ≤ e−j∗/T ≤\nc c\n⇒ σ2+ ∥θ∥2 ≤ ce1− Tj∗ ≤ eT1(cid:16) σ2+ ∥θ∥2(cid:17)\nT T\n∥θ∥2 (cid:16) ∥θ∥2(cid:17)\n⇒ σ2+\nT\n≤ σ P2\nj∗\n≤ eT1 σ2+\nT\n.\nHence, we have:\n1(cid:104)Tσ2+∥θ∥2 σ2 (cid:105)\nPj∗\nKL(Q∥P ) = −T +T log\nj∗ 2 σ2 σ2\nP j∗\n1(cid:104) Tσ2+∥θ∥2\ne1/T(cid:0) σ2+∥θ∥2/T(cid:1)\n(cid:105)\n≤ −T +T log\n2 σ2+∥θ∥2/T σ2\n1(cid:104)\n(cid:0)\n∥θ∥2 (cid:1)(cid:105)\n≤ 1+T log 1+ .\n2 Tσ2\n19\nFor the term log 1 , use the inequality log(1+et) ≤ 1+t for t > 0:\nδ j∗\n1 (j∗)2π2 1 (cid:16)π2(cid:17)\nlog = log = log +log +2log(j∗)\nδ 6δ δ 6\nj∗\n1 π2 (cid:16)\nσ2(cid:0) 1+exp(4N/T)(cid:1)\n(cid:17)\n≤ log +log +2log 1+T log\nδ 6 σ2+∥θ∥2/T\n1 π2 (cid:16) (cid:0) (cid:1)(cid:17)\n≤ log +log +2log 1+T log 1+exp(4N/T)\nδ 6\n1 π2 (cid:16) (cid:0) 4N(cid:1)(cid:17)\n≤ log +log +2log 1+T 1+\nδ 6 T\n1 π2\n≤ log +log +log(1+T +4N).\nδ 6\n√\nChoosing β = N, with probability at least 1−δ we get:\n1(cid:104) 1 β2L2(cid:105)\nKL(Q∥P )+log +\nj∗\nβ δ 8N\nj∗\n1 (cid:104)1 T (cid:16) ∥θ∥2(cid:17) 1 (cid:105) L2\n≤ √ + log 1+ +log +6log(N +T) + √ .\nN 2 2 Tσ2 δ 8 N\nThus the theorem is proved.\nBack to our context of multi-task learning in which we have m tasks with each task model:\nθi = [θ ,θi ], we can prove the following theorem.\nsh ns\nTheorem 3. With the assumption that adding Gaussian perturbation will rise the test error:\nL (θi) ≤ E (cid:2) L (θi+ϵ)(cid:3) . Let T be the number of parameter θi and N be the cardinality\nD ϵ∼N(0,σ2I) D i\nof S. We have the following inequality holds with probability 1−δ (over the choice of training set\nS ∼ D):\n(cid:2) Li (cid:0) θi(cid:1)(cid:3)m ≤ (cid:2)E (cid:2) L (θi+ϵ)(cid:3) +fi(cid:0) ∥θi∥2(cid:1)(cid:3)m , (7)\nD i=1 ϵ∼N(0,σ2I) S 2 i=1\nwhere\n(cid:34) (cid:35)\nfi(cid:0) ∥θi∥2(cid:1)\n=\n√1 1\n+\nT i log(cid:16)\n1+\n||θ||2(cid:17)\n+log\n1\n+6log(N +T )+\nL2\n.\n2 N 2 2 T iσ2 δ i 8\nProof. The result for the base case m = 1 can be achieved by using Theorem 2 where ξ = δ and\nf1 is defined accordingly. We proceed by induction, suppose that Theorem 3 is true for all i ∈ [n]\nwith probability 1−δ/2, which also means:\n(cid:2) Li (cid:0) θi(cid:1)(cid:3)n ≤ (cid:2)E (cid:2) L (θi+ϵ)(cid:3) +fi(cid:0) ∥θi∥2(cid:1)(cid:3)n .\nD i=1 ϵ∼N(0,σI) S 2 i=1\nUsing Theorem 2 for θn+1 and ξ = δ/2, with probability 1−δ/2, we have:\nLn+1(cid:0) θn+1(cid:1) ≤ E (cid:2) L (θn+1+ϵ)(cid:3) +fn+1(cid:0) ∥θn+1∥2(cid:1) .\nD ϵ∼N(0,σI) S 2\nUsing the inclusion–exclusion principle, with probability at least 1−δ, we reach the conclusion\nfor m = n+1.\nWe next prove the result in the main paper. Let us begin by formally restating the main theorem\nas follows:\n20\nTheorem 4. For any perturbation radius ρ ,ρ > 0, with probability 1−δ (over the choice of\nsh ns\ntraining set S ∼ D) we obtain:\n(cid:34) (cid:35)m\n(cid:2) Li (cid:0) θi(cid:1)(cid:3)m ≤ max max Li (cid:0) θ +ϵ ,θi +ϵi (cid:1) +fi(cid:0) ∥θi∥2(cid:1) , (8)\nD i=1 S sh sh ns ns 2\n∥ϵ sh∥2≤ρ sh ∥ϵi ns∥2≤ρns\ni=1\nwhere\nfi(cid:0) ∥θi∥2(cid:1)\nis defined the same as in Theorem 3.\n2\nProof. Theorem 3 gives us\n(cid:104) (cid:16) (cid:17)(cid:105)m (cid:104) (cid:104) (cid:16) (cid:17)(cid:105) (cid:16) (cid:17)(cid:105)m\nLi θi ≤ E Li θi+ϵ +fi ∥θi∥\nD ϵ∼N(0,σ2I) S 2\ni=1 i=1\n(cid:20)(cid:90) (cid:104) (cid:16) (cid:17)(cid:105) (cid:16) (cid:17)(cid:21)m\n= E Li θ +ϵ ,θi +ϵi p(ϵ )dϵ +fi ∥θi∥\nϵi\nns\nS sh sh ns ns sh sh 2\ni=1\n(cid:104) (cid:104) (cid:16) (cid:17)(cid:105) (cid:16) (cid:17)(cid:105)m\n=E E Li θ +ϵ ,θi +ϵi +fi ∥θi∥ ,\nϵsh ϵi\nns\nS sh sh ns ns 2\ni=1\nwhere p(ϵ ) is the density function of Gaussian distribution; ϵ and ϵi are drawn from their\nsh sh ns\ncorresponding Gaussian distributions.\nWe have ϵi ∼ N(0,σ2I ) with the dimension T , therefore ∥ϵi ∥ follows the Chi-square\nns ns i,ns ns\ndistribution. As proven in [70], we have for all i:\nP (cid:16) ∥ϵi ∥2 ≥ T σ2+2σ2(cid:112) T t+2tσ2(cid:17) ≤ e−t,∀t > 0\nns 2 i,ns i,ns\nP (cid:16) ∥ϵi ∥2 < T σ2+2σ2(cid:112) T t+2tσ2(cid:17) > 1−e−t\nns 2 i,ns i,ns\nfor all t > 0.\n√\nSelect t = ln( N), we derive the following bound for the noise magnitude in terms of the\nperturbation radius ρ for all i:\nns\n(cid:18) √ (cid:113) √ (cid:19) 1\nP ∥ϵi ∥2 ≤ σ2(2ln( N)+T +2 T ln( N)) > 1− √ . (9)\nns 2 i,ns i,ns\nN\nMoreover, we have ϵ ∼ N(0,σ2I ) with the dimension T , therefore ∥ϵ ∥ follows the Chi-\nsh sh sh sh\nsquare distribution. As proven in [70], we have:\n(cid:16) (cid:112) (cid:17)\nP ∥ϵ ∥2 ≥ T σ2+2σ2 T t+2tσ2 ≤ e−t,∀t > 0\nsh 2 sh sh\n(cid:16) (cid:112) (cid:17)\nP ∥ϵ ∥2 < T σ2+2σ2 T t+2tσ2 > 1−e−t\nsh 2 sh sh\nfor all t > 0.\n√\nSelect t = ln( N), we derive the following bound for the noise magnitude in terms of the\nperturbation radius ρ :\nsh\n(cid:18) √ (cid:113) √ (cid:19) 1\nP ∥ϵ ∥2 ≤ σ2(2ln( N)+T +2 T ln( N)) > 1− √ . (10)\nsh 2 sh sh\nN\n21\nBy choosing σ less than (cid:113) ρ sh √ and min i (cid:113) ρns √ , and\n2lnN1/2+T sh+2 T shlnN1/2 2lnN1/2+Ti,ns+2 Ti,nslnN1/2\nreferring to (9,10), we achieve both:\nP (cid:0) ∥ϵi ∥ < ρ (cid:1) > 1− 1 ,∀i,\nns ns N1/2\n1\nP (∥ϵ ∥ < ρ ) > 1− .\nsh sh N1/2\nFinally, we finish the proof as:\n(cid:2) Li (cid:0) θi(cid:1)(cid:3)m ≤ E (cid:2)E (cid:2) Li (cid:0) θ +ϵ ,θi +ϵi (cid:1)(cid:3) +fi(cid:0) ∥θi∥ (cid:1)(cid:3)m\nD i=1 ϵ sh ϵi ns S sh sh ns ns 2 i=1\n(cid:20) (cid:21)m\n≤ max\n||ϵ sh||<ρ\nsh\nmax\n||ϵi\nns||<ρnsLi S(cid:0) θ sh+ϵ sh,θi ns+ϵi ns(cid:1) + √2\nN\n− N1 +fi(cid:0) ∥θi∥ 2(cid:1)\ni=1\nTo reach the final conclusion, we redefine:\nfi(cid:0) ∥θi∥ (cid:1) = √2 − 1 +fi(cid:0) ∥θi∥ (cid:1) .\n2 2\nN N\nHere we note that we reach the final inequality due to the following derivations:\nE (cid:2)E (cid:2) Li (cid:0) θ +ϵ ,θi +ϵi (cid:1)(cid:3)(cid:3)m\nϵ sh ϵi ns S sh sh ns ns i=1\n(cid:34) (cid:35)m\n(cid:90) (cid:90)\n≤ Li (cid:0) θ +ϵ ,θi +ϵi (cid:1) dϵi + √1 dϵ\nS sh sh ns ns ns sh\nB Bi N\nsh ns i=1\n(cid:34) (cid:35)m\n(cid:90) (cid:90)\n+ Li (cid:0) θ +ϵ ,θi +ϵi (cid:1) dϵi + √1 dϵ\nS sh sh ns ns ns sh\nBc Bi N\nsh ns i=1\n(cid:90)\n(cid:34)\n(cid:90)\n(cid:35)m\n(cid:18) (cid:19)\n≤ Li (cid:0) θ +ϵ ,θi +ϵi (cid:1) dϵi dϵ + 1− √1 √1 + √1\nS sh sh ns ns ns sh\nB Bi N N N\nsh ns i=1\n≤ max\n(cid:20)\nmax (cid:2) Li (cid:0) θ +ϵ ,θi +ϵi\n(cid:1)(cid:3)(cid:21)m\n+√2 − 1 ,\n||ϵ sh||<ρ sh ||ϵi ns||<ρns S sh sh ns ns N N\ni=1\nwhere B = {ϵ : ||ϵ || ≤ ρ }, Bc is the compliment set, and Bi = (cid:8) ϵi : ||ϵi || ≤ ρ (cid:9).\nsh sh sh sh sh ns ns ns ns\nB Gradient aggregation strategies overview\nThis section details how the gradient_aggregate operation is defined according to recent gradient-\nbasedmulti-tasklearningmethodsthatweemployedasbaselinesinthemainpaper, includingMGDA\n[8], PCGrad [9], CAGrad [10] and IMTL [11]. Assume that we are given m vectors g1,g2,...,gm\nrepresent task gradients. Typically, we aim to find a combined gradient vector as:\ng = gradient_aggregate(g1,g2,...,gm)\n.\n22\nB.1 Multiple-gradient descent algorithm - MGDA\n[8] apply MGDA [71] to find the minimum-norm gradient vector that lies in the convex hull composed\nby task gradients g1,g2,...,gm:\nm m\n(cid:88) (cid:88)\ng = argmin|| w gi||2,s.t. w = 1 and ,w ≥ 0∀i.\ni i i\ni=1 i=1\nThis approach can guarantee that the obtained solutions lie on the Pareto front of task objective\nfunctions.\nB.2 Projecting conflicting gradients - PCGrad\nPCgrad resolves the disagreement between tasks by projecting gradients that conflict with each\nother, i.e. ⟨gi,gj⟩ < 0, to the orthogonal direction of each other. Specifically, gi is replaced by its\nprojection on the normal plane of gj:\ngi·gj\ngi = gi− gj.\nPC ||gj||2\nThen compute the aggregated gradient based on these deconflict vectors g = (cid:80)mgi .\ni PC\nB.3 Conflict Averse Gradient Descent - CAGrad\nCAGrad [10] seeks a worst-case direction in a local ball around the average gradient of all tasks, g ,\n0\nthat minimizes conflict with all of the gradients. The updated vector is obtained by optimizing the\nfollowing problem:\nmax min⟨gi,g⟩ s.t. ||g−g0|| ≤ c|||g0|,\ng∈R i∈[m]\nwhere g0 = 1 (cid:80)mgi is the averaged gradient and c is a hyper-parameter.\nm i\nB.4 Impartial multi-task learning - IMTL\nIMTL[11]proposestobalanceper-taskgradientsbyfindingthecombinedvectorg, whoseprojections\nonto {gi}m are equal. Following this, they obtain the closed-form solution for the simplex vector\ni=1\nw for reweighting task gradients:\n(cid:16) (cid:17)−1\nw = g1U⊤ DU⊤\nwhereui = gi/(cid:13) (cid:13)gi(cid:13) (cid:13),U = (cid:2) u1−u2,··· ,u1−um(cid:3),andD = (cid:2) g1−g2,··· ,g1−gm(cid:3)Theaggregated\nvector is then calculated as g = (cid:80)mw gi.\ni i\nC Implementation Details\nIn this part, we provide implementation details regarding the empirical evaluation in the main paper\nalong with additional comparison experiments.\n23\nC.1 Baselines\nIn this subsection, we briefly introduce some of the comparative methods that appeared in the main\ntext:\n• Linear scalarization (LS) minimizes the unweighted sum of task objectives (cid:80)mLi(θ).\ni\n• Scale-invariant (SI) aims toward obtaining similar convergent solutions even if losses are scaled\nwith different coefficients via minimizing (cid:80)mlogLi(θ).\ni\n• Random loss weighting (RLW) [72] is a simple yet effective method for balancing task losses or\ngradients by random weights.\n• Dynamic Weight Average (DWA) [2] simply adjusts the weighting coefficients by taking the\nrate of change of loss for each task into account.\n• GradDrop [65] presents a probabilistic masking process that algorithmically eliminates all\ngradient values having the opposite sign w.r.t a predefined direction.\nC.2 Image classification\nNetwork Architectures. For two datasets in this problem, Multi-MNIST and CelebA, we replicate\nexperiments from [8, 25] by respectively using the Resnet18 (11M parameters) and Resnet50 (23M\nparameters) [61] with the last output layer removed as the shared encoders and constructing linear\nclassifiers as the task-specific heads, i.e. 2 heads for Multi-MNIST and 40 for CelebA, respectively.\nTraining Details. We train the all the models under our proposed framework and baselines\nusing:\n• Multi-MNIST: Adam optimizer [73] with a learning rate of 0.001 for 200 epochs using a batch\nsize of 256. Images from the three datasets are resized to 36×36.\n• CelebA: Batch-size of 256 and images are resized to 64×64×3. Adam [73] is used again with\na learning rate of 0.0005, which is decayed by 0.85 for every 10 epochs, our model is trained\nfor 50 epochs in total.\nRegarding the hyperparameter for SAM [21], we use their adaptive version [74] where both ρ\nsh\nand ρ are set equally and extensively tuned from 0.005 to 5.\nns\nC.3 Scene understanding\nTwo datasets used in this problem are NYUv2 and CityScapes. Similar to [13], all images in the\nNYUv2 dataset are resized to 288×384 while all images in the CityScapes dataset are resized to\n128×256 to speed up the training process. We follow the exact protocol in [13] for implementation.\nSpecifically, SegNet [66] is adopted as the architecture for the backbone and Multi-Task Attention\nNetwork MTAN [2] is applied on top of it. We train each method for 200 epochs using Adam\noptimizer [73] with an initial learning rate of 1e−4 and reduced it to 5e−5 after 100 epochs. We\nuse a batch size of 2 for NYUv2 and 8 for CityScapes. The last 10 epochs are averaged to get the\nfinal results, and all experiments are run with three random seeds.\n24\nD Additional Results\nTo further show the improvement of our proposed training framework over the conventional one, this\nsection provides additional comparison results in terms of qualitative results, predictive performance,\nconvergent behavior, loss landscape, model sharpness, and gradient norm. Please note that in\nthese experiments, we choose IMTL and F-IMTL as two examples for standard and flat-aware\ngradient-based MTL training respectively. We also complete the ablation study in the main paper\nby providing results on all three datasets in the Multi-MNIST dataset.\nD.1 Image segmentation qualitative result\nIn this section, we provide qualitative results of our method of the CityScapes experiment. We\ncompare our proposed method against its main baseline by highlighting typical cases where our\nmethod excels in generalization performance. Figure 6 shows some visual examples of segmentation\noutputs on the test set. Note that in the CityScapes dataset, the “void\" class is identified as unclear\nand pixels labeled as void do not contribute to either objective or score [60].\n(a) A training sample (after augmentation)\n(b) Corresponding original image (before augmentation)\n(c) Predictions on an unseen image\n(d) Predictions on an unseen image\nFigure 6: Semantic segmentation prediction comparison on CityScapes . From left to right are\ninput images, ground truth, and segmentation outputs from SegNet [66] using ERM training and\nsharpness-aware training. Regions that are represented in gray color are ignored during training.\n(Best viewed in color).\n25\nWhile there is only a small gap between the segmentation performance of IMTL and F-IMTL,\nwe found that a small area, which is the car hood and located at the bottom of images, is often\nincorrectly classified. For example, in Figure 6, the third and fourth rows compare the prediction\nof SegNet [66] with ERM training and with our proposed method. It can be seen that both of\nthem could not detect this area correctly, this is because this unclear “void\" class did not appear\nduring training. Even worse, the currently employed data augmentation technique in the codebase\nof Nash-MTL and other recent multi-task learning methods [13, 10] consists of RandomCrop, which\noften unintentionally excludes edge regions. For example, Figure 6a shows an example fed to the\nneural network for training, which excludes the car hood and its logo, compared to the original\nimage (Figure 6b). Therefore, we can consider this \"void\" class as a novel class in this experiment,\nsince its appearance is ignored in both training and evaluation. Even though, in Figures 6c and 6d\nour training method is still able to distinguish between this unknown area and other nearby known\nclasses, which empirically shows the robustness and generalization ability of our method over ERM.\nD.2 Predictive performance\nIn this part, we provide experimental justification for an intriguing insight into the connection\nbetween model sharpness and model calibration. Empirically, we found that when a model converges\nto flatter minima, it tends to be more calibrated. We start by giving the formal definition of a\nwell-calibrated classification model and three metrics to measure the calibration of a model, then we\nanalyze our empirical results.\nConsider a C-class classification problem with a test set of N samples given in the form (x ,y )N\ni i i=1\nwhere y is the true label for the sample x . Model outputs the predicted probability for a given\ni i\nsample x to fall into C classes, is given by\ni\npˆ(x ) = [pˆ(y = 1|x ),...,pˆ(y = C|x )].\ni i i\npˆ(y = c|x ) is also the confidence of the model when assigning the sample x to class c. The predicted\ni i\nlabel yˆ is the class with the highest predicted value, pˆ(x ) := max pˆ(y = c|x ). We refer to pˆ(x ) as\ni i c i i\nthe confidence score of a sample x .\ni\nModel calibration is a desideratum of modern deep neural networks, which indicates that the\npredicted probability of a model should match its true probability. This means that the classification\nnetwork should be not only accurate but also confident about its prediction, i.e. being aware of\nwhen it is likely to be incorrect. Formally stated, the perfect calibration [75] is:\nP(yˆ= y|pˆ= q) = q,∀q ∈ [0,1]. (11)\nMetric. The exact computation of Equation 11 is infeasible, thus we need to define some metrics\nto evaluate how well-calibrated a model is.\n• Brier score ↓ (BS) [76] assesses the accuracy of a model’s predicted probability by taking into\naccount the absolute difference between its confidence for a sample to fall into a class and the\ntrue label of that sample. Formally,\nN C\n1 (cid:88)(cid:88)\nBS = (pˆ(y = c|x )−1[y = c])2.\ni i\nN\ni=1 c=1\n26\n• Expected calibration error ↓ (ECE) compares the predicted probability (or confidence) of a\nmodel to its accuracy [77, 75]. To compute this error, we first bin the confidence interval [0,1]\ninto M equal bins, then categorize data samples into these bins according to their confidence\nscores. We finally compute the absolute value of the difference between the average confidence\nand the average accuracy within each bin, and report the average value over all bins as the\nECE. Specifically, let B denote the set of indices of samples having their confidence scores\nm\nbelonging to the mth bin. The average accuracy and the average confidence within this bin are:\n1 (cid:88)\nacc(B ) = 1[yˆ = y ],\nm i i\n|B |\nm\ni∈Bm\n1 (cid:88)\nconf(B ) = pˆ(x ).\nm i\n|B |\nm\ni∈Bm\nThen the ECE of the model is defined as:\nM\n(cid:88) |B m|\nECE = |acc(B )−conf(B )|.\nm m\nN\nm=1\nIn short, the lower ECE neural networks obtain, the more calibrated they are.\n• Predictive entropy (PE) is a widely-used measure of uncertainty [78, 79, 80] via the predictive\nprobability of the model output. When encountering an unseen sample, a well-calibrated model\nis expected to yield a high PE, representing its uncertainty in predicting out-of-domain (OOD)\ndata.\nC\n1 (cid:88)\nPE = −pˆ(y = c|x )logpˆ(y = c|x ).\ni i\nC\nc=1\nFigures 7 and 8 plot the distribution of the model’s predicted entropy in the case of in-domain\nand out-domain testing, respectively. We can see when considering the flatness of minima, the model\nshows higher predictive entropy on both in-domain and out-of-domain, compared to ERM. This also\nmeans that our model outputs high uncertainty prediction when it is exposed to a sample from a\ndifferent domain.\n27\n17.5 O ERu Mrs\n30\nO ERu Mrs\n15.0\n25\n12.5\n20\n10.0\n15\n7.5\n5.0 10\n2.5 5\n0.0 0\n0.0 0.5 1.0 1.5 2.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75\n16 Ours Ours\nERM 25 ERM\n14\n12 20\n10\n15\n8\n6 10\n4\n5\n2\n0 0\n0.0 0.5 1.0 1.5 2.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50\nFigure 7: Histograms of predictive entropy of ResNet18 [61] on in domain dataset, train and test on\nMultiMNIST (left) and MultiFashion (right). We use the orange lines to denote ERM training while\nblue lines indicate our proposed method.\n6\nOurs Ours\nERM 3.5 ERM\n5\n3.0\n4 2.5\n3 2.0\n1.5\n2\n1.0\n1\n0.5\n0 0.0\n0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0\nOurs Ours\n4.0 ERM 4 ERM\n3.5\n3.0 3\n2.5\n2.0 2\n1.5\n1.0 1\n0.5\n0.0 0\n0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 2.5\nFigure 8: Out of domain: model is trained on MultiMNIST, then tested on MultiFashion (left)\nand vice versa (right). Models trained with ERM give over-confident predictions as their predictive\nentropy concentrates around 0.\n28\nHere, we calculate the results for both tasks 1 and 2 as a whole and plot their ECE in Figure 9.\nWhen we look at the in-domain prediction in more detail, our model still outperforms ERM in terms\nof expected calibration error. We hypothesize that considering flat minima optimizer not only lowers\nerrors across tasks but also improves the predictive performance of the model.\nERM Ours\n1.0\nGap Gap\nAccuracy Accuracy\n0.8\n0.6\n0.4\n0.2\nECE=4.47 ECE=2.49\n0.0\n1.0\nGap Gap\nAccuracy Accuracy\n0.8\n0.6\n0.4\n0.2\nECE=11.72 ECE=3.28\n0.0\n1.0\nGap Gap\nAccuracy Accuracy\n0.8\n0.6\n0.4\n0.2\nECE=6.54 ECE=4.91\n0.0\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\nConfidence Confidence\nFigure9: Thepredictiveperformance(measuredbytheexpectedcalibrationerror)ofneuralnetworks\nhas been enhanced by using our proposed training method (right column).\nWe also report the Brier score and ECE for each task in Table 6 and Table 7. As can be observed\n29\nycaruccA\ndetcepxE\nycaruccA\ndetcepxE\nycaruccA\ndetcepxE\nTable 6: Brier score on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST datasets. We use\nthe bold font to highlight the best results.\nDataset Task Multi-Fashion Multi-Fashion+MNIST MultiMNIST\nTopleft 0.237 0.055 0.082\nERM Bottomright 0.254 0.217 0.106\n. Average 0.246 0.136 0.094\nTopleft 0.172 0.037 0.059\nOurs Bottomright 0.186 0.189 0.075\nAverage 0.179 0.113 0.067\nfrom these tables, our method shows consistent improvement in the model calibration when both\nscores decrease over all scenarios.\nTable 7: Expected calibration error on Multi-Fashion, Multi-Fashion+MNIST and MultiMNIST\ndatasets. Here we set the number of bins equal to 10.\nDataset Task Multi-Fashion Multi-Fashion+MNIST MultiMNIST\nTopleft 0.113 0.027 0.039\nERM Bottomright 0.121 0.104 0.050\nAverage 0.117 0.066 0.045\nTopleft 0.034 0.015 0.022\nOurs Bottomright 0.032 0.083 0.028\nAverage 0.033 0.049 0.025\nD.3 Effect of choosing perturbation radius ρ.\nThe experimental results analyzing the sensitivity of model w.r.t ρ are given in Figure 10. We evenly\npicked ρ from 0 to 3.0 to run F-CAGrad on three Multi-MNIST datasets.\n96\n94\n92 MultiFashion\nMultiMnist\nMultiFashion+MNIST\n90\n88\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nFigure 10: Average accuracy when varying ρ from 0 to 3.0 (with error bar from three independent\nruns).\nWe find that the average accuracy of each task is rather stable from ρ = 0.5, which means the\n30\nycaruccA\neffect of different values of ρ in a reasonably small range is similar. It can also easy to notice that\nthe improvement tends to saturate when ρ ≥ 1.5.\nD.4 Gradient conflict.\nIn the main paper, we measure the percentage of gradient conflict on the MultiFashion+MNIST\ndataset. Here, we provide the full results on three different datasets. As can be seen from Figure 11,\nthere is about half of the mini-batches lead to the conflict between task 1 and task 2 when using\ntraditional training. Conversely, our proposed method significantly reduces such confliction (less\nthan 5%) via updating the parameter toward flat regions.\nOurs ERM\n50 60 50\n40 50 40\n40\n30 30\n30\n20 20\n20\n10 10 10\n0 0 0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\nFigure 11: Task gradient conflict proportion of models trained with our proposed method and\nERM across MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nD.5 Loss landscape\nThirdly, we provide additional visual comparisons of the loss landscapes trained with standard\ntraining and with our framework across two tasks of three datasets of Multi-MNIST. As parts of the\nobtained visualizations have been presented in the main paper, we provide the rest of them in this\nsubsection. The results in Figures 12 and 13 consistently show that our method obtains significantly\nflatter minima on both two tasks, encouraging the model to generalize well.\nFigure 12: Loss landscapes of task 1 and task 2 on MultiFashion\n31\n)%(\ntcilfnoc\ntneidarG\nFigure 13: Loss landscapes of task 1 and task 2 on MultiFashion+MNIST\nD.6 Training curves\nSecondly, we compare the test accuracy of trained models under the two settings in Fig. 15. It can\nbe seen that from the early epochs (20-th epoch), the flat-based method outperforms the ERM-based\nmethod on all tasks and datasets. . Although the ERM training model is overfitted after such a\nlong training, our model retains a high generalizability, as discussed throughout previous sections.\nOurs ERM\n1.00 1.000 1.00\n0.98 0.975\n0.95\n0.96 0.950\n0.94 0.925 0.90\n0.92 0.900 0.85\n0.90 0.875\n0.88 0.850 0.80\n0.86\n0.84 0.825 0.75\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs\n1.000 1.00 1.00\n0.975\n0.95 0.95\n0.950\n0.925 0.90 0.90\n0.900 0.85\n0.875 0.85\n0.80\n0.850\n0.80\n0.825 0.75\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\nFigure 14: Train accuracy of models trained with our proposed method and ERM across 2 tasks\n(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nFurthermore, we also plot the training accuracy curves across experiments in Figure 14 to\nshow that training accuracy scores of both ERM and our proposed method are similar and reach\n≈ 100% from 50-th epoch, which illustrates that the improvement is associated with generalization\nenhancement, not better training.\n32\nycaruccA\nycaruccA\nOurs ERM\n98\n96 88\n95 96 87\n86\n94\n94 85\n93\n84\n92 92 83\n91 82\n90 90 81\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs\n95 89\n94 88 86\n93 87 92 86 84\n91 85 82\n90 84\n89 83 80\n88\n82\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs\nFigure 15: Test accuracy of models trained with our proposed method and ERM across 2 tasks\n(rows) of MultiFashion, MultiFashion+MNIST and MultiMNIST datasets (columns).\nD.7 Model sharpness\nFourthly, Figure 16 displays the evolution of ρ-sharpness of models along training epochs under\nconventional loss function (ERM) and worst-case loss function (ours) on training sets of three\ndatasets from Multi-MNIST, with multiple values of ρ. We can clearly see that under our framework,\nfor both tasks, the model can guarantee uniformly low loss value in the ρ-ball neighborhood of\nparameter across training process. In contrast, ERM suffers from sharp minima from certain epochs\nwhen the model witnesses a large gap between the loss of worst-case perturbed model and current\nmodel. This is the evidence for the benefit that our framework brings to gradient-based methods,\nwhich is all tasks can concurrently find flat minima thus achieving better generalization.\nD.8 Gradient norm\nFinally, we demonstrate the gradient norm of the loss function w.r.t the worst-case perturbed\nparameter of each task. On the implementation side, we calculate the magnitude of the flat gradient\ngi,flat for each task at different values of ρ in Figure 17. As analyzed by equation (6) from the main\npaper, following the negative direction of gi,SAM will lower the L norm of the gradient, which\nsh 2\norients the model towards flat regions. This is empirically verified in Figure 17. In contrast, as the\nnumber of epochs increases, gradnorm of the model trained with ERM tends to increase or fluctuate\naround a value higher than that of model trained with SAM.\n33\nycaruccA\nycaruccA\n0.012 0.0175 Ours ERM 0.010\n0.0150\n0.010 0.008\n0.0125 0.008 0.0100 0.006\n0.006\n0.0075 0.004\n0.004 0.0050\n0.002 0.0025 0.002\n0.000 0.0000 0.000\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n0.012 0.025 0.010\n0.010\n0.020 0.008 0.008\n0.015 0.006\n0.006\n0.004 0.010 0.004\n0.002 0.005 0.002\n0.000 0.000 0.000\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(a) ρ=0.005\n4 Ours ERM 0.7\n0.150\n0.6\n0.125 3 0.5\n0.100 0.4\n0.075 2 0.3\n0.050 1 0.2\n0.025 0.1\n0.000 0 0.0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n1.2 0.20\n0.150\n1.0\n0.125 0.15\n0.100 0.8\n0.075 0.6 0.10\n0.050 0.4\n0.05\n0.025 0.2\n0.000 0.0 0.00\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(b) ρ=0.05\n8 80 Ours ERM 30\n25\n6 60 20\n4 40 15\n10\n2 20\n5\n0 0 0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n7 60 20\n6 50\n15\n5 40\n4 30 10\n3\n20\n2 5\n1 10\n0 0 0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(c) ρ=0.5\nFigure 16: Sharpness of models trained with our proposed method and ERM with different values\nof ρ. For each ρ, the top and bottom row respectively represents the first and second task, and each\ncolumn respectively represents each dataset in Multi-MNIST: from left to right are MultiFashion,\nMultiFashion+MNIST, MultiMNIST.\n34\nssenprahS\nssenprahS\nssenprahS\nssenprahS\nssenprahS\nssenprahS\n0.25 Ours ERM\n0.20 0.08\n0.20\n0.15 0.06\n0.15\n0.10 0.10 0.04\n0.05 0.05 0.02\n0.00 0.00 0.00\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n0.25 0.12\n0.4\n0.20 0.10\n0.3 0.08\n0.15\n0.06\n0.10 0.2\n0.04\n0.05 0.1 0.02\n0.00 0.0 0.00\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(a) ρ=0.005\n1.0 1.75 Ours ERM 0.8\n1.50\n0.8\n1.25 0.6\n0.6 1.00\n0.4\n0.4 0.75\n0.2 0.50 0.2\n0.25\n0.0 0.00 0.0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n1.0 2.5 0.7\n0.6\n0.8 2.0\n0.5\n0.6 1.5 0.4\n0.4 1.0 0.3\n0.2\n0.2 0.5\n0.1\n0.0 0.0 0.0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(b) ρ=0.05\n14 7\n7 Ours ERM\n12 6\n6\n5 10 5\n4 8 4\n3 6 3\n2 4 2\n1 2 1\n0 0 0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\n7 7\n6 20 6\n5 15 5\n4 4\n3 10 3\n2 2\n5\n1 1\n0 0 0\n0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200\nEpochs Epochs Epochs\n(c) ρ=0.5\nFigure 17: Gradient magnitude at the worst-case perturbations of models trained with our\nproposed method and ERM with different values of ρ. For each ρ, the top and bottom row\nrespectively represents the first and second task, and each column respectively represents each\ndataset in Multi-MNIST: from left to right are MultiFashion, MultiFashion+MNIST, MultiMNIST.\n35\nmron\ntneidarG\nmron\ntneidarG\nmron\ntneidarG\nmron\ntneidarG\nmron\ntneidarG\nmron\ntneidarG",
    "pdf_filename": "Improving_Multi-task_Learning_via_Seeking_Task-based_Flat_Regions.pdf"
}