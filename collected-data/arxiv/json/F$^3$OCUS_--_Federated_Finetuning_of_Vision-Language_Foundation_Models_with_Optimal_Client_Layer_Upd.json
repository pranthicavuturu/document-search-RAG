{
    "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Upd",
    "abstract": "Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learn- ing (FL) requires the usage of parameter-efficient fine- tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors viz., client-specific layer importance score that selects the most important VLM layers for fine- tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first theoretically motivate and leverage the principal eigenvalue magnitude of layerwise Neural Tan- gent Kernels and show its effectiveness as client-specific layer importance score. Next, we propose a novel layer updating strategy dubbed F3OCUS that jointly optimizes the layer importance and diversity factors by employing a data-free, multi-objective, meta-heuristic optimization on the server. We explore 5 different meta-heuristic algorithms and compare their effectiveness for selecting model layers and adapter layers towards PEFT-FL. Furthermore, we re- lease a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9 modality-specific clients and utilize it to train and evaluate our method. Overall, we conduct more than 10,000 client-level experiments on 6 Vision-Language FL task settings involving 58 medical image datasets and 4 different VLM architectures of varying sizes to demonstrate the effectiveness of the proposed method.",
    "body": "F3OCUS - Federated Finetuning of Vision-Language Foundation Models with\nOptimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics\nPramit Saha\nUniversity of Oxford\npramit.saha@eng.ox.ac.uk\nFelix Wagner\nUniversity of Oxford\nfelix.wagner@eng.ox.ac.uk\nDivyanshu Mishra\nUniversity of Oxford\ndivyanshu.mishra@eng.ox.ac.uk\nCan Peng\nUniversity of Oxford\ncan.peng@eng.ox.ac.uk\nAnshul Thakur\nUniversity of Oxford\nanshul.thakur@eng.ox.ac.uk\nDavid Clifton\nUniversity of Oxford; Oxford-Suzhou Centre for Advanced Research\ndavid.clifton@eng.ox.ac.uk\nKonstantinos Kamnitsas\nUniversity of Oxford; Imperial College London; University of Birmingham\nkonstantinos.kamnitsas@eng.ox.ac.uk\nJ. Alison Noble\nUniversity of Oxford\nalison.noble@eng.ox.ac.uk\nAbstract\nEffective training of large Vision-Language Models (VLMs)\non resource-constrained client devices in Federated Learn-\ning (FL) requires the usage of parameter-efficient fine-\ntuning (PEFT) strategies. To this end, we demonstrate the\nimpact of two factors viz., client-specific layer importance\nscore that selects the most important VLM layers for fine-\ntuning and inter-client layer diversity score that encourages\ndiverse layer selection across clients for optimal VLM layer\nselection. We first theoretically motivate and leverage the\nprincipal eigenvalue magnitude of layerwise Neural Tan-\ngent Kernels and show its effectiveness as client-specific\nlayer importance score. Next, we propose a novel layer\nupdating strategy dubbed F3OCUS that jointly optimizes\nthe layer importance and diversity factors by employing\na data-free, multi-objective, meta-heuristic optimization on\nthe server. We explore 5 different meta-heuristic algorithms\nand compare their effectiveness for selecting model layers\nand adapter layers towards PEFT-FL. Furthermore, we re-\nlease a new MedVQA-FL dataset involving overall 707,962\nVQA triplets and 9 modality-specific clients and utilize it to\ntrain and evaluate our method. Overall, we conduct more\nthan 10,000 client-level experiments on 6 Vision-Language\nFL task settings involving 58 medical image datasets and 4\ndifferent VLM architectures of varying sizes to demonstrate\nthe effectiveness of the proposed method.\n1. Introduction\nLarge Vision-Language Models (VLMs) have made sig-\nnificant advancements in multi-modal learning, excelling\nin tasks like Visual Question Answering (VQA) [9, 35,\n45, 46, 51].\nTheir effectiveness stems from their exten-\nsive parameters often reaching millions or billions, allow-\ning them to learn complex representations of image and\ntext data. Fine-tuning these models with task-specific data\nis crucial for adapting them to specialized applications.\nHowever, gathering diverse training data centrally is chal-\nlenging, especially in fields like healthcare, where strict\nprivacy regulations prevent data aggregation across dif-\nferent centers.\nTo address the privacy concerns, Feder-\nated Learning (FL) [2, 34, 47, 55] allows models to be\ntrained directly on local devices, such as in healthcare clin-\nics, without sharing sensitive data. Yet, fine-tuning large\nmodels locally is difficult due to limited computational\npower and smaller datasets, which hinders VLM adaptation.\narXiv:2411.11912v1  [cs.CV]  17 Nov 2024\n\nFigure 1. Distinction of our approach from prior works. (a) illus-\ntrates vanilla layer-selection process, which only selects param-\neter subsets based on the local client data without considering\nthe requirements of the other clients. (b) depicts our approach,\nF 3OCUS, which refines the client-specific layer selection by\njointly maximizing overall client-specific importance score and\nlayer selection diversity score across clients.\nBalancing privacy with these resource limitations requires\ninnovative solutions like Parameter-Efficient Fine-Tuning\n(PEFT) that fine-tunes either selected model parameters or\nadded parameters while keeping the original model fixed\n[8, 24, 30, 32, 43, 48, 49, 60, 70].\nCombined with FL,\nthese offer a privacy-preserving and resource-efficient strat-\negy for training large models collaboratively across multiple\nclients, particularly in computationally constrained settings.\nPrevious research [16, 65, 77, 79, 82, 87] has mostly fo-\ncused on naive combination of centralized finetuning meth-\nods with FedAvg [55]. However, these works are primar-\nily confined to single-modalities, addressing either visual\nor textual inputs independently. Besides, they do not con-\nsider the diverse characteristics and capacities of individual\nclients and typically assume homogeneous computational\nresources across all clients which is not applicable in most\nreal-world collaborative settings. Hence, some clients either\nunder-utilize the available resources or are unable to partic-\nipate due to lack of compute. Our flexible layer selection\nPEFT-FL framework effectively addresses these issues.\nThe naive combinations of centralized selective PEFT\nmethods [40, 42, 54, 61, 66, 72] and FL only consider lo-\ncal client data and task for selecting parameter subsets with-\nout considering other client requirements. This is especially\nFigure 2. Loss convergence of layer selection methods. The gap\nbetween the client-specific NTK and FOCUS demonstrates the im-\nportance of our multi-objective meta-heuristic optimization.\nproblematic for FL clients facing challenges like heteroge-\nneous modalities and computes, domain shifts, and statis-\ntical heterogeneity. In such cases, a poorly chosen client-\nspecific configuration can not only slow down the overall\nconvergence (see Fig. 2), but may also perform worse than\ntraining each client independently. Achieving the best per-\nformance requires a tailored approach that jointly considers\nclient-specific as well as global optimization requirements.\nTo this end, we present a novel framework called\nF 3OCUS (Federated Finetuning of Foundation Models\nwith Optimal Client-specific Layer Updating Strategy) to\nimprove layer selection by considering both local and global\nFL characteristics while respecting the client-specific re-\nsource constraints.\nWe propose a two-step “define and\nrefine” procedure at the beginning of every round: (a)\nclient-level strategy, that defines layer importance scores\nbased on the principal eigenvalue of layerwise Neural Tan-\ngent Kernel (LNTK) and (b) server-level strategy, that re-\nfines client-specific layer selection by maximizing the over-\nall client-specific importance scores while simultaneously\nminimizing the variance of the histogram of layer selec-\ntions across clients, thereby promoting a more uniform dis-\ntribution of layer participation. Our method (see Figs. 1\n& 4) provides the clients with a flexible and dynamic so-\nlution for selecting layers where each client can specify\ntheir computational budgets, while ensuring faster conver-\ngence (see Fig. 2). In order to showcase the effectiveness\nof F 3OCUS, we conduct over 10,000 client-level exper-\niments under 6 Vision-language FL task settings using 4\nVLMs and 58 medical image datasets that involve 4 types\nof heterogeneities based on data, modality, device, and task.\nOur primary contributions can be summed up as follows:\n• Dataset contribution: We release Ultra-MedVQA, the\nlargest medical VQA dataset to date, consisting of\n707,962 VQA triplets including 9 different modalities and\n\n12 distinct anatomies, covering diverse open-ended and\nclosed questions related to modality, tissue type, image\nview, anatomy, and disease (see Tab. 1 for comparison).\n• Technical contributions: We theoretically motivate and\npropose F 3OCUS, a new selective layer fine-tuning\nstrategy.\nWe introduce LNTK-based client-level layer\nselection and server-level multi-objective meta-heuristic\noptimization that jointly optimizes client-specific layer\nimportance score and inter-client layer diversity score.\nWe theoretically motivate and analyze the effectiveness\nof our layer selection strategy and prove its convergence.\n• Empirical contributions: Unlike previous works, we\nconsider more constrained and realistic client settings\ninvolving data, modality, task, and device heterogene-\nity. We conduct comprehensive evaluations of F 3OCUS\nwith 4 VLMs of varying sizes in diverse FL settings for\ntuning selective model layers as well as adapters. We em-\npirically show that fine-tuning the last few adapters/layers\nof VLMs is only as good as tuning randomly se-\nlected adapters/layers.\nConsequently, we present de-\ntailed insights into F 3OCUS’s performance improve-\nments through: (a) analysis of client and server-based\nlayer rank and importance score computation during\ntraining and (b) evaluation of different meta-heuristic op-\ntimization algorithms on the server viz., Genetic Algo-\nrithm, Artificial Bee Colony, Ant Colony Optimization,\nSimulated Annealing, and Swarm Particle Optimization.\n2. Background and Related works\nFederated Learning (FL) FL enables various clients to\ncollaboratively train models in a decentralized manner with-\nout sharing local data. The classical FL framework, FedAvg\n[55], offers a practical method for model aggregation. Sev-\neral modifications have emerged to address the adverse im-\npact of data heterogeneity in FL [2, 34, 47].\nCentralized selective fine-tuning: Various methods have\nbeen explored for selecting subsets of parameters for fine-\ntuning foundation models in centralized training.\nThese\ninclude optimizing non-structured mask matrices [36, 38,\n39, 63, 76, 80, 83, 84], employing layer-wise selection\n[33, 36, 39, 42] and pruning methods [40, 44, 58, 72, 86].\nFederated selective fine-tuning:\nRecent research has\nadapted these selective PEFT methods for FL [16, 28, 56,\n84]. Specifically, studies by [22, 41] explore layer-wise net-\nwork decomposition to facilitate selective model fine-tuning\non client devices. Partial model personalization algorithms\n[13, 57] aim to train tailored subnetworks on clients to im-\nprove local models. However, these studies do not provide\nadaptive or dynamic layer selection strategies that consider\nthe diverse characteristics of clients. Unlike prior works, we\naccount for client-specific differences in resources and data\ndistributions while also considering the global optimization\nrequirements to perform selective layer fine-tuning.\n3. Problem Formulation\nConsider an FL system with a central server and N clients,\nrepresented by N = {1, . . . , N}. Each client has its own\nprivate dataset Di, containing di = |Di| data points. The\nserver contains a pre-trained foundation model parameter-\nized by θ ∈RP , comprising L layers, indexed by L =\n{1, 2, . . . , L}.\nThe server’s objective is to fine-tune this\nmodel based on the clients’ data {Di}i∈N without directly\naccessing these datasets. The learning goal is formalized as:\nmin\nθ∈RP F(θ) =\nN\nX\ni=1\nαiFi(θ),\n(1)\nwhere αi =\ndi\nPN\nj=1 dj denotes relative sample size, and\nFi(θ) =\n1\ndi\nP\nBi∈Di Fi(θ; Bi) represents local training ob-\njective for client i, with Fi(θ; Bi) being the (potentially\nnon-convex) loss function of model θ on data batch Bi. The\nFL training process proceeds over T rounds. In each round\nt ∈[T], the server selects a subset of clients St and dis-\ntributes the updated global model θt to them for training.\nDue to resource constraints, instead of the entire model,\nclients update only a subset of layers during local train-\ning. Each client-specific masking vector can be denoted\nas mi,t ∈{0, 1}L, where ml\ni,t = 1 if layer l is selected\nfor training in round t, and ml\ni,t = 0 otherwise. Thus, the\nset of selected layers for client i at round t is denoted as\nLi,t = {l ∈L | ml\ni,t = 1}, and the union of all selected\nlayers across clients in round t is Lt = S\ni∈St Li,t.\nClients initialize their local models using global model\nfrom the server, θi,t, and perform τ steps of local training\nusing mini-batch SGD. For each local step k ∈[τ], client i\nsamples a batch of data Bi,t and calculates gradients for the\nselected layers as:\nX\nl∈Lt\ni\nGi,l(θk\ni,t; Bk\ni,t) =\nX\nl∈Lt\ni\n∇lFi(θi,t; Bi,t),\n(2)\nwhere ∇lF(θ) denotes the gradient of F(θ) with respect\nto the parameters of layer l. After computing the gradients,\nthe local model is updated with learning rate η as:\nθk\ni,t = θk−1\ni,t\n−η\nX\nl∈Lt\ni\nGl\ni(θk−1\ni,t ; Bk−1\ni,t ),\n∀k ∈{1, 2, . . . , τ},\n(3)\nThe accumulated weight update in one local round is:\nδi,t = 1\nη (θ0\ni,t −θτ\ni,t) =\nτ−1\nX\nk=0\nX\nl∈Lt\ni\nGl\ni(θk\ni,t; Bk\ni,t)\n(4)\nAccumulated update after federated aggregation on server:\nδt =\nX\ni∈St\nτ−1\nX\nk=0\nX\nl∈Lt\ni\nαiGl\ni(θk\ni,t; Bk\ni,t)\n(5)\n\nTable 1. # modalities (M) and VQA triplets in different datasets\nVQA-RAD\nSLAKE\nPath-VQA\nVQA-Med\nOmniMedVQA\nOurs\n# M\n3\n3\n2\n5\n12\n9\n# VQA\n3515\n14028\n32799\n5500\n127995\n707962\nFigure 3. Visualization of principal eigenvalue magnitudes of LNTK (see §4.1) for computing layer importance score of LLaVA-1.5-7b\n4. Client-level layer importance\n4.1. Layerwise Neural Tangent Kernel (LNTK)\nIn this section, we first motivate the usage of LNTK towards\nprioritizing selected layers for client-specific fine-tuning.\nTo capture the model training dynamics, consider the evolu-\ntion of θt and training loss L over input instances X ⊂Di:\n˙θt = −η∇θF(X; θt)∇F (X; θt)L\n˙L = ∇F (X; θt)LT ∇F (X; θt)\n˙θt = −η∇F (X; θt)LT Θ(X, X)∇F (X; θt)L\n(6)\nwhere the NTK matrix Θt(X, X) at time t is defined as:\nΘt(X, X) ≜∇θF(X; θt)∇θF(X; θt)T ∈Rn×n.\n(7)\nThe integral NTK of a model [31] can be computed as the\nsum of layerwise NTK (LNTK), where we define LNTK as:\nΘl(X, X) = ∇θlF(X; θl)∇θlF(X; θl)T ,\n(8)\nwhere ∇θlF(X; θl) denotes the Jacobian of F at input\npoints X with respect to the l-th layer parameters θl. The\nintegral NTK can be expressed as: Θ(X, X)\n(i)\n=\nL\nX\nl=1\n∇θl(X)∇θl(X)T\n(ii)\n=\nL\nX\nl=1\nX\nθp∈θl\n∇θl(X)∇θl(X)T (iii)\n=\nL\nX\nl=1\nΘl(X, X),\n(9)\nwhere (i) decomposes the matrix multiplication into the sum\nof vector multiplications; (ii) gathers addends by each mod-\nule; and (iii) follows the definition of the LNTK. Since\nΘl(X, X) is a positive semi-definite real symmetric matrix,\nwe perform an eigen-decomposition of LNTK as:\nΘl(X, X) = U lΛl(U l)T =\nnk\nX\nj=1\nλl\njul\nj(ul\nj)T\n(10)\nwhere Λl = diag(λl\n1, λl\n2, . . . , λl\nnk) contains eigenvalues λl\nj\nof Θl(X, X), and each λl\nj ≥0. The mean output of the l-th\nlayer in the eigenbasis of the LNTK can be described by:\n(U lE[f l(X)])j =\n\u0010\nI −e−ηλl\njt\u0011\n(U lyl)j\n(11)\nwhere f l(X) and yl are actual and target l-th layer out-\nputs. This formulation demonstrates that the convergence\nbehavior of a layer is largely influenced by the eigenval-\nues λl\nj of LNTK. Let λl\n1 ≥λl\n2 ≥· · · ≥λl\nn denote the\neigenvalues of Θl(X, X), where λl\n1 is the principal eigen-\nvalue. In particular, λl\n1 plays a dominant role in the con-\nvergence dynamics [10]. When using the largest possible\nlayer-specific learning rate, ηl ∼\n2\nλl\n1 , the training process\naligns most strongly with the direction associated with the\nprincipal eigenvalue as the NTK eigenvectors correspond-\ning to the principal eigenvalue are learned quicker due to\nspectral bias [10, 12, 59, 64]. This motivates the use of\nthe principal eigenvalue λl\n1 in our client-specific layer im-\nportance score, as it represents the maximum alignment of\neach layer’s parameter space with the client’s data distribu-\ntion. This provides a principled basis for prioritizing these\nlayers.\nFor a step of gradient descent, the loss reduction can be\ncharacterized by the directional derivative of the loss:\n∆L\n(i)\n= lim\nϵ→0\nL(θ + ϵ∇θL(θ)) −L(θ)\nϵ\n(ii)\n≈∇θL(θ)T ∇θL(θ)\n(iii)\n= ∇ZL(θ)T (∇θF(θ)T ∇θF(θ))∇ZL(θ)\n(iv)\n= ∇ZL(θ)T\n L\nX\nl=1\nΘl\n!\n∇ZL(θ)\n(v)\n=\nL\nX\nl=1\nnk\nX\nj=1\nλl\nj\n\u0010\n(ul\nj)T Y\n\u00112\n(vi)\n≈\nL\nX\nl=1\nλl\n1\n\u0010\n(ul\n1)T Y\n\u00112\n(12)\nwhere (i) follows the definition of the directional deriva-\ntive; (ii) follows the first-order Taylor expansion; (iii) ap-\nplies the chain rule of derivatives; (iv) follows from Eq. (8);\nand (v) follows the eigen-decomposition of the layerwise\nNTK under the assumption of squared error loss. Assuming\nthat the true labels align well with the top eigenvectors as\n\nFigure 4. Overview of our layer selection strategy, F 3OCUS. Each client sends layer importance scores based on the principal eigenvalue\nof LNTK to the server. The server refines client-specific layer selection by maximizing the cumulative client-specific importance scores\nwhile simultaneously minimizing the variance of the histogram of layer selections across clients. It sends the revised layer ranks back.\nFigure 5. Visualization of layer ranks of LLaVA-1.5 across rounds in different clients based on LNTK. Darker color implies higher rank.\ndiscussed earlier, i.e.,\n\u0000(ul\nj)T Y\n\u00012 is large for large λl\nj, di-\nrectional derivative of the loss function can be regarded as\nclosely related to the eigenspectrum of the layerwise NTKs.\nSpecifically, (vi) suggests that layers with higher principal\neigenvalues contribute more significantly to loss reduction\nduring training. Overall, Eq. 12 suggests that the loss de-\ncreases more rapidly along the eigenspaces corresponding\nto larger LNTK eigenvalues. Given that the principal eigen-\nvalue λl\n1 vary across layers as seen in Fig. 3, we propose\nto selectively fine-tune layers with larger λl\n1 to achieve effi-\ncient learning with limited computational resources.\nTherefore, we define the client-specific layer impor-\ntance score Sl\ni =\nλl\ni,1\nPL\nk=1 λk\ni,1 where the sum in the denomi-\nnator normalizes the principal eigenvalue across all layers,\nensuring that Sl\ni captures the relative importance of each\nlayer for client i in terms of its contribution to the model’s\npredictive capacity for that client’s data distribution. This\nformulation prioritizes layers whose NTK principal eigen-\nvalues dominate, indicating strong client-specific parameter\nalignment (see Figs. 3 & 5). See Algorithm in Suppl. §A.\n4.2. Convergence Analysis of LNTK\nWe begin with some necessary assumptions following pre-\nvious works [34, 73] and then introduce a set of assumptions\nto analyze the impact of the layers selected using LNTK:\nAssumption 1: (γ-Smoothness) There exists a constant\nγ > 0 such that for any θ, ϕ ∈Rp:\n∥∇Fi(θ) −∇Fi(ϕ)∥2 ≤γ∥θ −ϕ∥2,\n∀i ∈N.\n(13)\nAssumption\n2:\n(Unbiased\nand\nvariance-bounded\nstochastic gradient) The layerwise stochastic gradient\nGi,l(θt; Bi,t) computed on a randomly sampled data batch\nBi,t serves as an unbiased estimate of the layerwise full-\nbatch gradient:\nEBi,t[Gi,l(θt; Bi,t)] = ∇Fi,l(θt).\n(14)\n\nBesides, there exist constants σl > 0, ∀l ∈L such\nthat EBi,t∥Gl\ni(θt; Bi,t) −∇F l\ni (θt)∥2 ≤σ2\nl , ∀i ∈N and\nP\nl∈L σ2\nl ≤σ2.\nAssumption 3: (Gradient Diversity) The non-IID client\ndata distribution causes diverse gradients. There exist con-\nstants κl > 0, ∀l ∈L such that:\nEBi,t[∥∇F(θt) −∇F l\ni (θt; Bi,t)∥2] ≤κ2\nl , ∀i ∈N.\n(15)\nIn contrast to the theoretical analysis for standard FL\nsettings, our LNTK-based fine-tuning introduces three ad-\nditional challenges: (i) Each client only updates a sub-\nset of layers. Hence, the aggregated gradient is no longer\nan unbiased estimate of the local gradient ∇Fi(θt), i.e.,\nP\nl∈Lt ∇F l\ni (θt) ̸= ∇Fi(θt) where equality holds only\nif all layers are selected.\n(ii) LNTK-based layer selec-\ntion may vary across clients as seen in Fig. 5. The ag-\ngregated gradient of the selected layers is not equal to\nthe gradient computed based on the global loss function\ni.e., P\ni∈St\nP\nl∈Lt αi,t∇lFi(θt) ̸= P\nl∈Lt ∇lF(θt), where\nequality holds only if all clients select the same layers.\n(iii) The gradient magnitudes in (i-ii) vary across epochs.\nIn order to relate the aggregated and target gradient, we\ndefine a proxy layerwise loss ψi,l optimized by clients as:\nψl\ni(θt) ≜\nX\ni∈St\nml\ni,tαi,tF l\ni (θt),\ns.t ∇lψl\ni(θt) = δt\n(16)\nGiven assumptions 1-3, we formulate the convergence as:\nTheorem 1 (Convergence of LNTK-based layer selection):\nmin\nt∈[T ] E\n\u0002\n∥∇F(θt)∥2\n2\n\u0003\n≤\n2\n(η −γη2)T\n\"\n\u0002\nF(θ0) −F(θ∗)\n\u0003\n+γ(ησ)2T +\nT\nX\nt=1\n\u0012\nη + 1\n2γ −γη2\n\u0013  \nE\n\n\n\r\r\r\r\r\r\nX\nℓ/∈Lt\n∇ℓF(θt)\n\r\r\r\r\r\r\n2\n\n+\nX\nℓ∈Lt\nX\ni∈N\nαi,t(1 −ml\ni,t)2k2\nl\n!#\ns.t θ∗= arg min\nθ∈Rp F(θ)\n(17)\nRemark 1: With commonly chosen η = O\n\u0010\n1\n√\nT\n\u0011\n, RHS\nof (17) the last term →0 as T →∞. So, LNTK-based\nPEFT-FL converges to a small neighbourhood of a station-\nary point of standard FL, maintaining a non-zero error floor.\nSee Suppl. §B for complete proof and discussions.\n5. Server-level Overall Layer Importance\n5.1. Theoretical Motivation\nTo explicitly examine the impact of potential noise intro-\nduced by LNTK-based layer selection as well the variation\nof selection count across clients, we reformulate the conver-\ngence in Theorem 2 leveraging two additional assumptions:\nAssumption 4: (Bounded stochastic gradient) The ex-\npected squared norm of stochastic gradients is bounded uni-\nformly, i.e., for constant σg > 0 and any i, t:\nEBi,t[∥Gi(θt; Bi,t)∥2] ≤σ2\ng.\n(18)\nAssumption 5:\n(Normalized Layer Selection Noise\nBound) There exist some ξ2 ∈[0, 1) and any t, i, the nor-\nmalized layer selection noise due to LNTK is bounded by:\n∥θt −ˆθt,i∥2\n∥θt∥2\n≤ξ2\n(19)\nwhere ˆθt,i denotes LNTK-based client-specific layers.\nTheorem 2 (Impact of layer selection-based noise ξ and\nvariance of selection count s2\nd on LNTK convergence):\nmin\nt∈[T ] E\n\u0002\n∥∇F(θt)∥2\n2\n\u0003\n≤\n2\n(η −3γη2)T\n\"\nF(θ0) −F(θ∗)\n+\nT\nX\nt=1\nηξ2γ2Nsd\n2\n(1 + 3ηγ) E\n\n\n\r\r\r\r\r\nT\nX\nt=1\n∇ℓF(θt)\n\r\r\r\r\r\n2\n\n+ γη2NTsd\n2\n\u0000ηγσ2\ng(1 + 3γ) + 3σ2sd\n\u0001\n#\ns.t.\nθ∗= arg min\nθ∈Rp F(θ)\n(20)\nRemark 2: With η ≤min{ 1\n√\nT , 1\n6γ }, model converges to a\nneighborhood of a stationary point of FL with a small gap\ndue to layer selection-based noise ξ and variance of selec-\ntion count sd over clients. This motivates us to jointly min-\nimize the influence of ξ and sd for better convergence.\n5.2. Multi-objective Optimization\nMotivated by this, we refine the selected layers on server\n(see Fig.\n6) to achieve two primary objectives (see Eq.\n21): maximizing the cumulative client-specific importance\nscores based on LNTK and minimizing the variance of layer\nselection histogram (see Fig. 7) to encourage more balanced\nusage of each layer over clients. Let nl represent the num-\nber of clients that select layer l, with ¯n =\n1\nL\nPL\nl=1 nl as\nthe mean count of layer usage. (See Suppl. §A for more\ndetails.) The joint optimization problem is formulated as:\n\n\n\n\n\n\n\nmax\nN\nP\ni=1\nL\nP\nl=1\nSl\ni\nmin\n1\nL\nL\nP\nl=1\n(nl −¯n)2\ns.t:\nX\nl∈Li\nml\ni ≤Li,max,\n∀i ∈N\n(21)\nThe latter objective prevents over-reliance on specific layers\neven if they have high importance scores thereby increasing\ndiversity in layer selection across clients. The constraint\nincorporates client-specific computational budget Li,max.\nTraditional optimization methods struggle to optimize\nthese two conflicting objectives. Neural networks cannot be\n\nFigure 6. Visualization of refined layer ranks of LLaVA-1.5 based on F 3OCUS. Comparing it with Fig. 5 shows that the server-level\nmeta-heuristic optimization refines the client-level layer ranks by increasing inter-client layer diversity at every round.\nFigure 7. Layer selection histogram shows the impact of server-\nlevel optimization. It encourages more layers to participate (see\nblue-marked areas) by maximizing layer-selection diversity i.e.,\nreducing the variance of the selection count (vertical axis).\nemployed to optimize due to absence of data on server. Be-\nsides, the number of possible configurations is particularly\nhigh due to the large number of layers in foundation mod-\nels. Since this problem involves multiple objectives, there is\nno ”best” solution but rather a set of optimal trade-offs (the\nPareto front). Meta-heuristic algorithms are well-suited to\nexplore such complex, high-dimensional solution spaces in\nabsence of training data by balancing exploration and ex-\nploitation. To this end, we carefully select and investigate\n5 meta-heuristic algorithms spanning all 3 algorithm cate-\ngories: evolutionary, physics-based, and swarm-based. See\nSuppl. §A for detailed description and pseudo-code.\n5.3. Meta-heuristic algorithms\nEach of the following algorithms aim to maximize client-\nspecific importance while ensuring balanced layer utiliza-\ntion across clients based on their underlying principles:\n(1) Non-Dominated Sorting Genetic Algorithm (NSGA):\nWe use NSGA [20] to iteratively evolve a population of\nlayer selections. We initialize population based on client-\nspecific importance scores while incorporating probability-\nbased sampling for broader search space coverage. This\nguided randomness helps balance exploration (diversify-\ning choices) and exploitation (prioritizing high-importance\nlayers). Genetic operations like crossover and mutation\ngenerate new solutions, while non-dominated sorting and\ncrowding distance ensure diversity on the Pareto Front.\n(2) Artificial Bee Colony (ABC) [4]: Each bee represents\na potential layer selection for the clients. The optimization\nproceeds in three phases: Employed bees exploit local so-\nlutions, adjusting layer assignments based on importance\nscores and diversity, occasionally accepting worse solutions\nto escape local optima. Onlooker bees then select solutions\nbased on a probability weighted by importance and diver-\nsity, while Scout bees abandon unproductive solutions, re-\nplacing them with randomly generated ones to promote ex-\nploration. Non-dominated solutions are stored in a Pareto\narchive, which guides refinement over iterations.\n(3) Ant Colony Optimization (ACO) [21]: Each ant rep-\nresents a candidate layer selection, constructing paths in-\nfluenced by pheromone trails (which reinforce success-\nful layer choices from previous iterations) and importance\nscores (which guide ants toward layers that are likely ben-\neficial based on client-specific requirements). The Pareto\narchive preserves non-dominated solutions.\nPheromone\ntrails are then updated, with evaporation to prevent stale\npaths and additional pheromone deposits on layers selected,\nthereby encouraging their selection in subsequent iterations.\n(4) Simulated Annealing (SA): SA [71] starts with a high-\ntemperature, randomly initialized solution, gradually ex-\nploring optimal layer selections by cooling over iterations.\nIt accepts new configurations if it offers higher importance\nor lower variance based on Eq. 21. If the new configuration\nis less optimal, it may still be accepted based on a probabil-\nity that decreases with the temperature allowing SA to avoid\nbeing trapped in local optima. As the temperature cools, SA\ngradually refines the search, focusing on fine-tuning layer\nassignments that respect both client-specific importance and\na balanced distribution of layer usage.\n(5)\nMulti-Objective\nParticle\nSwarm\nOptimization\n(MOPSO) [18]: Each particle in the swarm represents a\ncandidate layer assignment, initialized with client-specific\n\nimportance scores to guide the selection of important\nlayers.\nRandomized probability-based sampling is used\nto ensure diverse initial positions, promoting exploration\nof the solution space.\nEach particle updates its veloc-\nity and position by balancing three influences: inertia\n(maintaining its current layer selection), a cognitive com-\nponent (best individual solution), and a social component\n(globally optimal solution).\n6. Experiments and Results\n6.1. FL settings, Datasets and Tasks\nWe evaluate our performance for fine-tuning selected (i)\nlayers, and (ii) adapters [29] with 4 VLMs of varying size\nand architecture, viz., ViLT [35], ALBEF [45], LlAVA-1.5\n[51], and BLIP-2 [46], for 3 FL task settings: (a) Visual\nQuestion Answering, (b) Image and Text-based Disease\nClassification, (c) Heterogeneous tasks combining (a), (b).\n(a) Visual Question Answering: We consider 3 scenarios\nwith data of varying sizes, class counts, and complexity:\n(i) Task 1 (with Domain gap): Five-client setting with\nSLAKE [50], VQA-RAD [37], VQA-Med 2019 [6],\nVQA-Med 2020 [1], and VQA-Med 2021 [7].\n(ii) Task 2 (with Modality gap): Modality specific 8-client\nsetting with CT, Ultrasound, Dermatoscopy, Fundus, His-\ntology, Microscopy, OCT, and X-Ray clients.\n(iii) Task 3 (with Modality gap): Modality specific 9-client\nsetting with our Ultra-MedVQA dataset (see Fig. 8).\n(b) Image and text-based multi-label disease classifica-\ntion: We consider 2 FL settings [62] with Dirichlet coeffi-\ncient γ = 0.5 for Chest X-Ray and Radiology report-based\nmulti-label disease detection (with 15 classes).\n(i) Task 4 (with label shift): 4 client-scenario with Open-I\n(ii) Task 5 (w/ label shift): 10 client scenario with MIMIC.\n(c) Heterogeneous tasks: We consider Task 6 (with task\nheterogeneity) combining three Visual Question answering\nclients, viz., SLAKE, VQA-RAD, VQA-Med 2019, and two\ndisease-classification clients, viz., Open-I and MIMIC.\nDevice Heterogeneity: In Tab. 2 and 5, to simulate varying\nresource constraints among clients, we adjust the number\nof trainable layers across different tasks. For Tasks 1 and\n6, 6 layers are finetuned for 2 clients, 4 layers for another 2\nclients, and 2 layers for the remaining client. For Task 2, 6\nlayers are finetuned for 3 clients, 4 layers for 3 clients, and\n2 layers for the last 2 clients. For Task 3, 2 layers are fine-\ntuned for 3 clients, 4 layers for another 3 clients and 6 layers\nfor the last 3 clients. For Task 4, 6 layers are finetuned for\n1 client, 4 layers for another client, and 2 layers for the re-\nmaining 2 clients. For Task 5, 6 layers are finetuned for 3\nclients, 4 layers for 1 client, and 2 layers for the last client.\nSee Suppl. §C for dataset and implementation details\n6.2. Performance comparison with State-of-the-arts\nWe compare F 3OCUS with 28 SOTA methods:\n(i) Tab. 3 shows comparison with 5 SOTA PEFT base-\nlines, viz., LayerNorm Tuning (LN) [5], LoRA [30], Bias\nTuning [11], Prompt Tuning [25], and FedDAT [14] in\nterms of communication (MBits), Computation (GFLOPs),\ntotal number of trainable parameters (Millions) and accu-\nracy in each client. F 3OCUS is observed to outperform\nall PEFTs except adapters [29] and FedDAT which finetune\nall adapters whereas F 3OCUS finetunes only selected 4\nadapter layers in each client leading to reduced communica-\ntion (9.7 MBits) and computational needs (80.6 GFLOPs).\n(ii) In Tab. 6, F 3OCUS is seen to consistently outperform\n12 SOTA Personalized FL baselines viz. perFedavg [23],\nMetaFed [17], FedPAC [75], FedAS [78], FLUTE [52],\nFedALA [81], FedProto [68], FedRod [15], FedAP [53],\nFedFomo [85], FedRep [19], and Fedper [3] for 5 tasks.\n(iii) We adapt 7 SOTA Pruning baselines viz. Federated\nDrop-out [74], Magnitude [26], FishMask [66], GradFlow\n[54], GraSP [72], SNIP [40], and Synflow [69] in our con-\ntext and compare with proposed method in Tabs. 2 and 5.\nLNTK surpasses the performance of the closest SOTA prun-\ning method by 2.11% and 2.30% while F 3OCUS outper-\nforms it by 5.35% and 5.33% respectively for homogeneous\nand heterogeneous device settings over all architectures.\n(iv) We also compare with 4 SOTA Layer Selection base-\nlines viz. Adapter-drop (denoted as ’last’) [61], RGN [42],\nFedselect [67], and SPT [27] (see Tabs. 2, 5). From Ta-\nble 2, we observe that LNTK outperforms the closest SOTA\nmethod by 2.08% and 2.17% for homogeneous and hetero-\ngeneous resource settings respectively. F 3OCUS further\nimproves performance over LNTK by 3.24% and 3.03%.\n6.3. Other Experimental Results\nWe plot the loss curves of LNTK and F 3OCUS and com-\npare them with several pruning methods in Fig. 2, which\ndemonstrates the impact of our server-level optimization on\nfaster convergence. We also visualize the principal eigen-\nvalue magnitudes of all 32 layers of LLaVA across different\nclients at the beginning of a round for Task 1 in Fig. 3\nand show the layer ranks over all rounds in Fig. 5. Con-\nsequently, the effect of server-level optimization on overall\nlayer selections is shown in Fig. 6 using relative layer ranks\nand in Fig. 7 using the layer selection histogram. In Fig.\n9, we motivate the current work by revealing that, contrary\nto common belief, fine-tuning the last ’K’ layers is only as\neffective as fine-tuning random ’K’ layers. Table 4 shows\nthe comparable performance of different meta-heuristic al-\ngorithms for all clients in Task 2. For the ’microscopy’\nclient in the same task, we present t-SNE feature visualiza-\ntions in Fig. 10 for (a) Federated Dropout (labeled random),\n(b) LNTK, and (c) F 3OCUS, highlighting the striking im-\nprovement in generating distinctive feature representations.\n\nFigure 8. Sample VQA triplets of 9 modality-specific medical clients from Ultra-MedVQA (Task 3)\nTable 2. Performance comparison on VLM adapter layer selection in terms of accuracy for Tasks 1-3 and F1-score for Tasks 4-6\nFine-tuning\nTask 1\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nMean Score\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nViLT LlAVA BLIP\nAll adapters\n43.04 40.02 43.36\n82.46 79.70 79.03\n78.55 75.19 76.40\n63.05 76.89 71.05\n65.68 77.93 76.30\n85.83 88.02 89.26\n69.77 72.96 72.57\nHomogeneous resources across clients (L=4 for all clients)\nFD [74]\n33.54 33.31 34.15\n73.94 68.84 68.78\n70.36 67.04 68.54\n52.82 67.78 60.21\n56.20 67.99 66.53\n76.90 79.05 80.36\n60.63 64.00 63.10\nLast [61]\n33.91 33.04 35.19\n70.53 65.63 66.92\n68.32 65.03 66.59\n54.40 65.94 58.45\n57.77 66.05 64.20\n77.62 79.51 79.96\n60.42 62.53 61.88\nMagnitude [26]\n31.60 32.40 30.26\n70.32 67.35 67.67\n69.18 68.73 69.02\n54.33 66.59 61.80\n56.01 66.90 64.98\n77.14 80.26 80.44\n59.76 63.71 62.36\nFishMask [66]\n37.45 35.34 37.77\n75.65 72.37 71.38\n71.99 71.63 72.10\n56.02 72.04 65.35\n59.20 71.86 70.22\n79.98 83.03 82.32\n63.38 67.71 66.52\nGradFlow [54]\n36.03 34.29 38.35\n74.81 72.65 72.58\n72.29 69.88 71.09\n56.38 71.19 64.98\n59.82 71.60 70.13\n80.33 82.89 82.56\n63.28 67.08 66.61\nGraSP [72]\n35.46 34.90 38.88\n75.03 72.03 71.74\n72.03 71.39 71.76\n55.83 70.73 65.43\n60.00 70.07 69.20\n81.02 83.48 82.19\n63.23 67.10 66.53\nSNIP [40]\n31.26 32.73 35.36\n73.96 69.40 70.15\n72.16 67.04 68.76\n54.21 66.07 60.39\n58.09 67.24 66.92\n78.20 80.34 81.13\n61.31 63.80 63.78\nRGN [42]\n32.71 32.52 34.81\n73.60 69.94 70.55\n70.88 68.75 69.73\n56.40 67.25 61.32\n57.54 68.64 68.39\n76.79 78.05 80.09\n61.32 64.19 64.15\nSynflow [69]\n35.68 35.38 37.89\n75.26 72.33 73.25\n72.49 71.36 71.63\n56.11 71.67 64.28\n59.27 70.17 71.32\n80.35 79.09 81.90\n63.19 66.67 66.71\nFedselect [67]\n36.80 34.48 38.88\n74.29 70.49 71.63\n71.29 70.52 70.06\n55.83 70.86 65.08\n58.53 71.33 72.02\n79.39 83.22 83.00\n62.69 66.82 66.78\nSPT [27]\n35.59 34.40 38.53\n75.50 72.16 71.32\n72.58 70.78 71.56\n56.74 71.98 65.71\n59.71 71.45 71.35\n80.13 83.89 82.99\n63.38 67.44 66.91\nLNTK (ours)\n39.74 36.80 40.43\n77.54 74.64 74.49\n74.01 72.68 73.93\n58.07 73.06 67.80\n61.50 73.00 73.43\n82.04 85.79 84.84\n65.48 69.33 69.15\nF 3OCUS (ours)\n42.41 39.85 43.04\n81.31 78.78 78.70\n77.86 75.01 76.45\n62.51 76.70 70.35\n64.62 77.26 76.14\n85.28 87.53 88.30\n69.00 72.52 72.16\nHeterogeneous resources across clients\nFD [74]\n32.65 33.54 34.04\n73.74 69.36 68.16\n70.27 66.73 67.81\n52.18 67.51 60.48\n55.56 67.71 66.16\n76.85 78.57 79.54\n60.21 63.90 62.70\nLast [61]\n33.65 33.80 35.30\n70.90 65.81 66.97\n68.22 65.92 66.31\n54.89 66.35 58.43\n57.91 65.47 64.75\n77.86 78.80 80.12\n60.57 62.69 61.98\nMagnitude [26]\n32.04 32.54 30.81\n69.84 67.00 67.53\n68.54 68.48 68.57\n53.83 66.19 61.46\n56.12 66.30 65.11\n77.68 80.48 80.78\n59.68 63.50 62.38\nFishMask [66]\n37.43 35.73 37.78\n75.60 72.01 71.70\n71.78 71.34 71.45\n56.58 72.08 65.17\n59.40 72.25 70.37\n80.49 82.66 82.01\n63.55 67.68 66.41\nGradFlow [54]\n35.17 34.86 38.43\n75.41 73.16 71.99\n72.45 70.40 71.20\n56.33 70.91 65.17\n59.44 71.48 70.19\n80.66 83.39 82.96\n63.24 67.37 66.66\nGraSP [72]\n36.11 35.20 38.68\n74.99 72.38 70.99\n71.54 71.54 71.98\n55.50 71.12 65.24\n59.50 69.63 69.17\n81.85 82.63 81.48\n63.25 67.08 66.26\nSNIP [40]\n30.53 33.39 36.24\n73.96 68.70 70.33\n72.00 66.63 68.03\n54.02 66.76 60.09\n58.82 67.15 66.90\n78.01 79.66 81.14\n61.22 63.72 63.79\nRGN [42]\n31.82 32.64 35.14\n73.74 70.12 70.52\n70.49 69.04 69.13\n56.02 67.68 61.42\n58.22 69.21 68.50\n77.04 78.52 79.75\n61.22 64.54 64.08\nSynflow [69]\n35.80 36.02 38.33\n75.70 72.17 72.57\n72.63 71.47 70.97\n57.02 72.26 64.49\n59.23 70.09 70.65\n80.40 78.53 82.78\n63.46 66.76 66.63\nFedselect [67]\n36.59 34.21 39.00\n74.85 69.71 70.94\n71.12 70.77 69.85\n55.71 70.70 64.35\n58.07 71.51 71.79\n79.26 83.42 83.45\n62.60 66.72 66.56\nSPT [27]\n35.35 34.72 38.58\n75.34 72.84 71.82\n72.92 70.87 72.10\n56.23 72.65 65.38\n59.38 72.05 71.20\n80.41 83.82 82.44\n63.27 67.82 66.92\nLNTK (ours)\n39.24 37.56 40.02\n77.59 74.55 74.96\n73.14 73.37 74.68\n58.12 73.40 67.50\n62.32 73.34 73.65\n82.44 86.32 84.96\n65.48 69.76 69.29\nF 3OCUS (ours)\n41.80 40.06 43.42\n81.85 77.83\n79.13\n77.16 75.04 76.33\n62.18 76.69 71.08\n64.42 77.59 75.73\n85.28 87.56 88.45\n68.78 72.46 72.36\nFigure 9. Performance of last ’K’ and random ’K’ adapters for\n32-layered LLaVA-1.5-7b on Task 4\nTable 3. Comparison with other PEFTs on Task 1 using ALBEF\nMethod\nMBit GFLOP Param(M)\nSLAKE VM2019 VM2020 VM2021 VR\nOverall\nFull\n3915.2\n156.5\n122.35\n77.45\n67.25\n15.06\n22.00\n41.52\n46.92\nAdap (all)\n28.8\n85.2\n0.90\n72.82\n64.45\n11.56\n21.00\n38.35\n43.17\nFedDAT\n86.1\n86.3\n2.69\n72.79\n63.58\n12.46\n23.00\n38.91\n43.93\nLN\n5.8\n84.7\n0.18\n69.53\n62.22\n10.59\n22.00\n36.89\n41.30\nLoRA\n19.5\n84.6\n0.61\n57.73\n60.18\n4.59\n15.00\n31.16\n35.09\nBias\n3.5\n84.7\n0.11\n68.25\n55.61\n11.17\n17.00\n35.47\n39.54\nPromptFL\n19.2\n85.0\n0.60\n65.63\n57.56\n4.78\n15.00\n40.26\n36.65\nF 3OCUS\n9.7\n80.6\n0.30\n74.69\n60.05\n12.84\n24.00\n42.30\n42.78\nTable 4. Comparison of meta-heuristic methods (Task 2 )\nFinetune\nC1\nC2\nC3\nC4\nC5\nC6\nC7\nC8\nOverall\n(CT) (US) (OCT) (Fundus) (Micro.) (Hist.) (Derma.) (XRay)\nNSGA\n88.67 79.41 78.29\n83.48\n70.68\n88.49\n67.69\n73.93\n78.78\nABC\n91.33 74.51 77.71\n85.71\n72.73\n88.70\n62.31\n76.55\n78.70\nACO\n86.67 78.10 78.86\n83.48\n70.68\n88.08\n66.92\n76.00\n78.60\nSA\n88.67 72.59 70.86\n85.93\n71.09\n89.45\n66.62\n76.48\n77.71\nMOPSO\n86.18 77.31 78.83\n84.53\n71.18\n87.58\n65.49\n75.97\n78.38\nSee Suppl. §D for more results and detailed discussions.\n\nTable 5. Performance comparison on VLM layer selection with heterogeneous resources across clients\nFine-tuning\nTask 1\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6\nViLT BLIP\nViLT BLIP\nViLT BLIP\nViLT BLIP\nViLT BLIP\nViLT BLIP\nFD [74]\n31.91 33.07\n75.75 70.03\n72.84 70.13\n50.25 58.12\n56.80 68.10\n77.65 82.70\nLast [61]\n32.75 34.31\n76.65 68.35\n72.63 68.56\n53.77 56.46\n59.07 66.25\n80.75 81.70\nMagnitude [26]\n31.35 30.96\n73.26 69.18\n70.88 71.02\n52.17 58.46\n58.25 67.04\n77.51 82.27\nFishMask [66]\n36.35 36.82\n77.71 73.56\n74.60 73.61\n54.35 63.08\n61.10 73.39\n81.90 83.09\nGradFlow [54]\n36.20 37.29\n77.09 74.06\n74.82 73.90\n54.18 62.58\n60.70 71.76\n81.22 82.24\nGraSP [72]\n35.36 37.87\n76.90 73.00\n73.80 74.83\n53.44 62.68\n60.91 70.85\n82.03 82.54\nSNIP [40]\n29.65 35.23\n75.59 73.88\n74.37 72.14\n51.91 58.04\n62.04 68.85\n79.25 82.38\nRGN [42]\n32.89 36.21\n75.34 72.35\n72.76 71.59\n53.80 62.16\n59.58 70.23\n77.95 81.14\nSynflow [69]\n34.99 37.39\n77.95 74.05\n74.81 73.79\n55.09 62.08\n60.97 72.10\n81.30 83.65\nFedselect [67]\n35.79 38.00\n76.66 72.80\n73.45 72.60\n53.55 61.87\n59.34 71.63\n80.29 84.10\nSPT [27]\n34.33 37.78\n77.03 73.66\n75.18 74.80\n54.18 63.03\n60.60 72.68\n81.53 83.97\nLNTK\n37.32 39.22\n79.15 76.47\n75.44 76.99\n55.81 65.40\n64.12 75.26\n83.66 86.20\nF 3OCUS\n40.85 42.37\n84.25 80.46\n79.65 78.95\n60.29 69.79\n65.86 78.94\n86.40 89.69\nTable 6. Comparison with SOTA personalized FL (L=4)\nTask perFedavgMetaFedFedPACFedASFLUTEFedALAFedProtoFedRodFedAPFedFomoFedRepFedperF 3OCUS\n1\n33.87\n35.42\n36.17\n36.46\n30.83\n35.50\n35.46\n36.23\n36.84\n35.44\n35.30\n36.14\n42.41\n2\n71.61\n75.46\n76.29\n76.51\n62.33\n74.56\n74.98\n77.65\n77.42\n75.13\n75.49\n75.93\n81.31\n4\n54.56\n57.26\n57.53\n58.78\n44.84\n57.18\n57.00\n59.35\n59.49\n57.05\n58.36\n58.08\n62.51\n5\n57.64\n59.15\n58.73\n59.17\n47.88\n57.42\n57.38\n58.77\n60.44\n58.34\n57.01\n57.58\n64.62\n6\n73.28\n76.27\n78.17\n78.51\n63.19\n75.88\n77.04\n76.10\n76.33\n75.66\n76.55\n76.76\n85.28\nFigure 10. t-SNE feature visualization for Client 6 of Task 2 (with\n26 classes) shows greater separability for F 3OCUS.\n7. Conclusion\nWe\npresented\nF 3OCUS,\na\nnovel\nand\ntheoretically\ngrounded approach for federated fine-tuning of Vision-\nLanguage foundation models in client-specific resource-\nconstrained settings. F 3OCUS effectively balances indi-\nvidual client requirements with collective layer diversity\nthereby improving model convergence and performance ac-\ncuracy. Our server-level multi-objective meta-heuristic op-\ntimization scheme does not require any data on server and\ncan be easily combined with any existing layer selection\nor pruning algorithms.\nAdditionally, we released Ultra-\nMedVQA, the largest medical VQA dataset, covering 12\nanatomies, for supporting further VLM research. Experi-\nmental results demonstrate that F 3OCUS achieves supe-\nrior performance across multiple vision-language tasks and\nmodels, providing a practical solution for fine-tuning large\nfoundation models in decentralized environments.Our code\nand dataset will be made available upon acceptance.\nReferences\n[1] Abeed S. Ben Abacha, Vivek V. Datla, Sadid A. Hasan, Dina\nDemner-Fushman, and Henning M¨uller.\nOverview of the\nvqa-med task at imageclef 2020: Visual question answer-\ning and generation in the medical domain. In CLEF 2020\nWorking Notes. 8\n[2] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro,\nMatthew Mattina, Paul N Whatmough, and Venkatesh\nSaligrama.\nFederated learning based on dynamic regular-\nization. arXiv preprint arXiv:2111.04263, 2021. 1, 3\n[3] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku-\nmar Singh, and Sunav Choudhary. Federated learning with\npersonalization layers.\narXiv preprint arXiv:1912.00818,\n2019. 8\n[4] Jagdish Chand Bansal, Harish Sharma, and Shimpi Singh\nJadon. Artificial bee colony algorithm: a survey. Interna-\ntional Journal of Advanced Intelligence Paradigms, 5(1-2):\n123–159, 2013. 7\n[5] Samyadeep Basu, Daniela Massiceti, Shell Xu Hu, and So-\nheil Feizi. Strong baselines for parameter efficient few-shot\nfine-tuning. arXiv preprint arXiv:2304.01917. 8\n[6] Asma Ben Abacha, Sadid A Hasan, Vivek V Datla,\n\nDina Demner-Fushman, and Henning M¨uller.\nVqa-med:\nOverview of the medical visual question answering task at\nimageclef 2019. In Proceedings of CLEF (Conference and\nLabs of the Evaluation Forum) 2019 Working Notes, 2019. 8\n[7] Asma Ben Abacha,\nMourad Sarrouti,\nDina Demner-\nFushman, Sadid A Hasan, and Henning M¨uller. Overview of\nthe vqa-med task at imageclef 2021: Visual question answer-\ning and generation in the medical domain. In Proceedings\nof the CLEF 2021 Conference and Labs of the Evaluation\nForum-working notes, 2021. 8\n[8] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bit-\nFit: Simple parameter-efficient fine-tuning for transformer-\nbased masked language-models. In Proceedings of the Asso-\nciation for Computational Linguistics (Volume 2: Short Pa-\npers), pages 1–9, Dublin, Ireland, 2022. 2\n[9] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay,\nAlexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar\nMa˜nas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman,\nMark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan\nLebensold, Candace Ross, Srihari Jayakumar, Chuan Guo,\nDiane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu\nSharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards,\nSamuel Lavoie, Pietro Astolfi, Reyhane Askari Hemmat, Jun\nChen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Ar-\njang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen,\nQuentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate\nSaenko, Asli Celikyilmaz, and Vikas Chandra. An introduc-\ntion to vision-language modeling, 2024. 1\n[10] Benjamin Bowman. A brief introduction to the neural tan-\ngent kernel. 2023. 4\n[11] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl:\nReduce memory, not parameters for efficient on-device\nlearning. In Advances in Neural Information Processing Sys-\ntems, pages 11285–11297, 2020. 8\n[12] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and\nQuanquan Gu. Towards understanding the spectral bias of\ndeep learning. arXiv preprint arXiv:1912.01198, 2019. 4\n[13] Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, and\nYaliang Li.\nEfficient personalized federated learning via\nsparse model-adaptation.\nIn International Conference on\nMachine Learning, pages 5234–5256. PMLR, 2023. 3\n[14] Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu,\nand Volker Tresp. Feddat: An approach for foundation model\nfinetuning in multi-modal heterogeneous federated learning.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 11285–11293, 2024. 8\n[15] Hong-You Chen and Wei-Lun Chao. On bridging generic\nand personalized federated learning for image classification.\narXiv preprint arXiv:2107.00778, 2021. 8\n[16] Jinyu Chen, Wenchao Xu, Song Guo, Junxiao Wang, Jie\nZhang, and Haozhao Wang. Fedtune: A deep dive into ef-\nficient federated fine-tuning with pre-trained transformers.\narXiv preprint arXiv:2211.08025, 2022. 2, 3\n[17] Yiqiang Chen, Wang Lu, Xin Qin, Jindong Wang, and Xing\nXie. Metafed: Federated learning among federations with\ncyclic knowledge distillation for personalized healthcare.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 2023. 8\n[18] CA Coello Coello and Maximino Salazar Lechuga. Mopso:\nA proposal for multiple objective particle swarm optimiza-\ntion. In Proceedings of the 2002 Congress on Evolutionary\nComputation. CEC’02 (Cat. No. 02TH8600), pages 1051–\n1056. IEEE, 2002. 7\n[19] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay\nShakkottai. Exploiting shared representations for personal-\nized federated learning. In International conference on ma-\nchine learning, pages 2089–2099. PMLR, 2021. 8\n[20] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT\nMeyarivan. A fast and elitist multiobjective genetic algo-\nrithm: Nsga-ii. IEEE transactions on evolutionary computa-\ntion, 6(2):182–197, 2002. 7\n[21] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant\ncolony optimization. IEEE computational intelligence mag-\nazine, 1(4):28–39, 2006. 7\n[22] Chen Dun, Cameron R Wolfe, Christopher M Jermaine, and\nAnastasios Kyrillidis. Resist: Layer-wise decomposition of\nresnets for distributed training. In Uncertainty in Artificial\nIntelligence, pages 610–620. PMLR, 2022. 3\n[23] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per-\nsonalized federated learning with theoretical guarantees: A\nmodel-agnostic meta-learning approach. Advances in neural\ninformation processing systems, 33:3557–3568, 2020. 8\n[24] Jonathan Frankle, David J. Schwab, and Ari S. Morcos.\nTraining batchnorm and only batchnorm: On the expressive\npower of random features in cnns, 2021. 2\n[25] Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and\nWenchao Xu. Promptfl: Let federated participants cooper-\natively learn prompts instead of models-federated learning\nin age of foundation model. IEEE Transactions on Mobile\nComputing, 2023. 8\n[26] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\ning both weights and connections for efficient neural net-\nwork. Advances in neural information processing systems,\n28, 2015. 8, 9, 10\n[27] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan\nZhuang.\nSensitivity-aware visual parameter-efficient fine-\ntuning. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 11825–11835, 2023. 8,\n9, 10\n[28] Agrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon Ren´e\nS¨utfeld, Edvin Listo Zec, and Olof Mogren. Scaling feder-\nated learning for fine-tuning of large language models. In In-\nternational Conference on Applications of Natural Language\nto Information Systems, pages 15–23. Springer, 2021. 3\n[29] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for nlp.\nIn International conference on machine\nlearning, pages 2790–2799. PMLR, 2019. 8\n[30] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n2, 8\n[31] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neu-\nral tangent kernel: Convergence and generalization in neural\n\nnetworks. Advances in neural information processing sys-\ntems, 31, 2018. 4\n[32] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In Computer Vision – ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXXIII, page 709–727. Springer-Verlag,\n2022. 2\n[33] Gal Kaplun, Andrey Gurevich, Tal Swisa, Mazor David,\nShai Shalev-Shwartz, and Eran Malach.\nLess is more:\nSelective layer finetuning with subtuning.\narXiv preprint\narXiv:2302.06354, 2023. 3\n[34] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,\nSashank Reddi, Sebastian Stich, and Ananda Theertha\nSuresh. Scaffold: Stochastic controlled averaging for feder-\nated learning. In International conference on machine learn-\ning. PMLR, 2020. 1, 3, 5\n[35] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In International conference on machine learning,\npages 5583–5594. PMLR, 2021. 1, 8\n[36] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna\nRumshisky. Revealing the dark secrets of bert. arXiv preprint\narXiv:1908.08593, 2019. 3\n[37] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina\nDemner-Fushman. A dataset of clinically generated visual\nquestions and answers about radiology images.\nScientific\ndata, 5(1):1–10, 2018. 8\n[38] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\nMixout: Effective regularization to finetune large-scale pre-\ntrained language models.\nIn International Conference on\nLearning Representations. 3\n[39] Jaejun Lee, Raphael Tang, and Jimmy Lin. What would elsa\ndo? freezing layers during transformer fine-tuning. arXiv\npreprint arXiv:1911.03090, 2019. 3\n[40] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS\nTorr. Snip: Single-shot network pruning based on connec-\ntion sensitivity. arXiv preprint arXiv:1810.02340, 2018. 2,\n3, 8, 9, 10\n[41] Sunwoo Lee, Tuo Zhang, and A Salman Avestimehr. Layer-\nwise adaptive model aggregation for scalable federated\nlearning. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, pages 8491–8499, 2023. 3\n[42] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Ku-\nmar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical\nfine-tuning improves adaptation to distribution shifts. arXiv\npreprint arXiv:2210.11466, 2022. 2, 3, 8, 9, 10\n[43] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-efficient prompt tuning. arXiv preprint\narXiv:2104.08691. 2\n[44] Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li,\nZhengang Li, Hang Liu, and Caiwen Ding.\nEfficient\ntransformer-based large scale language representations using\nhardware-friendly block structured pruning. arXiv preprint\narXiv:2009.08065, 2020. 3\n[45] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. Advances in neural infor-\nmation processing systems, 34:9694–9705, 2021. 1, 8\n[46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 1, 8\n[47] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia\nSmith. Federated learning: Challenges, methods, and future\ndirections. IEEE signal processing magazine, 37(3):50–60,\n2020. 1, 3\n[48] Xiang Lisa Li and Percy Liang.\nPrefix-tuning: Optimiz-\ning continuous prompts for generation.\narXiv preprint\narXiv:2101.00190, 2021. 2\n[49] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao\nWang.\nScaling & shifting your features: A new baseline\nfor efficient model tuning. arXiv preprint arXiv:2210.08823,\n2022. 2\n[50] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and\nXiao-Ming Wu. Slake: A semantically-labeled knowledge-\nenhanced dataset for medical visual question answering. In\n2021 IEEE 18th International Symposium on Biomedical\nImaging (ISBI), pages 1650–1654. IEEE, 2021. 8\n[51] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 1, 8\n[52] Renpu Liu, Cong Shen, and Jing Yang.\nFederated repre-\nsentation learning in the under-parameterized regime. arXiv\npreprint arXiv:2406.04596, 2024. 8\n[53] Wang Lu, Jindong Wang, Yiqiang Chen, Xin Qin, Renjun\nXu, Dimitrios Dimitriadis, and Tao Qin. Personalized feder-\nated learning with adaptive batchnorm for healthcare. IEEE\nTransactions on Big Data, 2022. 8\n[54] Ekdeep Singh Lubana and Robert P Dick. A gradient flow\nframework for analyzing network pruning. In International\nConference on Learning Representations. 2, 8, 9, 10\n[55] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\nHampson, and Blaise Aguera y Arcas.\nCommunication-\nefficient learning of deep networks from decentralized data.\nIn Artificial intelligence and statistics. PMLR, 2017. 1, 2, 3\n[56] John Nguyen, Kshitiz Malik, Maziar Sanjabi, and Michael\nRabbat.\nWhere to begin?\nexploring the impact of pre-\ntraining and initialization in federated learning.\narXiv\npreprint arXiv:2206.15387, 4, 2022. 3\n[57] Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed,\nMike Rabbat, Maziar Sanjabi, and Lin Xiao.\nFederated\nlearning with partial model personalization.\nIn Interna-\ntional Conference on Machine Learning, pages 17716–\n17758. PMLR, 2022. 3\n[58] John Rachwan, Daniel Z¨ugner, Bertrand Charpentier, Simon\nGeisler, Morgane Ayle, and Stephan G¨unnemann. Winning\nthe lottery ahead of time: Efficient early network pruning.\nIn International Conference on Machine Learning, pages\n18293–18309. PMLR, 2022. 3\n[59] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix\nDraxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and\n\nAaron Courville. On the spectral bias of neural networks. In\nInternational conference on machine learning, pages 5301–\n5310. PMLR, 2019. 4\n[60] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.\nEfficient parametrization of multi-domain deep neural net-\nworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018. 2\n[61] Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.\nAdapterdrop: On the efficiency of adapters in transformers.\narXiv preprint arXiv:2010.11918, 2020. 2, 8, 9, 10\n[62] Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos\nKamnitsas, and J. Alison Noble. Examining modality incon-\ngruity in multimodal federated learning for medical vision\nand language-based disease detection, 2024. 8\n[63] Zhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and\nKwang-Ting Cheng. Partial is better than all: Revisiting fine-\ntuning strategy for few-shot learning. In Proceedings of the\nAAAI conference on artificial intelligence, pages 9594–9602,\n2021. 3\n[64] Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang,\nDongsheng Li, Yujiang Wang, Robert Dick, Qin Lv, Yingy-\ning Zhao, Fan Yang, et al. Train faster, perform better: mod-\nular adaptive training in over-parameterized models.\nAd-\nvances in Neural Information Processing Systems, 36, 2024.\n4\n[65] Guangyu Sun, Matias Mendieta, Taojiannan Yang, and Chen\nChen. Exploring parameter-efficient fine-tuning for improv-\ning communication efficiency in federated learning. 2022.\n2\n[66] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neu-\nral networks with fixed sparse masks. Advances in Neural\nInformation Processing Systems, 34:24193–24205, 2021. 2,\n8, 9, 10\n[67] Rishub Tamirisa, Chulin Xie, Wenxuan Bao, Andy Zhou,\nRon Arel, and Aviv Shamsian. Fedselect: Personalized fed-\nerated learning with customized selection of parameters for\nfine-tuning.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 23985–\n23994, 2024. 8, 9, 10\n[68] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu,\nJing Jiang, and Chengqi Zhang. Fedproto: Federated proto-\ntype learning across heterogeneous clients. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, pages 8432–\n8440, 2022. 8\n[69] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya\nGanguli. Pruning neural networks without any data by iter-\natively conserving synaptic flow. Advances in neural infor-\nmation processing systems, 33:6377–6389, 2020. 8, 9, 10\n[70] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob\nVerbeek, and Herv´e J´egou. Three things everyone should\nknow about vision transformers. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XXIV. Springer, 2022. 2\n[71] Peter JM Van Laarhoven, Emile HL Aarts, Peter JM van\nLaarhoven, and Emile HL Aarts.\nSimulated annealing.\nSpringer, 1987. 7\n[72] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking\nwinning tickets before training by preserving gradient flow.\narXiv preprint arXiv:2002.07376, 2020. 2, 3, 8, 9, 10\n[73] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and\nH Vincent Poor. Tackling the objective inconsistency prob-\nlem in heterogeneous federated optimization.\nAdvances\nin neural information processing systems, 33:7611–7623,\n2020. 5\n[74] Dingzhu Wen, Ki-Jun Jeon, and Kaibin Huang. Federated\ndropout—a simple approach for enabling federated learning\non resource constrained devices. IEEE wireless communica-\ntions letters, 11(5):923–927, 2022. 8, 9, 10\n[75] Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized\nfederated learning with feature alignment and classifier col-\nlaboration. arXiv preprint arXiv:2306.11867, 2023. 8\n[76] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao\nChang, Songfang Huang, and Fei Huang. Raise a child in\nlarge language model: Towards effective and generalizable\nfine-tuning. arXiv preprint arXiv:2109.05687, 2021. 3\n[77] Mingzhao Yang, Shangchao Su, Bin Li, and Xiangyang Xue.\nExploring one-shot semi-supervised federated learning with\npre-trained diffusion models. In Proceedings of the AAAI\nConference on Artificial Intelligence, 2024. 2\n[78] Xiyuan Yang, Wenke Huang, and Mang Ye. Fedas: Bridg-\ning inconsistency in personalized federated learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11986–11995, 2024. 8\n[79] Sixing Yu, J Pablo Mu˜noz, and Ali Jannesari.\nFederated\nfoundation models:\nPrivacy-preserving and collaborative\nlearning for large models. arXiv preprint arXiv:2305.11414,\n2023. 2\n[80] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. arXiv preprint arXiv:2106.10199,\n2021. 3\n[81] Jianqing Zhang, Yang Hua, Hao Wang, Tao Song, Zhengui\nXue, Ruhui Ma, and Haibing Guan. Fedala: Adaptive local\naggregation for personalized federated learning. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, pages\n11237–11244, 2023. 8\n[82] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,\nRuiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. To-\nwards building the federatedgpt: Federated instruction tun-\ning.\nIn ICASSP 2024-2024 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2024. 2\n[83] Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xin-\nwei Long, and Bowen Zhou. Crash: Clustering, removing,\nand sharing enhance fine-tuning without full large language\nmodel. arXiv preprint arXiv:2310.15477, 2023. 3\n[84] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-\nYu Duan. Fine-tuning global model via data-free knowledge\ndistillation for non-iid federated learning. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10174–10183, 2022. 3\n[85] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Ye-\nung, and Jose M Alvarez.\nPersonalized federated learn-\n\ning with first order model optimization.\narXiv preprint\narXiv:2012.08565, 2020. 8\n[86] Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong,\nYanzhi Wang, and Xue Lin.\nPruning foundation mod-\nels for high accuracy without retraining.\narXiv preprint\narXiv:2410.15567, 2024. 3\n[87] Weiming Zhuang, Chen Chen, and Lingjuan Lyu.\nWhen\nfoundation model meets federated learning:\nMotiva-\ntions, challenges, and future directions.\narXiv preprint\narXiv:2306.15546, 2023. 2",
    "pdf_filename": "F$^3$OCUS_--_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_Client_Layer_Upd.pdf"
}