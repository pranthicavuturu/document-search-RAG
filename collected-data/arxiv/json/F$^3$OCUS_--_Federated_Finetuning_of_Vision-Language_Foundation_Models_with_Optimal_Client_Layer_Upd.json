{
    "title": "F3OCUS - Federated Finetuning of Vision-Language Foundation Models with",
    "abstract": "differentVLMarchitecturesofvaryingsizestodemonstrate theeffectivenessoftheproposedmethod. EffectivetrainingoflargeVision-LanguageModels(VLMs) onresource-constrainedclientdevicesinFederatedLearn- ing (FL) requires the usage of parameter-efficient fine- tuning (PEFT) strategies. To this end, we demonstrate the 1.Introduction impact of two factors viz., client-specific layer importance score that selects the most important VLM layers for fine- Large Vision-Language Models (VLMs) have made sig- tuningandinter-clientlayerdiversityscorethatencourages nificant advancements in multi-modal learning, excelling diverselayerselectionacrossclientsforoptimalVLMlayer in tasks like Visual Question Answering (VQA) [9, 35, selection. We first theoretically motivate and leverage the 45, 46, 51]. Their effectiveness stems from their exten- principal eigenvalue magnitude of layerwise Neural Tan- sive parameters often reaching millions or billions, allow- gent Kernels and show its effectiveness as client-specific ing them to learn complex representations of image and layer importance score. Next, we propose a novel layer textdata. Fine-tuningthesemodelswithtask-specificdata updating strategy dubbed F3OCUS that jointly optimizes is crucial for adapting them to specialized applications. the layer importance and diversity factors by employing However, gathering diverse training data centrally is chal- adata-free,multi-objective,meta-heuristicoptimizationon lenging, especially in fields like healthcare, where strict theserver. Weexplore5differentmeta-heuristicalgorithms privacy regulations prevent data aggregation across dif- and compare their effectiveness for selecting model layers ferent centers. To address the privacy concerns, Feder- andadapterlayerstowardsPEFT-FL.Furthermore,were- ated Learning (FL) [2, 34, 47, 55] allows models to be leaseanewMedVQA-FLdatasetinvolvingoverall707,962 traineddirectlyonlocaldevices,suchasinhealthcareclin- VQAtripletsand9modality-specificclientsandutilizeitto ics, without sharing sensitive data. Yet, fine-tuning large train and evaluate our method. Overall, we conduct more models locally is difficult due to limited computational than10,000client-levelexperimentson6Vision-Language powerandsmallerdatasets,whichhindersVLMadaptation. 4202 voN 71 ]VC.sc[ 1v21911.1142:viXra",
    "body": "F3OCUS - Federated Finetuning of Vision-Language Foundation Models with\nOptimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics\nPramitSaha FelixWagner DivyanshuMishra\nUniversityofOxford UniversityofOxford UniversityofOxford\npramit.saha@eng.ox.ac.uk felix.wagner@eng.ox.ac.uk divyanshu.mishra@eng.ox.ac.uk\nCanPeng AnshulThakur\nUniversityofOxford UniversityofOxford\ncan.peng@eng.ox.ac.uk anshul.thakur@eng.ox.ac.uk\nDavidClifton\nUniversityofOxford;Oxford-SuzhouCentreforAdvancedResearch\ndavid.clifton@eng.ox.ac.uk\nKonstantinosKamnitsas\nUniversityofOxford;ImperialCollegeLondon;UniversityofBirmingham\nkonstantinos.kamnitsas@eng.ox.ac.uk\nJ.AlisonNoble\nUniversityofOxford\nalison.noble@eng.ox.ac.uk\nAbstract FLtasksettingsinvolving58medicalimagedatasetsand4\ndifferentVLMarchitecturesofvaryingsizestodemonstrate\ntheeffectivenessoftheproposedmethod.\nEffectivetrainingoflargeVision-LanguageModels(VLMs)\nonresource-constrainedclientdevicesinFederatedLearn-\ning (FL) requires the usage of parameter-efficient fine-\ntuning (PEFT) strategies. To this end, we demonstrate the 1.Introduction\nimpact of two factors viz., client-specific layer importance\nscore that selects the most important VLM layers for fine- Large Vision-Language Models (VLMs) have made sig-\ntuningandinter-clientlayerdiversityscorethatencourages nificant advancements in multi-modal learning, excelling\ndiverselayerselectionacrossclientsforoptimalVLMlayer in tasks like Visual Question Answering (VQA) [9, 35,\nselection. We first theoretically motivate and leverage the 45, 46, 51]. Their effectiveness stems from their exten-\nprincipal eigenvalue magnitude of layerwise Neural Tan- sive parameters often reaching millions or billions, allow-\ngent Kernels and show its effectiveness as client-specific ing them to learn complex representations of image and\nlayer importance score. Next, we propose a novel layer textdata. Fine-tuningthesemodelswithtask-specificdata\nupdating strategy dubbed F3OCUS that jointly optimizes is crucial for adapting them to specialized applications.\nthe layer importance and diversity factors by employing However, gathering diverse training data centrally is chal-\nadata-free,multi-objective,meta-heuristicoptimizationon lenging, especially in fields like healthcare, where strict\ntheserver. Weexplore5differentmeta-heuristicalgorithms privacy regulations prevent data aggregation across dif-\nand compare their effectiveness for selecting model layers ferent centers. To address the privacy concerns, Feder-\nandadapterlayerstowardsPEFT-FL.Furthermore,were- ated Learning (FL) [2, 34, 47, 55] allows models to be\nleaseanewMedVQA-FLdatasetinvolvingoverall707,962 traineddirectlyonlocaldevices,suchasinhealthcareclin-\nVQAtripletsand9modality-specificclientsandutilizeitto ics, without sharing sensitive data. Yet, fine-tuning large\ntrain and evaluate our method. Overall, we conduct more models locally is difficult due to limited computational\nthan10,000client-levelexperimentson6Vision-Language powerandsmallerdatasets,whichhindersVLMadaptation.\n4202\nvoN\n71\n]VC.sc[\n1v21911.1142:viXra\nFigure2. Lossconvergenceoflayerselectionmethods. Thegap\nbetweentheclient-specificNTKandFOCUSdemonstratestheim-\nportanceofourmulti-objectivemeta-heuristicoptimization.\nproblematicforFLclientsfacingchallengeslikeheteroge-\nneous modalities and computes, domain shifts, and statis-\nFigure1. Distinctionofourapproachfrompriorworks. (a)illus- tical heterogeneity. In such cases, a poorly chosen client-\ntrates vanilla layer-selection process, which only selects param- specific configuration can not only slow down the overall\neter subsets based on the local client data without considering convergence(seeFig. 2),butmayalsoperformworsethan\nthe requirements of the other clients. (b) depicts our approach, trainingeachclientindependently. Achievingthebestper-\nF3OCUS, which refines the client-specific layer selection by formancerequiresatailoredapproachthatjointlyconsiders\njointly maximizing overall client-specific importance score and\nclient-specificaswellasglobaloptimizationrequirements.\nlayerselectiondiversityscoreacrossclients.\nTo this end, we present a novel framework called\nF3OCUS (Federated Finetuning of Foundation Models\nwith Optimal Client-specific Layer Updating Strategy) to\nBalancing privacy with these resource limitations requires improvelayerselectionbyconsideringbothlocalandglobal\ninnovative solutions like Parameter-Efficient Fine-Tuning FL characteristics while respecting the client-specific re-\n(PEFT)thatfine-tuneseitherselectedmodelparametersor source constraints. We propose a two-step “define and\nadded parameters while keeping the original model fixed refine” procedure at the beginning of every round: (a)\n[8, 24, 30, 32, 43, 48, 49, 60, 70]. Combined with FL, client-level strategy, that defines layer importance scores\ntheseofferaprivacy-preservingandresource-efficientstrat- basedontheprincipaleigenvalueoflayerwiseNeuralTan-\negyfortraininglargemodelscollaborativelyacrossmultiple gentKernel(LNTK)and(b)server-levelstrategy,thatre-\nclients,particularlyincomputationallyconstrainedsettings. finesclient-specificlayerselectionbymaximizingtheover-\nPreviousresearch[16,65,77,79,82,87]hasmostlyfo- all client-specific importance scores while simultaneously\ncusedonnaivecombinationofcentralizedfinetuningmeth- minimizing the variance of the histogram of layer selec-\nods with FedAvg [55]. However, these works are primar- tionsacrossclients,therebypromotingamoreuniformdis-\nily confined to single-modalities, addressing either visual tribution of layer participation. Our method (see Figs. 1\nor textual inputs independently. Besides, they do not con- & 4) provides the clients with a flexible and dynamic so-\nsiderthediversecharacteristicsandcapacitiesofindividual lution for selecting layers where each client can specify\nclients and typically assume homogeneous computational their computational budgets, while ensuring faster conver-\nresourcesacrossallclientswhichisnotapplicableinmost gence (see Fig. 2). In order to showcase the effectiveness\nreal-worldcollaborativesettings.Hence,someclientseither of F3OCUS, we conduct over 10,000 client-level exper-\nunder-utilizetheavailableresourcesorareunabletopartic- iments under 6 Vision-language FL task settings using 4\nipate due to lack of compute. Our flexible layer selection VLMs and 58 medical image datasets that involve 4 types\nPEFT-FLframeworkeffectivelyaddressestheseissues. ofheterogeneitiesbasedondata,modality,device,andtask.\nOurprimarycontributionscanbesummedupasfollows:\nThe naive combinations of centralized selective PEFT\nmethods [40, 42, 54, 61, 66, 72] and FL only consider lo- • Dataset contribution: We release Ultra-MedVQA, the\ncalclientdataandtaskforselectingparametersubsetswith- largest medical VQA dataset to date, consisting of\noutconsideringotherclientrequirements.Thisisespecially 707,962VQAtripletsincluding9differentmodalitiesand\n12 distinct anatomies, covering diverse open-ended and 3.ProblemFormulation\nclosed questions related to modality, tissue type, image\nConsideranFLsystemwithacentralserverandN clients,\nview,anatomy,anddisease(seeTab. 1forcomparison).\nrepresented by N = {1,...,N}. Each client has its own\n• Technicalcontributions: Wetheoreticallymotivateand\nprivate dataset D , containing d = |D | data points. The\npropose F3OCUS, a new selective layer fine-tuning i i i\nserver contains a pre-trained foundation model parameter-\nstrategy. We introduce LNTK-based client-level layer\nized by θ ∈ RP, comprising L layers, indexed by L =\nselection and server-level multi-objective meta-heuristic\n{1,2,...,L}. The server’s objective is to fine-tune this\noptimization that jointly optimizes client-specific layer\nmodelbasedontheclients’data{D } withoutdirectly\nimportance score and inter-client layer diversity score. i i∈N\naccessingthesedatasets.Thelearninggoalisformalizedas:\nWe theoretically motivate and analyze the effectiveness\nofourlayerselectionstrategyandproveitsconvergence.\nN\n(cid:88)\n• Empirical contributions: Unlike previous works, we min F(θ)= α F (θ), (1)\ni i\nconsider more constrained and realistic client settings\nθ∈RP\ni=1\ni in tyv .o Wlv ein cg ond da uta c, tcm oo md pa rl eit hy e, nt sa is vk e, eva an ld uad tie ov nic se ofh Fete 3r Oo Cge Une S- Fwh (e θr )e =α i 1=\n(cid:80)\n(cid:80)N jd =i 1d Fj (d θe ;n Bot )es rer pe rl ea st eiv ne tss la om cap ll te ras ii nz ie n, ga on bd\n-\nwith 4 VLMs of varying sizes in diverse FL settings for i di Bi∈Di i i\njective for client i, with F (θ;B ) being the (potentially\ntuningselectivemodellayersaswellasadapters. Weem- i i\nnon-convex)lossfunctionofmodelθondatabatchB .The\npiricallyshowthatfine-tuningthelastfewadapters/layers i\nFLtrainingprocessproceedsoverT rounds. Ineachround\nof VLMs is only as good as tuning randomly se-\nt ∈ [T], the server selects a subset of clients S and dis-\nlected adapters/layers. Consequently, we present de- t\ntributestheupdatedglobalmodelθ tothemfortraining.\ntailed insights into F3OCUS’s performance improve- t\nDuetoresourceconstraints,insteadoftheentiremodel,\nments through: (a) analysis of client and server-based\nclients update only a subset of layers during local train-\nlayer rank and importance score computation during\ning. Each client-specific masking vector can be denoted\ntrainingand(b)evaluationofdifferentmeta-heuristicop-\nas m ∈ {0,1}L, where ml = 1 if layer l is selected\ntimization algorithms on the server viz., Genetic Algo- i,t i,t\nfortraininginroundt, andml = 0otherwise. Thus, the\nrithm, Artificial Bee Colony, Ant Colony Optimization, i,t\nset of selected layers for client i at round t is denoted as\nSimulatedAnnealing,andSwarmParticleOptimization.\nL = {l ∈ L | ml = 1}, and the union of all selected\ni,t i,t\n(cid:83)\n2.BackgroundandRelatedworks layersacrossclientsinroundtisL t = i∈StL i,t.\nClients initialize their local models using global model\nFederated Learning (FL) FL enables various clients to from the server, θ , and perform τ steps of local training\ni,t\ncollaborativelytrainmodelsinadecentralizedmannerwith- usingmini-batchSGD.Foreachlocalstepk ∈ [τ],clienti\noutsharinglocaldata.TheclassicalFLframework,FedAvg samplesabatchofdataB andcalculatesgradientsforthe\ni,t\n[55],offersapracticalmethodformodelaggregation. Sev- selectedlayersas:\neralmodificationshaveemergedtoaddresstheadverseim-\npactofdataheterogeneityinFL[2,34,47]. (cid:88) G (θk ;Bk )= (cid:88) ∇F (θ ;B ), (2)\ni,l i,t i,t l i i,t i,t\nCentralized selective fine-tuning: Various methods have l∈Lt l∈Lt\ni i\nbeen explored for selecting subsets of parameters for fine-\nwhere∇ F(θ)denotesthegradientofF(θ)withrespect\nl\ntuning foundation models in centralized training. These\ntotheparametersoflayerl. Aftercomputingthegradients,\ninclude optimizing non-structured mask matrices [36, 38,\nthelocalmodelisupdatedwithlearningrateηas:\n39, 63, 76, 80, 83, 84], employing layer-wise selection\n[33,36,39,42]andpruningmethods[40,44,58,72,86].\nFederated selective fine-tuning: Recent research has θk =θk−1−η (cid:88) Gl(θk−1;Bk−1), ∀k∈{1,2,...,τ},\ni,t i,t i i,t i,t\nadapted these selective PEFT methods for FL [16, 28, 56, l∈Lt\ni\n84].Specifically,studiesby[22,41]explorelayer-wisenet- (3)\nworkdecompositiontofacilitateselectivemodelfine-tuning Theaccumulatedweightupdateinonelocalroundis:\nonclientdevices. Partialmodelpersonalizationalgorithms\nτ−1\n[13,57]aimtotraintailoredsubnetworksonclientstoim- δ = 1 (θ0 −θτ )=(cid:88)(cid:88) Gl(θk ;Bk ) (4)\ni,t η i,t i,t i i,t i,t\nprovelocalmodels. However,thesestudiesdonotprovide k=0l∈Lt\ni\nadaptiveordynamiclayerselectionstrategiesthatconsider\nAccumulatedupdateafterfederatedaggregationonserver:\nthediversecharacteristicsofclients.Unlikepriorworks,we\naccountforclient-specificdifferencesinresourcesanddata\nτ−1\ndistributionswhilealsoconsideringtheglobaloptimization δ = (cid:88)(cid:88)(cid:88) α Gl(θk ;Bk ) (5)\nt i i i,t i,t\nrequirementstoperformselectivelayerfine-tuning.\ni∈Stk=0l∈Lt\ni\nTable1.#modalities(M)andVQAtripletsindifferentdatasets\nVQA-RAD SLAKE Path-VQA VQA-Med OmniMedVQA Ours\n#M 3 3 2 5 12 9\n#VQA 3515 14028 32799 5500 127995 707962\nFigure3.VisualizationofprincipaleigenvaluemagnitudesofLNTK(see§4.1)forcomputinglayerimportancescoreofLLaVA-1.5-7b\n4.Client-levellayerimportance\n4.1.LayerwiseNeuralTangentKernel(LNTK)\n(UlE[fl(X)])\nj\n=(cid:16) I−e−ηλl jt(cid:17) (Ulyl)\nj\n(11)\nwhere fl(X) and yl are actual and target l-th layer out-\nInthissection,wefirstmotivatetheusageofLNTKtowards\nputs. This formulation demonstrates that the convergence\nprioritizing selected layers for client-specific fine-tuning.\nbehavior of a layer is largely influenced by the eigenval-\nTocapturethemodeltrainingdynamics,considertheevolu-\nues λl of LNTK. Let λl ≥ λl ≥ ··· ≥ λl denote the\ntionofθ andtraininglossLoverinputinstancesX ⊂D : j 1 2 n\nt i eigenvalues of Θl(X,X), where λl is the principal eigen-\n1\nθ˙ t =−η∇ θF(X;θ t)∇ F(X;θ t)L value. In particular, λl 1 plays a dominant role in the con-\nvergence dynamics [10]. When using the largest possible\nL˙ =∇ (X;θ )LT∇ (X;θ ) (6)\nF t F t layer-specific learning rate, ηl ∼ 2 , the training process\nθ˙ =−η∇ (X;θ )LTΘ(X,X)∇ (X;θ )L λl 1\nt F t F t aligns most strongly with the direction associated with the\nwheretheNTKmatrixΘ t(X,X)attimetisdefinedas: principal eigenvalue as the NTK eigenvectors correspond-\ning to the principal eigenvalue are learned quicker due to\nΘ (X,X)≜∇ F(X;θ )∇ F(X;θ )T ∈Rn×n. (7) spectral bias [10, 12, 59, 64]. This motivates the use of\nt θ t θ t\nthe principal eigenvalue λl in our client-specific layer im-\nTheintegralNTKofamodel[31]canbecomputedasthe 1\nportancescore, asitrepresentsthemaximumalignmentof\nsumoflayerwiseNTK(LNTK),wherewedefineLNTKas:\neachlayer’sparameterspacewiththeclient’sdatadistribu-\ntion. Thisprovidesaprincipledbasisforprioritizingthese\nΘl(X,X)=∇ F(X;θl)∇ F(X;θl)T, (8)\nθl θl layers.\nwhere ∇ F(X;θl) denotes the Jacobian of F at input\nθl Forastepofgradientdescent,thelossreductioncanbe\npoints X with respect to the l-th layer parameters θl. The\ncharacterizedbythedirectionalderivativeoftheloss:\nintegralNTKcanbeexpressedas: Θ(X,X)\nL L L ∆L( =i) lim L(θ+ϵ∇ θL(θ))−L(θ) ( ≈ii) ∇ L(θ)T∇ L(θ)\n( =i)(cid:88) ∇ (X)∇ (X)T ( =ii)(cid:88) (cid:88) ∇ (X)∇ (X)T (i =ii)(cid:88) Θl(X,X)ϵ→, 0 ϵ θ θ\nθl θl θl θl\nl=1 l=1θp∈θl l=1 (i =ii) ∇ ZL(θ)T(∇ θF(θ)T∇ θF(θ))∇ ZL(θ)\n(9)\nwhere(i)decomposesthematrixmultiplicationintothesum ( =iv) ∇ L(θ)T (cid:32) (cid:88)L Θl(cid:33) ∇ L(θ)( =v)(cid:88)L (cid:88)nk λl (cid:16) (ul)TY(cid:17)2\nZ Z j j\nofvectormultiplications;(ii)gathersaddendsbyeachmod-\nl=1 l=1j=1\nule; and (iii) follows the definition of the LNTK. Since\nΘl(X,X)isapositivesemi-definiterealsymmetricmatrix, ( ≈vi)(cid:88)L λl (cid:16) (ul)TY(cid:17)2\n1 1\nweperformaneigen-decompositionofLNTKas: l=1\n(12)\nnk where(i)followsthedefinitionofthedirectionalderiva-\nΘl(X,X)=UlΛl(Ul)T =(cid:88) λl jul j(ul j)T (10) tive; (ii) follows the first-order Taylor expansion; (iii) ap-\nj=1 pliesthechainruleofderivatives;(iv)followsfromEq. (8);\nwhereΛl =diag(λl,λl,...,λl )containseigenvaluesλl and (v) follows the eigen-decomposition of the layerwise\n1 2 nk j\nofΘl(X,X),andeachλl ≥0. Themeanoutputofthel-th NTKundertheassumptionofsquarederrorloss.Assuming\nj\nlayerintheeigenbasisoftheLNTKcanbedescribedby: that the true labels align well with the top eigenvectors as\nFigure4.Overviewofourlayerselectionstrategy,F3OCUS.Eachclientsendslayerimportancescoresbasedontheprincipaleigenvalue\nofLNTKtotheserver. Theserverrefinesclient-specificlayerselectionbymaximizingthecumulativeclient-specificimportancescores\nwhilesimultaneouslyminimizingthevarianceofthehistogramoflayerselectionsacrossclients.Itsendstherevisedlayerranksback.\nFigure5.VisualizationoflayerranksofLLaVA-1.5acrossroundsindifferentclientsbasedonLNTK.Darkercolorimplieshigherrank.\ndiscussedearlier, i.e.,(cid:0) (ul)TY(cid:1)2 islargeforlargeλl,di- valuesdominate,indicatingstrongclient-specificparameter\nj j\nrectional derivative of the loss function can be regarded as alignment(seeFigs. 3&5). SeeAlgorithminSuppl. §A.\ncloselyrelatedtotheeigenspectrumofthelayerwiseNTKs.\n4.2.ConvergenceAnalysisofLNTK\nSpecifically, (vi) suggests that layers with higher principal\neigenvalues contribute more significantly to loss reduction Webeginwithsomenecessaryassumptionsfollowingpre-\nduring training. Overall, Eq. 12 suggests that the loss de- viousworks[34,73]andthenintroduceasetofassumptions\ncreases more rapidly along the eigenspaces corresponding toanalyzetheimpactofthelayersselectedusingLNTK:\ntolargerLNTKeigenvalues.Giventhattheprincipaleigen- Assumption 1: (γ-Smoothness) There exists a constant\nvalue λl 1 vary across layers as seen in Fig. 3, we propose γ >0suchthatforanyθ,ϕ∈Rp:\ntoselectivelyfine-tunelayerswithlargerλl toachieveeffi-\n1\ncientlearningwithlimitedcomputationalresources.\nTherefore, we define the client-specific layer impor- ∥∇F i(θ)−∇F i(ϕ)∥ 2 ≤γ∥θ−ϕ∥ 2, ∀i∈N. (13)\ntancescoreSl =\nλl\ni,1 wherethesuminthedenomi-\ni (cid:80)L λk Assumption 2: (Unbiased and variance-bounded\nk=1 i,1\nnatornormalizestheprincipaleigenvalueacrossalllayers, stochastic gradient) The layerwise stochastic gradient\nensuring that Sl captures the relative importance of each G (θ ;B )computedonarandomlysampleddatabatch\ni i,l t i,t\nlayerforclientiintermsofitscontributiontothemodel’s B serves as an unbiased estimate of the layerwise full-\ni,t\npredictive capacity for that client’s data distribution. This batchgradient:\nformulation prioritizes layers whose NTK principal eigen- E [G (θ ;B )]=∇F (θ ). (14)\nBi,t i,l t i,t i,l t\nBesides, there exist constants σ > 0,∀l ∈ L such Assumption 4: (Bounded stochastic gradient) The ex-\nl\nthat E ∥Gl(θ ;B ) − ∇Fl(θ )∥2 ≤ σ2,∀i ∈ N and pectedsquarednormofstochasticgradientsisboundeduni-\n(cid:80) B σi 2,t ≤σi 2.t i,t i t l\nformly,i.e.,forconstantσ >0andanyi,t:\nl∈L l g\nAssumption 3: (Gradient Diversity) The non-IID client\ndatadistributioncausesdiversegradients. Thereexistcon- E Bi,t[∥G i(θ t;B i,t)∥2]≤σ g2. (18)\nstantsκ l >0,∀l∈Lsuchthat: Assumption 5: (Normalized Layer Selection Noise\nBound)Thereexistsomeξ2 ∈ [0,1)andanyt,i, thenor-\nE [∥∇F(θ )−∇Fl(θ ;B )∥2]≤κ2,∀i∈N. (15)\nBi,t t i t i,t l malizedlayerselectionnoiseduetoLNTKisboundedby:\nIn contrast to the theoretical analysis for standard FL\n∥θ −θˆ ∥2\nsettings, our LNTK-based fine-tuning introduces three ad- t t,i ≤ξ2 (19)\n∥θ ∥2\nditional challenges: (i) Each client only updates a sub- t\nset of layers. Hence, the aggregated gradient is no longer whereθˆ denotesLNTK-basedclient-specificlayers.\nt,i\nan unbiased estimate of the local gradient ∇F (θ ), i.e., Theorem 2 (Impact of layer selection-based noise ξ and\ni t\n(cid:80) ∇Fl(θ ) ̸= ∇F (θ ) where equality holds only varianceofselectioncounts2 onLNTKconvergence):\nl∈Lt i t i t d\nif all layers are selected. (ii) LNTK-based layer selec-\ntion may vary across clients as seen in Fig. 5. The ag- (cid:34)\ngregated gradient of the selected layers is not equal to minE(cid:2) ∥∇F(θ )∥2(cid:3) ≤ 2 F(θ0)−F(θ∗)\nthe gradient computed based on the global loss function t∈[T] t 2 (η−3γη2)T\nei. qe u., a(cid:80) lityi∈ hS ot l(cid:80) dsl o∈ nL lt yα ifi, at∇ lll cF lii e( nθ tt s) s̸= ele(cid:80) ctl t∈ hL et s∇ amlF e( laθ yt) e, rsw .here +(cid:88)T ηξ2γ 22Ns d (1+3ηγ)E (cid:13) (cid:13) (cid:13) (cid:13)(cid:88)T ∇ ℓF(θ t)(cid:13) (cid:13) (cid:13) (cid:13)2 \n(cid:13) (cid:13)\n(iii)Thegradientmagnitudesin(i-ii)varyacrossepochs. t=1 t=1\n(cid:35)\nInordertorelatetheaggregatedandtargetgradient,we\n+\nγη2NTs\nd (cid:0) ηγσ2(1+3γ)+3σ2s (cid:1) s.t. θ∗ =argminF(θ)\ndefineaproxylayerwiselossψ i,l optimizedbyclientsas: 2 g d θ∈Rp\n(20)\nψl(θ )≜ (cid:88) ml α Fl(θ ), s.t∇ψl(θ )=δ (16) Remark2: Withη ≤ min{√1 T, 61 γ},modelconvergestoa\ni t i,t i,t i t l i t t neighborhood of a stationary point of FL with a small gap\ni∈St\ndue to layer selection-based noise ξ and variance of selec-\nGiven assumptions 1-3, we formulate the convergence as: tioncounts overclients. Thismotivatesustojointlymin-\nd\nTheorem1(ConvergenceofLNTK-basedlayerselection): imizetheinfluenceofξands forbetterconvergence.\nd\n5.2.Multi-objectiveOptimization\n(cid:34)\ntm ∈[i Tn ]E(cid:2) ∥∇F(θ t)∥2 2(cid:3) ≤ (η−γ2\nη2)T\n(cid:2) F(θ0)−F(θ∗)(cid:3) Motivated by this, we refine the selected layers on server\n(see Fig. 6) to achieve two primary objectives (see Eq.\nT (cid:18) (cid:19)(cid:32) (cid:13) (cid:13) (cid:13) (cid:13)2 21): maximizingthecumulativeclient-specificimportance\n+γ(ησ)2T +(cid:88) η+ 21\nγ\n−γη2 E (cid:13) (cid:13) (cid:13)(cid:88) ∇ ℓF(θ t)(cid:13) (cid:13)\n(cid:13)\n scoresbasedonLNTKandminimizingthevarianceoflayer\nt=1 (cid:13)ℓ∈/Lt (cid:13) selectionhistogram(seeFig.7)toencouragemorebalanced\n(cid:33)(cid:35)\n+ ℓ(cid:88) ∈Lti(cid:88) ∈Nα i,t(1−ml i,t)2k l2 s.tθ∗ =arg θm ∈i Rn pF(θ) tu b hs e ea rg moe f eo acf nlie e ca n oc t uh s nl t ta hy oae t fr s laeo ylv e ee c rr t uc l sl ai aye gen ert .s l. , (L Sw ee i et thn Sl un¯r pe p=p lr .es L1 §e An (cid:80)t ft L\nl\noh\n=\nre\n1\nmn nu\nl\nom ra es-\n(17)\n(cid:16) (cid:17) details.) Thejointoptimizationproblemisformulatedas:\nRemark 1: With commonly chosen η = O √1 , RHS\nT\nof (17) the last term → 0 as T → ∞. So, LNTK-based\nP arE yF pT o- iF nL toc fo sn tv ae nr dg ae rs dt Fo La ,msm aa inll tan ie ni ig nh gb ao nu orh no -zo ed roof era ros rta flti oo on r-\n.\n\n max\n(cid:80)N (cid:80)L\nS il\n(cid:88)\ni=1l=1 s.t: ml ≤L , ∀i∈N\nSeeSuppl. §Bforcompleteproofanddiscussions. L i i,max\n min L1 (cid:80) (n l−n¯)2 l∈Li\nl=1\n5.Server-levelOverallLayerImportance (21)\nThelatterobjectivepreventsover-relianceonspecificlayers\n5.1.TheoreticalMotivation\neveniftheyhavehighimportancescorestherebyincreasing\nTo explicitly examine the impact of potential noise intro- diversity in layer selection across clients. The constraint\nducedbyLNTK-basedlayerselectionaswellthevariation incorporatesclient-specificcomputationalbudgetL .\ni,max\nofselectioncountacrossclients,wereformulatetheconver- Traditional optimization methods struggle to optimize\ngenceinTheorem2leveragingtwoadditionalassumptions: thesetwoconflictingobjectives.Neuralnetworkscannotbe\nFigure6. VisualizationofrefinedlayerranksofLLaVA-1.5basedonF3OCUS. ComparingitwithFig. 5showsthattheserver-level\nmeta-heuristicoptimizationrefinestheclient-levellayerranksbyincreasinginter-clientlayerdiversityateveryround.\ngeneratenewsolutions,whilenon-dominatedsortingand\ncrowdingdistanceensurediversityontheParetoFront.\n(2)ArtificialBeeColony(ABC)[4]: Eachbeerepresents\napotentiallayerselectionfortheclients. Theoptimization\nproceedsinthreephases: Employedbeesexploitlocalso-\nlutions, adjusting layer assignments based on importance\nFigure7. Layerselectionhistogramshowstheimpactofserver- scoresanddiversity,occasionallyacceptingworsesolutions\nlevel optimization. It encourages more layers to participate (see toescapelocaloptima.Onlookerbeesthenselectsolutions\nblue-marked areas) by maximizing layer-selection diversity i.e., based on a probability weighted by importance and diver-\nreducingthevarianceoftheselectioncount(verticalaxis).\nsity,whileScoutbeesabandonunproductivesolutions,re-\nplacingthemwithrandomlygeneratedonestopromoteex-\nploration. Non-dominated solutions are stored in a Pareto\nemployedtooptimizeduetoabsenceofdataonserver. Be-\narchive,whichguidesrefinementoveriterations.\nsides, thenumberofpossibleconfigurationsisparticularly\n(3) Ant Colony Optimization (ACO) [21]: Each ant rep-\nhighdue tothelargenumber oflayersinfoundation mod-\nresents a candidate layer selection, constructing paths in-\nels.Sincethisprobleminvolvesmultipleobjectives,thereis\nfluenced by pheromone trails (which reinforce success-\nno”best”solutionbutratherasetofoptimaltrade-offs(the\nfullayerchoicesfrompreviousiterations)andimportance\nPareto front). Meta-heuristic algorithms are well-suited to\nscores(whichguideantstowardlayersthatarelikelyben-\nexploresuchcomplex,high-dimensionalsolutionspacesin\neficial based on client-specific requirements). The Pareto\nabsence of training data by balancing exploration and ex-\narchive preserves non-dominated solutions. Pheromone\nploitation. To this end, we carefully select and investigate\ntrails are then updated, with evaporation to prevent stale\n5 meta-heuristic algorithms spanning all 3 algorithm cate-\npathsandadditionalpheromonedepositsonlayersselected,\ngories: evolutionary,physics-based,andswarm-based. See\ntherebyencouragingtheirselectioninsubsequentiterations.\nSuppl. §Afordetaileddescriptionandpseudo-code.\n(4)SimulatedAnnealing(SA):SA[71]startswithahigh-\ntemperature, randomly initialized solution, gradually ex-\n5.3.Meta-heuristicalgorithms\nploringoptimallayerselectionsbycoolingoveriterations.\nEach of the following algorithms aim to maximize client- Itacceptsnewconfigurationsifitoffershigherimportance\nspecific importance while ensuring balanced layer utiliza- orlowervariancebasedonEq. 21. Ifthenewconfiguration\ntionacrossclientsbasedontheirunderlyingprinciples: islessoptimal,itmaystillbeacceptedbasedonaprobabil-\n(1)Non-DominatedSortingGeneticAlgorithm(NSGA): itythatdecreaseswiththetemperatureallowingSAtoavoid\nWe use NSGA [20] to iteratively evolve a population of beingtrappedinlocaloptima.Asthetemperaturecools,SA\nlayer selections. We initialize population based on client- gradually refines the search, focusing on fine-tuning layer\nspecificimportancescoreswhileincorporatingprobability- assignmentsthatrespectbothclient-specificimportanceand\nbased sampling for broader search space coverage. This abalanceddistributionoflayerusage.\nguided randomness helps balance exploration (diversify- (5) Multi-Objective Particle Swarm Optimization\ningchoices)andexploitation(prioritizinghigh-importance (MOPSO) [18]: Each particle in the swarm represents a\nlayers). Genetic operations like crossover and mutation candidate layer assignment, initialized with client-specific\nimportance scores to guide the selection of important 6.2.PerformancecomparisonwithState-of-the-arts\nlayers. Randomized probability-based sampling is used\nWecompareF3OCUS with28SOTAmethods:\nto ensure diverse initial positions, promoting exploration\n(i) Tab. 3 shows comparison with 5 SOTA PEFT base-\nof the solution space. Each particle updates its veloc-\nlines, viz., LayerNorm Tuning (LN) [5], LoRA [30], Bias\nity and position by balancing three influences: inertia\nTuning [11], Prompt Tuning [25], and FedDAT [14] in\n(maintaining its current layer selection), a cognitive com-\ntermsofcommunication(MBits),Computation(GFLOPs),\nponent(bestindividualsolution),andasocialcomponent\ntotal number of trainable parameters (Millions) and accu-\n(globallyoptimalsolution).\nracy in each client. F3OCUS is observed to outperform\nallPEFTsexceptadapters[29]andFedDATwhichfinetune\n6.ExperimentsandResults all adapters whereas F3OCUS finetunes only selected 4\nadapterlayersineachclientleadingtoreducedcommunica-\n6.1.FLsettings,DatasetsandTasks tion(9.7MBits)andcomputationalneeds(80.6GFLOPs).\n(ii)InTab. 6,F3OCUS isseentoconsistentlyoutperform\nWe evaluate our performance for fine-tuning selected (i)\n12SOTAPersonalizedFLbaselinesviz. perFedavg[23],\nlayers, and (ii) adapters [29] with 4 VLMs of varying size\nMetaFed [17], FedPAC [75], FedAS [78], FLUTE [52],\nand architecture, viz., ViLT [35], ALBEF [45], LlAVA-1.5\nFedALA [81], FedProto [68], FedRod [15], FedAP [53],\n[51], and BLIP-2 [46], for 3 FL task settings: (a) Visual\nFedFomo[85],FedRep[19],andFedper[3]for5tasks.\nQuestion Answering, (b) Image and Text-based Disease\n(iii) We adapt 7 SOTA Pruning baselines viz. Federated\nClassification,(c)Heterogeneoustaskscombining(a),(b).\nDrop-out [74], Magnitude [26], FishMask [66], GradFlow\n(a)VisualQuestionAnswering: Weconsider3scenarios [54],GraSP[72],SNIP[40],andSynflow[69]inourcon-\nwithdataofvaryingsizes,classcounts,andcomplexity: text and compare with proposed method in Tabs. 2 and 5.\n(i) Task 1 (with Domain gap): Five-client setting with LNTKsurpassestheperformanceoftheclosestSOTAprun-\nSLAKE [50], VQA-RAD [37], VQA-Med 2019 [6], ing method by 2.11% and 2.30% while F3OCUS outper-\nVQA-Med2020[1],andVQA-Med2021[7]. formsitby5.35%and5.33%respectivelyforhomogeneous\n(ii) Task2(withModalitygap): Modalityspecific8-client andheterogeneousdevicesettingsoverallarchitectures.\nsettingwithCT,Ultrasound,Dermatoscopy,Fundus,His- (iv)Wealsocomparewith4SOTALayerSelectionbase-\ntology,Microscopy,OCT,andX-Rayclients. linesviz. Adapter-drop(denotedas’last’)[61],RGN[42],\n(iii) Task3(withModalitygap): Modalityspecific9-client Fedselect [67], and SPT [27] (see Tabs. 2, 5). From Ta-\nsettingwithourUltra-MedVQAdataset(seeFig. 8). ble2,weobservethatLNTKoutperformstheclosestSOTA\nmethodby2.08%and2.17%forhomogeneousandhetero-\n(b) Image and text-based multi-label disease classifica-\ngeneous resource settings respectively. F3OCUS further\ntion: Weconsider2FLsettings[62]withDirichletcoeffi-\nimprovesperformanceoverLNTKby3.24%and3.03%.\ncientγ =0.5forChestX-RayandRadiologyreport-based\nmulti-labeldiseasedetection(with15classes).\n6.3.OtherExperimentalResults\n(i) Task4(withlabelshift): 4client-scenariowithOpen-I\nWeplotthelosscurvesofLNTKandF3OCUS andcom-\n(ii) Task5(w/labelshift): 10clientscenariowithMIMIC.\npare them with several pruning methods in Fig. 2, which\n(c) Heterogeneous tasks: We consider Task 6 (with task\ndemonstratestheimpactofourserver-leveloptimizationon\nheterogeneity)combiningthreeVisualQuestionanswering\nfaster convergence. We also visualize the principal eigen-\nclients,viz.,SLAKE,VQA-RAD,VQA-Med2019,andtwo\nvaluemagnitudesofall32layersofLLaVAacrossdifferent\ndisease-classificationclients,viz.,Open-IandMIMIC.\nclients at the beginning of a round for Task 1 in Fig. 3\nDeviceHeterogeneity:InTab. 2and5,tosimulatevarying and show the layer ranks over all rounds in Fig. 5. Con-\nresource constraints among clients, we adjust the number sequently,theeffectofserver-leveloptimizationonoverall\nof trainable layers across different tasks. For Tasks 1 and layerselectionsisshowninFig. 6usingrelativelayerranks\n6,6layersarefinetunedfor2clients,4layersforanother2 and in Fig. 7 using the layer selection histogram. In Fig.\nclients,and2layersfortheremainingclient. ForTask2,6 9,wemotivatethecurrentworkbyrevealingthat,contrary\nlayersarefinetunedfor3clients,4layersfor3clients,and tocommonbelief,fine-tuningthelast’K’layersisonlyas\n2layersforthelast2clients. ForTask3,2layersarefine- effective as fine-tuning random ’K’ layers. Table 4 shows\ntunedfor3clients,4layersforanother3clientsand6layers thecomparableperformanceofdifferentmeta-heuristical-\nforthelast3clients. ForTask4,6layersarefinetunedfor gorithms for all clients in Task 2. For the ’microscopy’\n1client,4layersforanotherclient,and2layersforthere- clientinthesametask,wepresentt-SNEfeaturevisualiza-\nmaining 2 clients. For Task 5, 6 layers are finetuned for 3 tionsinFig.10for(a)FederatedDropout(labeledrandom),\nclients,4layersfor1client,and2layersforthelastclient. (b)LNTK,and(c)F3OCUS,highlightingthestrikingim-\nSeeSuppl. §Cfordatasetandimplementationdetails provementingeneratingdistinctivefeaturerepresentations.\nFigure8.SampleVQAtripletsof9modality-specificmedicalclientsfromUltra-MedVQA(Task3)\nTable2.PerformancecomparisononVLMadapterlayerselectionintermsofaccuracyforTasks1-3andF1-scoreforTasks4-6\nFine-tuning Task1 Task2 Task3 Task4 Task5 Task6 MeanScore\nViLTLlAVABLIP ViLTLlAVA BLIP ViLTLlAVABLIP ViLTLlAVABLIP ViLTLlAVABLIP ViLTLlAVABLIP ViLTLlAVABLIP\nAlladapters 43.04 40.02 43.36 82.46 79.70 79.03 78.55 75.19 76.40 63.05 76.89 71.05 65.68 77.93 76.30 85.83 88.02 89.26 69.77 72.96 72.57\nHomogeneousresourcesacrossclients(L=4forallclients)\nFD[74] 33.54 33.31 34.15 73.94 68.84 68.78 70.36 67.04 68.54 52.82 67.78 60.21 56.20 67.99 66.53 76.90 79.05 80.36 60.63 64.00 63.10\nLast[61] 33.91 33.04 35.19 70.53 65.63 66.92 68.32 65.03 66.59 54.40 65.94 58.45 57.77 66.05 64.20 77.62 79.51 79.96 60.42 62.53 61.88\nMagnitude[26] 31.60 32.40 30.26 70.32 67.35 67.67 69.18 68.73 69.02 54.33 66.59 61.80 56.01 66.90 64.98 77.14 80.26 80.44 59.76 63.71 62.36\nFishMask[66] 37.45 35.34 37.77 75.65 72.37 71.38 71.99 71.63 72.10 56.02 72.04 65.35 59.20 71.86 70.22 79.98 83.03 82.32 63.38 67.71 66.52\nGradFlow[54] 36.03 34.29 38.35 74.81 72.65 72.58 72.29 69.88 71.09 56.38 71.19 64.98 59.82 71.60 70.13 80.33 82.89 82.56 63.28 67.08 66.61\nGraSP[72] 35.46 34.90 38.88 75.03 72.03 71.74 72.03 71.39 71.76 55.83 70.73 65.43 60.00 70.07 69.20 81.02 83.48 82.19 63.23 67.10 66.53\nSNIP[40] 31.26 32.73 35.36 73.96 69.40 70.15 72.16 67.04 68.76 54.21 66.07 60.39 58.09 67.24 66.92 78.20 80.34 81.13 61.31 63.80 63.78\nRGN[42] 32.71 32.52 34.81 73.60 69.94 70.55 70.88 68.75 69.73 56.40 67.25 61.32 57.54 68.64 68.39 76.79 78.05 80.09 61.32 64.19 64.15\nSynflow[69] 35.68 35.38 37.89 75.26 72.33 73.25 72.49 71.36 71.63 56.11 71.67 64.28 59.27 70.17 71.32 80.35 79.09 81.90 63.19 66.67 66.71\nFedselect[67] 36.80 34.48 38.88 74.29 70.49 71.63 71.29 70.52 70.06 55.83 70.86 65.08 58.53 71.33 72.02 79.39 83.22 83.00 62.69 66.82 66.78\nSPT[27] 35.59 34.40 38.53 75.50 72.16 71.32 72.58 70.78 71.56 56.74 71.98 65.71 59.71 71.45 71.35 80.13 83.89 82.99 63.38 67.44 66.91\nLNTK(ours) 39.74 36.80 40.43 77.54 74.64 74.49 74.01 72.68 73.93 58.07 73.06 67.80 61.50 73.00 73.43 82.04 85.79 84.84 65.48 69.33 69.15\nF3OCUS(ours) 42.41 39.85 43.04 81.31 78.78 78.70 77.86 75.01 76.45 62.51 76.70 70.35 64.62 77.26 76.14 85.28 87.53 88.30 69.00 72.52 72.16\nHeterogeneousresourcesacrossclients\nFD[74] 32.65 33.54 34.04 73.74 69.36 68.16 70.27 66.73 67.81 52.18 67.51 60.48 55.56 67.71 66.16 76.85 78.57 79.54 60.21 63.90 62.70\nLast[61] 33.65 33.80 35.30 70.90 65.81 66.97 68.22 65.92 66.31 54.89 66.35 58.43 57.91 65.47 64.75 77.86 78.80 80.12 60.57 62.69 61.98\nMagnitude[26] 32.04 32.54 30.81 69.84 67.00 67.53 68.54 68.48 68.57 53.83 66.19 61.46 56.12 66.30 65.11 77.68 80.48 80.78 59.68 63.50 62.38\nFishMask[66] 37.43 35.73 37.78 75.60 72.01 71.70 71.78 71.34 71.45 56.58 72.08 65.17 59.40 72.25 70.37 80.49 82.66 82.01 63.55 67.68 66.41\nGradFlow[54] 35.17 34.86 38.43 75.41 73.16 71.99 72.45 70.40 71.20 56.33 70.91 65.17 59.44 71.48 70.19 80.66 83.39 82.96 63.24 67.37 66.66\nGraSP[72] 36.11 35.20 38.68 74.99 72.38 70.99 71.54 71.54 71.98 55.50 71.12 65.24 59.50 69.63 69.17 81.85 82.63 81.48 63.25 67.08 66.26\nSNIP[40] 30.53 33.39 36.24 73.96 68.70 70.33 72.00 66.63 68.03 54.02 66.76 60.09 58.82 67.15 66.90 78.01 79.66 81.14 61.22 63.72 63.79\nRGN[42] 31.82 32.64 35.14 73.74 70.12 70.52 70.49 69.04 69.13 56.02 67.68 61.42 58.22 69.21 68.50 77.04 78.52 79.75 61.22 64.54 64.08\nSynflow[69] 35.80 36.02 38.33 75.70 72.17 72.57 72.63 71.47 70.97 57.02 72.26 64.49 59.23 70.09 70.65 80.40 78.53 82.78 63.46 66.76 66.63\nFedselect[67] 36.59 34.21 39.00 74.85 69.71 70.94 71.12 70.77 69.85 55.71 70.70 64.35 58.07 71.51 71.79 79.26 83.42 83.45 62.60 66.72 66.56\nSPT[27] 35.35 34.72 38.58 75.34 72.84 71.82 72.92 70.87 72.10 56.23 72.65 65.38 59.38 72.05 71.20 80.41 83.82 82.44 63.27 67.82 66.92\nLNTK(ours) 39.24 37.56 40.02 77.59 74.55 74.96 73.14 73.37 74.68 58.12 73.40 67.50 62.32 73.34 73.65 82.44 86.32 84.96 65.48 69.76 69.29\nF3OCUS(ours) 41.80 40.06 43.42 81.85 77.83 79.13 77.16 75.04 76.33 62.18 76.69 71.08 64.42 77.59 75.73 85.28 87.56 88.45 68.78 72.46 72.36\nTable3.ComparisonwithotherPEFTsonTask1usingALBEF\nMethod MBit GFLOPParam(M) SLAKEVM2019VM2020VM2021 VR Overall\nFull 3915.2 156.5 122.35 77.45 67.25 15.06 22.00 41.52 46.92\nAdap(all) 28.8 85.2 0.90 72.82 64.45 11.56 21.00 38.35 43.17\nFedDAT 86.1 86.3 2.69 72.79 63.58 12.46 23.00 38.91 43.93\nLN 5.8 84.7 0.18 69.53 62.22 10.59 22.00 36.89 41.30\nLoRA 19.5 84.6 0.61 57.73 60.18 4.59 15.00 31.16 35.09\nBias 3.5 84.7 0.11 68.25 55.61 11.17 17.00 35.47 39.54\nPromptFL 19.2 85.0 0.60 65.63 57.56 4.78 15.00 40.26 36.65\nF3OCUS 9.7 80.6 0.30 74.69 60.05 12.84 24.00 42.30 42.78\nTable4.Comparisonofmeta-heuristicmethods(Task2)\nFinetune C1 C2 C3 C4 C5 C6 C7 C8 Overall\n(CT) (US) (OCT)(Fundus)(Micro.)(Hist.)(Derma.)(XRay)\nNSGA 88.6779.41 78.29 83.48 70.68 88.49 67.69 73.93 78.78\nABC 91.3374.51 77.71 85.71 72.73 88.70 62.31 76.55 78.70\nACO 86.6778.10 78.86 83.48 70.68 88.08 66.92 76.00 78.60\nSA 88.6772.59 70.86 85.93 71.09 89.45 66.62 76.48 77.71\nFigure 9. Performance of last ’K’ and random ’K’ adapters for MOPSO 86.1877.31 78.83 84.53 71.18 87.58 65.49 75.97 78.38\n32-layeredLLaVA-1.5-7bonTask4\nSeeSuppl. §Dformoreresultsanddetaileddiscussions.\nTable5.PerformancecomparisononVLMlayerselectionwithheterogeneousresourcesacrossclients\nFine-tuning Task1 Task2 Task3 Task4 Task5 Task6\nViLT BLIP ViLT BLIP ViLT BLIP ViLT BLIP ViLT BLIP ViLT BLIP\nFD[74] 31.91 33.07 75.75 70.03 72.84 70.13 50.25 58.12 56.80 68.10 77.65 82.70\nLast[61] 32.75 34.31 76.65 68.35 72.63 68.56 53.77 56.46 59.07 66.25 80.75 81.70\nMagnitude[26] 31.35 30.96 73.26 69.18 70.88 71.02 52.17 58.46 58.25 67.04 77.51 82.27\nFishMask[66] 36.35 36.82 77.71 73.56 74.60 73.61 54.35 63.08 61.10 73.39 81.90 83.09\nGradFlow[54] 36.20 37.29 77.09 74.06 74.82 73.90 54.18 62.58 60.70 71.76 81.22 82.24\nGraSP[72] 35.36 37.87 76.90 73.00 73.80 74.83 53.44 62.68 60.91 70.85 82.03 82.54\nSNIP[40] 29.65 35.23 75.59 73.88 74.37 72.14 51.91 58.04 62.04 68.85 79.25 82.38\nRGN[42] 32.89 36.21 75.34 72.35 72.76 71.59 53.80 62.16 59.58 70.23 77.95 81.14\nSynflow[69] 34.99 37.39 77.95 74.05 74.81 73.79 55.09 62.08 60.97 72.10 81.30 83.65\nFedselect[67] 35.79 38.00 76.66 72.80 73.45 72.60 53.55 61.87 59.34 71.63 80.29 84.10\nSPT[27] 34.33 37.78 77.03 73.66 75.18 74.80 54.18 63.03 60.60 72.68 81.53 83.97\nLNTK 37.32 39.22 79.15 76.47 75.44 76.99 55.81 65.40 64.12 75.26 83.66 86.20\nF3OCUS 40.85 42.37 84.25 80.46 79.65 78.95 60.29 69.79 65.86 78.94 86.40 89.69\nTable6.ComparisonwithSOTApersonalizedFL(L=4)\nTask perFedavgMetaFedFedPACFedASFLUTEFedALAFedProtoFedRodFedAPFedFomoFedRepFedperF3OCUS\n1 33.87 35.42 36.17 36.46 30.83 35.50 35.46 36.23 36.84 35.44 35.30 36.14 42.41\n2 71.61 75.46 76.29 76.51 62.33 74.56 74.98 77.65 77.42 75.13 75.49 75.93 81.31\n4 54.56 57.26 57.53 58.78 44.84 57.18 57.00 59.35 59.49 57.05 58.36 58.08 62.51\n5 57.64 59.15 58.73 59.17 47.88 57.42 57.38 58.77 60.44 58.34 57.01 57.58 64.62\n6 73.28 76.27 78.17 78.51 63.19 75.88 77.04 76.10 76.33 75.66 76.55 76.76 85.28\nriorperformanceacrossmultiplevision-languagetasksand\nmodels, providingapracticalsolutionforfine-tuninglarge\nfoundationmodelsindecentralizedenvironments.Ourcode\nanddatasetwillbemadeavailableuponacceptance.\nReferences\n[1] AbeedS.BenAbacha,VivekV.Datla,SadidA.Hasan,Dina\nFigure10.t-SNEfeaturevisualizationforClient6ofTask2(with Demner-Fushman, and Henning Mu¨ller. Overview of the\n26classes)showsgreaterseparabilityforF3OCUS. vqa-med task at imageclef 2020: Visual question answer-\ning and generation in the medical domain. In CLEF 2020\nWorkingNotes. 8\n7.Conclusion [2] DurmusAlpEmreAcar,YueZhao,RamonMatasNavarro,\nMatthew Mattina, Paul N Whatmough, and Venkatesh\nWe presented F3OCUS, a novel and theoretically Saligrama. Federated learning based on dynamic regular-\ngrounded approach for federated fine-tuning of Vision- ization. arXivpreprintarXiv:2111.04263,2021. 1,3\nLanguage foundation models in client-specific resource- [3] ManojGhuhanArivazhagan,VinayAggarwal,AadityaKu-\nconstrained settings. F3OCUS effectively balances indi- marSingh,andSunavChoudhary. Federatedlearningwith\npersonalization layers. arXiv preprint arXiv:1912.00818,\nvidual client requirements with collective layer diversity\n2019. 8\ntherebyimprovingmodelconvergenceandperformanceac-\n[4] Jagdish Chand Bansal, Harish Sharma, and Shimpi Singh\ncuracy. Ourserver-levelmulti-objectivemeta-heuristicop-\nJadon. Artificial bee colony algorithm: a survey. Interna-\ntimization scheme does not require any data on server and\ntionalJournalofAdvancedIntelligenceParadigms, 5(1-2):\ncan be easily combined with any existing layer selection\n123–159,2013. 7\nor pruning algorithms. Additionally, we released Ultra- [5] SamyadeepBasu,DanielaMassiceti,ShellXuHu,andSo-\nMedVQA, the largest medical VQA dataset, covering 12 heilFeizi. Strongbaselinesforparameterefficientfew-shot\nanatomies, for supporting further VLM research. Experi- fine-tuning. arXivpreprintarXiv:2304.01917. 8\nmental results demonstrate that F3OCUS achieves supe- [6] Asma Ben Abacha, Sadid A Hasan, Vivek V Datla,\nDina Demner-Fushman, and Henning Mu¨ller. Vqa-med: [18] CACoelloCoelloandMaximinoSalazarLechuga. Mopso:\nOverview of the medical visual question answering task at A proposal for multiple objective particle swarm optimiza-\nimageclef2019. InProceedingsofCLEF(Conferenceand tion. InProceedingsofthe2002CongressonEvolutionary\nLabsoftheEvaluationForum)2019WorkingNotes,2019.8 Computation. CEC’02 (Cat. No. 02TH8600), pages 1051–\n[7] Asma Ben Abacha, Mourad Sarrouti, Dina Demner- 1056.IEEE,2002. 7\nFushman,SadidAHasan,andHenningMu¨ller.Overviewof [19] LiamCollins,HamedHassani,AryanMokhtari,andSanjay\nthevqa-medtaskatimageclef2021:Visualquestionanswer- Shakkottai. Exploitingsharedrepresentationsforpersonal-\ning and generation in the medical domain. In Proceedings izedfederatedlearning. InInternationalconferenceonma-\nof the CLEF 2021 Conference and Labs of the Evaluation chinelearning,pages2089–2099.PMLR,2021. 8\nForum-workingnotes,2021. 8 [20] KalyanmoyDeb,AmritPratap,SameerAgarwal,andTAMT\n[8] EladBenZaken,YoavGoldberg,andShauliRavfogel. Bit- Meyarivan. A fast and elitist multiobjective genetic algo-\nFit: Simpleparameter-efficientfine-tuningfortransformer- rithm:Nsga-ii.IEEEtransactionsonevolutionarycomputa-\nbasedmaskedlanguage-models.InProceedingsoftheAsso- tion,6(2):182–197,2002. 7\nciationforComputationalLinguistics(Volume2: ShortPa- [21] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant\npers),pages1–9,Dublin,Ireland,2022. 2 colonyoptimization. IEEEcomputationalintelligencemag-\n[9] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, azine,1(4):28–39,2006. 7\nAlexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar [22] ChenDun,CameronRWolfe,ChristopherMJermaine,and\nMan˜as, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, AnastasiosKyrillidis. Resist: Layer-wisedecompositionof\nMark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan resnetsfordistributedtraining. InUncertaintyinArtificial\nLebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Intelligence,pages610–620.PMLR,2022. 3\nDianeBouchacourt,HaiderAl-Tahan,KarthikPadthe,Vasu [23] AlirezaFallah,AryanMokhtari,andAsumanOzdaglar.Per-\nSharma, Hu Xu, Xiaoqing Ellen Tan, Megan Richards, sonalizedfederatedlearningwiththeoreticalguarantees: A\nSamuelLavoie,PietroAstolfi,ReyhaneAskariHemmat,Jun model-agnosticmeta-learningapproach.Advancesinneural\nChen,KushalTirumala,RimAssouel,MazdaMoayeri,Ar- informationprocessingsystems,33:3557–3568,2020. 8\njangTalattof,KamalikaChaudhuri,ZechunLiu,XilunChen, [24] Jonathan Frankle, David J. Schwab, and Ari S. Morcos.\nQuentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Trainingbatchnormandonlybatchnorm: Ontheexpressive\nSaenko,AsliCelikyilmaz,andVikasChandra. Anintroduc- powerofrandomfeaturesincnns,2021. 2\ntiontovision-languagemodeling,2024. 1 [25] Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and\n[10] BenjaminBowman. Abriefintroductiontotheneuraltan- WenchaoXu. Promptfl: Letfederatedparticipantscooper-\ngentkernel. 2023. 4 atively learn prompts instead of models-federated learning\n[11] HanCai,ChuangGan,LigengZhu,andSongHan. Tinytl: inageoffoundationmodel. IEEETransactionsonMobile\nReduce memory, not parameters for efficient on-device Computing,2023. 8\nlearning.InAdvancesinNeuralInformationProcessingSys- [26] SongHan,JeffPool,JohnTran,andWilliamDally. Learn-\ntems,pages11285–11297,2020. 8 ing both weights and connections for efficient neural net-\n[12] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and work. Advances in neural information processing systems,\nQuanquan Gu. Towards understanding the spectral bias of 28,2015. 8,9,10\ndeeplearning. arXivpreprintarXiv:1912.01198,2019. 4 [27] HaoyuHe,JianfeiCai,JingZhang,DachengTao,andBohan\n[13] Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, and Zhuang. Sensitivity-aware visual parameter-efficient fine-\nYaliang Li. Efficient personalized federated learning via tuning. InProceedingsoftheIEEE/CVFInternationalCon-\nsparse model-adaptation. In International Conference on ferenceonComputerVision,pages11825–11835,2023. 8,\nMachineLearning,pages5234–5256.PMLR,2023. 3 9,10\n[14] Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, [28] AgrinHilmkil,SebastianCallh,MatteoBarbieri,LeonRene´\nandVolkerTresp.Feddat:Anapproachforfoundationmodel Su¨tfeld,EdvinListoZec,andOlofMogren. Scalingfeder-\nfinetuninginmulti-modalheterogeneousfederatedlearning. atedlearningforfine-tuningoflargelanguagemodels.InIn-\nInProceedingsoftheAAAIConferenceonArtificialIntelli- ternationalConferenceonApplicationsofNaturalLanguage\ngence,pages11285–11293,2024. 8 toInformationSystems,pages15–23.Springer,2021. 3\n[15] Hong-You Chen and Wei-Lun Chao. On bridging generic [29] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,Bruna\nandpersonalizedfederatedlearningforimageclassification. Morrone,QuentinDeLaroussilhe,AndreaGesmundo,Mona\narXivpreprintarXiv:2107.00778,2021. 8 Attariyan, and Sylvain Gelly. Parameter-efficient transfer\n[16] Jinyu Chen, Wenchao Xu, Song Guo, Junxiao Wang, Jie learning for nlp. In International conference on machine\nZhang, andHaozhaoWang. Fedtune: Adeepdiveintoef- learning,pages2790–2799.PMLR,2019. 8\nficient federated fine-tuning with pre-trained transformers. [30] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\narXivpreprintarXiv:2211.08025,2022. 2,3 Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.\n[17] YiqiangChen,WangLu,XinQin,JindongWang,andXing LoRA:Low-rankadaptationoflargelanguagemodels.InIn-\nXie. Metafed: Federated learning among federations with ternationalConferenceonLearningRepresentations,2022.\ncyclic knowledge distillation for personalized healthcare. 2,8\nIEEE Transactions on Neural Networks and Learning Sys- [31] Arthur Jacot, Franck Gabriel, and Cle´ment Hongler. Neu-\ntems,2023. 8 raltangentkernel:Convergenceandgeneralizationinneural\nnetworks. Advances in neural information processing sys- Alignbeforefuse:Visionandlanguagerepresentationlearn-\ntems,31,2018. 4 ingwithmomentumdistillation. Advancesinneuralinfor-\n[32] MenglinJia, LumingTang, Bor-ChunChen, ClaireCardie, mationprocessingsystems,34:9694–9705,2021. 1,8\nSergeBelongie,BharathHariharan,andSer-NamLim. Vi- [46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nsualprompttuning. InComputerVision–ECCV2022:17th Blip-2: Bootstrapping language-image pre-training with\nEuropeanConference,TelAviv,Israel,October23–27,2022, frozen image encoders and large language models. In In-\nProceedings, Part XXXIII, page 709–727. Springer-Verlag, ternational conference on machine learning, pages 19730–\n2022. 2 19742.PMLR,2023. 1,8\n[33] Gal Kaplun, Andrey Gurevich, Tal Swisa, Mazor David, [47] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia\nShai Shalev-Shwartz, and Eran Malach. Less is more: Smith. Federatedlearning: Challenges,methods,andfuture\nSelective layer finetuning with subtuning. arXiv preprint directions. IEEEsignalprocessingmagazine,37(3):50–60,\narXiv:2302.06354,2023. 3 2020. 1,3\n[34] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, [48] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-\nSashank Reddi, Sebastian Stich, and Ananda Theertha ing continuous prompts for generation. arXiv preprint\nSuresh. Scaffold: Stochasticcontrolledaveragingforfeder- arXiv:2101.00190,2021. 2\natedlearning.InInternationalconferenceonmachinelearn- [49] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao\ning.PMLR,2020. 1,3,5 Wang. Scaling & shifting your features: A new baseline\nforefficientmodeltuning.arXivpreprintarXiv:2210.08823,\n[35] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\n2022. 2\nand-languagetransformerwithoutconvolutionorregionsu-\npervision. InInternationalconferenceonmachinelearning, [50] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and\npages5583–5594.PMLR,2021. 1,8 Xiao-MingWu. Slake: Asemantically-labeledknowledge-\nenhanceddatasetformedicalvisualquestionanswering. In\n[36] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna\n2021 IEEE 18th International Symposium on Biomedical\nRumshisky.Revealingthedarksecretsofbert.arXivpreprint\nImaging(ISBI),pages1650–1654.IEEE,2021. 8\narXiv:1908.08593,2019. 3\n[51] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n[37] JasonJLau, SoumyaGayen, AsmaBenAbacha, andDina\nImproved baselines with visual instruction tuning. In Pro-\nDemner-Fushman. A dataset of clinically generated visual\nceedingsoftheIEEE/CVFConferenceonComputerVision\nquestions and answers about radiology images. Scientific\nandPatternRecognition,pages26296–26306,2024. 1,8\ndata,5(1):1–10,2018. 8\n[52] Renpu Liu, Cong Shen, and Jing Yang. Federated repre-\n[38] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\nsentationlearningintheunder-parameterizedregime. arXiv\nMixout: Effectiveregularizationtofinetunelarge-scalepre-\npreprintarXiv:2406.04596,2024. 8\ntrained language models. In International Conference on\n[53] Wang Lu, Jindong Wang, Yiqiang Chen, Xin Qin, Renjun\nLearningRepresentations. 3\nXu,DimitriosDimitriadis,andTaoQin. Personalizedfeder-\n[39] JaejunLee,RaphaelTang,andJimmyLin. Whatwouldelsa\natedlearningwithadaptivebatchnormforhealthcare. IEEE\ndo? freezing layers during transformer fine-tuning. arXiv\nTransactionsonBigData,2022. 8\npreprintarXiv:1911.03090,2019. 3\n[54] EkdeepSinghLubanaandRobertPDick. Agradientflow\n[40] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS\nframeworkforanalyzingnetworkpruning. InInternational\nTorr. Snip: Single-shotnetworkpruningbasedonconnec-\nConferenceonLearningRepresentations. 2,8,9,10\ntionsensitivity. arXivpreprintarXiv:1810.02340,2018. 2,\n[55] Brendan McMahan, Eider Moore, Daniel Ramage, Seth\n3,8,9,10\nHampson, and Blaise Aguera y Arcas. Communication-\n[41] SunwooLee,TuoZhang,andASalmanAvestimehr. Layer-\nefficientlearningofdeepnetworksfromdecentralizeddata.\nwise adaptive model aggregation for scalable federated InArtificialintelligenceandstatistics.PMLR,2017. 1,2,3\nlearning. In Proceedings of the AAAI Conference on Arti-\n[56] John Nguyen, Kshitiz Malik, Maziar Sanjabi, and Michael\nficialIntelligence,pages8491–8499,2023. 3\nRabbat. Where to begin? exploring the impact of pre-\n[42] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Ku- training and initialization in federated learning. arXiv\nmar,HuaxiuYao,PercyLiang,andChelseaFinn. Surgical preprintarXiv:2206.15387,4,2022. 3\nfine-tuningimprovesadaptationtodistributionshifts. arXiv [57] Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed,\npreprintarXiv:2210.11466,2022. 2,3,8,9,10 Mike Rabbat, Maziar Sanjabi, and Lin Xiao. Federated\n[43] BrianLester,RamiAl-Rfou,andNoahConstant.Thepower learning with partial model personalization. In Interna-\nofscaleforparameter-efficientprompttuning.arXivpreprint tional Conference on Machine Learning, pages 17716–\narXiv:2104.08691. 2 17758.PMLR,2022. 3\n[44] Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, [58] JohnRachwan,DanielZu¨gner,BertrandCharpentier,Simon\nZhengang Li, Hang Liu, and Caiwen Ding. Efficient Geisler,MorganeAyle,andStephanGu¨nnemann. Winning\ntransformer-basedlargescalelanguagerepresentationsusing the lottery ahead of time: Efficient early network pruning.\nhardware-friendlyblockstructuredpruning. arXivpreprint In International Conference on Machine Learning, pages\narXiv:2009.08065,2020. 3 18293–18309.PMLR,2022. 3\n[45] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, [59] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix\nShafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and\nAaronCourville. Onthespectralbiasofneuralnetworks. In [72] ChaoqiWang,GuodongZhang,andRogerGrosse. Picking\nInternationalconferenceonmachinelearning,pages5301– winningticketsbeforetrainingbypreservinggradientflow.\n5310.PMLR,2019. 4 arXivpreprintarXiv:2002.07376,2020. 2,3,8,9,10\n[60] Sylvestre-AlviseRebuffi,HakanBilen,andAndreaVedaldi. [73] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and\nEfficient parametrization of multi-domain deep neural net- HVincentPoor. Tacklingtheobjectiveinconsistencyprob-\nworks.InProceedingsoftheIEEEConferenceonComputer lem in heterogeneous federated optimization. Advances\nVisionandPatternRecognition,2018. 2 in neural information processing systems, 33:7611–7623,\n[61] Andreas Ru¨ckle´, Gregor Geigle, Max Glockner, Tilman 2020. 5\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. [74] Dingzhu Wen, Ki-Jun Jeon, and Kaibin Huang. Federated\nAdapterdrop: Ontheefficiencyofadaptersintransformers. dropout—asimpleapproachforenablingfederatedlearning\narXivpreprintarXiv:2010.11918,2020. 2,8,9,10 onresourceconstraineddevices. IEEEwirelesscommunica-\n[62] PramitSaha,DivyanshuMishra,FelixWagner,Konstantinos tionsletters,11(5):923–927,2022. 8,9,10\nKamnitsas,andJ.AlisonNoble.Examiningmodalityincon- [75] Jian Xu, Xinyi Tong, and Shao-Lun Huang. Personalized\ngruity in multimodal federated learning for medical vision federatedlearningwithfeaturealignmentandclassifiercol-\nandlanguage-baseddiseasedetection,2024. 8 laboration. arXivpreprintarXiv:2306.11867,2023. 8\n[63] ZhiqiangShen, ZechunLiu, JieQin, MariosSavvides, and [76] RunxinXu,FuliLuo,ZhiyuanZhang,ChuanqiTan,Baobao\nKwang-TingCheng.Partialisbetterthanall:Revisitingfine- Chang, Songfang Huang, and Fei Huang. Raise a child in\ntuningstrategyforfew-shotlearning. InProceedingsofthe large language model: Towards effective and generalizable\nAAAIconferenceonartificialintelligence,pages9594–9602, fine-tuning. arXivpreprintarXiv:2109.05687,2021. 3\n2021. 3 [77] MingzhaoYang,ShangchaoSu,BinLi,andXiangyangXue.\n[64] Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang, Exploringone-shotsemi-supervisedfederatedlearningwith\nDongshengLi,YujiangWang,RobertDick,QinLv,Yingy- pre-trained diffusion models. In Proceedings of the AAAI\ningZhao,FanYang,etal. Trainfaster,performbetter:mod- ConferenceonArtificialIntelligence,2024. 2\nular adaptive training in over-parameterized models. Ad- [78] XiyuanYang,WenkeHuang,andMangYe. Fedas: Bridg-\nvancesinNeuralInformationProcessingSystems,36,2024. inginconsistencyinpersonalizedfederatedlearning.InPro-\n4 ceedingsoftheIEEE/CVFConferenceonComputerVision\n[65] GuangyuSun,MatiasMendieta,TaojiannanYang,andChen andPatternRecognition,pages11986–11995,2024. 8\nChen. Exploringparameter-efficientfine-tuningforimprov- [79] Sixing Yu, J Pablo Mun˜oz, and Ali Jannesari. Federated\ning communication efficiency in federated learning. 2022. foundation models: Privacy-preserving and collaborative\n2 learningforlargemodels. arXivpreprintarXiv:2305.11414,\n[66] Yi-LinSung,VarunNair,andColinARaffel. Trainingneu- 2023. 2\nral networks with fixed sparse masks. Advances in Neural [80] EladBenZaken,ShauliRavfogel,andYoavGoldberg.Bitfit:\nInformationProcessingSystems,34:24193–24205,2021. 2, Simpleparameter-efficientfine-tuningfortransformer-based\n8,9,10 maskedlanguage-models.arXivpreprintarXiv:2106.10199,\n[67] Rishub Tamirisa, Chulin Xie, Wenxuan Bao, Andy Zhou, 2021. 3\nRonArel,andAvivShamsian. Fedselect: Personalizedfed- [81] JianqingZhang, YangHua, HaoWang, TaoSong, Zhengui\neratedlearningwithcustomizedselectionofparametersfor Xue,RuhuiMa,andHaibingGuan. Fedala: Adaptivelocal\nfine-tuning. In Proceedings of the IEEE/CVF Conference aggregationforpersonalizedfederatedlearning.InProceed-\nonComputerVisionandPatternRecognition,pages23985– ingsoftheAAAIConferenceonArtificialIntelligence,pages\n23994,2024. 8,9,10 11237–11244,2023. 8\n[68] YueTan,GuodongLong,LuLiu,TianyiZhou,QinghuaLu, [82] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,\nJingJiang,andChengqiZhang. Fedproto: Federatedproto- RuiyiZhang,TongYu,GuoyinWang,andYiranChen. To-\ntypelearningacrossheterogeneousclients.InProceedingsof wardsbuildingthefederatedgpt: Federatedinstructiontun-\ntheAAAIConferenceonArtificialIntelligence,pages8432– ing. In ICASSP 2024-2024 IEEE International Confer-\n8440,2022. 8 enceonAcoustics,SpeechandSignalProcessing(ICASSP).\n[69] HidenoriTanaka,DanielKunin,DanielLYamins,andSurya IEEE,2024. 2\nGanguli. Pruningneuralnetworkswithoutanydatabyiter- [83] Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xin-\nativelyconservingsynapticflow. Advancesinneuralinfor- weiLong,andBowenZhou. Crash: Clustering,removing,\nmationprocessingsystems,33:6377–6389,2020. 8,9,10 andsharingenhancefine-tuningwithoutfulllargelanguage\n[70] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob model. arXivpreprintarXiv:2310.15477,2023. 3\nVerbeek, and Herve´ Je´gou. Three things everyone should [84] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-\nknowaboutvisiontransformers. InComputerVision–ECCV YuDuan. Fine-tuningglobalmodelviadata-freeknowledge\n2022: 17thEuropeanConference,TelAviv,Israel,October distillationfornon-iidfederatedlearning. InProceedingsof\n23–27,2022,Proceedings,PartXXIV.Springer,2022. 2 the IEEE/CVF conference on computer vision and pattern\n[71] Peter JM Van Laarhoven, Emile HL Aarts, Peter JM van recognition,pages10174–10183,2022. 3\nLaarhoven, and Emile HL Aarts. Simulated annealing. [85] Michael Zhang, Karan Sapra, Sanja Fidler, Serena Ye-\nSpringer,1987. 7 ung, and Jose M Alvarez. Personalized federated learn-\ning with first order model optimization. arXiv preprint\narXiv:2012.08565,2020. 8\n[86] Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong,\nYanzhi Wang, and Xue Lin. Pruning foundation mod-\nels for high accuracy without retraining. arXiv preprint\narXiv:2410.15567,2024. 3\n[87] Weiming Zhuang, Chen Chen, and Lingjuan Lyu. When\nfoundation model meets federated learning: Motiva-\ntions, challenges, and future directions. arXiv preprint\narXiv:2306.15546,2023. 2",
    "pdf_filename": "F$^3$OCUS_--_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_Client_Layer_Upd.pdf"
}