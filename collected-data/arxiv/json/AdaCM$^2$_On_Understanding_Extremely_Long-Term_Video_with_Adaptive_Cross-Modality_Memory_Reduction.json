{
    "title": "AdaCM$^2$ On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
    "abstract": "The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most ex- isting LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nev- ertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effec- tively handling complex question-answering tasks. To ad- dress the challenges of long videos and complex prompts, we propose AdaCM2, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video un- derstanding tasks, such as video captioning, video ques- tion answering, and video classification, demonstrate that AdaCM2 achieves state-of-the-art performance across mul- tiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%.",
    "body": "AdaCM2: On Understanding Extremely Long-Term Video with Adaptive\nCross-Modality Memory Reduction\nYuanbin Man1∗, Ying Huang1, Chengming Zhang2, Bingzhe Li3, Wei Niu4, Miao Yin1†\n1Department of CSE, UT Arlington, 2Department of CS, University of Houston,\n3Department of CS, UT Dallas, 4School of Computing, University of Georgia\n{yxm6616, yxh1071}@mavs.uta.edu, czhang48@uh.edu,\nbingzhe.li@utdallas.edu, wniu@uga.edu, miao.yin@uta.edu\nAbstract\nThe advancements in large language models (LLMs) have\npropelled the improvement of video understanding tasks by\nincorporating LLMs with visual models. However, most ex-\nisting LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent\nattempts to understand long-term videos by extracting and\ncompressing visual features into a fixed memory size. Nev-\nertheless, those methods leverage only visual modality to\nmerge video tokens and overlook the correlation between\nvisual and textual queries, leading to difficulties in effec-\ntively handling complex question-answering tasks. To ad-\ndress the challenges of long videos and complex prompts,\nwe propose AdaCM2, which, for the first time, introduces\nan adaptive cross-modality memory reduction approach to\nvideo-text alignment in an auto-regressive manner on video\nstreams. Our extensive experiments on various video un-\nderstanding tasks, such as video captioning, video ques-\ntion answering, and video classification, demonstrate that\nAdaCM2 achieves state-of-the-art performance across mul-\ntiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple\ntasks in the LVU dataset with a GPU memory consumption\nreduction of up to 65%.\n1. Introduction\nVideo understanding is an important task in computer vi-\nsion and artificial intelligence, which involves processing\nand reasoning over visual and textual information. While\nthe recent success of large language models (LLMs) [5,\n31, 34, 39] has significantly improved video-language mod-\nels [27, 29], prior work has primarily focused on short\nvideo understanding tasks, typically with videos ranging\n*Yuanbin Man is a Student Researcher in the Dept. of CSE at UT Arlington\nwhile in a Master’s study at BU. †Miao Yin is the corresponding author.\nLLM\nQ-Former\nMemory Bank\nSingle-Modality Correlation-\nbased Token Merge\nWhat does the \nMan play?\nPlay Football\nLLM\nPlay Football\nWhat does the \nMan play?\nDual-Modality Attention Mechanism\nW\nd\nt\nm\np\n?\nFigure 1. (Left) Existing approaches compress visual features of\nvideos via single-modality correlation; (Right) Our AdaCM2 re-\nduces video memory adaptively based on cross-modality attention.\nfrom 5 to 15 seconds. However, long-term video under-\nstanding [45], a sub-technique that develops models to pro-\ncess richer information, has played a crucial role in real-\nworld applications such as movie analysis and video re-\ntrieval. Unfortunately, it poses significant challenges as\nvideo length increases, especially the large memory con-\nsumption challenge. The number of frames the model must\nprocess grows rapidly, leading to substantial memory con-\nsumption, thereby preventing prior approaches from pro-\ncessing such long videos.\nTo solve the large memory consumption challenge, many\napproaches focus on compressing video tokens. For in-\nstance, MA-LMM [16] employs a memory bank to com-\npress visual tokens based on the cosine similarities of ad-\njacent two frames. Koala [37] passes multiple segments\nof video into tokenizer functions that aggregate visual to-\nkens to handle long videos. Even though those methods re-\nduce memory consumption, they still suffer from two sig-\nnificant limitations. 1) Ignoring text-driven information: As\nshown in Figure 1, existing works compress visual infor-\nmation without considering textual information, leading to\nthe loss of vital visual tokens that are highly related to\n1\narXiv:2411.12593v1  [cs.CV]  19 Nov 2024\n\nWhat is described in this video? \n00:00:34\n01:40:08\n…\n00:50:53\n00:27:11\n02:02:02\n0:05\n…\n…\n…\n02:10:13\nA group of people is climbing in an indoor gym. \nWhat is the number on the back of the man’s white football jersey? \nThe number is 10. \n00:01\n09:06\n…\n06:51\n03:41\n20:38\n0:05\n…\n…\n…\n20:39\nWhat is the woman doing in the video? \nShe is shopping in a grocery store. \nWho is the woman speaking to? \nThe cashier. \nFigure 2. The case study of AdaCM2 zero-shot on Ego4D[15] dataset. As shown, AdaCM2 can 1) summarize an extremely long video\nlasting over 2 hours with limited memory consumption and identify the number on a person’s back at the end accurately, 2) answer\nquestions related to a mid-length video spanning more than 20 minutes.\nthe text prompt, particularly in Visual Question Answer-\ning (VQA) [2, 12, 20, 50] tasks with complex text prompts.\n2) The lack of similarity adaptability: Previous methods at-\ntempt to compress visual tokens within the fixed and prede-\nfined frame interval, failing to capture the dynamic nature\nof video information and leading to inefficient and inflexi-\nble memory reduction. As shown in Figure 3c, the similari-\nties of adjacent frames vary across different layers. More-\nover, temporally distant frames still exhibit high similar-\nity in deep layers. Consequently, if not appropriately ad-\ndressed, such limitations will severely hinder their perfor-\nmance in many practical applications.\nTo address the above limitations, in this paper, we pro-\npose AdaCM2, on understanding extremely long-term video\nwith adaptive cross-modality memory reduction. As illus-\ntrated in Figure 1, our key idea is to adaptively preserve a\ncertain number of crucial visual tokens that are most rele-\nvant to text queries across different layers based on cross-\nmodality attention. We introduce two intriguing observa-\ntions: 1) Only a subset of visual key states exhibit high cor-\nrelations with text query; 2) Correlation varies across dif-\nferent layers. Our observations open opportunities for ef-\nfective memory reduction by fully exploiting the inherent\ncross-modal capabilities of the existing visual encoders to\ncompress visual tokens. Specifically, we first extract visual\nrepresentations from video frames using a frozen visual en-\ncoder, i.e., Q-Former [23]. Then, to solve the substantial\nmemory consumption challenge and model long-term tem-\nporal connection, we propose AdaCM2 attention to learn\nthe “query” regressively in a frame-by-frame manner within\nthe Q-Former. AdaCM2 attention mechanism enables dif-\nferent numbers of visual tokens preserved across differ-\nent layers based on cross-modality correlation. Finally, the\nlearned queries are sent to LLM to generate the answer. To\nthe best of our knowledge, AdaCM2 is the first framework\nthat leverages cross-modal attention integrated into the vi-\nsual encoder to enable extremely long-term video under-\nstanding with efficient memory reduction. Moreover, our\nAdaCM2 can improve BLIP-based [11, 22, 23] models in\na plug-and-play manner, enhancing their capability to pro-\ncess long-term video effectively. Our core contributions are\nsummarized as follows:\n• We propose an efficient video memory reduction frame-\nwork, AdaCM2 for long-term video understanding based\non cross-modality attention, which, for the first time,\nfacilitates interaction between visual features and text\nprompts in video understanding.\n• We present a cross-modality attention module that adap-\ntively analyzes and evaluates visual tokens according\nto the correlation with input text prompts, enabling ex-\ntremely long-term video tasks with a dynamic crucial\nvideo token-preserving strategy.\n• We conduct experiments on multiple video understand-\ning tasks. Our proposed AdaCM2 achieves 4.5% accu-\nracy improvement across multiple tasks in the LVU [45]\ndataset. Particularly, AdaCM2 exhibits promising perfor-\nmance on the VQA and video captioning tasks. More im-\nportantly, AdaCM2 reduces GPU memory consumption\nby 65%, highlighting superior performance and efficiency\nin practice.\n2\n\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\n46\n48\n50\n52\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\nVisual Tokens\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\nText Tokens\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n(a)\n0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 0.016\nAttention Score\n0\n100\n200\n300\n400\n500\n600\n700\nFrequency\n(b)\n0\n10\n20\n30\n40\n50\n60\n70\nAdjacent Frame Distance\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCosine Similarity\nLayer 2\nLayer 4\nLayer 6\nLayer 8\nLayer 10\nLayer 12\n(c)\nFigure 3. Visualization for cross-modality attention, generated using a randomly sampled video from the MSR-VTT [48] dataset. (a) Cross-\nattention score map of the 74th frame in the final layer and last head. (b) Cross-attention score distribution of the 80th frame in the final\nlayer and last head. (c) The layer-wise cosine similarities of attention scores between the current frame and adjacent frames.\n2. Related Work\nVideo-Language Models. With the recent advancements\nof large language models (LLMs) [5, 31, 34, 39], video-\nlanguage models have been integrated by LLMs with im-\nage encoders for multi-modal understanding and reason-\ning [3, 7, 30]. BLIP-2 [23] introduces a lightweight query-\ning transformer [23] to bridge the modality gap between\nthe frozen pre-trained image encoder and LLMs. Instruct-\nBLIP [11] further extracts informative visual features tai-\nlored to the given instruction with instruction-aware Query\nTransformer. LLaVA [53] leverages language-model to gen-\nerate multi-modal instruction-following data and improves\nmodel generalization ability. VisionLLM [7] provides a uni-\nfied perspective for vision and language tasks by treating\nimages as a foreign language. It aligns vision-centric tasks\nwith language tasks, which can be flexibly defined and man-\naged using language instructions. However, these models\nare prone to significant memory overhead when applied to\nlong video understanding tasks.\nLong-Term Video Understanding. Long video under-\nstanding focuses on detecting long-range patterns in videos\nlonger than 30 seconds or even several minutes. To re-\nduce memory and computational requirements, [16, 35] re-\nduce the redundancy of visual information based on the co-\nsine similarities of adjacent visual tokens. Other works like\nVis4mer [19] leverage a standard transformer encoder for\nshort-range spatiotemporal feature extraction and a multi-\nscale temporal Selective Structured State-Spaces (S4) [42]\ndecoder for long-range temporal reasoning. Koala [37]\nsplits a long video into multiple segments and then aggre-\ngates visual tokens to process long videos. Considering that\nthe goal of long-term video understanding is to answer the\ntext question corresponding to the video, our method con-\nsiders the correlation between the visual features and the in-\nput text information based on adaptive cross-modality atten-\ntion, significantly reducing memory consumption and en-\nabling extremely long-term video understanding.\nKV Cache Eviction. KV cache eviction, a memory re-\nduction method that retains important context key-value\npairs, is widely adopted in LLMs inference. H2O [54] finds\nthat keeping the recent tokens, together with “heavy-hitter”\ntokens measured by attention scores, is sufficient to main-\ntain LLM’s performance. Similarly, KeyFormer [1] retains\nonly the key tokens in the KV cache by identifying these\ncrucial tokens through a novel scoring function. In addition,\nbased on the observation that KV cache states are highly\nsimilar between adjacent layers in the middle-to-deep sec-\ntions of LLM, MiniCache [26] compresses the KV cache\nacross layers using a novel depth-based approach, signifi-\ncantly reducing the memory footprint for LLM inference.\nBuilding on the success of the eviction-based method in\nmanaging long-context LLMs, we efficiently process long-\nterm videos based on a similar philosophy. Moreover, our\nmethod focuses on reducing the redundancy of visual to-\nkens in the long video understanding task, which is more\nchallenging due to more information contained in videos\nand the interactions between visual and text modalities.\n3. Observations\nTo investigate the main bottleneck that hinders long-term\nvideo understanding, we conduct a comprehensive study\non the process between video frames and text prompts,\nwhich generally perform in the Q-Former. This process\naligns visual and textual information and causes substan-\ntial memory consumption when video lengths increase. In\nthis section, we will first analyze the cross-attention spar-\nsity within a frame and then show the generalization of\ncross-attention sparsity across videos and layers. These ob-\nservations demonstrate the redundancy in the dual-modality\nprocessing and inspire the foundation of our approach\nAdaCM2 proposed in Section 4.\n3.1. Intra-Frame Cross-Attention Sparsity\nExisting approaches compress visual tokens solely based\non the similarities among video frames. However, those ap-\n3\n\n0:05\nVisual\nEncoder\nVisual\nEncoder\nVideo\nQ-Former\nVideo\nQ-Former\nLLM\nQ: Where is the woman doing in the\nvideo? And who is the woman talking\nabout in last of the video ?\nA: She is shopping in the grocery\nstore. And she is talking to cashier.\n…\n…\nAdaptive Memory Reduction\nVideo Q-Former\nVideo Cache \n…\nF\nF\nN\nSelf-Attention\nCross-Attention\nAttention\nMap \n𝒇𝒕\n𝑸𝒕\n𝑸𝒕\n𝑸𝒍\n(1) Partitioning\n(2) Identifying\n(3) Reducing\nLayeri-1\nt-1\n𝛼1\n1 − 𝛼1\n𝛽1\nLayer 1:\nLayer i:\n…\n…\n1 − 𝛼𝑖\n𝛽𝑖\n𝛼𝑖\nLayer 1:\nLayer 2:\nLayer i-1:\nLayer i:\nVideo Cache\nAttention Map\nVideo Cache\nLayer 1:\nLayer 2:\nLayer i-1:\nLayer i:\nVideo Cache\nLayer 1:\nLayer 2:\nLayer i-1:\nLayer i:\nTextual Token\nLearnable Query Token\nVisual Token\nEvicted Video Cache\nLayeri\n𝑿𝒕𝒆𝒙𝒕\nLayer2\nLayer1\nAdaptive Memory\nReduction\nFigure 4. The framework of AdaCM2. With video and text query as input, AdaCM2 first utilizes a visual encoder to extract visual features\nfrom video frames. Then, video Q-Former embeds the correlation between visual features and the text prompt into a learnable query in\na regressive manner. Finally, LLM generates the answer based on the length-limited query embedding. To reduce memory consumption\nchallenge during the process of Adaptive Memory Reduction, the Video Cache is partitioned into previous and recent parts. Based on cross-\nmodality attention score, AdaCM2 then identifies important visual features and removes layer-wise unimportant visual tokens from cache.\nThe snowflake denotes frozen pre-trained models, while the fire tag represents models that are fine-tuned.\nproaches may miss key visual tokens highly related to tex-\ntual input, leading to accuracy loss for tasks with complex\ntext questions. Motivated by [14, 46], which have shown\nthat retaining only a subset of salient tokens responsible for\nthe majority of attention scores can be sufficient to maintain\nthe performance of LLMs, we observe the similar cross-\nattention sparsity in the visual-textual alignment process.\nObservation 1: Only a subset of visual tokens exhibits\nhigh correlations to the text query within a frame. In\nFigure 3a, we visualize the cross-attention scores for the\n74-th frame in the final layer and the last attention head by\nperforming inference on Q-Former with randomly sampling\ndata from the MSR-VTT [48] dataset. The dark color indi-\ncates that the visual token exhibits a slight cross-modality\ncorrelation with the text token. We can observe that only a\nsubset of visual tokens within a frame exhibits high corre-\nlations to text tokens. Moreover, the cross-attention scores\nexhibit a normal distribution as depicted in Figure 3b, with\nsignificant correlations primarily observed in the tail val-\nues. This observation motivates us to develop an algorithm\nthat identifies necessary visual tokens according to cross-\nmodality attention scores, thereby preserving crucial visual\ninformation most important to the textual prompts.\n3.2. Layer-Wise Cross-Attention Similarity\nWe have demonstrated the cross-attention sparsity within\na video frame. Due to the significantly increased frame\nlength, a long-term video contains a substantial number of\nvisual tokens. These tokens are grouped and undergo multi-\nlayer cross-attention operations with prompt tokens. To re-\nduce the memory consumption of redundant visual tokens\nglobally, we further analyze cross-attention sparsity across\nframes and layers.\nObservation 2: Correlation varies across different\nlayers. Based on the BLIP2 [23] Model, we conduct zero-\nshot inference across multiple datasets and tasks. Figure 3c\nreveals that the cross-attention scores exhibit high similarity\nbetween adjacent frames, where cosine similarity exceeds\n90% among the recent five frames. More importantly, the\nsimilarity is more significant in deeper layers than in shal-\nlow layers. This observation highlights the consistent redun-\ndancy throughout the video and the diverse levels of redun-\ndancy across different layers. Correspondingly, we propose\nto adaptively reduce visual memory consumption across\ndifferent layers according to layer-wise cross-modality at-\ntention, making memory compression dynamic and flexi-\nble.\n4. Methodology: AdaCM2\nWe present AdaCM2, an adaptive cross-modality memory\nreduction framework for extremely long-term video un-\nderstanding. AdaCM2 consists of three stages, including\n1) video feature extraction by a visual encoder; 2) adap-\ntive memory reduction based on cross-modality attention\nwith visual-textual embedding alignment; 3) text genera-\n4\n\ntion with a large language model, as illustrated in Figure 4.\nAdaCM2 adaptively reduces peak memory consumption by\nregressively generating learnable query tokens that preserve\nthe temporal continuity of video and a layer-wise cross-\nmodality memory reduction algorithm.\n4.1. Video Feature Extraction\nAs shown in Figure 4, similar to common video under-\nstanding workflow [23], AdaCM2 extracts video features\nusing a pre-trained visual encoder. Given a video with\na sequence of T frames, the encoder first encodes each\nframe and generates the corresponding video features X =\n[x1, x2, x3, · · · , xT ], where xt ∈RP ×C is the frame fea-\ntures at time t, P and C denote the number of visual tokens\nabout each frame and the channel dimension of each token,\nrespectively. Then, to incorporate temporal information into\nthe frame-level features, a positional embedding is applied\nas follows:\nft = xt + E(t), ft ∈RP ×C,\n(1)\nwhere E(·) represents the position embedding of a frame,\nand ft indicates the frame features with temporal informa-\ntion at time t.\n4.2. Adaptive Memory Reduction with Cross-\nModality Attention\nAfter extracting visual features from the video, a Q-Former\nis leveraged to align visual and textual features. By learning\na query Ql ∈RNl×C in the Q-Former model, the visual\nfeatures are refined to align the text description based on\nmulti-layers cross-modal attention mechanisms, where Nl\nis the number of learnable query tokens and C denotes the\nnumber of feature channels.\nVisual-Textual Feature Alignment in a Regressive\nManner. Unlike existing methods that directly process all\nframes into the Q-Former and align visual and textual infor-\nmation by one shot, we propose to learn the query Ql re-\ngressively in a frame-by-frame manner, enabling the reduc-\ntion of irrelevant visual tokens based on the cross-modality\ncorrelation in a limited memory size.\nAdaCM2 utilizes video cache and current visual features\nto align text features. To be specific, let Kt ∈RtP ×C, Vt ∈\nRtP ×C to represent video cache at time t, which are stored\nin memory by visual tokens before time t and at time t as:\nKt = [Kt-1, ftWK],\n(2)\nVt = [Vt-1, ftWV ],\n(3)\nwhere WK and WV are weight matrics. Therefore, the\ncross-modality attention calculation in the Q-former can be\ndefined as\nAt = St · Vt = softmax\n\u0012Qt · KT\nt\n√\nC\n\u0013\n· Vt,\n(4)\nAttention Map\nSum of Columns\nAttention Score Ranking (Top-N)\nRecent Tokens\nEvicted Tokens\nConserved Tokens\nVideo Cache\nVideo Cache\n(Conserved)\n𝑲\"\n𝑽\"\nFigure 5. Illustration for our video memory reduction. The video\ncache is first partitioned into recent and previous parts. Important\nvisual tokens with high cross-modality attention scores in the pre-\nvious cache are then preserved.\nwhere St ∈RN×tP is the attention score, Qt ∈RN×C is\nthe concatenation of the learnable query Ql and the text em-\nbedding Xtext, N is the number of query and text tokens.\nLayer-Wise Video Memory Reduction. Storing all vi-\nsual tokens in the video cache is impossible due to the\nincreasingly large memory cost while processing video\nframes, especially for long video understanding tasks. As\nshown in Observation 3.1, there is substantial visual redun-\ndancy in videos where only a subset of visual tokens ex-\nhibit high correlations with text tokens. Based on this ob-\nservation, in AdaCM2, we propose a cross-modality atten-\ntion mechanism to reduce memory consumption according\nto the visual-textual redundancy. AdaCM2 first identifies\nimportant visual features based on cross-modality attention\nscore and then adaptively reduces video memory by remov-\ning layer-wise visual tokens not less correlated to textual\ninformation.\n1\nIdentifying Important Visual Features based on\nCross-Modality Attention Score. For a video token ft(i) at\nspatial location i ∈{1, · · · , tP}, the cross-modality atten-\ntion score Sc\nt (i) accumulates the attention scores between\nft(i) and text tokens, which can be formulated as\nSc\nt (i) =\nj=N\nX\nj=1\nSt(j, i).\n(5)\nCross-modality attention scores capture the correlation be-\ntween visual and textual features. Therefore, we identify a\nvideo token as pivotal if it receives a high cross-modality\nattention score.\n2 Adaptively Reducing Video Cache Layer-Wise. As\ndemonstrated in Observation 3.2, visual features exhibit dif-\nferent levels of similarity with textual features across differ-\nent layers. Guided by cross-modal attention score, we fur-\nther design an adaptive video memory reduction algorithm\nfor the video cache, as shown in Figure 5.\nAccording to the order that visual token stored in the\nvideo cache, at time t, we split the Kt for the current layer\n5\n\ninto previous cache, ˆ\nKt, and recent cache, ˜\nKt:\nKt = [ ˆ\nKt, ˜\nKt],\n| ˆ\nKt|/| ˜\nKt| = (1 −α)/α\n(6)\nwhere | · | measures the number of tokens in cache, α is\nthe split ratio. For the visual tokens in recent cache, ˜\nKt,\nthat reserves the latest information, we still keep it in the\nmemory. On the other hand, for the previous cache, ˆ\nKt, we\nonly retain visual tokens with top-β cross-modality atten-\ntion scores in memory and thus obtain the conserved cache\nKt. The process is defined as follows:\nKt =\nn\nˆ\nKt(i) | i ∈argtop( ˆSc\nt , β)\no\n,\n|Kt|/| ˆ\nKt| = β,\n(7)\nwhere argtop(·, β) denotes the indices of visual tokens\nwith top-β cross-modality attention scores, ˆSc\nt is the cross-\nmodality attention scores of tokens in ˆ\nKt, β is the con-\nserve ratio. Finally, the video cache Kt can be compressed\nas Kt = [Kt, ˜\nKt]. Based on our Observation 3.2, i.e.,\nredundancy across layers varies, our adaptive video mem-\nory reduction is performed layer-wise. Correspondingly, we\nset α’s and β’s for different layers. It is worth noting that\nthe adaptive reduction is also applied to another type of\nvideo cache, Vt.\nLet r = α+(1−α)β ≤1. Through deductive reasoning,\nthe final size of video cache KT can be derived as:\n|KT | = P\nT\nX\nt=1\nrt = Pr\n\u00001 −rT \u0001\n1 −r\n(8)\nWith the continuous increase of video duration, T →∞,\n|KT | is converged to a constant Pr/(1 −r), showing the\nmaximum memory requirement of AdaCM2 to processing\nunbounded video length.\n4.3. Text Generation\nWith the regressive processing of video frames, the learn-\nable query Ql ∈RN×C in Q-Former has modeled the long-\nterm temporal connection from the input video. Then, the\nLLM will generate the answer based on the learned Ql, a\nlength-limited vector aggregating the long-term input video\ninformation. Specifically, assume Y = {y1, y2, · · · , yM}\nis the answer comprising a sequence of M words, we mini-\nmize the cross-entropy loss during training as follows:\nL(V , P , Y ) = −\nM\nX\ni=1\nyi log p(yi|y<i, Ql, Xtext)\n(9)\nwhere p(yi|y<i, Ql, Xtext) denotes the probability for the\ni-th word yi given the preceding sequence of ground truth\nwords y<i.\nAlgorithm 1 Layer-wise Adaptive Video Reduction.\n1: Input: Video frames {ft}T\nt=1, hyper-parameters α, β\nfor each layer;\n2: Output: Reduced video KV cache Kt, Vt.\n3: for t = 1 to T do\n4:\nKt ←[Kt−1, ftWK]; Vt ←[Vt−1, ftWV ];\n5:\nPartitioning: Kt →[ ˆ\nKt, ˜\nKt]; Vt →[ ˆVt, ˜Vt];\n6:\nIdentifying: Determine Sc\nt using Eq.5;\n7:\nReducing: Obtain Kt using Eq.7;\n8:\nKt ←[Kt, ˜\nKt]; Vt ←[V t, ˜Vt];\n9: end for\n5. Experiments\n5.1. Tasks and Datasets\nLong-Term Video Understanding. We conduct experi-\nments on three common long-term video understanding\ndatasets, including LVU [45], Breakfast [21] and COIN[38].\nWe report the Top-1 accuracy as the evaluation metric. In\nparticular, for the LVU dataset, we focus our experiments\non seven classification tasks: relationship, speaking style,\nscene, director, genre, writer, and release year. These tasks\nprovide a diverse range of challenges for evaluating model\nperformance in different aspects of video understanding.\nVideo Question Answering. We evaluate AdaCM2on\ntwo popular video question answering datasets: MSRVTT-\nQA [48], MSVD-QA [6]. MSRVTT-QA and MSVD-QA\ncontain shorter videos, ranging from 10 to 15 seconds. Re-\ngarding the dataset scale, MSRVTT-QA includes 10,002\nvideos, MSVD-QA has 1,971 videos.\nVideo Captioning. We present the video captioning re-\nsults with the METEOR [4] and CIDEr [40] metrics on\nthree popular datasets, MSRVTT [48], MSVD [6], and\nYouCook2 [55]. It is noted that YouCook2 encompasses\nvideos longer than five minutes, posing a significant mem-\nory consumption challenge to understand the videos across\nsuch long periods.\n5.2. Implementation Details\nFor the visual encoder, we utilize the pre-trained image en-\ncoder ViT-G/14 [51] from EVA-CLIP[33]. To efficiently\nalign the visual and text features, we employ the pre-trained\nQ-Former weights from InstructBLIP [11]. For the LLM de-\ncoding, we use the pre-trained LLM, Vicuna-7B [8] V1.1.\nDuring training, we keep the visual encoder and the LLM\ndecoder frozen, and fine-tune the trainable parameters of\nQ-former. For video input, we extract frames at a speed of\n10 fps. All experiments are tested on our server with eight\nNVIDIA RTX 6000 Ada GPUs, two AMD EPYC 9254\nCPUs, and 1.5 TB memory.\n6\n\nTable 1. Comparison with state-of-the-art methods on the LVU dataset. The underlined number means the second best.\nModel\nContent\nMetadata\nAvg\nRelation\nSpeak\nScene\nDirector\nGenre\nWriter\nYear\nObj T4mer [45]\n54.8\n33.2\n52.9\n47.7\n52.7\n36.3\n37.8\n45.1\nPerformer [9]\n50.0\n38.8\n60.5\n58.9\n49.5\n48.2\n41.3\n49.6\nOrthoformer [32]\n50.0\n38.3\n66.3\n55.1\n55.8\n47.0\n43.4\n50.8\nVideoBERT [36]\n52.8\n37.9\n54.9\n47.3\n51.9\n38.5\n36.1\n45.6\nLST [19]\n52.5\n37.3\n62.8\n56.1\n52.7\n42.3\n39.2\n49.0\nVIS4mer [19]\n57.1\n40.8\n67.4\n62.6\n54.7\n48.8\n44.8\n53.7\nS5 [43]\n67.1\n42.1\n73.5\n67.3\n65.4\n51.3\n48.0\n59.2\nMA-LMM [16]\n58.2\n44.8\n80.3\n74.6\n61.0\n70.4\n51.9\n63.0\nOurs\n63.1\n40.2\n86.2\n75.4\n68.0\n77.0\n62.5\n67.5\nTable 2. Comparison on the Breakfast and COIN datasets. Top-1\naccuracy is reported.\nModel\nBreakfast\nCOIN\nTSN [44]\n-\n73.4\nVideoGraph [18]\n69.5\n-\nTimeception [17]\n71.3\n-\nGHRM [13]\n75.5\n-\nD-Sprv. [25]\n89.9\n90.0\nViS4mer [19]\n88.2\n88.4\nS5 [43]\n90.7\n90.8\nMA-LMM [16]\n93.0\n93.2\nOurs\n94.4\n93.3\nTable 3. Comparison on the MSRVTT, MSVD and YouCook2\ndatasets.\nModel\nMSRVTT\nMSVD\nYouCook2\nM\nC\nM\nC\nM\nC\nUniVL [28]\n28.2 49.9\n29.3 52.8\n-\n127.0\nSwinBERT [24]\n29.9 53.8\n41.3 120.6\n15.6 109.0\nGIT [41]\n32.9 73.9\n51.1 180.2\n17.3 129.8\nmPLUG-2 [47]\n34.9 80.3\n48.4 165.8\n-\n-\nVideoCoca [49]\n-\n73.2\n-\n-\n-\n128.0\nVideoLLaMA [52] 32.9 71.6\n49.8 175.3\n16.5 123.7\nMA-LMM [16]\n33.4 74.6\n51.0 179.1\n17.6 131.2\nOurs\n33.0 73.1\n51.4 189.4\n17.6 125.6\n5.3. Main Results\nLong-Term\nVideo\nUnderstanding.\nWe\ncompare\nAdaCM2 with state-of-the-art methods on the LVU [45]\nbenchmark. As shown in Table 1, our model outperforms\nthe existing long-term video understanding models, in-\ncluding Object transformer [45], S4 [43], VIS4mer [19],\nVideoBERT [36], LST [19], and MA-LMM [16] across\nboth content understanding and metadata prediction tasks.\nThe results indicate that AdaCM2 achieves a significant\nimprovement across most tasks, increasing Top-1 accuracy\nby 4% compared to MA-LMM [16].\nIn Table 2, we evaluate our AdaCM2 on the Breakfast\n[21] and COIN [38] datasets. It is worth noting that these\ndatasets present a greater memory consumption challenge\ndue to the longer and more diverse videos they contain. It\nis seen that our AdaCM2 outperforms MA-LMM [16] by\n1.4% and 0.1% in Top-1 accuracy, respectively. These re-\nsults demonstrate that our approach achieves superior per-\nformance in long-term video understanding.\nVideo Captioning. Table 3 summarizes the experi-\nmental results on video captioning datasets, including\nMSRVTT, MSVD, and YouCook2. AdaCM2 achieves\n51.4% Meteor and 189.4% CIDEr on the MSVD datasets.\nThe results show that AdaCM2 outperforms the prior state-\nof-the-art approach, MA-LMM [16], with gains of 0.4%\nand 10.3%, respectively. Although mPLUG-2 has slightly\nbetter performance on MSRVTT, it demands extensive data\nfor pre-training, leading to a training overhead.\nMemory Analysis. In addition to performance eval-\nuation, we also conduct experiments on memory us-\nage with randomly selected videos from the LVU [45],\nMSRVTT [48], and MSVD [6] datasets. As shown in Fig-\nure 6, existing methods like InstructBLIP [11] and VideoL-\nLaMA [52] instantly exhibit rapid increases in memory con-\nsumption as the number of frames increases, leading to out-\nof-memory (OOM) errors. In contrast, AdaCM2 achieves\na significant reduction in memory usage by nearly 65%,\nand maintains almost constant memory consumption with-\nout sacrificing performance. Furthermore, most of the occu-\npied memory is consumed by LLM, with only a small frac-\ntion allocated for adaptive cross-modality alignment, which\ncan be further alleviated if we use a lightweight LLM.\n5.4. Ablation Studies\nMemory Reduction. Our method reduces video mem-\nory adaptively based on cross-modality attention scores,\n7\n\n0\n200\n400\n600\n800\n1000\nNumber of Frames\n20\n25\n30\n35\n40\n45\nGPU Memory(GB)\nInstructBLIP\nVideoLLaMA\nMA-LMM\nOurs\n300\n350\n400\n16.9\n17.0\n17.1\nFigure 6. Practical memory consumption analysis compared to ex-\nisting methods. The GPU memory usage for InstructBLIP shows\nan exponential increase, leading to out-of-memory (OOM) errors\nafter processing 100 frames. In contrast, VideoLLaMA exhibits\na linear increase in memory usage. AdaCM2 maintains almost\nconstant memory usage without sacrificing performance even as\nframes significantly increase.\nRelation\nSpeak\nScene\nDirector\nGenre\nWriter\nYear\nAvg\nTasks\n0\n20\n40\n60\n80\nTop-1 Accuracy (%)\nRandom Eviction\nOurs\nFigure 7. Performance Comparison between two memory reduc-\ntion strategies, random eviction v.s AdaCM2. Top-1 accuracy is\nreported here.\nachieving extremely long-term video understanding with\nlow memory cost. To investigate the importance of our\nmemory reduction, we compare our cache strategy with the\nrandom eviction on the LVU dataset, which randomly dis-\ncards the same number of visual tokens, as shown in Fig-\nure 7. It is seen that our approach significantly outperforms\nthe random one, indicating the effectiveness of our cross-\nattention-based memory reduction.\nHyperparameters Analysis. Split ratio α and conserve\nratio β are hyper-parameters controlling the preservation of\ncritical visual tokens. In order to determine the appropriate\nsettings, we explore the influence of α and β on memory re-\nduction and performance on MSRVTT and MSVT datasets.\nFigure 8 shows that more retaining tokens result in higher\nmemory usage. Moreover, with the increase of α (β), the ac-\ncuracy rises to a peak and then decreases. This decrease is\ndue to the negative impact of retaining redundant informa-\ntion, which diverts the model’s attention away from essen-\ntial tokens. To achieve an optimal balance between perfor-\nmance and memory efficiency, we set α = 0.1 and β = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nThe split ratio \n40.0\n42.5\n45.0\n47.5\nTop-1 Accuracy (%)\nAccuracy\n16.8\n17.0\n17.2\n17.4\nMemory (GB)\nMemory\n(a) MSRVTT-QA dataset, β = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nThe conserve ratio \n40.0\n42.5\n45.0\n47.5\nTop-1 Accuracy (%)\nAccuracy\n16.8\n17.0\n17.2\n17.4\nMemory (GB)\nMemory\n(b) MSRVTT-QA dataset, α = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nThe split ratio \n55\n56\n57\n58\n59\n60\n61\nTop-1 Accuracy (%)\nAccuracy\n16.8\n16.9\n17.0\n17.1\n17.2\n17.3\n17.4\n17.5\nMemory (GB)\nMemory\n(c) MSVD-QA dataset, β = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nThe conserve ratio \n55\n56\n57\n58\n59\n60\n61\nTop-1 Accuracy (%)\nAccuracy\n16.8\n16.9\n17.0\n17.1\n17.2\n17.3\n17.4\n17.5\nMemory (GB)\nMemory\n(d) MSVD-QA dataset, α = 0.1\nFigure 8. Influence of split ratio α and conserve ratio β on perfor-\nmance and memory usage.\nTable 4. Ablation study of LLM decoding methods, with results\nreported for the METEOR and CIDEr metrics.\nModel\nMSRVTT\nMSVD\nYouCook2\nM\nC\nM\nC\nM\nC\nFlanT5-XL\n21.6↓58.9↓48.7↓166.9↓15.3↓107.6↓\nVicuna-7B\n33.0\n73.1\n51.4\n189.4\n17.6\n125.6\nfor all layers.\nLLM Decoding. LLM plays an essential role in produc-\ning instruction cues. To investigate the influence of LLMs\non our AdaCM2, we compare the results using multiple\nLLMs, including FlanT5-XL [10] and Vicuna-7B [8]. Table\n4 indicates that Vicuna-7B achieves better performance in\nall tasks. Therefore, we choose the Vicuna-7B model as our\nLLM backbone. Since our AdaCM2 framework is designed\nto accommodate various modern LLMs, we plan to conduct\nfurther tests in future work.\n6. Conclusion\nIn this paper, we present AdaCM2, an adaptive cross-\nmodality memory reduction framework for extremely long-\nterm video understanding. The key idea of AdaCM2 is to\nadaptively preserve a certain number of crucial visual to-\nkens most relevant to text queries across different layers\nbased on cross-modality attention, addressing the substan-\ntial memory consumption and modeling long-term tem-\nporal connection. Moreover, our AdaCM2 enables BLIP-\nbased [11, 22, 23] models in a plug-and-play manner, en-\nhancing their capability to process long-term video effec-\ntively. Experiments on video question understanding and\ncaptioning tasks demonstrate the superiority of the pro-\nposed AdaCM2 over existing state-of-the-art approaches.\n8\n\nReferences\n[1] Muhammad\nAdnan,\nAkhil\nArunkumar,\nGaurav\nJain,\nPrashant Nair, Ilya Soloveychik, and Purushotham Kamath.\nKeyformer: Kv cache reduction through key tokens selection\nfor efficient generative inference. Proceedings of Machine\nLearning and Systems, 7, 2024. 3\n[2] Aishwarya Agrawal, Dhruv Batra, Marcus Rohrbach, Jiasen\nLu, Michael Bernstein, and Devi Parikh. Visual question an-\nswering, 2015. 2\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:\na visual language model for few-shot learning.\nAdvances\nin neural information processing systems, 35:23716–23736,\n2022. 3\n[4] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-\nmatic metric for MT evaluation with improved correlation\nwith human judgments. In Proceedings of the ACL Workshop\non Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization, pages 65–72, Ann Arbor,\nMichigan, 2005. Association for Computational Linguistics.\n6\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei.\nLanguage mod-\nels are few-shot learners. arXiv preprint arXiv:2005.14165,\n2020. 1, 3\n[6] David L. Chen and William B. Dolan. Collecting highly par-\nallel data for paraphrase evaluation. In Proceedings of the\n49th Annual Meeting of the Association for Computational\nLinguistics (ACL-2011), Portland, OR, 2011. 6, 7\n[7] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin,\nChenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing\nMao, and Mike Zheng Shou. Videollm-online: Online video\nlarge language model for streaming video. In CVPR, 2024.\n3\n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality, 2023. 6, 8\n[9] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tam´as Sarl´os, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\nDavid Belanger, Lucy J. Colwell, and Adrian Weller. Re-\nthinking attention with performers. CoRR, abs/2009.14794,\n2020. 7\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson,\nShixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models, 2022. 8\n[11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023. 2, 3,\n6, 7, 8\n[12] Ana Claudia Akemi Matsuki de Faria, Felype de Castro Bas-\ntos, Jose Victor Nogueira Alves da Silva, Vitor Lopes Fabris,\nValeska de Sousa Uchoa, Decio Gonc alves de Aguiar Neto,\nand Claudio Filipi Goncalves dos Santos. Visual question\nanswering: A survey on techniques and common trends in\nrecent literature, 2023. 2\n[13] Naglaa Fathy El Dessouky and Hakeem Omar Alquaiti. Im-\npact of green human resource management (ghrm) practices\non organizational performance.\nIn 2020 Second Interna-\ntional Sustainability and Resilience Conference: Technol-\nogy and Innovation in Building Designs(51154), pages 1–4,\n2020. 7\n[14] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei\nHan, and Jianfeng Gao.\nModel tells you what to dis-\ncard: Adaptive KV cache compression for LLMs. In The\nTwelfth International Conference on Learning Representa-\ntions, 2024. 4\n[15] Kristen\nGrauman,\nAndrew\nWestbury,\nEugene\nByrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:\nAround the world in 3,000 hours of egocentric video. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18995–19012, 2022. 2\n[16] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei\nCao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim.\nMa-lmm: Memory-augmented large multimodal model for\nlong-term video understanding. CVPR, 2024. 1, 3, 7\n[17] Noureldien Hussein, Efstratios Gavves, and Arnold W. M.\nSmeulders.\nTimeception for complex action recognition,\n2019. 7\n[18] Noureldien Hussein, Efstratios Gavves, and Arnold W. M.\nSmeulders. Videograph: Recognizing minutes-long human\nactivities in videos, 2019. 7\n[19] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip\nclassification with state-space video models. arXiv preprint\narXiv:2204.01692, 2022. 3, 7\n[20] Vahid Kazemi and Ali Elqursh.\nShow, ask, attend, and\nanswer: A strong baseline for visual question answering.\nCoRR, abs/1704.03162, 2017. 2\n[21] H. Kuehne, A. B. Arslan, and T. Serre. The language of ac-\ntions: Recovering the syntax and semantics of goal-directed\nhuman activities. In Proceedings of Computer Vision and\nPattern Recognition Conference (CVPR), 2014. 6, 7\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 2, 8\n9\n\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023. 2,\n3, 4, 5, 8\n[24] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe\nGan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert:\nEnd-to-end transformers with sparse attention for video cap-\ntioning, 2022. 7\n[25] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus\nRohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning\nto recognize procedural activities with distant supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 13853–13863,\n2022. 7\n[26] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza\nHaffaria, and Bohan Zhuang.\nMinicache: Kv cache com-\npression in depth dimension for large language models. In\nNIPS, 2024. 3\n[27] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2019. 1\n[28] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\nDuan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.\nUnivl: A unified video and language pre-training model for\nmultimodal understanding and generation, 2020. 7\n[29] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. Howto100m:\nLearning a text-video embedding by watching hundred mil-\nlion narrated video clips. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n2630–2640, 2019. 1\n[30] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 3\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to follow instructions with\nhuman feedback. arXiv preprint arXiv:2203.02155, 2022. 1,\n3\n[32] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra,\nFlorian Metze, Christoph Feichtenhofer, Andrea Vedaldi,\nand Joao F Henriques. Keeping your eye on the ball: Tra-\njectory attention in video transformers. Advances in neural\ninformation processing systems, 34:12493–12506, 2021. 7\n[33] Ledell Wu Quan Sun, Yuxin Fang, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 6\n[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019. Technical report. 1, 3\n[35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\nZhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\nTian Ye, Yanting Zhang, et al. Moviechat: From dense token\nto sparse memory for long video understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18221–18232, 2024. 3\n[36] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning.\nCoRR, abs/1904.01766,\n2019. 7\n[37] Reuben Tan, Ximeng Sun, Ping Hu, Jui-Hsien Wang, Hanieh\nDeilamsalehy, Bryan A. Plummer, Bryan Russell, and Kate\nSaenko. Koala: Key frame-conditioned long video-llm. In\nCVPR, 2024. 1, 3\n[38] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin:\nA large-scale dataset for comprehensive instructional video\nanalysis. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2019. 6, 7\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models, 2023. 1, 3\n[40] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. CoRR, abs/1411.5726, 2014. 6\n[41] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nKevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.\nGit: A generative image-to-text transformer for vision and\nlanguage, 2022. 7\n[42] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu,\nMohamed Omar, and Raffay Hamid.\nSelective structured\nstate-spaces for long-form video understanding. In CVPR\n2023, 2023. 3\n[43] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu,\nMohamed Omar, and Raffay Hamid.\nSelective structured\nstate-spaces for long-form video understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6387–6397, 2023. 7\n[44] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks for action recognition in videos, 2017. 7\n[45] Chao-Yuan Wu and Philipp Kr¨ahenb¨uhl.\nTowards Long-\nForm Video Understanding. In CVPR, 2021. 1, 2, 6, 7\n[46] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han,\nand Mike Lewis. Efficient streaming language models with\nattention sinks. arXiv, 2023. 4\n[47] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,\nYuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,\nGuohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jin-\ngren Zhou. mplug-2: A modularized multi-modal foundation\nmodel across text, image and video, 2023. 7\n[48] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language.\nIn 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5288–5296, 2016. 3, 4, 6, 7\n[49] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-\nham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-\ntext modeling with zero-shot transfer from contrastive cap-\ntioners, 2023. 7\n10\n\n[50] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and\nAlex Smola. Stacked attention networks for image question\nanswering. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 2\n[51] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-\ncas Beyer. Scaling vision transformers, 2022. 6\n[52] Hang Zhang, Xin Li, and Lidong Bing.\nVideo-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding, 2023. 7\n[53] Hang Zhang, Xin Li, and Lidong Bing.\nVideo-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858, 2023. 3\n[54] Zhenyu Zhang, Ying Sheng, Tianyi Zhou andTianlong Chen,\nLianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian,\nChristopher R´e, Clark Barrett andZhangyang Wang, and\nBeidi Chen. H2o: Heavy-hitter oracle for efficient generative\ninference of large language models. In NIPS. NIPS, 2023. 3\n[55] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. In AAAI Conference on Artificial Intelligence, pages\n7590–7598, 2018. 6\n11",
    "pdf_filename": "AdaCM$^2$_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_Reduction.pdf"
}