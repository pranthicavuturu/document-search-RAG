{
    "title": "AdaCM2: On Understanding Extremely Long-Term Video with Adaptive",
    "abstract": "What does the What does the Man play? Man play? The advancements in large language models (LLMs) have propelledtheimprovementofvideounderstandingtasksby Single-Modality Correlation- Dual-Modality Attention Mechanism based Token Merge incorporatingLLMswithvisualmodels.However,mostex- W Memory Bank d isting LLM-based models (e.g., VideoLLaMA, VideoChat) t m areconstrainedtoprocessingshort-durationvideos.Recent p Q-Former ? attemptstounderstandlong-termvideosbyextractingand compressingvisualfeaturesintoafixedmemorysize.Nev- LLM LLM ertheless, those methods leverage only visual modality to Play Football Play Football merge video tokens and overlook the correlation between Figure1.(Left)Existingapproachescompressvisualfeaturesof visual and textual queries, leading to difficulties in effec- videosviasingle-modalitycorrelation;(Right)OurAdaCM2 re- tively handling complex question-answering tasks. To ad- ducesvideomemoryadaptivelybasedoncross-modalityattention. dress the challenges of long videos and complex prompts, we propose AdaCM2, which, for the first time, introduces anadaptivecross-modalitymemoryreductionapproachto from 5 to 15 seconds. However, long-term video under- video-textalignmentinanauto-regressivemanneronvideo standing[45],asub-techniquethatdevelopsmodelstopro- streams. Our extensive experiments on various video un- cess richer information, has played a crucial role in real- derstanding tasks, such as video captioning, video ques- world applications such as movie analysis and video re- tion answering, and video classification, demonstrate that AdaCM2achievesstate-of-the-artperformanceacrossmul- trieval. Unfortunately, it poses significant challenges as video length increases, especially the large memory con- tiple datasets while significantly reducing memory usage. sumptionchallenge.Thenumberofframesthemodelmust Notably, it achieves a 4.5% improvement across multiple process grows rapidly, leading to substantial memory con- tasksintheLVUdatasetwithaGPUmemoryconsumption sumption, thereby preventing prior approaches from pro- reductionofupto65%. cessingsuchlongvideos. Tosolvethelargememoryconsumptionchallenge,many 1.Introduction approaches focus on compressing video tokens. For in- stance, MA-LMM [16] employs a memory bank to com- Video understanding is an important task in computer vi- press visual tokens based on the cosine similarities of ad- sion and artificial intelligence, which involves processing jacent two frames. Koala [37] passes multiple segments and reasoning over visual and textual information. While of video into tokenizer functions that aggregate visual to- the recent success of large language models (LLMs) [5, kenstohandlelongvideos.Eventhoughthosemethodsre- 31,34,39]hassignificantlyimprovedvideo-languagemod- duce memory consumption, they still suffer from two sig- els [27, 29], prior work has primarily focused on short nificantlimitations.1)Ignoringtext-driveninformation:As video understanding tasks, typically with videos ranging shown in Figure 1, existing works compress visual infor- mation without considering textual information, leading to *YuanbinManisaStudentResearcherintheDept.ofCSEatUTArlington whileinaMaster’sstudyatBU.†MiaoYinisthecorrespondingauthor. the loss of vital visual tokens that are highly related to 1 4202 voN 91 ]VC.sc[ 1v39521.1142:viXra",
    "body": "AdaCM2: On Understanding Extremely Long-Term Video with Adaptive\nCross-Modality Memory Reduction\nYuanbinMan1∗,YingHuang1,ChengmingZhang2,BingzheLi3,WeiNiu4,MiaoYin1†\n1DepartmentofCSE,UTArlington,2DepartmentofCS,UniversityofHouston,\n3DepartmentofCS,UTDallas,4SchoolofComputing,UniversityofGeorgia\n{yxm6616, yxh1071}@mavs.uta.edu, czhang48@uh.edu,\nbingzhe.li@utdallas.edu, wniu@uga.edu, miao.yin@uta.edu\nAbstract\nWhat does the What does the\nMan play? Man play?\nThe advancements in large language models (LLMs) have\npropelledtheimprovementofvideounderstandingtasksby Single-Modality Correlation- Dual-Modality Attention Mechanism\nbased Token Merge\nincorporatingLLMswithvisualmodels.However,mostex- W\nMemory Bank d\nisting LLM-based models (e.g., VideoLLaMA, VideoChat) t\nm\nareconstrainedtoprocessingshort-durationvideos.Recent p\nQ-Former ?\nattemptstounderstandlong-termvideosbyextractingand\ncompressingvisualfeaturesintoafixedmemorysize.Nev- LLM LLM\nertheless, those methods leverage only visual modality to\nPlay Football Play Football\nmerge video tokens and overlook the correlation between\nFigure1.(Left)Existingapproachescompressvisualfeaturesof\nvisual and textual queries, leading to difficulties in effec-\nvideosviasingle-modalitycorrelation;(Right)OurAdaCM2 re-\ntively handling complex question-answering tasks. To ad-\nducesvideomemoryadaptivelybasedoncross-modalityattention.\ndress the challenges of long videos and complex prompts,\nwe propose AdaCM2, which, for the first time, introduces\nanadaptivecross-modalitymemoryreductionapproachto\nfrom 5 to 15 seconds. However, long-term video under-\nvideo-textalignmentinanauto-regressivemanneronvideo\nstanding[45],asub-techniquethatdevelopsmodelstopro-\nstreams. Our extensive experiments on various video un-\ncess richer information, has played a crucial role in real-\nderstanding tasks, such as video captioning, video ques-\nworld applications such as movie analysis and video re-\ntion answering, and video classification, demonstrate that\nAdaCM2achievesstate-of-the-artperformanceacrossmul- trieval. Unfortunately, it poses significant challenges as\nvideo length increases, especially the large memory con-\ntiple datasets while significantly reducing memory usage.\nsumptionchallenge.Thenumberofframesthemodelmust\nNotably, it achieves a 4.5% improvement across multiple\nprocess grows rapidly, leading to substantial memory con-\ntasksintheLVUdatasetwithaGPUmemoryconsumption\nsumption, thereby preventing prior approaches from pro-\nreductionofupto65%.\ncessingsuchlongvideos.\nTosolvethelargememoryconsumptionchallenge,many\n1.Introduction approaches focus on compressing video tokens. For in-\nstance, MA-LMM [16] employs a memory bank to com-\nVideo understanding is an important task in computer vi-\npress visual tokens based on the cosine similarities of ad-\nsion and artificial intelligence, which involves processing\njacent two frames. Koala [37] passes multiple segments\nand reasoning over visual and textual information. While\nof video into tokenizer functions that aggregate visual to-\nthe recent success of large language models (LLMs) [5,\nkenstohandlelongvideos.Eventhoughthosemethodsre-\n31,34,39]hassignificantlyimprovedvideo-languagemod-\nduce memory consumption, they still suffer from two sig-\nels [27, 29], prior work has primarily focused on short\nnificantlimitations.1)Ignoringtext-driveninformation:As\nvideo understanding tasks, typically with videos ranging\nshown in Figure 1, existing works compress visual infor-\nmation without considering textual information, leading to\n*YuanbinManisaStudentResearcherintheDept.ofCSEatUTArlington\nwhileinaMaster’sstudyatBU.†MiaoYinisthecorrespondingauthor. the loss of vital visual tokens that are highly related to\n1\n4202\nvoN\n91\n]VC.sc[\n1v39521.1142:viXra\n0:05\n… … … …\n00:00:34 00:27:11 00:50:53 01:40:08 02:02:02 02:10:13\nWhatisdescribed in this video? A group of people is climbing in an indoor gym.\nWhatisthenumberonthebackoftheman’swhite football jersey?\nThe number is 10.\n0:05\n… … … …\n00:01 03:41 06:51 09:06 20:38 20:39\nWhatis the woman doing in the video?\nShe is shopping in a grocerystore.\nWho is the woman speaking to? The cashier.\nFigure2.ThecasestudyofAdaCM2 zero-shotonEgo4D[15]dataset.Asshown,AdaCM2 can1)summarizeanextremelylongvideo\nlasting over 2 hours with limited memory consumption and identify the number on a person’s back at the end accurately, 2) answer\nquestionsrelatedtoamid-lengthvideospanningmorethan20minutes.\nthe text prompt, particularly in Visual Question Answer- the Q-Former. AdaCM2 attention mechanism enables dif-\ning(VQA)[2,12,20,50]taskswithcomplextextprompts. ferent numbers of visual tokens preserved across differ-\n2)Thelackofsimilarityadaptability:Previousmethodsat- entlayersbasedoncross-modalitycorrelation.Finally,the\ntempttocompressvisualtokenswithinthefixedandprede- learnedqueriesaresenttoLLMtogeneratetheanswer.To\nfined frame interval, failing to capture the dynamic nature thebestofourknowledge,AdaCM2 isthefirst framework\nof video information and leading to inefficient and inflexi- that leverages cross-modal attention integrated into the vi-\nblememoryreduction.AsshowninFigure3c,thesimilari- sual encoder to enable extremely long-term video under-\nties of adjacent frames vary across different layers. More- standing with efficient memory reduction. Moreover, our\nover, temporally distant frames still exhibit high similar- AdaCM2 can improve BLIP-based [11, 22, 23] models in\nity in deep layers. Consequently, if not appropriately ad- a plug-and-play manner, enhancing their capability to pro-\ndressed, such limitations will severely hinder their perfor- cesslong-termvideoeffectively.Ourcorecontributionsare\nmanceinmanypracticalapplications. summarizedasfollows:\n• We propose an efficient video memory reduction frame-\nTo address the above limitations, in this paper, we pro-\nposeAdaCM2,onunderstandingextremelylong-termvideo work,AdaCM2 forlong-termvideounderstandingbased\non cross-modality attention, which, for the first time,\nwith adaptive cross-modality memory reduction. As illus-\nfacilitates interaction between visual features and text\ntrated in Figure 1, our key idea is to adaptively preserve a\npromptsinvideounderstanding.\ncertainnumberofcrucialvisualtokensthataremostrele-\n• Wepresentacross-modalityattentionmodulethatadap-\nvant to text queries across different layers based on cross-\ntively analyzes and evaluates visual tokens according\nmodality attention. We introduce two intriguing observa-\nto the correlation with input text prompts, enabling ex-\ntions:1)Onlyasubsetofvisualkeystatesexhibithighcor-\ntremely long-term video tasks with a dynamic crucial\nrelations with text query; 2) Correlation varies across dif-\nvideotoken-preservingstrategy.\nferent layers. Our observations open opportunities for ef-\n• We conduct experiments on multiple video understand-\nfective memory reduction by fully exploiting the inherent\ning tasks. Our proposed AdaCM2 achieves 4.5% accu-\ncross-modal capabilities of the existing visual encoders to\nracyimprovementacrossmultipletasksintheLVU[45]\ncompressvisualtokens.Specifically,wefirstextractvisual\ndataset.Particularly,AdaCM2 exhibitspromisingperfor-\nrepresentationsfromvideoframesusingafrozenvisualen-\nmanceontheVQAandvideocaptioningtasks.Moreim-\ncoder, i.e., Q-Former [23]. Then, to solve the substantial\nportantly, AdaCM2 reduces GPU memory consumption\nmemoryconsumptionchallengeandmodellong-termtem-\nporal connection, we propose AdaCM2 attention to learn by65%,highlightingsuperiorperformanceandefficiency\ninpractice.\nthe“query”regressivelyinaframe-by-framemannerwithin\n2\n1.00 Layer 2\n700 Layer 4\n0.014 Layer 6\n600 0.95 Layer 8\n0.012 Layer 10 Layer 12 500 0.90\n0.010\n0.008 400 0.85\n300\n0.006 0.80\n200\n0.004 0.75\n100\n0.002\n0.70\n0\n0.0000.0020.0040.0060.0080.0100.0120.0140.016 0 10 20 30 40 50 60 70\nVisual Tokens Attention Score Adjacent Frame Distance\n(a) (b) (c)\nFigure3.Visualizationforcross-modalityattention,generatedusingarandomlysampledvideofromtheMSR-VTT[48]dataset.(a)Cross-\nattentionscoremapofthe74thframeinthefinallayerandlasthead.(b)Cross-attentionscoredistributionofthe80thframeinthefinal\nlayerandlasthead.(c)Thelayer-wisecosinesimilaritiesofattentionscoresbetweenthecurrentframeandadjacentframes.\n2.RelatedWork KV Cache Eviction. KV cache eviction, a memory re-\nduction method that retains important context key-value\nVideo-Language Models. With the recent advancements pairs,iswidelyadoptedinLLMsinference.H O[54]finds\n2\nof large language models (LLMs) [5, 31, 34, 39], video- thatkeepingtherecenttokens,togetherwith“heavy-hitter”\nlanguage models have been integrated by LLMs with im- tokens measured by attention scores, is sufficient to main-\nage encoders for multi-modal understanding and reason- tain LLM’s performance. Similarly, KeyFormer [1] retains\ning[3,7,30].BLIP-2[23]introducesalightweightquery- only the key tokens in the KV cache by identifying these\ning transformer [23] to bridge the modality gap between crucialtokensthroughanovelscoringfunction.Inaddition,\nthe frozen pre-trained image encoder and LLMs. Instruct- based on the observation that KV cache states are highly\nBLIP [11] further extracts informative visual features tai- similar between adjacent layers in the middle-to-deep sec-\nloredtothegiveninstructionwithinstruction-awareQuery tions of LLM, MiniCache [26] compresses the KV cache\nTransformer.LLaVA[53]leverageslanguage-modeltogen- across layers using a novel depth-based approach, signifi-\nerate multi-modal instruction-following data and improves cantly reducing the memory footprint for LLM inference.\nmodelgeneralizationability.VisionLLM[7]providesauni- Building on the success of the eviction-based method in\nfied perspective for vision and language tasks by treating managinglong-contextLLMs,weefficientlyprocesslong-\nimagesasaforeignlanguage.Italignsvision-centrictasks term videos based on a similar philosophy. Moreover, our\nwithlanguagetasks,whichcanbeflexiblydefinedandman- method focuses on reducing the redundancy of visual to-\naged using language instructions. However, these models kens in the long video understanding task, which is more\nare prone to significant memory overhead when applied to challenging due to more information contained in videos\nlongvideounderstandingtasks. andtheinteractionsbetweenvisualandtextmodalities.\nLong-Term Video Understanding. Long video under-\n3.Observations\nstandingfocusesondetectinglong-rangepatternsinvideos\nlonger than 30 seconds or even several minutes. To re- To investigate the main bottleneck that hinders long-term\nducememoryandcomputationalrequirements,[16,35]re- video understanding, we conduct a comprehensive study\nducetheredundancyofvisualinformationbasedontheco- on the process between video frames and text prompts,\nsinesimilaritiesofadjacentvisualtokens.Otherworkslike which generally perform in the Q-Former. This process\nVis4mer [19] leverage a standard transformer encoder for aligns visual and textual information and causes substan-\nshort-range spatiotemporal feature extraction and a multi- tial memory consumption when video lengths increase. In\nscale temporal Selective Structured State-Spaces (S4) [42] this section, we will first analyze the cross-attention spar-\ndecoder for long-range temporal reasoning. Koala [37] sity within a frame and then show the generalization of\nsplits a long video into multiple segments and then aggre- cross-attentionsparsityacrossvideosandlayers.Theseob-\ngatesvisualtokenstoprocesslongvideos.Consideringthat servationsdemonstratetheredundancyinthedual-modality\nthegoaloflong-termvideounderstandingistoanswerthe processing and inspire the foundation of our approach\ntext question corresponding to the video, our method con- AdaCM2proposedinSection4.\nsidersthecorrelationbetweenthevisualfeaturesandthein-\n3.1.Intra-FrameCross-AttentionSparsity\nputtextinformationbasedonadaptivecross-modalityatten-\ntion, significantly reducing memory consumption and en- Existing approaches compress visual tokens solely based\nablingextremelylong-termvideounderstanding. onthesimilaritiesamongvideoframes.However,thoseap-\n3\nsnekoT\ntxeT\n0\n2\n4\n6\n8 0121416181022242628203\n0 2 4 6 8 01 21 41 61 81 02 22 42 62 82 03 23 43 63 83 04 24 44 64 84 05 25 45 65 85 06 26 46 66 86 07 27 47 67 87 08 28 48 68 88 09 29 49 69 89\nycneuqerF\nytiralimiS\nenisoC\n0:05 Video Q-Former Q:Whereisthewomandoinginthe\nvideo?Andwhoisthewomantalking\n… aboutinlastofthevideo?\nF\nF\nAttention N\nVisual … Visual\n𝑸𝒕 Map\n𝑿𝒕𝒆𝒙𝒕\nEncoder Encoder AdaptiveMemory\nReduction\nLLM\nVideo Video Layer1 Layer2 … Layeri-1 Layeri\nQ-Former Q-Former 𝒇𝒕 t-1 Video Cache 𝑸𝒍\nAdaptive Memory Reduction\n𝑸𝒕 VideoCache AttentionMap VideoCache A sto: rS e.h Ae ni ds ss hh eo ip sp ti an lg kinin gtoth ce asg hr io ec re .ry\nLayer 1: 1− 𝛼1 𝛼1 L La ay ye er r 1 2: : Layer 1:\n𝛽1 L La ay ye er r i i- :1: Layer 2:\nVideoCache Textual Token\nLayer i: Layer 1: Layer i-1:\n1− 𝛼𝑖 𝛼𝑖 Layer 2: Learnable Query Token\n𝛽𝑖 Layer i-1: Layer i: Visual Token\nLayer i:\n(1)Partitioning (2)Identifying (3)Reducing Evicted Video Cache\nFigure4.TheframeworkofAdaCM2.Withvideoandtextqueryasinput,AdaCM2firstutilizesavisualencodertoextractvisualfeatures\nfromvideoframes.Then,videoQ-Formerembedsthecorrelationbetweenvisualfeaturesandthetextpromptintoalearnablequeryin\naregressivemanner.Finally,LLMgeneratestheanswerbasedonthelength-limitedqueryembedding.Toreducememoryconsumption\nchallengeduringtheprocessofAdaptiveMemoryReduction,theVideoCacheispartitionedintopreviousandrecentparts.Basedoncross-\nmodalityattentionscore,AdaCM2thenidentifiesimportantvisualfeaturesandremoveslayer-wiseunimportantvisualtokensfromcache.\nThesnowflakedenotesfrozenpre-trainedmodels,whilethefiretagrepresentsmodelsthatarefine-tuned.\nproachesmaymisskeyvisualtokenshighlyrelatedtotex- visualtokens.Thesetokensaregroupedandundergomulti-\ntual input, leading to accuracy loss for tasks with complex layercross-attentionoperationswithprompttokens.Tore-\ntext questions. Motivated by [14, 46], which have shown duce the memory consumption of redundant visual tokens\nthatretainingonlyasubsetofsalienttokensresponsiblefor globally, we further analyze cross-attention sparsity across\nthemajorityofattentionscorescanbesufficienttomaintain framesandlayers.\nthe performance of LLMs, we observe the similar cross- Observation 2: Correlation varies across different\nattentionsparsityinthevisual-textualalignmentprocess. layers.BasedontheBLIP2[23]Model,weconductzero-\nObservation1:Onlyasubsetofvisualtokensexhibits shotinferenceacrossmultipledatasetsandtasks.Figure3c\nhigh correlations to the text query within a frame. In revealsthatthecross-attentionscoresexhibithighsimilarity\nFigure 3a, we visualize the cross-attention scores for the between adjacent frames, where cosine similarity exceeds\n74-thframeinthefinallayerandthelastattentionheadby 90% among the recent five frames. More importantly, the\nperforminginferenceonQ-Formerwithrandomlysampling similarity ismore significant in deeperlayers than in shal-\ndatafromtheMSR-VTT[48]dataset.Thedarkcolorindi- lowlayers.Thisobservationhighlightstheconsistentredun-\ncates that the visual token exhibits a slight cross-modality dancythroughoutthevideoandthediverselevelsofredun-\ncorrelationwiththetexttoken.Wecanobservethatonlya dancyacrossdifferentlayers.Correspondingly,wepropose\nsubset of visual tokens within a frame exhibits high corre- to adaptively reduce visual memory consumption across\nlations to text tokens. Moreover, the cross-attention scores different layers according to layer-wise cross-modality at-\nexhibitanormaldistributionasdepictedinFigure3b,with tention, making memory compression dynamic and flexi-\nsignificant correlations primarily observed in the tail val- ble.\nues.Thisobservationmotivatesustodevelopanalgorithm\nthat identifies necessary visual tokens according to cross- 4.Methodology:AdaCM2\nmodalityattentionscores,therebypreservingcrucialvisual\ninformationmostimportanttothetextualprompts. We present AdaCM2, an adaptive cross-modality memory\nreduction framework for extremely long-term video un-\n3.2.Layer-WiseCross-AttentionSimilarity\nderstanding. AdaCM2 consists of three stages, including\nWe have demonstrated the cross-attention sparsity within 1) video feature extraction by a visual encoder; 2) adap-\na video frame. Due to the significantly increased frame tive memory reduction based on cross-modality attention\nlength,a long-termvideocontains asubstantialnumber of with visual-textual embedding alignment; 3) text genera-\n4\n… …\nSelf-Attention Cross-Attention\ntionwithalargelanguagemodel,asillustratedinFigure4.\nAdaCM2adaptivelyreducespeakmemoryconsumptionby\n𝑲\" 𝑽\"\nregressivelygeneratinglearnablequerytokensthatpreserve\nthe temporal continuity of video and a layer-wise cross-\nmodalitymemoryreductionalgorithm.\nSum of Columns\n4.1.VideoFeatureExtraction\nVideo Cache\nAs shown in Figure 4, similar to common video under- Attention Score Ranking (Top-N) (Conserved)\nstanding workflow [23], AdaCM2 extracts video features\nusing a pre-trained visual encoder. Given a video with Conserved Tokens Evicted Tokens Recent Tokens Video Cache\na sequence of T frames, the encoder first encodes each Figure5.Illustrationforourvideomemoryreduction.Thevideo\ncacheisfirstpartitionedintorecentandpreviousparts.Important\nframeandgeneratesthecorrespondingvideofeaturesX =\n[x ,x ,x ,··· ,x ],wherex ∈ RP×C istheframefea- visualtokenswithhighcross-modalityattentionscoresinthepre-\n1 2 3 T t viouscachearethenpreserved.\nturesattimet,P andC denotethenumberofvisualtokens\nabouteachframeandthechanneldimensionofeachtoken,\nrespectively.Then,toincorporatetemporalinformationinto whereS ∈ RN×tP istheattentionscore,Q ∈ RN×C is\nt t\ntheframe-levelfeatures,apositionalembeddingisapplied theconcatenationofthelearnablequeryQ andthetextem-\nl\nasfollows: beddingX ,N isthenumberofqueryandtexttokens.\ntext\nf =x +E(t),f ∈RP×C, (1) Layer-WiseVideoMemoryReduction.Storingallvi-\nt t t\nsual tokens in the video cache is impossible due to the\nwhere E(·) represents the position embedding of a frame, increasingly large memory cost while processing video\nandf indicatestheframefeatureswithtemporalinforma- frames, especially for long video understanding tasks. As\nt\ntionattimet. showninObservation3.1,thereissubstantialvisualredun-\ndancy in videos where only a subset of visual tokens ex-\n4.2. Adaptive Memory Reduction with Cross-\nhibit high correlations with text tokens. Based on this ob-\nModalityAttention\nservation, in AdaCM2, we propose a cross-modality atten-\nAfterextractingvisualfeaturesfromthevideo,aQ-Former tionmechanismtoreducememoryconsumptionaccording\nisleveragedtoalignvisualandtextualfeatures.Bylearning to the visual-textual redundancy. AdaCM2 first identifies\na query Q\nl\n∈ RNl×C in the Q-Former model, the visual importantvisualfeaturesbasedoncross-modalityattention\nfeatures are refined to align the text description based on scoreandthenadaptivelyreducesvideomemorybyremov-\nmulti-layers cross-modal attention mechanisms, where N ing layer-wise visual tokens not less correlated to textual\nl\nisthenumberoflearnablequerytokensandC denotesthe information.\nnumberoffeaturechannels. 1 Identifying Important Visual Features based on\nVisual-Textual Feature Alignment in a Regressive Cross-ModalityAttentionScore.Foravideotokenf (i)at\nt\nManner. Unlike existing methods that directly process all spatiallocationi ∈ {1,··· ,tP},thecross-modalityatten-\nframesintotheQ-Formerandalignvisualandtextualinfor- tion score Sc(i) accumulates the attention scores between\nt\nmation by one shot, we propose to learn the query Q re- f (i)andtexttokens,whichcanbeformulatedas\nl t\ngressivelyinaframe-by-framemanner,enablingthereduc-\nj=N\ntionofirrelevantvisualtokensbasedonthecross-modality (cid:88)\nSc(i)= S (j,i). (5)\ncorrelationinalimitedmemorysize. t t\nAdaCM2utilizesvideocacheandcurrentvisualfeatures j=1\ntoaligntextfeatures.Tobespecific,letK ∈RtP×C,V ∈\nt t Cross-modalityattentionscorescapturethecorrelationbe-\nRtP×C torepresentvideocacheattimet,whicharestored\ntween visual and textual features. Therefore, we identify a\ninmemorybyvisualtokensbeforetimetandattimetas:\nvideo token as pivotal if it receives a high cross-modality\nattentionscore.\nK =[K ,f W ], (2)\nt t-1 t K\n2 Adaptively Reducing Video Cache Layer-Wise. As\nV =[V ,f W ], (3)\nt t-1 t V demonstratedinObservation3.2,visualfeaturesexhibitdif-\nwhere W and W are weight matrics. Therefore, the ferentlevelsofsimilaritywithtextualfeaturesacrossdiffer-\nK V\ncross-modalityattentioncalculationintheQ-formercanbe ent layers. Guided by cross-modal attention score, we fur-\ndefinedas therdesignanadaptivevideomemoryreductionalgorithm\nforthevideocache,asshowninFigure5.\n(cid:18)\nQ\n·KT(cid:19)\nA =S ·V =softmax t√ t ·V , (4) According to the order that visual token stored in the\nt t t t\nC videocache,attimet,wesplittheK forthecurrentlayer\nt\n5\nAttention\nMap\nintopreviouscache,Kˆ ,andrecentcache,K˜ : Algorithm1Layer-wiseAdaptiveVideoReduction.\nt t\nK\nt\n=[Kˆ t,K˜ t], 1: Input: Video frames {f t}T t=1, hyper-parameters α,β\n(6) foreachlayer;\n|Kˆ |/|K˜ |=(1−α)/α\nt t 2: Output: ReducedvideoKVcacheK t,V t.\n3: fort=1toT do\nwhere | · | measures the number of tokens in cache, α is\nthe split ratio. For the visual tokens in recentcache, K˜ , 4: K t ←[K t−1,f tW K];V t ←[V t−1,f tW V];\nthat reserves the latest information, we still keep it in tht e 5: Partitioning:K t →[Kˆ t,K˜ t];V t →[Vˆ t,V˜ t];\nmemory.Ontheotherhand,forthepreviouscache,Kˆ ,we 6: Identifying:DetermineS tcusingEq.5;\nt\nonly retain visual tokens with top-β cross-modality atten-\n7: Reducing:ObtainK tusingEq.7;\ntionscoresinmemoryandthusobtaintheconservedcache 8: K t ←[K t,K˜ t];V t ←[V t,V˜ t];\n9: endfor\nK .Theprocessisdefinedasfollows:\nt\n(cid:110) (cid:111)\nK = Kˆ (i)|i∈argtop(Sˆc,β) ,\nt t t\n(7) 5.Experiments\n|K |/|Kˆ |=β,\nt t\n5.1.TasksandDatasets\nwhere argtop(·,β) denotes the indices of visual tokens\nwithtop-β cross-modalityattentionscores,Sˆc isthecross- Long-Term Video Understanding. We conduct experi-\nt\nmodality attention scores of tokens in Kˆ , β is the con- ments on three common long-term video understanding\nt\nserveratio.Finally,thevideocacheK canbecompressed datasets,includingLVU[45],Breakfast[21]andCOIN[38].\nt\nas K = [K ,K˜ ]. Based on our Observation 3.2, i.e., We report the Top-1 accuracy as the evaluation metric. In\nt t t\nredundancy across layers varies, our adaptive video mem- particular, for the LVU dataset, we focus our experiments\noryreductionisperformedlayer-wise.Correspondingly,we on seven classification tasks: relationship, speaking style,\nset α’s and β’s for different layers. It is worth noting that scene,director,genre,writer,andreleaseyear.Thesetasks\nthe adaptive reduction is also applied to another type of provideadiverserangeofchallengesforevaluatingmodel\nvideocache,V . performanceindifferentaspectsofvideounderstanding.\nt\nLetr =α+(1−α)β ≤1.Throughdeductivereasoning, Video Question Answering. We evaluate AdaCM2on\nthefinalsizeofvideocacheK canbederivedas: twopopularvideoquestionansweringdatasets:MSRVTT-\nT\nQA [48], MSVD-QA [6]. MSRVTT-QA and MSVD-QA\n(cid:88)T Pr(cid:0) 1−rT(cid:1)\ncontainshortervideos,rangingfrom10to15seconds.Re-\n|K |=P rt = (8)\nT 1−r garding the dataset scale, MSRVTT-QA includes 10,002\nt=1\nvideos,MSVD-QAhas1,971videos.\nWith the continuous increase of video duration, T → ∞, VideoCaptioning.Wepresentthevideocaptioningre-\n|K T| is converged to a constant Pr/(1−r), showing the sults with the METEOR [4] and CIDEr [40] metrics on\nmaximum memory requirement of AdaCM2 to processing three popular datasets, MSRVTT [48], MSVD [6], and\nunboundedvideolength. YouCook2 [55]. It is noted that YouCook2 encompasses\nvideoslongerthanfiveminutes,posingasignificantmem-\n4.3.TextGeneration\noryconsumptionchallengetounderstandthevideosacross\nWith the regressive processing of video frames, the learn- suchlongperiods.\nablequeryQ ∈RN×C inQ-Formerhasmodeledthelong-\nl\nterm temporal connection from the input video. Then, the\n5.2.ImplementationDetails\nLLM will generate the answer based on the learned Q , a\nl\nlength-limitedvectoraggregatingthelong-terminputvideo Forthevisualencoder,weutilizethepre-trainedimageen-\ninformation. Specifically, assume Y = {y 1,y 2,··· ,y M} coder ViT-G/14 [51] from EVA-CLIP[33]. To efficiently\nistheanswercomprisingasequenceofM words,wemini- alignthevisualandtextfeatures,weemploythepre-trained\nmizethecross-entropylossduringtrainingasfollows: Q-FormerweightsfromInstructBLIP[11].FortheLLMde-\ncoding, we use the pre-trained LLM, Vicuna-7B [8] V1.1.\nM\n(cid:88) During training, we keep the visual encoder and the LLM\nL(V,P,Y)=− y logp(y |y ,Q ,X ) (9)\ni i <i l text decoder frozen, and fine-tune the trainable parameters of\ni=1\nQ-former.Forvideoinput,weextractframesataspeedof\nwhere p(y |y ,Q ,X ) denotes the probability for the 10fps.Allexperimentsaretestedonourserverwitheight\ni <i l text\ni-th word y given the preceding sequence of ground truth NVIDIA RTX 6000 Ada GPUs, two AMD EPYC 9254\ni\nwordsy . CPUs,and1.5TBmemory.\n<i\n6\nTable1.Comparisonwithstate-of-the-artmethodsontheLVUdataset.Theunderlinednumbermeansthesecondbest.\nContent Metadata\nModel Avg\nRelation Speak Scene Director Genre Writer Year\nObj T4mer[45] 54.8 33.2 52.9 47.7 52.7 36.3 37.8 45.1\nPerformer[9] 50.0 38.8 60.5 58.9 49.5 48.2 41.3 49.6\nOrthoformer[32] 50.0 38.3 66.3 55.1 55.8 47.0 43.4 50.8\nVideoBERT[36] 52.8 37.9 54.9 47.3 51.9 38.5 36.1 45.6\nLST[19] 52.5 37.3 62.8 56.1 52.7 42.3 39.2 49.0\nVIS4mer[19] 57.1 40.8 67.4 62.6 54.7 48.8 44.8 53.7\nS5[43] 67.1 42.1 73.5 67.3 65.4 51.3 48.0 59.2\nMA-LMM[16] 58.2 44.8 80.3 74.6 61.0 70.4 51.9 63.0\nOurs 63.1 40.2 86.2 75.4 68.0 77.0 62.5 67.5\nTable2.ComparisonontheBreakfastandCOINdatasets.Top-1 Table 3. Comparison on the MSRVTT, MSVD and YouCook2\naccuracyisreported. datasets.\nModel Breakfast COIN MSRVTT MSVD YouCook2\nModel\nTSN[44] - 73.4 M C M C M C\nVideoGraph[18] 69.5 -\nUniVL[28] 28.2 49.9 29.3 52.8 - 127.0\nTimeception[17] 71.3 -\nSwinBERT[24] 29.9 53.8 41.3 120.6 15.6 109.0\nGHRM[13] 75.5 - GIT[41] 32.9 73.9 51.1 180.2 17.3 129.8\nD-Sprv.[25] 89.9 90.0 mPLUG-2[47] 34.9 80.3 48.4 165.8 - -\nViS4mer[19] 88.2 88.4 VideoCoca[49] - 73.2 - - - 128.0\nS5[43] 90.7 90.8 VideoLLaMA[52] 32.9 71.6 49.8 175.3 16.5 123.7\nMA-LMM[16] 93.0 93.2 MA-LMM[16] 33.4 74.6 51.0 179.1 17.6 131.2\nOurs 94.4 93.3 Ours 33.0 73.1 51.4 189.4 17.6 125.6\n5.3.MainResults TheresultsshowthatAdaCM2 outperformsthepriorstate-\nof-the-art approach, MA-LMM [16], with gains of 0.4%\nLong-Term Video Understanding. We compare\nand 10.3%, respectively. Although mPLUG-2 has slightly\nAdaCM2 with state-of-the-art methods on the LVU [45]\nbetterperformanceonMSRVTT,itdemandsextensivedata\nbenchmark. As shown in Table 1, our model outperforms\nforpre-training,leadingtoatrainingoverhead.\nthe existing long-term video understanding models, in-\nMemory Analysis. In addition to performance eval-\ncluding Object transformer [45], S4 [43], VIS4mer [19],\nuation, we also conduct experiments on memory us-\nVideoBERT [36], LST [19], and MA-LMM [16] across\nage with randomly selected videos from the LVU [45],\nboth content understanding and metadata prediction tasks.\nMSRVTT [48], and MSVD [6] datasets. As shown in Fig-\nThe results indicate that AdaCM2 achieves a significant\nure6,existingmethodslikeInstructBLIP[11]andVideoL-\nimprovementacrossmosttasks,increasingTop-1accuracy\nLaMA[52]instantlyexhibitrapidincreasesinmemorycon-\nby4%comparedtoMA-LMM[16].\nsumptionasthenumberofframesincreases,leadingtoout-\nIn Table 2, we evaluate our AdaCM2 on the Breakfast\nof-memory (OOM) errors. In contrast, AdaCM2 achieves\n[21] and COIN [38] datasets. It is worth noting that these\na significant reduction in memory usage by nearly 65%,\ndatasets present a greater memory consumption challenge\nandmaintainsalmostconstantmemoryconsumptionwith-\ndue to the longer and more diverse videos they contain. It\noutsacrificingperformance.Furthermore,mostoftheoccu-\nis seen that our AdaCM2 outperforms MA-LMM [16] by\npiedmemoryisconsumedbyLLM,withonlyasmallfrac-\n1.4% and 0.1% in Top-1 accuracy, respectively. These re-\ntionallocatedforadaptivecross-modalityalignment,which\nsults demonstrate that our approach achieves superior per-\ncanbefurtheralleviatedifweusealightweightLLM.\nformanceinlong-termvideounderstanding.\nVideo Captioning. Table 3 summarizes the experi-\n5.4.AblationStudies\nmental results on video captioning datasets, including\nMSRVTT, MSVD, and YouCook2. AdaCM2 achieves Memory Reduction. Our method reduces video mem-\n51.4% Meteor and 189.4% CIDEr on the MSVD datasets. ory adaptively based on cross-modality attention scores,\n7\n45 47.5 Accuracy Memory 17.4 47.5 Accuracy Memory 17.4\nInstructBLIP 40 17.1 VideoLLaMA 45.0 17.2 45.0 17.2\nMA-LMM 42.5 42.5 35 17.0 Ours 17.0 17.0\n40.0 40.0\n30 16.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 16.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 16.8\nThe split ratio The conserve ratio\n300 350 400\n(a)MSRVTT-QAdataset,β=0.1 (b)MSRVTT-QAdataset,α=0.1\n25\n20 61 17.5 61 17.5\n60 Accuracy Memory 17.4 60 Accuracy Memory 17.4\n0 200 400 600 800 1000 59 17.3 59 17.3\nNumber of Frames 58 17.2 58 17.2\n17.1 17.1\nFigure6.Practicalmemoryconsumptionanalysiscomparedtoex- 57 17.0 57 17.0\nistingmethods.TheGPUmemoryusageforInstructBLIPshows 56 16.9 56 16.9\nanexponentialincrease,leadingtoout-of-memory(OOM)errors 55 0.1 0.2 0.3 0.4 0.5 0.6 0.7 16.8 55 0.1 0.2 0.3 0.4 0.5 0.6 0.7 16.8\nThe split ratio The conserve ratio\nafter processing 100 frames. In contrast, VideoLLaMA exhibits\n(c)MSVD-QAdataset,β=0.1 (d)MSVD-QAdataset,α=0.1\na linear increase in memory usage. AdaCM2 maintains almost\nFigure8.Influenceofsplitratioαandconserveratioβonperfor-\nconstant memory usage without sacrificing performance even as\nmanceandmemoryusage.\nframessignificantlyincrease.\nTable 4. Ablation study of LLM decoding methods, with results\nRandom Eviction Ours reportedfortheMETEORandCIDErmetrics.\n80\nMSRVTT MSVD YouCook2\n60 Model\nM C M C M C\n40 FlanT5-XL 21.6↓ 58.9↓ 48.7↓ 166.9↓ 15.3↓ 107.6↓\nVicuna-7B 33.0 73.1 51.4 189.4 17.6 125.6\n20\n0\nRelation Speak Scene Director Genre Writer Year Avg\nforalllayers.\nTasks\nLLMDecoding.LLMplaysanessentialroleinproduc-\nFigure7.PerformanceComparisonbetweentwomemoryreduc-\ntion strategies, random eviction v.s AdaCM2. Top-1 accuracy is ing instruction cues. To investigate the influence of LLMs\nreportedhere. on our AdaCM2, we compare the results using multiple\nLLMs,includingFlanT5-XL[10]andVicuna-7B[8].Table\n4 indicates that Vicuna-7B achieves better performance in\nachieving extremely long-term video understanding with alltasks.Therefore,wechoosetheVicuna-7Bmodelasour\nlow memory cost. To investigate the importance of our LLMbackbone.SinceourAdaCM2frameworkisdesigned\nmemoryreduction,wecompareourcachestrategywiththe toaccommodatevariousmodernLLMs,weplantoconduct\nrandom eviction on the LVU dataset, which randomly dis- furthertestsinfuturework.\ncards the same number of visual tokens, as shown in Fig-\nure7.Itisseenthatourapproachsignificantlyoutperforms 6.Conclusion\nthe random one, indicating the effectiveness of our cross-\nattention-basedmemoryreduction. In this paper, we present AdaCM2, an adaptive cross-\nHyperparametersAnalysis.Splitratioαandconserve modalitymemoryreductionframeworkforextremelylong-\nratioβarehyper-parameterscontrollingthepreservationof term video understanding. The key idea of AdaCM2 is to\ncriticalvisualtokens.Inordertodeterminetheappropriate adaptively preserve a certain number of crucial visual to-\nsettings,weexploretheinfluenceofαandβonmemoryre- kens most relevant to text queries across different layers\nductionandperformanceonMSRVTTandMSVTdatasets. based on cross-modality attention, addressing the substan-\nFigure 8 shows that more retaining tokens result in higher tial memory consumption and modeling long-term tem-\nmemoryusage.Moreover,withtheincreaseofα(β),theac- poral connection. Moreover, our AdaCM2 enables BLIP-\ncuracyrisestoapeakandthendecreases.Thisdecreaseis based [11, 22, 23] models in a plug-and-play manner, en-\nduetothenegativeimpactofretainingredundantinforma- hancing their capability to process long-term video effec-\ntion,whichdivertsthemodel’sattentionawayfromessen- tively. Experiments on video question understanding and\ntialtokens.Toachieveanoptimalbalancebetweenperfor- captioning tasks demonstrate the superiority of the pro-\nmanceandmemoryefficiency,wesetα=0.1andβ =0.1 posedAdaCM2overexistingstate-of-the-artapproaches.\n8\n)%(\nycaruccA\n1-poT\n)BG(yromeM\nUPG\n)%(\nycaruccA\n1-poT\n)%(\nycaruccA\n1-poT\n)BG( yromeM\n)BG(\nyromeM\n)%(\nycaruccA\n1-poT\n)%(\nycaruccA\n1-poT\n)BG( yromeM\n)BG(\nyromeM\nReferences Chen,AakankshaChowdhery,AlexCastro-Ros,MariePel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav\n[1] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain,\nMishra,AdamsYu,VincentZhao,YanpingHuang,Andrew\nPrashantNair,IlyaSoloveychik,andPurushothamKamath.\nDai,HongkunYu,SlavPetrov,EdH.Chi,JeffDean,Jacob\nKeyformer:Kvcachereductionthroughkeytokensselection\nDevlin,AdamRoberts,DennyZhou,QuocV.Le,andJason\nfor efficient generative inference. Proceedings of Machine\nWei.Scalinginstruction-finetunedlanguagemodels,2022.8\nLearningandSystems,7,2024. 3\n[11] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat\n[2] AishwaryaAgrawal,DhruvBatra,MarcusRohrbach,Jiasen\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nLu,MichaelBernstein,andDeviParikh.Visualquestionan-\nFung,andStevenHoi.Instructblip:Towardsgeneral-purpose\nswering,2015. 2\nvision-languagemodelswithinstructiontuning,2023. 2,3,\n[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,Antoine 6,7,8\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men- [12] AnaClaudiaAkemiMatsukideFaria,FelypedeCastroBas-\nsch,KatherineMillican,MalcolmReynolds,etal.Flamingo: tos,JoseVictorNogueiraAlvesdaSilva,VitorLopesFabris,\na visual language model for few-shot learning. Advances ValeskadeSousaUchoa,DecioGoncalvesdeAguiarNeto,\ninneuralinformationprocessingsystems,35:23716–23736,\nand Claudio Filipi Goncalves dos Santos. Visual question\n2022. 3 answering: A survey on techniques and common trends in\n[4] Satanjeev Banerjee and Alon Lavie. METEOR: An auto- recentliterature,2023. 2\nmatic metric for MT evaluation with improved correlation [13] NaglaaFathyElDessoukyandHakeemOmarAlquaiti. Im-\nwithhumanjudgments.InProceedingsoftheACLWorkshop pactofgreenhumanresourcemanagement(ghrm)practices\nonIntrinsicandExtrinsicEvaluationMeasuresforMachine on organizational performance. In 2020 Second Interna-\nTranslationand/orSummarization,pages65–72,AnnArbor, tional Sustainability and Resilience Conference: Technol-\nMichigan,2005.AssociationforComputationalLinguistics. ogyandInnovationinBuildingDesigns(51154),pages1–4,\n6 2020. 7\n[5] TomB.Brown,BenjaminMann,NickRyder,MelanieSub- [14] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan- Han, and Jianfeng Gao. Model tells you what to dis-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- card: Adaptive KV cache compression for LLMs. In The\nhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Twelfth International Conference on Learning Representa-\nHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler, tions,2024. 4\nJeffreyWu,ClemensWinter,ChristopherHesse,MarkChen, [15] Kristen Grauman, Andrew Westbury, Eugene Byrne,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nJackClark,ChristopherBerner,SamMcCandlish,AlecRad- Hamburger,HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d:\nford, Ilya Sutskever, and Dario Amodei. Language mod- Aroundtheworldin3,000hoursofegocentricvideo.InPro-\nelsarefew-shotlearners. arXivpreprintarXiv:2005.14165, ceedingsoftheIEEE/CVFConferenceonComputerVision\n2020. 1,3 andPatternRecognition,pages18995–19012,2022. 2\n[6] DavidL.ChenandWilliamB.Dolan.Collectinghighlypar- [16] BoHe,HengduoLi,YoungKyunJang,MenglinJia,Xuefei\nallel data for paraphrase evaluation. In Proceedings of the Cao,AshishShah,AbhinavShrivastava,andSer-NamLim.\n49th Annual Meeting of the Association for Computational Ma-lmm: Memory-augmented large multimodal model for\nLinguistics(ACL-2011),Portland,OR,2011. 6,7 long-termvideounderstanding. CVPR,2024. 1,3,7\n[7] JoyaChen,ZhaoyangLv,ShiweiWu,KevinQinghongLin, [17] Noureldien Hussein, Efstratios Gavves, and Arnold W. M.\nChenanSong,DifeiGao,Jia-WeiLiu,ZitengGao,Dongxing Smeulders. Timeception for complex action recognition,\nMao,andMikeZhengShou. Videollm-online:Onlinevideo 2019. 7\nlargelanguagemodelforstreamingvideo. InCVPR,2024. [18] Noureldien Hussein, Efstratios Gavves, and Arnold W. M.\n3 Smeulders. Videograph: Recognizing minutes-long human\n[8] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao activitiesinvideos,2019. 7\nWu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao [19] MdMohaiminulIslamandGedasBertasius.Longmovieclip\nZhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. classificationwithstate-spacevideomodels. arXivpreprint\nVicuna:Anopen-sourcechatbotimpressinggpt-4with90%* arXiv:2204.01692,2022. 3,7\nchatgptquality,2023. 6,8 [20] Vahid Kazemi and Ali Elqursh. Show, ask, attend, and\n[9] Krzysztof Choromanski, Valerii Likhosherstov, David Do- answer: A strong baseline for visual question answering.\nhan, Xingyou Song, Andreea Gane, Tama´s Sarlo´s, Peter CoRR,abs/1704.03162,2017. 2\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, [21] H.Kuehne,A.B.Arslan,andT.Serre. Thelanguageofac-\nDavid Belanger, Lucy J. Colwell, and Adrian Weller. Re- tions:Recoveringthesyntaxandsemanticsofgoal-directed\nthinkingattentionwithperformers. CoRR,abs/2009.14794, human activities. In Proceedings of Computer Vision and\n2020. 7 PatternRecognitionConference(CVPR),2014. 6,7\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret [22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Blip:Bootstrappinglanguage-imagepre-trainingforunified\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, vision-language understanding and generation. In ICML,\nShixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun 2022. 2,8\n9\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. ingsoftheIEEE/CVFConferenceonComputerVisionand\nBlip-2: Bootstrapping language-image pre-training with PatternRecognition,pages18221–18232,2024. 3\nfrozenimageencodersandlargelanguagemodels,2023. 2, [36] ChenSun,AustinMyers,CarlVondrick,KevinMurphy,and\n3,4,5,8 Cordelia Schmid. Videobert: A joint model for video and\n[24] KevinLin,LinjieLi,Chung-ChingLin,FaisalAhmed,Zhe language representation learning. CoRR, abs/1904.01766,\nGan,ZichengLiu,YumaoLu,andLijuanWang. Swinbert: 2019. 7\nEnd-to-endtransformerswithsparseattentionforvideocap- [37] ReubenTan,XimengSun,PingHu,Jui-HsienWang,Hanieh\ntioning,2022. 7 Deilamsalehy,BryanA.Plummer,BryanRussell,andKate\n[25] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Saenko. Koala:Keyframe-conditionedlongvideo-llm. In\nRohrbach,Shih-FuChang,andLorenzoTorresani.Learning CVPR,2024. 1,3\ntorecognizeproceduralactivitieswithdistantsupervision.In [38] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nProceedingsoftheIEEE/CVFConferenceonComputerVi- DanyangZhang,LiliZhao,JiwenLu,andJieZhou. Coin:\nsionandPatternRecognition(CVPR),pages13853–13863, Alarge-scaledatasetforcomprehensiveinstructionalvideo\n2022. 7 analysis. InIEEEConferenceonComputerVisionandPat-\n[26] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza ternRecognition(CVPR),2019. 6,7\nHaffaria, and Bohan Zhuang. Minicache: Kv cache com- [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\npressionindepthdimensionforlargelanguagemodels. In Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste\nNIPS,2024. 3 Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\n[27] JiasenLu,DhruvBatra,DeviParikh,andStefanLee. Vil-\nlaume Lample. Llama: Open and efficient foundation lan-\nbert:Pretrainingtask-agnosticvisiolinguisticrepresentations\nguagemodels,2023. 1,3\nforvision-and-languagetasks. InAdvancesinNeuralInfor-\n[40] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nmationProcessingSystems(NeurIPS),2019. 1\nParikh. Cider: Consensus-based image description evalua-\n[28] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\ntion. CoRR,abs/1411.5726,2014. 6\nDuan,TianruiLi,JasonLi,TaroonBharti,andMingZhou.\n[41] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,\nUnivl:Aunifiedvideoandlanguagepre-trainingmodelfor\nKevinLin,ZheGan,ZichengLiu,CeLiu,andLijuanWang.\nmultimodalunderstandingandgeneration,2020. 7\nGit: A generative image-to-text transformer for vision and\n[29] AntoineMiech,DimitriZhukov,Jean-BaptisteAlayrac,Ivan\nlanguage,2022. 7\nLaptev, Josef Sivic, and Andrew Zisserman. Howto100m:\n[42] JueWang,WentaoZhu,PichaoWang,XiangYu,LindaLiu,\nLearningatext-videoembeddingbywatchinghundredmil-\nMohamed Omar, and Raffay Hamid. Selective structured\nlionnarratedvideoclips. InProceedingsoftheIEEE/CVF\nstate-spaces for long-form video understanding. In CVPR\nInternationalConferenceonComputerVision(ICCV),pages\n2023,2023. 3\n2630–2640,2019. 1\n[43] JueWang,WentaoZhu,PichaoWang,XiangYu,LindaLiu,\n[30] OpenAI. Gpt-4 technical report. arXiv preprint\nMohamed Omar, and Raffay Hamid. Selective structured\narXiv:2303.08774,2023. 3\nstate-spacesforlong-formvideounderstanding. InProceed-\n[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\ningsoftheIEEE/CVFConferenceonComputerVisionand\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nPatternRecognition,pages6387–6397,2023. 7\nAgarwal,KatarinaSlama,AlexRay,JohnSchulman,Jacob\n[44] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nAskell,PeterWelinder,PaulChristiano,JanLeike,andRyan\nnetworksforactionrecognitioninvideos,2017. 7\nLowe. Traininglanguagemodelstofollowinstructionswith\n[45] Chao-Yuan Wu and Philipp Kra¨henbu¨hl. Towards Long-\nhumanfeedback.arXivpreprintarXiv:2203.02155,2022.1,\nFormVideoUnderstanding. InCVPR,2021. 1,2,6,7\n3\n[46] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han,\n[32] MandelaPatrick,DylanCampbell,YukiAsano,IshanMisra, andMikeLewis. Efficientstreaminglanguagemodelswith\nFlorian Metze, Christoph Feichtenhofer, Andrea Vedaldi, attentionsinks. arXiv,2023. 4\nand Joao F Henriques. Keeping your eye on the ball: Tra-\n[47] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,\njectoryattentioninvideotransformers. Advancesinneural\nYuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,\ninformationprocessingsystems,34:12493–12506,2021. 7\nGuohaiXu,JiZhang,SongfangHuang,FeiHuang,andJin-\n[33] LedellWuQuanSun,YuxinFang,XinlongWang,andYue grenZhou.mplug-2:Amodularizedmulti-modalfoundation\nCao.Eva-clip:Improvedtrainingtechniquesforclipatscale. modelacrosstext,imageandvideo,2023. 7\narXivpreprintarXiv:2303.15389,2023. 6 [48] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alarge\n[34] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario video description dataset for bridging video and language.\nAmodei,andIlyaSutskever. Languagemodelsareunsuper- In2016IEEEConferenceonComputerVisionandPattern\nvisedmultitasklearners. 2019. Technicalreport. 1,3 Recognition(CVPR),pages5288–5296,2016. 3,4,6,7\n[35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng [49] ShenYan,TaoZhu,ZiruiWang,YuanCao,MiZhang,So-\nZhang,HaoyangZhou,FeiyangWu,HaozheChi,XunGuo, hamGhosh,YonghuiWu,andJiahuiYu. Videococa:Video-\nTianYe,YantingZhang,etal.Moviechat:Fromdensetoken textmodelingwithzero-shottransferfromcontrastivecap-\ntosparsememoryforlongvideounderstanding.InProceed- tioners,2023. 7\n10\n[50] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and\nAlexSmola. Stackedattentionnetworksforimagequestion\nanswering. InProceedingsoftheIEEEConferenceonCom-\nputerVisionandPatternRecognition(CVPR),2016. 2\n[51] XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLu-\ncasBeyer. Scalingvisiontransformers,2022. 6\n[52] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tunedaudio-visuallanguagemodelforvideoun-\nderstanding,2023. 7\n[53] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tunedaudio-visuallanguagemodelforvideoun-\nderstanding. arXivpreprintarXiv:2306.02858,2023. 3\n[54] ZhenyuZhang,YingSheng,TianyiZhouandTianlongChen,\nLianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian,\nChristopher Re´, Clark Barrett andZhangyang Wang, and\nBeidiChen.H2o:Heavy-hitteroracleforefficientgenerative\ninferenceoflargelanguagemodels. InNIPS.NIPS,2023. 3\n[55] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards\nautomatic learning of procedures from web instructional\nvideos. InAAAIConferenceonArtificialIntelligence,pages\n7590–7598,2018. 6\n11",
    "pdf_filename": "AdaCM$^2$_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_Reduction.pdf"
}