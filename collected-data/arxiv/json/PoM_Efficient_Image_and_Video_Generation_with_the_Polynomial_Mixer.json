{
    "title": "PoM Efficient Image and Video Generation with the Polynomial Mixer",
    "context": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and com- pute grow quadratically. To alleviate this problem, we pro- pose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire se- quence into an explicit state. PoM has a linear complex- ity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Trans- formers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources. The code is available at https://github.com/davidpicard/HoMM. In a sudden change of pace, high quality image and video generation have evolved from a task seemingly impossible to achieve to a task almost solved by available commercial or open-source tools like Stable Diffusion 3 [19], Sora [7] or MovieGen [61]. At the heart of this success lies the Multi- head Attention (MHA) in the transformer architecture [72] that has excellent scaling properties [58, 82]. These so- called scaling laws [44] enable brute-forcing complex prob- lems such as image and video generation by using very large models trained on gigantic data, at the expense of an ever increasing computational cost. The main focus of current research lies thus in scaling transformer-based approaches to larger models handling larger datasets. The issue with transformers is that the computational cost increases quadratically with the sequence length due to the pairwise computation in MHA. This means that gener- 256 1,024 2,048 4,096 10−2 10−1 100 Image resolution Time in second/image PoM forward+backward PoM forward MHA forward+backward MHA forward Figure 1. Comparison between the speed of PoM and Multi- Head Attention (MHA) in the same DiT-XL/2 architecture for different image resolutions. We use an H100 GPU and compute the average time on 100 synthetic training batches to perform the forward or forward+backward passes. We use synthetic data to remove the influence from data loading. Training with PoM is less costly than inference with MHA at higher resolutions. ating an image at twice the spatial resolution (respectively a video at twice the resolution and double the duration) results in 4 times more patches and thus 16 times more computa- tional cost (respectively 8 times more patches and thus 64 times more computational cost). Attempts at having trans- formers with sub-quadratic complexity [11, 47, 76] intro- duce the additional constraint of fixing the number of to- kens, which prevents generating images or videos of dif- ferent sizes. Alternatively, recurrent models such as State- Space Models (SSM) [26, 27] have been investigated for the task [38, 69, 79] since their complexity is linear with the se- quence length [25]. However, they introduce an arbitrary causal raster scan of the sequence that does not fit the 2D geometry of images very well. In this paper, we enable better scaling in large gener- ative models by introducing a new building block called the Polynomial Mixer (PoM). PoM has a linear complexity 1 arXiv:2411.12663v1  [cs.CV]  19 Nov 2024",
    "body": "PoM: Efficient Image and Video Generation with the Polynomial Mixer\nDavid Picard1, Nicolas Dufour1,2\n1LIGM, ´Ecole Nationale des Ponts et Chauss´ees, IP Paris, Univ Gustave Eiffel, CNRS, France\n2LIX, ´Ecole Polytechnique, IP Paris, CNRS, France\n{david.picard,nicolas.dufour}@enpc.fr\nAbstract\nDiffusion models based on Multi-Head Attention (MHA)\nhave become ubiquitous to generate high quality images\nand videos. However, encoding an image or a video as\na sequence of patches results in costly attention patterns,\nas the requirements both in terms of memory and com-\npute grow quadratically. To alleviate this problem, we pro-\npose a drop-in replacement for MHA called the Polynomial\nMixer (PoM) that has the benefit of encoding the entire se-\nquence into an explicit state. PoM has a linear complex-\nity with respect to the number of tokens. This explicit state\nalso allows us to generate frames in a sequential fashion,\nminimizing memory and compute requirement, while still\nbeing able to train in parallel. We show the Polynomial\nMixer is a universal sequence-to-sequence approximator,\njust like regular MHA. We adapt several Diffusion Trans-\nformers (DiT) for generating images and videos with PoM\nreplacing MHA, and we obtain high quality samples while\nusing less computational resources. The code is available\nat https://github.com/davidpicard/HoMM.\n1. Introduction\nIn a sudden change of pace, high quality image and video\ngeneration have evolved from a task seemingly impossible\nto achieve to a task almost solved by available commercial\nor open-source tools like Stable Diffusion 3 [19], Sora [7] or\nMovieGen [61]. At the heart of this success lies the Multi-\nhead Attention (MHA) in the transformer architecture [72]\nthat has excellent scaling properties [58, 82]. These so-\ncalled scaling laws [44] enable brute-forcing complex prob-\nlems such as image and video generation by using very large\nmodels trained on gigantic data, at the expense of an ever\nincreasing computational cost. The main focus of current\nresearch lies thus in scaling transformer-based approaches\nto larger models handling larger datasets.\nThe issue with transformers is that the computational\ncost increases quadratically with the sequence length due to\nthe pairwise computation in MHA. This means that gener-\n256\n1,024\n2,048\n4,096\n10−2\n10−1\n100\nImage resolution\nTime in second/image\nPoM forward+backward\nPoM forward\nMHA forward+backward\nMHA forward\nFigure 1. Comparison between the speed of PoM and Multi-\nHead Attention (MHA) in the same DiT-XL/2 architecture for\ndifferent image resolutions. We use an H100 GPU and compute\nthe average time on 100 synthetic training batches to perform the\nforward or forward+backward passes. We use synthetic data to\nremove the influence from data loading. Training with PoM is less\ncostly than inference with MHA at higher resolutions.\nating an image at twice the spatial resolution (respectively a\nvideo at twice the resolution and double the duration) results\nin 4 times more patches and thus 16 times more computa-\ntional cost (respectively 8 times more patches and thus 64\ntimes more computational cost). Attempts at having trans-\nformers with sub-quadratic complexity [11, 47, 76] intro-\nduce the additional constraint of fixing the number of to-\nkens, which prevents generating images or videos of dif-\nferent sizes. Alternatively, recurrent models such as State-\nSpace Models (SSM) [26, 27] have been investigated for the\ntask [38, 69, 79] since their complexity is linear with the se-\nquence length [25]. However, they introduce an arbitrary\ncausal raster scan of the sequence that does not fit the 2D\ngeometry of images very well.\nIn this paper, we enable better scaling in large gener-\native models by introducing a new building block called\nthe Polynomial Mixer (PoM). PoM has a linear complexity\n1\narXiv:2411.12663v1  [cs.CV]  19 Nov 2024\n\nlike SSMs while still enabling all pairwise information to\nbe processed like in MHA, obtaining effectively the best of\nboth worlds. From a theoretical standpoint, we prove PoM\ncan be used as a drop-in replacement for attention. Doing\nso in the popular DiT architecture [56, 58] results in im-\nproved scaling such that at higher resolutions, it becomes\nless costly to train a model with PoM than to perform infer-\nence with a model using MHA, as shown on Figure 5.\nTo sum up, the contributions of this paper are the follow-\ning:\n✓We introduce the Polynomial Mixer (PoM), a replace-\nment for MHA that has a linear complexity with respect\nto the sequence length and without sacrificing generation\nquality;\n✓We prove that models equipped with PoM are universal\nsequence-to-sequence approximators;\n✓We train DiT-inspired image generative models and ob-\ntain results of similar quality while being much more\ncompute efficient at higher resolutions;\n✓We train video generative models leveraging PoM with a\nconstant processing cost per frame while not sacrificing\non visual quality.\nOur contribution is therefore primarily fundamental: We\nshow that it is possible to train generative models with an\nalternative mecanism to MHA. We believe this direction\nwill not only ground future research on high resolution im-\nages and very long videos generation, but also could benefit\nmany areas of research (e.g., large language models, vision-\nlanguage models, etc).\n2. Related Work\nDiffusion\nDiffusion models [35, 57, 67] learn a neural\noperator that produces natural images from noise using a\nforward-reverse set of processes. The forward process con-\nsists in pushing the distribution of natural images forward\nto a known distribution, typically Gaussian, which can be\ndone by adding increasing level of noise to the image. The\nreverse process does not have an explicit solution, but can\nbe approximated by a neural network by regressing the local\ninverse of the forward process, i.e., solving\nmin\nθ\nEt∼U(0,1)\n\u0002\n∥εt −fθ(xt, t)∥2\u0003\n,\n(1)\ns.t. xt = αtx0 + γtεt, εt ∼N(0, 1).\n(2)\nHere, αt and γt are chosen such that x0 corresponds to\na natural image whereas x1 corresponds to pure Gaussian\nnoise. A great amount of research has been put into finding\nbetter noise schedules (αt and γt) [4, 31, 45], or improv-\ning the quantity that is regressed [51, 52, 64], keeping the\ngeneral idea of learning to invert step by step the stochastic\ndifferential equation that transforms an image into noise.\nFor image and generation, most efforts have been poured\ninto designing efficient architectures at the task.\nWhile\nthe original DDPM papers [35, 57] sample images in\npixel space, making it unsuitable for large resolution, the\nmost groundbreaking improvement was introduced by Sta-\nble Diffusion [63] with the addition of a variational auto-\nencoder (VAE) that allows the diffusion process to be per-\nformed in a lower dimensional latent space. Stable Diffu-\nsion uses a U-Net architecture complemented by attention\nlayers [63, 65].\nTo benefit more from the scaling prop-\nerties of transformers [44, 82], simpler approaches based\nsolely on transformer layers has been proposed in DiT [58]\nand the subsequent flow-matching version SiT [56]. Most\nmodern text-to-image generation models are now based on\nTransformer layers rather than the U-Net [9, 19, 20, 32].\n[12, 28], train efficient pixel space transformers models by\nleveraging multiscale training and SwinAttention.\nSimi-\nlarly, RIN [10, 40] also proposes an approach using atten-\ntion only, albeit in a Perceiver-IO [42] inspired architecture\nthat uses cross-attention to perform most of the computa-\ntion in a smaller latent space, and has been successfully ex-\ntended to text-to-image [18]. In addition to architectures\nand sampling [2, 84, 86], the importance of training is also\nhighlighted in recent works, from resampling the training\ndata [24, 54] to RL [49, 74, 78] and model averaging [46].\nIn video generation [29, 36, 66, 73, 83], early attempts\nhave focused on extending existing text-to-image models to\nbenefit from their large scale pretraining [5, 21, 22, 34, 37,\n48]. However, the drawback of such approaches is that they\nre-use the VAE of existing text-to-image models which does\nnot encode temporal information, which is thus not com-\npressed. As such, novel architectures using a 2D+t VAE\nsuch as CogVideoX [80], PyramidFlow [43] can benefit\nfrom a smaller latent space leading to less computational\ncosts.\nFast alternative to attention\nSince the introduction of\nTransformers [72], many effort have been made to reduce\nthe quadratic complexity of MHA [11, 47, 76]. Notably,\nmethods like Reformer [47] use fast approximate neigh-\nbors to reduce the size of the attention matrix based on\nthe assumption that most tokens will have zero attention.\nTo go further, Linformer [76] proposes to compute an ex-\nplicit low rank projection of the keys and the values to re-\nduce the complexity of MHA for each query from the size\nof the sequence n to an arbitrary chosen number k ≪n.\nThe main drawback of such approach is that n and k are\nfixed, which means that the model can no longer process\nsequences of varying length. With the advent of Large Lan-\nguage Models and their ability to process extremely long\nsequences [1, 17, 68], recent efforts have been put on more\nefficient implementations such as Flash-Attention [13, 14]\nor KV-cache [6, 55] which seem sufficient for text. However\nfor visual content, the sequence length grows quadratically\nwith the resolution, which, because MHA is also quadratic\n2\n\nin the number of tokens, leads to quartic computational and\nmemory complexity.\nAlternatively, some attempts have been made to just re-\nmove the Multi-Head Attention, such as in Mlp-Mixer [70]\nand Resmlp [71] that replace MHA with simple projec-\ntion on the transpose tensor (i.e., considering the sequence\ndimension as the features). These approaches have been\nshown to obtain competitive results, but similarly to Lin-\nformer, they imply a fix sequence length since this length\nis now an intrinsic dimension of the projection in the\ntranspose direction.\nMore recently, State-Space Models\n(SSM) [26, 27] have become the focus of recent work espe-\ncially in language modeling [15, 23, 50, 88]. SSM are recur-\nrent models, which is highly beneficial for language model-\ning because of the causal property of text. In that case, the\ncomplexity to generate the next token becomes constant. In\nvisual content however, there is no such natural causality\npattern in the spatial dimensions. Attempt to use such mod-\nels for vision tasks have been successful [53, 59, 87], albeit\nat the cost of enforcing an arbitrary 1-dimensional scan or-\nder of the tokens that does not encode well the 2D nature\nof an image. In image generation using diffusion [38, 79],\nsince the model has to be iterated, this results in a doubly\nsequential processing (space and iterations) that does not\nbenefit from the parallel nature of processing images. For\nvideo however, the causal aspect is natural over the time di-\nmension, and recurrent approaches may be more efficient.\n3. Polynomial Mixer and Polymorpher\nWe define a Polymorpher block as a sequence-to-sequence\nfunction mapping Rd×n to Rd×n, composed of two residual\nblocks, a Polynomial Mixer and a feed-forward block.\nFor a sequence X ∈Rd×n, the Polynomial Mixer (PoM)\nshown on Figure 2 is defined as follows:\nPoM(X) = Wo\n\u0002\nσ(WsX) ◦H(X)1⊤\u0003\n, with\n(3)\nH(X) =\n\"\nh(W1X); . . . ;\nk\nY\nm=1\nh(WmX)\n#\n1,\n(4)\nwhere k is the degree of the Polynomial Mixer, σ is the\nsigmoid function, h an activation function, ◦and Q the\nelement-wise (Hadamard) product, and 1 a vector of the\nappropriate dimension filled with ones. The notation [·; ·]\nis for vertical concatenation. The matrices Wo ∈Rd×kD,\nWs ∈RkD×d and W1, . . . , Wk ∈RD×d are the learnable\nparameters of the Polynomial Mixer.\nThe idea of the Polynomial Mixer is to that the sequence\nX ∈Rd×n is uniquely summarized into the representa-\ntion H(X) ∈RkD×1. Each element in X then gets to\nquery H(X) independently thanks to the map S(WsX) ∈\nRkD×n. The queried information is then projected back into\nthe original space with Wo.\nX\nh\nh(W1X); . . . ; Qk\nm h(WmX)\ni\n1\nH(X)\nS(X) = σ(WsX)\nS(X) ◦H(X)1⊤\nWoZ\nP\nFigure 2. Diagram for the Polynomial Mixer. The input se-\nquence is split into two paths. The top path expands each token\nusing a polynomial before they are mixed (averaged)² into a single\nrepresentation. The bottom path expands the tokens into gating\ncoefficients. Both paths are recombined and projected back into\nthe input dimension.\nContrarily to MHA that computes all pairwise exchanges\nof information between tokens in the sequence, the Polyno-\nmial Mixer follows a state-representation (H(X)) approach\nwhere all information is shared in a common memory loca-\ntion that all tokens can access. This state-representation is\ndefined by mixing all tokens of the sequence after they are\nmapped to a high dimensional space by a learned polyno-\nmial, hence the name Polynomial Mixer, and a similar ap-\nproach has been successfully used for learning image rep-\nresentation [41]. The main benefit is that the complexity\nof the approach is no longer quadratic but linear with the\nsequence length n.\nTaking inspiration from transformers with MHA, we de-\nfine a Polymorpher block P as alternating residual Polyno-\nmial Mixers with feed-forward networks as follows:\nP(X) = X + PoM(X) + FF(X + PoM(X)),\n(5)\nwith FF(X) being a two-layer feed-forward network.\nA Polymorpher is a drop-in replacement for any\nTransformer-based architecture as it performs the same role\nof sequence-to-sequence mapping. The main difference is\nin its parametrization: A Transformer is configured by the\nnumber of heads and their dimension in MHA, whereas the\nPolymorpher is configured by its degree k and the dimen-\nsion D of each polynomial.\n3.1. Polymorpher for causal sequences\nA causal sequence can easily be modeled in PoM by adding\na mask M that prevents summing future tokens into the\nblackboard. This corresponds to the following definition\nPoM(X, M) = Wo [σ(WsX) ◦H(X)] ,\n(6)\nH(X) =\n\"\nh(W1X); . . . ;\nk\nY\nm=1\nh(WmX)\n#\nM ⊤.\n(7)\nNow H(X) ∈RkD×n and M ∈{0, 1}n×n is a binary\nmatrix that defines which pairs of tokens are related. Just\n3\n\nlike for MHA, a binary matrix defines an attention pattern\nthat can be arbitrarily chosen.\nIn the special case of causal sequences, M is a lower tri-\nangular matrix. Moreover, one can express the mixing part\nof the Polynomial Mixer as an iterative process as follows:\nH(X):,i =\nX\nj≤i\n\"\nh(W1X); . . . ;\nk\nY\nm=1\nh(WmX)\n#\n:,j\n,\n(8)\n= H(X):,i−1 +\n\"\nh(W1X); . . . ;\nk\nY\nm=1\nh(WmX)\n#\n:,i\n.\n(9)\nIn this formula, H(X):,i is an explicit hidden state that is\nupdated by adding the polynomial mapping of the next to-\nken. Such a configuration enables O(1) inference complex-\nity in the auto-regressive setup, a property that is shared\nwith recurrent networks, but not transformers. Like SSMs,\nPolymorphers have the best of both worlds, they can train\non the whole sequence in parallel and do the inference in\nthe recursive way.\nIn addition, Polymorphers can handle block causal se-\nquences. Let M be a block causal matrix for some integer\nblock size K:\nMi,j = 1 if j ≤⌈i/K⌉K else 0.\n(10)\nWe can now rewrite H as\nH(X):,i =H(X):,⌊i/K⌋K\n+\n⌈i/K⌉K\nX\nj=⌊i/K⌋K\n\"\nh(W1X); . . . ;\nk\nY\nm=1\nh(WmX)\n#\n:,j\n.\n(11)\nIn this configuration, we can sequentially process groups of\ntokens at a time during inference, which reduces the mem-\nory requirement. This is in particular practical for video se-\nquences where it makes sense to have a causal mask in the\ntemporal dimension that makes each frame depend on the\nprevious ones, while keeping the ability of all the tokens\n(patches) of a frame to look at each others, since causality\ndoes not have much sense in the spatial dimension.\n3.2. Theoretical analysis\nWe first show that PoM is equivariant, which means that\npermutations in the input sequence result in permuted out-\nputs. This is a key property that made transformers popular\nand does not hold for other architectures like convolutions:\nProposition 1 (Permutation equivariance). A Polynomial\nMixer is permutation equivariant, i.e., let X ∈Rd×n be\na set of vectors and P a column permutation matrix, then\nPoM(XP) = PoM(X)P.\nProof. For a permutation P, we have\nPoM(XP) = Wo\n\u0002\nσ(WsXP) ◦H(XP)1⊤\u0003\n.\n(12)\nNotice that H(XP) = H(X) because the sum is permu-\ntation invariant, and σ(WsXP) = σ(WsX)P because σ\nis an element-wise operation. Noticing that H(X)1⊤has\nall identical columns allows us to move P outside of the\nbrackets to conclude the proof.\nMore importantly, we can also prove a universal approx-\nimation theorem for Polymorphers similar to what is well\nknown for Transformers [81]. As the polynomial mixer is\nequivariant, it requires the use of positional encoding, which\nalso underlines the similarity between PoM and MHA.\nWe use the following standard definition of distance be-\ntween functions that map sequences to sequences. Given\ntwo functions f and g : Rdn\n→Rdn and an integer\nA ≤p ≤∞, we define the distance dp as:\ndp(f, g) =\n\u0012Z\n∥f(X) −g(X)∥p\npdX\n\u00131/p\n.\n(13)\nThe following theorem holds:\nTheorem 2 (Universal approximation). Let 1 ≤p ≤∞\nand ϵ > 0, then for any given f ∈F the set of continuous\nfunctions that map a compact domain in Rd×n to Rd×n,\nthere exists a Polymorpher g with learned positional encod-\ning such that dp(f, g) ≤ϵ.\nThe proof follows exactly the same scheme as in [81],\nwhere most of the heavy lifting is done by the feed-forward\nnetworks. Their main argument is to show that MHA can\nmap every token in the sequence to a unique value that de-\npends on the entire sequence, and then the feed-forward\nblocks can map those unique values to the desired output. In\nour case, we just have to ensure that the Polynomial Mixer\nhas the same properties as MHA, which is obtained using\nthe following lemma:\nLemma 3 (Contextual mapping (informal)). There exists\nk > 0 for which any Polynomial Mixer q of degree k is a\ncontextual mappings on Rd×n, that is:\n• For any X ∈Rd×n with different entries, q(X) has dif-\nferent entries.\n• For any X, X′ ∈Rd×n that differ at least by one element,\nthen all entries of q(L) and q(L′) are different.\nThe proof is deferred to the appendix and primarily\nuses the fact that a sufficiently high degree polynomial is\nuniquely defined by a sequence of point-wise evaluation.\nAs noted in [81], having the contextual mapping property\nis not so common as it requires to summarize uniquely the\ncontext while preserving the identity of the current token.\nWith these results, we show that a Polymorpher is as po-\ntent as a Transformer for sequence modeling.\n4\n\nX\n[t, c]\nMLP\nLayerNorm\nModulation\nPoM\n·\n+\nLayerNorm\nModulation\nFeed forward\n·\n+\n(a) Image block\nLayerNorm/Modulation\nLayerNorm/Modulation\nMLP\nPoM (cross)\n·\n+\nLayerNorm/Modulation\nPoM (self)\n·\n+\nX\nc]\nt]\n(b) Video block\nFigure 3. Building blocks for our diffusion models using PoM.\nFor class-conditional image generation (a), we follow strictly\nDiT[58] in the AdaLN variant, replacing multi-head attention with\nPoM. For text to video generation (b), we follow a hybrid approach\nin which the encoded text tokens are incorporated into the video\ntokens using PoM instead of cross attention, while the time is used\nas a modulation. Modulation means component-wise scale and\nshift modification based on the coefficients predicted by the MLP\n(similarly to the AdaLN approach).\n4. Diffusion with PoM\nArmed with the definition of PoM and Polymorphers, we\nnow design diffusion models taking inspiration from mod-\nels based on MHA, and show that PoM can replace attention\nin practice. We follow the design choices of DiT [58] and\npropose a class-conditional image generation polymorpher\nas well as a text-to-video generation polymorpher.\n4.1. Architecture design\nImage generation\nFor image generation,\nthe class-\nconditional polymorpher is similar to the AdaLN variant of\nDiT. The image is encoded through the VAE of SD1.5 [63]\nand then features are aggregated into visual tokens X. We\nadd a 2D cosine positional encoding to them before we feed\nthem to the model. The class c and the time step t are em-\nbedded using an embedding matrix and a cosine embedding\nrespectively before being summed together.\nThe model consists in several blocks that combine mod-\nulations, PoM and feed forward networks as shown on Fig-\nure 3a. In each block, the modulation consists in predicting\nfrom the condition c + t a scale γ and a shift β that modify\nthe input by\nx ←γ(x −β).\n(14)\nSimilarly to DiT, the MLP also predicts gates σ that can\nshut down an entire block f thanks to\nx ←x + (1 + s)f(x),\n(15)\nwith the 1 in 1+s being added so that there is a full residual\nconnection when the MLP predicts f(x) = 0. For naming\nthe architectures, we follow the same parametrization as in\nDiT. Namely, an S/2 model has a kernel size and stride of 2\nfor aggregating the VAE features into tokens, and 12 blocks\nof dimension 384. Similarly, an XL/2 model that has 28\nblocks of dimension 1152. For the PoM operation inside\neach block, we use an polynomial of order 2 with an expan-\nsion factor of 2 unless specified otherwise. Pytorch code for\nthe blocks is given in appendix.\nVideo generation\nFor video generation from text, we\nextend the DiT architecture to handle text as a condi-\ntion. We first encode video clips using the 3D VAE from\nCogXVideo [80] and then group the features into visual to-\nkens using a kernel size of 2×2×2 (with 2×2 for the spatial\naxes, and 2 for the temporal axis resulting in a downscaling\nfactor of 16×16×8). We add a 3D cosine positional encod-\ning to the visual tokens before feeding them to the model.\nThe text is encoded using T5 [62] embeddings and the time\nstep is encoded using a cosine embedding.\nThe model consists in blocks using PoM to aggregate in-\nformation between the text condition and the visual tokens\nas shown on Figure 3b. More precisely, a first PoM opera-\ntion is used in a cross fashion, similar to cross-attention, to\naggregate information from the text tokens into the visual\ntokens. Then, a second PoM operation is used to aggregate\ninformation among the visual tokens themselves, similar to\nwhat self-attention would do. Finally, a feed forward mod-\nule processes the visual tokens only. The time step embed-\nding is used in an MLP to predict the coefficients of modu-\nlations and gates at each of the operations.\nWe train a single model of size XL/2 that consists in 20\nlayers of dimension 1152 resulting in 1.1B parameters.\n4.2. Training setup\nFor class-conditional image generation, we train on Ima-\ngeNet. We rescale each image to 256 pixels on their small-\nest size and then take a crop of size 256 × 256. We use both\nthe original images and horizontally flipped version for a\ntotal of 2.4M images. We train a model fθ either using the\ndiffusion loss:\nLD = Et∼U[0,1]∥εt −fθ(xt, c, t)∥2,\n(16)\nor the flow matching loss:\nLFM = Et∼U[0,1]∥vt −fθ(xt, c, t)∥2,\n(17)\nwith vt = εt −x0. For each experimental result, we men-\ntion which loss is used, but the models are trained similarly\n5\n\nFigure 4. Qualitative results on class-conditional generation. We show images sampled with the model DiPoM-XL/2 trained with the\nflow-matching loss LFM at several resolutions for different classes. We use classifier-free guidance with ω = 4s/s0 with s the scale of the\nimage and s0 the reference scale (256).\nwithout requiring change in training hyper-parameters. We\nuse AdamW with a constant learning rate of 10−4 followed\nby a short cooldown with square root decay [30].\nFor video, we used WebVid-2M [3] that we rescale to\n240 × 384 at 16 fps. We keep only the first 5 seconds, cor-\nresponding to 80 frames. This results in a total of 2.5M\nclips. We train using the flow matching loss LFM. We also\nuse AdamW with a constant learning rate of 10−4 followed\nby a short cooldown with square root decay.\n5. Experiments\nWe first show result on class-conditional image generation\nand then of text-to-video generation.\n5.1. Class-conditional image generation\nQuantitative results\nWe compare the results of our XL/2\nmodel trained with the diffusion loss to the state of the art\non Table 1. We compute the Fr´echet Inception Distance\n(FID), the Inception Score (IS), precision (P) and recall (R)\nusing the code from ADM [16] on 50k generated images.\nThe table is split between methods on masked encoding\n(Mask-GIT [8]), diffusion models based on SSM and diffu-\nsion models based on attention. Results are extracted from\nthe corresponding papers. Our images are generated with\n250 steps of the DDIM sampler for the model trained with\nthe diffusion loss LD, and 125 steps of Heun sampler for\nthe model trained with the flow-matching loss LFM, with\nclassifier free guidance (CFG, ω = 0.7 in both cases).\nUsing the evaluation code and reference set from\nADM [16], we obtain an FID of 2.46, which is slightly\n6\n\nModel\nSample config\n#train\nFID↓\nIS↑\nPrecision↑\nRecall↑\nMask-GIT [8]\n6.18\n182.1\n0.80\n0.51\nDIFFUSSM-XL† [79]\n250 steps DDPM\n660M\n2.28\n259.1\n0.86\n0.56\nDiM-H† [69]\n25 steps DPM++\n480M\n2.21\n-\n-\n-\nADM-G [16]\n250 steps DDIM\n500M\n4.59\n186.7\n0.83\n0.53\nLDM-4-G [63]\n250 steps DDIM\n215M\n3.60\n247.7\n0.87\n0.48\nRIN [40]\n1000 steps DDPM\n600M\n3.42\n182.0\n-\n-\nDiT-XL/2† [58]\n250 steps DDPM\n1.8B\n2.27\n278.2\n0.83\n0.57\nSiT-XL/2† [56]\n125 steps Heun\n1.8B\n2.15\n254.9\n0.81\n0.60\nDiPoM-XL/2 LD (ours)\n250 steps DDIM\n950M\n2.46\n240.6\n0.78\n0.60\nDiPoM-XL/2 LFM (ours)\n125 steps Heun\n950M\n3.70\n255.2\n0.79\n0.56\nTable 1. Quantitative results on ImageNet 256 × 256 class-conditional generation. #train denotes the number of training images seen\nduring train (i.e., batch size × number of training steps). † denotes methods evaluated against the Imagenet training set instead of the usual\nADM evaluation archive. We color in blue (respectively in red) DiT and PoM architectures of equivalent size that are trained with the\nsame diffusion loss LD (respectively flow-matching loss LFM), and we bold the best values between the two, even though the results are\nnot evaluated against the same reference set.\nDegree\nExpand\nFID↓\nIS↑\nPrecision\nRecall\n1\n12\n90.1\n15.1\n0.27\n0.36\n2\n6\n87.0\n15.8\n0.29\n0.37\n3\n4\n86.1\n16.0\n0.29\n0.38\n4\n3\n88.8\n15.5\n0.28\n0.36\n6\n2\n90.7\n15.0\n0.28\n0.36\nTable 2. Comparison of different degrees of Polynomial Mixer\nat a constant memory budget with a S/2 model on 10k images from\nImagenet. Having a degree ≥2 is necessary to get good perfor-\nmances, but there is a trade-off between the degree and the expan-\nsion factor.\nabove that of the comparable DiT architecture, but notice\nthat our model was trained for only half of the number steps\nof DiT. In addition, we found FID to be very unreliable as\na metric, as it is highly varying with the reference set. For\nexample, using the validation set of ImageNet, we obtained\n3.45 FID1. We obtain a slightly lower IS compared to DiT,\nbut this could be improved by using a higher CFG. Indeed,\nwe show in appendix that the trade-off between FID ans IS\ncan reach as high as 300 IS at the cost of a much higher\nFID. We obtain a precision/recall trade-off comparable to\nDiT, slightly lower on precision but also higher on recall.\nOverall, the results obtained using PoM are on par with\nthe literature, showing that PoM can be used as a drop-in\nreplacement for multi-head attention in a neural architec-\nture, without requiring either architectural changes or train-\ning hyper-parameter tuning.\nQualitative results\nWe further fine-tune the model on\nhigher resolution data for a small number of steps to ob-\n1It was shown in [60] that randomness affects significantly the results.\n50\n100\n150\n200\nIS\n101\n101.5\n102\n10\n20\n30\n40\n50\nFlops (G)\nFID\nFigure 5. Scaling laws for a DiT-like architecture with attention\nreplaced by PoM. FIDs and Inception Scores (IS) are computed\non 10k samples with classifier free guidance (ω = 1), and shown\nwith a linear regression in log space. Performances scale with the\ncomputation budget, similarly to transformers.\ntain a collection of models able to sample images up to\n1024 × 1024 resolution (which is the maximum resolution\nwe found reasonable to upscale to on ImageNet). We show\nselected samples at these higher resolution on Figure 4. At\nhigher resolution, some classes are collapsed due to the lack\nof available data.\nAblation study\nWe study the impact of the degree of the\npolynomial on Table 2. To enable a fair comparison, we\nconsider a set of S/2 models that have the same dimension\nfor H(X) and compute different trade-offs between the de-\ngree of the polynomials and their dimension. As we can\nsee, having at least second order polynomials is crucial to\n7\n\nModels\nSubject\nBackground\nTemporal\nMotion\nDynamic\nAesthetic\nImaging\nObject\nConsistency\nConsistency\nFlickering\nSmoothness\nDegree\nQuality\nQuality\nClass\nLaVie [77]\n91.4%\n97.5%\n98.3%\n96.4%\n49.7%\n54.9%\n61.9%\n91.8%\nModeScope [75]\n89.9%\n95.3%\n98.3%\n95.8%\n66.4%\n52.1%\n58.6%\n82.3%\nVideoCrafter [33]\n86.2%\n92.9%\n97.6%\n91.8%\n89.7%\n44.4%\n57.2%\n87.3%\nCogVideo [37]\n92.2%\n96.2%\n97.6%\n96.5%\n42.2%\n38.2%\n41.0%\n73.4%\nV-DiPoM-XL/2 no-mask\n90.6%\n96.6%\n99.7%\n97.3%\n31.7%\n28.6%\n47.1%\n29.3%\nV-DiPoM-XL/2 b-causal\n80.4%\n92.2%\n98.1%\n97.4%\n37.5%\n30.5%\n47.9%\n30.0%\nModels\nMultiple\nHuman\nColor\nSpatial\nScene\nAppearance\nTemporal\nOverall\nObjects\nAction\nRelationship\nStyle\nStyle\nConsistency\nLaVie [77]\n33.3%\n96.8%\n86.4%\n34.1%\n52.7%\n23.6%\n25.9%\n26.4%\nModeScope [75]\n39/0%\n92.4%\n81.7%\n33.7%\n39.3%\n23.4%\n25.4%\n25.8%\nVideoCrafter [33]\n25.9%\n93.0%\n78.8%\n36.7%\n43.4%\n21.6%\n25.4%\n25.2%\nCogVideo [37]\n18.1%\n78.2%\n79.6%\n18.2%\n28.2%\n22.0%\n7.8%\n7.70%\nV-DiPoM-XL/2 no-mask\n1.9%\n21.6%\n76.3%\n7.6%\n2.8%\n21.4%\n13.4%\n15.1%\nV-DiPoM-XL/2 b-causal\n3.3%\n31.0%\n69.5%\n10.4%\n3.0%\n21.2%\n17.2%\n17.3%\nTable 3. Quantitative results on VBench [39]. We compare the same architecture of a V-DiPoM-XL/2 trained with the flow-matching\nloss LFM using either no mask (denoted no-mask) or block-causal masking (denoted b-causal). We report results from the literature taken\nfrom [39] to provide some calibration, but noting that the comparison is not fair as these models are trained on much larger and richer\ndatasets than ours, leading to much richer vocabulary and better semantic understanding.\nobtain the best performance. This is consistent with the in-\ntuition that H(X) has to contain sufficient statistics about\nthe sequence and that using only the mean is not sufficient\nfor that purpose.\nWe also study scaling laws for PoM by training models at\ndifferent scales (S/2, B/2, L/2 and XL/2 following the DiT\nnaming scheme), has shown on Figure 5. PoM enjoys ex-\nponential decrease of the FID with respect to the sampling\ncomputing complexity as shown by a linear regression on\nthe logarithmic plot. This is similar to what was observed\nfor transformers in DiT [58].\n5.2. Text to video generation\nWe evaluate our model generating videos of 5 seconds at\n16 fps and 240p resolution on VBench [39] and show the\nresults in Table 3. Note that contrarily to ImageNet, video\ngeneration is not as well standardized and models differ dra-\nmatically in terms of size, complexity and training dataset.\nNotably, most text-to-video generation models are trained\non a mix of images and videos to get more diverse cap-\ntions. In our case, we want to study the impact of enforc-\ning temporal causality in the generation process and as such\nwe limit our train set to WebVid-2M [3] only. Due to this\nsmaller training set, we observed that our models are limited\nto a smaller vocabulary of objects, motion and styles.\nWe compare the standard architecture (denoted no-mask)\nwith the use of a block-causal mask as detailed in sec-\ntion 3.1 (denoted as b-causal).\nAs we can see, the im-\npact of using a block causal mask is negative on some tasks\nlike subject consistency, background consistency and color.\nThis can be explain by the model struggling to follow the\nprompt for the first frames in the block causal case, which\npenalizes consistency, whereas the no-mask case can lever-\nage information from later frames to improve consistency.\nInterestingly, using a block causal mask improves tempo-\nral tasks like dynamic degree, human action and temporal\nstyle, which shows the importance of modeling properly the\ntemporal aspect for these tasks.\n6. Discussion\nIn this paper, we presented PoM, the Polynomial Mixer, a\nbuilding block for neural networks that aims at replacing\nattention. PoM has a complexity linear with the sequence\nlength and we prove it is a universal senquence-to-sequence\napproximator. To demonstrate the effectiveness of PoM, we\ntrain image and video generation models with it in lieu of\nMulti-Head Attention. These diffusion models obtain com-\npetitive results while being able to generate higher resolu-\ntion images faster than with attention.\nPoM is very interesting for high-definition video of long\nduration. However, the extreme cost of training such model\nmakes this endeavor clearly out of the scope of a research\npaper. Another area where PoM could shine is LLMs and\nmore particularly multimodal LLMs.\nIndeed, LLMs are\ncausal, which means the generation of text could greatly\nbenefit from the O(1) complexity of PoM for causal se-\nquence. In addition, recent works [85] show that next to-\nken prediction and diffusion objectives can be merged in a\nsingle model. In that case the ability of PoM to seamlessly\nadapt from causal to block-causal masking scheme greatly\nreduces the complexity of such mixed training objective. As\nfor high definition video, the extreme cost of training such\nlarge models also renders this endeavor out of the scope of\na research paper.\n8\n\n7. Acknowledgment\nThis work was granted access to the HPC resources of\nIDRIS under the allocation 2024-AD011013085R2 made\nby GENCI. The authors would like to thank Vincent Lepetit,\nG¨ul Varol, Loic Landrieu and Dimitris Samaras for their in-\nsightful comments and suggestions.\nReferences\n[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,\nAleman, F.L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al.: Gpt-4 technical report. arXiv preprint\narXiv:2303.08774 (2023) 2\n[2] Bai, X., Melas-Kyriazi, L.: Fixed point diffusion models. In:\nCVPR (2024) 2\n[3] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in\ntime: A joint video and image encoder for end-to-end re-\ntrieval. In: ICCV (2021) 6, 8\n[4] Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nB., Karras, T., Liu, M.Y.: ediff-i: Text-to-image diffusion\nmodels with ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324 (2022) 2\n[5] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS.W., Fidler, S., Kreis, K.:\nAlign Your Latents: High-\nResolution Video Synthesis with Latent Diffusion Models .\nIn: CVPR (2023) 2\n[6] Brandon, W., Mishra, M., Nrusimha, A., Panda, R., Kelly,\nJ.R.: Reducing transformer key-value cache size with cross-\nlayer attention. arXiv preprint arXiv:2405.12981 (2024) 2\n[7] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo,\nY., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luh-\nman, E., Ng, C., Wang, R., Ramesh, A.:\nVideo gen-\neration models as world simulators (2024),\nhttps :\n//openai.com/research/video-generation-\nmodels-as-world-simulators 1\n[8] Chang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.:\nMaskgit: Masked generative image transformer. In: CVPR\n(2022) 6, 7\n[9] Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z.,\nLuo, P., Lu, H., Li, Z.: Pixart-\\sigma: Weak-to-strong train-\ning of diffusion transformer for 4k text-to-image generation.\nIn: ECCV (2024) 2\n[10] Chen, T., Li, L.: Fit: Far-reaching interleaved transformers.\narXiv (2023) 2\n[11] Child, R., Gray, S., Radford, A., Sutskever, I.: Generat-\ning long sequences with sparse transformers. arXiv preprint\narXiv:1904.10509 (2019) 1, 2\n[12] Crowson, K., Baumann, S.A., Birch, A., Abraham, T.M.,\nKaplan, D.Z., Shippole, E.: Scalable high-resolution pixel-\nspace image synthesis with hourglass diffusion transformers.\nIn: ICML (2024) 2\n[13] Dao, T.:\nFlashattention-2:\nFaster attention with bet-\nter\nparallelism\nand\nwork\npartitioning.\narXiv\npreprint\narXiv:2307.08691 (2023) 2\n[14] Dao, T., Fu, D., Ermon, S., Rudra, A., R´e, C.: Flashattention:\nFast and memory-efficient exact attention with io-awareness.\nIn: NeurIPS (2022) 2\n[15] Dao, T., Gu, A.: Transformers are ssms: Generalized models\nand efficient algorithms through structured state space dual-\nity. In: Int. Conf. Mach. Learn. (2024) 3\n[16] Dhariwal, P., Nichol, A.: Diffusion models beat gans on im-\nage synthesis. In: NeurIPS (2021) 6, 7\n[17] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al.:\nThe llama 3 herd of models. arXiv preprint\narXiv:2407.21783 (2024) 2\n[18] Dufour, N., Besnier, V., Kalogeiton, V., Picard, D.: Don’t\ndrop your samples! coherence-aware training benefits con-\nditional diffusion. In: CVPR (2024) 2\n[19] Esser, P., Kulal, S., Blattmann, A., Entezari, R., M¨uller, J.,\nSaini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.:\nScaling rectified flow transformers for high-resolution image\nsynthesis. In: Int. Conf. Mach. Learn. (2024) 1, 2\n[20] Gao, P., Zhuo, L., Lin, Z., Liu, C., Chen, J., Du, R., Xie,\nE., Luo, X., Qiu, L., Zhang, Y., et al.: Lumina-t2x: Trans-\nforming text into any modality, resolution, and duration\nvia flow-based large diffusion transformers. arXiv preprint\narXiv:2405.05945 (2024) 2\n[21] Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B.,\nJacobs, D., Huang, J.B., Liu, M.Y., Balaji, Y.: Preserve your\nown correlation: A noise prior for video diffusion models.\nIn: ICCV (2023) 2\n[22] Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,\nRambhatla, S.S., Shah, A., Yin, X., Parikh, D., Misra, I.:\nFactorizing text-to-video generation by explicit image con-\nditioning. In: ECCV (2024) 2\n[23] Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pi-\nlault, J., Ibrahim, A., Millidge, B.: Zamba: A compact 7b\nssm hybrid model. arXiv preprint arXiv:2405.16712 (2024)\n3\n[24] Gokaslan, A., Cooper, A.F., Collins, J., Seguin, L., Jacob-\nson, A., Patel, M., Frankle, J., Stephenson, C., Kuleshov, V.:\nCommoncanvas: Open diffusion models trained on creative-\ncommons images. In: CVPR (2024) 2\n[25] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling\nwith selective state spaces. In: ICLR (2024) 1\n[26] Gu, A., Goel, K., Re, C.: Efficiently modeling long se-\nquences with structured state spaces. In: ICLR (2021) 1,\n3\n[27] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A.,\nR´e, C.: Combining recurrent, convolutional, and continuous-\ntime models with linear state space layers. In: NeurIPS\n(2021) 1, 3\n[28] Gu, J., Zhai, S., Zhang, Y., Susskind, J.M., Jaitly, N.: Ma-\ntryoshka diffusion models. In: ICLR (2023) 2\n[29] Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Li, F.F., Essa,\nI., Jiang, L., Lezama, J.: Photorealistic video generation with\ndiffusion models. In: ECCV (2025) 2\n[30] H¨agele, A., Bakouch, E., Kosson, A., Allal, L.B., Werra,\nL.V., Jaggi, M.: Scaling Laws and Compute-Optimal Train-\ning Beyond Fixed Training Durations. In: NeurIPS (2024),\nhttp://arxiv.org/abs/2405.18392 6\n9\n\n[31] Hang, T., Gu, S.: Improved noise schedule for diffusion\ntraining. arXiv preprint arXiv:2407.03297 (2024) 2\n[32] Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.:\nDiffit: Diffusion vision transformers for image generation.\nIn: ECCV (2024) 2\n[33] He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent\nvideo diffusion models for high-fidelity long video genera-\ntion. arXiv preprint arXiv:2211.13221 (2022) 8\n[34] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., et al.:\nImagen video: High definition video generation with diffu-\nsion models. arXiv preprint arXiv:2210.02303 (2022) 2\n[35] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic\nmodels. In: NeurIPS (2020) 2\n[36] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,\nFleet, D.J.: Video diffusion models. In: NeurIPS (2022) 2\n[37] Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.:\nCogvideo: Large-scale pretraining for text-to-video gener-\nation via transformers. In: ICLR (2023) 2, 8\n[38] Hu, V.T., Baumann, S.A., Gui, M., Grebenkova, O., Ma, P.,\nFischer, J., Ommer, B.: Zigma: A dit-style zigzag mamba\ndiffusion model. In: ECCV (2024) 1, 3\n[39] Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang,\nY., Wu, T., Jin, Q., Chanpaisit, N., et al.: Vbench: Com-\nprehensive benchmark suite for video generative models. In:\nCVPR (2024) 8\n[40] Jabri, A., Fleet, D.J., Chen, T.: Scalable adaptive compu-\ntation for iterative generation. In: Int. Conf. Mach. Learn.\n(2023) 2, 7\n[41] Jacob, P., Picard, D., Histace, A., Klein, E.: Metric learning\nwith horde: High-order regularizer for deep embeddings. In:\nICCV (2019) 3\n[42] Jaegle, A., Borgeaud, S., Alayrac, J.B., Doersch, C., Ionescu,\nC., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer,\nE., et al.: Perceiver io: A general architecture for structured\ninputs & outputs. In: ICLR (2022) 2\n[43] Jin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N.,\nHuang, Q., Song, Y., Mu, Y., Lin, Z.: Pyramidal flow match-\ning for efficient video generative modeling. arXiv preprint\narXiv:2410.05954 (2024) 2\n[44] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei,\nD.: Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361 (2020) 1, 2\n[45] Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T.,\nLaine, S.: Analyzing and improving the training dynamics\nof diffusion models. In: CVPR (2024) 2\n[46] Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T.,\nLaine, S.: Analyzing and improving the training dynamics\nof diffusion models. In: CVPR (2024) 2\n[47] Kitaev, N., Kaiser, L., Levskaya, A.: Reformer: The efficient\ntransformer. In: ICLR (2020) 1, 2\n[48] Kwon, M., Oh, S.W., Zhou, Y., Liu, D., Lee, J.Y., Cai, H.,\nLiu, B., Liu, F., Uh, Y.: Harivo: Harnessing text-to-image\nmodels for video generation. In: ECCV (2024) 2\n[49] Lee, S.H., Li, Y., Ke, J., Yoo, I., Zhang, H., Yu, J., Wang,\nQ., Deng, F., Entis, G., He, J., et al.: Parrot: Pareto-optimal\nmulti-reward reinforcement learning framework for text-to-\nimage generation. In: ECCV (2025) 2\n[50] Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedi-\ngos, I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz,\nS., et al.: Jamba: A hybrid transformer-mamba language\nmodel. arXiv preprint arXiv:2403.19887 (2024) 3\n[51] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.:\nFlow matching for generative modeling. In: ICLR (2022) 2\n[52] Liu, X., Gong, C., et al.: Flow straight and fast: Learning\nto generate and transfer data with rectified flow. In: ICLR\n(2023) 2\n[53] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q.,\nLiu, Y.: Vmamba: Visual state space model (2024) 3\n[54] Liu, Y., Zhang, Y., Jaakkola, T., Chang, S.: Correcting diffu-\nsion generation through resampling. In: CVPR (2024) 2\n[55] Luohe, S., Hongyi, Z., Yao, Y., Zuchao, L., Hai, Z.: Keep the\ncost down: A review on methods to optimize llm’s kv-cache\nconsumption. arXiv preprint arXiv:2407.18003 (2024) 2\n[56] Ma, N., Goldstein, M., Albergo, M.S., Boffi, N.M., Vanden-\nEijnden, E., Xie, S.: Sit: Exploring flow and diffusion-based\ngenerative models with scalable interpolant transformers. In:\nECCV (2024) 2, 7\n[57] Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion\nprobabilistic models. In: Int. Conf. Mach. Learn. (2021) 2\n[58] Peebles, W., Xie, S.: Scalable diffusion models with trans-\nformers. In: ICCV (2023) 1, 2, 5, 7, 8\n[59] Pei, X., Huang, T., Xu, C.: Efficientvmamba: Atrous selec-\ntive scan for light weight visual mamba (2024) 3\n[60] Picard, D.:\nTorch.manual seed(3407) is all you\nneed: On the influence of random seeds in deep learning ar-\nchitectures for computer vision (2023) 7\n[61] Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A.,\nLee, A., Vyas, A., Shi, B., Ma, C.Y., Chuang, C.Y., et al.:\nMovie gen:\nA cast of media foundation models. arXiv\npreprint arXiv:2410.13720 (2024) 1\n[62] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits\nof transfer learning with a unified text-to-text transformer.\nJMLR (2020) 5\n[63] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Om-\nmer, B.: High-resolution image synthesis with latent diffu-\nsion models. In: CVPR (2022) 2, 5, 7\n[64] Shi, Y., De Bortoli, V., Campbell, A., Doucet, A.: Diffusion\nschr¨odinger bridge matching. In: NeurIPS (2024) 2\n[65] Si, C., Huang, Z., Jiang, Y., Liu, Z.: Freeu: Free lunch in\ndiffusion u-net. In: CVPR (2024) 2\n[66] Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\nS., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al.: Make-a-\nvideo: Text-to-video generation without text-video data. In:\nICLR (2023) 2\n[67] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A.,\nErmon, S., Poole, B.:\nScore-based generative modeling\nthrough stochastic differential equations. In: International\nConference on Learning Representations (2021) 2\n[68] Team, G., Anil, R., Borgeaud, S., Alayrac, J.B., Yu, J., Sori-\ncut, R., Schalkwyk, J., Dai, A.M., Hauth, A., Millican, K.,\net al.: Gemini: a family of highly capable multimodal mod-\nels. arXiv preprint arXiv:2312.11805 (2023) 2\n10\n\n[69] Teng, Y., Wu, Y., Shi, H., Ning, X., Dai, G., Wang, Y., Li, Z.,\nLiu, X.: Dim: Diffusion mamba for efficient high-resolution\nimage synthesis. arXiv preprint arXiv:2405.14224 (2024) 1,\n7\n[70] Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L.,\nZhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D.,\nUszkoreit, J., et al.: Mlp-mixer: An all-mlp architecture for\nvision. In: NeurIPS (2021) 3\n[71] Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-\nNouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve, G.,\nVerbeek, J., et al.: Resmlp: Feedforward networks for im-\nage classification with data-efficient training. IEEE TPAMI\n(2022) 3\n[72] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is\nall you need. In: NeurIPS (2017) 1, 2\n[73] Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo,\nH., Zhang, H., Saffar, M.T., Castro, S., Kunze, J., Erhan, D.:\nPhenaki: Variable length video generation from open domain\ntextual descriptions. In: ICLR (2022) 2\n[74] Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., Naik, N.:\nDiffusion model alignment using direct preference optimiza-\ntion. In: CVPR (2024) 2\n[75] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang,\nS.: Modelscope text-to-video technical report (2023) 8\n[76] Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Lin-\nformer: Self-attention with linear complexity. arXiv preprint\narXiv:2006.04768 (2020) 1, 2\n[77] Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang,\nY., Yang, C., He, Y., Yu, J., Yang, P., et al.: Lavie: High-\nquality video generation with cascaded latent diffusion mod-\nels. arXiv preprint arXiv:2309.15103 (2023) 8\n[78] Wei, F., Zeng, W., Li, Z., Yin, D., Duan, L., Li, W.: Pow-\nerful and flexible: Personalized text-to-image generation via\nreinforcement learning. In: ECCV (2024) 2\n[79] Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without at-\ntention. In: CVPR. pp. 8239–8249 (2024) 1, 3, 7\n[80] Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J.,\nYang, Y., Hong, W., Zhang, X., Feng, G., et al.: Cogvideox:\nText-to-video diffusion models with an expert transformer.\narXiv preprint arXiv:2408.06072 (2024) 2, 5\n[81] Yun, C., Bhojanapalli, S., Rawat, A.S., Reddi, S., Kumar,\nS.: Are transformers universal approximators of sequence-\nto-sequence functions? In: ICLR (2020) 4\n[82] Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling\nvision transformers. In: CVPR (2022) 1, 2\n[83] Zhao, H., Lu, T., Gu, J., Zhang, X., Zheng, Q., Wu, Z.,\nXu, H., Jiang, Y.G.: Magdiff: Multi-alignment diffusion for\nhigh-fidelity video generation and editing. In: ECCV (2024)\n2\n[84] Zhao, Y., Xu, Y., Xiao, Z., Jia, H., Hou, T.: Mobilediffu-\nsion: Instant text-to-image generation on mobile devices. In:\nECCV (2024) 2\n[85] Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M.,\nShamis, L., Kahn, J., Ma, X., Zettlemoyer, L., Levy, O.:\nTransfusion: Predict the next token and diffuse images with\none multi-modal model (2024) 8\n[86] Zhou, Z., Chen, D., Wang, C., Chen, C.: Fast ode-based\nsampling for diffusion models in around 5 steps. In: CVPR\n(2024) 2\n[87] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang,\nX.: Vision mamba: Efficient visual representation learn-\ning with bidirectional state space model. arXiv preprint\narXiv:2401.09417 (2024) 3\n[88] Zuo, J., Velikanov, M., Rhaiem, D.E., Chahed, I., Belkada,\nY., Kunsch, G., Hacid, H.: Falcon mamba: The first com-\npetitive attention-free 7b language model. arXiv preprint\narXiv:2410.05355 (2024) 3\n11\n\nA. PoM pytorch code\nIn this section, we provide code in Pytorch for the main\nparts of the Polynomial Mixer as well as our diffusion\nblocks.\nWe found that writing dedicated functions for specific\ndegrees led to faster runtime due to the ability of the Py-\nTorch’s compiler to optimize them. We show below imple-\nmentation for degrees 2, 3 and 4.\n1 @torch.compile\n2 def po2(x: torch.Tensor):\n3\nh1, h2 = gelu(x).chunk(2, dim=-1)\n4\nh2 = h2 * h1\n5\nreturn torch.cat([h1, h2], dim=-1)\n6\n7 @torch.compile\n8 def po3(x: torch.Tensor):\n9\nh1, h2, h3 = gelu(x).chunk(3, dim=-1)\n10\nh2 = h2 * h1\n11\nh3 = h3 * h2\n12\nreturn torch.cat([h1, h2, h3], dim=-1)\n13\n14 @torch.compile\n15 def po4(x: torch.Tensor):\n16\nh1, h2, h3, h4 = gelu(x).chunk(4, dim=-1)\n17\nh2 = h2 * h1\n18\nh3 = h3 * h2\n19\nh4 = h4 * h3\n20\nreturn torch.cat([h1, h2, h3, h4], dim=-1)\nListing 1. Pytorch code for order specific PoM functions.\nNext, we show the function that computes both the poly-\nnomial and the mixing depending on the degree and the\npresence of a mask.\n1 def high_order_aggregation_(x: torch.Tensor, k:\nint, mask=None):\n2\nif k == 2:\n3\nh = po2(x)\n4\nelif k == 3:\n5\nh = po3(x)\n6\nelif k == 4:\n7\nh = po4(x)\n8\nelse:\n9\nh = list(gelu(x).chunk(k, dim=-1))\n10\nfor i in range(1, k):\n11\nh[i] = h[i] * h[i-1]\n12\nh = torch.cat(h, dim=-1)\n13\nif mask is None:\n14\nh = h.mean(dim=1, keepdims=True)\n15\nelse:\n16\nif mask.dim()==2:\n17\nh = mask_mixer(h, mask.to(h.device))\n18\nelif mask.dim() ==3:\n19\nh = full_mask_mixer(h, mask.to(h.\ndevice))\n20\nelse:\n21\nraise Exception(’unsupported dim for\nmask (should be 2,3 or None)’)\n22\nreturn h\nListing 2. Pytorch code for the complete polynomial and mixing\npart.\nIn the case the mask is 3 dimensional (batch, queries,\ncontext), we have a dedicated function that performs the\npartial sums. Note that this implementation is not optimized\nand that more speedup could be gained with a compiled\nmask.\n1 def full_mask_mixer(h, mask):\n2\nmask = mask.type(h.dtype)\n3\nh = torch.einsum(’bnd, bmn -> bmd’, h, mask)\n# b batch, n context tokens, m query tokens,\nd dim\n4\nh = h / (1.e-7 + mask.sum(dim=2, keepdims=\nTrue))\n5\nreturn h\nListing 3. Pytorch code for the mixer part with full mask.\nThe selection operation is very simple and consists in an\nelement-wise product. The whole PoM operation is just the\ncomputation of H(X) followed by the selection.\n1 @torch.compile\n2 def high_order_selection_(x: torch.Tensor, h:\ntorch.Tensor):\n3\nreturn F.sigmoid(x) * h\n4\n5 def pom(xq: torch.Tensor, xc: torch.Tensor, k:\nint, mask=None):\n6\nh = high_order_aggregation_(xc, k, mask)\n7\no = high_order_selection_(xq, h)\n8\nreturn o\nListing 4. Pytorch code for the selection part and the whole PoM\nfunction.\nIn the PoM module, we add the projections W1...m, Ws\nand Wo for each part of the PoM operation.\n1 class PoM(nn.Module):\n2\ndef __init__(self, dim, order, order_expand,\nbias=True):\n3\nsuper().__init__()\n4\nself.dim = dim\n5\nself.order = order\n6\nself.order_expand = order_expand\n7\nself.ho_proj = nn.Linear(dim, order*\norder_expand*dim, bias=bias)\n8\nself.se_proj = nn.Linear(dim, order*\norder_expand*dim, bias=bias)\n9\nself.ag_proj = nn.Linear(order*\norder_expand*dim, dim, bias=bias)\n10\nself.hom = hom\n11\n12\ndef forward(self, xq, xc=None, mask=None):\n13\nif xc is None:\n14\nxc = xq # self attention\n15\n16\ns = self.se_proj(xq)\n17\nh = self.ho_proj(xc)\n18\nsh = self.hom(s, h, self.order, mask)\n19\n20\n# output projection\n21\nreturn self.ag_proj(sh)\nListing 5. Pytorch module for PoM.\n12\n\nFor image diffusion, the base building block is simply\na PoM module followed by an MLP, with residual connec-\ntions and AdaLN modulations.\n1 def modulation(x, scale, bias):\n2\nreturn x * (1+scale) + bias\n3\n4 class DiPBlock(nn.Module):\n5\ndef __init__(self, dim: int, order: int,\norder_expand: int, ffw_expand: int):\n6\nsuper().__init__()\n7\nself.dim = dim\n8\nself.order = order\n9\nself.order_expand = order_expand\n10\nself.ffw_expand = ffw_expand\n11\n12\nself.mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n13\nself.pom = PoM(dim, order=order,\norder_expand=order_expand, bias=True)\n14\nself.ffw_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n15\nself.ffw = nn.Sequential(nn.Linear(dim,\nffw_expand * dim, bias=True),\n16\nnn.GELU(),\n17\nnn.Linear(\nffw_expand * dim, dim, bias=True))\n18\nself.cond_mlp = nn.Sequential(\n19\nnn.SiLU(),\n20\nnn.Linear(dim, 4\n* dim, bias=True))\n21\nself.gate_mlp = nn.Sequential(\n22\nnn.SiLU(),\n23\nnn.Linear(dim, 2\n* dim, bias=True))\n24\n25\n26\ndef forward(self, x, c):\n27\ns1, b1, s2, b2 = self.cond_mlp(c).chunk\n(4, -1)\n28\ng1, g2 = self.gate_mlp(c).chunk(2, -1)\n29\n30\n# mha\n31\nx_ln = modulation(self.mha_ln(x), s1, b1)\n32\nx = x + self.pom(x_ln) * (1 + g1)\n33\n34\n#ffw\n35\nx_ln = modulation(self.ffw_ln(x), s2, b2)\n36\nx = x + self.ffw(x_ln)*(1+g2)\n37\n38\nreturn x\nListing 6. Pytorch module for the image diffusion block.\nFor text-to-video, we add a second PoM module that\ngathers information from the text.\n1 class TextVideoDiPBlock(nn.Module):\n2\ndef __init__(self, dim: int, order: int,\norder_expand: int, ffw_expand: int):\n3\nsuper().__init__()\n4\nself.dim = dim\n5\nself.order = order\n6\nself.order_expand = order_expand\n7\nself.ffw_expand = ffw_expand\n8\n9\nself.mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n10\nself.x_mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n11\nself.c_mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n12\nself.pom = PoM(dim, order=order,\norder_expand=order_expand, bias=True)\n13\nself.c_pom = PoM(dim, order=order,\norder_expand=order_expand, bias=True)\n14\nself.ffw_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n15\nself.ffw = nn.Sequential(nn.Linear(dim,\nffw_expand * dim, bias=True),\n16\nnn.GELU(),\n17\nnn.Linear(\nffw_expand * dim, dim, bias=True))\n18\nself.cond_mlp = nn.Sequential(\n19\nnn.SiLU(),\n20\nnn.Linear(dim, 8\n* dim, bias=True))\n21\nself.gate_mlp = nn.Sequential(\n22\nnn.SiLU(),\n23\nnn.Linear(dim, 3\n* dim, bias=True))\n24\n25\n26\ndef forward(self, x, t, c, mask,\ntemporal_mask=None):\n27\nsx, bx, sc, bc, s1, b1, s2, b2 = self.\ncond_mlp(t).chunk(8, -1)\n28\ngc, g1, g2 = self.gate_mlp(t).chunk(3,\n-1)\n29\n30\n# ca\n31\nx_ln = modulation(self.x_mha_ln(x), sx,\nbx)\n32\nc_ln = modulation(self.c_mha_ln(c), sc,\nbc)\n33\nx = x + self.c_pom(x_ln, c_ln, mask) * (1\n+ gc)\n34\n35\n# sa\n36\nx_ln = modulation(self.mha_ln(x), s1, b1)\n37\nx = x + self.pom(x_ln, mask=temporal_mask\n) * (1 + g1)\n38\n39\n#ffw\n40\nx_ln = modulation(self.ffw_ln(x), s2, b2)\n41\nx = x + self.ffw(x_ln)*(1+g2)\n42\n43\nreturn x\nListing 7. Pytorch module for the video diffusion block.\nB. Condition adherence\nIn this section we study the trade-off between image qual-\nity as measured with FID and condition adherence as mea-\nsured with Inception Score (IS) by varying the weight ω of\nthe classifier-free guidance (CFG). We show the results for\na model of size L2 trained with the diffusion loss LD on\nGigure 6. Inference is performed with 250 steps of DDIM\nsampling. As we can see, the model is perfectly able to bal-\nance FID and IS, leading to a typical ’U’ curve where CFG\nimproves both FID and IS at first, but then improvements of\n13\n\n100\n150\n200\n250\n300\n10\n15\n20\nInception Score\nFID\nFigure 6. Image quality versus condition adherence trade-off.\nFID/IS curve for the L2 model with 250 DDIM sampling steps.\nValues are computed on 10k images against the validation set of\nImageNet.\nIS comes at the cost of FID. This is typical of mode collapse\nwith the model generating low diversity but high quality\nimages, similarly to what is observed with attention-based\nmodels.\nC. Proof of Lemma 3\nWe first need to show that set with different entries are\nmapped to different vectors. We first separate PoM into its\ntwo components:\ns(X) = σ(WsX)\n(18)\nHk(X) =\n\"\nh(W1X); . . . ;\nY\nm\nh(WmX)\n#\n11⊤\n(19)\nPoM(X) = Wo(s(X) ◦Hk(X))\n(20)\nAssuming ker(Wo) =, and noting that Hk(X) is the\nsame for every column, we just have to show that s(X)\nhas different columns. This is easily achieved by having\nker(Ws) = since σ is injective and the composition of in-\njective functions is itself injective.\nSecond, we have to show that sets that differ by at least\none element are mapped to all different entries. To simplify\nnotations, we will consider the special case where all matri-\nces are the identity or an identity block positioned such as\nto perform submatrix selection. All the matrices can thus\nbe removed from the formula. A similar argument can be\nmade for matrices that are full rank as they preserve injec-\ntivity. We will also consider linear activations everywhere,\nwhich can be made as close as one wish by partitioning the\nimage of the activation function and performing piecewise\nlinear approximation.\nWith this simplified version of PoM, we have to show\nthat for 2 sets X, X′ differing by at least one element (i.e.,\n∃x′ ∈X′, ∀x ∈X, x ̸= x′), then there exist k such that\n∀x ∈X, x′ ∈X′, x\nX\nxi∈X\nxk\ni ̸= x\nX\nxi∈X\nxk\ni .\n(21)\nConsider the functions P(t) and P ′(t) defined as fol-\nlows:\nP(t) =\nX\nxiinX\nxt\ni\n(22)\nP ′(t) =\nX\nxiinX′\nxt\ni\n(23)\nSince X and X′ differ by at least one element, there ex-\nists at least one xi ∈X such that xi ̸= x′\ni, ∀x′\ni ∈X′. This\nimplies that the functions P(t) and P ′(t) are not identical\nsince are sums of exponentials with different bases.\nSince P(t) and P ′(t) are different functions, there must\nexist some k for which P(k) ̸= P ′(k). In other words, there\nexists a k such that:\nX\nxi∈X\nxk\ni ̸=\nX\nx′\ni∈X′\nx′k\ni\n(24)\nFor this k, let us denote Sk = P\nxi∈X xk\ni . We need to\nshow that xSk ̸= x′S′\nk for all x ∈X and x′ ∈X′. Assume\nfor the sake of contradiction that there exist x ∈X and\nx′ ∈X′ such that xSk = x′S′\nk. This implies:\nx\nX\nxi∈X\nxk\ni = x′ X\nx′\ni∈X′\nx′k\ni\n(25)\nRearranging, we get:\nx\nx′ =\nP\nx′\ni∈X′ x′k\ni\nP\nxi∈X xk\ni\n(26)\nSince Sk ̸= S′\nk, the right-hand side is not equal to 1.\nHowever, for this equality to hold for all x ∈X and x′ ∈\nX′, the ratio x/x′ would need to be constant for all pairs\n(x, x′), which is not possible given that X and X′ differ by\nat least one element.\nTherefore, there exists a k such that xSk ̸= x′S′\nk for all\nx ∈X and x′ ∈X′.\nD. Uncurated examples\nIn the following pages, we show randomly selected samples\nwith obtained after 250 steps of DDIM sampling with the\nXL/2 model trained with the diffusion loss LD.\n14\n\nFigure 7. Uncurated 256² images for the class magpie (18).\n15\n\nFigure 8. Uncurated 256² images for the class loggerhead, loggerhead turtle, Caretta caretta (33).\n16\n\nFigure 9. Uncurated 256² images for the class macaw (88).\n17\n\nFigure 10. Uncurated 256² images for the class otter (360).\n18\n\nFigure 11. Uncurated 256² images for the class balloon (417).\n19\n\nFigure 12. Uncurated 256² images for the class ice cream, icecream (928).\n20\n\nFigure 13. Uncurated 256² images for the class seashore, coast, seacoast, sea-coast (978).\n21\n\nFigure 14. Uncurated 256² images for the class volcano (980).\n22",
    "pdf_filename": "PoM_Efficient_Image_and_Video_Generation_with_the_Polynomial_Mixer.pdf"
}