{
    "title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer",
    "abstract": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images 100 and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and com- putegrowquadratically. Toalleviatethisproblem,wepro- 10−1 PoMforward+backward poseadrop-inreplacementforMHAcalledthePolynomial PoMforward Mixer(PoM)thathasthebenefitofencodingtheentirese- MHAforward+backward quence into an explicit state. PoM has a linear complex- MHAforward itywithrespecttothenumberoftokens. Thisexplicitstate 10−2 256 1,024 2,048 4,096 also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still Imageresolution being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, Figure 1. Comparison between the speed of PoM and Multi- just like regular MHA. We adapt several Diffusion Trans- HeadAttention(MHA)inthesameDiT-XL/2architecturefor formers (DiT) for generating images and videos with PoM differentimageresolutions. WeuseanH100GPUandcompute theaveragetimeon100synthetictrainingbatchestoperformthe replacingMHA,andweobtainhighqualitysampleswhile forward or forward+backward passes. We use synthetic data to using less computational resources. The code is available removetheinfluencefromdataloading.TrainingwithPoMisless athttps://github.com/davidpicard/HoMM. costlythaninferencewithMHAathigherresolutions. 1.Introduction atinganimageattwicethespatialresolution(respectivelya In a sudden change of pace, high quality image and video videoattwicetheresolutionanddoubletheduration)results generationhaveevolvedfromataskseeminglyimpossible in 4 times more patches and thus 16 times more computa- toachievetoataskalmostsolvedbyavailablecommercial tional cost (respectively 8 times more patches and thus 64 oropen-sourcetoolslikeStableDiffusion3[19],Sora[7]or timesmorecomputationalcost). Attemptsathavingtrans- MovieGen[61]. AttheheartofthissuccessliestheMulti- formers with sub-quadratic complexity [11, 47, 76] intro- headAttention(MHA)inthetransformerarchitecture[72] duce the additional constraint of fixing the number of to- that has excellent scaling properties [58, 82]. These so- kens, which prevents generating images or videos of dif- calledscalinglaws[44]enablebrute-forcingcomplexprob- ferentsizes. Alternatively, recurrentmodelssuchasState- lemssuchasimageandvideogenerationbyusingverylarge SpaceModels(SSM)[26,27]havebeeninvestigatedforthe models trained on gigantic data, at the expense of an ever task[38,69,79]sincetheircomplexityislinearwiththese- increasing computational cost. The main focus of current quence length [25]. However, they introduce an arbitrary research lies thus in scaling transformer-based approaches causal raster scan of the sequence that does not fit the 2D tolargermodelshandlinglargerdatasets. geometryofimagesverywell. The issue with transformers is that the computational In this paper, we enable better scaling in large gener- costincreasesquadraticallywiththesequencelengthdueto ative models by introducing a new building block called thepairwisecomputationinMHA.Thismeansthatgener- thePolynomialMixer(PoM).PoMhasalinearcomplexity 1 4202 voN 91 ]VC.sc[ 1v36621.1142:viXra egami/dnocesniemiT",
    "body": "PoM: Efficient Image and Video Generation with the Polynomial Mixer\nDavidPicard1,NicolasDufour1,2\n1LIGM,E´coleNationaledesPontsetChausse´es,IPParis,UnivGustaveEiffel,CNRS,France\n2LIX,E´colePolytechnique,IPParis,CNRS,France\n{david.picard,nicolas.dufour}@enpc.fr\nAbstract\nDiffusion models based on Multi-Head Attention (MHA)\nhave become ubiquitous to generate high quality images\n100\nand videos. However, encoding an image or a video as\na sequence of patches results in costly attention patterns,\nas the requirements both in terms of memory and com-\nputegrowquadratically. Toalleviatethisproblem,wepro- 10−1\nPoMforward+backward\nposeadrop-inreplacementforMHAcalledthePolynomial\nPoMforward\nMixer(PoM)thathasthebenefitofencodingtheentirese-\nMHAforward+backward\nquence into an explicit state. PoM has a linear complex-\nMHAforward\nitywithrespecttothenumberoftokens. Thisexplicitstate 10−2\n256 1,024 2,048 4,096\nalso allows us to generate frames in a sequential fashion,\nminimizing memory and compute requirement, while still Imageresolution\nbeing able to train in parallel. We show the Polynomial\nMixer is a universal sequence-to-sequence approximator, Figure 1. Comparison between the speed of PoM and Multi-\njust like regular MHA. We adapt several Diffusion Trans- HeadAttention(MHA)inthesameDiT-XL/2architecturefor\nformers (DiT) for generating images and videos with PoM differentimageresolutions. WeuseanH100GPUandcompute\ntheaveragetimeon100synthetictrainingbatchestoperformthe\nreplacingMHA,andweobtainhighqualitysampleswhile\nforward or forward+backward passes. We use synthetic data to\nusing less computational resources. The code is available\nremovetheinfluencefromdataloading.TrainingwithPoMisless\nathttps://github.com/davidpicard/HoMM.\ncostlythaninferencewithMHAathigherresolutions.\n1.Introduction\natinganimageattwicethespatialresolution(respectivelya\nIn a sudden change of pace, high quality image and video videoattwicetheresolutionanddoubletheduration)results\ngenerationhaveevolvedfromataskseeminglyimpossible in 4 times more patches and thus 16 times more computa-\ntoachievetoataskalmostsolvedbyavailablecommercial tional cost (respectively 8 times more patches and thus 64\noropen-sourcetoolslikeStableDiffusion3[19],Sora[7]or timesmorecomputationalcost). Attemptsathavingtrans-\nMovieGen[61]. AttheheartofthissuccessliestheMulti- formers with sub-quadratic complexity [11, 47, 76] intro-\nheadAttention(MHA)inthetransformerarchitecture[72] duce the additional constraint of fixing the number of to-\nthat has excellent scaling properties [58, 82]. These so- kens, which prevents generating images or videos of dif-\ncalledscalinglaws[44]enablebrute-forcingcomplexprob- ferentsizes. Alternatively, recurrentmodelssuchasState-\nlemssuchasimageandvideogenerationbyusingverylarge SpaceModels(SSM)[26,27]havebeeninvestigatedforthe\nmodels trained on gigantic data, at the expense of an ever task[38,69,79]sincetheircomplexityislinearwiththese-\nincreasing computational cost. The main focus of current quence length [25]. However, they introduce an arbitrary\nresearch lies thus in scaling transformer-based approaches causal raster scan of the sequence that does not fit the 2D\ntolargermodelshandlinglargerdatasets. geometryofimagesverywell.\nThe issue with transformers is that the computational In this paper, we enable better scaling in large gener-\ncostincreasesquadraticallywiththesequencelengthdueto ative models by introducing a new building block called\nthepairwisecomputationinMHA.Thismeansthatgener- thePolynomialMixer(PoM).PoMhasalinearcomplexity\n1\n4202\nvoN\n91\n]VC.sc[\n1v36621.1142:viXra\negami/dnocesniemiT\nlike SSMs while still enabling all pairwise information to the original DDPM papers [35, 57] sample images in\nbeprocessedlikeinMHA,obtainingeffectivelythebestof pixel space, making it unsuitable for large resolution, the\nbothworlds. Fromatheoreticalstandpoint,weprovePoM mostgroundbreakingimprovementwasintroducedbySta-\ncan be used as a drop-in replacement for attention. Doing ble Diffusion [63] with the addition of a variational auto-\nso in the popular DiT architecture [56, 58] results in im- encoder (VAE) that allows the diffusion process to be per-\nproved scaling such that at higher resolutions, it becomes formed in a lower dimensional latent space. Stable Diffu-\nlesscostlytotrainamodelwithPoMthantoperforminfer- sion uses a U-Net architecture complemented by attention\nencewithamodelusingMHA,asshownonFigure5. layers [63, 65]. To benefit more from the scaling prop-\nTosumup,thecontributionsofthispaperarethefollow- erties of transformers [44, 82], simpler approaches based\ning: solelyontransformerlayershasbeenproposedinDiT[58]\n✓ We introduce the Polynomial Mixer (PoM), a replace- and the subsequent flow-matching version SiT [56]. Most\nment for MHAthat has a linear complexity with respect moderntext-to-imagegenerationmodelsarenowbasedon\ntothesequencelengthandwithoutsacrificinggeneration Transformer layers rather than the U-Net [9, 19, 20, 32].\nquality; [12,28],trainefficientpixelspacetransformersmodelsby\n✓ We prove that models equipped with PoM are universal leveraging multiscale training and SwinAttention. Simi-\nsequence-to-sequenceapproximators; larly, RIN [10, 40] also proposes an approach using atten-\n✓ We train DiT-inspired image generative models and ob- tiononly,albeitinaPerceiver-IO[42]inspiredarchitecture\ntain results of similar quality while being much more that uses cross-attention to perform most of the computa-\ncomputeefficientathigherresolutions; tioninasmallerlatentspace,andhasbeensuccessfullyex-\n✓ WetrainvideogenerativemodelsleveragingPoMwitha tended to text-to-image [18]. In addition to architectures\nconstant processing cost per frame while not sacrificing andsampling[2,84,86],theimportanceoftrainingisalso\nonvisualquality. highlighted in recent works, from resampling the training\nOurcontributionisthereforeprimarilyfundamental: We data[24,54]toRL[49,74,78]andmodelaveraging[46].\nshow that it is possible to train generative models with an In video generation [29, 36, 66, 73, 83], early attempts\nalternative mecanism to MHA. We believe this direction havefocusedonextendingexistingtext-to-imagemodelsto\nwillnotonlygroundfutureresearchonhighresolutionim- benefitfromtheirlargescalepretraining[5,21,22,34,37,\nagesandverylongvideosgeneration,butalsocouldbenefit 48]. However,thedrawbackofsuchapproachesisthatthey\nmanyareasofresearch(e.g.,largelanguagemodels,vision- re-usetheVAEofexistingtext-to-imagemodelswhichdoes\nlanguagemodels,etc). not encode temporal information, which is thus not com-\npressed. As such, novel architectures using a 2D+t VAE\n2.RelatedWork such as CogVideoX [80], PyramidFlow [43] can benefit\nfrom a smaller latent space leading to less computational\nDiffusion Diffusion models [35, 57, 67] learn a neural\ncosts.\noperator that produces natural images from noise using a\nforward-reversesetofprocesses. Theforwardprocesscon-\nsists in pushing the distribution of natural images forward Fast alternative to attention Since the introduction of\nto a known distribution, typically Gaussian, which can be Transformers [72], many effort have been made to reduce\ndonebyaddingincreasinglevelofnoisetotheimage. The the quadratic complexity of MHA [11, 47, 76]. Notably,\nreverse process does not have an explicit solution, but can methods like Reformer [47] use fast approximate neigh-\nbeapproximatedbyaneuralnetworkbyregressingthelocal bors to reduce the size of the attention matrix based on\ninverseoftheforwardprocess,i.e.,solving the assumption that most tokens will have zero attention.\nTo go further, Linformer [76] proposes to compute an ex-\nminE (cid:2) ∥ε −f (x ,t)∥2(cid:3) , (1)\nt∼U(0,1) t θ t plicit low rank projection of the keys and the values to re-\nθ\nducethecomplexityofMHAforeachqueryfromthesize\ns.t. x =α x +γ ε , ε ∼N(0,1). (2)\nt t 0 t t t\nof the sequence n to an arbitrary chosen number k ≪ n.\nHere, α and γ are chosen such that x corresponds to The main drawback of such approach is that n and k are\nt t 0\na natural image whereas x corresponds to pure Gaussian fixed, which means that the model can no longer process\n1\nnoise. Agreatamountofresearchhasbeenputintofinding sequencesofvaryinglength. WiththeadventofLargeLan-\nbetter noise schedules (α and γ ) [4, 31, 45], or improv- guage Models and their ability to process extremely long\nt t\ning the quantity that is regressed [51, 52, 64], keeping the sequences[1,17,68],recenteffortshavebeenputonmore\ngeneralideaoflearningtoinvertstepbystepthestochastic efficient implementations such as Flash-Attention [13, 14]\ndifferentialequationthattransformsanimageintonoise. orKV-cache[6,55]whichseemsufficientfortext.However\nForimageandgeneration,mosteffortshavebeenpoured forvisualcontent,thesequencelengthgrowsquadratically\ninto designing efficient architectures at the task. While withtheresolution,which,becauseMHAisalsoquadratic\n2\ninthenumberoftokens,leadstoquarticcomputationaland (cid:104) h(W1X);...;(cid:81)k mh(WmX)(cid:105) 1 H(X)\nmemorycomplexity.\n(cid:80)\nAlternatively, someattemptshavebeenmadetojustre- S(X)◦H(X)1⊤\nmovetheMulti-HeadAttention,suchasinMlp-Mixer[70] X WoZ\nand Resmlp [71] that replace MHA with simple projec- S(X)=σ(WsX)\ntiononthetransposetensor(i.e., consideringthesequence\ndimension as the features). These approaches have been\nshown to obtain competitive results, but similarly to Lin-\nformer, they imply a fix sequence length since this length\nFigure 2. Diagram for the Polynomial Mixer. The input se-\nis now an intrinsic dimension of the projection in the\nquenceissplitintotwopaths. Thetoppathexpandseachtoken\ntranspose direction. More recently, State-Space Models\nusingapolynomialbeforetheyaremixed(averaged)²intoasingle\n(SSM)[26,27]havebecomethefocusofrecentworkespe-\nrepresentation. The bottom path expands the tokens into gating\nciallyinlanguagemodeling[15,23,50,88].SSMarerecur- coefficients. Both pathsare recombined andprojected backinto\nrentmodels,whichishighlybeneficialforlanguagemodel- theinputdimension.\ningbecauseofthecausalpropertyoftext. Inthatcase,the\ncomplexitytogeneratethenexttokenbecomesconstant. In\nContrarilytoMHAthatcomputesallpairwiseexchanges\nvisual content however, there is no such natural causality\nofinformationbetweentokensinthesequence,thePolyno-\npatterninthespatialdimensions. Attempttousesuchmod-\nmialMixerfollowsastate-representation(H(X))approach\nelsforvisiontaskshavebeensuccessful[53,59,87],albeit\nwhereallinformationissharedinacommonmemoryloca-\natthecostofenforcinganarbitrary1-dimensionalscanor-\ntion that all tokens can access. This state-representation is\nder of the tokens that does not encode well the 2D nature\ndefinedbymixingalltokensofthesequenceaftertheyare\nofanimage. Inimagegenerationusingdiffusion[38,79],\nmapped to a high dimensional space by a learned polyno-\nsince the model has to be iterated, this results in a doubly\nmial, hence the name Polynomial Mixer, and a similar ap-\nsequential processing (space and iterations) that does not\nproach has been successfully used for learning image rep-\nbenefit from the parallel nature of processing images. For\nresentation [41]. The main benefit is that the complexity\nvideohowever,thecausalaspectisnaturaloverthetimedi-\nof the approach is no longer quadratic but linear with the\nmension,andrecurrentapproachesmaybemoreefficient.\nsequencelengthn.\nTakinginspirationfromtransformerswithMHA,wede-\n3.PolynomialMixerandPolymorpher\nfineaPolymorpherblockP asalternatingresidualPolyno-\nWedefineaPolymorpher blockasasequence-to-sequence mialMixerswithfeed-forwardnetworksasfollows:\nfunctionmappingRd×ntoRd×n,composedoftworesidual\nP(X)=X+PoM(X)+FF(X+PoM(X)), (5)\nblocks,aPolynomialMixerandafeed-forwardblock.\nForasequenceX ∈Rd×n,thePolynomialMixer(PoM)\nwithFF(X)beingatwo-layerfeed-forwardnetwork.\nshownonFigure2isdefinedasfollows:\nA Polymorpher is a drop-in replacement for any\nTransformer-basedarchitectureasitperformsthesamerole\nPoM(X)=W\n(cid:2)\nσ(W\nX)◦H(X)1⊤(cid:3)\n, with (3)\no s of sequence-to-sequence mapping. The main difference is\n(cid:34) k (cid:35) in its parametrization: A Transformer is configured by the\n(cid:89)\nH(X)= h(W 1X);...; h(W mX) 1, (4) numberofheadsandtheirdimensioninMHA,whereasthe\nm=1 Polymorpher is configured by its degree k and the dimen-\nsionDofeachpolynomial.\nwhere k is the degree of the Polynomial Mixer, σ is the\n(cid:81)\nsigmoid function, h an activation function, ◦ and the 3.1.Polymorpherforcausalsequences\nelement-wise (Hadamard) product, and 1 a vector of the\nAcausalsequencecaneasilybemodeledinPoMbyadding\nappropriate dimension filled with ones. The notation [·;·]\nis for vertical concatenation. The matrices W ∈ Rd×kD, a mask M that prevents summing future tokens into the\no\nW ∈ RkD×d andW ,...,W ∈ RD×d arethelearnable blackboard. Thiscorrespondstothefollowingdefinition\ns 1 k\nparametersofthePolynomialMixer.\nPoM(X,M)=W [σ(W X)◦H(X)], (6)\no s\nTheideaofthePolynomialMixeristothatthesequence\nX ∈ Rd×n is uniquely summarized into the representa- (cid:34) (cid:89)k (cid:35)\nH(X)= h(W X);...; h(W X) M⊤. (7)\ntion H(X) ∈ RkD×1. Each element in X then gets to 1 m\nm=1\nqueryH(X)independentlythankstothemapS(W X) ∈\ns\nRkD×n.Thequeriedinformationisthenprojectedbackinto Now H(X) ∈ RkD×n and M ∈ {0,1}n×n is a binary\ntheoriginalspacewithW . matrix that defines which pairs of tokens are related. Just\no\n3\nlike for MHA, a binary matrix defines an attention pattern Proof. ForapermutationP,wehave\nthatcanbearbitrarilychosen.\nPoM(XP)=W\n(cid:2)\nσ(W\nXP)◦H(XP)1⊤(cid:3)\n. (12)\nInthespecialcaseofcausalsequences,M isalowertri- o s\nangularmatrix. Moreover,onecanexpressthemixingpart\nNotice that H(XP) = H(X) because the sum is permu-\nofthePolynomialMixerasaniterativeprocessasfollows:\ntation invariant, and σ(W XP) = σ(W X)P because σ\ns s\n(cid:34) k (cid:35) is an element-wise operation. Noticing that H(X)1⊤ has\nH(X) =(cid:88) h(W X);...; (cid:89) h(W X) , (8) all identical columns allows us to move P outside of the\n:,i 1 m\nbracketstoconcludetheproof.\nj≤i m=1 :,j\n(cid:34) (cid:89)k (cid:35) Moreimportantly,wecanalsoproveauniversalapprox-\n=H(X) + h(W X);...; h(W X) .\n:,i−1 1 m imation theorem for Polymorphers similar to what is well\nm=1 :,i known for Transformers [81]. As the polynomial mixer is\n(9)\nequivariant,itrequirestheuseofpositionalencoding,which\nalsounderlinesthesimilaritybetweenPoMandMHA.\nIn this formula, H(X) is an explicit hidden state that is\n:,i Weusethefollowingstandarddefinitionofdistancebe-\nupdatedbyaddingthepolynomialmappingofthenextto-\ntween functions that map sequences to sequences. Given\nken. SuchaconfigurationenablesO(1)inferencecomplex-\ntwo functions f and g : Rdn → Rdn and an integer\nity in the auto-regressive setup, a property that is shared\nA≤p≤∞,wedefinethedistanced as:\nwithrecurrentnetworks,butnottransformers. LikeSSMs, p\nPolymorphers have the best of both worlds, they can train (cid:18)(cid:90) (cid:19)1/p\non the whole sequence in parallel and do the inference in d (f,g)= ∥f(X)−g(X)∥pdX . (13)\np p\ntherecursiveway.\nIn addition, Polymorphers can handle block causal se- Thefollowingtheoremholds:\nquences. LetM beablockcausalmatrixforsomeinteger\nTheorem 2 (Universal approximation). Let 1 ≤ p ≤ ∞\nblocksizeK:\nandϵ > 0,thenforanygivenf ∈ F thesetofcontinuous\nM =1ifj ≤⌈i/K⌉K else0. (10) functions that map a compact domain in Rd×n to Rd×n,\ni,j\nthereexistsaPolymorphergwithlearnedpositionalencod-\nWecannowrewriteH as ingsuchthatd (f,g)≤ϵ.\np\nH(X) =H(X) The proof follows exactly the same scheme as in [81],\n:,i :,⌊i/K⌋K\nwheremostoftheheavyliftingisdonebythefeed-forward\n⌈i/K⌉K (cid:34) k (cid:35)\n(cid:88) (cid:89) networks. Their main argument is to show that MHA can\n+ h(W X);...; h(W X) .\n1 m mapeverytokeninthesequencetoauniquevaluethatde-\nj=⌊i/K⌋K m=1 :,j pends on the entire sequence, and then the feed-forward\n(11)\nblockscanmapthoseuniquevaluestothedesiredoutput.In\nourcase,wejusthavetoensurethatthePolynomialMixer\nInthisconfiguration,wecansequentiallyprocessgroupsof\nhas the same properties as MHA, which is obtained using\ntokensatatimeduringinference,whichreducesthemem-\nthefollowinglemma:\noryrequirement. Thisisinparticularpracticalforvideose-\nquenceswhereitmakessensetohaveacausalmaskinthe Lemma 3 (Contextual mapping (informal)). There exists\ntemporal dimension that makes each frame depend on the k > 0 for which any Polynomial Mixer q of degree k is a\nprevious ones, while keeping the ability of all the tokens contextualmappingsonRd×n,thatis:\n(patches)of aframetolook ateachothers, since causality • Forany X ∈ Rd×n withdifferent entries, q(X) hasdif-\ndoesnothavemuchsenseinthespatialdimension. ferententries.\n• ForanyX,X′ ∈Rd×nthatdifferatleastbyoneelement,\n3.2.Theoreticalanalysis\nthenallentriesofq(L)andq(L′)aredifferent.\nWe first show that PoM is equivariant, which means that\nThe proof is deferred to the appendix and primarily\npermutationsintheinputsequenceresultinpermutedout-\nuses the fact that a sufficiently high degree polynomial is\nputs. Thisisakeypropertythatmadetransformerspopular\nuniquely defined by a sequence of point-wise evaluation.\nanddoesnotholdforotherarchitectureslikeconvolutions:\nAs noted in [81], having the contextual mapping property\nProposition 1 (Permutation equivariance). A Polynomial isnotsocommonasitrequirestosummarizeuniquelythe\nMixer is permutation equivariant, i.e., let X ∈ Rd×n be contextwhilepreservingtheidentityofthecurrenttoken.\na set of vectors and P a column permutation matrix, then Withtheseresults,weshowthataPolymorpherisaspo-\nPoM(XP)=PoM(X)P. tentasaTransformerforsequencemodeling.\n4\nshutdownanentireblockf thanksto\n+ +\n· ·\nx←x+(1+s)f(x), (15)\nFeedforward PoM(self)\nwiththe1in1+sbeingaddedsothatthereisafullresidual\nconnectionwhentheMLPpredictsf(x) = 0. Fornaming\nModulation LayerNorm/Modulation\nthearchitectures,wefollowthesameparametrizationasin\nLayerNorm\n+ DiT.Namely,anS/2modelhasakernelsizeandstrideof2\n+ · foraggregatingtheVAEfeaturesintotokens,and12blocks\n·\nof dimension 384. Similarly, an XL/2 model that has 28\nPoM(cross)\nPoM blocks of dimension 1152. For the PoM operation inside\nLayerNorm/Modulation LayerNorm/Modulation eachblock,weuseanpolynomialoforder2withanexpan-\nsionfactorof2unlessspecifiedotherwise.Pytorchcodefor\nModulation\nMLP theblocksisgiveninappendix.\nLayerNorm MLP\nVideo generation For video generation from text, we\nX [t,c] X c] t]\n(a)Imageblock (b)Videoblock extend the DiT architecture to handle text as a condi-\ntion. We first encode video clips using the 3D VAE from\nFigure3. BuildingblocksforourdiffusionmodelsusingPoM. CogXVideo[80]andthengroupthefeaturesintovisualto-\nFor class-conditional image generation (a), we follow strictly kensusingakernelsizeof2×2×2(with2×2forthespatial\nDiT[58]intheAdaLNvariant,replacingmulti-headattentionwith\naxes,and2forthetemporalaxisresultinginadownscaling\nPoM.Fortexttovideogeneration(b),wefollowahybridapproach\nfactorof16×16×8).Weadda3Dcosinepositionalencod-\ninwhichtheencodedtexttokensareincorporatedintothevideo\ning to the visual tokens before feeding them to the model.\ntokensusingPoMinsteadofcrossattention,whilethetimeisused\nThetextisencodedusingT5[62]embeddingsandthetime\nas a modulation. Modulation means component-wise scale and\nshiftmodificationbasedonthecoefficientspredictedbytheMLP stepisencodedusingacosineembedding.\n(similarlytotheAdaLNapproach). ThemodelconsistsinblocksusingPoMtoaggregatein-\nformationbetweenthetextconditionandthevisualtokens\nasshownonFigure3b. Moreprecisely,afirstPoMopera-\n4.DiffusionwithPoM\ntionisusedinacrossfashion,similartocross-attention,to\naggregate information from the text tokens into the visual\nArmed with the definition of PoM and Polymorphers, we\ntokens. Then,asecondPoMoperationisusedtoaggregate\nnowdesigndiffusionmodelstakinginspirationfrommod-\ninformationamongthevisualtokensthemselves,similarto\nelsbasedonMHA,andshowthatPoMcanreplaceattention\nwhatself-attentionwoulddo. Finally,afeedforwardmod-\nin practice. We follow the design choices of DiT [58] and\nuleprocessesthevisualtokensonly. Thetimestepembed-\nproposeaclass-conditionalimagegenerationpolymorpher\ndingisusedinanMLPtopredictthecoefficientsofmodu-\naswellasatext-to-videogenerationpolymorpher.\nlationsandgatesateachoftheoperations.\n4.1.Architecturedesign WetrainasinglemodelofsizeXL/2thatconsistsin20\nlayersofdimension1152resultingin1.1Bparameters.\nImage generation For image generation, the class-\nconditionalpolymorpherissimilartotheAdaLNvariantof 4.2.Trainingsetup\nDiT.TheimageisencodedthroughtheVAEofSD1.5[63]\nFor class-conditional image generation, we train on Ima-\nandthenfeaturesareaggregatedintovisualtokensX. We\ngeNet. Werescaleeachimageto256pixelsontheirsmall-\nadda2Dcosinepositionalencodingtothembeforewefeed\nestsizeandthentakeacropofsize256×256. Weuseboth\nthemtothemodel. Theclasscandthetimesteptareem-\nthe original images and horizontally flipped version for a\nbeddedusinganembeddingmatrixandacosineembedding\ntotalof2.4Mimages. Wetrainamodelf eitherusingthe\nrespectivelybeforebeingsummedtogether. θ\ndiffusionloss:\nThemodelconsistsinseveralblocksthatcombinemod-\nulations,PoMandfeedforwardnetworksasshownonFig- L =E ∥ε −f (x ,c,t)∥2, (16)\nD t∼U[0,1] t θ t\nure3a. Ineachblock,themodulationconsistsinpredicting\nfromtheconditionc+tascaleγ andashiftβ thatmodify ortheflowmatchingloss:\ntheinputby\nL =E ∥v −f (x ,c,t)∥2, (17)\nFM t∼U[0,1] t θ t\nx←γ(x−β). (14)\nwithv = ε −x . Foreachexperimentalresult,wemen-\nt t 0\nSimilarly to DiT, the MLP also predicts gates σ that can tionwhichlossisused,butthemodelsaretrainedsimilarly\n5\nFigure4. Qualitativeresultsonclass-conditionalgeneration. WeshowimagessampledwiththemodelDiPoM-XL/2trainedwiththe\nflow-matchinglossL atseveralresolutionsfordifferentclasses.Weuseclassifier-freeguidancewithω=4s/s withsthescaleofthe\nFM 0\nimageands thereferencescale(256).\n0\nwithoutrequiringchangeintraininghyper-parameters. We 5.1.Class-conditionalimagegeneration\nuseAdamWwithaconstantlearningrateof10−4followed\nbyashortcooldownwithsquarerootdecay[30]. Quantitativeresults WecomparetheresultsofourXL/2\nmodel trained with the diffusion loss to the state of the art\nFor video, we used WebVid-2M [3] that we rescale to on Table 1. We compute the Fre´chet Inception Distance\n240×384at16fps. Wekeeponlythefirst5seconds,cor- (FID),theInceptionScore(IS),precision(P)andrecall(R)\nresponding to 80 frames. This results in a total of 2.5M using the code from ADM [16] on 50k generated images.\nclips. WetrainusingtheflowmatchinglossL FM. Wealso The table is split between methods on masked encoding\nuseAdamWwithaconstantlearningrateof10−4followed\n(Mask-GIT[8]),diffusionmodelsbasedonSSManddiffu-\nbyashortcooldownwithsquarerootdecay. sionmodelsbasedonattention. Resultsareextractedfrom\nthe corresponding papers. Our images are generated with\n250stepsoftheDDIMsamplerforthemodeltrainedwith\nthe diffusion loss L , and 125 steps of Heun sampler for\nD\n5.Experiments the model trained with the flow-matching loss L , with\nFM\nclassifierfreeguidance(CFG,ω =0.7inbothcases).\nWefirstshowresultonclass-conditionalimagegeneration Using the evaluation code and reference set from\nandthenoftext-to-videogeneration. ADM [16], we obtain an FID of 2.46, which is slightly\n6\nModel Sampleconfig #train FID↓ IS↑ Precision↑ Recall↑\nMask-GIT[8] 6.18 182.1 0.80 0.51\nDIFFUSSM-XL†[79] 250stepsDDPM 660M 2.28 259.1 0.86 0.56\nDiM-H†[69] 25stepsDPM++ 480M 2.21 - - -\nADM-G[16] 250stepsDDIM 500M 4.59 186.7 0.83 0.53\nLDM-4-G[63] 250stepsDDIM 215M 3.60 247.7 0.87 0.48\nRIN[40] 1000stepsDDPM 600M 3.42 182.0 - -\nDiT-XL/2†[58] 250stepsDDPM 1.8B 2.27 278.2 0.83 0.57\nSiT-XL/2†[56] 125stepsHeun 1.8B 2.15 254.9 0.81 0.60\nDiPoM-XL/2L (ours) 250stepsDDIM 950M 2.46 240.6 0.78 0.60\nD\nDiPoM-XL/2L (ours) 125stepsHeun 950M 3.70 255.2 0.79 0.56\nFM\nTable1. QuantitativeresultsonImageNet256×256class-conditionalgeneration. #traindenotesthenumberoftrainingimagesseen\nduringtrain(i.e.,batchsize×numberoftrainingsteps).†denotesmethodsevaluatedagainsttheImagenettrainingsetinsteadoftheusual\nADMevaluationarchive. Wecolorinblue(respectivelyinred)DiTandPoMarchitecturesofequivalentsizethataretrainedwiththe\nsamediffusionlossL (respectivelyflow-matchinglossL ),andweboldthebestvaluesbetweenthetwo,eventhoughtheresultsare\nD FM\nnotevaluatedagainstthesamereferenceset.\nDegree Expand FID↓ IS↑ Precision Recall\n50\n1 12 90.1 15.1 0.27 0.36\n2 6 87.0 15.8 0.29 0.37 200\n3 4 86.1 16.0 0.29 0.38 40\n4 3 88.8 15.5 0.28 0.36\n150\n6 2 90.7 15.0 0.28 0.36\n30\n100\nTable2.ComparisonofdifferentdegreesofPolynomialMixer 20\nataconstantmemorybudgetwithaS/2modelon10kimagesfrom\nImagenet. Havingadegree≥ 2isnecessarytogetgoodperfor-\n50\nmances,butthereisatrade-offbetweenthedegreeandtheexpan- 10\nsionfactor.\n101 101.5 102\nFlops(G)\nabove that of the comparable DiT architecture, but notice\nthatourmodelwastrainedforonlyhalfofthenumbersteps Figure5.ScalinglawsforaDiT-likearchitecturewithattention\nofDiT.Inaddition, wefoundFIDtobeveryunreliableas replacedbyPoM.FIDsandInceptionScores(IS)arecomputed\nametric, asitishighlyvaryingwiththereferenceset. For on10ksampleswithclassifierfreeguidance(ω = 1),andshown\nexample,usingthevalidationsetofImageNet,weobtained withalinearregressioninlogspace. Performancesscalewiththe\n3.45FID1.WeobtainaslightlylowerIScomparedtoDiT, computationbudget,similarlytotransformers.\nbutthiscouldbeimprovedbyusingahigherCFG.Indeed,\nweshowinappendixthatthetrade-offbetweenFIDansIS\ntain a collection of models able to sample images up to\ncan reach as high as 300 IS at the cost of a much higher\n1024×1024resolution(whichisthemaximumresolution\nFID. We obtain a precision/recall trade-off comparable to\nwefoundreasonabletoupscaletoonImageNet). Weshow\nDiT,slightlyloweronprecisionbutalsohigheronrecall.\nselectedsamplesatthesehigherresolutiononFigure4. At\nOverall,theresultsobtainedusingPoMareonparwith\nhigherresolution,someclassesarecollapsedduetothelack\nthe literature, showing that PoM can be used as a drop-in\nofavailabledata.\nreplacement for multi-head attention in a neural architec-\nture,withoutrequiringeitherarchitecturalchangesortrain-\ninghyper-parametertuning.\nAblationstudy Westudytheimpactofthedegreeofthe\npolynomial on Table 2. To enable a fair comparison, we\nconsiderasetofS/2modelsthathavethesamedimension\nQualitative results We further fine-tune the model on\nforH(X)andcomputedifferenttrade-offsbetweenthede-\nhigher resolution data for a small number of steps to ob-\ngree of the polynomials and their dimension. As we can\n1Itwasshownin [60]thatrandomnessaffectssignificantlytheresults. see, having at least second order polynomials is crucial to\n7\nDIF\nSI\nModels Subject Background Temporal Motion Dynamic Aesthetic Imaging Object\nConsistency Consistency Flickering Smoothness Degree Quality Quality Class\nLaVie[77] 91.4% 97.5% 98.3% 96.4% 49.7% 54.9% 61.9% 91.8%\nModeScope[75] 89.9% 95.3% 98.3% 95.8% 66.4% 52.1% 58.6% 82.3%\nVideoCrafter[33] 86.2% 92.9% 97.6% 91.8% 89.7% 44.4% 57.2% 87.3%\nCogVideo[37] 92.2% 96.2% 97.6% 96.5% 42.2% 38.2% 41.0% 73.4%\nV-DiPoM-XL/2no-mask 90.6% 96.6% 99.7% 97.3% 31.7% 28.6% 47.1% 29.3%\nV-DiPoM-XL/2b-causal 80.4% 92.2% 98.1% 97.4% 37.5% 30.5% 47.9% 30.0%\nModels Multiple Human Color Spatial Scene Appearance Temporal Overall\nObjects Action Relationship Style Style Consistency\nLaVie[77] 33.3% 96.8% 86.4% 34.1% 52.7% 23.6% 25.9% 26.4%\nModeScope[75] 39/0% 92.4% 81.7% 33.7% 39.3% 23.4% 25.4% 25.8%\nVideoCrafter[33] 25.9% 93.0% 78.8% 36.7% 43.4% 21.6% 25.4% 25.2%\nCogVideo[37] 18.1% 78.2% 79.6% 18.2% 28.2% 22.0% 7.8% 7.70%\nV-DiPoM-XL/2no-mask 1.9% 21.6% 76.3% 7.6% 2.8% 21.4% 13.4% 15.1%\nV-DiPoM-XL/2b-causal 3.3% 31.0% 69.5% 10.4% 3.0% 21.2% 17.2% 17.3%\nTable3. QuantitativeresultsonVBench[39]. WecomparethesamearchitectureofaV-DiPoM-XL/2trainedwiththeflow-matching\nlossL usingeithernomask(denotedno-mask)orblock-causalmasking(denotedb-causal). Wereportresultsfromtheliteraturetaken\nFM\nfrom[39]toprovidesomecalibration, butnotingthatthecomparisonisnotfairasthesemodelsaretrainedonmuchlargerandricher\ndatasetsthanours,leadingtomuchrichervocabularyandbettersemanticunderstanding.\nobtainthebestperformance. Thisisconsistentwiththein- penalizesconsistency,whereastheno-maskcasecanlever-\ntuition that H(X) has to contain sufficient statistics about age information from later frames to improve consistency.\nthesequenceandthatusingonlythemeanisnotsufficient Interestingly, using a block causal mask improves tempo-\nforthatpurpose. ral tasks like dynamic degree, human action and temporal\nWealsostudyscalinglawsforPoMbytrainingmodelsat style,whichshowstheimportanceofmodelingproperlythe\ndifferentscales(S/2, B/2, L/2andXL/2followingtheDiT temporalaspectforthesetasks.\nnaming scheme), has shown on Figure 5. PoM enjoys ex-\nponentialdecreaseoftheFIDwithrespecttothesampling 6.Discussion\ncomputing complexity as shown by a linear regression on\nIn this paper, we presented PoM, the Polynomial Mixer, a\nthe logarithmic plot. This is similar to what was observed\nbuilding block for neural networks that aims at replacing\nfortransformersinDiT[58].\nattention. PoM has a complexity linear with the sequence\nlengthandweproveitisauniversalsenquence-to-sequence\n5.2.Texttovideogeneration\napproximator.TodemonstratetheeffectivenessofPoM,we\nWe evaluate our model generating videos of 5 seconds at train image and video generation models with it in lieu of\n16 fps and 240p resolution on VBench [39] and show the Multi-HeadAttention. Thesediffusionmodelsobtaincom-\nresultsinTable3. NotethatcontrarilytoImageNet, video petitive results while being able to generate higher resolu-\ngenerationisnotaswellstandardizedandmodelsdifferdra- tionimagesfasterthanwithattention.\nmaticallyintermsofsize,complexityandtrainingdataset. PoMisveryinterestingforhigh-definitionvideooflong\nNotably, most text-to-video generation models are trained duration. However,theextremecostoftrainingsuchmodel\non a mix of images and videos to get more diverse cap- makes this endeavor clearly out of the scope of a research\ntions. In our case, we want to study the impact of enforc- paper. Another area where PoM could shine is LLMs and\ningtemporalcausalityinthegenerationprocessandassuch more particularly multimodal LLMs. Indeed, LLMs are\nwe limit our train set to WebVid-2M [3] only. Due to this causal, which means the generation of text could greatly\nsmallertrainingset,weobservedthatourmodelsarelimited benefit from the O(1) complexity of PoM for causal se-\ntoasmallervocabularyofobjects,motionandstyles. quence. In addition, recent works [85] show that next to-\nWecomparethestandardarchitecture(denotedno-mask) kenpredictionanddiffusionobjectivescanbemergedina\nwith the use of a block-causal mask as detailed in sec- singlemodel. InthatcasetheabilityofPoMtoseamlessly\ntion 3.1 (denoted as b-causal). As we can see, the im- adaptfromcausaltoblock-causal maskingschemegreatly\npactofusingablockcausalmaskisnegativeonsometasks reducesthecomplexityofsuchmixedtrainingobjective.As\nlikesubjectconsistency,backgroundconsistencyandcolor. forhighdefinitionvideo, theextremecostoftrainingsuch\nThis can be explain by the model struggling to follow the largemodelsalsorendersthisendeavoroutofthescopeof\npromptforthefirstframesintheblockcausalcase, which aresearchpaper.\n8\n7.Acknowledgment [14] Dao,T.,Fu,D.,Ermon,S.,Rudra,A.,Re´,C.:Flashattention:\nFastandmemory-efficientexactattentionwithio-awareness.\nThis work was granted access to the HPC resources of In:NeurIPS(2022) 2\nIDRIS under the allocation 2024-AD011013085R2 made [15] Dao,T.,Gu,A.:Transformersaressms:Generalizedmodels\nbyGENCI.TheauthorswouldliketothankVincentLepetit, andefficientalgorithmsthroughstructuredstatespacedual-\nGu¨lVarol,LoicLandrieuandDimitrisSamarasfortheirin- ity.In:Int.Conf.Mach.Learn.(2024) 3\nsightfulcommentsandsuggestions. [16] Dhariwal,P.,Nichol,A.:Diffusionmodelsbeatgansonim-\nagesynthesis.In:NeurIPS(2021) 6,7\n[17] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nReferences\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\n[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., A., et al.: The llama 3 herd of models. arXiv preprint\nAleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., arXiv:2407.21783(2024) 2\nAnadkat, S., et al.: Gpt-4 technical report. arXiv preprint [18] Dufour, N., Besnier, V., Kalogeiton, V., Picard, D.: Don’t\narXiv:2303.08774(2023) 2 dropyoursamples! coherence-awaretrainingbenefitscon-\nditionaldiffusion.In:CVPR(2024) 2\n[2] Bai,X.,Melas-Kyriazi,L.:Fixedpointdiffusionmodels.In:\n[19] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Mu¨ller, J.,\nCVPR(2024) 2\nSaini,H.,Levi,Y.,Lorenz,D.,Sauer,A.,Boesel,F.,etal.:\n[3] Bain,M.,Nagrani,A.,Varol,G.,Zisserman,A.: Frozenin\nScalingrectifiedflowtransformersforhigh-resolutionimage\ntime: A joint video and image encoder for end-to-end re-\nsynthesis.In:Int.Conf.Mach.Learn.(2024) 1,2\ntrieval.In:ICCV(2021) 6,8\n[20] Gao, P., Zhuo, L., Lin, Z., Liu, C., Chen, J., Du, R., Xie,\n[4] Balaji,Y.,Nah,S.,Huang,X.,Vahdat,A.,Song,J.,Zhang,\nE., Luo, X., Qiu, L., Zhang, Y., etal.: Lumina-t2x: Trans-\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nforming text into any modality, resolution, and duration\nB., Karras, T., Liu, M.Y.: ediff-i: Text-to-image diffusion\nvia flow-based large diffusion transformers. arXiv preprint\nmodels with ensemble of expert denoisers. arXiv preprint\narXiv:2405.05945(2024) 2\narXiv:2211.01324(2022) 2\n[21] Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B.,\n[5] Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim, Jacobs,D.,Huang,J.B.,Liu,M.Y.,Balaji,Y.:Preserveyour\nS.W., Fidler, S., Kreis, K.: Align Your Latents: High- own correlation: A noise prior for video diffusion models.\nResolutionVideoSynthesiswithLatentDiffusionModels. In:ICCV(2023) 2\nIn:CVPR(2023) 2 [22] Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S.,\n[6] Brandon, W., Mishra, M., Nrusimha, A., Panda, R., Kelly, Rambhatla, S.S., Shah, A., Yin, X., Parikh, D., Misra, I.:\nJ.R.:Reducingtransformerkey-valuecachesizewithcross- Factorizing text-to-video generationby explicit image con-\nlayerattention.arXivpreprintarXiv:2405.12981(2024) 2 ditioning.In:ECCV(2024) 2\n[7] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, [23] Glorioso,P.,Anthony,Q.,Tokpanov,Y.,Whittington,J.,Pi-\nY., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luh- lault, J., Ibrahim, A., Millidge, B.: Zamba: A compact 7b\nman, E., Ng, C., Wang, R., Ramesh, A.: Video gen- ssmhybridmodel.arXivpreprintarXiv:2405.16712(2024)\neration models as world simulators (2024), https: 3\n//openai.com/research/video-generation- [24] Gokaslan, A., Cooper, A.F., Collins, J., Seguin, L., Jacob-\nmodels-as-world-simulators 1 son,A.,Patel,M.,Frankle,J.,Stephenson,C.,Kuleshov,V.:\n[8] Chang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.: Commoncanvas:Opendiffusionmodelstrainedoncreative-\nMaskgit: Maskedgenerativeimagetransformer.In: CVPR commonsimages.In:CVPR(2024) 2\n(2022) 6,7 [25] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling\nwithselectivestatespaces.In:ICLR(2024) 1\n[9] Chen,J.,Ge,C.,Xie,E.,Wu,Y.,Yao,L.,Ren,X.,Wang,Z.,\n[26] Gu, A., Goel, K., Re, C.: Efficiently modeling long se-\nLuo,P.,Lu,H.,Li,Z.:Pixart-\\sigma:Weak-to-strongtrain-\nquences with structured state spaces. In: ICLR (2021) 1,\ningofdiffusiontransformerfor4ktext-to-imagegeneration.\n3\nIn:ECCV(2024) 2\n[27] Gu,A.,Johnson,I.,Goel,K.,Saab,K.,Dao,T.,Rudra,A.,\n[10] Chen,T.,Li,L.: Fit: Far-reachinginterleavedtransformers.\nRe´,C.:Combiningrecurrent,convolutional,andcontinuous-\narXiv(2023) 2\ntime models with linear state space layers. In: NeurIPS\n[11] Child, R., Gray, S., Radford, A., Sutskever, I.: Generat-\n(2021) 1,3\ninglongsequenceswithsparsetransformers.arXivpreprint\n[28] Gu, J., Zhai, S., Zhang, Y., Susskind, J.M., Jaitly, N.: Ma-\narXiv:1904.10509(2019) 1,2\ntryoshkadiffusionmodels.In:ICLR(2023) 2\n[12] Crowson, K., Baumann, S.A., Birch, A., Abraham, T.M., [29] Gupta,A.,Yu,L.,Sohn,K.,Gu,X.,Hahn,M.,Li,F.F.,Essa,\nKaplan, D.Z., Shippole, E.: Scalablehigh-resolutionpixel- I.,Jiang,L.,Lezama,J.:Photorealisticvideogenerationwith\nspaceimagesynthesiswithhourglassdiffusiontransformers. diffusionmodels.In:ECCV(2025) 2\nIn:ICML(2024) 2 [30] Ha¨gele, A., Bakouch, E., Kosson, A., Allal, L.B., Werra,\n[13] Dao, T.: Flashattention-2: Faster attention with bet- L.V.,Jaggi,M.: ScalingLawsandCompute-OptimalTrain-\nter parallelism and work partitioning. arXiv preprint ingBeyondFixedTrainingDurations.In: NeurIPS(2024),\narXiv:2307.08691(2023) 2 http://arxiv.org/abs/2405.18392 6\n9\n[31] Hang, T., Gu, S.: Improved noise schedule for diffusion multi-rewardreinforcementlearningframeworkfortext-to-\ntraining.arXivpreprintarXiv:2407.03297(2024) 2 imagegeneration.In:ECCV(2025) 2\n[32] Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.: [50] Lieber,O.,Lenz,B.,Bata,H.,Cohen,G.,Osin,J.,Dalmedi-\nDiffit: Diffusion vision transformers for image generation. gos,I.,Safahi,E.,Meirom,S.,Belinkov,Y.,Shalev-Shwartz,\nIn:ECCV(2024) 2 S., et al.: Jamba: A hybrid transformer-mamba language\n[33] He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent model.arXivpreprintarXiv:2403.19887(2024) 3\nvideodiffusionmodelsforhigh-fidelitylongvideogenera- [51] Lipman,Y.,Chen,R.T.,Ben-Hamu,H.,Nickel,M.,Le,M.:\ntion.arXivpreprintarXiv:2211.13221(2022) 8 Flowmatchingforgenerativemodeling.In:ICLR(2022) 2\n[34] Ho,J.,Chan,W.,Saharia,C.,Whang,J.,Gao,R.,Gritsenko, [52] Liu, X., Gong, C., et al.: Flow straight and fast: Learning\nA.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,D.J.,etal.: to generate and transfer data with rectified flow. In: ICLR\nImagenvideo: Highdefinitionvideogenerationwithdiffu- (2023) 2\nsionmodels.arXivpreprintarXiv:2210.02303(2022) 2 [53] Liu,Y.,Tian,Y.,Zhao,Y.,Yu,H.,Xie,L.,Wang,Y.,Ye,Q.,\n[35] Ho,J.,Jain,A.,Abbeel,P.:Denoisingdiffusionprobabilistic Liu,Y.:Vmamba:Visualstatespacemodel(2024) 3\nmodels.In:NeurIPS(2020) 2 [54] Liu,Y.,Zhang,Y.,Jaakkola,T.,Chang,S.:Correctingdiffu-\nsiongenerationthroughresampling.In:CVPR(2024) 2\n[36] Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,\nFleet,D.J.:Videodiffusionmodels.In:NeurIPS(2022) 2 [55] Luohe,S.,Hongyi,Z.,Yao,Y.,Zuchao,L.,Hai,Z.:Keepthe\ncostdown:Areviewonmethodstooptimizellm’skv-cache\n[37] Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.:\nconsumption.arXivpreprintarXiv:2407.18003(2024) 2\nCogvideo: Large-scale pretraining for text-to-video gener-\n[56] Ma,N.,Goldstein,M.,Albergo,M.S.,Boffi,N.M.,Vanden-\nationviatransformers.In:ICLR(2023) 2,8\nEijnden,E.,Xie,S.:Sit:Exploringflowanddiffusion-based\n[38] Hu,V.T.,Baumann,S.A.,Gui,M.,Grebenkova,O.,Ma,P.,\ngenerativemodelswithscalableinterpolanttransformers.In:\nFischer, J., Ommer, B.: Zigma: A dit-style zigzag mamba\nECCV(2024) 2,7\ndiffusionmodel.In:ECCV(2024) 1,3\n[57] Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion\n[39] Huang,Z.,He,Y.,Yu,J.,Zhang,F.,Si,C.,Jiang,Y.,Zhang,\nprobabilisticmodels.In:Int.Conf.Mach.Learn.(2021) 2\nY., Wu, T., Jin, Q., Chanpaisit, N., et al.: Vbench: Com-\n[58] Peebles, W., Xie, S.: Scalablediffusionmodelswithtrans-\nprehensivebenchmarksuiteforvideogenerativemodels.In:\nformers.In:ICCV(2023) 1,2,5,7,8\nCVPR(2024) 8\n[59] Pei,X.,Huang,T.,Xu,C.: Efficientvmamba: Atrousselec-\n[40] Jabri, A., Fleet, D.J., Chen, T.: Scalable adaptive compu-\ntivescanforlightweightvisualmamba(2024) 3\ntation for iterative generation. In: Int. Conf. Mach. Learn.\n[60] Picard, D.: Torch.manual seed(3407) is all you\n(2023) 2,7\nneed: Ontheinfluenceofrandomseedsindeeplearningar-\n[41] Jacob,P.,Picard,D.,Histace,A.,Klein,E.: Metriclearning\nchitecturesforcomputervision(2023) 7\nwithhorde:High-orderregularizerfordeepembeddings.In:\n[61] Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A.,\nICCV(2019) 3\nLee, A., Vyas, A., Shi, B., Ma, C.Y., Chuang, C.Y., et al.:\n[42] Jaegle,A.,Borgeaud,S.,Alayrac,J.B.,Doersch,C.,Ionescu,\nMovie gen: A cast of media foundation models. arXiv\nC.,Ding,D.,Koppula,S.,Zoran,D.,Brock,A.,Shelhamer,\npreprintarXiv:2410.13720(2024) 1\nE.,etal.: Perceiverio: Ageneralarchitectureforstructured\n[62] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\ninputs&outputs.In:ICLR(2022) 2\nMatena,M.,Zhou,Y.,Li,W.,Liu,P.J.: Exploringthelimits\n[43] Jin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N.,\nof transfer learning with a unified text-to-text transformer.\nHuang,Q.,Song,Y.,Mu,Y.,Lin,Z.:Pyramidalflowmatch-\nJMLR(2020) 5\ning for efficient video generative modeling. arXiv preprint\n[63] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Om-\narXiv:2410.05954(2024) 2\nmer, B.: High-resolutionimagesynthesiswithlatentdiffu-\n[44] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B.,\nsionmodels.In:CVPR(2022) 2,5,7\nChess,B.,Child,R.,Gray,S.,Radford,A.,Wu,J.,Amodei,\n[64] Shi,Y.,DeBortoli,V.,Campbell,A.,Doucet,A.: Diffusion\nD.: Scalinglawsforneurallanguagemodels.arXivpreprint\nschro¨dingerbridgematching.In:NeurIPS(2024) 2\narXiv:2001.08361(2020) 1,2\n[65] Si, C., Huang, Z., Jiang, Y., Liu, Z.: Freeu: Free lunch in\n[45] Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., diffusionu-net.In:CVPR(2024) 2\nLaine, S.: Analyzing and improving the training dynamics [66] Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\nofdiffusionmodels.In:CVPR(2024) 2 S.,Hu,Q.,Yang,H.,Ashual,O.,Gafni,O.,etal.: Make-a-\n[46] Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., video: Text-to-videogenerationwithouttext-videodata.In:\nLaine, S.: Analyzing and improving the training dynamics ICLR(2023) 2\nofdiffusionmodels.In:CVPR(2024) 2 [67] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A.,\n[47] Kitaev,N.,Kaiser,L.,Levskaya,A.:Reformer:Theefficient Ermon, S., Poole, B.: Score-based generative modeling\ntransformer.In:ICLR(2020) 1,2 through stochastic differential equations. In: International\n[48] Kwon, M., Oh, S.W., Zhou, Y., Liu, D., Lee, J.Y., Cai, H., ConferenceonLearningRepresentations(2021) 2\nLiu, B., Liu, F., Uh, Y.: Harivo: Harnessing text-to-image [68] Team,G.,Anil,R.,Borgeaud,S.,Alayrac,J.B.,Yu,J.,Sori-\nmodelsforvideogeneration.In:ECCV(2024) 2 cut, R., Schalkwyk, J., Dai, A.M., Hauth, A., Millican, K.,\n[49] Lee, S.H., Li, Y., Ke, J., Yoo, I., Zhang, H., Yu, J., Wang, etal.: Gemini: afamilyofhighlycapablemultimodalmod-\nQ.,Deng,F.,Entis,G.,He,J.,etal.: Parrot: Pareto-optimal els.arXivpreprintarXiv:2312.11805(2023) 2\n10\n[69] Teng,Y.,Wu,Y.,Shi,H.,Ning,X.,Dai,G.,Wang,Y.,Li,Z., [86] Zhou, Z., Chen, D., Wang, C., Chen, C.: Fast ode-based\nLiu,X.:Dim:Diffusionmambaforefficienthigh-resolution samplingfordiffusionmodelsinaround5steps.In: CVPR\nimagesynthesis.arXivpreprintarXiv:2405.14224(2024) 1, (2024) 2\n7 [87] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang,\n[70] Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., X.: Vision mamba: Efficient visual representation learn-\nZhai,X.,Unterthiner,T.,Yung,J.,Steiner,A.,Keysers,D., ing with bidirectional state space model. arXiv preprint\nUszkoreit,J.,etal.: Mlp-mixer: Anall-mlparchitecturefor arXiv:2401.09417(2024) 3\nvision.In:NeurIPS(2021) 3 [88] Zuo,J.,Velikanov,M.,Rhaiem,D.E.,Chahed,I.,Belkada,\n[71] Touvron, H., Bojanowski, P., Caron, M., Cord, M., El- Y., Kunsch, G., Hacid, H.: Falcon mamba: The first com-\nNouby,A.,Grave,E.,Izacard,G.,Joulin,A.,Synnaeve,G., petitive attention-free 7b language model. arXiv preprint\nVerbeek, J., et al.: Resmlp: Feedforward networks for im- arXiv:2410.05355(2024) 3\nageclassificationwithdata-efficienttraining.IEEETPAMI\n(2022) 3\n[72] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attentionis\nallyouneed.In:NeurIPS(2017) 1,2\n[73] Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo,\nH.,Zhang,H.,Saffar,M.T.,Castro,S.,Kunze,J.,Erhan,D.:\nPhenaki:Variablelengthvideogenerationfromopendomain\ntextualdescriptions.In:ICLR(2022) 2\n[74] Wallace,B.,Dang,M.,Rafailov,R.,Zhou,L.,Lou,A.,Pu-\nrushwalkam, S., Ermon, S., Xiong, C., Joty, S., Naik, N.:\nDiffusionmodelalignmentusingdirectpreferenceoptimiza-\ntion.In:CVPR(2024) 2\n[75] Wang,J.,Yuan,H.,Chen,D.,Zhang,Y.,Wang,X.,Zhang,\nS.:Modelscopetext-to-videotechnicalreport(2023) 8\n[76] Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Lin-\nformer:Self-attentionwithlinearcomplexity.arXivpreprint\narXiv:2006.04768(2020) 1,2\n[77] Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang,\nY., Yang, C., He, Y., Yu, J., Yang, P., et al.: Lavie: High-\nqualityvideogenerationwithcascadedlatentdiffusionmod-\nels.arXivpreprintarXiv:2309.15103(2023) 8\n[78] Wei, F., Zeng, W., Li, Z., Yin, D., Duan, L., Li, W.: Pow-\nerfulandflexible:Personalizedtext-to-imagegenerationvia\nreinforcementlearning.In:ECCV(2024) 2\n[79] Yan,J.N.,Gu,J.,Rush,A.M.: Diffusionmodelswithoutat-\ntention.In:CVPR.pp.8239–8249(2024) 1,3,7\n[80] Yang,Z.,Teng,J.,Zheng,W.,Ding,M.,Huang,S.,Xu,J.,\nYang,Y.,Hong,W.,Zhang,X.,Feng,G.,etal.: Cogvideox:\nText-to-video diffusion models with an expert transformer.\narXivpreprintarXiv:2408.06072(2024) 2,5\n[81] Yun, C., Bhojanapalli, S., Rawat, A.S., Reddi, S., Kumar,\nS.: Are transformers universal approximators of sequence-\nto-sequencefunctions?In:ICLR(2020) 4\n[82] Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling\nvisiontransformers.In:CVPR(2022) 1,2\n[83] Zhao, H., Lu, T., Gu, J., Zhang, X., Zheng, Q., Wu, Z.,\nXu,H.,Jiang,Y.G.: Magdiff: Multi-alignmentdiffusionfor\nhigh-fidelityvideogenerationandediting.In:ECCV(2024)\n2\n[84] Zhao, Y., Xu, Y., Xiao, Z., Jia, H., Hou, T.: Mobilediffu-\nsion:Instanttext-to-imagegenerationonmobiledevices.In:\nECCV(2024) 2\n[85] Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M.,\nShamis, L., Kahn, J., Ma, X., Zettlemoyer, L., Levy, O.:\nTransfusion: Predictthenexttokenanddiffuseimageswith\nonemulti-modalmodel(2024) 8\n11\nA.PoMpytorchcode In the case the mask is 3 dimensional (batch, queries,\ncontext), we have a dedicated function that performs the\nIn this section, we provide code in Pytorch for the main\npartialsums.Notethatthisimplementationisnotoptimized\nparts of the Polynomial Mixer as well as our diffusion\nand that more speedup could be gained with a compiled\nblocks.\nmask.\nWe found that writing dedicated functions for specific\ndegrees led to faster runtime due to the ability of the Py- 1 def full_mask_mixer(h, mask):\nTorch’scompilertooptimizethem. Weshowbelowimple- 2 mask = mask.type(h.dtype)\n3 h = torch.einsum(’bnd, bmn -> bmd’, h, mask)\nmentationfordegrees2,3and4. # b batch, n context tokens, m query tokens,\nd dim\n1 @torch.compile 4 h = h / (1.e-7 + mask.sum(dim=2, keepdims=\n2 def po2(x: torch.Tensor): True))\n3 h1, h2 = gelu(x).chunk(2, dim=-1) 5 return h\n4 h2 = h2 * h1\n5 return torch.cat([h1, h2], dim=-1) Listing3.Pytorchcodeforthemixerpartwithfullmask.\n6\n7 @torch.compile Theselectionoperationisverysimpleandconsistsinan\n8 def po3(x: torch.Tensor): element-wiseproduct. ThewholePoMoperationisjustthe\n9 h1, h2, h3 = gelu(x).chunk(3, dim=-1)\n10 h2 = h2 * h1 computationofH(X)followedbytheselection.\n11 h3 = h3 * h2\n12 return torch.cat([h1, h2, h3], dim=-1) 1 @torch.compile\n2 def high_order_selection_(x: torch.Tensor, h:\n13\ntorch.Tensor):\n14 @torch.compile\n15 def po4(x: torch.Tensor): 3 return F.sigmoid(x) * h\n16 h1, h2, h3, h4 = gelu(x).chunk(4, dim=-1) 4\n17 h2 = h2 * h1 5 def pom(xq: torch.Tensor, xc: torch.Tensor, k:\nint, mask=None):\n18 h3 = h3 * h2\n19 h4 = h4 * h3 6 h = high_order_aggregation_(xc, k, mask)\n20 return torch.cat([h1, h2, h3, h4], dim=-1) 7 o = high_order_selection_(xq, h)\n8 return o\nListing1.PytorchcodefororderspecificPoMfunctions.\nListing4. PytorchcodefortheselectionpartandthewholePoM\nfunction.\nNext,weshowthefunctionthatcomputesboththepoly-\nnomial and the mixing depending on the degree and the\nInthePoMmodule, weaddtheprojectionsW ,W\n1...m s\npresenceofamask.\nandW foreachpartofthePoMoperation.\no\n1 def h ii ng th ,_o mr ad se kr =_ Na og ng er )e :gation_(x: torch.Tensor, k: 1 class PoM(nn.Module):\n2 if k == 2: 2 d be if as_ =_ Ti rn ui et )_ :_(self, dim, order, order_expand,\n3 4 5 6 7\n8\ne e el l li i sf f e:h h hk k= = == == =p p po o o3 42 3 4: :( ( (x x x) ) ) 3 4 5 6 7 ordes s s s s ru e e e e _p l l l l ee f f f f xr . . . . p( d o o h a) i r r o n. m d d _ d_ e e p *_ = r r r di _ o in d = e j mi i x ,t m o p =_ r a b_ d n n i( e d n a) r . s= L =i bo n ir e ad a se r )r (_ de ix mp ,an od rder*\n109 h fo= r l iis it n(g re al nu g( ex () 1. ,ch ku )n :k(k, dim=-1)) 8 ordes re _l ef x. ps ae n_ dp *r do ij m,= bn in a. sL =i bn ie aa sr )(dim, order*\n1 11 2 h = h t[ oi r] ch= .ch a[ ti (] h,* dih m[ =i -- 11 )] 9 ordes re _l ef x. pa ag n_ dp *r do ij m,= dn in m. ,Li bn ie aa sr =( bo ir ad se )r*\n1 13 4 if ma hsk = i hs .mN eo an ne (: dim=1, keepdims=True) 10 self.hom = hom\n1 1\n1\n15 6\n7\n8\nelse: i ef lim fhas m=k a. smd kai .sm dk( i_) mm= (i= )x2 e:\n=r =( 3h :, mask.to(h.device))\n1 1\n1\n11 2\n3\n4\ndef f io frw xa xcr cd i( =sse xNl qof n, #e:x sq e, lfxc a= tN to en ne t, iom nask=None):\n19 device))h = full_mask_mixer(h, mask.to(h. 1 15 6 s = self.se_proj(xq)\n2 20 1 else r: aise Exception(’unsupported dim for 1 17 8 h sh= =se sl ef l. fh .o h_ op mr (o sj ,(x hc ,) self.order, mask)\nmask (should be 2,3 or None)’) 19\n22 return h 2 20 1 # reo tu ut rp nut sep lr fo .j ae gc _t pi ro on j(sh)\nListing2. Pytorchcodeforthecompletepolynomialandmixing\nListing5.PytorchmoduleforPoM.\npart.\n12\nFor image diffusion, the base building block is simply 10 self.x_mha_ln = nn.LayerNorm(dim,\naPoMmodulefollowedbyanMLP,withresidualconnec- elementwise_affine=False, eps=1e-6)\ntionsandAdaLNmodulations. 11 self.c_mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6)\n1 def modulation(x, scale, bias): 12 self.pom = PoM(dim, order=order,\n2 return x * (1+scale) + bias order_expand=order_expand, bias=True)\n3 13 self.c_pom = PoM(dim, order=order,\n4 class DiPBlock(nn.Module): order_expand=order_expand, bias=True)\n5 def __init__(self, dim: int, order: int, 14 self.ffw_ln = nn.LayerNorm(dim,\norder_expand: int, ffw_expand: int): elementwise_affine=False, eps=1e-6)\n6 super().__init__() 15 self.ffw = nn.Sequential(nn.Linear(dim,\n7 self.dim = dim ffw_expand * dim, bias=True),\n8 self.order = order 16 nn.GELU(),\n9 self.order_expand = order_expand 17 nn.Linear(\n10 self.ffw_expand = ffw_expand ffw_expand * dim, dim, bias=True))\n11 18 self.cond_mlp = nn.Sequential(\n12 self.mha_ln = nn.LayerNorm(dim, 19 nn.SiLU(),\nelementwise_affine=False, eps=1e-6) 20 nn.Linear(dim, 8\n13 self.pom = PoM(dim, order=order, * dim, bias=True))\norder_expand=order_expand, bias=True) 21 self.gate_mlp = nn.Sequential(\n14 self.ffw_ln = nn.LayerNorm(dim, 22 nn.SiLU(),\nelementwise_affine=False, eps=1e-6) 23 nn.Linear(dim, 3\n15 self.ffw = nn.Sequential(nn.Linear(dim, * dim, bias=True))\nffw_expand * dim, bias=True), 24\n16 nn.GELU(), 25\n17 nn.Linear( 26 def forward(self, x, t, c, mask,\nffw_expand * dim, dim, bias=True)) temporal_mask=None):\n18 self.cond_mlp = nn.Sequential( 27 sx, bx, sc, bc, s1, b1, s2, b2 = self.\n19 nn.SiLU(), cond_mlp(t).chunk(8, -1)\n20 nn.Linear(dim, 4 28 gc, g1, g2 = self.gate_mlp(t).chunk(3,\n* dim, bias=True)) -1)\n21 self.gate_mlp = nn.Sequential( 29\n22 nn.SiLU(), 30 # ca\n23 nn.Linear(dim, 2 31 x_ln = modulation(self.x_mha_ln(x), sx,\n* dim, bias=True)) bx)\n24 32 c_ln = modulation(self.c_mha_ln(c), sc,\nbc)\n25\n26 def forward(self, x, c): 33 x = x + self.c_pom(x_ln, c_ln, mask) * (1\n27 s1, b1, s2, b2 = self.cond_mlp(c).chunk + gc)\n(4, -1) 34\n28 g1, g2 = self.gate_mlp(c).chunk(2, -1) 35 # sa\n29 36 x_ln = modulation(self.mha_ln(x), s1, b1)\n30 # mha 37 x = x + self.pom(x_ln, mask=temporal_mask\n31 x_ln = modulation(self.mha_ln(x), s1, b1) ) * (1 + g1)\n32 x = x + self.pom(x_ln) * (1 + g1) 38\n33 39 #ffw\n34 #ffw 40 x_ln = modulation(self.ffw_ln(x), s2, b2)\n35 x_ln = modulation(self.ffw_ln(x), s2, b2) 41 x = x + self.ffw(x_ln)*(1+g2)\n36 x = x + self.ffw(x_ln)*(1+g2) 42\n37 43 return x\n38 return x Listing7.Pytorchmoduleforthevideodiffusionblock.\nListing6.Pytorchmodulefortheimagediffusionblock.\nFor text-to-video, we add a second PoM module that\nB.Conditionadherence\ngathersinformationfromthetext.\n1 class TextVideoDiPBlock(nn.Module): In this section we study the trade-off between image qual-\n2 def __init__(self, dim: int, order: int, ityasmeasuredwithFIDandconditionadherenceasmea-\norder_expand: int, ffw_expand: int): suredwithInceptionScore(IS)byvaryingtheweightω of\n3 super().__init__() theclassifier-freeguidance(CFG).Weshowtheresultsfor\n4 self.dim = dim\n5 self.order = order a model of size L2 trained with the diffusion loss L D on\n6 self.order_expand = order_expand Gigure6. Inferenceisperformedwith250stepsofDDIM\n7 self.ffw_expand = ffw_expand sampling. Aswecansee,themodelisperfectlyabletobal-\n8\nanceFIDandIS,leadingtoatypical’U’curvewhereCFG\n9 self.mha_ln = nn.LayerNorm(dim,\nelementwise_affine=False, eps=1e-6) improvesbothFIDandISatfirst,butthenimprovementsof\n13\n∃x′ ∈X′,∀x∈X,x̸=x′),thenthereexistksuchthat\n20\n(cid:88) (cid:88)\n∀x∈X,x′ ∈X′,x xk ̸=x xk. (21)\ni i\nxi∈X xi∈X\nConsider the functions P(t) and P′(t) defined as fol-\n15\nlows:\n(cid:88)\nP(t)= xt (22)\ni\nxiinX\n(cid:88)\n10 P′(t)= xt (23)\ni\n100 150 200 250 300\nxiinX′\nInceptionScore SinceX andX′ differbyatleastoneelement,thereex-\nistsatleastonex ∈ X suchthatx ̸= x′,∀x′ ∈ X′. This\ni i i i\nFigure6. Imagequalityversusconditionadherencetrade-off. implies that the functions P(t) and P′(t) are not identical\nFID/IS curve for the L2 model with 250 DDIM sampling steps. sincearesumsofexponentialswithdifferentbases.\nValuesarecomputedon10kimagesagainstthevalidationsetof SinceP(t)andP′(t)aredifferentfunctions,theremust\nImageNet. existsomekforwhichP(k)̸=P′(k).Inotherwords,there\nexistsaksuchthat:\nIScomesatthecostofFID.Thisistypicalofmodecollapse (cid:88) (cid:88)\nxk ̸= x′k (24)\nwith the model generating low diversity but high quality i i\nimages, similarly to what is observed with attention-based xi∈X x′ i∈X′\nmodels. For this k, let us denote S = (cid:80) xk. We need to\nshowthatxS ̸=x′S′\nforallk\nx∈X\nax ni∈ dX\nx′\n∈i\nX′. Assume\nC.ProofofLemma3 k k\nfor the sake of contradiction that there exist x ∈ X and\nWe first need to show that set with different entries are x′ ∈X′suchthatxS =x′S′. Thisimplies:\nk k\nmappedtodifferentvectors. WefirstseparatePoMintoits\n(cid:88) (cid:88)\ntwocomponents: x xk =x′ x′k (25)\ni i\ns(X)=σ(W X) (18)\nxi∈X x′ i∈X′\ns\n(cid:34) (cid:35)\nRearranging,weget:\n(cid:89)\nH (X)= h(W X);...; h(W X) 11⊤ (19)\nk 1 m\nm x\n(cid:80) x′∈X′x′ ik\n= i (26)\nPoM(X)=W o(s(X)◦H k(X)) (20) x′ (cid:80) xk\nxi∈X i\nAssuming ker(W ) =, and noting that H (X) is the\no k Since S ̸= S′, the right-hand side is not equal to 1.\nsame for every column, we just have to show that s(X) k k\nHowever, for this equality to hold for all x ∈ X and x′ ∈\nhas different columns. This is easily achieved by having\nX′, the ratio x/x′ would need to be constant for all pairs\nker(W ) = since σ is injective and the composition of in-\ns (x,x′),whichisnotpossiblegiventhatX andX′differby\njectivefunctionsisitselfinjective.\natleastoneelement.\nSecond,wehavetoshowthatsetsthatdifferbyatleast\nTherefore,thereexistsak suchthatxS ̸= x′S′ forall\noneelementaremappedtoalldifferententries. Tosimplify k k\nx∈X andx′ ∈X′.\nnotations,wewillconsiderthespecialcasewhereallmatri-\nces are the identity or an identity block positioned such as\nD.Uncuratedexamples\nto perform submatrix selection. All the matrices can thus\nbe removed from the formula. A similar argument can be Inthefollowingpages,weshowrandomlyselectedsamples\nmadeformatricesthatarefullrankastheypreserveinjec- with obtained after 250 steps of DDIM sampling with the\ntivity. Wewillalsoconsiderlinearactivationseverywhere, XL/2modeltrainedwiththediffusionlossL .\nD\nwhichcanbemadeascloseasonewishbypartitioningthe\nimage of the activation function and performing piecewise\nlinearapproximation.\nWith this simplified version of PoM, we have to show\nthatfor2setsX,X′ differingbyatleastoneelement(i.e.,\n14\nDIF\nFigure7.Uncurated256²imagesfortheclassmagpie(18).\n15\nFigure8.Uncurated256²imagesfortheclassloggerhead,loggerheadturtle,Carettacaretta(33).\n16\nFigure9.Uncurated256²imagesfortheclassmacaw(88).\n17\nFigure10.Uncurated256²imagesfortheclassotter(360).\n18\nFigure11.Uncurated256²imagesfortheclassballoon(417).\n19\nFigure12.Uncurated256²imagesfortheclassicecream,icecream(928).\n20\nFigure13.Uncurated256²imagesfortheclassseashore,coast,seacoast,sea-coast(978).\n21\nFigure14.Uncurated256²imagesfortheclassvolcano(980).\n22",
    "pdf_filename": "PoM_Efficient_Image_and_Video_Generation_with_the_Polynomial_Mixer.pdf"
}