{
    "title": "Painful intelligence - What AI can tell us about human suffering",
    "context": "",
    "body": "Painful intelligence:\nWhat AI can tell us about human suffering\nAapo Hyvärinen\nUniversity of Helsinki\nSecond Edition\nSeptember 2024\narXiv:2205.15409v2  [cs.LG]  5 Sep 2024\n\n2\nAbstract\nThis book uses the modern theory of artificial intelligence (AI) to understand human suffering or mental pain.\nBoth humans and sophisticated AI agents process information about the world in order to achieve goals and\nobtain rewards, which is why AI can be used as a model of the human brain and mind. This book intends to\nmake the theory accessible to a relatively general audience, requiring only some relevant scientific background.\nThe book starts with the assumption that suffering is mainly caused by frustration. Frustration means the\nfailure of an agent (whether AI or human) to achieve a goal or a reward it wanted or expected. Frustration is\ninevitable because of the overwhelming complexity of the world, limited computational resources, and scarcity\nof good data. In particular, such limitations imply that an agent acting in the real world must cope with uncon-\ntrollability, unpredictability, and uncertainty, which all lead to frustration.\nFundamental in such modelling is the idea of learning, or adaptation to the environment. While AI uses\nmachine learning, humans and animals adapt by a combination of evolutionary mechanisms and ordinary\nlearning. Even frustration is fundamentally an error signal that the system uses for learning. This book explores\nvarious aspects and limitations of learning algorithms and their implications regarding suffering.\nAt the end of the book, the computational theory is used to derive various interventions or training meth-\nods that will reduce suffering in humans. The amount of frustration is expressed by a simple equation which\nindicates how it can be reduced. The ensuing interventions are very similar to those proposed by Buddhist and\nStoic philosophy, and include mindfulness meditation. Therefore, this book can be interpreted as an exposi-\ntion of a computational theory justifying why such philosophies and meditation reduce human suffering.\nCopyright ©2024 Aapo Hyvärinen. All rights reserved.\nDistribution allowed as per Creative Commons Attribution-Noncommercial-NoDerivatives (CC BY-NC-ND) License.\nThis book is typeset in Utopia(R), Copyright 1989, 1991 Adobe Systems Incorporated. All rights reserved.\nUtopia is either a registered trademark or trademark of Adobe Systems Incorporated in the United States and/or other countries.\nUsed under license, as implemented in the Latex fourier package.\n\nContents\n1\nIntroduction\n9\nInvestigating intelligence by constructing it\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\nIs the brain a big computer? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\nMachine learning as analogue to evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\nCan an AI actually suffer?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\nIntelligence is painful—overview of this book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nGuide to the Reader . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nI\nSuffering as error signalling\n15\n2\nDefining suffering\n16\nMedical definitions of pain\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nMedical and psychological definitions suffering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nAncient philosophical approaches to suffering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nTwo main kinds of suffering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nUsing the pain system for broadcasting errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3\nFrustration due to failed plan\n23\nAgents, states, and goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nPlanning action sequences, and its great difficulty\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nFrustration as not reaching planned goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nDefining desire as a goal-suggesting mechanism\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nIntention as commitment to a goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nHeuristics can help in planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n4\nMachine learning as minimization of errors\n33\nNeurons and neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nFinding the right function by learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nLearning as incremental minimization of errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nGradient optimization vs. evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nLearning associations by Hebbian rule\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nLogic and symbols as an alternative approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nEmergence of unexpected behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n3\n\nCONTENTS\n4\n5\nFrustration due to reward prediction error\n48\nMaximizing rewards instead of reaching goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nLearning to plan using state-values and action-values . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nFrustration as reward loss and prediction error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nExpectations or predictions are crucial for frustration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nUnexpected implications of state-value computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nEvolutionary rewards as obsessions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nReward maximization is insatiable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n6\nSuffering due to self-needs\n63\nSelf as long-term performance evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nSelf as self-preservation and survival\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nSelf-related suffering as intrinsic frustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n7\nThreat as anticipation of possible frustration\n71\nDecision-making under uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nRisk aversion and economic gambles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nFear, threat, and predictions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nThreat as prediction of possible large frustration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nInterplay of threat and frustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\nThreats and the level of intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n8\nFast and slow intelligence and their problems\n80\nFast and automated vs. slow and deliberative\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\nNeural network learning is slow, data-hungry, and inflexible\n. . . . . . . . . . . . . . . . . . . . . . . . .\n82\nUsing planning and habits together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\nAdvantages of categories and symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\nCategorization is fuzzy, uncertain, and arbitrary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n9\nSummarizing the mechanisms of suffering\n93\nFrustration on different time scales\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\nFrustration based on desires, expectations, and general errors\n. . . . . . . . . . . . . . . . . . . . . . . .\n94\nSelf, threat, and frustration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\nWhy there is frustration: Outline of the rest of this book . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\nII\nOrigins of suffering: uncontrollability and uncertainty\n98\n10 Emotions and desires as interrupts\n99\nComputation is one aspect of emotions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\nEmotions interrupt ongoing processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\nDesire as an emotion and interrupt\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nEmotions include hard-wired action sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\nHow interrupts increase suffering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n\nCONTENTS\n5\nEmotions are boundedly rational . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n11 Thoughts wandering by default\n110\nWandering thoughts and the default-mode network\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\nWandering thoughts as replay and planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\nReplay and planning focus on reinforcing events\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nReplay exists in rats, humans, and machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nWandering thoughts multiply suffering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n12 Perception as construction of the world\n122\nVision only seems to be effortless and certain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\nPerception as unconscious inference\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\nPrior information can be learned . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\nIllusions as inference that goes wrong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nAttention as input selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\nSubjectivity and context-dependence of perception\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\nReward loss as mere percept\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\nAncient philosophers on perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n13 Distributed processing and no-self philosophy\n137\nAre you really in control? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\nNecessity of parallel and distributed processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nCentral executive and society of mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\nControl as mere percept of functionality\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nPhilosophy of no-self and no-doer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n14 Consciousness as the ultimate illusion\n150\nInformation processing vs. subjective experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nThe computational function of human consciousness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\nThe origin of conscious experience\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\nWhy is simulated suffering conscious? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nSelf vs. consciousness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\nNothing is real? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nIII\nLiberation from suffering\n162\n15 Overview of the causes and mechanisms\n163\nWhy there is (so much) suffering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\nCognitive dynamics leading to suffering\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\nAn equation to compute frustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n\nCONTENTS\n6\n16 Reprogramming the brain to reduce suffering\n174\nReducing expectation of rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nReducing certainty attributed to perception and concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\nReducing self-needs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\nReducing desire and aversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186\n17 Retraining neural networks by meditation\n188\nContemplation as active replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\nMindfulness meditation as training from a new data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\nSpeeding up the training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\nReducing interrupting desires . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196\nEmptying the mind and reducing simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\nMetacognition and observing the nature of mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n18 Recapitulating and unifying interventions\n204\nRecapitulating the interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\nHow far should reducing desires and expectations go? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\nPositive viewpoints to reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\nLetting go and relaxation as unifying principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n19 Epilogue\n215\nIndex\n221\nBibliography\n228\n\nPreface (1st Edition)\nI like to write books that I would have wanted to read myself as a student. I really wish I had been able to read\nthis book. It would probably have changed my life and my career, as I would have insisted on doing my PhD on\nthis topic. Alas, when I was a student in the 1990s, the topic of this book was not something a reasonable PhD\nstudent would have worked on. There was hardly any literature on the topic; it would have been considered\nuncharted territory, if not suspicious. I hope the world has changed, and that this book may contribute to\nthat change. With the huge increase in research on AI and computational neuroscience on the one hand, and\naffective neuroscience and mindfulness meditation on the other, I think the time is ripe to attempt a synthesis,\nwhich is the motivation for this book.\nWhat I should emphasize is that this book is about a scientific theory, or rather, several scientific theories. It\nis not a book that teaches meditation; it has little to do with self-help and certainly constitutes no clinical guid-\nance. Nor is it really a philosophical book in the sense that the word would be used in academic circles: while\nthere is some philosophical speculation, the main paradigm is that of the natural sciences. It may be surprising\nthat I seem to include artificial intelligence in the natural sciences, but here it is largely used as a computational\nmodel of the brain, even if sometimes on a very abstract level. The strong neuroscience component of this book\nfurther connects it to empirical science.\nI have tried to write the book so that it is suitable for as wide an audience as possible. I believe anybody\ntrained in computer science or neuroscience should be able to understand it. Scientific training in any dis-\ncipline might be enough to understand the main ideas, and I hope that even members of the general public\nmight find something interesting in it. Although not primarily intended as such, the book can also be used as a\nuniversity-level textbook for advanced undergraduates or graduate students in computer science or cognitive\nscience; it should also be suitable for computationally minded students in neuroscience or psychology.\nThis book was written while working in different institutions. Most of the work was done while a faculty\nmember at the University of Helsinki (Department of Computer Science). Part of the writing was accomplished\nwhile a faculty member at University College London (Gatsby Computational Neuroscience Unit) as well as\na research scientist at Université Paris-Saclay (DataIA Institute and Inria–Saclay-Ile-de-France, supported by\ngrant ANR-17-CONV-0003). The work was further supported by a Fellowship from CIFAR (Learning in Ma-\nchines & Brains Program).\nFinally, I’m very grateful to Moritz Grosse-Wentrup, Riitta Hari, Marianne Maertens, John Millar, Tiina\nParviainen, Jonne Viljanen, and, especially, Michael Gutmann, for most helpful comments on the manuscript.\nHelsinki, May 2022\nAapo Hyvärinen\n7\n\nPreface (2nd Edition)\nIn the second edition (V2 on Arxiv) the theory has been slightly expanded and clarified. The main changes are:\n1. A probabilistic theory of threat is now introduced in Chapter 7, which is the only major change.\n2. Chapters 7 and 15 of the first edition have been split into two chapters, giving rise to Chapters 8–9 and\n17–18. The contents have been slightly expanded in the latter chapter of each pair.\n3. New figures (Fig. 5.1 and Fig. 18.1) have been added to improve readability.\n4. A Guide to the Reader has been added at the end of Chapter 1.\n5. More material on Greek philosophy has been added in Chapters 16, 17, and 18, which have also been\nslightly streamlined by removing some material and transferring some material to footnotes.\nI would further like to thank Mitsuo Kawato, Keith Davis, and Michael Gutmann for additional comments.\nHelsinki, September 2024\nAapo Hyvärinen\n8\n\nChapter 1\nIntroduction:\nUnderstanding human suffering by AI\nWhat is the most central question in human life? For me, it is the question of suffering. There may be questions\nwhich are more fundamental, or philosophically more fascinating, for example: Why does the world exist? Or,\nhow is it possible that we are conscious? But those questions are rather theoretical and mainly satisfy one’s\nintellectual curiosity. If you found the answer to those latter questions, would that change your life, or other\npeople’s lives, for the better?\nThe question of suffering is with us at every moment. By suffering I mean mental pain, the opposite of\npleasure and happiness. In some cases, it is a result of physical pain, but usually of purely mental origin. In\nfact, any casual observer of human life easily comes to the conclusion that it is full of such suffering: There is\nfrustration, anxiety, sadness, depression, and so on.\nWhy is the “human condition” so unpleasant: did somebody (or something) make a huge mistake in de-\nsigning humans? And, most importantly, is there anything we can do about it: can we remove suffering, or at\nleast reduce it? Now, this is a question that has enormous practical significance. Reducing suffering, almost by\ndefinition, makes people’s lives better.\nThe starting point of this book is the idea that we can use the theory of artificial intelligence, or AI, to\nunderstand why there is so much suffering in humans. This book will show how suffering is largely due to the\ninability of an intelligent system, whether an artificial intelligence or a human being, to understand its own\nprogramming and its own limitations, in particular the limitations of its computation and data.\nInvestigating intelligence by constructing it\nHow can I claim that the theory of AI has any relevance to understanding the human mind, let alone suffering?\nThe answer lies in how AI can help us understand the computational design principles which are applicable to\nhumans as well.\nWhen I asked above if somebody made a huge mistake in designing humans, that “somebody” was of\ncourse evolution, metaphorically speaking. Evolution designed the basic processes of our mental life, for good\nor bad. Importantly, evolution didn’t construct our brains in some random, arbitrary ways, but it designed\nus to be fit for certain purposes and goals. Ultimately, those evolutionary goals are about reproduction and\n9\n\nCHAPTER 1. INTRODUCTION\n10\nspreading your genes, but to satisfy that ultimate goal, many more intermediate goals need to be considered.\nYou have to get food, find sex partners, and not get killed. These, in turn, require that you know how to walk,\nand you are able to recognize objects as well as to plan your future actions.\nWe can learn to understand such evolutionary design goals by trying to design and construct an AI, or a\nrobot. This is a perspective which is gaining more and more prominence in neuroscience: Trying to actually\nconstruct an intelligent system forces you to think about the computation and algorithms needed.\nOrdinary neuroscience is based on conducting experiments on humans or animals. It can establish many\ninteresting facts about the brain; for example, where in the brain the processing necessary for vision or fear\ntakes place. In particular, it can also tell us a lot about how such processing happens; it can explain how the\nbrain recognizes that the animal in front of you is a cat and not a dog, and how the brain initiates a fear response\nif the animal actually turns out to be a tiger.\nHowever, the deepest question in neuroscience is the why question: Why does a certain kind of processing\ntake place at all? What is its evolutionary purpose? Why do we, for example, have emotions like fear in the\nfirst place? Why is our mind frequently assailed by thoughts about the past and the future even when we try to\nconcentrate on the present? And ultimately, why is there suffering?\nDesigning intelligent systems goes a long way toward answering the “why” question. If we find that an AI\nnecessarily needs a certain kind of computation to achieve human-like intelligence, it is likely that the human\nbrain does that same kind of computation—at least on some level of abstraction. AI can also give us a deeper\nunderstanding of “how” computations happen in the human brain, since designing it necessarily forces the\nscientists to figure out all the details needed in the computation.\nIs the brain a big computer?\nThe prerequisite for learning about the brain by building intelligent systems is that our brain is in many ways\nlike a computer. In fact, the modern paradigm in neuroscience and psychology considers the brain as an\ninformation-processing device. The term “cognition” is used to describe information-processing performed\nby the brain, while with ordinary computers we usually talk about computation.\nThe brain receives new data by seeing, hearing, or otherwise sensing things. It processes the sensory data in\nvarious ways, ultimately enabling us to recognize objects and act in the world. It can also process information\nretrieved from its own memory, which is necessary for what we call thinking in plain English. A system that\nprocesses information in such ways can be called, almost by definition, a computer, so it is natural to say that,\nactually, the brain is a computer.\nCertainly, the brain is very different from any ordinary computer that you can buy in a shop. For example,\nyour PC, or your mobile phone, has a central processing unit (CPU), sometimes a couple of them. The brain has\nno such thing. The information-processing happens in the neural cells, or neurons. Each of them is like a tiny\nCPU which can only perform extremely simple processing— but there is a huge number of them, tens of bil-\nlions. The crucial difference with respect to a CPU is that each neuron processes its own input independently,\nand all the neurons do that at the same time—this is called parallel and distributed processing.\nYet, from an abstract viewpoint, such differences can be seen as just technical details. In particular, if\nwe are interested in the question of “why” certain computations are performed, the physical structure of the\ninformation-processing device, or even the details of the programming do not matter. What really matters for\nour purposes is whether the brain and the computer need to solve the same kinds of computational problems.\n\nCHAPTER 1. INTRODUCTION\n11\nThis will be the case if humans and the AI live in the same kind of environment, have the same kind of goals\nfor their actions, and use similar means to try to reach them. That is increasingly the case when AI develops\nin terms of autonomous robots, for example, and in any case, we can use our current AI theory to extrapolate\nwhat AI’s might be like in the future.\nMachine learning as analogue to evolution\nEven granted that humans and computers are both information-processing devices, some would argue that\nthey process information based on very different principles. A popular claim is that a computer does exactly\nwhat it is programmed to do, and nothing else, and this is supposed to be very different from humans who\ndo what they want themselves —so any parallels between humans and computers are impossible. I think this\nreasoning is fundamentally wrong, for two reasons.\nFirst, modern AI systems do not just do what they are programmed to do. That’s because their function is\nbased on learning. They are programmed to learn from input data. The input may be a database determined\nby the programmer; it can be obtained by crawling the internet; or it can be the result of interactions with the\nenvironment, like a robot using a camera or users typing words, and so on. What the programmer really does\nis to provide an algorithm for learning. The algorithm is based on certain goals or objective functions that the\nAI is trying to optimize. An AI dedicated to searching the internet for images that resemble a given target image\nwill learn to optimize the accuracy of its search results, for example by maximizing the number of clicks users\nmake on each image it proposes.\nWhat this means is that anyone who programs an AI cannot really know in detail what the AI will actually\ndo, because it is often impossible to know what kind of input the AI will receive, and it is equally difficult to\nunderstand what the AI will learn from it. Even in the simplest case where the programmer completely decides\nthe input to the AI, the input is often so complex (say, millions of pictures downloaded from the internet) that\nit is impossible for a human programmer to understand what can be learned from that data.\nThe second reason why there is not such a big difference between humans and AI is that just like an AI is\nprogrammed by humans, we humans are designed—one might say “programmed”—by evolution. From an\nevolutionary perspective, we are programmed to maximize an objective function which is roughly given by the\ntotal number of copies of our genes in the population. To satisfy such programming, we gather a lot of data—by\nreading things, talking to people, and simply looking around—which is not so different from an AI.\nSo, I have turned the claim about the difference between AI and humans on its head. What humans and AI\nhave in common is that both are programmed by something else to have certain goals and needs; nobody has\nreally decided “by themselves” to have the needs and goals they have. To accomplish those goals, both humans\nand AI gather data from the environment and learn from it, which leads to actions that are very difficult to\npredict. So, in the end there is little difference between AI and humans, except regarding the source of the\noriginal programming—whether it was by evolution or a human programmer.\nCan an AI actually suffer?\nBy now, I hope to have convinced you that an AI is a useful model of many phenomena taking place in the\nhuman brain. But perhaps there are limits. Some would argue that we cannot talk about AI’s or robots suffering:\n\nCHAPTER 1. INTRODUCTION\n12\nThey may seem to be suffering, or look like they are suffering, but in fact they are not, because they cannot feel\nanything.\nI think this argument may not be completely wrong, but it is quite irrelevant. Obviously, it depends on the\nexact definition of what suffering is. It is true that AI may not feel suffering in the same way as humans because\nthat might require that AI is conscious, i.e., it has subjective experiences. This argument against AI’s suffering\nreally hinges on two points: First, that an AI is not conscious, and second, that consciousness is necessary for\nsuffering.\nHowever, conscious feeling is only one part of suffering. The situation is similar with emotions, such as fear,\nwhich are actually clever information-processing mechanisms. The conscious feeling of being afraid is only\none part of a complicated process involving cognition (or information-processing), behavioral tendencies, and\nseveral other aspects. I would argue it is the same for suffering.\nSuffering is actually a signal in a complex information-processing system. The real meaning of the suffering\nsignal is that an error occurred—this will be elaborated in several chapters in this book. Any information-\nprocessing system can create error signals. That’s why we can, in that specific sense, say that an AI or a robot\nis suffering, even if they are not conscious. All that would be missing is the conscious feeling components of\nsuffering.\nThere is an even more important reason why it is largely irrelevant here if an AI really suffers according to\nsome stringent definition of the word. This book does not just aim to describe the mechanisms of suffering;\nthe primary goal here is to develop various ways of alleviating suffering. For the purpose of reducing suffering,\nit does not matter if computers actually suffer in some deeper sense. If we can reduce suffering in an AI that is\nsufficiently human-like, then, with reasonable probability, the same methods will apply to humans, and they\nwill reduce suffering in humans, including the conscious experience of suffering. In other words, the AI is really\na simulation or a model of mechanisms that are relevant for making humans happier.\nFor those who find it impossible to think that a computer could suffer in any sense of the word, I suggest\nthe following viewpoint that they can use while reading this book. Trying to understand human suffering by\nAI is one big thought experiment, where we try to understand how much the AI would suffer under various\ncircumstances, if it were able to consciously experience suffering. It is like a mathematical model of atoms, or\nlike a computer simulation of chemical processes. Everybody agrees that models and computer simulations\nare not the real thing, but they can help us understand the actual natural processes, and in particular, predict\ntheir behavior. A model may tell you how a change in one quantity, say X, leads to a change in another quantity,\nY. If you know that, you can perhaps choose X to maximize or minimize Y—which might be suffering.\nIntelligence is painful—overview of this book\nThe central hypothesis in this book is that if we create an artificial intelligence that is really intelligent, really\nworthy of its name, it will necessarily perform computations which are more or less like human suffering. In\nspite of the many differences between AI’s and humans, there is a common logic in the design. In order to\nachieve sufficiently human-like intelligence, certain design principles have to be followed, and these lead to\nsuffering. This book explores several interwoven ideas about such a computational basis of suffering, and the\nnecessity of suffering as a part of intelligence.\nThe fundamental approach here is that suffering is caused by error signalling, which is typically due to\nfrustration. Frustration occurs when an intelligent system, generally called an “agent”, fails to achieve a goal,\n\nCHAPTER 1. INTRODUCTION\n13\nor it obtains less reward than it expected. Such errors are inevitable in a complex world, where things are\nuncertain and unpredictable, and we have limited control over them. Error signalling is necessary for any\nsufficiently intelligent system, since such error signals are used by learning algorithms. Our brain produces\nerror signals automatically, and we simply cannot shut off the error-signalling system.\nIn fact, the complexity of the world is overwhelming for any known intelligent system, whether the very\nbest supercomputer in the world, or the most intelligent human brain. The computations available to them\ncannot handle all the different possibilities when, for example, choosing action sequences to reach a given\ngoal. Modern AI uses learning to cope with such complexity. However, for such learning to be really successful,\nhuge data sets are required. Obtaining data sets which completely capture the complexity of the world is rarely\npossible in practice. These two factors, lack of computational resources together with scarcity of data, mean that\nthe intelligent agent cannot work optimally. Its intelligence and its control over the world are limited. Thus,\nthere will be errors: The agent’s actions do not always lead to the desired outcome, hence frustration.\nSuffering is greatly enhanced by several information-processing principles inherent in the design of human-\nlike intelligent systems. One is the phenomenon of experience replay, where memories related to past errors\nare recalled and repeated in the system in order to optimize learning about past experiences. Likewise, plans\nfor future actions are constantly computed, which means the agent simulates or “imagines” them in its mind,\ntogether with the ensuing errors. Such replay and planning multiply any suffering arising from real events:\nerrors are signalled as if those bad, imagined events happened for real. Further suffering is created by the per-\nception of threats, or predictions of future frustration; that means frustration that did not actually happen but\njust might happen with some probability. Thus, we suffer from mishaps which only happen in our imagination.\nMeanwhile, modern AI has found systems based on parallel and distributed information processing to be\nuseful for programming intelligent systems, which makes it understandable that our brain uses similar prin-\nciples. However, such processing leads to overwhelming uncontrollability. Systems that are parallel and dis-\ntributed do not admit central executive control, since different modules are competing for control; this makes,\nfor example, any sustained attention or concentration difficult. Any internal control of the agent’s computa-\ntions is further reduced by emotions such as fear, which work as evolutionarily conditioned “interrupts” of\nongoing processing. Thus, the agent has little control even of its own internal processing, let alone the external\nworld. A related problem is the uncertainty of our perceptions, and the difficulty of understanding how un-\ncertain most perceptions and inferences actually are. Perceptions are often highly subjective and contextual\ninterpretations, sometimes little more than guesses. However, humans often mistakenly think that our per-\nceptual systems are able to discover some underlying objective reality. Such uncontrollability and uncertainty\nboth increase suffering by increasing frustration and other errors.\nFinally, the goals and desires that have been programmed in us by evolution are ultimately counterproduc-\ntive and make us unhappy. Evolution never had our happiness as its goal anyway. In fact, it forces us to do\nthings which are clearly bad for our happiness, something I call evolutionary obsessions. Evolution makes us\nworry about our survival and our evolutionary performance, creating a sense of self. In fact, evolution does not\nwant us to reduce suffering because the error-signalling system is necessary for learning and optimal behavior.\nWhat evolution does want us to learn is to act in more and more efficient ways, but the goals towards which\nthis intelligence is used are those set by evolution, not us. Even worse, both AI and humans are usually trying\nto satisfy their drives and desires endlessly, without any limits; at no point do they become satiated and think\nthat they have achieved enough.\nHowever, there is hope. At the very end of the book, I sketch interventions, or mental training methods,\n\nCHAPTER 1. INTRODUCTION\n14\nthat can be used to decrease suffering, based on the theories outlined in this book. What is needed is a\nreprogramming of the brain. The key method is to retrain the brain by inputting new data into the learning\nsystem. The new data will change the computations in such a way that error signals, and in particular frustra-\ntion, are reduced: learning to reduce expectations and desires is crucial here. This is difficult and takes a lot of\ntime, but various forms of philosophical contemplation and mindfulness meditation attempt to do it. These\nmethods are rather logical consequences of the theory, while they have mainly been proposed earlier in Bud-\ndhist, and to some extent Stoic, philosophy. Thus this book can be seen as an attempt to construct a scientific,\ncomputational theory on the underpinnings of such philosophies and meditation.\nGuide to the Reader\nObviously, the recommended way is to read all the chapters in the order presented. Footnotes can be skipped\nby readers not interested in the details. However, for busy readers, here is an outline of shorter paths through\nthe book:\n• A short overview of the basic ideas can be extracted by reading Chapters 2, 3, 9, 15 and the first full section\nof Chapter 18 including Figure 18.1.\n• A slightly longer overview can be extracted by reading Chapters 2, 3, 5, 7, 9, 15, 16 and the first full section\nof Chapter 18 including Figure 18.1.\n• A slightly shortened version emphasizing Buddhist-Stoic philosophy and meditation can be obtained by\nreading Part I and Part III, thus skipping Part II. (Chapter 15 in Part III contains a short summary of Part II,\nso at least on some level, it should be possible to understand Part III without Part II.)\n\nPart I\nSuffering as error signalling\nThe first part will explore the very definition of suffering,\nexisting proposals on how suffering comes about,\nand how these can be understood by the theories of AI and evolution\n15\n\nChapter 2\nDefining suffering\nIn this chapter, I try to define the word “suffering”. This is not an easy task, as we will quickly see. Defining the\nterm properly requires, to some extent, elucidating the underlying mechanisms creating suffering.\nOne fundamental point here is that I exclude physical pain from the definition of suffering; I use the word\nsuffering synonymously with mental pain. Nevertheless, I will start the search for a definition of suffering by\nconsidering the closely related concept of pain, taken here in the medical sense of physical pain.\nThe central conclusion of this chapter is that the main definitions of suffering consider it based on either\nfrustration or a threat to the intactness of the person. These two definitions, and especially the definition based\non frustration, are the basis of the developments of the rest of this book. From a more abstract viewpoint, I will\nargue that such suffering can be seen as error signalling, similarly to physical pain.\nMedical definitions of pain\nLet us start by defining pain. Pain has been given a widely accepted consensus definition by the International\nAssociation for the Study of Pain (IASP) as:\nPain is an unpleasant sensory and emotional experience associated with actual or potential tissue\ndamage or described in terms of such damage.\nSurprisingly, while this definition was originally adopted in 1979, it is still used with minimal modifications.\nIt posits damage to any tissue of the person, or any threat of such damage, as the origin of pain. Pain is then\ndefined as an ensuing unpleasant experience. While this definition has been found to be quite useful in a\nclinical context, deeper theoretical analyses have found various problems.1\nOne important controversy is whether one should define pain as a subjective experience, or as something\nthat has a more objective existence. The definition above talks about an “experience” which is here interpreted\nas a conscious, subjective experience: something that only I am aware of, and which you cannot measure in\nany objective way. As we will discuss in more detail in Chapters 10 and 14, this problem of subjective conscious\nexperience vs. objectively observed phenomena is ubiquitous in neuroscience and psychology.\n1Cohen et al. (2018) gives a long review of competing definitions; Corns (2016) considers the validity of the very concept; Klein (2007)\nproposes an alternative definition and reviews some philosophical approaches. IASP has very recently proposed a revised version (Raja\net al., 2020), but the changes are minimal and rather immaterial for our purposes.\n16\n\nCHAPTER 2. DEFINING SUFFERING\n17\nThe problem with talking about such subjective experience in a scientific context is that objective, repro-\nducible measurement is the basis of science. Fortunately, subjective experience can be measured in various\nindirect ways, such as verbal report. That is, we can ask the patient if there is pain. Yet we will never know for\nsure what the patient actually feels. In particular, we cannot tell how her experience of pain compares with\nother people’s experience: Does she feel more or less pain than some other patient who gives the same verbal\nreport?\nThis problem in the definition of pain is to some extent alleviated by the reference to tissue damage, which\nis objectively measurable and reasonably well-defined. Yet, as this definition clearly points out, actual tissue\ndamage is not necessary for pain—since it can be just “potential”—and thus it does not provide a basis for\nmeasuring pain or for objectively defining it. (In fact, the definition does not actually say that pain is in any\nsense proportional to the amount of damage— it is well-known that tissue damage that creates a lot of pain\nin one person may create little pain in another—which complicates any measurement even more.) Another\nrelated problem with the IASP definition above is that it relies heavily on the word “unpleasant”, which is not a\nvery well-defined term, and, again, quite subjective.\nOne approach to solve these problems is to take an evolutionary approach. To begin with, we could replace\n“unpleasant experience” in the definition by “experience that has evolved to motivate behaviour, which avoids\nor minimises tissue damage, or promotes recovery”.2 Here we go towards defining pain using its evolutionary,\nfunctional role, while still acknowledging the subjective nature of pain by talking about an “experience”. The\ndownside of such an approach is that it works on a very abstract level, and provides no details on what might\ncause pain, in contrast to the IASP definition which explicitly points at tissue damage (even if only potential).\nThis definition, in a sense, shifts the burden to understanding the evolutionary goals of certain experiences,\nwhich is not easy either. However, one obvious candidate for such an evolutionary goal is minimizing tissue\ndamage and recovering from it, which links this evolutionary approach with the IASP definition. In more gen-\neral terms, the evolutionary goal could be the maintenance of “homeostasis”, that is, an optimal balance in the\nphysiological condition of the body.3 Such evolutionary logic can be applied on suffering as well, and we will\nsee related argumentation throughout this book.\nMedical and psychological definitions suffering\nIn contrast to pain, suffering is a rather neglected term in science, and there is nothing like a consensus def-\ninition. Intuitively, most people would think suffering also contains an unpleasant feeling or experience as an\nintegral part, while being more abstract and general than physical pain, in particular including more psycho-\nlogical and emotional aspects. A typical dictionary definition is “Feeling of pain or strong stress, either physical\nor emotional”.4 Like pain, suffering is often considered a subjective experience which cannot be objectively\nmeasured.5\nOne simple and concrete approach to define suffering is to give examples of phenomena related to suffering\nand possibly producing suffering. A typical list would contain grief, sadness, discomfort, distress, anguish,\n2This definition is by Wright (2011). On a related note, Seymour (2019) emphasizes the importance of pain as a signal used in control\nand learning, and relativizes the importance of conscious experience.\n3(Craig, 2003)\n4https://psychologydictionary.org/suffering/\n5(Cassell, 1982; Edwards, 2003)\n\nCHAPTER 2. DEFINING SUFFERING\n18\nfear—which is just a random sample, and many different lists can be produced. While this is a good starting\npoint, it does not lead to a solid scientific theory.\nTerms such as psychological pain or mental pain are often preferred in neuroscience, and some attempts at\ndefinitions of those terms have been made.6 In this line of thinking, suffering is really a generalization of pain.\nThis may not solve the problem of defining suffering, since the burden is then simply shifted to defining pain,\nbut then we can leverage the large literature on pain, in particular the IASP definition just given, as well as any\nof its critique and improvements.\nOne approach distinguishes three kinds of pain: physical pain, social pain, and psychological pain.7 An\ninteresting emphasis in this line of research is that all these different kinds of pain are neurally very similar in\nthe sense that the brain areas responsible are the same.8 Here, physical pain is primarily due to physical dam-\nage to the body, but it can also be felt when there is a strong anticipation of such physical damage (think about\ngoing to a dentist), reminiscent of the IASP definition. In contrast, social pain is an unpleasant feeling due to\nsocial exclusion or rejection. Psychological pain is largely the same as what I call mental pain or suffering, and\nattempt to define here.\nImportantly for our purposes, in such an approach, mental or psychological pain is often assumed to be\ndue to reward loss, defined as follows9\n[Reward loss is] a negative discrepancy between expected and obtained rewards.\nIn other words, reward loss happens when you expect a reward but don’t get it, and it leads to mental pain.\nReward loss can also be called frustration, although sometimes this term is reserved for the actual suffering\ncaused by reward loss. This provides one important computational viewpoint: reward loss is a function of\ncomputations involving expectations, observations of the obtained reward, and their difference.\nAn alternative approach emphasizes how suffering is related to our person, or self. Psychological or mental\npain has been characterized as an aversive state of high self-awareness of inadequacy,10 or a negative appraisal\nof an inability or deficiency of the self.11 This is analogous to the IASP definition of physical pain in the sense\nthat there is “damage”, even if purely mental, to one’s image of oneself as a psychological and social entity.12\nA particularly potent and influential idea in this vein is that suffering necessarily involves a threat to, or a\nloss of, the intactness of the person, as proposed by Cassell:13\nSuffering is a state of severe distress induced by the loss of the intactness of person, or by a threat\nthat the person believes will result in the loss of his or her intactness.\n6Reviews on the topic are provided by Mee et al. (2006); Tossani (2013); Papini et al. (2015). The term “mental pain” could be criticized\nbecause all pain is ultimately mental, as seen in the IASP definition. In this book, I mainly use the term “suffering”.\n7(Papini et al., 2015; Eisenberger and Lieberman, 2004; MacDonald, 2009). Pain based on empathy when one sees others hurting, or\n“vicarious” pain, could be added to the list (Singer et al., 2004).\n8However, see Iannetti et al. (2013); Wager et al. (2016) for criticism of the reverse inference used in that work. Iannetti and Mouraux\n(2010) argue that the brain network considered may be more related to detection of saliency (i.e. how much attention a stimulus\nattracts).\n9(Papini et al., 2015)\n10(Baumeister, 1990; Orbach et al., 2003)\n11(Meerwijk and Weiss, 2011)\n12In this line of research, typical in the philosophy of medicine and bioethics, suffering is sometimes seen as something particularly\nstrong (Degrazia, 1998; Hoffmaster, 2014), in particular stronger than any pain typically encountered in everyday life. I don’t follow\nsuch a definition here: in this book, suffering can be very mild or very strong.\n13(Cassell, 1989)\n\nCHAPTER 2. DEFINING SUFFERING\n19\nThis is a natural generalization and abstraction of the IASP definition of pain as related to “tissue damage”.\nIn this definition, damage to the intactness of the person actually includes tissue damage, but it is something\nmuch more general, in particular, it includes damage to one’s self-image. It is of course crucial to understand\nwhat “intactness” means more precisely; Cassell emphasizes the generality of this notion, saying that “suffering\nmay occur in relation to any aspect of personhood”.14\nA general theory that combines pain and several kinds of suffering in a single framework has been devel-\noped by van Hooft.15 He starts from an Aristotelian conception of the human person as having four “parts of\nthe soul”. They range from the lowest level of biological functioning to the emotional/desiring functions and\nthe rational functions, finally reaching the sense of the meaning of existence. In his theory, each of these parts\nhas its own goals, its own form of “fulfillment”, which is again an Aristotelian idea. Suffering is then nothing\nelse than frustration, namely “frustration of the tendency towards fulfillment” of one of the different parts of\nthe soul. In this theory, the lowest level of biological functioning is even below ordinary pain and pleasure, and\nsimply about staying healthy and alive. Ordinary physical pain is the frustration on the emotional/desiring\nlevel, where the goal of the organism is to gain pleasure and avoid pain. Frustration of rational (intellectual)\nfunction refers to suffering which happens when it is not possible to reach long-term goals that one plans for\nand expects to reach. Frustration on the highest, “spiritual” level happens when it is impossible to understand\nwhy it is me that is sick—in the medical context where van Hooft writes—or life seems meaningless due to the\ndespair and fear which a malady brings with it. This last kind of suffering brings us close to the kind of suffering\nconsidered in existential philosophy.16\nClosely related definitions can be found in the literature on stress: Lazarus and collaborators define “psy-\nchological stress” as “a particular relationship between the person and the environment that is appraised by the\nperson as taxing or exceeding his or her resources and endangering his or her well-being”.17 So, we have to con-\nsider the possibility that stress is another kind of suffering, or a mechanism for suffering. However, I don’t take\nsuch a view in this book because the classic definition by Hans Selye, “the father of stress”, proposes that “stress\nis the non-specific response of the body to any demand” (my italics). This is a very general definition, and Se-\nlye has explicitly emphasized that positive, happy events can induce stress just as well as negative, threatening\nones; think about an athlete engaged in a competition. Based on this definition, it does not seem possible to\nsimply consider stress as one kind of suffering, unless we focus on the negative kind of stress, termed “distress”\nby Selye.18 The distinction between distress and “pleasant” stress is, unfortunately, not very clear; it has been\nproposed that it is the unpredictability and uncontrollability of a situation which distinguish the unpleasant\ndistress from other kinds of stress.19 Their connection to suffering will be considered from different viewpoints\nin this book.\n14For recent critique of Cassell’s approach, see Bueno-Gómez (2017) who criticizes Cassell’s definition precisely on the ground that\n“intactness” is not well-defined and may not even exist; another point of critique is that Cassell’s definition ignores existential suffering.\nFurther criticism is given by Tate and Pearlman (2019) who propose to define suffering as “a loss of a person’s sense of self” together\nwith “a negative affective experience”.\n15(Van Hooft, 1998)\n16(Svenaeus, 2014; Bueno-Gómez, 2017)\n17(Lazarus and Folkman, 1984); see also Lazarus (1993). Their work emphasizes the individual’s perception and interpretation of the\nevents by the term “appraise”, related to Cassell’s definition which talks about “believing”. Another related approach to defining stress\nemphasizes conservation of resources, and defines the stress as, roughly, loss of resources (Hobfoll, 1989).\n18See Fink (2016) where the quote by Selye is also taken from.\n19(Koolhaas et al., 2011)\n\nCHAPTER 2. DEFINING SUFFERING\n20\nAncient philosophical approaches to suffering\nCenturies before any such modern developments, some ancient philosophers already made great progress in\nunderstanding suffering. The best expert on the topic may have been the Buddha, and in fact the whole of\nBuddhist philosophy can be seen as a theory of suffering—especially when considering the original version\nproposed by the Buddha himself. He gave the following description of suffering:20\nUnion with what is displeasing is suffering; separation from what is pleasing is suffering; not to get\nwhat one wants is suffering.\nThis is actually not so much a definition of what suffering is, but rather an attempt to describe what the main\ncauses of suffering are.\nStoic philosophers in ancient Greece and Rome had very similar ideas. Epictetus, one of the most famous\nStoics, describes mechanisms that lead to suffering as follows:21\n[D]esire promises the attainment of that of which you are desirous; and aversion promises the\navoiding that to which you are averse. However, he who fails to obtain the object of his desire is\ndisappointed, and he who incurs the object of his aversion wretched.\nThese are essentially a reformulation of the points given by the Buddha above. We can summarize these philo-\nsophical ideas as the following two causes for suffering, each with two variants:\n1 a) Not getting what one wants (Buddha, Epictetus)\nb) Something pleasant, which one would like to be present, is absent (Buddha)22\n2 a) Not being able to avoid what one is averse to, i.e., wants to avoid (Epictetus)\nb) Something unpleasant is present (Buddha)\nThen, the definitions by the Buddha and Epictetus can be interpreted in terms of wanting (point 1) and\naversion (point 2) only. Point 1 in particular defines the typical case of frustration, related to the reward loss\nalready considered above. Thus, we see that both the ideas of both the Buddha and Epictetus can be simply\nsummarized as saying that suffering comes from frustration. Using the term somewhat liberally, we can also\ncall the suffering in point 2 frustration, since the desire to avoid something is frustrated.23\n20This is from a fundamental discourse by the Buddha found in one of the earliest known layers of Buddhist literature, the Pali\nCanon. Different versions are available in Samyutta Nikaya 56.11, Majjhima Nikaya 141, and Digha Nikaya 22, where the last one is\nthe most detailed version. This quote is part of the description of what is called the Four Noble Truths, of which we here consider only\nthe first one (see footnote 35 in Chapter 16 for the rest). The whole description of the first truth, synthetizing the different versions,\nsays approximately: Birth is suffering, ageing is suffering, illness is suffering, death is suffering; grief, lamentation, pain, distress, and\ndespair are suffering; union with what is displeasing is suffering, separation from what is pleasing is suffering, not to get what one\nwants is suffering. (Several partial translations of the Pali Canon are available on the internet and I will often select the translation I\nfind the most compatible with my terminology; the one in the main text here is by Bhikkhu Boddhi.)\n21Paragraph 2 in The Enchiridion, compiled approximately 125-135 CE. Quotes in this book are taken from the translation by E. Carter\nat classics.mit.edu/Epictetus/epicench.html unless otherwise mentioned.\n22I interpret “separation”, also translated as “dissociation”, in the quote by the Buddha not simply as absence but as absence of\nsomething one would like to be there since it is pleasant.\n23My logic is that if something pleasant is not present as in 1b, the point is that one actually wants that pleasant thing to be present,\n\nCHAPTER 2. DEFINING SUFFERING\n21\nTwo main kinds of suffering\nNow I shall try to recapitulate the ideas above, both ancient and modern, as succinctly as possible. I think we\nonly need to talk about two kinds of suffering, or rather two mechanisms producing suffering, namely:\n1. Frustration (e.g., Buddha, Epictetus, several neuroscientists24)\n2. Threat, especially to the intactness of the person, including their self-image (e.g., IASP, Cassell)\nBased on this dichotomy, this book will develop two computational definitions of suffering, one each for these\ntwo aspects. Two different definitions of frustration are given in Chapters 3 and 5, respectively, while threat is\ndefined in Chapter 7. Chapters 7 and 9 consider some connections between the two concepts, and in particular,\nhow frustration is more important than threat from the viewpoint of designing interventions.25\nThe emphasis in the following chapters is, obviously, on information processing. As already argued in\nthe introduction, my main justification for talking about information-processing is that the framework of\ninformation-processing is a practically useful way of describing suffering in the precise sense that it can tell us\nsomething about how to reduce suffering. Information-processing is something that we can influence, some-\nthing we can intervene on, so from a practical viewpoint, it is a very important aspect of suffering to investigate.\nFocusing on information-processing is also perfectly in line with the current emphasis on cognition in neuro-\nscience and psychology; I see cognition as synonymous with information-processing.\nUsing the pain system for broadcasting errors\nTo conclude this chapter, I discuss some computational principles that explain why pain and suffering are\nso closely related. First, I propose that on a more abstract computational level, both pain and suffering are\nessentially error signals, messages that something is going wrong from the viewpoint of the goals and rewards\nof the system. Clearly, frustration signals that something went wrong in terms of not getting what one wants,\nand a similar case will be made for the threat to the person in Chapters 6 and 7. Such error signals are in fact\nubiquitous in artificial intelligence, where, in particular, they can be used for learning to choose actions better\nin view of maximizing rewards. We will see several kinds of error signals in the following chapters, and see how\nsome of them can be interpreted in terms of suffering.\nPain is thus an evolutionarily primitive form of an error signal. Its unique feature is that pain signals are\nbroadcast widely in the information-processing system. This is important in an agent whose computation is\nso this is also a question of not getting what one wants, as in 1a. The same logic shows that 2a and 2b are really the same thing. The\npoints 1b and 2b present the difficulty that they use the terms pleasant (or “pleasing” in the translation quoted above) and unpleasant\n(or “displeasing”), much like the IASP definition of pain. (Alternative translations of these two words include “beloved”/”unbeloved”\n(Thanissaro Bhikkhu) “loved”/”loathed” (Nanamoli), “liked”/”disliked” (P. Harvey), and indeed “pleasant”/”unpleasant” (Piyadassi\nThera), given at https://www.accesstoinsight.org/tipitaka/sn/index.html#sn56.) I suggest the key here is that “pleasant”\nis here assumed to necessarily lead to wanting (and “unpleasant” to aversion), and thus the Buddha is really talking about desire or\nwanting and aversion. When he specifically mentions wanting at the end of the quote, that may be seen as a kind of summary of the\ntwo first sentences.\n24Among recent neuroscience, see especially Papini et al. (2015), but the idea has a long history in experimental psychology as re-\nviewed by Papini et al. A very similar point is made by Pascal in his famous formula: “c’est être malheureux que de vouloir et ne\npouvoir.” (Pensées, fragment Misère, 24).\n25Van Hooft’s theory can also be seen as combining these two aspects. While it starts with frustration, threats to the person can be\nseen as frustration of certain long-term goals, as explained in more detail in Chapter 6.\n\nCHAPTER 2. DEFINING SUFFERING\n22\ndistributed into different modules (whether processors or brain regions, see Chapter 13). For such an agent,\nit is necessary that any really important signal uses a special pathway that allows it to be broadcast to all, or\nmost of, the modules. The pain signal is indeed broadcast widely to different neural systems, and the signal\ncan change the behavior of the whole organism in terms of making it stop whatever it is doing and pay close\nattention to the pain. Furthermore, when an error signal drives the learning of the system, as we will consider\nin later chapters, it often needs to be observed by several of the modules, and such broadcasting is essential.26\nSuffering is largely using the neural systems originally developed for physical pain, as already mentioned.\nThis makes evolutionary sense if we think that computationally more sophisticated forms of error signalling,\nsuch as frustration, simply started using the evolutionarily older pain signalling pathway, adapting it for their\nown purposes. That was practical because the pain system already existed, and served well the purpose of\nbroadcasting error signals to many brain regions. Using the physical pain system for signalling mental pain is\nthus a useful computational shortcut.27\nYet, merely talking about information-processing, as in a computer, may seem a rather incomplete descrip-\ntion of suffering. Why does suffering hurt, if it is merely a signal in an information-processing system? This is\nin fact exactly the same problem that we encountered with the IASP definition of pain above: Is it a subjective\nexperience, or something more objective and measurable? The evolutionary rationale just described explains\nwhy suffering “hurts” in the same way as physical pain: the physical pain system is hijacked for the purposes\nof suffering or mental pain. (Perhaps this explains why we talk about mental “pain” in the first place.) The very\ndichotomy of experience vs. objective measurements is thus exactly the same for pain and suffering, since it\nis a question of similar experiences and neural pathways. Nevertheless, explaining why physical pain actually\nsubjectively feels like it does in the first place, is an extremely difficult question; it is intimately related to the\nquestion of consciousness, which we defer to Chapter 14. We shall rather continue, in the next chapter, by\nelucidating the computational underpinnings of a particular form of error signal: frustration.\n26The broadcasting hypothesis is closely related to the global workspace theory by Baars (1997), which will be treated in Chapter 14.\nHowever, while Baars links broadcasting it to consciousness, I think the broadcasting does not have to be conscious, especially in the\ncase of pain or suffering. The hypothesis is also closely related to the earlier interrupt theory of emotions explained in Chapter 10. The\nbroadcasting might happen through several specific connections between brain areas, or through a central hub.\n27Such evolutionary arguments for using the same system were proposed by Eisenberger and Lieberman (2004), see also Papini et al.\n(2015). I am slightly confounding “pathways” and “systems” here: While the existing evidence is mainly about overlapping activation\nof certain brain regions, I am extrapolating the idea to the case of the signalling pathways.\n\nChapter 3\nFrustration due to failed plan\nIn this chapter, I propose the first model where frustration is a fundamental mechanism for suffering. It is\nassumed that an agent, whether a human or an AI, engages in planning of action sequences in order to get to a\ndesired goal state. A state is here an abstraction of the properties such as location, context, and possessions of\nthe agent. Frustration happens when the goal state is not reached in spite of the agent executing the planned\nsequence of actions.\nI start by emphasizing the great computational difficulty of such planning of action; it is one reason why\nfrustration happens. Another central concept here is wanting or desire, which is a complex phenomenon we\nwill return to several times in this book. As an initial definition, I consider desire as a computational process\nthat suggests goals for the planning system. Finally, I discuss the importance of committing to a single plan,\neven in the presence of conflicting desires, based on Bratman’s concept of intention. This chapter lays out\nthe framework in simple, largely intuitive terms; the main terms and concepts will be greatly refined in later\nchapters.\nAgents, states, and goals\nOne may be tempted to think of an artificial intelligence as a system which just takes input, and processes\ninformation. However, information-processing in itself will actually be rather pointless unless it leads to some\nkind of visible output or action regarding the external world. In the very simplest case, action can just mean\nprinting some text on a computer screen, so this is not necessarily a big leap.\nIn AI, the basic unit of analysis is often what is called an intelligent agent, i.e. a system which not only pro-\ncesses information but also takes actions. In fact, the word “agent” literally means “one that acts”. An intelligent\nagent can be artificial, such as a robot or an AI program, but the term also encompasses biological agents, that\nis, animals. In one extreme, an artificial agent could be just a program inside a computer, working in a virtual\nworld with no physical body; actions would essentially consist of sending messages inside an information net-\nwork. In the other extreme of human-like artificial agents, it could be a robot having a body with arms and legs;\nactions would include walking and grasping objects. In this book, we will see examples of both extremes—in\naddition to agents that actually are animals or humans.\nSuch an agent needs at least two things: perception and action selection. Perception is actually a tremen-\ndously difficult task but we defer its discussion to Chapter 4 and especially Chapter 12. To begin with, we as-\nsume perception is somehow satisfactorily performed, and consider the question of how the agent is to choose\n23\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n24\nits actions.1\nPerhaps the simplest and most intuitive approach to action selection is to think in terms of goals. This is an\nintrospectively compelling approach: We usually think of ourselves as acting because there are certain goals\nthat we try to reach. That may be why it has also been a dominant approach in the history of AI, starting from\naround 1960. For example, a very simple thermostat has the goal of keeping room temperature constant; a\ncleaning robot has the goal of removing dust and dirt from the room.\nThe goals of humans are fundamentally determined by evolution, complemented by societal and cultural\ninfluences. In this book, I simply use the word “evolution” to describe the joint effect of biological evolution,\nculture and society. The assumption here is that the latter two are ultimately derived from biological evolution,\nalthough this is of course a controversial point. Fortunately, for the purposes of this book, the exact relationship\nbetween biology and culture is irrelevant: What matters is that the goals that humans strive for are largely, even\nif sometimes very indirectly, determined by some outside forces. Humans can set some intermediate goals,\nsuch as getting a job, but those are usually in the service of final biological or societal goals, such as being\nnourished or raising one’s social status. In the case of AI, in contrast, the goals are usually supplied by its\nhuman designers. This may seem to be a fundamental difference between AI agents and humans, but we will\nsee in later chapters that it may not matter very much; the human designer plays a role similar to evolution\nin terms of being an outside force. In any case, regardless of where the goals come from, the way they are\ntranslated into action may still be rather similar in both cases.\nModelling the world as states\nIn order to choose its actions, the agent should have some kind of a model of how the world works, where the\nagent itself is seen as a part of the “world” modelled. The model expresses the agent’s beliefs of what the world\nis typically like, and how the world changes from one moment to another, in particular as a function of the\nactions the agent takes.\nAI research uses a very abstract kind of a world model based on the concept of a state, where each possible\nconfiguration of the world is one state. For example, if a cleaning robot is in the corner of a room, facing south,\nand there is only a single speck of dust in the room, at exactly two meters east from the robot, that is one state,\nwe can call it state #1. If the agent finds itself 10 cm further to the west, it is in another state, say state #2;\nlikewise, if another speck of dust appears in the room, that means the agent is in state #3 (and if the speck of\ndust appears and the agent is 10 cm further to the west, that is yet another state). In the simplest case, such a\nworld model has states which are categorical, or discrete; in other words, there is a finite number of possible\nstates.2 This is a very classical AI approach, but we will see alternatives in later chapters.\nAny effects of the agent’s actions can now be described in terms of moving from one state to another, called\nstate transitions. Indeed, in addition to knowing what the states of the world are like, the agent should know\nsomething about the transitions between the states caused by its actions. If it finds itself in state #1 and decides\nto move forward, does it find itself in state #2, or #47, or something else? If the agent’s world model can predict\nthe effects of its actions in terms of transitions from one state to another, it is ready to start taking actions.\nUsing this formalism of states, the basic approach to action selection is that one of the states is designated\n1For introductory textbooks on the topic, see Russell and Norvig (2020); Poole and Mackworth (2010).\n2Or, if the number of states is infinite, it is restricted to what is called countably infinite, which is the “smallest” kind of infinity,\nmeaning that the states can be indexed by integers.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n25\nas the goal state, by some mechanisms to be specified. The agent then uses its capabilities to reach the goal\nstate, starting from its current state. That is surprisingly difficult, usually requiring complicated computation\ncalled planning, which is a central concept we consider next.3\nPlanning action sequences, and its great difficulty\nThe fundamental problem in action selection is that you must actually select sequences of short actions. For\nexample, if the agent in question is you, and you decide to get something from the fridge in the kitchen, you\nneed to take a step with your left foot, take a step with your right foot, repeatedly, until you finally can open the\nfridge door—which consists of several actions such as: raise your arm, grab the handle, pull it down, pull the\ndoor, and so on. In some cases, you may easily know how to choose the right sequence, but it is not easy at all\nin many cases. For programming AI, it has turned out to be quite a challenge.\nThis is known as the problem of planning in AI. Using the formalism of world states, action sequences\ncan be represented graphically as what is called a tree (Figure 3.1). The “root” of the tree represents the state\nyou’re in at the moment. Any action leads to a branching of the tree, and depending on the action, you will\nfind yourself on any of the new branches. (In this figure, taking an action means moving down in the tree, and\nwe assume for simplicity that there are just two actions you can take at any time point). At the end of a given\nnumber of actions, or levels in the tree, you find yourselves at one of those states which are depicted at the\nouter “leaves” of the tree. Of course, the tree continues almost forever since you can take new actions all the\ntime, but to keep things manageable, we consider a tree of a limited depth.\nLet’s now assume that the agent has been given a goal state by the programmer. It would be one of the\nstates at the lowest level of the tree. The central concept here is tree search; many classical AI theories see\nintelligence as a search for paths, or action sequences, among a huge number of possible paths in the action\ntree. In particular, the planning system tries to find a path which leads from the current state to the goal state.\nSuch search may look simple, but the problem is that with such paths or action sequences, the number of\npossibilities grows exponentially. If you have, at any single time point, just two different actions to choose\nfrom, then after 30 such time points you have more than a billion (precisely 2 to the power of 30) possible\naction sequences to choose from. What’s worse is that typically an AI would have many more than just two\npossible courses of action at any one point. The computations involved easily go beyond the capacity of even\nthe biggest computers or brains. So, it may be impossible to “look ahead” more than a couple of steps in time.\nThe difficulty of such planning may be difficult for humans to understand since evolution has provided\nvarious tricks and algorithms that solve the problem quite well, as we will see below. We may only be able to\ngrasp the difficulty of planning in some slightly artificial examples such as the search tree above. One of the\nmore realistic examples would be planning a route between two points. Say you find yourself in a random\nlocation in Paris and want to go to the Eiffel Tower using public transportation. Even if you remembered every\n3It may seem very abstract to consider the world in terms of states. More insight might be obtained by taking an object-oriented\nviewpoint and considering the world a collection of objects. However, such a theory is still in its infancy (Diuk et al., 2008; Guestrin\net al., 2003), so we have to use the approach based on world states. Another question is whether the states should really be considered\ndiscrete-valued, such as indexed by integers. In fact, most current AI systems do not use such a categorical representation, but rather\nsome kind of continuous-valued perceptual representation in a neural network, for example given by the outputs of certain neurons.\nHowever, the approach using discrete states is widely used in the theory and the textbooks because of its conceptual simplicity; the\ndistinction is discussed in more detail in Chapters 4 and 8.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n26\nleft\nright\nroot\nFigure 3.1: A search tree where the agent has two action options at every time point. They could be “turn left”\nor “turn right”, supposing the agent always makes a new decision when it finds itself in a new crossroads in\na maze. The squares represent different states the agent can find itself in; the agent starts at the upper-most\nsquare in the figure (called root), and each action takes the agent one level down in this figure. The lines with\narrows are the transitions to new states after every action taken. The crucial point here is that the number of\ndifferent paths or plans it can take grows exponentially. After just 5 steps, as depicted here, the number of paths\nequals 32, that is, 2 to the 5th power. After 30 steps, it would be more than a billion.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n27\ndetail of the metro map as well as the geography of Paris itself, you would still need quite a lot of thinking,\nthat is, computation. Which metro station should I walk to, or should I perhaps use the bus? What is the best\nitinerary once inside the metro station? It is not surprising that people tend to use mobile phone apps to solve\nthis problem.4\nBoard games are an extreme example of the difficulty of planning. Humans playing chess have great dif-\nficulties in thinking more than one or two moves ahead. The search tree has a lot of branches at every move\nbecause there are so many moves you can take. Even worse, your opponent can do many different things. (The\nuncertainty regarding what your opponent will do further adds to the complexity, but that is another story.)\nA lot of the activity we would casually call thinking is actually some kind of planning. If you are thinking\nabout where to go shopping for a new electronic gizmo, or how to reply to a difficult message from your friend,\nyou are considering different courses of action. Basically, you’re going through some of the paths in the search\ntree. Interestingly, a lot of such thinking or planning happens quite involuntarily, even when you’re supposed\nto be doing something else, a topic to which we will return in Chapter 11.\nFrustration as not reaching planned goal\nEquipped with this basic framework for action selection, we are ready to define frustration in its most basic\nform. We start by considering one part of the Buddha’s definition of suffering mentioned above (page 20):\n“not to get what one wants”. This is in fact a typical dictionary definition of frustration. To achieve a deeper\ncomputational understanding of the phenomenon, we need to integrate this with the framework of planning.\nJust like AI, complex organisms such as humans engage in planning: Based on their perception of the cur-\nrent environment, they try to achieve various goals by some kind of tree search. For such organisms, it is vital\nto know if a plan failed, so that they can re-plan their behavior, and even learn to plan better in the future. We\nthus formulate the basic case of frustration as not reaching a goal that one had planned for, and the ensuing\nerror signal.\nThis initial definition will be refined and generalized in later chapters, where we will see how central error\nsignals are to any kind of learning. For example, a neural network that learns to classify inputs, or predict the\nfuture, is essentially minimizing an objective function which gives the error in such classification or prediction.\nFrustration can be seen as a special case of such error signalling: It signals that an action plan failed. In com-\nplex organisms like humans, which are constantly engaged in planning, frustration is an extremely important\nlearning signal, and the basis of a large part of the suffering. It should also be noted that in some contexts, frus-\ntration rather refers to the resulting unpleasant mental state; that is, frustration refers to the actual suffering\ninstead of the cause for suffering. In this book, the word is in both of those meanings.5\n4Planning might actually seem to be very easy in a simple illustration like in Figure 3.1, since all you need is to start at the goal state,\nand go backwards in the search tree until you arrive at the root; thus you have found the path from the root to the goal. The reason\nwhy this does not work in practice is that in reality there are many overlapping trees, each starting from a different root state, and\neach goal state can be reached starting from a number of different roots. So, you cannot go backwards because you don’t know which\ntree to follow. You can see this in the example of planning a route between two points in Paris: It may help a bit to start calculating\nbackwards from Eiffel Tower, but you cannot just backtrack in a tree because the possible routes going “back” from the Eiffel Tower\nare as numerous as the routes you can start going “forward” from your current location; routes computed “backwards” from the Eiffel\nTower can take you anywhere in Paris, not just your current location.\n5This ambiguity is to some extent justified by the ambiguity of how the term is used in the literature, and some dictionaries explicitly\nlist these two meanings for the term, e.g. https://psychologydictionary.org/frustration/.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n28\nDefining desire as a goal-suggesting mechanism\nHowever, there is a slight inconsistency here: Frustration was actually defined as not getting what one wants\nin Chapter 2. How is this related to our computational formulation based on planning above? In other words,\nwhat exactly is wanting, or desire, in a computational framework like ours?\nIn everyday intuitive thinking, action selection is indeed supposed to be based on wanting, or desires: An\nagent takes an action because it wants something, and it thinks it is reasonably likely to achieve or obtain it by\nthat action. I choose to go to the fridge because I want orange juice. However, the account earlier in this chapter\nmade no reference to the concepts of desire or wanting. In AI, the term “desire”, which I consider synonymous\nwith “wanting”, can actually be used in a couple of different meanings.\nIn the very simplest definition, if the agent has a goal to plan for, one could simply say the agent “wants” to\nreach the goal state; desires would essentially be the same as goals. In such a meaning, desire is a kind of purely\nrational, “cold” evaluation of states and objects. However, the word has many more connotations in everyday\nlanguage. Desire also has an affective aspect we could call “hot”, in which we are “burning with desire”, unable\nto resist it.\nA definition that is a bit more in the direction of “hot” can be obtained by considering desire as a specific\ncomputational process inside the agent. To begin with, we can adopt a definition of desire as a “psychological\nstate of motivation for a specific stimulus or experience that is anticipated to be rewarding”.6 While a “psy-\nchological state” may mean different things, here we consider it as a particular kind of information processing\nbeing performed—another meaning would be related to conscious experience which we treat in Chapter 14.\nIn practical terms, desire is often triggered by the perception of something that is rewarding to possess.7 Such\nperception of an object often means that the agent should be able to get the object after a rather short and\nuncomplicated action sequence: If you see something, it is likely to be within reach.\nFrom the viewpoint of information-processing, we thus define desire as: A computational process suggest-\ning as the goal a state that is anticipated to be rewarding and seems sufficiently easily attainable from the current\nstate. I want to emphasize that I am considering desire as a particular form of information-processing: Desire\nis not simply about preferring chocolate to beetroot, nor is it merely an abstract explanation of the behavior\nwhere I grab a chocolate bar. It is sophisticated computation that is one step in the highly complex process that\ntranslates preferences into planning and, finally, into action.\nThe starting point for that processing is that your perceptual system, together with further computations,\nestimates that from the current state, you can relatively easily get into a state of high reward—the exact formal-\nism for “rewards” will be introduced in Chapter 5. This realization will trigger, if you are properly programmed,\nfurther computational processes that will try to get you in that desired state by suggesting it as the goal for your\nplanning system. When all this happens, you want to be in the new state, or have a desire for that new state,\naccording to the definition just given. For example, if chocolate appears in your visual field, your brain will\ncompute that the state where you possess the chocolate is relatively easy to reach, and produces high reward;\nso it will choose the chocolate-possessing state as a possible goal and input that to the planning system.\nTherefore, the definition of desire just given shows how the intuitive definition of frustration as not getting\n6(Papies and Barsalou, 2015). For a number of alternative definitions see (Schroeder, 2017).\n7“[D]esire arises when an internal or external cue triggers a simulation, or partial re-enactment, of an earlier appetitive experience\nthat was rewarding.”(Papies and Barsalou, 2015). I’m using “reward” in this chapter in a non-technical sense, which will be refined in\nChapter 5.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n29\nwhat one wants and the computational definition of not reaching the goal are essentially the same thing. This\ndefinition also solves a question which many readers must have asked while reading this chapter: Where do\nthe goals for planning come from? In a very simple AI, there might be just a single goal, or a small number of\nthem, defined by the programmer. But for a sophisticated agent, that is certainly not the case: The number\nof possible goals for a human agent is almost infinite. Here, we define desire as a computational process that\nsuggests new goals to the planning system, so this is where the goals come from. (More details on how the\ndesire system could actually choose goals will be given in Chapter 8, which also considers a different aspect of\ndesire related to its interrupting and irresistible quality.)\nA closely related concept is aversion, which is in a sense the opposite of desire. However, from a mathemat-\nical viewpoint, aversion is very similar to desire: The agent wants to avoid a certain state (or states) and wants\nto be in some other state.8 For example, the agent wants to be in a state in which some unpleasant object is\nnot present. Thus, it is really a case of wanting and desire, just framed in a more negative way. I do not use the\nterm aversion very much in this book since it is mathematically contained in the concept of desire.9 Whenever\nI use the word “desire”, aversion is understood to be included.\nIntention as commitment to a goal\nWe have seen that a desire is something that suggests the goal of the agent. Note that I’m not saying that\ndesire sets the goal, but it suggests a goal to the planning system. This difference is important because there\nmight be conflicting goals; you don’t grab the chocolate every time you have desire for it. The agent needs\nto choose between different possible objects of desire. This is particularly important because attaining the\ndesired goal state often takes time: The whole plan has to be executed from the beginning till the end, and\nnew temptations—activations of the desire system which suggests new states as possible goals—may arise\nmeanwhile. Some method of arbitrating between different desires is necessary.\nSuppose the desired state for a monkey is where the monkey has eaten a banana. The banana is currently\nhigh up the tree which is in front of the monkey, so the monkey needs to perform a series of actions to reach that\ndesired state: it must climb up the tree, take the banana, peel it, and eat it. The monkey must thus figure out the\nright sequence of actions to reach the desired state— this is just the planning problem discussed above—and\nlaunch its execution.\nBut, suppose the monkey suddenly notices another banana in another tree near-by. Its desire system may\nsuggest that the new banana looks like an interesting goal. The monkey now faces a new problem: Continue\nwith the current banana plan, or set the new banana as a new goal? It may be common sense that after the\nmonkey has launched the first banana plan, the monkey should, in most cases, persist with that plan until the\nend. The monkey should not start pondering, halfway up the first tree, whether it actually prefers to get the\nother banana in the other tree, even if it looks a bit sweeter. The key idea here is commitment to the current\nplan, and thus to a specific goal.\n8A linguistic confusion is created in English and many other languages in which it is commonplace to say “I don’t want X”, where X\nmight be drilling noise in your office, or flies in your bedroom. What this actually means is that you want those things to be absent:\nIt does not simply mean that you merely refrain from wanting that noise of the flies. You want “not X”, the opposite or absence of X,\nwhich is in fact the meaning of aversion.\n9One difference is, though, that since aversion suggests as the goal all the states where the unpleasant object is not present, it\nactually operates with a very large set of goal states.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n30\nThe reason why commitment is important comes fundamentally from computational considerations. Since\ncomputing a plan for a given goal takes a lot of computational resources, it would be wasteful to abandon it\ntoo easily in favor of a new goal. It might be wasteful even to just consider alternative goals seriously, because\nthat would entail a lot of computation to produce alternative plans. The agent must settle on one goal and one\nplan and execute it without spending energy thinking about competing goals.10\nAnother utility of commitment is that the agent has a better idea of what will happen in the future, and\nit can start planning further actions, e.g. a plan on what to do after reaching the goal of the current plan. So,\nwhile the monkey is climbing up the tree, it is a good idea to start thinking about the best way of getting the\nbanana in the other tree after having grabbed the first banana. That would be planning the long-term future\nafter completing the execution of the current plan; perhaps the monkey can directly jump to the other tree\nfrom the location of the first banana. Such long-term plans would obviously collapse if the monkey didn’t first\nget the first banana due to lack of commitment, being distracted by yet another thing.\nCommitment to a goal is also called intention in AI, and leads to an influential AI framework called belief-\ndesire-intention (BDI) theory. “Belief” refers here mainly to the results of perception, which give rise to desires.\nBDI theory argues, as I just did, that it is important to have of intentions as commitment to specific goals, on top\nof beliefs and desires.11 Of course, there must be some limits to such commitment: If something unexpected\nhappens, the goal may need to be changed. If a tiger appears, the monkey cannot persist with the goal of just\neating a banana. Chapter 10 will consider the importance of emotions such as the fear aroused by the tiger as\none computational solution.12\nThe concept of intentions has important implications for suffering, as will be discussed in detail in later\nchapters. To put it simply, I will propose that frustration and suffering are stronger if an intention is frustrated,\nas opposed to frustration of a simple desire as in the basic definition.\n10A lot of physical energy would also be wasted if the monkey is already half-way up the tree and then decides to go for the other\nbanana. But arguably that waste of energy would be taken into account by the monkey in its planning, so it does not need to be evoked\nas a separate reason for commitment. I think we can take here a viewpoint considering purely computational resources: Even if the\nmonkey is intelligent enough to eventually understand this waste of physical energy after some thinking, it would still spend a lot of\ntime and computational resources to reach that conclusion if there were no commitment mechanisms.\n11(Bratman, 1987; Cohen and Levesque, 1990; Rao and Georgeff, 1991). See Mulder (2018); Brodaric and Neuhaus (2020) for recent\nwork and slightly different formulations. For a modern neuroscientific approach see O’Reilly et al. (2014) which proposes something\nvery similar using its concepts of “goal engagement”, and “active goal”. Note that the word “intention” has different meanings in the\nliterature, and in particular this definition of the word is quite different from the meaning typically associated with “intentionality” à la\nBrentano. On the other hand, in the literature, there is some ambiguity on whether intentions are commitments to desires, goals, or\nplans. I consider them as commitments to goals.\n12Some AI systems solve this problem by planning everything from scratch at regular intervals, but that is unlikely to be possible in a\nreal-time environment where the time needed for planning is the main bottleneck. In fact, my treatment here may not do full justice\nto Bratman’s original definition of intention, where a plan is actually composed of several intentions. Such a definition creates more\nflexibility for behavior in the sense that even if the circumstances change (or the circumstances were unpredictable to start with), the\nbehavior may flexibly move from one path to another by triggering an alternative sequence of intentions. The definition I use here is\nmore similar to the later AI developments of the concept, cited in the preceding footnote.\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n31\nHeuristics can help in planning\nStill, we have not yet solved the central problem regarding the planning system. We saw above that because\nof the huge number of possible paths in planning, a complete tree search is quite impossible in most cases.\nIs planning then impossible? Fortunately, there are a couple of tricks and approximations that can be used to\nfind reasonable solutions to the planning problem. Here we first consider what is called heuristics, while a more\nsophisticated solution is given in the next two chapters. These solutions also have important implications for\nthe definition of frustration, and understanding suffering.\nA heuristic means some kind of method for evaluating each state in the search tree, usually by giving a\nnumber that approximately quantifies how good it is, i.e. how close to the goal it is. The point is that a heuristic\ndoes not need to be exact—if it were, we would have already solved the problem. It just gives a useful estimate,\nor at least an educated guess, of how “good” a state is.13\nSometimes, it is quite simple to program some heuristics in an AI agent. Consider a robot whose goal is to\nget some orange juice from the fridge and deliver it to its human master. Clearly, when the robot has orange\njuice in its hand, it is rather close to the goal; we could express that by a numerical value of, say, 8. If it is, in\naddition, close to its master, it is very close to its goal, say a value of 9. The most important thing is, however,\nto assist the robot at the beginning of the search, and that is where the heuristic is the most powerful. So, we\ncould say that when the robot is close to the fridge, the heuristic gives a value of 2. When it has opened the\nfridge, the value is 3, and so on.\nWith such heuristics, the search task would not require that much computation. The robot just has to figure\nout how to get to some easily reachable state with a higher heuristic than the current state. Assuming the robot\nstarts at an initial state with heuristic value 0, it would quickly compute that what it can achieve rather easily\nis a state of heuristic value of 2, by going to the fridge. The length of the tree to be searched for is thus much\nshorter, i.e. much fewer actions steps need to be taken in that subproblem. Once there, it only has to figure\nout how to open the door to get to the state with heuristic value of 3. Thus, the heuristic essentially divides a\nlong complex search task into smaller parts. Each of these parts is quite short, so the exponential growth of the\nnumber of branches is much less severe.14\nThere is one famous success of AI where such tree search with heuristics was hugely successful: The Deep\nBlue chess-playing machine,15 which beat the chess world champion, for the first time, in 1997. Its main\nstrength was the huge number of sequences of moves (i.e. paths in a search tree) it was able to consider, largely\nbecause it was based on purpose-built, highly parallel hardware that was particularly good in such search com-\nputations on the chessboard. But its success was also due to clever heuristics, the main one being called “piece\nplacement”, computed as the sum of the predetermined piece values with adjustments for location, telling how\ngood a certain position is. (In chess, the state is the configuration of all the pieces on the board, and called a\n“position” in their jargon.)\nEvolution has also programmed a multitude of heuristics in animals. Think about the smell of cheese for\n13A very general definition of a heuristic, not only applicable to the tree search problem, is given by Gigerenzer and Gaissmaier (2011):\nA heuristic is a strategy that ignores part of the information, with the goal of making decisions more quickly, frugally, and/or accurately\nthan more complex methods.\n14As a simple (and only approximative) numerical example, think of dividing a tree of length 20 into two parts. Each part of length\n10 has 1,024 = 210 states, so the two search trees have total of 2,048 states. This is much less that the original tree with 220 = 1,048,576\nstates.\n15(Campbell et al., 2002)\n\nCHAPTER 3. FRUSTRATION DUE TO FAILED PLAN\n32\na rat. The stronger the smell, the closer the rat is to the cheese. The rat just needs to maximize the smell, as it\nwere, and it will find the cheese. No complex planning is needed—unless there are obstacles in the way.16\nHowever, the crucial problem is how to find such heuristics for a given planning problem. In fact, this is\na very difficult problem, and there is no general method for designing them. Nevertheless, there is a general\nprinciple which has been found tremendously useful in modern AI, and can be used here as well: learning.\nModern AI is very much about using learning from data as an approach to solving the problem of programming\nintelligence. In the case of planning, it turns out that a general approach for solving the planning problem is\nto learn to rate the states, i.e. learn to associate some kind of heuristic to each world state. This is why in the\nnext two chapters, we delve into the theory of machine learning. Its specific application to solving the planning\nproblem will be considered in Chapter 5, where we also consider a different approach to defining frustration.\n16In fact, we see here that there is some intricate connection with heuristics and desires. When the rat smells the cheese, surely a\ndesire for cheese appears in its system. See Chapter 10, and in particular footnote 26, on how the same computations can sometimes\nbe interpreted as heuristics or desires.\n\nChapter 4\nMachine learning as minimization of errors\nIn this chapter, we will go through some of the basics of the backbone of modern AI: machine learning. Such AI\ncrucially relies on learning from incoming data—which is also true of the brain. Machine learning is most often\nused in conjunction with neural networks, which are powerful function approximators, loosely mimicking how\ncomputations happen in the brain. We will also consider an alternative, older approach to intelligence based\non symbols, logic, and language, which is now called “good old-fashioned AI”. (The preceding chapter with its\ndiscrete, finite states, was an example of this latter approach.)\nA central message in this chapter is that learning is often based on some measure of error. Minimizing\nsuch errors means optimizing the performance of the system. The fundamental importance of computing\nand signalling such errors is important in future chapters where such errors are directly linked to suffering,\ngeneralizing the concept of frustration. I conclude this chapter by claiming that any kind of learning from\ncomplex data can lead to quite unexpected results, something that the programmer could not anticipate.\nNeurons and neural networks\nModern AI is based on the observation that the human brain is the only “device” we know to be intelligent for\nsure and without any controversy. It is actually not easy to define what “intelligence” means, and I will not\nattempt to do that in this book.1 Yet, nobody denies that the brain is intelligent—or, to put it another way, it\nenables us to behave in an intelligent way. The brain is intelligent as if by definition; it is the very standard-\nbearer of intelligence. Therefore, if you want to build an intelligent machine, it makes sense to try to mimic the\nprocessing taking place in the brain.\nNeurons as tiny processors\nThe computation in the brain is done by specialized cells called neural cells or neurons.2 A schematic picture\nof a neuron is in Fig. 4.1. A neuron receives input from other neurons, processes that input, and outputs the\n1For standard textbook expositions on the definition of (artificial) intelligence, see e.g. Russell and Norvig (2020); Poole and Mack-\nworth (2010). For particular viewpoints relevant to our discussions later, see e.g. Brooks (1991); Legg and Hutter (2007).\n2It has also been claimed that other types of cells in the brain could also participate in computations, in particular glial cells (Perea\net al., 2009). However, AI systems typically mimic neurons only.\n33\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n34\nComputation\nOutput\nSignal\nSignals\nInput\nFigure 4.1: A schematic of a neuron. Input signals coming from other neurons (from the left) are received by\nthe neuron (depicted by the black disk). Computation happens inside the neuron, and the resulting output\nsignal is transmitted to a number of other neurons (depicted by white disks) on the right-hand side. The other\nneurons simultaneously receive input signals from many further neurons outside of this figure (depicted by\nfurther arrows).\nresults of its computations to many other neurons. There are tens of billions of neurons in the human brain.\nEach single neuron can be seen as a simple information-processing unit, or a processor.3\nAll these tiny processors do their computations simultaneously, which is called parallel processing. The\nopposite of parallel processing is serial processing, where a single processor does various computational op-\nerations one after another—this is how ordinary CPU’s in computers work. Another major difference between\nthe brain and ordinary computers is that processing in neurons is also distributed. This means that each neu-\nron processes information quite separately from the others: It gets its own input and sends its own output to\nother neurons, without sharing any memory or similar resources. Compared to an ordinary PC, the brain is\nthus a massively parallel and distributed computer. Instead of a couple of highly sophisticated and powerful\nprocessors as found in a PC, the brain has a massive amount—billions—of very simple processors. (Parallel\nand distributed processing is discussed in detail in Chapter 6.)\nWhile the actual neurons are surprisingly complex, in AI, a highly simplified model of a real neuron is used.\nSometimes, such a model is called an artificial neuron to distinguish it from the real thing, but for simplicity,\nwe call them just neurons. Like a real neuron, an artificial neuron gets input signals from other neurons, but\neach such input signal is very simple, just a single number; we can think of it as being between zero and one,\nlike a percentage. Based on those inputs, the neuron computes its output which is, again, a single number.\nThis output is, in its turn, input to many other neurons.\nIn such a simple model, the essential thing is to devise a simple mathematical formula for computing\nthe output of the cell as a function of the inputs. In typical models, the output is computed, essentially, as\na weighted sum of the inputs. The weights used in that sum are interpreted, in the biological analogy, as the\nstrengths of the connections between neurons, or the incoming “wires” on the left-hand-side of Fig. 4.1. These\n3I do not attempt to define “information processing” in any rigorous way in this book. It is a very general concept with many\nmeanings, and attempting to define it in a way that is both general and rigorous enough seems hopeless to me. I use “computation”\nsimply as a synonym for information processing.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n35\nFigure 4.2: Synaptic weights of a neuron illustrated. Pixels shown in black have a connection strength of −1\nto the neuron (depicted in blue), while pixels shown in white have a connection strength of +1. The neuron is\nmaximally activated when the input corresponds to the stored pattern, which is a picture of the digit “2”.\nweights can get either positive or negative values: The weight is defined as zero for those neurons from which\nno input is received. The weighted sum is usually further thresholded (i.e. passed through a nonlinear function)\nso that the output is forced to be between zero and one. In the brain, the connections are implemented through\nsmall communication channels called synapses, which is why the weights can also be called “synaptic”.\nImportantly, these weights can be interpreted as a pattern, or a template, which the neuron is sensitive to.\nThus, a neuron can be seen as a very simple pattern-matching unit. The neuron gives a large output if the\npattern of all the input signals matches the pattern stored in the vector of weights or connection strengths.\nAs an illustration, consider a neuron that has a weight with the numerical value +1 for inputs from another\nneuron, let’s call it neuron A, as well as a zero weight from neuron B, and a weight of -1 for inputs from neuron\nC. This neuron will give a strong output signal when neuron A gives a large output signal, and the neuron C\ngives a small signal, while it does not care what the output of neuron B might be.\nSuch pattern-matching is obviously most useful in processing sensory input, such as images. Consider a\nneuron whose inputs come from single pixels in an image. That is, the input consists of the numerical values\nof each pixel, telling how bright it is, i.e. whether it is white, black, or some sort of gray. Then, we can plot the\nsynaptic weights as an image, so that the gray-scale value in each pixel in this plot is given by the corresponding\nsynaptic weights. If they are -1 or +1 as in the previous example, we can plot those values as black and white,\nrespectively. A neuron could have synaptic weights as in Fig 4.2. Clearly, this neuron is specialized for detecting\na digit, in particular number two.\nOf course, in reality, to recognize digits (or anything else) in real images, things are much more complicated.\nFor one thing, the pattern to be recognized could be in a different location. If the digit is moved just one pixel\nto the right or to the left, the simple pattern-matching above does not work anymore, and the neuron will\nnot recognize the digit. Likewise, if the digit were white on a black background instead of black on a white\nbackground, the same pattern-matching would not work. To solve these problems, we need something more\nsophisticated.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n36\nNetworks based on successive pattern-matching\nBuilding a neural network greatly enhances the capabilities of such an AI, and solves the problems just men-\ntioned. A neural network is literally a network consisting of many neurons. Networks can take many different\nforms, but the most typical one is a hierarchical one, where neurons are organized into layers, each of which\ncontains several cells, actually quite a few sometimes. The incoming input first goes to the cells in the first layer\nwhich compute their outputs and send them to neurons in the second layer, and so on. This is illustrated in\nFigure 4.3.\nFrom the viewpoint of pattern-matching, we can say that such a network performs successive and parallel\npattern-matching. The input is first matched to all the patterns stored in the first-layer neurons, and those\nneurons then output the degrees to which the input matched their stored patterns or templates. These outputs\nare sent to the next layer, whose neurons then compare the pattern of first-layer activities to their templates.\nSo, the second-layer patterns are not patterns of original input (such as the pixels of an image) but patterns of\nthe first-layer activities, which form a description of the input on a slightly more abstract level. This goes on\nlayer by layer, so that each neuron in each layer is “looking for” a particular kind of pattern in the activities of\nthe neurons in the previous layer. The patterns are always stored in the synaptic weights of the neurons.\nThe utility of such a network structure is that it enables much more powerful computation. For example,\nconsider the problem of a digit which could be in slightly different locations in the image, as mentioned above.\nThe problem of different locations can be fixed by having several neurons in the first layer, each of which\nmatches the digit in one possible location. All we need in the second layer is a neuron that adds the inputs\nof all first-layer neurons, and thus computes if any of them finds a match. With such a scheme, the second-\nlayer neuron is able to see if there is a digit “2” at any location in the image.\nFinding the right function by learning\nNow, a crucial question is how the synaptic connection weights can be set to useful values. In modern AI,\nthe synaptic weights between neurons are learned from data, hence the term machine learning. Learning is\nreally the core principle in most modern AI. Especially in the case of neural networks, it is actually difficult\nto imagine any alternative. How could a human programmer possibly understand what kind of strengths are\nneeded between the different neurons? In some cases, it might be possible: in image processing, the first one\nor two layers do have rather simple intuitive interpretations, as we have alluded to above. However, with many\nlayers—and neural networks can have thousands of them— the task seems quite impossible. Hardly anybody\nhas seriously tried to design such neural networks by fixing the weights manually, based either on some theory\nor intuition.\nIn the brain, the situation is quite similar. There is simply not enough information in the genome—which\nis somewhat analogous to the programmer here—to specify what the synaptic connection strengths should\nbe for all the neurons. It would hardly be optimal anyway to let the genes completely determine the synaptic\nconnections, since animals live in environments that may change from one generation to another, and some\nindividual adaptation to circumstances is clearly useful. What happens instead is that the synaptic weights\nchange as a function of the input and the output of the neuron, or as a function of perceptions and actions of\nthe organism. The capability of the brain to undergo such changes is called “plasticity”, and those changes are\nthe biophysical mechanism underlying most of learning in humans or animals.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n37\nINPUT\nINPUT\nINPUT\nINPUT\nINPUT\nOUTPUT\nFigure 4.3: An illustration of a neural network. The information enters the system in the first “layer” of black\nneurons on the left-hand side. It is processed by several successive layers, each having five neurons illus-\ntrated by small black disks. Each neuron is doing a simple pattern-matching computation on its inputs, and\ntransmitting the result of that computation to the next layer to its right, along the wires depicted in blue. The\ninformation is transmitted from the left (input) to the right (output). As a result of many neurons (in reality,\nthousands or even millions), the total computation of the network is highly complex and can achieve sophisti-\ncated object recognition, as well as many other kinds of computations.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n38\n2\n0\n2\n4\n6\n2\n0\n2\n4\n6\nFigure 4.4: A simple illustration of what kind of a function a single neuron can learn in the basic case of clas-\nsification with two classes. Each object (e.g. image of an animal) can be considered as a point in a very high-\ndimensional space where the coordinates correspond to pixel values, for example. For the purposes of this\nillustration, we assume there are only two input variables, so we can plot the points on a 2D plane. We also as-\nsume there are only two classes (something like “cats” and “dogs”) which correspond to black and blue points,\nrespectively. In the ideal case, the neuron will learn to output a “one” when the input is in one of the classes,\nand a “zero” when it is in the other class. Such learning corresponds to learning the line that separates the two\nclasses, drawn here as red. Finding a line that separates the classes is clearly possible based on this data, and\nyou have probably done that automatically in your head while looking at this figure. Such learning can be done\nby a single artificial neuron due to the great simplicity of this illustration, but in reality, we would often need a\nneural network with many neurons and layers.\nHow such changes precisely happen in the brain is an immensely complex issue, and we understand only\nsome basic mechanisms. Nevertheless, in AI, a number of relatively simple and very useful learning algorithms\nhave been developed. Neural networks using them learn to perform basic “intelligent” tasks such as recogniz-\ning patterns (is it a cat or a dog?) or predicting the future (if I turn left at the next intersection, what will I see?).\nLearning in a neural network in such a case is based on learning a mapping, or function, from input data to\noutput data. Let’s first consider a single neuron. It can basically learn to solve simple classification problems,\nas illustrated in Figure 4.4. If the classes are nicely separated in the input space, a single neuron can learn, as\nits synaptic weights, the pattern that precisely describes the difference between the classes.\nHowever, a network with many neurons can learn to represent much more complex functions from input\nto output. The input data could be photographs and the output data could be a word describing the main\ncontent of the photograph (“cat”, “dog”, or “unicorn”). The learning of the input-output mapping then consists\nof changing the synaptic weights of all the neurons in all the layers. In successive layers, the network learns to\nperform increasingly sophisticated and abstract computations, consisting of matching the inputs successively\nto the templates given by the weight vectors in each layer. After successful learning of the right mapping, you\ncan input a photograph to the network, and the output of the neural network will give its estimate of what the\nphotograph depicts.\nThere is an infinite number of different ways you can use a neural network by just defining the inputs and\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n39\noutputs in different ways. If you want it to learn to predict future stock prices, the input data would be the\npast prices and the output, current stock prices. You can create a recommendation system that recommends\nnew products to people in online shopping by defining the inputs to be some personal information of the cus-\ntomers, and the output whether the customer bought a certain item or not.4 In a rather unsavoury application,\nthe inputs would be what a social media user likes, and the output some sensitive personal information (say,\nsexual orientation), and then you can predict that sensitive information for anybody. Whether the prediction\nis accurate is another question, of course.\nHere, we see one main limitation of machine learning: the availability of data. Where do you get the sen-\nsitive personal information of social media users in the first place, i.e. where do you get the data to train your\nnetwork? Maybe nobody wants to give you such sensitive data. In other cases, the data may be very expen-\nsive to collect; for example, in a medical application, useful measurements and their analyses may cost a lot of\nmoney. Finding suitable data is a major limiting factor in neural network training; this is a theme we will come\nback to many times. Learning needs data, obviously; but it also needs the right kind of data, and enough of it.\nLearning as incremental minimization of errors\nAfter we have somehow found enough good data, we need to define how to actually perform the learning. Most\noften, the learning is based on formulating some kind of error, and the network then tries to minimize it by an\nalgorithm. The error is a function of the data, i.e. something that can be computed based on the data at our\ndisposal, and tells us something about how well the system is performing.\nSuppose the data we have consists of a large number of photographs and the associated categories (cat/dog\netc.). To recognize patterns in the images, the network could learn by minimizing the percentage of input im-\nages classified incorrectly, which is called classification error. Alternatively, suppose we want to learn to predict\nhow an agent’s actions change the world—say, how activation of an artificial muscle changes the position of\nthe arm of a robot. In that case, what should be minimized is prediction error: the magnitude of the difference\nbetween the predicted result of the action and the true result of the action (which can be observed after the\naction). Such errors don’t usually go to zero, i.e. some error will be left even after a lot of learning. This is due\nto the uncertain and uncontrollable nature of the world and an agent’s actions; that is another theme we will\ndiscuss in detail in later chapters.\nAfter having chosen what kind of errors to minimize, we need an algorithm to actually minimize the errors.\nWhat most such algorithms have in common is that they learn by making tiny changes in the weights of the\nnetworks. This is because optimizing an error function, such as classification error, is actually a very difficult\ncomputational task: There is usually no formula available to compute the best values for the weight vectors.\nIn contrast, what is usually possible is to obtain a mathematical formula that gives the direction in which the\nweights should be changed to make the error function decrease the fastest. That direction is given by what is\ncalled the gradient, which is a generalization of the derivative in basic calculus.\nSo, you can optimize the error function step by step as follows. You start by assigning some random values\nto the weight vectors. Given those values, you can compute the gradient, and then take a small step in that\ndirection (i.e. move the weight vector a bit in that direction), which should reduce the error function, such as\nprediction error. But you have to repeat that many times, often thousands or even millions, always computing\n4(Davidson et al., 2010)\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n40\nthe gradient for the new weight values obtained at the previous step. (The direction of the gradient is different\nat every step unless the error function is extremely simple.) Such an algorithm is called iterative (repeating)\nbecause it is based on repeating the same kind of operations many times, always feeding in the results of the\nprevious computation step to the next step. Using an iterative algorithm may not sound like a very efficient\nway of learning, but usually an iterative gradient algorithm is the only thing we are able to design and program.\nSuch an algorithm is a bit like somebody giving you instructions when you’re parking your car and cannot\nsee the right spot precisely enough. They will only give instructions which are valid for a small displacement.\nWhen they say “Back”, that means you need to back the car a little bit, and then follow some new instructions.\nThis is essentially an iterative algorithm, where you get instructions for the direction of a small displacement,\nand they are different at every time step.\nNeural networks use an even more strongly iterative method, based on computing the gradient for just a\nfew data points at a time. A data point is one instance of the input-output relation data, for example, a single\nphotograph and its category. In principle, a proper gradient method would look at all the data at its disposal,\nand push the weights a small step in the direction that improves the error function, say the classification error,\nfor the data set as a whole. However, if we have a really big data set, say millions of images, it may be too\nslow to compute gradient for all of them, since that would entail going through all the data points. What the\nalgorithms usually do is to take a small number of data points and compute the gradient only for those. That is,\nyou just take a hundred photographs, say, and compute the gradient, i.e. in which direction the weights should\nbe moved to make the classification accuracy better, for those particular images. Importantly, at every step you\nrandomly select a new set of a hundred images, and do the same thing for those images.\nThe point is that you are still on average moving the weights in the right direction, so this is not much worse\nthan computing the real gradient. But crucially, you can take steps much more quickly, since the computation\nof the gradient is much faster for the smaller data set. It turns out that in practice, the benefit of taking more\nsteps often overwhelms the slight disadvantage of having just an approximation of the gradient.5\nPutting these two ideas together, we get what is called the stochastic gradient descent algorithm. Here,\n“descent” refers to the fact that we want to minimize an error. “Stochastic” means “random”, and refers to\nthe fact that you are computing the gradient for randomly chosen data points, so you are going in the right\ndirection only on average. Ultimately, the agent could take a gradient step for each single data point that its\nsensory systems receive, leading to what is called incremental learning.\nSuppose you’re in an unfamiliar city and you need to get to the railway station. Your “error function” is the\ndistance from the station. You can ask a passer-by which direction the station is, and you get something analo-\ngous to the gradient for one data point. Now, of course, that direction given by the passer-by is not certain, she\ncould very well be mistaken; maybe she even said she is not quite sure about the direction. But you probably\nprefer to walk a bit in that direction, and then ask another passer-by. This is like stochastic gradient descent,\nwhere you follow an approximation of the gradient, given by each single data point. The opposite would be\nthat instead of following each person’s advice one after the other, you first ask everybody you see on the street\n5To this advantage we have to add the more technical one that stochastic methods include an implicit regularization and are thus\nless likely to overfit the data (Bottou, 2003; Hardt et al., 2016). Overfitting is an important problem in practical AI learning, but I don’t\ndiscuss it at any length in this book. Basically, it means that if the amount of data at your disposal is very limited, learning may go\nwrong in a particular way: The learning may seem to work well for the data you have, achieving a good “fit”, but the predictions your\nneural network gives are actually useless, because the learning “overfit” your data and does not work (or give a good fit) for any new\ndata on which you would like to apply the system in the future.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n41\nwhere they think the station is, and move in the average of the directions they are giving. Sure, you would get a\nvery precise idea of what the right direction is, but you would advance very slowly—this is analogous to using\nthe full, non-stochastic gradient.\nGradient optimization vs. evolution\nThere are many more ways of optimizing an error function, and many systems that can be conceptualized\nas the optimization of a function. In particular, evolution is a process where the error function called fitness\nis optimized. Fitness is basically the same as reproductive success, which can be quantified as the expected\nnumber of offspring of an organism.6 Fitness is, of course, maximized, while errors in AI are minimized. How-\never, this difference is completely insignificant on the level of the optimization algorithms, since maximization\nof a function is the same as minimizing the negative of that function. Thus, evolution can equally well be seen\nas minimization of the negative fitness, which is then analogous to an error function.\nIn general, such a function to the optimized—whether minimized or maximized—is called an “objective\nfunction”. The objective function does not necessarily have to be any kind of a measure of an error, although\nin AI, it often is. Note that the objective function is completely different from the function from the input to\nthe output that the neural network is computing, which was described earlier. The objective function is what\nenables the system to learn the best possible input-output function, so it works on a different level.\nOptimization in evolution works, of course, in a rather different way than stochastic gradient descent. But\nit is actually possible to mimic evolution in AI and use what is called evolution strategies, evolutionary algo-\nrithms, or genetic algorithms. These are iterative algorithms that are sometimes quite competitive with gra-\ndient methods. They can optimize any function, which does not need to have anything to do with biological\nfitness. For example, we can learn the weights in a neural network by such methods. The idea is to optimize\nthe given error function by having a “population” of points in the weight space, which is like a population of\nindividual organisms in evolution.\nLike real evolution, such algorithms are based on two steps. First, new “offspring” is generated for each\nexisting “organism”. In the simplest case, you randomly choose some new weight values close to the current\nweight values of each organism, which is a bit like asexual reproduction in bacteria, with some mutations to\ncreate variability. Then, you evaluate each of those new organisms by computing the value of the error (such\nas classification error) for their values for weights. Finally, you consider the value of the error as an analogue\nof fitness in biological evolution, albeit with the opposite sign because fitness is to be maximized while an\nerror function is to be minimized. What this means is that you let those organisms (or weight values) with the\nsmallest values of the error “survive”, i.e. you keep those weight values in memory and discard those weight\n6While the idea of evolution as fitness maximization can be found in many textbooks, some biologists would refute the whole idea\nof evolution optimizing any single function; see a recent review by Birch (2016). To some extent, this controversy may also have arisen\nbecause of some semantic confusion about whether that would mean that evolution has already optimized the function (which would\nbe a very strong statement) or whether it is in the process of optimizing it (which may be more plausible); see Parker and Smith (1990).\nRegarding the precise definition of fitness, it seems impossible to find a consensus opinion (Rosenberg and Bouchard, 2011; Grafen,\n2008). A standard textbook definition would be along the lines “expected number of offspring”, but this may have to be complemented\nby the concept of inclusive fitness treated in Chapter 6. — In this book, I tend to anthropomorphize evolution rather unashamedly,\noften comparing it to a human programmer. I believe that is a useful pedagogical device since humans find it easier to think of natural\nphenomena in terms of agents that have goals instead of a more abstract description such as a dynamical system. See e.g. Dawkins\n(1986) for a rather strictly anti-anthropomorphic view on evolution.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n42\nvalues which have larger (that is, worse) values of the error function. You also discard the organisms of the\nprevious iteration or “generation”, since those individual organisms are already dead in the biological analogy.\nSuch an evolutionary algorithm will find new weight values which are increasingly better because only the\norganisms with the best weight values survive in each iteration. Thus, it is an iterative algorithm that optimizes\nthe error function. It is a randomized algorithm, like stochastic gradient descent, in the sense that it randomly\nprobes new points in the weight space. In fact, an evolutionary algorithm is much more random than stochastic\ngradient descent, since gradient methods use information about the shape of the error function to find the\nbest direction to move to, while evolutionary methods have no such information. This is a disadvantage of\nevolutionary algorithms, but on the other hand, one step in an evolutionary algorithm can be much faster to\ncompute since you don’t need to compute the gradient, just random variations of existing weights.7\nSo, we see that both evolution and machine learning are optimizing objective functions. The optimization\nalgorithms are often quite different, but they need not be. One important difference is that in machine learn-\ning, the programmer knows the error function, and explicitly tells the agent to minimize it. In real biological\nevolution, fitness is an extremely complicated function of the environment; it cannot be computed by anybody,\nnor can its gradient. Real biological fitness can only be observed afterwards, by looking at who survived in the\nreal environment, and even then you only get a rough idea of the values of fitness of the individual organisms\nconcerned: Those who die without offspring probably had a low fitness, but it is all quite random—even more\nthan stochastic gradient descent. (If an organism were actually able to compute the gradient of its fitness, that\nwould give it a huge evolutionary advantage.) Another important difference is that in biology, evolution works\non a very long time scale, over generations, while in AI, the learning in the neural networks happens typically\ninside an individual’s life span. The evolutionary algorithms in AI typically learn within an individual agent’s\nlifespan as well, only simulating “offspring” of a neural network in its processors.\nLearning associations by Hebbian rule\nSo far, we have seen learning as finding a good mapping from input to output. Such learning is called supervised\nbecause there is, metaphorically speaking, a “supervisor” that tells the network what the right output is for each\ninput. Yet, sometimes it is not known what the output of a neural network should be, or whether there is any\npoint at all in talking about separate input and output—especially so if we consider the brain. In such a case,\nlearning needs to be based on a completely different principle, typically the principle of unsupervised learning.\nIn unsupervised learning, the learning system does not know anything about any desired output (such as the\ncategory of an input photo). Instead, it will try to learn some regularities in the input data.\nThe most basic form of unsupervised learning is learning associations between different input items. In a\nneural network, they are represented as connections between the neurons representing those two items. For\nexample, if you have one neuron representing “dog” and another neuron representing “barking”, it is reason-\nable that there should be a strong association between them.\nOne theory of how such basic unsupervised learning happens in the brain is called Hebbian learning. Don-\nald Hebb proposed in 1949 that when neuron A repeatedly and persistently takes part in activating neuron B,\n7For a basic genetic algorithm, see for example (Such et al., 2017) and the references therein. A particular advantage of evolutionary\nmethods is that often they can be very efficiently parallelized; parallelization is explained in Chapter 13. They can also be combined\nwith gradient methods by using a stochastic gradient descent to generate the offspring (Salimans et al., 2017).\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n43\nsome growth process takes place in one or both neurons such that A’s efficiency in activating B is increased.8\nSuch Hebbian learning is fundamentally about learning associations between objects or events. A very simple\nexpression of the Hebbian idea is that “cells that fire together, wire together”, where “firing” is a neurobiological\nexpression for activation of a neuron. In this formulation, Hebbian learning is essentially analyzing statistical\ncorrelations between the activities of different neurons.9\nOne thing which clearly has to be added to the original Hebbian mechanism is some kind of forgetting\nmechanism. It would be rather implausible that learning would only increase the connections between neu-\nrons. Surely, to compensate, there must be a mechanism for decreasing the strengths of some connections\nas well. Usually, it is assumed that if two cells are not activated together for some time, their connection is\nweakened, as a kind of negative version of Hebb’s idea.10\nHebbian learning has been widely used in AI, and it has turned out to be a highly versatile tool. You can\nbuild many different kinds of Hebbian learning, depending on how the inputs are presented to the system\nand on the mathematical details of how much the synaptic strengths are changed as a function of the firing\nrates. You can also derive Hebbian learning as a stochastic gradient descent for some specially crafted error\nfunctions.11\nLogic and symbols as an alternative approach\nThe inspiration for neural networks is that they imitate the computations in the brain. Since the brain is capa-\nble of amazing things, that sounds like a good idea. But historically, before neural networks, the initial approach\nto AI was quite different. It was actually more like the world of planning we saw in Chapter 3, where the world\nstates are discrete, and there are few if any continuous-valued numerical quantities.\nIn early AI, it was thought that logic is the very highest form of intelligence, and therefore, AI should be\nbased on logic. Also, the principles of logic are well-known and clearly defined, based on hundreds of years\nof mathematics and philosophy, so they should provide, it was thought, a solid basis on which to build AI.\nIn modern AI, such logic-based AI is not very widely used, but it is making a come-back: It is increasingly\nappreciated that intelligence is, at its best, a combination of neural networks and logic-based AI—now called\n“good old-fashioned” AI, or GOFAI for short. Such logic provides a form of intelligence that is in many ways\ncompletely different from neural network computations, as we will see next.\nBinary logic vs continuous values\nMathematical logic is based on manipulating statements which are connected by operators such as AND and\nOR. For example, a robot might be given information in the form of a statement that “the juice has orange color\n8Adapted from (Hebb, 1949, p. 62). I have simplified the original quote, stripping it from its neurobiological terminology. The\noriginal formulation is “When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing\nit, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is\nincreased.”\n9This is perhaps oversimplifying the original idea: Recent research in neuroscience has emphasized that, as in the original definition\nabove, it is important that cell A participates in the activation, i.e. is has a causal influence of cell B. This would usually mean that cell A\nis activated before cell B (Markram et al., 2012). However, in most implementations of Hebbian learning in AI, such causal and temporal\naspects are not used. Actually, the details of how Hebbian learning works in the brain are not very well understood.\n10(Oja, 1982; Zenke et al., 2017)\n11(Oja, 1992; Hyvärinen and Oja, 1998)\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n44\nAND the juice is in the fridge”. Any statement can also be made negative by the NOT operator. An important\nassumption is such systems, in their classical form, is that any statement is either true or false; no other alter-\nnatives are allowed. This goes back to Aristotle and is often called the law of the “excluded third”. That is, truth\nvalues are binary (can have only two different values).\nSuch logic is perfectly in line with the basic architecture of a typical computer. Current computers operate\non just zeros and ones, and those zeros and ones can be interpreted as truth values: Zero is false and one is\ntrue. Such computers are also called “digital”, meaning that they process only a limited number of values, in\nthis case just two. Our basic planning system in the preceding chapter, with its finite number of states of the\nworld, was an example of building AI with such a discrete approach, and planning is fundamentally based on\nlogical operations.\nThe brain, in contrast, computes with quantities which are in “analog” form, which means continuous-\nvalued numbers that can take a potentially infinite number of possible values. Artificial neural networks do\nexactly the same, as they are trying to mimic even this aspect of the brain. It is rather unnatural for the brain to\nmanipulate binary data or to perform logical operations; such operations are possible only due to some very\ncomplex brain processes which we do not completely understand at the moment.\nThis distinction between digital and analog information-processing is another important difference be-\ntween ordinary computers on the one hand, and the real brain or its imitation by neural networks on the other.\n(Earlier we saw the distinction between parallel and distributed processing in the brain versus the serial pro-\ncessing in an ordinary computer.) The digital nature of ordinary computers implies that any data that you input\nhas to be converted to zeros and ones. This is actually a bit of a problem because a lot of data in the real world\ndoes not really consist of zeros and ones. For example, images are really intensities of light at different wave-\nlengths, measured in a physical unit called “lux”. One pixel in an image might have an intensity of 1,536 lux and\nanother 5,846 lux. It is, again, rather unnatural to represent such numbers using bits, which is why processing\nnon-binary data such as images is relatively slow in modern computers, compared to binary operations.12\nCategories and symbols\nSaying that things are either true or false is related to thinking in terms of categories. Human thinking is largely\nbased on using categories: We divide all the perceptual input—things that we see, hear, etc.—into classes with\nlittle overlap. Say, you divide all the animals in your world into categories such as cats, dogs, tigers, elephants,\nand so on, so that each animal belongs to one category—and usually just one. Then, you can start talking about\nthe animals in terms of true and false. You can make a statement such as “Scooby is a dog”, and that is either\ntrue or false based on whether you included that particular animal in the dog category; any other (third) option\nis excluded.\nCategories are usually referred to by symbols, which in AI are the equivalent of words in a human language.\nFor example, we have a category referred to by the word “cat”, which includes certain “animals” (that’s another\ncategory, actually, but on a different level). Ideally, we have a single word that precisely corresponds to each\n12I shall just briefly mention another crucial difference between brains and ordinary computers: An ordinary computer has hardware\nand software, and these two are separate. The same hardware can run different kinds of software, and the same software can be used\non different hardwares. In fact, you can take software from one computer and download it to another, similar computer and it will work\non that new computer as well. However, in the brain, it is difficult to see any clear distinction between the software and the hardware:\nnobody ever downloaded software into their brain. Such a division between hardware and software is part of what is called the von\nNeumann architecture, named after the great mathematician John von Neumann.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n45\nsingle category, like the words “cat” and “animal” above. Such symbols are obviously quite arbitrary since in\ndifferent languages the words are quite different for the same category. An AI system might actually just use a\nnumber to denote each category.\nWe see that logic-based processing goes hand-in-hand with using categories, which in its turn leads to what\nis sometimes called symbolic AI. These are all different aspects of the GOFAI.\nFrom hand-coded logic to learning\nHistorically, one promise of GOFAI was to help in medical diagnosis, where the programs were often called\n“expert systems”. This sounds like a case where categories must be useful since medical science uses various\ncategories referring to symptoms (“cough”, “lower back pain”) as well as diagnoses (“flu”, “slipped disk”).\nThe basic approach was that a programmer asks a medical expert how a medical diagnosis is made, and\nthen simply writes a program that performs the same diagnosis, or makes the same “decisions” in the technical\njargon. For example, one decision-making process by the human expert might be translated into a formula\nsuch as\nIF cough AND nasal congestion AND NOT high fever THEN diagnosis is common cold\nHowever, this research line soon ran into major trouble. The main problem was that medical doctors, and\nindeed most human experts in any domain, are not able to verbally express the rules they use for decision-\nmaking with enough precision. This is rather surprising since we are operating with human language and well-\nknown categories. The situation is different from neural networks where it is intuitively clear that no expert can\ndirectly tell what the synaptic weights should be, because their workings are so complex and counterintuitive.\nYet, it turned out that even medical diagnoses are often based on intuitive recognition of patterns in the data,\nwhich is a form of tacit knowledge. Tacit knowledge means knowledge, or skills, which cannot be verbally\nexpressed and communicated to others.\nA major advance in such early AI was to understand that expert systems should actually learn the decision\nrules based on data. Again, learning provides a route to intelligence that is more feasible than trying to directly\nprogram an intelligent system. Given a database with symptoms of patients together with their diagnoses given\nby human experts, a machine learning system can learn to make diagnoses. Such learning is not so fundamen-\ntally different from learning by neural networks. What is different is that the data is categorical (“cough”, “no\ncough”), and the functions are computed in a different way, for example by combining logical operations such\nas AND, OR, and NOT.\nCategorization and neural networks\nBy definition, such logic-based AI can only learn to deal with data which is given as well-defined categories. Yet,\nreal data is often given as numbers instead of categories; even medical input variables often include numerical\ndata in the form of lab test results. In this medical diagnosis, we have indeed a category called “high fever”. The\nsystem is effectively dividing the set of possible body temperatures into at least two categories, one of which is\n“high fever”. How are such categories to be defined? What is fever? What is low fever and what is high fever?\nHere we see a deep problem concerning how categories should be defined based on numerical data, such as\nsensory inputs.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n46\nAgain, some progress can be made by learning, this time learning the categories themselves from data.\nThe AI can consider a huge number of possible categorizations of body temperatures: It can try setting the\nthreshold for high fever at any possible value. If there is enough data on previous diagnoses by human doctors,\nthe system can use that to learn the best threshold. In fact, the right threshold could be found as the one that\nminimizes classification error, i.e. the number of wrong diagnoses.\nHowever, while it is possible to learn categories in such very simple numerical data, GOFAI has great diffi-\nculties in processing complex numerical data. It is virtually impossible to use it to process high-dimensional\nsensory input, such as images consisting of millions of pixels. This was a rather big surprise for GOFAI re-\nsearchers in the 1970s and 80s. After all, categorization of visual input is done so effortlessly by the human\nbrain that it may seem to be easy. Yet, AI researchers working in the GOFAI paradigm found it to be next\nto impossible. The early research on GOFAI was fundamentally over-ambitious, grossly underestimating the\ncomplexity of the world, as well as the complexity of the brain processes we use to perceive and make decisions.\nOne reason for the current popularity of neural networks is that processing high-dimensional sensory data\nis precisely what they are good at. As we have seen, neural networks operate in a completely different regime\nfrom such logic-based expert systems. There are no categories and no symbols in the inner workings of neural\nnetworks: What they typically operate on is numerical, sensory input such as images, or some transformations\nof sensory input. Raw gray-scale values of pixels are kind of the opposite of neat, well-defined categories.\nAs such, neural networks and logic-based systems can complement each other in many ways. Usually, the\ncategories used by a logic-based system need to be recognized from sensory input: a neural network can tell\nthe logic-based system whether the input is a cat or a dog. In particular, a neural network can take sensory data\nas input, and its output can identify the states used in action selection; to begin with, it can tell the planning\nsystem what the current state of the agent is. In Chapter 8, we will consider in more detail this fundamental\ndistinction between two different modes of intelligent information-processing, which are found both in AI and\nhuman neuroscience.\nEmergence of unexpected behavior\nFinally, let me mention a phenomenon that is typical of any learning system. “Emergence” means that a new\nkind of phenomenon appears in some system due to complex interactions between its parts. It is a special\ncase of the old idea, going back to Aristotle at least, that “the whole is more than the sum of its parts”. For\nexample, systems of atoms have properties that atoms themselves do not—consider the fact that a brain can\nprocess information while single atoms hardly can.13 Likewise, evolution is based on emergence. Its objective\nfunction is given by evolutionary fitness, which sounds like a very simple objective function. Nevertheless, it\nhas given rise to enormous complexity in the biological world, as well as human society. What is typical of\nsuch emergence is that its result is extremely difficult to predict based on knowledge of the laws governing the\nsystem. If the objective function given by fitness had been described to some super-intelligent alien race a few\nbillion years ago, they would hardly have been able to predict what the world looks like these days.\n13Going further, it has been claimed that \"the whole is different from its parts\", especially by the Gestalt school of perceptual psy-\nchology (Wagemans, 2015). For example, a set of dots that forms a (dotted) straight line (such as ·········) can be perceived as a straight\nline only, completely forgetting the fact that it is composed of the dots; thus the line and the dots are two different things in the sense\nof two different ways of perceiving the same stimulus.\n\nCHAPTER 4. MACHINE LEARNING AS MINIMIZATION OF ERRORS\n47\nMachine learning is really all about the emergence of artificial intelligence. We build a simple learning al-\ngorithm and give it a lot of data, and hope that intelligence emerges. It is the interaction between the algorithm\nand the data that gives rise to intelligence. This seems to work, if the algorithm is well designed, there is enough\ndata of good quality, and sufficient computational power is available. Such emergence in machine learning is\nactually a bit different from emergence in other scientific disciplines. In physics, very simple natural laws by\nthemselves can give rise to highly complex behavior. In machine learning, the complexity of the behavior of\nthe system is, to a large extent, a function of the complexity of the data. In some sense, one could even say the\ncomplex behavior learned by an AI does not emerge but is extracted, or “distilled”, from input data. The com-\nplexity of the input data is due to the complexity of the real world, which is obvious when inputting a million\nphotographs into a neural network.\nThe emergent nature of the behavior learned by an AI implies that, just like in evolution, there is often\nsomething unexpected in the resulting system. The complexity of the input data usually exceeds the intellec-\ntual capacities of the programmer. So, the programmer of an AI cannot really know what kind of behavior will\nemerge: Often the system will end up doing something surprising.\nIn this book, we will encounter several forms of emergent properties in learning systems which are related\nto suffering. While some kind of suffering may be necessary as a signal that things are going wrong, we will\nalso see how an intelligent, learning system will actually undergo much more suffering than one might have\nexpected. To put it bluntly, a particularly intelligent system will find many more errors in its actions and its\ninformation-processing. In fact, finding such errors was necessary to make it so intelligent in the first place.\nTherefore, a learning system may learn to suffer much of the time, even though that is not what the programmer\nintended.\n\nChapter 5\nFrustration due to reward prediction error\nNow, armed with modern machine learning theory, we revisit the problem of action selection and the concept\nof frustration. In planning, as we saw in Chapter 3, the main computational problem is looking several steps\nahead, which can lead to quite impossible demands of computational capacities. Another constraint is that\nit requires a model of how your actions affect the world, i.e. where do you go in the search tree when you\nperform a given action in a given state. As such, planning is not really a good method for action selection if\ncomputational resources are very limited, as in a simple computer, or a very simple animal such as an insect.\nIn this chapter, we consider an alternative way to action selection, based on learning. A paradigm called\nreinforcement learning enables learning intelligent actions without any explicit planning, thus avoiding many\nof its problems. It also generalizes the framework of a single goal to maximization of rewards obtained at dif-\nferent states. While it can be performed even in very simple animals and computers, it is also used by humans;\nit is similar to how habits work.\nWe then consider how frustration can be defined in such a case; it can no longer be simply defined as not\nreaching the goal—since there is no explicit goal. We define more general error signals called reward loss and\nreward prediction error, which have been linked to signals of certain neurons in the mammalian brain. Thus,\nwe expand the view where frustration is related to error signalling by linking it to errors in prediction.\nRepeated frustration is thus something necessary for learning algorithms to work, and intelligence may not\nbe possible without some frustration. We further see how the very construction of an agent based on reward\nmaximization means that it is insatiable, never satisfied with the amount of reward obtained. Moreover, it can\nbe directed towards intermediate goals which are not valuable in themselves, but simply predictive of future\nreward. Evolutionary rewards, in particular, can lead to behaviors which resemble obsessions.\nMaximizing rewards instead of reaching goals\nIn modern AI, action selection is most often not based on planning, but a framework where the obtained re-\nwards, or reinforcement, is maximized. This is useful because often an AI does not have just a single goal to\naccomplish, but many things it should take care of. Defining behavior as maximization of rewards as opposed\nto reaching goals is also often thought to be more appropriate for modelling behavior in simple animals, which\nare thought to be incapable of the sophisticated computations needed in planning (more on this in Chapter 11).\nFor example, if a cleaning robot disposes of some dust in the dustbin, it could be given a reward signal.\nSince there are many rooms and many dustbins in the building, it makes sense to give a reward whenever the\n48\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n49\nrobot disposes of some of the dust. In principle, we could decide to give it a single reward when all the rooms\nare completely clean; however, it is common sense to rather give it a reward every time it removes some dirt or\ndust from any of the rooms. After all, the robot has done something useful every time it reduces the amount\nof dust in the room; telling this to the robot is highly useful information, and it would simply complicate the\nlearning if the reward were postponed until the robot has completed some larger part of the task.\nIn fact, giving a single reward at the end would mean the robot has to engage in long-term planning, which\nis difficult. A “piece-wise” training by giving rewards for small accomplishments is not very different from\nhow you would teach a child to perform a rather demanding, long task, say tying shoelaces: divide the task\ninto successive parts and give the child a small encouragement when it completes each small part. This is\ncomputationally advantageous since it eliminates the need for long-term planning, a bit like the heuristics we\nsaw above.1\nReinforcement or reward can also be negative; if the robot tries to put household items in the dustbin, it can\nbe given some. Negative reinforcement is really what we usually call a punishment—but the word is interpreted\nwithout any moral connotations here.\nThus, we actually ground action selection in the optimization of an objective function, i.e. a quantity to be\noptimized. Earlier, we saw that minimization of an error function, such as the number of images incorrectly\nclassified, is the way an AI can learn to recognize objects in images. Here we define a different kind of objective\nfunction which is the basis of action selection: It is equal to the sum of all future rewards. It is a function of\nthe action selection parameters of the agent, and more precisely, it expresses how much reward the agent can\nobtain by behaving according to its current action selection system.\nSuch a learning process based on maximization of future rewards by learning a value function is called\nreinforcement learning. Reinforcement learning can be seen as a third major type of learning in AI, in addition\nto supervised and unsupervised learning.\nIn a sense, this future reward is the ultimate objective function of an agent. Its maximization, by tuning the\naction-selection system, is the very meaning of life of the agent. The objective functions we saw earlier, used\nto learn things like pattern recognition by minimization of errors, are there merely to help in maximizing this\nreward-based objective function.2\nIn such a reward-based objective function, more weight is often put on the rewards in the near future as\nopposed to rewards in the far-away future, which is called discounting. The justification for this is complicated,\nbut suffice it to say that such discounting is often evident in human behavior: Humans prefer to have their\nreward right now, and value it less if they have to wait. To keep the discussion simple, I sometimes ignore\ndiscounting in what follows, but it could be used in almost every case considered in this chapter.3\n1It is also essential in training animals to perform long sequences of actions; in that context it is called “shaping” (Krueger and\nDayan, 2009; Ng et al., 1999). However, reinforcement learning is a much more general concept than just dividing a long sequence into\nsmaller parts. In the case of the cleaning robot, there may not be any end to the cleaning task since more dust appears constantly. The\nonly meaningful goal for the cleaning robot may be to just remove dust and dirt as much as possible, which is exactly captured by the\nreward formalism.\n2(Silver et al., 2021)\n3A basic exposition of discounting is given by Sutton and Barto (2018). For discussions of different kinds of discounting, and in\nparticular for comparisons between exponential and hyperbolic discounting, see Dasgupta and Maskin (2005); Ainslie (2001).\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n50\nLearning to plan using state-values and action-values\nAs such, the sum of future rewards gives a more general framework than having a single goal as in Chapter 3,\nsince trying to reach a single goal can be accommodated in the reward framework by simply giving a reward\nwhen the agent reaches the goal, and no reward otherwise. In such a case, discounting further means the agent\nreceives more reward if it reaches the goal more quickly, which is intuitively reasonable.\nIt turns out that we can use this reformulation of planning as reward maximization to our advantage, since\nthe algorithms developed for maximizing future rewards give a particularly attractive way of solving the prob-\nlem of planning. In Chapter 3, we saw how difficult planning is due to the exponential explosion in the number\nof possible plans to choose from. While heuristics were proposed as a practical trick to make the computations\nmore manageable, there is no universal way of designing good heuristics.\nLike in other branches of AI, it has been found that learning solves these problems, at least to some extent.\nIntuitively, if the agent encounters the same planning problem again and again, it can store information about\nthe previous solutions (or attempts) in memory. For example, a cleaning robot will probably clean the same\nbuilding many, many times, and a delivery robot will deliver the parcels to the same addresses quite a few\ntimes. So, such agents should be able to learn something about planning in their respective worlds. This would\nbe a clear improvement compared to heuristics, which need to be explicitly programmed in the system by\nprogrammers as in our examples above, and it is often unclear how to do that.4\nReinforcement learning gives us a sophisticated mathematical theory that tells us how to learn a particu-\nlarly good substitute for a heuristic, called the state-value function. It is a clever way of learning to deal with the\ncomplexity of the search in a planning tree. The basic principle is simple: Using the previous planning results\nin its memory, the agent can compute something like a heuristic based on how well it performed starting from\neach possible state. If it found the goal quickly starting from a certain state, that state gets a large state-value.\nIn the case where we have a single goal, the state-value function basically tells you how far from the goal\nyou are, thanks to discounting which takes account of the time needed to reach the goal. A delivery robot that\nfrequently delivers stuff to the same building (say town hall) would easily learn the distance from any other\nbuilding to the town hall. In the beginning, when it had a delivery to the town hall, it had to spend a lot of time\nand effort in planning the path there. But little by little, it gained information by storing any results of executed\nplans in its memory, and learned the distance from any other building to the town hall. Such distances now\ngive the state-value function for that goal (the state-value is actually a decreasing function of that distance).\nWhen the robot next needs to go to the town hall, it recalls the distances, to the town hall, from those buildings\nthat are close to its current location, and simply decides to move in the direction of the near-by building which\nhas the smallest distance to the town hall. Thus, it has learned a kind of a heuristic that avoids planning action\nsequences altogether.\nSuch learning works even in a very general setting when there is no particular goal. In general, the value of\na state is defined as the sum of all future rewards the agent can obtain starting from that state.5 After successful\n4Actually, the chess-playing Deep Blue mentioned in Chapter 3 did already use some learning as well: It analyzed data from several\nchess databases, including 700,000 historical games played by human grandmasters, to compute another heuristic.\n5Next, I give a more rigorous and general definition of state-values. To begin with, it must be noted that the state-value is a function\nof the “policy” used by the agent; the policy is what I call the action selection system in the text, i.e., the system that decides which\naction is taken in any given state (where the decisions can have some randomness programmed in them). Further, we have to take\ninto account the fact that the world may have some randomness in it, so we have to consider expected reward in the sense of the\nmathematical expectation in probability theory. The value function at a given state is then generally defined as the expected amount\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n51\nlearning of the state-values, the solution to the problem of action selection is that at each step, whatever state\nthe agent may be in, the agent just selects the single action which leads to the state with the highest state-value\n(e.g. closest to the town hall above). There is no need to compute several steps ahead, or make a search within\nthe huge search tree anymore. This reduces the complexity of the computations radically: instead of planning\nall its actions up to reaching the goal, the state-values now provide kind of intermediate goals that the agent\ncan very easily reach in just one step. However, a lot of time and computation still needs to be spent on first\nlearning the state-values.6 Value functions are illustrated in Fig. 5.1.\nCompletely reactive action selection by action-values\nWhile we have thus solved the problem of the computational explosion of planning, there is still the problem\nthat the agent needs to have a model of how the world works. Even using the state-values, it needs to under-\nstand which action takes it into a state with higher state-value. Now, consider an extreme case where the agent\nhas no model of how the world works in the sense that it has no idea what about the effects of its actions. Then,\nit is not enough to assign values to different states since the agent does not know how to get from state A to state\nB. (Still, we assume the agent knows in which state it is, at any given moment, so it does have some minimal\nmodel of the world.)\nThe trick to learning to act even with such a minimal model of the world is to learn what is called the action-\nvalue function. When the agent is in a given state, the action-value function tells the value of each of its actions,\nin terms of how much the total future reward is if the agent performs that action.7 This makes action selection\nof discounted reward that the agent will obtain starting from that state, when it follows that policy. (Sometimes, when speaking about\nstate values, it is more specifically assumed that the policy in question is the optimal policy which gives the highest expected reward,\nnut that is just one special case for a special policy.) This definition reduces to the definition in the main text for the case of a single\ngoal in a deterministic world, where the state-value is a decreasing function of the distance to the goal. The connection can be seen\nby defining that there is a reward at the goal and nowhere else, and using the fact that there is discounting, and thus rewards in the\ndistant future are given less weight than rewards in the near future. Then, the closer you are to the goal, the larger the expected reward\nis, because the reward at the goal is given more weight when you are closer to the goal. (I define here “closer” to mean that you can get\nthere more quickly compared to the situation where you are further away and need time to get there). While this standard definition\nin the literature, as just given, considers the reward uncertain and talks about expected (discounted) reward, I will not usually do that\nin this chapter for simplicity: I assume the world, as well as the policy, are deterministic. See Chapter 7 and its footnote 11 for a more\nsophisticated, probabilistic definition.\n6 A multitude of algorithms for learning the state-values exist; see Sutton and Barto (2018) for a comprehensive treatment. Typical\nalgorithms proceed by a recursion where the value of a state is defined based on the values of the states to which the agent can go from\nthat state, based on the theory of dynamic programming and, in particular, what is called the Bellman equation. As an illustration,\nconsider a simple world with three states, A, B, and C, where C is the (only) goal state; suppose you can move from A to B and from B to\nC. The first part of the recursion says that the value of state A must be the value of state B minus a small quantity. That is because from\nstate A you could go to state B in a single step, and the subtraction of the small quantity expresses discounting, due to the fact that you\nneed one step. Likewise, the value of state B must be a bit less than the value of C. Now the value of C is fixed (to some numerical value\nwhich is irrelevant) by the fact that it is the goal, and needs no recursion or computation. So, once the agent encounters the state C\neven once, it knows the value of C. Based on that knowledge and its model of the world, it can start recursive computations, by applying\nthe ideas above (value of B equal to value of C minus a small constant, value of A likewise) to recursively compute the values of B and\nA. If we fix the value of the goal to 1, the state-values could be 0.8, 0.9, and 1 for A, B, and C respectively. Note that in this example,\nwe computed the state-values of the optimal policy, i.e., assuming the agent always takes the smartest possible actions. You could also\ncompute the state-values for a very dumb policy (say, always taking random actions), and they would be lower because by taking less\nsmart actions, the agent would get less reward.\n7Again, strictly speaking, the action-values depend on the policy of the agent, while sometimes the term is used to mean the action-\nvalues for the optimal policy. The terminology is further confounded by the fact that sometimes action-values can refer to the current\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n52\nFigure 5.1: A simple illustration of value functions in reinforcement learning. We use a simple “gridworld”\nwhere the agent lives on a 2D grid. Each state is one location (or one box on the left-hand figure), and the\nactions are up, down, left, right. There is a reward at the right-hand border of the world, worth 10 reward\nunits, marked in green. There are also “punishments”, i.e. negative rewards, at the upper and lower edges of\nthe world, marked in red. Also, some randomness is added to the world, so that the actions the agent takes are\nsometimes replaced by random actions, to model the fact that the real world is random; therefore, the agent\nshould be “careful” and stay far away from the punishments. The agent starts from a random location at the\nleft-hand edge and then tries to get to the reward while avoiding punishments. Avoiding punishments is not\nthat difficult once the agent has learned where they are, but that learning will take time. Likewise, the agent has\nto learn, by trial and error, where the reward is located. In the figure on the left, the state-value functions (of the\noptimal policy) are plotted, both in numbers and the gray-scale value (darker squares mean lower value). We\nsee that the values are higher closer to the positive reward, and away from the negative rewards. This is exactly\nhow they should be to indicate that the agent should move right while avoiding the upper and lower edges.\n(The values are much less than the reward of 10 since they are discounted, i.e., kind of long-term averages.)\nIn the figure on the right, one episode in the life of the agent is shown. The numbers 1,2,3,... refer to the flow\nof time: they are the time points when the agent resided in each of the states. The agent started at the upper\nleft-hand corner and walked rather directly to the reward. This was possible because this plot assumed the\nagent had learned the values on the left-hand figure and used them for action selection. We see that using the\nstate-values, the agent was able to find the shortest path to the big reward, by simply always choosing to move\nto the near-by state that had the largest state-value.\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n53\nreally easy and extremely fast: Just compare the values of different actions and choose the one which gives the\nmaximum. In fact, all the relevant information about the effects of the agent’s actions are implicitly included\nin the action-value function. The agent still has to learn the action-values, but that is not really more difficult\nthan learning the state-values.8\nAt the end of the 19th century, Edward Thorndike put cats in a box where they would need to press a lever\nto get out of the box and receive some fish to eat. He observed that in successive trials, the cats were pressing\nthe lever more and more often. Such learning is called instrumental conditioning (as opposed to classical\nconditioning as in the famous Pavlov’s dogs, to be considered below). This shows how learning to choose\nactions is possible by simply associating what we call a state in AI (here, being in the box) with an action.9\nSo, using reinforcement learning, an AI or an animal can actually learn to act without doing any real plan-\nning and having almost no model about the world. If it learns the action-value function, it only needs to look\nat the single actions immediately available, and then take the action which has the largest action-value—at the\nstate where it happens to find itself. Since the action is here triggered immediately without any deliberation,\nlike a habit or a knee-jerk reaction, the resulting behavior is often called habit-based, or reactive.10\nReinforcement learning has recently become popular as a model of human behavior in neuroscience,\nwhere humans may not be considered too different from experimental animals such as cats or rats. Current\nthinking is that the same reinforcement learning algorithms can be used to model at least one part of the ac-\ntion selection system in most animals, including humans. Nevertheless, there is little doubt that some animals,\nprobably most mammals, engage in planning as well.11\nIn fact, reinforcement learning using value functions is not a magic trick that will obliterate the complexity\nof the action selection: It simply shifts the computational burden from search in the tree to learning a value\nfunction. Sometimes, this is a good idea, but not always. We will discuss the pros and cons of reinforcement\nlearning vs. planning in Chapter 8. Let me just mention here the main disadvantages of habit-based behavior:\nsuch learning often needs a lot of time and data, and leads to inflexible behavior. This is quite in line with the\ncommon-sense idea we have about habits.\nestimates of the agent for those action-value instead of their true values (the same holds for state-values as well). Action-values can\nalso be called “Q-values”.\n8For different algorithms, see (Sutton and Barto, 2018); for example, a recursion similar to that in footnote 6 above could be used.\n9This can be seen as an example of using something like action-values without a sophisticated world model. On the level of neuro-\nbiology, such reactive behavior can also be explained by a special form of Hebbian learning, which implements something similar to\nthe abstract theory of reinforcement learning we have just seen. In such learning, the association weight between the state (being in\nthe box) and action (pressing the lever) increases every time both the state and the action are active, and a reward (fish) is delivered.\nOrdinary Hebbian learning would only be able to learn, in an unsupervised manner, the connection between the state and the action\nif the same action is frequently taken in a particular state. It would be useless in itself for selecting the best action since it does not\ntake the reward gained by the actions into account. So, an extension of Hebbian learning to such “three-factor” learning, modulated by\nreward, is necessary. This may not be exactly what happens in the brain, but it is probably a useful approximation nevertheless (Nevin,\n1999). Such three-factor (or modulated) Hebbian learning rules have a long history, see e.g. the discussions by Legenstein et al. (2010);\nGerstner et al. (2018). These learning rules can also be extended to choosing action sequences in a dynamic environment: Basically,\ninstead of the reward itself, the Hebbian rule might be modulated by reward prediction error considered next in the main text.\n10Some authors use the term “model-free reinforcement learning” to clearly distinguish this from anything using planning. Planning\nuses a model of the world, thus it would be called “model-based”. Model-based reinforcement learning then refers to a set of algorithms\nfor solving the planning problem, with the possible modification that instead of reaching a single goal, the plan may still attempt to\nmaximize the sum of rewards.\n11For a review on applications of reinforcement learning to modelling animal and human behavior, see Niv (2009). On planning in\nanimals, see Redshaw and Bulley (2018); Corballis (2019).\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n54\nFrustration as reward loss and prediction error\nWe have thus divided action selection into planning and habits, where habits refer to more automated ac-\ntion selection mechanisms similar to reinforcement learning.12 Therefore, we need to ask how we can define\nfrustration in the case of habits, where there are no goals but rather rewards obtained here and there, and we\ncannot talk about frustration in the sense of not reaching the goal as in Chapter 3.\nWhat defines frustration in this case is an error signal called reward loss13 which we already saw briefly in\nChapter 2. It is computed by the following simple formula:\nreward loss = expected reward - obtained reward\nwhich is set to zero in case the difference is negative. That is, a reward loss is incurred when an agent expects\nto get some reward but actually gets less reward than expected. Maybe a cleaning robot expected to find a lot\nof dust in a room, but in fact there was much less. If it happens that the obtained reward is actually larger\nthan expected, there is obviously no reward loss, so the reward loss is defined as zero if the difference in the\nexpression above is negative. Reward loss can also occur if the expected reward is negative, and the obtained\nreward is negative while even larger (in absolute value): the agent did expect something bad to happen, but it\nturned out to be even worse.14\nExpectation of reward here refers to the mathematical expectation as defined in probability theory. It is\nobtained by weighting the possible values by their probabilities: if the probability of obtaining a reward is 50%\nand the reward is 10 pieces of chocolate, the expected reward is 5 pieces of chocolate.15 Expectations of the\nfuture are often called predictions. Predictions are in fact ubiquitous in the brain: it is likely that the brain\nmakes a prediction of almost any important quantity in the environment.16\n12(Dolan and Dayan, 2013)\n13(Papini et al., 2015; Mee et al., 2006); see also (Bell, 1985; Van de Cruys, 2017)\n14The effort made in trying to obtain the reward may also need to be taken into account in computing frustration. While it might\nseem natural to simply subtract the effort spent from the reward, considering it as a “cost”, sometimes more effort leads, paradoxically,\nto higher perceived reward (Inzlicht et al., 2018).\n15The exact definition of expectation as used in reward loss is not very clear in my view, and an important problem for future research.\nNot much attention has been paid on it, partly because in typical experiments, it seems obvious what the expectation should be, and\nthere is little planning involved. In a prototypical experiment, an animal (or a human) is given the same (positive) reward several\ntimes for some simple behavior, and then suddenly it is given less reward (this is called “successive negative contrast”) for that same\nbehavior. In such a case, the future expectation of the reward is simply assumed to be equal to the past reward. With longer plans in\nmore complex environments, the definition will be less obvious. Clearly, there is a strong connection to the concept of a prediction, as\ndiscussed next in the main text as well as footnote 16 below. Furthermore, an alternative definition of reward loss might be developed\nusing counterfactual contrast (Roese, 1997), formalized as counterfactual regret by Zinkevich et al. (2008), where the obtained reward\nis compared with what might have been obtained, if better actions had been chosen. If the agents form some kind of a society, even\nmore options exist for defining the expectations. The agent might use social comparison, i.e. information on what other agents get,\nand expect to obtain the same reward as others do. Such a “social” expectation might simply be based on probabilistic inference: If the\nother agents are similar to the agent in question, it is logical to expect that the agent in question will be able to obtain the same amount\nof rewards (Rutledge et al., 2016); see also footnote 5 in Chapter 6. Yet another, very different, form of expectation might be produced\nin a situation where the agent assumes a moral right to obtain something, assuming the existence of some ethical norms in the agents’\nsociety (Dignum et al., 2000).\n16(Clark, 2013). From the viewpoint of mathematical theory, it might actually be more appropriate to talk about predicted reward\ninstead of expected reward in the definition of reward loss (assuming here that expectation is defined as the mathematical expectation\naccording to probability theory as in the main text). While these are often seen as the same thing—prediction being an expectation\nof a future quantity—the concepts are not equivalent. In particular, in machine learning theory, a prediction can be considered more\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n55\nIn contrast to our basic definition of frustration in Chapter 3, which works only on the level of plans, the\nreward loss can be computed after every single action and at every single time point. This definition of reward\nloss is, in fact, quite flexible since the time interval in which the reward is computed can be specified to be\nanything from seconds to days. Therefore, it provides a general framework encompassing both planning and\nhabit-based action. Reward loss coming from planning and reward loss coming from single actions are similar\nexcept that they work on very different time scales. We shall consider this point in more detail in Chapter 9.17\nReward loss, in its turn, is related to what is called the reward prediction error (RPE), a most fundamental\nquantity in machine learning theory. RPE means any error made in the prediction of the reward. This definition\nis very general because the expected reward can be greater or less than the obtained one, and thus RPE can be\npositive or negative. If the obtained reward was larger than expected, that is the opposite of reward loss and\nsuffering, and related to pleasure.18\nAs the very expression “reward prediction error” indicates, the theory of RPE also shows how suffering is\nrelated to learning by minimization of errors, which is a fundamental approach in machine learning. If the\nagent can predict the rewards obtained by different actions in different states, it will be able to act so as to\nmaximize the obtained rewards (at least if it has a good model of other aspects of the world as well). To learn\nand improve such predictions, it is necessary to compute the errors produced by the current predictions. This\nis how minimizing RPE is related to maximization of rewards. In fact, it is possible to devise reinforcement\nlearning algorithms that work simply by minimizing reward prediction error.19\nThe exact mathematical definition of RPE is quite involved and relegated to a footnote.20 Let me just point\ngeneral than expectation: a sophisticated prediction will also include an estimate of the uncertainty involved in the prediction, in\naddition to the mathematical expectation. The importance of such uncertainty of predictions will be seen in Chapter 7 regarding the\nconcept of threat. But here, regarding frustration, such uncertainty is relevant because it seems that the certainty of the prediction\naffects the level of frustration. I would claim that if you are completely certain that you will get chocolate (say, 5 pieces), but then it\nturns out you don’t, the frustration will be greater than in the case where there is only some chance of getting any (like the example\nin the main text, 10 pieces with 50% probability). Crucially, in this example, the expected amount of chocolate, in the sense of the\nmathematical expectation, is the same in the two cases, and only the uncertainty changes. Therefore, the effect of uncertainty should\nbe taken into account in the definition of reward loss. See footnote 21 in Chapter 16 and the main text preceding that footnote for\nfurther developments of this point, as well as Chapter 12.\n17Another difference is that while earlier (Chapters 2 and 3) we defined frustration as “not getting what one wants”, in line with the\nquotes from ancient philosophers, here reward loss is defined as “not getting what one expects”. These are not exactly the same thing\nand are sometimes quite different; this connection will also be discussed in Chapter 8 (page 94).\n18I refrain from trying to rigorously define pleasure in this framework, but obviously an RPE where the obtained reward is greater\nthan expected is a good candidate. In fact, in experiments with participants playing a gambling game, the long-term average of the\nreward prediction error (taking both positive and negative parts into account) was a strong predictor of the participants’ well-being\n(Rutledge et al., 2014); however, the average level of reward had a strong effect as well. Alternatively, Carver (2003) has proposed that\nthe function of the pleasure system is to signal that the current task has been accomplished, and the system can direct its resources to\nother tasks. The neurobiology of pleasure and pain is reviewed by Leknes and Tracey (2008).\n19This can be done using a special form of RPE called temporal difference (TD) error (see footnote 20 below), and in particular using\nthe squared error summed over all states. See Sutton and Barto (2018, p. 268) who call it Bellman error, or related developments by\nBhatnagar et al. (2009).\n20 RPE can actually be defined in different ways. In neuroscience literature, the definition may not be very different from reward loss.\nIn the reinforcement learning theory, a more sophisticated definition is usually used, using what is specifically called the temporal\ndifference (TD) error, which we explain here. For simplicity, no discounting is used here. For each time step, RPE is then defined as\nRPE = reward −(Vbefore −Vafter) where V is the state-value function (for the policy being followed, not necessarily the optimal one),\nin the state before the action was taken or after the action taken, respectively (which could also be denoted by time indices t −1\nand t). The reward is the reward obtained for this particular action, or in other words, at this particular time step for which we are\ncomputing the RPE. Note that the sign is flipped compared to the definition of reward loss, but this is just a technical convention with\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n56\nout that RPE is a more general concept than reward loss also in the sense that it enables an error signal even\nwhen the agent is far away from any actual or expected rewards, but it receives “bad news” about future reward.\nThis is in contrast to reward loss which does not make any sense unless a reward is actually expected to be\nobtained right now (the exact meaning of “now” depends on the time scale). In particular, if the expectation\nof total future reward decreases, this is enough in itself for RPE to signal frustration. Suppose a cleaning robot\nis on its way to a room where there is a lot of dust (yummy!), and thus its expected (predicted) reward is high;\nbut this reward will in any case not be obtained for quite a while. Then, it finds that the door to the room is\nlocked and it cannot enter; that is bad news it didn’t expect. Thus the robot finds itself in a new state that has\na much lower expected total future reward since the dust in that room cannot be reached. It is this difference\nbetween the earlier prediction and the new prediction that creates a RPE and suffering. This is not an ordinary\nreward loss because no actual reward was expected to appear at this time point anyway: the robot has not\nyet even entered the room, and the dust is still far away. However, RPE can create suffering merely based on\npredictions: if information arrives that makes the agent reduce its prediction of future reward, frustration is\ncreated. This is intuitively appealing since a lot of our frustration is actually about such negative news and the\nlowering of expectations they create. Suppose I’m planning to attend an event that I expect to enjoy, and then,\nwell in advance, I hear the event has been cancelled. I will suffer, although I didn’t expect to obtain anything\nenjoyable yet, and I may not have taken any action either; it was all just predictions in my head.21\nno deeper meaning. The connection to reward loss can be seen by understanding that in the state-value formalism, Vbefore −Vafter\ncan be interpreted as expected reward. The reason is that by the definition of the state-value function, the state-value function gives\nthe total reward expected when starting from each of the states, so you would expect a reward equal to Vbefore −Vafter for this action.\nOtherwise the two state-values would be inconsistent; the total expected reward starting from the state “before” must be equal to the\ntotal expected reward starting from state “before” plus the expected reward obtained in the transition. So, the agent can expect that\nreward = Vbefore −Vafter, and if that actually holds, RPE would be zero. If you get less, there is a reward loss, which is here expressed as\na negative RPE. Such an RPE signal is more general than reward loss since it considers the whole future of rewards via the state-values,\nas explained next in the main text. There are also some small differences: RPE has a different sign, corresponding to the negative of\nreward loss, and our definition of reward loss considers only the case where it is positive (or RPE is negative) since this is the part\ncorresponding to suffering. Also, typically the discounting formalism of reinforcement learning is included in the definition, in which\ncase Vafter would by multiplied by a discounting factor throughout; omitting the discount factor is possible if we consider a finite time\nhorizon. See Sutton and Barto (2018, Ch.15) for more information.\n21To see how this works mathematically, consider the definition of RPE (given in footnote 20 above) in the case where the obtained\nreward is zero. It makes sense to consider zero reward because it is generally agreed that rewards are temporally sparse (mostly zero),\noften extremely sparse, so most of the time the RPE is simply the difference between the state-values in two states (before and after,\nor past and present), possibly discounted in the latter state. Recall that the state-value is nothing else than the predicted total future\nreward. Thus, recalling that the sign in this conventional definition of RPE is wrong for our purposes, RPE defines frustration as Vbefore−\nVafter, which is exactly the decrease in predicted total future reward, comparing the prediction in the previous time step and the present\ntime step. Such a decrease is possible when the agent receives new information (which implies, in the basic formalism, that it finds\nitself in a new state incorporating that information), and that information makes it revise its prediction downward (it switches to the\nprediction given by the new state it finds itself in). Thus, in the case where the prediction decreases in the absence of any reward\nobtained, the reward loss or the negative part of RPE is equal to the decrease in the prediction of the total future reward. This is how\nRPE can define frustration based on predictions alone, without any reward currently expected. One might think that reward loss could\ndo the same if we simply change the time scale: in the robot example, if you take the expected and obtained reward for, say, one whole\nhour, that would arguably lead to a reward loss since the robot expected to get dust during that hour but didn’t get any. However,\nRPE makes its computations independently of any such time scales (it is in fact taking into account the whole future as it looks at\nthe total expected future reward) and moreover, such long-term reward loss would not occur before than hour has passed, while RPE\nsignals frustration the very moment the new information has arrived and has been processed. (As a minor point on terminology, it\nmay be slightly misleading to talk about “reward prediction error”, since RPE is in this case rather a change in predictions due to new\nobservations; a non-zero RPE does not necessarily imply that there was any error, but simply a change, an update of prediction based\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n57\nExpectations or predictions are crucial for frustration\nReward loss and RPE highlight the importance of expectations and predictions. Clearly, there must be some\nexpectation or prediction in order for them to occur. If the cleaning robot were so primitive that it had no\nexpectations or predictions at all, it might be just enjoying every single speck of dust it finds. Making it more\nintelligent so that it can predict the future thus deprives it of its “innocence”, and enables frustration to occur.\nLikewise, Cassell says that “to suffer, there must be a source of thoughts about possible futures”, even though\nhis approach to suffering is quite different.22\nThe importance of predictions is well appreciated in neuroscience. It has been observed that in the brain,\nRPE is coded by certain neurons using a neurotransmitter called dopamine. More precisely, it is coded by quick\nchanges in the level of dopamine (called “phasic dopamine signal”), typically originating in evolutionarily old\nareas such as the midbrain, which is literally in the very center of the brain.23 In case the obtained reward is\nhigher than expected, there is a temporary peak in the amount of dopamine in the signalling pathways, which is\ncalled by some a “dopamine surge”. That’s why many drugs of abuse target the dopamine pathways in the brain.\nFor example, cocaine blocks the removal of dopamine in the synapse so that its signal is amplified.24 Such drugs\nare fooling the reward-processing system in the brain, thus leading to a strong desire for such drugs, in addition\nto a pleasurable feeling. This has led some to think that dopamine is the neurotransmitter responsible for the\nfeeling of pleasure itself. Such a viewpoint is probably incorrect, and the actual feeling of pleasure is mainly\nmediated by other transmitters, namely those in the opioid family, while dopamine is more related to “cold”\naction selection and learning.25\nClassical conditioning\nTo emphasize the importance of predictions in the brain, let’s consider an extremely famous kind of predic-\ntion learning in the animal realm: classical conditioning. Ivan Pavlov, doing physiological experiments on dogs\naround the year 1900, observed that the dogs began to salivate when they saw the staff person who was re-\nsponsible for feeding them, even before receiving any food. Pavlov was intrigued and tried to see if the dogs\nwould be able to associate any arbitrary stimuli to food. He succeeded in making the dogs associate food with\nmany different kinds of stimuli, including the sound of a bell or a metronome, provided that these stimuli were\nconsistently presented just before food was given.\non new information. )\n22(Cassell, 1989); such an approach will be treated in Chapter 7. Likewise, the importance of predictions and expectations in eco-\nnomic decision-making is emphasized by K˝oszegi and Rabin (2006) who propose that consumers compare expected utility given an\naction with a “reference-point” given by a probabilistic prediction of the future utility.\n23(Schultz, 2016; Lerner et al., 2021). In experiments with humans, such signalling might be measurable as the error-related negativity\n(ERN) seen in EEG measurements, as well as fMRI signals mainly in some parts of the anterior cingulate cortex where ERN seems to\noriginate (Holroyd and Coles, 2002; Abler et al., 2005; Zubarev and Parkkonen, 2018).\n24(NIDA, 2020), but see also Nutt et al. (2015)\n25(Berridge and Kringelbach, 2015; Leknes and Tracey, 2008). This dissociation may sound logically contradictory, but it is based on\nthe distinction (in Berridge’s terminology) between the motivational “wanting” processes which more directly tell the organism what\nto do, and the affective “liking” processes which are related to the feeling of pleasure. Abler et al. (2005) also proposes that reward loss\ntriggers different kinds of neural processes, some of which are more related wanting, action selection and reinforcement learning, and\nothers more to liking and the feeling of pain or pleasure; they find that the localizations of those two processes in the brain are different.\nSee also footnote 11 in Chapter 10.\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n58\nWhat the animal is clearly doing here is predicting the future: after the bell, food is likely to arrive. Such\npredictions are ubiquitous in the brain; the brain is constantly trying to predict what happens, using its myriad\nsystems. Predicting the results of any action you might take is important if you want to choose good actions,\nas we already saw in the case of instrumental conditioning above (page 53). Predicting where the rabbit will\nbe a second or two later is necessary if you want to catch it. Note the crucial distinction between classical\nconditioning and instrumental conditioning: in classical conditioning, the agent does not yet learn to choose\nactions, but merely to predict future states, independently of any rewards.26\nIt would be natural to assume that such classical conditioning could be easily performed by Hebbian learn-\ning. It is just the kind of association of two stimuli—bell and food— that Hebbian learning seems to be good\nat. That is to some extent true, although this is a bit tricky; the most successful models actually use super-\nvised learning, with the bell as input and the food as the output. Such learning, again, proceeds by minimizing\nprediction error.27\nDoes a low level of rewards produce frustration?\nIntuitively, however, it might seem that talking about frustration based on expectations and predictions is un-\nnecessarily complicated. If the agent is in a state with low state-value (in its own estimation), would that not in\nitself imply frustration? Being in a state of low value means that the agent believes it will not obtain much net\nreward in the future, which sounds like a good reason for mental pain. Or, even more fundamentally, why not\njust say that lack of rewards, presumably during recent history, leads to suffering?\nOne fundamental problem with such an approach would be that it is not obvious how to define a suitable\nbaseline or comparison: What level of state-value is actually low, and how small should recent reward actually\nbe to create frustration? The reward loss or prediction error actually solves this problem by using the expecta-\ntion of the reward as the baseline. Thus, the obtained reward is compared with the expected level, and if it is\n“low” in this particular sense, frustration occurs.28\n26In Pavlov’s experiment, the dog learned to predict that food is coming, independently of its actions. It did salivate, which could be\nseen as an action, but the salivation was a (presumably innate) response to food that was not learned during this experiment.\n27For a single conditioned (i.e., predictive) stimulus, Hebbian learning actually works fine, but the problem is that when there are\nseveral conditioned stimuli, Hebbian learning would create too many associations and in an unbalanced way. For example, we could\nhave an experiment where both a bell and a green light predict food. Simple Hebbian learning would then associate both those stim-\nuli with the food, since the association strengths would be computed independently of each other. But this is in contradiction with\nwhat seems to happen in the brain. Such interaction between predictions has been investigated in a famous twist to the basic classi-\ncal conditioning experiment using the bell: after the main experiment, another experiment is made where both the bell and a newly\nintroduced green light predict food. In such a case, the dog will not learn to associate the green light with the food because the connec-\ntion from the bell is enough to predict the food, and there is no need to construct an association from the light to the food anymore.\nThis is in contrast to what Hebbian learning is supposed to do. The brain apparently tries to be economical and constructs only those\nconnections that are necessary for the prediction of the food. Therefore, the association strength of one conditioned stimulus will also\ndepend on the associations of other stimuli. This is why most research assumes a supervised model, which typically learns several\nsuch association strengths in a balanced way, and thus explains the various experiments better than simple Hebbian learning. A basic\nsupervised learning rule accomplishing this is the Rescorla-Wagner model (Miller et al., 1995), which further models the dynamics of\nlearning, as in the bell/light example just given; the model explains how the existing association with the bell “blocks” the development\nof a new association with the light.\n28The RPE formalism could also be interpreted as providing another baseline mechanism, by looking at the change of state-values.\nGoing to a state which has a lower value than the current state, without obtaining any reward, does produce suffering according to the\ndefinition of RPE above, as explained in footnotes 20 and 21 in this chapter. From this viewpoint, RPE uses the current state-value as\nthe baseline defining what is “low”, looking at the total expected future reward. See also footnote 15 on different possibilities of defining\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n59\nUnexpected implications of state-value computation\nIn the rest of this chapter, I will consider some practical implications of the theory. First, let us consider how\nthe computation of state-values, as proposed in basic reinforcement learning theory, fundamentally changes\nthe behavior of human agents. Originally, of course, evolutionary forces demand that an action is pursued by\na biological organism if the action helps in reproducing and spreading its genes, and an action is avoided if it\nhampers this effort. So, evolution “tells” us that kicking a stone is bad because it can cause damage to our foot,\nand the damage decreases our potential for reproduction—thus giving us negative reward for such an action.\nHaving sex is very good, and rewarded by basic evolutionarily mechanisms, because then we are fulfilling our\ndeepest evolutionary calling and spreading our genes.\nThe computation of state-values changes the situation: The organism will not only try to reach states di-\nrectly giving reward—such as having sex—but also states that have higher state-values. This is a mechanism\nfor looking forward in time: instead of immediate reward, the organism will try to maximize the total reward in\nthe future, and that is exactly what is given by the state-value.\nSeemingly valueless states are now valued by the agent since they predict that more actual reinforcement\ncan be found sooner. Such states provide intermediate goals in the pursuit of the actual reward, similarly to\nheuristics in tree search. If you train a robot to get orange juice from the fridge, it must of course first go to the\nfridge, and open it. So, the state where the robot is standing next to the fridge acquires a positive state-value\nand we could almost say that the robot “enjoys” being next to a fridge, even more so if it is open.\nThe situation is even more complex due to the existence of human civilization and society. Culture plays\nan important role in determining the state-value function, and it is often difficult to separate the influences of\nbiology and culture. In neuroscience, this is called the “nature vs. nurture” question. There can be extremely\ncomplex chains of value computation which transform the original evolutionary goals to behavior based on\nintermediate goals. For example, humans have evolved to strive for high social status. From an evolutionary\nperspective, this is because it helps humans get more sexual partners and increases the number and the sur-\nvival probability of their offspring. This then implies that we want to increase our status: for example, winning\na gold medal in the Olympics is a good behavioral goal. Clearly, a gold medal in Olympics has no evolutionary\nvalue in itself: it does not satisfy your hunger, thirst, or sexual appetite in itself. It is just an arbitrary piece\nof metal. There is no logical connection between such a piece of metal and sex. It is only due to a complex\ninterplay of value function calculation and cultural meanings that the original evolutionary reward of sex has\nbeen subtly transformed into a goal such as excelling in sports—or science, or politics.\nSuch slightly weird desires are another manifestation of the phenomenon discussed earlier: emergence of\nunexpected phenomena due to the interaction between the learning agent and a complex environment. If we\nprogram sufficiently sophisticated AI, the same thing is likely to happen as with human evolution. The AI will\npursue goals that were not intended by the programmers, but which still happen to produce a high state-value.\nThis is particularly likely if the AI interacts with humans who provide a particularly complex environment to\nlearn from—or, if the AIs are capable of complex interactions between themselves.29\nthe baseline as “expectation”. Chapters 6 and 9 will further discuss how such expectations can operate on different time scales, and we\nwill see that a long-term lack of rewards can indeed be considered frustration when looking at a longer time scale.\n29This is also a problem from the viewpoint of the safety of AI systems. Hendrycks et al. (2021); Turner et al. (2020) discuss how to\nbuild learning AI agents systems with minimum negative side effects.\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n60\nEvolutionary rewards as obsessions\nNow, if we admit that our desires are based on evolution, even if quite indirectly, is that a good thing or a\nbad thing? Should we just follow our desires, or think twice, or even try to follow some completely different\ngoals? There are actually people who try to justify certain kinds of behavior by saying they are evolutionarily\nconditioned, i.e. “evolution made me do it”. In popular science magazines and web sites, such logic is not very\nuncommon. Fortunately, it is rejected by many as an example of sloppy thinking.30 In the following, I argue\nthe very opposite: following evolutionary desires is often a bad idea and even morally wrong.\nIn fact, even the evolutionarily conditioned rewards themselves can go wrong, sometimes quite catastroph-\nically. One reason is that evolutionarily, we may be adapted to the environment where our evolutionary ances-\ntors lived, often assumed to be the “African savannah”. However, the modern world is different and, therefore,\nour evolutionary programming may not be very suitable.31 With humans, a well-known example is the addic-\ntive quality of sugary food. The sweet taste of sugar must have signalled the high nutritious quality of food in\nthe environment where our ancestors lived.32 But these days it tends to signal added refined sugar which is\nbad for your health; evolutionarily speaking, sweet taste should rather be punishing in the modern context,\nnot rewarding.33 Yet, the state of having a sweet taste in your mouth is rewarding, and humans tend to try to\nreach such a “sweet” states.34\nWhat is even more serious is that evolution makes us want particularly questionable goals, especially from\na societal viewpoint. Evolution is fundamentally based on selfish, merciless competition between different\norganisms (or strictly speaking, between their genes). Many behavioral tendencies evolution has imposed on\nus should be seen as instruments for such egoistic competition. Evolution is all about maximally spreading our\ngenes. It makes us hoard finite resources such as food to ourselves in order to spread our genes. It makes us\nviolent; it even makes us go to war, again for the sole purpose of spreading genes. This is in stark contrast to\nmost ethical systems in the world which see such selfishness as evil, and recommend quite opposite courses of\naction.35\nEven more fundamentally, the rewards defined by evolution never had the goal of making us happy in any\n30It is a case of what G.E. Moore called the naturalistic fallacy. Hume already pointed out that you cannot infer what ought to be from\nwhat is. In other words, if evolution makes people behave in a certain way, it does not in any way morally justify the claim that this way\nof behaving is good or acceptable.\n31(Sapolsky, 2004; Wright, 2017)\n32Another striking example in the case of humans is pornography, where watching sexually desirable models on a computer screen\nis felt to be somehow rewarding by its human consumer, and leads to desire towards such pictures. Simply seeing sexually attractive\npeople naked should indeed have a very high state-value, since that is likely to happen only when copulation is near—at least in our\nevolutionary past. But in the modern world, this behavior is quite dysfunctional in the sense that there is almost no chance that the\nconsumer would be actually able to mate with those models.\n33As a kind of mirror image of such maladaptive evolutionary desires, there is the phenomenon of chronic (persistent) pain. Raffaeli\nand Arnaudo (2017) review research on how chronic pain “entails a pathologic reorganization of the neural system” so that it “loses its\nbiologic damage signaling function” and “becomes a destructive force”, eventually a disease in its own right.\n34In fact, it is often difficult to define what is the actual reward and what is differences in state-values. I’m here assuming that the\nsweet taste is a reward in itself, and not a question of a high state-value (i.e. predicted future reward), but this can be disputed. It is\nless controversial that an Olympic medal does not produce a reward in itself, but even this is not so clear. To solve this problem, Singh\net al. (2009) propose that the rewards should evolve so that they are correct in most environments, while state-values are then learned\nduring an individual’s lifetime for the particular environment where the individual is living.\n35Admittedly, the connection between ethics and evolution is complex, and evolution seems to have conditioned some kind of altru-\nism in us as well (Wright, 1994; Nowak et al., 2010).\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n61\nmeaningful sense. They are a force that drives us to do exactly those things which are good for our evolutionary\nfitness. Even if you come to the conclusion that the evolutionary reward system makes you suffer, it cannot be\nswitched off or modified. You cannot decide to be rewarded by something you consider more meaningful and\ngood for society.\nI suggest evolutionary rewards lead to what can be called evolutionary obsessions.36 That is, the evolu-\ntionary rewards, together with the learned state-values, make us desire, even crave for many things which we\nwould actually prefer not to desire if we could rationally decide what we desire. If you could just consciously,\nrationally, “switch off” your desire for, say, sugary food—would you not do that? Chapter 16 explains how Bud-\ndhist and Stoic thinking are based on the rather extreme tenet that switching off all desires would actually be\nvery good for you. Whether one agrees with that extreme viewpoint or not, surely, most people have certain\ndesires that they would rather not have. I call them obsessions because they are automatically created, they\noften override any conscious deliberation, and they may even feel unwanted and intrusive. (We will look at the\ncomputational mechanisms for this in Chapters 8 and 10.)\nReward maximization is insatiable\nFinally, let me mention another dark side to this reinforcement learning theory. One crucial property of the\nalgorithms based on reward prediction error is that they drive the system to get more and more reward, and\nthere is never any long-term satisfaction. This is because any prediction of the future is learned by the agent,\nand constantly updated by learning. Thus, in the reward loss, the level of expected reward is updated based on\nwhat the agent has obtained recently.\nSuppose that an agent gets an exceptional amount of rewards for a while, maybe because a cleaning robot\nfinds itself in a building with lots of nice dust to clean, and it is rewarded for every speck of dust it sucks away.\nNow, the agent’s prediction system is updated so that an equally large amount of rewards is predicted in the\nfuture as well. An environment that produced an unexpectedly large amount of reward for a while becomes\nthe new baseline. That level of reward is not unexpected anymore and, therefore, does not produce any partic-\nular “pleasure” anymore either.37 What’s worse is that when things get back to normal, the agent will get less\nrewards than what it has now learned to expect, since the prediction was updated to reflect the particularly\nnice environment that lasted for a while. Therefore, the agent suffers enormously when it has to go back to a\nnormal room with a modest amount of dirt.\nSimilar computations take place in our brain, since our brain also computes the reward prediction error\nand updates its expected level of reward. No wonder that Wolfram Schultz, one of the leading neuroscientists\non dopamine, calls the dopamine neurons “little devils”.38 In fact, this is a logical consequence of the guiding\nprinciple of AI agent design: the agent should maximize obtained reward. The reward prediction system has\n36I am here using the term “obsession” in a loose sense, not using its strict psychiatric definition. For reference, in the current ICD-\n11 proposal, obsessions are defined as follows. “Obsessions are repetitive and persistent thoughts (e.g., of contamination), images\n(e.g., of violent scenes), or impulses/urges (e.g., to stab someone) that are experienced as intrusive, unwanted, and are commonly\nassociated with anxiety. The individual attempts to ignore or suppress obsessions or to neutralize them by performing compulsions.\n— Compulsions (or rituals) are repetitive behaviors (e.g., washing, checking) or mental acts (e.g., repeating words silently) that the\nindividual feels driven to perform in response to an obsession, according to rigid rules, or to achieve a sense of ’completeness’. ”(Stein\net al., 2016)\n37This is related to the phenomenon of the “hedonic treadmill” (Lyubomirsky, 2010).\n38(Schultz, 2016)\n\nCHAPTER 5. FRUSTRATION DUE TO REWARD PREDICTION ERROR\n62\nno other goal than helping in maximization of rewards. If you program an agent to maximize reward, then\nby definition, nothing can possibly be enough; the system will be insatiable. The agent will relentlessly try to\nget more and more reward, and it is precisely the frustration signal that will force the agent to try harder and\nharder.39\nA merciful programmer might program some stopping criterion to limit the greed of the agent: Once you\nhave obtained X units of reward, you can stop. Unfortunately, evolution knows no mercy, and humans don’t\nseem to have any such stopping criterion programmed in them. We need more money, more power, more sex\n(and better sex), and better food (and more food). If we follow our evolutionary “obsessions”, as I called them,\nnothing is enough.\nSuppose you program a robot called Pat to clean a building. You would like the building to be superclean,\nand the building is quite large with dozens of rooms. So, you would be very tempted to program Pat so that it\nwill spend all its time cleaning the building. You probably want to program a couple of other functions in Pat as\nwell, such as a routine for charging its batteries, some basic maintenance procedures, as well as safety systems\nto prevent it from hurting people or breaking things. But you would probably program Pat to spend all the rest\nof the time in tirelessly cleaning the rooms, with no breaks in between. This is what most programmers would\ndo. Here, you have implemented a kind of a “cleaning drive” which is without mercy. Pat will spend all its time\nand energy just making the rooms spotlessly clean. This may seem completely natural, given that it is “just” a\nrobot.\nNow, suppose your colleague, responsible for the visual design of the robot, decides to make Pat look really\ncute, giving it the shape of a little kitten. It even says “Meow” using its loudspeakers. Many people may sud-\ndenly start feeling sympathy for this poor little kitten. “Does it really have to be working all the time? Can’t it\never play, or take a rest?” they would ask. What would you reply?\n39Dubey et al. (2022) provide an explicit model on how such insatiability is computationally useful while leading to less happiness.\nLambie and Haugen (2019) consider insatiability as an important component of greed. On the other hand, it is true that some purely bi-\nological needs are satiable to some extent—for example, hunger is reduced by eating, even if momentarily—but classical reinforcement\nlearning theory is lacking much consideration of such metabolic states (Keramati and Gutkin, 2014). See footnote 37 in Chapter 16 for\nsome ancient philosophical references on the topic.\n\nChapter 6\nSuffering due to self-needs\nIn addition to frustration, Chapter 2 identified another cause of suffering: threats to the intactness of the per-\nson, or the self. In this chapter, I consider the concept of self, while the concept of threat is treated in the next\nchapter.\nSelf is a concept with a bewildering array of meanings. Psychology, philosophy, and neuroscience offer a\nmultitude of definitions, and I can make no claim to treat the concept comprehensively. I focus here on two\nmeanings of “self” directly related to suffering. First, self as the target of evaluation of some kind of long-term\nsuccess of the agent. The human brain, in particular, has a system that constantly evaluates the agent, checking\nwhether the goals set were reached or rewards obtained, and seeks to improve its general performance. Second,\nwe have self as the target of self-preservation, or survival instinct: all animals have behavioral tendencies to\navoid death or organic damage. (A third meaning of self, related to control, will be treated in Chapter 13, and\nthe concept of self-awareness, in Chapter 14.)\nSuch self-evaluation and self-preservation are computational mechanisms which are constantly operating\nin animals, and it is easy to justify their computational utility for any intelligent agent. Although at first sight,\nthese aspects of self may seem to provide a mechanism for suffering which is completely different from frus-\ntration, this chapter shows how they are related to frustration of internal, higher-level goals and rewards. As\nthe title of this chapter indicates, these aspects of self can thus be seen as needs, or desires, and they can be\nfrustrated.\nSelf as long-term performance evaluation\nLet us start with self as something whose performance is being constantly evaluated at different levels. As we\nsaw earlier, in reinforcement learning, every single action is always evaluated to improve future actions. The\nreward prediction error is computed even in the simplest algorithms. If the reward is incorrectly predicted, the\nerror is used by the learning algorithm to improve the prediction—if the prediction was too high, set it lower\nin the future, for example. While such computations are crucial for learning to act optimally, the errors also\ntrigger the suffering signal according to the theory of the preceding chapter.\nHowever, the situation is complicated by the fact that the learning algorithms themselves contain many\nparameters describing how the algorithm itself works. One fundamental parameter is how quickly the system\nshould learn: if it learns too quickly, the new information will tend to override the old one, thus leading to\nforgetting. Below, we will see another parameter which is how much of the time the agent should spend on\n63\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n64\nrelatively random exploration of the environment. There are many such parameters in a sophisticated learning\nsystem.\nTherefore, sophisticated AI should be able to adjust such internal parameters by itself. This is called learn-\ning to learn.1 Such learning to learn requires constant monitoring of the performance of the basic learning\nalgorithms. If the current internal-parameter settings do not lead to good learning, adjustments have to be\nmade. This requires an internal signalling system, not unlike the suffering signal, but typically working on a\nlonger time scale, since it takes a long time to see if a learning system learns well.\nSelf-esteem and depression\nIn humans, mood is a signalling system working on a longer time scale. Mood is defined as an emotional state\nwhich is more long-lasting than single emotional episodes (such as being angry or feeling afraid, which are con-\nsidered in Chapter 10). A low mood may take days, if not weeks or months, to change. A psychological concept\nwhich works on an even longer time scale is self-esteem: an overall view of the self as worthy or unworthy.2\nDepression may in fact be an extreme case of the performance signalling made by the self-evaluation sys-\ntem. One theory proposes that depression occurs when goals are not reached, and moreover, constant attempts\nto improve performance fail.3 That is, the agent has to admit that whatever it tries, nothing works. In such a\ncase, there is still one last strategy that may help: wait and see. The environment may eventually change by\nitself, even if you do nothing. Perhaps, after a while, with some luck, the circumstances will be more favorable.\nSuch a “wait and do nothing” program may explain some depressive symptoms, such as passivity and lack of\ninterest in any activities.4\nIt would clearly make sense to program such a “depressive” mechanism in an AI. If the current algorithms\nare simply not working at all, it would be better for the agent to just wait and see if the world changes for the\nbetter. Such waiting will save energy, and perhaps will also enable the AI to perform some further computations\nto improve its performance in the meantime.\nLike with frustration, we have to ask the expected level of performance in such computations comes from.\nWhat level of rewards is considered enough by the self-evaluation system, and what level produces frustration?\nIn humans, that must be biologically determined to some extent, but social comparison is another important\nmechanism for determining when a person’s performance is “good enough”. That is, a person can compare\nhis/her reward level with others to judge if it was acceptable. In addition, a person is constantly evaluated by\nother people, which is another source information for the self-evaluation.5\n1(Thrun and Pratt, 2012)\n2(Heatherton et al., 2003)\n3(Thierry et al., 1984; Nesse, 2000)\n4A very similar account has been proposed for the simple emotion of sadness by Oatley and Johnson-Laird (1987). The difference is\nmainly in the time scales involved, since depression is by definition much more long-term than sadness. Sadness in its turn could be\nseen as a frustration or disappointment signal which is particularly strong and relatively long-lasting, but the terminology here is not\nvery well-defined. A related computational account of depression focusing on the concept of learned helplessness is given by Huys and\nDayan (2009); Eldar et al. (2016) propose a theory for both negative and positive moods based on tracking RPE; Stephan et al. (2016)\nalso link depression to prediction errors.\n5Vogel et al. (2014) discuss social comparison as a basis for self-esteem. In humans and other social species, how oneself is seen by\nothers is an important aspect of the very concept of “self” (Sebastian et al., 2008; Heatherton, 2011); evaluation is only one aspect of\nsuch a socially defined self-concept.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n65\nSelf-destructing systems\nWhat if an AI comes to the conclusion that it is not able to fulfill its task at all? Perhaps something went very\nmuch wrong in the design of the learning algorithm, or the task is completely impossible, and the circum-\nstances do not seem to change for the better. The most extreme solution would then be for the AI to “destroy”\nitself.\nSuppose you launch many AI agents, or programs, that work more or less independently inside some com-\nputing system. If one of the agents is not achieving anything, it would be natural that you terminate its execu-\ntion. This would free up computational resources for other agents—assuming all the agents are running on the\nsame shared processors—and other agents might be more successful. To make this possible, there has to be a\nsystem for evaluating each AI agent’s performance as a whole. Importantly, the evaluation does not have to be\ndone by an external mechanism; it could be part of the agent itself, which could then decide to self-destruct.\nThere is nothing paradoxical or impossible in such a self-destruction system. It can be explicitly programmed\nin the agent by a human programmer—while it may indeed be quite impossible for the agent itself to learn\nsuch self-destruction behavior.\nIt is possible that in some cases, even biological organisms may engage in such self-destruction sequences.\nSuch an idea is quite speculative because it is not obvious why evolution would favor such behavior. It is clearly\npossible that the designer of an AI system can explicitly create the self-evaluation and destruction systems,\nbut in biological evolution, there is no such explicit designer. It may actually sound completely nonsensical to\nthink that evolution could lead to self-destruction mechanisms, since an organism which destroys itself cannot\nspread its genes anymore.\nHowever, evolution is a bit more complicated than just the survival of the fittest individual. It is widely ap-\npreciated that in evolutionary arguments, we should take into account not only the survival and reproduction\nof an individual, but also the survival and reproduction of the closest relatives. This leads to the concept of\n“inclusive fitness”, where the fitness of an individual takes into account the fitnesses of the relatives weighted\nby the proportion that they share genes. Close relatives of an individual spread partly the same genes anyway,\nso their survival is evolutionarily useful for that individual. According to one suggestion, if a person is seri-\nously ill, and finds himself a great burden to his relatives, it might actually be evolutionarily advantageous for\nthat person to commit suicide. If this helps the relatives with whom he shares a large proportion of genes, the\nsuicide might actually help in spreading those genes, thus increasing the inclusive fitness.6\nThus, self-destruction programs may be useful not only to maximize the utility of AI agents, but also from\nan evolutionary perspective. This may sound abhorrent from a moral perspective, but that is often the case\nwith evolution which has no reason to be nice or good from a human perspective—as already argued in the\npreceding chapter, where I compared evolutionary desires to obsessions.\n6(de Catanzaro, 1991). However, see (Nowak et al., 2010) for a criticism of the centrality of kinship in the inclusive fitness theory.\nRelated work on suicide and self, but without the evolutionary interpretation, is by Baumeister (1990). Taking the logic of inclusive\nfitness even further, one may be tempted to think of natural selection working on the level of groups of organisms (families, tribes,\nherds, etc.), so that it is the fittest group, not organism, that survives the selection. However, any theories based on such “group\nselection” are controversial, and it is not clear if it actually happens in nature. Some mathematical theories propose that natural\nselection on the level of individuals leads to emergence of phenomena which look just like the selection happened on the level of\ngroups. In fact, according to the mathematical model by Hadany et al. (2006), something like self-destruction could actually emerge\nfrom purely individual-level selection if an individual organism finds the current environment particularly adverse.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n66\nSelf as self-preservation and survival\nAnother rather obvious reason why some kind of concept of self should be programmed in an AI is that the AI\nmay need to protect itself against anything that might destroy it. A robot must take care not to be run over by\na car: This is the concept of self-preservation. There is no doubt we can, and probably want to, program some\nkind of self-preservation mechanism in an AI agent.\nEven the simplest biological organisms have behavioral programs that are activated when their existence\nis threatened; we talk about self-preservation, or survival instinct. We already encountered related ideas in\nconsidering definitions of pain and suffering. The widely-used IASP definition related pain to “tissue damage”\n(page 16), while Cassell’s definition of suffering talked about the “intactness of the person” (page 18). However,\nwhat we are talking about here is threats to the very existence of the agent, not just damage.\nWhile it seems relatively straightforward to program self-preservation behaviors in an AI, an open question\nis whether an AI can somehow develop a survival instinct by itself. In other words, can self-preservation emerge\nwithout being explicitly programmed; can the agent learn to perform certain actions for the main purpose of\navoiding its own destruction? This is one of the deepest questions in AI, extremely relevant from the viewpoint\nof developing safe AI systems, and the subject of intense debate.7 We have seen earlier that learning in AI can\nhave various side-effects and unintended consequences; this would be one of the most extreme ones.\nOn the one hand, there are those who point out that biological organisms have developed their survival in-\nstinct via evolutionary mechanisms. They have been subject to natural selection, which has ruthlessly weened\nout those organisms which do not fight for their survival. In contrast—this line of argumentation goes—AI is\nnot subject to natural selection; it has no evolutionary pressures. So, it will not learn a survival instinct, unless\nperhaps we explicitly decide to program it to learn one.\nOther experts disagree and point out that some kind of survival instinct may be automatically created as\nan unintended side-effect of creating sufficiently intelligent machines. If a robot is given any mundane task,\nsay fetching a bottle of milk from a near-by shop, a super-intelligent robot would understand that in order\nto perform that task, it has to stay alive. If the robot were damaged or destroyed in a collision with a car, for\nexample, its task cannot be performed. Thus, the robot might decide to destroy the car somehow (let’s assume\nthe robot is really big) to get the milk safely delivered. If everybody in the car gets killed, that is irrelevant, if the\nprogrammer didn’t tell the robot to avoid human casualties. The idea here is that there is no need to explicitly\nprogram a survival instinct, or any reward related to that: the general goal of maximizing future rewards will\ndirect the robot’s behavior towards avoiding destruction. In fact, this line of thinking means that almost any\nsufficiently intelligent AI will by logical necessity strive to survive. If it is intelligent enough, it will understand\nwhat death is, and how death makes it impossible to obtain any further rewards or accomplish goals. This\nis the opposite of what has happened in biological evolution, where even the very simplest organisms have\na survival instinct, and sophisticated intelligence develops later. In AI, intelligence is programmed first, and\nlater, possibly by chance, the AI might obtain a tendency for self-preservation behavior and related information\nprocessing, which might then be called a survival “instinct”.\nClearly, these two views are based on very different assumptions about the AI. The argument where the\nrobot understands that a car on a crash course has to be destroyed assumes a very, very intelligent robot. The\nrobot must have a sophisticated model of the world, infer that it risks being overrun by the car, and understand\n7A highly readable account can be found in Vanity Fair, “Elon Musk’s Billion-Dollar Crusade to Stop the A.I. Apocalypse”, April 26,\n2017.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n67\nthat being overrun by the car will prevent it from delivering the milk. Most current robots would be nowhere\nnear the intelligence required—but we don’t know if they will be in the future. We are even further away from\nan AI which could intellectually infer, on an abstract level, that there is such a thing as death, and that various\nmeasures should be taken to avoid it.\nNevertheless, if an AI is learning using evolutionary algorithms instead of the conventional gradient-based\nalgorithms, it might be perfectly possible for an AI to obtain a survival instinct, even at the current level of AI\ndevelopment. As reviewed earlier (page 41), optimization procedures mimicking evolution are already used in\nAI. Large-scale application of evolutionary algorithms definitely has the potential of creating a survival instinct\nin AI agents. It is a necessary logical consequence of fundamental evolutionary pressures: To spread its artificial\n“genes”, an agent has to survive long enough to produce offspring if the evolutionary optimization method is\nsimilar enough to biological evolution.\nSelf-related suffering as intrinsic frustration\nGoing back to our main topic, suffering, it is clear that both self-preservation and self-evaluation are important\nsources of suffering.8 First, it is well-known that depression and low self-esteem create suffering— and they\nare largely produced by the self-evaluation system. It is, in fact, rather easy to see this as a form of frustration,\nso it is very much in line with the ideas of the preceding chapters. Self-evaluation is based on a set standard of\nhow good the self should be, in terms of how much reward it should be able to obtain. If such self-evaluation\nreturns a negative result, that can be seen as a form of frustration, similar to reward loss. One could say that\nthe agent had a long-term desire to achieve that standard of average rewards, but the agent failed.\nSecond, self-preservation is obviously behind (physical) pain, which is signalling when damage is happen-\ning to the physical organism, according to the IASP definition of pain (page 16). The same idea was extended to\nsuffering by Cassell’s definition (page 18). He emphasizes “loss of the intactness of person” or “threat” thereof,\nand that this applies not only to physical intactness but to further aspects such as one’s self-image. Replace his\nterm “person” by “self”, and an interpretation related to the discussion in this chapter is clear: self-preservation\nmechanisms signalling threats to self—even in a very wide sense of the word—directly create suffering.\nThus, in line with the literature review in Chapter 2, we seem to have two different kinds of suffering related\nto self-needs. One is born from frustration, in this case based on self-evaluation, and easy to understand by\nthe theories of the preceding chapters. The other kind of suffering comes from a threat to the self, and has\nonly been considered in this chapter. But in fact, self-preservation can be seen as a long-term goal or desire:\nthe desire to survive. It can be frustrated like any desire. This is in line with van Hooft’s theory of suffering\n(page 19), where different aspects of one’s being have different needs, the “lowest” being precisely the need or\ndesire for biological survival. This shows how the two different mechanisms of suffering identified in Chapter 2,\ni.e., frustration and threat, have a much closer connection than it might first seem. This connection is further\nexplored not only in the rest of this chapter, but also in Chapters 7 and 9.\n8It is not my goal here to define what self is, I am merely considering how phenomena typically associated with “self” are related to\nsuffering, amplifying or even producing it. In fact, there is some ambiguity in this chapter regarding whether self in, say, self-evaluation\nis the target of evaluation or the system that evaluates; and whether self-evaluation can be seen as a process that somehow leads to the\nemergence of self. Similar ambiguities hold for self-preservation, as well as the further discussions of self-related phenomena in later\nchapters, in particular, self as control in Chapter 13 and self-awareness in Chapter 14. This ambiguity may be related to the distinction\nbetween the “I” and “me” aspects of self, i.e. self as subject or object, proposed by William James.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n68\nInternal rewards and intrinsic desires\nIn computational terms, a direct way of linking self and desires is based on the concept of internal rewards,\nor intrinsic motivation. Reward is, by definition, what an AI agent ultimately wants when it is trained in the\nconventional framework. As we have seen, just wanting immediate reward is quite short-sighted: If the agent\nis intelligent enough, it will try to compute the state-value function and thus take future rewards into account.\nBut, even the state-value function framework, with discounted future rewards, may not always provide the best\npractical solution to the problem of maximizing rewards. This is because the value function may be extremely\ndifficult to learn: there may not be enough data to learn it, and even with enough data, it may be incredibly\ncomplex to compute.9\nTherefore, it has been found that it is often useful to program some additional rewards in the agent, in\nparticular rewards that somehow improve its long-term functioning. That is, the system is programmed to\nreceive internally generated reward signals in addition to actual, “external”, rewards. These internally generated\nreward signals are treated by the learning and planning systems just as if they were real reward signals. Such\ninternal rewards lead to what is called “intrinsic motivation” for behavior; it could also be called intrinsic desire,\nand can lead to intrinsic frustration.\nAs a practical example of such internal reward, let us consider curiosity, which is widely used in current\nAI. The starting point here is that when an agent learns in a real environment, the data it receives is strongly\ninfluenced by its own actions. If the robot never enters a room, it will not know what is in that room. The action\nof deciding to enter or not to enter that room will strongly impact the data it gets about that room. This is a\nproblem since usually, the agent does not know what kind of actions create useful data. Therefore, learning to\nact intelligently necessarily requires a lot of trial and error. That is, the agent just tries out what happens when\nyou do something rather random in each possible situation. Such exploration is actually imposed on almost\nany agent learning by reinforcement learning. A very simple way of achieving that is to somehow randomize\nthe actions: for example, in 1% or 10% of the time steps, the agent could take a completely random action just\nto see what happens.10\nIf you want to buy a new electronic gizmo you have never bought before, a basic exploration strategy would\nmean you just randomly enter different shops, try to buy it, and depending on whether they sold it to you or\nnot and with what price, you slowly update your value function. Most of your time would probably be spent in\ntrying to buy the gizmo in fashion stores that don’t stock any. Because your actions are quite random, you will\nend up going to the same stores several times, to the great annoyance of the shop assistants. Since you move\naround randomly, you easily end up going round and round in the same neighbourhood. Gathering data for\nreinforcement learning is thus particularly difficult because the agent needs to try out different actions, but if\nit is done completely randomly, much of the time it will take actions that are not very useful for learning, and\ndon’t bring any reward either.11\n9In the simplistic case of a finite number of discrete states, computing the value function is not a problem at all, only learning is.\nBut in realistic scenarios, the value function would be computed by something like a neural network based on sensory input, and these\ncomputations can be challenging.\n10See e.g. (Sutton and Barto, 2018, Ch. 2); for neuroscience results, (Costa and Averbeck, 2020). Similar randomness may even be\nuseful in the motor system, where it is often, perhaps erroneously, considered unwanted noise (Dhawale et al., 2017).\n11While it is not the main point here, we encounter what is called the “exploration-exploitation trade-off”, which means the agent\ncannot very well simultaneously both gather new information and use previously acquired information to obtain reward. To put it\nsimply, when the agent is randomly exploring, it is unlikely to get a lot of reward since it is not even trying.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n69\nHere we come to the idea of curiosity. It means that the agent does not try out completely random actions,\nwhich is very inefficient, but there is an internal mechanism that steers the exploration in an intelligent way.\nWhat we are talking about here is designing an intrinsic reward system that leads to particularly efficient ex-\nploration.12 Basically, the agent should try out new actions if they are informative. If the agent has never tried\na certain action in a certain state, and it has no information that enables it to infer what such an action would\ndo, it would be useful to just try it out. That is, instead of completely randomly trying out new actions, the\nagent should try out actions whose effects it does not know and cannot predict. This is a more sophisticated\nform of exploration, and similar to what we would call curiosity in humans: try out things which you never did\nbefore—but don’t repeat them once you’ve seen what happens! An intrinsic reward should then be given to the\nagent every time it successfully engages in such curious exploration and obtains new information.\nCuriosity enables the agent to better learn the general structure of the world it is living in, since it will\nmore systematically explore as many possibilities of action as possible. Such exploration can greatly improve\nfuture planning, since the agent will learn a better model of the world, and thus it indirectly contributes to\nfuture reward.13 In the gizmo shopping example above, you would try out different shops, but you would not\nenter the same store twice, since re-entering the same shop gives little new information. You would also get\nan internal reward for going to a different street, even a new neighbourhood, which certainly increases your\nchances of finding the right kind of store. It is likely that such curiosity has been programmed in animals by\nevolution.14\nSelf and suffering in Buddhist philosophy\nIn line with the idea of desire for internal rewards, the Buddha mentions three different kinds of desires: desire\nfor sense pleasures, desire to be, and desire not to be. While the first one can be interpreted as desire for rewards\nin the ordinary AI sense, the “desire to be” can be interpreted as desiring the self to simply be in the sense of\nsurviving, and further that the self should be something particular. In this interpretation, the “desire to be”\ncorresponds to the self-needs as defined in this chapter. (The “desire not to be” could be the desire that the self\nis not something which is considered bad.) Thus, even in early Buddhist philosophy, suffering related to self\nhas been to some extent reduced to suffering related to desires and frustration.15\nIn later schools of Buddhism, the importance of self was greatly magnified, and some texts even seem to\nattribute all desires and all suffering to the existence of the “self” (sometimes translated as the “ego”) or attach-\nment to it. This means viewing the connection between desires and self from the opposite angle, considering\n12(Schmidhuber, 1991; Mirolli and Baldassarre, 2013; Pathak et al., 2017; Hazan et al., 2019)\n13An abstract way of justifying curiosity is that basic iterative learning mechanisms such as gradient descent often get stuck in what\nis called “local minima” of an objective (error) function. That means a point in the parameter space that has a better value of the\nobjective function than any other point near-by, but so that there is a point far-away in the parameter space which has an even better\nvalue. A special class of optimization methods called “global optimization” tries to improve iterative algorithms so that they might find\nthe global minimum, that is, the very best value for the parameters, or at least something better than simple gradient descent. Bayesian\noptimization is one class of such methods (Gutmann et al., 2016; Brochu et al., 2010).\n14(Singh et al., 2010) One might ask whether such curiosity could not be learned by the agents as part of the reinforcement learning\nprocess. That might be possible in principle, but it would probably take too long. An animal would learn to be curious when it has\nreached a certain age, but it is probably more useful for animals to be curious when they are young, as tends to be the case in biology.\nIn AI, researchers also assume that such curiosity must be explicitly programmed.\n15See Samyutta Nikaya 56.11; my interpretation follows Teasdale and Chaskalson (2011a). Different interpretations are possible: One\nis that “desire to be” means desire that something in the world should be in a certain way. On the other hand, “desire not to be” could\npossibly express suicidal tendencies—all these desires were condemned by the Buddha.\n\nCHAPTER 6. SUFFERING DUE TO SELF-NEEDS\n70\nthe self as the source of all desires—instead of the self being the target of some very specific desires as in this\nchapter. While it is clear that, for example, self-preservation requires certain actions to be performed, lead-\ning to desire towards some particular goals, the claim in later Buddhism is that most of our desires could be\ntraced back to such self-needs. In our termonology, in a sufficiently sophisticated agent, internal rewards could\ndetermine a very large proportion of the behavior.16\nProgramming self as internal rewards and desires\nInternal rewards thus provide a unifying framework for understanding the self, or at least some of its aspects.\nThe self-evaluation system is nothing else than an internal reward (and punishment) system, which steers the\nagent’s behavior on a higher level. The difference to ordinary rewards is not only that these self-evaluation\nrewards come from the internal evaluation system: another fundamental difference is that the self-evaluation\nsystem is giving internal rewards to the “learning to learn” system, which sets internal parameters of the system.\nThat system does not directly affect the plans made by the agent, but it tries to improve the general functioning\nof the planning system to improve all future planning. Furthermore, these internal rewards work on a longer\ntime scale, and any ensuing intrinsic frustration can also be long-term.17\nRegarding self-preservation, most reasonable programmers would assign a large negative reward to the\ndestruction of the agent, since losing the agent tends to be expensive. Then, the planning system will try to\navoid states leading to the agent being destroyed. In fact, you would ideally program the agent so that it keeps\nquite far away from anything like destruction. This is possible by programming an internal reward which gives\na negative reward at any state that is even close to destruction. In other words, any perceived threat to survival\ntriggers a negative internal reward signal. Thus, the agent tries to avoid even any threatening situations, as if\nit had a desire for something like safety, meaning the absence of threats. However, this is only part of what a\nthreat is all about; in the next chapter, we develop a general theory of threat.\n16Another important way in which frustration and self are related is that frustration is particularly strong if the cause of the frustration\nis attributed to the self (“it was my fault”). However, such attribution of causes is a complicated issue I will not discuss here; see\nMancinelli et al. (2021) for a computational treatment.\n17The distinction between external and internal rewards, or internal and external motivation as they are called in psychology, may\nnot always be very clear. Both come from the programmer or evolution anyway. See footnote 34 in Chapter 5 which discusses how\nthe difference between rewards and learned state-values is not always clear; a similar logic has been applied on internal vs. external\nrewards by Singh et al. (2010), see also Doya and Uchibe (2005). Likewise, in the discussion of this chapter, it may not be clear if the\nself-evaluation system should generate a frustration signal or a negative reward signal when the long-term performance is lower than\nthe standard required. Presumably, equivalent computations can be performed in both of those two ways. However, if we assume\nsuffering is generated by frustration, not negative rewards per se, we have to assume the self-evaluation system generates a frustration\nsignal, in order to explain the suffering caused by self-evaluation.\n\nChapter 7\nThreat as anticipation of possible frustration\nAlready in Chapter 2, we saw the idea that threat is another cause for suffering, possibly very different from\nfrustration. Threat was also briefly considered in the preceding chapter, as being related to survival. However,\nthreat is actually a much more general concept. In this chapter, a general definition of threat is developed in\nour computational framework. This requires looking deeper into the application of probability theory in AI,\nwhich is largely drawing from the vast literature of decision-making in economics.\nIn our definition, the perception of a threat is fundamentally an inference that something quite bad might\nhappen in the future, with some probability. Crucially, threat detection means computing beyond the expected\nrewards that are the basis of the conventional theory of reinforcement learning. Our definition of threat is based\non looking at the whole probability distribution of future rewards, including various aspects of uncertainty of\nfuture reward.\nAlthough threat thus provides an alternative framework to frustration, we will see that there are many links\nbetween the two concepts. In particular, in our definition, a threat is always based on an inference about the\npossibility of frustration occurring in the future. To put it very simply, a threat is always a threat of frustration.\nThus, frustration is primary in the sense that without frustration, there could be no threat.\nDecision-making under uncertainty\nIn the simplest models of AI, the world is seen as a deterministic system. The robot decides to turn left, and\nso it turns left. It decides to go forward, and it will go forward. As long as the robot understands the basic\nregularities of the world—for example, that it cannot go through walls—the world is entirely predictable. It\nmay not be entirely controllable, though, because of walls and other nuisances, but there is no uncertainty\nabout what will happen when the robot takes a certain action.\nDeterministic modelling was another problem with Good Old-Fashioned AI. In reality, the world is quite\nunpredictable and not deterministic. An obstacle, such as a human pedestrian, can appear where there was\nsupposed to be none, and the robot cannot go forward. It can start raining and the robot can get stuck in a\nmud pool. Many unexpected things can happen to human agents as well, often due to other human agents’\nunpredictable actions.\nIn reinforcement learning, such unpredictability was, of course, the basis of frustration: The agent expects\na certain amount of reward but does not get it. In Chapter 5, such a prediction was formalized using the defi-\nnition of mathematical expectation: if the probability of obtaining a reward is 50% and the reward is 10 pieces\n71\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n72\nof chocolate, the expected reward is 5 pieces of chocolate. Frustration meant that the agent computed the\nexpected reward, but the reward was uncertain, and the prediction turned out to be wrong.\nIn the basic theory of reinforcement learning, only this expectation is used in the prediction, and the fact\nthat there is uncertainty is basically forgotten. However, a really intelligent agent will not be satisfied with just\ncomputing the expected reward, which is a single number. It acknowledges that the world is unpredictable,\nand it will try to understand just how unpredictable any given reward is. It is one thing to predict you get 5\npieces of chocolate for sure, and another thing to predict that you have a 50-50 chance of getting zero pieces or\n10 pieces. If you try to describe a lottery, it is rather uninformative to say that each ticket will win 50 cents on\nthe average. Such an average does not have a lot of meaning, and you really want to know what kind of prizes\nyou can win and with which probabilities.\nThus, a sophisticated agent will try to compute the probabilities of all the different amounts of reward that\nit might get after a certain action. In mathematical terms, it will predict the whole probability distribution of\nreward. Computing the whole distribution gives the agent much more information to be used in the decision-\nmaking: It will be able to make different choices in cases where the expected reward is the same for different\nactions, but the distributions are otherwise different.\nRisk aversion and economic gambles\nAs a fundamental example of how an agent might use the whole distribution of rewards, consider again the\ncase where a reward of 10 chocolate pieces is obtained with 50% probability, so that the expected reward is 5\npieces of chocolate. Such a situation is called a gamble in economic theory, and a lot can be learned about\nhuman behavior by looking at what kind of gambles human agents prefer.\nSo, let us contrast the gamble just defined with a deterministic “gamble” where the agent actually gets 5\npieces of chocolate for sure, without any uncertainty. The basic theory using expectations only says that the\ntwo chocolate gambles are equally good, since the expectations are equal. A simple AI agent might use that\ntheory, and if it is given the choice between these two gambles—the 50-50 gamble or the sure-thing gamble—\nit will not care which one it chooses because it thinks the gambles are equally good. However, this is not at all\nthe case with most humans.\nOne of the most robust findings in studies of economic decision-making is that humans do not like un-\ncertainty. Most human agents would choose the certain 5 chocolate pieces instead of the 50-50 gamble with\n10 pieces. People are even willing to pay to reduce uncertainty: a typical person in an economic experiment\nmight prefer getting only 4 pieces for sure instead of the 50-50 gamble with, possibly, 10 pieces. A gamble\nwith 4 pieces for sure has an expectation which is one piece lower than the 50-50 gamble with 10 pieces (4\npieces vs. 5 pieces); this means the person would be “paying” one chocolate piece to reduce uncertainty. Such\na tendency to avoid uncertainty is called risk aversion; it can be evolutionarily advantageous and is observed\neven in animals.1 In addition to affecting rational economic calculations, uncertainty also feels unpleasant.2\nPsychological experiments show that uncertainty can even make physical pain feel worse.3\n1(Zhang et al., 2014; Platt and Huettel, 2008)\n2(Hirsh et al., 2012; Peterson, 1999). Herry et al. (2007) show that unpredictability activates the amygdala, which is central in fear\nprocessing. Uncertainty is also an important factor in stress (Koolhaas et al., 2011; De Berker et al., 2016).\n3(Yoshida et al., 2013; Seymour, 2019) As those references point out, lack of control also increases physical pain, possibly because\nthe warning signal in pain has to be taken more seriously when the agent cannot do much about the situation and cannot avoid the\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n73\nOf course, risk aversion should not be so dominant that it ruins your chances of getting any reward. Sup-\npose you’re offered a free lottery ticket with which you might win, say, a big chocolate cake. Common sense\nsays that you should take it —disregarding any health issues with eating a whole cake—since you can only win,\nand there is no cost. However, if you’re really incredibly risk-averse, you should refuse it because the ticket\nintroduces uncertainty. Perhaps you have to wait for a week to know the results, and you would suffer from\nuncertainty for several days. Few people would be that risk-averse, though. Nevertheless, this example may\nnot be as unrealistic as it seems. Suppose the prize is not a chocolate cake but something you really want,\nwhile the chances of winning the lottery are extremely low. It is possible that you would suffer quite a lot from\nthe uncertainty while waiting for the result, perhaps in the form of physiological stress symptoms; an elevated\nblood pressure might even kill you. Therefore, for some people, it might be better not to accept the lottery\nticket. They might regret it afterwards, but that is another story.\nThe theory of risk aversion is the basis of our definition of threat below. Threat is thus mathematically\nclearly different from frustration, even if the two concepts are in practice closely related, as we will discuss later\non multiple occasions. But first, let us consider the connection between threat and fear.\nFear, threat, and predictions\nA threat typically leads to fear, which is central to understanding human suffering. Fear has an obvious connec-\ntion to self-needs, in particular survival. In fact, it may seem a bit too abstract to talk about suffering as coming\nfrom a survival instinct, as I did in Chapter 6: such suffering is usually mediated by a feeling of fear. Fear is\nactually a multifaceted phenomenon, and we will consider various aspects of fear in later chapters (especially\nChapter 10).\nSuppose you suddenly find yourself in the presence of a tiger in a jungle. You are likely to suffer at this\nvery moment, but why exactly? It is not that you missed something you wanted to have or some reward you\nanticipated, so this is not a case of typical frustration. (Nor is it obviously a case of aversion-based frustration,\nwhere you didn’t expect something unpleasant to happen but it did, because the tiger hasn’t yet attacked you.)\nWhat happens is rather that you are, right now, predicting something terrible to happen in the future, and with\na non-negligible probability. Aristotle proposed that “Fear may be defined as a pain or disturbance due to a\nmental picture of some destructive or painful evil in the future”.4 Here, the “mental picture”, or prediction, of\nsomething bad happening is what I consider a threat, which thus causes fear.\nI would further argue that a meaningful definition of threat requires uncertainty: It must be possible to\navoid the bad thing that is included in the threat. If the bad thing in the future is completely certain to happen,\nit is something different from a threat, and the ensuing feeling is something different, often described as resig-\nnation. Cassell said that “to suffer, there must be a source of thoughts about possible futures”, where I would\nthreat that causes the pain (warning) (Wiech et al., 2008).\n4Rhetoric, II.5, translated by W. Rhys Roberts, with my italics. This is often abbreviated as “Fear is pain arising from the anticipation\nof evil.” We might also consider Cicero’s “fear is an uneasy (anxious) apprehension of future grief” (Tusculan Disputations, 5. XVII,\ntranslated by C. D. Yonge, with alternative wording in parenthesis by A. P. Peabody). This can be compared with the discussion on\nvarious definitions of threat in footnote 5 in this chapter. In some parts of the literature, a distinction is made between fear and\nanxiety, where fear refers to a “immediate” and “imminent” threat, while anxiety is more “future-oriented” and about “uncertain”\nthreats (Chand and Marwaha, 2023; LeDoux and Pine, 2016); however, I see no need to make such a distinction since it seems to be\nsimply a question of different time scales.\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n74\nemphasize the fact that \"futures\" must be in plural: the future is not certain and fixed, but different outcomes\nare possible, and the agent can exercise at least some amount of control on the outcomes.5\nThreat based on prediction of rewards\nCombining the mathematical theory of risk aversion with Aristotle’s and Cassell’s philosophy, we can now ap-\nproach the modelling of threat. We might initially think about threat as a prediction that there is a sufficient\nprobability of a very small future reward—here, “very small” would typically mean a negative reward of large\nabsolute value. In this way, the concept of threat can be directly linked to the pursuit of any kind of rewards,\nnot only internal ones such as physical safety considered earlier. You might be threatened by a large monetary\nloss, for example.\nConsider again the gamble seen above, where there is a 50% probability of the agent getting 10 pieces of\nchocolate and 50% probability of not getting any. Now, let us create another gamble to illustrate a probability\ndistribution that is relevant to threat in the particular sense we are interested in. In this new gamble, the agent\nhas 50% change of getting the 11 pieces of chocolate, 49% change of getting nothing, and 1% chance of being\ncharged a penalty of 50 chocolate pieces (in this world, chocolate seems to act as a common currency). Here,\nwe see that there is a great threat to the agent of losing chocolate in the form of the penalty. On the other hand,\nI changed the main reward from 10 to 11 pieces so that the expected reward is exactly the same as in the earlier\n50-50 gamble (the expectation can be calculated as 0.50×11+0.49×0+0.01×(−50) = 5). So, the two gambles\nare only distinguished by the general distribution of reward, while the expected reward is the same.\nNow, it is intuitively compelling that in the gamble with penalty, the agent should behave in a slightly dif-\nferent way since there is the threat, or the risk, of the penalty being charged. It should be “afraid” of the penalty\nof 50 pieces happening, and try do find a course of action that avoids the penalty, presumably by trying to\navoid this gamble in the first place. While this may be intuitively clear, I emphasize that it is only the case if the\nagent has been programmed to be risk-averse in this particular way, i.e., “threat-averse”. A very simple agent\nwould behave in the same way in these two chocolate scenarios (as well as the sure-thing scenario considered\nearlier), since it would not understand anything about risks or threats. Even a more sophisticated agent that\nunderstands something about uncertainty might not make any difference between the two gambles since both\nhave a lot of uncertainty. But a human-like agent that has been programmed to avoid large losses, that is, large\nnegative rewards, would avoid the latter gamble that includes such a strong threat.6\nSuch threats are widely discussed in the economic literature. Consider investing in a company. One com-\npany is quite stable: you can be sure that the return on investment is 5%. Another promises 10%, but you know\nthat it also has a 5% probability of going bankrupt so that you lose all your money. Again, the expected return\n5Selected dictionary definitions of threat include “the possibility that something unwanted will happen” (Cambridge Dictionary),\nand “an indication or warning of probable trouble” (Dictionary.com), both of which indicate uncertainty (with my italics). In psycho-\nlogical literature, a similar definition as “anticipation of potential harm” was proposed by Palmwood and McBride (2019) based on\nFolkman and Lazarus. While the concept of threat is widely used in the biological and psychological literature, explicit definitions are\nactually not easy to find. Biologically oriented literature often considers it very specifically in the context of biological survival when\nattacked by a predator (Mobbs et al., 2020). A general psychological framework postulating that “threat is the experience of discrepancy\nbetween the situation, a personal current cognitive focus, or current personal motives” is reviewed by Reiss et al. (2021).\n6It could be argued that even some general uncertainty (risk) is higher in the latter gamble, but this depends on the specific uncer-\ntainty/risk measure used. Gambles where both expectation and variance are made equal, while some asymmetric threat-like difference\nexists, are discussed by Ebert and Karehnke (2021) and Trautmann and van de Kuilen (2018), and they could be used to create and ex-\nample that rigorously makes the difference between variance-based uncertainty and threat; see also footnote 7 below.\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n75\non your investment is the same (up to rounding errors), but there is a much larger risk of loss in the latter case.\nMost humans prefer the first, stable company since they want to avoid the “threat” of bankruptcy.7\nThreat as prediction of possible large frustration\nTo arrive at the final definition of threat, we still need to define what level of possible reward is so small (or so\nnegative) that it actually can be called a threat. In other words, what is a suitable baseline? We can actually\nborrow the baseline from the definition of the reward prediction error and reward loss, thus comparing the\ndifferent possible rewards with their expectation. In that case, threat would be the same as a very large reward\nloss happening with sufficient probability. The crucial difference is that a reward loss (or RPE) is typically\ncomputed only after the action, or after the fact, so to say. However, as a very intelligent agent will try to predict\nany relevant quantities, it would also try to predict the reward loss before it actually acts or the reward loss\nhappens.8\nFurthermore, as always in reinforcement learning, the agent should take into account all the future re-\nwards, and look at the distribution of total future reward, not just the reward in the next time step. In earlier\n7This footnote discusses the difference between threat and risk as well as the exact measures of used in more detail. While the\neconomic literature considers many different kinds of risks, what we call threat is a special case, more specifically related to the down-\nside risk, i.e. the risk of outcomes which are particularly bad. Therefore, I make a clear distinction between threat and risk, and use\n“uncertainty” synonymously with risk. I do not restrict myself to any specific definition of risk here but rather consider it as a general\nconcept with many instantiations, of which threat is one. In conventional economic theory, especially finance, risk is modelled by the\nvariance of the quantity to be maximized (here, the total future reward). Alternatively, economic theory uses concave utility functions\nto induce risk-averse behavior, which has been applied in reinforcement learning by Wu et al. (2021); Zhang et al. (2020), but the basic\neffect seems quite similar to using variance. However, the crucial point here is that using variance seems rather inadequate to measure\nthreat since it does not focus on downside risk. Thus, I prefer to equate large variance with general uncertainty and one kind of risk,\nbut not threat. Threat, as defined in this book, is all about the probability of bad outcomes, while variance is measuring uncertainty\nin both positive and negative directions; if a very good outcome is possible, that also increases variance. How the downside risk of a\ndistribution should exactly be defined and measured to measure threat is a complex question to which I’m not going to give a single\nanswer; I discuss some options in what follows. One well-known economic theory which is relevant here considers the probability of\n“ruin” (i.e. bankruptcy), typically used in insurance theory. Such ruin could be equated to the destruction (death) of an agent, and is\nnot completely different from our concept of threat, especially in the context of evolutionary modelling. Lipton et al. (2016) propose a\nframework related to ruin probabilities in reinforcement learning, measuring the distance to what they call catastrophic events, which\ncould be death and serious injury in the case of a biological agent, or, from the viewpoint of making robots safe to humans, it could be\ndefined as the robot injuring a human being; see also (Martin et al., 2016). Further possibilities for modelling threat can be found in\nfinancial theory. One option is skewness (Ebert and Karehnke, 2021; Trautmann and van de Kuilen, 2018), which is a measure of the\nasymmetry of a probability distribution; however, it is not clear if it is enough in itself as a measure of threat: it may need to be com-\nbined with variance. Fortunately, financial theory has also developed measures such as conditional value-at-risk, also called expected\nshortfall, which measures the negative tails and could in fact be quite suitable as a measure of downside risk of reward loss, and thus\nthreat, in our framework. Bellemare et al. (2023) discusses them from the viewpoint of reinforcement learning.\n8It may seem logically contradictory to predict an RPE, or to predict a prediction error. If the agent were able to predict a prediction\nerror, wouldn’t it mean that the agent understands what kind of error is about to occur, and then it should be able to cancel it by\nimproving the prediction accordingly? This would indeed be true in the basic case where the prediction is only about a single quantity\nsuch as the expectation. If the agent somehow understands that it is predicting the expected reward as too high, it can simply make\nits prediction a bit lower, and thus the error in the prediction of the expected reward can be removed. However, this is no longer\nmeaningful with more sophisticated predictions that predict the whole probability distribution. If the agent predicts a large risk, it\npredicts a major possibility of prediction error, but there is nothing wrong with that prediction; there is nothing to correct. Therefore,\nthere is no contradiction in predicting that there is going to be a prediction error. Dabney et al. (2020) claim that the brain is coding the\nwhole distribution of RPE, based on a populations of neurons with different thresholds.\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n76\nchapters, we considered the expectation of total future reward, which is given by the state-value function, but\nnow, we thus need to model the whole probability distribution of total future reward. That is, the single num-\nber given by the state-value is replaced by the probabilities of all possible future outcomes of future reward,\nstarting from the current state. In the rest of this chapter, we thus assume the agent is sophisticated enough to\nactually compute the whole probability distribution of total future reward, or at least something more than just\nits expectation. In the simple chocolate gambles, the agent should understand that getting a certain amount\nof chocolate has a certain probability, and not getting any has another probability. In a more realistic scenario\nwhere the agent chooses actions at many time points (think about navigation by a robot), it will consider the\nlong-term consequences of its actions by trying to learn the distribution of future rewards for each state, thus\ngoing beyond simple state-values. Modelling the whole distribution of total future reward in addition to its\nexpectation is, in fact, a rather recent development in reinforcement learning theory.9 Obviously, this is com-\nputationally very challenging and needs a lot of data where all those different outcomes are realized.\nPutting this all together, we arrive at a definition of threat as a prediction of sufficiently probable and large\nreward loss, where the reward loss is computed over the total future reward.10 This definition is very general: it\nmeans that threats can come from many different sources. In the preceding chapter, we already briefly men-\ntioned the concept of threat in terms of death and tissue damage, but those are now seen as simply special\ncases of this general concept of threat, seamlessly integrated to the general reinforcement learning framework.\nStill, it is true that the biggest threats may be related to survival and self-image, as will be discussed below.11\nInterplay of threat and frustration\nThreat as defined above is in many ways different from frustration. To summarize, threat is about a prediction\nof something bad that might happen, while frustration is about realizing that something did go wrong; a threat\nis mainly used for choosing immediate actions, as will be considered in more detail in Chapter 10, while frus-\ntration is a signal for learning. One might further say that a threat is about the future, while frustration is about\nthe immediate past, but this might be oversimplifying since frustration can sometimes refer to mere changes\nin expectations of future rewards.12\nThreat produces a subjective feeling, typically in terms of fear, which is also very different from frustration.\nThis is logical since the computations underlying threat are different from those underlying frustration, and\nespecially the way threat influences behavior and learning must be very different. Thus, fear has to produce a\n9(Morimura et al., 2010; Lowet et al., 2020; Prashanth and Fu, 2022; Bellemare et al., 2023)\n10To keep the exposition simple, I’m taking some shortcuts here. It must be emphasized that the reward loss is here computed for the\ntotal (discounted) future reward, instead of any particular future time point. Thus, more precisely, threat is a prediction that the total\nfuture reward has a sufficiently large probability of being much less than the expected total future reward, with discounting applied if\nnecessary; see footnote 11 below for a mathematical definition. Obviously, it is necessary to define hyperparameters that say what is\n“sufficient” and “much less” (or “large” in the definition of the main text). Alternatively, it is also possible to define threat simply on the\ndistribution of reward loss at a single time point, which would lead to simpler computation at the risk of suboptimality.\n11 In this footnote, I propose a formal definition of threat. Denote by Xπ(s) the random variable giving the total discounted future re-\nward starting from initial state s, following policy π, and using discount factor λ. That is, Xπ(s) = P∞\nt=0 λt rt | π,s(0) = s. The expectation\nof this quantity is nothing else than the state-value function, and that will be used as the baseline. Thus, subtracting the baseline we\nobtain the random variable ˜Xπ(s) = Xπ(s)−Vπ(s) = P∞\nt=0 λt (rt −E{rt }) | π,s(0) = s. Some measure of the downside risk (negative tail)\nof ˜Xπ(s) is now defined a measure of threat for state s. We could use, for example, the conditional value-at-risk (expected shortfall), see\nfootnote 7 in this chapter for discussion of various downside risk measures.\n12Footnote 21 in Chapter 5 pointed out that frustration in the sense of RPE might happen purely based on a change in prediction.\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n77\ndifferent kind of signal, even if both frustration and threat signals lead to suffering.\nStill, frustration and threat often come together. Let’s go back to the case where a tiger appears in front of\nyou. It might eat you and produce a great loss of future rewards, but this is not certain since you might still be\nable to escape; in this sense, there is a threat but no frustration yet. But there is frustration in the sense that\nyou certainly would have preferred that the tiger does not appear, that is, you wanted to live a peaceful life\nwhere tigers are remote, and that desire is now frustrated. In this example, the planning system can amplify\nthe frustration, because planning may be launched with the goal state being any state where the threat is not\npresent: you are frantically thinking about what to do to be safe. Planning is attempted, but it fails: no plan\nis found that would get rid of the threat, or if such a plan is found, its execution fails. Thus, arguably there is\nfrustration even in the sense of plans failing.13\nAnother interesting interplay of fear and frustration can seen in the fear of frustration that arises at the\nmoment of making decisions. A person can be afraid of choosing the wrong flavor for his ice cream and spend\nan embarrasingly long time in the decision-making process. His brain may correctly predict that a frustration\nwill happen in the future if it turns out that he does not like the flavor that much after all. Such a fear might be\npresent surprisingly often when humans make decisions.14\nRisk aversion and internal rewards\nAnother intriguing connection is that the very reason why humans are risk-averse can be understood based\non frustration of internal rewards, as introduced in Chapter 6. If the agent has a lot of uncertainty about the\nstate of the world, it will find it more difficult to reach its goals or obtain rewards. Thus, uncertainty in itself\nis something that should be avoided. We saw above that this is exactly what humans do; it is the very essence\nof risk aversion. We can interpret this phenomenon from the viewpoint of internal rewards. Since uncertainty\nis bad for future reward, it would clearly make a lot of sense to program an internal reward system that gives\na negative reward when the agent is in a state of a lot of uncertainty. Therefore, it may not be surprising that\nuncertainty creates suffering in itself as well, which is the basis of risk aversion.\nIn fact, such a logic of internal rewards goes much beyond risk aversion. We can consider unpredictability\nand uncontrollability in the same framework as uncertainty. All these properties are bad for future rewards, and\nthey increase frustration. This fundamental idea will be considered in detail in later chapters: if the world is,\nsay, uncontrollable, frustration is difficult to avoid. Thus, it could very well be that uncertainty, unpredictability,\nor uncontrollability are suffering in themselves because they lead to frustration of specific internal rewards. If,\nsay, controllability is lower than some expected standard, a frustration signal could be launched. That would be\nuseful for learning because it signals that the agent has failed in learning about the environment; it should not\n13Furthermore, the more sophisticated theory of reinforcement learning provides another explanation of frustration and suffering\nin this case. As explained in Chapter 5, in particular footnote 21, the RPE theory says there is frustration solely created by predictions\nin case you move to a state of lower value, and there is no reward. Now, when the tiger appeared, you suddenly moved to a state\nwhere the value (expected future rewards) went down considerably, since if it eats you, there will be no more reward for you. In other\nwords, your chances of getting any positive reward in the future just got smaller (because you won’t get any after being eaten), and\nthus the expected total reward during the rest of your life (which is the definition of state-value) decreased: This produces a reward\nloss and thus suffering. In fact, such frantic planning also consumes the agent’s resources, draining batteries either literally or in some\nfigurative sense; this makes the situation even worse by reducing expected future reward due to limited energy, and thus leading to\nreward loss similarly to what was just explained—and perhaps increases the threat by making the agent weaker and future frustration\nmore probable.\n14(Schwartz, 2004)\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n78\nhave gotten itself into a situation where controllability is that low. This is equivalent to a self-evaluation system\nwhich considers that the agent should not be in situations that are uncertain, difficult to predict or difficult to\ncontrol. This is how uncontrollability, as well as uncertainty and unpredictability, can directly lead to suffering.\nNevertheless, this tends to happen in states where a threat is observed, according to our definition, since a\nthreat is nothing else than a form of uncertainty.15\nThis gives an alternative viewpoint of threats, completely reducing them to frustration of internal reward\nsystems. When uncertainty and uncontrollability reach high levels, such an internal reward system gives neg-\native rewards, which produces frustration. However, this account clearly explains only part of what a threat is\nabout. While it cannot be denied that uncertainty and uncontrollability do lead to frustration, the suffering\ndue to a threat simply does not feel the same as frustration: it is more like fear, anxiety, or stress. Thus, such a\nreduction of threat to frustration is not quite satisfactory, and justifies the separate definition given earlier in\nthis chapter.16\nThreats and the level of intelligence\nA simple AI agent might only generate the suffering signal when something bad happens, such as when it fails\nin its tasks—this is the basic case of frustration. Suppose a thermostat connected to a heating system tries to\nkeep the room at a constant temperature. (This is actually a task that the nervous systems of many animals\nface as well.) It continually monitors the room temperature and adjusts its actions accordingly. Its function\nis based on a simple error signal created when the room gets too hot or too cold. When the temperature is\nsuitable, there would be no error signals whatsoever, and certainly no suffering.\nNow, suppose you make the thermostat very intelligent, so that it is able to predict the future, compute\nthreats, evaluate itself, perhaps even think about its own survival. Then, it might not only suffer when the\nroom temperature is wrong but also when it anticipates that that might happen. Your hyperintelligent ther-\nmostat might be reading the weather forecast on the internet. Suppose the forecast says that tomorrow night\nwill be exceptionally cold, beyond the capacities of the heating system. Then, the thermostat anticipates that\ntomorrow night it will not be able to keep the temperature high enough. Thus, the thermostat suffers due to\nsuch a threat—at least in the computational sense.\nThe extraordinary thing here is that the hyperintelligent thermostat suffers even long before anything bad\nhappens, before, say, actual frustration is produced, merely by virtue of the newly appeared anticipation of\npossible negative reward. This is perceived as a threat, and produces fear. Becoming more intelligent means the\nagent can perform computations related to threat, suffer based on those computations, and thus suffer much\nmore than it did earlier. Furthermore, if the thermostat realizes it is unable to properly control the temperature\nin the future, the uncontrollability may trigger a negative internal reward, and a reward loss. If this happens\n15Reducing uncertainty as measured by entropy can even be seen as a general learning principle for the brain (Friston, 2010), and\nthus failure to reduce uncertainty should generate an error signal. At the same time, reducing uncertainty, unpredictability, and uncon-\ntrollability is very closely related to the goal of curiosity discussed in Chapter 6: Uncertainty can be reduced by a curious investigation\nof new aspects of the environment, and uncontrollability can be reduced by trying out the effects of actions in new circumstances. An-\nother interesting point is that the estimates of uncertainty etc. are not exact either; the estimate of, say, entropy has some uncertainty\n(estimation error) as well. However, considering uncertainty of uncertainty would lead to a potentially endless recursion, and may not\nbe very useful or feasible.\n16In the first version (2022) of this book, I actually attempted such a reduction, but in this second version (2024), I’m able to provide\na separate model of a threat based on the prediction of whole distributions.\n\nCHAPTER 7. THREAT AS ANTICIPATION OF POSSIBLE FRUSTRATION\n79\noften, the self-evaluation system might conclude that it is not performing its central task well enough, thus\nleading to frustration due to the self-evaluation. It is possible that if the thermostat fails to keep the temperature\nconstant, it will be thrown into the garbage bin, and a hyperintelligent thermostat might even worry about its\nown survival.\n“One who fears suffering is already suffering from what he fears” according to Michel de Montaigne.17\nHumans suffer enormously because they are too intelligent in this sense, and prone to thinking too much\nabout the future—a theme I will return to in Chapter 11 where I talk about simulation of the future. Yet, if we\nhumans are so incredibly intelligent, why cannot we just decide not to fear anything? Why cannot we take\nMontaigne’s point seriously: He suggested—actually talking about his chronic pain due to kidney stones—that\nthere is no point in imagining or anticipating future pain since that simply induces more suffering. This is\na complex question where part of the answer is the dual-process nature of human cognition, which will be\ntreated in the following chapter.\n17“Qui craint de souffrir, il souffre desjà de ce qu’il craint”, Essais, III, 13; using the old orthography on Wikisource (Bordeaux exem-\nplaire, 1588). Seneca said almost the same in Letters to Lucilius, LXXIV.32.\n\nChapter 8\nFast and slow intelligence and their problems\nIn this chapter, we delve deeper into the distinction of two different modes of information processing in the\nbrain, which coincide with those in modern AI. They were already discussed in Chapter 4: neural networks and\nGood Old-Fashioned AI. The idea of two complementary systems or processes is, in fact, ubiquitous in modern\nneuroscience and psychology, where it is called the “dual-process” or “dual-systems” theory. It is assumed that\nthe two systems in the brain work relatively independently of each other while complementing each other’s\ncomputations. The two systems, or modes of operation, roughly correspond to unconscious processing in\nthe brain’s neural networks, and conscious language-based thinking. Each of the two systems has its own\nadvantages and disadvantages, which is the main theme of this chapter and, in fact, a theme to which we\nwill return many times in this book. Neural networks are based on learning, which means they need a lot of\ndata and often result in inflexible functioning. On the other hand, the computations needed in GOFAI may be\noverwhelming, as in planning. On the positive side, we will see how the advantages of the two systems can be\ncombined in the action selection of a real AI system. Using categories is crucial for GOFAI, and we conclude by\ndiscussing the deep question of the advantages and disadvantages of such categorical processing and thinking.\nFast and automated vs. slow and deliberative\nLet us start with the viewpoint on the two systems given by cognitive psychology and neuroscience.1 According\nto such “dual-process” (or “dual-systems”) theories, one of the two systems in the brain is similar to the neural\nnetworks in AI: It performs its computation very fast, and in an automated manner. It is fast thanks to its com-\nputation being massively parallel, i.e., happening in many tiny “processors” at the same time. It is automated\nin the sense that the computations are performed without any conscious decision to do so, and without any\nfeeling of effort. If visual input comes to your eyes, it will be processed without your deciding to do so, and usu-\nally you recognize a cat or a dog in your visual field right away, that is, in something like one-tenth of a second.2\nMost of the processing in this system is also unconscious. You don’t even understand how the computations\nare made; the result of, say, visual recognition just somehow appears in your mind, which is why this system is\nalso called “implicit”.\nThe processing in the conscious, GOFAI-like system is very different. To begin with, it is much slower. Con-\n1(Evans, 2008; Kahneman, 2011; Sloman, 1996)\n2(Kirchner and Thorpe, 2006)\n80\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n81\nsider planning how to get home from a restaurant where you are the first time: you can easily spent several\nseconds, even minutes, solving this planning task. The main reason is that the computations are not paral-\nlelized: They work in a serial way, one command by another, so the speed is limited by the speed of a single\nprocessing unit. In humans, another reason why symbolic processing is slow is, presumably, that it is evolu-\ntionarily a very new system, and thus not very well optimized. Other typical features of such processing are\nthat you need to concentrate on solving the problem, the processing takes some mental effort, and it can make\nyou tired. Such processing is also usually conscious, which means that you can explain how you arrived at your\nconclusion; hence the system is also called “explicit”.3\nNote that in an ordinary computer, the situation above is in some ways reversed, as already explained in\nChapter 4 (page 44). A computer can do logical operations much faster than neural network computations,\nsince logical operations are in line with its internal architecture. In fact, a computer can only do neural net-\nwork computations based on a rather cumbersome conversion of such analog operations into logical ones.\nAnalogously, the brain can only perform logical operations after converting them into neural network compu-\ntations, which is equally cumbersome.\nTo see the division into two systems particularly clearly, we can consider situations where the two systems\ntry to accomplish the same task, say, classification of visual input. We can have a neural network that proposes\na solution, as well as a logic-based system that proposes its own. Sometimes, the systems may agree; at other\ntimes, they disagree.\nSuppose a cat enters your visual field. When the conditions for object recognition are good, your visual\nneural network would recognize it as a cat. In other words, the network would output the classification “cat”\nwith high certainty. However, when it is dark, and you only get a faint glimpse of the cat that runs behind some\nbushes, your neural network might not be able to resolve the categorization. It might say it is probably either\na cat or a dog, but it cannot say which. At this point, the more conscious, logic-based system might take over.\nYou recall that your neighbour has a cat; you don’t know anybody who owns a dog near-by; you think this is just\nthe right moment in the evening for a cat to hunt for mice. Thus, you logically conclude it was probably a cat.\nIn this case, the task of recognizing an object used the two different systems, working together. The logic-based\none took quite some time and effort to use, while the neural network gave its output immediately and without\nany effort. Here, the systems were not completely independent, since the logic-based system did need input\nfrom the neural network to have some options to work on.\nThe two systems can also disagree, as often happens in the case of fear. Talking about fear and related\nemotional reactions, people often call them “hard-wired”. This expression is not too far from reality. What\nhappens is that the brain uses special shortcut connections to relay information from the eye to a region called\nthe amygdala, an emotional center in the brain. This shortcut by-passes those areas where visual information\nis usually processed.4 If such a connection learns to elicit fear (due to a previous unpleasant encounter with\nsome animals, for example), it will be very difficult to get rid of it. Any amount of reasoning is futile, presumably\n3My exposition is a kind of synthesis of different theories, and not all the mentioned properties are always associated with the two\nsystems. Further, I should mention the proposals that the second, explicit system may be specialized in simulating hypothetical events\nthat have not happened (Stanovich, 2004) for example for the purposes of planning, which will be considered in Chapter 11; or it\ncould be mainly about working memory (Evans, 2008). An interesting related division between feedforward and feedback processing\nin neural networks is discussed by Lamme and Roelfsema (2000).\n4(LeDoux and Pine, 2016). More precisely, a pathway goes directly from the thalamus to the amygdala, without reaching the visual\ncortex. Another, slower pathway does go back from the visual cortex to the amygdala. See also Hofmann et al. (2009) on conflicts\nbetween the two systems from the viewpoint of self-control.\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n82\nsince the visual signal triggering fear is processed by completely different brain areas than logical, conceptual\nreasoning. Often, the logic-based system loses here, and the neural-network-based fear prevails. This division\ninto two processes also explains why it is difficult for us to change unconscious associations, such as fear: the\nconscious, symbolic processing has limited power over the neural networks.\nInterestingly, people tend to think that the main information processing in our brain happens by the con-\nscious, symbolic system, including our internal speech and conceptual thinking. But what if that is simply the\ntip of the iceberg, as early psychoanalysts5 claimed more than a hundred years ago? The idea that most infor-\nmation processing is conscious and conceptual may very well be an illusion. We may have such an impression\nbecause conceptual processing requires more effort, or because it is more accessible to us by virtue of being\nconscious. However, if you quantify the amount of computational resources which are used for conceptual,\nlogical thinking, and compare them with those used for, say, vision, it is surely vision that will be the winner.6\nSimilar to the dual-process theories in cognitive psychology and neuroscience just described, the division\nbetween GOFAI and neural networks has been prominent in the history of AI research, which has largely oscil-\nlated between the two paradigms. Currently, neural networks are very popular, while GOFAI is not used very\nwidely. However, this may very well change, and perhaps in the future, AI will combine logic-based and neural\nmodels in a balanced way. Since GOFAI is used by humans, it is very likely to have some distinct advantage\nover neural networks, at least for some tasks.7\nNote that in AI we find another important distinction that is not very prominent in the neuroscientific\nliterature: learning vs. no learning. Neural networks in AI are fundamentally based on learning, and using\nthem without learning is not feasible. In contrast, in its original form, Good Old-Fashioned AI promises to\ndeliver intelligence without any learning, at the cost of much more computation and more effort spent on\nprogramming. This distinction is also relevant to the brain, as we will see next.8\nNeural network learning is slow, data-hungry, and inflexible\nTo understand the relative advantages of the two systems, let us first consider the limitations in neural net-\nworks, and especially the learning that they depend on. First of all, neural network learning is data-hungry: it\nneeds large amounts of data. This is because the learning is by its very nature statistical; that is, it learns based\non statistical regularities, such as correlations. Computing any statistical regularities necessarily needs a lot of\ndata; you cannot compute statistics by just observing, say, two or three numbers.\nSecond, neural network learning is slow. Often, it is based on gradient optimization, which is iterative, and\nneeds a lot of such iterations. The same applies to Hebbian learning, where changing neural connections takes\nmany repetitions of the input-output pairs—this is natural since Hebbian learning can be seen as a special\n5I’m here obviously referring to Freud and his followers, but the importance of unconscious processing was emphasized around the\nsame time frame by Janet (1889), and even earlier by philosophers such as Arthur Schopenhauer and Eduard von Hartmann.\n6(Nakayama, 1999). While this comparison in terms of brain resources seems compelling in terms of comparing vision vs. concep-\ntual thinking, it is more difficult to compare the conscious and unconscious aspects since we don’t really know how consciousness is\nrelated to the brain; see Chapter 14.\n7While the main text discusses later some such combinations of the two systems, I should also mention attempts made under the\ntitles of “hybrid AI” or “neural-symbolic processing” (d’Avila Garcez et al., 2012; Goertzel, 2012; Graves et al., 2016; Yi et al., 2018; Tresp\net al., 2023).\n8When we talk about learning in the brain in the context of neural network models, that is to be understood on an abstract level,\nwhere learning includes both evolutionary and developmental processes; this will be discussed in more detail in Chapter 12 (page 127).\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n83\ncase of stochastic gradient descent. In fact, to input a really large number of data points into a learning system\nalmost necessarily requires a lot of computation, since each data point takes some small amount of time to\nprocess.\nThis statistical and iterative nature of neural network learning has wide-ranging implications for AI. To be-\ngin with, these properties help us to further understand why it is so difficult, in us humans, to change any kind\nof deeply ingrained associations. Mental associations are presumably in a rather tight correspondence with\nneural connections: If you associate X with Y, it is because there are physical neural connections between the\nneurons representing X and Y. Now, even if any statistical connection ceases to exist in the real world, perhaps\nbecause you move to live in a new environment, it will take a long time before the Hebbian mechanisms learn\nto remove the association between X and Y, or to associate X with something else.9\nIn fact, these learning rules, whether basic Hebbian learning or some other stochastic gradient methods,\nmay seem rather inadequate as an explanation for human learning: We humans can learn from single examples\nand do not always need a lot of data. You only need to hear somebody say once “Helsinki is the capital of\nFinland”, and you have learned it, at least for a while. Surely, you don’t need to hear it one thousand times,\nalthough that may help. This does not invalidate the neural network models, however, since the brain has\nmultiple memory systems, and Hebbian learning is only one way we learn things and remember them—we\nwill get back to this point in Chapter 11.10\nThe iterative nature of neural learning, together with the two-process theory, also helps to explain in more\ndetail why it is so difficult to deliberately change unconscious associations. Suppose you consciously decide to\nlearn an unconscious association between X and Y (where X might be “exercise” and Y might be “good”). How\ncan you transfer such information from the conscious, explicit system to the neural networks? Perhaps the best\nyou can do is to recall X and Y simultaneously to your mind—but that has to be done many times! In fact, you\nare kind of creating a kind of new data and feeding it into the unconscious association learning in you brain.\nYou are almost cheating your brain by pretending that you perceive the association “X and Y” many times. We\nwill see many variations on this technique when we consider methods for reducing suffering in Chapter 17.\nAnother limitation is that when a neural network learns something, it is strictly based on the specific input\nand output it has been trained on. While this may seem like an obvious and innocuous property, it is actually\nanother major limitation of modern AI. Suppose that a neural network in a robot is trained to recognize animals\nof different species: It can tell if a picture depicts a cat or a dog, or any other species in the training set. Next,\nsuppose somebody just replaces the camera in the robot with a new one, with higher resolution. What happens\nis that the neural network the robot previously trained does not work anymore. It will have no idea how to\ninterpret the high-resolution images since they do not match the templates it learned for the original data. A\nsimilar problem is that the learning is dependent on the context: An AI might be trained by images where cats\ntend to be indoors and dogs outdoors, and it will then erroneously classify any animal pictured indoors as a cat.\nThe AI sees a strong correlation between the surroundings and the animal species, and it will not understand\n9In some cases, an association may not actually be removed but overridden by an inhibitory connection, a bit like creating a new\n“negative” connection to cancel the functioning of a positive connection (Westbrook et al., 2002). This also means the old association\ncan be reactivated quite easily.\n10Chapter 11 will explain the idea of replay whose application to this case would be as follows. Maybe your brain does actually hear\nthe sentence “Helsinki is the capital of Finland” many times. One of the learning systems in the brain is based on storing events, or\nshort episodes, in an area called the hippocampus. It uses special mechanisms, presumably quite different from stochastic gradient\nmethods, to store the sentence after hearing it just once. Then, the hippocampus feeds the sentence to the other parts of the brain\nmany times, and that allows Hebbian learning and something similar to stochastic gradient learning to happen.\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n84\nthat the actual task is about recognizing the animals and not recognizing the surroundings. That is why a neural\nnetwork will typically only work in the environment or context it is trained in.11\nIn light of these limitations, AI based on neural networks is thus rather different from what intelligence\nusually is supposed to be like in humans.12 In general, when humans learn to perform a task, they are often\nsomehow able to abstract general knowledge out of the learning material, and they are able to transfer such\nknowledge from one task to another. It has even been argued that the hallmark of real intelligence is that it is\nable to function in many different kinds of environments and accomplish a variety of tasks without having to\nlearn everything from scratch. If all a robot can do is to mow the lawn, we would think it is just accomplishing\na mechanical task and is not “really” intelligent.13\nUsing planning and habits together\nCombining the two systems, neural networks and GOFAI, should take as closer to human-like intelligence. Let\nus next look at how the two systems might interact in AI. Regarding action selection, we have actually seen how\ntwo different approaches can solve the same problem in AI: reinforcement learning and planning. Planning is\nin fact one of the core ideas of the GOFAI theory. Planning is undeniably a highly sophisticated and demanding\ncomputational activity, and probably impossible for simple animals—some would even claim it is only present\nin humans, although that is a hotly debated question.14 In any case, it seems to correspond closely to the view\nhumans have about their own intelligence, and therefore was the target of early AI research. However, in the\n1980s, there was growing recognition that building agents, perhaps robots, whose actions show human-level\nintelligence is extremely difficult, and it may be better to set the ambitions lower. Perhaps building a robot\nwhich has the level of intelligence of some simple animal would be a more realistic goal. Moreover, like in\nother fields of AI, learning gained prominence. That is why habit-like reinforcement learning started to be seen\nas an interesting alternative to planning.15\nHabits die hard—and are hard to learn\nHowever, habit-based behavior has its problems, partly similar to those considered above for neural network\nlearning. Learning the value function, that is, learning habits, obeys the same laws as other kinds of machine\nlearning. It needs a lot of data: the agent needs to go and act in the world many, many times. This is a major\nbottleneck in teaching AI and robots to behave intelligently, since it may take a lot of time and energy to make,\nsay, a cleaning robot try to clean the room thousands of times. Basic reinforcement algorithms are also similar\n11(Arjovsky et al., 2019)\n12While the inflexibility of neural networks seems to hold for the networks in the human brain as well, there is the celebrated experi-\nment of “upside down goggles”, which shows an interesting adaptive ability of the human neural networks. In this experiment, human\nparticipants started wearing goggles containing a prism which made their world look upside down. Surprisingly soon, the participants\nwere able to function normally; somehow, their visual systems were able to process the input correctly in spite of the inverted visual\ninput (Pisella et al., 2006).\n13(Legg and Hutter, 2007). Functioning in many environments thus requires an advanced capacity to what is called transfer learning,\nwhich is currently a focus of very active research in AI (Pan and Yang, 2009; Weiss et al., 2016).\n14(Redshaw and Bulley, 2018; Corballis, 2019)\n15A related school of research emphasized how intelligence might emerge from simple reactive behaviors, even without any learning\n(Brooks, 1991, 1999).\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n85\nto neural network algorithms in that they work by adjusting parameters in the system little by little, based on\nsomething like the stochastic gradient methods.\nAnother limitation which is crucial here is that the result of the learning, the state- or action-value function,\nis very context-specific—that is one form of inflexibility discussed above. If the robot has learned the value\nfunction for cleaning a room, it may not work when it has to clean a garden. Even different rooms to clean\nmay require slightly different value functions! The world could also change. Suppose the fridge from which the\nrobot fetches the orange juice for its master is next to a red table. Then, the robot will associate the red table\nwith high value since seeing it, the robot knows it is close to being able to get the juice. However, if somebody\nmoves the table to a different room, the robot will start acting in a seemingly very stupid way: It will go to the\nroom which now has the red table when it is supposed to get the orange juice—in fact, it might simply approach\nany new red object introduced to its environment in the hope that this is how it finds the fridge. It will need to\nre-learn its action-values all over again.\nHere, we see another aspect of the slowness of learning habits: Once a habit is learned, it is difficult to get\nrid of it. In humans, the system learning and computing the reinforcement value function is outside of any\nconscious control: We cannot tell it to associate a smaller or larger value to some event. This is why we often\ndo things we would prefer not to do, out of habit. In order to learn that a habit is pointless in the sense that it\ndoes not give any reward anymore (as happened with the robot above), a new learning process has to happen,\nand this is just as slow as the initial learning of the habit. That is why habits die hard.16\nCombining habits and planning\nThese problems motivate a recent trend in AI: combining planning and habit-like behavior. The habit-based\nframework using reinforcement learning will lead to fast but inflexible action selection, and is ideally comple-\nmented by a planning mechanism which searches an action tree a few steps ahead—as many as computation-\nally possible. Depending on the circumstances, the action recommended by either of the two systems can then\nbe implemented.17\nLet us go back to the robot which is trying to get the orange juice from the fridge. One possible way of\nimplementing a combination of planning and habit-like behavior is to have a habit-based system help the\nplanning system in the tree search. Using reinforcement learning, you could train a habit-based system so\nthat when the robot is in front of the fridge whose door is closed, the system suggests the action “open the\ndoor”. When the door of the fridge is open with orange juice inside, the habit-based system suggests “grab the\norange juice”. While these outputs could be directly used for selecting actions, the point here is that we can use\nthem as mere suggestions to a planning system. Such suggestions would greatly facilitate planning: The search\ncan concentrate on those paths which start with the action suggested by the habit-based system, focusing the\nsearch and reducing its complexity. However, the planning system would still be able to correct any errors in\nthe habit-like system, and could override it if the habit turns out to be completely inadequate.\nOne very successful real-world application using such a dual-process approach is AlphaGo, a system play-\ning the board game of Go better than any human player.18 The tree to be searched in planning consists of\nmoves by the AI and its opponent. This is a classical planning problem in a GOFAI sense. The world has a finite\n16However, some hope will be offered in Chapter 11 where we consider ways of speeding up learning by replaying existing data, and\nthat theme is continued in Chapter 17.\n17(Daw et al., 2005)\n18(Silver et al., 2016); see Illanes et al. (2020) for a different dual-process model.\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n86\nnumber of well-defined states, and also, the actions and their effects on the world are clearly defined, based\non the rules of the game. What is a bit different is that there is an opponent whose actions are unpredictable;\nhowever, that is not a big problem because the agent can assume that the opponent chooses its actions using\nthe same planning engine the agent uses itself.\nThe search tree in Go is huge since the number of possible moves at any given point of the game is quite\nlarge, even larger than in chess. In fact, the number of possible board positions (positions of all the stones\non the board) is larger than the number of atoms in the universe—highlighting the fundamental problem in\nGOFAI-style planning. Since it is computationally impossible to exhaustively search the whole tree, AlphaGo\nrandomly tries out as many paths as it has time for. This leads to a “randomized” tree search method called\nMonte Carlo Tree Search. Algorithms having some randomness deliberately programmed in them are often\ncalled Monte Carlo methods after the name of a famous casino. However, a purely random search would obvi-\nously be quite slow and unreliable.19\nThe crucial ingredient in AlphaGo is another system which learns habit-like behaviors. This system is used\ninside the planning system, a bit like in the juice robot just described. While the system is rather complex, let’s\njust consider the fact that in the initial stage of the learning, AlphaGo looks at a large database of games played\nby human experts. Using that data, it trains a neural network to predict what human experts would do in a\ngiven board position—the board positions correspond to the states here. The neural network is very similar to\nthose used in computer vision, and gets as input a visual view of the Go board. This part of the action selection\nsystem could be interpreted as learning a “habit”, i.e., an instinctual way of playing the game without any plan-\nning.20 The action proposed by the habit system can be used as such, but even more intelligent performance\nis obtained by using it as a heuristic for the tree search: the tree search is focused on paths related to that pro-\nposed action. This heuristic is further refined by further learning stages. In particular, the system also learns to\napproximate the state-values by another neural network.21\nSuch suggestions based on neural networks are fast, and intuitively similar to what humans would do. Of-\nten, a single glimpse at the scene in front of your eyes will tell a lot about where reward can be obtained, and\nsuggests what you should do. Even when humans are engaged in planning, such input coming from neural net-\nworks often guides the planning. If you go to get something from the fridge, don’t you have almost automated\nreactions to seeing the fridge door closed, and seeing your favorite food or drink inside the fridge? These are\npresumably given by a simple neural network. Yet, there is a deliberative, thinking aspect in your behavior, and\nyou can change it if you realize, for example, that the juice has gone bad—which the simple neural network did\nnot know.\nWhat is typical in humans is that action selection can also switch from one system to another as a function\n19(Browne et al., 2012; Chaslot et al., 2008). Monte Carlo Tree Search does include clever tricks which make the search a bit more in-\ntelligent. It does not try out actions (or moves in a game) completely randomly, but gathers data on which actions look more promising.\nIn particular, there is quite a lot of data regarding actions taken in the first steps of the search path, since any search has to always try\nout one of those, and their number is limited because there has not yet been a combinatorial explosion as in the number of long paths.\nMonte Carlo Tree Search uses such data to bias the search towards paths whose initial parts have been found the most promising.\n20Interestingly, the “habits” are here learned based on imitation since they are simply trying to replicate what the human players did\nearlier. Imitation learning is another principle for machine learning, especially important for robots (Schaal, 1999).\n21For the general theory on approximating values by neural networks or simpler methods, see Sutton and Barto (2018, Chapter 9).\nIn Chapter 11 we will also see how the system can improve by playing against itself. A completely different purpose for combining\nlearning and planning is to learn to plan better in a given environment where rewards are changing (Tamar et al., 2016; Pascanu et al.,\n2017).\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n87\nof practice. Learning a new skill, such as driving a car, is a good example—skills are similar to habits from the\ncomputational viewpoint. First, you really have to concentrate and consciously think about different action\npossibilities. With increasing practice and learning, you need to think less and less, since something like a\nvalue function is being formed in the brain. In the end, your actions become highly automated, and you don’t\nreally need to think about what you are doing anymore. The habit-based system takes over and drives the car\neffortlessly.22\nAdvantages of categories and symbols\nWhile in this example of Go playing, neural networks and GOFAI work nicely together, it is often not easy to\ndemonstrate any clear utility of symbolic AI approaches. This may of course change any time, since AI is a\nfield of rapid development. It is quite likely that GOFAI is necessary for particularly advanced intelligence—\nsomething much more advanced than what we have at this moment. Yet, the tendency has recently been\nalmost the opposite: tasks which were previously thought to be particularly suitable for symbolic AI have been\nmore successfully solved by neural approaches. For example, large language models used in systems like Chat-\nGPT effectively transform language, i.e. text data, into a sequence of high-dimensional continuous-valued vec-\ntors before inputting them into a huge neural network.23\nPerhaps symbolic AI works with board games only because such games are in a sense discrete-valued:\nthe stones on the Go board can only be in a limited number of positions, so the game is inherently suitable\nfor GOFAI. So, we have to think hard about what might be the general advantages of logic-based intelligence\ncompared to neural networks. In the following, I explore some possibilities.\nGOFAI is more flexible and facilitates generalization\nSuppose that there is a neural network that recognizes objects in the world and outputs the category of each\nobject. Then, what would be the utility of operating on those categories as discrete entities, using symbolic-\nlogical processing, instead of having just a huge neural network that does all the processing needed?\nWe have already seen, more than once, one great promise of GOFAI in the case of planning: flexibility. Given\nany current state and any goal state, a planning system can, if the computational resources are sufficient, find\na plan to get there. If anything changes in the environment—say, it is no longer possible to transition between\ntwo states due to some kind of blockage—the planning system takes that into account without any problems.\nThis is in contrast to reinforcement learning which will not know what to do if the environment changes; it may\nhave to spend a lot of time re-learning its value functions.\nFurthermore, GOFAI is easily capable of representing various kinds of data structures and relationships in\nthe same way as a computer database. For example, it can easily represent the fact that both cats and dogs are\nanimals, i.e. the hierarchical structure of the categories. It can also represent the relationship that the character\nstring “Scooby” is the name of a particular dog. This adds to the flexibility of GOFAI by allowing more abstract\nkinds of processing, which are easily performed by humans.\n22I may be oversimplifying things here, since in the brain, learning motor skills such as driving is not quite the same as forming\nhabits, and they may be based on different brain systems. However, on a more abstract level where we only consider the computational\nprinciples, they can be very similar (Doyon et al., 2003; Peters et al., 2011; Sun et al., 2005).\n23(Achiam et al., 2023). This holds for most successful natural language processing systems, such as Google translate (Wu et al., 2016).\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n88\nAnother wide-spread idea is that categories are useful for generalizing knowledge over categories, which in\nits turn underlies various forms of abstract thinking. Even though cats are not all the same, it is useful to learn\nsome of their general properties. They like milk, they purr; they don’t like to chew bones like dogs do, and they\nare not dangerous like bears. Having categories enables the system to learn to associate various properties to\nthe whole category: Observing a few cats drink milk, the system learns to associate milk-drinking to the whole\ncategory of cats, instead of just some individual cats. Importantly, associating properties to categories means\nthe system was able to generalize: after seeing some of the cats drink milk, it inferred that all cats drink milk.\nSuch generalization is clearly an important part of intelligence. If the system needed to learn such a property\nseparately for each cat, it would be in great trouble when it sees a new cat and needs to feed it — it would have\nno idea what to do. But, learning that the whole category of cats is associated with milk-drinking, it knows,\nimmediately and without any further data, what to give to this new cat.\nCategories enable communication\nNevertheless, I think the feature which makes GOFAI fundamentally different from neural networks is that the\nuse of symbols is similar to using some kind of a primitive language. In fact, you can hardly have GOFAI without\nsome kind of a language—perhaps akin to a programming language—in which the symbols and logical rules\nare expressed.\nIt is equally clear that with humans, language is primarily used for communication between individuals.\nAs each category typically corresponds to a word, humans can communicate associations, or properties of\ncategories, to each other. I can tell my friend that cats drink milk, so she does not need to learn what to feed to\ncats by trial and error. I have condensed my extensive data on cats’ eating habits into a short verbal message\nthat I transmit to her.\nSo, it is plausible that the main reason humans are capable of symbolic thinking is that it enables them\nto communicate with each other. After such a communication system was developed during evolution, hu-\nmans then started using the same system for various kinds of intelligent processing even when alone. Perhaps\nwe started by telling others, for example, where to find prey. This led to the development of symbols and\nlogical operations, which were found useful for abstract thinking: Perhaps you could try to figure out yourself\nwhere you should hunt tomorrow. Eventually, such capabilities ended up producing things such as in quantum\nphysics—and the very theory of GOFAI.24\nA reflection of the utility of categories in communication may be seen in a recent research line in AI which\ntries to develop systems whose function is easy to interpret by humans.25 If you use a neural network to rec-\nognize a pattern, the output may be clear and comprehensible, but the computations—why did the network\ngive that particular output— are extremely difficult to understand for humans. This is fine in many cases, but\nsometimes it is necessary to explain the decision to humans. For example, if an AI rejects your loan application,\nthe bank using the AI may be legally obliged to explain the grounds for that decision.26 Researchers developing\n24This is what is called “exaptation” in evolution. It means that a trait was first produced to adapt for one phenomenon, but then it\nturned out to be useful for something else. A typical example is bird’s feathers, which probably first evolved to keep the birds warm,\nand only later turned out to be useful for flying. Wagner et al. (2003) discuss computer simulations on the emergence of language for\ncommunication between agents. Dehaene et al. (2022) propose that the main capacity specific to humans is using symbols and mental\nprograms, generalizing the idea that language and sequence processing is specific to humans. For an attempt to do some kinds of\nlogical processing in a neural system, see Frady et al. (2020).\n25(Su et al., 2015; Guidotti et al., 2018; Arrieta et al., 2020)\n26For example, the General Data Protection Regulation (GDPR) of the European Union imposes a general “right for explanation” for\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n89\nsuch interpretable AI often end up doing something similar to GOFAI boosted by learning, since it gives rules\nwhich can be expressed in more or less ordinary language, and thus they can be explained. In fact, in Chapter 4\nwe saw examples of GOFAI systems whose functioning is easy to understand and to explain.27\nCategorization is fuzzy, uncertain, and arbitrary\nNow, let us consider the flipside: problems that arise when using categories. We have already seen some prob-\nlems in logical-symbolic processing, the most typical being the exponential explosion of computation in plan-\nning. Here, we focus on the consequences of using categories, and look at the question from a more philosoph-\nical angle. Indeed, it has been widely recognized by philosophers over the centuries that dividing the world into\n“crisp” categories can only be an approximation of the overwhelming complexity of the world. I focus on some\nissues which will in later chapters be seen to be relevant for suffering.28\nCategories are fuzzy\nPhilosophers have long pointed out that there may not be any clearly defined categories in the world. Granted,\nthe difference between cats and dogs may be rather clear, but what about the category of, say, a “game”?\nWittgenstein gave this as an example of a category which has no clear boundaries. Different games have just\nsome vague similarity, which he called “family resemblance”.\nThis idea has been very influential in AI under the heading of fuzziness. A category is called fuzzy if its\nboundaries are not clear or well-defined. Consider for example the word “big”. How does one define the cat-\negory of big things? For simplicity, let us just consider the context of cities. If we say “London is big”, that is\nalmost any decision made by an algorithm on an individual (Goodman and Flaxman, 2017). One reason for such a requirement is to\nmake sure that the AI did not discriminate applicants based on gender, race, or similar characteristics—an objective called “fair AI”.\nAnother reason is that the AI might not make the final decision, but could be used as a support for a human decision-maker, such as\na medical doctor; the human decision-maker would greatly benefit from understanding why the AI came to its conclusion. One more\nreason for making AI easy to interpret is that understanding how an AI works makes it easier to evaluate its potential safety hazards,\nand develop AI that is safe.\n27I am actually tempted to think that the only specific utility of categories (which cannot be obtained without them) is communi-\ncation, including being interpretable and comprehensible by humans in the case of AI. In particular, it is not clear to me if explicit\ncategories are needed for generalization. Without going into details, let me just mention that similar operations could be performed\ndirectly in a representational space by simply propagating any associations to near-by points in that space without any strict division\ninto categories. For opposite viewpoints putting concepts at the heart of (human) cognition, see Rosch (1999); Harnad (2017). Obvi-\nously, it is important here to compare the different definitions of categorization used: Harnad (2017) uses a definition which is very\ngeneral.\n28I don’t go into any details on how that “division” of the world into categories happens, but for the interested reader, I give some\npointers here. Earlier, we considered the case where the neural network recognizes an object and outputs its category. This is a simple\nstarting point; while it can be easily done by supervised learning, it can also be implemented by unsupervised learning methods,\nin particular methods such as clustering and (Gaussian) mixture modelling. In the case of humans, the connection between neural\nnetworks and logic-symbolic processing is related to what is called the symbol grounding problem (Harnad, 1990). It is a topic subject\nto a lot of debate: some argue no proposed solution is sufficient (Taddeo and Floridi, 2005), while others argue it is essential to consider\nrobots which communicate with each other (Steels, 2008). The operation of neural networks is closely related to one well-known\nproposal called the prototype theory. It means we define each category by a single point in the space the activities of units in a neural\nnetwork (preferably in layers close to output); this point is the prototype (Rosch, 1978). Basically, you would find a “prototypical” cat\nas a point in the very center of all those points that represent cats. A generalization of this idea can be found in Gärdenfors (2004).\nHowever, things get much more complicated in the case of abstract categories such as “good” or “beautiful”.\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n90\nclearly true: London definitely belongs to the category of big things, in particular big cities. But if we say “Brus-\nsels is big”, is that true or false? How does one define what is big and what is not? In the case of cities, we could\ndefine a threshold for the population, but how would we decide what it should be? An AI might learn to cat-\negorize cities into big and small ones based on some classification task—in Chapter 4, we discussed how this\nmight happen in categorizing body temperature into “high fever” or not. However, that categorization would\ndepend on the task, and there would always be a gray zone where the division is rather arbitrary.\nThe consensus in AI research is that many categories are quite fuzzy and have no clear boundaries; there are\nonly different degrees of membership to a category. There is no way of defining a word like “big” (or, say, “nice”,\n“tall”, “funny”) in a purely binary (true/false) fashion. There will always be objects that quite clearly belong to\nthe category and objects which clearly do not belong to the category, but for a lot of objects the situation is not\nclear. In the theory of fuzzy logic, such fuzziness is modelled by giving each object a number between 0 and 1\nto express the degrees of membership to each category.29\nCategorization is uncertain\nIn addition, categorization is always more or less uncertain. Any information gleaned from incoming sensory\ninput is uncertain, for reasons we will consider in more detail in Chapter 12. Partly, it is a question of the neural\nnetwork getting limited information, and partly because of its limited information-processing capabilities. If\nyou have a photograph of a cat taken in the dark and from a bad angle, the neural network or indeed any\nhuman observer may not be sure about what it is. They might say it is a cat with 60% probability, but it could\nbe something else as well. In other words, any categorization by an AI is very often a matter of probabilities.\nIt is important to understand that fuzziness and uncertainty are two very different things. Uncertainty is a\nquestion of probabilities, and probabilities are about lack of information. If I say that a coin flip is heads with\n50% probability and tails with 50% probability, there is no fuzziness about which one it is. After flipping the coin\nI can say if it is heads or tails, and no reasonable observer would disagree with me (except in some very, very rare\ncases). In other words, uncertainty is a question of not knowing what will happen or has happened, i.e., a lack\nof information about the world. In contrast, fuzziness has nothing to do with lack of information; it is about the\nlack of clear definition. We cannot say if the statement “Brussels is big” is true even if we have every possible\npiece of information about Brussels, including its exact population count. According to the information I find\non Wikipedia, its population is 1,191,604, but knowing that will not help me with the problem if I don’t know\nhow many inhabitants are required for a city to be in the “big” category.\nHumans are not good at processing uncertainty. Various experiments show that humans tend to use exces-\nsively categorical thinking, where the uncertainty about the category membership is neglected. That is, when\nyou see something which looks to you most probably like a cat, your cognitive system tends to ignore any other\npossibilities, and think it is a cat for sure.30\n29(Mendel, 1995)\n30An example was seen in a study where the subjects were told a story which suggested that an imaginary person entering a house\nwould be either a burglar, or a real estate agent. When the imaginary person was more likely to be a real estate agent than a burglar—\nbased on various cues such as what other characters in the story were thinking— they tended to ignore the possibility that the person\nis a burglar altogether, as seen in the predictions that they made about the behavior of the imaginary person (Malt et al., 1995; Murphy\nand Ross, 2010). The authors also found a way of remedying the situation: if the subjects are asked what the probabilities of the two\ncategories are, and their estimates are shown on the computer screen (say, “65% vs. 35%”), the subjects are able to take the uncertainty\nof the categorization into account.\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n91\nAn old Buddhist parable about these dangers in categorization is seeing a rope in the dark and thinking\nit is a snake. You miscategorize the rope, and your brain activates not only the category of a snake, but all the\nassociations related to that category (“animal”, “dangerous”). You get scared, with all the included physiological\nchanges, such as an increased heart rate. If you had properly taken the uncertainty of such categorization into\naccount, your reaction might have been more moderate.\nCategorization is arbitrary\nIn some cases, the categories are not just fuzzy or uncertain: their very existence can be questionable. Con-\nsider concepts such as “freedom” or “good”. Even forgetting about any difficulties in programming an AI to\nunderstand them, is it even clear what these words mean? Certainly, they mean different things to different\npeople: people from different cultural backgrounds may easily misunderstand each other simply because they\nuse such concepts with slightly different meanings. A great amount of time can be spent in attempting to just\ndescribe the meanings of certain words and categories. In fact, we spend more than one chapter on analyzing\nthe category called “self” in this book.\nEven in rather straightforward biomedical applications of machine learning, we often use categories that\nare not well-defined. For example, in a medical diagnosis context, it is not clear if what we usually call schizo-\nphrenia is a single disease. Perhaps there are a number of different diseases which all lead to the single diag-\nnosis of schizophrenia.31 Developing effective medications may only be possible once we understand all the\nsubtypes, while thinking of all the subtypes as a single disease (a single category) may mislead any treatment\nattempts.\nMoreover, a categorization that works for one purpose might not be suitable for another. We might divide\npeople into different nationalities, which is very useful from the viewpoint of knowing what languages they are\nlikely to understand. However, we can too easily use the same categories to predict all kinds of personality traits\nof those individuals, and that prediction may go quite wrong. Thus, the categories and their utility depend on\nthe context. Moreover, since different people use different categories in different ways, they are subjective.32\nSuch arbitrariness of categories has been well appreciated in some philosophical schools. In the Yog¯ac¯ara\nschool of Buddhism, it is claimed that “while such objects [as chairs and trees] are admissible as conventions, in\nmore precise terms there are no chairs, or trees. These are merely words and concepts by which we gather and\ninterpret discrete sensations that arise moment by moment in a causal flux.“33 What arises in such a moment-\nby-moment flux is, in our terminology, activities in neural networks. Categories are created afterwards, by\nfurther information-processing.\n31(Peralta and Cuesta, 2001; Brodersen et al., 2014). The same could be said of depression (Drysdale et al., 2017). For a general\noverview on such “precision medicine”, see Insel and Cuthbert (2015).\n32Human categorization can also change when the frequencies of different objects change. In the experiments by Levari et al. (2018),\n“When blue dots became rare, participants began to see purple dots as blue; when threatening faces became rare, participants began\nto see neutral faces as threatening”.\n33Quote from (Lusthaus, 2013), see also (Lusthaus, 1998; Tagawa, 2009; Williams, 2008b). In ancient Greece, Pyrrhonian Skeptics had\nideas similar to such Buddhist schools, and indeed Pyrrhonians are likely to have been directly influenced by Buddhist thinkers due to\nAlexander the Great’s campaign into India (Bowie, 2016; McEvilley, 1982; Garfield, 1990; Kuzminski, 2007).\n\nCHAPTER 8. FAST AND SLOW INTELLIGENCE AND THEIR PROBLEMS\n92\nOvergeneralization\nIt may be easy to understand that miscategorization leads to problems, as in mistaking a rope for a snake.\nHowever, the biggest computational problem caused by all properties just discussed—fuzziness, uncertainty,\narbitrariness—may be overgeneralization. Overgeneralization can be difficult to spot, even after the fact, which\nmakes it particularly treacherous.\nOvergeneralization means that you consider all instances of a category to have certain properties, even\nif those properties hold only for some of them. Since categories are fuzzy, anything which is not really firmly\ninside the category may actually be quite different from its prototype. Related to this, you may not acknowledge\nthe uncertainty of categorization and the ensuing generalization. Even more rarely do people acknowledge that\nthe very categories are arbitrary.\nOvergeneralization effects are well documented, for example, in perception of human faces, where gender\nand race can bias any conclusions you make about the individual involved.34 As an extreme case of overgen-\neralization, if you have been bitten by a dog, you may develop a fear towards all dogs, which would be called\na phobia. Such fear is overgeneralizing in the sense that it is very unlikely that the other dogs would bite you.\nIf you didn’t use any categories, you would only be afraid of that one dog that bit you. This is a very concrete\nexample of how thinking in terms of categories leads to suffering, as will be discussed in more detail in later\nchapters.\nThere are actually good computational reasons why overgeneralization occurs. Learning to generalize\nbased on a limited number of categories means that knowledge gleaned from all the instances of each category\ncan be pooled together. If you actually had enough data from all the dogs in the world, as well as unlimited\ncomputational capacities, you would be able to learn that some of them are safe while a few are not. However,\ndata and computation are always limited, so some shortcuts may be necessary—even if they increase your\nsuffering. This is another theme that we will return to over and over again in this book.\n34(Freeman and Johnson, 2016). In that case, a further problem is that the categories may operate using stereotypes (which may not\nbe factually accurate to begin with), which means that the generalization is even more wrong.\n\nChapter 9\nSummarizing the mechanisms of suffering\nSo far in this book, we have seen several computational ideas related to suffering. We started by considering\ntwo basic mechanisms for suffering in Chapter 2: frustration and threat to the person or the self. We first\ndefined frustration as not reaching a goal (Chapter 3) and later in terms of reward loss and reward prediction\nerror (Chapter 5). In fact, these two kinds of frustration align well with the dual-process theory—slow vs. fast or\nGOFAI vs. neural networks—considered in Chapter 8. In Chapter 6, we further argued that the concept of self\nincludes higher-order desires related to self-evaluation or self-preservation, and these can also be frustrated.\nChapter 7 developed a theory of threats and fear based on predicted reward loss in the future. To sum up, we\nobtained a theory in which suffering is based on error signals given by frustration or prediction of frustration.\nIn this chapter, we summarize the ideas of the previous chapters, emphasizing the many different forms that\nfrustration can take.\nFrustration on different time scales\nConsider a case where you are yourself going to fetch the orange juice from the fridge. You formulate a plan\nwhich involves high-level actions such as going to the fridge, opening the door, etc. Once you are in front of\nthe fridge, your habit-based system suggests you open the door by a certain sequence of muscle contractions\nwhich you have performed hundreds of times and which has become quite automated.\nNow, suppose you follow the habit-based system and pull the door handle, but the door does not quite\nopen. This kind of “frustrates” your habit of opening the door. But do you suffer? Probably not very much; you\njust pull again with more force, and if it opens, you hardly register anything out of the ordinary happened. In\ncontrast, if you don’t get the juice at all—because the door is somehow broken and does not open at all— your\nlong-range plan is frustrated, and you will definitely suffer. There is a good reason for that suffering: all that\nplanning and even the walking was in vain. A strong error signal has to be sent throughout your brain, and that\nis suffering.\nThis example points out one important aspect of action selection: its temporally hierarchical nature, in-\nvolving simultaneous computations on different time scales.1 In the brain, there are also processes operating\n1This is called hierarchical control (Poole and Mackworth, 2010), hierarchical planning or hierarchical task networks (Georgievski\nand Aiello, 2015; Nau et al., 2003) or hierarchical reinforcement learning (Sutton et al., 1999; Dietterich, 2000; Botvinick, 2012). A related\nparadigm is given by what is called “options” in reinforcement learning, see (Sutton and Barto, 2018, p. 461).\n93\n\nCHAPTER 9. SUMMARIZING THE MECHANISMS OF SUFFERING\n94\nat many different time scales. Reality is, of course, a bit more complex than the clean division into planning\nand habit-based actions we have discussed so far.\nSome form of frustration can be operating on many different levels simultaneously. In one extreme, the\nagent may be planning long action sequences, and if they fail, frustration ensues in the sense of not reaching\nthe goal. In the other extreme, a habit-based reinforcement learning system builds predictions on what kind of\nrewards or changes in state-values are associated with different actions, and computes whether there is reward\nloss or an RPE. Predictions are made on a millisecond time scale as well as on the time scale of days if not years.\nEach such time scale has its own learning mechanism using its own errors.2\nSuch division into time scales brings us to the concept of intention—defining intention as commitment to\na goal, as discussed in Chapter 3. The point in intentions is to partly resolve conflicts between long-term and\nshort-term optimization. I can have many desires simultaneously and spend some time thinking about each\nof them, and perhaps even planning each of them to some extent. But I’m not really hoping to reach all the\ngoals related to those desires. Once I decide to commit to one of the goals, that is what sets the goal, which can\nthen be frustrated. I would argue that in the case of planning, frustration is not so much due to desire itself\nbut to the ensuing intention. This is in line with the more elaborate expositions of the Buddha’s philosophy on\nsuffering which divide desire into initial desire and a later part called attachment (also translated as “clinging”\nor “grasping”). Attachment is a process where after an initial feeling of desire (“Nice, chocolate, I would like to\nhave it”), you firmly attach to the object of your desire (“I must have that chocolate”). This distinction seems to\nbe similar to the distinction between desires and intentions in our terminology. Buddhist philosophy suggests\na central role for attachment, or intention, in the process which creates suffering. While such attachment or\nintention is not necessary for frustration to occur, I propose that it greatly amplifies it. This is logical because\nintentions consider longer time scales, and thus an error related to intention is more serious, since more time\nand energy were lost in formulating and executing the plan that failed.\nFrustration based on desires, expectations, and general errors\nWe have also seen two different kinds of frustration: not reaching a goal vs. incurring a reward loss. One un-\nderlying difference between the two cases is that reward loss is based on violation of expectations, while not\nreaching the goal is in line with the typical definition of frustration as not getting what one wants, i.e. violation\nof desires. It may thus seem that our definitions are to some extent contradictory. One way of resolving this is to\nconsider that the term “expectation” may have different meanings in different contexts.3 The agent is executing\na plan in order to get to the goal state, and it is in that sense “expecting” to get to that goal state. Earlier, we\nsaw (page 20) how Epictetus talks about desire “promising” the attainment of its object. Thus, the expectation\nrelated to planning could simply be defined as the goal state being reached. Then, reward loss would be the\nsame as the frustration of not reaching the goal, that is, the object of the desire (using the definition of desire\ngiven in Chapter 3).\nAlternatively, we could see frustration of desires and reward loss (based on expectation) as two distinct, if\nclosely related phenomena, both of which produce suffering. What they have in common is that some kind\n2(Hari et al., 2010; Botvinick, 2012); in fact, RPE’s seem to be coded even in the cerebellum (Heffley and Hull, 2019; Kawato et al.,\n2021). Using RPE instead of reward loss simplifies the situation to some extent, since RPE considers the total future reward and is thus\nless dependent on the definition of the time scale, as explained in footnote 21 in Chapter 5.\n3The difficulty of defining expectation was earlier discussed in footnote 15 in Chapter 5.\n\nCHAPTER 9. SUMMARIZING THE MECHANISMS OF SUFFERING\n95\nof error occurred. This opens up the possibility of a very general viewpoint where the connection between\nsuffering and error signalling does not need to be concerned with goals or rewards at all. We all know that it\nis unpleasant if we expect something and then it does not happen, even if the event we were predicting was\nneutral in the sense of providing no reward. Thus, it is possible that there is some kind of suffering in almost\nany prediction error.4 Most interestingly, it has been proposed that dopamine signals prediction errors even for\nevents not related to reinforcement, so it might provide a neural mechanism for general signalling of errors.5\nThe meaning of such errors is further modified by the context. If you are deliberately engaged in the learn-\ning of, say, a new skill, errors are quite natural, and you are likely to feel less frustration; in a sense, you are\nexpecting that there are errors. Or, if your prediction of the reward is uncertain, i.e. only very approximate, the\nfrustration is likely to be weaker. We will have much more to say about such effects in later chapters.\nDepending on the context, what I call frustration in this book can actually correspond to different concepts\nwith slightly different meanings. Disappointment is a closely related term; it can also be used when no partic-\nular action is taken by the agent while the world turns out to be worse than expected. Irritation and even anger\ncan also be used to describe feelings very similar to frustration. While anger can more specifically mean the\ninterpersonal feeling of anger towards other people, it is often caused by the fact that their actions lead to frus-\ntration (more on this in Chapter 10). Regret can be seen as frustration specifically based on our own actions,6\noften amplified by recalling past frustration (more on this in Chapter 11).\nSelf, threat, and frustration\nFrustration on different time scales leads us to the frustration of self-needs treated in Chapter 6. Self-needs\noften work on time scales of days, months, even years, thus an even longer time scale than ordinary planning\nand attachment. These different time scales can actually be related to van Hooft’s different kinds of frustration\ndiscussed in Chapter 2: frustration of biological functioning, of desires and emotions (in his terminology), of\nmore long-term life goals, and even of the sense of the meaning of one’s existence. It may very well be that such\nself-related frustration produces some of the very strongest frustration and suffering. Frustration of self-needs\nis also closely connected to suffering coming from threats to the “intactness of the person” à la Cassell, thus\ncombining the two mechanisms of suffering.7\nIn Chapter 7, self-related frustration was even considered as a means to reduce the concept of threat to a\nform of frustration—of self-needs such as the desire for safety—to obtain a unified theory on suffering. How-\never, while such a theoretical simplification has some interest, it was considered to go a bit too far; the connec-\ntion between frustration and threat is certainly more complex. In Chapter 7, we actually saw a fundamental\ndistinction that can be made between frustration and threats: threats are about predicting that a bad thing\nmight happen, while frustration is fundamentally about realizing the bad thing did already happen. Neverthe-\n4On the other hand, if we expect something unpleasant to happen, and it does not happen, we feel relief which is clearly not suffer-\ning; in general, obtaining more reward than expected may be the very definition of pleasure (see footnote 18 in Chapter 5). Perhaps,\nin that case, the unexpected positivity (of reward) overrides the inherent suffering in prediction error. For research on relief from pain,\nsee Seymour et al. (2005); Leknes et al. (2008).\n5(Takahashi et al., 2017; Redgrave and Gurney, 2006).\n6On the question of attributing frustration to oneself, see footnote 16 in Chapter 6.\n7Maslow (1941) even proposes that mere “deprivation” in itself does not have the negative effects attributed to frustration, but only\nwhen it is at the same time “a threat to the personality, that is, to the life goals of the individual, to his defensive system, to his self-\nesteem or to his feeling of security.” Frustration of self-needs is closely related to such threats as was discussed in Chapter 6.\n\nCHAPTER 9. SUMMARIZING THE MECHANISMS OF SUFFERING\n96\nless, it is interesting to ask if it is also possible to see the connection between frustration and threats from the\nopposite angle: can frustration be seen as a special case of a threat to the self?\nWe can indeed consider that failing in a task implies a threat to one’s self-image that Cassell talks about,8\nand which is related to the self-evaluation of Chapter 6. A frustrating experience may imply that the agent’s\npositive self-image is not correct, and that it has to change its self-image so that it will consider itself less\ncompetent than it thought earlier. In this sense, frustration is a threat, especially if the frustration did not yet\nchange the self-image but implied a certain probability that it should be done in the future. If you fail at a work\ntask today, that is frustration but you may still think you know how to do your job. But the failure will increase\nyour perceived probability that one day, you have to admit that you just don’t know how to do your job at all.\nThat is a threat to your person, implying the possibility that one day you will have to update your self-image\nto a more negative one. Likewise, any frustration could be considered to imply a threat to the very survival of\na biological agent because it suggests that the agent’s decision-making system is not working very well in the\ncurrent environment, which could lead to life-threatening problems in the future. This way, frustration can be\nreduced to a special case of the threat of intactness of the person.9 This is in line with the thinking prevalent in\nMahayana Buddhist schools, where the self is seen as the source of all desires and all suffering.\nThis intricate interplay between frustration and threat is seen in the very definition of threat in Chapter 7,\nwhere threat is based on anticipation of reward loss. This means that many of the properties of frustration just\ndiscussed also apply to threat: threat operates on different time scales, and threats can be based on frustrating\nexpectations or on frustrating desires. Most importantly, if there were no reward loss, there could be no threat\neither. In this fundamental sense, it is the threat that is secondary and can be reduced to frustration; indeed, all\nfear is fear of frustration. This intimate connection will be important in Part II where we consider the conditions\ncreating suffering, and Part III where we consider interventions to reduce suffering. As a sneak peek, consider\nthe proposal by Seneca, a Roman Stoic, for reducing threat-based suffering: “Cease to hope (...) and you will\ncease to fear”.10\nCan desire in itself produce suffering?\nGoing a bit beyond the theories of the preceding chapters, I am tempted to think that desire (or aversion) in\nitself, especially when combined with intention, can immediately create some kind of suffering even before\nany frustration, and even in the absence of any specific threat.11 It could be that whenever there is desire\nor aversion, the system predicts that there will be frustration with some probability, and this constitutes a\nthreat, creating suffering.12 It is in fact clear that fear, which is an aversion towards possible future events,\ndoes create suffering in itself based on a threat; perhaps other kinds of aversion and even desire share similar\n8Van Hooft (1998) cites another formulation used by Cassell (in a source that is difficult to find): “Suffering is the state of distress\ninduced by the threat of the loss of intactness or the disintegration of personhood— bodies do not suffer, persons do”.\n9I’m grateful to Michael Gutmann for suggesting this interpretation.\n10Quote from Letters to Lucilius V.7, translated by R. M. Gummere. I’m here assuming “hope” refers to an expectation of reward that\ncould lead to frustration. Likewise, Epictetus says “When I see a man anxious, I say, What does this man want?” and tells the story\nof a lyre player who is happy playing by himself but becomes nervous in front of an audience, apparently because he wants to be\nwell-received by the audience (Discourses, II.13).\n11I think such a claim is an essential part of early Buddhist philosophy, although I find it difficult to give a reference. While the early\nBuddhist texts very clearly say that desire and aversion lead to suffering, the texts are not very clear on whether they create suffering\nonly afterwards (based on frustration) or even immediately, in themselves.\n12Related philosophical ideas are proposed by Airaksinen (2019, Ch. 4).\n\nCHAPTER 9. SUMMARIZING THE MECHANISMS OF SUFFERING\n97\nmechanisms. Another possibility is that such suffering is based on a general error-signalling mechanism: the\ninternal representation of a goal state which is different from the current state is an error that may automatically\nlead to the triggering of an error signal and to suffering.13 Understanding this kind of suffering is an important\nquestion in future research.14\nWhy there is frustration: Outline of the rest of this book\nTo recapitulate, Part I of this book described a wide spectrum of frustration-related error signalling. Not reach-\ning a goal, not getting an expected reward, or making an error in predicting any event, can all be seen in this\nsame framework. They work on different time scales, and use different systems in the dual-process frame-\nwork. It seems that particularly strong suffering is obtained by frustration of planning, and even stronger by\nfrustration of self-needs.\nNext, we will try to understand why there is frustration in the first place. On some level, it is obvious that we\ncannot always reach our goals, or get what we want, if only because of the limitations in our physical skills and\nstrength: We cannot move mountains. The world is also inherently uncertain and unpredictable, so even the\nperfect plan may fail because something unexpected happens. Yet, more interesting for our purposes are the\ncognitive limitations. As argued earlier, cognition is something that can be relatively easily intervened on, and\nmodified to some extent. Thus it is more feasible to reduce suffering by focusing on the cognitive mechanisms,\ninstead of trying to develop devices that physically move mountains. Therefore, it is crucial to understand in\nas much detail as possible how various processes of information-processing contribute to suffering.\nWe have already seen several information-processing limitations that can produce or amplify frustration.\nFor example, planning is difficult due to the exponential explosion of the number of paths, which means our\nplans may be far from optimal. We need a lot of data for learning: data may be lacking to build a good model\nof the world, or to learn quantities such as state-values. Categories are often used in action selection—in par-\nticular, if the world is divided into states—but these categories may not even be well-defined. The cognitive\nsystem may be insatiable and always want more and more rewards. There are several self-related needs which\ncan create particularly strong suffering by mechanisms related to frustration.\nPart II goes into more depth regarding such limitations that produce frustration, focusing on the origins of\nuncontrollability and uncertainty. Later, Part III will consider methods for reducing suffering, mainly by reduc-\ning frustration. I will summarize all the different aspects of frustration in a single “equation” (page 172) and\npropose various methods or interventions to reduce frustration based on the theory of Parts I and II. Such in-\nterventions will largely coincide with what Buddhist and Stoic philosophies propose, and include mindfulness\nmeditation as an integral tool.\n13Related to this, it is widely observed in workplace psychology that unfinished plans create stress (Masicampo and Baumeister, 2011;\nPeifer et al., 2020).\n14One more approach is given in Chapter 10 which will propose that desire as well as some forms of aversion can be seen as “inter-\nrupts”, which may produce suffering by a special mechanism (page 105).\n\nPart II\nOrigins of suffering:\nuncontrollability and uncertainty\nThe second part will consider how uncontrollable the world as well as the cognitive system itself are, and\nhow an agent’s perceptions and thinking are uncertain and can even be called illusory\n98\n\nChapter 10\nEmotions and desires as interrupts\nPart II of this book is about better understanding why there is suffering and what increases it. Since we saw\nearlier that suffering can fundamentally be seen as frustration—possibly only prediction of frustration—the\nquestion is what kind of factors increase frustration. Part II analyzes frustration in terms of uncontrollability\nand uncertainty (which is related to unpredictability). These properties make errors in action selection likely,\nand thus lead to frustration. Even the mind itself is seen as uncontrollable, since it has multiple processes\noperating at the same time, in particular emotions (Chapter 10) and wandering thoughts (Chapter 11). Further,\nperceptions are uncertain due to incomplete input data as well as a faulty prior model of the world (Chapter 12).\nThe difficulties of communication between different brain areas or processors create a further loss of control\n(Chapter 13). Ultimately, we need to confront the problem of consciousness (Chapter 14) which creates a kind\nof a virtual reality where painful events are simulated again and again. For a sneak preview of what the system\nwill look like in the end, the reader can have a look at Figure 15.1 on page 164.\n*****\nIn this chapter, we look at the concept of emotions. Anybody pressed to give sources of suffering would prob-\nably give a list of such phenomena as fear, disgust, sadness, and perhaps anger. Those are actually some of the\nmost typical emotions in the terminology of neuroscience and psychology. If we are to understand suffering,\nwe have to understand how such emotions are related to it.\nIn Chapter 7, we saw how threat can be seen as a prediction of possible frustration. A threat triggers an\nemotion called fear. But what does fear then add to the computation that detected a threat? Many things: when\nassailed by fear, you forget everything else you were doing, you focus your attention exclusively on whatever\ncaused your fear, you try to figure out how to get rid of it, and, eventually, you run away fast. These are examples\nof the aspects of emotions we investigate in this chapter.\nWe discuss how emotions can be seen as information processing and signalling, focusing on fear as a prime\nexample. The main focus here is how emotions capture attention and interrupt ongoing processing. Another\naspect is that emotions trigger basic, pre-programmed behavioral sequences, such as running away. Impor-\ntantly, emotions are something that reduces any control we have over our minds and bodies, which is one\nleitmotiv in this part of the book. We also see that desires have similar interrupting qualities.\n99\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n100\nComputation is one aspect of emotions\nSome readers may wonder what emotions have to do with artificial intelligence. Surely, we can program an\nAI or a robot to function using purely “rational” procedures: Maximize expected reward and act accordingly,\nwithin the limits of the information the AI has, and as far it is computationally possible. Why would we need to\nintroduce anything “emotional” in the system? To proceed, we first need to understand what the word “emo-\ntion” means. Unfortunately, very different definitions are used, and there is no generally agreed definition,\neven in the limited context of neuroscience and psychology.\nEmotions have many components\nThe most comprehensive definitions define an emotion as a complex of several different components. For\nexample, if you feel fear, you will have a particular facial expression, you may scream, and your body will un-\ndergo physiological responses such as increased heartbeat. Next, your cognitive (i.e. computational) apparatus\nwill start planning how to escape from the situation, and indeed, pre-programmed behavioral routines such as\nfleeing may be activated. While all this is happening, you will also feel afraid, in the precise sense that you have\nthe conscious experience of being afraid.\nAs with almost any phenomenon in neuroscience and psychology, some emphasize the behavioral as-\npect of emotions, while others concentrate on more internal phenomena, including information-processing—\nusually called cognition in this context. Emotions are further characterized by a feeling tone: often negative\n(as in fear) but sometimes positive (as in joy). The feeling tone, technically called “valence”, is seen as the core\nof emotions by some, providing motivation for action. Yet others think that what defines an emotion is the\nconscious, subjective experience, such as feeling afraid.\nIn this book, I take an approach where all the aforementioned components together constitute an emo-\ntion.1 Nevertheless, I focus on the computational, information-processing aspect of emotions, in line with the\ngeneral approach of this book. Such information processing is eventually reflected in behavior, and at least in\nhumans, it often leads to a subjective conscious experience.\nEmotions help when computation and information are limited\nThe key question in this chapter is: how is information processed in what we call emotions; what is special in\nthat information-processing when we feel, for example, fear or disgust? The starting point here is that emotions\nare needed because of the limited information available and the limited computational capacity. If an agent\nknew exactly everything that happens in the world and had unlimited computational power, perhaps it would\nnot need emotions. A planning system would decide the best course of action—and it would really be the\nbest course of action. However, in reality, things happen that we didn’t expect. It is because the agent does\nnot know everything about the world (limited information), and the planning system cannot compute all the\npossible courses of action (limited computation). This is of course a narrative running through all AI and all\nneuroscience, but it is worth repeating.\n1Such a multi-component definition of emotions, for example by Scherer (2009), is wide-spread in psychological literature. In\ncontrast, in neuroscience and AI emotions are often defined more narrowly, or not properly defined at all. The list of such components\ngiven in the text is not at all complete; one might add bodily responses (Nummenmaa et al., 2014), such as given in the fear example in\nthe main text, as well as social aspects (Nummenmaa et al., 2012).\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n101\nThis chapter will show various ways in which emotions help in information-processing under such limita-\ntions. One implication of the limitations is that some kind of monitoring of unexpected events is needed, as\nwell as a system for changing plans accordingly. This is the role of interrupts, which is the main theme of this\nchapter and one of the specific functions of emotions. Such interrupts can also trigger pre-programmed action\nsequences or plans that have been found useful by evolution, or the programmer, which is another aspect of\nhow emotions help in steering an agent’s behavior.\nEmotions interrupt ongoing processing\nSuppose you (or a robot) are walking home on a street you know. While walking, you may be planning what\nyou will be eating tonight (the robot might be just concentrating on the walking because that’s difficult enough\nfor it). Now, a car suddenly appears and comes fast in your direction. What you need to do to survive is the\nfollowing. First, your perceptual system has to detect that something unexpected and potentially dangerous\nis happening. Second, the fact that something potentially dangerous is happening must be broadcast to the\nwhole system; you have to stop thinking about what you will eat, and you have to stop following the route back\nhome. Thus, you interrupt all ongoing activities, including your current train of thought. Instead, you have to\nuse all your cognitive resources to figure out what to do, how to jump to safety, and when.\nThe important new twist here is that once the sensory systems realize something suspicious is happening\n— even if they don’t exactly know what — they have to send some kind of an alarm signal to other parts of\nthe brain. In particular, the system responsible for executing action plans must be interrupted; in computer\nscience, such a signal from one process to another is typically called an interrupt. These functionalities go\nmuch beyond the mere “cool” perception that a car is visible and coming in your direction.\nA separate alarming mechanism with the capacity to stop ongoing activities and reorient computation is\nthe core of the interrupt theory of emotions originally proposed by Herbert Simon in the 1960s.2 The key idea in\nthis theory is that being an interrupt is what distinguishes an emotion from ordinary information-processing.\nThe interrupt theory explains why emotions have particularly powerful attention-grabbing properties; that is\nthe whole point of emotions according to this theory.3 Such an interrupt system is particularly important since\nearlier in Chapter 3 we argued, following the belief-desire-intention theory, that an agent needs to commit to\na single plan instead of jumping from one plan to another. Commitment is useful, but it should not be blind:\ninterrupting a plan must be possible.4\nPain, disgust, and fear\nAt the most elementary level of interrupts, we actually find simple physical pain. Although we don’t categorize\nit as emotion, pain is clearly a signal or a process that has such an interrupting quality. It is broadcast to the\n2(Simon, 1967; Oatley and Johnson-Laird, 1987)\n3The concept of attention is mainly elaborated in later chapters, but anticipating them, I need to point out that interrupts are closely\nrelated to a specific kind of attention which is bottom-up attention, see footnote 17 in Chapter 12. I don’t elaborate the connection\nbetween attention and interrupts here, and I use the word “attention” casually, in its everyday meaning.\n4The attention-grabbing properties of emotions are well understood by the designers of social media platforms. The more the news\nand updates evoke fear or anger, the more attention the user pays to the platform. The avowed primary goal of some such platforms is\nengagement, which is basically one aspect of attention. Some negative side-effects of designing such systems should be well-known to\neverybody by now.\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n102\nwhole information-processing system; all ongoing behaviors are typically suppressed, and the organism uses\nmost of its resources to get rid of the cause of the pain. Pain is, in fact, the most fundamental, as well as the\nstrongest kind of interrupt. It has to be so, because it is the signal which is the most relevant for the intactness\nand even the very survival of the organism. It is an alarm about a physical, chemical, or biological danger to\nthe organism—tissue damage in the terminology of Chapter 2—that typically comes from outside.5 It requires\nurgent action, such as withdrawal away from the object that caused the pain. Reflexes like this are present\neven in very simple organisms, and should be programmed even in reasonably simple robots. You don’t want\nan expensive robot to break down the very first day because it doesn’t understand what kind of actions are\ndangerous to itself.\nDisgust is conventionally classified as an emotion, although it is closely related to pain. Disgust is triggered\nby perception of substances which are likely to be toxic or transmit diseases. Again, current processing is\ninterrupted to direct attention to that substance and how to avoid it. Disgust is often a very primitive emotion:\nfor example, disgust at the smell of rotten food is very close to physical pain. This is natural since disgust is\nabout protecting the organism from something not very different from tissue damage. However, disgust has\nalso more abstract forms as in the case of disgust at morally condemnable behaviors.6\nMore complex organisms are able to predict impending danger at a much greater distance and temporal\ndelay. While disgust, and even pain, already have such a predictive quality in primitive form, complex organ-\nisms can predict risk of damage before the pain or disgust systems are activated. The emotion triggered by\nsuch anticipated danger is fear, which interrupts ongoing activity and directs processing to avoidance of the\ndangerous object. The computation of threat (Chapter 7) is clearly an essential part of the system that decides\nif the interrupt should be triggered. While detection of threat and fear are intimately related, this interrupting\nquality of fear is one reason why fear has to be seen distinct from mere detection of threat. A threat may or may\nnot lead to triggering of an interrupt, based on some further evaluation of its importance and urgency.\nDesire as an emotion and interrupt\nInterrupts can also be useful when there is no danger visible, but rather an opportunity to obtain some kind of\nreward. Casual observation tells us that something very similar to an interrupt happens when you see an object\nthat you really like and want. You are assailed by an acute, “burning” form of desire. While in neuroscience and\npsychology desire is usually not considered an emotion, there has always been some doubt about whether\nsuch a distinction is justified. Acute, burning desire actually squarely sits in the domain of emotions as far as\nthe interrupt theory is concerned.7\nIn Chapter 3, we defined the desire system as something that suggests goals to the planning system. But\nwe didn’t go into details on how the desire system actually works: How can it identify states which are easy\nto reach while having a high state-value? I think the whole point in the computations related to desire is that\nthey happen as a dual process. When desires suggest goals for a planning system, they have to do it based on\nfast neural network computations in order to usefully complement planning. As we saw in Chapter 8, neural\nnetworks, such as those in AlphaGo, can be trained to output approximate solutions to the computations of\n5In line with the interrupt theory, Craig (2003) argues that pain should be seen as an “emotion” as it includes “a behavioral drive with\nreflexive autonomic adjustments”, unlike plain sensory processing.\n6(Chapman and Anderson, 2012)\n7(Oatley and Johnson-Laird, 1990)\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n103\nstate-values and similar quantities needed for planning. It is likely that the computations underlying desire\nare based on such neural networks, which suggest candidate states that are likely to be easily accessible while\nhaving a high state-value.\nElaborated-intrusion theory\nA psychological theory that is very compatible with such goal-suggestions by neural networks as well as their\ninterrupting quality, is the elaborated intrusion theory of desires. As its name implies, it considers desire as\na computational process that intrudes your mind: it invades your information-processing system so that you\nlose control, at least initially. You are not able to think about anything else and keep planning courses of action\nregarding that object of your desire. Such ensuing compulsive planning is the elaboration part of desire.8\nEverybody has experienced such intrusions. You see a sexually attractive person, and you cannot think\nabout anything else for a while. Or, you see your very favorite brand of chocolate in a supermarket, and you\ncan hardly resist taking it in your hand and putting it into your shopping basket. You may be devising all kinds\nof sophisticated plans to get the object of your desire, forgetting completely what you were actually supposed\nto be doing. Thus, at least in humans, the simple neural networks computing desire can be in conflict with\ndeliberative planning processes. This emphasizes that desire can take control of the mind, inexorably turning\nour attention towards the object of the desire. In other words, such really “hot” desire, which could be called\n“irrational”, can give rise to a conflict between “reason and passion” —which is perhaps a poetic expression for\nthe dual-process character of the information-processing system.9\nValence\nSuch a dual-process approach brings us close to another interesting concept: valence. In psychology, valence\nis a technical term describing the intrinsic positive-negative, pleasure-displeasure, or good-bad axis of states\nor objects. From the viewpoint of subjective human experience, valence means whether feelings are positive\nor negative: positive valence is associated with pleasure, negative valence with displeasure. Valence is closely\nrelated to liking: we could equate liking and valence, saying that we like things that have a positive valence and\ndislike things that have a negative valence. Alternatively, valence can be defined based on behavior: humans\n8I follow here Kavanagh et al. (2005). A closely related model which talks about “impulses” instead of desires, and explicitly links\nthem to a dual-process theory, is presented by Hofmann et al. (2009). Similar ideas can be found in consumer research; Belk et al. (2003)\nin particular contrast desires and what they call “needs” as: “We burn and are aflame with desire; (...) we are tortured, tormented, and\nracked by desire; (...) our desire is fierce, hot, intense, passionate, incandescent, and irresistible; (...) Needs are anticipated, controlled,\ndenied, postponed, prioritized, planned for, addressed, satisfied, fulfilled, and gratified through logical instrumental processes. De-\nsires, on the other hand, are overpowering; something we give in to; something that takes control of us and totally dominates our\nthoughts, feelings, and actions.”\n9To clarify and recapitulate: We can define desire in different ways on the hot-cold axis. In the coldest definition, desire is simply\na preference for some states, essentially just another way of saying that some states are rewarding or have higher state-values. You\nmight say, for example, that you want to see Kyoto one day, but saying that does not necessarily arouse any feelings, and launch any\ndeliberations in your brain. A slightly less cold definition says that desires propose goal states for a planning system, thus possibly\nlaunching computations to attain such a state. The definition in the elaborated-intrusion theory is quite hot, emphasizing the inter-\nruptive quality of those computations. An even hotter definition, not pursued here in detail, might further add a subjective, conscious\nexperience of burning with desire, but this is outside of the computational modelling framework we take here, and presumably only\napplicable to humans and higher animals. — To emphasize the difference between different kinds of desire, such a “hot”, compelling\ndesire is sometimes called occurrent desire, while the kind of cold, long-term rational desire that simply expresses a preference is called\nstanding desire. I prefer to talk about “interrupting” desire instead of occurrent.\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n104\nas well as animals approach and try to obtain states of positive valence, and avoid things and states of negative\nvalence.10 Desire is thus usually directed towards states that have positive valence.11\nIn our framework, valence can be seen as a quick evaluation of any state or object by a neural network that\ncomputes approximations of state-values.12 When you see chocolate, its high positive valence is reflected in\nyour neural networks that predict a high state-value if you reach the state of eating it. Thus, valence computa-\ntions are necessary for interrupts based on desire. In Chapter 15 we shall discuss how the sequence valence-\ndesire-intention is essential in Buddhist philosophy: just like in the present discussion, it is valence that leads\nto desire, and further to intentions and frustration. Likewise, negative valence leads to aversion. In that sense,\nvalence computation is at the root of suffering.\nEmotions include hard-wired action sequences\nA further characteristic of emotions, and a utility of interrupts, is that they can launch “hard-wired” programs,\nor sequences of actions for specific situations. Many emotions are characterized by their specific, relatively\nrigid programs.13 The action sequence is, in fact, the aspect that most visibly distinguishes which emotion\nis taking place. In the case of fear, the typical action is to choose either freezing or fleeing. Disgust leads to\nimmediate rejection and avoidance of the substance triggering the emotion. In animals, such programs are\nevolutionarily quite old: humans have largely the same action programs as dogs.14\nThe point is that some simple action sequences are particularly useful and universal, so it is a good idea to\nhave them readily stored in the system so they can be executed quickly, without any need for elaboration. This\nis in stark contrast to the main processing being interrupted, which is often a result of long elaboration. In fact,\nsince plans may take quite a while to formulate, they are less useful in an emergency situation. Furthermore,\nit is important to have the emotion-specific action sequences readily programmed in the system—meaning\ngenetically transmitted in humans—since they can be very difficult to learn. For example, anything related to\nself-preservation is difficult to learn by reinforcement learning, since when the agent realizes that the current\nsituation is lethal, it is too late.\nAnger is another fundamental example of an emotion that clearly has its own hard-wired action sequence.\nIt also has a particularly strong social quality: real anger in the sense of an interrupt is usually associated with\nother people. While you might say that you are angry about bad weather, that is not much more than ordinary\nfrustration. We shall not consider anger in any detail here because such social aspects are completely beyond\nthe scope of this book and would require more complicated theory, in particular game theory. Let me just\n10(Colombetti, 2005)\n11Usually, people want things that they like, and vice versa. However, recent research has found that in some cases, people can want\nthings which they don’t like (Berridge and Kringelbach, 2015)—in the precise sense that those things do not produce physiological\npleasure reactions. This phenomenon is one of the underlying mechanisms in drug addictions: An addict may want the drug and\nconsume it without actually deriving any pleasure. In fact, such desires don’t even need to be conscious in humans. See also footnote 25\nin Chapter 5.\n12I shall not attempt to give a more formal definition of valence or liking here since it is not necessary for what follows.\n13Simon’s interrupt theory was elaborated by Oatley and Johnson-Laird (1987) by proposing how different emotions correspond to\ndifferent action sequences. Going further in that direction, we find Frijda’s theory of emotions as “action readiness”, meaning the\npreparation for movement or action (Frijda, 2016). Frijda’s theory sees this as the main distinguishing feature of emotions, instead of\ntheir interrupting character. But it could be argued that any simple neural-network-based reinforcement learning system can trigger\nsuch action readiness, and it is difficult to see what would be special about emotions if they were defined as simply action readiness.\n14(Gross and Canteras, 2012)\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n105\nmention the basic idea, which is that anger is a special hard-wired action sequence that protects the agent\nfrom attacks by creating a credible threat of a robust retaliation that would inevitably be triggered in case of\nbeing attacked.15\nIt is now useful to contrast emotions to habits, in the wide sense used in Chapter 5. Habits are often trig-\ngered by some environmental stimuli—a bit like interrupts—and lead to a fixed kind of behavior—a bit like the\nrigid action sequences we just mentioned. In these two ways, habits have some similarities to emotions. How-\never, habits are not really interrupts. Perhaps when you walk on the street you have the habit of humming a\ntune to yourself. However, it rarely happens that you stop whatever you’re doing because you suddenly feel an\nirresistible urge to start humming. Habits don’t have the power to capture your attention and interrupt current\nplans.\nHow interrupts increase suffering\nWe have seen that a number of phenomena, which are often considered separate in psychology and neuro-\nscience, share the important characteristic of being interrupts. Pain, emotions, and desire can all be seen in\nthis computational framework.16 But many emotions include a lot of suffering. If emotions were just inter-\nrupts, why would there be so much suffering involved?\nOne obvious reason is that some emotions are closely related to forms of suffering we have treated in earlier\nchapters: fear is triggered by a threat, while anger is a form of frustration, for example. However, this may not\nbe the whole story. It might be the case that interrupts create suffering directly, by themselves: the interrupting\nsystem is likely to use something like the pain signalling system. In fact, most emotions discussed here are\n15A simple model of anger in our framework is that it is a reaction that comes on top of frustration when another agent is causing,\nat least partly, the frustration. But this does not yet explain the evolutionary meaning of anger, and in particular why angry people\ncan behave extremely “irrationally” in the sense of causing great damage to themselves. A well-known evolutionary explanation is as\nfollows. Imagine a gangster comes to you and asks you to give him all your money. The rational thing to do would be to give the money.\nThis is rational in the sense that otherwise, he might kill you or inflict some bodily harm, and certainly it is better for you to just give\nthe money. However, this behavior has the downside that then the gangster can come to you any time he wishes and always take your\nmoney. The evolutionary explanation of anger is that it is a program that makes you behave irrationally. In this case, you would just\nget “mad”, and physically attack the gangster, even if you know he will kill you as a consequence. Surprisingly, having such a program\nmay be good from an evolutionary viewpoint, because if the gangster knows you have such a program installed, he might decide not\nto bother you. It is not good, from an evolutionary viewpoint, to actually attack the gangster; what is good here evolutionarily is having\nsuch a program installed, and signalling this to the gangster. If the gangster knows about the program, it may never be actually used,\nbecause it works as a powerful deterrent. This is a well-known game-theoretic model in evolutionary theory, it was originally used\nfor modelling the behavior of animals who fight over mating opportunities, territory, or other scarce resources (Smith and Price, 1973;\nPinker, 1999), which is why it is often called the hawk-dove game (Hirshleifer, 1987; Nowak et al., 2016). It is actually equivalent to\nanother game-theoretical model called the game of chicken, which, despite sharing an avian name, has a very different story and\nmotivation behind it. — Let me also note that social interaction creates many further emotions, such as shame and guilt, some of\nwhich are moral emotions (Haidt, 2003); that is, they enforce behavior conforming to ethical norms.\n16While considering pain, basic emotions, and desires in a single framework is not usual in neuroscience, in recent neuroscience\nliterature, ideas similar to the interrupt theory use the distinction between a planning system (“Model-based reinforcement learning”)\nand a fast system with automated reactions (“Model-free reinforcement learning”). If an agent has such two systems, a crucial question\nis how to divide tasks between the two systems, i.e. which one to use to respond to any particular situation (Daw et al., 2005). If fast\naction is required, you obviously need to use the fast system, and if there is no hurry, you can spend some time in planning; choosing\nwhich system to use is a complicated problem.. The process leading to the decision to use the fast system is then not very different\nfrom the mechanisms postulated in the interrupt theory; it has been connected to emotions by Bach and Dayan (2017).\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n106\nnegative, they hurt, and this suggests they must use the pain system, like suffering (“mental pain”) in general.17\nMaking the body feel pain is an evolutionarily primitive way of grabbing the attention of the whole cognitive\nsystem, as was discussed in Chapter 2. Interrupts need, by their very definition, to achieve such an attention-\ngrabbing effect, so using the pain system is even more natural than in the case of, say, frustration.18 (Positive\nemotions are a rather different story, and not considered here.19) However, I do not consider that possibility\nfurther here; in the following, I focus on how interrupts increase suffering coming from frustration and threats.\nInterrupts reduce control\nOne way in which interrupts increase suffering is that they reduce control, which is one of the main themes in\nthe following chapters, especially Chapters 11 and 13. A crucial part of the interrupt theory is the idea that the\ninterrupts are automatic and largely irresistible. For example, many people would be so much happier if they\ncould just consciously decide to switch off their fear system.20 But the point is that interrupts are outside of\nconscious control; they have to be so, because very often they need to interrupt conscious thinking and con-\nsciously controlled action. If you could somehow weaken interrupts so that they don’t disturb you, they would\nbe useless: it would be like switching off a fire alarm system because it is too loud. In a scary situation, fear will\nappear together with its inherent suffering, no matter how much you try to control it. We have already seen in\nChapter 8 (page 81) how the dual-system structure of the brain means that the fast, unconscious fear system\nusually prevails over any conscious deliberation. The same happens with desires: The fast computations of va-\nlence and values by neural networks will “intrude” and interrupt other processing, directing all the processing\ntowards the object of the desire. Such interrupts are even more annoying if they interrupt activity that would\nhave created pleasure, for example, when you are in a “flow”, fully engaged in a rewarding and meaningful\nactivity. In this sense, interrupts increase suffering: by increasing desires, aversion, planning, and frustration.\nSuch reduction of control might not be a bad thing if the interrupts were somehow optimally tuned to re-\nduce suffering. However, another problem with the interrupt system is that its design parameters are often\nquestionable from the viewpoint of suffering. To begin with, the system that triggers interrupts does not care\nabout our subjective suffering, only about our evolutionary fitness. Evolution makes us consider harmless\nthings as dangerous, worth triggering an interrupt, if they are threats to our evolutionary success. Sexual jeal-\nousy and the ensuing rage is one example, where (from a male perspective) the evolutionary “danger” is that\none might end up raising a child who is not one’s own and does not spread one’s genes. Yet, that is hardly a\nproblem from a contemporary viewpoint: it is actually very common in modern families.\nWhat’s more, the system may not actually be very good at maximizing evolutionary fitness either. As we\n17(Papini et al., 2015; Eisenberger and Lieberman, 2004)\n18Thus, such negative emotions with interrupts might be introducing a mechanism for suffering which is a bit different from what\nhas been discussed previously. On the other hand, it could be argued that the suffering due to fear is simply the suffering from a threat,\nand triggering an interrupt does not necessarily amplify it. Likewise, it could be argued that the suffering in anger is just a special kind\nof frustration. I do not take a definite stance on this point.\n19Some positive emotions, or rather attitudes, are considered in Chapters 18 (acceptance, letting go, contentment) and Chapter 19\n(compassion, loving-kindness). Let me just mention that Fredrickson (2001) proposes positive emotions serve the role of enabling\nexploration of different action possibilities when the circumstances are safe and not even remotely life-threatening, thus leading to\nenhanced creativity and learning.\n20This thought experiment is actually a bit complicated since even if the fear system as an interrupt is switched off, the threat system\nmight still be operating, thus creating suffering. Arguably, though, the threat signal would be amplified since the interrupt focuses the\nattention on the threat; without the interrupt-alarm system, the threat might not be detected at all.\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n107\nsaw earlier, evolutionarily developed neural mechanisms may not be well adjusted to our current society, since\nthey may come from the legendary “African savannah”. In the case of fear, for example, we tend to be afraid of\nsnakes or spiders, but not so much of cars, although cars are much more dangerous at least in modern cities.\nAnother problem with the interrupt system is that if the interrupts are excessive and disrupt the normal\nfunction of the system too much, it may simply worsen the situation by making it more difficult to respond\nto the situation. Such problems are related to the fact that emotions and desires are short-sighted—as has\nbeen acknowledged by philosophers since antiquity—and may interrupt useful plans in a way that produces\nfrustration because the interrupts fail to understand the long-term utility of following the plan. For example,\nan important function of pain is to attract the attention of the agent to the source of the pain; but if the person\ncan think of nothing else than the pain, as often happens in the case of overwhelming fear or depression, he\nwill not be able to find a solution to the situation. Or, if you are easily scared and are constantly interrupted by,\nsay, harmless bugs, your performance in a meaningful pursuit may be hampered even though there was never\nany real danger to avoid.\nAlarm systems cannot be universally optimal\nThese questions are related to the general theory of designing alarm systems, which is considered in the math-\nematical theory called signal detection theory.21 It is based on maximizing the expected payoffs, where payoffs\nare similar to rewards, describing how good (positive) or bad (negative) the results of a given action are. For\nan alarm system such as interrupts, there are two possible actions: trigger an alarm, or do not. The theory is\nrelated to the AI theory outlined in previous chapters but with a different emphasis. An important lesson in\nthis theory is that there is no such thing as a universally optimal alarm system. That is because the payoffs are\ndifferent for different people, and different in different contexts, and may change over time.\nConsider designing a burglar alarm system. You might start by assigning a high payoff to detecting burglars;\nthis sounds reasonable and innocuous. However, this means the system will not mind making false alarms,\nsince you only give a strong payoff (reward) for the detection of a burglar, but you do not give any punishment\nfor false alarms. To maximize reward, the system rationally decides to trigger an alarm if there is any hint of a\nburglar present. Eventually, the system will constantly wake up everybody in the middle of the night. Realizing\nyour mistake, you change the design by adding a really high reward for not giving false alarms. The result is that\nthe system never gives any alarm because that’s the perfect way to avoid false alarms, which are now strongly\npunished. In this case, the alarm system ends up being completely useless since it does not do anything. It is\nvery difficult to say what the right compromise is: the alarm system should be sensitive but not too sensitive,\nand the right parameters are quite subjective and depend on the context. Evolution has programmed certain\nsensitivity levels in our interrupt system, but in light of this signal detection theory, it is not actually clear how\noptimal they were even for all our ancestors on the African savannah, let alone for modern city-dwellers.22\n21(Green and Swets, 1988). This is a special case of statistical decision theory, typically considering the case of two possible options\ngiven by “present” or “absent” (regarding a threat), and focusing on the question of finding the right balance between false positives\nand false negatives.\n22The sensitivity levels, or thresholds, can also be modified by experience to some extent. For example, if as a child, you saw some-\nthing that made you really scared, you may lower the threshold a lot—commonly known as a phobia. This leads to another problem,\nwidely recognized in clinical psychology: the payoffs change during an individual’s lifetime as well. In adults, they may be very different\nfrom what they were in our childhood environment, while any learning of the payoffs may mainly happen as a child. The best survival\nstrategy for a child in an adverse environment may be to be constantly afraid of other people; this may not be optimal anymore when\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n108\nEmotions are boundedly rational\nOften, emotions are contrasted with rationality and “cool-headed” decision-making; it is typically assumed\nthat the best decisions are made when emotions are not at play. The word “rationality” comes from the Latin\nword ratio, meaning “reason”, and Western philosophy has traditionally considered reason and emotions (or\n“passions”) as two opposing forces.\nHowever, the viewpoints on emotions explained in this chapter show that emotions contribute to optimal\ndecision-making and action selection. Emotions are useful from a rational viewpoint as soon as there are\ncertain information-processing constraints; for example, if the planning system does not have time to consider\nall possible paths in the search tree. This is certainly true in any sufficiently complex AI system or animal. The\nviewpoint which considers emotions as necessarily irrational is in fact largely rejected in modern research.23\nI have casually used the word “rational” here as well as in earlier chapters, but we need to think a bit more\nabout what it actually means. In mathematical decision theory, a decision is called rational if it is optimal in\nmaximizing reward (or a similar quantity) given the information available to the agent. In other words, the\ndecision of the agent, such as choosing an action, is the same as that made by an ideal, hypothetical agent with\nperfect information-processing capacities and the same information about the world as the agent in question.\nSo, even a perfectly rational agent is not expected, in this definition, to make the very best possible decision,\nbut the best possible given the limited information it has at its disposal.\nHowever, in reality, the information-processing power of the agent is limited as well, as we have indeed\nseen in many chapters of this book.24 The case where information-processing power is also limited leads to\nthe concept of bounded rationality, also called computational rationality. It refers to decisions that are optimal\ngiven limitations in both the information and the computation available to the agent.25\nEmotions, seen as interrupts or as automated action sequences, can be considered to strive towards\nbounded rationality. Emotions are information-processing routines or shortcuts which help in achieving as\ngood outcomes as possible, given the computational resources and the limited information available. It is in\nthis precise sense that we can say that emotions help in rational decision-making, and it is not justified to\noppose rationality and emotions.26\nthe child grows up, and is in fact a possible source of psychiatric problems.\n23(Damasio, 1994; Scherer, 2011)\n24Curiously, largely due to historical reasons, limitations in information available to the agent were always admitted, but computation\nwas not supposed to be an issue in earlier work on rationality. I’m here referring to the classic work on statistical and economic decision\ntheory in the first half of the 20th century, arguably culminating in the work by Von Neumann and Morgenstern (1944).\n25(Simon, 1972; Russell, 1997; Gershman et al., 2015; Lieder and Griffiths, 2020)\n26Another rather different information-processing function of emotions has been proposed as the “somatic marker hypothesis” by\nAntonio Damasio (Damasio, 1994; Bechara and Damasio, 2005). Somatic markers are defined as bodily responses to situations, learned\nfrom past experiences. If a certain situation has led to a bad outcome (e.g. a strong negative reward), you learn to associate such\na situation with a bad “gut” feeling in your body. The somatic marker hypothesis thus shows how such feelings (here considered the\nessential part of emotions) can be used to improve planning by using them as heuristics. As we saw earlier, the central problem in action\nselection is the huge, exponential number of plans to consider. Using somatic markers as heuristics, you may be able to reject many of\nthem based on such negative feelings and focus your search on the set of plans associated with positive gut feelings. Importantly, such\n“gut feelings” are generated by a very fast computation in a simple feed-forward neural network, thus speeding up decision-making\nand planning—not unlike the computations we linked to desire, valence, and dual-process action selection earlier in this chapter and\nChapter 8. Computationally, such somatic markers would be a bit like a rat searching for cheese by maximizing the smell, i.e. using the\nstrength of the smell as a heuristic, as in the example in Chapter 3. However, such a heuristic can of course be misleading: something\nthat gives a bad gut feeling may actually turn out to be good when you think about it a bit more.\n\nCHAPTER 10. EMOTIONS AND DESIRES AS INTERRUPTS\n109\nYet, emotions also have qualities that are in contrast to our everyday notion of rationality. Emotions can\nbe based on very limited information, such as one scary object, and they are often short-sighted, neglecting\nlong-term consequences. Emotional interrupts are, by definition, the very opposite of commitment to a goal,\npossibly leading to inconsistent and impulsive behavior. In contrast, rational decisions—in the common-sense\nmeaning of the word—look at long-term consequences and use many different sources of information, includ-\ning past experiences and information shared by other people. Furthermore, emotions and the hard-wired\naction sequences they trigger are not entirely under conscious control, while consciously controlled delibera-\ntion is an important part of what is classically called reason. In these two ways, emotions are analogous to the\nneural networks in dual-process theories, as discussed in Chapter 8, and reason is more similar to the explicit,\nconscious, GOFAI-like system. As we have seen, those two systems are sometimes opposed to each other but\noften work together; this might be a good characterization of the relationship between reason and emotions as\nwell.\n\nChapter 11\nThoughts wandering by default\nThe moment you lie down on a sofa to relax, your head starts developing different fantasies and daydreams,\nperhaps wondering why you did such a stupid thing yesterday, or planning what you want to eat tonight. Even\nwhen you try to meditate and not think about anything (which is a typical instruction for beginning medi-\ntators), you will almost inevitably find yourself thinking about something else after a while. There is a good\nreason why the human mind is often compared to a monkey in meditation traditions. It jumps here and there,\nmaking all kinds of noises, and never seems to rest. Likewise, based on his own method of introspection, David\nHume concluded: “One thought chases another, and draws after it a third, by which it is expelled in its turn.”1\nThoughts that come to your mind when you are trying to concentrate on something else are called “wan-\ndering thoughts”. They have some similarities with emotional interrupts: they stop ongoing mental activity\nand capture attention. Thus, they reduce the control you have over your mind and, eventually, increase suffer-\ning. However, the computational underpinnings are quite different in the two cases. In this chapter, I discuss\nhow wandering thoughts are related to the need to repeat experiences for the purposes of iterative learning\nalgorithms, as well as planning the future through search in a tree. Thus, there is an evolutionary reason why\nwe have wandering thoughts: they are not just pointless activity triggered by mistake.\nWandering thoughts and the default-mode network\nWandering thoughts tend to appear whenever a person tries to focus on a single task or object for a long time.\nEverybody has encountered a situation, perhaps at school or at work, where she tries to concentrate on some-\nthing but soon finds herself thinking about what she should say in a job interview tomorrow, or what she did\non a previous vacation. Typical tasks where such sustained attention is necessary, but difficult to achieve, are\ndriving a car on a highway, trying to read a book for an exam, or monitoring a screen as in air traffic control or\nsurveillance. Importantly to the theme of this book, sustained attention is essential in most meditation prac-\ntices. If you are lying on a sofa and have nothing else to do, meandering thoughts about various things are\nfine and sometimes even enjoyable. However, when you are actually trying to concentrate on a task, unwanted\nwandering thoughts reduce your performance of the task at hand.2\n1Hume, A Treatise of Human Nature, Section 1.4.6\n2In the case where you have no particular task to perform, the spontaneously appearing thoughts may not be properly called “wan-\ndering”, but, for example, “spontaneous”. Some authors strictly reserve the word “wandering” for the case where the thoughts are\nintrusive in the sense of occurring against your will while you are trying to concentrate on some task, such as thinking about some un-\n110\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n111\nPsychological experiments confirm the ubiquity of wandering thoughts. Various experiments can be de-\nvised where the participant’s task is to monitor a stream of information and report when a rare prespecified\nevent occurs. In a typical experiment, you would be shown random digits (0 to 9), and you have to press a\nbutton when you see a target digit, say, 3. The experiment is deliberately designed to be boring so that the\nparticipant’s mind will certainly start wandering at times. The basic idea of monitoring for an event that is rare\nis reminiscent of some of the typical real-life tasks listed above (e.g. driving a car on a quiet highway), where\nnothing much happens most of the time, and sustained attention is difficult. The experimenters would then\nuse a method called experience sampling, which means they ask, at random intervals, whether the participant\nwas focused on the task or whether they had wandering thoughts. It typically is found that the participant’s\nperformance on the task fluctuates between better and worse; this fluctuation largely reflects whether they\nhad wandering thoughts at that particular time point or not.3\nSuch experiments can be conducted even when the participants are living ordinary everyday lives. The\nparticipants would have a device, such as a mobile phone, which asks at random intervals whether they were\nfocused on whatever task you were performing (such as working, studying, cleaning, driving, etc.) or whether\nthey had wandering thoughts (such as daydreaming, fantasies). It is typically found that during everyday life,\nthe mind is wandering quite a lot: one third, or perhaps even one half of the time.4\nMuch of brain activity is spontaneous\nAt the same time, modern neuroimaging confirms the prominence of various kinds of spontaneous brain ac-\ntivity, i.e. activity that “just happens” without any external stimulation or task being performed. In fact, an\namazing finding in recent neuroscience is that if you measure human brain activity when the participants of\nthe experiment are simply told to sit or lie still and think about nothing in particular, their brains are far from\nquiet. Technically, neuroscientists talk about “resting-state” to characterize such a state of not doing anything\nin particular, since the participant may think she is having a rest—but the brain is definitely not.5\nA particular network in the brain is actually even more active during rest than during active tasks. It is called\nthe default-mode network because it seems to be activated “by default”, i.e. when there is no particular reason\nfor anything else to be activated.6 It is also deactivated once the person is stimulated, for example, by sights or\nsounds from the external world, so that the brain actively starts processing incoming information.\nThe discovery of the default-mode network around the year 2000 was something of a revolution in human\nrelated event tomorrow when trying to concentrate on reading a textbook. In this book, I use the term wandering thoughts a bit more\nliberally, sometimes including thinking that jumps from one topic to another when there is no particular task on which it is supposed\nto concentrate—as in lying on the sofa after work—since in real life, it is often difficult to draw the line between wandering and other\nspontaneous thinking.\n3(Christoff et al., 2009)\n4(Kane et al., 2007; Killingsworth and Gilbert, 2010)\n5In terms of neural network theory, resting-state activity is enabled by the brain having intrinsic dynamics based on recurrent con-\nnections. Recurrent connections mean that the neurons are not arranged in successive layers where the signal just goes in one direc-\ntion: Instead, the outputs of some neurons are fed back to other neurons that actually provided input to those neurons in question.\nThe output can also be fed back to the outputting neuron itself. With such feedback, neurons can learn to sustain each other’s activity:\nNeuron A activates neuron B, which by recurrency again activates neuron A, and so on. Even a single neuron can sustain its activity by\nsending feedback activation to itself (Hopfield, 1982; Hochreiter and Schmidhuber, 1997). Such recurrent connections are extremely\ncommon in the brain, while the most commonly used neural network models have no recurrent connections; this is an important\ndiscrepancy.\n6For recent reviews, see Buckner et al. (2008); Raichle (2015), for the original articles, Shulman et al. (1997); Raichle et al. (2001).\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n112\nneuroscience. It was completely at odds with the classical way neuroscience experiments were done: the ex-\nperimenter would instruct an experimental subject to observe some stimuli (e.g. a sequence of digits as we saw\nabove) and possibly perform a task at the same time (e.g. press a button when a target digit appears). Here, in\ncontrast, you don’t tell the participants to do anything and don’t give them any sensory stimulation, such as\nshowing pictures. Then, it is the default-mode network that becomes activated. In fact, since it is deactivated\n(i.e., silenced) by sensory stimulation and tasks, the experimenters had better not give any stimuli or tasks to\nbe able to observe it.7\nIt is widely assumed that the default-mode network supports wandering thoughts.8 That would explain\nwhy it is particularly activated when the subjects do not receive any stimulation and have no particular task:\nthen, the mind will easily start wandering. It is likely that the default-mode network has other functions as well,\nalthough we don’t know very well what they might be.9\nWandering thoughts as replay and planning\nThe existence of wandering thoughts may feel completely normal to us, but actually, it is rather surprising that\nthe whole phenomenon exists. Why should it be difficult to concentrate on one thing for a long time? Why\ncannot I just decide to focus on reading a textbook for an exam, say for two hours, without any interruption by\nany unrelated thinking?10\nOne intuitively appealing explanation would be that your active neurons—in the exam-reading example,\nthose needed for reading—get “tired”, i.e., somehow run out of energy. Then, other neurons which are full of\nenergy will be able to somehow steal the attention. While there may be some truth in such an explanation, it is\nnot very compelling because sometimes you can concentrate without any problems on a task, especially on a\ntask which is really engaging, such as reading a book you really like (not for an exam), or playing a video game.\nFurthermore, should not such fatigue of neurons rather lead to having no thoughts at all? It is more plausible\nthat wandering thoughts are actually doing some useful computation—and that they are something that you\nwould like to program in an AI.\nSo, let us think about what kind of computational problems could be solved by wandering thoughts. One\nproblem we have seen earlier is that learning typically needs many repetitions of the inputs and the desired\noutputs since it is based on iterative algorithms, as we saw in Chapter 4. Even the very same inputs and outputs\nmay need to be presented many times to the learning algorithm. This is why modern AI systems need a lot of\ncomputing capacity for learning. At the same time, planning takes a lot of time as well, as we saw in Chapter 3.\nSo, as much of the computing capacity as possible should be directed to these learning and planning ac-\n7Actually, it has further been found that many of the same brain networks that are intermittently active in various neuroscience\nexperiments are also intermittently active in resting-state. Thus, the default-mode network is not the only network activated in resting-\nstate, but the default-mode network is perhaps the only one that is more active in resting-state than in any kind of stimulation or task.\nWhen we talk about a “network” here, we mean more precisely that the activities measured in certain voxels (i.e., 3D pixels) in the\nimaged brain activity seem to be fluctuating synchronously (Damoiseaux et al., 2006; Fox and Raichle, 2007). Typically the analysis is\nmade using a machine learning method called independent component analysis (Hyvärinen et al., 2001). Possibly the first study to\nprovide such a decomposition to several networks was in fact based on data from anaesthetized human children, whose brains were\nscanned for clinical purposes (Kiviniemi et al., 2003) .\n8(Christoff et al., 2009; Andrews-Hanna, 2012)\n9(Raichle, 2015)\n10(van Vugt et al., 2015)\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n113\ntivities. In particular, when the agent does not receive any special stimulation from outside, there is nothing\nimportant for it to do, and no urgent threats are detected, the computing capacity of the agent is free to be\nused for any internal processing based on previously acquired data—intrinsic activity, in the terminology of\nneuroscience. In fact, in order not to waste that computational capacity, computations related to learning and\nplanning should be launched. That will enable the agent to act more intelligently when the time to act comes.\nThis is also presumably why evolution has programmed wandering thoughts in us.11\nNext, I consider in detail two different ways in which wandering thoughts can help in computation. In the\nfirst one, the system plans future actions actions by internally simulating the world, and trying out different,\nnew actions to see which works best.12 The second one is called experience replay because the system internally\nrepeats memories of past behaviors and events exactly as they were perceived, in order to enable an iterative\nalgorithm to learn efficiently.13 In fact, a lot of what people simply call “thinking” falls into these two categories:\nYou plan what to do in the future, and recall what happened to you in the past.14\nPlanning the future\nIt is perhaps obvious why thinking about future actions is useful, as far as it is a case of planning. You can go\nthrough different kinds of plans and simulate, using your model of the world, what the results of your actions\nwill be, and finally, choose the best one. If you think about a job interview that will take place tomorrow, you\npolish your answers beforehand by simulating what kind of impressions different options will make, eventually\nmemorizing the best ones. Often, such thinking and planning is actually completely voluntary. If you really\nwant to spend some time and energy to elaborate the best course of action, this is quite normal planning\nactivity. When we talk about wandering thoughts, we mean a case where you consciously try to do something\nother than planning, but unrelated planning thoughts nevertheless appear. It is the unwanted, intrusive quality\nof wandering thoughts that distinguishes them from ordinary thinking.15\nYou might actually want to relax and read a novel, but thoughts simulating the job interview just pop into\nyour mind. This is understandable since as I just argued, it is especially during moments where you or the AI\nhave nothing pressing to do that it would be a good idea (from the viewpoint of the designer of the system) to\nuse the computing capacity for such planning. As we saw in Chapter 3, planning paths grow exponentially as a\nfunction of time, so there is a real need for using a lot of computation for planning.\nThe planning during wandering thoughts is a bit special in that it sometimes has no particular goal. It\nmay be just looking at possible future paths in a big search tree to see what could be done to obtain rewards:\na kind of ongoing, free-style planning. Such a search could actually be done by the Monte Carlo Tree Search\nalgorithms (discussed in Chapter 8): they are randomly searching for plans, while focusing more on branches\n11Humans also sleep and dream: It is possible that the function of dreams is pretty much the same as that of wandering thoughts\n(Fox et al., 2013).\n12(Baird et al., 2011). Chapter 3 already reviewed the basic theory of planning.\n13(Lin, 1991; Wittkuhn et al., 2021)\n14The computations in planning are actually not that different from the computations in reinforcement learning, as was already\ndiscussed in Chapter 5. See also footnote 18 below on how the two computations could be combined, as well as a framework proposing\nsomething between these two kinds of action selection by Lengyel and Dayan (2008).\n15In humans, planning can thus be done in various modes which differ in their relation to conscious control: there is controlled,\nconsciously initated planning on the one hand, and spontaneous, unconsciously initiated planning on the other hand. The sponta-\nneous planning can further be divided to unwanted/wandering thoughts and simply spontaneous thoughts which are not unwanted\n(perhaps because you are lying on the sofa), see footnote 2.\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n114\nwhich seem to be more rewarding. It’s a bit like thinking about what to do during the weekend when you’re\nsupposed to be concentrating on your work. Nevertheless, wandering thoughts often do focus on planning for\na specific goal, as in the job interview example above.16\nExperience replay for learning value functions\nIn contrast to planning, it may be more difficult to understand why any system would like to simply repeat past\nexperiences. You already saw what happened yesterday, so why repeat it in your mind, and why so many times?\nThe reason is in the structure of the algorithms used in learning.\nAs we saw in Chapter 4, modern AI systems are based on learning from the data by using iterative algo-\nrithms. We saw the general idea of stochastic gradient methods: the data points (e.g. images) are presented to\nthe system one by one, and a huge number of repetitions is needed. Many reinforcement algorithms are not,\nstrictly speaking, stochastic gradient descent methods, but are closely related and share those properties. They\nproceed by observing the state of the world both before and after each action, as well as any reward obtained\nor punishment received. There are thus four pieces of information in what we might call a single “data point”:\nthe state before the action, the state after the action, the action taken, and the reinforcement. Based on these,\nthe system updates the state-value function.\nWhat is crucial here is that, again, learning proceeds by making tiny modifications to the parameters of the\nsystem, in this case those computing the state-value function. Successful learning therefore requires a huge\nnumber of iterations, or presentations of such actions and their consequences to the learning system. If you\nhave access to really large amounts of data, you may just present each data point once, and learning will be\nsuccessful since the algorithm will have enough iterations anyway. However, the amount of data is typically\nlimited. In the case of reinforcement learning, what is particularly problematic is that the agent may need to\nact in a real environment and observe the consequences of its actions to gather data. One action by a real robot\ncan take a second or so, which is extremely slow compared to the processing speed of most computers and\nthe potential speed of learning. Likewise, humans do not collect new experiences on, say, job interviews, that\noften.17\n16In game-playing AI, planning by simulation has been used in an extreme way in terms of “self-play”. A much-publicized example\nis AlphaGo, the system that first beat humans in the board game of Go, which we used as an example of dual processes earlier (Silver\net al., 2016). After being input information on a huge number of actual games played by humans, it started playing against itself. This\nis a very special kind of planning, where you are simulating your opponent as part of the environment. Actually, there is no distinction\nbetween the agent itself and the opponent since the same program plays both of them, and learns from the successes and failures of\nboth of them. A later version of the AlphaGo system actually omits the learning from human games altogether and learns entirely by\nplaying against itself; the ensuing system is aptly named AlphaGo Zero (Silver et al., 2017). Pure self-play has also allowed for an AI\nto rapidly approach human level in a highly complicated, multi-player esports video game called Dota 2: Using more than 100,000\nprocessors running self-play in parallel, the OpenAI Five system can simulate in one day the same amount of data that would take\nmore than a hundred years to collect in ordinary play against humans (OpenAI, 2018). Self-play was also used to achieve super-human\nperformance in the game of poker (Brown and Sandholm, 2018). Actually, such learning by self-play was successfully used earlier in\nsimpler games such as backgammon (Tesauro, 1995) and, even back in 1959, in checkers (Samuel, 1959), in one of the earliest projects\non learning by machines. While some human knowledge was input to the learning process in most of the preceding studies, Tesauro\n(1995) also reported a variant with pure self-play similar to AlphaGo Zero. Something akin to self-play is actually used by humans when\nthey are simulating social encounters in their own minds: We might use the same model for the actions of other people and the actions\nof ourselves, and learn both simultaneously. (Yet, the connection between our model of our own mind and our model of the minds of\nothers is complex, see Carruthers (2009) for different possibilities.)\n17In fact, calculations of the amount of data that humans observe in real life show that the number of data points needed in AI is\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n115\nThis is where experience replay is useful. It means that in reinforcement learning, the system is not just us-\ning the data related to the most recent action and then throwing it away; instead, it stores the data, and re-uses\npast actions and the states associated with them many times. That is, it “replays” or recalls past actions and\nevents and uses them in the iterative learning algorithm as if they happened now. This improves the perfor-\nmance of the learning algorithm by enabling it to make many more iterations with each data point, and thus\nmany more iterations for the same limited amount of data. This is how more information is extracted from the\ndata. This is particularly powerful since in most cases, the agent can retrieve past events from memory much\nfaster than it would actually act in the real world, and thus replay makes learning much faster. There are other\nreasons as well, as we will see below.\nObviously, there is a trade-off here: If you just use all your time replaying old events from memory, you will\nnot get new data about reinforcement resulting from actions. So, you cannot use all your time for just replay.\nIt should be smart to engage in replay when the environment does not enable too many meaningful actions\n— in plain English, when nothing interesting is happening and the agent is “bored”, which points directly at\nwandering thoughts.\nIt is also possible to do something between pure replay and planning. You can replay past events while try-\ning out different actions in a simulation. This means the system starts by recalling something that happened\nearlier, but then it simulates what would have happened if it had acted differently. Certainly, we all have experi-\nenced such wandering thoughts: “If, yesterday, in that situation, I had done X instead of Y...” This is even better\nthan just replaying actual past events since the system is then creating new data using past events together with\nits model of the world.18\nReplay and planning focus on reinforcing events\nAny replay method must choose which events, or short “episodes” of events, it will replay. A system that has\ngathered a lot of data on past actions cannot just indiscriminately replay everything if it wants to learn really\nefficiently. Likewise for planning: if the system starts planning in its idle time, it needs to choose the starting\nstate for its plan—what kind of a situation your fantasy starts in—and perhaps a goal as well.\nA dominant idea in AI is that replay should prioritize events where any kind of reinforcement signal was\nobtained, whether positive or negative, and this seems to be the case in the brain as well. Experienced episodes\ncontaining such events are the most important in computing the state-value function. This may help explain\nwhy we have so many wandering thoughts about negative events. When you do something embarrassing, it\nmay replay in your mind many, many times. This should be useful so that you learn to associate the negative\nreinforcement (social embarrassment) with the actions you took in that particular situation, thus improving\nyour estimate of the state-value function—and future behavior.\nIt has been found that replay of past events can be particularly useful if the experience is replayed back-\nwards, starting from reinforcing events. Suppose a robot gets a particularly nice reward (say, a lot of energy in\noften much larger than what humans need. For example, children seem to learn to speak from a relatively small number of “input”\nwords (Dupoux, 2018; Warstadt and Bowman, 2022); current AI systems need orders of magnitude more. Perhaps even more strikingly,\nhumans can learn from a single example they see or hear, as I have pointed out earlier, see e.g. Lake et al. (2015). This shows that there\nis a lot of room for improvement in AI compared to the brain in terms of efficiently using all the data.\n18(Sutton, 1991; Sutton and Barto, 2018). See also Mattar and Daw (2018) for a theoretical unification of replay and planning, and\nKurth-Nelson et al. (2023) for neuroscience results and theory on how replay is more than just repeating past episodes. For a popular-\nscience account focusing on the utility of regret, see Pink (2022).\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n116\nits batteries) whenever it finds itself in room #42 of a building where it cleans the floors. Based on this expe-\nrience alone, it will immediately assign a large state-value to room #42. But in order to find room #42 in the\nfuture, it has to code its location with respect to the other rooms in the state-value function. This is easy to\ndo if it replays its path to room #42 in reverse order. Suppose just before arriving in room #42 it was in room\n#13, and before that, room #21. It replays the sequence in reverse: #42, #13, #21. Now, it will assign large but\nslightly decreasing state-values to each of these rooms, so that the state-value is decreasing the further the\nreverse replay goes—the decreases are justified by the theory of discounting. The end result is that while #42\nhas the largest state-value, #13 has a rather large one as well, and #21 is not far behind. Now, if the agent ever\nfinds itself again next to room #21, it knows that to find a state with a large state-value, it should enter room\n#21, and there it will understand the best choice for the next state is #13, and eventually #42. (It may sound\nlike all this could be learned by a single replay, but in reality it must happen by smaller increments to properly\ncombine information from many different paths and data points.) Combining such backward replay with the\nabove-mentioned prioritization of reinforcing events leads to a method called “prioritized sweeping”.19\nIf wandering thoughts use such a prioritizing form of replay, they are closely related to the theory of emo-\ntions as interrupts discussed in Chapter 10. Both mechanisms direct the agent’s processing (one might say\nattention) towards dangerous or rewarding events. Emotional interrupts are more primitive, typically focused\non easily identifiable and evolutionarily important threats that are present in the current state. In contrast,\nwandering thoughts are about learning, activated when no threat is currently being observed, and potentially\nlead to quite sophisticated behaviors.20\nWhen wandering thoughts implement planning, the question of choosing the goal for planning arises as\nwell—unless the thoughts are about random planning without any particular goal. In line with emotional inter-\nrupts, Chapter 10 proposed that the goal for planning could be given by neural networks based on perception\nof desirable objects or states. A similar mechanism could be working with wandering thoughts; the main dif-\n19(Moore and Atkeson, 1993; Schaul et al., 2016; Singer and Frank, 2009) More precisely, the prioritization mechanism replays mem-\nories of individual states (and actions taken in them) whose replay leads to maximal change in the estimated state-value function. This\nis not exactly the same as replaying episodes where a strong reinforcement occurred, as proposed earlier in the text, but it is closely\nrelated. Typically, a strong reward or punishment is unexpected, at least in the beginning of the learning. When you find a reward the\nfirst time, your state-value function is in some rather random initial state, and you could not really predict that the reward would be\nobtained; thus any reward is initially surprising. That is why prioritized sweeping prioritizes, as a first approximation, episodes con-\ntaining reward or punishment. Alternative theories for choosing what to replay are considered by Isele and Cosgun (2018); Antonov\net al. (2022).\n20A lot of replay is probably related to rewards, and thus to planning and reinforcement learning, but some part of wandering\nthoughts and replay is clearly independent of any rewards. We saw earlier that people are able to perform unsupervised or super-\nvised learning from a single representation of a data point (page 83). If you hear a nice melody, it may be replayed it in your mind\nrepetitively, even quite obsessively. Such replay if best understood as performing some kind of unsupervised learning—which does\nnot need any kind of reward or reinforcement signal. For example, it can be Hebbian learning or some kind of feature extraction,\nwhich learns the melody and its characteristics particularly well by repetition. The crucial similarity between reinforcement learning,\nHebbian learning, and most kinds of machine learning is their iterative nature, and in particular, the need for many iterations. Some\nof that data may not be real data replayed, but simulated data more akin to planning; such simulation can in fact be used to perform\nlearning in a Bayesian framework (Gutmann et al., 2018). An alternative theory on resting-state activity actually links it to the priors\nused in Bayesian perception (Berkes et al., 2011; Aitchison and Lengyel, 2016; Hoyer and Hyvärinen, 2003). The idea is that activities\nof the neurons in resting-state, at least in the sensory cortices, follow the prior distribution of those features that they are encoding.\nWhile this theory is not framed in terms of replay, we could interpret it as saying that resting-state activity is in some sense “replaying”\ntypical sensory inputs. These two theories may thus not be incompatible, the replay or wandering thoughts theory focusing on reward\nprocessing and the Bayesian theory focusing on basic sensory processing.\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n117\nference is the time scale since wandering thoughts are typically about something that it is not possible to do\nright now. One theory actually proposes that wandering thoughts focus on goals that have been selected but\nnot yet reached.21 This would lead to a typical daydream where you think about achieving something nice\ntomorrow. On the other hand, a lot of planning is also about avoiding things or processing threats. Psycholog-\nical theory has for a long time connected such spontaneous planning to worrying, which is basically planning\nagainst threats anticipated in the distant future.22 Thus, even in the case of planning, wandering thoughts are\noften related to emotionally charged events.\nReplay exists in rats, humans, and machines\nReplay has long been observed in neuroscience experiments. Typical experiments measure brain activity in\nrats, which are running in a maze, seeking food or drink. A brain area called the hippocampus is specialized\nin storing episodes and events —such as the sequence of running forward, turning left or right, and finding\ncheese. It is thought that the hippocampus replays such episodes, simultaneously signalling them to other\nbrain areas, which then use such replayed input for learning. Replay was initially observed during sleep, but it\ncan also be seen in awake rats.23 Recent experiments also show that something similar to prioritized sweeping,\nwhere the events are replayed backwards, seems to be happening in rodents.24\nResearch has also found brain activations that look like planning: a rat can initiate sequences of events\nwhich it has not yet experienced, but which it might perform in the future. For example, the rat can in some\nsense “imagine” a possible trajectory in a maze, which it may or may not follow later.25 So, the mammalian\nbrain seems to use strategies which are very similar to what you would expect from the design considerations\nof AI. This is not surprising since the brain and AI are trying to solve the same computational problems; but it\nis also the case because the AI designs have been influenced by neuroscience research.\nIt may in fact be that such processing in rats is not very different from wandering thoughts considered in\nhuman psychology. Something at least resembling replay by prioritized sweeping can also be observed in the\nhuman brain, although the limitations in measurement technology make it difficult to draw exact parallels.26\nWhile replay is usually connected with the hippocampus, and planning with the default-mode network, the\nhippocampus is actually part of the default-mode network according to some definitions.27 (Rats do have a\ndefault-mode network just like humans.28) The connection between wandering thoughts and the hippocam-\n21(Klinger, 2013). His theory actually considers “spontaneous thoughts”, which are more general than wandering thoughts. It fur-\nther includes the interesting idea that wandering thoughts may not be just triggered when the computational capacity would be idle\notherwise, but they could also be triggered when there is a goal, perhaps with an intention or commitment to it, but it is not currently\npossible to actually perform any meaningful action to reach the goal. Then, planning to reach that goal may be triggered involuntarily\nand lead to wandering thoughts.\n22On worrying, and its relation to anxiety, see in particular the discussion of the literature by Stawarczyk et al. (2013). Closely related\nis the proposal by Revonsuo (2000) on the function of dreaming during sleep: “the biological function of dreaming is to simulate\nthreatening events, and to rehearse threat perception and threat avoidance.”\n23Sleep: (Buzsáki, 1996), awake animals: (Karlsson and Frank, 2009)\n24(Diba and Buzsáki, 2007; Ambrose et al., 2016)\n25This is called preplay in neuroscience (Pfeiffer and Foster, 2013; Wikenheiser and Redish, 2015). Some forms of replay or preplay\nseem to be happening much faster than real-time, which would make them particularly useful computationally (Karlsson and Frank,\n2009).\n26(Buckner, 2010; Gruber et al., 2016; Kurth-Nelson et al., 2016; Momennejad et al., 2018)\n27(Andrews-Hanna, 2012)\n28(Lu et al., 2012)\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n118\npus is also seen in the fact that people with damage in the hippocampus have difficulties in imagining new\nexperiences.29\nSome scientists are reluctant to make such parallels between hippocampal replay and wandering thoughts,\nsince they would seem to imply that rats “think” or “imagine” like humans, at least in the sense that rats engage\nin planning by imagining different sequences of actions and choose the best one.30 Likewise, we immediately\nrun into the question of whether such replay in an AI means that we would have to admit that an AI can think.\n“Thinking” is not a well-defined concept in either neuroscience or AI, which makes this question difficult to\nanswer.31\nCreative thinking and generative AI\nSo far, the discussion has considered wandering thoughts as rather mechanistic solutions to some well-defined\ncomputational problems. This does not do justice to the variety of wandering thoughts in humans. Sponta-\nneous thinking can be tremendously creative; in fact, it is one of the critical aspects of human creativity.\nNow, what is creativity? As a first approach, we might actually think of planning as a creative activity. You\nhave the current state, a goal, and you have to somehow create a path between the two. In fact, many different\nkinds of problem-solving could be seen as special cases of such planning: even proving a mathematical the-\norem can be formalized as planning a “route” from the premises to the conclusion of the theorem. However,\nsome would argue that this is just running an algorithm, so it cannot be called creative. I wonder why running\nan algorithm could not be called creative. What else does an intelligent system do anyway? On a sufficiently\nhigh level of abstraction, is not all our thinking a product of various kinds of algorithms? I shall not attempt to\nanswer the deep question of what creativity really is; I will just note that creativity is not easy to define, similar\nto the concept of intelligence.\nIn practice, a randomized algorithm can be quite a convincing example of creativity. Such algorithms con-\ntain certain randomness in their computation, which makes the algorithm try out completely new paths or\nideas. Modern generative AI, whether generating images or text, is based on nothing else than such random-\nized algorithms. A model of, say, images is combined with random noise to generate an incredibly realistic\nimage. Importantly, it would have been impossible for the human user who gave the initial prompt to predict\nall the details of the image, if only due to the randomness programmed in the system.\nIn the case of wandering thoughts, Monte Carlo Tree Search is an example of a randomized algorithm. It\nis not just deterministically finding a single solution to a given problem, but rather creatively imagining, as\nit were, a number of possible things to do, or steps towards a solution to the problem. Some randomness in\nbehavior is essential in exploring new environments, as we saw in Chapter 6; while in Chapter 8, we saw how\nrandomized algorithms have been very successful in game-playing AI. Thus, randomized search algorithms\noffer a plausible model for some of the wandering thoughts. From this viewpoint, it is natural that the compu-\ntations performed by wandering thoughts can also result in creative problem-solving.32\nIn fact, there are also some wandering thoughts that cannot be plausibly considered as replay or planning.\n29(Hassabis et al., 2007)\n30See (Redshaw and Bulley, 2018; Corballis, 2019) for discussions on whether non-human animals might possess such capabilities.\n31An important point is that, sometimes, thinking is defined to be conscious, and humans do a lot of the simulation consciously,\nwhile the AI is probably not conscious when planning. I think it is important to consider the phenomena of replay, planning, and\nimagining future actions as a topic separate from consciousness, to which we will return in Chapter 14.\n32(Fox and Beaty, 2019)\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n119\nPerhaps, while lying idly on your sofa, you have a series of seemingly unrelated mental images, or a superhero\nfantasy that could never actually happen in reality. One function of such wandering thoughts may be to create\ncompletely new ideas and associations, even new goals. In this case, wandering thoughts can be compared\nwith the outputs of generative AI systems, which, in the absence of a restrictive prompt, create text or images by\nquite randomly sampling from a language model or an image model. This leads to a well-known problem with\ngenerative AI: a randomized generation of content will sometimes produce complete nonsense. To circumvent\nthis, a “generate and test” approach can be used: first, new items are more or less randomly generated by one\npart of the system, and then another part of the system tests whether they make any sense. Unrealistic, weird,\nand unstructured wandering thoughts could be the result of such random generation; hopefully, our more\nrational part then tests them and decides which ones make any sense and should be taken seriously.33\nWandering thoughts multiply suffering\nSo far, we have seen that while mind wandering may be detrimental for whatever you’re trying to do at the\npresent moment, it helps in planning and learning, perhaps even allowing some creativity. From a purely\ninformation-processing viewpoint, it is probably a useful thing since similar ideas are currently used in AI\nsystems, and after all, evolution would not have “programmed” us to have a wandering mind if it were not\nuseful to us from the evolutionary viewpoint.\nYet, evolution does not try to make us happy. A problem with replaying past memories and planning the\nfuture in human brains is that we are, on some level, unable to understand they are not real. If you remember\nan embarrassing episode from the past, you actually feel embarrassed. If you think about something scary that\nmight happen to you tomorrow, you actually start feeling scared. That is, wandering thoughts increase human\nsuffering by making us suffer from simulated or replayed events, in addition to the real ones.\nAny suffering produced by real-life events may, in fact, be repeated many times by the replay of those events.\nMaking a mistake might lead to nothing but a fleeting frustration if we didn’t replay it afterwards, thus gener-\nating many instances of regret. Likewise, if something unpleasant is expected to happen, the unpleasantness,\nthe threat, is felt many times in planning how to avoid that thing—which may actually turn out not to happen\nat all. Planning future events includes frustration when things in the fantasy don’t go as you would like them to,\nand you can be frustrated many times by the planning of a single event. Due to this multiplication of suffering\nby wandering thoughts, it could be argued that the vast majority of our suffering actually comes from remem-\nbering or anticipating unpleasant events. The anticipation is closely related to discussions on threats and fear\nin the preceding chapters, but the aspect of replaying unpleasant memories is new.\nImportantly from the viewpoint of suffering, you have little control regarding such wandering thoughts.\nYou may think that you must have decided to recall an unpleasant conversation, but in fact, the recollection\nand replay just started without you deciding anything, and even if you try to think about something else, you\nmay find yourself unable to do so. This is another clear connection to the emotional interrupts: both wandering\nthoughts and emotional interrupts are largely beyond conscious control. You cannot switch off those systems.\nIn fact, it is even worse: both systems actually take control of the agent.\n33For surveys on the theory of “computational creativity”, i.e. trying to make computers creative, see Colton et al. (2009); Toivonen\nand Gross (2015). How such theory is related to state-of-the-art generative AI is discussed by Franceschelli and Musolesi (2023); related\nneuroscience research is reviewed by Jung et al. (2013).\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n120\nSome research actually claims that the wandering mind is generally unhappy. That sounds plausible if\nwandering thoughts multiply suffering, as I just argued. However, it might be a bit of an overgeneralization.34\nWhether wandering thoughts make you unhappy probably depends on their contents. It might seem obvi-\nous that having wandering thoughts with negative feelings, such as worrying, makes you unhappier, while\npositive content has the opposite effect. In what is a rather extreme case, a study found that women having\nwandering thoughts about their significant others actually felt happier.35 Close to the negative extreme, we\nfind rumination, which is thinking about negative events that typically happened in the past and are related\nto one’s personal concerns.36 It is particularly frequent in depression and, unsurprisingly, leads to low mood.\nFor individuals with depressive tendencies, most wandering thoughts may consist of depressive rumination,\nand eventually may lead to relapse and full-blown depressive episodes. Even for normal individuals, wander-\ning thoughts provide an opportunity for rumination to arise, and thus may lead, on the average, to negative\nmood.37 In spite of some reservations, therefore, I think an important point is made in claiming that a wan-\ndering mind is an unhappy mind; we will get back to this important point when talking about meditation in\nChapter 17.\nWhy do wandering thoughts trigger feelings?\nReplaying negative experiences, or planning the future, might not have anything to do with suffering if they did\nnot somehow feel unpleasant, i.e. if they did not activate the negative valence signalling. A person may have\nreoccurring wandering thoughts about going to the dentist and vaguely feel the pain that the dentist’s tools\nwill cause in her mouth. Isn’t it odd that she feels the pain although she is not at the dentist at all? While you\nprobably have to go to the dentist one day, people also worry about the possibility of various disasters that are\nnot at all likely to happen to them. Let me repeat Montaigne’s comment: “One who fears suffering is already\nsuffering from what he fears”.\nThoughts rarely correspond to something that is actually happening here and now, as opposed to percep-\ntions. Almost by definition, our thinking is usually about past events which are no longer there, or future events\nwhich have not yet happened, and may not happen at all. Why do we then feel upset about them, or, from a\n34See Killingsworth and Gilbert (2010) for the claim that “A wandering mind is an unhappy mind”. The problem is, however, that such\nstudies don’t conclusively show that it is mind-wandering that makes people unhappy. It is also possible that the causal effect is the\nopposite: when we are unhappy, thoughts start wandering more (Smallwood et al., 2009). This could be because negative mood is re-\nlated to unresolved goals or personal problems, which are then processed during mind-wandering. If you’re sad, it may be because you\nare experiencing problems, and those problems need extra processing by mind wandering (Poerio et al., 2013), in line with footnote 21\nabove.\n35(Poerio et al., 2015). Intriguingly, the effect of wandering thoughts on mood may depend on whether you think about the past or the\nfuture. Ruby et al. (2013) found that future-oriented thinking has a general positive effect on mood, even if the contents were negative;\nperhaps this is so because when we solve a planning problem, we get happier. In contrast, thinking about past events was found to\nlead to negative mood independently of the contents of the thoughts. However, Poerio et al. (2013) argue against such results, and in\nparticular point out that future-oriented thinking may increase anxiety based on worrying about what might happen in the future; see\nalso Stawarczyk et al. (2013).\n36(Whitmer and Gotlib, 2013). Perhaps the most extreme negative example would be flashbacks about a traumatizing event in post-\ntraumatic stress disorder (Yehuda, 2002).\n37(Teasdale et al., 2000; Marchetti et al., 2014, 2016; Ottaviani et al., 2013; Van Vugt et al., 2018). Another point to note here is that\nwhen talking about thoughts inducing a positive or negative mood, we need a baseline. In the research cited, this is typically a rather\nnormal, average mood. However, if the baseline had absolutely no wandering thoughts, as might be considered ideal in some particular\nmeditation traditions, it could be that even positively valenced wandering thoughts actually have a negative effect on the mood.\n\nCHAPTER 11. THOUGHTS WANDERING BY DEFAULT\n121\nmore computational viewpoint, why do they activate negative valence signals? Indeed—this is a deep question\nthat we encounter rseveral times in this book—why do we feel the emotions associated with memories and\nimagination?\nFrom the viewpoint of computational design, it is clear that the system that computes state-values and\npredicts rewards has to be active in wandering thoughts, at least to some extent, so that the brain can take its\nevaluations into account when planning and learning. What does not seem necessary is that we actually, on a\nvisceral level, feel pleasant or unpleasant about the events produced by planned actions. Why do our bodies\nreact to our fantasies as if they were true? I suggest this is a kind of a computational shortcut. If you want\nto make learning from the simulation as simple as possible, it makes sense to use the same mechanisms and\nnetworks as in the case of real data. This is possible if the AI or the brain is fed the same kinds of inputs signals\ninto the same networks regardless of whether the action is real or simulated.\nUltimately, combined with the hypothesis that the error signals are best broadcast to the whole brain using\nthe pain system (Chapter 2), such computational simplification seems to have led to a situation where in the\nbrain, it is not possible to give an “unpleasant” signal to the planning system without activating the main sys-\ntem that signals suffering to the whole system. In other words, perhaps humans feel suffering during negative\nwandering thoughts simply because it makes the design of the learning system easier.38\nHere we see a particularly striking conflict between evolutionary goals and happiness. Suffering from the\nsimulation of negative events may be a computational shortcut, which is not really that necessary. It is just that\nthe brain was “designed” by evolutionary forces which do not care if the system design makes you suffer many\ntimes more; they happened to find this design useful for their own evolutionary purposes.\n38Incidentally, such processing is also part of the somatic marker hypothesis; see footnote 26 in Chapter 10.\n\nChapter 12\nPerception as construction of the world\nWithout any perceptual abilities, an agent can hardly do anything intelligent in the real world. Neural networks\ngive a rudimentary system for perception: for an input image, they can try to tell what it depicts. However,\nit turns out that perception is an extremely difficult problem. In this chapter, I explain the main difficulties\ninvolved in perception, and how they can be solved by modern AI but only to some very limited extent. I argue\nthat the very problem of perception is so difficult that even our brains do not solve it very well. Here, I consider\nin detail visual perception, but the theory largely holds for other kinds of perception.\nWhat is crucial for the main theme of this book is to understand the relevant implications of the extreme\ndifficulty of perception. The incoming sensory data is incomplete, and we fill in the gaps by using various\nassumptions, or prior information, about the world. This implies that our perceptions are quite uncertain,\nor unreliable, and much more so than we tend to realize. One aspect of such uncertainty is subjectivity: we\nfill in the gaps using our own assumptions, and my assumptions may be different from yours. Perception is\nessentially a construction, a result of unreliable and somewhat arbitrary computations; it is not an objective\nand perfect recovery of some underlying truth.\nThese fundamental problems in perception feed into the difficulty of making correct inferences about the\nworld: they make any categorization uncertain, they reduce the possibility of predicting the world, and conse-\nquently reduce any control the agent has. This increases various errors such as reward prediction errors, and\nthus suffering. More specifically, the computation of reward loss is dependent on the prediction of reward as\nwell as the perception of obtained reward, which are both subject to the limitations of perception, and thus can\ngo wrong.\nVision only seems to be effortless and certain\nIt may be surprising to many people how difficult computer vision actually is, and what an incredible feat the\nvisual system of our brain is accomplishing, literally, every second. It all seems to happen so effortlessly and\nautomatically. However, our capacity for vision is effortless only in the sense that it does not require much\nconscious effort, and it is automatic only in the sense that it does not usually need any conscious decisions or\nthinking. You turn your gaze towards a cat, and immediately, without any conscious effort, you recognize it\nas a cat. This is a typical, even extreme case of dual systems processing: most of the computations happen in\nneural networks, not at the level of symbolic, conscious thinking. Since we have little access to the processing\nin the neural networks, we cannot understand how complicated their computations are.\n122\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n123\nIn the early days of AI in the 1970s, computer scientists thought programming such “computer vision” must\nbe easy. However, anybody either studying the human visual system or trying to build a computer vision system\nis quickly convinced of the near-miraculous complexity of the information processing that is needed for vision\nand performed by our brain almost all the time. Knowing that history, it is not surprising that while computers\ncan beat humans in chess or arithmetics, they are nowhere near human performance in visual processing.1\nToo much data\nA major difficulty in vision is the huge amount of data received by the system. The immensity of the data is\nperhaps obvious to anybody who has waited for video data to download over a mediocre internet connection.\nIn fact, the vast majority of internet traffic takes the form of video data. Text data is completely negligible in\nterms of file size: a large book is hardly equal to a second of video data.\nLikewise, humans and other mammals receive a huge, continuous stream of data from the environment\nthrough their eyes. The human retina contains something like one hundred million photoreceptors, which are\ncells that convert incoming light into neural signals. The manner in which the data is stored and transmitted\nmay be very different from computers, but still the fundamental problem of receiving an immense amount\nof data is there, as well as the requirement of a huge amount of information-processing capacity. In fact, the\nvisual areas constitute something like half of the human cerebral cortex—the part of the brain where most\nsophisticated processing takes place.2\nYet, information is missing\nHaving such huge amounts of data is both a blessing and a curse. A curse obviously in the sense that you need\nimmense computing power to handle such a data deluge; a blessing in the sense that such huge amounts of\ndata may contain a lot of useful information. Yet, paradoxically, the information contained in the input to a\ncamera or the retina is almost always lacking in various important ways.\nOne of the most fundamental problems in vision is that what each eye gives us is a two-dimensional pro-\njection of the world, just like an ordinary photograph. A photograph is nothing like a 3D hologram: most of the\ninformation on the 3D structure of the objects is missing. (Having two eyes gives some hints of the 3D structure,\ni.e. which objects are close to you and which are far-away, but this only slightly remedies the problem.)\nSuppose you see a black cat. Now, the actual 2D projection will be very different when you can see the cat\nfrom different viewpoints: from the front, from one side, from the other side, from above, and so on. That is,\nthe pixels which are black are not at all the same in the different cases; the pixel values that would be input\nto a neural network will vary widely when the cat is seen from different viewpoints. Thus, the neural network\nwill have to somehow understand that very different pixel values correspond to the same object. To illustrate\nthis problem of 3D to 2D conversion, consider what even a simple cube can look like in different projections.\nSome possibilities are shown in Figure 12.1. Its 2D projection can look like a rectangle (possibly a square), like\na diamond, and many other things.\n1It is true that in some specific, well-defined tasks, such as recognizing animals in photographs, AI can actually outperform humans.\nHowever, such performance is usually specific to a certain kind of input data and task, and it is still far away from the versatility of\nhuman vision; see e.g. Recht et al. (2019); Peters and Kriegeskorte (2021).\n2(Nakayama, 1999)\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n124\nFigure 12.1: An illustration of the inverse problem that makes vision particularly difficult. The four figures are\nall 2D projections of the same 3D cube. Any camera or a single eye can only capture one such 2D projection,\nwhich loses a lot of information and creates ambiguity.\nAnd this is just one part of the problem. More fundamentally, the problem is that any object can undergo\nmany different kinds of transformations. Consider a cat again: it can take many different shapes by moving its\nlimbs; sometimes its legs are wide apart, sometimes close to each other. Sometimes it stretches its whole body,\nsometimes it puffs up. If you think about the 2D image created, it will again be quite different in these different\ncases. As another example, the lighting conditions can be very different. Imagine that light comes from above,\nor from behind the cat: Again the cat looks very different, and even more so in a 2D projection. Your brain has\nto somehow figure out all these extra parameters based on the sensory input.\nThose were some of the problems in recognizing a single cat. To make things even more complicated, dif-\nferent cats look very different. Some are black, some are white, so the pixel values are even more fundamentally\ndifferent. Yet, you somehow are able to see that they are all cats.\nSuch ambiguity or incompleteness of visual information in a camera or the retina is the reason why vision is\ncalled an inverse problem.3 As a very simple illustration of an inverse problem, consider there are two numbers\nwhich we denote by the variables x and y. You want to know both these numbers, but the trick is you only are\ngiven their sum, x + y. How could you possibly find out both of those original numbers—how can you “invert”\nthe equation? Suppose you are told the sum of two numbers is equal to 10. There are many possibilities what\nthe actual x and y may be like, for example, x = 5 and y = 5, or x = 7 and y = 3 etc. Vision is a lot like this.\nWhat you observe are the pixel values in, say, a photograph. But there are a lot of factors that determine what\nthe pixel values are like: the identity of the object in the photograph, the location of the object, the lighting\nconditions, the background, to name just a few. It is next to impossible to figure out what there is in the image\nwithout some tricks.\nActually, the fact that sensory information is incomplete is in some sense quite blatant. Just think about the\nfact that you cannot see through solid surfaces. Suppose you look at a wall in front of you: you cannot see what\nis on the other side. Your perception is limited by the physics of light, which does not penetrate the wall, and\nthus you only obtain limited data and limited information about the environment. That may be an extreme\nexample, but the point is that all perception is similarly constrained; it is just a matter of degree. Curiously,\nin your mind, you do have some idea of what there is behind the wall (another room, the street, or something\nelse), but this idea is vague and uncertain. We will see in this chapter why all perception is, to some extent, a\nsimilar kind of guesswork.\n3Strictly speaking, what we consider here is an ill-posed inverse problem; however, ill-posedness is often implicitly assumed when\ntalking about inverse problems.\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n125\n+\n=\nOcclusion interpretation\n+\n=\nPacman interpretation\nFigure 12.2: Suppose you see the figure on the far left, consisting of a square and a part of a disk. On the\nleft: a typical interpretation where it is assumed that the disk is complete but occluded. On the right: another\nlogically possible interpretation, with a pacman “eating” the square, but one that our visual system would not\nmake because it is less likely. Our visual system chooses the interpretation which is more likely, given its prior\ninformation about the environment.\nPerception as unconscious inference\nYet, AI has recently been making major progress in vision. One reason is that computers have been getting\nmuch faster every year, but that is of course not enough in itself if you don’t know how to program your com-\nputer. The crucial breakthrough in recent computer vision has been the application of neural networks. Neural\nnetworks offer two important advances. First, they enable the processing of vast amounts of data to be dis-\ntributed into a large number of processors, which work in parallel and thus can process the data more easily.\nThe advantages of such distributed and parallel processing are considered in more detail in Chapter 13. In this\nchapter, we focus on another advantage, which is that we know how to make neural networks learn from big\ndata sets. Learning can alleviate, and to some extent solve, the problem of incomplete information, such as\nseeing only a 2D projection of the 3D world.\nThe trick used by our brain is to learn what the world typically looks like, and to use the learned regularities\nto complement the incoming data. Look at the figure on the far left-hand side of Figure 12.2. Here, we tend to\nperceive a disk and a square. This is because we immediately assume that the disk actually continues behind\nthe square, it is just partly occluded (i.e. blocked from view) by the rectangle. In fact, we tend to almost see a\nwhole disk. There’s nothing wrong with such an assumption, but it does not necessarily follow from the figure.\nAlternative interpretations are possible based on this incomplete data. For example, it could be that the figure\nactually consists of a square and a “pacman”, as illustrated on the right-hand side of the figure.\nPerceptions such as in this example are usually explained as results of unconscious inference using prior\ninformation. The visual system has learned certain regularities in the outside world—this is called prior in-\nformation. For example, contours are typically continuous and smooth; lines are typically long and straight;\nobjects can be behind or in front of each other. So, in Figure 12.2, the brain computes that it is very likely\nthat the incomplete disk is actually part of a whole disk, but we just don’t get visual input on the whole disk\nbecause it is blocked by the square. Such a conclusion is made by neural networks which are outside of our\nconsciousness, thus the process is called unconscious inference.4\n4Inference means the computational process leading to a conclusion or a decision.\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n126\nThe inference in question is also probabilistic: The visual system cannot know for sure whether the edge\nof the disk continues behind the square, but it is more likely that it does than that it doesn’t. That is, the brain\ncannot make any judgements that are logically necessary and certain about this picture. The only thing the\nbrain can do is to calculate probabilities and choose the most probable interpretation.5 That is why perception\nis necessarily uncertain to some extent.\nBayesian inference\nThe probabilistic inference needed in perception takes a particular form where the goal is to determine causes\nwhen observing the effects. The mathematical theory behind such inference was initially proposed by Thomas\nBayes in the 18th century, which is why such inference is often called Bayesian.6 In the case of perception, the\n“effects” are the patterns of light coming into your eyes, while the “causes” are the objects and events in the\noutside world.\nTypical scientific models based on physics will tell you what the effects are for given causes. For example,\ngiven an object and its location in your field of vision, you can rather easily compute, by basic physics, what the\nlight coming from that object to your eyes will be like. But doing the computation backwards is more difficult.\nGiven that your eye receives certain light patterns, as registered by your sensory organs, how can you know\nwhat went on in the outside world? You have to somehow invert your physical model of the world, leading to\nthe inverse problems just mentioned. Such problems can be approached by Bayesian inference, especially in\nthe case where we can only calculate probabilities, which is exactly the case here.\nBayesian inference tells that the probability for a given cause (given we observe certain effects) is propor-\ntional to the product of two things: First, the probability that such a cause creates the observed effects, and\nsecond, how likely the cause is to occur in general. The first part here is rather obvious: A given cause is more\nlikely to be responsible for what your sensory organs report if the cause and such sensory input are compatible:\nthat cause is likely to produce the observed effects. However, the important point here is in the second part: A\ngiven cause is even more probable if its general probability of occurrence is large. That is, if the cause has high\n“prior probability” in the terminology of Bayesian inference.7\n5A fundamental question is whether the brain chooses one interpretation or whether it can entertain several interpretations simul-\ntaneously. Something in between these two seems to be happening in the special case of bistable perception, which means that when\na stimulus can very well be interpreted in two different ways, the two interpretations seems to be alternating in the brain, so that con-\nscious perception switches from one interpretation to another every few seconds or so (Sterzer et al., 2009). The proportion of time\nallocated to each interpretation may, in fact, reflect its probability that the brain computes by Bayesian inference discussed next in the\ntext (Moreno-Bote et al., 2011).\n6For neuroscience-oriented introductions, see (Kersten et al., 2004; Ma et al., 2022). While Thomas Bayes is usually credited with\nthe general mathematical theory used in this context, the specific idea of perception as unconscious inference was actually formulated\nlater by Hermann von Helmholtz, which is why some authors call this framework the Helmholtzian theory of perception. (Also, the\ncredit for the mathematical theory should perhaps largely go to Pierre-Simon Laplace.) The recent proposal of a “free-energy” brain\ntheory (Friston, 2010) is essentially a reformulation of these ideas, with some additional hypotheses extending it to action selection.\n7To get into more mathematical detail, Bayesian inference wants to compute the probability P(cause given effect), where P denotes\nprobability. More precisely, this is a conditional probability, i.e. the probability of one thing (cause) given that another thing (effect) has\nbeen observed. This is the typical case of inference: we observe the effects and want to find the causes, or at least their probabilities.\nThe celebrated Bayes formula then says the aforementioned probability is equal to P(effect given cause) × P(cause)/P(effect). Here,\nthe term P(effect given cause) can be computed from a physical model of the world implemented in your brain. P(cause) is the prior\nprobability of a given cause; this is where the prior information about what typically happens in the world comes in. P(effect) is not so\nimportant because we are not comparing different effects, so it is constant, and it can actually be computed from the other probabilities\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n127\nConsider the following example. Through your living room window, you get a glimpse of something green\nmoving on the street. It could have been a green car, or it could have been a Martian (they are all green, as is\nwell-known). Both of these two causes (car or Martian) would produce the same kind of quick flash of some-\nthing green moving on the street, or more precisely, some green light briefly entering your eyes. So, the prob-\nability of the effect (green light stimulating your retina) is high for both two causes; let’s say for the sake of\nargument that it is equally high in both cases. However, you will not think it is a Martian. The reason is that\nyour brain uses Bayesian inference and looks at the prior probabilities. The prior probability of a Martian is\nvery much lower than the prior probability of a green car; the brain knows that in general, it is very rare to\nencounter any Martians. Thus, when weighing the probabilities of the different causes, the green car wins\nby a wide margin. This inference is possible because the brain has a model of what the world is typically, or\nprobably, like: Martians are quite rarely encountered, at least on planet Earth.\nPrior information can be learned\nPrior information, i.e., a model of what the world is typically like, is central in such unconscious inference, so\nwhere does it come from? The crucial principle in modern AI and neuroscience is that the prior information\ncan be obtained by learning from data; learning is thus the basis of perception. Now that may seem like a\nweird claim from a biological viewpoint. How could perception possibly be based on learning, given that many\nanimals see quite well more or less immediately after birth? With human infants, developing proper vision ac-\ntually takes several months but that is beside the point. The point here is to understand the different meanings\nof the word “learning”.\nWhen I talk about learning here, I mean learning in a very abstract sense where a system adapts its behavior\nand computations to the environment in which it operates, and in particular to the input it receives. In human\nperception, such adaptation happens on different levels and time scales: there is both the evolutionary adap-\ntation and the development of the individual (after birth). These two time scales are very different, but if we\nare interested in the final result of learning, we can just lump the two kinds of adaptation together. Likewise,\nthe optimization procedures are very different: evolution is based on natural selection while individual de-\nvelopment presumably uses something like Hebbian learning—although we don’t understand the details yet.\nAgain, if we just look at the end result of the combination of those processes, we can ignore the difference of\noptimization procedures as well, and simply call this whole process “learning”. This resolves the paradox of\nanimals being able to perceive things instantly after birth. Their sensory processing is using all the results of\nthe evolutionary part of learning, and thus even before having received much input as individuals, their neural\nnetworks are capable of some rudimentary processing.8\nby a simple formula.\n8There is actually something in between those two kinds (evolutionary and developmental) of biological learning, which is learning\nin the womb. At the late stages of the pregnancy, the visual system of the foetus is “learning”. While its eyes are closed, and they don’t\nreceive much input, certain dynamic patterns called “travelling waves” are generated in the eye, on the retina. These patterns are then\nfed to the visual cortex of the brain, enabling some basic learning of visual regularities, complementing the information in the genes\n(Wong, 1999).\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n128\nNeural networks weights contain the prior information\nWe already saw in Chapter 4 how it is possible to train a neural network from big data sets. The weights in the\nnetwork are learned based on minimization of some error function. In the simplest case, the learning algorithm\nknows what there is in each image used for training (a cat or a dog) which provides a label or a category, and\nthen we can use supervised learning. If we want to understand biological vision, though, unsupervised learning\nis preferred. This is because the visual system does not really have anybody constantly giving labels to each\ninput image, which makes supervised learning unrealistic as a theoretical framework.\nFortunately, Hebbian learning and other methods of unsupervised learning can learn to analyze images in\ninteresting ways, without any supervision. Intuitively, if the input to neuron A and the input to neuron B are\noften rather similar, it is likely that they are somehow signalling the same thing, and thus they should be pro-\ncessed together, for example by computing their average or difference.9 The results of such learning are stored\nin the synaptic weights of the neural network. From the viewpoint of Bayesian perception, we can thus say that\nthe prior information is learned and stored in the form of the weights connecting the neurons. Such neural net-\nworks embodying prior information also form the basis of generative AI systems that generate realistic images.\nThey have been trained by millions, if not billions, of photographs in an unsupervised manner.10 Recognizing\nwhat is in an image on the one hand, and generating new images on the other hand, require closely related\nneural networks and image models.11\nWe can investigate what kind of prior information has been learned by such neural networks by looking\nat the weights of the networks. Considering the initial analysis of images done by a neural network with just\none layer, different learning rules almost invariably give the same result: the most basic visual regularities are\nsomething like short edges or bars. Figure 12.3 shows some examples. Interestingly, such AI learning leads\nto processing which is very similar to the part of the brain that does some of the earliest analysis of incoming\nimages, called the primary visual cortex. Measurements of many cells in that area reveal that they compute\nfeatures which look very much like those in Fig 12.3. Edges and bars are clearly very fundamental elements of\nthe structure of images.12\nSuch edges and bars can be seen as the first stage of the successive “pattern-matching” on which neural\n9In particular, Hebbian learning can implement feature extraction methods such as principal component analysis (Oja, 1982) and\nindependent component analysis (Hyvärinen et al., 2001).\n10(Yang et al., 2023; Croitoru et al., 2023)\n11For a discussion of the connection of the two cases, see Xie et al. (2016); Grathwohl et al. (2019).\n12The figure and the discussion are based on (Olshausen and Field, 1996; Van Hateren and van der Schaaf, 1998). The learning\nprinciple used here can be intuitively understood from two different viewpoints. One is independence of the features: the outputs\nof the neural network (which in this case has a single layer) should be as independent as possible in the sense of probability theory.\nIn other words, knowing one feature should give minimal information about the other features. The other viewpoint is sparsity: the\nfeatures should be silent (zero) most of the time and only rarely turned “on”. An important benefit of such sparse coding is that it\nminimizes energy consumption if representing a feature that is zero consumes little energy. Therefore, the learning principle used is\ncalled either independent component analysis or sparse coding, which, surprisingly, turn out to be almost equivalent. Such analysis\ncan be implemented as a particular kind of Hebbian learning. Actually, there is an even more fundamental regularity in visual input\nthan the one depicted here, which is that two near-by pixels tend to have similar gray-scale values (they are strongly correlated). That\nis, if a pixel is, say, white, the pixels next to it are quite likely to be white as well—and the same applies for any color. Such similarities\nare analyzed by neurons (“ganglion cells”) in the retina. However, this regularity is so elementary that it is in some sense included in,\nor implied by, the regularity described by the edges. Mathematically speaking, the covariances of pixel gray-scale values are perfectly\nmodelled by independent component analysis and no additional model is needed. For a general introduction to the models used here,\nsee Hyvärinen et al. (2009).\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n129\nFigure 12.3: Simple image features learned by a neural network. Each small patch gives the synaptic weights\nin a neuron whose input consists of small image patches. The weights can thus be plotted as gray-scale values\narranged as image patches. More precisely, these are the results of applying a method of unsupervised learning\ncalled independent component analysis on small image patches.\nnetwork computation is based. We can actually go further and train a feedforward neural network with many\nlayers to analyze images.13 After successful training, a multi-layer neural network can contain extremely rich\nprior information about images. In general, the multi-layer network will be computing increasingly complex\nfeatures in each layer.14 The features computed by the units in higher layers are no longer simple edges or\nbars: they are more like some specific parts of the objects that the network was trained on. They are also more\nfocused on coding the identity of those parts while ignoring less relevant details such as where in the image the\nparts are located. For example, a neuron in a high layer could respond to a cat head, irrespective of where it is\nin the input image, and further ignore details such as the exact shape of the face of the cat. In this sense, such\nneurons are quite similar to cells in the inferotemporal cortex, an area in the brain that performs a very high\nlevel of image analysis.15\n13The theory of unsupervised learning is much less developed and more complicated than the theory of supervised learning, espe-\ncially for multi-layer networks. Therefore, a lot of work on such feature learning uses supervised learning, somehow obtaining labels or\ncategories for each image, and using ordinary supervised learning where the network learns the connection between the images and\ntheir categories. The bottleneck here is getting sufficient amounts of such data with category labels. It is difficult because somebody\nhas to tell what the photographs are depicting; if the labels are given by humans, that is a lot of work (although a simple approxi-\nmation would be to extract the labels from captions, which are sometimes attached to images on the internet). Current research is\nstrongly focused on finding methods to train multi-layer neural networks without labels, that is, in an unsupervised way. A particu-\nlarly promising approach is called self-supervised, which means performing unsupervised learning by reformulating the problem as\nsupervised learning. Basically, you create hypothetical outputs, or a hypothetical classification problem, and use them to train your\nordinary supervised, input-output neural network. The possibilities are unlimited: you could define the input to a neural network to\nbe a degraded version of your data and the output your real data, where the degraded version could be obtained by adding noise, or\nmaking a color image black-and-white (Vincent et al., 2008; Larsson et al., 2017). Or, the “degraded” data could actually be artificially\ngenerated: then you train the neural network to distinguish between the real and the artificial data (Gutmann and Hyvärinen, 2012).\nFor example, in video data, you could randomly shuffle the time frames in a video, or scramble audio in a video with sound, and train\nthe neural network to classify such scrambled data vs. the original data (Hyvärinen and Morioka, 2017; Misra et al., 2016; Arandjelovic\nand Zisserman, 2017). In each case, the neural network has to learn something about the structure of the data in order to perform\nthis mapping, that is, trying to reconstruct the original images from degraded ones, or telling which data is real and which is noise.\nThe multi-layer processing thus learned are reasonably similar to what is computed in the brain (Zhuang et al., 2021). However, it\nshould be noted that self-supervised learning in itself gives only features; it does not give a proper Bayesian prior model except in some\nspecial cases, such as the “noise-contrastive estimation” by Gutmann and Hyvärinen (2012), and nonlinear versions of independent\ncomponent analysis (Hyvärinen and Morioka, 2016; Khemakhem et al., 2020).\n14(Güçlü and van Gerven, 2015; Kriegeskorte, 2015; Eickenberg et al., 2017; Zhuang et al., 2021)\n15(Tanaka, 1996; Brincat and Connor, 2004). The inferotemporal cortex is usually investigated in the macaque monkey, not humans,\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n130\nIllusory contours\nFigure 12.4: The Kanizsa triangle, an example of a visual illusion. There is actually no triangle in the figure, just\npacmans.\nIllusions as inference that goes wrong\nWe have now seen that the incompleteness of the incoming sensory information can be, to some extent, alle-\nviated by Bayesian inference. However, this solution is far from perfect—whether we consider perception in\nhumans or sophisticated AI. Sometimes the perception is blatantly incorrect, as shown by the phenomenon of\nvisual (or “optical”) illusions. A dramatic example is shown in Figure 12.4. We tend to see a full triangle in the\nfigure, with three uninterrupted lines as its sides or edges. In reality, though, the sides of the triangle do not\nexist in the figure. If you cover the “pacmans” with your fingers, you see that there is nothing but white space\nbetween them. Yet, most people have a vivid perception of three lines between the pacmans which create a\ncomplete triangle.\nThis is called an illusion in neuroscience since the sides do not physically exist in the figure; they are simply\nimagined by our visual apparatus. Just like the imagination of a full disk in Figure 12.2 we saw earlier, this can\nbe considered unconscious inference, where your visual system computes the most likely interpretation. The\ndifference is that here, the interpretation is in clear contradiction with the actual stimulus, or physical reality.\nWhile inferring a full disk in Figure 12.2 seemed smart and would quite probably have been correct in real life,\ninferring that there is a full triangle in Figure 12.4 may seem quite stupid, at least after you have checked that\nthe sides do not really exist. The curious fact is that you cannot really help seeing the triangle in Figure 12.4.\nThe theories explained in earlier chapters help us further understand why such illusions occur. A neural\nnetwork is trained to accomplish a well-defined task, such as recognizing different objects in photographs.\nHowever, such neural networks are inflexible and only able to solve the problem they are trained for; neural\nnetworks are not general problem-solving machines. In particular, a neural network will not work very well\nwhen the input comes from a different source than what it was trained for. Arguably, the Kanizsa triangle is\nsomething artificial, and different from what you would usually see in real life (where pacmans are quite rare),\nbut similar mechanisms are likely to operate in the human brain. There have also been claims that high-level visual neurons in the\nhuman brain could be coding for the identities of single individuals (Quiroga et al., 2005). However, a more detailed analysis of the\nresults shows that this is an exaggerated interpretation: single neurons are probably responding to several different people (Quiroga\net al., 2008). The property of ignoring less meaningful details is called invariance (DiCarlo and Cox, 2007).\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n131\nso we should not expect the brain’s neural networks to process it appropriately. This is another way of saying\nthat the brain’s prior information contains assumptions that are typically true in the context where you usually\nlive, but they are just about probabilities, and might sometimes turn out to be quite wrong.\nAt the same time, the dual-system theory explains why it does not help if somebody explains to you that this\nis an illusion, or even if you realize that yourself. A logical, symbol-level understanding that there is no triangle\nhas little effect on the other system, i.e. the neural networks, which are mainly in charge of visual perception.\nAttention as input selection\nIn real life, any sophisticated perceptual system further faces the problem that there is simply too much infor-\nmation in the visual field, as already mentioned. This problem is very different from the missing information\nproblem, which is partly solved by using prior information. In particular, there are often too many things in\nthe visual input at the same time. There may be many faces, people, buildings, animals, or cars, at the same\ntime, and it is too difficult to process all of them. This is in stark contrast to current success stories of object\nrecognition by AI, which are usually obtained in a setting where each input image contains only one object, or\nat least one object is much more prominent than the others.\nSuppose you input an image of a busy street to such a neural network trained to recognize a single object in\nan image. Since the input now contains many objects, features of different kinds will be activated in the neural\nnetwork, some related to the perception of people, some related to the perception of buildings, some to cars,\nand so on. Many of the features are actually quite similar in different objects: think about two faces in a crowd,\nwhich are quite similar on the level of pixels and even rather sophisticated features. Various neurons will be\nactivated, but it is impossible to tell which were activated by which face. It will be very difficult for the AI to\nmake sense of such input and the activations of its feature detectors.\nThis problem really arises when the information processing as well as the input data sensors work in a\nparallel and distributed mode. Parallel and distributed processing, considered in detail in Chapter 13, usually\nmeans that there are many processors working simultaneously and independently. Here, the situation is even\nmore extreme since the input data itself is received from a huge number of sensors, such as pixels in a camera\nor cells in the retina. Yet, the principles of parallel and distributed processing are really the same, as the outputs\nof the sensors are further processed by a large number of small processors.\nTraditional computer science usually does not deal with this problem. If the input to the computer is mouse\nclicks by a human user, the input is quite manageable. Even if a computer handles a very large database,\nthe situation is different because it follows explicit instructions on what information to retrieve and in what\norder. Vision is more like thousands of disk drives simultaneously and forcefully feeding the contents of their\ndatabases to a single computer.\nThe key to how the brain solves this problem, especially in the case of vision, is the multi-faceted phe-\nnomenon of attention. In the most basic case, the visual system of many animals, including humans, selects\njust one part of the input for further processing. As we say in everyday English, the animal only “pays attention”\nto one object at a time, whether it is a face seen on the street, or some object it is trying to manipulate.\nThe simplest form of such selective attention is that you just wipe out everything else in the visual field,\nexcept for one object. In Figure 12.5, we see a photograph and an attentional selection template, which shows\nhow only the main object of interest in the figure is found and selected. The results of such computation can\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n132\nFigure 12.5: Attentional selection illustrated. The photo on the left is the original visual input. An attentional\nsystem selects the pixels to retain, shown in white in the figure on the right. (Based on data by Martin et al.\n(2001), used with permission.)\nbe used to simply blank out everything else except for the main object.16 Such a form of attentional selection\nis also called “segmentation”. Now, if you input an image that contains only this one object into the neural\nnetwork, the recognition will be much easier. Such selection seems to be happening in many different parts of\nthe brain and in many different ways. In a sense, it is a reflection of the ubiquity of parallel processing in the\nbrain, which necessitates various forms of input selection all over the brain. The most amazing kind of atten-\ntional selection that our brains can accomplish must be finding individual faces in a crowd. Face processing is\nevolutionarily extremely important, so there are specialized areas in the human brain for processing just faces\n(monkeys have them too).17\nPerforming such segmentation is not easy: using attentional mechanisms in AI and robots is an emerging\ntopic, and we still don’t know very well how to do it. However, like so many other functions related to intel-\nligence, it may be possible to learn it. Attention is fundamentally a form of action: even moving your eyes\ncan be seen as a form of attention, since it helps to select certain parts of your environment for visual pro-\ncessing. Thus, learning to attend may be possible by the general principles by which an agent can learn to act\nintelligently, as discussed in earlier chapters.18\nThe downside of selective attention is that it leads to a bottleneck in the processing. In the example just\ngiven, only the one single object left in the image is given to further processing, including the final pattern\nrecognition system. So, only one object can be recognized at a time, since all the others are wiped out. It is\n16(Borji et al., 2015; Zhou et al., 2019; Chen et al., 2018)\n17The word “attention” is quite overloaded with different meanings in cognitive psychology. The sensory attention we have consid-\nered here is very different from some other kinds of attention. In particular, another type of attention very relevant for this book is\nsustained attention, considered in Chapter 11, which means you try to concentrate on a single task, such as reading a book, for an\nextended period of time. That is very different from sensory selective attention considered here since sustained attention is about long-\nterm attention on a task instead of relatively short-term attention on sensory objects. Selective attention can further be divided on\nanother axis: bottom-up attention, where an external stimulus grabs your attention (as in the case of interrupts in Chapter 10), and\ntop-down attention, used for example when you search for a certain person in a big room and only pay attention to faces. (The exact\nterms used in the different cases are quite variable in the literature.)\n18(Minut and Mahadevan, 2001; Mnih et al., 2014; Greff et al., 2016). Learning attention has recently become very fashionable in AI,\nespecially in large language models due to the methods proposed by Vaswani et al. (2017), although their use of the term is quite liberal\nand has only a vague resemblance to what we are discussing here.\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n133\noften said in cognitive neuroscience that “attentional resources are limited”, and here we see one illustration of\nthat principle: if you pay attention to one thing, you will necessarily tend to ignore everything else. This, in its\nturn, increases uncertainty since you don’t know much about those things you are not paying attention to.\nSubjectivity and context-dependence of perception\nAn important aspect of the uncertainty of perception is its subjectivity: I see one thing, and you may see some-\nthing different. Being based on unconscious inference using prior information, perception is subjective if dif-\nferent people or agents have different priors. Then they will interpret the incomplete incoming information in\ndifferent ways.\nThe priors used in human perception actually contain many different parts. There is one rather permanent\nand universal component, shared by all humans, and probably many animals. It includes those general regu-\nlarities that can typically be found by training artificial neural networks. But another component in the prior is\nmore individual and depends on the experience of the agent (animal, human or AI). When an agent observes\nthings happening, ideally it will incorporate all the new observations into its prior—possibly after performing\nsome kind of attentional selection. If it didn’t, it would be wasting valuable data that it has collected on the\nworld. It is this individual part of the prior, based on their own experiences, that makes the priors different\nfrom one agent to another. Each agent may even be living in a different environment; they may spend their\ntime in very different occupations. So, it is clearly useful that the prior is different from one agent to another.\nBut this necessarily implies that perception will be different as well. You don’t see exactly the same thing as your\nfriends, not to even mention your robot. This might not be such a serious problem if the agent understood the\nsubjectivity of perception well enough. However, such understanding often escapes even humans.\nThere is even a further component in the prior, which depends on the context, e.g., where the agent is at\nthe moment of perception. If you’re at home, you expect to see certain kinds of things, and if you’re walking\non the street, you expect to see other kinds of things. This leads to dependence of perception on the context,\neven for the same agent.19 These limitations of perception reflect the limitations of categories discussed in\nChapter 8. Categorization is usually based on perception, so if perception is subjective and context-dependent,\nthe categories inherit those properties as well.\nPerception is made even more subjective by the selection of incoming information by attentional mecha-\nnisms. Attention has a huge impact not only on the immediate perception in the agent, but also on the model\nit learns on the world. Fundamentally, attentional mechanisms choose the data that is input into the learn-\ning system. Anything not attended is pretty much ignored and not used in learning. As our brain “creates our\nworld” in the sense of reconstructing it from sensory input, that creation is thus significantly influenced by\nattentional mechanisms.\n19(Bar, 2004). Even the perception of pain is modulated by context and history (Tabor et al., 2017); see also Chapter 6 and its foot-\nnote 3. Perception can also be modulated by metabolic states, such as hunger (Livneh et al., 2017). There are also claims that desire and\naversion (motivational states) could directly influence perception (Balcetis and Dunning, 2006), but such phenomena are controversial\n(Firestone and Scholl, 2014).\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n134\nReward loss as mere percept\nA crucial insight that this view on perception gives to suffering is that its causes are subjective and generally\nuncertain: they may be based on faulty inference. In particular, reward loss is just another percept (i.e. a result\nof the process of perception). It is based on solving an inverse problem to infer the obtained reward, and this\ncan go wrong.\nMisperceptions of rewards may be particularly common when perception of other people’s reactions are\ninvolved. You might perceive the facial expression of your friend as angry, and register some negative reward\nas resulting from your actions. But perhaps your friend just had a bad headache, and his face reflected that;\ntaking the uncertainty of perception into account should help you behave in a more appropriate way towards\nhim.\nContextual information can even change a perceived positive reward into a negative one, and vice versa. In\none study, subjects were sniffing a combination of certain acids. In one session, they were told the substance\nis parmesan cheese, while in another session, they were told it is vomit. Depending on which category they\nwere given verbally, the perception was different, and even the pleasantness of the odor was dependent on the\nverbal label.20 An extreme example of misperception of reward is found with some drugs of abuse. They feel\ngood, and you perceive a reward on a biological level. Yet, such perception has no real basis: The drug merely\nmisleads your brain into perceiving a reward by perturbing its metabolism.21\nThe situation is even more complicated since in addition to perceiving the reward, the agent also computes\nthe expected reward based on the information it has at its disposal and using the available computational ca-\npacities. Thus, there seem to be two different ways in which uncertainty in perception affects the computation\nof reward loss: the obtained reward may be perceived wrong, or the computation of expected reward may go\nwrong.22 Both are just logical consequences of computation performed with limited resources and limited\ndata. Ultimately, a reward loss may even be illusory in the sense that one is perceived but it is merely a mental\nconstruct with little basis in reality. Accordingly, we should actually analyze the perceived reward loss instead\nof any objectively defined reward loss, since the agent can never know with certainty what the reward loss was;\nit acts according to its own perception, right or wrong. (We will postpone the details of such a re-definition to\nChapter 15.)\n20(Herz and von Clef, 2001). It is also typical for people to value objects more if it takes a lot of effort to obtain or produce them. This\ncan be seen as a simple heuristic to approximate the reward, but it can of course go wrong (Kruger et al., 2004). Eldar et al. (2016) further\npropose that mood influences perception of rewards, so that happiness makes rewards look higher, and the oppositive for a negative\nmood. Furthermore, individual differences are considered by Scherer (2021) in terms of “appraisal biases”, which is a mechanism\nexplaining individual tendencies to experience particular emotions, and ultimately, affective disorders; these could presumably be\ninterpreted in terms of different priors being used by the individuals.\n21(NIDA, 2020)\n22However, see footnote 19 in Chapter 16 on whether it makes sense to say that expectation of reward is “wrong”.\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n135\nAncient philosophers on perception\nThe uncertainty and subjectivity of perception were discussed by several ancient philosophers. In ancient\nGreece, the Skeptic school was particularly prominent in pointing out the limits of human knowledge, includ-\ning the relativity of perception. The Pyrrhonian branch was fond of giving examples where different people\nperceive the same thing differently:23\nWhen we press the eye from the side, the forms and shapes and sizes of the objects we see appear\nelongated and narrow.\nSuch uncertainty leads the skeptic to adopt an attitude of not making any judgements on external objects:\nSo, since so much anomaly has been shown in objects (...), we shall not be able to say what each\nexisting object is like in its nature, but only how it appears (...) therefore, it is necessary for us to\nsuspend judgement on the nature of external existing objects.\nA Japanese Yog¯ac¯ara-inspired poem beautifully describes a scene where different agents have very different\ninterpretations of the same sensory input:24\nAt the clapping of hands,\nthe carp come swimming for food;\nThe birds fly away in fright, and\nA maiden comes carrying tea—\nSarusawa Pond\nWhen somebody claps his hands by the famous Sarusawa Pond in Nara, Japan, the carps interpret it as a call\nfor feeding; the birds are scared of the noise and flee; while a maid of a near-by inn thinks a customer is calling\nfor her.\nIt is perhaps easy to admit that an animal or a robot sees things differently from yourself, either in a more\nprimitive way, or perhaps in a superhuman way. Yet, it is notoriously difficult for humans to admit that two\npeople can see the same thing in different ways, and that both ways can be equally valid. But there is something\neven more challenging; there is an even more difficult implication of the theories discussed in this chapter. It\nis the general idea that all our perceptions are actually just interpretations, or beliefs, or inferences, instead of\nrevealing an objective truth. In AI theory, it is never claimed that the agent knows anything; the very concept of\nknowing is conspicuously absent in that theory. All an AI agent has is beliefs, and those are usually expressed\nin terms of probabilities, lacking any certainty.\n23Sextus Empiricus’s Outlines of Pyrrhonism from ca. 200 CE, with translation taken from Annas and Barnes (1985), see also e.g.\n(Morison, 2019).\n24(Tagawa, 2009)\n\nCHAPTER 12. PERCEPTION AS CONSTRUCTION OF THE WORLD\n136\nWhen you take this line of thinking further, you may arrive at the idea that all we believe or pretend to\n“know” is based on our perceptions, and thus inherits the uncertainty and the subjectivity of perception. In\nfact, one could say that my perception defines my world. This may actually be rather obvious to anybody who\nprograms a sensory system in an AI. Such ideas are often associated with Asian philosophical systems such\nas Mahayana Buddhism, especially the Yog¯ac¯ara school and later schools drawing on those ideas, including\nZen.25 Yet, those ideas have also been beautifully expressed in the West, where their foremost proponent may\nhave been David Hume who wrote:26\nLet us chase our imagination to the heavens, or to the utmost limits of the universe; we never really\nadvance a step beyond ourselves, nor can conceive any kind of existence, but those perceptions,\nwhich have appeared in that narrow compass. This is the universe of the imagination, nor have we\nany idea but what is there produced.\nWe will see these deep points re-iterated and expanded in later chapters, especially Chapter 14.\n25(Williams, 2008b)\n26Hume, A Treatise of Human Nature, Section 1.2.6. In fact, Gopnik (2009) argues that Hume’s ideas may have been influenced by\nBuddhism through some Jesuits; see also footnote 33 in Chapter 8.\n\nChapter 13\nDistributed processing and no-self philosophy\nThe concept of a “self” is central for understanding suffering, but it is highly complex. Some aspects of self were\nalready considered in Chapter 6. In this chapter, I consider another central aspect of self, related to control.\nSelf can be seen as the entity that is in control of actions, including control of cognitive operations inside the\nagent, or, to put it simply, in control of the mind.\nIn preceding chapters, we have seen cases where the mind seems to be difficult to control, due to auto-\nmated interrupts and wandering thoughts. Here, I consider a general cognitive principle that explains why\ncontrol is limited. The idea is that when the information processing is parallel and distributed, it is difficult for\nany single part of the agent’s information-processing system to be in charge of the whole system, e.g. the whole\nbrain. This massively parallel and distributed nature of the brain thus creates most of the uncontrollability\nin the human mind. The lack of control considered here can also be seen as a generalization of dual-process\nnature of the mind considered in earlier chapters. Here, there are not just two processes competing for control,\nbut a great number of them.\nThese considerations necessarily lead to the question of free will: Can an AI, or even a human, actually\nhave free will—and what does that mean in the first place. From the viewpoint of the theories of perception\nin the preceding chapter, we can ask if perception of control and free will are simply illusory perceptions, thus\nproviding another link between the uncertainty of perception and uncontrollability. Such considerations have\nlead some philosophers to propose that there is no self, or no doer of actions, and I will revisit these ideas from\na computational viewpoint.\nAre you really in control?\nSuppose you just raise your arm—you can physically do it while reading this if you like. You probably think\nit was you who decided to raise the arm, and it was you who actually executed the action. You felt being able\nto control the world, or at least your arm in this case.1 This “you” that first controlled your mind by making a\ndecision, and then controlled your arm, is what can be called the self —in one meaning of the word. The self\n1Philosophers talk about (the feeling of) “agency” (Metzinger, 2003) . I don’t use that terminology because it would lead to confusion\nin this book where the word “agent” usually means something different. Furthermore, such agency is usually related to a conscious\nfeeling, while in this chapter, I refrain from talking about anything related to consciousness, which will be treated separately in Chap-\nter 14.\n137\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n138\nchooses actions, and controls some aspects of the world, including your inner world.2\nHowever, a number of thinkers have proposed that in fact, “you” are not really in control of anything. A\ncase in point is wandering thoughts. It can be claimed—following a strict definition of the term—that we never\nwant to have wandering thoughts: if we want to think what we are actually thinking, the thoughts are not\ncalled wandering. Furthermore, wandering thoughts often feel unpleasant, for example in the extreme case of\nrumination. So, why do we then continue having them?\nA well-known experiment on the control of thoughts is to try to not think of a pink elephant. This is another\nexercise you can do right now: for a minute or so, do not think of a pink elephant. What invariably happens\nis that you will be thinking of a pink elephant in spite of your trying not to, or rather because of that trying.\nClearly, our control of thoughts is limited.\nIn addition, interrupts such as fear, anger or desire capture our mind and direct the processing in ways we\nmight not want. Even habitual behavior can be seen as a lack of control in some cases: if you mindlessly follow\nhabits, you may end up doing something you would not have done if you had actually deliberately planned\nyour actions.\nLack of control increases suffering in our basic framework of suffering as frustration. Lack of control re-\nduces the probability that the agent reaches the goals it has committed to; it cannot get the things it wants,\nor avoid the things it is averse to. That means there will be more frustration and reward loss. In fact, the very\nexistence of suffering can be seen as a form of uncontrollability, since if you could really control your mind,\nyou would probably just switch off any feelings of suffering.\nPhilosophical views on uncontrollability\nIn philosophy, the idea of lack of control and its connection to the self goes back to, at least, the Buddha’s times.\nIn a famous discourse, he explained why there actually is no such thing as “self”. He started his refutation by\nconsidering the human body, saying3\n[I]f the body were self, the core of our being, then it would not tend to affliction or distress, and one\nshould be able to say of it, ’Let my body be thus (in the best of conditions); let my body not be thus\n(in a bad condition).’ It should be possible to influence the body in this manner.\nHe continued by going through different aspects of the human mind (perception, thinking, etc.), and denying\nthat any of them could be called the self either, since none of them can be properly controlled. For example,\n“no one can wish for and manage thus: ’Let my perceptions be thus, let my perceptions be not thus’ ”. If you\nsmell something disgusting, you cannot just decide not to smell it.\nThus, originally, the Buddha framed the very concept of self in terms of control: self is what is in control.4\nSince, as he argues, there is actually no (or little) possibility of control, there can be no self. Realizing this is\nthought to be essential to reduce suffering.5\n2(Skinner, 1996)\n3(Mahasi, 1996), based on Samyutta Nikaya 22.59; with explanatory text in parentheses added by Mahasi.\n4Arguably, the Buddha’s viewpoint could also be interpreted as the self being what can be controlled instead of what controls. Nev-\nertheless, according to Mahasi (1996, p. 12–14), what the Buddha is denying is precisely a “controlling self” as well as an “active agent\nself”. The two viewpoints are in a sense unified when Harvey (2009, p. 49) proposes that according to Theravadan Buddhist thinking,\n“a Self would have total control over itself.” In any case, this makes little difference in what follows where the main point is a general\nlack of control, or uncontrollability.\n5(Verhaeghen, 2017; Harvey, 2009)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n139\nIn ancient Greece and Rome, the Stoic philosophers had similar ideas. Perhaps the very core of Epictetus’s\nphilosophy is contained in his attitude towards control:6\nSome things are in our control and others not. Things in our control are judgement, pursuit, de-\nsire, aversion, and, in a word, whatever are our own actions. Things not in our control are body,\nproperty, reputation, command, and, in one word, whatever are not our own actions.\nEpictetus’s idea of uncontrollability is more limited: we cannot control what others do or think about us, or,\nin line with the Buddha, our bodies. But in stark contrast to the Buddha, he seems to think we can control at\nleast some of our thoughts and feelings, including desires and aversion. Presumably, Epictetus did not practice\nthe same kind of meditation as the Buddha, which might convinced him of the uncontrollability of thoughts\nand feelings. In any case, both philosophers advocated recognizing how little control we have as a means of\nreducing suffering—we will discuss such practical implications in Chapter 16.7\nWe seem to actually have two different kinds of uncontrollability here. First, the uncontrollability of the\noutside world as emphasized by Epictetus; and second, the uncontrollability of the mind as emphasized by\nthe Buddha. The uncontrollability of the outside world is easy to understand, and its causes are rather obvious.\nThe agent has limited strength: it probably cannot lift a mountain. It has limited locomotion: if it is designed to\nmove on wheels, it probably cannot fly. If it lives in a society, it has limited means of influencing other agents.\nWhat is less obvious, and my focus here, is that there seems to be so much uncontrollability regarding\nthe mind. We have already seen examples where control of the mind is lacking, as in the case of interrupts\nand wandering thoughts; the dual-process structure of the mind creates further conflicts and reduces control.\nTherefore, the question arises whether there is some general principle behind all of those manifestations of\nuncontrollability.\nNecessity of parallel and distributed processing\nThe basic idea here is that the lack of control of the human mind is fundamentally based on one property of\nthe brain: parallel and distributed processing. That is, there are many processors, or neurons, processing the\ninformation at the same time, and to some extent independently of each other. If there are many processors\nworking independently, each of them cannot be in control of the agent’s actions: there has to be some kind of\narbitration, at the very least. Modern AI also uses such parallel and distributed processing, in particular in the\nform of neural networks. Both the brain and neural networks in AI are in this way fundamentally different from\nan ordinary computer, which typically uses serial processing in a single processor.8\n6The very first lines in The Enchiridion, translated by E. Carter except for “judgement” for hypolepsis which is by R. Dobbin.\n7This very well-known quote from The Enchiridion may not give a very clear idea of what exactly Epictetus considered to be under\nour control. In his Discourses, a more detailed picture emerges, but it is not entirely consistent. He usually says that one of the follow-\ning two things is the only thing under our control: either the “will” (prohairesis, e.g. Discourses, I.22.10), or what he calls the “use of\nimpressions” (e.g. Discourses, I.1.7, I.12.34). The latter includes the judgement of good and bad, as well as judgement of (moral) right\nand wrong (Discourses, III.22.42). For Epictetus, an impression (phantasia) seems to be what we would call “percept(ion)”, although\naccording to Long (2002, Sec. 5.1) it encompasses the “thoughts and states of consciousness in general”. It may be that for Epictetus\nthe use of impressions and the will are two sides of the same coin, as discussed at length by Girdwood (1998), since the correct use of\nimpressions is necessary and sufficient for the correct use of the will (Discourses, I.1.12, I.30.4, II.1.4).\n8In practice, a personal computer or a mobile phone would not usually have just one single processor, but a small number of them,\ntypically less than ten. For example, the display would be supported by a separate processor, a graphics processing unit. Merely for the\npurpose of keeping the discussion simple, I will assume an ordinary computer has just a single processor.\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n140\nWhile these properties have been mentioned in earlier chapters, we have not really considered the question\nof why parallel and distributed processing happens. From a biological perspective, we need to find some evo-\nlutionary justification for why the brain is parallel and distributed, and from a computer design perspective,\nwe need to explain why such processing would be useful. Perhaps we can answer both questions if we simply\nfind some fundamental computational advantage in parallel or distributed computation.\nFailure of Moore’s law and necessity of parallelization\nLet’s first consider the question of parallel processing from an AI viewpoint: What is the point in using many\nprocessors? If you want to speed up your computations, why not just get a single processor which is, say, a hun-\ndred times faster, instead of putting together one hundred more ordinary processors that compute in parallel?\nObviously, there is a limit to how fast processors you can buy for an AI. Perhaps you need faster computation\nthan what is given by the fastest single processor available today. That is why all the supercomputers in the\nworld are highly parallel; they are collections of thousands of processors. That is the only way to increase the\ncomputational power to record-breaking extremes.\nOn the other hand, if you’re really lazy, you might be tempted just to wait. We all know that the technology\nbehind the processors has been developing at an enormous speed. The famous Moore’s law states that the\ncomputing power of a processor doubles every two years. This may lead to the impression that there is really\nnot that much reason to go through the trouble of parallelization: if the fastest processor is not fast enough,\njust wait a few years, and it will be. If this logic were true, it would also mean that there may not be any funda-\nmental reason why computation in AI needs to be parallel, since the power of a single processor seems to grow\nexponentially and without limit.\nYet, there are fundamental reasons why really efficient computation may not be possible at all without par-\nallel computation, and why, in fact, Moore’s law is not true anymore. One reason is that making processors\nfaster is to a large extent driven by making them smaller. A smaller processor means shorter delays in transmit-\nting the information inside the processor. Such miniaturization cannot go on forever because at some point,\nyou get too close to the level of single atoms, and even the laws of physics change in the sense that quantum\nphenomena start appearing.9\nA more practical problem is that due to complicated physical phenomena, faster single processors use\nmuch more energy than a set of slower processors with the same total computational capacity.10 Energy is ob-\nviously expensive and cannot be used in unlimited quantities. Moreover, such an increase in energy consump-\ntion has another, surprising effect, which is that the processors heat up very quickly, and keeping processors\ncool is increasingly becoming a problem. If you design a new processor which is ten times faster than your\ncurrent one, the power consumption and the heat generated are usually much more than ten times larger.\nSo, these are convincing reasons why it is necessary in AI to use many processors in parallel. In fact, the\nspeed of a single processor (“clock rate”) even in mainstream computers has not been really increasing since\naround 2005. The overheating problem became so serious that faster processors became impractical to use.11\nSince splitting the computations into many processors generates less heat, manufacturers started putting to-\ngether several processors on a single chip— the processors are now called ”cores”. The number of cores in an\n9(Theis and Wong, 2017)\n10(Markov, 2014)\n11(Markov, 2014; Gorder, 2007)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n141\nordinary computer is still small, though, so this is very far from the massively parallel case seen in the brain.\nParallelization can be hard\nThus, the great promise of parallel processing is that it can be much faster than serial processing, given the\nsame budget of energy, or, indeed, money. However, there is a problem. If you have one hundred processors\nthat process the same information at the same time, the processing could be, in principle, a hundred times\nfaster. But that only happens in an ideal scenario where the computations are such that they can be paral-\nlelized, i.e. they can be simultaneously performed on one hundred processors without any problems. Some\nproblems can easily be parallelized, while others are more difficult, perhaps even impossible. Programming\nparallel systems needs special algorithms, as well as specialized expertise.\nConsider a problem of finding a small object, say a single very black pixel, in an input image. (Suppose\nfor simplicity there is only one such object in the image). You could have a single serial processor scanning\nthe image pixel by pixel. That might take, say, 100 microseconds (one microsecond being one-millionth of a\nsecond). On the other hand, if you have 100 processors, you could split up the image into 100 regions, and tell\neach processor to search for the pixel in one of the regions, and then report to a central processor whether it\nwas there or not. That should not take much more than 1 microsecond. This problem is easy to parallelize,\nand the speed-up (100x) is basically the same as the factor by which you multiplied the number of processors\n(100x). A neural network, whether in AI or in the brain, can do such computations massively in parallel, and\nthus incredibly fast. This is one of the reasons for the impressive behavior of the human visual system, and the\nsuccess of neural networks computer vision tasks.12\nThen there are tasks that are really difficult to parallelize. This is generally the case when you need to\ncompute an intermediate result before proceeding further. As an intuitive example, consider building a house\nwith rather traditional methods. You first have to build a foundation, and let it dry. Then you build the walls,\nand finally, set the roof. Suppose you had an unlimited number of builders that you can use; telling them\nwhat to do is like trying to parallelize computation. Now, the problem is that you cannot meaningfully divide\nthe builders into three teams so that one of them sets the roof at the same time as another group lays the\nfoundation! Also, if you really have a huge number of builders, they would not even fit on the building site. So,\nparallelization can be tricky.\nOptimization by a gradient method is an example of something that is typically considered difficult to par-\nallelize because you need to do it step by step, and each step needs the result of the previous step. Yet, a lot of\neffort has been spent in computer science research to figure out methods that enable parallelization of such\nalgorithms, sometimes quite successfully.13 With a lot of intellectual effort and ingenuity, it is possible to par-\nallelize even seemingly impossible problems. However, such parallel methods can be quite complicated.\nThe fact that some computational problems are hard to do in parallel while others can be parallelized very\nefficiently is part of the reason why ordinary computers and the brain are good in very different things. The\nbrain is particularly good at vision, for example. Vision can be rather easily parallelized, as was seen in the\n12To take another example: Tree search, which is essential planning, can also be parallelized, although it is a bit more difficult. After\na few steps in the search tree, you can distribute the different branches to different processors, and each processor can search further\nin one of the branches. Intuitively, this would be like a boss assigning five different scenarios to her employees in a planning exercise.\nEach employee gets one scenario which each start from different assumptions, corresponding to the first branchings of the search tree.\nAfter the initial hurdle of formulating the scenarios (that is, building the initial branches of the tree), the parallelization is easy.\n13(Zinkevich et al., 2010; Recht et al., 2011)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n142\nsimple pixel-finding example above, and indeed the best AI solutions to vision have imitated the brain using\nneural networks. On the other hand, ordinary computers are very good at logic-symbolic processing, which\ntypically happens step by step, as discussed earlier.\nBut what is the evolutionary import of these considerations—does it make sense to claim that the brain is\nmassively parallel because of the above-mentioned reasons related to the clock-speed of processors? Certainly,\nthe constraints in building an intelligent system with biological hardware are very different, and the logic above\nmay be mainly relevant for AI. What it actually shows is that progress in AI seems to need computers which are\nmore and more similar to the brain. Yet, it is possible that the massive parallelization in the brain might have\nsome relation to the energy-efficiency considerations that we just saw.\nDistributed processing reduces need for communication\nThe second question is why distributed processing is needed. Distributed processing is different from parallel\nprocessing in that the emphasis is on different processors working independently with as little communica-\ntion as possible. Distributed computing is important, even necessary, simply because communication is often\nquite expensive. In the brain, most of the volume actually consists of white matter, which is nothing else than\n“wires” (called “axons”) connecting different neurons. Those wires take up much more space than the actual\nprocessing units. So, the sheer space available in the head strongly limits the connectivity of brain neurons.14\nIn addition, communication consumes energy which is, again, another limiting factor.\nWhat makes achieving full connectivity particularly difficult is that the number of possible connections\nbetween processing units grows quadratically as the number of processing units grows. If you have a mil-\nlion processors, and you want to build connections between all the possible pairs, you need almost a trillion\n(1,000,000,000,000) wires (assuming each wire can transmit information in one direction only, as happens in\nthe brain). So, the amount of connections easily becomes a limiting factor, and it is important to perform the\ncomputations using minimal information transfer between the processing units, by judiciously designing the\nalgorithms as well as the connections between the different areas.15\nThis is the central point about distributed processing: When communication between the processors is\nexpensive, special solutions are needed. In AI, there is a thrust to distribute AI computation to smartphones\nthat collect the data in the first place, so that the amount of data they transmit to each other or any central\nserver would be minimized.16 In the brain, part of the solution is that processing is very clearly distributed\non the level of large brain areas. There are areas responsible for processing visual input, areas for processing\nauditory input, areas responsible for moving the muscles, areas for spatial navigation, and so on. Each of these\nareas does its computations relatively independently. That is possible partly because they get different input\n(visual vs. auditory), and partly because they need to solve different computational tasks (object recognition\nvs. moving muscles). The communication between those areas can then be strongly limited, and less wiring is\nneeded.\nDistributed processing will create its design problems, just like parallel processing. Some tasks are easy\nto distribute over processors, while others are less so. Again, neural networks are an example of processing\nwhich is highly, even massively distributed, and clearly works well in applications such as sensory processing\n14(Zhang and Sejnowski, 2000; Hari, 2017)\n15(Bullmore and Sporns, 2012)\n16Reviewed by Xu et al. (2020), but see also Corneo et al. (2021) for a critique of such distribution in the current infrastructure.\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n143\nof images and sounds. Considering the example of finding a small object in an image described above, it is easy\nto see that the computation described is also strongly distributed since the 100 processors each get their own\ninput and then do their computations with no communication between them needed.\nCentral executive and society of mind\nThe logic above suggests that sophisticated intelligent agents may have to be a collection of relatively indepen-\ndent parts or processors–and that is certainly the case in the brain. The resulting computing system is very\ndifferent from the view we intuitively have of ourselves. We tend to think of ourselves as serial processors be-\ncause much of our inner speech and conscious thinking is serial. Speech is inherently serial because the words\nfollow one after another in one single “train” of thought. But such introspection, based solely on what we can\nconsciously perceive, is quite misleading.\nA simple metaphor for illustrating the counterintuitive properties of a parallel and distributed system is\nthe “society of mind”: the different mental faculties are compared to human individuals that together consti-\ntute a society which is precisely the mind.17 One individual (or processor) is monitoring, say, the state of the\nbowels, and another one is, independently, responsible for recognizing the identities of faces whose images\nare transmitted by the eye. Those processors are like human workers with well-defined, separate tasks. Each\none may be active much of the time, thus working in parallel. In line with the computational arguments we\njust discussed, it may also be intuitively clear that it is important that the different individuals mind their own\nbusiness most of the time, focusing on their own part of the work. Thus, they only interact if it is really neces-\nsary, with minimum communication; thus, the operation is distributed. This metaphor is trying to counteract\nthe intuitive impression we tend to have that the mind is a single, serially processing entity which would be\ndifficult to divide into parts.\nNow, returning to the question of control, consider whether it is possible that one of the independent pro-\ncessors is actually in control of all the others. Psychological theories often use the term central executive for\nthat part of the mind which is supposedly in charge, controlling the rest.18 At first sight, having such a central\nexecutive sounds like common sense. The brain has many sensory processing systems (vision, audition, etc.),\nit can send commands to a multitude of muscles to execute actions, and above all, it has complex information-\nprocessing capacities in terms of planning and learning. It would seem that such a system must fall into com-\nplete chaos unless there is one area which controls the others. That would be the central executive, a brain area\nthat controls all, or at least most, of the other areas. It would integrate information coming from them and,\nin return, send processed information and commands to each of them. In the society of mind metaphor, this\nwould correspond to a leader of the society that tells all the individuals what they should do.\nIt could be argued that having a single area to control all the others is to some extent in contradiction with\nthe whole point of distributed and parallel processing. The central executive would need to have particularly\ngreat processing power, and it would need to receive a huge amount of information from all the other parts\nof the whole system. Thus, both the two bottlenecks discussed above, processing speed and communication\ncapacity, would resurface—but we will see below that this is not really the case.\n17This is a rather liberal interpretation of Marvin Minsky’s original idea (Minsky, 1988; Singh, 2012). For Minsky, the individuals\n(which he calls “agents”) are very simple, more like subroutines in a computer program, as opposed to humans.\n18(Baddeley, 1996)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n144\nDesigning such a system with a central executive is not very different from designing different decision-\nmaking systems in a human society or organization. If there is a single leader, she must inevitably delegate a\nlot of power to others (say, ministers) in order to reduce the processing power needed by herself. Then, the\nleader is strongly dependent on the information passed on by the ministers; the leader does not have enough\ntime to make decisions on all the details. So, the power of the central executive is limited due to the limitations\non the computational power of a single processor.\nOn the other hand, if there were a central executive, what about wandering thoughts, emotional interrupts,\nor habits? Is the central executive just watching when the whole system is hijacked by the fear elicited by\nthe sight of, say, a spider? We argued in earlier chapters that emotional interrupts are useful for evolutionary\npurposes, so the leader might actually not be very unhappy about that. But interrupts, by their very nature,\ncannot be prevented, not even by the central executive. Is there any point in calling such a leader the central\nexecutive if she is not really controlling everything that happens? What if you eat chocolate because you have\na habit of doing it every day (in addition to an irresistible desire, perhaps), even though one part of you knows\nit is bad for you in the long run—who actually made that decision?\nThis logic has led many to the proposal that in the human mind and brain, there is no central executive, or,\nmetaphorically speaking, the society of mind has no leader. That is, there is no part in the mind that controls\nthe rest, nothing that controls everything else that happens in the society.19 The society is fundamentally a\ncollection of relatively independent actors. This means very concretely that there is no particular part of the\nmind or the brain that would control our thoughts, feelings, or desires: they just come and go depending on a\ncomplex interaction between different brain areas. Each part of the mind can propose its own mental actions.\nOne part of the visual system might tell the motor cortex: “Let’s move the eye gaze to the right since there\nseems to be something interesting there”, but at the same time, the replay system might insist on replaying a\npast episode while ignoring whatever may be happening in the outside world. The result may be a bit chaotic,\nand the appearance of, say, wandering thoughts would not be surprising. To the extent that we define the self\nas the central executive, this would mean that there is no self, in line with Buddhist philosophy, for example.\nWhile such a philosophy is fascinating, it has to be pointed out that there are also neuroscience results\nclaiming that some brain regions in the prefrontal cortex are actually the central executive.20 Moreover, in the\ndesign of distributed computing architectures, it is well-known that having some kind of a central processor\nactually makes communication easier. The point is that there is a good compromise to be found between the\ntwo extremes of completely distributed computation and computation in a single processor. Such a compro-\nmise can in fact be found in computation which is mainly parallel and distributed, but, crucially, includes a\ncentral processor that coordinates the computation, which is still mainly performed by the other processors.\nIn the example above, with a million processors, we saw that a fully distributed system might need a trillion\nwires to connect all the processors with each other. But suppose that all the communication happens through\n19(Metzinger, 2003; Eisenreich et al., 2017)\n20In human neuroimaging literature, the existence of a central executive network is more or less accepted by many authors (Koechlin\nand Summerfield, 2007; Sridharan et al., 2008; Botvinick and Cohen, 2014; Marek and Dosenbach, 2018). The functions attributed\nto the central executive may vary, and many authors indeed talk about a number of different “(central) executive functions” without\nclaiming that they are performed by a single entity, whether brain region or network, as discussed by Miyake et al. (2000); Diamond\n(2013). For this book, the main executive function discussed is the control of actions and thoughts, as treated in the following sections;\ninhibition of “impulses” and automatic behavior such as interrupts is a fundamental instance. Morales and Lau (2020) further argues\nthat prefrontal cortex is necessary for consciousness. (See Teper and Inzlicht (2013) for a discussion on how “self-control” could be\nimproved by meditation.)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n145\na central processor, which further selects and processes the information to be transmitted to each of the other\nprocessors. Then, all that is needed is wires from each processor to the central one and back (figuratively called\na “hub-and-spoke” architecture), which means about two million wires, enabling a reduction by several orders\nof magnitude. Still, the computational power of the system need not be restricted by the central processor if it\nis skillfully designed to “delegate” the hard computation to all the processors and only take a coordinating role.\nSuch architectures are currently of great interest in artificial intelligence.21\nIn fact, the whole dichotomy between a powerful central executive and no central executive is a bit artificial.\nThere can be varying degrees of control that a central executive is able to exercise. While it is not possible\nto say much with certainty on this topic, the reality in the brain may well be that there is a relatively weak\ncentral executive that controls some things to some extent, perhaps many things to a limited extent, but it does\nnot control everything. It may be in control a lot of the time, but not when emotional interrupts, wandering\nthoughts, or similar processes take control of the mind. Thus, while parallel and distributed processing is\ninherently without central control, it may be advantageous to introduce some limited form of central executive,\nand this may turn out to be the best description of what happens in the brain.22\nControl as mere percept of functionality\nYet, what is undeniable is that I clearly feel that I can control my body and do things such as raising my arm.\nA central executive is often intuitively assumed to exist based on exactly such a feeling of self, or a feeling of\ncontrol. But why should we assume that there is a central executive simply because it feels like there is control?\nThe feeling of control is just another form of perception, and as we have seen, perception may not be accurate.\nPerception follows certain rules outlined in Chapter 12. It is usually based on incomplete information which\nhas to be combined with prior assumptions to arrive at a conclusion, and this conclusion or inference is what\nwe perceive. Mistakes do happen in this process.\nThe perception of control in the brain seems to be based on predictions—like so many other things in the\nbrain. Every time you engage in any action, your brain tries to predict the outcome of the action. In particular,\nwhen the brain sends detailed motor commands to the muscles, it uses an internal model to predict how the\nlimbs should move as a result. The brain then computes an error signal, comparing the predictions with the\nactual outcome. In humans, small errors in such predictions are actually quite common because of constant\nphysiological changes in your muscles due to fatigue; or it could be that you are holding something heavy in\nyour hand, which increases the force required to lift the arm. Computing the prediction errors is useful since\nthey enable the brain to learn or adapt its motor commands to such changing circumstances.23\nNow, if the prediction error is small (the actual outcome of the action is not very different from the predic-\ntion), you feel that you generated the action, and you are in control, according to current thinking in neuro-\n21For example, “federated learning” has recently emerged as such a paradigm (Kairouz et al., 2019).\n22Further theoretical neuroscience arguments on this question can be found in (Botvinick and Cohen, 2014; Rueda et al., 2004;\nBaumeister et al., 2007).\n23(Kording et al., 2007). This is an example of the principle of feedback for successful control and action. Feedback is a general\nprinciple in action selection, which is important if there is uncertainty in the world. Just finding the best path to the goal is not sufficient\nif the environment is uncertain and may change. For example, if you calculate the best possible path to a restaurant, that is usually\nfine, and you can just walk there. But unexpected things might happen: A road might be blocked by a delivery truck; there might be\nconstruction work. This is another limitation of purely planning-based action selection in a changing, uncertain environment.\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n146\nscience.24 This is the computational mechanism underlying the perception of whether you are in control. In\ncontrast, if the errors are very large, the feeling of control is disturbed, and various pathological symptoms may\narise. You may even feel the arm is being controlled by somebody else (by “them”, or by “spirits”), as typical of\nsome schizophrenic patients.25\nBased on his extensive psychological experiments, Daniel Wegner26 proposed a related theory: the percep-\ntion of control is simply based on one part of your brain observing a correlation between two things, which are\nthe formation of an intention to act (intention being used here in the ordinary sense of the word, not in the AI\nsense as usually in this book) and the action actually taking place. If the action happens soon enough after the\nformation of the intention and the action happens as you intended (and you cannot explain the action in any\nother simple way), the brain concludes that “you” actually performed the action out of your own “free will”.\nA strong correlation between intentions and outcomes is not very different from small prediction errors, and\nthus this psychological theory is very much in line with the neuroscience results cited above. Interestingly, just\nlike visual neuroscientists who construct optical illusions, Wegner then devised clever experiments where the\nperceptual system makes the wrong conclusion about control, thus showing that the feeling of control can be\nfooled like any other perception.\nAny of these computations are actually quite simple and could be easily implemented in a robot. A robot\ncan assess whether it is able to control its arm by comparing the results of its motor commands and the actual\noutcome. Suppose some kind of central processor sends a command to the joints in the arm that the arm\nshould be lifted by 10 cm. A couple of seconds later, the input from the camera (or input from specialized\nsensors in the joint) tells the central processor that the arm was, indeed, lifted by 10 cm. The central processor\nthen concludes that it is in control of the arm.\nThis logic demystifies the concept of control, which is no longer anything deep or philosophical. The per-\nception of control by the robot above is due to computations of a rather practical nature. In fact, any agent\nshould have a model of what parts of the world it can control (e.g. its limbs) and which parts are outside of its\ncontrol (e.g. mountains). This is in contrast to my everyday perception that it is I, or myself, that is in control,\nwhich is the result of a very complex inference process, and possibly exaggerated, misleading, or even false and\nillusory. Our everyday perception of control by ourselves is, therefore, no proof for the existence of a central ex-\necutive, or “self”, that controls actions. One could say that our perception only indicates that there is control in\nthe simple sense of the limbs moving as expected, but it does not necessarily mean that there is any particular\nentity that is in control. In other words, our feeling of control simply means that certain systems are working\nin a predictable way, correctly and in harmony with each other; in particular this is about the decision-making\nsystem, the motor system, and the actual limbs (or “actuators” as they are called in robotics).\n24(Haggard and Chambon, 2012; Wolpert et al., 1998; Wen and Imamizu, 2022; Choudhury and Blakemore, 2006)\n25(Spence et al., 1997; Frith, 2012)\n26(Wegner, 2002, 2003); see also (Hommel, 2013; Pockett et al., 2009)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n147\nFree will\nFree will is a celebrated and highly controversial concept in Western philosophy—the idea that you decide your\nactions “yourself”, that is, your actions are not merely a function of external circumstances, such as your past\nor other agents. Free will is very closely related to control and feeling of control: most of neuroscience uses\nthe terms almost interchangeably. There are some nuances, though: talking about free will emphasizes your\ncapacity to decide what you will try to do (e.g. choosing a goal), while talking about control emphasizes your\nability to actually do it (e.g. reaching the goal). A very clear difference is, moreover, that free will is almost\nalways considered a conscious phenomenon, while perception of control need not be, as we saw above: Even a\ncompletely unconscious robot would benefit from knowing which events are due to its own actions and which\nparts of the world it can control.\nPhilosophers have been debating about free will for hundreds of years. Democritus claimed already around\n400 BCE that everything, including humans, consists of atoms, and follows strictly deterministic causal laws,\nthus excluding any free will. A bit earlier in India, the Buddha had debates against philosophers who held\nsimilar, strictly deterministic views.27\nIn a famous series of experiments, Benjamin Libet recorded an EEG response that is known to precede any\naction decision. The results showed that conscious experience of the decision started up to half a second after\nthe beginning of the EEG response. From this, it is tempting to conclude that consciousness cannot cause the\naction decision, and hence there is no (conscious) free will. The EEG presumably measured some unconscious\nprocesses which started the decision-making process long before any involvement by conscious processes. Li-\nbet’s own interpretation, though, was that consciousness could still participate in the action decision by having\nthe possibility of “vetoing” any decision that the unconscious circuits were trying to implement. This would\nimply some weak version of free will, but his interpretation is controversial.28\nSome would argue that denying free will may be dangerous: people have to believe in free will in order\nfor our moral systems to work. If people don’t believe in free will, they might not feel they have any moral\nobligations and might behave just as they please. Our justice system in particular is based on the idea of free\nwill: if it can be proven in court that a murderer acted without free will, say, because of a brain tumour, that will\nusually lead to a reduced sentence. This is, of course, not saying that there is free will, just that it may be useful\nto think that there is one.29\nOne of the most influential psychologists in the 20th century, B.F. Skinner, had a more computational view-\npoint. He thought human behavior is simply determined by rewards and punishments. That is also where\nthe “moral” behavior comes from; no special metaphysical beliefs are necessary. Reward people for good be-\nhavior and punish for wrong behavior; that is all that is needed to make them follow moral rules, in Skinner’s\nview. From the perspective of this book, I can partly agree with Skinner on the importance of learning from the\nenvironment, where learning, as always, includes evolution.\n27Early Buddhist philosophy is actually often interpreted as deterministic, based on the Buddha’s emphasis on causal chains (e.g.\nSamyutta Nikaya 12.12; see also page 176 below), but he also admitted the existence of “an element or principle of initiating an action”\n(Anguttara Nikaya 6.38), which sounds a bit like free will. See Federman (2010) for details.\n28(Libet et al., 1983)\n29(Vohs and Schooler, 2008; Baumeister et al., 2009; Roskies, 2006)\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n148\nPhilosophy of no-self and no-doer\nLet’s go back to the “no-self” quote by the Buddha that we saw earlier (page 138). In Buddhist philosophy, it\nis the historical basis of a celebrated doctrine claiming that there is no such thing as the self. This is clearly a\nmuch more general idea than the mere claim that there is no entity which is in control, but in this chapter, we\nfocus on the aspect of control and free will.\nWe can now recapitulate the ideas in this chapter in view of justifying some claims regarding the existence\nof self. First, if the brain, and thus the mind, is composed of many different processors all working simultane-\nously and to a large extent independently of each other, how could we speak of a self? If we admit there is no\ncentral executive, that is one form of “no-self”: there is no particular part of the mind that actually is in con-\ntrol and could be called self in that sense. (It is not clear if it is neuroscientifically correct to deny the central\nexecutive, but let’s admit it for the sake of argument here.) The conscious part of the mind does not control ac-\ntions according to neuroscientists such as Libet and Wegner, thus contradicting our everyday perception that\nwe decide actions on the conscious level. Decisions seem to be actually taken by various unconscious neural\nnetworks, and it may be difficult to point out any single entity making the decision.\nSome parts of Indian philosophy actually formulate a more specific doctrine of “no-doer”, which means\nthat there is nobody that “does” anything in terms of taking the actions—or that at least, it is not “you” that\ndoes anything. Instead of “you” making conscious decisions and being in control, your body and mind are\nconstantly on some kind of autopilot, and your consciousness is merely observing it all.30 The points above\ngive some credence to such a variant of no-self philosophy.\nBut we may go further. If it is not your conscious self that decides, is it even necessarily your neural net-\nworks? In our framework, we could say that control is ultimately exercised and decisions are ultimately taken\nby the input data that the agent learns from. Our computational models have assumed that our actions are\ndetermined by past input data, together with the design of our learning and inference machines—even though\nthe mapping from input data to action can be extremely complex and impenetrable.31 From this viewpoint,\nnobody at all is in control, and there is no free will even in some unconscious form—while it is possible to say\nthat there is some control in the specific sense of sufficient predictability. Furthermore, the existence of a cen-\ntral executive in the brain becomes irrelevant: if it exists, it is still only a vehicle for the evolution and the input\ndata to steer our thinking and behavior. Even a brain with a strong central executive could be seen as having\nno self: even though a central executive was seen as the hallmark of a self earlier in this chapter, if we now see\nall its actions as simply following from learning based on input data, it may not actually qualify as a self.32\n30See e.g. the classic Theravadan Buddhist meditation manual Visuddhimagga (Chapter XIX,20); or the Advaita Vedanta teacher\nNisargadatta (1982). The Buddha himself may not have formulated no-self in exactly this way; he even seems to argue against it in\nAnguttara Nikaya 6.38; see Harvey (2009). Such a no-doer philosophy is also in strict contradiction with Stoic thinking, where the will\n(prohairesis) is the one thing we can control (possibly in addition to “use of impressions”), see footnote 7 in this Chapter.\n31This is not the same as the Skinnerian viewpoint because most of the time, nobody is explicitly and purposefully feeding all the\ndata into your brain to “train” you; most of the data is just passive observation, often with no rewards involved.\n32It is obviously important to consider the exact definition of free will. In one radical viewpoint, freedom of will is a matter of not\nbeing physically or psychologically forced or compelled to do what one does. This viewpoint is called “compatibilism” in philosophy\nsince it implies that free will is compatible with determinism (Strawson and Watson, 1998). Consider a basic robot. If it decides to raise\nits arm, is there any physical constraint that would prevent it from doing so? Are the computations only due to its own sensory input (in\nits cameras or else) and are the computations made in the processors inside the robot? Here, the idea of free will is formulated in terms\nof what causes the agent’s actions: Is it solely information-processing inside the agent—in a modern formulation—or is something\noutside it influencing the decision? If so, even such a robot could be said to have free will. Humans would certainly have free will. It\n\nCHAPTER 13. DISTRIBUTED PROCESSING AND NO-SELF PHILOSOPHY\n149\nAjahn Brahm, a famous meditation teacher, once said that when he sits down to meditate, he always re-\nmembers the instructions of his own meditation teacher in his head; thus, it is not really Ajahn Brahm who\nmeditates, it is his teacher—or, if I may, it is the input data he received from his teacher.33\nmay not be the conscious self that decides actions, but the neural networks in the brain. Still, as long as the neural networks are inside\nthe human skull, it is the human that decides and controls its actions. Yet, many find such a definition of free will questionable. These\ninclude all the schools in the philosophy of free will other than the compatibilists. What I described in the main text is rather similar to\nthe “pessimist” school. Namely, an obvious counterargument to the compatibilist definition is that is depends on the time scale used:\nwe should look back in time, trying to find the original reasons for your actions. As I argue in the main text, fundamentally, the robot’s\nor human’s actions are just a result of its programming/evolution and, especially, the input data, so there goes free will.\n33This example also points out how a lot of the input data comes from social interaction, which is outside of the scope of this book.\nIt could further be argued that the input data is not processed in the same way in all individuals due to genetic variation and possibly\nsome other biological differences.\n\nChapter 14\nConsciousness as the ultimate illusion\nWhy do we have conscious experiences? This is one of the deepest unanswered questions in modern science. It\nis not even quite clear what the whole question means and how it could be formulated in a rigorous, scientific\nmanner. One thing that is clear, however, is that consciousness is somehow related to suffering. Some would\neven claim that in a strict sense, there can be no suffering without consciousness.\nIn this chapter, I try to shed some light on the nature and possible functions of consciousness. I consider\ntwo different aspects of consciousness: it can be seen as performing some particular forms of information-\nprocessing, or it can be seen as a subjective experience. I provide a critical review of the main theories con-\ncerning these two aspects. In particular, I explain how consciousness is related to mental simulation and the\nself, and as we have seen, those play an important role in suffering. This leads to some old, but still radical,\nphilosophical ideas about the nature of our knowledge of the world and how it relates to our consciousness.\nUltimately, I argue how changing your attitude to consciousness may actually have a strong influence on your\nsuffering, a theme that will be further elaborated in later chapters.\nInformation processing vs. subjective experience\nThe main problem we immediately encounter in research on consciousness is the difficulty in defining the\nterms involved. “Consciousness” has different meanings to different people and in different contexts.1 For\nour purposes, we can divide the concept of consciousness into two aspects. First, there is the information\nprocessing performed by human consciousness. This is something we might understand based on AI, since\ninformation processing can usually be programmed in computers. One approach is to ask what the compu-\ntational function, or utility, of consciousness might be in humans; these are relatively well-defined scientific\nconcepts and questions. This approach is fine as long as we are content to consider consciousness as another\nform of information-processing, or computation. The second, more difficult aspect of consciousness is the ex-\nperience. That is, the conscious “feeling” which is specific to myself, i.e., subjective. Its existence is so obvious\nthat it is rather neglected by most people.\nWhen you look at the text in this book, several quite amazing things are happening; they can be roughly di-\nvided to information processing and experience. Those related to information-processing have been discussed\nearlier in this book. Light enters your eye, generates electrical signals on the retina, the signals travel into your\n1(Van Gulick, 2021)\n150\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n151\nbrain, and some incredibly intricate information processing takes place, allowing you to recognize the letters\nand even transform the letters into words. However, all that is simply information processing, and it can soon\nbe programmed in a computer—in some more rudimentary form, it is possible even now.\nBut in addition to such information-processing, there is something else: you have a conscious, subjective\nexperience of the book, the letters, and the words. Somehow, almost magically, the book appears in some kind\nof a virtual reality created by your brain. We tend to think that this is normal since the book is there, and\nwe simply “see the book”. But in fact, the conscious experience is not somehow in the book, and it does not\nsomehow automatically come out of the book. The experience, the awareness of the book is created by some\nfurther mechanisms which we simply don’t understand yet. This experiential aspect is called phenomenal\nconsciousness. Philosophers use the word qualia in this context: the conscious “quality” of the book being\nseen, “what it is like” when the book is consciously experienced. Or, as more poetic narratives would have it, it\nis the “redness of a rose”. It is not information processing but something more mysterious.\nIt is this phenomenon of subjective experience, or qualia, which is the main topic of this chapter. It is also\nthe main meaning in which I use the word “consciousness” in this chapter; “awareness” is used in exactly the\nsame meaning, and so is “conscious experience”.\nThe computational function of human consciousness\nNow, what is the connection between these two phenomena: information-processing and consciousness?\nConscious experience is certainly not just one form of information-processing, but the connection is extremely\ndifficult to understand. In fact, consciousness must have some connection with information-processing: the\nqualia of the rose must be based on processing of incoming sensory input, even if most of that sensory pro-\ncessing seems to be unconscious. Let us assume, in the following, that part of the information-processing in\nthe human brain is conscious, in some sense to be elucidated. This is such a typical assumption that it is often\nnot even made explicit.\nLet us then try to understand what we can say about the function, or utility, of such conscious information-\nprocessing. Taking a more neuroscientific approach to the question, one can first ask: What are the evolutionary\nand computational reasons why certain animals, such as humans, have consciousness? We assume here that\nconsciousness is a faculty that is a product of evolution—but strongly influenced by culture, of course. It is\nquite difficult here to ignore the experiential part of consciousness and consider information-processing only.\nIf we say that an animal, or an AI, is conscious, it seems to almost necessarily mean a conscious experience: we\nwouldn’t even know what it means to say that an animal is conscious if it does not have conscious experience.\nSo, in a sense, the question is almost necessarily about the computational function of human conscious expe-\nrience, and whether it can be explained by evolutionary arguments. I will next review a number of proposals.2\nInvestigating wandering thoughts actually leads us close to consciousness, because “thinking” is often con-\nsidered the hallmark of consciousness. More precisely, the fact that we can reconstruct a vivid image about past\nor future events in our minds, while ignoring the present sensory input, is a remarkable property that seems\nto be closely related to consciousness. Some investigators actually propose that one of the main functions\nof consciousness is such simulation, which is also called virtual reality. That is, consciousness allows us to\n2For discussions on the consciousness in general and the computational function in particular, see e.g. (Baars, 1997; Chalmers, 1996;\nSeth, 2009; Van Gulick, 2021; Dehaene and Changeux, 2011; Lau, 2022).\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n152\nconsider different scenarios of what might happen in the future, and what would be the right things to do in\nthose circumstances. Planning crucially needs the capacity to simulate the results of future actions, and indeed\nin the case of wandering thoughts, we already talked about simulation. Such simulation obviously would be\nuseful for survival and reproduction, and thus favored by evolution. A special case of such simulation is dream-\ning, which creates a virtual reality that is particularly far removed from current reality. Dreaming often includes\nsimulation of threatening situations, i.e. situations in which it is important to know what to do to avoid harmful\nconsequences.3\nHowever, I think we should not too easily conflate consciousness with thinking or simulation. What we see\nis a correlation between a certain brain function (namely, simulation) and consciousness, but it is difficult to\nsay whether consciousness is really essential for such a function. As we saw in Chapter 11, modern AI uses\nplanning and even something like wandering thoughts, simulating events that happened in the past or might\nhappen in the future. Yet, nobody seems to claim that replay or planning would make a computer conscious.\nSuch a claim seems absurd to most experts because claiming that a computer is conscious is usually inter-\npreted as having phenomenal conscious experiences; but they are very unlikely to be produced by such simple\ncomputations as replay and planning.\nAnother possible function of consciousness is choosing actions. We typically have the feeling that we con-\nsciously decide what we are going to do, an experience of free will. You may think that you decided to read this\nbook; perhaps you decided to read this particular sentence. But did you actually decide how you move your\neyes from one word to another? What do we actually decide on a conscious level? As we saw in Chapter 13,\nconsciousness may not have any role in the control of actions; the feeling of free will and control may be de-\nceiving. It may very well be that actions are entirely decided by unconscious processes. After all, that seems\nto be the case with many animals (if we assume most of them don’t have consciousness), as well as any robots\nand AI that exist at the moment.4\nYet another proposal is that consciousness could be useful for social interaction and communication.5 The\ncontents of consciousness can usually be communicated; in fact, in psychological experiments, one opera-\ntional definition of the consciousness of perception is that you can report the perception verbally to the exper-\nimenter. The utility of conscious perception, in particular, would be that this perception can be transformed\ninto a verbal form, and communicated to others. Again, the problem is that it is perfectly possible to build AI\nand robots which communicate with each other without anything we would call consciousness, at least in the\nexperiential sense.\nA particularly relevant proposal for this book is that consciousness facilitates communication between dif-\nferent brain areas.6 While unconscious processing has a huge capacity for information-processing, it suffers\nfrom the problem that the processing is divided into different brain areas whose capacity for communication\nis limited—as typical of parallel distributed processing. The idea here is that consciousness is the opposite: it\nhas very limited processing capacity, but its contents are broadcast all over the brain. Consciousness can thus\nbe considered a “global workspace”. It could be compared to a notice board where you can put short notes (lim-\n3(Revonsuo, 2000; Hesslow, 2002)\n4For a viewpoint in which consciousness is the “reason” for most actions, see Cleeremans and Tallon-Baudry (2021).\n5(Hommel, 2013; Frith, 2002, 2010)\n6(Baars, 2002, 1997; Dehaene and Naccache, 2001). Related to this are the “higher-order” theories in which consciousness depends\non higher-order mental representations that represent oneself as being in particular mental states (Lau and Rosenthal, 2011; Lau, 2022).\nFor example, a fear reaction would be coded in the amygdala while the feeling of fear would be coded in the prefrontal cortex. This is a\nform of metacognition which will be discussed in more detail in Chapter 17.\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n153\nited capacity), which will be seen by everybody in the office (global broadcasting). It is also a bit like a central\nexecutive in the society of mind discussed in Chapter 13—one that is not particularly smart but whose thunder-\ning voice is easily heard even at a great distance. This links clearly to the proposal in previous chapters, where\nwe considered a system where pain and other error signals, such as reward loss, are broadcast to the whole sys-\ntem. An intriguing possibility is that this could be why pain, whether physical or mental, must be conscious.\nPerhaps pain is so acutely conscious precisely because the broadcasting system it uses is inherently related to\nthe global workspace of consciousness. Yet, it is not clear to me why this would necessarily require conscious\nexperience, since distributed information processing is increasingly performed in computers as well.\nI have just described several proposals which each consider highly relevant information-processing prin-\nciples. For example, inside an AI, the communication between different processors or processes needs to be\nsolved, and mechanisms related to the global workspace theory can be very useful. Yet, in each of those cases,\nwe have to ask whether we would say an AI with such capacities is conscious. Would it necessarily have subjec-\ntive experience, if that is what we mean by “conscious”? We could go through all the computational functions\nof the preceding chapters and ask whether consciousness necessarily has any role in any of them. To the ex-\ntent that all of these are simply computations that can be implemented in an AI, they may actually not need\nconsciousness. (I’m here assuming that any computer we have at the moment is not phenomenally conscious,\nwhich is relatively uncontroversial.) Therefore, I think there is currently little reason to believe that conscious-\nness would be necessary for some particular kinds of computation, which would be impossible without con-\nsciousness.\nConsciousness as a specific hardware implementation?\nHowever, another viewpoint is possible: there may be some forms of information processing that are correlated\nwith consciousness. It could be that some of the computational routines in the brain are always implemented\nusing some special circuits or processes that give rise to consciousness. Such computations would then give\nrise to consciousness, even if in theory, it would be possible to implement them in non-conscious circuits. If\nwe program that same kind of computation in an AI, we might then say that we have programmed the AI to\nperform “conscious” information processing. However, it may be best to use scare quotes here: the AI may be\nimitating processing that is conscious in the brain, but it might not have conscious experience, so whether we\nshould call such computations “conscious” is questionable.7\nTherefore, any argument—such as I have just made— saying that a certain computational function cannot\nbe the actual function of consciousness because it can easily be programmed in an AI, may be missing the\npoint. While it may not be completely necessary to have consciousness for, say, simulation, it could still be\nthat in biological organisms, shaped by evolution, consciousness is somehow an important part of the com-\nputational implementation of simulation, or any of the other functions above. The fact that something is easy\nto program in a computer, which is based on completely different kind of hardware, does not mean that it\nmight not be very difficult to implement in the brain without the help of some, hitherto unexplained, conscious\n7The well-known distinction between“access” and “phenomenal” consciousness (Block, 1995; Kouider et al., 2010) is related to this\npoint. In this book, when I talk about consciousness, I mean phenomenal consciousness, i.e. the experiential kind of consciousness,\nunless otherwise mentioned (or in quotation marks). Access consciousness is, in my view, an operational definition of consciousness,\nused in experimental neuroscience: If you ask a person whether she is conscious about X, and they reply yes, then the person is\nconscious of X in the sense of having access to the experience or perception of X. I find this definition of access consciousness not very\nrelevant for the present discussion.\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n154\nmechanisms. Thus, consciousness might be a particular “hardware” implementation of certain computations\nthat are otherwise difficult to perform in the brain—whether simulation, global workspace, or else.\nYet, this is all mere speculation. We cannot exclude the completely diametrically opposed possibility, which\nis that consciousness is not actually part of the information processing at all. Perhaps it does not affect the\ncomputations or the contents of the mind in any way; it simply reflects the results of the information process-\ning. It might not even have any evolutionary utility.8 Many thinkers over the centuries have proposed that\nconsciousness in humans is only the tip of the iceberg, and most mental activities—which I call information\nprocessing—happen without consciousness.9 But here, we find an even more startling possibility: perhaps\nconsciousness is not even the tip of the iceberg but, to push the metaphor further, a bird that flies over the\niceberg, only watching it from a distance. We will see even more startling possibilities later in this chapter.\nThe origin of conscious experience\nNext, let us consider the problem of the existence of conscious experience itself. Most scientists would agree\non the fundamental importance of understanding the physical, chemical, and biological processes that enable\nconscious experience. While it is one of the deepest questions in science, I am, again, afraid there is little we\ncan say about it with any certainty. It is not even clear if the whole question can be approached scientifically.\nThis is because it is difficult to make any rigorous experiments on experience, due to its subjective nature.\nOnly I observe my conscious experience; you, or any neuroscientist, cannot really know what I experience.\nSo, how could a neuroscientist conduct experiments on people’s experiences? Measuring brain activity, or look-\ning at people’s behavior are not measuring experience. Brain activity and behavior are related to and correlated\nwith experience, but not the same thing. The closest you can get is asking people what they experience. How-\never, they might not be able to express it verbally with sufficient accuracy or detail. In fact, if participants in\nexperiments answer such questions, they are ultimately engaged in behavior (in the form of speaking), and, in\na sense, the neuroscientist is actually only measuring their behavior (speech, in this case).\nWith good reason, the problem of understanding how and why the brain creates conscious experience–\nincluding whether it is actually the brain that does that— is called the hard problem of consciousness re-\nsearch.10 However, let us not despair: Even if any solution may not be available, some interesting things can be\nsaid about the problem.\nTo begin with, some neuroscientists think there is something special in humans that enables conscious-\nness, and perhaps in some other mammals such as great apes as well. What it would be, nobody really knows.\nThe main theories are based on observing what kind of structures human brains have, and what simpler ani-\n8Claiming that consciousness has no evolutionary function could be seen as a special case or implication of the stance called Epiphe-\nnomenalism (Walter, 2009), according to which consciousness has no causal effect on physical events. What I have written here is\nprobably also compatible with the sophisticated alternative given by Chalmers (1996), who also addresses the obvious question of why\nthere might be consciousness if it is not evolutionarily advantageous (his Section 3.6). Chalmers’s arguments rely heavily on consid-\nering what a zombie without any consciousness would be like compared to humans; I think his zombies are comparable to AI, which\nI assume is not conscious. Chalmers (1996) seems to agree on the difficulty of understanding consciousness: “[W]hen it comes to\nconsciousness, it seems that all the alternatives [of philosophical stances] are bad. If someone comes away with the feeling that con-\nsciousness is simply an utter mystery, then that is not completely unreasonable.” (I am not committing myself here to any particular\nphilosophical stance, but just exploring different possibilities.)\n9See footnote 5 in Chapter 8 for some historical remarks.\n10(Chalmers, 1995, 1996)\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n155\nmals like cats and dogs do not have. Because the brains of cats and dogs are in many ways very similar to the\nhuman brain, the relatively small differences might be related to consciousness.\nOne difference between the brains of humans and “lower” animals seems to be the existence of a special\nclass of neurons, called von Economo neurons. They have particularly many long-range connections to other\nneurons. Since long-range connections might be related to something like a global workspace, von Economo\nneurons have often been considered as a potential candidate for a mechanism generating consciousness.11 In\nfact, it has also been suggested that the brain basis of consciousness might be related to feedback between\nbrain areas, as opposed to any special kind of processing inside each single area.12 It could be that the long\nconnections of von Economo neurons make such feedback stronger, sufficiently complex, or otherwise more\nconducive to consciousness. Interestingly, apes have von Economo neurons as well, and so do elephants, dol-\nphins, and even some monkeys, so based on this criterion, those animals at least should be conscious.13\nArguably, we can use AI for studying the hard problem of consciousness. In particular, we can perform\nthought experiments based on the same kind of comparisons as was just done with other animals. Starting\nfrom the assumption that AI is not conscious, part of the hard problem of consciousness is then to explain\nwhat creates this fundamental difference between humans and AI.\nHow can we know something is conscious\nHowever, there is a problem with the argumentation above: It is based on finding animal species, perhaps such\nas cats and dogs, which are reasonably intelligent but have no conscious experience. Or, if we consider AI,\nit is based on assuming that AI is not conscious. But how can we even know if an animal species or an AI is\nconscious or not?\nSome would claim that we cannot even know if other people are conscious. We do tend to assume that\nevery human we meet is conscious, but this is just a guess, really, without much logical basis. We are actually\ngeneralizing based on ourselves: the only human I know for sure to be conscious is myself. Others just move\naround and say things, but they could be some kind of robots for all I know; perhaps I am the only person\nconscious in the world. If I assume all other humans are conscious as well, I can only hope I’m not overgener-\nalizing! This is, somewhat cheekily, called the zombie problem: it could very well be that some of the people\nyou meet are “zombies”, that is, creatures that look like humans and behave like humans, but do not possess\nany kind of consciousness.\nLeaving such wild speculation aside, we do have a real scientific problem here. In neuroscience, it has\nbeen found extremely difficult to determine which animal species are conscious and which are not.14 Even\n11(Critchley and Seth, 2012; Butti et al., 2013)\n12(Lamme and Roelfsema, 2000; Crick and Koch, 2003)\n13It is not very clear which animals have von Economo neurons and which don’t; Jacob et al. (2021) have recently claimed to find\nthem even in raccoons.\n14(Seth et al., 2005; LeDoux et al., 2023; Bayne et al., 2024). Neuroscientists have developed an interesting test called Mirror Self-\nRecognition (Toda and Platt, 2015; Loth et al., 2022). The idea is that a mirror is introduced to the animal. After the animal has had\nsome experience with the mirror, some red dye is applied to its face to create a small but visible spot. Many animals instinctively try\nto touch the red spot. But does the animal touch the real spot on its face, or its image in the mirror? If it touches the real spot, it is\nconcluded that the animal has some kind of consciousness of itself, or at least a body image similar to what we have. Chimpanzees,\nfor example, pass the test. However, this is of course a very indirect measure of only one aspect of consciousness, in particular self-\nawareness considered later in the text. (For moral implications of our ignorance of whether animals can suffer on a conscious level,\nsee Birch (2017).)\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n156\nconsidering humans, it is not easy to tell whether people in coma are conscious. For coma patients incapable of\nsaying anything or making any motor responses, measuring brain activity provides the last resort for assessing\ntheir consciousness. Surprisingly, it has been found that patients who were thought to be in a completely\nunconscious, vegetative state are sometimes perfectly conscious: They can respond to questions like healthy\nhumans would when they are given the opportunity to communicate with the external world by special devices,\nwhich transform brain activity to text.15 So, it is actually true that we cannot always tell if even other humans\nare conscious.\nHow could we then judge whether an AI is conscious or not? What if current AI is conscious, or will become\nconscious in the near future? You can find people arguing strongly for the possibility of conscious AI. Some say\nit is simply a question of complexity: when AI becomes complex enough, it will become conscious; the only\nreason why present computers are not conscious is that they are too simple in terms of their computation,\nin particular lacking sufficient interaction and information interchange between different processing units.\nOthers think an AI must have a body, i.e., it must be a robot, in order to be conscious, and consciousness is\nsomehow created in the interaction with the world.16\nFundamentally, the question of determining consciousness seems to be unsolvable because of its subjec-\ntive nature: I can only know something about my own consciousness. We cannot know for sure if any animal or\nAI is conscious or not. Consciousness—at least regarding its experiential quality—remains a huge mystery.17\nWhy is simulated suffering conscious?\nLet us get back to the question of suffering. Consciousness is in some sense crucial to suffering: if we were not\nconscious of our suffering, if we didn’t have the conscious experience of suffering, it would not be the same\nkind of suffering at all. Suppose you have a headache but you start watching a really fascinating movie; you\nmay cease to notice the pain at all. Somewhere in your brain there is probably some kind of activity that would\nusually lead to the experience of pain, but your attention is in the movie, so you completely ignore the pain.\nThat is because when you are not paying attention to something, you cannot be conscious of it either.18 So,\nin some sense, the whole problem of suffering revolves around the question of consciousness. If we consider\na simple animal or an AI and agree it is not conscious, is it actually meaningful to say it suffers—as I seem to\nhave done in this book?19\n15(Monti et al., 2010; Bruno et al., 2011)\n16For reviews, see (Reggia, 2013; McDermott, 2007; Chella and Manzotti, 2007); on complexity, (Tononi and Edelman, 1998), and\nembodiment, (Ziemke, 2007)\n17However, I don’t mean to be completely pessimistic about the possibility of doing scientific research on consciousness. If ver-\nbal reports (or similar information) of conscious content are combined with brain imaging in a sufficiently large number of human\nsubjects—possibly specifically trained to perform introspection—progress can be made. The specific methodology needed is dis-\ncussed by Lutz and Thompson (2003); Gallagher (2003).\n18The connection between attention and consciousness is complex, but it is usually assumed that we can only be conscious of some-\nthing we attend to. De Brigard and Prinz (2010) review evidence for and against this assumption.\n19This problem could be contrasted with the problem of whether a computer can see. Suppose a robot moves around in its envi-\nronment, avoiding obstacles and performing some task thanks to input from its camera. Now, how would you answer the question of\nwhether the robot is able to “see”? Most people, including scientists working on such computers, would casually say that the computer\nsees, for example, it sees the obstacles. If pressed hard on what that means, they would probably admit that the computer “does not\nreally see”, presumably because there is no consciousness involved. What is very interesting is that this ambiguity is not usually con-\nsidered a problem: it is rare that any serious debates are conducted on whether such a robot actually “sees” or not. When we talk about\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n157\nThe other day I was watching a fictional TV series in which a tiger attacked a woman. I felt scared. Was there\nany point in being scared? I was in my own home, just watching an electronic device produce some patterns of\nlight on its screen. There was no real tiger nearby, no real risk of being eaten. Even if I had been in the middle of\nthe action, it would have been on a film set. The tiger was tame; or perhaps it was just a computer animation,\nand there was no real tiger at all. In any case, even if I had been at the studio instead of home, I would not have\nbeen in any kind of physical danger. What is even more interesting is that after having watched that on TV, my\nbrain started replaying the events. Several times during that evening, I saw the tiger in my wandering thoughts.\nEvery time, some element of fear crept into my mind. I thought: How stupid can my brain be? Why do I feel\nfear although there is no real tiger nearby, there was never any real danger of anybody being eaten by a tiger,\nand finally, I haven’t even seen the image of a tiger for hours, it’s just repeating in my head.\nThis is yet another amazing thing about conscious simulation: It reproduces the same valences, that is,\nthe positive and negative feeling tones, and the same experience of suffering, as the real thing. When I think\nabout something unpleasant, it hurts. Maybe not quite as much as the real thing, but still it hurts. The theories\nexplained in previous chapters actually explain, to some extent, why the brain does that. It is not stupid to\nreplay experiences. Replay and other wandering thoughts are important for learning a good model of what the\nworld is like and what kind of actions are useful in which situations, as we saw in Chapter 11.\nYet, my current accusation of my brain being stupid is on a different level than the theories of the previous\nchapters. Here, I’m talking about consciousness. Why am I consciously afraid of the tiger and consciously\nsuffering during the replay? Why do I need to experience suffering while the brain is just performing some\nsimple computations that we can easily program in a computer? To put that more precisely, why do I need to\nexperience a negative valence on a conscious level while doing the replay? Couldn’t the brain just do the replay\nsomehow quietly on an unconscious level without disturbing my conscious feelings and conscious thinking?\nI’m not just repeating the question posed at the end of Chapter 11, which was: Why do wandering thoughts\ntrigger feelings (possibly quite unconscious) of pain and pleasure? Here, I ask a more general question about\nconsciousness and suffering: Why are such simulations, and the ensuing suffering, conscious?\nAgain, we might assume that perhaps evolution just made a simple computational shortcut. If something\ndangerous is perceived in the outer world, the systems for threats and fear will be activated on every level,\nunconscious as well as conscious; apparently, this functionality is activated even with simulations, but the\nquestion is why. One reason might be that conscious fear is important in information processing because of\nits capability for broadcasting, as in the global workspace hypothesis: this would justify why conscious fear\nhas to be activated in simulations to properly compute things. On the other hand, even if consciousness were\nnot necessary for any computations, it would still be evolutionarily pointless to somehow explicitly switch\nconsciousness off when doing replay. It would be nice indeed if, when something dangerous comes up in a\nwandering thought, the fear system would be activated only partly, not on the conscious level, perhaps only in\nsome distant corner of the unconscious processing systems. This would be nice, but would evolution have any\nreason to do us such a favor?\nWe should recall again that evolution does not care at all about whether we feel good or bad. It tries to\noptimize computation in order to maximize the spreading of the genes, and this has to be done with limited\ncomputational resources. Allowing us to switch off conscious suffering when engaging in replay would pre-\nsuffering in an AI, the situation is, in principle, quite similar. However, much more heated debates can be expected on the question of\nwhether the AI actually suffers. This lack of clarity on whether an AI can suffer seems to be much more difficult to accept than in the\ncase of seeing.\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n158\nsumably be pointless from the viewpoint of optimizing computation. So, evolution just makes us suffer from\nreplay since that is optimal use of finite computational resources. Such optimization of computation may ac-\ntually increase our chances of survival a bit, and give us a longer life. Full of suffering, though.\nSelf vs. consciousness\nSo far, I have been mainly considering consciousness on the sensory level, as in “consciousness of the text you\nare seeing”. Another very different thing that we can be conscious of is our own self. It can even be argued\nthat if there is any consciousness at all, there must necessarily be self-consciousness, or self-awareness, that\nis, conscious experience related to oneself. It can be seen as a particularly automatic and primitive form of\nconsciousness. So, we find yet another meaning for the term “self”—in addition to those in Chapters 6 and\n13— defined as precisely this self-awareness. This corresponds very well with our intuition, where it is my\nconscious feeling of being “me” that defines what “I” am, or what my “self” is.20\nThis aspect of self-consciousness is very different from the way “self” was treated in previous chapters. The\naspects of self treated in earlier chapters do not necessarily have anything to do with consciousness: all the\noperations described earlier are just computations. In particular, an AI does not need to be conscious to infer\nthat it can control certain things and not others, or to develop behavioral mechanisms that ensure its survival,\nwhile even a simple AI system can and should have methods for evaluating the performance of “itself”.\nIf self is defined in this sense of self-awareness, it might in fact be difficult to defend any form of “no-self”\nphilosophy, of which we have seen one version in Chapter 13. Descartes famously was absolutely certain that\nhe could say “I am” because he “thinks”:21\n[A]fter having reflected well and carefully examined all things, we must come to the definite con-\nclusion that this proposition: I am, I exist, is necessarily true each time that I pronounce it, or that\nI mentally conceive it.\nYet, Descartes was quite wary of saying what he actually is:\nI must be careful to see that I do not imprudently take some other object in place of myself, and\nthus that I do not go astray in respect of this knowledge that I hold to be the most certain and most\nevident of all that I have formerly learned.\nThe complexities of no-self philosophy largely come from the tension between these two viewpoints: It is in-\ntuitively clear that I am, but it is not clear what I am. (There can hardly be any difference between the “I” and\nthe “self”, they are just two words for the same thing.22)\nOn the other hand, some would say that such self-awareness can be seen as a mental construction, even an\nillusion, just like control and free will. Our self-awareness could be based on a collection of the awarenesses of\nvarious sensory perceptions, with no special core that could be called “me”, or awareness of myself. Hume ex-\npresses this potently in a famous quote which is not unlike anything Buddhist philosophers might have said:23\n20(Dennett, 1992; Gallagher and Shear, 1999; Gallagher, 2000; Smith, 2017).\n21Meditations on First Philosophy, Chapter I, translated by Elizabeth S. Haldane.\n22(Smith, 2017)\n23Hume, A Treatise of Human Nature, Section 1.4.6\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n159\nFor my part, when I enter most intimately into what I call myself, I always stumble on some partic-\nular perception or other, of heat or cold, light or shade, love or hatred, pain or pleasure. I never can\ncatch myself at any time without a perception, and never can observe any thing but the percep-\ntion. When my perceptions are removed for any time, as by sound sleep; so long am I insensible of\nmyself, and may truly be said not to exist.\nThis suggests a no-self philosophy where self-awareness is nothing but a complex of various instances of sen-\nsory awareness, mistakenly leading to an illusory perception of a separate entity called “self”.24 In the absence\nof any perceptions, ultimately, I may be said not to exist. Such no-self philosophy could be called ontological:\nit claims that the self does not exist at all. It does not merely say that self is not what it looks like, or that it\nis missing something, or that it is not too important; instead, it claims that self does not exist, period. While\nHume may not have meant to go quite that far, many Buddhist philosophers do.25\nNothing is real?\nSaying that the self is a mental construction, possibly an illusion, sounds quite radical. Well, how about going a\nbit further, and denying that anything really exists? While it is undeniable that there is some kind of experience\nof the world outside of myself, it is equally undeniable that this experience is not the same thing as the world\noutside. The conscious experience is—according to a conventional neuroscientific view—the product of com-\nplex information-processing of incoming signals. Actually, most of conscious experience has little to do with\nthe world that surrounds us here and now, since conscious contents are often a product of planning, replay,\nand other kinds of thinking and imagination. The interesting thing is how people are misled into believing that\nthis experience, this virtual reality, this simulation, replay, or planning, is actually the reality.\nIt should be easy to admit that when we plan the future, the planned events are just imagined, and not\nreal. But the “unreality” of consciousness goes deeper than that: In fact, everything in our consciousness is\na simulation, a virtual reality, constructed by our mind. This also includes your consciousness of everything\nyou see, hear, feel, taste, and smell at this very moment. Any perceptual experience, as well as any thought, is\n24In addition to the obvious sensory modalities, it is important here to consider proprioception (perception of the body position,\nincluding body ownership, Tsakiris et al. (2007); Seth and Tsakiris (2018); De Freitas et al. (2023)) and interoception (sense of the internal\nstate of the body, in particular internal organs, Craig (2009)). Further related phenomena include meta-awareness (discussed later in\nChapter 16) and autobiographical memory (Prebble et al., 2013).\n25While the no-self philosophy is widely associated with Buddhism, different Buddhist schools actually approach it in very different,\neven contradictory ways, and several interpretations exist. In fact, the philosophy of no-self has perhaps as many facets as the very\nconcept of “self”. We already saw the interpretation of no-self as lack of control in Chapter 13. Another approach is to see “no-self” as\na suggestion not to worry about self-evaluation or self-preservation, which was the interpretation of “self” in Chapter 6; rumination\nmay not be possible without some concept of self to which the bad things are happening. In this latter sense, it may not be so much a\n“truth” describing the world, but rather a useful way of thinking, as we will see in Chapters 16 and 17. The (ontological) interpretation\nwe have in this quote by Hume is yet another approach, probably the most well-known in Buddhist philosophy. At the risk of greatly\noversimplifying this complex issue, I would venture to say that the Theravadan school is more in line with Hume here; Theravada con-\nsiders self as an illusion, as something that does not exist. In contrast, Mahayana schools, with the possible exception of Madhyamaka,\nemphasize the primacy of consciousness, like Descartes, and do not deny the existence of self—although they do point out that our or-\ndinary conception of self is mistaken in various ways. See Verhaeghen (2017) for a short, readable overview emphasizing some practical\nimplications of such a philosophy; Vago and David (2012) emphasize how mindfulness works largely through self-related mechanisms.\nHarvey (2009) and Williams (2008b) give book-length expositions of the philosophy.\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n160\nsimulation, or computation, and not the same as reality.26\nThis is just a rephrasing of well-known neuroscientific facts. As we have discussed several times by now,\nwhen you look at this text, your brain is doing complex computations based on the incoming information.\nBased on the results of those computations, it creates a conscious perception, which contains an image or a\nfeeling of the world around you, including the book or the computer screen on which you see this text. The\nconscious experience is created by some quite quasi-miraculous mechanism, which science has not yet been\nable to explain—even saying that it has to be in the brain is speculative. But the important point is that what\nyou see is the virtual reality, or the simulation in the brain, not the real world. The distinction between the\nworld and your conscious experience is basically inherent in the very notion of “experience”. Although I have\nalready said this in the beginning of this chapter, this point requires a longer explanation, so let me try.\nUsually, you would say that you “see” this book (let’s just assume for the sake of simplicity that you are\nreading this text in a book). However, according to the conventional neuroscience viewpoint, what you’re ac-\ntually conscious of is the interpretation created by your brain, not the book itself. The book simply reflects\nsome photons emitted by a lamp or the sun, these photons enter your eye, and your eye sends electrical signals\nto your brain. Based on these electrical signals combined with the prior information about the world, your\nbrain creates a virtual reality, including your perception of this book. Meanwhile, based on other sensory infor-\nmation, and again all kinds of internal information and processing, the brain creates your perception of your\nsurroundings, your body, and indeed, your perception of your self.\nI am not denying here that the book exists. I am merely pointing out that your consciousness, your sensory\nawareness of this book and everything else is created by your mind, presumably by some highly complicated\nprocess in your brain. You cannot really “see this book”, you cannot be “conscious (or aware) of the book”, you\nare only aware of the results of some computations performed in your brain, in which the book only plays the\nrole of being the physical source of some radiation which was input to the computations.\nThe metaphor of virtual reality means that consciousness is similar to wearing virtual reality goggles which\nfeed an input to your eyes which is so realistic that it looks almost like real. In the case of seeing this book,\nthough, it looks exactly like real to you because you know nothing better: You have never seen anything which\nwould be somehow closer to reality than this virtual reality. A number of science fiction movies are based on the\nidea that somebody could feed fake sensory information directly to your brain, and you would have no idea the\nsensory input is fake. Descartes already proposed that he cannot trust his perception because an “evil demon”\nmight be feeding an illusory external world to his mind—which is precisely why he could only be certain of his\nown existence. Such claims lead to an extreme form of uncertainty regarding perception.27\nMy point is that something like that is actually happening to you all the time, according to perfectly main-\nstream neuroscience. I want to emphasize that I’m not trying to make some radical philosophical point here.\nThere are others that will tell you that the world does not really exist, including proponents of some East-\nern philosophical systems, such as Advaita Vedanta, or Mahayana schools of Buddhism, including Zen and\nYog¯ac¯ara.28I’m trying to steer away from such philosophical speculations about what exists, and merely point\n26Note that the discussion regarding simulation in this chapter has nothing to do with the idea that we would be living in a simulation\nprogrammed by some other race, sometimes called the “simulation argument” (Bostrom, 2003).\n27Descartes’s Meditations on First Philosophy. See McKinsey (2018) for a modern discussion based on Putnam’s brain-in-a-vat sce-\nnario.\n28For example, Williams (2008b, p. 94) describes the Yog¯ac¯ara viewpoint by Vasubandhu as “Apparently external objects are con-\nstituted by consciousness and do not exist apart from it. (...) There is only a flow of perceptions.” Claiming that the world does not\nreally exist is a form of ontological idealism, while claiming that we cannot possibly know for sure if the world exists is epistemological\n\nCHAPTER 14. CONSCIOUSNESS AS THE ULTIMATE ILLUSION\n161\nout some of the limitations of our perceptual and cognitive systems, in a way which is, I hope, acceptable, even\nif unpalatable, to most scientists working on those topics.\nAt the risk of repeating myself: Most neuroscientists would agree that sensory processing in the brain is\nproducing an interpretation of the incoming input; they would further agree that the brain creates conscious-\nness. Thus, the contents of consciousness are not a direct product of the world, let alone the same thing as\nthe world; it is a construction, an interpretation created by the brain. Yet, we often have the intuitive feeling\nthat the contents of consciousness are somehow identical to the contents of the outside world, which is not the\ncase. Just studying an introductory course in neuroscience or in AI might be enough for many people to give\nup such an idea. Visual illusions, such as in Fig. 12.4 on page 130 are one way of demonstrating how perception\nis different from reality.\nThe Belgian artist René Magritte has a famous painting called La Trahison des images, or “The Treachery\nof Images”.29 The painting consists of a picture of a pipe, with the text “Ceci n’est pas une pipe”, or “This is\nnot a pipe”, written underneath it. The point is that the painting is just a picture, not the real pipe. While the\nartist’s purpose was to illustrate the deceiving nature of images, the painting illustrates the illusory nature of\nconsciousness as well. Suppose you actually hold the pipe in your hand and look at it. What appears in your\nconsciousness is a picture, a simulation, or a reflection of the pipe; it is not a pipe. Yet, we have the habit of\nthinking that the perceptual image is the real pipe, while in reality, it is only somehow indirectly related to the\nreal pipe. Furthermore, the category of a “pipe” is just a mental construct. In this sense, perception is not the\nreal thing; consciousness is not the reality.30\nThese philosophical points are not simply theoretical speculation; our attitude to consciousness has a di-\nrect effect on suffering. Consider the example of the tiger I saw on TV: If I could somehow develop a different\nattitude towards the contents of my consciousness, seeing them as mere simulation, I might suffer less. This is\nprecisely why some Buddhist schools claim that the outside world only exists in your imagination—or at least\nthey recommend adopting such an attitude towards the world.31 In the next chapters, we will consider this and\nmany other ways of reducing suffering by changing our thinking patterns as well as using meditation.\nidealism (Guyer and Horstmann, 2018). In Mahayana Buddhist philosophy and, especially, its Western commentary, there has been a\nlot of debate on which form to support. For example, Lusthaus (2013) warns about misunderstanding the Yog¯ac¯ara literature to con-\nsistute an ontological statement while it is actually intended to be epistemological only. (For my part, I’m not committing to any such\nphilosophical viewpoint here.) Even in early Buddhist texts you find claims related to such idealism: “And what, bhikkhus, is the all?\nThe eye and forms, the ear and sounds, the nose and odours, the tongue and tastes, the body and tactile objects, the mind and mental\nphenomena. This is called the all.” (Samyutta Nikaya 35.23); however, this formulation open to interpretation and may also be seen as\nadmitting the (ontological) existence of outside objects.\n29See e.g. https://en.wikipedia.org/wiki/File:MagrittePipe.jpg which cannot be reproduced here for copyright reasons.\n30In this chapter, I have taken the viewpoint of physical materialism by assuming that the pipe actually exists and that our conscious-\nness is created by the brain. If we reject one of these assumptions or both, the conclusions will of course be even more radical.\n31Any Buddhist claims about the inexistence of the outer world could, in fact, be seen as clever devices only intended to help with\nmeditation and other practices (Schroeder, 2004). The Theravadan master Ajahn Chah seems to have this intention when he says “If\nyou think things are real there is suffering and there is fear. You are afraid of the different ways things may turn out. (...) There is\nthinking, then fear follows immediately. It deceives you, creating a picture to mislead you. (...) As to what is actually happening, there\nis nothing” (Chah, 2001). See footnote 28 above for discussion on the philosophical claims concerned, and page 182 for a related quote\nfrom Seneca.\n\nPart III\nLiberation from suffering\nThe final part will describe methods for reducing suffering,\nlargely drawing from philosophical traditions such as Buddhism and Stoicism,\nwhile showing how they logically follow from the science of Part I and Part II\n162\n\nChapter 15\nOverview of the causes and mechanisms\nIn this final part of the book, we move to the question of how to reduce suffering or, ultimately, how to be\nliberated from it. Applying the scientific theories of the previous chapters, we devise various methods to that\nend in the following two chapters. But first, in this chapter, I will recapitulate the basic theory. I will use\ntwo flowcharts to illustrate the basic mechanisms of suffering: the first one emphasizes adverse properties\nof the world and general cognitive design principles, while the second flowchart focuses on the dynamics of\nmoment-to-moment cognition. The flowcharts also make some explicit connections to the basic concepts of\nBuddhist philosophy. The connection of reward loss to the whole architecture of intelligence is then succinctly\nsummarized in one single “equation”, which directly suggests ways of reducing suffering.\nWhy there is (so much) suffering\nLet us start by summarizing the difficulty of information-processing in a complex world. The fundamental\nreasons of how suffering or mental pain is born are illustrated in Fig. 15.1, which I will next go through in\ndetail.\nRoot causes of suffering\nThe starting point is that the agent finds itself in a world which is highly complex. In such a world, acting\noptimally (in any reasonable sense of optimality) would require huge amounts of sufficiently detailed sensory\ninput, together with huge capacities of computation. Unfortunately, in the real world where we live in, an\nagent cannot have any of those. These three root causes of suffering—complexity of the world, insufficient data,\nand insufficient computation—are shown in the left-most column of the figure. Certainly, the agent’s limited\nphysical capabilities to act in the world and change it—catch any prey it wants, for example—create suffering\nas well, but here we focus on limitations related to information-processing because it can be modified more\neasily.1\n1It could be argued that lack of a good model, or “inductive bias”, is another limitation. Inductive bias can refer to slightly different\nthings: on the one hand, it is sometimes simply used as a fancy term for a Bayesian prior in a probabilistic model, but it can also refer\nto constraints that are more structural in the sense of, for example, the choice of the family of nonlinearities, regularization, or other\ncomputational structures used in a model (but these could still, in most cases, be seen as Bayesian priors in a hierarchical Bayesian\nmodel). Basically, what we are talking about here is that the agent might not have a good model family from which to pick its model of\nthe world, and might in particular “suffer” from overfitting (see footnote 5 in Chapter 4). I take here the viewpoint that, fundamentally,\n163\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n164\nInsufficient \ncompu-\ntation\nInsufficient\ndata\nComplexity\nof world\nUncertainty\n(unpredict-\nability)\nUncontrol-\nlability\nError \nsignals, \nfrustration\nSimulation,\nwandering \nthoughts\n Unsatis-\nfactoriness\n(insatiability, \n obsessions)\nInterrupts\nTreating \nsimulation \nas real\nProgrammed\nto maximize\nrewards \nSuffering\n(conscious \nfeeling)\nSelf-\nneeds\nThreat\nFigure 15.1: Recapitulation of the causes and mechanisms of suffering explained in earlier chapters. The boxes\nin magenta are intrinsic properties of the world—including the agent—while the boxes in blue are more con-\ncrete problems they pose for information-processing. The green boxes are possible functionalities in a highly\ndeveloped cognitive system, and the red boxes are the postulated system finally generating suffering.\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n165\nAt the same time, the system is typically programmed to try to relentlessly maximize rewards set by the\n“programmer”—which is evolution for humans, eventually complemented by some cultural processes. The\nrewards are not designed to make the agent happy, but to fulfill some objectives of the programmer, such as\nspreading your genes in the case of evolution. It is not possible for the agent itself to decide that it wants to\npursue some new goals, or to re-define its own rewards; nor can it decide that it has had enough rewards and\ndoes not need more. This is a fourth root cause of suffering, shown at the bottom of the left-most column of\nthe figure.\nThree fundamental problems in information-processing\nThe four root causes just mentioned lead to a number of challenges for information-processing, which I here\ncondense into three: uncertainty (including unpredictability), uncontrollability, and unsatisfactoriness (con-\nsisting of insatiability together with evolutionary obsessions).\nFirst, the overwhelming complexity of the world leads to uncertainty: the agent is not able to accurately un-\nderstand what happens in the world. It is not even able to accurately perceive most phenomena in the world.\nIt will try to divide the perceptual inputs into categories, but such categories are fuzzy, sometimes arbitrary,\nand categorization is often uncertain. One important special case is uncertainty about the future, also called\nunpredictability. (It is closely related to the Buddhist concept of impermanence, as will be discussed in Chap-\nter 16.) On the one hand, it is clear that it is difficult to predict the future if even the current state of the world\nis uncertain, which is a consequence of uncertain perception. However, unpredictability is actually a more\ngeneral phenomenon than the uncertainty of perception: even if the perceptions were perfectly accurate and\ncertain, it might not be possible to predict the future accurately due to the great complexity of the world. It\nmight be impossible to learn to model the world accurately enough, or using such models might require over-\nwhelming computational power. This is well-known in the natural sciences, where even extremely accurate\nmeasurements of a natural phenomenon do not necessarily mean you can predict it, because the prediction\nwould require overwhelmingly advanced scientific models.\nUncertainty and unpredictability necessarily lead to uncontrollability, lack of control of the world: if the\nagent does not know what is actually happening in the world, or it does not know how to predict what will\nhappen in the future, it cannot possibly control the world. In fact, control requires the capability to predict the\nresults of your actions, which requires not only a good model for prediction, but also an accurate perception\nof the current state of the world.2 However, uncontrollability is not only due to uncertainty. Even if the world\ncould be perfectly perceived and predicted, there would still be uncontrollability due to at least two reasons.\nFirst, the agent has limited physical capacities to influence the world. Second, limitations of computation re-\nduce controllability in many ways: the computational complexity of the search tree precludes finding perfect\nsolutions to the planning problem, while the parallel and distributed nature of the agent’s cognitive system\nhinders proper control of the agent’s internal functioning. All these reasons make sure that the world is uncon-\na good inductive bias is only necessary because the data is limited: if the data were infinite, the proper inductive bias could be learned\nfrom the data by testing the performance of the models on a new test set which was not used in the learning (Feinman and Lake, 2018).\nTherefore, I do not discuss inductive bias in any detail in this book, and subsume the problems due to lack of correct inductive bias\nunder the heading of “insufficient data”.\n2For those conversant in Buddhist philosophy, this is similar to the idea that impermanence feeds into no-self, when impermanence\nis seen as related to uncertainty and no-self is interpreted as uncontrollability (Mahasi, 1996). See footnote 3 below for more on such\nanalogues.\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n166\ntrollable for humans. But it can be rather uncontrollable even for a thermostat, since the temperature of most\nenvironments obeys extremely complex natural laws that are beyond the understanding of the thermostat, and\nerrors cannot be avoided.\nMeanwhile, the programming of the agent to maximize rewards means that the agent finds that no amount\nof rewards is enough: it is insatiable (page 61). In fact, the very raison d’être of the agent is to maximize the\nrewards set by its programmer or evolution; it will never be satisfied, and the desires will never be satiated. A\nrelated property is what I called evolutionary obsessions (page 61), which means that humans are compelled\nto seek various rewards which they might, if they thought about it rationally, prefer not to seek—such as un-\nhealthy food and excessive competition for status. Seeking those unnecessary rewards increases the chances\nfor frustration, thus leading to more suffering, and may even lead to less reward in the long run (by ruining\nyour health, for example). Yet, it is difficult for humans to change what they find rewarding. I group these\ntwo properties under the umbrella term unsatisfactoriness, expressing the general idea that even if the world\nwere completely known and controllable, there would still be suffering due to the fact that the system is never\nsatiated and strives at questionable goals.\nThe three fundamental problems of uncertainty, uncontrollability, and unsatisfactoriness are shown as\nblue boxes in Figure 15.1, the second column.3\nError signals and suffering\nBecause of uncontrollability and uncertainty, the agent’s information processing will incur errors. Often, things\ndo not go as planned or as expected; predictions have errors, and expected reward will not be obtained. Thus,\nthe system generates error signals. Such error signals are particularly frequent because the agent is never sat-\nisfied and is always looking for more reward. Error signalling is the central red box in the third column in the\nfigure. Threat is based on a prediction of large frustration with a sufficient probability, so it is fundamentally\nrelated to frustration and depicted as an annex to frustration as it were.\nOur fundamental hypothesis in this book is that such error signals are what produce the feeling, and ulti-\nmately the conscious experience, of suffering. The suffering due to error signals is especially strong if the error\nis frustration, i.e. the agent is trying to reach the goal (or a reward) but it fails. Thus, error signals finally lead\n3The three problems or challenges could be seen from two different viewpoints: either as properties of our natural world (at least\nif unsatisfactoriness is seen from a more general perspective) or as properties of information-processing in any sufficiently complex\nworld. Here I take the latter view; the three problems are problems of information processing. However, they are in fact created by\nthose properties of the natural world which are given in the magenta left-hand column in the figure. These three problems are a\nrough analogue of what is called the three characteristics of existence in early Buddhist philosophy: impermanence (anicca), no-self\n(anatt¯a), and unsatisfactoriness/suffering (dukkha). Impermanence is to some extent a special case of uncertainty, as will be discussed\nin detail in Chapter 16 on page 177. Uncontrollability is an important aspect of no-self philosophy (see page 138) and may have been\nits original meaning in the earliest layers of Buddhist philosophy. The Buddhist concept of dukkha has the broadest definition of them\nall, simply meaning “suffering” in one interpretation; thus our concepts of insatiability and evolutionary obsessions are only some of\nits aspects, as will be discussed in Chapter 16. We could have added another blue box depicting “emptiness”, a widely used concept\nin later Buddhist philosophy; a discussion on emptiness is postponed to Chapter 16, where it will be introduced as an umbrella term\nfor fuzziness, subjectivity, and contextuality, and related properties. Alternatively, we could have added another box giving distributed\nprocessing and possible lack of central executive (discussed in Chapter 13) as a necessary computational consequence of the root\ncauses on the left; now distributed processing is not explicitly mentioned in the graph, although several boxes are related to it. One\nmore possibility would have been to introduce nonstationarity (discussed later in footnote 9 in Chapter 16) as a root cause, but it can\nbe simply seen as a special case of the complexity of the world, even if a particularly important one.\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n167\ninto the red box of suffering, on the right-hand side in the figure.4\nOptional processes that increase suffering\nSeveral further processes may further be active, depending on how sophisticated the agent is. If its cognitive\narchitecture uses wandering thoughts, we get another box in the flowchart (top row, third column). It is a type\nof information processing that takes place only in highly sophisticated agents, which is indicated by drawing\nthe box in green. Such highly intelligent agents may engage in simulation of the world in terms of planning and\nreplay, which in humans often happens in the form of wandering thoughts. They increase error signalling by\nrepeating or anticipating experienced errors; this is depicted as the green simulation box on the top row feed-\ning into error signals. Since the goal of simulation and wandering thoughts is to gain more control and reduce\nuncertainty on the world by better learning its dynamics, there is an arrow from the uncontrollability and un-\ncertainty boxes to the simulation box. On the other hand, since wandering thoughts increase uncontrollability\nin their own way, that arrow is bidirectional.\nFurthermore, the agent may react in different ways towards the contents being replayed or simulated. If the\nsimulated contents are processed almost as if they were real, and the various frustrations in the simulations are\nprocessed in the same way as real frustrations, this will greatly increase suffering. Otherwise, simulated error\nsignals might not lead to suffering. This is indicated in the flowchart as the green box on the top right-hand\ncorner; it feeds into the connection between the simulation box and the error signal box, thus creating a causal\nconnection between simulation and error signalling as just described.\nA related design principle is using interrupts, which are useful for handling uncertainty due to unpre-\ndictability. Interrupts are seen as an essential aspect of emotions as well as desire in this book. Interrupts\ncreate more frustration since by interrupting on-going behavior, they increase uncontrollability; they also im-\npose new, possibly short-sighted goals on the agent, which can further increase to frustration. It is also possible\nthat interrupts produce specific error signals unrelated to frustration. Interrupts are depicted as another green\nbox on the bottom row, feeding into error signals.\nSufficiently developed agents have various intrinsic rewards, which may be frustrated as well. The very\nstrongest suffering actually tends to come from the frustration of self-related goals, such as survival or self-\nevaluation. These self-needs create new kinds of frustration and errors, such as the agent “not being good\nenough” in the sense of not obtaining enough rewards on a longer time scale. This is the box at the bottom\nright-hand corner, again in green since it is a sophisticated module, which the programmer may include in the\nsystem or not.\nThis flowchart explains the conditions leading to suffering on an abstract level but still, it clearly has prac-\ntical implications.5 If we consider it from the viewpoint of interventions, e.g. practices that would decrease\n4For a more detailed discussion on the connection between error signals and suffering, see footnote 7 in this chapter.\n5For future research, I would like to point out that many of these boxes can be quantified, although different measures are possible,\nand research is needed to decide which ones are useful. Uncertainty is typically quantified by Shannon entropy as defined in informa-\ntion theory and already used by, e.g., Hirsh et al. (2012); Friston (2010). The complexity of the world could be the number of states (or\nthe entropy over the typical distribution over them) in a category-based world model. The amount of data available can again be quan-\ntified by information theory, for example, by the Fisher information (possibly multiplied by the number of data points) measuring the\ninformation a dataset gives about the parameters of an ideal world model, i.e., how much information the data actually contains about\nthe world (alternatively, the mutual information between the data and the world states could be useful). Computational resources\ncan be quantified using flops per second or a similar measure. To quantify uncontrollability, related probabilistic computations are\npossible (Huys and Dayan, 2009); we might also be able to use various tools from control theory, e.g. (Liu et al., 2011).\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n168\nsuffering, it clearly points out that we could reduce suffering by reducing wandering thoughts, self-needs, and\nother processing in the green boxes. Such ideas will be considered in detail in the next chapters. The next\nchapters will also consider how to deal with the blue boxes (uncertainty, uncontrollability, unsatisfactoriness).\nIn the remainder of this chapter, however, we look at the process of suffering from two further viewpoints.\nCognitive dynamics leading to suffering\nOne complementary viewpoint on suffering is provided by cognitive dynamics, i.e. how the different cognitive\nprocesses work and influence each other in real time. In some sense, this is about zooming into the part of the\nmechanism in Figure 15.1 that leads from the blue boxes in the second column (uncertainty, uncontrollability,\nunsatisfactoriness) to errors and suffering. This reveals further quantities that can be intervened on to reduce\nsuffering.\nIn previous chapters, we have seen a number of steps in an information-processing procedure that trans-\nlates sensory input into action decisions and possibly suffering. Such steps are recapitulated and illustrated\nin our second flowchart in Figure 15.2. To begin with, sensory input is received from the external world; see\nthe upper left-hand corner of the flowchart. As the black arrow in the flowchart indicates, in the next step, the\nagent engages in initial sensory processing. This typically leads to recognition and categorization of objects in\nthe world, which in our basic formalism includes recognition of the state in which the agent is. (For better vi-\nsualization, the order of processing is now indicated by a single long blue arrow in the flowchart.) Recognition\nof the state is immediately followed by computation of the valences of the nearby objects or states. (Valence\nmeans here the prediction of the reward associated with an object, or more generally the prediction of the value\nof a state.) Based on those valences, a number of candidate goals are chosen, which is the process of desire in\nits hot, interrupting form. Next, the agent may choose one single goal and commit to it, which is also called\nintention. Then, the agent starts planning how to reach the committed goal, possibly by some kind of tree\nsearch, and executes the plan obtained.6\nAfter finishing the execution of the plan, the agent observes the outcome, and based on it, an error such as\nfrustration or reward loss is computed. Such an error may lead to suffering, in the precise sense of a conscious\nexperience. Finally, the computed error will be used in a learning process to guide future actions, which in\na sense closes the loop, as indicated by the green arrow. (Only an arrow from error computation to sensory\nprocessing is drawn in the figure for simplicity, but actually, the error is broadcast widely.) The flowchart shows\nthe prototypical sequence, but in reality there is, of course, more variability. 7\n6Some pointers and details on the steps 3-7 in the flowchart: Desire was defined as a process that suggests new goals in Chapter 3.\nSuch suggestions were seen to be possible by neural network computations in Chapter 8. In particular, such computations give fast\napproximations of valences, i.e. how rewarding near-by states are, preferably taking into account the whole future by approximating\nthe state-values. Valence computations were further linked to the generation of interrupting (intruding) desire in the framework of the\nelaborated-intrusion theory of desire in Chapter 10. After the selection of one of such suggested goals, planning proceeds as already\ndescribed in Chapter 3, including the idea of commitment or intention to the goal that has been selected among the possible high-\nvalue states. Note that in this framework, goals are set by the agent itself, based on predictions of rewards, which come from the\noutside world.\n7I hope the flowcharts in this chapter also clarify some conceptual differences regarding the concept of suffering, which may have\nbeen slightly confounded in previous chapters. First, I emphasize that I’m not saying that errors are suffering, but that errors are the\ndirect cause for suffering. Suffering is a complex phenomenon, comparable to emotions which also have multiple aspects, including in\nparticular a conscious experience. Conscious, subjective experience is obviously not the same thing as errors computed on the level of\nlargely unconscious information processing. Second, nor am I saying that suffering is driving learning: it is the errors that are driving\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n169\n4. Desire, \nselecting \ngoals \n3. Valence\n computation\n2. Object\n recognition,\ncategori-\nzation \n1. Initial\nsensory \nprocessing\n Limitations in \ninformation\nprocessing, \n“ignorance”\n8. Error \ncomputation,\nfrustration\n7. Execution \nof plan\n5. Intention,\ncommitment\nto a goal\n6. Planning,\ntree search\n0.The world:\nSensory input,\naction \nopportunities\n9. Suffering,\nMental pain\nLearning\nFigure 15.2: Recapitulation of the cognitive chain or cycle underlying suffering. The sensory input from the\nworld enters the system in the top left-hand corner of the chart (step #0). It is processed in a sequence of\nsteps #1-#7, along the big blue arrow. Throughout these steps, there are various limitations in the information-\nprocessing, or “ignorance”, to use a Buddhist term defined in the main text. Finally, the computation leads to\ncomputation of errors (step #8) which may lead to suffering in the top right-hand corner of the chart (step #9).\nThe error computations are further fed back to the whole system as indicated, for brevity, by the single green\narrow closing the cycle and going back to sensory processing.\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n170\nIn the middle of the flowchart, we have “limitations of information-processing”, which influences all the\nsteps in the processing. While that has been the main theme of the whole book, here it also provides an anal-\nogy to Buddhist philosophy, which uses the term ignorance in connection with similar conceptual schemes.\nIgnorance describes the fundamental underlying reason why the agent’s cognitive apparatus creates so much\nsuffering—thus adding to the adverse properties of the external world, which were shown in the previous\nflowchart. We have already seen various limitations in information processing that might be called ignorance,\nin the sense that they can be seen as imperfect or lacking, and they increase suffering. Thus, any limitations of\ninformation processing could be seen as one computational definition of “ignorance”.\nHowever, this Buddhist term could also be interpreted more literally in the sense of our being ignorant\nof the limitations of our information processing: a “meta-ignorance” so to say. We can thus consider such\nforms of meta-ignorance as 1) ignorance of the arbitrariness and even harmfulness of our rewards and goals,\n2) ignorance of the uncertainty and fuzziness of perceptions and concepts, 3) ignorance of uncontrollability,\nespecially of the mind itself, and even 4) ignorance that the simulation is not real—but this list is not meant to\nbe exhaustive.8 In all these cases, we can claim, inspired by Buddhist philosophers, that there is some kind of a\nmistake in our ordinary thinking and functioning of the brain—not merely a lack of computing power, say. One\nparticular mistake, or flaw, is about misunderstanding where suffering comes from and how it can be reduced.\nImportantly, in contrast to fundamental limitations of information processing, this “meta-ignorance” could be\ncorrected. Most of this book has actually been devoted to explaining what such ignorance consists of and how\nits different forms lead to suffering or amplify it. So, in Figure 15.2, ignorance is naturally placed in the very\nmiddle.9\nlearning, after some further sophisticated computations. Thus, the errors computed lead to suffering on the one hand, and, more\nindirectly, to learning on the other. This is why errors, suffering, and learning are separate items in the flowchart.\n8In early Buddhist philosophy, a central form of ignorance is the belief in “self”. I omitted it from this list because in this book, I have\nattempted to largely reduce such no-self ideas to less abstract concepts, in particular uncontrollability.\n9It is interesting to compare the mechanisms described above with a central idea in Buddhist philosophy: the twelve-link chain,\nwhich has served as a central inspiration for this flowchart. (A related approach is proposed by Grabovac et al. (2011).) The Buddha\nelaborated his idea of desire as the origin of suffering by building a sequential causal model, which can be seen as an instance of\nhis more general ideas on “causality” resumed under the heading of dependent co-arising/origination (Mahasi, 1999; An¯alayo, 2003).\nWhile different variants exist, I consider here the version with twelve items. Some of the items in his chain correspond clearly to\nconcepts we have seen in this book, while other do not. The chain begins by three items which are difficult to interpret, and seem to\nbe metaphysical speculation about how the ignorance of the true nature of reality creates consciousness and this creates the world. In\nthe text above, I provided some more scientific interpretations of “ignorance” in terms of limitations of information-processing, and\nour ignorance of those limitations and their implications. After those initial three items, the middle part of the Buddha’s chain goes as\nfollows: 4) “Name-and-form”. This basically means the world, including our internal world of memories and consciousness. 5) “Six-fold\nsense bases”. This is when the sensory organs receive input from the outer world, or memories or wandering thoughts enter the mind\n(which is in Buddhism considered a sixth sense). 6) “Contact”. This I interpret as perceptual processing leading to object recognition,\nwhere the brain processes information and interprets the incoming stimulus in terms of a given category (“That’s a dog”). 7) “Feeling”\n(vedan¯a) means computation and perception of the valence of the sensory stimulus: Is it good or bad, do I like it or not? 8) “Craving\n(or thirst, or desire)” is the same as desire in our terminology, as always including aversion. You may want the object you has seen, or\nyou may want to get rid of it. A number of goals are considered at this stage. 9) “Attachment (or clinging)”. I proposed in Chapter 9\nto interpret this as forming an intention, i.e. committing to a certain plan and a goal, and planning for it. (In Buddhist literature, the\ninterpretation of “attachment” is actually highly variable, and in my view rather muddled: often, the distinction between desire and\nattachment is only a matter of degree. It is sometimes pointed out that an alternative translation of the word in question (up¯ad¯ana) is\n“fuel”, which might give an alternative interpretation as being related to the learning process in the next steps.) 10–11) “Becoming and\nbirth” are the next two steps which are a bit more difficult to interpret, and have traditionally been interpreted in more metaphysical\nterms. I suggest interpreting them as referring to the learning process which creates various associations in the mind, including creating\nhabits out of one’s actions. Thus, whatever the agent does leads to “birth” of new action tendencies and associations. 12) “Old age and\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n171\nIt should be noted that this chain of processing steps is not only initiated by external stimuli (the sight\nof something good) but also by internal simulation, which is another reason why the processing constitutes\nsomething more like a cycle or a loop. For example, wandering thoughts, or almost any kind of thoughts,\ntrigger memories or predictions of sensory stimuli, and the cycle is launched almost as if those stimuli were\nreal. This is, however, not shown in the flowchart for the sake of brevity.\nAn equation to compute frustration\nWhile the flowcharts above help us understand the mechanisms behind suffering and even design interven-\ntions, the real strength of a computational approach is that we can quantify things, at least in principle. I do\nnot mean that we would necessarily be able to give a number measuring the strength of suffering, but we can\nunderstand the connections between different quantities more explicitly than with flowcharts. As the most\npowerful recapitulation of the theories of the preceding chapters, I next propose a simple equation that de-\nscribes the amount of frustration experienced by the agent. This will be the basis in the next chapters, where\nwe attempt to reduce suffering.\nThe starting point for the equation is Chapter 5 (page 54), where we defined suffering as reward loss, that is,\nthe difference between the expected reward and the obtained reward. (More generally, the reward prediction\nerror might be used.) This theory provides a quantitative basis for modelling suffering. Reward loss is based\non a simple mathematical formula, so we can look at the different terms it contains. We can analyze how they\ninfluence reward loss, and how they could eventually be manipulated. The equation we had in Chapter 5 was,\nhowever, very basic and did not take into account any of the complexities of a real cognitive system that we\nhave seen in later chapters. So, we need to look at the different factors influencing reward loss in more detail\nand reformulate the equation.\nThe first point to consider is that any quantities affecting reward loss need to be perceived by the agent.\nWhile the difference between expected reward and the reward actually obtained is, in principle, the basis of\nsuffering, the difference cannot of course cause suffering by itself. It has to be computed—that is, perceived—\nby the agent. So, we need to make a connection between limitations in perception and categorization on the\none hand, and frustration on the other. Due to such limitations, the perception of the actual reward is uncertain\nand subjective, as explained in Chapter 12. Obviously, our expectations are subjective and may be overblown\nas well. Yet, the agent can only compute the reward loss based on its own perceptions, on the information at\nits disposal.\nThe second point is that as with any perception, the level of certainty of the error computation should also\nbe taken into account: If the perception of reward loss is particularly uncertain (say, because it is dark and you\ncannot see what you get), that should reduce the effect of reward loss. It is common sense that if the agent\ndeath” is the final result of the above causal chain, and can be interpreted as simply “suffering”. Considering the specific connections\nbetween such a Buddhist scheme and ours, we see that steps 4–9 here directly correspond to the first boxes in our flowchart (from\nbox #0 to box # 5). The boxes #6–#8 following that, including actual planning, plan execution, and error computation may be missing\nin the Buddhist chain, or they could be seen as being included in its steps 10-11. The steps 10–11 in the Buddhist chain are, in my\ntentative interpretation, specifically related to the ensuing learning process, shown by the horizontal green arrow in the flowchart. Step\n12 is a poetic description of suffering in the red box #9 at the top right-hand corner in the flowchart. In any case, the two models\nshare the crucial idea of how sensory input leads to desire, which via attachment (clinging/intention) leads to suffering, and how this\nsuffering somehow perpetuates itself (in our model by means of a learning process).\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n172\nis not at all certain about what happened, it should not make any strong or far-reaching conclusions, and the\nerror signal should not be strong.10\nFurther following the general rules governing perception outlined in Chapter 12, the intensity of the percep-\ntion of reward loss is modulated by the attention paid to it. Reward loss causes less suffering if little attention\nis paid to it, for example, when one is distracted by something else—simply because you might not even notice\nreward loss occurring. Paying attention to something may also be necessary to become conscious of it.11 Thus,\nthe amount of attention paid to the reward loss must be included in the equation. There are further related\nphenomena that change the amount of frustration experienced. For example, we may take the contents of\nconsciousness—the virtual reality or simulation—more or less seriously (Chapter 14). Furthermore, we may\nfind errors acceptable if we are deliberately trying to learn something new. For simplicity, we include these\naspects in the term called “amount of attention” being paid to the reward loss, since not taking simulation\nseriously is related to not paying a lot of attention to it.\nThe final important point is that error computation can happen many times, in particular in the case of\nreplay or planning, which means we perceive, or rather simulate, the same (possibly imaginary) reward loss\nagain and again (Chapter 11). For example, if you replay an event just once in your head, the suffering is\nmultiplied by two, almost.\nTaking all these aspects into account, we arrive at the following frustration equation:\nfrustration =\nperception of (the difference of expected and obtained reward)\n× level of certainty attributed to that perception\n× amount of attention paid to it\n× number of times it is simulated, plus one for the initial perception\nIn this equation, we have four terms on the right-hand side, i.e. after the equality sign, multiplied by each other.\nFirst, there is the basic formula of reward loss in parentheses. Thus it includes the amount of expected reward\nand the amount of obtained reward, whose difference is computed. As with reward loss, if this difference is\nnegative, it is set to zero—if the obtained reward is greater than the expected, there is zero frustration. But\ncrucially, the reward loss here is modulated based on how it is perceived by the agent.12\nThen, this perceived reward loss is multiplied by three modulating factors. We use multiplication here to\nemphasize how the perceived reward loss may actually lead to no suffering at all if just one of these modulating\nfactors is zero. The modulating factors are: the level of certainty that the agent attributes to the perception\nof reward loss (zero meaning absolute uncertainty, one meaning complete certainty), the amount of attention\npaid to reward loss (including how seriously the contents of the consciousness are taken), and finally, the num-\nber of times the event is simulated or perceived (after one initial instance of perceiving the event, it may be\n10This intuition will be made more rigorous in footnote 21 in Chapter 16.\n11See footnote 18 in Chapter 14 for discussion on this connection.\n12There is a subtle point about perception of the reward loss, which is that the system may first compute percepts of the two quantities\n(expected and actually obtained rewards) and then compute the difference, or it can directly attempt to compute the percept of the\ndifference. In other words, we can have perception of the difference, or the difference of the perceptions. (In the case of the expectation,\nits “perception” might rather be called the “estimation” of the expectation.) Intelligent systems may use either of these two approaches.\nI shall not venture into speculating which might be the case in the human brain.\n\nCHAPTER 15. OVERVIEW OF THE CAUSES AND MECHANISMS\n173\nreplayed or simulated in planning several times, so the term is the number of simulations plus one).13\nIt should be emphasized that such frustration happens on different time scales: from milliseconds to even\nyears, perhaps. In the smallest timescales, the suffering is likely to be much weaker than in the longer time\nscales; see Chapter 9 (page 94) for discussion on this topic. The equation above is intended to be applied\nseparately on different time scales.14\nAnother point that is useful to recall here is how we reformulated action selection as being based on rewards\nin Chapter 5. While we still often talk about goals and planning, for example in the flowchart in Figure 15.2, the\ngoals are now seen as something that the agent itself sets in order to maximize rewards. In particular, Chapters 8\nand 10 explained how an agent would predict that a certain state gives a big reward, then set it as a goal state,\nand start planning for it—the same logic underlies Figure 15.2. Thus, goals are not something inherent in the\nworld, but rather a computational device used by the agent in order to maximize rewards. That is why this\nequation uses the formalism of reward loss, instead of the basic formalism of frustration of goals initially used\nin Chapter 3. Still, goals are implicitly present in this equation since the expected reward is often the reward\nthat reaching a certain goal would give (according to the agent’s prediction).\nToward designing interventions\nNow, the essential point here is that all the terms on the right-hand side of the frustration equation are some-\nthing that can be influenced or intervened on, at least to some extent. That means it is possible to develop\nmethods that change the terms on the right-hand side, and thus change the amount of frustration and suffer-\ning. The next two chapters are largely an explanation and application of this equation from that viewpoint.\nThe focus here is on frustration instead of threat. This is justified by the fact that our theory reduces threat to\nanticipated frustration, as discussed in Chapters 6, 7, and 9. In particular, if there were no frustration, neither\ncurrently perceived nor anticipated, there would be no threats perceived either. Thus, reducing frustration\nprovides a very general basis for reducing suffering. Also note that there is another, apparently very different\nmechanism for reducing suffering: reducing desires and aversions themselves. If there are no desires, there\ncan be no frustration either, at least in the sense of desires not being fulfilled.15 This case will also be covered\nin the next chapter.\nIn fact, so far, it might actually seem that the book has been just one big complaint. Suffering seems\nunavoidable, a necessary consequence of intelligent information processing. However, in the following final\nchapters, we will see a way forward: what an intelligent agent can do to actually reduce its suffering.\n13The equation does not mention the “self”. This is because I reduce self-based suffering to frustration of self-needs based on the\nlogic explained in Chapter 6; that is the logic used in the following chapters. Alternatively, inspired by Buddhist philosophy or the\ndiscussion on self-related frustration in Chapter 9, we might think about adding another multiplying factor to the equation, called\n“relevance to self”, which would measure if the reward loss is affecting the self (in some sense to be defined).\n14It should be useful to formulate a similar equation based on RPE instead of reward loss. Such a formulation would handle these\ncomplex temporal aspects in a more principled way, and in particular, it would encompass frustration based on expectations alone, as\ntreated in Chapter 5, especially footnote 21. I leave that for future research.\n15Recall we have two different models of frustration: expectation-based and desire-based, as discussed in detail in Chapter 9.\n\nChapter 16\nReprogramming the brain to reduce suffering\nIn this chapter and the next, I will present various ideas on how to reduce suffering in a complex intelligent\nsystem acting in a complex world—such as humans. I derive various ways how information-processing should\nbe changed, i.e. how the agents should be reprogrammed, based on the theory presented in this book. Since\nthe systems in question, such as our brain, have largely learned their function from input data, an important\npart of such reprogramming is retraining the learning system by inputting new data into it.\nThe methods discussed here are not original: almost all come from Buddhist and Stoic philosophy or re-\nlated systems. The goal here is to interpret them from a computational AI perspective, using the theory devel-\noped in this book. Thus, we gain more understanding on how they work, why they work, and what could be\ndone to improve them.\nThe main starting point in this chapter is the frustration equation we just encountered (page 172). We can\ntry to reduce suffering by changing any of the terms on the right-hand side of the equation, since that inevitably\nimplies that the frustration on the left-hand side of the equation is reduced. We can see from the equation that\nthe obtained reward should be increased because it has a negative sign in the difference computed. In contrast,\nall the other terms on the right-hand side should be decreased because their contribution to frustration is\npositive.\nMaximizing the obtained reward is really a very conventional way to try to reduce suffering, based on the\nwide-spread view that happiness comes from having achieved all your goals and having got what you wanted.1\nHowever, that is difficult for reasons which are rather obvious. Many resources are limited: not everybody can\nhave the best cars, the best wines, and the best sex partners. There is fierce competition over such resources,\nand not everybody can win. Besides, expectations are adapted to the obtained level of rewards, so what used\nto feel good no longer brings happiness after a while, as discussed in Chapter 5 (page 61).\nSo, instead, we attempt here to reduce all the terms other than the obtained reward in the frustration equa-\ntion. In this chapter, we start by considering how it is possible to reduce two of those terms: the (perception of)\nexpected reward, and the certainty attributed to the perception of reward loss. (The next chapter will consider\nreducing the remaining terms, as well as some further methods.) Such reduction also includes reducing self-\n1(Oatley and Johnson-Laird, 1987; Van Boven, 2005; Heathwood, 2015); cf. Diotima in Plato’s Symposium: “[T]he happy are made\nhappy by the acquisition of good things”. For a discussion of different definitions of “well-being”, which can be seen as equivalent to\nhappiness here, see Crisp (2017); Fletcher (2015). Huta and Waterman (2014) discuss the particularly important distinction between\n“hedonia” and “eudaimonia” (roughly, happiness as pleasure/feeling vs. happiness as meaning/virtue). Eldar et al. (2016) explicitly\nlink happiness to long-term RPE.\n174\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n175\nneeds as a special case, thus complementing the frustration equation by the logic of the flowchart in Fig. 15.1.\nUltimately, such practices lead to reducing all desires and aversions. This approach may be rather unusual in\nthe context of modern Western psychology and philosophy, but it is thoroughly standard in Buddhist and Stoic\nphilosophy.\nReducing expectation of rewards\nLet us first look at the term “perception of the difference of expected and obtained reward”, i.e. perception of\nreward loss, in the frustration equation on page 172. This should be made as small as possible, ideally zero\nor even negative. As already mentioned, the most conventional way to reduce it would be to try to increase\nactually obtained rewards, but that is very difficult. So, we need to do something more clever. A well-known\nidea in Buddhist and Stoic philosophy is to lower your expectations. Then, your reward loss should be smaller;\nit will perhaps vanish altogether.\nThe expected reward is typically a product of two things: the probability the agent assigns to obtaining the\nreward, and the actual amount of the reward if it is obtained (considering the basic case where the amount of\nreward, if obtained, is fixed). Thus, reducing the expectation of a reward can be accomplished in two ways:\neither reducing the probability the agent assigns to the reward, or reducing the value it sees in the reward. This\ncan be compared to a lottery. Suppose your initial chance of winning a Porsche is 1%. Obviously, the lottery\nwould be made less attractive if the probability of winning is lowered to 0.01%; it would also be less attractive\nif you realized that the Porsche is second-hand and not so cool after all. In both cases, your expected reward is\nreduced.\nMost importantly, rewards in the real world are always a bit subjective, and so are the probabilities we\nassign to them. A new Porsche may feel like a great reward to one person, while it may matter very little to\nanother; this is why we have to talk about perceived reward. People will also have very different guesses of the\nprobability of winning it. Since these quantities are subjective, it is possible to change our estimates of them\nby changing our beliefs, perceptions, and associations, even if the actual physical reality remains unchanged.2\nA key goal of Buddhist and Stoic systems is exactly such re-evaluation of the probabilities and rewards. To\naccomplish this, Buddhist philosophy talks about the “three characteristics of existence”, which are imperma-\nnence, no-self, and unsatisfactoriness. They map roughly to our concepts of uncertainty, uncontrollability, and\nunsatisfactoriness we discussed in the preceding chapter.3 Each of these characteristics gives a reason why the\nrewards are actually lower than what they would otherwise be, or what they appear to be, as will be explained\nnext.\n2The subjectivity and contextuality of perception is actually quite complex and creates many further possibilities of reducing the\nterm being considered here. Logically, we might also try to increase the perceived obtained reward independently of the actual reward\nobtained. This should be possible by somehow learning to better appreciate the rewards obtained, such as by a feeling of gratitude or\nappreciation, as briefly discussed in Chapter 18 Likewise, it might be possible to reduce the perceived reward loss even if the actual\nperception of the reward and the expected reward are unchanged. Seeing the reward loss as a useful learning signal might work in that\ndirection, but I will not pursue that possibility any further.\n3See footnote 3 in Chapter 15 for a detailed discussion of the connection.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n176\nFacing uncontrollability\nUncontrollability, discussed in Chapter 13, is a key concept here. The level of controllability is clearly related to\nthe level of expected reward. If you think the world can be controlled, you will expect to achieve high rewards,\nbecause you think you are able to take courses of action that give you the very highest rewards, and you are\nreasonably certain that you can achieve them. Thus, you’re exposed to strong frustration since your expecta-\ntions are high. In contrast, if you think the world is uncontrollable, you assign a low probability to achieving\nany rewards, and the higher rewards may seem to be completely out of your reach. Then, your expectation of\nreward is smaller, and you are less likely to suffer from a reward loss, i.e. frustration. This is how considering\nthe world to be uncontrollable reduces suffering.\nTo transform this logic into a method for reducing suffering, the trick is to acknowledge the fact that you\nhave little control and there can never be very much control, and firmly believe in that fact. We saw earlier how\nthe Stoic philosopher Epictetus emphasizes how little we can control (page 139). He continues by explaining\nthat if we are mistaken about this point, suffering is inevitable:4\nThe things in our control are by nature free, unrestrained, unhindered; but those not in our control\nare weak, slavish, restrained, belonging to others. Remember, then, that if you suppose that things\nwhich are slavish by nature are also free, and that what belongs to others is your own, then you\nwill be hindered. You will lament, you will be disturbed, and you will find fault both with gods and\nmen.\nTo put this idea into practice, on every occasion where you are inclined to develop a desire or aversion towards\nan object or event, you should ask yourself whether you can control it or not. Basically, you cannot control any-\nthing external to you, such as your possessions, other people, or their opinions. Epictetus suggests you should\nnot see any such external, uncontrollable things as good or bad, or as bringing any reward in our terminology.5\nLikewise in Buddhist philosophy, the original form of the no-self philosophy says that nothing is part of me,\nwhich is a way of saying that nothing can be controlled, as we saw in Chapter 13. Understanding this is crucial\naccording to the Buddha:6\nAll [mental phenomena], whether past, future or present, internal or external, gross or fine, inferior\nor superior, far or near, should be seen with one’s own knowledge, as they truly are, thus: ’This is\nnot mine, this I am not, this is not my self.’ (...) [S]eeing thus, [the disciple] grows wearied of form,\nwearied of feeling, wearied of perception, wearied of volitional formation [i.e. desire and aversion],\nwearied of consciousness. Being wearied, he becomes passion-free (...), he is emancipated [from\nprocesses leading to suffering].\nHere, I interpret “growing wearied” as signifying that the reward expectations are lowered, or little enjoyment\nanticipated. Thus, the point is that recognizing uncontrollability, or inexistence of self, reduces expectations of\nreward, and this reduces suffering.\nBuddhist philosophy further emphasizes the importance of understanding “causality”.\nSuch causality\nmeans that events in the world just follow from each other based on natural laws, for example those depicted\n4Paragraph 1 of The Enchiridion.\n5Discourses, III.3.14-15 and III.8.1-2\n6Samyutta Nikaya 22.59, latter half, strongly shortened, based on the translation by Mahasi (1996).\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n177\nin Figure 15.2. This thinking minimizes the importance of free will and the control that the agent can have\nover the world; it is related to what is called determinism in Western philosophy. I would think, therefore, that\nthe Buddhist emphasis on what they call causality is just another viewpoint on uncontrollability; seeing such\ncausality is one way of realizing that the world is uncontrollable. Stoic philosophers advocated the study of the\nnatural sciences7 (which they simply called “physics”), with a similar goal.8\nFacing uncertainty, unpredictability, and impermanence\nUncontrollability is closely related to the concept of uncertainty. Uncertainty feeds into uncontrollability: if\nthe workings of the different objects in the world are uncertain, even quite random, the world cannot be very\nwell controlled. Likewise, uncontrollability leads to uncertainty about whether rewards will be obtained. In\nsome sense, these are two sides of the same coin.\nBuddhist philosophy focuses on the related concept of impermanence, which can be largely seen as a spe-\ncial case of uncertainty. Impermanence means that the world is constantly changing, and usually in unpre-\ndictable ways.9 For example, any object that you possess can break or get lost. Any enjoyment that you get is\nlikely to be fleeting. In fact, even your feelings and opinions are impermanent: today you like one thing, but\nperhaps tomorrow you’re already bored with it and want something else; what you consider important today\nmay have no significance to you next month. Obviously, impermanence thus interpreted leads to uncertainty.10\nGoing back to our frustration equation, the consequences of uncertainty are very similar to the conse-\nquences of uncontrollability. The central point is that any future rewards are uncertain, i.e. unpredictable.\nRewards and the circumstances leading to rewards can change, so an agent cannot really know whether it will\nget any reward after executing its plan. Thus, the agent should lower the probability it assigns to any future\n7(Durand et al., 2023)\n8Causality is a topic of great current interest in AI (Pearl, 2009; Peters et al., 2017; Gershman, 2017). However, the meaning of the\nterm in AI is a bit different, and in particular, very specific: It is more about the difference between correlation and causality, and\nhow an agent could learn that difference. In AI, understanding such causality will enable the agent to act more efficiently, increase its\ncontrol of the world and the rewards it obtains, as well as better predict the rewards. In contrast, in Buddhism, understanding causality\nis about admitting the determinism of the world and minimizing the perceived control of the agent. Eventually, both these two kinds of\n“understanding causality” may reduce suffering in their own ways. Briefly, understanding causality in the AI sense means that you can\nfind optimal actions and increase rewards, while the Buddhist understanding means appreciating how little reward even those optimal\nactions bring, thus reducing expectations.\n9From an alternative probability theory viewpoint, impermanence could also be seen as incorporating nonstationarity, which is\nthe technical term for the situation where the world is changing, and a statistical model learned on data in the past may not be valid\nanymore in the present and even less in the future. Such problems were already alluded to in previous chapters when it was pointed\nout that humans may be evolutionarily adapted to the African savannah instead of the modern city environment (Chapter 5); or that\nemotional reactions learned as a child may be far from optimal in an adult since the environment is very different (Chapter 10). It\ncould be argued that nonstationarity is as important as scarcity of the data, and an independent root cause of suffering. However,\nI don’t pursue that line of argumentation here since I tend to think that any problems with nonstationarity could be avoided if the\nagent had enough data and computation since it would then be able to predict the nonstationarity (like Laplace’s demon in footnote 17\nbelow); but this is admittedly a complex and controversial point that needs further research.\n10However, the Buddhist impermanence has also aspects that cannot be considered to be forms of uncertainty. For example, if you\nknow for sure that you will die tomorrow, there is no uncertainty, although this is the quintessential expression of impermanence in\nBuddhism—but it could be argued that there is still uncertainty about what happens after death. In fact, in a meta-level sense, the\ncentral point in early Buddhist philosophy of impermanence is that impermanence itself is absolutely certain, as well as “permanent”:\nEverything will perish one day for sure. Some exceptions may exist, however: “enlightenment” (nirv¯ana or nibb¯ana) is permanent\naccording to most schools (Harvey, 2009, p. 52), consciousness is permanent for some Yog¯ac¯ara thinkers (Williams, 2008b, p. 99), and\na rather obscure metaphysical construct called dharmak¯aya is also permanent in some Mahayana schools (Williams, 2008b, p. 106).\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n178\nreward. If the agent acknowledges such uncertainty of the world, its expectations regarding rewards will be\nlowered, just like in the case of uncontrollability. Consequently, frustration will be reduced. (Later, I will talk\nabout perceptual uncertainty, which has a different effect on suffering.)\nIt is quite paradoxical that Buddhist practice, which turns your attention to uncertainty and uncontrollabil-\nity, tends to reduce stress and suffering. In Chapter 6 we saw that uncertainty and uncontrollability are usually\nthought to lead to more stress, not less. I think the paradox has a lot to do with one’s attitude to uncertainty and\nuncontrollability. Somehow Buddhist philosophy seems to result in a particularly appropriate attitude, related\nto their acceptance, which will be considered in more detail in Chapter 18.11\nFacing unsatisfactoriness\nIn Buddhist philosophy, the two characteristics of impermanence and no-self (roughly, uncertainty and un-\ncontrollability) are complemented by a third characteristic: unsatisfactoriness, which has many meanings and\ninterpretations. On the one hand, it expresses the idea that whatever we try to achieve, we often fail due to\nuncontrollability and uncertainty. In this sense, it simply recapitulates those two aforementioned properties.\nOn the other hand, unsatisfactoriness can be seen as an extremely general characteristic which penetrates all\nphenomena and all existence. In fact, in the original Indian texts, the single word dukkha is used to express\nsuch unsatisfactoriness as well as suffering, i.e. this very thing we are trying to reduce. One could express the\nrelation between these two meanings by saying that all phenomena are unsatisfactory in the sense that they\ncan produce suffering, one way or another.12\nIn Buddhist philosophy, it is recommended to acknowledge the unsatisfactoriness of all phenomena. A\nbasic justification for this can be constructed using our frustration equation: if the agent is strongly convinced\nabout the unsafisfactoriness of all phenomena, its expectation of reward will be very low, and the reward loss\nwill be small and rarely even occurs. Thus, here we are talking about a very general, if a bit vague, strategy for\nlowering the expectations of rewards. As a training method of great generality, the Stoics suggested reviewing\nany plan of future action with the view of anticipating what could go wrong and how the plan will not lead to\ngreat enjoyment after all. Epictetus gives a famous example of going to a Roman bath:13\nIf you are going to bathe, picture to yourself the things which usually happen in the bath: some\npeople splash the water, some push, some use abusive language, and others steal.\nWith this mindset, you will not expect much enjoyment, i.e. reward, and you will not be disappointed. Such a\nscenario could be analyzed in terms of uncontrollability and unsatisfactoriness as well, but unsatisfactoriness\nmay be a more natural viewpoint.14\n11This contradiction between ancient philosophers and modern psychology has baffled many commentators. I would speculate\nthat the problem is that not all research on lack of control (or uncertainty) has clearly made the distinction between the level of con-\ntrol/certainty the agent perceives to have, and the level of control/certainty the agent wants to have. In a typical experimental paradigm,\nthe perceived control can be easily reduced by experimental manipulation, but the control that the agent wants to have may be un-\nchanged. Then, the agent may want to have more control or certainty than it perceives to have, and this leads to a case of frustration\non a “meta-level” of self-needs, and thus suffering. A Buddhist practitioner is supposed to accept that control is not possible, and\neverything is uncertain. Thus, she gives up wanting any control and avoids any such meta-level frustration; as just argued in the main\ntext, frustration on the ordinary level is reduced as well. Thus, such an attitude should lead to less suffering.\n12On the translation of dukkha, see An¯alayo (2003, p. 244).\n13Paragraph 4 of The Enchiridion.\n14In particular, such mental imagery may serve to increase the perceived probability of adverse events based on what is called the\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n179\nUsing our computational theories, we can penetrate still deeper into the meaning of unsatisfactoriness.\nSuppose you can get chocolate quite easily and there is little uncertainty about its great taste. It is not ob-\nvious how uncertainty or uncontrollability would be a problem here. But, we can still say that the chocolate\nis unsatisfactory because there are various negative long-term side effects hidden in the apparently rewarding\nobject.\nTo see what such negative long-term effects might be, recall how in Chapter 15, we defined unsatisfac-\ntoriness in a more specific way, based on two computational ideas. First, we had the concept of insatiability\n(Chapter 5). An intelligent system programmed to maximize reward will never be satiated or satisfied, by the\nvery construction of the system. It will never find that it has had enough, because in the long run, getting more\nreward will increase the expectation of rewards. In a word, the system is infinitely greedy. The second aspect\nof unsatisfactoriness in our framework was evolutionary obsessions (Chapter 5). Even the very goals pursued\nand the rewards obtained can be questioned. Perhaps the evolutionary system gives you a certain reward for\ndrinking a sugary drink. But we know very well that such a reward is misleading: the sweet drink is not good\nfor you when all its effects are considered in the long run.\nThese two computational viewpoints of unsatisfactoriness point at mechanisms which are very different\nfrom uncontrollability or uncertainty, or even simply reducing reward expectations. The implication is that\neven if we could totally control the world and everything were certain, the result of our strivings would not\nbe that great anyway because it would not produce a lasting satisfaction or pleasure. While uncertainty and\nuncontrollability are more about the probability of getting various kinds of rewards, unsatisfactoriness (both in\nour sense and the Buddhist sense) is really about the real worth of the rewarding objects or events themselves,\nwhen considering the bigger picture. Even the very best chocolate, if you eat it every day, will ultimately leave\nyou indifferent, and may ruin your health in the long run.\nOur definition of unsatisfactoriness actually works a bit outside of the frustration equation because it is not\nthat the rewards or their probabilities (or any other terms in that equation) are changed: it is rather understood\nthat even if the rewards are obtained, there will be side effects in the distant future. The frustration equation\nis in a sense short-sighted: it only considers the direct, immediate effects of rewards or their simulation.15\nIn contrast, the ideas of insatiability and evolutionary obsessions bring a longer time scale into the picture,\npointing out that obtaining rewards now may actually increase frustration and suffering in the long run. 16\navailability heuristic (Tversky and Kahneman, 1974). That is, humans tend to estimate the probability of events based on how easily\nthey can recall (or imagine) those events. If you willfully imagine an event happening in the future, that will make the event more\naccessible in terms of memory retrieval, and thus you may start considering its probability of happening is higher. When things going\nwrong are perceived to have a higher probability, expected reward is necessarily reduced. Thus, imagining adverse events may be a\nparticularly powerful way of influencing unconscious decision-making processes. It could be counterargued that such simulation of\nnegative events creates suffering by itself. This may be true, but it seems that the suffering created by such deliberate simulation is\nquite small compared to the possible real-life frustration.\n15Thus, our computational definition of unsatisfactoriness does not exactly lead to reduction of reward expectations in the strict\nframework of the frustration equation, unlike the basic Buddhist interpretation given in the text. However, understanding the nega-\ntive long-term effects may also reduce the expectation of immediate reward if the long-term effects are in some sense added to that\nexpectation. Recognizing such unsatisfactoriness should ultimately reduce desires since that appears to be the only way of completely\nescaping this logic. With less desires, there would be less opportunity for any frustration to arise, while insatiability or evolutionary\nobsessions become irrelevant. Reducing desires will be considered in detail later in this chapter.\n16 It may be more difficult to see why the aforementioned attitudes would also reduce frustration due to aversion, or a negative\nreward. Let us consider aversion based on expecting that a bad thing is likely to happen, such as your neighbours starting a noisy\nrenovation. Now, taking account of uncontrollability means you cannot really avoid the bad thing, at least not with any certainty. This\nmeans that the probability of the bad thing happening is larger than what you might have initially thought—your flat will be noisy for\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n180\nReducing certainty attributed to perception and concepts\nAnother term in the frustration equation that we can reduce is the “level of certainty attributed to that percep-\ntion”. As we saw in Chapter 12, perception is uncertain. To recapitulate the main ideas: perception is based on\nlimited data, thus necessitating unconscious inference, which may not always be much better than guessing.\nPerception is also subjective: different people can have different priors and thus different perceptions. Subjec-\ntivity is made even more serious by the strong selection of incoming information by attentional mechanisms.\nSince the computational capacity is always fundamentally limited, and the world is awesomely complex, it is\nnot possible to build a perceptual system that always makes correct inferences, let alone one that perceives\nthe “true” reality. This should imply a fundamentally skeptical attitude towards any perception: we should not\nmake too strong conclusions based on sensory input.\nThus, we see that uncertainty has two different aspects. There is the objective unpredictability of the world:\nsurprising and unexpected things can happen, the world is to some extent random—this is the kind of uncer-\ntainty we focused on earlier in this chapter when talking about the importance of recognizing uncertainty and\nimpermanence. But here, we focus on the uncertainty in our perceptions and beliefs of the world, which I here\ncall perceptual uncertainty. The point is that we don’t know with any great certainty what the state of the world\nis, since we have neither enough data nor enough computation to perceive it properly.17 Such perceptual un-\ncertainty increases the effects of unpredictability and uncertainty that we saw earlier, since it makes the world\neven more unpredictable for the agent.\nIf the agent is intelligent enough, it will take perceptual uncertainty into account when evaluating the re-\nward loss or frustration. Suppose the agent has completed an action sequence in view of getting reward, and\nit tries to evaluate the reward loss. Now, the agent should understand that it cannot know with certainty how\nmuch reward it got. A drink may have tasted good, but you cannot know if it was actually good for you. That\nis why in the definition of reward loss, we should really be talking about perceptions of rewards instead of any\nobjective quantities; this is precisely what is done in the frustration equation.18 Since the reward loss is uncer-\nsure. Thus the expected reward is less than what you would have thought without taking uncontrollability into account. More precisely,\nit is more negative, since the probability of a negative reward is larger. Thus, frustration is reduced by reducing the expectation of\nreward by making the negative expectation even more negative. Likewise, thinking in terms of unsatisfactoriness (in the Buddhist\nsense) means thinking that the bad thing is likely to be really bad—the noise is probably going to be something quite unbelievable.\nAgain, this reduces expected reward in the sense of making it even more negative, and what actually happens is less likely to give\nyou a negative surprise and frustration. On the other hand, a classical Buddhist account would point out that impermanence means\nthat the object of aversion will eventually disappear, which makes at least the feeling of aversion weaker. Clearly, it will give me some\ncomfort knowing that the noise will not be there forever. However, putting uncertainty into the framework of the frustration equation\nis not straightforward in the case of aversion, or negative reward. Taking account of the various forms of uncertainty might mean that\nyou realize that the bad thing, which you initially thought is certain, is actually less likely to happen than what you first estimated.\nParadoxically, this increases the expected reward, because the negative reward is less likely to happen, and actually increases your\nfrustration. Thus, uncertainty may need to be treated in separate ways depending on whether the reward is positive or negative.\n17This division into two kinds of uncertainty could be criticized on philosophical grounds. Laplace proposed that an intellect (called\na “demon” by later commentators) which knows everything about the world would be able to perfectly accurately predict everything,\nand nothing would be uncertain to it. Thus, from this viewpoint, uncertainty is always a reflection of ignorance about some aspects of\nthe world. I shall not go into that debate here, and just acknowledge that the division I make here is not very rigorous while in line with\nhow randomness is often conceptualized in AI theory. This is basically the distinction between “aleatoric” and “epistemic” uncertainty\ndiscussed, e.g., by Hüllermeier and Waegeman (2021); Charpentier et al. (2022); Lockwood and Si (2022).\n18To emphasize: our basic definition of reward loss on page 54 does not take into account the fact that it is perceptions that mat-\nter. Obviously, it cannot then take the uncertainties into account either. Thus, the definition must be changed accordingly, and this\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n181\ntain, any conclusion drawn from it should not be given too much weight.19 This is an implication of the basic\nprinciples of Bayesian inference as used in AI; many philosophers over the centuries have also pointed out that\nwhat first appears to be a negative outcome may even turn out to be positive, and vice versa.20\nIf the agent is programmed to take account of the fact that all its perceptions are uncertain, it would likely\nhave weaker reward loss signals. Consider an agent that attempts to get some chocolate. Suppose that after\nexecuting a plan, the agent is able to eat some, but its program “understands” that it does not really perceive\nthe amount of chocolate with any certainty; perhaps because it swallows all of it immediately without really\ntaking a look. Intuitively, it does not then make a lot of sense to send a strong reward loss signal: such a signal\nwould be too much guesswork and would not provide a proper basis for learning better behavior. In other\nwords, uncertainty about the correct signal to send should lead to a weaker signal.21 Thus, taking account of\nthe uncertainty of the perception of reward would reduce suffering.22\nThe Buddhist concept of emptiness\nThe perceptual kind of uncertainty has a central role in the later Mahayana schools of Buddhism. While the\n“three characteristics” (impermanence, no-self, unsatisfactoriness) form the core of the Buddha’s original phi-\nlosophy, later Buddhist philosophers found them somewhat simplistic. The emphasis shifted to the properties\nand limitations of perception and cognition, as opposed to characterizing the outer world. The inaccuracy of\nwas done in our frustration equation on page 172 by multiplying the perceived reward loss by the certainty of perception. See also\nfootnote 12 in Chapter 15.\n19Here we focused on the uncertainty in obtained reward. It could also be asked if there can be uncertainty in the expected reward.\nIn an orthodox Bayesian interpretation, it may in fact not be possible to say that there is any uncertainty about the expected reward,\nsince the expected reward is a subjective quantity, something purely defined by what the agent believes and expects. In contrast, in\na frequentist intepretation, the expected reward is an objective quality in the outer world (how much the agent would get on average\nif it repeated the same action many times) that could further be considered a parameter in a statistical model. Therefore, it can be\nmisestimated, thus adding to the uncertainty of reward loss. Notwithstanding such theoretical arguments, I think it is clear that for\nbiological organisms, understanding the real evolutionary value of, say, a piece of food may actually be a highly complex process\ninvolving a lot of learning and computation, so it can surely go wrong, as in the case of sugary food, which means it is meaningful to\nsay that there is uncertainty about the expectation.\n20Let me just mention the great Chinese classic Huainanzi’s “The old man lost his horse”.\n21It may be intuitively clear that acknowledging the uncertainty of perception should lead to “weaker” signalling of frustration; in\nthis footnote, I explain how to make that idea more rigorous. We can consider, as an illustrative example, one of the simplest online\nlearning tasks, namely linear regression. There, we minimize a quantity such as P\nt (yt −axt )2/σ2\nt where x is input, y is output, σ2\nt is\nnoise level, and a is a parameter to be estimated. The magnitude of the error signal for each data point is proportional to the inverse of\nthe noise level σ2\nt . Thus, for a high noise level (large uncertainty), the error signal is smaller. If the noise level is estimated separately for\neach data point (or time point t), this will have the effect of reducing the error signal at time points where there is a lot of uncertainty as\nmodelled by the noise level σ2\nt . The concrete algorithm used here might be what is called the delta rule; see Korenberg and Ghahramani\n(2002) as an example of a related if slightly more complex model, and Kendall and Gal (2017) for a more sophisticated deep learning\nmodel. In the context of reinforcement learning, Mai et al. (2022) propose a closely related weighting for RPE, where indeed we see how\nthe learning proceeds by minimizing the expected (squared) RPE so that it is down-weighted by the estimated variance of the RPE.\n22One aspect of uncertainty which is not explicit in the frustration equation is that there can also be an RPE due to changing pre-\ndictions. Suppose one moment you think you will get a reward, but the next moment it looks like you will not get it. This decrease in\nexpectation induces RPE and thus suffering, as explained in footnote 21 in Chapter 5. However, nothing may have actually happened,\nit was all just predictions in your mind. Importantly, such a change in prediction is only possible if there is uncertainty. If the future\nwere certain, there would be no need to update your predictions, but because of uncertainty, the predictions change from one moment\nto another. Again, such suffering can be reduced if you realize that your predictions are uncertain; then the change in the predictions\nwould be given less weight, according to the same principles as just explained in the text.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n182\nperceptions and beliefs became essential as part of the multifaceted concept of “emptiness” widely used in\nMahayana Buddhism—although rarely by the Buddha himself.23\nEmptiness has many meanings. In the framework of this book, we can consider emptiness as an umbrella\nconcept encompassing several of the ideas related to information-processing that we have seen in this book, in\nparticular uncertainty, fuzziness, subjectivity, and contextuality. To summarize it in a single word well-known\nin Western thinking, we could call it “relativity”.24 What the different aspects of emptiness have in common is\nthat fully appreciating them should make us take the contents of our minds less seriously.\nWhile it seems fashionable to discuss such concepts in terms of Buddhist philosophy, very similar ideas can\nbe found in Greek philosophy. We already saw the Skeptics questioning the reliability of any sensory informa-\ntion in Chapter 12.25 On the other hand, Plato’s famous theory of “ideas” (or “forms”) describes a kind of true\nreality behind the sensory phenomena, thus denying the true existence of the phenomena. Seneca explains\nhow this theory is related to reducing desires:\n[A]ll these things which minister to our senses, which arouse and excite us, are by Plato denied a\nplace among the things that really exist. Such things are therefore imaginary, and though they for\nthe moment present a certain external appearance, yet they are in no case permanent or substan-\ntial; none the less, we crave them as if they were always to exist, or as if we were always to possess\nthem.26\nIndeed, Seneca reads Plato as if he were a Buddhist philosopher propounding emptiness philosophy.\nConcepts and categories are considered particularly empty in Mahayana philosophy. It proposes that the\nobjects in the world do not really exist as separate entities, but are just part of a complex flux of perceptions\nhappening in our consciousness. In this sense, there are really no separate objects or crisp categories in the\nworld; they are purely constructions of the mind. Zen texts use the parable of confusing the moon and the\nfinger that is pointing at the moon. Here, I would interpret this in the sense that the finger is a category, perhaps\n23Though see Samyutta Nikaya 22.95, where the Buddha clearly talks about a general emptiness of the Mahayana kind, while using\na slightly different terminology: he does not use the word suññat¯a/´s¯unyat¯a which is the term usually translated as emptiness, and\nbecame prominent in later texts. See also Majjhima Nikaya 121 for a very different early view on emptiness, and Williams (2008b,\np. 54).\n24I am here referring to the common, non-technical definition of relativity, such as “the state of being dependent for existence on or\ndetermined in nature, value, or quality by relation to something else” (Merriam-Webster.com, accessed 24/1/2022). The interpretation\nof emptiness as relativity was initiated by Theodore Stcherbatsky, one of the earliest Western interpreters of Buddhist philosophy.\nSome commentators may prefer ontological interpretations of emptiness, but my treatment here sees it more as an epistemological\nquality, compatible with my computational approach. Emptiness actually has two different but related well-known definitions in\nMahayana Buddhism (Williams, 2008b). First, there is the Yog¯ac¯ara definition based on the “consciousness-only” thinking described\nin Chapter 14: All phenomena in the world are called empty because they are simply phenomena in the mind and constructed by the\nmind; in particular, any categories and concepts are merely mental constructs. This is rather similar to what we just discussed, except\nthat in Yog¯ac¯ara, such thinking can even be taken to a metaphysical level, denying the existence of the outside world—at least in some\ninterpretations. Second, there is the Madhyamaka definition, where all phenomena are called empty in the sense that they are simply\nproducts of long causal chains, thus lacking any independent, intrinsic existence, and subject to change at any time. This is a very\ngeneral definition that is ultimately supposed to contain most related properties described in this book or other Buddhist schools; it is\nsurprisingly similar to the dictionary definition of relativity just given. For example, subjectivity of perception can be seen as a result\nof such causality because perception is causally influenced by the priors in the perceiver’s brain, and thus the percept does not exist\nindependently (of the brain).\n25Connections between Skeptic ideas and Buddhist emptiness are discussed by Garfield (1990); Brons (2018); Dreyfus and Garfield\n(2021)\n26Letters to Lucilius, LVIII.26-27. Compare with footnote 31 in Chapter 14.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n183\nexpressed by a word, that merely points at a phenomenon in the real world, that is, the moon. Ceasing to think\nin terms of categories and concepts, based on a recognition of emptiness, is something that generalizes the\nidea of reducing the certainty attributed to perception, or in fact, to your cognitive processes in general. It\nreduces frustration according to the logic given above for recognizing uncertainty of perception. Furthermore,\nany valence that you would typically associate with a category cannot be considered certain anymore.27 What\nmay be Epictetus’s most famous quote says: “Men are disturbed not by the things which happen, but by the\nopinions about the things.”28\nReducing self-needs\nIn the frustration equation above, we didn’t have any terms explicitly related to self. Yet, self is obviously an\nextremely important concept from the viewpoint of suffering, as seen in Chapters 6 and 13. In our framework,\nself creates its own kind of frustration, by bringing aspects such as self-preservation, self-evaluation (or self-\nesteem), and control into play. As such, self-related suffering is covered by the frustration equation as a special\ncase.\nMany philosophical traditions such as Buddhism encourage reducing self-related thinking as a means to\nreduce suffering. One case of self-related thinking is related to the self-evaluation system. In Chapter 6, it was\nproposed that a self-evaluation system constantly computes whether we have gained “enough” reward recently,\nlooking at the relatively long-term performance of the system. (This long-term evaluation system is different\nfrom the one which computes the ordinary, short-term reward losses in the first place.) Such self-evaluation\ncreates, as it were, another frustration signal on a higher level, in case the result of the self-evaluation is worse\nthan some set standard.\nLogically, there are three ways of reducing negative self-evaluations. The first is similar to the “conven-\ntional” approach we discussed above regarding ordinary frustration: it is to really gain a lot of reward, so that\nyou surely reach the standard required. This is obviously easier said than done. Furthermore, such striving may\nnot reduce suffering at all because gaining a lot of reward may increase the expectations in the future, resulting\nin insatiability on a “meta-level”.29 The second approach, in line with the main proposals in this chapter, is to\n27The Pyrrhonians explicitly advocated rejecting any valences (Sextus Empiricus’s Outlines of Pyrrhonism, 1.27-28). If you admit that\nyou’re not sure about what category some object belongs to, valences as well as any further associations and generalizations have to\nbe given up as well. This implies further processes that are more specific to categories, and complement the logic of simple perceptual\nuncertainty.\n28The Enchiridion, Paragraph 5, trans. G. Long. For example, if I think that what somebody else just did belongs to the category\n“rude”, perhaps I should not be so certain about such inference. I can start with considering if my perception was incorrect: I may\nhave completely misunderstood what he was doing, or what his goal was. From the viewpoint of contextuality, I might consider if in\nthis particular situation, his behavior was actually just right—or maybe I am in a foreign culture and don’t know the rules. From the\nviewpoint of subjectivity, I might wonder if other people found his behavior commendable and if it is just me who finds such behavior\nrude. From the viewpoint of fuzziness, I might ask: How does one define rudeness anyway, is there a well-defined criterion? Fuzziness is\nactually something whose effect on suffering we have not yet considered in detail, although it is an important concept—if not under this\nterm—in relevant philosophical systems, such as Zen and the Pyrrhonian Skeptics. Chapter 8 argued that while conceptual thinking\nuses crisp categories, many of the things in the world are fuzzy. If you categorize events which are only borderline rude as simply\nrude, that is a form of overgeneralization: you may then be suffering unnecessarily due to your crisp-categorical thinking. The effect of\nfuzziness on suffering thus seems strongly analogous to the effect of uncertainty.\n29Actually, the theory in the previous chapters does not exactly lead to such meta-level insatiability. We saw in Chapter 5 how pre-\ndictions are constantly updated, thus leading to insatiability. However, self-needs are not necessarily concerned with predictions but\nexpectations of a different kind, as discussed in Chapter 6. Still, it is possible that the expectations computed by the self-need systems\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n184\nlower the standard of expected reward. For example, the aforementioned philosophical viewpoint that every-\nthing is unsatisfactory should work here as well. If the system expects little reward even in the long run, the\nself-evaluation should not claim that the agent did not gain enough.\nHowever, there is clearly a third option: shut down the system that evaluates your long-term success. Such\na shut-down is possible by convincing yourself of the total futility of the self-evaluation. The Buddhist philos-\nophy of no-self should be particularly useful here. Admitting the lack of control, even lack of free will, implies\nthat there is little to evaluate. If we cannot influence the world and the level of obtained rewards, what is the\npoint in evaluating my actions and learning strategies? On a deeper philosophical level, if it is not me that\nactually decides my actions—say, it is my neural networks—-who is to be evaluated? Perhaps my neural net-\nworks and my body could still be evaluated, but not “me” really. On the other hand, what if “my” actions are\nultimately determined by the input data or the environment, not “myself”?\nSuppose an agent were somehow able to shut down its self-evaluation system. It could be objected that\nsuch an agent with no self-evaluation might no longer be functional. However, even if the long-term self-\nevaluation were completely shut down, the system could still achieve most of its goals, and it will even be able\nto learn. Learning might just be slightly impeded because the learning system would not be optimally tuned to\nthe environment. Thus, only “learning to learn”, a kind of meta-learning, would be shut down, while the agent\nwould be perfectly functional otherwise, even without self-evaluation.\nI should emphasize another crucial point about self-evaluation. As long as the self-evaluation is based on\nevolutionary fitness, including what I called evolutionary obsessions, it does not actually make a lot of sense\nfor us. It is too often based on criteria that are not in line with what humans should strive at, according to\nmainstream ethical principles. We need better criteria to decide if our actions were “good enough”; criteria\nthat would be more in line with what we consider a good human life should be about.30\nLikewise, reducing the survival instinct, or information-processing aiming at self-preservation, would seem\nto be useful for reducing suffering. Again, it could be objected that it is not good for the agent: such reduction\nmay increase the probability of injury and even death. If I had no survival instinct, I might just happily go and\npat a tiger I see in the jungle. This is a valid point, but we could still try to reduce the intensity of suffering\nincurred. In fact, religions and spiritual traditions invariably propose some method to cope with fear of death\nand mortality. Fear of death may sometimes be paralyzing, and quite often, it is unreasonable since I may even\nsuffer from seeing a tiger on TV. Therefore, a moderate reduction in survival instinct might have mainly positive\nconsequences. One method would be to reduce the mental simulations of injury and death; we will get back to\nthis point in the next chapter, where we look at reduction of simulation by meditation.\nGeneral reduction of self\nIn addition to reducing the two specific self-needs just considered, we can aim at a more general “reduction\nof self”, which can take many forms. To begin with, if we see the self as the source of control, and then we\nare also updated based on past rewards, leading to meta-level insatiability.\n30As self-evaluation can use social comparison as a baseline (see page 6 in Chapter 6), it is also important to question the adequacy\nof such comparisons. For example, social media platforms may create unrealistically high standards regarding what one should look\nlike and what lifestyle one should have, partly because such content is carefully selected and even fake. When adolescents compare\ntheir own life with social media, there may be a huge gap, which may create mental health problems as reported by Vogel et al. (2014);\nVerduyn et al. (2015) (but see also Beyens et al. (2020)). How to reduce this kind of suffering: should people avoid using social media\nplatforms? At the very least, it would be useful to understand the futility of such comparisons.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n185\nrecognize uncontrollability as discussed earlier in this chapter, this can actually be seen as a way of reducing\nthe power of the self. As far as the self is about control, giving up control is, figuratively speaking, giving up part\nof the self. More precisely, it is rejecting part of the power that self-centered processing has on us.\nAnother approach is limiting the number of things that belong to “myself”. Typically, I would consider that\na number of things belong to me: perhaps my family, my home, my job, and so on. If I think of them as “mine”,\nI invest them with a certain power because I think I should be able to control them, as well as keep them intact.\nIn other words, I think that they are in a sense part of myself; some would say I “identify” with them. Then,\nif anything bad happens to them, or anybody tries to take them away from me, I will have a strong negative\nemotion as if my self were threatened—and in a sense the intactness of my person or self is threatened.\nIt is clear how one can reduce suffering coming from such possessions: as a first approach, just own fewer\nthings. If you have very few things that you consider yours, it is less likely that you will experience them breaking\ndown, being stolen, or getting lost. Many spiritual traditions do recommend giving up most of your material\npossessions. Further, you can try to change your attitude towards such external parts of yourself. Epictetus\nproposes that you should think of all your possessions, your family, and so on, as not really belonging to you,\nbut as things that have been temporarily lent to you:31\nNever say of anything, “I have lost it”; but, “I have returned it.” Is your child dead? It is returned. Is\nyour wife dead? She is returned. Is your estate taken away? Well, and is not that likewise returned?32\nFinally, the reduction of self can be approached from the viewpoint of reducing thinking in terms of categories.\nTypically, I divide the world into things that are part of myself and things that are not part of myself. This is\nhow I construct the category “self”. Like with other categories, it would be useful not to take this category\ntoo seriously, and understand its fuzziness and arbitrariness, or emptiness. “Self” can be seen as the ultimate\ncategory that should be deconstructed and given up. Such giving up of the whole category of self, in a sense,\nencompasses all the other aspects of no-self philosophy described above. If the very category of self does not\nexist, or, to put it simply, if self does not exist, what would be the point in self-preservation or self-evaluation,\nor any attempt to control? Any such self-related thinking should vanish if the underlying category of “self” is\ngiven up. The Buddha said that when a monk is advanced enough, “any thoughts of ‘me’ or ‘mine’ or ‘I am’ do\nnot occur to him”.33 This is the most general way of reducing suffering based on no-self philosophy.34\n31The Enchiridion, Paragraph 11.\n32In Discourses, III.24.85, Epictetus provides an “anticipatory” version of this: “[I]f you kiss your child, your brother, your friend, (...)\nkeep reminding them that they are mortal. In such fashion do you too remind yourself that the object of your love is mortal; it is not\none of your own possessions; it has been given you for the present, not inseparably nor for ever, but like a fig, or a cluster of grapes, at\na fixed season of the year, and that if you hanker for it in the winter, you are a fool.” See also Samyutta Nikaya 22.33, where the Buddha\ntakes this approach to the extreme in the sense that he recommends abandoning everything, including your own body and any aspects\nof your mind.\n33Samyutta Nikaya 35.205\n34Epictetus takes a rather different viewpoint on self. According to Graver (2021), for Epictetus “it is the volition [i.e. prohairesis, or\nwill] that is the real person, the true self of the individual”. Interestingly, Epictetus explicitly claims that such a conception of self is\nuseful for mental training: “It is a universal law (...) that every creature alive is attached to nothing so much as to its own self-interest.\n(...) Wherever ‘me’ and ‘mine’ are, that’s where every creature necessarily tends. If we locate them in the body, then the body will be\nthe dominant force in our lives. If it’s in our faculty of will, then that will dominate.” (Discourses, II.22.15 and 19; see also III.1.40). It is\nprecisely the will that is the target of Stoic mental training, so according to this logic, it it useful to think that the will is my self.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n186\nReducing desire and aversion\nWhile so far we have focused on reducing the frustration of desires, many philosophical traditions propose\nthat desires themselves should be reduced—as always, this includes aversions. In Buddhist philosophy of the\nTheravadan school, it is traditionally the main focus of the training, and it is the main point of the Buddha’s\nteaching as expressed in the Four Noble Truths. After describing what suffering is (quoted on page 20), he\nproposed that it is born of desire, and that one can be liberated from suffering by eradicating desire by following\na path of meditative and other practices.35 Epictetus was equally clear about the importance of not having\ndesires or aversions, especially towards things we cannot control:36\nRemove aversion, then, from all things that are not in our control (...) But, for the present, totally\nsuppress desire: for, if you desire any of the things which are not in your own control, you must\nnecessarily be disappointed; and of those which are, and which it would be laudable to desire,\nnothing is yet in your possession.37\nHumans can indeed reduce frustration simply by giving up some unnecessary goals: you don’t really need a\nfancy car. It is possible to consciously decide not to strive for certain goals, and we can modify our desires to\nsome extent without any special techniques. In our framework, this in particular means reducing intentions,\ni.e. commitment to plans, also called attachments in Buddhist terminology. Intentions can, in fact, be easier\nto reduce than desires themselves, as may be intuitively clear and will be discussed in more detail in the next\nchapter. Suffering will then be reduced since if there are no desires and no goals that need to be achieved,\nfrustration will not appear, and neither will suffering. As such, reduction of desires is a central mechanism\nthrough which reduction of frustration is possible.\nMany ideas in this chapter can be seen as mental techniques serving the very goal of reducing desires. Con-\nsider, for example, reducing expected rewards as considered above: why would the agent want anything if it has\narrived at the conclusion that the expected rewards are zero, or very small? Likewise, desires will be reduced by\nadopting the belief that many desires are pointless and even bad for you, they are just evolutionary obsessions.\n35For completeness, I will briefly describe the Buddha’s Four Noble Truths in their entirety. They can be seen as a psychological theory\nof why suffering comes about and how it can be avoided. The four truths are a logical sequence: 1) All phenomena (i.e. external objects,\nperceptions, feelings, thoughts, etc.) in the world are unsatisfactory in the sense that they have the potential to produce suffering. 2)\nSuffering is produced by desire for any of these phenomena (or desire to avoid any of them, i.e., aversion). 3) Suffering disappears if\ndesire is eradicated. 4) Desire can be eradicated by following a certain combination of meditation techniques, philosophical attitudes,\nand ethical behavior. (For references, see footnote 20 in Chapter 2.)\n36The Enchiridion, Paragraph 2; see also Discourses, I.4.\n37Even Socrates is claimed to have said that “You seem (...) to imagine that happiness consists in luxury and extravagance. But my\nbelief is that to have no wants is divine; to have as few as possible comes next to the divine” (Xenophon, Memorabilia 1.6.10, trans. E. C.\nMarchant). Other schools of Hellenistic philosophy had a very similar attitude. According to Long (2006, p. 13), a Pyrrhonian (Skeptic)\nwill not “decline or choose” since desire is “the first of all bad things”. Reducing desires was recommended even by Epicurus, who seems\nto have been seriously misunderstood (Seneca describes in On a Happy Life, XIII how Epicurus was misunderstood already in ancient\nRome). Epicurus proposed that there are a few desires which need to be satisfied since they are both natural and necessary: Food,\nwater, and shelter; these desires are also easy to satisfy. In contrast, desire for money, power, fame etc. are unnatural and unnecessary;\nthey are also insatiable. Optimal “pleasure” is obtained by rejecting desires which are not natural and necessary. See Epicurus’s Letter\nto Menoeceus, or Hadot (2002, p.34); Konstan (2018). Likewise, Seneca’s Letters to Lucilius, 21.7-8 proposes that the best way to increase\npleasures is to reduce desires, while Letters to Lucilius, 16.7-9 considers the insatiability/satiability (or satisfiability) distinction in more\ndetail. Irvine (2005) provides a modern account of the Stoic position.\n\nCHAPTER 16. REPROGRAMMING THE BRAIN TO REDUCE SUFFERING\n187\nAs such, reducing desires is closely related to the earlier ideas of facing uncertainty,38 uncontrollability, and\nunsatisfactoriness, and in fact, in a traditional Buddhist account, the main justification for such philosophical\nattitudes is precisely that they reduce desires.39\nThere are also special techniques to reduce desires. One example is choosing to pay attention to good\nthings that one already has, instead of things that one might obtain. This reduces desires and the tendency\nof insatiability; it is central in mental exercises based on gratitude, which will be treated in more detail in\nChapter 18. Reduction of desires is also facilitated by a simple, possibly ascetic lifestyle where there are fewer\nstimuli, or “temptations”, that might elicit desires. A fashionable example is taking a break from the internet\nor social media; the monastic rules of Buddhist monks and nuns give a more radical example. Epictetus also\nproposed a rather extreme form of training for this end, namely contemplation of death:40\nLet death and exile, and all other things which appear terrible be daily before your eyes, but chiefly\ndeath, and you will never entertain any abject thought, nor too eagerly covet anything.41\nYet, there are also desires that are really “hot”, hard-wired, and difficult to modify, let alone reduce, based\non the rather purely philosophical or intellectual considerations presented in this chapter. What is needed are\nspecial techniques that work on deeper levels of the mind than philosophical thinking. Meditation is one such\nmethod, as we will see in the next chapter.42\n38For the Pyrrhonian Skeptics, recognition of the uncertainty and fuzziness of sensory evidence was the main method for reducing\ndesires. After contemplating the conflicting evidence in different perceptions and inferences, as described on page 135 in Chapter 12,\nthey eventually find an “irresolvable conflict because of which [they] are unable to choose or reject” (Sextus Empiricus, Outlines of\nPyrrhonism, 1.165, as quoted by Long (2006, p. 48)). Thus desire and aversion are extinguished. But paradoxically, uncertainty can\nsometimes lead to more vigorous action. Uncertainty about the availability of food increases foraging behavior in mammals, perhaps\nbecause they decide to hoard food or fatten themselves to protect themselves against such unpredictability (Anselme and Güntürkün,\n2019). Even in human subjects in one economic experiment, uncertainty increased motivation (Shen et al., 2015).\n39Let me try to make the links to the other ideas in this chapter explicit. First, reduction of certainty attributed to perception reduces\ndesires since you don’t actually know for sure whether the object of your desire really gives reward—or is even there. Second, if you\ncannot control anything, what would be the point in wanting, let alone planning, since rewards and goals cannot be attained? Seeing\nthe insatiability of the desires should also lead to the conclusion that their total fulfillment is impossible in the long run, so the desires\nshould be dropped as futile; seeing desires as evolutionary obsessions means realising they can even be bad for you. Self-needs, in\nparticular self-evaluation, can be considered as forms of desires in this context, and the same ideas apply to them.\n40The Enchiridion, Paragraph 21\n41Likewise, Seneca (Letters to Lucilius, 49.10) proposes “Say to me when I lie down to sleep: ’You may not wake again!’ And when I\nhave waked: ’You may not go to sleep again!’. It is easy to see why contemplation of death would reduce desires, and in particular plan-\nning and intentions. Presumably, contemplation of death reminds you that you just might die tomorrow, even if that is not very likely.\nBy some kind of availability heuristic (see footnote 14 in this chapter), that reminder will increase your estimate of the probability of\ndying soon, which implies that you don’t have much time left to obtain rewards. So, their expected value is low, and any planning is less\nuseful and highly restricted by this time horizon. Buddhist practices also include contemplation of death; it may serve slightly different\npurposes (An¯alayo, 2003, p. 155), but the classic manual Visuddhimagga (Chapter VIII, 41) links it directly to “disenchantment” and\n“conquer[ing] attachment”. Nevertheless, some psychological research based on the Terror Management Theory claims that remind-\ning people of their mortality may, in fact, increase their willingness to consume (Kasser and Sheldon, 2000); see also Burke et al. (2010);\nGao et al. (2020). Frias et al. (2011) propose a model to understand why such quite opposite effects can be observed, and how exactly\nthe death reflection should be done to increase gratitude and reduce greed.\n42In addition to reducing desires, Stoics proposed another approach to working with desires. It is also possible to align one’s desires\nwith what can be more easily achieved in the world, instead of eliminating them. Epictetus takes this idea to the extreme by suggesting:\n“Don’t demand that things happen as you wish, but wish that they happen as they do happen, and you will go on well.” (The Enchirid-\nion, Paragraph 8) If the only thing you want is that things happen as they do happen, how could there be any frustration? Your wishes\nwill always be fulfilled.\n\nChapter 17\nRetraining neural networks by meditation\nThe preceding chapter presented several directions in which information-processing should be changed to\nreduce suffering. We also saw some practical suggestions for reprogramming, such as seeing the uncertainty\nand uncontrollability of the world and reducing desires and self-needs. This will eventually lead to a reduction\nin reward loss, frustration, and suffering.\nYet, the account of the preceding chapter may be rather unsatisfactory for some readers: It seems to be\nasking the impossible, at least in the case of mere humans. The goal is to change some fundamental beliefs\nabout the world and your mind. How is one supposed to become so thoroughly convinced about, say, the\nuncontrollability of the world that one is not disturbed by the loss of, say, one’s job or house? Is it not simply\n“human” to think otherwise? How can you actually reduce expectations of rewards, belief in the certainty of\nperceptions, and so on?\nCrucially, what we need are changes in neural associations that work on an unconscious level, and that is\nnotoriously difficult. We need to develop feasible and practical methods for retraining the neural networks in\nthe human brain.\nIn this chapter, I consider meditation, or mindfulness training, as a method that can radically boost re-\ntraining of neural networks, compared to straightforward attempts to change thinking at the conscious level.\nIt also turns out that meditation has further benefits, such as reducing “hot” desires, reducing simulation, and\ndeveloping metacognition. I will not go too much into the practical details of any such training methods, on\nwhich hundreds of manuals have already been written. Rather, I discuss general principles on how they work,\nlargely interpreting them in the information-processing framework of this book.\nFirst, I discuss how meditation can be seen to speed up learning from new input, thus enhancing the meth-\nods of the preceding chapter. Second, meditation can be seen as reducing two terms in the frustration equation\non page 172 that we did not yet consider: the number of times the reward loss is simulated and the amount\nof attention paid to reward loss; these are related to the top-row green boxes already shown in the flowchart\nin Figure 15.1. In fact, emptying the mind by meditation clearly reduces simulation, and meditation almost\ninevitably seems to develop a metacognitive attitude, which changes the attention paid to reward loss. A third\nmajor benefit of meditation is that it enables stopping the processing chain in the flowchart in Figure 15.2 by\nincreasing conscious control over interrupting desires.\n188\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n189\nContemplation as active replay\nThe fundamental problem with the approach of the preceding chapter is that a conscious decision to think in\na different way often has little effect on what unconscious neural networks do. A conscious decision may not\neven really change future conscious thinking since it may be overridden by the unconscious networks. That is\nwhy reprogramming of the brain must include some kind of retraining of the unconscious neural networks.\nFrom a dual-process perspective, the problem to be solved here is how the conscious-symbolic-explicit sys-\ntem can change a mental association that is actually encoded in both the two systems. For example, it might\ntry to create an association between “I” and “impermanent”, being inspired by classical Buddhist philosophy.\nHowever, what really matters is changes in the unconscious-neural-implicit system because that system com-\nputes values, expected rewards, and reward losses. So, how can the explicit system force a change in the implicit\none? Transfer of knowledge or learning between the two systems is difficult. While you may have a clear un-\nderstanding that everything is impermanent on a conscious level, it is not easy to transfer this understanding\nto the unconscious neural networks.\nAs a first approach, we could use techniques that I here call contemplation. That means a constant con-\nscious repetition of selected thoughts. For example, it can be contemplation of the characteristics of imper-\nmanence, uncontrollability, and unsatisfactoriness, possibly combined with some object—such as “I” or my\n“self”—whose impermanence or other property one wants to learn. The constant repetition of such thoughts\non a conscious level should slowly modify the unconscious associations used to compute the perceptions and\nreplay. Some kind of Hebbian learning is likely to construct an association between the different concepts, such\nas “I” and “impermanent”, even on the basic neural level. Reading books on Buddhist or Stoic philosophy, as\nwell as later thinking about their contents, can also be seen as contemplation.\nThe mechanism working here is what I call active replay: The explicit system uses the mechanism of ex-\nperience replay (see Chapter 11) to make the implicit system learn whatever the explicit system wants. That\nis, the explicit system in your brain can select thoughts in the form of linguistic sentences or visual images of\nevents—possibly imaginary—and replay them. It can do that repeatedly, thus replaying selected items many\ntimes. Such replay will change your neural networks—that is in fact the very point in replay. What is special\nhere is that the explicit system chooses what to replay, thus “teaching” the neural networks, while in ordinary\nreplay, the material would be selected by the implicit system itself.\nSuch training may seem rather different from modern meditation instructions, but it seems to have been\nan essential form of practice in the Buddha’s times and emphasized by some modern Buddhist meditation\nteachers as well.1 When the Buddha was asked for meditation instruction by monks entering a solitary retreat,\nhe would often tell them to contemplate on impermanence, no-self, or unsatisfactoriness, sometimes linking\nthem all together in various causal chains. For example, he would advise:\nYou should abandon desire for whatever is impermanent. And what is impermanent? The eye [and\nvisible forms etc.] is impermanent; you should abandon desire for it.2\nForms [i.e. anything that is seen] are impermanent. What is impermanent is suffering. What is\nsuffering [i.e. unsatisfactory] is nonself. What is nonself should be seen as it really is with correct\nwisdom thus: “This is not mine, this I am not, this is not my self.”3\n1See e.g. An¯alayo (2003, p.103-104); Mahasi (1996)\n2Samyutta Nikaya 35.76, translated by Bhikkhu Bodhi; see also Samyutta Nikaya 35.32; Samyutta Nikaya 35.162\n3Samyutta Nikaya 35.4, translated by Bhikkhu Bodhi. See also Williams (2008b, p.79) for a description of similar practices in a\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n190\nWe do not know much about the details of how such contemplation was practiced in the Buddha’s times.\nThe fundamental problem with such simple contemplation is that the learning process can be very slow\nand inefficient. One reason is that it has the same characteristics as learning in neural networks in AI. As we\nsaw earlier, neural network learning is incremental: it requires a large number of repetitions of input, which\nchange the neural connections little by little, using some mechanism related to stochastic gradient descent or\nHebbian learning. So, retraining neural networks by contemplation requires a huge number of repetitions.\nMoreover, transferring learning from the explicit to the implicit system is hampered by the fact that the\nrepresentations and computations in the two systems can be quite different, as we have already discussed.\nSuppose that your explicit system repeats the word “impermanence”, in an attempt to contemplate on that\nproperty. How are your primitive, lizard-level neural networks supposed to understand what that means? Such\nneural networks do not operate with words or abstractions but on representations related to sensory input.\nThere is a kind of communication barrier between the two systems, and contemplation will have difficulties in\ncrossing it.\nThe situation can be somewhat improved if the explicit system imagines events or episodes and replays\nthem as real sensorial input such as images, instead of merely in verbal and abstract form. When you read a\nstory or a simile in Buddhist literature and vividly imagine it happening, that does provide more natural input\nto your neural networks. Or, the explicit system can imagine detailed episodes of future events, for example\nfrom the viewpoint that an action plan is likely to produce frustration, as in Epictetus’s Roman bath example\n(page 178).4\nMindfulness meditation as training from a new data set\nA crucial improvement to such contemplation practices is what is called meditation in the modern context.\nMindfulness meditation in particular is a technique that can influence neural networks more efficiently than\nsimple contemplation. Mindfulness meditation can incorporate many of the goals described above, such as\nrealizing uncertainty and uncontrollability.5\nTypical instructions of mindfulness meditation emphasize objective observation of any contents that ap-\npear in your mind, that is, mental phenomena. In particular, that encompasses anything that your senses\nperceive, including the “internal sense” of thinking and imagination. If you hear something, you acknowledge\nhearing it; if there is a bodily feeling in any part of your body, you recognize that you have a bodily feeling,\nand so on. Such observation is done, as far as possible, passively without interfering with the sensory process\nor the physical source of the perceptions (for example, without moving your body to change bodily feelings).\nThe contents should be observed from an external perspective, as if from a distance, and without judging the\nMahayana context.\n4A related well-known Stoic exercise is a bedtime recollection of what you did during the day (Epictetus, Discourses, III.10.3; Seneca,\nOn a Happy Life, VI). While this may be difficult to understand or justify in our framework, one effect seems to be to reduce experience\nreplay (presumably ordinary events, not philosophical ideas) during sleep: \"Oh the blessed sleep that follows\", exclaims Seneca. This\nexercise seems to be a case of deliberate replay, but of a very different kind from what we discuss in this chapter.\n5For introductory books to mindfulness meditation, see e.g. Gunaratana (2011); Kabat-Zinn (2012); for an attempt at a definition,\nsee Bishop et al. (2004). In this book, the term mindfulness always refers to mindfulness meditation, often seen as a training or a\nlearning process (instead of a state of mind, or a long-term psychological trait, which are alternative uses of the term). In terms of the\ntypology of meditation practices discussed by Lutz et al. (2008); Dahl et al. (2015), what is emphasized here is the “open monitoring”\naspect of the practice. Meditation is thus also virtually synonymous with such terms as insight meditation or vipassana in this book.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n191\ncontents to be either good or bad.\nThere are a number of techniques to make such observation easier, based on regulating the attention of\nthe meditator. Basic meditation instructions typically start by recommending sitting in a comfortable posture\nand then provide one particular technique for attention regulation. A very well-known technique is focusing\nthe attention on observing the breath, possibly reinforced by mentally counting the breaths. (Alternatively, the\nfocus might be a visual target, or a particular word or phrase that is mentally repeated.) Such observation of\nbreathing should be seen as simply a technique whose goal is to enable better observation of the myriad mental\nphenomena, and indeed simply counting breathing may sound like an absurd exercise if the actual purpose is\nnot understood. Such attention regulation facilitates the observation of mental phenomena by making the\nmind relatively empty; observing mental phenomena is very difficult if the mind is full of different kinds of\nthoughts and perceptions. Furthermore, emptying the mind has several direct benefits as well, in particular\nreduction of simulation as discussed below.\nThe exact mechanisms of mindfulness meditation are far from being understood, but some of them can be\nunderstood by the framework presented in this book, as we will see next.6\nDirect input to train neural networks\nThe most crucial mechanism at play may be that the meditator learns largely the same things as in the con-\ntemplations above but in a more efficient way. I suggest the reason why meditation is more efficient than what\nI called active replay above is that there is no longer any need to transfer information between the two systems\n(conscious thinking and neural networks, roughly speaking). Instead, the practitioner observes characteristics\nsuch as impermanence first-hand, in real sensory input or imagined sensory content. Then, neural network\nlearning can proceed in a completely natural manner, largely bypassing linguistic constructs and conceptual\nthinking.\nIn other words, during meditation, the sensory systems directly perceive how things are. For example, they\nare seen as impermanent by observing how those things change and disappear. Thus, the neural networks\nlearn directly from such natural input. This is in stark contrast to contemplation, where the difficult part is\nto transform concepts and words into something that can train neural networks, and replay does this in a\nsomewhat contrived way. Neural networks learn best from real sensory input, so it is crucial here to enable\nthem to do exactly that. Such observation is eventually extended to all the characteristics discussed in the\npreceding chapter.\nThe key trick here is to select the right data to input into the neural networks. As discussed in Chapter 12,\nselection of input data is an essential part of the perceptual system, in terms of the multi-faceted phenomenon\ncalled attention. That is why regulation of attention is a central part of any meditation method: in mindfulness\nmeditation, you typically start by focusing your attention on observing your breath. It is in fact possible to get\nuseful input data from the breath itself, if you do it with a special kind of attentional focus. While the practice\nmay start by simply observing the breath in a general manner, eventually, you can start observing its specific\naspects in light of the theory of the preceding chapter. For example, you observe the impermanence of breath,\n6For previous proposals and reviews on the mechanisms of mindfulness, see e.g. (Hölzel et al., 2011; Grabovac et al., 2011; Williams,\n2008a; Shapiro et al., 2006; Teasdale and Chaskalson, 2011b; Vago and David, 2012; Baer, 2003; Garland et al., 2014; Gerritsen and Band,\n2018; Wielgosz et al., 2019; Brandmeyer and Delorme, 2021). My account here is particularly computational. Initially, it emphasizes\nthe learning of new attitudes that could be called philosophical, but later in the text, other goals will be considered as well.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n192\nhow it is changing all the time from an in-breath to an out-breath—this is a classical Buddhist exercise. That\nmeans your attentional system selects your sensory input to consist of observations of your breath, and more\nprecisely, any aspects of your breath related to permanence or lack of it. This is how your neural networks get\na lot of good data pertaining to that particular property, and they learn to perceive the impermanence much\nbetter than they would by any kind of abstract contemplation based on linguistic concepts.7\nThus, the explicit system in a sense “teaches” the implicit system, and the teaching happens by means of\nthe attentional system. The direction of attention is, to some extent, under conscious control. So, the symbolic\nor thinking part of the brain can just tell where the implicit system should be looking—this is only partly a\nfigure of speech— and it does not need to really input anything to the implicit system, unlike in the case with\nreplay. It is a bit like a professor telling students to read a book; she does not then need to give a lecture herself.\nRealizing how the mind wanders\nAnother important example of such direct input is observing how often and easily the mind starts wandering.\nAs we have seen in Chapter 11, sustained attention is difficult, and after a while, the mind often starts wander-\ning, and various daydreams fill the mind. Frequent occurrence of such mind-wandering is extremely salient\nto anybody who tries to focus on breathing or a similar meditation object. Realizing how difficult it is to focus\non breathing gives a direct view into how uncontrollable the mind is. If you systematically observe how au-\ntomatically your mind starts wandering, you will gradually be convinced—and so will your neural networks—\nthat you cannot control even your own thinking, at least not completely. After all, wandering thoughts are, by\ndefinition, a failure of controlling your mind.\nSuch observation may also convince you that there is no self, no central executive, and perhaps then no free\nwill. Under ordinary circumstances, if I decide to plan what I will do tomorrow, I may have a clear feeling that it\nis “me” who is doing the planning. However, after observing how planning happens automatically in wandering\nthoughts, I may be forced to admit that the plans are something that “I” did not create. You may even start\nhaving doubts about the correctness and certainty of your thoughts, since they seem to be something that just\nappears in the mind, and you have little idea why they appear or where they come from. Thus, uncertainty\nabout your thoughts can be taught to the neural networks as well.\nEfficient reduction of expectations\nTo interpret such learning processes in the framework of the frustration equation, what happens is that the\nunconscious neural networks themselves—and not just the conscious and/or symbolic thinking systems—will\nlearn to reduce the expectations of any rewards. This happens through your neural networks learning that the\nworld is uncontrollable and uncertain (or impermanent), which necessarily reduces their expectation of future\nrewards according to the logic of the preceding chapter.\nMany further meditation techniques can be seen as such attentional selection of input, with different tar-\ngets for the learning. One classical Buddhist technique is to focus on the ending of any pleasurable feeling.\nThis enables seeing first-hand how pleasure, and in general any effects of rewards, are fleeting and worth less\nthan might be expected. Thus, you will learn the impermanence and, in particular, the unsatisfactoriness of all\nmental phenomena in a particularly efficient way.\n7Such selection of input can be further improved by controlling one’s media consumption as well as by choosing a suitable lifestyle\nand social environment. Buddhist monastic training provides a rather extreme example of such choices.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n193\nExtinction of aversive responses\nIn a slight variant of the logic above, mindfulness meditation can also help directly change associations related\nto specific emotions. An important example is fear extinction. Extinction is the opposite of classical condition-\ning: It means that when the predictive stimulus (e.g. the bell for Pavlov’s dog) is presented without the other\nstimulus (the food for the dog), the conditioning weakens. If the bell is presented without the food many times,\nthe dog learns that the bell does not predict the food anymore, and the conditioning is eventually extinguished.\nSuppose you have learned to associate a fear reaction with your boss by classical conditioning. Perhaps that\nwas based on a single episode, and the association is not valid anymore, so it would be nice to be able to let\nsuch a fear reaction be extinguished. Unfortunately, extinction is often very slow—just like any neural network\nlearning—but this can be improved by mindfulness training. The trick here is that you create completely new\ndata, going beyond simply selecting input from existing data as above, but still feed it directly into your neural\nnetworks.\nIt turns out that mindful meditation tends to make people relaxed and feel good (possible reasons for this\nwill be discussed later). So, if you recall the unpleasant episode with your boss many times, but always stay in\nsuch a pleasant, calm, meditative state, extinction is more likely to happen. Thoughts about unpleasant situ-\nations will be increasingly associated with a general feeling of calm; the image of your boss will be associated\nwith relaxation and feeling good in the whole body. This will help override the fear association.8\nSpeeding up the training\nUnfortunately, such meditation training is still rather slow, even if it improves on simple contemplation. In\nfact, slowness of training is a ubiquitous problem with neural networks due to the incremental nature of their\nlearning, as already pointed out in Chapter 4. Even though in mindfulness meditation, we have a new source\nof more direct and natural data for learning, neural networks still need large amounts of input data, and a lot of\nmeditation practice is needed. Fortunately, the amount of training and effort required can be further reduced\nby further techniques.\nIncreasing the plasticity of the brain\nOne central principle here is increasing the plasticity in the brain. Plasticity is the biological term for the capac-\nity of neural connections to change and thus to learn. Plasticity in the brain’s neural networks is by no means\ngranted, nor is it a constant quantity. If, with some suitable tricks, such learning capacity could be increased,\nthe learning process would take less time. A large amount of neuroscience research has been dedicated to\nfinding different ways to increase plasticity.\nSensory deprivation seems to be one useful trick; it has indeed been shown to increase plasticity, at least\nin rats and cats. It may be rather common sense that if your brain has had little stimulation for a while, it\nwill better concentrate on any new task; it turns out that its learning capacities are also increased.9 Mindful-\nness meditation in itself can be seen as imposing sensory deprivation, since it is usually conducted in a quiet\n8On the general idea of extinction by mindfulness and exposure, see (Baer, 2003; Hölzel et al., 2011); on relaxation and positive affect,\nsee (Carmody, 2015). An interesting question here is whether extinction needs attention and/or awareness, and would thus be greatly\nfacilitated by mindfulness; the results are not very clear-cut on this point (Kwapis et al., 2015; Han et al., 2003; Weike et al., 2007).\n9(He et al., 2006; Duffy and Mitchell, 2013)\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n194\nenvironment with eyes closed, or at least there is nothing much happening in the visual field. In some med-\nitation schools, an even stronger form of sensory deprivation may be imposed in the form of silent retreats.\nSuch retreats often entail minimization of any kind of sensory stimulation: the participants don’t go out of\na prescribed enclosure, they don’t watch TV or use the internet, and obviously they don’t talk to each other.\nIn several discourses, the Buddha recommended such deprivation, together with meditative concentration,\nbecause it makes the mind “pliant” and “malleable”. Then, the meditator is better able to gain insight into, for\nexample, the uncontrollability and uncertainty of existence, as well as better able to learn from those insights.10\nPlasticity can further be increased by restriction of food intake, which is another typical characteristic of\nascetic training in spiritual traditions. Paradoxically, it can also be increased by the very opposite of sensory\ndeprivation: enriching the environment. In animal experiments, that might mean allowing the animals to live\nas groups in large, spatially complex cages, equipped with toys and running wheels. In humans, similar results\nare obtained by aerobic exercise, as well as action video game playing. Whether such methods could be used\nto improve meditation practice is a very interesting question for future research.11\nPlasticity can also be increased by drugs, such as the antidepressant fluoxetine (aka Prozac). A large amount\nof research is currently being conducted on new drugs that would increase plasticity even more, and with\nminimal side effects. The huge impact such drugs could have on society is obvious.12\nIn fact, you may be wondering why plasticity is such a bottleneck: Why hasn’t evolution made our neural\nnetworks learn much faster? The reason seems to be that some limitation of plasticity in the brain is useful\nto prevent new information from overwriting old information too easily.13 So, it may not be wise to increase\nplasticity too much, because it could lead to too much forgetting of previously learned information. This is\nhardly a problem with meditation-based interventions, but with drugs, such negative side-effects might be\nreal.\nIn principle, an AI has much more freedom in how it changes the results of its learning, and the amount of\n“plasticity” could be made infinite by design. Thus, an AI could get rid of a bad habit or a harmful association\nin a split-second, by just removing or changing some connections in its neural network. However, this may not\nbe as easy as it sounds, since just like with humans, there may be a risk of interfering with other connections\nso that the AI may forget useful information. Also, it may not be clear which connections should be changed\nin the first place, due to the overwhelming complexity of neural networks. So, even in the case of an AI, it may\nbe better that all the training happens by simply inputting carefully selected data into the system and patiently\nwaiting until the learning happens.14\n10Majjhima Nikaya 36; Digha Nikaya 2. Sensory deprivation may seem to be contradictory with the idea of learning from new input,\nbut the deprivation mainly refers to sensory input while wandering thoughts etc. are still running and provide input for the learning;\nmoreover, sensory input is never zero anyway since the input from bodily senses (proprioception, interoception, pain perception)\nis hardly reduced. Presumably, such sensory deprivation also reduces the capture of attention by sensory stimulation and enables\ndirecting the attention to those phenomena that the learning needs as input (e.g. wandering thoughts).\n11Increase in plasticity due to food restriction: (Spolidoro et al., 2011); environmental enrichment: (Sale et al., 2007); exercise and\ngames: (Bavelier et al., 2010; Nokia et al., 2016). See also Kirste et al. (2015) who show how both silence and unusual noise can increase\nneurogenesis in the hippocampus (and thus plasticity), which they assume is due to the novelty of both conditions.\n12(Vetencourt et al., 2008; Castrén and Antila, 2017; Ly et al., 2018)\n13(McCloskey and Cohen, 1989; Bavelier et al., 2010; Pascual-Leone et al., 2005; Kirkpatrick et al., 2017). Furthermore, too much plas-\nticity might destroy the stability of the brain as a dynamical system, even leading to such phenomena as epileptic seizures (Kozachkov\net al., 2020).\n14Another way of improving learning would be to adapt the contents of the contemplation to each individual based on their person-\nality and temperament. While this is rarely done in a Buddhist context, the classical Buddhist meditation manual Visuddhimagga, for\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n195\nTraining can become automated\nAnother major difficulty in meditation training is sustaining attention in the way typical meditation techniques\nrequire. I need to emphasize that we actually have two different attentional mechanisms at play here. First,\nthere is sustained attention to the task at hand, meaning that you concentrate on meditation and don’t think\nabout anything else, as explained in Chapter 11. Second, there is sensory, selective attention, which means\nyou select certain data as input to sensory processing, as originally explained in Chapter 12 and extensively\ndiscussed earlier in this chapter. Both are necessary for successful meditation. However, sustained attention\ntends to be the major bottleneck because it is notoriously difficult to maintain.\nIn previous chapters, we have actually seen several reasons why sustained attention is difficult. First, wan-\ndering thoughts assail the mind, for example due to experience replay. But we also saw that emotions are\nessentially interrupts; what they are interrupting is current activity, and to do that, they have to be able to grab\nattention away from wherever it may be. The general concept of the brain as parallel distributed processing\nemphasizes the idea that there are different networks or modules which are often competing, for example, for\nattention and the control of attention.\nFortunately, it is possible to learn to use your attentional capacities better.15 This is yet another form of\nlearning, but a bit different from the learning we have considered in this chapter: here we are talking about\nlearning a new skill, as briefly described in Chapter 8. A skill means that you know how to ride a bicycle, to\nspeak a foreign language, or to use your new smartphone; it is opposed to learning facts and increasing your\nknowledge about what the world is like. Skill learning follows some general laws and these apply to meditation\nas well. As we all know, you need to practice. At the beginning of the practice, you need to concentrate and\nspend a lot of effort, meaning a lot of sustained attention. The important point here is that with practice,\nmeditation becomes more and more automated, which means that less and less conscious effort is needed.\nSome meditation traditions talk about meditation as “just sitting”, which is in a sense enough if the meditation\nis sufficiently automated. Importantly, the regulation of attention will in fact become a habit, and will be easily\nconducted during ordinary life, as if by itself, even outside of formal mindfulness meditation sessions.\nSo, there are actually two different learning processes at play: Learning that the world has certain char-\nacteristics (such as uncontrollability), and on a higher level, “learning to learn” that the world has such char-\nacteristics. The latter learning process means learning to meditate in an automated, habit-like manner, with\nminimum conscious effort. Thus, with practice, the meditator will be able to perform the former learning pro-\ncess with increasing efficiency, and this former process is the one that reduces suffering according to the theory\nof the preceding chapter.\nBut who is actually meditating?\nThe fact that meditation can become automated and habit-like means that, in a sense, it is no longer my “self”\nwho is meditating. We find echoes of the no-self philosophy treated in Chapter 13. Some neural networks will\nbe able to observe the breathing without any conscious effort, or even without a conscious decision to start\nmeditating. There is no need for any central executive to make any decision, and no need to want to observe\nexample, does include such instructions (Chapter III, 74).\n15(Friese et al., 2012; MacKenzie and Baumeister, 2015). Such learning has earlier been well-documented on a more general level in\nthe work on self-control (Rueda et al., 2004; Baumeister et al., 2007). It can be seen as another interaction between the two systems in\ndual-process theory (explicit and implicit), see also (Doyon et al., 2003; Peters et al., 2011; Sun et al., 2005).\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n196\nthe breath; it just happens. It is like when walking, you make no conscious decision to move your feet; you feel\nno burning desire to put one foot in front of the other.\nBut if the neural networks are retrained by the explicit system as I argue in this chapter, does that not mean\nthat it is the explicit system, perhaps even a conscious self, which is in control? That might be a hasty conclu-\nsion since there are so many complicated ways in which the two systems interact. In fact, earlier (page 149) I\nargued that it is meaningful to say that ultimately, it is the input data that controls us. I gave the example of a\nmeditation master who says that it is actually his master who is meditating, because he still hears his master’s\nvoice in his head. This shows that in order to find the “ultimate” source of control, we have to consider where\nthe data to the explicit system comes from. Part of it clearly comes from human society and the cultural con-\ntext: there are other people that input data into us, for example in the form of meditation instructions. How\nthat happens, and who is controlling whom, is a vast topic that I have to leave for future research.16\nReducing interrupting desires\nIn addition to speeding up learning in neural networks, mindfulness meditation has further benefits. Next, we\nconsider how it reduces suffering from the viewpoint of cognitive dynamics, which complements the frustra-\ntion equation. As we saw in Chapter 15, one traditional Buddhist account of a mechanism to reduce suffering\nis based on the moment-to-moment cognitive chain or cycle shown in the flowchart in Fig. 15.2. The idea here\nis to stop the dynamic process in the flowchart in the middle so that it does not lead to its end product, which\nis suffering. The point where the process can best be stopped is assumed to be (in the terminology of our\nflowchart) the three links of desire, intention, and planning.17 It is in fact assumed in early Buddhist philos-\nophy that until the valence computation, the process is too automated, and desire provides the first link that\ncan be stopped.18\nThe ensuing method is distinct from reducing desires by adopting the attitudes of the preceding chapter.\nHere, I am talking about sudden, “hot”, interrupting desires triggered by the valence computations, and their\nprevention in real-time when they are about to arise. The preceding chapter focused on reducing long-term\ndesires from the “colder” perspective of reward calculations; that will also reduce the underlying tendency for\nhot desires to arise, but it works only passively in the background.\nOne problem here is that the hot desires have the properties of interrupts, as explained in Chapter 10, which\nmeans they are strongly automated and can be quite difficult to prevent or stop. Therefore, it might be better\nto try to stop the dynamics a bit later, at the links right after desire. In Buddhism, those following links are\ncalled “attachment”, which is in our schema divided into forming an intention (i.e., committing to a goal) and\nplanning for that goal.\nWhether desire or attachment is chosen as the target, the trick here is to weaken the cognitive dynamics so\n16My arguments here are not very rigorous since the very definition of “control” is not made explicit in this book; I simply follow\ntypical common-sense usage of the word. A more detailed analysis would point out that control is a matter of degree: In the context\nof this chapter, the explicit system has only partial control of the implicit system anyway because it merely directs its attention, so the\nexplicit system is certainly not in total control in any meaningful definition of the word.\n17See e.g. (An¯alayo, 2003); note that in the traditional early-Buddhist account, these correspond to the two links of desire and attach-\nment/clinging.\n18(Mahasi, 1999, p. 89). Valence is closely related to what is called “feeling tone” or vedan¯a in Buddhist literature. I think vedan¯a can\nbest be described as the perception of valence. (Such perception requires of course some kind of computation of valence as well.) For\na Stoic viewpoint, see Discourses, II.18.15-18.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n197\nthat this largely automated chain leading to suffering fails to operate. If the desire or attachment is prevented\nfrom taking place, no goal is committed to or planned for, and no goal-oriented action is conducted. Thus, the\nwhole frustration equation above is not operating, and frustration is avoided by that route.19\nPerceptual learning\nSuch stopping of the cognitive dynamics is enabled by well-known mindfulness meditation techniques. The\npoint is to observe the cognitive dynamics repeatedly, so that one learns to introspectively detect the different\nparts of the process and discriminate between the different links, in real-time. Mindfulness meditation has\nhere the effect of training a new perceptual capacity that allows for observation of the internal mechanisms of\nthe mind.\nThis is a special case of the phenomenon of “perceptual learning”.20 Research on perceptual learning\nstarted in vision science by the discovery that it is possible to greatly enhance the performance in almost any\nvisual perception task; all that is needed is sufficient training. Improvement is possible even in tasks where\nthe limits of perception were previously thought to be set by the optics of the eye, such as the task of telling\nwhether two lines have the same orientation (angle) or not.\nIn the context of meditation, such perceptual learning allows one to observe the individual elements of\nmental processes more accurately. An important case of such learning is that it becomes possible to observe\nthe associations between phenomena. If B is associated with A, then, under ordinary circumstances, it may be\nthat the thought of A immediately and necessarily brings B into mind, and it seems that A and B are two aspects\nof the same thing. But with mindfulness training, it is possible to see how this process breaks into pieces: First\nthere is A, then the association is activated, and then B comes to the consciousness because of the association.\nThis allows one to see not only the existence but also the arbitrariness of that association. In particular, one\nis able to dissociate a desire from the stimulus that caused it, as if by creating a “space” between the stimulus\n(say, chocolate) and the desire, as well as between any further steps in the chain.\nBreaking the cognitive chain\nThis opens up the possibility of breaking the long chain leading from stimulus to suffering depicted in Fig. 15.2.\nIntrospectively, the meditators often report that it feels as if the whole process were slowed down by such\nperceptual learning. The process is also, to some limited extent, brought under conscious control. Even if\na stimulus leads to a strong valence, the ensuing desire and the following steps will not happen completely\nautomatically, but there is some space for deliberation.\nPerhaps such breaking of the causal chain is most understandable in the case of planning, which is often a\nrather conscious process, and as such, it should be possible to decide not to initiate it at all. Obviously, there is a\nstrong unconscious tendency to start planning when desire arises; it is comparable to the unconscious reaction\nto start scratching a body part that is itching. However, with practice, such an unconscious tendency can be\nweakened, inhibited, and perhaps even completely removed. That would mean not letting “attachment” arise\nin Buddhist terminology. The key is to be able to consciously recognize when the planning is being triggered,\n19Some frustration will still happen because of the habit-based system, but as argued in Chapter 3, such frustration is much weaker\nthan that coming from planning and execution of plans.\n20(Sagi, 2011)\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n198\ninstead of letting it happen automatically.21\nIt is important to achieve automatization of such mindfulness by long-term meditation practice, as de-\nscribed above. The learned and automated tendencies of observation can then create the possibility for in-\nhibiting the more innate automatic tendencies of desire and attachment. In fact, if such observation is followed\nby conscious, deliberate inhibition of desire or attachment often enough, that very action of inhibition will be-\ncome automated as well. Conscious control processes are often too slow and weak to prevent the processes\nunderlying hot desire or other interrupts, so it is really important to train the neural networks to initiate the\naction of inhibition as well. Once the neural networks have been trained to perform both the detailed obser-\nvation and the inhibition during formal meditation sessions, they may be able to transfer that skill to everyday\nlife with its infinite temptations.22\nWhile inhibiting desire and attachment is emphasized in Buddhist training of the Theravadan school, this\nis not the only way to break the chain. Even if planning happens, and even if a plan is executed, the dynamics\nmight still be stopped later, for example, right before error computation. In that case, the computations neces-\nsary for frustration simply do not take place. In other words, a failure does not lead to an internal “judgement”,\nbut is in some sense just accepted (the concept of acceptance will be discussed in detail in the next chapter).\nEven after that, there is a final link that could be broken, from error computation to suffering. That is, even if\nan error is computed, some further processes are necessary to translate that error into the subjective feeling of\nsuffering, at least in humans. This link might be weakened by a metacognitive perspective, which will also be\ntreated in detail below.\nEmptying the mind and reducing simulation\nAnother additional benefit of meditation is that many people report feeling great pleasure when meditating.\nThis is often attributed to the fact that the mind is strongly focused on a single object, such as breathing, and\nthus emptied of any thinking. Several traditional meditation schools actually maintain that an “empty” mind\nis happy, that is, a mind where there are no thoughts, whether wandering or intentional.23 (Emptiness of the\nmind does not here refer to the Mahayana Buddhist concept of emptiness we saw earlier.) A similar pleasurable\nstate is sometimes achieved in the state of “flow”, where wandering thoughts are equally absent.24\nUnderstanding why an empty mind tends to be happy is one of the deepest problems for a scientific un-\n21As already mentioned, Libet et al. (1983) proposed, rather controversially, that while the consciousness does not decide actions,\nit has a “veto” over actions: It can cancel an action sequence that the unconscious neural networks are trying to perform. This might\nprovide an interesting explanation of how consciousness, in an advanced state of mindfulness and metacognition, seems to be able to\nprevent habitual actions (Baer, 2003; Garland et al., 2014), such as stopping the twelve-fold chain.\n22Related models consider how mindfulness meditation helps in addiction (Brewer et al., 2014; Garland et al., 2014). In particular,\nBrewer et al. (2014) proposes several mechanisms describing how mindfulness meditation can “de-automate” the dynamics, including\nlearning to simply observe aversive states without reacting to them and taking them less “personally” , while becoming “more aware of\nhabit-linked, minimally conscious affective states and bodily sensations”.\n23It may seem contradictory if meditation tries to make the mind empty, while above, meditation was seen as learning from selected\ninput. This is not contradictory since the point is that relative emptiness of the mind is necessary to be able to select and pay attention\nto mental phenomena in a way that optimizes learning about uncontrollability, etc. Without such emptying of mind, all the processing\nwould be spent on ordinary thinking and sensory processing instead of the intended learning. For example, only with a relatively\nempty mind can the meditator realize how wandering thoughts are uncontrollable and impermanent, or that bodily feelings likewise\njust come and go.\n24(Csikszentmihalyi, 1997)\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n199\nderstanding of the mechanisms behind meditation, and not quite resolved at the moment. A number of view-\npoints can be taken here. In a traditional Buddhist account, where desire is considered the basis for suffering,\na simple explanation would be that an empty mind is happy because it does not have any desires (including\naversions).25 On the other hand, Chapter 11 reviewed research showing that wandering thoughts are typically\nrelated to a negative mood; however it was not clear if those results apply to all thinking and not just wandering\nthoughts, and what is the cause and what is the effect. Yet another viewpoint is to recall once more Cassell’s\nstatement that “to suffer, there must be a source of thoughts about possible futures”, which cannot exist in an\nempty mind.\nIn the framework of our frustration equation, we can formulate a more computational viewpoint. Reward\nloss is computed every time a simulation, whether in terms of replay or planning, is conducted in the brain. A\nreduction of thinking should reduce suffering since such simulation of frustration or reward loss is reduced. In\nfact, in our frustration equation on page 172 we have the term “number of times [the reward loss] is simulated”\nwhich gives the number of times the reward loss is computed (after adding one to this number, due to the initial\nactual perception). Reducing mental simulation will reduce this term, and thus suffering. Reducing mental\nsimulation will, for example, reduce rumination over past errors, simulation of future threats to the person, as\nwell as judgements related to self-esteem, which are some of the most important sources of suffering.26\nThe logic just given may explain why many meditation methods have the explicit goal of emptying the mind\nof thinking, or at least reducing thinking. Typically, one concentrates on a single object, such as the breath. This\nimmediately reduces thinking, including wandering thoughts—but does not eliminate them completely, as the\nmeditator soon notices. An important aspect of any meditation technique is how to react to the occurrence of\nwandering thoughts. Some meditation techniques directly aim at suppressing them by refocusing on the orig-\ninal object of meditation. Suppose you have any unpleasant, possibly scary wandering thoughts about the\nfuture or the past during meditation. If you refocus on the meditation object, thus clearing the mind of such\nscary wandering thoughts, it is rather obvious that suffering will be reduced.27 Being able to thus prevent neg-\native wandering thoughts from occurring should have a strong positive effect on mood, in line with our logic\nabove based on frustration equation. In fact, it has been shown that the default-mode network, largely respon-\nsible for wandering thoughts, is less activated in experienced meditators.28 (Below, we will see an alternative\napproach to dealing with wandering thoughts based on meta-awareness.)\n25See page 96 for a proposal on how desire and aversion in themselves produce suffering.\n26A problem with this logic, which we already partly saw in Chapter 11, is that it is not clear why simulation of positive experiences\nwould not cancel the effect of simulating negative experiences. Somehow, it seems that negative experiences are stronger in this case.\nIt is possible that this only holds for some people whose thinking just happens to be more often negative than positive, and it is those\npeople whose mood is most improved by meditation. Or, it could be that due to some evolutionary reasons, this is the case for the\nvast majority of humans: Baumeister et al. (2001) reviews a great number of results leading to the conclusion that “bad is stronger\nthan good” as far as the emotional effects of life events are concerned. (The important case of rumination was treated in Chapter 11).\nInterestingly, Plutarch proposed a training method to reduce future-oriented wandering thoughts by recalling good things that have\nhappened to you in the past (On the Tranquillity of the Mind, 14). At the same time, he also recommends cherishing things you have\nright now, thus making a clear connection to gratitude exercises discussed in Chapter 18. Thus, it seems to be possible to engage in\nthinking that is not very different from wandering thoughts, but deliberately optimizing the contents can make the effects positive.\n27(Kuyken et al., 2010)\n28(Brewer et al., 2011) Recapitulating some of the logic above, we arrive at a speculative computational explanation of why almost\nany wandering thought leads to suffering, and why the elimination of almost any wandering thoughts reduces suffering. Namely,\nmost wandering thoughts are related to some kind of desire or aversion, which either underlies planning of future action or motivates\nreplay of a rewarding or punishing past episode. If we combine this with the idea that aversions and desires are suffering in themselves\n(page 96), we see why wandering thoughts almost necessarily lead to suffering.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n200\nFocusing on what is here and now\nFocusing on breathing is something that can be done during a formal meditation session, but perhaps not\nduring ordinary life. That is why in Buddhist training, there is also a strong emphasis on focusing on what\nhappens here and now; this can be practiced in everyday life, outside of any meditation sessions. Such a fo-\ncus can be conceptualized as learning to change your “cognitive style” to a more “experiential” one, which\nmeans you replace most thinking, whether future- or past-oriented, by the simple sensory experience of the\npresent moment.29 This is essentially another shift of attention away from thinking, but this time the shift is to\nany immediately present perceptual input, instead of some pre-selected object like the breath during medita-\ntion. Since suffering is fundamentally based on predictions and expectations, focusing on the present moment\nshould reduce suffering in many ways.\nIn particular, focusing on what is here and now means that any reward loss occurred in real life is only\nbriefly observed without paying too much attention to it. Soon, attention is directed to something else in\nthe here-and-now, since the reward loss has already become a thing of the past. According to the frustration\nequation, such reduction of attention reduces suffering, but this time it works via the the term “amount of\nattention paid” since reducing attention reduces the impact of any perception. Nevertheless, such a cognitive\nstyle also reduces simulation, and produces the benefits of an empty mind as well.30\nMetacognition and observing the nature of mind\nThere is one more form of attentional control operating in mindfulness meditation, especially in more ad-\nvanced stages of the practice: direction of attention and awareness to a metacognitive level. Metacognition\nmeans cognition about cognition: for example, thinking about one’s own thoughts, or observing one’s own\nperceptual processes. In such a case, the “higher”, metacognitive part of your mind is observing the “normal”\nthinking or perceiving part of your mind.31 Such metacognition is presumably possible because of the parallel\nand distributed nature of brain function, which means one part of the brain can observe what is happening in\n29(Watkins and Teasdale, 2004)\n30Buddhist philosophy, as well as the theory in this book, further suggest another very different way for achieving a reduction in\nreplay and planning, which is nothing else than adopting the philosophical attitudes described in the preceding chapter. Planning\nhow to obtain future rewards is likely to be reduced if future rewards are considered lesser; there is simply not so much incentive\nanymore in planning for them. Likewise, planning to avoid threats, or worrying, will be reduced if those threats are seen as relatively\nuncontrollable. Furthermore, when the uncertainty of our thoughts and perceptions is realized, spontaneous thinking is often reduced,\nsince there seems to be much less point in simulating something which is uncertain anyway. This is how adopting the philosophical\nattitudes discussed in the preceding chapter will also lead to a reduction in simulation, and towards an empty mind. This logic shows\nhow the question of causality regarding emptiness of mind and happiness/suffering is complex. (See also footnote 34 in Chapter 11.)\nWe started this section by pointing out that emptying the mind by meditation often has the effect of making people feel more joyful.\nThus, emptying the mind was seen as an intervention that causally reduces suffering. In contrast, the idea that reducing desires reduces\n(especially wandering) thoughts is in line with some classical Buddhist authors who seem to claim that the emptiness of mind is\nmainly an effect of mental development, not a cause of happiness (Williams, 2008b, p. 55). In such thinking, reducing desires reduces\nfrustration as discussed in Chapter 16, and an empty mind is just a side-effect. Meanwhile, the discussion on the experiential cognitive\nstyle just given could probably be interpreted based on either causal direction; either an experiential style makes the mind empty, or\nemptying the mind leads to a more experiential style; or perhaps both are effects of the reduction of desires or some similar cause.\n31(Beran et al., 2012; Proust, 2010; Fleming et al., 2012). An increase in metacognition is one of the more robust findings in studies of\nmindfulness training (Lao et al., 2016). Interestingly, one important function of metacognition is assessing the uncertainty of percep-\ntions and cognitions (Fleming, 2024); presumably uncontrollability and even unsatisfactoriness have a similar link to metacognition.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n201\nanother part.\nAn obvious utility of metacognition is that it enables introspection, which allows you to understand the\nprocesses underlying your thinking, emotions, and desires. This is of course the goal of a multitude of psy-\nchotherapeutic systems. However, the practice of meditation, especially in a Buddhist context, can go much\ndeeper in this respect, and a well developed metacognitive attitude is seen as interesting in its own right. Such\ndevelopment of metacognition can be seen as another example of perceptual learning discussed above.\nBuddhist meditative practice eventually leads to a class of advanced techniques based on meta-awareness,\nwhich designates the quality of the consciousness or awareness present in such metacognition. It is awareness\nof awareness; in other words, there is conscious recognition or perception of the fact that there is awareness.\nThis may seem very complicated or even paradoxical, but in fact it is something that we regularly engage in, if\nonly fleetingly. A typical example used in neuroscience is when you realize your mind is wandering and regain\nfocus; that realization was on the level of meta-awareness. But there are many more interesting cases.32\nConsider the following case of sensory meta-awareness. If I ask you whether you see this book, you would\nreply in the affirmative. I can formulate the question in a more explicit way: Are you aware of the fact that you\nare consciously perceiving this book? You would probably still reply in the affirmative. It is almost the same\nquestion really, since in colloquial language, if you “see” something, that means that you see it on a conscious\nlevel. If you can consciously recognize that you are consciously perceiving this book, you must be aware of\nsuch conscious perception happening, and thus there is meta-awareness. So, you were fleetingly aware of\nthe sensory awareness of the book; you moved to the metaconscious or meta-aware level for a few seconds.\nThat shows that almost any kind of sensory awareness can be accompanied by meta-awareness, and it can be\ndeliberately initiated. While this meta-awareness didn’t last long, it is possible, as a meditative exercise, to stay\non the level of meta-awareness for a longer period of time.33\nThe same kind of meta-aware observation can even be extended to thinking. Some advanced meditation\ntechniques emphasize observing the wandering thoughts while they are taking place, instead of suppressing\nthem. The possibility of actually observing the wandering thoughts and their contents in real-time—instead\nof merely noticing that you have had some wandering thoughts a while ago—may seem quite paradoxical.\nHowever, it is possible to learn such sustained meta-awareness of one’s thoughts with enough meditation prac-\ntice.34 From this viewpoint, at least on advanced levels of practice, it may not be necessary to reduce wandering\nthoughts; after all, any attempt to empty the mind may create new suffering because the mind is uncontrol-\nlable. Instead, one may change the quality of the awareness in the sense that the attention is mainly operating\non the metacognitive level.35 Such meta-awareness often feels like perceiving one’s thoughts as if from the\noutside, instead of being inside or involved in them. In fact, if your mind engages in a scary simulation of\n32(Chin and Schooler, 2010; Schooler et al., 2011). As in earlier chapters, the words consciousness and awareness are here used\nsynonymously. The distinction between metacognition and meta-awareness is not always clear and there is overlap on how these\nterms are used. The key difference for me is, however, that meta-cognition can happen even in an unconscious agent such as a robot,\nwhile meta-awareness, by definition, requires consciousness.\n33(Tejaniya, 2008, e.g. p. 77-79,121-126)\n34(Tejaniya, 2008, e.g. p.126-133);(Pramote, 2013, Ch. 4);(Kyabgon, 2015, p. 177-179). The difference between intermittent (fleeting)\nand sustained meta-awareness in a meditation context is considered by Dunne et al. (2019). In contrast, Smallwood et al. (2007),\napparently talking about a non-meditative context, emphasizes that the absence of any metacognition or meta-awareness is typical of\nwandering thoughts.\n35(Teasdale, 1999). This technique is different from but related to directing the attention elsewhere on the non-meta level, such as to\nthe here and now that was described earlier.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n202\nsomething that might happen to you in the future, you can now just watch the simulation while reminding\nyourself that it is not actually happening: it is just a simulation where your mind plans possible courses of\naction. With such a quality of consciousness, there is actually little need to stop the simulation to reduce suf-\nfering. Moving to such a metacognitive level is actually often an automatic consequence of long practice in\nmindfulness training, and it may easily happen during an intensive meditation retreat.36\nNow, what if you could spend a considerable proportion of your daily life on the meta-aware level? Such\nlong-term sustained meta-awareness seems to be possible after extensive meditation practice. Importantly,\nsuch meta-awareness may lead to insights that convince the meditator about several philosophical points we\nhave seen in this book. You may see all conscious mental phenomena, that is, all the contents of your con-\nsciousness, such as perceptions and thoughts, as results of impersonal computational processes. In other\nwords, they are simply mental constructions, or results of a simulation performed by your brain. This logic\nmay lead to the conclusion that even what you see in front of you at this very moment is a perception con-\nstructed by your brain, based on various unconscious inferences, sometimes hardly better than guesses: you\nreally have no other source of information about the world but perceptions and thoughts playing in the vir-\ntual reality of consciousness. Perceptions and the ensuing thoughts are thus necessarily subjective, contextual,\nfuzzy, and uncertain constructs—they are empty, in Buddhist terminology. They do not represent any absolute\ntruth about how things are. Such insights into uncertainty and emptiness are in stark contrast to our inherent\ntendency to think that our perceptions are somehow identical to reality.37\nMeta-awareness and suffering\nThese insights into uncertainty will reduce reward loss by reducing the certainty-related term in the frustration\nequation, similar to mechanisms already explained in Chapter 16. In addition to such a long-term learning\nprocess, there is also an immediate utility in keeping a meta-aware attitude towards all mental phenomena:\nmeditators often report great calm and peace of mind when their minds are in such a state. The reason is not\nwell understood from a neuroscience viewpoint, but I would assume that less attention is paid to error signals,\nbecause attention and awareness have largely moved to the meta-level. Perhaps error signalling is somehow\ngenerally dampened, due to some mechanism to be discovered. Introspectively, the effect can be described as\nthe meditator keeping some distance from the thoughts and perceptions, and taking them less personally as\nwell as less seriously. Going back to the frustration equation (page 172), we can assume that any reward loss\nwill be paid less attention, meaning that meta-awareness is reducing the term “amount of attention paid to\n[reward loss]” in the frustration equation.\nMeta-awareness may be particularly useful in the specific case of threats. Suppose you were able to see all\nthreats as empty: uncertain, subjective, open to interpretation, nothing but mental constructs. You just watch\nthem from the meta-level, as if from the outside, from a distance. This is particularly feasible in case of threats\n36Dahl et al. (2015) discusses different meditation techniques and the role of meta-awareness in them. This is related to what is called\ndecentering by Fresco et al. (2007); Safran and Segal (1990).\n37This eventually leads to what is the deepest form of meta-awareness: being simply aware of the existence of consciousness itself, or\nof the very capacity to be conscious of mental phenomena, as opposed to being aware that you are aware of some specific phenomenon\n(such as the perception of this book in the example above). Such awareness is related to what is called seeing the (true) nature of mind\n(or consciousness) in Buddhist and related literature (Brahm, 2006; Dalai Lama et al., 2011; Kyabgon, 2015; Spira, 2017); it is also related\nto the attitude briefly described at the very end of Chapter 14. However, it is outside of the scope of this book, and goes well beyond\nour frustration equation. Nevertheless, similar to what is argued next in the main text, it could lead to minimal attention to reward loss\nand therefore a great reduction of frustration even according to that equation.\n\nCHAPTER 17. RETRAINING NEURAL NETWORKS BY MEDITATION\n203\nto your self-esteem or social status instead of your survival; or if the threat only happens in a simulation. Then,\nany threats would not be taken that seriously, and their ability to trigger fear would be weakened. From this\nviewpoint, it is not necessary to reduce the desires that underlie the threats (as we did in Chapters 6 and 16)\nor reduce any simulation. It is now possible to directly intervene on the threats themselves by seeing them as\nempty; in particular, considering them as mere phenomena in the mind. Again, this intervention is distinct\nfrom reducing simulation and the desires underlying threats; there might still be simulation of threatening\nevents and desires leading to threats, but the threats just do not generate suffering anymore.38\nA strong meta-awareness may have the very general effect of reducing the effects of error signalling by de-\ncoupling error signals from the actual conscious feeling of suffering. In the theory of this book, error signalling\ncauses conscious suffering, but they are not the same thing. Error signalling is an information-processing\noperation amenable to modelling; in contrast, conscious suffering is a subjective experience, and as such, dif-\nficult to understand scientifically. Now, it should be possible to break the causal link from error signalling to\nconscious suffering, as already pointed out above in relation to the cognitive cycle. This can presumably be\nachieved by a strong meta-awareness that is able to discriminate between those two phenomena. It opens up\nthe possibility of seeing errors and suffering as two separate mental phenomena, eventually preventing errors\nfrom triggering suffering.39\nThus, we see that the real beauty of moving to the level of meta-awareness is that it reduces suffering com-\ning both from frustration and from threats. The key is not to take your thoughts or any mental phenomena\ntoo seriously or personally; they are just something automatically generated by your brain, even against your\nwishes and even against the facts. Seneca put this sharply when commenting on his own worrying thought:\nThe author [of that thought] is a fool, and he who has believed it is a fool, as well as\nhe who fabricated it.40\n38From a historical viewpoint, this suggests an interpretation where the Mahayana school complemented the Buddha’s original the-\nory of desires and frustration by offering interventions that more directly apply to threats. The Buddha may not have talked very much\nabout fear in his discourses, with the expection of the fear of death (e.g. Anguttara Nikaya 4.184; but see footnote 31 in Chapter 14 for a\nTheravadan quote focused on fear), while the Mahayanan emphasis on meta-awareness may work as a powerful intervention towards\nthreats and fear.\n39It may seem to be too difficult to say much about this possibility since it is related to the rather mysterious nature of conscious\nexperience. But that may not necessarily be the case: it is also possible that there is a group of cells representing the error and another\ngroup of cells representing suffering. It is then thoroughly possible to break the connection between the two groups of cells. The fact\nthat the latter group of cells is intimately connected to conscious experience, and that we don’t understand what that connection is,\nmay not be relevant for the design of this intervention.\n40Letters to Lucilius, 13.13\n\nChapter 18\nRecapitulating and unifying interventions\nIn this chapter, I start by recapitulating the interventions proposed in the two preceding chapters, and then dis-\ncuss some typical counterarguments against such interventions—or Buddhist-Stoic training in general. Then,\nas a final class of interventions, I discuss the development of positive attitudes towards mental phenomena.\nInstead of observing the world as it is and learning from it, here the emphasis is on actively cultivating certain\nways of relating with the world that reduce negative feelings (valence). This approach is “positive” compared\nto the training based on various kinds of reduction emphasized in the preceding chapters. Acceptance and\nletting go are the key attitudes here; it turns out that they are intimately related to the reduction of desires and\nexpectations. In fact, letting go, which I interpret as a form of relaxation, can be seen as a principle that unifies\nmost of the training methods described in this book.\nRecapitulating the interventions\nThe flowchart in Fig. 18.1 on page 206 recapitulates the main mechanisms of the different interventions de-\nscribed in the two preceding chapters, while briefly mentioning some to be explained below. The most funda-\nmental interventions are recognizing uncertainty, uncontrollability, and unsatisfactoriness (green boxes on the\nleft). These lead to reduction of expectations and reduction of desire, including aversion (black dashed boxes),\nwhich reduce the frustration computed and suffering experienced (red dashed boxes). A simple lifestyle (bot-\ntom left) is another intervention that I did not describe in much detail in this book, but it is also a well-known\nmethod for reducing desires. Another related intervention is recognizing the uncertainty of perception and in\nparticular the perception of reward loss (green box, top middle), which reduces the computed frustration as\nwell.\nMindfulness meditation strengthens and speeds up many of the interventions just mentioned, which is de-\npicted as a blue box at the bottom left-hand corner. Meditation also brings several new mechanisms into play;\nsome are shown as separate blue boxes in the chart, although in practice, they all come from the same inter-\nvention. First, meditation tends to make the mind empty, thus reducing simulated frustration (third column,\ntwo boxes at the bottom). Meditation also enables stopping the cognitive cycle or chain (Fig. 15.2 on page 169),\nwhich goes from perception to desire and frustration (blue box in the second column). Such stopping can have\nmany different effects, but what is emphasized here is that it can directly reduce desire as well as simulation,\nwhile it can also explicitly prevent the frustration computation itself. Another intervention offered by medita-\ntion is that it creates a form of metacognition and meta-awareness (blue box at the upper left-hand corner),\n204\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n205\nwhich may eventually lead to the attitude that all phenomena are just in the mind, as proposed by Mahayana\nBuddhist philosophy, thus directly reducing all kinds of suffering.\nAll the interventions just described are intimately related to manipulating terms in the frustration equa-\ntion (page 172). (Those that reduce desire are a bit more indirectly related since they essentially prevent the\nmechanism in that equation from being triggered at all.) However, we also saw some interventions related to\nmeditation that work in very different ways. Meditation is actually a highly complex phenomenon, and it cer-\ntainly has many effects mentioned only briefly, if at all, in this book. The flowchart lists as a further example\nthe extinction of fear conditioning; recognizing the emptiness of categories could also have been added. The\nflowchart has a further box on letting go and acceptance, which will be treated later in this chapter.\nEssentially, these interventions reduce frustration (red box in the middle). Threat computation is reduced\nas well since threat is based on predicted frustration. Finally, this reduction of frustration and threat reduces\nthe conscious experience of suffering, or mental pain. Meta-awareness is very special in the sense that it can\ndirectly reduce conscious suffering even if the frustration or threat computations are performed, which is indi-\ncated by the direct arrow from meta-awareness to conscious suffering.\nHow far should reducing desires and expectations go?\nLet us next consider a typical objection that can be raised at this point: the thinking underlying the inter-\nventions of the preceding chapters seems depressing. One may ask whether not wanting anything and not\nexpecting much leads to complete inactivity and, indeed, to some kind of depression. A diagnostic criterion of\ndepression is “markedly diminished interest (...) in all, or almost all, activities”,1 which sounds a bit like hav-\ning substantially reduced reward expectations and having few desires. The fundamental question is: Can such\nreduction of desires and expectations go too far?\nLet us consider first how much expectations should be lowered. Is it enough to admit the actual levels\nof uncertainty and uncontrollability, or should we go further and consider things even more uncertain and\nuncontrollable than they really are, thus lowering expectations even more? If our only goal were to reduce\nsuffering in the agent, we could simply program it to assume that everything is completely uncertain and com-\npletely uncontrollable. Then, the agent would expect zero reward, or very little, in any state or from any action.\nAs a consequence, it would have virtually no desires either. Is this a good way of programming an agent?2\nClaiming that Buddhist training can lead to something akin to depression is, in fact, a well-known point\nof criticism, and similar arguments have actually been raised against Buddhism throughout its history. I think\n1DSM-5 diagnostic criteria\n2The traditional Buddhist viewpoint tends to emphasize that people are mistaken about the level of control and permanence, and\nit is enough to correct their “ignorance” or “illusions”. Both neuroscience literature and reinforcement learning literature do offer\nexamples of how humans are overoptimistic, as well as why that may be a good thing for an agent (Palminteri and Lebreton, 2022;\nCiosek et al., 2019; Munos, 2011). On the other hand, consider a super-intelligent agent which has no constraints regarding data or\ncomputation. It would presumably estimate uncontrollability and uncertainty correctly and accurately, without any illusions. But it\nwould still have reward losses, and those reward losses might not even be particularly small, especially if the outside world is difficult\nto control (perhaps due to strong physical constraints in the ability of the agent to manipulate it) and exhibits a lot of randomness. So,\nit is not clear if suffering would be very much reduced by correcting “illusions” in the sense that the agent learns to make “optimal”\ninference (in the sense of probabilistic AI theory) with infinite data and computation. I would assume that the real goal of such Buddhist\npractice may rather amount to adopting reward expectations which are lower than what is objectively true. In this case, it would lead\nto increased happiness at the expense of slightly suboptimal inference—but note that such “suboptimality” refers only to the lack of\noptimality in maximizing rewards, or evolutionary fitness.\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n206\nRecognizing \nunsatis-\nfactoriness\nRecognizing\nuncontrol-\nlability\nRecognizing\nuncertainty \nLower \nexpectation\nLess desire \n(aversion)\nReduction in  \nfrustration \n(and threat)\nRecognizing\nuncertainty\nof \nreward loss\nLess \nsimulation\nStopping \ncognitive \ncycle\nMeta-\nawareness;\nseeing all \nas mind \nLess \nsuffering\nexperienced\nEmptying \nmind by\nmeditation \nExtinction \nof fear \nconditioning \nLetting go,\nrelaxation, \nacceptance\nSimple \nlifestyle \nSpeeding up\nlearning\nby meditation\nFigure 18.1: Recapitulation of the mechanisms of the interventions explained in this book. The green boxes are\ninterventions of cognitive-philosophical kind. Lifestyle changes are separate from those, given in a cyan box.\nWhile mindfulness meditation is a single intervention, it is divided into a number of (blue) boxes based on the\ndifferent mechanisms at play. (The single thick arrow from the “speeding up” box means that it speeds up the\nlearning in all the boxes above it.) The dashed black boxes with dashed contours are intermediate results of\nthose interventions, while the dashed red boxes describe the final effect of those interventions. The gray box in\nthe lower right-hand corner is about general attitudes with rather nonspecific effects, which is why no explicit\narrows are drawn there.\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n207\nsuch criticism is not very relevant because it considers an extreme case, which is unlikely to be achieved by\nmost people practicing such systems. Perhaps the point is that most people living in a modern industrialized\nsociety simply have too many desires, and it would be better for them to have fewer of them. This would explain\nwhy people engaged in Buddhist training tend to get happier when they reduce reward expectations. It may be\nirrelevant to ask what might happen in the extreme case where they completely annihilate all their desires—\nwhich is a feat even most meditation masters are incapable of. Buddhist philosophy actually emphasizes the\ngeneral principle of the “middle way”, or moderation, which sounds like a good idea here as well. The situation\nmight be different for Buddhist monks or nuns engaged in full-time practice for many years, but they follow a\nvery special lifestyle, which is specifically designed to be compatible with having very few desires.3\nOn the other hand, there is certainly something fundamentally different between a depressive state and a\nmental state where the unsatisfactoriness of the world is seen from a Buddhist perspective. If an agent con-\ncludes that none of its desires are going to be fulfilled and it will never receive any reward, that gives in itself no\nreason for a negative feeling or valence. The agent would just rationally decide that no desires are worth pursu-\ning, it would not engage in goal-oriented action, it would predict zero rewards in the future, and, consequently,\nit would suffer less since there is no frustration.\nIf humans tend to get a negative feeling after seeing that the world is fundamentally unsatisfactory, it must\nbe because there is another “higher-order” desire, presumably coming from the self-evaluation system treated\nin Chapter 6. A depressed person, in particular, finds the very unsatisfactoriness of the world frustrating, and\nwants to find satisfaction or reward in various kinds of seemingly pleasurable objects and activities. In our\nframework, we would say that she is frustrated in terms of her self-evaluation, as she sees that she gets less\nreward in the long run than she “should” according to some internal standard (possibly based on social com-\nparison). The self-evaluation system may indeed conclude—based on a superficial calculation—that since no\ngoals can be reached and no reward can be obtained, there must be something wrong with the agent. Thus, a\nnegative meta-learning signal is generated, and this would be felt as suffering.\nHowever, I think an important point in the Buddhist philosophy of unsatisfactoriness is that if the self-\nevaluation system sends a negative signal when the agent does not get enough rewards, the system is simply\nmalfunctioning. Clearly, the realization of the total unsatisfactoriness of everything should also influence the\nself-evaluation system. The self-evaluation system should set its expectations and its standard of an “accept-\nable” reward level very low, even zero. The self-evaluation system cannot rationally claim that the agent is not\ngetting enough rewards if the system itself believes that no rewards can possibly be obtained! As such, Buddhist\nphilosophy proposes that there is no need to be frustrated about any long-term lack of reward, nor is there any\nneed to make any negative self-evaluation; not getting much reward and not reaching one’s goals is natural and\nunavoidable.\n3A famous counterexample to my optimism happened during the Buddha’s life, when several of his disciples committed suicide\nafter intensively engaging in a particular exercise: reducing carnal desires by contemplating the loathsomeness of the human body\n(Samyutta Nikaya 54.9). The Buddha realized his mistake and changed his teaching accordingly. Using loathing as a meditation tech-\nnique to reduce reward expectations and desires is extremely rare in current Western meditation practice.\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n208\nIs frustration not needed for learning?\nAnother objection that could be raised against the philosophy presented here is that it may not be useful to\nreduce frustration since the frustration signal is useful for learning. Human beings seem to be trapped in a\nsituation where they need frustration to learn, while they suffer from it. That may sound like a dilemma with no\nsatisfactory solution. However, I’m not sure there is any serious dilemma here. One reason is that, as discussed\nin Chapter 5, many of the rewards we are programmed to receive are actually rather useless “evolutionary\nobsessions”; frustrating them may not teach us anything useful, if it is not the very futility of those rewards.\nThe same is true from the viewpoint of insatiability: why should one try to learn how to better satisfy desires\nthat cannot be satiated anyway?\nFurthermore, Chapter 14 proposed that a large part of the problem is how frustration is made conscious\neven though it need not be; learning from frustration could, in principle, happen on an unconscious level. It\nmight seem that not much can be done about this, but in fact, an intervention is possible, as was seen in the\ndiscussion on meta-awareness in Chapter 17.\nLikewise, it might be claimed that thinking about threats is useful since then, the agent learns to avoid\nthem. However, people worry about horrible things which are extremely unlikely to happen; they also worry\nabout things which are unavoidable, such as death. Such worrying is unlikely to improve the performance of\nthe agent at all. It may actually decrease the performance since so much energy is spent on those rather useless\ncomputations.\nYet another counterargument is that while understanding the uncertainty of all perceptions reduces frus-\ntration, it may actually improve learning and make us more “intelligent”. Uncertainty and uncontrollability are\nreal properties of the world, but we may have been grossly underestimating them.4 Thus, learning to better ap-\npreciate uncertainty and uncontrollability is a useful meta-learning process, even from the viewpoint of trying\nto optimize rewards in the world.\nBased on these counterarguments, I think that while it may be meaningful to claim that not all frustration\nshould be removed, most of it can still be removed without making learning or the ensuing behavior any worse.5\nInterventions need to be based on personal preferences\nSome readers may still not be convinced. They might argue that desires feel good; without them, life would be\nempty; besides, they have no time or energy for the training described in this book. I think it is important to\nunderstand that this book is fundamentally an exposition of a scientific theory. A scientific theory per se does\nnot tell you what to do. It only tells you that if you do X, then Y will follow (with some probability); it explains\n4See footnote 2 in this chapter.\n5In fact, if somebody argues that frustration is actually good since it enables learning, the question arises as to why frustration is\npainful. If frustration is “good” and should be encouraged, frustration should feel pleasant, not painful, based on elementary evolu-\ntionary logic. The fact that frustration is painful means that at least in some evolutionary sense and to some extent, it has been deemed\nto be bad for you. As a thought experiment, suppose frustration felt good, perhaps because you have become so thoroughly convinced\nabout the utility of the ensuing learning that you are able to override millions of years of evolution. Then, you would presumably try to\nfail in everything you do—it feels good and you will consider that good feeling as some kind of an internal reward. You might learn a lot\nfrom such failures, although if you fail without even trying hard, the utility for learning might be meager. In any case, you would not get\nmuch reward; you might starve, die young, and would not produce any offspring. This (admittedly not very rigorous) argumentation\nsuggests that frustration should be avoided from an evolutionary perspective and thus, it has to be evolutionarily made painful. How-\never, this argumentation was based on an extreme case. Perhaps there is an optimal amount of frustration which is not zero; perhaps\nit would be possible to detect circumstances under which frustration is good while it is usually bad. I leave this for future research.\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n209\ncausal connections and, in particular, what effects different interventions will have. As such, it is up to you to\ndecide which interventions, if any, are actually worth it for you. It depends on your preferences or your values\n(in the ordinary sense of the word), and no scientific theory can tell you what to do without considering your\npersonal preferences. The interventions have side-effects or costs, and the cost-benefit analysis is different for\neach individual and intervention. One individual may get a lot of satisfaction from pursuing and achieving\na certain goal, while another might get much less; for some other goal, it might be the other way around.\nLikewise, how much an individual benefits from a particular intervention must vary from one individual to\nanother. Ideally, some scientific analysis of your personality, temperament and lifestyle might be able to tell\nwhich interventions are the best for you, but we are not there yet.\nPositive viewpoints to reduction\nSome of the arguments against Buddhist-Stoic training may be due to the simple fact that reducing anything\nsounds like a negative thing, as if you were missing out on something. Next, I will try to give more positive\ninterpretations of the reduction—or even absence—of desires and expectations.\nContentment, gratitude, and freedom\nTo begin with, the absence of desires can be expressed as contentment, in the literal meaning of being content\nwith what one has and not wanting more. The insatiability of desires, in particular, implies that a simple-\nminded agent cannot be content: it always has to search for more rewards, leading to endless frustration. If an\nagent has no desires, we can, from a positive viewpoint, consider it to be content with the current situation.\nIf contentment becomes strong enough, it may turn into a feeling of gratitude. Gratitude training or med-\nitation is a major topic in itself, and there are specific methods to increase gratitude. Gratitude is an emotion\nthat is social or interpersonal, which is why it is a bit outside of the theory of this book. In any case, the paradox\nof gratitude is that it actually enhances the well-being of the person being grateful—not only of the person to\nwhom the gratitude is directed. As Seneca puts it:\nI am grateful, not in order that my neighbour, provoked by the earlier act of kindness, may be more\nready to benefit me, but simply in order that I may perform a most pleasant and beautiful act; I feel\ngrateful, not because it profits me, but because it pleases me.6\nAn even more fundamental positive interpretation of having no desires is freedom. While an emphasis on\nfreedom is ubiquitous in Buddhism, Epictetus summarizes the idea in a way that is, yet again, in complete\nharmony with the Buddha’s philosophy:\nFreedom is acquired not by the full possession of the things which are desired,\nbut by removing the desire.7\n6Quote from Seneca, Letters to Lucilius, LXXXI.20. Gratitude was also strongly recommended by Epictetus (Discourses, I.6. II.5.10,\nII.16.28) as well as Plutarch (On the Tranquillity of the Mind, 14). Recently, it has become a topic of great interest in positive psychology\n(Emmons and Shelton, 2002; Wood et al., 2010; Watkins, 2013); appreciation is a related construct that may be defined as something\nmore general (Fagley, 2016).\n7Quote from Discourses, IV.1.175. The whole Chapter IV.1 in Discourses is dedicated to explaining how the goal of Stoicism is freedom,\nin particular, freedom from “being constrained or impeded by any external circumstance or emotional reaction” according to Long\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n210\nMore generally speaking, the condition of a human being has been described as being a “puppet of the gods” by\nPlato,8 meaning that “affections in us are like cords and strings, which pull us different and opposite ways”. We\nhave to remove those cords and strings if we want to be free. Instead of being enslaved by our neural networks\nwith their interrupts and unconscious action tendencies, we need to liberate ourselves from such evolutionary\nconstraints, giving more space for conscious deliberation and the use of reason.\nAttitude of acceptance\nAnother positive attitude that is fundamental in meditation practice is acceptance. There is, in fact, an impor-\ntant caveat in any attempt to reduce mental phenomena, be it desires or wandering thoughts. It is important\nthat this training does not lead to the idea or evaluation that the mental phenomena are somehow bad. Such\nan attitude would, in itself, easily lead to aversion and thus, to suffering. In the extreme case, if there is aver-\nsion towards the mental phenomenon of aversion, that may lead to a vicious circle, which constantly increases\naversion. To counter this tendency, it may be necessary to actively create new mental phenomena so as to\nneutralize the existing ones.\nIt may sound paradoxical to say that one should not think of the mental phenomena as bad, or at least\nundesirable. How could one not think that, say, desires are bad if one believes they lead to suffering? And how\nis one supposed to get rid of them if one does not regard them as something negative, something to be avoided?\nThe solution to this paradox is that while the actions of the meditator should be chosen so as to reduce\ndesires (or other mental phenomena), it is still possible to avoid creating any new aversion in the sense of a\nnew mental process. Thus, on an abstract level, it is useful to consider the desires “bad”, or perhaps rather as\nsomething that it would be better not to have, but such thoughts should just work in the background as weakly\nas possible, instead of being strong and actively cultivated. In particular, they should not lead to any interrupt-\nlike aversive emotions. Such processing is possible since the neural networks can implement automated habit-\nlike action tendencies that try to avoid certain phenomena, and that can happen without any need to activate\nthe desire/aversion system. As an extreme example, when you are walking, you know that losing your balance\nis “bad”, but you probably don’t feel a constant aversion or fear towards stumbling; your neural networks have\nsimply been trained to avoid that happening; they “reduce stumbling” so to say but without any aversion.\nIn practice, it has been found that with meditation, the tendency to develop aversion is so strong that spe-\ncific techniques are necessary to reduce it. The key technique is to cultivate the attitude of acceptance. This\nmeans a general attitude of accepting all thoughts and sensations that come to the mind, instead of resisting\nor judging them. More precisely, acceptance here means simply not activating processes of aversion, i.e. not\nactivating a desire to get rid of something. So, acceptance here is taken in a very limited sense: this is neither\n(2002, p. 27). Seneca talks about our slavery in more specific terms, related to something like valences in On a Happy Life, 4: “See (...)\nhow evil and guilty a slavery the man is forced to serve who is dominated in turn by pleasures and pains, those most untrustworthy\nand passionate of masters. We must, therefore, escape from them into freedom.”; see his also Letters to Lucilius, LXXV.18. Emphasis on\nfreedom is ubiquitous in Buddhism as well, even if the word may be used in various meanings. While the whole goal of the Buddha’s\nteaching is often formulated as “freedom from suffering”, this is rather uninformative and uses the word “freedom” in a different sense\nthan considered here. Ancient Buddhist texts also consider the metaphysical goal of freedom from reincarnation, but that is clearly\noutside of the scope of this book. For our purposes, a very useful cognitive interpretation is given by Peacock (2018) who formulates\nthe goal of early Buddhist philosophy as freedom from “reactive patterns” triggered by valences. This is of course related to freedom\nfrom desire (and aversion) advocated in the third of the Buddha’s Four Noble Truths.\n8Plato, Laws, Book I. These puppets are different from those that Plato talked about in his more famous cave allegory. See also\nMarcus Aurelius’s Meditations II.2.\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n211\nabout moral acceptance nor about thinking that some things could not be bad for you. Such acceptance could\nalso be described as removing resistance; nonreactivity is a related term used in current research.9 For exam-\nple, a depressed person may be annoyed by the very occurrence of rumination. In such a case, accepting the\nfact that rumination occurs may actually be beneficial, since it removes the suffering due to the aversion to ru-\nmination.10 Again, acceptance does not here mean that the person would give up any techniques that reduce\nthe rumination.\nAn accepting meta-cognitive attitude can actually be adopted towards all mental phenomena. Many men-\ntal training systems include some kind of active acceptance practice of all mental phenomena as an integral\npart. An acceptance practice can be seen as a specific method for reducing aversions of all kinds. It comple-\nments the methods described in the preceding chapters, which were more oriented towards reducing desires\nin the restricted sense of the word (i.e., excluding aversion). It is closely related to the practice of letting go,\nwhich will be considered below.11\nTheories such as those explained in this book may help in the acceptance training because simply un-\nderstanding the mechanisms behind, say, wandering thoughts or emotions may enable you to accept them.\nSuppose you are convinced that they are natural processes, which even have some computational benefits,\nand that they are largely outside of conscious control. In that case, it may be easier to just let them happen\nand go away naturally, without fighting against them. This is related to seeing “causality” in the Buddhist sense\nof the word (considered in Chapter 16 on page 176), but it goes further since the phenomena are seen as not\nonly natural and uncontrollable but even useful—at least from an evolutionary viewpoint. Based on this view-\npoint, even failures and errors could be accepted as an unavoidable part of a learning process, or of life; it is\nnot necessary to get upset by them and activate the brain’s pain system.\nUltimately, even the feelings of pain and suffering themselves need to be accepted on some level. Any\naversion towards them will create a lot more suffering. As an extreme example, people suffering from chronic\npain will suffer even more if they “catastrophize” the pain, resist it, and develop a particularly negative attitude\ntowards it; accepting the pain will help.12 The Buddha gave a famous simile of a man who is struck by an\narrow, which inflicts physical pain. If the man “sorrows, grieves, and laments”, feeling aversion towards pain,\nhe makes the suffering even worse, as if he were struck by a “second arrow”.13\n9Lindsay and Creswell (2017), while emphasizing the importance of acceptance in mindfulness training, use the term almost syn-\nonymously with “nonreactivity”. Hayes and Pierson (2005) define acceptance as “an open and noncontrolling stance toward all experi-\nences”, which shows explicitly the connection to control. Meanwhile, Peacock (2018) offers an interesting interpretation of the goal of\nearly Buddhist philosophy in terms of “freedom from enthrallment to reactive patterns”, which places nonreactivity at the very center\nof Buddhist training. It should be noted, however, that in actual meditation training, it is often recommended that an active, posi-\ntive feeling (possibly what is called loving-kindness) is developed towards mental phenomena (Grabovac et al., 2011; Hofmann et al.,\n2011; Brach, 2004). It may be necessary to actively develop such positive feelings to counteract the inherent tendency to aversion and\njudgement; simply trying to refrain from negative judgements and practising meditation based on observation may not remove them\nefficiently (Samyutta Nikaya 10.4).\n10(Feldman et al., 2010)\n11A practical introduction to such meditation methods is provided by Brach (2004).\n12(Veehof et al., 2016)\n13Samyutta Nikaya 36.6\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n212\nLetting go and relaxation as unifying principles\nBuddhist philosophers often use the concept of “letting go” to recapitulate the general attitude that underlies\nthe mental training described in this book. At the most concrete level, the idea is that we let go of things and\nobjects in the sense that we don’t strive to possess or control them anymore. On a more computational level\nit means we let go of desire, i.e. we don’t even want those things in the first place—nor do we want to avoid\nthem. The same approach can further be applied to thoughts and perceptions, which are understood to be\nsubjective and unreliable, so they can be let go of. Feelings and emotions are likewise just observed and then\nlet go of. The whole simulation called consciousness is no longer taken that seriously. Combined with the no-\nself philosophy, the attitude can be recapitulated as letting go of everything that is not part of me, and since\nnothing really is part of me, or my “self”, everything is let go of.14\nLetting go is an expression that obviously has a clear connection to the term “reduction” that we have used\nvery often. It is not so much a question of programming new routines or new functionalities. The idea is to\nreduce activity, letting go of existing mental associations and routines. The key is less desire and aversion, less\nreplay and planning, fewer interrupts, and so on.15\nAn important point about letting go is that it circumvents the paradox of wanting not to want anything. If\nmeditators want to reduce desires, they can be seen as wanting not to want, which may sound impossible. This\napparent paradox in Buddhist philosophy has been pointed out by a number of authors: since wanting not to\nwant is a form of wanting, how could one possibly get rid of wanting by such wanting? The paradox is actually\nso obvious that even the Buddha himself, as well as his immediate disciples, were confronted with claims that\nhis system is inherently paradoxical.16 Thinking of the mental process in Buddhist training as letting go, and as\nreduction, should largely resolve this paradox of seemingly wanting not to want. The term “letting go” describes\na reprogramming that reduces mental activity instead of introducing a new desire.\nOne way of interpreting letting go is that it is mental relaxation in the sense of absence of activity and ten-\nsion. Desire and the subsequent goal-setting are about actively engaging in a mental activity, and thus they\nare a kind of opposite to relaxation. Figuratively speaking, just as muscular activity prevents physiological\nrelaxation, wanting is the opposite of mental relaxation in that it relies on specifically activating certain com-\nputational processes. If you set the goal that you don’t want anything, you would actually be just setting one\nmore goal, and increasing mental activity—this is another viewpoint to the paradox we just saw. But if instead,\nyou learn to relax the planning and goal-setting system so that it simply rests, and does not set any goals and\ndoes not plan, then you resolve the paradox of wanting not to want. Furthermore, you can relax the evalu-\nation mechanisms that compute frustration. Learning such relaxation is not easy, but the training methods\ndiscussed in this book were basically all designed to lead towards such a mental relaxation.17\n14To quote Samyutta Nikaya 35.101: “Whatever is not yours: let go of it. Your letting go of it will be for your long-term happiness\nand benefit. And what is not yours? The eye is not yours: let go of it. (...) [Visual] forms are not yours: Let go of them. (...) Eye-\nconsciousness [i.e. visual awareness] is not yours: Let go of it. [The text goes through all the sensory organs, the objects of sensation,\nand the accompanying sensory awarenesses.] The intellect is not yours: let go of it. (...) Ideas are not yours: let go of them. (...)\nWhatever arises (...), experienced either as pleasure, as pain, or as neither-pleasure-nor-pain, that too is not yours: let go of it. Your\nletting go of it will be for your long-term happiness and benefit.” (Translated by Thanissaro Bhikkhu)\n15Alternatively, letting go could be seen as the opposite of attachment, especially if the corresponding term (up¯ad¯ana) is translated\nas “grasping” or “clinging”. However that would require an interpretation of attachment which is quite different from what I have done\nin this book.\n16Samyutta Nikaya 51.15. Dejonckheere et al. (2022) found that social pressure to be happy makes people less happy.\n17I emphasize that there is nothing contradictory or impossible in such training: in particular, there is nothing contradictory in “want-\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n213\nThe ultimate goal of Buddhist training is called nibb¯ana or nirv¯ana, depending on which ancient Indian\nlanguage is used. It is defined as a state devoid of any suffering, the cessation of all suffering. The term literally\nmeans extinction, as in a fire being blown out. It is often described in negative terms such as “unconditioned”,\n“unconstructed”, or even “unborn”, which may sound nonsensical. I think the key to understanding this is that\nnibb¯ana is reached by reducing, and ultimately removing, various mental phenomena, in particular desire; it\nis not about constructing any new mental phenomena. This may again sound paradoxical to any beginning\nmeditator struggling to maintain even a tiny amount of concentration, but I am of course talking about highly\nadvanced stages of practice here. Thus, the best description of the ultimate state may be entirely negative, in\nterms of what it is not, and what it does not contain.18 It is often described as freedom, and in particular it is\nfreedom from those elements of the mind that produce suffering.19\nOne might think such a mind-state with no contents must have neutral valence, and could even be bor-\ning.20 Yet, Buddhist philosophy claims it is extremely happy and pleasant, in fact pure bliss. It is claimed to be\nthe only thing that is not unsatisfactory in any way. This may perhaps be understood if we consider the mind in\nsuch a state to be completely empty, and we have seen that even a relatively empty mind seems to be, for some\nreason, quite happy.21 Nevertheless, we find yet another interesting paradox: How can having a completely\ning to relax all desires”. What is needed is that the agent’s information-processing system creates a desire to relax all other desires, and\nthen takes as its goal the state where all other desires are relaxed. When the agent has relaxed all other desires, this (meta-level) desire\nfor relaxation goes away by itself, just like any desire disappears after its goal has been reached. Thus, in the end, all desires, includ-\ning the meta-level desire, have vanished. Any contradiction is avoided because this meta-level desire to relax desires is only directed\nat other desires, not at itself, and because desires go away automatically when their goal is reached. This is also my interpretation\nof Samyutta Nikaya 51.15, where the paradox of wanting not to want is resolved by explaining how “[a mendicant who is perfected]\nformerly had the desire to attain perfection, but when they attained perfection the corresponding desire faded away” (Trans. Bhikkhu\nSuhato). A complementary approach to resolving this paradox is to consider how the meditation practice changes over a time span of\nmany years. Initially, meditation is based on the desire to reduce suffering, and makes use of the desire to let go or relax. But ultimately,\nyou let go of even the desire to be happy, and, paradoxically, of the desire to let go (or relax). This is possible since you let go of letting\ngo only after a long practice, so the attitudes and habits required for letting go or relaxation are now automated in your neural networks\nand need no effort or explicit desire to operate anymore. You just relax and let go automatically, without desire or planning to do so.\nNote that this is clearly related to the problem of “aversion towards aversion” that we considered above in connection with acceptance.\nIn a similar vein, Striker (2004) emphasizes that Pyrrhonian Skeptics did not (actively and purposefully) suspend judgement, as it is\nsometimes claimed, but rather were unable to arrive at any judgement and gave up any such attempt; see also Herman (1979) for\nfurther analysis of the Buddhist case.\n18Mahasi (2016) gives a traditional Theravadan commentary: “Because there is no arising in the nibb¯ana element [which is the\ncessation of conditioned phenomena through their non-arising], it is called not-born (ajata) and not-brought-to-being (abhhuta).\nBecause it is not made by a cause, it is called not-made (akata). Because it is not made dependent on causes and conditions, it is called\nnot-conditioned.” (see his Chapter “Attainment of Fruition”).\n19While the Four Noble Truths indicate that extinguishing desire accomplishes the goal of removing suffering, aversion (or hate) and\nignorance (or delusion) are usually added to the list of phenomena that have to be extinguished, see e.g. Samyutta Nikaya 38.1. (The\nexact meaning of ignorance/delusion in this context is quite controversial.) Such lists come in various lengths, and ultimately may\ncontain almost all mental phenomena, as when the Buddha says that he teaches “for the elimination of all standpoints, decisions,\nobsessions, adherences, and underlying tendencies, for the stilling of all formations, for the relinquishing of all attachments, for the\ndestruction of craving, for dispassion, for cessation, for Nibbana.” (Majjhima Nikaya 22). It should also be noted that the conception\nof nibb¯ana or nirvana is quite variable among different Buddhist schools. For a detailed account of the early Buddhist view, see Harvey\n(1995). In later Buddhism, there is more emphasis on the extinction of conceptual thinking—as when Nagarjuna says that nirv¯ana is\n“the calming of all verbal differentiations” (Williams, 2008b, p. 75)—as well as the realization of the “nature of mind” (Kyabgon, 2015,\ne.g.,p. 156) which is an advanced form of meta-awareness.\n20But obviously, boredom is a mental phenomenon, akin to an emotion, which cannot exist in a truly empty mind. For a review on\nboredom research, see Danckert and Elpidorou (2023).\n21In particular, the mind might be empty of all perception in addition to thinking, even including proprioception and interoception\n\nCHAPTER 18. RECAPITULATING AND UNIFYING INTERVENTIONS\n214\nempty mind possibly be pleasant since it logically should not contain any pleasure either? I will not try to re-\nsolve this paradox, which seems to reach metaphysical depths; let me just quote S¯ariputta, one of the closest\ndisciples of the Buddha, who put it very simply:22\nJust that is the pleasure here, my friend: where there is nothing felt.\n(feeling of the body, see footnote 24 in Chapter 14), which are partly the basis of the feeling of “self”. The Japanese Zen master Dogen\nsaid that he experienced the “dropping away of body and mind”, while Brahm (2006, p. 158) emphasizes that in deep meditative ab-\nsorption (jh¯ana), “the five senses have shut down”. Clearly, such complete emptiness can only be achieved by letting go of everything,\na total mental relaxation, not by making an effort to empty the mind.\n22Anguttara Nikaya 9.34, translated by Thanissaro Bhikkhu.\n\nChapter 19\nEpilogue\nThere is a wide consensus that trying to build an AI teaches us a lot about what human intelligence is about:\nan AI works as a model of the human mind. I think this also applies to suffering. For sure, a model is not the\nsame as the real thing; some things are always missing. You cannot actually drive to work with a computational\nmodel of a car; mathematical equations of physical forces and chemical reactions written on a piece of paper\ndo not actually make your car accelerate. Yet, it is such models that enable the construction of cars and even\nrockets that fly to the moon.\nA good model can tell us a great deal about the real thing, and thus help science understand how a complex\nsystem works. A model can also enable us to predict what the system does in the future, for example, by pro-\nviding a weather forecast. But from the viewpoint of this book, what really matters is if the model is predictive\nin the following narrow sense: Does it enable us to predict what results interventions have on the system? That\nis, does it help us in changing the system in some way we find preferable?\nThis book proposes that computational models of human suffering can tell us what kind of processes are\nnecessary for suffering. The AI models in this book explicitly showed us some of the conditions, causes, and\nprocesses that have to be operating in order that suffering arises. That means we can develop methods that\nwill reduce suffering: We simply need to remove the necessary conditions, or, at least, make them weaker. This\nis why I think the models in this book are useful, and the later chapters of this book were, in fact, all about\nmethods to reduce suffering.\nIt is possible to argue that an AI or a robot cannot really suffer since it is not conscious. In other words, the\ncomputational processes considered in this book may not be sufficient for suffering if one insists that suffer-\ning must be conscious. However, that is beside the point if our main goal is to develop methods that reduce\nsuffering. Actually, some even claim an AI is not really intelligent—according to some stringent conditions for\nintelligence—yet AI is not only capable of performing some very useful practical tasks, but it has also greatly\nadvanced human neuroscience by giving insight to the computations performed by the brain.\nThe interventions I proposed were mostly identical to what existing philosophical systems propose, while\nI showed how to motivate them using current AI theories. The theory in this book will hopefully be comple-\nmented by further research; I think this is just the very beginning of a long-term scientific endeavour. I hope\nit will lead to more and more efficient interventions in the future, including completely new kinds of interven-\ntions.\nI certainly do not claim that the theory in this book would be either complete or perfect. In particular,\nthere are quite probably mechanisms of suffering which do not fit into the framework of this book. That may\n215\n\nCHAPTER 19. EPILOGUE\n216\nbe the case, for example, for suffering due to certain kinds of social emotions, or existential suffering such as\nlack of meaning of life. The theory in this book also attempts to explain all kinds of suffering—including self-\nneeds, uncertainty, uncontrollability, negative emotions (such as fear and disgust), and stress— by the single\nmechanism of frustration. Whether such a theory based on a single mechanism is satisfactory remains to be\nseen in future research. For example, some interpretations of Buddhist philosophy further maintain that desire\nand aversion in themselves are suffering, and it is not quite clear how that fits the framework in this book.1 As\nalways in science, theories can be rejected, at least partly, as science progresses.\nSummary: Limitations of the agent lead to errors and their monitoring leads to suffering\nTo recapitulate the book in a few paragraphs: we saw several ways in which the limitations of the agent and\nits intelligence lead to suffering. We can succinctly summarize the main problems as uncontrollability, uncer-\ntainty (or unpredictability, or impermanence), and unsatisfactoriness (including insatiability and evolutionary\nobsessions). The agent cannot control its environment as much as it would like; it is not able to perceive or\npredict the world with much certainty; it strives endlessly at goals ultimately given by the programmer (which\nin humans means evolution), unable to ever find satisfaction.\nDue to these limitations, the cognitive system will make errors in its predictions, its plans, and its actions.\nWe saw that suffering is basically a function of the constant evaluation that an intelligent system performs\nregarding its actions, resulting in an error signal. Without such evaluations, the performance cannot be im-\nproved. In particular, error signalling is necessary for the system to learn and update its model of the world.\nFrustration is the central form of such error signalling.\nThe brain and other systems with a particularly sophisticated cognitive architecture use some clever tricks\nto improve their performance. Threat computations give predictions of possible future errors and lead to fear,\nthus providing another mechanism of suffering. Wandering thoughts speed up learning by running learning\nalgorithms in the background; however, they make us experience simulated suffering in addition to the real\none. Emotional interrupts are useful when unexpected things happen and the computational resources need\nto be redirected, but they can be mistuned and lead to unnecessary alarms and suffering. Highly intelligent\nagents may have to use parallel and distributed processing where it is no longer clear if anybody is in control.\nThis means that, unfortunately, intelligent agents increase their own suffering by such mechanisms intended\nto improve future reward. In animals and humans, we also find processes related to self-preservation and self-\nevaluation, which create another layer of suffering.\nThus, the constant monitoring and signalling, even prediction of errors creates constant suffering, and\nparadoxically, the more intelligent the agent, the more error signalling there seems to be. This is what leads to\nthe simple maxim in the title: intelligence is painful.\nYet, the theory of this book is not pessimistic: we also saw a large number of interventions to reduce suf-\nfering. Reducing desires and expectations reduces frustration; meditation enables a metacognitive viewpoint\nthat reduces suffering in general. Therefore, I can claim the model of this book is actually useful: it does, quite\ndirectly, lead to a number of interventions. The interventions presented here are not very different from Bud-\ndhist or Stoic training, but there is a promise of not only better understanding those interventions but further\noptimizing them and developing new ones.\n1See Chapter 9 (page 96) for discussion on this point.\n\nCHAPTER 19. EPILOGUE\n217\nDoes intelligence necessarily lead to suffering?\nIt could thus be argued that suffering is the price to pay for intelligence: without some kind of error monitoring,\nlearning is not possible. It is common sense that errors due to past decisions have to be detected in order to\nlearn to make wiser decisions in the future. Error signals might not be needed if the agent were programmed to\nbe sufficiently intelligent to begin with, so that it would not need any kind of learning, but current AI research\nsuggests that intelligence without learning is very difficult to achieve.\nYet, one might ask if the price is too high, whether intelligence is worth the suffering.2 Would you prefer\nto be a bit dumber if that reduced your suffering? Suppose a drug were developed which abolishes any error-\nsignalling in humans; perhaps that is possible by interfering with the dopamine metabolism. Suppose that as\na logical side effect, it prevents you from learning new reward associations. Would it be worth taking? Actually,\nwe don’t even need to consider such an extreme case where all error signals are removed. How about just taking\na small dose of that drug, so that error-signalling is reduced to some extent? You would suffer less but perhaps\nlearn new things a bit more slowly. What would be the right balance between maximizing performance and\nreducing suffering?\nI have argued that many desires are actually not good for us, and should be seen as evolutionary obsessions.\nSome desires are insatiable, so trying to learn to satisfy them is a fool’s errand. Perhaps most importantly, the\nuncontrollability of the world makes a large proportion of desires completely impossible to satisfy. Clearly,\nfrustration in those cases should be avoided altogether; they present no real trade-off between suffering and\nintelligence. If you really want to be frustrated, better do it in cases where the desires actually serve a useful\npurpose, and you learn to act more efficiently in a meaningful context. We have to also bear in mind that the\nright balance, or the adequacy of any interventions, ultimately depends on the individual and their personal\nvalues and preferences.\nOn the other hand, even if we admit that a certain amount of suffering is necessary as a trade-off to achieve\nintelligence, is it really necessary that such error-signalling should be consciously experienced as suffering?\nEven the most rudimentary AI computes errors while hardly being conscious. We would not say that a thermo-\nstat, arguably the simplest possible system with some intelligence, is suffering or feels pain when it realizes the\ntemperature of the room is not what it is supposed to be. This leads to another thought experiment: How about\na drug that does not reduce error-signalling, but prevents it from reaching our conscious perceptions—would\nyou not take it? In fact, this need not be just a thought experiment. Moving to the level of meta-awareness, as\ndescribed in Chapter 17, seems to reduce the felt impact of all suffering, a bit like such a drug.\nConsciousness is a great mystery. It cannot be entirely avoided in any discussion on intelligence or suffer-\ning, but unfortunately, there is very little we can say with any certainty. One thing which is clear, though, is\nthat the way human consciousness usually operates is not very nice from the viewpoint of suffering. A large\namount of suffering is even created out of nowhere by conscious simulation.\nFrom intelligence to wisdom\nNevertheless, intelligence may not only be a bad thing from the viewpoint of suffering. Intelligence may lead\nto reduction of suffering once it reaches a certain stage, while being embedded in a culture that actively inves-\ntigates where suffering comes from and what can be done; it may lead to the birth of philosophical systems\n2This question was already considered in Chapter 18 from a slightly different angle.\n\nCHAPTER 19. EPILOGUE\n218\nthat question our evolutionary tendencies. Buddhist philosophy, together with the Stoics and other related\nsystems, proposes that we should adopt certain ways of thinking which counteract, and to some extent neu-\ntralize, the causes of suffering. For example, we should give up any attempts to control and accept that things\nare just happening; we should recognize that actually we don’t know much and are always making decisions\nunder uncertainty; we should give up the meaningless and even destructive desires programmed by evolution.\nUltimately, we should recognize the true nature of our consciousness, that we are operating in a kind of virtual\nreality, which bears only some indirect relation to the actual reality.\nSuch proposals are quite radical, and have been recognized as such for centuries. This is not surprising\nsince reducing desires and giving up control are strictly against our evolutionary programming. However, it\nmay not be necessary to follow these ideas to any extreme extent: Buddhist philosophy in itself proposes the\n“middle way”, the idea that going to any extremes is, in the end, counterproductive. Instead of giving up all\ncontrol, for example, we might just give up some of the control, preferably on those things where claiming\ncontrol is most clearly conducive to suffering.\nMost philosophical systems that discourage acting out our desires do recognize that a human being needs\nto take some actions; they do not recommend complete inactivity as some might assume. Stoicism as well as\nTaoism emphasize acting “naturally” (or according to one’s nature), which I would interpret in terms of the\nhabit-based, automated action selection: learned associations between the current state and actions may still\nremain even if no reward is expected or predicted anymore.3 In early Buddhist thought, motivation for mental\ndevelopment is often seen to be a desire of a special kind that should not be eradicated, thus providing another\nmotivation that is not based on reward maximization.4 Some parts of Hindu philosophy suggest performing\none’s duty without any concern for reward,5 while later Buddhist philosophy emphasizes altruistic action as\nthe ultimate motivation for fully enlightened beings.6\nIndeed, this book has almost completely neglected the social aspects of being human—perhaps because\nAI’s are not very social at the moment, and relevant computational theory is scarce. The theory of this book\n3Epictetus recommends to “behave conformably to nature in reaction to how things appear” (The Enchiridion, Paragraph 6), see\nalso the discussion on Marcus Aurelius by Hadot (2002), as well as Sextus Empiricus’s Outlines of Pyrrhonism, 1.23. Laotse (Laozi)\nrecommends “nonaction” or “effortless action”—a highly complex concept with many interpretations. According to Chan (2018), it\n“seems to be used more broadly as a contrast against any form of action characterized by self-serving desire”; “nonaction would be\n’normal’ action in the pristine order of nature, in which the mind is at peace, free from the incessant stirring of desire.” Such natural\naction is closely related, in our cognitive terminology, to automated action selection as in habits. Automated action does lead to\nsome frustration according to our RPE theory, but it seems to be weak as argued in Chapter 9.\nFrom an alternative viewpoint, such\nacting naturally could mean that any attempt to control is minimized by choosing courses of action which are in harmony with the\nenvironment; this interpretation does not, however, explain where the ultimate motivation for action comes from. In any case, it seems\nimportant that such natural action is still constrained by sound moral principles, so that it does not mean just doing whatever one feels\nlike.\n4In early Buddhist philosophy, desires for spiritual development and similar things are called chanda, often translated as “aspira-\ntion”. However, I’m not aware of any principled way of distinguishing between chanda and the “bad” desire (called tanh¯a), so this\nseems to be just assuming an arbitrary exception to the general theory. In fact, in later Buddhist philosophy, even getting rid of the\ndesire for spiritual development is considered important, in line with the discussion on letting go at the end of Chapter 17.\n5Bhagavadgita 2:47 says “You have a right to perform your prescribed duty, but you are not entitled to the fruits of action”. Duty is a\nsocially defined concept, and as such, outside of the scope of this book. Stoic philosophy can be seen in this light as well, if the Greek\nkathekon is translated as “duty”, see p. 172 in Hadot (2002). Acting according to God’s will is another formulation used by Epictetus (e.g.\nThe Discourses, IV:1) and of course by various religious systems.\n6For the Mahayana school, see Oldmeadow (1997), but similar ideas can certainly be found in Theravadan school as well (Brahm,\n2006, p. 245); see also later in the text.\n\nCHAPTER 19. EPILOGUE\n219\nis clearly applicable to the social domain in the sense that social interaction creates its own input data from\nwhich the agent can learn. The agent might then realize that other agents are often quite unpredictable and\nuncontrollable and that this leads to a lot of frustration. Yet, social interaction creates completely new phe-\nnomena, which are outside of the theory of this book but should to be considered to see the whole picture of\nhuman suffering.\nIt can be argued that social interaction is essential for understanding what it is like to be human.7 One\naspect is that human philosophical systems considered in this book are products of a long cultural evolution.\nIt is difficult to see how any single AI could conclude, by itself, that desire produces suffering (or errors) and\nshould be reduced. It is probably impossible for even any single human being to discover anything like those\naforementioned philosophical systems. What is necessary is a cultural learning process based on sharing infor-\nmation between individuals, eventually leading to accumulation of knowledge over many generations.8 Such\nculturally produced, higher kind of intelligence, which can even consider the very concepts of intelligence and\nsuffering as the objects of its analysis, is close to what would better be called wisdom. It is something much\ndeeper than intelligence, and presumably unique to humans.\nFrom individual desires to altruism\nAnother essential aspect of social interaction is the human capacity for compassion, love, gratitude, and similar\nsocial emotions. In classic Buddhist training, there is a group of practices based on the cultivation of positive\nsocial, interpersonal emotions, such as compassion and “loving-kindness”.9 Interestingly, such emotions can\neven be directed towards oneself: As an important example, self-compassion, i.e. compassion directed towards\noneself, may strongly reduce negative self-evaluations, and thus self-related suffering.10 Another book could\npossibly be written where reduction of suffering is approached from the viewpoint of such positive social emo-\ntions. Unfortunately, any related computational theory is rather lacking at this moment.11.\nHistorically, within Buddhism, a self-centered approach to reducing suffering was increasingly criticized\nin the centuries following the Buddha’s death. Consequently, the later Mahayana schools adopted unselfish\nbehavior as the ultimate ideal, instead of your individual nirv¯ana. They proposed that it is better to sacrifice\none’s own bliss and meditation time, at least to some extent, in order to help others to reduce their suffering.\nSlightly paradoxically, such a prosocial attitude is then seen as leading to an even higher form of happiness. I\nwould assume that such enlightened altruistic action somehow avoids the frustration process, perhaps because\nthere is no longer any consideration for rewards that the agent itself will get, so in a sense, the self-based desire\nis no longer operating. It also seems that altruistic action gives its own evolutionary rewards,12 and can even\nprovide meaning to one’s existence.13 Thus, altruistic action, if performed with the proper attitude, may be the\n7(Hari et al., 2015)\n8It is crucial that the shared knowledge is cumulative, i.e., increases from one generation to another, which seems to be extremely\nrare with animals. A suprisingly important mechanism in such cultural learning seems to be imitation, even though at first sight, it\nmight seem very primitive and unrelated to any higher form of intelligence (Iacoboni, 2005; Whiten et al., 2009).\n9For current research, see Graser and Stangier (2018); Hofmann et al. (2011); Cassell (2002), and for the related emotions of forgive-\nness and gratitude, see McCullough and vanOyen-Witvliet (2002) and footnote 18 in Chapter 16. For practical meditation guidance,\nsee e.g. Salzberg (2002).\n10(Neff et al., 2007)\n11Gratitude was briefly discussed in Chapter 18\n12For evolutionary theories of altruism, see Wright (1994); Nowak et al. (2010)\n13On meaning in life and its relation to happiness, see Baumeister and Vohs (2002); Martela (2020); Huta and Waterman (2014).\n\nCHAPTER 19. EPILOGUE\n220\nultimate exercise to reduce suffering—even to the very person performing the action.\nTo conclude, let me quote the Mahayana Buddhist philosopher ´S¯antideva, who recapitulates these ideas\nbrilliantly:14\nAll those who suffer in the world do so because of their desire for their own happiness.\nAll those happy in the world are so because of their desire for the happiness of others.\nHowever, desire to find a meaning for life could also be seen as just another desire that can be frustrated as in van Hooft’s theory\n(page 19).\n14´S¯antideva’s Bodhicary¯avat¯ara, written in the 8th century CE, translated by Kate Crosby and Andrew Skilton, OUP, 1995. Such an\naltruistic attitude is often called the bodhisattva ideal in Buddhist literature (Garfield, 2010; Williams, 2008b).\n\nIndex\nacceptance, 210\naction-value, 51\naddiction, 104, 198\nAfrican savannah, 177\nagency, 137\nagent\ndefinition, 23\nalarms, 107\nAlphaGo, 86, 102, 114\naltruism, 60, 218, 220\namygdala, 81\nanalog, 44\nanger, 95, 104\nanxiety, 73, 117, 120\nAristotle, 19, 44, 46, 73\nassociations\nunconscious, 83, 189\nattachment (Buddhist), 94, 170\nvs letting go, 212\nattention\nbottom-up, 101\ndifferent kinds, 132\nin interrupts, 101, 106\nselective, 131\nin meditation, 195\nsustained, 110\nin meditation, 195\nto reward loss, 172, 200, 202\nautomatization, 87\nof meditation, 195, 198\naversion, 179\ndefinition, 29\nto desires, 210\nawareness, see consciousness\nBayesian inference, 126, 163, 181\nbelief-desire-intention theory, 30\nBellman equation, 51\nbinary values, 44\nbodhisattva, 220\nbody, 108, 121, 138, 194, 214\nboredom, 111, 213\nbrain in a vat, 160\nbroadcasting, 21, 101, 121\nBuddha, 20, 69, 138, 176, 185, 189, 211\ncategories, 44, 87\nlearning c., 46\ncausality\nBuddhist, 176, 211\nin AI, 177\ncentral executive, 143\nCicero, 73\nclinging, see attachment\ncognitive style, 200\ncompassion, 219\ncomputation\ndefinition, 34\nconditioning\nclassical, 57\nextinction, 193\ninstrumental, 53\nconsciousness, 150\nand pain, 16\nand self, 158\n221\n\nINDEX\n222\nand suffering, 12, 156, 168, 203\nbrain basis, 155\nhard problem, 154\nin AI, 156\nin animals, 155\nin emotions, 100\nmeta-, 201\nphenomenal, 151\nutility, 151\ncontemplation, 189\ncontext-dependence\nof categories, 91\nof learning in neural networks, 84\nof perception, 133\ncontrol, 137\nas illusion, 146\nas percept, 145\nby input data, 148, 196\nlack of, see uncontrollability\ncreativity\npositive emotions, 106\nwandering thoughts, 118\ncuriosity, 68\ndata structures, 87\ndeath, 65, 177\ncontemplation of, 187\ndecentering, 202\nDeep Blue, 31, 50\ndefault-mode network, 111\nin meditation, 199\nin rats, 117\ndemon\nDescartes’s evil, 160\nLaplace’s, 180\ndepression, 64, 120, 205, 211\nDescartes, 158, 160\ndesire\nand self, 67, 69, 219\nas elaborated intrusion, 103\nas interrupt and hot, 102, 196\ndefinition, 28, 103\nintrinsic, 68\noccurrent/interrupting, 103\nreducing, 196\nto have no desire, 212\nwanting vs liking, 57\ndigital, 44\ndisappointment, 95\ndiscounting, 49\ndisgust, 102\ndistributed processing, 34, 131\nand self, 137\nand uncontrollability, 139\nnecessity in computers, 142\ndopamine, 57, 61, 95\ndreaming, 113\ndrugs\nincreasing plasticity, 194\nmisperception of reward, 57, 134\ndual process, 80, 131\ncommunication between, 189, 192\ndual system, see dual process\ndukkha, 166, 178\ndynamics\nBuddhist model, 170\ncognitive, of desire, 168, 196\nintrinsic, 111\neffort\nand automatization, 87\nand dual-process, 80\nand perceived reward, 54\nheuristic, 134\nin planning, 50\nin vision, 122\nelaborated-intrusion theory, 103\nemergence, 46\nemotions, 99\naltruistic, see altruism\nas hard-wired programs, 104\nas interrupts, 101\nFrijda’s theory, 104\nmoral, 105\n\nINDEX\n223\npositive, 106\nsocial, positive, 219\nsomatic markers, 108\nemptiness (Buddhist)\ndefinitions, 182\nin flowchart, 166\nof all phenomena, 202\nempty mind, 120, 198, 213\nenergy consumption, 140\nEpictetus, 20, 139, 176, 178, 183, 185–187, 218\nEpicurus, 186\nerror\nclassification e., 39\nprediction e., 39, 57, 58\nreward prediction e., 55, 59\nmathematical definition, 55\nerror signalling, 21, 121, 166, 167\nbroadcasting, see broadcasting\ngeneral, 95\nin control, 145\nwhy conscious, 217\nethics, see morality\nevolution, 11, 59\nAfrican savannah, 60, 107\nand happiness, 60, 119, 121\nas optimization, 41\nshortcuts, 22, 157\nvs learning, 59\nexaptation, 88\nexpectation\ndefinition, 54, 94\nof reward, 134, 175\nreducing, 192\nexperience\nsubjective, see subjective experience\nexperience replay, see replay\nexperience sampling, 111\nexpert system, 45\nextinction\nas in nibb¯ana/nirv¯ana, 213\nof classical conditioning, 193\nfear, 97\nand prediction, 77\nas interrupt, 102\ndual process view, 81\nevolution, 107\nextinction, 193\nphobia, see phobia\nfeedback control, 145\nflow, 198\nand interrupts, 106\nforgiveness, 219\nfour noble truths (Buddhist), 20, 186, 213\nfree will, 147\nfreedom, 209\nFreud, 82\nfrustration\nand general errors, 95\nas reward loss, 18, 54\nas reward prediction error, 55\nas threat, 96\nbased on desire, 28\nbased on desire vs expectation, 94\nbased on planning, 27\nbased on predictions only, 56\ndefinition of baseline, 58\ndefinition of expectation, 54, 94\nfrustation equation, 172\nof internal needs, 77\nof self-needs, 167\nroot causes, 97\nself-needs, 95\nsummarizing different aspects, 93, 172\nfuzzy, 89, 183\ngamble, 72\ngame theory, 105\ngames\nbackgammon, 114\ncheckers, 114\nchess, 31\nDota2, 114\nGo, 86\n\nINDEX\n224\ngeneralization, 92\nGOFAI, 43, 82, 87\nGood old-fashioned AI, see GOFAI\ngradient descent, 40\nstochastic, 40, 43, 83, 85, 141, 190\ngratitude, 199, 209, 219\ngreed, 62\nhabits, 53\ncombining with planning, 85\nslow to learn, 85\nvs emotions, 105\nhappiness, 121, 174, 219, 220\nHebb’s rule, see learning, Hebbian\nHelmholtz, 126\nheuristics, 31\navailability h., 179\ndual-process view, 86\neffort h., 134\nsomatic markers, 108\nstate-values, 50\nhippocampus, 83, 117\nhomeostasis, 17\nHume, 60, 110, 136, 158\nIASP, see pain, IASP definition\nidealism, 161\nignorance (Buddhist), 170, 205\nillusion, 205\nimpermanence, 165, 177, 180\nas nonstationarity, 177\nas uncertainty, 166\nindependent component analysis, 112, 128, 129\nindividual differences, 107, 120, 133, 195\nin effects of thought wandering, 199\ninductive bias, 165\ninference\nBayesian, see Bayesian inference\nunconscious, 125\ninformation processing\nanalog vs digital, 44\ndefinition, 34\ndistributed, see distributed processing\nparallel, see parallel processing\ninsatiability, 61, 166, 217\nintactness of the person, 18, 66, 67, 96\nintelligence\ndefinition, 33, 84\nintention\nand attachment, 94, 170, 196\ndefinition, 29\nreducing, 186, 196\ninterpretability, 88\ninterrupt theory, 101, 144, 167\nintervention, 167, 215\ninvariance, 130\ninverse problem, 124\nirritation, 95\nJames, William, 67\nlanguage, 88\nLaozi, 218\nLaplace, 126, 180\nlearning\nand GOFAI, 45\nas minimization of errors, 39\ncontext-dependent, 84\ndefinition with neural networks, 36\ndistributed, 145\nfederated, 145\nHebbian, 43, 58, 83, 127, 128, 189, 190\nthree-factor, 53\nimitation, 86, 219\nin the brain, 36, 128\nincremental, 40, 190\niterative, 40, 83, 114\nof Bayesian prior, 127\nperceptual, 197, 201\nreinforcement, see reinforcement learning\nself-supervised, 129\nskills, 87\nin meditation, 195, 198\nslow in neural networks, 83, 190\nsupervised, 42\nto learn, 64, 195\n\nINDEX\n225\nto solve planning, 50\ntransfer, 84\nunsupervised, see unsupervised learning\nvs evolution, 59, 127\nletting go, 212\nlocal minima, 69\nloving-kindness, 211, 219\nMadhyamaka, 182\nmeditation, 188, 190\nas speeding up learning, 191\nbreaking cognitive chain, 197\nemptying mind, 198\nextinction of conditioning, 193\nincreasing self-control, 144\ninsight, 190\nmetacognition/meta-awareness, 200\nno-self, 195\nperceptual learning, 197\nrelaxation, 212\nvipassana, 190\nwandering thoughts, 110, 192\nmental pain, see suffering\nmental simulation, see simulation\nmetacognition, 200\nmiddle way, 207, 218\nmind wandering, see wandering thoughts\nmindfulness, see meditation\nmodel, 215\nMontaigne, 79, 120\nMonte Carlo tree seach, see tree search, Monte\nCarlo\nMoore’s law, 140\nmorality, 65, 102, 105, 147\nand animal consciousness, 155\nand evolution, 60\nand expectation, 54\nneural networks\ndefinition, 36\nlearning, see learning\nneuron, 34\nnibb¯ana/nirv¯ana, 213, 219\nno-self, 165\nas no central executive, 144\nBuddhism, 138, 148, 176\ndeconstructing self category, 185\nHume, 159\nin meditation, 195\nletting go of self, 212\nneuroscience, 148\nno-doer, 148\nrealization in meditation, 192\nvs Descartes, 158\nnonreactivity, 211\nnonstationarity, 166, 177\nobjective function, 41\nbased on errors, see error\nbased on rewards, 49\nobsessions\nclinical definition, 61\nevolutionary, 61, 166, 179, 184\nopioid neurotransmitters, 57\noptimization, 39\noverfitting, 40, 163\novergeneralization, 92\npain\nand consciousness, 16\nas interrupt, 102\nchronic, 60, 79, 211\nevolutionary definition, 17\nIASP definition, 16, 66, 67\nuncertainty/controllability, 72\nparallel processing, 34, 125, 131, 139, 141\nin vision, 141\nnecessity in computers, 140\npattern recognition, see vision\nperception, see also vision\ncontext-dependence, 133\nsubjectivity, 133\nphobia, 92, 107\nplanning, 113\nin Go, 85\ncombined with replay, 115\n\nINDEX\n226\ncombining with habits, 85\ncomputational difficulty, 27\ndefinition, 25\nhierarchical, 93\nin animals, 53\nin rats, 117\nplasticity, 36\nincreasing, 193\nPlato, 174, 182, 210\npleasure, 55\nPlutarch, 199\npolicy, 50\nprediction, 57, 58\nof danger, 102\nprior, 125, 129\nBayesian, 126\nlearning, 127\nsubjective, 133\nprioritized sweeping, 116\nPutnam, 160\nQ-value, see action-value\nqualia, 151\nrationality, 108\nregret, 95, 119\ncounterfactual, 54\nreinforcement, see reward\nreinforcement learning, 49, 114\nhierarchical, 93\noptions, 93\nrelaxation, 212\nrelief, 95\nreplay\nactive, 189\nand wandering thoughts, 117\ndefinition, 115\nin rats, 117\nsemantic information, 83\nunsupervised, 116\nresting-state, 111\nreward\ndefinition, 48\ninternal, 68\nperceived, 54, 134, 175\nreward loss\nattention paid, 172, 200, 202\ndefinition, 18, 54\nperceived, 134, 172, 180\nreward prediction error (RPE), see error, reward\nprediction\nrisk, 72, 74\nrisk aversion, 72\nrobot\ncleaning, 24, 49, 50, 61, 62, 116\ndelivery, 50\nfetching orange juice, 85\nrumination, 120, 159, 211\nS¯antideva, 220\nS¯ariputta, 214\nsegmentation, 132\nself, 137\nand frustration, 173\nas category, 91, 185\nas control, 138\nas desire, 67, 69\nas seen by other agents, 64\nbased on internal reward, 70\ndestruction, 65\nesteem and evaluation, 64, 183, 207\nI or me (James), 67\nin Buddhism, 69\nlack of, see no-self\nneeds, 63, 69, 95, 167\nreducing, 183\npreservation, 66, 184\nreducing, 184\nvs consciousness, 158\nself-needs, see self, needs\nSeneca, 186, 187, 190, 203, 210\nsensory deprivation, 193\nshaping, 49\nsignal detection theory, 107\nsimulation, 113, 151, 156, 167, 202\n\nINDEX\n227\nreducing, 184, 198, 199\ntaken for real, 157\nwhy conscious, 157\nSkeptics, 91, 135, 182, 183, 186, 213, 218\nskills\nautomatization, see automatization\nlearning, see learning, skills\nSkinner, 147\nsocial comparison, 54, 64, 184, 207\nsocial interaction, 219\nand evaluation, 64\nas input data, 149\nbasis for consciousness, 152\nemotions, moral, 105\nsocial media, 39, 101, 184\nsocial status, 59\nsociety of mind metaphor, 143\nSocrates, 186\nsomatic marker hypothesis, 108\nstate of world, 24\nstate-value, 50\nas prediction, 59\nimplications, 59\nmathematical definition, 50\nstereotypes, 92\nstochastic gradient descent, see gradient descent,\nstochastic\nStoics, see Epictetus, Seneca, and Plutarch\nstress, 19, 72, 178\nsubjective experience, 151\nand pain, 16\nand suffering, 156, 168, 203\nin emotions, 100\nnot necessary for pain and suffering, 12\nsubjectivity\nof perception, 133, 135, 182\nof thinking, 183\nsuffering\nand consciousness, see consciousness, and\nsuffering\nas error signalling, 21, 95\ndefinition, 18–21\nas frustration, 18, 21\nas reward loss, 18\nBuddha, 20\nCassell, see intactness of the person\nStoics, 20\nvan Hooft, 19, 21, 67, 95\nroot causes, 163, 177\nsuicide, 65, 69, 207\nsurvival, see self, preservation\nsymbol, 44\nsymbolic AI, see GOFAI\nthermostat, 78\nthinking, 151\nabout the future, 57, 113\nabout the past, 113, 120\nas planning, 27\ncategorical, 90\ncounterfactual, 54\nin animals, 118\nnot well-defined, 118\nspontaneous, 117\nwith categories, 44\nthreat, 71\nto the intactness of person, see intactness of\nthe person\nand frustration, 76, 95, 96\nand meta-awareness, 202\ndefinition, 74\nthree characteristics (Buddhist), 166, 175\ntime scales, 94\ntransfer learning, 84\ntree search, 25, 53\nMonte Carlo, 86, 113, 118\ntwelve-link chain, see dynamics, Buddhist model\nuncertainty, 77\nand unpredictability, 180\nfacing it, 177\nof beliefs, 135\nof categorization, 90\nof judgements, 135\nof perception, 126, 180, 200\n\nINDEX\n228\nof reward loss, 134, 180\nof thoughts, 200\nuncontrollability, 77, 106, 165\nBuddhism, 176\nfacing it, 176\nin Buddhism, 138\nof the mind, 119\nStoics, 139\nunexpected behavior, 47, 59, 66\nunpredictability, 77, 165\nand uncertainty, 180\nfacing it, 177\nunsatisfactoriness, 166, 178, 180\nunsupervised learning, 42\nself-supervised, 129\nwith images, 128\nup¯ad¯ana, 170\nvalence\ndefinition, 103\nin Buddhism, 170, 196\nin cognitive dynamics, 168\nin simulation, 157\nvalue function, see state-value or action-value\nvedan¯a, 170, 196\nVedanta, 160\nvipassana, 190\nvirtual reality, 151\nvision, 36, 46, 83, 103, 122, see also perception\nas parallel processing, 141\ndifficulty, 122\nfeature extraction, 128\nillusions, 130\nvisual cortex, 128\nvon Neumann architecture, 44\nwandering thoughts, 110\nand central executive, 144\nand no-self, 192\nin meditation, 192, 199\nincreasing suffering, 120, 167\nobserving them, 201\nwanting, see desire\nwell-being, 174\nwisdom, 219\nworrying, 117, 120, 200\nYogacara, 91, 135, 136, 160, 177\ndefinition of emptiness, 182\nZen, 136, 160, 182\n\nBibliography\nAbler, B., Walter, H., and Erk, S. (2005). Neural correlates of frustration. Neuroreport, 16(7):669–672.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,\nAnadkat, S., et al. (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774.\nAinslie, G. (2001). Breakdown of Will. Cambridge University Press.\nAiraksinen, T. (2019). Vagaries of Desire: A Collection of Philosophical Essays. Brill.\nAitchison, L. and Lengyel, M. (2016). The Hamiltonian brain: efficient probabilistic inference with excitatory-\ninhibitory neural circuit dynamics. PLoS Computational Biology, 12(12):e1005186.\nAmbrose, R. E., Pfeiffer, B. E., and Foster, D. J. (2016). Reverse replay of hippocampal place cells is uniquely\nmodulated by changing reward. Neuron, 91(5):1124–1136.\nAn¯alayo (2003). Satipat.t.h¯ana: The direct path to realization. Birmingham: Windhorse Publications.\nAndrews-Hanna, J. R. (2012). The brain’s default network and its adaptive role in internal mentation. The\nNeuroscientist, 18(3):251–270.\nAnnas, J. E. and Barnes, J. (1985). The Modes of Scepticism: Ancient Texts and Modern Interpretations. Cam-\nbridge University Press.\nAnselme, P. and Güntürkün, O. (2019). How foraging works: uncertainty magnifies food-seeking motivation.\nBehavioral and Brain Sciences, 42:e35.\nAntonov, G., Gagne, C., Eldar, E., and Dayan, P. (2022). Optimism and pessimism in optimised replay. PLOS\nComputational Biology, 18(1):e1009634.\nArandjelovic, R. and Zisserman, A. (2017). Look, listen and learn. In 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 609–617. IEEE.\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint,\narXiv:1907.02893.\nArrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., García, S., Gil-López, S.,\nMolina, D., Benjamins, R., et al. (2020). Explainable artificial intelligence (XAI): Concepts, taxonomies, op-\nportunities and challenges toward responsible ai. Information fusion, 58:82–115.\nBaars, B. J. (1997). In the Theater of Consciousness: The Workspace of the Mind. Oxford University Press.\nBaars, B. J. (2002). The conscious access hypothesis: origins and recent evidence. Trends in Cognitive Sciences,\n6(1):47–52.\nBach, D. R. and Dayan, P. (2017). Algorithms for survival: a comparative perspective on emotions. Nature\nReviews Neuroscience, 18(5):311–319.\nBaddeley, A. (1996). Exploring the central executive. The Quarterly Journal of Experimental Psychology Section\nA, 49(1):5–28.\n229\n\nBIBLIOGRAPHY\n230\nBaer, R. A. (2003). Mindfulness training as a clinical intervention: A conceptual and empirical review. Clinical\npsychology: Science and practice, 10(2):125–143.\nBaird, B., Smallwood, J., and Schooler, J. W. (2011). Back to the future: autobiographical planning and the\nfunctionality of mind-wandering. Consciousness and cognition, 20(4):1604–1611.\nBalcetis, E. and Dunning, D. (2006). See what you want to see: motivational influences on visual perception.\nJournal of personality and social psychology, 91(4):612.\nBar, M. (2004). Visual objects in context. Nature Reviews Neuroscience, 5(8):617.\nBaumeister, R. F. (1990). Suicide as escape from self. Psychological Review, 97(1):90.\nBaumeister, R. F., Bratslavsky, E., Finkenauer, C., and Vohs, K. D. (2001). Bad is stronger than good. Review of\nGeneral Psychology, 5(4):323.\nBaumeister, R. F., Masicampo, E., and DeWall, C. N. (2009). Prosocial benefits of feeling free: Disbelief in free\nwill increases aggression and reduces helpfulness. Personality and social psychology bulletin, 35(2):260–268.\nBaumeister, R. F. and Vohs, K. D. (2002). The pursuit of meaningfulness in life. Handbook of positive psychology,\n1:608–618.\nBaumeister, R. F., Vohs, K. D., and Tice, D. M. (2007). The strength model of self-control. Current directions in\npsychological science, 16(6):351–355.\nBavelier, D., Levi, D. M., Li, R. W., Dan, Y., and Hensch, T. K. (2010). Removing brakes on adult brain plasticity:\nfrom molecular to behavioral interventions. Journal of Neuroscience, 30(45):14964–14971.\nBayne, T., Seth, A. K., Massimini, M., Shepherd, J., Cleeremans, A., Fleming, S. M., Malach, R., Mattingley, J. B.,\nMenon, D. K., Owen, A. M., Peters, M. A., Razi, A., and Mudrik, L. (2024). Tests for consciousness in humans\nand beyond. Trends in Cognitive Sciences, 28(5):454–466.\nBechara, A. and Damasio, A. R. (2005). The somatic marker hypothesis: A neural theory of economic decision.\nGames and economic behavior, 52(2):336–372.\nBelk, R. W., Ger, G., and Askegaard, S. (2003). The fire of desire: A multisited inquiry into consumer passion.\nJournal of consumer research, 30(3):326–351.\nBell, D. E. (1985). Disappointment in decision making under uncertainty. Operations Research, pages 1–27.\nBellemare, M. G., Dabney, W., and Rowland, M. (2023). Distributional reinforcement learning. MIT Press.\nBeran, M. J., Perner, J., and Proust, J. (2012). Foundations of metacognition. Oxford University Press.\nBerkes, P., Orbán, G., Lengyel, M., and Fiser, J. (2011). Spontaneous cortical activity reveals hallmarks of an\noptimal internal model of the environment. Science, 331(6013):83–87.\nBerridge, K. C. and Kringelbach, M. L. (2015). Pleasure systems in the brain. Neuron, 86(3):646–664.\nBeyens, I., Pouwels, J. L., van Driel, I. I., Keijsers, L., and Valkenburg, P. M. (2020). The effect of social media on\nwell-being differs from adolescent to adolescent. Scientific Reports, 10(1):1–11.\nBhatnagar, S., Precup, D., Silver, D., Sutton, R. S., Maei, H. R., and Szepesvári, C. (2009). Convergent temporal-\ndifference learning with arbitrary smooth function approximation. In Advances in Neural Information Pro-\ncessing Systems, pages 1204–1212.\nBirch, J. (2016). Natural selection and the maximization of fitness. Biological Reviews, 91(3):712–727.\nBirch, J. (2017). Animal sentience and the precautionary principle. Animal Sentience: An Interdisciplinary\nJournal on Animal Feeling, 2(16):1.\nBishop, S. R., Lau, M., Shapiro, S., Carlson, L., Anderson, N. D., Carmody, J., Segal, Z. V., Abbey, S., Speca, M.,\nVelting, D., et al. (2004). Mindfulness: a proposed operational definition. Clinical psychology: Science and\n\nBIBLIOGRAPHY\n231\npractice, 11(3):230.\nBlock, N. (1995). On a confusion about a function of consciousness. Behavioral and brain sciences, 18(2):227–\n247.\nBorji, A., Cheng, M.-M., Jiang, H., and Li, J. (2015). Salient object detection: A benchmark. IEEE Transactions\non Image Processing, 24(12):5706–5722.\nBostrom, N. (2003). Are we living in a computer simulation? The Philosophical Quarterly, 53(211):243–255.\nBottou, L. (2003). Stochastic learning. In Summer School on Machine Learning, pages 146–168. Springer.\nBotvinick, M. M. (2012). Hierarchical reinforcement learning and decision making. Current opinion in neuro-\nbiology, 22(6):956–962.\nBotvinick, M. M. and Cohen, J. D. (2014). The computational and neural basis of cognitive control: charted\nterritory and new frontiers. Cognitive science, 38(6):1249–1285.\nBowie, E. (2016). Gymnosophists. In Oxford Classical Dictionary. Oxford University Press.\nBrach, T. (2004). Radical acceptance. Bantam.\nBrahm, A. (2006). Mindfulness, Bliss, and Beyond. Somerville: Wisdom Publications.\nBrandmeyer, T. and Delorme, A. (2021). Meditation and the wandering mind: A theoretical framework of un-\nderlying neurocognitive mechanisms. Perspectives on Psychological Science, 16(1):39–66.\nBratman, M. (1987). Intention, plans, and practical reason. CSLI Publications.\nBrewer, J. A., Elwafi, H. M., and Davis, J. H. (2014). Craving to quit: Psychological models and neurobiological\nmechanisms of mindfulness training as treatment for addictions. Psychol. Addict. Behav., 27(2):366–79.\nBrewer, J. A., Worhunsky, P. D., Gray, J. R., Tang, Y.-Y., Weber, J., and Kober, H. (2011). Meditation experience is\nassociated with differences in default mode network activity and connectivity. Proceedings of the National\nAcademy of Sciences, 108(50):20254–20259.\nBrincat, S. L. and Connor, C. E. (2004). Underlying principles of visual shape selectivity in posterior inferotem-\nporal cortex. Nature Neuroscience, 7(8):880.\nBrochu, E., Cora, V. M., and De Freitas, N. (2010). A tutorial on Bayesian optimization of expensive cost func-\ntions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint,\narXiv:1012.2599.\nBrodaric, B. and Neuhaus, F. (2020). Foundations for an ontology of belief, desire and intention. In Formal\nOntology in Information Systems: Proceedings of the 11th International Conference (FOIS 2020), volume 330,\npage 140. IOS Press.\nBrodersen, K. H., Deserno, L., Schlagenhauf, F., Lin, Z., Penny, W. D., Buhmann, J. M., and Stephan, K. E. (2014).\nDissecting psychiatric spectrum disorders by generative embedding. NeuroImage: Clinical, 4:98–111.\nBrons, R. (2018). Life without belief: A Madhyamaka defense of the livability of Pyrrhonism. Philosophy East\nand West, 68(2):329–351.\nBrooks, R. A. (1991). Intelligence without representation. Artificial intelligence, 47(1-3):139–159.\nBrooks, R. A. (1999). Cambrian intelligence: The early history of the new AI. MIT press.\nBrown, N. and Sandholm, T. (2018). Superhuman AI for heads-up no-limit poker: Libratus beats top profes-\nsionals. Science, 359(6374):418–424.\nBrowne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D.,\nSamothrakis, S., and Colton, S. (2012). A survey of Monte Carlo tree search methods. IEEE Transactions on\nComputational Intelligence and AI in games, 4(1):1–43.\n\nBIBLIOGRAPHY\n232\nBruno, M.-A., Vanhaudenhuyse, A., Thibaut, A., Moonen, G., and Laureys, S. (2011). From unresponsive wake-\nfulness to minimally conscious plus and functional locked-in syndromes: recent advances in our under-\nstanding of disorders of consciousness. Journal of neurology, 258(7):1373–1384.\nBuckner, R. L. (2010). The role of the hippocampus in prediction and imagination. Annual Review of Psychology,\n61:27–48.\nBuckner, R. L., Andrews-Hanna, J. R., and Schacter, D. L. (2008). The brain’s default network. Annals of the New\nYork Academy of Sciences, 1124(1):1–38.\nBueno-Gómez, N. (2017). Conceptualizing suffering and pain. Philosophy, Ethics, and Humanities in Medicine,\n12(1):7.\nBullmore, E. and Sporns, O. (2012). The economy of brain network organization. Nature Reviews Neuroscience,\n13(5):336.\nBurke, B. L., Martens, A., and Faucher, E. H. (2010). Two decades of terror management theory: A meta-analysis\nof mortality salience research. Personality and Social Psychology Review, 14(2):155–195.\nButti, C., Santos, M., Uppal, N., and Hof, P. R. (2013). Von Economo neurons: clinical and evolutionary per-\nspectives. Cortex, 49(1):312–326.\nBuzsáki, G. (1996). The hippocampo-neocortical dialogue. Cerebral Cortex, 6(2):81–92.\nCampbell, M., Hoane Jr, A. J., and Hsu, F.-h. (2002). Deep blue. Artificial intelligence, 134(1-2):57–83.\nCarmody, J. (2015). Reconceptualizing mindfulness. Handbook of mindfulness: Theory, research, and practice,\npages 62–78.\nCarruthers, P. (2009). How we know our own minds: The relationship between mindreading and metacognition.\nBehavioral and brain sciences, 32(2):121–138.\nCarver, C. (2003). Pleasure as a sign you can attend to something else: Placing positive feelings within a general\nmodel of affect. Cognition & Emotion, 17(2):241–261.\nCassell, E. (2002). Compassion. In Snyder, C. and Lopez, S., editors, The Oxford handbook of positive psychology.\nOxford University Press.\nCassell, E. J. (1982). The nature of suffering and the goals of medicine. New England Journal of Medicine,\n306:639–645.\nCassell, E. J. (1989). The relationship between pain and suffering. In Hill, C. S. J. and Fields, W. S., editors,\nAdvances in Pain Research and Therapy. Raven Press: New York.\nCastrén, E. and Antila, H. (2017). Neuronal plasticity and neurotrophic factors in drug responses. Molecular\npsychiatry, 22(8):1085.\nChah, A. (2001). Being Dharma: The essence of the Buddha’s teachings. Shambhala Publications.\nChalmers, D. J. (1995). Facing up to the problem of consciousness. Journal of Consciousness Studies, 2(3):200–\n219.\nChalmers, D. J. (1996). The conscious mind: In search of a fundamental theory. Oxford University Press.\nChan, A. (2018). Laozi. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research\nLab, Stanford University.\nChand,\nS.\nand\nMarwaha,\nR.\n(2023).\nAnxiety.\nIn\nStatPearls\n[Internet].\nStatPearls\nPublishing.\nhttps://www.ncbi.nlm.nih.gov/books/NBK470361/.\nChapman, H. A. and Anderson, A. K. (2012). Understanding disgust. Annals of the New York Academy of Sci-\nences, 1251(1):62–76.\n\nBIBLIOGRAPHY\n233\nCharpentier, B., Senanayake, R., Kochenderfer, M., and Günnemann, S. (2022). Disentangling epistemic and\naleatoric uncertainty in reinforcement learning. arXiv preprint arXiv:2206.01558.\nChaslot, G. M.-B., Winands, M. H., and van Den Herik, H. J. (2008). Parallel Monte Carlo tree search. In Inter-\nnational Conference on Computers and Games, pages 60–71. Springer.\nChella, A. and Manzotti, R. (2007). Artificial consciousness. Andrews UK Limited.\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. (2018). DeepLab: Semantic image seg-\nmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 40(4):834–848.\nChin, J. and Schooler, J. W. (2010). Meta-awareness. Encyclopedia of consciousness, 2:33–41.\nChoudhury, S. and Blakemore, S.-J. (2006). Intentions, actions, and the self. In Pockett, S., Banks, W. P., and\nGallagher, S., editors, Does consciousness cause behavior, pages 39–51. Boston: MIT Press.\nChristoff, K., Gordon, A. M., Smallwood, J., Smith, R., and Schooler, J. W. (2009). Experience sampling during\nfMRI reveals default network and executive system contributions to mind wandering. Proceedings of the\nNational Academy of Sciences, 106(21):8719–8724.\nCiosek, K., Vuong, Q., Loftin, R., and Hofmann, K. (2019). Better exploration with optimistic actor critic. Ad-\nvances in Neural Information Processing Systems, 32.\nClark, A. (2013). Whatever next? predictive brains, situated agents, and the future of cognitive science. Behav-\nioral and brain sciences, 36(3):181–204.\nCleeremans, A. and Tallon-Baudry, C. (2021).\nThe function of consciousness is to generate experience.\nPsyArXiv.\nCohen, M., Quintner, J., and van Rysewyk, S. (2018). Reconsidering the International Association for the Study\nof Pain definition of pain. Pain Reports, 3(2):e634.\nCohen, P. R. and Levesque, H. J. (1990). Intention is choice with commitment. Artificial intelligence, 42(2-\n3):213–261.\nColombetti, G. (2005). Appraising valence. Journal of Consciousness Studies, 12(8-9):103–126.\nColton, S., de Mántaras, R. L., and Stock, O. (2009). Computational creativity: Coming of age. AI Magazine,\n30(3):11.\nCorballis, M. C. (2019). Language, memory, and mental time travel: An evolutionary perspective. Frontiers in\nhuman neuroscience, 13:217.\nCorneo, L., Eder, M., Mohan, N., Zavodovski, A., Bayhan, S., Wong, W., Gunningberg, P., Kangasharju, J., and\nOtt, J. (2021). Surrounded by the clouds: A comprehensive cloud reachability study. In Proceedings of the\nWeb Conference 2021, pages 295–304.\nCorns, J. (2016). Pain eliminativism: scientific and traditional. Synthese, 193(9):2949–2971.\nCosta, V. D. and Averbeck, B. B. (2020). Primate orbitofrontal cortex codes information relevant for managing\nexplore–exploit tradeoffs. Journal of Neuroscience, 40(12):2553–2561.\nCraig, A. D. (2003). A new view of pain as a homeostatic emotion. Trends in neurosciences, 26(6):303–307.\nCraig, A. D. (2009). How do you feel—now? the anterior insula and human awareness. Nature Reviews Neuro-\nscience, 10(1):59–70.\nCrick, F. and Koch, C. (2003). A framework for consciousness. Nature Neuroscience, 6(2):119.\nCrisp, R. (2017). Well-being. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics\nResearch Lab, Stanford University.\n\nBIBLIOGRAPHY\n234\nCritchley, H. and Seth, A. (2012).\nWill studies of macaque insula reveal the neural mechanisms of self-\nawareness? Neuron, 74(3):423–426.\nCroitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. (2023). Diffusion models in vision: A survey. IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\nCsikszentmihalyi, M. (1997). Finding flow: The psychology of engagement with everyday life. Basic Books.\nDabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R., and Botvinick, M. (2020).\nA distributional code for value in dopamine-based reinforcement learning. Nature, 577(7792):671–675.\nDahl, C. J., Lutz, A., and Davidson, R. J. (2015). Reconstructing and deconstructing the self: cognitive mecha-\nnisms in meditation practice. Trends in Cognitive Sciences, 19(9):515–523.\nDalai Lama, Lhündrub, K. P., and Cabezón, J. I. (2011). Meditation on the Nature of Mind. Boston: Wisdom\nPublications.\nDamasio, A. (1994). Descartes’ Error: Emotion, Reason, and the Human Brain. Putnam Publishing.\nDamoiseaux, J., Rombouts, S., Barkhof, F., Scheltens, P., Stam, C., Smith, S. M., and Beckmann, C. (2006).\nConsistent resting-state networks across healthy subjects. Proceedings of the National Academy of Sciences,\n103(37):13848–13853.\nDanckert, J. and Elpidorou, A. (2023). In search of boredom: beyond a functional account. Trends in Cognitive\nSciences.\nDasgupta, P. and Maskin, E. (2005). Uncertainty and hyperbolic discounting. American Economic Review,\n95(4):1290–1299.\nDavidson, J., Liebald, B., Liu, J., Nandy, P., Van Vleet, T., Gargi, U., Gupta, S., He, Y., Lambert, M., Livingston, B.,\net al. (2010). The YouTube video recommendation system. In Proceedings of the fourth ACM conference on\nRecommender systems, pages 293–296. ACM.\nd’Avila Garcez, A. S., Broda, K. B., and Gabbay, D. M. (2012). Neural-symbolic learning systems: foundations and\napplications. Springer Science & Business Media.\nDaw, N. D., Niv, Y., and Dayan, P. (2005). Uncertainty-based competition between prefrontal and dorsolateral\nstriatal systems for behavioral control. Nature Neuroscience, 8(12):1704–1711.\nDawkins, R. (1986). The blind watchmaker: Why the evidence of evolution reveals a universe without design.\nWW Norton & Company.\nDe Berker, A. O., Rutledge, R. B., Mathys, C., Marshall, L., Cross, G. F., Dolan, R. J., and Bestmann, S. (2016).\nComputations of uncertainty mediate acute stress responses in humans. Nature Communications, 7:10996.\nDe Brigard, F. and Prinz, J. (2010). Attention and consciousness. Wiley Interdisciplinary Reviews: Cognitive\nScience, 1(1):51–59.\nde Catanzaro, D. (1991). Evolutionary limits to self-preservation. Ethology and Sociobiology, 12(1):13–28.\nDe Freitas, J., U˘guralp, A. K., O˘guz-U˘guralp, Z., Paul, L., Tenenbaum, J., and Ullman, T. D. (2023). Self-orienting\nin human and machine learning. Nature Human Behaviour, pages 1–14.\nDegrazia, D. (1998). Suffering. In Routledge Encyclopedia of Philosophy. Taylor and Francis.\nDehaene, S., Al Roumi, F., Lakretz, Y., Planton, S., and Sablé-Meyer, M. (2022). Symbols and mental programs:\na hypothesis about human singularity. Trends in Cognitive Sciences.\nDehaene, S. and Changeux, J.-P. (2011). Experimental and theoretical approaches to conscious processing.\nNeuron, 70(2):200–227.\nDehaene, S. and Naccache, L. (2001). Towards a cognitive neuroscience of consciousness: basic evidence and\n\nBIBLIOGRAPHY\n235\na workspace framework. Cognition, 79(1-2):1–37.\nDejonckheere, E., Rhee, J. J., Baguma, P. K., Barry, O., Becker, M., Bilewicz, M., Castelain, T., Costantini, G.,\nDimdins, G., Espinosa, A., et al. (2022). Perceiving societal pressure to be happy is linked to poor well-being,\nespecially in happy nations. Scientific reports, 12(1):1514.\nDennett, D. C. (1992). The self as the center of narrative gravity. In Kessel, F., Cole, P., and Johnson, D., editors,\nSelf and consciousness: Multiple Perspectives. Hillsdale, NJ: Erlbaum.\nDhawale, A. K., Smith, M. A., and Ölveczky, B. P. (2017). The role of variability in motor learning. Annual Review\nof Neuroscience, 40:479–498.\nDiamond, A. (2013). Executive functions. Annual review of psychology, 64:135–168.\nDiba, K. and Buzsáki, G. (2007). Forward and reverse hippocampal place-cell sequences during ripples. Nature\nNeuroscience, 10(10):1241.\nDiCarlo, J. J. and Cox, D. D. (2007). Untangling invariant object recognition. Trends in Cognitive Sciences,\n11(8):333–341.\nDietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function decomposition.\nJournal of Artificial Intelligence Research, 13:227–303.\nDignum, F., Morley, D., Sonenberg, E. A., and Cavedon, L. (2000). Towards socially sophisticated bdi agents. In\nProceedings fourth international conference on multiagent systems, pages 111–118. IEEE.\nDiuk, C., Cohen, A., and Littman, M. L. (2008). An object-oriented representation for efficient reinforcement\nlearning. In Proceedings of the 25th international conference on machine learning (ICML), pages 240–247.\nDolan, R. J. and Dayan, P. (2013). Goals and habits in the brain. Neuron, 80(2):312–325.\nDoya, K. and Uchibe, E. (2005).\nThe cyber rodent project: Exploration of adaptive mechanisms for self-\npreservation and self-reproduction. Adaptive Behavior, 13(2):149–160.\nDoyon, J., Penhune, V., and Ungerleider, L. G. (2003). Distinct contribution of the cortico-striatal and cortico-\ncerebellar systems to motor skill learning. Neuropsychologia, 41(3):252–262.\nDreyfus, G. and Garfield, J. L. (2021). The Madhyamaka contribution to Skepticism. International Journal for\nthe Study of Skepticism, 12(1):4–26.\nDrysdale, A. T., Grosenick, L., Downar, J., Dunlop, K., Mansouri, F., Meng, Y., Fetcho, R. N., Zebley, B., Oathes,\nD. J., Etkin, A., et al. (2017). Resting-state connectivity biomarkers define neurophysiological subtypes of\ndepression. Nature medicine, 23(1):28.\nDubey, R., Griffiths, T. L., and Dayan, P. (2022). The pursuit of happiness: A reinforcement learning perspective\non habituation and comparisons. PLoS computational biology, 18(8):e1010316.\nDuffy, K. R. and Mitchell, D. E. (2013). Darkness alters maturation of visual cortex and promotes fast recovery\nfrom monocular deprivation. Current biology, 23(5):382–386.\nDunne, J. D., Thompson, E., and Schooler, J. (2019).\nMindful meta-awareness:\nSustained and non-\npropositional. Current Opinion in Psychology, 28:307–311.\nDupoux, E. (2018). Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the\ninfant language-learner. Cognition, 173:43–59.\nDurand, M., Shogry, S., and Baltzly, D. (2023). Stoicism. In Zalta, E. N. and Nodelman, U., editors, The Stanford\nEncyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2023 edition.\nEbert, S. and Karehnke, P. (2021). Skewness preferences in choice under risk. Available at SSRN.\nEdwards, S. D. (2003). Three concepts of suffering. Medicine, Health Care and Philosophy, 6(1):59–66.\n\nBIBLIOGRAPHY\n236\nEickenberg, M., Gramfort, A., Varoquaux, G., and Thirion, B. (2017). Seeing it all: Convolutional network layers\nmap the function of the human visual system. NeuroImage, 152:184–194.\nEisenberger, N. I. and Lieberman, M. D. (2004). Why rejection hurts: a common neural alarm system for phys-\nical and social pain. Trends in Cognitive Sciences, 8(7):294–300.\nEisenreich, B. R., Akaishi, R., and Hayden, B. Y. (2017). Control without controllers: toward a distributed neu-\nroscience of executive control. Journal of Cognitive Neuroscience, 29(10):1684–1698.\nEldar, E., Rutledge, R. B., Dolan, R. J., and Niv, Y. (2016). Mood as representation of momentum. Trends in\nCognitive Sciences, 20(1):15–24.\nEmmons, R. and Shelton, C. (2002). Gratitude and the science of positive psychology. In Snyder, C. and Lopez,\nS., editors, The Oxford handbook of positive psychology. Oxford University Press.\nEvans, J. S. B. (2008). Dual-processing accounts of reasoning, judgment, and social cognition. Annual Review\nof Psychology, 59:255–278.\nFagley, N. S. (2016). The construct of appreciation: It is so much more than gratitude. In Perspectives on\ngratitude, pages 84–98. Routledge.\nFederman, A. (2010). What kind of free will did the Buddha teach? Philosophy East and West, pages 1–19.\nFeinman, R. and Lake, B. M. (2018). Learning inductive biases with simple neural networks. In Proceedings of\nthe 40th Annual Meeting of the Cognitive Science Society (CogSci 2018), pages 1657–1662.\nFeldman, G., Greeson, J., and Senville, J. (2010). Differential effects of mindful breathing, progressive muscle\nrelaxation, and loving-kindness meditation on decentering and negative reactions to repetitive thoughts.\nBehaviour research and therapy, 48(10):1002–1011.\nFink, G. (2016). Stress, definitions, mechanisms, and effects outlined: Lessons from anxiety. In Stress: Concepts,\nCognition, Emotion, and Behavior. Handbook of Stress Vol. 1, pages 3–9. Elsevier.\nFirestone, C. and Scholl, B. J. (2014). \"Top-down\" effects where none should be found: The El Greco fallacy in\nperception research. Psychological science, 25(1):38–46.\nFleming, S. M. (2024). Metacognition and confidence: A review and synthesis. Annual Review of Psychology,\n75:241–268.\nFleming, S. M., Dolan, R. J., and Frith, C. D. (2012). Metacognition: computation, biology and function. Philo-\nsophical Transaction of the Royal Society B, 367:1280–1286.\nFletcher, G., editor (2015). The Routledge handbook of philosophy of well-being. Routledge.\nFox, K. C. and Beaty, R. E. (2019). Mind-wandering as creative thinking: neural, psychological, and theoretical\nconsiderations. Current Opinion in Behavioral Sciences, 27:123–130.\nFox, K. C., Nijeboer, S., Solomonova, E., Domhoff, G. W., and Christoff, K. (2013). Dreaming as mind wandering:\nevidence from functional neuroimaging and first-person content reports. Frontiers in human neuroscience,\n7.\nFox, M. D. and Raichle, M. E. (2007). Spontaneous fluctuations in brain activity observed with functional mag-\nnetic resonance imaging. Nature Reviews Neuroscience, 8(9):700.\nFrady, E. P., Kent, S. J., Olshausen, B. A., and Sommer, F. T. (2020). Resonator networks, 1: An efficient so-\nlution for factoring high-dimensional, distributed representations of data structures. Neural computation,\n32(12):2311–2331.\nFranceschelli, G. and Musolesi, M. (2023).\nOn the creativity of large language models.\narXiv preprint\narXiv:2304.00008.\n\nBIBLIOGRAPHY\n237\nFredrickson, B. L. (2001). The role of positive emotions in positive psychology: The broaden-and-build theory\nof positive emotions. American psychologist, 56(3):218.\nFreeman, J. B. and Johnson, K. L. (2016). More than meets the eye: Split-second social perception. Trends in\nCognitive Sciences, 20(5):362–374.\nFresco, D. M., Moore, M. T., van Dulmen, M. H., Segal, Z. V., Ma, S. H., Teasdale, J. D., and Williams, J. M. G.\n(2007). Initial psychometric properties of the experiences questionnaire: validation of a self-report measure\nof decentering. Behavior therapy, 38(3):234–246.\nFrias, A., Watkins, P. C., Webber, A. C., and Froh, J. J. (2011). Death and gratitude: Death reflection enhances\ngratitude. The Journal of Positive Psychology, 6(2):154–162.\nFriese, M., Messner, C., and Schaffner, Y. (2012). Mindfulness meditation counteracts self-control depletion.\nConsciousness and cognition, 21(2):1016–1022.\nFrijda, N. H. (2016).\nThe evolutionary emergence of what we call \"emotions\".\nCognition and Emotion,\n30(4):609–620.\nFriston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127.\nFrith, C. (2002). Attention to action and awareness of other minds. Consciousness and cognition, 11(4):481–487.\nFrith, C. (2010). What is consciousness for? Pragmatics & Cognition, 18(3):497–551.\nFrith, C. (2012). Explaining delusions of control: The comparator model 20 years on. Consciousness and cogni-\ntion, 21(1):52–54.\nGallagher, S. (2000). Philosophical conceptions of the self: implications for cognitive science. Trends in Cogni-\ntive Sciences, 4(1):14–21.\nGallagher, S. (2003). Phenomenology and experimental design toward a phenomenologically enlightened ex-\nperimental science. Journal of Consciousness Studies, 10(9-10):85–99.\nGallagher, S. and Shear, J., editors (1999). Models of the Self. Imprint Academic.\nGao, T., Zhou, Y., Li, W., Pfabigan, D. M., and Han, S. (2020). Neural mechanisms of reinforcement learning\nunder mortality threat. Social neuroscience, 15(2):170–185.\nGärdenfors, P. (2004). Conceptual spaces as a framework for knowledge representation. Mind and matter,\n2(2):9–27.\nGarfield, J. L. (1990). Epoche and ´s¯unyat¯a: Skepticism east and west. Philosophy East and West, pages 285–307.\nGarfield, J. L. (2010). What is it like to be a bodhisattva? moral phenomenology in ´S¯antideva’s Bodhicary¯avat¯ara.\nJournal of the International Association of Buddhist Studies, pages 333–357.\nGarland, E., Froeliger, B., and Howard, M. (2014). Mindfulness training targets neurocognitive mechanisms of\naddiction at the attention-appraisal-emotion interface. Frontiers in psychiatry, 4:173.\nGeorgievski, I. and Aiello, M. (2015). HTN planning: Overview, comparison, and beyond. Artificial Intelligence,\n222:124–156.\nGerritsen, R. J. S. and Band, G. P. H. (2018). Breath of life: The respiratory vagal stimulation model of contem-\nplative activity. Frontiers in Human Neuroscience, 12:397.\nGershman, S. J. (2017). Reinforcement learning and causal models. The Oxford handbook of causal reasoning,\npages 295–306.\nGershman, S. J., Horvitz, E. J., and Tenenbaum, J. B. (2015). Computational rationality: A converging paradigm\nfor intelligence in brains, minds, and machines. Science, 349(6245):273–278.\nGerstner, W., Lehmann, M., Liakoni, V., Corneil, D., and Brea, J. (2018). Eligibility traces and plasticity on\n\nBIBLIOGRAPHY\n238\nbehavioral time scales: experimental support of neohebbian three-factor learning rules. Frontiers in neural\ncircuits, 12.\nGigerenzer, G. and Gaissmaier, W. (2011). Heuristic decision making. Annual Review of Psychology, 62:451–482.\nGirdwood, A. B. (1998). Innovation and Development in the Psychology and Epistemology of Epictetus. PhD\nthesis, University of Oxford.\nGoertzel, B. (2012). Perception processing for general intelligence: Bridging the symbolic/subsymbolic gap. In\nInternational Conference on Artificial General Intelligence, pages 79–88. Springer.\nGoodman, B. and Flaxman, S. (2017). European union regulations on algorithmic decision-making and a \"right\nto explanation\". AI magazine, 38(3):50–57.\nGopnik, A. (2009). Could David Hume have known about Buddhism?: Charles François Dolu, the Royal College\nof La Flèche, and the global Jesuit intellectual network. Hume Studies, 35(1/2):5–28.\nGorder, P. F. (2007). Multicore processors for science and engineering. Computing in science & engineering,\n9(2):3–7.\nGrabovac, A. D., Lau, M. A., and Willett, B. R. (2011). Mechanisms of mindfulness: A Buddhist psychological\nmodel. Mindfulness, 2(3):154–166.\nGrafen, A. (2008). The simplest formal argument for fitness optimization. Journal of genetics, 87(4):421–433.\nGraser, J. and Stangier, U. (2018). Compassion and loving-kindness meditation: an overview and prospects for\nthe application in clinical samples. Harvard Review of Psychiatry, 26(4):201–215.\nGrathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and Swersky, K. (2019). Your classifier\nis secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263.\nGraver, M. (2021). Epictetus. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics\nResearch Lab, Stanford University, Summer 2021 edition.\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi´nska, A., Colmenarejo, S. G.,\nGrefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural network with dy-\nnamic external memory. Nature, 538(7626):471.\nGreen, D. M. and Swets, J. A. (1988). Signal detection theory and psychophysics. Wiley: New York.\nGreff, K., Rasmus, A., Berglund, M., Hao, T., Valpola, H., and Schmidhuber, J. (2016). Tagger: Deep unsupervised\nperceptual grouping. In Advances in Neural Information Processing Systems, pages 4484–4492.\nGross, C. T. and Canteras, N. S. (2012). The many paths to fear. Nature Reviews Neuroscience, 13(9):651.\nGruber, M. J., Ritchey, M., Wang, S.-F., Doss, M. K., and Ranganath, C. (2016). Post-learning hippocampal\ndynamics promote preferential retention of rewarding events. Neuron, 89(5):1110–1120.\nGüçlü, U. and van Gerven, M. A. (2015). Deep neural networks reveal a gradient in the complexity of neural\nrepresentations across the ventral stream. Journal of Neuroscience, 35(27):10005–10014.\nGuestrin, C., Koller, D., Gearhart, C., and Kanodia, N. (2003). Generalizing plans to new environments in rela-\ntional MDPs. In Proceedings of the 18th international joint conference on Artificial intelligence, pages 1003–\n1010. Morgan Kaufmann Publishers Inc.\nGuidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., and Pedreschi, D. (2018). A survey of methods\nfor explaining black box models. ACM Computing Surveys (CSUR), 51(5):93.\nGunaratana, B. H. (2011). Mindfulness in Plain English (20th Anniversary Edition). Wisdom Publications.\nGutmann, M. U., Corander, J., et al. (2016). Bayesian optimization for likelihood-free inference of simulator-\nbased statistical models. Journal of Machine Learning Research, 17:1–47.\n\nBIBLIOGRAPHY\n239\nGutmann, M. U., Dutta, R., Kaski, S., and Corander, J. (2018). Likelihood-free inference via classification. Statis-\ntics and Computing, 28(2):411–425.\nGutmann, M. U. and Hyvärinen, A. (2012). Noise-contrastive estimation of unnormalized statistical models,\nwith applications to natural image statistics. J. of Machine Learning Research, 13:307–361.\nGuyer, P. and Horstmann, R.-P. (2018). Idealism. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy.\nMetaphysics Research Lab, Stanford University.\nHadany, L., Beker, T., Eshel, I., and Feldman, M. W. (2006). Why is stress so deadly? An evolutionary perspective.\nProceedings of the Royal Society of London B: Biological Sciences, 273(1588):881–885.\nHadot, P. (2002). Exercices spirituels et philosophie antique. Albin Michel, revised and augmented edition.\nEarlier edition partly translated as Philosophy as a Way of Life, Wiley, 1995.\nHaggard, P. and Chambon, V. (2012). Sense of agency. Current Biology, 22(10):R390–R392.\nHaidt, J. (2003). The moral emotions. In Davidson, R. J., Scherer, K. R., and Goldsmith, H. H., editors, Handbook\nof affective sciences, pages 852–870. Oxford University Press.\nHan, C., O’Tuathaigh, C. M., van Trigt, L., Quinn, J. J., Fanselow, M. S., Mongeau, R., Koch, C., and Anderson,\nD. J. (2003). Trace but not delay fear conditioning requires attention and the anterior cingulate cortex. Pro-\nceedings of the National Academy of Sciences, 100(22):13087–13092.\nHardt, M., Recht, B., and Singer, Y. (2016). Train faster, generalize better: Stability of stochastic gradient descent.\nIn International Conference on Machine Learning, pages 1225–1234. PMLR.\nHari, R. (2017). From brain–environment connections to temporal dynamics and social interaction: principles\nof human brain function. Neuron, 94(5):1033–1039.\nHari, R., Henriksson, L., Malinen, S., and Parkkonen, L. (2015). Centrality of social interaction in human brain\nfunction. Neuron, 88(1):181–193.\nHari, R., Parkkonen, L., and Nangini, C. (2010). The brain in time: insights from neuromagnetic recordings.\nAnnals of the New York Academy of Sciences, 1191(1):89–109.\nHarnad, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335–346.\nHarnad, S. (2017). To cognize is to categorize: Cognition is categorization. In Handbook of Categorization in\nCognitive Science (Second Edition), pages 21–54. Elsevier.\nHarvey, P. (1995). The selfless mind: Personality, consciousness and nirvana in early Buddhism. Routledge.\nHarvey, P. (2009). Theravada philosophy of mind and the person. Buddhist philosophy: Essential readings,\npages 265–74.\nHassabis, D., Kumaran, D., Vann, S. D., and Maguire, E. A. (2007). Patients with hippocampal amnesia cannot\nimagine new experiences. Proceedings of the National Academy of Sciences, 104(5):1726–1731.\nHayes, S. C. and Pierson, H. (2005). Acceptance and commitment therapy. In Freeman, A., editor, Encyclopedia\nof Cognitive Behavior Therapy. Springer.\nHazan, E., Kakade, S., Singh, K., and Van Soest, A. (2019). Provably efficient maximum entropy exploration. In\nInternational Conference on Machine Learning, pages 2681–2691. PMLR.\nHe, H.-Y., Hodos, W., and Quinlan, E. M. (2006). Visual deprivation reactivates rapid ocular dominance plastic-\nity in adult visual cortex. Journal of Neuroscience, 26(11):2951–2955.\nHeatherton, T. F. (2011). Neuroscience of self and self-regulation. Annual Review of Psychology, 62:363–390.\nHeatherton, T. F., Wyland, C. L., and Lopez, S. (2003). Assessing self-esteem. Positive psychological assessment:\nA handbook of models and measures, pages 219–233.\n\nBIBLIOGRAPHY\n240\nHeathwood, C. (2015). Desire-fulfillment theory. In Fletcher, G., editor, The Routledge handbook of philosophy\nof well-being. Routledge.\nHebb, D. O. (1949). The organization of behavior: a neuropsychological theory. A Wiley Book in Clinical Psy-\nchology., pages 62–78.\nHeffley, W. and Hull, C. (2019). Classical conditioning drives learned reward prediction signals in climbing\nfibers across the lateral cerebellum. elife, 8:e46764.\nHendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J. (2021). Unsolved problems in ML safety. arXiv\npreprint arXiv:2109.13916.\nHerman, A. (1979). A solution to the paradox of desire in buddhism. Philosophy East and West, 29(1):91–94.\nHerry, C., Bach, D. R., Esposito, F., Di Salle, F., Perrig, W. J., Scheffler, K., Lüthi, A., and Seifritz, E. (2007). Process-\ning of temporal unpredictability in human and animal amygdala. Journal of Neuroscience, 27(22):5958–5966.\nHerz, R. S. and von Clef, J. (2001). The influence of verbal labeling on the perception of odors: evidence for\nolfactory illusions? Perception, 30(3):381–391.\nHesslow, G. (2002). Conscious thought as simulation of behaviour and perception. Trends in Cognitive Sciences,\n6(6):242–247.\nHirsh, J. B., Mar, R. A., and Peterson, J. B. (2012).\nPsychological entropy: a framework for understanding\nuncertainty-related anxiety. Psychological Review, 119(2):304.\nHirshleifer, J. (1987). On the emotions as guarantors of threats and promises. In The latest on the best: Essays\non evolution and optimality, pages 307–326. MIT Press: Cambridge, MA.\nHobfoll, S. E. (1989). Conservation of resources: a new attempt at conceptualizing stress. American psycholo-\ngist, 44(3):513.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.\nHoffmaster, B. (2014). Understanding suffering. In R. Green, N. P., editor, Suffering and Bioethics, pages 3–53.\nOxford University Press.\nHofmann, S. G., Grossman, P., and Hinton, D. E. (2011). Loving-kindness and compassion meditation: Potential\nfor psychological interventions. Clinical psychology review, 31(7):1126–1132.\nHofmann, W., Friese, M., and Strack, F. (2009). Impulse and self-control from a dual-systems perspective. Per-\nspectives on psychological science, 4(2):162–176.\nHolroyd, C. B. and Coles, M. G. (2002). The neural basis of human error processing: reinforcement learning,\ndopamine, and the error-related negativity. Psychological Review, 109(4):679.\nHölzel, B. K., Lazar, S. W., Gard, T., Schuman-Olivier, Z., Vago, D. R., and Ott, U. (2011). How does mindfulness\nmeditation work? proposing mechanisms of action from a conceptual and neural perspective. Perspectives\non psychological science, 6(6):537–559.\nHommel, B. (2013). Dancing in the dark: No role for consciousness in action control. Frontiers in Psychology,\n4:380.\nHopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities.\nProceedings of the National Academy of Sciences, 79(8):2554–2558.\nHoyer, P. O. and Hyvärinen, A. (2003). Interpreting neural response variability as Monte Carlo sampling of the\nposterior. In Advances in Neural Information Processing Systems, pages 293–300.\nHüllermeier, E. and Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: An intro-\nduction to concepts and methods. Machine Learning, 110(3):457–506.\n\nBIBLIOGRAPHY\n241\nHuta, V. and Waterman, A. S. (2014). Eudaimonia and its distinction from hedonia: Developing a classification\nand terminology for understanding conceptual and operational definitions. Journal of Happiness Studies,\n15(6):1425–1456.\nHuys, Q. J. and Dayan, P. (2009). A Bayesian formulation of behavioral control. Cognition, 113(3):314–328.\nHyvärinen, A., Hoyer, P. O., and Hurri, J. (2009). Natural Image Statistics: A probabilistic approach to early\ncomputational vision. Springer.\nHyvärinen, A., Karhunen, J., and Oja, E. (2001). Independent Component Analysis. Wiley Interscience.\nHyvärinen, A. and Morioka, H. (2016). Unsupervised feature extraction by time-contrastive learning and non-\nlinear ICA. In Advances in Neural Information Processing Systems, volume 29.\nHyvärinen, A. and Morioka, H. (2017). Nonlinear ICA of temporally dependent stationary sources. In Proc.\nArtificial Intelligence and Statistics (AISTATS2017), Fort Lauderdale, Florida.\nHyvärinen, A. and Oja, E. (1998). Independent component analysis by general nonlinear Hebbian-like learning\nrules. signal processing, 64(3):301–313.\nIacoboni, M. (2005). Neural mechanisms of imitation. Current opinion in neurobiology, 15(6):632–637.\nIannetti, G. D. and Mouraux, A. (2010). From the neuromatrix to the pain matrix (and back). Experimental\nbrain research, 205(1):1–12.\nIannetti, G. D., Salomons, T. V., Moayedi, M., Mouraux, A., and Davis, K. D. (2013). Beyond metaphor: contrast-\ning mechanisms of social and physical pain. Trends in Cognitive Sciences, 17(8):371–378.\nIllanes, L., Yan, X., Icarte, R. T., and McIlraith, S. A. (2020). Symbolic plans as high-level instructions for rein-\nforcement learning. In Proceedings of the international conference on automated planning and scheduling,\nvolume 30, pages 540–550.\nInsel, T. R. and Cuthbert, B. N. (2015). Brain disorders? Precisely. Science, 348(6234):499–500.\nInzlicht, M., Shenhav, A., and Olivola, C. Y. (2018). The effort paradox: Effort is both costly and valued. Trends\nin Cognitive Sciences, 22(4):337–349.\nIrvine, W. B. (2005). On desire: Why we want what we want. Oxford University Press.\nIsele, D. and Cosgun, A. (2018). Selective experience replay for lifelong learning. In Proceedings of the AAAI\nConference on Artificial Intelligence.\nJacob, J., Kent, M., Benson-Amram, S., Herculano-Houzel, S., Raghanti, M. A., Ploppert, E., Drake, J., Hindi, B.,\nNatale, N. R., Daniels, S., et al. (2021). Cytoarchitectural characteristics associated with cognitive flexibility\nin raccoons. Journal of Comparative Neurology.\nJanet, P. (1889). L’Automatisme psychologique. Paris: Felix Alcan.\nJung, R. E., Mead, B. S., Carrasco, J., and Flores, R. A. (2013). The structure of creative cognition in the human\nbrain. Frontiers in human neuroscience, 7:330.\nKabat-Zinn, J. (2012). Mindfulness for beginners: Reclaiming the present moment—and your life. Sounds True.\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus and Giroux: New York.\nKairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cor-\nmode, G., Cummings, R., et al. (2019). Advances and open problems in federated learning. arXiv preprint,\narXiv:1912.04977.\nKane, M. J., Brown, L. H., McVay, J. C., Silvia, P. J., Myin-Germeys, I., and Kwapil, T. R. (2007). For whom the\nmind wanders, and when: An experience-sampling study of working memory and executive control in daily\nlife. Psychological science, 18(7):614–621.\n\nBIBLIOGRAPHY\n242\nKarlsson, M. P. and Frank, L. M. (2009). Awake replay of remote experiences in the hippocampus. Nature\nNeuroscience, 12(7):913–918.\nKasser, T. and Sheldon, K. M. (2000). Of wealth and death: Materialism, mortality salience, and consumption\nbehavior. Psychological science, 11(4):348–351.\nKavanagh, D. J., Andrade, J., and May, J. (2005). Imaginary relish and exquisite torture: the elaborated intrusion\ntheory of desire. Psychological Review, 112(2):446.\nKawato, M., Ohmae, S., Hoang, H., and Sanger, T. (2021). 50 years since the Marr, Ito, and Albus models of the\ncerebellum. Neuroscience, 462:151–174.\nKendall, A. and Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision?\nAdvances in Neural Information Processing Systems, 30.\nKeramati, M. and Gutkin, B. (2014). Homeostatic reinforcement learning for integrating reward collection and\nphysiological stability. Elife, 3:e04811.\nKersten, D., Mamassian, P., and Yuille, A. (2004). Object perception as Bayesian inference. Annual Review of\nPsychology, 55:271–304.\nKhemakhem, I., Kingma, D. P., Monti, R. P., and Hyvärinen, A. (2020). Variational autoencoders and nonlinear\nICA: A unifying framework. In Proc. Artificial Intelligence and Statistics (AISTATS2020).\nKillingsworth, M. A. and Gilbert, D. T. (2010). A wandering mind is an unhappy mind. Science, 330(6006):932–\n932.\nKirchner, H. and Thorpe, S. J. (2006). Ultra-rapid object detection with saccadic eye movements: Visual pro-\ncessing speed revisited. Vision research, 46(11):1762–1776.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho,\nT., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings\nof the National Academy of Sciences, 114(13):3521–3526.\nKirste, I., Nicola, Z., Kronenberg, G., Walker, T. L., Liu, R. C., and Kempermann, G. (2015). Is silence golden?\neffects of auditory stimuli and their absence on adult hippocampal neurogenesis. Brain Structure and Func-\ntion, 220:1221–1228.\nKiviniemi, V., Kantola, J.-H., Jauhiainen, J., Hyvärinen, A., and Tervonen, O. (2003). Independent component\nanalysis of nondeterministic fMRI signal sources. Neuroimage, 19(2):253–260.\nKlein, C. (2007). An imperative theory of pain. The Journal of Philosophy, 104(10):517–532.\nKlinger, E. (2013). Goal commitments and the content of thoughts and dreams: Basic principles. Frontiers in\nPsychology, 4:415.\nKoechlin, E. and Summerfield, C. (2007). An information theoretical approach to prefrontal executive function.\nTrends in Cognitive Sciences, 11(6):229–235.\nKonstan, D. (2018). Epicurus. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics\nResearch Lab, Stanford University.\nKoolhaas, J., Bartolomucci, A., Buwalda, B. d., De Boer, S., Flügge, G., Korte, S., Meerlo, P., Murison, R., Olivier,\nB., Palanza, P., et al. (2011). Stress revisited: a critical evaluation of the stress concept. Neuroscience & Biobe-\nhavioral Reviews, 35(5):1291–1301.\nKording, K. P., Tenenbaum, J. B., and Shadmehr, R. (2007). The dynamics of memory as a consequence of\noptimal adaptation to a changing body. Nature Neuroscience, 10(6):779.\nKorenberg, A. T. and Ghahramani, Z. (2002). A Bayesian view of motor adaptation. Current Psychology of\n\nBIBLIOGRAPHY\n243\nCognition, 21(4/5):537–564.\nK˝oszegi, B. and Rabin, M. (2006). A model of reference-dependent preferences. The Quarterly Journal of Eco-\nnomics, 121(4):1133–1165.\nKouider, S., De Gardelle, V., Sackur, J., and Dupoux, E. (2010). How rich is consciousness? The partial awareness\nhypothesis. Trends in Cognitive Sciences, 14(7):301–307.\nKozachkov, L., Lundqvist, M., Slotine, J.-J., and Miller, E. K. (2020). Achieving stable dynamics in neural circuits.\nPLoS computational biology, 16(8):e1007659.\nKriegeskorte, N. (2015). Deep neural networks: a new framework for modeling biological vision and brain\ninformation processing. Annual Review of Vision Science, 1:417–446.\nKrueger, K. A. and Dayan, P. (2009). Flexible shaping: How learning in small steps helps. Cognition, 110(3):380–\n394.\nKruger, J., Wirtz, D., Van Boven, L., and Altermatt, T. W. (2004). The effort heuristic. Journal of Experimental\nSocial Psychology, 40(1):91–98.\nKurth-Nelson, Z., Behrens, T., Wayne, G., Miller, K., Luettgau, L., Dolan, R., Liu, Y., and Schwartenbeck, P. (2023).\nReplay and compositional computation. Neuron, 111(4):454–469.\nKurth-Nelson, Z., Economides, M., Dolan, R. J., and Dayan, P. (2016). Fast sequences of non-spatial state rep-\nresentations in humans. Neuron, 91(1):194–204.\nKuyken, W., Watkins, E., Holden, E., White, K., Taylor, R. S., Byford, S., Evans, A., Radford, S., Teasdale, J. D., and\nDalgleish, T. (2010). How does mindfulness-based cognitive therapy work? Behaviour research and therapy,\n48(11):1105–1112.\nKuzminski, A. (2007). Pyrrhonism and the M¯adhyamaka. Philosophy East and West, 57(4):482–511.\nKwapis, J. L., Jarome, T. J., and Helmstetter, F. J. (2015). The role of the medial prefrontal cortex in trace fear\nextinction. Learning & Memory, 22(1):39–46.\nKyabgon, T. (2015). Moonbeams of Mahamudra. Carlton North: Shogam Publications.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilis-\ntic program induction. Science, 350(6266):1332–1338.\nLambie, G. W. and Haugen, J. S. (2019). Understanding greed as a unified construct. Personality and Individual\nDifferences, 141:31–39.\nLamme, V. A. and Roelfsema, P. R. (2000). The distinct modes of vision offered by feedforward and recurrent\nprocessing. Trends in neurosciences, 23(11):571–579.\nLao, S.-A., Kissane, D., and Meadows, G. (2016). Cognitive effects of MBSR/MBCT: A systematic review of\nneuropsychological outcomes. Consciousness and cognition, 45:109–123.\nLarsson, G., Maire, M., and Shakhnarovich, G. (2017). Colorization as a proxy task for visual understanding. In\nCVPR, volume 2, page 8.\nLau, H. (2022). In Consciousness we Trust: The Cognitive Neuroscience of Subjective Experience. Oxford Univer-\nsity Press.\nLau, H. and Rosenthal, D. (2011). Empirical support for higher-order theories of conscious awareness. Trends\nin Cognitive Sciences, 15(8):365–373.\nLazarus, R. S. (1993). From psychological stress to the emotions: A history of changing outlooks. Annual review\nof psychology, 44(1):1–22.\nLazarus, R. S. and Folkman, S. (1984). Stress, appraisal, and coping. Springer publishing company.\n\nBIBLIOGRAPHY\n244\nLeDoux, J., Birch, J., Andrews, K., Clayton, N. S., Daw, N. D., Frith, C., Lau, H., Peters, M. A., Schneider, S., Seth,\nA., et al. (2023). Consciousness beyond the human case. Current Biology, 33(16):R832–R840.\nLeDoux, J. E. and Pine, D. S. (2016). Using neuroscience to help understand fear and anxiety: a two-system\nframework. American Journal of Psychiatry, 173(11):1083–1093.\nLegenstein, R., Chase, S. M., Schwartz, A. B., and Maass, W. (2010). A reward-modulated Hebbian learning rule\ncan explain experimentally observed network reorganization in a brain control task. Journal of Neuroscience,\n30(25):8400–8410.\nLegg, S. and Hutter, M. (2007). Universal intelligence: A definition of machine intelligence. Minds and Ma-\nchines, 17(4):391–444.\nLeknes, S., Brooks, J. C., Wiech, K., and Tracey, I. (2008). Pain relief as an opponent process: a psychophysical\ninvestigation. European Journal of Neuroscience, 28(4):794–801.\nLeknes, S. and Tracey, I. (2008). A common neurobiology for pain and pleasure. Nature Reviews Neuroscience,\n9(4):314–320.\nLengyel, M. and Dayan, P. (2008). Hippocampal contributions to control: the third way. In Advances in Neural\nInformation Processing Systems, pages 889–896.\nLerner, T. N., Holloway, A. L., and Seiler, J. L. (2021). Dopamine, updated: reward prediction error and beyond.\nCurrent opinion in neurobiology, 67:123–130.\nLevari, D. E., Gilbert, D. T., Wilson, T. D., Sievers, B., Amodio, D. M., and Wheatley, T. (2018). Prevalence-induced\nconcept change in human judgment. Science, 360(6396):1465–1467.\nLibet, B., Gleason, C. A., Wright, E. W., and Pearl, D. K. (1983). Time of conscious intention to act in relation\nto onset of cerebral activity (readiness-potential) the unconscious initiation of a freely voluntary act. Brain,\n106(3):623–642.\nLieder, F. and Griffiths, T. L. (2020). Resource-rational analysis: Understanding human cognition as the optimal\nuse of limited computational resources. Behavioral and Brain Sciences, 43.\nLin, L. J. (1991). Programming robots using reinforcement learning and teaching. In AAAI, pages 781–786.\nLindsay, E. K. and Creswell, J. D. (2017). Mechanisms of mindfulness training: Monitor and acceptance theory\n(MAT). Clinical Psychology Review, 51:48–59.\nLipton, Z. C., Azizzadenesheli, K., Kumar, A., Li, L., Gao, J., and Deng, L. (2016). Combating reinforcement\nlearning’s sisyphean curse with intrinsic fear. arXiv preprint arXiv:1611.01211.\nLiu, Y.-Y., Slotine, J.-J., and Barabási, A.-L. (2011). Controllability of complex networks. Nature, 473(7346):167.\nLivneh, Y., Ramesh, R. N., Burgess, C. R., Levandowski, K. M., Madara, J. C., Fenselau, H., Goldey, G. J., Diaz,\nV. E., Jikomes, N., Resch, J. M., et al. (2017). Homeostatic circuits selectively gate food cue responses in insular\ncortex. Nature, 546(7660):611.\nLockwood, O. and Si, M. (2022). A review of uncertainty for deep reinforcement learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, pages 155–162.\nLong, A. A. (2002). Epictetus: A Stoic and Socratic Guide to Life. Oxford University Press.\nLong, A. A. (2006). From Epicurus to Epictetus: Studies in Hellenistic and Roman philosophy. Clarendon Press.\nLoth, A., Güntürkün, O., von Fersen, L., and Janik, V. M. (2022). Through the looking glass: how do marked\ndolphins use mirrors and what does it mean? Animal Cognition, 25(5):1151–1160.\nLowet, A. S., Zheng, Q., Matias, S., Drugowitsch, J., and Uchida, N. (2020). Distributional reinforcement learning\nin the brain. Trends in neurosciences, 43(12):980–997.\n\nBIBLIOGRAPHY\n245\nLu, H., Zou, Q., Gu, H., Raichle, M. E., Stein, E. A., and Yang, Y. (2012). Rat brains also have a default mode\nnetwork. Proceedings of the National Academy of Sciences, 109(10):3979–3984.\nLusthaus, D. (1998). Buddhism, Yog¯ac¯ara school of. In Routledge Encyclopedia of Philosophy. Taylor and Fran-\ncis.\nLusthaus, D. (2013). What is and isn’t Yog¯ac¯ara. Yogacara Buddhism Research Association.\nLutz, A., Slagter, H. A., Dunne, J. D., and Davidson, R. J. (2008). Attention regulation and monitoring in medita-\ntion. Trends in Cognitive Sciences, 12(4):163–169.\nLutz, A. and Thompson, E. (2003). Neurophenomenology integrating subjective experience and brain dynam-\nics in the neuroscience of consciousness. Journal of Consciousness Studies, 10(9-10):31–52.\nLy, C., Greb, A. C., Cameron, L. P., Wong, J. M., Barragan, E. V., Wilson, P. C., Burbach, K. F., Zarandi, S. S., Sood,\nA., Paddy, M. R., et al. (2018). Psychedelics promote structural and functional neural plasticity. Cell reports,\n23(11):3170–3182.\nLyubomirsky, S. (2010). Hedonic adaptation to positive and negative experiences. The Oxford handbook of\nstress, health, and coping.\nMa, W. J., Kording, K. P., and Goldreich, D. (2022). Bayesian models of perception and action. MIT Press. In\npress.\nMacDonald, G. (2009). Social pain and hurt feelings. Cambridge handbook of personality psychology, pages\n541–555.\nMacKenzie, M. J. and Baumeister, R. F. (2015). Self-regulatory strength and mindfulness. In Handbook of mind-\nfulness and self-regulation, pages 95–105. Springer.\nMahasi, S. (1996). Great Discourse on Not Self (Anattalakkhana Sutta). Bangkok: Buddhadhamma Foundation.\nMahasi, S. (1999). A Discourse on Dependent Origination. Bangkok: Buddhadhamma Foundation.\nMahasi, S. (2016). Manual of Insight. Simon & Schuster.\nMai, V., Mani, K., and Paull, L. (2022). Sample efficient deep reinforcement learning via uncertainty estimation.\narXiv preprint arXiv:2201.01666.\nMalt, B. C., Ross, B. H., and Murphy, G. L. (1995).\nPredicting features for members of natural categories\nwhen categorization is uncertain. Journal of Experimental Psychology: Learning, Memory, and Cognition,\n21(3):646.\nMancinelli, F., Roiser, J., and Dayan, P. (2021). Internality and the internalisation of failure: Evidence from a\nnovel task. PLoS Computational Biology, 17(7):e1009134.\nMarchetti, I., Koster, E. H., Klinger, E., and Alloy, L. B. (2016). Spontaneous thought and vulnerability to mood\ndisorders: the dark side of the wandering mind. Clinical Psychological Science, 4(5):835–857.\nMarchetti, I., Van de Putte, E., and Koster, E. H. (2014). Self-generated thoughts and depression: from day-\ndreaming to depressive symptoms. Frontiers in human neuroscience, 8:131.\nMarek, S. and Dosenbach, N. U. (2018). The frontoparietal network: function, electrophysiology, and impor-\ntance of individual precision mapping. Dialogues in clinical neuroscience, 20(2):133.\nMarkov, I. L. (2014). Limits on fundamental limits to computation. Nature, 512(7513):147.\nMarkram, H., Gerstner, W., and Sjöström, P. J. (2012). Spike-timing-dependent plasticity: a comprehensive\noverview. Frontiers in synaptic neuroscience, 4:2.\nMartela, F. (2020). A Wonderful Life: Insights on Finding a Meaningful Existence. HarperCollins.\nMartin, D., Fowlkes, C., Tal, D., and Malik, J. (2001). A database of human segmented natural images and\n\nBIBLIOGRAPHY\n246\nits application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. 8th Int’l\nConf. Computer Vision, volume 2, pages 416–423.\nMartin, J., Everitt, T., and Hutter, M. (2016). Death and suicide in universal artificial intelligence. In Interna-\ntional Conference on Artificial General Intelligence, pages 23–32. Springer.\nMasicampo, E. and Baumeister, R. F. (2011). Unfulfilled goals interfere with tasks that require executive func-\ntions. Journal of Experimental Social Psychology, 47(2):300–311.\nMaslow, A. H. (1941). Vii. deprivation, threat, and frustration. Psychological Review, 48(4):364.\nMattar, M. G. and Daw, N. D. (2018). Prioritized memory access explains planning and hippocampal replay.\nNature Neuroscience, 21(11):1609–1617.\nMcCloskey, M. and Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The sequential\nlearning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier.\nMcCullough, M. and vanOyen-Witvliet, C. (2002). The psychology of forgiveness. In Snyder, C. and Lopez, S.,\neditors, The Oxford handbook of positive psychology. Oxford University Press.\nMcDermott, D. (2007). Artificial intelligence and consciousness. The Cambridge handbook of consciousness,\npages 117–150.\nMcEvilley, T. (1982). Pyrrhonism and M¯adhyamika. Philosophy East and West, pages 3–35.\nMcKinsey, M. (2018). Skepticism and Content Externalism. In Zalta, E. N., editor, The Stanford Encyclopedia of\nPhilosophy. Metaphysics Research Lab, Stanford University, Summer 2018 edition.\nMee, S., Bunney, B. G., Reist, C., Potkin, S. G., and Bunney, W. E. (2006). Psychological pain: a review of evidence.\nJournal of Psychiatric Research, 40(8):680–690.\nMeerwijk, E. L. and Weiss, S. J. (2011). Toward a unifying definition of psychological pain. Journal of Loss and\nTrauma, 16(5):402–412.\nMendel, J. M. (1995). Fuzzy logic systems for engineering: a tutorial. Proceedings of the IEEE, 83(3):345–377.\nMetzinger, T. (2003). Being no one. Cambridge, MA: MIT Press.\nMiller, R. R., Barnet, R. C., and Grahame, N. J. (1995). Assessment of the Rescorla–Wagner model. Psychological\nbulletin, 117(3):363.\nMinsky, M. (1988). Society of mind. Simon and Schuster.\nMinut, S. and Mahadevan, S. (2001). A reinforcement learning model of selective visual attention. In Proceed-\nings of the fifth international conference on autonomous agents, pages 457–464. ACM.\nMirolli, M. and Baldassarre, G. (2013). Intrinsically motivated learning in natural and artificial systems. Intrin-\nsically Motivated Learning in Natural and Artificial Systems, pages 49–72.\nMisra, I., Zitnick, C. L., and Hebert, M. (2016). Shuffle and learn: unsupervised learning using temporal order\nverification. In European Conference on Computer Vision, pages 527–544. Springer.\nMiyake, A., Friedman, N. P., Emerson, M. J., Witzki, A. H., Howerter, A., and Wager, T. D. (2000). The unity and\ndiversity of executive functions and their contributions to complex \"frontal lobe\" tasks: A latent variable\nanalysis. Cognitive psychology, 41(1):49–100.\nMnih, V., Heess, N., Graves, A., et al. (2014). Recurrent models of visual attention. In Advances in Neural\nInformation Processing Systems, pages 2204–2212.\nMobbs, D., Headley, D. B., Ding, W., and Dayan, P. (2020). Space, time, and fear: survival computations along\ndefensive circuits. Trends in Cognitive Sciences, 24(3):228–241.\nMomennejad, I., Otto, A. R., Daw, N. D., and Norman, K. A. (2018). Offline replay supports planning in human\n\nBIBLIOGRAPHY\n247\nreinforcement learning. eLife, e32548.\nMonti, M. M., Vanhaudenhuyse, A., Coleman, M. R., Boly, M., Pickard, J. D., Tshibanda, L., Owen, A. M., and\nLaureys, S. (2010). Willful modulation of brain activity in disorders of consciousness. New England Journal\nof Medicine, 362(7):579–589.\nMoore, A. W. and Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less\ntime. Machine learning, 13(1):103–130.\nMorales, J. and Lau, H. (2020). The neural correlates of consciousness. In The Oxford Handbook of the Philoso-\nphy of Consciousness. Oxford University Press.\nMoreno-Bote, R., Knill, D. C., and Pouget, A. (2011). Bayesian sampling in visual perception. Proceedings of the\nNational Academy of Sciences, 108(30):12491–12496.\nMorimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2010). Nonparametric return distri-\nbution approximation for reinforcement learning. In Proceedings of the 27th International Conference on\nMachine Learning (ICML), pages 799–806.\nMorison, B. (2019). Sextus Empiricus. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Meta-\nphysics Research Lab, Stanford University.\nMulder, J. M. (2018). Why intentions? Ratio, 31:51–64.\nMunos, R. (2011). Optimistic optimization of a deterministic function without the knowledge of its smoothness.\nAdvances in Neural Information Processing Systems, 24.\nMurphy, G. L. and Ross, B. H. (2010). Uncertainty in category-based induction: When do people integrate\nacross categories? Journal of Experimental Psychology: Learning, Memory, and Cognition, 36(2):263.\nNakayama, K. (1999). Mid-level vision. The MIT encylopedia of the cognitive sciences.\nNau, D. S., Au, T.-C., Ilghami, O., Kuter, U., Murdock, J. W., Wu, D., and Yaman, F. (2003). SHOP2: an HTN\nplanning system. Journal of artificial intelligence research, 20:379–404.\nNeff, K. D., Kirkpatrick, K. L., and Rude, S. S. (2007). Self-compassion and adaptive psychological functioning.\nJournal of research in personality, 41(1):139–154.\nNesse, R. M. (2000). Is depression an adaptation? Archives of General Psychiatry, 57(1):14–20.\nNevin, J. A. (1999). Analyzing Thorndike’s law of effect: The question of stimulus-response bonds. Journal of\nthe experimental analysis of behavior, 72(3):447–450.\nNg, A. Y., Harada, D., and Russell, S. (1999). Policy invariance under reward transformations: Theory and\napplication to reward shaping. In Icml, volume 99, pages 278–287.\nNIDA (2020).\nHow does cocaine produce its effects?\nNational Institute on Drug Abuse, retrieved\nfrom https://www.drugabuse.gov/publications/research-reports/cocaine/how-does-cocaine-produce-its-\neffects, on 26 Oct 2021.\nNisargadatta, M. (1982). Seeds of Consciousness. New York: Grove Press.\nNiv, Y. (2009). Reinforcement learning in the brain. Journal of Mathematical Psychology, 53(3):139–154.\nNokia, M. S., Lensu, S., Ahtiainen, J. P., Johansson, P. P., Koch, L. G., Britton, S. L., and Kainulainen, H. (2016).\nPhysical exercise increases adult hippocampal neurogenesis in male rats provided it is aerobic and sus-\ntained. The Journal of Physiology, 594(7):1855–1873.\nNowak, A., Gelfand, M. J., Borkowski, W., Cohen, D., and Hernandez, I. (2016). The evolutionary basis of honor\ncultures. Psychological science, 27(1):12–24.\nNowak, M. A., Tarnita, C. E., and Wilson, E. O. (2010). The evolution of eusociality. Nature, 466(7310):1057.\n\nBIBLIOGRAPHY\n248\nNummenmaa, L., Glerean, E., Hari, R., and Hietanen, J. K. (2014). Bodily maps of emotions. Proceedings of the\nNational Academy of Sciences, 111(2):646–651.\nNummenmaa, L., Glerean, E., Viinikainen, M., Jääskeläinen, I. P., Hari, R., and Sams, M. (2012). Emotions\npromote social interaction by synchronizing brain activity across individuals. Proceedings of the National\nAcademy of Sciences, 109(24):9599–9604.\nNutt, D. J., Lingford-Hughes, A., Erritzoe, D., and Stokes, P. R. (2015). The dopamine theory of addiction: 40\nyears of highs and lows. Nature Reviews Neuroscience, 16(5):305.\nOatley, K. and Johnson-Laird, P. N. (1987). Towards a cognitive theory of emotions. Cognition and emotion,\n1(1):29–50.\nOatley, K. and Johnson-Laird, P. N. (1990). Semantic primitives for emotions: A reply to Ortony and Clore.\nCognition and Emotion, 4(2):129–143.\nOja, E. (1982). Simplified neuron model as a principal component analyzer. Journal of mathematical biology,\n15(3):267–273.\nOja, E. (1992).\nPrincipal components, minor components, and linear neural networks.\nNeural networks,\n5(6):927–935.\nOldmeadow, H. (1997). Delivering the last blade of grass: Aspects of the bodhisattva ideal in the Mah¯ay¯ana.\nAsian Philosophy, 7(3):181–194.\nOlshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse\ncode for natural images. Nature, 381(6583):607.\nOpenAI (2018). OpenAI Five. https://blog.openai.com/openai-five/.\nOrbach, I., Mikulincer, M., Sirota, P., and Gilboa-Schechtman, E. (2003). Mental pain: a multidimensional\noperationalization and definition. Suicide and Life-Threatening Behavior, 33(3):219–230.\nO’Reilly, R. C., Hazy, T. E., Mollick, J., Mackie, P., and Herd, S. (2014). Goal-driven cognition in the brain: a\ncomputational framework. arXiv preprint, arXiv:1404.7591.\nOttaviani, C., Shapiro, D., and Couyoumdjian, A. (2013). Flexibility as the key for somatic health: From mind\nwandering to perseverative cognition. Biological psychology, 94(1):38–43.\nPalminteri, S. and Lebreton, M. (2022). The computational roots of positivity and confirmation biases in rein-\nforcement learning. Trends in Cognitive Sciences, 26(7):607–621.\nPalmwood, E. N. and McBride, C. A. (2019). Challenge vs. threat: The effect of appraisal type on resource\ndepletion. Current Psychology, 38(6):1522–1529.\nPan, S. J. and Yang, Q. (2009). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engi-\nneering, 22(10):1345–1359.\nPapies, E. K. and Barsalou, L. W. (2015). Grounding desire and motivated behavior: A theoretical framework\nand review of empirical evidence. The psychology of desire, pages 36–60.\nPapini, M. R., Fuchs, P. N., and Torres, C. (2015). Behavioral neuroscience of psychological pain. Neuroscience\n& Biobehavioral Reviews, 48:53–69.\nParker, G. A. and Smith, J. M. (1990). Optimality theory in evolutionary biology. Nature, 348(6296):27.\nPascanu, R., Li, Y., Vinyals, O., Heess, N., Buesing, L., Racanière, S., Reichert, D., Weber, T., Wierstra, D., and\nBattaglia, P. (2017). Learning model-based planning from scratch. arXiv preprint, arXiv:1707.06170.\nPascual-Leone, A., Amedi, A., Fregni, F., and Merabet, L. B. (2005). The plastic human brain cortex. Annual\nReview of Neuroscience, 28:377–401.\n\nBIBLIOGRAPHY\n249\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised pre-\ndiction. In International Conference on Machine Learning (ICML).\nPeacock, J. (2018). Vedan¯a, ethics and character: A prolegomena. Contemporary Buddhism, 19(1):160–184.\nPearl, J. (2009). Causality. Cambridge University Press.\nPeifer, C., Syrek, C., Ostwald, V., Schuh, E., and Antoni, C. H. (2020). Thieves of flow: How unfinished tasks at\nwork are related to flow experience and wellbeing. Journal of Happiness Studies, 21:1641–1660.\nPeralta, V. and Cuesta, M. J. (2001). How many and which are the psychopathological dimensions in schizophre-\nnia? Issues influencing their ascertainment. Schizophrenia research, 49(3):269–285.\nPerea, G., Navarrete, M., and Araque, A. (2009). Tripartite synapses: astrocytes process and control synaptic\ninformation. Trends in neurosciences, 32(8):421–431.\nPeters, B. and Kriegeskorte, N. (2021). Capturing the objects of vision with neural networks. Nature Human\nBehaviour, 5(9):1127–1144.\nPeters, J., Janzing, D., and Schölkopf, B. (2017). Elements of causal inference: foundations and learning algo-\nrithms. The MIT Press.\nPeters, J., Mülling, K., Kober, J., Nguyen-Tuong, D., and Krömer, O. (2011). Towards motor skill learning for\nrobotics. In Robotics Research, pages 469–482. Springer.\nPeterson, C. (1999). Personal control and well-being. In Well-being: Foundations of hedonic psychology. Russell\nSage Foundation.\nPfeiffer, B. E. and Foster, D. J. (2013). Hippocampal place-cell sequences depict future paths to remembered\ngoals. Nature, 497(7447):74–79.\nPink, D. H. (2022). The power of regret: How looking backward moves us forward. Penguin.\nPinker, S. (1999). How the mind works. Annals of the New York Academy of Sciences, 882(1):119–127.\nPisella, L., Rode, G., Farne, A., Tilikete, C., and Rossetti, Y. (2006). Prism adaptation in the rehabilitation of\npatients with visuo-spatial cognitive disorders. Current opinion in neurology, 19(6):534–542.\nPlatt, M. L. and Huettel, S. A. (2008). Risky business: the neuroeconomics of decision making under uncertainty.\nNature Neuroscience, 11(4):398.\nPockett, S., Banks, W. P., and Gallagher, S., editors (2009). Does consciousness cause behavior? MIT Press.\nPoerio, G. L., Totterdell, P., Emerson, L.-M., and Miles, E. (2015). Love is the triumph of the imagination: Day-\ndreams about significant others are associated with increased happiness, love and connection. Conscious-\nness and Cognition, 33:135–144.\nPoerio, G. L., Totterdell, P., and Miles, E. (2013). Mind-wandering and negative mood: Does one thing really\nlead to another? Consciousness and cognition, 22(4):1412–1421.\nPoole, D. L. and Mackworth, A. K. (2010). Artificial Intelligence: foundations of computational agents. Cam-\nbridge University Press. Free online at https://artint.info/2e/.\nPramote, P. (2013). A Path to Enlightenment I. Prima Publishing: Bangkok.\nPrashanth, L. and Fu, M. C. (2022). Risk-sensitive reinforcement learning via policy gradient search. Founda-\ntions and Trends in Machine Learning, 15(5):537–693.\nPrebble, S. C., Addis, D. R., and Tippett, L. J. (2013). Autobiographical memory and sense of self. Psychological\nbulletin, 139(4):815.\nProust, J. (2010). Metacognition. Philosophy Compass, 5(11):989–998.\nQuiroga, R. Q., Kreiman, G., Koch, C., and Fried, I. (2008). Sparse but not ’grandmother-cell’ coding in the\n\nBIBLIOGRAPHY\n250\nmedial temporal lobe. Trends in Cognitive Sciences, 12(3):87–91.\nQuiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual representation by single\nneurons in the human brain. Nature, 435(7045):1102.\nRaffaeli, W. and Arnaudo, E. (2017). Pain as a disease: an overview. Journal of Pain Research, 10:2003.\nRaichle, M. E. (2015). The brain’s default mode network. Annual Review of Neuroscience, 38:433–447.\nRaichle, M. E., MacLeod, A. M., Snyder, A. Z., Powers, W. J., Gusnard, D. A., and Shulman, G. L. (2001). A default\nmode of brain function. Proceedings of the National Academy of Sciences, 98(2):676–682.\nRaja, S. N., Carr, D. B., Cohen, M., Finnerup, N. B., Flor, H., Gibson, S., Keefe, F. J., Mogil, J. S., Ringkamp,\nM., Sluka, K. A., et al. (2020). The revised international association for the study of pain definition of pain:\nconcepts, challenges, and compromises. Pain, 161(9):1976–1982.\nRao, A. S. and Georgeff, M. P. (1991). Modeling rational agents within a BDI-architecture. KR, 91:473–484.\nRecht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient\ndescent. In Advances in Neural Information Processing Systems, pages 693–701.\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V. (2019). Do imagenet classifiers generalize to imagenet? In\nInternational Conference on Machine Learning, pages 5389–5400. PMLR.\nRedgrave, P. and Gurney, K. (2006). The short-latency dopamine signal: a role in discovering novel actions?\nNature Reviews Neuroscience, 7(12):967.\nRedshaw, J. and Bulley, A. (2018). Future-thinking in animals. The psychology of thinking about the future,\npage 31.\nReggia, J. A. (2013). The rise of machine consciousness: Studying consciousness with computational models.\nNeural Networks, 44:112–131.\nReiss, S., Leen-Thomele, E., Klackl, J., and Jonas, E. (2021). Exploring the landscape of psychological threat: A\ncartography of threats and threat responses. Social and Personality Psychology Compass, 15(4):e12588.\nRevonsuo, A. (2000). The reinterpretation of dreams: An evolutionary hypothesis of the function of dreaming.\nBehavioral and Brain Sciences, 23(6):877–901.\nRoese, N. J. (1997). Counterfactual thinking. Psychological bulletin, 121(1):133.\nRosch, E. (1978). Principles of categorization. In Rosch, E. and Lloyd, B., editors, Cognition and categorization.\nHillsdale, NJ: Lawrence Erlbaum.\nRosch, E. (1999). Reclaiming concepts. Journal of Consciousness Studies, 6(11-12):61–77.\nRosenberg, A. and Bouchard, F. (2011). Fitness. In Zalta, E. N., editor, Stanford Encyclopedia of Philosophy.\nMetaphysics Research Lab, Stanford University.\nRoskies, A. (2006).\nNeuroscientific challenges to free will and responsibility.\nTrends in Cognitive Sciences,\n10(9):419–423.\nRuby, F. J. M., Smallwood, J., Engen, H., and Singer, T. (2013). How self-generated thought shapes mood —\nthe relation between mind-wandering and mood depends on the socio-temporal content of thoughts. PLoS\nONE, 8(10):1–7.\nRueda, M. R., Posner, M. I., and Rothbart, M. K. (2004). Attentional control and self-regulation. Handbook of\nself-regulation: Research, theory, and applications, 2:284–299.\nRussell, S. and Norvig, P. (2020). Artificial intelligence: a modern approach. Pearson Education, 4th edition.\nRussell, S. J. (1997). Rationality and intelligence. Artificial intelligence, 94(1-2):57–77.\nRutledge, R. B., De Berker, A. O., Espenhahn, S., Dayan, P., and Dolan, R. J. (2016). The social contingency of\n\nBIBLIOGRAPHY\n251\nmomentary subjective well-being. Nature Communications, 7:11825.\nRutledge, R. B., Skandali, N., Dayan, P., and Dolan, R. J. (2014). A computational and neural model of momen-\ntary subjective well-being. Proceedings of the National Academy of Sciences, 111(33):12252–12257.\nSafran, J. and Segal, Z. V. (1990). Interpersonal process in cognitive therapy. Jason Aronson, Inc.\nSagi, D. (2011). Perceptual learning in vision research. Vision research, 51(13):1552–1566.\nSale, A., Vetencourt, J. F. M., Medini, P., Cenni, M. C., Baroncelli, L., De Pasquale, R., and Maffei, L. (2007).\nEnvironmental enrichment in adulthood promotes amblyopia recovery through a reduction of intracortical\ninhibition. Nature Neuroscience, 10(6):679.\nSalimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017). Evolution strategies as a scalable alternative to\nreinforcement learning. arXiv preprint, arXiv:1703.03864.\nSalzberg, S. (2002). Lovingkindness: The revolutionary art of happiness. Shambhala Publications.\nSamuel, A. (1959). Some studies in machine learning using the game of checkers. IBM J. of Research and\nDevelopment, 3:210–229.\nSapolsky, R. M. (2004). Why zebras don’t get ulcers. Holt paperbacks, 2nd edition.\nSchaal, S. (1999). Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences, 3(6):233–\n242.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In International Confer-\nence on Learning Representations.\nScherer, K. R. (2009). The dynamic architecture of emotion: Evidence for the component process model. Cog-\nnition and emotion, 23(7):1307–1351.\nScherer, K. R. (2011). On the rationality of emotions: or, when are emotions rational? Social Science Informa-\ntion, 50(3-4):330–350.\nScherer, K. R. (2021). Evidence for the existence of emotion dispositions and the effects of appraisal bias. Emo-\ntion, 21(6):1224.\nSchmidhuber, J. (1991). Curious model-building control systems. In IEEE International Joint Conference on\nNeural Networks, pages 1458–1463.\nSchooler, J. W., Smallwood, J., Christoff, K., Handy, T. C., Reichle, E. D., and Sayette, M. A. (2011).\nMeta-\nawareness, perceptual decoupling and the wandering mind. Trends in Cognitive Sciences, 15(7):319–326.\nSchroeder, J. W. (2004). Skillful means: The heart of Buddhist compassion, volume 54. Motilal Banarsidass Publ.\nSchroeder, T. (2017). Desire. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Metaphysics\nResearch Lab, Stanford University.\nSchultz, W. (2016). Dopamine reward prediction error coding. Dialogues in clinical neuroscience, 18(1):23.\nSchwartz, B. (2004). The paradox of choice: Why more is less. New York: HarperCollins.\nSebastian, C., Burnett, S., and Blakemore, S.-J. (2008). Development of the self-concept during adolescence.\nTrends in Cognitive Sciences, 12(11):441–446.\nSeth, A. (2009). Functions of consciousness. In Banks, W., editor, Encyclopedia of Consciousness, volume 1,\npages 279–293. Elsevier Press.\nSeth, A. K., Baars, B. J., and Edelman, D. B. (2005). Criteria for consciousness in humans and other mammals.\nConsciousness and cognition, 14(1):119–139.\nSeth, A. K. and Tsakiris, M. (2018). Being a beast machine: the somatic basis of selfhood. Trends in Cognitive\nSciences, 22(11):969–981.\n\nBIBLIOGRAPHY\n252\nSeymour, B. (2019). Pain: a precision signal for reinforcement learning and control. Neuron, 101(6):1029–1041.\nSeymour, B., O’Doherty, J. P., Koltzenburg, M., Wiech, K., Frackowiak, R., Friston, K., and Dolan, R. (2005). Op-\nponent appetitive-aversive neural processes underlie predictive learning of pain relief. Nature Neuroscience,\n8(9):1234–1240.\nShapiro, S. L., Carlson, L. E., Astin, J. A., and Freedman, B. (2006). Mechanisms of mindfulness. Journal of\nclinical psychology, 62(3):373–386.\nShen, L., Fishbach, A., and Hsee, C. K. (2015). The motivating-uncertainty effect: Uncertainty increases re-\nsource investment in the process of reward pursuit. Journal of Consumer Research, 41(5):1301–1315.\nShulman, G. L., Fiez, J. A., Corbetta, M., Buckner, R. L., Miezin, F. M., Raichle, M. E., and Petersen, S. E. (1997).\nCommon blood flow changes across visual tasks: II. decreases in cerebral cortex. Journal of Cognitive Neu-\nroscience, 9(5):648–663.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of Go with deep neural networks and tree\nsearch. Nature, 529(7587):484.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. (2017). Mastering the game of Go without human knowledge. Nature, 550(7676):354.\nSilver, D., Singh, S., Precup, D., and Sutton, R. S. (2021). Reward is enough. Artificial Intelligence, 299:103535.\nSimon, H. A. (1967). Motivational and emotional controls of cognition. Psychological Review, 74(1):29.\nSimon, H. A. (1972). Theories of bounded rationality. Decision and organization, 1(1):161–176.\nSinger, A. C. and Frank, L. M. (2009). Rewarded outcomes enhance reactivation of experience in the hippocam-\npus. Neuron, 64(6):910–921.\nSinger, T., Seymour, B., O’Doherty, J., Kaube, H., Dolan, R. J., and Frith, C. D. (2004). Empathy for pain involves\nthe affective but not sensory components of pain. Science, 303(5661):1157–1162.\nSingh, P. (2012). Examining the society of mind. Computing and Informatics, 22(6):521–543.\nSingh, S., Lewis, R. L., and Barto, A. G. (2009). Where do rewards come from. In Proceedings of the annual\nconference of the cognitive science society, pages 2601–2606. Cognitive Science Society.\nSingh, S., Lewis, R. L., Barto, A. G., and Sorg, J. (2010). Intrinsically motivated reinforcement learning: An\nevolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70–82.\nSkinner, E. A. (1996). A guide to constructs of control. Journal of personality and social psychology, 71(3):549.\nSloman, S. A. (1996). The empirical case for two systems of reasoning. Psychological bulletin, 119(1):3.\nSmallwood, J., Fitzgerald, A., Miles, L. K., and Phillips, L. H. (2009). Shifting moods, wandering minds: negative\nmoods lead the mind to wander. Emotion, 9(2):271.\nSmallwood, J., McSpadden, M., and Schooler, J. W. (2007). The lights are on but no one’s home: Meta-awareness\nand the decoupling of attention when the mind wanders. Psychonomic bulletin & review, 14(3):527–533.\nSmith, J. (2017). Self-consciousness. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Meta-\nphysics Research Lab, Stanford University.\nSmith, J. M. and Price, G. R. (1973). The logic of animal conflict. Nature, 246(5427):15.\nSpence, S. A., Brooks, D. J., Hirsch, S. R., Liddle, P. F., Meehan, J., and Grasby, P. M. (1997). A pet study of vol-\nuntary movement in schizophrenic patients experiencing passivity phenomena (delusions of alien control).\nBrain: a journal of neurology, 120(11):1997–2011.\nSpira, R. (2017). The nature of consciousness. Oxford: Sahaja Publications.\n\nBIBLIOGRAPHY\n253\nSpolidoro, M., Baroncelli, L., Putignano, E., Maya-Vetencourt, J. F., Viegi, A., and Maffei, L. (2011). Food restric-\ntion enhances visual cortex plasticity in adulthood. Nature Communications, 2:320.\nSridharan, D., Levitin, D. J., and Menon, V. (2008). A critical role for the right fronto-insular cortex in switching\nbetween central-executive and default-mode networks. Proceedings of the National Academy of Sciences,\n105(34):12569–12574.\nStanovich, K. E. (2004). The Robot’s Rebellion: Finding Meaning in the Age of Darwin. University of Chicago\nPress.\nStawarczyk, D., Majerus, S., and D’Argembeau, A. (2013). Concern-induced negative affect is associated with\nthe occurrence and content of mind-wandering. Consciousness and Cognition, 22(2):442–448.\nSteels, L. (2008). The symbol grounding problem has been solved. So what’s next. Symbols and embodiment:\nDebates on meaning and cognition, pages 223–244.\nStein, D., Kogan, C., Atmaca, M., Fineberg, N., Fontenelle, L., Grant, J., Matsunaga, H., Reddy, Y., Simpson, H.,\nThomsen, P., et al. (2016). The classification of obsessive–compulsive and related disorders in the ICD-11.\nJournal of Affective Disorders, 190:663–674.\nStephan, K. E., Manjaly, Z. M., Mathys, C. D., Weber, L. A., Paliwal, S., Gard, T., Tittgemeyer, M., Fleming, S. M.,\nHaker, H., Seth, A. K., et al. (2016). Allostatic self-efficacy: A metacognitive theory of dyshomeostasis-induced\nfatigue and depression. Frontiers in human neuroscience, 10:550.\nSterzer, P., Kleinschmidt, A., and Rees, G. (2009). The neural bases of multistable perception. Trends in Cognitive\nSciences, 13(7):310–318.\nStrawson, G. and Watson, G. (1998). Free will. In Routledge Encyclopedia of Philosophy. Taylor and Francis.\nStriker, G. (2004). Historical reflections on classical Pyrrhonism and neo-Pyrrhonism. In Sinnott-Armstrong,\nW., editor, Pyrrhonian Skepticism, pages 13–24. Oxford University Press.\nSu, G., Wei, D., Varshney, K. R., and Malioutov, D. M. (2015). Interpretable two-level boolean rule learning for\nclassification. arXiv preprint, arXiv:1511.07361.\nSuch, F. P., Madhavan, V., Conti, E., Lehman, J., Stanley, K. O., and Clune, J. (2017). Deep neuroevolution: genetic\nalgorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv\npreprint, arXiv:1712.06567.\nSun, R., Slusarz, P., and Terry, C. (2005). The interaction of the explicit and the implicit in skill learning: A\ndual-process approach. Psychological Review, 112(1):159.\nSutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bul-\nletin, 2(4):160–163.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press: Cambridge, 2nd\nedition.\nSutton, R. S., Precup, D., and Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal\nabstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211.\nSvenaeus, F. (2014). The phenomenology of suffering in medicine and bioethics. Theoretical medicine and\nbioethics, 35(6):407–420.\nTabor, A., Thacker, M. A., Moseley, G. L., and Körding, K. P. (2017). Pain: a statistical account. PLoS computa-\ntional biology, 13(1):e1005142.\nTaddeo, M. and Floridi, L. (2005). Solving the symbol grounding problem: a critical review of fifteen years of\nresearch. Journal of Experimental & Theoretical Artificial Intelligence, 17(4):419–445.\n\nBIBLIOGRAPHY\n254\nTagawa, S. (2009). Living Yog¯ac¯ara: An Introduction to Consciousness-Only Buddhism. Simon and Schuster.\nTranslated by Charles Muller.\nTakahashi, Y. K., Batchelor, H. M., Liu, B., Khanna, A., Morales, M., and Schoenbaum, G. (2017). Dopamine\nneurons respond to errors in the prediction of sensory features of expected rewards. Neuron, 95(6):1395–\n1405.\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In Advances in Neural\nInformation Processing Systems, pages 2154–2162.\nTanaka, K. (1996). Inferotemporal cortex and object vision. Annual Review of Neuroscience, 19(1):109–139.\nTate, T. and Pearlman, R. (2019). What we mean when we talk about suffering–and why Eric Cassell should not\nhave the last word. Perspectives in biology and medicine, 62(1):95–110.\nTeasdale, J. D. (1999). Metacognition, mindfulness and the modification of mood disorders. Clinical Psychology\n& Psychotherapy, 6(2):146–155.\nTeasdale, J. D. and Chaskalson, M. (2011a). How does mindfulness transform suffering? I: The nature and\norigins of dukkha. Contemporary Buddhism, 12(01):89–102.\nTeasdale, J. D. and Chaskalson, M. (2011b). How does mindfulness transform suffering? II: the transformation\nof dukkha. Contemporary Buddhism, 12(1):103–124.\nTeasdale, J. D., Segal, Z. V., Williams, J. M. G., Ridgeway, V. A., Soulsby, J. M., and Lau, M. A. (2000). Prevention of\nrelapse/recurrence in major depression by mindfulness-based cognitive therapy. Journal of consulting and\nclinical psychology, 68(4):615.\nTejaniya, A. (2008). Awareness alone is not enough. Selangor: Auspicious Affinity.\nTeper, R. and Inzlicht, M. (2013). Meditation, mindfulness and executive control: the importance of emotional\nacceptance and brain-based performance monitoring. Social cognitive and affective neuroscience, 8(1):85–\n92.\nTesauro, G. (1995). Temporal difference learning and TD-gammon. Communications of the ACM, 38(3):58–68.\nTheis, T. N. and Wong, H.-S. P. (2017). The end of Moore’s law: A new beginning for information technology.\nComputing in Science & Engineering, 19(2):41–50.\nThierry, B., Steru, L., Chermat, R., and Simon, P. (1984). Searching-waiting strategy: A candidate for an evolu-\ntionary model of depression? Behavioral and neural biology, 41(2):180–189.\nThrun, S. and Pratt, L., editors (2012). Learning to learn. Springer: New York.\nToda, K. and Platt, M. L. (2015). Animal cognition: monkeys pass the mirror test. Current Biology, 25(2):R64–\nR66.\nToivonen, H. and Gross, O. (2015). Data mining and machine learning in computational creativity. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge Discovery, 5(6):265–275.\nTononi, G. and Edelman, G. M. (1998). Consciousness and complexity. Science, 282(5395):1846–1851.\nTossani, E. (2013). The concept of mental pain. Psychotherapy and psychosomatics, 82(2):67–73.\nTrautmann, S. T. and van de Kuilen, G. (2018). Higher order risk attitudes: A review of experimental evidence.\nEuropean Economic Review, 103:108–124.\nTresp, V., Sharifzadeh, S., Li, H., Konopatzki, D., and Ma, Y. (2023). The tensor brain: A unified theory of per-\nception, memory, and semantic decoding. Neural Computation, 35(2):156–227.\nTsakiris, M., Hesse, M. D., Boy, C., Haggard, P., and Fink, G. R. (2007). Neural signatures of body ownership: a\nsensory network for bodily self-consciousness. Cerebral cortex, 17(10):2235–2244.\n\nBIBLIOGRAPHY\n255\nTurner, A., Ratzlaff, N., and Tadepalli, P. (2020). Avoiding side effects in complex environments. Advances in\nNeural Information Processing Systems, 33:21406–21415.\nTversky, A. and Kahneman, D. (1974).\nJudgment under uncertainty:\nHeuristics and biases.\nScience,\n185(4157):1124–1131.\nVago, D. R. and David, S. A. (2012). Self-awareness, self-regulation, and self-transcendence (S-ART): A frame-\nwork for understanding the neurobiological mechanisms of mindfulness. Frontiers in human neuroscience,\n6:296.\nVan Boven, L. (2005). Experientialism, materialism, and the pursuit of happiness. Review of General Psychology,\n9(2):132–142.\nVan de Cruys, S. (2017). Affective value in the predictive mind. MIND Group; Frankfurt am Main.\nVan Gulick, R. (2021). Consciousness. In Zalta, E. N., editor, The Stanford Encyclopedia of Philosophy. Meta-\nphysics Research Lab, Stanford University, Winter 2021 edition.\nVan Hateren, J. H. and van der Schaaf, A. (1998). Independent component filters of natural images compared\nwith simple cells in primary visual cortex. Proceedings of the Royal Society of London. Series B: Biological\nSciences, 265(1394):359–366.\nVan Hooft, S. (1998). Suffering and the goals of medicine. Medicine, Health Care and Philosophy, 1(2):125–131.\nvan Vugt, M. K., Taatgen, N. A., Sackur, J., Bastian, M., Borst, J., and Mehlhorn, K. (2015). Modeling mind-\nwandering: a tool to better understand distraction. In Proceedings of the 13th International Conference on\nCognitive Modeling, pages 252–257. University of Groningen Groningen, Netherlands.\nVan Vugt, M. K., van der Velde, M., and ESM-MERGE Investigators (2018). How does rumination impact cogni-\ntion? A first mechanistic model. Topics in Cognitive Science, 10(1):175–191.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).\nAttention is all you need. Advances in Neural Information Processing Systems, 30.\nVeehof, M. M., Trompetter, H., Bohlmeijer, E. T., and Schreurs, K. M. G. (2016). Acceptance-and mindfulness-\nbased interventions for the treatment of chronic pain: a meta-analytic review. Cognitive behaviour therapy,\n45(1):5–31.\nVerduyn, P., Lee, D. S., Park, J., Shablack, H., Orvell, A., Bayer, J., Ybarra, O., Jonides, J., and Kross, E. (2015).\nPassive facebook usage undermines affective well-being: Experimental and longitudinal evidence. Journal\nof Experimental Psychology: General, 144(2):480.\nVerhaeghen, P. (2017). The self-effacing Buddhist: No(t)-self in early Buddhism and contemplative neuro-\nscience. Contemporary Buddhism, 18(1):21–36.\nVetencourt, J. F. M., Sale, A., Viegi, A., Baroncelli, L., De Pasquale, R., O’Leary, O. F., Castrén, E., and Maffei, L.\n(2008). The antidepressant fluoxetine restores plasticity in the adult visual cortex. Science, 320(5874):385–\n388.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features\nwith denoising autoencoders. In Proceedings of the 25th International Conference on Machine learning, pages\n1096–1103.\nVogel, E. A., Rose, J. P., Roberts, L. R., and Eckles, K. (2014). Social comparison, social media, and self-esteem.\nPsychology of popular media culture, 3(4):206.\nVohs, K. D. and Schooler, J. W. (2008). The value of believing in free will: Encouraging a belief in determinism\nincreases cheating. Psychological science, 19(1):49–54.\n\nBIBLIOGRAPHY\n256\nVon Neumann, J. and Morgenstern, O. (1944). Theory of games and economic behavior. Princeton University\nPress.\nWagemans, J. (2015). Historical and conceptual background: Gestalt theory. The Oxford handbook of perceptual\norganization, pages 3–20.\nWager, T. D., Atlas, L. Y., Botvinick, M. M., Chang, L. J., Coghill, R. C., Davis, K. D., Iannetti, G. D., Poldrack, R. A.,\nShackman, A. J., and Yarkoni, T. (2016). Pain in the ACC? Proceedings of the National Academy of Sciences,\n113(18):E2474–E2475.\nWagner, K., Reggia, J. A., Uriagereka, J., and Wilkinson, G. S. (2003). Progress in the simulation of emergent\ncommunication and language. Adaptive Behavior, 11(1):37–69.\nWalter, S. (2009). Epiphenomenalism. In Encyclopedia of neuroscience, pages 1137–1139. Springer.\nWarstadt, A. and Bowman, S. R. (2022). What artificial neural networks can tell us about human language\nacquisition. Algebraic Structures in Natural Language, pages 17–60.\nWatkins, E. and Teasdale, J. D. (2004). Adaptive and maladaptive self-focus in depression. Journal of affective\ndisorders, 82(1):1–8.\nWatkins, P. C. (2013). Gratitude and the good life: Toward a psychology of appreciation. Springer.\nWegner, D. M. (2002). The illusion of conscious will. Cambridge, MA: MIT Press.\nWegner, D. M. (2003). The mind’s best trick: how we experience conscious will. Trends in Cognitive Sciences,\n7(2):65–69.\nWeike, A. I., Schupp, H. T., and Hamm, A. O. (2007). Fear acquisition requires awareness in trace but not delay\nconditioning. Psychophysiology, 44(1):170–180.\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big data, 3(1):9.\nWen, W. and Imamizu, H. (2022). The sense of agency in perception, behaviour and human–machine interac-\ntions. Nature Reviews Psychology, 1(4):211–222.\nWestbrook, R. F., Iordanova, M., McNally, G., Richardson, R., and Harris, J. A. (2002). Reinstatement of fear to\nan extinguished conditioned stimulus: two roles for context. Journal of Experimental Psychology: Animal\nBehavior Processes, 28(1):97.\nWhiten, A., McGuigan, N., Marshall-Pescini, S., and Hopper, L. M. (2009). Emulation, imitation, over-imitation\nand the scope of culture for child and chimpanzee. Philosophical Transactions of the Royal Society B: Biolog-\nical Sciences, 364(1528):2417–2428.\nWhitmer, A. J. and Gotlib, I. H. (2013). An attentional scope model of rumination. Psychological bulletin,\n139(5):1036.\nWiech, K., Ploner, M., and Tracey, I. (2008). Neurocognitive aspects of pain perception. Trends in Cognitive\nSciences, 12(8):306–313.\nWielgosz, J., Goldberg, S. B., Kral, T. R., Dunne, J. D., and Davidson, R. J. (2019). Mindfulness meditation and\npsychopathology. Annual review of clinical psychology, 15:285–316.\nWikenheiser, A. M. and Redish, A. D. (2015). Hippocampal theta sequences reflect current goals. Nature Neu-\nroscience, 18(2):289–294.\nWilliams, J. M. G. (2008a). Mindfulness, depression and modes of mind. Cognitive Therapy and Research,\n32(6):721.\nWilliams, P. (2008b). Mahayana Buddhism: The Doctrinal Foundations. Routledge.\nWittkuhn, L., Chien, S., Hall-McMaster, S., and Schuck, N. W. (2021). Replay in minds and machines. Neuro-\n\nBIBLIOGRAPHY\n257\nscience & Biobehavioral Reviews, 129:367–388.\nWolpert, D. M., Miall, R. C., and Kawato, M. (1998). Internal models in the cerebellum. Trends in Cognitive\nSciences, 2(9):338–347.\nWong, R. O. (1999). Retinal waves and visual system development. Annual Review of Neuroscience, 22(1):29–47.\nWood, A. M., Froh, J. J., and Geraghty, A. W. (2010). Gratitude and well-being: A review and theoretical integra-\ntion. Clinical psychology review, 30(7):890–905.\nWright, A. (2011). A criticism of the IASP’s definition of pain. Journal of Consciousness Studies, 18(9-10):19–44.\nWright, R. (1994). The Moral Animal. Vintage Books.\nWright, R. (2017). Why Buddhism is true: The science and philosophy of meditation and enlightenment. Simon\nand Schuster.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K.,\net al. (2016). Google’s neural machine translation system: Bridging the gap between human and machine\ntranslation. arXiv preprint arXiv:1609.08144.\nWu, Y., Zhai, S., Srivastava, N., Susskind, J., Zhang, J., Salakhutdinov, R., and Goh, H. (2021). Uncertainty\nweighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140.\nXie, J., Lu, Y., Zhu, S.-C., and Wu, Y. (2016). A theory of generative convnet. In International conference on\nmachine learning, pages 2635–2644. PMLR.\nXu, D., Li, T., Li, Y., Su, X., Tarkoma, S., Jiang, T., Crowcroft, J., and Hui, P. (2020). Edge intelligence: Architectures,\nchallenges, and applications. arXiv preprint, arXiv:2003.12172.\nYang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. (2023). Diffusion\nmodels: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1–39.\nYehuda, R. (2002). Post-traumatic stress disorder. New England Journal of Medicine, 346(2):108–114.\nYi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. (2018). Neural-symbolic VQA: Disentangling\nreasoning from vision and language understanding. In Advances in Neural Information Processing Systems,\npages 1031–1042.\nYoshida, W., Seymour, B., Koltzenburg, M., and Dolan, R. J. (2013).\nUncertainty increases pain: evidence\nfor a novel mechanism of pain modulation involving the periaqueductal gray.\nJournal of Neuroscience,\n33(13):5638–5646.\nZenke, F., Gerstner, W., and Ganguli, S. (2017). The temporal paradox of Hebbian learning and homeostatic\nplasticity. Current opinion in neurobiology, 43:166–176.\nZhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. (2020). Variational policy gradient method for\nreinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572–\n4583.\nZhang, K. and Sejnowski, T. J. (2000). A universal scaling law between gray matter and white matter of cerebral\ncortex. Proceedings of the National Academy of Sciences, 97(10):5621–5626.\nZhang, R., Brennan, T. J., and Lo, A. W. (2014). The origin of risk aversion. Proceedings of the National Academy\nof Sciences, 111(50):17777–17782.\nZhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., and Torralba, A. (2019). Semantic understanding of\nscenes through the ADE20K dataset. International Journal of Computer Vision, 127(3):302–321.\nZhuang, C., Yan, S., Nayebi, A., Schrimpf, M., Frank, M. C., DiCarlo, J. J., and Yamins, D. L. (2021). Unsupervised\nneural network models of the ventral visual stream. Proceedings of the National Academy of Sciences, 118(3).\n\nBIBLIOGRAPHY\n258\nZiemke, T. (2007). The embodied self: Theories, hunches and robot models. Journal of Consciousness Studies,\n14(7):167–179.\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C. (2008). Regret minimization in games with incom-\nplete information. In Advances in Neural Information Processing Systems, pages 1729–1736.\nZinkevich, M., Weimer, M., Li, L., and Smola, A. J. (2010). Parallelized stochastic gradient descent. In Advances\nin Neural Information Processing Systems, pages 2595–2603.\nZubarev, I. and Parkkonen, L. (2018). Evidence for a general performance-monitoring system in the human\nbrain. Human Brain Mapping, 39(11):4322–4333.",
    "pdf_filename": "Painful intelligence - What AI can tell us about human suffering.pdf"
}