{
    "title": "Adversarial Machine Learning and Cybersecurity - Risks, Challenges, and Legal Implications",
    "context": "",
    "body": "Adversarial Machine \nLearning and Cybersecurity: \nRisks, Challenges, and Legal Implications\nAuthors:\t\t\n\t\nMicah Musser*\nJonathan Spring*\nChristina Liaghati\nDaniel Rohrer\nJonathan Elliott\nRumman Chowdhury\n*Workshop Organizer\n  \t\n\t\n\t\nAndrew Lohn*\nRam Shankar Siva Kumar*\nCindy Martinez\nHeather Frase\nMikel Rodriguez\nStefan Hermanek\n  \t\n\t\n\t\nJames X. Dempsey*\nBrenda Leong\nCrystal D. Grant \nJohn Bansemer\nMitt Regan\nApril 2023\n\n \nCenter for Security and Emerging Technology | 1 \nAuthors \nMicah Musser is a research analyst with the CyberAI Project at CSET, where Andrew \nLohn is a senior fellow. James X. Dempsey is senior policy advisor for the Program on \nGeopolitics, Technology, and Governance, Stanford Cyber Policy Center, and lecturer \nat the UC Berkeley School of Law. Jonathan Spring is a cybersecurity specialist at the \nCybersecurity and Infrastructure Security Agency (CISA), and was at the time of the \nJuly 2022 workshop an analyst at the CERT Division of the Software Engineering \nInstitute at Carnegie Mellon University. Ram Shankar Siva Kumar is a data cowboy at \nMicrosoft Security Research and tech policy fellow at the CITRIS Policy Lab and the \nGoldman School of Public Policy at UC Berkeley.   \nBrenda Leong is a partner at BNH.ai, a boutique law firm focused on the legal issues \nsurrounding AI. Christina Liaghati is AI strategy execution and operations manager for \nthe AI and Autonomy Innovation Center at the MITRE Corporation. Cindy Martinez is a \npolicy analyst focusing on AI governance and regulation. Crystal D. Grant is a data \nscientist and geneticist who studies the relationship between emerging technologies \nand civil liberties. Daniel Rohrer is vice president of software product security, focused \non advancing architecture and research at NVIDIA. Heather Frase and John Bansemer \nboth work at CSET, where Heather is a senior fellow leading the AI standards and \ntesting line of research, and John is the director of the CyberAI Project. Jonathan Elliott \nis chief of test and evaluation in the Test and Evaluation Division of the Chief Digital \nand Artificial Intelligence Office. Mikel Rodriguez works on securing AI-enabled \nsystems at DeepMind, and was formerly the director of the AI and Autonomy \nInnovation Center at the MITRE Corporation at the time of the July 2022 workshop. \nMitt Regan is McDevitt professor of jurisprudence and co-director of the Center on \nNational Security at Georgetown University Law Center. Rumman Chowdhury is the \nfounder of Parity Consulting, an ethical AI consulting group, and was formerly the \ndirector of Machine Learning, Ethics, Transparency, and Accountability at Twitter at the \ntime of the July 2022 workshop. Stefan Hermanek is a product manager with \nexpertise and experience in AI safety and robustness, and AI red-teaming.  \n \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 2 \n \nViews expressed in this document do not necessarily represent the views of the \nU.S. government or any institution, organization, or entity with which the authors \nmay be affiliated.  \nReference to any specific commercial product, process, or service by trade name, \ntrademark, manufacturer, or otherwise, does not constitute or imply an \nendorsement, recommendation, or favoring by the U.S. government, including the \nU.S. Department of Defense, the Cybersecurity and Infrastructure Security Agency, \nor any other institution, organization, or entity with which the authors may be \naffiliated. \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 3 \nExecutive Summary \nIn July 2022, the Center for Security and Emerging Technology (CSET) at Georgetown \nUniversity and the Program on Geopolitics, Technology, and Governance at the \nStanford Cyber Policy Center convened a workshop of experts to examine the \nrelationship between vulnerabilities in artificial intelligence systems and more \ntraditional types of software vulnerabilities. Topics discussed included the extent to \nwhich AI vulnerabilities can be handled under standard cybersecurity processes, the \nbarriers currently preventing the accurate sharing of information about AI \nvulnerabilities, legal issues associated with adversarial attacks on AI systems, and \npotential areas where government support could improve AI vulnerability \nmanagement and mitigation. \nAttendees at the workshop included industry representatives in both cybersecurity and \nAI red-teaming roles; academics with experience conducting adversarial machine \nlearning research; legal specialists in cybersecurity regulation, AI liability, and \ncomputer-related criminal law; and government representatives with significant AI \noversight responsibilities.  \nThis report is meant to accomplish two things. First, it provides a high-level discussion \nof AI vulnerabilities, including the ways in which they are disanalogous to other types \nof vulnerabilities, and the current state of affairs regarding information sharing and \nlegal oversight of AI vulnerabilities. Second, it attempts to articulate broad \nrecommendations as endorsed by the majority of participants at the workshop. These \nrecommendations, categorized under four high-level topics, are as follows:  \n1. Topic: Extending Traditional Cybersecurity for AI Vulnerabilities \n1.1. Recommendation: Organizations building or deploying AI models should \nuse a risk management framework that addresses security throughout \nthe AI system life cycle. \n1.2. Recommendation: Adversarial machine learning researchers, \ncybersecurity practitioners, and AI organizations should actively \nexperiment with extending existing cybersecurity processes to cover AI \nvulnerabilities. \n1.3. Recommendation: Researchers and practitioners in the field of \nadversarial machine learning should consult with those addressing AI \nbias and robustness, as well as other communities with relevant \nexpertise. \n\n \nCenter for Security and Emerging Technology | 4 \n2. Topic: Improving Information Sharing and Organizational Security Mindsets \n2.1. Recommendation: Organizations that deploy AI systems should pursue \ninformation sharing arrangements to foster an understanding of the \nthreat. \n2.2. Recommendation: AI deployers should emphasize building a culture of \nsecurity that is embedded in AI development at every stage of the \nproduct life cycle. \n2.3. Recommendation: Developers and deployers of high-risk AI systems \nmust prioritize transparency. \n3. Topic: Clarifying the Legal Status of AI Vulnerabilities \n3.1. Recommendation: U.S. government agencies with authority over \ncybersecurity should clarify how AI-based security concerns fit into their \nregulatory structure. \n3.2. Recommendation: There is no need at this time to amend anti-hacking \nlaws to specifically address attacking AI systems. \n4. Topic: Supporting Effective Research to Improve AI Security \n4.1. Recommendation: Adversarial machine learning researchers and \ncybersecurity practitioners should seek to collaborate more closely than \nthey have in the past. \n4.2. Recommendation: Public efforts to promote AI research should more \nheavily emphasize AI security, including through funding open-source \ntooling that can promote more secure AI development. \n4.3. Recommendation: Government policymakers should move beyond \nstandards-writing toward providing test beds or enabling audits for \nassessing the security of AI models. \n \n \n \n\n \nCenter for Security and Emerging Technology | 5 \nTable of Contents \nAuthors ....................................................................................................................................................... 1 \nExecutive Summary ................................................................................................................................ 3 \nIntroduction ............................................................................................................................................... 6 \nExtending Traditional Cybersecurity for AI Vulnerabilities .................................................... 10 \nRecommendations ........................................................................................................................... 12 \nImproving Information Sharing and Organizational Security Mindsets .............................. 15 \nRecommendations ........................................................................................................................... 15 \nClarifying the Legal Status of AI Vulnerabilities........................................................................ 18 \nRecommendations ........................................................................................................................... 20 \nSupporting Effective Research to Improve AI Security ............................................................ 22 \nRecommendations ........................................................................................................................... 22 \nAcknowledgments............................................................................................................................... 25 \nEndnotes ................................................................................................................................................. 26 \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 6 \nIntroduction \nArtificial intelligence (AI) technologies, especially machine learning, are rapidly being \ndeployed in a wide range of commercial and governmental contexts. These \ntechnologies are vulnerable to an extensive set of manipulations that can trigger \nerrors, infer private data from training datasets, degrade performance, or disclose \nmodel parameters.1 Researchers have demonstrated major vulnerabilities in numerous \nAI models, including many that have been deployed in public-facing contexts.2 As \nAndrew Moore testified before the U.S. Senate Committee on Armed Services in May \n2022, defending AI systems from adversarial attacks is “absolutely the place where \nthe battle’s being fought at the moment.”3  \nHowever, AI vulnerabilities may not map straightforwardly onto the traditional \ndefinition of a patch-to-fix cybersecurity vulnerability (see the section below on \n“Extending Traditional Cybersecurity for AI Vulnerabilities”). The differences between \nAI vulnerabilities and more standard patch-to-fix vulnerabilities have generated \nambiguity regarding the status of AI vulnerabilities and AI attacks. This in turn poses a \nseries of corporate responsibility and public policy questions: Can AI vulnerabilities be \naddressed using traditional methods of cyber risk remediation or mitigation? Are the \ncompanies developing and using machine learning products equipped to adequately \ndefend them? What legal liability exists for the developers of AI systems, or for the \nattackers who undermine them? How can policymakers support the creation of a more \nsecure AI ecosystem?  \nIn July 2022, the Center for Security and Emerging Technology (CSET) at Georgetown \nUniversity and the Program on Geopolitics, Technology, and Governance at the \nStanford Cyber Policy Center convened a workshop of experts to address these \nquestions. Attendees included industry representatives in both cybersecurity and AI \nred-teaming roles; academics with experience conducting adversarial machine learning \nresearch; legal specialists in cybersecurity regulation, AI liability, and computer-related \ncriminal law; and government representatives with significant AI oversight \nresponsibilities. This report summarizes the main takeaways of the workshop, with \nrecommendations for researchers, industry professionals, and policymakers. \n\n \nCenter for Security and Emerging Technology | 7 \nBox 1: Explanation of Key Terms \nMany of the terms used throughout this report can have multiple meanings. For the \nsake of clarifying our discussion, we use the following terms as described here:  \nArtificial intelligence (AI): a set of technologies that enable computers to learn to \nperform tasks traditionally performed by humans. This report uses AI \ninterchangeably with machine learning. There are other approaches to AI research \nbeyond machine learning, but this report focuses on vulnerabilities to machine \nlearning-based models. An important subset of current AI approaches is deep \nlearning, and the field of adversarial machine learning focuses largely on attacking \nand defending deep learning-based models.  \nAI system: a system which includes an AI model as a key component. This \ndefinition includes all components of the overall system—including preprocessing \nsoftware, physical sensors, logical rules, and hardware devices—and can be \ncontrasted with the term “AI model,” which we use to refer only to the parameters \nof the mathematical model produced by an AI training process.  \nVulnerability: this report adopts the definition of a vulnerability provided by the \nCERT Guide to Coordinated Vulnerability Disclosure as “a set of conditions or \nbehaviors that allows the violation of an explicit or implicit security policy. \nVulnerabilities can be caused by software defects, configuration or design \ndecisions, unexpected interactions between systems, or environmental changes.”4 \nAI vulnerability: a vulnerability in an AI system, including both vulnerabilities that \nexploit the mathematical features of AI models, as well as vulnerabilities that arise \nfrom the interaction of an AI model with other components of an overall AI system.  \nTraditional software vulnerability: a term used in this report to refer to \nvulnerabilities in operating systems, workstation applications, server software, and \nmobile applications with which much of the modern vulnerability management \ncommunity is most familiar. Example community efforts for traditional software \n\n \nCenter for Security and Emerging Technology | 8 \nvulnerabilities include CVSSv2 and CVSSv3 and the CVE program.* However, there \nare many other types of broader cybersecurity vulnerabilities—including \nvulnerabilities in open-source software, hardware devices, industrial control \nsystems, blockchain protocols, and so on—that may or may not differ from AI \nvulnerabilities in the ways that this report discusses.  \nHigh-risk AI system: an AI system that is intended to automate or influence a \nsocially sensitive decision, including those affecting access to housing, credit, \nemployment, healthcare, and so forth. Vulnerabilities—as well as other negative \nproperties, such as bias, unfairness, or discriminatory behavior—are particularly \nworrisome in these types of systems, as system failures may cause severe harm to \nindividuals. Where considerations affecting the design and deployment of high-risk \nAI systems are discussed in this report, they should be regarded as minimum \nrequirements for high-risk systems and do not imply endorsement of the use of AI \nto automate high-risk decision-making in general. \nAlthough the repertoire of attacks studied by adversarial machine learning researchers \nis expanding, many of these attacks are still focused on lab settings, and a holistic \nunderstanding of vulnerabilities in deployed systems is lacking. Due in part to these \nuncertainties, participants generally shied away from proposing sweeping legal or \nregulatory changes regarding AI \nvulnerabilities. At the same time, \nworkshop participants agreed that the \nrisk of attacks on AI systems is likely \nto grow over time, and that it is \nimportant to begin developing \nmechanisms for addressing AI \nvulnerabilities now. The fact that \ncomputer- and software-based \nprocesses already harbor many \n \n* The Common Vulnerability Scoring System provides a set of properties of a cybersecurity vulnerability \nthat can be used to triage a vulnerability, and a suggested system for ranking vulnerabilities based on \nthose properties. See FIRST, “Common Vulnerability Scoring System SIG,” accessed January 30, 2023, \nhttps://www.first.org/cvss/. The Common Vulnerabilities and Exposures program aims to provide unique \nidentifiers for all publicly disclosed vulnerabilities. See CVE, https://www.cve.org/, accessed January 29, \n2023.  \nWorkshop participants agreed \nthat the risk of attacks on AI \nsystems is likely to grow over \ntime, and that it is important to \nbegin developing mechanisms for \naddressing AI vulnerabilities now. \n\n \nCenter for Security and Emerging Technology | 9 \nreadily exploited vulnerabilities not related to AI should not distract AI developers, AI \nusers, and policymakers from acting now to address AI vulnerabilities. \nThis report summarizes the general consensus of the workshop related to adversarial \nmachine learning and offers recommendations to improve future responses. These \nrecommendations are divided into four parts:  \n1. Recommendations regarding the degree to which AI vulnerabilities can be \nhandled under existing cybersecurity processes. \n2. Recommendations regarding shifts in organizational culture and information \nsharing for organizations and individuals actively involved in building AI models, \nintegrating models in business products, and using AI systems. \n3. Recommendations regarding the legal issues surrounding AI vulnerabilities. \n4. Recommendations regarding future areas of research and government support \nthat can lead to the development of more secure AI systems. \nAt a high level, we emphasize that—although adversarial machine learning is a \ncomplex field with highly technical tools—the problems posed by AI vulnerabilities \nmay be as much social as they are technological. Some of our recommendations \nemphasize opportunities for industry and government policymakers to promote AI \nsecurity by investing in technical research. However, the majority of our \nrecommendations focus on changes to processes, institutional culture, and awareness \namong AI developers and users. While we hope that these recommendations will \nprompt organizations using AI to proactively think about the security issues \nsurrounding AI systems, we also underscore that individual authors maintain a wide \nrange of viewpoints and do not necessarily endorse each particular recommendation in \nisolation.  \n \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 10 \n1. Extending Traditional Cybersecurity for AI Vulnerabilities \nIn many senses, attacks on AI systems are not new. Malicious actors have been \nattempting to evade algorithm-based spam filters or manipulate recommender \nalgorithms for decades. But machine learning models have risen sharply in prevalence \nover the last decade—including in an increasing number of high-risk contexts.* At the \nsame time, researchers have repeatedly demonstrated that vulnerabilities in machine \nlearning algorithms and training processes are pervasive and challenging to \nremediate.† Machine learning-based image and voice recognition systems have been \nfooled with perturbations imperceptible to humans, datasets have been poisoned in \nways that pervert system outputs or render them unreliable, and sensitive data meant \nto remain private has been reconstructed.5 \nTo date, these attacks have mostly occurred in research settings, though there is some \nevidence of real-world hackers exploiting vulnerabilities in deep learning systems.6 \nMoreover, there is a shared expectation that, with the continued incorporation of AI \nmodels into a wider range of use cases, the frequency of deep learning-based attacks \nwill grow. Workshop participants suspected that these attacks are likely to be most \ncommon wherever there are either clear financial benefits to defeating a machine \nlearning model that would motivate private hackers or strategic advantages to doing \nso that would motivate a nation-state.  \nWe face a challenge in responding to AI vulnerabilities. On the one hand, existing \ncybersecurity frameworks are meant to be general enough to cover emerging classes \n \n* We note this trend to underscore that all AI systems are likely to carry certain types of AI \nvulnerabilities that can become particularly worrisome in high-risk contexts. This observation of a trend \nis not an endorsement of the use of AI in such contexts. Most workshop participants expressed serious \nreservations about the use of AI in at least some high-risk contexts, but in this report we do not and \ncannot offer a general rule for evaluating the ethical concerns associated with these use cases.  \n† The field of adversarial machine learning focuses primarily on attacks on deep learning systems. Deep \nlearning is currently the predominant focus of machine learning research, but it is important to \nemphasize that attacks on deep learning-based models are in some ways elaborations of attacks on \nother, more traditional machine learning methods. We do not view the category of “AI vulnerabilities” in \ndeep learning models as different in kind from other types of vulnerabilities, including vulnerabilities in \nolder machine learning models; however, the increasing social impact of deep learning models and the \nlevel of research attention paid to them warrants some specific focus on these vulnerabilities. In the \nremainder of this report, we use “deep learning” to distinguish a subset of relatively newer models (and \ncorresponding attacks) from the full range of machine learning models and their vulnerabilities.  \n\n \nCenter for Security and Emerging Technology | 11 \nof vulnerabilities such as those generated by deep learning methods. Indeed, it is \npossible to analyze the risks of AI exploitation under standard risk or vulnerability \nmanagement frameworks.7 On the other hand, AI vulnerabilities are distinct from \ntraditional software vulnerabilities in some important respects and may require \nextensions of, or adjustments to, existing cybersecurity risk governance frameworks. At \na high level of abstraction, AI and traditional software vulnerabilities differ in the \nfollowing ways: \n1. AI vulnerabilities generally derive from a complex interaction between training \ndata and the training algorithm. This makes the existence of certain types of \nvulnerabilities highly dependent on the particular dataset(s) that may be used to \ntrain an AI model, often in ways that are difficult to predict or mitigate prior to \nfully training the model itself. This feature also makes it difficult to test the full \nspace of potential user inputs in order to understand how a system may respond \nto those inputs.8  \n2. “Patching” a vulnerability in an AI model may require retraining it, potentially at \nconsiderable expense, or it may not even be feasible at all.9 Model retraining to \nreduce security vulnerabilities may also degrade overall performance on non-\nmalicious system inputs. \n3. In many contexts, vulnerabilities in AI systems may be highly ephemeral, as in \norganizations using continuous training pipelines where models are frequently \nupdated with new data. In other contexts, vulnerabilities may be highly-context \ndependent, as for instance in organizations that deploy locally fine-tuned \nversions of a central model across many devices. In either situation, attacks—as \nwell as mitigations—may not transfer well across all versions of the model.  \n4. There is often deep uncertainty regarding what “counts” as a vulnerability in an \nAI system. For example, adversarial examples are inputs to an AI system that \nhave been perturbed in some way in order to deliberately degrade the system’s \nperformance. But it is hard to distinguish between worrisome “attacks” and \nneutral—or even expected—user manipulations, like wearing sunglasses to \nmake it harder for facial recognition systems to recognize someone. While this \nproblem is not necessarily unique to AI, it does complicate the matter of defining \nindividual AI vulnerabilities.10  \nThese differences are likely to change how vulnerabilities in AI systems are handled. \nFor instance, if fully “patching” a vulnerability is impossible, AI developers and \ndeployers may be more likely to leave systems with known vulnerabilities online.11 \n\n \nCenter for Security and Emerging Technology | 12 \nResponses will likely focus relatively more on risk mitigation and relatively less on risk \nremediation, in which the underlying vulnerability is fully removed.  \nRecommendations \n1.1. Organizations building or deploying AI models should use a risk management \nframework that addresses security throughout the AI system life cycle. Risk \nmanagement frameworks are a key element of any organization’s cybersecurity \npolicy,12 and we encourage their use for managing AI security. As with other types of \nrisk management frameworks, it is important for organizations to incorporate them \nthroughout the product development pipeline. AI vulnerabilities will, however, present \nsome unique considerations for risk management frameworks. For instance, if \nvulnerabilities in machine learning models cannot be as easily patched as many \ntraditional software vulnerabilities, then organizations may opt to mitigate \nvulnerabilities rather than fix or decommission the model, especially when AI models \nare a part of a complex system where the removal of one component may result in \nhard-to-predict changes to the overall system.*  \nImportant questions in using risk management frameworks include: How can we \nassure that this model is reliable and robust? In what contexts is it safe to deploy AI \nmodels? What compensating controls are available? How should organizations \nstructure the process of deciding between taking a vulnerable system or feature offline \nentirely or leaving it in place with mitigations? Are important trade-offs at stake, such \nas trade-offs between applying defensive measures to a model and ensuring that \noverall performance remains high? What decisions might be in the best interests of the \nend-users of a machine learning product, as well as in the best interests of the \nindividuals who will ultimately be affected by it, and is there a way to involve those \ngroups throughout development of the system? In the case of high-risk AI systems, \norganizations should consider what decisions regarding trade-offs most benefit \n \n* Note that the National Institute of Standards and Technology has, as of 2023, released draft 1.0 of its \nArtificial Intelligence Risk Management Framework. See National Institute of Standards and Technology, \n“NIST AI 100-1: Artificial Intelligence Risk Management Framework (AI RMF 1.0),” January 23, 2023, \nhttps://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf. This document is “intended for voluntary use and \nto improve the ability to incorporate trustworthiness considerations into the design, development, use, \nand evaluation of AI products, services, and systems.” We view this document as an important step \ntoward effectively augmenting existing risk management practices to account for the risks posed by AI \nsystems.  \n\n \nCenter for Security and Emerging Technology | 13 \nunderrepresented groups, but they should also carefully evaluate whether those \ngroups are harmed by the use of an AI system in the first place.  \n1.2. Adversarial machine learning researchers, cybersecurity practitioners, and AI \norganizations should actively experiment with extending existing cybersecurity \nprocesses to cover AI vulnerabilities. The cybersecurity community has developed \nmany tools for tracking and mitigating vulnerabilities and for guiding incident response. \nOn the vulnerability management side, these include the Common Vulnerabilities and \nExposures (CVE) system for enumerating known vulnerabilities, the Common \nVulnerability Scoring System (CVSS) for evaluating the potential risk associated with \nknown vulnerabilities, and the Coordinated Vulnerability Disclosure (CVD) process for \ncoordinating between security researchers and software vendors.13 Although these \nprocesses were not designed with AI systems in mind, there was agreement at the \nworkshop that these mechanisms are likely broad enough for managing many types of \nAI vulnerabilities. However, more collaboration between cybersecurity practitioners, \nmachine learning engineers, and adversarial machine learning researchers is necessary \nto appropriately apply them to AI vulnerabilities.14 Assessing AI vulnerabilities requires \ntechnical expertise that is distinct from the skill set of cybersecurity practitioners, and \norganizations should be cautioned against repurposing existing security teams without \nadditional training and resources.  \nThe differences between AI vulnerabilities \nand traditional software vulnerabilities \nmight make the use of these processes \nmore complicated. At the same time, \nworkshop participants did not feel that the \ndifferences are sufficiently strong to justify \ncreating a separate set of processes to \nhandle AI vulnerabilities.15 We therefore \nencourage researchers and organizations to \nincorporate AI vulnerabilities into \nestablished risk management practices.  \n1.3. Researchers and practitioners in the \nfield of adversarial machine learning should consult with those addressing AI bias \nand robustness, as well as other communities with relevant expertise. Multiple \nworkshop participants noted that in some important respects, AI vulnerabilities may be \nmore analogous to other topics such as algorithmic bias than they are to traditional \nsoftware vulnerabilities. AI fairness researchers have extensively studied how poor \ndata, design choices, and risk decisions have led to model failures that cause real-\nMore collaboration between \ncybersecurity practitioners, \nmachine learning engineers, \nand adversarial machine \nlearning researchers is \nnecessary to appropriately \napply [existing cybersecurity \nprocesses] to AI vulnerabilities. \n\n \nCenter for Security and Emerging Technology | 14 \nworld harm; the AI security community should seek to better understand these lessons \nin developing their own frameworks for evaluating risk and assessing the assurance of \nAI for use in consequential applications. In general, workshop participants believed \nthat it is important to cultivate greater engagement among adversarial machine \nlearning researchers, the AI bias field, cybersecurity practitioners, other relevant expert \ngroups, and affected communities.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 15 \n2. Improving Information Sharing and Organizational Security Mindsets \nSeveral structural features make it difficult to assess precisely how large the threat of \nattacks on AI systems is. For one, most information about existing AI vulnerabilities has \ncome from theoretical or academic research settings, from cybersecurity companies, or \nfrom internal researchers red-teaming their organizations’ AI systems.16 Second, the \nabsence of a systematic and standardized means for tracking AI assets (such as \ndatasets and models) and their corresponding vulnerabilities makes it difficult to know \nhow widespread vulnerable systems are.17 And third, for some types of attacks on AI \nsystems, attack detection may require meaningful machine learning or data science \nexpertise to implement, or at least a familiarity with the patterns of behavior that may \nsignal an AI-based attack. Since many cybersecurity teams may not have all the \nrelevant expertise to detect such attacks, organizations may lack the capability—and \nperhaps the incentive—to identify and disclose AI attacks that do occur.18 \nEven if vulnerabilities are identified or malicious attacks are observed, this information \nis rarely transmitted to others, whether peer organizations, other companies in the \nsupply chain, end users, or government or civil society observers. Although some \npotential mechanisms for disseminating information exist,19 a specialized, trusted \nforum for incident information sharing on a protected basis is lacking. Several \nworkshop participants from industry and government organizations noted that they \nwould benefit from regular exchanges of information, but that networks for \ninformation sharing do not currently exist, and that bureaucratic, policy, and cultural \nbarriers currently inhibit such sharing. \nThese conditions mean that, under current arrangements, the problem will likely \nremain mostly unnoticed until long after attackers have successfully exploited \nvulnerabilities. In order to avoid this outcome, we recommend that organizations \ndeveloping AI models take significant steps to formalize or make use of information \nsharing arrangements, to monitor for potential attacks on AI systems, and to foster \ntransparency.  \nRecommendations \n2.1. Organizations that deploy AI systems should pursue information sharing \narrangements to foster an understanding of the threat. Currently, there are few \ntrusted mechanisms for organizations that have observed potential attacks on their AI \nsystems to share that information with others, making it difficult for anyone to \nunderstand the scope or nature of the problem. Existing attempts to share information \nabout the risks of AI systems—such as the Artificial Intelligence Incident Database—\n\n \nCenter for Security and Emerging Technology | 16 \nrely on public reporting and primarily focus on machine learning failures or misuse \nrather than intentional manipulation.20 While these efforts are important, they are best \nsituated for compiling publicly known AI failures, rather than for incentivizing \norganizations to share information about emerging security threats in a more trusted \nenvironment. Mechanisms to encourage more open sharing of information could take a \nwide range of forms, extending from informal but regular meetings of key industry \nstakeholders to more formalized structures, repositories, or organizations.21 \n2.2. AI deployers should emphasize building a culture of security that is embedded \nin AI development at every stage of the product life cycle. Many machine learning \nlibraries provide functions that, by default, prioritize processing speed and minor \nimprovements in performance over security.22 Product teams who only consider \nsecurity concerns after building their \nmodels will likely embed insecurities in the \ndevelopment pipeline for their models, \nwhich may be difficult or impossible to \nremove once models are fully trained.* As \nwith all software, organizations should \nmake security a priority in every part of the \nAI pipeline. This entails providing robust \nsupport for adversarial machine learning \nteams, as well as incorporating those \nteams in every stage of product \ndevelopment to avoid the problem of \n“outsourcing” security concerns to a \nseparate team.  \n2.3. Developers and deployers of high-risk AI systems must prioritize transparency. \nAI models should be assumed to come with inherent vulnerabilities—not to mention \nother types of failure modes inherent to all statistical models—that are difficult if not \n \n* As one example, most machine learning libraries provide default image processing functions that are \nsusceptible to adversarial evasion. While alternative preprocessing functions can easily be used, most \nlibraries make use of the less secure methods by default, and models trained on images that have been \npreprocessed in one way cannot easily transfer to images preprocessed in a different way without \nsubstantial retraining. See Lohn, “Downscaling Attack and Defense.” Other security-relevant decisions \nthat are difficult to alter after model training include decisions that affect the security and integrity of \ntraining data, the use of other types of input filtering, the choice of upstream models for use in fine-\ntuning, and so on.  \nProduct teams who only \nconsider security concerns \nafter building their models \nwill likely embed insecurities \ninto the development pipeline \nfor their models, which may \nbe difficult or impossible to \nremove once models are fully \ntrained. \n\n \nCenter for Security and Emerging Technology | 17 \nimpossible to patch. The presumed existence of some of these vulnerabilities in high-\nrisk contexts has significant implications for social well-being and privacy. Given this \nfact, workshop participants felt that the security features of machine learning models \nalso carry transparency implications. A minimum standard of transparency along these \nlines might hold that consumers and private citizens should generally be informed \nwhen they are being made subject to an AI model in a high-risk context. In addition, \nwhere the designers of a model make important decisions about relevant trade-offs—\nsuch as those that may exist between security, performance, robustness, or fairness—\nmany participants felt that such decisions should be disclosed to protect end-users or \nprivate citizens affected by a model’s decisions, as well as to help enable recourse \nwhen decisions are harmful or discriminatory.23 While participants disagreed about just \nhow far this principle of transparency should be taken—with many emphasizing that it \nshould not extend to simply disclosing the existence of every vulnerability to the \npublic—the more minimal transparency standards discussed in this section were \nwidely supported. \n \n \n \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 18 \n3. Clarifying the Legal Status of AI Vulnerabilities \nThere is no comprehensive AI legislation in the United States (and not likely to be one \nanytime soon).24 However, many areas of law—including criminal law, consumer \nprotection statutes, privacy law, civil rights law, government procurement \nrequirements, common law rules of contract, negligence and product liability, and even \nrules of the U.S. Securities and Exchange Commission regarding the disclosure \nobligations of publicly-owned companies—are relevant to different aspects of AI. Just \nas AI fits, albeit uneasily, within traditional cybersecurity risk frameworks, so also is it \ncovered under existing law, but in ways \nand to a degree that courts and regulators \nhave not yet fully clarified. Much of the \npolicy attention on AI to date has focused \non concerns with regard to bias and \ndiscrimination. The Federal Trade \nCommission (FTC), U.S. Equal Employment \nOpportunity Commission, and Consumer \nFinancial Protection Bureau (among \nothers) have all issued their own guidance \nregarding the use of AI models in contexts \nthat might violate federal civil rights laws, \nas well as anti-discrimination and \nconsumer protection laws.25  \nAt the state level, the New York State Department of Financial Services has warned \ninsurers that the data they use and the algorithms as well as the predictive models \nthey apply may produce forms of discrimination prohibited by state law.26 And in \nCalifornia, privacy legislation requires the relatively new California Privacy Protection \nAgency to adopt regulations “governing access and opt-out rights with respect to \nbusinesses’ use of automated decisionmaking technology, including profiling and \nrequiring businesses’ response to access requests to include meaningful information \nabout the logic involved in those decisionmaking processes.”27 \nIn keeping with our view that AI vulnerabilities should be handled under existing \ncybersecurity processes as far as possible, we suggest that AI vulnerabilities are likely \nbest handled by extending and adapting cybersecurity law, not by trying to regulate AI \nsecurity as an independent topic. Unfortunately, cybersecurity law itself is still evolving \nand many questions are unsettled or contingent. Requirements vary sector-by-sector, \nwith overlapping federal and state rules. Under the resulting patchwork, protected \nJust as AI fits, albeit uneasily, \nwithin traditional cybersecurity \nrisk frameworks, so also is it \ncovered under existing law, but \nin ways and to a degree that \ncourts and regulators have not \nyet fully clarified. \n \n\n \nCenter for Security and Emerging Technology | 19 \nhealthcare data, financial information, software and information systems acquired by \nthe federal government, and critical infrastructure, among other categories of systems \nor data, face certain cybersecurity requirements imposed by statute or executive \norder.28 But there is no comprehensive cybersecurity law that imposes clear statutory \nobligations on the vast majority of companies. While common law doctrines of \nnegligence, product liability, and contract do apply to AI-based products and systems, \nlegal principles in those fields (including questions about the existence of a duty of \ncare, the economic loss rule in negligence cases, and disclaimers of warranty) mean \nthat almost no cases ever yield a clear ruling on liability for security failings. In federal \ncourts, the standing doctrine further makes it difficult to reach the merits of a claim. \nThe near total absence of cybersecurity cases decided on the merits has stunted the \ndevelopment of clear standards with respect to traditional types of vulnerabilities, let \nalone those associated with AI.  \nAt the same time, the FTC has claimed that its authority to regulate unfair and \ndeceptive business practices extends to cover businesses that fail to secure customer \ninformation with “reasonable” cybersecurity measures.29 The FTC has brought \nnumerous cases against companies that have failed to secure consumer data. While no \ncase to date directly makes this claim, it is easy to imagine that deploying vulnerable \nAI systems might trigger similar forms of regulatory oversight, especially where those \nvulnerabilities stem from features common to all machine learning models that \ncompanies can reasonably anticipate. In addition, AI companies that make claims about \nthe robustness and performance of their models could be charged with deceptive \npractices if those models contain foreseeable vulnerabilities that undermine the \ncompany’s claims. Through its workshops and nonbinding statements, the FTC has \nmade it clear that it is concerned about the impact of AI. \nAlthough the FTC has brought scores of enforcement actions against companies for \nfailure to protect consumer data, uncertainty hangs over the FTC’s authority, especially \nits assertion that failure to provide reasonable security falls under the unfairness prong \nof its unfair and deceptive practices jurisdiction.30 Federal regulatory oversight of AI \nvulnerabilities would, barring legislative action, similarly need to begin from an \nambiguous status of authority.  \nWhen it comes to deterring attacks on AI systems, an important law is the federal \nComputer Fraud and Abuse Act.31 The CFAA makes it illegal to access information \nfrom a computer without authorization (or beyond authorized access), as well as to \n“damage” a computer by knowingly transmitting a “program, information, code, or \ncommand.”32 It has been controversial, especially regarding whether the law applies to \nthe activities of good faith cybersecurity researchers probing systems for \n\n \nCenter for Security and Emerging Technology | 20 \nvulnerabilities. Those legal risks can be, and for many entities have been, mitigated by \nvulnerability disclosure programs that authorize, or even invite, independent security \nresearchers to probe a system or product. In terms of the CFAA’s application to AI, \nmany of the its provisions turn on whether an attacker has first gained “unauthorized \naccess” or exceeded authorized access to a protected computer, a step that may not be \nrequired for many adversarial AI attacks. However, one section of the CFAA makes it \nillegal to cause damage without authorization to a protected computer, where \ndamages is broadly defined to mean any impairment to the integrity or availability of \ndata, a program, a system, or information. \nOur recommendations regarding the legal status of AI vulnerabilities begin from the \nposition that this is a rapidly evolving topic within multiple fields of the law. Workshop \nparticipants did not feel that it was \nappropriate at this time to call for \ncomprehensive legislation to \naddress liability for vulnerabilities \nin AI systems. Our understanding \nof the law is still too immature to \nknow whether major changes are \nneeded. At the same time, \nworkshop participants did \ngenerally support the following \nrecommendations regarding the \nlegal oversight of AI \nvulnerabilities.  \nRecommendations \n3.1. U.S. government agencies with authority over cybersecurity should clarify how \nAI-based security concerns fit into their regulatory structure. The FTC has issued \nmeaningful guidance on how companies using AI can avoid violating the Fair Credit \nReporting Act and the Equal Credit Opportunity Act.33 A wide number of federal \nagencies, including the National Institute of Standards and Technology, the \nCybersecurity and Infrastructure Security Agency, and the FTC, also provide significant \ncybersecurity guidance to private industry. While NIST’s current efforts to develop an \nAI Risk Management Framework include some discussion of AI security,34 current \nguidance on AI security remains vague and does not articulate concrete risks or discuss \nappropriate countermeasures.35 Moreover, despite a 2019 executive order requiring \nfederal agencies to document any potential regulatory authority they might have over \nWorkshop participants did not feel \nthat it was appropriate at this time \nto call for comprehensive \nlegislation to address liability for \nvulnerabilities in AI systems. Our \nunderstanding of the law is still too \nimmature to know whether major \nchanges are needed. \n\n \nCenter for Security and Emerging Technology | 21 \nAI systems, many agencies declined to respond or have offered extremely surface-\nlevel responses.36 We encourage agencies with regulatory authority over cybersecurity \nto articulate more concretely how AI vulnerability fits within that regulatory authority. \nAs part of that effort, agencies should formulate concrete guidance on minimum \nsecurity standards for AI. \n3.2. There is no need at this time to amend anti-hacking laws to specifically address \nattacking AI systems. It is unclear whether some types of attacks studied by \nadversarial machine learning researchers fit within the main federal anti-hacking law, \nthe CFAA, in large part because they do not require “unauthorized access”—a key \nphrase in the CFAA—in the way that traditional hacking does.37 However, any attempt \nto criminalize “AI hacking” would likely raise thorny overbreadth concerns, similar to \nthose that have plagued the CFAA for years.38 And while many types of “AI hacking” \nmay not be covered by the CFAA, malicious attacks on AI-based systems are likely \nalready illegal under other laws.39 For now, the best course is to see how the issues \nplay out in the courts. Therefore, we advise against any attempts to adopt new laws \naimed at punishing adversarial machine learning.  \n \n \n\n \nCenter for Security and Emerging Technology | 22 \n4. Supporting Effective Research to Improve AI Security \nMany of the barriers to developing secure AI systems are social and cultural in nature, \nnot technical. The incentives facing academics, industry professionals, and government \nresearchers all encourage—to some extent—a fixation on marginal improvements in \nsummary performance metrics as the primary sign of progress. While adversarial \nmachine learning is a fast-burgeoning field, by some counts it comprises less than 1 \npercent of all academic AI research—and the research that does exist is heavily \nfocused toward a small subset of attack types, such as adversarial examples, that may \nnot represent plausible real-world attack scenarios.40 Security is often a secondary \nconsideration for organizations looking to deploy machine learning models. As long as \nthat remains true, technical interventions can have, at best, a limited ability to make AI \nsystems more secure in general. \nAt the same time, the research community’s level of knowledge about adversarial \nmachine learning remains low. While the number of successful attack strategies \nexplored by researchers has exploded, the feasibility of technically defusing these \nvulnerabilities is uncertain; in particular, it remains unclear how much general-purpose \ndefense against multiple types of attacks is possible. In this report, we have alluded \nseveral times to potential trade-offs between security and performance. Even though \nthe existence of trade-offs is clear, it is difficult to assess their extent, establish options \nfor response, engage all relevant stakeholders in risk management, and characterize \nthe extent to which different goals are being traded against one another. We are not \naware of an established process for considering these trade-offs.  \nWhile this situation calls for more investment in AI security research generally, it \nrepresents a place for government policymakers in particular to make a sizable impact. \nSecurity is an area where industry may underinvest, which creates an opportunity for \npublicly-funded research. Workshop participants felt that funding additional research \ninto AI security should be an important priority, and that policymakers can take a few \nspecific actions to most effectively push this area of research forward.  \nRecommendations \n4.1. Adversarial machine learning researchers and cybersecurity practitioners \nshould seek to collaborate more closely than they have in the past. As mentioned \nabove, adversarial machine learning research comprises only a very small amount of AI \nresearch generally, with the research that does exist focusing heavily on the specific \nattack vector of adversarial examples.41 Much research into these topics further focuses \non threat scenarios that may be unrealistic—such as by assuming that attackers can \n\n \nCenter for Security and Emerging Technology | 23 \nmanipulate individual pixels in input images—such that research into more likely types \nof threat models may currently be receiving insufficient attention. We suggest that \nfurther collaboration between adversarial machine learning researchers and \ncybersecurity researchers could help more effectively identify the most realistic threat \nscenarios facing AI deployers so they can adequately focus their mitigation efforts.   \n4.2. Public efforts to promote AI research should more heavily emphasize AI \nsecurity, including through funding open-source tooling that can promote more \nsecure AI development. In recent years, the federal government has dedicated \nsignificant funding to AI research under the National AI Initiative Act of 2020, the \nCHIPS and Science Act of 2022, and numerous agency-specific initiatives or projects.42 \nMany of these funds will be used for federal research into AI or disbursed as grants to \nAI researchers via the National Science Foundation, NIST, the U.S. Department of \nEnergy, and a potential National Artificial Intelligence Research Resource. Much of the \nrhetoric around these initiatives emphasizes public funding for “basic” AI research, as \nwell as curation of government datasets for specific applications.43 We suggest that AI \nsecurity should be viewed as a necessary \ncomponent of basic AI research. \nGovernment policymakers should consider \nhow they can best support security-\noriented research that effectively \ncomplements the research that private \nindustry is already incentivized to pursue, \nrather than replicating or competing with it. \nIn particular, supporting the development \nof open-source tools that can better help AI \nengineers incorporate security into their products may be a worthwhile goal. As one \nworkshop participant observed, “Adding a machine learning model [to a product] is \ntwo lines of code; adding defenses can take hundreds.” Existing machine learning \nlibraries are extensively tooled to support the easy design of hundreds of model \narchitectures, but very few contain significant support for common defensive \ntechniques studied by adversarial machine learning researchers. This is an area with a \nclear gap in technical tooling that government support could help overcome.  \n4.3. Government policymakers should move beyond standards-writing toward \nproviding test beds or enabling audits for assessing the security of AI models. In \nrecent years, government agencies have begun providing high-level guidance that \nencourages AI developers to carefully consider the potential security impacts of their \nmodels. We welcome these goals and recommend further efforts in this direction, \nAs one workshop participant \nobserved, “Adding a machine \nlearning model [to a product] \nis two lines of code; adding \ndefenses can take hundreds.” \n\n \nCenter for Security and Emerging Technology | 24 \nespecially among agencies that may potentially have regulatory authority over the use \nof AI systems. Yet machine learning is too broad of a field for many of these standards \nto provide direct answers to questions such as: How do I verify the security of this \nparticular model? Or: What specific techniques apply to this use case to reduce the \nthreat of (for example) membership inference attacks? \nWhile standards-writing is important, we suggest that government policymakers \nshould also engage more actively in providing test beds and audits for AI models.44 \nOne example of such a program is the Face Recognition Vendor Test, an ongoing \nvoluntary test which facial recognition developers can use to identify potential biases \nin their models.45 Similarly, one way for government policymakers to promote research \nin AI security would be to identify a small number of high-risk scenarios and develop \naudit tools that could be used by vendors to probe their models for security-relevant \nvulnerabilities. Such programs would also allow policymakers to develop a better \nunderstanding of the degree to which existing AI products are vulnerable to known \nvectors of attack, while also tracking this level of vulnerability over time.   \n \n \n \n \n \n \n \n \n \n \n \n\n \nCenter for Security and Emerging Technology | 25 \nAcknowledgments \nOur conversations at the July 2022 workshop were deeply enriched by several other \nparticipants who could not join in the writing process for this final report. For their \ncontributions to our thinking, we would like to thank Aalok Mehta, lead of U.S. public \npolicy at OpenAI; Cristian Canton Ferrer, engineering leader at Meta’s Responsible AI \nteam; and Kendra Albert, public interest technology lawyer and clinical instructor at \nthe Cyberlaw Clinic at Harvard Law School. In addition, we would like to thank Danny \nHague for his logistic support in running the July 2022 workshop, as well as Shelton \nFitch and Jason Ly for their editorial and design support.  \n \n \n \n \n \n \n \n \n \n \n \n \n© 2023 by the Center for Security and Emerging Technology. This work is licensed \nunder a Creative Commons Attribution-Non Commercial 4.0 International License. \nTo view a copy of this license, visit https://creativecommons.org/licenses/by-nc/4.0/. \n \nDocument Identifier: doi: 10.51593/2022CA003 \n \n\n \nCenter for Security and Emerging Technology | 26 \nEndnotes \n \n1 For an overview of various types of attacks on AI systems, see Elham Tabassi, Kevin J. Burns, Michael \nHadjimichael, Andres D. Molina-Markham, and Julian Sexton, “Draft NISTIR 8269: A Taxonomy and \nTerminology of Adversarial Machine Learning” (National Institute of Standards and Technology, October \n2019), https://doi.org/10.6028/NIST.IR.8269-draft. Section 2.1.2., “Techniques,” describes the broad \nclasses of attacks studied by adversarial machine learning researchers. For a breakdown of adversarial \nmachine learning that focuses more heavily on techniques at various stages of the “kill chain,” see \n“ATLAS,” MITRE, accessed September 27, 2022, https://atlas.mitre.org/.  \n2 For a list of some of these incidents, see “Case Studies,” MITRE, accessed September 27, 2022, \nhttps://atlas.mitre.org/studies/. The AI Incident Database also lists several incidents that are tagged with \nthe label “Intent:Deliberate or expected”; see AI Incident Database, accessed January 18, 2023, \nhttps://incidentdatabase.ai/apps/discover?classifications=CSET%3AIntent%3ADeliberate%20or%20exp\nected.  \n3 Hearing, “To receive testimony on artificial intelligence applications to operations in cyberspace,” \nbefore the United States Senate Committee on Armed Services, 117th Congress (2022) (statement of \nAndrew Moore, vice president and general manager of Cloud AI and Industry Solutions, Google Cloud), \n1:46:45, https://www.armed-services.senate.gov/hearings/to-receive-testimony-on-artificial-\nintelligence-applications-to-operations-in-cyberspace.  \n4 Allen D. Householder, “1.2. CVD Context and Terminology Notes,” The CERT Guide to Coordinated \nVulnerability Disclosure, accessed September 22, 2022, \nhttps://vuls.cert.org/confluence/display/CVD/1.2.+CVD+Context+and+Terminology+Notes.  \n5 Ian Goodfellow, Nicolas Papernot, Sandy Huang, Rocky Duan, Pieter Abbeel, and Jack Clark, “Attacking \nMachine Learning with Adversarial Examples,” OpenAI Blog, February 24, 2017, \nhttps://openai.com/blog/adversarial-example-research/; Ian J. Goodfellow, Jonathon Shlens, and \nChristian Szegedy, “Explaining and Harnessing Adversarial Examples,” arXiv [stat.ML], December 20, \n2014, doi.org/10.48550/arXiv.1412.6572; Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song, \n“Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning,” arXiv [cs.CR], December \n15, 2017, doi.org/10.48550/arXiv.1712.05526; and Reza Shokri, Marco Stronati, Congzheng Song, and \nVitaly Shmatikov, “Membership Inference Attacks against Machine Learning Models,” arXiv [cs.CR], \nOctober 18, 2016, doi.org/10.48550/arXiv.1610.05820.  \n6 See “Case Studies,” MITRE; “ATLAS,” MITRE; and Andrew Moore, Testimony on artificial intelligence. \nSome of these emerging in-the-wild attacks may not resemble the sophisticated methods studied by \nAML researchers. After the release of ChatGPT in November 2022, users quickly began coordinating to \ndevelop input prompts that could cause the model to ignore its safety guidelines and output toxic or \npolitically charged content. See, for instance, Michael King, “Meet DAN — The ‘Jailbreak’ Version of \nChatGPT and How to Use it — AI Unchained and Unfiltered,” Medium, February 5, 2023, \n \n\n \nCenter for Security and Emerging Technology | 27 \n \nhttps://medium.com/@neonforge/meet-dan-the-jailbreak-version-of-chatgpt-and-how-to-use-it-ai-\nunchained-and-unfiltered-f91bfa679024. \n7 See Jonathan Spring, April Galyardt, Allen D. Householder, and Nathan VanHoudnos, “On managing \nvulnerabilities in AI/ML systems,” New Security Paradigms Workshop 2020 (October 2020), \nhttps://doi.org/10.1145/3442167.3442177; and Andrew Grotto and James X. Dempsey, “Vulnerability \nDisclosure and Management for AI/ML Systems: A Working Paper with Policy Recommendations,” \navailable at SSRN, November 15, 2021, https://ssrn.com/abstract=3964084.  \n8 There is a whole class of product vulnerabilities defined around separating input data from code; see, \nfor example, “CWE-707: Improper Neutralization,” MITRE, accessed January 18, 2023, \nhttps://cwe.mitre.org/data/definitions/707.html. However, the tactics for addressing these product \nvulnerabilities do not generally seem to apply to AI vulnerabilities, even if “input sanitization” as a broad \nidea may be relevant. \n9 “Patching” an AI vulnerability is difficult to define precisely, but for many types of AI vulnerabilities it is \noften thought of as entailing some form of model retraining which results in an AI system no longer \ncontaining the original vulnerability. However, attempts to formalize this idea are difficult, and in some \ncases formal definitions of an AI “patch” for a specific type of vulnerability have been shown to fail to \nguarantee the removal of the vulnerability. See Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas \nPapernot, “On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning,” arXiv [cs.LG], \nOctober 22, 2021, arXiv:2110.11891. Even where model retraining would eliminate a vulnerability, it \nmay be financially or computationally prohibitive to do so, and there is rarely any guarantee that model \nretraining will not introduce further vulnerabilities. For many types of vulnerabilities in composed AI \nsystems, “patching” may be more straightforward and may involve more familiar practices, such as \ncertain types of input filtering.  \n10 Existing cybersecurity processes may be flexible enough to account for some ambiguity; for instance, \nthe CVE process generally defers to product owners to define vulnerabilities in their own systems. See \n“Section 7.1: What Is a Vulnerability,” in CVE Numbering Authority (CNA) Rules (CVE Board, March 5, \n2020), https://cve.mitre.org/cve/cna/CNA_Rules_v3.0.pdf. But in some cases, identifying the “product \nowner” of an AI system may be difficult, especially if the training model, “foundation model,” or fine-\ntuned instance of a model are all owned by different organizations and the training algorithm is \npatented by yet another organization. This concern is analogous to issues in cybersecurity that surround \nsupply chain management. See, for example, “Software Bill of Materials,” Cybersecurity and \nInfrastructure Security Agency, accessed January 18, 2023, https://www.cisa.gov/sbom. \n11 It is worth acknowledging that system administrators already often make similar risk calculations in \nchoosing to leave vulnerable systems online because patching will be difficult or time-consuming. See \nFrank Li, Lisa Rogers, Arunesh Mathur, Nathan Malkin, and Marshini Chetty, “Keepers of the Machines: \nExamining How System Administrators Manage Software Updates,” in SOUPS'19: Proceedings of the \n \n\n \nCenter for Security and Emerging Technology | 28 \n \nFifteenth USENIX Conference on Usable Privacy and Security (August 12, 2019): \nhttps://dl.acm.org/doi/10.5555/3361476.3361496.  \n12 Notable or relevant examples of risk management frameworks include Joint Task Force \nTransformation Initiative, “SP 800-39: Managing Information Security Risk: Organization, Mission, and \nInformation System View” (National Institute of Standards and Technology: March 2011), \nhttps://csrc.nist.gov/publications/detail/sp/800-39/final; Brett Tucker, “Advancing Risk Management \nCapability Using the OCTAVE FORTE Process” (Software Engineering Institute, Carnegie Mellon \nUniversity: November 2020), http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=644636; ISO \nTechnical Committee 262 on Risk Management, “ISO 31000:2018 Risk management — Guidelines” \n(International Organization for Standardization: February 2018), \nhttps://www.iso.org/standard/65694.html; Board of Governors of the Federal Reserve System and Office \nof the Comptroller of the Currency, “SR Letter 11-7: Supervisory Guidance on Model Risk Management,” \nApril 4, 2011, https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm; Ross Anderson, \nSecurity Engineering: A Guide to Building Dependable Distributed Systems, 3rd. Edition (Wiley, 2020); \nand Patrick Hall, James Curtis, and Parul Pandey, Machine Learning for High-Risk Applications: \nTechniques for Responsible AI (Sebastopol, CA: O’Reilly, 2022).  \n13 CVE, https://www.cve.org/, accessed January 29, 2023; FIRST, “Common Vulnerability Scoring \nSystem SIG,” accessed January 30, 2023, https://www.first.org/cvss/; Allen D. Householder, Garret \nWassermann, Art Manion, and Christopher King, “The CERT Guide to Coordinated Vulnerability \nDisclosure,” (Software Engineering Institute, Carnegie Mellon University: August 2017), \nhttps://resources.sei.cmu.edu/library/asset-view.cfm?assetid=503330. An example of a relevant guiding \ndocument for incident response processes is Paul Cichonski, Thomas Millar, Tim Grance, and Karen \nScarfone, “SP 800-61 Rev. 2: Computer Security Incident Handling Guide” (National Institute of \nStandards and Technology, August 2012), https://csrc.nist.gov/publications/detail/sp/800-61/rev-2/final.  \n14 As one example of cross-disciplinary inspiration, note that some organizations have experimented \nwith “bug bounty” programs to highlight flaws in machine learning models. Although modeled off of \n“bug bounties” that have traditionally been used to identify security vulnerabilities, these AI bug \nbounties to date have largely focused on issues like fairness and algorithmic bias. See Kyra Yee and \nIrene Font Peradejordi, “Sharing learnings from the first algorithmic bias bounty challenge,” Twitter \nEngineering, September 7, 2021, \nhttps://blog.twitter.com/engineering/en_us/topics/insights/2021/learnings-from-the-first-algorithmic-\nbias-bounty-challenge; and “AI Audit Challenge,” Stanford Institute for Human-Centered Artificial \nIntelligence, accessed September 27, 2022, https://hai.stanford.edu/policy/ai-audit-challenge.  \n15 One significant topic of conversation focused on how the “unpatchability” of AI vulnerabilities affects \nthe feasibility of CVD processes, which typically operate with the goal of keeping discovered \nvulnerabilities secret until a patch can be developed and deployed. One participant suggested that \nunpatchability might make more secrecy societally desirable, as disclosing vulnerabilities without \navailable patches increases their risk of exploitation. However, another participant suggested that end-\nusers should be informed about inherent vulnerabilities in systems they use so that they can apply \n \n\n \nCenter for Security and Emerging Technology | 29 \n \nmitigations in a timely manner. The unpatchability of AI vulnerabilities therefore instead undermines the \ncase for providing the limited period of secrecy that CVD processes generally include, which are \nprimarily societally beneficial insofar as they incentivize the creation of effective patches. Ultimately, \nwhile there was general agreement that some aspects of CVD processes may be challenged by the \nnature of AI vulnerabilities, there was no consensus on whether or how the basic CVD framework \nshould be changed to accommodate those challenges. It is worth emphasizing that this conversation \nrequires careful nuance in distinguishing between patching a vulnerability, remediating it, and mitigating \nit: remediation can include decommissioning a system as well as patching it, while mitigating involves \nreducing the impact or incidence of exploitation rather than removing the underlying vulnerability. See \nfor example DoDI 8531.01, “DoD Vulnerability Management,” September 15, 2020, \nhttps://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dodi/853101p.pdf, section 3.5. The \nmachine learning engineering community, however, does not generally make these distinctions in \ndiscussing vulnerability management. Lack of shared vocabulary may drive misunderstandings and in \ngeneral will slow down deliberation about the best policy options.  \n16 The “Case Studies” compiled by MITRE include a variety of attacks on AI systems. As of September \n2022, only one of these cases—that of the Tay chatbot—resembles an in-the-wild attack that matches a \nstandard attack type as studied by adversarial machine learning researchers (in this case, a data \npoisoning attack). See “Tay Poisoning,” MITRE, accessed September 29, 2022, \nhttps://atlas.mitre.org/studies/AML.CS0009. Similarly, only one deliberate or malicious incident tracked \nby the AI Incident Database appears to clearly map onto an in-the-wild attack as studied by the field of \nadversarial machine learning, again as an example of data poisoning. See Patrick Hall, “Incident 88: \nJewish Baby Stroller Image Algorithm,” Artificial Intelligence Incident Database, accessed September \n27, 2022, https://incidentdatabase.ai/cite/88.  \n17 To our knowledge, as of September 2022, only one CVE entry in the National Vulnerability Database \ndiscussed a vulnerability in a machine learning model—in this case, a vulnerability arising from the \npossibility of using a model extraction attack to bypass the spam filtering model used by Proofpoint \nEmail Protection. See “CVE-2019-20634 Detail,” National Vulnerability Database, accessed September \n27, 2022, https://nvd.nist.gov/vuln/detail/CVE-2019-20634.  \n18 Several workshop participants noted that they felt that “AI vulnerabilities” are often treated as \nparticularly attention-grabbing—potentially more so than many types of traditional software \nvulnerabilities—in ways that create strong incentives against disclosure. Especially in absence of \nestablished norms about vulnerability disclosure and management, organizations may actively prefer to \navoid learning about vulnerabilities in their AI systems and may underinvest in attempts to find such \nvulnerabilities. This issue is not unique to AI, and although norms around other types of vulnerabilities \nhave gradually emerged, bad policies can sometimes create similar perverse incentives for vulnerability \nmanagement more generally. For some recent commentary on how certain requirements for software \nsecurity can generate perverse incentives, see Curtis Kang, “How to Comply With the DoD’s Newer and \nStricter Software Requirements,” Flashpoint, August 24, 2022, https://flashpoint.io/blog/department-of-\n \n\n \nCenter for Security and Emerging Technology | 30 \n \ndefense-software-requirements/ and Walter Haydock, “Security Release Criteria,” Deploying Securely, \nJuly 22, 2022, https://haydock.substack.com/p/security-release-criteria. \n19 Currently existing mechanisms include national government CSIRT advisories (such as from CISA, \nNCSC-NL, or JPCERT/CC), trusted community advisories (such as CERT/CC vulnerability notes), and \nindustry PSIRT bulletins (many companies have well-developed product security teams that regularly \nissue advisories to their constituents; see “FIRST Teams,” FIRST, accessed March 16, 2023, \nhttps://www.first.org/members/teams/), as well as the option to simply publish information about \nidentified vulnerabilities in scholarly, personal, or organizational venues. For a high-level overview of \nvarious types of information-sharing structures, see The MITRE Corporation, “Cyber Information-Sharing \nModels: An Overview,” October 2012, https://www.mitre.org/sites/default/files/pdf/cyber_info \n_sharing.pdf.  \n20 As of September 27, 2022, the AI Incident Database tags only four incidents with the label \n“Intent:Deliberate or expected.” See AI Incident Database, accessed September 27, 2022, \nhttps://incidentdatabase.ai/apps/discover?classifications=CSET%3AIntent%3ADeliberate%20or%20exp\nected&display=details&incident_id=88&page=1. Of these four incidents, one is clearly a case of \nmalicious data poisoning; one refers to manipulation of trading algorithms resulting in a “flash crash” in \n2010 which may or may not have been intended by the “attacker”; one relates to blockchain security and \nnot AI; and one refers to a public deepfake intended to warn the public about the dangers of deepfakes, \nwhich was not used to manipulate an AI system. The MITRE list of “Case Studies” for AI attacks is \nslightly more extensive and includes 15 examples of AI attacks. Although ATLAS continues to accept \npublic contributions of case studies, it is likely to represent a severe undercount of exploits actually \nbeing deployed against deep learning systems due to concerns around shining a public spotlight on \nthese exploits.  \n21 On the variety of information sharing governance structures and their advantages, see Elaine M. \nSedenberg and James X. Dempsey, “Cybersecurity Information Sharing Governance Structures: An \nEcosystem of Diversity, Trust, and Tradeoffs,” arXiv [cs.CY], May 31, 2018, \nhttps://doi.org/10.48550/arXiv.1805.12266. Some workshop participants noted that the features of AI \nvulnerabilities—such as the “unpatcheability” concerns discussed earlier—could disincentive \norganizations from publicly discussing discovered vulnerabilities. Other participants noted that this same \n“unpatcheability” may increase the value of public disclosure, as this feature makes it more important for \nusers to understand the risks they may be accepting in absence of a patch for the vulnerability. We \nemphasize that there are both technical and cultural barriers posed by AI vulnerabilities that have so far \nhindered extensive information sharing, but also that many workshop participants did not view these \nbarriers as insurmountable and expressed substantial desire for more openness.  \n22 See Andrew Lohn, “Downscaling Attack and Defense: Turning What You See Back Into What You \nGet,” arXiv [cs.CR], https://doi.org/10.48550/arXiv.2010.02456 and Andrew Lohn, “Poison in the Well: \nSecuring the Shared Resources of Machine Learning” (Center for Security and Emerging Technology, \nJune 2021), https://doi.org/10.51593/2020CA013.  \n \n\n \nCenter for Security and Emerging Technology | 31 \n \n23 Many approaches for increasing transparency around AI product development already exist. See \nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal \nDaumé III, and Kate Crawford, “Datasheets for Datasets,” Communications of the ACM 64, no. 12 \n(December 2021): 86–92, https://doi.org/10.1145/3458723 and Margaret Mitchell, Simone Wu, Andrew \nZaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and \nTimnit Gebru, “Model Cards for Model Reporting,” in FAT '19: Proceedings of the Conference on \nFairness, Accountability, and Transparency (January 2019): 220–229, \nhttps://doi.org/10.1145/3287560.3287596. For our purposes, we emphasize that sometimes security-\nrelevant decisions involve balancing multiple competing goals; for instance, adversarial retraining to \ndefend against adversarial attacks may degrade model performance on non-adversarial inputs. We \nsuggest that—in addition to the forms of transparency advocated for by these frameworks—\norganizations should also consider incorporating information about defensive measures used in training \na model, their potential impact on other aspects of the model, and ideally the reasoning behind such \ndecisions. However, deciding what exact information to disclose is a difficult process, as some types of \ndisclosure may enable better-targeted attacks of an AI system; in addition, methods that attempt to \nincrease the auditability of an AI model may themselves be susceptible to adversarial attacks. Dylan \nSlack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju, “Fooling LIME and SHAP: \nAdversarial Attacks on Post hoc Explanation Methods,” arXiv [cs.LG], November 6, 2019, \nhttps://arxiv.org/abs/1911.02508.  \n24 In October 2022, the White House released a Blueprint for an AI Bill of Rights, self-described as “an \nexercise in envisioning a future where the American public is protected from the potential harms, and \ncan fully enjoy the benefits, of automated systems.\" It goes on to say that  some of the protections it \ndescribes “are already required by the U.S. Constitution or implemented under existing U.S. laws,” but it \nis not specific in describing those existing laws and it stops short of proposing new legislation. \n“Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People,” (The \nWhite House Office of Science and Technology Policy, October 22), https://www.whitehouse.gov/wp-\ncontent/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf, 8. To date, the most comprehensive \nproposed legislation on AI in the world is the draft AI Act in the EU, which contains several requirements \nregarding security, but at a high level of generality. For example, Chapter 2 of the current draft includes \nthe following requirement: “High-risk AI systems shall be resilient as regards attempts by unauthorized \nthird parties to alter their use or performance by exploiting the system vulnerabilities. The technical \nsolutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the \nrelevant circumstances and the risks. The technical solutions to address AI specific vulnerabilities shall \ninclude, where appropriate, measures to prevent and control for attacks trying to manipulate the training \ndataset (‘data poisoning’), inputs designed to cause the model to make a mistake (‘adversarial \nexamples’), or model flaws.” Draft Regulation (EU) 2021/0106(COD) of the European Commission of 21 \nApril 2021 on a proposal for a regulation of the European Parliament and of the Council laying down \nharmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union \nlegislative acts, art. 15(4). \n25 See Andrew Smith, “Using Artificial Intelligence and Algorithms,” FTC: BUSINESS BLOG \n \n\n \nCenter for Security and Emerging Technology | 32 \n \n(April 8, 2020), https://www.ftc.gov/news-events/blogs/business-blog/2020/04/using-artificial-\nintelligence-algorithms; U.S. Consumer Financial Protection Bureau, Circular 2022-03, Adverse action \nnotification requirements in connection with credit decisions based on complex algorithms (2022) \nhttps://www.consumerfinance.gov/compliance/circulars/circular-2022-03-adverse-action-notification-\nrequirements-in-connection-with-credit-decisions-based-on-complex-algorithms/; U.S. Department of \nJustice, Algorithms, Artificial Intelligence, and Disability Discrimination in Hiring (2022) \nhttps://beta.ada.gov/ai-guidance/; EEOC, The Americans with Disabilities Act and the Use of Software, \nAlgorithms, and Artificial Intelligence to Assess Job Applicants and Employees (2022) \nhttps://www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use-software-algorithms-and-\nartificial-intelligence; and Elisa Jillson, “Aiming for truth, fairness, and equity in your company’s use of \nAI,” FTC: BUSINESS BLOG (April 19, 2021), https://www.ftc.gov/news-events/blogs/business-\nblog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai.  \n26 Insurance Circular Letter No. 1 (2019), N. Y. State Dep’t Fin. Serv., RE: Use of External Consumer Data \nand Information Sources in Underwriting for Life Insurance (January 18, 2019), \nhttps://dfs.ny.gov/industry_guidance/circular_letters/cl2019_01.  \n27 CAL. CIV. CODE, § 1798.185(a)(16).  \n28 See, for instance, Exec. Order No. 14028, 3 C.F.R. 556 (2022) and Cyber Incident Reporting for Critical \nInfrastructure Act of 2022, Pub. L. No. 117-103, §§ 101–107, 136 Stat. 1038–1059 (2022). \n29 See Federal Trade Commission, “FTC Report to Congress on Privacy and Security,” September 13, \n2021, https://www.ftc.gov/system/files/documents/reports/ftc-report-congress-privacy-\nsecurity/report_to_congress_on_privacy_and_data_security_2021.pdf. The FTC has even found in the \nconcepts of unfair or deceptive a “de facto” breach disclosure requirement. See Federal Trade \nCommission, “Security Beyond Prevention: The Importance of Effective Breach Disclosures,” May 20, \n2022, https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2022/05/security-beyond-prevention-\nimportance-effective-breach-disclosures.  \n30 In FTC v. Wyndham Worldwide Corp., 799 F. 3d 236, 257 (3d Cir. 2015), the Third Circuit held in \nessence that the FTC’s authority did extend to cybersecurity practices, and in LabMD, Inc. v. FTC, 894 \nF.3d 1221 (11th Cir. 2018) the Eleventh Circuit assumed, but did not expressly hold, that unfairness \nencompassed negligent cybersecurity practices. However, the Supreme Court’s June 30, 2022 ruling in \n“West Virginia v. Environmental Protection Agency, 597 U.S. ___ (2022)” cast doubt on the authority of \nthe FTC to address data security. In this case, the Court indicated that, “in certain extraordinary cases,” \nregulatory agencies could not issue rules on “major questions” affecting “a significant portion of the \nAmerican economy” without “clear congressional authorization.” While the court was focused on the \nissuance of rules, its logic may also apply to enforcement actions taken in the absence of rules. \n31 The Computer Fraud and Abuse Act of 1986, 18 U.S.C. § 1030. \n \n\n \nCenter for Security and Emerging Technology | 33 \n \n32 For an overview of the CFAA’s relevance for adversarial machine learning research, see Ram Shankar \nSiva Kumar and Kendra Albert, “Smashing the ML Stack for Fun and Lawsuits,” talk, Black Hat, filmed \nAugust 4, 2021, video of talk, 31:53, https://www.youtube.com/watch?v=e3_4ViYRi20.  In Van Buren v. \nUnited States, 593 U. S. ___ (2021), the Supreme Court clarified that “unauthorized access” does not \ninclude circumstances where a company’s terms of service merely forbid certain uses of information on a \ncomputer system that the individual is otherwise allowed to access. In May 2022, the U.S. Department \nof Justice issued a policy on the CFAA stating that attorneys for the government “should decline \nprosecution if available evidence shows the defendant’s conduct consisted of, and the defendant \nintended, good-faith security research.” Press Release, Department of Justice, 9-48.000 - Computer \nFraud and Abuse Act (May 19, 2022), https://www.justice.gov/opa/press-release/file/1507 \n126/download.  \n33 See Smith, “Using Artificial Intelligence,” and Jillson, “Aiming for Truth.”  \n34 NIST, “AI Risk Management Framework,” 14–15.  \n35 An exception may be National Highway Traffic Safety Administration, “Cybersecurity Best Practices \nfor the Safety of Modern Vehicles” (U.S. Department of Transportation, September 2022), \nhttps://www.nhtsa.gov/sites/nhtsa.gov/files/2022-09/cybersecurity-best-practices-safety-modern-\nvehicles-2022-tag.pdf. The non-binding document describes NHTSA’s guidance to the automotive \nindustry for improving vehicle cybersecurity for safety. It mentions machine learning only once, and uses \nthe phrase “artificial intelligence” not at all, but much of what it recommends is applicable to AI-based \nsystems. \n36 Alex Engler, “The AI Bill of Rights Makes Uneven Progress on Algorithmic Protections,” Lawfare, \nOctober 7, 2022, https://www.lawfareblog.com/ai-bill-rights-makes-uneven-progress-algorithmic-\nprotections.  \n37 Kumar and Albert, “Smashing the ML Stack.\" \n38 See, for instance, Andrew Crocker and Kurt Opsahl, “Supreme Court Overturns Overbroad \nInterpretation of CFAA, Protecting Security Researchers and Everyday Users,” Electronic Frontier \nFoundation, June 3, 2021, https://www.eff.org/deeplinks/2021/06/supreme-court-overturns-overbroad-\ninterpretation-cfaa-protecting-security.  \n39 For example, much attention around malicious adversarial attacks has focused around a 2018 result \nfinding that, with a few pieces of tape—and without requiring “unauthorized access” to a self-driving \ncar’s vision recognition system—an autonomous vehicle could be tricked into recognizing a stop sign as \na “speed limit 45” sign. Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei \nXiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song, “Robust Physical-World Attacks on Deep \nLearning Models,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) \n(June 2018), https://doi.org/10.1109/CVPR.2018.00175. If such an attack occurred in the real world and \ncaused physical harm to innocent bystanders, it might be difficult to prosecute it under the CFAA—but \n \n\n \nCenter for Security and Emerging Technology | 34 \n \nlikely not very difficult to prosecute as, say, a form of endangerment. Other AI attacks resulting in \nfinancial losses instead of physical harm are similarly likely to be criminalized by other laws than the \nCFAA. For instance, wire fraud charges have a broader jurisdictional sweep than the CFAA, and could \nlikely apply in many cases where AI attacks result in financial losses, even if the CFAA does not. See \nComputer Crime and Intellectual Property Section, Prosecuting Computer Crimes (Department of Justice \nOffice of Legal Education, 2010), https://www.justice.gov/criminal/file/442156/download, p. 26.   \n40 Helen Toner and Ashwin Acharya, \"Exploring Clusters of Research in Three Areas of AI Safety\" \n(Center for Security and Emerging Technology, February 2022), https://doi.org/10.51593/20210026. \nNote that AI safety is broader than AI security alone, although some AI security topics relating to \nconfidentiality issues may not have been captured by this analysis.  \n41 Toner and Acharya, “Exploring Clusters of Research.” \n42 See National Artificial Intelligence Initiative Act of 2020, 15 U.S.C. §§ 9401–9462 and Research and \nDevelopment, Competition, and Innovation Act of 2022, Pub. L. 117-167, ———.  \n43 For instance, see Daniel E. Ho, Jennifer King, Russell C. Wald, and Christopher Wan, “Building a \nNational AI Research Resource: A Blueprint for the National Research Cloud,” (Stanford Institute for \nHuman-Centered Artificial Intelligence: October 2021): https://hai.stanford.edu/sites/default/files/2022-\n01/HAI_NRCR_v17.pdf.  \n44 One initiative in this direction is Guaranteeing AI Robustness Against Deception (GARD), sponsored \nby DARPA, which resulted in a virtual testbed, toolbox, and benchmarking dataset for use in identifying \nweaknesses to and defenses against adversarial machine learning methods. GARD Project, “Holistic \nEvaluation of Adversarial Defenses,” accessed January 30, 2023, https://www.gardproject.org/.  \n45 See National Institute of Standards and Technology, “Face Recognition Vendor Test (FRVT) Ongoing,” \naccessed October 7, 2022, https://www.nist.gov/programs-projects/face-recognition-vendor-test-frvt-\nongoing. For the importance of evaluating bias in facial recognition systems, see Joy Buolamwini and \nTimnit Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,” \nProceedings of Machine Learning Research 81 (2018): https://proceedings.mlr.press/v81/buolamwini18a \n/buolamwini18a.pdf.",
    "pdf_filename": "Adversarial Machine Learning and Cybersecurity - Risks, Challenges, and Legal Implications.pdf"
}