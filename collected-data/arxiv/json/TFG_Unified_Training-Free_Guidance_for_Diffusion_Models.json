{
    "title": "TFG: Unified Training-Free Guidance",
    "abstract": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods,thougheffectiveinvariousindividualapplications,oftenlacktheoretical groundingandrigoroustestingonextensivebenchmarks. Asaresult,theycould evenfailonsimpletasks,andapplyingthemtoanewproblembecomesunavoidably difficult. This paper introduces a novel algorithmic framework encompassing existingmethodsasspecialcases,unifyingthestudyoftraining-freeguidanceinto theanalysisofanalgorithm-agnosticdesignspace. Viatheoreticalandempirical investigation, we propose an efficient and effective hyper-parameter searching strategythatcanbereadilyappliedtoanydownstreamtask. Wesystematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performanceby8.5%onaverage. Ourframeworkandbenchmarkofferasolid foundationforconditionalgenerationinatraining-freemanner.1 1 Introduction Recent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have demonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24], proteins[1,72],audio[35,29],3Dobjects[40,41],andmanymore. Diffusionmodelsestimatethe gradientoflogdensity(i.e.,Steinscore,[67])ofthedatadistribution[65]viadenoisinglearning objectives, and can generate new samples via an iterative denoising process. With impressive scalabilitytobillionsofdata[58],futurediffusionmodelshavethepotentialtoserveasfoundational generativemodelsacrossawiderangeofapplications. Consequently,theproblemofconditional generationbasedonthesemodels,i.e.,tailoringoutputstosatisfyuser-definedcriteriasuchaslabels, attributes,energies,andspatial-temporalinformation,isbecomingincreasinglyimportant[63,2]. Conditionalgenerationmethodslikeclassifier-basedguidance[66,7]andclassifier-freeguidance[23] typicallyrequiretrainingaspecializedmodelforeachconditioningsignal(e.g.,anoise-conditional classifieroratext-conditionaldenoiser). Thisresource-intensiveandtime-consumingprocessgreatly limitstheirapplicability. Incontrast,training-freeguidanceaimstogeneratesamplesthatalignwith certaintargetsspecifiedthroughanoff-the-shelf differentiabletargetpredictorwithoutinvolvingany additionaltraining. Here,atargetpredictorcanbeanyclassifier,lossfunction,probabilityfunction, orenergyfunctionusedtoscorethequalityofthegeneratedsamples. In classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to predictthetargetpropertyonbothcleanandnoisysamples,incorporatingguidanceinthediffusion ∗Equalcontribution.Correspondingtomailto:haotianye@stanford.edu. 1Codeisavailableathttps://github.com/YWolfeee/Training-Free-Guidance. 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]GL.sc[ 2v16751.9042:viXra",
    "body": "TFG: Unified Training-Free Guidance\nfor Diffusion Models\nHaotianYe1∗ HaoweiLin2∗ JiaqiHan1∗ MinkaiXu1 ShengLiu1\nYitaoLiang2 JianzhuMa3 JamesZou1\nStefanoErmon1\n1StanfordUniversity 2PekingUniversity 3TsinghuaUniversity\nAbstract\nGiven an unconditional diffusion model and a predictor for a target property\nof interest (e.g., a classifier), the goal of training-free guidance is to generate\nsamples with desirable target properties without additional training. Existing\nmethods,thougheffectiveinvariousindividualapplications,oftenlacktheoretical\ngroundingandrigoroustestingonextensivebenchmarks. Asaresult,theycould\nevenfailonsimpletasks,andapplyingthemtoanewproblembecomesunavoidably\ndifficult. This paper introduces a novel algorithmic framework encompassing\nexistingmethodsasspecialcases,unifyingthestudyoftraining-freeguidanceinto\ntheanalysisofanalgorithm-agnosticdesignspace. Viatheoreticalandempirical\ninvestigation, we propose an efficient and effective hyper-parameter searching\nstrategythatcanbereadilyappliedtoanydownstreamtask. Wesystematically\nbenchmark across 7 diffusion models on 16 tasks with 40 targets, and improve\nperformanceby8.5%onaverage. Ourframeworkandbenchmarkofferasolid\nfoundationforconditionalgenerationinatraining-freemanner.1\n1 Introduction\nRecent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have\ndemonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24],\nproteins[1,72],audio[35,29],3Dobjects[40,41],andmanymore. Diffusionmodelsestimatethe\ngradientoflogdensity(i.e.,Steinscore,[67])ofthedatadistribution[65]viadenoisinglearning\nobjectives, and can generate new samples via an iterative denoising process. With impressive\nscalabilitytobillionsofdata[58],futurediffusionmodelshavethepotentialtoserveasfoundational\ngenerativemodelsacrossawiderangeofapplications. Consequently,theproblemofconditional\ngenerationbasedonthesemodels,i.e.,tailoringoutputstosatisfyuser-definedcriteriasuchaslabels,\nattributes,energies,andspatial-temporalinformation,isbecomingincreasinglyimportant[63,2].\nConditionalgenerationmethodslikeclassifier-basedguidance[66,7]andclassifier-freeguidance[23]\ntypicallyrequiretrainingaspecializedmodelforeachconditioningsignal(e.g.,anoise-conditional\nclassifieroratext-conditionaldenoiser). Thisresource-intensiveandtime-consumingprocessgreatly\nlimitstheirapplicability. Incontrast,training-freeguidanceaimstogeneratesamplesthatalignwith\ncertaintargetsspecifiedthroughanoff-the-shelf differentiabletargetpredictorwithoutinvolvingany\nadditionaltraining. Here,atargetpredictorcanbeanyclassifier,lossfunction,probabilityfunction,\norenergyfunctionusedtoscorethequalityofthegeneratedsamples.\nIn classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to\npredictthetargetpropertyonbothcleanandnoisysamples,incorporatingguidanceinthediffusion\n∗Equalcontribution.Correspondingtomailto:haotianye@stanford.edu.\n1Codeisavailableathttps://github.com/YWolfeee/Training-Free-Guidance.\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]GL.sc[\n2v16751.9042:viXra\nFigure1: (a)IllustrationoftheunifiedsearchspaceofourproposedTFG,wheretheheight(color)\nstands for performance. Existing algorithms search along sub-manifolds, while TFG results in\nimprovedguidancethankstoitsextendedsearchspace. (b)Thelabelaccuracy(higherthebetter)and\nFréchetinceptiondistance(FID,lowerthebetter)ofdifferentmethodsforthelabelguidancetaskon\nCIFAR10[30],averagedacrosstenlabels. Ours(TFG-4)performsmuchclosertotraining-based\nmethods. (c∼h)TFGgeneratedsamplesacrossvarioustasksinvision,audio,andgeometrydomains.\nprocessisstraightforwardsincethegradientoftheclassifierisanunbiaseddrivingterm. Training-\nfreeguidance,however,isfundamentallymoredifficult. Theprimarychallengeliesinleveraging\na target predictor trained solely on clean samples to offer guidance on noisy samples. Although\nvariousapproacheshavebeenproposed[18,63,6,2,78]andareeffectiveforsomeindividualtasks,\ntheoreticalgroundingandcomprehensivebenchmarksarestillmissing. Indeed,existingmethods\nfailtoproducesatisfactorysamplesforlabelguidanceevenonsimpledatasetssuchasCIFAR10\n(Figure1). Moreover,thelackofquantitativecomparisonsbetweenthesemethodsmakesitdifficult\nforpractitionerstoidentifyanappropriatealgorithmforanewapplicationscenario.\nThispaperproposesanovelandgeneralalgorithmicframeworkfor(andalsonamedas)TrainingFree\nGuidance(TFG). WeshowthatexistingapproachesarespecialcasesoftheTFGastheycorrespond\ntoparticularhyper-parametersubspaceinourunifiedspace. Inotherwords,TFGnaturallysimplifies\nandreducesthestudyoftraining-freeguidance,aswellasthecomparisonsbetweenexistingmethods,\nintotheanalysisofhyper-parameterchoicesinourunifieddesignspace. Withinourframework,we\nanalyzetheunderlyingtheoreticalmotivationofeachhyper-parameterandconductcomprehensive\nexperimentstoidentifytheirinfluence. Oursystematicstudyoffersnovelinsightsintotheprinciples\nbehindtraining-freeguidance,allowingforatransparentandefficientsurveyoftheproblem.\n2\nBasedontheframework,weproposeahyper-parametersearchingstrategyforgeneraldownstream\ntasks. We comprehensively benchmark TFG and existing algorithms across 16 tasks (ranging\nfromimagestomolecules)and40targets. TFGachievessuperiorperformanceacrossalldatasets,\noutperforming existing methods by 8.5% on average. In particular, it excels in generating user-\nrequiredsamplesinvariousscenarios,regardlessofthecomplexityoftargetsanddatasets.\nInsummary,we(1)proposeTFGthatunifiesexistingalgorithmsintoadesignspace,(2)theoretically\nand empirically analyze the space to propose an effective space-searching strategy for general\nproblems,and(3)benchmarkallmethodsonnumerousqualitativelydifferenttaskstopresentthe\nsuperiority of TFG and the guideline for future research in training-free conditional generation\nalgorithms. This advancement demonstrates the efficacy of TFG and establishes a robust and\ncomprehensivebenchmarkforfutureresearchintraining-freeconditionalgenerationalgorithms.\n2 Background\nGenerativediffusionmodel. Agenerativediffusionmodelisaneuralnetworkthatcanbeusedto\nsamplefromanunconditionaldistributionp (x)withthesupportonanycontinuoussamplespace\n0\nX [21,62,64,27]. Forinstance,X couldbe[−1,1]d×d×3 representingtheRGBcolorsofd×d\nimages[4,22],orR3drepresentingthe3Dcoordinatesofmoleculeswithdatoms[24,74,73]. Given\nadatax sampledfromp (x),atimestept∈[T]≜{1,··· ,T},acorrespondingnoisydatapoint\n0 √0 √\nisconstructedasx = α¯ x + 1−α¯ ϵwhereϵ∼N(0,I)and{α¯ }T isasetofpre-defined\nt t 0 t t t=1\nmonotonicallydecreasingparametersusedtocontrolthenoiselevel.Following[21],wefurtherdefine\nα =α¯ /α¯ fort>1andα =α¯ . Thediffusionmodelϵ :X ×[T](cid:55)→X parameterizedbyθis\nt t t−1 1 1 θ\ntrainedtopredictthenoiseϵthatwasaddedonx withp.d.fp (x )=(cid:82) p (x )p (x |x )dx 2.\nt t t x0 0 0 t|0 t 0 0\nIntheory,thiscorrespondstolearningthescoreofp (x)[65],i.e.,\nt\n(cid:88)T √\nargmin E ∥ϵ (x ,t)−ϵ∥=− 1−α¯ ∇logp . (1)\nx0∼p0(x0),ϵ∼N(0,I) θ t t t\nϵθ\nt=1\nForsampling,westartfromx ∼ N(0,I)andgraduallysamplex ∼ p (x |x ). This\nT t−1 t−1|t t−1 t\nconditionalprobabilityisnotdirectlycomputable,andinpractice,DDIM[62]samplesx via\nt−1\n√\n√ (cid:113) x − α¯ x\nx = α¯ x + 1−α¯ −σ2 t √ t 0|t +σ ϵ, (2)\nt−1 t−1 0|t t−1 t 1−α¯ t\nt\nwhere{σ }T areDDIMparameters,ϵ∼N(0,I),and\nt t=1\n√\nx − 1−α¯ ϵ (x ,t)\nx =m(x )≜ t √ t θ t (3)\n0|t t α¯\nt\nisthepredictedsamplegivenx . AccordingtotheTweedie’sformula[11,51],x equalstothe\nt 0|t\nconditionalexpectationE[x |x ]underperfectoptimizationofϵ inEquation(1). Ithasbeentheo-\n0 t θ\nreticallyestablishedthattheabovesamplingprocessresultsinx ∼p(x)undercertainassumptions.\n0\nTarget predictor. For a user required target c, we use a predictor f (x) : X (cid:55)→ R ∪{0} 3 to\nc +\nrepresent how well a sample x is aligned with the target (higher the better). Here f (x) can be\nc\na conditional probability p 0(c|x) for a label c [62, 14], a Boltzmann distribution exp−ec(x) for\nany pre-defined energy function e [31, 63, 38], the similarity of two features [47], or even their\nc\ncombinations. Thegoalistosamplesfromtheconditionaldistribution\np (x)f (x)\np 0(x|c)≜ (cid:82) p0 (x˜)fc (x˜)dx˜. (4)\nx˜ 0 c\n2Inthispaper,weusep(x)torepresenttheprobabilitydensityfunction(p.d.f.),andp (x),p (x|x˜)to\nt t|s\nrepresenttheprobabilityattimesteptandtheconditionalprobabilityofxattimesteptgivenx˜attimesteps.\n3Here c can has any mathematical form. We assume in this paper that f (x) has finite two norm, i.e.\n(cid:82) [f2(x)]<+∞,suchthattheprobabilisticexplanationiswell-defined. c\nx∈X c\n3\nTraining-basedguidancefordiffusionmodels. [66]proposestotrainatime-dependentclassifierto\nfitf (x ,t)≜E f (x ). Thiscanberegardedasapredictorovernoisysamples. Since\nc t x0∼p0|t(·|xt) c 0\n(cid:90)\n∇ logp (x |c)=∇ log p (x |x )p (x |c)dx\nxt t t xt t|0 t 0 0 0 0\nx0\n(cid:90)\n=∇ log p (x )p (x |x )f (x )dx\nxt t t 0|t 0 t c 0 0\nx0\n=∇ logp (x )+∇ logf (x ,t), (5)\nxt t t xt c t\nif we denote the trained classifier as f(x ) (that implicitly depends on c and model parameters),\nt √\nwe can replace ϵ (x ,t) in Equation (3) by ϵ (x ,t)− 1−α¯ ∇ logf(x ) upon sampling to\nθ t θ t t xt t\nobtainunbiasedsamplex ∼p (x |c). Ontheotherhand,[23]proposestheclassifier-freediffusion\n0 0 0\nguidanceapproach. Insteadoftrainingatime-dependentpredictorf,itencodesconditionscdirectly\nintothediffusionmodelasϵ (x,c,t)andtrainsthiscondition-awarediffusionmodelwithsample-\nθ\nconditionpairs. Bothmethodshavebeenproveneffectivewhentrainingresourcesareavailable.\nThispaperincontrastfocusesonconditionalgenerationinatraining-freemanner: givenadiffusion\nmodelϵ (x,t)andanoff-the-shelftargetpredictorf(x)(weomitthesubscriptcbelow),weaim\nθ\ntogeneratesamplesfromp (x|c)withoutanyadditionaltraining. Unliketraining-basedmethods\n0\nthatcanaccuratelyestimatef(x ,t),training-freeguidanceissignificantlymoredifficultsinceit\nt\ninvolvesguidinganoisydatax usingf(x)definedoverthecleandataspace.\nt\n2.1 Existingalgorithms\nMostexistingmethodstakeadvantageofthepredictedsamplex definedinEquation(3)andusethe\n0|t\ngradientoff(x)forguidance. Wereviewandsummarizefiveexistingapproachesbelow,andprovide\naschematicandacopyofpseudo-codeinAppendixBforthesakeofreference. Duetothevarietyin\nunderlyingintuitionsandimplementations,coupledwithalackofquantitativecomparisonsamong\nthesemethods,itischallengingtodiscernwhichoperationsarecrucialandwhicharesuperfluous,a\nproblemweaddressinSection3.\nDPS[6]wasinitiallyproposedtosolvegeneralnoisyinverseproblemsforimagegeneration: fora\ngivenconditionyandatransformationoperatorA,weaimtogenerateimagexsuchthat∥A(x)−y∥\n2\nissmall. Forinstance,insuper-resolutiontask[71],theoperatorAisadown-samplingoperator,and\nyisalow-resolutionimage. DPSreplaces∇logf(x ,t)inEquation(5)by∇ logf(m(x )). As\nt xt t\nsuggestedin[63],thiscorrespondstoapointestimationoftheconditionaldensityp (x |x ).\n0|t 0 t\nLGD[63]replacesthepointestimationinDPSandproposestoestimatef(x ,t)withaGaussian\nt\nkernelE f(x,t),wheretheexpectationiscomputedusingMonte-Carlosampling[59].\nx∼N(x0|t,σ t2I)\nFreeDoM [78] generalizes DPS by introducing a “recurrent strategy” (called “time-travel strat-\negy”[39,10,70])thatiterativelydenoisesx fromx andaddsnoisetox toregeneratex back\nt−1 t t−1 t\nandforth. Thisstrategyempiricallyenhancesthestrengthoftheguidanceatthecostofadditional\ncomputation. FreeDoMalsopointsouttheimportanceofalteringguidancestrengthatdifferenttime\nstepst,butacomprehensivestudyonwhichscheduleisbetterisnotprovided.\nMPGD [18] is proposed for manifold preserving tasks, e.g., the target predictor is supposed to\ngenerate samples on a given manifold. It computes the gradient of logf(x ) to x instead of\n0|t 0|t\nx , i.e., ∇ logf(x ) to avoid the back-propagation through the diffusion model ϵ that is\nt x0|t 0|t θ\nhighlyinefficient. Thisstrategyiseffectiveinmanifold-preservingproblems,butwhetheritcanbe\ngeneralizedtogeneraltraining-freeproblemsisunclear. Inadditiontothecomputationdifference,\ntheoreticalunderstandingonthedifferencebetweengradientstox andx ismissing.4\n0|t t\nUGD[2]buildsonFreeDoM,withthedifferencethatitadditionallysolvesabackwardoptimization\nproblem∆ =argmax f(x +∆)andguidesx andx simultaneously.UGDalsoimplements\n0 ∆ 0|t 0|t t\nthe“recurrentstrategy”tofurtherimprovegenerationquality.\n4MPGDadditionallyproposedtouseanauto-encodertoimprovethequalityofx . However,anauto-\n0|t\nencoderisusuallyinaccessibleorrequirestraining;thus,wedon’tapplyitinourtraining-freescenario.\n4\n3 TFG: AUnifiedFrameworkforTraining-freeGuidance\nDespitethearrayofalgorithmsavailableandtheirreportedsuccessesinvariousapplications,we\nconductacasestudyonCIFAR10[30]toillustratethechallengingnatureoftraining-freeguidance\nandtheinsufficiencyofexistingmethods.Specifically,foreachofthetenlabels,weusethepretrained\ndiffusionmodelandclassifiersfrom [7,9]togenerate2048samples,wherethehyper-parameters\nare selected via a grid search for the fairness of comparison. We compute the FID and the label\naccuracyevaluatedbyanotherclassifier[20]andpresentresultsinFigure1. Eveninsucharelatively\nsimple setting, all training-free approaches significantly underperform training-based guidance,\nwithasignificantportionofgeneratedimagesbeinghighlyunnatural(whenguidanceisstrong)or\nirrelevanttothelabel(whenguidanceisweak). Thesefindingsrevealthefundamentalchallenges\nandhighlightthenecessityofacomprehensivestudy. Unfortunately,comparisonsandanalysesof\nexistingapproachesaremissingorprimarilyqualitative,limitingdeeperinvestigationinthisfield.\n3.1 Unificationandextension\nThissectionsintroducesourunifiedframeworkfortraining-freeguidance(TFG,Algorithm1)and\nformallydefinesitsdesignspaceinDefinition3.1. WedemonstratetheadvantageofTFGbydrawing\nconnectionsbetweenTFGandotheralgorithmstoshowthatexistingalgorithmsareencompassedas\nspecialcases. Basedonthis,allcomparisonsandstudiesoftraining-freealgorithmsautomatically\nbecomethestudywithinthehyper-parameterspaceofourframework. Thisallowsustoanalyzethe\ntechniquestheoreticallyandempirically,andchooseanappropriatehyper-parameterforaspecific\ndownstreamtaskefficientlyandeffectively,asshowninSection4.\nAlgorithm1Training-FreeGuidance\n1: Input: Unconditionaldiffusionmodelϵ ,targetpredictorf,guidancestrengthρ,µ,γ¯,numberofsteps\nθ\nT,N ,N\nrecur iter\n2: x ∼N(0,I)\nT\n3: fort=T,··· ,1do\n√\n4: Definefunctionf˜(x)=E f(x+γ¯ 1−α¯ δ)\nδ∼N(0,I) t\n5: forr=1,··· ,N r√ecurdo\n√\n6: x =(x − 1−α¯ ϵ (x ,t))/ α¯ ▷Obtainthepredicteddata\n0|t t t θ t t\n7: ∆ =ρ ∇ logf˜(x )\nt t xt 0|t\n8: ∆ =∆ +µ ∇ logf˜(x +∆ ) ▷IterateN timesstartingfrom∆ =0\n0 0 t x0|t 0|t √0 √ iter 0\n9: x =Sample(x ,x ,t)+∆ / α + α¯ ∆ ▷SamplefollowsEquation(2)\nt−1 √ t √0|t t t t−1 0\n10: x ∼N( α x , 1−α I) ▷Recurrentstrategy\nt t t−1 t\n11: endfor\n12: endfor\n13: Output:Conditionalsamplex\n0\nDefinition3.1. GivenadenoisingstepT,thehyper-parameterspace(designspace)ofAlgorithm1\nisdefinedas\nH ={(N ,N ,γ¯,ρ,µ):N ,N ∈N,γ¯ ≥0,ρ,µ∈(cid:0)R ∪{0}(cid:1)T }. (6)\nTFG recur iter recur recur +\nWeuseH torepresentthecompletehyper-parameterspaceandH (N =N )torepresent\nTFG TFG recur 0\nthesubspaceconstrainedonN =N .\nrecur 0\nDefinition3.1definesthehyper-parameterspacespannedbyTFG,whereonehyper-parameterin\nH isaninstantiationoftheframework.Intuitively,N controlstherecurrenceofthealgorithm,\nTFG recur\nN controlstheiteratingwhencomputing∆ (Line8),γ¯controlstheextentwesmooththeoriginal\niter 0\nguidancefunctionf (Line4),andρ,µcontrolthestrengthoftwotypesofguidance(Lines7and8).\nAcomprehensiveexplanationoftheeffectofeachhyper-parametercanbefoundinSection3.2.\nBelowisthemajortheoremshowingthatallalgorithmspresentedinSection2.1correspondtospecial\ncasesofTFG,thusunifyingthemintoourframeworkandobviatingtheneedforseparateanalyses.\nTheorem3.2. Thehyper-parameterspaceof\n• MPGD[18]H isequivalenttoH (N =N =1,ρ=0,γ¯ =0).\nMPGD TFG recur iter\n• LGD[63]H isequivalenttoH (N =1,N =0,µ=0).\nLGD TFG recur iter\n5\n• UGD[2]H isequivalenttoH (γ¯ =0).\nUGD TFG\n• DPS[6]H isequivalenttoH (N =1,N =0,µ=0,γ¯ =0).\nDPS TFG recur iter\n• FreeDoM[78]H isequivalenttoH (N =0,µ=0,γ¯ =0).\nFreeDoM TFG iter\nThecompleteanalysisandproofofTheorem3.2ispostponedtoAppendixC.Itimpliesthatexisting\nalgorithmsarelimitedinexpressivity,coveringonlyasubsetofH . Incontrast,TFGcoversthe\nTFG\nentire space and is guaranteed to perform better. In addition, TFG streamlines nuances between\nexistingmethods,allowingforaunifiedwaytocompareandstudydifferenttechniques.Consequently,\ntheversatileframeworkthatTFGprovidescansimplifyitsadaptationtovariousapplications.\n3.2 Algorithmanddesignspaceanalysis\nWenowpresentaconcreteanalysisofTFGanditsdesignspaceHindetail. Similartostandard\nclassifier-based guidance, TFG guides x at each denoising step t. To provide appropriate and\nt\ninformative guidance, TFG essentially leverages four techniques for guidance: Mean Guidance\n(Line 8) controlled by N ,µ, Variance Guidance (Line 7) controlled by ρ, Implicit Dynamic\niter\n(Line4)controlledbyγ¯,andRecurrence(Line5)controlledbyN .\nrecur\nMeanGuidancecomputesthegradientoff˜(x)tox andisthemoststraightforwardapproach.\n0|t\nHowever,thismethodcanyieldinaccurateguidance. Toshowthis,noticethatunderperfectoptimiza-\ntionwehavex =E[x |x ],andwhenp (E[x |x ])isclosetozero,thepredictorhasrarelybeen\n0|t 0 t 0 0 t\ntrainedondatafromtheregionclosetox ,makingthegradientunstableandnoisy. Tomitigatethis,\n0|t\nonecaniterativelyaddgradientsoff˜(x)tox ,encouragingx toescapelow-probabilityregions.\n0|t 0|t\nVarianceGuidanceprovidesanalternativeapproachforimprovingthegradientestimation. The\nreason why we dub it variance guidance might be ambiguous, as the only difference is that the\ngradientistakenwithrespecttox (Line7)insteadofx (Line8). Thelemmabelowdemonstrates\nt 0|t\nthatthisessentiallycorrespondstoacovariancere-scaledguidance.\n√\nLemma3.3. Ifthemodelisoptimizedperfectly,i.e.,ϵ (x,t)=− 1−α¯ ∇logp (x),wehave\nθ t t\n√\nα¯\n∆ = t Σ ∇ f˜(x ), (7)\nt 1−α 0|t x0|t 0|t\nt\nwhereΣ ≜(cid:82) p (x|x )(x−E[x |x ])(x−E[x |x ])⊤dxisthecovarianceofx |x .\n0|t x 0|t t 0 t 0 t 0 t\nLemma 3.3 suggests that variance guidance refines mean guidance by incorporating the second-\norderinformationofx |x ,specificallyconsideringthecorrelationamongcomponentswithinx .\n0 t 0|t\nConsequently, positively correlated components could have guidance mutually reinforced, while\nnegativelycorrelatedcomponentscouldhaveguidancecanceled.Thisalsoimpliesthatmeanguidance\nandvarianceguidanceareintrinsicallyleveragingdifferentordersofinformationforguidance. In\nTFG,varianceguidanceiscontrolledbyρ .\nt\nImplicitDynamictransformsthepredictorf intoitscon- Table 1: Influence of the number of\nvolutionviaaGaussiankernelN(0,γ¯(1−α¯ )I). This Monte-Carlosamplesinestimatingthe\nt\noperationisinitiallyintroducedbyLGD[63]toestimate expectationofLine4. BoththeFIDand\np (x |x ). However,itisunclearwhytheformispre- the accuracy remain unchanged when\n0|t 0 t\nselectedasaGaussiandistribution.Wearguethatthistech- #Samples varies, suggesting that the\nniqueiseffectivebecauseitcreatesanimplicitdynamic number of samples is less important.\nonx . Specifically,startingfromx ,ititerativelyadds MoredetailsareinAppendixE.1.\n0|t 0|t\nnoise to x 0|t, evaluates gradient, and moves x 0|t based Varianceonly Meanonly\n#Samples\nonthegradient. Therepeatingprocessconvergestothe FID Acc(%) FID Acc(%)\ndensityproportionaltof(x)whenN iter goestoinfinity, 1 90.6 65.8 101 36.2\ndrivingx tohigh-densityregions. Thisexplanationis 2 91.0 65.2 100 35.6\n0|t\njustifiedbyTable1: theperformanceremainsnearlyun- 4 90.7 64.9 99.7 36.2\nchangedaswegraduallydecreasethenumberofMonte-Carlosamplesinestimatingtheexpectation\n(Line4)downto1,implyingthattheprecisenessofestimationisnotessential,butaddingnoisesis.\nRecurrencehelpsstrengthentheguidancebyiteratingthepreviousthreetechniquestoobtainx\nt−1\nandresamplex backandforth. ThiscanbeunderstoodasanOrnstein–Uhlenbeckprocess[42]on\nt\nx whereLine6∼9correspondstothedrifttermandx →x (Line10)thewhitenoiseterm.\nt−1 t−1 t\nIntuitively,itfindsatrade-offbetweentheerrorinheritedfromprevioussteps(themoreyourecur,the\n6\nCIFAR10 ImageNet CIFAR10 ImageNet\nNrecur=4 increase Nrecur=4 increase\nNrecur=2 decrease Nrecur=2 decrease\nNrecur=1 constant Nrecur=1 constant\n0.25 0.5 1.0 2.0 4.0 0.25 0.5 1.0 2.0 4.0\nNiter=4 increase Niter=4 increase\nNiter=2 decrease Niter=2 decrease\nNiter=1 constant Niter=1 constant\n0.25 0.5 1.0 2.0 4.0 0.25 0.5 1.0 2.0 4.0\nFID FID FID FID\nFigure2:ComparisonofthreestructuresinEquation(8)ofρandµonCIFAR10andImageNet,under\ndifferentchoicesoftheresthyper-parametersinH .Wesetρ=0,γ¯ =0whenstudyingstructures\nTFG\nofµ,andsimilarlyforρ.Resultsareaveragedacrossalllabels.Thecomparativerelationshipbetween\nstructuresremainsunchangedwhentherestoftheparametersvary.\nlesspreviouserrorstays)andtheaccumulatederrorinthisstep(themoreyourecur,themoreerrorin\nthecurrentguidanceyousuffer). Empirically,wealsofindthatthegenerationqualityimprovesand\nthendeterioratesasweincreaseN .\nrecur\n4 DesignSpaceofTFG: AnalysisandSearchingStrategy\nAdmittedly, a more extensive design space only yields a better performance if an effective and\nrobusthyper-parametersearchingstrategycanbeapplied. Forexample,arbitrarilycomplexneural\nnetworksareguaranteedtohavebetteroptimalperformancethansimplelinearmodels,butfinding\nthecorrectmodelparametersissignificantlymoredifficult. Thissectiondivesintothiscoreproblem\nbycomprehensivelyanalyzingthehyper-parameterspacestructureofH ,andfurtherproposinga\nTFG\ngeneralsearchingalgorithmapplicableforanygeneraldownstreamtasks.\nThe hyper-parameters of H can be categorized into two parts: time-dependent vectors ρ,µ,\nTFG\nand time-independent sacalars N ,N ,γ¯. While a grid search can potentially result in the\nrecur iter\nbest performance, performing such an extensive search in H is highly impractical, especially\nTFG\nconsideringthevectorparametersρ,µ. Fortunately,belowwedemonstratethat,ifwedecompose\nρintoρ¯·s (t)(sameforµ)whereρ¯isascalarands (t)isa“structure”(anon-negativefunction)\nρ ρ\n(cid:80)\nsuchthat s (t) = T,thensomestructuresareconsistentlybetterthanothersregardlessofthe\nt ρ\notherhyper-parameters. Thisallowsustopre-locateanappropriatestructureforthegiventaskand\nefficientlyoptimizetherestofthescalarhyper-parameters. Ouranalysisisconductedonthelabel\nguidancetaskonCIFAR-10[30]andImageNet[55],withexperimentalsettingsidenticaltoSection3.\nStructureanalysis. MotivatedbythedefaultstructureselectedinUGDandLGD,weconsiderthree\nstructuresforboths (t)ands (t)as\nρ µ\nα (1−α )\ns(t)= t (increase), s(t)= t (decrease), s(t)=1(constant). (8)\n(cid:80)T\nα\n(cid:80)T\n(1−α )\nt=1 t t=1 t\nThesestructuresareselectedtobequalitativelydifferent,whileeachisjustifiedtobereasonable\nundercertainconditions[18,78,2]. Weleavethestudyofmorestructurestofutureworks. The\nrestoftheparametersaregrid-searchedforthecomprehensivenessoftheanalysis. Fors (t),we\nρ\nsetN = {1,2,4}andρ¯= {0.25,0.5,1.0,2.0,4.0};andfors (t),wesetN = {1,2,4}and\nrecur µ iter\nµ¯ ={0.25,0.5,1.0,2.0,4.0}. Werunlabelguidanceforeachconfigurationandeachofthetenlabels\nonCIFAR10(fourlabelsonImageNet,duetocomputationconstraints).\nAspresentedinFigure2,therelationshipbetweendifferentstructuresremainsunchangedwhenthe\nrest of the parameters vary. For instance, on both datasets, the Validity-FID performance curves\nconsistentlymovetop-left(implyingabetterperformance)whenweswitchfrom“decrease”structure\n(redlines)to“constant”structure(yellowlines)to“increase”structure(bluelines)forbothρ,µand\ndifferentvaluesofN andN . Thisinvariantrelationshipisessentialasitallowsforanefficient\nrecur iter\nhyper-parameterssearchinH byfirstdeterminingappropriatestructuresfors (t),s (t)undera\nTFG ρ µ\nsimplesubspace,andthenselectingtherestscalarparameters.\n7\nycaruccA ycaruccA ycaruccA ycaruccA\nTable2: Listof14tasktypeswebenchmark.Eachtaskisrunwithmultipleindividualtargets(38intotal).We\nevaluatetheguidancevalidity(howwellasampleisalignedwiththetargetpredictor)andtheguidancefidelity\n(howwellasampleisalignedwiththeunconditionaldistribution)accordingtothetasktype.\nDiffusionModel Task-ID Targets GuidanceValidity GuidanceFidelity\nGaussiandeblur \\ LPIPS↓ FID↓\nCat-DDPM\nSuper-resolution \\ LPIPS↓ FID↓\nCombinedguidance(gender+age) 2genders×2ages Accuracy(%)↑ KID(log)↓\nCelebA-DDPM\nCombinedguidance(gender+hair) 2genders×2haircolors Accuracy(%)↑ KID(log)↓\nCIFAR10-DDPM Labelguidance(CIFAR10) 10labels(0,···,9) Accuracy(%)↑ FID↓\nLabelguidance(ImageNet) 4labels(111,···,444) Accuracy(%)↑ FID↓\nImageNet-DDPM\nFine-grainedguidance 4labels(111,···,444) Accuracy(%)↑ FID↓\nStable-Diffusion Styletransfer 4styles Stylescore↓ CLIPscore↑\nMolecule-EDM QuantumProperties(×6) Propertydistribution MAE↓ Validratio↑\nAudiodeclipping \\ DTW(%)↓ FAD↓\nAudio-Diffusion\nAudioinpainting \\ DTW(%)↓ FAD↓\nComputationcostanalysis. Amongthescalars\nNiter=4\np toa tr aa lm ce ote mrs p, uN tar te ic ou nr aa lnd coN stit ,er wdi hr ie lc etl ρy ¯,i µ¯nfl ,γ¯uen dc oe nth oe\nt.\nN Ni it te er r= =2 1\nWithacertainrange,performanceincreaseswhen\nthevalueofN ,N increase5,andthetrade- recur iter\noffbetweengenerationqualityandcomputation\ntimeispresentedinFigure3: recurrenceleadsto NNr re ec cu ur r= =4 2 N Ni it te er r= =4 2\na N times cost with clear performance gain;\nNrecur=1 Niter=1\nrecur 0.25 0.5 1.0 2.0 4.0\niteration(onx 0|t)resultsinlessincreaseofcom- FID Nrecur=1 Nrecur=2 Nrecur=4\nputationtime,anditseffectplateaus. Inpractice,\nFigure3: AccuracyandFIDonCIFAR10under\nuserscandeterminetheirvaluesbasedoncompu-\ndifferentN andN . s (t),s (t)arefixed\nrecur iter ρ µ\ntationresources,butanupperboundof4suffices\nto“increase”structure,andρ=γ =0.\ntounlockanear-optimalperformance.\nSearching strategy. The above analysis successfully simplify the task-specific hyper-parameter\nsearchproblemwithoutsignificantperformancesacrifice. Itremainstobedecidedthescalarvalues\nρ¯,µ¯,γ¯. Hereweproposeastrategybasedonbeamsearchtoeffectivelyandefficientlyselecttheir\nvalues. Specifically,oursearchingstrategystartswithaninitialsetT ={(ρ¯ ,µ¯ ,γ¯ )},where\ninit init init\ntheseinitialvaluesaresmallenoughtoapproximateTFGasanunconditionalgeneration. Ateach\nsearchingstep,foreachtupleinT,weseparatelydoublethevaluesofρ¯,µ¯,andγ¯togenerateupto\n3|T|newconfigurations. Weconductasmall-sizedgenerationtrialforeachnewconfigurationand\nupdateT tobethetopK configurationswiththehighestevaluationresultsthataredeterminedby\nuserrequirements(e.g.,accuracy,FID,oracombination). ThisiterativeprocessisrepeateduntilT\nstabilizesorthemaximumnumberofsearchstepsisreached. Noticethatthisprocessisconducted\nwithamuchsmallersamplesize,andconsequently,thecomputationtimeishighlycontrollable.\n5 Benchmarking\nThissectioncomprehensivelybenchmarkstraining-freeguidanceundertheTFGframeworkandthe\ndesignsearchingstrategyinSection4. Weconsider7datasets,16differenttasks,and40individual\ntargetswithatotalexperimentalcostofmorethan2,000A100GPUhours. Forcomparison,wealso\nrunexperimentsforeachoftheexistingmethods(wherethedesignsearchingisconductedinthe\ncorrespondingsubspace). Allmethods,tasks,searchstrategies,andevaluationsareunifiedinour\ncodebase,withdetailsspecifiedinAppendicesDandE.\n5.1 Settings\nDiffusion models. (1) CIFAR10-DDPM [48] is a U-Net [54] model trained on CIFAR10 [30]\nimages. (2)ImageNet-DDPM[7]isanlargerU-NetmodeltrainedonImageNet-1k[55]images. (3)\nCat-DDPMistrainedonCat[12]images. (4)CelebA-DDPMistrainedonCelebA-HQdataset[26]\nthatconsistsmillionsofhumanfacialimages. (5)Molecule-EDM[24]isanequivariantdiffusion\n5WenoticethattheFIDworsenswhenN orN aretoolarge(e.g.,10).\nrecur iter\n8\nycaruccA\n)selpmas\n652\n/ s( tsoC\nemiT\nTable3: BenchmarkingTFGandexistingalgorithmson16tasktypesand40individualtargets. Eachcell\npresentstheguidancevalidity/generationfidelityaveragedacrossmultipletargetsinthetask(e.g.,labels,image\nstyles).Thebestguidancevalidityisbold,andthesecondbestunderline.Therelativeimprovementofguidance\nvalidityiscomputedbetweenTFGandtheexistingmethodwiththehighestguidancevalidity.\nTask-ID DPS LGD FreeDoM MPGD UGD TFG Rel.Improvement\nDeblur(↓,↓) 0.390/98.3 0.270/85.1 0.245/87.4 0.177/69.3 0.200/69.3 0.150/64.5 +15.3%\nSuperresolution(↓,↓) 0.420/109 0.360/96.7 0.191/74.5 0.283/82.0 0.249/75.9 0.190/65.9 +0.524%\nGender+Age(↑,↓) 71.6/-4.26 52.0/-5.10 68.7/-3.89 68.6/-4.79 75.1/-4.37 75.2/-3.86 +0.133%\nGender+Hair(↑,↓) 73.0/-3.90 55.0/-5.00 67.1/-3.50 63.9/-4.33 71.3/-4.12 76.0/-3.60 +4.11%\nCIFAR10(↑,↓) 50.1/172 32.2/102 34.8/135 38.0/88.3 45.9/94.2 52.0/91.7 +3.59%\nImageNet(↑,↓) 38.8/193 11.5/210 19.7/200 6.80/239 25.5/205 40.9/176 +5.41%\nFine-grained(↑,↓) 0.00/348 0.48/246 0.58/258 0.58/249 1.07/255 1.27/256 +18.7%\nStyleTransfer(↓,↑) 5.06/31.7 5.42/31.3 5.26/31.2 4.08/31.5 4.97/31.5 3.16/29.0 +22.5%\nPolarizabilityα(↓,↑) 51169.7/92.3 7.155/84.3 5.922/88.0 4.26/88.4 5.45/73.8 3.90/84.2 +8.45%\nDipoleµ(↓,↑) 63.2/77.3 1.51/86.6 1.35/89.5 1.51/73.5 1.56/57.6 1.33/74.9 +1.48%\nHeatcapacityCv(↓,↑) 5.26/78.4 3.77/77.1 2.84/90.9 2.86/86.1 3.02/84.0 2.77/85.5 +2.57%\nHighestMOenergyϵHOMO(↓,↑) 0.744/83.8 0.664/66.4 0.623/62.3 0.554/53.4 0.582/58.2 0.568/77.3 -2.53%\nLowestMOenergyϵLUMO(↓,↑) NA/NA 1.20/90.9 1.16/90.2 1.06/82.2 1.27/85.1 0.984/80.1 +7.17%\nMOenergygapϵ∆(↓,↑) 1.38/75.7 1.19/85.3 1.17/88.5 1.07/72.5 1.15/75.7 0.893/62.5 +16.7%\nAudiodeclipping(↓,↓) 633/3.60 157/2.33 126/0.173 178/0.402 150/0.262 101/0.172 +19.8%\nAudioinpainting(↓,↓) 643/4.71 103/2.22 41.3/0.08 608/4.63 116/0.53 36.3/0.06 +12.1%\nmodelpretrainedonmoleculedatasetQM9[50]thatperformsmoleculegenerationfromscratch.\n(6) Stable-Diffusion (v1.5) [53] is a latent text-to-image model that generate images with text\nprompts. (7)Audio-Diffusion6isaaudiodiffusionmodelbasedonDDPMtrainedtogeneratemel\nspectrogramsof256x256correspondingto5secondsofaudio.\nTasks.Ourtasks(Table2)coverawiderangeofinterests,includingGaussiandeblur,super-resolution,\nlabelguidance,styletransfer,moleculepropertyguidance,audiodeclipping,audioinpainting,and\nguidancecombination. Eachtaskisrunonmultipledatasetsorwithmultipletargets(e.g.,different\nlabels,molecularproperties,styles).\nOthersettings. WeconsistentlysetthetimestepT = 100andtheDDIMparameterη = 1. We\nconsiderN = 1,N = 4anduseasinglesampleforImplicitDynamic(Line4)throughout\nrecur iter\nall experiments and methods for fair comparison. For TFG, the structures of ρ and µ are set to\n“increase”andthescalarsρ¯,µ¯,γ¯aredeterminedviaoursearchingstrategy. Wefollowthesettingin\noriginalpapersiftheyspecifytheirhyper-parameters. blueForspecifictricksinthecodethatarenot\nmentionedinpapers,wechoosetoalignwithoriginalpapers. Otherwise,valuesaredeterminedvia\nsearchingwith1/8ofthesamplesizeandamaximumsearchstepof6. Forfairnessofcomparison,\nweuseaccuracyasthemetricduringthesearchandcomparedifferentalgorithmsonthemetric,but\nwereportbothaccuracyandFID.\n5.2 Benchmarkingresults\nWecompareallsixmethodsinTable3. TFGoutperformsexistingalgorithmsin13over14settings,\nachievinganaverageguidancevalidityimprovementof7.4%comparedtothebestexistingalgorithm.\nNotice that we do not compare with the best algorithm in terms of generation fidelity because\nobtaininghighrealnesssamplesisnotourobjectiveintraining-freeguidance,andanunconditional\nmodelsufficestogeneratehighrealnesssamples(withextremelylowvalidity).Interestingly,different\nmethodsachievethesecondbestperformanceondifferenttasks,suggestingthevarianceofthese\nmethods,whileTFGisconsistentthankstotheunification.\nWe want to highlight that despite the superior performance of TFG, the key intention of our ex-\nperiments is not restrained to comparing TFG with existing methods, but more importantly to\nsystematicallybenchmarkunderthetraining-freeguidancesettingtoseehowmuchwehaveachieved\ninvarioustaskswithdifferentdifficulties. Belowwegothrougheachtaskseparatelyandconduct\nrelevantablationstudiestoprovideamorefine-grainedanalysis.\nFine-grainedlabelguidance. Inadditiontothestandardlabelguidance,weforthefirsttimestudy\ntheout-of-distributionfine-grainedlabelguidanceunderthetraining-freesetting,aproblemwhere\nnoexistingtraining-basedmethodsareavailable. Weconsiderthebird-speciesguidanceusingan\n6https://huggingface.co/teticio/audio-diffusion-256\n9\nEfficienNettrainedtoclassify525fine-grainedbirdspecies. Thisproblemremainshighlydifficult\nforleadingtext-to-imagegenerativemodelssuchasDALLE.Underrecurrence,TFGcangenerateat\nmost2.24%ofaccuratebirds,comparedwiththeunconditionalgenerationrateof0.\nRecurrenceonlabelguidance. Wegobacktothe Table 4: The accuracy / FID for TFG with\nfailure case we study in Section 3, i.e., the stan- differentrecurrencestepN onthreelabel\nrecur\ndardlabelguidanceproblemonCIFAR10wherethe guidancedatasets,averagedacrossalllabels.\ntraining-basedmethodoffersan85%accuracy,while\ntheaccuracyofTFGwithoutrecurrenceaccuracyis Recurrence 1 2 4\n52%only. AspresentedinTable4,increasingN recur CIFAR10 52.0/91.7 66.8/88.7 77.1/73.9\nsignificantlyclosesthegapfrom33%to8%. Similar ImageNet 40.9/177 52.3/163 59.8/165\nimprovementisobservedinotherdatasetsaswell. Fine-grained 1.27/256 1.66/259 2.24/259\nMultiple guidance and bias mitigation. We next\nTable 5: The accuracy of multi-label guid-\nconsider the scenario with multiple targets: control\nanceonCelebA, wherelabels0and 1cor-\nthegenerationofhumanfacesbasedongenderand\nrespond tofemale andmale (gender), non-\nhair color (or age) using two predictors. It is well\nblonde and blonde (hair color), and young\nknownthatthelabelimbalanceinCelebA-HQcauses\nandold(age). Theaccuracyislowerformi-\nclassifierstofocusonspuriouscorrelations[76],such\nnoritygroups,indicatinganimplicitbiasin\nasusinghaircolorstoclassifygender,abiasedfeature\nthegenerationprocess.Despitethis,itisstill\nweaimtoavoid. ThestratifiedperformanceofTFG\nmuchhigherthanunconditionalgeneration.\non“gender+age”and“gender+hair”guidanceare\npresented in Table 5. Despite the highly disparate Targetlabel 0+0 0+1 1+0 1+1\nperformance,training-freeguidancelargelyalleviates\ngender+hair 92.2 72.7 89.8 46.7\ntheimbalance:only1%ofimagesinCelebAare“male\ngender+age 92.9 73.6 93.6 69.1\n+blondehair”,whilethegeneratedaccuracyis46.7%.\nMoleculepropertyguidance. Toourknowledge,wearethefirsttostudytraining-freeguidance\nformoleculegeneration. WeinterestinglyfindinTable3thatTFGiseffectiveinguidingmolecules\ntowardsdesirableproperties,yieldingthehighestguidancevalidityon5outof6targetswith5.64%\nMAE improvement over existing methods, verifying the generality of our approach as a unified\nframeworkincompletelyunseendomains. Noticethat,unlikeimages,moleculeswithbettervalidity\nusuallyhavelowergenerationfidelity,afindingreflectedinpreviouswork[3].\nAudioGuidance. Weextendourinvestigationtotheaudiomodality,whereTFGachievessignificant\nrelative improvements over existing methods. Given that the audio domain is rarely explored in\ntraining-freeguidanceliterature,ourbenchmarkswillcontributetofutureresearchinthisarea.\n6 DiscussionsandLimitations\nRecently,training-freeguidancefordiffusionmodelshasgainedincreasingattentionandhasbeen\nadoptedinvariousapplications. TFGisbasedonanextensiveliteraturereviewovertenalgorithmic\npapersfordifferentpurposes,includingimages,audio,molecules,andmotions[34,8,43,32,19,13,\n16,15,45,38,68].Whileweincorporateseveralkeyalgorithmsintoourframework,weacknowledge\nthatencompassingallapproachesisimpossible,asitwouldmaketheunificationbloatedandless\npractical. Weseektofindabalancepointbyunifyingmostrepresentativealgorithmswhilekeeping\nthetechniquesclearandeasilystudied.\nAnoftendiscussedproblemiswhywecareabouttraining-freeguidance,giventheever-growing\ncommunityoflanguage-basedgenerativemodelssuchastheimagegeneratorofGPT4. Inpractice,\ntherearecountlessconditionalgenerationtaskswheretheconditionsarehardtoaccuratelyconvey\nto or represent by language encoders. For instance, it can fail to under a complex property of a\nmoleculeorgenerateCelebA-stylefaces. WegiveanillustrativeanalysisinAppendixA.1. Despite\nthattraining-freeguidanceisimportant,thispaperdoesnotsystematicallyanalyzewhattypesof\nconditional generation are, in general, more suitable for the framework and what types are for\nlanguage-based models. That said, training-free guidance is fundamentally difficult due to the\nmisalignment between the training objective of target predictors and the diffusion, with a more\ndetaileddiscussioninAppendixA.2.Thispaperdoesnotcomprehensivelyanalyzethismisalignment,\nand the gap between training-based and TFG remains high in some tasks like molecule property\nguidance. Wehopethatfutureworkscananalyticallydiveintotheseproblems.\n10\nReferences\n[1] JoshAbramson,JonasAdler,JackDunger,RichardEvans,TimGreen,AlexanderPritzel,Olaf\nRonneberger,LindsayWillmore,AndrewJBallard,JoshuaBambrick,etal. Accuratestructure\npredictionofbiomolecularinteractionswithalphafold3. Nature,pages1–3,2024.\n[2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,\nJonasGeiping,andTomGoldstein. Universalguidancefordiffusionmodels. 2023IEEE/CVF\nConferenceonComputerVisionandPatternRecognitionWorkshops(CVPRW),pages843–852,\n2023.\n[3] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant\nenergy-guidedSDEforinversemoleculardesign. InTheEleventhInternationalConferenceon\nLearningRepresentations,2023.\n[4] GeorgiosBatzolis,JanStanczuk,Carola-BibianeSchönlieb,andChristianEtmann. Conditional\nimagegenerationwithscore-baseddiffusionmodels. arXivpreprintarXiv:2111.13606,2021.\n[5] MikołajBin´kowski,DanicaJSutherland,MichaelArbel,andArthurGretton. Demystifying\nmmdgans. arXivpreprintarXiv:1801.01401,2018.\n[6] HyungjinChungandJongChulYe. Score-baseddiffusionmodelsforacceleratedmri. Medical\nimageanalysis,80:102479,2022.\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvancesinneuralinformationprocessingsystems,34:8780–8794,2021.\n[8] KieranDidi,FranciscoVargas,SimonVMathis,VincentDutordoir,EmileMathieu,UrszulaJ\nKomorowska,andPietroLio.Aframeworkforconditionaldiffusionmodellingwithapplications\ninmotifscaffoldingforproteindesign,2024.\n[9] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,\nThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.\nAnimageisworth16x16words: Transformersforimagerecognitionatscale. arXivpreprint\narXiv:2010.11929,2020.\n[10] YilunDu,ConorDurkan,RobinStrudel,JoshuaBTenenbaum,SanderDieleman,RobFergus,\nJaschaSohl-Dickstein,ArnaudDoucet,andWillSussmanGrathwohl. Reduce,reuse,recycle:\nCompositional generation with energy-based diffusion models and mcmc. In International\nconferenceonmachinelearning,pages8489–8510.PMLR,2023.\n[11] Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical\nAssociation,106(496):1602–1614,2011.\n[12] JeremyElson,John(JD)Douceur,JonHowell,andJaredSaul. Asirra: Acaptchathatexploits\ninterest-aligned manual image categorization. In Proceedings of 14th ACM Conference on\nComputerandCommunicationsSecurity(CCS).AssociationforComputingMachinery,Inc.,\nOctober2007.\n[13] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with\ndifferentiablemotionestimators. arXivpreprintarXiv:2401.18085,2024.\n[14] AlexandrosGraikos,SrikarYellapragada,andDimitrisSamaras. Conditionalgenerationfrom\nunconditionaldiffusionmodelsusingdenoiserrepresentations.arXivpreprintarXiv:2306.01900,\n2023.\n[15] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, and Josh Susskind.\nControl3diff: Learningcontrollable3ddiffusionmodelsfromsingle-viewimages. In2024\nInternationalConferenceon3DVision(3DV),pages685–696.IEEE,2024.\n[16] XuHan,CaihuaShan,YifeiShen,CanXu,HanYang,XiangLi,andDongshengLi. Training-\nfreemulti-objectivediffusionmodelfor3dmoleculegeneration. InTheTwelfthInternational\nConferenceonLearningRepresentations,2023.\n11\n[17] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage\nrecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,\npages770–778,2016.\n[18] YutongHe,NaokiMurata,Chieh-HsinLai,YuhtaTakida,ToshimitsuUesaka,DongjunKim,\nWei-HsiangLiao, YukiMitsufuji, JZicoKolter, RuslanSalakhutdinov, andStefanoErmon.\nManifoldpreservingguideddiffusion. InTheTwelfthInternationalConferenceonLearning\nRepresentations,2024.\n[19] CarlosHernandez-Olivan,KoichiSaito,NaokiMurata,Chieh-HsinLai,MarcoAMartínez-\nRamirez,Wei-HsiangLiao,andYukiMitsufuji.Vrdmg:Vocalrestorationviadiffusionposterior\nsamplingwithmultipleguidance. InICASSP2024-2024IEEEInternationalConferenceon\nAcoustics,SpeechandSignalProcessing(ICASSP),pages596–600.IEEE,2024.\n[20] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.\nGanstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. Advancesin\nneuralinformationprocessingsystems,30,2017.\n[21] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances\ninneuralinformationprocessingsystems,33:6840–6851,2020.\n[22] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTim\nSalimans. Cascadeddiffusionmodelsforhighfidelityimagegeneration. JournalofMachine\nLearningResearch,23(47):1–33,2022.\n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598,2022.\n[24] EmielHoogeboom,VıctorGarciaSatorras,ClémentVignac,andMaxWelling. Equivariant\ndiffusionformoleculegenerationin3d. InInternationalconferenceonmachinelearning,pages\n8867–8887.PMLR,2022.\n[25] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptuallossesforreal-timestyletransfer\nandsuper-resolution. InComputerVision–ECCV2016: 14thEuropeanConference,Amsterdam,\nTheNetherlands,October11-14,2016,Proceedings,PartII14,pages694–711.Springer,2016.\n[26] TeroKarras,TimoAila,SamuliLaine,andJaakkoLehtinen. Progressivegrowingofgansfor\nimprovedquality,stability,andvariation. arXivpreprintarXiv:1710.10196,2017.\n[27] BahjatKawar,MichaelElad,StefanoErmon,andJiamingSong. Denoisingdiffusionrestoration\nmodels. AdvancesinNeuralInformationProcessingSystems,35:23593–23606,2022.\n[28] KevinKilgour,MauricioZuluaga,DominikRoblek,andMatthewSharifi. Fr\\’echetaudiodis-\ntance:Ametricforevaluatingmusicenhancementalgorithms.arXivpreprintarXiv:1812.08466,\n2018.\n[29] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. Diffwave: Aversatile\ndiffusionmodelforaudiosynthesis. arXivpreprintarXiv:2009.09761,2020.\n[30] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages.\n2009.\n[31] YannLeCun,SumitChopra,RaiaHadsell,MRanzato,andFujieHuang. Atutorialonenergy-\nbasedlearning. Predictingstructureddata,1(0),2006.\n[32] Jean-MarieLemercier,JuliusRichter,SimonWelker,EloiMoliner,VesaVälimäki,andTimo\nGerkmann. Diffusionmodelsforaudiorestoration. arXivpreprintarXiv:2402.09821,2024.\n[33] AnatLevin,YairWeiss,FredoDurand,andWilliamTFreeman. Understandingandevaluating\nblind deconvolution algorithms. In 2009 IEEE conference on computer vision and pattern\nrecognition,pages1964–1971.IEEE,2009.\n[34] MarkLevy,BrunoDiGiorgi,FlorisWeers,AngelosKatharopoulos,andTomNickson. Con-\ntrollable music production with diffusion models and guidance gradients. arXiv preprint\narXiv:2311.00613,2023.\n12\n[35] HaoheLiu,ZehuaChen,YiYuan,XinhaoMei,XuboLiu,DaniloMandic,WenwuWang,and\nMarkDPlumbley. Audioldm: Text-to-audiogenerationwithlatentdiffusionmodels. arXiv\npreprintarXiv:2301.12503,2023.\n[36] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining\nXie. Aconvnetforthe2020s,2022.\n[37] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes\n(celeba)dataset. RetrievedAugust,15(2018):11,2018.\n[38] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive\nenergypredictionforexactenergy-guideddiffusionsamplinginofflinereinforcementlearning.\nInInternationalConferenceonMachineLearning,pages22825–22855.PMLR,2023.\n[39] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVanGool. Repaint: Inpaintingusingdenoisingdiffusionprobabilisticmodels. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n11461–11471,June2022.\n[40] ShitongLuoandWeiHu. Diffusionprobabilisticmodelsfor3dpointcloudgeneration. In\nProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n2837–2845,2021.\n[41] ZhaoyangLyu,ZhifengKong,XudongXu,LiangPan,andDahuaLin. Aconditionalpoint\ndiffusion-refinementparadigmfor3dpointcloudcompletion. arXivpreprintarXiv:2112.03530,\n2021.\n[42] RossAMaller,GernotMüller,andAlexSzimayer. Ornstein–uhlenbeckprocessesandexten-\nsions. Handbookoffinancialtimeseries,pages421–437,2009.\n[43] EloiMoliner, JaakkoLehtinen, andVesaVälimäki. Solvingaudioinverseproblemswitha\ndiffusionmodel. InICASSP2023-2023IEEEInternationalConferenceonAcoustics,Speech\nandSignalProcessing(ICASSP),pages1–5.IEEE,2023.\n[44] MeinardMüller. Dynamictimewarping. Informationretrievalformusicandmotion,pages\n69–84,2007.\n[45] NithinGopalakrishnanNair,AnoopCherian,SuhasLohit,YeWang,ToshiakiKoike-Akino,\nVishalMPatel,andTimKMarks. Steereddiffusion: Ageneralizedframeworkforplug-and-\nplayconditionalimagesynthesis. InProceedingsoftheIEEE/CVFInternationalConferenceon\nComputerVision,pages20850–20860,2023.\n[46] KamalNasrollahiandThomasBMoeslund.Super-resolution:acomprehensivesurvey.Machine\nvisionandapplications,25:1423–1468,2014.\n[47] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,\nIlyaSutskever,andMarkChen. Glide: Towardsphotorealisticimagegenerationandediting\nwithtext-guideddiffusionmodels. arXivpreprintarXiv:2112.10741,2021.\n[48] AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilistic\nmodels. InInternationalconferenceonmachinelearning,pages8162–8171.PMLR,2021.\n[49] AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,ZacharyDeVito,\nZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer. Automaticdifferentiationin\npytorch. 2017.\n[50] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld.\nQuantumchemistrystructuresandpropertiesof134kilomolecules. ScientificData,1,2014.\n[51] HerbertERobbins. Anempiricalbayesapproachtostatistics. InBreakthroughsinStatistics:\nFoundationsandbasictheory,pages388–394.Springer,1992.\n[52] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.\n13\n[53] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConferenceonComputerVisionandPatternRecognition(CVPR),pages10684–10695,June\n2022.\n[54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In Medical image computing and computer-assisted\nintervention–MICCAI2015: 18thinternationalconference,Munich,Germany,October5-9,\n2015,proceedings,partIII18,pages234–241.Springer,2015.\n[55] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,Zhiheng\nHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.\nImageNetLargeScaleVisualRecognitionChallenge. InternationalJournalofComputerVision\n(IJCV),115(3):211–252,2015.\n[56] BabakSalehandAhmedElgammal. Large-scaleclassificationoffine-artpaintings: Learning\ntherightmetricontherightfeature. arXivpreprintarXiv:1505.00855,2015.\n[57] VictorGarciaSatorras,EmielHoogeboom,andMaxWelling. E(n)equivariantgraphneural\nnetworks. arXivpreprintarXiv:2102.09844,2021.\n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-\n5b: Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels. Advancesin\nNeuralInformationProcessingSystems,35:25278–25294,2022.\n[59] AlexanderShapiro. Montecarlosamplingmethods. Handbooksinoperationsresearchand\nmanagementscience,10:353–425,2003.\n[60] Herbert A Simon. Spurious correlation: A causal interpretation. Journal of the American\nstatisticalAssociation,49(267):467–479,1954.\n[61] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsuper-\nvisedlearningusingnonequilibriumthermodynamics. InInternationalconferenceonmachine\nlearning,pages2256–2265.PMLR,2015.\n[62] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXiv\npreprintarXiv:2010.02502,2020.\n[63] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz,\nYongxinChen,andArashVahdat. Loss-guideddiffusionmodelsforplug-and-playcontrollable\ngeneration. InInternationalConferenceonMachineLearning,pages32483–32498.PMLR,\n2023.\n[64] YangSong,ConorDurkan,IainMurray,andStefanoErmon. Maximumlikelihoodtrainingof\nscore-baseddiffusionmodels. Advancesinneuralinformationprocessingsystems,34:1415–\n1428,2021.\n[65] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. Advancesinneuralinformationprocessingsystems,32,2019.\n[66] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and\nBenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXiv\npreprintarXiv:2011.13456,2020.\n[67] CharlesStein. Aboundfortheerrorinthenormalapproximationtothedistributionofasumof\ndependentrandomvariables. InProceedingsoftheSixthBerkeleySymposiumonMathematical\nStatisticsandProbability,Volume2: ProbabilityTheory,volume6,pages583–603.University\nofCaliforniaPress,1972.\n[68] HaoyuanSun,BoXia,YongzheChang,andXueqianWang. Generalizingalignmentparadigm\noftext-to-imagegenerationwithpreferencesthroughf-divergenceminimization.arXivpreprint\narXiv:2409.09774,2024.\n14\n[69] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,and\nHervéJégou. Trainingdata-efficientimagetransformers&distillationthroughattention,2021.\n[70] YinhuaiWang,JiwenYu,andJianZhang. Zero-shotimagerestorationusingdenoisingdiffusion\nnull-space model. In The Eleventh International Conference on Learning Representations,\n2023.\n[71] ZhihaoWang,JianChen,andStevenCHHoi. Deeplearningforimagesuper-resolution: A\nsurvey. IEEEtransactionsonpatternanalysisandmachineintelligence,43(10):3365–3387,\n2020.\n[72] JosephLWatson,DavidJuergens,NathanielRBennett,BrianLTrippe,JasonYim,HelenE\nEisenach,WoodyAhern,AndrewJBorst,RobertJRagotte,LukasFMilles,etal. Denovo\ndesignofproteinstructureandfunctionwithrfdiffusion. Nature,620(7976):1089–1100,2023.\n[73] MinkaiXu,AlexanderSPowers,RonODror,StefanoErmon,andJureLeskovec. Geometric\nlatentdiffusionmodelsfor3dmoleculegeneration. InInternationalConferenceonMachine\nLearning,pages38592–38610.PMLR,2023.\n[74] MinkaiXu,LantaoYu,YangSong,ChenceShi,StefanoErmon,andJianTang. Geodiff: Ageo-\nmetricdiffusionmodelformolecularconformationgeneration.arXivpreprintarXiv:2203.02923,\n2022.\n[75] JingkangYang,PengyunWang,DejianZou,ZitangZhou,KunyuanDing,WenxuanPeng,Haoqi\nWang,GuangyaoChen,BoLi,YiyouSun,XuefengDu,KaiyangZhou,WayneZhang,Dan\nHendrycks,YixuanLi,andZiweiLiu. Openood: Benchmarkinggeneralizedout-of-distribution\ndetection. 2022.\n[76] HaotianYe,JamesZou,andLinjunZhang. Freezethentrain: Towardsprovablerepresentation\nlearningunderspuriouscorrelationsandfeaturenoise. InInternationalConferenceonArtificial\nIntelligenceandStatistics,pages8968–8990.PMLR,2023.\n[77] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, AlexanderKu, YinfeiYang, BurcuKaragolAyan, etal. Scalingautoregressive\nmodelsforcontent-richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789, 2(3):5,\n2022.\n[78] JiwenYu,YinhuaiWang,ChenZhao,BernardGhanem,andJianZhang. Freedom: Training-\nfreeenergy-guidedconditionaldiffusionmodel. ProceedingsoftheIEEE/CVFInternational\nConferenceonComputerVision(ICCV),2023.\n[79] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based r-cnns for fine-\ngrained category detection. In Computer Vision–ECCV 2014: 13th European Conference,\nZurich,Switzerland,September6-12,2014,Proceedings,PartI13,pages834–849.Springer,\n2014.\n[80] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconferenceoncomputervisionandpatternrecognition,pages586–595,2018.\n15\nA TheMotivationofStudyingTraining-freeGuidance\nInthissection,wearguethattraining-freeguidanceusingoff-the-shelfmodelsisacrucialandtimely\nresearchproblemdeservingmoreattentionandeffort. Webeginbyprovidinganillustrativeanalysis\nthathighlightsthelimitationsofcurrentstrongtext-to-imagegenerativemodels,underscoringthe\nnecessityfortraining-freeguidance. Furthermore,weassertthattraining-freeguidanceremainsa\nsignificantchallenge,withpreviousliteratureunderestimatingitscomplexity. Givenitsnecessityand\ninherentdifficulties,wecallforincreasedfocusfromtheresearchcommunityonthisproblemand\nofferourbenchmarksandcodebasetohelpaccelerateprogressinthisarea.\nA.1 FailurecaseofimagegenerationwithGPT4\nFigure 4: Prompting GPT4 to generate property guided molecules. It is hard for the image\ngenerator to understand the target and generate faithful samples. In this dialog, GPT4 claims to\ngenerateabenzenemoleculebutthesampleisapparentlynotabenzene. Therearealsomanyinvalid\ncarbonatomswithmorethan4bondsandthepolarizabilitytargetisnotachieved.\nIt’shardforGPT4tounderstandtargets. InFigure4,weaskGPT-4togenerateamoleculewith\npolarizabilityα = 3,whichisataskweusetoevaluatetraining-freeguidance(refertoFigure16\nforvisualization). WefoundthattheGPT-4generatedmoleculeisapparentlyinvalidandunrealistic:\nthegeneratedmoleculecontainsmanycarbonatomswithmorethan4bonds(themaximumallowed\nnumberis4);andthegeneratedmoleculeisapparentlynotabenzenewhichisclaimedbythetext\noutputs. Fromthiscasewemayunderstandthatitishardtofollowdiverseuser-definedinstructions\nforthefoundationalgenerativemodels,wheretheuser-definedtargetsmaybesubtle,fine-grained,\ncombinatorial,andopen-ended.\n16\nTothisend,training-freeguidanceofferstwokeyadvantages: (1)Itallowsforgreaterspecificityin\ntargetrequirementsbyenablingtheuseofadifferentiableobjectivefunction,makingthegeneration\nprocessmoresteerable;(2)Theobjectivefunctionisplug-and-play,facilitatingtheadditionofnew\ntargetsandtaskstoapre-trainedgenerativemodel. Sincethereisnoneedtoretrainthediffusionor\npredictionmodels,thisapproachmakesthegenerativeprocesslightweightandapplicabletovarious\ndownstreamtasks.\nIt’shardforGPT4tocapturethetargeteddistribution. Anotherimportantmetritfortraining-\nfreeguidanceistheflexibilityofchoosingdiffusionmodels. Forthesametarget,wecanswitchfrom\ndifferentdiffusionmodelstochangetheunconditionalbackgrounddistribution. Forexample,itis\nhardforGPT4togenerateCelebA-likesamplesthoughit“knowsCelebAdatasetverywell”:\nFigure5: PromptingGPT4togenerateCelebA-likeimages. WefirstpromptChatGPTtoprobe\nitsknowledgeofCelebAdatasetandthenaskittogenerateayoungmanfigureinCelebAstyle.\nHowever,thegeneratedfigureisapprentlynotinthedistributionofCelebA(refertoFigure12)for\ncomparison.\nTheflexibilitytousedifferentdiffusionmodelsprovidesanopportunitytogenerateawiderrange\nofuser-definedtargets. Withtraining-freeguidance,individualscanselecttheirpreferreddiffusion\nmodeltoestablishthebackgrounddistributionandusethepredictionmodeltosteerthegeneration\ntowardsspecificproperties. Thisapproachmayrepresentafuturedirectionforhuman-AIinteraction.\n17\nFigure6:(Left)TheaccuracyandFIDofdifferentmethodsunderdifferentsettingsonCIFAR10[30],\naverage across ten labels and 2048 samples per label. The suffix number in UGD and FreeDoM\nrepresentsrecurrentstepN ,and(fake)standsforasyntheticsettingwhereweapplyatraining-\nrecur\nbasedclassifierbutsett=0andusethesametraining-freeguidancemethods. Ahugeperformance\ngap between different settings suggests the intrinsic difficulty of training-free guidance. (Right)\nIllustrationofgenerated“ship”usingMPGDunderdifferentsettings(top)andthesamplingtrajectory\nofthepredictedcleanimagex (down).\n0|t\nA.2 Thefundamentalchallengeoftraining-freeguidance\nDespitethearrayofalgorithmsavailableandtheirreportedsuccessesinvariousapplications,we\nconductacasestudyonCIFAR10[30]toillustratethechallengingnatureoftraining-freeguidance\nandtheinsufficiencyofexistingmethods. Specifically,wecomparethetraining-basedapproachand\ntraining-freeapproachforthelabelguidancetaskonCIFAR10,withthediffusionmodelpretrained\nby [7], the training-based time-dependent classifier f(x,t) by [7], and the training-free standard\nlabelclassifierf(x)pretrainedonlyoncleanCIFAR10by[9]. anda“fake”training-freeclassifier\ndefinedasf(x,t)| . Thefirstservesastheoraclebenchmark,whilethesecondcorrespondsto\nt=0\nthe standard training-free guidance. The third setting, as considered in LGD [63], uses a “fake”\ntraining-freeclassifiersinceitsparametersaresharedacrossdifferenttimestepstduringtraining,\nresultinginanimplicitregularizationthatisnotavailableforpracticalpredictors. Thissettingserves\nasacomparisontohelpidentifythedifficultyoftraining-freeguidance.\nQuantitativeandqualitativeresultsareshowninFigure6. Alltraining-freeapproachessignificantly\nunder-performtraining-basedguidance,withasignificantportionofgeneratedimagesbeinghighly\nunnatural(whenguidanceisstrong)orirrelevanttothelabel(whenguidanceisweak). Amoreclear\nillustrationonthiscanbefoundinFigure7. Intermsof“fake”classifiers,itleadstoaremarkable\ndifferencefromrealtraining-freeclassifiersevenunderidenticalexperimentalsettings. Itgenerates\nFigure7: IllustrationonCIFAR10dogsgeneratedwithdifferentalgorithms. Comparedwithtraining-\nbasedmethod,training-freemethodsfallbehindbutTFGsignificantlyoutperformsexistingmethods.\n18\nmuchlessmessyimagesduetotheimplicitregularizationfromthetrainingprocess,wherenoisy\nimages are also “seen” (although t is fixed to 0 upon guidance). Unfortunately, such types of\npredictors are inaccessible in practice (otherwise, we can use classifier-based guidance directly).\nFrom the comparison, we know that the key challenge of training-free guidance is the lack of a\n“smoothing”classifierthatcanproducefaithfulguidanceinthe“unseen”noisyimagespace.\nTheobservationlargelyuncoverstheessentialdifficultiesoftraining-freeguidance,andmotivatesus\ntosystematicallystudytechniquesthatcanimprovegenerationquality. Unfortunately,comparisons\nbetweenexistingtechniquesareambiguoussincedifferentmethodsaretestedondistinctandprimarily\nqualitativeapplications,whichinturnhindersthein-depthstudyinthisfield. Tothisend,weresolve\ntorevisitthiscomplicatedscenarioanddesignaclearandcomprehensiveframeworkfortraining-free\nguidance.\n19\nB Pseudo-codeandschematics\nWehavepresentedthepseudo-codeofTFGinAlgorithm1. Below,weprovideacopyoftheDPS\n(Algorithm 2), MPGD (Algorithm 3), FreeDoM (Algorithm 4), UGD (Algorithm 5), and LGD\n(Algorithm6). NoticethatLGDdoesnotprovideapseudo-code,andwepresenttheiralgorithm\nfollowingtheirpaperasamodificationofDPS.Wedonotchangetheoriginalalgorithms’notations\nfor reference. Please see the proof in Appendix C for the equivalence analysis. We provide a\nschematicofexistingalgorithmsinFigure8.\nAlgorithm2DPS-Gaussian\nRequire: N,y,{ζ }N ,{σ˜ }N\ni i=1 i i=1\nx ∼N(0,I)\nN\nfori=N −1to0do\nsˆ←s (x ,i)\nθ i\nxˆ\n0\n← √1 α¯i(x i+(1−α¯ i)sˆ)\nz∼N(0,I)\n√\nx′ ← √ αi(1−α¯i−1)x + α¯i−1βixˆ +σ˜ z\ni−1 1−α¯i i 1−α¯i 0 i\nx ←x′ −ζ ∇ ∥y−A(xˆ )∥2\ni−1 i−1 i xi 0 2\nendfor\nreturnxˆ\n0\nAlgorithm3MPGDforpixeldiffusionmodels\n1: x ∼N(0,I)\nT\n2: fort=T,...,1do\n3: ϵ t ∼N(0,I) √\n4: x\n0|t\n= √1 α¯t(x t− 1−α¯ tϵ θ(x t,t))\n5: ifrequiresmanifoldprojectionthen\n6: x =g (x ,L(x ;y),c )\n0|t M 0|t 0|t t\n7: else\n8: x =x −c ∇ L(x ;y)\n0|t 0|t t x0|t 0|t\n9: endif\n√\n10: x = α¯ x\nt−1 t−1 0|t\n(cid:112)\n11: + 1−α¯ −σ2ϵ (x ,t)+σ ϵ\nt−1 t θ t t t\n12: endfor\n13: returnx\n0\nAlgorithm4FreeDoM+EfficientTime-TravelStrategy\nRequire: conditionc,unconditionalscoreestimators(·,t),time-independentdistancemeasuringfunctionDθ(c,·),pre-definedparameters\nβt,α¯t,learningrateρt,andtherepeattimesoftimetravelofeachstep{r1,···,rT}.\nxT ∼N(0,I)\nfort=T,...,1do\nfori=rt,...,1do\nϵ1∼N(0,I)ift>1,elseϵ1=0. √\nxt−1=(1+ 1 2βt)xt+βts(xt,t)+ βtϵ1\nx0|t= √1 α¯t(xt+(1−α¯t)s(xt,t))\ngt=∇xtDθ(c,x0|t(xt)))\nxt−1=xt−1−ρtgt\nifi>1then\nϵ2∼N√(0,I) √\nxt= 1−βtxt−1+ βtϵ2\nendif\nendfor\nendfor\nreturnx0\n20\nAlgorithm5UniversalGuidance(itsαisourα¯)\nParameter:Recurrentstepsk,gradientstepsmforbackwardguidanceandguidancestrengths(t),\nRequired:z sampledfromN(0,I),diffusionmodelϵ ,noisescales{α }T ,guidancefunctionf,loss\nT θ t t=1\nfunctionℓ,andpromptc\nfort=T,T −1,...,1do\nforn=1,2,...,kdo\n√\nCalculatezˆ 0as zt−( 1− √α αt t)ϵθ(zt,t)\nϵˆ (z ,t)=ϵ (z ,t)+s(t)·∇ ℓ(c,f(zˆ ))\nθ t θ t zt 0\nifm>0then\nCalculate∆z byminimizingℓ(c,f(zˆ +∆)).withmstepsofgradientdescent\n0 0\n(cid:112)\nPerformbackwarduniversalguidancebyϵˆ ←ϵˆ − α /(1−α )∆z\nθ θ t t 0\nendif\nz ←S(z ,ϵˆ ,t)\nt−1 t θ\nϵ′ ∼N(0,I)\nz ←(cid:112) α /α z +(cid:112) 1−α /α ϵ′\nt t t−1 t−1 t t−1\nendfor\nendfor\nAlgorithm6LGD(fromDPS)\nRequire: N,y,{ζ }N ,{σ˜ }N ,n\ni i=1 i i=1\nx ∼N(0,I)\nN\nfori=N −1to0do\nsˆ←s (x ,i)\nθ i\nxˆ\n0\n← √1 α¯i(x i+(1−α¯ i)sˆ)\nz∼N(0,I)\n√\nx ← √ αi(1−α¯i−1)x + α¯i−1βixˆ +σ˜ z\nxi−1 ←x 1− −α¯ ζi ∇ li og(cid:16) 11− (cid:80)α¯ ni e0 xp(−i ℓ (x(j))(cid:17) ▷x(j)sampledi.i.d.fromN(xˆ , σi2 I)\ni−1 i−1 i xi n j=1 y i i 0 1+σ2\ni\nendfor\nreturnxˆ\n0\n(a) (cid:31) (cid:29)(cid:29)(cid:29) (cid:31) (cid:31) (cid:29)(cid:29)(cid:29) (cid:31)\nDiffusion (cid:30) (cid:24) (cid:24)(cid:26)(cid:25) (cid:28)\nsampling\nprocess (cid:31) (cid:31) (cid:31) (cid:31) (cid:31)\n(cid:28)(cid:27)(cid:30)(cid:26)(cid:25) (cid:28)(cid:27)(cid:24)(cid:23)(cid:25) (cid:28)(cid:27)(cid:24) (cid:28)(cid:27)(cid:24)(cid:26)(cid:25) (cid:28)(cid:27)(cid:25)\nthe (cid:24)-th step\n(b) (cid:31) (cid:31) (cid:31)∇(cid:31)(cid:24)(cid:22)logf((cid:31) (cid:28)(cid:27)(cid:24)) (cid:31) (cid:31) (cid:31) (cid:31)∇(cid:31) (cid:24)(cid:22)logΣ(cid:21)(cid:22)f((cid:31)(cid:31)(cid:21)(cid:29)) (cid:31)\nDPS (cid:24) (cid:24)(cid:26)(cid:25) (cid:24)(cid:26)(cid:25) LGD (cid:24) (cid:24)(cid:26)(cid:25) (cid:24)(cid:26)(cid:25)\n(cid:31)(cid:31)(cid:30)(cid:29)logf((cid:31)(cid:31)(cid:30)(cid:29))\nFreeDoM (cid:31) (cid:24) (cid:31) (cid:24)(cid:26)(cid:25)\n(cid:31)∇(cid:31)(cid:24)(cid:22)logf((cid:31) (cid:28)(cid:27)(cid:24))\n(cid:31) (cid:24)(cid:26)(cid:25) (cid:31) (cid:28)(cid:27)(cid:24) (cid:31)(cid:31)(cid:28)(cid:29) logf((cid:31)(cid:31)(cid:28)(cid:29))\n(cid:31)(cid:31)(cid:27)(cid:29) logf((cid:31)(cid:31)(cid:27)(cid:29))\niterate\nMPGD (cid:31) (cid:24) (cid:31) (cid:24)(cid:26)(cid:25) UGD (cid:31) (cid:24) (cid:31) (cid:24)(cid:26)(cid:25) (cid:31)∇(cid:31)(cid:24)(cid:22)logf((cid:31) (cid:28)(cid:27)(cid:24)) (cid:31) (cid:24)(cid:26)(cid:25)\n(cid:31) (cid:31)∇(cid:31)(cid:28)(cid:27)(cid:24)(cid:22)(cid:31) (cid:31)∇(cid:31)(cid:28)(cid:27)(cid:24)(cid:22)(cid:31)\n(cid:31)\n(cid:31)∇(cid:31) (cid:28)(cid:27)(cid:24)(cid:22)logf((cid:31) (cid:28)(cid:27)(cid:24))\n(cid:31)\n(cid:28)(cid:27)(cid:24) (cid:28)(cid:27)(cid:24) (cid:28)(cid:27)(cid:24)\n(cid:28)(cid:27)(cid:24) (cid:28)(cid:27)(cid:24) iterate\nFigure 8: (a) The reversed diffusion process. (b) Illustration of different training-free guidance\nalgorithmsatthet-threverseddiffusionstep.\n21\nC Proofs\nWeproveTheorem3.2andLemma3.3below.\nC.1 ProofofTheorem3.2\nProof. Foreachalgorithm,weprovetheequivalenceofdesignspaceseparatelybelow. Noticethat\nwhenγ¯ =0,f˜degradesbacktof.\nMPGD(Algorithm3). Belowwedemonstratethatanyhyper-parameter{c }T inAlgorithm3is\nt t=1\nequivalenttotheTFGwithf(x )=exp{−L(x ;y)}andhyper-parameter\n0|t 0|t\nN =1,N =1,γ¯ =0,ρ=0,µ=(c ,··· ,c )⊤.\nrecur iter 1 T\nToshowthis,noticethatsincebothρandγ¯arezero,Line4andLine7takenoeffect. Whenusing\ntheidenticalsamplingfunction(Line9),TFGgeneratesx using\nt−1\n√\nx =Sample(x ,x ,t)+c α¯ ∇ logf(x )\nt−1 t 0|t t t−1 x0|t\n√\n0|t\n√ (cid:113) x − α¯ x √\n= α¯ x + 1−α¯ −σ2 t √ t 0|t +σ ϵ +c α¯ ∇ logf(x )\nt−1 0|t t−1 t 1−α¯ t t t t−1 x0|t 0|t\nt\n√ (cid:113)\n= α¯ (x −c ∇ L(x ;y))+ 1−α¯ −σ2ϵ (x ,t)+σ ϵ ,\nt−1 0|t t x0|t 0|t t−1 t θ t t t\nwhichisexactlytheformulausedinMPGD.\nDPS(Algorithm2)andFreeDoM(Algorithm4). WeprovebothalgorithmstogetherasDPSis\naspecialcaseofFreeDoM(withoutrecurrence). Specifically,anyhyper-parameter{ρ }T ,time\nt t=1\ntravel step r and distance function D(c,·) in Algorithm 4 is equivalent to TFG with f(x ) =\n0|t\nexp{−D(c,x )}andhyper-parameter\n0|t\n√ √\nN =r,N =0,γ¯ =0,ρ=( α ρ ,··· , α ρ )⊤,µ=0.\nrecur iter 1 1 T T\nToshowthis,noticethatbothalgorithmshavetheidenticalresamplingstepfromx tox ,soit\nt−1 t\nsufficestoprovethattheformulatogeneratex ineachrecurrentstepisthesame. ForFreeDoM,\nt−1\nwehave\nx =Sample(x ,x ,t)−ρ ∇ D(c,x )\nt−1 t 0|t √t xt 0|t\n√\n=Sample(x ,x ,t)−( α ρ )∇ logf(x )/ α ,\nt 0|t t t xt 0|t t\nandthelastlineequalsthecombinationofLine7andLine7inTFG.\nLGD(Algorithm6). Anyhyper-parameter{ζ }T ,ninLGDisequivalenttoTFGwithf(x)=\nt t=1\nexp{−ℓ (x)},samplesizeninLine4,andhyper-parameter\ny\nN =1,N =0,γ¯ =1,ρ=0,µ=(ζ ,··· ,ζ )⊤.\nrecur iter 1 T\nNoticethatα¯ inTFGequals1/(1+σ2)intheDPSalgorithm. Withthis,theequivalenceisclear\nt t\nfromthepseudo-codeofbothalgorithms.\nUGD(Algorithm5). Anyhyper-parameterk,m,s(t)inUGDisequivalenttoTFGwithf(x)=\nexp{−ℓ(c,f(x))}andhyper-parameter\nN =k,N =m,γ¯ =0,\nrecur iter\n√ √ (cid:114) α (cid:114) α\nρ=(− α s(1)δ ,··· ,− α s(T)δ )⊤,µ=(− 1 δ ,··· ,− T δ )⊤,\n1 1 T T 1−α¯ 1 1−α¯ T\n1 T\nwhereδ isthecoefficientofϵ (x ,t)insamplerSintheUGDalgorithm(whichisnegative).Toshow\nt θ t √\nthis,noticethat∆ =ρ ∇ logf˜(x )=−ρ ∇ ℓ(c,f(x ))= α s(t)δ ∇ ℓ(c,f(x )). By\nt t xt 0|t t xt 0|t t t xt 0|t\nreplacingthisintoLine9,theequivalenceofguidanceVarianceGuidancecanbeeasilyobserved. A\nsimilardeductioncanbemadeforthemeanguidanceaswell.\n22\nC.2 ProofofLemma3.3\n√\nProof. Recallthatx = xt− 1− √α¯tϵθ(xt,t). Accordingtoasimplechainrule,wehave\n0|t α¯t\n∆ =ρ ∇ logf˜(x ) (9)\nt t xt√ 0|t\nI− 1−α¯ ∇ ϵ (x ,t)\n=ρ √t xt θ t ∇ logf˜(x ). (10)\nt α¯ x0|t 0|t\nt\nTheperfectoptimizationassumptionimpliestherelationshipbetweenϵ andthescoreofp (x ),\nθ t t\nwhichweleverageandobtain\n√\nI− 1−α¯ ∇ ϵ (x ,t)\n∆ =ρ √t xt θ t ∇ logf˜(x ) (11)\nt t α¯ x0|t 0|t\nt\nI+(1−α¯ )∇2 logp (x )\n=ρ t √ xt t t ∇ logf˜(x ). (12)\nt α¯ x0|t 0|t\nt\nThus, it remains to draw a connection between the conditional covariance between Σ and\n0|t\n∇2 logp (x ). Omitthesubscriptx in∇ ,wehave\nxt t t t xt\n∇2p (x )\n∇2logp (x )= t t −∇logp (x )(∇logp (x ))⊤ (13)\nt t p (x ) t t t t\nt t\n1 (cid:90)\n= p (x )∇2p (x |x )dx −∇logp (x )(∇logp (x ))⊤ (14)\np (x ) 0 0 t|0 t 0 0 t t t t\nt t x0∈X\n(15)\nNoticethat\n1\n∇2p (x |x )=p (x |x )∇2logp (x |x )+ ∇p (x |x )(∇p (x |x ))⊤\nt|0 t 0 t|0 t 0 t|0 t 0 p (x |x ) t|0 t 0 t|0 t 0\nt|0 t 0\n√ √\np (x |x ) x − α¯ x x − α¯ x\n=− t|0 t 0 I+p (x |x )( t t 0)( t t 0)⊤.\n1−α¯ t|0 t 0 1−α¯ 1−α¯\nt t t\nReplacing the LHS in the above equation in Equation (14), and noticing that E[x |x ] =\n0 t\nxt+(1−α¯t√)∇logpt(xt),wehave\nα¯t\n√ √\n1 (cid:90) x − α¯ x x − α¯ x\n∇2logp (x )=− I+ p (x |x )( t t 0)( t t 0)⊤dx\nt t 1−α¯ 0|t 0 t 1−α¯ 1−α¯ 0\nt x0∈X t t\n−∇logp (x )(∇logp (x ))⊤\nt t t t\n√ √\n1 (cid:90) x − α¯ x x − α¯ x\n=− I+ p (x |x )( t t 0)( t t 0)⊤dx\n1−α¯ 0|t 0 t 1−α¯ 1−α¯ 0\n√\nt x0∈X\n√\nt t\n−(cid:0) α¯ tE[x 0|x t]−x t(cid:1)(cid:0) α¯ tE[x 0|x t]−x t(cid:1)⊤\n1−α¯ 1−α¯\nt √ t\n1 (cid:104) α¯ x −x (cid:105)\n=− I+Cov t 0 t|x\n1−α¯ 1−α¯ t\nt t\n1 α¯\n=− I+ t Σ .\n1−α¯ (1−α )2 0|t\nt t\nTheproofisfinishedbysubstituting∇2logp (x )inEquation(12)bytheaboveresult.\nt t\n23\nD Taskdetails\nD.1 GaussianDeblur\nIncomputervision,theGaussiandeblurtaskaddressesthechallengeofrestoringclaritytoimages\nthathavebeenblurredbyaGaussianprocess. Gaussianblur,acommonimagedegradation,simulates\neffects such as out-of-focus photography or atmospheric disturbances. It is characterized by the\nconvolutionofanimagewithaGaussiankernel,aprocessthatspreadsthepixelvaluesoutwards,\nleadingtoasmooth,uniformblur[33]. Thedeblurringtaskseekstoreversethiseffect,aimingto\nretrievetheoriginalsharpimage.\nGuidance target. Specifically, we apply a 61 × 61 sized Gaussian blur with kernal intensity\n3.0 to original 256×256 images. A random noise with a variance of σ2 = 0.052 is added to\nthe noisy images. If we denote the above process as a blurring operator A : x → y, where\nblur\nx∈R256×256×3,y ∈R256×256×3areoriginalimagesandnoisyimages,thenthetargetofGaussian\nDebluristogenerateacleanimagex suchthat:\n0\nmaxp(x )=maxexp(−∥A (x )−y∥ ),\n0 blur 0 2\nx0 x0\nwhich means if we project the generated image into the noisy space, the noisy samples A (x)\nblur\nshouldbesimilartothegroundtruthnoisyimagesy.\nEvaluationmetrics. Inourexperiments,weevaluateeachguidancemethodonasetof256samples\ngeneratedbyCat-DDPM.WeapplytheFID[26]toassessthefidelity,LearnedPerceptualImage\nPatchSimilarity(LPIPS)[80]toevaluatetheguidancevalidity.\n24\nFigure9: Quantitativecomparisonofdifferenttraining-freeguidancemethodsonGaussiandeblur\ntask. OurTFGmethodcanproducecleanimageswithoutbackgroundnoise(unlikeFreeDoMand\nUGD),faithfulimagefeatures(unlikeDPS)andvividimagedetails(comparedtoLGD).N isset\nrecur\nto1forallmethods.\n25\nD.2 SuperResolution\nSuper-resolutionincomputervisionreferstotheprocessofenhancingtheresolutionofanimaging\nsystem,aimedatreconstructingahigh-resolutionimagefromoneormorelow-resolutionobserva-\ntions. Thistechniqueisfundamentalinovercomingtheinherentlimitationsofimagingsensorsand\nimprovingthedetailandqualityofdigitalimages. Super-resolutionhasbroadapplications,ranging\nfromsatelliteimagingandsurveillancetomedicalimagingandconsumerphotography[46].\nGuidance target. Specifically, we simply down-sample to original 256 × 256 images to the\nresolutionof64×64. Arandomnoisewithavarianceofσ2 = 0.052 isalsoaddedtothenoisy\nimages. If we denote the above process as a degradation operator A : x → y, where x ∈\ndown\nR256×256×3,y ∈R256×256×3areoriginalimagesanddown-sampledimages,thenthetargetofsuper-\nistogenerateahighresolutionimagex suchthat:\n0\nmaxp(x )=maxexp(−∥A (x )−y∥ ),\n0 down 0 2\nx0 x0\nwhichmeansifweprojectthegeneratedimageintothedownsampledimagespace,thedownsampled\nsamplesA (x)shouldbesimilartothegroundtruthdownsampledimagesy.\ndown\nEvaluationmetrics. SimilartoGaussianDeblur,weevaluateeachguidancemethodonasetof\n256samplesgeneratedbyCat-DDPM.WeapplyFID[26]toassessthefidelity,LearnedPerceptual\nImagePatchSimilarity(LPIPS)[80]toevaluatetheguidancevalidity.\n26\nFigure10: Quantitativecomparisonofdifferenttraining-freeguidancemethodsonsuper-resolution\ntask. OurTFGmethodcanproducecleanimages,faithfulimagefeatures(unlikeDPS,MPGD)and\nvividimagedetails(comparedtoLGD,UGD).N issetto1forallmethods.\nrecur\n27\nD.3 LabelGuidance\nLabelguidanceisastandardtaskforconditionalgenerationstudiedinpreviousliterature[7,23].\nThe target is to generate images conditioned on a certain label. We found this standard task is\nrarelystudiedintraining-freeguidanceworkandthereexistanevidentperformancegapbetween\ntraining-basedguidanceandexistingtraining-freeguidanceasshowninSection3.\nLabelsets. Inourexperiments, westudiedlabelsfromCIFAR10[30]andImageNet[55]. We\naveragetheresultson10labelsfromCIFAR10ifthereisnoextraexplanation. ForImageNet,which\nisresource-hungrytodocomprehensiveinference,werandomlyselect4labels(111,222,333,444)\ntoevaluatethemethods. Thecorrespondinglabel-IDandtheirnamesareasfollows:\nTable6: CIFAR-10DatasetLabels\nLabel-ID LabelName\n0 Airplane\n1 Automobile\n2 Bird\n3 Cat\n4 Deer\n5 Dog\n6 Frog\n7 Horse\n8 Ship\n9 Truck\nTable7: SelectedImageNet-1KDatasetLabels\nLabel-ID LabelName\n111 nematode,nematodeworm,roundworm\n222 Kuvasz\n333 Hamster\n444 bicycle-built-for-two,tandembicycle,tandem\nGuidancetarget. Foreachdataset,weusetheoutputprobabilityofapre-trainedclassifierh(·)as\nthetargetprobability. Ourtargetistomaximizethecertainclassificationprobabilityofagivenlabel,\ni.e.,\nmaxp(x )=maxsoftmax(h(x )) ,\n0 0 i\nx0 x0\nwhereiisthelabel-IDofthetargetlabel,andh(·)isthelogitsofx producedbythepre-trained\n0\nclassifier. ForCIFAR10andImageNet,weuseapre-trainedclassifierbasedonResNet[17]and\nVIT[9]thatareprovidedfrom[75]and[9]respectively. TheimageresolutionforCIFAR10and\nImageNetare32×32and256×256respectively.\nEvaluationmetrics. WefollowtheimagegenerationliteraturetouseFréchetinceptiondistance\n(FID)[20]toassessthefidelityofgeneratedimages. Thereferenceimagesarefilteredfromtheentire\ndatasetofCIFAR10orImageNetwiththetargetlabel,andwesetsamplesizesas2560and256for\nCIFARandImageNetrespectively. Forvalidity,weuseanotherpre-trainedclassifiertocompute\naccuracyotherthantheoneusedinprovidingguidancetoavoidover-confidence:\n#classifiedastargetlabel\naccuracy =\n#generatedsamples\nForCIFAR10,weuseapre-trainedConvNeXT[36]downloadedfromHuggingFaceHub7. Andfor\nImageNet,weusethepre-trainedDeiT[69]downloadedfromHuggingFaceHub8.\n7https://huggingface.co/ahsanjavid/convnext-tiny-finetuned-cifar10\n8https://huggingface.co/facebook/deit-small-patch16-224\n28\nFigure11: Quantitativecomparisonofdifferenttraining-freeguidancemethodsonImageNetlabel\nguidance(withtarget=222,Kuvasz). ThesuffixofFreeDoM,UGD,TFGrepresentsthenumber\nofrecurrenceN . Noticethatallthesamplesaregeneratedbasedonthesameseedandwedo\nrecur\nnotconductcherry-picking. ItisapprentthatTFGgeneratesthemostvalidsamplesamongallthe\ncomparedmethods.\n29\nD.4 CombinedGuidance\nAn interesting scenario for conditional generation is to assign multiple targets for a single sam-\nple. Conditionalgenerationwithmultipleconditionsiscrucialinmachinelearningforenhancing\nthe relevance and applicability of AI across complex, real-world scenarios. It enables models to\nproducemorecontextuallyappropriateandpersonalizedoutputs,crucialforfieldsrequiringhigh\ncustomization.\nMotivations. Combined guidance is to use multiple target functions to guide the same sample\ntowards multiple targets for the same sample. It is more efficient for training-free guidance to\ndo combined guidance as the space of combinatorial targets is potentially huge, which makes it\nunrealistictotrainalltargetcombinationsfortraining-basedguidance. Wealsofinditrelatedto\nthetopicofspuriouscorrelations[60],wherecertaincombinationsofattributesmaydominantthe\nothercombinations. Forexample,haircolormayhaveastrongcorrelationwithgenderinCelebA\ndataset [37]. It is beneficial to explore training-free guidance on reducing the bias of generation\nmodelstrainedonthesebiaseddataandaddresstheconcernsrelatedtofairnessandequality.\nGuidancetarget. WestudycombinedguidanceonCelebA-DDPM,whichistrainedonCelebA-\nHQ[26]dataset. Theimageresolutionis256×256. Wechoosetwosettingsofcombinedguidance\nwith two attributes: (gender, hair color) and (gender, age). Each attribute has two labels, where\ngender∈ {male,female}, age∈ {young,old}, and hair color∈ {black,blonde}. We have abinary\nclassifierforeachattributethatisdownloadedfromHuggingFaceHub91011. Thetargetistosample\nimagesthatmaximizethemarginalprobability:\nmaxp (x )=maxp (x )p (x ),\ncombined 0 target1 0 target2 0\nx0 x0\nwherep (x )iscomputedusinglabelguidanceasshowninAppendixD.3.\ntarget 0\nEvaluationmetrics. AsitishardtofiltermanyreferenceimagesforcombinedtargetsinCelebA-\nHQdataset,weadoptKernelInceptiondistance(KID)[5]toassessfidelityofgeneratedsamples\nusing1,000randomsampledimagesofCelebA-HQasreferenceimages. Wegenerate256samples\nforeachevaluatedmethod. WefollowMPGD[18]tousethelogarithmofKID,i.e.KID(log). For\nvalidity,weuseanotherthreeattributeclassifierstocomputetheaccuracyconsideringtheconjunction\nofattributes:\n#∧ (classifiedastargetlabel)\naccuracy = targetlabel\n#generatedsamples\nTheevaluationclassifiersarealsodownloadedfromHuggingFaceHub121314.\n9Age:https://huggingface.co/nateraw/vit-age-classifier\n10Gender:https://huggingface.co/rizvandwiki/gender-classification-2\n11Haircolor:https://huggingface.co/enzostvs/hair-color\n12Age(Evaluation):ibombonato/swin-age-classifier\n13Gender(Evaluation):https://huggingface.co/rizvandwiki/gender-classification\n14Age(Evaluation):https://huggingface.co/londe33/hair_v02\n30\nFigure12:Quantitativecomparisonofdifferenttraining-freeguidancemethodsoncombinedguidance\ntask(male+young). OurTFGmethodcanproduceimageswithhighfidelityandvaliditycompared\ntoallthebaselines. Noticethatweusethefixedseedforallthemethodsinthisfigureanddonot\nconductcherrypicking. N issetto1forallmethods.\nrecur\nD.5 Fine-grainedGuidance\nFine-grainedclassificationisaspecializedtaskincomputervisionthatfocusesondistinguishing\nbetween highly similar subcategories within a larger, general category. This task is particularly\nchallengingduetothesubtledifferencesamongtheobjectsorentitiesbeingclassified. Forexample,\nin the context of animal species recognition, fine-grained classification would involve not just\ndistinguishingabirdfromacat,butidentifyingthespecificspeciesofbirds,suchasdifferentiating\nbetweenacrowandaraven[79].\nStudyingfine-grainedgenerationinthecontextofgenerativemodelslikeStableDiffusionorDALL-E\npresentsuniquechallengesduetotheinherentcomplexityofgeneratinghighlydetailedandspecific\nimages. Fine-grainedgenerationinvolvescreatingimagesthatnotonlybelongtoabroadcategory\nbutalsocapturethesubtlenuancesandspecificcharacteristicsofanarrowlydefinedsubcategory.\nForexample,generatingimagesofspecificdogbreedsindistinctposesorenvironmentsrequiresthe\nmodeltounderstandandreplicateminutedetailsthatdistinguishonebreedfromanother.\n31\nMotivations. TodeveloppersonalizedAI,itisimportanttoexploreifthefoundationalgenerative\nmodelscansynthesizefine-grained,accuratetargetsamplesaccordingtouser-definedtarget.However,\nthisusuallyrequireshigh-qualityanddetailedtrainingdata,andthemodelshouldbehighlysensitive\ntosmallvariationsininputtoaccuratelyproducethedesiredoutput,whichcanbedifficultforstrong\ntext2imagegenerationmodelsDALL-E15orImagen16. Wefirststudythisprobleminatraining-free\nguidancescenario.\nGuidancetarget. Westudytheout-of-distributionfine-grainedlabelguidanceonImageNet-DDPM,\nwhichlearnsthegenerationofsomespeciesofbirdsbutnotcomprehensively. WeuseanEfficientNet\ntrainedtoclassify525fine-grainedbirdspeciesdownloadedfromHuggingFaceHub.17 Theclassifier\nistrainedonBirdSpeciesdatasetonKaggle18.Wefollowthesamewayinlabel-guidancetomaximize\nsoftmaxprobabilityfortargetfine-grianedlabel. Werandomlysample4labels(111,222,333,444)\ninBirdSpeciesdataset,whichare:\nTable8: SelectedBirdSpeciesDatasetLabels\nLabel-ID LabelName\n111 Brownheadedcowbird\n222 Fairytern\n333 Luciferhummingbird\n444 Scarletmacaw\nEvaluation metrics. Similar to label guidance, we use FID to evaluate generation fidelity by\nfilteringdataofthetargetlabelasreferenceimages. WealsocomputetheFIDwith256sampled\nimages. Foraccuracy,weadoptanotherdownloadedpre-trainedclassifiertrainedonBirdSpecies\ndatasetfromHuggingFaceHub.19.\n15https://openai.com/index/dall-e-2/\n16https://deepmind.google/technologies/imagen-2/\n17https://huggingface.co/chriamue/bird-species-classifier\n18https://www.kaggle.com/datasets/gpiosenka/100-bird-species/data\n19https://huggingface.co/dennisjooo/Birds-Classifier-EfficientNetB2\n32\nFigure13: Thesampled256imagesforfine-grainedguidancewithtargetlabel222(fairytern)by\nImageNet-DDPMwithTFG. Akeyfeatureoffairyternisitsblack-coloredhead. Weobservethat\nTFGgenerallysamplesimageswithblack,roundshapesandsuccessfullygeneratessomebirdswith\ntargetfeature(redcircled).\nD.6 StyleTransfer\nStyle transfer is a significant task in computer vision (CV) that focuses on applying the stylistic\nelementsofoneimageontoanotherwhilepreservingthecontentofthetargetimage. Thistaskis\npivotalbecauseitbridgesthegapbetweenartisticexpressionandtechnologicalinnovation,allowing\nfor the creation of novel and aesthetically pleasing visual content. Applications of style transfer\nare vast, ranging from enhancing user engagement in digital media and advertising to aiding in\narchitecturaldesignbyvisualizingchangesinreal-time.\nGuidancetarget. Thetargetinourexperimentsistoguideatext-to-imagelatentdiffusionmodel\nStable-Diffusion-v-1-5[53]20 togenerateimagesthatfitboththetextinputpromptsandthestyle\nof the reference images. The guidance objective involves calculating the Gram matrices [25] of\ntheintermediatelayersoftheCLIPimageencoderforboththegeneratedimagesandthereference\nstyleimages. TheFrobeniusnormofthesematricesservesasthemetricfortheobjectivefunction.\nSpecifically, for a reference style image x and a decoded image D(z ) generated from the\nref 0|t\n20https://huggingface.co/runwayml/stable-diffusion-v1-5\n33\nestimatedcleanlatentvariablez ,wecomputetheGrammatricesG(x )andG(D(z )). These\n0|t ref 0|t\nmatrices are derived from the features of the 3rd layer of the image encoder, in accordance with\nthemethodologiesdescribedinMPGD[18]andFreeDoM[78]. Thetargetfunctioniscomputedas\nfollows:\nmaxp (x )=maxexp(−∥G(x )−G(D(z ))∥2),\nstyle 0 ref 0|t F\nx0 x0\nwhere∥·∥2 denotestheFrobeniusnormofamatrix.\nF\nEvaluationmetrics. WeuseStylescoreandCLIPscoretoassesstheguidancevalidityandfidelity,\nrespectively. Forreferencestyleimagesandtextprompts,weselect4imagesfromWikiArt[56]\nthatarealsousedbyMPGD[18],and64textpromptsfromPartiprompts[77]. Foreachstyle,we\ngenerate64imagesbasedonthe64differenttextprompts. Toavoidover-confidenceofCLIPscore,\nweusetwodifferentCLIPmodelsdownloadedfromHuggingFaceHubtocomputeguidanceand\nevaluationmetrics,respectively.2122. Thestyleimagesandexamplarpromptsareshownasfollows.\nFigure14: Fourstyleimagesusedinstyletransfertask.\nTable9: Examplesforusedpromptsforstyletransfertasks.\nContentDescription ContentDescription\nAbookwiththewords’Don’tPanic!’ GroundviewoftheGreatPyramidsand\nwrittenonit Sphinxonthemoon’ssurface,Earthinthe\nsky\nAcanalinVenice Awhitetowel\nPortraitofatigerwearingatrain DowntownShanghaiatsunrise,detailed\nconductor’shatandholdingaskateboard inkwash\nBackgroundpatternwithalternatingroses Asmilingslothholdingaquarterstaffanda\nandskulls book,VWvanparkedongrass\nApickuptruck Conceptofenergy\nhAshoewithasockdrapedoverit Akitchenwithoutarefrigerator\nTimesSquareduringtheday Asquirrel\nAturkeywalkinginthekitchen Abowl\nTheStatueofLibertyinMinecraft Amanwithwildhairlookingintoacrystal\nball\nConcentriccircles Afirehydrantwithgraffitionit\n21Guidance:https://huggingface.co/openai/clip-vit-base-patch16\n22Evaluation:https://huggingface.co/openai/clip-vit-base-patch32\n34\nFigure15: Quantitativecomparisonofdifferenttraining-freeguidancemethodsonstyletransfertask\nwiththetargetimageasTheStarryNightbyVonGogh. OurTFGgeneratestheimageswiththemost\nfaithfulstyle,whileDPS,LGD,FreeDoM,andUGDfailtocapturethetargetstyle. Theimagesof\nMPGDisalsoofgoodquality,butthestylescoreisalsoinferiorthanTFGbyalargemargin. Weset\nN =1forallmethods.\nrecur\n35\nD.7 MoleculePropertyGuidance\nSetup. Ourbenchmarksetupgenerallyfollows[24,3]butwithcertainspecificationstoguaranteethe\noverallframeworkabidesbythetraining-freeregime. Fordataset,weemployQM9[50]andadopt\nthesplitin[24]with100,000trainingsamples. Following[24]and[3],thetrainingsetisfurther\nsplitintotwohalvesthatguaranteesthereisnodataleakage. Thefirsthalfisleveragedtotraina\nproperty prediction network with EGNN [57] as the backbone, which serves as the ground truth\noracletoprovidethelabelusedforMAEcomputation. Wereusethecheckpointsreleasedby[3]\nforthelabelingnetworkforall6properties. Thesecondhalfisusedtotrainthediffusionmodel\naswellastheguidancenetwork. WeadopttheunconditionalgenerationversionofEDM[24]as\nthediffusionmodel. Theguidancenetworkingeneraltakesthesamearchitectureasdefinedby[3]\nthat, again, featuresEGNNasthebackbonebutoutputsasinglescalarasthepredictedquantum\nmechanicsproperty. Theonlydifferenceliesinattrainingtimewemaskthediffusiontimestepby\nzerosandalwaysusetheoriginalcleanmoleculestructureasinput,ensuringtraining-freeobjective.\nAllthepretrainedmodelsaretrainedseparatelyfordifferentproperties. Atsamplingtime,weemploy\nDDIM[62]samplerwith100samplingsteps,asopposedto[24,3]thattake1000samplingsteps.\nGuidancetarget. Westudytraining-freeguidedgenerationofmoleculeson6quantummechanics\nproperties,includingpolarizability(α),dipolemoment(µ),heatcapacity(C ),highestorbitalenergy\nv\n(ϵ ),lowestorbitalenergy(ϵ )andtheirgap(∆ϵ). Denotetheoraclepropertyprediction\nHOMO LUMO\nnetworkasE. Ourguidancetargetinthiscaseisgivenby\nf(x,c):=exp(−∥E(x)−c∥2), (16)\n2\nwherexistheinputmoleculeandcisthetargetpropertyvalue.\nEvaluationmetrics. WeuseMeanAbsoluteError(MAE)andvalidityasourevaluationmetrics.\nInparticular,MAEiscomputedbetweenthetargetvalueandthepredictedvaluegatheredfromthe\nlabelingnetwork. ValidityiscomputedbyRDKitwhichmeasureswhetherthemoleculeischemically\nfeasible. Wegenerate4096moleculesforeachpropertyforevaluation.\n36\nPolarizability(𝛼)\n40 60 80 100\nTFG\n(Ours)\nDPS\nLGD\nFreeDoM\nMPGD\nUGD\nFigure16: Quanlitativecomparisonofdifferenttraining-freeguidancemethodsonmoleculegenera-\ntiontaskwiththetargetpropertyα(polarizability). OurTFGgeneratesvalidmoleculeswithbetter\ndesigntarget,whilebaselinesoftenfailtoproducevalidmoleculesorofferpoorguidancetowardsthe\ndesigntarget. Themoleculesgeneratedbyourapproachareincreasinglypolarizableasαgoesup.\n37\nD.8 AudioDeclipping\nAudio declipping is a task in digital audio restoration where distorted audio signals are repaired.\nClippingoccurswhentheamplitudeorfrequencyofanaudiosignalexceedsthemaximumlimitthat\ntherecordingsystemcanhandle,leadingtoaharsh,distortedsoundwithportionsofthewaveform\n“cutoff.”Declippingaimstoreconstructthemissingpartsoftheseclippedwaveforms,restoringthe\naudio’soriginaldynamicsandreducingdistortion. Thisprocessimprovesthequalityandclarityof\ntheaudio,makingitmorepleasanttolistentowhilepreservingtheoriginalsound’sintegrity.\nGuidancetarget. Specifically,weapplyadistortionoperationtozerooutthehighfrequencyand\nlowfrequency(forthehighestandlowest40dimensions)signalinthespaceofmelspectrograms.\nIf we denote the above process as a blurring operator A : x → y, where x ∈ R256×256,y ∈\nblur\nR256×256aremelspectrogramsandnoisymelspectrogramsfor5saudios,thenthetargetofAudio\nDeclippingistogenerateacleanaudiox suchthat:\n0\nmaxp(x )=maxexp(−∥A (x )−y∥ ,\n0 blur 0 2\nx0 x0\nwhichmeansifweprojectthegeneratemelspectrogramintothenoisyspace, thenoisysamples\nA (x)shouldbesimilartothegroundtruthnoisymelspectrogramy.\nblur\nEvaluationmetrics. Inourexperiments,weevaluateeachguidancemethodonasetof256samples\ngenerated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the\nguidancevalidity,andFréchetAudioDistance(FAD)[28]toassessthegenerationfidelity.\n38\nD.9 AudioInpainting\nAudio inpainting is a digital audio restoration task that involves filling in missing or corrupted\nsegments of an audio signal. Similar to image inpainting in the visual domain, this technique\nreconstructs the missing portions of sound by analyzing the surrounding context and seamlessly\nrestoring the lost information. Applications of audio inpainting range from repairing damaged\nrecordings to reconstructing gaps in audio streams due to data loss. The goal is to produce a\nnatural-soundingresultthatpreservesthecontinuityandoverallqualityoftheoriginalaudio.\nGuidancetarget. Specifically,weapplyadistortionoperationtozerooutthemiddle80dimensions\ninthespaceofmelspectrograms.IfwedenotetheaboveprocessasablurringoperatorA :x→y,\nblur\nwherex∈R256×256,y ∈R256×256aremelspectrogramsandnoisymelspectrogramsfor5saudios,\nthenthetargetofAudioDeclippingistogenerateacleanaudiox suchthat:\n0\nmaxp(x )=maxexp(−∥A (x )−y∥ ,\n0 blur 0 2\nx0 x0\nwhichmeansifweprojectthegeneratemelspectrogramintothenoisyspace, thenoisysamples\nA (x)shouldbesimilartothegroundtruthnoisymelspectrogramy.\nblur\nEvaluationmetrics. Inourexperiments,weevaluateeachguidancemethodonasetof256samples\ngenerated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the\nguidancevalidity,andFréchetAudioDistance(FAD)[28]toassessthegenerationfidelity.\n39\nE ExperimentalDetails\nE.1 DetailsofTable1\nInTable1,westudytheeffectofMonte-CarlosamplesizesinestimatingtheexpectationofLine4in\nTFGalgorithm. AsthenoiseisaddedonbothMeanGuidance(∆ )andVarianceGuidance(∆ ),we\n0 t\ndecoupletheeffectintoaddingnoisesolelyon∆ (Meanonly)or∆ (Varianceonly). Inthesetting\n0 t\nofVarianceonly,wesetµ=0,N =0,N =4,s (t)=“increase”,andpickthebestρ¯andγ¯\niter recur ρ\nviahyper-parametersearch. InthesettingofMeanonly,wesetρ=0,N =4,N =1,s (t)=\niter recur µ\n“increase”, andpickthebestµ¯ andγ¯ viahyper-parametersearch. Wefoundthatthesamplesize\nused in Monte-Carlo method play a neglect-able role on the performance if we set the optimal\nhyper-parameter. ItisalsonoteworthythattheMonte-Carlosamplingdoesaffecttheperformance\nofgeneratedquality. Forexample,wecanfindthatdifferenttargetsshowninAppendixE.3have\ndifferentsearchedγ¯. Thisindicatesthatthebestγ¯formanytargetsareapparentlynotzero.\nE.2 Comparisonwithgridsearch\nWecomparetheperformanceofourbeamsearchparameterswiththefullgridsearchonesonCIFAR-\n10 label guidance task (Table 10). Overall, the performance of both search methods is identical,\nwhilegridsearchismuchslowerthanoursearchstrategy,indicatingthatourbeamsearchstrategyis\neffectiveandefficient.\nTable 10: The searched (ρ¯,µ¯,γ¯) of exhaustive grid search and our beam search strategy on the\nCIFAR-10labelguidancetask. Weshowthevaliditymetricofthecorrespondingresultsandthegap\n∆=∥validity −validity ∥. Overall,theperformanceofbothmethodsisidentical.\nbeam grid\nTarget 0 1 2 3 4 5 6 7 8 9 Avg.\n(ρ¯,µ¯,γ¯)grid (1,2,0.001) (0.25,2,0.001) (2,0.25,1) (4,0.5,0.1) (1,0.5,0.001) (2,0.25,0.001) (0.25,0.5,1) (1,0.5,0.001) (1,0.25,0.001) (0.5,2,0.001)\nvaliditygrid 80.44% 35.38% 28.25% 56.32% 29.57% 41.70% 52.66% 42.14% 83.35% 73.22% 52.30%\n(ρ¯,µ¯,γ¯)beam (1,2,0.001) (0.25,2,0.001) (2,0.25,1) (4,0.25,0.01) (1,0.5,0.001) (2,0.25,0.001) (0.25,0.5,1) (1,0.5,0.001) (1,0.25,0.001) (0.5,2,0.001)\nvaliditybeam 80.44% 35.38% 28.25% 52.81% 29.57% 41.70% 52.66% 42.14% 83.35% 73.22% 51.95%\n∆ 0.00% 0.00% 0.00% 3.51% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.35%\nE.3 Detailedresultsofeachtargetandhyper-parameters\nInthissection,wepresentthehyper-parameterssearchedviathestrategyintroducedinSection4and\nthecorrespondingexperimentalresultsforTFGasshowninTable11. Welistseveralobservations\nbelow.\n• Overall,optimalparametersvarywidelybetweenproblemsanddatasets. Forexample,even\nwith thesame model and objective (e.g., labelclassifier on ImageNetor CIFAR10), the\nbesthyperparametersvarywidelyfromtargettotarget. Thishighlightstheimportanceof\nhyperparametersearch.\n• TheimprovementofTFGoverexistingmethodsdependsheavilyonthedifferencebetween\ntheoptimalparametersandthesubspacesofexistingmethods. Forexample,theρ¯forUGD\nis the same as TFG for gender-age guidance task, where TFG only has 0.133% validity\nimprovementoverUGD.Onthecontrary,theirvaluesdifferonthefine-grainedclassification\ntask, andTFGhasan18.7%validityimprovementoverUGD.Overall, wesupposethis\ndependsonwhethertheoptimalparameterliesinthesubspacethatexistingmethodscan\nfind.\n• ThoughthebaselinesmentionedinourpapershouldbeaspecialcaseofTFG,theresultsfor\nthehighestMOenergyguidanceinTable3showthatMPGDoutperformsTFGslightly. We\nwanttopointoutthatthereasonTFGcouldoccasionallyhaveslightlyworseperformance\ninpracticeisduetothebeamsearchcomputationlimitwecurrentlypose. Morespecifically,\nweallowTFGtosearchatmostsixsteps(forallhyper-parameters)andallothermethods\nforsevensteps(intheirsubspaces). FortheMOenergytask,thesearchedparameterfor\nMPGDisthatµ¯ = 0.016(thisistheonlyparameterthatweneedtosearchforMPGD),\nwherethebest(andlaststep)ofTFGisthatµ¯ =0.004(becauseitusesonesteptodouble\nanotherparameter). Ifweallocatemorecomputationalbudgetforthebeamsearchsteps,\nTFGwilloutperformMPGDonthistarget(infact,eightstepssuffice).\n40\nTable11: Theparameter(ρ¯,µ¯,γ¯)selectedbybeamsearchstrategyforallmethods,tasks,andtargets.\nThesearchspaceofeachmethodcanbefoundinSection3.1. Forthedetailedsemanticsofeachtask,\npleaserefertoAppendixD.\nDPS LGD MPGD FreeDoM UGD TFG\nTarget ρ¯ µ¯ γ¯ ρ¯ µ¯ γ¯ ρ¯ µ¯ γ¯ ρ¯ µ¯ γ¯ ρ¯ µ¯ γ¯ ρ¯ µ¯ γ¯\nCIFAR-10labelguidance\n0 1 0 0 16 0 1 0 2 0 1 0 0 2 2 0 1 2 0.001\n1 8 0 0 16 0 1 0 4 0 0.5 0 0 4 4 0 0.25 2 0.001\n2 1 0 0 16 0 1 0 0.25 0 1 0 0 0.25 0.25 0 2 0.25 1\n3 4 0 0 8 0 1 0 8 0 2 0 0 1 1 0 4 0.25 0.01\n4 0.5 0 0 2 0 1 0 0.25 0 0.5 0 0 4 4 0 1 0.5 0.001\n5 4 0 0 0.25 0 1 0 0.25 0 1 0 0 4 4 0 2 0.25 0.001\n6 1 0 0 4 0 1 0 0.5 0 16 0 0 4 4 0 0.25 0.5 1\n7 2 0 0 0.5 0 1 0 0.5 0 0.5 0 0 4 4 0 1 0.5 0.001\n8 2 0 0 16 0 1 0 2 0 1 0 0 4 4 0 1 0.25 0.001\n9 4 0 0 0.5 0 1 0 2 0 1 0 0 4 4 0 0.5 2 0.001\nImageNetlabelguidance\n111 2 0 0 2 0 1 0 8 0 1 0 0 8 8 0 2 0.5 0.1\n222 2 0 0 2 0 1 0 0.25 0 0.5 0 0 2 2 0 0.5 1 0.1\n333 2 0 0 2 0 1 0 0.25 0 0.25 0 0 8 8 0 1 4 1\n444 4 0 0 4 0 1 0 4 0 1 0 0 4 4 0 0.5 2 0.1\nFine-grainedguidance\n111 0.25 0 0 0.25 0 1 0 0.25 0 0.25 0 0 0.25 0.25 0 0.5 0.5 0.01\n222 0.25 0 0 1 0 1 0 0.25 0 0.5 0 0 4 4 0 0.5 0.5 0.01\n333 0.25 0 0 0.25 0 1 0 0.5 0 0.25 0 0 4 4 0 0.5 0.5 0.01\n444 0.25 0 0 0.25 0 1 0 0.25 0 1 0 0 1 1 0 0.5 0.5 0.01\nCombinedGuidance(gender&hair)\n(0,0) 4 0 0 0.25 0 1 0 16 0 1 0 0 16 16 0 1 2 0.01\n(0,1) 4 0 0 16 0 1 0 16 0 0.5 0 0 8 8 0 2 8 0.01\n(1,0) 4 0 0 16 0 1 0 16 0 0.25 0 0 4 4 0 1 1 0.01\n(1,1) 2 0 0 0.25 0 1 0 8 0 2 0 0 2 2 0 0.5 1 0.1\nCombinedGuidance(gender&age)\n(0,0) 8 0 0 0.25 0 1 0 0.25 0 1 0 0 1 1 0 1 2 0.01\n(0,1) 1 0 0 16 0 1 0 16 0 1 0 0 0.5 0.5 0 0.5 8 1\n(1,0) 4 0 0 0.25 0 1 0 8 0 0.5 0 0 0.5 0.5 0 0.5 2 0.01\n(1,1) 2 0 0 0.25 0 1 0 16 0 0.25 0 0 1 1 0 1 0.5 0.1\nSuper-resolution\n\\ 16 0 0 16 0 1 0 16 0 16 0 0 8 8 0 4 2 0.01\nGaussianDeblur\n\\ 16 0 0 16 0 1 0 16 0 16 0 0 16 16 0 1 8 0.01\nStyleTransfer\n0 2 0 0 1 0 1 0 4 0 0.25 0 0 1 1 0 0.25 8 0.01\n1 4 0 0 0.25 0 1 0 4 0 0.5 0 0 1 1 0 0.25 2 0.1\n2 2 0 0 0.25 0 1 0 8 0 0.25 0 0 1 1 0 0.25 8 0.1\n3 2 0 0 2 0 1 0 2 0 0.25 0 0 0.25 0.25 0 0.25 8 0.01\nMoleculeProperty\nα 0.005 0 0 0.005 0 1 0 0.01 0 0.01 0 0 0.02 0.02 0 0.016 0.001 0.0001\nµ 0.02 0 0 0.01 0 1 0 0.005 0 0.02 0 0 0.005 0.005 0 0.001 0.002 0.1\nC v 0.005 0 0 0.005 0 1 0 0.005 0 0.005 0 0 0.005 0.005 0 0.004 0.001 0.001\nϵ 0.005 0 0 0.005 0 1 0 0.01 0 0.005 0 0 0.005 0.005 0 0.002 0.004 0.001\nHOMO\nϵ 0.005 0 0 0.01 0 1 0 0.01 0 0.005 0 0 0.005 0.005 0 0.016 0.002 0.0001\nLUMO\n∆ 0.005 0 0 0.01 0 1 0 0.01 0 0.01 0 0 0.005 0.005 0 0.032 0.001 0.001\nAudioDeclipping\n\\ 1 0 0 16 0 1 0 16 0 4 0 0 4 4 0 1 1 0.1\nAudioInpainting\n\\ 16 0 0 16 0 1 0 16 0 4 0 0 16 16 0 0.25 2 0.1\nE.4 TricksimplementedinFreeDoMcodebase\nInthecodebaseofFreeDoM23,thescheduleofguidancestrengthfordifferentapplicationsisdifferent.\n√\nFor example, the guidance strength has a schedule coefficient α¯ for face generation, and the\nt\nscheduleforstyletransferiscomplexandinvolvesacorrectionterm,themeanofgradients’norm,\nandaspecificconstantcoefficient0.2. Thepaperdoesnotmentionthisparticularschedule,leaving\n23https://github.com/vvictoryuki/FreeDoM\n41\ntherationaleforchoosingtheseschedulesunclear. Wechoosenottoincludethetricksandfindthat\nwithourunifiedhyper-parametersearchingstrategy,theperformanceofFreeDoMissimilar.\nE.5 HardwareandSoftware\nWerunmostoftheexperimentsonclustersusingNVIDIAA100s. Weimplementedourexperiments\nusingPyTorch[49]andtheHuggingFacelibrary.24 Overall,weestimatedthatatotalof2,000GPU\nhourswereconsumed.\n24https://huggingface.co/\n42\nNeurIPSPaperChecklist\n1. Claims\nQuestion: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe\npaper’scontributionsandscope?\nAnswer: [Yes]\nJustification: Wehaveboththeoreticallyandempiricallyjustifiedourcontributions.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmadeinthepaper.\n• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe\ncontributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor\nNAanswertothisquestionwillnotbeperceivedwellbythereviewers.\n• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow\nmuchtheresultscanbeexpectedtogeneralizetoothersettings.\n• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals\narenotattainedbythepaper.\n2. Limitations\nQuestion: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?\nAnswer: [Yes]\nJustification: SeeSec.6.\nGuidelines:\n• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat\nthepaperhaslimitations,butthosearenotdiscussedinthepaper.\n• Theauthorsareencouragedtocreateaseparate\"Limitations\"sectionintheirpaper.\n• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto\nviolationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,\nmodelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors\nshouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe\nimplicationswouldbe.\n• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas\nonlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften\ndependonimplicitassumptions,whichshouldbearticulated.\n• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.\nForexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution\nisloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe\nusedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle\ntechnicaljargon.\n• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms\nandhowtheyscalewithdatasetsize.\n• If applicable, the authors should discuss possible limitations of their approach to\naddressproblemsofprivacyandfairness.\n• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby\nreviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover\nlimitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest\njudgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-\ntantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers\nwillbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.\n3. TheoryAssumptionsandProofs\nQuestion: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand\nacomplete(andcorrect)proof?\nAnswer: [Yes]\n43\nJustification: SeeSec.C.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.\n• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-\nreferenced.\n• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.\n• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif\ntheyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort\nproofsketchtoprovideintuition.\n• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented\nbyformalproofsprovidedinappendixorsupplementalmaterial.\n• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.\n4. ExperimentalResultReproducibility\nQuestion: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-\nperimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions\nofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?\nAnswer: [Yes]\nJustification: SeeSec.D.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhetherthecodeanddataareprovidedornot.\n• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken\ntomaketheirresultsreproducibleorverifiable.\n• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.\nForexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully\nmightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay\nbenecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame\ndataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften\nonegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed\ninstructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase\nofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare\nappropriatetotheresearchperformed.\n• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-\nsionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe\nnatureofthecontribution. Forexample\n(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow\ntoreproducethatalgorithm.\n(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe\nthearchitectureclearlyandfully.\n(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould\neitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce\nthemodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct\nthedataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.\nInthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin\nsomeway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers\ntohavesomepathtoreproducingorverifyingtheresults.\n5. Openaccesstodataandcode\nQuestion: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-\ntionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental\nmaterial?\n44\nAnswer: [Yes]\nJustification: Seesupplementaryfile.\nGuidelines:\n• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy)formoredetails.\n• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe\npossible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot\nincludingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source\nbenchmark).\n• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto\nreproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:\n//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.\n• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow\ntoaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.\n• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew\nproposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they\nshouldstatewhichonesareomittedfromthescriptandwhy.\n• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized\nversions(ifapplicable).\n• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe\npaper)isrecommended,butincludingURLstodataandcodeispermitted.\n6. ExperimentalSetting/Details\nQuestion: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: SeeSec.D.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail\nthatisnecessarytoappreciatetheresultsandmakesenseofthem.\n• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental\nmaterial.\n7. ExperimentStatisticalSignificance\nQuestion:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate\ninformationaboutthestatisticalsignificanceoftheexperiments?\nAnswer: [No]\nJustification: Ourexperimentisconductedonanextensivelylargescale,andexistingworks\nofthesamelinewerenotreportedsincethenumbersarestable.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theauthorsshouldanswer\"Yes\"iftheresultsareaccompaniedbyerrorbars,confi-\ndenceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport\nthemainclaimsofthepaper.\n• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for\nexample,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall\nrunwithgivenexperimentalconditions).\n• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,\ncalltoalibraryfunction,bootstrap,etc.)\n• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).\n45\n• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror\nofthemean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis\nofNormalityoferrorsisnotverified.\n• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor\nfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative\nerrorrates).\n• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow\ntheywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.\n8. ExperimentsComputeResources\nQuestion: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-\nputerresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce\ntheexperiments?\nAnswer: [Yes]\nJustification: SeeSec.5.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,\norcloudprovider,includingrelevantmemoryandstorage.\n• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual\nexperimentalrunsaswellasestimatethetotalcompute.\n• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute\nthantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat\ndidn’tmakeitintothepaper).\n9. CodeOfEthics\nQuestion: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe\nNeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: WefollowtheNeurIPSCodeofEthics.\nGuidelines:\n• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.\n• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea\ndeviationfromtheCodeofEthics.\n• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-\nerationduetolawsorregulationsintheirjurisdiction).\n10. BroaderImpacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietalimpactsoftheworkperformed?\nAnswer: [Yes]\nJustification: SeeSec.6.\nGuidelines:\n• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.\n• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal\nimpactorwhythepaperdoesnotaddresssocietalimpact.\n• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses\n(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations\n(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific\ngroups),privacyconsiderations,andsecurityconsiderations.\n46\n• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied\ntoparticularapplications,letalonedeployments. However,ifthereisadirectpathto\nanynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate\ntopointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto\ngeneratedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout\nthatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain\nmodelsthatgenerateDeepfakesfaster.\n• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing\nfrom(intentionalorunintentional)misuseofthetechnology.\n• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom\nfeedbackovertime,improvingtheefficiencyandaccessibilityofML).\n11. Safeguards\nQuestion: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible\nreleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,\nimagegenerators,orscrapeddatasets)?\nAnswer: [NA]\nJustification: Itisnotapplicabletotheconcernofthispaper.\nGuidelines:\n• TheanswerNAmeansthatthepaperposesnosuchrisks.\n• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith\nnecessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring\nthatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing\nsafetyfilters.\n• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors\nshoulddescribehowtheyavoidedreleasingunsafeimages.\n• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo\nnotrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest\nfaitheffort.\n12. Licensesforexistingassets\nQuestion: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin\nthepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand\nproperlyrespected?\nAnswer: [Yes]\nJustification: Wehavecitedtherelatedpapers.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.\n• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.\n• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea\nURL.\n• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.\n• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof\nserviceofthatsourceshouldbeprovided.\n• If assets are released, the license, copyright information, and terms of use in the\npackageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets\nhascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe\nlicenseofadataset.\n• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof\nthederivedasset(ifithaschanged)shouldbeprovided.\n47\n• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto\ntheasset’screators.\n13. NewAssets\nQuestion:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation\nprovidedalongsidetheassets?\nAnswer: [NA]\nJustification: Wedonotincludenewassets.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.\n• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir\nsubmissions via structured templates. This includes details about training, license,\nlimitations,etc.\n• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose\nassetisused.\n• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither\ncreateananonymizedURLorincludeananonymizedzipfile.\n14. CrowdsourcingandResearchwithHumanSubjects\nQuestion: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper\nincludethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as\nwellasdetailsaboutcompensation(ifany)?\nAnswer: [NA]\nJustification: Wedonotinvolvecrowdsourcingandresearchwithhumansubjects.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-\ntionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe\nincludedinthemainpaper.\n• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,\norotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata\ncollector.\n15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman\nSubjects\nQuestion: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether\nsuchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)\napprovals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor\ninstitution)wereobtained?\nAnswer: [NA]\nJustification: Wedonotinvolvehumansubjectsandstudies.\nGuidelines:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)\nmayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you\nshouldclearlystatethisinthepaper.\n• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions\nandlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe\nguidelinesfortheirinstitution.\n• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if\napplicable),suchastheinstitutionconductingthereview.\n48",
    "pdf_filename": "TFG_Unified_Training-Free_Guidance_for_Diffusion_Models.pdf"
}