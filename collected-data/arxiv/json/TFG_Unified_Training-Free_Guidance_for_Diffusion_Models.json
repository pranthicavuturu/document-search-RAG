{
    "title": "TFG Unified Training-Free Guidance for Diffusion Models",
    "abstract": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.1 1 Introduction Recent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have demonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24], proteins[1, 72], audio [35, 29], 3D objects [40, 41], and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data [58], future diffusion models have the potential to serve as foundational generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important [63, 2]. Conditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples. In classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusion ∗Equal contribution. Corresponding to mailto:haotianye@stanford.edu. 1Code is available at https://github.com/YWolfeee/Training-Free-Guidance. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2409.15761v2  [cs.LG]  19 Nov 2024",
    "body": "TFG: Unified Training-Free Guidance\nfor Diffusion Models\nHaotian Ye1∗\nHaowei Lin2∗\nJiaqi Han1∗\nMinkai Xu1\nSheng Liu1\nYitao Liang2\nJianzhu Ma3\nJames Zou1\nStefano Ermon1\n1Stanford University\n2Peking University\n3Tsinghua University\nAbstract\nGiven an unconditional diffusion model and a predictor for a target property\nof interest (e.g., a classifier), the goal of training-free guidance is to generate\nsamples with desirable target properties without additional training. Existing\nmethods, though effective in various individual applications, often lack theoretical\ngrounding and rigorous testing on extensive benchmarks. As a result, they could\neven fail on simple tasks, and applying them to a new problem becomes unavoidably\ndifficult. This paper introduces a novel algorithmic framework encompassing\nexisting methods as special cases, unifying the study of training-free guidance into\nthe analysis of an algorithm-agnostic design space. Via theoretical and empirical\ninvestigation, we propose an efficient and effective hyper-parameter searching\nstrategy that can be readily applied to any downstream task. We systematically\nbenchmark across 7 diffusion models on 16 tasks with 40 targets, and improve\nperformance by 8.5% on average. Our framework and benchmark offer a solid\nfoundation for conditional generation in a training-free manner.1\n1\nIntroduction\nRecent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have\ndemonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24],\nproteins[1, 72], audio [35, 29], 3D objects [40, 41], and many more. Diffusion models estimate the\ngradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning\nobjectives, and can generate new samples via an iterative denoising process. With impressive\nscalability to billions of data [58], future diffusion models have the potential to serve as foundational\ngenerative models across a wide range of applications. Consequently, the problem of conditional\ngeneration based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels,\nattributes, energies, and spatial-temporal information, is becoming increasingly important [63, 2].\nConditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23]\ntypically require training a specialized model for each conditioning signal (e.g., a noise-conditional\nclassifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly\nlimits their applicability. In contrast, training-free guidance aims to generate samples that align with\ncertain targets specified through an off-the-shelf differentiable target predictor without involving any\nadditional training. Here, a target predictor can be any classifier, loss function, probability function,\nor energy function used to score the quality of the generated samples.\nIn classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to\npredict the target property on both clean and noisy samples, incorporating guidance in the diffusion\n∗Equal contribution. Corresponding to mailto:haotianye@stanford.edu.\n1Code is available at https://github.com/YWolfeee/Training-Free-Guidance.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2409.15761v2  [cs.LG]  19 Nov 2024\n\nFigure 1: (a) Illustration of the unified search space of our proposed TFG, where the height (color)\nstands for performance. Existing algorithms search along sub-manifolds, while TFG results in\nimproved guidance thanks to its extended search space. (b) The label accuracy (higher the better) and\nFréchet inception distance (FID, lower the better) of different methods for the label guidance task on\nCIFAR10 [30], averaged across ten labels. Ours (TFG-4) performs much closer to training-based\nmethods. (c∼h) TFG generated samples across various tasks in vision, audio, and geometry domains.\nprocess is straightforward since the gradient of the classifier is an unbiased driving term. Training-\nfree guidance, however, is fundamentally more difficult. The primary challenge lies in leveraging\na target predictor trained solely on clean samples to offer guidance on noisy samples. Although\nvarious approaches have been proposed [18, 63, 6, 2, 78] and are effective for some individual tasks,\ntheoretical grounding and comprehensive benchmarks are still missing. Indeed, existing methods\nfail to produce satisfactory samples for label guidance even on simple datasets such as CIFAR10\n(Figure 1). Moreover, the lack of quantitative comparisons between these methods makes it difficult\nfor practitioners to identify an appropriate algorithm for a new application scenario.\nThis paper proposes a novel and general algorithmic framework for (and also named as) Training Free\nGuidance (TFG). We show that existing approaches are special cases of the TFG as they correspond\nto particular hyper-parameter subspace in our unified space. In other words, TFG naturally simplifies\nand reduces the study of training-free guidance, as well as the comparisons between existing methods,\ninto the analysis of hyper-parameter choices in our unified design space. Within our framework, we\nanalyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive\nexperiments to identify their influence. Our systematic study offers novel insights into the principles\nbehind training-free guidance, allowing for a transparent and efficient survey of the problem.\n2\n\nBased on the framework, we propose a hyper-parameter searching strategy for general downstream\ntasks. We comprehensively benchmark TFG and existing algorithms across 16 tasks (ranging\nfrom images to molecules) and 40 targets. TFG achieves superior performance across all datasets,\noutperforming existing methods by 8.5% on average. In particular, it excels in generating user-\nrequired samples in various scenarios, regardless of the complexity of targets and datasets.\nIn summary, we (1) propose TFG that unifies existing algorithms into a design space, (2) theoretically\nand empirically analyze the space to propose an effective space-searching strategy for general\nproblems, and (3) benchmark all methods on numerous qualitatively different tasks to present the\nsuperiority of TFG and the guideline for future research in training-free conditional generation\nalgorithms. This advancement demonstrates the efficacy of TFG and establishes a robust and\ncomprehensive benchmark for future research in training-free conditional generation algorithms.\n2\nBackground\nGenerative diffusion model. A generative diffusion model is a neural network that can be used to\nsample from an unconditional distribution p0(x) with the support on any continuous sample space\nX [21, 62, 64, 27]. For instance, X could be [−1, 1]d×d×3 representing the RGB colors of d × d\nimages [4, 22], or R3d representing the 3D coordinates of molecules with d atoms [24, 74, 73]. Given\na data x0 sampled from p0(x), a time step t ∈[T] ≜{1, · · · , T}, a corresponding noisy datapoint\nis constructed as xt = √¯αtx0 + √1 −¯αtϵ where ϵ ∼N(0, I) and {¯αt}T\nt=1 is a set of pre-defined\nmonotonically decreasing parameters used to control the noise level. Following [21], we further define\nαt = ¯αt/¯αt−1 for t > 1 and α1 = ¯α1. The diffusion model ϵθ : X × [T] 7→X parameterized by θ is\ntrained to predict the noise ϵ that was added on xt with p.d.f pt(xt) =\nR\nx0 p0(x0)pt|0(xt|x0)dx02.\nIn theory, this corresponds to learning the score of pt(x) [65], i.e.,\narg min\nϵθ\nT\nX\nt=1\nEx0∼p0(x0),ϵ∼N (0,I)∥ϵθ(xt, t) −ϵ∥= −\n√\n1 −¯αt∇log pt.\n(1)\nFor sampling, we start from xT ∼N(0, I) and gradually sample xt−1 ∼pt−1|t(xt−1|xt). This\nconditional probability is not directly computable, and in practice, DDIM [62] samples xt−1 via\nxt−1 = √¯αt−1x0|t +\nq\n1 −¯αt−1 −σ2\nt\nxt −√¯αtx0|t\n√1 −¯αt\n+ σtϵ,\n(2)\nwhere {σt}T\nt=1 are DDIM parameters, ϵ ∼N(0, I), and\nx0|t = m(xt) ≜xt −√1 −¯αtϵθ(xt, t)\n√¯αt\n(3)\nis the predicted sample given xt. According to the Tweedie’s formula [11, 51], x0|t equals to the\nconditional expectation E[x0|xt] under perfect optimization of ϵθ in Equation (1). It has been theo-\nretically established that the above sampling process results in x0 ∼p(x) under certain assumptions.\nTarget predictor. For a user required target c, we use a predictor fc(x) : X 7→R+ ∪{0} 3 to\nrepresent how well a sample x is aligned with the target (higher the better). Here fc(x) can be\na conditional probability p0(c|x) for a label c [62, 14], a Boltzmann distribution exp−ec(x) for\nany pre-defined energy function ec [31, 63, 38], the similarity of two features [47], or even their\ncombinations. The goal is to samples from the conditional distribution\np0(x|c) ≜\np0(x)fc(x)\nR\n˜x p0(˜x)fc(˜x)d˜x.\n(4)\n2In this paper, we use p(x) to represent the probability density function (p.d.f.), and pt(x), pt|s(x|˜x) to\nrepresent the probability at time step t and the conditional probability of x at time step t given ˜x at time step s.\n3Here c can has any mathematical form. We assume in this paper that fc(x) has finite two norm, i.e.\nR\nx∈X [f 2\nc (x)] < +∞, such that the probabilistic explanation is well-defined.\n3\n\nTraining-based guidance for diffusion models. [66] proposes to train a time-dependent classifier to\nfit fc(xt, t) ≜Ex0∼p0|t(·|xt)fc(x0). This can be regarded as a predictor over noisy samples. Since\n∇xt log pt(xt|c) = ∇xt log\nZ\nx0\npt|0(xt|x0)p0(x0|c)dx0\n= ∇xt log\nZ\nx0\npt(xt)p0|t(x0|xt)fc(x0)dx0\n= ∇xt log pt(xt) + ∇xt log fc(xt, t),\n(5)\nif we denote the trained classifier as f(xt) (that implicitly depends on c and model parameters),\nwe can replace ϵθ(xt, t) in Equation (3) by ϵθ(xt, t) −√1 −¯αt∇xt log f(xt) upon sampling to\nobtain unbiased sample x0 ∼p0(x0|c). On the other hand, [23] proposes the classifier-free diffusion\nguidance approach. Instead of training a time-dependent predictor f, it encodes conditions c directly\ninto the diffusion model as ϵθ(x, c, t) and trains this condition-aware diffusion model with sample-\ncondition pairs. Both methods have been proven effective when training resources are available.\nThis paper in contrast focuses on conditional generation in a training-free manner: given a diffusion\nmodel ϵθ(x, t) and an off-the-shelf target predictor f(x) (we omit the subscript c below), we aim\nto generate samples from p0(x|c) without any additional training. Unlike training-based methods\nthat can accurately estimate f(xt, t), training-free guidance is significantly more difficult since it\ninvolves guiding a noisy data xt using f(x) defined over the clean data space.\n2.1\nExisting algorithms\nMost existing methods take advantage of the predicted sample x0|t defined in Equation (3) and use the\ngradient of f(x) for guidance. We review and summarize five existing approaches below, and provide\na schematic and a copy of pseudo-code in Appendix B for the sake of reference. Due to the variety in\nunderlying intuitions and implementations, coupled with a lack of quantitative comparisons among\nthese methods, it is challenging to discern which operations are crucial and which are superfluous, a\nproblem we address in Section 3.\nDPS [6] was initially proposed to solve general noisy inverse problems for image generation: for a\ngiven condition y and a transformation operator A, we aim to generate image x such that ∥A(x)−y∥2\nis small. For instance, in super-resolution task [71], the operator A is a down-sampling operator, and\ny is a low-resolution image. DPS replaces ∇log f(xt, t) in Equation (5) by ∇xt log f(m(xt)). As\nsuggested in [63], this corresponds to a point estimation of the conditional density p0|t(x0|xt).\nLGD [63] replaces the point estimation in DPS and proposes to estimate f(xt, t) with a Gaussian\nkernel Ex∼N (x0|t,σ2\nt I)f(x, t), where the expectation is computed using Monte-Carlo sampling [59].\nFreeDoM [78] generalizes DPS by introducing a “recurrent strategy” (called “time-travel strat-\negy” [39, 10, 70]) that iteratively denoises xt−1 from xt and adds noise to xt−1 to regenerate xt back\nand forth. This strategy empirically enhances the strength of the guidance at the cost of additional\ncomputation. FreeDoM also points out the importance of altering guidance strength at different time\nsteps t, but a comprehensive study on which schedule is better is not provided.\nMPGD [18] is proposed for manifold preserving tasks, e.g., the target predictor is supposed to\ngenerate samples on a given manifold. It computes the gradient of log f(x0|t) to x0|t instead of\nxt, i.e., ∇x0|t log f(x0|t) to avoid the back-propagation through the diffusion model ϵθ that is\nhighly inefficient. This strategy is effective in manifold-preserving problems, but whether it can be\ngeneralized to general training-free problems is unclear. In addition to the computation difference,\ntheoretical understanding on the difference between gradients to x0|t and xt is missing.4\nUGD [2] builds on FreeDoM, with the difference that it additionally solves a backward optimization\nproblem ∆0 = arg max∆f(x0|t +∆) and guides x0|t and xt simultaneously. UGD also implements\nthe “recurrent strategy” to further improve generation quality.\n4MPGD additionally proposed to use an auto-encoder to improve the quality of x0|t. However, an auto-\nencoder is usually inaccessible or requires training; thus, we don’t apply it in our training-free scenario.\n4\n\n3\nTFG: A Unified Framework for Training-free Guidance\nDespite the array of algorithms available and their reported successes in various applications, we\nconduct a case study on CIFAR10 [30] to illustrate the challenging nature of training-free guidance\nand the insufficiency of existing methods. Specifically, for each of the ten labels, we use the pretrained\ndiffusion model and classifiers from [7, 9] to generate 2048 samples, where the hyper-parameters\nare selected via a grid search for the fairness of comparison. We compute the FID and the label\naccuracy evaluated by another classifier [20] and present results in Figure 1. Even in such a relatively\nsimple setting, all training-free approaches significantly underperform training-based guidance,\nwith a significant portion of generated images being highly unnatural (when guidance is strong) or\nirrelevant to the label (when guidance is weak). These findings reveal the fundamental challenges\nand highlight the necessity of a comprehensive study. Unfortunately, comparisons and analyses of\nexisting approaches are missing or primarily qualitative, limiting deeper investigation in this field.\n3.1\nUnification and extension\nThis sections introduces our unified framework for training-free guidance (TFG, Algorithm 1) and\nformally defines its design space in Definition 3.1. We demonstrate the advantage of TFG by drawing\nconnections between TFG and other algorithms to show that existing algorithms are encompassed as\nspecial cases. Based on this, all comparisons and studies of training-free algorithms automatically\nbecome the study within the hyper-parameter space of our framework. This allows us to analyze the\ntechniques theoretically and empirically, and choose an appropriate hyper-parameter for a specific\ndownstream task efficiently and effectively, as shown in Section 4.\nAlgorithm 1 Training-Free Guidance\n1: Input: Unconditional diffusion model ϵθ, target predictor f, guidance strength ρ, µ, ¯γ, number of steps\nT, Nrecur, Niter\n2: xT ∼N(0, I)\n3: for t = T, · · · , 1 do\n4:\nDefine function ˜f(x) = Eδ∼N (0,I)f(x + ¯γ√1 −¯αtδ)\n5:\nfor r = 1, · · · , Nrecur do\n6:\nx0|t = (xt −√1 −¯αtϵθ(xt, t))/√¯αt\n▷Obtain the predicted data\n7:\n∆t = ρt∇xt log ˜f(x0|t)\n8:\n∆0 = ∆0 + µt∇x0|t log ˜f(x0|t + ∆0)\n▷Iterate Niter times starting from ∆0 = 0\n9:\nxt−1 = Sample(xt, x0|t, t) + ∆t/√αt + √¯αt−1∆0\n▷Sample follows Equation (2)\n10:\nxt ∼N(√αtxt−1, √1 −αtI)\n▷Recurrent strategy\n11:\nend for\n12: end for\n13: Output: Conditional sample x0\nDefinition 3.1. Given a denoising step T, the hyper-parameter space (design space) of Algorithm 1\nis defined as\nHTFG = {(Nrecur, Niter, ¯γ, ρ, µ) : Nrecur, Nrecur ∈N, ¯γ ≥0, ρ, µ ∈\n\u0000R+ ∪{0}\n\u0001T }.\n(6)\nWe use HTFG to represent the complete hyper-parameter space and HTFG(Nrecur = N0) to represent\nthe subspace constrained on Nrecur = N0.\nDefinition 3.1 defines the hyper-parameter space spanned by TFG, where one hyper-parameter in\nHTFG is an instantiation of the framework. Intuitively, Nrecur controls the recurrence of the algorithm,\nNiter controls the iterating when computing ∆0 (Line 8), ¯γ controls the extent we smooth the original\nguidance function f (Line 4), and ρ, µ control the strength of two types of guidance (Lines 7 and 8).\nA comprehensive explanation of the effect of each hyper-parameter can be found in Section 3.2.\nBelow is the major theorem showing that all algorithms presented in Section 2.1 correspond to special\ncases of TFG, thus unifying them into our framework and obviating the need for separate analyses.\nTheorem 3.2. The hyper-parameter space of\n• MPGD [18] HMPGD is equivalent to HTFG(Nrecur = Niter = 1, ρ = 0, ¯γ = 0).\n• LGD [63] HLGD is equivalent to HTFG(Nrecur = 1, Niter = 0, µ = 0).\n5\n\n• UGD [2] HUGD is equivalent to HTFG(¯γ = 0).\n• DPS [6] HDPS is equivalent to HTFG(Nrecur = 1, Niter = 0, µ = 0, ¯γ = 0).\n• FreeDoM [78] HFreeDoM is equivalent to HTFG(Niter = 0, µ = 0, ¯γ = 0).\nThe complete analysis and proof of Theorem 3.2 is postponed to Appendix C. It implies that existing\nalgorithms are limited in expressivity, covering only a subset of HTFG. In contrast, TFG covers the\nentire space and is guaranteed to perform better. In addition, TFG streamlines nuances between\nexisting methods, allowing for a unified way to compare and study different techniques. Consequently,\nthe versatile framework that TFG provides can simplify its adaptation to various applications.\n3.2\nAlgorithm and design space analysis\nWe now present a concrete analysis of TFG and its design space H in detail. Similar to standard\nclassifier-based guidance, TFG guides xt at each denoising step t. To provide appropriate and\ninformative guidance, TFG essentially leverages four techniques for guidance: Mean Guidance\n(Line 8) controlled by Niter, µ, Variance Guidance (Line 7) controlled by ρ, Implicit Dynamic\n(Line 4) controlled by ¯γ, and Recurrence (Line 5) controlled by Nrecur.\nMean Guidance computes the gradient of ˜f(x) to x0|t and is the most straightforward approach.\nHowever, this method can yield inaccurate guidance. To show this, notice that under perfect optimiza-\ntion we have x0|t = E[x0|xt], and when p0(E[x0|xt]) is close to zero, the predictor has rarely been\ntrained on data from the region close to x0|t, making the gradient unstable and noisy. To mitigate this,\none can iteratively add gradients of ˜f(x) to x0|t, encouraging x0|t to escape low-probability regions.\nVariance Guidance provides an alternative approach for improving the gradient estimation. The\nreason why we dub it variance guidance might be ambiguous, as the only difference is that the\ngradient is taken with respect to xt (Line 7) instead of x0|t (Line 8). The lemma below demonstrates\nthat this essentially corresponds to a covariance re-scaled guidance.\nLemma 3.3. If the model is optimized perfectly, i.e., ϵθ(x, t) = −√1 −¯αt∇log pt(x), we have\n∆t =\n√¯αt\n1 −αt\nΣ0|t∇x0|t ˜f(x0|t),\n(7)\nwhere Σ0|t ≜\nR\nx p0|t(x|xt)(x −E[x0|xt])(x −E[x0|xt])⊤dx is the covariance of x0|xt.\nLemma 3.3 suggests that variance guidance refines mean guidance by incorporating the second-\norder information of x0|xt, specifically considering the correlation among components within x0|t.\nConsequently, positively correlated components could have guidance mutually reinforced, while\nnegatively correlated components could have guidance canceled. This also implies that mean guidance\nand variance guidance are intrinsically leveraging different orders of information for guidance. In\nTFG, variance guidance is controlled by ρt.\nTable 1: Influence of the number of\nMonte-Carlo samples in estimating the\nexpectation of Line 4. Both the FID and\nthe accuracy remain unchanged when\n#Samples varies, suggesting that the\nnumber of samples is less important.\nMore details are in Appendix E.1.\n#Samples\nVariance only\nMean only\nFID\nAcc(%)\nFID\nAcc(%)\n1\n90.6\n65.8\n101\n36.2\n2\n91.0\n65.2\n100\n35.6\n4\n90.7\n64.9\n99.7\n36.2\nImplicit Dynamic transforms the predictor f into its con-\nvolution via a Gaussian kernel N(0, ¯γ(1 −¯αt)I). This\noperation is initially introduced by LGD [63] to estimate\np0|t(x0|xt). However, it is unclear why the form is pre-\nselected as a Gaussian distribution. We argue that this tech-\nnique is effective because it creates an implicit dynamic\non x0|t. Specifically, starting from x0|t, it iteratively adds\nnoise to x0|t, evaluates gradient, and moves x0|t based\non the gradient. The repeating process converges to the\ndensity proportional to f(x) when Niter goes to infinity,\ndriving x0|t to high-density regions. This explanation is\njustified by Table 1: the performance remains nearly un-\nchanged as we gradually decrease the number of Monte-Carlo samples in estimating the expectation\n(Line 4) down to 1, implying that the preciseness of estimation is not essential, but adding noises is.\nRecurrence helps strengthen the guidance by iterating the previous three techniques to obtain xt−1\nand resample xt back and forth. This can be understood as an Ornstein–Uhlenbeck process[42] on\nxt−1 where Line 6∼9 corresponds to the drift term and xt−1 →xt (Line 10) the white noise term.\nIntuitively, it finds a trade-off between the error inherited from previous steps (the more you recur, the\n6\n\nincrease\ndecrease\nconstant\nNrecur=4\nNrecur=2\nNrecur=1\n0.25\n0.5\n1.0\n2.0\n4.0\nincrease\ndecrease\nconstant\nNrecur=4\nNrecur=2\nNrecur=1\n0.25\n0.5\n1.0\n2.0\n4.0\nFID\nFID\nCIFAR10\nImageNet\nAccuracy\nAccuracy\nCIFAR10\nImageNet\nincrease\ndecrease\nconstant\nNiter=4\nNiter=2\nNiter=1\n0.25\n0.5\n1.0\n2.0\n4.0\nFID\nincrease\ndecrease\nconstant\nNiter=4\nNiter=2\nNiter=1\n0.25\n0.5\n1.0\n2.0\n4.0\nFID\nAccuracy\nAccuracy\nFigure 2: Comparison of three structures in Equation (8) of ρ and µ on CIFAR10 and ImageNet, under\ndifferent choices of the rest hyper-parameters in HTFG. We set ρ = 0, ¯γ = 0 when studying structures\nof µ, and similarly for ρ. Results are averaged across all labels. The comparative relationship between\nstructures remains unchanged when the rest of the parameters vary.\nless previous error stays) and the accumulated error in this step (the more you recur, the more error in\nthe current guidance you suffer). Empirically, we also find that the generation quality improves and\nthen deteriorates as we increase Nrecur.\n4\nDesign Space of TFG: Analysis and Searching Strategy\nAdmittedly, a more extensive design space only yields a better performance if an effective and\nrobust hyper-parameter searching strategy can be applied. For example, arbitrarily complex neural\nnetworks are guaranteed to have better optimal performance than simple linear models, but finding\nthe correct model parameters is significantly more difficult. This section dives into this core problem\nby comprehensively analyzing the hyper-parameter space structure of HTFG, and further proposing a\ngeneral searching algorithm applicable for any general downstream tasks.\nThe hyper-parameters of HTFG can be categorized into two parts: time-dependent vectors ρ, µ,\nand time-independent sacalars Nrecur, Niter, ¯γ. While a grid search can potentially result in the\nbest performance, performing such an extensive search in HTFG is highly impractical, especially\nconsidering the vector parameters ρ, µ. Fortunately, below we demonstrate that, if we decompose\nρ into ¯ρ · sρ(t) (same for µ) where ¯ρ is a scalar and sρ(t) is a “structure” (a non-negative function)\nsuch that P\nt sρ(t) = T, then some structures are consistently better than others regardless of the\nother hyper-parameters. This allows us to pre-locate an appropriate structure for the given task and\nefficiently optimize the rest of the scalar hyper-parameters. Our analysis is conducted on the label\nguidance task on CIFAR-10 [30] and ImageNet [55], with experimental settings identical to Section 3.\nStructure analysis. Motivated by the default structure selected in UGD and LGD, we consider three\nstructures for both sρ(t) and sµ(t) as\ns(t) =\nαt\nPT\nt=1 αt\n(increase), s(t) =\n(1 −αt)\nPT\nt=1(1 −αt)\n(decrease), s(t) = 1(constant).\n(8)\nThese structures are selected to be qualitatively different, while each is justified to be reasonable\nunder certain conditions [18, 78, 2]. We leave the study of more structures to future works. The\nrest of the parameters are grid-searched for the comprehensiveness of the analysis. For sρ(t), we\nset Nrecur = {1, 2, 4} and ¯ρ = {0.25, 0.5, 1.0, 2.0, 4.0}; and for sµ(t), we set Niter = {1, 2, 4} and\n¯µ = {0.25, 0.5, 1.0, 2.0, 4.0}. We run label guidance for each configuration and each of the ten labels\non CIFAR10 (four labels on ImageNet, due to computation constraints).\nAs presented in Figure 2, the relationship between different structures remains unchanged when the\nrest of the parameters vary. For instance, on both datasets, the Validity-FID performance curves\nconsistently move top-left (implying a better performance) when we switch from “decrease” structure\n(red lines) to “constant” structure (yellow lines) to “increase” structure (blue lines) for both ρ, µ and\ndifferent values of Nrecur and Niter. This invariant relationship is essential as it allows for an efficient\nhyper-parameters search in HTFG by first determining appropriate structures for sρ(t), sµ(t) under a\nsimple subspace, and then selecting the rest scalar parameters.\n7\n\nTable 2: List of 14 task types we benchmark. Each task is run with multiple individual targets (38 in total). We\nevaluate the guidance validity (how well a sample is aligned with the target predictor) and the guidance fidelity\n(how well a sample is aligned with the unconditional distribution) according to the task type.\nDiffusion Model\nTask-ID\nTargets\nGuidance Validity Guidance Fidelity\nCat-DDPM\nGaussian deblur\n\\\nLPIPS ↓\nFID ↓\nSuper-resolution\n\\\nLPIPS ↓\nFID ↓\nCelebA-DDPM\nCombined guidance (gender+age)\n2 genders × 2 ages\nAccuracy (%) ↑\nKID (log) ↓\nCombined guidance (gender+hair) 2 genders × 2 hair colors\nAccuracy (%) ↑\nKID (log) ↓\nCIFAR10-DDPM\nLabel guidance (CIFAR10)\n10 labels (0, · · · , 9)\nAccuracy (%) ↑\nFID ↓\nImageNet-DDPM\nLabel guidance (ImageNet)\n4 labels (111, · · · , 444)\nAccuracy (%) ↑\nFID ↓\nFine-grained guidance\n4 labels (111, · · · , 444)\nAccuracy (%) ↑\nFID ↓\nStable-Diffusion\nStyle transfer\n4 styles\nStyle score ↓\nCLIP score ↑\nMolecule-EDM\nQuantum Properties (×6)\nProperty distribution\nMAE ↓\nValid ratio ↑\nAudio-Diffusion\nAudio declipping\n\\\nDTW (%) ↓\nFAD ↓\nAudio inpainting\n\\\nDTW (%) ↓\nFAD ↓\nAccuracy\nTime Cost (s / 256 samples)\nFID\nNrecur=4\nNiter=4\nNiter=2\nNiter=1\nNrecur=1\nNrecur=2\nNrecur=4\nNrecur=2\nNrecur=1\nNiter=4\nNiter=2\nNiter=1\n0.25\n0.5\n1.0\n2.0\n4.0\nFigure 3: Accuracy and FID on CIFAR10 under\ndifferent Nrecur and Niter. sρ(t), sµ(t) are fixed\nto “increase” structure, and ρ = γ = 0.\nComputation cost analysis. Among the scalars\nparameters, Nrecur and Niter directly influence the\ntotal computational cost, while ¯ρ, ¯µ, ¯γ do not.\nWith a certain range, performance increases when\nthe value of Nrecur, Niter increase5, and the trade-\noff between generation quality and computation\ntime is presented in Figure 3: recurrence leads to\na Nrecur times cost with clear performance gain;\niteration (on x0|t) results in less increase of com-\nputation time, and its effect plateaus. In practice,\nusers can determine their values based on compu-\ntation resources, but an upper bound of 4 suffices\nto unlock a near-optimal performance.\nSearching strategy. The above analysis successfully simplify the task-specific hyper-parameter\nsearch problem without significant performance sacrifice. It remains to be decided the scalar values\n¯ρ, ¯µ, ¯γ. Here we propose a strategy based on beam search to effectively and efficiently select their\nvalues. Specifically, our searching strategy starts with an initial set T = {(¯ρinit, ¯µinit, ¯γinit)}, where\nthese initial values are small enough to approximate TFG as an unconditional generation. At each\nsearching step, for each tuple in T, we separately double the values of ¯ρ, ¯µ, and ¯γ to generate up to\n3|T| new configurations. We conduct a small-sized generation trial for each new configuration and\nupdate T to be the top K configurations with the highest evaluation results that are determined by\nuser requirements (e.g., accuracy, FID, or a combination). This iterative process is repeated until T\nstabilizes or the maximum number of search steps is reached. Notice that this process is conducted\nwith a much smaller sample size, and consequently, the computation time is highly controllable.\n5\nBenchmarking\nThis section comprehensively benchmarks training-free guidance under the TFG framework and the\ndesign searching strategy in Section 4. We consider 7 datasets, 16 different tasks, and 40 individual\ntargets with a total experimental cost of more than 2,000 A100 GPU hours. For comparison, we also\nrun experiments for each of the existing methods (where the design searching is conducted in the\ncorresponding subspace). All methods, tasks, search strategies, and evaluations are unified in our\ncodebase, with details specified in Appendices D and E.\n5.1\nSettings\nDiffusion models. (1) CIFAR10-DDPM [48] is a U-Net [54] model trained on CIFAR10 [30]\nimages. (2) ImageNet-DDPM [7] is an larger U-Net model trained on ImageNet-1k [55] images. (3)\nCat-DDPM is trained on Cat [12] images. (4) CelebA-DDPM is trained on CelebA-HQ dataset [26]\nthat consists millions of human facial images. (5) Molecule-EDM [24] is an equivariant diffusion\n5We notice that the FID worsens when Nrecur or Niter are too large (e.g., 10).\n8\n\nTable 3: Benchmarking TFG and existing algorithms on 16 task types and 40 individual targets. Each cell\npresents the guidance validity/generation fidelity averaged across multiple targets in the task (e.g., labels, image\nstyles). The best guidance validity is bold, and the second best underline. The relative improvement of guidance\nvalidity is computed between TFG and the existing method with the highest guidance validity.\nTask-ID\nDPS\nLGD\nFreeDoM\nMPGD\nUGD\nTFG\nRel. Improvement\nDeblur (↓, ↓)\n0.390 / 98.3\n0.270 / 85.1 0.245 / 87.4 0.177 / 69.3 0.200 / 69.3 0.150 / 64.5\n+15.3%\nSuper resolution (↓, ↓)\n0.420 / 109\n0.360 / 96.7 0.191 / 74.5 0.283 / 82.0 0.249 / 75.9 0.190 / 65.9\n+0.524%\nGender+Age (↑, ↓)\n71.6 / -4.26\n52.0 / -5.10 68.7 / -3.89 68.6 / -4.79 75.1 / -4.37 75.2 / -3.86\n+0.133%\nGender+Hair (↑, ↓)\n73.0 / -3.90\n55.0 / -5.00 67.1 / -3.50 63.9 / -4.33 71.3 / -4.12 76.0 / -3.60\n+4.11%\nCIFAR10 (↑, ↓)\n50.1 / 172\n32.2 / 102\n34.8 / 135\n38.0 / 88.3\n45.9 / 94.2\n52.0 / 91.7\n+3.59%\nImageNet (↑, ↓)\n38.8 / 193\n11.5 / 210\n19.7 / 200\n6.80 / 239\n25.5 / 205\n40.9 / 176\n+5.41%\nFine-grained (↑, ↓)\n0.00 / 348\n0.48 / 246\n0.58 / 258\n0.58 / 249\n1.07 / 255\n1.27 / 256\n+18.7%\nStyle Transfer (↓, ↑)\n5.06 / 31.7\n5.42 / 31.3\n5.26 / 31.2\n4.08 / 31.5\n4.97 / 31.5\n3.16 / 29.0\n+22.5%\nPolarizability α (↓, ↑)\n51169.7 / 92.3 7.155 / 84.3 5.922 / 88.0 4.26 / 88.4\n5.45 / 73.8\n3.90 / 84.2\n+8.45%\nDipole µ (↓, ↑)\n63.2 / 77.3\n1.51 / 86.6\n1.35 / 89.5\n1.51 / 73.5\n1.56 / 57.6\n1.33 / 74.9\n+1.48%\nHeat capacity Cv (↓, ↑)\n5.26 / 78.4\n3.77 / 77.1\n2.84 / 90.9\n2.86 / 86.1\n3.02 / 84.0\n2.77 / 85.5\n+2.57%\nHighest MO energy ϵHOMO (↓, ↑)\n0.744 / 83.8\n0.664 / 66.4 0.623 / 62.3 0.554 / 53.4 0.582 / 58.2 0.568 / 77.3\n-2.53%\nLowest MO energy ϵLUMO (↓, ↑)\nNA / NA\n1.20 / 90.9\n1.16 / 90.2\n1.06 / 82.2\n1.27 /85.1\n0.984 / 80.1\n+7.17%\nMO energy gap ϵ∆(↓, ↑)\n1.38 / 75.7\n1.19 / 85.3\n1.17 / 88.5\n1.07 / 72.5\n1.15 / 75.7 0.893 / 62.5\n+16.7%\nAudio declipping (↓, ↓)\n633 / 3.60\n157 / 2.33\n126 / 0.173 178 / 0.402 150 / 0.262 101 / 0.172\n+19.8%\nAudio inpainting (↓, ↓)\n643 / 4.71\n103 / 2.22\n41.3 / 0.08\n608 / 4.63\n116 / 0.53\n36.3 / 0.06\n+12.1%\nmodel pretrained on molecule dataset QM9 [50] that performs molecule generation from scratch.\n(6) Stable-Diffusion (v1.5) [53] is a latent text-to-image model that generate images with text\nprompts. (7) Audio-Diffusion6 is a audio diffusion model based on DDPM trained to generate mel\nspectrograms of 256x256 corresponding to 5 seconds of audio.\nTasks. Our tasks (Table 2) cover a wide range of interests, including Gaussian deblur, super-resolution,\nlabel guidance, style transfer, molecule property guidance, audio declipping, audio inpainting, and\nguidance combination. Each task is run on multiple datasets or with multiple targets (e.g., different\nlabels, molecular properties, styles).\nOther settings. We consistently set the time step T = 100 and the DDIM parameter η = 1. We\nconsider Nrecur = 1, Niter = 4 and use a single sample for Implicit Dynamic (Line 4) throughout\nall experiments and methods for fair comparison. For TFG, the structures of ρ and µ are set to\n“increase” and the scalars ¯ρ, ¯µ, ¯γ are determined via our searching strategy. We follow the setting in\noriginal papers if they specify their hyper-parameters. blueFor specific tricks in the code that are not\nmentioned in papers, we choose to align with original papers. Otherwise, values are determined via\nsearching with 1/8 of the sample size and a maximum search step of 6. For fairness of comparison,\nwe use accuracy as the metric during the search and compare different algorithms on the metric, but\nwe report both accuracy and FID.\n5.2\nBenchmarking results\nWe compare all six methods in Table 3. TFG outperforms existing algorithms in 13 over 14 settings,\nachieving an average guidance validity improvement of 7.4% compared to the best existing algorithm.\nNotice that we do not compare with the best algorithm in terms of generation fidelity because\nobtaining high realness samples is not our objective in training-free guidance, and an unconditional\nmodel suffices to generate high realness samples (with extremely low validity). Interestingly, different\nmethods achieve the second best performance on different tasks, suggesting the variance of these\nmethods, while TFG is consistent thanks to the unification.\nWe want to highlight that despite the superior performance of TFG, the key intention of our ex-\nperiments is not restrained to comparing TFG with existing methods, but more importantly to\nsystematically benchmark under the training-free guidance setting to see how much we have achieved\nin various tasks with different difficulties. Below we go through each task separately and conduct\nrelevant ablation studies to provide a more fine-grained analysis.\nFine-grained label guidance. In addition to the standard label guidance, we for the first time study\nthe out-of-distribution fine-grained label guidance under the training-free setting, a problem where\nno existing training-based methods are available. We consider the bird-species guidance using an\n6https://huggingface.co/teticio/audio-diffusion-256\n9\n\nEfficienNet trained to classify 525 fine-grained bird species. This problem remains highly difficult\nfor leading text-to-image generative models such as DALLE. Under recurrence, TFG can generate at\nmost 2.24% of accurate birds, compared with the unconditional generation rate of 0.\nTable 4: The accuracy / FID for TFG with\ndifferent recurrence step Nrecur on three label\nguidance datasets, averaged across all labels.\nRecurrence\n1\n2\n4\nCIFAR10\n52.0 / 91.7\n66.8 / 88.7\n77.1 / 73.9\nImageNet\n40.9 / 177\n52.3 / 163\n59.8 / 165\nFine-grained\n1.27 / 256\n1.66 / 259\n2.24 / 259\nRecurrence on label guidance. We go back to the\nfailure case we study in Section 3, i.e., the stan-\ndard label guidance problem on CIFAR10 where the\ntraining-based method offers an 85% accuracy, while\nthe accuracy of TFG without recurrence accuracy is\n52% only. As presented in Table 4, increasing Nrecur\nsignificantly closes the gap from 33% to 8%. Similar\nimprovement is observed in other datasets as well.\nTable 5: The accuracy of multi-label guid-\nance on CelebA, where labels 0 and 1 cor-\nrespond to female and male (gender), non-\nblonde and blonde (hair color), and young\nand old (age). The accuracy is lower for mi-\nnority groups, indicating an implicit bias in\nthe generation process. Despite this, it is still\nmuch higher than unconditional generation.\nTarget label\n0+0\n0+1\n1+0\n1+1\ngender + hair\n92.2\n72.7\n89.8\n46.7\ngender + age\n92.9\n73.6\n93.6\n69.1\nMultiple guidance and bias mitigation. We next\nconsider the scenario with multiple targets: control\nthe generation of human faces based on gender and\nhair color (or age) using two predictors. It is well\nknown that the label imbalance in CelebA-HQ causes\nclassifiers to focus on spurious correlations [76], such\nas using hair colors to classify gender, a biased feature\nwe aim to avoid. The stratified performance of TFG\non “gender + age” and “gender + hair” guidance are\npresented in Table 5. Despite the highly disparate\nperformance, training-free guidance largely alleviates\nthe imbalance: only 1% of images in CelebA are “male\n+ blonde hair”, while the generated accuracy is 46.7%.\nMolecule property guidance. To our knowledge, we are the first to study training-free guidance\nfor molecule generation. We interestingly find in Table 3 that TFG is effective in guiding molecules\ntowards desirable properties, yielding the highest guidance validity on 5 out of 6 targets with 5.64%\nMAE improvement over existing methods, verifying the generality of our approach as a unified\nframework in completely unseen domains. Notice that, unlike images, molecules with better validity\nusually have lower generation fidelity, a finding reflected in previous work [3].\nAudio Guidance. We extend our investigation to the audio modality, where TFG achieves significant\nrelative improvements over existing methods. Given that the audio domain is rarely explored in\ntraining-free guidance literature, our benchmarks will contribute to future research in this area.\n6\nDiscussions and Limitations\nRecently, training-free guidance for diffusion models has gained increasing attention and has been\nadopted in various applications. TFG is based on an extensive literature review over ten algorithmic\npapers for different purposes, including images, audio, molecules, and motions [34, 8, 43, 32, 19, 13,\n16, 15, 45, 38, 68]. While we incorporate several key algorithms into our framework, we acknowledge\nthat encompassing all approaches is impossible, as it would make the unification bloated and less\npractical. We seek to find a balance point by unifying most representative algorithms while keeping\nthe techniques clear and easily studied.\nAn often discussed problem is why we care about training-free guidance, given the ever-growing\ncommunity of language-based generative models such as the image generator of GPT4. In practice,\nthere are countless conditional generation tasks where the conditions are hard to accurately convey\nto or represent by language encoders. For instance, it can fail to under a complex property of a\nmolecule or generate CelebA-style faces. We give an illustrative analysis in Appendix A.1. Despite\nthat training-free guidance is important, this paper does not systematically analyze what types of\nconditional generation are, in general, more suitable for the framework and what types are for\nlanguage-based models. That said, training-free guidance is fundamentally difficult due to the\nmisalignment between the training objective of target predictors and the diffusion, with a more\ndetailed discussion in Appendix A.2. This paper does not comprehensively analyze this misalignment,\nand the gap between training-based and TFG remains high in some tasks like molecule property\nguidance. We hope that future works can analytically dive into these problems.\n10\n\nReferences\n[1] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf\nRonneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure\nprediction of biomolecular interactions with alphafold 3. Nature, pages 1–3, 2024.\n[2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,\nJonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 843–852,\n2023.\n[3] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant\nenergy-guided SDE for inverse molecular design. In The Eleventh International Conference on\nLearning Representations, 2023.\n[4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional\nimage generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021.\n[5] Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying\nmmd gans. arXiv preprint arXiv:1801.01401, 2018.\n[6] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical\nimage analysis, 80:102479, 2022.\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems, 34:8780–8794, 2021.\n[8] Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir, Emile Mathieu, Urszula J\nKomorowska, and Pietro Lio. A framework for conditional diffusion modelling with applications\nin motif scaffolding for protein design, 2024.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[10] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus,\nJascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle:\nCompositional generation with energy-based diffusion models and mcmc. In International\nconference on machine learning, pages 8489–8510. PMLR, 2023.\n[11] Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical\nAssociation, 106(496):1602–1614, 2011.\n[12] Jeremy Elson, John (JD) Douceur, Jon Howell, and Jared Saul. Asirra: A captcha that exploits\ninterest-aligned manual image categorization. In Proceedings of 14th ACM Conference on\nComputer and Communications Security (CCS). Association for Computing Machinery, Inc.,\nOctober 2007.\n[13] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with\ndifferentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024.\n[14] Alexandros Graikos, Srikar Yellapragada, and Dimitris Samaras. Conditional generation from\nunconditional diffusion models using denoiser representations. arXiv preprint arXiv:2306.01900,\n2023.\n[15] Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, and Josh Susskind.\nControl3diff: Learning controllable 3d diffusion models from single-view images. In 2024\nInternational Conference on 3D Vision (3DV), pages 685–696. IEEE, 2024.\n[16] Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, and Dongsheng Li. Training-\nfree multi-objective diffusion model for 3d molecule generation. In The Twelfth International\nConference on Learning Representations, 2023.\n11\n\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[18] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim,\nWei-Hsiang Liao, Yuki Mitsufuji, J Zico Kolter, Ruslan Salakhutdinov, and Stefano Ermon.\nManifold preserving guided diffusion. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[19] Carlos Hernandez-Olivan, Koichi Saito, Naoki Murata, Chieh-Hsin Lai, Marco A Martínez-\nRamirez, Wei-Hsiang Liao, and Yuki Mitsufuji. Vrdmg: Vocal restoration via diffusion posterior\nsampling with multiple guidance. In ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 596–600. IEEE, 2024.\n[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840–6851, 2020.\n[22] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine\nLearning Research, 23(47):1–33, 2022.\n[23] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n[24] Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant\ndiffusion for molecule generation in 3d. In International conference on machine learning, pages\n8867–8887. PMLR, 2022.\n[25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer\nand super-resolution. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694–711. Springer, 2016.\n[26] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n[27] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration\nmodels. Advances in Neural Information Processing Systems, 35:23593–23606, 2022.\n[28] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr\\’echet audio dis-\ntance: A metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466,\n2018.\n[29] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\n[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n[31] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-\nbased learning. Predicting structured data, 1(0), 2006.\n[32] Jean-Marie Lemercier, Julius Richter, Simon Welker, Eloi Moliner, Vesa Välimäki, and Timo\nGerkmann. Diffusion models for audio restoration. arXiv preprint arXiv:2402.09821, 2024.\n[33] Anat Levin, Yair Weiss, Fredo Durand, and William T Freeman. Understanding and evaluating\nblind deconvolution algorithms. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 1964–1971. IEEE, 2009.\n[34] Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Con-\ntrollable music production with diffusion models and guidance gradients. arXiv preprint\narXiv:2311.00613, 2023.\n12\n\n[35] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv\npreprint arXiv:2301.12503, 2023.\n[36] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining\nXie. A convnet for the 2020s, 2022.\n[37] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes\n(celeba) dataset. Retrieved August, 15(2018):11, 2018.\n[38] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive\nenergy prediction for exact energy-guided diffusion sampling in offline reinforcement learning.\nIn International Conference on Machine Learning, pages 22825–22855. PMLR, 2023.\n[39] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n11461–11471, June 2022.\n[40] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n2837–2845, 2021.\n[41] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. A conditional point\ndiffusion-refinement paradigm for 3d point cloud completion. arXiv preprint arXiv:2112.03530,\n2021.\n[42] Ross A Maller, Gernot Müller, and Alex Szimayer. Ornstein–uhlenbeck processes and exten-\nsions. Handbook of financial time series, pages 421–437, 2009.\n[43] Eloi Moliner, Jaakko Lehtinen, and Vesa Välimäki. Solving audio inverse problems with a\ndiffusion model. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1–5. IEEE, 2023.\n[44] Meinard Müller. Dynamic time warping. Information retrieval for music and motion, pages\n69–84, 2007.\n[45] Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino,\nVishal M Patel, and Tim K Marks. Steered diffusion: A generalized framework for plug-and-\nplay conditional image synthesis. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 20850–20860, 2023.\n[46] Kamal Nasrollahi and Thomas B Moeslund. Super-resolution: a comprehensive survey. Machine\nvision and applications, 25:1423–1468, 2014.\n[47] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[48] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\nmodels. In International conference on machine learning, pages 8162–8171. PMLR, 2021.\n[49] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\n[50] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld.\nQuantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.\n[51] Herbert E Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics:\nFoundations and basic theory, pages 388–394. Springer, 1992.\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 10684–10695, 2022.\n13\n\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June\n2022.\n[54] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks\nfor biomedical image segmentation. In Medical image computing and computer-assisted\nintervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9,\n2015, proceedings, part III 18, pages 234–241. Springer, 2015.\n[55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\n(IJCV), 115(3):211–252, 2015.\n[56] Babak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning\nthe right metric on the right feature. arXiv preprint arXiv:1505.00855, 2015.\n[57] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural\nnetworks. arXiv preprint arXiv:2102.09844, 2021.\n[58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. Advances in\nNeural Information Processing Systems, 35:25278–25294, 2022.\n[59] Alexander Shapiro. Monte carlo sampling methods. Handbooks in operations research and\nmanagement science, 10:353–425, 2003.\n[60] Herbert A Simon. Spurious correlation: A causal interpretation. Journal of the American\nstatistical Association, 49(267):467–479, 1954.\n[61] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International conference on machine\nlearning, pages 2256–2265. PMLR, 2015.\n[62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[63] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz,\nYongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable\ngeneration. In International Conference on Machine Learning, pages 32483–32498. PMLR,\n2023.\n[64] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. Advances in neural information processing systems, 34:1415–\n1428, 2021.\n[65] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\ndistribution. Advances in neural information processing systems, 32, 2019.\n[66] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n[67] Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of\ndependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical\nStatistics and Probability, Volume 2: Probability Theory, volume 6, pages 583–603. University\nof California Press, 1972.\n[68] Haoyuan Sun, Bo Xia, Yongzhe Chang, and Xueqian Wang. Generalizing alignment paradigm\nof text-to-image generation with preferences through f-divergence minimization. arXiv preprint\narXiv:2409.09774, 2024.\n14\n\n[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efficient image transformers & distillation through attention, 2021.\n[70] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion\nnull-space model. In The Eleventh International Conference on Learning Representations,\n2023.\n[71] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning for image super-resolution: A\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 43(10):3365–3387,\n2020.\n[72] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E\nEisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo\ndesign of protein structure and function with rfdiffusion. Nature, 620(7976):1089–1100, 2023.\n[73] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric\nlatent diffusion models for 3d molecule generation. In International Conference on Machine\nLearning, pages 38592–38610. PMLR, 2023.\n[74] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geo-\nmetric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923,\n2022.\n[75] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi\nWang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan\nHendrycks, Yixuan Li, and Ziwei Liu. Openood: Benchmarking generalized out-of-distribution\ndetection. 2022.\n[76] Haotian Ye, James Zou, and Linjun Zhang. Freeze then train: Towards provable representation\nlearning under spurious correlations and feature noise. In International Conference on Artificial\nIntelligence and Statistics, pages 8968–8990. PMLR, 2023.\n[77] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5,\n2022.\n[78] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-\nfree energy-guided conditional diffusion model. Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2023.\n[79] Ning Zhang, Jeff Donahue, Ross Girshick, and Trevor Darrell. Part-based r-cnns for fine-\ngrained category detection. In Computer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 834–849. Springer,\n2014.\n[80] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 586–595, 2018.\n15\n\nA\nThe Motivation of Studying Training-free Guidance\nIn this section, we argue that training-free guidance using off-the-shelf models is a crucial and timely\nresearch problem deserving more attention and effort. We begin by providing an illustrative analysis\nthat highlights the limitations of current strong text-to-image generative models, underscoring the\nnecessity for training-free guidance. Furthermore, we assert that training-free guidance remains a\nsignificant challenge, with previous literature underestimating its complexity. Given its necessity and\ninherent difficulties, we call for increased focus from the research community on this problem and\noffer our benchmarks and code base to help accelerate progress in this area.\nA.1\nFailure case of image generation with GPT4\nFigure 4: Prompting GPT4 to generate property guided molecules. It is hard for the image\ngenerator to understand the target and generate faithful samples. In this dialog, GPT4 claims to\ngenerate a benzene molecule but the sample is apparently not a benzene. There are also many invalid\ncarbon atoms with more than 4 bonds and the polarizability target is not achieved.\nIt’s hard for GPT4 to understand targets.\nIn Figure 4, we ask GPT-4 to generate a molecule with\npolarizability α = 3, which is a task we use to evaluate training-free guidance (refer to Figure 16\nfor visualization). We found that the GPT-4 generated molecule is apparently invalid and unrealistic:\nthe generated molecule contains many carbon atoms with more than 4 bonds (the maximum allowed\nnumber is 4); and the generated molecule is apparently not a benzene which is claimed by the text\noutputs. From this case we may understand that it is hard to follow diverse user-defined instructions\nfor the foundational generative models, where the user-defined targets may be subtle, fine-grained,\ncombinatorial, and open-ended.\n16\n\nTo this end, training-free guidance offers two key advantages: (1) It allows for greater specificity in\ntarget requirements by enabling the use of a differentiable objective function, making the generation\nprocess more steerable; (2) The objective function is plug-and-play, facilitating the addition of new\ntargets and tasks to a pre-trained generative model. Since there is no need to retrain the diffusion or\nprediction models, this approach makes the generative process lightweight and applicable to various\ndownstream tasks.\nIt’s hard for GPT4 to capture the targeted distribution.\nAnother important metrit for training-\nfree guidance is the flexibility of choosing diffusion models. For the same target, we can switch from\ndifferent diffusion models to change the unconditional background distribution. For example, it is\nhard for GPT4 to generate CelebA-like samples though it “knows CelebA dataset very well”:\nFigure 5: Prompting GPT4 to generate CelebA-like images. We first prompt ChatGPT to probe\nits knowledge of CelebA dataset and then ask it to generate a young man figure in CelebA style.\nHowever, the generated figure is apprently not in the distribution of CelebA (refer to Figure 12) for\ncomparison.\nThe flexibility to use different diffusion models provides an opportunity to generate a wider range\nof user-defined targets. With training-free guidance, individuals can select their preferred diffusion\nmodel to establish the background distribution and use the prediction model to steer the generation\ntowards specific properties. This approach may represent a future direction for human-AI interaction.\n17\n\nFigure 6: (Left) The accuracy and FID of different methods under different settings on CIFAR10 [30],\naverage across ten labels and 2048 samples per label. The suffix number in UGD and FreeDoM\nrepresents recurrent step Nrecur, and (fake) stands for a synthetic setting where we apply a training-\nbased classifier but set t = 0 and use the same training-free guidance methods. A huge performance\ngap between different settings suggests the intrinsic difficulty of training-free guidance. (Right)\nIllustration of generated “ship” using MPGD under different settings (top) and the sampling trajectory\nof the predicted clean image x0|t (down).\nA.2\nThe fundamental challenge of training-free guidance\nDespite the array of algorithms available and their reported successes in various applications, we\nconduct a case study on CIFAR10 [30] to illustrate the challenging nature of training-free guidance\nand the insufficiency of existing methods. Specifically, we compare the training-based approach and\ntraining-free approach for the label guidance task on CIFAR10, with the diffusion model pretrained\nby [7], the training-based time-dependent classifier f(x, t) by [7], and the training-free standard\nlabel classifier f(x) pretrained only on clean CIFAR10 by [9]. and a “fake” training-free classifier\ndefined as f(x, t)|t=0. The first serves as the oracle benchmark, while the second corresponds to\nthe standard training-free guidance. The third setting, as considered in LGD [63], uses a “fake”\ntraining-free classifier since its parameters are shared across different time steps t during training,\nresulting in an implicit regularization that is not available for practical predictors. This setting serves\nas a comparison to help identify the difficulty of training-free guidance.\nQuantitative and qualitative results are shown in Figure 6. All training-free approaches significantly\nunder-perform training-based guidance, with a significant portion of generated images being highly\nunnatural (when guidance is strong) or irrelevant to the label (when guidance is weak). A more clear\nillustration on this can be found in Figure 7. In terms of “fake” classifiers, it leads to a remarkable\ndifference from real training-free classifiers even under identical experimental settings. It generates\nFigure 7: Illustration on CIFAR10 dogs generated with different algorithms. Compared with training-\nbased method, training-free methods fall behind but TFG significantly outperforms existing methods.\n18\n\nmuch less messy images due to the implicit regularization from the training process, where noisy\nimages are also “seen” (although t is fixed to 0 upon guidance). Unfortunately, such types of\npredictors are inaccessible in practice (otherwise, we can use classifier-based guidance directly).\nFrom the comparison, we know that the key challenge of training-free guidance is the lack of a\n“smoothing” classifier that can produce faithful guidance in the “unseen” noisy image space.\nThe observation largely uncovers the essential difficulties of training-free guidance, and motivates us\nto systematically study techniques that can improve generation quality. Unfortunately, comparisons\nbetween existing techniques are ambiguous since different methods are tested on distinct and primarily\nqualitative applications, which in turn hinders the in-depth study in this field. To this end, we resolve\nto revisit this complicated scenario and design a clear and comprehensive framework for training-free\nguidance.\n19\n\nB\nPseudo-code and schematics\nWe have presented the pseudo-code of TFG in Algorithm 1. Below, we provide a copy of the DPS\n(Algorithm 2), MPGD (Algorithm 3), FreeDoM (Algorithm 4), UGD (Algorithm 5), and LGD\n(Algorithm 6). Notice that LGD does not provide a pseudo-code, and we present their algorithm\nfollowing their paper as a modification of DPS. We do not change the original algorithms’ notations\nfor reference. Please see the proof in Appendix C for the equivalence analysis. We provide a\nschematic of existing algorithms in Figure 8.\nAlgorithm 2 DPS - Gaussian\nRequire: N, y, {ζi}N\ni=1, {˜σi}N\ni=1\nxN ∼N(0, I)\nfor i = N −1 to 0 do\nˆs ←sθ(xi, i)\nˆx0 ←\n1\n√¯αi (xi + (1 −¯αi)ˆs)\nz ∼N(0, I)\nx′\ni−1 ←\n√αi(1−¯αi−1)\n1−¯αi\nxi +\n√\n¯αi−1βi\n1−¯αi\nˆx0 + ˜σiz\nxi−1 ←x′\ni−1 −ζi∇xi∥y −A(ˆx0)∥2\n2\nend for\nreturn ˆx0\nAlgorithm 3 MPGD for pixel diffusion models\n1: xT ∼N(0, I)\n2: for t = T, . . . , 1 do\n3:\nϵt ∼N(0, I)\n4:\nx0|t =\n1\n√¯αt (xt −√1 −¯αtϵθ(xt, t))\n5:\nif requires manifold projection then\n6:\nx0|t = gM(x0|t, L(x0|t; y), ct)\n7:\nelse\n8:\nx0|t = x0|t −ct∇x0|tL(x0|t; y)\n9:\nend if\n10:\nxt−1 = √¯αt−1x0|t\n11:\n+\np\n1 −¯αt−1 −σ2\nt ϵθ(xt, t) + σtϵt\n12: end for\n13: return x0\nAlgorithm 4 FreeDoM + Efficient Time-Travel Strategy\nRequire: condition c, unconditional score estimator s(·, t), time-independent distance measuring function Dθ(c, ·), pre-defined parameters\nβt, ¯αt, learning rate ρt, and the repeat times of time travel of each step {r1, · · · , rT }.\nxT ∼N (0, I)\nfor t = T, ..., 1 do\nfor i = rt, ..., 1 do\nϵ1 ∼N (0, I) if t > 1, else ϵ1 = 0.\nxt−1 = (1 + 1\n2 βt)xt + βts(xt, t) + √βtϵ1\nx0|t =\n1\n√¯\nαt (xt + (1 −¯αt)s(xt, t))\ngt = ∇xtDθ(c, x0|t(xt)))\nxt−1 = xt−1 −ρtgt\nif i > 1 then\nϵ2 ∼N (0, I)\nxt = √1 −βtxt−1 + √βtϵ2\nend if\nend for\nend for\nreturn x0\n20\n\nAlgorithm 5 Universal Guidance (its α is our ¯α)\nParameter: Recurrent steps k, gradient steps m for backward guidance and guidance strength s(t),\nRequired: zT sampled from N(0, I), diffusion model ϵθ, noise scales {αt}T\nt=1, guidance function f, loss\nfunction ℓ, and prompt c\nfor t = T, T −1, . . . , 1 do\nfor n = 1, 2, . . . , k do\nCalculate ˆz0 as zt−(√1−αt)ϵθ(zt,t)\n√αt\nˆϵθ(zt, t) = ϵθ(zt, t) + s(t) · ∇ztℓ(c, f(ˆz0))\nif m > 0 then\nCalculate ∆z0 by minimizing ℓ(c, f(ˆz0 + ∆)). with m steps of gradient descent\nPerform backward universal guidance by ˆϵθ ←ˆϵθ −\np\nαt/(1 −αt)∆z0\nend if\nzt−1 ←S(zt, ˆϵθ, t)\nϵ′ ∼N(0, I)\nzt ←\np\nαt/αt−1zt−1 +\np\n1 −αt/αt−1ϵ′\nend for\nend for\nAlgorithm 6 LGD (from DPS)\nRequire: N, y, {ζi}N\ni=1, {˜σi}N\ni=1, n\nxN ∼N(0, I)\nfor i = N −1 to 0 do\nˆs ←sθ(xi, i)\nˆx0 ←\n1\n√¯αi (xi + (1 −¯αi)ˆs)\nz ∼N(0, I)\nxi−1 ←\n√αi(1−¯αi−1)\n1−¯αi\nxi +\n√\n¯αi−1βi\n1−¯αi\nˆx0 + ˜σiz\nxi−1 ←xi−1 −ζi∇xi log\n\u0010\n1\nn\nPn\nj=1 exp(−ℓy(x(j)\ni )\n\u0011\n▷x(j)\ni\nsampled i.i.d. from N(ˆx0,\nσ2\ni\n1+σ2\ni I)\nend for\nreturn ˆx0\n\u001f\u001e\n\u001d\u001d\u001d\n\u001f\u001c\u001b\u001e\u001a\u0019\n\u001f\u001c\u001b\u0018\u0017\u0019\n\u001f\u0018\n\u001f\u001c\u001b\u0018\n\u001f\u0018\u001a\u0019\n\u001d\u001d\u001d\n\u001f\u001c\u001b\u0018\u001a\u0019\n\u001f\u001c\u001b\u0019\n\u001f\u001c\nthe \u0018-th step\n(a)\n(b)\niterate\n\u001f∇\u001f\u0018\u0016logf(\u001f\u001c\u001b\u0018)\n\u001f∇\u001f\u0018\u0016logf(\u001f\u001c\u001b\u0018)\n\u001f\u0018\n\u001f\u0018\u001a\u0019\n\u001f\u0018\u001a\u0019\nDPS\nDiffusion\nsampling\nprocess\n\u001f∇\u001f\u0018\u0016logf(\u001f\u001c\u001b\u0018)\n\u001f\u0018\n\u001f\u0018\u001a\u0019\n\u001f\u0018\u001a\u0019\nUGD\nLGD\n\u001f\u0018\n\u001f\u0018\u001a\u0019\nMPGD\n\u001f\u0018\n\u001f\u0018\u001a\u0019\n\u001f\u0018\u001a\u0019\nFreeDoM\nlogf(\u001f\n\u001f\u001e\u001d)\nlogf(\u001f\n\u001f\u001c\u001d)\nlogf(\u001f\n\u001f\u001b\u001d)\n\u001f\u001c\u001b\u0018\n\u001f\n\u001f\u001e\u001d\n\u001f\n\u001f\u001c\u001d\n\u001f\n\u001f\u001b\u001d\n\u001f\u0018\u001a\u0019\n\u001f\u0018\u001a\u0019\n\u001f\u0018\n\u001f∇\u001f\u0018\u0016logΣ\u0015\u0016f(\u001f\u001f\u0015\u001d)\niterate\n\u001f∇\u001f\u001c\u001b\u0018\u0016logf(\u001f\u001c\u001b\u0018)\n\u001f\u001c\u001b\u0018\n\u001f\u001c\u001b\u0018\n\u001f\u001c\u001b\u0018\n\u001f\u001c\u001b\u0018\n\u001f\u001c\u001b\u0018\n\u001f∇\u001f\u001c\u001b\u0018\u0016\n\u001f∇\u001f\u001c\u001b\u0018\u0016\nFigure 8: (a) The reversed diffusion process. (b) Illustration of different training-free guidance\nalgorithms at the t-th reversed diffusion step.\n21\n\nC\nProofs\nWe prove Theorem 3.2 and Lemma 3.3 below.\nC.1\nProof of Theorem 3.2\nProof. For each algorithm, we prove the equivalence of design space separately below. Notice that\nwhen ¯γ = 0, ˜f degrades back to f.\nMPGD (Algorithm 3).\nBelow we demonstrate that any hyper-parameter {ct}T\nt=1 in Algorithm 3 is\nequivalent to the TFG with f(x0|t) = exp{−L(x0|t; y)} and hyper-parameter\nNrecur = 1, Niter = 1, ¯γ = 0, ρ = 0, µ = (c1, · · · , cT )⊤.\nTo show this, notice that since both ρ and ¯γ are zero, Line 4 and Line 7 take no effect. When using\nthe identical sampling function (Line 9), TFG generates xt−1 using\nxt−1 = Sample(xt, x0|t, t) + ct\n√¯αt−1∇x0|t log f(x0|t)\n= √¯αt−1x0|t +\nq\n1 −¯αt−1 −σ2\nt\nxt −√¯αtx0|t\n√1 −¯αt\n+ σtϵt + ct\n√¯αt−1∇x0|t log f(x0|t)\n= √¯αt−1(x0|t −ct∇x0|tL(x0|t; y)) +\nq\n1 −¯αt−1 −σ2\nt ϵθ(xt, t) + σtϵt,\nwhich is exactly the formula used in MPGD.\nDPS (Algorithm 2) and FreeDoM (Algorithm 4).\nWe prove both algorithms together as DPS is\na special case of FreeDoM (without recurrence). Specifically, any hyper-parameter {ρt}T\nt=1, time\ntravel step r and distance function D(c, ·) in Algorithm 4 is equivalent to TFG with f(x0|t) =\nexp{−D(c, x0|t)} and hyper-parameter\nNrecur = r, Niter = 0, ¯γ = 0, ρ = (√α1ρ1, · · · , √αT ρT )⊤, µ = 0.\nTo show this, notice that both algorithms have the identical resampling step from xt−1 to xt, so it\nsuffices to prove that the formula to generate xt−1 in each recurrent step is the same. For FreeDoM,\nwe have\nxt−1 = Sample(xt, x0|t, t) −ρt∇xtD(c, x0|t)\n= Sample(xt, x0|t, t) −(√αtρt)∇xt log f(x0|t)/√αt,\nand the last line equals the combination of Line 7 and Line 7 in TFG.\nLGD (Algorithm 6).\nAny hyper-parameter {ζt}T\nt=1, n in LGD is equivalent to TFG with f(x) =\nexp{−ℓy(x)}, sample size n in Line 4, and hyper-parameter\nNrecur = 1, Niter = 0, ¯γ = 1, ρ = 0, µ = (ζ1, · · · , ζT )⊤.\nNotice that ¯αt in TFG equals 1/(1 + σ2\nt ) in the DPS algorithm. With this, the equivalence is clear\nfrom the pseudo-code of both algorithms.\nUGD (Algorithm 5).\nAny hyper-parameter k, m, s(t) in UGD is equivalent to TFG with f(x) =\nexp{−ℓ(c, f(x))} and hyper-parameter\nNrecur = k, Niter = m, ¯γ = 0,\nρ = (−√α1s(1)δ1, · · · , −√αT s(T)δT )⊤, µ = (−\nr\nα1\n1 −¯α1\nδ1, · · · , −\nr\nαT\n1 −¯αT\nδT )⊤,\nwhere δt is the coefficient of ϵθ(xt, t) in sampler S in the UGD algorithm (which is negative). To show\nthis, notice that ∆t = ρt∇xt log ˜f(x0|t) = −ρt∇xtℓ(c, f(x0|t)) = √αts(t)δt∇xtℓ(c, f(x0|t)). By\nreplacing this into Line 9, the equivalence of guidance Variance Guidance can be easily observed. A\nsimilar deduction can be made for the mean guidance as well.\n22\n\nC.2\nProof of Lemma 3.3\nProof. Recall that x0|t = xt−√1−¯αtϵθ(xt,t)\n√¯αt\n. According to a simple chain rule, we have\n∆t = ρt∇xt log ˜f(x0|t)\n(9)\n= ρt\nI −√1 −¯αt∇xtϵθ(xt, t)\n√¯αt\n∇x0|t log ˜f(x0|t).\n(10)\nThe perfect optimization assumption implies the relationship between ϵθ and the score of pt(xt),\nwhich we leverage and obtain\n∆t = ρt\nI −√1 −¯αt∇xtϵθ(xt, t)\n√¯αt\n∇x0|t log ˜f(x0|t)\n(11)\n= ρt\nI + (1 −¯αt)∇2\nxt log pt(xt)\n√¯αt\n∇x0|t log ˜f(x0|t).\n(12)\nThus, it remains to draw a connection between the conditional covariance between Σ0|t and\n∇2\nxt log pt(xt). Omit the subscript xt in ∇xt, we have\n∇2 log pt(xt) = ∇2pt(xt)\npt(xt)\n−∇log pt(xt)(∇log pt(xt))⊤\n(13)\n=\n1\npt(xt)\nZ\nx0∈X\np0(x0)∇2pt|0(xt|x0)dx0 −∇log pt(xt)(∇log pt(xt))⊤\n(14)\n(15)\nNotice that\n∇2pt|0(xt|x0) = pt|0(xt|x0)∇2 log pt|0(xt|x0) +\n1\npt|0(xt|x0)∇pt|0(xt|x0)(∇pt|0(xt|x0))⊤\n= −pt|0(xt|x0)\n1 −¯αt\nI + pt|0(xt|x0)(xt −√¯αtx0\n1 −¯αt\n)(xt −√¯αtx0\n1 −¯αt\n)⊤.\nReplacing the LHS in the above equation in Equation (14), and noticing that E[x0|xt] =\nxt+(1−¯αt)∇log pt(xt)\n√¯αt\n, we have\n∇2 log pt(xt) = −\n1\n1 −¯αt\nI +\nZ\nx0∈X\np0|t(x0|xt)(xt −√¯αtx0\n1 −¯αt\n)(xt −√¯αtx0\n1 −¯αt\n)⊤dx0\n−∇log pt(xt)(∇log pt(xt))⊤\n= −\n1\n1 −¯αt\nI +\nZ\nx0∈X\np0|t(x0|xt)(xt −√¯αtx0\n1 −¯αt\n)(xt −√¯αtx0\n1 −¯αt\n)⊤dx0\n−\n\u0000√¯αtE[x0|xt] −xt\n1 −¯αt\n\u0001\u0000√¯αtE[x0|xt] −xt\n1 −¯αt\n\u0001⊤\n= −\n1\n1 −¯αt\nI + Cov\nh√¯αtx0 −xt\n1 −¯αt\n|xt\ni\n= −\n1\n1 −¯αt\nI +\n¯αt\n(1 −αt)2 Σ0|t.\nThe proof is finished by substituting ∇2 log pt(xt) in Equation (12) by the above result.\n23\n\nD\nTask details\nD.1\nGaussian Deblur\nIn computer vision, the Gaussian deblur task addresses the challenge of restoring clarity to images\nthat have been blurred by a Gaussian process. Gaussian blur, a common image degradation, simulates\neffects such as out-of-focus photography or atmospheric disturbances. It is characterized by the\nconvolution of an image with a Gaussian kernel, a process that spreads the pixel values outwards,\nleading to a smooth, uniform blur [33]. The deblurring task seeks to reverse this effect, aiming to\nretrieve the original sharp image.\nGuidance target.\nSpecifically, we apply a 61 × 61 sized Gaussian blur with kernal intensity\n3.0 to original 256 × 256 images. A random noise with a variance of σ2 = 0.052 is added to\nthe noisy images. If we denote the above process as a blurring operator Ablur : x →y, where\nx ∈R256×256×3, y ∈R256×256×3 are original images and noisy images, then the target of Gaussian\nDeblur is to generate a clean image x0 such that:\nmax\nx0 p(x0) = max\nx0 exp(−∥Ablur(x0) −y∥2),\nwhich means if we project the generated image into the noisy space, the noisy samples Ablur(x)\nshould be similar to the ground truth noisy images y.\nEvaluation metrics.\nIn our experiments, we evaluate each guidance method on a set of 256 samples\ngenerated by Cat-DDPM. We apply the FID [26] to assess the fidelity, Learned Perceptual Image\nPatch Similarity (LPIPS) [80] to evaluate the guidance validity.\n24\n\nFigure 9: Quantitative comparison of different training-free guidance methods on Gaussian deblur\ntask. Our TFG method can produce clean images without background noise (unlike FreeDoM and\nUGD), faithful image features (unlike DPS) and vivid image details (compared to LGD). Nrecur is set\nto 1 for all methods.\n25\n\nD.2\nSuper Resolution\nSuper-resolution in computer vision refers to the process of enhancing the resolution of an imaging\nsystem, aimed at reconstructing a high-resolution image from one or more low-resolution observa-\ntions. This technique is fundamental in overcoming the inherent limitations of imaging sensors and\nimproving the detail and quality of digital images. Super-resolution has broad applications, ranging\nfrom satellite imaging and surveillance to medical imaging and consumer photography [46].\nGuidance target.\nSpecifically, we simply down-sample to original 256 × 256 images to the\nresolution of 64 × 64. A random noise with a variance of σ2 = 0.052 is also added to the noisy\nimages. If we denote the above process as a degradation operator Adown : x →y, where x ∈\nR256×256×3, y ∈R256×256×3 are original images and down-sampled images, then the target of super-\nis to generate a high resolution image x0 such that:\nmax\nx0 p(x0) = max\nx0 exp(−∥Adown(x0) −y∥2),\nwhich means if we project the generated image into the downsampled image space, the downsampled\nsamples Adown(x) should be similar to the ground truth downsampled images y.\nEvaluation metrics.\nSimilar to Gaussian Deblur, we evaluate each guidance method on a set of\n256 samples generated by Cat-DDPM. We apply FID [26] to assess the fidelity, Learned Perceptual\nImage Patch Similarity (LPIPS) [80] to evaluate the guidance validity.\n26\n\nFigure 10: Quantitative comparison of different training-free guidance methods on super-resolution\ntask. Our TFG method can produce clean images, faithful image features (unlike DPS, MPGD) and\nvivid image details (compared to LGD, UGD). Nrecur is set to 1 for all methods.\n27\n\nD.3\nLabel Guidance\nLabel guidance is a standard task for conditional generation studied in previous literature [7, 23].\nThe target is to generate images conditioned on a certain label. We found this standard task is\nrarely studied in training-free guidance work and there exist an evident performance gap between\ntraining-based guidance and existing training-free guidance as shown in Section 3.\nLabel sets.\nIn our experiments, we studied labels from CIFAR10 [30] and ImageNet [55]. We\naverage the results on 10 labels from CIFAR10 if there is no extra explanation. For ImageNet, which\nis resource-hungry to do comprehensive inference, we randomly select 4 labels (111, 222, 333, 444)\nto evaluate the methods. The corresponding label-ID and their names are as follows:\nTable 6: CIFAR-10 Dataset Labels\nLabel-ID\nLabel Name\n0\nAirplane\n1\nAutomobile\n2\nBird\n3\nCat\n4\nDeer\n5\nDog\n6\nFrog\n7\nHorse\n8\nShip\n9\nTruck\nTable 7: Selected ImageNet-1K Dataset Labels\nLabel-ID\nLabel Name\n111\nnematode, nematode worm, roundworm\n222\nKuvasz\n333\nHamster\n444\nbicycle-built-for-two, tandem bicycle, tandem\nGuidance target.\nFor each dataset, we use the output probability of a pre-trained classifier h(·) as\nthe target probability. Our target is to maximize the certain classification probability of a given label,\ni.e.,\nmax\nx0 p(x0) = max\nx0 softmax(h(x0))i,\nwhere i is the label-ID of the target label, and h(·) is the logits of x0 produced by the pre-trained\nclassifier. For CIFAR10 and ImageNet, we use a pre-trained classifier based on ResNet [17] and\nVIT [9] that are provided from [75] and [9] respectively. The image resolution for CIFAR10 and\nImageNet are 32 × 32 and 256 × 256 respectively.\nEvaluation metrics.\nWe follow the image generation literature to use Fréchet inception distance\n(FID) [20] to assess the fidelity of generated images. The reference images are filtered from the entire\ndataset of CIFAR10 or ImageNet with the target label, and we set sample sizes as 2560 and 256 for\nCIFAR and ImageNet respectively. For validity, we use another pre-trained classifier to compute\naccuracy other than the one used in providing guidance to avoid over-confidence:\naccuracy = #classified as target label\n#generated samples\nFor CIFAR10, we use a pre-trained ConvNeXT [36] downloaded from HuggingFace Hub7. And for\nImageNet, we use the pre-trained DeiT [69] downloaded from HuggingFace Hub8.\n7https://huggingface.co/ahsanjavid/convnext-tiny-finetuned-cifar10\n8https://huggingface.co/facebook/deit-small-patch16-224\n28\n\nFigure 11: Quantitative comparison of different training-free guidance methods on ImageNet label\nguidance (with target = 222, Kuvasz). The suffix of FreeDoM, UGD, TFG represents the number\nof recurrence Nrecur. Notice that all the samples are generated based on the same seed and we do\nnot conduct cherry-picking. It is apprent that TFG generates the most valid samples among all the\ncompared methods.\n29\n\nD.4\nCombined Guidance\nAn interesting scenario for conditional generation is to assign multiple targets for a single sam-\nple. Conditional generation with multiple conditions is crucial in machine learning for enhancing\nthe relevance and applicability of AI across complex, real-world scenarios. It enables models to\nproduce more contextually appropriate and personalized outputs, crucial for fields requiring high\ncustomization.\nMotivations.\nCombined guidance is to use multiple target functions to guide the same sample\ntowards multiple targets for the same sample. It is more efficient for training-free guidance to\ndo combined guidance as the space of combinatorial targets is potentially huge, which makes it\nunrealistic to train all target combinations for training-based guidance. We also find it related to\nthe topic of spurious correlations [60], where certain combinations of attributes may dominant the\nother combinations. For example, hair color may have a strong correlation with gender in CelebA\ndataset [37]. It is beneficial to explore training-free guidance on reducing the bias of generation\nmodels trained on these biased data and address the concerns related to fairness and equality.\nGuidance target.\nWe study combined guidance on CelebA-DDPM, which is trained on CelebA-\nHQ [26] dataset. The image resolution is 256 × 256. We choose two settings of combined guidance\nwith two attributes: (gender, hair color) and (gender, age). Each attribute has two labels, where\ngender∈{male, female}, age∈{young, old}, and hair color∈{black, blonde}. We have a binary\nclassifier for each attribute that is downloaded from HuggingFace Hub91011. The target is to sample\nimages that maximize the marginal probability:\nmax\nx0 pcombined(x0) = max\nx0 ptarget1(x0)ptarget2(x0),\nwhere ptarget(x0) is computed using label guidance as shown in Appendix D.3.\nEvaluation metrics.\nAs it is hard to filter many reference images for combined targets in CelebA-\nHQ dataset, we adopt Kernel Inception distance (KID) [5] to assess fidelity of generated samples\nusing 1,000 random sampled images of CelebA-HQ as reference images. We generate 256 samples\nfor each evaluated method. We follow MPGD [18] to use the logarithm of KID, i.e. KID (log). For\nvalidity, we use another three attribute classifiers to compute the accuracy considering the conjunction\nof attributes:\naccuracy = # ∧target label (classified as target label)\n#generated samples\nThe evaluation classifiers are also downloaded from HuggingFace Hub121314.\n9Age: https://huggingface.co/nateraw/vit-age-classifier\n10Gender: https://huggingface.co/rizvandwiki/gender-classification-2\n11Hair color: https://huggingface.co/enzostvs/hair-color\n12Age (Evaluation): ibombonato/swin-age-classifier\n13Gender (Evaluation): https://huggingface.co/rizvandwiki/gender-classification\n14Age (Evaluation): https://huggingface.co/londe33/hair_v02\n30\n\nFigure 12: Quantitative comparison of different training-free guidance methods on combined guidance\ntask (male+young). Our TFG method can produce images with high fidelity and validity compared\nto all the baselines. Notice that we use the fixed seed for all the methods in this figure and do not\nconduct cherry picking. Nrecur is set to 1 for all methods.\nD.5\nFine-grained Guidance\nFine-grained classification is a specialized task in computer vision that focuses on distinguishing\nbetween highly similar subcategories within a larger, general category. This task is particularly\nchallenging due to the subtle differences among the objects or entities being classified. For example,\nin the context of animal species recognition, fine-grained classification would involve not just\ndistinguishing a bird from a cat, but identifying the specific species of birds, such as differentiating\nbetween a crow and a raven [79].\nStudying fine-grained generation in the context of generative models like Stable Diffusion or DALL-E\npresents unique challenges due to the inherent complexity of generating highly detailed and specific\nimages. Fine-grained generation involves creating images that not only belong to a broad category\nbut also capture the subtle nuances and specific characteristics of a narrowly defined subcategory.\nFor example, generating images of specific dog breeds in distinct poses or environments requires the\nmodel to understand and replicate minute details that distinguish one breed from another.\n31\n\nMotivations.\nTo develop personalized AI, it is important to explore if the foundational generative\nmodels can synthesize fine-grained, accurate target samples according to user-defined target. However,\nthis usually requires high-quality and detailed training data, and the model should be highly sensitive\nto small variations in input to accurately produce the desired output, which can be difficult for strong\ntext2image generation models DALL-E15 or Imagen16. We first study this problem in a training-free\nguidance scenario.\nGuidance target.\nWe study the out-of-distribution fine-grained label guidance on ImageNet-DDPM,\nwhich learns the generation of some species of birds but not comprehensively. We use an EfficientNet\ntrained to classify 525 fine-grained bird species downloaded from HuggingFace Hub.17 The classifier\nis trained on Bird Species dataset on Kaggle18. We follow the same way in label-guidance to maximize\nsoftmax probability for target fine-grianed label. We randomly sample 4 labels (111, 222, 333, 444)\nin Bird Species dataset, which are:\nTable 8: Selected Bird Species Dataset Labels\nLabel-ID\nLabel Name\n111\nBrown headed cowbird\n222\nFairy tern\n333\nLucifer hummingbird\n444\nScarlet macaw\nEvaluation metrics.\nSimilar to label guidance, we use FID to evaluate generation fidelity by\nfiltering data of the target label as reference images. We also compute the FID with 256 sampled\nimages. For accuracy, we adopt another downloaded pre-trained classifier trained on Bird Species\ndataset from HuggingFace Hub.19.\n15https://openai.com/index/dall-e-2/\n16https://deepmind.google/technologies/imagen-2/\n17https://huggingface.co/chriamue/bird-species-classifier\n18https://www.kaggle.com/datasets/gpiosenka/100-bird-species/data\n19https://huggingface.co/dennisjooo/Birds-Classifier-EfficientNetB2\n32\n\nFigure 13: The sampled 256 images for fine-grained guidance with target label 222 (fairy tern) by\nImageNet-DDPM with TFG. A key feature of fairy tern is its black-colored head. We observe that\nTFG generally samples images with black, round shapes and successfully generates some birds with\ntarget feature (red circled).\nD.6\nStyle Transfer\nStyle transfer is a significant task in computer vision (CV) that focuses on applying the stylistic\nelements of one image onto another while preserving the content of the target image. This task is\npivotal because it bridges the gap between artistic expression and technological innovation, allowing\nfor the creation of novel and aesthetically pleasing visual content. Applications of style transfer\nare vast, ranging from enhancing user engagement in digital media and advertising to aiding in\narchitectural design by visualizing changes in real-time.\nGuidance target.\nThe target in our experiments is to guide a text-to-image latent diffusion model\nStable-Diffusion-v-1-5 [53]20 to generate images that fit both the text input prompts and the style\nof the reference images. The guidance objective involves calculating the Gram matrices [25] of\nthe intermediate layers of the CLIP image encoder for both the generated images and the reference\nstyle images. The Frobenius norm of these matrices serves as the metric for the objective function.\nSpecifically, for a reference style image xref and a decoded image D(z0|t) generated from the\n20https://huggingface.co/runwayml/stable-diffusion-v1-5\n33\n\nestimated clean latent variable z0|t, we compute the Gram matrices G(xref) and G(D(z0|t)). These\nmatrices are derived from the features of the 3rd layer of the image encoder, in accordance with\nthe methodologies described in MPGD [18] and FreeDoM [78]. The target function is computed as\nfollows:\nmax\nx0 pstyle(x0) = max\nx0 exp(−∥G(xref) −G(D(z0|t))∥2\nF ),\nwhere ∥· ∥2\nF denotes the Frobenius norm of a matrix.\nEvaluation metrics.\nWe use Style score and CLIP score to assess the guidance validity and fidelity,\nrespectively. For reference style images and text prompts, we select 4 images from WikiArt [56]\nthat are also used by MPGD [18], and 64 text prompts from Partiprompts [77]. For each style, we\ngenerate 64 images based on the 64 different text prompts. To avoid over-confidence of CLIP score,\nwe use two different CLIP models downloaded from HuggingFace Hub to compute guidance and\nevaluation metrics, respectively.2122. The style images and examplar prompts are shown as follows.\nFigure 14: Four style images used in style transfer task.\nTable 9: Examples for used prompts for style transfer tasks.\nContent Description\nContent Description\nA book with the words ’Don’t Panic!’\nwritten on it\nGround view of the Great Pyramids and\nSphinx on the moon’s surface, Earth in the\nsky\nA canal in Venice\nA white towel\nPortrait of a tiger wearing a train\nconductor’s hat and holding a skateboard\nDowntown Shanghai at sunrise, detailed\nink wash\nBackground pattern with alternating roses\nand skulls\nA smiling sloth holding a quarterstaff and a\nbook, VW van parked on grass\nA pickup truck\nConcept of energy\nh A shoe with a sock draped over it\nA kitchen without a refrigerator\nTimes Square during the day\nA squirrel\nA turkey walking in the kitchen\nA bowl\nThe Statue of Liberty in Minecraft\nA man with wild hair looking into a crystal\nball\nConcentric circles\nA fire hydrant with graffiti on it\n21Guidance: https://huggingface.co/openai/clip-vit-base-patch16\n22Evaluation: https://huggingface.co/openai/clip-vit-base-patch32\n34\n\nFigure 15: Quantitative comparison of different training-free guidance methods on style transfer task\nwith the target image as The Starry Night by Von Gogh. Our TFG generates the images with the most\nfaithful style, while DPS, LGD, FreeDoM, and UGD fail to capture the target style. The images of\nMPGD is also of good quality, but the style score is also inferior than TFG by a large margin. We set\nNrecur = 1 for all methods.\n35\n\nD.7\nMolecule Property Guidance\nSetup. Our benchmark setup generally follows [24, 3] but with certain specifications to guarantee the\noverall framework abides by the training-free regime. For dataset, we employ QM9 [50] and adopt\nthe split in [24] with 100,000 training samples. Following [24] and [3], the training set is further\nsplit into two halves that guarantees there is no data leakage. The first half is leveraged to train a\nproperty prediction network with EGNN [57] as the backbone, which serves as the ground truth\noracle to provide the label used for MAE computation. We reuse the checkpoints released by [3]\nfor the labeling network for all 6 properties. The second half is used to train the diffusion model\nas well as the guidance network. We adopt the unconditional generation version of EDM [24] as\nthe diffusion model. The guidance network in general takes the same architecture as defined by [3]\nthat, again, features EGNN as the backbone but outputs a single scalar as the predicted quantum\nmechanics property. The only difference lies in at training time we mask the diffusion time step by\nzeros and always use the original clean molecule structure as input, ensuring training-free objective.\nAll the pretrained models are trained separately for different properties. At sampling time, we employ\nDDIM [62] sampler with 100 sampling steps, as opposed to [24, 3] that take 1000 sampling steps.\nGuidance target. We study training-free guided generation of molecules on 6 quantum mechanics\nproperties, including polarizability (α), dipole moment (µ), heat capacity (Cv), highest orbital energy\n(ϵHOMO), lowest orbital energy (ϵLUMO) and their gap (∆ϵ). Denote the oracle property prediction\nnetwork as E. Our guidance target in this case is given by\nf(x, c) := exp(−∥E(x) −c∥2\n2),\n(16)\nwhere x is the input molecule and c is the target property value.\nEvaluation metrics. We use Mean Absolute Error (MAE) and validity as our evaluation metrics.\nIn particular, MAE is computed between the target value and the predicted value gathered from the\nlabeling network. Validity is computed by RDKit which measures whether the molecule is chemically\nfeasible. We generate 4096 molecules for each property for evaluation.\n36\n\n40\n60\n80\n100\nTFG\n(Ours)\nDPS\nLGD\nMPGD\nFreeDoM\nUGD\nPolarizability (𝛼)\nFigure 16: Quanlitative comparison of different training-free guidance methods on molecule genera-\ntion task with the target property α (polarizability). Our TFG generates valid molecules with better\ndesign target, while baselines often fail to produce valid molecules or offer poor guidance towards the\ndesign target. The molecules generated by our approach are increasingly polarizable as α goes up.\n37\n\nD.8\nAudio Declipping\nAudio declipping is a task in digital audio restoration where distorted audio signals are repaired.\nClipping occurs when the amplitude or frequency of an audio signal exceeds the maximum limit that\nthe recording system can handle, leading to a harsh, distorted sound with portions of the waveform\n“cut off.” Declipping aims to reconstruct the missing parts of these clipped waveforms, restoring the\naudio’s original dynamics and reducing distortion. This process improves the quality and clarity of\nthe audio, making it more pleasant to listen to while preserving the original sound’s integrity.\nGuidance target.\nSpecifically, we apply a distortion operation to zero out the high frequency and\nlow frequency (for the highest and lowest 40 dimensions) signal in the space of mel spectrograms.\nIf we denote the above process as a blurring operator Ablur : x →y, where x ∈R256×256, y ∈\nR256×256 are mel spectrograms and noisy mel spectrograms for 5s audios, then the target of Audio\nDeclipping is to generate a clean audio x0 such that:\nmax\nx0 p(x0) = max\nx0 exp(−∥Ablur(x0) −y∥2,\nwhich means if we project the generate mel spectrogram into the noisy space, the noisy samples\nAblur(x) should be similar to the ground truth noisy mel spectrogram y.\nEvaluation metrics.\nIn our experiments, we evaluate each guidance method on a set of 256 samples\ngenerated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the\nguidance validity, and Fréchet Audio Distance (FAD) [28] to assess the generation fidelity.\n38\n\nD.9\nAudio Inpainting\nAudio inpainting is a digital audio restoration task that involves filling in missing or corrupted\nsegments of an audio signal. Similar to image inpainting in the visual domain, this technique\nreconstructs the missing portions of sound by analyzing the surrounding context and seamlessly\nrestoring the lost information. Applications of audio inpainting range from repairing damaged\nrecordings to reconstructing gaps in audio streams due to data loss. The goal is to produce a\nnatural-sounding result that preserves the continuity and overall quality of the original audio.\nGuidance target.\nSpecifically, we apply a distortion operation to zero out the middle 80 dimensions\nin the space of mel spectrograms. If we denote the above process as a blurring operator Ablur : x →y,\nwhere x ∈R256×256, y ∈R256×256 are mel spectrograms and noisy mel spectrograms for 5s audios,\nthen the target of Audio Declipping is to generate a clean audio x0 such that:\nmax\nx0 p(x0) = max\nx0 exp(−∥Ablur(x0) −y∥2,\nwhich means if we project the generate mel spectrogram into the noisy space, the noisy samples\nAblur(x) should be similar to the ground truth noisy mel spectrogram y.\nEvaluation metrics.\nIn our experiments, we evaluate each guidance method on a set of 256 samples\ngenerated by Audio-diffusion. We apply the Dynamic time warping (DTW) [44] to assess the\nguidance validity, and Fréchet Audio Distance (FAD) [28] to assess the generation fidelity.\n39\n\nE\nExperimental Details\nE.1\nDetails of Table 1\nIn Table 1, we study the effect of Monte-Carlo sample sizes in estimating the expectation of Line 4 in\nTFG algorithm. As the noise is added on both Mean Guidance (∆0) and Variance Guidance (∆t), we\ndecouple the effect into adding noise solely on ∆0 (Mean only) or ∆t (Variance only). In the setting\nof Variance only, we set µ = 0, Niter = 0, Nrecur = 4, sρ(t) = “increase”, and pick the best ¯ρ and ¯γ\nvia hyper-parameter search. In the setting of Mean only, we set ρ = 0, Niter = 4, Nrecur = 1, sµ(t) =\n“increase”, and pick the best ¯µ and ¯γ via hyper-parameter search. We found that the sample size\nused in Monte-Carlo method play a neglect-able role on the performance if we set the optimal\nhyper-parameter. It is also noteworthy that the Monte-Carlo sampling does affect the performance\nof generated quality. For example, we can find that different targets shown in Appendix E.3 have\ndifferent searched ¯γ. This indicates that the best ¯γ for many targets are apparently not zero.\nE.2\nComparison with grid search\nWe compare the performance of our beam search parameters with the full grid search ones on CIFAR-\n10 label guidance task (Table 10). Overall, the performance of both search methods is identical,\nwhile grid search is much slower than our search strategy, indicating that our beam search strategy is\neffective and efficient.\nTable 10: The searched (¯ρ, ¯µ, ¯γ) of exhaustive grid search and our beam search strategy on the\nCIFAR-10 label guidance task. We show the validity metric of the corresponding results and the gap\n∆= ∥validitybeam −validitygrid∥. Overall, the performance of both methods is identical.\nTarget\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nAvg.\n(¯ρ, ¯µ, ¯γ)grid\n(1,2,0.001)\n(0.25,2,0.001)\n(2,0.25,1)\n(4,0.5,0.1)\n(1,0.5,0.001)\n(2,0.25,0.001)\n(0.25,0.5,1)\n(1,0.5,0.001)\n(1,0.25,0.001)\n(0.5,2,0.001)\nvaliditygrid\n80.44%\n35.38%\n28.25%\n56.32%\n29.57%\n41.70%\n52.66%\n42.14%\n83.35%\n73.22%\n52.30%\n(¯ρ, ¯µ, ¯γ)beam\n(1,2,0.001)\n(0.25,2,0.001)\n(2,0.25,1)\n(4,0.25,0.01)\n(1,0.5,0.001)\n(2,0.25,0.001)\n(0.25,0.5,1)\n(1,0.5,0.001)\n(1,0.25,0.001)\n(0.5,2,0.001)\nvaliditybeam\n80.44%\n35.38%\n28.25%\n52.81%\n29.57%\n41.70%\n52.66%\n42.14%\n83.35%\n73.22%\n51.95%\n∆\n0.00%\n0.00%\n0.00%\n3.51%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.35%\nE.3\nDetailed results of each target and hyper-parameters\nIn this section, we present the hyper-parameters searched via the strategy introduced in Section 4 and\nthe corresponding experimental results for TFG as shown in Table 11. We list several observations\nbelow.\n• Overall, optimal parameters vary widely between problems and datasets. For example, even\nwith the same model and objective (e.g., label classifier on ImageNet or CIFAR10), the\nbest hyperparameters vary widely from target to target. This highlights the importance of\nhyperparameter search.\n• The improvement of TFG over existing methods depends heavily on the difference between\nthe optimal parameters and the subspaces of existing methods. For example, the ¯ρ for UGD\nis the same as TFG for gender-age guidance task, where TFG only has 0.133% validity\nimprovement over UGD. On the contrary, their values differ on the fine-grained classification\ntask, and TFG has an 18.7% validity improvement over UGD. Overall, we suppose this\ndepends on whether the optimal parameter lies in the subspace that existing methods can\nfind.\n• Though the baselines mentioned in our paper should be a special case of TFG, the results for\nthe highest MO energy guidance in Table 3 show that MPGD outperforms TFG slightly. We\nwant to point out that the reason TFG could occasionally have slightly worse performance\nin practice is due to the beam search computation limit we currently pose. More specifically,\nwe allow TFG to search at most six steps (for all hyper-parameters) and all other methods\nfor seven steps (in their subspaces). For the MO energy task, the searched parameter for\nMPGD is that ¯µ = 0.016 (this is the only parameter that we need to search for MPGD),\nwhere the best (and last step) of TFG is that ¯µ = 0.004 (because it uses one step to double\nanother parameter). If we allocate more computational budget for the beam search steps,\nTFG will outperform MPGD on this target (in fact, eight steps suffice).\n40\n\nTable 11: The parameter (¯ρ, ¯µ, ¯γ) selected by beam search strategy for all methods, tasks, and targets.\nThe search space of each method can be found in Section 3.1. For the detailed semantics of each task,\nplease refer to Appendix D.\nDPS\nLGD\nMPGD\nFreeDoM\nUGD\nTFG\nTarget\n¯ρ\n¯µ\n¯γ\n¯ρ\n¯µ\n¯γ\n¯ρ\n¯µ\n¯γ\n¯ρ\n¯µ\n¯γ\n¯ρ\n¯µ\n¯γ\n¯ρ\n¯µ\n¯γ\nCIFAR-10 label guidance\n0\n1\n0\n0\n16\n0\n1\n0\n2\n0\n1\n0\n0\n2\n2\n0\n1\n2\n0.001\n1\n8\n0\n0\n16\n0\n1\n0\n4\n0\n0.5\n0\n0\n4\n4\n0\n0.25\n2\n0.001\n2\n1\n0\n0\n16\n0\n1\n0\n0.25\n0\n1\n0\n0\n0.25\n0.25\n0\n2\n0.25\n1\n3\n4\n0\n0\n8\n0\n1\n0\n8\n0\n2\n0\n0\n1\n1\n0\n4\n0.25\n0.01\n4\n0.5\n0\n0\n2\n0\n1\n0\n0.25\n0\n0.5\n0\n0\n4\n4\n0\n1\n0.5\n0.001\n5\n4\n0\n0\n0.25\n0\n1\n0\n0.25\n0\n1\n0\n0\n4\n4\n0\n2\n0.25\n0.001\n6\n1\n0\n0\n4\n0\n1\n0\n0.5\n0\n16\n0\n0\n4\n4\n0\n0.25\n0.5\n1\n7\n2\n0\n0\n0.5\n0\n1\n0\n0.5\n0\n0.5\n0\n0\n4\n4\n0\n1\n0.5\n0.001\n8\n2\n0\n0\n16\n0\n1\n0\n2\n0\n1\n0\n0\n4\n4\n0\n1\n0.25\n0.001\n9\n4\n0\n0\n0.5\n0\n1\n0\n2\n0\n1\n0\n0\n4\n4\n0\n0.5\n2\n0.001\nImageNet label guidance\n111\n2\n0\n0\n2\n0\n1\n0\n8\n0\n1\n0\n0\n8\n8\n0\n2\n0.5\n0.1\n222\n2\n0\n0\n2\n0\n1\n0\n0.25\n0\n0.5\n0\n0\n2\n2\n0\n0.5\n1\n0.1\n333\n2\n0\n0\n2\n0\n1\n0\n0.25\n0\n0.25\n0\n0\n8\n8\n0\n1\n4\n1\n444\n4\n0\n0\n4\n0\n1\n0\n4\n0\n1\n0\n0\n4\n4\n0\n0.5\n2\n0.1\nFine-grained guidance\n111\n0.25\n0\n0\n0.25\n0\n1\n0\n0.25\n0\n0.25\n0\n0\n0.25\n0.25\n0\n0.5\n0.5\n0.01\n222\n0.25\n0\n0\n1\n0\n1\n0\n0.25\n0\n0.5\n0\n0\n4\n4\n0\n0.5\n0.5\n0.01\n333\n0.25\n0\n0\n0.25\n0\n1\n0\n0.5\n0\n0.25\n0\n0\n4\n4\n0\n0.5\n0.5\n0.01\n444\n0.25\n0\n0\n0.25\n0\n1\n0\n0.25\n0\n1\n0\n0\n1\n1\n0\n0.5\n0.5\n0.01\nCombined Guidance (gender & hair)\n(0,0)\n4\n0\n0\n0.25\n0\n1\n0\n16\n0\n1\n0\n0\n16\n16\n0\n1\n2\n0.01\n(0,1)\n4\n0\n0\n16\n0\n1\n0\n16\n0\n0.5\n0\n0\n8\n8\n0\n2\n8\n0.01\n(1,0)\n4\n0\n0\n16\n0\n1\n0\n16\n0\n0.25\n0\n0\n4\n4\n0\n1\n1\n0.01\n(1,1)\n2\n0\n0\n0.25\n0\n1\n0\n8\n0\n2\n0\n0\n2\n2\n0\n0.5\n1\n0.1\nCombined Guidance (gender & age)\n(0,0)\n8\n0\n0\n0.25\n0\n1\n0\n0.25\n0\n1\n0\n0\n1\n1\n0\n1\n2\n0.01\n(0,1)\n1\n0\n0\n16\n0\n1\n0\n16\n0\n1\n0\n0\n0.5\n0.5\n0\n0.5\n8\n1\n(1,0)\n4\n0\n0\n0.25\n0\n1\n0\n8\n0\n0.5\n0\n0\n0.5\n0.5\n0\n0.5\n2\n0.01\n(1,1)\n2\n0\n0\n0.25\n0\n1\n0\n16\n0\n0.25\n0\n0\n1\n1\n0\n1\n0.5\n0.1\nSuper-resolution\n\\\n16\n0\n0\n16\n0\n1\n0\n16\n0\n16\n0\n0\n8\n8\n0\n4\n2\n0.01\nGaussian Deblur\n\\\n16\n0\n0\n16\n0\n1\n0\n16\n0\n16\n0\n0\n16\n16\n0\n1\n8\n0.01\nStyle Transfer\n0\n2\n0\n0\n1\n0\n1\n0\n4\n0\n0.25\n0\n0\n1\n1\n0\n0.25\n8\n0.01\n1\n4\n0\n0\n0.25\n0\n1\n0\n4\n0\n0.5\n0\n0\n1\n1\n0\n0.25\n2\n0.1\n2\n2\n0\n0\n0.25\n0\n1\n0\n8\n0\n0.25\n0\n0\n1\n1\n0\n0.25\n8\n0.1\n3\n2\n0\n0\n2\n0\n1\n0\n2\n0\n0.25\n0\n0\n0.25\n0.25\n0\n0.25\n8\n0.01\nMolecule Property\nα\n0.005\n0\n0\n0.005\n0\n1\n0\n0.01\n0\n0.01\n0\n0\n0.02\n0.02\n0\n0.016\n0.001\n0.0001\nµ\n0.02\n0\n0\n0.01\n0\n1\n0\n0.005\n0\n0.02\n0\n0\n0.005\n0.005\n0\n0.001\n0.002\n0.1\nCv\n0.005\n0\n0\n0.005\n0\n1\n0\n0.005\n0\n0.005\n0\n0\n0.005\n0.005\n0\n0.004\n0.001\n0.001\nϵHOMO\n0.005\n0\n0\n0.005\n0\n1\n0\n0.01\n0\n0.005\n0\n0\n0.005\n0.005\n0\n0.002\n0.004\n0.001\nϵLUMO\n0.005\n0\n0\n0.01\n0\n1\n0\n0.01\n0\n0.005\n0\n0\n0.005\n0.005\n0\n0.016\n0.002\n0.0001\n∆\n0.005\n0\n0\n0.01\n0\n1\n0\n0.01\n0\n0.01\n0\n0\n0.005\n0.005\n0\n0.032\n0.001\n0.001\nAudio Declipping\n\\\n1\n0\n0\n16\n0\n1\n0\n16\n0\n4\n0\n0\n4\n4\n0\n1\n1\n0.1\nAudio Inpainting\n\\\n16\n0\n0\n16\n0\n1\n0\n16\n0\n4\n0\n0\n16\n16\n0\n0.25\n2\n0.1\nE.4\nTricks implemented in FreeDoM codebase\nIn the codebase of FreeDoM23, the schedule of guidance strength for different applications is different.\nFor example, the guidance strength has a schedule coefficient √¯αt for face generation, and the\nschedule for style transfer is complex and involves a correction term, the mean of gradients’ norm,\nand a specific constant coefficient 0.2. The paper does not mention this particular schedule, leaving\n23https://github.com/vvictoryuki/FreeDoM\n41\n\nthe rationale for choosing these schedules unclear. We choose not to include the tricks and find that\nwith our unified hyper-parameter searching strategy, the performance of FreeDoM is similar.\nE.5\nHardware and Software\nWe run most of the experiments on clusters using NVIDIA A100s. We implemented our experiments\nusing PyTorch [49] and the HuggingFace library.24 Overall, we estimated that a total of 2,000 GPU\nhours were consumed.\n24https://huggingface.co/\n42\n\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: We have both theoretically and empirically justified our contributions.\nGuidelines:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: See Sec. 6.\nGuidelines:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n43\n\nJustification: See Sec. C.\nGuidelines:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: See Sec. D.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n44\n\nAnswer: [Yes]\nJustification: See supplementary file.\nGuidelines:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: See Sec. D.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [No]\nJustification: Our experiment is conducted on an extensively large scale, and existing works\nof the same line were not reported since the numbers are stable.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n45\n\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: See Sec. 5.\nGuidelines:\n• The answer NA means that the paper does not include experiments.\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: We follow the NeurIPS Code of Ethics.\nGuidelines:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: See Sec. 6.\nGuidelines:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n46\n\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: It is not applicable to the concern of this paper.\nGuidelines:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We have cited the related papers.\nGuidelines:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n47\n\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: We do not include new assets.\nGuidelines:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: We do not involve crowdsourcing and research with human subjects.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: We do not involve human subjects and studies.\nGuidelines:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n48",
    "pdf_filename": "TFG_Unified_Training-Free_Guidance_for_Diffusion_Models.pdf"
}