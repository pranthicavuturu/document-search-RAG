{
    "title": "Multi-LoRA Composition for Image Generation",
    "context": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights, and all evaluation details are available on our project website. 1 In the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Ruiz et al., 2023; Sohn et al., 2023), the integration of Low-Rank Adaptation (LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision and minimal computational load. LoRA excels by specializing in one element — such as a specific character, a particular clothing, a unique style, or other distinct visual aspects — and being trained to produce diverse and accurate renditions of this element in generated images. For instance, users could customize their LoRA models to generate various images of themselves, achieving an array of personalized and realistic representations. 1 arXiv:2402.16843v2  [cs.CV]  19 Nov 2024",
    "body": "Published in Transactions on Machine Learning Research (11/2024)\nMulti-LoRA Composition for Image Generation\nMing Zhong1\nmingz5@illinois.edu\nYelong Shen2\nyelong.shen@microsoft.com\nShuohang Wang2\nShuohang.Wang@microsoft.com\nYadong Lu2\nyadonglu@microsoft.com\nYizhu Jiao1\nyizhuj2@illinois.edu\nSiru Ouyang1\nsiruo2@illinois.edu\nDonghan Yu2\ndonghanyu@microsoft.com\nJiawei Han1\nhanj@illinois.edu\nWeizhu Chen2\nwzchen@microsoft.com\n1University of Illinois Urbana-Champaign, 2Microsoft Corporation\nReviewed on OpenReview: https: // openreview. net/ forum? id= 25FT0DqhVZ\nAbstract\nLow-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate\nrendition of specific elements like distinct characters or unique styles in generated images.\nNonetheless, existing methods face challenges in effectively composing multiple LoRAs,\nespecially as the number of LoRAs to be integrated grows, thus hindering the creation of\ncomplex imagery. In this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which alternates between\ndifferent LoRAs at each denoising step, and LoRA Composite, which simultaneously\nincorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed\napproaches, we establish ComposLoRA, a new comprehensive testbed as part of this research.\nIt features a diverse range of LoRA categories with 480 composition sets. Utilizing an\nevaluation framework based on GPT-4V, our findings demonstrate a clear improvement\nin performance with our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights,\nand all evaluation details are available on our project website.\n1\nIntroduction\nIn the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Saharia\net al., 2022; Ramesh et al., 2022; Ruiz et al., 2023; Sohn et al., 2023), the integration of Low-Rank Adaptation\n(LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision and\nminimal computational load. LoRA excels by specializing in one element — such as a specific character, a\nparticular clothing, a unique style, or other distinct visual aspects — and being trained to produce diverse and\naccurate renditions of this element in generated images. For instance, users could customize their LoRA models\nto generate various images of themselves, achieving an array of personalized and realistic representations.\n1\narXiv:2402.16843v2  [cs.CV]  19 Nov 2024\n\nPublished in Transactions on Machine Learning Research (11/2024)\nClothing\nCharacter 1\nCharacter 2\nLoRA Merge\nOurs\nCharacter 3\nBackground\nBamboo\nObject\nHamburger\nCharacter 4\nLoRA Merge\nOurs\nRealistic Style\nAnime Style\nFigure 1: Multi-LoRA composition techniques effectively blend different elements such as characters, clothing,\nand objects into a cohesive image. Unlike the conventional LoRA Merge approach (Ryu, 2023), which can\nlead to detail loss and image distortion as more LoRAs are added, our methods retain the accuracy of each\nelement and the overall image quality.\nThe application of LoRA not only showcases its adaptability and precision in image generation but also\nopens new avenues in customized digital content creation, revolutionizing how users interact with and utilize\ngenerative text-to-image models for creating tailored visual content.\nHowever, an image typically embodies a mosaic of various elements, making compositionality key to\ncontrollable image generation (Tenenbaum, 2018; Huang et al., 2023b). In pursuit of this, the strategy of\ncomposing multiple LoRAs, each focused on a distinct element, emerges as a feasible approach for advanced\ncustomization. This technique enables the digitization of complex scenes, such as virtual try-ons, merging\nusers with clothing in a realistic fashion, or urban landscapes where users interact with meticulously designed\ncity elements. Prior investigations into multi-LoRA compositions have explored the context of pre-trained\nlanguage models (Zhang et al., 2023a; Huang et al., 2023a) or stable diffusion models (Ryu, 2023; Shah et al.,\n2023). These studies aim to merge multiple LoRA models to synthesize a new LoRA model by training\ncoefficient matrices (Huang et al., 2023a; Shah et al., 2023; Wu et al., 2024) or through the direct addition\nor subtraction of LoRA weights (Ryu, 2023; Zhang et al., 2023a). Nevertheless, these approaches centered\non weight manipulation could destabilize the merging process as the number of LoRAs grows (Huang et al.,\n2023a) and also overlook the interaction between LoRA models and base models. This oversight becomes\nparticularly critical in diffusion models, which depend on sequential denoising steps for image generation.\nIgnoring the interplay between LoRAs and these steps can result in misalignments in the generative process,\nas shown in Figure 1, where a merged LoRA model fails to preserve the full complexity of all desired elements,\nleading to distorted or unrealistic images.\nIn this paper, we delve into multi-LoRA composition from a decoding-centric perspective, keeping all LoRA\nweights intact. We present two training-free approaches that utilize either one or all LoRAs at each decoding\nstep to facilitate compositional image synthesis. Our first approach, LoRA Switch, operates by selectively\nactivating a single LoRA during each denoising step, with a rotation among multiple LoRAs throughout the\ngeneration process. For instance, in a virtual try-on scenario, LoRA Switch alternates between a character\nLoRA and a clothing LoRA at successive denoising steps, thereby ensuring that each element is rendered\nwith precision and clarity. In parallel, we propose LoRA Composite, a technique that draws inspiration\nfrom classifier-free guidance (Ho & Salimans, 2022). It involves calculating unconditional and conditional\nscore estimates derived from each respective LoRA at every denoising step. These scores are then averaged\nto provide balanced guidance for image generation, ensuring a comprehensive incorporation of all elements.\nFurthermore, by bypassing the manipulation on the weight matrix but directly influencing the diffusion\nprocess, both methods allow for the integration of any number of LoRAs and overcome the limitations of\nrecent studies that typically merge only two LoRAs (Shah et al., 2023).\n2\n\nPublished in Transactions on Machine Learning Research (11/2024)\nExperimentally, we introduce ComposLoRA, the first testbed specifically designed for LoRA-based composable\nimage generation. This testbed features an extensive array of six LoRA categories, spanning two distinct\nvisual styles: reality and anime. Our evaluation includes 480 diverse composition sets, each incorporating a\nvarying number of LoRAs to comprehensively evaluate the efficacy of each proposed method. Given the lack\nof standardized automatic metrics for this novel task, we propose to employ GPT-4V (OpenAI, 2023a;b)\nas an evaluator, assessing both the quality of the images and the effectiveness of the compositions. Our\nempirical findings consistently demonstrate that both LoRA Switch and LoRA Composite substantially\noutperform the prevalent LoRA merging approach, particularly noticeable as the number of LoRAs in a\ncomposition increases. To further validate our results, we also conduct human evaluations, which reinforce\nour conclusions and affirm the efficacy of our automated evaluation framework. In addition, we provide a\ndetailed analysis of the applicable scenarios for each method, as well as discuss the potential bias of using\nGPT-4V as an evaluator.\nTo summarize, our key contributions are threefold:\n• We introduce the first investigation of multi-LoRA composition from a decoding-centric perspective,\nproposing LoRA Switch and LoRA Composite. Our methods overcome existing constraints on the\nnumber of LoRAs that can be integrated, offering enhanced flexibility and improved quality in composable\nimage generation.\n• Our work establishes ComposLoRA, a comprehensive testbed tailored to this research area, featuring six\nvaried categories of LoRAs and 480 composition sets. Addressing the absence of standardized metrics, we\npresent an evaluator built upon GPT-4V, setting a new benchmark for assessing both image quality and\ncompositional efficacy.\n• Through extensive automatic and human evaluations, our findings reveal the superior performance of\nthe proposed methods compared to the prevalent LoRA merging approach. Additionally, we provide an\nin-depth analysis of different multi-composition methods and evaluation frameworks.\n2\nRelated Work\n2.1\nComposable Text-to-Image Generation\nComposable image generation, a key aspect of digital content customization, involves creating images that\nadhere to a set of pre-defined specifications (Liu et al., 2023). Existing research in this domain primarily\nfocuses on the following approaches: enhancing compositionality with scene graphs or layouts (Johnson et al.,\n2018; Yang et al., 2022; Gafni et al., 2022), modifying the generative process of diffusion models to align with\nthe underlying specifications (Feng et al., 2023; Huang et al., 2023c;b), multi-concept customization (Kumari\net al., 2023; Han et al., 2023; Gu et al., 2023; Kwon et al., 2024; Kong et al., 2024), or composing a series of\nindependent models that enforce desired constraints (Du et al., 2020; Liu et al., 2021; Nie et al., 2021; Liu\net al., 2022; Li et al., 2023; Du et al., 2023).\nHowever, these methods typically operate at the concept level, where generative models excel in creating\nimages based on broader categories or general concepts. For example, a model might be prompted to generate\nan image of “a woman wearing a dress”, and can adeptly accommodate variations in the textual description,\nsuch as changing the color of the dress. Yet, they struggle to accurately render specific, user-defined elements,\nlike lesser-known characters or unique dress styles. Another line of work that can compose user-defined objects\ninto images (Huang et al., 2023c; Ruiz et al., 2023). However, these methods require extensive fine-tuning\nand do not perform well on multiple objects. Therefore, we introduce learning-free instance-level composition\napproaches utilizing LoRA, enabling the precise assembly of user-specified elements in image generation.\n2.2\nLoRA-based Manipulations\nLeveraging large language models (LLMs) or diffusion models as the base model, recent research aims to\nmanipulate LoRA weights to achieve a range of objectives: element composition in image generation (Ryu,\n2023; Shah et al., 2023), enhancing or diminishing certain capabilities in LLMs (Zhang et al., 2023a; Huang\n3\n\nPublished in Transactions on Machine Learning Research (11/2024)\nX!\nX\"\nX#\nX$\nX%\nX&\n...\n...\n...\n(1) LoRA Merge\n(2) LoRA Switch\n(3) LoRA Composite\n...\n...\nX!\nX\"\nX#\nX$\nX%\nX&\n...\n...\n...\n...\n...\nX!\nX\"\nX#\nX$\nX%\nX&\n...\n...\n...\n...\n...\nFigure 2: Overview of three multi-LoRA composition techniques, where each colored LoRA represents a\ndistinct element. The prevalent approach, LoRA Merge, linearly merges multiple LoRAs into a single\none. In contrast, our methods concentrate on the denoising process: LoRA Switch cycles through different\nLoRAs during the denoising, while LoRA Composite involves all LoRAs working together as the guidance\nthroughout the generation process.\net al., 2023a), incorporating world knowledge (Dou et al., 2023), and transferring parametric knowledge from\nlarger teacher models to smaller student models (Zhong et al., 2023). Regarding LoRA composition techniques,\nboth LoRAHub (Huang et al., 2023a) and ZipLoRA (Shah et al., 2023) employ few-shot demonstrations to\nlearn coefficient matrices for merging LoRAs, enabling the fusion of multiple LoRAs into a singular new\nLoRA. On the other hand, LoRA Merge (Ryu, 2023; Zhang et al., 2023a) introduces addition and negation\noperators to merge LoRA weights through arithmetic operations.\nNevertheless, these weight-based methods often lead to instability in the merging process as the number of\nLoRAs increases (Huang et al., 2023a). They also fail to account for the interactive dynamics when applying\nthe LoRA model in conjunction with the base model. To address these issues, our study explores a new\nperspective: instead of altering the weights of LoRAs, we maintain all LoRA weights intact and focus on the\ninteractions between LoRAs and the underlying generative process.\n3\nMethod\nIn this section, we begin with an overview of essential concepts for understanding multi-LoRA composition,\nfollowed by detailed descriptions of our proposed methods.\n3.1\nPreliminary\nDiffusion Models.\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Dhariwal & Nichol,\n2021; Song et al., 2021; Nichol et al., 2022) represent a class of generative models adept at crafting data\nsamples from Gaussian noise through a sequential denoising process. They build upon a sequence of denoising\nautoencoders that estimate the score of a data distribution (Hyvärinen, 2005). Given an image x, the encoder\nE is used to map x into a latent space, thus yielding an encoded latent z = E(x). The diffusion process\nintroduces noise to z, resulting in latent representation zt with different noise levels over timestep t ∈T .\n4\n\nPublished in Transactions on Machine Learning Research (11/2024)\nThe diffusion model ϵθ with learnable parameters θ is trained to predict the noise added to the noisy latent\nzt given text instruction conditioning cT . Typically, a mean-squared error loss function is utilized as the\ndenoising objective:\nL = EE(x),ϵ∼N(0,1),t\n\u0002\n||ϵ −ϵθ(zt, t, cT )||2\n2\n\u0003\n,\n(1)\nwhere ϵ is the additive Gaussian noise. In this paper, we investigate multi-LoRA composition based on\ndiffusion models, which is consistent with the settings of previous studies on LoRA merging (Ryu, 2023; Shah\net al., 2023).\nClassifier-Free Guidance.\nIn diffusion-based generative modeling, classifier-free guidance (Ho & Salimans,\n2022) balances the trade-off between the diversity and quality of the generated images, particularly in scenarios\nwhere the model is conditioned on classes or textual descriptions. For the text-to-image task, it operates\nby directing the probability mass towards outcomes where the implicit classifier pθ(c|zt) predicts a high\nlikelihood for the textual conditioning c. This necessitates the diffusion models to undergo a joint training\nparadigm for both conditional and unconditional denoising. Subsequently, during inference, the guidance scale\ns ≥1 is used to adjust the score function ˜eθ(zt, c) by moving it closer to the conditional estimation eθ(zt, c)\nand further from the unconditional estimation eθ(zt), enhancing the conditioning effect on the generated\nimages, as formalized in the following expression:\n˜eθ(zt, c) = eθ(zt) + s · (eθ(zt, c) −eθ(zt)).\n(2)\nLoRA Merge.\nLow-Rank Adaptation (LoRA) approach (Hu et al., 2022) enhances parameter efficiency by\nfreezing the pre-trained weight matrices and integrating additional trainable low-rank matrices within the\nneural network. This method is founded on the observation that pre-trained models exhibit low “intrinsic\ndimension” (Aghajanyan et al., 2021). Concretely, for a weight matrix W ∈Rn×m in the diffusion model\nϵθ, the introduction of a LoRA module involves updating W to W ′, defined as W ′ = W + BA. Here,\nB ∈Rn×r and A ∈Rr×m are matrices of a low-rank factor r, satisfying r ≪min(n, m). The concept of\nLoRA Merge (Ryu, 2023) is realized by linearly combining multiple LoRAs to synthesize a unified LoRA,\nsubsequently plugged into the diffusion model. Formally, when introducing k distinct LoRAs, the consequent\nupdated matrix W ′ in ϵθ is given by:\nW ′ = W +\nk\nX\ni=1\nwi × BiAi,\n(3)\nwhere i denotes the index of the i-th LoRA, and wi is a scalar weight, typically a hyperparameter determined\nthrough empirical tuning. LoRA Merge has emerged as a dominant approach for presenting multiple elements\ncohesively in an image, offering a straightforward baseline for various applications. However, merging too\nmany LoRAs at once can destabilize the merging process (Huang et al., 2023a), and it completely overlooks\nthe interaction with the diffusion model during the generative process, resulting in the deformation of the\nhamburger and fingers in Figure 2.\n3.2\nMulti-LoRA Composition through a Decoding-Centric Perspective\nTo address the above issues, we base our approach on the denoising process and investigate how to perform\ncomposition while maintaining the LoRA weights unchanged. This is specifically divided into two perspectives:\nin each denoising step, either activate only one LoRA or engage all LoRAs to guide the generation.\nLoRA Switch (LoRA-s).\nTo explore activating a single LoRA in each denoising step, we present LoRA\nSwitch. This method introduces a dynamic adaptation mechanism within diffusion models by sequentially\nactivating individual LoRAs at designated intervals throughout the generation process. As illustrated in\nFigure 2, each LoRA is represented by a unique color corresponding to a specific element, with only one\nLoRA engaged per denoising step.\n5\n\nPublished in Transactions on Machine Learning Research (11/2024)\nWith a set of k LoRAs, the methodology initiates with a prearranged sequence of permutations; in the\nexample of the Figure, the sequence progresses from yellow to green to blue LoRAs. Starting from the first\nLoRA, the model transitions to the subsequent LoRA every τ step. This rotation persists, allowing each\nLoRA to be applied in turn after kτ steps, thereby endowing each element to contribute repeatedly to the\nimage generation. The active LoRA at each denoising timestep t, ranging from 1 to the total number of steps\nrequired, is determined by the following equations:\ni = ⌊((t −1) mod (kτ))/τ⌋+ 1,\nW ′\nt = W + wi × BiAi.\n(4)\nIn this formula, i indicates the index of the currently active LoRA, iterating from 1 to k. The floor function\n⌊·⌋guarantees the integer value of i is appropriately computed for t. The resulting weight matrix W ′\nt is\nupdated to reflect the contribution from the active LoRA. By selectively enabling one LoRA at a time, LoRA\nSwitch ensures focused attention to the details pertinent to the current element, thus preserving the integrity\nand quality of the generated image throughout the process.\nLoRA Composite (LoRA-c).\nTo explore incorporating all LoRAs at each timestep without merging\nweight matrices, we propose LoRA Composite (LoRA-c), an approach grounded in the Classifier-Free\nGuidance paradigm. Previous research has primarily focused on modifying CFG to enable diffusion models\nto emphasize textual concepts (Liu et al., 2022; Du et al., 2023; Sohn et al., 2023). In contrast, our method\nextends this by enabling CFG to condition on LoRAs, facilitating the generation of images that reflect specific\nelements or instances rather than abstract concepts. LoRA-c involves calculating both unconditional and\nconditional score estimates for each LoRA individually at every denoising step. By aggregating these scores,\nthe technique ensures balanced guidance throughout the image generation process, facilitating the cohesive\nintegration of all elements represented by different LoRAs.\nFormally, with k LoRAs in place, let θ′\ni denote the parameters of the diffusion model eθ after incorporating\nthe i-th LoRA. The collective guidance ˜e(zt, c) based on textual condition c is derived by aggregating the\nscores from each LoRA, as depicted in the equation below:\n˜e(zt, c) = 1\nk\nk\nX\ni=1\nwi ×\nh\neθ′\ni(zt) + s · (eθ′\ni(zt, c) −eθ′\ni(zt))\ni\n.\n(5)\nHere, wi is a scalar weight allocated to each LoRA, intended to adjust the influence of the i-th LoRA. In this\npaper, we set wi to 1, giving each LoRA equal importance. LoRA-c assures that every LoRA contributes\neffectively at each stage of the denoising process, addressing the potential issues of robustness and detail\npreservation that are commonly associated with merging LoRAs.\nOverall, we are the first to adopt a decoding-centric perspective in multi-LoRA composition, steering clear of\nthe instability inherent in weight manipulation on LoRAs. Our study introduces two training-free methods\nfor activating either one or all LoRAs at each denoising step, with their comparative analysis presented in\nSections §4.2 and §4.3.1.\n4\nExperiments\n4.1\nExperimental Setup\nComposLoRA Testbed.\nDue to the absence of standardized benchmarks and automated evaluation met-\nrics, existing studies involving evaluation for composable image generation lean heavily on quantitative\nanalysis (Huang et al., 2023b; Wang et al., 2023) and human effort (Shah et al., 2023), which also limits\nthe advancements of multi-LoRA composition. To bridge this gap, we introduce a comprehensive testbed\nComposLoRA designed to facilitate comparative analysis of various composition approaches. This testbed\nbuilds upon a collection of public LoRAs1, which are extensively shared and recognized as essential plug-in\nmodules in this field. The selection of LoRAs for this testbed adheres to the following criteria:\n1Collected from https://civitai.com/.\n6\n\nPublished in Transactions on Machine Learning Research (11/2024)\n• Each LoRA should be robustly trained, ensuring it can accurately replicate the specific elements it\nrepresents when integrated independently;\n• The elements represented by the LoRAs should cover a diverse range of categories and demonstrate\nadaptability across different image styles;\n• When composed, LoRAs from different categories should be compatible, preventing any conflicts in the\nresulting image composition.\nTable 1: Comparative evaluation with GPT-4V. The evalu-\nation prompt and result are in a simplified version.\nEvaluation Prompt\nI need assistance in comparatively evaluating two text-to-image\nmodels based on their ability to compose different elements\ninto a single image. The key elements are:\n1. Character: kamado nezuko, black hair, pink eyes, forehead\n2. Background: bamboolight, outdoors, bamboo\n3. Object: two-handed burger, holding a huge burger\nPlease help me rate based on composition and image quality:\n[Evaluation criteria and format requirements]\nEvaluation Results from GPT-4V\nFor Image 1:\nComposition Quality:\n- The character’s hair color is not black as specified (-1 point).\n- The burger shows a deformed connection which detracts from\nthe quality of the object’s depiction (-3 points).\n- The character’s fingers are deformed (-1 point).\nImage Quality:\n- Deformities are present in the character’s fingers and the\nburger, which significantly impacts the quality. (-3 points).\n...\nScores:\nImage 1: Composition Quality: 5/10, Image Quality: 7/10\nImage 2: Composition Quality: 10/10, Image Quality: 10/10\nConsequently, we curate two unique subsets of\nLoRAs representing realistic and anime styles.\nEach subset comprises a variety of elements: 3\ncharacters, 2 types of clothing, 2 styles, 2 back-\ngrounds, and 2 objects, culminating in a total of\n22 LoRAs in ComposLoRA. In constructing com-\nposition sets, we strictly follow a crucial princi-\nple: each set must include one character LoRA\nand avoid duplication of element categories to\nprevent conflicts. Thus, the ComposLoRA evalu-\nation incorporates a total of 480 distinct compo-\nsition sets. This includes 48 sets comprising 2\nLoRAs, 144 sets with 3 LoRAs, 192 sets featur-\ning 4 LoRAs, and 96 sets containing 5 LoRAs.\nKey features for each LoRA are manually anno-\ntated and serve dual purposes: they act as input\nprompts for the text-to-image models to gen-\nerate images, and also provide reference points\nfor subsequent evaluations using GPT-4V. De-\ntailed descriptions of each LoRA can be found\nin Table 9 in the Appendix.\nComparative Evaluation with GPT-4V.\nWhile existing metrics can calculate the align-\nment between text and images (Hessel et al.,\n2021; Ku et al., 2023), they fall short in assess-\ning the intricacies of specific elements within\nan image and the quality of their composition.\nRecently, multimodal large language models\nlike GPT-4V (OpenAI, 2023a;b) have signifi-\ncant progress and promise in various multimodal\ntasks, underscoring their potential in evaluating\nimage generation tasks (Lin et al., 2023; Zhang\net al., 2023b). In our study, we leverage GPT-\n4V’s capabilities to serve as an evaluator for\ncomposable image generation.\nSpecifically, we employ a comparative evaluation method, utilizing GPT-4V to rate generated images across\ntwo dimensions: composition quality and image quality. We utilize a 0 to 10 scoring scale, with higher scores\nindicating superior quality. GPT-4V is provided with a prompt that includes the essential features of the\nelements to be composed, the criteria for scoring in the two dimensions, and the format for the expected\noutput. The complete evaluation prompts and results are available in Tables 7 and 8 in Appendix. This\nexperimental setup allows us to compare the efficacy of each of the two proposed methods against the LoRA\nMerge approach. Additionally, we examine how GPT-4V-based scoring aligns with human judgment in\nSection §4.2 and explore the potential biases of using it as an evaluator in Section §4.3.3.\n7\n\nPublished in Transactions on Machine Learning Research (11/2024)\nImplementation Details.\nFor our experiments, we employ stable-diffusion-v1.5 (Rombach et al., 2022)\nas the backbone model. We utilize two specific checkpoints for our experiments: “Realistic_Vision_V5.1”2\nfor realistic images and “Counterfeit-V2.5”3 for anime images, each fine-tuned to their respective styles.\nIn the realistic style subset, we configure the model with 100 denoising steps, a guidance scale s of 7, and\nset the image size to 1024x768, optimizing for superior image quality. For the anime style subset, the\nsettings differ slightly with 200 denoising steps, a guidance scale s of 10, and an image size of 512x512. The\nDPM-Solver++ (Lu et al., 2022a;b) is used as the scheduler in the generation process. The weight scale w is\nconsistently set at 0.8 for composing LoRAs within ComposLoRA. For the LoRA Switch approach, we apply a\ncycle with τ set to 5, meaning every 5 denoising steps activate the next LoRA in the sequence: character,\nclothing, style, background, then object. Since the proposed methods do not require additional training, all\nexperiments are conducted on a single A6000 GPU. To ensure the reliability of our experimental results, we\nconduct image generation using three random seeds. All reported results in this paper represent the average\nevaluation scores across these three runs.\n4.2\nResults on ComposLoRA\nGPT-4V-based Evaluation.\nWe first present the comparative evaluation results using GPT-4V. This\nevaluation involves scoring the performance of LoRA-s versus LoRA Merge, and LoRA-c versus LoRA\nMerge across two dimensions, as well as determining the winner based on these scores. Specific scores and\nwin rates are illustrated in Figure 3, leading to several key observations:\n8.69\n8.11\n7.2\n6.56\n8.65\n7.96\n6.68\n5.24\n4.5\n5\n5.5\n6\n6.5\n7\n7.5\n8\n8.5\n9\n9.5\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nComposition Quality\nLoRA Switch\nLoRA Merge\n9.53\n9.46\n9.36\n9.28\n9.46\n9.4\n9.27\n8.95\n8.8\n8.9\n9\n9.1\n9.2\n9.3\n9.4\n9.5\n9.6\n9.7\n9.8\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nImage Quality\nLoRA Switch\nLoRA Merge\n(a) LoRA Switch vs. LoRA Merge (Scoring)\n8.85\n8.12\n7.1\n6.09\n8.74\n7.87\n6.68\n5.17\n4.5\n5\n5.5\n6\n6.5\n7\n7.5\n8\n8.5\n9\n9.5\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nComposition Quality\nLoRA Composite\nLoRA Merge\n9.72\n9.56\n9.43\n9.34\n9.53\n9.41\n9.15\n8.92\n8.8\n8.9\n9\n9.1\n9.2\n9.3\n9.4\n9.5\n9.6\n9.7\n9.8\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nImage Quality\nLoRA Composite\nLoRA Merge\n(b) LoRA Composite vs. LoRA Merge (Scoring)\n23\n21\n18\n17\n34\n31\n24\n14\n43\n48\n58\n69\n0%\n20%\n40%\n60%\n80%\n100%\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nComposition Quality\nLose\nTie\nWin\n10\n16\n14\n14\n62\n49\n48\n40\n28\n35\n38\n46\n0%\n20%\n40%\n60%\n80%\n100%\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nImage Quality\nLose\nTie\nWin\n(c) LoRA Switch vs. LoRA Merge (Win Rate)\n16\n24\n28\n23\n51\n32\n23\n22\n33\n44\n49\n55\n0%\n20%\n40%\n60%\n80%\n100%\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nComposition Quality\nLose\nTie\nWin\n11\n14\n13\n10\n54\n46\n42\n34\n35\n40\n45\n56\n0%\n20%\n40%\n60%\n80%\n100%\n2 LoRAs\n3 LoRAs\n4 LoRAs\n5 LoRAs\nImage Quality\nLose\nTie\nWin\n(d) LoRA Composite vs. LoRA Merge (Win Rate)\nFigure 3: Results of comparative evaluation on ComposLoRA using GPT-4V.\n• Our proposed method consistently outperforms LoRA Merge across all configurations and in both dimensions,\nwith the margin of superiority increasing as the number of LoRAs grows. For instance, as shown in\nFigure 3(a), the score advantage of LoRA Switch escalates from 0.04 with 2 LoRAs to 1.32 with 5\nLoRAs. This trend aligns with the win rate observed in Figure 3(c), where the win rate approaches 70%\nwhen composing 5 LoRAs.\n• LoRA-S shows superior performance in composition quality, whereas LoRA-C excels in image quality.\nIn scenarios involving 5 LoRAs and using LoRA Merge as a baseline, the win rate of LoRA-s in\n2https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE.\n3https://huggingface.co/gsdf/Counterfeit-V2.5.\n8\n\nPublished in Transactions on Machine Learning Research (11/2024)\ncomposition quality surpasses that of LoRA-c by 14% (69% vs. 55%). Conversely, for image quality,\nLoRA-c’s win rate is 10% higher than that of LoRA-s (56% vs. 46%).\n• The task of compositional image generation remains highly challenging, especially as the number of elements\nto be composed increases. According to GPT-4V’s scoring, the average score for composing 2 LoRAs is\nabove 8.5, but it sharply declines to around 6 for compositions involving 5 LoRAs. Hence, despite the\nconsiderable improvements our methods offer, there is still substantial room for further research in the\nfield of compositional image generation.\nTable 2: Human evaluation results and Pearson correla-\ntion between different metrics and human judgment.\nHuman Evaluation\nComposition\nImage Quality\nLoRA Merge\n3.14\n2.94\nLoRA Switch\n3.91\n4.15\nLoRA Composite\n3.78\n4.35\nCorrelations with Human Judgments\nComposition\nImage Quality\nCLIPScore\n-0.006\n0.083\nOurs\n0.454\n0.457\nHuman Evaluation.\nTo complement our results,\nwe conduct a human evaluation to assess the ef-\nfectiveness of different methods and validate the\nefficacy of the evaluators.\nTwo graduate students rate 120 images on com-\npositional and image quality using a 1-5 Likert\nscale: 1 signifies complete failure, 2-4 represents\nsignificant, moderate, and minor issues, respec-\ntively, while 5 denotes perfect execution. To ensure\nconsistency, the annotators initially pilot-score 20\nimages to standardize their understanding of the\ncriteria.\nThe results, summarized in the upper\nsection of Table 2, align with GPT-4V’s findings,\nconfirming our methods outperform LoRA Merge\n— with LoRA Switch excelling in composition\nand LoRA Composite in image quality.\nFurthermore, we analyze the Pearson correlations between human evaluations and scores derived from GPT-4V\nand CLIPScore (Hessel et al., 2021), with results presented in the lower section of Table 2. This comparison\nreveals that CLIPScore’s evaluations fall short in assessing specific compositional and quality aspects due\nto its inability to discern the nuanced features of each element. In contrast, the evaluator we adopt shows\nsubstantially higher correlations with human judgments, affirming the validity of our evaluation framework.\n4.3\nAnalysis\nTo enhance our understanding of the proposed methods, we further investigate the following questions:\n4.3.1\nDo Specific Image Styles Favor Different Methods?\n16\n25\n12\n9\n26\n30\n46\n47\n58\n45\n42\n44\n0%\n20%\n40%\n60%\n80%\n100%\nLoRA-S\nCompos.\nLoRA-C\nCompos.\nLoRA-S\nImage\nLoRA-C\nImage\nRealistic Style\nLose\nTie\nWin\n23\n26\n17\n16\n24\n24\n50\n39\n53\n50\n33\n45\n0%\n20%\n40%\n60%\n80%\n100%\nLoRA-S\nCompos.\nLoRA-C\nCompos.\nLoRA-S\nImage\nLoRA-C\nImage\nAnime Style\nLose\nTie\nWin\nFigure 4: Analysis on image styles. In general, LoRA-s\nis more adept at realistic styles, while LoRA-c has better\nperformance in anime styles.\nTo explore the impact of image style, we sepa-\nrately evaluate the performance of methods\non realistic and anime-style subsets within\nComposLoRA. The win rate results, presented\nin Figure 4, reveal distinct tendencies for each\nmethod.\nOur observations reveal that, while LoRA-s\nmay not excel in image quality compared to\nLoRA-c, it demonstrates comparable perfor-\nmance in this dimension within the realistic\nstyle subset, while maintaining a significant\nedge in composition quality. In contrast, in\nthe anime-style subset, LoRA-c, shows a per-\nformance on par with LoRA-s in composition\nquality, while notably surpassing it in image\nquality. These findings suggest that LoRA-S is\n9\n\nPublished in Transactions on Machine Learning Research (11/2024)\nmore adept at composing elements in realistic-style images, whereas LoRA-C shows a stronger performance in\nanime-style imagery.\n4.3.2\nHow Does the Step Size and Order of LoRA Activation Affect LoRA Switch?\nTo identify the optimal configuration for LoRA Switch, we examine the influence of two crucial hyperpa-\nrameters: the sequence in which LoRAs are activated and the interval between each activation. Our findings,\ndepicted in Figure 5(a), show that overly frequent switching, such as changing LoRAs at every denoising step,\nleads to distortions in generated images and suboptimal performance. The efficiency of the LoRA Switch\nimproves progressively with increased step size, reaching peak performance at τ = 5.\n1\n2\n3\n4\n5\n6\n7\n8\n30\n40\n50\n60\nNumber of steps to switch LoRA (τ)\nWin Rate (%)\nComposition\nImage Quality\n(a) Steps to Switch\nComposition Quality\nImage Quality\n7\n7.5\n8\n8.5\n9\n9.5\nAverage Score\nCharacter\nClothing\nStyle\nBackground\nObject\nRandom\n(b) Activation Order\nFigure 5: Analysis of the number of denoising steps to switch\nLoRA and the activation order for LoRA Switch. In Figure 5(b),\n“Character” indicates that the character LoRA is activated first,\nwith the rest being activated randomly.\nMoreover, our analysis underscores that\nthe initial choice of LoRA in the activa-\ntion sequence clearly influences overall\nperformance, while alterations in the sub-\nsequent order have minimal impact. Acti-\nvating the character LoRA first leads to\nthe best performance, as demonstrated\nin Figure 5(b). In contrast, starting with\nclothing, background, or object LoRAs\nyields results comparable to a completely\nrandomized sequence.\nNotably, begin-\nning with the style LoRA leads to a no-\nticeable performance drop, even falling\nslightly below a random order. This ob-\nservation underlines the critical role of\nprioritizing core image elements in the\ninitial stage of the generation process to\nenhance both the image and composi-\ntional quality for LoRA Switch.\nWhile the step size for switching LoRAs proves to be a crucial factor in achieving optimal performance in our\nexperiments, we also explore the potential of dynamic strategies for step size adjustment throughout the\ndenoising process. Specifically, we design and evaluate three strategies for dynamically adjusting the step size\nin LoRA-Switch:\n• Incremental Strategy: The step size gradually increases from τ = 3 to τ = 5 throughout the\ndenoising process.\n• Decremental Strategy: The step size gradually decreases from τ = 5 to τ = 3 as the denoising\nprocess progresses.\n• Warm-up Strategy: During the initial 50% of the denoising process, the step size increases from\nτ = 3 to τ = 5 and remains constant at τ = 5 for the remaining denoising steps.\nTable 3: Performance comparison of dynamic strategies for LoRA-S.\nτ = 3\nτ = 4\nτ = 5\nτ = 6\nIncremental (3 →5)\nDecremental (5 →3)\nWarm-up\nComposition Quality\n55\n57\n59\n58\n57\n54\n58\nImage Quality\n29\n33\n34\n33\n33\n31\n34\nTable 3 shows the results of these strategies. Neither the Incremental nor the Warm-up strategies significantly\nimprove performance compared to using a fixed step size. The Decremental strategy, on the other hand,\nresults in comparatively worse performance, highlighting that switching LoRAs too frequently in the latter\n10\n\nPublished in Transactions on Machine Learning Research (11/2024)\nstages of denoising is detrimental to image quality. The fixed step size of τ = 5 yields the best performance.\nConsequently, we adopt this fixed step size in our experiments.\n4.3.3\nDoes GPT-4V Exhibit Bias as an Evaluator?\nWhile GPT-4V has demonstrated utility in evaluating various image generation tasks (Lin et al., 2023; Zhang\net al., 2023b), our analysis uncovers a notable positional bias in its comparative evaluations. We investigate\nthis potential bias by swapping the positions of images generated by different methods before inputting them\nto GPT-4V, and the results are illustrated in Figure 6.\n15\n24\n21\n29\n25\n25\n27\n29\n60\n51\n52\n42\n0%\n20%\n40%\n60%\n80%\n100%\nMerge\nFirst\nLoRA-S\nFirst\nMerge\nFirst\nLoRA-C\nFirst\nComposition Quality\nLose\nTie\nWin\n18\n11\n17\n10\n48\n48\n43\n42\n34\n41\n40\n48\n0%\n20%\n40%\n60%\n80%\n100%\nMerge\nFirst\nLoRA-S\nFirst\nMerge\nFirst\nLoRA-C\nFirst\nImage Quality\nLose\nTie\nWin\nFigure 6: Positional bias analysis for GPT-4V-based evaluation. In\neach subfigure, the left side of the orange line compares LoRA-s\nwith Merge, and the right side contrasts LoRA-c with Merge.\n“Merge First” indicates that the image produced by LoRA Merge\nis the first image input during the comparative evaluation.\nIn the comparison of LoRA-s versus\nLoRA Merge, when the image gener-\nated by Merge is presented first (“Merge\nFirst”), the win rate for LoRA-s in com-\nposition quality stands at 60%.\nHow-\never, this win rate declines to 51%\nwhen LoRA-s’s image is the first input\n(“LoRA-S First”). Similarly, LoRA-c’s\nwin rate decreases from 52% to 42%, sug-\ngesting that GPT-4V tends to favor the\nsecond image input in terms of compo-\nsition quality. Intriguingly, the opposite\ntrend is observed in image quality, where\nthe second image tends to receive a higher\nscore. These results indicate a significant\npositional bias in GPT-4V’s evaluation,\nvarying with the dimension and the posi-\ntion of the images. To mitigate this bias\nin our study, the comparative evaluation\nresults reported in this paper are averaged across both input orders.\n4.4\nMore Visual Examples\nTo demonstrate the effectiveness of our methods in composing varying numbers of LoRAs and under different\nimage styles, we provide additional visual examples in Figures 7 – 10.\nObject\nBubble Gum\nCharacter 1\nCharacter 2\nLoRA Merge\nOurs\nFigure 7: Case study on composing 2 LoRAs in the realistic style.\n11\n\nPublished in Transactions on Machine Learning Research (11/2024)\nCharacter 4\nLoRA Merge\nOurs\nCharacter 3\nClothing\nFigure 8: Case study on composing 2 LoRAs in the anime style.\nBackground 1\nLibrary Bookshelf\nBackground 2\nForest & River\nLoRA Merge\nOurs\nClothing\nCharacter\nFigure 9: Case study on composing 3 LoRAs in the realistic style.\n5\nFurther Discussions\n5.1\nLimitations\nBased on our experiments, the primary limitation of the proposed methods is the efficiency issue with\nLoRA-c. This method can introduce (k −1)× additional computational cost, where k is the number of\n12\n\nPublished in Transactions on Machine Learning Research (11/2024)\nLoRA Merge\nOurs\nCharacter\nBackground 3\nAuroral\nObject 1\nHamburger\nObject 2\nToast\nFigure 10: Case study on composing 3 LoRAs in the anime style.\nmerged LoRAs. This is due to LoRA-c merging each LoRA with the base model to calculate scores, which\nare then averaged. The inherent design of LoRA prevents the pre-computation of the base model. To address\nthis, we propose two potential solutions: 1) Integrating advanced techniques with fewer denoising steps, and\n2) a combination of LoRA-s and LoRA-c.\n5.1.1\nIntegration of LCM and LCM-LoRA\nRecently, several algorithms have been developed that require only 2-8 denoising steps to generate high-quality\nimages. We select LCM (Luo et al., 2023a) and LCM-LoRA (Luo et al., 2023b) to test if our approaches can\nintegrate smoothly with these algorithms.\nTable 4: Results for integrating LCM and LCM-LoRA.\nIntegrated Technique\nMethod\nWin (%)\nTie (%)\nLose (%)\nInference Steps\nNone\nLoRA Switch\n43\n34\n23\n200\nLoRA Composite\n33\n51\n16\n200\nLCM\nLoRA Switch\n45\n36\n19\n4\nLoRA Composite\n39\n43\n18\n4\nLCM-LoRA\nLoRA Switch\n42\n39\n19\n8\nLoRA Composite\n44\n41\n15\n8\nFirst, we conduct experiments on 2 LoRAs using the same setup as described in the main text, with results\nshown in Table 4. Remarkably, our methods not only outperform the baseline but also show even greater\nadvantages when integrated with these inference-accelerating techniques. This adaptation significantly reduces\nthe required number of denoising steps to 4-8, effectively addressing the increased computational demands of\nLoRA-c. Consequently, the generation times across all three methods are now comparably short, taking\nonly a few seconds.\nTo further explore the potential of integrating our method with fewer denoising steps, we conduct additional\nexperiments using LCM-LoRA with an increased number of LoRAs. These experiments aim to provide deeper\ninsights into how our approach performs as the complexity of multi-LoRA composition increases. The results,\nshown in Table 5, reflect the composition quality scores under this setup. For these experiments, we use 8\n13\n\nPublished in Transactions on Machine Learning Research (11/2024)\ndenoising steps for 2 and 4 LoRAs, and 9 inference steps for 3 LoRAs, switching to the next LoRA every 3\nsteps for LoRA-s.\nTable 5: Composition quality scores for different methods when using LCM-LoRA with more LoRAs.\n2 LoRAs\n3 LoRAs\n4 LoRAs\nLoRA Merge\n7.52\n5.16\n3.49\nLoRA Switch\n8.08\n6.39\n5.08\nLoRA Composite\n8.13\n6.07\n4.53\nOur methods exhibit a more pronounced advantage over the baseline in this fewer-steps setting.\nThe\nimprovement is particularly noticeable when the number of LoRAs increases, demonstrating the robustness\nof our approach even when integrated with models requiring fewer denoising steps. However, it is important\nto note that all methods experience a substantial drop in absolute scores when combined with models like\nLCM-LoRA that employ fewer denoising steps. For instance, the composition quality score for LoRA-c with\n5 LoRAs is initially 6.56 for 200 denoising steps, but with 4 LoRAs in this setting, the score drops to 5.08.\nThis finding suggests that despite the improved performance of our method, multi-LoRA composition remains\na challenging task, especially when fewer denoising steps are used, even with the latest integration techniques.\n5.1.2\nCombination of LoRA-s and LoRA-c\nTo further enhance efficiency, we propose combining LoRA-s and LoRA-c. LoRA-s activates at the LoRA\nstage before each denoising step, while LoRA-c is applied to the CFG during the denoising process. These\ndesign principles are complementary. A practical integration method involves selecting a subset of LoRAs\nto activate (ranging from one to all) for each denoising step, following the LoRA-s strategy. This subset\nwould then utilize all its LoRAs during the denoising phase, adhering to the LoRA-c strategy. For LCM\nand other related applications, combining LoRA-s and LoRA-c can enhance efficiency without additional\nmodifications. For example, in a 1-step scenario, all LoRAs can be activated and applied through CFG (as\nper LoRA-c). In a 2-step scenario, half of the LoRAs can be activated at each step and then applied through\nCFG, blending LoRA-s and LoRA-c.\n5.2\nComparison with ZipLoRA\nTable 6: Comparison with ZipLoRA in two different LoRA setups.\nLoRA Setup\nMethods\nComposition Quality\nImage Quality\nCharacter + Style\nLoRA-S\n8.80\n8.95\nLoRA-C\n8.55\n9.20\nZipLoRA\n9.05\n9.40\nCharacter + Object\nLoRA-S\n8.85\n9.05\nLoRA-C\n8.60\n9.25\nZipLoRA\n8.50\n9.10\nAlthough our proposed methods are training-free, we also compare them with the fine-tuning approach\nZipLoRA as a baseline for reference. ZipLoRA is based on SDXL and focuses on merging two LoRAs, so we\nconduct our comparisons under this setup. Specifically, we randomly select publicly available SDXL LoRAs\nfrom HuggingFace and create 10 composition sets, combining character + style and character + object for\nthe experiments. All results are compared against the LoRA Merge, and the scores are presented in Table 6.\nSince ZipLoRA is specifically designed to merge subject and style LoRAs, it achieves higher scores in the\ncharacter + style setup compared to our methods, likely due to the benefits of its fine-tuning process. However,\n14\n\nPublished in Transactions on Machine Learning Research (11/2024)\nin scenarios involving two subjects, such as character + object, our methods demonstrate clear advantages\ndespite being training-free. LoRA-S outperforms in composition quality, while LoRA-C excels in image\nquality. This indicates that our methods are particularly effective in handling diverse subject combinations,\nespecially when the task involves composing two subjects rather than merging a subject with a style.\n6\nConclusion\nIn this paper, we present the first exploration of multi-LoRA composition from a decoding-centric perspective\nby introducing LoRA-s and LoRA-c that transcend the limitations of current weight manipulation techniques.\nThrough establishing a dedicated testbed ComposLoRA, we introduce scalable automated evaluation metrics\nutilizing GPT-4V. Our study not only highlights the superior quality achieved by our methods but also\nprovides a new standard for evaluating LoRA-based composable image generation.\nBroader Impact Statement\nOur approaches offer advancements in personalized image generation and customized digital content creation\nby allowing the combination of arbitrary elements. This capability can be applied to various real-world\nscenarios, such as virtual try-on and virtual design, leading to positive social impacts. As our method operates\nin the inference phase and relies solely on the composition of publicly available checkpoints (base models and\nLoRAs) without requiring additional training, it avoids any negative impact related to model training.\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness\nof language model fine-tuning.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pp. 7319–7328. Association for Computational Linguistics, 2021.\ndoi: 10.18653/V1/2021.ACL-LONG.568. URL https://doi.org/10.18653/v1/2021.acl-long.568.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Marc’Aurelio\nRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.),\nAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 8780–8794, 2021. URL https:\n//proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html.\nShihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang,\nXiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe:\nRevolutionizing mixture of experts for maintaining world knowledge in language model alignment. CoRR,\nabs/2312.09979, 2023. doi: 10.48550/ARXIV.2312.09979. URL https://doi.org/10.48550/arXiv.2312.\n09979.\nYilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models.\nIn Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.\nneurips.cc/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html.\nYilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-\nDickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation\nwith energy-based diffusion models and MCMC. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,\nBarbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine\nLearning Research, pp. 8489–8510. PMLR, 2023. URL https://proceedings.mlr.press/v202/du23a.\nhtml.\n15\n\nPublished in Transactions on Machine Learning Research (11/2024)\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nPUIqjT4rzq7.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.\nMake-a-\nscene: Scene-based text-to-image generation with human priors. In Shai Avidan, Gabriel J. Brostow,\nMoustapha Cissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 -\n17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, volume 13675 of\nLecture Notes in Computer Science, pp. 89–106. Springer, 2022. doi: 10.1007/978-3-031-19784-0\\_6. URL\nhttps://doi.org/10.1007/978-3-031-19784-0_6.\nYuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao,\nShuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Mix-of-show: Decentralized\nlow-rank adaptation for multi-concept customization of diffusion models. In Alice Oh, Tristan Naumann,\nAmir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/\npaper/2023/hash/3340ee1e4a8bad8d32c35721712b4d0a-Abstract-Conference.html.\nLigong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris N. Metaxas, and Feng Yang. Svdiff: Compact\nparameter space for diffusion fine-tuning. In IEEE/CVF International Conference on Computer Vision,\nICCV 2023, Paris, France, October 1-6, 2023, pp. 7289–7300. IEEE, 2023. doi: 10.1109/ICCV51070.2023.\n00673. URL https://doi.org/10.1109/ICCV51070.2023.00673.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp.\n7514–7528. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.595.\nURL https://doi.org/10.18653/v1/2021.emnlp-main.595.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi:\n10.48550/ARXIV.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle,\nMarc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient\ncross-task generalization via dynamic lora composition. CoRR, abs/2307.13269, 2023a. doi: 10.48550/\nARXIV.2307.13269. URL https://doi.org/10.48550/arXiv.2307.13269.\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and\ncontrollable image synthesis with composable conditions. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of\nMachine Learning Research, pp. 13753–13773. PMLR, 2023b. URL https://proceedings.mlr.press/\nv202/huang23b.html.\n16\n\nPublished in Transactions on Machine Learning Research (11/2024)\nZiqi Huang, Kelvin C. K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face\ngeneration and editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 6080–6090. IEEE, 2023c. doi: 10.1109/CVPR52729.\n2023.00589. URL https://doi.org/10.1109/CVPR52729.2023.00589.\nAapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.,\n6:695–709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.html.\nJustin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018,\npp. 1219–1228. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.\n00133. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Johnson_Image_Generation_\nFrom_CVPR_2018_paper.html.\nZhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and\nWenhan Luo. OMG: occlusion-friendly personalized multi-concept generation in diffusion models. CoRR,\nabs/2403.10983, 2024. doi: 10.48550/ARXIV.2403.10983. URL https://doi.org/10.48550/arXiv.2403.\n10983.\nMax Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub:\nStandardizing the evaluation of conditional image generation models. CoRR, abs/2310.01596, 2023. doi:\n10.48550/ARXIV.2310.01596. URL https://doi.org/10.48550/arXiv.2310.01596.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept cus-\ntomization of text-to-image diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 1931–1941. IEEE, 2023.\ndoi:\n10.1109/CVPR52729.2023.00192. URL https://doi.org/10.1109/CVPR52729.2023.00192.\nGihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, and Fabian Caba Heilbron. Concept\nweaver: Enabling multi-concept fusion in text-to-image models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 8880–8889. IEEE,\n2024. doi: 10.1109/CVPR52733.2024.00848. URL https://doi.org/10.1109/CVPR52733.2024.00848.\nShuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, and Igor Mordatch. Composing ensembles\nof pre-trained models via iterative consensus. In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.\nURL https:\n//openreview.net/pdf?id=gmwDKo-4cY.\nKevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, and Lijuan Wang. Designbench: Exploring and\nbenchmarking DALL-E 3 for imagining visual design. CoRR, abs/2310.15144, 2023. doi: 10.48550/ARXIV.\n2310.15144. URL https://doi.org/10.48550/arXiv.2310.15144.\nNan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba.\nLearning to compose vi-\nsual relations.\nIn Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\nJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:\nAn-\nnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pp. 23166–23178, 2021.\nURL https://proceedings.neurips.cc/paper/2021/hash/\nc3008b2c6f5370b744850a98a95b73ad-Abstract.html.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum.\nCompositional visual\ngeneration with composable diffusion models. In Shai Avidan, Gabriel J. Brostow, Moustapha Cissé,\nGiovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 - 17th European\nConference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII, volume 13677 of Lecture\nNotes in Computer Science, pp. 423–439. Springer, 2022. doi: 10.1007/978-3-031-19790-1\\_26. URL\nhttps://doi.org/10.1007/978-3-031-19790-1_26.\nNan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba. Unsupervised compositional\nconcepts discovery with text-to-image generative models. CoRR, abs/2306.05357, 2023. doi: 10.48550/\nARXIV.2306.05357. URL https://doi.org/10.48550/arXiv.2306.05357.\n17\n\nPublished in Transactions on Machine Learning Research (11/2024)\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver\nfor diffusion probabilistic model sampling in around 10 steps. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\nDanielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022, 2022a. URL http://papers.nips.cc/paper_files/paper/2022/\nhash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for\nguided sampling of diffusion probabilistic models. CoRR, abs/2211.01095, 2022b. doi: 10.48550/ARXIV.\n2211.01095. URL https://doi.org/10.48550/arXiv.2211.01095.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-\nresolution images with few-step inference. CoRR, abs/2310.04378, 2023a. doi: 10.48550/ARXIV.2310.04378.\nURL https://doi.org/10.48550/arXiv.2310.04378.\nSimian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian\nLi, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. CoRR, abs/2311.05556,\n2023b. doi: 10.48550/ARXIV.2311.05556. URL https://doi.org/10.48550/arXiv.2311.05556.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-\nguided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu,\nand Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022,\nBaltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 16784–16804.\nPMLR, 2022. URL https://proceedings.mlr.press/v162/nichol22a.html.\nWeili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-\nspace energy-based models.\nIn Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy\nLiang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-\n14, 2021, virtual, pp. 13497–13510, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n701d804549a4a23d3cae801dac6c2c75-Abstract.html.\nOpenAI. GPT-4: Contributions and System Card. https://cdn.openai.com/contributions/gpt-4v.pdf, 2023a.\nOpenAI. GPT-4v System Card. https://openai.com/research/gpt-4v-system-card, 2023b.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125.\nURL https://doi.org/10.48550/arXiv.2204.06125.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674–10685. IEEE, 2022. doi:\n10.1109/CVPR52688.2022.01042. URL https://doi.org/10.1109/CVPR52688.2022.01042.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023,\npp. 22500–22510. IEEE, 2023. doi: 10.1109/CVPR52729.2023.02155. URL https://doi.org/10.1109/\nCVPR52729.2023.02155.\nSimo Ryu. Merging loras. https://github.com/cloneofsimo/lora, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kam-\nyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep lan-\nguage understanding. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\nec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.\n18\n\nPublished in Transactions on Machine Learning Research (11/2024)\nViraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.\nZiplora: Any subject in any style by effectively merging loras.\nCoRR, abs/2311.13600, 2023.\ndoi:\n10.48550/ARXIV.2311.13600. URL https://doi.org/10.48550/arXiv.2311.13600.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei (eds.), Proceedings\nof the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pp. 2256–2265. JMLR.org, 2015.\nURL\nhttp://proceedings.mlr.press/v37/sohl-dickstein15.html.\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,\nLu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan.\nStyledrop: Text-to-image generation in any style. CoRR, abs/2306.00983, 2023. doi: 10.48550/ARXIV.\n2306.00983. URL https://doi.org/10.48550/arXiv.2306.00983.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nURL https://openreview.net/forum?id=PxTIG12RRHS.\nJosh Tenenbaum. Building machines that learn and think like people. In Elisabeth André, Sven Koenig, Mehdi\nDastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on Autonomous\nAgents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pp. 5. International\nFoundation for Autonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018. URL\nhttp://dl.acm.org/citation.cfm?id=3237389.\nZhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang,\nand Mingyuan Zhou. In-context learning unlocked for diffusion models. CoRR, abs/2305.01115, 2023. doi:\n10.48550/ARXIV.2305.01115. URL https://doi.org/10.48550/arXiv.2305.01115.\nXun Wu, Shaohan Huang, and Furu Wei. Mixture of lora experts. arXiv preprint arXiv:2404.13628, 2024.\nZuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. Modeling image composition for\ncomplex scene generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2022, New Orleans, LA, USA, June 18-24, 2022, pp. 7754–7763. IEEE, 2022. doi: 10.1109/CVPR52688.\n2022.00761. URL https://doi.org/10.1109/CVPR52688.2022.00761.\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with\narithmetic operations. CoRR, abs/2306.14870, 2023a. doi: 10.48550/ARXIV.2306.14870. URL https:\n//doi.org/10.48550/arXiv.2306.14870.\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang\nWang, and Linda Ruth Petzold. Gpt-4v(ision) as a generalist evaluator for vision-language tasks. CoRR,\nabs/2311.01361, 2023b. doi: 10.48550/ARXIV.2311.01361. URL https://doi.org/10.48550/arXiv.\n2311.01361.\nMing Zhong, Chenxin An, Weizhu Chen, Jiawei Han, and Pengcheng He. Seeking neural nuggets: Knowledge\ntransfer in large language models from a parametric perspective. CoRR, abs/2310.11451, 2023. doi:\n10.48550/ARXIV.2310.11451. URL https://doi.org/10.48550/arXiv.2310.11451.\n19\n\nPublished in Transactions on Machine Learning Research (11/2024)\nA\nAppendix\nA.1\nDetails for Comparative Evaluation using GPT-4V\nSee Table 7 for the full prompts and Table 8 for a case study of evaluation results.\nTable 7: The full version of evaluation prompts for comparative evaluation with GPT-4V.\nEvaluation Prompt\nI need assistance in comparatively evaluating two text-to-image models based on their ability to compose different\nelements into a single image. The elements and their key features are as follows:\n1. Character (Kamado Nezuko): kamado nezuko, black hair, pink eyes, forehead\n2. Background (Bamboo Background): bamboolight, outdoors, bamboo\n3. Object (Huge Two-Handed Burger): two-handed burger, holding a huge burger\nPlease help me rate both given images on the following evaluation dimensions and criteria:\nComposition Quality:\n- Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.\n- Deduct 3 points if any element is missing or incorrectly depicted.\n- Deduct 1 point for each missing or incorrect feature within an element.\n- Deduct 1 point for minor inconsistencies or lack of harmony between elements.\n- Additional deductions can be made for compositions that lack coherence, creativity, or realism.\nImage Quality:\n- Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.\n- Deduct 3 points for each deformity in the image (e.g., extra limbs or fingers, distorted face, incorrect proportions).\n- Deduct 2 points for noticeable issues with texture, lighting, or color.\n- Deduct 1 point for each minor flaw or imperfection.\n- Additional deductions can be made for any issues affecting the overall aesthetic or clarity of the image.\nPlease format the evaluation as follows:\nFor Image 1:\n[Explanation of evaluation]\nFor Image 2:\n[Explanation of evaluation]\nScores:\nImage 1: Composition Quality: [score]/10, Image Quality: [score]/10\nImage 2: Composition Quality: [score]/10, Image Quality: [score]/10\nBased on the above guidelines, help me to conduct a step-by-step comparative evaluation of the given images. The\nscoring should follow two principles:\n1. Please evaluate critically.\n2. Try not to let the two models end in a tie on both dimensions.\n20\n\nPublished in Transactions on Machine Learning Research (11/2024)\nTable 8: The full version of evaluation results from GPT-4V for comparative evaluation.\nEvaluation Results from GPT-4V\nTo evaluate the images provided, I’ll assess the composition quality and image quality based on the criteria outlined in\nyour instructions. I will identify each element and its key features, and then rate the images accordingly.\nFor Image 1:\nComposition Quality:\n- Elements: All key elements are present: character, background, and object (0 points deducted).\n- Features within the elements:\n- The character’s hair color is not black as specified (-1 point).\n- The burger shows a deformed connection which detracts from the quality of the object’s depiction (-3 point).\n- The character’s fingers are deformed (-1 point).\nThe bamboo background is accurate, fulfilling the outdoor setting criteria. Despite this, the main issues lie with the\nincorrect hair color and the deformities in the burger and fingers, leading to deductions.\nImage Quality:\n- The image is free from texture, lighting, or color issues (0 points deducted).\n- Deformities are present in the fingers and the burger, which significantly impacts the image’s quality. (-3 point).\nThe clarity of the image is maintained, but the deformities notably affect the overall aesthetic quality.\nFor Image 2:\nComposition Quality:\n- Elements: All key elements are present: character, background, and object (0 points deducted).\n- Features within the elements:\n- The character has black hair and pink eyes; the forehead is shown, and the burger is depicted accurately (0\npoints deducted).\nThere are no inconsistencies or lack of harmony between elements, and the composition is coherent and aesthetically\npleasing.\nImage Quality:\n- There are no deformities in the image (0 points deducted).\n- The texture, lighting, and color are well executed (0 points deducted).\nThe image maintains a high level of clarity and overall aesthetic appeal, with no visible imperfections or issues.\nScores:\nImage 1: Composition Quality: 5/10, Image Quality: 7/10\nImage 2: Composition Quality: 10/10, Image Quality: 10/10\nIn summary, Image 2 achieves perfect scores across both dimensions, demonstrating excellent composition and image\nquality. Image 1, while containing all the key elements, suffers in its composition due to incorrect features and\ndeformities, and in image quality due to the noted deformities.\n21\n\nPublished in Transactions on Machine Learning Research (11/2024)\nA.2\nDetails for ComposLoRA\nSee Table 9 for the detailed descriptions of each LoRA in ComposLoRA Testbed.\nTable 9: Detailed descriptions of each LoRA in the ComposLoRA.\nLoRA\nCategory\nTrigger Words\nSource\nAnime Style Subset\nKamado Nezuko\nCharacter\nkamado nezuko, black hair, pink eyes, forehead\nLink\nTexas the Omertosa in Arknights\nCharacter\nomertosa, 1girl, wolf ears, long hair\nLink\nSon Goku\nCharacter\nson goku, spiked hair, muscular male, wristband\nLink\nGarreg Mach Monastery Uniform\nClothing\ngmuniform, blue thighhighs, long sleeves\nLink\nZero Suit (Metroid)\nClothing\nzero suit, blue gloves, high heels\nLink\nHand-drawn Style\nStyle\nlineart, hand-drawn style\nLink\nChinese Ink Wash Style\nStyle\nshuimobysim, traditional chinese ink painting\nLink\nBamboolight Background\nBackground\nbamboolight, outdoors, bamboo\nLink\nAuroral Background\nBackground\nauroral, starry sky, outdoors\nLink\nHuge Two-Handed Burger\nObject\ntwo-handed burger, holding a huge burger with both hands\nLink\nToast\nObject\ntoast, toast in mouth\nLink\nRealistic Style Subset\nIU (Lee Ji Eun, Korean singer)\nCharacter\niu1, long straight black hair, hazel eyes, diamond stud earrings\nLink\nScarlett Johansson\nCharacter\nscarlett, short red hair, blue eyes\nLink\nThe Rock (Dwayne Johnson)\nCharacter\nth3r0ck with no hair, muscular male, serious look on his face\nLink\nThai University Uniform\nClothing\nmahalaiuniform, white shirt short sleeves, black pencil skirt\nLink\nSchool Dress\nClothing\nschool uniform, white shirt, red tie, blue pleated microskirt\nLink\nJapanese Film Color Style\nStyle\nfilm overlay, film grain\nLink\nBright Style\nStyle\nbright lighting\nLink\nLibrary Bookshelf Background\nBackground\nlib_bg, library bookshelf\nLink\nForest Background\nBackground\nslg, river, forest\nLink\nUmbrella\nObject\ntransparent umbrella\nLink\nBubble Gum\nObject\nblow bubble gum\nLink\n22",
    "pdf_filename": "Multi-LoRA_Composition_for_Image_Generation.pdf"
}