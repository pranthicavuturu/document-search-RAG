{
    "title": "Published in Transactions on Machine Learning Research (11/2024)",
    "abstract": "Low-RankAdaptation(LoRA)isextensivelyutilizedintext-to-imagemodelsfortheaccurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of compleximagery. Inthispaper,westudymulti-LoRAcompositionthroughadecoding-centric perspective. Wepresenttwotraining-freemethods: LoRASwitch,whichalternatesbetween different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights, and all evaluation details are available on our project website. 1 Introduction In the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Saharia etal.,2022;Rameshetal.,2022;Ruizetal.,2023;Sohnetal.,2023), theintegrationofLow-RankAdaptation (LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision and minimal computational load. LoRA excels by specializing in one element — such as a specific character, a particularclothing,auniquestyle,orotherdistinctvisualaspects—andbeingtrainedtoproducediverseand accuraterenditionsofthiselementingeneratedimages. Forinstance,userscouldcustomizetheirLoRAmodels to generate various images of themselves, achieving an array of personalized and realistic representations. 1 4202 voN 91 ]VC.sc[ 2v34861.2042:viXra",
    "body": "Published in Transactions on Machine Learning Research (11/2024)\nMulti-LoRA Composition for Image Generation\nMing Zhong1 mingz5@illinois.edu\nYelong Shen2 yelong.shen@microsoft.com\nShuohang Wang2 Shuohang.Wang@microsoft.com\nYadong Lu2 yadonglu@microsoft.com\nYizhu Jiao1 yizhuj2@illinois.edu\nSiru Ouyang1 siruo2@illinois.edu\nDonghan Yu2 donghanyu@microsoft.com\nJiawei Han1 hanj@illinois.edu\nWeizhu Chen2 wzchen@microsoft.com\n1University of Illinois Urbana-Champaign, 2Microsoft Corporation\nReviewed on OpenReview: https://openreview.net/forum?id=25FT0DqhVZ\nAbstract\nLow-RankAdaptation(LoRA)isextensivelyutilizedintext-to-imagemodelsfortheaccurate\nrendition of specific elements like distinct characters or unique styles in generated images.\nNonetheless, existing methods face challenges in effectively composing multiple LoRAs,\nespecially as the number of LoRAs to be integrated grows, thus hindering the creation of\ncompleximagery. Inthispaper,westudymulti-LoRAcompositionthroughadecoding-centric\nperspective. Wepresenttwotraining-freemethods: LoRASwitch,whichalternatesbetween\ndifferent LoRAs at each denoising step, and LoRA Composite, which simultaneously\nincorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed\napproaches, we establish ComposLoRA, a new comprehensive testbed as part of this research.\nIt features a diverse range of LoRA categories with 480 composition sets. Utilizing an\nevaluation framework based on GPT-4V, our findings demonstrate a clear improvement\nin performance with our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights,\nand all evaluation details are available on our project website.\n1 Introduction\nIn the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Saharia\netal.,2022;Rameshetal.,2022;Ruizetal.,2023;Sohnetal.,2023), theintegrationofLow-RankAdaptation\n(LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision and\nminimal computational load. LoRA excels by specializing in one element — such as a specific character, a\nparticularclothing,auniquestyle,orotherdistinctvisualaspects—andbeingtrainedtoproducediverseand\naccuraterenditionsofthiselementingeneratedimages. Forinstance,userscouldcustomizetheirLoRAmodels\nto generate various images of themselves, achieving an array of personalized and realistic representations.\n1\n4202\nvoN\n91\n]VC.sc[\n2v34861.2042:viXra\nPublished in Transactions on Machine Learning Research (11/2024)\nRealistic Style Anime Style\nObject\nCharacter3\nCharacter 1 Hamburger\nClothing\nCharacter4\nBackground\nCharacter 2 Bamboo LoRAMerge Ours\nLoRAMerge Ours\nFigure 1: Multi-LoRA composition techniques effectively blend different elements such as characters, clothing,\nand objects into a cohesive image. Unlike the conventional LoRA Merge approach (Ryu, 2023), which can\nlead to detail loss and image distortion as more LoRAs are added, our methods retain the accuracy of each\nelement and the overall image quality.\nThe application of LoRA not only showcases its adaptability and precision in image generation but also\nopens new avenues in customized digital content creation, revolutionizing how users interact with and utilize\ngenerative text-to-image models for creating tailored visual content.\nHowever, an image typically embodies a mosaic of various elements, making compositionality key to\ncontrollable image generation (Tenenbaum, 2018; Huang et al., 2023b). In pursuit of this, the strategy of\ncomposing multiple LoRAs, each focused on a distinct element, emerges as a feasible approach for advanced\ncustomization. This technique enables the digitization of complex scenes, such as virtual try-ons, merging\nusers with clothing in a realistic fashion, or urban landscapes where users interact with meticulously designed\ncity elements. Prior investigations into multi-LoRA compositions have explored the context of pre-trained\nlanguage models (Zhang et al., 2023a; Huang et al., 2023a) or stable diffusion models (Ryu, 2023; Shah et al.,\n2023). These studies aim to merge multiple LoRA models to synthesize a new LoRA model by training\ncoefficient matrices (Huang et al., 2023a; Shah et al., 2023; Wu et al., 2024) or through the direct addition\nor subtraction of LoRA weights (Ryu, 2023; Zhang et al., 2023a). Nevertheless, these approaches centered\non weight manipulation could destabilize the merging process as the number of LoRAs grows (Huang et al.,\n2023a) and also overlook the interaction between LoRA models and base models. This oversight becomes\nparticularly critical in diffusion models, which depend on sequential denoising steps for image generation.\nIgnoring the interplay between LoRAs and these steps can result in misalignments in the generative process,\nasshownin Figure1, whereamerged LoRAmodel failstopreservethefull complexity ofalldesiredelements,\nleading to distorted or unrealistic images.\nIn this paper, we delve into multi-LoRA composition from a decoding-centric perspective, keeping all LoRA\nweightsintact. Wepresenttwotraining-freeapproachesthatutilizeeitheroneorallLoRAsateachdecoding\nstep to facilitate compositional image synthesis. Our first approach, LoRA Switch, operates by selectively\nactivating a single LoRA during each denoising step, with a rotation among multiple LoRAs throughout the\ngeneration process. For instance, in a virtual try-on scenario, LoRA Switch alternates between a character\nLoRA and a clothing LoRA at successive denoising steps, thereby ensuring that each element is rendered\nwith precision and clarity. In parallel, we propose LoRA Composite, a technique that draws inspiration\nfrom classifier-free guidance (Ho & Salimans, 2022). It involves calculating unconditional and conditional\nscore estimates derived from each respective LoRA at every denoising step. These scores are then averaged\nto provide balanced guidance for image generation, ensuring a comprehensive incorporation of all elements.\nFurthermore, by bypassing the manipulation on the weight matrix but directly influencing the diffusion\nprocess, both methods allow for the integration of any number of LoRAs and overcome the limitations of\nrecent studies that typically merge only two LoRAs (Shah et al., 2023).\n2\nPublished in Transactions on Machine Learning Research (11/2024)\nExperimentally, we introduce ComposLoRA, the first testbed specifically designed for LoRA-based composable\nimage generation. This testbed features an extensive array of six LoRA categories, spanning two distinct\nvisual styles: reality and anime. Our evaluation includes 480 diverse composition sets, each incorporating a\nvarying number of LoRAs to comprehensively evaluate the efficacy of each proposed method. Given the lack\nof standardized automatic metrics for this novel task, we propose to employ GPT-4V (OpenAI, 2023a;b)\nas an evaluator, assessing both the quality of the images and the effectiveness of the compositions. Our\nempirical findings consistently demonstrate that both LoRA Switch and LoRA Composite substantially\noutperform the prevalent LoRA merging approach, particularly noticeable as the number of LoRAs in a\ncomposition increases. To further validate our results, we also conduct human evaluations, which reinforce\nour conclusions and affirm the efficacy of our automated evaluation framework. In addition, we provide a\ndetailed analysis of the applicable scenarios for each method, as well as discuss the potential bias of using\nGPT-4V as an evaluator.\nTo summarize, our key contributions are threefold:\n• We introduce the first investigation of multi-LoRA composition from a decoding-centric perspective,\nproposing LoRA Switch and LoRA Composite. Our methods overcome existing constraints on the\nnumber of LoRAs that can be integrated, offering enhanced flexibility and improved quality in composable\nimage generation.\n• Our work establishes ComposLoRA, a comprehensive testbed tailored to this research area, featuring six\nvaried categories of LoRAs and 480 composition sets. Addressing the absence of standardized metrics, we\npresent an evaluator built upon GPT-4V, setting a new benchmark for assessing both image quality and\ncompositional efficacy.\n• Through extensive automatic and human evaluations, our findings reveal the superior performance of\nthe proposed methods compared to the prevalent LoRA merging approach. Additionally, we provide an\nin-depth analysis of different multi-composition methods and evaluation frameworks.\n2 Related Work\n2.1 Composable Text-to-Image Generation\nComposable image generation, a key aspect of digital content customization, involves creating images that\nadhere to a set of pre-defined specifications (Liu et al., 2023). Existing research in this domain primarily\nfocuses on the following approaches: enhancing compositionality with scene graphs or layouts (Johnson et al.,\n2018; Yang et al., 2022; Gafni et al., 2022), modifying the generative process of diffusion models to align with\nthe underlying specifications (Feng et al., 2023; Huang et al., 2023c;b), multi-concept customization (Kumari\net al., 2023; Han et al., 2023; Gu et al., 2023; Kwon et al., 2024; Kong et al., 2024), or composing a series of\nindependent models that enforce desired constraints (Du et al., 2020; Liu et al., 2021; Nie et al., 2021; Liu\net al., 2022; Li et al., 2023; Du et al., 2023).\nHowever, these methods typically operate at the concept level, where generative models excel in creating\nimagesbasedonbroadercategoriesorgeneralconcepts. Forexample, amodelmightbepromptedtogenerate\nan image of “a woman wearing a dress”, and can adeptly accommodate variations in the textual description,\nsuch as changing the color of the dress. Yet, they struggle to accurately render specific, user-defined elements,\nlikelesser-knowncharactersoruniquedressstyles. Anotherlineofworkthatcancomposeuser-definedobjects\ninto images (Huang et al., 2023c; Ruiz et al., 2023). However, these methods require extensive fine-tuning\nand do not perform well on multiple objects. Therefore, we introduce learning-free instance-level composition\napproaches utilizing LoRA, enabling the precise assembly of user-specified elements in image generation.\n2.2 LoRA-based Manipulations\nLeveraging large language models (LLMs) or diffusion models as the base model, recent research aims to\nmanipulate LoRA weights to achieve a range of objectives: element composition in image generation (Ryu,\n2023; Shah et al., 2023), enhancing or diminishing certain capabilities in LLMs (Zhang et al., 2023a; Huang\n3\nPublished in Transactions on Machine Learning Research (11/2024)\nX ! ... X \" ... X # ... X $ ... X % ... X &\n(1) LoRAMerge\nX ! ... X \" ... X # ... X $ ... X % ... X &\n(2) LoRASwitch\nX ! ... X \" ... X # ... X $ ... X % ... X &\n(3) LoRAComposite\nFigure 2: Overview of three multi-LoRA composition techniques, where each colored LoRA represents a\ndistinct element. The prevalent approach, LoRA Merge, linearly merges multiple LoRAs into a single\none. In contrast, our methods concentrate on the denoising process: LoRA Switch cycles through different\nLoRAs during the denoising, while LoRA Composite involves all LoRAs working together as the guidance\nthroughout the generation process.\net al., 2023a), incorporating world knowledge (Dou et al., 2023), and transferring parametric knowledge from\nlargerteachermodelstosmallerstudentmodels(Zhongetal.,2023). RegardingLoRAcompositiontechniques,\nboth LoRAHub (Huang et al., 2023a) and ZipLoRA (Shah et al., 2023) employ few-shot demonstrations to\nlearn coefficient matrices for merging LoRAs, enabling the fusion of multiple LoRAs into a singular new\nLoRA. On the other hand, LoRA Merge (Ryu, 2023; Zhang et al., 2023a) introduces addition and negation\noperators to merge LoRA weights through arithmetic operations.\nNevertheless, these weight-based methods often lead to instability in the merging process as the number of\nLoRAs increases (Huang et al., 2023a). They also fail to account for the interactive dynamics when applying\nthe LoRA model in conjunction with the base model. To address these issues, our study explores a new\nperspective: instead of altering the weights of LoRAs, we maintain all LoRA weights intact and focus on the\ninteractions between LoRAs and the underlying generative process.\n3 Method\nIn this section, we begin with an overview of essential concepts for understanding multi-LoRA composition,\nfollowed by detailed descriptions of our proposed methods.\n3.1 Preliminary\nDiffusion Models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Dhariwal & Nichol,\n2021; Song et al., 2021; Nichol et al., 2022) represent a class of generative models adept at crafting data\nsamplesfrom Gaussian noisethrougha sequentialdenoisingprocess. Theybuild upona sequence ofdenoising\nautoencoders that estimate the score of a data distribution (Hyvärinen, 2005). Given an image x, the encoder\nE is used to map x into a latent space, thus yielding an encoded latent z = E(x). The diffusion process\nintroduces noise to z, resulting in latent representation z t with different noise levels over timestep t ∈ T.\n4\nPublished in Transactions on Machine Learning Research (11/2024)\nThe diffusion model ϵ θ with learnable parameters θ is trained to predict the noise added to the noisy latent\nz t given text instruction conditioning c T. Typically, a mean-squared error loss function is utilized as the\ndenoising objective:\nL=E E(x),ϵ∼N(0,1),t(cid:2) ||ϵ−ϵ θ(z t,t,c T)||2 2(cid:3) , (1)\nwhere ϵ is the additive Gaussian noise. In this paper, we investigate multi-LoRA composition based on\ndiffusion models, which is consistent with the settings of previous studies on LoRA merging (Ryu, 2023; Shah\net al., 2023).\nClassifier-Free Guidance. Indiffusion-basedgenerativemodeling, classifier-freeguidance(Ho&Salimans,\n2022)balancesthetrade-offbetweenthediversityandqualityofthegeneratedimages,particularlyinscenarios\nwhere the model is conditioned on classes or textual descriptions. For the text-to-image task, it operates\nby directing the probability mass towards outcomes where the implicit classifier p θ(c|zt) predicts a high\nlikelihood for the textual conditioning c. This necessitates the diffusion models to undergo a joint training\nparadigmforbothconditionalandunconditionaldenoising. Subsequently,duringinference,theguidancescale\ns≥1 is used to adjust the score function e˜θ(zt,c) by moving it closer to the conditional estimation e θ(zt,c)\nand further from the unconditional estimation e θ(zt), enhancing the conditioning effect on the generated\nimages, as formalized in the following expression:\ne˜θ(zt,c)=e θ(zt)+s·(e θ(zt,c)−e θ(zt)). (2)\nLoRA Merge. Low-Rank Adaptation (LoRA) approach (Hu et al., 2022) enhances parameter efficiency by\nfreezing the pre-trained weight matrices and integrating additional trainable low-rank matrices within the\nneural network. This method is founded on the observation that pre-trained models exhibit low “intrinsic\ndimension” (Aghajanyan et al., 2021). Concretely, for a weight matrix W ∈ Rn×m in the diffusion model\nϵ θ, the introduction of a LoRA module involves updating W to W′, defined as W′ = W +BA. Here,\nB ∈ Rn×r and A ∈ Rr×m are matrices of a low-rank factor r, satisfying r ≪ min(n,m). The concept of\nLoRA Merge (Ryu, 2023) is realized by linearly combining multiple LoRAs to synthesize a unified LoRA,\nsubsequently plugged into the diffusion model. Formally, when introducing k distinct LoRAs, the consequent\nupdated matrix W′ in ϵ θ is given by:\nk\nX\nW′ =W + w i×B iA i, (3)\ni=1\nwhere i denotes the index of the i-th LoRA, and w i is a scalar weight, typically a hyperparameter determined\nthrough empirical tuning. LoRA Merge has emerged as a dominant approach for presenting multiple elements\ncohesively in an image, offering a straightforward baseline for various applications. However, merging too\nmany LoRAs at once can destabilize the merging process (Huang et al., 2023a), and it completely overlooks\nthe interaction with the diffusion model during the generative process, resulting in the deformation of the\nhamburger and fingers in Figure 2.\n3.2 Multi-LoRA Composition through a Decoding-Centric Perspective\nTo address the above issues, we base our approach on the denoising process and investigate how to perform\ncompositionwhilemaintainingtheLoRAweightsunchanged. Thisisspecificallydividedintotwoperspectives:\nin each denoising step, either activate only one LoRA or engage all LoRAs to guide the generation.\nLoRA Switch (LoRA-s). To explore activating a single LoRA in each denoising step, we present LoRA\nSwitch. This method introduces a dynamic adaptation mechanism within diffusion models by sequentially\nactivating individual LoRAs at designated intervals throughout the generation process. As illustrated in\nFigure 2, each LoRA is represented by a unique color corresponding to a specific element, with only one\nLoRA engaged per denoising step.\n5\nPublished in Transactions on Machine Learning Research (11/2024)\nWith a set of k LoRAs, the methodology initiates with a prearranged sequence of permutations; in the\nexample of the Figure, the sequence progresses from yellow to green to blue LoRAs. Starting from the first\nLoRA, the model transitions to the subsequent LoRA every τ step. This rotation persists, allowing each\nLoRA to be applied in turn after kτ steps, thereby endowing each element to contribute repeatedly to the\nimage generation. The active LoRA at each denoising timestep t, ranging from 1 to the total number of steps\nrequired, is determined by the following equations:\ni=⌊((t−1)mod(kτ))/τ⌋+1,\n(4)\nW t′ =W +w i×B iA i.\nIn this formula, i indicates the index of the currently active LoRA, iterating from 1 to k. The floor function\n⌊·⌋ guarantees the integer value of i is appropriately computed for t. The resulting weight matrix W′ is\nt\nupdated to reflect the contribution from theactive LoRA.By selectivelyenabling one LoRAat a time, LoRA\nSwitchensuresfocusedattentiontothedetailspertinenttothecurrentelement,thuspreservingtheintegrity\nand quality of the generated image throughout the process.\nLoRA Composite (LoRA-c). To explore incorporating all LoRAs at each timestep without merging\nweight matrices, we propose LoRA Composite (LoRA-c), an approach grounded in the Classifier-Free\nGuidance paradigm. Previous research has primarily focused on modifying CFG to enable diffusion models\nto emphasize textual concepts (Liu et al., 2022; Du et al., 2023; Sohn et al., 2023). In contrast, our method\nextendsthisbyenablingCFGtoconditiononLoRAs, facilitatingthegenerationofimagesthatreflectspecific\nelements or instances rather than abstract concepts. LoRA-c involves calculating both unconditional and\nconditional score estimates for each LoRA individually at every denoising step. By aggregating these scores,\nthe technique ensures balanced guidance throughout the image generation process, facilitating the cohesive\nintegration of all elements represented by different LoRAs.\nFormally, with k LoRAs in place, let θ i′ denote the parameters of the diffusion model e θ after incorporating\nthe i-th LoRA. The collective guidance e˜(zt,c) based on textual condition c is derived by aggregating the\nscores from each LoRA, as depicted in the equation below:\nk\n1 X h i\ne˜(zt,c)= w i× e θ′(zt)+s·(e θ′(zt,c)−e θ′(zt)) . (5)\nk i i i\ni=1\nHere, w i is a scalar weight allocated to each LoRA, intended to adjust the influence of the i-th LoRA. In this\npaper, we set w i to 1, giving each LoRA equal importance. LoRA-c assures that every LoRA contributes\neffectively at each stage of the denoising process, addressing the potential issues of robustness and detail\npreservation that are commonly associated with merging LoRAs.\nOverall, we are the first to adopt a decoding-centric perspective in multi-LoRA composition, steering clear of\nthe instability inherent in weight manipulation on LoRAs. Our study introduces two training-free methods\nfor activating either one or all LoRAs at each denoising step, with their comparative analysis presented in\nSections §4.2 and §4.3.1.\n4 Experiments\n4.1 Experimental Setup\nComposLoRA Testbed. Due to the absence of standardized benchmarks and automated evaluation met-\nrics, existing studies involving evaluation for composable image generation lean heavily on quantitative\nanalysis (Huang et al., 2023b; Wang et al., 2023) and human effort (Shah et al., 2023), which also limits\nthe advancements of multi-LoRA composition. To bridge this gap, we introduce a comprehensive testbed\nComposLoRA designed to facilitate comparative analysis of various composition approaches. This testbed\nbuilds upon a collection of public LoRAs1, which are extensively shared and recognized as essential plug-in\nmodules in this field. The selection of LoRAs for this testbed adheres to the following criteria:\n1Collectedfromhttps://civitai.com/.\n6\nPublished in Transactions on Machine Learning Research (11/2024)\n• Each LoRA should be robustly trained, ensuring it can accurately replicate the specific elements it\nrepresents when integrated independently;\n• The elements represented by the LoRAs should cover a diverse range of categories and demonstrate\nadaptability across different image styles;\n• When composed, LoRAs from different categories should be compatible, preventing any conflicts in the\nresulting image composition.\nConsequently, we curate two unique subsets of\nLoRAs representing realistic and anime styles.\nEach subset comprises a variety of elements: 3 Table 1: Comparative evaluation with GPT-4V. The evalu-\ncharacters, 2 types of clothing, 2 styles, 2 back- ation prompt and result are in a simplified version.\ngrounds,and2objects,culminatinginatotalof\n22 LoRAs in ComposLoRA. In constructing com-\nposition sets, we strictly follow a crucial princi-\nple: each set must include one character LoRA\nand avoid duplication of element categories to\nprevent conflicts. Thus, the ComposLoRA evalu-\nationincorporatesatotalof480distinctcompo-\nEvaluation Prompt\nsition sets. This includes 48 sets comprising 2 Ineedassistanceincomparativelyevaluatingtwotext-to-image\nLoRAs, 144 sets with 3 LoRAs, 192 sets featur- models based on their ability to compose different elements\nintoasingleimage. Thekeyelementsare:\ning 4 LoRAs, and 96 sets containing 5 LoRAs.\nKey features for each LoRA are manually anno- 1. Character: kamadonezuko,blackhair,pinkeyes,forehead\ntatedandservedualpurposes: theyactasinput 2. Background: bamboolight,outdoors,bamboo\n3. Object: two-handedburger,holdingahugeburger\nprompts for the text-to-image models to gen-\nerate images, and also provide reference points Pleasehelpmeratebasedoncompositionandimagequality:\nfor subsequent evaluations using GPT-4V. De-\n[Evaluation criteria and format requirements]\ntailed descriptions of each LoRA can be found\nin Table 9 in the Appendix. Evaluation Results from GPT-4V\nForImage1:\nComparative Evaluation with GPT-4V. CompositionQuality:\n-Thecharacter’shaircolorisnotblackasspecified(-1point).\nWhile existing metrics can calculate the align-\n-Theburgershowsadeformedconnectionwhichdetractsfrom\nment between text and images (Hessel et al., thequalityoftheobject’sdepiction(-3 points).\n2021; Ku et al., 2023), they fall short in assess- -Thecharacter’sfingersaredeformed(-1 point).\ning the intricacies of specific elements within\nImageQuality:\nan image and the quality of their composition. - Deformities are present in the character’s fingers and the\nRecently, multimodal large language models burger,whichsignificantlyimpactsthequality. (-3 points).\nlike GPT-4V (OpenAI, 2023a;b) have signifi- ...\ncantprogressandpromiseinvariousmultimodal Scores:\ntasks,underscoringtheirpotentialinevaluating Image1: CompositionQuality: 5/10,ImageQuality: 7/10\nimage generation tasks (Lin et al., 2023; Zhang Image2: CompositionQuality: 10/10,ImageQuality: 10/10\net al., 2023b). In our study, we leverage GPT-\n4V’s capabilities to serve as an evaluator for\ncomposable image generation.\nSpecifically, we employ a comparative evaluation method, utilizing GPT-4V to rate generated images across\ntwo dimensions: composition quality and image quality. We utilize a 0 to 10 scoring scale, with higher scores\nindicating superior quality. GPT-4V is provided with a prompt that includes the essential features of the\nelements to be composed, the criteria for scoring in the two dimensions, and the format for the expected\noutput. The complete evaluation prompts and results are available in Tables 7 and 8 in Appendix. This\nexperimental setup allows us to compare the efficacy of each of the two proposed methods against the LoRA\nMerge approach. Additionally, we examine how GPT-4V-based scoring aligns with human judgment in\nSection §4.2 and explore the potential biases of using it as an evaluator in Section §4.3.3.\n7\nPublished in Transactions on Machine Learning Research (11/2024)\nImplementation Details. For our experiments, we employ stable-diffusion-v1.5 (Rombach et al., 2022)\nas the backbone model. We utilize two specific checkpoints for our experiments: “Realistic_Vision_V5.1”2\nfor realistic images and “Counterfeit-V2.5”3 for anime images, each fine-tuned to their respective styles.\nIn the realistic style subset, we configure the model with 100 denoising steps, a guidance scale s of 7, and\nset the image size to 1024x768, optimizing for superior image quality. For the anime style subset, the\nsettings differ slightly with 200 denoising steps, a guidance scale s of 10, and an image size of 512x512. The\nDPM-Solver++ (Lu et al., 2022a;b) is used as the scheduler in the generation process. The weight scale w is\nconsistently set at 0.8 for composing LoRAs within ComposLoRA. For the LoRA Switch approach, we apply a\ncycle with τ set to 5, meaning every 5 denoising steps activate the next LoRA in the sequence: character,\nclothing, style, background, then object. Since the proposed methods do not require additional training, all\nexperiments are conducted on a single A6000 GPU. To ensure the reliability of our experimental results, we\nconduct image generation using three random seeds. All reported results in this paper represent the average\nevaluation scores across these three runs.\n4.2 Results on ComposLoRA\nGPT-4V-based Evaluation. We first present the comparative evaluation results using GPT-4V. This\nevaluation involves scoring the performance of LoRA-s versus LoRA Merge, and LoRA-c versus LoRA\nMerge across two dimensions, as well as determining the winner based on these scores. Specific scores and\nwin rates are illustrated in Figure 3, leading to several key observations:\nComposition Quality Image Quality Composition Quality Image Quality\n9.5 9 8.698.65 99 .. 78 9.5 9 8.858.74 99 .. 78 9.72\n78 .. 55\n78\n8.117.96\n7.2 6.68 6.56\n9999 .... 3456 9.53\n9.46 9.46 9.4 9.36 9.27 9.28\n78 .. 55\n78\n8.12\n7.87 7.1 6.68\n9999 .... 3456 9.53 9.56\n9.41 9.43 9.34\n6.5 9.2 6.5 6.09 9.2 9.15\n6 9.1 6 9.1\n5.5 5.24 9 8.95 5.5 5.17 9 8.92\n5 8.9 5 8.9\n4.5 8.8 4.5 8.8\n2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs\nLoRA Switch LoRA Merge LoRA Switch LoRA Merge LoRA Composite LoRA Merge LoRA Composite LoRA Merge\n(a) LoRASwitchvs. LoRAMerge(Scoring) (b) LoRACompositevs. LoRAMerge(Scoring)\nComposition Quality Image Quality Composition Quality Image Quality\n100% 100% 100% 100%\n80% 43 48 58 80% 28 35 38 46 80% 33 44 49 55 80% 35 40 45 56\n69\n60% 60% 60% 60%\n24 00 %% 34 31 24 14 24 00 %% 62 49 48 40 24 00 %% 51 32 23 22 24 00 %% 54 46 42 34\n23 21 18 17 10 16 14 14 16 24 28 23 11 14 13 10\n0% 0% 0% 0%\n2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs 2 LoRAs 3 LoRAs 4 LoRAs 5 LoRAs\nLose Tie Win Lose Tie Win Lose Tie Win Lose Tie Win\n(c) LoRASwitchvs. LoRAMerge(WinRate) (d) LoRACompositevs. LoRAMerge(WinRate)\nFigure 3: Results of comparative evaluation on ComposLoRA using GPT-4V.\n• OurproposedmethodconsistentlyoutperformsLoRAMergeacrossallconfigurationsandinbothdimensions,\nwith the margin of superiority increasing as the number of LoRAs grows. For instance, as shown in\nFigure 3(a), the score advantage of LoRA Switch escalates from 0.04 with 2 LoRAs to 1.32 with 5\nLoRAs. This trend aligns with the win rate observed in Figure 3(c), where the win rate approaches 70%\nwhen composing 5 LoRAs.\n• LoRA-S shows superior performance in composition quality, whereas LoRA-C excels in image quality.\nIn scenarios involving 5 LoRAs and using LoRA Merge as a baseline, the win rate of LoRA-s in\n2https://huggingface.co/SG161222/Realistic_Vision_V5.1_noVAE.\n3https://huggingface.co/gsdf/Counterfeit-V2.5.\n8\nPublished in Transactions on Machine Learning Research (11/2024)\ncomposition quality surpasses that of LoRA-c by 14% (69% vs. 55%). Conversely, for image quality,\nLoRA-c’s win rate is 10% higher than that of LoRA-s (56% vs. 46%).\n• The task of compositional image generation remains highly challenging, especially as the number of elements\nto be composed increases. According to GPT-4V’s scoring, the average score for composing 2 LoRAs is\nabove 8.5, but it sharply declines to around 6 for compositions involving 5 LoRAs. Hence, despite the\nconsiderable improvements our methods offer, there is still substantial room for further research in the\nfield of compositional image generation.\nHumanEvaluation. Tocomplementourresults,\nwe conduct a human evaluation to assess the ef-\nfectiveness of different methods and validate the\nTable 2: Human evaluation results and Pearson correla-\nefficacy of the evaluators.\ntion between different metrics and human judgment.\nTwo graduate students rate 120 images on com-\nHuman Evaluation\npositional and image quality using a 1-5 Likert\nComposition Image Quality\nscale: 1 signifies complete failure, 2-4 represents\nsignificant, moderate, and minor issues, respec- LoRA Merge 3.14 2.94\nLoRA Switch 3.91 4.15\ntively,while5denotesperfectexecution. Toensure\nLoRA Composite 3.78 4.35\nconsistency, the annotators initially pilot-score 20\nCorrelations with Human Judgments\nimages to standardize their understanding of the\ncriteria. The results, summarized in the upper Composition Image Quality\nsection of Table 2, align with GPT-4V’s findings, CLIPScore -0.006 0.083\nconfirmingourmethodsoutperformLoRAMerge Ours 0.454 0.457\n— with LoRA Switch excelling in composition\nand LoRA Composite in image quality.\nFurthermore,weanalyzethePearsoncorrelationsbetweenhumanevaluationsandscoresderivedfromGPT-4V\nand CLIPScore (Hessel et al., 2021), with results presented in the lower section of Table 2. This comparison\nreveals that CLIPScore’s evaluations fall short in assessing specific compositional and quality aspects due\nto its inability to discern the nuanced features of each element. In contrast, the evaluator we adopt shows\nsubstantially higher correlations with human judgments, affirming the validity of our evaluation framework.\n4.3 Analysis\nTo enhance our understanding of the proposed methods, we further investigate the following questions:\n4.3.1 Do Specific Image Styles Favor Different Methods?\nTo explore the impact of image style, we sepa-\nrately evaluate the performance of methods\nRealistic Style AnimeStyle\n100% 100% on realistic and anime-style subsets within\n80% 58 45 42 44 80% 53 50 33 45 C inom Fp igo us rL eoR 4A ,. reT vh eae lw dii sn tir na ct te tere ns du el nts c, iep sr fe os ren eate cd h\n60% 60%\nmethod.\n40% 26 30 46 47 40% 24 24 50 39 Our observations reveal that, while LoRA-s\n20% 20%\n0%\n16 25 12 9\n0%\n23 26 17 16 m Loay RAno -t c,e ix tce dl emin onim sta rg ae teq su ca ol mity pac ro am blp ea pre ed rfot ro\n-\nLoRA-S LoRA-C LoRA-S LoRA-C LoRA-S LoRA-C LoRA-S LoRA-C\nCompos. Compos. Image Image Compos. Compos. Image Image mance in this dimension within the realistic\nLose Tie Win Lose Tie Win\nstyle subset, while maintaining a significant\nedge in composition quality. In contrast, in\nFigure 4: Analysis on image styles. In general, LoRA-s the anime-style subset, LoRA-c, shows a per-\nis more adept at realistic styles, while LoRA-c has better formance on par with LoRA-s in composition\nperformance in anime styles.\nquality, while notably surpassing it in image\nquality. These findings suggest that LoRA-S is\n9\nPublished in Transactions on Machine Learning Research (11/2024)\nmore adept at composing elements in realistic-style images, whereas LoRA-C shows a stronger performance in\nanime-style imagery.\n4.3.2 How Does the Step Size and Order of LoRA Activation Affect LoRA Switch?\nTo identify the optimal configuration for LoRA Switch, we examine the influence of two crucial hyperpa-\nrameters: the sequence in which LoRAs are activated and the interval between each activation. Our findings,\ndepicted in Figure 5(a), show that overly frequent switching, such as changing LoRAs at every denoising step,\nleads to distortions in generated images and suboptimal performance. The efficiency of the LoRA Switch\nimproves progressively with increased step size, reaching peak performance at τ = 5.\nMoreover, our analysis underscores that\nthe initial choice of LoRA in the activa-\ntion sequence clearly influences overall 9.5\n60\nperformance, while alterations in the sub- 9\nsequent order have minimal impact. Acti- 50 8.5\nvating the character LoRA first leads to\n40 8\nthe best performance, as demonstrated\n7.5\nin Figure 5(b). In contrast, starting with 30 Composition\nclothing, background, or object LoRAs ImageQuality 7 CompositionQuality ImageQuality\nyieldsresultscomparabletoacompletely 1 2 3 4 5 6 7 8 Character Clothing Style\nNumberofstepstoswitchLoRA(τ) Background Object Random\nrandomized sequence. Notably, begin-\n(a) StepstoSwitch (b) ActivationOrder\nning with the style LoRA leads to a no-\nticeable performance drop, even falling\nFigure 5: Analysis of the number of denoising steps to switch\nslightly below a random order. This ob-\nLoRA and the activation order for LoRA Switch. In Figure 5(b),\nservation underlines the critical role of\n“Character” indicates that the character LoRA is activated first,\nprioritizing core image elements in the\nwith the rest being activated randomly.\ninitial stage of the generation process to\nenhance both the image and composi-\ntional quality for LoRA Switch.\nWhile the step size for switching LoRAs proves to be a crucial factor in achieving optimal performance in our\nexperiments, we also explore the potential of dynamic strategies for step size adjustment throughout the\ndenoising process. Specifically, we design and evaluate three strategies for dynamically adjusting the step size\nin LoRA-Switch:\n• Incremental Strategy: The step size gradually increases from τ = 3 to τ = 5 throughout the\ndenoising process.\n• Decremental Strategy: The step size gradually decreases from τ =5 to τ =3 as the denoising\nprocess progresses.\n• Warm-up Strategy: During the initial 50% of the denoising process, the step size increases from\nτ =3 to τ =5 and remains constant at τ =5 for the remaining denoising steps.\nTable 3: Performance comparison of dynamic strategies for LoRA-S.\nτ =3 τ =4 τ =5 τ =6 Incremental(3→5) Decremental(5→3) Warm-up\nCompositionQuality 55 57 59 58 57 54 58\nImageQuality 29 33 34 33 33 31 34\nTable 3 shows the results of these strategies. Neither the Incremental nor the Warm-up strategies significantly\nimprove performance compared to using a fixed step size. The Decremental strategy, on the other hand,\nresults in comparatively worse performance, highlighting that switching LoRAs too frequently in the latter\n10\n)%(etaRniW\nerocSegarevA\nPublished in Transactions on Machine Learning Research (11/2024)\nstages of denoising is detrimental to image quality. The fixed step size of τ =5 yields the best performance.\nConsequently, we adopt this fixed step size in our experiments.\n4.3.3 Does GPT-4V Exhibit Bias as an Evaluator?\nWhile GPT-4V has demonstrated utility in evaluating various image generation tasks (Lin et al., 2023; Zhang\net al., 2023b), our analysis uncovers a notable positional bias in its comparative evaluations. We investigate\nthis potential bias by swapping the positions of images generated by different methods before inputting them\nto GPT-4V, and the results are illustrated in Figure 6.\nIn the comparison of LoRA-s versus\nLoRA Merge, when the image gener-\nComposition Quality Image Quality atedbyMergeispresentedfirst(“Merge\n100% 100%\nFirst”), the win rate for LoRA-s in com-\n80% 51 52 42 80% 34 41 40 48 position quality stands at 60%. How-\n60\n60% 60% ever, this win rate declines to 51%\n40% 25 25 27 29 40% 48 48 43 42 (w “h Le on RL Ao -SR FA i- rs st’s ”)i .m Sa ig me ili as rt lh y,e Lfi ors Rt Ain -p cu ’st\n20% 20%\n15 24 21 29 18 11 17 10 winratedecreasesfrom52%to42%, sug-\n0% 0% gesting that GPT-4V tends to favor the\nMerge LoRA-S Merge LoRA-C Merge LoRA-S Merge LoRA-C\nFirst First First First First First First First second image input in terms of compo-\nLose Tie Win Lose Tie Win sition quality. Intriguingly, the opposite\ntrend is observed in image quality, where\nFigure6: PositionalbiasanalysisforGPT-4V-basedevaluation. In thesecondimagetendstoreceiveahigher\neach subfigure, the left side of the orange line compares LoRA-s score. These results indicate a significant\nwith Merge, and the right side contrasts LoRA-c with Merge. positional bias in GPT-4V’s evaluation,\n“MergeFirst”indicatesthattheimageproducedbyLoRA Merge varying with the dimension and the posi-\nis the first image input during the comparative evaluation. tion of the images. To mitigate this bias\nin our study, the comparative evaluation\nresults reported in this paper are averaged across both input orders.\n4.4 More Visual Examples\nTo demonstrate the effectiveness of our methods in composing varying numbers of LoRAs and under different\nimage styles, we provide additional visual examples in Figures 7 – 10.\nCharacter 1\nObject\nBubble Gum\nCharacter 2\nLoRAMerge Ours\nFigure 7: Case study on composing 2 LoRAs in the realistic style.\n11\nPublished in Transactions on Machine Learning Research (11/2024)\nCharacter3\nClothing\nCharacter4\nLoRAMerge Ours\nFigure 8: Case study on composing 2 LoRAs in the anime style.\nBackground1\nCharacter Library Bookshelf\nClothing Background2 LoRAMerge Ours\nForest & River\nFigure 9: Case study on composing 3 LoRAs in the realistic style.\n5 Further Discussions\n5.1 Limitations\nBased on our experiments, the primary limitation of the proposed methods is the efficiency issue with\nLoRA-c. This method can introduce (k−1)× additional computational cost, where k is the number of\n12\nPublished in Transactions on Machine Learning Research (11/2024)\nBackground3 Object 1\nAuroral Hamburger\nObject 2\nCharacter Toast\nLoRAMerge Ours\nFigure 10: Case study on composing 3 LoRAs in the anime style.\nmerged LoRAs. This is due to LoRA-c merging each LoRA with the base model to calculate scores, which\nare then averaged. The inherent design of LoRA prevents the pre-computation of the base model. To address\nthis, we propose two potential solutions: 1) Integrating advanced techniques with fewer denoising steps, and\n2) a combination of LoRA-s and LoRA-c.\n5.1.1 Integration of LCM and LCM-LoRA\nRecently,severalalgorithmshavebeendevelopedthatrequireonly2-8denoisingstepstogeneratehigh-quality\nimages. We select LCM (Luo et al., 2023a) and LCM-LoRA (Luo et al., 2023b) to test if our approaches can\nintegrate smoothly with these algorithms.\nTable 4: Results for integrating LCM and LCM-LoRA.\nIntegrated Technique Method Win (%) Tie(%) Lose(%) InferenceSteps\nLoRA Switch 43 34 23 200\nNone\nLoRA Composite 33 51 16 200\nLoRA Switch 45 36 19 4\nLCM\nLoRA Composite 39 43 18 4\nLoRA Switch 42 39 19 8\nLCM-LoRA\nLoRA Composite 44 41 15 8\nFirst, we conduct experiments on 2 LoRAs using the same setup as described in the main text, with results\nshown in Table 4. Remarkably, our methods not only outperform the baseline but also show even greater\nadvantageswhenintegratedwiththeseinference-acceleratingtechniques. Thisadaptationsignificantlyreduces\nthe required number of denoising steps to 4-8, effectively addressing the increased computational demands of\nLoRA-c. Consequently, the generation times across all three methods are now comparably short, taking\nonly a few seconds.\nTo further explore the potential of integrating our method with fewer denoising steps, we conduct additional\nexperimentsusingLCM-LoRAwithanincreasednumberofLoRAs. Theseexperimentsaimtoprovidedeeper\ninsights into how our approach performs as the complexity of multi-LoRA composition increases. The results,\nshown in Table 5, reflect the composition quality scores under this setup. For these experiments, we use 8\n13\nPublished in Transactions on Machine Learning Research (11/2024)\ndenoising steps for 2 and 4 LoRAs, and 9 inference steps for 3 LoRAs, switching to the next LoRA every 3\nsteps for LoRA-s.\nTable 5: Composition quality scores for different methods when using LCM-LoRA with more LoRAs.\n2LoRAs 3LoRAs 4LoRAs\nLoRAMerge 7.52 5.16 3.49\nLoRASwitch 8.08 6.39 5.08\nLoRAComposite 8.13 6.07 4.53\nOur methods exhibit a more pronounced advantage over the baseline in this fewer-steps setting. The\nimprovement is particularly noticeable when the number of LoRAs increases, demonstrating the robustness\nof our approach even when integrated with models requiring fewer denoising steps. However, it is important\nto note that all methods experience a substantial drop in absolute scores when combined with models like\nLCM-LoRA that employ fewer denoising steps. For instance, the composition quality score for LoRA-c with\n5 LoRAs is initially 6.56 for 200 denoising steps, but with 4 LoRAs in this setting, the score drops to 5.08.\nThisfindingsuggeststhat despite theimprovedperformance ofourmethod, multi-LoRAcomposition remains\na challenging task, especially when fewer denoising steps are used, even with the latest integration techniques.\n5.1.2 Combination of LoRA-s and LoRA-c\nTo further enhance efficiency, we propose combining LoRA-s and LoRA-c. LoRA-s activates at the LoRA\nstage before each denoising step, while LoRA-c is applied to the CFG during the denoising process. These\ndesign principles are complementary. A practical integration method involves selecting a subset of LoRAs\nto activate (ranging from one to all) for each denoising step, following the LoRA-s strategy. This subset\nwould then utilize all its LoRAs during the denoising phase, adhering to the LoRA-c strategy. For LCM\nand other related applications, combining LoRA-s and LoRA-c can enhance efficiency without additional\nmodifications. For example, in a 1-step scenario, all LoRAs can be activated and applied through CFG (as\nper LoRA-c). In a 2-step scenario, half of the LoRAs can be activated at each step and then applied through\nCFG, blending LoRA-s and LoRA-c.\n5.2 Comparison with ZipLoRA\nTable 6: Comparison with ZipLoRA in two different LoRA setups.\nLoRA Setup Methods Composition Quality Image Quality\nLoRA-S 8.80 8.95\nCharacter+Style LoRA-C 8.55 9.20\nZipLoRA 9.05 9.40\nLoRA-S 8.85 9.05\nCharacter+Object LoRA-C 8.60 9.25\nZipLoRA 8.50 9.10\nAlthough our proposed methods are training-free, we also compare them with the fine-tuning approach\nZipLoRA as a baseline for reference. ZipLoRA is based on SDXL and focuses on merging two LoRAs, so we\nconduct our comparisons under this setup. Specifically, we randomly select publicly available SDXL LoRAs\nfrom HuggingFace and create 10 composition sets, combining character + style and character + object for\nthe experiments. All results are compared against the LoRA Merge, and the scores are presented in Table 6.\nSince ZipLoRA is specifically designed to merge subject and style LoRAs, it achieves higher scores in the\ncharacter+stylesetupcomparedtoourmethods,likelyduetothebenefitsofitsfine-tuningprocess. However,\n14\nPublished in Transactions on Machine Learning Research (11/2024)\nin scenarios involving two subjects, such as character + object, our methods demonstrate clear advantages\ndespite being training-free. LoRA-S outperforms in composition quality, while LoRA-C excels in image\nquality. This indicates that our methods are particularly effective in handling diverse subject combinations,\nespecially when the task involves composing two subjects rather than merging a subject with a style.\n6 Conclusion\nIn this paper, we present the first exploration of multi-LoRA composition from a decoding-centric perspective\nbyintroducingLoRA-sandLoRA-cthattranscendthelimitationsofcurrentweightmanipulationtechniques.\nThrough establishing a dedicated testbed ComposLoRA, we introduce scalable automated evaluation metrics\nutilizing GPT-4V. Our study not only highlights the superior quality achieved by our methods but also\nprovides a new standard for evaluating LoRA-based composable image generation.\nBroader Impact Statement\nOur approaches offer advancements in personalized image generation and customized digital content creation\nby allowing the combination of arbitrary elements. This capability can be applied to various real-world\nscenarios,suchasvirtualtry-onandvirtualdesign,leadingtopositivesocialimpacts. Asourmethodoperates\nin the inference phase and relies solely on the composition of publicly available checkpoints (base models and\nLoRAs) without requiring additional training, it avoids any negative impact related to model training.\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness\nof language model fine-tuning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pp. 7319–7328. Association for Computational Linguistics, 2021.\ndoi: 10.18653/V1/2021.ACL-LONG.568. URL https://doi.org/10.18653/v1/2021.acl-long.568.\nPrafullaDhariwalandAlexanderQuinnNichol.Diffusionmodelsbeatgansonimagesynthesis.InMarc’Aurelio\nRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.),\nAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 8780–8794, 2021. URL https:\n//proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html.\nShihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang,\nXiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe:\nRevolutionizing mixture of experts for maintaining world knowledge in language model alignment. CoRR,\nabs/2312.09979, 2023. doi: 10.48550/ARXIV.2312.09979. URL https://doi.org/10.48550/arXiv.2312.\n09979.\nYilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models.\nIn Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.\nneurips.cc/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html.\nYilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-\nDickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation\nwith energy-based diffusion models and MCMC. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,\nBarbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine\nLearning Research, pp. 8489–8510. PMLR, 2023. URL https://proceedings.mlr.press/v202/du23a.\nhtml.\n15\nPublished in Transactions on Machine Learning Research (11/2024)\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nPUIqjT4rzq7.\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-\nscene: Scene-based text-to-image generation with human priors. In Shai Avidan, Gabriel J. Brostow,\nMoustapha Cissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 -\n17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, volume 13675 of\nLecture Notes in Computer Science, pp. 89–106. Springer, 2022. doi: 10.1007/978-3-031-19784-0\\_6. URL\nhttps://doi.org/10.1007/978-3-031-19784-0_6.\nYuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao,\nShuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Mix-of-show: Decentralized\nlow-rank adaptation for multi-concept customization of diffusion models. In Alice Oh, Tristan Naumann,\nAmir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/\npaper/2023/hash/3340ee1e4a8bad8d32c35721712b4d0a-Abstract-Conference.html.\nLigongHan,YinxiaoLi,HanZhang,PeymanMilanfar,DimitrisN.Metaxas,andFengYang. Svdiff: Compact\nparameter space for diffusion fine-tuning. In IEEE/CVF International Conference on Computer Vision,\nICCV 2023, Paris, France, October 1-6, 2023, pp. 7289–7300. IEEE, 2023. doi: 10.1109/ICCV51070.2023.\n00673. URL https://doi.org/10.1109/ICCV51070.2023.00673.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free\nevaluation metric for image captioning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and\nScott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp.\n7514–7528. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.595.\nURL https://doi.org/10.18653/v1/2021.emnlp-main.595.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi:\n10.48550/ARXIV.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle,\nMarc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/\n2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhuChen. Lora: Low-rankadaptationoflargelanguagemodels. InThe Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL\nhttps://openreview.net/forum?id=nZeVKeeFYf9.\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient\ncross-task generalization via dynamic lora composition. CoRR, abs/2307.13269, 2023a. doi: 10.48550/\nARXIV.2307.13269. URL https://doi.org/10.48550/arXiv.2307.13269.\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and\ncontrollable image synthesis with composable conditions. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of\nMachine Learning Research, pp. 13753–13773. PMLR, 2023b. URL https://proceedings.mlr.press/\nv202/huang23b.html.\n16\nPublished in Transactions on Machine Learning Research (11/2024)\nZiqi Huang, Kelvin C. K. Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face\ngeneration and editing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 6080–6090. IEEE, 2023c. doi: 10.1109/CVPR52729.\n2023.00589. URL https://doi.org/10.1109/CVPR52729.2023.00589.\nAapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.,\n6:695–709, 2005. URL http://jmlr.org/papers/v6/hyvarinen05a.html.\nJustin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018,\npp. 1219–1228. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.\n00133. URL http://openaccess.thecvf.com/content_cvpr_2018/html/Johnson_Image_Generation_\nFrom_CVPR_2018_paper.html.\nZhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and\nWenhan Luo. OMG: occlusion-friendly personalized multi-concept generation in diffusion models. CoRR,\nabs/2403.10983, 2024. doi: 10.48550/ARXIV.2403.10983. URL https://doi.org/10.48550/arXiv.2403.\n10983.\nMax Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub:\nStandardizing the evaluation of conditional image generation models. CoRR, abs/2310.01596, 2023. doi:\n10.48550/ARXIV.2310.01596. URL https://doi.org/10.48550/arXiv.2310.01596.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept cus-\ntomization of text-to-image diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 1931–1941. IEEE, 2023. doi:\n10.1109/CVPR52729.2023.00192. URL https://doi.org/10.1109/CVPR52729.2023.00192.\nGihyunKwon,SimonJenni,DingzeyuLi,Joon-YoungLee,JongChulYe,andFabianCabaHeilbron. Concept\nweaver: Enabling multi-concept fusion in text-to-image models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 8880–8889. IEEE,\n2024. doi: 10.1109/CVPR52733.2024.00848. URL https://doi.org/10.1109/CVPR52733.2024.00848.\nShuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, and Igor Mordatch. Composing ensembles\nof pre-trained models via iterative consensus. In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https:\n//openreview.net/pdf?id=gmwDKo-4cY.\nKevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, and Lijuan Wang. Designbench: Exploring and\nbenchmarking DALL-E 3 for imagining visual design. CoRR, abs/2310.15144, 2023. doi: 10.48550/ARXIV.\n2310.15144. URL https://doi.org/10.48550/arXiv.2310.15144.\nNan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba. Learning to compose vi-\nsual relations. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and\nJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pp. 23166–23178, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\nc3008b2c6f5370b744850a98a95b73ad-Abstract.html.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual\ngeneration with composable diffusion models. In Shai Avidan, Gabriel J. Brostow, Moustapha Cissé,\nGiovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 - 17th European\nConference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII, volume 13677 of Lecture\nNotes in Computer Science, pp. 423–439. Springer, 2022. doi: 10.1007/978-3-031-19790-1\\_26. URL\nhttps://doi.org/10.1007/978-3-031-19790-1_26.\nNan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba. Unsupervised compositional\nconcepts discovery with text-to-image generative models. CoRR, abs/2306.05357, 2023. doi: 10.48550/\nARXIV.2306.05357. URL https://doi.org/10.48550/arXiv.2306.05357.\n17\nPublished in Transactions on Machine Learning Research (11/2024)\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver\nfor diffusion probabilistic model sampling in around 10 steps. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\nDanielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022,2022a. URLhttp://papers.nips.cc/paper_files/paper/2022/\nhash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for\nguided sampling of diffusion probabilistic models. CoRR, abs/2211.01095, 2022b. doi: 10.48550/ARXIV.\n2211.01095. URL https://doi.org/10.48550/arXiv.2211.01095.\nSimianLuo,YiqinTan,LongboHuang,JianLi,andHangZhao. Latentconsistencymodels: Synthesizinghigh-\nresolutionimageswithfew-stepinference. CoRR,abs/2310.04378,2023a. doi: 10.48550/ARXIV.2310.04378.\nURL https://doi.org/10.48550/arXiv.2310.04378.\nSimian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian\nLi, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. CoRR, abs/2311.05556,\n2023b. doi: 10.48550/ARXIV.2311.05556. URL https://doi.org/10.48550/arXiv.2311.05556.\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-\nguided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu,\nand Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022,\nBaltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 16784–16804.\nPMLR, 2022. URL https://proceedings.mlr.press/v162/nichol22a.html.\nWeili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-\nspace energy-based models. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy\nLiang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-\n14, 2021, virtual, pp. 13497–13510, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n701d804549a4a23d3cae801dac6c2c75-Abstract.html.\nOpenAI. GPT-4: Contributions and System Card. https://cdn.openai.com/contributions/gpt-4v.pdf, 2023a.\nOpenAI. GPT-4v System Card. https://openai.com/research/gpt-4v-system-card, 2023b.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125.\nURL https://doi.org/10.48550/arXiv.2204.06125.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674–10685. IEEE, 2022. doi:\n10.1109/CVPR52688.2022.01042. URL https://doi.org/10.1109/CVPR52688.2022.01042.\nNatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023,\npp. 22500–22510. IEEE, 2023. doi: 10.1109/CVPR52729.2023.02155. URL https://doi.org/10.1109/\nCVPR52729.2023.02155.\nSimo Ryu. Merging loras. https://github.com/cloneofsimo/lora, 2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kam-\nyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho,\nDavid J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep lan-\nguageunderstanding. InNeurIPS,2022. URLhttp://papers.nips.cc/paper_files/paper/2022/hash/\nec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.\n18\nPublished in Transactions on Machine Learning Research (11/2024)\nViraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.\nZiplora: Any subject in any style by effectively merging loras. CoRR, abs/2311.13600, 2023. doi:\n10.48550/ARXIV.2311.13600. URL https://doi.org/10.48550/arXiv.2311.13600.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei (eds.), Proceedings\nof the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference Proceedings, pp. 2256–2265. JMLR.org, 2015. URL\nhttp://proceedings.mlr.press/v37/sohl-dickstein15.html.\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,\nLu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan.\nStyledrop: Text-to-image generation in any style. CoRR, abs/2306.00983, 2023. doi: 10.48550/ARXIV.\n2306.00983. URL https://doi.org/10.48550/arXiv.2306.00983.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nURL https://openreview.net/forum?id=PxTIG12RRHS.\nJoshTenenbaum. Buildingmachinesthatlearnandthinklikepeople. InElisabethAndré,SvenKoenig,Mehdi\nDastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on Autonomous\nAgents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pp. 5. International\nFoundation for Autonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018. URL\nhttp://dl.acm.org/citation.cfm?id=3237389.\nZhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang,\nand Mingyuan Zhou. In-context learning unlocked for diffusion models. CoRR, abs/2305.01115, 2023. doi:\n10.48550/ARXIV.2305.01115. URL https://doi.org/10.48550/arXiv.2305.01115.\nXun Wu, Shaohan Huang, and Furu Wei. Mixture of lora experts. arXiv preprint arXiv:2404.13628, 2024.\nZuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. Modeling image composition for\ncomplex scene generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2022, New Orleans, LA, USA, June 18-24, 2022, pp. 7754–7763. IEEE, 2022. doi: 10.1109/CVPR52688.\n2022.00761. URL https://doi.org/10.1109/CVPR52688.2022.00761.\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with\narithmetic operations. CoRR, abs/2306.14870, 2023a. doi: 10.48550/ARXIV.2306.14870. URL https:\n//doi.org/10.48550/arXiv.2306.14870.\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang\nWang, and Linda Ruth Petzold. Gpt-4v(ision) as a generalist evaluator for vision-language tasks. CoRR,\nabs/2311.01361, 2023b. doi: 10.48550/ARXIV.2311.01361. URL https://doi.org/10.48550/arXiv.\n2311.01361.\nMing Zhong, Chenxin An, Weizhu Chen, Jiawei Han, and Pengcheng He. Seeking neural nuggets: Knowledge\ntransfer in large language models from a parametric perspective. CoRR, abs/2310.11451, 2023. doi:\n10.48550/ARXIV.2310.11451. URL https://doi.org/10.48550/arXiv.2310.11451.\n19\nPublished in Transactions on Machine Learning Research (11/2024)\nA Appendix\nA.1 Details for Comparative Evaluation using GPT-4V\nSee Table 7 for the full prompts and Table 8 for a case study of evaluation results.\nTable 7: The full version of evaluation prompts for comparative evaluation with GPT-4V.\nEvaluation Prompt\nIneedassistanceincomparativelyevaluatingtwotext-to-imagemodelsbasedontheirabilitytocomposedifferent\nelementsintoasingleimage. Theelementsandtheirkeyfeaturesareasfollows:\n1. Character(KamadoNezuko): kamadonezuko,blackhair,pinkeyes,forehead\n2. Background(BambooBackground): bamboolight,outdoors,bamboo\n3. Object(HugeTwo-HandedBurger): two-handedburger,holdingahugeburger\nPleasehelpmeratebothgivenimagesonthefollowingevaluationdimensionsandcriteria:\nCompositionQuality:\n-Scoreonascaleof0to10,in0.5increments,where10isthebestand0istheworst.\n-Deduct3pointsifanyelementismissingorincorrectlydepicted.\n-Deduct1pointforeachmissingorincorrectfeaturewithinanelement.\n-Deduct1pointforminorinconsistenciesorlackofharmonybetweenelements.\n-Additionaldeductionscanbemadeforcompositionsthatlackcoherence,creativity,orrealism.\nImageQuality:\n-Scoreonascaleof0to10,in0.5increments,where10isthebestand0istheworst.\n-Deduct3pointsforeachdeformityintheimage(e.g.,extralimbsorfingers,distortedface,incorrectproportions).\n-Deduct2pointsfornoticeableissueswithtexture,lighting,orcolor.\n-Deduct1pointforeachminorflaworimperfection.\n-Additionaldeductionscanbemadeforanyissuesaffectingtheoverallaestheticorclarityoftheimage.\nPleaseformattheevaluationasfollows:\nForImage1:\n[Explanation of evaluation]\nForImage2:\n[Explanation of evaluation]\nScores:\nImage1: CompositionQuality: [score]/10,ImageQuality: [score]/10\nImage2: CompositionQuality: [score]/10,ImageQuality: [score]/10\nBasedontheaboveguidelines,helpmetoconductastep-by-stepcomparativeevaluationofthegivenimages. The\nscoringshouldfollowtwoprinciples:\n1. Pleaseevaluatecritically.\n2. Trynottoletthetwomodelsendinatieonbothdimensions.\n20\nPublished in Transactions on Machine Learning Research (11/2024)\nTable 8: The full version of evaluation results from GPT-4V for comparative evaluation.\nEvaluation Results from GPT-4V\nToevaluatetheimagesprovided,I’llassessthecompositionqualityandimagequalitybasedonthecriteriaoutlinedin\nyourinstructions. Iwillidentifyeachelementanditskeyfeatures,andthenratetheimagesaccordingly.\nForImage1:\nCompositionQuality:\n-Elements: Allkeyelementsarepresent: character,background,andobject(0pointsdeducted).\n-Featureswithintheelements:\n-Thecharacter’shaircolorisnotblackasspecified(-1 point).\n-Theburgershowsadeformedconnectionwhichdetractsfromthequalityoftheobject’sdepiction(-3 point).\n-Thecharacter’sfingersaredeformed(-1 point).\nThebamboobackgroundisaccurate,fulfillingtheoutdoorsettingcriteria. Despitethis,themainissuesliewiththe\nincorrecthaircolorandthedeformitiesintheburgerandfingers,leadingtodeductions.\nImageQuality:\n-Theimageisfreefromtexture,lighting,orcolorissues(0pointsdeducted).\n-Deformitiesarepresentinthefingersandtheburger,whichsignificantlyimpactstheimage’squality. (-3 point).\nTheclarityoftheimageismaintained,butthedeformitiesnotablyaffecttheoverallaestheticquality.\nForImage2:\nCompositionQuality:\n-Elements: Allkeyelementsarepresent: character,background,andobject(0pointsdeducted).\n-Featureswithintheelements:\n- The character has black hair and pink eyes; the forehead is shown, and the burger is depicted accurately (0\npointsdeducted).\nTherearenoinconsistenciesorlackofharmonybetweenelements,andthecompositioniscoherentandaesthetically\npleasing.\nImageQuality:\n-Therearenodeformitiesintheimage(0pointsdeducted).\n-Thetexture,lighting,andcolorarewellexecuted(0pointsdeducted).\nTheimagemaintainsahighlevelofclarityandoverallaestheticappeal,withnovisibleimperfectionsorissues.\nScores:\nImage1: CompositionQuality: 5/10,ImageQuality: 7/10\nImage2: CompositionQuality: 10/10,ImageQuality: 10/10\nInsummary,Image2achievesperfectscoresacrossbothdimensions,demonstratingexcellentcompositionandimage\nquality. Image 1, while containing all the key elements, suffers in its composition due to incorrect features and\ndeformities,andinimagequalityduetothenoteddeformities.\n21\nPublished in Transactions on Machine Learning Research (11/2024)\nA.2 Details for ComposLoRA\nSee Table 9 for the detailed descriptions of each LoRA in ComposLoRA Testbed.\nTable 9: Detailed descriptions of each LoRA in the ComposLoRA.\nLoRA Category Trigger Words Source\nAnime Style Subset\nKamadoNezuko Character kamadonezuko,blackhair,pinkeyes,forehead Link\nTexastheOmertosainArknights Character omertosa,1girl,wolfears,longhair Link\nSonGoku Character songoku,spikedhair,muscularmale,wristband Link\nGarregMachMonasteryUniform Clothing gmuniform,bluethighhighs,longsleeves Link\nZeroSuit(Metroid) Clothing zerosuit,bluegloves,highheels Link\nHand-drawnStyle Style lineart,hand-drawnstyle Link\nChineseInkWashStyle Style shuimobysim,traditionalchineseinkpainting Link\nBamboolightBackground Background bamboolight,outdoors,bamboo Link\nAuroralBackground Background auroral,starrysky,outdoors Link\nHugeTwo-HandedBurger Object two-handedburger,holdingahugeburgerwithbothhands Link\nToast Object toast,toastinmouth Link\nRealistic Style Subset\nIU(LeeJiEun,Koreansinger) Character iu1,longstraightblackhair,hazeleyes,diamondstudearrings Link\nScarlettJohansson Character scarlett,shortredhair,blueeyes Link\nTheRock(DwayneJohnson) Character th3r0ckwithnohair,muscularmale,seriouslookonhisface Link\nThaiUniversityUniform Clothing mahalaiuniform,whiteshirtshortsleeves,blackpencilskirt Link\nSchoolDress Clothing schooluniform,whiteshirt,redtie,bluepleatedmicroskirt Link\nJapaneseFilmColorStyle Style filmoverlay,filmgrain Link\nBrightStyle Style brightlighting Link\nLibraryBookshelfBackground Background lib_bg,librarybookshelf Link\nForestBackground Background slg,river,forest Link\nUmbrella Object transparentumbrella Link\nBubbleGum Object blowbubblegum Link\n22",
    "pdf_filename": "Multi-LoRA_Composition_for_Image_Generation.pdf"
}