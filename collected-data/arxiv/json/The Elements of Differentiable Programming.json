{
    "title": "The Elements of Differentiable Programming",
    "context": "",
    "body": "The Elements of\nDifferentiable Programming\nMathieu Blondel\nGoogle DeepMind\nmblondel@google.com\nVincent Roulet\nGoogle DeepMind\nvroulet@google.com\nDraft version 2 (last update: July 24, 2024)\narXiv:2403.14606v2  [cs.LG]  24 Jul 2024\n\nContents\n1\nIntroduction\n6\n1.1\nWhat is differentiable programming? . . . . . . . . . . . .\n6\n1.2\nBook goals and scope . . . . . . . . . . . . . . . . . . . .\n9\n1.3\nIntended audience . . . . . . . . . . . . . . . . . . . . . .\n10\n1.4\nHow to read this book? . . . . . . . . . . . . . . . . . . .\n10\n1.5\nRelated work . . . . . . . . . . . . . . . . . . . . . . . . .\n10\nI\nFundamentals\n12\n2\nDifferentiation\n13\n2.1\nUnivariate functions . . . . . . . . . . . . . . . . . . . . .\n13\n2.1.1\nDerivatives . . . . . . . . . . . . . . . . . . . . . .\n13\n2.1.2\nCalculus rules . . . . . . . . . . . . . . . . . . . .\n17\n2.1.3\nLeibniz’s notation . . . . . . . . . . . . . . . . . .\n19\n2.2\nMultivariate functions . . . . . . . . . . . . . . . . . . . .\n20\n2.2.1\nDirectional derivatives . . . . . . . . . . . . . . . .\n20\n2.2.2\nGradients\n. . . . . . . . . . . . . . . . . . . . . .\n21\n2.2.3\nJacobians\n. . . . . . . . . . . . . . . . . . . . . .\n25\n2.3\nLinear differentiation maps\n. . . . . . . . . . . . . . . . .\n30\n2.3.1\nThe need for linear maps . . . . . . . . . . . . . .\n31\n2.3.2\nEuclidean spaces . . . . . . . . . . . . . . . . . . .\n32\n\n2.3.3\nLinear maps and their adjoints . . . . . . . . . . .\n33\n2.3.4\nJacobian-vector products . . . . . . . . . . . . . .\n33\n2.3.5\nVector-Jacobian products . . . . . . . . . . . . . .\n35\n2.3.6\nChain rule . . . . . . . . . . . . . . . . . . . . . .\n36\n2.3.7\nFunctions of multiple inputs (fan-in) . . . . . . . .\n36\n2.3.8\nFunctions with multiple outputs (fan-out) . . . . .\n38\n2.3.9\nExtensions to non-Euclidean linear spaces . . . . .\n39\n2.4\nSecond-order differentiation . . . . . . . . . . . . . . . . .\n40\n2.4.1\nSecond derivatives . . . . . . . . . . . . . . . . . .\n40\n2.4.2\nSecond directional derivatives . . . . . . . . . . . .\n41\n2.4.3\nHessians . . . . . . . . . . . . . . . . . . . . . . .\n42\n2.4.4\nHessian-vector products . . . . . . . . . . . . . . .\n43\n2.4.5\nSecond-order Jacobians . . . . . . . . . . . . . . .\n44\n2.5\nHigher-order differentiation . . . . . . . . . . . . . . . . .\n45\n2.5.1\nHigher-order derivatives . . . . . . . . . . . . . . .\n45\n2.5.2\nHigher-order directional derivatives . . . . . . . . .\n45\n2.5.3\nHigher-order Jacobians . . . . . . . . . . . . . . .\n46\n2.5.4\nTaylor expansions . . . . . . . . . . . . . . . . . .\n46\n2.6\nDifferential geometry\n. . . . . . . . . . . . . . . . . . . .\n47\n2.6.1\nDifferentiability on manifolds . . . . . . . . . . . .\n48\n2.6.2\nTangent spaces and pushforward operators . . . . .\n48\n2.6.3\nCotangent spaces and pullback operators\n. . . . .\n50\n2.7\nGeneralized derivatives\n. . . . . . . . . . . . . . . . . . .\n53\n2.7.1\nRademacher’s theorem\n. . . . . . . . . . . . . . .\n53\n2.7.2\nClarke derivatives . . . . . . . . . . . . . . . . . .\n54\n2.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3\nProbabilistic learning\n59\n3.1\nProbability distributions . . . . . . . . . . . . . . . . . . .\n59\n3.1.1\nDiscrete probability distributions . . . . . . . . . .\n59\n3.1.2\nContinuous probability distributions\n. . . . . . . .\n60\n3.2\nMaximum likelihood estimation . . . . . . . . . . . . . . .\n61\n3.2.1\nNegative log-likelihood\n. . . . . . . . . . . . . . .\n61\n3.2.2\nConsistency w.r.t. the Kullback-Leibler divergence .\n61\n3.3\nProbabilistic supervised learning\n. . . . . . . . . . . . . .\n62\n3.3.1\nConditional probability distributions\n. . . . . . . .\n62\n\n3.3.2\nInference . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.3.3\nBinary classification . . . . . . . . . . . . . . . . .\n63\n3.3.4\nMulticlass classification . . . . . . . . . . . . . . .\n65\n3.3.5\nRegression . . . . . . . . . . . . . . . . . . . . . .\n67\n3.3.6\nMultivariate regression\n. . . . . . . . . . . . . . .\n68\n3.3.7\nInteger regression . . . . . . . . . . . . . . . . . .\n69\n3.3.8\nLoss functions . . . . . . . . . . . . . . . . . . . .\n70\n3.4\nExponential family distributions . . . . . . . . . . . . . . .\n71\n3.4.1\nDefinition\n. . . . . . . . . . . . . . . . . . . . . .\n71\n3.4.2\nThe log-partition function . . . . . . . . . . . . . .\n72\n3.4.3\nMaximum entropy principle . . . . . . . . . . . . .\n74\n3.4.4\nMaximum likelihood estimation . . . . . . . . . . .\n75\n3.4.5\nProbabilistic learning with exponential families . . .\n76\n3.5\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\nII\nDifferentiable programs\n79\n4\nParameterized programs\n80\n4.1\nRepresenting computer programs . . . . . . . . . . . . . .\n80\n4.1.1\nComputation chains . . . . . . . . . . . . . . . . .\n80\n4.1.2\nDirected acylic graphs . . . . . . . . . . . . . . . .\n81\n4.1.3\nComputer programs as DAGs . . . . . . . . . . . .\n83\n4.1.4\nArithmetic circuits . . . . . . . . . . . . . . . . . .\n85\n4.2\nFeedforward networks . . . . . . . . . . . . . . . . . . . .\n86\n4.3\nMultilayer perceptrons . . . . . . . . . . . . . . . . . . . .\n87\n4.3.1\nCombining affine layers and activations . . . . . . .\n87\n4.3.2\nLink with generalized linear models . . . . . . . . .\n87\n4.4\nActivation functions . . . . . . . . . . . . . . . . . . . . .\n88\n4.4.1\nReLU and softplus . . . . . . . . . . . . . . . . . .\n88\n4.4.2\nMax pooling and log-sum-exp . . . . . . . . . . . .\n89\n4.4.3\nSigmoids: binary step and logistic functions\n. . . .\n90\n4.4.4\nProbability mappings: argmax and softargmax . . .\n91\n4.5\nResidual neural networks\n. . . . . . . . . . . . . . . . . .\n93\n4.6\nRecurrent neural networks . . . . . . . . . . . . . . . . . .\n94\n4.6.1\nVector to sequence\n. . . . . . . . . . . . . . . . .\n95\n\n4.6.2\nSequence to vector\n. . . . . . . . . . . . . . . . .\n96\n4.6.3\nSequence to sequence (aligned) . . . . . . . . . . .\n96\n4.6.4\nSequence to sequence (unaligned)\n. . . . . . . . .\n97\n4.7\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n5\nControl flows\n99\n5.1\nComparison operators . . . . . . . . . . . . . . . . . . . .\n99\n5.2\nSoft inequality operators\n. . . . . . . . . . . . . . . . . . 101\n5.2.1\nHeuristic definition\n. . . . . . . . . . . . . . . . . 101\n5.2.2\nStochastic process perspective\n. . . . . . . . . . . 102\n5.3\nSoft equality operators\n. . . . . . . . . . . . . . . . . . . 104\n5.3.1\nHeuristic definition\n. . . . . . . . . . . . . . . . . 104\n5.3.2\nStochastic process perspective\n. . . . . . . . . . . 106\n5.3.3\nGaussian process perspective . . . . . . . . . . . . 109\n5.4\nLogical operators\n. . . . . . . . . . . . . . . . . . . . . . 110\n5.5\nContinuous extensions of logical operators . . . . . . . . . 111\n5.5.1\nProbabilistic continuous extension\n. . . . . . . . . 111\n5.5.2\nTriangular norms and co-norms . . . . . . . . . . . 113\n5.6\nIf-else statements\n. . . . . . . . . . . . . . . . . . . . . . 114\n5.6.1\nDifferentiating through branch variables . . . . . . 115\n5.6.2\nDifferentiating through predicate variables . . . . . 116\n5.6.3\nContinuous relaxations\n. . . . . . . . . . . . . . . 117\n5.7\nElse-if statements . . . . . . . . . . . . . . . . . . . . . . 120\n5.7.1\nEncoding K branches . . . . . . . . . . . . . . . . 120\n5.7.2\nConditionals . . . . . . . . . . . . . . . . . . . . . 121\n5.7.3\nDifferentiating through branch variables . . . . . . 122\n5.7.4\nDifferentiating through predicate variables . . . . . 123\n5.7.5\nContinuous relaxations\n. . . . . . . . . . . . . . . 124\n5.8\nFor loops . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n5.9\nScan functions . . . . . . . . . . . . . . . . . . . . . . . . 127\n5.10 While loops\n. . . . . . . . . . . . . . . . . . . . . . . . . 128\n5.10.1 While loops as cyclic graphs\n. . . . . . . . . . . . 128\n5.10.2 Unrolled while loops . . . . . . . . . . . . . . . . . 129\n5.10.3 Markov chain perspective . . . . . . . . . . . . . . 132\n5.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n\n6\nData structures\n136\n6.1\nLists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n6.1.1\nBasic operations . . . . . . . . . . . . . . . . . . . 137\n6.1.2\nOperations on variable-length lists . . . . . . . . . 138\n6.1.3\nContinuous relaxations using soft indexing . . . . . 140\n6.2\nDictionaries\n. . . . . . . . . . . . . . . . . . . . . . . . . 143\n6.2.1\nBasic operations . . . . . . . . . . . . . . . . . . . 143\n6.2.2\nContinuous relaxation using kernel regression\n. . . 145\n6.2.3\nDiscrete probability distribution perspective . . . . 146\n6.2.4\nLink with attention in Transformers\n. . . . . . . . 147\n6.3\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nIII\nDifferentiating through programs\n150\n7\nFinite differences\n151\n7.1\nForward differences\n. . . . . . . . . . . . . . . . . . . . . 151\n7.2\nBackward differences\n. . . . . . . . . . . . . . . . . . . . 152\n7.3\nCentral differences . . . . . . . . . . . . . . . . . . . . . . 153\n7.4\nHigher-accuracy finite differences . . . . . . . . . . . . . . 154\n7.5\nHigher-order finite differences . . . . . . . . . . . . . . . . 155\n7.6\nComplex-step derivatives\n. . . . . . . . . . . . . . . . . . 156\n7.7\nComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n7.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8\nAutomatic differentiation\n159\n8.1\nComputation chains . . . . . . . . . . . . . . . . . . . . . 159\n8.1.1\nForward-mode . . . . . . . . . . . . . . . . . . . . 160\n8.1.2\nReverse-mode . . . . . . . . . . . . . . . . . . . . 162\n8.1.3\nComplexity of entire Jacobians . . . . . . . . . . . 167\n8.2\nFeedforward networks . . . . . . . . . . . . . . . . . . . . 169\n8.2.1\nComputing the adjoint\n. . . . . . . . . . . . . . . 169\n8.2.2\nComputing the gradient . . . . . . . . . . . . . . . 170\n8.3\nComputation graphs . . . . . . . . . . . . . . . . . . . . . 172\n8.3.1\nForward-mode . . . . . . . . . . . . . . . . . . . . 172\n8.3.2\nReverse-mode . . . . . . . . . . . . . . . . . . . . 173\n\n8.3.3\nComplexity, the Baur-Strassen theorem . . . . . . . 173\n8.4\nImplementation\n. . . . . . . . . . . . . . . . . . . . . . . 174\n8.4.1\nPrimitive functions\n. . . . . . . . . . . . . . . . . 174\n8.4.2\nClosure under function composition\n. . . . . . . . 175\n8.4.3\nExamples of JVPs and VJPs\n. . . . . . . . . . . . 176\n8.4.4\nAutomatic linear transposition\n. . . . . . . . . . . 177\n8.5\nCheckpointing . . . . . . . . . . . . . . . . . . . . . . . . 178\n8.5.1\nRecursive halving\n. . . . . . . . . . . . . . . . . . 179\n8.5.2\nDynamic programming\n. . . . . . . . . . . . . . . 181\n8.5.3\nOnline checkpointing\n. . . . . . . . . . . . . . . . 183\n8.6\nReversible layers . . . . . . . . . . . . . . . . . . . . . . . 184\n8.6.1\nGeneral case . . . . . . . . . . . . . . . . . . . . . 184\n8.6.2\nCase of orthonormal JVPs\n. . . . . . . . . . . . . 184\n8.7\nRandomized forward-mode estimator . . . . . . . . . . . . 185\n8.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n9\nSecond-order automatic differentiation\n187\n9.1\nHessian-vector products . . . . . . . . . . . . . . . . . . . 187\n9.1.1\nFour possible methods\n. . . . . . . . . . . . . . . 187\n9.1.2\nComplexity . . . . . . . . . . . . . . . . . . . . . . 188\n9.2\nGauss-Newton matrix . . . . . . . . . . . . . . . . . . . . 192\n9.2.1\nAn approximation of the Hessian . . . . . . . . . . 192\n9.2.2\nGauss-Newton chain rule . . . . . . . . . . . . . . 193\n9.2.3\nGauss-Newton vector product . . . . . . . . . . . . 193\n9.2.4\nGauss-Newton matrix factorization . . . . . . . . . 194\n9.2.5\nStochastic setting . . . . . . . . . . . . . . . . . . 195\n9.3\nFisher information matrix . . . . . . . . . . . . . . . . . . 195\n9.3.1\nDefinition using the score function . . . . . . . . . 195\n9.3.2\nLink with the Hessian . . . . . . . . . . . . . . . . 196\n9.3.3\nEquivalence with the Gauss-Newton matrix\n. . . . 196\n9.4\nInverse-Hessian vector product\n. . . . . . . . . . . . . . . 198\n9.4.1\nDefinition as a linear map . . . . . . . . . . . . . . 198\n9.4.2\nImplementation with matrix-free linear solvers . . . 198\n9.4.3\nComplexity . . . . . . . . . . . . . . . . . . . . . . 199\n9.5\nSecond-order backpropagation\n. . . . . . . . . . . . . . . 200\n9.5.1\nSecond-order Jacobian chain rule . . . . . . . . . . 200\n\n9.5.2\nComputation chains . . . . . . . . . . . . . . . . . 202\n9.5.3\nFan-in and fan-out\n. . . . . . . . . . . . . . . . . 203\n9.6\nBlock diagonal approximations . . . . . . . . . . . . . . . 204\n9.6.1\nFeedforward networks . . . . . . . . . . . . . . . . 204\n9.6.2\nComputation graphs . . . . . . . . . . . . . . . . . 206\n9.7\nDiagonal approximations\n. . . . . . . . . . . . . . . . . . 206\n9.7.1\nComputation chains . . . . . . . . . . . . . . . . . 207\n9.7.2\nComputation graphs . . . . . . . . . . . . . . . . . 208\n9.8\nRandomized estimators . . . . . . . . . . . . . . . . . . . 209\n9.8.1\nGirard-Hutchinson estimator\n. . . . . . . . . . . . 209\n9.8.2\nBartlett estimator for the factorization . . . . . . . 210\n9.8.3\nBartlett estimator for the diagonal . . . . . . . . . 211\n9.9\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n10 Inference in graphical models as differentiation\n213\n10.1 Chain rule of probability . . . . . . . . . . . . . . . . . . . 213\n10.2 Conditional independence . . . . . . . . . . . . . . . . . . 214\n10.3 Inference problems . . . . . . . . . . . . . . . . . . . . . . 215\n10.3.1 Joint probability distributions . . . . . . . . . . . . 215\n10.3.2 Likelihood . . . . . . . . . . . . . . . . . . . . . . 215\n10.3.3 Maximum a-posteriori inference . . . . . . . . . . . 215\n10.3.4 Marginal inference . . . . . . . . . . . . . . . . . . 216\n10.3.5 Expectation, convex hull, marginal polytope . . . . 216\n10.3.6 Complexity of brute force . . . . . . . . . . . . . . 218\n10.4 Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 218\n10.4.1 The Markov property . . . . . . . . . . . . . . . . 219\n10.4.2 Time-homogeneous Markov chains . . . . . . . . . 221\n10.4.3 Higher-order Markov chains . . . . . . . . . . . . . 222\n10.5 Bayesian networks . . . . . . . . . . . . . . . . . . . . . . 222\n10.5.1 Expressing variable dependencies using DAGs\n. . . 222\n10.5.2 Parameterizing Bayesian networks\n. . . . . . . . . 223\n10.5.3 Ancestral sampling\n. . . . . . . . . . . . . . . . . 224\n10.6 Markov random fields . . . . . . . . . . . . . . . . . . . . 224\n10.6.1 Expressing factors using undirected graphs . . . . . 224\n10.6.2 MRFs as exponential family distributions . . . . . . 225\n10.6.3 Conditional random fields . . . . . . . . . . . . . . 227\n\n10.6.4 Sampling . . . . . . . . . . . . . . . . . . . . . . . 227\n10.7 Inference on chains\n. . . . . . . . . . . . . . . . . . . . . 227\n10.7.1 The forward-backward algorithm . . . . . . . . . . 228\n10.7.2 The Viterbi algorithm . . . . . . . . . . . . . . . . 229\n10.8 Inference on trees . . . . . . . . . . . . . . . . . . . . . . 231\n10.9 Inference as differentiation\n. . . . . . . . . . . . . . . . . 232\n10.9.1 Inference as gradient of the log-partition . . . . . . 232\n10.9.2 Semirings and softmax operators . . . . . . . . . . 233\n10.9.3 Inference as backpropagation . . . . . . . . . . . . 235\n10.10Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n11 Differentiating through optimization\n239\n11.1 Implicit functions\n. . . . . . . . . . . . . . . . . . . . . . 239\n11.1.1 Optimization problems\n. . . . . . . . . . . . . . . 240\n11.1.2 Nonlinear equations . . . . . . . . . . . . . . . . . 240\n11.1.3 Application to bilevel optimization . . . . . . . . . 240\n11.2 Envelope theorems . . . . . . . . . . . . . . . . . . . . . . 241\n11.2.1 Danskin’s theorem . . . . . . . . . . . . . . . . . . 242\n11.2.2 Rockafellar’s theorem . . . . . . . . . . . . . . . . 243\n11.3 Implicit function theorem . . . . . . . . . . . . . . . . . . 244\n11.3.1 Univariate functions . . . . . . . . . . . . . . . . . 244\n11.3.2 Multivariate functions . . . . . . . . . . . . . . . . 246\n11.3.3 JVP and VJP of implicit functions . . . . . . . . . 247\n11.3.4 Proof of the implicit function theorem . . . . . . . 248\n11.4 Adjoint state method\n. . . . . . . . . . . . . . . . . . . . 249\n11.4.1 Differentiating nonlinear equations . . . . . . . . . 249\n11.4.2 Relation with envelope theorems . . . . . . . . . . 250\n11.4.3 Proof using the method of Lagrange multipliers . . 251\n11.4.4 Proof using the implicit function theorem\n. . . . . 251\n11.4.5 Reverse mode as adjoint method with backsubstitution252\n11.5 Inverse function theorem\n. . . . . . . . . . . . . . . . . . 255\n11.5.1 Differentiating inverse functions\n. . . . . . . . . . 255\n11.5.2 Link with the implicit function theorem\n. . . . . . 255\n11.5.3 Proof of inverse function theorem\n. . . . . . . . . 255\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n\n12 Differentiating through integration\n258\n12.1 Differentiation under the integral sign\n. . . . . . . . . . . 258\n12.2 Differentiating through expectations\n. . . . . . . . . . . . 259\n12.2.1 Parameter-independent distributions . . . . . . . . 259\n12.2.2 Parameter-dependent distributions . . . . . . . . . 260\n12.2.3 Application to expected loss functions . . . . . . . 262\n12.2.4 Application to experimental design . . . . . . . . . 263\n12.3 Score function estimators, REINFORCE\n. . . . . . . . . . 264\n12.3.1 Scalar-valued functions . . . . . . . . . . . . . . . 264\n12.3.2 Variance reduction . . . . . . . . . . . . . . . . . . 267\n12.3.3 Vector-valued functions . . . . . . . . . . . . . . . 268\n12.3.4 Second derivatives . . . . . . . . . . . . . . . . . . 269\n12.4 Path gradient estimators, reparametrization trick\n. . . . . 270\n12.4.1 Location-scale transforms . . . . . . . . . . . . . . 270\n12.4.2 Differentiable transforms\n. . . . . . . . . . . . . . 272\n12.4.3 Inverse transforms . . . . . . . . . . . . . . . . . . 273\n12.4.4 Pushforward operators\n. . . . . . . . . . . . . . . 275\n12.4.5 Change-of-variables theorem\n. . . . . . . . . . . . 277\n12.5 Stochastic programs . . . . . . . . . . . . . . . . . . . . . 278\n12.5.1 Stochastic computation graphs . . . . . . . . . . . 278\n12.5.2 Examples\n. . . . . . . . . . . . . . . . . . . . . . 281\n12.5.3 Unbiased gradient estimators . . . . . . . . . . . . 283\n12.5.4 Local vs. global expectations . . . . . . . . . . . . 284\n12.6 Differential equations . . . . . . . . . . . . . . . . . . . . 286\n12.6.1 Parameterized differential equations\n. . . . . . . . 286\n12.6.2 Continuous adjoint method . . . . . . . . . . . . . 288\n12.6.3 Gradients via the continuous adjoint method . . . . 290\n12.6.4 Gradients via reverse-mode on discretization . . . . 293\n12.6.5 Reversible discretization schemes . . . . . . . . . . 294\n12.6.6 Proof of the continuous adjoint method . . . . . . 296\n12.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\nIV\nSmoothing programs\n300\n13 Smoothing by optimization\n301\n\n13.1 Primal approach . . . . . . . . . . . . . . . . . . . . . . . 301\n13.1.1 Infimal convolution\n. . . . . . . . . . . . . . . . . 302\n13.1.2 Moreau envelope\n. . . . . . . . . . . . . . . . . . 303\n13.1.3 Vector-valued functions . . . . . . . . . . . . . . . 307\n13.2 Legendre–Fenchel transforms, convex conjugates . . . . . . 309\n13.2.1 Definition\n. . . . . . . . . . . . . . . . . . . . . . 309\n13.2.2 Closed-form examples . . . . . . . . . . . . . . . . 310\n13.2.3 Properties . . . . . . . . . . . . . . . . . . . . . . 312\n13.2.4 Conjugate calculus\n. . . . . . . . . . . . . . . . . 314\n13.2.5 Fast Legendre transform\n. . . . . . . . . . . . . . 314\n13.3 Dual approach . . . . . . . . . . . . . . . . . . . . . . . . 315\n13.3.1 Duality between strong convexity and smoothness . 315\n13.3.2 Smoothing by dual regularization . . . . . . . . . . 316\n13.3.3 Equivalence between primal and dual regularizations 318\n13.3.4 Regularization scaling . . . . . . . . . . . . . . . . 319\n13.3.5 Generalized entropies . . . . . . . . . . . . . . . . 320\n13.4 Smoothed ReLU functions\n. . . . . . . . . . . . . . . . . 324\n13.5 Smoothed max operators . . . . . . . . . . . . . . . . . . 326\n13.5.1 Definition and properties . . . . . . . . . . . . . . 326\n13.5.2 Reduction to root finding . . . . . . . . . . . . . . 327\n13.5.3 The softmax . . . . . . . . . . . . . . . . . . . . . 328\n13.5.4 The sparsemax . . . . . . . . . . . . . . . . . . . . 329\n13.5.5 Recovering smoothed ReLU functions\n. . . . . . . 332\n13.6 Relaxed step functions (sigmoids) . . . . . . . . . . . . . . 332\n13.7 Relaxed argmax operators . . . . . . . . . . . . . . . . . . 333\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n14 Smoothing by integration\n339\n14.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . 339\n14.1.1 Convolution operators . . . . . . . . . . . . . . . . 339\n14.1.2 Convolution with a kernel . . . . . . . . . . . . . . 340\n14.1.3 Discrete convolution . . . . . . . . . . . . . . . . . 341\n14.1.4 Differentiation . . . . . . . . . . . . . . . . . . . . 343\n14.1.5 Multidimensional convolution . . . . . . . . . . . . 343\n14.1.6 Link between convolution and infimal convolution . 343\n14.1.7 The soft infimal convolution\n. . . . . . . . . . . . 344\n\n14.1.8 The soft Moreau envelope . . . . . . . . . . . . . . 345\n14.2 Fourier and Laplace transforms . . . . . . . . . . . . . . . 346\n14.2.1 Convolution theorem\n. . . . . . . . . . . . . . . . 346\n14.2.2 Link between Fourier and Legendre transforms . . . 346\n14.2.3 The soft Legendre-Fenchel transform . . . . . . . . 347\n14.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n14.3.1 Smoothed step function . . . . . . . . . . . . . . . 350\n14.3.2 Smoothed ReLU function . . . . . . . . . . . . . . 351\n14.4 Perturbation of blackbox functions . . . . . . . . . . . . . 353\n14.4.1 Expectation in a location-scale family\n. . . . . . . 353\n14.4.2 Gradient estimation by reparametrization\n. . . . . 354\n14.4.3 Gradient estimation by SFE, Stein’s lemma\n. . . . 355\n14.4.4 Link between reparametrization and SFE . . . . . . 356\n14.4.5 Variance reduction and evolution strategies\n. . . . 357\n14.4.6 Zero-temperature limit\n. . . . . . . . . . . . . . . 358\n14.5 Gumbel tricks . . . . . . . . . . . . . . . . . . . . . . . . 359\n14.5.1 The Gumbel distribution\n. . . . . . . . . . . . . . 359\n14.5.2 Perturbed comparison . . . . . . . . . . . . . . . . 360\n14.5.3 Perturbed argmax . . . . . . . . . . . . . . . . . . 361\n14.5.4 Perturbed max . . . . . . . . . . . . . . . . . . . . 362\n14.5.5 Gumbel trick for sampling . . . . . . . . . . . . . . 363\n14.5.6 Perturb-and-MAP . . . . . . . . . . . . . . . . . . 364\n14.5.7 Gumbel-softmax . . . . . . . . . . . . . . . . . . . 366\n14.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 367\nV\nOptimizing differentiable programs\n369\n15 Optimization basics\n370\n15.1 Objective functions\n. . . . . . . . . . . . . . . . . . . . . 370\n15.2 Oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n15.3 Variational perspective of optimization algorithms . . . . . 372\n15.4 Classes of functions . . . . . . . . . . . . . . . . . . . . . 372\n15.4.1 Lipschitz functions\n. . . . . . . . . . . . . . . . . 372\n15.4.2 Smooth functions . . . . . . . . . . . . . . . . . . 373\n15.4.3 Convex functions\n. . . . . . . . . . . . . . . . . . 375\n\n15.4.4 Strongly-convex functions . . . . . . . . . . . . . . 377\n15.4.5 Nonconvex functions\n. . . . . . . . . . . . . . . . 378\n15.5 Performance guarantees . . . . . . . . . . . . . . . . . . . 380\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\n16 First-order optimization\n384\n16.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . 384\n16.1.1 Variational perspective\n. . . . . . . . . . . . . . . 384\n16.1.2 Convergence for smooth functions . . . . . . . . . 385\n16.1.3 Momentum and accelerated variants . . . . . . . . 387\n16.2 Stochastic gradient descent . . . . . . . . . . . . . . . . . 388\n16.2.1 Stochastic gradients . . . . . . . . . . . . . . . . . 389\n16.2.2 Vanilla SGD . . . . . . . . . . . . . . . . . . . . . 390\n16.2.3 Momentum variants . . . . . . . . . . . . . . . . . 391\n16.2.4 Adaptive variants\n. . . . . . . . . . . . . . . . . . 392\n16.3 Projected gradient descent\n. . . . . . . . . . . . . . . . . 392\n16.3.1 Variational perspective\n. . . . . . . . . . . . . . . 393\n16.3.2 Optimality conditions . . . . . . . . . . . . . . . . 394\n16.3.3 Commonly-used projections . . . . . . . . . . . . . 394\n16.4 Proximal gradient method . . . . . . . . . . . . . . . . . . 395\n16.4.1 Variational perspective\n. . . . . . . . . . . . . . . 396\n16.4.2 Optimality conditions . . . . . . . . . . . . . . . . 396\n16.4.3 Commonly-used proximal operators . . . . . . . . . 397\n16.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n17 Second-order optimization\n399\n17.1 Newton’s method . . . . . . . . . . . . . . . . . . . . . . 399\n17.1.1 Variational perspective\n. . . . . . . . . . . . . . . 399\n17.1.2 Regularized Newton method\n. . . . . . . . . . . . 400\n17.1.3 Approximate direction . . . . . . . . . . . . . . . . 401\n17.1.4 Convergence guarantees . . . . . . . . . . . . . . . 401\n17.1.5 Linesearch . . . . . . . . . . . . . . . . . . . . . . 401\n17.1.6 Geometric interpretation\n. . . . . . . . . . . . . . 402\n17.1.7 Stochastic Newton’s method . . . . . . . . . . . . 403\n17.2 Gauss-Newton method\n. . . . . . . . . . . . . . . . . . . 404\n17.2.1 With exact outer function . . . . . . . . . . . . . . 405\n\n17.2.2 With approximate outer function . . . . . . . . . . 406\n17.2.3 Linesearch . . . . . . . . . . . . . . . . . . . . . . 407\n17.2.4 Stochastic Gauss-Newton . . . . . . . . . . . . . . 407\n17.3 Natural gradient descent\n. . . . . . . . . . . . . . . . . . 408\n17.3.1 Variational perspective\n. . . . . . . . . . . . . . . 408\n17.3.2 Stochastic natural gradient descent . . . . . . . . . 409\n17.4 Quasi-Newton methods . . . . . . . . . . . . . . . . . . . 410\n17.4.1 BFGS\n. . . . . . . . . . . . . . . . . . . . . . . . 410\n17.4.2 Limited-memory BFGS\n. . . . . . . . . . . . . . . 411\n17.5 Approximate Hessian diagonal inverse preconditionners\n. . 411\n17.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n18 Duality\n413\n18.1 Dual norms\n. . . . . . . . . . . . . . . . . . . . . . . . . 413\n18.2 Fenchel duality . . . . . . . . . . . . . . . . . . . . . . . . 414\n18.3 Bregman divergences\n. . . . . . . . . . . . . . . . . . . . 417\n18.4 Fenchel-Young loss functions . . . . . . . . . . . . . . . . 420\n18.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\nReferences\n422\n\nThe Elements of\nDifferentiable Programming\nMathieu Blondel1 and Vincent Roulet1\n1Google DeepMind\nABSTRACT\nArtificial intelligence has recently experienced remarkable\nadvances, fueled by large models, vast datasets, acceler-\nated hardware, and, last but not least, the transformative\npower of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex com-\nputer programs (including those with control flows and data\nstructures), making gradient-based optimization of program\nparameters possible.\nAs an emerging paradigm, differentiable programming builds\nupon several areas of computer science and applied mathe-\nmatics, including automatic differentiation, graphical mod-\nels, optimization and statistics. This book presents a com-\nprehensive review of the fundamental concepts useful for\ndifferentiable programming. We adopt two main perspec-\ntives, that of optimization and that of probability, with clear\nanalogies between the two.\nDifferentiable programming is not merely the differentiation\nof programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differen-\ntiable, we inherently introduce probability distributions over\ntheir execution, providing a means to quantify the uncer-\ntainty associated with program outputs.\n\nAcknowledgements\nWe thank the following people for sending us feedback, suggestions\nand typos: Fabian Pedregosa, Kevin Murphy, Niklas Schmitz, Nidham\nGazagnadou, Bruno De Backer, David López, Guillaume Gautier, Sam\nDuffield, Logan Bruns, Wojciech Stokowiec, (add your name here!).\n2\n\nSource code\nWe provide some Python source code to accompany the book on github.\n3\n\nNotation\nTable 1: Naming conventions\nNotation\nDescription\nX ⊆RD\nInput space (e.g., features)\nY ⊆RM\nOutput space (e.g., classes)\nSk ⊆RDk\nOutput space on layer or state k\nW ⊆RP\nWeight space\nΛ ⊆RQ\nHyperparameter space\nΘ ⊆RR\nDistribution parameter space, logit space\nN\nNumber of training samples\nT\nNumber of optimization iterations\nx ∈X\nInput vector\ny ∈Y\nTarget vector\nsk ∈Sk\nState vector k\nw ∈W\nNetwork (model) weights\nλ ∈Λ\nHyperparameters\nθ ∈Θ\nDistribution parameters, logits\nπ ∈[0, 1]\nProbability value\nπ ∈△M\nProbability vector\n4\n\n5\nTable 2: Naming conventions (continued)\nNotation\nDescription\nf\nNetwork function\nf(·; x)\nNetwork function with x fixed\nL\nObjective function\nℓ\nLoss function\nκ\nKernel function\nϕ\nOutput embedding, sufficient statistic\nstep\nHeaviside step function\nlogisticσ\nLogistic function with temperature σ\nlogistic\nShorthand for logistic1\npθ\nModel distribution with parameters θ\nρ\nData distribution over X × Y\nρX\nData distribution over X\nµ, σ2\nMean and variance\nZ\nRandom noise variable\n\n1\nIntroduction\n1.1\nWhat is differentiable programming?\nA computer program is a sequence of elementary instructions for per-\nforming a task. In traditional computer programming, the program is\ntypically manually written by a programmer. However, for certain tasks,\nparticularly those involving intricate patterns and complex decision-\nmaking, such as image recognition or text generation, manually writing\na program is extremely challenging, if not impossible.\nIn contrast, modern neural networks offer a different approach. They\nare constructed by combining parameterized functional blocks and\nare trained directly from data using gradient-based optimization. This\nend-to-end training process, where the network learns both feature\nextraction and task execution simultaneously, allows neural networks to\ntackle complex tasks that were previously considered insurmountable\nfor traditional, hand-coded programs. This new programming paradigm\nhas been referred to as “differentiable programming” or “software 2.0”,\nterms popularized among others by LeCun (2018) and Karpathy (2017).\nWe given an informal definition below.\n6\n\n1.1. What is differentiable programming?\n7\nDefinition 1.1 (Differentiable programming). Differentiable program-\nming is a programming paradigm in which complex computer pro-\ngrams (including those with control flows and data structures) can\nbe differentiated end-to-end automatically, enabling gradient-based\noptimization of parameters in the program.\nModern neural networks as parameterized programs\nIn differentiable programming, as in regular computer programming, a\nclassical program is defined as the composition of elementary operations,\nforming a computation graph. The key difference is that the program\n(such as a neural network) contains parameters that can be adjusted\nfrom data and can be differentiated end-to-end, using automatic\ndifferentiation (autodiff). Typically, it is assumed that the program\ndefines a mathematically valid function (a.k.a. pure function): the\nfunction should return identical values for identical arguments and\nshould not have any side effects. Moreover, the function should have\nwell-defined derivatives, ensuring that it can be used in a gradient-\nbased optimization algorithm. Therefore, differentiable programming\nis not only the art of differentiating through programs but also of\ndesigning meaningful differentiable programs.\nWhy are derivatives important?\nMachine learning typically boils down to optimizing a certain objec-\ntive function, which is the composition of a loss function and a model\n(network) function. Derivative-free optimization is called zero-order\noptimization. It only assumes that we can evaluate the objective\nfunction that we wish to optimize. Unfortunately, it is known to suffer\nfrom the curse of dimensionality, i.e., it only scales to small di-\nmensional problems, such as less than 10 dimensions. Derivative-based\noptimization, on the other hand, is much more efficient and can scale to\nmillions or billions of parameters. Algorithms that use first and second\nderivatives are known as first-order and second-order algorithms,\nrespectively.\n\n8\nIntroduction\nWhy is autodiff important?\nBefore the autodiff revolution, researchers and practitioners needed\nto manually implement the gradient of the functions they wished to\noptimize. Manually deriving gradients can become very tedious for\ncomplicated functions. Moreover, every time the function is changed\n(for example, for trying out a new idea), the gradient needs to be re-\nderived. Autodiff is a game changer because it allows users to focus on\nquickly and creatively experimenting with functions for their tasks.\nDifferentiable programming is not just deep learning\nWhile there is clearly overlap between deep learning and differentiable\nprogramming, their focus is different. Deep learning studies artificial\nneural networks composed of multiple layers, able to learn intermediate\nrepresentations of the data. Neural network architectures have been\nproposed with various inductive biases. For example, convolutional\nneural networks are designed for images and transformers are designed\nfor sequences. On the other hand, differentiable programming studies the\ntechniques for designing complex programs and differentiating through\nthem. It is useful beyond deep learning: for instance in reinforcement\nlearning, probabilistic programming and scientific computing in general.\nDifferentiable programming is not just autodiff\nWhile autodiff is a key ingredient of differentiable programming, this\nis not the only one. Differentiable programming is also concerned with\nthe design of principled differentiable operations. In fact, much research\non differentiable programming has been devoted to make classical com-\nputer programming operations compatible with autodiff. As we shall\nsee, many differentiable relaxations can be interpreted in a probabilis-\ntic framework. A core theme of this book is the interplay between\noptimization, probability and differentiation. Differentiation is useful\nfor optimization and conversely, optimization can be used to design\ndifferentiable operators.\n\n1.2. Book goals and scope\n9\nOur vision for differentiable programming\nComputer programming offers powerful tools like control flows, data\nstructures, and standard libraries, enabling users to construct complex\nprograms for solving intricate problems. Our long-term vision is to\nachieve parity between traditional and differentiable programming, em-\npowering programmers to seamlessly express differentiable programs\n(such as neural networks) using the full suite of tools they are accus-\ntomed to. However, as discussed earlier, differentiable programming is\nnot simply a matter of applying automatic differentiation to existing\ncode. Programs must be designed with differentiability in mind. This\nusually comes to inducing a probability distribution over the program\nor its components. While significant work remains to fully realize this\nambitious goal, we hope this book offers a solid foundation.\n1.2\nBook goals and scope\nThe present book aims to provide an comprehensive introduction to\ndifferentiable programming with an emphasis on core mathematical\ntools.\n• In Part I, we review fundamentals: differentiation and proba-\nbilistic learning.\n• In Part II, we review differentiable programs. This includes\nneural networks, sequence networks and control flows.\n• In Part III, we review how to differentiate through programs.\nThis includes automatic differentiation, but also differentiating\nthrough optimization and integration (in particular, expectations).\n• In Part IV, we review smoothing programs. We focus on two\nmain techniques: infimal convolution, which comes from the world\nof optimization and convolution, which comes from the world of\nintegration. We also strive to spell out the connections between\nthem.\n\n10\nIntroduction\n• In Part V, we review optimizing programs: basic optimiza-\ntion concepts, first-order algorithms, second-order algorithms and\nduality.\nOur goal is to present the fundamental techniques useful for differentiable\nprogramming, not to survey how these techniques have been used in\nvarious applications.\n1.3\nIntended audience\nThis book is intended to be a graduate-level introduction to differentiable\nprogramming. Our pedagogical choices are made with the machine\nlearning community in mind. Some familiarity with calculus, linear\nalgebra, probability theory and machine learning is beneficial.\n1.4\nHow to read this book?\nThis book does not need to be read linearly chapter by chapter. When\nneeded, we indicate at the beginning of a chapter what chapters are\nrecommended to be read as a prerequisite.\n1.5\nRelated work\nDifferentiable programming builds upon a variety of connected topics.\nWe review in this section relevant textbooks, tutorials and software.\nStandard textbooks on backpropagation and automatic differenti-\nation are that of Werbos (1994) and Griewank and Walther (2008).\nA tutorial with a focus on machine learning is provided by Baydin\net al. (2018). Automatic differentiation is also reviewed as part of more\ngeneral textbooks, such as those of Deisenroth et al. (2020), Murphy\n(2022) (from a linear algebra perspective) and Murphy (2023) (from a\nfunctional perspective; autodiff section authored by Roy Frostig). The\npresent book was also influenced by Peyré (2020)’s textbook on data\nscience. The history of reverse-mode autodiff is reviewed by Griewank\n(2012).\nA tutorial on different perspectives of backpropagation is “There and\nBack Again: A Tale of Slopes and Expectations” (link), by Deisenroth\n\n1.5. Related work\n11\nand Ong. A tutorial on implicit differentiation is “Deep Implicit Layers -\nNeural ODEs, Deep Equilibirum Models, and Beyond” (link), by Kolter,\nDuvenaud, and Johnson.\nThe standard reference on inference in graphical models and its\nconnection with exponential families is that of Wainwright and Jor-\ndan (2008). Differential programming is also related to probabilistic\nprogramming; see, e.g., Meent et al. (2018).\nA review of smoothing from the infimal convolution perspective is\nprovided by Beck and Teboulle (2012). A standard textbook on convex\noptimization is that of Nesterov (2018). A textbook on first-order\noptimization methods is that of Beck (2017).\nAutodiff implementations that accelerated the autodiff revolution\nin machine learning are Theano (Bergstra et al., 2010) and Autograd\n(Maclaurin et al., 2015). Major modern implementations of autodiff\ninclude Tensorflow (Abadi et al., 2016), JAX (Bradbury et al., 2018),\nand PyTorch (Paszke et al., 2019). We in particular acknowledge the\nJAX team for influencing our view of autodiff.\n\nPart I\nFundamentals\n\n2\nDifferentiation\nIn this chapter, we review key differentiation concepts. In particular,\nwe emphasize on the fundamental role played by linear maps.\n2.1\nUnivariate functions\n2.1.1\nDerivatives\nTo study functions, such as defining their derivatives, we need to capture\ntheir infinitesimal variations around points as defined by the notion of\nlimit.\nDefinition 2.1 (Limit). A function f : R →R tends to c ∈R as\nits input v ∈R approaches w ∈R, if, for any ε > 0, there exists\nR > 0 such that for any v ∈R satisfying 0 < |v −w| ≤R, we have\n|f(v) −c| ≤ε. We say that c is the limit if f as v approaches w\nand denote it\nlim\nv→w f(w) = c.\nLimits are preserved under additions and multiplications. Namely, if\nlimv→w f(w) = c and limv→w g(w) = d, then, denoting (af + bg)(w) :=\naf(w) + bg(w) for any a, b ∈R and (fg)(w) := f(w)g(w), we have, by\n13\n\n14\nDifferentiation\ndefinition of the limit, limv→w(af +bg)(w) = ac+bd, limv→w(fg)(w) =\ncd. The preservation of the limit under additions and multiplications\nby a scalar is generally referred as the linearity of the limit, a property\nthat many definitions below inherit.\nWith the notion of limit we can already delineate a class of “well-\nbehaved” functions, that is, functions whose limits at any point equals\nto the value of the function at that point. Such a property defines\ncontinuous functions.\nDefinition 2.2 (Continuous function). A function f : R →R is\ncontinuous at a point w ∈R if\nlim\nv→w f(v) = f(w).\nA function f is said to be continuous if it is continuous at all points\nin its domain.\nAlthough the notion of continuity appears to be a benign assumption,\nseveral simple functions, such as the Heavyside step function (displayed\nin the left panel of Fig. 2.2), are not continuous and require special\ntreatment.\nRemark 2.1 (Landau’s notation). In the following, we use Landau’s\nlittle o notation. We write\ng(v) = o(f(v)) as v →w\nif\nlim\nv→w\n|g(v)|\n|f(v)| = 0.\nThat is, the function f dominates g in the limit v →w. For example,\nf is continuous at w if and only if\nf(w + δ) = f(w) + o(1) as δ →0.\nWe now explain derivatives. Consider a function f : R →R. As\nillustrated in Fig. 2.1, its value on an interval [w0, w0 + δ] can be\napproximated by the secant between its values f(w0) and f(w0 + δ),\na linear function with slope (f(w0 + δ) −f(w0))/δ. In the limit of an\n\n2.1. Univariate functions\n15\nFigure 2.1: A function f can be locally approximated around a point w0 by a secant,\na linear function w 7→aw+b with slope a and intercept b, crossing f at w0 with value\ny0 = f(w0) and crossing at w0 + δ with value uδ = f(w0 + δ). Using u0 = aw0 + b\nand uδ = a(w0 + δ) + b, we find that its slope is a = (f(w0 + δ) −f(w0))/δ and the\nintercept is b = f(w0) −aw0. The derivative f ′(w) of a function f at a point w0 is\nthen defined as the limit of the slope a when δ →0. It is the slope of the tangent\nof f at w0. The value f(w) of the function at w can then be locally approximated\naround w0 by w 7→f ′(w0)w + f(w0) −f ′(w0)w0 = f(w0) + f ′(w0)(w −w0).\ninfinitesimal variation δ around w0, the secant converges to the tangent\nof f at w0 and the resulting slope defines the derivative of f at w0. The\ndefinition below formalizes this intuition.\nDefinition 2.3 (Derivative). The derivative of f : R →R at w ∈\nR is defined as\nf′(w) := lim\nδ→0\nf(w + δ) −f(w)\nδ\n,\n(2.1)\nprovided that the limit exists. If f′(w) is well-defined at a particular\nw, we say that the function f is differentiable at w.\nHere, and in the following definitions, if f is differentiable at any\nw ∈R, we say that it is differentiable everywhere or differentiable for\nshort. If f is differentiable at a given w, then it is necessarily continuous\nat w as shown in the following proposition. Non-differentiability of a\ncontinuous function at a given point w is generally illustrated by a kink\nof this function at w as shown in Fig. 2.2.\n\n16\nDifferentiation\n2\n1\n0\n1\n2\n0.0\n0.5\n1.0\nDiscontinuous at 0\n2\n1\n0\n1\n2\n0.0\n0.5\n1.0\nContinuous\nnon-differentiable at 1 and -1\n2\n1\n0\n1\n2\n0.0\n0.5\n1.0 Differentiable everywhere\nFigure 2.2: Graphical representation of discontinuity or non-differentiability. A\ndiscontinuous function presents a jump in function values at a given point (left\npanel). A continuous but non-differentiable everywhere function presents kinks at the\npoints of non-differentiability (middle panel). A differentiable everywhere function is\nsmooth (right panel).\nProposition 2.1 (Differentiability implies continuity). If f : R →R\nis differentiable at w ∈R, then it is continuous at w ∈R.\nProof. In little o notation, f is differentiable at w if there exists f′(w) ∈\nR, such that\nf(w + δ) = f(w) + f′(w)δ + o(δ) as δ →0.\nSince f′(w)δ + o(δ) = o(1) as δ →0, f is continuous at w.\nIn addition to enabling the construction of a linear approximation\nof f in a neighborhood of w, since it is the slope of the tangent of f at\nw, the derivative f′ informs us about the monotonicity of f around\nw. If f′(w) is positive, the function is increasing around w. Conversely,\nif f′(w) is negative, the function is decreasing. Such information can be\nused to develop iterative algorithms seeking to minimize f by computing\niterates of the form wt+1 = wt −γf′(wt) for γ > 0, which move along\ndescent directions of f around wt.\nFor several elementary functions such as wn, ew, ln w, cos w or sin w,\ntheir derivatives can be obtained directly by applying the definition of\nthe derivative in Eq. (2.1) as illustrated in Example 2.1.\nExample 2.1 (Derivative of power function). Consider f(w) = wn\n\n2.1. Univariate functions\n17\nfor w ∈R, n ∈N \\ {0}. For any δ ∈R, we have\nf(w + δ) −f(w)\nδ\n= (w + δ)n −wn\nδ\n=\nPn\nk=0\n\u0000n\nk\n\u0001δkwn−k −wn\nδ\n=\nn\nX\nk=1\n \nn\nk\n!\nδk−1wn−k\n=\n \nn\n1\n!\nwn−1 +\nn\nX\nk=2\n \nn\nk\n!\nδk−1wn−k,\nwhere, in the second line, we used the binomial theorem. Since\n\u0000n\n1\n\u0001 = n and limδ→0\nPn\nk=2\n\u0000n\nk\n\u0001δk−1wn−k = 0, we get f′(w) = nwn−1.\nRemark 2.2 (Functions on a subset U of R). For simplicity, we pre-\nsented the definition of the derivative for a function defined on the\nwhole set of real numbers R. If a function f : U →R is defined on a\nsubset U ⊆R of the real numbers, as it is the case for f(w) = √w\ndefined on U = R+, the derivative of f at w ∈U is defined by\nthe limit in (2.1) provided that the function f is well defined on a\nneighborhood of w, that is, there exists r > 0 such that w + δ ∈U\nfor any |δ| ≤r. The function f is then said differentiable ev-\nerywhere or differentiable for short if it is differentiable at any\npoint w in the interior of U, the set of points w ∈U such that\n{w +δ : |δ| ≤r} ⊆U for r sufficiently small. For points lying at the\nboundary of U (such as a and b if U = [a, b]), one may define the\nright and left derivatives of f at a and b, meaning that the limit is\ntaken by approaching a from the right or b from the left.\n2.1.2\nCalculus rules\nFor a given w ∈R and two functions f : R →R and g : R →R, the\nderivative of elementary operations on f and g such as their sums,\nproducts or compositions can easily be derived from the definition of\nthe derivative, under appropriate conditions on the differentiability\nproperties of f and g at w. For example, if the derivatives of f and g\n\n18\nDifferentiation\nexist at w, then the derivatives of their weighted sum or product exist,\nand satisfy the rules\n∀a, b ∈R, (af + bg)′(w) = af′(w) + bg′(w)\n(Linearity)\n(fg)′(w) = f′(w)g(w) + f(w)g′(w),\n(Product rule)\nwhere (fg)(w) = f(w)g(w). The linearity can be verified directly from\nthe linearity of the limits. For the product rule, in little o notation, we\nhave, as δ →0,\n(fg)(w + δ) = (f(w) + f′(w)δ + o(δ))(g(w) + g′(w)δ + o(δ))\n= f(w)g(w) + f′(w)g(w)δ + f(w)g′(w)δ + o(δ),\nhence the result.\nIf the derivatives of g at w and of f at g(w) exist, then the derivative\nof the composition (f ◦g)(w) := f(g(w)) at w exists and is given by\n(f ◦g)′(w) = f′(g(w))g′(w).\n(Chain rule)\nWe prove this result more generally in Proposition 2.2. As seen in the\nsequel, the linearity and the product rule can be seen as byproducts of\nthe chain rule, making the chain rule the cornerstone of differentiation.\nConsider a function that can be expressed using sums, products or\ncompositions of elementary functions, such as f(w) = ew ln w + cos w2.\nIts derivative can be computed by applying the aforementioned rules\non the decomposition of f into elementary operations and functions, as\nillustrated in Example 2.2.\nExample 2.2 (Applying rules of differentiation). Consider f(w) =\new ln w + cos w2. The derivative of f on w > 0 can be computed\n\n2.1. Univariate functions\n19\nstep by step as follows, denoting sq(w) := w2,\nf′(w) = (exp · ln)′(w) + (cos ◦sq)′(w)\n(Linearity)\n(exp · ln)′(w) = exp′(w) · ln(w) + exp(w) · ln′(w)\n(Product rule)\n(cos ◦sq)′(w) = cos′(sq(w)) sq′(w)\n(Chain rule)\nexp′(w) = exp(w),\nln′(w) = 1/w,\n(Elem. func.)\nsq′(w) = 2w,\ncos′(w) = −sin(w).\n(Elem. func.)\nWe therefore obtain that f′(w) = ew ln w + ew/w −2w sin w2.\nSuch a process is purely mechanical and lends itself to an automated\nprocedure, which is the main idea of automatic differentiation presented\nin Chapter 8.\n2.1.3\nLeibniz’s notation\nThe notion of derivative was first introduced independently by Newton\nand Leibniz in the 18th century (Ball, 1960). The latter considered\nderivatives as the quotient of infinitesimal variations. Namely, denoting\nu = f(w) a variable depending on w through f, Leibniz considered the\nderivative of f as the quotient\nf′ = du\ndw\nwith\nf′(w) = du\ndw\n\f\f\f\f\nw\nwhere du and dw denote infinitesimal variations of u and w respectively\nand the symbol |w denotes the evaluation of the derivative at a given\npoint w. This notation simplifies the statement of the chain rule first\ndiscovered by Leibniz (Rodriguez and Lopez Fernandez, 2010) as we\nhave for v = g(w) and u = f(v)\ndu\ndw = du\ndv · dv\ndw.\nThis hints that derivatives are multiplied when considering compositions.\nAt evaluation, the chain rule in Leibniz notation recovers the formula\npresented above as\ndu\ndw\n\f\f\f\f\nw\n= du\ndv\n\f\f\f\f\ng(w)\ndv\ndw\n\f\f\f\f\nw\n= f′(g(w))g′(w) = (f ◦g)′(w).\n\n20\nDifferentiation\nThe ability of Leibniz’s notation to capture the chain rule as a mere\nproduct of quotients made it popular throughout the centuries, especially\nin mechanics (Ball, 1960). The rationale behind Leibniz’s notation,\nthat is, the concept of “infinitesimal variations” was questioned by\nlater mathematicians for its potential logical issues (Ball, 1960). The\nnotation f′(w) first introduced by Euler and further popularized by\nLagrange (Cajori, 1993) has then taken over in numerous mathematical\ntextbooks. The concept of infinitesimal variations has been rigorously\ndefined by considering the set of hyperreal numbers. They extend\nthe set of real numbers by considering each number as a sum of a\nnon-infinitesimal part and an infinitesimal part (Hewitt, 1948). The\nformalism of infinitesimal variations further underlies the development\nof automatic differentiation algorithms through the concept of dual\nnumbers.\n2.2\nMultivariate functions\n2.2.1\nDirectional derivatives\nLet us now consider a function f : RP →R with multi-dimensional\ninput w = (w1, . . . , wP ) ∈RP . The most important example in machine\nlearning is a function which, to the parameters w ∈RP of a neural\nnetwork, associates a loss value in R. Variations of f need to be defined\nalong specific directions, such as the variation f(w + δv)−f(w) of\nf around w ∈RP in the direction v ∈RP by an amount δ > 0.\nThis consideration naturally leads to the definition of the directional\nderivative.\nDefinition 2.4 (Directional derivative). The directional derivative\nof f at w in the direction v is given by\n∂f(w)[v] := lim\nδ→0\nf(w + δv) −f(w)\nδ\n,\nprovided that the limit exists.\nOne example of directional derivative consists in computing the\n\n2.2. Multivariate functions\n21\nderivative of a function f at w in any of the canonical directions\nei := (0, . . . , 0, 1\n|{z}\ni\n, 0, . . . , 0).\nThis allows us to define the notion of partial derivatives, denoted for\ni ∈[P]\n∂if(w) := ∂f(w)[ei] = lim\nδ→0\nf(w + δei) −f(w)\nδ\n.\nThis is also denoted in Leibniz’s notation as ∂if(w) = ∂f(w)\n∂wi\nor ∂if(w) =\n∂wif(w). By moving along only the ith coordinate of the function, the\npartial derivative is akin to using the function ϕ(ωi) = f(w1, . . . , ωi, . . . , wP )\naround ωi, letting all other coordinates fixed at their values wi.\n2.2.2\nGradients\nWe now introduce the gradient vector, which gathers the partial deriva-\ntives. We first recall the definitions of linear map and linear form.\nDefinition 2.5 (Linear map, linear form). A function l : RP →RM\nis a linear map if for any a1, a2 ∈R, v1, v2 ∈RD,\nl[a1v1 + a2v2] = a1l(v1) + a2l[v2].\nA linear map with values in R, l : RP →R, is called a linear form.\nLinearity plays a crucial role in the differentiability of a function.\nDefinition 2.6 (Differentiability, single-output case). A function f :\nRP →R is differentiable at w ∈RP if its directional derivative\nis defined along any direction, linear in any direction, and if\nlim\n∥v∥2→0\n|f(w + v) −f(w) −∂f(w)[v]|\n∥v∥2\n= 0.\nWe can now introduce the gradient.\nDefinition 2.7 (Gradient). The gradient of a differentiable func-\ntion f : RP →R at a point w ∈RP is defined as the vector of\n\n22\nDifferentiation\npartial derivatives\n∇f(w) :=\n\n\n\n\n∂1f(w)\n...\n∂P f(w)\n\n\n\n=\n\n\n\n\n∂f(w)[e1]\n...\n∂f(w)[eP ]\n\n\n\n.\nBy linearity, the directional derivative of f at w in the direction\nv = PP\ni=1 viei is then given by\n∂f(w)[v] =\nP\nX\ni=1\nvi∂f(w)[ei] = ⟨v, ∇f(w)⟩.\nHere, ⟨·, ·⟩denotes the inner product. We provide its definition in\nEuclidean spaces in Section 2.3.2.\nIn the definition above, the fact that the gradient can be used to\ncompute the directional derivative is a mere consequence of linearity.\nHowever, in more abstract cases presented in later sections, the gradient\nis defined through this property.\nAs a simple example, any linear function of the form f(w) =\n⟨a, w⟩= PP\ni=1 aiwi is differentiable as we have (⟨a, w + v⟩−⟨a, w⟩−\n⟨a, v⟩)/∥v∥2 = 0 for any v and in particular for ∥v∥→0. Moreover, its\ngradient is naturally given by ∇f(w) = a.\nGenerally, to show that a function is differentiable and find its\ngradient, one approach is to approximate f(w + v) around v = 0. If we\ncan find a vector g such that\nf(w + v) = f(w) + ⟨g, v⟩+ o(∥v∥2),\nthen f is differentiable at w since ⟨g, ·⟩is linear. Moreover, g is then\nthe gradient of f at w.\nRemark 2.3 (Gateaux and Fréchet differentiability). Multiple defini-\ntions of differentiability exist. The one presented in Definition 2.6 is\nabout Fréchet differentiable functions. Alternatively, if f : RP →\nR has well-defined directional derivatives along any directions then\nthe function is Gateaux differentiable. Note that the existence of\ndirectional derivatives in any directions is not a sufficient condition\nfor the function to be differentiable. In other words, any Fréchet\n\n2.2. Multivariate functions\n23\ndifferentiable function is Gateaux differentiable, but the converse\nis not true. As a counter-example, one can verify that the function\nf(x1, x2) = x3\n1/(x2\n1 + x2\n2) is Gateaux differentiable at 0 but not\n(Fréchet) differentiable at 0 (because the directional derivative at 0\nis not linear).\nSome authors also require Gateaux differentiable functions to\nhave linear directional derivatives along any direction. These are\nstill not Fréchet differentiable functions. Indeed, the limit in Defini-\ntion 2.6 is over any vectors tending to 0 (potentially in a pathological\nway), while directional derivatives look at such limits uniquely in\nterms of a single direction.\nIn the remainder of this chapter, all definitions of differentiability\nare in terms of Fréchet differentiability.\nExample 2.3 illustrates how to compute the gradient of the logistic\nloss and validate its differentiability.\nExample 2.3 (Gradient of logistic loss). Consider the logistic loss\nℓ(θ, y) := −⟨y, θ⟩+log PM\ni=1 eθi, that measures the prediction error\nof the logits θ ∈RM w.r.t. the correct label y ∈{e1, . . . , eM}. Let\nus compute the gradient of this loss w.r.t. θ for fixed y, i.e., we\nwant to compute the gradient of f(θ) := ℓ(θ, y). Let us decompose\nf as f = l + logsumexp with l(θ) := ⟨−y, θ⟩and\nlogsumexp(θ) := log\nM\nX\ni=1\nexp(θi),\nthe log-sum-exp function. The function l is linear so differentiable\nwith gradient ∇l(θ) = −y. We therefore focus on logsumexp.\nDenoting exp(θ) = (exp(θ1), . . . , exp(θM)), using that exp(x) =\n1 + x + o(x), log(1 + x) = x + o(x), and denoting ⊙the elementwise\n\n24\nDifferentiation\nproduct, we get\nlogsumexp(θ + v) = log (⟨exp(θ + v), 1⟩)\n= log (⟨exp(θ) ⊙exp(v), 1⟩)\n= log (⟨exp(θ) ⊙(1 +v + o(∥v∥2)), 1⟩)\n= log (⟨exp(θ), 1⟩+ ⟨exp(θ), v⟩+ o(∥v∥2))\n= log (⟨exp(θ), 1⟩) +\n\u001c\nexp(θ)\n⟨exp(θ), 1⟩, v\n\u001d\n+ o(∥v∥2),\nThe above decomposition of logsumexp(θ + v) shows that it is\ndifferentiable, and that ∇logsumexp(θ) = softargmax(θ), where\nsoftargmax(θ) :=\n\neθ1/\n\n\nM\nX\nj=1\neθj\n\n, . . . , eθM /\n\n\nM\nX\nj=1\neθj\n\n\n\n.\nIn total, we then get that ∇f(θ) = −y + softargmax(θ).\nLinearity of gradients\nThe notion of differentiability for multi-input functions naturally inherits\nfrom the linearity of derivatives for single-input functions. For any\nu1, . . . , uM ∈R and any multi-input functions f1, . . . , fM differentiable\nat w, the function u1f1 + . . . + uMfM is differentiable at w and its\ngradient is\n∇(u1f1 + . . . + uMfM)(w) = u1∇f1(w) + . . . + uM∇fM(w).\nWhy is the gradient useful?\nThe gradient defines the steepest ascent direction of f from w. To see\nwhy, we note that\narg max\nv∈RP ,∥v∥2≤1\n∂f(w)[v] =\narg max\nv∈RP ,∥v∥2≤1\n⟨v, ∇f(w)⟩= ∇f(w)/∥∇f(w)∥2,\nwhere we assumed ∇f(w) ̸= 0. The gradient ∇f(w) is orthogonal to\nthe level set of the function (the set of points w sharing the same\nvalue f(w)) and points towards higher values of f as illustrated in\nFig. 2.3. Conversely, the negative gradient −∇f(w) points towards lower\n\n2.2. Multivariate functions\n25\nFigure 2.3: The gradient of a function\nf : R2 →R at (w1, w2) is the normal\nvector to the tangent space of the level\nset Lf(w1,w2) = {(w′\n1, w′\n2) : f(w′\n1, w′\n2) =\nf(w1, w2)} and points towards points\nwith higher function values.\nFigure 2.4: The directional derivative\nof a parametric curve f : R →R2 at w\nis the tangent to the curve at the point\nf(w) ∈R2.\nvalues of f. This observation motivates the development of optimization\nalgorithms such as gradient descent. It is based on iteratively performing\nthe update wt+1 = wt −γ∇f(wt), for γ > 0. It therefore seeks for a\nminimizer of f by moving along the steepest descent direction around\nwt given, up to a multiplicative factor, by −∇f(wt).\n2.2.3\nJacobians\nLet us now consider a multi-output function f : RP →RM defined by\nf(w) := (f1(w), . . . , fM(w)), where fj : RP →R. A typical example\nin machine learning is a neural network. The notion of directional\nderivative can be extended to such function by defining it as the vector\ncomposed of the coordinate-wise directional derivatives:\n∂f(w)[v] := lim\nδ→0\nf(w + δv) −f(w)\nδ\n= lim\nδ→0\n\n\n\n\nf1(w+δv)−f1(w)\nδ...\nfM(w+δv)−fM(w)\nδ\n\n\n\n∈RM,\nwhere the limits (provided that they exist) are applied coordinate-wise.\nThe directional derivative of f in the direction v ∈RP is therefore the\nvector that gathers the directional derivative of each fj, i.e., ∂f(w)[v] =\n(∂fj(w)[v])M\nj=1. In particular, we can define the partial derivatives of\n\n26\nDifferentiation\nf at w as the vectors\n∂if(w) := ∂f(w)[ei] =\n\n\n\n\n∂if1(w)\n...\n∂ifM(w)\n\n\n\n∈RM.\nAs for the usual definition of the derivative, the directional derivative\ncan provide a linear approximation of a function around a current input\nas illustrated in Fig. 2.4 for a parametric curve f : R →R2.\nJust as in the single-output case, differentiability is defined not only\nas the existence of directional derivatives in any direction but also by\nthe linearity in the chosen direction.\nDefinition 2.8 (Differentiability, multi-output case). A function f :\nRP →RM is (Fréchet) differentiable at a point w ∈RP if its\ndirectional derivative is defined along any directions, linear along\nany directions, and,\nlim\n∥v∥2→0\n∥f(w + v) −f(w) −∂f(w)[v]∥2\n∥v∥2\n= 0.\nThe partial derivatives of each coordinate’s function are gathered in\nthe Jacobian matrix.\nDefinition 2.9 (Jacobian). The Jacobian of a differentiable func-\ntion f : RP →RM at w is defined as the matrix gathering partial\nderivatives of each coordinate’s function provided they exist,\n∂f(w) :=\n\n\n\n\n∂1f1(w)\n. . .\n∂P f1(w)\n...\n...\n...\n∂1fM(w)\n. . .\n∂P fM(w)\n\n\n\n∈RM×P .\nThe Jacobian can be represented by stacking columns of partial\nderivatives or rows of gradients,\n∂f(w) =\n\u0010\n∂1f(w), . . . , ∂P f(w)\n\u0011\n=\n\n\n\n\n∇f1(w)⊤\n...\n∇fM(w)⊤\n\n\n\n.\n\n2.2. Multivariate functions\n27\nBy linearity, the directional derivative of f at w along any input\ndirection v = PP\ni=1 viei ∈RP is then given by\n∂f(w)[v] =\nP\nX\ni=1\nvi∂if(w) = ∂f(w)v ∈RM.\nNotice that we use bold ∂to indicate the Jacobian matrix. The\nJacobian matrix naturally generalizes the concepts of derivatives and\ngradients presented earlier. As for the single input case, to show that\na function is differentiable, one approach is to approximate f(w + v)\naround v = 0. If we find a linear map l such that\nf(w + v) = f(w) + l[v] + o(∥v∥2),\nthen f is differentiable at w. Moreover, if l is represented by matrix J\nsuch that l[v] = Jv then J = ∂f(w).\nAs a simple example, any linear function f(w) = Aw for A ∈RM×P\nis differentiable, since all its coordinate-wise components are single-\noutput linear functions, and the Jacobian of f at any w is given by\n∂f(w) = A.\nRemark 2.4 (Special cases of the Jacobian). For single-output func-\ntions f : RP →R, i.e., M = 1, the Jacobian matrix reduces to a\nrow vector identified as the transpose of the gradient, i.e.,\n∂f(w) = ∇f(w)⊤∈R1×P .\nFor a single-input function f : R →RM, the Jacobian reduces to a\nsingle column vector of directional derivatives, denoted\n∂f(w) = f′(w) :=\n\n\n\n\nf′\n1(w)\n...\nf′\nM(w)\n\n\n\n∈RM×1.\nFor a single-input single-output function f : R →R, the Jacobian\nreduces to the derivative of f, i.e.,\n∂f(w) = f′(w) ∈R.\n\n28\nDifferentiation\nExample 2.4 illustrates the form of the Jacobian matrix for the\nelement-wise application of a differentiable function such as the softplus\nactivation. This example already shows that the Jacobian takes a simple\ndiagonal matrix form. As a consequence, the directional derivative\nassociated with this function is simply given by an element-wise product\nrather than a full matrix-vector product as suggested in Definition 2.9.\nWe will revisit this point in Section 2.3.\nExample 2.4 (Jacobian matrix of the softplus activation). Consider\nthe element-wise application of the softplus defined for w ∈RP by\nf(w) :=\n\n\n\n\nσ(w1)\n...\nσ(wP )\n\n\n\n∈RP\nwhere\nσ(w) := log(1 + ew).\nSince σ is differentiable, each coordinate of this function is differen-\ntiable and the overall function is differentiable. The jth coordinate of\nf is independent of the ith coordinate of w for i ̸= j, so ∂ifj(w) = 0\nfor i ̸= j. For i = j, the result boils down to the derivative of σ\nat wj. That is, ∂jfj(w) = σ′(wj), where σ′(w) = ew/(1 + ew). The\nJacobian of f is therefore a diagonal matrix\n∂f(w) = diag(σ′(w1), . . . , σ′(wP )) :=\n\n\n\n\n\n\n\nσ′(w1)\n0\n. . .\n0\n0\n...\n...\n...\n...\n...\n...\n0\n0\n. . .\n0\nσ′(wP )\n\n\n\n\n\n\n\n.\nVariations along outputs\nRather than considering variations of f along an input direction v ∈RP ,\nwe may also consider the variations of f along an output direction\nu ∈RM, namely, computing the gradient ∇⟨u, f⟩(w) of the single-\noutput function\n⟨u, f⟩(w) := ⟨u, f(w)⟩∈R.\nIn particular, we may consider computing the gradients ∇fj(w) of each\nfunction coordinate fj = e⊤\nj f at w, where ej is the jth canonical vector\n\n2.2. Multivariate functions\n29\nin RM. The infinitesimal variations of f at w along any output direction\nu = PM\nj=1 ujej ∈RM are given by\n∇⟨u, f⟩(w) =\nM\nX\nj=1\nuj∇fj(w) = ∂f(w)⊤u ∈RP ,\nwhere ∂f(w)⊤is the Jacobian’s transpose. Using the definition of\nderivative as a limit, we obtain for i ∈[P]\n∇i⟨u, f⟩(w) = [∂f(w)⊤u]i = lim\nδ→0\n⟨u, f(w + δei) −f(w)⟩\nδ\n.\nChain rule\nEquipped with a generic definition of differentiability and the associated\nobjects, gradients and Jacobians, we can now generalize the chain rule,\npreviously introduced for single-input single-output functions.\nProposition 2.2 (Chain rule). Consider f : RP →RM and g :\nRM →RR. If f is differentiable at w ∈RP and g is differen-\ntiable at f(w) ∈RM, then the composition g ◦f is differentiable\nat w ∈RP and its Jacobian is given by\n∂(g ◦f)(w) = ∂g(f(w))∂f(w).\nProof. We progressively approximate g ◦f(w + v) using the differentia-\nbility of f at w and g at f(w),\ng(f(w + v)) = g(f(w) + ∂f(w)v + o(∥v∥))\n= g(f(w)) + ∂g(f(w))∂f(w)v + o(∥v∥).\nHence, g ◦f is differentiable at w with Jacobian ∂g(f(w))∂f(w).\nProposition 2.2 can be seen as the cornerstone of any derivative\ncomputations. For example, it can be used to rederive the linearity or the\nproduct rule associated to the derivatives of single-input single-outptut\nfunctions.\nWhen g is scalar-valued, combined with Remark 2.4, we obtain a\nsimple expression for ∇(g ◦f).\n\n30\nDifferentiation\nProposition 2.3 (Chain rule, scalar-valued case). Consider f : RP →\nRM and g : RM →R. The gradient of the composition is given by\n∇(g ◦f)(w) = ∂f(w)⊤∇g(f(w)).\nThis is illustrated with linear regression in Example 2.5.\nExample 2.5 (Linear regression). Consider the squared residuals of\na linear regression of N inputs x1, . . . , xN ∈RD onto N targets\ny1, . . . , yN ∈R with a vector w ∈RD, that is, f(w) = ∥Xw −\ny∥2\n2 = PN\ni=1(⟨xi, w⟩−yi)2 for X = (x1, . . . , xN)⊤∈RN×D and\ny = (y1, . . . , yN)⊤∈RN.\nThe function f can be decomposed into a linear mapping\nf1(w) = Xw and a squared error f2(p) = ∥p −y∥2\n2, so that\nf = f2 ◦f1. We can then apply the chain rule in Proposition 2.3 to\nget\n∇f(w) = ∂f1(w)⊤∇f2(f1(w))\nprovided that f1, f2 are differentiable at w and f1(w), respectively.\nThe function f1 is linear so differentiable with Jacobian ∂f1(w) =\nX. On the other hand the partial derivatives of f2 are given by\n∂jf2(p) = 2(pj −yj) for j ∈{1, . . . , N}. Therefore, f2 is differen-\ntiable at any p and its gradient is ∇f2(p) = 2(p−y). By combining\nthe computations of the Jacobian of f1 and the gradient of f2, we\nthen get the gradient of f as\n∇f(w) = 2X⊤(f1(w) −y) = 2X⊤(Xw −y).\n2.3\nLinear differentiation maps\nThe Jacobian matrix is useful as a representation of the partial deriva-\ntives. However, the core idea underlying the definition of differentiable\nfunctions, as well as their implementation in an autodiff framework,\nlies in the access to two key linear maps. These two maps encode in-\nfinitesimal variations along input or output directions and are referred\nto, respectively, as Jacobian-vector product (JVP) and Vector-\njacobian product (VJP). This section formalizes these notions, in the\ncontext of Euclidean spaces.\n\n2.3. Linear differentiation maps\n31\n2.3.1\nThe need for linear maps\nSo far, we have focused on functions f : RP →RM, that take a vector as\ninput and produce a vector as output. However, functions that use matrix\nor even tensor inputs/outputs are common place in neural networks. For\nexample, consider the function of matrices of the form f(W ) = W x,\nwhere x ∈RD and W ∈RM×D. This function takes a matrix as input,\nnot a vector. Of course, a matrix W ∈RM×D can always be “flattened”\ninto a vector w ∈RMD, by stacking the columns of W . We denote this\noperation by w = vec(W ) and its inverse by W = vec−1(w). We can\nthen equivalently write f(W ) as ˜f(w) = f(vec−1(w)) = vec−1(w)x,\nso that the previous framework applies. However, we will now see that\nthis would be inefficient.\nIndeed, the resulting Jacobian of ˜f at any w consists in a matrix\nof size RM×MD, which, after some computations, can be observed to\nbe mostly filled with zeros. Getting the directional derivative of f at\nW ∈RM×D in a direction V ∈RM×D would consist in (i) vectorizing\nV into v = vec(V ), (ii) computing the matrix-vector product ∂˜f(w)v\nat a cost of M3D2 computations (ignoring the fact that the Jacobian\nhas zero entries), (iii) re-shaping the result into a matrix.\nOn the other hand, since f is linear in its matrix input, we can\ninfer that the directional derivative of f at any W ∈RM×D in any\ndirection V ∈RM×D is simply given by the function itself applied on\nV . Namely, we have ∂f(W )[V ] = f(V ) = V x, which is simple to\nimplement and clearly only requires MD operations. Note that the\ncost would have been the same, had we ignored the non-zero entries of\n∂˜f(w). The point here is that by considering the operations associated\nto the differentiation of a function as linear maps rather than using the\nassociated representation as a Jacobian matrix, we can streamline the\nassociated implementations and exploit the structures of the underlying\ninput or output space. To that end, we now recall the main abstractions\nnecessary to extend the previous definitions in the context of Euclidean\nspaces.\n\n32\nDifferentiation\n2.3.2\nEuclidean spaces\nLinear spaces, a.k.a. vector spaces, are spaces equipped (and closed\nunder) an addition rule compatible with multiplication by a scalar\n(we limit ourselves to the field of reals). Namely, in a linear space E,\nthere exists the operations + and ·, such that for any u, v ∈E, and\na ∈R, we have u + v ∈E and a · u ∈E. Euclidean spaces are linear\nspaces equipped with a basis e1, . . . , eP ∈E. Any element v ∈E can be\ndecomposed as v = PP\ni=1 viei for some unique scalars v1, . . . , vP ∈R. A\ncanonical example of Euclidean space is the set RP of all vectors of size\nP that we already covered. The set of matrices RP1×P2 of size P1 × P2\nis also naturally a Euclidean space generated by the set of canonical\nmatrices Eij ∈{0, 1}P1×P2 for i ∈[P1], j ∈[P2] filled with zero except\nat the (i, j)th entry filled with one. For example, W ∈RP1×P2 can be\nwritten W = PP1,P2\ni,j=1 WijEij.\nEuclidean spaces are naturally equipped with a notion of inner\nproduct defined below.\nDefinition 2.10 (Inner product). An inner product on a linear\nspace E is a function ⟨·, ·⟩: E × E →R that is\n• bilinear: x 7→⟨x, w⟩and y 7→⟨v, y⟩are linear for any w, v ∈\nE,\n• symmetric: ⟨w, v⟩= ⟨v, w⟩for any w, v ∈E,\n• positive definite: ⟨w, w⟩≥0 for any w ∈E, and ⟨w, w⟩= 0\nif and only if w = 0.\nAn inner product defines a norm ∥w∥:=\np\n⟨w, w⟩.\nThe norm induced by an inner product defines a distance ∥w −v∥\nbetween w, v ∈E, and therefore a notion of convergence.\nFor vectors, where E = RP , the inner product is the usual one\n⟨w, v⟩= PP\ni=1 wivi. For matrices, where E = RP1×P2, the inner product\nis the so-called Frobenius inner product. It is defined for any W , V ∈\n\n2.3. Linear differentiation maps\n33\nRP1×P2 by\n⟨W , V ⟩:= ⟨vec(W ), vec(V )⟩=\nP1,P2\nX\ni,j=1\nWijVij = tr(W ⊤V ),\nwhere tr(Z) := PP\ni=1 Zii is the trace operator defined for square matrices\nZ ∈RP×P . For tensors of order R, which generalize matrices to E =\nRP1×...×PR, the inner product is defined similarly for W, V ∈RP1×...×PR\nby\n⟨W, V⟩:= ⟨vec(W), vec(V)⟩=\nP1,...,PR\nX\ni1,...,iR=1\nWi1...iRVi1...iR,\nwhere Wi1...iR is the (i1, . . . , iR)th entry of W.\n2.3.3\nLinear maps and their adjoints\nThe notion of linear map defined in Definition 2.5 naturally extends\nto Euclidean spaces. Namely, a function l : E →F from a Euclidean\nspace E onto a Euclidean space F is a linear map if for any w, v ∈E\nand a, b ∈R, we have l[aw + bv] = a · l[w] + b · l[v]. When E = RP and\nF = RM, there always exists a matrix A ∈RM×P such that l[x] = Ax.\nTherefore, we can think of A as the “materialization” of l.\nWe can define the adjoint operator of a linear map.\nDefinition 2.11 (Adjoint operator). Given two Euclidean spaces E\nand F equipped with inner products ⟨·, ·⟩E and ⟨·, ·⟩F, the adjoint\nof a linear map l : E →F is the unique linear map l∗: F →E such\nthat for any v ∈E and u ∈F,\n⟨l[v], u⟩F = ⟨v, l∗[u]⟩E.\nThe adjoint can be thought as the counterpart of the matrix trans-\npose for linear maps. When l[v] = Av, we have l∗[u] = A⊤u since\n⟨l[v], u⟩F = ⟨Av, u⟩F = ⟨v, A⊤u⟩E = ⟨v, l∗[u]⟩E.\n2.3.4\nJacobian-vector products\nWe now define the directional derivative using linear maps, leading\nto the notion of Jacobian-vector product (JVP). This can be used to\n\n34\nDifferentiation\nfacilitate the treatment of functions on matrices or be used for further\nextensions to infinite-dimensional spaces. In the following, E and F\ndenote two Euclidean spaces equipped with inner products ⟨·, ·⟩E and\n⟨·, ·⟩F. We start by defining differentiability in general Euclidean spaces.\nDefinition 2.12 (Differentiability in Euclidean spaces). A function f :\nE →F is differentiable at a point w ∈E if the directional\nderivative along v ∈E\n∂f(w)[v] := lim\nδ→0\nf(w + δv) −f(w)\nδ\nis well-defined for any v ∈E, linear in v and if\nlim\n∥v∥F→0\n∥f(w + v) −f(w) −∂f(w)[v]∥F\n∥v∥F\n= 0.\nWe can now formally define the Jacobian-vector product.\nDefinition 2.13 (Jacobian-vector product). For a differentiable func-\ntion f : E →F, the linear map ∂f(w) : E →F, mapping v to\n∂f(w)[v] is called the Jacobian-vector product (JVP) by anal-\nogy with Definition 2.9. The function ∂f is a function from E to a\nlinear map from E to F. That is, we have\n∂f : E →(E →F).\nStrictly speaking, v belongs to E. Therefore it may not be a vector,\nif for instance E is the set of real matrices. We adopt the name JVP, as\nit is now largely adopted.\nRecovering the gradient\nPreviously, we saw that for differentiable functions with vector input\nand scalar output, the directional derivative is equal to the inner prod-\nuct between the direction and the gradient. The same applies when\nconsidering differentiable functions from a Euclidean space with single\noutputs, except that the gradient is now an element of the input space\nand the inner product is the one associated with the input space.\n\n2.3. Linear differentiation maps\n35\nProposition 2.4 (Gradient). If a function f : E →R is differen-\ntiable at w ∈E, then there exists ∇f(w) ∈E, called the gradient\nof f at w such that the directional derivative of f at w along any\ninput direction v ∈E is given by\n∂f(w)[v] = ⟨∇f(w), v⟩E.\nIn Euclidean spaces, the existence of the gradient can simply be\nshown by decomposing the partial derivative along a basis of E. Such a\ndefinition generalizes to infinite-dimensional (e.g., Hilbert spaces) spaces\nas seen in Section 2.3.9.\n2.3.5\nVector-Jacobian products\nFor functions with vector input and vector output, we already discussed\ninfinitesimal variations along output directions. The same approach\napplies for Euclidean spaces and is tied to the adjoint of the JVP as\ndetailed in Proposition 2.5.\nProposition 2.5 (Vector-Jacobian product). If a function f : E →\nF is differentiable at w ∈E, then its infinitesimal variation along an\noutput direction u ∈F is given by the adjoint map ∂f(w)∗: F →\nE of the JVP, called the vector-Jacobian product (VJP). It\nsatisfies\n∇⟨u, f⟩F(w) = ∂f(w)∗[u],\nwhere we denoted ⟨u, f⟩F(w) := ⟨u, f(w)⟩F. The function ∂f(·)∗\nis a function from E to a linear map from F to E. That is, we have\n∂f(·)∗: E →(F →E).\nProof. The chain rule presented in Proposition 2.2 naturally generalizes\nto Euclidean spaces (see Proposition 2.6). Since ⟨u, ·⟩is linear, its\ndirectional derivative is itself. Therefore, the directional derivative of\n⟨u, f⟩F is\n∂(⟨u, f⟩F)(w)[v] = ⟨u, ∂f(w)[v]⟩F\n= ⟨∂f(w)∗[u], v⟩F.\n\n36\nDifferentiation\nAs this is true for any v ∈E, ∂f(w)∗[u] is the gradient of ⟨u, f⟩F per\nProposition 2.4.\n2.3.6\nChain rule\nThe chain rule presented before in terms of Jacobian matrices can\nreadily be formulated in terms of linear maps to take advantage of the\nimplementations of the JVP and VJP as linear maps.\nProposition 2.6 (Chain rule, general case). Consider f : E →F\nand g : F →G for E, F, G some Euclidean spaces. If f is differ-\nentiable at w ∈E and g is differentiable at f(w) ∈F, then the\ncomposition g ◦f is differentiable at w ∈E. Its JVP is given by\n∂(g ◦f)(w)[v] = ∂g(f(w))[∂f(w)[v]]\nand its VJP is given by\n∂(g ◦f)(w)∗[u] = ∂f(w)∗[∂g(f(w))∗[u]].\nThe proof follows the one of Proposition 2.2. When the last function\nis scalar-valued, which is often the case in machine learning, we obtain\nthe following simplified result.\nProposition 2.7 (Chain rule, scalar case). Consider f : E →F and\ng : F →R, the gradient of the composition is given by\n∇(g ◦f)(w) = ∂f(w)∗[∇g(f(w))].\n2.3.7\nFunctions of multiple inputs (fan-in)\nOftentimes, the inputs of a function do not belong to only one Euclidean\nspace but to a product of them. An example is f(x, W ) = W x, which\nis defined on E = RD × RM×D. In such a case, it is convenient to\ngeneralize the notion of partial derivatives to handle blocks of inputs.\nConsider a function f(w1, . . . , wS) defined on E = E1 × . . . × ES,\nwhere wi ∈Ei. We denote the partial derivative with respect to the\nith input wi along vi ∈Ei as ∂if(w1, . . . , wS)[vi]. Equipped with this\n\n2.3. Linear differentiation maps\n37\nnotation, we can analyze how JVPs or VJPs are decomposed along\nseveral inputs.\nProposition 2.8 (Multiple inputs). Consider a differentiable func-\ntion of the form f(w) = f(w1, . . . , wS) with signature f : E →F,\nwhere w := (w1, . . . , wS) ∈E and E := E1 × · · · × ES. Then the\nJVP with the input direction v = (v1, . . . , vS) ∈E is given by\n∂f(w)[v] = ∂f(w1, . . . , wS)[v1, . . . , vS] ∈F\n=\nS\nX\ni=1\n∂if(w1, . . . , wS)[vi].\nThe VJP with the output direction u ∈F is given by\n∂f(w)∗[u] = ∂f(w1, . . . , wS)∗[u] ∈E\n= (∂1f(w1, . . . , wS)∗[u], . . . , ∂Sf(w1, . . . , wS)∗[u]).\nExample 2.6 (Matrix-vector product). Consider f(x, W ) = W x,\nwhere W ∈RM×D and x ∈RD. This corresponds to setting\nE = E1 × E2 = RD × RM×D and F = RM. For the JVP, letting\nv ∈RD and V ∈RM×D, we obtain\n∂f(x, W )[v, V ] = W v + V x ∈F.\nWe can also access the individual JVPs as\n∂1f(x, W )[v] = W v ∈F,\n∂2f(x, W )[V ] = V x ∈F.\nFor the VJP, letting u ∈RM, we obtain\n∂f(x, W )∗[u] = (W ⊤u, ux⊤) ∈E.\nWe can access the individual VJPs by\n∂1f(x, W )∗[u] = W ⊤u ∈E1,\n∂2f(x, W )∗[u] = ux⊤∈E2.\n\n38\nDifferentiation\nRemark 2.5 (Nested inputs). It is sometimes convenient to group\ninputs into meaningful parts. For instance, if the input is naturally\nbroken down into two parts x = (x1, x2), where x1 is a text\npart and x2 is an image part, and the network parameters are\nnaturally grouped into three layers w = (w1, w2, w3), we can write\nf(x, w) = f((x1, x2), (w1, w2, w3)). This is mostly a convenience\nand we can again reduce it to a function of a single input, thanks\nto the linear map perspective in Euclidean spaces.\nRemark 2.6 (Hiding away inputs). It will often be convenient to\nignore inputs when differentiating. We use the semicolon for this\npurpose. For instance, a function of the form L(w; x, y) (notice\nthe semicolon) has signature L: W →R because we treat x and\ny as constants. Therefore, the gradient is ∇L(w; x, y) ∈W. On\nthe other hand, the function L(w, x, y) (notice the comma) has\nsignature L: W × X × Y →R so its gradient is ∇L(w, x, y) ∈\nW × X × Y. If we need to access partial gradients, we use indexing,\ne.g., ∇1L(w, x, y) ∈W or ∇wL(w, x, y) ∈W when there is no\nambiguity.\n2.3.8\nFunctions with multiple outputs (fan-out)\nSimilarly, it is often convenient to deal with functions that have multiple\noutputs.\nProposition 2.9 (Multiple outputs). Consider a differentiable func-\ntion of the form f(w) = (f1(w), . . . , fT (w)), with signatures f : E →\nF and fi : E →Fi, where F := F1 × · · · × FT . Then the JVP with\nthe input direction v ∈E is given by\n∂f(w)[v] = (∂f1(w)[v], . . . , ∂fT (w)[v]) ∈F.\n\n2.3. Linear differentiation maps\n39\nThe VJP with the output direction u = (u1, . . . , uT ) ∈F is\n∂f(w)∗[u] = ∂f(w)∗[u1, . . . , uT ] ∈E\n=\nT\nX\ni=1\n∂fi(w)∗[ui].\nCombined with the chain rule, we obtain that the Jacobian of\nh(w) := g(f(w)) = g(f1(w), . . . , fT (w))\nis ∂h(w) = PT\ni=1 ∂if(g(w)) ◦∂gi(w) and therefore the JVP is\n∂h(w)[v] =\nT\nX\ni=1\n∂if(g(w))[∂gi(w)[v]].\n2.3.9\nExtensions to non-Euclidean linear spaces\nWe focused on Euclidean spaces, i.e., linear spaces with a finite basis.\nHowever, the notions introduced earlier can be defined in more generic\nspaces.\nFor example, directional derivatives (see Definition 2.12) can\nbe defined in any linear space equipped with a norm and complete\nwith respect to this norm. Such spaces are called Banach spaces.\nCompleteness is a technical assumption that requires that any Cauchy\nsequence converges (a Cauchy sequence is a sequence whose elements\nbecome arbitrarily close to each other as the sequence progresses). A\nfunction f : E →F defined from a Banach space E onto a Banach space\nF is then called Gateaux differentiable if its directional derivative is\ndefined along any direction (where limits are defined w.r.t. the norm in\nF). Some authors also require the directional derivative to be linear to\ndefine a Gateaux differentiable function.\nFréchet differentiability can also naturally be generalized to\nBanach spaces. The only difference is that, in generic Banach spaces,\nthe linear map l satisfying Definition 2.12 must be continuous, i.e., there\nmust exist C > 0, such that l[v] ≤C∥v∥, where ∥· ∥is the norm in the\nBanach space E.\nThe definitions of gradient and VJPs require in addition a notion of\ninner product. They can be defined in Hilbert spaces, that is, linear\n\n40\nDifferentiation\nspaces equipped with an inner product and complete with respect to\nthe norm induced by the inner product (they could also be defined\nin a Banach space by considering operations in the dual space, see,\ne.g. (Clarke et al., 2008)). The existence of the gradient is ensured by\nRiesz’s representation theorem which states that any continuous\nlinear form in a Hilbert space can be represented by the inner product\nwith a vector. Since for a differentiable function f : E →R, the JVP\n∂f(w) : E →R is a linear form, Riesz’s representation theorem ensures\nthe existence of the gradient as the element g ∈E such that ∂f(w)v =\n⟨g, v⟩for any v ∈E. The VJP is also well-defined as the adjoint of the\nJVP w.r.t. the inner product of the Hilbert space.\nAs an example, the space of squared integrable functions on R is a\nHilbert space equipped with the inner product ⟨a, b⟩:=\nR a(x)b(x)dx.\nHere, we cannot find a finite number of functions that can express all\npossible functions on R. Therefore, this space is not a mere Euclidean\nspace. Nevertheless, we can consider functions on this Hilbert space\n(called functionals to distinguish them from the elements of the space).\nThe associated directional derivatives and gradients, can be defined\nand are called respectively, functional derivative and functional\ngradient, see, e.g., Frigyik et al. (2008) and references therein.\n2.4\nSecond-order differentiation\n2.4.1\nSecond derivatives\nFor a single-input, single-output differentiable function f : R →R,\nits derivative at any point is itself a function f′ : R →R. We may\nthen consider the derivative of the derivative at any point: the second\nderivative.\nDefinition 2.14 (Second derivative). The second derivative f(2)(w)\nof a differentiable function f : R →R at w ∈R is defined as the\nderivative of f′ at w, that is,\nf(2)(w) := lim\nδ→0\nf′(w + δ) −f′(w)\nδ\n,\n\n2.4. Second-order differentiation\n41\nFigure 2.5: Points at which the second derivative is small are points along which\nthe function is well approximated by its tangent line. On the other hand, point with\nlarge second derivative tend to be badly approximated by the tangent line.\nprovided that the limit is well-defined. If the second derivative\nof a function f is well-defined at w, the function is said twice\ndifferentiable at w.\nIf f has a small second derivative at a given w, the derivative around\nw is almost constant. That is, the function behaves like a line around\nw, as illustrated in Fig. 2.5. Hence, the second derivative is usually\ninterpreted as the curvature of the function at a given point.\n2.4.2\nSecond directional derivatives\nFor a multi-input function f : RP →R, we saw that the directional\nderivative encodes infinitesimal variations of f along a given direction.\nTo analyze the second derivative, the curvature of the function at a\ngiven point w, we can consider the variations along a pair of directions,\nas defined below.\nDefinition 2.15 (Second directional derivative). The second direc-\ntional derivative of f : RP →R at w ∈RP along v, v′ ∈RP\nis defined as the directional derivative of w 7→∂f(w)[v] along v′,\nthat is,\n∂2f(w)[v, v′] := lim\nδ→0\n∂f(w + δv′)[v] −∂f(w)[v]\nδ\n,\n\n42\nDifferentiation\nprovided that ∂f(w)[v] is well-defined around w and that the limit\nexists.\nOf particular interest are the variations of a function around the\ncanonical directions: the second partial derivatives, defined as\n∂2\nijf(w) := ∂2f(w)[ei, ej]\nfor ei, ej the ith and jth canonical directions in RP , respectively. In\nLeibniz notation, the second partial derivatives are denoted\n∂2\nijf(w) = ∂2f(w)\n∂wi∂wj\n.\n2.4.3\nHessians\nFor a multi-input function, twice differentiability is simply defined as\nthe differentiability of any directional derivative ∂f(w)[v] w.r.t. w.\nDefinition 2.16 (Twice differentiability). A function f : RP →R is\ntwice differentiable at w ∈RP if it is differentiable and ∂f : RP →\n(RP →R) is also differentiable at w.\nAs a result, the second directional derivative is a bilinear form.\nDefinition 2.17 (Bilinear map, bilinear form). A function b : RP ×\nRP →RM is a bilinear map if b[v, ·] : RP →R is linear for any v\nand b[·, v′] is linear for any v′. That is,\nb[v, v′] =\nP\nX\ni=1\nvib[ei, v′] =\nP\nX\ni=1\nP\nX\nj=1\nviv′\njb[ei, ej],\nfor v = PP\ni=1 viei and v′ = PP\ni=1 v′\niei. A bilinear map with values\nin R, b : RP × RP →R, is called a bilinear form.\nThe second directional derivative can be computed along any two\ndirections from the knowledge of the second partial derivatives gathered\nin the Hessian.\n\n2.4. Second-order differentiation\n43\nDefinition 2.18 (Hessian). The Hessian of a twice differentiable\nfunction f : RP →R at w is the P × P matrix gathering all second\npartial derivatives:\n∇2f(w) :=\n\n\n\n\n∂11f(w)\n. . .\n∂1P f(w)\n...\n...\n...\n∂P1f(w)\n. . .\n∂PP f(w)\n\n\n\n,\nprovided that all second partial derivatives are well-defined.\nThe second directional derivative at w is bilinear in any direc-\ntions v = PP\ni=1 viei and v′ = PP\ni=1 v′\niei. Therefore,\n∂2f(w)[v, v′] =\nP\nX\ni,j=1\nviv′\nj∂2f(w)[ei, ej] = ⟨v, ∇2f(w)v′⟩.\nGiven the gradient of f, the Hessian is equivalent to the transpose\nof the Jacobian of the gradient. By slightly generalizing the notation ∇\nto denote the transpose of the Jacobian of a function (which matches\nits definition for single-output functions), we have that the Hessian can\nbe expressed as ∇2f(w) = ∇(∇f)(w), which justifies its notation.\nSimilarly as for the differentiability of a function f, twice differen-\ntiability of f at w is equivalent to having the second partial derivatives\nnot only defined but also continuous in a neighborhood of w. Remark-\nably, by requiring twice differentiability, i.e., continuous second partial\nderivatives, the Hessian is guaranteed to be symmetric (Schwarz, 1873).\nProposition 2.10 (Symmetry of the Hessian). If a function f : RP →\nR is twice differentiable at w, then its Hessian ∇2f(w) is symmetric,\nthat is, ∂2\nijf(w) = ∂2\njif(w) for any i, j ∈{1, . . . P}.\nThe symmetry of the Hessian means that it can alternatively be\nwritten as ∇2f(w) = (∂2\njif(w))P\ni,j=1 = ∂(∇f)(w), i.e., the Jacobian of\nthe gradient of f.\n2.4.4\nHessian-vector products\nSimilarly to the Jacobian, we can exploit the formal definition of the\nHessian as a bilinear form to extend its definition for Euclidean spaces.\n\n44\nDifferentiation\nIn particular, we can define the notion of Hessian-vector product.\nDefinition 2.19 (Hessian-vector product). If a function f : E →R\ndefined on a Euclidean space E with inner product ⟨·, ·⟩, is twice\ndifferentiable at w ∈E, then for any v ∈E, there exists ∇2f(w)[v],\ncalled the Hessian-vector product (HVP) of f at w along v\nsuch that for any v′ ∈E,\n∂2f(w)[v, v′] = ⟨v′, ∇2f(w)[v]⟩.\nIn particular for E = RP , the HVP is ∇2f(w)[v] = (∂2f(w)[v, ei])P\ni=1.\nFrom an autodiff point of view, the HVP can be implemented in\nfour different ways, as explained in Section 9.1.\n2.4.5\nSecond-order Jacobians\nThe previous definitions naturally extend to multi-output functions\nf : E →F, where f := (f1, . . . , fM), fj : E →Fj and F := F1×· · ·×FM.\nThe second directional derivative is defined by gathering the second\nderivatives of each coordinate’s function. That is, for w, v, v′ ∈E,\n∂f(w)[v, v′] = (∂fj(w)[v, v′])M\nj=1 ∈F.\nThe function f is twice differentiable if and only if all its coordinates are\ntwice differentiable. The second directional derivative is then a bilinear\nmap. We can then compute second directional derivatives as\n∂2f(w)[v, v′] =\nP\nX\ni,j=1\nviv′\nj∂2f(w)[ei, ej] = (⟨v, ∇2fj(w)v′⟩)M\nj=1.\nWhen E = RP and Fj = R, so that F = RM, the bilinear map can be\nmaterialized as a tensor\n∂2f(w) = (∂2f(w)[ei, ej])P\ni,j=1 ∈RM×P×P ,\nthe “second-order Jacobian” of f. However, similarly to the Hessian,\nit is usually more convenient to apply the bilinear map to prescribed\nvectors v and v′ than to materialize as a tensor.\n\n2.5. Higher-order differentiation\n45\n2.5\nHigher-order differentiation\n2.5.1\nHigher-order derivatives\nDerivatives can be extended to any order. Formally, the nth derivative\ncan be defined inductively as follows for a single-input, single-output\nfunction.\nDefinition 2.20 (nth order derivative). The nth derivative f(n) of a\nfunction f : R →R at w ∈R is defined as\nf(n)(w) := (f(n−1))′(w) = lim\nδ→0\nf(n−1)(w + δ) −f(n−1)(w)\nδ\nprovided that fn−1 is differentiable around w and that the limit\nexists. In such a case, the function is said n times differentiable at\nw.\n2.5.2\nHigher-order directional derivatives\nFor a multi-input function f, we can naturally extend the notion of\ndirectional derivative as follows.\nDefinition 2.21 (nth order directional derivative). The nth directional\nderivative of f : RP →R at w ∈RP along v1, . . . , vn is defined as\n∂nf(w)[v1, . . . , vn]\n= ∂(∂n−1f(w)[v1, . . . , vn−1])[vn]\n= lim\nδ→0\n∂f(w + δvn)[v1, . . . , vn−1] −∂f(w)[v1, . . . , vn−1]\nδ\nA multi-input function f is n-times differentiable if it is n −1\ndifferentiable and its n −1 directional derivative along any direction\nis differentiable. As a consequence the nth directional derivative is a\nmultilinear form.\nDefinition 2.22 (Multilinear map, multilinear form). A function c :\n⊗n\ni=1RP →RM is a multilinear map if it is linear in each coor-\ndinate given all others fixed, that is, if vj 7→c[v1, . . . , vj, . . . , vn]\n\n46\nDifferentiation\nis linear in vj for any j ∈[n]. It is a multilinear form if it has\nvalues in R.\nThe nth order directional derivative is then given by\n∂nf(w)[v1, . . . , vn] =\nP\nX\ni1,...,in=1\nv1,i1 . . . vn,in∂nf(w)[ei1, . . . , ein].\nIts materialization is an nth order tensor\n∇nf(w) = (∂nf(w)[ei1, . . . , ein])P\ni1,...,in=1 ∈RP×...×P .\n2.5.3\nHigher-order Jacobians\nAll above definitions extend directly to the case of multi-output functions\nf : E →F, where F = F1 × · · · × FM. The nth directional derivatives\n∂nf(w)[v1, . . . , vn] = (∂nfj(w)[v1, . . . , vn])M\nj=1.\nThe function f is then n times differentiable if if it is n−1 differentiable\nand its n −1 directional derivative along any direction is differentiable.\nAs a consequence, the nth directional derivative is a multilinear map.\nThe nth directional derivative can be decomposed into partial derivatives\nas\n∂nf(w)[v1, . . . , vn] =\nP\nX\ni1,...,in=1\nv1,i1 . . . vn,in∂nf(w)[ei1, . . . , ein].\nWhen E = RP and F = RM, it is materialized by an n + 1th order\ntensor\n∂nf(w) = (∂nfj(w)[ei1, . . . , ein])M,P,...,P\nj=1,i1,...,in=1 ∈RM×P×...×P .\n2.5.4\nTaylor expansions\nWith Landau’s little o notation, we have seen that if a function is\ndifferentiable it is approximated by a linear function in v,\nf(w + v) = f(w) + ⟨∇f(w), v⟩+ o(∥v∥2).\nSuch an expansion of the function up to its first derivative is called the\nfirst-order Taylor expansion of f around w.\n\n2.6. Differential geometry\n47\nIf the function f is twice differentiable, we can approximate it by a\nquadratic in v, leading to the second-order Taylor expansion of f\naround w,\nf(w + v) = f(w) + ⟨∇f(w), v⟩+ 1\n2⟨v, ∇2f(w)v⟩+ o(∥v∥2\n2).\nCompared to the first-order Taylor approximation, it is naturally more\naccurate around w, as reflected by the fact that ∥v∥3\n2 ≤∥v∥2\n2 for\n∥v∥2 ≤1.\nMore generally, we can build the nth order Taylor expansion of\na n times differentiable function f : RP →RM around w ∈RP by\nf(w + v) = f(w) + ∂f(w)[v] + 1\n2∂2f(w)[v, v] + . . .\n+ 1\nn!∂nf(w)[v, . . . , v\n|\n{z\n}\nn times\n] + o(∥v∥n\n2).\nNote that, using the change of variable w′ = w + v ⇐⇒v = w′ −w,\nit is often convenient to write the nth Taylor expansion of f(w′) around\nw as\nf(w′) = f(w) +\nn\nX\nj=1\n1\nj!∂jf(w)[w′ −w, . . . , w′ −w\n|\n{z\n}\nj times\n] + o(∥w′ −w∥n\n2).\nTaylor expansions will prove useful in Chapter 7 for computing deriva-\ntives by finite differences.\n2.6\nDifferential geometry\nIn this chapter, we progressively generalized the notion of derivative\nfrom real numbers to vectors and variables living in a linear space (a.k.a.\nvector space), either finite dimensional or infinite dimensional. We can\nfurther generalize these notions by considering a local notion of linearity.\nThis is formalized by smooth manifolds in differential geometry,\nwhose terminology is commonly adopted in the automatic differentiation\nliterature and software. In this section, we give a brief overview of\nderivatives on smooth manifolds (simply referred as manifolds), and\nrefer to Boumal (2023) for a complete introduction.\n\n48\nDifferentiation\n2.6.1\nDifferentiability on manifolds\nEssentially, a manifold is a set that can be locally approximated by\na Euclidean space. The most common example is a sphere like the\nEarth. Seen from the Moon, the Earth is not a plane, but locally, at a\nhuman level, it can be seen as a flat surface. Euclidean spaces are also\ntrivial examples of manifold. A formal characterization of the sphere\nas a manifold is presented in Example 2.7. For now, we may think of\na “manifold” as some set (e.g., the sphere) contained in some ambient\nEuclidean space; note however that manifolds can be defined generally\nwithout being contained in a Euclidean space (Boumal, 2023, Chapter\n8). Differentiability in manifolds is simply inherited from the notion of\ndifferentiability in the ambient Euclidean space.\nDefinition 2.23 (Differentiability of restricted functions). Let M and\nN be manifolds. A function f : M →N defined from M ⊆E to\nN ⊆F, with E and F Euclidean spaces, is differentiable if f is\nthe restriction of a differentiable function ¯f : E →F, so that f\ncoincides with ¯f on M.\nOur objective is to formalize the directional derivatives and gradients\nfor functions defined on manifolds. This formalization leads to the\ndefinitions of tangent spaces and cotangent spaces, and the associated\ngeneralizations of JVP and VJP operators as pushforward and pullback\noperators, respectively.\n2.6.2\nTangent spaces and pushforward operators\nTo generalize the notion of directional derivatives of a function f, the\none property we want to preserve is the chain rule. Rather than starting\nfrom the variations of f at a given point along a direction, we start with\nthe variations of f along curves. Namely, on a manifold like the sphere\nSP in RP , we can look at curves α : R →SP passing by w ∈SP at\ntime 0, that is, α(0) = w. For single-input functions like α, we denoted\nfor simplicity α′(0) := (α′\n1(0), . . . , α′\nP (0)). The directional derivative of\nf must typically serve to define the derivative of f ◦α at 0, such that\n(f ◦α)′(0) = ∂f(w)[α′(0)]. In the case of the sphere, as illustrated in\n\n2.6. Differential geometry\n49\nFig. 2.6, the derivative α′(0) of a curve α passing through a point w is\nalways tangent to the sphere at w. The tangent plane to the sphere\nat w, then captures all possible relevant vectors to pass to the JVP\nwe are building. To define the directional derivative of a function f on\na manifold, we therefore restrict ourselves to an operator defined on\nthe tangent space TwM, whose definition below is simplified for our\npurposes.\nDefinition 2.24 (Tangent space). The tangent space of a mani-\nfold M at w ∈M is defined as\nTwM := {v = α′(0) for any α : R →M differentiable s.t. α(0) = w}.\nIn the case of the sphere in Fig. 2.6, the tangent space is a plane, that\nis, a Euclidean space. This property is generally true: tangent spaces\nare Euclidean spaces such that we will be able to define directional\nderivatives as linear operators. Now, if f is differentiable and goes from\na manifold M to a manifold N, then f ◦α is a differentiable curve in N.\nTherefore, (f ◦α)′(0) is the derivative of a curve passing through f(w)\nat 0 and is tangent to N at f(w). Hence, the directional derivative\nof f : M →N at w can be defined as a function from the tangent\nspace TwM of M at w onto the tangent space Tf(w)N of N at f(w).\nOverall, we built the directional derivative (JVP) by considering how a\ncomposition of f with any curve α pushes forward the derivative of α\ninto the derivative of f ◦α. The resulting JVP is called a pushforward\noperator in differentiable geometry.\nDefinition 2.25 (Pushforward operator). Given two manifolds M\nand N, the pushforward operator of a differentiable function\nf : M →N at w ∈M is the linear map ∂f(w) : TwM →Tf(w)N\ndefined by\n∂f(w)[v] := (f ◦α)′(0),\nfor any v ∈TwM such that v = α′(0), for a differerentiable curve\nα : R →M, passing by w at 0, i.e., α(0) = w.\n\n50\nDifferentiation\nFigure 2.6: A differentiable function f defined from a sphere M to a sphere N\ndefines a push-forward operator that maps tangent vectors (derivatives of functions\non the sphere passing by w) in the tangent space TwM to tangent vectors of N at\nf(w) in the tangent space Tf(w)N.\n2.6.3\nCotangent spaces and pullback operators\nTo generalize the JVP, we composed f : M →N with any single-input\nfunction α : R →M giving values on the manifold. The derivative of\nany such α is then pushed forward from TwM to Tf(w)N by the action\nof f. To define the VJP, we take a symmetric approach. We consider all\nsingle-output differentiable functions β : N →R defined on y ∈N with\ny = f(w) for some w ∈M. We then want to pull back the derivatives\nof β when precomposing it by f. Therefore, the space on which the\nVJP acts is the space of directional derivatives of any β : N →R at y,\ndefining the cotangent space.\nDefinition 2.26 (Cotangent space). The cotangent space of a\nmanifold N at y ∈N is defined as\nT ∗\ny N = {u = ∂β(y) for any β : N →R differentiable}\n= {u : TyN →R for any linear map u},\nNote that elements of the cotangent space are linear mappings, not\nvectors. This distinction is important to define the pullback operator\nas an operator on functions as done in measure theory. From a linear\nalgebra viewpoint, the cotangent space is exactly the dual space of\nTyN, that is, the set of linear maps from TyN to R, called linear forms.\nAs TyN is a Euclidean space, its dual space T ∗\ny N is also a Euclidean\nspace. The pullback operator is then defined as the operator that gives\n\n2.6. Differential geometry\n51\naccess to directional derivatives of β ◦f given the directional derivative\nof β at f(w).\nDefinition 2.27 (Pullback operator). Given two manifolds M and\nN, the pullback operator of a differentiable function f : M →N\nat w ∈M is the linear map ∂f(w)⋆: T ∗\nf(w)N →T ∗\nwM defined by\n∂f(w)⋆u := ∂(β ◦f)(w),\nfor any u ∈Tf(w)N ∗such that ∂β(f(w)) = u, for a differentiable\nfunction β : N →R.\nContrary to the pushforward operator that acts on vectors, the\npullback operator acts on linear forms. Hence, the slight difference in\nnotation between ∂f(w)⋆and ∂f(w)∗, the adjoint operator of ∂f(w).\nTo properly define the adjoint operator ∂f(w)∗, we need a notion\nof inner product. Since tangent spaces are Euclidean spaces, we can\ndefine an inner product ⟨·, ·⟩w for each TwM and w ∈M, making M\na Riemannian manifold. Equipped with these inner products, the\ncotangent space can be identified with the tangent space, and we can\ndefine gradients.\nDefinition 2.28 (Gradients in Riemannian manifolds). Let M be a\nRiemannian manifold equipped with inner products ⟨·, ·⟩w. For any\ncotangent vector u ∈T ∗\nwM, with w ∈M, there exists a unique\ntangent vector u ∈TwM such that\n∀v ∈TwM, u[v] = ⟨u, v⟩w.\nIn particular for any differentiable function f : M →R, we can\ndefine the gradient of f as the unique tangent vector ∇f(w) ∈\nTwM such that\n∀v ∈TwM, ∂f(w)[v] = ⟨∇f(w), v⟩.\nTherefore, rather than pulling back directional derivatives, we can\npull back gradients. The corresponding operator is then naturally the\nadjoint ∂f(w)∗of the pushforward operator. Namely, given two Rieman-\nnian manifolds M and N, and a differentiable function f : M →N,\n\n52\nDifferentiation\nFunction\nf\nM →N\nPush-forward\n∂f(w)\nTwM →Tf(w)N\nPullback\n∂f(w)⋆\nT ∗\nf(w)N →T ∗\nwM\nAdjoint of pushforward\n∂f(w)∗\nTf(w)N →TwM\nTable 2.1: For a differentiable function f defined from a manifold M onto a manifold\nN, the JVP is generalized with the notion of pushforward ∂f(w). The counterpart\nof the pushforward is the pullback operation ∂f(w)⋆that acts on linear forms in the\ntangent spaces. For Riemannian manifolds, the pullback operation can be identified\nwith the adjoint operator ∂f(w)∗of the pushforward operator as any linear form is\nrepresented by a vector.\nwe have\n(∂f(w)⋆u) [v] = ⟨∂f(w)∗[u], v⟩for any v ∈TwM\nfor u = ⟨·, u⟩∈T ∗\nf(w)N represented by u ∈Tf(w)N.\nExample 2.7 (The sphere as a manifold). The sphere SP in RP is\ndefined as the set of points w ∈RP , satisfying c(w) := ⟨w, w⟩−1 =\n0, with JVP ∂c(w)[v] = 2⟨w, v⟩.\nFor any v = (v1, . . . , vP−1) ∈RP−1 close enough to a point w\non the sphere, we can define ψ1(v) =\np\n1 −⟨v, v⟩such that ψ(v) =\n(v1, . . . , vP−1, ψ1(v)) satisfies ⟨ψ(v), ψ(v)⟩= 1, that is c(ψ(w)) =\n1. With the help of the mapping ψ−1 from a neighborhood of w\nin the sphere to RP−1, we can see the sphere locally as a space of\ndimension P −1.\nThe tangent space can be naturally characterized in terms of\nthe constraining function c. Namely, the curve α : R →S such that\nα(0) = w satisfies for any δ ∈R, c(α(δ)) = 0. Hence, differentiating\nthe implicit equation, we have\n(c ◦α)′(0) = ∂c(w)[α′(0)].\nThat is, α′(0) is in the null space of ∂c(w), denoted\nNull(∂c(w)) := {v ∈RP : ∂c(w)[v] = 0}.\n\n2.7. Generalized derivatives\n53\nThe tangent space of S at w is then\nTwM = Null(2⟨w, ·⟩)\n= {v ∈RP : ⟨w, v⟩= 0}\nWe naturally recover that the tangent space is a Euclidean space\nof dimension P −1, defined as the set of points orthogonal to w.\n2.7\nGeneralized derivatives\nWhile we largely focus on differentiable functions in this book, it is\nimportant to characterize non-differentiable functions. We distinguish\nhere two cases: continuous functions and non-continuous functions. For\nthe former case, there exists generalizations of the notion of directional\nderivative, gradient and Jacobian, presented below. For non-continuous\nfunctions, even if derivatives exist almost everywhere, they may be\nuninformative. For example, piecewise constant functions, encountered\nin e.g. control flows (Chapter 5), are almost everywhere differentiable\nbut with zero derivatives. In such cases, surrogate functions can be\ndefined to ensure the differentiability of a program (Part IV).\n2.7.1\nRademacher’s theorem\nWe first recall the definition of locally Lipschitz continuous function.\nDefinition 2.29 (Locally Lipschitz continuous function). A function\nf : E →F, is Lipschitz continuous if there exists C ≥0 such that\nfor any x, y ∈E,\n∥f(x) −f(y)∥≤C∥x −y∥.\nA function f : E →F is locally Lipschitz continuous if for any\nx ∈E, there exists a neighborhood U of x such that f restricted\nto U is Lipschitz continuous.\nRademacher’s theorem (Rademacher, 1919) ensures that f is differ-\nentiable almost everywhere; see also Morrey Jr (2009) for a standard\nproof.\n\n54\nDifferentiation\nProposition 2.11 (Rademacher’s theorem). Let E and F denote Eu-\nclidean spaces. If f : E →F is locally Lipschitz-continuous, then f\nis almost everywhere differentiable, that is the set of points in E at\nwhich f is not differentiable is of (Lebesgue) measure zero.\n2.7.2\nClarke derivatives\nRademacher’s theorem hints that the definitions of directional deriva-\ntives, gradients and Jacobians may be generalized to locally Lipschitz\ncontinuous functions. This is what Clarke (1975) did in his seminal\nwork, which laid the foundation of nonsmooth analysis. The first\nbuilding block is a notion of generalized directional derivative.\nDefinition 2.30 (Clarke generalized directional derivative). The Clarke\ngeneralized directional derivative of a locally Lipschitz con-\ntinuous function f : E →R at w ∈E in the direction v ∈E\nis\n∂Cf(w)[v] := lim sup\nu→w\nδ↘0\nf(u + δv) −f(u)\nδ\n,\nprovided that the limit exists, where δ ↘0 means that δ approaches\n0 by non-negative values and the limit superior is defined as\nlim sup\nx→a\nf(x) := lim\nε→0 sup{f(x) : x ∈B(a, ε) \\ {a}}\nfor B(a, ε) := {x ∈E : ∥x −a∥≤ε} the ball centered at a of\nradius ε.\nThere are two differences with the usual definition of a directional\nderivative: (i) we considered slopes of the function in a neighborhood of\nthe point rather than at the given point, (ii) we took a limit superior\nrather than a usual limit. The first point is rather natural in the light\nof Rademacher’s theorem: we can properly characterize variations on\npoints where the function is differentiable, therefore we may take the\nlimits of these slopes as a candidate slope for the point of interest. The\nsecond point is more technical but essential: it allows us to characterize\nthe directional derivative as the supremum of some linear forms (Clarke\n\n2.7. Generalized derivatives\n55\net al., 2008). These linear forms in turn define a set of generalized\ngradients (Clarke et al., 2008, Chapter 2).\nDefinition 2.31 (Clarke generalized gradient). A Clarke general-\nized gradient of a locally Lipschitz function f : E →R at w ∈E\nis a point g ∈E such that ∀v ∈E\n∂f(w)[v] ≥⟨g, v⟩.\nThe set of Clarke generalized gradients is called the Clarke subd-\nifferential of f at w.\nDefinition 2.30 and Definition 2.31 can be used in non-Euclidean\nspaces, such as Banach or Hilbert spaces (Clarke et al., 2008). In\nEuclidean spaces, the Clarke generalized gradients can be characterized\nmore simply thanks to Rademacher’s theorem (Clarke et al., 2008,\nTheorem 8.1). Namely, they can be defined as a convex combination of\nlimits of gradients of f evaluated at a sequence in E \\ Ωthat converges\nto w (Proposition 2.12). In the following, the convex hull of a set S ⊆E,\nthe set of convex combinations of elements of S, is denoted\nconv(S) := {λ1s1 + . . . + λmsm : m ∈N, λi ≥0,\nm\nX\ni=1\nλi = 1, si ∈S}.\nProposition 2.12 (Characterization of Clarke generalized gradients).\nLet f : E →R be a locally Lipschitz continuous and denote Ωthe\nset of points at which f is not differentiable (Proposition 2.11). An\nelement g ∈E is a Clarke generalized gradient of f at w ∈E if and\nonly if\ng ∈conv\n\u0012\u001a\nlim\nn→+∞∇f(vn): (vn)+∞\nn=1 s.t. vn ∈E \\ Ω, vn\n→\nn→+∞w\n\u001b\u0013\n.\nThe Jacobian of a function f : E →F between two Euclidean spaces\ncan be generalized similarly (Clarke et al., 2008, Section 3.3).\nDefinition 2.32 (Clarke generalized Jacobian). Let f : E →F be a\nlocally Lipschitz continuous and denote Ωthe set of points at which\nf is not differentiable (Proposition 2.11). A Clarke generalized\n\n56\nDifferentiation\nJacobian of f at w ∈E is an element J of\nconv\n\u0012\u001a\nlim\nn→+∞∂f(vn): (vn)+∞\nn=1 s.t. vn ∈E \\ Ω, vn\n→\nn→+∞w\n\u001b\u0013\n.\nFor a continuously differentiable function f : E →F or f : E →R,\nthere is a unique generalized gradient, that is the usual gradient (Clarke\net al., 2008, Proposition 3.1, page 78). The chain-rule can be generalized\nto these objects (Clarke et al., 2008). Recently, Bolte and Pauwels (2020)\nand Bolte et al. (2022) further generalized Clarke gradients through the\ndefinition of conservative gradients to define automatic differentiation\nschemes for nonsmooth functions.\n2.8\nSummary\n• The usual definition of derivatives of real-valued univariate\nfunctions extends to multivariate functions f : RP →R through\nthe notion of directional derivative ∂f(w)[v] at w ∈RP in\nthe direction v ∈RP .\n• To take advantage of the representation of w = PP\nj=1 wjej using\nthe canonical bases {e1, . . . , eP }, the definition of differentiable\nfunctions requires the linearity of the directional derivative w.r.t.\nthe direction v.\n• This requirement gives rise to the notion of gradient ∇f(w) ∈\nRP , the vector that gathers the partial derivatives and further\ndefines the steepest ascent direction at w.\n• For vector-input vector-output functions f : RP →RM, the di-\nrectional derivative leads to the definition of Jacobian matrix\n∂f(w) ∈RM×P , the matrix which gathers all partial derivatives\n(notice that we use bold ∂). The chain rule is then the product\nof Jacobian matrices.\n• These notions can be extended to general Euclidean spaces, such as\nthe spaces of matrices or tensors. For functions of the form f : E →\nR, the gradient is ∇f(w) ∈E. More generally, for functions\nof the form f : E →F, the Jacobian ∂f(w) can be seen as a\n\n2.8. Summary\n57\nlinear map (notice the non-bold ∂). The directional derivative\nat w ∈E naturally defines a linear map l[v] = ∂f(w)[v], where\n∂f(w): E →F is called the Jacobian vector product (JVP)\nand captures the infinitesimal variation at w ∈E along the input\ndirection v ∈E.\n• Its adjoint ∂f(w)∗: F →E defines another linear map l[u] =\n∂f(w)∗[u] called the vector Jacobian product (VJP) and\ncaptures the infinitesimal variation at w ∈E along the output\ndirection u ∈F. The chain rule is then the composition of\nthese linear maps.\n• For the particular case when we compose a scalar-valued function\nℓ(such as a loss function) with a vector-valued function f (such\nas a network function), the gradient is given by ∇(ℓ◦f)(w) =\n∂f(w)∗∇ℓ(f(w)). This is why being able to apply the adjoint to\na gradient, which as we shall see can be done with reverse-mode\nautodiff, is so pervasive in machine learning.\n• The definitions of JVP and VJP operators can further be general-\nized in the context of differentiable geometry. In that framework,\nthe JVP amounts to the pushforward operator that acts on\ntangent vectors. The VJP amounts to the pullback operator\nthat acts on cotangent vectors.\n• We also saw that the Hessian matrix of a function f(w) from\nRP to R is denoted ∇2f(w) ∈RP×P . It is symmetric if the second\npartial derivatives are continuous. Seen as linear map, the Hessian\nleads to the notion of Hessian-vector product (HVP), which\nwe saw can be reduced to the JVP or the VJP of ∇f(w).\n• The main take-away message of this chapter is that computing the\ndirectional derivative or the gradient of compositions of functions\ndoes not require computing intermediate Jacobians but only to\nevaluate linear maps (JVPs or VJPs) associated with these inter-\nmediate functions. The goal of automatic differentiation, presented\nin Chapter 8, is precisely to provide an efficient implementation\n\n58\nDifferentiation\nof these maps for computation chains or more generally for\ncomputation graphs.\n\n3\nProbabilistic learning\nIn this chapter, we review how to perform probabilistic learning. We\nalso introduce exponential family distributions, as they play a key role\nin this book.\n3.1\nProbability distributions\n3.1.1\nDiscrete probability distributions\nA discrete probability distribution over a set Y is specified by its\nprobability mass function (PMF) p: Y →[0, 1]. The probability of\ny ∈Y is then defined by\nP(Y = y) := p(y),\nwhere Y denotes a random variable. When Y follows a distribution p,\nwe write Y ∼p (with some abuse of notation, we use the same letter p\nto denote the distribution and the PMF). The expectation of ϕ(Y ),\nwhere Y ∼p and ϕ: Y →RM, is then\nE[ϕ(Y )] =\nX\ny∈Y\np(y)ϕ(y),\n59\n\n60\nProbabilistic learning\nits variance (for one-dimensional variables) is\nV[ϕ(Y )] = E[(ϕ(Y ) −E[ϕ(Y )])2] =\nX\ny∈Y\np(y)(ϕ(y) −E[ϕ(Y )])2\nand its mode is\narg max\ny∈Y\np(y).\nThe Kullback-Leibler (KL) divergence (also known as relative entropy)\nbetween two discrete distributions over Y, with associated PMFs p and\nq, is the statistical “distance” defined by\nKL(p, q) :=\nX\ny∈Y\np(y) log\n\u0012p(y)\nq(y)\n\u0013\n= EY ∼p log\n\u0012p(Y )\nq(Y )\n\u0013\n.\n3.1.2\nContinuous probability distributions\nA continuous probability distribution over Y is specified by its proba-\nbility density function (PDF) p: Y →R+. The probability of A ⊆Y\nis then\nP(Y ∈A) =\nZ\nA\np(y)dy.\nThe definitions of expectation, variance and KL divergence are defined\nanalogously to the discrete setting, simply replacing P\ny∈Y with\nR\nY.\nSpecifically, the expectation of ϕ(Y ) is\nE[ϕ(Y )] =\nZ\nY\np(y)ϕ(y)dy,\nthe variance is\nV[ϕ(Y )] = E[(ϕ(Y ) −E[ϕ(Y )])2] =\nZ\nY\np(y)(ϕ(y) −E[ϕ(Y )])2dy\nand the KL divergence is\nKL(p, q) :=\nZ\nY\np(y) log\n\u0012p(y)\nq(y)\n\u0013\ndy = EY ∼p log\n\u0012p(Y )\nq(Y )\n\u0013\n.\nThe mode is defined as the arg maximum of the PDF.\nWhen Y = R, we can also define the cumulative distribution\nfunction (CDF)\nP(Y ≤b) =\nZ b\n−∞\np(y)dy.\n\n3.2. Maximum likelihood estimation\n61\nThe probability of Y lying in the semi-closed interval (a, b] is then\nP(a < Y ≤b) = P(Y ≤b) −P(Y ≤a).\n3.2\nMaximum likelihood estimation\n3.2.1\nNegative log-likelihood\nWe saw that a probability distribution over Y is specified by p(y), which\nis called the probability mass function (PMF) for discrete variables\nor the probability density function (PDF) for continuous variables. In\npractice, the true distribution p generating the data is unknown and we\nwish to approximate it with a distribution pλ, with parameters λ ∈Λ.\nGiven a finite set of i.i.d. observations y1, . . . , yN, how do we fit λ ∈Λ\nto the data? This can be done by maximizing the likelihood of the\ndata, i.e., we seek to solve\nbλN := arg max\nλ∈Λ\nN\nY\ni=1\npλ(yi).\nThis is known as maximum likelihood estimation (MLE). Because\nthe log function is monotonically increasing, this is equivalent to mini-\nmizing the negative log-likelihood, i.e., we have\nbλN = arg min\nλ∈Λ\n−\nN\nX\ni=1\nlog pλ(yi).\nExample 3.1 (MLE for the normal distribution). Suppose we set pλ\nto the normal distribution with parameters λ = (µ, σ), i.e.,\npλ(y) :=\n1\nσ\n√\n2π exp\n \n−1\n2\n\u0012y −µ\nσ\n\u00132!\n.\nThen, given observations y1, . . . , yN, the MLE estimators for µ and\nσ2 are the sample mean and the sample variance, respectively.\n3.2.2\nConsistency w.r.t. the Kullback-Leibler divergence\nIt is well-known that the MLE estimator is consistent, in the sense of\nthe Kullback-Leibler divergence. That is, denoting the true distribution\n\n62\nProbabilistic learning\np and\nλ∞:= arg min\nλ∈Λ\nKL(p, pλ) = EY ∼p log\n\u0012 p(Y )\npλ(Y )\n\u0013\n,\nthen bλN →λ∞in expectation over the observations, as N →∞. This\ncan be seen by using\nKL(p, pλ) ≈1\nN\nN\nX\ni=1\nlog\n\u0012 p(yi)\npλ(yi)\n\u0013\n= 1\nN\nN\nX\ni=1\nlog p(yi) −log pλ(yi)\nand the law of large numbers.\n3.3\nProbabilistic supervised learning\n3.3.1\nConditional probability distributions\nMany times in machine learning, instead of a probability P(Y = y) for\nsome y ∈Y, we wish to define a conditional probability P(Y = y|X =\nx), for some input x ∈X. This can be achieved by reduction to an\nunconditional probability distribution,\nP(Y = y|X = x) := pλ(y)\nwhere\nλ := f(x, w)\nand f is a model function with model parameters w ∈W. That is,\nrather than being a deterministic function from X to Y, f is a function\nfrom X to Λ, the set of permissible distribution parameters of the\noutput distribution associated with the input.\nWe emphasize that λ could be a single parameter or a collection of\nparameters. For instance, in the Bernoulli distribution, λ = π, while in\nthe univariate normal distribution, λ = (µ, σ).\nIn Section 3.4 and throughout this book, we will also use the notation\npθ instead of pλ when θ are the canonical parameters of an exponential\nfamily distribution (Section 3.4).\n3.3.2\nInference\nThe main advantage of this probabilistic approach is that our prediction\nmodel is much richer than if we just learned a function from X to Y.\n\n3.3. Probabilistic supervised learning\n63\nWe now have access to the whole distribution over possible outcomes in\nY and can compute various statistics:\n• Probability: P(Y = y|X = x) or P(Y ∈A|X = x),\n• Expectation: E[ϕ(Y )|X = x] for some function ϕ,\n• Variance: V[ϕ(Y )|X = x],\n• Mode: arg maxy∈Y pλ(y).\nWe now review probability distributions useful for binary classification,\nmulticlass classification, regression, multivariate regression, and integer\nregression. In the following, to make the notation more lightweight, we\nomit the dependence on x.\n3.3.3\nBinary classification\nFor binary outcomes, where Y = {0, 1}, we can use a Bernoulli\ndistribution with parameter\nλ := π ∈[0, 1].\nWhen a random variable Y is distributed according to a Bernoulli\ndistribution with parameter π, we write\nY ∼Bernoulli(π).\nThe PMF of this distribution is\npπ(y) :=\n\n\n\nπ\nif y = 1\n1 −π\nif y = 0\n.\nThe Bernoulli distribution is a binomial distribution with a single\ntrial. Since y ∈{0, 1}, the PMF can be rewritten as\npπ(y) = πy(1 −π)1−y.\nThe mean is\nE[Y ] = π = P(Y = 1)\nand the variance is\nV[Y ] = π(1 −π) = P(Y = 1)P(Y = 0).\n\n64\nProbabilistic learning\n0\n1\ny\n0.0\n0.5\n1.0\n0.2\n0.8\nPMF\n0\n1\ny\n0.0\n0.5\n1.0\n0.2\n1.0\nCDF\n3\n0\n3\n0.0\n0.5\n1.0\nMean function A′( )\n5\n0\n5\n0\n2\n4\nLoss L( , y)\ny = 1\ny = 0\nFigure 3.1: The Bernoulli distribution, whose PMF and CDF are here illustrated\nwith parameter π = 0.8. Its mean function is π = A′(θ) = logistic(θ) =\n1\n1+exp(−θ),\nwhere θ is for instance the output of a neural network. The negative log-likelihood\nleads to the logistic loss, L(θ, y) = softplus(θ) −θy = log(1 + exp(θ)) −θy. The\nloss curve is shown for y ∈{0, 1}.\nParameterization using a sigmoid\nSince the parameter π of a Bernoulli distribution needs to belong to\n[0, 1], we typically use a sigmoid function (Section 4.4.3), such as a\nlogistic function as the output layer:\nπ := f(x, w) := logistic(g(x, w)),\nwhere g: X × W →R is for example a neural network and\nlogistic(a) :=\n1\n1 + exp(−a) ∈(0, 1).\nWhen g is linear in w, this is known as binary logistic regression.\nRemark 3.1 (Link with the logistic distribution). The logistic distri-\nbution with mean and scale parameters µ and σ is a continuous\nprobability distribution with PDF\npµ,σ(u) := p0,1\n\u0012u −µ\nσ\n\u0013\nwhere\np0,1(z) :=\nexp(−z)\n(1 + exp(−z))2 .\nIf a random variable U follow a logistic distribution with pa-\nrameters µ and σ, we write U ∼Logistic(µ, σ). The CDF of\n\n3.3. Probabilistic supervised learning\n65\nU ∼Logistic(µ, σ) is\nP(U ≤u) =\nZ u\n−∞\npµ,σ(u)du = logistic\n\u0012u −µ\nσ\n\u0013\n.\nTherefore, if\nU ∼Logistic(µ, σ)\nand\nY ∼Bernoulli\n\u0012\nlogistic\n\u0012u −µ\nσ\n\u0013\u0013\n,\nthen\nP(Y = 1) = P(U ≤u).\nHere, U can be interpreted as a latent continuous variable and u\nas a threshold.\n3.3.4\nMulticlass classification\nFor categorical outcomes with M possible choices, where Y = [M],\nwe can use a categorical distribution with parameters\nλ := π ∈△M,\nwhere we define the probability simplex\n△M := {π ∈RM\n+ : ⟨π, 1⟩= 1},\ni.e., the set of valid discrete probability distributions. When Y follows\na categorical distribution with parameter π, we write\nY ∼Categorical(π).\nThe PMF of the categorical distribution is\npπ(y) := ⟨π, ϕ(y)⟩= πy,\nwhere\nϕ(y) := ey\nis the standard basis vector for the coordinate y ∈[M].\nSince Y is a categorical variable, it does not make sense to compute\nthe expectation of Y but we can compute that of ϕ(Y ) = eY ,\nEY ∼pπ[ϕ(Y )] = π.\n\n66\nProbabilistic learning\n1\n2\n3\ny\n0.0\n0.5\n1.0\n0.3\n0.6\n0.1\nPMF\n1\n2\n3\ny\n0.0\n0.5\n1.0\n0.3\n0.9\n1.0\nCDF\n3\n0\n3\ns\n0.0\n0.5\n1.0\nA( ), y\n5\n0\n5\ns\n0\n2\n4\n6\nLoss L( , y)\ny = e1\ny = e2\ny = e3\nFigure 3.2: The categorical distribution, whose PMF and CDF are here il-\nlustrated with parameter π = (0.3, 0.6, 0.1). Its mean function is π = ∇A(θ) =\nsoftargmax(θ), where θ ∈RM is for instance the output of a neural network. Here,\nfor illustration purpose, we choose to set θ = (s, 1, 0) and vary only s. Since the\nmean function ∇A(θ) belongs to R3, we choose to display ⟨∇A(θ), ei⟩= ∇A(θ)i,\nfor i ∈{1, 2, 3}. The negative log-likelihood leads to the logistic loss, L(θ, y) =\nlogsumexp(θ) −⟨θ, y⟩. The loss curve is shown for y ∈{e1, e2, e3}, again with\nθ = (s, 1, 0) and varying s.\nTherefore, as was also the case for the Bernoulli distribution, the mean\nand the probability distribution (represented by the vector π) are the\nsame in this case.\nParameterization using a softargmax\nSince the parameter vector π of a categorical distribution needs to\nbelong to △M, we typically use a softargmax as the output layer:\nπ := f(x, w) := softargmax(g(x, w)),\nwhere g: X × W →RM is for example a neural network and\nsoftargmax(u) :=\nexp(u)\nP\nj exp(uj) ∈relint(△M).\nThe output of the softargmax is in the relative interior of △M, relint(△M) =\n△M ∩RM\n>0. That is, the produced probabilities are always strictly posi-\ntive. The categorical distribution is a multinomial distribution with\na single trial. When g is linear in w, this is therefore known as multi-\nclass or multinomial logistic regression, though strictly speaking a\nmultinomial distribution could use more than one trial.\n\n3.3. Probabilistic supervised learning\n67\n3.3.5\nRegression\nFor real outcomes, where Y = R, we can use, among other choices, a\nnormal distribution with parameters\nλ := (µ, σ),\nwhere µ ∈R is the mean parameter and σ ∈R+ is the standard\ndeviation parameter. The PDF is\npµ,σ(y) :=\n1\nσ\n√\n2π exp\n \n−1\n2\n(y −µ)2\nσ2\n!\n.\nThe expectation is\nEY ∼pµ,σ[Y ] = µ.\nOne advantage of the probabilistic perspective is that we are not limited\nto predicting the mean. We can also compute the CDF\nP(Y ≤y) = 1\n2\n\u0014\n1 + erf\n\u0012y −µ\nσ\n√\n2\n\u0013\u0015\n,\n(3.1)\nwhere we used the error function\nerf(z) :=\n2\n√π\nZ z\n0\ne−t2 dt.\nThis function is available in most scientific computing libraries, such as\nSciPy (Virtanen et al., 2020). From the CDF, we also easily obtain\nP(a < Y ≤b) = 1\n2\n\u0014\nerf\n\u0012b −µ\nσ\n√\n2\n\u0013\n−erf\n\u0012a −µ\nσ\n√\n2\n\u0013\u0015\n.\nParameterization\nTypically, in regression, the mean is output by a model, while the\nstandard deviation σ is kept fixed (typically set to 1). Since µ is uncon-\nstrained, we can simply set\nµ := f(x, w) ∈R,\nwhere f : X × W →R is for example a neural network. That is, the\noutput of f is the mean of the distribution,\nEY ∼pµ,1[Y ] = µ = f(x, w).\nWe can also use µ to predict P(Y ≤y) or P(a < Y ≤b), as shown\nabove.\n\n68\nProbabilistic learning\n5\n0\n5\ny\n0.0\n0.2\n0.4\nPDF\n=\n2\n= 0\n= 2\n5\n0\n5\ny\n0.0\n0.5\n1.0\nCDF (Y\ny)\n=\n2\n= 0\n= 2\n5\n0\n5\n=\n5\n0\n5\nMean function A′( )\n5\n0\n5\n0\n20\n40\nLoss L( , y)\ny =\n2\ny = 0\ny = 2\nFigure 3.3: The Gaussian distribution, with mean parameter µ and variance\nσ2 = 1. Its mean function is µ = A′(θ) = θ, where θ is for instance the output of a\nneural network. The negative log-likelihood leads to the squared loss, L(θ, y) =\n(y −θ)2. The loss curve is shown for y ∈{−2, 0, 2}.\n3.3.6\nMultivariate regression\nMore generally, for multivariate outcomes, where Y = RM, we can\nuse a multivariate normal distribution with parameters\nλ := (µ, Σ),\nwhere µ ∈RM is the mean and Σ ∈RM×M is the covariance matrix.\nThe PDF is\npµ,Σ(y) :=\n1\nq\n2πM|Σ|\nexp\n\u0012\n−1\n2⟨y −µ, Σ−1(y −µ)⟩\n\u0013\n.\nUsing a diagonal covariance matrix is equivalent to using M independent\nnormal distributions for each Yj, for j ∈[M]. The expectation is\nEY ∼pµ,Σ[Y ] = µ.\nParameterization\nTypically, in multivariate regression, the mean is output by a model,\nwhile the covariance matrix is kept fixed (typically set to the identity\nmatrix). Since µ is again unconstrained, we can simply set\nµ := f(x, w) ∈RM.\nMore generally, we can parametrize the function f so as to output both\nthe mean µ and the covariance matrix Σ, i.e.,\n(µ, Σ) := f(x, w) ∈RM × RM×M.\n\n3.3. Probabilistic supervised learning\n69\n0\n10\n20\ny\n0.0\n0.2\nPMF (Y = y)\n= 1\n= 4\n= 10\n0\n10\n20\ny\n0.0\n0.5\n1.0\nCDF (Y\ny)\n= 1\n= 4\n= 10\n2.5\n0.0\n2.5\n0\n10\n20\nMean function A′( )\n2.5\n0.0\n2.5\n0\n20\n40\nLoss L( , y)\ny = 1\ny = 4\ny = 10\nFigure 3.4: The Poisson distribution, with mean parameter λ. For the PMF\nand the CDF, the lines between markers are shown for visual aid: the Poisson\ndistribution does not assign probability mass to non-integer values. Its mean function\nis λ = A′(θ) = exp(θ), where θ is for instance the output of a neural network.\nThe negative log-likelihood leads to the Poisson loss, L(θ, y) = −log pλ(y) =\n−yθ + exp(θ) + log(y!), which is a convex function of θ. The loss curve is shown for\ny ∈{1, 4, 10}.\nThe function f must be designed such that Σ is symmetric and positive\nsemi-definite. This is easy to achieve for instance by parametrizing\nΣ = SS⊤for some matrix S.\n3.3.7\nInteger regression\nFor integer outcomes, where Y = N, we can use, among other choices,\na Poisson distribution with mean parameter λ > 0. Its PMF is\nP(Y = y) = pλ(y) := λy exp(−λ)\ny!\n.\nIts CDF is\nP(Y ≤y) =\ny\nX\ni=0\nP(Y = y).\nThe Poisson distribution implies that the index of dispersion (the\nratio between variance and mean) is 1, since\nE[Y ] = V[Y ] = λ.\nWhen this assumption is inappropriate, one can use generalized Poisson\ndistributions (Satterthwaite, 1942).\n\n70\nProbabilistic learning\nParameterization using an exponential\nSince the parameter λ of a Poisson distribution needs to be strictly\npositive, we typically use an exponential function as output layer:\nλ := f(x, w) := exp(g(x, w)) > 0,\nwhere g: X × W →R.\n3.3.8\nLoss functions\nIn the conditional setting briefly reviewed in Section 3.3.1, we can use\nmaximum likelihood estimation (MLE) to estimate the model parame-\nters w ∈W of f. Given a set of input-output pairs (x1, y1), . . . , (xN, yN),\nwe choose the model parameters that maximize the likelihood of the\ndata,\nbwN := arg max\nw∈W\nN\nY\ni=1\npλi(yi),\nwhere\nλi := f(xi, w).\nAgain, this is equivalent to minimizing the negative log-likelihood,\nbwN = arg min\nw∈W\n−\nN\nX\ni=1\nlog pλi(yi).\nInterestingly, MLE allows us to recover several popular loss functions.\n• For the Bernoulli distribution with parameter λi = πi = logistic(g(xi, w)),\nwe have\n−log pλi(yi) = −[yi log πi + (1 −yi) log(1 −πi)] ,\nwhich is the binary logistic loss function.\n• For the categorical distribution with parameters λi = πi =\nsoftargmax(g(xi, w)), we have\n−log pλi(yi) = log\nM\nX\nj=1\nexp(πi,j) −πi,yi\n= logsumexp(πi) −⟨πi, eyi⟩,\n\n3.4. Exponential family distributions\n71\nwhich is the multiclass logistic loss function, also known as\ncross-entropy loss.\n• For the normal distribution with mean λi = µi = f(xi, w) and\nfixed variance σ2\ni , we have\n−log pλi(yi) = 1\nσ2\ni\n(yi −µi)2 + 1\n2 log σ2\ni + 1\n2 log(2π),\nwhich is, up to constant and with unit variance, the squared loss\nfunction.\n• For the Poisson distribution with mean λi = exp(θi), where θi :=\ng(xi, w), we have\n−log pλi(yi) = −yi log(λi) + λi + log(yi!)\n= −yiθi + exp(θi) + log(yi!)\nwhich is the Poisson loss function. The loss function is convex\nw.r.t. λi and θi for yi ≥0.\n3.4\nExponential family distributions\n3.4.1\nDefinition\nThe exponential family is a class of probability distributions, whose\nPMF or PDF can be written in the form\npθ(y) = h(y) exp [⟨θ, ϕ(y)⟩]\nexp(A(θ))\n= h(y) exp [⟨θ, ϕ(y)⟩−A(θ)] ,\nwhere θ are the natural or canonical parameters of the distribution.\nThe function h is known as the base measure. The function ϕ is the\nsufficient statistic: it holds all the information about y and is used\nto embed y in a vector space. The function A is the log-partition\nor log-normalizer (see below for a details). All the distributions we\nreviewed in Section 3.3 belong to the exponential family. With some\nabuse of notation, we use pλ for the distribution in original form and\npθ for the distribution in exponential family form. As we will see,\nwe can go from θ to λ and vice-versa. We illustrate how to rewrite a\ndistribution in exponential family form below.\n\n72\nProbabilistic learning\nExample 3.2 (Bernoulli distribution). The PMF of the Bernoulli\ndistribution with parameter λ = π equals\npλ(y) := πy(1 −π)1−y\n= exp(log(πy(1 −π)1−y))\n= exp(y log(π) + (1 −y) log(1 −π))\n= exp(log(π/(1 −π))y + log(1 −π))\n= exp(θy −log(1 + exp(θ)))\n= exp(θy −softplus(θ))\n=: pθ(y).\nTherefore, Bernoulli distributions belong to the exponential family,\nwith natural parameter θ = logit(π) := log(π/(1 + π)).\nWe rewrite the previously-described distributions in exponential\nfamily form in Table 3.1. This list is non-exhaustive: there are many\nmore distributions in the exponential family! (Barndorff-Nielsen, 2014)\n3.4.2\nThe log-partition function\nThe log-partition function A is the logarithm of the distribution’s\nnormalization factor. That is,\nA(θ) := log\nX\ny∈Y\nh(y) exp [⟨θ, ϕ(y)⟩]\nfor discrete random variables and\nA(θ) := log\nZ\nY\nh(y) exp [⟨θ, ϕ(y)⟩] dy\nfor continuous random variables. We denote the set of valid parameters\nΘ := {θ ∈RM : A(θ) < +∞} ⊆RM.\nWe can conveniently rewrite A(θ) for discrete random variables as\nA(θ) = logsumexp(B(θ)) := log\nX\ny∈Y\n[B(θ)]y,\nand similarly for continuous variables. Here, we defined the affine map\nB(θ) := (⟨θ, ϕ(y)⟩+ log h(y))y∈Y.\n\n3.4. Exponential family distributions\n73\nTable 3.1: Examples of distributions in the exponential family.\nBernoulli\nCategorical\nY\n{0, 1}\n[M]\nλ\nπ = logistic(θ)\nπ = softmax(θ)\nθ\nlogit(π)\nlog π + exp(A(θ))\nϕ(y)\ny\ney\nA(θ)\nsoftplus(θ)\nlogsumexp(θ)\nh(y)\n1\n1\nNormal (location only)\nNormal (location-scale)\nY\nR\nR\nλ\nµ = θσ\n(µ, σ2) = (−θ1\n2θ2 , −1\n2θ2 )\nθ\nµ\nσ\n( µ\nσ2 , −1\n2σ2 )\nϕ(y)\ny\nσ\n(y, y2)\nA(θ)\nθ2\n2 = µ2\n2σ2\n−θ2\n1\n4θ2 −1\n2 log(−2θ2) = µ2\nσ2 + log σ\nh(y)\nexp( −y2\n2σ2 )\n√\n2πσ\n1\n√\n2π\nMultivariate normal\nPoisson\nY\nRM\nN\nλ\n(µ, Σ) = (−1\n2θ−1\n2 θ1, −1\n2θ−1\n2 )\nλ = exp(θ)\nθ\n(Σ−1µ, −1\n2Σ−1)\nlog λ\nϕ(y)\n(y, yy⊤)\ny\nA(θ)\n−1\n4θ⊤\n1 θ−1\n2\n−1\n2 log | −2θ2|\nexp(θ)\n= 1\n2µ⊤Σ−1µ + 1\n2 log |Σ|\nh(y)\n(2π)−M/2\n1/y!\n\n74\nProbabilistic learning\nSince A(θ) is the composition of logsumexp, a convex function, and of\nB, an affine map, we immediately obtain the following proposition.\nProposition 3.1 (Convexity of the log-partition). A(θ) is a convex\nfunction.\nA major property of the log-partition function is that its gradient\ncoincides with the expectation of ϕ(Y ) according to pθ.\nProposition 3.2 (Gradient of the log-partition).\nµ(θ) := ∇A(θ) = EY ∼pθ[ϕ(Y )] ∈M.\nProof. The result follows directly from\n∇A(θ) = ∂B(θ)∗∇logsumexp(B(θ)) = (ϕ(y))y∈Y softmax(B(θ)).\nThe gradient ∇A(θ) is therefore often called the mean function.\nThe set of achievable means µ(θ) is defined by\nM := conv(ϕ(Y)) := {Ep[ϕ(Y )]: p ∈P(Y)},\nwhere conv(S) is the convex hull of S and P(Y) is the set of valid\nprobability distributions over Y.\nSimilarly, the Hessian ∇2A(θ) coincides with the covariance matrix\nof ϕ(Y ) according to pθ (Wainwright and Jordan, 2008, Chapter 3).\nWhen the exponential family is minimal, which means that the\nparameters θ uniquely identify the distribution, it is known that ∇A\nis a one-to-one mapping from Θ to M. That is, µ(θ) = ∇A(θ) and\nθ = (∇A)−1(µ(θ)).\n3.4.3\nMaximum entropy principle\nSuppose we observe the empirical mean bµ =\n1\nN\nPn\ni=1 ϕ(yi) ∈M of\nsome observations y1, . . . , yN. How do we find a probability distribution\nachieving this mean? Clearly, such a distribution may not be unique.\n\n3.4. Exponential family distributions\n75\nOne way to choose among all possible distributions is by using the\nmaximum entropy principle. Let us define the Shannon entropy by\nH(p) := −\nX\ny∈Y\np(y) log p(y)\nfor discrete variables and by\nH(p) := −\nZ\nY\np(y) log p(y)dy\nfor continuous variables. This captures the level of “uncertainty” in\np, i.e., it is maximized when the distribution is uniform. Then, the\nmaximum entropy distribution satisfying the first-order moment\ncondition (i.e., whose expectation matches the empirical mean) is\np⋆:= arg max\np∈P(Y)\nH(p)\ns.t.\nEY ∼p[ϕ(Y )] = bµ.\nIt can be shown that the maximum entropy distribution is necessarily\nin the exponential family with sufficient statistics defined by ϕ and its\ncanonical parameters θ coincide with the Lagrange multipliers of\nthe above constraint (Wainwright and Jordan, 2008, Section 3.1).\n3.4.4\nMaximum likelihood estimation\nSimilarly as in Section 3.2, to fit the parameters θ ∈Θ of an exponential\nfamily distribution to some i.i.d. observations y1, . . . , yN, we can use\nthe MLE principle, i.e.,\nbθN = arg max\nθ∈Θ\nN\nY\ni=1\npθ(yi) = arg min\nθ∈Θ\n−\nN\nX\ni=1\nlog pθ(yi).\nFortunately, for exponential family distributions, the log probabil-\nity/density enjoys a particularly simple form.\nProposition 3.3 (Negative log-likelihood). The negative log-likelihood\nof an exponential family distribution is\n−log pθ(y) = A(θ) −⟨θ, ϕ(y)⟩−log h(y).\n\n76\nProbabilistic learning\nIts gradient is\n−∇θ log pθ(y) = ∇A(θ) −ϕ(y) = EY ∼pθ[ϕ(Y )] −ϕ(y)\nand its Hessian is\n−∇2\nθ log pθ(y) = ∇2A(θ),\nwhich is independent of y.\nIt follows from Proposition 3.1 that θ 7→−log pθ(y) is convex.\nInterestingly, we see that the gradient is the residual between the\nexpectation of ϕ(Y ) according to the model and the observed ϕ(y).\n3.4.5\nProbabilistic learning with exponential families\nIn the supervised probabilistic learning setting, we wish to estimate a\nconditional distribution of the form pw(y | x). Given a model function\nf, such as a neural network, a common approach for defining such a\nconditional distribution is by reduction to the unconditional setting,\npw(y | x) := pθ(y)\nwhere\nθ := f(x, w).\nIn other words, the role of f is to produce the parameters of pθ given\nsome input x. It is a function from X × W to Θ. Note that f must be\ndesigned such that it produces an output in\nΘ := {θ ∈RM : A(θ) < +∞}.\nMany times, Θ will be the entire RM but this is not always the case.\nFor instance, as we previously discussed, for a multivariate normal\ndistribution, where θ = (µ, Σ) = f(x, w), we need to ensure that Σ is\na positive semidefinite matrix.\nTraining\nGiven input-output pairs {(xi, yi)}N\ni=1, we then seek to find the param-\neters w of f(x, w) by minimizing the negative log-likelihood\narg min\nw∈W\n−\nN\nX\ni=1\nlog pθi(yi) = arg min\nw∈W\nN\nX\ni=1\nA(θi) −⟨θi, ϕ(yi)⟩\n\n3.5. Summary\n77\nwhere θi := f(xi, w). While −log pθ(y) is a convex function of θ for\nexponential family distributions, we emphasize that −log pf(x,w)(y) is\ntypically a nonconvex function of w, when f is a nonlinear function,\nsuch as a neural network.\nInference\nOnce we found w by minimizing the objective function above, there are\nseveral possible strategies to perform inference for a new input x.\n• Expectation. When the goal is to compute the expectation of\nϕ(Y ), we can use ∇A(f(x, w)). That is, we compute the distribu-\ntion parameters associated with x by θ = f(x, w) and then we\ncompute the mean by µ = ∇A(θ). When f is linear in w, the\ncomposition ∇A ◦f is called a generalized linear model.\n• Probability. When the goal is to compute the probability of a\ncertain y, we can compute the distribution parameters associated\nwith x by θ = f(x, w) and then we can compute P(Y = y|X =\nx) = pθ(y). In the particular case of the categorical distribution\n(of which the Bernoulli distribution is a special case), we point\nout again that the mean and the probability vector coincide:\nµ = p = ∇A(θ) = softargmax(θ) ∈△M.\n• Other statistics. When the goal is to compute other quantities,\nsuch as the variance or the CDF, we can convert the natural pa-\nrameters θ to the original distribution parameters λ (see Table 3.1\nfor examples). Then, we can use established formulas for the\ndistribution in original form, to compute the desired quantities.\n3.5\nSummary\n• We reviewed discrete and continuous probability distributions.\n• We saw how to fit distribution parameters to data using the\nmaximum likelihood estimation (MLE) principle and saw its\nconnection with the Kullback-Leibler divergence.\n\n78\nProbabilistic learning\n• Instead of designing a model function from the input space X to\nthe output space Y, we saw that we can perform probabilistic\nsupervised learning by designing a model function from X to\ndistribution parameters Λ.\n• Leveraging the so-obtained parametric conditional distribution\nthen allowed us to compute, not only output probabilities, but\nalso various statistics such as the mean and the variance of the\noutputs.\n• We reviewed the exponential family, a principled generalization\nof numerous distributions, which we saw is tightly connected with\nthe maximum entropy principle.\n• Importantly, the approaches described in this chapter produce per-\nfectly valid computation graphs, meaning that we can combine\nthem with neural networks and we can use automatic differentia-\ntion, to compute their derivatives.\n\nPart II\nDifferentiable programs\n\n4\nParameterized programs\nNeural networks can be thought of as parameterized programs: programs\nwith learnable parameters. In this chapter, we begin by reviewing how to\nrepresent programs mathematically. We then review several key neural\nnetwork architectures and components.\n4.1\nRepresenting computer programs\n4.1.1\nComputation chains\nTo begin with, we consider simple programs that apply a sequence of\nfunctions f1, . . . , fK to an input s0 ∈S0. We call such programs com-\nputation chains. For example, an image may go through a sequence of\ntransformations such as cropping, rotation, normalization, and so on. In\nneural networks, the transformations are typically parameterized, and\nthe parameters are learned, leading to feedforward networks, presented\nin Section 4.2. Another example of sequence of functions is a for loop,\npresented in Section 5.8.\n80\n\n4.1. Representing computer programs\n81\n...\n...\nFigure 4.1: A computation chain is a sequence of function compositions. In the\ngraph above, each intermediate node represents a single function. The first node\nrepresents the input, the last node the output. Edges represent the dependencies of\nthe functions with respect to previous outputs or to the initial input.\nFormally, a computation chain can be written as\ns0 ∈S0\ns1 := f1(s0) ∈S1\n...\nsK := fK(sK−1) ∈SK\nf(s0) := sK.\n(4.1)\nHere, s0 is the input, sk ∈Sk is an intermediate state of the program,\nand sK ∈SK is the final output. Of course, the domain (input space)\nof fk must be compatible with the image (output space) of fk−1. That is,\nwe should have fk : Sk−1 →Sk. We can write the program equivalently\nas\nf(s0) = (fK ◦· · · ◦f2 ◦f1)(s0)\n= fK(. . . f2(f1(s0))).\nA computation chain can be represented by a directed graph, shown\nin Fig. 4.1. The edges in the chain define a total order. The order is\ntotal, since two nodes are necessarily linked to each other by a path.\n4.1.2\nDirected acylic graphs\nIn generic programs, intermediate functions may depend, not only on\nthe previous function output, but on the outputs of several different\nfunctions. Such dependencies are best expressed using graphs.\nA directed graph G = (V, E) is defined by a set of vertices or\nnodes V and a set of edges E defining directed dependencies between\n\n82\nParameterized programs\nFigure 4.2: Example of a directed acyclic graph. Here the nodes are V =\n{0, 1, 2, 3, 4}, the edges are E = {(0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (1, 4), (3, 4)}. Parents\nof the node 3 are pa(3) = {0, 1, 2}. Children of node 1 are ch(1) = {3, 4}. There is\na unique root, 0, and a unique leaf, 4; 0 →3 →4 is a path from 0 to 4. This is an\nacyclic graph since there is no cycle (i.e., a path from a node to itself). We can order\nnodes 0 and 3 as 0 ≤3 since there is no path from 3 to 0. Similarly, we can order 1\nand 2 as 1 ≤2 since there is no path from 2 to 1. Two possible topological orders of\nthe nodes are (0, 1, 2, 3, 4) and (0, 2, 1, 3, 4).\nvertices. An edge (i, j) ∈E is an ordered pair of vertices i ∈V and\nj ∈V. It is also denoted i →j, to indicate that j depends on i. For\nrepresenting inputs and outputs, it will be convenient to use incoming\nhalf-edges →j and outgoing half-edges i →.\nIn a graph G = (V, E), the parents of a vertex j is the set of nodes\npointing to j, denoted pa(j) := {i : i →j}. The children of a vertex i\nis the set of nodes i is pointing to, that is, ch(i) := {j : i →j}. Vertices\nwithout parents are called roots and vertices without children are called\nleaves.\nA path from i to j is defined by a sequence of vertices j1, . . . , jm,\npotentially empty, such that i →j1 →. . . →jm →j. An acyclic graph\nis a graph such that there exists no vertex i with a path from i to i. A\ndirected acyclic graph (DAG) is a graph that is both directed and\nacyclic.\nThe edges of a DAG define a partial order of the vertices, denoted\ni ⪯j if there exists a path from i to j. The order is partial, since\ntwo vertices may not necessarily be linked to each other by a path.\nNevertheless, we can define a total order called a topological order:\nany order such that i ≤j if and only if there is no path from j to i.\n\n4.1. Representing computer programs\n83\nFigure 4.3: Representation of f(x1, x2) = x2ex1√x1 + x2ex1 as a DAG, with\nfunctions and variables as nodes. Edges indicate function and variable dependencies.\nThe function f is decomposed as 8 elementary functions in topological order.\n4.1.3\nComputer programs as DAGs\nWe assume that a program defines a mathematically valid function (a.k.a.\npure function): the program should return identical values for identical\narguments and should not have any side effects. We also assume that the\nprogram halts, i.e., that it terminates in a finite number of steps. As\nsuch a program is made of a finite number of intermediate functions and\nintermediate variables, the dependencies between functions and variables\ncan be expressed using a directed acyclic graph (DAG). Without loss of\ngenerality, we make the following simplifying assumptions:\n1. There is a single input s0 ∈S0.\n2. There is a single output sK ∈SK.\n3. Each intermediate function fk in the program outputs a single\nvariable sk ∈Sk.\nWe number the nodes as V := {0, 1, . . . , K}. Node 0 is the root, corre-\nsponding to the input s0 ∈S0. Node K is the leaf, corresponding to the\nfinal output sK ∈SK. Because of the third assumption above, apart\nfrom s0, each variable sk is in bijection with a function fk. Therefore,\nnode 0 represents the input s0, and nodes 1, . . . , K represent both a\nfunction fk and an output variable sk.\nEdges in the DAG represent dependencies. The parents i1, . . . , ipk :=\npa(k) of node k, where pk := |pa(k)|, indicate the variables spa(k) :=\nsi1, . . . , sipk that the function fk needs to perform its computation. Put\n\n84\nParameterized programs\nAlgorithm 4.1 Executing a program\nFunctions: f1, . . . , fK in topological order\nInput: input s0 ∈S0\n1: for k := 1, . . . , K do\n2:\nRetrieve parent nodes i1, . . . , ipk := pa(k)\n3:\nCompute sk := fk(spa(k)) := fk(si1, . . . , sipk)\n4: Output: f(s0) := sK\ndifferently, the parents i1, . . . , ipk indicate the functions fi1, . . . , fipk\nthat need to be evaluated, prior to evaluating fk.\nAn example of computation graph in our formalism is presented\nin Fig. 4.3.\nExecuting a program\nTo execute a program, we need to ensure that we evaluate the intermedi-\nate functions in the correct order. Therefore, we assume that the nodes\n0, 1, . . . , K are in a topological order (if this is not the case, we need\nto perform a topological sort first). We can then execute a program by\nevaluating for k ∈[K]\nsk := fk(spa(k)) := fk(si1, . . . , sipk) ∈Sk.\nNote that we can either view fk as a single-input function of spa(k),\nwhich is a tuple of elements, or as a multi-input function of si1, . . . , sipk.\nThe two views are essentially equivalent.\nThe procedure for executing a program is summarized in Algo-\nrithm 4.1.\nDealing with multiple program inputs or outputs\nWhen a program has multiple inputs, we can always group them into\ns0 ∈S0 as s0 = (s0,1, . . . , s0,N0) with S0 = (S0,1 × · · · × S0,N0), since\nlater functions can always filter out what elements of s0 they need.\nLikewise, if an intermediate function fk has multiple outputs, we can\nalways group them as a single output sk = (sk,1, . . . , sk,Nk) with Sk =\n\n4.1. Representing computer programs\n85\nFigure 4.4: Two possible representations of a program. Left: Functions and output\nvariables are represented by the same nodes. Right: functions and variables are\nrepresented by a disjoint set of nodes.\n(Sk,1 × · · · × Sk,Nk), since later functions can filter out the elements of\nsk that they need.\nAlternative representation: bipartite graphs\nIn our formalism, because a function fk always has a single output sk, a\nnode k can be seen as representing both the variable sk and the function\nfk. Alternatively, as shown in Fig. 4.4, we can represent variables and\nfunctions as separate nodes, that is, using a bipartite graph. This\nformalism is akin to factor graphs (Frey et al., 1997; Loeliger, 2004)\nused in probabilistic modeling, but with directed edges. One advantage\nof this formalism is that is allows functions to explicitly have multiple\noutputs. We focus on our formalism for simplicity.\n4.1.4\nArithmetic circuits\nArithmetic circuits are one of the simplest examples of computation\ngraph, originating from computational complexity theory. Formally,\nan arithmetic circuit over a field F, such as the reals R, is a directed\nacyclic graph (DAG) whose root nodes are elements of F and whose\nfunctions fk are either + or ×. The latter are often called gates.\nContrary to the general computation graph case, because each fk is\neither + or ×, it is important to allow the graph to have several root\nnodes. Root nodes can be either variables or constants, and should\nbelong to F.\nArithmetic circuits can be used to compute polynomials. There\nare potentially multiple arithmetic circuits for representing a given\npolynomial. One important question is then to find the most efficient\narithmetic circuit for computing a given polynomial. To compare arith-\nmetic circuits representing the same polynomial, an intuitive notion of\n\n86\nParameterized programs\ncomplexity is the circuit size, as defined below.\nDefinition 4.1 (Circuit and polynomial sizes). The size S(C) of a\ncircuit C is the number of edges in the directed acyclic graph\nrepresenting C. The size S(f) of a polynomial f is the smallest S(C)\namong all C representing f.\nFor more information on arithmetic circuits, we refer the reader to\nthe monograph of Chen et al. (2011).\n4.2\nFeedforward networks\nA feedforward network can be seen as a computation chain with pa-\nrameterized functions fk,\ns0 := x\ns1 := f1(s0, w1)\ns2 := f2(s1, w2)\n...\nsK := fK(sK−1, wK),\nfor a given input x ∈X and learnable parameters w1, . . . , wK ∈\nW1 × · · · × WK. Each function fk is called a layer and each sk ∈Sk\ncan be seen as an intermediate representation of the input x. The\ndimensionality of Sk is known as the width (or number of hidden units)\nof layer k. A feedforward network defines a function sK =: f(x, w) from\nX × W to SK, where w := (w1, . . . , wK) ∈W := W1 × . . . × WK.\nGiven such a parameterized program, we can learn the parameters\nby adjusting w to fit some data. For instance, given a dataset of (xi, yi)\npairs, we may minimize the squared loss ∥yi−f(xi, w)∥2\n2 on average over\nthe data, w.r.t. w. The minimization of such a loss requires accessing\nits gradients with respect to w.\n\n4.3. Multilayer perceptrons\n87\n4.3\nMultilayer perceptrons\n4.3.1\nCombining affine layers and activations\nIn the previous section, we did not specify how to parametrize the\nfeedforward network. A typical parametrization, called the multilayer\nperceptron (MLP), uses fully-connected (also called dense) layers of\nthe form\nsk = fk(sk−1, wk) := ak(Wksk−1 + bk),\nwhere we defined the tuple wk := (Wk, bk) and where we assumed that\nWk and bk are a matrix and vector of appropriate size. We can further\ndecompose the layer into two functions. The function s 7→Wks + bk\nis called an affine layer. The function v 7→ak(v) is a parameter-free\nnonlinearity, often called an activation function (see Section 4.4).\nMore generally, we may replace the matrix-vector product Wksk−1\nby any parametrized linear function of sk−1. For example, convolu-\ntional layers use the convolution of an input sk−1 with some filters\nWk, seen as a linear map.\nRemark 4.1 (Dealing with multiple inputs). Sometimes, it is neces-\nsary to deal with networks of multiple inputs. For example, sup-\npose we want to design a function g(x1, x2, wg), where x1 ∈X1\nand x2 ∈X2. A simple way to do so is to use the concatenation\nx := x1 ⊕x2 ∈X2 ⊕X2 as input to a network f(x, wf). Alterna-\ntively, instead of concatenating x1 and x2 at the input layer, they\ncan be concatenated after having been through one or more hidden\nlayers.\n4.3.2\nLink with generalized linear models\nWhen the depth is K = 1 (only one layer), the output of an MLP is\ns1 = a1(W1x + b1).\nThis is called a generalized linear model (GLM); see Section 3.4.\nTherefore, MLPs include GLMs as a special case. In particular, when\na1 is the softargmax (see Section 4.4), we obtain (multiclass) logistic\n\n88\nParameterized programs\nregression. For general depth K, the output of an MLP is\nsK = aK(WKsK−1 + bK).\nThis can be seen as a GLM on top of learned representation sK−1\nof the input x. This is the main appeal of MLPs: they learn the feature\nrepresentation and the output model at the same time! We will see that\nMLPs can also be used as subcomponents in other architectures.\n4.4\nActivation functions\nAs we saw in Section 4.3, feedforward networks typically use an ac-\ntivation function ak at each layer. In this section, we present various\nnonlinearities from scalar to scalar or from vector to scalar. We also\npresent probability mappings that can be used as such activations.\n4.4.1\nReLU and softplus\nMany activations are scalar-to-scalar functions, but they can also\nbe applied to vectors in an element-wise fashion. The ReLU (rectified\nlinear unit) is a popular nonlinearity defined as the non-negative part\nof its input\nrelu(u) := max(u, 0) =\n\n\n\nu,\nu ≥0\n0,\nu < 0\n.\nIt is a piecewise linear function and includes a kink at u = 0. A multilayer\nperceptron with ReLU activations is called a rectifier neural network.\nThe layers take the form\nsk = relu(Aksk−1 + bk),\nwhere the ReLU is applied element-wise. The ReLU can be replaced\nwith a smooth approximation (i.e., without kinks), called the softplus\nsoftplus(u) := log(1 + eu).\nUnlike the ReLU, it is always strictly positive. Other smoothed variants\nof the ReLU are possible, see Section 13.4.\n\n4.4. Activation functions\n89\n4.4.2\nMax pooling and log-sum-exp\nMany activations are vector-to-scalar functions: they reduce vec-\ntors to a scalar value. This scalar value can be seen as a statistic,\n“summarizing” the vector.\nMax pooling\nA simple way to do so is to use the maximum value, also known as max\npooling. Given a vector u ∈RM, it is defined as\nmax(u) := max\nj∈[M] uj.\nLog-sum-exp as a soft maximum\nAs a smooth approximation of it, we can use the log-sum-exp function\nlogsumexp(u) := softmax(u) := log\nM\nX\nj=1\neuj,\nwhich is known to behave like a soft maximum. The log-sum-exp can\nbe seen as a generalization of the softplus, as we have for all u ∈R\nlogsumexp((u, 0)) = softplus(u).\nA numerically stable implementation of the log-sum-exp is given by\nlogsumexp(u) = logsumexp(u −c 1) + c,\nwhere c := maxj∈[M] uj.\nMore generally, we can introduce a temperature parameter γ > 0\nlogsumexpγ(u) = γ · logsumexp(u/γ).\nIt can be shown that for all u ∈RM,\nmax(u) ≤logsumexpγ(u) ≤max(u) + γ · log(M).\nTherefore, logsumexpγ(u) →max(u) as γ →0. Other definitions of\nsoft maximum are possible; see Section 13.5.\n\n90\nParameterized programs\nLog-sum-exp as a log-domain sum\nBesides its use as a soft maximum, the log-sum-exp often arises for\ncomputing sums in the log domain. Indeed, suppose we want to compute\ns := PM\nj=1 ui, where ui > 0. If we define ˜ui := log ui and ˜s := log s, we\nthen have\n˜s = log\nM\nX\nj=1\nexp(˜ui).\nWritten differently, we have the identity\nlog\n M\nX\ni=1\nui\n!\n= logsumexp(log(u)).\nWe can therefore see the log-sum-exp as the sum counterpart of the\nidentity for products\nlog\n\n\nM\nY\nj=1\nui\n\n=\nM\nX\ni=1\nlog(ui).\nAs an example, we use the log-sum-exp to perform the forward-backward\nalgorithm in the log-domain in Section 10.7.1.\n4.4.3\nSigmoids: binary step and logistic functions\nOftentimes, we want to map a real value to a number in [0, 1], that can\nrepresent the probability of an event. For that purpose, we generally\nuse sigmoids. A sigmoid is a function with a characteristic “S”-shaped\ncurve. These functions are scalar-to-scalar probability mappings: they\nare used to squash real values to [0, 1].\nBinary step function\nAn example is the binary step function, also known as Heaviside\nstep function,\nstep(u) :=\n\n\n\n1,\nu ≥0\n0,\nu < 0\n.\nIt is a mapping from R to {0, 1}. Unfortunately, it has a discontinuity: a\njump in its graph at u = 0. Moreover, because the function is constant\n\n4.4. Activation functions\n91\nat all other points, it has zero derivative at these points, which makes it\ndifficult to use as part of a neural network trained with backpropagation.\nLogistic function\nA better sigmoid is the logistic function, which is a mapping from R\nto (0, 1) and is defined as\nlogistic(u) :=\n1\n1 + e−u\n=\neu\n1 + eu\n= 1\n2 + 1\n2 tanh\n\u0012u\n2\n\u0013\n.\nIt maps (−∞, 0) to (0, 0.5),[0, +∞) to [0.5, 1) and it satisfies logistic(0) =\n0.5. It can therefore be seen as mapping from real values to probability\nvalues. The logistic can be seen as a differentiable approximation to the\ndiscontinuous binary step function step(u). The logistic function can\nbe shown to be the derivative of softplus, i.e., for all u ∈R\nsoftplus′(u) = logistic(u).\nTwo important properties of the logistic function are that for all u ∈R\nlogistic(−u) = 1 −logistic(u)\nand\nlogistic′(u) = logistic(u) · logistic(−u)\n= logistic(u) · (1 −logistic(u)).\nOther sigmoids are possible; see Section 13.6.\n4.4.4\nProbability mappings: argmax and softargmax\nIt is often useful to transform a real vector into a vector of probabilities.\nThis is a mapping from RM to the probability simplex, defined by\n△M :=\n\n\nπ ∈RM : ∀j ∈[M], πj ≥0,\nM\nX\nj=1\nπj = 1\n\n\n.\nTwo examples of such vector-to-vector probability mappings are the\nargmax and the softargmax.\n\n92\nParameterized programs\nArgmax\nThe argmax operator is defined by\nargmax(u) := ϕ\n \narg max\nj∈[M]\nuj\n!\n,\nwhere ϕ(j) denotes the one-hot encoding of an integer j ∈[M], that is,\nϕ(j) := (0, . . . , 0, 1\n|{z}\nj\n, 0, . . . , 0) = ej ∈{0, 1}M.\nThis mapping puts all the probability mass onto a single coordinate (in\ncase of ties, we pick a single coordinate arbitrarily). Unfortunately, this\nmapping is a discontinuous function.\nSoftargmax\nAs a differentiable everywhere relaxation, we can use the softargmax\ndefined by\nsoftargmax(u) :=\nexp(u)\nPM\nj=1 exp(uj).\nThis operator is commonly known in the literature as softmax but this\nis a misnomer: this operator really defines a differentiable relaxation\nof the argmax. The output of the softargmax belongs to the relative\ninterior of the probability simplex, meaning that it can never reach the\nborders of the simplex. If we denote π = softargmax(u), this means\nthat πj ∈(0, 1), that is, πj can never be exactly 0 or 1. The softargmax\nis the gradient of log-sum-exp,\n∇logsumexp(u) = softargmax(u).\nThe softargmax can be seen as a generalization of the logistic function,\nas we have for all u ∈R\n[softargmax((u, 0))]1 = logistic(u).\nRemark 4.2 (Degrees of freedom and invertibility of softargmax). The\nsoftargmax operator satisfies the property for all u ∈RM and c ∈R\nπ := softargmax(u) = softargmax(u + c 1).\n\n4.5. Residual neural networks\n93\nThis means that the softargmax operator has M −1 degrees of\nfreedom and is a non-invertible function. However, due to the\nabove property, without loss of generality, we can impose u⊤1 =\nPM\ni=1 ui = 0 (if this is not the case, we simply do ui ←ui −¯u,\nwhere ¯u :=\n1\nM\nPM\nj=1 uj). Using this constraint together with\nlog πi = ui −log\nM\nX\nj=1\nexp(uj),\nwe then obtain\nM\nX\ni=1\nlog πi = −M log\nM\nX\nj=1\nexp(uj)\nso that\nui = [softargmax−1(π)]i = log πi −1\nM\nM\nX\nj=1\nlog πj.\n4.5\nResidual neural networks\nWe now discuss another feedforward network parametrization: residual\nneural networks. Consider a feedforward network with K + 1 layers\nf1, . . . , fK, fK+1. Surely, as long as fK+1 can exactly represent the\nidentity function, the set of functions that this feedforward network\ncan express should be a superset of the functions that f1, . . . , fK can\nexpress. In other words, depth should in theory not hurt the expressive\npower of feedforward networks. Unfortunately, the assumption that each\nfk can exactly represent the identity function may not hold in practice.\nThis means that deeper networks can sometimes be more difficult to\ntrain than shallower ones, making the accuracy saturate or degrade as\na function of depth.\nThe key idea of residual neural networks (He et al., 2016) is to design\nlayers fk, called residual blocks, that make it easier to represent the\nidentity function. Formally, a residual block takes the form\nsk = fk(sk−1, wk) := sk−1 + hk(sk−1, wk).\n\n94\nParameterized programs\nThe function hk is called residual, since it models the difference sk −\nsk−1. The addition with sk−1 is often called a skip connection. As\nlong as it is easy to adjust wk so that hk(sk−1, wk) = 0, fk can freely\nbecome the identity function. For instance, if we use\nhk(sk−1, wk) := Ckak(Wksk−1 + bk) + dk,\nwhere wk := (Wk, bk, Ck, dk), it suffices to set Ck and dk to a zero\nmatrix and vector. Residual blocks are known to remedy the so-called\nvanishing gradient problem.\nMany papers and software packages include an additional activation\nand instead define the residual block as\nsk = fk(sk−1, wk) := ak(sk−1 + hk(sk−1, wk)),\nwhere ak is typically chosen to be the ReLU activation. Whether to\ninclude this additional activation or not is essentially a modelling choice.\nIn practice, residual blocks may also include additional operations such\nas batch norm and convolutional layers.\n4.6\nRecurrent neural networks\nRecurrent neural networks (RNNs) are a class of neural networks that\noperate on sequences of vectors, either as input, output or both. Their\nactual parametrization depends on the setup but the core idea is to\nmaintain a state vector that is updated from step to step by a recursive\nfunction that uses shared parameters across steps. Unrolling this\nrecursion defines a valid computational graph, as we will see in Chapter 8.\nWe distinguish between the following setups illustrated in Fig. 4.5:\n• Vector to sequence (one to many):\nfd : RD × RP →RL×M\n• Sequence to vector (many to one):\nfe : RL×D × RP →RM\n• Sequence to sequence (many to many, aligned):\nfa : RL×D × RP →RL×M\n\n4.6. Recurrent neural networks\n95\n(a) One to many (decoder)\n(b) Many to one (encoder)\n(c) Sequence to sequence aligned\n(d) Sequence to sequence unaligned\nFigure 4.5: Recurrent neural network architectures\n• Sequence to sequence (many to many, unaligned):\nfu : RL×D × RP →RL′×M\nwhere L stands for length. Note that we use the same number of\nparameters P for each setup for notational convenience, but this of\ncourse does not need to be the case. Throughout this section, we use\nthe notation p1:L := (p1, . . . , pL) for a sequence of L vectors.\n4.6.1\nVector to sequence\nIn this setting, we define a decoder function p1:L = fd(x, w) from an\ninput vector x ∈RD and parameters w ∈RP to an output sequence\np1:L ∈RL×M. This is for instance useful for image caption generation,\nwhere a sentence (a sequence of word embeddings) is generated from\nan image (a vector of pixels). Formally, we may define p1:L := fd(x, w)\n\n96\nParameterized programs\nthrough the recursion\nzl := g(x, zl−1, wg)\nl ∈[L]\npl := h(zl, wh)\nl ∈[L].\nwhere w := (wg, wh, z0). The goal of g is to update the current decoder\nstate zl given the input x, and the previous decoder state zl−1. The\ngoal of h is to generate the output pl given the current decoder state\nzl. Importantly, the parameters of g and h are shared across steps.\nTypically, g and h are parametrized using one-hidden-layer MLPs. Note\nthat g has multiple inputs; we discuss how to deal with such cases in\nSection 4.3.\n4.6.2\nSequence to vector\nIn this setting, we define an encoder function p = fe(x1:L, w) from an\ninput sequence x1:L ∈RL×D and parameters w ∈RP to an output\nvector p ∈RM. This is for instance useful for sequence classification,\nsuch as sentiment analysis. Formally, we may define p := fe(x1:L, w)\nusing the recursion\nsl := γ(xl, sl−1, wg)\nl ∈[L]\np = pooling(s1:L)\nwhere w := (wg, s0). The goal of γ is similar as g, except that it updates\nencoder states and does not take previous predictions as input. The\npooling function is typically parameter-less. Its goal is to reduce a\nsequence to a vector. Examples include using the last state, the average\nof states and the coordinate-wise maximum of states.\n4.6.3\nSequence to sequence (aligned)\nIn this setting, we define a function p1:L = fa(x1:L, w) from an in-\nput sequence x1:L ∈RL×D and parameters w ∈RP to an output\nsequence p1:L ∈RL×M, which we assume to be of the same length.\nAn example of application is part-of-speech tagging, where the goal is\nto assign each word xl to a part-of-speech (noun, verb, adjective, etc).\n\n4.6. Recurrent neural networks\n97\nFormally, we may define p1:L = fa(x1:L, w) as\nsl := γ(xl, sl−1, wγ)\nl ∈[L]\npl = h(sl, wh)\nl ∈[L]\nwhere w := (wγ, wh, s0). The function γ and h are similar as before.\n4.6.4\nSequence to sequence (unaligned)\nIn this setting, we define a function p1:L′ = fu(x1:L, w) from an input\nsequence x1:L ∈RL×D and parameters w ∈RP to an output sequence\np1:L′ ∈RL′×M, which potentially has a different length. An example\nof application is machine translation, where the sentences in the source\nand target languages do not necessarily have the same length. Typically,\np1:L′ = fu(x1:L, w) is defined as the following two steps\nc := fe(x1:L, we)\np1:L′ := fd(c, wd)\nwhere w := (we, wd), and where we reused the previously-defined\nencoder fe and decoder fd. Putting the two steps together, we obtain\nsl := γ(xl, sl−1, wγ)\nl ∈[L]\nc = pooling(s1:L)\nzl := g(c, pl−1, zl−1, wg)\nl ∈[L′]\npl := h(zl, wh)\nl ∈[L′].\nThis architecture is aptly named the encoder-decoder architecture.\nNote that we denoted the length of the target sequence as L′. However,\nin practice, the target length can be input dependent and is often not\nknown ahead of time. To deal with this issue, the vocabulary (of size D\nis our notation) is typically augmented with an “end of sequence” (EOS)\ntoken so that, at inference time, we know when to stop generating the\noutput sequence. One disadvantage of this encoder-decoder architecture,\nhowever, is that all the information about the input sequence is contained\nin the context vector c, which can therefore become a bottleneck.\n\n98\nParameterized programs\n4.7\nSummary\n• Programs can be mathematically represented as a directed acyclic\ngraph.\n• Neural networks are parameterized programs.\n• Feedfoward networks are parameterized computation chains.\n• Multilayer perceptrons (MLPs), residual neural networks (ResNets)\nand convolutional neural network (CNNs) are all particular parametriza-\ntions of feedforward networks.\n\n5\nControl flows\nControl flows, such as conditionals or loops, are an essential part of\ncomputer programming, as they allow us to express complex programs.\nIt is therefore natural to ask whether these constructs can be included\nin a differentiable program. This is what we study in this chapter.\n5.1\nComparison operators\nControl flows rely on comparison operators, a.k.a. relational oper-\nators. Formally, we can define a comparison operator π = op(u1, u2)\nas a function from u1 ∈R and u2 ∈R to π ∈{0, 1}. The binary\n(Boolean) output π can then be used within a conditional statement\n(see Section 5.6, Section 5.7) to decide whether to execute one branch\nor another. We define the following operators, illustrated in Fig. 5.1:\n• greater than:\ngt(u1, u2) :=\n\n\n\n1\nif u1 ≥u2\n0\notherwise\n= step(u1 −u2)\n99\n\n100\nControl flows\n• less than:\nlt(u1, u2) :=\n\n\n\n1\nif u1 ≤u2\n0\notherwise\n= 1 −gt(u1, u2)\n= step(u2 −u1)\n• equal:\neq(u1, u2) :=\n\n\n\n1\nif |u1 −u2| = 0\n0\notherwise\n= gt(u2, u1) · gt(u1, u2)\n= step(u2 −u1) · step(u1 −u2)\n• not equal:\nneq(u1, u2) :=\n\n\n\n1\nif |u1 −u2| > 0\n0\notherwise\n= 1 −eq(u1, u2)\n= 1 −step(u2 −u2) · step(u1 −u2),\nwhere step: R →{0, 1} is the Heaviside step function\nstep(u) :=\n\n\n\n1\nif u ≥0\n0\notherwise\n.\nThe Heaviside step function is piecewise constant. At u = 0, the function\nis discontinuous. At u ̸= 0, it is continuous and has null derivative.\nSince the comparison operators we presented are all expressed in terms\nof the step function, they are all continuous and differentiable almost\neverywhere, with null derivative. Therefore, while their derivatives are\nwell-defined almost everywhere, they are uninformative and prevent\ngradient backpropagation.\n\n5.2. Soft inequality operators\n101\n0\n2\n4\nu1\n0\n2\n4\nu2\nu1 greater than u2\n0\n2\n4\nu1\n0\n2\n4\nu2\nu1 equal to u2\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n0\n2\n4\nu1\n0\n2\n4\nu2\nu1 greater than u2\n0\n2\n4\nu1\n0\n2\n4\nu2\nu1 equal to u2\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nSmoothed operators with logistic\nFigure 5.1: The greater than and equal to operators are discontinuous functions,\nleading to black or white pictures. They can be smoothed with appropriate approxi-\nmations of the Heaviside step function.\n5.2\nSoft inequality operators\n5.2.1\nHeuristic definition\nTo obtain a continuous relaxation of inequality operators, we can\nheuristically replace the step function in the expression of “greater\nthan” and “less than” by a sigmoid function sigmoidσ, where σ > 0 is a\nscaling parameter. Such a sigmoid function should satisfy the following\nproperties:\n• sigmoidσ(−u) = 1 −sigmoidσ(u),\n• limu→∞sigmoidσ(u) = 1,\n• limu→−∞sigmoidσ(u) = 0,\n• sigmoidσ(0) = 1\n2.\n\n102\nControl flows\nTwo examples of sigmoids satisfying the aforementioned properties are\nthe logistic function\nsigmoidσ(u) := logisticσ(u) :=\n1\n1 + e−u/σ ∈(0, 1)\nand the standard Gaussian’s CDF\nsigmoidσ(u) := Φ(u/σ).\nWe may then define the soft “greater than”\ngt(µ1, µ2) = step(µ1 −µ2)\n≈sigmoidσ(µ1 −µ2)\n=: gtσ(µ1, µ2)\nand the soft “less than”\nlt(µ1, µ2) = step(µ2 −µ1)\n≈sigmoidσ(µ2 −µ1)\n=: ltσ(µ1, µ2)\n= 1 −sigmoidσ(µ1 −µ2)\n= 1 −gtσ(µ1 −µ2).\nIn the limit, we have that sigmoidσ(µ1 −µ2) →1 when µ1 −µ2 →∞.\nIn the limit, sigmoidσ therefore outputs a value of 1 if µ1 and µ2 are\ninfinitely apart. Besides the logistic function and the standard Gaussian’s\nCDF, other sigmoid functions are possible, as discussed in Section 13.6.\nIn particular, with sparse sigmoids, there exists a finite value τ such\nthat µ1 −µ2 ≥τ =⇒sigmoidσ(µ1 −µ2) = 1.\n5.2.2\nStochastic process perspective\nWhen the sigmoid used to replace the step function is the logistic\nfunction or the standard Gaussian’s CDF, we can revisit the previous\nheuristic definition of gtσ(µ1, µ2) and ltσ(µ1, µ2) from a more formal\nperspective. Indeed, to real values µ1 ∈R and µ2 ∈R, we can associate\nrandom variables\nU1 ∼pµ1,σ1\nand\nU2 ∼pµ2,σ2,\n\n5.2. Soft inequality operators\n103\nthereby forming a stochastic process (we assume that σ1 and σ2 are\nfixed). Alternatively, we can also write\n(U1, U2) ∼pµ1,σ1 ⊗pµ2,σ2,\nwhere for two distributions p1 and p2, we denote p1 ⊗p2 their outer\nproduct (p1 ⊗p2)(u1, u2) := p1(u1)p2(u2). We can then define\ngtσ(µ1, µ2) = E [gt(U1, U2)]\n= E [step(U1 −U2)]\n= P(U1 −U2 > 0)\n= 1 −P(U1 −U2 ≤0)\n= 1 −FU1−U2(0),\nwhere FX is the cumulative distribution function (CDF) of the\nrandom variable X, and σ is a function of σ1 and σ2. Similarly, we\nobtain\nltσ(µ1, µ2) = E [lt(U1, U2)]\n= E [step(U2 −U1)]\n= P(U1 −U2 ≤0)\n= FU1−U2(0).\nWe see that the soft inequality operators are based on the CDF of the\ndifference between U1 and U2.\nFrom a perturbation perspective, we can also define noise variables\nZ1 ∼p0,1 and Z2 ∼p0,1 such that U1 = µ1 + σ1Z1 and U2 = µ2 + σ2Z2\n(Section 12.4.1). We then have\ngtσ(µ1, µ2) = E [gt(µ1 + σ1Z1, µ2 + σ2Z2)]\nltσ(µ1, µ2) = E [lt(µ1 + σ1Z1, µ2 + σ2Z2)] .\nGaussian case\nWhen U1 ∼Normal(µ1, σ2\n1) and U2 ∼Normal(µ2, σ2\n2), we have\nU1 −U2 ∼Normal(µ1 −µ2, σ2\n1 + σ2\n2).\n(5.1)\n\n104\nControl flows\nDenoting Φ the standard Gaussian’s CDF, we then obtain\ngtσ(µ1, µ2) = Φ\n\u0012µ1 −µ2\nσ\n\u0013\nltσ(µ1, µ2) = Φ\n\u0012µ2 −µ1\nσ\n\u0013\n,\nwhere σ :=\nq\nσ2\n1 + σ2\n2. The corresponding distribution for Z1 and Z2 is\nGaussian noise.\nLogistic case\nWhen U1 ∼Gumbel(µ1, σ) and U2 ∼Gumbel(µ2, σ), we have\nU1 −U2 ∼Logistic(µ1 −µ2, σ).\n(5.2)\nWe then obtain (see Proposition 14.3)\ngtσ(µ1, µ2) = logistic\n\u0012µ1 −µ2\nσ\n\u0013\nltσ(µ1, µ2) = logistic\n\u0012µ2 −µ1\nσ\n\u0013\n.\nThe corresponding distribution for Z1 and Z2 is Gumbel noise.\nRecovering hard inequality operators\nWe easily recover the “hard” inequality operator by\ngt(µ1, µ2) = E [gt(U1, U2)] ,\nwhere Ui ∼δµi and where δµi is the delta distribution that assigns a\nprobability of 1 to µi.\n5.3\nSoft equality operators\n5.3.1\nHeuristic definition\nThe equality operator eq(µ1, µ2) can be seen as an extreme kind of\nsimilarity function between numbers, that can only output the values\n0 or 1. To define soft equality operators, a natural idea is therefore\n\n5.3. Soft equality operators\n105\nto replace the equality operator by a more general similarity function.\nA similarity function should achieve its maximum at µ1 = µ2 and it\nshould decrease as µ1 and µ2 move apart. A common family of similarity\nfunctions are kernels. Briefly, a kernel k(µ1, µ2) can be seen as the\ninner product\nk(µ1, µ2) := ⟨ϕ(µ1), ϕ(µ2)⟩\nbetween the embbedings ϕ(µ1) and ϕ(µ2) of µ1 and µ2 in some (po-\ntentially infinite-dimensional) space H, a reproducing kernel Hilbert\nspace to be precise; see Schölkopf and Smola (2002) and Shawe-Taylor\nand Cristianini (2004) for an in-depth review of kernels. To obtain a\nsimilarity measure between 0 and 1 approximating the equality operator,\nwe can normalize to obtain\neq(µ1, µ2) ≈\nk(µ1, µ2)\np\nk(µ1, µ1)k(µ2, µ2)\n= ⟨ϕ(µ1), ϕ(µ2)⟩\n∥ϕ(µ1)∥∥ϕ(µ2)∥,\nwhere ∥ϕ(µ)∥:=\np\n⟨ϕ(µ), ϕ(µ)⟩=\np\nκ(µ, µ). This is the cosine simi-\nlarity between ϕ(µ1) and ϕ(µ2).\nA particular kind of kernel are isotropic kernels of the form\nk(µ1, µ2) := κ(µ1 −µ2),\nthat depend only on the difference between inputs. When the kernel has\na scale parameter σ > 0, we use the notation κσ. We can then define a\nsoft equality operator as\neq(µ1, µ2) ≈eqσ(µ1, µ2) := κσ(µ1 −µ2)\nκσ(0)\n.\nSeveral isotropic kernels can be chosen such as the Gaussian kernel\nκσ(t) := exp\n \n−t2\n2σ2\n!\nor the logistic kernel\nκσ(t) := sech2\n\u0012 t\n2σ\n\u0013\n,\n\n106\nControl flows\n2\n0\n2\n0.00\n0.25\n0.50\n0.75\n1.00\nSoft equal zero\n2\n0\n2\n0.00\n0.25\n0.50\n0.75\n1.00\nSoft greater than zero\nHard\nLogistic\nGaussian\nFigure 5.2: Soft equality and soft greater than operators can be defined as normalized\nkernels (PDF) and as CDF functions, respectively.\nwhere we defined the hyperbolic secant\nsech(u) := 2/(exp(u) + exp(−u)).\nAs their names suggest, these kernels arise naturally from a probabilistic\nperspective, that we present below.\nThe soft equality operators obtained with these kernels are illustrated\nin Fig. 5.2. Intuitively, we replaced a bar located at µ1 = µ2 with a\nbump function. The soft equality operator obtained with the logistic\nkernel coincides with the expression Petersen et al. (2021) arrive at (see\ntheir Eq. 9), in a different manner.\n5.3.2\nStochastic process perspective\nWe again adopt the stochastic process perspective, in which we associate\nrandom variables\nU1 ∼pµ1,σ1\nand\nU2 ∼pµ2,σ2\nto real values µ1 ∈R and µ2 ∈R. However, to handle the equality\noperator, we cannot simply use the expectation of eq(U1, U2) since\nE[eq(U1, U2)] = P(U1 = U2) = 0,\nU1 and U2 being independent continuous variables. While we cannot\nuse the probability of U1 = U2, or equivalently of U1 −U2 = 0, we can\nconsider using the probability density function (PDF) fU1−U2 of U1 −U2\nevaluated at 0. To ensure that the maximum is achieved at 0 with value\n\n5.3. Soft equality operators\n107\n1, we can normalize the PDF to define\neqσ(µ1, µ2) = fU1−U2(0)\nf0(0)\n.\nIt is well-known that the PDF of the sum of two random variables is\nthe convolution of their respectives PDFs. We therefore have\nfU1−U2(t) = (fU1 ∗f−U2)(t)\n=\nZ ∞\n−∞\nfU1(τ)f−U2(t −τ)dτ.\nIn particular, with t = 0, if fX is the PDF of a location-scale family\ndistributed random variable, we obtain\nfU1−U2(0) = (fU1 ∗f−U2)(0)\n=\nZ ∞\n−∞\nfU1(τ)f−U2(−τ)dτ\n=\nZ ∞\n−∞\nfU1(τ)fU2(τ)dτ\n:= ⟨fU1, fU2⟩\n:= k(µ1, µ2).\nWe indeed recover an inner product and therefore a kernel.\nCDF and PDF of absolute difference\nWhile P(U1 = U2) = 0, we can also consider using P(|U1 −U2| ≤ε) =\nF|U1−U2|(ε) as an alternative notion of soft equality. For any random\nvariable X, we have\nF|X|(x) = P(|X| ≤x)\n= P(−x ≤X ≤x)\n= P(X ≤x) −P(X ≤−x)\n= FX(x) −FX(−x).\nTherefore,\nP(|U1 −U2| ≤ε) = FU1−U2(ε) −FU1−U2(ε).\n\n108\nControl flows\nWe can also derive the PDF of |X| as\nf|X|(x) = F ′\nX(x) −F ′\nX(−x)\n= fX(x) + fX(−x)\nand in particular\nf|X|(0) = 2fX(0).\nTherefore\nf|U1−U2|(0) = 2fU1−U2(0),\nfurther justifying using the PDF of U1 −U2 evaluated at 0. When X\nfollows a normal distribution, |X| follows the so-called folded normal\ndistribution.\nGaussian case\nWhen U1 ∼Normal(µ1, σ2\n1) and U2 ∼Normal(µ2, σ2\n2), we obtain from\nEq. (5.1)\nfU1−U2(t) =\n1\n√\n2π exp\n \n−(t −(µ1 −µ2))2\n2(σ2\n1 + σ2\n2)\n!\nso that\neqσ(µ1, µ2) = exp\n \n(µ1 −µ2)2\n2(σ2\n1 + σ2\n2)\n!\n∈[0, 1].\nWe indeed recover κσ(µ1 −µ2)/κσ(0), where κσ is the Gaussian kernel\nwith σ =\nq\nσ2\n1 + σ2\n2. For the CDF of the absolute difference, we obtain\nP(|U1 −U2| ≤ε) = Φ\n\u0012ε −(µ1 −µ2)\nσ\n\u0013\n−Φ\n\u0012−ε −(µ1 −µ2)\nσ\n\u0013\n.\nLogistic case\nWhen U1 ∼Gumbel(µ1, σ) and U2 ∼Gumbel(µ2, σ), recalling that\nsech(u) := 2/(exp(u) + exp(−u)),\nwe obtain from Eq. (5.2)\nfU1−U2(t) = 1\n4σsech2\n\u0012t −(µ1 −µ2)\n2σ\n\u0013\n\n5.3. Soft equality operators\n109\nso that\neqσ(µ1, µ2) = sech2\n\u0012µ1 −µ2\n2σ\n\u0013\n∈[0, 1].\nWe indeed recover κσ(µ1 −µ2)/κσ(0), where κσ is the logistic kernel\nwith σ = σ1 = σ2.\n5.3.3\nGaussian process perspective\nThe previous approach relied on mapping µ1 and µ2 to two indepen-\ndent random variables U1 ∼pµ1,σ1 and U2 ∼pµ2,σ2 (we assume that σ1\nand σ2 are fixed). Instead, we can consider mapping µ1 and µ2 to two\ndependent random variables U1 and U2, whose covariance depends on\nthe similarity between µ1 and µ2. We can do so by using a Gaussian\nprocess (Hida and Hitsuda, 1976).\nA Gaussian process on R is a stochastic process {Uµ : µ ∈R} indexed\nby µ ∈R such that any subset of K random variables (Uµ1, . . . , UµK)\nassociated with (µ1, . . . , µK) ∈R is a multivariate Gaussian random\nvariable. The Gaussian process is characterized by the mean function\nµ 7→E[Uµ], and its covariance function (µi, µj) 7→Cov(Uµi, Uµj). For\nthe mean function, we may simply choose E[Uµ] = µ. For the covariance\nfunction, we need to ensure that the variance of any combination of\nrandom variables in the Gaussian process is non-negative. This property\nis satisfied by kernel functions. We can therefore define\nCov(Uµi, Uµj) := k(µ1, µ2),\nfor some kernel k. Equipped with such a mapping from real numbers\nto random variables, we need a measure of similarity between random\nvariables. A natural choice is their correlation\ncorr(Uµi, Uµj) :=\nCov(Uµi, Uµj)\nq\nVar(Uµi) Var(Uµj)\n∈[0, 1].\nWe therefore obtain\ncorr(Uµi, Uµj) =\nk(µ1, µ2)\np\nk(µ1, µ1)k(µ2, µ2)\n= ⟨ϕ(µ1), ϕ(µ2)⟩\n∥ϕ(µ1)∥∥ϕ(µ2)∥,\n\n110\nControl flows\nwhich coincides with the cosine similarity measure we saw before. In\nthe particular case K = 2 and when kσ(µ1, µ2) = κ(µ1 −µ2), we then\nrecover the previous heuristically-defined soft equality operator\neqσ(µ1, µ2) = corr(Uµ1, Uµ2) = κ(µ1 −µ2)\nκ(0)\n.\n5.4\nLogical operators\nLogical operators can be used to perform Boolean algebra. Formally,\nwe can define them as functions from {0, 1} × {0, 1} to {0, 1}. The and\n(logical conjunction a.k.a. logical product), or (logical disjunction a.k.a.\nlogical addition) and not (logical negation a.k.a. logical complement)\noperators, for example, are defined by\nand(π, π′) :=\n\n\n\n1\nif π = π′ = 1\n0\notherwise\nor(π, π′) :=\n\n\n\n1\nif 1 ∈{π, π′}\n0\notherwise\nnot(π) :=\n\n\n\n0\nif π = 1\n1\nif π = 0\n.\nClassical properties of these operators include\n• Commutativity:\nand(π, π′) = and(π′, π)\nor(π, π′) = and(π′, π)\n• Associativity:\nand(π, and(π′, π′′)) = and(and(π, π′), π′′)\nor(π, or(π′, π′′)) = or(or(π, π′), π′′)\n• Distributivity of and over or:\nand(π, or(π′, π′′)) = or(and(π, π′), and(π, π′′))\n\n5.5. Continuous extensions of logical operators\n111\n• Neutral element:\nand(π, 1) = π\nor(π, 0) = π\n• De Morgan’s laws:\nnot(or(π, π′)) = and(not(π), not(π′))\nnot(and(π, π′)) = or(not(π), not(π′)).\nMore generally, for a binary vector π = (π1, . . . , πK) ∈{0, 1}K,\nwe can define all (universal quantification, ∀) and any (existential\nquantification, ∃) operators, which are functions from {0, 1}K to {0, 1},\nas\nall(π) :=\n\n\n\n1\nif π1 = · · · = πK = 1\n0\notherwise\nand\nany(π) :=\n\n\n\n1\nif 1 ∈{π1, . . . , πK}\n0\notherwise\n.\n5.5\nContinuous extensions of logical operators\n5.5.1\nProbabilistic continuous extension\nWe can equivalently write the and, or and not operators as\nand(π, π′) = π · π′\nor(π, π′) = π + π′ −π · π′\nnot(π) = 1 −π.\nThese are extensions of the previous definitions: we can use them as\nfunctions from [0, 1]×[0, 1] →[0, 1], as illustrated in Fig. 5.3. This means\nthat we can use the soft comparison operators defined in Section 5.2 to\nobtain π, π′ ∈[0, 1]. Likewise, we can define continuous extensions of\n\n112\nControl flows\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nAnd operator\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nFigure 5.3: The Boolean and and or operators are functions from {0, 1} × {0, 1}\nto {0, 1} (corners in the figure) but their continuous extensions and(π, π′) := π · π′\nas well as or(π, π′) := π + π′ −π · π′ define a function from [0, 1] × [0, 1] to [0, 1].\nall and any, which are functions from [0, 1]K to [0, 1], as\nall(π) =\nK\nY\ni=1\nπi\nany(π) = 1 −\nK\nY\ni=1\n(1 −πi).\nFrom a probabilistic perspective, if we let Y and Y ′ to be two\nindependent random variables distributed according to Bernoulli dis-\ntributions with parameter π and π′, then\nand(π, π′) = P(Y = 1 ∩Y ′ = 1) = P(Y = 1) · P(Y ′ = 1)\nor(π, π′) = P(Y = 1 ∪Y ′ = 1)\n= P(Y = 1) + P(Y ′ = 1) −P(Y = 1 ∩Y ′ = 1)\n= P(Y = 1) + P(Y ′ = 1) −P(Y = 1)P(Y ′ = 1)\nnot(π) = P(Y ̸= 1) = 1 −P(Y = 1).\nIn probability theory, these correspond to the product rule of two\nindependent variables, the addition rule, and the complement rule.\nLikewise, if we let Y = (Y1, . . . , YK) ∈{0, 1}K be a random variable\ndistributed according to a multivariate Bernoulli distribution with\n\n5.5. Continuous extensions of logical operators\n113\nparameters π = (π1, . . . , πK), then\nall(π) = P(Y1 = 1 ∩· · · ∩YK = 1)\n=\nK\nY\ni=1\nP(Yi = 1)\nany(π) = P(Y1 = 1 ∪· · · ∪YK = 1)\n= 1 −P(¬(Y1 = 1 ∪· · · ∪YK = 1))\n= 1 −P(Y1 ̸= 1 ∩· · · ∩YK ̸= 1)\n= 1 −\nK\nY\ni=1\n(1 −P(Yi = 1)).\nThese are the chain rule of probability and the addition rule of proba-\nbility for K independent variables.\n5.5.2\nTriangular norms and co-norms\nMore generally, in the fuzzy logic literature (Klir and Yuan, 1995;\nJayaram and Baczynski, 2008), the concepts of triangular norms and\nco-norms have been introduced to provide continuous relaxations of the\nand and or operators, respectively.\nDefinition 5.1 (Triangular norms and conorms). A triangular norm,\na.k.a. t-norm, is a function from [0, 1] × [0, 1] to [0, 1] which is is\ncommutative, associative, neutral w.r.t. 1 and is monotone, meaning\nthat t(π, π′) ≤t(τ, τ ′) for all π ≤τ and π′ ≤τ ′. A triangular\nconorm, a.k.a. t-conorm, is defined similarly but is neutral w.r.t. 0.\nThe previously-defined probabilistic extensions of and and or are\nexamples of triangle norm and conorm. More examples are given in\nTable 5.1. Thanks to the associative property of these operators, we can\ngeneralize them to vectors π ∈[0, 1]K to define continuous extensions\nof the all and any operators, as shown in Table 5.2. For more examples\nand analysis, see for instance van Krieken (2024, Chapters 2 and 3).\n\n114\nControl flows\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nAnd operator\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nExtremum T-Norm\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nAnd operator\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n′\nOr operator\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\nukasiewicz T-Norm\nFigure 5.4: Alternative relaxations of the Boolean and and or operators using\ntriangular norms (t-norms).\nTable 5.1: Examples of triangular norms and conorms, which are continuous\nrelaxations of the and and or operators, respectively. More instances can be obtained\nby smoothing out the min and max operators.\nt-norm (relaxed and)\nt-conorm (relaxed or)\nProbabilistic\nπ · π′\nπ + π′ −π · π′\nExtremum\nmin(π, π′)\nmax(π, π′)\nŁukasiewicz\nmax(π + π′ −1, 0)\nmin(π + π′, 1)\n5.6\nIf-else statements\nAn if-else statement executes different code depending on a condition.\nFormally, we can define the ifelse: {0, 1} × V × V →V function by\nifelse(π, v1, v0) :=\n\n\n\nv1\nif π = 1\nv0\nif π = 0\n(5.3)\n= π · v1 + (1 −π) · v0.\nThe π variable is called the predicate. It is a binary (Boolean) variable,\nmaking the function ifelse undefined if π ̸∈{0, 1}. The function is\ntherefore discontinuous and nondifferentiable w.r.t. π ∈{0, 1}. On the\nother hand, v0 ∈V and v1 ∈V, which correspond to the false and\ntrue branches, can be continuous variables. If π = 1, the function\nis linear w.r.t. v1 and constant w.r.t. v0. Conversely, if π = 0, the\nfunction is linear w.r.t. v0 and constant w.r.t. v1. We now discuss how\nto differentiate through ifelse.\n\n5.6. If-else statements\n115\nTable 5.2: Continuous extensions of the all and any operators.\nAll (∀)\nAny (∃)\nProbabilistic\nQK\ni=1 πi\n1 −QK\ni=1(1 −πi)\nExtremum\nmin(π1, . . . , πK)\nmax(π1, . . . , πK)\nŁukasiewicz\nmax(PK\ni=1 πi −(K −1), 0)\nmin(PK\ni=1 πi, 1)\n5.6.1\nDifferentiating through branch variables\nFor π ∈{0, 1} fixed, ifelse(π, v1, v0) is a valid function w.r.t. v1 ∈V\nand v0 ∈V, and can therefore be used as a node in a computational\ngraph (Section 8.3). Due to the linearity w.r.t. v1 and v0, we obtain\nthat the Jacobians w.r.t. v1 and v0 are\n∂v0ifelse(π, v1, v0) :=\n\n\n\n0\nif π = 1\nI\nif π = 0\n= (1 −π) · I\nand\n∂v1ifelse(π, v1, v0) :=\n\n\n\nI\nif π = 1\n0\nif π = 0\n= π · I,\nwhere I is the identity matrix of appropriate size. Most of the time,\nif-else statements are composed with other functions. Let g1 : U1 →V\nand g0 : U0 →V be differentiable functions. We then define v1 := g1(u1)\nand v0 := g0(u0), where u1 ∈U1 and u0 ∈U0. The composition of\nifelse, g1 and g0 is then the function f : {0, 1} × U1 × U0 →V defined by\nf(π, u1, u0) := ifelse(π, g1(u1), g0(u0))\n= π · g1(u1) + (1 −π) · g0(u0).\nWe obtain that the Jacobians are\n∂u1f(π, u1, u0) = π · ∂g1(u1)\nand\n∂u0f(π, u1, u0) = (1 −π)∂g0(u0).\n\n116\nControl flows\nAs long as g1 and g0 are differentiable functions, we can therefore\ndifferentiate through the branch variables u1 and u0 without any issue.\nMore problematic is the predicate variable π, as we now discuss.\n5.6.2\nDifferentiating through predicate variables\nThe predicate variable π is binary and therefore cannot be differentiated\ndirectly. However, π can be the output of a comparison operator. For\nexample, suppose we want to express the function fh : R × U1 × U0 →V\ndefined by\nfh(p, u1, u0) :=\n\n\n\ng1(u1)\nif p ≥0\ng0(u0)\notherwise\n.\nUsing our notation, this can be rewritten as\nfh(p, u1, u0) := ifelse(gt(p, 0), g1(u1), g0(u0))\n= ifelse(step(p), g1(u1), g0(u0))\n= step(p)g1(u1) + (1 −step(p))g0(u0).\nThe Heaviside step function has a discontinuity at p = 0, but it is\ncontinuous and differentiable with derivative step′(p) = 0 for all p ̸= 0.\nThe function fh therefore has null derivative w.r.t. p ̸= 0,\n∂pfh(p, u1, u0) = ∂1fh(p, u1, u0)\n= step′(p)(g1(u1) −g0(u0))\n= 0.\nIn other words, while fh has well-defined derivatives w.r.t. p for p ̸= 0,\nthe derivatives are uninformative. As another example, let us now\nconsider the function\ngh(u1, u0) := fh(t(u1), u1, u0),\nfor some differentiable function t. This time, u1 influences both the\npredicate and the true branch. Then, using Proposition 2.8, we obtain\n∂u1gh(u1, u0) = ∂t(u1)∂1fh(t(u1), u1, u0) + ∂2fh(t(u1), u1, u0)\n= ∂2fh(t(u1), u1, u0).\nIn other words, the derivatives of the predicate t(u1) do not influence\nthe derivatives of gh.\n\n5.6. If-else statements\n117\n5.6.3\nContinuous relaxations\nFortunately, we recall that\nifelse(π, v1, v0) = π · v1 + (1 −π) · v0.\nThis function is perfectly well-defined, even if π ∈[0, 1], instead of\nπ ∈{0, 1}. That is, this definition is an extension of Eq. (5.3) from the\ndiscrete set {0, 1} to the continuous unit segment [0, 1]. We saw that\ngt(a, b) ≈sigmoid(a −b) ∈[0, 1],\nwhere we use sigmoid to denote a differentiable S-shaped function\nmapping R to [0, 1]. For instance, we can use the logistic function or\nthe standard Gaussian’s CDF. If we now define\nfs(p, u1, u0) := ifelse(sigmoid(p), g1(u1), g0(u0))\n= sigmoid(p)g1(u1) + (1 −sigmoid(p))g0(u0),\n(5.4)\nthe Jacobian becomes\n∂pfs(p, u1, u0) = sigmoid′(p)(g1(u1) −g0(u0)).\nIf sigmoid = logistic, the Jacobian is non-null everywhere, allowing\ngradients to backpropagate through the computational graph. This is\nan example of smoothing by regularization, as studied in Chapter 13.\nProbabilistic perspective\nFrom a probabilistic perspective, we can view Eq. (5.4) as the expecta-\ntion of gi(ui), where i ∈{0, 1} is a binary random variable distributed\naccording to a Bernoulli distribution with parameter π = sigmoid(p):\nfs(p, u1, u0) = Ei∼Bernoulli(sigmoid(p)) [gi(ui)] .\nTaking the expectation over the two possibles branches makes the\nfunction differentiable with respect to p, since sigmoid(p) is differentiable.\nOf course, this comes at the cost of evaluating both branches, instead\nof a single one. The probabilistic perspective suggests that we can also\ncompute the variance if needed as\nVi∼Bernoulli(sigmoid(p)) [gi(ui)]\n=Ei∼Bernoulli(sigmoid(p))\nh\n(fs(p, u1, u0) −gi(ui))2i\n.\n\n118\nControl flows\nifelse\n  \nifelse\n  \nSoft \ncomparison\noperator\nHard \ncomparison\noperator\nFigure 5.5: Computation graphs of programs using if-else statements with either\nhard or soft comparison operators. By using a hard comparison operator (step\nfunction, left panel) the predicate π is a discrete variable (represented by a dashed\nline). Depending on the value (0 or 1) of the predicate π, only one branch (red or blue)\ncontributes to the output. Derivatives along a path of continuous variables (dense\nlines) can be computed. However, discrete variables such as the predicate prevent the\npropagation of meaningful derivatives. By using a soft comparison operator (sigmoid,\nright panel), the predicate is a continuous variable and derivatives with respect to\nthe input p can be taken. In this case both branches (corresponding to g0 and g1)\ncontribute to the output and therefore need to be evaluated.\nThe probabilistic viewpoint also suggests different scales at which a\nsmoothing can be defined as illustrated in Fig. 5.6.\nAnother perspective (Petersen et al., 2021) is based on the logistic\ndistribution. Indeed, if P is a random variable following a logistic\ndistribution with mean p and scale 1, we saw in Remark 3.1 that the\nCDF is P(P ≤0) = logistic(−p) = 1 −logistic(p) and therefore\nfs(p, u1, u0) = ifelse(logistic(p), g1(u1), g0(u0))\n= logistic(p)g1(u1) + (1 −logistic(p))g0(u0)\n= P(P > 0) · g1(u1) + P(P ≤0) · g0(u0).\nRemark 5.1 (Global versus local smoothing). Consider the function\nf(x, y, z) :=\n\n\n\ny\nif a ≤x ≤b\nz\notherwise\n.\nThe derivatives w.r.t. y and z are well-defined. The derivative w.r.t.\nx on the other hand is not well-defined since it involves comparison\noperators and the logical operator and. Using our notation, we can\n\n5.6. If-else statements\n119\nrewrite the function as\nf(x, y, z) = ifelse(and(gt(x, a), lt(x, b)), y, z).\nA local smoothing approach consists in replacing gt and lt by gtσ\nand ltσ locally in the program:\nfloc\nσ (x, y, z) := ifelse(and(gtσ(x, a), ltσ(x, b)), y, z)\n= πaπby + (1 −πaπb)z\nwhere\nπa := sigmoidσ(x −a)\nπb := sigmoidσ(b −x),\nfor any sigmoid function sigmoidσ. A global smoothing approach\ninstead uses the expectation of the entire program\nfglob\nσ\n(x, y, z) := E[ifelse(and(gt(x + σZ, a), lt(x + σZ, b)), y, z)]\n= ifelse(π, y, z)\nwhere\nπ := E[and(gt(x + σZ, a), lt(x + σZ, b))]\n= P(a ≤x + σZ ≤b)\n= sigmoidσ(b −x) −sigmoidσ(a −x)\n= πb −πa,\nfor sigmoidσ the CDF of σZ. We therefore obtain\nfglob\nσ\n(x, y, z) = (πb −πa)y + (1 −(πb −πa))z.\nThe difference stems from the fact that the local approach smoothes\nout a ≤x and x ≤b independently (treating 1X≥a and 1X≤b as\nindependent random variables), while the global approah smoothes\nout a ≤x ≤b simultaenously. In practice, both approaches ap-\nproximate the original function well as σ →0 and coincide for σ\nsufficiently small as illustrated in Fig. 5.6.\n\n120\nControl flows\n2\n0\n2\n0\n1\n= 1.0\n2\n0\n2\n0\n1\n= 0.5\n2\n0\n2\n0\n1\n= 0.1\nOrignal function\nLocally smoothed\nGlobally smoothed\nFigure 5.6: Global versus local smoothing approaches on a gate function f(x) := 1\nif x ∈[−1, 1], and f(x) := 0 otherwise. In our notation, we can write f(x) =\nifelse(and(gt(x, −1), lt(x, 1)), 1, 0). A local approach smoothes out gt and lt separately.\nA global approach uses the expectation of the whole program, see Remark 5.1. We\nobserve that, though the approaches differ for large σ, they quickly coincide for\nsmaller σ.\n5.7\nElse-if statements\nIn the previous section, we focused on if-else statements: conditionals\nwith only two branches. We now generalize our study to conditionals\nincluding else-if statements, that have K branches.\n5.7.1\nEncoding K branches\nFor conditionals with only 2 branches, we encoded the branch that\nthe conditional needs to take using the binary variable π ∈{0, 1}. For\nconditionals with K branches, we need a way to encode which of the K\nbranches the conditional needs to take. To do so, we can use a vector\nπ ∈{e1, . . . , eK}, where ei denotes the standard basis vector (a.k.a.\none-hot vector)\nei := (0, . . . , 1\n|{z}\ni\n, . . . , 0),\na vector with a single one in the coordinate i and K −1 zeros. The\nvector ei is the encoding of a categorical variable i ∈[K].\n\n5.7. Else-if statements\n121\nCombining booleans\nTo form, such a vector π ∈{e1, . . . , eK}, we can combine the previously-\ndefined comparison and logical operators to define π = (π1, . . . , πK).\nHowever, we need to ensure that only one πi is non-zero. We give an\nexample in Example 5.1.\nArgmax and argmin operators\nAnother way to form π is to use the argmax and argmin operators\nargmax(p) :=\narg max\nπ∈{e1,...,eK}\n⟨π, p⟩\nargmin(p) :=\narg min\nπ∈{e1,...,eK}\n⟨π, p⟩= argmax(−p).\nThey can be seen as a natural generalization of the greater than and\nless than operators. In case of ties, we break them arbitrarily.\n5.7.2\nConditionals\nWe can now express a conditional statement as the function\ncond: {e1, . . . , eK} × VK →V defined by\ncond(π, v1, . . . , vK) :=\n\n\n\n\n\n\n\n\n\nv1\nif π = e1\n...\nvK\nif π = eK\n(5.5)\n=\nK\nX\ni=1\nπivi.\nSimilarly as for the ifelse function, the cond function is discontinuous\nand nondifferentiable w.r.t. π ∈{e1, . . . , eK}. However, given π = ei\nfixed for some i, the function is linear in vi and constant in vj for j ̸= i.\nWe illustrate how to express a simple example, using this formalism.\nExample 5.1 (Soft-thresholding operator). The soft-thresholding op-\nerator (see also Section 16.4) is a commonly-used operator to pro-\n\n122\nControl flows\nmote sparsity. It is defined by\nSoftThreshold(u, λ) :=\n\n\n\n\n\n\n\n0\nif |u| ≤λ\nu −λ\nif u ≥λ\nu + λ\nif u ≤−λ\n.\nTo express it in our formalism, we can define π ∈{e1, e2, e3} using\ncomparison operators as\nπ := (lt(|u|, λ), gt(u, λ), lt(u, −λ))\n= (step(λ −|u|), step(u −λ), step(−u −λ)).\nEquivalently, we can also define π using an argmax operator as\nπ := argmax((λ −|u|, u −λ, −u −λ)).\nIn case of ties, which happens at |u| = λ, we keep only one non-zero\ncoordinate in π. We can then rewrite the operator as\nSoftThreshold(u, λ) = cond(π, 0, u −λ, u + λ).\nAs we will see, replacing argmax with softargmax induces a cat-\negorical distribution over the three possible branches. The mean\nvalue can be seen as a smoothed out version of the operator, and we\ncan also compute the standard deviation, as illustrated in Fig. 5.7.\n5.7.3\nDifferentiating through branch variables\nFor π fixed, cond(π, v1, . . . , vK) is a valid function w.r.t. vi, and can\ntherefore again be used as a node in a computational graph. Due to the\nlinearity w.r.t. vi, we obtain that the Jacobian w.r.t. vi is\n∂vicond(π, v1, . . . , vK) :=\n\n\n\nI\nif π = ei\n0\nif π ̸= ei\n.\nLet gi : Ui →V be a differentiable function and ui ∈Ui. If we define\nthe composition\nf(π, u1, . . . , uK) := cond(π, g1(u1), . . . , gK(uK)),\n\n5.7. Else-if statements\n123\n4\n2\n0\n2\n4\n6\n4\n2\n0\n2\n4\n6\nMean\nStandard deviation\nHard\nFigure 5.7: A conditional with three branches: the soft-thresholding operator (see\nExample 5.1). It is a piecewise linear function (dotted black line). Using a softargmax,\nwe can induce a categorical probability distribution over the three branches. The\nexpected value (blue line) can be seen as a smoothed out version of the operator.\nThe induced distribution allows us to also compute the standard deviation.\nwe then obtain that the Jacobian w.r.t. ui is\n∂uif(π, u1, . . . , uK) :=\n\n\n\n∂gi(ui)\nif π = ei\n0\nif π ̸= ei\n.\nAs long as the gi functions are differentiable, we can therefore differen-\ntiate through the branch variables ui for π fixed.\n5.7.4\nDifferentiating through predicate variables\nAs we saw, π can be obtained by combining comparison and logical\noperators, or it can be obtained by argmax and argmin operators.\nWe illustrate here why these operators are problematic. For example,\nsuppose we want to express the function\nfa(p, u1, . . . , uK) :=\n\n\n\n\n\n\n\n\n\nv1\nif p1 = maxj pj\n...\nvK\nif pK = maxj pj\n.\nIn our notation, this can be expressed as\nfa(p, u1, . . . , uK) := cond(argmax(p), g1(u1), . . . , gK(uK)),\n\n124\nControl flows\nAs for the ifelse case, the Jacobian w.r.t. p is null almost everywhere,\n∂pfa(p, u1, . . . , uK) = 0.\n5.7.5\nContinuous relaxations\nSimilarly to the Heaviside step function, the argmax and argmin func-\ntions are piecewise constant, with discontinuities in case of ties. Their\nJacobian are zero almost everywhere, and undefined in case of ties.\nTherefore, while their Jacobian is well-defined almost everywhere, they\nare uninformative and prevent gradient backpropagation. We can replace\nthe argmax with a softargmax\nsoftargmax(p) :=\nexp(p)\nPK\ni=1 exp(pi) ∈△K\nand similarly\nsoftargmin(p) := softargmax(−p) ∈△K.\nOther relaxations of the argmax are possible, as discussed in Section 13.7.\nSee also Section 14.5.3 for the perturbation perspective.\nFortunately, the definition\ncond(π, v1, . . . , vK) =\nK\nX\ni=1\nπivi\nis perfectly valid if we use π ∈△K instead of π ∈{e1, . . . , eK}, and\ncan therefore be seen as an extension of Eq. (5.5). If we now define\nfs(p, u1, . . . , uK) := cond(softargmax(p), g1(u1), . . . , gK(uK))\n=\nK\nX\ni=1\n[softargmax(p)]i · gi(ui),\n(5.6)\nthe Jacobian becomes\n∂pfs(p, u1, . . . , uK) = ∂softargmax(p)(g1(u1), . . . , gK(uK)),\nwhich is non-null everywhere, allowing gradients to backpropagate\nthrough the computational graph.\n\n5.8. For loops\n125\nProbabilistic perspective\nFrom a probabilistic perspective, we can view Eq. (5.6) as the ex-\npectation of gi(ui), where i ∈[K] is a categorical random variable\ndistributed according to a categorical distribution with parameter\nπ = softargmax(p):\nfs(p, u1, . . . , uK) = Ei∼Categorical(softargmax(p)) [gi(ui)] .\nTaking the expectation over the K possible branches makes the function\ndifferentiable with respect to p, at the cost of evaluating all branches,\ninstead of a single one. Similarly as for the if-else case, we can compute\nthe variance if needed as\nVi∼Categorical(softargmax(p)) [gi(ui)]\n=Ei∼Categorical(softargmax(p))\nh\n(fs(p, u1, . . . , uK) −gi(ui))2i\n.\nThis is illustrated in Fig. 5.7.\n5.8\nFor loops\nFor loops are a control flow for sequentially calling a fixed number K\nof functions, reusing the output from the previous iteration. In full\ngenerality, a for loop can be written as follows.\nAlgorithm 5.1 r = forloop(s0)\nfor k := 1, . . . , K do\nsk := fk(sk−1)\nr := sK\nAs illustrated in Fig. 5.8, this defines a computation chain. Assuming\nthe functions fk are all differentiable, this defines a valid computation\ngraph, we can therefore use automatic differentiation to differentiate\nforloop w.r.t. its input s0. Feedforward networks, reviewed in Section 4.2,\ncan be seen as parameterized for loops, i.e.,\nfk(sk−1) := gk(sk−1, wk),\nfor some differentiable function gk.\n\n126\nControl flows\nExample 5.2 (Unrolled gradient descent). Suppose we want to min-\nimize w.r.t. w the function\nL(w, λ) := 1\nN\nN\nX\ni=1\nℓ(h(xi, w), yi) + λ\n2 ∥w∥2\n2.\nGiven an initialization w0, gradient descent (Section 16.1) performs\niterations of the form\nwk = f(wk−1, γk, λ) := wk−1 −γk∇1L(wk−1, λ).\nGradient descent can therefore be expressed as a for loop with\nfk(wk−1) := f(wk−1, γk, λ).\nThis means that we can differentiate through the iterations of\ngradient descent, as long as f is differentiable, meaning that L is\ntwice differentiable. This is useful for instance to perform gradient-\nbased optimization of the hyperparameters γk or λ. This a special\ncase of bilevel optimization; see also Chapter 11.\nExample 5.3 (Bubble sort). Bubble sort is a simple sorting algo-\nrithm that works by repeatedly swapping elements if necessary.\nMathematically, swapping two elements i and j can be written as\na function from RN × [N] × [N] to RN defined by\nswap(v, i, j) := v + (vj −vi)ei + (vi −vj)ej.\nWe can then write bubble sort as\nfor i := 1, . . . , N do\nfor j := 1, . . . , N −i −1 do\nv′ := swap(v, j, j + 1)\nπ := step(vj −vj+1)\nv ←ifelse(π, v′, v)\nReplacing the Heaviside step function with the logistic function\ngives a smoothed version of the algorithm.\n\n5.9. Scan functions\n127\n...\nFigure 5.8: A for loop forms a com-\nputation chain. A feed forward network\ncan be seen as a parameterized for loop,\nwhere each function fk depends on some\nparameters wk.\n...\nFigure 5.9: Computation graph of the\nscan function. Sequence-to-sequence\nRNNs can be seen as a parameterized\nscan function.\n5.9\nScan functions\nScan is a higher-order function (meaning a function of a function)\noriginating from functional programming. It is useful to perform an\noperation f on individual elements uk while carrying the result sk of\nthat operation to the next iteration.\nAlgorithm 5.2 r = scan(s0, u1, . . . , uK)\nfor k := 1, . . . , K do\nsk, vk := f(sk−1, uk)\nr := (sK, v1, . . . , vK)\nAs illustrated in Fig. 5.9, this again defines a valid computational\ngraph and can be differentiated through using autodiff, assuming the\nfunction f is differentiable. Sequence-to-sequence RNNs, reviewed in\nSection 4.6, can be seen as a parameterized scan. An advantage\nof this abstraction is that parallel scan algorithms have been studied\nextensively in computer science (Blelloch, 1989; Sengupta et al., 2010).\nExample 5.4 (Prefix sum). Scan can be seen as a generalization of\nthe prefix sum (a.k.a. cumulated sum) from the addition to any\n\n128\nControl flows\nbinary operation. Indeed, a prefix sum amounts to perform\nv1 := u1\nv2 := u1 + u2\nv3 := u1 + u2 + u3\n...\nwhich can be expressed as a scan by defining\nvk := sk−1 + uk\nf(sk−1, uk) := (vk, vk)\nstarting from s0 = 0 (sK and vK are redundant in this case).\n5.10\nWhile loops\n5.10.1\nWhile loops as cyclic graphs\nA while loop is a control flow used to repeatedly perform an operation,\nreusing the output of the previous iteration, until a certain condition is\nmet. Suppose f : S →{0, 1} is a function to determine whether to stop\n(π = 1) or continue (π = 0) and g: S →S is a function for performing\nan operation. Then, without loss of generality, a while loop can be\nwritten as follows.\nAlgorithm 5.3 r = whileloop(s)\nπ ←f(s)\nwhile π = 0 do\ns ←g(s)\nπ ←f(s)\nr := s\nThis definition is somewhat cyclic, as we used the while keyword.\nHowever, we can equivalently rewrite the algorithm recursively.\n\n5.10. While loops\n129\nifelse\nFigure 5.10: A while loop can be represented as a cyclic graph. The while loop\nstops if π = 1 and performs another iteration s ←g(s), π ←f(s) if π = 0.\nAlgorithm 5.4 r = whileloop(s)\nπ := f(s)\nif π = 0 then\nr := s\nelse\nr := whileloop(g(s))\nUnlike for loops and scan, the number of iterations of while loops is\nnot known ahead of time, and may even be infinite. In this respect, a\nwhile loop can be seen as a cyclic graph, as illustrated in Fig. 5.10.\nImportance of lazy evaluation\nWe can also implement Algorithm 5.4 in terms of the ifelse function\ndefined in Section 5.6 as\nr := ifelse(f(s), s, whileloop(g(s)))\n= f(s) · s + (1 −f(s)) · whileloop(g(s)).\nHowever, to avoid an infinite recursion, it is crucial that ifelse supports\nlazy evaluation. That is, whileloop(g(s)) in the definition above should\nbe evaluated if and only if π = f(s) = 0. In other words, the fact that\nf(s) ∈{0, 1} is crucial to ensure that the recursion is well-defined.\n5.10.2\nUnrolled while loops\nTo avoid the issues with unbounded while loops, we can enforce that a\nwhile loop stops after T iterations, i.e., we can truncate the while loop.\n\n130\nControl flows\nUnrolling Algorithm 5.4 gives (here with T = 3)\nπ0 := f(s0)\nif π0 = 1 then\nr := s0\nelse\ns1 := g(s0), π1 := f(s1)\nif π1 = 1 then\nr := s1\nelse\ns2 := g(s1), π2 := f(s2)\nif π2 = 1 then\nr := s2\nelse\nr := s3 := g(s2)\nUsing the ifelse function, we can rewrite it as\nr = ifelse(π0,\ns0,\nifelse(π1,\ns1,\nifelse(π2,\ns2,\ns3)))\nwhich is itself equivalent to\nr = π0s0 + (1 −π0) [π1s1 + (1 −π1) [π2s2 + (1 −π2)s3]]\n= π0s0 + (1 −π0)π1s1 + (1 −π0)(1 −π1)π2s2 + (1 −π0)(1 −π1)(1 −π2)s3.\n\n5.10. While loops\n131\nMore generally, for T ∈N, the formula is\nr =\nT\nX\ni=0\n((1 −π0) . . . (1 −πi−1)) πisi\n=\nT\nX\ni=0\n\n\ni−1\nY\nj=0\n(1 −πj)\n\nπisi,\nwhere we defined\nsi := g(si−1) := gi(s0) := g ◦· · · ◦g\n|\n{z\n}\ni times\n(s0) ∈S\nπi := f(si) ∈{0, 1}.\nSee also (Petersen et al., 2021). If we further define the shorthand\nnotation\n˜π0 := π0\n˜πi :=\n\n\ni−1\nY\nj=0\nπj\n\nπi\ni ∈{1, . . . , T},\nso that ˜π := (˜π0, ˜π1, . . . , ˜πT ) ∈△T+1 is a discrete probability distribu-\ntion containing the probabilities to stop at each of the T iterations, we\ncan rewrite the output of a truncated while using a conditional,\nr = cond(˜π, s0, s1, . . . , sT ).\nThis is illustrated in Fig. 5.11.\nExample 5.5 (Computing the square root using Newton’s method).\nComputing the square root √x of a real number x > 0 can be cast\nas a root finding problem, which we can solve using Newton’s\nmethod. Starting from an initialization s0, the iterations read\nsi+1 := g(si) := 1\n2\n\u0012\nsi + x\nsi\n\u0013\n.\nTo measure the error on iteration i, we can define\nε(si) := 1\n2(s2\ni −x)2.\n\n132\nControl flows\ncond\n...\n...\n...\nFigure 5.11: Computation graph of an unrolled truncated while loop. As in Fig. 5.5,\nwe depict continuous variables in dense lines and discrete variables in dashed lines.\nThe output of a while loop with at most T iterations can be written as a conditional\nwith T + 1 branches, cond(˜π, s0, . . . , sT ) = PT\nt=0 ˜πtst.\nAs a stopping criterion, we can then use\nπi :=\n\n\n\n1\nif ε(si) ≤τ\n0\notherwise\n= step(τ −ε(si)),\nwhere 0 < τ ≪1 is an error tolerance and step is the Heaviside\nstep function.\n5.10.3\nMarkov chain perspective\nGiven the function g: S →S and the initialization s0 ∈S, a while\nloop can only go through a discrete set of values s0, s1, s2, . . . defined\nby si = g(si−1). This set is potentially countably infinite if the while\nloop is unbounded, and finite if the while loop is guaranteed to stop.\nWhether the loop moves from the state si to the state si+1, or stays\nat si, is determined by the stopping criterion πi ∈{0, 1}. To model\nthe state of the while loop, we can then consider a Markov chain\nwith a discrete space {s0, s1, s2, . . . }, which we can always identify with\n\n5.10. While loops\n133\n{0, 1, 2, . . . }, with transition probabilities\nP(St+1 = si|St = sj) = pi,j :=\n\n\n\n\n\n\n\nπi\nif i = j\n(1 −πi)\nif i = j + 1\n0\notherwise\n,\nand initial state S0 = s0. Here, St is the value at iteration t of the loop.\nNote that since πi ∈{0, 1}, the pi,j values are “degenerate” probabilities.\nHowever, this framework lets us generalize to a smooth version of the\nwhile loop naturally. To illustrate the framework, if the while loop stops\nat T = 3, the transition probabilities can be cast as a matrix\nP := (pi,j)T\ni,j=0 :=\ns0\ns1\ns2\ns3\n\n\n\n\n\n\n\n\ns0\n0\n1\n0\n0\ns1\n0\n0\n1\n0\ns2\n0\n0\n0\n1\ns3\n0\n0\n0\n1\n.\nThe output r of the while-loop is determined by the time at which the\nstate stays at the same value\nI = min{i ∈{1, 2, . . .} s.t. Si = Si−1}.\nNote that I itself is a random variable, as it is defined by the Si variables.\nIt is called a stopping time. The output of the chain is then\nr = E[SI]\n=\n+∞\nX\ni=1\nP(I = i)E[Si|I = i]\n=\n+∞\nX\ni=1\nP(I = i)si−1\n=\n+∞\nX\ni=1\ni−2\nY\nj=0\n(1 −πj)πi−1si−1\n=\n+∞\nX\ni=0\ni−1\nY\nj=0\n(1 −πj)πisi.\n\n134\nControl flows\nBecause the stopping time is not known ahead of time, the sum over i\ngoes from 0 to ∞. However, if we enforce in the stopping criterion that\nthe while loop runs no longer than T iterations, by setting\nπi := or(f(si), eq(i, T)) ∈{0, 1},\nwe then naturally recover the expression found by unrolling the while\nloop before,\nr = E[SI] =\nT\nX\ni=0\ni−1\nY\nj=0\n(1 −πj)πisi.\nFor example, with T = 3, the transition probability matrix is\nP =\ns0\ns1\ns2\ns3\n\n\n\n\n\n\n\n\ns0\nπ0\n1 −π0\n0\n0\ns1\n0\nπ1\n1 −π1\n0\ns2\n0\n0\nπ2\n1 −π2\ns3\n0\n0\n0\n1\n.\nSmoothed while loops\nWith the help of this framework, we can backpropagate even through the\nwhile loop’s stopping criterion, provided that we smooth out the predi-\ncate. For example, we saw that the stopping criterion in Example 5.5 is\nf(si) = step(τ −ε(si)) and therefore\nπi := or(f(si), eq(i, T)) ∈{0, 1}.\nDue to the step function, the derivative of the while loop with respect\nto τ will always be 0, just like it was the case for if-else statements. If\nwe change the stopping criterion to f(si) = sigmoid(τ −ε(si)), we then\nhave (recall that or is well defined on [0, 1] × [0, 1])\nπi := or(f(si), eq(i, T)) ∈[0, 1].\nWith sigmoid, we obtain more informative derivatives. In particular,\nwith sigmoid = logistic, the derivatives w.r.t. τ are always non-zero.\n\n5.11. Summary\n135\nThe smoothed output is expressed as before as the expectation\nr = E[SI] =\nT\nX\ni=0\ni−1\nY\nj=0\n(1 −πj)πisi\n=\nT\nX\ni=0\ni−1\nY\nj=0\n(1 −sigmoid(ε(si) −τ))sigmoid(ε(si) −τ)si.\nInstead of enforcing a number T of iterations, it is also possible to stop\nwhen the probability of stopping becomes high enough (Petersen et al.,\n2021), assuming that the probability of stopping converges to 1.\n5.11\nSummary\n• For conditionals, we saw that differentiating through the branch\nvariables is not problematic given a fixed predicate.\n• However, for the predicate variable, we saw that a differentiable\nrelaxation is required to avoid null derivatives.\n• We introduced soft comparison operators in a principled manner,\nusing a stochastic process perspective, as well as the continuous\nextension of logical operators.\n• For loops and scan define valid computational graphs, as their\nnumber of iterations is fixed ahead of time. Feedforward networks\nand RNNs can be seen as parameterized for loops and scan,\nrespectively.\n• Unlike for loops and scan, the number of iterations of while loops\nis not known ahead of time and may even be infinite. However,\nunrolled while loops define valid directed acyclic graphs. We\ndefined a principled way to differentiate through the stopping\ncriterion of a while loop, thanks to a Markov chain perspective.\n\n6\nData structures\nIn computer science, a data structure is a specialized format for organiz-\ning, storing and accessing data. Mathematically, a data structure forms\na so-called algebraic structure: it consists of a set and the functions\nto operate on that set. In this chapter, we review how to incorporate\ndata structures into differentiable programs, with a focus on lists and\ndictionaries.\n6.1\nLists\nA list is an ordered sequence of elements. We restrict ourselves to lists\nwhose elements all belong to the same value space V. Formally, we\ndenote a list of fixed length K with values in V by a K-tuple\nl := (l1, . . . , lK) ∈LK(V)\nwhere each li ∈V and where\nLK(V) := VK = V × · · · × V\n|\n{z\n}\nK times\n.\n136\n\n6.1. Lists\n137\n6.1.1\nBasic operations\nGetting values\nWe first present how to retrieve values from a list l ∈LK(V). We define\nthe function list.get: LK(V) × [K] →V as\nlist.get(l, i) := li.\nThe function is continuous and differentiable in l ∈LK(V) but not in\ni ∈[K], as it is a discrete variable. In the particular case V = R, LK(V)\nis equivalent to RK and we can therefore write\nlist.get(l, i) = ⟨l, ei⟩,\nwhere {e1, . . . , eK} is the standard basis of RK.\nSetting values\nWe now present how to replace values from a list l ∈LK(V). We define\nthe function list.set: LK(V) × [K] × V →LK(V) as\n[list.set(l, i, v)]j :=\n\n\n\nv\nif i = j\nlj\nif i ̸= j\n,\nfor j ∈[K]. In the functional programming spirit, the function returns\nthe whole new list, even though a single element has been modified.\nAgain, the function is continuous and differentiable in l ∈LK(V) and\nv ∈V but not in i ∈[K]. In the particular case V = R, given a list\nl = (l1, . . . , lK), we can write\nlist.set(l, i, v) = (v −li)ei.\nThat is, we subtract the old value li and add the new value v at the\nlocation i ∈[K].\nImplementation\nA fixed-length list can be implemented as an array, which enables O(1)\nrandom access to individual elements. The hardware counterpart of\nan array is random access memory (RAM), in which memory can be\nretrieved by address (location).\n\n138\nData structures\n6.1.2\nOperations on variable-length lists\nSo far, we focused on lists of fixed length K. We now turn our attention\nto variable-length lists, whose size can decrease or increase over time.\nIn addition to the list.get and list.set functions, they support functions\nthat can change the size of a list.\nInitializing lists\nIn order to initialize a list, we define list.init: V →L1(V) as\nlist.init(v) := (v),\nwhere used (v) to denote a 1-tuple.\nPushing values\nIn order to add new values either to the left or to the right, we define\nlist.pushLeft: LK(V) × V →LK+1(V) as\nlist.pushLeft(l, v) := (v, l1, . . . , lK).\nand list.pushRight: LK(V) × V →LK+1(V) as\nlist.pushRight(l, v) := (l1, . . . , lK, v).\nPopping values\nIn order to remove values either from the left or from the right, we\ndefine list.popLeft: LK(V) →LK−1(V) × V as\nlist.popLeft(l) := (l2, . . . , lK), l1\nand list.popRight: LK(V) →LK−1(V) × V as\nlist.popRight(l) := (l1, . . . , lK−1), lK.\nThe set L0(V) is a singleton which contains the empty list.\n\n6.1. Lists\n139\nInserting values\nThe pushLeft and pushRight functions can only insert values at the\nbeginning and at the end of a list, respectively. We now study the insert\nfunction, whose goal is to be able to add a new value at an arbitrary\nlocation, shifting all values to the right and increasing the list size by 1.\nWe define the function list.insert : LK(V) × [K + 1] × V →LK+1(V) as\n[list.insert(l, i, v)]j :=\n\n\n\n\n\n\n\nlj\nif j < i\nv\nif j = i\nlj−1\nif j > i\n,\nfor j ∈[K+1]. As for the list.set function, list.insert is readily continuous\nand differentiable in l and v, but not in i, as it is a discrete variable.\nAs special cases, we naturally recover\nlist.insert(l, 1, v) = pushLeft(l, v),\nlist.insert(l, K + 1, v) = pushRight(l, v).\nDifferentiability\nThe list.init, list.push and list.pop functions are readily continuous and\ndifferentiable with respect to their arguments (a continuous relaxation\nis not needed). As for the list.set function, the list.insert function is\ncontinuous and differentiable in l and v, but not in i.\nImplementation\nUnder the hood, a variable-length list can be implemented as a linked\nlist or as a dynamic array. A linked list gives O(K) random access while\na dynamic array allows O(1) random access, at the cost of memory\nreallocations.\nStacks and queues\nThe list.pushRight and list.popRight functions can be used to implement\na stack (last in first out a.k.a. LIFO behavior). The list.pushLeft and\nlist.popRight functions can be used to implement a queue (first in first\nout a.k.a. FIFO behavior).\n\n140\nData structures\n6.1.3\nContinuous relaxations using soft indexing\nGetting values\nIn order to be able to differentiate list.get w.r.t. indexing, a natural\nidea is to replace the integer index i ∈[K] by a distribution πi ∈△K,\nwhich we can interpret as a soft index. An integer index i ∈[K] is\nthen equivalent to a delta distribution πi ∈{e1, . . . , eK}. We define\nthe continuous relaxation list.softGet: LK(V) × △K →conv(V) as\nlist.softGet(l, πi) :=\nK\nX\nj=1\nπi,jlj\n= cond(πi, l1, . . . , lK)\n= EI∼Categorical(πi)[lI],\nwhere cond is studied in Section 5.7. In the particular case V = R, we\nobtain\nlist.softGet(l, i) = ⟨l, πi⟩.\nThis is illustrated in Fig. 6.1.\nThe choice of the distribution πi = (πi,1, . . . , πi,K) encodes the\nimportance of the elements (l1, . . . , lK) w.r.t. li. If we consider that the\nsmaller |i −j| is, the more related li and lj are, then it makes sense\nto define a distribution centered around i (i.e., such that the mode of\nthe distribution is achieved at i). For example, limiting ourselves to the\nneighbors li−1 and li+1 (i.e., a window of size 1), we can define the\nsparse distribution\nπi := 1\n4 · ei−1 + 1\n2ei + 1\n4 · ei+1 ∈△K.\nIn this particular case, the continuous relaxation of the list.get function\ncan then be expressed as a discrete convolution,\nlist.softGet(l, πi) = (list.get(l, ·) ∗κ) (i) =\n∞\nX\nj=−∞\nlist.get(l, i −j)κ(j),\nwhere κ(−1) := 1\n4, κ(1) := 1\n4, κ(0) := 1\n2, and κ(j) := 0 for j ̸∈{−1, 0, 1}.\nAssuming V = RM, the computational complexity of list.softGet is\nO(M · |supp(πi)|).\n\n6.1. Lists\n141\n3\n4\n2\n1\n5\n3\n4\n2\n1\n5\nFigure 6.1: The list.get(l, i) function is continuous and differentiable in l but not\nin i. Its relaxation list.sofGet(l, πi) is differentiable in both l and πi. When V = R,\nlist.softGet(l, πi) can be seen as taking the inner product between the list l and the\nprobability distribution πi, instead of the delta distribution (canonical vector) ei.\nSetting values\nTo differentiate w.r.t. indexing, we can define the continuous relaxation\nlist.softSet: LK(V) × △K × V →LK(conv(V)) as\n[list.softSet(l, πi, v)]j := E[list.set(l, I, v)]j\n= P(I = j)v + P(I ̸= j)lj\n= πi,jv + (1 −πi,j)lj,\nwhere j ∈[K] and I ∼Categorical(πi). Equivalently, we can write\nlist.softSet(l, πi, v) = (πi,1v + (1 −πi,1)l1, . . . , π1,Kv + (1 −π1,K)lK)\n= (ifelse(πi,1, v, l1), . . . , ifelse(πi,K, v, lK)),\nwhere ifelse is studied in Section 5.6. Since\nifelse(π, u1, u0) = EI∼Bernouilli(π)[uI],\nthis relaxation amounts to using an element-wise expectation. As a\nresult, the list output by list.softSet takes values in conv(V) instead of\nV. Note however that when V = RM, then conv(V) = RM as well.\n\n142\nData structures\nInserting values\nTo differentiate value insertion w.r.t. indexing, we can define the contin-\nuous relaxation list.softInsert: LK(V) × △K+1 × V →LK+1(conv(V))\n[list.softInsert(l, πi, v)]j := E[list.insert(l, I, v)]\n= P(I > j)lj + P(I = j)v + P(I < j)lj−1,\nwhere I ∼Categorical(πi). The three necessary probabilities can easily\nbe calculated for j ∈[K + 1] by\nP(I > j) =\n\n\n\n0\nif j = K + 1\nPK+1\nk=j+1 πi,k\notherwise\nP(I = j) = πi,j\nP(I < j) =\n\n\n\n0\nif j = 1\nPj−1\nk=1 πi,k\notherwise\n.\nMulti-dimensional indexing\nIn multi-dimensional lists (arrays or tensors), each element li ∈V of\na list l ∈LK1,...,KT (V) can now be indexed by a multivariate integer\ni = (i1, . . . , iT ) ∈[K1]×· · ·×[KT ], where T ∈N is the number of axes of\nl. We can always flatten a multi-dimensional list into an uni-dimensional\nlist by replacing the multi-dimensional index i ∈[K1] × · · · × [KT ] by\na flat index i ∈[K1 . . . KT ]. The converse operation, converting a flat\nuni-dimensional array into a multi-dimensional array, is also possible.\nTherefore, there is a bijection between [K] and [K1] × · · · × [KT ] for\nK := K1 . . . KT .\nThis means that the previous discussion on soft indexing in the\nuni-dimensional setting readily applies to the multi-dimensional setting.\nAll it takes is the ability to define a probability distribution πi ∈\n△K1×···×KT . For example, when working with images, we can define a\nprobability distribution putting probability mass only on the neighboring\npixels of pixel i, a standard approach in image processing. Another\nsimple approach is to use a product of axis-wise probability distributions.\n\n6.2. Dictionaries\n143\n6.2\nDictionaries\nA dictionary (a.k.a. associative array or map) is an unordered list of\nkey-value pairs, such that each possible key appears at most once in\nthe list. We denote the set of keys by K and the set of values by V (both\nbeing potentially infinite). We can then define the set of dictionaries of\nsize L from K to V by\nDL(K, V) := LL(K × V) = (K × V)L\nand one such dictionary by\nd := ((k1, v1), . . . , (kL, vL)) ∈DL(K, V).\n6.2.1\nBasic operations\nGetting values\nThe goal of the dict.get function is to retrieve the value associated with\na key, assuming that the dictionary contains this key. Formally, we\ndefine the dict.get: DL(K, V) × K →V ∪{∞} function as\ndict.get(d, k) :=\n\n\n\nvi\nif ∃i ∈[L] s.t. k = ki\n∞\nif k ̸∈{k1, . . . , kL}\n.\nThe function is continuous and differentiable in the dictionary d, but\nnot in the key k. Equivalently, we can write the function as\ndict.get(d, k) :=\nPL\ni=1 eq(k, ki)vi\nPL\ni=1 eq(k, ki) .\nThe denominator encodes the fact that the function is undefined if no\nkey in the dictionary d matches the key k. Assuming k ∈{k1, . . . , kL}\nand V = RM, we can also write\ndict.get(d, k) = vi\nwhere\ni = arg max\nj∈[L]\n∥k −kj∥2,\nwhich shows that we can see dict.get as a nearest neighbor search.\n\n144\nData structures\n0.0\n0.5\n1.0\nKeys\n0.0\n0.2\n0.4\nValues\n(k1, v1)\n(k2, v2)\n(k3, v3)\n(k4, v4)\nKey-value pairs\nKernel Estimator\nFigure 6.2: Given a set of key-value pairs (ki, vi) ∈K × V defining a dictionary d,\nwe can estimate a continuous mapping from K to V using Nadaraya–Watson kernel\nregression (here, illustrated with K = V = R). When keys are normalized to have\nunit norm, this recovers softargmax attention from Transformers.\nSetting values\nThe goal of the dict.set function is to replace the value associated with\nan existing key. Formally, we define the dict.set : DL(K, V) × K × V →\nDL(K, V) function as\n(dict.set(d, k, v))i :=\n\n\n\n(ki, v)\nif ki = k\n(ki, vi)\nif ki ̸= k\n.\nThe function leaves the dictionary unchanged if no key in the dictionary\nmatches the input key k. The function is continuous and differentiable\nin d and v, but not in k.\nImplementation\nWhile we view dictionaries as lists of key-value pairs, in practice, a\ndictionary (a.k.a. associative array) is often implemented using a hash\ntable or search trees. The hardware counterpart of a dictionary is called\ncontent-addressable memory (CAM), a.k.a. associative memory.\n\n6.2. Dictionaries\n145\n6.2.2\nContinuous relaxation using kernel regression\nA dictionary can been seen as a (potentially non-injective) function that\nassociates a value v to each key k. To obtain a continuous relaxation of\nthe operations associated to a dictionary, we can adopt a probabilistic\nperspective of the mapping from keys to values. We can view keys and\nvalues as two continuous random variables K and V . We can express\nthe conditional PDF f(v|k) of V |K in terms of the joint PDF f(k, v)\nof (K, V ) and the marginal PDF f(k) of K as\nf(v|k) = f(k, v)\nf(k) .\nIntegrating, we obtain the conditional expectation\nE[V |K = k] =\nZ\nV\nf(v|k)vdv =\nZ\nV\nf(k, v)\nf(k) vdv.\nThis is the Bayes predictor, in the sense that E[V |K] is the minimizer\nof E[(h(K) −V )2] over the space of measurable functions h: K →\nV. Using a sample of L input-output pairs (ki, vi), corresponding to\nkey-value pairs in our case, Nadaraya–Watson kernel regression\nestimates the joint PDF and the marginal PDF using kernel density\nestimation (KDE). Using a product of isotropic kernels κσ and ρσ for\nkey-value pairs, we can define\nbfσ(k, v) := 1\nL\nL\nX\ni=1\nκσ(k −ki)ρσ(v −vi).\nThe corresponding marginal distribution on the keys is then given as\nbfσ(k) :=\nZ\nV\nbfσ(k, v)dv\n= 1\nL\nL\nX\ni=1\nκσ(k −ki)\nZ\nV\nρσ(v −vi)dv\n= 1\nL\nL\nX\ni=1\nκσ(k −ki).\n\n146\nData structures\nReplacing f with bfσ, we obtain the following estimator of the conditional\nexpectation\nbE[V |K = k] :=\nZ\nV\nbfσ(k, v)\nbfσ(k)\nvdv\n=\nZ\nV\n1\nL\nPL\ni=1 κσ(k −ki)ρσ(v −vi)\n1\nL\nPL\ni=1 κσ(k −ki)\nvdv\n=\nPL\ni=1 κσ(k −ki)\nR\nV ρσ(v −vi)vdv\nPL\ni=1 κσ(k −ki)\n=\nPL\ni=1 κσ(k −ki)vi\nPL\ni=1 κσ(k −ki) .\nIn the above, we assumed that ρσ(v −vi) = pvi,σ(v), where pvi,σ(v) is\nthe PDF of a distribution whose mean is vi, so that\nZ\nV\nρσ(v −vi)vdv = EV ∼pvi,σ[V ] = vi.\nGiven a dictionary d = ((k1, v1), . . . , (kL, vL)), we can therefore define\nthe dict.softGet: DL(K, V) × K →conv(V) function as\ndict.softGet(d, k) :=\nPL\ni=1 κσ(k −ki)vi\nPL\ni=1 κσ(k −ki) .\nThis kernel regression perspective on dictionaries was previously pointed\nout by Zhang et al. (2021). It is illustrated in Fig. 6.2 with K = V = R.\n6.2.3\nDiscrete probability distribution perspective\nWhile the set of possible keys K is potentially infinite, the set of\nkeys {k1, . . . , kL} ⊂K associated with a particular dictionary d =\n((k1, v1), . . . , (kL, vL)) is finite. To a particular key k, we can therefore\nassociate a discrete probability distribution πk = (πk,1, . . . , πk,L) ∈△L\nover the keys (k1, . . . , kL) of d, defined by\nπk,i :=\nκσ(k −ki)\nPL\nj=1 κσ(k −kj)\n∀i ∈[L].\n\n6.2. Dictionaries\n147\n...\nWeight\nAvg\n...\nFigure 6.3: Computation graph of the dict.softGet function. We can use a kernel\nκσ to produce a discrete probability distribution πk = (πk,1, . . . , πk,L) ∈△L, that\ncaptures the affinity between the dictionary keys (k1, . . . , kL) and the input key k.\nThe dict.softGet function can then merely be seen as a convex combination (weighted\naverage) of values (v1, . . . , vL) using the probability values (πk,1, . . . , πk,L) as weights.\nThis distribution captures the affinity between the input key k and the\nkeys (k1, . . . , kL) of dictionary d. As illustrated in Fig. 6.3, we obtain\ndict.softGet(d, k) = Ei∼Categorical(πk)[vi]\n=\nL\nX\ni=1\nπk,ivi.\nIn the limit σ →0, we recover\ndict.get(d, k) =\nPL\ni=1 eq(k, ki)vi\nPL\ni=1 eq(k, ki) .\nWhile the dict.get function is using a mapping from keys k ∈\n{k1, . . . , kL} to integer indices [L], the dict.softGet function is using a\nmapping from keys k ∈{k1, . . . , kL} to distributions πk ∈△L. This\nperspective allows us to reuse the soft functions we developed for lists\nin Section 6.1. For example, we can softly replace the value associated\nwith key k by performing\nlist.softSet(d, πk, (k, v)).\nUnlike dict.set, the function is differentiable w.r.t. the distribution πk.\n6.2.4\nLink with attention in Transformers\nIn the case when κσ is the Gaussian kernel, assuming that the keys\nare normalized to have unit norm (which is often the case in practical\n\n148\nData structures\nimplementations (Schlag et al., 2021; Dehghani et al., 2023)), we obtain\nκσ(k −ki) = exp(−∥k −ki∥2\n2/(2σ2))\n= exp(−(∥k∥2\n2 + ∥ki∥2)/(2σ2)) exp(⟨k, ki⟩/σ2)\n= exp(−σ2) exp(⟨k, ki⟩/σ2)\nso that\nπk,i =\nκσ(k −ki)\nPL\nj=1 κσ(k −kj)\n=\nexp(⟨k, ki⟩/σ2)\nPL\nj=1 exp(⟨k, kj⟩/σ2).\nWe recognize the softargmax operator. Given, a dictionary\nd = ((k1, v1), . . . , (kL, vL)), we thus recover attention from Transform-\ners (Vaswani et al., 2017) as\ndict.softGet(d, k) =\nexp(⟨k, ki⟩/σ2)vi\nPL\nj=1 exp(⟨k, kj⟩/σ2).\nTransformers can therefore be interpreted as relying on a differentiable\ndictionary mechanism. Besides Transformers, content-based memory\naddressing is also used in neural Turing machines (Graves et al., 2014).\n6.3\nSummary\n• Operations on lists are continuous and differentiable w.r.t. the\nlist, but not w.r.t. the integer index. Similarly, operations on\ndictionaries are continuous and differentiable w.r.t. the dictionary,\nbut not w.r.t. the input key.\n• Similarly to the way we handled the predicate in conditionals,\nwe can replace the integer index (respectively the key) with a\nprobability distribution over the indices (respectively the keys).\n• This allows us to obtain a probabilistic relaxation of operations\non lists. In particular, the relaxation for list.get amounts to per-\nforming a convolution. The relaxation for dict.get amounts to\ncomputing a conditional expectation using kernel regression.\n\n6.3. Summary\n149\n• When using a Gaussian kernel with keys normalized to unit norm,\nwe recover softargmax attention from Transformers.\n\nPart III\nDifferentiating through\nprograms\n\n7\nFinite differences\nOne of the simplest way to numerically compute derivatives is to use\nfinite differences, which approximate the infinitesimal definition of deriva-\ntives. Finite differences only require function evaluations, and can\ntherefore work with blackbox functions (i.e., they ignore the composi-\ntional structure of functions). Without loss of generality, our exposition\nfocuses on computing directional derivatives ∂f(w)[v], for a function\nf : E →F, evaluated at w ∈E, in the direction v ∈E.\n7.1\nForward differences\nFrom Definition 2.4 and Definition 2.13, the directional derivative and\nmore generally the JVP are defined as a limit,\n∂f(w)[v] := lim\nδ→0\nf(w + δv) −f(w)\nδ\n.\nThis suggests that we can approximate the directional derivative and\nthe JVP using\n∂f(w)[v] ≈f(w + δv) −f(w)\nδ\n,\n151\n\n152\nFinite differences\nfor some 0 < δ ≪1. This formula is called a forward difference. From\nthe Taylor expansion in Section 2.5.4, we indeed have\nf(w+δv)−f(w) = δ∂f(w)[v]+δ2\n2 ∂2f(w)[v, v]+δ3\n3! ∂3f(w)[v, v, v]+. . .\nso that\nf(w + δv) −f(w)\nδ\n= ∂f(w)[v] + δ\n2∂2f(w)[v, v] + δ2\n3! ∂3f(w)[v, v, v] + . . .\n= ∂f(w)[v] + o(δ).\nThe error incurred by choosing a finite rather than infinitesimal δ in the\nforward difference formula is called the truncation error. The Taylor\napproximation above shows that this error is of the order of o(δ).\nHowever, we cannot choose a too small value of δ, because the\nevaluation of the function f on a computer rounds the value of f to\nmachine precision. Mathematically, a scalar-valued function f evaluated\non a computer becomes a function ˜f such that ˜f(w) ≈[f(w)/ε]ε,\nwhere [f(w)/ε] denotes the closest integer of f(w)/ε ∈R and ε is\nthe machine precision, i.e., the smallest non-zero real number encoded\nby the machine. This means that the difference f(w + δv) −f(w)\nevaluated on a computer is prone to round-off error of the order of\no(ε). We illustrate the trade-off between truncation and round-off errors\nin Fig. 7.1.\n7.2\nBackward differences\nAs an alternative, we can approximate the directional derivative and\nthe JVP by\n∂f(w)[v] ≈f(w) −f(w −δv)\nδ\n,\nfor some 0 < δ ≪1. This formula is called a backward difference.\nFrom the Taylor expansion in Section 2.5.4, we easily verify that (f(w)−\nf(w −δv))/δ = ∂f(w)[v] + o(δ), so that the truncation error is the\nsame as for the forward difference.\n\n7.3. Central differences\n153\n10\n13 10\n11\n10\n9\n10\n7\n10\n5\n10\n3\n10\n1\n10\n16\n10\n14\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\nApproximation error\nRound-off error \n is dominant\nTruncation error \n is dominant\nForward difference\nCentral difference\nComplex Step\nFigure 7.1: Numerical differentiation of f(x) := softplus(x) = log(1 + exp(x)),\nto approximate f ′(x) = logistic(x) at x = 1. The forward and central difference\nmethods induce both truncation error (for large δ) and round-off error (for small δ).\nThe complex step method enjoys smaller round-off error.\n7.3\nCentral differences\nRather than using an asymmetric formula to approximate the derivative,\nwe can use\n∂f(w)[v] ≈f(w + δv) −f(w −δv)\n2δ\n,\nfor some 0 < δ ≪1. This formula is called a central difference. From\nthe Taylor expansion in Section 2.5.4, we have\nf(w + δv) −f(w −δv) =2δ∂f(w)[v] + 2δ3\n3! ∂3f(w)[v, v, v]\n+ 2δ5\n5! ∂5f(w)[v, . . . , v] + . . .\nso that\nf(w + δv) −f(w −δv)\n2δ\n=∂f(w)[v] + δ2\n3! ∂3f(w)[v, v, v] + . . .\n=∂f(w)[v] + o(δ2).\nWe see that the terms corresponding to derivatives of even order\ncanceled out, allowing the formula to achieve o(δ2) truncation error.\nFor any δ < 1, the truncation error of the central difference is much\n\n154\nFinite differences\nsmaller than the one of the forward or backward differences as confirmed\nempirically in Fig. 7.1.\n7.4\nHigher-accuracy finite differences\nThe truncation error can be further reduced by making use of additional\nfunction evaluations. One can generalize the forward difference scheme\nby a formula of the form\n∂f(w)[v] ≈\np\nX\ni=0\nai\nδ f(w + iδv)\nrequiring p+1 evaluations. To select the ai and reach a truncation error\nof order o(δp), we can use a Taylor expansion on each term of the sum\nto get\np\nX\nk=0\nai\nδ f(w + iδv) =\np\nX\nk=0\nai\np\nX\nj=0\nijδj−1\nj!\n∂jf(w)[v, . . . , v] + o(δp).\nBy grouping the terms in the sum for each order of derivative, we obtain\na set of p+1 equations to be satisfied by the p+1 coefficients a0, . . . , ap,\nthat is,\na0 + a1 + . . . + ap = 0\na1 + 2a2 + . . . + pap = 1\na1 + 2ja2 + . . . + pjap = 0\n∀j ∈{2, . . . , p}.\nThis system of equations can be solved analytically to derive the co-\nefficients. Backward differences can be generalized similarly by using\n∂f(w)[v] ≈Pp\ni=0\nai\nδ f(w −iδv). Similarly, the central difference scheme\ncan be generalized by using\n∂f(w)[v] ≈\np\nX\ni=−p\nai\nδ f(w + iδv),\nto reach a truncation error of order o(δ2p). Solving for the coefficients\na−p, . . . , ap as above reveals that a0 = 0. Therefore, only 2p evaluations\nare necessary.\n\n7.5. Higher-order finite differences\n155\n7.5\nHigher-order finite differences\nTo approximate higher order derivatives, we can follow a similar rea-\nsoning. Namely, we can generalize the forward difference scheme to\napproximate the derivative of order k by\n∂kf(w)[v, . . . , v] ≈\np\nX\ni=0\nai\nδk f(w + iδv).\nAs before, we can expand the terms in the sum. For the approximation\nto capture only the kth derivative, we now require the coefficients ai to\nsatisfy\n0ja0 + 1ja1 + 2ja2 + . . . + pjap = 0\n∀j ∈{0, . . . , k −1}.\n0ka0 + 1ka1 + 2ka2 + . . . + pkap = k!\n0ja0 + 1ja1 + 2ja2 + . . . + pjap = 0\n∀j ∈{k + 1, . . . , p}.\nWith the resulting coefficients, we obtain a truncation error of order\no(δp−k+1), while making p + 1 evaluations. For example, for p = k = 2,\nwe can approximate the second-order derivative as\n∂2f(w)[v, v] ≈−(3/2)f(x) + 2f(x + δv) −(1/2)f(x + 2δv)\nδ2\n,\nwith a truncation error of order o(δ).\nThe central difference scheme can be generalized similarly by\n∂kf(w)[v, . . . , v] ≈\np\nX\ni=−p\nai\nδk f(w + iδv),\nto reach truncation errors of order o(δ2p+2−2⌈(k+1)/2⌉). For example, for\nk = 2, p = 1, we obtain the second-order central difference\n∂2f(w)[v, v] ≈f(w + δv) + f(w −δv) −2f(w)\nδ2\n.\nBy using a Taylor expansion we see that, this time, the terms corre-\nsponding to derivatives of odd order cancel out and the truncation\nerror is o(δ2) while requiring 3 evaluations.\n\n156\nFinite differences\n7.6\nComplex-step derivatives\nSuppose f is well defined on CP , the space of P-dimensional complex\nnumbers. Let us denote the imaginary unit by i = √−1. Then, the\nTaylor expansion of f reads\nf(w + (iδ)v) =f(w) + (iδ)∂f(w)[v] + (iδ)2\n2\n∂2f(w)[v, v]\n+ (iδ)3\n3! ∂3f(w)[v, v, v] + . . .\n=f(w) + (iδ)∂f(w)[v] −δ2\n2 ∂2f(w)[v, v]\n−i(δ)3\n3! ∂3f(w)[v, v, v] + . . .\n.\nWe see that the real part corresponds to even-degree terms and the\nimaginary part corresponds to odd-degree terms. We therefore obtain\nRe(f(w + (iδ)v)) = f(w) + o(δ2)\nand\nIm\n\u0012f(w + (iδ)v)\nδ\n\u0013\n= ∂f(w)[v] + o(δ2).\nThis suggests that we can compute directional derivatives using the\napproximation\n∂f(w)[v] ≈Im\n\u0012f(w + (iδ)v)\nδ\n\u0013\n,\nfor 0 < δ ≪1. This is called the complex-step derivative approxi-\nmation (Squire and Trapp, 1998; Martins et al., 2003).\nContrary to forward, backward and central differences, we see that\nonly a single function call is necessary. A function call on complex\nnumbers may take roughly twice the cost of a function call on real\nnumbers. However, thanks to the fact that a difference of functions is\nno longer needed, the complex-step derivative approximation usually\nenjoys smaller round-off error as illustrated in Fig. 7.1. That said, one\ndrawback of the method is that all elementary operations within the\nprogram implementing the function f must be well-defined on complex\nnumbers, e.g., using overloading.\n\n7.7. Complexity\n157\nTable 7.1: Computational complexity in number of function evaluations for com-\nputing the directional derivative and the gradient of a function f : RP →R by finite\ndifferences and complex step derivatives.\nDirectional derivative\nGradient\nForward difference\n2\nP + 1\nBackward difference\n2\nP + 1\nCentral difference\n2\n2P\nComplex step\n1\nP\n7.7\nComplexity\nWe now discuss the computational complexity in terms of function\nevaluations of finite differences and complex-step derivatives. For con-\ncreteness, as this is the most common use case in machine learning, we\ndiscuss the case of a single M = 1 output, i.e., we want to differentiate\na function f : RP →R. Whether we use forward, backward or central\ndifferences, the computational complexity of computing the directional\nderivative ∂f(w)[v] in any direction v amounts to two calls to f. For\ncomputing the gradient ∇f(w), we can use (see Definition 2.7) that\n[∇f(w)]j = ⟨∇f(w), ej⟩= ∂f(w)[ej],\nfor j ∈[P]. For forward and backward differences, we therefore need\nP +1 function calls to compute the gradient, while we need 2P function\ncalls for central differences. For the complex step approximation, we need\nP complex function calls. We summarize the complexities in Table 7.1.\n7.8\nSummary\n• Finite differences are a simple way to numerically compute deriva-\ntives using only function evaluations.\n• Central differences achieve smaller truncation error than forward\nand backward differences. It is possible to achieve smaller trunca-\ntion error, at the cost of more function evaluations.\n\n158\nFinite differences\n• Complex-step derivatives achieve smaller round-off error than\ncentral differences but require the function and the program\nimplementing it to be well-defined on complex numbers.\n• However, whatever the method used, finite differences require a\nnumber of function calls that is proportional to the number of\ndimensions. They are therefore seldom used in machine learning,\nwhere there can be millions or billions of dimensions. The main\nuse cases of finite differences are therefore i) for blackbox functions\nof low dimension and ii) for test purposes (e.g., checking that a\ngradient function is correctly implemented).\n• For modern machine learning, the main workhorse is automatic\ndifferentiation, as it leverages the compositional structure of func-\ntions. This is what we study in the next chapter.\n\n8\nAutomatic differentiation\nIn Chapter 2, we reviewed the fundamentals of differentiation and\nstressed the importance of two linear maps: the Jacobian-vector product\n(JVP) and its adjoint, the vector-Jacobian product (VJP). In this chap-\nter, we review forward-mode and reverse-mode autodiff using these\ntwo linear maps. We start with computation chains and then gener-\nalize to feedforward networks and general computation graphs. We\nalso review checkpointing, reversible layers and randomized estimators.\n8.1\nComputation chains\nTo begin with, consider a computation chain (Section 4.1.1) repre-\nsenting a function f : S0 →SK expressed as a sequence of compositions\nf := fK ◦· · · ◦f1, where fk : Sk−1 →Sk. The computation of f can be\n159\n\n160\nAutomatic differentiation\nunrolled into a sequence of operations\ns0 ∈S0\ns1 := f1(s0) ∈S1\n...\nsK := fK(sK−1) ∈SK\nf(s0) := sK.\n(8.1)\nOur goal is to compute the variations of f around a given input s0. In\na feedforward network, this amounts to estimating the influence of a\ngiven input s0 for fixed parameters (we will see how to estimate the\nvariations w.r.t. parameters w in the sequel).\nJacobian matrix.\nWe first consider the computation of the full Jaco-\nbian ∂f(s0), seen as a matrix, as the notation ∂indicates. Following\nProposition 2.2, we have\n∂f(s0) = ∂fK(sK−1)∂fK−1(sK−2) . . . ∂f2(s1)∂f1(s0),\n(8.2)\nwhere ∂fk(sk−1) are the Jacobians of the intermediate functions com-\nputed at s0, . . . , sK, as defined in Eq. (8.1). The main drawback of\nthis approach is computational: computing the full ∂f(s0) requires\nto materialize the intermediate Jacobians in memory and to perform\nmatrix-matrix multiplications. However, in practice, computing the full\nJacobian is rarely needed. Indeed, oftentimes, we only need to right-\nmultiply or left-multiply with ∂f(s0). This gives rise to forward-mode\nand reverse-mode autodiff, respectively.\n8.1.1\nForward-mode\nWe now interpret the Jacobian ∂f(s0) as a linear map, as the non-bold\n∂indicates. Following Proposition 2.6, ∂f(s0) is the composition of the\nintermediate linear maps,\n∂f(s0) = ∂fK(sK−1) ◦∂fK−1(sK−2) ◦. . . ◦∂f2(s1) ◦∂f1(s0).\n\n8.1. Computation chains\n161\n...\n...\n...\n...\nFigure 8.1: Forward-mode autodiff for a chain of computations. For readability,\nwe denoted the intermediate JVP as a function of two variables ∂fk : sk−1, tk−1 7→\n∂fk(sk−1)[tk−1] with ∂fk(sk−1)[tk−1] = tk.\nEvaluating ∂f(s0) on an input direction v ∈S0 can be decomposed,\nlike the function Eq. (8.1) itself, into intermediate computations\nt0 := v\nt1 := ∂f1(s0)[t0]\n...\ntK := ∂fK(sK−1)[tK−1]\n∂f(s0)[v] := tK.\nEach intermediate ∂fk(sk−1)[tk−1] amounts to a Jacobian-vector prod-\nuct (JVP) and can be performed in a forward manner, along the\ncomputation of the intermediate states sk. This can also be seen as\nmultiplying the matrix defined in Eq. (8.2) with a vector, from right\nto left. This is illustrated in Fig. 8.1 and the procedure is summarized\nin Algorithm 8.1.\nComputational complexity.\nThe JVP follows exactly the computations\nof f, with an additional variable tk being propagated. If we consider that\ncomputing ∂fk is roughly as costly as computing fk, then computing a\nJVP has roughly twice the computational cost of f. See Section 8.3.3\nfor a more general and more formal statement.\n\n162\nAutomatic differentiation\nAlgorithm 8.1 Forward-mode autodiff for chains of computations\nFunctions: f := fK ◦. . . ◦f1\nInputs: input s0 ∈S0, input direction v ∈S0\n1: Initialize t0 := v\n2: for k := 1, . . . , K do\n3:\nCompute sk := fk(sk−1) ∈Sk\n4:\nCompute tk := ∂fk(sk−1)[tk−1] ∈Sk\nOutputs: f(s0) := sK, ∂f(s0)[v] = tK\nMemory usage.\nThe memory usage of a program at a given evaluation\nstep is the number of variables that need to be stored in memory to\nensure the execution of all remaining steps. The memory cost of a\nprogram is then the maximal memory usage over all evaluation steps.\nFor our purposes, we analyze the memory usage and memory cost\nby examining the given program. Formal definitions of operations on\nmemory such as read, write, delete and associated memory costs are\npresented by Griewank and Walther (2008, Chapter 4).\nFor example, to execute the chain f = fK ◦· · · ◦f1, at each step k,\nwe only need to have access to sk−1 to execute the rest of the program.\nAs we compute sk, we can delete sk−1 from memory and replace it by\nsk. Therefore, the memory cost associated to the evaluation of f is just\nthe maximal dimension of the sk variables.\nFor forward mode autodiff, as we follow the computations of f, at\neach step k, we only need to have access to sk−1 and tk−1 to execute the\nrest of the program. The memory used by sk−1 and tk−1 can directly be\nused for sk, tk once they are computed. The memory usage associated\nto the JVP is summarized in Fig. 8.2. Overall the memory cost of the\nJVP is then exactly twice the memory cost of the function itself.\n8.1.2\nReverse-mode\nIn machine learning, most functions whose gradient we need to compute\ntake the form ℓ◦f, where ℓis a scalar-valued loss function and f is a\nnetwork. As seen in Proposition 2.3, the gradient takes the form\n∇(ℓ◦f)(s0) = ∂f(s0)∗[∇ℓ(f(s0))].\n\n8.1. Computation chains\n163\nMemory\nusage\nAlgorithm steps\nFigure 8.2: Memory usage of forward-mode autodiff for a computation chain. Here\nt0 = v, sK = f(s0), tK = ∂f(s0)[v].\nThis motivates the need for applying the adjoint ∂f(s0)∗to ∇ℓ(f(s0)) ∈\nSK and more generally to any output direction u ∈SK. From Propo-\nsition 2.7, we have\n∂f(s0)∗= ∂f1(s0)∗◦. . . ◦∂fK(sK−1)∗.\nEvaluating ∂f(s0)∗on an output direction u ∈SK is decomposed as\nrK = u\nrK−1 = ∂fK(sK−1)∗[rK]\n...\nr0 = ∂f1(s0)∗[r1]\n∂f(s0)∗[u] = r0.\nEach intermediate adjoint ∂fk(sk−1)∗amounts to a vector-Jacobian\nproduct (VJP). The key difference with the forward mode is that the\nprocedure runs backward through the chain, hence the name reverse\nmode autodiff. This can also be seen as multiplying Eq. (8.2) from left\nto right. The procedure is illustrated in Fig. 8.3 and summarized in\nAlgorithm 8.2.\nComputational complexity.\nIn terms of number of operations, the\nVJP simply passes two times through the chain, once forward, then\nbackward. If we consider the intermediate VJPs to be roughly as costly\nas the intermediate functions themselves, the VJP amounts just to twice\nthe cost of the original function, just as the JVP. See Section 8.3.3 for\na more generic and formal statement.\n\n164\nAutomatic differentiation\nBackward pass\n...\n...\n...\n...\nForward pass\nFigure 8.3: Reverse mode of automatic differentiation for a computation chain.\nFor readability, we denoted the intermediate VJPs as functions of two variables\n∂f ∗\nk : (sk−1, rk) 7→∂fk(sk−1)∗[rk], with ∂fk(sk−1)∗[rk] = rk−1.\nAlgorithm 8.2 Reverse-mode autodiff for chains of computations\nFunctions: f := fK ◦. . . ◦f1,\nInputs: input s0 ∈S0, output direction u ∈SK\n1: for k := 1, . . . , K do\n▷Forward pass\n2:\nCompute sk := fk(sk−1) ∈Sk\n3: Initialize rK := u.\n4: for k := K, . . . , 1 do\n▷Backward pass\n5:\nCompute rk−1 := ∂fk(sk−1)∗[rk] ∈Sk−1\nOutputs: f(s0) := sK, ∂f(s0)∗[u] = r0\n\n8.1. Computation chains\n165\nAlgorithm steps\nForward pass\nBackward pass\nMemory\nusage\nFigure 8.4: Memory usage of reverse mode autodiff for a computation chain.\nMemory usage.\nRecall that the memory usage of a program at a given\nevaluation step is the number of variables that need to be stored in\nmemory to ensure the execution of the remaining steps. If we inspect\nAlgorithm 8.3, to execute all backward steps, that is the loop in line 4,\nwe need to have access to all the intermediate inputs s0, . . . , sK−1.\nTherefore, the memory cost of reverse-mode autodiff is proportional to\nthe length of the chain K. Fig. 8.4 illustrates the memory usage during\nreverse mode autodiff. It grows linearly until the end of the forward\npass and then progressively decreases until it outputs the value of the\nfunction and the VJP. The memory cost can be mitigated by means of\ncheckpointing techniques presented in Section 8.5.\nDecoupled function and VJP evaluations.\nThe additional memory\ncost of reverse mode autodiff comes with some advantages. If we need\nto compute ∂f(s0)∗[ui] for n different output directions ui, we only\nneed to compute and store once the intermediate computations sk and\nthen make n calls to the backward pass. In other words, by storing in\nmemory the intermediate computations sk, we may instantiate a VJP\noperator, which we may apply to any u through the backward pass.\n\n166\nAutomatic differentiation\nFormally, the forward and backward passes can be decoupled as\nforward(f, s0) := (f(s0), ∂f(s0)∗)\nwhere\n∂f(s0)∗[u] := backward(u; s0, . . . , sK−1).\nIn functional programming terminology, the VJP ∂f(s0)∗is a closure,\nas it contains the intermediate computations s0, . . . sK. The same can\nbe done for the JVP ∂f(s0) if we want to apply to multiple directions\nvi.\nExample 8.1 (Multilayer perceptron with fixed parameters ). As a run-\nning example, consider a multilayer perceptron (MLP) with one\nhidden layer and (for now) given fixed weights. As presented in\nChapter 4, an MLP can be decomposed as\ns0 = x\ns1 = f1(s0) = σ(A1s0 + b1)\ns2 = f2(s1) = A2s1 + b2\nf(x) = s2,\nfor A1, A2, b1, b2 some fixed parameters and σ an activation func-\ntion such as the softplus activation function σ(x) = log(1+ex) with\nderivative σ′(x) = ex/(1 + ex).\nEvaluating the JVP of f on an input x along a direction v can\nthen be decomposed as\nt0 = v\nt1 = σ′(A1s0 + b1) ⊙(A1t0)\nt2 = A2t1\n∂f(x)[v] = t2,\nwhere we used in the second line the JVP of element-wise function\nas in Example 8.3.\nEvaluating the VJP of f at x requires to evaluate the interme-\n\n8.1. Computation chains\n167\ndiate VJPs at the stored activations\nr2 = u\nr1 = ∂f2(s1)∗[r2] = A⊤\n2 r2\nr0 = ∂f1(s0)∗[r1] = A⊤\n1 (σ′(A1s0 + b1) ⊙r1)\n∂f(x)∗[u] = r0.\n8.1.3\nComplexity of entire Jacobians\nIn this section, we analyze the time and space complexities of forward-\nmode and reverse-mode autodiff for computing the entire Jacobian\nmatrix ∂f(s0) of a computation chain f = fK ◦· · · ◦f1. We assume\nSk ⊆RDk, DK = M and D0 = D.\nComplexity of forward-mode autodiff\nUsing Definition 2.9, we find that we can extract each column ∂jf(s0) ∈\nRM of the Jacobian matrix, for j ∈[D], by multiplying with the standard\nbasis vector ej ∈RD:\n∂1f(s0) = ∂f(s0)e1\n...\n∂Df(s0) = ∂f(s0)eD.\nComputing the full Jacobian matrix therefore requires D JVPs with\nvectors in RD. Assuming each fk in the chain composition has the form\nfk : RDk−1 →RDk, seen as a matrix, ∂fk(sk−1) has size Dk × Dk−1.\nTherefore, the computational cost of D JVPs is O\n\u0010\nD PK\nk=1 DkDk−1\n\u0011\n.\nThe memory cost is O(maxk∈[K] Dk), since we can release intermediate\ncomputations after each layer is processed. Setting D1 = · · · = DK−1 =\nD for simplicity and using DK = M, we obtain that the computational\ncost of computing D JVPs and therefore of computing the full Jacobian\nmatrix by forward-mode autodiff is O(MD2 + KD3). The memory cost\nis O(max{D, M}). If a function is single-input D = 1, then the forward\nmode computes at once all the Jacobian, which reduces to a single\ndirectional derivative.\n\n168\nAutomatic differentiation\nForward-mode\nReverse-mode\nTime\nO(MD2 + KD3)\nO(M2D + KMD2)\nSpace\nO(max{M, D})\nO(KD + M)\nTable 8.1: Time and space complexities of forward-mode and reverse-mode autodiff\nfor computing the full Jacobian of a chain of functions f = fK ◦· · · ◦f1, where\nfk : RD →RD if k = 1, . . . , K −1 and fK : RD →RM. We assume ∂fk is a dense\nlinear operator. Forward mode requires D JVPs. Reverse mode requires M VJPs.\nComplexity of reverse-mode autodiff\nUsing Definition 2.9, we find that we can extract each row of the Jaco-\nbian matrix, which corresponds to the transposed gradients ∇fi(s0) ∈\nRD, for i ∈[M], by multiplying with the standard basis vector ei ∈RM:\n∇f1(s0) = ∂f(s0)∗e1\n...\n∇fM(s0) = ∂f(s0)∗eM.\nComputing the full Jacobian matrix therefore requires M VJPs with\nvectors in RM. Assuming as before that each fk in the chain composition\nhas the form fk : RDk−1 →RDk, the computational cost of M VJPs\nis O\n\u0010\nM PK\nk=1 DkDk−1\n\u0011\n. However, the memory cost is O(PK\nk=1 Dk),\nas we need to store the intermediate computations for each of the\nK layers. Setting D0 = · · · = DK−1 = D for simplicity and using\nDK = M, we obtain that the computational cost of computing M VJPs\nand therefore of computing the full Jacobian matrix by reverse-mode\nautodiff is O(M2D + KMD2). The memory cost is O(KD + M). If the\nfunction is single-output (M = 1), reverse-mode autodiff computes at\nonce the Jacobian, which reduces to the gradient.\nWhen to use forward-mode vs. reverse-mode autodiff?\nWe summarize the time and space complexities in Table 8.1. Generally,\nif M < D, reverse-mode is more advantageous at the price of some\nmemory cost. If M ≥D, forward mode is more advantageous.\n\n8.2. Feedforward networks\n169\n8.2\nFeedforward networks\nIn the previous section, we derived forward-mode autodiff and reverse-\nmode autodiff for computation chains with an input s0 ∈S0. In this\nsection, we now derive reverse-mode autodiff for feedforward networks,\nin which each layer fk is now allowed to depend explicitly on some\nadditional parameters wk ∈Wk. The recursion is\ns0 := x ∈S0\ns1 := f1(s0, w1) ∈S1\n...\nsK := fK(sK−1, wK) ∈SK\nf(x, w) := sK,\nwhere S0 = X and w = (w1, . . . , wK) ∈W1 × · · · × WK. Each fk is\nnow a function of two arguments. The first argument depends on the\nprevious layer, but the second argument does not. This is illustrated in\nFig. 8.5. We now explain how to differentiate a feedforward network.\n8.2.1\nComputing the adjoint\nThe function has the form f : E →F, where E := X × (W1 × · · · ×\nWK) and F := SK. From Section 2.3, we know that the VJP has the\nform ∂f(x, w)∗: F →E. Therefore, we want to be able to compute\n∂f(x, w)∗[u] ∈E for any u ∈F.\nFortunately, the backward recursion is only a slight modification\nof the computation chain case. Indeed, since fk : Ek →Fk, where\nEk := Sk−1 × Wk and Fk := Sk, the intermediate VJPs have the form\n∂fk(sk−1, wk)∗: Fk →Ek. We therefore arrive at the recursion\nrK = u ∈SK\n(rK−1, gK) = ∂fK(sK−1, wK)∗[rK] ∈SK−1 × WK\n...\n(r0, g1) = ∂f1(s0, w1)∗[r1] ∈S0 × W1.\nThe final output is\n∂f(x, w)∗[u] = (r0, (g1, . . . , gK)).\n\n170\nAutomatic differentiation\n...\n...\nFigure 8.5: Computation graph of an MLP as a function of its parameters.\n8.2.2\nComputing the gradient\nWe often compose a network with a loss function\nL(w; x, y) := ℓ(f(x, w); y) ∈R.\nFrom Proposition 2.7, the gradient is given by\n∇L(w; x, y) = (g1, . . . , gK) ∈W1 × · · · × WK\nwhere\n∂f(x, w)∗[u] = (r0, (g1, . . . , gK)),\nwith u = ∇ℓ(f(x, w); y) ∈SK. The output r0 ∈S0, where S0 = X,\ncorresponds to the gradient w.r.t. x ∈X and is typically not needed,\nexcept in generative modeling settings. The full procedure is summarized\nin Algorithm 8.3.\n\n8.2. Feedforward networks\n171\n...\n...\n...\n...\nForward pass\nBackward pass\n...\nFigure 8.6: Reverse mode of automatic differentiation, a.k.a., gradient back-\npropagation to compute the gradient of the loss of an MLP on an input label pair.\nFor readability, we denoted the intermediate VJPs as functions of three variables\n∂f ∗\nk : (sk−1, wk−1, rk) 7→∂fk(sk−1, wk)[rk] with ∂fk(sk−1, wk)∗[rk] = (rk−1, gk).\nAlgorithm 8.3 Gradient back-propagation for feedforward networks\nFunctions: f1, . . . , fK in sequential order\nInputs: data point (x, y) ∈X × Y\nparameters w = (w1, . . . wK) ∈W1 × · · · × WK\n1: Initialize s0 := x\n▷Forward pass\n2: for k := 1, . . . , K do\n3:\nCompute and store sk := fk(sk−1, wk) ∈Sk\n4: Compute ℓ(sK; y) and u := ∇ℓ(sK; y) ∈SK\n5: Initialize rK := u ∈SK\n▷Backward pass\n6: for k := K, . . . , 1 do\n7:\nCompute (rk−1, gk) := ∂fk(sk−1, wk)∗[rk] ∈Sk−1 × Wk\n8: Outputs: L(w; x, y) := ℓ(sK; y), ∇L(w; x, y) = (g1, . . . , gK)\n\n172\nAutomatic differentiation\nAlgorithm 8.4 Forward-mode autodiff for computation graphs\nFunctions: f1, . . . , fK in topological order\nInputs: input s0 ∈S0, input direction v ∈S0\n1: Initialize t0 := v\n2: for k := 1, . . . , K do\n▷Forward pass\n3:\nRetrieve parent nodes i1, . . . , ipk := pa(k)\n4:\nCompute sk := fk(si1, . . . , sipk)\n5:\nCompute\ntk := ∂fk(si1, . . . , sipk)[ti1, . . . , tipk]\n=\npk\nX\nj=1\n∂jfk(si1, . . . , sipk)[tij].\n6: Outputs: f(s0) := sK, ∂f(s0)[v] = tK\n8.3\nComputation graphs\nIn the previous sections, we reviewed autodiff for computation chains\nand its extension to feedforward networks. In this section, we generalize\nautodiff to computation graphs introduced in Section 4.1.3.\n8.3.1\nForward-mode\nThe forward mode corresponds to computing a JVP in an input direction\nv ∈S0. The algorithm consists in computing intermediate JVPs along\nthe forward pass. We initialize t0 := v ∈S0. Using Proposition 2.8, the\nderivatives on iteration k ∈[K] are propagated as\ntk := ∂fk(si1, . . . , sipk)[ti1, . . . , tipk]\n=\npk\nX\nj=1\n∂jfk(si1, . . . , sipk)[tij],\nwhere i1, . . . , ipk = pa(k). The final output is ∂f(s0)[v] = tK. The\nresulting generic forward-mode autodiff is summarized in Algorithm 8.4.\nAlthough not explicitly mentioned, we can release sk and tk from\nmemory when no child node depends on node k.\n\n8.3. Computation graphs\n173\n8.3.2\nReverse-mode\nThe reverse mode corresponds to computing a VJP in an output di-\nrection u ∈SK. We first perform a forward pass to compute the\nintermediate values and store the corresponding VJP as a pointer, since\nthe VJP shares some computations with the function itself. From Propo-\nsition 2.8, the VJP returns a tuple with the same length as the number\nof inputs to the function:\nδi1,k, . . . , δipk,k = ∂fk(si1, . . . , sipk)∗[rk].\nAfter the forward pass, we traverse the graph in reverse topological\norder as illustrated in Fig. 8.7. If an intermediate value sk is used by\nlater functions fj1, . . . , fjck for j1, . . . , jck = ch(k), the derivatives with\nrespect to sk need to sum all the variations through the fj functions in\na variable rk,\nrk :=\nX\nj∈{j1,...,jck}\nδk,j.\nIn practice, as we go backward through the computation graph, we can\naccumulate the VJPs corresponding to node k by doing in-place updates\nof the rk values. The topological ordering ensures that rk has been fully\ncomputed when we reach node k. The resulting generic reverse-mode\nautodiff is presented in Algorithm 8.5.\n8.3.3\nComplexity, the Baur-Strassen theorem\nFor computing the gradient of a function f : E →R represented by\na computation graph, we saw that the reverse mode is more efficient\nthan the forward mode. As we previously stated, assuming that the\nelementary functions fk in the DAG and their VJP have roughly the\nsame computational complexity, then f and ∇f have roughly the same\ncomputational complexity. This fact is crucial and is the pillar on which\nmodern machine learning relies: it allows us to optimize high-dimensional\nfunctions by gradient descent.\nFor arithmetic circuits, reviewed in Section 4.1.4, this crucial fact\nis made more precise in the celebrated Baur-Strassen theorem (Baur\nand Strassen, 1983). If f is a polynomial, then so is its gradient ∇f.\n\n174\nAutomatic differentiation\nAlgorithm 8.5 Reverse-mode autodiff for computation graphs\nFunctions: f1, . . . , fK in topological order\nInputs: input s0 ∈S0, output direction u ∈SK\n1: for k := 1, . . . , K do\n▷Forward pass\n2:\nRetrieve parent nodes i1, . . . , ipk := pa(k)\n3:\nCompute sk := fk(si1, . . . , sipk)\n4:\nInstantiate VJP lk := ∂fk(si1, . . . , sipk)∗\n5: Initialize rK := u, rk := 0 ∀k ∈{0, . . . , K −1}\n▷Backward pass\n6: for k := K, . . . , 1 do\n7:\nRetrieve parent nodes i1, . . . , ipk = pa(k)\n8:\nCompute δi1,k, . . . , δipk,k := lk[rk]\n9:\nCompute rij ←rij + δij,k ∀j ∈{1, . . . , pk}\n10: Outputs: f(s0) := sK, ∂f(s0)∗[u] = r0\nThe theorem gives an upper bound on the size of the best circuit for\ncomputing ∇f from the size of the best circuit for computing f.\nProposition 8.1 (Baur-Strassen’s theorem). For any polynomial\nf : E →R, we have\nS(∇f) ≤5 · S(f),\nwhere the size S(f) of a polynomial f is defined in Definition 4.1.\nA simpler proof by backward induction was given by Morgenstern\n(1985). See also the proof of Theorem 9.10 in Chen et al. (2011). For\ngeneral computation graphs, that have more primitive functions than\njust + and ×, a similar result can be obtained; see, e.g., (Bolte et al.,\n2022, Theorem 2).\n8.4\nImplementation\n8.4.1\nPrimitive functions\nAn autodiff system implements a set A of primitive or elementary\nfunctions, which serve as building blocks for creating other functions, by\nfunction composition. For instance, we saw that in arithmetic circuits\n\n8.4. Implementation\n175\nForward mode\nReverse mode\nComputations\nFigure 8.7: Left: Assuming a topological order, the computation of fk on iteration\nk involves pk inputs computed by fi1, . . . , fipk , where {i1, . . . , ipk} = pa(k), and is\nused in ck functions fj1, . . . , fjck , where {j1, . . . , jck} = ch(k). Middle: Forward-\nmode autodiff on iteration k, denoting the shorthand sp\nk := (si1, . . . , sipk ). This\ncomputes ∂ifk(sp\nk)[tij] for incoming tij, j = 1, . . . , pk, then sum these results to\npass tk to the next iterations. Right: Reverse-mode autodiff on iteration k. As we\ntraverse the graph backward, we first sum the contributions coming from each child\ncomputation fj1, . . . , fjck for {j1, . . . , jck} = ch(k) to get rk. We then feed this rk\nto each ∂ifk(sp\nk) and continue the procedure in reverse topological order.\n(Section 4.1.4), A = {+, ×}. More generally, A may contain all the\nnecessary functions for expressing programs. We emphasize, however,\nthat A is not necessarily restricted to low-level functions such as log\nand exp, but may also contain higher-level functions. For instance,\neven though the log-sum-exp can be expressed as the composition\nof elementary operations (log, sum, exp), it is usually included as a\nprimitive on its own, both because it is a very commonly-used building\nblock, but also for numerical stability reasons.\n8.4.2\nClosure under function composition\nEach function fk in a computation graph belongs to a set F, the class of\nfunctions supported by the system. A desirable property of an autodiff\nimplementation is that the set F is closed under function composition,\nmeaning that if f ∈F and g ∈F, then f ◦g ∈F. This means that\ncomposed functions can themselves be used for composing new functions.\nThis property is also crucial for supporting higher-order differentiation\n(Chapter 9) and automatic linear transposition (Section 8.4.4). When\n\n176\nAutomatic differentiation\nfk is a composition of elementary functions in A, then fk itself is a\nnested DAG. However, we can always inline each composite function,\nsuch that all functions in the DAG belong to A.\n8.4.3\nExamples of JVPs and VJPs\nAn autodiff system must implement for each f ∈A its JVP for support-\ning the forward mode, and its VJP for supporting the reverse mode.\nWe give a couple of examples. We start with the JVP and VJP of linear\nfunctions.\nExample 8.2 (JVP and VJP of linear functions). Consider the matrix-\nvector product f(W ) = W x ∈RM, where x ∈RD is fixed and\nW ∈RM×D. As already mentioned in Section 2.3.1, the JVP of f\nat W ∈RM×D along an input direction V ∈RM×D is simply\n∂f(W )[V ] = f(V ) = V x ∈RM.\nTo find the associated VJP, we note that for any u ∈RM and\nV ∈RM×D, we must have ⟨∂f(W )[V ], u⟩= ⟨V , ∂f(W )∗[u]⟩.\nUsing the properties of the trace, we have\n⟨∂f(W )[V ], u⟩= ⟨V x, u⟩= tr(x⊤V ⊤u) = ⟨V , ux⊤⟩.\nTherefore, we find that the VJP is given by\n∂f(W )∗[u] = ux⊤∈RM×D.\nSimilarly, consider now a matrix-matrix product f(W ) = W X,\nwhere W ∈RM×D and where X ∈RD×N is fixed. The JVP at\nW ∈RM×D along an input direction V ∈RM×D is simply\n∂f(W )[V ] = f(V ) = V X ∈RM×N.\nThe VJP along the output direction U ∈RM×N is\n∂f(W )∗[U] = UX⊤∈RM×D.\nAnother simple example are element-wise separable functions.\n\n8.4. Implementation\n177\nExample 8.3 (JVP and VJP of separable function). Consider the func-\ntion f(w) := (g1(w1), . . . , gP (wP )), where each gi : R →R has a\nderivative g′\ni. The Jacobian matrix is then a diagonal matrix\n∂f(w) = diag(g′\n1(w1), . . . , g′\nP (wP )) ∈RP×P .\nIn this case, the JVP and VJP are actually the same\n∂f(w)[v] = ∂f(w)∗[v] = (g′\n1(w1), . . . , g′\nP (wP )) ⊙v,\nwhere ⊙indicates element-wise multiplication.\n8.4.4\nAutomatic linear transposition\nOn first sight, if we want to support both forward ans reverse modes, it\nappears like we need to implement both the JVP and the VJP for each\nprimitive operation f ∈A. Fortunately, there exists a way to recover\nVJPs from JVPs, and vice-versa.\nWe saw in Section 2.3 that if l(w) is a linear map, then its JVP is\n∂l(w)[v] = l(v) (independent of w). Conversely, the VJP is ∂l(w)∗[u] =\nl∗(u), where l∗is the adjoint operator of l (again, independent of w).\nLet us define l(u; w) := ∂f(w)∗[u], i.e., the VJP of f in the output\ndirection u. Since l(u; w) is linear in u, we can apply the reasoning\nabove to compute its VJP\n∂l(u; w)∗[v] = l∗(v; w) = ∂f(w)∗∗[v] = ∂f(w)[v],\nwhich is independent of u. In words, the VJP of a VJP is the cor-\nresponding JVP! This means that we can implement forward-mode\nautodiff even if we only have access to VJPs. As an illustration and\nsanity check, we give the following example.\nExample 8.4 (Automatic transpose of “dot”). If we define\nf(x, W ) := W x, from Example 8.2, we know that\n∂f(x, W )∗[u] = (W ⊤u, ux⊤)\n= (f(u, W ⊤), f(x⊤, u))\n=: l(u; x, W ).\n\n178\nAutomatic differentiation\nUsing Proposition 2.9, we obtain\n∂l(u; x, W )∗[v, V ] = f(v, W ) + f(x, V )\n= W v + V x\n= ∂f(x, W )[v, V ].\nThe other direction, automatically creating a VJP from a JVP, is\nalso possible but is more technical and relies on the notion of partial\nevaluation (Frostig et al., 2021; Radul et al., 2022).\n8.5\nCheckpointing\nWe saw that forward-mode autodiff can release intermediate computa-\ntions from memory along the way, while reverse-mode autodiff needs to\ncache all of them. This means that the memory complexity of reverse-\nmode autodiff, in its standard form, grows linearly with the number of\nnodes in the computation graph. A commonly-used technique to circum-\nvent this issue is checkpointing, which trades-off computation time for\nbetter memory usage. Checkpointing works by selectively storing only a\nsubset of the intermediate nodes, called checkpoints, and by comput-\ning others on-the-fly. The specific choice of the checkpoint locations in\nthe computation graph determines the memory-computation trade-off.\nWhile it is possible to heuristically set checkpoints at user-specified\nlocations, it is also possible to perform a checkpointing strategy algo-\nrithmically, as studied in-depth by Griewank (1992) and Griewank and\nWalther (2008). In this section, we review two such algorithms: recursive\nhalving and dynamic programming (divide-and-conquer). Our exposi-\ntion focuses on computation chains f = fK ◦. . . ◦f1 with fi : RD →RD\nfor simplicity.\nComputational and memory complexities at two extremes.\nLet C(K)\nbe the number of calls to the individual functions fi (we ignore the\ncost of computing the intermediate VJPs) and M(K) be the number\nof function inputs cached, when performing reverse-mode autodiff on\na chain f = fK ◦. . . ◦f1. On one extreme, if we store all intermediate\ncomputations, as done in Algorithm 8.1, to compute only the VJP\n\n8.5. Checkpointing\n179\nAlgorithm 8.6 Reverse-mode autodiff with constant memory\nvjp_full_recompute(fK ◦. . . ◦f1, s0, u) := ∂(fK ◦. . . ◦f1)(s0)∗[u]\nInputs: Chain fK ◦. . .◦f1, input s0 ∈S0, output direction u ∈SK\n1: if K = 1 then\n2:\nreturn ∂f1(s0)∗[u]\n3: else\n4:\nSet rK = u\n5:\nfor k := K, . . . , 1 do\n6:\nCompute sk−1 = (fk−1 ◦. . . ◦f1)(s0)\n7:\nCompute rk−1 = ∂fk(sk−1)∗[rk]\n8:\nreturn: r0\n∂f(s0)∗[u], we have\nC(K) = K −1\nand\nM(K) = K.\nThis is optimal w.r.t. computational complexity, but suboptimal w.r.t.\nmemory. On the other extreme, if we only store the initial input, as\ndone in Algorithm 8.6, then we have\nC(K) = K(K −1)/2\nand\nM(K) = 1.\nThis is optimal w.r.t. memory but leads to a computational complexity\nthat is quadratic in K.\n8.5.1\nRecursive halving\nAs a first step towards obtaining a better computation-memory trade-off,\nwe may split the chain sK = fK ◦· · · ◦f1(s0) as\nsK/2 = fK/2 ◦. . . ◦f1(s0)\nsK = fK ◦. . . ◦fK/2+1(sK/2),\nfor K even. Then, rather than recomputing all intermediate computa-\ntions sk from the input s0 as in Algorithm 8.6, we can store sK/2 and\nrecompute sk for k > K/2 starting from sK/2. Formally, this strategy\namounts to the following steps.\n1. Compute sK/2 = fK/2 ◦. . . ◦f1(s0)\n\n180\nAutomatic differentiation\nAlgorithm 8.7 Reverse-mode autodiff with recursive halving\nvjp_halving(fK ◦. . . ◦f1, s0, u) := ∂(fK ◦. . . ◦f1)(s0)∗[u]\nFunctions: Chain fK ◦. . . ◦f1\nInputs: input s0 ∈S0, output direction u ∈SK\n1: if K = 1 then\n2:\nreturn ∂f1(s0)∗[u]\n3: else\n4:\nCompute sK/2 = fK/2 ◦. . . ◦f1(s0)\n5:\nCompute rK/2 = vjp_halving(fK ◦. . . ◦fK/2+1, sK/2, u)\n6:\nCompute r0 = vjp_halving(fK/2 ◦. . . ◦f1, s0, rK/2)\n7:\nreturn: r0\n2. Compute rK/2 = vjp_full_recompute(fK ◦. . .◦fK/2+1, sK/2, u)\n3. Compute r0 = vjp_full_recompute(fK/2 ◦. . . ◦f1, s0, rK/2)\nAt the expense of having to store the additional checkpoint sK/2, this\nalready roughly halves the computational complexity compared to Al-\ngorithm 8.6.\nWe can then apply this reasoning recursively, as formalized in Al-\ngorithm 8.7. The algorithm is known as recursive binary schedule\n(Griewank, 2003) and illustrated in Fig. 8.8. In terms of number of\nfunction evaluations C(K), for K even, we make K/2 function calls,\nand we call the procedure recursively twice, that is,\nC(K) = 2C(K/2) + K/2.\nIf the chain is of length 1, we directly use the VJP, so C(1) = 0. Hence,\nthe numbers of function calls, if K is a power of 2, is\nC(K) = K\n2 log2 K.\nIn terms of memory usage, Algorithm 8.7 uses s0 not only at line 4\nbut also at line 6. So when the algorithm is called recursively on the\nsecond half of the chain at line 5, one memory slot is taken by s0. This\nline is called recursively until the chain is reduced to a single function.\nAt that point, the total number of memory slots used is equal to the\n\n8.5. Checkpointing\n181\n0\n1\n2\n4\n6\n8\nFunction step \nTime step\nStorage in memory\nForward computation\nBackward computation\nFigure 8.8: Illustration of checkpointing with recursive halving, for a chain of\n8 functions. The chain is first fully evaluated while storing some computations\nas checkpoints in memory. Then, during the backward pass, we recompute some\nintermediate values from the latest checkpoint available. In contrast, vanilla reverse-\nmode autodiff (with full caching of the intermediate computations) would lead to a\nsimple triangle shape.\nnumber of times we split the function in half, that is log2 K for K a\npower of 2. On the other hand, the input s0 is no longer used after line 6\nof Algorithm 8.7. At that line, the memory slot taken by s0 can be\nconsumed by the recursive call on the first-half. In other words, calling\nthe algorithm recursively on the first half does not incur extra memory\ncost. So if K is a power of 2, the memory cost of Algorithm 8.7 is\nM(K) = log2 K.\n8.5.2\nDynamic programming\nRecursive halving requires log2 K memory slots for a chain of length\nK. However, as illustrated in Fig. 8.8, at a given time step, all memory\nslots may not be exploited.\nTo optimize the approach, we observe that recursive halving is\njust one instance of a program that splits the chain and calls itself\nrecursively on each part. In other words, it is a form of divide-and-\nconquer algorithm. Rather than splitting the chain in half, we may\nconsider splitting the chain at some index l. One split is used to reverse\n\n182\nAutomatic differentiation\nthe computations from l + 1 to K by a recursive call that consumes one\nmemory slot. The other split is used on a recursive call that reverses\nthe computations from 0 to l. That second call does not require an\nadditional memory slot, as it can use directly the memory slot used\nby the original input s0. To split the chain in such two parts, we need\nl intermediate computations to go from s0 to sl. The computational\ncomplexity C(k, s), counted as the number of function evaluations, for\na chain of length k with s memory slots then satisfies the recurrence\nC(k, s) = C(k −l, s −1) + C(l, s) + l,\nfor all l ∈{1, . . . , k −1}. By simply taking l = k/2, we recover exactly\nthe computational complexity of recursive halving. To refine the latter,\nwe may split the chain by selecting l to minimize the complexity. An\noptimal scheme must satisfy the recursive equation,\nC∗(k, s) :=\nmin\n1≤l≤K−1{C∗(k −l, s −1) + C∗(l, s) + l}.\n(8.3)\nNote that C∗(K, S) can be computed from C∗(k, s) for k = 1, . . . , K −1,\ns = 1, . . . , S −1. This suggests a dynamic programming approach to\nfind an optimal scheme algorithmically. For a chain of length k = 1, the\ncost is null as we directly reverse the computation, so C∗(1, s) := 0. On\nthe other hand for a memory s = 1, there is only one possible scheme\nthat saves only the initial input as in Algorithm 8.6, so C∗(k, 1) :=\n(k(k −1))/2. The values C∗(k, s) can then be computed incrementally\nfrom k = 1 to K and s = 1 to S using Eq. (8.3). The optimal splits can\nbe recorded along the way as\nl∗(k, s) := arg min\n1≤l≤k−1\n{C∗(k −l, s −1) + C∗(l, s) + l}.\nThe optimal split for K, S can then be found by backtracking the\noptimal splits along both branches corresponding to C∗(k −l, s −1)\nand C∗(l, s). As the final output consists in traversing a binary tree,\nit was called treeverse (Griewank, 1992). Note that the dynamic\nprogramming procedure is generic and could a priori incorporate varying\ncomputational costs for the intermediate functions fk.\n\n8.5. Checkpointing\n183\nAnalytical formula\nIt turns out that we can also find an optimal scheme analytically. This\nscheme was found by Griewank (1992), following the analysis of optimal\ninversions of sequential programs by divide-and-conquer algorithms\ndone by Grimm et al. (1996); see also Griewank (2003, Section 6) for\na simple proof. The main idea consists in considering the number of\ntimes an evaluation step fk is repeated. As we split the chain at l, all\nsteps from 1 to l will be repeated at least once. In other words, treating\nthe second half of the chain incurs one memory cost, while treating the\nfirst half of the chain incurs one repetition cost. Griewank (1992) shows\nthat for fixed K, S, we can find the minimal number of repetitions\nanalytically and build the corresponding scheme with simple formulas\nfor the optimal splits.\nCompared to the dynamic programming approach, it means that we\ndo not need to compute the pointers l∗(k, s), and we can use a simple\nformula to set l∗(k, s). We still need to traverse the corresponding binary\ntree given K, S and the l∗(k, s) to obtain the schedules. Note that such\noptimal scheme does not take into account varying computational costs\nfor the functions fk.\n8.5.3\nOnline checkpointing\nThe optimal scheme presented above requires knowing the total number\nof nodes in the computation graph ahead of time. However, when\ndifferentiating through for example a while loop (Section 5.10), this is\nnot the case. To circumvent this issue, online checkpointing schemes\nhave been developed and proven to be nearly optimal (Stumm and\nWalther, 2010; Wang et al., 2009). These schemes start by defining a set\nof S checkpoints with the first S computations, then these checkpoints\nare rewritten dynamically as the computations keep going. Once the\ncomputations terminate, the optimal approach presented above for a\nfixed length is applied on the set of checkpoints recorded.\n\n184\nAutomatic differentiation\nAlgorithm 8.8 Reverse-mode autodiff for reversible chains.\nFunctions: f := fK ◦. . . ◦f1, with each fk invertible\nInputs: input s0 ∈S0, output direction u ∈SK\n1: Compute sK = fK ◦. . . ◦f1(s0)\n2: for k := K, . . . , 1 do\n3:\nCompute sk−1 = f−1\nk (sk)\n4:\nCompute rk−1 = ∂fk(sk−1)∗[rk]\nOutputs: f(s0) := sK, ∂f(s0)∗[u] = r0\n8.6\nReversible layers\n8.6.1\nGeneral case\nThe memory requirements of reverse-mode autodiff can be completely\nalleviated when the functions fk are invertible (meaning that f−1\nk\nexists)\nand when f−1\nk\nis easily accessible. In that case, rather than storing the\nintermediate computations sk−1, necessary to compute the VJP rk 7→\n∂fk(sk−1)∗[rk], one can compute them on the fly during the backward\npass from sk using sk−1 = f−1\nk (sk). We summarize the procedure for\nthe case of computation chains in Algorithm 8.8. Compared to vanilla\nreverse-mode autodiff in Algorithm 8.2, the algorithm has optimal\nmemory complexity, as we can release sk and rk as we go.\nIn practice, f−1\nk\noften does not exist or may not be easily accessible.\nHowever, network architectures can be constructed to be easily invertible\nby design. Examples include reversible residual networks (Gomez et\nal., 2017), orthonormal RNNs (Helfrich et al., 2018), neural ODEs\n(Section 12.6), and momentum residual neural networks (Sander et al.,\n2021a); see also references therein.\n8.6.2\nCase of orthonormal JVPs\nWhen the JVP of each fk is an orthonormal linear mapping, i.e.,\n∂fk(sk−1)−1 = ∂fk(sk−1)∗,\n\n8.7. Randomized forward-mode estimator\n185\nit is easy to check that the VJP of f = fK ◦. . . ◦f1 is equal to the JVP\nof f−1 = f−1\n1\n◦. . . ◦f−1\nK , that is\n∂f(s0)∗[u] = ∂f−1(sK)[u].\nIn other words, in the case of orthormal JVPs, reverse-mode autodiff of\nf coincides with forward-mode autodiff of f−1.\n8.7\nRandomized forward-mode estimator\nForward-mode autodiff does not require to store intermediate activations.\nHowever, for a function f : RP →R, computing the gradient ∇f using\nforward-mode autodiff requires P JVPs, which is intractable if P is large.\nCan we approximate ∇f with fewer JVPs? The following proposition\ngives an unbiased estimator of ∇f that only involves JVPs.\nProposition 8.2 (Unbiased forward-mode estimator of the gradient).\nLet f : RP →R be a differentiable function. Then,\n∇f(µ) = EZ∼p [∂f(µ)[Z]Z]\n= EZ∼p [⟨∇f(µ), Z⟩Z] .\nwhere p := Normal(0, 1)P is the isotropic Gaussian distribution.\nThis estimator is for instance used by Baydin et al. (2022). It\ncan be seen as the zero-temperature limit of the gradient of a\nperturbed function, estimated by the score-function estimator (SFE);\nsee Section 14.4.6.\nIn practice, the expectation above can be approximated by drawing\nM noise vectors z1, . . . , zM, and averaging ⟨∇f(µ), zi⟩over i ∈[M].\nA word of caution: while this estimator can be useful for example\nwhen we do not want to store the intermediate activations for memory\nreasons, this of course comes at the cost of increasing the variance,\nwhich influences the convergence rate of SGD, as seen in Section 16.2.\n8.8\nSummary\n• Computer programs can be seen as directed acyclic graphs, where\nnodes correspond to the output of intermediate operations in\n\n186\nAutomatic differentiation\nthe program, and edges represent the dependencies of current\noperations on past operations.\n• Automatic differentiation (autodiff) for a function f : RP →RM\nhas two main modes: forward mode and reverse mode.\n• The forward mode: i) uses JVPs, ii) builds the Jacobian one\ncolumn at a time, iii) is efficient for tall Jacobians (M ≥P), iv)\nneed not store intermediate computations.\n• The reverse mode: i) uses VJPs, builds the Jacobian one row at\na time, iii) is efficient for wide Jacobians (P ≥M), iv) needs to\nstore intermediate computations, in order to be computationally\noptimal.\n• To trade computational efficiency for better memory efficiency,\nwe can use checkpointing techniques.\n• The complexity of computing the gradient of a function f : RP →\nR using the reverse mode is at most a constant time bigger than\nthat of evaluating the function itself. This is the Baur-Strassen\ntheorem, in arithmetic circuits. This astonishing result is one of\nthe pillars of modern machine learning.\n\n9\nSecond-order automatic differentiation\nWe review in this chapter how to perform automatic differentiation for\nsecond-order derivatives.\n9.1\nHessian-vector products\nWe consider in this section a function f : E →R. Similarly to the\nJacobian, for most purposes, we do not need access to the full Hessian\nbut rather to the Hessian-vector product (HVP) ∇2f(w)[v] at w ∈E,\nin a direction v ∈E, as defined in Definition 2.19. The latter can be\ncomputed in four different ways, depending on how we combine the two\nmain modes of autodiff.\n9.1.1\nFour possible methods\nAn HVP can be computed in four different ways.\n1. Reverse on reverse: The Hessian can be seen as the transposed\nJacobian of the gradient, hence the HVP can be computed as the\nVJP of the gradient,\n∇2f(w)[v] = ∂(∇f)(w)∗[v].\n187\n\n188\nSecond-order automatic differentiation\n2. Forward on reverse: Owing to its symmetry (see Proposi-\ntion 2.10), the Hessian can also be seen as the Jacobian of the\ngradient, hence the HVP can be computed as the JVP of the\ngradient,\n∇2f(w)[v] = ∂(∇f)(w)[v].\n3. Reverse on forward: Recall that for any function g: E →E, the\nVJP can equivalently be defined as the gradient along an output\ndirection v ∈E, that is,\n∂g(w)∗[v] = ∇⟨g, v⟩(w),\nwhere we recall the shorthand ⟨g, v⟩(w) := ⟨v, g(w)⟩, so that\n⟨g, v⟩is a function of w. In our case, we can therefore rewrite the\nreverse-on-reverse approach as\n∂(∇f)(w)∗[v] = ∇⟨∇f, v⟩(w).\nWe know that ⟨∇f, v⟩(w) = ⟨∇f(w), v⟩= ∂f(w)[v] is the JVP\nof f at w along v. Therefore, we can also compute the HVP as\nthe gradient of the JVP of f at w along v,\n∇2f(w)[v] = ∇(∂f(·)[v])(w),\nwhere we use the notation (∂f(·)[v])(w) := ∂f(w)[v] to insist on\nthe fact that it is a function of w.\n4. Forward on forward: Finally, we can use the definition of the\nHVP in Definition 2.19 as a vector of second partial derivatives\nalong v and each canonical direction. That is, assuming E = RP ,\nwe can compute the JVP of the JVP P times,\n∇2f(w)[v] = (∂2f(w)[v, ei])P\ni=1.\nThe four different ways of computing the HVP are summarized in\nTable 9.1.\n9.1.2\nComplexity\nTo get a sense of the computational and memory complexity of the four\napproaches, we consider a chain of functions f := fK ◦· · · ◦f1 as done\n\n9.1. Hessian-vector products\n189\nMethod\nComputation\nReverse on reverse (VJP of gradient)\n∂(∇f)(w)∗[v]\nForward on reverse (JVP of gradient)\n∂(∇f)(w)[v]\nReverse on forward (gradient of JVP)\n∇(∂f(·)[v])(w)\nForward on forward (JVPs of JVPs)\n(∂2f(w)[v, ei])P\ni=1\nTable 9.1: Four different ways of computing the HVP ∇2f(w)[v].\nFigure 9.1: Computation graph corresponding to reverse mode autodiff for eval-\nuating the gradient of f = fK ◦. . . f1. While f is a simple chain, ∇f is a DAG.\nin Section 8.1. To simplify our analysis, we assume fk : RP →RP for\nk ∈{1, . . . , K −1} and fK : RP →R.\nThe computation graph of the reverse mode is illustrated in Fig. 9.1.\nWhile f = fK ◦· · · ◦f1 would be represented by a simple chain, the\ncomputational graph of ∇f is no longer a chain: it is a DAG. This\nis due to the computations of ∂fk(sk−1)[rk], where both sk−1 and rk\ndepend on s0.\nWe illustrate the computation graphs of reverse-on-reverse and\nforward-on-reverse in Fig. 9.2 and Fig. 9.3 respectively. By applying\nreverse mode on reverse mode, at each fan-in operation sk−1, rk 7→\n∂fk(sk−1)[rk], the reverse mode on ∇f branches out in two paths that\nare later merged by a sum. By applying forward mode on top of reverse\nmode, the flow of computations simply follows the one of ∇f.\nWith this in mind, following a similar calculation as for Table 8.1,\nwe obtain the following results. We assume that each ∂fk(sk−1) is a\ndense linear operator, so that its application has the same cost as a\nmatrix-vector multiplication. For the memory complexity, we consider\nthat the inputs of each operation is saved to compute the required\n\n190\nSecond-order automatic differentiation\nGradient computation\nby reverse mode auto-diff\nHVP computations\nby reverse mode on top of reverse mode\nFigure 9.2: Computation graph for computing the HVP ∇2f(x)[v] by using reverse\nmode on top of reverse mode. As the computation graph of ∇f induces fan-in\noperations sk−1, rk 7→∂fk(sk−1)[rk], the reverse mode applied on ∇f induces\nbranching of the computations at each such node.\nGradient computation\nby reverse mode auto-diff\nHVP computations\nby forward mode on top of reverse mode\nFigure 9.3: Computation graph for computing the HVP ∇2f(x)[v] by using forward\nmode on top of reverse mode. The forward mode naturally follows the computations\ndone for the gradient, except that it passes through the derivatives of the intermediate\noperations.\n\n9.1. Hessian-vector products\n191\nderivatives in the backward passes.\n1. Reverse on reverse: O(KP 2) time and O(KP) space.\n2. Forward on reverse: O(KP 2) time and O(KP) space.\n3. Reverse on forward: O(KP 2) time and O(KP) space.\n4. Forward on forward: O(KP 3) time and O(3P) space for the\nP JVPs with e1, . . . , eP .\nWe see that, for chains of functions, “reverse on reverse”, “forward\non reverse” and “reverse on forward” all have similar time complexities\nup to some constant factors. Using reverse mode on top of reverse\nmode requires storing the information backpropagated, i.e., the rk\n(resp. the information forwarded, i.e., the tk in Fig. 8.1), to perform\nthe final reverse pass. By using forward mode on top of reverse mode,\nthis additional cost is not incurred, making it slightly less memory\nexpensive. In addition, reverse mode on top of reverse mode induces a\nfew additional summations due to the branching and merge operations\ndepicted in Fig. 9.2. The same holds when using reverse on top of\nforward as we cannot avoid fan-in operations (this time of the form\nsk−1, tk−1 7→∂fk(sk−1)[tk−1]). Unfortunately, “forward on forward” is\nprohibitively expensive.\nTo summarize, among the four approaches presented to compute\nHVPs, the forward-over-reverse mode is a priori the most preferable\nin terms of computational and memory complexities. Note, however,\nthat computations of higher derivatives can benefit from dedicated\nautodiff implementations such as Taylor mode autodiff, that do not\nmerely compose forward and reverse modes. For general functions f, it\nis reasonable to benchmark the first three methods to determine which\nmethod is the best for the function at hand.\n\n192\nSecond-order automatic differentiation\n9.2\nGauss-Newton matrix\n9.2.1\nAn approximation of the Hessian\nThe Hessian matrix ∇2L(w) of a function L: W →R is often used to\nconstruct a quadratic approximation of L(w),\nL(w + v) ≈⟨∇L(w), v⟩+ 1\n2⟨v, ∇2L(w)v⟩.\nUnfortunately, when L is nonconvex, ∇2L(w) is typically an indefinite\nmatrix, which means that the above approximation is a nonconvex\nquadratic w.r.t. v. For instance, if L = ℓ◦f with ℓconvex, then L is\nconvex if f is linear, but it is typically nonconvex if f is nonlinear. The\n(generalized) Gauss-Newton matrix is a principled alternative to the\nHessian, which is defined for L := ℓ◦f.\nDefinition 9.1 (Gauss-Newton matrix). Given a differentiable func-\ntion f : W →M and a twice differentiable function ℓ: M →R, the\n(generalized) Gauss-Newton matrix of the composition L = ℓ◦f\nevaluated at a point w ∈W is defined as\n∇2\nGN(ℓ◦f)(w) := ∂f(w)∗∇2ℓ(f(w))∂f(w).\nAs studied in Section 17.2, the Gauss-Newton matrix is a key ingre-\ndient of the Gauss-Newton method. An advantage of the Gauss-Newton\nmatrix is its positive semi-definiteness provided that ℓis convex.\nProposition 9.1 (Positive semi-definiteness of the GN matrix). If ℓis\nconvex, then ∇2\nGN(ℓ◦f)(w) is positive semi-definite for all f.\nThis means that the approximation\nL(w + v) ≈⟨∇L(w), v⟩+ 1\n2⟨v, ∇2\nGNL(w)v⟩\nis a convex quadratic w.r.t. v.\nUsing the chain rule, we find that the Hessian of L = ℓ◦f decomposes\ninto the sum of two terms (see also Proposition 9.7).\n\n9.2. Gauss-Newton matrix\n193\nProposition 9.2 (Approximation of the Hessian). For f differentiable\nand ℓtwice differentiable, we have\n∇2(ℓ◦f)(w) = ∂f(w)∗∇2ℓ(f(w))∂f(w) + ∂2f(w)∗[∇ℓ(f(w))]\n= ∇2\nGN(ℓ◦f)(w) +\nZ\nX\nj=1\n∇jℓ(f(w))∇2fj(w).\nIf f is linear, then the Hessian and Gauss-Newton matrices coincide,\n∇2(ℓ◦f)(w) = ∇2\nGN(ℓ◦f)(w).\nThe Gauss-Newton operator ∇2\nGN(ℓ◦f) can therefore be seen as an\napproximation of the Hessian ∇2(ℓ◦f), with equality if f is linear.\n9.2.2\nGauss-Newton chain rule\nA chain rule for computing the Hessian of a composition of two functions\nis presented in Proposition 9.7, but the formula is relatively complicated,\ndue to the cross-terms. In contrast, a Gauss-Newton chain rule is\nstraightforward.\nProposition 9.3 (Gauss-Newton chain rule).\n∇2\nGN(ℓ◦f ◦g)(w) = ∂g(w)∗∇2\nGN(ℓ◦f)(g(w))∂g(w).\n9.2.3\nGauss-Newton vector product\nAs for the Hessian, we rarely need to materialize the full Gauss-Newton\nmatrix in memory. Indeed, we can define the Gauss-Newton vector\nproduct (GNVP), a linear map for a direction v ∈W, as\n∇2\nGN(ℓ◦f)(w)[v] := ∂f(w)∗∇2ℓ(f(w))∂f(w)v,\n(9.1)\nwhere ∇2ℓ(θ)u is the HVP of ℓ, a linear map from M to M. The\nGNVP can be computed using the JVP of f, the HVP of ℓand the\nVJP of f. Instantiating the VJP requires 1 forward pass through f,\nfrom which we get both the value f(w) and the adjoint linear map\nu 7→(∂f(w)∗u). Evaluating the VJP requires 1 backward pass through\n\n194\nSecond-order automatic differentiation\nf. Evaluating the JVP requires 1 forward pass through f. In total,\nevaluating v 7→∇2\nGN(ℓ◦f)(w)v therefore requires 2 forward passes and\n1 backward pass through f.\n9.2.4\nGauss-Newton matrix factorization\nIn this section, we assume W ⊆RP and M ⊆RM. When ℓis convex,\nwe know that the Gauss-Newton matrix is positive semi-definite and\ntherefore it can be factorized into ∇2\nGN(ℓ◦f)(w) = V V ⊤for some\nV ∈RP×R, where R ≤min{P, M} is the rank of the matrix. Such a\ndecomposition can actually be computed easily from a factorization of\nthe Hessian of ℓ. For instance, suppose we know the eigendecomposition\nof the Hessian of ℓ, ∇2ℓ(f(w)) = PM\nj=1 λiuiu⊤\ni , where the ui are the\neigenvectors and the λi ≥0 are the eigenvalues (which we know are\nnon-negative due to positive semidefiniteness). Then, the Gauss-Newton\nmatrix can be decomposed as\n∇2\nGN(ℓ◦f) =\nM\nX\nj=1\nλi∂f(w)∗uiu⊤\ni ∂f(w)∗\n=\nM\nX\nj=1\n\u0010p\nλi∂f(w)∗ui\n\u0011 \u0010p\nλi∂f(w)∗ui\n\u0011⊤\n=\nM\nX\nj=1\nviv⊤\ni\nwhere vi :=\np\nλi∂f(w)∗ui.\nStacking the vectors vi into a matrix V = (v1, . . . , vM), we recover\nthe factorization ∇2\nGN(ℓ◦f)(w) = V V ⊤. To form this decomposition,\nwe need to perform the eigendecomposition of ∇2ℓ(f(w)) ∈RM×M,\nwhich takes O(M3) time. We also need M calls to the VJP of f at w.\nCompared to the direct implementation in Eq. (9.1), the factorization,\nonce computed, allows us to compute the Gauss-Newton vector product\n(GNVP) as ∇2\nGN(ℓ◦f)(w)[v] = V V ⊤v. The factorization only requires\nP × M memory, while the direct implementation in Eq. (9.1) requires\nus to maintain the intermediate computations of f. The computation-\nmemory trade-offs therefore depend on the function considered.\n\n9.3. Fisher information matrix\n195\n9.2.5\nStochastic setting\nSuppose the objective function is of the form\nL(w; x, y) := ℓ(f(w; x); y).\nWith some slight abuse of notation, we then have that the Gauss-Newton\nmatrix associated with a pair (x, y) is\n∇2\nGNL(w; x, y) := ∂f(w; x)∗∇2ℓ(θ; y)∂f(w; x).\nGiven a distribution ρ over (x, y) pairs, the Gauss-Newton matrix\nassociated with the averaged loss\nL(w) := EX,Y ∼ρ [L(w; X, Y )]\nis then\n∇2\nGNL(w) = EX,Y ∼ρ\nh\n∇2\nGNL(w; X, Y )\ni\n.\n9.3\nFisher information matrix\n9.3.1\nDefinition using the score function\nThe Fisher information is a way to measure the amount of information\nin a random variable S.\nDefinition 9.2 (Fisher information matrix). The Fisher informa-\ntion matrix, or Fisher for short, associated with the negative\nlog-likelihood L(w; S) = −log qw(S) of a probability distribution\nqw with parameters w is the covariance of the gradients of L at w\nfor S distributed according to qw,\n∇2\nFL(w) := ES∼qw[∇L(w; S) ⊗∇L(w; S)]\n= ES∼qw[∇w log qw(S) ⊗∇w log qw(S)].\nThe gradient ∇w log qw(S) is known as the score function.\nAs studied in Section 17.3, the Fisher information matrix is a key\ningredient of the natural gradient descent method.\n\n196\nSecond-order automatic differentiation\n9.3.2\nLink with the Hessian\nProvided that the probability distribution is twice differentiable w.r.t.\nw with integrable second derivatives, the Fisher information matrix can\nalso be expressed as the Hessian of the negative log-likelihood (Amari,\n1998; Martens, 2020).\nProposition 9.4 (Connection with the Hessian). The Fisher infor-\nmation matrix of the negative log-likelihood L(w; S) = −log qw(S)\nsatisfies\n∇2\nFL(w) = ES∼qw[∇2L(w; S)] = ES∼qw[−∇2\nw log qw(S)].\nRemark 9.1 (Empirical Fisher). We emphasize that in the above\ndefinitions, S is sampled from the model distribution qw, not from\nthe data distribution ρ. That is, we have\n∇2\nFL(w) = ES∼qw[∇w log qw(S)∇w log qw(S)⊤]\n̸= ES∼ρ[∇w log qw(S)∇w log qw(S)⊤]\nThe latter is sometimes called ambiguously the “empirical” Fisher,\nthough this name has generated confusion (Kunstner et al., 2019).\n9.3.3\nEquivalence with the Gauss-Newton matrix\nSo far, we discussed the Fisher information for a generic random variable\nS ∼qw. We now discuss the supervised probabilistic learning setting\nwhere S = (X, Y ) and where, using the product rule of probability,\nwe define the PDF qw(X, Y ) := ρX(X)pθ(Y ), with the shorthand θ :=\nf(w; X).\nProposition 9.5 (Fisher matrix in supervised setting). Suppose\n(X, Y ) ∼qw where the PDF of qw is qw(X, Y ) := ρX(X)pθ(Y ).\nIn that case, the Fisher information matrix of the negative log-\n\n9.3. Fisher information matrix\n197\nlikelihood L(w; x, y) = −log qw(x, y) decomposes as,\n∇2\nFL(w) = E(X,Y )∼qw[∇w log qw(X, Y ) ⊗∇w log qw(X, Y )]\n= EX∼ρX [EY ∼pθ [∇w log pθ(Y ) ⊗∇w log pθ(Y )]]\n= EX∼ρX\nh\n∂f(w; X)∗∇2\nFℓ(θ)∂f(w; X)\ni\n,\nwhere we defined the shorthand θ := f(w; X) and where we defined\nthe negative log-likelihood loss ℓ(θ; Y ) := −log pθ(Y ).\nWhen pθ is an exponential family distribution, we can show that the\nFisher information matrix and the Gauss-Newton matrix are equivalent.\nProposition 9.6 (Equivalence between Fisher and Gauss-Newton). If\npθ is an exponential family distribution, then\n∇2\nFL(w) = EX∼ρXEY ∼pθ[∇L(w; X, Y ) ⊗∇L(w; X, Y )]\n= EX∼ρXEY ∼pθ[∂f(w; X)∗∇ℓ(θ; Y ) ⊗∇ℓ(θ; Y )∂f(w; X)]\n= EX∼ρXEY ∼pθ[∂f(w; X)∗∇2ℓ(θ; Y )∂f(w; X)]\n= EX,Y ∼ρ\nh\n∇2\nGNL(w; X, Y )\ni\n,\nwhere ρX(x) :=\nR ρ(x, y)dy.\nProof. From Proposition 3.3, if pθ is an exponential family distribution,\n∇2ℓ(θ, y) is actually independent of y. Using Bartlett’s second identity\nEq. (12.3), we then obtain\n∇2ℓ(θ; ·) = EY ∼pθ[∇2ℓ(θ; Y )]\n= EY ∼pθ[∇2ℓ(θ; Y )]\n= EY ∼pθ[−∇2\nθ log pθ(Y )]\n= EY ∼pθ[∇θ log pθ(Y ) ⊗∇θ log pθ(Y )]\n= EY ∼pθ[∇ℓ(θ; Y ) ⊗∇ℓ(θ; Y )],\nwhere we used · to indicate that the results holds for all y. Plugging the\nresult back in the Fisher information matrix concludes the proof.\n\n198\nSecond-order automatic differentiation\n9.4\nInverse-Hessian vector product\n9.4.1\nDefinition as a linear map\nWe saw in Section 17.1 that Newton’s method uses iterations as\nwt+1 = wt −∇2L(wt)−1∇L(wt).\nThe inverse is well-defined if for example L is strictly convex. Otherwise,\nwe saw that some additional regularization can be added. Newton’s\nmethod therefore requires to access inverse-Hessian vector products\n(IHVPs), as defined below.\nDefinition 9.3 (Inverse-Hessian vector product). For a twice differ-\nentiable function L : RP →R, we define the inverse-Hessian\nVector Product (IHVP) of L at w ∈RP as the linear map\nu 7→∇2L(w)−1u,\nprovided that it exists. In other words, it is the linear map which\nto u associates v such that ∇2L(w)v = u.\n9.4.2\nImplementation with matrix-free linear solvers\nNumerous direct methods exist to compute the inverse of a matrix,\nsuch as the Cholesky decomposition, QR decomposition and Gaussian\nelimination. However, these algorithms require accessing elementary\nentries of the matrix, while an autodiff framework gives access to the\nHessian through HVPs. Fortunately, there exists so-called matrix-free\nalgorithms, that can solve a linear system of equations\nH[v] = u\nby only accessing the linear map v 7→H[v] for any v. Among such\nalgorithms, we have the conjugate gradient (CG) method, that applies\nfor H positive-definite, i.e., such that ⟨v, H[v]⟩> 0 for all v ̸= 0, or the\ngeneralized minimal residual (GMRES) method, that applies for\nany invertible H. A longer list of solvers can be found in public software\nsuch as SciPy (Virtanen et al., 2020). The IHVP of a strictly convex\n\n9.4. Inverse-Hessian vector product\n199\nfunction (ensuring that the Hessian is positive definite) can therefore\nbe computed by instantiating CG on the HVP,\n∇2L(w)−1u ≈CG(∇2L(w)[·], u).\nPositive-definiteness of the Hessian is indeed guaranteed for strictly\nconvex functions for example, while for generic non-convex functions,\nsuch property may be verified around a minimizer but not in general.\nThe conjugate gradient method is recalled in Algorithm 9.1 in its\nsimplest form. In theory, the exact solution of the linear system is found\nafter at most T = P iterations of CG, though in practice numerical\nerrors may prevent from getting an exact solution.\nAlgorithm 9.1 Conjugate gradient method\nInputs: linear map H[·] : RP →RP , target u ∈RP , initialization\nv0 (default 0), number of iterations T (default P), target accuracy\nε (default machine precision)\n1: r0 = u −H[v0]\n2: p0 = r0\n3: for t = 0, . . . T do\n4:\nαt =\n⟨rt,rt⟩\n⟨pt,H[pt]⟩\n5:\nvt+1 = vt + αtpt\n6:\nrt+1 = rt −αtH[pt]\n7:\nif ⟨rt+1, rt+1⟩≤ε then break\n8:\nβt = ⟨rt+1,rt+1⟩\n⟨rt,rt⟩\n9:\npt+1 = rt+1 + βtpt\nOutput: vT , such that H[vT ] ≈u\n9.4.3\nComplexity\nFor a given matrix H ∈RP×P , solving Hv = u can be done with\ndecomposition methods (LU, QR, Cholesky) in O(P 3) time. For matrix-\nfree methods such as CG or GMRES, the cost per iteration is O(P 2).\nSince they theoretically solve the linear system in O(P) iterations, the\ncost to obtain an exact solution is theoretically the same, O(P 3).\n\n200\nSecond-order automatic differentiation\nHowever, CG or GMRES differ from decomposition methods in that\nthey are iterative methods, meaning that, at each iteration, they get\ncloser to a solution. Unlike decomposition methods, this means that we\ncan stop them before an exact solution is found. In practice, the number\nof iterations required to find a good approximate solution depends on\nthe matrix. Well conditioned matrices require only few iterations. Badly\nconditioned matrices lead to some numerical instabilities for CG, so\nthat more than P iterations may be needed to get a good solution. In\ncontrast, decomposition methods proceed in two steps: first they build\na decomposition of H at a cost of O(P 3), and second they solve a linear\nsystem at a cost of O(P 2), by leveraging the structure. LU and QR\ndecompositions are known to be generally more stable and are therefore\noften preferred in practice, when we can access entries of H at no cost.\nIf we do not have access to the Hessian H, but only to its HVP,\naccessing entries of H comes at a prohibitive cost. Indeed, entries of H\ncan still be recovered from HVPs, since e⊤\ni Hej = Hi,j, but accessing\neach row or column of H costs one HVP (matrix-vector product). To\naccess the information necessary to use a decomposition method, we\ntherefore need P calls to HVPs before being able to actually compute\nthe solution. For the same number of calls, CG or GMRES will already\nhave found an approximate solution. In addition, a CG method does\nnot require to store any memory.\n9.5\nSecond-order backpropagation\n9.5.1\nSecond-order Jacobian chain rule\nThe essential ingredient to develop forward-mode and reverse-mode\nautodiff hinged upon the chain rule for composed functions, h = g ◦f.\nFor second derivatives, a similar rule can be obtained. To do so, we\nslightly abuse notations and denote\n∂2h(w)∗[u] := ∇2⟨h, u⟩(w) ∈RP×P ,\nwhere h : RP →RQ, w ∈RP , u ∈RQ, and where we recall the\nshorthand notation ⟨u, h⟩(w) := ⟨u, h(w)⟩. Moreover, we view the\nabove quantity as a linear map. Strictly speaking, the superscript ∗is\n\n9.5. Second-order backpropagation\n201\nnot a linear adjoint anymore, since v1, v2 7→∂2h(w)[v1, v2] is no longer\nlinear but bilinear. However, this superscript plays the same role as the\nVJP, since it takes an output vector and returns the input derivatives\nthat correspond to infinitesimal variations along that output vector.\nProposition 9.7 (Hessian chain-rule). For two twice differentiable\nfunctions f : RP →RM and g : RM →RQ, the second directional\nderivative of the composition g ◦f is a bilinear map from RP × RP\nto RQ along input directions v1, v2 ∈RP of the form\n∂2(g ◦f)(w)[v1, v2] = ∂g(f(w))[∂2f(w)[v1, v2]]\n+ ∂2g(f(w))[∂f(w)[v1], ∂f(w)[v2]].\nThe Hessian of the composition g ◦f along an output direction\nu ∈RQ is, seen as a linear map,\n∂2(g ◦f)(w)∗[u] = ∂2f(w)∗[∂g(f(w))∗u]\n(9.2)\n+ ∂f(w)∗∂2g(f(w))∗[u]∂f(w).\nFor the composition of f : RP →RM with a scalar-valued function\nfunction ℓ: RM →R, we have in matrix form\n∇2(ℓ◦f)(w) =\nM\nX\nj=1\n(∇ℓ(f(w)))j∇2fj(w)\n+ ∂f(w)⊤∇2ℓ(f(w))∂f(w).\nNote that, while the Hessian is usually defined for scalar-valued\nfunctions h: RP →R, the above definition is for a generalized notion\nof Hessian that works for any function h: RP →RQ.\nThe Hessian back-propagation rule in Eq. (9.2) reveals two terms.\nThe first one ∂2f(w)∗[∂g(f(w))∗u] simply computes the Hessian of\nthe intermediate function along the output direction normally back-\npropagated by a VJP. The second term ∂f(w)∗∂2g(f(w))∗[u]∂f(w)\nshows how intermediate first-order variations influence second order\nderivatives of the output.\n\n202\nSecond-order automatic differentiation\nExample 9.1 (Composition with an elementwise nonlinear function).\nConsider the element-wise application of a twice differentiable\nscalar-valued function f(x) = (f(xi))M\ni=1 followed by some twice\ndifferentiable function ℓ. Note that ∇2fi(x) = f′′(xi)eie⊤\ni . Hence,\nthe Hessian of the composition reads\n∇2(ℓ◦f)(x) =\nM\nX\ni=1\n(∇ℓ(f(x)))if′′(xi)eie⊤\ni\n+ diag(f′(x))∇2ℓ(f(x)) diag(f′(x))\n= diag(∇ℓ(f(w)) ⊙f′′(x))\n+ ∇2ℓ(f(x)) ⊙(f′(x)f′(x)⊤),\nwhere f′(x) := (f′(xi))M\ni=1 and f′′(x) := (f′′(xi))M\ni=1.\nExample 9.2 (Hessian of the composition with a linear function). Consider\na linear function f(W ) = W x, for W ∈RM×D, composed with\nsome twice differentiable function ℓ: RM →R. From Proposi-\ntion 9.7, we get, in terms of linear maps,\n∇2(ℓ◦f)(W ) = ∂f(W )∗∇2ℓ(f(W ))∂f(W ).\nAs already noted in Section 2.3, we have that ∂f(W )[V ] = V x\nand ∂f(W )∗[u] = ux⊤. Hence, the Hessian seen as a linear map\nreads\n∇2(ℓ◦f)(W )[V ] = ∂f(W )∗[∇2ℓ(f(W ))[∂f(W )[V ]]]\n= HV xx⊤,\nwhere H := ∇2ℓ(f(W )).\n9.5.2\nComputation chains\nFor a simple computation chain f = fK ◦. . . ◦f1 as in Section 8.1, the\nformula derived in Proposition 9.7 suffices to develop an algorithm that\nbackpropagates the Hessian, as shown in Algorithm 9.2. Compared to\nAlgorithm 8.2, we simply backpropagate both the vectors rk and the\nmatrices Rk using intermediate first and second derivatives.\n\n9.5. Second-order backpropagation\n203\nAlgorithm 9.2 Hessian backprop for computation chains\nFunctions: f := fK ◦. . . ◦f1,\nInputs: input x, output direction u\n1: Initialize and store s0 := x\n▷Forward pass\n2: for k := 1, . . . , K do\n3:\nCompute and store sk := fk(sk−1)\n4: Initialize rK := ∇ℓ(sK), RK := ∇2ℓ(sK)\n▷Backward pass\n5: for k := K, . . . , 1 do\n6:\nCompute rk−1 := ∂fk(sk−1)∗[rk]\n7:\nCompute Rk−1 := ∂2fk(sk−1)∗[rk] + ∂fk(sk−1)∗Rk∂fk(sk−1)\n8:\nRelease sk−1 from memory\nOutputs: ℓ(f(x)) = ℓ(sK), ∇(ℓ◦f)(x) = r0, ∇2(ℓ◦f)(x) = R0\n9.5.3\nFan-in and fan-out\nFor generic computation graphs (see Section 8.3), we saw that multi-\ninput functions (fan-in) were crucial. For Hessian backpropagation in\ncomputation graphs, we therefore need to develop a similar formula.\nProposition 9.8 (Hessian chain-rule for fan-in). Consider n+1 twice\ndifferentiable functions f1, . . . , fn and g with fi : RP →RMi and\ng : RM1 × . . . × RMn →RQ. The Hessian of g ◦f for f(w) =\n(f1(w), . . . , fn(w)) along an output direction u ∈RQ is given by\n∂2(g ◦f)(w)∗[u] =\nn\nX\ni=1\n∂2fi(w)∗[∂ig(f(w))∗[u]]\n+\nn\nX\ni,j=1\n∂fi(w)∗∂2\ni,jg(f(w))∗[u]∂fj(w).\nThe gradient backpropagation expression for fan-in is simple because\nthe functions fi are not linked by any path. In contrast, the Hessian\nbackpropagation involves cross-product terms\n∂fi(w)∗∂2\ni,jg(f(w))∗[u]∂fj(w) for i ̸= j. The nodes associated to the\nfi computations cannot be treated independently anymore.\nOn the other hand, developing a backpropagation rule for fan-out\ndoes not pose any issue, since each output function can be treated\n\n204\nSecond-order automatic differentiation\nindependently.\nProposition 9.9 (Hessian chain-rule for fan-out). Consider n+1 twice\ndifferentiable functions g1, . . . , gn and f with gi : RM →RQi and\nf : RP →RM. The Hessian of g ◦f for g(w) = (g1(w), . . . , gn(w))\nalong a direction u = (u1, . . . , un) ∈RQ1 × . . . × RQn is given by\n∂2(g ◦f)(w)∗[u] =\nn\nX\ni=1\n∂2f(w)∗[∂gi(f(w))∗[ui]]\n+\nn\nX\ni=1\n∂f(w)∗∂2gi(f(w))∗[u]∂f(w).\n9.6\nBlock diagonal approximations\nRather than computing the whole Hessian or Gauss-Newton matrices,\nwe can consider computing block-diagonal or diagonal approximations,\nwhich are easier to invert. The approximation rules we present in this\nsection build upon the Hessian chain rule studied in Section 9.5.\n9.6.1\nFeedforward networks\nRecall the definition of a feedforward network:\ns0 := x\nsk := fk(sk−1, wk) ∀k ∈{1, . . . , K}\nf(x, w) := sK,\nwhere w := (w1, . . . , wK). Rather than computing the entire Hessian of\nℓ◦f w.r.t. w, we can compute the Hessians w.r.t. each set of parameters\nwk. For the case of computation chains, the Hessian backpropagation\nrecursion we used in Algorithm 9.2 was\nRk−1 := ∂2fk(sk−1)∗[rk] + ∂fk(sk−1)∗Rk∂fk(sk−1).\n\n9.6. Block diagonal approximations\n205\nExtending this recursion to the feedforward network case, we obtain,\nstarting from rK := ∇ℓ(sK) and RK := ∇2ℓ(sK),\nrk−1 := ∂fk(sk−1, wk)∗[rk]\n \nRk−1\n∼\n∼\nHk\n!\n:= ∂2fk(sk−1, wk)∗[rk]\n+ ∂fk(sk−1, wk)∗Rk∂fk(sk−1, wk),\nwhere we used ∼to indicate that these blocks are not used. The Hessians\nw.r.t each set of parameters are then\nR0 = ∇2\nxx(ℓ◦f)(x, w)\nH1 = ∇2\nw1w1(ℓ◦f)(x, w)\n...\nHK = ∇2\nwKwK(ℓ◦f)(x, w)).\nThe validity of this result stems from the fact that we can view the\nHessian w.r.t. wk as computing the Hessian w.r.t. wk of\n˜fK ◦. . . ◦˜fk+1 ◦fk(sk−1, wk)\nwhere ˜fi := fi(·, wk), for i ∈{k + 1, . . . , K}. As the computations of\nthe block-wise Hessians share most of the computations, they can be\nevaluated in a single backward pass just as the gradients.\nExample 9.3 (Block-wise computation of the Gauss-Newton matrix).\nOur blockwise backpropagation scheme can readily be adapted for\nthe Gauss-Newton matrix as\n \nRk−1\n∼\n∼\nGk\n!\n:= ∂fk(sk−1, wk)∗Rk∂fk(sk−1, wk),\nstarting from RK := ∇2ℓ(sK). The outputs R0, G0, . . . , GK give a\nblock-wise approximation of the Gauss-Newton matrix.\nNow, consider a simple multilayer perceptron such that\nfk(sk−1, wk) := a(Wksk−1)\nwith\nwk := vec(Wk)\nUsing Example 9.2 and Example 9.1 adapted to the Gauss-Newton\n\n206\nSecond-order automatic differentiation\nmatrix, we can compute the block-wise decomposition of the Gauss-\nNewton matrix as, for k = K, . . . , 1,\nRk−1 := W ⊤\nk JkWk\nJk := Rk ⊙(a′(Wksk−1)a′(Wksk−1)⊤)\nGk := Jk ⊗sk−1s⊤\nk−1\nstarting from RK := ∇2ℓ(sK). The outputs G1, . . . , GK correspond\nto the block-wise elements of the Gauss-Newton matrix of f for the\nvectorized weights w1, . . . , wK. Similar computations were done in\nKFRA (Botev et al., 2017) and BackPack (Dangel et al., 2019).\n9.6.2\nComputation graphs\nFor generic computation graphs, consider a function f(x, w) defined\nby, denoting i1, . . . , ipk := pa(k),\nsk := fk(si1, . . . , sipk)\n∀k ∈{1, . . . , K}\nsuch that f(x, w) = sK, and k is following a topological ordering of the\ngraph (see Section 8.3). We can consider the following backpropagation\nscheme, for k = K, . . . , 1 and j ∈pa(k)\nrij ←rij + ∂jfk(si1, . . . , sipk)∗[rk]\n(9.3)\nRij ←Rij + ∂2\njjfk(si1, . . . , sipk)∗[rk]\n+ ∂jfk(si1, . . . , sipk)∗Rk∂jfk(si1, . . . , sipk),\n(9.4)\nstarting from RK := ∇2ℓ(sK) and rK := ∇ℓ(sK). Recall that for\nmultiple inputs, the chain-rule presented in Proposition 9.8 involves\nthe cross-derivatives. For this reason the back-propagation scheme\nin Eq. (9.3) only computes an approximation. For example, one can\nverify that using Eq. (9.3) to compute the Hessian of ℓ(f1(w), f2(w))\ndoes not provide an exact expression for the Hessian of f. This scheme\nis easy to implement and may provide a relevant proxy for the Hessian.\n9.7\nDiagonal approximations\nSimilarly to the idea of designing a backpropagation scheme that ap-\nproximates blocks of the Hessian, we can design a backpropagation\n\n9.7. Diagonal approximations\n207\nscheme that approximates the diagonal of the Hessian. The approach\nwas originally proposed by Becker and Le Cun (1988) for feedforward\nnetworks, but our exposition, new to our knowledge, has the benefit\nthat it naturally extends to computational graphs, as we shall see.\n9.7.1\nComputation chains\nThe idea stems from modifying the Hessian backpropagation rule\nin Proposition 9.7 to only keep the diagonal of the Hessian. Formally,\ngiven a matrix M ∈RD×D, we denote by diag(M) = (Mii)D\ni=1 ∈RD\nthe vector of diagonal entries of M, and for a vector m ∈RD, we\ndenote Diag(m) = PD\ni=1 mieie⊤\ni the diagonal matrix with entries mi.\nFor the backpropagation of the Hessian of ℓ◦fK ◦. . . ◦f1, we see from\nAlgorithm 9.2 that diag(Hk−1) can be expressed in terms of Hk as\ndiag(Hk−1) = diag(∂2fk(sk−1)∗rk)\n+ diag(∂fk(sk−1)∗Hk∂fk(sk−1)).\nUnfortunately, that recursion needs access to the whole Hessian Hk,\nand would therefore be too expensive. A natural idea is to modify the\nrecursion to approximate diag(Hk) by backpropagating vectors:\ndk−1 := diag(∂2fk(sk−1)∗rk)\n+ diag(∂fk(sk−1)∗Diag(dk)∂fk(sk−1)).\nThe diagonal matrix Diag(dk) serves as a surrogate for Hk. Each\niteration of this recursion can be computed in linear time in the output\ndimension Dk since\ndk−1,i =\nDk\nX\nj=1\nrk,j · ∂2\ni,ifk,j(sk−1) +\nDk\nX\nj=1\ndk,j(∂ifk,j(sk−1))2.\nTo initialize the recursion, we can set dK := diag(∇2ℓ(sK)). As an\nalternative, as proposed by Elsayed and Mahmood (2022), if HK has\na simple form, we can use ∇2ℓ(sK) instead of Diag(dK) at the first\niteration. This is the case for instance if fK is a cross-entropy loss. The\nrecursion is repeated until we obtain the approximate diagonal Hessian\nd0 ≈diag(∇2(ℓ◦f)(x)). The gradients rk, needed to compute dk, are\ncomputed along the way and the algorithm can therefore also return\nr0 = ∇(ℓ◦f)(x).\n\n208\nSecond-order automatic differentiation\n9.7.2\nComputation graphs\nAlthough this diagonal approximation was originally derived for feed-\nforward networks Becker and Le Cun (1988), it is straightforward to\ngeneralize it to computation graphs. Namely, for a function f(x, w) de-\ncomposed along a computation graph, we can backpropagate a diagonal\napproximation in reverse topological order as\nrij ←rij + ∂jfk(si1, . . . , sipk)∗[rk]\ndij ←dij + diag(∂2\njjfk(si1, . . . , sipk)∗[rk])\n+ diag(∂jfk(si1, . . . , sipk)∗Diag(dk)∂jfk(si1, . . . , sipk)),\n(9.5)\nfor j ∈pa(k), starting from rK = ∇ℓ(sK) and dK = diag(∇2ℓ(sK))\nor Diag(dK) = ∇2ℓ(sK). To implement such an algorithm, each ele-\nmentary function in the computational graph needs to be augmented\nwith an oracle that computes the Hessian diagonal approximation of\nthe current function, given the previous ones. An example with MLPs\nis presented in Example 9.4.\nExample 9.4 (Hessian diagonal approximation for MLPs ). Consider\na multilayer perceptron\nsk := ak(Wksk−1)\n∀k ∈{1, . . . , K −1}\nf(w, x) := sK\nstarting from s0 = x. Here ak is the element-wise activation func-\ntion (potentially the identity) and w encapsulates the weight ma-\ntrices W1, . . . , WK. We consider the derivatives w.r.t. the flattened\nmatrices, so that gradients and diagonal approximations w.r.t. these\nflattened quantities are vectors. The backpropagation scheme (9.5)\n\n9.8. Randomized estimators\n209\nthen reduces to, denoting tk = Wksk−1,\nrk−1 := W ⊤\nk (a′(tk) ⊙rk)\ngk := vec((a′(tk) ⊙rk)s⊤\nk−1)\nδk := rk ⊙a′′(tk) + dk ⊙a′(tk)2\ndk−1 :=\n\n\nDk\nX\nj=1\nW 2\nk,ijδk,j\n\n\nDk\ni=1\nhk := vec(δk(s2\nk−1)⊤)\nstarting from rK = ∇ℓ(sK) and, e.g., dK = diag(∇2ℓ(sK)). The\nalgorithm returns g1, . . . , gK as the gradients of f w.r.t. w1, . . . , wK,\nwith wi = vec(Wi), and h1, . . . , hK as the diagonal approximations\nof the Hessian w.r.t. w1, . . . , wK.\n9.8\nRandomized estimators\nIn this section, we describe randomized estimators of the diagonal of\nthe Hessian or Gauss-Newton matrices.\n9.8.1\nGirard-Hutchinson estimator\nWe begin with a generic estimator, originally proposed for trace es-\ntimation by Girard (1989) and extended by Hutchinson (1989). Let\nA ∈RP×P be an arbitrary square matrix, whose matrix-vector product\n(matvec) is available. Suppose ω ∈RP is an isotropic random vector,\ni.e., such that Eω∼p[ωω⊤] = I. For example, two common choices are\nthe Rademacher distribution p = Uniform({−1, 1}) and the standard\nnormal distribution p = Normal(0, I). Then, we have\nEω∼p[⟨ω, Aω⟩] = tr(A).\nApplications include generalized cross-validation, computing the Kullback-\nLeibler divergence between two Gaussians, and computing the deriva-\ntives of the log-determinant.\nThe approach can be extended (Bekas et al., 2007; Baston and\nNakatsukasa, 2022; Hallman et al., 2023) to obtain an estimator of the\n\n210\nSecond-order automatic differentiation\ndiagonal of A,\nEω∼p[ω ⊙Aω] = Diag(A),\nwhere ⊙denotes the Hadamard product (element-wise multiplication).\nThis suggests that we can use the Monte-Carlo method to estimate the\ndiagonal of A,\nDiag(A) ≈1\nS\nS\nX\ni=1\nωi ⊙Aωi,\nwith equality as S →∞, since the estimator is unbiased. Since, as\nreviewed in Section 9.1 and Section 9.2, we know how to multiply\nefficiently with the Hessian and the Gauss-Newton matrices, we can\napply the technique with these matrices. The variance is determined\nby the number S of samples drawn and therefore by the number of\nmatvecs performed. More elaborated approaches have been proposed to\nfurther reduce the variance (Meyer et al., 2021; Epperly et al., 2023).\n9.8.2\nBartlett estimator for the factorization\nSuppose the objective function is of the form L(w; x, y) := ℓ(f(w; x); y)\nwhere ℓis the negative log-likelihood ℓ(θ; y) := −log pθ(y) of an ex-\nponential family distribution, and θ := f(w; x), for some network f.\nWe saw from the equivalence between the Fisher and Gauss-Newton\nmatrices in Proposition 9.6 (which follows from the Bartlett identity)\nthat\n∇2\nGNL(w; x, ·) = EY ∼pθ[∂f(w; x)∗∇ℓ(θ; Y ) ⊗∇ℓ(θ; Y )∂f(w; x)]\n= EY ∼pθ[∇L(w; x, Y ) ⊗∇L(w; x, Y )],\nwhere · indicates that the result holds for any value of the second\nargument. This suggests a Monte-Carlo scheme\n∇2\nGNL(w; x, ·) ≈1\nS\nS\nX\nj=1\n[∇L(w; x, yij) ⊗∇L(w; x, yij)]\nwhere yi1, . . . , yiS ∼pθ and θ = f(w, x). In words, we can approxi-\nmate the Gauss-Newton matrix with S gradient computations. This\nfactorization can also be used to approximate the GNVP in Eq. (9.1).\n\n9.8. Randomized estimators\n211\n9.8.3\nBartlett estimator for the diagonal\nFollowing a similar approach, we obtain\ndiag(∇2\nGNL(w; x, ·)) = EY ∼pθ[∇L(w; x, Y ) ⊙∇L(w; x, Y )],\nwhere ⊙indicates the element-wise (Hadamard) product. Using a Monte-\nCarlo scheme, sampling yi1, . . . , yiS from pθ, we therefore obtain\ndiag(∇2\nGNL(w; x, ·)) ≈1\nS\nS\nX\nj=1\n∇L(w; x, yij) ⊙∇L(w; x, yij),\nwith equality when all labels in the support of pθ have been sampled.\nThat estimator, used for instance in (Wei et al., 2020, Appendix C.1.),\nrequires access to individual gradients evaluated at the sampled\nlabels. Another possible estimator of the diagonal is given by\n1\nS diag(∇2\nGNL(w; x, ·))\n=EY1,...,YS∼pθ\n\"\n∇1\nS\nS\nX\ni=1\nL(w; x, Yi) ⊙∇1\nS\nS\nX\ni=1\nL(w; x, Yi)\n#\n.\nLetting γi := ∇L(w; x, Yi), this follows from\nE\n\nX\ni\nγi ⊙\nX\nj\nγj\n\n= E\n\nX\ni\nγi ⊙γi +\nX\ni̸=j\nγi ⊙γj\n\n\n= E\n\"X\ni\nγi ⊙γi\n#\nwhere we used that E[γi ⊙γj] = E[γi] ⊙E[γj] = 0 since γi and γj are\nindependent variables for i ̸= j and have zero mean, from Bartlett’s\nfirst identity Eq. (12.2). We can then use the Monte-Carlo method to\nobtain\n1\nS diag(∇2\nGNL(w; x, ·)) ≈\n\n∇1\nS\nS\nX\nj=1\nL(w; x, yij)\n\n⊙\n\n∇1\nS\nS\nX\nj=1\nL(w; x, yij)\n\n,\nwith equality when all labels in the support of pθ have been sampled.\nThis estimator can be more convenient to implement, since it only needs\naccess to the gradient of the averaged losses. However, it may suffer\nfrom higher variance. A special case of this estimator is used by Liu\net al. (2023), where they draw only one y for each x.\n\n212\nSecond-order automatic differentiation\n9.9\nSummary\n• By using a Hessian chain rule, we can develop a “Hessian backprop-\nagation”. While it is reasonably simple for computation chains,\nit becomes computationally prohibitive for computation graphs,\ndue to the cross-product terms occurring with fan-in.\n• A better approach is to use Hessian-vector products (HVPs). We\nsaw that there are four possible methods to compute HVPs, but\nthe forward-over-reverse method is a priori the most efficient.\nSimilarly as for computing gradients, computing HVPs is only a\nconstant times more expensive than evaluating the function itself.\n• The Gauss-Newton matrix associated with the composition ℓ◦f\ncan be seen as an approximation of the Hessian. It is a positive\nsemidefinite matrix if ℓis convex, and can be used to build a\nprincipled quadratic approximation of a function. It is equivalent\nto the Fisher information matrix in the case of exponential families.\nGauss-Newton-vector products can be computed efficiently, like\nHVPs.\n• We also described other approximations, such as (block) diagonal\napproximations, and randomized estimators.\n\n10\nInference in graphical models as differentiation\nA graphical model specifies how random variables depend on each\nother and therefore determines how their joint probability distribution\nfactorizes. In this chapter, we review key concepts in graphical models\nand how they relate to differentiation, drawing in the process analogies\nwith computation chains and computation graphs.\n10.1\nChain rule of probability\nThe chain rule of probability is a fundamental law in probability theory\nfor computing the joint probability of events. In the case of only two\nevents A1 and A2, it reduces to the product rule\nP(A1 ∩A2) = P(A2|A1)P(A1).\nFor two discrete random variables S1 and S2, using the events A1 :=\n{S1 = s1} and A2 := {S2 = s2}, the product rule becomes\nP(S1 = s1, S2 = s2) = P(S2 = s2|S1 = s1)P(S1 = s1).\nMore generally, using the product rule, we have for K events\nP (A1 ∩. . . ∩AK) = P (AK | A1 ∩. . . ∩AK−1) P (A1 ∩. . . ∩AK−1) .\n213\n\n214\nInference in graphical models as differentiation\nApplying the product rule one more time, we have\nP (A1 ∩. . . ∩AK−1) = P (AK−1 | A1 ∩. . . ∩AK−2) P (A1 ∩. . . ∩AK−2) .\nRepeating the process recursively, we arrive at the chain rule of\nprobability\nP (A1 ∩. . . ∩AK) =\nK\nY\nj=1\nP(Aj | A1 ∩· · · ∩Aj−1)\n=\nK\nY\nj=1\nP\n\nAj\n\f\f\f\f\f\nj−1\n\\\ni=1\nAi\n\n.\nFor K discrete random variables Sj, using the events Aj := {Sj = sj},\nthe chain rule of probability becomes\nP (S1 = s1, . . . , SK = sK) =\nK\nY\nj=1\nP(Sj = sj | S1 = s1, . . . , Sj−1 = sj−1).\nImportantly, this factorization holds without any independence assump-\ntion on the variables S1, . . . , SK. We can further simplify this expression\nif we make conditional independence assumptions.\n10.2\nConditional independence\nWe know that if two events A and B are independent, then\nP(A|B) = P(A).\nSimilarly, if two random variables S1 and S2 are independent, then\nP(S2 = s2|S1 = s1) = P(S2 = s2).\nMore generally, if we work with K variables S1, . . . , SK, some variables\nmay depend on each other, while others may not. To simplify the\nnotation, given a set C, we define the shorthands\nSC := (Si : i ∈C)\nsC := (si : i ∈C).\nWe say that a variable Sj is independent of SD conditioned on SC,\nwith C ∩D = ∅, if for any sj, sC, sD\nP(Sj = sj | SC = sC, SD = sD) = P(Sj = sj | SC = sC).\n\n10.3. Inference problems\n215\n10.3\nInference problems\n10.3.1\nJoint probability distributions\nWe consider a collection of K variables s := (s1, . . . , sK), potentially\nordered or unordered. Each s belongs to the Cartesian product\nS := S1 × · · · × SK. Throughout this chapter, we assume that the sets\nSk are discrete for concreteness, with Sk := {v1, . . . , vMk}. Note that\nbecause Sk is discrete, we can always identify it with {1, . . . , Mk}. A\ngraphical model specifies a joint probability distribution\nP(S = s) = P(S1 = s1, . . . , SK = sK)\n= p(s)\n= p(s1, . . . , sK),\nwhere p is the probability mass function of the joint probability distribu-\ntion. Summing over the Cartesian product of all possible configurations,\nwe obtain\nX\ns∈S\np(s) =\nX\ns1,...,sK∈S\np(s1, . . . , sK) = 1.\nAs we shall see, the graph of a graphical model encodes the dependen-\ncies between the variables (S1, . . . , SK) and therefore how their joint\ndistribution factorizes. Given access to a joint probability distribution,\nthere are several inference problems one typically needs to solve.\n10.3.2\nLikelihood\nA simple task is to compute the likelihood of some observations\ns = (s1, . . . , sK),\nP(S1 = s1, . . . , Sk = sk) = p(s1, . . . , sk).\nIt is also common to compute the log-likelihood,\nlog P(S1 = s1, . . . , Sk = sk) = log p(s1, . . . , sk).\n10.3.3\nMaximum a-posteriori inference\nAnother common task is to compute the most likely configuration,\narg max\ns1∈S1,...,sK∈SK\np(s1, . . . , sK).\n\n216\nInference in graphical models as differentiation\nThis is the mode of the joint probability distribution. This is also known\nas maximum a-posteriori (MAP) inference in the literature (Wainwright\nand Jordan, 2008).\n10.3.4\nMarginal inference\nThe operation of marginalization consists in summing (or integrat-\ning) over all possible values of a given variable in a joint probability\ndistribution. This allows us to compute the marginal probability of\nthe remaining variables. For instance, we may want to marginalize all\nvariables but Sk = sk. To do so, we define the Cartesian product\nCk(sk) := S1 × · · · × Sk−1\n|\n{z\n}\nAk−1\n×{sk} × Sk+1 × · · · × SK\n|\n{z\n}\nBk+1\n.\n(10.1)\nSumming over all variables but Sk, we obtain the marginal probability\nof Sk = sk as\nP(Sk = sk) =\nX\ns1,...,sK∈Ck(sk)\np(s1, . . . , sK)\n=\nX\ns1,...,sk−1∈Ak−1\nX\nsk+1,...,sK∈Bk+1\np(s1, . . . , sK)\nDefining similarly\nCk,l(sk, sl) := S1 × · · · × {sk} × · · · × {sl} × · · · × SK,\nwe obtain\nP(Sk = sk, Sl = sl) =\nX\ns1,...,sK∈Ck,l(sk,sl)\np(s1, . . . , sK).\nIn particular, we may want to compute the marginal probability of two\nconsecutive variables, P(Sk−1 = sk−1, Sk = sk).\n10.3.5\nExpectation, convex hull, marginal polytope\nAnother common operation is to compute the expectation of ϕ(S) under\na distribution p. It is defined by\nµ := ES∼p[ϕ(S)] =\nX\ns∈S\np(s)ϕ(s) ∈M\n\n10.3. Inference problems\n217\nFor the expectation under pθ, we write\nµ(θ) := ES∼pθ[ϕ(S)] =\nX\ns∈S\npθ(s)ϕ(s) ∈M.\nIn exponential family distributions (Section 3.4), the function ϕ is called\na statistic. It decomposes as\nϕ(s) := (ϕC(sC))C∈C,\nwhere C ⊆[K]. Intuitively, ϕ(s) can be thought as an encoding or\nembedding of s (a potentially discrete object such as a sequence of\nintegers) in a vector space. Under this decomposition, we can also\ncompute\nµC := ES[ϕC(SC)] =\nX\ns∈S\np(s)ϕC(sC).\nConvex hull\nThe mean µ belongs to the convex hull of ϕ(S) := {ϕ(s): s ∈S},\nM := conv(ϕ(S)) :=\n(X\ns∈S\np(s)ϕ(s): p ∈P(S)\n)\n,\nwhere P(S) is the set of all possible probability distributions over S. In\nother words, M is the set of all possible convex combinations of ϕ(s)\nfor s ∈S. The vertices of M are all the s ∈S.\nCase of binary encodings: the marginal polytope\nIn the special case of a discrete set Sk = {v1, . . . , vM} and of a binary\nencoding (indicator function) ϕ(s), the set M is called the marginal\npolytope (Wainwright and Jordan, 2008), because each point µ ∈\nM contains marginal probabilities. To see why, consider the unary\npotential\n[ϕ(s)]k,i = [ϕk(sk)]i = I(sk = vi)\n(10.2)\n\n218\nInference in graphical models as differentiation\nwhere I(p) := 1 if p is true, 0 otherwise. We then obtain the marginal\nprobability of Sk = vi,\n[µ]k,i = ES[ϕ(S)k,i]\n= ESk[ϕk(Sk)i]\n= ESk[I(Sk = vi)]\n=\nX\nsk∈Sk\nP(Sk = sk)I(sk = vi)\n= P(Sk = vi).\nLikewise, consider the pairwise potential\n[ϕ(s)]k,l,i,j = [ϕk,l(sk, sl)]i,j = I(sk = vi, sl = vj).\n(10.3)\nWe then obtain the marginal probability of Sk = vi and Sl = vj,\n[µ]k,l,i,j = ES[ϕ(S)k,l,i,j]\n= ESk,Sl[ϕk,l(Sk, Sl)i,j]\n= ESk,Sl[I(Sk = vi, Sl = vj)]\n=\nX\nsk∈Sk\nX\nsl∈Sl\nP(Sk = sk, Sl = sl)I(sk = vi, sl = vj)\n= P(Sk = vi, Sl = vj).\nWe can do the same with higher-order potential functions.\n10.3.6\nComplexity of brute force\nApart from computing the likelihood, which is trivial, computing the\nmarginal, mode and expectation by brute force takes O(QK\nk=1 |Sk|) time.\nIn particular, if |Sk| = M ∀k ∈[K], brute force takes O(MK) time.\n10.4\nMarkov chains\nIn this section, we briefly review Markov chains. Our notation is chosen\nto emphasize the analogies with computation chains.\n\n10.4. Markov chains\n219\n10.4.1\nThe Markov property\nWhen random variables are organized sequentially as S1, . . . , SK,\na simple example of conditional independence is when each variable\nSk ∈Sk only depends on the previous variable Sk−1 ∈Sk−1, that is,\nP(Sk = sk | Sk−1 = sk−1, . . . , S1 = s1) = P(Sk = sk | Sk−1 = sk−1)\n:= pk(sk | sk−1),\nA probability distribution satisfying the above is said to satisfy the\nMarkov property, and is called a Markov chain. A computation\nchain is specified by the functions fk, that take sk−1 as input and\noutput sk. In analogy, a Markov chain is specified by the conditional\nprobability distributions pk of Sk given Sk−1. We can then define the\ngenerative process\nS0 := s0\nS1 ∼p1(· | S0)\nS2 ∼p2(· | S1)\n...\nSK ∼pK(· | SK−1).\nStrictly speaking, we should write Sk | Sk−1 ∼pk(· | Sk−1). We choose\nour notation both for conciseness and for analogy with computation\nchains. Furthermore, to simplify the notation, we assume without loss\nof generality that S0 is deterministic (if this is not the case, we can\nalways move S0 to S1 and add a dummy variable as S0). That is,\nP(S0 = s0) = p0(s0) := 1 and S0 := {s0}. This amounts to setting the\ninitial distribution of S1 as\nP(S1 = s1) := P(S0 = s0)P(S1 = s1|S0 = s0) = P(S1 = s1|S0 = s0).\n\n220\nInference in graphical models as differentiation\n...\nend\n...\n...\n...\nstart\n...\nend\n...\n...\n...\nstart\nFigure 10.1: Left: Markov chain. Right: Computation graph of the forward-\nbackward and the Viterbi algorithms: a lattice.\nWe can then compute the joint probability of the Markov chain by\nP(S1 = s1, . . . , SK = sK) = p(s1, . . . , sK)\n=\nK\nY\nk=1\nP(Sk = sk | Sk−1 = sk−1)\n=\nK\nY\nk=1\npk(sk | sk−1),\nwhere we left the dependence on s0 implicit, since p0(s0) = 1. A Markov\nchain with Sk = {1, 2, 3} is illustrated in Fig. 10.1. A chain defines\na totally ordered set {1, . . . , K}, since two nodes in the graph are\nnecessarily linked to each other by a path.\nExample 10.1 (Chain of categorical distributions). Suppose our goal\nis predict, from x ∈X, a sequence of length K, where each Sk\nbelongs to Sk = {1, . . . , M}. In natural language processing, this\ntask is called sequence tagging. We can define\nSk ∼Categorical(πk−1,k,Sk−1)\nwhere\nπk−1,k,i := softargmax(θk−1,k,i) ∈△M\n= (πk−1,k,i,j)M\nj=1\nθk−1,k,i := (θk−1,k,i,j)M\nj=1 ∈RM\nθk−1,k,i,j := fk−1,k(x, i, j, wk) ∈R.\n\n10.4. Markov chains\n221\nWe therefore have\nP(Sk = j | Sk−1 = i) = pk(j | i)\n= πk−1,k,i,j\n= [softargmax(θk−1,k,i)]j\n=\nexp(θk−1,k,i,j)\nP\nj′ exp(θk−1,k,i,j′)\nand\nlog P(Sk = j | Sk−1 = i) = log pk(j | i)\n= θk−1,k,i,j −logsumexp(θk−1,k,i)\n= θk−1,k,i,j −log\nX\nj′\nexp(θk−1,k,i,j′).\nWe emphasize that because k −1 and k are always consecutive,\nthe representation θk−1,k,i,j is inefficient; we could use θk,i,j instead.\nOur notation is designed for consistency with Markov random fields.\n10.4.2\nTime-homogeneous Markov chains\nA time-homogeneous discrete-time Markov chain corresponds to the\ncase when the distribution of Sk given Sk−1 is the same regardless of k:\np1 = · · · = pK = p.\nThe finite-space case corresponds to when each Sk ∈S can take a\nfinite set of values S = {v1, . . . , vM} and\nP(Sk = vj | Sk−1 = vi) = p(vj|vi) = πi,j,\nwhere πi,j ∈[0, 1] is the transition probability from vi to vj. Because\nthe set S = {v1, . . . , vM} is discrete, we can always identify it with\n{1, . . . , M}. That is, we can instead write\nP(Sk = j | Sk−1 = i) = p(j|i) = πi,j.\n\n222\nInference in graphical models as differentiation\n10.4.3\nHigher-order Markov chains\nMore generally, a nth-order Markov chain may depend, not only on the\nlast variable, but on the last n variables,\nP(Sk = sk | Sk−1 = sk−1, . . . , S1 = s1)\n=P(Sk = sk | Sk−1 = sk−1, . . . , Sk−n = sk−n)\n=pk(sk|sk−1, . . . , sk−n).\nAutoregressive models such as Transformers can be seen as specifying\na higher-order Markov chain, with a context window of size n. The\nlarger context makes exact inference using dynamic programming com-\nputationally intractable. This is why practitioners use beam search or\nancestral sampling (Section 10.5.3) instead.\n10.5\nBayesian networks\nIn this section, we briefly review Bayesian networks. Our notation is\nchosen to emphasize the analogies with computation graphs.\n10.5.1\nExpressing variable dependencies using DAGs\nMarkov chains and more generally higher-order Markov chains are a spe-\ncial case of Bayesian network. Similarly to computation graphs reviewed\nin Section 8.3, variable dependencies can be expressed using a directed\nacyclic graph (DAG) G = (V, E), where the vertices V = {1, . . . , K}\nrepresent variables and edges E represent variable dependencies. The set\n{i1, . . . , ink} = pa(k) ⊆V, where nk := |pa(k)|, indicates the variables\nSi1, . . . , Sink that Sk depends on. This defines a partially ordered\nset (poset). For notational simplicity, we again assume without loss of\ngenerality that S0 is deterministic. A computation graph is specified\nby functions f1, . . . , fK in topological order. In analogy, a Bayesian\nnetwork is specified by conditional probability distributions pk of Sk\n\n10.5. Bayesian networks\n223\ngiven Spa(k). We can then define the generative process\nS0 := s0\nS1 ∼p1(· | S0)\nS2 ∼p2(· | Spa(2))\n...\nSK ∼pK(· | Spa(K)).\nUsing the chain rule of probability and variable independencies expressed\nby the DAG, the joint probability distribution is then (assuming a\ntopological order for S0, S1, . . . , SK)\nP(S = s) := P(S1 = s1, . . . , SK = sK)\n=\nK\nY\nk=1\nP(Sk = sk|Spa(k) = spa(k))\n:=\nK\nY\nk=1\npk(sk|spa(k))\nThis representation is well suited to express causal relationships between\nrandom variables.\n10.5.2\nParameterizing Bayesian networks\nIn a Bayesian framework, observed data, latent variables, parameters\nand noise variables are all treated as random variables. If the conditional\ndistribution pk associated to node k depends on some parameters, they\ncan be provided to pk as conditioning, using parent nodes.\nA Bayesian network is specified by the conditional distributions pk.\nTherefore, unlike computation graphs, there is no notion of function fk in\na Bayesian network. However, the root nodes of the Bayesian network can\nbe the output of a neural network. For instance, autoregressive models,\nsuch as RNNs or Transformers, specify the conditional probability\ndistribution of a token given past tokens, and the chain rule of probability\nis used to obtain a probability distribution over entire sequences.\n\n224\nInference in graphical models as differentiation\n10.5.3\nAncestral sampling\nA major advantage of Bayesian networks is that, provided that each\nconditional distribution pk is normalized, the joint distribution of S =\n(S1, . . . , SK) is automatically normalized. This means that we can very\neasily draw i.i.d. samples from the joint distribution, by following the\ngenerative process: we follow the topological order k = 1, . . . , K and\non iteration k we draw a value sk ∼pk(·|spa(k)) conditioned on the\nprevious values spa(k). This is known as ancestral sampling.\n10.6\nMarkov random fields\n10.6.1\nExpressing factors using undirected graphs\nA Markov random field (MRF), a.k.a. undirected graphical model,\nspecifies a distribution that factorizes as\nP(S = s) = p(s) := 1\nZ\nY\nC∈C\nψC(sC),\nwhere C is the set of maximal cliques of G, that is, subsets of V that\nare fully connected, Z is a normalization constant defined by\nZ :=\nX\ns∈S\nY\nC∈C\nψC(sC),\nand ψC : SC →R+ is a potential function (a.k.a. compatibility func-\ntion), with SC := (Sj)j∈C. According to the Hammersley-Clifford theo-\nrem, an MRF can be equivalently defined in terms of Markov properties;\nwe refer the interested reader to Wainwright and Jordan (2008). For the\nsake of this chapter, the definition above is sufficient for our purposes.\nExample 10.2 (Markov chains as Markov random fields). For a chain,\nletting S = (S1, . . . , SK) and s = (s1, . . . , sK), recall that\nP(S = s) =\nK\nY\nk=1\npk(sk | sk−1).\nThis is equivalent to an MRF with Z = 1 (since a chain is auto-\n\n10.6. Markov random fields\n225\nmatically normalized),\nC := {{0, 1}, {1, 2}, . . . , {K −1, K}}\nand with potential function\nψ{k−1,k}(sk−1, sk) := pk(sk|sk−1).\nMore generally, a Bayesian network can be similarly written as an\nMRF by creating appropriate potential functions corresponding to\nthe parents of each node.\n10.6.2\nMRFs as exponential family distributions\nLet us define the potential functions\nψC(sC; θC) := exp(⟨θC, ϕC(sC)⟩)\nfor some sufficient statistic function ϕC : SC →ΘC and parameters\nθC ∈ΘC. Then,\npθ(s) :=\n1\nZ(θ)\nY\nC∈C\nψC(sC; θC)\n=\n1\nZ(θ)\nY\nC∈C\nexp(⟨θC, ϕC(sC)⟩)\n=\n1\nZ(θ) exp\n X\nC∈C\n⟨θC, ϕC(sC)⟩\n!\n=\n1\nZ(θ) exp (⟨θ, ϕ(s)⟩)\n= exp (⟨θ, ϕ(s)⟩−A(θ))\n\n226\nInference in graphical models as differentiation\nwhere\nϕ(s) := (ϕC(sC))C∈C\nθ := (θC)C∈C\nZ(θ) :=\nX\ns∈S\nY\nC∈C\nψC(sC; θC)\n=\nX\ns∈S\nexp (⟨θ, ϕ(s)⟩)\nA(θ) := log Z(θ)\nTherefore, for this choice of potential functions, we can view an MRF\nas an exponential family distribution (Section 3.4) with natural pa-\nrameters θ, sufficient statistic ϕ and log-partition function A(θ).\nExample 10.3 (Ising model). The Ising model is a classical example\nof MRF. Let Y = (Y1, . . . , YM) ∈{0, 1}M be an unordered collec-\ntion of binary variables Yi ∈{0, 1}. This forms a graph G = (V, E),\nwhere V = [M] and E ⊆V 2, such that (i, j) ∈E means that Yi in-\nteracts with Yj. In statistical physics, Yi may indicate the presence\nor absence of particles, or the orientation of magnets. In image\nprocessing, Yi may represent a black and white pixel. In multi-label\nclassification, Yi may indicate the presence or absence of a label.\nThe probability of y = (y1, . . . , yM) ∈{0, 1}M is then\nP(Y = y) = pθ(y)\n= exp\n\nX\ni∈V\nθiyi +\nX\n(i,j)∈E\nθi,jyiyj −A(θ)\n\n\n= exp\n X\nC∈C\n⟨θC, ϕC(y)⟩−A(θ)\n!\n,\nwhere C := V ∪E and θ ∈R|V|+|E| is the concatenation of (θi)i∈V\nand (θi,j)(i,j)∈E. These models are also known as Boltzmann ma-\nchines in a neural network context. MAP inference in general\nIsing models is known to be NP-hard, but when the interaction\nweights θi,j are non-negative, MAP inference can be reduced to\n\n10.7. Inference on chains\n227\ngraph cut algorithms (Greig et al., 1989). There are two ways the\nabove equation can be extended. First, we can use higher-order\ninteractions, such as yiyjyk for (i, j, k) ∈V3. Second, we may want\nto use categorical variables, which leads to the Potts model.\n10.6.3\nConditional random fields\nConditional random fields (Lafferty et al., 2001; Sutton, McCallum,\net al., 2012) are a special case of Markov random field, in which a\nconditioning variable is explicitly incorporated. For example, when\nthe goal is to predict a variable y conditioned on a variable x, CRFs\nare defined as\nP(Y = y | X = x) = p(y | x) =\n1\nZ(x)\nY\nC∈C\nψC(yC, x).\nNote that the potential functions ΨC are allowed to depend on the\nwhole x, as x is just a conditioning variable.\n10.6.4\nSampling\nContrary to Bayesian networks, MRFs require an explicit normalization\nconstant Z. As a result, sampling from a distribution represented by a\ngeneral MRF is usually more involved than for Bayesian networks. A\ncommonly-used technique is Gibbs sampling.\n10.7\nInference on chains\nIn this section, we review how to perform marginal inference and\nmaximum a-posteriori inference on joint distributions of the form\np(s1, . . . , sK) = 1\nZ\nK\nY\nk=1\nψk(sk−1, sk),\nwhere\nZ :=\nX\ns∈S\nK\nY\nk=1\nψk(sk−1, sk−1)\nand where we used ψk as a shorthand for ψk−1,k, since k −1 and k are\nconsecutive. As explained in Example 10.2, this also includes Markov\n\n228\nInference in graphical models as differentiation\nchains by setting\nψk(sk−1, sk) := pk(sk | sk−1),\nin which case Z = 1.\n10.7.1\nThe forward-backward algorithm\nThe key idea of the forward-backward algorithm is to use the distribu-\ntivity of multiplication over addition to write\nZ =\nX\ns1∈S1\nψ1(s0, s1)\nX\ns2∈S2\nψ2(s1, s2) · · ·\nX\nsK∈SK\nψK(sK−1, sK).\nWe can compute these sums recursively, either forward or backward.\nRecalling the definitions of Ak−1 and Bk+1 in Eq. (10.1), we define the\nsummations up to and down to k,\nαk(sk) :=\nX\ns1,...,sk−1∈Ak−1\nk\nY\nj=1\nψj(sj−1, sj)\n=\nX\nsk−1∈Sk−1\nψk(sk−1, sk) · · ·\nX\ns1∈S1\nψ2(s1, s2)ψ1(s0, s1)\nβk(sk) :=\nX\nsk+1,...,sK∈Bk+1\nK\nY\nj=k+1\nψj(sj−1, sj)\n=\nX\nsk+1∈Sk+1\nψk+1(sk, sk+1) · · ·\nX\nsK∈SK\nψK(sK−1, sK).\nWe can compute the two quantities by recursing forward and backward\nαk(sk) =\nX\nsk−1∈Sk−1\nψk(sk−1, sk)αk−1(sk−1)\nβk(sk) =\nX\nsk+1∈Sk+1\nψk+1(sk, sk+1)βk+1(sk+1)\nwhere we defined the initializations\nα1(s1) := ψ1(s0, s1)\n∀s1 ∈S1\nβK(sK) := 1\n∀sK ∈SK.\n\n10.7. Inference on chains\n229\nThe normalization term can then be computed by\nZ =\nX\nsK∈SK\nαK(sK)βK(sK) =\nX\ns1∈S1\nα1(s1)β1(s1)\nand the marginal probabilities by\nP(Sk = sk) = 1\nZ αk(sk)βk(sk)\nP(Sk−1 = sk−1, Sk = sk) = 1\nZ αk−1(sk−1)ψk(sk−1, sk)βk(sk).\nWe can also compute the conditional probabilities by\nP(Sk = sk | Sk−1 = sk−1) = P(Sk−1 = sk−1, Sk = sk)\nP(Sk−1 = sk−1)\n= αk−1(sk−1)ψk(sk−1, sk)βk(sk)\nαk−1(sk−1)βk−1(sk−1)\n= ψk(sk−1, sk)βk(sk)\nβk−1(sk−1)\n.\nIn practice, the two recursions are often implemented in the log-domain\nfor numerical stability,\nlog αk(sk) = log\nX\nsk−1∈Sk−1\nexp(log ψk(sk−1, sk) + log αk−1(sk−1))\nlog βk(sk) = log\nX\nsk+1∈Sk+1\nexp(log ψk+1(sk, sk+1) + log βk+1(sk+1)).\nWe recognize the log-sum-exp operator, which can be implemented\nin a numerically stable way (Section 4.4.2). The overall dynamic\nprogramming procedure, a.k.a. forward-backward algorithm (Baum\nand Petrie, 1966; Rabiner, 1989), is summarized in Algorithm 10.1. We\nnotice that the forward and backward passes are actually independent\nof each other, and can therefore be performed in parallel.\n10.7.2\nThe Viterbi algorithm\nSimilarly, using the distributivity of multiplication over maximization,\nmax\ns1∈S1,...,sK∈SK\nK\nY\nk=1\nψk(sk−1, sk)\n= max\nsK∈SK\nmax\nsK−1∈SK−1 ψK(sK−1, sK) . . . max\ns1∈S1 ψ2(s1, s2)ψ1(s0, s1).\n\n230\nInference in graphical models as differentiation\nAlgorithm 10.1 Marginal inference on a chain\nPotential functions: ψ1, . . . , ψK\nInput: s0\n1: Initialize α1(s1) := ψ1(s0, s1) ∀s1 ∈S1\n2: for k := 2, . . . , K do\n▷Forward pass\n3:\nfor sk ∈Sk do\n4:\nαk(sk) :=\nX\nsk−1∈Sk−1\nψk(sk−1, sk)αk−1(sk−1)\n5: Initialize βK(sK) := 1 ∀sK ∈SK\n6: for k := K −1, . . . , 1 do\n▷Backward pass\n7:\nfor sk ∈Sk do\n8:\nβk(sk) :=\nX\nsk+1∈Sk+1\nψk+1(sk, sk+1)βk+1(sk+1)\n9: Compute Z =\nX\nsK∈SK\nαK(sK)βK(sK) =\nX\nsK∈SK\nαK(sK)\nOutputs: ∀k ∈[K]:\nP(Sk = sk) = 1\nZ αk(sk)βk(sk)\nP(Sk−1 = sk−1, Sk = sk) = 1\nZ αk−1(sk−1)ψk(sk−1, sk)βk(sk)\nLet us define for k ∈[K]\nδk(sk) :=\nmax\nsk−1∈Sk−1 ψk(sk−1, sk) . . . max\ns1∈S1 ψ2(s1, s2)ψ1(s0, s1).\nWe can compute these quantities recursively, since for k ∈[K]\nδk(sk) =\nmax\nsk−1∈Sk−1 ψk(sk−1, sk)δk−1(sk−1),\nwith δ1(sk) := ψ(s0, sk). We finally have\nmax\ns1∈S1,...,sK∈SK p(s1, . . . , sK) = 1\nZ max\nsK∈SK δK(sK).\nIn practice, for numerical stability, we often implement the forward\nrecursion in the log-domain. Using the fact that the logarithm is\nmonotonic, we indeed have for all k ∈[K]\nlog δk(sk) =\nmax\nsk−1∈Sk−1 log ψk(sk−1, sk) + log δk−1(sk−1).\n\n10.8. Inference on trees\n231\nTo enable efficient backtracking, during the forward pass, we compute\nqk(sk) := arg max\nsk−1∈Sk−1\nψk(sk−1, sk)δk−1(sk−1)\nwhich can be thought as backpointers from s⋆\nk to s⋆\nk−1.\nThe resulting dynamic programming procedure, a.k.a. Viterbi algo-\nrithm (Viterbi, 1967; Forney, 1973), is summarized in Algorithm 10.2.\nAlgorithm 10.2 MAP inference on a chain\nPotential functions: ψ1, . . . , ψK\nInput: s0\n1: Initialize δ1(s1) := ψ1(s0, s1) ∀s1 ∈S1\n2: for k := 2, . . . , K do\n▷Forward pass\n3:\nfor sk ∈Sk do\n4:\nδk(sk) :=\nmax\nsk−1∈Sk−1 ψk(sk−1, sk)δk−1(sk−1)\n5:\nqk(sk) := arg max\nsk−1∈Sk−1\nψk(sk−1, sk)δk−1(sk−1)\n6: δ⋆:= max\nsK∈SK δK(sK)\n7: s⋆\nK := arg max\nsK∈SK\nδK(sK)\n8: for k := K −1, . . . , 1 do\n▷Backtracking\n9:\ns⋆\nk := qk+1(s⋆\nk+1)\nOutputs:\nmax\ns1∈S1,...,sK∈SK p(s1, . . . , sK) ∝δ⋆\narg max\ns1∈S1,...,sK∈SK\np(s1, . . . , sK) = (s⋆\n1, . . . , s⋆\nK)\n10.8\nInference on trees\nMore generally, efficient inference based on dynamic programming\ncan be performed when dependencies between variables are expressed\nusing a tree or polytree. The resulting marginal inference and MAP\ninference algorithms are often referred to as the sum-product and\nmax-sum algorithms. The sum-product algorithm is also known as\nbelief propagation or message passing, since it can be interpreted\nas propagating “local messages” through the graph. See for instance\n(Wainwright and Jordan, 2008, Section 2.5.1) for more details.\n\n232\nInference in graphical models as differentiation\n10.9\nInference as differentiation\nIn this section, we review the profound connections between differenti-\nating the log-partition function of an exponential family distribution\non one hand, and performing marginal inference (as well as maximum\na-posteriori inference in the zero-temperature limit) on the other hand.\n10.9.1\nInference as gradient of the log-partition\nWe first discuss a well-known fact in the graphical model literature: when\nusing a binary encoding as the sufficient statistic ϕ in an exponential\nfamily distribution, the gradient ∇A(θ) of the log-partition A(θ) gathers\nall the marginals (Wainwright and Jordan, 2008).\nTo see why, recall from Section 3.4 the definition of an exponential\nfamily distribution\npθ(s) = h(s) exp [⟨θ, ϕ(s)⟩−A(θ)]\nand of its log-partition\nA(θ) := log\nX\ns∈S\nh(s) exp [⟨θ, ϕ(s)⟩] .\nFrom Proposition 3.2,\nµ(θ) := ∇A(θ) = EY ∼pθ[ϕ(Y )] ∈M.\nTherefore, with the binary encodings in Eq. (10.2) and Eq. (10.3),\nP(Sk = vi) = [∇A(θ)]k,i\nP(Sk = vi, Sl = vj) = [∇A(θ)]k,l,i,j.\nPut differently, if we have an efficient algorithm for computing A(θ), we\ncan perform reverse-mode autodiff on A(θ) to obtain ∇A(θ), and\ntherefore obtain the marginal probabilities. Following Section 8.3.3, the\ncomplexity of computing all marginal probabilities is therefore roughly\nthe same as that of computing A(θ).\nIn the special case of chains, we obtain\nP(Sk = vi) = [∇A(θ)]k,i = 1\nZ αk(vi)βk(vi)\nP(Sk−1 = vi, Sk = vj) = [∇A(θ)]k−1,k,i,j = 1\nZ αk−1(vi)ψk(vi, vj)βk(vj),\n\n10.9. Inference as differentiation\n233\nwhere we left the dependence of Z, α and β on θ implicit.\nIf we define Aε(θ) := εA(θ/ε), in the zero-temperature limit ε →0,\nwe obtain that µ(θ) is a binary encoding of the mode, i.e., of the\nmaximum a-posteriori inference solution.\nWe now show i) how to unify the forward pass of the forward-\nbackward and Viterbi algorithms using semirings and softmax operators\nii) how to compute the gradient of the log-partition using backpropaga-\ntion.\n10.9.2\nSemirings and softmax operators\nThe forward passes in the forward-backward and Viterbi algorithms are\nclearly similar. In fact, they can be formally linked to each other using\nsemirings.\nDefinition 10.1 (Semiring). A semiring is a set K equipped with\ntwo binary operations (⊕, ⊗) such that\n• ⊗is commutative and associative,\n• ⊕is associative and distributive over ⊕,\n• ⊗and ⊕have identity element ¯0 and ¯1, respectively.\nWe use the notations ⊕, ⊗, ¯0 and ¯1 to clearly distinguish them from\nthe classical addition, multiplication, 0 and 1.\nWe recall the following laws for binary operations:\n• Commutativity of ⊕: a ⊕b = b ⊕a,\n• Associativity of ⊕: a ⊕(b ⊕c) = (a ⊕b) ⊕c,\n• Distributivity of ⊗over ⊕: a ⊗(b ⊕c) = (a ⊗b) ⊕(a ⊗c).\nA set equipped with a binary operation supporting associativity and an\nidentity element is called a monoid. A monoid such that every element\nhas an inverse element is called a group. The difference between a ring\nand a semiring is that the latter only requires (K, ⊕) and (K, ⊗) to be\nmonoids, not groups.\n\n234\nInference in graphical models as differentiation\nEquipped with these definitions, we can interpret the forward passes\nin the Viterbi and forward-backward algorithms as follows:\n• the forward-backward algorithm in the exponential domain uses\nthe semiring R+ equipped with (+, ×) and identity elements (0, 1);\n• the Viterbi algorithm in the log domain uses the semiring R\nequipped with (max, +) and identity elements (−∞, 0);\n• the forward-backward algorithm in the log domain uses the semir-\ning R equipped with (maxε, +) and identity elements (−∞, 0),\nwhere we defined the soft max operator (log-add-exp)\nmaxε(a, b) := ε log((exp(a) + exp(b))/ε),\nwith ε := 1 by default.\nIt can be checked that indeed maxε is commutative, associative, and\naddition is distributive over maxε. Its identity element is −∞. By\nassociativity,\nmaxε(a1, maxε(a2, a3)) = logsumexpε(a1, a2, a3)\n= ε log\nX\ni\nexp(ai/ε).\nIn contrast, note that the sparsemax in Section 13.5 is not associative.\nThanks to associativity, we can introduce the shorthand notations\nmaxε\nv∈V\nf(v) := ε log\nX\nv∈V\nexp(f(v)/ε) ∈R.\nand\nargmaxε\nv∈V\nf(v) :=\n \nexp(f(v′)/ε)/\nX\nv∈V\nexp(f(v)/ε)\n!\nv′∈V\n∈P(V).\nMany algorithms can be generalized thanks to the use of semirings;\nsee among others Aji and McEliece (2000) and Mohri et al. (2008). The\ndistributive and associative properties play a key role in breaking down\nlarge problems into smaller ones (Verdu and Poor, 1987).\n\n10.9. Inference as differentiation\n235\n10.9.3\nInference as backpropagation\nIn this section, we show that, algorithmically, backtracking is recovered\nas a special case of backpropagation. See also (Eisner, 2016; Mensch\nand Blondel, 2018).\nFor notation simplicity, we assume S0 = {1} and Sk = {1, . . . , M}\nfor all k ∈[K]. We focus on the case\nlog ψk(i, j) = ⟨θk, ϕk(i, j)⟩= θk,i,j.\nWe also introduce the shorthands\na1,j := log α1(j) = θ1,1,j\nak,j := log αk(j) = maxε\ni∈[M]\nθk,i,j + ak−1,i\nand\nqk,j := argmaxε\ni∈[M]\nθk,i,j + ak−1,i.\nOur goal is to compute the gradient w.r.t. θ ∈RK×M×M of\nlog Z = A = maxε\nj∈[M]\naK,j.\nThe soft argmax counterpart of this quantity is\nQ := argmaxε\nj∈[M]\naK,j ∈△M,\nwhere we used P([M]) = △M.\nComputing the gradient of A is similar to computing the gradient\nof a feedforward network, in the sense that θk influences not only ak\nbut also ak+1, . . . , aK. Let us introduce the adjoint variable\nrk,i := ∂A\n∂ak,i\n,\nwhich we initialize as\nrK,i =\n∂A\n∂aK,i\n= Qi.\n\n236\nInference in graphical models as differentiation\nSince θk,i,j directly influences ak,j, we have for k ∈[K], i ∈[M] and\nj ∈[M]\nµk,i,j :=\n∂A\n∂θk,i,j\n= ∂A\n∂ak,j\n· ∂ak,j\n∂θk,i,j\n= rk,j · qk,j,i.\nSince ak,i directly influences ak+1,j for j ∈[M], we have for k ∈\n{1, . . . , K −1} and i ∈[M]\nrk,i = ∂A\n∂ak,i\n=\nX\nj∈[M]\n∂A\n∂ak+1,j\n∂ak+1,j\n∂ak,i\n=\nX\nj∈[M]\nrk+1,jqk+1,j,i\n=\nX\nj∈[M]\nµk+1,i,j.\nWe summarize the procedure in Algorithm 10.3. The forward pass\nuses the softmax operator maxε and the softargmax operator argmaxε.\nIn the hard max case, in Algorithm 10.2, we used q to store backpointers\nfrom integer to integer. In the soft max case, in Algorithm 10.3, we used q\nto store soft backpointers, that is, discrete probability distributions. In\nthe zero-temperature limit, backpropagation outputs a binary encoding\nof the solution of backtracking.\n\n10.10. Summary\n237\nAlgorithm 10.3 Inference on a chain as backprop with max operators\nInput: θ ∈RK×M×M\nMax operator: maxε\n1: Initialize a1,j := θ1,1,j ∀j ∈[M]\n2: for k := 2, . . . , K do\n▷Forward pass\n3:\nfor j ∈[M] do\n4:\nak,j := maxε\ni∈[M]\nθk,i,j + ak−1,j ∈R\n5:\nqk,j := argmaxε\ni∈[M]\nθk,i,j + ak−1,j ∈△M\n6: A := maxε\ni∈[M]\naK,i ∈R\n7: Q := argmaxε\ni∈[M]\naK,i ∈△M\n8: Initialize rK,j = Qj\n∀j ∈[K]\n9: for k := K −1, . . . , 1 do\n▷Backward pass\n10:\nfor i ∈[M] do\n11:\nfor j ∈[M] do\n12:\nµk+1,i,j = rk+1,j · qk+1,j,i\n13:\nrk,i ←µk+1,i,j\nOutputs:\nmaxε\ni1,...,iK∈[M]Kθ1,1,i1 + PK\nk=2 θk,ik−1,ik = A, ∇A(θ) = µ\n10.10\nSummary\n• Graphical models represent the conditional dependencies between\nvariables and therefore specify how their joint distribution factor-\nizes.\n• There are clear analogies between the worlds of functions and of\ndistributions: the counterparts of computation chains and compu-\ntation graphs are Markov chains and Bayesian networks.\n• Inference on chains and more generally on trees, for exponential\nfamily distributions, is equivalent, both statistically and algorith-\nmically, to differentiating the log-partition function.\n• The forward-backward algorithm can be seen as using a sum-\n\n238\nInference in graphical models as differentiation\nproduct algebra, while the Viterbi algorithm can be seen as using\na max-plus algebra. Equivalently, in the log domain, we can see\nthe former as using a soft max, and the latter as using a hard\nmax.\n\n11\nDifferentiating through optimization\nIn this chapter, we study how to differentiate through optimization\nproblems, and more generally through nonlinear systems of equations.\n11.1\nImplicit functions\nImplicit functions are functions that do not enjoy an explicit decompo-\nsition into elementary functions, for which automatic differentiation, as\nstudied in Chapter 8, can therefore not be directly applied. We describe\nin this chapter techniques to differentiate through such functions and\nhow to integrate them into an autodiff framework.\nFormally, we will denote an implicit function by w⋆(λ), where\nw⋆: Λ →W. One question is then how to compute the Jacobian\n∂w⋆(λ). As a first application one can consider sensitivity analysis\nof a system. For example, w⋆(λ) could correspond to the equilibrium\nstate of a physical system and in this case, ∂w⋆(λ) would tell us about\nthe sensitivity of the system to some parameters λ ∈Λ.\n239\n\n240\nDifferentiating through optimization\n11.1.1\nOptimization problems\nAnother example is a function implicitly defined as the solution (assumed\nunique) of an optimization problem\nw⋆(λ) = arg max\nw∈W\nf(w, λ),\nwhere f : W × Λ →R and W denotes a constraint set. Note that we\nuse an arg max for convenience, but the same applies when using an\narg min.\n11.1.2\nNonlinear equations\nMore generally, w⋆(λ) can be defined as the root of some function\nF : W × Λ →W, i.e., w⋆(λ) is implicitly defined as the function\nsatisfying the (potentially nonlinear) system of equations\nF(w, λ) = 0\nfor all λ ∈Λ.\n11.1.3\nApplication to bilevel optimization\nBesides sensitivity analysis, another example of application is bilevel\noptimization. Many times, we want to minimize a function defined as\nthe composition of a fixed function and the solution of an optimization\nproblem. Formally, let f, g: W × Λ →R. We consider the composition\nh(λ) defined as\nh(λ) := g(w⋆(λ), λ),\nwhere\nw⋆(λ) = arg max\nw∈W\nf(w, λ).\n(11.1)\nThis includes for instance hyperparameter optimization, where f is an\ninner log-likelihood objective, g is an outer validation loss, w ∈W\nare model parameters and λ ∈Λ are model hyperparameters, such as\nregularization strength, as illustrated in Fig. 11.1. To minimize h(λ) one\ngenerally resorts to a gradient descent scheme w.r.t. λ, which requires\ncomputing ∇h(λ). Assuming that w⋆(λ) is differentiable at λ, by the\nchain rule, we obtain the Jacobian\n∂h(λ) = ∂1g(w⋆(λ), λ)∂w⋆(λ) + ∂2g(w⋆(λ), λ).\n\n11.2. Envelope theorems\n241\n0\n1\n2\n10\n5\n0\n5\nw (\n1)\nw (\n2)\nw (\n)\nFigure 11.1: Hyperparameter optimization in nonlinear regression can be cast as a bi-\nlevel optimization problem. Each line corresponds to the estimator obtained by fitting\nsome training data (in blue circles) using a different hyperparameter λ. Formally,\ndenoting f the training objective, the estimators are w⋆(λ) := arg minw f(w; λ).\nThe goal is to find the best hyperparameter that fits some validation data (here in\ncyan diamonds), that is, minimizing h(λ) := g(w⋆(λ), λ), where g is the validation\nobjective. A too small λ1 leads to overfitting the training objective and performs\nbadly on validation objective. Conversely, a larger λ2 underfits both training and\nvalidation objectives. The optimal parameter λ⋆minimizes the validation objective\nand may be obtained by iterating gradient descent w.r.t. λ. This requires gradients\nof h(λ) = g(w⋆(λ), λ) w.r.t. λ.\nUsing ∂h(λ)⊤= ∇h(λ) (see Remark 2.4), we obtain the gradient\n∇h(λ) = ∂w⋆(λ)⊤∇1g(w⋆(λ), λ) + ∇2g(w⋆(λ), λ).\nThe only problematic term is ∂w⋆(λ), as it requires argmax differenti-\nation. Indeed, most of the time, there is no explicit formula for w⋆(λ)\nand it does not decompose into elementary functions.\n11.2\nEnvelope theorems\nIn the special case g = f, the composition h defined in Eq. (11.1) is\nsimply given by\nh(λ) = f(w⋆(λ), λ) = max\nw∈W f(w, λ).\nThat is, we no longer need argmax differentiation, but only max\ndifferentiation, which, as we shall now see is much easier. The function\nh is often called a value function (Fleming and Rishel, 2012). The\n\n242\nDifferentiating through optimization\nFunctions \n for varying \nFunction \n \nFigure 11.2: The graph of h(λ) = maxw∈W f(w, λ) is the upper-envelope of the\ngraphs of the functions λ 7→f(w, λ) for all w ∈W.\nreason for the name “envelope” is illustrated in Fig. 11.2. We emphasize\nthat there is not one, but several envelope theorems, depending on the\nassumptions on f.\n11.2.1\nDanskin’s theorem\nWhen f is concave-convex, we can use Danskin’s theorem.\nTheorem 11.1 (Danskin’s theorem). Let f : W ×Λ →R and W be\na compact convex set. Let\nh(λ) := max\nw∈W f(w, λ)\nand\nw⋆(λ) := arg max\nw∈W\nf(w, λ).\nIf f is concave in w, convex in λ, and the maximum w⋆(λ) is\nunique, then the function h is differentiable with gradient\n∇h(λ) = ∇2f(w⋆(λ), λ).\nIf the maximum is not unique, we get a subgradient.\n\n11.2. Envelope theorems\n243\nInformally, Danskin’s theorem means that we can treat w⋆(λ) as if\nit were a constant of λ, i.e., we do not need to differentiate through it,\neven though it depends on λ. Danskin’s theorem can also be used to\ndifferentiate through a minimum, h(λ) = minw∈W f(w, λ), if f(w, λ)\nis convex in w and concave in λ, as we now illustrate.\nExample 11.1 (Ilustration of Danskin’s theorem). Let us define h(λ) =\nminw∈R f(w, λ), where f(w, λ) = λ\n2w2+bw+c and λ > 0. Let w⋆(λ)\nbe the minimum. The derivative of f w.r.t. λ is 1\n2w2. From Dan-\nskin’s theorem, we have h′(λ) = 1\n2w⋆(λ). Let us check that this\nresult is correct. The derivative of f w.r.t. w is λw + b. Setting it to\nzero, we get w⋆(λ) = −b\nλ. We thus obtain h′(λ) = 1\n2\nb2\nλ2 . Let’s check\nthat the result is indeed correct. Plugging w⋆(λ) back into f(w, λ),\nwe get h(λ) = −1\n2\nb2\nλ + c. Using ( 1\nλ)′ = −1\nλ2 , we indeed obtain the\nsame result for h′(λ).\nDanskin’s theorem has a simple interpretation for functions that are\nlinear in λ as shown below.\nExample 11.2 (Convex conjugate). Let f(w, λ) := ⟨w, λ⟩−Ω(w)\nwith Ωconvex. We then have h(λ) = maxw∈W ⟨w, λ⟩−Ω(w) =:\nΩ∗(λ), where Ω∗denotes the convex conjugate of Ω. Since f satisfies\nthe conditions of Danskin’s theorem and since we have ∇2f(w, λ) =\nw, we obtain ∇h(λ) = ∇Ω∗(λ) = w⋆(λ). In other words, in this\nspecial case, the gradient of the max is equal to the argmax. This\nis due to the fact that f(w, λ) is linear in λ.\nAnother application is saddle point optimization.\nExample 11.3 (Saddle point problem). Consider the saddle point\nproblem minλ∈Λ maxw∈W f(w, λ). If it is difficult to minimize w.r.t.\nλ but easy to maximize w.r.t. w, we can rewrite the problem as\nminλ∈Λ h(λ), where h(λ) := maxw∈W f(w, λ), and use ∇h(λ) to\nperform (projected) gradient descent w.r.t. λ.\n11.2.2\nRockafellar’s theorem\nA related theorem can be proved under different assumptions about f,\nin particular without concavity w.r.t. w.\n\n244\nDifferentiating through optimization\nTheorem 11.2 (Rockafellar’s envelope theorem). Let f : W × Λ →\nR and W be a compact convex set. Let\nh(λ) := max\nw∈W f(w, λ)\nand\nw⋆(λ) := arg max\nw∈W\nf(w, λ).\nIf f is continuously differentiable in λ for all w ∈W, ∇1f is\ncontinuous and the maximum w⋆(λ) is unique, then the function\nh is differentiable with gradient\n∇h(λ) = ∇2f(w⋆(λ), λ).\nSee Rockafellar and Wets (2009, Theorem 10.31). Compared to\nDanskin’s theorem, Rockafellar’s theorem does not require f to be\nconcave-convex, but requires stronger assumptions on the differentiabil-\nity of f.\n11.3\nImplicit function theorem\n11.3.1\nUnivariate functions\nThe implicit function theorem (IFT) provides conditions under which\nan implicit relationship of the form F(x, λ) = 0 can be rewritten as a\nfunction x = x⋆(λ) locally, and provides a way to compute its derivative\nw.r.t. λ.\nTheorem 11.3 (Implicit function theorem, univariate case). Let\nF : R × R →R. Assume F(x, λ) is a continuously differentiable\nfunction in a neighborhood U of (x0, λ0) such that F(x0, λ0) = 0\nand ∂1F(x0, λ0) ̸= 0. Then there exists a neighborhood V ⊆U of\n(x0, λ0) in which there is a function x⋆(λ) such that\n• x⋆(λ0) = x0,\n• F(x⋆(λ), λ) = 0 for all λ in the neighborhood V,\n\n11.3. Implicit function theorem\n245\n• ∂x⋆(λ) = −∂2F(x⋆(λ),λ)\n∂1F(x⋆(λ),λ).\nWe postpone the proof to the multivariate case and begin with a\nclassical example of application of the theorem.\nExample 11.4 (Equation of the unit circle). We use w ≡x and λ ≡\ny for clarity. Let F(x, y) := x2 + y2 −1. In general, we cannot\nrewrite the unit circle equation F(x, y) = 0 as a function from y\nto x, because for every y ∈[−1, 1], there are always two possible\nx values, namely, x =\np\n1 −y2 or x = −\np\n1 −y2. However, locally\naround some point (x0, y0), e.g., such that x0 > 0 and y0 > 0\n(upper-right quadrant), the function x = x⋆(y) =\np\n1 −y2 is well-\ndefined. Using ∂1F(x, y) = 2x and ∂2F(x, y) = 2y, we get ∂x⋆(y) =\n−\n2y\n2x⋆(y) = −\ny\n√\n1−y2 in that neighborhood (the upper right quadrant\nin this case). This is indeed the same derivative expression as if we\nused the chain rule on\np\n1 −y2 and is well-defined on y ∈[0, 1).\nIn the above simple example, we can easily derive an explicit function\nrelating y to x in a given neighborhood, but this is not always the case.\nThe IFT gives us conditions guaranteeing that such function exists and\na way to differentiate it, but not a way to construct such a function.\nIn fact, finding x⋆(λ) such that F(x⋆(λ), λ) = 0 typically involves a root\nfinding algorithm, an optimization algorithm, a nonlinear system solver,\netc.\nExample 11.5 (Polynomial). Let F(w, λ) = w5 + w3 + w −λ. Ac-\ncording to the Abel-Ruffini theorem (Tignol, 2015), quintics (poly-\nnomials of degree 5) do no enjoy roots in terms of radicals and\none must resort to numerical root finding. In addition, odd-degree\npolynomials have real roots. Moreover, ∂1F(w, λ) = 5w4 + 3w2 + 1\nis strictly positive. Therefore, by the intermediate value theorem,\nthere must be only one root w⋆(λ) such that F(w⋆(λ), λ) = 0. This\nunique root can for example be found by bisection. Using the IFT,\nits derivative is found to be ∂w⋆(λ) = (5w⋆(λ)4 + 3w⋆(λ)2 + 1)−1.\nWhile an implicit function is differentiable at a point if the assump-\ntions of the IFT hold in a neighborhood of that point, the reciprocal is\n\n246\nDifferentiating through optimization\nnot true: failure of the IFT assumptions does not necessarily mean that\nthe implicit function is not differentiable, as we now illustrate.\nExample 11.6 (IFT conditions are not necessary for differentiability).\nConsider F(w, λ) = (w −λ)2. We clearly have that F(w⋆(λ), λ) = 0\nif we define w⋆(λ) = λ, the identity function. It is clearly differen-\ntiable for all λ, yet the assumptions of the IFT fail, since we have\n∂1F(w, λ) = 2(w −λ) and therefore ∂1F(0, 0) = 0.\n11.3.2\nMultivariate functions\nWe now present the IFT in the general multivariate setting. Informally,\nif F(w⋆(λ), λ) = 0, then by the chain rule, we have\n∂1F(w⋆(λ), λ)∂w⋆(λ) + ∂2F(w⋆(λ), λ) = 0,\nmeaning that the Jacobian ∂w⋆(λ), assuming that it exists, satisfies\n−∂1F(w⋆(λ), λ)∂w⋆(λ) = ∂2F(w⋆(λ), λ).\nThe IFT gives us conditions for the existence of ∂w⋆(λ).\nTheorem 11.4 (Implicit function theorem, multivariate case). Let us\ndefine F : W × Λ →W. Assume F(w, λ) is a continuously differen-\ntiable function in a neighborhood of (w0, λ0) such that F(w0, λ0) =\n0 and ∂1F(w0, λ0) is invertible, i.e., its determinant is nonzero.\nThen there exists a neighborhood of λ0 in which there is a function\nw⋆(λ) such that\n• w⋆(λ0) = w0,\n• F(w⋆(λ), λ) = 0 for all λ in the neighborhood,\n• −∂1F(w⋆(λ), λ)∂w⋆(λ) = ∂2F(w⋆(λ), λ)\n⇐⇒∂w⋆(λ) = −∂1F(w⋆(λ), λ)−1∂2F(w⋆(λ), λ).\nWe begin with a simple unconstrained optimization algorithm.\n\n11.3. Implicit function theorem\n247\nExample 11.7 (Unconstrained optimization). Assume we want to\ndifferentiate through w⋆(λ) = arg minw∈RP f(w, λ), where f is\nstrictly convex in w, which ensures that the solution is unique. From\nthe stationary conditions, if we define F(w, λ) := ∇1f(w, λ), then\nw⋆(λ) is uniquely characterized as the root of F in the first argu-\nment, i.e., F(w⋆(λ), λ) = 0. We have ∂1F(w, λ) = ∇2\n1f(w, λ), the\nHessian of f in w, and ∂2F(w, λ) = ∂2∇1f(w, λ), the cross deriva-\ntives of f. Therefore, assuming that the Hessian is well-defined and\ninvertible at (w⋆(λ), λ), we can use the IFT to differentiate through\nw⋆(λ) and obtain ∂w⋆(λ) = −∇2\n1f(w⋆(λ), λ)∂2∇1f(w⋆(λ), λ).\nNext, we generalize the previous example, by allowing constraints\nin the optimization problem.\nExample 11.8 (Constrained optimization). Now, assume we want\nto differentiate through w⋆(λ) = arg minw∈C f(w, λ), where f is\nstrictly convex in w and C ⊆W is a convex set. A solution is\ncharacterized by the fixed point equation w⋆(λ) = PC(w⋆(λ) −\nη∇1f(w⋆(λ), λ)), for any η > 0, where P(y) := arg minx∈C ∥x−y∥2\n2\nis the Euclidean projection of y onto C. Therefore, w⋆(λ) is the\nroot of F(w, λ) = w −PC(w −η∇1f(w, λ)) (see Chapter 16). We\ncan differentiate through w⋆(λ) using the IFT, assuming that the\nconditions of the theorem apply. Note that ∂1F(w, λ) requires\nthe expression of the Jacobian ∂PC(y). Fortunately, PC(y) and its\nJacobian are easy to compute for many sets C (Blondel et al., 2021).\n11.3.3\nJVP and VJP of implicit functions\nTo integrate an implicit function w⋆(λ) in an autodiff framework, we\nneed to be able to compute its JVP or VJP. This is the purpose of the\nnext proposition.\nProposition 11.1 (JVP and VJP of implicit functions). Let w⋆: Λ →\nW be a function implicitly defined as the solution of F(w⋆(λ), λ) =\n\n248\nDifferentiating through optimization\n0, for some function F : W × Λ →W. Define\nA := −∂1F(w⋆(λ), λ)\nB := ∂2F(w⋆(λ), λ).\nAssume the assumptions of the IFT hold. The JVP t := ∂w⋆(λ)v\nin the input direction v ∈Λ is obtained by solving the linear system\nAt = Bv.\nThe VJP ∂w⋆(λ)∗u in the output direction u ∈W is obtained by\nsolving the linear system\nA∗r = u.\nUsing the solution r, we get\n∂w⋆(λ)∗u = ∂w⋆(λ)∗A∗r = B∗r.\nNote that in the above linear systems, we can access to A and B as\nlinear maps, the JVPs of F. Their adjoints, A∗and B∗, correspond to\nthe VJPs of F. To solve these systems, we can therefore use matrix-free\nsolvers as detailed in Section 9.4. For example, when A is symmetric pos-\nitive semi-definite, we can use the conjugate gradient method (Hestenes,\nStiefel, et al., 1952). When A is not symmetric positive definite, we can\nuse GMRES (Saad and Schultz, 1986) or BiCGSTAB (Vorst and Vorst,\n1992).\n11.3.4\nProof of the implicit function theorem\nWe prove the theorem using the inverse function theorem presented\nin Theorem 11.5. Define\nf(λ, w) = (λ, F(w, λ))\nwhich goes from RQ × RP onto RQ × RP . The Jacobian of f is\n∂f(λ, w) =\n \nI\n0\n∂2F(w, λ)\n∂1F(w, λ)\n!\n.\nSo at w0, λ0, we have det(∂f(λ0, w0)) = det(I) det(∂1F(w0, λ0)) >\n0 since we assumed ∂1F(w0, λ0) invertible. By the inverse function\n\n11.4. Adjoint state method\n249\ntheorem, the function f is then invertible in a neighborhood N of\nf(λ0, w0) = (λ0, 0). In particular, it is invertible in N ∩{(λ, 0), λ ∈\nRQ}. The solution of the implicit equation in a neighborhood of λ0\nis then (λ, w∗(λ)) = f−1(λ, 0). By the inverse function theorem, f−1\nis continuously differentiable inverse and so is w∗(λ). The derivative\n∂w∗(λ) from the differential of the inverse as\n \n∼\n∼\n∂w∗(λ)\n∼\n!\n= ∂f−1(λ, 0),\nand by the inverse function theorem, we have ∂f−1(λ, 0) = (∂f(λ, w∗(λ)))−1.\nSo using block matrix inversions formula\n \nA\nB\nC\nD\n!−1\n=\n \n∼\n∼\n−(D −CA−1B)−1CA−1\n∼\n!\n,\nwe get the claimed expression. Though we expressed the proof in terms of\nJacobians and matrices, the result naturally holds for the corresponding\nlinear operators, JVPs, VJPs, and their inverses.\n11.4\nAdjoint state method\n11.4.1\nDifferentiating nonlinear equations\nWe describe in this section the adjoint state method (a.k.a. adjoint\nmethod, method of adjoints, adjoint sensitivity method). The method\ncan be used to compute the gradient of the composition of an explicit\nfunction and an implicit function, defined through an equality\nconstraint (e.g., a nonlinear equation). The method dates back\nto Céa (1986).\nSuppose a variable s ∈S (which corresponds to a state in optimal\ncontrol) is implicitly defined given some parameters w ∈W through\nthe (potentially nonlinear) equation c(s, w) = 0, where c: S × W →S.\nAssuming s is uniquely determined for all w ∈W, this defines an\nimplicit function s⋆(w) from W to S such that c(s⋆(w), w) = 0.\nGiven an objective function L: S × W →R, the goal of the adjoint\nstate method is then to compute the gradient of\nL(w) := L(s⋆(w), w).\n\n250\nDifferentiating through optimization\nHowever, this is not trivial as s⋆(w) is an implicit function. For instance,\nthis can be used to convert the equality-constrained problem\nmin\nw∈W L(s, w)\ns.t.\nc(s, w) = 0.\ninto the unconstrained problem\nmin\nw∈W L(s⋆(w), w).\nAccess to ∇L(w) allows us to solve this problem by gradient descent.\nProposition 11.2 (Adjoint state method). Let c : S × W →S be a\nmapping defining constraints of the form c(s, w). Assume that for\neach w ∈W, there exists a unique s⋆(w) satisfying c(s⋆(w), w) = 0\nand that s⋆(w) is differentiable. The gradient of\nL(w) := L(s⋆(w), w),\nfor some differentiable function L : S × W →R, is given by\n∇L(w) = ∇2L(s⋆(w), w) + ∂2c(s⋆(w), w)∗r⋆(w),\nwhere r⋆(w) is the solution of the linear system\n∂1c(s⋆(w), w)∗r = −∇1L(s⋆(w), w).\nAs shown in the proof below, r⋆(w) corresponds to a Lagrange\nmultiplier. The linear system can be solved using matrix-free solvers.\n11.4.2\nRelation with envelope theorems\nBecause s is uniquely determined for any w ∈W by c(s, w) = 0, we can\nalternatively rewrite L(w) as the trivial minimization or maximization,\nL(w) = min\ns∈S L(s, w)\ns.t.\nc(s, w) = 0\n= max\ns∈S L(s, w)\ns.t.\nc(s, w) = 0.\nTherefore, the adjoint state method can be seen as an envelope theorem\nfor computing ∇L(w), for the case when w is involved in both the\nobjective function and in the equality constraint.\n\n11.4. Adjoint state method\n251\n11.4.3\nProof using the method of Lagrange multipliers\nClassically, the adjoint state method is derived using the method of\nLagrange multipliers. Let us introduce the Lagrangian associated with\nL and c,\nL(s, w, r) := L(s, w) + ⟨r, c(s, w)⟩,\nwhere r ∈S is the Lagrange multiplier associated with the equality\nconstraint c(s, w) = 0. In the optimal control literature, r is often\ncalled the adjoint variable or adjoint state. The gradients of the\nLagrangian are\n∇sL(s, w, r) = ∇1L(s, w) + ∂1c(s, w)∗r\n∇wL(s, w, r) = ∇2L(s, w) + ∂2c(s, w)∗r\n∇rL(s, w, r) = c(s, w),\nwhere ∂ic(s, w)∗are the adjoint operators. Setting ∇rL(s, w, r) to\nzero gives the constraint c(s, w) = 0. Setting ∇sL(s, w, r) to zero gives\nthe so-called adjoint state equation\n∂1c(s, w)∗r = −∇1L(s, w).\nSolving this linear system w.r.t. r at s = s⋆(w) gives the adjoint variable\nr⋆(w). We then get\n∇L(w) = ∇2L(s⋆(w), w, r⋆(w))\n= ∇2L(s⋆(w), w) + ∂2c(s⋆(w), w)∗r⋆(w),\nwhich concludes the proof.\n11.4.4\nProof using the implicit function theorem\nA more direct proof is possible thanks to the implicit function theorem\n(Section 11.3). Using the chain rule, we get\n∇L(w) = ∇2L(s⋆(w), w) + ∂s⋆(w)∗∇1L(s⋆(w), w),\nwhere ∂s⋆(w)∗is the VJP of s⋆, a linear map from S to W.\nComputationally, the main difficulty is to apply ∂s⋆(w)∗to the\nvector u = ∇1L(s⋆(w), w) ∈S. Using the implicit function theorem\n\n252\nDifferentiating through optimization\n(Section 11.3) on the implicit function c(s⋆(w), w) = 0, and Proposi-\ntion 11.1, we get the linear system A∗r = u, where A∗:= ∂1c(s⋆(w), w)∗\nis a linear map from S to S. After solving for r, we get ∂s⋆(w)∗u = B∗r,\nwhere B∗:= ∂2c(s⋆(w), w)∗is a linear map from S to W. Putting ev-\nerything together, we get\n∇L(w) = ∇2L(s⋆(w), w) + ∂2c(s⋆(w), w)∗r.\n11.4.5\nReverse mode as adjoint method with backsubstitution\nIn this section, we revisit reverse-mode autodiff from the perspective\nof the adjoint state method. For clarity, we focus our exposition on\nfeedforward networks with input x ∈X and network weights w =\n(w1, . . . , wk) ∈W1 × . . . × WK,\ns0 := x ∈X\ns1 := f1(s0, w1) ∈S1\n...\nsK := fK(sK−1, wK) ∈SK\nf(w) := sK.\n(11.2)\nHere we focus on gradients with respect to the parameters w, hence\nthe notation f(w). We can use the adjoint state method to recover\nreverse-mode autodiff, and prove its correctness in the process. While\nwe focus for simplicity on feedforward networks, our exposition can be\ngeneralized to computation graphs.\nFeedforward networks as the solution of a nonlinear equation\nWhile we defined the set of intermediate computations s = (s1, . . . , sK) ∈\nS1 × . . . × SK as a sequence of operations, they can also be defined as\nthe unique solution of the nonlinear equation c(s, w) = 0, where\nc(s, w) :=\n\n\n\n\n\n\ns1 −f1(x, w1)\ns2 −f2(s1, w2)\n...\nsK −fK(sK−1, wK)\n\n\n\n\n\n\n.\n\n11.4. Adjoint state method\n253\nThis defines an implicit function s⋆(w) = (s⋆\n1(w), . . . , s⋆\nK(w)), the\nsolution of this nonlinear system, which is given by the variables\ns1, . . . , sK defined in (11.2). The output of the feedforward network is\nthen f(w) = s⋆\nK(w).\nIn machine learning, the final layer s⋆\nK(w) is typically fed into a\nloss ℓ, to define\nL(w) := ℓ(s⋆\nK(w); y).\nNote that an alternative is to write L(w) as\nL(w) = min\ns∈S ℓ(s; y)\ns.t.\nc(s, w) = 0.\nMore generally, if we just want to compute the VJP of s⋆\nK(w) in\nsome direction uK ∈SK, we can define the scalar-valued function\nL(w) := ℓ(s⋆\nK(w); uK) := ⟨s⋆\nK(w), uK⟩\nso that\n∂f(w)∗uK = ∇L(w).\nLet us define u ∈S1 × · · · × SK−1 × SK as u := (0, . . . , 0, ∇1ℓ(f(w); y))\n(gradient of the loss ℓcase) or u := (0, . . . , 0, uK) (VJP of f in the\ndirection uK case). Using the adjoint state method, we know that the\ngradient of this objective is obtained as\n∇L(w) = ∂2c(s(w), w)∗r⋆(w),\nfor r⋆(w) the solution of the linear system\n∂1c(s(w), w)∗r = −u.\nSolving the linear system using backsubtitution\nThe JVP of the constraint function c at s⋆(w), materialized as a matrix,\ntakes the form of a block lower-triangular matrix\n∂1c(s⋆(w), w) =\n\n\n\n\n\n\n\n\n\n\nI\n0\n. . .\n. . .\n0\n−A1\nI\n...\n...\n0\n−A2\nI\n...\n...\n...\n...\n...\n...\n0\n0\n. . .\n0\n−AK\nI\n\n\n\n\n\n\n\n\n\n\n,\n\n254\nDifferentiating through optimization\nwhere Ak := ∂1fk(sk−1, wk). Crucially the triangular structure of the\nJVP stems from the fact that each intermediate activation only de-\npends from the past intermediate activations. Therefore, the constraints,\ncorresponding to the lines of the Jacobian, cannot introduce non-zero\nvalues beyond its diagonal. The VJP takes the form of a block upper-\ntriangular matrix\n∂1c(s⋆(w), w)∗=\n\n\n\n\n\n\n\n\n\n\nI\n−A∗\n1\n0\n. . .\n0\n0\nI\n−A∗\n2\n...\n...\n...\n...\nI\n...\n0\n...\n...\n...\n−A∗\nK\n0\n. . .\n. . .\n0\nI\n\n\n\n\n\n\n\n\n\n\n.\nSolving an upper triangular system like ∂1c(s(w), w)∗r = u can then\nbe done efficiently by backsubstitution. Starting from the last adjoint\nstate rK = u, we can compute each adjoint state rk from that computed\nat k + 1. Namely, for k ∈(K −1, . . . , 1), we have\nrk −A∗\nk+1rk+1 = 0 ⇐⇒rk = ∂fk+1(sk, wk+1)∗rk+1.\nThe VJPs with respect to the parameters are then obtained by\n∂2c(s(w), w)∗r =\n\n\n\n\n\n\n∂2f1(x, w1)∗r1\n∂2f2(s1(w), w2)∗r2\n...\n∂2fK(s1(w), wK)∗rK\n\n\n\n\n\n\n,\nrecovering reverse-mode autodiff.\nThe Lagrangian perspective of backpropagation for networks with\nseparate parameters w = (w1, . . . , wK) is well-known; see for instance\nLeCun (1988) or Recht (2016). The Lagrangian perspective of back-\npropagation through time (Werbos, 1990) for networks with shared\nparameter w is discussed for instance by Franceschi et al. (2017). Our\nexposition uses the adjoint state method, which can itself be proved\neither using the method of Lagrange multipliers (Section 11.4.3) or by\nthe implicit function theorem (Section 11.4.4), combined with backsub-\ntitution for solving the upper-triangular linear system. Past works often\n\n11.5. Inverse function theorem\n255\nminimize over w but we do not require this, as gradients are not neces-\nsarily used for optimization. Our exposition also supports computing\nthe VJP of any vector-valued function f, while existing works derive\nthe gradient of a scalar-valued loss function.\n11.5\nInverse function theorem\n11.5.1\nDifferentiating inverse functions\nIn some cases (see for instance Section 12.4.4), it is useful to compute\nthe Jacobian of an inverse function f−1. The inverse function theorem\nbelow allows us to relate the Jacobian of f−1 with the Jacobian of f.\nTheorem 11.5 (Inverse function theorem). Assume f : W →W is\ncontinuously differentiable with invertible Jacobian ∂f(w0) at w0.\nThen f is bijective from a neighborhood of w0 to a neighborhood\nof f(w0). Moreover, the inverse f−1 is continuously differentiable\nnear ω0 = f(w0) and the Jacobian of the inverse ∂f−1(ω) is\n∂f(w)∂f−1(ω) = I ⇔∂f−1(ω) = (∂f(w))−1,\nwith w = f−1(ω).\n11.5.2\nLink with the implicit function theorem\nThe inverse function theorem can be used to prove the implicit function\ntheorem; see proof of Theorem 11.4. Conversely, recall that, in order to\nuse the implicit function theorem, we need to choose a root objective\nF : W × Λ →W. If we set W = Λ = RQ and F(w, ω) = f(w) −\nω, with f : RQ →RQ, then we have that the root w⋆(ω) satisfying\nF(w⋆(ω), ω) = 0 is exactly w⋆(ω) = f−1(ω). Moreover, ∂1F(w, ω) =\n∂f(w) and ∂2F(w, ω) = −I. By applying the implicit function theorem\nwith this F, we indeed recover the inverse function theorem.\n11.5.3\nProof of inverse function theorem\nWe first give a proof of the formula assuming that f−1 is well-defined\nand continuously differentiable in a neighborhood of f(w0). In that\n\n256\nDifferentiating through optimization\ncase, we have for any ω in a neighborhood of f(w0),\nf ◦f−1(ω) = ω.\nDifferentiating both sides w.r.t. ω, we get\n∂f(f−1(ω))∂f−1(ω) = I,\nwhere I is the identity function in RQ. In particular, for w = f−1(ω)\nwe recover the formula presented in the statement.\nNow, it remains to show that invertibility of the JVP ensures that\nthe function is invertible in a neighborhood of f(w0) and that the\ninverse is continuously differentiable. For that, denote l = ∂f(w0) such\nthat l−1 is well-defined by definition. f is invertible with continuously\ndifferentiable inverse, if and only if l−1(f(w)) −f(w0) is invertible with\ncontinuously differentiable inverse. So without loss of generality, we\nconsider ∂f(w0) = I, f(w0) = 0, w0 = 0.\nAs f is continuously differentiable, there exists a neighborhood\nN = {w : ∥w −w0∥2 ≤δ} on which we have ∥∂f(w) −I ∥2 ≤1/2. In\nthis neighborhood, the function g(w) = f(w) −w is contractive by the\nmean value theorem with contraction factor 1/2. For any ω such that\n∥ω −f(w0)∥2 ≤δ/2, the sequence wk+1 = wk −f(wk) −ω′ remains in\nN and converges (since it is a Cauchy sequence by the contraction of g)\nto a unique fixed point w∞satisfying w∞= w∞−f(w∞) −ω ⇐⇒\nf(w∞) = ω. This shows the existence of the inverse in the neighborhood\nM = {ω : ∥ω −ω0∥2 ≤δ/2} of ω0 = f(w0) onto N.\nWe tackle now the differentiability (hence the continuity) of f−1.\nFor any ω in the neighborhood of ω0 with inverse w := f−1(ω) ∈N,\nthe JVP of f at w satisfies by assumption ∥∂f(w) −I ∥2 ≤1/2. Hence,\na = ∂f(w) −I defines a convergent series b = P+∞\nk=0 ak and one verifies\neasily that b = ∂f(w)−1, that is ∂f(w) is invertible and ∥∂f(w)−1∥≤2.\nTo compute the JVP of the inverse, we consider then ∂f(w)−1 as the\ncandidate JVP and examine\n∥f−1(ω + η) −f(ω) −(∂f(w))−1η∥2\n∥η∥2\n.\nDenote then v such that f−1(ω + η) = w + v. As g(w) = f(w) −w is\n1/2-contractive in N, we have ∥v−η∥2 = ∥g(w+v)−g(w)∥2 ≤1/2∥v∥2.\n\n11.6. Summary\n257\nSo ∥v∥2 ≥∥η∥/2. We then get\n∥f−1(ω + η) −f(ω) −(∂f(w))−1η∥2\n∥η∥2\n= ∥v −(∂f(w))−1(f(w + v) −f(w))∥2\n∥η∥2\n≤4∥f(w + v) −f(w) −∂f(w)v∥2\n∥u∥2\nAs ∥η∥2 →0, we have ∥v∥2 →0 and so ∥f(w+v)−f(w)−∂f(w)v∥2/∥v∥2 →\n0. Hence, f−1 is differentiable with JVP ∂f−1(ω) = (∂f(w))−1 =\n(∂f(f−1(ω)))−1. This shows that f−1 is continuous and so ∂f−1(ω) is\ncontinuous as a composition of continuous functions.\n11.6\nSummary\n• Implicit functions are functions that cannot be decomposed into\nelementary operations and for which autodiff can therefore not\nbe directly applied. Examples are optimization problems and\nnonlinear equations.\n• Envelope theorems can be used for differentiating through the\nmin or max value (not solution) of a function.\n• More generally, the implicit function theorem allows us to dif-\nferentiate through implicit functions. It gives conditions for the\nexistence of derivatives and how to obtain them.\n• The adjoint state method can be used to obtain the gradient of\nthe composition of an explicit function and of an implicit function,\nspecified by equality constraints. It can be used to prove the\ncorrectness of reverse-mode autodiff.\n• The inverse function theorem can be used to differentiate function\ninverses.\n• In a sense, the implicit function theorem can be thought as the\nmother theorem, as it can be used to prove envelope theorems,\nthe adjoint state method and the inverse function theorem.\n\n12\nDifferentiating through integration\nIn this chapter, we study how to differentiate through integrals, with a\nfocus on expectations and solutions of ordinary differential equations.\n12.1\nDifferentiation under the integral sign\nGiven two Euclidean spaces Θ and Y, and a function f : Θ × Y →R,\nwe often want to differentiate an integral of the form\nF(θ) :=\nZ\nY\nf(θ, y)dy.\nProvided that we can swap integration and differentiation, we have\n∇F(θ) =\nZ\nY\n∇θf(θ, y)dy.\nThe conditions enabling us to do so are best examined in the context\nof measure theory. We refer the reader to e.g. (Cohn, 2013) for a course\non measure theory and Flanders (1973) for an in-depth study of the\ndifferentiation under the integral sign. Briefly, if Θ = Y = R, the\nfollowing conditions are sufficient.\n1. f is measurable in both its arguments, and f(θ, ·) is integrable\nfor almost all θ ∈Θ fixed,\n258\n\n12.2. Differentiating through expectations\n259\n2. f(·, y) is absolutely continuous for almost all y ∈Y, that is,\nthere exists an integrable function g(·, y) such that f(θ, y) =\nf(θ0, y) +\nR θ\nθ0 g(τ, y)dτ,\n3. ∂1f(θ, y) (which exists almost everywhere if f(·, y) is absolutely\ncontinuous), is locally integrable, that is, for any closed interval\n[θ0, θ1], the integral\nR θ1\nθ0\nR |∂1f(θ, y)|dydθ is finite.\nAny differentiable function f : Θ × Y →R is absolutely continuous.\nHowever, the conditions also hold if f is just absolutely continuous, that\nis, if f(·, y) is differentiable for almost all y. This weaker assumption\ncan be used to smooth out differentiable almost-everywhere functions,\nsuch as the ReLu, as we study in Section 14.4.\n12.2\nDifferentiating through expectations\nA special case of differentiating through integrals is differentiating\nthrough expectations. We can distinguish between two cases, depending\non whether the parameters θ we wish to differentiate are involved in\nthe distribution or in the function, whose expectation we compute.\n12.2.1\nParameter-independent distributions\nWe first consider expectations of the form\nF(θ) := EY ∼p[g(Y, θ)] =\nZ\nY\ng(y, θ)p(y)dy,\nfor a random variable Y ∈Y ⊆RM, distributed according to a distri-\nbution p, and a function g: Y × Θ →R. Importantly, the distribution\nis independent of the parameters θ. Under mild conditions recalled\nin Section 12.1, we can swap differentiation and integration to obtain\n∇F(θ) = ∇θ\nZ\nY\ng(y, θ)p(y)dy\n=\nZ\nY\n∇θg(y, θ)p(y)dy\n= EY ∼p[∇θg(Y, θ)].\n\n260\nDifferentiating through integration\nGenerally, the expectation cannot be computed in closed form. However,\nprovided that we can sample from p, we can define a Monte-Carlo\nestimator of the value\nbFN(θ) := 1\nN\nN\nX\ni=1\ng(Yi, θ)\nand of the gradient\n∇bFN(θ) = 1\nN\nN\nX\ni=1\n∇θg(Yi, θ),\nfor N i.i.d. samples Y1, . . . , YN from p. These estimators are unbiased,\nmeaning that E[ bFN(θ)] = F(θ) and E[∇bFN(θ)] = ∇F(θ), and converge\nto the true quantity as N →+∞. This suggests a simple implementation\nin an autodiff framework of the approximation of ∇F(θ):\n1. Sample y1, . . . , yn from p.\n2. Compute bFN(θ) = 1\nn\nPn\ni=1 g(yi, θ).\n3. Compute the gradient ∇bFN(θ) by automatic differentiation.\nComputing higher order derivatives follow the same principle: to get\nan approximation of ∇2F(θ), we can simply compute ∇2 bFN(θ) by\nautodiff. As such, the implementation delineated above is akin to the\n“discretize-then-optimize” approach used to differentiate through the\nsolution of an ODE (Section 12.6): we implement an approximation of\nthe objective and simply call autodiff on it.\n12.2.2\nParameter-dependent distributions\nA more challenging case arises when the distribution depends on the\nparameters θ:\nE(θ) := EY ∼pθ[g(Y )] =\nZ\nY\ng(y)pθ(y)dy,\nwhere Y ∈Y ⊆RM is a random variable, distributed according to\na distribution pθ parameterized by θ ∈Θ and where g: Y →R is,\ndepending on the setting, potentially a blackbox function (i.e., we do not\n\n12.2. Differentiating through expectations\n261\nhave access to its gradients). Typically, θ ∈Θ could be parameters we\nwish to estimate, or it could indirectly be generated by θ = f(x, w) ∈Θ,\nwhere f is a neural network with parameters w ∈W we wish to estimate.\nThe main difficulty in computing ∇E(θ) stems from the fact that θ\nare the parameters of the distribution pθ. Estimating an expectation\nE(θ) = EY ∼pθ[g(Y )] using Monte-Carlo estimation requires us to sample\nfrom pθ. However, it is not clear how to differentiate E w.r.t. θ if θ is\ninvolved in the sampling process.\nContinuous case\nWhen Y is a continuous set (that is, pθ(y) is a probability density\nfunction), we can rewrite E(θ) as\nE(θ) =\nZ\nY\npθ(y)g(y)dy.\nProvided that we can swap integration and differentiation (see Sec-\ntion 12.1), we then have\n∇E(θ) = ∇θ\nZ\nY\npθ(y)g(y)dy\n=\nZ\nY\n∇θpθ(y)g(y)dy.\nUnfortunately, this integral is not an expectation and it could be in-\ntractable in general.\nDiscrete case\nWhen Y is a discrete set (that is, pθ(y) is a probability mass function),\nwe can rewrite E(θ) as\nE(θ) =\nX\ny∈Y\npθ(y)g(y).\nWe then obtain\n∇E(θ) =\nX\ny∈Y\ng(y)∇θpθ(y).\nAgain ∇E(θ) is not an expectation. We therefore cannot use Monte-\nCarlo estimation to estimate the gradient. Instead, we can compute it\n\n262\nDifferentiating through integration\nby brute force, i.e., by summing over all possible y ∈Y. However, this is\nclearly only computationally tractable if |Y| is small or if pθ is designed\nto have sparse support, i.e., so that the set {y ∈Y : pθ(y) ̸= 0} is\nsmall. Moreover, even if these conditions hold, summing over y could be\nproblematic if g(y) is expensive to compute. Therefore, exact gradients\nare seldom used in practice.\nIn Sections 12.3 and 12.4, we review the score function and pathwise\ngradient estimators, to (approximately) compute ∇E(θ), allowing us\nto optimize θ (or w using the chain rule) by gradient-based algorithms.\n12.2.3\nApplication to expected loss functions\nDifferentiating through expectations is particularly useful when working\nwith expected loss functions of the form\nL(θ; y) := E ˆY ∼pθ[ℓ( ˆY , y)],\nwhere y is some ground truth. Equivalently, we can set ℓ= −r, where\nr is a reward function. As we shall see, the score function estimator\nwill support a discrete loss function ℓ: Y × Y →R, while the pathwise\ngradient estimator will require a differentiable loss function ℓ: RM ×Y →\nR. Intuitively, L(θ; y) will be low if pθ assigns high probability to\npredictions by with low loss value ℓ(by, y).\nIn the classification setting, where Y = [M], pθ is often chosen to be\nthe Gibbs distribution, which is a categorical distribution induced\nby a softargmax\npθ(y) :=\nexp(θy)\nP\ni∈[M] exp(θi) = [softmax(θ)]y ∈(0, 1),\nwhere θy := f(x, y, w) ∈R are logits produced by a neural network f.\nMore generally, in the structured prediction setting, where Y ⊆RM but\n|Y| ≫M, we often use the distribution\npθ(y) :=\nexp(⟨ϕ(y), θ⟩)\nP\ny′∈Y exp(⟨ϕ(y′), θ⟩),\nwhere θ = f(x, w) ∈RM.\n\n12.2. Differentiating through expectations\n263\nGiven a distribution ρ over X × Y, we then want to minimize the\nexpected loss function, also known as risk,\nR(w) := E(X,Y )∼ρ[L(f(X, w); Y )].\nTypically, minimizing R(w) is done through some form of gradient\ndescent, which requires us to be able to compute\n∇R(w) = E(X,Y )∼ρ[∇wL(f(X, w); Y )]\n= E(X,Y )∼ρ[∂2f(x, w)∗∇L(f(X, w); Y )].\nComputing ∇R(w) therefore boils down to computing the gradient of\nL(θ; y), which is the gradient of an expectation.\n12.2.4\nApplication to experimental design\nIn experimental design, we wish to minimize a function g(λ), which we\nassume costly to evaluate. As an example, evaluating g(λ) could require\nus to run a scientific experiment with parameters λ ∈RQ. As another\nexample, in hyperparameter optimization, evaluating g(λ) would require\nus to run a learning algorithm with hyperparameters λ ∈RQ. Instead\nof solving the problem arg minλ∈RQ g(λ), we can lift the problem to\nprobability distributions and solve arg minθ∈RM E(θ), where E(θ) =\nEλ∼pθ[g(λ)]. This requires the probability distribution pθ to assign\nhigh probability to λ values that achieve small g(λ) value. Solving this\nproblem by stochastic gradient descent requires us to be able to compute\nestimates of ∇E(θ). This can be done for instance with SFE explained\nin Section 12.3, which does not require gradients of g, unlike implicit\ndifferentiation explained in Chapter 11. This approach also requires us\nto choose a distribution pθ over λ. For continuous hyperparameters,\na natural choice would be the normal distribution λ ∼Normal(µ, Σ),\nsetting θ = (µ, Σ). Once we obtained θ by minimizing E(θ), we need a\nway to recover λ. This can be done for example by choosing the mode of\nthe distribution, i.e., arg maxλ∈RQ pθ(λ), or the mean of the distribution\nEλ∼pθ(λ)[λ]. Of course, in the case of the normal distribution, they\ncoincide.\n\n264\nDifferentiating through integration\n12.3\nScore function estimators, REINFORCE\n12.3.1\nScalar-valued functions\nThe key idea of the score function estimator (SFE), also known as\nREINFORCE, is to rewrite ∇E(θ) as an expectation. The estimator is\nbased on the logarithmic derivative identity\n∇θ log pθ(y) = ∇θpθ(y)\npθ(y)\n⇐⇒∇θpθ(y) = pθ(y)∇θ log pθ(y).\nUsing this identity, we obtain the following gradient estimator.\nProposition 12.1 (SFE for scalar-valued functions). Given a family\nof distributions pθ on Y, for θ ∈Θ, define\nE(θ) := EY ∼pθ[g(Y )] =\nZ\nY\npθ(y)g(y)dy,\nwhere Y ∈Y ⊆RM and g: Y →R. Then,\n∇E(θ) = EY ∼pθ[g(Y )∇θ log pθ(Y )].\nProof.\n∇E(θ) =\nZ\nY\n∇θpθ(y)g(y)dy\n=\nZ\nY\npθ(y)g(y)∇θ log pθ(y)dy\n= EY ∼pθ[g(Y )∇θ log pθ(Y )].\nThe gradient of the log-PDF w.r.t. θ, ∇θ log pθ(y), is known as the\nscore function, hence the estimator name. SFE is suitable when two\nrequirements are met: it is easy to sample from pθ and the score function\nis available in closed form. Since the SFE gradient is an expectation,\nwe can use Monte-Carlo estimation to compute an unbiased estimator\nof ∇E(θ):\n∇E(θ) ≈bγN(θ) := 1\nN\nN\nX\ni=1\ng(Yi)∇θ log pθ(Yi),\n(12.1)\n\n12.3. Score function estimators, REINFORCE\n265\nwhere Y1, . . . , YN are sampled from pθ.\nInterestingly, the gradient of g is not needed in this estimator.\nTherefore, there is no differentiability assumption about g. This is why\nSFE is useful when g is a discrete loss function or more generally a\nblackbox function.\nExample 12.1 (SFE with a language model). In a language model,\nthe probability of a sentence y = (y1, . . . , yL) is typically factored\nusing the chain rule of probability (see Section 10.1)\npθ(y) := pθ(y1)pθ(y2|y1) . . . pθ(yL|y1, . . . , yL−1),\nwhere pθ is modeled using a transformer or RNN. Note that the\nprobabilities are normalized by construction, so there is no need for\nan explicit normalization constant. Thanks to this factorization, it is\neasy to sample from pθ using ancestral sampling (see Section 10.5.3)\nand the log-probability enjoys the simple expression\n∇θ log pθ(y) = ∇θ log pθ(y1) + ∇θ log pθ(y2|y1) + . . .\n+ ∇θ log pθ(yL|y1, . . . , yL−1).\nThis gradient is easy to compute, since the token-wise distributions\npθ(yj|y1, . . . , yj−1) are typically defined using a softargmax. We can\ntherefore easily compute ∇E(θ) under pθ using SFE. This is for\ninstance useful to optimize an expected reward, in order to finetune\nor align a language model (Ziegler et al., 2019).\nAnother example when ∇θpθ(y) is available in closed form is in the\ncontext of reinforcement learning, where pθ(y) is a Markov Decision\nProcess (MDP) and is called the policy. Applying the SFE leads to the\n(vanilla) policy gradient method (Sutton et al., 1999) and can then be\nused to compute the gradient of an expected cumulative reward. How-\never, SFE is more problematic when used with the Gibbs distribution,\ndue to the explicit normalization constant.\nExample 12.2 (SFE with a Gibbs distribution). The Gibbs distribu-\n\n266\nDifferentiating through integration\ntion is parameterized, for θ ∈RY,\npθ(y) := exp(θy/γ −A(θ)) = exp(θy/γ)/ exp(A(θ))\nwhere we defined the log-partition function\nA(θ) := log\nX\ny∈Y\nexp(θy/γ).\nA typical parametrization is θy = f(x, y, w) with f the output of\nnetwork on a sample x with parameters w. We then have\nlog pθ(y) = θy/γ −A(θ),\nso that\n∇θ log pθ(y) = ey/γ −∇A(θ).\nWe therefore see that ∇θ log pθ(y) crucially depends on ∇A(θ), the\ngradient of the log-partition. This gradient is available for some\nstructured sets Y, see e.g. (Mensch and Blondel, 2018), but not in\ngeneral.\nAs another example, we apply SFE in Section 14.4 to derive the\ngradient of perturbed functions.\nDifferentiating through both the distribution and the function\nSuppose both the distribution and the function now depend on θ. When\ng is scalar-valued and differentiable w.r.t. θ, we want to differentiate\nE(θ) := EY ∼pθ[g(Y, θ)].\nUsing the product rule, we obtain\n∇E(θ) = EY ∼pθ[g(Y, θ)∇θ log pθ(Y )] + EY ∼pθ[∇θg(Y, θ)].\nDifferentiating through joint distributions\nSuppose we now want to differentiate through\nE(θ) := EY1∼pθ,Y2∼qθ[g(Y1, Y2)].\n\n12.3. Score function estimators, REINFORCE\n267\nThe gradient is then given by\n∇E(θ) = EY1∼pθ,Y2∼qθ[(∇θ log pθ(Y1) + ∇log qθ(Y2))g(Y1, Y2)],\nwhich is easily seen by applying Proposition 12.1 on the joint distribution\nρθ := pθ·qθ. The extension to more than two variables is straightforward.\n12.3.2\nVariance reduction\nBias and variance\nRecall the definition of bγN in Eq. (12.1). SFE is an unbiased estimator,\nmeaning that\n∇E(θ) = E[bγN(θ)],\nwhere the expectation is taken with respect to the N samples drawn.\nSince the gradient is vector-valued, we need to define a scalar-valued\nnotion of variance. We do so by using the squared Euclidean distance\nin the usual variance definition to define\nV[bγN(θ)] := E[∥bγN(θ) −∇E(θ)∥2\n2]\n= E[∥bγN(θ)∥2\n2] −∥∇E(θ)∥2\n2.\nThe variance naturally goes to zero as N →∞.\nBaseline\nSFE is known to suffer from high variance (Mohamed et al., 2020).\nThis means that this estimator may require us to draw many samples\nfrom the distribution pθ to work well in practice. One of the simplest\nvariance reduction technique consists in shifting the function g with a\nconstant β, called a baseline, to obtain\n∇E(θ) = EY ∼pθ[(g(Y ) −β)∇θ log pθ(Y )].\nThe reason this is still a valid estimator of ∇E(θ) stems from\nEY ∼pθ[∇θ log pθ(Y )] = EY ∼pθ\n\u0014∇θpθ(Y )\npθ(Y )\n\u0015\n= ∇θEY ∼pθ[1]\n= ∇θ 1\n= 0,\n\n268\nDifferentiating through integration\nfor any valid distribution pθ. The baseline β is often set to the running\naverage of past values of the function g, though it is neither optimal\nnor does it guarantee to lower the variance (Mohamed et al., 2020).\nControl variates\nAnother general technique are control variates. Let us denote the\nexpectation of a function h: RM →R under the distribution pθ as\nH(θ) := EY ∼pθ[h(Y )].\nSuppose that H(θ) and its gradient ∇H(θ) are known in closed form.\nThen, for any γ ≥0, we clearly have\nE(θ) = EY ∼pθ[g(Y )]\n= EY ∼pθ[g(Y ) −γ(h(Y ) −H(θ))]\n= EY ∼pθ[g(Y ) −γh(Y )] + γH(θ)\nand therefore\n∇E(θ) = ∇θEY ∼pθ[g(Y ) −γh(Y )] + γ∇H(θ).\nApplying SFE, we then obtain\n∇E(θ) = EY ∼pθ[(g(Y ) −γh(Y ))∇θ log pθ(Y )] + γ∇H(θ).\nExamples of h include a bound on f or a second-order Taylor expansion\nof f, assuming that these approximations are easier to integrate than f\n(Mohamed et al., 2020).\n12.3.3\nVector-valued functions\nIt is straightforward to extend the SFE to vector-valued functions.\nProposition 12.2 (SFE for vector-valued functions). Given a family\nof distributions pθ on Y, for θ ∈Θ, define\nE(θ) := EY ∼pθ[g(Y )] =\nZ\nY\npθ(y)g(y)dy,\nwhere Y ∈Y, g: Y →G. The JVP of E at θ ∈Θ along v ∈Θ is\n∂E(θ)v = EY ∼pθ[⟨∇θ log pθ(Y ), v⟩g(Y )] ∈G\n\n12.3. Score function estimators, REINFORCE\n269\nand the VJP of E at θ ∈Θ along u ∈G is\n∂E(θ)∗u = EY ∼pθ[∇θ log pθ(Y )⟨u, g(Y )⟩] ∈Θ\nThe Jacobian of E at θ ∈Θ can then be written as\n∂E(θ) = EY ∼pθ[g(Y ) ⊗∇θ log pθ(Y )],\nwhere ⊗denote the outer product.\nProof. The VJP of E at θ ∈Θ along u ∈Θ amounts to compute the\ngradient of the scalar function\n⟨E(θ), u⟩= EY ∼pθ[⟨g(Y ), u⟩]\nThe expression of the VJP follows by using the SFE on the scalar valued\nintegrand ⟨g(Y ), u⟩. The JVP is obtained as the adjoint operator of the\nVJP and the Jacobian follows.\nDifferentiating through both the distribution and the function\nIf θ now influences both the distribution and the function,\nE(θ) := EY ∼pθ[g(Y, θ)],\nthen, we obtain\n∂E(θ) = EY ∼pθ[g(Y, θ) ⊗∇θ log pθ(Y )] + EY ∼pθ[∂θg(Y, θ)].\n12.3.4\nSecond derivatives\nUsing the previous subsection with g(y, θ) = g(y)∇θ log pθ(θ), we easily\nobtain an estimator of the Hessian.\nProposition 12.3 (SFE for the Hessian). Let us define the scalar-\nvalued function E(θ) := EY ∼pθ[g(Y )]. Then,\n∇2E(θ) =EY ∼pθ[g(Y )∇θ log pθ(Y ) ⊗∇θ log pθ(Y )]+\nEY ∼pθ[g(Y )∇2\nθ log pθ(Y )].\n\n270\nDifferentiating through integration\nThis can also be derived using the second-order log-derivative\n∇2\nθ log pθ(y) =\n1\npθ(y)∇2\nθpθ(y) −\n1\npθ(y)2 ∇θpθ(y) ⊗∇θpθ(y)\nso that\n∇2\nθpθ(y) = pθ(y)\nh\n∇2\nθ log pθ(y) + ∇θ log pθ(y) ⊗∇θ log pθ(y)\ni\n.\nLink with the Bartlett identities\nThe Bartlett identities are expressions relating the moments of the score\nfunction (gradient of the log-likelihood function). Using Proposition 12.1\nwith g(y) = 1 and\nR\nY pθ(y)dy = 1, we obtain\nEY ∼pθ[∇θ log pθ(Y )] = 0,\n(12.2)\nwhich is known as Bartlett’s first identity. Similarly, using Proposi-\ntion 12.3, we obtain\nEY ∼pθ[∇2\nθ log pθ(Y )] + EY ∼pθ[∇θ log pθ(Y ) ⊗∇θ log pθ(Y )]\n=EY ∼pθ[∇2\nθ log pθ(Y )] + cov[log pθ(Y )]\n=0,\n(12.3)\nwhich is known as Bartlett’s second identity.\n12.4\nPath gradient estimators, reparametrization trick\nAs we saw previously, the main difficulty in computing gradients of\nexpectations arises when the parameters θ play a role in the distribution\npθ being sampled. The key idea of path gradient estimators (PGE),\nalso known as reparametrization trick, is to rewrite the expectation in\nsuch a way that the parameters are moved from the distribution to the\nfunction, using a change of variable.\n12.4.1\nLocation-scale transforms\nThe canonical example of path gradient estimator is differentiating\nthrough the expectation\nE(µ, σ) := EU∼Normal(µ,σ2)[g(U)],\n\n12.4. Path gradient estimators, reparametrization trick\n271\nwhere g: R →R is a differentiable function. If we let Z ∼Normal(0, 1),\nit is easy to check that U = µ + σZ. We can therefore write\nE(µ, σ) = EZ∼Normal(0,1)[g(µ + σZ)].\nThe key advantage is that we can now easily compute the derivatives\nby mere application of the chain rule, since the parameters µ and σ are\nmoved from the distribution to the function:\n∂\n∂µE(µ, σ) = EZ∼Normal(0,1)[g′(µ + σZ)]\n∂\n∂σE(µ, σ) = σ · EZ∼Normal(0,1)[g′(µ + σZ)].\nThe change of variable\nU := µ + σZ\n(12.4)\nis called a location-scale transform. Such a transformation exists,\nnot only for the normal distribution, but for location-scale family\ndistributions, i.e., distributions parametrized by a location parameter µ\nand a scale parameter σ > 0, such that U is distributed according to a\ndistribution in the same family as Z is distributed. Besides the normal\ndistribution, examples of location-scale family distributions include the\nCauchy distribution the uniform distribution, the logistic distribution,\nthe Laplace distribution, and Student’s t-distribution.\nWe can easily relate the cumulative distribution function (CDF)\nand the probability density function (PDF) of Z to that of U, and\nvice-versa.\nProposition 12.4 (CDF and PDF of location-scale family distributions).\nLet FZ(z) := P(Z ≤z) and fZ(z) := F ′\nZ(z). If U := µ + σZ, then\nFZ(z) = FU(µ + σz) ⇐⇒FU(u) = FZ\n\u0012u −µ\nσ\n\u0013\nfZ(z) = σfU(µ + σz) ⇐⇒fU(u) = 1\nσfZ\n\u0012u −µ\nσ\n\u0013\n.\n\n272\nDifferentiating through integration\nProof. We have\nFZ(z) = P(Z ≤z)\n= P\n\u0012U −µ\nσ\n≤z\n\u0013\n= P(U ≤µ + σz)\n= FU(µ + σz)\nand we obtain fZ(z) by differentiating FZ(z).\n12.4.2\nDifferentiable transforms\nWe can generalize the idea of path gradient estimator (PGE) to any\nchange of variable\nU := T(Z, θ),\nwhere T : RM × RQ →RM is a differentiable transformation. For exam-\nple, if we gather µ and σ as θ := (µ, σ), we can write the location-scale\ntransform as\nU = T(Z, θ) = µ + σZ.\nWe can derive the path gradient estimator for any such differentiable\ntransformation T.\nProposition 12.5 (Path gradient estimator). Let us define\nE(θ) := EU∼pθ[g(U)],\nwhere U ∈U ⊆RM and g: RM →R is differentiable. Suppose\nthere is a differentiable transformation T : RM × RQ →RM such\nthat if Z ∼p (where p does not depend on θ) and U := T(Z, θ),\nthen U ∼pθ. Then, we have\nE(θ) = EZ∼p[h(Z, θ)] = EZ∼p[g(T(Z, θ))],\n\n12.4. Path gradient estimators, reparametrization trick\n273\nwhere h(z, θ) := g(T(z, θ)). This implies\n∇E(θ) = EZ∼p[∇2h(Z, θ)]\n= EZ∼p[∂2T(Z, θ)∗∇g(T(Z, θ))].\nThe path gradient estimator (a.k.a. reparametrization trick) gives an\nunbiased estimator of ∇E(θ). It has however two key disadvantages.\nFirst, it assumes that g is differentiable (almost everywhere), which\nmay not always be the case. Second, it assumes that g is well-defined\non RM, not on U, which could be problematic for some discrete loss\nfunctions, such as the zero-one loss function or ranking loss functions.\nAs an example of differentiable transform, in machine learning, we\ncan sample Gaussian noise Z and make it go through a neural network\nwith parameters w to generate an image X := T(Z, w). In statistics,\nmany distributions are related to each other through differentiable\ntransforms, as we recall below.\nExample 12.3 (Some differentiable transforms in statistics). We give\nbelow a non-exhaustive list of differentiable transform examples.\n• If X ∼Normal(µ, σ2), then exp(X) ∼Lognormal(µ, σ2).\n• If U ∼Uniform(0, 1), then −log(U)/λ ∼Exponential(λ).\n• If X1, . . . , XN ∼Exponential(λ) (i.i.d.), then PN\ni=1 Xi ∼\nGamma(N, λ).\n• If Xi ∼Gamma(αi, θ) for i ∈[K], then\n\u0012\nX1\nPK\ni=1 Xi , . . . ,\nXK\nPK\ni=1 Xi\n\u0013\n∼\nDirichlet(α1, . . . , αK).\n12.4.3\nInverse transforms\nThe inverse transform method can be used for sampling from a proba-\nbility distribution, given access to its associated quantile function.\nRecall that the cumulative distribution function (CDF) associated with\na random variable Y is the function FY : R →[0, 1] defined by\nFY (y) := P(Y ≤y).\n\n274\nDifferentiating through integration\nThe quantile function is then a function QY : [0, 1] →R such that\nQY (π) = y for π = FY (y). Assuming FY is continuous and strictly\nincreasing, we have that QY is the inverse CDF,\nQY (π) = F −1\nY (π).\nIn the general case of CDF functions that are not strictly increasing,\nthe quantile function is usually defined as\nQY (π) := inf{y ∈R: π ≤FY (y)}.\nGiven access to the quantile function QY (π) associated with a distribu-\ntion p, inverse transform sampling allows us to sample from p by first\ndrawing a sample from the uniform distribution and then making\nthis sample go through the quantile function.\nProposition 12.6 (Inverse transform sampling). Suppose Y ∼p,\nwhere p is a distribution with quantile function QY . If U ∼\nUniform(0, 1), then QY (U) ∼p.\nProof. If π ≤FY (t), then by definition of QY , QY (π) ≤t. If π ≥FY (t),\nthen by definition of QY , FY (QY (π)) ≥π, so FY (QY (π)) ≥FY (t) and\nsince a CDF is always non-decreasing, QY (π) ≥t. Hence, we have,\nQY (π) ≤t ⇐⇒π ≤FY (t), so\nP(QY (U) ≤t) = P(U ≤FY (t))\n= FY (t).\nThe CDFs of QY (U) and Y coincide, hence they have the same distri-\nbution.\nIf the quantile function is differentiable, we can therefore use it\nas a transformation within the reparametrization trick. Indeed,\nif Y ∼pθ, where pθ is a distribution with parameter θ and quantile\nfunction QY (π, θ), then we have\nE(θ) = EY ∼pθ[g(Y )] = Eπ∼Uniform(0,1)[g(QY (π, θ))]\nand therefore, by the reparametrization trick (Proposition 12.5),\n∇E(θ) = Eπ∼Uniform(0,1)[∂2QY (π, θ)∗∇g(QY (π, θ))].\n\n12.4. Path gradient estimators, reparametrization trick\n275\nExample 12.4 (Examples of quantile functions). If\nY ∼Exponential(λ), the CDF of Y is π = FY (y) = 1−exp(−λy) for\ny ≥0 and therefore the quantile function is QY (π, λ) = −log(1−π)\nλ\n.\nIf Y ∼Normal(µ, σ2), the CDF is FY (y) = 1\n2\nh\n1 + erf\n\u0010\ny−µ\nσ\n√\n2\n\u0011i\nand\nthe quantile function is QY (π, θ) = µ + σ\n√\n2 · erf−1(2π −1), where\nθ = (µ, σ). This therefore defines an alternative transformation to\nthe location-scale transformation in Eq. (12.4).\nNote that, in the above example, the error function erf and its\ninverse do not enjoy analytical expressions but autodiff packages usually\nprovide numerical routines to compute them and differentiate through\nthem. Nonetheless, one caveat of the inverse transform is that it indeed\nrequires access to (approximations of) the quantile function and its\nderivatives, which may be difficult for complicated distributions.\n12.4.4\nPushforward operators\nPushforward distributions\nWe saw so far that the reparametrization trick is based on using a\nchange of variables in order to differentiate an expectation w.r.t. the\nparameters of the distribution. In this section, we further formalize that\napproach using pushforward distributions.\nDefinition 12.1 (Pushforward distribution). Suppose Z ∼p, where\np is a distribution over Z. Given a continuous map T : Z →U,\nthe pushforward distribution of p through T is the distribution q\naccording to which U := T(Z) ∈U is distributed, i.e., U ∼q.\nAlthough not explicit in the above, the transformation T can depend\non some learnable parameters, for example if T is a neural network.\nIntuitively, the pushforward distribution is obtained by moving the\nposition of all the points in the support of p. Inverse transform sampling\nstudied in Section 12.4.3 can be seen as performing the pushforward\nof the uniform distribution through T = Q, where Q is the quantile\nfunction. The Gumbel trick studied in Section 14.5 can be seen as a the\npushforward of Gumbel noise through T = argmax (a discontinuous\n\n276\nDifferentiating through integration\nfunction) and Gumbel noise can itself be obtained by pushing forward\nthe uniform distribution through T = −log(−log(·)) (Remark 14.2). In\na generative modeling setting, as we mentioned previously, we use the\npushforward of Gaussian noise through a parametrized transformation\nX = T(Z, w) called a generator, typically a neural network.\nA crucial aspect of the pushforward distribution q is that it can be\nimplicitly defined, meaning that we do not necessarily need to know\nthe explicit form of the associated PDF. In fact, it is easy to to sample\nfrom q, provided that it is easy to sample from p:\nU ∼q ⇐⇒Z ∼p, U := T(Z).\nHence the usefulness of the pushforward distribution in generative\nmodeling. Furthermore, if p has associated PDF pZ, we can compute\nthe expectation of a function f according to q as\nEU∼q[f(U)] = EZ∼p[f(T(Z))] =\nZ\nZ\nf(T(z))pZ(z)dz,\neven though we do not know the explicit form of the PDF of q.\nPushforward measures\nMore generally, we can define the notion of pushforward, in the language\nof measures. Denote M(Z) the set of measures on a set Z. A measure\nα ∈M(Z), that has a density dα(z) := pZ(z)dz, can be integrated\nagainst a funtcion f as\nZ\nZ\nf(z)dα(z) =\nZ\nZ\nf(z)pZ(z)dz.\nA measure α is called a probability measure if it is positive and satisfies\nα(Z) =\nR\nZ dα(z) =\nR\nZ pZ(z)dz = 1. See Peyré and Cuturi (2019,\nChapter 2) for a concise introduction.\nDefinition 12.2 (Pushforward operator and measure). Given a con-\ntinuous map T : Z →U and some measure α ∈M(Z), the push-\nforward measure β = T♯α ∈M(U) is such that for all continuous\n\n12.4. Path gradient estimators, reparametrization trick\n277\nfunctions f ∈C(U)\nZ\nU\nf(u)dβ(u) =\nZ\nZ\nf(T(z))dα(z).\nEquivalently, for any measurable set A ⊂U, we have\nβ(A) = α({z ∈Z : T(z) ∈A}) = α(T −1(A)),\nwhere T −1(A) = {z ∈Z : T(z) ∈A}.\nImportantly, the pushforward operator preserves positivity and mass,\ntherefore if α is a probability measure, then so is T♯α. The pushforward\nof a probability measure therefore defines a pushforward distribution\n(since a distribution can be parametrized by a probability measure).\n12.4.5\nChange-of-variables theorem\nWe saw that a pushforward distribution associated with a variable U\nis implicitly defined through a transform U := T(Z) and can be easily\nsampled from as long as it is easy to sample Z. However, in some\napplications (e.g., density estimation), we may want to know the PDF\nassociated with U. Assuming the transform T is invertible, we have\nZ = T −1(U) and therefore for A ⊆U, we have\nP(U ∈A) = P(Z ∈T −1(A)) =\nZ\nT −1(A)\npZ(z)dz.\nUsing the change-of-variables theorem from multivariate calculus,\nassuming T −1 is available, we can give an explicit formula for the PDF\nof the pushforward distribution, see e.g. (Schwartz, 1954; Taylor, 2002).\nProposition 12.7 (PDF of the pushforward distribution). Suppose\nZ ∼p, where p is a distribution over Z, with PDF pZ. Given\na diffeomorphism T : Z →U (i.e., an invertible and differen-\ntiable map), the pushforward distribution of p through T is the\ndistribution q such that U := T(Z) ∼q and its PDF is\nqU(u) = | det(∂T −1(u))|pZ(T −1(u)),\n\n278\nDifferentiating through integration\nwhere ∂T −1(u) is the Jacobian of T −1 : U →Z.\nUsing this formula, we obtain\nP(U ∈A) =\nZ\nA\npU(u)du\n=\nZ\nA\n| det(∂T −1(u))|pZ(T −1(u))du.\nUsing the inverse function theorem (Theorem 11.5), we then have\n∂T −1(u) = (∂T(T −1(u)))−1,\nunder the assumption that T(z) is continuously differentiable and\nhas invertible Jacobian ∂T(z). Normalizing flows are parametrized\ntransformations T designed such that T −1 and its Jacobian ∂T −1 are\neasy to compute; see e.g. Kobyzev et al. (2019) and Papamakarios et al.\n(2021) for a review.\n12.5\nStochastic programs\nA stochastic program is a program that involves some form of random-\nness. In a stochastic program, the final output, as well as intermediate\nvariables, may therefore be random variables. In other words, a stochas-\ntic program induces a probability distribution over program outputs,\nas well as over execution trajectories.\n12.5.1\nStochastic computation graphs\nA stochastic program can be represented by a stochastic computation\ngraph as originally introduced by Schulman et al. (2015). Departing from\nthat work, our exposition explicitly supports two types of intermediate\noperations: sampling from a conditional distribution or evaluating a\nfunction. These operations can produce either deterministic variables\nor random variables.\nFunction and distribution nodes\nFormally, we define a stochastic computation graph as a directed acyclic\ngraph G = (V, E), where V = Vf ∪Vp, Vf is the set of function nodes\n\n12.5. Stochastic programs\n279\nand Vp is the set of distribution nodes. Similarly to computation graphs\nreviewed in Section 4.1.3, we number the nodes as V = {0, 1, . . . , K}.\nNode 0 corresponds to the input s0 ∈S0, which we assume to be deter-\nministic. It is the variable with respect to which we wish to differentiate.\nNode K corresponds to the program output SK ∈SK, which we assume\nto be a random variable. A node k ∈{1, . . . , K} can either be a func-\ntion node k ∈Vf with an associated function fk or a distribution\nnode k ∈Vp, with associated conditional distribution pk. A stochastic\nprogram has at least one distribution node, the source of randomness.\nOtherwise, it is a deterministic program. As for computation graphs,\nthe set of edges E is used to represent dependencies between nodes. We\ndenotes the parents of node k by pa(k).\nDeterministic and random variables\nWe distinguish between two types of intermediate variables: deter-\nministic variables sk and random variables Sk. Therefore, a dis-\ntribution pk or a function fk may receive both types of variables\nas conditioning or input. It is then convenient to split pa(k) as\npa(k) = determ(k) ∪random(k), where we defined the determinis-\ntic parents determ(k) := {i1, . . . , ipk} and the random parents\nrandom(k) := {j1, . . . , jqk}. Therefore, si1, . . . , sipk are the determinis-\ntic parent variables and Sj1, . . . , Sjqk are the random parent variables,\nof node k.\nExecuting a stochastic program\nWe assume that nodes 0, 1, . . . , K are in topological order (if this is\nnot the case, we need to perform a topological sort). Given parent\nvariables si1, . . . , sipk and Sj1, . . . , Sjqk, a node k ∈{1, . . . , K} produces\nan output as follows.\n• If k ∈Vp (distribution node), the output is\nSk ∼pk(· | sdeterm(k), Srandom(k))\n⇐⇒Sk ∼pk(· | si1, . . . , sipk, Sj1, . . . , Sjqk)\n\n280\nDifferentiating through integration\nNote that technically pk is the distribution of Sk conditioned on its\nparents, not the distribution of Sk. Therefore, we should in princi-\nple write Sk | sdeterm(k), Srandom(k) ∼pk(· | sdeterm(k), Srandom(k)).\nWe avoid this notation for conciseness and for symmetry with\nfunction nodes.\nContrary to a function node, a distribution node can have no\nparents. That is, if k ∈Vp, it is possible that pa(k) = ∅. A good\nexample would be a parameter-free noise distribution.\n• If k ∈Vf (function node), the output is in general\nSk := fk(sdeterm(k), Srandom(k))\n:= fk(si1, . . . , sipk, Sj1, . . . , Sjqk)\nand in the special case qk = |random(k)| = 0, the output is\nsk := fk(sdeterm(k))\n:= fk(si1, . . . , sipk).\nUnless the associated conditional distribution pk is a delta distribution,\nthat puts all the probability mass on a single point, the output of a\ndistribution node k ∈Vp is necessarily a random variable Sk ∈Sk.\nFor function nodes k ∈Vf, the output of the function fk is a random\nvariable Sk ∈Sk if at least one of the parents of k produces a random\nvariable. Otherwise, if all parents of k produce deterministic variables,\nthe output of fk is a deterministic variable sk ∈Sk.\nThe entire procedure is summarized in Algorithm 12.1. We emphasize\nthat SK = f(s0) ∈SK is a random variable. Therefore, a stochastic\nprogram (implicitly) induces a distribution over SK, and also over\nintermediate random variables Sk. Executing the stochastic program\nallows us to draw samples from that distribution.\nSpecial cases\nIf all nodes are function nodes, we recover computation graphs, reviewed\nin Section 4.1.3. If all nodes are distribution nodes, we recover Bayesian\nnetworks, reviewed in Section 10.5.\n\n12.5. Stochastic programs\n281\nAlgorithm 12.1 Executing a stochastic program\nNodes: 1, . . . , K in topological order, where node k is either a\nfunction fk or a conditional distribution pk\nInput: input s0 ∈S0\n1: for k := 1, . . . , K do\n2:\nRetrieve pa(k) = determ(k) ∪random(k)\n3:\nif k ∈Vp then\n▷Distribution node\n4:\nSk ∼pk(·|sdeterm(k), Srandom(k))\n5:\nelse if k ∈Vf then\n▷Function node\n6:\nif |random(k)| ̸= 0 then\n7:\nSk := fk(sdeterm(k), Srandom(k))\n▷Output is a R.V.\n8:\nelse if |random(k)| = 0 then\n9:\nsk := fk(sdeterm(k))\n▷Output is deterministic\n10: Output: f(s0) := SK ∈SK\n12.5.2\nExamples\nWe now present several examples that illustrate our formalism. We use\nthe legend below in the following illustrations.\nFunction\nSampler\nDeterministic\n variable\nStochastic\n variable\n• Example 1 (SFE estimator):\nS1 ∼p1(· | s0)\nS2 := f2(S1)\nE(s0) := E[S2]\n∇E(s0) = ES1[f2(S1)∇s0 log p1(S1 | s0)]\n• Example 2 (Pathwise estimator):\n\n282\nDifferentiating through integration\nS1 ∼p1\nS2 := f2(S1, s0)\nE(s0) := E[S2]\n∇E(s0) = ES1 [∇s0f2(S1, s0)]\n• Example 3 (SFE estimator + chain rule):\ns1 := f1(s0)\nS2 ∼p2(· | s1)\nS3 := f3(S2)\nE(s0) := E[S3]\n∇E(s0) = ∂f(s0)∗ES2[f3(S2)∇s1 log p2(S2 | s1)]\n• Example 4:\n\n12.5. Stochastic programs\n283\ns1 := f1(s0)\ns2 := f2(s0)\nS3 ∼p3(· | s1)\nS4 ∼p4(· | s2, S3)\nS5 := f5(S4)\nE(s0) := E[S5] = ES3 [ES4[f5(S4)]]\n∇E(s0) = ES3 [∂f1(s0)∗∇s1 log p(S3 | s1)ES4[f5(S4)]]\n+ ES3 [ES4 [∂f2(s0)∗∇s2 log p4(S4|s2, S3)f5(S4)]]\nAs can be seen, the gradient expressions can quickly become quite\ncomplicated, demonstrating the merits of automatic differentiation in\nstochastic computation graphs.\n12.5.3\nUnbiased gradient estimators\nThe output of a stochastic program is a random variable\nSK := f(s0).\nIt implicitly defines a probability distribution p(·|s0) such that SK ∼\np(·|s0). Executing the stochastic program once gives us an i.i.d. sample\nfrom p(·|s0).\nSince derivatives are defined for deterministic variables, we need a\nway to convert a random variable to a deterministic variable. One way\nto do so is to consider the expected value (another way would be the\nmode)\nE(s0) := E[SK] = E[f(s0)] ∈conv(SK),\nwhere the expectation is over SK ∼p(·|s0) or equivalently over the\nintermediate random variables Sk\nSk ∼pk(·|sdeterm(k), Srandom(k)),\nfor k ∈Vp (the distribution nodes). We then wish to compute the\ngradient or more generally the Jacobian of E(s0).\n\n284\nDifferentiating through integration\nIf all nodes in the stochastic computation graph are function nodes,\nwe can estimate the gradient of E(s0) using the pathwise estimator\na.k.a. reparametrization trick (Section 12.4). This is the approach taken\nby Kingma and Welling (2013) and Rezende et al. (2014).\nIf all nodes in the stochastic computation graph are distribution\nnodes, we can use the SFE estimator (Section 12.3). Schulman et al.\n(2015) propose a surrogate loss so that using autodiff on that loss\nproduces an unbiased gradient of the expectation, using the SFE esti-\nmator. Foerster et al. (2018) extend the approach to support high-order\ndifferentiation. Krieken et al. (2021) further extend the approach by\nsupporting different estimators per node, as well as control variates.\nConverting distribution nodes into function nodes and vice-versa\nOur formalism uses two types of nodes: distribution nodes with asso-\nciated conditional distribution pk and function nodes with associated\nfunction fk. It is often possible to convert between node types.\nConverting a distribution node into a function node is exactly the\nreparametrization trick studied in Section 12.4. We can use transforma-\ntions such as the location-scale transform or the inverse transform.\nConverting a function node into a distribution node can be done\nusing the change-of-variables theorem, studied in Section 12.4.5, on a\npushforward distribution.\nBecause the pathwise estimator has lower variance than SFE, this is\nthe method of choice when the fk functions are available. The conversion\nfrom distribution node to function node and vice-versa is illustrated in\nFig. 12.1.\n12.5.4\nLocal vs. global expectations\nA stochastic computation graph can be seen as a stochastic process,\na collection of random variables Sk, indexed by k, the position in the\ntopological order. However, random variables are incompatible with\nautodiff. Replacing random variables by their expectation can be seen\nas a way to make them compatible with autodiff. Two strategies are\nthen possible.\n\n12.5. Stochastic programs\n285\nTransformation \n(location-scale transform, inverse transform)\nChange-of-variables theorem\nParametric (explicit) distribution\nScore Function Estimator\n(SFE)\nPushforward (implicit) distribution\nPath Gradient Estimator \n(PGE)\nFigure 12.1: It is sometimes possible to convert a distribution node to a function\nnode and vice-versa using a suitable transformation.\nAs we saw in the previous section, a strategy is to consider the\nexpectation of the last output SK. This strategy corresponds to a\nglobal smoothing. The two major advantages are that i) we do not\nneed to assume that fk+1 is well-defined on conv(Sk) and ii) this induces\na probability distribution over program executions. This is for instance\nuseful to compute the variance of the program. The gradient of the\nprogram’s expected value can be estimated by the reparametrization\ntrick or by the SFE, depending on the type of nodes used.\nA second strategy is to replace an intermediate random variable\nSk ∈Sk, for k ∈{1, . . . , K}, by its expectation E[Sk] ∈conv(Sk). This\nstrategy corresponds to a local smoothing. A potential drawback of\nthis approach is that E[Sk] belongs to conv(Sk), the convex hull of Sk.\nTherefore, the function fk+1 in which E[Sk] is fed must be well-defined\non conv(Sk), which may not always be the case. In the case of control\nflows, another disadvantage is computational. We saw in Section 5.6 and\nSection 5.7 that using a soft comparison operator within a conditional\nstatement induces a distribution on a binary or categorical random\nvariable, corresponding to the branch to be selected. A conditional\nstatement can then be locally smoothed out by replacing the random\n\n286\nDifferentiating through integration\nvariable by its expectation i.e., a convex combination of all the\nbranches. This means that, unless the distribution has sparse support,\nall branches must be evaluated.\n12.6\nDifferential equations\n12.6.1\nParameterized differential equations\nFrom residual networks to neural ODEs\nStarting from s0 := x, residual networks, reviewed in Section 4.5, iterate\nfor k ∈{1, . . . , K}\nsk := sk−1 + hk(sk−1, wk).\nA residual network can be seen as parameterizing incremental discrete-\ntime input changes (hence the name “residual”)\nsk −sk−1 = hk(sk, wk).\nChen et al. (2018) proposed to parameterize continuous-time (instan-\ntaneous) changes instead. They considered the evolution s(t) of the\ninputs in continuous time driven by a function h(t, s, w) parameterized\nby w, starting from x. Formally, the evolution s(t) is the solution of\nthe ordinary differential equation (ODE)\ns(0) = x\ns′(t) = h(t, s(t), w)\nt ∈[0, T]\n(12.5)\nHere, s′(t) is the vector of derivatives of s as defined in Remark 2.4, and\nT denotes a final time for the trajectory. The output of such a neural\nODE (Chen et al., 2018) is then f(x, w) := s(T). Alternatively, the\noutput can be seen as the solution of an integration problem\nf(x, w) = s(T) = x +\nZ T\n0\nh(t, s(t), w)dt.\n(12.6)\nDifferential equations like Eq. (12.5) arise in many contexts beyond neu-\nral ODEs, ranging from modeling physical systems to pandemics (Braun\nand Golubitsky, 1983). Moreover, the differential equation presented\nin Eq. (12.5) is just an example of an ordinary differential equation,\nwhile controlled differential equations or stochastic differential equations\ncan also be considered.\n\n12.6. Differential equations\n287\nExistence of a solution\nFirst and foremost, the question is whether s(t) is well-defined. For-\ntunately, the answer is positive under mild conditions, as shown by\nPicard-Lindelöf’s theorem recalled below (Butcher, 2016, Theorem 16).\nTheorem 12.1 (Exsistence and uniqueness of ODE solutions). If h :\n[0, T] × S →S is continuous in its first variable and Lipschitz-\ncontinuous in its second variable, then there exists a unique differ-\nentiable map s : [0, T] →S satisfying\ns(0) = s0\ns′(t) = h(t, s(t))\nt ∈[0, T],\nfor some given s0 ∈S.\nFor time-independent linear functions h(t, s) = As, the integral\nin Eq. (12.6) can be computed in closed form as\nst = exp(tA)(s0),\nwhere exp(A) is the matrix exponential. Hence, the output s(T) can\nbe expressed as a simple function of the parameters (A in this case).\nHowever, generally, we do not have access to such analytical solutions,\nand, just as for solving optimization problems in Chapter 11, we need\nto resort to some iterative algorithms.\nIntegration methods\nTo numerically solve an ODE, we can use integration methods,\nwhose goal is to build a sequence sk that approximates the solution\ns(t) at times tk. The simplest integration method is the explicit Euler\nmethod, that approximates the solutions between times tk−1 and tk as\ns(tk−1) −s(tk) =\nZ tk\ntk−1\nh(t, s(t), w)dt\n≈δkh(tk−1, s(tk−1), w),\nfor a time-step\nδk := tk −tk−1.\n\n288\nDifferentiating through integration\nThe resulting integration scheme consists in computing starting from\ns0 = x, for k ∈{1, . . . , K},\nsk := sk−1 + δkh(tk−1, sk−1, w).\nAssimilating δkh(tk−1, sk−1, w) with hk(sk−1, wk), we find that residual\nnetworks are essentially the discretization of a neural ODE by an\nexplicit Euler method; more precisely, a non-autonomous neural ODEs,\nsee e.g. (Davis et al., 2020).\nEuler’s forward method is only one integration method among\nmany. To cite a few, there are implicit Euler methods, semi-implicit\nmethods, Runge-Kutta methods, linear multistep methods, etc. See,\ne.g., Gautschi (2011) for a detailed review. The quality of an integration\nmethod is measured by its consistency and its stability (Gautschi, 2011).\nThese concepts naturally influence the development of evaluation and\ndifferentiation techniques for ODEs. We briefly summarize them below.\nGiven a fixed time interval δk = δ and K = ⌈T/δ⌉points, an\nintegration method is consistent of order k if ∥sk −s(kδ)∥= O(δk)\nas δ →0 and therefore k →+∞. The higher the order k, the fewer points\nwe need to reach an approximation error ε on the points considered.\nThe term ∥sk −s(kδ)∥= O(δk) is reminiscent of the error encountered\nin finite differences (Chapter 7) and is called the truncation error.\nThe (absolute) stability of a method is defined by the set of time-\nsteps such that the integration method can integrate s′(t) = λs(t) for\nsome λ ∈C without blowing up as t →+∞.\n12.6.2\nContinuous adjoint method\nSince different parameters w induce different trajectories associated to\nh(t, s, w) in Eq. (12.5), we may want to select one of these trajectories\nby minimizing some criterion. For example, we may consider selecting\nw ∈W by minimizing a loss L on the final point of the trajectory,\nmin\nw∈W L(f(x, w), y),\n(12.7)\nwhere\nf(x, w) := s(T) = x +\nZ T\n0\nh(t, s(t), w)dt.\n\n12.6. Differential equations\n289\nTo solve such problems, we need to access gradients of ℓcomposed with\nf through VJPs of the solution of the ODE. The VJPs can actually\nbe characterized as solutions of an ODE themselves thanks to the\ncontinuous time adjoint method (Pontryagin, 1985), presented\nbelow, and whose proof is postponed to Section 12.6.6.\nProposition 12.8 (Continuous-time adjoint method). Consider a func-\ntion h : [0, T]×S×W →S, continuous in its first variable, Lipschitz-\ncontinuous and continuously differentiable in its second variable.\nAssume that ∂3h(t, s, w) exists for any t, s, w, and is also continu-\nous in its first variable, Lipschitz-continuous in its second variable.\nDenote s : S →S the solution of the ODE\ns(0) = x\ns′(t) = h(t, s(t), w)\nt ∈[0, T],\nand f(x, w) = s(T) the final state of the ODE at time T.\nThen, the function f is differentiable, and for an output direction\nu ∈S, its VJP along u is given by\n∂f(x, w)∗u = (r(0), g)\nfor\ng =\nZ T\n0\n∂3h(t, s(t), w)∗r(t)dt\nand for r solving the adjoint (backward) ODE\nr′(t) = −∂2h(t, s(t), w)∗r(t)\nr(T) = u.\nIn particular, the gradient ∇(L ◦f)(x, w) for L : S →R a\ndifferentiable loss is obtained by solving the adjoint ODE with\nr(T) = ∇L(s(T)).\nExample 12.5 (Fitting data through the solution of an ODE). As an\nillustrative example, we can consider optimizing the parameters of\nan ODE to fit some data points. Namely, we may seek a continuous\n\n290\nDifferentiating through integration\n0.5\n1.0\n1.5\n0.0\n0.5\n1.0\ns(t; w1)\ns(t; w2)\ns(t; w * )\nFigure 12.2: Finding the optimal parameters of an ODE to fit some observed data.\nThe dots represent the trajectories of a dynamical system observed at regular times\n(time is represented here by a gradient color, the lighter the color, the larger the\ntime). Each line represents the solution of an ODE given by some hyperparameters\nw. The objective is to find the hyperparameters of the ODE such that its solution\nfits the data points. Green and orange lines fail to do so while the blue line fits\nthe data. To compute such parameters w, we need to backpropagate through the\nsolution of the ODE.\ntime solution z(t; w) of a modified Lotka Volterra ODE\nz′(t; w) =\n \nαz1(t; w) −βz1(t; w)z2(t; w)\n−γz2(t; w) + δz1(t; w)z2(t; w)\n!\n+ c,\nfor w = (α, β, γ, δ, c), that fits some observations z1, . . . , zT . The\noptimization problem consists then of\nmin\nw\nT\nX\nτ=1\n(z(tj; w) −zj)2,\nand requires backpropagating through the solution z(·; w) of the\nODE w.r.t. to its candidate parameters w. Fig. 12.2 illustrates such\na problem with varying candidate parameters\n12.6.3\nGradients via the continuous adjoint method\nProposition 12.8 gives a formal definition of the gradient. However, just\nas computing the mapping f(x, w) itself, computing its VJP or the\n\n12.6. Differential equations\n291\ngradient of L ◦f requires solving an integration problem. Note that\nthe integration of r(t) in Proposition 12.8 requires also values of s(t).\nTherefore, we need to integrate both r(t) and s(t). Such an approach\nis generally referred as optimize-then-discretize because we first\nformulate the gradient in continuous time (the “optimize part”) and\nthen discretize the resulting ODE.\nSimple discretization scheme\nA first approach consists in defining a backward discretization scheme\nthat can approximate s(t) backward in time. Namely, by defining\nσ(t) = s(T −t), ρ(t) = r(T −t), and γ(t) =\nR T\nt ∂3h(τ, s(τ), w)∗r(τ)dτ,\nthe derivative of L ◦f is given by (ρ(T), γ(T)). The functions σ, ρ, γ\nare solutions of a standard ODE\nσ(0) = s(T),\nσ′(t) = −h(T −t, σ(t), w),\nρ(0) = ∇L(s(T)), ρ′(t) = ∂2h(T −t, σ(t), w)∗ρ(t),\nγ(0) = 0,\nγ′(t) = ∂3h(T −t, σ(t), w)∗ρ(t).\nThe above ODE can then be solved by any integration method. Note,\nhowever, that it requires first computing s(T) and ∇L(s(T)) by an\nintegration method. The overall computation of the gradient using\nan explicit Euler method to solve forward and backward ODEs is\nsummarized in Algorithm 12.2.\nAlgorithm 12.2 naturally looks like the reverse mode of autodiff for\na residual neural networks with shared weights. A striking difference\nis that the intermediate computations sk are not kept in memory and,\ninstead, new variables ˆsk are computed along the backward ODE. One\nmay believe that by switching to continuous time, we solved the memory\nissues encountered in reverse-mode autodiff. Unfortunately, this comes\nat the cost of numerical stability. As we use a discretization scheme\nto recompute the intermediate states backward in time through ˆsk in\nAlgorithm 12.2, we accumulate some truncation errors.\nTo understand the issue here, consider applying Algorithm 12.2\nrepeatedly on the same parameters but using ˆs0 instead of s0 = x each\ntime. In the continuous realm, σ(T) = s(0). But after discretization,\nˆs0 ≈σ(T) does not match s0. Therefore, by applying Algorithm 12.2\n\n292\nDifferentiating through integration\nAlgorithm 12.2 Gradient computation via continuous adjoint method\nwith Euler explicit discretization\n1: Functions: h : [0, T] × S × W →R, L : S →R\n2: Inputs: input x, parameters w, number of discretization steps K.\n3: Set discretization step δ = T/K, denote hk(s, w) = h(kδ, s, w).\n4: Set s0 := x\n5: for k := 1, . . . , K do\n▷Forward discretization\n6:\nCompute sk := sk−1 + δhk−1(sk−1, w).\n7: Compute u := ∇L(sK).\n8: Initialize rK := u, ˆsK = sK, gK = 0\n9: for k := K, . . . , 1 do\n▷Backward discretization\n10:\nCompute ˆsk−1 := ˆsk −δhk(ˆsk, w)\n11:\nCompute rk−1 := rk + δ∂2hk(ˆsk, w)∗rk\n12:\nCompute gk−1 := gk + δ∂3hk(ˆsk, w)∗rk\n13: Output: (r0, g0) ≈∇(L ◦f)(x, w)\nwith s0 = ˆs0, we would not get the same output even if in continu-\nous time we naturally should have. This phenomenon is illustrated in\nFig. 12.3. It intuitively shows why Algorithm 12.2 induces some noise\nin the estimation of the gradient.\nMultiple shooting scheme\nAn alternative approach consists in integrating both the forward and\nbackward ODEs jointly. Namely, we may solve an ODE with boundary\nvalues\ns′(t) = h(t, s(t), w),\ns(0) = x,\nr′(t) = −∂2h(t, s(t), w)∗r(t),\nr(T) = ∇L(s(T))\ng′(t) = −∂3h(t, s(t), w)∗r(t),\ng(T) = 0,\nby means of a multiple shooting method or a collocation method (Stoer\net al., 1980). This approach still requires ∇L(s(T)) to be approximated\nfirst.\n\n12.6. Differential equations\n293\n...\nForward approx. \nof the ODE\nForward \ndiscretization. \nerror\nSolution \nof ODE\nTotal forward\ndiscretization \nerror\n...\nLocal forward \ndiscretization. \nerror\nBackward approx. \nof the ODE\nTotal backward\ndiscretization \nerror\nFigure 12.3: Forward and backward discretizations when using the continuous\nadjoint method.\n12.6.4\nGradients via reverse-mode on discretization\nA simpler approach consists in replacing the objective in Eq. (12.7) by\nits version discretized using some numerical method, such as an Euler\nforward discretization scheme. That is, we seek to solve\nmin\nw∈W\nL(sK)\nwhere\nsk = sk−1 + δh(kδ, sk−1, w) k ∈{1, . . . , K},\nwith s0 = 0 and δ some discretization step. Gradients of the objective\ncan be computed by automatic differentiation. That approach is often\nreferred to as discretize-then-optimize. At first glance, this approach\nmay suffer from very high memory requirements. Indeed, to get an\naccurate solution of the ODE, a numerical integration method may\nrequire K to be very large. Since a naive implementation of reverse-mode\nautomatic differentiation has a memory that scales linearly with K,\ncomputing the gradient by a discretize-then-optimize method could be\nprohibitive. However, the memory requirements may easily be amortized\nusing checkpointing, as explained in Section 8.5; see also (Gholaminejad\net al., 2019).\nAs for the optimize-then-discretize method, we still accumulate\nsome truncation errors in the forward discretization process. This dis-\ncretization error occurs when computing the gradient in reverse-mode\n\n294\nDifferentiating through integration\ntoo. The discretize-then-optimize method can be seen as computing\ngradients of a surrogate objective. For that objective, the gradients are\ncorrect and well-defined. However, they may not match the gradients of\nthe true ODE formulation.\nTo compare the discretize-then-optimize and optimize-then-discretize\napproaches, Gholaminejad et al. (2019) compared their performance on\nan ODE whose solution can be computed analytically by selecting h\nto be linear in s. The authors observed that discretize-then-optimize\ngenerally outperformed optimize-then-discretize. A middle ground can\nactually be found by using reversible differentiation schemes.\n12.6.5\nReversible discretization schemes\nOur exposition of the optimize-then-discretize or discretize-then-optimize\napproaches used a simple Euler explicit discretization scheme. However,\nfor both approaches, we could have used other discretization schemes\ninstead, such as reversible discretization schemes.\nA reversible discretization scheme is a discretization scheme such\nthat we have access to a closed-form formula for the inverse of its\ndiscretization step. Formally, a discretization method M builds an\napproximation (sk)K\nk=1 of the solution of an ODE s′(t) = h(t, s(t)) on\nan interval [0, T] by computing for k ∈(1, . . . , K)\ntk, sk, ck = M(tk−1, sk−1, ck−1; h, δ),\n(12.8)\nwhere δ > 0 is some fixed discretization step, tk is the time step (typically\ntk = tk−1+δ), sk is the approximation of s(tk), and ck is some additional\ncontext variables used by the discretization method to build the iterates.\nAn explicit Euler method does not have a context, but just as an\noptimization method may update some internal states, a discretization\nmethod can update some context variable. The discretization scheme\nin Eq. (12.8) is a forward discretization scheme as we took a positive\ndiscretization step. By taking a negative discretization step, we obtain\nthe corresponding backward discretization scheme, for k ∈(K, . . . , 1),\ntk−1, sk−1, ck−1 = M(tk, sk, ck; h, −δ).\n\n12.6. Differential equations\n295\nA discretization method is reversible if we have access to M−1 to\nrecompute the inputs of the discretization step from its outputs,\ntk−1, sk−1, ck = M−1(tk, sk, ck; h, δ).\nA reversible discretization method is symmetric if the backward dis-\ncretization scheme is exactly the inverse of the forward discretization\nscheme, i.e.,\nM(tk, sk, ck; h, −δ) = M−1(tk, sk, ck; h, δ).\nThe explicit Euler method is clearly not symmetric and a priori not\nreversible, unless we can solve for yk−1, the equation yk = yk−1 −\nδf(yk−1).\nLeapfrog method\nThe (asynchronous) leapfrog method (Zhuang et al., 2021; Mutze,\n2013) on the other hand is an example of symmetric reversible discretiza-\ntion method. For a constant discretization step δ, given tk−1, sk−1, ck−1\nand a function h, it computes\n¯tk−1 := tk−1 + δ\n2\n¯sk−1 := sk−1 + δ\n2ck−1\n¯ck−1 := h(¯tk−1, ¯sk−1)\ntk := ¯tk−1 + δ\n2\nsk := ¯sk−1 + δ\n2 ¯ck−1\nck := 2¯ck−1 −ck−1\nM(tk−1, sk−1, ck−1; h, δ) := (tk, sk, ck).\nOne can verify that we indeed have M(tk, sk, ck; h, −δ) = (tk−1, sk, ck).\nBy using a reversible symmetric discretization scheme in the optimize-\nthen-discretize approach, we ensure that, at the end of the backward\ndiscretization pass, we recover exactly the original input. Therefore, by\nrepeating forward and backward discretization schemes we always get\nthe same gradient, which was not the case for an Euler explicit scheme.\n\n296\nDifferentiating through integration\nBy using a reversible discretization scheme in the discretize-then-\noptimize method, we address the memory issues of reverse mode autodiff.\nAs explained in Section 8.6, we can recompute intermediate values during\nthe backward pass rather than storing them.\nMomentum residual networks\nIn the leapfrog method, the additional variables ck may actually be\ninterpreted as velocities of a system whose acceleration is driven by\nthe given function, that is, s′′(t) = h(t, s(t), w). Such an interpretation\nsuggests alternatives to the usual neural ODE paradigm. For instance,\nmomentum neural networks (Sander et al., 2021b), can be inter-\npreted as the discretization of a second-order ordinary differential\nequations, which are naturally amenable to reversible differentiation\nschemes with a low memory footprint.\n12.6.6\nProof of the continuous adjoint method\nIn the following, we denote s(t, x, w) the solution of the ODE at time t\ngiven the input x and the parameters w. We focus here on the formula-\ntion of the VJP. The proof relies on the existence of partial derivatives\nof s(t, x, w), which we do not cover here and refer to, e.g., Pontryagin\n(1985) for a complete proof of such facts given the assumptions.\nWe use the ODE constraint to introduce adjoint variables, this time\nin the form of a continuously differentiable function r. For any such a\nfunction r, we have\n⟨f(x, w), u⟩= ⟨s(T, x, w), u⟩\n+\nZ T\n0\n⟨r(t), h(t, s(t, x, w), w) −∂ts(t, x, w)⟩dt,\nusing Leibniz notations such as ∂ts(t, x, w) = ∂1s(t, x, w). The VJPs\n\n12.6. Differential equations\n297\nthen decompose as\n∂wf(x, w)∗[u]\n= ∂ws(T, x, w)∗u\n+\nZ T\n0\n(∂ws(t, x, w)∗∂∗\nsh(t, s(t, x, w), w)∗−∂2\nwts(t, x, w)∗)r(t)dt\n+\nZ T\n0\n∂wh(t, s(t, x, w), w)∗r(t)dt,\n∂xf(x, w)∗[u]\n= ∂xs(T, x, w)∗u\n+\nZ T\n0\n(∂xs(t, x, w)∗∂∗\nsh(t, s(t, x, w), w)∗−∂2\nxts(t, x, w)∗)r(t)dt\nHere the second derivative terms ∂2\nwts(t, x, w)∗r, ∂2\nxts(t, x, w)∗r cor-\nrespond to second derivatives of ⟨s(t, x, w), r⟩. Since the Hessian is\nsymmetric (Schwartz’s theorem presented in Proposition 2.10), we can\nswap the derivatives in t and w or x. Then, to express the gradient\nuniquely in terms of first derivatives of s, we use an integration by part\nto have for example\nZ T\n0\n∂2\nwts(t, x, w)∗r(t)dt =\nZ T\n0\n∂2\ntws(t, x, w)∗r(t)dt\n= (∂ws(T, x, w)∗r(T) −∂ws(0, x, w)∗r(0))\n−\nZ T\n0\n∂ws(t, x, w)∗r(t)∗∂tr(t)dt.\nSince s(0) = x, we have ∂ws(0, x, w)∗r(0) = 0. The VJP w.r.t. w can\nthen be written as\n∂wf(x, w)∗[u]\n= ∂ws(T, x, w)∗[u −r(T)]\n+\nZ T\n0\n∂ws(t, x, w)∗[∂sh(t, s(t, x, w), w)∗r(t) + ∂tr(t)]dt\n+\nZ T\n0\n∂wh(t, s(t, x, w), w)∗r(t)dt.\nBy choosing r(t) to satisfy the adjoint ODE\n∂tr(t) = −∂sh(t, s(t, x, w), w)∗r(t),\nr(T) = u,\n\n298\nDifferentiating through integration\nthe expression of the VJP simplifies as\n∂wf(x, w)∗[u] =\nZ T\n0\n∂wh(t, s(t, x, w), w)∗r(t)dt.\nFor the VJP w.r.t. to x, we can proceed similarly. Using an integration\nby part, we have, this time, ∂xs(0, x, w)∗r(0) = r(0) since s(0) = x.\nChoosing the same curve r(t) satisfying the adjoint ODE we get\n∂xf(x, w)∗[u] = r(0).\nThe existence of a curve r solution of the backward ODE can easily be\nshown from Picard Lindelöf’s theorem and the assumptions.\n12.7\nSummary\n• We studied how to differentiate integrals, with a focus on expec-\ntations and solutions of a differential equation.\n• For differentiating through expectations, we studied two main\nmethods: the score function estimator (SFE, a.k.a. REINFORCE)\nand the path gradient estimator (PGE, a.k.a. reparametrization\ntrick).\n• The SFE is suitable when it is easy to sample from the distribution\nand its log-PDF is explicitly available. It is an unbiased estimator,\nbut is known to suffer from high variance.\n• The PGE is suitable for pushforward distributions, distributions\nthat are implicitly defined through a transformation, or a se-\nquence of them. These distributions can be easily sampled from,\nby injecting a source of randomness (such as noise) through the\ntransformations. An unbiased, low-variance estimator of the gra-\ndient of their expectation is easily obtained, provided that we can\ninterchange integration and differentiation.\n• If we have an explicit distribution, we can sometimes convert it\nto an implicit distribution, thanks to the location-scale trans-\nformation or the inverse transformation.\n\n12.7. Summary\n299\n• Conversely, if we have an implicit distribution, we can convert it to\nan explicit distribution using the change-of-variables theorem.\nHowever, this formula requires to compute the determinant of\nan inverse Jacobian, and is computationally expensive in general.\nNormalizing flows use invertible transformations so that the inverse\nJacobian is cheap to compute, by design.\n• Stochastic computation graphs can use a mix of explicit and\nimplicit distributions at each node.\n• For differentiating through the solution of a differential equation,\ntwo approaches can be considered.\n• We can express the gradient as the solution of a differential equa-\ntion thanks to the continuous adjoint method. We may then\ndiscretize backwards in time the differential equation that the gra-\ndient satisfies. This is the optimize-then-discretize approach.\n• We can also first discretize the problem in such a way that the\ngradient can simply be computed by reverse mode auto-diff, ap-\nplied on the discretization steps. This is the discretize-then-\noptimize approach. The optimize-then-discretize approach has\nno memory cost, but discrepancies between the forward and back-\nward discretization passes often lead to numerical errors. The\ndiscretize-then-optimize introduces no such discrepancies but may\ncome at a large memory cost.\n• Reversible discretization schemes can circumvent the memory\ncost, as they enable the recomputation of intermediate discretiza-\ntion steps backwards in time.\n\nPart IV\nSmoothing programs\n\n13\nSmoothing by optimization\nWhen a function is non-differentiable (or worse, discontinuous), a rea-\nsonable approach is to replace it by a differentiable approximation\n(or at least, by a continuous relaxation). We refer to the process of\ntransforming a non-differentiable function into a differentiable one as\n“smoothing” the original function. In this chapter, we begin by review-\ning a smoothing technique based on infimal convolution. We then\nreview an equivalent dual approach, based on the Legendre-Fenchel\ntransform. We illustrate how to apply these techniques to compute\nsmoothed ReLUs and smoothed max operators, as well as continuous\nrelaxations of step functions and argmax operators.\n13.1\nPrimal approach\nWe first review how to smooth functions in the original, primal space\nof the function, using the infimal convolution and more particularly the\nMoreau envelope, a.k.a. Moreau-Yoshida regularization. In this chapter,\nwe consider functions taking potentially infinite positive values, that is,\nfunctions taking values in the half-extended real line R ∪{∞}. For a\n301\n\n302\nSmoothing by optimization\nfunction f : RM →R ∪∞, we define its domain as\ndom(f) = {u ∈RM : f(u) < ∞}.\n13.1.1\nInfimal convolution\nAs we elaborate in Section 14.1.6, the infimal convolution, sometimes\nabbreviated inf-conv, can be seen as a counterpart of the usual convolu-\ntion, in which integration has been replaced by minimization (hence its\nname). We give its formal definition below.\nDefinition 13.1 (Infimal convolution). The infimal convolution be-\ntween two functions f : RM →R ∪{∞} and g: RM →R ∪{∞} is\ndefined by\n(f□g)(µ) :=\ninf\nu∈RM f(u) + g(µ −u)\n= inf\nz∈RM f(µ + z) + g(z)\n=\ninf\nu,z∈RM f(u) + g(z) s.t. u = µ + z.\nIt is easy to check that the three definitions are indeed equivalent,\nby using the change of variable u := µ + z, which is a location-scale\ntransform; see Section 12.4.1. Similarly to the classical convolution,\nthe infimal convolution between two functions f and g creates a new\nfunction f□g, and it is commutative, meaning that for all µ ∈RM,\nwe have\n(f□g)(µ) = (g□f)(µ).\nComputing the infimal convolution involves the resolution of a mini-\nmization problem, that may or may not enjoy an analytical solution.\nSome examples are given in Table 13.1.\nExistence\nThe infimal convolution (f□g)(µ) exists if the infimum infu∈RM f(u) +\ng(µ −u) is finite (Bauschke and Combettes, 2017, Proposition 12.6).\nA sufficient condition to achieve this is that u 7→f(u) + g(µ −u) is\nconvex for all µ ∈RM. However, this is not a necessary condition. For\n\n13.1. Primal approach\n303\nTable 13.1: Examples of infimal convolutions. We use ιC to denote the indicator\nfunction of the set C.\nf(u)\ng(z)\n(f□g)(µ)\nf(u)\n0\ninfu∈RM f(u)\nf(u)\nι{v}(z)\nf(µ −v)\nιC(u)\nιD(z)\nιC+D(µ)\nιC(u)\n∥z∥2\ndC(µ) = infu∈C ∥µ −u∥2\nf(u)\n1\n2∥z∥2\n2\nenvf(µ) = infu∈RM 1\n2∥µ −u∥2\n2 + f(u)\nexample, the infimum can be finite even if f or g are nonconvex, for\nexample if their domain is a compact set.\nInfimal convolution with a regularization function\nWhen a function f is non-differentiable, a commonly-used technique is\nto replace it by its infimal convolution f□R, with some regularization\nR. The most used regularization is the squared 2-norm, leading to the\nMoreau envelope, as we now review.\n13.1.2\nMoreau envelope\nWhen R(z) := 1\n2∥z∥2\n2, the infimal convolution f□R gives the so-called\nMoreau envelope of f, also known as Moreau-Yoshida regularization\nof f.\nDefinition 13.2 (Moreau envelope). Given a function f : RM →\nR ∪{∞}, its Moreau envelope is defined as\nenvf(µ) :=\n\u0012\nf□1\n2∥· ∥2\n2\n\u0013\n(µ)\n=\ninf\nu∈RM f(u) + 1\n2∥µ −u∥2\n2\n= inf\nz∈RM f(µ + z) + 1\n2∥z∥2\n2.\n\n304\nSmoothing by optimization\nIntuitively, the Moreau envelope is the minimal value over u ∈RM\nof a trade-off between staying close to the input µ according to the\nproximity term 1\n2∥µ −u∥2\n2 and minimizing f(u). Provided that the\nminimizer exists and is unique, we can define the associated proximal\noperator of f as\nproxf(µ) := arg min\nu∈RM\n1\n2∥µ −u∥2\n2 + f(u),\nIn other words, we have for proxf(µ) well defined,\nenvf(µ) = f(proxf(µ)) + 1\n2∥µ −proxf(µ)∥2\n2.\n(13.1)\nProperties\nA crucial property of the Moreau envelope envf is that for any convex\nfunction f, it is always a smooth function, even when f itself is not\nsmooth. By smooth, we formally mean that the resulting function envf\nis differentiable everywhere with Lipschitz-continuous gradients. We say\nL-smooth, if the gradients are L-Lipshcitz continuous. Such a property\ncan determine the efficiency of optimization algorithms as reviewed in\nSection 15.4. We recap below useful properties of the Moreau envelope.\nProposition 13.1 (Properties of Moreau envelope). Let f : RM →\nR ∪{∞}.\n1. Smoothness: If f is convex, the function envf is 1-smooth.\n2. Gradient: Provided that proxf(µ) is well-defined on µ ∈RM,\nthe gradient of the Moreau envelope can be expressed in terms\nof the proximal operator as\n∇envf(µ) = µ −proxf(µ).\n3. Moreau decomposition: If f is convex, then for any µ ∈\nRM, we have the following identity\nproxf(µ) + proxf∗(µ) = µ,\n\n13.1. Primal approach\n305\nwhere f∗is the convex conjugate of f, detailed in Section 13.2.\nIn particular, we get\n∇envf∗(µ) = proxf(µ)\n4. Convexity: envf is convex if f is convex.\n5. Infimums coincide envf has the same infimum as the origi-\nnal function f:\nmin\nµ∈RM envf(µ) = min\nu∈RM f(u).\nProof.\n1. This is best seen using the dual approach detailed in Section 13.3.\n2. This follows from Danskin’s theorem, reviewed in Section 11.2.\n3. See, e.g., Bauschke and Combettes (2017, Theorem 14.3).\n4. This follows from the fact that the infimum of a jointly convex\nfunction is convex.\n5. We have\ninf\nµ∈RM envf(µ) =\ninf\nµ∈RM\ninf\nu∈RM\n1\n2∥µ −u∥2\n2 + f(u)\n=\ninf\nu∈RM\ninf\nµ∈RM\n1\n2∥µ −u∥2\n2 + f(u)\n=\ninf\nu∈RM f(u).\nExamples\nTo illustrate smoothing from the Moreau envelope perspective, we\nshow how to smooth the 1-norm. In this case, we obtain an analytical\nexpression for the Moreau envelope.\n\n306\nSmoothing by optimization\n3\n2\n1\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nHuber loss\nAbsolute loss\nFigure 13.1: The Huber loss is the Moreau envelope of the absolute loss.\nExample 13.1 (Smoothing the 1-norm via infimal convolution). We\nwish to smooth f(u) := ∥u∥1 = PM\nj=1 |ui|. The corresponding prox-\nimal operator is the soft-thresholding operator (see Section 16.4),\nproxf(µ) = arg min\nu∈RM\n1\n2∥µ −u∥2\n2 + ∥u∥1\n= sign(µ) · max(|µ| −1, 0).\nUsing Eq. (13.1) and after some algebraic manipulations, we obtain\nenvf(µ) =\nM\nX\nj=1\nhuber(µj) ≈\nM\nX\nj=1\n|µj|,\nwhere we defined the Huber loss\nhuber(µj) :=\n\n\n\nµ2\nj\n2\nif |µj| ≤1\n|µj| −1\n2\nif |µj| > 1\n.\nThis is illustrated in Fig. 13.1 with M = 1.\nWe also illustrate in Fig. 13.2 that the Moreau envelope of nonconvex\nfunctions can be approximately computed numerically.\n\n13.1. Primal approach\n307\n2\n0\n2\n0\n1\n2\n3\nReLU\nOriginal\nMoreau env\n2\n0\n2\n0.00\n0.25\n0.50\n0.75\n1.00\nRamp\n2\n0\n2\n0.00\n0.25\n0.50\n0.75\n1.00\nStep\nFigure 13.2: The Moreau envelope is not limited to convex functions. For instance,\nthe ramp function is continuous but nonconvex, and the step function is not only\nnonconvex but also discontinuous. In this figure, we approximately computed the\ninfimum over u ∈R in Definition 13.2 by restricting the search on a finite grid, in a\nclosed interval.\n13.1.3\nVector-valued functions\nThe Moreau envelope is defined by envf(µ) := infu∈RM f(u)+ 1\n2∥µ−u∥2\n2.\nAs such, it is limited to scalar-valued functions f : RM →R. To extend\nthe Moreau envelope to vector-valued functions f : RM →RT , where\nf(u) = (f1(u), . . . , fT (u)) and fi : RM →R for i ∈[T], we may choose\nto smooth each fj separately to define\nenvf(µ) := (envf1(µ), . . . , envfT (µ)),\nwhere\nenvfi(µ) =\ninf\nui∈RM fi(ui) + 1\n2∥µ −ui∥2\n2.\nThis approach requires to solve T separate minimization problems and\nperforms the smoothing of each output coordinate i ∈[T] independently.\nFrom Proposition 2.9, we then have that the VJP of envf(u) with any\ndirection d ∈RT is\n∂envf(µ)∗[d] =\nT\nX\ni=1\n∂envfi(µ)∗[di]\n=\nT\nX\ni=1\ndi∇envfi(µ).\nIn the particular case f(u) = (f1(u1), . . . , fT (uT )), we obtain\n∂envf(µ)∗[d] =\nT\nX\ni=1\ndienvfi(µi).\n\n308\nSmoothing by optimization\nAn alternative was proposed by Roulet and Harchaoui (2022). For a\ndifferentiable function f : RM →RT , we recall that the VJP of f with\na direction d ∈RT reads\n∂f(u)∗[d] = ∇⟨f, d⟩(u),\nwhere we defined the scalar-valued function ⟨f, d⟩(u) := ⟨f(u), d⟩. As a\nresult, if f is non differentiable, a natural idea is to approximate its VJP\n∂f(u)∗[d] (had it existed) by the gradient ∇env⟨f,d⟩(µ) of the Moreau\nenvelope\nenv⟨f,d⟩(µ) =\ninf\nu∈RM⟨f(u), d⟩+ 1\n2∥µ −u∥2\n2.\n(13.2)\nThis requires a single optimization problem to solve, independently of\nthe number of outputs T. Moreover, for d = ei, this recovers envfi(µ)\nas a special case.\nThis approach allows in principle to perform reverse-mode autodiff\n(gradient backpropagation) on a neural network whose layers use the\nMoreau envelope. Indeed, following Proposition 13.1, the approximate\nVJP of f with a direction d is given by\n∂f(µ)∗[d] ≈∇env⟨f,d⟩(µ) = µ −u⋆,\nwhere u⋆is the solution of the minimization problem in Eq. (13.2).\nHowever, we emphasize that this minimization problem could be difficult\nto solve in general. Indeed, when performing gradient backpropagation,\nthe direction d is not necessarily non-negative, therefore the function\nbeing minimized in Eq. (13.2) could be nonconvex, even if each fi is\nconvex. Another potential caveat is that the direction d influences the\nsmoothing strength, while in principle we should be able to smooth a\nfunction independently of whether we compute its VJP or not. To see\nthat, for example in the particular case f(u) = (f1(u1), . . . , fT (uT )),\none easily checks that for d = (d1, . . . , dT ), we get\nenv⟨f,d⟩(µ) =\nT\nX\ni=1\nenvdifi(µi).\nSmoothing vector-valued functions by Moreau envelope (or more gen-\nerally, by infimal convolution) remains an open area of research. We\nwill see in Chapter 14 that smoothing by convolution more naturally\nsupports vector-valued functions.\n\n13.2. Legendre–Fenchel transforms, convex conjugates\n309\n13.2\nLegendre–Fenchel transforms, convex conjugates\nThe Legendre-Fenchel transform, a.k.a. convex conjugate, is a way to\nturn a function f into a new function, denoted f∗. We now review it in\ndetail, as it plays a major role for the dual approach to smoothing.\n13.2.1\nDefinition\nConsider the class of affine functions of the form\nu 7→⟨u, v⟩−b.\nThese functions are parametrized by their slope v ∈RM and their\nintercept −b ∈R. Now, suppose we fix v. Given a function f(u), affine\nlower bounds of f(u) are all the functions of u such that b satisfies for\nall u ∈RM,\n⟨u, v⟩−b ≤f(u) ⇐⇒⟨u, v⟩−f(u) ≤b.\nThe tightest lower bound is then defined by b such that\nb :=\nsup\nu∈dom(f)\n⟨u, v⟩−f(u),\nwhere we recall that the domain of f is defined by\ndom(f) := {u ∈RM : f(u) < ∞}.\nThis leads to the definition of Legendre-Fenchel transform, a.k.a.\nconvex conjugate.\nDefinition 13.3 (Legendre-Fenchel transform, convex conjugate). Given\na function f : RM →R ∪{∞}, its convex conjugate is defined by\nf∗(v) :=\nsup\nu∈dom(f)\n⟨u, v⟩−f(u).\nWe use a sup rather than a max to indicate that f∗(v) is potentially\n∞. Following the previous discussion, −f∗(v) is the intercept of the\ntightest affine lower bound with slope v of f(u). This is illustrated\nFig. 13.3.\n\n310\nSmoothing by optimization\n0\n1\nu\nf (u)\nf * (v)\nv\nFigure 13.3: For a fixed slope v, the function u 7→uv −f ∗(v) is the tighest affine\nlower bound of f with slope v.\nThe Legendre-Fenchel transform is a function transformation, as it\nproduces a new function f∗. It can be seen as a dual representation\nof a function: instead of representing a convex function f by its graph\n(u, f(u)) for u ∈dom(f), we can represent it by the set of tangents\nwith slope v and intercept −f∗(v) for v ∈dom(f∗), as illustrated\nin Fig. 13.4. As the name “convex conjugate” indicates, it is convex,\neven if the original function is not.\n13.2.2\nClosed-form examples\nComputing f∗(v) involves the resolution of a maximization problem,\nwhich could be difficult in general without assumption on f. In some\ncases, however, we can compute an analytical expression, as we now\nillustrate.\nExample 13.2 (Analytical conjugate examples). When f(u) = 1\n2∥u∥2\n2,\nwith dom(f) = RM, the conjugate is\nf∗(v) = max\nu∈RM⟨u, v⟩−1\n2∥u∥2\n2.\n\n13.2. Legendre–Fenchel transforms, convex conjugates\n311\n0.0\n0.5\n1.0\nu\n1.0\n0.5\n0.0\nf (u)\nf * (v)\nv\n2\n1\n0\n1\nv\n0.0\n0.5\n1.0\nf * (v)\nFigure 13.4: Left: instead of representing a convex function f by its graph (u, f(u))\nfor u ∈dom(f), we can represent it by the set of tangents with slope v and intercept\n−f ∗(v) for v ∈dom(f ∗). Right: by varying the slope v of all possible tangents, we\nobtain a function of the slope v rather than of the original input u. The colors of the\ntangents on the left are chosen to match the colors of the vertical lines on the right.\nSetting the gradient u 7→⟨u, v⟩−1\n2∥u∥2\n2 to zero, we obtain u⋆= v.\nPlugging u⋆back, we therefore obtain\nf∗(v) = ⟨u⋆, v⟩−1\n2∥u⋆∥2\n2 = 1\n2∥v∥2\n2.\nTherefore, f = f∗in this case.\nWhen f(u) = ⟨u, log u⟩, with dom(f) = RM\n+ , the minimizer of\nu 7→⟨u, v⟩−⟨u, log u⟩is u⋆= exp(v −1) and the conjugate is\nf∗(v) =\nM\nX\nj=1\nexp(vj −1).\nSee for instance Boyd and Vandenberghe (2004) or Beck (2017) for\nmany more examples.\n\n312\nSmoothing by optimization\nConstraining the domain\nWe can incorporate constraints using an indicator function with\nvalues in the extended real line R ∪{∞},\nιC(u) :=\n\n\n\n0\nif u ∈C\n+∞\notherwise\n.\nExample 13.3 (Incorporating constraints). If f(u) = ιC(u), where\nC is a convex set, then\nf∗(v) =\nsup\nu∈dom(f)\n⟨u, v⟩−f(u) = sup\nu∈C\n⟨u, v⟩:= σC(v),\nwhich is known as the support function of C. The corresponding\nargmax (assuming that it exists),\nv 7→arg max\nu∈C\n⟨u, v⟩,\nis known as the linear maximization oracle (LMO) of C. As\nanother example, if f(u) = ⟨u, log u⟩+ ι△M (u) then\nf∗(v) = logsumexp(v) = log\nM\nX\ni=1\nexp(vj).\nWe postpone a proof to Proposition 13.9.\n13.2.3\nProperties\nThe conjugate enjoys several useful properties, that we now summarize.\nProposition 13.2 (Convex conjugate properties).\n1. Convexity: f∗(v) is a convex function for all f : RM →\nR ∪{∞} (even if f is nonconvex).\n2. Fenchel-Young inequality: for all u, v ∈RM\nf(u) + f∗(v) −⟨u, v⟩≥0.\n\n13.2. Legendre–Fenchel transforms, convex conjugates\n313\n3. Gradient: if the supremum in Definition 13.3 is uniquely\nachieved, then f∗(v) is differentiable at v and its gradient is\n∇f∗(v) = arg max\nu∈dom(f)\n⟨u, v⟩−f(u).\nOtherwise, f∗(v) is sub-differentiable at v and we get a sub-\ngradient instead.\n4. Maps: If f and f∗are differentiable, then\nv = ∇f(u) ⇐⇒u = ∇f∗(v) ⇐⇒f∗(v)+f(u)−⟨u, v⟩= 0.\n5. Biconjugate: f = f∗∗if and only if f is convex and closed\n(i.e., its sublevel sets form a closed set), otherwise f∗∗≤f.\nProof.\n1. This follows from the fact that v 7→supu∈C g(u, v) is convex if g\nis convex in v. Note that this is true even if g is nonconvex in u.\nHere, g(u, v) = ⟨u, v⟩−f(u), which is affine in v and therefore\nconvex in v.\n2. This follows immediately from Definition 13.3.\n3. This follows from Danskin’s theorem, reviewed in Section 11.2.\nAnother way to see this is by observing that\nf∗(v) = ⟨g, v⟩−f(g)\nf∗(v′) ≥⟨g, v′⟩−f(g),\nwhere g := arg max\nu∈dom(f)\n⟨u, v⟩−f(u). Subtracting the two, we obtain\nf∗(v′) ≥f∗(v) + ⟨g, v′ −v⟩.\nNow, using that f∗is convex and Definition 15.6, we obtain that\ng = ∇f∗(v).\n4. See, e.g., Bauschke and Combettes (2017, Proposition 16.10).\n5. See Boyd and Vandenberghe (2004, Section 3.3).\n\n314\nSmoothing by optimization\n13.2.4\nConjugate calculus\nWhile deriving a convex conjugate expression can be difficult in general,\nin some cases, it is possible to use simple rules to derive conjugates in\nterms of other conjugates.\nProposition 13.3 (Conjugate calculus rules).\n1. Separable sum of functions: if f(u) = PM\nj=1 fj(uj), then\nf∗(v) =\nM\nX\nj=1\nf∗\nj (vj).\n2. Scalar multiplication: if f(u) = c · g(u), for c > 0, then\nf∗(v) = c · g∗(v/c).\n3. Addition to an affine function and translation: if f(u) =\ng(u) + ⟨α, u⟩+ β, then\nf∗(v) = g∗(v −α) −β.\n4. Composition with an invertible linear map: if f(u) =\ng(Mu), where x 7→Mx is an invertible linear map, then\nf∗(v) = g∗(M−T v).\n5. Non-separable sum of functions: if h1 and h2 are convex\nfunctions, then (h1 + h2)∗= h∗\n1□h∗\n2, where □is the infimal\nconvolution operator.\n13.2.5\nFast Legendre transform\nWhen an analytical expression is not available, we can resort to numeri-\ncal schemes to approximately compute the transform / conjugate. When\nf is convex, because −f is concave, the maximization in Definition 13.3\nis that of a concave function. Therefore, the conjugate can be computed\nto arbitrary precision in polynomial time using classical iterative algo-\n\n13.3. Dual approach\n315\nrithms for constrained optimization such as projected gradient descent\n(Section 16.3) or conditional gradient a.k.a. Frank-Wolfe (Jaggi, 2013).\nWithout convexity assumption on f, f∗(v) can be approximated by\nf∗(v) ≈sup\nu∈U\n⟨u, v⟩−f(u),\nwhere U ⊆dom(f) is a discrete grid of values. We can then compute\nf∗(v) for several inputs v ∈V using the linear-time Legendre transform\nalgorithm (Lucet, 1997), where V ⊆dom(f∗) is another discrete grid.\nThe complexity is O(|U| · |V|), which is linear in the grid sizes. However,\nthe grid sizes are typically |U| = |V| = O(NM), for N equally-distributed\npoints in each of the M dimensions. Therefore, this approach is limited\nto small-dimensional settings, e.g., M ∈{1, 2, 3}.\n13.3\nDual approach\nPreviously, we presented how to smooth a function by performing\nits infimal convolution with a primal-space regularization R. We now\npresent how to smooth a function by regularizing its Legendre-Fenchel\ntransform (convex conjugate) instead. This dual, equivalent approach,\nis often mathematically more convenient.\n13.3.1\nDuality between strong convexity and smoothness\nWe begin by stating a well-known result that will underpin this whole\nsection: smoothness and strong convexity are dual to each other (Hiriart-\nUrruty and Lemaréchal, 1993; Kakade et al., 2009; Beck, 2017; Zhou,\n2018).\nProposition 13.4 (Duality between strong convexity and smoothness).\nf is 1\nµ-strongly convex w.r.t. the norm ∥· ∥over dom(f) if and only\nif f∗is µ-smooth w.r.t. the dual norm ∥· ∥∗over dom(f∗).\nFor a review of the notions of smoothness and strong convexity,\nsee Section 15.4. We give two examples of strongly-convex and smooth\nconjugate pairs in Table 13.2.\n\n316\nSmoothing by optimization\nTable 13.2: Examples of strongly-convex and smooth conjugate pairs.\nFunction\nNorm\nDomain\nConjugate\nDual norm\nDual domain\n1\n2∥u∥2\n2\n∥· ∥2\nRM\n1\n2∥v∥2\n2\n∥· ∥2\nRM\n⟨u, log u⟩\n∥· ∥1\n△M\nlogsumexp(v)\n∥· ∥∞\nRM\n13.3.2\nSmoothing by dual regularization\nThe duality between smoothness and strong convexity suggests a generic\napproach in order to smooth a function f : RM →R, by going through\nthe dual space.\n1. Compute the conjugate f∗:\nf∗(v) :=\nsup\nu∈dom(f)\n⟨u, v⟩−f(u).\n2. Add strongly-convex regularization Ωto the conjugate:\nf∗\nΩ(v) := f∗(v) + Ω(v).\n(13.3)\n3. Go back to the primal space, by computing the conjugate of f∗\nΩ:\nfΩ(u) := f∗∗\nΩ(u) = max\nv∈RM⟨u, v⟩−f∗\nΩ(v).\nNote that u and v belong to different spaces, i.e., u ∈dom(f) and\nv ∈dom(f∗). Following Proposition 13.4, if Ωis µ-strongly convex,\nthen fΩ(u) is 1\nµ-smooth. Furthermore, following Proposition 13.2, fΩ(u)\nis convex, even if f is nonconvex. Therefore, fΩ(u) is a smooth and\nconvex relaxation of f(u).\nSteps 1 and 3 are the most challenging, as they both require the\nderivation of a conjugate. While an analytical solution may not exist in\ngeneral, in some simple cases, there is, as we now illustrate.\nExample 13.4 (Smoothing the 1-norm via dual regularization). We re-\nvisit Example 13.1, this time from the dual perspective. We wish\nto smooth out the 1-norm f(u) := ∥u∥1 = PM\nj=1 |uj|.\n1. Compute the conjugate. The conjugate of any norm ∥· ∥\n\n13.3. Dual approach\n317\nis the indicator function of the dual norm’s unit ball {v ∈\nRM : ∥v∥∗≤1} (see e.g. Boyd and Vandenberghe (2004,\nExample 3.26)). The dual norm of ∥u∥1 is ∥v∥∞. Moreover,\n{v ∈RM : ∥v∥∞≤1} = [−1, 1]M.\nRecalling that ιC is the indicator function of C, we obtain\nf∗(v) = ι[−1,1]M (v).\n2. Adding strongly-convex regularization. We add quadratic\nregularization Ω(v) := 1\n2∥v∥2\n2 to define\nf∗\nΩ(v) := ι[−1,1]M (v) + Ω(v).\n3. Going back to the primal.\nfΩ(u) = f∗∗\nΩ(u) = ⟨u, v⋆⟩−Ω(v⋆) =\nM\nX\ni=1\nhuber(ui),\nwhere v⋆= clip (u) := max (min (u, 1) , −1).\nWe therefore indeed recover the Huber loss from Example 13.1.\nReLU functions can be smoothed out in a similar way, as we see in\nmore details in Section 13.4.\nThe dual approach allows us to easily bound the smoothed function\nin terms of the original function.\nProposition 13.5 (Bounds). If LΩ≤Ω(v) ≤UΩfor all v ∈dom(Ω),\nthen for all u ∈RM,\nf(u) −UΩ≤fΩ(u) ≤f(u) −LΩ.\nProof. Let us define\nv⋆:= arg max\nv∈RM ⟨u, v⟩−f∗(v)\nv⋆\nΩ:= arg max\nv∈RM ⟨u, v⟩−f∗\nΩ(v),\n\n318\nSmoothing by optimization\nwhere we recall that f∗\nΩ:= f∗+ Ω. We then have for all u ∈RM\nfΩ(u) = ⟨u, v⋆\nΩ⟩−f∗\nΩ(v⋆\nΩ) ≥⟨u, v⋆⟩−f∗\nΩ(v⋆) = f(u) −Ω(v⋆)\nand similarly\nf(u) −Ω(v⋆\nΩ) = ⟨u, v⋆⟩−f∗(v⋆) −Ω(v⋆\nΩ) ≥⟨u, v⋆\nΩ⟩−f∗\nΩ(v⋆\nΩ) = fΩ(u).\nCombining the two with LΩ≤Ω(v) ≤UΩfor all v ∈dom(Ω), we obtain\nf(u) −UΩ≤f(u) −Ω(v⋆) ≤fΩ(u) ≤f(u) −Ω(v⋆\nΩ) ≤f(u) −LΩ.\nRemark 13.1 (The gradient is differentiable almost everywhere). From\nProposition 13.2, the gradient of fΩ(u) equals\n∇fΩ(u) = arg max\nv∈RM ⟨u, v⟩−f∗\nΩ(v).\nIf Ωis strongly convex, then fΩis smooth, meaning that ∇fΩ\nis Lipschitz continuous. From Rademacher’s theorem reviewed in\nSection 2.7.1, ∇fΩis then differentiable almost everywhere (that is,\nfΩis twice differentiable almost everywhere). We use this property\nin the sequel to define continuous differentiable almost everywhere\nrelaxations of step functions and argmax operators.\n13.3.3\nEquivalence between primal and dual regularizations\nSo far, we saw two approches to obtain a smooth approximation of a\nfunction f. The first approach is based on the infimal convolution f□R,\nwhere R: dom(f) →R denotes primal regularization. The second ap-\nproach is based on regularizing the Legendre-Fenchel transform (convex\nconjugate) f∗of f with some dual regularization Ω, to define fΩ. It\nturns out that both approaches are equivalent.\nProposition 13.6 (Equivalence between primal and dual regularizations).\nLet f : RM →R ∪{∞} and R: RM →R ∪{∞}, both convex and\nclosed. Then, fΩ= f□R with Ω= R∗.\n\n13.3. Dual approach\n319\nProof. We have\nfΩ(u) = (f∗+ Ω)∗(u) =\nsup\nv∈dom(f∗)\n⟨u, v⟩−f∗(v) −Ω(v).\nIf h1 and h2 are convex, we have (h1 + h2)∗= h∗\n1□h∗\n2 (Beck, 2017,\nTheorem 4.17). Using h1 = f∗and h2 = Ω= R∗gives the desired result\nusing that f∗∗= f and R∗∗= R since both are convex and closed (see\nProposition 13.2).\nIn particular, with Ω= 1\n2∥· ∥2\n2 = Ω∗, this shows that the Moreau\nenvelope can equivalently be written as\nenvf = fΩ= fΩ∗.\nGiven the equivalence between the primal and dual approaches, using\none approach or the other is mainly a matter of mathematical or\nalgorithmic convenience, depending on the case.\nIn this book, we focus on applications of smoothing techniques to dif-\nferentiable programming. For applications to non-smooth optimization,\nsee Nesterov (2005) and Beck and Teboulle (2012).\n13.3.4\nRegularization scaling\nDual approach\nIf Ωis 1-strongly convex, then fΩis a 1-smooth approximation of the\noriginal function f. To control the smoothness of the approximation,\nit suffices to regularize with εΩfor ε > 0, leading to a 1/ε-smooth\napproximation fεΩof f. Moreover, one can check that\nfεΩ(v) = εfΩ(v/ε)\n∇fεΩ(v) = ∇fΩ(v/ε).\nTherefore, if we know how to compute fΩ, we can also compute fεΩand\nits gradient easily. Furthermore, the approximation error induced by\nthe smoothing can be quantified using Proposition 13.5 as we then have\nf(u) −εUΩ≤fΩ(u) ≤f(u) −εLΩ,\nprovided that LΩ≤Ω(v) ≤UΩfor all v ∈dom(Ω).\n\n320\nSmoothing by optimization\nPrimal approach\nFollowing Definition 13.2, if we use dual regularization εΩ, where ε > 0\ncontrols the regularization strength, the corresponding primal regular-\nization is R = εΩ∗(·/ε). That is, we have\nfεΩ= f□εΩ∗(·/ε).\nIn the particular case Ω(v) = 1\n2∥v∥2\n2, we have\nR(u) = ε\n2∥u/ε∥2\n2 = 1\n2ε∥u∥2\n2 = 1\nεΩ(u).\nWe therefore get\nfεΩ= f□1\nεΩ= 1\nε(εf□Ω) = 1\nεenvεf.\n13.3.5\nGeneralized entropies\nA natural choice of dual regularization Ω(π), when π ∈△M is a discrete\nprobability distribution, is a negative entropy function, also known as\nnegentropy. Since negentropies play a major role in smoothed max\noperators, we discuss them in detail here.\nInformation content and entropy\nAn entropy function measures the amount of “suprise” of a random\nvariable or equivalently of a distribution. To define an entropy, we must\nfirst define the information content I(E) of an event E. The value\nreturned by such a function should be 0 if the probability of the event is\n1, as there is no surprise. Conversely, information content should attain\nits maximal value if the probability of the event is 0, as it is maximally\nsurprising. Furthermore, the more probable an event E is, the less\nsurprising it is. Therefore, when p(E) increases, I(E) should decrease.\nOverloading the notation, we also write the information content of the\noutcome y of a random variable Y as the information content of the\nevent {Y = y},\nI(y) := I({Y = y}).\n\n13.3. Dual approach\n321\nGiven an information content function, we can then define the\nentropy H(Y ) of a random variable Y ∈Y as the expected information\ncontent,\nH(Y ) := E[I(Y )].\nDifferent definitions of information content lead to different definitions\nof entropy.\nShannon’s entropy\nA definition of information content satisfying the criteria above is\nI(E) := log\n\u0012\n1\np(E)\n\u0013\n= −log p(E).\nIndeed, −log 1 = 0, −log 0 = ∞and −log is a decreasing function over\n(0, 1]. Using this information content definition leads to Shannon’s\nentropy (Shannon, 1948)\nH(Y ) = E[I(Y )] = −\nX\ny∈Y\np(y) log p(y).\nWe can therefore define the Shannon entropy of a discrete probability\ndistribution π ∈△M as\nH(π) = −\nM\nX\ni=1\nπi log πi = −⟨π, log π⟩\nand use the corresponding negentropy as regularization\nΩ(π) = −H(π) = ⟨π, log π⟩.\nThe function is strongly convex w.r.t. ∥· ∥1 over △M. However, it is not\nstrongly convex over RM\n+ , since this is not a bounded set; see for instance\n(Blondel, 2019, Proposition 2). Since Ωis added to f∗in Eq. (13.3),\nwe can therefore use this choice of Ωto smooth out a function f if\ndom(f∗) ⊆△M.\nGini’s entropy\nAs an alternative, we can define information content as\nI(E) = 1\n2(1 −p(E)).\n\n322\nSmoothing by optimization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTsallis \n1 (Shannon)\nTsallis \n= 1.5\nTsallis \n= 2 (Gini)\nFigure 13.5: Tsallis entropies of the distribution π = (π, 1 −π) ∈△2, for π ∈[0, 1].\nAn entropy is a non-negative concave function that attains its maximum at the\nuniform distribution, here (0.5, 0.5). A negative entropy, a.k.a. negentropy, can be used\nas a dual regularization function Ωto smooth out a function f when dom(f ∗) ⊆△M.\nThe 1\n2 factor is for later mathematical convenience. This again satisfies\nthe criteria of an information content function. Indeed, i) when p(E) = 1,\nI(E) = 0 ii) when p(E) = 0, I(E) attains its maximum of\n1\n2 iii)\nthe function is decreasing w.r.t. p(E). Using this information content\ndefinition leads to Gini’s entropy a.k.a. Gini index (Gini, 1912)\nH(Y ) = E[I(Y )] = 1\n2\nX\ny∈Y\np(y)(1 −p(y)).\nWe can use Gini’s negative entropy to define for all π ∈△M\nΩ(π) = 1\n2⟨π, π −1⟩= 1\n2(∥π∥2\n2 −1).\nThe function is strongly convex w.r.t. ∥· ∥2 over RM. We can therefore\nuse this choice of Ωto smooth out a function f if dom(f∗) ⊆RM. This\nmeans that the set of functions that we can smooth out with Gini\nentropy is larger than the set of functions we can smooth out with\nShannon entropy.\nTsallis entropies\nGiven α ≥1, a more general information content definition is\nI(E) =\n1\nα(α −1)(1 −p(E)α−1).\n\n13.3. Dual approach\n323\n(1, 0, 0)\n(0, 1, 0)\n(0, 0, 1)\nTsallis \n1 (Shannon)\n(1, 0, 0)\n(0, 1, 0)\n(0, 0, 1)\nTsallis \n= 1.5\n(1, 0, 0)\n(0, 1, 0)\n(0, 0, 1)\nTsallis \n= 2 (Gini)\nFigure 13.6: Contours of Tsallis entropies on the probability simplex.\nUsing this definition leads to the Tsallis entropy (Tsallis, 1988)\nH(Y ) = E[I(Y )] =\n1\nα(α −1)\nX\ny∈Y\np(y)(1 −pα−1(y)).\nThe Tsallis entropy recovers the Shannon entropy in the limit α →1\nand the Gini entropy when α = 2. We can use the Tsallis negative\nentropy to define for all π ∈△M\nΩ(π) =\n1\nα(α −1)⟨π, πα−1 −1⟩=\n1\nα(α −1)(∥π∥α\nα −1),\nwhere ∥v∥p is the p-norm for (p ≥1)\n∥v∥p :=\n M\nX\ni=1\nvp\ni\n! 1\np\n,\nso that\n∥v∥p\np =\nM\nX\ni=1\nvp\ni .\nTsallis entropies for α →1 (Shannon entropy), α = 1.5 and α = 2 (Gini\nentropy) are illustrated in Fig. 13.5 and Fig. 13.6.\nDefinition and properties of generalized entropies\nSo far, we saw how to define an entropy as the expected information\ncontent. However, generalized entropies (DeGroot, 1962; Grünwald and\nDawid, 2004) do not necessarily need to take this form. We follow the\ndefinition of Blondel et al. (2020).\n\n324\nSmoothing by optimization\nDefinition 13.4 (Entropy function). A function H : △M →R+ is\nan entropy if\n1. H(π) = 0 if π ∈{e1, . . . , eM},\n2. H is strictly concave,\n3. H(Pπ) = H(π) for any permutation matrix P.\nThis definition implies that H is non-negative and is uniquely maxi-\nmized by the uniform distribution (Blondel et al., 2020, Proposition 4).\nThis is indeed what we expect from an entropy function. An example is\nthe squared p-norm entropy (Blondel et al., 2020)\nH(π) = 1\n2 −1\n2∥π∥2\np.\nSince the squared p-norm is strongly convex for p ∈(1, 2] (Ball et al.,\n2002), this entropy is strongly concave for p ∈(1, 2] and can therefore\nbe used to smooth out functions.\nWe now illustrate how to apply these techniques to compute smoothed\nReLUs and smoothed max operators, as well as continuous relaxations\nof step functions and argmax operators.\n13.4\nSmoothed ReLU functions\nTo demonstrate the application of the smoothing techniques discussed in\nthis chapter, we begin by explaining how to smooth the ReLU function.\nThe ReLU function is defined by\nrelu(u) :=\n\n\n\nu\nif u ≥0\n0\notherwise\n= max(u, 0).\nWe recall that in order to smooth a function f by the dual approach,\nwe calculate its conjugate f∗, add regularization Ωto it to obtain\nf∗\nΩ:= f∗+ Ωand then obtain fΩby computing f∗∗\nΩ.\nHere, we wish to smooth out f = relu. Its convex conjugate is\nrelu∗(π) = ι[0,1](π) =\n\n\n\n0\nif π ∈[0, 1]\n∞\nif π ̸∈[0, 1]\n.\n\n13.4. Smoothed ReLU functions\n325\nTo notice why, we observe that\nrelu(u) = max\nπ∈[0,1] u · π = max\nπ∈{0,1} u · π =\n\n\n\nu\nif u ≥0\n0\notherwise\n.\n(13.4)\nIndeed, since the objective is linear in π, the maximum is attained at\none of the extreme points of [0, 1], so that we can replace the constraint\nπ ∈[0, 1] with π ∈{0, 1}. This shows that the ReLU is exactly the\nsupport function of [0, 1]. Since the conjugate of the support function\nis the indicator function, we indeed obtain relu∗= ι[0,1]. We therefore\nhave\nrelu∗\nΩ(π) = relu∗(π) + Ω(π) = ι[0,1](π) + Ω(π)\nand for some choice of Ω, we need to be able to compute\nreluΩ(u) = max\nπ∈R u · π −(ι[0,1](π) + Ω(π))\n= max\nπ∈[0,1] u · π −Ω(π).\nThe softplus\nIf we use the regularizer Ω(π) = π log π + (1 −π) log(1 −π), which\ncomes from using Shannon’s negentropy ⟨π, log π⟩with π = (π, 1 −π),\nwe obtain\nreluΩ(u) = softplus(u) = log(1 + exp(u)).\nThis result is a special case of Proposition 13.9.\nThe sparseplus\nIf we use the regularizer Ω(π) = π(π −1), which comes from using\nGini’s negentropy with 1\n2⟨π, π −1⟩with π = (π, 1 −π), we obtain\nreluΩ(u) = sparseplus(u) =\n\n\n\n\n\n\n\n0,\nu ≤−1\n1\n4(u + 1)2,\n−1 < u < 1\nu,\nu ≥1\n.\nSee Fig. 13.8 (left figure) for a comparison of softplus and sparseplus.\n\n326\nSmoothing by optimization\n13.5\nSmoothed max operators\nAs a more elaborate application of the smoothing techniques discussed\nin this chapter, we explain how to smooth max operators. Smoothed\nmax operators include smoothed ReLU functions as a special case.\n13.5.1\nDefinition and properties\nWith a slight notation overloading, given a vector u = (u1, . . . , uM) ∈\nRM, we define its maximum as\nmax(u) := max\nj∈[M] uj.\nTo obtain a smooth approximation maxΩof max, we again apply the\ndual approach. The conjugate of max is\nmax∗(π) = ι△M (π).\nTo notice why, we observe that the vertices of the probability simplex\n△M are the standard basis vectors e1, . . . , eM. Since the objective is\nlinear, we then have\nmax(u) = max\nπ∈△M⟨u, π⟩=\nmax\nπ∈{e1,...,eM}⟨u, π⟩.\nIn other words, the maximum operator is exactly the support function\nof △M. Since the conjugate of the support function is the indicator\nfunction, we indeed obtain max∗= ι△M . We can therefore write\nmax∗\nΩ(π) = max∗(π) + Ω(π) = Ω(π) + ι△M (π)\nand\nmaxΩ(u) = (Ω+ ι△M )∗(u)\n= max\nπ∈RM⟨u, π⟩−(Ω(π) + ι△M (π))\n= max\nπ∈△M⟨u, π⟩−Ω(π).\nThe smoothed max operator maxΩcan be useful in a neural network,\nfor example as a smoothed max pooling layer. Its properties have been\nstudied in (Mensch and Blondel, 2018, Lemma 1), as we recall here for\nconvenience.\n\n13.5. Smoothed max operators\n327\nProposition 13.7 (Properties of maxΩ). The following properties\nhold.\n1. Bounds: if LΩ≤Ω(π) ≤UΩfor all π ∈△M, then max(u)−\nUΩ≤maxΩ(u) ≤max(u) −LΩfor all u ∈RM.\n2. Monotonicity: if u ≤v (element-wise), then maxΩ(u) ≤\nmaxΩ(v).\n3. Commutativity: if Ω(Pπ) = Ω(π) for any permutation\nmatrix P and any π ∈△M, then maxΩ(Pu) = maxΩ(u) for\nany permutation matrix P.\n4. Distributivity of +: maxΩ(u + c 1) = maxΩ(u) + c for all\nu ∈RM and all c ∈R.\nThese properties are leveraged in (Mensch and Blondel, 2018) to\ncreate differentiable dynamic programs. We consider in the following two\npossible choices of Ωleading to the softmax and sparsemax operators\nillustrated in Fig. 13.7.\nSmoothed min operators\nThe minimum operator can be expressed in terms of the maximum\noperator, since for all u ∈RM,\nmin(u) = −max(−u).\nGiven a smoothed max operator maxΩ, we can therefore easily define a\nsmoothed min operator as\nminΩ(u) := −maxΩ(−u).\n13.5.2\nReduction to root finding\nComputing maxΩ(u) for a general strongly-convex regularization Ωin-\nvolves the resolution of a maximum over probability simplex constraints.\nFor convenience, let us define the notation\nδΩ(u) := (Ω+ ιRM\n+ )∗(u) = max\nv∈RM\n+\n⟨u, v⟩−Ω(v).\n\n328\nSmoothing by optimization\nThe following proposition shows that we can reduce computing maxΩ\nto solving a root equation involving δΩ.\nProposition 13.8 (Computing maxΩas root finding). Suppose Ωis\nstrongly convex. For all u ∈RM,\nmaxΩ(u) = min\nτ∈R τ + δΩ(u −τ 1)\n= τ ⋆+ δΩ(u −τ ⋆1)\nand\n∇maxΩ(u) = ∇δΩ(u −τ ⋆1),\nwhere τ ⋆is the solution w.r.t. τ of the above min, which satisfies\nthe root equation\n⟨∇δΩ(u −τ ⋆1), 1⟩= 1.\nProof. The idea is to keep the non-negativity constraint explicit, but to\nuse a Lagrange multiplier for the equality constraint of the probability\nsimplex. We then have\nmaxΩ(u) = max\nv∈△M⟨u, v⟩−Ω(v)\n= max\nv∈RM\n+\nmin\nτ∈R⟨u, v⟩−Ω(v) −τ(⟨v, 1⟩−1)\n= min\nτ∈R τ + max\nv∈RM\n+\n⟨u −τ 1, v⟩−Ω(v)\n= min\nτ∈R τ + δΩ(u −τ 1),\nwhere we used that we can swap the min and the max, since (u, v) 7→\n⟨u, v⟩−Ω(v) is convex-concave and v ∈△M is an affine constraint. The\ngradient ∇δΩ(u) follows from Danskin’s theorem. The root equation\nfollows from computing the derivative of τ 7→τ +δΩ(u−τ 1) and setting\nit to zero.\n13.5.3\nThe softmax\nWhen Ωis Shannon’s negentropy, we obtain that maxΩis the softmax,\nalready briefly discussed in Section 4.4.2.\n\n13.5. Smoothed max operators\n329\nProposition 13.9 (Analytical expression of the softmax). When Ω(π) =\n⟨π, log π⟩, we get\nsoftmax(u) := maxΩ(u)\n= max\nπ∈△M⟨u, π⟩−Ω(π)\n= logsumexp(u)\n= log\nM\nX\nj=1\neuj.\nProof. Since dom(Ω) = RM\n+ , we have δΩ= Ω∗(i.e., the non-negativity\nconstraint is redundant). From Example 13.2, we therefore have δΩ(u) =\nPM\nj=1 exp(uj −1). From Proposition 13.8, maxΩ(u) = τ ⋆+ δΩ(u −τ ⋆1)\nwhere τ ⋆satisfies ⟨∇δΩ(u −τ ⋆1), 1⟩= 1. Since ∇δΩ(u) = exp(u −\n1), we need to solve PM\nj=1 exp(uj −1 −τ) = 1. We therefore get\nτ ⋆+ 1 = logsumexp(u) and therefore maxΩ(u) = logsumexp(u) −\n1 + PM\nj=1 exp(uj −logsumexp(u)) = logsumexp(u).\nSince −log M ≤Ω(π) ≤0 for all π ∈△M, following Proposi-\ntion 13.7, we get for all u ∈RM\nmax(u) ≤softmax(u) ≤max(u) + log M.\nA unique property of the softmax, which is not the case of all maxΩ\noperators, is that it supports associativity.\nProposition 13.10 (Associativity of the softmax). For all a, b, c ∈\nR,\nsoftmax(softmax(a, b), c) = softmax(a, softmax(b, c)).\n13.5.4\nThe sparsemax\nAlternatively, choosing Ωto be Gini’s negentropy leads to the sparsemax\n(Martins and Astudillo, 2016; Mensch and Blondel, 2018).\n\n330\nSmoothing by optimization\nProposition 13.11 (Variational formulation of sparsemax). When Ω(π) =\n1\n2⟨π, π −1⟩, we have\nsparsemax(u) := maxΩ(u)\n= max\nπ∈△M⟨u, π⟩−Ω(π)\n= ⟨u, π⋆⟩−Ω(π⋆)\nwhere\nπ⋆= sparseargmax(u) := arg min\nπ∈△M ∥u −π∥2\n2.\nProof. This follows from the fact that Ω(π) is up to a constant equal\nto 1\n2∥π∥2\n2 and completing the square.\nTherefore, computing the sparsemax can use the sparseargmax (the\nEuclidean projection onto the probability simplex) as a building block.\nWe discuss how to compute it in more detail in Section 13.7. Applying\nProposition 13.8 gives an alternative formulation.\nProposition 13.12 (Sparsemax as root finding). When Ω(π) = 1\n2⟨π, π−\n1⟩, we have\nsparsemax(u) = maxΩ(u) = min\nτ∈R τ + 1\n2\nM\nX\ni=1\n[ui −τ]2\n+\nand τ ⋆satisfies\nM\nX\ni=1\n[ui −τ]+ = 1.\nProof. First, we compute the expression of δΩ(u) = maxv∈RM\n+ ⟨u, v⟩−\nΩ(v). Setting the gradient of v 7→⟨u, v⟩−Ω(v) and clipping, we\nobtain v⋆= [u]+. Plugging v⋆back, we obtain δΩ(u) = 1\n2\nPM\ni=1[ui]2\n+.\nUsing Proposition 13.8 proves the proposition’s first part. Setting the\nderivative w.r.t. τ to zero gives the second part.\n\n13.5. Smoothed max operators\n331\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\nmax(u1, u2, 0)\n0\n2\n4\nValue\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\nsoftmax(u1, u2, 0)\n2\n4\nValue\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\nsparsemax(u1, u2, 0)\n0\n2\n4\nValue\nFigure 13.7: Max, softmax and sparsemax functions. The max function has non-\nsmooth contour lines (set of points {u ∈R3 : f(u) = c} for some constant c\nrepresented by dashed gray lines). So the gradient along these contour lines switch\nsuddenly at the corners of the contour lines switch. This shows that the max function\nis not differentiable everywhere, namely, non-differentiable on the set of points\n{u ∈R3 : ui = uj for any i ̸= j}. The contour lines of the softmax and sparsemax\nfunctions on the other hand are smooth illustrating that these functions are smooth\ncounterpart of the max function.\nIt can be shown (Duchi et al., 2008; Condat, 2016) that the exact\nsolution τ ⋆is obtained by\nτ ⋆= 1\nj⋆\n\n\nj⋆\nX\ni=1\nu[i] −1\n\n,\n(13.5)\nwhere j⋆is the largest j ∈[M] such that\nuj −1\nj\n\n\nj\nX\ni=1\nu[i] −1\n\n> 0,\nand where we used the notation u[1] ≥u[2] ≥· · · ≥u[M]. As an\nalternative, we can also compute τ ⋆approximately using a bisection or\nby gradient descent w.r.t. τ.\nSince\n1\n2M ≤∥π∥2\n2 ≤1\n2, we get −M−1\n2M\n≤∥π∥2\n2 ≤0 for all π ∈△M.\nFollowing Proposition 13.7, we therefore get for all u ∈RM\nmax(u) ≤sparsemax(u) ≤max(u) + M −1\n2M\n.\n\n332\nSmoothing by optimization\n13.5.5\nRecovering smoothed ReLU functions\nUsing the vector u = (u, 0) ∈R2 as input, the smoothed max operator\nrecovers the smoothed ReLU:\nmaxΩ((u, 0)) = reluΨ(u),\nwhere we defined Ψ(π) := Ω((π, 1 −π)). With Ωbeing Shannon’s\nnegentropy, we recover Ψ(π) = π log π +(1−π) log(1−π); with Ωbeing\nGini’s negentropy, we recover Ψ(π) = π(π −1), that we used to smooth\nthe ReLU.\n13.6\nRelaxed step functions (sigmoids)\nWe now turn to creating continuous relaxations of step functions. The\nbinary step function, a.k.a. Heaviside step function, is defined by\nstep(u) :=\n\n\n\n1\nif u ≥0\n0\notherwise\n.\nFrom Eq. (13.4), its variational form is\nstep(u) = arg max\nπ∈[0,1]\nu · π.\nWe can therefore define the relaxation\nstepΩ(u) := arg max\nπ∈[0,1]\nu · π −Ω(π).\nNotice that, unlike the case of the smoothed ReLU, it is a regularized\nargmax, not a regularized max. Following Remark 13.1, strongly convex\nregularization Ωensures that stepΩ(u) is a Lipschitz continuous function\nof u and is therefore, at least, differentiable almost everywhere, unlike\nstep(u).\nThe logistic function\nIf we use the regularizer Ω(π) = π log π + (1 −π) log(1 −π), we obtain\nthe closed form\nstepΩ(u) = logistic(u) :=\n1\n1 + e−u =\neu\n1 + eu .\nThis function is differentiable everywhere.\n\n13.7. Relaxed argmax operators\n333\nThe sparse sigmoid\nAs an alternative, if we use Ω(π) = π(π −1), we obtain a piecewise\nlinear sigmoid,\nstepΩ(u) = sparsesigmoid(u) :=\n\n\n\n\n\n\n\n0,\nu ≤−1\n1\n2(u + 1),\n−1 < u < 1\n1,\nu ≥1\n.\nUnlike the logistic function, it can reach the exact values 0 or 1. However,\nthe function has two kinks, where the function is non-differentiable.\nLink between smoothed ReLU functions and sigmoids\nIt turns out that the three sigmoids we presented above (step, logistic,\nsparsesigmoid) are all equal to the derivative of their corresponding\nsmoothed ReLU function:\nstep(u) = relu′(u)\nlogistic(u) = softplus′(u)\nsparsesigmoid(u) = sparseplus′(u)\nand more generally\nrelu′\nΩ(u) = stepΩ(u).\nThis is a consequence of Danskin’s theorem; see Example 11.2. We\nillustrate the smoothed ReLU functions and relaxed step functions\n(sigmoids) in Fig. 13.8.\n13.7\nRelaxed argmax operators\nWe now turn to argmax operators, which are a generalization of step\nfunctions. With a slight notation overloading, let us now define\nargmax(u) := ϕ(arg max\nj∈[M]\nuj),\nwhere ϕ(j) = onehot(j) = ej is used to embed any integer j ∈[M] into\nRM. Following the previous discussion, we have the variational form\nargmax(u) = arg max\nπ∈△M ⟨u, π⟩=\narg max\nπ∈{e1,...,eM}\n⟨u, π⟩,\n\n334\nSmoothing by optimization\n2\n1\n0\n1\n2\n0\n1\n2\nActivations\nReLU\nSoftPlus\nSparsePlus\n2\n1\n0\n1\n2\n0.0\n0.5\n1.0\nSigmoids\nHeaviside\nLogistic\nSparseSigmoid\nFigure 13.8: Smoothed ReLU functions and relaxed step functions (sigmoids).\nDifferentiating the left functions gives the right functions.\nwhere the second equality uses that a linear function is maximized at\none of the vertices of the simplex. This variational form suggests to\ndefine the relaxation\nargmaxΩ(u) := arg max\nπ∈△M ⟨u, π⟩−Ω(π).\nAgain, following Remark 13.1, argmaxΩ(u) is guaranteed to be, at least,\na differentiable almost everywhere function of u if Ωis strongly convex.\nSimilarly to sigmoids, it turns out that these mappings are equal to\nthe gradient of their corresponding smoothed max operator:\nargmaxΩ(u) = ∇maxΩ(u).\nThis is again a consequence of Danskin’s theorem.\nThe softargmax\nWhen using Shannon’s entropy Ω(π) = ⟨π, log π⟩, we obtain\nargmaxΩ(u) = softargmax(u) =\nexp(u)\nPM\nj=1 exp(uj),\nwhich is differentiable everywhere.\nProof. We know that maxΩ(u) = logsumexp(u) and that ∇maxΩ(u) =\nargmaxΩ(u). Differentiating logsumexp(u) gives softargmax(u).\n\n13.7. Relaxed argmax operators\n335\nThe sparseargmax\nWhen using Gini’s entropy Ω(π) = 1\n2⟨π, π−1⟩, which is up to a constant\nequal to 1\n2∥π∥2\n2, we obtain the sparseargmax (Martins and Astudillo,\n2016)\nargmaxΩ(u) = sparseargmax(u)\n:= arg max\nπ∈△M ⟨u, π⟩−1\n2⟨π, π −1⟩\n= arg max\nπ∈△M ⟨u, π⟩−1\n2∥π∥2\n2\n= arg min\nπ∈△M ∥u −π∥2\n2,\nwhich is nothing but the Euclidean projection onto the probability\nsimplex (see also Section 16.3). The Euclidean projection onto the\nprobability simplex △M can be computed exactly using a median-\nfinding-like algorithm. The complexity is O(M) expected time and\nO(M log M) worst-case time (Brucker, 1984; Michelot, 1986; Duchi\net al., 2008; Condat, 2016). Computing the Euclidean projection onto\nthe probability simplex boils down to computing τ ⋆given in Eq. (13.5).\nOnce we computed it, we have\nsparseargmax(u) = [u −τ ⋆]+,\nAt its name indicates, and as the above equation shows, sparseargmax\nis sparse, but it is only differentiable almost everywhere. Note that\nthe operator is originally known as sparsemax (Martins and Astudillo,\n2016), but this is a misnomer, as it is really an approximation of the\nargmax. Therefore, in analogy with the softargmax, we use the name\nsparseargmax. We compare the argmax, softmax and sparseargmax in\nFig. 13.9 and Fig. 13.10.\nRelaxed argmin operators\nThe argmin operator can be expressed in terms of the argmax operator,\narg min(u) = arg max(−u).\n\n336\nSmoothing by optimization\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n= argmax(u1, u2, 0)\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n= softargmax(u1u2, 0)\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n1\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n2\n2.5\n0.0\n2.5\nu1\n2.5\n0.0\n2.5\nu2\n3\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n0.0\n0.5\n1.0\nValue\n= sparseargmax(u1u2, 0)\nFigure 13.9: Values of argmax(u), softargmax(u), and sparseargmax(u) for u =\n(u1, u2, 0), when varying u1 and u2. The argmax is a piecewise constant, discontinuous\nfunction. The softargmax is a continuous and differentiable everywhere function, but\nit is always strictly positive and therefore dense. The sparseargmax is a continuous\nfunction and its output can be sparse, but it is only a differentiable almost everywhere\nfunction.\n\n13.8. Summary\n337\nFigure 13.10: Same as Fig. 13.9 but using a 3D plot.\nGiven a relaxed argmax operator argmaxΩ, we can therefore define a\nrelaxed argmin by\nargminΩ(u) := argmaxΩ(−u).\nWe then have for all u ∈RM\nargminΩ(u) = ∇minΩ(u).\n13.8\nSummary\n• When a function f is non-differentiable (or worse, discontinuous),\na reasonable approach is to replace it by its smooth approximation\n(or continuous relaxation).\n• The first approach we reviewed is infimal convolution between f\nand primal regularization R. The Moreau envelope is a special\ncase, obtained by using R = 1\n2∥· ∥2\n2.\n• The second approach we reviewed is regularizing the convex con-\njugate f∗of f with some dual regularization Ω. We saw that the\nprimal and dual approaches are equivalent when R = Ω∗.\n• The Legendre-Fenchel transformation, a.k.a. convex conjugate,\ncan be seen as a dual representation of a function: instead of\nrepresenting f by its graph (u, f(u)) for u ∈dom(f), we can\nrepresent it by the set of tangents with slope v and intercept\n\n338\nSmoothing by optimization\n−f∗(v) for v ∈dom(f∗) As its name indicates, it is convex, even\nif the original function is not.\n• We showed how to apply smoothing techniques to create smoothed\nReLU functions and smoothed max operators. We also showed that\ntaking their gradients allowed us to obtain generalized sigmoid\nfunctions and argmax operators.\n\n14\nSmoothing by integration\nIn this chapter, we review smoothing techniques based on convolution.\n14.1\nConvolution\n14.1.1\nConvolution operators\nThe convolution between two functions f and g produces another\nfunction, denoted f ∗g. It is defined by\n(f ∗g)(µ) :=\nZ ∞\n−∞\nf(u)g(µ −u) du,\n(14.1)\nassuming that the integral is well defined. It is therefore the integral of\nthe product of f and g after g is reflected about the y-axis and shifted.\nIt can be seen as a generalization of the moving average. Using the\nchange of variable z := µ −u, we can also write\n(f ∗g)(µ) =\nZ ∞\n−∞\nf(µ −z)g(z) dz = (g ∗f)(µ).\n(14.2)\nThe convolution operator is therefore commutative.\n339\n\n340\nSmoothing by integration\n14.1.2\nConvolution with a kernel\nThe convolution is frequently used together with a kernel κ to create a\nsmooth approximation f ∗κ of f. The most frequently used kernel is\nthe Gaussian kernel with width σ, defined by\nκσ(z) :=\n1\n√\n2πσe−1\n2 ( z\nσ )2.\nThis is the probability density function (PDF) of the normal distribution\nwith zero mean and variance σ2. The term\n1\n√\n2πσ is a normalization\nconstant, ensuring that the kernel sums to 1 for all σ. We therefore say\nthat κσ is a normalized kernel.\nAveraging perspective\nApplying the definition of the convolution in Eq. (14.1), we obtain\n(f ∗κσ)(µ) :=\n1\n√\n2πσ\nZ ∞\n−∞\nf(u)e−1\n2 ( µ−u\nσ\n)2du\n= EU∼pµ,σ[f(U)],\nwhere\npµ,σ(u) := κσ(µ −u) = κσ(z) =\n1\n√\n2πσe−1\n2 ( µ−u\nσ\n)2\nis the PDF of the Gaussian distribution with mean µ and variance\nσ2. Therefore, we can see f ∗κσ as the expectation of f(u) over a\nGaussian centered around µ. This property is true for all translation-\ninvariant kernels, that correspond to a location-scale family distribution\n(e.g., the Laplace distribution). The convolution therefore performs an\naveraging with all points, with points nearby µ given more weight by\nthe distribution. The parameter σ controls the importance we want to\ngive to farther points. We call this viewpoint averaging, as we replace\nf(u) by E[f(U)].\nPerturbation perspective\nConversely, using the alternative definition of the convolution operator\nin Eq. (14.2), which stems from the commutativity of the convolution,\n\n14.1. Convolution\n341\nwe have\n(f ∗κσ)(µ) :=\nZ ∞\n−∞\nf(µ −z)e−1\n2 ( z\nσ )2dz\n= EZ∼p0,σ[f(µ −Z)]\n= EZ∼p0,σ[f(µ + Z)],\nwhere, in the third line, we used that p0,σ is sign invariant, i.e., p0,σ(z) =\np0,σ(−z). This viewpoint shows that smoothing by convolution with\na Gaussian kernel can also be seen as injecting Gaussian noise or\nperturbations to the function’s input.\nLimit case\nWhen σ →0, the kernel κσ converges to a Dirac delta function,\nlim\nσ→0 κσ(z) = δ(z).\nSince the Dirac delta is the multiplicative identity of the convolution\nalgebra (this is also known as the sifting property), when σ →0, f ∗κσ\nconverges to f, i.e.,\nlim\nσ→0(f ∗κσ)(u) = f(u).\n14.1.3\nDiscrete convolution\nMany times, we work with functions whose convolution does not have\nan analytical form. In these cases, we can use a discrete convolution on\na grid of values. For two functions f and g defined over Z, the discrete\nconvolution is defined by\n(f ∗g)[i] :=\n∞\nX\nj=−∞\nf[j]g[i −j].\nAs for its continuous counterpart, the discrete convolution is commuta-\ntive, namely,\n(f ∗g)[i] =\n∞\nX\nj=−∞\nf[i −j]g[j] = (g ∗f)[i].\n\n342\nSmoothing by integration\n3\n2\n1\n0\n1\n2\n3\n0\n2\n4\n6\n8\n10\n= 0.25\n= 0.5\n= 1.0\nFigure 14.1: Smoothing of the signal f[t] := t2 + 0.3 sin(6πt) with a sampled and\nrenormalized Gaussian kernel.\nWhen g has finite support over the set S := {−M, −M+1, . . . , 0, . . . , M−\n1, M}, meaning that g[i] = 0 for all i ̸∈S, a finite summation may be\nused instead, i.e.,\n(f ∗g)[i] =\nM\nX\nj=−M\nf[i −j]g[j] = (g ∗f)[i].\nIn practice, convolution between a discrete signal f : Z →R and a\ncontinuous kernel κ: R →R is implemented by discretizing the kernel.\nOne of the simplest approaches consists in sampling points on an interval,\nevaluating the kernel at these points and renormalizing the obtained\nvalues, so that the sampled kernel sums to 1. This is illustrated with\nthe Gaussian kernel in Fig. 14.1. Since the Gaussian kernel decays\nexponentially fast, we can choose a small interval around 0. For a survey\nof other possible discretizations of the Gaussian kernel, see Getreuer\n(2013).\n\n14.1. Convolution\n343\n14.1.4\nDifferentiation\nRemarkably, provided that the two functions are integrable with inte-\ngrable derivatives, the derivative of the convolution satisfies\n(f ∗g)′ = (f′ ∗g) = (f ∗g′),\nwhich simply stems from switching derivative and integral in the defini-\ntion of the convolution. Moreover, we have the following proposition.\nProposition 14.1 (Differentiability of the convolution). If g is n-times\ndifferentiable with compact support over R and f is locally inte-\ngrable over R, then f ∗g is n-times differentiable over R.\n14.1.5\nMultidimensional convolution\nSo far, we studied the convolution of one-dimensional functions. The\ndefinition can be naturally extended to multidimensional functions\nf : RM →R and g: RM →R as\n(f ∗g)(µ) :=\nZ\nRM f(u)g(µ −u) du,\nassuming again that the integral exists. Typically, a Gaussian kernel\nwith diagonal covariance matrix is used\nκσ(z) :=\nM\nY\nj=1\n1\n√\n2πσj\ne\n−1\n2 (\nzj\nσj )2\n=\n1\n√\n2π\nMσM e−1\n2\n∥z∥2\n2\nσ2 ,\n(14.3)\nwhere, in the second equality, we assumed σ1 = · · · = σM. In an image\nprocessing context, where M = 2, it is approximated using a discrete\nconvolution and it is called a Gaussian blur.\n14.1.6\nLink between convolution and infimal convolution\nThe infimal convolution we studied in Section 13.1 takes the form\n(f□g)(µ) :=\ninf\nu∈RM f(u) + g(µ −u).\nIn comparison, the classical convolution takes the form\n(F ∗G)(µ) :=\nZ\nRM F(u)G(µ −u) du.\n\n344\nSmoothing by integration\nThe two forms of convolution are clearly related. Infimal convolution\nperforms an infimum and uses the sum of f and g: it uses a min-\nplus algebra. Classical convolution performs an integral and uses the\nproduct of F and G: it uses a sum-product algebra.\n14.1.7\nThe soft infimal convolution\nThe link between the infimal convolution and the classical convolution\ncan be further elucidated if we replace the infimum with a soft minimum\nin the definition of the infimal convolution.\nDefinition 14.1 (Soft infimal convolution). The soft infimal convo-\nlution between f : RM →R and g: RM →R is\n(f□εg)(µ) := softminε\nu∈RM\nf(u) + g(µ −u),\nwhere we defined the soft minimum (assuming that it exists) over\nS of any function h: S →R as\nsoftminε\nu∈S\nh(u) := −ε log\nZ\nS\nexp (−h(u)/ε) du.\nWe recover the infimal convolution as ε →0.\nComputation using a convolution\nWe now show that we can rewrite the soft infimal convolution using\na classical convolution. Indeed, by using the exponential change of\nvariable (sometimes referred to as Cole-Hopf transformation in a\npartial differential equation context)\nCε{f}(u) := exp(−f(u)/ε)\nC−1\nε {F}(v) = −ε log F(v),\nwe can define each function in the exponential domain,\nFε := Cε{f}\nGε := Cε{g}\nHε := Cε{hε}.\n\n14.1. Convolution\n345\nIt is easy to check that we then have\nHε(µ) = (Fε ∗Gε)(µ).\nBack to log domain, we obtain\nhε(µ) = C−1\nε {Hε}(µ).\nCombining the transformation and its inverse, we can write\nhε(µ) = C−1\nε {Cε{f} ∗Cε{g}}(µ).\nWhat we have shown is that, after an exponential change of variable,\nthe soft infimal convolution can be reduced to the computation of a\nconvolution. This is useful as a discrete convolution on a grid of size n\ncan be computed in O(n log n).\n14.1.8\nThe soft Moreau envelope\nWe saw in Section 13.1.2 that the infimal convolution between f and\nR(z) = 1\n2z2 is the Moreau envelope,\nMf(µ) := (f□R)(µ) =\ninf\nu∈RM f(u) + 1\n2∥µ −u∥2\n2.\nReplacing the infimal convolution with a soft infimal convolution, we\ncan define the “soft” Moreau envelope,\nMε\nf(µ) := (f□εR)(µ) = softminε\nu∈RM\nf(u) + 1\n2∥µ −u∥2\n2.\nWe emphasize that this is operation is not the same as the convolution\nof f with a Gaussian kernel. Indeed, we have\nMε\nf(µ) = −ε log\nZ\nRM exp\n\u0012\n(−f(u) −1\n2∥µ −u∥2\n2)/ε\n\u0013\ndu.\nwhile\n(f ∗κσ)(µ) :=\nZ\nRM f(u)κσ(µ −u) du,\nwhere κσ is for instance defined in Eq. (14.3).\nWe saw that the Moreau envelope is a smooth function. One may\ntherefore ask what do we gain from using a soft Moreau envelope. The\nbenefit can be computational, as the latter can be approximated using\na discrete convolution.\n\n346\nSmoothing by integration\n14.2\nFourier and Laplace transforms\nLet us define the Fourier transform of f by\nF(s) := F{f}(s) :=\nZ ∞\n−∞\nf(t)e−i2πst dt,\ns ∈R.\nNote that F{f} is a function transformation: it transforms f into\nanother function F.\n14.2.1\nConvolution theorem\nNow, consider the convolution\nh(t) := (f ∗g)(t).\nIf we define the three transformations\nF := F{f}, G := F{g}, H := F{h},\nthe convolution theorem states that\nH(s) = F{h}(s) = F(s) · G(s),\ns ∈R.\nWritten differently, we have\nF{f ∗g} = F{f} · F{g}.\nIn words, in the Fourier domain, the convolution operation becomes a\nmultiplication. Conversely,\nh(t) = (f ∗g)(t) = F−1{F · G}(t),\nt ∈R.\nThe convolution theorem also holds if we replace the Fourier transform\nwith the Laplace transform or with the two-sided (bilateral) Laplace\ntransform.\n14.2.2\nLink between Fourier and Legendre transforms\nIn Section 13.2, we studied another function transformation: the convex\nconjugate, also known as Legendre-Fenchel transform. We recap the\nanalogies between these transforms in Table 14.1. In particular, the\ncounterpart of\nF{f ∗g} = F{f} · F{g}.\n\n14.2. Fourier and Laplace transforms\n347\nTable 14.1: Analogy between Fourier and Legendre transforms. See Proposition 13.3\nfor more conjugate calculus rules.\nFourier F{f}\nLegendre f∗\nSemiring\n(+, ·)\n(min, +)\nScaling (a > 0)\nf(t) = g(t/a)\nf(t) = ag(t/a)\nF{f}(t) = aF{g}(as)\nf∗(s) = ag∗(s)\nTranslation\nf(t) = g(t −t0)\nf(t) = g(t −t0)\nF{f}(s) = e−i2πt0sF{g}(s)\nf∗(s) = g∗(s) + t0\nConvolution\nh = f ∗g\nh = f□g\nF{h} = F{f} · F{g}\nh∗= f∗+ g∗\nGaussian / quadratic\nf(t) = e−at2\nf(t) = a\n2t2\nF{f}(s) =\nq\nπ\nae−π2s2/a\nf∗(s) =\n1\n2as2\nSmoothing\nf ∗κσ\nf□1\n2ε∥· ∥2\n2\nfor the infimal convolution is\n(f□g)∗= f∗+ g∗.\nIn words, the Legendre-Fenchel transform is to the infimal convolution\nwhat the Fourier transform is to the convolution.\n14.2.3\nThe soft Legendre-Fenchel transform\nWe saw in Section 13.2 that the Legendre-Fenchel transform (convex\nconjugate) of a function f : RM →R is\nf∗(v) := max\nu∈RM ⟨u, v⟩−f(u).\nIf necessary, we can support constraints by including an indicator\nfunction in the definition of f. The conjugate can be smoothed out using\na log-sum-exp, which plays the role of a soft maximum (Section 13.5).\n\n348\nSmoothing by integration\nDefinition 14.2 (Soft convex conjugate).\nf∗\nε (v) := softmaxε\nu∈RM\n⟨u, v⟩−f(u),\nwhere we defined the soft maximum (assuming that it exists) over\nS of any function g: S →R as\nsoftmaxε\nu∈S\ng(u) := ε log\nZ\nS\nexp (g(u)/ε) du.\nIn the limit ε →0, we recover the convex conjugate.\nComputation using a convolution\nWe now show that this smoothed conjugate can be rewritten using a\nconvolution if we apply a bijective transformation to f.\nProposition 14.2 (Smoothed convex conjugate as convolution). The\nsmoothed conjugate can be rewritten as\nf∗\nε (v) = Q−1\nε\n\u001a\n1\nQε{f} ∗Gε\n\u001b\n(v)\nwhere\nGε := Cε\n\u001a1\n2∥· ∥2\n2\n\u001b\n= exp\n \n−1\n2\n∥· ∥2\n2\nε\n!\nQε{f} := Cε\n\u001a\nf(·) −1\n2∥· ∥2\n2\n\u001b\n= exp\n\u0012 1\n2ε∥· ∥2\n2 −1\nεf(·)\n\u0013\nQ−1\nε {F} := 1\n2∥· ∥2\n2 −ε log(F(·)).\nThis insight was tweeted by Gabriel Peyré in April 2020.\n\n14.2. Fourier and Laplace transforms\n349\n3\n2\n1\n0\n1\n2\n3\n0\n2\n4\n6\n8\nFigure 14.2: Applying the smoothed conjugate twice gives a smoothed biconjugate\n(convex envelope) of the function.\nProof.\nfε(v) := ε log\nZ\nexp\n\u00121\nε⟨u, v⟩−1\nεf(u))\n\u0013\ndu\n= ε log\nZ\nexp\n\u0012\n−1\n2ε∥u −v∥2\n2 + 1\n2ε∥u∥2\n2 + 1\n2ε∥v∥2\n2 −1\nεf(u))\n\u0013\ndu\n= ε log\nZ\nexp\n\u0012\n−1\n2ε∥u −v∥2\n2 + 1\n2ε∥u∥2\n2 −1\nεf(u))\n\u0013\ndu + 1\n2∥v∥2\n2\n= ε log\nZ\nGε(v −u)Qε{f}(u)du + 1\n2∥v∥2\n2\n= ε log(Qε{f} ∗Gε)(v) + 1\n2∥v∥2\n= 1\n2∥v∥2 −ε log\n\u0012\n1\nQε{f} ∗Gε\n\u0013\n(v)\n= Q−1\nε\n\u001a\n1\nQε{f} ∗Gε\n\u001b\n(v)\nWhat did we gain from this viewpoint? The convex conjugate can\noften be difficult to compute in closed form. If we replace RM with a dis-\ncrete set S (i.e., a grid), we can then approximate the smoothed convex\n\n350\nSmoothing by integration\nconjugate in O(n log n), where n = |S|, using a discrete convolution,\n(Qε{f} ∗Gε)(v) ≈\nX\nu∈S\nGε(v −u)Qε{f}(u)\n= Kq,\nwhere K is the n×n Gaussian kernel matrix whose entries correspond to\nexp(−1\n2ε∥u−u′∥2\n2) for u, u′ ∈S and q is the n-dimensional vector whose\nentries correspond to exp(1\nε(1\n2∥v∥2\n2 −f(u)) for u ∈S. This provides\na GPU-friendly alternative to the fast Legendre transform algorithm,\ndiscussed in Section 13.2. Of course, due to the curse of dimensionality,\nthe technique is limited to functions defined on low-dimensional sets.\nWe illustrate in Fig. 14.2 the application of the technique to computing\nan approximate biconjugate (convex envelope) of a function.\nRemark 14.1 (Link with the two-sided Laplace transform). For\none-dimensional functions, instead of using a convolution, we can\nalso write the soft convex conjugate as\nf∗\nε (v) = ε log\nZ ∞\n−∞\nexp\n\u00121\nε [uv −f(u)])\n\u0013\ndu\n= ε log B\nn\ne−f\nε\no \u0012\n−v\nε\n\u0013\n= −C−1\nε\n{B {Cε{f}}}\n\u0012\n−v\nε\n\u0013\nwhere we defined the two-sided (bilateral) Laplace transform\nB{g}(v) :=\nZ ∞\n−∞\ne−uvg(u)du\nand where we assumed that the integral exists.\n14.3\nExamples\nIn this section, we review practical examples for which the convolution\nwith a Gaussian kernel enjoys an analytical solution.\n14.3.1\nSmoothed step function\n\n14.3. Examples\n351\nExample 14.1 (Smoothed Heaviside). The Heaviside step function\nis defined by\nstep(u) := h(u) :=\n\n\n\n1\nif u ≥0\n0\notherwise\n.\nWith the Gaussian kernel, we therefore obtain\n(h ∗κσ)(µ) =\nZ µ\n−∞\nκσ(z)h(µ −z)dz +\nZ ∞\nµ\nκσ(z)h(µ −z)dz\n=\nZ µ\n−∞\nκσ(z)dz\n= Φσ(µ)\n= 1\n2\n\u0014\n1 + erf\n\u0012 µ\n√\n2σ\n\u0013\u0015\n,\nwhere Φσ(µ) is the CDF of the Gaussian distribution with zero\nmean and variance σ2, and where we used the error function\nerf(z) :=\n2\n√π\nZ z\n0\ne−t2 dt,\nthat we both already encountered in Chapter 3. Although there is\nno closed form for the error function, it is commonly available in\nnumerical analysis software, such as SciPy.\n14.3.2\nSmoothed ReLU function\nExample 14.2 (Smoothed ReLU). The ReLU is defined by\nr(u) :=\n\n\n\nu\nif u ≥0\n0\notherwise\n= u · h(u).\n\n352\nSmoothing by integration\n3\n2\n1\n0\n1\n2\n3\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n= 0.5\n= 1.0\n= 2.0\n3\n2\n1\n0\n1\n2\n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 14.3: Smoothing of the ReLU and Heaviside functions by convolution with\na Gaussian kernel, for three values of the width σ.\nSimilarly to the previous example, we obtain\n(r ∗κσ)(µ) =\nZ µ\n−∞\nκσ(z)r(µ −z)dz\n=\nZ µ\n−∞\nκσ(z)(µ −z)dz\n= µ\nZ µ\n−∞\nκσ(z)z −\nZ µ\n−∞\nκσ(z)zdz\n= µΦσ(µ) + σ2κσ(µ).\nIn the second integral, setting a :=\n1\n2σ2 , we used\nZ\nze−az2dz = −1\n2a\nZ\netdt = −1\n2aet + C = −1\n2ae−az2 + C\nand t := −az2 ⇒zdz = −1\n2adt.\nTo illustrate differentiation of the convolution, we show how to\ndifferentiate the smoothed ReLu.\nExample 14.3 (Differentiating the smoothed ReLU). Differentiating\nthe smoothed ReLU from Example 14.2, we obtain\n(r ∗κσ)′ = (r′ ∗κσ) = h ∗κσ = Φσ.\nTherefore, unsurprisingly, the derivative of the smoothed ReLU is\nthe smoothed Heaviside step function. Differentiating once again,\n\n14.4. Perturbation of blackbox functions\n353\nwe obtain,\n(r ∗κσ)′′ = (h ∗κσ)′ = (h′ ∗κσ) = δ ∗κσ = κσ,\nwhere the derivative h′ is well-defined almost everywhere. We can\narrive at the same result by using that h ∗κσ = Φσ and Φ′\nσ = κσ,\nsince Φσ and κσ are the CDF and PDF of the Gaussian with zero\nmean and σ2 variance.\n14.4\nPerturbation of blackbox functions\nIn this section, we review how to approximately compute a convolution\nwith a kernel and its gradient using Monte-Carlo estimation.\n14.4.1\nExpectation in a location-scale family\nA rather intuitive approach to smooth a function f : RM →R is to\naverage its values on an input µ, perturbed by some additive noise\nZ ∼p, for some noise distribution p. This defines the surrogate\nfσ(µ) := EZ∼p[f(µ + σZ)].\nThe parameter σ controls the perturbation strength: as σ →0, we\nnaturally recover f. An equivalent viewpoint is obtained by defining\nthe transformation (change of variables)\nU := µ + σZ.\nWe then have\nU ∼pµ,σ,\nwhere pµ,σ is the location-family distribution generated by the noise\ndistribution p. It is the pushforward distribution of Z through the\ntransformation (see Section 12.4.4). In this notation, the initial noise\ndistribution p is then simply p = p0,1. The perturbed function can then\nbe expressed from these two perspectives as\nfσ(µ) = EZ∼p0,1[f(µ + σ · Z)]\n= EU∼pµ,σ[f(U)].\n(14.4)\n\n354\nSmoothing by integration\nWriting the expectation as the integral of a p.d.f, we naturally recover\nthe smoothing by convolution presented earlier,\nfσ(µ) =\nZ\nf(µ + σz)p0,1(z)dz\n= f ∗κσ(µ),\nwhere we defined the kernel\nκσ(z) := p0,σ(−z).\nIn the sequel, we assume that the noise distribution decomposes as\np0,1(z) := exp(−ν(z))/C,\nwhere ν(z) is the log-density of the noise distribution and C is a\nnormalization constant. For instance, the Gaussian distribution with\ndiagonal covariance matrix and the corresponding Gaussian kernel\nare obtained with ν(z) = 1\n2∥z∥2\n2 and C =\n√\n2π\nM.\nApproximation by Monte-Carlo estimation\nInstead of approximating the integral above (continuous convolution)\nwith a discrete convolution on a grid, as we did in Section 14.1.3,\nthe expectation perspective suggests that we can estimate fσ(µ) by\nMonte-Carlo estimation: we simply draw samples from the distribution,\nevaluate the function at these samples and average. Beyond mere Monte-\nCarlo estimation, more elaborate approximation schemes are studied in\n(Chaudhuri and Solar-Lezama, 2010).\n14.4.2\nGradient estimation by reparametrization\nIf f is differentiable with finite expectation and Leibniz’s integration\nrule holds, we have\n∇fσ(µ) = EZ∼p0,1[∇f(µ + σ · Z)].\n(14.5)\nIf f is differentiable almost everywhere, the formula may still hold, see\nSection 12.1. For example, if f is the ReLU, then ∇f is the Heaviside\nstep function, and we obtain the correct gradient of fσ using the formula\n\n14.4. Perturbation of blackbox functions\n355\nabove; see Example 14.1. However, if f is not absolutely continuous,\nthe formula may not hold. For example, if f is the Heaviside function,\nthe right-hand side of (14.5) is 0 which does not match the gradient of\nfσ; see again Example 14.1.\nFrom the second expression of fσ in (14.4), we can see the formula\nof the gradient in (14.5) as a reparametrization trick U = µ + σZ; see\nSection 12.4. Namely, we have\n∇fσ(µ) = ∇µEU∼pµ,σ[f(U)]\n= ∇µEZ∼p0,1[f(µ + σ · Z)]\n= EZ∼p0,1[∇µf(µ + σ · Z)]\n= EZ∼p0,1[∇f(µ + σ · Z)].\n(14.6)\n14.4.3\nGradient estimation by SFE, Stein’s lemma\nIn some cases, we may not have access to ∇f or f may not be absolutely\ncontinuous and therefore the formula in (14.5) cannot apply. For these\ncases, we can use the score function estimator (SFE) from Section 12.3.\nHere, for fσ(µ) = EU∼pµ,σ[f(U)], we obtain\n∇fσ(µ) = EU∼pµ,σ[f(U)∇µ log pµ,σ(U)].\nSince the PDF can be written as\npµ,σ(u) = 1\nσp0,1((u −µ)/σ),\nwhere\np0,1(z) := exp(−ν(z))/C,\nwe obtain\n∇µ log pµ,σ(u) = ∇ν((u −µ)/σ)/σ.\nTo summarize, we have shown that\n∇fσ(µ) = EU∼pµ,σ[f(U)∇ν((U −µ)/σ)/σ]\n= EZ∼p0,1[f(µ + σ · Z)∇ν(Z)/σ],\n(14.7)\nwhere we used the change of variable Z = (U−µ)/σ. The same technique\ncan also be used if we want to estimate the gradient w.r.t. θ = (µ, σ) or\n\n356\nSmoothing by integration\nif we want to estimate the Jacobian of the expectation of a vector-valued\nfunction.\nIn the particular case of Gaussian noise, since ∇ν(z) = z, we obtain\n∇fσ(µ) = EZ∼p0,1[f(µ + σ · Z)Z/σ].\nThis is known as Stein’s lemma. It should be noted that the above is\nan unbiased estimator of the gradient of the smoothed function fσ, but\na biased estimator of the gradient of the original function f (assuming\nthat it exists). However, smoothing is usually a good thing, as it can\naccelerate the convergence of gradient-based algorithms. Computing\nthe gradient of perturbed general programs is studied in detail in\n(Kreikemeyer and Andelfinger, 2023).\n14.4.4\nLink between reparametrization and SFE\nUsing the log-derivative identity, we have for any distribution with\ndifferentiable density p\nEZ∼p[h(Z)∇log p(Z)] =\nZ\nRM h(z)\n\u0012∇p(z)\np(z)\n\u0013\np(z)dz\n=\nZ\nRM h(z)∇p(z)dz.\nUsing integration by parts and assuming that h(z)p(z) goes to zero\nwhen ∥z∥→∞, we have\nZ\nRM h(z)∇p(z)dz = −\nZ\nRM p(z)∇h(z)dz.\nWe have therefore the identity\nEZ∼p[h(Z)∇log p(Z)] = −EZ∼p[∇h(Z)].\nImportantly, contrary to the SFE estimator from Section 12.3, this\nidentity uses gradients with respect to z, not with respect to the\nparameters of the distribution. Nevertheless, using the reparametrization\n\n14.4. Perturbation of blackbox functions\n357\nh(z) := f(µ + σ · z), we have ∇h(z) = ∇f(µ + σ · z) · σ so that\n∇fσ(µ) = ∇µEU∼pµ,σ[f(U)]\n= EZ∼p0,1[∇f(µ + σ · Z)]\n(reparametrization trick)\n= −EZ∼p0,1[h(Z)∇log p(Z)/σ]\n= EZ∼p0,1[h(Z)∇ν(Z)/σ]\n= EZ∼p0,1[f(µ + σ · Z)∇ν(Z)/σ]\n(score function estimator)\nEssentially, integration by parts allowed us to convert the reparametriza-\ntion trick estimator into the SFE estimator. For more applications of\nintegration by parts in machine learning, see Francis Bach’s excellent\nblog post.\n14.4.5\nVariance reduction and evolution strategies\nAs discussed in Chapter 12, the SFE suffers from high variance. We now\napply variance reduction techniques to it. To do so, we assume that\n∇ν(Z) has zero mean for Z ∼p0,1. This assumption for example holds\nfor Gaussian or centered Gumbel noise distributions. This assumption\nimplies that\nEZ∼p0,1[f(µ)∇ν(Z)/σ] = f(µ)EZ∼p0,1[∇ν(Z)/σ] = 0\nand therefore\n∇fσ(µ) = EZ∼p0,1 [(f(µ + σ · Z) −f(µ))∇ν(Z)/σ] .\n(14.8)\nThis is an example of control variate discussed in Section 12.3. This\ncan be interpreted as using a finite difference for computing a direc-\ntional derivative in the random direction Z (see “limit case” below).\nInspired by a central finite difference, we can also use\n∇fσ(µ) = EZ∼p0,1 [(f(µ + σ · Z) −f(µ −σ · Z))∇ν(Z)/(2σ)] . (14.9)\nThese estimators have been used as part of blackbox (zero-order) op-\ntimization algorithms, such as evolution strategies (Salimans et\nal., 2017) or random gradient-free optimization (Nesterov and\nSpokoiny, 2017). For quadratic functions, it is easy to show that the\nsecond estimator achieves lower variance (Recht and Frostig, 2017). The\n\n358\nSmoothing by integration\n100\n101\n102\n103\n104\n105\n106\n107\nNumber of samples\n10\n1\n100\n101\n102\n103\nGradient error\nSFE\nSFE with forward difference\nSFE with central difference\nFigure 14.4: Comparison of the score function estimator (SFE) with or without\nvariance reduction for blackbox gradient estimation. We show the error |∇f(µ) −\n∇fσ(µ)| for f(u) := u3 and fσ(µ) := E[f(µ + σZ)], where Z ∼Normal(0, 1) and\nσ := 0.1. To estimate ∇fσ(µ), we compare three estimators: the vanilla SFE Eq. (14.7),\nthe SFE estimator with forward difference (variance reduced) Eq. (14.8), and the SFE\nestimator with central difference (varianced reduced) Eq. (14.9). In all three cases,\nwe approximate the expectation by Monte-Carlo estimation using some number of\nsamples. The variance-reduced estimators not only achieve smaller error, they are\nalso more numerically stable as σ gets smaller.\nidea of sampling both Z and −Z simultaneously is called antithetic\n(Geweke, 1988) or mirrored sampling (Brockhoff et al., 2010). Evolution\nstrategies have also been used to obtain unbiased gradient estimators\nof partially unrolled computational graphs (Vicol et al., 2021). We\nempirically compare the SFE with or without variance reduction for\nblackbox gradient estimation in Fig. 14.4.\n14.4.6\nZero-temperature limit\nWe now discuss the limit case σ →0. That is, we assume that we do\nnot want to perform smoothing and that ∇f exists. We recall that the\ndirectional derivative of f at µ in the direction z is\n∂f(µ)[z] = ⟨∇f(µ), z⟩\n= lim\nσ→0 [f(µ + σ · z) −f(µ)] /σ.\n\n14.5. Gumbel tricks\n359\nWhen σ →0 and Z follows the standard Gaussian distribution, meaning\nthat ∇ν(z) = z, Eq. (14.8) therefore becomes\n∇fσ(µ) = EZ∼p0,1 [∂f(µ)[Z]∇ν(Z)]\n= EZ∼p0,1 [∂f(µ)[Z]Z]\n= EZ∼p0,1 [⟨∇f(µ), Z⟩Z]\n= EZ∼p0,1\nh\n∇f(µ)ZZ⊤i\n= ∇f(µ).\nThis should not be too surprising, as we already know from the convo-\nlution perspective that fσ(µ) = (f ∗κσ)(µ) →f(µ) when σ →0. This\nrecovers the randomized forward-mode estimator already presented in\nSection 8.7.\n14.5\nGumbel tricks\n14.5.1\nThe Gumbel distribution\nThe Gumbel distribution is a distribution frequently used in extreme\nvalue theory. It arises naturally as the distribution of the logarithm of a\nnegative exponential family. We consider its centered version with PDF\np(z) = exp(−ν(z)),\nwhere\nν(z) = z + γ + exp(−(z + γ)),\nand where γ ≈0.577 is Euler’s constant. We extend it to a multivariate\ndistribution by taking M independent centered Gumbel distributions\nZ = (Z1, . . . , Zm) with associated location-scale family\nµ + σZ ∼pµ,σ,\nand p0,1 = p. As EZ∼p0,1[∇ν(Z)] = 0, we can use the Gumbel noise as\nan alternative to the Gaussian noise used in Section 14.4. Thankfully,\nin particular cases, we can compute closed-form expressions of the\nexpectation.\n\n360\nSmoothing by integration\nRemark 14.2 (Sampling Gumbel noise). If U ∼Uniform(0, 1), then\nZ := −log(−log(U)) satisfies Z ∼Gumbel(0, 1).\nRemark 14.3 (Link between Gumbel and exponential distribution).\nA random variable Z is distributed as a pµ,1 Gumbel distribution\nif and only if exp(−Z) is distributed as an exponential distribution\nExp(exp(µ −γ)). To see this, one can simply compute the CDF\nof exp(−Z) and recognize the CDF of Exp(exp(µ −γ)). There-\nfore, when comparing Gumbel distributions we can use standard\nproperties of the exponential distribution.\n14.5.2\nPerturbed comparison\nTo start with, the Gumbel distribution can be used to smooth a binary\ncomparison like the greater than or equal operators. Recall that the\nlatter is defined for any µ1, µ2 ∈R as\ngt(µ1, µ2) :=\n\n\n\n1\nif µ1 ≥µ2\n0\nif µ1 < µ2\n= step(µ1 −µ2),\nwhere step is the Heaviside function. As shown below, by perturbing each\nvariable with Gumbel noise, we recover logistic(a −b) = 1/(1 + e−(a−b))\nas an approximation of step(a −b).\nProposition 14.3 (Gumbel trick for binary variables). Let\nZ1, Z2 ∼Gumbel(0, 1) be independent random variables. The differ-\nence of their location-scale transform (Section 12.4.1) is distributed\naccording to a logistic distribution (Remark 3.1), i.e.,\nµ1 + σZ1 −(µ2 + σZ2) ∼Logistic(µ1 −µ2, σ),\nfor µ1, µ2 ∈R and σ > 0. In particular, we have\nE[gt(µ1 + σZ1, µ2 + σZ2)] =\n1\n1 + e−(µ1−µ2)/σ .\n\n14.5. Gumbel tricks\n361\nProof. We first derive the CDF of µ1 + σZ1 −(µ2 + σZ2) as\nP(µ1 + σZ1 −(µ2 + σZ2) ≤t) = P (µ1/σ + Z1 ≤(µ2 + t)/σ + Z2)\n= P\n\u0010\ne−(µ1/σ+Z1) ≥e−((µ2+t)/σ+Z2)\u0011\n.\nBy Remark 14.3, e−(µ1/σ+Z1) ∼Exp(exp(µ1/σ −γ)), and similarly for\ne−(µ2+t)/σ+Z2. Now one easily shows that if U ∼Exp(u), V ∼Exp(v)\nindependent, then P(U ≤V ) = u/(u + v). Hence, we get\nP(µ1 + σZ1 −(µ2 + σZ2) ≤t) =\ne(µ2+t)/σ−γ\ne(µ2+t)/σ−γ + eµ1/σ−γ\n=\n1\n1 + e−(t−(µ1−µ2))/σ .\nWe recognize the CDF of the logistic distribution with mean µ1 −µ2\nand scale σ, denoted Logistic(µ1 −µ2, σ). For the last claim, we simply\nhave that\nE[gt(µ1 + σZ1, µ2 + σZ2)] = E [step(µ1 + σZ1 −(µ2 + σZ2)]\n= P(µ1 + σZ1 −(µ2 + σZ2) ≥0)\n=\n1\n1 + e−(µ1−µ2)/σ .\n14.5.3\nPerturbed argmax\nSuppose we want to smooth\ny(u) :=\narg max\ny∈{e1,...,eM}\n⟨y, u⟩= ϕ(i(u)),\nwhere\ni(u) := arg max\ni∈[M]\nui\nϕ(i) := ei\nwith ϕ(i) is the one-hot encoding of i ∈[M]. It turns out that the\nfunction y(u) perturbed using Gumbel noise enjoys a closed form\nexpectation, which is nothing else than the softargmax.\n\n362\nSmoothing by integration\nProposition 14.4 (Gumbel trick for categorical variables). Given M\nindependent Gumbel random variables Z ∼p0,1, define\nY := i(µ + σ · Z) ∈[M],\nfor µ ∈RM and σ > 0. Then, Y is distributed according to\nqµ,σ := Categorical(softargmax(µ/σ)).\nMoreover, we have\nyσ(µ) = EZ∼p0,1[y(µ + σ · Z)]\n= EY ∼qµ,σ[ϕ(Y )]\n= softargmax(µ/σ).\nProof. For k ∈[M], we have that\nP(Y = k) = P\n \narg max\ni∈[M]\n{µi + σZi} = k\n!\n= P\n \narg min\ni∈[M]\n{e−µi/σ−Zi} = k\n!\nBy Remark 14.3, we have that e−µi/σ−Zi ∼Exp(exp(µi/σ−γ)). One eas-\nily verifies as an exercise, that, for U1, . . . , UM independent exponential\nvariables with parameters u1, . . . , um, we have P(arg mini∈[M]{Ui} =\nk) = uk/ PM\ni=1 ui. Hence, we get\nP(Y = k) =\nexp(µk/σ)\nPM\ni=1 exp(µi/σ),\nthat is,\nY ∼Categorical(softargmax(µ/σ)).\nThe last claim follows from the distribution of Y and the definition of\nϕ.\n14.5.4\nPerturbed max\nA similar result holds if we now wish to perturb the max instead of the\nargmax.\n\n14.5. Gumbel tricks\n363\nProposition 14.5 (Link to log-sum-exp). Given M independent Gum-\nbel random variables Z ∼p0,1, and,\nf(u) := max\ni∈[M] ui,\nthe random variable\nV := f(µ + σ · Z).\nis distributed according to\nqµ,σ := pσLSE(µ/σ),σ.\nMoreover, we have\nfσ(µ) = EZ∼p0,1[f(U + σ · Z)] = EV ∼qµ,σ[V ] = σ · LSE(θ/σ).\nProof. We derive the CDF of f(µ + σ · Z) as\nP(max\ni∈[M]{µi + σZi} ≤t) = P\n \nmin\ni∈[M]{e−(µi/σ−Zi)} ≥e−t/σ\n!\nWe have e−(µi/σ−Zi) ∼Exp(exp(µi/σi)−γ) and for U1, . . . , UM indepen-\ndent exponential random variables with parameters ui, mini∈[M] Ui ∼\nExp(PM\ni=1 ui). Hence,\nP\n \nmax\ni∈[M]{µi + σZi} ≤t\n!\n= exp\n \n−\nM\nX\ni=1\n(exp(µi/σ −γ)) exp(−t/σ)\n!\n= exp(−exp(−(t −σLSE(µ/σ))/σ −γ)).\nWe recognize the CDF of the centered Gumbel distribution with location-\nscale parameters σLSE(µ/σ), σ.\nFor further reading on the Gumbel trick, see Tim Vieira’s great\nblog.\n14.5.5\nGumbel trick for sampling\nThe Gumbel trick is also useful in its own right for sampling with-\nout computing the normalization constant of the softargmax. Indeed,\n\n364\nSmoothing by integration\nProposition 14.4 ensures that if Z is Gumbel noise, then Y is dis-\ntributed according to Categorical(softargmax(µ/σ)). Computing the\narg-maximum, as required to compute Y , can be done in one pass. There-\nfore, we obtain a one-pass algorithm to sample directly from the logits\nµ, without explicitly computing the probabilities softargmax(µ/σ).\nOne may wonder whether such trick could also be used with the\nnormal distribution. Unfortunately, there is no closed form in this case\nbecause it would require integrating the CDF of the maximum of M −1\nGaussian distributions. However, other tricks can be defined such as\nusing Weibull distributions, see Balog et al. (2017).\n14.5.6\nPerturb-and-MAP\nPreviously, we discussed the Gumbel trick in the classification setting,\nwhere Y = [M]. In the structured prediction setting, outputs are\ntypically embedded in RM but the output space is very large. That is,\nY ⊆RM but |Y| ≫M. Structured outputs are then decoded using a\nmaximum a-posteriori (MAP) oracle\nf(u) := max\ny∈Y ⟨y, u⟩\ny(u) := arg max\ny∈Y\n⟨y, u⟩.\nFor this setting, the perturbed versions of f and y,\nfσ(µ) := EZ∼p0,1[f(µ + σ · Z)]\nyσ(µ) := EZ∼p0,1[y(µ + σ · Z)],\nno longer enjoy a closed form in general. However, we can approximate\nthem using Monte-carlo estimation. For the gradient of ∇fσ(µ), two\nestimators exist (Abernethy et al., 2016; Berthet et al., 2020).\nProposition 14.6 (Gradient of perturbed max). Let Y ⊆RM and\np0,1 be a noise distribution with density\np0,1(z) := exp(−ν(z))/C.\n\n14.5. Gumbel tricks\n365\nThen, fσ(µ) is smooth, and its gradient is given by\n∇fσ(µ) = EZ∼p0,1[y(µ + σ · Z)]\n= EZ∼p0,1[f(µ + σ · Z)∇ν(Z)σ]\n∈conv(Y).\nWe therefore have ∇fσ(µ) = yσ(µ).\nThe first estimator is simply a consequence of the reparametrization\ntrick seen in Eq. (14.6) and of y = ∇f, which follows from Danskin’s\ntheorem (see Section 11.2). The second estimator is just SFE seen in\nEq. (14.7). The first estimator usually has lower variance, as it uses\nmore information, namely that y = ∇f.\nThe Jacobian of yσ(µ) also has two estimators (Abernethy et al.,\n2016; Berthet et al., 2020).\nProposition 14.7 (Jacobian of perturbed argmax). Under the same\nnotation as in Proposition 14.6, we have\n∂yσ(µ) = EZ∼p0,1\nh\ny(µ + σZ)∇ν(Z)⊤/σ\ni\n= EZ∼p0,1\nh\nf(µ + σZ)\n\u0010\n∇ν(Z)∇ν(Z)⊤−∇2ν(Z)\n\u0011\n/σ2i\n.\nThe first estimator uses SFE. The second estimator is obtained by\ndifferentiating through\nyσ(µ) = ∇fσ(µ) = EZ∼p0,1[f(µ + σ · Z)∇ν(Z)/σ].\nThe first estimator usually has lower variance. Note that we cannot use\nthe reparametrization trick this time, since y is discontinuous, contrary\nto f.\nLink between perturbation and regularization\nAs shown in (Berthet et al., 2020, Proposition 2.2), assuming Y is a\nconvex polytope with non-empty interior and p has a strictly positive\ndensity, the function\nfσ(µ) := EZ∼p0,1[f(µ + σ · Z)] = EZ∼p0,1[max\ny∈Y ⟨µ + σ · Z, y⟩]\n\n366\nSmoothing by integration\nis strictly convex and its convex conjugate f∗\nσ(y) is Legendre-type. We\ncan therefore rewrite fσ(µ) from the regularization perspective as\nfσ(µ) = max\ny∈Y ⟨µ, y⟩−f∗\nσ(y).\nand ∇fσ(µ) = yσ(µ) is a mirror map, a one-to-one mapping from\nRM to the interior of Y. Unfortunately, f∗\nσ(y) does not enjoy a closed\nform in general. Conversely, does any regularization has a corresponding\nnoise distribution? The reciprocal is not true.\n14.5.7\nGumbel-softmax\nSuppose we want to smooth out the composition h(u) := g(y(u)) by\nhσ(µ) := EZ∼p0,1 [g(y(µ + σZ))]\nwhere\ny(u) :=\narg max\ny∈{e1,...,eM}\n⟨y, u⟩.\nThis is useful to compute the expectation of a loss (instead of the\nloss of an expectation). To compute the gradient of hσ(µ), we can\nreadily use SFE. However, we saw that it suffers from high variance.\nUnfortunately, we cannot use the reparametrization trick here, since\ny(u) is a discontinuous function.\nThe key idea of the Gumbel-sofmax (Jang et al., 2016; Maddison\net al., 2016) is to replace y(u) with a softargmax (with temperature\nparameter τ) to define\nhσ,τ(µ) := EZ∼p0,1 [g(softargmaxτ(µ + σZ))] .\nSince the softargmax is a regularized argmax, we can see the Gumbel-\nsoftmax approach as using both regularization and perturbation. The\nkey benefit is that we can now use the reparametrization trick to get an\nunbiased estimator of ∇hσ,τ(µ). However, this will be a biased estimator\nof ∇hσ(µ), the amount of bias being controlled by the temperature τ.\nIn particular, in the limit case τ →0, we have hσ,τ(µ) →hσ(µ). One\ncaveat, however, is that the function g needs to be well defined on △M,\ninstead of {e1, . . . , eM}.\n\n14.6. Summary\n367\nThe use of the softargmax transformation defines a continuous\ndistribution (Jang et al., 2016; Maddison et al., 2016), that we now\nexplain with σ = 1.\nProposition 14.8 (Gumbel-softargmax / Concrete distributions). Let\nus define the continuous random variable\nT := softargmaxτ(µ + Z) ∈△M,\nwhere Z ∼p0,1 is a Gumbel random variable. Then T is distributed\naccording to a distribution with density\npµ,τ(t) := Γ(M)τ M−1\n M\nX\ni=1\nπi\ntτ\ni\n!−M M\nY\ni=1\nπi\ntτ+1\ni\n,\nwhere π := softargmax(µ).\nWe can extend the Gumbel softargmax to the structured setting by\nreplacing\ny(u) := arg max\ny∈Y\n⟨y, u⟩,\nwith its regularized variant (Paulus et al., 2020). Similarly as before,\none caveat is that g needs to be well defined on conv(Y) instead of Y.\nMoreover, regularizing y is not always easy computationally.\n14.6\nSummary\n• We studied smoothing techniques based on function convolution\nwith a kernel. Due to the commutativity of the convolution, we\ncan alternatively see these as the expectation of the function,\nperturbed with noise, assuming the kernel corresponds to the\nPDF of some noise distribution.\n• Their gradients can be estimated using the path gradient estimator\n(PGE) or score function estimator (SFE), depending on whether\nthe gradient of the original function is available or not.\n• We saw that Stein’s lemma is a special case of SFE used with\nGaussian noise. The so-called “evolution strategies” are just a\n\n368\nSmoothing by integration\nvariant of that with variance reduction and can be interpreted as\nrandomized finite difference.\n• When using Gumbel noise, we were able to derive closed-form\nexpressions for the expectation in specific cases: perturbed com-\nparison, perturbed argmax and perturbed max.\n• We also studied the connections between smoothing by optimiza-\ntion and smoothing by integration. Infimal convolution is the\ncounterpart of convolution, and the Legendre-Fenchel transform\nis the counterpart of Fourier and Laplace’s transforms. Infimal\nconvolution uses a min-plus algebra in the log domain, while\nthe convolution uses a sum-product algebra in the exponential\ndomain.\n\nPart V\nOptimizing differentiable\nprograms\n\n15\nOptimization basics\n15.1\nObjective functions\nConsider a function L, for example evaluating the error or “loss” L(w)\nachieved by a model with parameters w ∈W, where W = RP . To find\nthe best possible model parameterization, we seek to minimize L(w),\nthat is, to compute approximately\nL⋆:= inf\nw∈W L(w),\nassuming that the infimum exists (i.e., L(w) is lower bounded). We will\ndenote a solution, if it exists, by\nw⋆∈arg min\nw∈W\nL(w) :=\n\u001a\nw ∈W : L(w) = min\nw′∈W L(w′)\n\u001b\n.\nIn general, an analytical solution is not available and computing such\na minimum approximately requires an optimization algorithm. An\noptimization algorithm is an iterative procedure, which, starting from an\ninitial point w0, outputs after t iterations a point wt that approximates\nthe minimum of L up to some accuracy ε, i.e.,\nL(wt) −L⋆≤ε.\n(15.1)\n370\n\n15.2. Oracles\n371\n15.2\nOracles\nTo produce iterates w1, w2, . . . that converge to a minimum, the al-\ngorithm naturally needs to have access to information about L. For\nexample, the algorithm needs a priori to be able to evaluate L to know\nif it decreased its value or not. Such information about the function\nis formalized by the notion of oracles (Nemirovski and Yudin, 1983).\nFormally, oracles are procedures that an algorithm can call to access\ninformation about the objective L(w) at any given point w ∈W. We\nusually mainly consider the following three oracles.\n• Zero-order oracle: evaluating the function L(w) ∈R.\n• First-order oracle: evaluating the gradient ∇L(w) ∈W for L\ndifferentiable.\n• Second-order oracle: evaluating the Hessian matrix ∇2L(w),\nor evaluating the Hessian-vector product (HVP) ∇2L(w)v ∈W,\nfor L twice differentiable and any vector v ∈W.\nGiven an oracle O for a function L, we can formally define an optimiza-\ntion algorithm as a procedure which computes the next iterate as a\nfunction of all past and current information. Formally, an algorithm A\nbuilds a sequence w1, . . . , wt from a starting point w0 as\nwt+1 := A(w0, . . . , wt, O(w0), . . . , O(wt), λ),\nwhere λ ∈Λ ⊆RQ encapsulates some hyperparameters of the algorithm,\nsuch as the stepsize. Oftentimes, algorithms build the next iterate simply\nfrom the information collected at the current iterate, without using all\npast iterates. That is, they take the form wt+1 = A(wt, O(wt), λ). A\nclassical example is the gradient descent algorithm, that uses a first-order\noracle to compute iterates of the form\nwt+1 := wt −γ∇L(wt),\nwhere the stepsize γ is a hyperparameter of the algorithm. The notion of\noracle therefore delineates different classes of algorithms. For instance,\nwe may consider zero-order algorithms or first-order algorithms.\n\n372\nOptimization basics\n15.3\nVariational perspective of optimization algorithms\nOne of the most basic optimization algorithms is the proximal point\nmethod, which produces wt+1 from wt by\nwt+1 := arg min\nw∈W\nL(w) + 1\n2γ ∥w −wt∥2\n2.\nIn words, the next iterate is produces by solving a trade-off between\nminimizing the function L and staying close to wt. Unfortunately, the\noptimization problem involved in performing this parameter update is\nas difficult as the original optimization problem, making the proximal\npoint method impractical.\nAs we shall see in Chapter 16 and Chapter 17, many optimization\nalgorithms can be seen as an approximation of the proximal point\nmethod, in the sense that they solve\nwt+1 := arg min\nw∈W\n˜L(w, wt) + 1\n2γ ∥w −wt∥2\n2.\nor more generally\nwt+1 := arg min\nw∈W\n˜L(w, wt) + 1\nγ d(w, wt),\nwhere ˜L(w, wt) is an approximation of L(w) around wt and d(w, w′)\nis some form of distance between w and w′. Different choices of ˜L and\nd lead to different optimization algorithms, and to different trade-offs.\n15.4\nClasses of functions\nWhen studying algorithms theoretically, stronger results can often be\nstated by restricting to certain classes of functions. We already covered\ncontinuous and differentiable functions in Chapter 2. We review a few\nimportant other classes in this section.\n15.4.1\nLipschitz functions\nLipschitz continuity is a stronger form of continuity. Intuitively, a\nLipschitz continuous function is limited in how fast it can change.\n\n15.4. Classes of functions\n373\nDefinition 15.1 (Lipschitz-continuous functions). A function g: W →\nF is β-Lipschitz continuous if for all w, v ∈W\n∥g(w) −g(v)∥2 ≤β∥w −v∥2.\nNote that the definition is valid even for vector-valued functions.\nWith respect to arbitrary norms\nThanks to dual norms reviewed in Section 18.1, we can state a more\ngeneral definition of Lipschitz continuity based on arbitrary norms,\ninstead of the 2-norm. Moreover, we may consider Lipschitz-continuity\nover a subset of the input domain.\nDefinition 15.2 (Lipschitz continuous functions w.r.t. a norm). A func-\ntion g: W →F is said to be β-Lipschitz w.r.t. a norm ∥· ∥over a\nset C ⊆W if for all w, v ∈C\n∥g(w) −g(v)∥∗≤β∥w −v∥.\nWhen ∥· ∥= ∥· ∥2, we recover Definition 15.1, since the 2-norm is\ndual to itself.\n15.4.2\nSmooth functions\nA differentiable function L is said to be β-smooth if its gradients are\nβ-Lipschitz continuous. Setting g(w) = ∇L(w) in Definition 15.1, we\nobtain the following definition.\nDefinition 15.3 (Smooth functions). A differentiable function\nL: W →R is β-smooth for β > 0 if for all w, v ∈W\n∥∇L(w) −∇L(v)∥2 ≤β∥w −v∥2.\nSmoothness ensures that the information provided by the gradient\nat some w is meaningful in a neighborhood of w, since its variations are\nupper-bounded. If the variations were not bounded, the gradient at v\n\n374\nOptimization basics\narbitrarily close to w could drastically change, rendering the information\nprovided by a first-order oracle potentially useless.\nSmoothness of a function can be interpreted as having a quadratic\nupper bound on the function as formalized below.\nProposition 15.1 (Smooth functions). If a differentiable function\nL : W →R is β-smooth then for all w, v ∈W,\n|L(w) −L(v) + ⟨∇L(v), w −v⟩| ≤β\n2 ∥w −v∥2\n2.\nIn particular, we have\nL(w) ≤L(v) + ⟨∇L(v), w −v⟩+ β\n2 ∥w −v∥2\n2.\nProof. This is shown by bounding |L(v) −L(w) −⟨∇L(w), v −w⟩|\nusing the integral representation of the objective along w −v, i.e.,\n|L(v) −L(w) −⟨∇L(w), v −w⟩| = |\nR 1\n0 ⟨∇L(w + s(v −w)), v −w⟩ds −\n⟨∇L(w), v −w)⟩| ≤\nR 1\n0 ∥∇L(w + s(v −w))ds −∇L(w)∥2∥v −w∥2 ≤\nL∥w −v∥2\n2/2, where the last inequality follows from the smoothness\nassumption and standard integration.\nIn other words, L(w) is upper-bounded and lower-bounded around\nv by a quadratic function of w. We will see in Section 16.1 that this\ncharacterization gives rise to a variational perspective on gradient\ndescent.\nWith respect to arbitrary norms\nWe can generalize the definition of smoothness in Definition 15.3 to\narbitrary norms.\nDefinition 15.4 (Smooth functions w.r.t. a norm). A function L :\nW →R is β-smooth w.r.t. a norm ∥· ∥over a set C if for all\nw, v ∈C\n∥∇L(w) −∇L(v)∥∗≤β\n2 ∥w −v∥.\n\n15.4. Classes of functions\n375\nAn equivalent characterization, generalizing Proposition 15.1 to\narbitrary norms, is given below (see, e.g. Beck (2017, Theorem 5.8)).\nProposition 15.2 (Smooth functions w.r.t. a norm). If a differentiable\nfunction L : W →R is β-smooth w.r.t. a norm ∥· ∥over a set C,\nthen for all w, v ∈C\n| L(w) −L(v) −⟨∇L(v), w −v⟩\n|\n{z\n}\nBL(w,v)\n| ≤β\n2 ∥w −v∥2,\nwhere Bf is the Bregman divergence generated by f (Definition 18.2).\n15.4.3\nConvex functions\nA convex function is a function such that its value on the average of two\nor more points is smaller than the average of the values of the functions\nat these points. This is illustrated in Figure 15.2 and formalized below.\nDefinition 15.5 (Convex functions). A function L : W →R is said\nto be convex if for all w, v ∈W and τ ∈[0, 1]\nL(τw + (1 −τ)v) ≤τL(w) + (1 −τ)L(v).\nThe function L is strictly convex if the above inequality is strict\nfor all w ̸= v.\nThe above characterization can easily be generalized to multiple\npoints. Namely, for w1, . . . , wn ∈W and τ1, . . . τn ≥0 such that\nPn\ni=1 τi = 1 (that is, τ1, . . . , τn defines a probability distribution over\n[n]), we have if L is convex that\nL\n n\nX\ni=1\nτiwi\n!\n≤\nn\nX\ni=1\nτiL(wi).\nThe point Pn\ni=1 τiwi is called a convex combination. This can be seen\nas comparing the function at the average point to the average of the\nvalues at theses points and can further be generalized to any random\nvariable.\n\n376\nOptimization basics\nProposition 15.3 (Jensen’s inequality). A function L: W →R is\nconvex if it satisfies Jensen’s inequality, that is, for any random\nvariable W on W,\nL(E[W]) ≤E[L(W)],\nprovided that the expectations are well-defined.\nIf the function considered is differentiable, an alternative charac-\nterization of convexity is to observe how linear approximations of the\nfunction lower bound the function. This is illustrated in Figure 15.2\nand formalized below.\nDefinition 15.6 (Convex differentiable functions). A differentiable func-\ntion L : W →R is convex if and only if for all w, v ∈W\nL(v) ≥L(w) + ⟨∇L(w), v −w⟩.\nThe function L is strictly convex if and only if the above inequality\nis strict for any w ̸= v.\nThe above characterization pinpoints the relevance of convex func-\ntion in optimization: if we can find a point ˆw with null gradient, then\nwe know that we have found the minimum as we have\n∇L( ˆw) = 0 =⇒∀v ∈RP , L(v) ≥L( ˆw) =⇒L( ˆw) = L⋆.\nThis means that by having access to the gradient of the function or an\napproximation thereof, we have access to a sufficient criterion to know\nwhether we found a global minimum. In the case of a gradient descent\non a smooth function, convexity ensures convergence to a minimum at\na sublinear rate as detailed below.\nFinally, if the function is twice differentiable, convexity of a function\ncan be characterized in terms of the Hessian of the function.\nProposition 15.4 (Convex twice differentiable functions). A twice dif-\nferentiable function L : W →R is convex if and only if its Hessian\nis positive semi-definite,\n∀w ∈W, ∇2L(w) ⪰0, i.e., ∀w, v ∈W, ⟨v, ∇2L(w)v⟩≥0.\n\n15.4. Classes of functions\n377\nThe function L is strictly convex if and only if the Hessian is positive-\ndefinite, ∀w ∈W, ∇2L(w) ≻0, i.e., ∀w, v ∈W, ⟨v, ∇2L(w)v⟩>\n0.\n15.4.4\nStrongly-convex functions\nConvexity can also be strengthened by considering µ-strongly convex\nfunctions.\nDefinition 15.7 (Strongly-convex functions). A function L : W →\nR is µ-strongly convex for µ > 0 if for all w, v ∈W and τ ∈[0, 1]\nL(τw + (1 −τ)v) ≤τL(w) + (1 −τ)L(v) −µ\n2 τ(1 −τ)∥w −v∥2\n2.\nA differentiable function L is µ-strongly convex if and only if for\nall w, v ∈W\nL(v) ≥L(w) + ⟨∇L(w), w −v⟩+ µ\n2 ∥w −v∥2\n2.\nA twice differentiable function is µ-strongly convex if and only if\nits Hessian satisfies\n∀w ∈W, ∇2L(w) ⪰µ I, i.e., ∀w, v ∈W, ⟨v, ∇2L(w)v⟩≥µ∥v∥2\n2.\nThe characterization of strong convexity for differentiable functions\nstates that L(w) is lower-bounded by a quadratic. This enables the\ndesign of linearly convergent algorithms as explained later. We naturally\nhave the implications\nL strongly convex =⇒L strictly convex =⇒L convex.\nWith respect to arbitrary norms\nA function can be strongly convex w.r.t. an arbitrary norm, simply\nby replacing the 2-norm in Definition 15.7 with that norm. For differ-\nentiable strongly convex functions, we have the following alternative\ncharacterization, generalizing Definition 15.7 to arbitrary norms.\n\n378\nOptimization basics\nProposition 15.5 (Differentiable strongly-convex functions). If a dif-\nferentiable function L : W →R is µ-strongly convex w.r.t. a norm\n∥· ∥over a set C, then for all w, v ∈C\nµ\n2 ∥w −v∥2 ≤L(w) −L(v) −⟨∇L(v), w −v⟩\n|\n{z\n}\nBL(w,v)\n.\nObviously, if a function L is µ-strongly convex, then, λL is (µλ)-\nstrongly convex. Because all norms are equivalent, if a function is\nstrongly convex w.r.t. a norm, it is also strongly-convex w.r.t. another\nnorm. However, stating the norm w.r.t. which strong convexity holds can\nlead to better constant µ (the higher, the better in terms of convergence\nrates of, e.g., a gradient descent). We also emphasize that it is important\nto mention over which set strong convexity holds. We give examples\nbelow.\nExample 15.1 (Strongly convex functions). The function f(u) =\n1\n2∥u∥2\n2 is 1-strongly convex w.r.t. ∥· ∥2 over RM.\nThe function f(u) = ⟨u, log u⟩is 1-strongly convex w.r.t. ∥· ∥1\nover △M. Applying Proposition 15.5, we obtain for all p, q ∈△M\n1\n2∥p −q∥2\n1 ≤Bf(p, q) = KL(p, q),\nwhich is known as Pinsker’s inequality. We empirically verify\nthe inequality in Fig. 15.1.\nMore generally, f(u) is 1\nµ-strongly convex w.r.t. ∥· ∥1 over any\nbounded set C ⊂RM\n+ such that µ = supu∈C ∥u∥1 (Blondel, 2019).\nHowever, it is not strongly convex over RM\n+ , as it is not bounded.\n15.4.5\nNonconvex functions\nIn general, the minimum of a function necessarily has a null gradient,\nthat is,\nw⋆∈arg min\nw∈W\nL(w) =⇒∇L(w⋆) = 0.\n\n15.4. Classes of functions\n379\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL(p, q)\n0.5||p\nq||2\n1\nFigure 15.1: Graphical verification of Pinsker’s inequality, 1\n2∥p −q∥2\n1 ≤KL(p, q),\nwith p := (π, 1 −π) and q := (0.3, 0.7).\nTo see this, consider the function F : t →L(w⋆−t∇L(w∗)). If\n∇L(w⋆) ̸= 0, then F ′(0) = −∥∇L(w⋆)∥2\n2 ̸= 0. Therefore, there exists\na small t > 0 such that F(t) < F(0), i.e., L(w⋆) is not the minimum.\nHowever, if the function is not convex, the converse is a priori not true:\nfinding a point that has a null gradient does not ensure that we have\nfound a global minimum as illustrated in Figure 15.3.\nFor non-convex functions, a point with null gradient is called a\nstationary point. A stationary point may define a local maximum\nor a local minimum. Formally, ˆw is a local minimum if\n∃r > 0, s.t. ∀v ∈W satisfying ∥v −ˆw∥≤r, we have L(v) ≥L( ˆw).\nA local maximum is defined similarly, except that L(v) ≤L( ˆw) in a\nneighborhood of ˆw. For non-convex functions, convergence rates are\ntherefore generally expressed in terms of convergence of the norm of the\ngradient ∥∇f(wt)∥2 towards 0. Such theoretical results do not ensure\nconvergence to the global minimum but rather convergence to a point\nwhere no further progress may a priori be possible with just gradient\ninformation.\n\n380\nOptimization basics\nFigure 15.2: Convex function: any secant is\nabove the function, any tangent is below the func-\ntion, a point with zero gradient is a minimum.\nGlobal minimum\nLocal minimum\nFigure 15.3: Non-convex func-\ntion: a point with zero gradient\nis not necessarily the global min-\nimum.\n15.5\nPerformance guarantees\nFor a given class of functions, we can define the performance of an\nalgorithm as the number of iterations the algorithm would need to find\nan ε-accurate solution as in Eq. (15.1). This is called the computational\ncomplexity of the algorithm, denoted\nt = T(ε).\nAlternatively, the performance of an algorithm can be stated in terms\nof convergence rate, i.e., the accuracy that the algorithm reaches\nafter t iterations,\nε = R(t),\nwhere R is a decreasing positive function vanishing as t →+∞. Usu-\nally, R incorporates properties of the function minimized, such as its\nsmoothness constant β and information on the initial point, such as its\nfunction value. The corresponding computational complexity T(ε) is\nthen given as the minimum number of iterations t such that R(t) ≤ε,\nT(ε) = min{t ∈N: R(t) ≤ε}.\nConvergence rates can generally be classified by considering the\nprogress ratio on iteration t, defined by\nρt :=\nR(t)\nR(t −1).\n\n15.5. Performance guarantees\n381\nThe asymptotic convergence rate is then defined by\nρ∞:= lim\nt→+∞ρt.\nWe can classify the rates as follows.\n1. Sublinear convergence rates, ρ∞= 1: the longer the algorithm\nruns, the slower it makes progress. That is, the relative progress\neventually tends to stall as t →+∞. Examples of R(t) in this\ncategory include O(1/t), O(1/t2) or more generally O(1/tα) for\nsome α > 0. This is equivalent to T(ε) = O(ε−1/α).\n2. Linear convergence rates, ρ∞= c ∈(0, 1): the algorithm\neventually reaches a state of constant relative progress at each\niteration, leading to an overall rate R(t) = O(exp(−ct)) for c\ndepending on the properties of the objective. This corresponds to\nT(ε) = O(c−1 ln ε−1).\n3. Superlinear convergence rates, ρ∞= 0: the relative progress\nis better at each new iteration. This can happen for, e.g., R(t) =\nO(exp(−t2)), leading to T(ε) = O(\n√\nln ε−1) or\nR(t) = O(exp(−exp(t))), also called a quadratic rate, leading to\nT(ε) = O(ln ln ε−1).\nThis is illustrated in Fig. 15.4.\nNote that the term “linear” may be misleading as the rates are in\nfact exponential. They are called “linear” because of their behavior in\nlog scale.\nUpper and lower bounds\nThe best performance of a class of algorithms equipped with a given\noracle (e.g. first-order oracle) can be upper-bounded or lower-bounded.\nThis allows to show that an algorithm with access limited to a certain\ntype of oracle cannot theoretically do better than a certain number. For\nexample, the computational complexity to minimize β-smooth functions\nrestricted on [0, 1]P with first-order oracles is lower bounded by\nc\nεP\n(Nemirovski and Yudin, 1983, p. 1.1.7). For example, with P = 10\nand ε = 10−3, this gives 1030 iterations. Note that these results are\npessimistic by construction. The actual performance of an algorithm\non a specific instance of this function class may be much better than\n\n382\nOptimization basics\n0\n20\n40\n60\n80\n100\nIteration t\n10\n20\n10\n17\n10\n14\n10\n11\n10\n8\n10\n5\n10\n2\nConvergence rate R(t) (log scale)\nR(t) = 1/ t (sublinear)\nR(t) = 1/t (sublinear)\nR(t) = 1/t2 (sublinear)\nR(t) = e\nt (linear)\nR(t) = e\nt2 (superlinear)\n0\n20\n40\n60\n80\n100\nIteration t\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nProgress ratio \nt =\nR(t)\nR(t\n1)\nFigure 15.4: Left: convergence rates. Right: progress ratios. An algorithm with\nsublinear convergence rates eventually eventually stops making progress. An algorithm\nwith linear convergence rate eventually reaches a state of constant progress. An\nalgorithm with superlinear convergence rate makes faster progress after each iteration.\nthis worst-case scenario, as it is the case with popular algorithms such\nas quasi-Newton methods. Better computational complexities can be\nachieved by further restricting the class of functions to the set of convex\nfunctions, which play a central role in optimization and many other\nfields.\nZero-order vs. first-order\nFor the class of smooth strongly convex functions, the computational\ncomplexity of the best first-order algorithm is (up to constant and\nlogarithmic factors) P times better than that of the best zero-order al-\ngorithm (Nesterov, 2018; Nesterov and Spokoiny, 2017). This theoretical\ncomparison shows that, while zero-order optimization algorithms may\nperform on par with first-order optimization algorithms for problems\nwith a low dimension P, they can be much slower for high dimensional\nproblems, i.e., P ≫1.\nIn different settings, for example with stochastic oracles (Duchi\net al., 2015) or for different classes of functions, slightly different com-\nparisons may be achieved, such as a\n√\nP factor instead of P. However,\nthe same conclusion holds in the current frameworks considered: first-\norder optimization algorithms can provide fast rates that are dimension\nindependent while the rates of zero-order optimization algorithms gen-\n\n15.6. Summary\n383\nerally depend on the dimension of the problem, making them unfit for\nhigh-dimensional problems.\nThis explains the immense success of first-order algorithms for train-\ning neural networks. Fortunately, using reverse-mode autodiff, as studied\nin Chapter 8, it can be shown that computing a gradient has roughly\nthe same complexity as evaluating the function itself Section 8.3.3\n15.6\nSummary\n• The information available to us on a function can be formalized\nby the notion of oracle. Zero-order oracles can only evaluate the\nfunction; first-order oracles can also compute the gradient; second-\norder oracles can also compute the Hessian or the Hessian-vector\nproduct (HVP).\n• Most optimization algorithms reviewed in this book can be viewed\nfrom a variational perspective, in which the next iteration is\nproduced by optimizing a trade-off between an approximation of\nthe function and a proximity term. Different approximations and\ndifferent proximity terms lead to different algorithms.\n• We also reviewed different classes of functions, and performance\nguarantees.\n\n16\nFirst-order optimization\n16.1\nGradient descent\nGradient descent is one of the simplest algorithms in our toolbox to\nminimize a function. At each iteration, it moves along the negative\ngradient direction, scaled by a stepsize γ:\nwt+1 = wt −γ∇L(wt).\n(16.1)\nThe path taken by a gradient descent on a simple quadratic is illustrated\nin Fig. 16.1 for different choices of the stepsize.\n16.1.1\nVariational perspective\nConsider the linear approximation of L(w) around wt,\nL(w) ≈L(wt) + ⟨∇L(wt), w −wt⟩.\nOne can easily check that the gradient descent update in Eq. (16.1) can\nbe rewritten as the solution of a minimization problem, namely,\nwt+1 = arg min\nw∈W\nL(wt) + ⟨∇L(wt), w −wt⟩+ 1\n2γ ∥w −wt∥2\n2.\n(16.2)\nIn words, a gradient descent update optimizes a trade-off between staying\nclose to the current wt, thanks to the proximity term\n1\n2γ ∥w −wt∥2\n2, and\n384\n\n16.1. Gradient descent\n385\n1\n0\n1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent\n Stepsize 0.5\n1\n0\n1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent\n Stepsize 1.8\nFigure 16.1: Trajectory taken by a gradient descent on an objective f(w) =\n0.05w2\n1 + 0.5w2\n2 with a small (left) or large (right) stepsize. In each case the iterates\nfollow the normal vectors to the contour lines (dashed lines), that is, the negative\ngradients. A small stepsize gives a slow convergence but a larger stepsize induces\noscillations.\nminimizing the linearization of L around wt. Intuitively, by choosing γ\nsufficiently small, we ensure that the minimizer of the regularized linear\napproximation stays in a neighborhood where the linear approximation\nis valid. This viewpoint is useful to motivate gradient descent extensions.\n16.1.2\nConvergence for smooth functions\nAs long as ∇L(wt) ̸= 0, the function Lt(γ) := L(wt −γ∇L(wt)) has\na negative derivative at 0, i.e., L′\nt(0) = −∥∇L(wt)∥2\n2. Hence, as long\nas ∇L(wt) ̸= 0, there exists a stepsize ensuring a decrease in objective\nvalues at each iterate. However, without further assumptions, such a\nstepsize may depend on each iterate and may be infinitesimally small.\nTo quantify the convergence of gradient descent with a constant stepsize,\nwe restrict to the class of smooth functions. By applying Proposition 15.1\non the iterate of gradient descent, we obtain that\nL(wt+1) ≤L(wt) −γ∥∇L(wt)∥2\n2 + βγ2\n2 ∥∇L(wt)∥2\n2.\nTherefore, for β-smooth functions, by selecting γ ≤1\nβ, we get that\nL(wt+1) −L(wt) ≤−γ\n2∥∇L(wt)∥2\n2,\nwhich illustrates the main mechanism behind gradient descent: each\niteration decreases the objective by a constant times the norm of the\n\n386\nFirst-order optimization\ngradient of the current iterate. This equation can further be summed\nover all iterates up to T. This telescopes the objective values, leading to\nmin\nt∈{0,...,T−1} ∥∇L(wt)∥2\n2 ≤1\nT\nT−1\nX\nt=0\n∥∇L(wt)∥2\n2\n≤2\nγT\n\u0010\nL(w0) −L(wT )\n\u0011\n≤2\nγT\n\u0010\nL(w0) −L⋆\u0011\n,\nwhere we recall that L⋆is the infimum of L. Therefore, after sufficiently\nmany iterations, gradient descent finds a point whose gradient norm is\narbitrarily small.\nNon-convex case\nWithout further assumptions, i.e., in the non-convex case, the above\nresult (i.e., convergence to a stationary point, measured by the gradient\nnorm) is the best we may get in theory. Denoting Ts(ε) the number\nof iterations needed for a gradient descent to output a point that is\nε-stationary, i.e., ∥∇L( ˆw)∥2 ≤ε, we have Ts(ε) ≤O(ε−2).\nConvex case\nBy adding a convexity assumption on the objective, we can use the lower\nbound provided by the convexity assumption to ensure convergence to\na minimum. Namely, for a β-smooth and convex function f, and with\nstepsize γ ≤1/β, we have that (Nesterov, 2018)\nL(wT ) −L⋆≤1\nγT ∥w0 −w⋆∥2\n2.\nThat is, we get a sublinear convergence rate, and the associated compu-\ntational complexity to find a minimum is T(ε) = O(1/ε).\nStrongly convex case\nIf we further strengthen the assumptions by considering β-smooth, µ-\nstrongly convex functions, the convergence rate of a gradient descent\n\n16.1. Gradient descent\n387\ncan be shown to be (Nesterov, 2018), for any stepsize γ ≤1/β,\nL(wT ) −L⋆≤(1 −γµ)T \u0010\nL(w0) −L⋆\u0011\n≤exp (−γµT)\n\u0010\nL(w0) −L⋆\u0011\n.\nThat is, we obtain a linear convergence rate and the associated computa-\ntional complexity is T(ε) = O(ln ε−1). The above convergence rates may\nbe further refined (Nesterov, 2018); we focused above on the simplest\nresult for clarity.\nStrong convexity can also be replaced by a weaker assumption,\ngradient-dominating property (Polyak, 1963), i.e., ∥∇L(v)∥2\n2 ≥c(L(v)−\nL⋆) for some constant c and any v ∈W. A convex, gradient-dominating\nfunction can also be minimized at a linear rate.\n16.1.3\nMomentum and accelerated variants\nWe started with gradient descent as a simple example of first-order\noptimization algorithm. However, different optimization algorithms can\nbe designed from the access to first-order oracles and the knowledge\nof the class of functions considered. For example, consider quadratic\nconvex functions w 7→1\n2w⊤Aw + b⊤w, that are a basic example of\nsmooth strongly convex functions if A is positive definite. An optimal\nmethod in this case is the heavy-ball method of Polyak (1964), that can\nbe written as\nvt+1 := νvt −γ∇L(wt)\nwt+1 := wt + vt+1.\nThe heavy-ball method uses an additional variable vt, that can be\ninterpreted as the velocity of a ball driven by the negative gradient\nto converge towards a minimum. Intuitively, this additional velocity\ncircumvents the oscillations that a gradient descent may present as\nillustrated in Fig. 16.2 compared to Fig. 16.1. For ν = 0, we recover\nusual gradient descent. For ν > 0, the velocities accumulate a form\nof an inertia momentum, where ν is interpreted as the “mass” of the\nball. In terms of convergence rates, the heavy-ball method can be\nshown to converge linearly similarly to gradient descent, but with a rate\n\n388\nFirst-order optimization\n1\n0\n1\nw1\n0.5\n0.0\n0.5\nw2\nw0\nw1\nw2\nw *\nGradient descent with momentum\n Stepsize 1.8 Momentum 0.2\nFigure 16.2: Trajectory taken by a gradient descent with momentum. Compared to\ngradient descent without momentum, for the same stepsize, the oscillations previously\nobserved in Fig. 16.1 are no longer present, and the algorithm converges then faster\nto the minimum.\nO(exp(−T\np\nµ/β)) for appropriate choices of ν, γ. In comparison, by\nchoosing an optimal stepsize for the gradient descent, its convergence\nrate is O(exp(−Tµ/β)) which is provably worse, as we always have\nµ/β ≤1.\nBeyond the case of quadratic functions, accelerated variants of\ngradient descent for convex or strongly convex functions have been\ndeveloped by Nesterov (2018). Such variants have inspired the design\nof optimization algorithms in stochastic settings presented below.\n16.2\nStochastic gradient descent\nIn machine learning, we are usually interested in minimizing the ex-\npected loss of the model over the data distribution ρ:\nmin\nw∈W L(w) := ES∼ρ [L(w; S)] .\nFor example, L is often set to L(w; S) := ℓ(Y, f(X, w)), where ℓis a\nloss function, f is a neural network and S = (X, Y ) is a random pair,\ncomposed of an input X and an associated target Y , sampled from ρ.\nIn this setting, since the data distribution ρ is generally unknown and\nmay be infinite, we cannot exactly evaluate the expected loss L(w) or\nits gradient ∇L(w).\nIn practice, we are often given a fixed dataset of n pairs si = (xi, yi).\nThis is a special case of the expected loss setting, since this can be seen\n\n16.2. Stochastic gradient descent\n389\nas a empirical distribution ρ = ρn\nL(w) = ES∼ρn [L(w; S)] = 1\nn\nn\nX\ni=1\nL(w; (Xi, Yi)).\nThe gradient of L(w) is then\n∇L(w) := 1\nn\nn\nX\ni=1\n∇L(w; (xi, yi)).\nIn this case, we see that the full gradient ∇L(w), as needed by gradient\ndescent, is the average of the individual gradients. That is, the cost of\ncomputing ∇L(w) is proportional to the number of training points n. For\nn very large, that is a very large amount of samples, this computational\ncost can be prohibitive. Stochastic gradients circumvent this issue.\n16.2.1\nStochastic gradients\nUsually, even if we do not know ρ, we can sample from it, i.e., we have\naccess to samples S ∼ρ. We can then use a stochastic gradient of\nthe form ∇L(w; S) as a random estimate of ∇L(w). This may look like\na rough estimate but, on average, this is a valid approximation since\nES∼ρ [∇L(w; S)] = ∇L(w).\nWe say that ∇L(w; S) is an unbiased estimator of ∇L(w). To fur-\nther improve the approximation, we may also consider mini-batch\nestimates by sampling m ≪n data points Si := (Xi, Yi) and using\n1\nm\nPm\ni=1 ∇L(w; Si), whose expectation still matches ∇L(w), while po-\ntentially reducing the approximation error by averaging multiple stochas-\ntic gradients. Computationally, the main advantage is that the cost is\nnow proportional to m instead of n.\nIn whole generality, one can consider stochastic first-order oracles\ndefined below.\nDefinition 16.1 (Stochastic first-order oracles). A stochastic first-\norder oracle of an expected objective L(w) is a random estimate\ng(w; S) of ∇L(w) with S sampled according to some distribution\n\n390\nFirst-order optimization\nq. A stochastic gradient is said to be an unbiased estimator if\nES∼q [g(w; S)] = ∇L(w).\nThe variance of a stochastic gradient is\nES∼q\nh\n∥g(w; S) −∇L(w)∥2\n2\ni\n.\nWhen q = ρ, we recover stochastic gradients. When q is the product\nof m independent samples according to p, we recover mini-batch stochas-\ntic gradients. First-order stochastic optimization algorithms build upon\nstochastic first-order oracles to approximately find the minimum of the\nexpected objective. In such a setting, the iterates of the algorithm are\nby definition random. Convergence rates therefore need to be expressed\nin probabilistic terms by considering for example the expected objective\nvalue according to the randomness of the oracles.\n16.2.2\nVanilla SGD\nEquipped with a stochastic first-order oracle, such as (mini-batch)\nstochastic gradients, we can define stochastic gradient descent as\nwt+1 = wt −γg(wt; St)\nwhere St ∼q.\nWe assume that St is independent of wt. Compared to the usual gradient\ndescent, the main impediment of the stochastic setting is the additional\nnoise induced by the stochastic estimates: their variance.\nFor example, consider applying a stochastic gradient descent on the\nexpectation of β-smooth convex functions L(w; s) with unbiased oracles.\nTo harness the randomness of the iterates, consider after T iterations\noutputting the average of the first T iterates, that is ¯wT := 1\nT\nPT\nt=1 wt.\nMoreover, suppose that the variance of the stochastic first-order oracles\nis bounded by σ2 for all minimizers w⋆of L. Denoting by ES0,...,ST −1\nthe randomness associated to the stochastic oracles, we have then that\nfor a stepsize γ ≤1/(4β), (Lan, 2012),\nES0,...,ST −1[L( ¯wT )] −L⋆≤1\nγT ∥w0 −w⋆∥2\n2 + 2γσ2.\nThe resulting convergence rate illustrates that a stochastic gradient\ndescent converges to the minimum of the expected objective up to a\n\n16.2. Stochastic gradient descent\n391\nconstant term depending on the variance of the oracle and the stepsize.\nOne can diminish the variance by considering mini-batches: if the\nvariance of a single stochastic gradient is σ2\n1, considering a mini-batch\nof m gradients reduces the variance of the corresponding oracle to\nσm = σ2\n1/m. To decrease the additional term, one may also decrease\nthe stepsizes over the iterations. For example, by choosing a decreasing\nstepsize like γt = t−1/2, the convergence rate is then of the order\nO((∥w0 −w⋆∥2\n2 + σ2 ln t)/\np\n(t)). The stepsize can also be selected as\na constant γ0 that decreases the average objective for the first T0\niterations and reduced by a multiplicative factor at regular intervals\nlike γj = ργj−1 for ρ ∈(0, 1) to handle iterations between Tj, Tj+1.\nAlternative stepsize schedules such as a cosine decay (Loshchilov and\nHutter, 2016) have recently become popular.\nThe literature on alternative optimization schemes for stochastic\noptimization is still rapidly evolving, with new heuristics regularly\nproposed. We present below two popular techniques.\n16.2.3\nMomentum variants\nAccelerated optimization algorithms developed in the deterministic\nsetting may be extended to the stochastic setting. For example, the\nheavy-ball method can be adapted to the stochastic setting, leading to\nstochastic gradient descent with momentum (Sutskever et al., 2013)\ngenerally implemented as\nvt+1 := νvt + g(wt; St)\nwt+1 := wt −γvt+1.\nAs mentioned earlier the momentum method can be modified to han-\ndle non-quadratic smooth strongly convex functions. This leads to\nNesterov’s accelerated method in the deterministic setting. This has\nbeen adapted to the stochastic with a so-called Nesterov momen-\ntum (Sutskever et al., 2013)\nvt+1 := νvt + g(wt + νvt; St)\nwt+1 := wt −γvt+1.\n\n392\nFirst-order optimization\n16.2.4\nAdaptive variants\nIn any gradient descent-like algorithm, selecting the stepsize is key for\ngood performance. While a constant stepsize may be used if the function\nis smooth, we may not know in advance the smoothness constant of the\nobjective, which means that additional procedures may be required to\nselect appropriately the stepsize. In the deterministic case, line-searches\nsuch as the Armijo or Wolfe’s rules (Wright and Nocedal, 1999) can be\nused to check whether the selected stepsize decreases sufficiently the\nobjective at each iteration. Such rules have be adapted in the stochastic\nsetting (Vaswani et al., 2019).\nAnother way to decrease the sensitivity of the algorithm with respect\nto the stepsize has been to estimate first and second-order moments\nof the gradients and use the latter as a form of preconditioning to\nsmooth the trajectory of the iterates. This led to the popular Adam\noptimizer (Kingma and Ba, 2014). It takes the form,\nmt+1 := ν1mt + (1 −ν1)gt\nvt+1 := ν2vt + (1 −ν2)(gt)2\nˆmt+1 := mt+1/(1 −νt\n1)\nˆvt+1 := vt+1/(1 −νt\n2)\nwt+1 := wt −γ ˆmt+1/\n\u0010p\nˆvt+1 + ε\n\u0011\n,\nwhere gt := g(wt; St), (gt)2 denotes the element-wise square of gt and\nν1, ν2, γ, ε are hyper-parameters of the algorithm. Numerous variants\nexist, such as varying the stepsize γ above along the iterations.\n16.3\nProjected gradient descent\nOftentimes, we seek to find the solution of a minimization problem\nsubject to constraints on the variables, of the form\nmin\nw∈C L(w),\n(16.3)\nwhere C ⊆W = RP is a set of constraints. We say that an approximate\nsolution bw to Eq. (16.3) is feasible if bw ∈C. Naturally, the design\n\n16.3. Projected gradient descent\n393\nof algorithms for the constrained setting now depends, not only on\ninformation about L, but also on information about C.\nSimilarly to L, different oracles can be considered about C. One of\nthe most commonly used oracle is the Euclidean projection\nDefinition 16.2 (Euclidean projection). The Euclidean projection\nonto the set C is defined by\nprojC(w) := arg min\nv∈C\n∥w −v∥2\n2.\nThis projection, which is well-defined when C is a convex set, can\nbe used in projected gradient descent, that we briefly review below.\nTypically, the projection on a particular set C requires a dedicated\nalgorithm to compute it.\nOther possible oracles are linear maximization oracles (LMO)\nused in Frank-Wolfe algorithms and Bregman projection oracles,\nused in mirror descent algorithms. The algorithm choice can be dictated\nby what oracle about C is available.\n16.3.1\nVariational perspective\nProjected gradient descent is a natural generalization of gradient descent,\nbased on the Euclidean projection oracle. Its iterates read\nwt+1 := projC(wt −γ∇L(wt)).\nAt each iteration, we attempt to decrease the objective by moving along\nthe negative gradient direction, while ensuring that the next iterate\nremains feasible, thanks to the projection step.\nSimilarly to the variational perspective of gradient descent in Eq. (16.2),\nthe projected gradient descent update is equivalent to\nwt+1 = arg min\nw∈C\nL(wt) + ⟨∇L(wt), w −wt⟩+ 1\n2γ ∥w −wt∥2\n2.\nThis shows that projected gradient descent minimizes a trade-off between\nstaying close to wt and minimizing the linearization of L around wt,\nwhile staying in C.\n\n394\nFirst-order optimization\nIn terms of convergence rates, they remain the same as gradient\ndescent (Nesterov, 2018). For example, projected gradient descent on a\nsmooth convex function still converges at a rate R(T) = O(1/T).\nThere are numerous extensions of vanilla projected gradient descent.\nSimilarly to gradient descent, the stepsize can be automatically adjusted\nusing linesearch techniques and there exists accelerated variants. If\nwe replace ∇L(w) with a stochastic gradient ∇L(w; S), we obtain a\nstochastic projected gradient descent.\n16.3.2\nOptimality conditions\nIn the unconstrained case, a minimum necessarily has a zero gradient.\nIn the constrained setting, there may not be any feasible parameters\nwith zero gradient. Instead, the optimality of a point is characterized\nby the fact that no better solution can be found by moving along the\ngradient at that point, while staying in the constraints. Formally, it\nmeans that for any γ > 0, a minimizer w⋆of L on C satisfies\nw⋆= projC(w⋆−γ∇L(w⋆)).\nIt can be shown that this condition is equivalent (Nesterov, 2018) to\n⟨∇L(w⋆), w −w⋆⟩≥0\n∀w ∈C.\n16.3.3\nCommonly-used projections\nWe now briefly review a few useful Euclidean projections.\n• If C = RP , we obviously have\nprojC(w) = w.\nTherefore, in the unconstrained setting, projected gradient descent\nindeed recovers gradient descent.\n• If C = [a, b]P (box constraints), we have\nprojC(w) = clip(w, a, b) := min{max{w, a}, b}.\nwhere the min and max are applied coordinate-wise.\n\n16.4. Proximal gradient method\n395\n• As a special case of the above, if C = RP\n+ (non-negative orthant),\nprojC(w) = max{w, 0},\nalso known as non-negative part or ReLu.\n• If C = △P (unit probability simplex),\nprojC(w) = max{w −τ 1, 0},\nwhere τ ∈R is a constant ensuring that projC(w) normalizes to 1.\nIt is known that τ can be found in O(P log P) using a sort. This\ncan be improved to O(P) using a median-finding like algorithm.\n16.4\nProximal gradient method\nThe constrained setting (with C a convex set) can be recast as uncon-\nstrained optimization, by extending our analysis to functions taking\ninfinite values. Let us denote the indicator function of the set C by\nιC(w) :=\n\n\n\n0\nif w ∈C\n+∞\notherwise\n.\nClearly, the constrained problem in Eq. (16.3) can then be rewritten as\nmin\nw∈W L(w) + ιC(w).\nThis suggests that constrained optimization is a special case of com-\nposite objectives of the form\nmin\nw∈W L(w) + Ω(w),\nwhere Ωis a convex but potentially non-differentiable function. We\nassume that we have access to an oracle associated with Ωcalled the\nproximal operator.\nDefinition 16.3 (Proximal operator). The proximal operator asso-\nciated with Ω: W →R is\nproxΩ(w) := arg min\nv∈W\n1\n2∥w −v∥2\n2 + Ω(v).\n\n396\nFirst-order optimization\nThis leads to the proximal gradient method, reviewed below.\n16.4.1\nVariational perspective\nWith this method, the update reads\nwt+1 = proxγΩ(wt −γ∇L(wt)).\nThis update again enjoys an intuitive variational perspective, namely,\nwt+1 = arg min\nw∈W\nL(wt) + ⟨∇L(wt), w −wt⟩+ 1\n2γ ∥w −wt∥2\n2 + Ω(w).\nThat is, we linearize L around wt, but keep Ωas is.\nThe proximal gradient method is popularly used when the objective\nfunction contains a sparsity-inducing regularizer Ω. For example, for\nthe LASSO (Tibshirani, 1996), which aims at predicting targets y =\n(y1, . . . , yn)⊤∈RN from observations X = (x1, . . . , xn)⊤∈RN×P , we\nset L(w) = 1\n2∥Xw −y∥2\n2 and Ω(w) = λ∥w∥1, where λ > 0 controls\nthe regularization strength. In this case, proxΩis the so-called soft-\nthresholding operator (see below).\nConvergence guarantees of the proximal gradient method remain\nthe same as for gradient descent, such as a O(1/T) rate for smooth\nconvex functions.\n16.4.2\nOptimality conditions\nAn optimal solution of the problem is characterized by the fixed point\nequation\nw⋆= proxγΩ(w⋆−γ∇L(w⋆)),\nfor all γ > 0 (Nesterov, 2018). In other words, the proximal gradient\nmethod (which includes gradient descent and projected gradient descent\nas special cases), can be seen as fixed point iteration schemes. Such\na viewpoint suggests using acceleration methods from the fixed point\nliterature such as Anderson acceleration (Pollock and Rebholz, 2021).\nIt is also useful when designing implicit differentiation schemes as\npresented in Chapter 8.\n\n16.5. Summary\n397\n16.4.3\nCommonly-used proximal operators\nWe now briefly review a few useful proximal operators.\n• If Ω(w) = 0, we have\nproxγΩ(w) = w.\nTherefore, with this proximal operator, the proximal gradient\nmethod recovers gradient descent.\n• If Ω(w) = ιC(w), we have\nproxγΩ(w) = projC(w).\nTherefore, with this proximal operator, the proximal gradient\nmethod recovers projected gradient descent.\n• If Ω(w) = λ∥w∥1, we have\nproxγΩ(w) = (sign(w) · max(|w| −γλ, 0)),\nwhere the operations are applied coordinate-wise. This is the\nso-called soft-thresholding operator.\n• Ω(w) = λ P\ng∈G ∥wg∥2 where G is a partition of [P] and wg\ndenotes the subvector restricted to g, then we have\nh\nproxγΩ(w)\ni\ng = max(1 −λ · γ/∥wg∥2, 0)wg,\nwhich is used in the group lasso (Yuan and Lin, 2006) and can be\nused to encourage group sparsity.\nFor a review of more proximal operators, see for instance (Bach et al.,\n2012; Parikh, Boyd, et al., 2014).\n16.5\nSummary\n• From a variational perspective, gradient descent is the algorithm\nobtained when linearizing the objective function and using a\nquadratic regularization term.\n\n398\nFirst-order optimization\n• Projected gradient descent is the algorithm obtained when there\nis an additional constraint (the Euclidean projection naturally\nappearing, due to the quadratic regularization term).\n• When the objective is the sum of a differentiable function and\na non-differentiable function, proximal gradient is the algorithm\nobtained when the differentiable function is linearized but the\nnon-differentiable function is kept as is.\n• We also reviewed various stochastic gradient based algorithms,\nincluding vanilla SGD, SGD with momentum and Adam.\n\n17\nSecond-order optimization\nWe review in this chapter methods whose iterations take the form\nwt+1 := wt −γtBt∇L(wt),\nwhere γt is a stepsize and Bt is a pre-conditioning matrix involving\nsecond-order derivatives.\n17.1\nNewton’s method\n17.1.1\nVariational perspective\nWe saw in Eq. (16.2) that gradient descent can be motivated from\na variational perspective, in which we use a linear approximation of\nthe objective around the current iterate, obtained from the current\ngradient. Similarly, if we have access not only to the gradient but also\nto the Hessian of the objective, we can use a quadratic approximation\nof the objective around the current iterate. More precisely, given a\nfunction L(w), we may consider minimizing the second-order Taylor\napproximation of L(w) around the current iterate wt,\nL(w) ≈L(wt) + ⟨∇L(wt), w −wt⟩+ 1\n2⟨w −wt, ∇2L(wt)(w −wt)⟩.\n399\n\n400\nSecond-order optimization\nNewton’s method simply iteratively minimizes this quadratic approx-\nimation around the current iteration wt, namely,\nwt+1 = arg min\nw∈W\nL(wt) + ⟨∇L(wt), w−wt⟩+ 1\n2⟨w−wt, ∇2L(wt)(w−wt)⟩.\n(17.1)\nIf the Hessian is positive definite at wt, which we denote by ∇2L(wt) ≻\n0, then the minimum is well-defined and unique (this is for example the\ncase if L is strictly convex). The iterates can then be written analytically\nas\nwt+1 = wt −∇2L(wt)−1∇L(wt).\nIf the Hessian is not positive definite, the minimum may not be defined.\nIgnoring this issue and taking the analytical formulation could be\ndangerous, as it could amount to computing the maximum of the\nquadratic instead if, for example, the quadratic was strictly concave\n(i.e., ∇2L(w) ≺0).\n17.1.2\nRegularized Newton method\nA simple technique to circumvent this issue consists in adding a regu-\nlarization term to the Hessian. Namely, from a variational viewpoint,\nwe can add a proximity term 1\n2∥w −wt∥2\n2, encouraging to stay close to\nthe current wt. The iterates of this regularized Newton method then\ntake the form\nwt+1 = arg min\nw∈W\nL(wt) + ⟨∇L(wt), w−wt⟩1\n2⟨w−wt, ∇2L(wt)(w−wt)⟩\n+ ηt\n2 ∥w −wt∥2\n2,\nwhere ηt controls the regularization strength. Assuming ηt > 0 is strong\nenough to make ∇2L(wt) + ηt I positive-definite, we have\nwt+1 = wt −dt,\nwhere we defined the direction\ndt := (∇2L(wt) + ηt I)−1∇L(wt).\n(17.2)\n\n17.1. Newton’s method\n401\nOther techniques to circumvent this issue include using cubic regular-\nization and modifying the spectral decomposition of the Hessian, by\nthresholding the eigenvalues or taking their absolute values. We refer\nthe interested reader to, e.g., (Nesterov, 2018; Wright and Nocedal,\n1999) for more details.\n17.1.3\nApproximate direction\nWe observe a main impediment for implementing such a second-order\noptimization algorithm: even if we had access to the Hessian of the\nobjective for free and this Hessian was positive definite, computing the\nexact direction dt in Eq. (17.2) requires computing an inverse-Hessian\nvector product (IHVP) with the gradient ∇L(wt). Doing so exactly\nrequires solving a linear system\n(∇2L(wt) + ηt I)dt = ∇L(wt),\nwhich a priori takes O(P 3) time. In practice, however, we can compute\nIHVPs approximately, as explained in Section 9.4.\n17.1.4\nConvergence guarantees\nWhile implementing Newton’s method comes at a higher computational\ncost, it can also benefit from faster convergence rates. Briefly, if Newton’s\nmethod is initialized at a point w0 ∈W close enough from the mini-\nmizer w⋆of a µ-strongly convex function with M-Lipschitz continuous\nHessian (namely ∥w0 −w∗∥2 ≤2µ\n3M ), then Newton’s method converges\nat a quadratic rate (Nesterov, 2018), that is, R(t) ≤O(exp(exp(−t))\n(see Section 15.5 for a brief introduction to performance guarantees).\nThis is far superior to gradient descent. Such an efficiency motivated\nthe development of interior point methods, that have been a break-\nthrough in constrained optimization, thanks to the use of log-barrier\npenalties (Nesterov, 2018).\n17.1.5\nLinesearch\nIn practice, we may not have access to an initial point close enough\nfrom the minimizer. In that case, even for strictly convex functions for\n\n402\nSecond-order optimization\nwhich Newton’s steps are well-defined, taking wt+1 = wt −dt may not\nensure a decrease of the objective values. Nevertheless, the direction dt\nmay define a descent direction as defined below.\nDefinition 17.1 (Descent direction). A point d ∈W defines a de-\nscent direction −d for an objective L at w, if there exists a\npositive stepsize γ > 0 such that\nL(w −γd) ≤L(w).\nIf L is differentiable, −d is a descent direction if ⟨−d, ∇L(w)⟩< 0.\nFor Newton’s method without regularization, dt = ∇2L(wt)−1∇L(wt)\nis then a descent direction at wt, as long as ∇L(wt) ̸= 0 and ∇2L(wt) ≻\n0. If ∇2L(wt) ̸≻0, choosing ηt > 0 such that ∇2L(wt) + ηt I ≻0, also\nensures that dt = −(∇2L(wt) + ηt I)−1∇L(wt) is a descent direction\n(as long as ∇L(wt) ̸= 0). Newton’s method is then generally equipped\nwith a linesearch method that attempts to take steps of the form x\nwt+1 = wt −γtdt\nwith γt chosen as the largest stepsize among {ρτ, τ ∈N} for ρ ∈(0, 1)\nuntil a sufficient decrease of the objective is satisfied such as, for c ∈\n(0, 1),\nL(wt −γtdt) ≤L(wt) −cγt⟨∇L(wt), ∇2L(wt)−1∇L(wt)⟩.\nFor strongly convex functions, such an implementation exhibits two\nphases: a first phase during which Newton’s steps are “damped” by\nusing a stepsize γt < 1 and a second phase of super-fast convergence\nduring which stepsizes γt = 1 are taken, and the objective decreases very\nfast. Even far from the optimum, Newton directions can advantageously\nadapt to the local geometry of the objective to speed-up convergence\ncompared to a regular gradient descent as explained below.\n17.1.6\nGeometric interpretation\nTo understand the efficiency of Newton’s method compared to gradient\ndescent, consider the minimization of a simple quadratic\nL(w) = 1\n2aw2\n1 + 1\n2bw2\n2\n\n17.1. Newton’s method\n403\nfor a ≫b ≥0, as illustrated in Fig. 17.1. A gradient descent moves\nalong the directions ∇L(w) = (aw1, bw2)⊤and its stepsize is limited\nby the variations in the first coordinate leading to some oscillations. If\nwe were simply rescaling the gradient by (a, b), i.e., taking steps of the\nform\nwt+1 = wt −γ diag(a−1, b−1)L(wt),\nthe variations in both coordinates would be normalized to one and the\nstepsize could simply be chosen to γ = 1 to directly get w⋆. In other\nwords, by adapting the geometry of the directions with the geometry\ninduced by the objective, we can circumvent the oscillations.\nThat’s exactly what Newton’s method does by modifying the gradi-\nent direction using the inverse of the Hessian. Formally, at iteration t,\nconsider the modified objective\n˜L(v) = L(Av) for A = ∇2L(wt)−1/2,\nwith L strictly convex and A the inverse matrix square root of the\nHessian. One easily verifies that a Newton step is equivalent to a\ngradient step on ˜L, that is,\nvt+1 = vt −∇˜L(vt)\n⇐⇒\nwt+1 = wt −(∇2L(wt))−1∇L(wt)\nwhere\nwt = Avt = ∇2L(wt)−1/2vt.\nIn the geometry induced by A, the objective is generally better condi-\ntioned as illustrated in Fig. 17.1. This explains the efficiency of Newton’s\nmethod. In particular for any strongly convex quadratic, a Newton step\nreaches the optimum in one iteration, while a gradient step can take\nmany more iterations.\n17.1.7\nStochastic Newton’s method\nConsider now an expected loss\nmin\nw∈W L(w) := ES∼ρ [L(w; S)] .\nIn that case, an estimate of the Hessian can be constructed just like for\nthe gradient using that\nES∼ρ\nh\n∇2L(w; S)\ni\n= ∇2L(w).\n\n404\nSecond-order optimization\nFigure 17.1: Left: Minimization of a quadratic L(w) = 1\n2aw2\n1 + 1\n2bw2\n2 by gradient\ndescent. For a ≫b ≥0, a gradient descent typically oscillates. Right: minimization\nby Newton’s method amounts to change the geometry of the problem to avoid\noscillations.\nDenote then\ng(w; S) ≈∇L(w),\nH(w; S′) ≈∇2L(w)\nsome stochastic estimates of respectively of the gradient and the Hessian\nwith S, S′ independently drawn from p or from mini-batch approaxima-\ntions with varying mini-batch sizes. One implementation of a stochastic\nNewton method can then be\nwt+1 = wt −γt(H(wt; S′) + ηt I)−1g(wt; S),\nfor ηt ≥0 such that (H(wt; S′) + ηt)−1 ≻0 and γt fixed or chosen to\nsatisfy some sufficient decrease condition. We refer the interested reader\nto, e.g., (Xu et al., 2020), for more details and variants.\n17.2\nGauss-Newton method\nNewton’s method (17.1) is usually not properly defined for non-convex\nobjective functions, since the Hessian may not be positive definite at the\ncurrent iterate. We saw in Section 9.2 that the Gauss-Newton matrix can\nbe used to define a positive-semidefinite approximation of the Hessian.\nHere, we revisit the Gauss-Newton method from a variational and\npartial linearization perspective. While the original Gauss-Newton\nmethod originates from nonlinear least-squares, we will first describe\nan extension to arbitrary convex loss functions, since it is both more\ngeneral and easier to explain.\n\n17.2. Gauss-Newton method\n405\n17.2.1\nWith exact outer function\nConsider a composite objective of the form\nL(w) := ℓ(f(w)),\nwhere ℓ: M →R is a convex function, such as a convex loss function\napplied on a given sample, and f : W →M is a nonlinear function,\nsuch as a neural network with parameters w ∈W, evaluated on the\nsame sample. We saw that gradient descent and Newton’s method\namount to using linear and quadratic approximations of L(w) around\nthe current iterate wt, respectively. As a middle ground between the\ntwo, the Gauss-Newton method uses the linearization of f around wt\nf(w) ≈f(wt) + ∂f(wt)(w −wt)\nbut keeps ℓas is to obtain the objective\nwt+1 := arg min\nw∈W\nℓ(f(wt) + ∂f(wt)(w −wt))\n= arg min\nw∈W\nℓ(∂f(wt)w + f(wt) −∂f(wt)wt)\n= arg min\nw∈W\nℓ(Jtw + δt),\nwhere we defined the shorthands Jt := ∂f(wt) and δt := f(wt) −\n∂f(wt)wt. We call ℓ(Jtw + δt) the partial linearization of L = ℓ◦f\nat wt, as opposed to the full linearization of L used in gradient descent.\nSince the composition of a convex function and of linear function is\nconvex, this objective is convex even if L(w) is nonconvex. In practice,\nwe often add a proximity term as regularization to define\nwt+1 := arg min\nw∈W\nℓ(Jtw + δt) + ηt\n2 ∥w −wt∥2\n2.\n(17.3)\nWe can see this update as an approximation of the proximal point\nupdate\narg min\nw∈W\nL(w) + ηt\n2 ∥w −wt∥2\n2,\nwhere L(w) has been replaced by its partial linearization. Solving\nEq. (17.3) using gradient-based solvers requires to compute the gradient\n\n406\nSecond-order optimization\nof w 7→ℓ(Jtw + δt), which is w 7→(Jt)∗∇ℓ(Jtw + δt). Computing\nthis gradient by autodiff therefore requires to perform a forward pass\nto compute the JVP Jtw and a backward pass to compute the VJP\n(Jt)∗∇ℓ(z). See Section 2.3 for an introduction to these operators and\nChapter 8 for an introduction to autodiff.\nThe Gauss-Newton method with arbitrary convex outer loss is\noften called modified Gauss-Newton (Nesterov, 2007) or prox-linear\n(Drusvyatskiy and Paquette, 2019). The classical Gauss-Newton and\nLevenberg-Marquardt (Levenberg, 1944; Marquardt, 1963) methods\noriginate from nonlinear least-squares and are recovered when ℓ(z) is\nquadratic (Kelley, 1995), such as ℓ(z) := 1\n2∥z−y∥2\n2, for y some reference\ntarget. The Gauss-Newton method corresponds classically to not using\nregularization (i.e., ηt = 0) and the Levenberg-Marquardt method uses\nregularization (usually called damping, potentially changing ηt across\niterations). See e.g., (Messerer et al., 2021), for a survey of different\nvariants.\n17.2.2\nWith approximate outer function\nAnother variant of the Gauss-Newton method consists in replacing the\nconvex loss ℓwith its quadratic approximation around zt := f(wt),\nqt(z) := ℓ(zt) + ⟨∇ℓ(zt), z −zt⟩+ 1\n2⟨z −zt, ∇2ℓ(zt)(z −zt)⟩≈ℓ(z)\nto define the update\nwt+1 := arg min\nw∈W\nqt(Jtw + δt) + ηt\n2 ∥w −wt∥2\n2.\nNotice that ℓhas been replaced by its quadratic approximation qt.\nThis objective is always a convex quadratic, unlike the objective\nof the Newton method in Eq. (17.1), which is a priori a nonconvex\nquadratic, if f is nonlinear. Simple calculations show that\nwt+1 = arg min\nw∈W\nL(wt) + ⟨qt, Jt(w−wt)⟩\n+ 1\n2⟨w−wt, (Jt)∗QtJt(w−wt)⟩+ ηt\n2 ∥w −wt∥2\n2\n\n17.2. Gauss-Newton method\n407\nwhere qt := ∇ℓ(f(wt)) ∈M = RZ, Qt := ∇2ℓ(f(wt)) ∈RZ×Z. The\nclosed form solution is\nwt+1 = wt −((Jt)∗QtJt + ηt I)−1(Jt)∗qt\n= wt −(∇2\nGN(ℓ◦f)(wt) + ηt I)−1∇L(wt),\nwhere we used the (generalized) Gauss-Newton matrix of L = ℓ◦f,\ndefined in Section 9.2.\n17.2.3\nLinesearch\nSimilarly to Newton’s method, the iterates of a Gauss-Newton method\nmay diverge when used alone. However, the direction −(∇2\nGN(ℓ◦f)(wt)+\nηt I)−1∇L(wt) defines a descent direction for any ηt > 0 and can be\ncombined with a stepsize γt (typically chosen using a linesearch) to\nobtain iterates of the form\nwt+1 = wt −γt(∇2\nGN(ℓ◦f)(wt) + ηt I)−1∇L(wt).\n17.2.4\nStochastic Gauss-Newton\nIn deep learning, the objective generally consists in an expectation\nover samples of the composition between a loss function and a network\nfunction:\nL(w) = ES∼ρ[L(w; S)] = E(X,Y )∼ρ[ℓ(f(w; X); Y )]\nwhere S = (X, Y ) denotes a sample pair of input X with associated\nlabel Y . In that case, as already studied in Section 9.2, the Gauss-\nNewton matrix ∇2\nGNL is the expectation of the individual Gauss-Newton\nmatrices\n∇2\nGNL(w; x, y) := ∂f(w; x)⊤∇2ℓ(f(w; x))∂f(w; x),\n∇2\nGNL(w) := E(X,Y )∼ρ[∇2\nGNL(w; x, y)].\nWe can estimate the gradient and the Gauss-Newton matrix by, respec-\ntively, g(w; S) ≈∇L(w), and G(w; S′) ≈∇2\nGNL(w) for S, S′ ∼ρ or\nusing mini-batch approximations. A stochastic Gauss-Newton method\ntherefore performs iterates\nwt+1 := wt −γt(G(w; S′) + ηt I)−1g(w, S),\nfor ηt ≥0 and γt fixed or selected to satisfy some criterion.\n\n408\nSecond-order optimization\n17.3\nNatural gradient descent\nNatural gradient descent (Amari, 1998) follows a similar principle as\ngradient descent: linearize the objective around the current iterate and\nminimize this approximation together with a proximity term. It differs\nfrom gradient descent in the choice of the proximity term: rather than\nusing a squared Euclidean distance between the parameters, it uses a\nKullback-Leibler divergence between the probability distributions\nthese parameters define.\nNegative log-likelihood\nWe consider objectives of the form\nmin\nw∈W L(w) = ES∼ρ [L(w; S)] = ES∼ρ [−log qw(S)] ,\nwhere ρ is an unknown data distribution (but from which we can sam-\nple) and where qw is a probability distribution parameterized by w. As\nreviewed in Chapter 3, the negative log-likelihood can be used as a loss\nfunction (many loss functions can be seen from this perspective, includ-\ning the squared and logistic loss functions). In the unsupervised setting,\nwhere S = Y , we simply use qw(Y ) as is. In the supervised setting,\nwhere S = (X, Y ), we use the product rule P(X, Y ) = P(X)P(Y |X) to\nparameterize qw(S) as\nqw(x, y) := ρX(x)pθ(y),\nwhere ρX is the marginal distribution for X, pθ(y) is the PMF/PDF\nof a probability distribution and θ = f(w; x) is for instance a neural\nnetwork with parameters w ∈W and input x ∈X.\n17.3.1\nVariational perspective\nNatural gradient descent is motivated by updates of the form\nwt+1 = arg min\nw∈W\nL(wt) + ⟨∇L(wt), w −wt⟩+ KL(qwt, qw),\nwhere KL(p, q) :=\nR p(z) log p(z)\nq(z)dz is the Kullback-Leibler (KL) diver-\ngence. Unlike gradient descent, the proximity term is therefore between\n\n17.3. Natural gradient descent\n409\nthe current distribution qwt and a candidate probability distribution\nqw. The above problem is intractable in general, as the KL may not\nhave a closed form. Nevertheless, its quadratic approximation can be\nshown (Amari, 1998) to admit a simple form,\nKL(qwt, qw) ≈1\n2⟨w −wt, ∇2\nFL(wt)(w −wt)⟩\nwhere we used the Fisher information matrix ∇2\nFL(w), studied in\nSection 9.3. Equipped with this quadratic approximation of the KL\ndivergence, natural gradient descent amounts to compute iterates as\nwt+1 := arg min\nw∈W\nL(wt) + ⟨∇L(wt), w −wt⟩\n+ 1\n2⟨w −wt, ∇2\nFL(wt)(w −wt)⟩+ ηt\n2 ∥w −wt∥2\n2,\nwhere a quadratic proximity-term was added to ensure a unique solution.\nThis is a srictly convex problem as ∇2\nFL(wt) is positive semi-definite.\nThe closed-form solution is\nwt+1 = wt −(∇2\nFL(wt) + ηt I)−1∇L(wt).\nBecause the Gauss-Newton and Fisher information matrices are equiv-\nalent when pθ is an exponential family distribution (Proposition 9.6),\nthe Gauss-Newton and natural gradient methods coincide in this case.\n17.3.2\nStochastic natural gradient descent\nIn practice, we may not have access to ∇L(wt) in closed form as it is\nan expectation over ρ. Moreover, ∇2\nFL(wt) may not be computable in\nclosed form either. To estimate the Fisher information matrix, we can\nuse that (see Section 9.3) using the shorthand θ := f(w, X),\n∇2\nFL(w) = EX∼ρXEY ∼pθ[∇L(w; X, Y ) ⊗∇L(w; X, Y )].\nWe can then build estimates g(wt; S) ≈∇L(wt, S) and F(wt; S′) ≈\n∇2\nFL(wt) for S sampled from ρ and S′ sampled from qwt(x, y) =\npX (x)ρθ(y). A stochastic natural gradient descent can then be imple-\nmented as\nwt+1 = wt −γt(F(wt; S′) + ηt I)−1g(wt; S),\n\n410\nSecond-order optimization\nwhere γt is a stepsize, possibly chosen by linesearch.\nIn deep learning, the product with the inverse Fisher or Gauss-\nNewton matrices can remain costly to compute. Several approximations\nhave been proposed, such as KFAC (Martens and Grosse, 2015; Botev\net al., 2017), which uses a computationally efficient structural approxi-\nmation to these matrices.\n17.4\nQuasi-Newton methods\n17.4.1\nBFGS\nA celebrated example of quasi-Newton method is the BFGS method\n(Broyden, 1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970), whose\nacronym follows from its author names. The rationale of the BFGS\nupdate stems once again from a variational viewpoint. We wish to\nbuild a simple quadratic model of the objective ht(w) = L(wt) +\n⟨∇L(wt), w −wt⟩+ 1\n2⟨w −wt, Qt(w −wt)⟩for some Qt built along\nthe iterations rather than taken as ∇2L(wt). One desirable property of\nsuch quadratic model would be that its gradients at consecutive iterates\nmatch the gradients of the original function, i.e., ∇ht(wt) = ∇L(wt)\nand ∇ht(wt−1) = ∇L(wt−1). A simpler condition, called the secant\ncondition consists in considering the differences of these vectors, that\nis, ensuring that\n∇ht(wt) −∇ht(wt−1) = ∇L(wt) −∇L(wt−1)\n⇐⇒Qt(wt −wt−1) = ∇L(wt) −∇L(wt−1)\n⇐⇒wt −wt−1 = Bt(∇L(wt) −∇L(wt−1)),\nfor Bt = (Qt)−1. Building Bt, a surrogate of the inverse of the Hessian\nsatisfying the secant equation, can then be done as\nBt+1 :=\n\u0010\nI −ρtst(yt)⊤\u0011\nBt \u0010\nI −ρtst(yt)⊤\u0011\n+ ρtst(st)⊤\nwhere\nst := wt+1 −wt\nyt := ∇L(wt+1) −∇L(wt)\nρt :=\n1\n⟨st, yt⟩.\n\n17.5. Approximate Hessian diagonal inverse preconditionners\n411\nA typical implementation of BFGS stores Bt ∈RP×P in memory, which\nis prohibitive when P is large.\n17.4.2\nLimited-memory BFGS\nIn practice, the limited-memory counterpart of BFGS, called LBFGS (Liu\nand Nocedal, 1989), is often preferred. The key observation of LBFGS\nis that we do not need to materialize Bt in memory: we only need\nto multiply it with the gradient ∇L(wt). That is, we can see Bt as\na linear map. Fortunately, the product between Bt and any vector v\ncan be computed efficiently if we store (s1, y1, ρ1), . . . , (st, yt, ρt) in\nmemory. In practice, a small history of past values is used to reduce\nmemory and computational cost. Because LBFGS has the benefits of\nsecond-order-like methods with much reduced cost, it has become a de-\nfacto algorithm, outperforming most other algorithms for medium-scale\nproblems without particular structure (Liu and Nocedal, 1989).\n17.5\nApproximate Hessian diagonal inverse preconditionners\nOne application of the approximations of the Hessian diagonal developed\nin Section 9.7 is to obtain cheap approximations of the Hessian diagonal\ninverse,\nBt := diag(|Ht\n11|−1, . . . , |Ht\nPP |−1).\nSuch a scaling would for instance be sufficient to make the quadratic\nexample presented in Fig. 17.1 work. Many optimization algorithms,\nincluding the popular ADAM, can be viewed as using a preconditioner\nthat approximates the inverse of the Hessian’s diagonal.\n17.6\nSummary\n• We reviewed Newton’s method, the Gauss-Newton method, natu-\nral gradient descent, quasi-Newton methods and preconditioning\nmethods.\n• We adopted a variational viewpoint, where the method’s next\niterate is computed as the solution of a trade-off between mini-\n\n412\nSecond-order optimization\nmizing an approximation of the function (linear, partially linear,\nquadratic) and a proximity term (squared Euclidean, KL).\n• All methods were shown to use iterates of the form\nwt+1 := wt −γtBt∇L(wt)\nbut have different trade-offs between the cost it takes to evaluate\nBt∇L(wt) and the richness of the information used about L.\n\n18\nDuality\nIn this chapter, we review duality principles in optimization.\n18.1\nDual norms\nWe introduce in this section dual norms, since they are useful in this\nbook.\nDefinition 18.1 (Dual norms). Given a norm ∥u∥, its dual is\n∥v∥∗:= max\n∥u∥≤1⟨u, v⟩.\nTherefore, the dual norm of ∥· ∥is the support function of the\nunit ball induced by the norm ∥· ∥,\nB∥·∥:= {u ∈RD : ∥u∥≤1}.\nWe give examples of pairs of dual norms below.\n413\n\n414\nDuality\nExample 18.1 (Dual norm of p-norms). The p-norm is defined by\n∥u∥p :=\n\n\nD\nX\nj=1\n|uj|p\n\n\n1/p\n.\nIts dual is ∥v∥q where q is such that 1\np + 1\nq = 1. For instance, the\ndual norm of the 2-norm is itself, since 1\n2 + 1\n2 = 1. The 1-norm and\nthe ∞-norm are dual of each other, since 1\n1 + 1\n∞= 1.\nThe definition of dual norm implies a generalization of Cauchy–Schwarz’s\ninequality: for all u, v ∈RD\n|⟨u, v⟩| ≤∥u∥∗∥v∥.\nSee, e.g., Beck (2017, Lemma 1.4).\nProposition 18.1 (Conjugate of norms and squared norms). We know\nthat the conjugate of the support function is the indicator function.\nTherefore, if f(u) = ∥u∥, then\nf∗(v) = ιB∥·∥(v) =\n\n\n\n0\nif ∥v∥∗≤1\n∞\notherwise\n.\nOn the other hand, if f(u) = 1\n2∥u∥2, then\nf∗(v) = 1\n2∥v∥2\n∗.\n18.2\nFenchel duality\nWe consider in this section standard objectives of the form\nmin\nw∈W L(w) := min\nw∈W ℓ(f(w)) + R(w),\nwhere f : W →M, ℓ: M →R and R: W →R. We first show that\nthe minimization of this objective, called the primal, can be lower\nbounded by a concave maximization objective, called the dual, even\nif the primal is nonconvex.\n\n18.2. Fenchel duality\n415\nProposition 18.2 (Weak duality). Let f : W →M (potentially non-\nlinear), ℓ: M →R (potentially nonconvex) and R: W →R (poten-\ntially nonconvex). Then\nmin\nw∈W ℓ(f(w)) + R(w) ≥max\nα∈M −Rf(α) −ℓ∗(−α),\nwhere we used the conjugate\nℓ∗(−α) := max\nθ∈M⟨−α, θ⟩−ℓ(θ)\nand the “generalized conjugate”\nRf(α) := max\nw∈W⟨α, f(w)⟩−R(w).\nMoreover, ℓ∗and Rf are both convex functions.\nWe emphasize that the result in Proposition 18.2 is fully general, in\nthe sense that it does not assume the linearity of f or the convexity of ℓ\nand R. The caveat, of course, is that Rf and ℓ∗are difficult to compute\nin general, if f is nonlinear, and if ℓand R are nonconvex.\nProof.\nmin\nw∈W ℓ(f(w)) + R(w)\n= min\nw∈W\nθ∈M\nℓ(θ) + R(w)\ns.t.\nθ = f(w)\n= min\nw∈W\nθ∈M\nmax\nα∈M ℓ(θ) + R(w) + ⟨α, θ −f(w)⟩\n≥max\nα∈M min\nw∈W\nθ∈M\nℓ(θ) + R(w) + ⟨α, θ −f(w)⟩\n= max\nα∈M min\nw∈W⟨α, −f(w)⟩+ R(w) + min\nθ∈M ℓ(θ) + ⟨α, θ⟩\n= max\nα∈M −max\nw∈W⟨α, f(w)⟩−R(w) −max\nθ∈M⟨−α, θ⟩−ℓ(θ)\n= max\nα∈M −Rf(α) −ℓ∗(−α).\n\n416\nDuality\nIn the case when f(w) = Aw, where A is a linear map, and when\nboth ℓand R are convex, we can state a much stronger result.\nProposition 18.3 (Strong duality). Let A be a linear map from W\nto M. Let ℓ: M →R and R: W →R be convex functions. Let A∗\ndenote the adjoint of A (Section 2.3). Then,\nmin\nw∈W ℓ(Aw) + R(w) = max\nα∈M −R∗(A∗α) −ℓ∗(−α).\nFurthermore, the primal solution satisfies\nw⋆∈arg max\nw∈W\n⟨Aα⋆, w⟩−R(w).\nWhen R is strictly convex, the primal solution is uniquely deter-\nmined by\nw⋆= ∇R∗(A∗α⋆).\nProof. Since f(w) = Aw, we have\nRf(α) := max\nw∈W⟨α, f(w)⟩−R(w)\n= max\nw∈W⟨α, Aw⟩−R(w)\n= max\nw∈W⟨A∗α, w⟩−R(w)\n= R∗(A∗α).\nFurthermore, the inequality in the proof of Proposition 18.2 is an\nequality, since the min max is that of a convex-concave function.\nThe maximization problem in Proposition 18.3 is called the Fenchel\ndual. By strong duality, the value of the maximum and the value of the\nminimum are equal. We can therefore choose to equivalently solve the\ndual instead of the primal. This can be advantageous when the space\nM is smaller than W.\nWe now apply the Fenchel dual to obtain the dual of regularized\nmulticlass linear classification.\n\n18.3. Bregman divergences\n417\nTable 18.1: Examples of loss conjugates. For regression losses (squared, absolute),\nwhere yi ∈RM, we define ti = ϕ(yi) = yi. For classification losses (logistic, per-\nceptron, hinge), where yi ∈[M], we define ti = ϕ(yi) = eyi. To simplify some\nexpressions, we defined the change of variable µi := yi −αi.\nℓi(θi)\nℓ∗\ni (−αi)\nSquared\n1\n2∥θi −ti∥2\n2\n1\n2∥αi∥2\n2 −⟨ti, αi⟩\nAbsolute\n∥θi −ti∥1\nι[−1,1]M (αi) −⟨ti, αi⟩\nLogistic\nLSE(θi) −⟨θ, ti⟩\n⟨µi, log µi⟩+ ι△M (µi)\nPerceptron\nmaxi∈[M] θi −θy\nι△M (µi)\nHinge\nmaxi∈[M][i ̸= y] + θi −θy\nι△M (µi) −⟨1 −ti, µi⟩\nExample 18.2 (Sum of separable loss functions). When the loss is\nℓ(θ) := PN\ni=1 ℓi(θi), where θ = Aw = (A1w, . . . , ANw) ∈MN and\nAi is a linear map from W to M, we obtain\nmin\nw∈W\nN\nX\ni=1\nℓi(Aiw) + R(w) = max\nα∈MN −R(A∗α) −\nN\nX\ni=1\nℓ∗\ni (−αi),\nwhere A∗α = (A∗\n1α1, . . . , A∗\nNαN). Typically, we define\nAiw := W xi,\nwhere W ∈RM×D is a reshaped version of w ∈W, xi ∈RD is a\ntraining sample, and M is the number of classes. In this case, we\nthen have\nA∗\ni αi = αix⊤\ni .\nExamples of loss function conjugates are given in Table 18.1.\n18.3\nBregman divergences\nBregman divergences are a measure of difference between two points.\nDefinition 18.2 (Bregman divergence). The Bregman divergence gen-\n\n418\nDuality\nerated by a differentiable convex function f : RD →R is\nBf(u, v) := f(u) −f(v) −⟨∇f(v), u −v⟩\n= ⟨∇f(v), v⟩−f(v) −[⟨∇f(v), u⟩−f(u)] ,\nwhere u, v ∈dom(f).\nIntuitively, the Bregman divergence is the difference between f(u)\nand its linearization u 7→f(v) + ⟨∇f(v), u −v⟩around v. This is\nillustrated in Fig. 18.1.\nExample 18.3 (Examples of Bregman divergences). If f(u) = 1\n2∥u∥2\n2,\nwhere dom(f) = RD, then\nBf(u, v) = 1\n2∥u −v∥2\n2,\nthe squared Euclidean distance. If f(u) = ⟨u, log u⟩, where\ndom(f) = RD\n+, then\nBf(u, v) =\nD\nX\nj=1\nuj log uj\nvj\n−\nD\nX\nj=1\nuj +\nD\nX\nj=1\nvj,\nthe (generalized) Kullback-Leibler divergence.\nProperties\nBregman divergences enjoy several useful properties.\nProposition 18.4 (Properties of Bregman divergences). Let f : RD →\nR be a differentiable convex function.\n1. Non-negativity: Bf(u, v) ≥0 for all u, v ∈dom(f).\n2. Positivity: Bf(u, v) = 0 if and only if u = v (when f is\nstrictly convex).\n3. Convexity: Bf(u, v) is convex in u.\n4. Dual-space form: Bf(u, v) = Bf∗(b, a), where b = ∇f(v) ∈\n\n18.3. Bregman divergences\n419\nu\nDf(u, v)\nv\nf(u)\nf(v) + ⟨∇f(v), u −v⟩\nFigure 18.1: The Bregman divergence generated by f is the difference between\nf(u) and its linearization around v.\ndom(f∗) and a = ∇f(u) ∈dom(f∗).\nProof. The properties follow immediately from the convexity of f(u).\n1. From Definition 15.6.\n2. From the unicity of minimizers.\n3. From the fact that u 7→Bf(u, v) is the sum of f(u) and a linear\nfunction of u.\nThe Bregman divergence can be used to define natural generaliza-\ntions of the Euclidean projection and proximal operators, reviewed in\nSection 16.3 and Section 16.4.\nDefinition 18.3 (Bregman proximal and projection operators). Let v ∈\ndom(f). The Bregman proximal operator is\nbproxf,g(v) :=\narg min\nu∈dom(f)∩dom(g)\nBf(u, v) + g(u).\nIn particular, the Bregman projection onto C ⊆dom(f) is\nbprojf,C(v) := arg min\nu∈C\nBf(u, v).\nIt turns out that these operators are intimately connected to the\ngradient mapping of the convex conjugate.\n\n420\nDuality\nProposition 18.5 (Link with conjugate’s gradient). If Ω= f + g,\nthen for all θ ∈dom(f∗)\n∇Ω∗(θ) = bproxf,g(∇f∗(θ)).\nIn particular, if Ω= f + ιC, then for all θ ∈dom(f∗)\n∇Ω∗(θ) = bprojf,C(∇f∗(θ)).\nWe give two examples below.\nExample 18.4 (Bregman projections on the simplex). If f(u) = 1\n2∥u∥2\n2,\nthen\nbprojf,△D(v) = arg min\nu∈△D\n1\n2∥u −v∥2\n2.\nIf f(u) = ⟨u, log u −1⟩, then\nbprojf,△D(v) = arg min\nu∈RD\n+\nKL(u, v) = softmax(θ),\nwhere v = ∇f∗(θ) = exp(θ).\nTherefore, the softmax can be seen as a projection onto the proba-\nbility simplex in the Kullback-Leilbler divergence sense!\n18.4\nFenchel-Young loss functions\nWe end this chapter with a brief review of the Fenchel-Young family of\nloss functions (Blondel et al., 2020), which includes all loss functions in\nTable 18.1.\nDefinition 18.4 (Fenchel-Young loss). The Fenchel-Young loss func-\ntion generated by Ωis\nℓΩ(θ, t) := Ω∗(θ) + Ω(t) −⟨θ, t⟩\nwhere θ ∈dom(Ω∗) and t ∈dom(Ω).\nTypically, we set θ = f(x, w), where f is a model prediction function\nwith parameters w and t = ϕ(y), where ϕ: Y →dom(Ω). For instance,\n\n18.5. Summary\n421\nsuppose we work with categorical outputs y ∈[M]. Then, we can set\nϕ(y) = ey, where ey is the one-hot encoding of y.\nThe important point to notice is that the Fenchel-Young loss is\ndefined over arguments in mixed spaces: θ belongs to the dual space,\nwhile t belongs to the primal space. In fact, the Fenchel-Young loss\nis intimately connected to the Bregman divergence, since BΩ(t, v) =\nΩ∗(θ) + Ω(t) −⟨θ, t⟩, if we set θ = ∇Ω(v). The key properties of\nFenchel-Young loss functions are summarized below.\nProposition 18.6 (Properties of Fenchel-Young loss functions).\n1. Non-negativity: ℓΩ(θ, t) ≥0 for all θ ∈dom(Ω∗) and\nt ∈dom(Ω).\n2. Positivity: ℓΩ(θ, t) = 0 if and only if ∇Ω∗(θ) = t, assuming\nΩis strictly convex.\n3. Convexity: ℓΩ(θ, t) is convex in θ (regardless of Ω) and in t\n(if Ωis convex)\n4. Relation with composite Bregman divergence:\n0 ≤\nBΩ(t, ∇Ω∗(θ))\n|\n{z\n}\npossibly nonconvex in θ\n≤ℓΩ(θ, t)\n|\n{z\n}\nconvex in θ\n.\nSee Blondel et al. (2020) for an in-depth study of more properties.\n18.5\nSummary\n• The convex conjugate serves as a powerful abstraction in Fenchal\nduality, decoupling the dual expression and function-specific\nterms.\n• The convex conjugate is also tightly connected to Bregman\ndivergences and can be used to derive the family of Fenchel-\nYoung loss functions, which can be seen as primal-dual Bregman\ndivergences.\n\nReferences\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, et al. (2016). “Tensorflow:\nLarge-scale machine learning on heterogeneous distributed systems”.\narXiv preprint arXiv:1603.04467.\nAbernethy, J., C. Lee, and A. Tewari. (2016). “Perturbation techniques\nin online learning and optimization”. Perturbations, Optimization,\nand Statistics. 233.\nAji, S. M. and R. J. McEliece. (2000). “The generalized distributive\nlaw”. IEEE transactions on Information Theory. 46(2): 325–343.\nAmari, S.-I. (1998). “Natural gradient works efficiently in learning”.\nNeural computation. 10(2): 251–276.\nBach, F., R. Jenatton, J. Mairal, G. Obozinski, et al. (2012). “Optimiza-\ntion with sparsity-inducing penalties”. Foundations and Trends® in\nMachine Learning. 4(1): 1–106.\nBall, K., E. A. Carlen, and E. H. Lieb. (2002). “Sharp uniform convexity\nand smoothness inequalities for trace norms”. Inequalities: Selecta\nof Elliott H. Lieb: 171–190.\nBall, W. W. R. (1960). A short account of the history of mathematics.\nCourier Corporation.\nBalog, M., N. Tripuraneni, Z. Ghahramani, and A. Weller. (2017). “Lost\nrelatives of the Gumbel trick”. In: International Conference on\nMachine Learning. PMLR. 371–379.\n422\n\nReferences\n423\nBarndorff-Nielsen, O. (2014). Information and exponential families: in\nstatistical theory. John Wiley & Sons.\nBaston, R. A. and Y. Nakatsukasa. (2022). “Stochastic diagonal esti-\nmation: probabilistic bounds and an improved algorithm”. arXiv\npreprint arXiv:2201.10684.\nBaum, L. E. and T. Petrie. (1966). “Statistical inference for probabilistic\nfunctions of finite state Markov chains”. The annals of mathematical\nstatistics. 37(6): 1554–1563.\nBaur, W. and V. Strassen. (1983). “The complexity of partial deriva-\ntives”. Theoretical computer science. 22(3): 317–330.\nBauschke Heinz, H. and L. Combettes Patrick. (2017). Convex Analysis\nand Monotone Operator Theory in Hilbert Spaces, 2011. 2nd ed.\n978–1.\nBaydin, A. G., B. A. Pearlmutter, A. A. Radul, and J. M. Siskind.\n(2018). “Automatic differentiation in machine learning: a survey”.\nJournal of Marchine Learning Research. 18: 1–43.\nBaydin, A. G., B. A. Pearlmutter, D. Syme, F. Wood, and P. Torr. (2022).\n“Gradients without backpropagation”. arXiv preprint arXiv:2202.08587.\nBeck, A. (2017). First-order methods in optimization. SIAM.\nBeck, A. and M. Teboulle. (2012). “Smoothing and first order methods:\nA unified framework”. SIAM Journal on Optimization. 22(2): 557–\n580.\nBecker, S. and Y. Le Cun. (1988). “Improving the convergence of back-\npropagation learning with second order methods”. In: Proceedings\nof the 1988 connectionist models summer school. 29–37.\nBekas, C., E. Kokiopoulou, and Y. Saad. (2007). “An estimator for the\ndiagonal of a matrix”. Applied numerical mathematics. 57(11-12):\n1214–1229.\nBergstra, J., O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-\njardins, J. Turian, D. Warde-Farley, and Y. Bengio. (2010). “Theano:\na CPU and GPU math expression compiler”. In: Proceedings of the\nPython for scientific computing conference (SciPy). Vol. 4. No. 3.\nAustin, TX. 1–7.\nBerthet, Q., M. Blondel, O. Teboul, M. Cuturi, J.-P. Vert, and F. Bach.\n(2020). “Learning with differentiable pertubed optimizers”. Advances\nin neural information processing systems. 33: 9508–9519.\n\n424\nReferences\nBlelloch, G. E. (1989). “Scans as primitive parallel operations”. IEEE\nTransactions on computers. 38(11): 1526–1538.\nBlondel, M. (2019). “Structured prediction with projection oracles”.\nAdvances in neural information processing systems. 32.\nBlondel, M., Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-\nLópez, F. Pedregosa, and J.-P. Vert. (2021). “Efficient and Modular\nImplicit Differentiation”. arXiv preprint arXiv:2105.15183.\nBlondel, M., A. F. Martins, and V. Niculae. (2020). “Learning with\nfenchel-young losses”. The Journal of Machine Learning Research.\n21(1): 1314–1382.\nBolte, J., R. Boustany, E. Pauwels, and B. Pesquet-Popescu. (2022).\n“On the complexity of nonsmooth automatic differentiation”. In:\nThe Eleventh International Conference on Learning Representations.\nBolte, J. and E. Pauwels. (2020). “A mathematical model for automatic\ndifferentiation in machine learning”. Advances in Neural Information\nProcessing Systems. 33: 10809–10819.\nBotev, A., H. Ritter, and D. Barber. (2017). “Practical Gauss-Newton\noptimisation for deep learning”. In: International Conference on\nMachine Learning. 557–565.\nBoumal, N. (2023). An introduction to optimization on smooth manifolds.\nCambridge University Press.\nBoyd, S. P. and L. Vandenberghe. (2004). Convex optimization. Cam-\nbridge university press.\nBradbury, J., R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D.\nMaclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-\nMilne, and Q. Zhang. (2018). JAX: composable transformations of\nPython+NumPy programs. Version 0.3.13. url: http://github.com/\ngoogle/jax.\nBraun, M. and M. Golubitsky. (1983). Differential equations and their\napplications. Vol. 2. Springer.\nBrockhoff, D., A. Auger, N. Hansen, D. V. Arnold, and T. Hohm.\n(2010). “Mirrored sampling and sequential selection for evolution\nstrategies”. In: Parallel Problem Solving from Nature, PPSN XI:\n11th International Conference, Kraków, Poland, September 11-15,\n2010, Proceedings, Part I 11. Springer. 11–21.\n\nReferences\n425\nBroyden, C. G. (1970). “The convergence of a class of double-rank\nminimization algorithms 1. general considerations”. IMA Journal\nof Applied Mathematics. 6(1): 76–90.\nBrucker, P. (1984). “An O(n) algorithm for quadratic knapsack prob-\nlems”. Operations Research Letters. 3(3): 163–166.\nButcher, J. C. (2016). Numerical methods for ordinary differential\nequations. John Wiley & Sons.\nCajori, F. (1993). A history of mathematical notations. Vol. 1. Courier\nCorporation.\nCéa, J. (1986). “Conception optimale ou identification de formes, calcul\nrapide de la dérivée directionnelle de la fonction coût”. M2AN-\nModélisation mathématique et analyse numérique. 20(3): 371–402.\nChaudhuri, S. and A. Solar-Lezama. (2010). “Smooth interpretation”.\nACM Sigplan Notices. 45(6): 279–291.\nChen, R. T., Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. (2018).\n“Neural ordinary differential equations”. Advances in neural infor-\nmation processing systems. 31.\nChen, X., N. Kayal, A. Wigderson, et al. (2011). “Partial derivatives in\narithmetic complexity and beyond”. Foundations and Trends® in\nTheoretical Computer Science. 6(1–2): 1–138.\nClarke, F. H., Y. S. Ledyaev, R. J. Stern, and P. R. Wolenski. (2008).\nNonsmooth analysis and control theory. Vol. 178. Springer Science\n& Business Media.\nClarke, F. H. (1975). “Generalized gradients and applications”. Trans-\nactions of the American Mathematical Society. 205: 247–262.\nCohn, D. L. (2013). Measure theory. Vol. 5. Springer.\nCondat, L. (2016). “Fast projection onto the simplex and the ℓ1 ball”.\nMathematical Programming. 158(1-2): 575–585.\nDangel, F., F. Kunstner, and P. Hennig. (2019). “Backpack: Packing\nmore into backprop”. arXiv preprint arXiv:1912.10985.\nDavis, J. Q., K. Choromanski, J. Varley, H. Lee, J.-J. Slotine, V.\nLikhosterov, A. Weller, A. Makadia, and V. Sindhwani. (2020).\n“Time dependence in non-autonomous neural odes”. arXiv preprint\narXiv:2005.01906.\nDeGroot, M. H. (1962). “Uncertainty, information, and sequential ex-\nperiments”. The Annals of Mathematical Statistics. 33(2): 404–419.\n\n426\nReferences\nDehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,\nA. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. (2023).\n“Scaling vision transformers to 22 billion parameters”. In: Interna-\ntional Conference on Machine Learning. PMLR. 7480–7512.\nDeisenroth, M. P., A. A. Faisal, and C. S. Ong. (2020). Mathematics\nfor machine learning. Cambridge University Press.\nDrusvyatskiy, D. and C. Paquette. (2019). “Efficiency of minimizing\ncompositions of convex functions and smooth maps”. Mathematical\nProgramming. 178: 503–558.\nDuchi, J. C., M. I. Jordan, M. J. Wainwright, and A. Wibisono. (2015).\n“Optimal rates for zero-order convex optimization: The power of two\nfunction evaluations”. IEEE Transactions on Information Theory.\n61(5): 2788–2806.\nDuchi, J. C., S. Shalev-Shwartz, Y. Singer, and T. Chandra. (2008).\n“Efficient projections onto the ℓ1-ball for learning in high dimensions”.\nIn: Proc. of ICML.\nEisner, J. (2016). “Inside-outside and forward-backward algorithms are\njust backprop (tutorial paper)”. In: Proceedings of the Workshop on\nStructured Prediction for NLP. 1–17.\nElsayed, M. and A. R. Mahmood. (2022). “HesScale: Scalable Compu-\ntation of Hessian Diagonals”. arXiv preprint arXiv:2210.11639.\nEpperly, E. N., J. A. Tropp, and R. J. Webber. (2023). “XTrace: Making\nthe most of every sample in stochastic trace estimation”. arXiv\npreprint arXiv:2301.07825.\nFlanders, H. (1973). “Differentiation under the integral sign”. The\nAmerican Mathematical Monthly. 80(6): 615–627.\nFleming, W. H. and R. W. Rishel. (2012). Deterministic and stochastic\noptimal control. Vol. 1. Springer Science & Business Media.\nFletcher, R. (1970). “A new approach to variable metric algorithms”.\nThe computer journal. 13(3): 317–322.\nFoerster, J., G. Farquhar, M. Al-Shedivat, T. Rocktäschel, E. Xing,\nand S. Whiteson. (2018). “Dice: The infinitely differentiable monte\ncarlo estimator”. In: International Conference on Machine Learning.\nPMLR. 1529–1538.\nForney, G. D. (1973). “The viterbi algorithm”. Proceedings of the IEEE.\n61(3): 268–278.\n\nReferences\n427\nFranceschi, L., M. Donini, P. Frasconi, and M. Pontil. (2017). “For-\nward and reverse gradient-based hyperparameter optimization”. In:\nInternational Conference on Machine Learning. PMLR. 1165–1173.\nFrey, B. J., F. R. Kschischang, H.-A. Loeliger, and N. Wiberg. (1997).\n“Factor graphs and algorithms”. In: Proceedings of the Annual Aller-\nton Conference on Communication Control and Computing. Vol. 35.\nCiteseer. 666–680.\nFrigyik, B. A., S. Srivastava, and M. R. Gupta. (2008). “An introduction\nto functional derivatives”. Dept. Electr. Eng., Univ. Washington,\nSeattle, WA, Tech. Rep. 1.\nFrostig, R., M. J. Johnson, D. Maclaurin, A. Paszke, and A. Radul.\n(2021). “Decomposing reverse-mode automatic differentiation”. arXiv\npreprint arXiv:2105.09469.\nGautschi, W. (2011). Numerical analysis. Springer Science & Business\nMedia.\nGetreuer, P. (2013). “A survey of Gaussian convolution algorithms”.\nImage Processing On Line. 2013: 286–310.\nGeweke, J. (1988). “Antithetic acceleration of Monte Carlo integration\nin Bayesian inference”. Journal of Econometrics. 38(1-2): 73–89.\nGholaminejad, A., K. Keutzer, and G. Biros. (2019). “ANODE: Uncon-\nditionally Accurate Memory-Efficient Gradients for Neural ODEs”.\nIn: International Joint Conferences on Artificial Intelligence.\nGini, C. (1912). “Variabilità e mutabilità”. Reprinted in Memorie di\nmetodologica statistica (Ed. Pizetti E, Salvemini, T). Rome: Libreria\nEredi Virgilio Veschi.\nGirard, A. (1989). “A fast ‘Monte-Carlo cross-validation’procedure for\nlarge least squares problems with noisy data”. Numerische Mathe-\nmatik. 56: 1–23.\nGoldfarb, D. (1970). “A family of variable-metric methods derived by\nvariational means”. Mathematics of computation. 24(109): 23–26.\nGomez, A. N., M. Ren, R. Urtasun, and R. B. Grosse. (2017). “The\nreversible residual network: Backpropagation without storing acti-\nvations”. Advances in neural information processing systems. 30.\nGraves, A., G. Wayne, and I. Danihelka. (2014). “Neural turing ma-\nchines”. arXiv preprint arXiv:1410.5401.\n\n428\nReferences\nGreig, D. M., B. T. Porteous, and A. H. Seheult. (1989). “Exact max-\nimum a posteriori estimation for binary images”. Journal of the\nRoyal Statistical Society Series B: Statistical Methodology. 51(2):\n271–279.\nGriewank, A. (1992). “Achieving logarithmic growth of temporal and spa-\ntial complexity in reverse automatic differentiation”. Optimization\nMethods and Software. 1(1): 35–54. doi: 10.1080/10556789208805505.\nGriewank, A. (2003). “A mathematical view of automatic differentia-\ntion”. Acta Numerica. 12: 321–398.\nGriewank, A. (2012). “Who invented the reverse mode of differentiation”.\nDocumenta Mathematica, Extra Volume ISMP. 389400.\nGriewank, A. and A. Walther. (2008). Evaluating derivatives: principles\nand techniques of algorithmic differentiation. SIAM.\nGrimm, J., L. Pottier, and N. Rostaing-Schmidt. (1996). “Optimal time\nand minimum space-time product for reversing a certain class of\nprograms”. PhD thesis. INRIA.\nGrünwald, P. D. and A. P. Dawid. (2004). “Game theory, maximum\nentropy, minimum discrepancy and robust Bayesian decision theory”.\nAnnals of Statistics: 1367–1433.\nHallman, E., I. C. Ipsen, and A. K. Saibaba. (2023). “Monte Carlo\nmethods for estimating the diagonal of a real symmetric matrix”.\nSIAM Journal on Matrix Analysis and Applications. 44(1): 240–269.\nHe, K., X. Zhang, S. Ren, and J. Sun. (2016). “Deep residual learning\nfor image recognition”. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 770–778.\nHelfrich, K., D. Willmott, and Q. Ye. (2018). “Orthogonal recurrent\nneural networks with scaled Cayley transform”. In: International\nConference on Machine Learning. PMLR. 1969–1978.\nHestenes, M. R., E. Stiefel, et al. (1952). Methods of conjugate gradients\nfor solving linear systems. Vol. 49. No. 1. NBS Washington, DC.\nHewitt, E. (1948). “Rings of real-valued continuous functions. I”. Trans-\nactions of the American Mathematical Society. 64(1): 45–99.\nHida, T. and M. Hitsuda. (1976). Gaussian processes. Vol. 120. American\nMathematical Soc.\n\nReferences\n429\nHiriart-Urruty, J.-B. and C. Lemaréchal. (1993). Convex analysis and\nminimization algorithms II. Vol. 305. Springer science & business\nmedia.\nHutchinson, M. F. (1989). “A stochastic estimator of the trace of the\ninfluence matrix for Laplacian smoothing splines”. Communications\nin Statistics-Simulation and Computation. 18(3): 1059–1076.\nJaggi, M. (2013). “Revisiting Frank-Wolfe: Projection-free sparse convex\noptimization”. In: International conference on machine learning.\nPMLR. 427–435.\nJang, E., S. Gu, and B. Poole. (2016). “Categorical reparameterization\nwith gumbel-softmax”. arXiv preprint arXiv:1611.01144.\nJayaram, B. and M. Baczynski. (2008). Fuzzy Implications. Vol. 231.\nSpringer Science & Business Media.\nKakade, S., S. Shalev-Shwartz, A. Tewari, et al. (2009). “On the duality\nof strong convexity and strong smoothness: Learning appl ications\nand matrix regularization”. Tech report. 2(1): 35.\nKarpathy, A. (2017). “Software 2.0”.\nKelley, C. T. (1995). Iterative methods for linear and nonlinear equations.\nSIAM.\nKingma, D. P. and J. Ba. (2014). “Adam: A method for stochastic\noptimization”. arXiv preprint arXiv:1412.6980.\nKingma, D. P. and M. Welling. (2013). “Auto-encoding variational\nbayes”. arXiv preprint arXiv:1312.6114.\nKlir, G. and B. Yuan. (1995). Fuzzy sets and fuzzy logic. Vol. 4. Prentice\nhall New Jersey.\nKobyzev, I., S. Prince, and M. A. Brubaker. (2019). “Normalizing flows:\nIntroduction and ideas”. stat. 1050: 25.\nKreikemeyer, J. N. and P. Andelfinger. (2023). “Smoothing methods\nfor automatic differentiation across conditional branches”. IEEE\nAccess.\nKrieken, E., J. Tomczak, and A. Ten Teije. (2021). “Storchastic: A frame-\nwork for general stochastic automatic differentiation”. Advances in\nNeural Information Processing Systems. 34: 7574–7587.\nKunstner, F., P. Hennig, and L. Balles. (2019). “Limitations of the em-\npirical Fisher approximation for natural gradient descent”. Advances\nin neural information processing systems. 32.\n\n430\nReferences\nLafferty, J., A. McCallum, and F. C. Pereira. (2001). “Conditional\nrandom fields: Probabilistic models for segmenting and labeling\nsequence data”.\nLan, G. (2012). “An optimal method for stochastic composite optimiza-\ntion”. Mathematical Programming. 133(1-2): 365–397.\nLeCun, Y. (1988). “A theoretical framework for back-propagation”. In:\nProceedings of the 1988 connectionist models summer school. Vol. 1.\n21–28.\nLeCun, Y. (2018). “Deep Learning est mort. Vive Differentiable Pro-\ngramming!”\nLevenberg, K. (1944). “A method for the solution of certain non-linear\nproblems in least squares”. Quarterly of applied mathematics. 2(2):\n164–168.\nLiu, D. C. and J. Nocedal. (1989). “On the limited memory method for\nlarge scale optimization”. Mathematical Programming. 45: 503–528.\nLiu, H., Z. Li, D. Hall, P. Liang, and T. Ma. (2023). “Sophia: A Scal-\nable Stochastic Second-order Optimizer for Language Model Pre-\ntraining”. arXiv preprint arXiv:2305.14342.\nLoeliger, H.-A. (2004). “An introduction to factor graphs”. IEEE Signal\nProcessing Magazine. 21(1): 28–41.\nLoshchilov, I. and F. Hutter. (2016). “SGDR: Stochastic gradient de-\nscent with warm restarts”. In: International Conference on Learning\nRepresentations.\nLucet, Y. (1997). “Faster than the fast Legendre transform, the linear-\ntime Legendre transform”. Numerical Algorithms. 16: 171–185.\nMaclaurin, D., D. Duvenaud, and R. P. Adams. (2015). “Autograd:\nEffortless gradients in numpy”. In: ICML 2015 AutoML workshop.\nVol. 238. No. 5.\nMaddison, C. J., A. Mnih, and Y. W. Teh. (2016). “The concrete\ndistribution: A continuous relaxation of discrete random variables”.\narXiv preprint arXiv:1611.00712.\nMarquardt, D. W. (1963). “An algorithm for least-squares estimation\nof nonlinear parameters”. Journal of the society for Industrial and\nApplied Mathematics. 11(2): 431–441.\n\nReferences\n431\nMartens, J. (2020). “New insights and perspectives on the natural\ngradient method”. Journal of Machine Learning Research. 21(1):\n5776–5851.\nMartens, J. and R. Grosse. (2015). “Optimizing neural networks with\nKronecker-factored approximate curvature”. In: International con-\nference on machine learning. 2408–2417.\nMartins, A. and R. Astudillo. (2016). “From softmax to sparsemax: A\nsparse model of attention and multi-label classification”. In: Inter-\nnational conference on machine learning. PMLR. 1614–1623.\nMartins, J. R., P. Sturdza, and J. J. Alonso. (2003). “The complex-\nstep derivative approximation”. ACM Transactions on Mathematical\nSoftware (TOMS). 29(3): 245–262.\nMeent, J.-W. van de, B. Paige, H. Yang, and F. Wood. (2018). “An intro-\nduction to probabilistic programming”. arXiv preprint arXiv:1809.10756.\nMensch, A. and M. Blondel. (2018). “Differentiable dynamic program-\nming for structured prediction and attention”. In: International\nConference on Machine Learning. PMLR. 3462–3471.\nMesserer, F., K. Baumgärtner, and M. Diehl. (2021). “Survey of sequen-\ntial convex programming and generalized Gauss-Newton methods”.\nESAIM: Proceedings and Surveys. 71: 64–88.\nMeyer, R. A., C. Musco, C. Musco, and D. P. Woodruff. (2021).\n“Hutch++: Optimal stochastic trace estimation”. In: Symposium on\nSimplicity in Algorithms (SOSA). SIAM. 142–155.\nMichelot, C. (1986). “A finite algorithm for finding the projection of a\npoint onto the canonical simplex of Rn”. Journal of Optimization\nTheory and Applications. 50(1): 195–200.\nMohamed, S., M. Rosca, M. Figurnov, and A. Mnih. (2020). “Monte\ncarlo gradient estimation in machine learning”. The Journal of\nMachine Learning Research. 21(1): 5183–5244.\nMohri, M., F. Pereira, and M. Riley. (2008). “Speech recognition with\nweighted finite-state transducers”. Springer Handbook of Speech\nProcessing: 559–584.\nMorgenstern, J. (1985). “How to compute fast a function and all its\nderivatives: A variation on the theorem of Baur-Strassen”. ACM\nSIGACT News. 16(4): 60–62.\n\n432\nReferences\nMorrey Jr, C. B. (2009). Multiple integrals in the calculus of variations.\nSpringer Science & Business Media.\nMurphy, K. P. (2022). Probabilistic Machine Learning: An introduction.\nMIT Press. url: http://probml.github.io/book1.\nMurphy, K. P. (2023). Probabilistic Machine Learning: Advanced Topics.\nMIT Press. url: http://probml.github.io/book2.\nMutze, U. (2013). “An asynchronous leapfrog method II”. arXiv preprint\narXiv:1311.6602.\nNemirovski, A. and D. Yudin. (1983). “Problem complexity and method\nefficiency in optimization”.\nNesterov, Y. (2005). “Smooth minimization of non-smooth functions”.\nMathematical programming. 103: 127–152.\nNesterov, Y. (2007). “Modified Gauss–Newton scheme with worst case\nguarantees for global performance”. Optimisation methods and soft-\nware. 22(3): 469–483.\nNesterov, Y. (2018). Lectures on convex optimization. Vol. 137. Springer.\nNesterov, Y. and V. Spokoiny. (2017). “Random gradient-free minimiza-\ntion of convex functions”. Foundations of Computational Mathemat-\nics. 17: 527–566.\nPapamakarios, G., E. Nalisnick, D. J. Rezende, S. Mohamed, and\nB. Lakshminarayanan. (2021). “Normalizing flows for probabilistic\nmodeling and inference”. The Journal of Machine Learning Research.\n22(1): 2617–2680.\nParikh, N., S. Boyd, et al. (2014). “Proximal algorithms”. Foundations\nand trends® in Optimization. 1(3): 127–239.\nPaszke, A., S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T.\nKilleen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E.\nYang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. (2019). “PyTorch: An Imperative\nStyle, High-Performance Deep Learning Library”. In: Advances in\nNeural Information Processing Systems 32. 8024–8035.\nPaulus, M., D. Choi, D. Tarlow, A. Krause, and C. J. Maddison. (2020).\n“Gradient estimation with stochastic softmax tricks”. Advances in\nNeural Information Processing Systems. 33: 5691–5704.\n\nReferences\n433\nPetersen, F., C. Borgelt, H. Kuehne, and O. Deussen. (2021). “Learning\nwith algorithmic supervision via continuous relaxations”. Advances\nin Neural Information Processing Systems. 34: 16520–16531.\nPeyré, G. (2020). “Mathematical foundations of data sciences”. Rn. 1:\n2.\nPeyré, G. and M. Cuturi. (2019). “Computational optimal transport:\nWith applications to data science”. Foundations and Trends® in\nMachine Learning. 11(5-6): 355–607.\nPollock, S. and L. G. Rebholz. (2021). “Anderson acceleration for con-\ntractive and noncontractive operators”. IMA Journal of Numerical\nAnalysis. 41(4): 2841–2872.\nPolyak, B. (1963). “Gradient methods for the minimisation of function-\nals”. USSR Computational Mathematics and Mathematical Physics.\n3(4): 864–878.\nPolyak, B. T. (1964). “Some methods of speeding up the convergence\nof iteration methods”. Ussr computational mathematics and mathe-\nmatical physics. 4(5): 1–17.\nPontryagin, L. S. (1985). “The mathematical theory of optimal processes\nand differential games”. Trudy Mat. Inst. Steklov. 169: 119–158.\nRabiner, L. R. (1989). “A tutorial on hidden Markov models and selected\napplications in speech recognition”. Proceedings of the IEEE. 77(2):\n257–286.\nRademacher, H. (1919). “Über partielle und totale differenzierbarkeit\nvon Funktionen mehrerer Variabeln und über die Transformation\nder Doppelintegrale”. Mathematische Annalen. 79(4): 340–359.\nRadul, A., A. Paszke, R. Frostig, M. Johnson, and D. Maclaurin. (2022).\n“You only linearize once: Tangents transpose to gradients”. arXiv\npreprint arXiv:2204.10923.\nRecht, B. (2016). “Mates of Costate”.\nRecht, B. and R. Frostig. (2017). “Nesterov’s Punctuated Equilibrium”.\nRezende, D. J., S. Mohamed, and D. Wierstra. (2014). “Stochastic back-\npropagation and approximate inference in deep generative models”.\nIn: International conference on machine learning. PMLR. 1278–\n1286.\nRockafellar, R. T. and R. J.-B. Wets. (2009). Variational analysis.\nVol. 317. Springer Science & Business Media.\n\n434\nReferences\nRodriguez, O. H. and J. M. Lopez Fernandez. (2010). “A semiotic\nreflection on the didactics of the chain rule”. The Mathematics\nEnthusiast. 7(2): 321–332.\nRoulet, V. and Z. Harchaoui. (2022). “Differentiable programming à la\nMoreau”. In: ICASSP 2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE. 3498–\n3502.\nSaad, Y. and M. H. Schultz. (1986). “GMRES: A generalized minimal\nresidual algorithm for solving nonsymmetric linear systems”. SIAM\nJournal on scientific and statistical computing. 7(3): 856–869.\nSalimans, T., J. Ho, X. Chen, S. Sidor, and I. Sutskever. (2017). “Evo-\nlution strategies as a scalable alternative to reinforcement learning”.\narXiv preprint arXiv:1703.03864.\nSander, M. E., P. Ablin, M. Blondel, and G. Peyré. (2021a). “Momentum\nresidual neural networks”. In: International Conference on Machine\nLearning. PMLR. 9276–9287.\nSander, M. E., P. Ablin, M. Blondel, and G. Peyré. (2021b). “Momentum\nresidual neural networks”. In: International Conference on Machine\nLearning. PMLR. 9276–9287.\nSatterthwaite, F. (1942). “Generalized poisson distribution”. The Annals\nof Mathematical Statistics. 13(4): 410–417.\nSchlag, I., K. Irie, and J. Schmidhuber. (2021). “Linear transformers\nare secretly fast weight programmers”. In: International Conference\non Machine Learning. PMLR. 9355–9366.\nSchölkopf, B. and A. J. Smola. (2002). Learning with kernels: support\nvector machines, regularization, optimization, and beyond. MIT\npress.\nSchulman, J., N. Heess, T. Weber, and P. Abbeel. (2015). “Gradient\nestimation using stochastic computation graphs”. Advances in neural\ninformation processing systems. 28.\nSchwartz, J. (1954). “The formula for change in variables in a multiple\nintegral”. The American Mathematical Monthly. 61(2): 81–85.\nSchwarz, H. (1873). “Communication”. Archives des Sciences Physiques\net Naturelles. 48: 38–44.\nSengupta, S., M. J. Harris, M. Garland, and J. D. Owens. (2010).\n“Efficient Parallel Scan Algorithms for Manycore GPUs.”\n\nReferences\n435\nShanno, D. F. (1970). “Conditioning of quasi-Newton methods for\nfunction minimization”. Mathematics of computation. 24(111): 647–\n656.\nShannon, C. E. (1948). “A mathematical theory of communication”.\nThe Bell system technical journal. 27(3): 379–423.\nShawe-Taylor, J. and N. Cristianini. (2004). Kernel methods for pattern\nanalysis. Cambridge university press.\nSquire, W. and G. Trapp. (1998). “Using complex variables to estimate\nderivatives of real functions”. SIAM review. 40(1): 110–112.\nStoer, J., R. Bulirsch, R. Bartels, W. Gautschi, and C. Witzgall. (1980).\nIntroduction to numerical analysis. Vol. 1993. Springer.\nStumm, P. and A. Walther. (2010). “New algorithms for optimal online\ncheckpointing”. SIAM Journal on Scientific Computing. 32(2): 836–\n854.\nSutskever, I., J. Martens, G. Dahl, and G. Hinton. (2013). “On the\nimportance of initialization and momentum in deep learning”. In:\nInternational conference on machine learning. PMLR. 1139–1147.\nSutton, C., A. McCallum, et al. (2012). “An introduction to conditional\nrandom fields”. Foundations and Trends® in Machine Learning. 4(4):\n267–373.\nSutton, R. S., D. McAllester, S. Singh, and Y. Mansour. (1999). “Policy\ngradient methods for reinforcement learning with function approxi-\nmation”. Advances in neural information processing systems. 12.\nTaylor, M. (2002). “Differential forms and the change of variable for-\nmula for multiple integrals”. Journal of mathematical analysis and\napplications. 268(1): 378–383.\nTibshirani, R. (1996). “Regression shrinkage and selection via the lasso”.\nJournal of the Royal Statistical Society: Series B (Methodological).\n58(1): 267–288.\nTignol, J.-P. (2015). Galois’ theory of algebraic equations. World Scien-\ntific Publishing Company.\nTsallis, C. (1988). “Possible generalization of Boltzmann-Gibbs statis-\ntics”. Journal of statistical physics. 52: 479–487.\nvan Krieken, E. (2024). “Optimisation in Neurosymbolic Learning Sys-\ntems”. PhD thesis. Vrije Universiteit Amsterdam.\n\n436\nReferences\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin. (2017). “Attention is all you need”.\nAdvances in neural information processing systems. 30.\nVaswani, S., A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S.\nLacoste-Julien. (2019). “Painless stochastic gradient: Interpolation,\nline-search, and convergence rates”. Advances in neural information\nprocessing systems. 32.\nVerdu, S. and H. V. Poor. (1987). “Abstract dynamic programming\nmodels under commutativity conditions”. SIAM Journal on Control\nand Optimization. 25(4): 990–1006.\nVicol, P., L. Metz, and J. Sohl-Dickstein. (2021). “Unbiased gradient\nestimation in unrolled computation graphs with persistent evolu-\ntion strategies”. In: International Conference on Machine Learning.\nPMLR. 10553–10563.\nVirtanen, P., R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,\nD. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright,\nS. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov,\nA. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, İ. Polat,\nY. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,\nR. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M.\nArchibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy\n1.0 Contributors. (2020). “SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python”. Nature Methods. 17: 261–272.\nViterbi, A. (1967). “Error bounds for convolutional codes and an asymp-\ntotically optimum decoding algorithm”. IEEE transactions on In-\nformation Theory. 13(2): 260–269.\nVorst, H. A. v. d. and H. A. van der Vorst. (1992). “Bi-CGSTAB: A\nFast and Smoothly Converging Variant of Bi-CG for the Solution of\nNonsymmetric Linear Systems”. SIAM Journal on Scientific and\nStatistical Computing. 13(2): 631–644. url: http://dx.doi.org/10.\n1137/0913035.\nWainwright, M. J. and M. I. Jordan. (2008). “Graphical models, expo-\nnential families, and variational inference”. Foundations and Trends®\nin Machine Learning. 1(1–2): 1–305.\n\nReferences\n437\nWang, Q., P. Moin, and G. Iaccarino. (2009). “Minimal repetition\ndynamic checkpointing algorithm for unsteady adjoint calculation”.\nSIAM Journal on Scientific Computing. 31(4): 2549–2567.\nWei, C., S. Kakade, and T. Ma. (2020). “The implicit and explicit\nregularization effects of dropout”. In: International conference on\nmachine learning. PMLR. 10181–10192.\nWerbos, P. J. (1990). “Backpropagation through time: what it does and\nhow to do it”. Proceedings of the IEEE. 78(10): 1550–1560.\nWerbos, P. J. (1994). The roots of backpropagation: from ordered deriva-\ntives to neural networks and political forecasting. Vol. 1. John Wiley\n& Sons.\nWright, S. and J. Nocedal. (1999). “Numerical optimization”. Springer\nScience. 35(67-68): 7.\nXu, P., F. Roosta, and M. W. Mahoney. (2020). “Second-order opti-\nmization for non-convex machine learning: An empirical study”. In:\nProceedings of the 2020 SIAM International Conference on Data\nMining. SIAM. 199–207.\nYuan, M. and Y. Lin. (2006). “Model selection and estimation in re-\ngression with grouped variables”. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology). 68(1): 49–67.\nZhang, A., Z. C. Lipton, M. Li, and A. J. Smola. (2021). “Dive into\ndeep learning”. arXiv preprint arXiv:2106.11342.\nZhou, X. (2018). “On the fenchel duality between strong convexity and\nlipschitz continuou s gradient”. arXiv preprint arXiv:1803.06573.\nZhuang, J., N. C. Dvornek, S. Tatikonda, and J. S. Duncan. (2021).\n“Mali: A memory efficient and reverse accurate integrator for neural\nodes”. arXiv preprint arXiv:2102.04668.\nZiegler, D. M., N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\nP. Christiano, and G. Irving. (2019). “Fine-tuning language models\nfrom human preferences”. arXiv preprint arXiv:1909.08593.",
    "pdf_filename": "The Elements of Differentiable Programming.pdf"
}