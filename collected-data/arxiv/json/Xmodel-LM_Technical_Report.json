{
    "title": "Xmodel-LM Technical Report",
    "abstract": "WeintroduceXmodel-LM,acompactandefficient1.1Blanguagemodelpretrained on around 2 trillion tokens. Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LMexhibitsremarkableperformancedespiteitssmallersize. Itnotably surpasses existing open-source language models of similar scale. Our model checkpoints and code are publicly accessible on GitHub at https://github. com/XiaoduoAILab/XmodelLM. 1 Introduction Largelanguagemodels(LLMs)havedemonstratedremarkableperformanceacrossvariousnatural languagetasks, oftenrequiringminimalexamplesofnaturallanguageinstructions[Brownetal., 2020],therebyalleviatingthenecessityforextensivefeatureengineering. Attheheartofthisprogresslieslanguagemodeling,typicallyformulatedasanunsuperviseddis- tribution estimate from sequences of symbols (s ,...,s ) within a variable-length sequence x. 1 n−1 Leveragingthenaturalsequentialorderoflanguages,thejointprobabilityoversymbolsiscommonly factorizedintoaproductofconditionalprobabilities: n (cid:89) p(x)= p(s |s ,...,s ) n 1 n−1 i=1 Recentadvancementsinnaturallanguageprocessing(NLP)havelargelystemmedfromthescalingup oflanguagemodelsizes,drivenbytheobservedfunctionalrelationshipbetweenmodelperformance and size [Henighan et al., 2020]. However, the accompanying rise in operational costs poses a significanthurdletotheirwidespreadadoption. Inthispaper,weintroduceXmodel-LM,acompactvision-languageassistantpoweredbyarelatively smalllanguagemodel.Remarkably,Xmodel-LMachievesperformancecomparabletostate-of-the-art modelsofsimilarscaleacrossnumerousLLMbenchmarktests,showcasingitspotentialforawide arrayofpracticaltasks. 2 Pretraining This chapter details the pretraining process of Xmodel-LM. First, we introduce the sources and compositionofourcorpus,aswellasourpreprocessingmethods. Second,wedescribetheconstruc- tionofourcustomizedtokenizer. Finally,wedetailthemodelarchitectureandtrainingparameter configurations. 4202 voN 91 ]LC.sc[ 5v65820.6042:viXra",
    "body": "Xmodel-LM Technical Report\nWangYichuan LiuYang YanYu WangQun HuangXucheng JiangLing\nXiaoduoAI\n{wangyichuan,liuyangfoam,yanyu,wangqun}@xiaoduotech.com\nAbstract\nWeintroduceXmodel-LM,acompactandefficient1.1Blanguagemodelpretrained\non around 2 trillion tokens. Trained on our self-built dataset (Xdata), which\nbalances Chinese and English corpora based on downstream task optimization,\nXmodel-LMexhibitsremarkableperformancedespiteitssmallersize. Itnotably\nsurpasses existing open-source language models of similar scale. Our model\ncheckpoints and code are publicly accessible on GitHub at https://github.\ncom/XiaoduoAILab/XmodelLM.\n1 Introduction\nLargelanguagemodels(LLMs)havedemonstratedremarkableperformanceacrossvariousnatural\nlanguagetasks, oftenrequiringminimalexamplesofnaturallanguageinstructions[Brownetal.,\n2020],therebyalleviatingthenecessityforextensivefeatureengineering.\nAttheheartofthisprogresslieslanguagemodeling,typicallyformulatedasanunsuperviseddis-\ntribution estimate from sequences of symbols (s ,...,s ) within a variable-length sequence x.\n1 n−1\nLeveragingthenaturalsequentialorderoflanguages,thejointprobabilityoversymbolsiscommonly\nfactorizedintoaproductofconditionalprobabilities:\nn\n(cid:89)\np(x)= p(s |s ,...,s )\nn 1 n−1\ni=1\nRecentadvancementsinnaturallanguageprocessing(NLP)havelargelystemmedfromthescalingup\noflanguagemodelsizes,drivenbytheobservedfunctionalrelationshipbetweenmodelperformance\nand size [Henighan et al., 2020]. However, the accompanying rise in operational costs poses a\nsignificanthurdletotheirwidespreadadoption.\nInthispaper,weintroduceXmodel-LM,acompactvision-languageassistantpoweredbyarelatively\nsmalllanguagemodel.Remarkably,Xmodel-LMachievesperformancecomparabletostate-of-the-art\nmodelsofsimilarscaleacrossnumerousLLMbenchmarktests,showcasingitspotentialforawide\narrayofpracticaltasks.\n2 Pretraining\nThis chapter details the pretraining process of Xmodel-LM. First, we introduce the sources and\ncompositionofourcorpus,aswellasourpreprocessingmethods. Second,wedescribetheconstruc-\ntionofourcustomizedtokenizer. Finally,wedetailthemodelarchitectureandtrainingparameter\nconfigurations.\n4202\nvoN\n91\n]LC.sc[\n5v65820.6042:viXra\n2.1 TrainingData\nData sourcing: In the process of constructing the training corpus and allocating weights, our\nprimary objective is to ensure the quality and diversity of the training data. Our original dataset\nprimarily consists of aggregated training data from other LLMs, such as Redpajama [Computer,\n2023],subsetsofthePile[Gaoetal.,2020]andStarCoder[Lietal.,2023]. Toaddressdeficiencies\ninthedistributionofbookandmathematicaldatawithinthetrainingdatadistribution,wehavealso\nincorporatedFanFics1 andOpenWebMath[Pasteretal.,2023]. Additionally, wehaveaddedthe\nChinesedatasourcePTD[Wangetal.,2024]andWanJuan[Heetal.,2023]toimbueourmodelwith\nacertainlevelofproficiencyinChinese.\nDataprocessing: Wearecommittedtoensurethequalityofthedataandreducingitsredundancy.\nWefirstemployheuristicmethodssuchasparagraphlengthandpunctuationratioforinitialfiltering.\nSubsequently,weutilizea5-gramKneser-NeymodelbasedonKenLMLibrary[Heafield,2011]to\ncomputetextperplexityforfurtherqualityfiltering. Inthenextstage,weemployealocality-sensitive\nhashingmethodbasedonSimHashtodeduplicatethetrainingdata. Tobalancededuplicationquality\nand efficiency, we implement a bucketing strategy on the entire dataset, enabling the deduplica-\ntion process to scale efficiently across large datasets. Finally, we tokenize all datasets using our\ncustom-trainedtokenizer,anddesigneddifferentsamplingweightsforthedatasetsbasedontheir\ncharacteristics,asshowninTable1.\nDataSource Dataset NumTokens SamplingWeight Epochs Category Language\nArxiv 31,336,679,261 0.0160 1 Academic English\nBook 29,633,248,538 0.0300 1 Book English\nC4 192,696,661,887 0.1000 1 Web English\nRedpajama CommonCrawl 1,251,868,330,446 0.5600 0.88 Web English\nGithub 59,063,773,003 0.0150 0.5 Code English\nStackexchange 22,728,030,774 0.0174 1.5 Social English\nWikipedia 34,312,919,854 0.0520 3 Academic English\nBookCorpus 562,392,085 0.0006 2 Book English\nPile\nPubMed 17,698,877,602 0.0100 1 Academic English\nAMPS AMPS 269,936,326 0.0003 2 Math English\nFanFics FanFics 1,803,437,344 0.0020 2 Book English\nOpenWebMath OpenWebMath 7,150,335,312 0.0080 2 Math English\nStarCoder StarCoder 306,812,862,958 0.0536 0.3 Code English\nLaw 9,080,387,832 0.0100 2 Academic Chinese\nNews 5,175,531,875 0.0050 2 Academic Chinese\nWanJuan\nPatent 4,559,904,057 0.0050 2 Academic Chinese\nWebtext 126,429,462,230 0.0644 1 Web Chinese\nPTD PTD 165,879,069,486 0.0507 0.6 Web Chinese\nTable1: Detailedcompositionofthetrainingset.\n2.2 Tokenizer\nWeemploytheunigramalgorithm[Kudo,2018]fordatatokenization,utilizingtheimplementation\nprovidedbySentence-Piece[KudoandRichardson,2018]. Incontrasttotheextensivevocabularies\nusedinprevailingopen-sourcemodels,ourtokenizeristrainedonamixedcorpusofChineseand\nEnglish,withavocabularysizeofonly32,000. ThecomparisonoftheXmodel-LMtokenizerwith\nothertokenizersisshownintheTable2. Despiteitssmallsize,ourtokenizerdemonstratesimpressive\ncompressionratesontestdata.\nTheXmodel-LMtokenizeristrainedusingasubsetoftheXmodel-LMpre-trainingcorpus,without\nanyappliedtextnormalization. Toimprovetheencodingofnumericdata,numbersaresplitinto\n1https://huggingface.co/datasets/marianna13/fanfics\n2\nTokenizer VocabSize CompressionRate↓\nLLaMA2 32,000 0.7524\nInternLM2 103,168 0.4124\nBaichuan2 125,696 0.4103\nXmodel-LM 32,000 0.3841\nTable2: ComparisonofvocabularysizeandtextcompressionrateofXmodel-LM’stokenizerwith\nothermodels. Lowervaluesindicatebettercompression.\nindividual digits. Character coverage is set to 0.9999, with rare characters being represented by\nUTF-8bytes. Additionally,wesetthemaximumtokenlengthto16toaccommodateChinesephrases.\n2.3 Modelarchitecture\nWeadoptasimilarmodelarchitecturetoLLama2[Touvronetal.,2023]withthefollowingdetails:\nHiddensize Intermediatesize Attentionheads KVheads Layers ContextLen\n2048 5632 32 4 24 4096\nTable3: DetailedsettingsofXmodel-LM.\nRotaryPositionalEmbedding. Weintegraterotarypositionalembeddings(RoPE)[Suetal.,2023]\nateachlayerofthenetwork.\nRMSNorm. Toenhancetrainingstability,weutilizetheRMSNorm[ZhangandSennrich,2019]\nfunctiontonormalizetheinputofeachtransformersub-layer,withoutnormalizingtheoutput. Linear\nlayersdonotincorporatebias,andwordembeddingsarenottied.\nSwiGLU. We replace the conventional ReLU non-linearity with the SwiGLU [Shazeer, 2020]\nactivationfunctiontooptimizeperformance.\nGrouped-queryattention. Forefficienttrainingandinference,weemploygrouped-queryattention\n(GQA)[Ainslieetal.,2023],featuring32attentionheadsand4KVheads.\n2.4 Training\nTrainingisconductedonasinglenodeequippedwith7×H800GPUs. Toenhancetrainingefficiency\nand boost Model FLOPS Utilization (MFU), we employ Distributed Data Parallel (DDP) and\nFlashAttention-V2.\nWe utilize the cumulative gradient updating method with a mini-batch size of 4 and a gradient\naccumulationstepof30perGPU,resultinginaglobalbatchsizeof840withasequencelength\nof4096. Thissetupyieldsatotaltokencountof3,440,640periteration. Trainingspans600,000\niterations,accumulatingtoatotaltokencountof2,064,384,000,000.\nForoptimization, weemploytheAdamWoptimizerwithamaximumlearningrateof6e-4. The\nlearning rate undergoes linear increase from zero to the maximum over the first 2000 updates,\nfollowedbyannealingto6e-5usingacosineschedule. Thebatchsizeissettoaround3.5Mtokens,\nwithweightdecayassignedas0.1. Additionally,weapplyagradientclippingthresholdof1.0to\nregulatethegradientvalue.\nRefer to the training log in the Figure 1, which includes trend graphs showing the training and\nvalidationlossesasthetrainingtokencountincreases. WeuseOpenWebText[Gokaslanetal.,2019],\nwhichisnotincludedinthetrainingset,asthevalidationsettocalculatethevalidationloss.\n3 Results\nBaseline models For the sake of comparability, we chose several recently popular decoder-only\narchitecturemodelswithascaleofaround1billionparametersforcomparison. Specifically, we\n3\nFigure1: Thetrendoftrainingandvalidationlossduringpretraining.\ncompareXmodel-LMwithOPT[Zhangetal.,2022],Pythia[Bidermanetal.,2023],TinyLLaMA\n[Zhang et al., 2024], MobileLLaMA [Thawakar et al., 2024], H2O-danube [Singer et al., 2024],\nInternLM2[Caietal.,2024]andQwen1.5[Baietal.,2023].\nCommonsensereasoningtasksForevaluatingourmodelsweusetheLanguageModelEvaluation\nHarness[Gaoetal.,2023]. Specifically,ourevaluationcomprised: ARC-Challenge[Clarketal.,\n2018b], ARC-Easy [Clark et al., 2018b], Boolq [Clark et al., 2019], HellaSwag [Zellers et al.,\n2019],OpenBookQA[Mihaylovetal.,2018],PiQA[Bisketal.,2019],SciQ[Welbletal.,2017],\nTriviaQA[Joshietal.,2017],Winogrande[Sakaguchietal.,2021]. Toensureevaluatingfairnessand\nreproducibility,allevaluationmetricsreportedinourstudyaremeasuredinthesameenvironment. It\nisworthmentioningthatweusedtherawaccuracymetrics,ratherthanthenormalizedones. The\nevaluationresultsarepresentedinTable4,allmodelsareevaluatedinazero-shotsettingonthese\ntasks.WenoticeXmodel-LMoutperformsseveralbaselinemodels,particularlysurpassingTinyLlama\ninvariousevaluationmetrics,andit’scomparabletoQwen1.5intermsofoverallperformance.\nModel ARC-c ARC-e Boolq HS. OB. PiQA SciQ TQ. Wino. Avg\nOPT-1.3B 23.29 57.03 57.80 41.52 23.20 71.71 84.30 7.48 59.59 47.32\nPythia-1.4B 25.60 57.58 60.34 39.81 20.20 71.06 85.20 5.01 56.20 47.00\nTinyLLaMA-3T-1.1B 27.82 60.31 57.83 44.98 21.80 73.34 88.90 11.30 59.12 48.59\nMobileLLaMA-1.4B 26.28 61.32 57.92 42.87 23.60 71.33 87.40 12.02 58.25 49.00\nQwen1.5-1.8B 32.25 64.69 66.48 45.49 23.80 73.45 92.90 1.01 61.17 51.25\nH2O-danube-1.8B 32.94 67.42 65.75 50.85 27.40 75.73 91.50 25.05 62.35 55.44\nInternLM2-1.8B 37.54 70.20 69.48 46.52 24.40 75.57 93.90 36.67 65.67 57.77\nXmodel-LM-1.1B 28.16 62.29 61.44 45.96 24.00 72.03 89.70 18.46 60.62 51.41\nTable4: Performanceoncommonsensereasoningtasks. Modelsmarkedingreenperformworsethan\nXmodel-LM,whilemodelsmarkedinredperformbetterthanXmodel-LM.\nProblem-solvingevaluationForexploringtheperformanceofthemodelbeyondcommon-sense\nreasoning, we also evaluate the model’s problem-solving capability. Specifically, our evaluation\ncomprised:\n• BIG-BenchHard(BBH)[Suzgunetal.,2022]: thisisasubsetof23challengingtasksfrom\ntheBIG-Benchbenchmark[Srivastavaetal.,2023]designedtogaugetheproficiencyofa\nlanguagemodelincomprehendingandexecutingcomplexinstructions.\n• TheGeneralLanguageUnderstandingEvaluation(GLUE)[Wangetal.,2018]: thisisa\ncollectionofresourcesfortraining,evaluating,andanalyzingnaturallanguageunderstanding\nsystems.\n• GradeSchoolMath8K(GSM8k)[Cobbeetal.,2021]: thiscomprises8.5khigh-quality\ngradeschoolmathwordproblems,diverseinlinguisticaspects. Itwascuratedspecifically\nforquestionansweringtasksinvolvingmulti-stepreasoningonbasicmathematicalproblems.\n4\n• MassiveMultitaskLanguageUnderstanding(MMLU)[Hendrycksetal.,2021]: thisisused\ntomeasureamodel’sbreadthofworldknowledgeanditsproblem-solvingproficiencies\nacrossdiversesubjects.\nTheevaluationresultsarepresentedinTable5. ItcanbeobservedthattheXmodel-LMhasachieved\nthe highest performance on BBH compared to the baseline models, and it demonstrates strong\ncompetitivenessoverall.\nModel BBH GLUE GSM8K MMLU Avg Avg\n3-shot 5-shot 5-shot 5-shot w.o. GSM8k\nOPT-1.3B 22.67 51.06 0.83 26.70 25.32 33.48\nPythia-1.4B 25.37 52.23 1.63 25.40 26.16 34.33\nMobileLLaMA-1.4B 23.48 43.34 1.44 24.60 23.22 30.47\nTinyLLaMA-3T-1.1B 26.75 48.25 1.97 25.70 25.67 33.57\nH2O-danube-1.8B 27.31 49.83 1.90 25.70 26.19 34.28\nInternLM2-1.8B 16.86 58.96 23.50 42.00 35.34 39.27\nQwen1.5-1.8B 13.84 64.57 33.59 45.10 39.28 41.17\nXmodel-LM-1.1B 27.34 52.61 2.58 25.90 27.11 35.28\nTable 5: Performance on problem-solving tasks. Models marked in green perform worse than\nXmodel-LM,whilemodelsmarkedinredperformbetterthanXmodel-LM.\nChineseabilityBesidesevaluatingthemodel’sEnglishproficiency,wealsoconductedassessments\nonitsChineselanguagecapabilitiesduetothepresenceofacertainproportionofChinesecontentin\nourcorpus. Specifically,ourevaluationcomprised:\n• ARC [Clark et al., 2018a], this consists of 7,787 science exam questions drawn from a\nvarietyofsources,includingsciencequestionsprovidedunderlicensebyaresearchpartner\naffiliatedwithAI2. WeutilizedtheChinesetranslationversionoftheoriginalworkinour\nstudy.\n• XCOPA [Edoardo M. Ponti and Korhonen, 2020], this is designed to assess how well\nmachinelearningmodelscantransfercommonsensereasoningacrossdifferentlanguages.\nItisatranslatedandreannotatedversionoftheEnglishCOPA[Gordonetal.,2011]and\nincludes11languagesfrom11differentlanguagefamiliesandvariousregionsworldwide.\n• XNLI[Conneauetal.,2018],thisisdesignedtoevaluatetheabilityofnaturallanguage\nprocessingsystemstounderstandandtransferknowledgeacrossmultiplelanguages.\nTheevaluationresultsarepresentedinTable6,allmodelsareevaluatedinazero-shotsettingon\nthesetasks. Weobservedthatbyadding15%Chinesetokens,ourmodelgainedacertaindegree\nofunderstandingandgenerationcapabilitiesinChinese,surpassingsomeexistingmodels,butstill\nweakercomparedtoInternLM2andQwen1.5.\nModel ARC-zh XCOPA-zh XNLI-zh Avg\nOPT-1.3B 18.80 53.00 33.45 35.08\nPythia-1.4B 21.03 52.60 34.06 35.90\nMobileLLaMA-1.4B 20.26 52.80 33.82 35.63\nTinyLLaMA-3T-1.1B 21.37 56.80 33.25 37.14\nH2O-danube-1.8B 21.79 55.60 34.74 37.38\nInternLM2-1.8B 27.69 66.80 34.58 43.00\nQwen1.5-1.8B 32.14 66.00 39.28 45.81\nXmodel-LM-1.1B 26.24 60.60 36.02 40.95\nTable6: PerformanceonChinesetasks. ModelsmarkedingreenperformworsethanXmodel-LM,\nwhilemodelsmarkedinredperformbetterthanXmodel-LM.\n5\n4 Casestudy\n4.1 Evolutionofmodel’sperformance\nWetrackedandrecordedthemodel’sperformanceonthecommon-sensereasoningtasksduringthe\npretrainingprocess,asshowninFigure2. ItcanbeobservedthattheperformanceofXmodel-LM\nimprovesastrainingprogresses,surpassingTinyLLaMAinmultipletasks. Moreover,weobservean\napproximatelinearlinkbetweenlogiterationstepsandmodelmetricsgainsacrossmosttasks.\nFigure2: Evolutionofperformanceincommonsensereasoningtasksduringpre-training\n4.2 Generalizationvsmemorization\nRecentadvancementsinmechanicalinterpretabilityhaveuncoveredaphenomenonknownassuper-\nposition,whichcanrepresentmorefeaturesthanexpectedinneuralnetworks. Duringthepre-training\nofXmodel-LM,weobservedshiftsintheL -norm(∥θ∥ )ofthemodelparameters,mirroringtrends\n2 2\nidentifiedinpriorresearch[TomHenighan,2023]. Specifically,thetrainingprocesscanberoughly\ndelineatedintothreestages,asdepictedinFigure3.\nFigure3: ShiftsintheL -normofparametersduringpre-training\n2\n6\nThisreplicationunderscoresasimilartripartitedivision,themodelisinamemorizationstatewhen\ntheamountoftrainingdataissmall,duringwhichtheL -normofthemodelgraduallyincreases.\n2\nAfterpassingthroughanintermediatestate,themodeltransitionstoageneralizationstate,during\nwhichtheL -normofthemodelgraduallydecreases.\n2\n5 Conclusions\nInsummary,ourworkinvolvestraininga1.1B-sizedlanguagemodel(Xmodel-LM)onaself-built\ntokenizerandcorpuscontainingbothChineseandEnglishdata. Ourmodeldemonstratescompetitive\nperformanceonevaluationdatasetscomparedtomodelsofsimilarscales. Oureffortscontributeto\ntheadvancementofknowledgebyshowcasingthepotentialoftrainingsmallermodelswithextensive\ndatasetsforfutureapplications.\nReferences\nJoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,FedericoLebrón,andSumitSanghai.\nGqa:Traininggeneralizedmulti-querytransformermodelsfrommulti-headcheckpoints,2023.\nJinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei\nHuang, BinyuanHui, LuoJi, MeiLi, JunyangLin, RunjiLin, DayihengLiu, GaoLiu, ChengqiangLu,\nKemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,ChuanqiTan,SinanTan,JianhongTu,\nPengWang,ShijieWang,WeiWang,ShengguangWu,BenfengXu,JinXu,AnYang,HaoYang,JianYang,\nShushengYang,YangYao,BowenYu,HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,Yichang\nZhang,ZhenruZhang,ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu.Qwentechnicalreport.\narXivpreprintarXiv:2309.16609,2023.\nStellaBiderman,HaileySchoelkopf,QuentinAnthony,HerbieBradley,KyleO’Brien,EricHallahan,Moham-\nmadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,AviyaSkowron,LintangSutawika,\nandOskarvanderWal. Pythia:Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling,2023.\nYonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi. Piqa:Reasoningaboutphysical\ncommonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https:\n//api.semanticscholar.org/CorpusID:208290939.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,Gretchen\nKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodels\narefew-shotlearners,2020.\nZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,ZehuiChen,ZhiChen,\nPeiChu,XiaoyiDong,HaodongDuan,QiFan,ZhaoyeFei,YangGao,JiayeGe,ChenyaGu,YuzheGu,Tao\nGui,AijiaGuo,QipengGuo,ConghuiHe,YingfanHu,TingHuang,TaoJiang,PenglongJiao,Zhenjiang\nJin,ZhikaiLei,JiaxingLi,JingwenLi,LinyangLi,ShuaibinLi,WeiLi,YiningLi,HongweiLiu,Jiangning\nLiu,JiaweiHong,KaiwenLiu,KuikunLiu,XiaoranLiu,ChengqiLv,HaijunLv,KaiLv,LiMa,Runyuan\nMa,ZerunMa,WenchangNing,LinkeOuyang,JiantaoQiu,YuanQu,FukaiShang,YunfanShao,Demin\nSong,ZifanSong,ZhihaoSui,PengSun,YuSun,HuanzeTang,BinWang,GuotengWang,JiaqiWang,\nJiayuWang,RuiWang,YudongWang,ZiyiWang,XingjianWei,QizhenWeng,FanWu,YingtongXiong,\nChaoXu,RuiliangXu,HangYan,YirongYan,XiaoguiYang,HaochenYe,HuaiyuanYing,JiaYu,JingYu,\nYuhangZang,ChuyuZhang,LiZhang,PanZhang,PengZhang,RuijieZhang,ShuoZhang,SongyangZhang,\nWenjianZhang,WenweiZhang,XingchengZhang,XinyueZhang,HuiZhao,QianZhao,XiaomengZhao,\nFengzheZhou,ZaidaZhou,JingmingZhuo,YichengZou,XipengQiu,YuQiao,andDahuaLin. Internlm2\ntechnicalreport,2024.\nChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristinaToutanova.\nBoolq:Exploringthesurprisingdifficultyofnaturalyes/noquestions,2019.\nPeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvindTafjord.\nThinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. ArXiv,abs/1803.05457,\n2018a.\nPeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvind\nTafjord. Thinkyouhavesolvedquestionanswering?tryarc,theai2reasoningchallenge,2018b.\n7\nKarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,MatthiasPlappert,\nJerryTworek,JacobHilton,ReiichiroNakano,ChristopherHesse,andJohnSchulman. Trainingverifiersto\nsolvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.\nTogether Computer. Redpajama: an open dataset for training large language models, 2023. URL https:\n//github.com/togethercomputer/RedPajama-Data.\nAlexisConneau, RutyRinott, GuillaumeLample, AdinaWilliams, SamuelR.Bowman, HolgerSchwenk,\nand Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the\n2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputational\nLinguistics,2018.\nOlgaMajewskaQianchuLiuIvanVulic´ EdoardoM.Ponti,GoranGlavašandAnnaKorhonen. XCOPA:A\nmultilingualdatasetforcausalcommonsensereasoning. InProceedingsofthe2020ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing(EMNLP),2020. URLhttps://ducdauge.github.io/files/\nxcopa.pdf.\nLeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,Horace\nHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy. Thepile:An800gbdatasetofdiverse\ntextforlanguagemodeling,2020.\nLeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,Laurence\nGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuennighoff,ChrisOciepa,\nJasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,EricTang,AnishThite,\nBenWang,KevinWang,andAndyZou. Aframeworkforfew-shotlanguagemodelevaluation,122023.\nURLhttps://zenodo.org/records/10256836.\nAaronGokaslan,VanyaCohen,ElliePavlick,andStefanieTellex. Openwebtextcorpus. http://Skylion007.\ngithub.io/OpenWebTextCorpus,2019.\nAndrewS.Gordon,ZornitsaKozareva,andMelissaRoemmele. Choiceofplausiblealternatives:Anevaluation\nofcommonsensecausalreasoning. InAAAISpringSymposium:LogicalFormalizationsofCommonsense\nReasoning,2011. URLhttps://api.semanticscholar.org/CorpusID:434646.\nConghuiHe,ZhenjiangJin,ChaoXu,JiantaoQiu,BinWang,WeiLi,HangYan,JiaqiWang,andDahuaLin.\nWanjuan:Acomprehensivemultimodaldatasetforadvancingenglishandchineselargemodels,2023.\nKennethHeafield. KenLM:Fasterandsmallerlanguagemodelqueries. InChrisCallison-Burch,PhilippKoehn,\nChristofMonz, andOmarF.Zaidan, editors, ProceedingsoftheSixthWorkshoponStatisticalMachine\nTranslation,pages187–197,Edinburgh,Scotland,July2011.AssociationforComputationalLinguistics.\nURLhttps://aclanthology.org/W11-2123.\nDanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacobSteinhardt.\nMeasuring massive multitask language understanding. Proceedings of the International Conference on\nLearningRepresentations(ICLR),2021.\nTomHenighan,JaredKaplan,MorKatz,MarkChen,ChristopherHesse,JacobJackson,HeewooJun,TomB.\nBrown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh,\nNickRyder,DanielM.Ziegler,JohnSchulman,DarioAmodei,andSamMcCandlish. Scalinglawsfor\nautoregressivegenerativemodeling,2020.\nMandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettlemoyer. Triviaqa:Alargescaledistantlysupervised\nchallengedatasetforreadingcomprehension,2017.\nTakuKudo. Subwordregularization: Improvingneuralnetworktranslationmodelswithmultiplesubword\ncandidates,2018.\nTakuKudoandJohnRichardson. Sentencepiece:Asimpleandlanguageindependentsubwordtokenizerand\ndetokenizerforneuraltextprocessing,2018.\nRaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,ChenghaoMou,Marc\nMarone,ChristopherAkiki,JiaLi,JennyChim,QianLiu,EvgeniiZheltonozhskii,TerryYueZhuo,Thomas\nWang, OlivierDehaene, MishigDavaadorj, JoelLamy-Poirier, JoãoMonteiro, OlehShliazhko, Nicolas\nGontier, NicholasMeade, ArmelZebaze, Ming-HoYee, LogeshKumarUmapathi, JianZhu, Benjamin\nLipkin,MuhtashamOblokulov,ZhiruoWang,RudraMurthy,JasonStillerman,SivaSankalpPatel,Dmitry\nAbulkhanov,MarcoZocca,MananDey,ZhihanZhang,NourFahmy,UrvashiBhattacharyya,WenhaoYu,\nSwayamSingh,SashaLuccioni,PauloVillegas,MaximKunakov,FedorZhdanov,ManuelRomero,Tony\nLee,NadavTimor,JenniferDing,ClaireSchlesinger,HaileySchoelkopf,JanEbert,TriDao,MayankMishra,\n8\nAlexGu,JenniferRobinson,CarolynJaneAnderson,BrendanDolan-Gavitt,DanishContractor,SivaReddy,\nDanielFried,DzmitryBahdanau,YacineJernite,CarlosMuñozFerrandis,SeanHughes,ThomasWolf,Arjun\nGuha,LeandrovonWerra,andHarmdeVries. Starcoder:maythesourcebewithyou!,2023.\nTodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconductelectricity?a\nnewdatasetforopenbookquestionanswering,2018.\nKeiranPaster,MarcoDosSantos,ZhangirAzerbayev,andJimmyBa. Openwebmath: Anopendatasetof\nhigh-qualitymathematicalwebtext,2023.\nKeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi.Winogrande:anadversarialwinograd\nschemachallengeatscale.Commun.ACM,64(9):99–106,aug2021.ISSN0001-0782.doi:10.1145/3474381.\nURLhttps://doi.org/10.1145/3474381.\nNoamShazeer. Gluvariantsimprovetransformer,2020.\nPhilippSinger,PascalPfeiffer,YauhenBabakhin,MaximilianJeblick,NischayDhankhar,GaborFodor,and\nSriSatishAmbati. H2o-danube-1.8btechnicalreport,2024.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdamR.Brown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,AgnieszkaKluska,AitorLewkowycz,\nAkshatAgarwal,AletheaPower,AlexRay,AlexWarstadt,AlexanderW.Kocurek,AliSafaya,AliTazarv,\nAliceXiang,AliciaParrish,AllenNie,AmanHussain,AmandaAskell,AmandaDsouza,AmbroseSlone,\nAmeet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas\nStuhlmüller,AndrewDai,AndrewLa,AndrewLampinen,AndyZou,AngelaJiang,AngelicaChen,Anh\nVuong, AnimeshGupta, AnnaGottardi, AntonioNorelli, AnuVenkatesh, ArashGholamidavoodi, Arfa\nTabassum,ArulMenezes,ArunKirubarajan,AsherMullokandov,AshishSabharwal,AustinHerrick,Avia\nEfrat,AykutErdem,AylaKarakas¸,B.RyanRoberts,BaoShengLoe,BarretZoph,BartłomiejBojanowski,\nBatuhanÖzyurt,BehnamHedayatnia,BehnamNeyshabur,BenjaminInden,BennoStein,BerkEkmekci,\nBillYuchenLin,BlakeHowald,BryanOrinion,CameronDiao,CameronDour,CatherineStinson,Cedrick\nArgueta,CésarFerriRamírez,ChandanSingh,CharlesRathkopf,ChenlinMeng,ChittaBaral,ChiyuWu,\nChrisCallison-Burch, ChrisWaites, ChristianVoigt, ChristopherD.Manning, ChristopherPotts, Cindy\nRamirez,ClaraE.Rivera,ClemenciaSiro,ColinRaffel,CourtneyAshcraft,CristinaGarbacea,Damien\nSileo,DanGarrette,DanHendrycks,DanKilman,DanRoth,DanielFreeman,DanielKhashabi,DanielLevy,\nDanielMoseguíGonzález,DaniellePerszyk,DannyHernandez,DanqiChen,DaphneIppolito,DarGilboa,\nDavidDohan,DavidDrakard,DavidJurgens,DebajyotiDatta,DeepGanguli,DenisEmelin,DenisKleyko,\nDenizYuret, DerekChen, DerekTam, DieuwkeHupkes, DigantaMisra, DilyarBuzan, DimitriCoelho\nMollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal,\nEleanorHagerman,ElizabethBarnes,ElizabethDonoway,ElliePavlick,EmanueleRodola,EmmaLam,\nEric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim,\nEuniceEngefuManyasi,EvgeniiZheltonozhskii,FanyueXia,FatemehSiar,FernandoMartínez-Plumed,\nFrancescaHappé, FrancoisChollet, FriedaRong, GauravMishra, GentaIndraWinata, GerarddeMelo,\nGermánKruszewski,GiambattistaParascandolo,GiorgioMariani,GloriaWang,GonzaloJaimovitch-López,\nGregorBetz,GuyGur-Ari,HanaGalijasevic,HannahKim,HannahRashkin,HannanehHajishirzi,Harsh\nMehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee\nWong,IanNg,IsaacNoble,JaapJumelet,JackGeissinger,JacksonKernion,JacobHilton,JaehoonLee,\nJaimeFernándezFisac,JamesB.Simon,JamesKoppel,JamesZheng,JamesZou,JanKocon´,JanaThompson,\nJanelleWingfield, JaredKaplan, JaremaRadom, JaschaSohl-Dickstein, JasonPhang, JasonWei, Jason\nYosinski,JekaterinaNovikova,JelleBosscher,JenniferMarsh,JeremyKim,JeroenTaal,JesseEngel,Jesujoba\nAlabi,JiachengXu,JiamingSong,JillianTang,JoanWaweru,JohnBurden,JohnMiller,JohnU.Balis,\nJonathanBatchelder,JonathanBerant,JörgFrohberg,JosRozen,JoseHernandez-Orallo,JosephBoudeman,\nJosephGuerr,JosephJones,JoshuaB.Tenenbaum,JoshuaS.Rule,JoyceChua,KamilKanclerz,Karen\nLivescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole,\nKevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar,\nKyleMcDonell,KyleRichardson,LariaReynolds,LeoGao,LiZhang,LiamDugan,LianhuiQin,Lidia\nContreras-Ochando,Louis-PhilippeMorency,LucaMoschella,LucasLam,LucyNoble,LudwigSchmidt,\nLuhengHe,LuisOliverosColón,LukeMetz,LütfiKeremS¸enel,MaartenBosma,MaartenSap,Maartjeter\nHoeve,MaheenFarooqi,ManaalFaruqui,MantasMazeika,MarcoBaturan,MarcoMarelli,MarcoMaru,\nMariaJoseRamírezQuintana,MarieTolkiehn,MarioGiulianelli,MarthaLewis,MartinPotthast,MatthewL.\nLeavitt,MatthiasHagen,MátyásSchubert,MedinaOrdunaBaitemirova,MelodyArnaud,MelvinMcElrath,\nMichaelA.Yee,MichaelCohen,MichaelGu,MichaelIvanitskiy,MichaelStarritt,MichaelStrube,Michał\nSwe˛drowski,MicheleBevilacqua,MichihiroYasunaga,MihirKale,MikeCain,MimeeXu,MiracSuzgun,\nMitchWalker,MoTiwari,MohitBansal,MoinAminnaseri,MorGeva,MozhdehGheini,MukundVarmaT,\nNanyunPeng,NathanA.Chi,NayeonLee,NetaGur-AriKrakover,NicholasCameron,NicholasRoberts,\nNickDoiron,NicoleMartinez,NikitaNangia,NiklasDeckers,NiklasMuennighoff,NitishShirishKeskar,\nNivedithaS.Iyer,NoahConstant,NoahFiedel,NuanWen,OliverZhang,OmarAgha,OmarElbaghdadi,\n9\nOmer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaulVicol,PegahAlipoormolabashi,PeiyuanLiao,PercyLiang,PeterChang,PeterEckersley,PhuMon\nHtut,PinyuHwang,PiotrMiłkowski,PiyushPatil,PouyaPezeshkpour,PritiOli,QiaozhuMei,QingLyu,\nQinlangChen,RabinBanjade,RachelEttaRudolph,RaeferGabriel,RahelHabacker,RamonRisco,Raphaël\nMillière,RhythmGarg,RichardBarnes,RifA.Saurous,RikuArakawa,RobbeRaymaekers,RobertFrank,\nRohanSikand,RomanNovak,RomanSitelew,RonanLeBras,RosanneLiu,RowanJacobs,RuiZhang,\nRuslanSalakhutdinov,RyanChi,RyanLee,RyanStovall,RyanTeehan,RylanYang,SahibSingh,SaifM.\nMohammad,SajantAnand,SamDillavou,SamShleifer,SamWiseman,SamuelGruetter,SamuelR.Bowman,\nSamuelS.Schoenholz,SanghyunHan,SanjeevKwatra,SarahA.Rous,SarikGhazarian,SayanGhosh,Sean\nCasey,SebastianBischoff,SebastianGehrmann,SebastianSchuster,SepidehSadeghi,ShadiHamdan,Sharon\nZhou,ShashankSrivastava,SherryShi,ShikharSingh,ShimaAsaadi,ShixiangShaneGu,ShubhPachchigar,\nShubhamToshniwal,ShyamUpadhyay,Shyamolima,Debnath,SiamakShakeri,SimonThormeyer,Simone\nMelzi,SivaReddy,SnehaPriscillaMakini,Soo-HwanLee,SpencerTorene,SriharshaHatwar,Stanislas\nDehaene,StefanDivic,StefanoErmon,StellaBiderman,StephanieLin,StephenPrasad,StevenT.Piantadosi,\nStuartM.Shieber,SummerMisherghi,SvetlanaKiritchenko,SwaroopMishra,TalLinzen,TalSchuster,Tao\nLi,TaoYu,TariqAli,TatsuHashimoto,Te-LinWu,ThéoDesbordes,TheodoreRothschild,ThomasPhan,\nTianleWang,TiberiusNkinyili,TimoSchick,TimofeiKornev,TitusTunduny,TobiasGerstenberg,Trenton\nChang,TrishalaNeeraj,TusharKhot,TylerShultz,UriShaham,VedantMisra,VeraDemberg,Victoria\nNyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar,\nWilliamFedus,WilliamSaunders,WilliamZhang,WoutVossen,XiangRen,XiaoyuTong,XinranZhao,\nXinyiWu,XudongShen,YadollahYaghoobzadeh,YairLakretz,YangqiuSong,YasamanBahri,YejinChoi,\nYichiYang,YidingHao,YifuChen,YonatanBelinkov,YuHou,YufangHou,YuntaoBai,ZacharySeid,\nZhuoyeZhao,ZijianWang,ZijieJ.Wang,ZiruiWang,andZiyiWu.Beyondtheimitationgame:Quantifying\nandextrapolatingthecapabilitiesoflanguagemodels,2023.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformerwithrotarypositionembedding,2023.\nMiracSuzgun,NathanScales,NathanaelScharli,SebastianGehrmann,YiTay,HyungWonChung,Aakanksha\nChowdhery,QuocV.Le,EdHuaihsinChi,DennyZhou,andJasonWei. Challengingbig-benchtasksand\nwhetherchain-of-thoughtcansolvethem.InAnnualMeetingoftheAssociationforComputationalLinguistics,\n2022. URLhttps://api.semanticscholar.org/CorpusID:252917648.\nOmkarThawakar,AshmalVayani,SalmanKhan,HishamCholakal,RaoM.Anwer,MichaelFelsberg,Tim\nBaldwin, Eric P. Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully\ntransparentgpt,2024.\nTristan Hume Nelson Elhage Robert Lasenby Stanislav Fort Nicholas Schiefer Christopher Olah\nTom Henighan, Shan Carter. Superposition, memorization, and double descent, 2023. URL https:\n//transformer-circuits.pub/2023/toy-double-descent/index.html.\nHugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,\nSoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,Moya\nChen,GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,\nVedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,\nViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,\nThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,\nPushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,Kalyan\nSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,\nRossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,Angela\nFan,MelanieKambadur,SharanNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov,andThomas\nScialom. Llama2:Openfoundationandfine-tunedchatmodels,2023.\nAlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.GLUE:Amulti-task\nbenchmarkandanalysisplatformfornaturallanguageunderstanding. InProceedingsofthe2018EMNLP\nWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages353–355,Brussels,\nBelgium,November2018.AssociationforComputationalLinguistics. doi:10.18653/v1/W18-5446. URL\nhttps://aclanthology.org/W18-5446.\nZihanWang,XinzhangLiu,ShixuanLiu,YitongYao,YuyaoHuang,ZhongjiangHe,XuelongLi,Yongxiang\nLi,ZhonghaoChe,ZhaoxiZhang,YanWang,XinWang,LuwenPu,HuihanXu,RuiyuFang,YuZhao,Jie\nZhang,XiaomengHuang,ZhilongLu,JiaxinPeng,WenjunZheng,ShiquanWang,BingkaiYang,Xueweihe,\nZhuoruJiang,QiyiXie,YanhanZhang,ZhongqiuLi,LinglingShi,WeiweiFu,YinZhang,ZiluHuang,Sishi\nXiong,YuxiangZhang,ChaoWang,andShuangyongSong. Telechattechnicalreport,2024.\nJohannesWelbl,NelsonF.Liu,andMattGardner. Crowdsourcingmultiplechoicesciencequestions,2017.\n10\nRowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachinereally\nfinishyoursentence?,2019.\nBiaoZhangandRicoSennrich. Rootmeansquarelayernormalization. CurranAssociatesInc.,RedHook,NY,\nUSA,2019.\nPeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama:Anopen-sourcesmalllanguagemodel,\n2024.\nSusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,ChristopherDewan,\nMonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguagemodels. arXiv\npreprintarXiv:2205.01068,2022.\n11",
    "pdf_filename": "Xmodel-LM_Technical_Report.pdf"
}