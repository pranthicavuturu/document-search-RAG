{
    "title": "Xmodel-LM Technical Report",
    "context": "We introduce Xmodel-LM, a compact and efficient 1.1B language model pretrained on around 2 trillion tokens. Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size. It notably surpasses existing open-source language models of similar scale. Our model checkpoints and code are publicly accessible on GitHub at https://github. com/XiaoduoAILab/XmodelLM. 1 Large language models (LLMs) have demonstrated remarkable performance across various natural language tasks, often requiring minimal examples of natural language instructions [Brown et al., 2020], thereby alleviating the necessity for extensive feature engineering. At the heart of this progress lies language modeling, typically formulated as an unsupervised dis- tribution estimate from sequences of symbols (s1, ..., sn−1) within a variable-length sequence x. Leveraging the natural sequential order of languages, the joint probability over symbols is commonly factorized into a product of conditional probabilities: p(x) = n Y i=1 p(sn|s1, ..., sn−1) Recent advancements in natural language processing (NLP) have largely stemmed from the scaling up of language model sizes, driven by the observed functional relationship between model performance and size [Henighan et al., 2020]. However, the accompanying rise in operational costs poses a significant hurdle to their widespread adoption. In this paper, we introduce Xmodel-LM, a compact vision-language assistant powered by a relatively small language model. Remarkably, Xmodel-LM achieves performance comparable to state-of-the-art models of similar scale across numerous LLM benchmark tests, showcasing its potential for a wide array of practical tasks. 2 Pretraining This chapter details the pretraining process of Xmodel-LM. First, we introduce the sources and composition of our corpus, as well as our preprocessing methods. Second, we describe the construc- tion of our customized tokenizer. Finally, we detail the model architecture and training parameter configurations. arXiv:2406.02856v5  [cs.CL]  19 Nov 2024",
    "body": "Xmodel-LM Technical Report\nWang Yichuan\nLiu Yang\nYan Yu\nWang Qun\nHuang Xucheng\nJiang Ling\nXiaoduoAI\n{wangyichuan,liuyangfoam,yanyu,wangqun}@xiaoduotech.com\nAbstract\nWe introduce Xmodel-LM, a compact and efficient 1.1B language model pretrained\non around 2 trillion tokens. Trained on our self-built dataset (Xdata), which\nbalances Chinese and English corpora based on downstream task optimization,\nXmodel-LM exhibits remarkable performance despite its smaller size. It notably\nsurpasses existing open-source language models of similar scale. Our model\ncheckpoints and code are publicly accessible on GitHub at https://github.\ncom/XiaoduoAILab/XmodelLM.\n1\nIntroduction\nLarge language models (LLMs) have demonstrated remarkable performance across various natural\nlanguage tasks, often requiring minimal examples of natural language instructions [Brown et al.,\n2020], thereby alleviating the necessity for extensive feature engineering.\nAt the heart of this progress lies language modeling, typically formulated as an unsupervised dis-\ntribution estimate from sequences of symbols (s1, ..., sn−1) within a variable-length sequence x.\nLeveraging the natural sequential order of languages, the joint probability over symbols is commonly\nfactorized into a product of conditional probabilities:\np(x) =\nn\nY\ni=1\np(sn|s1, ..., sn−1)\nRecent advancements in natural language processing (NLP) have largely stemmed from the scaling up\nof language model sizes, driven by the observed functional relationship between model performance\nand size [Henighan et al., 2020]. However, the accompanying rise in operational costs poses a\nsignificant hurdle to their widespread adoption.\nIn this paper, we introduce Xmodel-LM, a compact vision-language assistant powered by a relatively\nsmall language model. Remarkably, Xmodel-LM achieves performance comparable to state-of-the-art\nmodels of similar scale across numerous LLM benchmark tests, showcasing its potential for a wide\narray of practical tasks.\n2\nPretraining\nThis chapter details the pretraining process of Xmodel-LM. First, we introduce the sources and\ncomposition of our corpus, as well as our preprocessing methods. Second, we describe the construc-\ntion of our customized tokenizer. Finally, we detail the model architecture and training parameter\nconfigurations.\narXiv:2406.02856v5  [cs.CL]  19 Nov 2024\n\n2.1\nTraining Data\nData sourcing: In the process of constructing the training corpus and allocating weights, our\nprimary objective is to ensure the quality and diversity of the training data. Our original dataset\nprimarily consists of aggregated training data from other LLMs, such as Redpajama [Computer,\n2023], subsets of the Pile [Gao et al., 2020] and StarCoder [Li et al., 2023]. To address deficiencies\nin the distribution of book and mathematical data within the training data distribution, we have also\nincorporated FanFics1 and OpenWebMath [Paster et al., 2023]. Additionally, we have added the\nChinese data source PTD [Wang et al., 2024] and WanJuan [He et al., 2023] to imbue our model with\na certain level of proficiency in Chinese.\nData processing: We are committed to ensure the quality of the data and reducing its redundancy.\nWe first employ heuristic methods such as paragraph length and punctuation ratio for initial filtering.\nSubsequently, we utilize a 5-gram Kneser-Ney model based on KenLM Library [Heafield, 2011] to\ncompute text perplexity for further quality filtering. In the next stage, we employe a locality-sensitive\nhashing method based on SimHash to deduplicate the training data. To balance deduplication quality\nand efficiency, we implement a bucketing strategy on the entire dataset, enabling the deduplica-\ntion process to scale efficiently across large datasets. Finally, we tokenize all datasets using our\ncustom-trained tokenizer, and designed different sampling weights for the datasets based on their\ncharacteristics, as shown in Table 1.\nData Source\nDataset\nNum Tokens\nSampling Weight Epochs Category Language\nRedpajama\nArxiv\n31,336,679,261\n0.0160\n1\nAcademic\nEnglish\nBook\n29,633,248,538\n0.0300\n1\nBook\nEnglish\nC4\n192,696,661,887\n0.1000\n1\nWeb\nEnglish\nCommon Crawl 1,251,868,330,446\n0.5600\n0.88\nWeb\nEnglish\nGithub\n59,063,773,003\n0.0150\n0.5\nCode\nEnglish\nStackexchange\n22,728,030,774\n0.0174\n1.5\nSocial\nEnglish\nWikipedia\n34,312,919,854\n0.0520\n3\nAcademic\nEnglish\nPile\nBookCorpus\n562,392,085\n0.0006\n2\nBook\nEnglish\nPubMed\n17,698,877,602\n0.0100\n1\nAcademic\nEnglish\nAMPS\nAMPS\n269,936,326\n0.0003\n2\nMath\nEnglish\nFanFics\nFanFics\n1,803,437,344\n0.0020\n2\nBook\nEnglish\nOpenWebMath OpenWebMath\n7,150,335,312\n0.0080\n2\nMath\nEnglish\nStarCoder\nStarCoder\n306,812,862,958\n0.0536\n0.3\nCode\nEnglish\nWanJuan\nLaw\n9,080,387,832\n0.0100\n2\nAcademic Chinese\nNews\n5,175,531,875\n0.0050\n2\nAcademic Chinese\nPatent\n4,559,904,057\n0.0050\n2\nAcademic Chinese\nWebtext\n126,429,462,230\n0.0644\n1\nWeb\nChinese\nPTD\nPTD\n165,879,069,486\n0.0507\n0.6\nWeb\nChinese\nTable 1: Detailed composition of the training set.\n2.2\nTokenizer\nWe employ the unigram algorithm [Kudo, 2018] for data tokenization, utilizing the implementation\nprovided by Sentence-Piece [Kudo and Richardson, 2018]. In contrast to the extensive vocabularies\nused in prevailing open-source models, our tokenizer is trained on a mixed corpus of Chinese and\nEnglish, with a vocabulary size of only 32,000. The comparison of the Xmodel-LM tokenizer with\nother tokenizers is shown in the Table 2. Despite its small size, our tokenizer demonstrates impressive\ncompression rates on test data.\nThe Xmodel-LM tokenizer is trained using a subset of the Xmodel-LM pre-training corpus, without\nany applied text normalization. To improve the encoding of numeric data, numbers are split into\n1https://huggingface.co/datasets/marianna13/fanfics\n2\n\nTokenizer\nVocab Size\nCompression Rate ↓\nLLaMA 2\n32,000\n0.7524\nInternLM 2\n103,168\n0.4124\nBaichuan 2\n125,696\n0.4103\nXmodel-LM\n32,000\n0.3841\nTable 2: Comparison of vocabulary size and text compression rate of Xmodel-LM’s tokenizer with\nother models. Lower values indicate better compression.\nindividual digits. Character coverage is set to 0.9999, with rare characters being represented by\nUTF-8 bytes. Additionally, we set the maximum token length to 16 to accommodate Chinese phrases.\n2.3\nModel architecture\nWe adopt a similar model architecture to LLama 2 [Touvron et al., 2023] with the following details:\nHidden size\nIntermediate size\nAttention heads\nKV heads\nLayers\nContext Len\n2048\n5632\n32\n4\n24\n4096\nTable 3: Detailed settings of Xmodel-LM.\nRotary Positional Embedding. We integrate rotary positional embeddings (RoPE) [Su et al., 2023]\nat each layer of the network.\nRMSNorm. To enhance training stability, we utilize the RMSNorm [Zhang and Sennrich, 2019]\nfunction to normalize the input of each transformer sub-layer, without normalizing the output. Linear\nlayers do not incorporate bias, and word embeddings are not tied.\nSwiGLU. We replace the conventional ReLU non-linearity with the SwiGLU [Shazeer, 2020]\nactivation function to optimize performance.\nGrouped-query attention. For efficient training and inference, we employ grouped-query attention\n(GQA) [Ainslie et al., 2023], featuring 32 attention heads and 4 KV heads.\n2.4\nTraining\nTraining is conducted on a single node equipped with 7×H800 GPUs. To enhance training efficiency\nand boost Model FLOPS Utilization (MFU), we employ Distributed Data Parallel (DDP) and\nFlashAttention-V2.\nWe utilize the cumulative gradient updating method with a mini-batch size of 4 and a gradient\naccumulation step of 30 per GPU, resulting in a global batch size of 840 with a sequence length\nof 4096. This setup yields a total token count of 3,440,640 per iteration. Training spans 600,000\niterations, accumulating to a total token count of 2,064,384,000,000.\nFor optimization, we employ the AdamW optimizer with a maximum learning rate of 6e-4. The\nlearning rate undergoes linear increase from zero to the maximum over the first 2000 updates,\nfollowed by annealing to 6e-5 using a cosine schedule. The batch size is set to around 3.5M tokens,\nwith weight decay assigned as 0.1. Additionally, we apply a gradient clipping threshold of 1.0 to\nregulate the gradient value.\nRefer to the training log in the Figure 1, which includes trend graphs showing the training and\nvalidation losses as the training token count increases. We use OpenWebText [Gokaslan et al., 2019],\nwhich is not included in the training set, as the validation set to calculate the validation loss.\n3\nResults\nBaseline models For the sake of comparability, we chose several recently popular decoder-only\narchitecture models with a scale of around 1 billion parameters for comparison. Specifically, we\n3\n\nFigure 1: The trend of training and validation loss during pretraining.\ncompare Xmodel-LM with OPT [Zhang et al., 2022], Pythia [Biderman et al., 2023], TinyLLaMA\n[Zhang et al., 2024], MobileLLaMA [Thawakar et al., 2024], H2O-danube [Singer et al., 2024],\nInternLM2 [Cai et al., 2024] and Qwen1.5 [Bai et al., 2023].\nCommonsense reasoning tasks For evaluating our models we use the Language Model Evaluation\nHarness [Gao et al., 2023]. Specifically, our evaluation comprised: ARC-Challenge [Clark et al.,\n2018b], ARC-Easy [Clark et al., 2018b], Boolq [Clark et al., 2019], HellaSwag [Zellers et al.,\n2019], OpenBookQA [Mihaylov et al., 2018], PiQA [Bisk et al., 2019], SciQ [Welbl et al., 2017],\nTriviaQA [Joshi et al., 2017], Winogrande [Sakaguchi et al., 2021]. To ensure evaluating fairness and\nreproducibility, all evaluation metrics reported in our study are measured in the same environment. It\nis worth mentioning that we used the raw accuracy metrics, rather than the normalized ones. The\nevaluation results are presented in Table 4, all models are evaluated in a zero-shot setting on these\ntasks. We notice Xmodel-LM outperforms several baseline models, particularly surpassing TinyLlama\nin various evaluation metrics, and it’s comparable to Qwen1.5 in terms of overall performance.\nModel\nARC-c ARC-e Boolq\nHS.\nOB.\nPiQA SciQ\nTQ.\nWino.\nAvg\nOPT-1.3B\n23.29\n57.03\n57.80 41.52 23.20 71.71 84.30\n7.48\n59.59 47.32\nPythia-1.4B\n25.60\n57.58\n60.34 39.81 20.20 71.06 85.20\n5.01\n56.20 47.00\nTinyLLaMA-3T-1.1B\n27.82\n60.31\n57.83 44.98 21.80 73.34 88.90 11.30 59.12 48.59\nMobileLLaMA-1.4B\n26.28\n61.32\n57.92 42.87 23.60 71.33 87.40 12.02 58.25 49.00\nQwen1.5-1.8B\n32.25\n64.69\n66.48 45.49 23.80 73.45 92.90\n1.01\n61.17 51.25\nH2O-danube-1.8B\n32.94\n67.42\n65.75 50.85 27.40 75.73 91.50 25.05 62.35 55.44\nInternLM2-1.8B\n37.54\n70.20\n69.48 46.52 24.40 75.57 93.90 36.67 65.67 57.77\nXmodel-LM-1.1B\n28.16\n62.29\n61.44 45.96 24.00 72.03 89.70 18.46 60.62 51.41\nTable 4: Performance on commonsense reasoning tasks. Models marked in green perform worse than\nXmodel-LM, while models marked in red perform better than Xmodel-LM.\nProblem-solving evaluation For exploring the performance of the model beyond common-sense\nreasoning, we also evaluate the model’s problem-solving capability. Specifically, our evaluation\ncomprised:\n• BIG-Bench Hard (BBH) [Suzgun et al., 2022]: this is a subset of 23 challenging tasks from\nthe BIG-Bench benchmark [Srivastava et al., 2023] designed to gauge the proficiency of a\nlanguage model in comprehending and executing complex instructions.\n• The General Language Understanding Evaluation (GLUE) [Wang et al., 2018]: this is a\ncollection of resources for training, evaluating, and analyzing natural language understanding\nsystems.\n• Grade School Math 8K (GSM8k) [Cobbe et al., 2021]: this comprises 8.5k high-quality\ngrade school math word problems, diverse in linguistic aspects. It was curated specifically\nfor question answering tasks involving multi-step reasoning on basic mathematical problems.\n4\n\n• Massive Multitask Language Understanding (MMLU) [Hendrycks et al., 2021]: this is used\nto measure a model’s breadth of world knowledge and its problem-solving proficiencies\nacross diverse subjects.\nThe evaluation results are presented in Table 5. It can be observed that the Xmodel-LM has achieved\nthe highest performance on BBH compared to the baseline models, and it demonstrates strong\ncompetitiveness overall.\nModel\nBBH\nGLUE\nGSM8K\nMMLU\nAvg\nAvg\n3-shot\n5-shot\n5-shot\n5-shot\nw.o. GSM8k\nOPT-1.3B\n22.67\n51.06\n0.83\n26.70\n25.32\n33.48\nPythia-1.4B\n25.37\n52.23\n1.63\n25.40\n26.16\n34.33\nMobileLLaMA-1.4B\n23.48\n43.34\n1.44\n24.60\n23.22\n30.47\nTinyLLaMA-3T-1.1B\n26.75\n48.25\n1.97\n25.70\n25.67\n33.57\nH2O-danube-1.8B\n27.31\n49.83\n1.90\n25.70\n26.19\n34.28\nInternLM2-1.8B\n16.86\n58.96\n23.50\n42.00\n35.34\n39.27\nQwen1.5-1.8B\n13.84\n64.57\n33.59\n45.10\n39.28\n41.17\nXmodel-LM-1.1B\n27.34\n52.61\n2.58\n25.90\n27.11\n35.28\nTable 5: Performance on problem-solving tasks. Models marked in green perform worse than\nXmodel-LM, while models marked in red perform better than Xmodel-LM.\nChinese ability Besides evaluating the model’s English proficiency, we also conducted assessments\non its Chinese language capabilities due to the presence of a certain proportion of Chinese content in\nour corpus. Specifically, our evaluation comprised:\n• ARC [Clark et al., 2018a], this consists of 7,787 science exam questions drawn from a\nvariety of sources, including science questions provided under license by a research partner\naffiliated with AI2. We utilized the Chinese translation version of the original work in our\nstudy.\n• XCOPA [Edoardo M. Ponti and Korhonen, 2020], this is designed to assess how well\nmachine learning models can transfer commonsense reasoning across different languages.\nIt is a translated and reannotated version of the English COPA [Gordon et al., 2011] and\nincludes 11 languages from 11 different language families and various regions worldwide.\n• XNLI [Conneau et al., 2018], this is designed to evaluate the ability of natural language\nprocessing systems to understand and transfer knowledge across multiple languages.\nThe evaluation results are presented in Table 6, all models are evaluated in a zero-shot setting on\nthese tasks. We observed that by adding 15% Chinese tokens, our model gained a certain degree\nof understanding and generation capabilities in Chinese, surpassing some existing models, but still\nweaker compared to InternLM2 and Qwen1.5.\nModel\nARC-zh\nXCOPA-zh\nXNLI-zh\nAvg\nOPT-1.3B\n18.80\n53.00\n33.45\n35.08\nPythia-1.4B\n21.03\n52.60\n34.06\n35.90\nMobileLLaMA-1.4B\n20.26\n52.80\n33.82\n35.63\nTinyLLaMA-3T-1.1B\n21.37\n56.80\n33.25\n37.14\nH2O-danube-1.8B\n21.79\n55.60\n34.74\n37.38\nInternLM2-1.8B\n27.69\n66.80\n34.58\n43.00\nQwen1.5-1.8B\n32.14\n66.00\n39.28\n45.81\nXmodel-LM-1.1B\n26.24\n60.60\n36.02\n40.95\nTable 6: Performance on Chinese tasks. Models marked in green perform worse than Xmodel-LM,\nwhile models marked in red perform better than Xmodel-LM.\n5\n\n4\nCase study\n4.1\nEvolution of model’s performance\nWe tracked and recorded the model’s performance on the common-sense reasoning tasks during the\npretraining process, as shown in Figure 2. It can be observed that the performance of Xmodel-LM\nimproves as training progresses, surpassing TinyLLaMA in multiple tasks. Moreover, we observe an\napproximate linear link between log iteration steps and model metrics gains across most tasks.\nFigure 2: Evolution of performance in commonsense reasoning tasks during pre-training\n4.2\nGeneralization vs memorization\nRecent advancements in mechanical interpretability have uncovered a phenomenon known as super-\nposition, which can represent more features than expected in neural networks. During the pre-training\nof Xmodel-LM, we observed shifts in the L2-norm (∥θ∥2) of the model parameters, mirroring trends\nidentified in prior research [Tom Henighan, 2023]. Specifically, the training process can be roughly\ndelineated into three stages, as depicted in Figure 3.\nFigure 3: Shifts in the L2-norm of parameters during pre-training\n6\n\nThis replication underscores a similar tripartite division, the model is in a memorization state when\nthe amount of training data is small, during which the L2-norm of the model gradually increases.\nAfter passing through an intermediate state, the model transitions to a generalization state, during\nwhich the L2-norm of the model gradually decreases.\n5\nConclusions\nIn summary, our work involves training a 1.1B-sized language model (Xmodel-LM) on a self-built\ntokenizer and corpus containing both Chinese and English data. Our model demonstrates competitive\nperformance on evaluation datasets compared to models of similar scales. Our efforts contribute to\nthe advancement of knowledge by showcasing the potential of training smaller models with extensive\ndatasets for future applications.\nReferences\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.\nGqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu,\nPeng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang,\nShusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang\nZhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report.\narXiv preprint arXiv:2309.16609, 2023.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Moham-\nmad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika,\nand Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\ncommonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https:\n//api.semanticscholar.org/CorpusID:208290939.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen,\nPei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao\nGui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang\nJin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning\nLiu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan\nMa, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin\nSong, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang,\nJiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong,\nChao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu,\nYuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao,\nFengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2\ntechnical report, 2024.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\nBoolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\nThink you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457,\n2018a.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018b.\n7\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\nsolve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nTogether Computer. Redpajama: an open dataset for training large language models, 2023. URL https:\n//github.com/togethercomputer/RedPajama-Data.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, 2018.\nOlga Majewska Qianchu Liu Ivan Vuli´c Edoardo M. Ponti, Goran Glavaš and Anna Korhonen. XCOPA: A\nmultilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2020. URL https://ducdauge.github.io/files/\nxcopa.pdf.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse\ntext for language modeling, 2020.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.\nURL https://zenodo.org/records/10256836.\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.\ngithub.io/OpenWebTextCorpus, 2019.\nAndrew S. Gordon, Zornitsa Kozareva, and Melissa Roemmele. Choice of plausible alternatives: An evaluation\nof commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense\nReasoning, 2011. URL https://api.semanticscholar.org/CorpusID:434646.\nConghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin.\nWanjuan: A comprehensive multimodal dataset for advancing english and chinese large models, 2023.\nKenneth Heafield. KenLM: Faster and smaller language model queries. In Chris Callison-Burch, Philipp Koehn,\nChristof Monz, and Omar F. Zaidan, editors, Proceedings of the Sixth Workshop on Statistical Machine\nTranslation, pages 187–197, Edinburgh, Scotland, July 2011. Association for Computational Linguistics.\nURL https://aclanthology.org/W11-2123.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding. Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B.\nBrown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh,\nNick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for\nautoregressive generative modeling, 2020.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension, 2017.\nTaku Kudo. Subword regularization: Improving neural network translation models with multiple subword\ncandidates, 2018.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and\ndetokenizer for neural text processing, 2018.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas\nWang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas\nGontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin\nLipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry\nAbulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu,\nSwayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony\nLee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\n8\n\nAlex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun\nGuha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering, 2018.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of\nhigh-quality mathematical web text, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd\nschema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381.\nURL https://doi.org/10.1145/3474381.\nNoam Shazeer. Glu variants improve transformer, 2020.\nPhilipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, and\nSri Satish Ambati. H2o-danube-1.8b technical report, 2024.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz,\nAkshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv,\nAlice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone,\nAmeet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas\nStuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh\nVuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa\nTabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸s, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci,\nBill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick\nArgueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,\nChris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy\nRamirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien\nSileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy,\nDaniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa,\nDavid Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko,\nDeniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho\nMollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal,\nEleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam,\nEric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim,\nEunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed,\nFrancesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo,\nGermán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López,\nGregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh\nMehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee\nWong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee,\nJaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco´n, Jana Thompson,\nJanelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason\nYosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba\nAlabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis,\nJonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman,\nJoseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen\nLivescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole,\nKevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar,\nKyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia\nContreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt,\nLuheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem ¸Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru,\nMaria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L.\nLeavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath,\nMichael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał\nSw˛edrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun,\nMitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T,\nNanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\n9\n\nOmer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu,\nQinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël\nMillière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank,\nRohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang,\nRuslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman,\nSamuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean\nCasey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon\nZhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar,\nShubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone\nMelzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas\nDehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi,\nStuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao\nLi, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan,\nTianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton\nChang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria\nNyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar,\nWilliam Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao,\nXinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi,\nYichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid,\nZhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models, 2023.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding, 2023.\nMirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics,\n2022. URL https://api.semanticscholar.org/CorpusID:252917648.\nOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim\nBaldwin, Eric P. Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully\ntransparent gpt, 2024.\nTristan Hume Nelson Elhage Robert Lasenby Stanislav Fort Nicholas Schiefer Christopher Olah\nTom Henighan, Shan Carter.\nSuperposition, memorization, and double descent, 2023.\nURL https:\n//transformer-circuits.pub/2023/toy-double-descent/index.html.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela\nFan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels,\nBelgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL\nhttps://aclanthology.org/W18-5446.\nZihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang\nLi, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie\nZhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he,\nZhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi\nXiong, Yuxiang Zhang, Chao Wang, and Shuangyong Song. Telechat technical report, 2024.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017.\n10\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\nfinish your sentence?, 2019.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Curran Associates Inc., Red Hook, NY,\nUSA, 2019.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model,\n2024.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068, 2022.\n11",
    "pdf_filename": "Xmodel-LM_Technical_Report.pdf"
}