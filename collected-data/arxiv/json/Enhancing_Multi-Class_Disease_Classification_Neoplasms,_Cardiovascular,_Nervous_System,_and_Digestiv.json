{
    "title": "Enhancing Multi-Class Disease Classification Neoplasms, Cardiovascular, Nervous System, and Digestiv",
    "context": "terms of multi-class disease classification via pre-trained language cal conditions. We excluded non-cancer conditions and examined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and BERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained on medical data, demonstrated superior performance in medical text classification (97% accuracy). Sur- prisingly, XLNet followed closely (96% accuracy), demonstrating its generalizability across domains even though it was not pre- trained on medical data. LastBERT, a custom model based on the lighter version of BERT, also proved competitive with 87.10% accuracy (just under BERT’s 89.33%). Our findings confirm the importance of specialized models such as BioBERT and also support impressions around more general solutions like XLNet and well-tuned transformer architectures with fewer parameters (in this case, LastBERT) in medical domain tasks. Index Terms—Medical Conditions, Computational Biology, Neoplasms, Cardiovascular, Nervous System, Digestive system, Natural Language Processing, Deep learning, Transformer mod- els, BioBert, XLNet, LastBERT; The widespread of information and the internet has led to a huge growth in the content volume of electronic documents posted on the internet. Such large and free-form text is easy to use for automatic text classification [1]. The most common approach is called a bag of words, and binary (on a scale of 0 or 1) are features that can then be utilized in supervised classification algorithms such as Support Vector Machines (SVMs), Naive Bayesian Classifiers (Turbo & Tax- man / NHLBI ), etc.... [2]. Given the relative sparsity and simplicity with which some phrases can be dismissed, as well as little training data, research has turned toward focusing on more complex traits. The text classification, especially the medical test classification in the field of text mining, gets more attention, as it serves with a large dataset on medical records and literature [3]. One of the most important NLP areas is text classification, which helps in assigning a set of documents to the correct categories based on their content. The latest developments in the domain of NLP (Natural Language Processing) have completely changed how text categorization is implemented as a part of medical activities. Thanks to word embeddings, transformers, and deep learning architectures (i.e., the backbone of research in NLP), systems can now categorize medical texts more accurately and with higher efficiency than ever before! Word embeddings like Word2Vec and GloVe can elucidate medical lexicon learning by revealing semantic relations among words. More recently, transformer- based architectures (most notably BERT = Bidirectional En- coder Representations from Transformers) demonstrated an extraordinary ability to deal with the intricacies of natural language as well as capturing context [4]. These advanced natural language processing methods used to alleviate the issue of ambiguity and polysemy in medical text seem promising. For instance, transformer-based models do better than we are at solving the select few about-counts by letting their whole decision-making process consider the surrounding context to differentiate between noise and names of different medical problems that sound exactly like or similar to symptoms. These models are also able to find intricate patterns from big datasets, hence the detection of new or uncommon medical conditions. The aim of the research is to see how state-of-the-art natural language processing systems perform in identifying diseases from text data. Furthermore, we will compare multiple ad- vanced NLP models and discuss which of their capabilities are more suitable for medical text classification. We will arXiv:2411.12712v1  [cs.CL]  19 Nov 2024",
    "body": "Enhancing Multi-Class Disease Classification:\nNeoplasms, Cardiovascular, Nervous System, and\nDigestive Disorders Using Advanced LLMs\nAhmed Akib Jawad Karim\nDepartment of Computer Science and Engineering\nBRAC University\nDhaka, Bangladesh\nakibjawaad@gmail.com\nMuhammad Zawad Mahmud\nDepartment of Electrical and Computer Engineering\nNorth South University\nDhaka-1229, Bangladesh\nzawad.mahmud1@northsouth.edu\nSamiha Islam\nDepartment of Electrical and Computer Engineering\nNorth South University\nDhaka-1229, Bangladesh\nsamiha.islam2@northsouth.edu\nAznur Azam\nDepartment of Computer and Science Engineering\nBangladesh Army University of Science and Technology\nSaidpur, Bangladesh\naznurazam2@gmail.com\nAbstract—In this research, we explored the improvement in\nterms of multi-class disease classification via pre-trained language\nmodels over Medical-Abstracts-TC-Corpus that spans five medi-\ncal conditions. We excluded non-cancer conditions and examined\nfour specific diseases. We assessed four LLMs, BioBERT, XLNet,\nand BERT, as well as a novel base model (Last-BERT). BioBERT,\nwhich was pre-trained on medical data, demonstrated superior\nperformance in medical text classification (97% accuracy). Sur-\nprisingly, XLNet followed closely (96% accuracy), demonstrating\nits generalizability across domains even though it was not pre-\ntrained on medical data. LastBERT, a custom model based on\nthe lighter version of BERT, also proved competitive with 87.10%\naccuracy (just under BERT’s 89.33%). Our findings confirm the\nimportance of specialized models such as BioBERT and also\nsupport impressions around more general solutions like XLNet\nand well-tuned transformer architectures with fewer parameters\n(in this case, LastBERT) in medical domain tasks.\nIndex Terms—Medical Conditions, Computational Biology,\nNeoplasms, Cardiovascular, Nervous System, Digestive system,\nNatural Language Processing, Deep learning, Transformer mod-\nels, BioBert, XLNet, LastBERT;\nI. INTRODUCTION\nThe widespread of information and the internet has led to a\nhuge growth in the content volume of electronic documents\nposted on the internet. Such large and free-form text is\neasy to use for automatic text classification [1]. The most\ncommon approach is called a bag of words, and binary (on\na scale of 0 or 1) are features that can then be utilized in\nsupervised classification algorithms such as Support Vector\nMachines (SVMs), Naive Bayesian Classifiers (Turbo & Tax-\nman / NHLBI ), etc.... [2]. Given the relative sparsity and\nsimplicity with which some phrases can be dismissed, as well\nas little training data, research has turned toward focusing on\nmore complex traits. The text classification, especially the\nmedical test classification in the field of text mining, gets\nmore attention, as it serves with a large dataset on medical\nrecords and literature [3]. One of the most important NLP\nareas is text classification, which helps in assigning a set of\ndocuments to the correct categories based on their content. The\nlatest developments in the domain of NLP (Natural Language\nProcessing) have completely changed how text categorization\nis implemented as a part of medical activities. Thanks to word\nembeddings, transformers, and deep learning architectures\n(i.e., the backbone of research in NLP), systems can now\ncategorize medical texts more accurately and with higher\nefficiency than ever before! Word embeddings like Word2Vec\nand GloVe can elucidate medical lexicon learning by revealing\nsemantic relations among words. More recently, transformer-\nbased architectures (most notably BERT = Bidirectional En-\ncoder Representations from Transformers) demonstrated an\nextraordinary ability to deal with the intricacies of natural\nlanguage as well as capturing context [4]. These advanced\nnatural language processing methods used to alleviate the issue\nof ambiguity and polysemy in medical text seem promising.\nFor instance, transformer-based models do better than we are\nat solving the select few about-counts by letting their whole\ndecision-making process consider the surrounding context to\ndifferentiate between noise and names of different medical\nproblems that sound exactly like or similar to symptoms. These\nmodels are also able to find intricate patterns from big datasets,\nhence the detection of new or uncommon medical conditions.\nThe aim of the research is to see how state-of-the-art natural\nlanguage processing systems perform in identifying diseases\nfrom text data. Furthermore, we will compare multiple ad-\nvanced NLP models and discuss which of their capabilities\nare more suitable for medical text classification. We will\narXiv:2411.12712v1  [cs.CL]  19 Nov 2024\n\nalso consider the implications for public health surveillance\nand patient care in healthcare settings. Thus, the original\ncontributions of this work are as follows:\n• For this dataset, our model BioBERT blasted all the\navailable models and ended up with an accuracy rate of\n97%. Also, based on Table IV, this model outperformed\nexisting text classification models for medical conditions.\n• Our novelty is annotated with the introduction of our\ncustom language model, LastBERT which we trained\nas a smaller version of BERT. The model achieved\nan accuracy of 87.06 %, which is very comparable to\nthe BERT performance at 89.32% but with only 29M\nparameters vs the 110M in the case of BERT. This model\nof transformers used fewer computational resources pro-\nducing satisfactory results.\nII. RELATED WORK\nIn recent years, NLP has seen impressive advances with\nthe development of powerful (pre-trained) language models\nlike BERT and all its extensions (SciBERT, etc.), such as\nBERT. These models achieved remarkable performance in\ndifferent tasks that concerned text classification, such as\nmedical diagnosis or disease classification. Prior studies have\ndemonstrated the utility of these models in automatically\nrecognizing and categorizing diseases, suggesting that they\ncould strengthen multi-class disease classification for major\ncategories (e.g., neoplasms), as well as some other types such\nas cardiovascular, nervous system, or digestive diseases. Blom\n[5] in her MS thesis building a conversational agent with rasa\nto enrich a medical abstracts dataset used the same dataset for\nthe text classification part. She used one ML and two LLMs\nand got the highest accuracy from SciBERT, which is 65%.\nPrabhakar and Won [6] classified medical text using hybrid\ndeep learning models. The Hallmarks dataset and AIM dataset\nwere utilized in this study. The datasets are a collection of\nbiomedical paper abstracts with cancer hallmark annotations\nand biomedical publication abstracts, which were written\naround 1852. This set of data includes three characteristics\nof cancer, including initiating metastasis and invasion, cellular\nenergetics, and inflammation that is promoted by tumors. Their\nhybrid BiGRU performed remarkably with 95.76%. Ahmed\net al. [7] used deep neural networks to classify biomedical\ntexts for cardiovascular diseases. They used the OHSUMED-\n400 dataset, which contains abstracts of PubMed documents\nfrom 23 cardiovascular disease classes. This model, ultimately\na DNN with BLSTM, achieved 49.4% accuracy on their\ntest set of spectrograms. Chaib et al. [8] did multi-label\ntext classification of cardiovascular disease reports based on\nthe GL-LSTM model. For this study, they utilized a public\ndataset called Ohsumed. The master file of the Ohsumed\ndatabase includes 50,216 medical summaries for patients in\nthe year 1991 and consists of nearly half a million words\nfrom unstructured text. The collection is partitioned into three\nsubsets–training (10 percent), testing (51 out of every hundred\ndocuments are used), and background distribution (39/100\nrecords). The model already reached 92.7% accuracy. Cui et\nal. [9] studied multi-label text classification of cardiovascular\ndrug attributes based on BERT and BiGRU. The data was\ncollected from NMPA. After analysis of ablation and crossover\nexperiments, their proposed model achieved an accuracy of\n83.39%. Hagan et al. [10] classified cardiovascular disease\nusing ML methods. They used two publicly available datasets.\nThe first one is the arrhythmia dataset from the repository\nof machine learning databases provided by the University of\nCalifornia Irvine (UCI), and the second one is the Kaggle\ncardiovascular disease dataset. For the UCI dataset, ExtraTrees\noutperformed all with 96% accuracy, with gradient boosting\nachieving 94%. Although, for the Kaggle dataset, gradient\nboosting was able to come on top with 74% accuracy. Kanwa\net al. [11] classified cardiovascular disease using ML methods.\nThey used the cardiovascular disease dataset from Kaggle. The\ndataset contains a total of 70,001 records. Among the ML\nmodels, the XGBOOST and Na¨ıve Bayes performed well with\n92.34% and 92.31% accuracy, respectively.\nIII. METHODOLOGY\nA. Dataset\nThe Medical-Abstracts-TC-Corpus [12] was used in this\ninvestigation available at GitHub. This is a text dataset on\nfive various medical conditions. The five conditions are neo-\nplasms, digestive system diseases, nervous system diseases,\ncardiovascular diseases, and general pathological conditions.\nThese are labeled from 1 to 5 on the above sequence, respec-\ntively. There were 14,438 records, among which 3,163 were\nneoplasms, 1,494 were digestive system diseases, 1,925 were\nnervous system diseases, 3,051 were cardiovascular diseases,\nand 4,805 were general pathological conditions. As the general\npathological condition is not infected, we decide to exclude it.\nThe data was split into 80-20 ratios for training and testing,\nrespectively. A total of 11,550 records were taken for training\nand 2,888 for testing. Table I represents a partial medical\nabstract of each medical condition of the dataset.\nB. Data Prepossessing\nAs the database was imbalanced, we used the up-sampling\nand down-sampling techniques to balance it. Among the\ntraining set, data was divided into 80:20 ratio again. Here,\n80% was used for training, and 20% validation set was used\nfor validation. The test set was used to evaluate the models\nafter training was completed. Finally, pandas DataFrames were\nconverted into Hugging Face datasets for training, validation,\nand testing.\nC. Training Methodology\n1) BioBERT: Hugging Face model BioBERT (biobert-base-\ncased-v1.1) was loaded along with the BioBERT tokenizer\nand model for sequence classification, as well as a function\nto tokenize medical abstracts applying padding/truncation so\nthey all have the same size. These functions then tokenize\nthe datasets in a batched fashion for training or evaluation.\nTraining arguments were epoch = 10, batch size = 16, warmup\nsteps = 100, early stopping patience = 3, and weight decay of\n0.1, respectively, for training.\n\nTABLE I: Dataset Representation (Summary)\nLabel\nDisease\nMedical Abstract (Truncated)\n1\nNeoplasms\nNeuropeptide Y and neuron-specific enolase\nlevels in benign and malignant pheochromo-\ncytomas. Neuron-specific enolase (NSE) is the\nisoform of enolase, a glycolytic enzyme found\nin the neuroendocrine system...\n2\nDigestive\nsystem\ndiseases\nSexually transmitted diseases of the colon, rec-\ntum, and anus. The challenge of the nineties.\nDuring the past two decades, an explosive\ngrowth in both the prevalence and types of\nsexually transmitted diseases...\n3\nNervous sys-\ntem diseases\nDoes carotid restenosis predict an increased\nrisk of late symptoms, stroke, or death? The\nidentification of carotid restenosis as an unex-\npected late complication of carotid endarterec-\ntomy has prompted concerns regarding its im-\nportance as a source of new cerebral symp-\ntoms, stroke, and death....\n4\nCardiovascular\ndiseases\nPharmacomechanical thrombolysis and angio-\nplasty in the management of clotted hemodial-\nysis grafts: early and late clinical results. The\nresults of pharmacomechanical thrombolysis\nand angioplasty of 121 thrombosed hemodial-\nysis grafts...\n2) XLNet: The XLNetTokenizer and model were config-\nured for sequence classification, i.e., the ’xlnet-base-cased’\ntype. A function to tokenize medical abstracts, pad, and\ntruncate it as a maximum length of 512 tokens. The image\nprocessing function is then applied in a batched manner to the\ntraining, validation, and test datasets to get them ready based\non what comes next: model training evaluation. Setting the\ntraining arguments as epoch = 10, early stopping patience =\n2, weight decay= 0.1, and batch size=16 with warm-up steps\n=100 for training of pre-train BioBERT model.\n3) BERT: We have loaded a pre-trained BERT Tokenizer\n(for ’bert-base-cased’ model). An anonymization function is\nthen written to prepare our medical abstracts for the tokenizer\nby trimming and padding them, readying each input length\nof 512 tokens. This tokenization function is then applied to\nthe train, validation, and test datasets in a batched manner\nto preprocess them for model training and evaluation. The\ntraining arguments are epoch = 10, batch size = 16, warmup\nsteps = 100, early stopping patience = 2, and weight decay\n0.1 for training the model.\n4) LastBERT: BERT tokenizer was created, and the model\n’bert-base-uncased’ was loaded. It has a tokenization function,\nwhich is also specific to the task of processing medical ab-\nstracts, padding and truncating each ”abstract” – which guaran-\ntees that all are 512 tokens long. The CustomBERTModel\nclass is kind of a wrapper over the base BERT model (we use\nBertForSequenceClassification), and we simply add a dropout\nlayer on top of Bert’s output so as to avoid Overfitting. The\nforward method implements both the calculation of loss and\nlogits in it. This initializes the custom model as a function\ncreatecustomstudentmodel that tweaks configuration for\nsmaller BERT models by tuning parameters such as hidden\nsize, num attention heads, num of hidden layers, and interme-\ndiate size. After that, this customized model will be transferred\nto the available P100 GPU for training/inference. The training\nwas done with the following training arguments: number of\nepochs=10, batch size=16, warm-up steps = 100, weight decay\nis decaying rate = 0.1, and early stopping patience =2 [13].\nD. Work Flow Diagram\nThe workflow diagram of this study is visualized in Fig. 1\nFig. 1: Workflow diagram of the proposed study of multi-class\ndisease classification\nIV. RESULT ANALYSIS\nIn this section, model training curves, performance curves,\nconfusion matrix, and ROC curve for the two best models\nwith the worst model’s training curves and confusion matrix\nare shown.\nA. BioBERT\nFig. 2 displays the training loss and accuracy curve for the\nBioBERT Model. The training and validation losses decrease\nsteadily, which is a parameter for effective learning as well as\nimproved generalization. Meanwhile, accuracy rises to about\n1.0 and remains there confirming the performance of being\nable to classify those medical conditions becomes extremely\nstable as training goes on.\nFig. 3 represents the performance curve of BioBERT model.\nThe three metrics demonstrate an acceptable smooth behavior\nover the epochs, from around 0.89–0.90 in the first epoch\nto roughly 0.96 by (7/12). This means that performance\nimproves, without loss in accuracy or reliability as the model\nis trained further into more epochs.\nIn Fig. 4, the confusion matrix of the BioBERT is displayed.\nThe matrix is quite accurate with a large number of correct\n\nFig. 2: Loss and accuracy curve of BioBERT\nFig. 3: Performance curve of BioBERT\npredictions being along the diagonal which means that the\nmodel was able to correctly classify many samples in each\nclass. A few cases of neoplasms were wrongly labeled as other\ndiseases, thus the generalization power and robustness for a\ntext classification task are excellent.\nFig. 4: Confusion matrix of BioBERT\nThe ROC curve for the BioBERT model is visualized in\nFig. 5. All classes get an AUC of 1.00 because few false\npositives appear in the classification performance. The curve\ndisplays the capability of that model to differentiate between\nvarious healthcare classes in a dataset.\nFig. 5: ROC curve of BioBERT\nB. XLNet\nIn Fig. 6, the training loss and accuracy graph for the XLNet\nmodel is shown. The training and validation losses are always\nheading down which is also a good point that the model\nlearned something new so previously made errors decreased.\nAt the same time, accuracy also continuously grows to over\n0.9, which means better ability in each epoch to detect medical\nconditions correctly.\nFig. 6: Loss and accuracy curve of XLNet\nFig. 7 displays the performance metrics graph for the XLNet\nmodel. The lowest score for all was 0.88-0.89 on the first\nepoch. The three metrics show a smooth improvement, with\nvalues nearly reaching 0.96 by epoch six while still increasing\nin value. This means the model is getting better and improving\nfor text classification with every epoch by its ability to\nprecisely categorize output classes, which in turn conclusively\nincreases precision-recall and overall performance as it goes\non training.\nFig. 8, the confusion matrix of the XLNet model is shown.\nThe matrix is highly accurate, you can see most of the predic-\ntions are along a diagonal true positive and false negative. Still,\ncertain misclassifications do occur; a number of neoplasms\nwere classified as other diseases, though sparsely. To sum up,\n\nFig. 7: Performance curve of XLNet\nthe matrix shows that the XLNet model performs well in text\nclassification tasks and is able to predict correctly for over\n95% of samples across all classes.\nFig. 8: Confusion matrix of XLNet\nROC curves for the XLNet model are visualized in Fig. 9.\nThese class-averaged precision-recall curves are perfect, in the\nsense of having AUCs equal to 1.00, which essentially means\nthat there is a full separation and hence few false negatives\nor positives at all for ideal classifiers. This graph highlights\nthat the XLNet model performs among the best in accurately\nclassifying different medical classes present within our dataset.\nC. LastBERT\nFig. 10 shows the loss and accuracy curve for LastBERT.\nThe loss in the training is steadily decreasing, demonstrating\nthat the model keeps learning. Although validation loss de-\ncreases at the beginning, after the third epoch it starts to rise\na little which could mean overfitting is in implementation.\nHowever, the accuracy is pretty constant and remains close\nto 0.9, so this probably means that our model performs\nclassification very well throughout all training periods.\nThe confusion matrix of the LastBERT is shown in Fig.\n11. There is a higher concentration of true positive predictions\nin the diagonal on the matrix — and neoplasms, as well as\nFig. 9: ROC curve of XLNet\nFig. 10: Loss and accuracy curve of LastBERT\ncardiovascular heart disease are particularly diagnosed more\ncorrectly than often wrong ones. Nonetheless, some errors are\nworth noting — e.g., category confusion with nervous system\ndiseases. The matrix shows that LastBERT works fine, but\nit could be improved especially in separating similar disease\ncategories.\nFig. 11: Confusion matrix of LastBERT\n\nD. Model Performance Comparison\nTable II compares the performance metrics with our four\nlanguage models (BioBERT, XLNet, BERT, and LastBERT)\nfocusing on both macro-average columns, then weighted av-\nerage over precision/recall/F1 score. BioBERT and XLNet\nshow a good performance, which has more than 0.96 in\nall metrics (positive means classifying). Compared to these\nbaseline performance levels, BERT and LastBERT have worse\nmetrics overall, especially the latter, which has been shown\nonly as good for lower classification accuracy numbers.\nTABLE II: Performance Metrics Comparison\nModels\nMacro Average\nWeighted Average\nPrecision\nRecall\nF1 Score\nPrecision\nRecall\nF1 Score\nBioBERT\n0.97\n0.97\n0.97\n0.97\n0.97\n0.97\nXLNet\n0.96\n0.96\n0.96\n0.96\n0.96\n0.96\nBERT\n0.81\n0.82\n0.81\n0.83\n0.82\n0.83\nLastBERT\n0.78\n0.79\n0.78\n0.81\n0.80\n0.80\nTable III shows the number of parameters of the models\nin millions, accuracy, F1 score, precision, and recall of the\nfour applied LLMs in this study. Clearly, in terms of accuracy\nand f1 score, BioBERT outperformed all the other models. Al-\nthough XLNET was not trained on biomedical datasets, it still\nperformed well. Although it is the smaller model, lastBERT\nproduced respectable results with only 29M compared to all\nthe others. This means faster training in a low computational\nresource. BERT is a 110M large LLM.\nTABLE III: Model’s Results\nModels\nParameters\nAccuracy (%)\nF1 Score\nPrecision\nRecall\nBioBERT\n110M\n97.00\n0.9656\n0.9662\n0.9656\nXLNet\n110M\n96.00\n0.9577\n0.9588\n0.9578\nBERT\n110M\n89.33\n0.8932\n0.8943\n0.8933\nLastBERT\n29M\n87.10\n0.8706\n0.8943\n0.8722\nAs shown in Table IV, the models are compared to those\npreviously studied. It is evident from the table that the\nBioBERT model overpowers all others in the framework.\nTABLE IV: Result Comparison\nStudy\nDataset\nModel\nAccuracy (%)\nThis paper\nMedical-Abstracts-\nBioBERT\n97.00\nTC-Corpus\nThis paper\nMedical-Abstracts-\nXLNet\n96.00\nTC-Corpus\n[10]\nArrhythmia dataset\nExtraTrees\n96.00\nfrom UCI\n[6]\nHallmarks dataset\nBiGRU\n95.76\nand AIM dataset\n[8]\nOhsumed\nGL-LSTM\n92.70\n[11]\nCardiovascular Disease\nXGBoost\n92.34\ndataset from Kaggle\nV. CONCLUSION AND FUTURE WORK\nAdvanced large language models are used in this study to\nclassify text abstracts to determine which medical condition\nthe person is in. It is not surprising that BioBERT pre-trained\nwith medical text performed best (97% accuracy) among\nthe models, demonstrating once again how essential domain-\nspecific pre-trained model training is to medical-text classifi-\ncation tasks. An extremely strong performance of 96% fresh\nnormal precision, suggesting that general-purpose models still\nmay provide decent quality in the at-spontaneous field when\nfinetuned (XLNet not even created for the medical domain;\nyet?) Although the custom-developed LastBERT model was\nsmaller and more resource-efficient, It achieved a comparable\nperformance against a much larger baseline BERT, which\nsuggests that even with content resources, the use of purpose-\noptimized models could help. Several future works can be\nperformed to enhance the accuracy and efficiency of the\nmodel, and the following potential options could be considered\nfor experimentation. One possible first step in this direction\nwould be to experiment with including more domain-specific\ndata at training time from which models like XLNet and\nLastBERT could benefit. This could potentially lead us down\na second path of avant-garde algorithms, or maybe the answer\nis to do more work on hybrid, so as you get many benefits\nfrom one methodology and others benefitting in another, then\nat least have this slant going for classification can be pretty\nsolid. Another benefit of this research is the fact that it reduces\nmodel bloat, making them actually usable, which could come\nin handy for deploying solutions more extensively, including\nresource-limited healthcare settings.\nREFERENCES\n[1] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for\ntext classification,” arXiv preprint arXiv:1511.08630, 2015.\n[2] T. Joachims, “Text categorization with support vector machines: Learn-\ning with many relevant features,” in European conference on machine\nlearning.\nSpringer, 1998, pp. 137–142.\n[3] A. R. Aronson, “Effective mapping of biomedical text to the UMLS\nMetathesaurus: the MetaMap program.” in Proceedings of the AMIA\nSymposium.\nAmerican Medical Informatics Association, 2001, p. 17.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[5] F. A. M. Blom, “Building a Conversational Agent with Rasa to Enrich a\nMedical Abstracts Dataset,” Ph.D. dissertation, Tilburg University, 2023.\n[6] S. K. Prabhakar and D.-O. Won, “Medical text classification using\nhybrid deep learning models with multihead attention,” Computational\nintelligence and neuroscience, vol. 2021, no. 1, p. 9425655, 2021.\n[7] N. Ahmed, F. Dilmac¸, and A. Alpkocak, “Classification of Biomedical\nTexts for Cardiovascular Diseases with Deep Neural Network Using a\nWeighted Feature Representation Method,” in Healthcare, vol. 8, no. 4.\nMDPI, 2020, p. 392.\n[8] R. Chaib, N. Azizi, N. E. Hammami, I. Gasmi, D. Schwab, and A. Chaib,\n“GL-LSTM Model For Multi-label Text Classification Of Cardiovascular\nDisease Reports,” in 2022 2nd International Conference on Innovative\nResearch in Applied Science, Engineering and Technology (IRASET).\nIEEE, 2022, pp. 1–6.\n[9] H. Cui, L. Zhang, X. Zhu, X. Guo, and Y. Peng, “Multi-label text\nclassification of cardiovascular drug attributes based on BERT and\nBiGRU,” Journal of Intelligent & Fuzzy Systems, no. Preprint, pp. 1–11,\n2024.\n[10] R. Hagan, C. J. Gillan, and F. Mallett, “Comparison of machine learning\nmethods for the classification of cardiovascular disease,” Informatics in\nMedicine Unlocked, vol. 24, p. 100606, 2021.\n[11] F. Kanwal, M. K. Abid, M. S. Maqbool, N. Aslam, and M. Fuzail,\n“Optimized Classification of Cardiovascular Disease Using Machine\nLearning Paradigms,” VFAST Transactions on Software Engineering,\nvol. 11, no. 2, pp. 140–148, 2023.\n\n[12] T. Schopf, D. Braun, and F. Matthes, “Evaluating unsupervised text clas-\nsification: zero-shot and similarity-based approaches,” in Proceedings of\nthe 2022 6th International Conference on Natural Language Processing\nand Information Retrieval, 2022, pp. 6–15.\n[13] A. A. J. Karim, K. H. M. Asad, and M. G. R. Alam, “Larger models\nyield better results? streamlined severity classification of adhd-related\nconcerns using bert-based knowledge distillation,” 2024. [Online].\nAvailable: https://arxiv.org/abs/2411.00052",
    "pdf_filename": "Enhancing_Multi-Class_Disease_Classification_Neoplasms,_Cardiovascular,_Nervous_System,_and_Digestiv.pdf"
}