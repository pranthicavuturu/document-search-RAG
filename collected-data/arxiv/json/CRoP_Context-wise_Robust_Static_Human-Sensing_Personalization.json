{
    "title": "CRoP Context-wise Robust Static Human-Sensing Personalization",
    "abstract": "otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2024 Association for Computing Machinery. XXXX-XXXX/2024/11-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: November 2024. arXiv:2409.17994v4  [cs.AI]  19 Nov 2024",
    "body": "CRoP: Context-wise Robust Static Human-Sensing Personalization\nSAWINDER KAUR1, AVERY GUMP2, JINGYU XIN1, YI XIAO4, HARSHIT SHARMA4, NINA R\nBENWAY3, JONATHAN L PRESTON1, ASIF SALEKIN4\n1SYRACUSE UNIVERSITY\n2UNIVERSITY OF WISCONSIN-MADISON\n3UNIVERSITY OF MARYLAND-COLLEGE PARK\n4ARIZONA STATE UNIVERSITY\nThe advancement in deep learning and internet-of-things have led to diverse human sensing applications. However, distinct\npatterns in human sensing, influenced by various factors or contexts, challenge the generic neural network model’s performance\ndue to natural distribution shifts. To address this, personalization tailors models to individual users. Yet most personalization\nstudies overlook intra-user heterogeneity across contexts in sensory data, limiting intra-user generalizability. This limitation\nis especially critical in clinical applications, where limited data availability hampers both generalizability and personalization.\nNotably, intra-user sensing attributes are expected to change due to external factors such as treatment progression, further\ncomplicating the challenges. To address the intra-user generalization challenge, this work introduces CRoP, a novel static\npersonalization approach. CRoP leverages off-the-shelf pre-trained models as generic starting points and captures user-specific\ntraits through adaptive pruning on a minimal sub-network while preserving generic knowledge in the remaining parameters.\nCRoP demonstrates superior personalization effectiveness and intra-user robustness across four human-sensing datasets,\nincluding two from real-world health domains, underscoring its practical and social impact. Additionally, to support CRoP’s\ngeneralization ability and design choices, we provide empirical justification through gradient inner product analysis, ablation\nstudies, and comparisons against state-of-the-art baselines.\nAdditional Key Words and Phrases: Intra-user Generalization, Personalization, Contest-wise Robustness\nACM Reference Format:\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4,\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University . 2024. CRoP: Context-wise Robust\nStatic Human-Sensing Personalization. 1, 1 (November 2024), 33 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nRecent automated human sensing applications—like activity recognition, fall detection, and health tracking -\nrevolutionize daily life, especially in personal health management [85]. However, unique user patterns and natural\ndistribution shifts [24] caused by behaviors, physical traits, environment, and device placements [73, 78] lead\nto the underperformance of generic sensing models in practical use. To tackle this, various domain adaptation\ntechniques have been explored, with personalization widely used to adapt a generic model to the target user’s\nspecific domain or natural distribution [1, 9, 32, 39, 53, 62, 67]. In literature, personalization occurs either during\nthe enrollment phase (static) [10, 15, 45] or continuously throughout application use, a process known as continual\nlearning [13, 44, 87, 90].\nContinual learning methods involve retraining models with new data, either supervised or unsupervised.\nWhile these approaches enable models to adapt to new patterns and changes in data distribution over time, they\nAuthor’s address: Sawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif\nSalekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first\npage. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n© 2024 Association for Computing Machinery.\nXXXX-XXXX/2024/11-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: November 2024.\narXiv:2409.17994v4  [cs.AI]  19 Nov 2024\n\n2\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\noften face efficiency challenges. As Harun et al. [28] points out, these methods, particularly those that prevent\ncatastrophic forgetting [13, 19, 35, 50, 51, 74, 75, 87, 88], often struggle with memory, computation, and storage\nrequirements inefficiencies, limiting their real-world applicability. Frequent model updates on user devices can\nintroduce significant delays. This is especially true for devices with limited processing power, such as smartphones\nor wearables. These delays reduce the responsiveness of the application. They also lead to battery drain and\ninefficiency in real-time applications. Furthermore, to avoid catastrophic forgetting, several continual learning\napproaches rely on replay-based methods which require storage of previously encountered data [29, 80, 81].\nThis not only increases storage requirements but also raises privacy concerns. Constantly storing potentially\nidentifiable information increases the risk of data breaches or misuse. This is especially concerning in regulatory\nenvironments, such as healthcare [83].\nAdditionally, supervised continual learning approaches face the significant challenge of requiring expert-\nlabeled data for each new batch of user interactions, also termed as label delay [12]. This demand for continuous,\nhigh-quality labels can make it impractical for many applications, especially in settings where expert annotation\nis costly or unavailable. For example, in clinical or health monitoring contexts, each new data point may need to\nbe validated by professionals, which is time-consuming and difficult to scale.\nIn contrast, static personalization offers an efficient alternative by customizing the model with a one-time,\nlimited dataset collected during enrollment. This approach minimizes computation, requires no ongoing data or\nlabel storage or collection, and reduces user engagement, making it especially suitable for resource-constrained\nhuman-sensing applications. However, existing such studies often overlook intra-user variability due to factors\nlike changes in magnetic field [63], sensor position [57], terrain [37], or the health symptoms [55], leading to poor\nintra-user generalizability for contexts not present during personalization. For instance, a smartphone activity\nrecognition model personalized with handheld data may perform poorly when the phone is in a pocket.\nStatic personalization is particularly crucial for clinical datasets, which often suffer from data scarcity, leading\nto reduced robustness of lab-validated models for prospectively collected users [8]. It enhances model accuracy\nfor clinical users whose traits are underrepresented in the global model’s training data. In contrast, continuous\nsupervised personalization is generally infeasible in many health domains since ground truths must be validated\nby clinicians, making it impractical in continuous settings, especially in remote or mobile health applications.\nNotably, the distribution of clinical data is expected to shift, even within the same individual. For instance,\nin clinical speech technologies, changes in data distribution over time may occur due to the progression of\nneurodegenerative diseases, relevant for disease monitoring apps [72], or through desired learning mechanisms\nresulting from the use of technology, as seen in automated speech therapy apps [6]. Similarly, in wearable-based\nstress monitoring, psychophysiological data distribution varies as individuals face different stressors [54]. This\nresearch defines ‘context’ as the intra-user data distribution formed by varying factors.\nThis research gap is worsened since static personalization typically relies on a small sample set from the target\nuser, covering limited contexts—particularly in clinical settings or applications with data scarcity [6, 8]. Commer-\ncial human sensing technologies like Google Assistant, Amazon Alexa, and Apple’s Siri also statically personalize\nspeech recognition models using limited phrases [4, 58, 77]. Similarly, the Apple Watch uses initial calibration for\nenhanced running activity tracking [2, 59]. This limited context during personalization is problematic, as shown\nin Section 3, where we demonstrate that static personalization may improve performance in training contexts\nbut can also significantly degrade it in other unseen contexts for the same user.\nTherefore, given the importance of static personalization in human sensing, this paper addresses its intra-user\ngeneralizability gap. Since personalized models are tailored to individual users and not intended for use by others,\ninter-user generalizability is outside the scope of this work. An additional challenge is that several personalization\napproaches in the literature, such as EMGSense [15] and MobilePhys [45], train their own generic models,\nsometimes even using data from target users. This limits privacy-preserving generic model sharing, and, in some\ncases, requires target users to share sensitive data, raising additional privacy concerns. It also complicates adding\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n3\nAvailable \nContext\nUnseen \nContexts\nOff-the-Shelf Generic Model\nPersonalization for a Specific User\nPersonalization\nGeneralization:\nEvaluation on all \ncontexts\nFig. 1. Problem Setting\na new user, as the generic model must be updated and redistributed. While model redistribution is common in\nfederated learning, human-sensing personalization isn’t limited to federated learning methods. To address this\nlimitation, this paper only uses pre-trained, off-the-shelf models as generic models, which do not require data\nfrom the target users.\nIn summary, Figure 1 outlines the paper’s scope and objective. The goal is to develop an intra-user robust\napproach to personalize an off-the-shelf generic model for a specific user using limited data from limited contexts.\nThe primary objective is to ensure that the personalized model thus obtained exhibits robust generalization\ncapabilities across unseen contexts. Crucially, unseen context data takes no part in training or adjusting the\npersonalized model outcome, and the generic model remains entirely off-the-shelf, with no accessibility for\nmodification or design choices. These constraints highlight the real-world impact of this research, particularly in\nclinical settings where privacy concerns often limit data sharing [49, 61], and only trained off-the-shelf models\nare shared among researchers and developers.\nTo achieve the research objective in Figure 1, this paper introduces CRoP, a novel approach to create context-\nwise intra-user robust static personalized models. The key contributions are:\n(1) It facilitates utilizing readily available off-the-shelf pre-trained models with state-of-the-art accuracy,\neliminating the need for training customized generic models, thus reducing training effort and providing a\nstrong foundation for personalization.\n(2) Personalization with intra-user generalization has two challenges: i) learning user-specific patterns and\nii) keeping information about generic patterns intact. During personalization, CRoP leverages pruning\nand regularization to capture the user-specific patterns present in the available-context data on a minimal\nsub-network of the model. The remaining parameters of the model are utilized to retain generic patterns that\nare not present in the available context, i.e., personalization data, thus achieving intra-user generalizability\nto unseen contexts. Pruning is widely used in other areas, including in continuous learning literature (e.g.,\nPacknet [50], and Piggyback [51]), to obtain sub-networks for each new task which not only requires data\nfrom every task (here unseen context) but also needs identification of the task differences (i.e., difference\namong unseen contexts) in order to select the sub-network for inference. In contrast, this is the first paper\nto use pruning with adaptive intensity to capture the user’s traits in a compressed sub-network, thus\nstriking a balance between personalization and robustness in new, unseen contexts for the same user\nwithout identifying the contexts, their differences, or requiring data from unseen contexts for adaptation.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n4\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n(3) To showcase CRoP’s efficacy, comprehensive evaluations were performed on four human sensing datasets:\nPERCERT-R [7]: a clinical speech therapy dataset, WIDAR [94]: a lab-based WiFi-CSI dataset, ExtraSensory:\na real-world mobile sensing dataset [79], and a stress-sensing dataset [91], while considering two disjoint\ncontexts for each dataset. In order to obtain information about context variation within PERCEP-R and\nStress-sensing, we collaborated with the original authors of these two datasets. This additional annotation\nwill be released alongside this publication.\n(4) On an average across all datasets, CRoP shows a personalization benefit of 35.23 percent points and\ngeneralization benefit of 7.78 percent points. As compared to the best baseline (Packnet), these gains are\n9.18 and 9.17 percent points higher, respectively. Moreover, alongside a detailed ablation study discussion\nin Section 8, an empirical justification of CRoP’s design choices that enable intra-user generalizability\namong different contexts is provided in Section 7.1, employing Gradient Inner Product(GIP) [70] analysis.\nAdditionally, in order to demonstrate the feasibility of on-device personalization through CRoP, we compute\ntraining time and resource requirements for five platforms or devices.\nThe paper is arranged in different sections. Section 2 reviews the related work, while Section 3 presents the\nresults of a preliminary study on the WIDAR datasets, which motivates the proposed approach. Section 4 provides\na formal description of the problem statement, followed by Section 5, which outlines the detailed methodology.\nSection 6 covers the experimental setup, and results. In Section 7, we present an empirical study to justify the\napproach, and Section 8 offers an ablation study to support various design choices. Section 9 discusses runtime\nanalysis, Section 10 explores limitations and future directions, and Section 11 addresses the broader impact of the\nwork. Finally, Section 12 concludes the study.\nThe work is accompanied by an extensive appendix, which provides details of experiment setup, hyperparam-\neters, and link to code ensuring reproducibility in Appendix A. Additionally, Appendix B provides a detailed\nanalysis of the user-specific results for each dataset along with a detailed ablation study.\n2\nRELATED WORK\nAs discussed above, this work aims to personalize off-the-shelf models while ensuring intra-user generalizability\nleveraging limited-context data. While existing approaches attempt to address these adaptation challenges, they\noften impose restrictive conditions. Many require specialized training of the generic model with access to the\ngeneric data, assume knowledge of the target domain, or necessitate target users to share their data from a\nnew/previous-unseen context for adapting the model, in some studies with labels or annotations. Additionally,\nsome methods rely on repetitive model retraining, which hinders their suitability for real-world applications due\nto privacy concerns, computational inefficiency, or excessive user engagement.\nThis section summarizes and critiques key state-of-the-art approaches with similar objectives to those addressed\nin this work, highlighting their contributions and limitations in practical, user-centered adaptation settings.\n2.1\nDomain Adaptation and Generalization\nDomain adaptation (DA) and Domain Generalization (DG) techniques address performance drops due to distribu-\ntional shifts between source and target domains [36]. These methods are useful when models trained on generic\ndata need to adapt to new users or contexts. The key difference is that while DA approaches have access to target\ndomain data, DG techniques work without any target domain data, not even unlabeled [86].\nThe domain generalization techniques can be categorized into Data Manipulation, Representation Learning,\nand other learning strategies [86]. Data Manipulation approaches include techniques like data augmentation\nand synthetic data generation, which are used to increase data diversity, helping models learn invariant features\nacross different domains [68, 84, 95]. Representation Learning aims to capture domain-invariant features using\ntechniques like adversarial training and embedding learning to improve model robustness, such as Invariant\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n5\nrisk minimization (IRM)[3] and domain-adversarial neural networks (DANN)[22]. Learning Strategies such as\nmeta-learning [41] and ensemble learning [14] train the model on multiple simulated domains to improve its\nability to generalize. However, all of these approaches aim to train the generic model while this paper’s scope\ndoes not allow access to generic data and uses a pre-trained off-the-shelf model for adaptation.\nSimilarly, some domain adaptation approaches rely on generic model training while utilizing data from the\ntarget domain [47]. On the other hand, certain domain adaptation approaches require access to the source data\nused to train the generic model without requiring specialized training of the generic model. These approaches\nrely on discrepancy minimization and self-supervision to align source and target distributions by learning\ndomain-invariant representations [21, 42]. A few approaches that do not require access to the source domain\ndata or generic model training are referred to as source-free domain adaptation (SFDA)[43]. Liang et al. [43]\n(SHOT) proposed the transfer of hypothesis from source by freezing the parameter weights for the classifier layers\nand only allowing the feature extraction layer to be finetuned to the new domain. The approach is applicable\nto unsupervised domain adaptation scenarios and employs self-supervised pseudo-labeling to align the target\ndomain’s representations to the source hypothesis.\nGiven that the problem addressed by SHOT aligns with the criteria of having no access to generic data and\navoiding specialized generic model training, we adopted SHOT as one of the baseline approaches in this study.\nHowever, it is important to note that SHOT does not effectively tackle the limitations posed by restricted-context\ndata during the fine-tuning process. As a result, its performance may not adequately generalize to the intra-user\nvariability present in the data, which is a crucial aspect of our research focus.\n2.2\nContinual Learning\nContinual personalization approaches [13, 19, 50, 51, 74, 75, 87, 88] can improve intra-user generalizability by\ncontinually fine-tuning the model as new data arrives. Some of these approaches [19, 74, 75] require specialized\ntraining of the generic model, limiting the use of off-the-shelf pre-trained models. Others like PackNet [50]\nand Piggyback [51] propose supervised methods that require continued steam of labeled data, limiting their\napplication in health-care scenarios. However, all continuous learning approaches require repeated computation\noverhead to adjust the model outcome to new data [60], which can be infeasible in real-time applications, more so\nfor scalable platforms like wearables [65], which is prominent for health sensing such as stress or fall detection.\nDaniels et al. [13] proposes a continual training framework that is compatible with edge devices and requires less\ntraining effort, but it uses a different and smaller model architecture at the personalized devices for generating\nfeature embeddings. Thus hindering the direct usage of off-the-shelf models and increasing the training effort.\nNevertheless, since the problems addressed by Packnet, and Piggyback are the closest to the problem addressed\nin this study, we considered these approaches as baselines.\nPacknet [50] and Piggyback [51] adapt the generic model for a stream of continuously changing tasks. Packnet\nrelies on finetuning, pruning and re-training the model for each new task. On the other hand, Piggyback employs\npruning to learn a new binary mask for each new task which is then applied to a generic model in order to get\ntask-specific results. However, this requires correct identification of the task before choosing the mask. For every\nnew task, the same procedure is repeated, hoping that different sets of parameters will be important for different\ntasks. This aligns well with our initial study, where we identify that different contexts’ data focus on different\nsets of parameters.\nNotably, all continual learning approaches, including PackNet and Piggyback, face a significant limitation: they\nnecessitate access to data from the target domain, whether in a supervised or unsupervised format, in order to\neffectively adapt the model. While these approaches successfully address the challenge of catastrophic forgetting,\nthey primarily focus on contexts that the model has previously encountered, leaving them ill-equipped to adapt\nto entirely unseen domains. In contrast, our research framework specifically excludes access to data from these\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n6\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nunseen contexts during model adaptation, highlighting a critical gap in the capabilities of existing continual\nlearning methodologies. Consequently, PackNet, Piggyback, and similar approaches lack the mechanisms neces-\nsary to adapt to novel domains, underscoring the need for innovative strategies that can operate without prior\nexposure to the target data.\n2.3\nTest-Time Adaptation (TTA)\nContinual learning requires repeated training cycles to adapt models to new information, demanding high\ncomputational resources, storage, and long-term data retention, which raises privacy concerns [83]. This makes\ntraditional continual learning impractical for sensitive domains like healthcare [28].\nTest-time adaptation offers a more efficient and felxible alternative, allowing models to adapt in real-time\nduring deployment by making adjustments based on incoming data without necessitating a complete retraining\nprocess. By focusing on adapting the model in real-time, test-time adaptation minimizes resource consumption as\ncompared to continual learning approaches. Wang et al. [87] relies on generating pseudo-labels using weighted\naveraging and augmentation-averaged predictions, which can introduce bias [43]. While Wu et al. [90] achieves\ntest-time domain adaptation by manipulating batch-normalization layers, thus enforcing a restriction on model\narchitecture. However, Continual Test Time Domain Adaptation (CoTTA) [88] does not have such restrictions\nand allows the use of off-the-shelf models.\nCoTTA [88] is an unsupervised learning approach designed to enhance model adaptation in dynamic envi-\nronments. It utilizes a teacher model, which is initially set up as a replica of the generic model, to generate\npseudo-labels for training. During each iteration, the teacher model is updated using current model state through\na weighted sum, allowing the model to effectively account for evolving data patterns over time. Additionally, the\ncurrent model state is subject to a stochastic restoration of weights, implemented through a Bernoulli distribution\nwith a very low probability of success. This randomization introduces an element of variability that helps prevent\noverfitting and encourages exploration of the weight space, further enhancing the model’s ability to generalize to\nunseen data.\nSince CoTTA addresses a problem setting similar to ours, we adopt it as one of our baseline approaches. In\nour setup, data from the available context can be leveraged for test-time adaptation, while unseen context data\nis reserved solely for empirical evaluation. However, test-time adaptation methods, including CoTTA, tend to\nincrease resource demands during inference, leading to potential delays that can impact real-time usability. This\nadditional computational overhead highlights a key limitation of test-time adaptation in applications where\nreal-time response is essential.\n2.4\nPersonalization Approaches\nA few static personalization approaches [10, 15, 45] aimed for the additional goal of out-of-distribution robustness.\nHowever, these methods require access to the generic model—either to make specific design choices [10], which\nprevents them from utilizing off-the-shelf models, or to incorporate knowledge about the target user’s data\ndistribution during the generic model’s training phase [15, 45], raising privacy concerns, particularly in sensitive\nclinical applications. These requirements do not align with the research objectives of this paper, making them\nunsuitable as baselines.\nFor instance, EMGSense [15] is concerned with robustness to distributional shifts in Surface Electromyography\n(EMG) signals caused by the heterogeneity of biological factors across users. The generic model consists of a\nmulti-model voting ensemble DNN that needs to be trained on N source distributions and some unlabeled data\nfrom target users. To learn user-specific features, this model undergoes multiple training iterations, each using\nupdated user-specific data.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n7\nFeature →\nOff-the-shelf\nGeneric Data\nAdaptation to\nRobustness to\nApproach ↓\nGeneric model\nunseen\navailable context\nunseen Context\nArjovsky et al. [3], Ganin et al. [22]\n✗\n✗\n-NA-\n✓\nLong et al. [47]\n✗\n✗\n✓\n✗\nGanin and Lempitsky [21], Li et al. [42]\n✓\n✗\n✓\n✗\nSHOT [43]\n✓\n✓\n✓\n✗\nPacknet[50] and Piggyback[51]\n✓\n✓\n✓\n✗\nCoTTA [88]\n✓\n✓\n✓\n✗\nEMGSense [15] and MobilePhys [45]\n✗\n✗\n✓\n✗\nPTN [10]\n✓\n✓\n✓\n✗\nCRoP\n✓\n✓\n✓\n✓\nTable 1. Summary of Literature review\nAlso, MobilePhys [45] addresses distributional shifts that can arise due to environmental conditions (e.g.,\nlighting, motion) as well as individual physiological and visual (e.g., clothing, posture) differences in camera-based\ncontactless Photoplethysmography. The generic model is trained using meta-learning, which uses data from\ntarget users to enable quick adaptation to distributional shifts during personalization.\nConsequently, the major drawback faced by EMGSense[15] and MobilePhys [45] is the requirement of knowl-\nedge of the target user’s data in order to train the generic model. This necessitates the transfer of the user’s\nunlabelled data from the user’s device to a central server. Such data can contain sensitive information, and thus,\nits transfer suffers from legal and regulatory barriers [20]. More importantly, since the training of the generic\nmodel requires a subset of each target user’s unlabelled data, the introduction of any new user will require\nre-training of the generic model and its distribution, which is an overhead.\nRecently, Burns et al. [10] extends the idea of triplet loss [66] to the task of personalization (PTN). The\noptimization objective combines the minimization of the Euclidean distance between data from the same target\nclasses while maximizing the Euclidean distance between different target classes in order to learn an embedding\noptimized for the desired task. The features extracted using DNN have been shown to be superior to the engineered\nfeatures [64]. Thus, Burns et al. [10] uses these features to train a KNN for the final prediction task. The approach\nhas been shown to be robust for out-of-distribution data.\nHowever, none of these approaches address intra-user variability in data. Since PTN allows the usage of\noff-the-shelf model and does not require sharing of the personal data for generic model training, we consider\nPTN as one of the baselines.\n2.5\nSelecting Baselines\nThe problem addressed in this work has the following requirements: using an Off-the-shelf generic model, having\nno access to the generic data, and requiring adaptation to available context data and robustness to unseen data.\nAs discussed in the previous sections and summarized in Table 1, the approaches SHOT [43], PTN [10], PackNet\n[50], Piggyback [51], and Continual Test Time Domain Adaptation (CoTTA) [88] comply with the first three\nrequirements. Thus, we adapt these approaches to the problem statement addressed in this work and use them as\nour baselines. These approaches typically rely on training data from a specific context to adapt to that context.\nHowever, the scenario addressed in this work limits training data to only one context, with the expectation\nthat the approach will also perform well in unseen contexts. Since data from unseen contexts is unavailable,\nwe prevent these approaches from using it during training. Instead, the data used for training or adaptation is\nrestricted to data belonging to the available context only, while the unseen context data is used merely for testing.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n8\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n(a) Context C1\nModel\nGeneric\nPersonalized\nΔ\nUser\nC1\nC2\nC1\nC2\nC1\nC2\n0\n63.90\n77.09\n87.06\n65.02\n+23.16\n-11.88\n1\n61.80\n79.78\n89.38\n44.38\n+27.57\n-35.40\n2\n45.63\n79.81\n71.88\n64.45\n+29.75\n-26.62\nAverage\n+26.82\n-24.63\n(b) Context C2\nModel\nGeneric\nPersonalized\nΔ\nUser\nC1\nC2\nC1\nC2\nC1\nC2\n0\n60.80\n73.28\n57.46\n77.30\n-3.34\n+4.02\n1\n59.58\n73.18\n42.38\n93.75\n-17.29\n+20.57\n2\n46.13\n80.46\n40.19\n87.15\n-5.93\n+6.69\nAverage\n-8.85\n+10.43\nTable 2. Performance Comparison in terms of inference accuracy of Generic model with conventionally trained personalized\nmodel\n(a) Context C1\n(b) Context C2\nFig. 2. Heat map for the absolute magnitude of parameters belonging to penultimate layer for LeNet models finetuned using\ndata from context (a) C1 and (b) C2\n3\nMOTIVATION\nWhen learning patterns from human sensing data in a limited context, conventional fine-tuning approaches\ncan overwrite generic knowledge that is not relevant to that specific context but applicable to others, leading\nto a performance drop in those unrepresented contexts. To illustrate this, we conducted a preliminary study\ncomparing the performance of generic and conventionally-finetuned [31] personalized human-gesture-recognition\nmodels using the LeNet architecture [94] trained on the WIDAR dataset. Data preprocessing details are discussed\nin the Section 6 (Experiments).\nTable 2a compares the performance of generic and conventionally-finetuned[31] personalized models on each\nuser’s data belonging to context C1 and evaluated to the same user’s disjoint data in both C1 (available) and C2\n(unseen) contexts. It can be observed that conventional finetuning introduces a significant gain of 26.82% for\ncontext C1’s data but at the cost of 24.63% reduction in context C2. Similar patterns are seen when personalization\nis performed on context C2 as shown in Table 2b. Thus, conventional finetuning-based personalization of the\nmodels using the limited data from one context can significantly worsen the model’s performance in an unseen\ncontext.\nTo investigate this discrepancy in performance, we compare the distribution of parameter magnitudes of the\nmodels personalized on contexts C1 and C2 using conventional finetuning, as shown using heat maps in Figures\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n9\n2 (a) and 2 (b). Notably, there is a substantial difference in parameter magnitudes between models trained in\ndifferent contexts. Additionally, the parameters represented by black pixels in Figure 2 have magnitudes close to\nzero, indicating two crucial aspects: (a) Their contribution to model inference is negligible, implying redundancy.\n(b) Interestingly, some of these parameters have high magnitudes in the personalized model of another context,\nindicating that parameters considered unimportant in one context may be crucial in another.\nThe critical question arises: How can we effectively retain and transfer the valuable generic information about\nunseen contexts (e.g., one of the unseen contexts is C2 in the above example) to the personalized models without access\nto the unseen contexts? – that this paper addresses.\n4\nPROBLEM STATEMENT\nGiven a generic model M𝐺\n𝜃, the objective is to tailor a personalized model M\n𝑃𝑎\n𝑖\n𝜃\nspecifically for a user U𝑖utilizing\nthe data D𝑎\n𝑖associated with available context C𝑎, here 𝜃represents the parameters of the model. The primary\ngoal is to ensure that the personalized model M\n𝑃𝑎\n𝑖\n𝜃\nperforms reasonably well on U𝑖’s data D𝑢\n𝑖derived from\nunseen contexts C𝑢. Notably, there is no overlap between the data belonging to the available and unseen contexts,\nthat is, D𝑎\n𝑖∩D𝑢\n𝑖= 𝜙.\nIn other words, if M\n𝐶𝑎\n𝑖\n𝜃\nrepresents a conventionally-finetuned model trained for a user U𝑖on data D𝑎\n𝑖, then,\nthe models trained using CRoP, M\n𝑃𝑎\n𝑖\n𝜃, must on avg. perform better on both available C𝑎and unseen context C𝑢\nthan M\n𝐶𝑎\n𝑖\n𝜃. More formally, the learning objective can be defined as:\nM\n𝑃𝑎\n𝑖\n𝜃\n= argmin\n𝜃\n∑︁\n𝑑∈D𝑎\n𝑖\nℓ(M𝐺\n𝜃,𝑑),\nsuch that D𝑎\n𝑖∩D𝑢\n𝑖= 𝜙and\n∑︁\n𝑑∈{D𝑢\n𝑖,D𝑎\n𝑖}\nℓ(M\n𝑃𝑎\n𝑖\n𝜃,𝑑) <\n∑︁\n𝑑∈{D𝑢\n𝑖,D𝑎\n𝑖}\nℓ(M\n𝐶𝑎\n𝑖\n𝜃,𝑑),\nthat is, the loss incurred by the resulting personalized model M\n𝑃𝑎\n𝑖\n𝜃\non avg. across all contexts’ data is less than\nthe loss incurred by conventionally-finetuned model M\n𝐶𝑎\n𝑖\n𝜃. Here, ℓrepresents the standard cross-entropy loss.\nIt is important to emphasize that the above-mentioned optimization problem restricts the usage of data only to\nthe available context C𝑎and has no knowledge of data from the unseen context C𝑢. Hence, for 𝑑∈D𝑢\n𝑖(unseen\ncontext data), the information ℓ(M\n𝑃𝑎\n𝑖\n𝜃,𝑑), and ℓ(M\n𝐶𝑎\n𝑖\n𝜃,𝑑) is absent during the training process.\n5\nAPPROACH\n5.1\nRationale for The CRoP Approach Design\nAs previously discussed, the generic model’s parameters contain generalizable information across all contexts.\nAddressing the problem statement requires retaining this information to the greatest extent while enabling\nfine-tuning for the target user. Furthermore, our investigation revealed that different parameters hold varying\ndegrees of importance in distinct contexts. Hence, the careful selection of subsets of model parameters for\npersonalization and generalization is pivotal for the success of the approach, for which this paper leverages the\nmodel pruning paradigm.\nModel pruning is based on the idea that neural networks include redundant parameters, and removing\nthese parameters minimally impacts the model’s performance [48, 96]. Consequently, pruning the fine-tuned\npersonalized model ensures the retention of essential parameters to maintain accuracy for context C𝑎, meaning,\ncapturing the target user-specific traits. However, the pruned parameters can be replaced with corresponding\nparameters from the generic model, effectively restoring generic knowledge learned across all contexts on those\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n10\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nAlgorithm 1 CRoP\n1: Input: M𝐺\n𝜃: Generic Model ⋄D𝑎\n𝑖: User U′\n𝑖𝑠data for available context C𝑎⋄𝛼: coefficient of regularization\n⋄𝜏: tolerance for pruning\n2: Train the Generic model on the personal data D𝑎\n𝑖\nM\n𝑃𝑎\n𝑖\n𝜃′ = argmin\n𝜃\n∑︁\n𝑑∈D𝑎\n𝑖\nℓ(M𝐺\n𝜃,𝑑) + 𝛼∥M𝐺\n𝜃∥1\n3: Prune redundant parameters to obtain the pruned sub-structure\nM\n𝑃𝑎\n𝑖\n𝜃↓= ToleratedPrune(M\n𝑃𝑎\n𝑖\n𝜃′ ,𝜏)\n4: Copy the parameters of generic models to the pruned parameters in the personalized pruned model,\nM\n𝑃𝑎\n𝑖\n𝜃′′ =\n(\nM\n𝑃𝑎\n𝑖\n𝜃↓\n,𝜃↓≠0\nM𝐺\n𝜃\n, otherwise\n5: Finetune the personalized model on the D𝑎\n𝑖using early stopping with validation accuracy in C𝑎\nM\n𝑃𝑎\n𝑖\n𝜃\n= argmin\n𝜃\n∑︁\n𝑑∈D𝑎\n𝑖\nℓ(M\n𝑃𝑎\n𝑖\n𝜃′′,𝑑) + 𝛼∥M\n𝑃𝑎\n𝑖\n𝜃′′ ∥1,\ngeneric model parameters. This restoration may enhance generalizability, ensuring robust performance in unseen\ncontexts C𝑢. The approach presented in this paper is founded on this insightful strategy.\n5.2\nCRoP Approach\nAlgorithm 1 describes the presented approach, which takes as input: the generic model M𝐺, user U′\n𝑖𝑠data D𝑎\n𝑖\nfor available context C𝑎, the initial value for the coefficient of regularization 𝛼and tolerance for pruning 𝜏; and\ngenerates the target personalized model M\n𝑃𝑎\n𝑖\n𝜃. Here, 𝛼and 𝜏are hyperparameters whose values can be tuned for\nthe given data and model.\nThe approach initiates by finetuning the generic model M𝐺\n𝜃on data D𝑎\n𝑖, concurrently applying ℓ1 regularization\nto penalize model parameters (line 2). This regularization encourages sparsity by specifically targeting the\nmagnitude of redundant parameters [52]. This step is followed by the pruning of redundant weights using the\n‘ToleratedPrune’ module (line 3). The pruned weights are then replaced by the corresponding weights from the\ngeneric model M𝐺(line 4) to restore generalizability; this hybrid model is referred to as the ‘Mixed Model.’ This\nstep leads to the modification of the activated paths in the personalized model, resulting in changes in the model\ninferences. However, since the newly activated paths are determined by weights retained from two models and\nnot learned from data patterns, there is a consequent loss of accuracy, as shown and discussed in Section 7.2.\nTo mitigate such a loss, as a final step, the Mixed Model undergoes fine-tuning once again on the data from the\navailable context D𝑎\n𝑖(line 5). The detailed explanation of each of these steps is as follows:\nPersonalized Finetuning with Penalty (Algorithm 1 – Step 2): The approach uses data D𝑎\n𝑖to finetune the\ngeneric model M𝐺\n𝜃. As shown in the Section 3 (Motivation), such conventional finetuning enhances the model’s\naccuracy within the available context C𝑎. Nevertheless, its performance in unfamiliar contexts may get suboptimal.\nNotably, during the model’s fine-tuning process, we apply ℓ1 regularization to penalize the model weights, forcing\nthe magnitudes of redundant parameters to be close to zero [52]. The regularization coefficient 𝛼is a trainable\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n11\nAlgorithm 2 ToleratedPrune(M,𝜏, D)\n1: Input: M𝜃: A Model ⋄𝜏: tolerance for pruning ⋄D: data\n2: Pruning Amount 𝑝= 𝑘\n3: 𝐴𝑜= 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦(M𝜃, D)\n4: repeat\n5:\nM𝜃↓= M𝜃\n6:\nM𝜃= 𝑃𝑟𝑢𝑛𝑒(M𝜃, 𝑝)\n7:\n𝐴= 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦(M𝜃, D)\n8:\nIncrement Pruning Amount 𝑝= 𝑝+ 𝑘′\n9: until 𝐴< 𝐴𝑜−𝜏\n10: return M𝜃↓\nparameter optimized during training to minimize the overall loss. As a result, the parameters with high magnitudes\ncarry most of the information regarding the data patterns in D𝑎\n𝑖, offering two key benefits:\n(1) Minimal loss in C𝑎accuracy: A high fraction of parameters have close to zero magnitudes, and their removal\nresults in minimal information loss for context C𝑎; thus, the adverse impact of pruning in context C𝑎is\nminimized.\n(2) Maximal generalization: The inclusion of regularization aids ToleratedPrune (discussed below) module in\nefficiently pruning a higher number of parameters, which are then replaced with weights from the generic\nmodel. This restores information from the generic model, contributing to enhanced accuracy in unseen\ncontexts.\nToleratedPrune Module (Algorithm 1 – Step 3): Algorithm 2 outlines the ToleratedPrune module, taking a model\nM𝜃, pruning tolerance 𝜏, and the dataset D as inputs. It initiates with a modest pruning amount of 𝑘and\nincrementally increases this amount by 𝑘′ until the model’s accuracy exhibits a drop of 𝜏percent on D. Here,\n𝑘and 𝑘′ are hyperparameters within the range of (0, 1). The module returns M𝜃↓, representing the pruned\nstate of the model before the last pruning iteration. This state is such that further pruning would result in a\nhigher accuracy loss on dataset D than the tolerable amount 𝜏. This module performs pruning leveraging the\nconventional magnitude-based unstructured pruning [96].\nThus, step 3 in Algorithm 1 generates a pruned personalized model state M\n𝑃𝑎\n𝑖\n𝜃↓whose prediction accuracy\non context C𝑎is at most 𝜏percent lower than that of the earlier state M\n𝑃𝑎\n𝑖\n𝜃′ while using only a fraction of its\noriginal parameters. The non-zero weights corresponding to these parameters contribute significantly to model\ninference for the available context C𝑎. As a result, M\n𝑃𝑎\n𝑖\n𝜃↓is essentially the minimal sub-structure of the earlier\nstate model M\n𝑃𝑎\n𝑖\n𝜃′ , which is crucial for correct inference for context C𝑎. This enables replacing a maximal number\nof zeroed-out parameters to incorporate information from unseen contexts using the generic model M𝐺\n𝜃in the\nsubsequent steps.\nGenerating the Mixed Model (Algorithm 1 – Steps 4&5): For generating the Mixed Model M\n𝑃𝑎\n𝑖\n𝜃′′, the zeroed out\nparameters in the pruned model M\n𝑃𝑎\n𝑖\n𝜃↓are replaced by the corresponding parameters in the generic model M𝐺\n𝜃,\nenabling generic knowledge restoration.\nNotably, model pruning is often followed by a finetuning step [46, 48, 96], where the pruned model undergoes\nre-training to recover the performance lost during the pruning process. We have observed that, despite the Mixed\nModel exhibiting improved performance in the unseen context, there is a notable loss of accuracy in the available\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n12\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\ncontext due to inconsistent activated paths, as discussed earlier. Thus, the resulting Mixed Model is fine-tuned\nusing the available data D𝑎\n𝑖. Goyal et al. [25] suggests that fine-tuning process should mirror pre-training for\neffective generalization. Therefore, our fine-tuning objective aligns with the pre-training objective used in Line 2\nfor optimal results.\nDuring finetuning, the model state, including the mixed model, with the best validation loss on the seen context,\nis selected. We found that for some individuals, the mixed model is chosen as the optimal model, which indicates\nthat for some individuals, further finetuning is not required, and our approach can automatically handle that\nscenario.\n6\nEXPERIMENTS\nThis section displays the empirical efficacy of the presented approach. Section 6.1 provides a detailed discussion\nof the four human sensing datasets and their pre-processing used in our evaluations: PERCEPT-R [7], WIDAR\n[94], ExtraSensory [79] and a Stress-sensing dataset [91], and also explains the corresponding model architectures\nas used in literature. Section 6.2 details the metrics of evaluation used in this work along with their practical\nrelevance. This is followed by a detailed discussion of the empirical comparison of CRoP with five baselines\nSHOT[43], Packnet[50], Piggyback[51], CoTTa[88] and PTN[10] in Section 6.3. Additionally, detailed discussions\nabout interesting patterns, such as the impact of generic model quality on personalization, etc., are also provided.\nNotably, each dataset offers a varying distribution of data among different classes, necessitating different\nperformance metrics like inference accuracy and F1 score. For each dataset, our evaluation stayed aligned with\nthe metrics previously used in studies evaluated on these datasets, with further details available in the Appendix\nA. Additional details to support reproducibility, such as hyperparameters, links to the code, and computation\nresources utilized, are also provided in Appendix A.\n6.1\nDatasets and models\nThis work employs four real-world human-sensing datasets to demonstrate the empirical efficacy of CRoP, two of\nwhich are associated with health applications. First, the PERCEPT-R dataset has been used for binary classification\nfor predicting the correctness of /\nr\n/ sounds in automated speech therapy application [6]. Additionally, we use the\nStress Sensing dataset [91] collected using a psycho-physiological wrist-band, named Empatica E4 [17]. To further\ndemonstrate the efficacy of CRoP, we incorporate two benchmark human-sensing datasets, which include data\nfrom the same individuals across multiple contexts: WIDAR [94] and ExtraSensory [79]. Specifically, we employ\nWIDAR for a 6-class classification focusing on gesture recognition using WiFi signals, and ExtraSensory for\nbinary classification related to human activity recognition using accelerometer and gyroscope readings. Details\non the datasets, preprocessing for personalized evaluations, and generic model training are discussed below.\nPERCEPT-R:. The sound /\nr\n/ has been recognized as the most frequently impacted sound in residual speech\nsound disorders in American English [40] and considered to be the most difficult sound to treat. The PERCEPT-R\nCorpus was collected during 34 different cross-sectional and longitudinal studies of speech [7] for automated\nspeech analysis of /\nr\n/. The data used in this study come from the prospectively collected [6], and corpus version\n2.2.2, which includes both the publicly available open access subset (2.2.2p) and privately held data that was\nnot published in the open access subset after a review of consent/assent permissions. Items in the PERCEPT-R\nCorpus v2.2.2 primarily consist of single-word speech audio collected during clinical trials involving children\nwith speech sound disorders affecting /\nr\n/, along with age-matched peers with typical speech. The full corpus\ncontains 179,076 labeled utterances representing 662 single-rhotic words and phrases. Each audio file is paired\nwith a ground-truth label representing listener judgments of rhoticity, derived by averaging binary ratings (0\n= derhotic, 1 = fully rhotic) from multiple listeners. For this study, the heuristic threshold for converting these\naveraged ratings into binary ground-truth labels was 0.66.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n13\nTo use this dataset for our personalization evaluation, we collaborated with clinical experts to identify and\nacquire annotations of 16 participants who had correct and incorrect pronunciations of /\nr\n/ sound at pre-treatment\n(baseline-phase) and during different treatment phases. As outlined in Section 4, the evaluation treats pre-treatment\ndata as available context data, while data from other treatment phases serve as unseen context data.\nWIDAR:. WIDAR is a dataset collected for the purpose of gesture recognition. It was collected using off-the-shelf\nWiFi links (one transmitter and at least 3 receivers). 17 users performed 15 different gestures at different rooms\nand orientations (of the person). The channel state information is collected from these devices with amplitude\nnoises and phase offsets removed as a preprocessing step. The two contexts used for the current work are decided\nbased on the orientations of the torso data and room ID. Room 1 is a classroom with a number of objects (e.g.,\ndesks and chairs) in it, and Room 2 is a nearly empty hallway. The dissimilar data distributions can be attributed\nto the differences in the amount of interruptions in WiFi signals. We followed the same normalization methods\nas Yang et al. [92].\nExtraSensory: ExtraSensory is a human activity recognition dataset collected using the ExtraSensory mobile\napplication. A number of features were collected from different cellular devices and smartwatches, though we\njust used the accelerometer and gyroscope features obtained from the cellular devices. Labels for activities were\nself-reported by the users through the mobile application. For our evaluations on ExtraSensory, 5 users were left\nout of training a single generic model. The contexts are decided based on the location of the phone: hand, pocket,\nand bag.\nStress Sensing Dataset: This dataset measures the physiological impacts of various kinds of Stress. The dataset\nis collected using Empatica E4 Wristband to extract features such as EDA (Electrodermal Activity), a skin\ntemperature sensor (4 Hz), etc, contributing to a total of 34 features. The data was collected from 30 participants\nwith different demographics, who were assigned the labels ‘Stressed’ or ‘Calm’ based on the different stress-\ninducing or calming tasks they experienced. In order to adapt this dataset for the problem addressed in this work,\nwe collaborated with the original authors to identify participants who wore wristbands on both hands, which\nwere then chosen for the task of personalization. Additionally, during the data collection, participants were asked\nto perform several activities, a few of which restricted participants’ movement while others allowed them to\nmove. Based on this, the data was annotated with movement patterns, that is, still vs. moving. This additional\nannotation of the dataset will be made public along with this work. During this dataset’s evaluation, the context\nis defined by a combination of the hand on which the wristband was worn and whether or not the person was\nmoving during the data collection.\n6.1.1\nPre-Processing of the Datasets. We partitioned each dataset into two disjoint sets of users: (1) a generic\ndataset for training a generic model and (2) a personalized dataset for training a personalized model for each\nuser. To demonstrate the context-wise robustness, we further partitioned each user’s (belonging to the later set)\npersonalized dataset into different contexts (i.e., available C𝑎and unseen context C𝑢). Table 3 presents the details\nof this partitioning. For PRECEPT-R, we consider data from the pre-treatment phase as the available context, and\nthe treatment phases, where participants undergo clinical interventions, are considered the unseen context. For\nthe Stress Sensing dataset, the context is determined by two factors: the hand on which the sensor (Empatica E4\nwristband [17]) was worn during data collection and the movement status of the individual. For WIDAR, context\nis determined by the room and torso orientation during data collection, while for the ExtraSensory dataset,\nphone’s location on the user’s body (e.g., hand, pocket, bag) defines the context. The term ‘Scenario’ refers to the\ncombination of available C𝑎and unseen C𝑢contexts as outlined in Table 3. All datasets, along with context-wise\nannotations, will be made public.\nNotably, throughout the training of personalized models, CRoP refrains from utilizing any information from the\nunseen context C𝑢. Therefore, while the empirical study indicates an enhancement in the model’s performance for\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n14\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nDataset →\nPERCEPT-R\nWIDAR\nExtraSonsory\nStress Sensing\nTotal users\n515\n17\n60\n30\nUsers’ ID for\n17,25,28,336,344,361,362,55,\n0,1,2\n80,9D,B7,61,7C\n1,2,3\nPersonalization\n586,587,589,590,591,61,67,80\nScenario 1\nC𝑎: Baseline Study\nC𝑎: Room-1, Torso Orientation- 1,2,3\nC𝑎: Hand, Pocket\nC𝑎: Left hand, Still\nC𝑢: Treatment Phase\nC𝑢: Room 2, Torso Orientation- 4,5\nC𝑢: Bag\nC1\n𝑢: Right hand, Still; C2\n𝑢: Right hand, Moving\nScenario 2\n-N/A-\nC𝑎: Room 2, Torso Orientation- 4,5\nC𝑎: Bag, Pocket\nC𝑎: Right hand, Moving\nC𝑢: Room-1, Torso Orientation- 1,2,3\nC𝑢: Hand\nC1\n𝑢: Left hand, Moving; C2\n𝑢: Left hand, Still\nTable 3. Details of data used for personalization\none or a few unseen contexts, it is a proxy for all unseen contexts. This means that it is reasonable to anticipate a\nfavorable performance in other unseen contexts that are not available on the dataset.\nNotably, the stress sensing dataset has been evaluated across two different unseen context variations. In C1\n𝑢,\nthere was one change in context—a change in hand while keeping the same movement pattern as C𝑎. In C2\n𝑢, there\nwere two changes—a change in both hand and movement patterns.\n6.1.2\nTraining of the Generic models.\nPERCEPT-R. : For the identified 16 participants for our personalization evaluation, their speech data were\ncollected longitudinally, meaning their data could be separated into available and unseen contexts versus other\nspeakers in the corpus who only had speech data available from a one-time point. The generic model is trained\nfor the remaining 499 participants (having /\nr\n/ sounds disorder) using person-disjoint validation and test sets. The\naim of this dataset is to identify the correctness of /\nr\n/ sounds.\nModel: In line with the literature Benway et al. [7], we tried several model architectures such as CNN, DNN,\nBILSTM, etc, whose number of parameters were identified using grid search. Among those, the biLSTM model\ncontaining 4 bidirectional LSTM layers followed by 5 linear layers, accompanied by a Hardswish activation layer,\nwas identified as the one that exhibited the best results for the generic data and was used for this study.\nWIDAR:. We chose 3 users for personalization since these were the only users whose data was collected in\nboth rooms. Since the number of users in WIDAR is very small, the exclusion of all the 3 users for training the\ngeneric model would have resulted in substandard models. So, For each user, we generated different generic\nmodels by using data from the other 16 users with a 14/2 person disjoint random split for the train and validation\nset. Our classification target was the 6 gesture classes: 0,1,2,3,5 and 8, corresponding to push, sweep, clap, slide,\ndraw a circle, and draw zigzag, respectively, as evaluated in the original work [94].\nModel: The model used for WIDAR follows the LeNet architecture [94], which contains three 2D convolutional\nlayers followed by two linear layers. Each of these layers, except the final classification layer, is followed by a\nReLU activation layer.\nExtraSensory: We chose 5 users for personalization, and the generic model is trained on 42 users, with 10 users\nbeing left out for person-disjoint validation. The two target classes are walking and sitting.\nModel: The model follows a CNN-GRU-based architecture used in HAR literature [23, 26, 69, 93]. The model\nconsists of three batch-normalized 1D convolution layers followed by a linear layer that feeds into a batch-\nnormalized recursive (GRU) layer and two linear layers to generate embeddings. For the classification head, two\nlinear layers were used.\nStress Sensing Dataset: For this binary task, three users with data across all contexts were selected for personal-\nization. The generic model was trained on data from 21 users, with six additional users for disjoint validation and\ntest sets.\nModel: The model uses a simple multi-layer-perceptron (MLP) architecture [16, 18, 89] consisting of 3 linear\nlayers with hidden size of 128.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n15\n6.2\nMetrics for evaluation\nTo establish the efficacy of CRoP, we quantify the extent of personalization and generalization achieved through\nthe presented approach. Personalization is gauged by comparing our model’s M\n𝑃𝑎\n𝑖\n𝜃\naccuracy relative to the generic\nmodel M𝐺\n𝜃, while for generalization, we assess the accuracy of our model M\n𝑃𝑎\n𝑖\n𝜃\nagainst conventionally-finetuned\npersonalized models M\n𝐶𝑎\n𝑖\n𝜃.\nTaori et al. [76] argued that directly comparing model accuracies under distribution shifts is not ideal. They\nintroduced ‘effective robustness,’ a metric that assesses performance relative to an accuracy baseline. Since we\naim to compare our models against two baselines — the generic and conventionally finetuned models — we adopt\nthe ‘effective robustness metric’ and introduce two specific metrics for our comparison, detailed below. Both of\nthese metrics consider classification accuracy in the available C𝑎and unseen C𝑢contexts.\nIf A(M, D) represents the classification accuracy of the model M for dataset D and 𝑛is the number of users\nselected for personalization, the metrics of evaluations can be described as follows :\n(1) Personalization (Δ𝑃): It is defined as the sum of the difference between the accuracy of M\n𝑃𝑎\n𝑖\n𝜃\nand M𝐺\n𝜃over\nall the contexts averaged over all users\nΔ𝑃= 1\n𝑛\n∑︁\nU𝑖\n∑︁\nC∈{C𝑎,C𝑢}\n(A(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C))\n(2) Generalization (Δ𝐺): It is defined as the sum of the difference between the accuracy of M\n𝑃𝑎\n𝑖\n𝜃\nand M\n𝐶𝑎\n𝑖\n𝜃\nover\nall the contexts averaged over all users.\nΔ𝐺= 1\n𝑛\n∑︁\nU𝑖\n∑︁\nC∈{C𝑎,C𝑢}\n(A(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C))\nIn summary, the metric Δ𝑃suggests how well the model M\n𝑃𝑎\n𝑖\n𝜃\nperforms as compared to the generic model M𝐺\n𝜃.\nSince the generic model has not learned the person-specific patterns. This metric quantifies CRoP’s ability to learn\nperson-specific patterns. On the other hand, conventionally finetuned models M\n𝐶𝑎\n𝑖\n𝜃\nmay learn person-specific\npatterns and forget the generic information of different contexts. Thus, the metric Δ𝐺quantifies CRoP’s ability to\nretain generic information.\nAll the results in this section are computed as an average of accuracy obtained for three random seeds.\n6.3\nComparison with State-of-the-art\nTo demonstrate the efficacy of CRoP in achieving personalization Δ𝑃while maintaining generalization Δ𝐺, we\ncompare CRoP with 5 state-of-the-art approaches SHOT [43], PackNet [50], Piggyback [51], CoTTA [88], and\nPTN [10].\nTable 4 compares the performance of CRoP with aforementioned baseline approaches. The values for Δ𝑃and\nΔ𝐺are computed as average over all the participants used for personalization for each dataset. The detailed\nresults for participant-specific evaluations for each dataset are provided in Appendix B and Appendix B.3 shows\nthe errors bars for our approach.\nTable 4 shows that CRoP significantly outperforms all state-of-the-art (SOTA) methods. On average, the\npersonalization benefits Δ𝑃achieved by SHOT, PackNet, Piggyback and CoTTA are 2.16, 26.05, 18.01, 9.95, and\n4.13 percent points, respectively, while CRoP can achieve 35.23 percent points. However, while comparing Δ𝐺,\none can observe that personalized training using SHOT, PackNet, Piggyback and CoTTA harms generalizability\nby −25.73, −1.39, −9.43, −17.49 and −23.44 percent points respectively. On the other hand, CRoP shows an\naverage generalization benefit of 7.78.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n16\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nApproach\nSHOT\nPacknet\nPiggyback\nCoTTA\nPTN\nCRoP\nDataset\nScenrio\nΔ𝑃\nΔ𝐺\nΔ𝑃\nΔ𝐺\nΔ𝑃\nΔ𝐺\nΔ𝑃\nΔ𝐺\nΔ𝑃\nΔ𝐺\nΔ𝑃\nΔ𝐺\nPERCEPT-R\nScenario 1\n-3.11\n-5.62\n0.10\n-2.41\n-25.31\n-27.83\n-45.06\n-47.58\n-1.16\n-3.68\n5.08\n2.57\nStress Sensing\nScenario 1\n-8.19\n-62.16\n54.70\n0.70\n43.89\n-10.12\n21.93\n-32.07\n1.23\n-52.76\n67.81\n13.81\nSingle context Change\nScenario 2\n8.90\n-63.27\n75.80\n3.64\n66.22\n-5.94\n51.47\n-20.69\n21.76\n-50.40\n85.25\n13.08\nStress Sensing\nScenario 1\n-0.49\n-47.24\n52.46\n10.08\n32.40\n-9.97\n30.59\n-11.78\n6.40\n-35.98\n54.38\n12.00\nDouble context Change\nScenario 2\n3.57\n-45.49\n41.68\n-7.36\n42.76\n-6.25\n33.85\n-15.19\n12.30\n-36.74\n59.21\n10.15\nWIDAR\nScenario 1\n1.67\n-0.48\n-0.24\n-2.37\n0.84\n-1.28\n-1.05\n-3.18\n-1.99\n-4.37\n8.56\n6.43\nScenario 2\n1.28\n-0.03\n-3.55\n-5.16\n-8.97\n-10.57\n1.81\n0.21\n0.00\n-2.85\n5.90\n4.30\nExtraSensory\nScenario 1\n7.63\n-10.31\n12.19\n-5.76\n5.03\n-12.91\n-0.6\n-18.54\n1.69\n-16.72\n17.49\n-0.46\nScenario 2\n8.17\n2.99\n1.33\n-3.85\n5.22\n0.04\n-3.43\n-8.62\n-3.02\n-7.47\n13.52\n8.17\nTable 4. Comparison of CRoP with baseline approaches under the metrics of Personalization (Δ𝑃) and Generalization (Δ𝐺).\n(a) Perspective - models\n(b) Perspective - Contexts\nFig. 3. Detailed results for Stress sensing dataset User 3 under Scenario 1 using F1 score as the evaluation metric\nNotably, domain adaptation and continual learning methods, such as SHOT, Packnet, Piggyback, CoTTA, and\nPTN, leverage data from new contexts to adapt models while preserving knowledge from previous contexts or\ntasks. However, the problem addressed in this work requires the model to perform reasonably well in a completely\nunseen context, for which no data—labeled or unlabeled—is available during training. As a result, all baseline\napproaches struggle significantly in this scenario. Since the metrics in Section 6.2 evaluate performance across\nboth available and unseen contexts, the substantial performance drop in the unseen context negatively impacts\noverall results. Furthermore, the unsupervised approaches SHOT, CoTTA, and PTN achieved lower Δ𝑃and Δ𝐺\nthan supervised approaches Packnet, Piggyback, and CRoP, which is in line with literature [82].\nThe following discusses notable patterns observed in this section’s evaluations in Table 4, insights into the\ncharacteristics of static personalization across various human-sensing applications.\n6.3.1\nFor stress-sensing dataset, all the unsupervised approaches show better performance on double context change\nthan the single-context change. For Scenario 1, C𝑎contains samples of data collected using the left hand while\nstaying still. For C1\n𝑢(single context change), the hand changes to the right while staying still, but in C2\n𝑢(double\ncontext change), both hand and motion status changes. Since C2\n𝑢incurs greater shift in context than C1\n𝑢, one\ncan expect that a model trained on C𝑎will show similar or worse results for C2\n𝑢as compared to C1\n𝑢. However,\nunsupervised approaches—SHOT, CoTTA, and PTN—exhibit higher Δ𝑃and Δ𝐺values under C2\n𝑢than C1\n𝑢.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n17\nUser-specific results, however, indicate that models actually perform worse on C2\n𝑢. Figure 3(a) shows detailed\nresults in terms of F1 score for User 3 for generic model M𝐺\n𝜃, conventionally-finetuned model M\n𝑃𝑎\n𝑖\n𝜃, and models\ngenerated using unsupervised approaches: SHOT, CoTTA and PTN under Scenario 1. As expected, all models\nperform worse with the double context change C2\n𝑢(CU2 = Right-move) compared to the single context change\nC1\n𝑢(CU1 = Right-still). Despite this, the Δ𝑃and Δ𝐺values suggest otherwise, explained as follows:\nFirst, consider the Δ𝑃results, with the generic model as the accuracy baseline. Figure 3(b) shows that the\ngeneric model’s F1 score drops significantly with the double context change C2\n𝑢compared to the single context\nchange, C1\n𝑢, depicted on the blue lines. Although the performance benefits using the three approaches are almost\nsimilar in both unseen contexts, the difference in the performance as compared to the generic model is higher for\ndouble context change, thus resulting in higher Δ𝑃Similarly, for Δ𝐺, with the conventionally finetuned model\nas the baseline, the best results are seen in the available context, C𝑎(Left-still). Its performance is only slightly\naffected by the single context change but drops significantly in C2\n𝑢due to movement during data collection,\nresulting in a higher Δ𝐺for the double context change.\nSimilar patterns were observed in other users, indicating that the poor performance of M𝐺\n𝜃and M\n𝑃𝑎\n𝑖\n𝜃\nduring\ndouble context change drives the higher Δ𝑃and Δ𝐺values, giving an illusion of better results for double context\nchange scenarios.\n6.3.2\nSignificant performance drop for PERCEPT-R using SOTA approaches. For the Percept-R dataset, personal-\nization applied to the 16 participants reveals that not all participants benefit equally. In some cases, the global\nmodels already perform well, leaving little to no room for improvement through personalization. In such cases,\nperformance can even drop due to overfitting on the available context data. Our approach mitigates this by\nselecting the best model based on the highest validation accuracy from the available context, thus avoiding\noverfitting. Furthermore, CRoP introduces a trainable regularization coefficient during the initial finetuning phase,\nwhich penalizes model parameters while minimizing classification error (Algorithm 1 line 2 - trainable parameter\n𝛼). This enables the maximal removal of less important weights during the ToleratedPrune step without impacting\nperformance in the available context (Algorithm 1 line 3). Additionally, we found that finetuning after the model\nmixing step was often unnecessary, as the mixed model typically emerged as the best choice (Algorithm 1 line\n5). In contrast, the baseline approaches lack these constraints, leading to performance degradation in both the\navailable and unseen contexts, negatively affecting the overall results.\n6.3.3\nInferior generic model performance leads to higher gain in personalization. For ExtraSensory and Stress-\nsensing datasets, certain users experience sub-optimal performance with the generic model M𝐺\n𝜃and the per-\nsonalized finetuning helped not only in available context but also in the unseen contexts. For instance, the\ngeneric model showed suboptimal performance for certain users such as user ‘80’ for ExtraSensory dataset under\nScenario 1 and user ‘3’ for Stress-sensing dataset. Such users show higher benefit even in the unseen context\nwhen personalized using available context data, highlighting the importance of user-specific patterns. The details\nexplanation of these results are provided in Appendix B.\n6.3.4\nSignificant Performance gains for the stress-sensing dataset for all approaches. Psychophysiological stress\nresponse is inherently heterogeneous in inter- and intra-user scenarios [54], leading to subpar performance of\nthe generic model without personalization. As discussed in Section 6.3.3, subpar performance of generic models\nresults in higher gain during personalization. This is evident from the significantly high Δ𝑃values achieved by\nmost approaches for the stress-sensing dataset in Table 4.\nThese evaluations confirm that models personalized with CRoP exhibit higher generalizability to unseen\ncontexts, making them more intra-user robust.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n18\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n7\nEMPIRICAL JUSTIFICATION FOR CROP\nThis section empirically justifies and discusses how the use of different components of CRoP helps in incorporating\npersonalization (Δ𝑃) and generalization (Δ𝐺).\n7.1\nHow different steps of CRoP facilitate generalizability\nThis section empirically discusses how each step of CRoP (Algorithm 1) facilitates intra-user generalizability,\nsignifying similarity in the model’s behavior towards available C𝑎(available during personalization finetuning)\nand unseen contexts C𝑢.\nShi et al. [70] introduced the use of gradient inner product (GIP) to estimate the similarity between a model’s\nbehavior across different domains. If 𝐺𝑖and 𝐺𝑗represent the gradient incurred by the model for Domains 𝐷𝑖\nand 𝐷𝑗, then the sign of the product 𝐺𝑖∗𝐺𝑗represents whether the model treats two domains similarly or not.\nFor instance, 𝐺𝑖∗𝐺𝑗> 0 signifies that the gradient for both domains has the same direction. We used GIP to\nquantify generalization. A higher GIP value for a personalized model across available (C𝑎) and unseen contexts (C𝑢)\nindicates more similar behavior toward both domains, indicating higher intra-user generalizability. GIP is measured\nas: ∥Í\n𝑖𝐺𝑖∥2 −Í\n𝑖∥𝐺𝑖∥2.\n-5000\n0\n5000\n10000\n15000\n20000\nGeneric\nFinetuned \n(Line 2)\nPruned \n(Line 3)\nMixed \n(Line 4)\nFinal \n(Line 5)\nUser 0\nUser 1\nUser 2\nAverage\nFig. 4. Variation of GIP at different stages of CRoP signified\nby the lines in Algorithm 1\nFigure 4 shows that fine-tuning the generic model\n(Algorithm 1 – Step 2) on Context C𝑎, optimizes the\nmodel for this context, leading to a negative GIP, in-\ndicating a greater discrepancy between two contexts.\nSince model pruning results in generalization [34], an\nincrease in GIP value can be observed in the pruned\nmodel (Step 3).\nOn further analysis, we found that the model com-\nplementary to the pruned model (that is, the parame-\nters that were removed) also contributed towards inter-\ncontext behavior discrepancy (negative GIP value).\nHowever, the same parameters in the generic model\n(that are replaced in Step 4) formed a more general-\nizable set of weights, i.e., GIP ≥0. Thus, the model\nmixing step (Step 4) introduces further generalizability\n(GIP ≥0) in the personalized model.\nFinally, as discussed in Section 5.2 and following\nSection 7.2, the objective of the final finetuning step is to recover the performance loss, and it doesn’t aim to\nenhance generalizability further, as also shown in Figure 4.\n7.2\nJustification for the requirement of final finetuning step (Algorithm 1 Line 5) for some users\n(a) M𝑃𝑎\n𝑖\n𝜃′\n(b) Mixed Model\nFig. 5. Activation maps in C𝑎for Scenario 1\nIn Section 5.2, we discussed that the Mixed Model from Algorithm\n1 Line 4 suffers accuracy loss due to altered activation paths, as\nit combines weights from the generic and personalized stages. To\nillustrate, we compare the performance and activation maps of\nthe model states at step 2 (M𝑃𝑎\n𝑖𝜃′) and the Mixed Model (M𝑃𝑎\n𝑖𝜃′′).\nTable 5 shows that the Mixed Model M\n𝑃𝑎\n𝑖\n𝜃′′ does show an aver-\nage improvement of 14.43% in inference accuracy for the unseen\ncontext C𝑢, due to retainment of generic model weights; however,\nthere is significant loss in the available context C𝑎as compared to\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n19\nModel\nM\n𝑃𝑎\n𝑖\n𝜃′\nM\n𝑃𝑎\n𝑖\n𝜃′′\nA(M\n𝑃𝑎\n𝑖\n𝜃′′, C) −A(M\n𝑃𝑎\n𝑖\n𝜃′ , C)\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝑃𝑎\n𝑖\n𝜃′ , C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n87.06\n65.02\n70.93\n74.64\n-16.13\n+9.62\n83.67\n69.53\n-3.39\n+4.51\n1\n89.38\n44.38\n74.23\n72.70\n-15.15\n+28.32\n86.41\n54.45\n-2.97\n+10.07\n2\n71.88\n64.45\n60.09\n69.79\n-11.79\n+5.34\n77.02\n62.63\n+5.14\n-1.82\nAverage\n-14.36\n+14.43\n-0.41\n+4.25\nTable 5. Performance Comparison of model states after initial finetuning (Algorithm 1 line 2), model mixing (Algorithm 1\nline 4) and final finetuning (Algorithm 1 line 5) for WIDAR dataset under Scenario 1\nthe finetuned model M\n𝑃𝑎\n𝑖\n𝜃′ . We attribute this loss to both pruning-\ninduced information loss and inconsistent changes in activation\npaths during model mixing. Figure 5 shows the change in the activation map for a sample from context C𝑎,\nwhich was correctly classifier by M\n𝑃𝑎\n𝑖\n𝜃′ ; however, it got misclassified after the model mixing step. Significant\ndifferences in activation maps can be observed for this sample. This highlights the need for finetuning to recover\nlost information and restore consistent activation paths.\nTable 5 further shows that the final finetuning step indeed helps recover the loss in the available context C𝑎.\nAlthough this may slightly reduce accuracy in the unseen context due to overwriting some generic patterns, it\nprovides an average improvement of 4.25 percentage points in the unseen context C𝑢as compared to the model\nachieved after first finetuning (Algorithm 1 Line 2). This shows that the final finetuning is necessary for some\nusers to regain performance lost in available context C𝑎.\n8\nDETAILS OF ABLATION STUDY\nThis section presents evaluations showing the effectiveness of the design choices of CRoP, focusing on the\nWIDAR dataset in Scenario 1. Figure 6 compares the current design choices with alternative options available in\nthe literature. The comparison is done for each of the three users chosen for personalization under both available\nC𝑎and unseen C𝑢contexts. The metric used for this comparison is the inference accuracy. Similar patterns were\nobserved in other scenarios and datasets.\n8.1\nPruning mechanism\nCRoP uses one-shot magnitude-based pruning (𝑀𝑃) [48, 96] to remove the lowest-magnitude model parameters\nin the ToleratedPrune module (Algorithm 1, Line 3). Various other pruning methods exist, most relevant ones\nbeing: Gradient-Based Pruning (𝐺𝑃) [46], pruning top-magnitude weights instead of lower ones (𝑀𝑃−𝑇) [5],\nand iterative pruning (𝑀𝑃−𝐼) [56] [30]. A comparative discussion of these methods is provided below.\nMagnitude-based (𝑀𝑃) vs. Gradient-based Pruning (𝐺𝑃): Liu et al. [46] introduced gradient-based pruning\n(𝐺𝑃), which assigns importance to model parameters (i.e., kernels or nodes) based on the ℓ1 norm of gradients\n(computed from the training set data) and prunes the least important ones. In literature (e.g., [38]), the approach\nhas been shown to work on models trained and tested on the data having independent and identical distribution\n(IID). However, in this work, data from contexts C𝑎and C𝑢follow different distributions. Since, according to the\nproblem setup, only C𝑎data is accessible, using GP in CRoP leads to overfitting on C𝑎and poor performance on\nunseen contexts C𝑢. In contrast, 𝑀𝑃selects important parameters based solely on weight magnitude, making it\nmore robust and resulting in better performance on C𝑢. As shown in Figures 6(a), 𝐺𝑃performs well on C𝑎but\nconsistently underperforms compared to 𝑀𝑃on C𝑢across all three users.\nTop prune (𝑀𝑃−𝑇) vs. lower prune (𝑀𝑃): CRoP’s conventional magnitude-based pruning (𝑀𝑃) removes a\npercentage of the lowest-magnitude weights, which typically have minimal impact on model inference [48, 96].\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n20\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nUser0-Ca\nUser0-Cu\nUser1-Ca\nUser1-Cu\nUser2-Ca\nUser2-Cu\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nMP (CROP)\nGP\nMP-T\nMP-I\n(a) Pruning Mechanism\nUser0-Ca\nUser0-Cu\nUser1-Ca\nUser1-Cu\nUser2-Ca\nUser2-Cu\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nL1 (CRoP)\nL0\nL2\nPol\n(b) Regularization Mechanism\nFig. 6. Ablation study exploring different alternatives for (a) pruning and (b) regularization mechanisms using inference\naccuracy as the metric of evaluation\nHowever, Bartoldson et al. [5] suggests that pruning the top weights and retraining them (𝑀𝑃−𝑇) can improve\ngeneralization by creating flatter loss landscapes. Although 𝑀𝑃−𝑇yields models that generalize better than\ntraditional personalized models, Figure 6(a) shows that it performs significantly inferior to CRoP for all users in\nboth contexts. This is because CRoP aims to isolate a user-specific sub-network that performs well in the available\ncontext C𝑎, rather than finding a generic sub-network. Pruning top weights removes user-specific information\nlearned in step 2 of the Algorithm 1, which slightly improves performance in unseen context C𝑢but significantly\ndegrades performance in context C𝑎. Moreover, since the removed top weights are important for C𝑎, the final\nfinetuning stage (step 5 of the Algorithm 1) recaptures that information, overwriting the generic information,\nnullifying the approach’s effect.\nOne shot (𝑀𝑃) vs Iterative approach (𝑀𝑃−𝐼) The presented approach, i.e., 𝑀𝑃, uses a one-shot approach\nwhere the initially finetuned model undergoes one pass of pruning, mixing, and finetuning. However, we compared\nit with an iterative variation as well. To implement the iterative approach, 𝑀𝑃−𝐼, we allow the initially finetuned\nmodel to undergo multiple passes of pruning, mixing, and fine-tuning. As shown in Figure 6(a), 𝑀𝑃−𝐼does not\nsignificantly improve model performance, yet it incurs high computational costs through repetition.\nIn the one-shot approach, the ‘ToleratedPrune’ module (Algorithm 1 line 3 and Algorithm 2) calculates the\noptimal pruning amount by removing the maximum parameters without exceeding the accuracy loss threshold\n𝜏for context C𝑎. In the iterative approach, we gradually reach the same pruning amount by removing small\nfractions of parameters for each cycle. These parameters are replaced by weights from the generic model, but\nsince the finetuning loss function resembles the initial one, it drives those weights to lower magnitudes, causing\nthem to be pruned again in the next cycle. This repeats until the target pruning amount is reached, effectively\npruning a similar set of weights as the one-shot method but in smaller steps.\n8.2\nRegularization Mechanisms\nCRoP uses regularization to push model parameters toward zero, making pruning easier in later steps. Weight\npenalty-based regularization methods apply different norms to the model’s weights, such as ℓ0, ℓ1, ℓ2, and\npolarization. The ℓ1 norm is particularly effective because it not only reduces model parameters but also makes\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n21\nDataset\nPlatform\nTrain (s)\nRSS (MBs)\nTrain\nUsage (%)\nPERCEPT-R\nAMD Ryzen 9 5950X 16-Core Processor\n15.18\n2787.36\n24.99\nApple M1\n102.39\n352\n0.58\nNVIDIA RTX A6000\n1.94\n1132.61\n20\nAMD Ryzen 9 7950X 16-Core Processor\n14.22\n830.97\n24.9\nNVIDIA GeForce RTX 4090\n1.54\n1279.2\n36\nStress-Sensing\nAMD Ryzen 9 5950X 16-Core Processor\n6.95\n1469.59\n24.9\nApple M1\n11.71\n77\n0.85\nNVIDIA RTX A6000\n8.49\n1071.14\n7\nAMD Ryzen 9 7950X 16-Core Processor\n3.13\n633.27\n25\nNVIDIA GeForce RTX 4090\n4.31\n1249.79\n14\nTable 6. Resource requirement for one time training using CRoP\nthe least significant parameters to zero. This property also makes ℓ1 useful for feature selection [27]. Figure\n6(b) compares all these possible regularization choices. We observed that among ℓ0, ℓ1, ℓ2 and polarization [97],\nusing ℓ1 regularization is most effective in the unseen context. This is because, by forcing the least important\nparameter weights to zero, it allows maximum pruning with ‘ToleratedPrune,’ helping to recover as much generic\ninformation as possible.\n8.3\nFull finetune vs. partial finetune\nIn order to keep the zeroed-out parameters as zero, conventional pruning approaches finetune only the weights\nthat are retained during the pruning phase. However, in the presented approach, the zeroed-out weights are\nreplaced by corresponding weights from the generic model. Thus, there are no zero parameters. So, the presented\napproach finetunes all the parameters. We still evaluated both finetuning approaches and observed that there is\nno significant difference between the two approaches.\n9\nRUN-TIME ANALYSIS\nCRoP is a static personalization approach that requires one-time on-device training of the generic model during\npersonalization. We performed run-time evaluations to assess the viability of the CRoP framework across five\ndifferent deployment platforms: AMD Ryzen 9 5950X 16-Core Processor, AMD Ryzen 9 7950X 16-Core Processor,\nApple M1, NVIDIA RTX A6000 and NVIDIA GeForce RTX 4090. Table 6 shows the resources required in the\ntraining phase in terms of training time (s), process memory (MBs), and resource, i.e., GPU/CPU utilization (%) for\ntwo health-related datasets: PERCEPT-R and Stress-Sensing. According to our evaluation, except for the scalable\ndevice, Apple M1, the training time is a couple of seconds, and resource requirements are minimal. Even on\nscalable devices like Apple M1, computation time remains efficient, with only a slight increase, typically adding at\nmost a few minutes. Since this is a one-time process, CRoP is efficient compared to continuous learning methods\nthat require repeated adaptation. Particularly, this makes CRoP practical for critical applications, such as clinical\nsettings, where it incurs only a brief, one-time computation overhead during enrollment.\n10\nLIMITATIONS AND FUTURE DIRECTION\nSome limitations and future research are discussed below:\n(1) The paper performed a limited evaluation on pruning paradigms through ablation studies as it was not the\nprimary focus of the study. Section 8 on ablation study justifies CRoP’s design choice but does not establish\nany particular paradigm’s superiority in unseen contexts.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n22\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n(2) As the approach relies on using a pre-trained off-the-shelf model as an input, the quality of this model\ncan impact the performance of the final personalized models. As discussed in Section 6.3.3, users with\nsuboptimal performance of the generic model show higher gain through personalization. On the other\nhand, for certain participants in the PERCEPT-R dataset, a high-performing model leaves minimal room for\nimprovement through personalization.\n(3) We restrict our study to the models benchmarked and deployed for datasets used in this work without\naccounting for model variability. However, the different model architectures employed across all the datasets\nincorporate a range of layers such as convolutional, linear, BiLSTM, and GRU, which introduces some\nvariability.\n(4) Some evaluation results, for instance, Table 4 evaluations are shown as average. However, the metrics Δ𝑃\nand Δ𝐺are calculated individually for each person. This is because models personalized for one user cannot\nbe applied to other users in real-world situations. Consequently, we focus our evaluations on intra-user\ngeneralizability, excluding discussion for inter-user or inter-dataset generalizability.\n11\nBROADER IMPACT\nThis paper addresses a critical research gap, enhancing the practical utility of human-sensing solutions in real-\nworld applications, particularly in automated healthcare. Next-generation healthcare systems, which employ\nneural networks for tasks ranging from daily activity detection [73, 78] to safety-critical conditions like atrial\nfibrillation [11], benefit from personalization due to the heterogeneity in health sensing data [33, 67]. CRoP offers\nseveral advantages:\n(1) Eliminating the Generic Model Training: CRoP leverages off-the-shelf pre-trained models, eliminating\nthe need for training generic models. It is especially valuable in clinical settings where privacy concerns\nrestrict data sharing for training purposes [49]. This increases the feasibility of deploying personalized\nmodels in healthcare.\n(2) No privacy concerns: CRoP operates on local devices, eliminating the need for transferring potentially sen-\nsitive information to a central server. To demonstrate scalability on local devices, the resource consumption\nfor CRoP personalization on 5 devices is shown in Section 9.\n(3) Flexibility to use any model architectures: As model pruning has proven applicable to various model archi-\ntectures, CRoP is not restricted to any model architecture constraints, ensuring wide-ranging applicability.\n12\nCONCLUSION\nThis study introduces CRoP, a novel static personalization approach generating context-wise intra-user robust\nmodels from limited context data. Using pruning in a novel way to balance personalization and generalization,\nempirical analysis on four human-sensing datasets shows CRoP models exhibit an average increase of 35.23% in\npersonalization compared to generic models and 7.78% in generalization compared to conventionally-finetuned\npersonalized models. CRoP utilizes off-the-shelf models, reducing training effort and addressing privacy con-\ncerns. With practical benefits and quantitative performance enhancements, CRoP facilitates reliable real-world\ndeployment for AI-based human-sensing applications like healthcare.\nREFERENCES\n[1] Farhad Ahamed and Farnaz Farid. Applying internet of things and machine-learning for personalized healthcare: Issues and challenges.\nIn 2018 International Conference on Machine Learning and Data Engineering (iCMLDE), pages 19–21. IEEE, 2018.\n[2] Apple. Workout types on apple watch. https://support.apple.com/en-us/HT207934, 2023.\n[3] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2020. URL https://arxiv.org/abs/\n1907.02893.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n23\n[4] Ayo Awobajo. 3 tips to make google assistant your own. https://blog.google/products/assistant/how-to-personalize-google-assistant/,\n2023.\n[5] Brian R. Bartoldson, Ari S. Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network\npruning, 2020.\n[6] N. R. Benway and J. L. Preston. Artificial intelligence assisted speech therapy for /\nr\n/ using speech motor chaining and the percept\nengine: a single case experimental clinical trial with chainingai., 2023. URL https://surface.syr.edu/etd/1703.\n[7] Nina R Benway, Jonathan L Preston, Elaine Hitchcock, Yvan Rose, Asif Salekin, Wendy Liang, and Tara McAllister. Reproducible speech\nresearch with the artificial intelligence-ready PERCEPT corpora. J. Speech Lang. Hear. Res., 66(6):1986–2009, June 2023.\n[8] Visar Berisha, Chelsea Krantsevich, P Richard Hahn, Shira Hahn, Gautam Dasarathy, Pavan Turaga, and Julie Liss. Digital medicine and\nthe curse of dimensionality. NPJ Digit. Med., 4(1):153, October 2021.\n[9] Mehdi Boukhechba, Anna N Baglione, and Laura E Barnes. Leveraging mobile sensing and machine learning for personalized mental\nhealth care. Ergonomics in design, 28(4):18–23, 2020.\n[10] David Burns, Philip Boyer, Colin Arrowsmith, and Cari Whyne. Personalized activity recognition with deep triplet embeddings. Sensors,\n22(14), 2022. ISSN 1424-8220. doi: 10.3390/s22145222. URL https://www.mdpi.com/1424-8220/22/14/5222.\n[11] Jonah Comstock. Study: Apple watch paired with deep neural network detects atrial fibrillation with 97 percent accuracy, 2017.\n[12] Botos Csaba, Wenxuan Zhang, Matthias Müller, Ser-Nam Lim, Mohamed Elhoseiny, Philip Torr, and Adel Bibi. Label delay in online\ncontinual learning, 2024. URL https://arxiv.org/abs/2312.00923.\n[13] Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, and David Zhang. Efficient\nmodel adaptation for continual learning at the edge, 2023.\n[14] Antonio D’Innocente and Barbara Caputo. Domain generalization with domain-specific aggregation modules. In Thomas Brox,\nAndrés Bruhn, and Mario Fritz, editors, Pattern Recognition, pages 187–198, Cham, 2019. Springer International Publishing. ISBN\n978-3-030-12939-2.\n[15] Di Duan, Huanqi Yang, Guohao Lan, Tianxing Li, Xiaohua Jia, and Weitao Xu. Emgsense: A low-effort self-supervised domain adaptation\nframework for emg sensing. In 2023 IEEE International Conference on Pervasive Computing and Communications (PerCom), pages 160–170,\n2023. doi: 10.1109/PERCOM56429.2023.10099164.\n[16] Maciej Dzieżyc, Martin Gjoreski, Przemysław Kazienko, Stanisław Saganowski, and Matjaž Gams. Can we ditch feature engineering?\nend-to-end deep learning for affect recognition from physiological sensor data. Sensors, 20(22):6535, 2020.\n[17] empetica. Real-time physiological signals: E4 eda/gsr sensor, 2015. URL https://www.empatica.com/research/e4/.\n[18] Eda Eren and Tuğba Selcen Navruz. Stress detection with deep learning using bvp and eda signals. In 2022 International Congress on\nHuman-Computer Interaction, Optimization and Robotic Applications (HORA), pages 1–7. IEEE, 2022.\n[19] Enrico Fini, Victor G Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models\nare continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n[20] US Food and Drug Administration. Proposed regulatory framework for mondifications to artificial intelligence / machine learning-based\nsoftware as a medical device. US Food and Drug Administration: Silver Spring, MD, USA, 63, 2019. doi: 10.1016/j.apergo.2017.04.011.\n[21] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International\nConference on International Conference on Machine Learning - Volume 37, ICML’15, page 1180–1189. JMLR.org, 2015.\n[22] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor\nLempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17(1):2096–2030, January 2016. ISSN 1532-4435.\n[23] Taesik Gong, Yeonsu Kim, Jinwoo Shin, and Sung-Ju Lee. Metasense: few-shot adaptation to untrained conditions in deep mobile sensing.\nIn Proceedings of the 17th Conference on Embedded Networked Sensor Systems, SenSys ’19, page 110–123, New York, NY, USA, 2019.\nAssociation for Computing Machinery. ISBN 9781450369503. doi: 10.1145/3356250.3360020. URL https://doi.org/10.1145/3356250.3360020.\n[24] Taesik Gong, Yewon Kim, Adiba Orzikulova, Yunxin Liu, Sung Ju Hwang, Jinwoo Shin, and Sung-Ju Lee. Dapper: Label-free performance\nestimation after personalization for heterogeneous mobile sensing. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies, 7(2):1–27, 2023.\n[25] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning\nof zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\n19338–19347, June 2023.\n[26] Yujiao Hao, Rong Zheng, and Boyu Wang. Invariant feature learning for sensor-based human activity recognition. IEEE Transactions on\nMobile Computing, 21(11):4013–4024, 2022. doi: 10.1109/TMC.2021.3064252.\n[27] Amin Ul Haq, Jian Ping Li, Muhammad Hammad Memon, Jalaluddin khan, Asad Malik, Tanvir Ahmad, Amjad Ali, Shah Nazir, Ijaz Ahad,\nand Mohammad Shahid. Feature selection based on l1-norm support vector machine and effective recognition system for parkinson’s\ndisease using voice recordings. IEEE Access, 7:37718–37734, 2019. doi: 10.1109/ACCESS.2019.2906350.\n[28] Md Yousuf Harun, Jhair Gallardo, Tyler L. Hayes, and Christopher Kanan. How Efficient Are Today’s Continual Learning Algorithms?\n. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2431–2436, Los Alamitos, CA,\nUSA, June 2023. IEEE Computer Society. doi: 10.1109/CVPRW59228.2023.00241. URL https://doi.ieeecomputersociety.org/10.1109/\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n24\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nCVPRW59228.2023.00241.\n[29] Tyler L. Hayes, Nathan D. Cahill, and Christopher Kanan. Memory efficient experience replay for streaming learning, 2019. URL\nhttps://arxiv.org/abs/1809.05922.\n[30] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for\nefficient inference and training in neural networks. J. Mach. Learn. Res., 22(1), jan 2021. ISSN 1532-4435.\n[31] Jin-Hyuk Hong, Julian Ramos, and Anind K. Dey. Toward personalized activity recognition systems with a semipopulation approach.\nIEEE Transactions on Human-Machine Systems, 46(1):101–112, 2016. doi: 10.1109/THMS.2015.2489688.\n[32] Andrea Iaboni, Sofija Spasojevic, Kristine Newman, Lori Schindel Martin, Angel Wang, Bing Ye, Alex Mihailidis, and Shehroz S Khan.\nWearable multimodal sensors for the detection of behavioral and psychological symptoms of dementia using personalized machine\nlearning models. Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Monitoring, 14(1):e12305, 2022.\n[33] Wenhui Ji, Jingyu Zhu, Wanxia Wu, Nanxiang Wang, Jiqing Wang, Jiansheng Wu, Qiong Wu, Xuewen Wang, Changmin Yu, Gaofeng\nWei, et al. Wearable sweat biosensors refresh personalized health/medical diagnostics. Research, 2021.\n[34] Tian Jin, Michael Carbin, Daniel M. Roy, Jonathan Frankle, and Gintare Karolina Dziugaite. Pruning’s effect on generalization through\nthe lens of training and regularization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=OrcLKV9sKWp.\n[35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan,\nTiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming\ncatastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.\n1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114.\n[36] Wouter M. Kouw and Marco Loog. An introduction to domain adaptation and transfer learning, 2019. URL https://arxiv.org/abs/1812.\n11806.\n[37] Daniel B Kowalsky, John R Rebula, Lauro V Ojeda, Peter G Adamczyk, and Arthur D Kuo. Human walking in the real world: Interactions\nbetween terrain type, gait parameters, and energy expenditure. PLoS One, 16(1):e0228682, January 2021.\n[38] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto,\nToronto, Ontario, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.\n[39] Bishal Lamichhane, Joanne Zhou, and Akane Sano. Psychotic relapse prediction in schizophrenia patients using a personalized mobile\nsensing-based supervised deep learning model. IEEE Journal of Biomedical and Health Informatics, 2023.\n[40] Barbara A Lewis, Lisa Freebairn, Jessica Tag, Allison A Ciesla, Sudha K Iyengar, Catherine M Stein, and H Gerry Taylor. Adolescent\noutcomes of children with early speech sound disorders with and without language impairment. Am. J. Speech. Lang. Pathol., 24(2):\n150–163, May 2015.\n[41] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: meta-learning for domain generalization. In\nProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence\nConference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018.\nISBN 978-1-57735-800-8.\n[42] Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke Lu, and Heng Tao Shen. Maximum density divergence for domain adaptation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3918–3930, 2021. doi: 10.1109/TPAMI.2020.2991050.\n[43] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised\ndomain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020.\n[44] Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, and Yang Li. Enhancing continuous domain adaptation with multi-path transfer\ncurriculum, 2024.\n[45] Xin Liu, Yuntao Wang, Sinan Xie, Xiaoyu Zhang, Zixian Ma, Daniel McDuff, and Shwetak Patel. Mobilephys: Personalized mobile camera-\nbased contactless physiological sensing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 6(1), mar 2022. doi: 10.1145/3517225.\nURL https://doi.org/10.1145/3517225.\n[46] Xue Liu, Weijie Xia, and Zhimiao Fan. A deep neural network pruning method based on gradient l1-norm. In 2020 IEEE 6th International\nConference on Computer and Communications (ICCC), pages 2070–2074, 2020. doi: 10.1109/ICCC51575.2020.9345039.\n[47] Mingsheng Long, Jianmin Wang, Yue Cao, Jiaguang Sun, and Philip S. Yu. Deep learning of transferable representation for scalable\ndomain adaptation. IEEE Transactions on Knowledge and Data Engineering, 28(8):2027–2040, 2016. doi: 10.1109/TKDE.2016.2554549.\n[48] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In ICCV, pages\n5058–5066, 2017.\n[49] Bradley Malin, Kenneth Goodman, et al. Between access and privacy: challenges in sharing health data. Yearbook of medical informatics,\n27(01):055–059, 2018.\n[50] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 7765–7773, 2018. doi: 10.1109/CVPR.2018.00810.\n[51] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights.\nIn Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision – ECCV 2018, pages 72–88, Cham,\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n25\n2018. Springer International Publishing. ISBN 978-3-030-01225-0.\n[52] K. Mayank. Bxd primer series: Lasso regression models, l1 regularization in general and comparison with l2 regularization, 2023. URL\nhttps://www.linkedin.com/pulse/bxd-primer-series-lasso-regression-models-l1-general-comparison-k-/.\n[53] Lakmal Meegahapola, William Droz, Peter Kun, Amalia De Götzen, Chaitanya Nutakki, Shyam Diwakar, Salvador Ruiz Correa, Donglei\nSong, Hao Xu, and Miriam Bidoglia. Generalization and personalization of mobile sensing-based mood inference models: An analysis of\ncollege students in eight countries. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(4):1–32, 2023.\n[54] Sujay Nagaraj, Sarah Goodday, Thomas Hartvigsen, Adrien Boch, Kopal Garg, Sindhu Gowda, Luca Foschini, Marzyeh Ghassemi,\nStephen Friend, and Anna Goldenberg. Dissecting the heterogeneity of “in the wild” stress from multimodal sensor data. NPJ Digital\nMedicine, 6(1):237, 2023.\n[55] Laura Päeske, Tuuli Uudeberg, Hiie Hinrikus, Jaanus Lass, and Maie Bachmann. Correlation between electroencephalographic markers\nin the healthy brain. Sci. Rep., 13(1):6307, April 2023.\n[56] Michela Paganini and Jessica Forde. On iterative neural network pruning, reinitialization, and the similarity of masks, 2020.\n[57] Wonil Park, Victor J. Lee, Byungmo Ku, and Hirofumi Tanaka. Effect of walking speed and placement position interactions in\ndetermining the accuracy of various newer pedometers. Journal of Exercise Science & Fitness, 12(1):31–37, 2014. ISSN 1728-869X. doi:\nhttps://doi.org/10.1016/j.jesf.2014.01.003. URL https://www.sciencedirect.com/science/article/pii/S1728869X14000057.\n[58] David Phelan. Amazon admits listening to alexa conversations: Why it matters. https://shorturl.at/fxN78, 2019.\n[59] Michael Potuck. How to reset your apple watch fitness calibration for more accurate workout and activity data. https://9to5mac.com/\n2021/08/26/fix-apple-watch-workout-tracking-activity-tracking/, 2021.\n[60] A. Prabhu, H. Al Kader Hammoud, P. Dokania, P. S. Torr, S. Lim, B. Ghanem, and A. Bibi. Computationally budgeted continual learning:\nWhat does matter? In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3698–3707, Los Alamitos,\nCA, USA, jun 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.00360. URL https://doi.ieeecomputersociety.org/10.1109/\nCVPR52729.2023.00360.\n[61] Amy Rathbone, Simone Stumpf, Caroline Claisse, Elizabeth Sillence, Lynne Coventry, Richard D Brown, and Abigail C Durrant. People\nwith long-term conditions sharing personal health data via digital health technologies: A scoping review to inform design. PLOS Digit.\nHealth, 2(5):e0000264, May 2023.\n[62] Boyu Ren, Emma G Balkind, Brianna Pastro, Elana S Israel, Diego A Pizzagalli, Habiballah Rahimi-Eichi, Justin T Baker, and Christian A\nWebb. Predicting states of elevated negative affect in adolescents from smartphone sensors: A novel personalized machine learning\napproach. Psychological Medicine, pages 1–9, 2022.\n[63] Xavier Robert-Lachaine, Hakim Mecheri, Christian Larue, and Andre Plamondon. Effect of local magnetic field disturbances on inertial\nmeasurement units accuracy. Applied Ergonomics, 63:123–132, 09 2017. doi: 10.1016/j.apergo.2017.04.011.\n[64] Sadiq Sani, Stewart Massie, Nirmalie Wiratunga, and Kay Cooper. Learning deep and shallow features for human activity recognition.\nIn Gang Li, Yong Ge, Zili Zhang, Zhi Jin, and Michael Blumenstein, editors, Knowledge Science, Engineering and Management, pages\n469–482, Cham, 2017. Springer International Publishing. ISBN 978-3-319-63558-3.\n[65] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven. Introducing wesad, a multimodal dataset\nfor wearable stress and affect detection. In Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI ’18,\npage 400–408, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356923. doi: 10.1145/3242969.3242985.\nURL https://doi.org/10.1145/3242969.3242985.\n[66] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. 2015 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), Jun 2015. doi: 10.1109/cvpr.2015.7298682. URL http://dx.doi.org/10.1109/\nCVPR.2015.7298682.\n[67] Juliane R Sempionatto, Victor Ruiz-Valdepenas Montiel, Eva Vargas, Hazhir Teymourian, and Joseph Wang. Wearable and mobile\nsensors for personalized nutrition. ACS sensors, 6(5):1745–1760, 2021.\n[68] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across\ndomains via cross-gradient training. In ICLR (Poster). OpenReview.net, 2018. URL http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#\nShankarPCCJS18.\n[69] Qiang Shen, Haotian Feng, Rui Song, Stefano Teso, Fausto Giunchiglia, and Hao Xu. Federated multi-task attention for cross-individual\nhuman activity recognition. In Lud De Raedt, editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,\nIJCAI-22, pages 3423–3429. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi: 10.24963/ijcai.2022/475.\nURL https://doi.org/10.24963/ijcai.2022/475. Main Track.\n[70] Yuge Shi, Jeffrey Seely, Philip H. S. Torr, N. Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for\ndomain generalization. 2021.\n[71] Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision\n(WACV), pages 464–472. IEEE, 2017.\n[72] Gabriela M Stegmann, Shira Hahn, Julie Liss, Jeremy Shefner, Seward Rutkove, Kerisa Shelton, Cayla Jessica Duncan, and Visar Berisha.\nEarly detection and tracking of bulbar changes in ALS via frequent and remote speech analysis. NPJ Digit. Med., 3(1):132, October 2020.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n26\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n[73] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and\nMads Møller Jensen. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In\nProceedings of the 13th ACM conference on embedded networked sensor systems, pages 127–140, 2015.\n[74] C. Tang, L. Qendro, D. Spathis, F. Kawsar, C. Mascolo, and A. Mathur. Kaizen: Practical self-supervised continual learning with continual\nfine-tuning. In 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2829–2838, Los Alamitos, CA, USA,\njan 2024. IEEE Computer Society. doi: 10.1109/WACV57701.2024.00282. URL https://doi.ieeecomputersociety.org/10.1109/WACV57701.\n2024.00282.\n[75] Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, and Cecilia Mascolo. Balancing continual learning and\nfine-tuning for human activity recognition. ArXiv, abs/2401.02255, 2024. URL https://api.semanticscholar.org/CorpusID:266755926.\n[76] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural\ndistribution shifts in image classification. In Proceedings of the 34th International Conference on Neural Information Processing Systems,\nNIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n[77] Siri Team. Hey siri: An on-device dnn-powered voice trigger for apple’s personal assistant. https://machinelearning.apple.com/research/\nhey-siri, 2017.\n[78] Yunus Emre Ustev, Ozlem Durmaz Incel, and Cem Ersoy. User, device and orientation independent human activity recognition on\nmobile phones: Challenges and a proposal. In Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct\npublication, pages 1427–1436, 2013.\n[79] Yonatan Vaizman, Katherine Ellis, and Gert Lanckriet. Recognizing detailed human context in the wild from smartphones and\nsmartwatches. IEEE Pervasive Computing, 16(4):62–74, 2017. doi: 10.1109/MPRV.2017.3971131.\n[80] Gido M. van de Ven and Andreas S. Tolias. Generative replay with feedback connections as a general strategy for continual learning,\n2019. URL https://arxiv.org/abs/1809.10635.\n[81] Gido M van de Ven, Hava T Siegelmann, and Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural\nnetworks. Nat. Commun., 11(1):4069, August 2020.\n[82] C. Varma and Puja Prasad. Supervised and unsupervised machine learning approaches—a survey, 02 2023.\n[83] Tanvi Verma, Liyuan Jin, Jun Zhou, Jia Huang, Mingrui Tan, Benjamin Chen Ming Choong, Ting Fang Tan, Fei Gao, Xinxing Xu, Daniel S.\nTing, and Yong Liu. Privacy-preserving continual learning methods for medical image classification: a comparative analysis. Frontiers in\nMedicine, 10, 2023. ISSN 2296-858X. doi: 10.3389/fmed.2023.1227515. URL https://www.frontiersin.org/journals/medicine/articles/10.\n3389/fmed.2023.1227515.\n[84] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains\nvia adversarial data augmentation. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18,\npage 5339–5349, Red Hook, NY, USA, 2018. Curran Associates Inc.\n[85] Chan Wang, Tianyiyi He, Hong Zhou, Zixuan Zhang, and Chengkuo Lee. Artificial intelligence enhanced sensors - enabling technologies\nto next-generation healthcare and biomedical platform. Bioelectron. Med., 9(1):17, August 2023.\n[86] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S. Yu. Generalizing\nto unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 35(8):8052–8072, 2023.\ndoi: 10.1109/TKDE.2022.3178128.\n[87] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation, 2022.\n[88] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of Conference on Computer\nVision and Pattern Recognition, 2022.\n[89] Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: A strong baseline.\nIn 2017 International joint conference on neural networks (IJCNN), pages 1578–1585. IEEE, 2017.\n[90] Yanan Wu, Zhixiang Chi, Yang Wang, Konstantinos N. Plataniotis, and Songhe Feng. Test-time domain adaptation by learning\ndomain-aware batch normalization, 2024.\n[91] Yi Xiao, Harshit Sharma, Zhongyang Zhang, Dessa Bergen-Cico, Tauhidur Rahman, and Asif Salekin. Reading between the heat:\nCo-teaching body thermal signatures for non-intrusive stress detection. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., 7(4), jan\n2024. doi: 10.1145/3631441. URL https://doi.org/10.1145/3631441.\n[92] Jianfei Yang, Xinyan Chen, Dazhuo Wang, Han Zou, Chris Xiaoxuan Lu, Sumei Sun, and Lihua Xie. Sensefi: A library and benchmark\non deep-learning-empowered wifi human sensing, 2023.\n[93] Shuochao Yao, Shaohan Hu, Yiran Zhao, Aston Zhang, and Tarek Abdelzaher. Deepsense: A unified deep learning framework for time-\nseries mobile sensing data processing. In Proceedings of the 26th International Conference on World Wide Web, WWW ’17, page 351–360,\nRepublic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee. ISBN 9781450349130. doi:\n10.1145/3038912.3052577. URL https://doi.org/10.1145/3038912.3052577.\n[94] Y. Zhang, Y. Zheng, K. Qian, G. Zhang, Y. Liu, C. Wu, and Z. Yang. Widar3.0: Zero-effort cross-domain gesture recognition with wi-fi. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 44(11):8671–8688, nov 2022. ISSN 1939-3539. doi: 10.1109/TPAMI.2021.3105387.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n27\n[95] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Deep domain-adversarial image generation for domain generalisation.\nProceedings of the AAAI Conference on Artificial Intelligence, 34:13025–13032, 04 2020. doi: 10.1609/aaai.v34i07.7003.\n[96] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017.\n[97] Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. Neuron-level structured pruning using polarization\nregularizer. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing\nSystems, volume 33, pages 9865–9877. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/\n703957b6dd9e3a7980e040bee50ded65-Paper.pdf.\nA\nENSURING REPRODUCIBILITY\nDepending on the distribution of the data, different accuracy measures have been used in the literature such\nas balanced accuracy, standard accuracy, or F1 score. To ensure consistency with the original baseline papers\nfor each dataset [7, 91], we follow their evaluation metric. Detailed explanations and justifications are provided\nin Appendix A.1. To ensure reproducibility, we provide the hyperparameters used in both the general model\ntraining phase and the personalization phase in Appendix A.2. This includes learning rate, alpha, 𝜏, epoch count,\nand other settings specific to each dataset. Additionally Appendix A.3 and Appendix A.4 provide details of the\ncode and the compute resources, respectively.\nA.1\nMetrics for classification accuracy evaluation\nWe use accuracy to measure the performance of a model. However, the computation of this metric differs for the\nfour datasets. The details of the metrics used for all the datasets are as follows:\n(1) PERCEPT-R: For this dataset, Benway et al. [7] utilized balanced accuracy for the binary classification task,\nand we employed the same metrics in our study.\n(2) WIDAR: We use a 6-class classification for gesture recognition, and the distribution of the data among\nthese classes is nearly balanced. Thus, standard classification accuracy has been used for WIDAR.\n(3) ExtraSensory: The subset of the Extrasensory dataset used for this work aims for a binary classification for\nactivity recognition. We observed that the data distribution was quite imbalanced among the two classes,\nand therefore, balanced classification accuracy was used for this dataset. Balanced accuracy is computed as\nthe average of true positive rate and true negative rate.\n(4) Stress Sensing Dataset: For this binary classification problem, F1 score has been used as a performance\nmetric as suggested by the original authors [91].\nFor simplicity, we use the term ‘accuracy’ to encompass all the metrics discussed above.\nA.2\nHyperparameters\nThe approach uses several hyperparameters for generic model training and personalization. Table 7 and 8 show\nthe hyperparameter values for generic and personalized model training, respectively. These values correspond to\nthe best results obtained for the data belonging to the available context using a grid search. For training the generic\nmodel, in addition to the number of epochs, ‘Base Learning Rate’ and ‘Max Learning Rate’ (the arguments for\nCycleLR [71]) are the hyperparameters. For the personalized model, learning rate (fixed), 𝛼, 𝜏, number of epochs\nfor initial finetuning (Initial Epochs), and epochs for final finetuning (Final Epochs) are the hyperparameters. The\nrange of these hyperparameters used for grid search during personalization is also mentioned in Table 8.\nAdditionally, we use 𝑘= 𝑘′ = 0.05 for the ToleratedPrune module for PERCEPT-R, WIDAR, and ExtraSensory\ndatasets, while for the Stress-sensing dataset, 𝑘= 0.05 and 𝑘′ = 0.01 is being used. One may find different values\nto be suitable for other datasets and model architectures.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n28\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\nHyperparameter\nPERCEPT-R\nWIDAR\nExtraSensory\nStress-sensing\nBase Learning Rate\n1e-5\n1e-07\n1.2e-08\n5e-5\nMax Learning Rate\n1e-5\n5e-06\n7.5e-07\n5e-5\nEpochs\n300\n1000\n150\n1000\nTable 7. Hyperparameters for generic Models\nHyperparameter\nRange\nPERCEPT-R\nWIDAR\nExtraSensory\nStress Sensing\nLearning Rate\n1e-6 - 1e-1\n1e-5\n1e-6\n1e-6\n1e-5\n𝑎𝑙𝑝ℎ𝑎\n1e-6 - 10\n0.01\n0.0001\n0.5\n0.0001\n𝜏\n0.01 - 0.25\n0.05\n0.2\n0.01\n0.01\nInitial Epochs\n100 -1000\n300\n600\n600\n1000\nFinal Epochs\n100 - 1000\n300\n600\n1000\n1000\nTable 8. Hyperparameters for Personalized Models\nA.3\nCode\nThe code is provided at anonymous link arranged into dataset-specific folders. Each folder contains the pre-trained\ngeneric model, all the required modules, and the instructions to run the code. The seed values used for the\nevaluations are also provided in the shell files. The data partitioned into personalized and context-wise sets will\nbe released upon publication.\nA.4\nCompute Resources\nAll the computations have been performed on NVIDIA Quadro RTX 5000 with 48 RT Cores and 16GB GDDR6\nmemory.\nB\nDETAILED USER-SPECIFIC RESULTS\nThis section discusses the user-specific patterns. Appendix B.1 discusses detailed personalization (Δ𝑃) results\nwhile Appendix B.2 discusses generalization (Δ𝐺) results. Further, Appendix B.3 shows person-wise standard\ndeviation values for generic M𝐺\n𝜃, conventionally finetuned M\n𝐶𝑎\n𝑖\n𝜃\nand CRoP M\n𝑃𝑎\n𝑖\n𝜃\nmodels.\nB.1\nDetailed discussion of Δ𝑃results\nThe personalized models obtained using CRoP exhibit higher classification accuracy than the generic models on\nthe available context’s data D𝑎\n𝑖, showcasing the benefits of personalization. To demonstrate the existence of such\nimprovement, Tables 9a- 9h and Table 11a compare the performance of generic model M𝐺\n𝜃and personalized\nmodels obtained using CRoP M\n𝑃𝑎\n𝑖\n𝜃.\nWIDAR:. Tables 9a and 9b show that there is an average improvement of 25.25 and 11.88 percent points among\nthree users for the available context C𝑎for Scenario 1 and Scenario 2, respectively. However, this benefit comes\nat the cost of a reduction in accuracy for the unseen context. There is an average reduction of 16.69 and 5.97\npercent points for Scenario 1 and Scenario 2, respectively, for the unseen context C𝑢. Notably, the loss of accuracy\nin the unseen context is much lower as compared to the conventionally-finetune model as discussed in the Section\n3 (Motivation).\nExtraSensory: Similar patterns could be observed for the Extrasensory dataset. Tables 9c and 9d show that\nthere is an average increment of 16.40 and 18.37 percent points for the available context for Scenario 1 and\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n29\nScenario 2, respectively. Interestingly, the performance of the personalized model for Scenario 1 on unseen\ncontext C𝑢was not adversely impacted. This is attributed to the fact that the inertial sensing patterns of Bag\nand Pocket phone carrying modes capture the user’s body movement, whereas the phone-in-hand movement\npatterns can be distinct. In Scenario 1, C𝑎comprises pocket and C𝑢comprises bag, meaning both available and\nunseen contexts encompass similar inertial patterns, leading to advantageous performance even in the unseen\ncontext. This evaluation illustrates minimal intra-user generalizability loss on unseen contexts when both available\nand unseen contexts share similar user traits. However, in Scenario 2, where only the hand belongs to the unseen\ncontext C𝑢, there is an average loss of 5.02 percentage points on the unseen context.\nStress Sensing: The physiological features used in this dataset vary significantly from one user to other. Thus,\nTables 9e-9h show that the generic models do not perform well on personalized data. Personalized finetuning\nenables the model to learn person-specific patterns, allowing the model’s performance to improve not only in the\navailable context but also in the unseen context. This results in average personalization benefit (Δ𝑃) of 67.81 and\n85.25 for Scenario 1 and Scenario 2, respectively. It is important to note that for each Scenario, only one model is\ntrained for the available context and tested for two different unseen contexts. Moreover, double context change\n(Tables 9g and 9h) shows lower personalization benefit as compared to single context change (Tables 9e and 9f).\nPERCEPT-R:. In this dataset, the heterogeneity of features among individuals is reflected through the difference\nin prediction accuracy of the generic model. It can be observed in Table 11a that for some individuals, the generic\nmodel exhibits over 90% accuracy on the available context data, while for others, the generic model’s accuracy\ndrops to around 60%. This results in significant variability over gains in available and unseen contexts. Overall,\nCRoP yields an average personalization gain of 5.09%.\nOn average over all the datasets, a personalization benefit (Δ𝑃) of 35.23 percent points are seen as compared\nto the generic models across the four datasets under both scenarios.\nThese evaluations establish that the personalized models obtained using CRoP demonstrate improved perfor-\nmance over the available context data than the generic models and exhibit personalization.\nB.2\nDetailed discussion of Δ𝐺results\nThe personalized models obtained using CRoP (M\n𝑃𝑎\n𝑖\n𝜃) are expected to have higher accuracy on unseen context\nC𝑢than the conventionally-finetune personalized models (M\n𝐶𝑎\n𝑖\n𝜃) as discussed in Section 3. This section assesses\nwhether the results align with these expectations.\nWIDAR:. Tables 10a and 10b demonstrate that the personalized models M\n𝑃𝑎\n𝑖\n𝜃\nexhibit an average increment\nof 8.01 and 2.85 percent points in the unseen context for Scenario 1 and Scenario 2, respectively. However, an\naverage loss of 1.57 and an average gain of 1.44 percent points in C′\n𝑎𝑠accuracy could be observed for Scenario 1\nand Scenario 2, respectively.\nExtrasensory: Similar patterns could be observed for the ExtraSensory dataset where the average accuracy\non the unseen context improved by 4.97 and 12.61 percentage points for Scenario 1 and Scenario 2 as shown\nin Tables 9c and 10d, respectively. As expected, there is a loss of 5.43 and 4.44 percent points in the available\ncontexts for Scenario 1 and Scenario 2, respectively.\nStress Sensing: As observed in Tables 9e-9h, personalized finetuning improves models performance on unseen\ncontext as well, we can claim that there is some person-specific traits which are common in available and unseen\ncontext. While comparing our final models with conventionally-finetuned models (Tables 10e-10h), performance\nboost in both available and unseen context could be observed. This can be attributed to the generalization\nimprovement benefits of model pruning [34]. This results in average generalization benefit (Δ𝐺) of 13.81 and\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n30\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n(a) Scenario 1 for WIDAR dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n63.90\n77.09\n83.67\n69.53\n+19.77\n-7.56\n1\n61.80\n79.78\n86.41\n54.45\n+24.61\n-25.33\n2\n45.63\n79.81\n77.02\n62.63\n+31.38\n-17.18\nAverage\n+25.25\n-16.69\nΔ𝑃\n+8.55\n(b) Scenario 2 for WIDAR dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n73.28\n61.80\n82.59\n58.38\n+9.31\n-2.43\n1\n73.18\n59.58\n92.44\n47.90\n+19.27\n-11.67\n2\n80.45\n46.13\n87.5\n42.31\n+7.04\n-3.81\nAverage\n+11.88\n-5.97\nΔ𝑃\n+5.90\n(c) Scenario 1 for ExtraSensory dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n78.69\n69.83\n82.59\n69.66\n+3.9\n-0.17\n7C\n78.91\n76.41\n88.00\n71.63\n+9.09\n-4.78\n80\n55.84\n26.24\n82.36\n38.87\n+26.52\n+12.63\n9D\n73.74\n85.63\n82.81\n84.72\n+9.07\n-0.91\nB7\n56.06\n88.33\n89.50\n86.97\n+33.44\n-1.36\nAverage\n+16.40\n+1.08\nΔ𝑃\n+17.49\n(d) Scenario 2 for ExtraSensory dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n76.43\n80.00\n87.24\n73.44\n+10.81\n-6.56\n7C\n75.07\n92.32\n83.18\n89.39\n+8.11\n-2.93\n80\n54.40\n88.77\n84.49\n81.12\n+30.09\n-7.65\n9D\n75.58\n75.02\n82.58\n74.65\n+7.00\n-0.37\nB7\n59.73\n84.58\n95.56\n77.01\n+35.83\n-7.57\nAverage\n+18.37\n-5.02\nΔ𝑃\n+13.35\n(e) Scenario 1 for Stress Sensing - single context change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n88.39\n81.90\n94.54\n97.59\n+6.15\n+15.69\n2\n47.40\n50.0\n77.12\n90.47\n+29.72\n+40.47\n3\n36.90\n43.48\n96.36\n95.31\n+59.46\n+51.93\nAverage\n+31.78\n+36.03\nΔ𝑃\n+67.81\n(f) Scenario 2 for Stress Sensing - single context change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n66.54\n64.71\n92.38\n94.54\n+25.84\n+29.83\n2\n69.10\n50.70\n85.26\n89.65\n+16.16\n+38.95\n3\n4.76\n11.94\n74.40\n87.28\n+69.64\n+75.34\nAverage\n+37.21\n+48.04\nΔ𝑃\n+85.25\n(g) Scenario 1 for Stress Sensing - double context change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n88.39\n64.71\n94.54\n76.46\n+6.15\n+11.75\n2\n47.40\n50.70\n77.12\n63.22\n+29.72\n+12.52\n3\n36.90\n11.94\n96.36\n55.48\n+59.46\n+43.54\nAverage\n+31.78\n+22.60\nΔ𝑃\n+54.38\n(h) Scenario 2 for Stress Sensing - double context change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n66.54\n81.90\n92.38\n91.07\n+25.84\n+9.17\n2\n69.10\n50.00\n85.26\n62.84\n+16.16\n+12.84\n3\n4.76\n43.47\n74.40\n87.45\n+69.64\n+43.98\nAverage\n+37.21\n+22.00\nΔ𝑃\n+59.21\nTable 9. Detailed Personalization (Δ𝑃) results for WIDAR, ExtraSensory and Stress Sensing dataset\n13.08 for Scenario 1 and Scenario 2, respectively, for single context change. Similar personalization benefits could\nbe seen for double context change.\nPERCEPT-R:. As observed in Table 11b, the variability in generalization benefits among different individuals is\nless pronounced as compared to personalization benefits. On average, CRoP introduces a generalization benefit\nof 2.57%.\nOn average over all the datasets, a generalization benefit (Δ𝐺) of 7.78% percent points are seen over the\nconventionally-finetuned personalized models across all datasets under both scenarios.\nB.3\nError Bars\nTables 12a - 12i shows person-wise standard deviation values for generic M𝐺\n𝜃, conventionally finetuned M\n𝐶𝑎\n𝑖\n𝜃\nand CRoP M\n𝑃𝑎\n𝑖\n𝜃\nmodels.\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n31\n(a) Scenario 1 for WIDAR dataset\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n87.06\n65.02\n83.67\n69.53\n-3.38\n+4.5\n1\n89.38\n44.38\n86.41\n54.45\n-2.97\n+10.07\n2\n75.39\n53.19\n77.02\n62.63\n+1.63\n+9.44\nAverage\n-1.57\n+8.01\nΔ𝐺\n+6.43\n(b) Scenario 2 for WIDAR dataset\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n77.30\n57.46\n82.59\n58.38\n+5.29\n+0.92\n1\n93.75\n42.38\n92.45\n47.90\n-1.30\n+5.51\n2\n87.15\n40.19\n87.5\n42.31\n+0.34\n+2.13\nAverage\n+1.44\n+2.85\nΔ𝐺\n+4.30\n(c) Scenario 1 for ExtraSensory dataset\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n88.99\n68.09\n82.59\n69.66\n-6.40\n+1.57\n7C\n92.58\n61.74\n88.0\n71.63\n-4.58\n+9.89\n80\n86.51\n49.82\n82.36\n38.87\n-4.14\n-10.95\n9D\n88.89\n83.14\n82.81\n84.73\n-6.07\n+1.58\nB7\n95.44\n64.19\n89.50\n86.97\n-5.94\n+22.78\nAverage\n-5.43\n+4.97\nΔ𝐺\n-0.46\n(d) Scenario 2 for ExtraSensory dataset\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n93.90\n64.27\n87.24\n73.47\n-6.66\n+9.17\n7C\n89.19\n57.13\n83.13\n89.39\n-6.01\n+32.26\n80\n89.34\n70.23\n84.49\n81.12\n-4.85\n+10.89\n9D\n85.53\n72.95\n82.58\n74.65\n-2.95\n+1.7\nB7\n97.30\n67.99\n95.56\n77.01\n-1.74\n+9.02\nAverage\n-4.44\n+12.61\nΔ𝐺\n+8.17\n(e) Scenario 1 for Stress Sensing - single context change\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n91.17\n92.15\n94.54\n97.59\n+3.37\n+5.44\n2\n68.93\n82.81\n77.12\n90.48\n+8.19\n+7.67\n3\n84.78\n90.13\n96.36\n95.31\n+11.58\n+5.18\nAverage\n+7.71\n+6.10\nΔ𝐺\n+13.81\n(f) Scenario 2 for Stress Sensing - single context change\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n92.37\n96.46\n92.38\n94.54\n+0.01\n-1.93\n2\n75.57\n72.47\n85.26\n89.65\n+9.69\n+17.18\n3\n64.86\n82.52\n74.40\n87.28\n+9.54\n+4.76\nAverage\n+6.41\n+6.67\nΔ𝐺\n+13.08\n(g) Scenario 1 for Stress Sensing - double context change\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n91.17\n72.10\n94.544\n76.46\n+3.37\n+4.36\n2\n68.93\n64.13\n77.12\n63.22\n+8.19\n-0.91\n3\n84.78\n46.06\n96.36\n55.48\n+11.58\n+9.42\nAverage\n+7.71\n+4.29\nΔ𝐺\n+12.00\n(h) Scenario 2 for Stress Sensing - double context change\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n92.37\n87.22\n92.38\n91.07\n+0.01\n+3.85\n2\n75.57\n59.41\n85.26\n62.84\n+9.69\n+3.43\n3\n64.86\n83.49\n74.40\n87.45\n+9.54\n+3.75\nAverage\n+6.41\n+3.75\nΔ𝐺\n+10.16\nTable 10. Detailed Generalization (Δ𝐺) results for WIDAR ExtraSensory and Stress Sensing datasets\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\n32\n•\nSawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4\n1Syracuse University\n2University of Wisconsin-Madison\n3University of Maryland-College Park\n4Arizona State University\n(a) Personalization for PERCEPT-R\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M𝐺\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n17\n72.72\n60.51\n73.30\n63.19\n+0.58\n+2.68\n25\n96.79\n87.16\n96.30\n86.83\n-0.49\n-0.33\n28\n55.22\n54.88\n67.12\n60.17\n+11.90\n+5.29\n336\n100\n82.03\n100\n66.69\n0\n-15.34\n344\n77.54\n67.48\n81.9\n65.36\n+4.36\n-2.12\n361\n63.63\n89.93\n64.28\n81.73\n+0.64\n-8.2\n362\n59.03\n66.23\n78.52\n82.51\n+19.49\n+16.28\n55\n95.77\n85.34\n97.14\n80.41\n+1.38\n-4.93\n586\n65.85\n58.25\n73.17\n65.87\n+7.32\n+7.62\n587\n64.71\n65.1\n69.4\n65.19\n+4.68\n+0.09\n589\n66.34\n60.87\n63.69\n62.77\n-2.64\n+1.90\n590\n69.05\n61.04\n71.08\n73.26\n+2.03\n+12.22\n591\n61.91\n58.68\n72.03\n63.44\n+10.12\n+4.76\n61\n72.86\n69.42\n77.78\n66.66\n+4.92\n-2.76\n67\n80.12\n77.64\n81.00\n72.48\n+0.89\n-5.15\n80\n89.38\n85.54\n91.22\n87.87\n+1.85\n+2.33\nAverage\n+4.19\n+0.90\nΔ𝑃\n+5.09\n(b) Generalization for PERCEPT-R\nModel\nM\n𝐶𝑎\n𝑖\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nA(M\n𝑃𝑎\n𝑖\n𝜃, C) −A(M\n𝐶𝑎\n𝑖\n𝜃, C)\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n17\n70.56\n60.17\n73.30\n63.19\n+2.74\n+3.01\n25\n94.91\n83.8\n96.30\n86.83\n+1.39\n+3.03\n28\n64.81\n58.84\n67.12\n60.17\n+2.31\n+1.33\n336\n100\n60.79\n100\n66.69\n+0\n+5.90\n344\n82.77\n64.68\n81.9\n65.36\n-0.88\n+0.68\n361\n57.61\n78.66\n64.28\n81.73\n+6.67\n+3.07\n362\n75.44\n82.9\n78.52\n82.51\n+3.08\n-0.39\n55\n100\n76.86\n97.14\n80.41\n-2.86\n+3.55\n586\n70.94\n64.46\n73.17\n65.87\n+2.23\n+1.41\n587\n70.84\n66.23\n69.4\n65.19\n-1.44\n-1.04\n589\n67.26\n60.59\n63.69\n62.77\n-3.57\n+2.18\n590\n67.76\n70.54\n71.08\n73.26\n+3.32\n+2.73\n591\n71.83\n64.2\n72.03\n63.44\n+0.20\n-0.76\n61\n74.93\n65.23\n77.78\n66.66\n+2.86\n+1.43\n67\n79.83\n74.16\n81.00\n72.48\n+1.18\n-1.68\n80\n89.31\n90.33\n91.22\n87.87\n+1.92\n-1.68\nAverage\n+1.20\n+1.37\nΔ𝐺\n+2.57\nTable 11. Detailed Personalization (Δ𝑃)and Generalization (Δ𝐺) results for PERCEPT-R datasets\n, Vol. 1, No. 1, Article . Publication date: November 2024.\n\nCRoP: Context-wise Robust Static Human-Sensing Personalization\n•\n33\n(a) Scenario 1 for WIDAR dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n2.17\n0.88\n3.69\n0.49\n2.09\n0.29\n1\n1.49\n1.73\n1.94\n2.97\n0.70\n1.94\n2\n3.61\n0.68\n4.28\n5.50\n3.8\n2.31\n(b) Scenario 2 for WIDAR dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n0\n2.67\n1.60\n1.83\n0.95\n1.02\n0.41\n1\n1.58\n0.64\n2.29\n0.97\n2.22\n0.68\n2\n2.49\n0.19\n0.79\n0.56\n1.72\n0.39\n(c) Scenario 1 for ExtraSensory dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n2.63\n0\n4.14\n1.65\n3.28\n2.52\n7C\n1.45\n0\n1.17\n0.81\n1.36\n0.83\n80\n0.42\n0\n4.17\n3.04\n2.81\n1.44\n9D\n0.30\n0\n2.58\n1.08\n2.53\n0.94\nB7\n0.74\n0\n3.02\n7.28\n2.13\n3.48\n(d) Scenario 2 for ExtraSensory dataset\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n61\n3.93\n0\n4.03\n2.67\n4.72\n1.00\n7C\n2.45\n0\n2.35\n2.50\n2.85\n0.45\n80\n3.33\n0\n0.65\n3.09\n1.30\n1.01\n9D\n4.18\n0\n3.22\n0.88\n5.54\n0.80\nB7\n3.13\n0\n0.83\n3.27\n0.24\n2.79\n(e) Scenario 1 for Stress Sensing - single con-\ntext change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n3.55\n0\n5.78\n4.53\n7.94\n6.17\n2\n5.13\n0\n14.65\n3.71\n8.74\n10.49\n3\n17.61\n0\n8.17\n4.79\n15.95\n3.45\n(f) Scenario 2 for Stress Sensing - single con-\ntext change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n4.95\n0\n3.02\n0.66\n3.02\n2.34\n2\n13.68\n0\n2.67\n7.82\n11.51\n5.58\n3\n8.25\n0\n21.02\n7.00\n17.89\n6.57\n(g) Scenario 1 for Stress Sensing - double con-\ntext change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n3.55\n0\n5.75\n3.33\n7.94\n4.78\n2\n5.13\n0\n14.66\n10.19\n8.74\n8.70\n3\n17.61\n0\n8.17\n5.52\n15.95\n3.58\n(h) Scenario 2 for Stress Sensing - double con-\ntext change\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n1\n4.95\n0\n3.02\n3.00\n3.02\n2.52\n2\n13.68\n0\n2.67\n5.62\n11.51\n0.60\n3\n8.25\n0\n21.02\n2.23\n17.89\n2.07\n(i) PERCEPT-R\nModel\nM𝐺\n𝜃\nM\n𝑃𝑎\n𝑖\n𝜃\nM\n𝐶𝑎\n𝑖\n𝜃\nUser\nC𝑎\nC𝑢\nC𝑎\nC𝑢\nC𝑎\nC𝑢\n17\n7.17\n0\n9.75\n1.64\n4.18\n1.11\n25\n1.34\n0\n2.35\n2.20\n2.29\n0.85\n28\n3.38\n0\n15.16\n0.85\n15.14\n0.60\n336\n0\n0\n0\n7.25\n0\n5.64\n344\n5.50\n0\n4.86\n1.13\n4.10\n1.87\n361\n15.75\n0\n13.18\n3.96\n12.44\n6.39\n362\n3.10\n0\n7.59\n4.46\n4.53\n4.83\n55\n3.75\n0\n0\n3.32\n2.58\n2.77\n586\n4.01\n0\n1.09\n1.77\n3.26\n0.22\n587\n3.06\n0\n4.24\n1.20\n5.48\n1.43\n589\n0.45\n0\n2.33\n2.72\n1.44\n0.94\n590\n2.26\n0\n3.20\n2.17\n5.58\n0.57\n591\n3.36\n0\n3.11\n1.11\n3.91\n0.74\n61\n9.30\n0\n6.33\n2.37\n5.05\n1.67\n67\n5.50\n0\n3.59\n2.20\n3.11\n2.38\n80\n6.11\n0\n4.29\n1.61\n5.69\n0.68\nTable 12. Standard Deviation for Generic, conventionally finetunedd and CRoP models for WIDAR, ExtraSensory and Stress\nSensing dataset\n, Vol. 1, No. 1, Article . Publication date: November 2024.",
    "pdf_filename": "CRoP_Context-wise_Robust_Static_Human-Sensing_Personalization.pdf"
}