{
    "title": "CRoP: Context-wise Robust Static Human-Sensing Personalization",
    "abstract": "otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom permissions@acm.org. Â©2024AssociationforComputingMachinery. XXXX-XXXX/2024/11-ART$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn ,Vol.1,No.1,Article.Publicationdate:November2024. 4202 voN 91 ]IA.sc[ 4v49971.9042:viXra",
    "body": "CRoP: Context-wise Robust Static Human-Sensing Personalization\nSAWINDERKAUR1,AVERYGUMP2,JINGYUXIN1,YIXIAO4,HARSHITSHARMA4,NINAR\nBENWAY3,JONATHANLPRESTON1,ASIFSALEKIN4\n1SYRACUSEUNIVERSITY 2UNIVERSITYOFWISCONSIN-MADISON 3UNIVERSITYOFMARYLAND-COLLEGEPARK 4ARIZONASTATEUNIVERSITY\nTheadvancementindeeplearningandinternet-of-thingshaveledtodiversehumansensingapplications.However,distinct\npatternsinhumansensing,influencedbyvariousfactorsorcontexts,challengethegenericneuralnetworkmodelâ€™sperformance\nduetonaturaldistributionshifts.Toaddressthis,personalizationtailorsmodelstoindividualusers.Yetmostpersonalization\nstudiesoverlookintra-userheterogeneityacrosscontextsinsensorydata,limitingintra-usergeneralizability.Thislimitation\nisespeciallycriticalinclinicalapplications,wherelimiteddataavailabilityhampersbothgeneralizabilityandpersonalization.\nNotably,intra-usersensingattributesareexpectedtochangeduetoexternalfactorssuchastreatmentprogression,further\ncomplicatingthechallenges.Toaddresstheintra-usergeneralizationchallenge,thisworkintroducesCRoP,anovelstatic\npersonalizationapproach.CRoPleveragesoff-the-shelfpre-trainedmodelsasgenericstartingpointsandcapturesuser-specific\ntraitsthroughadaptivepruningonaminimalsub-networkwhilepreservinggenericknowledgeintheremainingparameters.\nCRoPdemonstratessuperiorpersonalizationeffectivenessandintra-userrobustnessacrossfourhuman-sensingdatasets,\nincludingtwofromreal-worldhealthdomains,underscoringitspracticalandsocialimpact.Additionally,tosupportCRoPâ€™s\ngeneralizationabilityanddesignchoices,weprovideempiricaljustificationthroughgradientinnerproductanalysis,ablation\nstudies,andcomparisonsagainststate-of-the-artbaselines.\nAdditionalKeyWordsandPhrases:Intra-userGeneralization,Personalization,Contest-wiseRobustness\nACMReferenceFormat:\nSawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4,\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity.2024.CRoP:Context-wiseRobust\nStaticHuman-SensingPersonalization. 1,1(November2024),33pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nRecentautomatedhumansensingapplicationsâ€”likeactivityrecognition,falldetection,andhealthtracking-\nrevolutionizedailylife,especiallyinpersonalhealthmanagement[85].However,uniqueuserpatternsandnatural\ndistributionshifts[24]causedbybehaviors,physicaltraits,environment,anddeviceplacements[73,78]lead\ntotheunderperformanceofgenericsensingmodelsinpracticaluse.Totacklethis,variousdomainadaptation\ntechniqueshavebeenexplored,withpersonalizationwidelyusedtoadaptagenericmodeltothetargetuserâ€™s\nspecificdomainornaturaldistribution[1,9,32,39,53,62,67].Inliterature,personalizationoccurseitherduring\ntheenrollmentphase(static)[10,15,45]orcontinuouslythroughoutapplicationuse,aprocessknownascontinual\nlearning[13,44,87,90].\nContinual learning methods involve retraining models with new data, either supervised or unsupervised.\nWhiletheseapproachesenablemodelstoadapttonewpatternsandchangesindatadistributionovertime,they\nAuthorâ€™saddress: SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,Asif\nSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity.\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat\ncopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirst\npage.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopy\notherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom\npermissions@acm.org.\nÂ©2024AssociationforComputingMachinery.\nXXXX-XXXX/2024/11-ART$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n,Vol.1,No.1,Article.Publicationdate:November2024.\n4202\nvoN\n91\n]IA.sc[\n4v49971.9042:viXra\n2 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\noftenfaceefficiencychallenges.AsHarunetal.[28]pointsout,thesemethods,particularlythosethatprevent\ncatastrophicforgetting[13,19,35,50,51,74,75,87,88],oftenstrugglewithmemory,computation,andstorage\nrequirementsinefficiencies,limitingtheirreal-worldapplicability.Frequentmodelupdatesonuserdevicescan\nintroducesignificantdelays.Thisisespeciallytruefordeviceswithlimitedprocessingpower,suchassmartphones\norwearables.Thesedelaysreducetheresponsivenessoftheapplication.Theyalsoleadtobatterydrainand\ninefficiencyinreal-timeapplications.Furthermore,toavoidcatastrophicforgetting,severalcontinuallearning\napproachesrelyonreplay-basedmethodswhichrequirestorageofpreviouslyencountereddata[29,80,81].\nThisnotonlyincreasesstoragerequirementsbutalsoraisesprivacyconcerns.Constantlystoringpotentially\nidentifiableinformationincreasestheriskofdatabreachesormisuse.Thisisespeciallyconcerninginregulatory\nenvironments,suchashealthcare[83].\nAdditionally, supervised continual learning approaches face the significant challenge of requiring expert-\nlabeleddataforeachnewbatchofuserinteractions,alsotermedaslabeldelay[12].Thisdemandforcontinuous,\nhigh-qualitylabelscanmakeitimpracticalformanyapplications,especiallyinsettingswhereexpertannotation\niscostlyorunavailable.Forexample,inclinicalorhealthmonitoringcontexts,eachnewdatapointmayneedto\nbevalidatedbyprofessionals,whichistime-consuminganddifficulttoscale.\nIncontrast,staticpersonalizationoffersanefficientalternativebycustomizingthemodelwithaone-time,\nlimiteddatasetcollectedduringenrollment.Thisapproachminimizescomputation,requiresnoongoingdataor\nlabelstorageorcollection,andreducesuserengagement,makingitespeciallysuitableforresource-constrained\nhuman-sensingapplications.However,existingsuchstudiesoftenoverlookintra-uservariabilityduetofactors\nlikechangesinmagneticfield[63],sensorposition[57],terrain[37],orthehealthsymptoms[55],leadingtopoor\nintra-usergeneralizabilityforcontextsnotpresentduringpersonalization.Forinstance,asmartphoneactivity\nrecognitionmodelpersonalizedwithhandhelddatamayperformpoorlywhenthephoneisinapocket.\nStaticpersonalizationisparticularlycrucialforclinicaldatasets,whichoftensufferfromdatascarcity,leading\ntoreducedrobustnessoflab-validatedmodelsforprospectivelycollectedusers[8].Itenhancesmodelaccuracy\nforclinicaluserswhosetraitsareunderrepresentedintheglobalmodelâ€™strainingdata.Incontrast,continuous\nsupervisedpersonalizationisgenerallyinfeasibleinmanyhealthdomainssincegroundtruthsmustbevalidated\nbyclinicians,makingitimpracticalincontinuoussettings,especiallyinremoteormobilehealthapplications.\nNotably,thedistributionofclinicaldataisexpectedtoshift,evenwithinthesameindividual.Forinstance,\nin clinical speech technologies, changes in data distribution over time may occur due to the progression of\nneurodegenerativediseases,relevantfordiseasemonitoringapps[72],orthroughdesiredlearningmechanisms\nresultingfromtheuseoftechnology,asseeninautomatedspeechtherapyapps[6].Similarly,inwearable-based\nstressmonitoring,psychophysiologicaldatadistributionvariesasindividualsfacedifferentstressors[54].This\nresearchdefinesâ€˜contextâ€™astheintra-userdatadistributionformedbyvaryingfactors.\nThisresearchgapisworsenedsincestaticpersonalizationtypicallyreliesonasmallsamplesetfromthetarget\nuser,coveringlimitedcontextsâ€”particularlyinclinicalsettingsorapplicationswithdatascarcity[6,8].Commer-\ncialhumansensingtechnologieslikeGoogleAssistant,AmazonAlexa,andAppleâ€™sSirialsostaticallypersonalize\nspeechrecognitionmodelsusinglimitedphrases[4,58,77].Similarly,theAppleWatchusesinitialcalibrationfor\nenhancedrunningactivitytracking[2,59].Thislimitedcontextduringpersonalizationisproblematic,asshown\ninSection3,wherewedemonstratethatstaticpersonalizationmayimproveperformanceintrainingcontexts\nbutcanalsosignificantlydegradeitinotherunseencontextsforthesameuser.\nTherefore,giventheimportanceofstaticpersonalizationinhumansensing,thispaperaddressesitsintra-user\ngeneralizabilitygap.Sincepersonalizedmodelsaretailoredtoindividualusersandnotintendedforusebyothers,\ninter-usergeneralizabilityisoutsidethescopeofthiswork.Anadditionalchallengeisthatseveralpersonalization\napproaches in the literature, such as EMGSense [15] and MobilePhys [45], train their own generic models,\nsometimesevenusingdatafromtargetusers.Thislimitsprivacy-preservinggenericmodelsharing,and,insome\ncases,requirestargetuserstosharesensitivedata,raisingadditionalprivacyconcerns.Italsocomplicatesadding\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 3\nPersonalization for a Specific User\nOff-the-Shelf Generic Model\nAvailable Unseen\nContext Contexts\nGeneralization:\nEvaluation on all\ncontexts\nPersonalization\nFig.1. ProblemSetting\nanewuser,asthegenericmodelmustbeupdatedandredistributed.Whilemodelredistributioniscommonin\nfederatedlearning,human-sensingpersonalizationisnâ€™tlimitedtofederatedlearningmethods.Toaddressthis\nlimitation,thispaperonlyusespre-trained,off-the-shelfmodelsasgenericmodels,whichdonotrequiredata\nfromthetargetusers.\nInsummary,Figure1outlinesthepaperâ€™sscopeandobjective.Thegoalistodevelopanintra-userrobust\napproachtopersonalizeanoff-the-shelfgenericmodelforaspecificuserusinglimiteddatafromlimitedcontexts.\nTheprimaryobjectiveistoensurethatthepersonalizedmodelthusobtainedexhibitsrobustgeneralization\ncapabilitiesacrossunseencontexts.Crucially,unseencontextdatatakesnopartintrainingoradjustingthe\npersonalized model outcome, and the generic model remains entirely off-the-shelf, with no accessibility for\nmodificationordesignchoices.Theseconstraintshighlightthereal-worldimpactofthisresearch,particularlyin\nclinicalsettingswhereprivacyconcernsoftenlimitdatasharing[49,61],andonlytrainedoff-the-shelfmodels\naresharedamongresearchersanddevelopers.\nToachievetheresearchobjectiveinFigure1,thispaperintroducesCRoP,anovelapproachtocreatecontext-\nwiseintra-userrobuststaticpersonalizedmodels.Thekeycontributionsare:\n(1) It facilitates utilizing readily available off-the-shelf pre-trained models with state-of-the-art accuracy,\neliminatingtheneedfortrainingcustomizedgenericmodels,thusreducingtrainingeffortandprovidinga\nstrongfoundationforpersonalization.\n(2) Personalizationwithintra-usergeneralizationhastwochallenges:i)learninguser-specificpatternsand\nii)keepinginformationaboutgenericpatternsintact.Duringpersonalization,CRoPleveragespruning\nandregularizationtocapturetheuser-specificpatternspresentintheavailable-contextdataonaminimal\nsub-networkofthemodel.Theremainingparametersofthemodelareutilizedtoretaingenericpatternsthat\narenotpresentintheavailablecontext,i.e.,personalizationdata,thusachievingintra-usergeneralizability\ntounseencontexts.Pruningiswidelyusedinotherareas,includingincontinuouslearningliterature(e.g.,\nPacknet[50],andPiggyback[51]),toobtainsub-networksforeachnewtaskwhichnotonlyrequiresdata\nfromeverytask(hereunseencontext)butalsoneedsidentificationofthetaskdifferences(i.e.,difference\namongunseencontexts)inordertoselectthesub-networkforinference.Incontrast,thisisthefirstpaper\nto use pruning with adaptive intensity to capture the userâ€™s traits in a compressed sub-network, thus\nstriking a balance between personalization and robustness in new, unseen contexts for the same user\nwithoutidentifyingthecontexts,theirdifferences,orrequiringdatafromunseencontextsforadaptation.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n4 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n(3) ToshowcaseCRoPâ€™sefficacy,comprehensiveevaluationswereperformedonfourhumansensingdatasets:\nPERCERT-R[7]:aclinicalspeechtherapydataset,WIDAR[94]:alab-basedWiFi-CSIdataset,ExtraSensory:\nareal-worldmobilesensingdataset[79],andastress-sensingdataset[91],whileconsideringtwodisjoint\ncontextsforeachdataset.InordertoobtaininformationaboutcontextvariationwithinPERCEP-Rand\nStress-sensing,wecollaboratedwiththeoriginalauthorsofthesetwodatasets.Thisadditionalannotation\nwillbereleasedalongsidethispublication.\n(4) On an average across all datasets, CRoP shows a personalization benefit of 35.23 percent points and\ngeneralizationbenefitof7.78percentpoints.Ascomparedtothebestbaseline(Packnet),thesegainsare\n9.18and9.17percentpointshigher,respectively.Moreover,alongsideadetailedablationstudydiscussion\ninSection8,anempiricaljustificationofCRoPâ€™sdesignchoicesthatenableintra-usergeneralizability\namongdifferentcontextsisprovidedinSection7.1,employingGradientInnerProduct(GIP)[70]analysis.\nAdditionally,inordertodemonstratethefeasibilityofon-devicepersonalizationthroughCRoP,wecompute\ntrainingtimeandresourcerequirementsforfiveplatformsordevices.\nThepaperisarrangedindifferentsections.Section2reviewstherelatedwork,whileSection3presentsthe\nresultsofapreliminarystudyontheWIDARdatasets,whichmotivatestheproposedapproach.Section4provides\naformaldescriptionoftheproblemstatement,followedbySection5,whichoutlinesthedetailedmethodology.\nSection6coverstheexperimentalsetup,andresults.InSection7,wepresentanempiricalstudytojustifythe\napproach,andSection8offersanablationstudytosupportvariousdesignchoices.Section9discussesruntime\nanalysis,Section10exploreslimitationsandfuturedirections,andSection11addressesthebroaderimpactofthe\nwork.Finally,Section12concludesthestudy.\nTheworkisaccompaniedbyanextensiveappendix,whichprovidesdetailsofexperimentsetup,hyperparam-\neters,andlinktocodeensuringreproducibilityinAppendixA.Additionally,AppendixBprovidesadetailed\nanalysisoftheuser-specificresultsforeachdatasetalongwithadetailedablationstudy.\n2 RELATEDWORK\nAsdiscussedabove,thisworkaimstopersonalizeoff-the-shelfmodelswhileensuringintra-usergeneralizability\nleveraginglimited-contextdata.Whileexistingapproachesattempttoaddresstheseadaptationchallenges,they\noftenimposerestrictiveconditions.Manyrequirespecializedtrainingofthegenericmodelwithaccesstothe\ngeneric data, assume knowledge of the target domain, or necessitate target users to share their data from a\nnew/previous-unseencontextforadaptingthemodel,insomestudieswithlabelsorannotations.Additionally,\nsomemethodsrelyonrepetitivemodelretraining,whichhinderstheirsuitabilityforreal-worldapplicationsdue\ntoprivacyconcerns,computationalinefficiency,orexcessiveuserengagement.\nThissectionsummarizesandcritiqueskeystate-of-the-artapproacheswithsimilarobjectivestothoseaddressed\ninthiswork,highlightingtheircontributionsandlimitationsinpractical,user-centeredadaptationsettings.\n2.1 DomainAdaptationandGeneralization\nDomainadaptation(DA)andDomainGeneralization(DG)techniquesaddressperformancedropsduetodistribu-\ntionalshiftsbetweensourceandtargetdomains[36].Thesemethodsareusefulwhenmodelstrainedongeneric\ndataneedtoadapttonewusersorcontexts.ThekeydifferenceisthatwhileDAapproacheshaveaccesstotarget\ndomaindata,DGtechniquesworkwithoutanytargetdomaindata,notevenunlabeled[86].\nThedomaingeneralizationtechniquescanbecategorizedintoDataManipulation,RepresentationLearning,\nandotherlearningstrategies[86].DataManipulationapproachesincludetechniqueslikedataaugmentation\nandsyntheticdatageneration,whichareusedtoincreasedatadiversity,helpingmodelslearninvariantfeatures\nacrossdifferentdomains[68,84,95].RepresentationLearningaimstocapturedomain-invariantfeaturesusing\ntechniqueslikeadversarialtrainingandembeddinglearningtoimprovemodelrobustness,suchasInvariant\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 5\nriskminimization(IRM)[3]anddomain-adversarialneuralnetworks(DANN)[22].LearningStrategiessuchas\nmeta-learning[41]andensemblelearning[14]trainthemodelonmultiplesimulateddomainstoimproveits\nabilitytogeneralize.However,alloftheseapproachesaimtotrainthegenericmodelwhilethispaperâ€™sscope\ndoesnotallowaccesstogenericdataandusesapre-trainedoff-the-shelfmodelforadaptation.\nSimilarly,somedomainadaptationapproachesrelyongenericmodeltrainingwhileutilizingdatafromthe\ntargetdomain[47].Ontheotherhand,certaindomainadaptationapproachesrequireaccesstothesourcedata\nusedtotrainthegenericmodelwithoutrequiringspecializedtrainingofthegenericmodel.Theseapproaches\nrely on discrepancy minimization and self-supervision to align source and target distributions by learning\ndomain-invariantrepresentations[21,42].Afewapproachesthatdonotrequireaccesstothesourcedomain\ndataorgenericmodeltrainingarereferredtoassource-freedomainadaptation(SFDA)[43].Liangetal.[43]\n(SHOT)proposedthetransferofhypothesisfromsourcebyfreezingtheparameterweightsfortheclassifierlayers\nandonlyallowingthefeatureextractionlayertobefinetunedtothenewdomain.Theapproachisapplicable\ntounsuperviseddomainadaptationscenariosandemploysself-supervisedpseudo-labelingtoalignthetarget\ndomainâ€™srepresentationstothesourcehypothesis.\nGiventhattheproblemaddressedbySHOTalignswiththecriteriaofhavingnoaccesstogenericdataand\navoidingspecializedgenericmodeltraining,weadoptedSHOTasoneofthebaselineapproachesinthisstudy.\nHowever,itisimportanttonotethatSHOTdoesnoteffectivelytacklethelimitationsposedbyrestricted-context\ndataduringthefine-tuningprocess.Asaresult,itsperformancemaynotadequatelygeneralizetotheintra-user\nvariabilitypresentinthedata,whichisacrucialaspectofourresearchfocus.\n2.2 ContinualLearning\nContinualpersonalizationapproaches[13,19,50,51,74,75,87,88]canimproveintra-usergeneralizabilityby\ncontinuallyfine-tuningthemodelasnewdataarrives.Someoftheseapproaches[19,74,75]requirespecialized\ntraining of the generic model, limiting the use of off-the-shelf pre-trained models. Others like PackNet [50]\nandPiggyback[51]proposesupervisedmethodsthatrequirecontinuedsteamoflabeleddata,limitingtheir\napplicationinhealth-carescenarios.However,allcontinuouslearningapproachesrequirerepeatedcomputation\noverheadtoadjustthemodeloutcometonewdata[60],whichcanbeinfeasibleinreal-timeapplications,moreso\nforscalableplatformslikewearables[65],whichisprominentforhealthsensingsuchasstressorfalldetection.\nDanielsetal.[13]proposesacontinualtrainingframeworkthatiscompatiblewithedgedevicesandrequiresless\ntrainingeffort,butitusesadifferentandsmallermodelarchitectureatthepersonalizeddevicesforgenerating\nfeatureembeddings.Thushinderingthedirectusageofoff-the-shelfmodelsandincreasingthetrainingeffort.\nNevertheless,sincetheproblemsaddressedbyPacknet,andPiggybackaretheclosesttotheproblemaddressed\ninthisstudy,weconsideredtheseapproachesasbaselines.\nPacknet[50]andPiggyback[51]adaptthegenericmodelforastreamofcontinuouslychangingtasks.Packnet\nreliesonfinetuning,pruningandre-trainingthemodelforeachnewtask.Ontheotherhand,Piggybackemploys\npruningtolearnanewbinarymaskforeachnewtaskwhichisthenappliedtoagenericmodelinordertoget\ntask-specificresults.However,thisrequirescorrectidentificationofthetaskbeforechoosingthemask.Forevery\nnewtask,thesameprocedureisrepeated,hopingthatdifferentsetsofparameterswillbeimportantfordifferent\ntasks.Thisalignswellwithourinitialstudy,whereweidentifythatdifferentcontextsâ€™datafocusondifferent\nsetsofparameters.\nNotably,allcontinuallearningapproaches,includingPackNetandPiggyback,faceasignificantlimitation:they\nnecessitateaccesstodatafromthetargetdomain,whetherinasupervisedorunsupervisedformat,inorderto\neffectivelyadaptthemodel.Whiletheseapproachessuccessfullyaddressthechallengeofcatastrophicforgetting,\ntheyprimarilyfocusoncontextsthatthemodelhaspreviouslyencountered,leavingthemill-equippedtoadapt\ntoentirelyunseendomains.Incontrast,ourresearchframeworkspecificallyexcludesaccesstodatafromthese\n,Vol.1,No.1,Article.Publicationdate:November2024.\n6 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nunseencontextsduringmodeladaptation,highlightingacriticalgapinthecapabilitiesofexistingcontinual\nlearningmethodologies.Consequently,PackNet,Piggyback,andsimilarapproacheslackthemechanismsneces-\nsarytoadapttonoveldomains,underscoringtheneedforinnovativestrategiesthatcanoperatewithoutprior\nexposuretothetargetdata.\n2.3 Test-TimeAdaptation(TTA)\nContinual learning requires repeated training cycles to adapt models to new information, demanding high\ncomputationalresources,storage,andlong-termdataretention,whichraisesprivacyconcerns[83].Thismakes\ntraditionalcontinuallearningimpracticalforsensitivedomainslikehealthcare[28].\nTest-timeadaptationoffersamoreefficientandfelxiblealternative,allowingmodelstoadaptinreal-time\nduringdeploymentbymakingadjustmentsbasedonincomingdatawithoutnecessitatingacompleteretraining\nprocess.Byfocusingonadaptingthemodelinreal-time,test-timeadaptationminimizesresourceconsumptionas\ncomparedtocontinuallearningapproaches.Wangetal.[87]reliesongeneratingpseudo-labelsusingweighted\naveragingandaugmentation-averagedpredictions,whichcanintroducebias[43].WhileWuetal.[90]achieves\ntest-timedomainadaptationbymanipulatingbatch-normalizationlayers,thusenforcingarestrictiononmodel\narchitecture.However,ContinualTestTimeDomainAdaptation(CoTTA)[88]doesnothavesuchrestrictions\nandallowstheuseofoff-the-shelfmodels.\nCoTTA[88]isanunsupervisedlearningapproachdesignedtoenhancemodeladaptationindynamicenvi-\nronments. It utilizes a teacher model, which is initially set up as a replica of the generic model, to generate\npseudo-labelsfortraining.Duringeachiteration,theteachermodelisupdatedusingcurrentmodelstatethrough\naweightedsum,allowingthemodeltoeffectivelyaccountforevolvingdatapatternsovertime.Additionally,the\ncurrentmodelstateissubjecttoastochasticrestorationofweights,implementedthroughaBernoullidistribution\nwithaverylowprobabilityofsuccess.Thisrandomizationintroducesanelementofvariabilitythathelpsprevent\noverfittingandencouragesexplorationoftheweightspace,furtherenhancingthemodelâ€™sabilitytogeneralizeto\nunseendata.\nSinceCoTTAaddressesaproblemsettingsimilartoours,weadoptitasoneofourbaselineapproaches.In\noursetup,datafromtheavailablecontextcanbeleveragedfortest-timeadaptation,whileunseencontextdata\nisreservedsolelyforempiricalevaluation.However,test-timeadaptationmethods,includingCoTTA,tendto\nincreaseresourcedemandsduringinference,leadingtopotentialdelaysthatcanimpactreal-timeusability.This\nadditionalcomputationaloverheadhighlightsakeylimitationoftest-timeadaptationinapplicationswhere\nreal-timeresponseisessential.\n2.4 PersonalizationApproaches\nAfewstaticpersonalizationapproaches[10,15,45]aimedfortheadditionalgoalofout-of-distributionrobustness.\nHowever,thesemethodsrequireaccesstothegenericmodelâ€”eithertomakespecificdesignchoices[10],which\nprevents them from utilizing off-the-shelf models, or to incorporate knowledge about the target userâ€™s data\ndistributionduringthegenericmodelâ€™strainingphase[15,45],raisingprivacyconcerns,particularlyinsensitive\nclinicalapplications.Theserequirementsdonotalignwiththeresearchobjectivesofthispaper,makingthem\nunsuitableasbaselines.\nForinstance,EMGSense[15]isconcernedwithrobustnesstodistributionalshiftsinSurfaceElectromyography\n(EMG)signalscausedbytheheterogeneityofbiologicalfactorsacrossusers.Thegenericmodelconsistsofa\nmulti-modelvotingensembleDNNthatneedstobetrainedonNsourcedistributionsandsomeunlabeleddata\nfromtargetusers.Tolearnuser-specificfeatures,thismodelundergoesmultipletrainingiterations,eachusing\nupdateduser-specificdata.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 7\nFeatureâ†’ Off-the-shelf GenericData Adaptationto Robustnessto\nApproachâ†“ Genericmodel unseen availablecontext unseenContext\nArjovskyetal.[3],Ganinetal.[22] âœ— âœ— -NA- âœ“\nLongetal.[47] âœ— âœ— âœ“ âœ—\nGaninandLempitsky[21],Lietal.[42] âœ“ âœ— âœ“ âœ—\nSHOT[43] âœ“ âœ“ âœ“ âœ—\nPacknet[50]andPiggyback[51] âœ“ âœ“ âœ“ âœ—\nCoTTA[88] âœ“ âœ“ âœ“ âœ—\nEMGSense[15]andMobilePhys[45] âœ— âœ— âœ“ âœ—\nPTN[10] âœ“ âœ“ âœ“ âœ—\nCRoP âœ“ âœ“ âœ“ âœ“\nTable1. SummaryofLiteraturereview\nAlso, MobilePhys [45] addresses distributional shifts that can arise due to environmental conditions (e.g.,\nlighting,motion)aswellasindividualphysiologicalandvisual(e.g.,clothing,posture)differencesincamera-based\ncontactlessPhotoplethysmography.Thegenericmodelistrainedusingmeta-learning,whichusesdatafrom\ntargetuserstoenablequickadaptationtodistributionalshiftsduringpersonalization.\nConsequently,themajordrawbackfacedbyEMGSense[15]andMobilePhys[45]istherequirementofknowl-\nedgeofthetargetuserâ€™sdatainordertotrainthegenericmodel.Thisnecessitatesthetransferoftheuserâ€™s\nunlabelleddatafromtheuserâ€™sdevicetoacentralserver.Suchdatacancontainsensitiveinformation,andthus,\nitstransfersuffersfromlegalandregulatorybarriers[20].Moreimportantly,sincethetrainingofthegeneric\nmodel requires a subset of each target userâ€™s unlabelled data, the introduction of any new user will require\nre-trainingofthegenericmodelanditsdistribution,whichisanoverhead.\nRecently, Burns et al. [10] extends the idea of triplet loss [66] to the task of personalization (PTN). The\noptimizationobjectivecombinestheminimizationoftheEuclideandistancebetweendatafromthesametarget\nclasseswhilemaximizingtheEuclideandistancebetweendifferenttargetclassesinordertolearnanembedding\noptimizedforthedesiredtask.ThefeaturesextractedusingDNNhavebeenshowntobesuperiortotheengineered\nfeatures[64].Thus,Burnsetal.[10]usesthesefeaturestotrainaKNNforthefinalpredictiontask.Theapproach\nhasbeenshowntoberobustforout-of-distributiondata.\nHowever, none of these approaches address intra-user variability in data. Since PTN allows the usage of\noff-the-shelfmodelanddoesnotrequiresharingofthepersonaldataforgenericmodeltraining,weconsider\nPTNasoneofthebaselines.\n2.5 SelectingBaselines\nTheproblemaddressedinthisworkhasthefollowingrequirements:usinganOff-the-shelfgenericmodel,having\nnoaccesstothegenericdata,andrequiringadaptationtoavailablecontextdataandrobustnesstounseendata.\nAsdiscussedintheprevioussectionsandsummarizedinTable1,theapproachesSHOT[43],PTN[10],PackNet\n[50],Piggyback[51],andContinualTestTimeDomainAdaptation(CoTTA)[88]complywiththefirstthree\nrequirements.Thus,weadapttheseapproachestotheproblemstatementaddressedinthisworkandusethemas\nourbaselines.Theseapproachestypicallyrelyontrainingdatafromaspecificcontexttoadapttothatcontext.\nHowever,thescenarioaddressedinthisworklimitstrainingdatatoonlyonecontext,withtheexpectation\nthattheapproachwillalsoperformwellinunseencontexts.Sincedatafromunseencontextsisunavailable,\nwepreventtheseapproachesfromusingitduringtraining.Instead,thedatausedfortrainingoradaptationis\nrestrictedtodatabelongingtotheavailablecontextonly,whiletheunseencontextdataisusedmerelyfortesting.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n8 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n(a)ContextC1 (b)ContextC2\nModel Generic Personalized Î” Model Generic Personalized Î”\nUser C1 C2 C1 C2 C1 C2 User C1 C2 C1 C2 C1 C2\n0 63.90 77.09 87.06 65.02 +23.16 -11.88 0 60.80 73.28 57.46 77.30 -3.34 +4.02\n1 61.80 79.78 89.38 44.38 +27.57 -35.40 1 59.58 73.18 42.38 93.75 -17.29 +20.57\n2 45.63 79.81 71.88 64.45 +29.75 -26.62 2 46.13 80.46 40.19 87.15 -5.93 +6.69\nAverage +26.82 -24.63 Average -8.85 +10.43\nTable2. PerformanceComparisonintermsofinferenceaccuracyofGenericmodelwithconventionallytrainedpersonalized\nmodel\n(a)ContextC1 (b)ContextC2\nFig.2. HeatmapfortheabsolutemagnitudeofparametersbelongingtopenultimatelayerforLeNetmodelsfinetunedusing\ndatafromcontext(a)C1and(b)C2\n3 MOTIVATION\nWhenlearningpatternsfromhumansensingdatainalimitedcontext,conventionalfine-tuningapproaches\ncanoverwritegenericknowledgethatisnotrelevanttothatspecificcontextbutapplicabletoothers,leading\ntoaperformancedropinthoseunrepresentedcontexts.Toillustratethis,weconductedapreliminarystudy\ncomparingtheperformanceofgenericandconventionally-finetuned[31]personalizedhuman-gesture-recognition\nmodelsusingtheLeNetarchitecture[94]trainedontheWIDARdataset.Datapreprocessingdetailsarediscussed\nintheSection6(Experiments).\nTable2acomparestheperformanceofgenericandconventionally-finetuned[31]personalizedmodelsoneach\nuserâ€™sdatabelongingtocontextC1andevaluatedtothesameuserâ€™sdisjointdatainbothC1(available)andC2\n(unseen)contexts.Itcanbeobservedthatconventionalfinetuningintroducesasignificantgainof26.82%for\ncontextC1â€™sdatabutatthecostof24.63%reductionincontextC2.Similarpatternsareseenwhenpersonalization\nisperformedoncontextC2asshowninTable2b.Thus,conventionalfinetuning-basedpersonalizationofthe\nmodelsusingthelimiteddatafromonecontextcansignificantlyworsenthemodelâ€™sperformanceinanunseen\ncontext.\nToinvestigatethisdiscrepancyinperformance,wecomparethedistributionofparametermagnitudesofthe\nmodelspersonalizedoncontextsC1andC2usingconventionalfinetuning,asshownusingheatmapsinFigures\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 9\n2(a)and2(b).Notably,thereisasubstantialdifferenceinparametermagnitudesbetweenmodelstrainedin\ndifferentcontexts.Additionally,theparametersrepresentedbyblackpixelsinFigure2havemagnitudescloseto\nzero,indicatingtwocrucialaspects:(a)Theircontributiontomodelinferenceisnegligible,implyingredundancy.\n(b)Interestingly,someoftheseparametershavehighmagnitudesinthepersonalizedmodelofanothercontext,\nindicatingthatparametersconsideredunimportantinonecontextmaybecrucialinanother.\nThecriticalquestionarises:Howcanweeffectivelyretainandtransferthevaluablegenericinformationabout\nunseencontexts(e.g.,oneoftheunseencontextsisC2intheaboveexample)tothepersonalizedmodelswithoutaccess\ntotheunseencontexts? â€“thatthispaperaddresses.\n4 PROBLEMSTATEMENT\nGivenagenericmodelM ğœƒğº ,theobjectiveistotailorapersonalizedmodelM ğœƒğ‘ƒ ğ‘–ğ‘ specificallyforauserUğ‘– utilizing\nthedataD ğ‘–ğ‘ associatedwithavailablecontextCğ‘,hereğœƒ representstheparametersofthemodel.Theprimary\ngoalistoensurethatthepersonalizedmodelM ğœƒğ‘ƒ ğ‘–ğ‘ performsreasonablywellonUğ‘–â€™sdata D ğ‘–ğ‘¢ derivedfrom\nunseencontextsCğ‘¢.Notably,thereisnooverlapbetweenthedatabelongingtotheavailableandunseencontexts,\nthatis,Dğ‘âˆ©Dğ‘¢ =ğœ™.\nğ‘– ğ‘–\nInotherwords,ifM ğœƒğ¶ ğ‘–ğ‘ representsaconventionally-finetuned modeltrainedforauserUğ‘– ondataD ğ‘–ğ‘ ,then,\nğ‘ƒğ‘\nthemodelstrainedusingCRoP,M ğœƒğ‘– ,mustonavg.performbetteronbothavailableCğ‘ andunseencontextCğ‘¢\nğ¶ğ‘\nthanM ğ‘–.Moreformally,thelearningobjectivecanbedefinedas:\nğœƒ\nMğ‘ƒ ğ‘–ğ‘\n=argmin\nâˆ‘ï¸ â„“(Mğº,ğ‘‘),\nğœƒ ğœƒ\nğœƒ ğ‘‘âˆˆDğ‘\nğ‘–\nsuchthatDğ‘âˆ©Dğ‘¢ =ğœ™ and\nğ‘– ğ‘–\nâˆ‘ï¸ ğ‘ƒğ‘ âˆ‘ï¸ ğ¶ğ‘\nâ„“(M ğ‘–,ğ‘‘) < â„“(M ğ‘–,ğ‘‘),\nğœƒ ğœƒ\nğ‘‘âˆˆ{Dğ‘¢,Dğ‘} ğ‘‘âˆˆ{Dğ‘¢,Dğ‘}\nğ‘– ğ‘– ğ‘– ğ‘–\nğ‘ƒğ‘\nthatis,thelossincurredbytheresultingpersonalizedmodelM ğ‘– onavg.acrossallcontextsâ€™dataislessthan\nğœƒ\nğ¶ğ‘\nthelossincurredbyconventionally-finetuned modelM ğ‘–.Here,â„“ representsthestandardcross-entropyloss.\nğœƒ\nItisimportanttoemphasizethattheabove-mentionedoptimizationproblemrestrictstheusageofdataonlyto\ntheavailablecontextCğ‘ andhasnoknowledgeofdatafromtheunseencontextCğ‘¢.Hence,forğ‘‘ âˆˆ D ğ‘–ğ‘¢ (unseen\nğ‘ƒğ‘ ğ¶ğ‘\ncontextdata),theinformationâ„“(M ğ‘–,ğ‘‘),andâ„“(M ğ‘–,ğ‘‘)isabsentduringthetrainingprocess.\nğœƒ ğœƒ\n5 APPROACH\n5.1 RationaleforTheCRoPApproachDesign\nAspreviouslydiscussed,thegenericmodelâ€™sparameterscontaingeneralizableinformationacrossallcontexts.\nAddressing the problem statement requires retaining this information to the greatest extent while enabling\nfine-tuningforthetargetuser.Furthermore,ourinvestigationrevealedthatdifferentparametersholdvarying\ndegrees of importance in distinct contexts. Hence, the careful selection of subsets of model parameters for\npersonalizationandgeneralizationispivotalforthesuccessoftheapproach,forwhichthispaperleveragesthe\nmodelpruningparadigm.\nModel pruning is based on the idea that neural networks include redundant parameters, and removing\ntheseparametersminimallyimpactsthemodelâ€™sperformance[48,96].Consequently,pruningthefine-tuned\npersonalizedmodelensurestheretentionofessentialparameterstomaintainaccuracyforcontextCğ‘,meaning,\ncapturingthetargetuser-specifictraits.However,theprunedparameterscanbereplacedwithcorresponding\nparametersfromthegenericmodel,effectivelyrestoringgenericknowledgelearnedacrossallcontextsonthose\n,Vol.1,No.1,Article.Publicationdate:November2024.\n10 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nAlgorithm1CRoP\n1: Input:M ğœƒğº :GenericModel â‹„ D ğ‘–ğ‘ :UserU ğ‘–â€²ğ‘  dataforavailablecontextCğ‘ â‹„ ğ›¼:coefficientofregularization\nâ‹„ ğœ:toleranceforpruning\n2: TraintheGenericmodelonthepersonaldataD ğ‘–ğ‘\nMğ‘ƒ ğ‘–ğ‘\n=argmin\nâˆ‘ï¸ â„“(Mğº,ğ‘‘)+ğ›¼âˆ¥Mğºâˆ¥\nğœƒâ€² ğœƒ ğœƒ 1\nğœƒ ğ‘‘âˆˆDğ‘\nğ‘–\n3: Pruneredundantparameterstoobtaintheprunedsub-structure\nğ‘ƒğ‘ ğ‘ƒğ‘\nM ğ‘– =ToleratedPrune(M ğ‘–,ğœ)\nğœƒâ†“ ğœƒâ€²\n4: Copytheparametersofgenericmodelstotheprunedparametersinthepersonalizedprunedmodel,\n(cid:40) ğ‘ƒğ‘\nğ‘ƒğ‘ M ğ‘– ,ğœƒâ†“ â‰ 0\nM ğ‘– = ğœƒâ†“\nğœƒâ€²â€² Mğº ,otherwise\nğœƒ\n5: FinetunethepersonalizedmodelontheD ğ‘–ğ‘ usingearlystoppingwithvalidationaccuracyinCğ‘\nğ‘ƒğ‘ âˆ‘ï¸ ğ‘ƒğ‘ ğ‘ƒğ‘\nM ğ‘– =argmin â„“(M ğ‘–,ğ‘‘)+ğ›¼âˆ¥M ğ‘– âˆ¥ ,\nğœƒ ğœƒâ€²â€² ğœƒâ€²â€² 1\nğœƒ ğ‘‘âˆˆDğ‘\nğ‘–\ngenericmodelparameters.Thisrestorationmayenhancegeneralizability,ensuringrobustperformanceinunseen\ncontextsCğ‘¢.Theapproachpresentedinthispaperisfoundedonthisinsightfulstrategy.\n5.2 CRoPApproach\nAlgorithm1describesthepresentedapproach,whichtakesasinput:thegenericmodelMğº,userU ğ‘–â€²ğ‘  dataD ğ‘–ğ‘\nforavailablecontextCğ‘,theinitialvalueforthecoefficientofregularizationğ›¼ andtoleranceforpruningğœ;and\nğ‘ƒğ‘\ngeneratesthetargetpersonalizedmodelM ğ‘– .Here,ğ›¼ andğœ arehyperparameterswhosevaluescanbetunedfor\nğœƒ\nthegivendataandmodel.\nTheapproachinitiatesbyfinetuningthegenericmodelMğº ondataDğ‘ ,concurrentlyapplyingâ„“ regularization\nğœƒ ğ‘– 1\nto penalize model parameters (line 2). This regularization encourages sparsity by specifically targeting the\nmagnitudeofredundantparameters[52].Thisstepisfollowedbythepruningofredundantweightsusingthe\nâ€˜ToleratedPruneâ€™module(line3).Theprunedweightsarethenreplacedbythecorrespondingweightsfromthe\ngenericmodelMğº (line4)torestoregeneralizability;thishybridmodelisreferredtoastheâ€˜MixedModel.â€™This\nstepleadstothemodificationoftheactivatedpathsinthepersonalizedmodel,resultinginchangesinthemodel\ninferences.However,sincethenewlyactivatedpathsaredeterminedbyweightsretainedfromtwomodelsand\nnotlearnedfromdatapatterns,thereisaconsequentlossofaccuracy,asshownanddiscussedinSection7.2.\nTomitigatesuchaloss,asafinalstep,theMixedModelundergoesfine-tuningonceagainonthedatafromthe\navailablecontextDğ‘\n(line5).Thedetailedexplanationofeachofthesestepsisasfollows:\nğ‘–\nPersonalized Finetuning with Penalty (Algorithm 1 â€“ Step 2): The approach uses data\nDğ‘\nto finetune the\nğ‘–\ngenericmodelMğº\n.AsshownintheSection3(Motivation),suchconventionalfinetuningenhancesthemodelâ€™s\nğœƒ\naccuracywithintheavailablecontextCğ‘.Nevertheless,itsperformanceinunfamiliarcontextsmaygetsuboptimal.\nNotably,duringthemodelâ€™sfine-tuningprocess,weapplyâ„“ regularizationtopenalizethemodelweights,forcing\n1\nthemagnitudesofredundantparameterstobeclosetozero[52].Theregularizationcoefficientğ›¼ isatrainable\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 11\nAlgorithm2ToleratedPrune(M,ğœ,D)\n1: Input:Mğœƒ:AModel â‹„ ğœ:toleranceforpruning â‹„ D:data\n2:\nPruningAmountğ‘ =ğ‘˜\n3:\nğ´\nğ‘œ\n=ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(Mğœƒ,D)\n4: repeat\n5: M ğœƒâ†“ =Mğœƒ\n6: Mğœƒ =ğ‘ƒğ‘Ÿğ‘¢ğ‘›ğ‘’(Mğœƒ,ğ‘)\n7:\nğ´=ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(Mğœƒ,D)\n8:\nIncrementPruningAmountğ‘ =ğ‘+ğ‘˜â€²\n9:\nuntilğ´ <ğ´\nğ‘œ\nâˆ’ğœ\n10: returnM ğœƒâ†“\nparameteroptimizedduringtrainingtominimizetheoverallloss.Asaresult,theparameterswithhighmagnitudes\ncarrymostoftheinformationregardingthedatapatternsinDğ‘\n,offeringtwokeybenefits:\nğ‘–\n(1) MinimallossinCğ‘accuracy:Ahighfractionofparametershaveclosetozeromagnitudes,andtheirremoval\nresultsinminimalinformationlossforcontextCğ‘;thus,theadverseimpactofpruningincontextCğ‘ is\nminimized.\n(2) Maximalgeneralization:TheinclusionofregularizationaidsToleratedPrune(discussedbelow)modulein\nefficientlypruningahighernumberofparameters,whicharethenreplacedwithweightsfromthegeneric\nmodel.Thisrestoresinformationfromthegenericmodel,contributingtoenhancedaccuracyinunseen\ncontexts.\nToleratedPruneModule(Algorithm1â€“Step3): Algorithm2outlinestheToleratedPrunemodule,takingamodel\nMğœƒ, pruning toleranceğœ, and the dataset D as inputs. It initiates with a modest pruning amount ofğ‘˜ and\nincrementallyincreasesthisamountbyğ‘˜â€² untilthemodelâ€™saccuracyexhibitsadropofğœ percentonD.Here,\nğ‘˜ andğ‘˜â€² are hyperparameters within the range of (0,1). The module returns M ğœƒâ†“, representing the pruned\nstateofthemodelbeforethelastpruningiteration.Thisstateissuchthatfurtherpruningwouldresultina\nhigheraccuracylossondatasetD thanthetolerableamountğœ.Thismoduleperformspruningleveragingthe\nconventionalmagnitude-basedunstructuredpruning[96].\nğ‘ƒğ‘\nThus,step3inAlgorithm1generatesaprunedpersonalizedmodelstateM ğ‘– whosepredictionaccuracy\nğœƒâ†“\nğ‘ƒğ‘\noncontext Cğ‘ isatmostğœ percentlowerthanthatoftheearlierstateM ğœƒâ€²ğ‘– whileusingonlyafractionofits\noriginalparameters.Thenon-zeroweightscorrespondingtotheseparameterscontributesignificantlytomodel\nğ‘ƒğ‘\ninferencefortheavailablecontextCğ‘.Asaresult,M ğ‘– isessentiallytheminimalsub-structureoftheearlier\nğœƒâ†“\nğ‘ƒğ‘\nstatemodelM ğœƒâ€²ğ‘– ,whichiscrucialforcorrectinferenceforcontextCğ‘.Thisenablesreplacingamaximalnumber\nofzeroed-outparameterstoincorporateinformationfromunseencontextsusingthegenericmodelMğº\ninthe\nğœƒ\nsubsequentsteps.\nğ‘ƒğ‘\nGeneratingtheMixedModel(Algorithm1â€“Steps4&5): ForgeneratingtheMixedModelM ğ‘– ,thezeroedout\nğœƒâ€²â€²\nparametersintheprunedmodelMğ‘ƒ ğ‘–ğ‘ arereplacedbythecorrespondingparametersinthegenericmodelMğº\n,\nğœƒâ†“ ğœƒ\nenablinggenericknowledgerestoration.\nNotably,modelpruningisoftenfollowedbyafinetuningstep[46,48,96],wheretheprunedmodelundergoes\nre-trainingtorecovertheperformancelostduringthepruningprocess.Wehaveobservedthat,despitetheMixed\nModelexhibitingimprovedperformanceintheunseencontext,thereisanotablelossofaccuracyintheavailable\n,Vol.1,No.1,Article.Publicationdate:November2024.\n12 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\ncontextduetoinconsistentactivatedpaths,asdiscussedearlier.Thus,theresultingMixedModelisfine-tuned\nusingtheavailabledataDğ‘\n.Goyaletal.[25]suggeststhatfine-tuningprocessshouldmirrorpre-trainingfor\nğ‘–\neffectivegeneralization.Therefore,ourfine-tuningobjectivealignswiththepre-trainingobjectiveusedinLine2\nforoptimalresults.\nDuringfinetuning,themodelstate,includingthemixedmodel,withthebestvalidationlossontheseencontext,\nisselected.Wefoundthatforsomeindividuals,themixedmodelischosenastheoptimalmodel,whichindicates\nthatforsomeindividuals,furtherfinetuningisnotrequired,andourapproachcanautomaticallyhandlethat\nscenario.\n6 EXPERIMENTS\nThissectiondisplaystheempiricalefficacyofthepresentedapproach.Section6.1providesadetaileddiscussion\nofthefourhumansensingdatasetsandtheirpre-processingusedinourevaluations:PERCEPT-R[7],WIDAR\n[94],ExtraSensory[79]andaStress-sensingdataset[91],andalsoexplainsthecorrespondingmodelarchitectures\nasusedinliterature.Section6.2detailsthemetricsofevaluationusedinthisworkalongwiththeirpractical\nrelevance.ThisisfollowedbyadetaileddiscussionoftheempiricalcomparisonofCRoPwithfivebaselines\nSHOT[43],Packnet[50],Piggyback[51],CoTTa[88]andPTN[10]inSection6.3.Additionally,detaileddiscussions\naboutinterestingpatterns,suchastheimpactofgenericmodelqualityonpersonalization,etc.,arealsoprovided.\nNotably, each dataset offers a varying distribution of data among different classes, necessitating different\nperformancemetricslikeinferenceaccuracyandF1score.Foreachdataset,ourevaluationstayedalignedwith\nthemetricspreviouslyusedinstudiesevaluatedonthesedatasets,withfurtherdetailsavailableintheAppendix\nA.Additionaldetailstosupportreproducibility,suchashyperparameters,linkstothecode,andcomputation\nresourcesutilized,arealsoprovidedinAppendixA.\n6.1 Datasetsandmodels\nThisworkemploysfourreal-worldhuman-sensingdatasetstodemonstratetheempiricalefficacyofCRoP,twoof\nwhichareassociatedwithhealthapplications.First,thePERCEPT-Rdatasethasbeenusedforbinaryclassification\nforpredictingthecorrectnessof/r/soundsinautomatedspeechtherapyapplication[6].Additionally,weusethe\nStressSensingdataset[91]collectedusingapsycho-physiologicalwrist-band,namedEmpaticaE4[17].Tofurther\ndemonstratetheefficacyofCRoP,weincorporatetwobenchmarkhuman-sensingdatasets,whichincludedata\nfromthesameindividualsacrossmultiplecontexts:WIDAR[94]andExtraSensory[79].Specifically,weemploy\nWIDARfora6-classclassificationfocusingongesturerecognitionusingWiFisignals,andExtraSensoryfor\nbinaryclassificationrelatedtohumanactivityrecognitionusingaccelerometerandgyroscopereadings.Details\nonthedatasets,preprocessingforpersonalizedevaluations,andgenericmodeltrainingarediscussedbelow.\nPERCEPT-R:. Thesound/r/hasbeenrecognizedasthemostfrequentlyimpactedsoundinresidualspeech\nsounddisordersinAmericanEnglish[40]andconsideredtobethemostdifficultsoundtotreat.ThePERCEPT-R\nCorpuswascollectedduring34differentcross-sectionalandlongitudinalstudiesofspeech[7]forautomated\nspeechanalysisof/r/.Thedatausedinthisstudycomefromtheprospectivelycollected[6],andcorpusversion\n2.2.2,whichincludesboththepubliclyavailableopenaccesssubset(2.2.2p)andprivatelyhelddatathatwas\nnotpublishedintheopenaccesssubsetafterareviewofconsent/assentpermissions.ItemsinthePERCEPT-R\nCorpusv2.2.2primarilyconsistofsingle-wordspeechaudiocollectedduringclinicaltrialsinvolvingchildren\nwithspeechsounddisordersaffecting/r/,alongwithage-matchedpeerswithtypicalspeech.Thefullcorpus\ncontains179,076labeledutterancesrepresenting662single-rhoticwordsandphrases.Eachaudiofileispaired\nwithaground-truthlabelrepresentinglistenerjudgmentsofrhoticity,derivedbyaveragingbinaryratings(0\n=derhotic,1=fullyrhotic)frommultiplelisteners.Forthisstudy,theheuristicthresholdforconvertingthese\naveragedratingsintobinaryground-truthlabelswas0.66.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 13\nTousethisdatasetforourpersonalizationevaluation,wecollaboratedwithclinicalexpertstoidentifyand\nacquireannotationsof16participantswhohadcorrectandincorrectpronunciationsof/r/soundatpre-treatment\n(baseline-phase)andduringdifferenttreatmentphases.AsoutlinedinSection4,theevaluationtreatspre-treatment\ndataasavailablecontextdata,whiledatafromothertreatmentphasesserveasunseencontextdata.\nWIDAR:. WIDARisadatasetcollectedforthepurposeofgesturerecognition.Itwascollectedusingoff-the-shelf\nWiFilinks(onetransmitterandatleast3receivers).17usersperformed15differentgesturesatdifferentrooms\nandorientations(oftheperson).Thechannelstateinformationiscollectedfromthesedeviceswithamplitude\nnoisesandphaseoffsetsremovedasapreprocessingstep.Thetwocontextsusedforthecurrentworkaredecided\nbasedontheorientationsofthetorsodataandroomID.Room1isaclassroomwithanumberofobjects(e.g.,\ndesksandchairs)init,andRoom2isanearlyemptyhallway.Thedissimilardatadistributionscanbeattributed\ntothedifferencesintheamountofinterruptionsinWiFisignals.Wefollowedthesamenormalizationmethods\nasYangetal.[92].\nExtraSensory: ExtraSensoryisahumanactivityrecognitiondatasetcollectedusingtheExtraSensorymobile\napplication.Anumberoffeatureswerecollectedfromdifferentcellulardevicesandsmartwatches,thoughwe\njustusedtheaccelerometerandgyroscopefeaturesobtainedfromthecellulardevices.Labelsforactivitieswere\nself-reportedbytheusersthroughthemobileapplication.ForourevaluationsonExtraSensory,5userswereleft\noutoftrainingasinglegenericmodel.Thecontextsaredecidedbasedonthelocationofthephone:hand,pocket,\nandbag.\nStressSensingDataset: ThisdatasetmeasuresthephysiologicalimpactsofvariouskindsofStress.Thedataset\nis collected using Empatica E4 Wristband to extract features such as EDA (Electrodermal Activity), a skin\ntemperaturesensor(4Hz),etc,contributingtoatotalof34features.Thedatawascollectedfrom30participants\nwithdifferentdemographics,whowereassignedthelabelsâ€˜Stressedâ€™orâ€˜Calmâ€™basedonthedifferentstress-\ninducingorcalmingtaskstheyexperienced.Inordertoadaptthisdatasetfortheproblemaddressedinthiswork,\nwecollaboratedwiththeoriginalauthorstoidentifyparticipantswhoworewristbandsonbothhands,which\nwerethenchosenforthetaskofpersonalization.Additionally,duringthedatacollection,participantswereasked\ntoperformseveralactivities,afewofwhichrestrictedparticipantsâ€™movementwhileothersallowedthemto\nmove.Basedonthis,thedatawasannotatedwithmovementpatterns,thatis,stillvs.moving.Thisadditional\nannotationofthedatasetwillbemadepublicalongwiththiswork.Duringthisdatasetâ€™sevaluation,thecontext\nisdefinedbyacombinationofthehandonwhichthewristbandwaswornandwhetherornotthepersonwas\nmovingduringthedatacollection.\n6.1.1 Pre-ProcessingoftheDatasets. Wepartitionedeachdatasetintotwodisjointsetsofusers:(1)ageneric\ndatasetfortrainingagenericmodeland(2)apersonalizeddatasetfortrainingapersonalizedmodelforeach\nuser.Todemonstratethecontext-wiserobustness,wefurtherpartitionedeachuserâ€™s(belongingtothelaterset)\npersonalizeddatasetintodifferentcontexts(i.e.,availableCğ‘ andunseencontextCğ‘¢).Table3presentsthedetails\nofthispartitioning.ForPRECEPT-R,weconsiderdatafromthepre-treatmentphaseastheavailablecontext,and\nthetreatmentphases,whereparticipantsundergoclinicalinterventions,areconsideredtheunseencontext.For\ntheStressSensingdataset,thecontextisdeterminedbytwofactors:thehandonwhichthesensor(EmpaticaE4\nwristband[17])waswornduringdatacollectionandthemovementstatusoftheindividual.ForWIDAR,context\nis determined by the room and torso orientation during data collection, while for the ExtraSensory dataset,\nphoneâ€™slocationontheuserâ€™sbody(e.g.,hand,pocket,bag)definesthecontext.Thetermâ€˜Scenarioâ€™referstothe\ncombinationofavailableCğ‘ andunseenCğ‘¢ contextsasoutlinedinTable3.Alldatasets,alongwithcontext-wise\nannotations,willbemadepublic.\nNotably,throughoutthetrainingofpersonalizedmodels,CRoPrefrainsfromutilizinganyinformationfromthe\nunseencontextCğ‘¢.Therefore,whiletheempiricalstudyindicatesanenhancementinthemodelâ€™sperformancefor\n,Vol.1,No.1,Article.Publicationdate:November2024.\n14 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nDatasetâ†’ PERCEPT-R WIDAR ExtraSonsory StressSensing\nTotalusers 515 17 60 30\nUsersâ€™IDfor 17,25,28,336,344,361,362,55, 0,1,2 80,9D,B7,61,7C 1,2,3\nPersonalization 586,587,589,590,591,61,67,80\nScenario1 Cğ‘:BaselineStudy Cğ‘:Room-1,TorsoOrientation-1,2,3 Cğ‘:Hand,Pocket Cğ‘:Lefthand,Still\nCğ‘¢:TreatmentPhase Cğ‘¢:Room2,TorsoOrientation-4,5 Cğ‘¢:Bag Cğ‘¢1:Righthand,Still;Cğ‘¢2:Righthand,Moving\nScenario2 -N/A- Cğ‘:Room2,TorsoOrientation-4,5 Cğ‘:Bag,Pocket Cğ‘:Righthand,Moving\nCğ‘¢:Room-1,TorsoOrientation-1,2,3 Cğ‘¢:Hand Cğ‘¢1:Lefthand,Moving;Cğ‘¢2:Lefthand,Still\nTable3. Detailsofdatausedforpersonalization\noneorafewunseencontexts,itisaproxyforallunseencontexts.Thismeansthatitisreasonabletoanticipatea\nfavorableperformanceinotherunseencontextsthatarenotavailableonthedataset.\nNotably,thestresssensingdatasethasbeenevaluatedacrosstwodifferentunseencontextvariations.InC ğ‘¢1,\ntherewasonechangeincontextâ€”achangeinhandwhilekeepingthesamemovementpatternasCğ‘.InC ğ‘¢2,there\nweretwochangesâ€”achangeinbothhandandmovementpatterns.\n6.1.2 TrainingoftheGenericmodels.\nPERCEPT-R. :Fortheidentified16participantsforourpersonalizationevaluation,theirspeechdatawere\ncollectedlongitudinally,meaningtheirdatacouldbeseparatedintoavailableandunseencontextsversusother\nspeakersinthecorpuswhoonlyhadspeechdataavailablefromaone-timepoint.Thegenericmodelistrained\nfortheremaining499participants(having/r/soundsdisorder)usingperson-disjointvalidationandtestsets.The\naimofthisdatasetistoidentifythecorrectnessof/r/sounds.\nModel:InlinewiththeliteratureBenwayetal.[7],wetriedseveralmodelarchitecturessuchasCNN,DNN,\nBILSTM,etc,whosenumberofparameterswereidentifiedusinggridsearch.Amongthose,thebiLSTMmodel\ncontaining4bidirectionalLSTMlayersfollowedby5linearlayers,accompaniedbyaHardswishactivationlayer,\nwasidentifiedastheonethatexhibitedthebestresultsforthegenericdataandwasusedforthisstudy.\nWIDAR:. Wechose3usersforpersonalizationsincetheseweretheonlyuserswhosedatawascollectedin\nbothrooms.SincethenumberofusersinWIDARisverysmall,theexclusionofallthe3usersfortrainingthe\ngenericmodelwouldhaveresultedinsubstandardmodels.So,Foreachuser,wegenerateddifferentgeneric\nmodelsbyusingdatafromtheother16userswitha14/2persondisjointrandomsplitforthetrainandvalidation\nset.Ourclassificationtargetwasthe6gestureclasses:0,1,2,3,5and8,correspondingtopush,sweep,clap,slide,\ndrawacircle,anddrawzigzag,respectively,asevaluatedintheoriginalwork[94].\nModel:ThemodelusedforWIDARfollowstheLeNetarchitecture[94],whichcontainsthree2Dconvolutional\nlayersfollowedbytwolinearlayers.Eachoftheselayers,exceptthefinalclassificationlayer,isfollowedbya\nReLUactivationlayer.\nExtraSensory: Wechose5usersforpersonalization,andthegenericmodelistrainedon42users,with10users\nbeingleftoutforperson-disjointvalidation.Thetwotargetclassesarewalkingandsitting.\nModel:ThemodelfollowsaCNN-GRU-basedarchitectureusedinHARliterature[23,26,69,93].Themodel\nconsists of three batch-normalized 1D convolution layers followed by a linear layer that feeds into a batch-\nnormalizedrecursive(GRU)layerandtwolinearlayerstogenerateembeddings.Fortheclassificationhead,two\nlinearlayerswereused.\nStressSensingDataset: Forthisbinarytask,threeuserswithdataacrossallcontextswereselectedforpersonal-\nization.Thegenericmodelwastrainedondatafrom21users,withsixadditionalusersfordisjointvalidationand\ntestsets.\nModel:Themodelusesasimplemulti-layer-perceptron(MLP)architecture[16,18,89]consistingof3linear\nlayerswithhiddensizeof128.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 15\n6.2 Metricsforevaluation\nToestablishtheefficacyofCRoP,wequantifytheextentofpersonalizationandgeneralizationachievedthrough\nğ‘ƒğ‘\nthepresentedapproach.Personalizationisgaugedbycomparingourmodelâ€™sM ğ‘– accuracyrelativetothegeneric\nğœƒ\nmodelMğº ,whileforgeneralization,weassesstheaccuracyofourmodelMğ‘ƒ ğ‘–ğ‘\nagainstconventionally-finetuned\nğœƒ ğœƒ\nğ¶ğ‘\npersonalizedmodelsM ğ‘–.\nğœƒ\nTaorietal.[76]arguedthatdirectlycomparingmodelaccuraciesunderdistributionshiftsisnotideal.They\nintroducedâ€˜effectiverobustness,â€™ametricthatassessesperformancerelativetoanaccuracybaseline.Sincewe\naimtocompareourmodelsagainsttwobaselinesâ€”thegenericandconventionallyfinetunedmodelsâ€”weadopt\ntheâ€˜effectiverobustnessmetricâ€™andintroducetwospecificmetricsforourcomparison,detailedbelow.Bothof\nthesemetricsconsiderclassificationaccuracyintheavailableCğ‘ andunseenCğ‘¢ contexts.\nIfA(M,D)representstheclassificationaccuracyofthemodelM fordatasetD andğ‘›isthenumberofusers\nselectedforpersonalization,themetricsofevaluationscanbedescribedasfollows:\n(1) Personalization(Î” ğ‘ƒ):ItisdefinedasthesumofthedifferencebetweentheaccuracyofM ğœƒğ‘ƒ ğ‘–ğ‘ andM ğœƒğº over\nallthecontextsaveragedoverallusers\nÎ” ğ‘ƒ = ğ‘›1 âˆ‘ï¸ âˆ‘ï¸ (A(M ğœƒğ‘ƒ ğ‘–ğ‘ ,C)âˆ’A(M ğœƒğº,C))\nUğ‘– Câˆˆ{Cğ‘,Cğ‘¢}\nğ‘ƒğ‘ ğ¶ğ‘\n(2) Generalization(Î” ğº):ItisdefinedasthesumofthedifferencebetweentheaccuracyofM ğœƒğ‘– andM ğœƒğ‘– over\nallthecontextsaveragedoverallusers.\n1 âˆ‘ï¸ âˆ‘ï¸ ğ‘ƒğ‘ ğ¶ğ‘\nÎ” ğº = ğ‘› (A(M ğœƒğ‘–,C)âˆ’A(M ğœƒğ‘–,C))\nUğ‘– Câˆˆ{Cğ‘,Cğ‘¢}\nInsummary,themetricÎ” ğ‘ƒ suggestshowwellthemodelM ğœƒğ‘ƒ ğ‘–ğ‘ performsascomparedtothegenericmodelM ğœƒğº .\nSincethegenericmodelhasnotlearnedtheperson-specificpatterns.ThismetricquantifiesCRoPâ€™sabilitytolearn\nğ¶ğ‘\nperson-specificpatterns.Ontheotherhand,conventionallyfinetunedmodelsM ğ‘– maylearnperson-specific\nğœƒ\npatternsandforgetthegenericinformationofdifferentcontexts.Thus,themetricÎ” ğº quantifiesCRoPâ€™sabilityto\nretaingenericinformation.\nAlltheresultsinthissectionarecomputedasanaverageofaccuracyobtainedforthreerandomseeds.\n6.3 ComparisonwithState-of-the-art\nTodemonstratetheefficacyofCRoPinachievingpersonalizationÎ” ğ‘ƒ whilemaintaininggeneralizationÎ” ğº,we\ncompareCRoPwith5state-of-the-artapproachesSHOT[43],PackNet[50],Piggyback[51],CoTTA[88],and\nPTN[10].\nTable4comparestheperformanceofCRoPwithaforementionedbaselineapproaches.ThevaluesforÎ” ğ‘ƒ and\nÎ” ğº arecomputedasaverageoveralltheparticipantsusedforpersonalizationforeachdataset.Thedetailed\nresultsforparticipant-specificevaluationsforeachdatasetareprovidedinAppendixBandAppendixB.3shows\ntheerrorsbarsforourapproach.\nTable 4 shows that CRoP significantly outperforms all state-of-the-art (SOTA) methods. On average, the\npersonalizationbenefitsÎ”\nğ‘ƒ\nachievedbySHOT,PackNet,PiggybackandCoTTAare2.16,26.05,18.01,9.95,and\n4.13percentpoints,respectively,whileCRoPcanachieve35.23percentpoints.However,whilecomparingÎ” ğº,\nonecanobservethatpersonalizedtrainingusingSHOT,PackNet,PiggybackandCoTTAharmsgeneralizability\nby âˆ’25.73, âˆ’1.39, âˆ’9.43, âˆ’17.49 and âˆ’23.44 percent points respectively. On the other hand, CRoP shows an\naveragegeneralizationbenefitof7.78.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n16 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nApproach SHOT Packnet Piggyback CoTTA PTN CRoP\nDataset Scenrio Î” ğ‘ƒ Î” ğº Î” ğ‘ƒ Î” ğº Î” ğ‘ƒ Î” ğº Î” ğ‘ƒ Î” ğº Î” ğ‘ƒ Î” ğº Î” ğ‘ƒ Î” ğº\nPERCEPT-R Scenario1 -3.11 -5.62 0.10 -2.41 -25.31 -27.83 -45.06 -47.58 -1.16 -3.68 5.08 2.57\nStressSensing Scenario1 -8.19 -62.16 54.70 0.70 43.89 -10.12 21.93 -32.07 1.23 -52.76 67.81 13.81\nSinglecontextChange Scenario2 8.90 -63.27 75.80 3.64 66.22 -5.94 51.47 -20.69 21.76 -50.40 85.25 13.08\nStressSensing Scenario1 -0.49 -47.24 52.46 10.08 32.40 -9.97 30.59 -11.78 6.40 -35.98 54.38 12.00\nDoublecontextChange Scenario2 3.57 -45.49 41.68 -7.36 42.76 -6.25 33.85 -15.19 12.30 -36.74 59.21 10.15\nWIDAR Scenario1 1.67 -0.48 -0.24 -2.37 0.84 -1.28 -1.05 -3.18 -1.99 -4.37 8.56 6.43\nScenario2 1.28 -0.03 -3.55 -5.16 -8.97 -10.57 1.81 0.21 0.00 -2.85 5.90 4.30\nExtraSensory Scenario1 7.63 -10.31 12.19 -5.76 5.03 -12.91 -0.6 -18.54 1.69 -16.72 17.49 -0.46\nScenario2 8.17 2.99 1.33 -3.85 5.22 0.04 -3.43 -8.62 -3.02 -7.47 13.52 8.17\nTable4. ComparisonofCRoPwithbaselineapproachesunderthemetricsofPersonalization(Î” )andGeneralization(Î” ).\nğ‘ƒ ğº\n(a)Perspective-models (b)Perspective-Contexts\nFig.3. DetailedresultsforStresssensingdatasetUser3underScenario1usingF1scoreastheevaluationmetric\nNotably,domainadaptationandcontinuallearningmethods,suchasSHOT,Packnet,Piggyback,CoTTA,and\nPTN,leveragedatafromnewcontextstoadaptmodelswhilepreservingknowledgefrompreviouscontextsor\ntasks.However,theproblemaddressedinthisworkrequiresthemodeltoperformreasonablywellinacompletely\nunseencontext,forwhichnodataâ€”labeledorunlabeledâ€”isavailableduringtraining.Asaresult,allbaseline\napproachesstrugglesignificantlyinthisscenario.SincethemetricsinSection6.2evaluateperformanceacross\nbothavailableandunseencontexts,thesubstantialperformancedropintheunseencontextnegativelyimpacts\noverallresults.Furthermore,theunsupervisedapproachesSHOT,CoTTA,andPTNachievedlowerÎ”\nğ‘ƒ\nandÎ”\nğº\nthansupervisedapproachesPacknet,Piggyback,andCRoP,whichisinlinewithliterature[82].\nThefollowingdiscussesnotablepatternsobservedinthissectionâ€™sevaluationsinTable4,insightsintothe\ncharacteristicsofstaticpersonalizationacrossvarioushuman-sensingapplications.\n6.3.1 Forstress-sensingdataset,alltheunsupervisedapproachesshowbetterperformanceondoublecontextchange\nthanthesingle-contextchange. ForScenario1,Cğ‘ containssamplesofdatacollectedusingthelefthandwhile\nstayingstill.ForC ğ‘¢1(singlecontextchange),thehandchangestotherightwhilestayingstill,butinC ğ‘¢2(double\ncontextchange),bothhandandmotionstatuschanges.Since C ğ‘¢2 incursgreatershiftincontextthanC ğ‘¢1,one\ncanexpectthatamodeltrainedonCğ‘ willshowsimilarorworseresultsforC ğ‘¢2 ascomparedtoC ğ‘¢1.However,\nunsupervisedapproachesâ€”SHOT,CoTTA,andPTNâ€”exhibithigherÎ” ğ‘ƒ andÎ” ğº valuesunderC ğ‘¢2thanC ğ‘¢1.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 17\nUser-specificresults,however,indicatethatmodelsactuallyperformworseonC ğ‘¢2.Figure3(a)showsdetailed\nresultsintermsofF1scoreforUser3forgenericmodelMğº ,conventionally-finetunedmodelMğ‘ƒ ğ‘–ğ‘\n,andmodels\nğœƒ ğœƒ\ngeneratedusingunsupervisedapproaches:SHOT,CoTTAandPTNunderScenario1.Asexpected,allmodels\nperformworsewiththedoublecontextchangeC ğ‘¢2(CU2=Right-move)comparedtothesinglecontextchange\nC ğ‘¢1(CU1=Right-still).Despitethis,theÎ” ğ‘ƒ andÎ” ğº valuessuggestotherwise,explainedasfollows:\nFirst,considerthe Î” ğ‘ƒ results,withthegenericmodelastheaccuracybaseline.Figure3(b)showsthatthe\ngenericmodelâ€™sF1scoredropssignificantlywiththedoublecontextchangeC ğ‘¢2comparedtothesinglecontext\nchange,C ğ‘¢1,depictedonthebluelines.Althoughtheperformancebenefitsusingthethreeapproachesarealmost\nsimilarinbothunseencontexts,thedifferenceintheperformanceascomparedtothegenericmodelishigherfor\ndoublecontextchange,thusresultinginhigherÎ” ğ‘ƒ Similarly,forÎ” ğº,withtheconventionallyfinetunedmodel\nasthebaseline,thebestresultsareseenintheavailablecontext,Cğ‘ (Left-still).Itsperformanceisonlyslightly\naffectedby thesingle context changebut dropssignificantly in C ğ‘¢2 due tomovement duringdata collection,\nresultinginahigherÎ” ğº forthedoublecontextchange.\nSimilarpatternswereobservedinotherusers,indicatingthatthepoorperformanceofMğº andMğ‘ƒ ğ‘–ğ‘\nduring\nğœƒ ğœƒ\ndoublecontextchangedrivesthehigherÎ” ğ‘ƒ andÎ” ğº values,givinganillusionofbetterresultsfordoublecontext\nchangescenarios.\n6.3.2 SignificantperformancedropforPERCEPT-RusingSOTAapproaches. ForthePercept-Rdataset,personal-\nizationappliedtothe16participantsrevealsthatnotallparticipantsbenefitequally.Insomecases,theglobal\nmodelsalreadyperformwell,leavinglittletonoroomforimprovementthroughpersonalization.Insuchcases,\nperformancecanevendropduetooverfittingontheavailablecontextdata.Ourapproachmitigatesthisby\nselecting the best model based on the highest validation accuracy from the available context, thus avoiding\noverfitting.Furthermore,CRoPintroducesatrainableregularizationcoefficientduringtheinitialfinetuningphase,\nwhichpenalizesmodelparameterswhileminimizingclassificationerror(Algorithm1line2-trainableparameter\nğ›¼).ThisenablesthemaximalremovaloflessimportantweightsduringtheToleratedPrunestepwithoutimpacting\nperformanceintheavailablecontext(Algorithm1line3).Additionally,wefoundthatfinetuningafterthemodel\nmixingstepwasoftenunnecessary,asthemixedmodeltypicallyemergedasthebestchoice(Algorithm1line\n5).Incontrast,thebaselineapproacheslacktheseconstraints,leadingtoperformancedegradationinboththe\navailableandunseencontexts,negativelyaffectingtheoverallresults.\n6.3.3 Inferiorgenericmodelperformanceleadstohighergaininpersonalization. ForExtraSensoryandStress-\nsensingdatasets,certainusersexperiencesub-optimalperformancewiththegenericmodelMğº\nandtheper-\nğœƒ\nsonalized finetuning helped not only in available context but also in the unseen contexts. For instance, the\ngenericmodelshowedsuboptimalperformanceforcertainuserssuchasuserâ€˜80â€™forExtraSensorydatasetunder\nScenario1anduserâ€˜3â€™forStress-sensingdataset.Suchusersshowhigherbenefitevenintheunseencontext\nwhenpersonalizedusingavailablecontextdata,highlightingtheimportanceofuser-specificpatterns.Thedetails\nexplanationoftheseresultsareprovidedinAppendixB.\n6.3.4 SignificantPerformancegainsforthestress-sensingdatasetforallapproaches. Psychophysiologicalstress\nresponseisinherentlyheterogeneousininter-andintra-userscenarios[54],leadingtosubparperformanceof\nthegenericmodelwithoutpersonalization.AsdiscussedinSection6.3.3,subparperformanceofgenericmodels\nresultsinhighergainduringpersonalization.ThisisevidentfromthesignificantlyhighÎ” ğ‘ƒ valuesachievedby\nmostapproachesforthestress-sensingdatasetinTable4.\nThese evaluations confirm that models personalized with CRoP exhibit higher generalizability to unseen\ncontexts,makingthemmoreintra-userrobust.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n18 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n7 EMPIRICALJUSTIFICATIONFORCROP\nThissectionempiricallyjustifiesanddiscusseshowtheuseofdifferentcomponentsofCRoPhelpsinincorporating\npersonalization(Î” ğ‘ƒ)andgeneralization(Î” ğº).\n7.1 HowdifferentstepsofCRoPfacilitategeneralizability\nThissectionempiricallydiscusseshoweachstepofCRoP(Algorithm1)facilitatesintra-usergeneralizability,\nsignifyingsimilarityinthemodelâ€™sbehaviortowardsavailableCğ‘ (availableduringpersonalizationfinetuning)\nandunseencontextsCğ‘¢.\nShietal.[70]introducedtheuseofgradientinnerproduct(GIP)toestimatethesimilaritybetweenamodelâ€™s\nbehavioracrossdifferentdomains.Ifğº\nğ‘–\nandğº\nğ‘—\nrepresentthegradientincurredbythemodelforDomainsğ·\nğ‘–\nandğ· ğ‘—,thenthesignoftheproductğº ğ‘– âˆ—ğº ğ‘— representswhetherthemodeltreatstwodomainssimilarlyornot.\nForinstance,ğº ğ‘– âˆ—ğº ğ‘— > 0signifiesthatthegradientforbothdomainshasthesamedirection.WeusedGIPto\nquantifygeneralization.AhigherGIPvalueforapersonalizedmodelacrossavailable(Cğ‘)andunseencontexts(Cğ‘¢)\nindicatesmoresimilarbehaviortowardbothdomains,indicatinghigherintra-usergeneralizability.GIPismeasured\nas:âˆ¥(cid:205) ğ‘–ğº ğ‘–âˆ¥2âˆ’(cid:205)\nğ‘–\nâˆ¥ğº ğ‘–âˆ¥2.\nFigure4showsthatfine-tuningthegenericmodel\n(Algorithm1â€“Step2)onContextCğ‘,optimizesthe\nmodelforthiscontext,leadingtoanegativeGIP,in- User 0 User 1 User 2 Average\ndicatingagreaterdiscrepancybetweentwocontexts. 20000\nSincemodelpruningresultsingeneralization[34],an\n15000\nincreaseinGIPvaluecanbeobservedinthepruned\nmodel(Step3). 10000\nOnfurtheranalysis,wefoundthatthemodelcom-\n5000\nplementarytotheprunedmodel(thatis,theparame-\ntersthatwereremoved)alsocontributedtowardsinter- 0\ncontext behavior discrepancy (negative GIP value).\n-5000\nHowever,thesameparametersinthegenericmodel Generic Finetuned Pruned Mixed Final\n(Line 2) (Line 3) (Line 4) (Line 5)\n(thatarereplacedinStep4)formedamoregeneral-\nizable set of weights, i.e., GIP â‰¥ 0. Thus, the model\nmixingstep(Step4)introducesfurthergeneralizability Fig.4. VariationofGIPatdifferentstagesofCRoPsignified\n(GIPâ‰¥ 0)inthepersonalizedmodel. bythelinesinAlgorithm1\nFinally, as discussed in Section 5.2 and following\nSection7.2,theobjectiveofthefinalfinetuningstepistorecovertheperformanceloss,anditdoesnâ€™taimto\nenhancegeneralizabilityfurther,asalsoshowninFigure4.\n7.2 Justificationfortherequirementoffinalfinetuningstep(Algorithm1Line5)forsomeusers\nInSection5.2,wediscussedthattheMixedModelfromAlgorithm\n1Line4suffersaccuracylossduetoalteredactivationpaths,as\nitcombinesweightsfromthegenericandpersonalizedstages.To\nillustrate,wecomparetheperformanceandactivationmapsof\nthemodelstatesatstep2(Mğ‘ƒ ğ‘–ğ‘ğœƒâ€²)andtheMixedModel(Mğ‘ƒ ğ‘–ğ‘ğœƒâ€²â€²).\nğ‘ƒğ‘\nTable5showsthattheMixedModelM ğ‘– doesshowanaver-\nğœƒâ€²â€²\nageimprovementof14.43%ininferenceaccuracyfortheunseen\n(a)Mğ‘ƒ ğ‘–ğ‘ (b)MixedModel\ncontextCğ‘¢,duetoretainmentofgenericmodelweights;however, ğœƒâ€²\nthereissignificantlossintheavailablecontextCğ‘ ascomparedto\nFig.5. ActivationmapsinCğ‘forScenario1\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 19\nModel Mğ‘ƒ ğ‘–ğ‘ Mğ‘ƒ ğ‘–ğ‘ A(Mğ‘ƒ ğ‘–ğ‘ ,C)âˆ’A(Mğ‘ƒ ğ‘–ğ‘ ,C) Mğ‘ƒ ğ‘–ğ‘ A(Mğ‘ƒ ğ‘–ğ‘ ,C)âˆ’A(Mğ‘ƒ ğ‘–ğ‘ ,C)\nğœƒâ€² ğœƒâ€²â€² ğœƒâ€²â€² ğœƒâ€² ğœƒ ğœƒ ğœƒâ€²\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n0 87.06 65.02 70.93 74.64 -16.13 +9.62 83.67 69.53 -3.39 +4.51\n1 89.38 44.38 74.23 72.70 -15.15 +28.32 86.41 54.45 -2.97 +10.07\n2 71.88 64.45 60.09 69.79 -11.79 +5.34 77.02 62.63 +5.14 -1.82\nAverage -14.36 +14.43 -0.41 +4.25\nTable5. PerformanceComparisonofmodelstatesafterinitialfinetuning(Algorithm1line2),modelmixing(Algorithm1\nline4)andfinalfinetuning(Algorithm1line5)forWIDARdatasetunderScenario1\nğ‘ƒğ‘\nthefinetunedmodelM ğ‘– .Weattributethislosstobothpruning-\nğœƒâ€²\ninducedinformationlossandinconsistentchangesinactivation\npaths during model mixing. Figure 5 shows the change in the activation map for a sample from context Cğ‘,\nğ‘ƒğ‘\nwhichwascorrectlyclassifierbyM ğ‘– ;however,itgotmisclassifiedafterthemodelmixingstep.Significant\nğœƒâ€²\ndifferencesinactivationmapscanbeobservedforthissample.Thishighlightstheneedforfinetuningtorecover\nlostinformationandrestoreconsistentactivationpaths.\nTable5furthershowsthatthefinalfinetuningstepindeedhelpsrecoverthelossintheavailablecontextCğ‘.\nAlthoughthismayslightlyreduceaccuracyintheunseencontextduetooverwritingsomegenericpatterns,it\nprovidesanaverageimprovementof4.25percentagepointsintheunseencontextCğ‘¢ ascomparedtothemodel\nachievedafterfirstfinetuning(Algorithm1Line2).Thisshowsthatthefinalfinetuningisnecessaryforsome\nuserstoregainperformancelostinavailablecontextCğ‘.\n8 DETAILSOFABLATIONSTUDY\nThis section presents evaluations showing the effectiveness of the design choices of CRoP, focusing on the\nWIDARdatasetinScenario1.Figure6comparesthecurrentdesignchoiceswithalternativeoptionsavailablein\ntheliterature.Thecomparisonisdoneforeachofthethreeuserschosenforpersonalizationunderbothavailable\nCğ‘ andunseenCğ‘¢ contexts.Themetricusedforthiscomparisonistheinferenceaccuracy.Similarpatternswere\nobservedinotherscenariosanddatasets.\n8.1 Pruningmechanism\nCRoPusesone-shotmagnitude-basedpruning(ğ‘€ğ‘ƒ)[48,96]toremovethelowest-magnitudemodelparameters\nintheToleratedPrunemodule(Algorithm1,Line3).Variousotherpruningmethodsexist,mostrelevantones\nbeing:Gradient-BasedPruning(ğºğ‘ƒ)[46],pruningtop-magnitudeweightsinsteadoflowerones(ğ‘€ğ‘ƒ âˆ’ğ‘‡)[5],\nanditerativepruning(ğ‘€ğ‘ƒ âˆ’ğ¼)[56][30].Acomparativediscussionofthesemethodsisprovidedbelow.\nMagnitude-based(ğ‘€ğ‘ƒ)vs.Gradient-basedPruning(ğºğ‘ƒ):Liuetal.[46]introducedgradient-basedpruning\n(ğºğ‘ƒ),whichassignsimportancetomodelparameters(i.e.,kernelsornodes)basedontheâ„“ normofgradients\n1\n(computedfromthetrainingsetdata)andprunestheleastimportantones.Inliterature(e.g.,[38]),theapproach\nhasbeenshowntoworkonmodelstrainedandtestedonthedatahavingindependentandidenticaldistribution\n(IID).However,inthiswork,datafromcontextsCğ‘ andCğ‘¢ followdifferentdistributions.Since,accordingtothe\nproblemsetup,onlyCğ‘ dataisaccessible,usingGPinCRoPleadstooverfittingonCğ‘ andpoorperformanceon\nunseencontextsCğ‘¢.Incontrast,ğ‘€ğ‘ƒ selectsimportantparametersbasedsolelyonweightmagnitude,makingit\nmorerobustandresultinginbetterperformanceonCğ‘¢.AsshowninFigures6(a),ğºğ‘ƒ performswellonCğ‘ but\nconsistentlyunderperformscomparedtoğ‘€ğ‘ƒ onCğ‘¢ acrossallthreeusers.\nTopprune(ğ‘€ğ‘ƒ âˆ’ğ‘‡)vs.lowerprune(ğ‘€ğ‘ƒ):CRoPâ€™sconventionalmagnitude-basedpruning(ğ‘€ğ‘ƒ)removesa\npercentageofthelowest-magnitudeweights,whichtypicallyhaveminimalimpactonmodelinference[48,96].\n,Vol.1,No.1,Article.Publicationdate:November2024.\n20 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nMP (CROP) MP-T L1 (CRoP) L2\nGP MP-I L0 Pol\nUser0-Ca User0-Ca\n0.80\n0.75 0.8\n0.70 0.7\n0.65\nUser2-Cu 0.60 User0-Cu User2-Cu 0.6 User0-Cu\n0.55 0.5\n0.50\n0.4\n0.45\n0.3\nUser2-Ca User1-Ca User2-Ca User1-Ca\nUser1-Cu User1-Cu\n(a)PruningMechanism (b)RegularizationMechanism\nFig.6. Ablationstudyexploringdifferentalternativesfor(a)pruningand(b)regularizationmechanismsusinginference\naccuracyasthemetricofevaluation\nHowever,Bartoldsonetal.[5]suggeststhatpruningthetopweightsandretrainingthem(ğ‘€ğ‘ƒ âˆ’ğ‘‡)canimprove\ngeneralizationbycreatingflatterlosslandscapes.Althoughğ‘€ğ‘ƒ âˆ’ğ‘‡ yieldsmodelsthatgeneralizebetterthan\ntraditionalpersonalizedmodels,Figure6(a)showsthatitperformssignificantlyinferiortoCRoPforallusersin\nbothcontexts.ThisisbecauseCRoPaimstoisolateauser-specificsub-networkthatperformswellintheavailable\ncontextCğ‘,ratherthanfindingagenericsub-network.Pruningtopweightsremovesuser-specificinformation\nlearnedinstep2oftheAlgorithm1,whichslightlyimprovesperformanceinunseencontextCğ‘¢ butsignificantly\ndegradesperformanceincontextCğ‘.Moreover,sincetheremovedtopweightsareimportantforCğ‘,thefinal\nfinetuningstage(step5oftheAlgorithm1)recapturesthatinformation,overwritingthegenericinformation,\nnullifyingtheapproachâ€™seffect.\nOneshot(ğ‘€ğ‘ƒ)vsIterativeapproach(ğ‘€ğ‘ƒ âˆ’ğ¼)Thepresentedapproach,i.e.,ğ‘€ğ‘ƒ,usesaone-shotapproach\nwheretheinitiallyfinetunedmodelundergoesonepassofpruning,mixing,andfinetuning.However,wecompared\nitwithaniterativevariationaswell.Toimplementtheiterativeapproach,ğ‘€ğ‘ƒâˆ’ğ¼,weallowtheinitiallyfinetuned\nmodeltoundergomultiplepassesofpruning,mixing,andfine-tuning.AsshowninFigure6(a),ğ‘€ğ‘ƒ âˆ’ğ¼ doesnot\nsignificantlyimprovemodelperformance,yetitincurshighcomputationalcoststhroughrepetition.\nIntheone-shotapproach,theâ€˜ToleratedPruneâ€™module(Algorithm1line3andAlgorithm2)calculatesthe\noptimalpruningamountbyremovingthemaximumparameterswithoutexceedingtheaccuracylossthreshold\nğœ forcontext Cğ‘.Intheiterativeapproach,wegraduallyreachthesamepruningamountbyremovingsmall\nfractionsofparametersforeachcycle.Theseparametersarereplacedbyweightsfromthegenericmodel,but\nsincethefinetuninglossfunctionresemblestheinitialone,itdrivesthoseweightstolowermagnitudes,causing\nthemtobeprunedagaininthenextcycle.Thisrepeatsuntilthetargetpruningamountisreached,effectively\npruningasimilarsetofweightsastheone-shotmethodbutinsmallersteps.\n8.2 RegularizationMechanisms\nCRoPusesregularizationtopushmodelparameterstowardzero,makingpruningeasierinlatersteps.Weight\npenalty-based regularization methods apply different norms to the modelâ€™s weights, such as â„“ , â„“ , â„“ , and\n0 1 2\npolarization.Theâ„“ normisparticularlyeffectivebecauseitnotonlyreducesmodelparametersbutalsomakes\n1\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 21\nTrain\nDataset Platform Train(s) RSS(MBs)\nUsage(%)\nAMDRyzen95950X16-CoreProcessor 15.18 2787.36 24.99\nAppleM1 102.39 352 0.58\nPERCEPT-R NVIDIARTXA6000 1.94 1132.61 20\nAMDRyzen97950X16-CoreProcessor 14.22 830.97 24.9\nNVIDIAGeForceRTX4090 1.54 1279.2 36\nAMDRyzen95950X16-CoreProcessor 6.95 1469.59 24.9\nAppleM1 11.71 77 0.85\nStress-Sensing NVIDIARTXA6000 8.49 1071.14 7\nAMDRyzen97950X16-CoreProcessor 3.13 633.27 25\nNVIDIAGeForceRTX4090 4.31 1249.79 14\nTable6. ResourcerequirementforonetimetrainingusingCRoP\ntheleastsignificantparameterstozero.Thispropertyalsomakesâ„“ usefulforfeatureselection[27].Figure\n1\n6(b)comparesallthesepossibleregularizationchoices.Weobservedthatamongâ„“ ,â„“ ,â„“ andpolarization[97],\n0 1 2\nusingâ„“ regularizationismosteffectiveintheunseencontext.Thisisbecause,byforcingtheleastimportant\n1\nparameterweightstozero,itallowsmaximumpruningwithâ€˜ToleratedPrune,â€™helpingtorecoverasmuchgeneric\ninformationaspossible.\n8.3 Fullfinetunevs.partialfinetune\nInordertokeepthezeroed-outparametersaszero,conventionalpruningapproachesfinetuneonlytheweights\nthatareretainedduringthepruningphase.However,inthepresentedapproach,thezeroed-outweightsare\nreplacedbycorrespondingweightsfromthegenericmodel.Thus,therearenozeroparameters.So,thepresented\napproachfinetunesalltheparameters.Westillevaluatedbothfinetuningapproachesandobservedthatthereis\nnosignificantdifferencebetweenthetwoapproaches.\n9 RUN-TIMEANALYSIS\nCRoPisastaticpersonalizationapproachthatrequiresone-timeon-devicetrainingofthegenericmodelduring\npersonalization.Weperformedrun-timeevaluationstoassesstheviabilityoftheCRoPframeworkacrossfive\ndifferentdeploymentplatforms:AMDRyzen95950X16-CoreProcessor,AMDRyzen97950X16-CoreProcessor,\nAppleM1,NVIDIARTXA6000andNVIDIAGeForceRTX4090.Table6showstheresourcesrequiredinthe\ntrainingphaseintermsoftrainingtime(s),processmemory(MBs),andresource,i.e.,GPU/CPUutilization(%)for\ntwohealth-relateddatasets:PERCEPT-RandStress-Sensing.Accordingtoourevaluation,exceptforthescalable\ndevice,AppleM1,thetrainingtimeisacoupleofseconds,andresourcerequirementsareminimal.Evenon\nscalabledeviceslikeAppleM1,computationtimeremainsefficient,withonlyaslightincrease,typicallyaddingat\nmostafewminutes.Sincethisisaone-timeprocess,CRoPisefficientcomparedtocontinuouslearningmethods\nthatrequirerepeatedadaptation.Particularly,thismakesCRoPpracticalforcriticalapplications,suchasclinical\nsettings,whereitincursonlyabrief,one-timecomputationoverheadduringenrollment.\n10 LIMITATIONSANDFUTUREDIRECTION\nSomelimitationsandfutureresearcharediscussedbelow:\n(1) Thepaperperformedalimitedevaluationonpruningparadigmsthroughablationstudiesasitwasnotthe\nprimaryfocusofthestudy.Section8onablationstudyjustifiesCRoPâ€™sdesignchoicebutdoesnotestablish\nanyparticularparadigmâ€™ssuperiorityinunseencontexts.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n22 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n(2) Astheapproachreliesonusingapre-trainedoff-the-shelfmodelasaninput,thequalityofthismodel\ncanimpacttheperformanceofthefinalpersonalizedmodels.AsdiscussedinSection6.3.3,userswith\nsuboptimalperformanceofthegenericmodelshowhighergainthroughpersonalization.Ontheother\nhand,forcertainparticipantsinthePERCEPT-Rdataset,ahigh-performingmodelleavesminimalroomfor\nimprovementthroughpersonalization.\n(3) Werestrictourstudytothemodelsbenchmarkedanddeployedfordatasetsusedinthisworkwithout\naccountingformodelvariability.However,thedifferentmodelarchitecturesemployedacrossallthedatasets\nincorporatearangeoflayerssuchasconvolutional,linear,BiLSTM,andGRU,whichintroducessome\nvariability.\n(4) Someevaluationresults,forinstance,Table4evaluationsareshownasaverage.However,themetricsÎ” ğ‘ƒ\nandÎ” ğº arecalculatedindividuallyforeachperson.Thisisbecausemodelspersonalizedforoneusercannot\nbeappliedtootherusersinreal-worldsituations.Consequently,wefocusourevaluationsonintra-user\ngeneralizability,excludingdiscussionforinter-userorinter-datasetgeneralizability.\n11 BROADERIMPACT\nThispaperaddressesacriticalresearchgap,enhancingthepracticalutilityofhuman-sensingsolutionsinreal-\nworldapplications,particularlyinautomatedhealthcare.Next-generationhealthcaresystems,whichemploy\nneuralnetworksfortasksrangingfromdailyactivitydetection[73,78]tosafety-criticalconditionslikeatrial\nfibrillation[11],benefitfrompersonalizationduetotheheterogeneityinhealthsensingdata[33,67].CRoPoffers\nseveraladvantages:\n(1) EliminatingtheGenericModelTraining:CRoPleveragesoff-the-shelfpre-trainedmodels,eliminating\ntheneedfortraininggenericmodels.Itisespeciallyvaluableinclinicalsettingswhereprivacyconcerns\nrestrictdatasharingfortrainingpurposes[49].Thisincreasesthefeasibilityofdeployingpersonalized\nmodelsinhealthcare.\n(2) Noprivacyconcerns:CRoPoperatesonlocaldevices,eliminatingtheneedfortransferringpotentiallysen-\nsitiveinformationtoacentralserver.Todemonstratescalabilityonlocaldevices,theresourceconsumption\nforCRoPpersonalizationon5devicesisshowninSection9.\n(3) Flexibilitytouseanymodelarchitectures:Asmodelpruninghasprovenapplicabletovariousmodelarchi-\ntectures,CRoPisnotrestrictedtoanymodelarchitectureconstraints,ensuringwide-rangingapplicability.\n12 CONCLUSION\nThisstudyintroducesCRoP,anovelstaticpersonalizationapproachgeneratingcontext-wiseintra-userrobust\nmodelsfromlimitedcontextdata.Usingpruninginanovelwaytobalancepersonalizationandgeneralization,\nempiricalanalysisonfourhuman-sensingdatasetsshowsCRoPmodelsexhibitanaverageincreaseof35.23%in\npersonalizationcomparedtogenericmodelsand7.78%ingeneralizationcomparedtoconventionally-finetuned\npersonalizedmodels.CRoPutilizesoff-the-shelfmodels,reducingtrainingeffortandaddressingprivacycon-\ncerns.Withpracticalbenefitsandquantitativeperformanceenhancements,CRoPfacilitatesreliablereal-world\ndeploymentforAI-basedhuman-sensingapplicationslikehealthcare.\nREFERENCES\n[1] FarhadAhamedandFarnazFarid.Applyinginternetofthingsandmachine-learningforpersonalizedhealthcare:Issuesandchallenges.\nIn2018InternationalConferenceonMachineLearningandDataEngineering(iCMLDE),pages19â€“21.IEEE,2018.\n[2] Apple.Workouttypesonapplewatch.https://support.apple.com/en-us/HT207934,2023.\n[3] MartinArjovsky,LÃ©onBottou,IshaanGulrajani,andDavidLopez-Paz.Invariantriskminimization,2020.URLhttps://arxiv.org/abs/\n1907.02893.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 23\n[4] AyoAwobajo.3tipstomakegoogleassistantyourown.https://blog.google/products/assistant/how-to-personalize-google-assistant/,\n2023.\n[5] BrianR.Bartoldson,AriS.Morcos,AdrianBarbu,andGordonErlebacher. Thegeneralization-stabilitytradeoffinneuralnetwork\npruning,2020.\n[6] N.R.BenwayandJ.L.Preston. Artificialintelligenceassistedspeechtherapyfor/r/usingspeechmotorchainingandthepercept\nengine:asinglecaseexperimentalclinicaltrialwithchainingai.,2023.URLhttps://surface.syr.edu/etd/1703.\n[7] NinaRBenway,JonathanLPreston,ElaineHitchcock,YvanRose,AsifSalekin,WendyLiang,andTaraMcAllister.Reproduciblespeech\nresearchwiththeartificialintelligence-readyPERCEPTcorpora.J.SpeechLang.Hear.Res.,66(6):1986â€“2009,June2023.\n[8] VisarBerisha,ChelseaKrantsevich,PRichardHahn,ShiraHahn,GautamDasarathy,PavanTuraga,andJulieLiss.Digitalmedicineand\nthecurseofdimensionality.NPJDigit.Med.,4(1):153,October2021.\n[9] MehdiBoukhechba,AnnaNBaglione,andLauraEBarnes.Leveragingmobilesensingandmachinelearningforpersonalizedmental\nhealthcare.Ergonomicsindesign,28(4):18â€“23,2020.\n[10] DavidBurns,PhilipBoyer,ColinArrowsmith,andCariWhyne.Personalizedactivityrecognitionwithdeeptripletembeddings.Sensors,\n22(14),2022.ISSN1424-8220.doi:10.3390/s22145222.URLhttps://www.mdpi.com/1424-8220/22/14/5222.\n[11] JonahComstock.Study:Applewatchpairedwithdeepneuralnetworkdetectsatrialfibrillationwith97percentaccuracy,2017.\n[12] BotosCsaba,WenxuanZhang,MatthiasMÃ¼ller,Ser-NamLim,MohamedElhoseiny,PhilipTorr,andAdelBibi.Labeldelayinonline\ncontinuallearning,2024.URLhttps://arxiv.org/abs/2312.00923.\n[13] ZacharyA.Daniels,JunHu,MichaelLomnitz,PhilMiller,AswinRaghavan,JoeZhang,MichaelPiacentino,andDavidZhang.Efficient\nmodeladaptationforcontinuallearningattheedge,2023.\n[14] AntonioDâ€™InnocenteandBarbaraCaputo. Domaingeneralizationwithdomain-specificaggregationmodules. InThomasBrox,\nAndrÃ©sBruhn,andMarioFritz,editors,PatternRecognition,pages187â€“198,Cham,2019.SpringerInternationalPublishing. ISBN\n978-3-030-12939-2.\n[15] DiDuan,HuanqiYang,GuohaoLan,TianxingLi,XiaohuaJia,andWeitaoXu.Emgsense:Alow-effortself-superviseddomainadaptation\nframeworkforemgsensing.In2023IEEEInternationalConferenceonPervasiveComputingandCommunications(PerCom),pages160â€“170,\n2023.doi:10.1109/PERCOM56429.2023.10099164.\n[16] MaciejDzieÅ¼yc,MartinGjoreski,PrzemysÅ‚awKazienko,StanisÅ‚awSaganowski,andMatjaÅ¾Gams.Canweditchfeatureengineering?\nend-to-enddeeplearningforaffectrecognitionfromphysiologicalsensordata.Sensors,20(22):6535,2020.\n[17] empetica.Real-timephysiologicalsignals:E4eda/gsrsensor,2015.URLhttps://www.empatica.com/research/e4/.\n[18] EdaErenandTuÄŸbaSelcenNavruz.Stressdetectionwithdeeplearningusingbvpandedasignals.In2022InternationalCongresson\nHuman-ComputerInteraction,OptimizationandRoboticApplications(HORA),pages1â€“7.IEEE,2022.\n[19] EnricoFini,VictorGTurrisidaCosta,XavierAlameda-Pineda,ElisaRicci,KarteekAlahari,andJulienMairal.Self-supervisedmodels\narecontinuallearners.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2022.\n[20] USFoodandDrugAdministration.Proposedregulatoryframeworkformondificationstoartificialintelligence/machinelearning-based\nsoftwareasamedicaldevice.USFoodandDrugAdministration:SilverSpring,MD,USA,63,2019.doi:10.1016/j.apergo.2017.04.011.\n[21] YaroslavGaninandVictorLempitsky.Unsuperviseddomainadaptationbybackpropagation.InProceedingsofthe32ndInternational\nConferenceonInternationalConferenceonMachineLearning-Volume37,ICMLâ€™15,page1180â€“1189.JMLR.org,2015.\n[22] YaroslavGanin,EvgeniyaUstinova,HanaAjakan,PascalGermain,HugoLarochelle,FranÃ§oisLaviolette,MarioMarchand,andVictor\nLempitsky.Domain-adversarialtrainingofneuralnetworks.J.Mach.Learn.Res.,17(1):2096â€“2030,January2016.ISSN1532-4435.\n[23] TaesikGong,YeonsuKim,JinwooShin,andSung-JuLee.Metasense:few-shotadaptationtountrainedconditionsindeepmobilesensing.\nInProceedingsofthe17thConferenceonEmbeddedNetworkedSensorSystems,SenSysâ€™19,page110â€“123,NewYork,NY,USA,2019.\nAssociationforComputingMachinery.ISBN9781450369503.doi:10.1145/3356250.3360020.URLhttps://doi.org/10.1145/3356250.3360020.\n[24] TaesikGong,YewonKim,AdibaOrzikulova,YunxinLiu,SungJuHwang,JinwooShin,andSung-JuLee.Dapper:Label-freeperformance\nestimationafterpersonalizationforheterogeneousmobilesensing.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous\nTechnologies,7(2):1â€“27,2023.\n[25] SachinGoyal,AnanyaKumar,SankalpGarg,ZicoKolter,andAditiRaghunathan. Finetunelikeyoupretrain:Improvedfinetuning\nofzero-shotvisionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages\n19338â€“19347,June2023.\n[26] YujiaoHao,RongZheng,andBoyuWang.Invariantfeaturelearningforsensor-basedhumanactivityrecognition.IEEETransactionson\nMobileComputing,21(11):4013â€“4024,2022.doi:10.1109/TMC.2021.3064252.\n[27] AminUlHaq,JianPingLi,MuhammadHammadMemon,Jalaluddinkhan,AsadMalik,TanvirAhmad,AmjadAli,ShahNazir,IjazAhad,\nandMohammadShahid.Featureselectionbasedonl1-normsupportvectormachineandeffectiverecognitionsystemforparkinsonâ€™s\ndiseaseusingvoicerecordings.IEEEAccess,7:37718â€“37734,2019.doi:10.1109/ACCESS.2019.2906350.\n[28] MdYousufHarun,JhairGallardo,TylerL.Hayes,andChristopherKanan. HowEfficientAreTodayâ€™sContinualLearningAlgorithms?\n.In2023IEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops(CVPRW),pages2431â€“2436,LosAlamitos,CA,\nUSA,June2023.IEEEComputerSociety. doi:10.1109/CVPRW59228.2023.00241. URLhttps://doi.ieeecomputersociety.org/10.1109/\n,Vol.1,No.1,Article.Publicationdate:November2024.\n24 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nCVPRW59228.2023.00241.\n[29] TylerL.Hayes,NathanD.Cahill,andChristopherKanan. Memoryefficientexperiencereplayforstreaminglearning,2019. URL\nhttps://arxiv.org/abs/1809.05922.\n[30] TorstenHoefler,DanAlistarh,TalBen-Nun,NikoliDryden,andAlexandraPeste.Sparsityindeeplearning:Pruningandgrowthfor\nefficientinferenceandtraininginneuralnetworks.J.Mach.Learn.Res.,22(1),jan2021.ISSN1532-4435.\n[31] Jin-HyukHong,JulianRamos,andAnindK.Dey.Towardpersonalizedactivityrecognitionsystemswithasemipopulationapproach.\nIEEETransactionsonHuman-MachineSystems,46(1):101â€“112,2016.doi:10.1109/THMS.2015.2489688.\n[32] AndreaIaboni,SofijaSpasojevic,KristineNewman,LoriSchindelMartin,AngelWang,BingYe,AlexMihailidis,andShehrozSKhan.\nWearablemultimodalsensorsforthedetectionofbehavioralandpsychologicalsymptomsofdementiausingpersonalizedmachine\nlearningmodels.Alzheimerâ€™s&Dementia:Diagnosis,Assessment&DiseaseMonitoring,14(1):e12305,2022.\n[33] WenhuiJi,JingyuZhu,WanxiaWu,NanxiangWang,JiqingWang,JianshengWu,QiongWu,XuewenWang,ChangminYu,Gaofeng\nWei,etal.Wearablesweatbiosensorsrefreshpersonalizedhealth/medicaldiagnostics.Research,2021.\n[34] TianJin,MichaelCarbin,DanielM.Roy,JonathanFrankle,andGintareKarolinaDziugaite.Pruningâ€™seffectongeneralizationthrough\nthelensoftrainingandregularization.InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyunCho,editors,Advancesin\nNeuralInformationProcessingSystems,2022.URLhttps://openreview.net/forum?id=OrcLKV9sKWp.\n[35] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiA.Rusu,KieranMilan,JohnQuan,\nTiagoRamalho,AgnieszkaGrabska-Barwinska,DemisHassabis,ClaudiaClopath,DharshanKumaran,andRaiaHadsell.Overcoming\ncatastrophicforgettinginneuralnetworks.ProceedingsoftheNationalAcademyofSciences,114(13):3521â€“3526,2017.doi:10.1073/pnas.\n1611835114.URLhttps://www.pnas.org/doi/abs/10.1073/pnas.1611835114.\n[36] WouterM.KouwandMarcoLoog.Anintroductiontodomainadaptationandtransferlearning,2019.URLhttps://arxiv.org/abs/1812.\n11806.\n[37] DanielBKowalsky,JohnRRebula,LauroVOjeda,PeterGAdamczyk,andArthurDKuo.Humanwalkingintherealworld:Interactions\nbetweenterraintype,gaitparameters,andenergyexpenditure.PLoSOne,16(1):e0228682,January2021.\n[38] AlexKrizhevskyandGeoffreyHinton.Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport,UniversityofToronto,\nToronto,Ontario,2009.URLhttps://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.\n[39] BishalLamichhane,JoanneZhou,andAkaneSano.Psychoticrelapsepredictioninschizophreniapatientsusingapersonalizedmobile\nsensing-basedsuperviseddeeplearningmodel.IEEEJournalofBiomedicalandHealthInformatics,2023.\n[40] BarbaraALewis,LisaFreebairn,JessicaTag,AllisonACiesla,SudhaKIyengar,CatherineMStein,andHGerryTaylor.Adolescent\noutcomesofchildrenwithearlyspeechsounddisorderswithandwithoutlanguageimpairment.Am.J.Speech.Lang.Pathol.,24(2):\n150â€“163,May2015.\n[41] DaLi,YongxinYang,Yi-ZheSong,andTimothyM.Hospedales.Learningtogeneralize:meta-learningfordomaingeneralization.In\nProceedingsoftheThirty-SecondAAAIConferenceonArtificialIntelligenceandThirtiethInnovativeApplicationsofArtificialIntelligence\nConferenceandEighthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,AAAIâ€™18/IAAIâ€™18/EAAIâ€™18.AAAIPress,2018.\nISBN978-1-57735-800-8.\n[42] JingjingLi,ErpengChen,ZhengmingDing,LeiZhu,KeLu,andHengTaoShen.Maximumdensitydivergencefordomainadaptation.\nIEEETransactionsonPatternAnalysisandMachineIntelligence,43(11):3918â€“3930,2021.doi:10.1109/TPAMI.2020.2991050.\n[43] JianLiang,DapengHu,andJiashiFeng. Dowereallyneedtoaccessthesourcedata?sourcehypothesistransferforunsupervised\ndomainadaptation.InInternationalConferenceonMachineLearning(ICML),pages6028â€“6039,2020.\n[44] HanbingLiu,JinggeWang,XuanZhang,YeGuo,andYangLi. Enhancingcontinuousdomainadaptationwithmulti-pathtransfer\ncurriculum,2024.\n[45] XinLiu,YuntaoWang,SinanXie,XiaoyuZhang,ZixianMa,DanielMcDuff,andShwetakPatel.Mobilephys:Personalizedmobilecamera-\nbasedcontactlessphysiologicalsensing.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,6(1),mar2022.doi:10.1145/3517225.\nURLhttps://doi.org/10.1145/3517225.\n[46] XueLiu,WeijieXia,andZhimiaoFan.Adeepneuralnetworkpruningmethodbasedongradientl1-norm.In2020IEEE6thInternational\nConferenceonComputerandCommunications(ICCC),pages2070â€“2074,2020.doi:10.1109/ICCC51575.2020.9345039.\n[47] MingshengLong,JianminWang,YueCao,JiaguangSun,andPhilipS.Yu.Deeplearningoftransferablerepresentationforscalable\ndomainadaptation.IEEETransactionsonKnowledgeandDataEngineering,28(8):2027â€“2040,2016.doi:10.1109/TKDE.2016.2554549.\n[48] Jian-HaoLuo,JianxinWu,andWeiyaoLin.Thinet:Afilterlevelpruningmethodfordeepneuralnetworkcompression.InICCV,pages\n5058â€“5066,2017.\n[49] BradleyMalin,KennethGoodman,etal.Betweenaccessandprivacy:challengesinsharinghealthdata.Yearbookofmedicalinformatics,\n27(01):055â€“059,2018.\n[50] ArunMallyaandSvetlanaLazebnik. Packnet:Addingmultipletaskstoasinglenetworkbyiterativepruning. In2018IEEE/CVF\nConferenceonComputerVisionandPatternRecognition,pages7765â€“7773,2018.doi:10.1109/CVPR.2018.00810.\n[51] ArunMallya,DillonDavis,andSvetlanaLazebnik.Piggyback:Adaptingasinglenetworktomultipletasksbylearningtomaskweights.\nInVittorioFerrari,MartialHebert,CristianSminchisescu,andYairWeiss,editors,ComputerVisionâ€“ECCV2018,pages72â€“88,Cham,\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 25\n2018.SpringerInternationalPublishing.ISBN978-3-030-01225-0.\n[52] K.Mayank.Bxdprimerseries:Lassoregressionmodels,l1regularizationingeneralandcomparisonwithl2regularization,2023.URL\nhttps://www.linkedin.com/pulse/bxd-primer-series-lasso-regression-models-l1-general-comparison-k-/.\n[53] LakmalMeegahapola,WilliamDroz,PeterKun,AmaliaDeGÃ¶tzen,ChaitanyaNutakki,ShyamDiwakar,SalvadorRuizCorrea,Donglei\nSong,HaoXu,andMiriamBidoglia.Generalizationandpersonalizationofmobilesensing-basedmoodinferencemodels:Ananalysisof\ncollegestudentsineightcountries.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies,6(4):1â€“32,2023.\n[54] SujayNagaraj,SarahGoodday,ThomasHartvigsen,AdrienBoch,KopalGarg,SindhuGowda,LucaFoschini,MarzyehGhassemi,\nStephenFriend,andAnnaGoldenberg.Dissectingtheheterogeneityofâ€œinthewildâ€stressfrommultimodalsensordata.NPJDigital\nMedicine,6(1):237,2023.\n[55] LauraPÃ¤eske,TuuliUudeberg,HiieHinrikus,JaanusLass,andMaieBachmann.Correlationbetweenelectroencephalographicmarkers\ninthehealthybrain.Sci.Rep.,13(1):6307,April2023.\n[56] MichelaPaganiniandJessicaForde.Oniterativeneuralnetworkpruning,reinitialization,andthesimilarityofmasks,2020.\n[57] WonilPark,VictorJ.Lee,ByungmoKu,andHirofumiTanaka. Effectofwalkingspeedandplacementpositioninteractionsin\ndeterminingtheaccuracyofvariousnewerpedometers.JournalofExerciseScience&Fitness,12(1):31â€“37,2014.ISSN1728-869X.doi:\nhttps://doi.org/10.1016/j.jesf.2014.01.003.URLhttps://www.sciencedirect.com/science/article/pii/S1728869X14000057.\n[58] DavidPhelan.Amazonadmitslisteningtoalexaconversations:Whyitmatters.https://shorturl.at/fxN78,2019.\n[59] MichaelPotuck.Howtoresetyourapplewatchfitnesscalibrationformoreaccurateworkoutandactivitydata.https://9to5mac.com/\n2021/08/26/fix-apple-watch-workout-tracking-activity-tracking/,2021.\n[60] A.Prabhu,H.AlKaderHammoud,P.Dokania,P.S.Torr,S.Lim,B.Ghanem,andA.Bibi.Computationallybudgetedcontinuallearning:\nWhatdoesmatter?In2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages3698â€“3707,LosAlamitos,\nCA,USA,jun2023.IEEEComputerSociety. doi:10.1109/CVPR52729.2023.00360. URLhttps://doi.ieeecomputersociety.org/10.1109/\nCVPR52729.2023.00360.\n[61] AmyRathbone,SimoneStumpf,CarolineClaisse,ElizabethSillence,LynneCoventry,RichardDBrown,andAbigailCDurrant.People\nwithlong-termconditionssharingpersonalhealthdataviadigitalhealthtechnologies:Ascopingreviewtoinformdesign.PLOSDigit.\nHealth,2(5):e0000264,May2023.\n[62] BoyuRen,EmmaGBalkind,BriannaPastro,ElanaSIsrael,DiegoAPizzagalli,HabiballahRahimi-Eichi,JustinTBaker,andChristianA\nWebb.Predictingstatesofelevatednegativeaffectinadolescentsfromsmartphonesensors:Anovelpersonalizedmachinelearning\napproach.PsychologicalMedicine,pages1â€“9,2022.\n[63] XavierRobert-Lachaine,HakimMecheri,ChristianLarue,andAndrePlamondon.Effectoflocalmagneticfielddisturbancesoninertial\nmeasurementunitsaccuracy.AppliedErgonomics,63:123â€“132,092017.doi:10.1016/j.apergo.2017.04.011.\n[64] SadiqSani,StewartMassie,NirmalieWiratunga,andKayCooper.Learningdeepandshallowfeaturesforhumanactivityrecognition.\nInGangLi,YongGe,ZiliZhang,ZhiJin,andMichaelBlumenstein,editors,KnowledgeScience,EngineeringandManagement,pages\n469â€“482,Cham,2017.SpringerInternationalPublishing.ISBN978-3-319-63558-3.\n[65] PhilipSchmidt,AttilaReiss,RobertDuerichen,ClausMarberger,andKristofVanLaerhoven.Introducingwesad,amultimodaldataset\nforwearablestressandaffectdetection.InProceedingsofthe20thACMInternationalConferenceonMultimodalInteraction,ICMIâ€™18,\npage400â€“408,NewYork,NY,USA,2018.AssociationforComputingMachinery.ISBN9781450356923.doi:10.1145/3242969.3242985.\nURLhttps://doi.org/10.1145/3242969.3242985.\n[66] FlorianSchroff,DmitryKalenichenko,andJamesPhilbin.Facenet:Aunifiedembeddingforfacerecognitionandclustering.2015IEEE\nConferenceonComputerVisionandPatternRecognition(CVPR),Jun2015.doi:10.1109/cvpr.2015.7298682.URLhttp://dx.doi.org/10.1109/\nCVPR.2015.7298682.\n[67] JulianeRSempionatto,VictorRuiz-ValdepenasMontiel,EvaVargas,HazhirTeymourian,andJosephWang. Wearableandmobile\nsensorsforpersonalizednutrition.ACSsensors,6(5):1745â€“1760,2021.\n[68] ShivShankar,VihariPiratla,SoumenChakrabarti,SiddharthaChaudhuri,PreethiJyothi,andSunitaSarawagi. Generalizingacross\ndomainsviacross-gradienttraining.InICLR(Poster).OpenReview.net,2018.URLhttp://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#\nShankarPCCJS18.\n[69] QiangShen,HaotianFeng,RuiSong,StefanoTeso,FaustoGiunchiglia,andHaoXu.Federatedmulti-taskattentionforcross-individual\nhumanactivityrecognition.InLudDeRaedt,editor,ProceedingsoftheThirty-FirstInternationalJointConferenceonArtificialIntelligence,\nIJCAI-22,pages3423â€“3429.InternationalJointConferencesonArtificialIntelligenceOrganization,72022.doi:10.24963/ijcai.2022/475.\nURLhttps://doi.org/10.24963/ijcai.2022/475.MainTrack.\n[70] YugeShi,JeffreySeely,PhilipH.S.Torr,N.Siddharth,AwniHannun,NicolasUsunier,andGabrielSynnaeve.Gradientmatchingfor\ndomaingeneralization.2021.\n[71] LeslieNSmith.Cyclicallearningratesfortrainingneuralnetworks.In2017IEEEwinterconferenceonapplicationsofcomputervision\n(WACV),pages464â€“472.IEEE,2017.\n[72] GabrielaMStegmann,ShiraHahn,JulieLiss,JeremyShefner,SewardRutkove,KerisaShelton,CaylaJessicaDuncan,andVisarBerisha.\nEarlydetectionandtrackingofbulbarchangesinALSviafrequentandremotespeechanalysis.NPJDigit.Med.,3(1):132,October2020.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n26 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n[73] AllanStisen,HenrikBlunck,SouravBhattacharya,ThorSiigerPrentow,MikkelBaunKjÃ¦rgaard,AnindDey,TobiasSonne,and\nMadsMÃ¸llerJensen.Smartdevicesaredifferent:Assessingandmitigatingmobilesensingheterogeneitiesforactivityrecognition.In\nProceedingsofthe13thACMconferenceonembeddednetworkedsensorsystems,pages127â€“140,2015.\n[74] C.Tang,L.Qendro,D.Spathis,F.Kawsar,C.Mascolo,andA.Mathur.Kaizen:Practicalself-supervisedcontinuallearningwithcontinual\nfine-tuning.In2024IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV),pages2829â€“2838,LosAlamitos,CA,USA,\njan2024.IEEEComputerSociety.doi:10.1109/WACV57701.2024.00282.URLhttps://doi.ieeecomputersociety.org/10.1109/WACV57701.\n2024.00282.\n[75] ChiIanTang,LorenaQendro,DimitrisSpathis,FahimKawsar,AkhilMathur,andCeciliaMascolo.Balancingcontinuallearningand\nfine-tuningforhumanactivityrecognition.ArXiv,abs/2401.02255,2024.URLhttps://api.semanticscholar.org/CorpusID:266755926.\n[76] RohanTaori,AchalDave,VaishaalShankar,NicholasCarlini,BenjaminRecht,andLudwigSchmidt.Measuringrobustnesstonatural\ndistributionshiftsinimageclassification.InProceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,\nNIPSâ€™20,RedHook,NY,USA,2020.CurranAssociatesInc.ISBN9781713829546.\n[77] SiriTeam.Heysiri:Anon-devicednn-poweredvoicetriggerforappleâ€™spersonalassistant.https://machinelearning.apple.com/research/\nhey-siri,2017.\n[78] YunusEmreUstev,OzlemDurmazIncel,andCemErsoy. User,deviceandorientationindependenthumanactivityrecognitionon\nmobilephones:Challengesandaproposal.InProceedingsofthe2013ACMconferenceonPervasiveandubiquitouscomputingadjunct\npublication,pages1427â€“1436,2013.\n[79] YonatanVaizman,KatherineEllis,andGertLanckriet. Recognizingdetailedhumancontextinthewildfromsmartphonesand\nsmartwatches.IEEEPervasiveComputing,16(4):62â€“74,2017.doi:10.1109/MPRV.2017.3971131.\n[80] GidoM.vandeVenandAndreasS.Tolias.Generativereplaywithfeedbackconnectionsasageneralstrategyforcontinuallearning,\n2019.URLhttps://arxiv.org/abs/1809.10635.\n[81] GidoMvandeVen,HavaTSiegelmann,andAndreasSTolias. Brain-inspiredreplayforcontinuallearningwithartificialneural\nnetworks.Nat.Commun.,11(1):4069,August2020.\n[82] C.VarmaandPujaPrasad.Supervisedandunsupervisedmachinelearningapproachesâ€”asurvey,022023.\n[83] TanviVerma,LiyuanJin,JunZhou,JiaHuang,MingruiTan,BenjaminChenMingChoong,TingFangTan,FeiGao,XinxingXu,DanielS.\nTing,andYongLiu.Privacy-preservingcontinuallearningmethodsformedicalimageclassification:acomparativeanalysis.Frontiersin\nMedicine,10,2023.ISSN2296-858X.doi:10.3389/fmed.2023.1227515.URLhttps://www.frontiersin.org/journals/medicine/articles/10.\n3389/fmed.2023.1227515.\n[84] RiccardoVolpi,HongseokNamkoong,OzanSener,JohnDuchi,VittorioMurino,andSilvioSavarese.Generalizingtounseendomains\nviaadversarialdataaugmentation.InProceedingsofthe32ndInternationalConferenceonNeuralInformationProcessingSystems,NIPSâ€™18,\npage5339â€“5349,RedHook,NY,USA,2018.CurranAssociatesInc.\n[85] ChanWang,TianyiyiHe,HongZhou,ZixuanZhang,andChengkuoLee.Artificialintelligenceenhancedsensors-enablingtechnologies\ntonext-generationhealthcareandbiomedicalplatform.Bioelectron.Med.,9(1):17,August2023.\n[86] JindongWang,CuilingLan,ChangLiu,YidongOuyang,TaoQin,WangLu,YiqiangChen,WenjunZeng,andPhilipS.Yu.Generalizing\ntounseendomains:Asurveyondomaingeneralization.IEEETransactionsonKnowledgeandDataEngineering,35(8):8052â€“8072,2023.\ndoi:10.1109/TKDE.2022.3178128.\n[87] QinWang,OlgaFink,LucVanGool,andDengxinDai.Continualtest-timedomainadaptation,2022.\n[88] QinWang,OlgaFink,LucVanGool,andDengxinDai.Continualtest-timedomainadaptation.InProceedingsofConferenceonComputer\nVisionandPatternRecognition,2022.\n[89] ZhiguangWang,WeizhongYan,andTimOates.Timeseriesclassificationfromscratchwithdeepneuralnetworks:Astrongbaseline.\nIn2017Internationaljointconferenceonneuralnetworks(IJCNN),pages1578â€“1585.IEEE,2017.\n[90] YananWu,ZhixiangChi,YangWang,KonstantinosN.Plataniotis,andSongheFeng. Test-timedomainadaptationbylearning\ndomain-awarebatchnormalization,2024.\n[91] YiXiao,HarshitSharma,ZhongyangZhang,DessaBergen-Cico,TauhidurRahman,andAsifSalekin. Readingbetweentheheat:\nCo-teachingbodythermalsignaturesfornon-intrusivestressdetection.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,7(4),jan\n2024.doi:10.1145/3631441.URLhttps://doi.org/10.1145/3631441.\n[92] JianfeiYang,XinyanChen,DazhuoWang,HanZou,ChrisXiaoxuanLu,SumeiSun,andLihuaXie.Sensefi:Alibraryandbenchmark\nondeep-learning-empoweredwifihumansensing,2023.\n[93] ShuochaoYao,ShaohanHu,YiranZhao,AstonZhang,andTarekAbdelzaher.Deepsense:Aunifieddeeplearningframeworkfortime-\nseriesmobilesensingdataprocessing.InProceedingsofthe26thInternationalConferenceonWorldWideWeb,WWWâ€™17,page351â€“360,\nRepublicandCantonofGeneva,CHE,2017.InternationalWorldWideWebConferencesSteeringCommittee.ISBN9781450349130.doi:\n10.1145/3038912.3052577.URLhttps://doi.org/10.1145/3038912.3052577.\n[94] Y.Zhang,Y.Zheng,K.Qian,G.Zhang,Y.Liu,C.Wu,andZ.Yang.Widar3.0:Zero-effortcross-domaingesturerecognitionwithwi-fi.IEEE\nTransactionsonPatternAnalysisandMachineIntelligence,44(11):8671â€“8688,nov2022.ISSN1939-3539.doi:10.1109/TPAMI.2021.3105387.\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 27\n[95] KaiyangZhou,YongxinYang,TimothyHospedales,andTaoXiang.Deepdomain-adversarialimagegenerationfordomaingeneralisation.\nProceedingsoftheAAAIConferenceonArtificialIntelligence,34:13025â€“13032,042020.doi:10.1609/aaai.v34i07.7003.\n[96] MichaelZhuandSuyogGupta.Toprune,ornottoprune:exploringtheefficacyofpruningformodelcompression,2017.\n[97] TaoZhuang,ZhixuanZhang,YuhengHuang,XiaoyiZeng,KaiShuang,andXiangLi.Neuron-levelstructuredpruningusingpolarization\nregularizer. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessing\nSystems,volume33,pages9865â€“9877.CurranAssociates,Inc.,2020.URLhttps://proceedings.neurips.cc/paper_files/paper/2020/file/\n703957b6dd9e3a7980e040bee50ded65-Paper.pdf.\nA ENSURINGREPRODUCIBILITY\nDependingonthedistributionofthedata,differentaccuracymeasureshavebeenusedintheliteraturesuch\nasbalancedaccuracy,standardaccuracy,orF1score.Toensureconsistencywiththeoriginalbaselinepapers\nforeachdataset[7,91],wefollowtheirevaluationmetric.Detailedexplanationsandjustificationsareprovided\ninAppendixA.1.Toensurereproducibility,weprovidethehyperparametersusedinboththegeneralmodel\ntrainingphaseandthepersonalizationphaseinAppendixA.2.Thisincludeslearningrate,alpha,ğœ,epochcount,\nandothersettingsspecifictoeachdataset.AdditionallyAppendixA.3andAppendixA.4providedetailsofthe\ncodeandthecomputeresources,respectively.\nA.1 Metricsforclassificationaccuracyevaluation\nWeuseaccuracytomeasuretheperformanceofamodel.However,thecomputationofthismetricdiffersforthe\nfourdatasets.Thedetailsofthemetricsusedforallthedatasetsareasfollows:\n(1) PERCEPT-R:Forthisdataset,Benwayetal.[7]utilizedbalancedaccuracyforthebinaryclassificationtask,\nandweemployedthesamemetricsinourstudy.\n(2) WIDAR:Weusea6-classclassificationforgesturerecognition,andthedistributionofthedataamong\ntheseclassesisnearlybalanced.Thus,standardclassificationaccuracyhasbeenusedforWIDAR.\n(3) ExtraSensory:ThesubsetoftheExtrasensorydatasetusedforthisworkaimsforabinaryclassificationfor\nactivityrecognition.Weobservedthatthedatadistributionwasquiteimbalancedamongthetwoclasses,\nandtherefore,balancedclassificationaccuracywasusedforthisdataset.Balancedaccuracyiscomputedas\ntheaverageoftruepositiverateandtruenegativerate.\n(4) StressSensingDataset:Forthisbinaryclassificationproblem,F1scorehasbeenusedasaperformance\nmetricassuggestedbytheoriginalauthors[91].\nForsimplicity,weusethetermâ€˜accuracyâ€™toencompassallthemetricsdiscussedabove.\nA.2 Hyperparameters\nTheapproachusesseveralhyperparametersforgenericmodeltrainingandpersonalization.Table7and8show\nthehyperparametervaluesforgenericandpersonalizedmodeltraining,respectively.Thesevaluescorrespondto\nthebestresultsobtainedforthedatabelongingtotheavailablecontextusingagridsearch.Fortrainingthegeneric\nmodel,inadditiontothenumberofepochs,â€˜BaseLearningRateâ€™andâ€˜MaxLearningRateâ€™(theargumentsfor\nCycleLR[71])arethehyperparameters.Forthepersonalizedmodel,learningrate(fixed),ğ›¼,ğœ,numberofepochs\nforinitialfinetuning(InitialEpochs),andepochsforfinalfinetuning(FinalEpochs)arethehyperparameters.The\nrangeofthesehyperparametersusedforgridsearchduringpersonalizationisalsomentionedinTable8.\nAdditionally,weuseğ‘˜ =ğ‘˜â€² =0.05fortheToleratedPrunemoduleforPERCEPT-R,WIDAR,andExtraSensory\ndatasets,whilefortheStress-sensingdataset,ğ‘˜ =0.05andğ‘˜â€² =0.01isbeingused.Onemayfinddifferentvalues\ntobesuitableforotherdatasetsandmodelarchitectures.\n,Vol.1,No.1,Article.Publicationdate:November2024.\n28 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\nHyperparameter PERCEPT-R WIDAR ExtraSensory Stress-sensing\nBaseLearningRate 1e-5 1e-07 1.2e-08 5e-5\nMaxLearningRate 1e-5 5e-06 7.5e-07 5e-5\nEpochs 300 1000 150 1000\nTable7. HyperparametersforgenericModels\nHyperparameter Range PERCEPT-R WIDAR ExtraSensory StressSensing\nLearningRate 1e-6-1e-1 1e-5 1e-6 1e-6 1e-5\nğ‘ğ‘™ğ‘â„ğ‘ 1e-6-10 0.01 0.0001 0.5 0.0001\nğœ 0.01-0.25 0.05 0.2 0.01 0.01\nInitialEpochs 100-1000 300 600 600 1000\nFinalEpochs 100-1000 300 600 1000 1000\nTable8. HyperparametersforPersonalizedModels\nA.3 Code\nThecodeisprovidedatanonymouslinkarrangedintodataset-specificfolders.Eachfoldercontainsthepre-trained\ngeneric model, all the required modules, and the instructions to run the code. The seed values used for the\nevaluationsarealsoprovidedintheshellfiles.Thedatapartitionedintopersonalizedandcontext-wisesetswill\nbereleaseduponpublication.\nA.4 ComputeResources\nAllthecomputationshavebeenperformedonNVIDIAQuadroRTX5000with48RTCoresand16GBGDDR6\nmemory.\nB DETAILEDUSER-SPECIFICRESULTS\nThissectiondiscussestheuser-specificpatterns.AppendixB.1discussesdetailedpersonalization(Î” ğ‘ƒ)results\nwhileAppendixB.2discussesgeneralization(Î” ğº)results.Further,AppendixB.3showsperson-wisestandard\ndeviationvaluesforgenericMğº ,conventionallyfinetunedMğ¶ ğ‘–ğ‘ andCRoPMğ‘ƒ ğ‘–ğ‘\nmodels.\nğœƒ ğœƒ ğœƒ\nB.1 DetaileddiscussionofÎ” results\nğ‘ƒ\nThepersonalizedmodelsobtainedusingCRoPexhibithigherclassificationaccuracythanthegenericmodelson\ntheavailablecontextâ€™sdataDğ‘\n,showcasingthebenefitsofpersonalization.Todemonstratetheexistenceofsuch\nğ‘–\nimprovement,Tables9a-9handTable11acomparetheperformanceofgenericmodelMğº\nandpersonalized\nğœƒ\nğ‘ƒğ‘\nmodelsobtainedusingCRoPM ğ‘– .\nğœƒ\nWIDAR:. Tables9aand9bshowthatthereisanaverageimprovementof25.25and11.88percentpointsamong\nthreeusersfortheavailablecontextCğ‘ forScenario1andScenario2,respectively.However,thisbenefitcomes\natthecostofareductioninaccuracyfortheunseencontext.Thereisanaveragereductionof16.69and5.97\npercentpointsforScenario1andScenario2,respectively,fortheunseencontextCğ‘¢.Notably,thelossofaccuracy\nintheunseencontextismuchlowerascomparedtotheconventionally-finetunemodelasdiscussedintheSection\n3(Motivation).\nExtraSensory: SimilarpatternscouldbeobservedfortheExtrasensorydataset.Tables9cand9dshowthat\nthereisanaverageincrementof16.40and18.37percentpointsfortheavailablecontextforScenario1and\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 29\nScenario 2, respectively. Interestingly, the performance of the personalized model for Scenario 1 on unseen\ncontextCğ‘¢ wasnotadverselyimpacted.ThisisattributedtothefactthattheinertialsensingpatternsofBag\nandPocketphonecarryingmodescapturetheuserâ€™sbodymovement,whereasthephone-in-handmovement\npatternscanbedistinct.InScenario1,Cğ‘ comprisespocketandCğ‘¢ comprisesbag,meaningbothavailableand\nunseencontextsencompasssimilarinertialpatterns,leadingtoadvantageousperformanceevenintheunseen\ncontext.Thisevaluationillustratesminimalintra-usergeneralizabilitylossonunseencontextswhenbothavailable\nandunseencontextssharesimilarusertraits.However,inScenario2,whereonlythehandbelongstotheunseen\ncontextCğ‘¢,thereisanaveragelossof5.02percentagepointsontheunseencontext.\nStressSensing: Thephysiologicalfeaturesusedinthisdatasetvarysignificantlyfromoneusertoother.Thus,\nTables9e-9hshowthatthegenericmodelsdonotperformwellonpersonalizeddata.Personalizedfinetuning\nenablesthemodeltolearnperson-specificpatterns,allowingthemodelâ€™sperformancetoimprovenotonlyinthe\navailablecontextbutalsointheunseencontext.Thisresultsinaveragepersonalizationbenefit(Î” ğ‘ƒ)of67.81and\n85.25forScenario1andScenario2,respectively.ItisimportanttonotethatforeachScenario,onlyonemodelis\ntrainedfortheavailablecontextandtestedfortwodifferentunseencontexts.Moreover,doublecontextchange\n(Tables9gand9h)showslowerpersonalizationbenefitascomparedtosinglecontextchange(Tables9eand9f).\nPERCEPT-R:. Inthisdataset,theheterogeneityoffeaturesamongindividualsisreflectedthroughthedifference\ninpredictionaccuracyofthegenericmodel.ItcanbeobservedinTable11athatforsomeindividuals,thegeneric\nmodelexhibitsover90%accuracyontheavailablecontextdata,whileforothers,thegenericmodelâ€™saccuracy\ndropstoaround60%.Thisresultsinsignificantvariabilityovergainsinavailableandunseencontexts.Overall,\nCRoPyieldsanaveragepersonalizationgainof5.09%.\nOnaverageoverallthedatasets,apersonalizationbenefit(Î” ğ‘ƒ)of35.23percentpointsareseenascompared\ntothegenericmodelsacrossthefourdatasetsunderbothscenarios.\nTheseevaluationsestablishthatthepersonalizedmodelsobtainedusingCRoPdemonstrateimprovedperfor-\nmanceovertheavailablecontextdatathanthegenericmodelsandexhibitpersonalization.\nB.2 DetaileddiscussionofÎ” results\nğº\nğ‘ƒğ‘\nThepersonalizedmodelsobtainedusingCRoP(M ğ‘– )areexpectedtohavehigheraccuracyonunseencontext\nğœƒ\nğ¶ğ‘\nCğ‘¢ thantheconventionally-finetunepersonalizedmodels(M ğœƒğ‘–)asdiscussedinSection3.Thissectionassesses\nwhethertheresultsalignwiththeseexpectations.\nğ‘ƒğ‘\nWIDAR:. Tables10aand10bdemonstratethatthepersonalizedmodelsM ğ‘– exhibitanaverageincrement\nğœƒ\nof8.01and2.85percentpointsintheunseencontextforScenario1andScenario2,respectively.However,an\naveragelossof1.57andanaveragegainof1.44percentpointsinC ğ‘â€²ğ‘  accuracycouldbeobservedforScenario1\nandScenario2,respectively.\nExtrasensory: SimilarpatternscouldbeobservedfortheExtraSensorydatasetwheretheaverageaccuracy\nontheunseencontextimprovedby4.97and12.61percentagepointsforScenario1andScenario2asshown\ninTables9cand10d,respectively.Asexpected,thereisalossof5.43and4.44percentpointsintheavailable\ncontextsforScenario1andScenario2,respectively.\nStressSensing: AsobservedinTables9e-9h,personalizedfinetuningimprovesmodelsperformanceonunseen\ncontextaswell,wecanclaimthatthereissomeperson-specifictraitswhicharecommoninavailableandunseen\ncontext.Whilecomparingourfinalmodelswithconventionally-finetuned models(Tables10e-10h),performance\nboost in both available and unseen context could be observed. This can be attributed to the generalization\nimprovementbenefitsofmodelpruning[34].Thisresultsinaveragegeneralizationbenefit(Î” ğº)of13.81and\n,Vol.1,No.1,Article.Publicationdate:November2024.\n30 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n(a)Scenario1forWIDARdataset (b)Scenario2forWIDARdataset\nModel Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C) Model Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n0 63.90 77.09 83.67 69.53 +19.77 -7.56 0 73.28 61.80 82.59 58.38 +9.31 -2.43\n1 61.80 79.78 86.41 54.45 +24.61 -25.33 1 73.18 59.58 92.44 47.90 +19.27 -11.67\n2 45.63 79.81 77.02 62.63 +31.38 -17.18 2 80.45 46.13 87.5 42.31 +7.04 -3.81\nAverage +25.25 -16.69 Average +11.88 -5.97\nÎ” ğ‘ƒ +8.55 Î” ğ‘ƒ +5.90\n(c)Scenario1forExtraSensorydataset (d)Scenario2forExtraSensorydataset\nModel Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C) Model Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n61 78.69 69.83 82.59 69.66 +3.9 -0.17 61 76.43 80.00 87.24 73.44 +10.81 -6.56\n7C 78.91 76.41 88.00 71.63 +9.09 -4.78 7C 75.07 92.32 83.18 89.39 +8.11 -2.93\n80 55.84 26.24 82.36 38.87 +26.52 +12.63 80 54.40 88.77 84.49 81.12 +30.09 -7.65\n9D 73.74 85.63 82.81 84.72 +9.07 -0.91 9D 75.58 75.02 82.58 74.65 +7.00 -0.37\nB7 56.06 88.33 89.50 86.97 +33.44 -1.36 B7 59.73 84.58 95.56 77.01 +35.83 -7.57\nAverage +16.40 +1.08 Average +18.37 -5.02\nÎ” ğ‘ƒ +17.49 Î” ğ‘ƒ +13.35\n(e)Scenario1forStressSensing-singlecontextchange (f)Scenario2forStressSensing-singlecontextchange\nModel Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C) Model Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 88.39 81.90 94.54 97.59 +6.15 +15.69 1 66.54 64.71 92.38 94.54 +25.84 +29.83\n2 47.40 50.0 77.12 90.47 +29.72 +40.47 2 69.10 50.70 85.26 89.65 +16.16 +38.95\n3 36.90 43.48 96.36 95.31 +59.46 +51.93 3 4.76 11.94 74.40 87.28 +69.64 +75.34\nAverage +31.78 +36.03 Average +37.21 +48.04\nÎ” ğ‘ƒ +67.81 Î” ğ‘ƒ +85.25\n(g)Scenario1forStressSensing-doublecontextchange (h)Scenario2forStressSensing-doublecontextchange\nModel Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C) Model Mğº Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğº,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 88.39 64.71 94.54 76.46 +6.15 +11.75 1 66.54 81.90 92.38 91.07 +25.84 +9.17\n2 47.40 50.70 77.12 63.22 +29.72 +12.52 2 69.10 50.00 85.26 62.84 +16.16 +12.84\n3 36.90 11.94 96.36 55.48 +59.46 +43.54 3 4.76 43.47 74.40 87.45 +69.64 +43.98\nAverage +31.78 +22.60 Average +37.21 +22.00\nÎ” ğ‘ƒ +54.38 Î” ğ‘ƒ +59.21\nTable9. DetailedPersonalization(Î” )resultsforWIDAR,ExtraSensoryandStressSensingdataset\nğ‘ƒ\n13.08forScenario1andScenario2,respectively,forsinglecontextchange.Similarpersonalizationbenefitscould\nbeseenfordoublecontextchange.\nPERCEPT-R:. AsobservedinTable11b,thevariabilityingeneralizationbenefitsamongdifferentindividualsis\nlesspronouncedascomparedtopersonalizationbenefits.Onaverage,CRoPintroducesageneralizationbenefit\nof2.57%.\nOn average over all the datasets, a generalization benefit (Î” ğº) of 7.78% percent points are seen over the\nconventionally-finetuned personalizedmodelsacrossalldatasetsunderbothscenarios.\nB.3 ErrorBars\nTables12a-12ishowsperson-wisestandarddeviationvaluesforgenericMğº ,conventionallyfinetunedMğ¶ ğ‘–ğ‘\nğœƒ ğœƒ\nğ‘ƒğ‘\nandCRoPM ğ‘– models.\nğœƒ\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 31\n(a)Scenario1forWIDARdataset (b)Scenario2forWIDARdataset\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n0 87.06 65.02 83.67 69.53 -3.38 +4.5 0 77.30 57.46 82.59 58.38 +5.29 +0.92\n1 89.38 44.38 86.41 54.45 -2.97 +10.07 1 93.75 42.38 92.45 47.90 -1.30 +5.51\n2 75.39 53.19 77.02 62.63 +1.63 +9.44 2 87.15 40.19 87.5 42.31 +0.34 +2.13\nAverage -1.57 +8.01 Average +1.44 +2.85\nÎ”ğº +6.43 Î”ğº +4.30\n(c)Scenario1forExtraSensorydataset (d)Scenario2forExtraSensorydataset\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n61 88.99 68.09 82.59 69.66 -6.40 +1.57 61 93.90 64.27 87.24 73.47 -6.66 +9.17\n7C 92.58 61.74 88.0 71.63 -4.58 +9.89 7C 89.19 57.13 83.13 89.39 -6.01 +32.26\n80 86.51 49.82 82.36 38.87 -4.14 -10.95 80 89.34 70.23 84.49 81.12 -4.85 +10.89\n9D 88.89 83.14 82.81 84.73 -6.07 +1.58 9D 85.53 72.95 82.58 74.65 -2.95 +1.7\nB7 95.44 64.19 89.50 86.97 -5.94 +22.78 B7 97.30 67.99 95.56 77.01 -1.74 +9.02\nAverage -5.43 +4.97 Average -4.44 +12.61\nÎ”ğº -0.46 Î”ğº +8.17\n(e)Scenario1forStressSensing-singlecontextchange (f)Scenario2forStressSensing-singlecontextchange\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nModel\nMğ¶ğ‘–ğ‘ Mğ‘ƒğ‘–ğ‘ A(Mğ‘ƒğ‘–ğ‘,C)âˆ’A(Mğ¶ğ‘–ğ‘,C)\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 91.17 92.15 94.54 97.59 +3.37 +5.44 1 92.37 96.46 92.38 94.54 +0.01 -1.93\n2 68.93 82.81 77.12 90.48 +8.19 +7.67 2 75.57 72.47 85.26 89.65 +9.69 +17.18\n3 84.78 90.13 96.36 95.31 +11.58 +5.18 3 64.86 82.52 74.40 87.28 +9.54 +4.76\nAverage +7.71 +6.10 Average +6.41 +6.67\nÎ”ğº +13.81 Î”ğº +13.08\n(g)Scenario1forStressSensing-doublecontextchange (h)Scenario2forStressSensing-doublecontextchange\nModel M\nğœƒğ¶ğ‘–ğ‘\nM\nğœƒğ‘ƒğ‘–ğ‘\nA(M\nğœƒğ‘ƒğ‘–ğ‘,C)âˆ’A(M ğœƒğ¶ğ‘–ğ‘,C)\nModel M\nğœƒğ¶ğ‘–ğ‘\nM\nğœƒğ‘ƒğ‘–ğ‘\nA(M\nğœƒğ‘ƒğ‘–ğ‘,C)âˆ’A(M ğœƒğ¶ğ‘–ğ‘,C)\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 91.17 72.10 94.544 76.46 +3.37 +4.36 1 92.37 87.22 92.38 91.07 +0.01 +3.85\n2 68.93 64.13 77.12 63.22 +8.19 -0.91 2 75.57 59.41 85.26 62.84 +9.69 +3.43\n3 84.78 46.06 96.36 55.48 +11.58 +9.42 3 64.86 83.49 74.40 87.45 +9.54 +3.75\nAverage +7.71 +4.29 Average +6.41 +3.75\nÎ”ğº +12.00 Î”ğº +10.16\nTable10. DetailedGeneralization(Î” )resultsforWIDARExtraSensoryandStressSensingdatasets\nğº\n,Vol.1,No.1,Article.Publicationdate:November2024.\n32 â€¢ SawinderKaur1,AveryGump2,JingyuXin1,YiXiao4,HarshitSharma4,NinaRBenway3,JonathanLPreston1,AsifSalekin4\n1SyracuseUniversity 2UniversityofWisconsin-Madison 3UniversityofMaryland-CollegePark 4ArizonaStateUniversity\n(a)PersonalizationforPERCEPT-R (b)GeneralizationforPERCEPT-R\nModel M ğœƒğº M ğœƒğ‘ƒğ‘–ğ‘ A(M ğœƒğ‘ƒğ‘–ğ‘,C)âˆ’A(M ğœƒğº,C) Model M ğœƒğ¶ğ‘–ğ‘ M ğœƒğ‘ƒğ‘–ğ‘ A(M ğœƒğ‘ƒğ‘–ğ‘,C)âˆ’A(M ğœƒğ¶ğ‘–ğ‘,C)\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n17 72.72 60.51 73.30 63.19 +0.58 +2.68 17 70.56 60.17 73.30 63.19 +2.74 +3.01\n25 96.79 87.16 96.30 86.83 -0.49 -0.33 25 94.91 83.8 96.30 86.83 +1.39 +3.03\n28 55.22 54.88 67.12 60.17 +11.90 +5.29 28 64.81 58.84 67.12 60.17 +2.31 +1.33\n336 100 82.03 100 66.69 0 -15.34 336 100 60.79 100 66.69 +0 +5.90\n344 77.54 67.48 81.9 65.36 +4.36 -2.12 344 82.77 64.68 81.9 65.36 -0.88 +0.68\n361 63.63 89.93 64.28 81.73 +0.64 -8.2 361 57.61 78.66 64.28 81.73 +6.67 +3.07\n362 59.03 66.23 78.52 82.51 +19.49 +16.28 362 75.44 82.9 78.52 82.51 +3.08 -0.39\n55 95.77 85.34 97.14 80.41 +1.38 -4.93 55 100 76.86 97.14 80.41 -2.86 +3.55\n586 65.85 58.25 73.17 65.87 +7.32 +7.62 586 70.94 64.46 73.17 65.87 +2.23 +1.41\n587 64.71 65.1 69.4 65.19 +4.68 +0.09 587 70.84 66.23 69.4 65.19 -1.44 -1.04\n589 66.34 60.87 63.69 62.77 -2.64 +1.90 589 67.26 60.59 63.69 62.77 -3.57 +2.18\n590 69.05 61.04 71.08 73.26 +2.03 +12.22 590 67.76 70.54 71.08 73.26 +3.32 +2.73\n591 61.91 58.68 72.03 63.44 +10.12 +4.76 591 71.83 64.2 72.03 63.44 +0.20 -0.76\n61 72.86 69.42 77.78 66.66 +4.92 -2.76 61 74.93 65.23 77.78 66.66 +2.86 +1.43\n67 80.12 77.64 81.00 72.48 +0.89 -5.15 67 79.83 74.16 81.00 72.48 +1.18 -1.68\n80 89.38 85.54 91.22 87.87 +1.85 +2.33 80 89.31 90.33 91.22 87.87 +1.92 -1.68\nAverage +4.19 +0.90 Average +1.20 +1.37\nÎ” ğ‘ƒ +5.09 Î”ğº +2.57\nTable11. DetailedPersonalization(Î” )andGeneralization(Î” )resultsforPERCEPT-Rdatasets\nğ‘ƒ ğº\n,Vol.1,No.1,Article.Publicationdate:November2024.\nCRoP:Context-wiseRobustStaticHuman-SensingPersonalization â€¢ 33\n(a)Scenario1forWIDARdataset (b)Scenario2forWIDARdataset\nModel Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘ Model Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n0 2.17 0.88 3.69 0.49 2.09 0.29 0 2.67 1.60 1.83 0.95 1.02 0.41\n1 1.49 1.73 1.94 2.97 0.70 1.94 1 1.58 0.64 2.29 0.97 2.22 0.68\n2 3.61 0.68 4.28 5.50 3.8 2.31 2 2.49 0.19 0.79 0.56 1.72 0.39\n(c)Scenario1forExtraSensorydataset (d)Scenario2forExtraSensorydataset\nModel Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘ Model Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n61 2.63 0 4.14 1.65 3.28 2.52 61 3.93 0 4.03 2.67 4.72 1.00\n7C 1.45 0 1.17 0.81 1.36 0.83 7C 2.45 0 2.35 2.50 2.85 0.45\n80 0.42 0 4.17 3.04 2.81 1.44 80 3.33 0 0.65 3.09 1.30 1.01\n9D 0.30 0 2.58 1.08 2.53 0.94 9D 4.18 0 3.22 0.88 5.54 0.80\nB7 0.74 0 3.02 7.28 2.13 3.48 B7 3.13 0 0.83 3.27 0.24 2.79\n(e)Scenario1forStressSensing-singlecon- (f)Scenario2forStressSensing-singlecon-\ntextchange textchange\nModel Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘ Model Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 3.55 0 5.78 4.53 7.94 6.17 1 4.95 0 3.02 0.66 3.02 2.34\n2 5.13 0 14.65 3.71 8.74 10.49 2 13.68 0 2.67 7.82 11.51 5.58\n3 17.61 0 8.17 4.79 15.95 3.45 3 8.25 0 21.02 7.00 17.89 6.57\n(g)Scenario1forStressSensing-doublecon- (h)Scenario2forStressSensing-doublecon-\ntextchange textchange\nModel Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘ Model Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘\nğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ User Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n1 3.55 0 5.75 3.33 7.94 4.78 1 4.95 0 3.02 3.00 3.02 2.52\n2 5.13 0 14.66 10.19 8.74 8.70 2 13.68 0 2.67 5.62 11.51 0.60\n3 17.61 0 8.17 5.52 15.95 3.58 3 8.25 0 21.02 2.23 17.89 2.07\n(i)PERCEPT-R\nModel Mğº Mğ‘ƒ ğ‘–ğ‘ Mğ¶ ğ‘–ğ‘\nğœƒ ğœƒ ğœƒ\nUser Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢ Cğ‘ Cğ‘¢\n17 7.17 0 9.75 1.64 4.18 1.11\n25 1.34 0 2.35 2.20 2.29 0.85\n28 3.38 0 15.16 0.85 15.14 0.60\n336 0 0 0 7.25 0 5.64\n344 5.50 0 4.86 1.13 4.10 1.87\n361 15.75 0 13.18 3.96 12.44 6.39\n362 3.10 0 7.59 4.46 4.53 4.83\n55 3.75 0 0 3.32 2.58 2.77\n586 4.01 0 1.09 1.77 3.26 0.22\n587 3.06 0 4.24 1.20 5.48 1.43\n589 0.45 0 2.33 2.72 1.44 0.94\n590 2.26 0 3.20 2.17 5.58 0.57\n591 3.36 0 3.11 1.11 3.91 0.74\n61 9.30 0 6.33 2.37 5.05 1.67\n67 5.50 0 3.59 2.20 3.11 2.38\n80 6.11 0 4.29 1.61 5.69 0.68\nTable12. StandardDeviationforGeneric,conventionallyfinetuneddandCRoPmodelsforWIDAR,ExtraSensoryandStress\nSensingdataset\n,Vol.1,No.1,Article.Publicationdate:November2024.",
    "pdf_filename": "CRoP_Context-wise_Robust_Static_Human-Sensing_Personalization.pdf"
}