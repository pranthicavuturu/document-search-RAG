{
    "title": "Regret-Free Reinforcement Learning for LTL Specifications",
    "abstract": "guaranteesbecomesessential. Reinforcement learning (RL) is a promising methodtolearnoptimalcontrolpoliciesforsys- AmongtheexistingperformanceguaranteesforRL,regret temswithunknowndynamics. Inparticular,syn- minimizationhaswidelybeenstudiedrecently(Aueretal., thesizing controllers for safety-critical systems 2008;Agarwaletal.,2014;Srinivasetal.,2012;Dannetal., basedonhigh-levelspecifications,suchasthose 2017). For an online learning algorithm, intuitively, re- expressedintemporallanguageslikelineartem- gretisdefinedasthedifferencebetweentheaccumulated porallogic(LTL),presentsasignificantchallenge (expected)rewardscollectedbyanoptimalpolicyandthe in control systems research. Current RL-based the algorithm during learning. Existing regret minimiza- methods designed for LTL tasks typically offer tionalgorithmsassumethatoptimalpoliciesarepositional, only asymptotic guarantees, which provide no meaningthatoptimalpolicies(deterministically)mapevery insightintothetransientperformanceduringthe state into a corresponding action. While this suffices for learningphase.WhilerunninganRLalgorithm,it mostofbasicrewardstructures,optimalpoliciesmaynotbe iscrucialtoassesshowclosewearetoachieving positionalformoregeneralrewardstructures. Inparticular, optimalbehaviorifwestoplearning. whenthecontrolobjectiveissetbyanLTLformula,optimal policyis,ingeneral,notpositional. Inthispaper,wepresentthefirstregret-freeon- line algorithm for learning a controller that ad- RL for LTL specifications has recently become popular. dresses the general class of LTL specifications Majorityoftheexistingmethodsprovidenoperformance over Markov decision processes (MDPs) with guarantees(Icarteetal.,2018;Camachoetal.,2019;Hasan- a finite set of states and actions. We begin by beig et al., 2019; Kazemi et al., 2022; Hahn et al., 2019; proposingaregret-freelearningalgorithmtosolve Ouraetal.,2020;Caietal.,2020;Bozkurtetal.,2021;Sick- infinite-horizonreach-avoidproblems. Forgen- ertetal.,2016). Controllersynthesisforfinite-horizonand eral LTL specifications, we show that the syn- infinite-horizonLTLwithprobablyapproximatelycorrect thesis problem can be reduced to a reach-avoid (PAC) guarantee has recently been studied (Fu & Topcu, problemwhenthegraphstructureisknown.Addi- 2014;Voloshinetal.,2022). ItisshownthatprovidingPAC tionally,weprovideanalgorithmforlearningthe guaranteesforlearningalgorithmsthattargetcontrollersyn- graphstructure,assumingknowledgeofamini- thesisagainstinfinite-horizonspecificationsrequiresaddi- mumtransitionprobability,whichoperatesinde- tionalknowledge,suchasminimumtransitionprobability pendentlyofthemainregret-freealgorithm. (Alur et al., 2022). However, there exists no regret-free RL-basedcontrollersynthesismethodforLTLtasks. Inthispaper,weproposeanonlinelearningalgorithmfor 1.Introduction controlsynthesisproblemsagainstLTLobjectives,which Reinforcementlearning(RL)isbecomingmoreprevalentin providessublinearlygrowingregretbounds. Specifically, computingefficientpoliciesforsystemswithunknowndy- wewanttomeasuretheperformanceofthelearnedcontrol namicalmodels.Althoughexperimentalevidencehighlights policyduringlearning.Forthat,wecomparethesatisfaction theeffectivenessofRLinnumerousapplications,ensuring probabilitiesofanoptimalpolicyandsequenceofpolicies the safety and reliability of algorithms is imperative for generatedbythealgorithmduringlearning. certain safety-critical systems. In such scenarios, the de- Weconsidertheclassofsystems,whosedynamicscanbe 1MPI-SWS, Kaiserslautern, Germany. Correspondence to: capturedbyafiniteMDPwithunknowntransitionproba- MahmoudSalamati<msalamati@mpi-sws.org>. bilitiesandfixedinitialstates . Thecontrolobjectiveis init to synthesize a control policy that maximizes probability Proceedings of the 41st International Conference on Machine of satisfying a given LTL specification φ. Let π∗ denote Learning,Vienna,Austria.PMLR235,2024.Copyright2024by an optimal policy, meaning that applying π∗ maximizes theauthor(s). 1 4202 voN 81 ]IA.sc[ 1v91021.1142:viXra",
    "body": "Regret-Free Reinforcement Learning for LTL Specifications\nRupakMajumdar1 MahmoudSalamati1 SadeghSoudjani1\nAbstract velopmentofalgorithmsthatprovideexplicitperformance\nguaranteesbecomesessential.\nReinforcement learning (RL) is a promising\nmethodtolearnoptimalcontrolpoliciesforsys- AmongtheexistingperformanceguaranteesforRL,regret\ntemswithunknowndynamics. Inparticular,syn- minimizationhaswidelybeenstudiedrecently(Aueretal.,\nthesizing controllers for safety-critical systems 2008;Agarwaletal.,2014;Srinivasetal.,2012;Dannetal.,\nbasedonhigh-levelspecifications,suchasthose 2017). For an online learning algorithm, intuitively, re-\nexpressedintemporallanguageslikelineartem- gretisdefinedasthedifferencebetweentheaccumulated\nporallogic(LTL),presentsasignificantchallenge (expected)rewardscollectedbyanoptimalpolicyandthe\nin control systems research. Current RL-based the algorithm during learning. Existing regret minimiza-\nmethods designed for LTL tasks typically offer tionalgorithmsassumethatoptimalpoliciesarepositional,\nonly asymptotic guarantees, which provide no meaningthatoptimalpolicies(deterministically)mapevery\ninsightintothetransientperformanceduringthe state into a corresponding action. While this suffices for\nlearningphase.WhilerunninganRLalgorithm,it mostofbasicrewardstructures,optimalpoliciesmaynotbe\niscrucialtoassesshowclosewearetoachieving positionalformoregeneralrewardstructures. Inparticular,\noptimalbehaviorifwestoplearning. whenthecontrolobjectiveissetbyanLTLformula,optimal\npolicyis,ingeneral,notpositional.\nInthispaper,wepresentthefirstregret-freeon-\nline algorithm for learning a controller that ad- RL for LTL specifications has recently become popular.\ndresses the general class of LTL specifications Majorityoftheexistingmethodsprovidenoperformance\nover Markov decision processes (MDPs) with guarantees(Icarteetal.,2018;Camachoetal.,2019;Hasan-\na finite set of states and actions. We begin by beig et al., 2019; Kazemi et al., 2022; Hahn et al., 2019;\nproposingaregret-freelearningalgorithmtosolve Ouraetal.,2020;Caietal.,2020;Bozkurtetal.,2021;Sick-\ninfinite-horizonreach-avoidproblems. Forgen- ertetal.,2016). Controllersynthesisforfinite-horizonand\neral LTL specifications, we show that the syn- infinite-horizonLTLwithprobablyapproximatelycorrect\nthesis problem can be reduced to a reach-avoid (PAC) guarantee has recently been studied (Fu & Topcu,\nproblemwhenthegraphstructureisknown.Addi- 2014;Voloshinetal.,2022). ItisshownthatprovidingPAC\ntionally,weprovideanalgorithmforlearningthe guaranteesforlearningalgorithmsthattargetcontrollersyn-\ngraphstructure,assumingknowledgeofamini- thesisagainstinfinite-horizonspecificationsrequiresaddi-\nmumtransitionprobability,whichoperatesinde- tionalknowledge,suchasminimumtransitionprobability\npendentlyofthemainregret-freealgorithm. (Alur et al., 2022). However, there exists no regret-free\nRL-basedcontrollersynthesismethodforLTLtasks.\nInthispaper,weproposeanonlinelearningalgorithmfor\n1.Introduction\ncontrolsynthesisproblemsagainstLTLobjectives,which\nReinforcementlearning(RL)isbecomingmoreprevalentin providessublinearlygrowingregretbounds. Specifically,\ncomputingefficientpoliciesforsystemswithunknowndy- wewanttomeasuretheperformanceofthelearnedcontrol\nnamicalmodels.Althoughexperimentalevidencehighlights policyduringlearning.Forthat,wecomparethesatisfaction\ntheeffectivenessofRLinnumerousapplications,ensuring probabilitiesofanoptimalpolicyandsequenceofpolicies\nthe safety and reliability of algorithms is imperative for generatedbythealgorithmduringlearning.\ncertain safety-critical systems. In such scenarios, the de-\nWeconsidertheclassofsystems,whosedynamicscanbe\n1MPI-SWS, Kaiserslautern, Germany. Correspondence to: capturedbyafiniteMDPwithunknowntransitionproba-\nMahmoudSalamati<msalamati@mpi-sws.org>. bilitiesandfixedinitialstates . Thecontrolobjectiveis\ninit\nto synthesize a control policy that maximizes probability\nProceedings of the 41st International Conference on Machine of satisfying a given LTL specification φ. Let π∗ denote\nLearning,Vienna,Austria.PMLR235,2024.Copyright2024by\nan optimal policy, meaning that applying π∗ maximizes\ntheauthor(s).\n1\n4202\nvoN\n81\n]IA.sc[\n1v91021.1142:viXra\nRegret-FreeReinforcementLearningforLTLSpecifications\nthe satisfaction probability for φ. Running an online al- wediscusstheexistingrelatedliterature;inSec.3,wepro-\ngorithm will produce a sequence of policies π ,π ,..., vide the required notations and definitions; in Sec. 4, we\n1 2\noneperepisode. Letv∗(s ),v (s )denote,respectively, describeouralgorithmforthereach-avoidproblemandex-\ninit k init\nthe satisfaction probabilities of π∗ and π when starting plain derivation of our claimed regret bounds; in Sec. 5,\nk\nfrom s . After passing K > 0 episodes, regret is de- weexplainhowourregret-freealgorithmcanbeextended\ninit\nfined as R(k) := (cid:80)K (v∗(s )−v (s )). The algo- tosolvethecontrollersynthesisproblemforgeneralLTL\nk=1 init k init\nrithmweproposeinthispaperisregretfree,meaningthat specifications;inSec.6,weprovideempiricalevaluationfor\nlim R(K)/K =0. ourproposedmethod;finally,inSec.7,westatesomecon-\nK→∞\ncludingremarksaswellassomemeaningfulfutureresearch\nThere exist regret minimization techniques for solving\ndirections.\ninfinite-horizoncontrollersynthesisproblems. Ononehand,\nregret-freealgorithmssuchasUCRL2(Aueretal.,2008)\ncanonlybeappliedtocommunicativeMDPs;forinfinite- 2.RelatedWork\nhorizon specifications such as reach-avoid, the represen-\nWediscussexistingresultsinfourrelateddomains.\ntativeMDP willbealwaysenon-communicating. Onthe\notherhand,thereexistonlineno-regretalgorithmsthatcan Non-Guaranteedreinforcementlearningtechniquesfor\nbeappliedtostochasticshortestpath(SSP),whichisclosely LTL specifications. During the course of the past years,\nrelatedtotheinfinite-horizonreachabilityprobleminLTL lotsofeffortshavebeendedicatedintosolvingthecontroller\n(Tarbouriech et al., 2020; Rosenberg et al., 2020). There synthesisproblemforsystemsmodeledasfiniteMDPswith\naremultipleobstacleswhichprohibitdirectapplicationof unknowntransitionprobabilitiesandhavetofulfillcertain\nthose methods to our setting. First, the primary outcome tasksencodedasformulainLTL.Earlyresultsfocusedonly\nweseekistheregretvaluewithrespecttoanoptimalpol- on the finite-horizon subset of LTL specifications. Icarte\nicy’s satisfaction probability, rather than regret based on et al. introduced reward machines, which use finite state\nanarbitrarycoststructure. Anycoststructuresuitablefor automatatoencodefinite-horizonspecifications,alongwith\napplyingstandardSSPalgorithmsrestrictscoststostrictly specialized (deep) Q-learning algorithms to support their\npositivevalues. Thisrestrictionpreventsaformulationthat approach(Icarteetal.,2018). Camachoetal. laterformal-\ncomputestheaccumulatedregretrelativetotheoptimalsat- izedtheautomaticderivationofrewardmachinesforfinite-\nisfactionprobability,whichisourmainobjective. Second, horizon subsets of LTL (Camacho et al., 2019). The de-\nonemainassumptionfortheproposedmethodsattacking velopmentofefficientandcompactautomata,suchaslimit\ntheSSPproblemistoassumethatthereexistsatleastone deterministicBu¨chiautomata(LDBA),forrepresentingLTL\npolicyunderwhichthetargetisreachablewithprobability formulashasledtosignificantadvancesinreinforcement\n1. To relax this assumption, existing methods often im- learningforthefullclassofLTLspecifications,including\nposestrongrequirements,suchasknowinganupperbound infinite-horizoncases(Sickertetal.,2016). Typically,one\nonthetotalaccumulatedcostandhavingsufficientlyhigh hastofirsttranslatethegivenLTLformulaintoanappro-\ncostassignmentsforvisitingnon-acceptingendcomponents. priate automaton, such as a Rabin or limit-deterministic\nOurformulationavoidsalloftheseassumptionsandonly Bu¨chi automaton, and then compute the product of this\nrequiresknowledgeoftheminimumtransitionprobability automatonwiththeactualMDPtoformulatethefinal(dis-\nintheMDP. counted)learningproblem. Thisformulationensuresthat,\nwithasufficientlylargediscountfactor(dependentonsys-\nWefirststateouralgorithmforreach-avoidspecifications,\ntemdynamicsandgoalspecifications),applyingstandard\nwhereinasetofgoalstatesmustbevisitedbeforehitting\nRLtechniqueswillleadthepolicytoconvergeasymptoti-\na set of bad states. In every episode, our algorithm (1)\ncallytotheoptimalone(Kazemietal.,2022;Bozkurtetal.,\ncomputes an interval MDP (iMDP) by taking all of the\n2021; Oura et al., 2020). However, these methods fail to\ncollectedobservationsintoaccount;(2)findsanoptimistic\nprovide a finite-time performance, and the critical lower\npolicyoverthecomputediMDPtosolvethetargetreach-\nboundforthediscountfactorcannotbeknowninadvance.\navoid problem; (3) executes the computed policy before\nan episode-specific deadline reached. We prove that the GuaranteedreinforcementlearningforLTLspecifica-\nregret of our algorithm grows sublinearly with respect to tionThetwomostpopularmetricsforevaluatingtheperfor-\nthe number of episodes. For a general LTL specification, manceoflearningalgorithmsareprobablyapproximately\nwe show that the synthesis problem can be reduced to a correct(PAC)andregretbounds.In(Fu&Topcu,2014),the\nreach-avoidproblem,ifweknowtheconnectiongraphof problemofsynthesizingcontrollersforfinite-horizonLTL\ntheMDP.Finally,weprovideapolynomialalgorithmfor specifications over finite MDPs with unknown transition\nlearningthegraphstructureofMDPs. probabilitieswasaddressed,andaPAClearningalgorithm\nwasproposed. However,itssamplecomplexityexplicitly\nThe rest of this paper is organized as follows: in Sec. 2,\ndependsonthehorizonlength,makingitunsuitableforfull\n2\nRegret-FreeReinforcementLearningforLTLSpecifications\nLTLwithinfinite-horizonspecifications. Afewyearslater, ratherthantheaccumulatedcost.\nasurprisingnegativeresultemerged: fullLTLisnotPAC\nlearnable(Yangetal.,2021;Aluretal.,2022). Uponcloser\n3.Preliminaries\nexamination, the main issue behind this result is the as-\nsumptionofanunknownminimumtransitionprobability. In 3.1.Notation\n(Voloshinetal.,2022),byassumingtheminimumtransition\nFor a matrix X ∈ Rm×n, we define the ∞-matrix-norm\nprobabilityisknown,aPAC-learnablecontrollersynthesis\n∥X∥ := max\n(cid:80)n\n∥X ∥. Given two integer\nwasproposed. However,theirapproachrequiresaccessto ∞ 1≤i≤m j=1 ij\nnumbersa,bs.t.a≤b,wedenotethesetofintegernumbers\nagenerativemodel,allowingdatacollectionfromanyarbi-\na≤l≤bby[a;b].\ntrarystate-actionpair. Inmanyrealisticscenarios, thisis\nimpracticalsinceinitializingtheMDPatarbitrarystatesis\n3.2.LinearTemporalLogic\nnot feasible. Our proposed algorithm, however, does not\nrequireaccesstoagenerativemodel. Recently,(Perezetal., WeconsiderspecificationsintheformofformulasinLinear\n2023)proposedanotherPAClearningalgorithmthatalso TemporalLogic(LTL).Here,wegiveabriefintroduction\ndoesnotrelyonagenerativemodel. However,thereisstill toLTL.FordetailedsyntaxandsemanticsofLTL,werefer\nnoregret-freeonlinealgorithmforcontrollersynthesiswith tothebookbyBaierandKatoen(Baier&Katoen,2008)\nLTLspecifications. andreferencestherein. WeconsiderLTLspecificationswith\nsyntax\nRegret-freereinforcementlearningforcommunicating\nMDPsUCRLandUCRL2(Aueretal.,2008;Auer&Ort-\nψ :=p|¬ψ|ψ ∧ψ |⃝ψ|ψ Uψ ,\nner,2007)arewell-knownregret-freelearningalgorithms 1 2 1 2\nproposedforcommunicatingMDPs. Thewayregretisde- wherep⊂S isanelementofthesetofatomicpropositions\nfined for communicating MDPs makes them particularly AP. Letρ=ρ ,ρ ,... beaninfinitesequenceofelements\n0 1\nsuitable for our objective: regret is measured over an in- from 2AP and denote ρ = ρ ,ρ ,... for any i ∈ N.\ni i i+1\nfinite sequence of states without discounting later obser- Then the satisfaction relation between ρ and a property\nvations, which aligns perfectly with the requirements of ψ, expressed in LTL, is denoted by ρ |= ψ. We denote\ninfinite-horizon LTL specifications. Notably, learning in ρ |= p if ρ ∈ p. Furthermore, ρ |= ¬ψ if ρ ̸|= ψ and\n0\ncommunicatingMDPscancontinueindefinitelywithoutthe ρ |= ψ ∧ψ if ρ |= ψ and ρ |= ψ . For next operator,\n1 2 1 2\nneedforresetting. However,weobservethatintheproblem ρ |= ⃝ψ holds if ρ |= ψ. The temporal until operator\n1\nwe aim to solve, even if the underlying system’s MDP is ρ |= ψ Uψ holdsif∃i ∈ N :ρ |= ψ ,and∀j ∈ N:0 ≤\n1 2 i 2\ncommunicating,itsproductwiththeautomatonmodeling j < i,ρ |= ψ . Disjunction (∨) can be defined by ρ |=\nj 1\nthespecificationmaybecomenon-communicating(Kazemi ψ ∨ψ ⇔ ρ |= ¬(¬ψ ∧¬ψ ). Theoperatorρ |= ♢ψ\n1 2 1 2\netal.,2022). (Fruitetal.,2018)proposedaregret-freeal- isusedtodenotethatthepropertywilleventuallyhappen\ngorithm for non-communicating MDPs, where the initial atsomepointinthefuture. Theoperatorρ|=□ψsignifies\nstateiswithinanon-transientsubsetofstates. Inourcase, that ψ must always be true at all times in the future. We\ntheinitialstateisfixedandlocatedwithinatransientsubset, also define ψ → ψ with ¬ψ ∨ ψ . For a given LTL\n1 2 1 2\nmaking algorithms like UCRL2, which are designed for specificationφ,wecanmonitorsatisfactionofφbyrunning\ncommunicatingMDPs,inapplicable. an appropriate automaton A = (Q,Σ,δ,q ,F), which\nφ init\nconsistsofafinitesetofstatesQ,afinitealphabetΣ=2AP,\nRegret-free reinforcement learning for non-\natransitionfunctionδ: Q×Σ (cid:55)→ 2Q,aninitialstateq ,\ncommunicating MDPs Goal-oriented reinforcement init\nandanacceptingconditionAcc. Forexample,theaccepting\nlearningisakeyclassofproblemsinRL,oftenformulated\nconditionindeterministicRabinautomaton(DRA)isinthe\nasashortestpathproblem(SPP)forMDPswithunknown\nformoftuples(J ,K )|i=1,...,m,consistingofsubsets\ntransitionprobabilities. Recently,severalexcitingtheoreti- i i\nJ andK ofQ. Aninfinitesequenceρisacceptedbythe\ncalresultshavebeenpublished(Tarbouriechetal.,2020; i i\nDRA A if there exists at least one pair (J ,K ) ∈ Acc\nRosenbergetal.,2020). Inparticular,theonlinelearning φ i i\nsuch that inf(ρ)∩J = ∅ and inf(ρ)∩K ̸= ∅, where\nalgorithmproposedby(Tarbouriechetal.,2020)provides i i\ninf(ρ)isthesetofstatesthatappearinfinitelyofteninρ.\nsub-linearregretboundsfortheaccumulatedcostinMDPs,\nassuming(1)theexistenceofaproperpolicythatreaches\nthegoalwithprobabilityone,and(2)allcostsarepositive. 3.3.MDPs\nThey argue that (1) can be relaxed if an upper bound on\nLet∆(X)bethesetofprobabilitydistributionsovertheset\ntheaccumulatedcostisknown,and(2)byperturbingthe\nX and AP be a set of atomic propositions. We consider\ncosts. However,theassumptionofamaximumaccumulated\nMDPs M = (S,A,T,s ,AP), where S and A denote\ninit\ncost is often unrealistic, and we are more interested in\nthe finite set of states and actions, respectively, T: S ×\ncomputingregretboundsovertheprobabilityofsatisfaction,\nA×S → ∆(S) denotes an unknown transition function,\n3\nRegret-FreeReinforcementLearningforLTLSpecifications\ns ∈ S istheinitialstateoftheMDPandAP istheset Algorithm1Regret-freealgorithmforreach-avoidspecifi-\ninit\nof atomic propositions. We denote labeling function by cations\nL: S → 2AP. Let Π denote the set of all deterministic Input: Reach-avoidspecification¬B UG, confidence\npositionalpoliciesoverM,thatisπ: S → Aforπ ∈ Π. parameterδ ∈(0,1),stateandactionsetsSandA,initial\nByfixingaspecificpolicyπ ∈Π,theMDPreducesintoa states .\ninit\nMarkovchainC =(S,P,s init,AP). Initialization: Sett=1,s\n1\n=s\ninit\nforepisodesk =1,2,... do\nFor a given MDP M, maximal end components (MECs)\nInitializeepisodek:\naresub-MDPswhichareprobabilisticallyclosed,thatis,(1)\nt ←t\nthereexistsapositionalpolicyunderwhichonecantravel k\nSetN (s,a):=#{t<t : s =s,a =a}\nbetweeneverypairofstateswithintheendcomponentwith k k t t\nFor all s,s′ ∈ S and a ∈ A compute the emprical\nprobability1,and(2)theendcomponentcannotbeexited\ntransitionfunction\nunderanypositionalpolicy. AnMDPiscalledcommuni-\ncatingifitiscomprisedofonlyoneMECincludingevery #{t<t : s =s,a =a,s =s′}\nTˆ (s′ |s,a):= k t t t+1\nstate. ForanMDPM,theunderlyinggraphisdenotedas k max{1,N (s,a)}\nk\nχ =(S,A,E),whereχ ⊆S×A×S isdefinedsuch\nM M\nthat(s,a,s′)∈E ifandonlyifT(s,a,s′)>0. Computepolicyπ˜ :\nk\nCharacterizeM astheintervalMDPwiththesetof\nk\n3.4.RegretAnalysis transitionfunctionsT satisfyingtheinequality(4)\nUse EVI (Alg. 2) to find a policy π˜ and optimistic\nk\nWe are interested in synthesizing policies that maximize MDPM˜\nk\nsatisfactionprobabilityofLTLspecificationsoverMDPs\nComputethedeadlineH usingEq.(9)\nk\nwithfinitesetofstatesandactionsandfixedinitialstates .\ninit Executepolicyπ˜\nk\nLetv∗(s )denotetheprobabilitywithwhichtheoptimal\ninit whiles ∈/ Gand(t−t )≤H do\nt k k\npolicyπ∗satisfiesthetargetspecificationφ,whenstartedat\nifs ∈/ Bthen\nt\ns . Learningtakesplaceoverconsecutiveepisodes. For\ninit Observethenextstates\nt+1\ntheepisodekwedefinev (s )denotetheprobabilityof\nk init else\nsatisfyingthetargetspecificationφunderthepolicyπ in\nk s ←s\nt+1 init\ntheMDPM ,whenstartedats .Wemeasurethesuccess\nφ init endif\nofthelearningalgorithmthroughitscorrespondingregret\nt←t+1\nthatisdefinedas\nendwhile\nendfor\nK\n(cid:88)\nR(K):= (v∗(s )−v (s )). (1)\ninit k init\nk=1\n4.Regret-FreeControllerSynthesisfor\nInpractice, oneisinterestedtoterminatetheonlinealgo-\nReach-AvoidSpecifications\nrithmbasedonareasonablestoppingcriterion.Letusdefine\nthenormalizedregret,asfollows: Inthissection,westudythecontrollersynthesisproblemfor\nMDPswithunknown(butfixed)transitionfunctionagainst\nR(K)\nR (K)= . (2) reach-avoid specifications. Let G and B be two distinct\na K atomicpropositions. Weareinterestedinfindingpolicies\nwhich can maximize the satisfaction probability for the\nAnonlinealgorithmiscalledregret-freeifitscorrespond-\n(unbounded)reach-avoidspecificationφ=¬BUG. Welet\ningregretgrowssublinearlywithrespecttothenumberof\nM =(S,A,T,s ,{G,B})betheMDPthatisresulted\nepisodesK. Runningaregret-freeonlinealgorithmenables φ init\nbymakingstateswithinGandB absorbing. Now,weare\nachievingarbitrarysmallvaluesofnormalizedregret.There-\nabletodefinetheproblemthatisthemainsubjectofstudy\nfore,onecouldfixathresholdε∈(0,1)andterminatethe\ninthissection.\nalgorithmoncethecorrespondingnormalizedregretgoes\nbelowε. Therefore,onecanconsiderthesmallestnumber Problem 1. Given an MDP M with unknown transition\nofepisodesk∗ ∈NafterwhichR (k∗ )<εwithconfi- function,minimumtransitionprobabilityp ∈(0,1),a\nreg a reg min\ndence1−δasacomplexitymetricfortheproposedonline reach-avoidspecificationφ = ¬BUG,andaconfidence\nlearningalgorithm,withrespecttoparametersδ,ε∈(0,1). parameter 0 < δ < 1, find an online learning algorithm\nIntuitively, after k > k∗ many learning episodes, with such that with confidence greater than 1−δ the resulted\nreg\nconfidence1−δtheaveragesatisfactionprobabilityforthe regretdefinedbyEq.(1)growssublinearlywithrespectto\npolicycomputedinthekthepisode,willbeε-optimal. thenumberofepisodesK.\n4\nRegret-FreeReinforcementLearningforLTLSpecifications\nAlgorithm2ExtendedValueIteration(EVI) Algorithm3AlgorithmforsolvingInnerMaxin(3)\nInput: MECsGandB,setofplausibletransitionfunc- Inputs: State-action pair (s,a), estimates Tˆ (· | s,a),\nk\ntionsT k,upper-boundoverhittingtimeΛ∈R| >S 0|,start d(s,a)=(cid:113) 8|S|log(2|A|k/δ) andthestatesinSaccording\ntimeofthekthepisodet k totheirvaluem νax ,( i.1, eN .k ,( Ss, ′a) =) {s′,...,s′ }withν (s′) ≥\nSetl=0,v˜ (s)=0fors∈/ Gandv˜ (s)=1fors∈G l 1 n l 1\n0 0 ν (s′)≥···≥ν (s′ ).\nrepeat l 2 l n\nSet\nl←l+1\nComputeP˜ andπ˜ suchthat pˆ(s′ 1)=Tˆ k(s,a,s′ 1)+d(s,a)/2\nk k\nForj >1,set\nv˜ =L˜ v˜\nl k l−1\n m a∈a Ax\nP˜\nk(s,am )∈a Tx k(s,a)P˜ k(s,a)⊤v˜\nl−1\ns∈/ G∪B pˆ(s′ j)=Tˆ k(s,a,s′ j)\n= 1 s∈G Setj ←n\n0\ns∈B\nwhile(cid:80) s′∈S′pˆ(s′ q)>1do\npˆ(s′)=q max{0,1−(cid:80) pˆ(s′)}\n(3) j s′∈S′\\{s′} q\nq j\nj ←j−1\nFors∈S,computeλ¯ solvingEq.(8) endwhile\nk\nu Rn et si ul l∥ tsv˜ :l P−\n˜\nkv˜ ,l π˜− k1∥ ∞ ≤ 21 tk andλ˜ k ≤Λ S Re et sP u˜ lk t( ss :, Pa\n˜\nk, (s s′ j ,) a= ,sp ′ˆ )(s′ j)foreverys′ j ∈S′.\n4.1.Methodology useamodifiedversionofextendedvalueiteration(EVI)to\ncomputeanoptimisticMDPM˜ ∈M andanoptimistic\nOur proposed algorithm is demonstrated in Alg. 1. We k k\npolicyπ˜ suchthatπ˜ maximizesprobabilityofreaching\nproposeouralgorithmintheknownparadigmofoptimism\nG on\nthek\noptimistic\nMk\nDP M˜ . Fixing the policy π˜ on\ninthefaceofuncertainty. Learningtakesplaceovercon- M˜ inducesadiscrete-timeMk arkovchain(DTMC)k with\nsecutive environmental episodes. Each episode is a finite trank sitionfunctiondenotedbyP˜ .\nsequences ,a ,s ,...s thatstartsfromtheinitialstate k\n1 1 2 L\nof M , i.e., s = s , and ends if either (1) one of the Alg. 2 illustrates our proposed extended value iteration\nφ 1 init\nMECs in G are reached meaning that s ∈ G, or (2) an (EVI)algorithm. Onekeyinputtoouralgorithmisavector\nL\nepisode-specificdeadlineisreached. Λ ∈ R|S|, whose entries contain an upper bound on the\n>0\nexpectedhittingtimerequiredtoreachGunderanoptimal\nComputingconfidenceintervals. Letδ ∈(0,1)beagiven\npolicyπ∗. Thefollowinglemmaoutlinestheprocedurefor\nconfidence threshold, t be the time point at which kth\nk\ncomputingΛ,assumingitisfinite.\nepisodebeginsandN (s,a)denotethenumberoftimesthe\nk\nstate-actionpair(s,a)hasbeenvisitedbeforethestartofthe Lemma4.2. LetM′ denotetheMDPthatisresultedby\nkthepisode.LetTˆ andM denotetheempiricaltransition connecting B to s in M with probability 1, and λ∗ ∈\nk k init\nfunctionandthesetofstatisticallyplausibleMDPs, both R|S| denotethevectorwhoseentriescontaintheexpected\n>0\ncomputedusingtheobservationsbeforethestartofthekth timetohitGinMDPM′underpolicyπ∗. Withconfidence\nepisode. Inparticular,wedefineM k astheintervalMDP 1−δ,wehave\nwithintervaltransitionfunctionT forwhichalltransition\nk\nfunctionsT ∈T satisfy log(δ)\nk k λ∗ ≤Λ:=|S| , (5)\nlog(1−p|S| )\n(cid:115) min\n8|S|log(2|A|k/3δ)\n∥T (·|s,a)−Tˆ (·|s,a)∥ ≤β (s,a)=: .\nk k 1 k max(1,N (s,a)i)fλ∗isfinite,i.e.,whenGisreachablewithnon-zeroproba-\nk\n(4) bility.\nIntuitively,wepicktheconfidenceboundontherighthand Remark 4.3. One may notice that the input B not only\nsideofEq.(4),suchthatthecorrespondinginequalityholds containsthestatescorrespondingtothereach-avoidspec-\nwithhighconfidence. Moreconcretely,wehavethefollow- ification,butalsoincludeseveryMECwhoseintersection\ningresult. withGisempty. InApp.A,wehavediscusseddetailsofan\nLemma 4.1. (Tarbouriech et al., 2020) Let E := algorithmforlearningtheMDPgraphχ Muptoanydesired\n(cid:83)∞ {M ∈M }. ThenP(E)≥1−δ. confidence,usingtheknowledgeoftheminimumtransition\nk=1 φ k\nprobabilityp . Givenχ , wecan(1)efficientlyiden-\nmin M\nExtended value iteration. In every episode k ∈ N, we tifyallMECswithinMthatdonotintersectwithGand\n5\nRegret-FreeReinforcementLearningforLTLSpecifications\nincludetheminB,and(2)determinewhetherλ∗ isfinite slowepisodes—,or(ii)byreachingoneoftheMECsinG—\nbyverifyingifGiswithinthereachablesetofs . correspondingtofastepisodes. Itmustbenotedthatevery\ninit\nvisit of states within MECs in B causes activation of an\nWehavesettwoconditionsforterminatingtheEVIalgo-\nartificialresetactionwhichsetss asthenextimmediate\nrithm. Thefirstcondition(∥v˜ −v˜ ∥ ≤ 1 )ensures init\nl l−1 ∞ 2tk state.\nthatforeverys∈S\nThefollowingtheoremstatesthatourproposedalgorithm\n1\nv˜ (s)≤L˜ v˜ (s)+ , (6) isregretfree,meaningthatthecorrespondingregretgrows\nk k k 2t k sublinearly.\nwhere L˜ is given by Eq. (3). Further, we get for every Theorem4.4. Withprobability1−6δ,Alg.1suffersaregret\nk\ns∈S\n1 (cid:114)\nv˜ (s)+ v∗(s)≥0, (7) 1\nk 2t\nk\nR(K)=O( 2Kα Klog δ)\nwherev∗andv˜ denotethevectorscontainingprobabilities (cid:114)\nk 2|A|Kα\nofreachingGstartingfromdifferentstates,whenpolicies +4|S| 8|A|Kα log( K)\nπ˜ andπ∗arefollowedonMDPsM˜ andM ,respectively K δ\nk k φ (cid:114)\n((Aueretal.,2008),Thm.7).\n+2 2Kα\nlog(3(Kα K)2\n)\nK δ\nLetM˜′ denotetheMDPthatisresultedbyconnectingB\nk α (1+log(Kα ))\nto s with probability 1 in M˜ , and λ˜ ∈ R|S| denote + K K\ninit k k >0 2\nthevectorwhoseentriescontaintheexpectedtimetohitG (cid:114)\ni Mn DP˜ Pk′ Mth ˜a ′t .is Toth ce oD mT pM uteC λ˜in ,d wuc ee nd eb ey da topp soly lvin eg thp eo fli oc ly loπ˜ wk ina gt +2√ K+2 2α KKlog(2(α K δK)2 )\nk k (cid:114)\nlinearsystemofequations:\n+4|S| 8|A|α\nKlog(2|A|α KK\n), (11)\nK δ\n(I−P˜′)λ˜ =u, (8)\nk k\nwhereα :=max H and\nK 1≤k≤K k\nwhere u(s) = 0 if s ∈ G and u(s) = 1 otherwise. We\nnoticethat,thesecondterminationconditionforEVI(λ˜ ≤ (cid:108) √ (cid:109)\nΛ)canbefulfilled,sinceλ˜ ≤λ∗ <Λ. k α K ≤ 3Λlog(2 K) (12)\nk\nCalculatingepisode-specificdeadline. Uponcomputation\nofM˜ andπ˜ forepisodek, wecomputeadeadlineH . 4.2.ProofSketchofThm.4.4\nk k k\nDeadlineH isdefinedas\nk InordertoboundthetotalaccumulatedregretR(K),wede-\n1 fineR(K)=(cid:80)K ∆ ,where∆ =v∗(s )−v (s ).\nH =min{n>1|∥Q˜n∥ ≤ }, (9) k=1 k k init k init\nk k ∞ 1 Asmentionedbefore,ouranalysiscategorizesepisodesinto\nkq\nslow or fast, corresponding to episodic regrets ∆(s) and\n .  k\nA˜ . . ˜ζ ∆(f), respectively. Note that for a fast episode, we have\nwhereq ∈ N>1,Q˜ k =   ..k . ..\n.\n.. ..k .   withg = 1 sinit ∆ Fok ( k rs) th= es0 l; os wim ei pl ia sr oly d, ef so ,r wa es ulo sew thep ei oso bd ve io, uw se uh pa pv ee rb∆ o( k uf n) d=0.\ng . 0\nbeing a |S| − 1 dimensional one-hot vector and A˜ ,˜ζ\ncorrespondtothesubstochasticpartitionofP˜ contaik ningk ∆( ks) ≤1. (13)\nk\ntransitionprobabilitiesforthestatesinS\\G:\nForfastepisodes,sinceitispossiblethatarunendsinone\n . .  ofMECsinBbeforereachingG,weneedtodefineareset\nA˜ . . ˜ζ . . β˜\nP˜\nk\n= ..k . ... ..k . ... ..k . . (10) t er va en rs yit ei po in sow dh ei kch cata nk be es bth roe ks et nat te os Iin ∈B Nto ins tein ri vt. alsTh sue cre hf to hr ae t,\n . .  k\n0\n.\n. 0\n.\n. 0\nthefirstI\nk\n−1intervalsstartfroms\ninit\nandendatB,and\ntheIth intervalstartsfroms andendatG. Wedenote\nk init\nNotethatweintentionallysetthetransitionprobabilityfrom theithintervalofthekthepisode—inwhichthepolicyπ\nk\nB tos init to1ing,inordertotaketheeffectofresetinto istaken—byρ k,i,andthecorrespondingvalueisdefinedas\nconsideration.\n(cid:40)\nRegret bound analysis. Every episode starts at s\ninit\nand\nv =\n1 iflast(ρ k,i)∈G\n(14)\nk,i\nendsbyeither(i)exceedingthedeadline—correspondingto 0 iflast(ρ )∈B.\nk,i\n6\nRegret-FreeReinforcementLearningforLTLSpecifications\nWe use the fact that v∗(s )−v (s ) ≤ I (v∗(s )− Lemma4.6. withprobabilityatleast1−δ,wehave\ninit k init k init\nv (s ))(becauseI ≥1)anddefine\nk init k\nK (cid:114)\n∆( kf) ≤I k(v∗(s init)−v k(s init)) (cid:88) ∆( kf,1) ≤4|S| 8|A|Kα Klog(2|A|K\nδ\nα K)\nk=1\n(cid:88)Ik\n(cid:114)\n= v∗(s )−v (s ) 3(Kα )2\ninit k,i init +2 2Kα log( K )\ni=1 K δ\n(cid:88)Ik\n+\nα K(1+log(Kα K))\n. (16)\n+ v (s )−v (s ).\nk,i init k init 2\ni=1\nInordertoprovethesublinearboundover(cid:80)K ∆(f,2),we\nOfcourse,whilerunningthepolicyπ inthekth episode, k=1 k\nk\nmakeuseoftheAzuma-Hoeffdinginequality.Thefollowing\nwedonotknowtheexactvaluesforv∗(s )andv (s ).\ninit k init lemma proves the bound over sum of ∆(f,2) for the fast\nInstead, we will try to over approximate the value for k\nv∗(s )−v (s )ineveryepisodebyusingideasinupper episodes.\ninit k init\nconfidenceboundalgorithmssuchasUCRL2andfurther Lemma4.7. Withprobabilityatleast1−δ,wehave\nrelatingv (s\n)to(cid:80)Ik\nv .\nk init i=1 k,i K (cid:114)\nWefurtherdefinethedecomposedregretterms (cid:88) ∆(f,2) ≤ 2Kα log1 , (17)\nk K δ\n∆(f,1)\n=(cid:88)Ik\nv∗(s )−v (s ),\nk=1\nk init k,i init whereα K =max 1≤k≤KH k.\ni=1\nand Later,inLemma4.8,weshowthatα K growslogarithmi-\ncallywithK whichprovesthat(cid:80)K ∆(f,2)growssublin-\n∆(f,2)\n=(cid:88)Ik\nv (s )−v (s ). earlywithK.\nk=1 k\nk k,i init k init\ni=1 LetF denotethenumberofepisodesinwhichGisnot\nK\nreached within the first H time steps. We can state that\nk\nTheproofsketchofourregretanalysisisasfollows. We\nwithhighprobabilityF growssublinearly.\nfirstprovethat(cid:80)K ∆(f)growssublinearlywithincreas- K\nk=1 k Lemma4.8. Withprobabilityatleast1−δ,wehave\ningnumberofepisodes(Lemmas4.5to4.7). Tothisend,\nweneedtoprovethatboth(cid:80)K k=1∆(f,1)and(cid:80)K k=1∆(f,2)\n√\n(cid:114)\n2(α K)2\ngrow sublinearly. In order to prove sublinear bound on F ≤2 K+2 2α Klog( K )\nthe sum (cid:80)K ∆(f,1), (1) in Lemma 4.5 we prove that K K δ\nk=1 (cid:114)\nα\nK\ngrowssublinearlywithK,and(2)inLemma4.6,we\n+4|S| 8|A|α\nKlog(2|A|α KK\n), (18)\nprove that (cid:80)K ∆(f,1) grows sublinearly with the num- K δ\nk=1\nberofepisodesK andlinearlywiththemaximumlength\nwhereα =max H .\nofepisodesα . Inordertoconcludethesublinearbound K 1≤k≤K k\nK\nover (cid:80)K ∆(f), in Lemma 4.7, we prove the sublinear\nk=1 ThefollowinglemmastatesadirectimplicationofLem.4.8.\ngrowthon(cid:80)K ∆(f,2). Consequently,weshowthatthe\nk=1 Lemma4.9. Withprobabilityatleast1−δ,wehave\nsumcorrespondingtotheslowepisodes, i.e., (cid:80)K ∆(s)\nk=1 k\nalsogrowssublinearlywithK,since(1)thenumberofslow K\nepisodesgrowsonlysublinearlywithK (Lemma4.8),and (cid:88) ∆ k(s) ≤F K, (19)\n(2)∆(s) ≤1foreveryslowepisode. Bysummingupallof k=1\nk\nthesearguments,ourproofiscompleteandweknowthat\n√ and F is upper-bounded by a function whose growth is\nK\nR(K)=O( K).\nsublinearwithrespecttothenumberofepisodes(Eq.(18)).\nLemma 4.5. Let Λ be an upper bound over λ∗(s), i.e.,\nλ∗(s) ≤ Λ for every s ∈ S \\(G∪B). Then, under the\n5.Regret-FreeControllerSynthesisforLTL\neventE wehave\nSpecifications\n(cid:108) √ (cid:109)\nα ≤ 3Λlog(2 K) (15)\nK\nInthissection,westudythecontrollersynthesisproblemfor\nMDPswithunknown(butfixed)transitionfunctionagainst\nNow,weproceedbyshowingwhy(cid:80)K ∆(f,1)growssub- LTL specifications. In the following, we state the main\nk=1 k\nlinearlywithK. problemofstudyinthissection.\n7\nRegret-FreeReinforcementLearningforLTLSpecifications\nProblem 2. Given an MDP M with unknown transition Algorithm4Regret-freelearningalgorithmforgeneralLTL\nfunction,minimumtransitionprobabilityp ∈(0,1),an specifications\nmin\nLTLspecificationφ,andaconfidenceparameter0<δ <1,\nInput: LTLspecificationφ, confidenceparameterδ ∈\nfindanonlinelearningalgorithmsuchthatwithconfidence (0,1), state and action sets S and A, connection graph\ngreater than 1−δ the resulted regret defined by Eq. (1) χ\nM\ngrowssublinearlywithrespecttothenumberofepisodes ComputeaDRAA whichacceptsφ\nφ\nK.\nUsethegraphconnectionχ andcomputeMECsGand\nM\nB\nWetransformthecontrollersynthesisproblemforgeneral\nRunAlg.1tocomputeandupdatethecontrolpolicyover\nLTLspecificationsintoasynthesisproblemforreach-avoid\nM\nφ\nspecification,forwhichonecanusetheregret-freeonline\nalgorithmproposedinSec.4. ForagivenLTLspecification\noveranMDP,thecorrespondingoptimalpolicy,ingeneral,\nbelongstotheclassof(deterministic)non-positionalpoli-\ncies,thataremappingsfromthefinitepathsovertheMDP\nintothesetofactions. Inordertorestrictthesetofpolicies\ntoMarkovian(positional)policies,weneedtocomputethe\ncorrespondingproductMDPbytakingtheproductbetween\ntheMDPMandtheautomatonA .\nφ\nDefinition 5.1. Given an MDP M = (S,A,T,s ,AP)\ninit\nand DRA A = (Q,Σ,δ,q ,F) corresponding to an\nφ init\nLTL formula φ, we denote the product MDP M =\nφ\n(S×,A×,T×,s× init,AP×),whereS× = S ×Q,A× = A, Figure1: Mapofthegridworldexamplewithl = 6. The\ns× init = (s init,q init), AP× = AP × Q, and T×: S× (cid:55)→ blueandgreencellsdefinetheinitialstateandgoal. Thered\n∆(S×)takingtheform\ncellscorrespondtowalls. Thegoaloftheagentistoreach\nthegoalwithouthittingthewalls.\n(cid:40)\nT(s,a,s′) q′ ∈δ(q,L(s′))\nT×((s,q),a,(s′,q′))=\n0 otherwise. 6.ExperimentalEvaluation\nInthissection,weevaluateanimplementationofouralgo-\nrithm. Alloftheexperimentswereperformedonalaptop\nFor the product MDP M , we define accepting maxi- withcorei7CPUat3.10GHz,with8GBofRAM.Wecon-\nφ\nmal end components (AMECs) as those MECs G , j ∈ sidered a reach-avoid controller synthesis problem in the\nj\n{1,2,...,m}inM forwhichG =(S×K ).Wedenote gridworldexampledescribedinFig.1. Theworldischarac-\nφ j j\ntheunionofeveryAMECinM byG.Likewise,theunion terizedbythenumberofcellspercolumnandrow,whichis\nφ\nofeverynon-acceptingMECB ,j ∈{1,2,...,m}inM denotedbyl∈N≥4. Theagentcanmoveusingthecardinal\nj φ\nforwhichB =(S×J )inM isdenotedasB. directions, i.e., A = {right,left,up,down}. Movement\nj j φ\nalonganintendeddirectionsucceedswithprobability0.9\nLetM =(S×,A×,T×,s× ,L,AP×)betheMDPcom-\nφ init andfailswithprobability0.1. Incaseoffailure,theagent\nputedastheproductoftheoriginalMDPMandthecor-\ndoesnotmove. Wallsareconsideredtobeabsorbing,i.e.,\nrespondingDRAA thatmonitorsprogressofsatisfaction\nφ theagentwillnotbeabletomoveafterhittingawall. We\nforagivenLTLspecificationφ. Whilethetransitionproba-\nhaveconductedexperimentsto(1)evaluatetheempirical\nbilitymatrixT isunknown,wecanfindtheMECswithin\nperformance of our algorithcm, (2) observe how episode\nM throughlearningthegraphχ (seeAlg.5)andcom-\nφ M length vary throughout the run of our algorithm, and (3)\nputingitsproductwiththeDRAA . InSec.A,weshow\nφ assessthesamplecomplexityofourmethod.\napolynomialalgorithmforlearningtheunderlyinggraph\noftheMDPM usingtheknowledgeofapositivelower Empiricalperformance. Fig.2illustratesthevariationsof\nφ\nboundovertheminimumtransitionprobabilitiesinT. Once empiricalmeanforthenormalizedregretR(K)/K forour\nweknowthegraphofM ,wecanuseAlgorithm47from regret-freealgorithmwhichisrunforthegridworldexample\nφ\n(Baier&Katoen,2008)tocharacterizealloftheMECsin with l = 6. We set δ = 0.1 over 10 runs. Furthermore,\nGandB. Alg.4outlinesourproposedonlineregret-free we group all of the cells associated with the wall into an\nmethodforsolvingthecontrollersynthesisproblemagainst absorbingstateB,suchthatwehave|S|=17and|A|=4.\nLTLspecifications. Thetargetspecificationisφ=¬BUG. Itcanbeobserved\n8\nRegret-FreeReinforcementLearningforLTLSpecifications\nFigure 2: Variations of the empirical normalized regret Figure3: Comparisonoftheempiricalnormalizedregret\n(R(K)/K)whenourproposedalgorithmisimplemented betweenourproposedregret-freealgorithmandtheω-PAC\nforthegridworldexamplewithl=6. algorithm (Perez et al., 2023) for the gridworld example\nwithl=4\nthattheempiricalmeanofregretdropsveryquickly,which\nimplies that the algorithm successfully finds an optimal k∗ (0.1,0.1)andk∗ (0.1,0.1)forgridworldexamples\nreg PAC\npolicywithinthefewfirstepisodes. with4≤l ≤16. Notethatchangesinlinfluencessizeof\nstatespace(|S|=(l−2)2+1),optimalpolicy’saverage\nWealsocomparetheperformanceofourproposedmethod\ntime for hitting the goal (Γ = 2(l−3)+1) and also the\nwiththeω-PACalgorithmproposedin(Perezetal.,2023),\n(minimum) ε-recurrence time T = (l −2)2 +1. It can\nwhich is the only existing method that supports guaran-\nbe observed that our algorithm provides a tighter bound\nteedcontrollersynthesisagainstinfinite-horizontemporal\nspeciallyforthelargerexamples.\nspecifications. The ω-PAC algorithm takes a confidence\nparameterδ ∈ (0,1)andaprecisionparameterε ∈ (0,1)\nandprovidesapolicywhichhasε-optimalsatisfactionprob- 7.DiscussionandConclusions\nabilitywithconfidence1−δ. Fig.3illustratesthatourpro-\nIn this paper, we proposed a regret-free algorithm for\nposedalgorithmconvergesmuchfaster. Webelievethatthis\nthecontrolsynthesisproblemoverMDPsagainstinfinite-\nisbecauseouralgorithmusestheintermediateconfidence\nhorizonLTLspecifications. Thedefinedregretquantifies\nbounds,whiletheω-PACalgorithmwaitsuntilenoughmany\nthe accumulated deviation over the probability of satisfy-\nsamplesarecollected,andonlythenupdatesitspolicy.\ningthegivenLTLspecification. Below,wehavediscussed\nEpisodelengthvariations. Fig.4illustratesthevariations severalaspectsoftheproposedscheme.\ninH fordifferentepisodes. Initially,ouralgorithmassigns\nk\nPossibilityofapplyingtheregret-freealgorithmsthatare\nverysmallvaluestoH ,becausetheexpectedtimetoreach\nk\nproposedforSSP.Theassumptionsthatareneededforsolv-\nGintheoptimisticMDPissmall. Astheempiricaltransi-\ningSSPinaregretfreeway,areinherentlynon-applicable\ntionprobabilitiesbecomemoreprecise,theestimationover\nto our target setting as we discussed in the related work\ntheexpectedtimetoreachGtakesmoreaccuratevalues.\nsection. Inparticular,relaxingtherequirementforexistence\nSamplecomplexity. Althoughourmethodandω-PACalgo- ofaproperpolicy(whichisequivalenttotheexistenceofa\nrithmprovidedifferentguarantees,werelatethemthrough policywhichsatisfiesthegivenLTLspecificationwithprob-\ndefinitionofarelatedcomplexitymetric. Thesamplecom- abilityone)requiresattributingalargeenoughcosttothe\nplexity of the ω-PAC algorithm is characterized with C policieswhichmayendupvisitingthenon-acceptingMECs.\nthatisthenumberoflearningepisodeswithnon-ε-optimal Suchanassumption(eveninthecasefindingsuchanupper\nsatisfaction probability.We define k∗ (δ,ε) as the small- boundisfeasible)wouldautomaticallyrequiredefininga\nreg\nest number of episodes k for which our regret-free algo- coststructureovertheproductMDPwhichwouldinturn\nrithm with confidence 1−δ satisfies R(k) ≤ ε. Further- changethefunctionofregretsothatitcanonlybedefined\nk\nmore, we define k∗ (δ,ε) as the minimum number of withrespecttotheaccumulatedcostandnottheprobability\nPAC\nepisodesafterwhichwithconfidence1−δ theω-PACal- ofsatisfyingthegivenspecification. However, wedesire\ngorithmsatisfies Ck ≤ε. Fig.5illustratesthevariationsof knowingthevalueofregretwithrespecttothesatisfaction\nk\n9\nRegret-FreeReinforcementLearningforLTLSpecifications\nFigure4: Variationsofthecomputeddeadline(H )forthe Figure5:Comparisonofthetheoreticalsamplecomplexities\nk\ngridworldexamplewithl=6. forourproposedalgorithmandtheω-PACalgorithm(Perez\netal.,2023),whenappliedtothegridworldexamplewith\nvarioussizes(4≤l≤16).\nprobabilityandnotanyartificiallydefinedaccumulatedcost.\nPossibilityofapplyingUCRL2algorithm. UCRL2isa 003.0011. URL http://dx.doi.org/10.7551/\nusefulregret-freealgorithmwhichisproposedforcommu- mitpress/7503.003.0011.\nnicatingMDPswithboundeddiameter. Oursettingisnot\nappropriateforapplyingUCRL2astheproductMDPwould Auer, P., Jaksch, T., and Ortner, R. Near-optimal regret\nnotbecommunicatingafterall. However,itmaybenoticed boundsforreinforcementlearning. Advancesinneural\nthatoursecondassumption,i.e.,theknowledgeoveranup- informationprocessingsystems,21,2008.\nperboundfortheexpectedhittingtimeoftheoptimalpolicy\nBaier, C.andKatoen, J.-P. Principlesofmodelchecking.\nisaweakerassumptionwithrespecttothemainassumption\nMITpress,2008.\nforUCRL2thatistheknowledgeoverthediameterofthe\nunderlyingMDP. Bozkurt, A. K., Wang, Y., Zavlanos, M. M., and Pajic,\nM. Model-free reinforcement learning for stochastic\nReferences games with linear temporal logic objectives. In 2021\nIEEEInternationalConferenceonRoboticsandAutoma-\nAgarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and\ntion(ICRA),pp.10649–10655.IEEEPress,2021.doi:10.\nSchapire, R. Taming the monster: A fast and sim- 1109/ICRA48506.2021.9561989. URLhttps://doi.\nple algorithm for contextual bandits. In Xing, E. P. org/10.1109/ICRA48506.2021.9561989.\nand Jebara, T. (eds.), Proceedings of the 31st Interna-\ntional Conference on Machine Learning, volume 32 Cai,M.,Xiao,S.,Li,B.,Li,Z.,andKan,Z. Reinforcement\nof Proceedings of Machine Learning Research, pp. learning based temporal logic control with maximum\n1638–1646, Bejing, China, 22–24 Jun 2014. PMLR. probabilisticsatisfaction. CoRR,abs/2010.06797,2020.\nURLhttps://proceedings.mlr.press/v32/ URLhttps://arxiv.org/abs/2010.06797.\nagarwalb14.html.\nCamacho,A.,Icarte,R.T.,Klassen,T.Q.,Valenzano,R.A.,\nandMcIlraith,S.A. Ltlandbeyond: Formallanguages\nAlur,R.,Bansal,S.,Bastani,O.,andJothimurugan,K. A\nforrewardfunctionspecificationinreinforcementlearn-\nframeworkfortransformingspecificationsinreinforce-\ning. InInternationalJointConferenceonArtificialIntel-\nmentlearning. InPrinciplesofSystemsDesign: Essays\nligence,2019.\nDedicatedtoThomasA.HenzingerontheOccasionof\nHis60thBirthday,pp.604–624.Springer,2022. Dann, C., Lattimore, T., and Brunskill, E. Unifying pac\nand regret: uniform pac bounds for episodic reinforce-\nAuer, P. and Ortner, R. Logarithmic Online Re- mentlearning. InProceedingsofthe31stInternational\ngret Bounds for Undiscounted Reinforcement Learn- ConferenceonNeuralInformationProcessingSystems,\ning, pp. 49–56. The MIT Press, September 2007. NIPS’17, pp. 5717–5727, Red Hook, NY, USA, 2017.\nISBN 9780262256919. doi: 10.7551/mitpress/7503. CurranAssociatesInc. ISBN9781510860964.\n10\nRegret-FreeReinforcementLearningforLTLSpecifications\nFruit, R., Pirotta, M., and Lazaric, A. Near optimal Oura, R., Sakakibara, A., and Ushio, T. Reinforcement\nexploration-exploitationinnon-communicatingmarkov learningofcontrolpolicyforlineartemporallogicspeci-\ndecision processes. In Proceedings of the 32nd Inter- ficationsusinglimit-deterministicgeneralizedbu¨chiau-\nnationalConferenceonNeuralInformationProcessing tomata. IEEE Control Systems Letters, 4(3):761–766,\nSystems,NIPS’18,pp.2998–3008,RedHook,NY,USA, July2020. ISSN2475-1456. doi: 10.1109/lcsys.2020.\n2018.CurranAssociatesInc. 2980552. URL http://dx.doi.org/10.1109/\nLCSYS.2020.2980552.\nFu,J.andTopcu,U. ProbablyapproximatelycorrectMDP\nlearningandcontrolwithtemporallogicconstraints. In Perez, M., Somenzi, F., and Trivedi, A. A PAC learning\nRobotics: ScienceandSystemsX,UniversityofCalifor- algorithmforLTLandomega-regularobjectivesinmdps.\nnia,Berkeley,USA,July12-16,2014,2014. CoRR, abs/2310.12248, 2023. doi: 10.48550/ARXIV.\n2310.12248. URLhttps://doi.org/10.48550/\nHahn,E.M.,Perez,M.,Schewe,S.,Somenzi,F.,Trivedi,A.,\narXiv.2310.12248.\nandWojtczak,D.Omega-regularobjectivesinmodel-free\nreinforcementlearning. InToolsandAlgorithmsforthe Rosenberg, A., Cohen, A., Mansour, Y., and Kaplan, H.\nConstructionandAnalysisofSystems: 25thInternational Near-optimalregretboundsforstochasticshortestpath.\nConference,TACAS2019,HeldasPartoftheEuropean In International Conference on Machine Learning, pp.\nJoint Conferences on Theory and Practice of Software, 8210–8219.PMLR,2020.\nETAPS2019,Prague,CzechRepublic,April6–11,2019,\nProceedings, Part I, pp. 395–412, Berlin, Heidelberg, Sickert, S., Esparza, J., Jaax, S., andKˇret´ınsky´, J. Limit-\n2019.Springer-Verlag. ISBN978-3-030-17461-3. doi: deterministic bu¨chi automata for linear temporal logic.\n10.1007/978-3-030-17462-0 27. URLhttps://doi. InChaudhuri,S.andFarzan,A.(eds.),ComputerAided\norg/10.1007/978-3-030-17462-0_27. Verification,pp.312–332,Cham,2016.SpringerInterna-\ntionalPublishing. ISBN978-3-319-41540-6.\nHasanbeig, M., Kantaros, Y., Abate, A., Kroening, D.,\nPappas, G. J., and Lee, I. Reinforcement learning Srinivas,N.,Krause,A.,Kakade,S.M.,andSeeger,M.W.\nfor temporal logic control synthesis with probabilistic Information-theoreticregretboundsforgaussianprocess\nsatisfaction guarantees. In 2019 IEEE 58th Confer- optimization in the bandit setting. IEEE Transactions\nence on Decision and Control (CDC), pp. 5338–5343. on Information Theory, 58(5):3250–3265, 2012. doi:\nIEEE Press, 2019. doi: 10.1109/CDC40024.2019. 10.1109/TIT.2011.2182033.\n9028919. URL https://doi.org/10.1109/\nTarbouriech,J.,Garcelon,E.,Valko,M.,Pirotta,M.,and\nCDC40024.2019.9028919.\nLazaric,A. No-regretexplorationingoal-orientedrein-\nIcarte,R.T.,Klassen,T.Q.,Valenzano,R.A.,andMcIlraith, forcement learning. In III, H. D. and Singh, A. (eds.),\nS.A. Usingrewardmachinesforhigh-leveltaskspecifi- Proceedingsofthe37thInternationalConferenceonMa-\ncationanddecompositioninreinforcementlearning. In chineLearning,volume119ofProceedingsofMachine\nInternationalConferenceonMachineLearning,2018. Learning Research, pp. 9428–9437. PMLR, 13–18 Jul\n2020.URLhttps://proceedings.mlr.press/\nKazemi,M.,Perez,M.,Somenzi,F.,Soudjani,S.,Trivedi, v119/tarbouriech20a.html.\nA.,andVelasquez,A. Translatingomega-regularspeci-\nficationstoaverageobjectivesformodel-freereinforce- Voloshin,C.,Le,H.M.,Chaudhuri,S.,andYue,Y. Policy\nmentlearning. InProceedingsofthe21stInternational optimizationwithlineartemporallogicconstraints. In\nConferenceonAutonomousAgentsandMultiagentSys- AdvancesinNeuralInformationProcessingSystems35:\ntems, AAMAS ’22, pp. 732–741, Richland, SC, 2022. Annual Conference on Neural Information Processing\nInternational Foundation for Autonomous Agents and Systems 2022, NeurIPS 2022, New Orleans, LA, USA,\nMultiagentSystems. ISBN9781450392136. November28-December9,2022,2022.\nKearns,M.andSingh,S. Near-optimalreinforcementlearn- Yang,C.,Littman,M.,andCarbin,M. Onthe(in)tractabil-\ninginpolynomialtime. MachineLearning,49:209–232, ity of reinforcement learning for ltl objectives. arXiv\n2002. URL https://api.semanticscholar. preprintarXiv:2111.12679,2021.\norg/CorpusID:2695116.\nLatouche,G.andRamaswami,V. IntroductiontoMatrix\nAnalytic Methods in Stochastic Modeling. Society for\nIndustrialandAppliedMathematics,1999. doi: 10.1137/\n1.9780898719734. URL https://epubs.siam.\norg/doi/abs/10.1137/1.9780898719734.\n11\nRegret-FreeReinforcementLearningforLTLSpecifications\nAlgorithm5Proposedalgorithmforlearninggraphs\nInput: Initialstates ,minimumtransitionprobabilityp(t) ,stateandactionsetsS andA.\ninit min\nComputen∗,thatisminimumnumberofsamplesrequiredtodeterminealloutgoingtransitionsforanystate-actionpair\n(s,a)∈S×Awithconfidence1−δ ,assumingminimumtransitionprobabilityp ,\n1 min\nE ←∅\nt←0\nfors∈S\\s do\ninit\nComputeapolicyπ(s)underwhichsisreachablefroms\ninit\nfora∈Ado\nn←#{t′ <t: s =s,a =a}\nt t\nwhilen<n∗do\ns ←s\nt init\nRunthetransitionfromstate-actionpair(s ,π(s)(s ))andobserves\nt t t+1\nifs =sthen\nt\nn←n+1\nbreak\nendif\nt←t+1\nendwhile\nendfor\nendfor\nA.GraphIdentification\nInordertoproposearegret-freecontrollersynthesismethod,weneedtoknowtheunderlyingMECs. Inthissection,we\nshowhowtousetheknowledgeofminimumtransitionprobabilityp ∈R>0ofagivenMDPMinordertoidentifythe\nmin\nunderlyinggraphoftheMDPwhichisvalidwithadesiredconfidence. Bythefollowinglemma,wecandeterminethe\nnumberofsamplesneededinordertoensurewhetheratransitionexistsornot.\nLemma A.1. (Voloshin et al., 2022) For any state-action pair (s,a) ∈ S ×A, let Tˆ (s′ | s,a) denote the empirical\nn\ntransitionprobabilityestimationforthetransitionprobabilityT(s′ |s,a)aftersampling(s,a)forntimes. Givenapositive\nlower bound over the minimum transition probability p ∈ (0,1) and a confidence parameter δ ∈ (0,1), we have\nmin\n(s,a,s′)∈/ E withconfidence1−δifTˆ (s′ |s,a)=0,for\nn∗\nn∗ >ψ−1(p ) (20)\nmin\n,where\n(cid:114)\n1 7\nψ(n)= ζ(n)+ ζ(n), (21)\n2 3\nandζ(n)=\nlog(4n2|S|2|A|pmin)\nifn≥1.\n(n−1)\nRemarkA.2. Tocomputepoliciesπ(s) fors ∈ S,oneneedstouseundiscountedRLformulationswhichexplicitlycan\nhandletheexploration-exploitationtrade-off. E3 (Kearns&Singh,2002)andω-PAC(Perezetal.,2023)aretwosuch\napproacheswhichuseε-returnmixingtimeandε-recurrencetime,respectively,inordertoavoidunboundedexplorations.\nInourcase,wecoulddefineanε-coveringtimeforapolicyπ(s),asthenumberoftimestepsrequiredtovisitsforn∗times\nwithprobabilityatleast1−εforε∈(0,1). SimilartothetheoreticalguranateesofmethodslikeE3,wecouldeasillysee\nthatourgraphlearningalgorithmhasasamplecomplexitythatispolynomialinthesizeofstateandactionspacesand\nmaximumε-coveringtimeamongallpolicies.\nAlg.5outlinesourproposedalgorithmtolearnthegraphofagivenMDP.Therearetwomainsteps: (1)foreverystate\ns ∈ S,weutilizeanRLalgorithmtogetapolicyπ(s) underwhich,sisreachablefroms (withpositiveprobability);\ninit\n(2)werunπ(s)foreverya∈Auntil(s,a)isvisitedforn∗times;uponreaching(s,a),wecollecttheresultedoutgoing\ntransitionbyrunningtheMDP.Withintherestofthispaper,wearegoingtousetheMDPgraphχ whichiscorrectwith\nM\nconfidence1−δ.\n12\nRegret-FreeReinforcementLearningforLTLSpecifications\nB.Proofs\nB.1.ProofofLem.4.2\nProof. Assumingthatλ∗isfiniteimpliesthatthereexistsatleastonepolicyunderwhichGisreachablefroms . Under\ninit\nsuchapolicy,intheworst-casescenario,eachstateintheMarkovchainmustbevisitedatleastonce. Visitingeverystate\nrequiresfollowingapathoflength|S|,whichoccurswithprobabilityp|S| . Afterlattemptsoftraversingthispath,the\nmin\nprobabilityofsuccessatleastonceisgivenby1−(1−p|S| )l.Ifl≥ log(δ) ,then1−(1−p|S| )l ≥1−δ.Finally,since\nmin log(1−p|S| ) min\nmin\neachofthelattemptstakes|S|stepsintheworstcase,thetotalnumberofstepsisboundedbyΛ≤|S|l=|S| log(δ) .\nlog(1−p|S| )\nmin\nB.2.ProofofLem.4.5\nProof. First,usingtheMarkov’sinequality(sincex(cid:55)→xr isanon-decreasingmappingfornon-negativereals),wehave\nE[λ˜ (s)r]\nP(λ˜ (s)≥H −1)≤ k .\nk k (H −1)r\nk\nNow,wenotethatbyLem.15in(Tarbouriechetal.,2020),wehaveifλ˜ (s)≤λforeverys∈S\\(G∪B)andλ≥2,then\nk\nE(λ˜ (s)r)≤2(rλ)r,\nk\nforanyr ≥1. Therefore,substitutingλwithΛ,wewillhave\n2(rΛ)r\nP(λ˜ (s)≥H −1)≤ , (22)\nk k (H −1)r\nk\nNotethatthereexistsy ∈S suchthat\n∥Q˜Hk−2∥\n∞\n=1⊤ yQ˜Hk−21=P(λ˜ k(y)>H k−2)\n=P(λ˜ (y)≥H −2). (23)\nk k\n√\nBydefinitionofH\nk\nwehave∥Q˜Hk−2∥\n∞\n>1/ k. CombiningthiswithEqs.(22),(23)yields\n2(rΛ)r √\n>1/ k,\n(H −1)r\nk\nwhichimpliesthat √\nH −1<rΛ(2 k)1/r.\nk\n(cid:108) √ (cid:109)\nByselectingr = log(2 k) ,weget\n(cid:108) 1 (cid:109)\n√ √ √ (cid:108) √ (cid:109)\nlog(2 k)\nH −1<⌈log(2 k)⌉Λ(2 k) ≤ 3Λlog(2 k) .\nk\nHence (cid:108) √ (cid:109)\nα ≤ 3Λlog(2 K) .\nK\nInordertoprovethesublinearregretbound,wemakeuseoftheAzuma-Hoeffdinginequality.\nLemmaB.1(Azuma-Hoeffdinginequality,Hoeffding1963). LetX ,X ,... beamartingaledifferencesequencewith\n1 2\n|X |≤cforalll. Thenforallγ >0andn∈N,\nl\n(cid:88)n γ2\nP{ X ≥γ}≤exp(− ).\nl 2nc2\nl=1\nNow,weproceedbyshowingwhy(cid:80)K ∆(f,1)growssublinearlywithK.\nk=1 k\n13\nRegret-FreeReinforcementLearningforLTLSpecifications\nB.3.ProofofLem.4.6\nProof. Inordertoreformulatetheregret,wedefinethefollowingrewardfunctionr: S →{0,1}\n(cid:40)\n0 s∈/ G\nr(s)= . (24)\n1 s∈G\nFurther,forthetimestephwithinepisodekwedefine\nHk,I (cid:88)k(h)−1\nΘ (s ):=v˜ (s )− r(s )\nk,h k,h k k,h k,t\nt=h\nwhereI : [1;H ]→[1;I ]mapsthetimepointsinepisodekintothecorrespondinginterval,andH denotesthelength\nk k k k,i\noftheithintervalwithinthekthepisodefor1≤i≤I andH =1. Therefore,wehave\nk k,0\n(cid:88)K\n∆(1)\n=(cid:88)K (cid:88)Ik\nΘ (s ).\nk k,Hk,i−1 k,Hk,i−1\nk=1 k=1i=1\nLetusfurtherdefine\n(cid:88)\nΦ :=v˜ (s )− p(y |s ,π˜ (s ))v˜ (y), (25)\nk,h k k,h+1 k,h k k,h k\ny∈S\nwherep(·|·,·)correspondstothetransitionprobabilityinthetrueMDPM . Similarly,wedenotebyp˜ (·|·,·)forthe\nφ k\ntransitionprobabilityintheoptimisticMDPM˜ . Notethatfors ∈G∪B,wehaveΘ (s )=0. Fors ∈/ G∪B,\nk k,h k,h k,h k,h\nwehave\nHk,I (cid:88)k(h)−1\nΘ (s )=v˜ (s )− r(s )\nk,h k,h k k,h k,t\nt=h\n≤L˜ v˜ (s )+ε\n−Hk,I (cid:88)k(h)−1\nr(s )\nk k k,h k k,t\nt=h\n(cid:88)\n= p˜ (y |s ,π˜ (s ))v˜ (y)\nk k,h k k,h k\ny∈S\nHk,I (cid:88)k(h)−1\n+ε −r(s )− r(s )\nk k,h k,t\nt=h+1\n(cid:88)\n= (p˜ (y |s ,π˜ (s ))\nk k,h k k,h\ny∈S\n−p(y |s ,π˜ (s )))v˜ (y)\nk,h k k,h k\n(cid:88)\nHk,I (cid:88)k(h)−1\n+ p(y |s ,π˜ (s ))v˜ (y)+ε − r(s )\nk,h k k,h k k k,t\ny∈S t=h+1\n≤2β (s ,π˜ (s ))×1\nk k,h k k,h\n(cid:88)\n+( p(y |s ,π˜ (s ))v˜ (y)−v˜ (s ))\nk,h k k,h k k k,h+1\ny∈S\nHk,I (cid:88)k(h)−1\n+ε +(v˜ (s )− r(s ))\nk k k,h+1 k,t\nt=h+1\n≤2β (s ,π˜ (s ))+Φ +ε +Θ (s ),\nk k,h k k,h k,h k k,h+1 k,h+1\nwhereweusedEq.(6)forthefirstinequality, thefactthatr(s ) = 0foreverys ∈/ G∪B forthesecondequality,\nk,t k,t\nv˜ (y)≤1foreveryy ∈S forthethirdequality,definitionofβ (Eq.(4))forthesecondinequality,anddefinitionofΦ\nk k k,h\n14\nRegret-FreeReinforcementLearningforLTLSpecifications\n(Eq.(25))forthelastinequality. Also,notethatBytelescopicsumweget\nHk,i+1−2\n(cid:88)\nΘ (s )= (Θ (s )−Θ (s ))\nk,Hk,i k,Hk,i k,h k,h k,h+1 k,h+1\nh=Hk,i\n+Θ (s )\nk,Hk,i+1−1 k,Hk,i+1−1\nHk,i+1−2\n(cid:88)\n≤ (2β (s ,π˜ (s ))+Φ +ε )\nk k,h k k,h k,h k\nh=1\n+Θ (s )\nk,Hk,i+1−1 k,Hk,i+1−1\nHk,i+1−2\n(cid:88)\n≤ 2β (s ,π˜ (s ))\nk k,h k k,h\nh=1\nHk,i+1−2\n(cid:88)\n+ Φ +H ε\nk,h k,i+1 k\nh=1\nwhere we used the arguments we made in the previous step of the proof to establish the first inequality, the fact that\nΘ (s ) = 0 by definition as s ∈ G∪B for every 1 ≤ i ≤ I for the last inequality . By\nk,Hk,i+1−1 k,Hk,i+1−1 k,tk,i+1−1 k\nsummingoveralloftheepisodeswehave\n(cid:88)K (cid:88)Ik\nΘ (s )\nk,Hk,i−1 k,Hk,i−1\nk=1i=1\n(cid:88)K (cid:88)Ik H (cid:88)k,i−1\n≤ Φ\nk,h\nk=1i=1h=Hk,i−1\n(cid:88)K (cid:88)Ik H (cid:88)k,i−1\n+2 β (s ,π˜ (s ))\nk k,h k k,h\nk=1i=1h=Hk,i−1\nK\n(cid:88)\n+ H ε . (26)\nk k\nk=1\nInordertoboundthefirstterm,wenotethat\n(cid:88)K (cid:88)Ik H (cid:88)k,i−1 (cid:88)K tk(cid:88)+1−1\nΦ = Φ\nk,h k,h\nk=1i=1h=Hk,i−1 k=1 h=tk\nTherefore,wecanwrite\n (cid:118) \nP\n(cid:88)K tk(cid:88)+1−1\nΦ\nk,h\n≥2(cid:117) (cid:117) (cid:116)2((cid:88)K\nH\nk)log(2((cid:80)K\nk=\nδ1H k)2\n)\nk=1 h=tk k=1\n(cid:32) (cid:88)K tk(cid:88)+1−1 (cid:114) 2n2 (cid:88)K (cid:33)\n≤P Φ ≥2 2nlog( )∩ H =n .\nk,h δ k\nk=1 h=tk k=1\nLetG denotethehistoryofallrandomeventsupto(andincluding)stephofepisodek,i.e.,q =\n(cid:80)k−1\nH +h. We\nq k′=1 k′\nhaveE(Φ |G )=0,andfurthermoreH isselectedatthebeginningofepisodek,andsoitisadaptedwithrespectto\nk,h q k\nG . HenceΦ isamartingaledifferencewith|Φ | ≤ 1. Therefore,byAzuma-Hoeffding’sinequality,wehavewith\nq k,h k,h\nprobability1− 2δ\n3\n (cid:118) \nP\n(cid:88)K tk(cid:88)+1−1\nΦ\nk,h\n≥2(cid:117) (cid:117) (cid:116)2((cid:88)K\nH\nk)log(3((cid:80)K\nk=\nδ1H k)2\n).\nk=1 h=tk k=1\n15\nRegret-FreeReinforcementLearningforLTLSpecifications\nNow,weproceedtoboundthesecondterminEq.26. LetN(1)denotethenumberofsamplesonlycollectedduringattempts\ninphase(1)andN(1)+ =max{1,N(1)}. AlsoletT denotethenumberoftimestepstakenwithintheepisodeswhich\nK,1\nendbeforethedeadlineH . Wecanwrite\nk\n(cid:88)K H (cid:88)k−1(cid:115)\n1\n(cid:88)N (cid:88)K(1)(cid:114)\n1\n≤\nN(1)(s ,π˜ (s )) n\nk=1 h=1 k k,h k k,h s,a n=1\n≤2(cid:112) |S||A|(cid:115) (cid:88) N(1)(s,a)≤2(cid:113)\n|S||A|T .\nK K,1\ns,a\n(cid:113)\nNow,firstnotethatbyLem.4in(Tarbouriechetal.,2020)wehavethatmappings log(cx) isanon-increasingfunctionfor\nx\nc≥4andx≥1. AlsonotingthatN+(s,a)≥N(1)+(s,a),wegetfor|A|≥2that\nk k\n(cid:115)\n8|S|log(2|A|N+/δ)\nβ (s,a)= k\nk max(1,N+(s,a))\nk\n(cid:118)\n(cid:117) (cid:117)8|S|log(2|A|N(1)+(s,a)/δ)\n≤(cid:116) k .\nmax(1,N(1)+(s,a))\nk\nTherefore,weobtain\n(cid:88)K H (cid:88)k−1\nβ (s ,π˜ (s\n))≤2|S|(cid:114)\n8|A|T\nlog(2|A|T\nk,1).\nk k,h k k,h K,1 δ\nk=1 h=1\nFinally,weboundthelastterminEq.(26).\n(cid:88)K\nH ε\n≤T (cid:88)K,1 α\nK ≤α\n1+log(Kα K)\n,\nk k 2t K 2\nk=1 t=1\nwhereα =max H .Puttingeverythingtogetheryieldsthatinequality(16)holdswithprobabilityatleast1−δ.\nK 1≤k≤K k\nB.4.ProofofLem.4.7\nProof. We define X := v (s )−v (s ). Note that E(X ) = 0 and |X | ≤ 1 for every 1 ≤ k ≤ K and\nk,i k init k,i init k,i k,i\n1≤i≤I . Wehave\nk\n∆(f,2)\n=(cid:88)Ik\nv (s )−v (s\n)=(cid:88)Ik\nX .\nk k init k,i init k,i\ni=1 i=1\nThereforebyapplicationoftheAzuma-HoeffdinglemmaandusingthefactthatKα\n≥(cid:80)K\nI weget\nk k=1 k\nK (cid:114)\nP{(cid:88) ∆(f,2) ≥ 2Kα log1 }\nk K δ\nk=1\n2Kα log1\n≤exp(− K δ)\n2(cid:80)K\nI\nk=1 k\n1\n≤exp(−log )=δ.\nδ\nInLemma4.5,wehavealreadyshownthatα growslogarithmicallywithK whichprovesthat(cid:80)K ∆(f,2)growssub\nK k=1 k\nlinearlywithK.\nLetF denotethenumberofepisodesinwhichGisnotreachedwithinthephase(1). Byusingargumentssimilartothe\nK\nonesstatedinLem.7of(Tarbouriechetal.,2020),wecanstatethatwithhighprobabilityF growssublinearly.\nK\n16\nRegret-FreeReinforcementLearningforLTLSpecifications\nB.5.ProofofLem.4.8\nProof. Letτ andλ˜ denotethehittingtimesofpolicyπ˜ inthetrueandoptimisticmodels,respectively. Wedefine\nk k k\nΓ (s )=1 −P(λ˜ ((s ))>H −h).\nk,h k,h τk(sk,h)>Hk−h k k,h k\nNotethatwehave\nK\n(cid:88)\nF = 1\nK τk(sk,1)>Hk−1\nk=1\nK K\n=(cid:88) Γ (s )+(cid:88) P(λ˜ (s )>H −1).\nk,1 k,1 k init k\nk=1 k=1\nLet\n . \nQ˜ . . β˜\nP˜′ = ..k . ... ..k . ,\nk\n . \n.\ng . 0\ndenotethetransitionprobabilitymatrixoftheDTMCcreatedbyconnectingBtos . Letp˜′(·|·,·)denotethetransition\ninit k\nprobabilityintheoptimisticmodelM˜′ (thatistheoptimisticMDPconstructedfromM˜ byconnectingstatesinBinto\nk k\ns ). Similarly,letp′(·|·,·)denotethetransitionprobabilityintheMDPM′ (thatistheMDPconstructedfromM by\ninit p p\nconnectingstatesinBintos ). Sincefor1≤h≤H −1,1 =1 wehave\ninit k τk(sk,h)>Hk−h τk(sk,h+1)>Hk−h−1\nΓ (s )=1\nk,h k,h τk(sk,h+1)>Hk−h−1\n−(cid:88) p˜′(y |s ,π˜ (s ))P(λ˜ (y)>H −h−1)\nk k,h k k,h k k\ny∈S\n≤1\nτk(sk,h+1)>Hk−h−1\n−((cid:88) p˜′(y |s ,π˜ (s ))−p′(y |s ,π˜ (s )))P(λ˜ (y)>H −h−1)\nk k,h k k,h k,h k k,h k k\ny∈S\n−(cid:88) p′(y |s ,π˜ (s ))P(λ˜ (y)>H −h−1)\nk,h k k,h k k\ny∈S\n≤1 +2β (s ,π˜ (s ))\nτk(sk,h+1)>Hk−h−1 k k,h k k,h\n−(cid:88) p′(y |s ,π˜ (s ))P(λ˜ (y)>H −h−1)\nk,h k k,h k k\ny∈S\n=Γ (s )+ψ +2β (s ,π˜ (s )),\nk,h+1 k,h+1 k,h k k,h k k,h\nwhereweestablishedthefirstinequalitybyaddingandsubtractingthetermp′(y |s ,π˜ (s )))P(λ˜ (y)>H −h−1),\nk,h k k,h k k\nthesecondinequalitybyusingthedefinitionofβ (Eq.(4)),andthelastequalitybyusingthefollowingdefinition\nk\nψ =P(λ˜ (s )>H −h−1)\nk,h k k,h+1 k\n−(cid:88) p′(y |s ,π˜ (s ))P(λ˜ (y)>H −h−1).\nk,h k k,h k k\ny∈S\nAlso,wehave\nΓ (s )=1 −P(λ˜ (s >0))\nk,Hk k,Hk τk(sk,Hk)>0 k k,Hk\n=1 −1\nτk(sk,Hk)>0 λ˜ k(sk,Hk>0)\n=1 −1 =0.\nsk,Hk∈/G sk,Hk∈/G\n17\nRegret-FreeReinforcementLearningforLTLSpecifications\nUsingthetelescopicsumweget\nH (cid:88)k−1\nΓ (s )= (Γ (s )−Γ (s ))+Γ (s )\nk,1 k,1 k,h k,h k,h+1 k,h+1 k,Hk k,Hk\nh=1\nH (cid:88)k−1 H (cid:88)k−1\n≤ ψ +2 β (s ,π˜ (s )),\nk,h k k,h k k,h\nh=1 h=1\nwherethelastinequalityisachievedbyknowingΓ (s )=0. Therefore,bysummingoverallepisodesweget\nk,Hk k,Hk\n(cid:88)K H (cid:88)k−1 (cid:88)K H (cid:88)k−1\nF ≤ ψ +2 β (s ,π˜ (s ))\nK k,h k k,h k k,h\nk=1 h=1 k=1 h=1\nK\n+(cid:88) P(λ˜ (s )>H −1).\nk init k\nk=1\nLetG denotethehistoryofallrandomeventsupto(andincluding)stephofepisodek,i.e.,q\n=(cid:80)k−1\nH +h. Wehave\nq k′=1 k\nE(ψ | G ) = 0,andfurthermoreH isselectedatthebeginningofepisodek,andsoitisadaptedwithrespecttoG .\nk,h q k q\nHenceψ isamartingaledifferencewith|ψ | ≤ 1. Therefore,usingsimilarargumentsasinproofofLem.4.6,by\nk,h k,h\nAzuma-Hoeffding’sinequality,wehavewithprobability1− 2δ\n3\n(cid:118)\n(cid:88)K H (cid:88)k−1\nψ\n≤2(cid:117) (cid:117) (cid:116)2((cid:88)K\nH\n)log(3((cid:80)K k=1H k)2\n)\nk,h k δ\nk=1 h=1 k=1\n(cid:114)\n3(Kα )2\n≤2 2Kα log( K ).\nK δ\nFurther,inthesameveinasproofofLem.4.6wehave\n(cid:88)K H (cid:88)k−1\nβ (s ,π˜ (s\n))≤2|S|(cid:114)\n8|A|Kα\nlog(2|A|Kα\nK).\nk k,h k k,h K δ\nk=1 h=1\nNow,weneedtobound(cid:80)K P(λ˜ ((s ))>H −1). UsingThm.2.5.3in(Latouche&Ramaswami,1999),wehave\nk=1 k init k\nK K\n(cid:88) P(λ˜ (s )>H −1)=(cid:88) 1 Q˜Hk−11,\nk init k sinit k\nk=1 k=1\nwhere1 denotesthe|S|−1-sizedone-hotvectoratthepositionofstates∈S. Finally,fromHolder’sinequality,wehave\ns\nK K\n(cid:88) P(λ˜ ((s ))>H −1)=(cid:88) 1 Q˜Hk−11\nk init k sinit k\nk=1 k=1\nK K\n≤(cid:88) ∥1 ∥ ∥Q˜Hk−11∥ ≤(cid:88) ∥Q˜Hk−11∥ .\nsinit 1 k ∞ k ∞\nk=1 k=1\nTherefore,bythechoiceofH\nk\n=min{n>1|∥Q˜n k∥\n∞\n≤ √1 k}weget\n(cid:88)K\nP(λ˜ ((s ))>H\n−1)≤(cid:88)K √1 ≤2√\nK.\nk init k\nk\nk=1 k=1\n18",
    "pdf_filename": "Regret-Free_Reinforcement_Learning_for_LTL_Specifications.pdf"
}