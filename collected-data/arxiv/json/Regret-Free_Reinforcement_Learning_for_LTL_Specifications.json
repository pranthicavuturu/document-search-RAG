{
    "title": "Regret-Free Reinforcement Learning for LTL Specifications",
    "abstract": "Reinforcement learning (RL) is a promising method to learn optimal control policies for sys- tems with unknown dynamics. In particular, syn- thesizing controllers for safety-critical systems based on high-level specifications, such as those expressed in temporal languages like linear tem- poral logic (LTL), presents a significant challenge in control systems research. Current RL-based methods designed for LTL tasks typically offer only asymptotic guarantees, which provide no insight into the transient performance during the learning phase. While running an RL algorithm, it is crucial to assess how close we are to achieving optimal behavior if we stop learning. In this paper, we present the first regret-free on- line algorithm for learning a controller that ad- dresses the general class of LTL specifications over Markov decision processes (MDPs) with a finite set of states and actions. We begin by proposing a regret-free learning algorithm to solve infinite-horizon reach-avoid problems. For gen- eral LTL specifications, we show that the syn- thesis problem can be reduced to a reach-avoid problem when the graph structure is known. Addi- tionally, we provide an algorithm for learning the graph structure, assuming knowledge of a mini- mum transition probability, which operates inde- pendently of the main regret-free algorithm.",
    "body": "Regret-Free Reinforcement Learning for LTL Specifications\nRupak Majumdar 1 Mahmoud Salamati 1 Sadegh Soudjani 1\nAbstract\nReinforcement learning (RL) is a promising\nmethod to learn optimal control policies for sys-\ntems with unknown dynamics. In particular, syn-\nthesizing controllers for safety-critical systems\nbased on high-level specifications, such as those\nexpressed in temporal languages like linear tem-\nporal logic (LTL), presents a significant challenge\nin control systems research. Current RL-based\nmethods designed for LTL tasks typically offer\nonly asymptotic guarantees, which provide no\ninsight into the transient performance during the\nlearning phase. While running an RL algorithm, it\nis crucial to assess how close we are to achieving\noptimal behavior if we stop learning.\nIn this paper, we present the first regret-free on-\nline algorithm for learning a controller that ad-\ndresses the general class of LTL specifications\nover Markov decision processes (MDPs) with\na finite set of states and actions. We begin by\nproposing a regret-free learning algorithm to solve\ninfinite-horizon reach-avoid problems. For gen-\neral LTL specifications, we show that the syn-\nthesis problem can be reduced to a reach-avoid\nproblem when the graph structure is known. Addi-\ntionally, we provide an algorithm for learning the\ngraph structure, assuming knowledge of a mini-\nmum transition probability, which operates inde-\npendently of the main regret-free algorithm.\n1. Introduction\nReinforcement learning (RL) is becoming more prevalent in\ncomputing efficient policies for systems with unknown dy-\nnamical models. Although experimental evidence highlights\nthe effectiveness of RL in numerous applications, ensuring\nthe safety and reliability of algorithms is imperative for\ncertain safety-critical systems. In such scenarios, the de-\n1MPI-SWS, Kaiserslautern, Germany. Correspondence to:\nMahmoud Salamati <msalamati@mpi-sws.org>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nvelopment of algorithms that provide explicit performance\nguarantees becomes essential.\nAmong the existing performance guarantees for RL, regret\nminimization has widely been studied recently (Auer et al.,\n2008; Agarwal et al., 2014; Srinivas et al., 2012; Dann et al.,\n2017). For an online learning algorithm, intuitively, re-\ngret is defined as the difference between the accumulated\n(expected) rewards collected by an optimal policy and the\nthe algorithm during learning. Existing regret minimiza-\ntion algorithms assume that optimal policies are positional,\nmeaning that optimal policies (deterministically) map every\nstate into a corresponding action. While this suffices for\nmost of basic reward structures, optimal policies may not be\npositional for more general reward structures. In particular,\nwhen the control objective is set by an LTL formula, optimal\npolicy is, in general, not positional.\nRL for LTL specifications has recently become popular.\nMajority of the existing methods provide no performance\nguarantees (Icarte et al., 2018; Camacho et al., 2019; Hasan-\nbeig et al., 2019; Kazemi et al., 2022; Hahn et al., 2019;\nOura et al., 2020; Cai et al., 2020; Bozkurt et al., 2021; Sick-\nert et al., 2016). Controller synthesis for finite-horizon and\ninfinite-horizon LTL with probably approximately correct\n(PAC) guarantee has recently been studied (Fu & Topcu,\n2014; Voloshin et al., 2022). It is shown that providing PAC\nguarantees for learning algorithms that target controller syn-\nthesis against infinite-horizon specifications requires addi-\ntional knowledge, such as minimum transition probability\n(Alur et al., 2022). However, there exists no regret-free\nRL-based controller synthesis method for LTL tasks.\nIn this paper, we propose an online learning algorithm for\ncontrol synthesis problems against LTL objectives, which\nprovides sublinearly growing regret bounds. Specifically,\nwe want to measure the performance of the learned control\npolicy during learning. For that, we compare the satisfaction\nprobabilities of an optimal policy and sequence of policies\ngenerated by the algorithm during learning.\nWe consider the class of systems, whose dynamics can be\ncaptured by a finite MDP with unknown transition proba-\nbilities and fixed initial state sinit. The control objective is\nto synthesize a control policy that maximizes probability\nof satisfying a given LTL specification φ. Let π∗denote\nan optimal policy, meaning that applying π∗maximizes\n1\narXiv:2411.12019v1  [cs.AI]  18 Nov 2024\n\nRegret-Free Reinforcement Learning for LTL Specifications\nthe satisfaction probability for φ. Running an online al-\ngorithm will produce a sequence of policies π1, π2, . . . ,\none per episode. Let v∗(sinit), vk(sinit) denote, respectively,\nthe satisfaction probabilities of π∗and πk when starting\nfrom sinit. After passing K > 0 episodes, regret is de-\nfined as R(k) := PK\nk=1(v∗(sinit) −vk(sinit)). The algo-\nrithm we propose in this paper is regret free, meaning that\nlimK→∞R(K)/K = 0.\nThere exist regret minimization techniques for solving\ninfinite-horizon controller synthesis problems. On one hand,\nregret-free algorithms such as UCRL2 (Auer et al., 2008)\ncan only be applied to communicative MDPs; for infinite-\nhorizon specifications such as reach-avoid, the represen-\ntative MDP will be alwayse non-communicating. On the\nother hand, there exist online no-regret algorithms that can\nbe applied to stochastic shortest path (SSP), which is closely\nrelated to the infinite-horizon reachability problem in LTL\n(Tarbouriech et al., 2020; Rosenberg et al., 2020). There\nare multiple obstacles which prohibit direct application of\nthose methods to our setting. First, the primary outcome\nwe seek is the regret value with respect to an optimal pol-\nicy’s satisfaction probability, rather than regret based on\nan arbitrary cost structure. Any cost structure suitable for\napplying standard SSP algorithms restricts costs to strictly\npositive values. This restriction prevents a formulation that\ncomputes the accumulated regret relative to the optimal sat-\nisfaction probability, which is our main objective. Second,\none main assumption for the proposed methods attacking\nthe SSP problem is to assume that there exists at least one\npolicy under which the target is reachable with probability\n1. To relax this assumption, existing methods often im-\npose strong requirements, such as knowing an upper bound\non the total accumulated cost and having sufficiently high\ncost assignments for visiting non-accepting end components.\nOur formulation avoids all of these assumptions and only\nrequires knowledge of the minimum transition probability\nin the MDP.\nWe first state our algorithm for reach-avoid specifications,\nwherein a set of goal states must be visited before hitting\na set of bad states. In every episode, our algorithm (1)\ncomputes an interval MDP (iMDP) by taking all of the\ncollected observations into account; (2) finds an optimistic\npolicy over the computed iMDP to solve the target reach-\navoid problem; (3) executes the computed policy before\nan episode-specific deadline reached. We prove that the\nregret of our algorithm grows sublinearly with respect to\nthe number of episodes. For a general LTL specification,\nwe show that the synthesis problem can be reduced to a\nreach-avoid problem, if we know the connection graph of\nthe MDP. Finally, we provide a polynomial algorithm for\nlearning the graph structure of MDPs.\nThe rest of this paper is organized as follows: in Sec. 2,\nwe discuss the existing related literature; in Sec. 3, we pro-\nvide the required notations and definitions; in Sec. 4, we\ndescribe our algorithm for the reach-avoid problem and ex-\nplain derivation of our claimed regret bounds; in Sec. 5,\nwe explain how our regret-free algorithm can be extended\nto solve the controller synthesis problem for general LTL\nspecifications; in Sec. 6, we provide empirical evaluation for\nour proposed method; finally, in Sec. 7, we state some con-\ncluding remarks as well as some meaningful future research\ndirections.\n2. Related Work\nWe discuss existing results in four related domains.\nNon-Guaranteed reinforcement learning techniques for\nLTL specifications. During the course of the past years,\nlots of efforts have been dedicated into solving the controller\nsynthesis problem for systems modeled as finite MDPs with\nunknown transition probabilities and have to fulfill certain\ntasks encoded as formula in LTL. Early results focused only\non the finite-horizon subset of LTL specifications. Icarte\net al. introduced reward machines, which use finite state\nautomata to encode finite-horizon specifications, along with\nspecialized (deep) Q-learning algorithms to support their\napproach (Icarte et al., 2018). Camacho et al. later formal-\nized the automatic derivation of reward machines for finite-\nhorizon subsets of LTL (Camacho et al., 2019). The de-\nvelopment of efficient and compact automata, such as limit\ndeterministic B¨uchi automata (LDBA), for representing LTL\nformulas has led to significant advances in reinforcement\nlearning for the full class of LTL specifications, including\ninfinite-horizon cases (Sickert et al., 2016). Typically, one\nhas to first translate the given LTL formula into an appro-\npriate automaton, such as a Rabin or limit-deterministic\nB¨uchi automaton, and then compute the product of this\nautomaton with the actual MDP to formulate the final (dis-\ncounted) learning problem. This formulation ensures that,\nwith a sufficiently large discount factor (dependent on sys-\ntem dynamics and goal specifications), applying standard\nRL techniques will lead the policy to converge asymptoti-\ncally to the optimal one (Kazemi et al., 2022; Bozkurt et al.,\n2021; Oura et al., 2020). However, these methods fail to\nprovide a finite-time performance, and the critical lower\nbound for the discount factor cannot be known in advance.\nGuaranteed reinforcement learning for LTL specifica-\ntion The two most popular metrics for evaluating the perfor-\nmance of learning algorithms are probably approximately\ncorrect (PAC) and regret bounds. In (Fu & Topcu, 2014), the\nproblem of synthesizing controllers for finite-horizon LTL\nspecifications over finite MDPs with unknown transition\nprobabilities was addressed, and a PAC learning algorithm\nwas proposed. However, its sample complexity explicitly\ndepends on the horizon length, making it unsuitable for full\n2\n\nRegret-Free Reinforcement Learning for LTL Specifications\nLTL with infinite-horizon specifications. A few years later,\na surprising negative result emerged: full LTL is not PAC\nlearnable (Yang et al., 2021; Alur et al., 2022). Upon closer\nexamination, the main issue behind this result is the as-\nsumption of an unknown minimum transition probability. In\n(Voloshin et al., 2022), by assuming the minimum transition\nprobability is known, a PAC-learnable controller synthesis\nwas proposed. However, their approach requires access to\na generative model, allowing data collection from any arbi-\ntrary state-action pair. In many realistic scenarios, this is\nimpractical since initializing the MDP at arbitrary states is\nnot feasible. Our proposed algorithm, however, does not\nrequire access to a generative model. Recently, (Perez et al.,\n2023) proposed another PAC learning algorithm that also\ndoes not rely on a generative model. However, there is still\nno regret-free online algorithm for controller synthesis with\nLTL specifications.\nRegret-free reinforcement learning for communicating\nMDPs UCRL and UCRL2 (Auer et al., 2008; Auer & Ort-\nner, 2007) are well-known regret-free learning algorithms\nproposed for communicating MDPs. The way regret is de-\nfined for communicating MDPs makes them particularly\nsuitable for our objective: regret is measured over an in-\nfinite sequence of states without discounting later obser-\nvations, which aligns perfectly with the requirements of\ninfinite-horizon LTL specifications. Notably, learning in\ncommunicating MDPs can continue indefinitely without the\nneed for resetting. However, we observe that in the problem\nwe aim to solve, even if the underlying system’s MDP is\ncommunicating, its product with the automaton modeling\nthe specification may become non-communicating (Kazemi\net al., 2022). (Fruit et al., 2018) proposed a regret-free al-\ngorithm for non-communicating MDPs, where the initial\nstate is within a non-transient subset of states. In our case,\nthe initial state is fixed and located within a transient subset,\nmaking algorithms like UCRL2, which are designed for\ncommunicating MDPs, inapplicable.\nRegret-free\nreinforcement\nlearning\nfor\nnon-\ncommunicating\nMDPs\nGoal-oriented\nreinforcement\nlearning is a key class of problems in RL, often formulated\nas a shortest path problem (SPP) for MDPs with unknown\ntransition probabilities. Recently, several exciting theoreti-\ncal results have been published (Tarbouriech et al., 2020;\nRosenberg et al., 2020). In particular, the online learning\nalgorithm proposed by (Tarbouriech et al., 2020) provides\nsub-linear regret bounds for the accumulated cost in MDPs,\nassuming (1) the existence of a proper policy that reaches\nthe goal with probability one, and (2) all costs are positive.\nThey argue that (1) can be relaxed if an upper bound on\nthe accumulated cost is known, and (2) by perturbing the\ncosts. However, the assumption of a maximum accumulated\ncost is often unrealistic, and we are more interested in\ncomputing regret bounds over the probability of satisfaction,\nrather than the accumulated cost.\n3. Preliminaries\n3.1. Notation\nFor a matrix X ∈Rm×n, we define the ∞-matrix-norm\n∥X∥∞:= max1≤i≤m\nPn\nj=1 ∥Xij∥. Given two integer\nnumbers a, b s.t. a ≤b, we denote the set of integer numbers\na ≤l ≤b by [a; b].\n3.2. Linear Temporal Logic\nWe consider specifications in the form of formulas in Linear\nTemporal Logic (LTL). Here, we give a brief introduction\nto LTL. For detailed syntax and semantics of LTL, we refer\nto the book by Baier and Katoen (Baier & Katoen, 2008)\nand references therein. We consider LTL specifications with\nsyntax\nψ := p | ¬ψ | ψ1 ∧ψ2 | ⃝ψ | ψ1 U ψ2,\nwhere p ⊂S is an element of the set of atomic propositions\nAP. Let ρ = ρ0, ρ1, . . . be an infinite sequence of elements\nfrom 2AP and denote ρi = ρi, ρi+1, . . . for any i ∈N.\nThen the satisfaction relation between ρ and a property\nψ, expressed in LTL, is denoted by ρ |= ψ. We denote\nρ |= p if ρ0 ∈p. Furthermore, ρ |= ¬ψ if ρ ̸|= ψ and\nρ |= ψ1 ∧ψ2 if ρ |= ψ1 and ρ |= ψ2. For next operator,\nρ |= ⃝ψ holds if ρ1 |= ψ. The temporal until operator\nρ |= ψ1 U ψ2 holds if ∃i ∈N : ρi |= ψ2, and ∀j ∈N :0 ≤\nj < i, ρj |= ψ1. Disjunction (∨) can be defined by ρ |=\nψ1 ∨ψ2 ⇔ρ |= ¬(¬ψ1 ∧¬ψ2). The operator ρ |= ♢ψ\nis used to denote that the property will eventually happen\nat some point in the future. The operator ρ |= □ψ signifies\nthat ψ must always be true at all times in the future. We\nalso define ψ1 →ψ2 with ¬ψ1 ∨ψ2. For a given LTL\nspecification φ, we can monitor satisfaction of φ by running\nan appropriate automaton Aφ = (Q, Σ, δ, qinit, F), which\nconsists of a finite set of states Q, a finite alphabet Σ = 2AP ,\na transition function δ: Q × Σ 7→2Q, an initial state qinit,\nand an accepting condition Acc. For example, the accepting\ncondition in deterministic Rabin automaton (DRA) is in the\nform of tuples (Ji, Ki) | i = 1, . . . , m, consisting of subsets\nJi and Ki of Q. An infinite sequence ρ is accepted by the\nDRA Aφ if there exists at least one pair (Ji, Ki) ∈Acc\nsuch that inf(ρ) ∩Ji = ∅and inf(ρ) ∩Ki ̸= ∅, where\ninf(ρ) is the set of states that appear infinitely often in ρ.\n3.3. MDPs\nLet ∆(X) be the set of probability distributions over the set\nX and AP be a set of atomic propositions. We consider\nMDPs M = (S, A, T, sinit, AP), where S and A denote\nthe finite set of states and actions, respectively, T : S ×\nA × S →∆(S) denotes an unknown transition function,\n3\n\nRegret-Free Reinforcement Learning for LTL Specifications\nsinit ∈S is the initial state of the MDP and AP is the set\nof atomic propositions. We denote labeling function by\nL: S →2AP . Let Π denote the set of all deterministic\npositional policies over M, that is π: S →A for π ∈Π.\nBy fixing a specific policy π ∈Π, the MDP reduces into a\nMarkov chain C = (S, P, sinit, AP).\nFor a given MDP M, maximal end components (MECs)\nare sub-MDPs which are probabilistically closed, that is, (1)\nthere exists a positional policy under which one can travel\nbetween every pair of states within the end component with\nprobability 1, and (2) the end component cannot be exited\nunder any positional policy. An MDP is called communi-\ncating if it is comprised of only one MEC including every\nstate. For an MDP M, the underlying graph is denoted as\nχM = (S, A, E), where χM ⊆S × A × S is defined such\nthat (s, a, s′) ∈E if and only if T(s, a, s′) > 0.\n3.4. Regret Analysis\nWe are interested in synthesizing policies that maximize\nsatisfaction probability of LTL specifications over MDPs\nwith finite set of states and actions and fixed initial state sinit.\nLet v∗(sinit) denote the probability with which the optimal\npolicy π∗satisfies the target specification φ, when started at\nsinit. Learning takes place over consecutive episodes. For\nthe episode k we define vk(sinit) denote the probability of\nsatisfying the target specification φ under the policy πk in\nthe MDP Mφ, when started at sinit. We measure the success\nof the learning algorithm through its corresponding regret\nthat is defined as\nR(K) :=\nK\nX\nk=1\n(v∗(sinit) −vk(sinit)).\n(1)\nIn practice, one is interested to terminate the online algo-\nrithm based on a reasonable stopping criterion. Let us define\nthe normalized regret, as follows:\nRa(K) = R(K)\nK\n.\n(2)\nAn online algorithm is called regret-free if its correspond-\ning regret grows sublinearly with respect to the number of\nepisodes K. Running a regret-free online algorithm enables\nachieving arbitrary small values of normalized regret. There-\nfore, one could fix a threshold ε ∈(0, 1) and terminate the\nalgorithm once the corresponding normalized regret goes\nbelow ε. Therefore, one can consider the smallest number\nof episodes k∗\nreg ∈N after which Ra(k∗\nreg) < ε with confi-\ndence 1 −δ as a complexity metric for the proposed online\nlearning algorithm, with respect to parameters δ, ε ∈(0, 1).\nIntuitively, after k > k∗\nreg many learning episodes, with\nconfidence 1 −δ the average satisfaction probability for the\npolicy computed in the kth episode, will be ε-optimal.\nAlgorithm 1 Regret-free algorithm for reach-avoid specifi-\ncations\nInput: Reach-avoid specification ¬B U G, confidence\nparameter δ ∈(0, 1), state and action sets S and A, initial\nstate sinit.\nInitialization: Set t = 1, s1 = sinit\nfor episodes k = 1, 2, . . . do\nInitialize episode k:\ntk ←t\nSet Nk(s, a) := #{t < tk : st = s, at = a}\nFor all s, s′ ∈S and a ∈A compute the emprical\ntransition function\nˆTk(s′ | s, a) := #{t < tk : st = s, at = a, st+1 = s′}\nmax{1, Nk(s, a)}\nCompute policy ˜πk:\nCharacterize Mk as the interval MDP with the set of\ntransition functions T satisfying the inequality (4)\nUse EVI (Alg. 2) to find a policy ˜πk and optimistic\nMDP ˜\nMk\nCompute the deadline Hk using Eq. (9)\nExecute policy ˜πk\nwhile st /∈G and (t −tk) ≤Hk do\nif st /∈B then\nObserve the next state st+1\nelse\nst+1 ←sinit\nend if\nt ←t + 1\nend while\nend for\n4. Regret-Free Controller Synthesis for\nReach-Avoid Specifications\nIn this section, we study the controller synthesis problem for\nMDPs with unknown (but fixed) transition function against\nreach-avoid specifications. Let G and B be two distinct\natomic propositions. We are interested in finding policies\nwhich can maximize the satisfaction probability for the\n(unbounded) reach-avoid specification φ = ¬B U G. We let\nMφ = (S, A, T, sinit, { G, B }) be the MDP that is resulted\nby making states within G and B absorbing. Now, we are\nable to define the problem that is the main subject of study\nin this section.\nProblem 1. Given an MDP M with unknown transition\nfunction, minimum transition probability pmin ∈(0, 1), a\nreach-avoid specification φ = ¬B U G, and a confidence\nparameter 0 < δ < 1, find an online learning algorithm\nsuch that with confidence greater than 1 −δ the resulted\nregret defined by Eq. (1) grows sublinearly with respect to\nthe number of episodes K.\n4\n\nRegret-Free Reinforcement Learning for LTL Specifications\nAlgorithm 2 Extended Value Iteration (EVI)\nInput: MECs G and B, set of plausible transition func-\ntions T k, upper-bound over hitting time Λ ∈R|S|\n>0, start\ntime of the kth episode tk\nSet l = 0, ˜v0(s) = 0 for s /∈G and ˜v0(s) = 1 for s ∈G\nrepeat\nl ←l + 1\nCompute ˜Pk and ˜πk such that\n˜vl = ˜Lk˜vl−1\n=\n\n\n\n\n\n\n\nmax\na∈A\nmax\n˜\nPk(s,a)∈T k(s,a)\n˜Pk(s, a)⊤˜vl−1\ns /∈G ∪B\n1\ns ∈G\n0\ns ∈B\n(3)\nFor s ∈S, compute ¯λk solving Eq. (8)\nuntil ∥˜vl −˜vl−1∥∞≤\n1\n2tk and ˜λk ≤Λ\nResults: ˜Pk, ˜πk\n4.1. Methodology\nOur proposed algorithm is demonstrated in Alg. 1. We\npropose our algorithm in the known paradigm of optimism\nin the face of uncertainty. Learning takes place over con-\nsecutive environmental episodes. Each episode is a finite\nsequence s1, a1, s2, . . . sL that starts from the initial state\nof Mφ, i.e., s1 = sinit, and ends if either (1) one of the\nMECs in G are reached meaning that sL ∈G, or (2) an\nepisode-specific deadline is reached.\nComputing confidence intervals. Let δ ∈(0, 1) be a given\nconfidence threshold, tk be the time point at which kth\nepisode begins and Nk(s, a) denote the number of times the\nstate-action pair (s, a) has been visited before the start of the\nkth episode. Let ˆTk and Mk denote the empirical transition\nfunction and the set of statistically plausible MDPs, both\ncomputed using the observations before the start of the kth\nepisode. In particular, we define Mk as the interval MDP\nwith interval transition function T k for which all transition\nfunctions Tk ∈T k satisfy\n∥Tk(· | s, a)−ˆTk(· | s, a)∥1 ≤βk(s, a) =:\ns\n8|S| log(2|A|k/3δ)\nmax (1, Nk(s, a)) .\n(4)\nIntuitively, we pick the confidence bound on the right hand\nside of Eq. (4), such that the corresponding inequality holds\nwith high confidence. More concretely, we have the follow-\ning result.\nLemma 4.1. (Tarbouriech et al., 2020) Let E\n:=\nS∞\nk=1{Mφ ∈Mk}. Then P(E) ≥1 −δ.\nExtended value iteration. In every episode k ∈N, we\nAlgorithm 3 Algorithm for solving InnerMax in (3)\nInputs: State-action pair (s, a), estimates ˆTk(· | s, a),\nd(s, a) =\nq\n8|S| log(2|A|k/δ)\nmax (1,Nk(s,a)) and the states in S according\nto their value νl, i. e., S′ = {s′\n1, . . . , s′\nn} with νl(s′\n1) ≥\nνl(s′\n2) ≥· · · ≥νl(s′\nn).\nSet\nˆp(s′\n1) = ˆTk(s, a, s′\n1) + d(s, a)/2\nFor j > 1, set\nˆp(s′\nj) = ˆTk(s, a, s′\nj)\nSet j ←n\nwhile P\ns′q∈S′ ˆp(s′\nq) > 1 do\nˆp(s′\nj) = max{0, 1 −P\ns′q∈S′\\{ s′\nj } ˆp(s′\nq)}\nj ←j −1\nend while\nSet ˜Pk(s, a, s′\nj) = ˆp(s′\nj) for every s′\nj ∈S′.\nResults: ˜Pk(s, a, s′)\nuse a modified version of extended value iteration (EVI) to\ncompute an optimistic MDP ˜\nMk ∈Mk and an optimistic\npolicy ˜πk such that ˜πk maximizes probability of reaching\nG on the optimistic MDP\n˜\nMk. Fixing the policy ˜πk on\n˜\nMk induces a discrete-time Markov chain (DTMC) with\ntransition function denoted by ˜Pk.\nAlg. 2 illustrates our proposed extended value iteration\n(EVI) algorithm. One key input to our algorithm is a vector\nΛ ∈R|S|\n>0, whose entries contain an upper bound on the\nexpected hitting time required to reach G under an optimal\npolicy π∗. The following lemma outlines the procedure for\ncomputing Λ, assuming it is finite.\nLemma 4.2. Let M′ denote the MDP that is resulted by\nconnecting B to sinit in M with probability 1, and λ∗∈\nR|S|\n>0 denote the vector whose entries contain the expected\ntime to hit G in MDP M′ under policy π∗. With confidence\n1 −δ, we have\nλ∗≤Λ := |S|\nlog(δ)\nlog(1 −p|S|\nmin)\n,\n(5)\nif λ∗is finite, i.e., when G is reachable with non-zero proba-\nbility.\nRemark 4.3. One may notice that the input B not only\ncontains the states corresponding to the reach-avoid spec-\nification, but also includes every MEC whose intersection\nwith G is empty. In App. A, we have discussed details of an\nalgorithm for learning the MDP graph χM up to any desired\nconfidence, using the knowledge of the minimum transition\nprobability pmin. Given χM, we can (1) efficiently iden-\ntify all MECs within M that do not intersect with G and\n5\n\nRegret-Free Reinforcement Learning for LTL Specifications\ninclude them in B, and (2) determine whether λ∗is finite\nby verifying if G is within the reachable set of sinit.\nWe have set two conditions for terminating the EVI algo-\nrithm. The first condition (∥˜vl −˜vl−1∥∞≤\n1\n2tk ) ensures\nthat for every s ∈S\n˜vk(s) ≤˜Lk˜vk(s) + 1\n2tk\n,\n(6)\nwhere ˜Lk is given by Eq. (3). Further, we get for every\ns ∈S\n˜vk(s) + 1\n2tk\nv∗(s) ≥0,\n(7)\nwhere v∗and ˜vk denote the vectors containing probabilities\nof reaching G starting from different states, when policies\n˜πk and π∗are followed on MDPs ˜\nMk and Mφ, respectively\n((Auer et al., 2008), Thm. 7).\nLet ˜\nM′\nk denote the MDP that is resulted by connecting B\nto sinit with probability 1 in\n˜\nMk, and ˜λk ∈R|S|\n>0 denote\nthe vector whose entries contain the expected time to hit G\nin ˜P ′\nk that is the DTMC induced by applying policy ˜πk at\nMDP ˜\nM′\nk. To compute ˜λk, we need to solve the following\nlinear system of equations:\n(I −˜P ′\nk)˜λk = u,\n(8)\nwhere u(s) = 0 if s ∈G and u(s) = 1 otherwise. We\nnotice that, the second termination condition for EVI (˜λk ≤\nΛ) can be fulfilled, since ˜λk ≤λ∗< Λ.\nCalculating episode-specific deadline. Upon computation\nof ˜\nMk and ˜πk for episode k, we compute a deadline Hk.\nDeadline Hk is defined as\nHk = min{n > 1 | ∥˜Qn\nk∥∞≤1\nk\n1\nq },\n(9)\nwhere q ∈N>1, ˜Qk =\n\n\n˜Ak\n...\n˜ζk\n. . .\n. . .\n. . .\ng\n...\n0\n\nwith g = 1sinit\nbeing a |S| −1 dimensional one-hot vector and ˜Ak, ˜ζk\ncorrespond to the substochastic partition of ˜Pk containing\ntransition probabilities for the states in S \\ G:\n˜Pk =\n\n\n˜Ak\n...\n˜ζk\n...\n˜βk\n. . .\n. . .\n. . .\n. . .\n. . .\n0\n...\n0\n...\n0\n\n.\n(10)\nNote that we intentionally set the transition probability from\nB to sinit to 1 in g, in order to take the effect of reset into\nconsideration.\nRegret bound analysis. Every episode starts at sinit and\nends by either (i) exceeding the deadline—corresponding to\nslow episodes—, or (ii) by reaching one of the MECs in G—\ncorresponding to fast episodes. It must be noted that every\nvisit of states within MECs in B causes activation of an\nartificial reset action which sets sinit as the next immediate\nstate.\nThe following theorem states that our proposed algorithm\nis regret free, meaning that the corresponding regret grows\nsublinearly.\nTheorem 4.4. With probability 1−6δ, Alg. 1 suffers a regret\nR(K) = O(\nr\n2KαK log 1\nδ )\n+ 4|S|\nr\n8|A|KαK log(2|A|KαK\nδ\n)\n+ 2\nr\n2KαK log(3(KαK)2\nδ\n)\n+ αK(1 + log(KαK))\n2\n+ 2\n√\nK + 2\nr\n2αKK log(2(αKK)2\nδ\n)\n+ 4|S|\nr\n8|A|αKK log(2|A|αKK\nδ\n),\n(11)\nwhere αK := max1≤k≤K Hk and\nαK ≤\nl\n3Λlog(2\n√\nK)\nm\n(12)\n4.2. Proof Sketch of Thm. 4.4\nIn order to bound the total accumulated regret R(K), we de-\nfine R(K) = PK\nk=1 ∆k, where ∆k = v∗(sinit) −vk(sinit).\nAs mentioned before, our analysis categorizes episodes into\nslow or fast, corresponding to episodic regrets ∆(s)\nk\nand\n∆(f)\nk , respectively. Note that for a fast episode, we have\n∆(s)\nk\n= 0; similarly, for a slow episode, we have ∆(f)\nk\n= 0.\nFor the slow episodes, we use the obvious upper bound\n∆(s)\nk\n≤1.\n(13)\nFor fast episodes, since it is possible that a run ends in one\nof MECs in B before reaching G, we need to define a reset\ntransition which takes the states in B to sinit. Therefore,\nevery episode k can be broken to Ik ∈N intervals such that\nthe first Ik −1 intervals start from sinit and end at B, and\nthe Ith\nk interval starts from sinit and end at G. We denote\nthe ith interval of the kth episode—in which the policy πk\nis taken—by ρk,i, and the corresponding value is defined as\nvk,i =\n(\n1\nif last(ρk,i) ∈G\n0\nif last(ρk,i) ∈B.\n(14)\n6\n\nRegret-Free Reinforcement Learning for LTL Specifications\nWe use the fact that v∗(sinit) −vk(sinit) ≤Ik(v∗(sinit) −\nvk(sinit)) (because Ik ≥1) and define\n∆(f)\nk\n≤Ik(v∗(sinit) −vk(sinit))\n=\nIk\nX\ni=1\nv∗(sinit) −vk,i(sinit)\n+\nIk\nX\ni=1\nvk,i(sinit) −vk(sinit).\nOf course, while running the policy πk in the kth episode,\nwe do not know the exact values for v∗(sinit) and vk(sinit).\nInstead, we will try to over approximate the value for\nv∗(sinit) −vk(sinit) in every episode by using ideas in upper\nconfidence bound algorithms such as UCRL2 and further\nrelating vk(sinit) to PIk\ni=1 vk,i.\nWe further define the decomposed regret terms\n∆(f,1)\nk\n=\nIk\nX\ni=1\nv∗(sinit) −vk,i(sinit),\nand\n∆(f,2)\nk\n=\nIk\nX\ni=1\nvk,i(sinit) −vk(sinit).\nThe proof sketch of our regret analysis is as follows. We\nfirst prove that PK\nk=1 ∆(f)\nk\ngrows sublinearly with increas-\ning number of episodes (Lemmas 4.5 to 4.7). To this end,\nwe need to prove that both PK\nk=1 ∆(f,1) and PK\nk=1 ∆(f,2)\ngrow sublinearly. In order to prove sublinear bound on\nthe sum PK\nk=1 ∆(f,1), (1) in Lemma 4.5 we prove that\nαK grows sublinearly with K, and (2) in Lemma 4.6, we\nprove that PK\nk=1 ∆(f,1) grows sublinearly with the num-\nber of episodes K and linearly with the maximum length\nof episodes αK. In order to conclude the sublinear bound\nover PK\nk=1 ∆(f), in Lemma 4.7, we prove the sublinear\ngrowth on PK\nk=1 ∆(f,2). Consequently, we show that the\nsum corresponding to the slow episodes, i.e., PK\nk=1 ∆(s)\nk\nalso grows sublinearly with K, since (1) the number of slow\nepisodes grows only sublinearly with K (Lemma 4.8), and\n(2) ∆(s)\nk\n≤1 for every slow episode. By summing up all of\nthese arguments, our proof is complete and we know that\nR(K) = O(\n√\nK).\nLemma 4.5. Let Λ be an upper bound over λ∗(s), i.e.,\nλ∗(s) ≤Λ for every s ∈S \\ (G ∪B). Then, under the\nevent E we have\nαK ≤\nl\n3Λlog(2\n√\nK)\nm\n(15)\nNow, we proceed by showing why PK\nk=1 ∆(f,1)\nk\ngrows sub-\nlinearly with K.\nLemma 4.6. with probability at least 1 −δ, we have\nK\nX\nk=1\n∆(f,1)\nk\n≤4|S|\nr\n8|A|KαK log(2|A|KαK\nδ\n)\n+ 2\nr\n2KαK log(3(KαK)2\nδ\n)\n+ αK(1 + log(KαK))\n2\n.\n(16)\nIn order to prove the sublinear bound over PK\nk=1 ∆(f,2)\nk\n, we\nmake use of the Azuma-Hoeffding inequality. The following\nlemma proves the bound over sum of ∆(f,2)\nk\nfor the fast\nepisodes.\nLemma 4.7. With probability at least 1 −δ, we have\nK\nX\nk=1\n∆(f,2)\nk\n≤\nr\n2KαK log 1\nδ ,\n(17)\nwhere αK = max1≤k≤K Hk.\nLater, in Lemma 4.8, we show that αK grows logarithmi-\ncally with K which proves that PK\nk=1 ∆(f,2)\nk\ngrows sublin-\nearly with K.\nLet FK denote the number of episodes in which G is not\nreached within the first Hk time steps. We can state that\nwith high probability FK grows sublinearly.\nLemma 4.8. With probability at least 1 −δ, we have\nFK ≤2\n√\nK + 2\nr\n2αKK log(2(αKK)2\nδ\n)\n+ 4|S|\nr\n8|A|αKK log(2|A|αKK\nδ\n),\n(18)\nwhere αK = max1≤k≤K Hk.\nThe following lemma states a direct implication of Lem. 4.8.\nLemma 4.9. With probability at least 1 −δ, we have\nK\nX\nk=1\n∆(s)\nk\n≤FK,\n(19)\nand FK is upper-bounded by a function whose growth is\nsublinear with respect to the number of episodes (Eq. (18)).\n5. Regret-Free Controller Synthesis for LTL\nSpecifications\nIn this section, we study the controller synthesis problem for\nMDPs with unknown (but fixed) transition function against\nLTL specifications. In the following, we state the main\nproblem of study in this section.\n7\n\nRegret-Free Reinforcement Learning for LTL Specifications\nProblem 2. Given an MDP M with unknown transition\nfunction, minimum transition probability pmin ∈(0, 1), an\nLTL specification φ, and a confidence parameter 0 < δ < 1,\nfind an online learning algorithm such that with confidence\ngreater than 1 −δ the resulted regret defined by Eq. (1)\ngrows sublinearly with respect to the number of episodes\nK.\nWe transform the controller synthesis problem for general\nLTL specifications into a synthesis problem for reach-avoid\nspecification, for which one can use the regret-free online\nalgorithm proposed in Sec. 4. For a given LTL specification\nover an MDP, the corresponding optimal policy, in general,\nbelongs to the class of (deterministic) non-positional poli-\ncies, that are mappings from the finite paths over the MDP\ninto the set of actions. In order to restrict the set of policies\nto Markovian (positional) policies, we need to compute the\ncorresponding product MDP by taking the product between\nthe MDP M and the automaton Aφ.\nDefinition 5.1. Given an MDP M = (S, A, T, sinit, AP)\nand DRA Aφ = (Q, Σ, δ, qinit, F) corresponding to an\nLTL formula φ, we denote the product MDP Mφ =\n(S×, A×, T ×, s×\ninit, AP ×), where S× = S × Q, A× = A,\ns×\ninit = (sinit, qinit), AP × = AP × Q, and T × : S× 7→\n∆(S×) taking the form\nT ×((s, q), a, (s′, q′)) =\n(\nT(s, a, s′)\nq′ ∈δ(q, L(s′))\n0\notherwise.\nFor the product MDP Mφ, we define accepting maxi-\nmal end components (AMECs) as those MECs Gj, j ∈\n{1, 2, . . . , m} in Mφ for which Gj = (S × Kj).We denote\nthe union of every AMEC in Mφ by G. Likewise, the union\nof every non-accepting MEC Bj, j ∈{1, 2, . . . , m} in Mφ\nfor which Bj = (S × Jj) in Mφ is denoted as B.\nLet Mφ = (S×, A×, T ×, s×\ninit, L, AP ×) be the MDP com-\nputed as the product of the original MDP M and the cor-\nresponding DRA Aφ that monitors progress of satisfaction\nfor a given LTL specification φ. While the transition proba-\nbility matrix T is unknown, we can find the MECs within\nMφ through learning the graph χM (see Alg. 5) and com-\nputing its product with the DRA Aφ. In Sec. A, we show\na polynomial algorithm for learning the underlying graph\nof the MDP Mφ using the knowledge of a positive lower\nbound over the minimum transition probabilities in T. Once\nwe know the graph of Mφ, we can use Algorithm 47 from\n(Baier & Katoen, 2008) to characterize all of the MECs in\nG and B. Alg. 4 outlines our proposed online regret-free\nmethod for solving the controller synthesis problem against\nLTL specifications.\nAlgorithm 4 Regret-free learning algorithm for general LTL\nspecifications\nInput: LTL specification φ, confidence parameter δ ∈\n(0, 1), state and action sets S and A, connection graph\nχM\nCompute a DRA Aφ which accepts φ\nUse the graph connection χM and compute MECs G and\nB\nRun Alg. 1 to compute and update the control policy over\nMφ\nFigure 1: Map of the gridworld example with l = 6. The\nblue and green cells define the initial state and goal. The red\ncells correspond to walls. The goal of the agent is to reach\nthe goal without hitting the walls.\n6. Experimental Evaluation\nIn this section, we evaluate an implementation of our algo-\nrithm. All of the experiments were performed on a laptop\nwith core i7 CPU at 3.10GHz, with 8GB of RAM. We con-\nsidered a reach-avoid controller synthesis problem in the\ngridworld example described in Fig. 1. The world is charac-\nterized by the number of cells per column and row, which is\ndenoted by l ∈N≥4. The agent can move using the cardinal\ndirections, i.e., A = { right, left, up, down }. Movement\nalong an intended direction succeeds with probability 0.9\nand fails with probability 0.1. In case of failure, the agent\ndoes not move. Walls are considered to be absorbing, i.e.,\nthe agent will not be able to move after hitting a wall. We\nhave conducted experiments to (1) evaluate the empirical\nperformance of our algorithcm, (2) observe how episode\nlength vary throughout the run of our algorithm, and (3)\nassess the sample complexity of our method.\nEmpirical performance. Fig. 2 illustrates the variations of\nempirical mean for the normalized regret R(K)/K for our\nregret-free algorithm which is run for the gridworld example\nwith l = 6. We set δ = 0.1 over 10 runs. Furthermore,\nwe group all of the cells associated with the wall into an\nabsorbing state B, such that we have |S| = 17 and |A| = 4.\nThe target specification is φ = ¬B U G. It can be observed\n8\n\nRegret-Free Reinforcement Learning for LTL Specifications\nFigure 2: Variations of the empirical normalized regret\n(R(K)/K) when our proposed algorithm is implemented\nfor the gridworld example with l = 6.\nthat the empirical mean of regret drops very quickly, which\nimplies that the algorithm successfully finds an optimal\npolicy within the few first episodes.\nWe also compare the performance of our proposed method\nwith the ω-PAC algorithm proposed in (Perez et al., 2023),\nwhich is the only existing method that supports guaran-\nteed controller synthesis against infinite-horizon temporal\nspecifications. The ω-PAC algorithm takes a confidence\nparameter δ ∈(0, 1) and a precision parameter ε ∈(0, 1)\nand provides a policy which has ε-optimal satisfaction prob-\nability with confidence 1 −δ. Fig. 3 illustrates that our pro-\nposed algorithm converges much faster. We believe that this\nis because our algorithm uses the intermediate confidence\nbounds, while the ω-PAC algorithm waits until enough many\nsamples are collected, and only then updates its policy.\nEpisode length variations. Fig. 4 illustrates the variations\nin Hk for different episodes. Initially, our algorithm assigns\nvery small values to Hk, because the expected time to reach\nG in the optimistic MDP is small. As the empirical transi-\ntion probabilities become more precise, the estimation over\nthe expected time to reach G takes more accurate values.\nSample complexity. Although our method and ω-PAC algo-\nrithm provide different guarantees, we relate them through\ndefinition of a related complexity metric. The sample com-\nplexity of the ω-PAC algorithm is characterized with C\nthat is the number of learning episodes with non-ε-optimal\nsatisfaction probability.We define k∗\nreg(δ, ε) as the small-\nest number of episodes k for which our regret-free algo-\nrithm with confidence 1 −δ satisfies R(k)\nk\n≤ε. Further-\nmore, we define k∗\nP AC(δ, ε) as the minimum number of\nepisodes after which with confidence 1 −δ the ω-PAC al-\ngorithm satisfies Ck\nk ≤ε. Fig. 5 illustrates the variations of\nFigure 3: Comparison of the empirical normalized regret\nbetween our proposed regret-free algorithm and the ω-PAC\nalgorithm (Perez et al., 2023) for the gridworld example\nwith l = 4\nk∗\nreg(0.1, 0.1) and k∗\nP AC(0.1, 0.1) for gridworld examples\nwith 4 ≤l ≤16. Note that changes in l influences size of\nstate space (|S| = (l −2)2 + 1), optimal policy’s average\ntime for hitting the goal (Γ = 2(l −3) + 1) and also the\n(minimum) ε-recurrence time T = (l −2)2 + 1. It can\nbe observed that our algorithm provides a tighter bound\nspecially for the larger examples.\n7. Discussion and Conclusions\nIn this paper, we proposed a regret-free algorithm for\nthe control synthesis problem over MDPs against infinite-\nhorizon LTL specifications. The defined regret quantifies\nthe accumulated deviation over the probability of satisfy-\ning the given LTL specification. Below, we have discussed\nseveral aspects of the proposed scheme.\nPossibility of applying the regret-free algorithms that are\nproposed for SSP. The assumptions that are needed for solv-\ning SSP in a regret free way, are inherently non-applicable\nto our target setting as we discussed in the related work\nsection. In particular, relaxing the requirement for existence\nof a proper policy (which is equivalent to the existence of a\npolicy which satisfies the given LTL specification with prob-\nability one) requires attributing a large enough cost to the\npolicies which may end up visiting the non-accepting MECs.\nSuch an assumption (even in the case finding such an upper\nbound is feasible) would automatically require defining a\ncost structure over the product MDP which would in turn\nchange the function of regret so that it can only be defined\nwith respect to the accumulated cost and not the probability\nof satisfying the given specification. However, we desire\nknowing the value of regret with respect to the satisfaction\n9\n\nRegret-Free Reinforcement Learning for LTL Specifications\nFigure 4: Variations of the computed deadline (Hk) for the\ngridworld example with l = 6.\nprobability and not any artificially defined accumulated cost.\nPossibility of applying UCRL2 algorithm. UCRL2 is a\nuseful regret-free algorithm which is proposed for commu-\nnicating MDPs with bounded diameter. Our setting is not\nappropriate for applying UCRL2 as the product MDP would\nnot be communicating after all. However, it may be noticed\nthat our second assumption, i.e., the knowledge over an up-\nper bound for the expected hitting time of the optimal policy\nis a weaker assumption with respect to the main assumption\nfor UCRL2 that is the knowledge over the diameter of the\nunderlying MDP.\nReferences\nAgarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and\nSchapire, R.\nTaming the monster: A fast and sim-\nple algorithm for contextual bandits.\nIn Xing, E. P.\nand Jebara, T. (eds.), Proceedings of the 31st Interna-\ntional Conference on Machine Learning, volume 32\nof Proceedings of Machine Learning Research, pp.\n1638–1646, Bejing, China, 22–24 Jun 2014. PMLR.\nURL https://proceedings.mlr.press/v32/\nagarwalb14.html.\nAlur, R., Bansal, S., Bastani, O., and Jothimurugan, K. A\nframework for transforming specifications in reinforce-\nment learning. In Principles of Systems Design: Essays\nDedicated to Thomas A. Henzinger on the Occasion of\nHis 60th Birthday, pp. 604–624. Springer, 2022.\nAuer, P. and Ortner, R.\nLogarithmic Online Re-\ngret Bounds for Undiscounted Reinforcement Learn-\ning, pp. 49–56.\nThe MIT Press, September 2007.\nISBN 9780262256919.\ndoi: 10.7551/mitpress/7503.\nFigure 5: Comparison of the theoretical sample complexities\nfor our proposed algorithm and the ω-PAC algorithm (Perez\net al., 2023), when applied to the gridworld example with\nvarious sizes (4 ≤l ≤16).\n003.0011. URL http://dx.doi.org/10.7551/\nmitpress/7503.003.0011.\nAuer, P., Jaksch, T., and Ortner, R. Near-optimal regret\nbounds for reinforcement learning. Advances in neural\ninformation processing systems, 21, 2008.\nBaier, C. and Katoen, J.-P. Principles of model checking.\nMIT press, 2008.\nBozkurt, A. K., Wang, Y., Zavlanos, M. M., and Pajic,\nM.\nModel-free reinforcement learning for stochastic\ngames with linear temporal logic objectives. In 2021\nIEEE International Conference on Robotics and Automa-\ntion (ICRA), pp. 10649–10655. IEEE Press, 2021. doi: 10.\n1109/ICRA48506.2021.9561989. URL https://doi.\norg/10.1109/ICRA48506.2021.9561989.\nCai, M., Xiao, S., Li, B., Li, Z., and Kan, Z. Reinforcement\nlearning based temporal logic control with maximum\nprobabilistic satisfaction. CoRR, abs/2010.06797, 2020.\nURL https://arxiv.org/abs/2010.06797.\nCamacho, A., Icarte, R. T., Klassen, T. Q., Valenzano, R. A.,\nand McIlraith, S. A. Ltl and beyond: Formal languages\nfor reward function specification in reinforcement learn-\ning. In International Joint Conference on Artificial Intel-\nligence, 2019.\nDann, C., Lattimore, T., and Brunskill, E. Unifying pac\nand regret: uniform pac bounds for episodic reinforce-\nment learning. In Proceedings of the 31st International\nConference on Neural Information Processing Systems,\nNIPS’17, pp. 5717–5727, Red Hook, NY, USA, 2017.\nCurran Associates Inc. ISBN 9781510860964.\n10\n\nRegret-Free Reinforcement Learning for LTL Specifications\nFruit, R., Pirotta, M., and Lazaric, A.\nNear optimal\nexploration-exploitation in non-communicating markov\ndecision processes. In Proceedings of the 32nd Inter-\nnational Conference on Neural Information Processing\nSystems, NIPS’18, pp. 2998–3008, Red Hook, NY, USA,\n2018. Curran Associates Inc.\nFu, J. and Topcu, U. Probably approximately correct MDP\nlearning and control with temporal logic constraints. In\nRobotics: Science and Systems X, University of Califor-\nnia, Berkeley, USA, July 12-16, 2014, 2014.\nHahn, E. M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A.,\nand Wojtczak, D. Omega-regular objectives in model-free\nreinforcement learning. In Tools and Algorithms for the\nConstruction and Analysis of Systems: 25th International\nConference, TACAS 2019, Held as Part of the European\nJoint Conferences on Theory and Practice of Software,\nETAPS 2019, Prague, Czech Republic, April 6–11, 2019,\nProceedings, Part I, pp. 395–412, Berlin, Heidelberg,\n2019. Springer-Verlag. ISBN 978-3-030-17461-3. doi:\n10.1007/978-3-030-17462-0 27. URL https://doi.\norg/10.1007/978-3-030-17462-0_27.\nHasanbeig, M., Kantaros, Y., Abate, A., Kroening, D.,\nPappas, G. J., and Lee, I.\nReinforcement learning\nfor temporal logic control synthesis with probabilistic\nsatisfaction guarantees.\nIn 2019 IEEE 58th Confer-\nence on Decision and Control (CDC), pp. 5338–5343.\nIEEE Press, 2019.\ndoi:\n10.1109/CDC40024.2019.\n9028919.\nURL https://doi.org/10.1109/\nCDC40024.2019.9028919.\nIcarte, R. T., Klassen, T. Q., Valenzano, R. A., and McIlraith,\nS. A. Using reward machines for high-level task specifi-\ncation and decomposition in reinforcement learning. In\nInternational Conference on Machine Learning, 2018.\nKazemi, M., Perez, M., Somenzi, F., Soudjani, S., Trivedi,\nA., and Velasquez, A. Translating omega-regular speci-\nfications to average objectives for model-free reinforce-\nment learning. In Proceedings of the 21st International\nConference on Autonomous Agents and Multiagent Sys-\ntems, AAMAS ’22, pp. 732–741, Richland, SC, 2022.\nInternational Foundation for Autonomous Agents and\nMultiagent Systems. ISBN 9781450392136.\nKearns, M. and Singh, S. Near-optimal reinforcement learn-\ning in polynomial time. Machine Learning, 49:209–232,\n2002.\nURL https://api.semanticscholar.\norg/CorpusID:2695116.\nLatouche, G. and Ramaswami, V. Introduction to Matrix\nAnalytic Methods in Stochastic Modeling. Society for\nIndustrial and Applied Mathematics, 1999. doi: 10.1137/\n1.9780898719734.\nURL https://epubs.siam.\norg/doi/abs/10.1137/1.9780898719734.\nOura, R., Sakakibara, A., and Ushio, T. Reinforcement\nlearning of control policy for linear temporal logic speci-\nfications using limit-deterministic generalized b¨uchi au-\ntomata. IEEE Control Systems Letters, 4(3):761–766,\nJuly 2020. ISSN 2475-1456. doi: 10.1109/lcsys.2020.\n2980552. URL http://dx.doi.org/10.1109/\nLCSYS.2020.2980552.\nPerez, M., Somenzi, F., and Trivedi, A. A PAC learning\nalgorithm for LTL and omega-regular objectives in mdps.\nCoRR, abs/2310.12248, 2023. doi: 10.48550/ARXIV.\n2310.12248. URL https://doi.org/10.48550/\narXiv.2310.12248.\nRosenberg, A., Cohen, A., Mansour, Y., and Kaplan, H.\nNear-optimal regret bounds for stochastic shortest path.\nIn International Conference on Machine Learning, pp.\n8210–8219. PMLR, 2020.\nSickert, S., Esparza, J., Jaax, S., and Kˇret´ınsk´y, J. Limit-\ndeterministic b¨uchi automata for linear temporal logic.\nIn Chaudhuri, S. and Farzan, A. (eds.), Computer Aided\nVerification, pp. 312–332, Cham, 2016. Springer Interna-\ntional Publishing. ISBN 978-3-319-41540-6.\nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W.\nInformation-theoretic regret bounds for gaussian process\noptimization in the bandit setting. IEEE Transactions\non Information Theory, 58(5):3250–3265, 2012. doi:\n10.1109/TIT.2011.2182033.\nTarbouriech, J., Garcelon, E., Valko, M., Pirotta, M., and\nLazaric, A. No-regret exploration in goal-oriented rein-\nforcement learning. In III, H. D. and Singh, A. (eds.),\nProceedings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Machine\nLearning Research, pp. 9428–9437. PMLR, 13–18 Jul\n2020. URL https://proceedings.mlr.press/\nv119/tarbouriech20a.html.\nVoloshin, C., Le, H. M., Chaudhuri, S., and Yue, Y. Policy\noptimization with linear temporal logic constraints. In\nAdvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022, 2022.\nYang, C., Littman, M., and Carbin, M. On the (in) tractabil-\nity of reinforcement learning for ltl objectives. arXiv\npreprint arXiv:2111.12679, 2021.\n11\n\nRegret-Free Reinforcement Learning for LTL Specifications\nAlgorithm 5 Proposed algorithm for learning graphs\nInput: Initial state sinit, minimum transition probability p(t)\nmin,state and action sets S and A.\nCompute n∗, that is minimum number of samples required to determine all outgoing transitions for any state-action pair\n(s, a) ∈S × A with confidence 1 −δ1, assuming minimum transition probability pmin,\nE ←∅\nt ←0\nfor s ∈S \\ sinit do\nCompute a policy π(s) under which s is reachable from sinit\nfor a ∈A do\nn ←#{t′ < t: st = s, at = a}\nwhile n < n∗do\nst ←sinit\nRun the transition from state-action pair (st, π(s)(st)) and observe st+1\nif st = s then\nn ←n + 1\nbreak\nend if\nt ←t + 1\nend while\nend for\nend for\nA. Graph Identification\nIn order to propose a regret-free controller synthesis method, we need to know the underlying MECs. In this section, we\nshow how to use the knowledge of minimum transition probability pmin ∈R>0 of a given MDP M in order to identify the\nunderlying graph of the MDP which is valid with a desired confidence. By the following lemma, we can determine the\nnumber of samples needed in order to ensure whether a transition exists or not.\nLemma A.1. (Voloshin et al., 2022) For any state-action pair (s, a) ∈S × A, let ˆTn(s′ | s, a) denote the empirical\ntransition probability estimation for the transition probability T(s′ | s, a) after sampling (s, a) for n times. Given a positive\nlower bound over the minimum transition probability pmin ∈(0, 1) and a confidence parameter δ ∈(0, 1), we have\n(s, a, s′) /∈E with confidence 1 −δ if ˆTn∗(s′ | s, a) = 0, for\nn∗> ψ−1(pmin)\n(20)\n, where\nψ(n) =\nr\n1\n2ζ(n) + 7\n3ζ(n),\n(21)\nand ζ(n) = log(4n2|S|2|A|pmin)\n(n−1)\nif n ≥1.\nRemark A.2. To compute policies π(s) for s ∈S, one needs to use undiscounted RL formulations which explicitly can\nhandle the exploration-exploitation trade-off. E3 (Kearns & Singh, 2002) and ω-PAC (Perez et al., 2023) are two such\napproaches which use ε-return mixing time and ε-recurrence time, respectively, in order to avoid unbounded explorations.\nIn our case, we could define an ε-covering time for a policy π(s), as the number of time steps required to visit s for n∗times\nwith probability at least 1 −ε for ε ∈(0, 1). Similar to the theoretical guranatees of methods like E3, we could easilly see\nthat our graph learning algorithm has a sample complexity that is polynomial in the size of state and action spaces and\nmaximum ε-covering time among all policies.\nAlg. 5 outlines our proposed algorithm to learn the graph of a given MDP. There are two main steps: (1) for every state\ns ∈S, we utilize an RL algorithm to get a policy π(s) under which, s is reachable from sinit (with positive probability);\n(2) we run π(s) for every a ∈A until (s, a) is visited for n∗times; upon reaching (s, a), we collect the resulted outgoing\ntransition by running the MDP. Within the rest of this paper, we are going to use the MDP graph χM which is correct with\nconfidence 1 −δ.\n12\n\nRegret-Free Reinforcement Learning for LTL Specifications\nB. Proofs\nB.1. Proof of Lem. 4.2\nProof. Assuming that λ∗is finite implies that there exists at least one policy under which G is reachable from sinit. Under\nsuch a policy, in the worst-case scenario, each state in the Markov chain must be visited at least once. Visiting every state\nrequires following a path of length |S|, which occurs with probability p|S|\nmin. After l attempts of traversing this path, the\nprobability of success at least once is given by 1−(1−p|S|\nmin)l. If l ≥\nlog(δ)\nlog(1−p|S|\nmin), then 1−(1−p|S|\nmin)l ≥1−δ. Finally, since\neach of the l attempts takes |S| steps in the worst case, the total number of steps is bounded by Λ ≤|S|l = |S|\nlog(δ)\nlog(1−p|S|\nmin).\nB.2. Proof of Lem. 4.5\nProof. First, using the Markov’s inequality (since x 7→xr is a non-decreasing mapping for non-negative reals), we have\nP(˜λk(s) ≥Hk −1) ≤E[˜λk(s)r]\n(Hk −1)r .\nNow, we note that by Lem. 15 in (Tarbouriech et al., 2020), we have if ˜λk(s) ≤λ for every s ∈S \\(G∪B) and λ ≥2, then\nE(˜λk(s)r) ≤2(rλ)r,\nfor any r ≥1. Therefore, substituting λ with Λ, we will have\nP(˜λk(s) ≥Hk −1) ≤\n2(rΛ)r\n(Hk −1)r ,\n(22)\nNote that there exists y ∈S such that\n∥˜QHk−2∥∞= 1⊤\ny ˜QHk−21 = P(˜λk(y) > Hk −2)\n= P(˜λk(y) ≥Hk −2).\n(23)\nBy definition of Hk we have ∥˜QHk−2∥∞> 1/\n√\nk. Combining this with Eqs. (22), (23) yields\n2(rΛ)r\n(Hk −1)r > 1/\n√\nk,\nwhich implies that\nHk −1 < rΛ(2\n√\nk)1/r.\nBy selecting r =\nl\nlog(2\n√\nk)\nm\n, we get\nHk −1 < ⌈log(2\n√\nk)⌉Λ(2\n√\nk)\n1\nl\nlog(2\n√\nk)\nm\n≤\nl\n3Λlog(2\n√\nk)\nm\n.\nHence\nαK ≤\nl\n3Λlog(2\n√\nK)\nm\n.\nIn order to prove the sub linear regret bound, we make use of the Azuma-Hoeffding inequality.\nLemma B.1 (Azuma-Hoeffding inequality, Hoeffding 1963). Let X1, X2, . . . be a martingale difference sequence with\n|Xl| ≤c for all l. Then for all γ > 0 and n ∈N,\nP{\nn\nX\nl=1\nXl ≥γ} ≤exp(−γ2\n2nc2 ).\nNow, we proceed by showing why PK\nk=1 ∆(f,1)\nk\ngrows sub linearly with K.\n13\n\nRegret-Free Reinforcement Learning for LTL Specifications\nB.3. Proof of Lem. 4.6\nProof. In order to reformulate the regret, we define the following reward function r: S →{0, 1}\nr(s) =\n(\n0\ns /∈G\n1\ns ∈G .\n(24)\nFurther, for the time step h within episode k we define\nΘk,h(sk,h) := ˜vk(sk,h) −\nHk,Ik(h)−1\nX\nt=h\nr(sk,t)\nwhere Ik : [1; Hk] →[1; Ik] maps the time points in episode k into the corresponding interval, and Hk,i denotes the length\nof the ith interval within the kth episode for 1 ≤i ≤Ik and Hk,0 = 1. Therefore, we have\nK\nX\nk=1\n∆(1)\nk\n=\nK\nX\nk=1\nIk\nX\ni=1\nΘk,Hk,i−1(sk,Hk,i−1).\nLet us further define\nΦk,h := ˜vk(sk,h+1) −\nX\ny∈S\np(y | sk,h, ˜πk(sk,h))˜vk(y),\n(25)\nwhere p(· | ·, ·) corresponds to the transition probability in the true MDP Mφ. Similarly, we denote by ˜pk(· | ·, ·) for the\ntransition probability in the optimistic MDP ˜\nMk. Note that for sk,h ∈G ∪B, we have Θk,h(sk,h) = 0. For sk,h /∈G ∪B,\nwe have\nΘk,h(sk,h) = ˜vk(sk,h) −\nHk,Ik(h)−1\nX\nt=h\nr(sk,t)\n≤˜Lk˜vk(sk,h) + εk −\nHk,Ik(h)−1\nX\nt=h\nr(sk,t)\n=\nX\ny∈S\n˜pk(y | sk,h, ˜πk(sk,h))˜vk(y)\n+ εk −r(sk,h) −\nHk,Ik(h)−1\nX\nt=h+1\nr(sk,t)\n=\nX\ny∈S\n(˜pk(y | sk,h, ˜πk(sk,h))\n−p(y | sk,h, ˜πk(sk,h)))˜vk(y)\n+\nX\ny∈S\np(y | sk,h, ˜πk(sk,h))˜vk(y) + εk −\nHk,Ik(h)−1\nX\nt=h+1\nr(sk,t)\n≤2βk(sk,h, ˜πk(sk,h)) × 1\n+ (\nX\ny∈S\np(y | sk,h, ˜πk(sk,h))˜vk(y) −˜vk(sk,h+1))\n+ εk + (˜vk(sk,h+1) −\nHk,Ik(h)−1\nX\nt=h+1\nr(sk,t))\n≤2βk(sk,h, ˜πk(sk,h)) + Φk,h + εk + Θk,h+1(sk,h+1),\nwhere we used Eq. (6) for the first inequality, the fact that r(sk,t) = 0 for every sk,t /∈G ∪B for the second equality,\n˜vk(y) ≤1 for every y ∈S for the third equality, definition of βk (Eq. (4)) for the second inequality, and definition of Φk,h\n14\n\nRegret-Free Reinforcement Learning for LTL Specifications\n(Eq. (25)) for the last inequality. Also, note that By telescopic sum we get\nΘk,Hk,i(sk,Hk,i) =\nHk,i+1−2\nX\nh=Hk,i\n(Θk,h(sk,h) −Θk,h+1(sk,h+1))\n+ Θk,Hk,i+1−1(sk,Hk,i+1−1)\n≤\nHk,i+1−2\nX\nh=1\n(2βk(sk,h, ˜πk(sk,h)) + Φk,h + εk)\n+ Θk,Hk,i+1−1(sk,Hk,i+1−1)\n≤\nHk,i+1−2\nX\nh=1\n2βk(sk,h, ˜πk(sk,h))\n+\nHk,i+1−2\nX\nh=1\nΦk,h + Hk,i+1εk\nwhere we used the arguments we made in the previous step of the proof to establish the first inequality, the fact that\nΘk,Hk,i+1−1(sk,Hk,i+1−1) = 0 by definition as sk,tk,i+1−1 ∈G ∪B for every 1 ≤i ≤Ik for the last inequality . By\nsumming over all of the episodes we have\nK\nX\nk=1\nIk\nX\ni=1\nΘk,Hk,i−1(sk,Hk,i−1)\n≤\nK\nX\nk=1\nIk\nX\ni=1\nHk,i−1\nX\nh=Hk,i−1\nΦk,h\n+ 2\nK\nX\nk=1\nIk\nX\ni=1\nHk,i−1\nX\nh=Hk,i−1\nβk(sk,h, ˜πk(sk,h))\n+\nK\nX\nk=1\nHkεk.\n(26)\nIn order to bound the first term, we note that\nK\nX\nk=1\nIk\nX\ni=1\nHk,i−1\nX\nh=Hk,i−1\nΦk,h =\nK\nX\nk=1\ntk+1−1\nX\nh=tk\nΦk,h\nTherefore, we can write\nP\n\n\nK\nX\nk=1\ntk+1−1\nX\nh=tk\nΦk,h ≥2\nv\nu\nu\nt2(\nK\nX\nk=1\nHk)log(2(PK\nk=1 Hk)2\nδ\n)\n\n\n≤P\n K\nX\nk=1\ntk+1−1\nX\nh=tk\nΦk,h ≥2\nr\n2nlog(2n2\nδ ) ∩\nK\nX\nk=1\nHk = n\n!\n.\nLet Gq denote the history of all random events up to (and including) step h of episode k, i.e., q = Pk−1\nk′=1 Hk′ + h. We\nhave E(Φk,h | Gq) = 0, and furthermore Hk is selected at the beginning of episode k, and so it is adapted with respect to\nGq. Hence Φk,h is a martingale difference with |Φk,h| ≤1. Therefore, by Azuma-Hoeffding’s inequality, we have with\nprobability 1 −2δ\n3\nP\n\n\nK\nX\nk=1\ntk+1−1\nX\nh=tk\nΦk,h ≥2\nv\nu\nu\nt2(\nK\nX\nk=1\nHk)log(3(PK\nk=1 Hk)2\nδ\n)\n\n.\n15\n\nRegret-Free Reinforcement Learning for LTL Specifications\nNow, we proceed to bound the second term in Eq. 26. Let N (1) denote the number of samples only collected during attempts\nin phase (1) and N (1)+ = max{1, N (1)}. Also let TK,1 denote the number of time steps taken within the episodes which\nend before the deadline Hk. We can write\nK\nX\nk=1\nHk−1\nX\nh=1\ns\n1\nN (1)\nk (sk,h, ˜πk(sk,h))\n≤\nX\ns,a\nN (1)\nK\nX\nn=1\nr\n1\nn\n≤2\np\n|S||A|\nsX\ns,a\nN (1)\nK (s, a) ≤2\nq\n|S||A|TK,1.\nNow, first note that by Lem. 4 in (Tarbouriech et al., 2020) we have that mappings\nq\nlog(cx)\nx\nis a non-increasing function for\nc ≥4 and x ≥1. Also noting that N +\nk (s, a) ≥N (1)+\nk\n(s, a), we get for |A| ≥2 that\nβk(s, a) =\ns\n8|S| log(2|A|N +\nk /δ)\nmax (1, N +\nk (s, a))\n≤\nv\nu\nu\nt8|S| log(2|A|N (1)+\nk\n(s, a)/δ)\nmax (1, N (1)+\nk\n(s, a))\n.\nTherefore, we obtain\nK\nX\nk=1\nHk−1\nX\nh=1\nβk(sk,h, ˜πk(sk,h)) ≤2|S|\nr\n8|A|TK,1 log(2|A|Tk,1\nδ\n).\nFinally, we bound the last term in Eq. (26).\nK\nX\nk=1\nHkεk ≤\nTK,1\nX\nt=1\nαK\n2t ≤αK\n1 + log(KαK)\n2\n,\nwhere αK = max1≤k≤K Hk. Putting everything together yields that inequality (16) holds with probability at least 1−δ.\nB.4. Proof of Lem. 4.7\nProof. We define Xk,i := vk(sinit) −vk,i(sinit). Note that E(Xk,i) = 0 and |Xk,i| ≤1 for every 1 ≤k ≤K and\n1 ≤i ≤Ik. We have\n∆(f,2)\nk\n=\nIk\nX\ni=1\nvk(sinit) −vk,i(sinit) =\nIk\nX\ni=1\nXk,i.\nTherefore by application of the Azuma-Hoeffding lemma and using the fact that Kαk ≥PK\nk=1 Ik we get\nP{\nK\nX\nk=1\n∆(f,2)\nk\n≥\nr\n2KαK log 1\nδ }\n≤exp(−2KαK log 1\nδ\n2 PK\nk=1 Ik\n)\n≤exp(−log 1\nδ ) = δ.\nIn Lemma 4.5, we have already shown that αK grows logarithmically with K which proves that PK\nk=1 ∆(f,2)\nk\ngrows sub\nlinearly with K.\nLet FK denote the number of episodes in which G is not reached within the phase (1). By using arguments similar to the\nones stated in Lem. 7 of (Tarbouriech et al., 2020), we can state that with high probability FK grows sublinearly.\n16\n\nRegret-Free Reinforcement Learning for LTL Specifications\nB.5. Proof of Lem. 4.8\nProof. Let τk and ˜λk denote the hitting times of policy ˜πk in the true and optimistic models, respectively. We define\nΓk,h(sk,h) = 1τk(sk,h)>Hk−h −P(˜λk((sk,h)) > Hk −h).\nNote that we have\nFK =\nK\nX\nk=1\n1τk(sk,1)>Hk−1\n=\nK\nX\nk=1\nΓk,1(sk,1) +\nK\nX\nk=1\nP(˜λk(sinit) > Hk −1).\nLet\n˜P ′\nk =\n\n\n˜Qk\n...\n˜βk\n. . .\n. . .\n. . .\ng\n...\n0\n\n,\ndenote the transition probability matrix of the DTMC created by connecting B to sinit. Let ˜p′\nk(· | ·, ·) denote the transition\nprobability in the optimistic model ˜\nM′\nk (that is the optimistic MDP constructed from ˜\nMk by connecting states in B into\nsinit). Similarly, let p′(· | ·, ·) denote the transition probability in the MDP M′\np (that is the MDP constructed from Mp by\nconnecting states in B into sinit). Since for 1 ≤h ≤Hk −1, 1τk(sk,h)>Hk−h = 1τk(sk,h+1)>Hk−h−1 we have\nΓk,h(sk,h) = 1τk(sk,h+1)>Hk−h−1\n−\nX\ny∈S\n˜p′\nk(y | sk,h, ˜πk(sk,h))P(˜λk(y) > Hk −h −1)\n≤1τk(sk,h+1)>Hk−h−1\n−(\nX\ny∈S\n˜p′\nk(y | sk,h, ˜πk(sk,h)) −p′(y | sk,h, ˜πk(sk,h)))P(˜λk(y) > Hk −h −1)\n−\nX\ny∈S\np′(y | sk,h, ˜πk(sk,h))P(˜λk(y) > Hk −h −1)\n≤1τk(sk,h+1)>Hk−h−1 + 2βk(sk,h, ˜πk(sk,h))\n−\nX\ny∈S\np′(y | sk,h, ˜πk(sk,h))P(˜λk(y) > Hk −h −1)\n= Γk,h+1(sk,h+1) + ψk,h + 2βk(sk,h, ˜πk(sk,h)),\nwhere we established the first inequality by adding and subtracting the term p′(y | sk,h, ˜πk(sk,h)))P(˜λk(y) > Hk −h −1),\nthe second inequality by using the definition of βk (Eq. (4)), and the last equality by using the following definition\nψk,h = P(˜λk(sk,h+1) > Hk −h −1)\n−\nX\ny∈S\np′(y | sk,h, ˜πk(sk,h))P(˜λk(y) > Hk −h −1).\nAlso, we have\nΓk,Hk(sk,Hk) = 1τk(sk,Hk )>0 −P(˜λk(sk,Hk > 0))\n= 1τk(sk,Hk )>0 −1˜λk(sk,Hk >0)\n= 1sk,Hk /∈G −1sk,Hk /∈G = 0.\n17\n\nRegret-Free Reinforcement Learning for LTL Specifications\nUsing the telescopic sum we get\nΓk,1(sk,1) =\nHk−1\nX\nh=1\n(Γk,h(sk,h) −Γk,h+1(sk,h+1)) + Γk,Hk(sk,Hk)\n≤\nHk−1\nX\nh=1\nψk,h + 2\nHk−1\nX\nh=1\nβk(sk,h, ˜πk(sk,h)),\nwhere the last inequality is achieved by knowing Γk,Hk(sk,Hk) = 0. Therefore, by summing over all episodes we get\nFK ≤\nK\nX\nk=1\nHk−1\nX\nh=1\nψk,h + 2\nK\nX\nk=1\nHk−1\nX\nh=1\nβk(sk,h, ˜πk(sk,h))\n+\nK\nX\nk=1\nP(˜λk(sinit) > Hk −1).\nLet Gq denote the history of all random events up to (and including) step h of episode k, i.e., q = Pk−1\nk′=1 Hk + h. We have\nE(ψk,h | Gq) = 0, and furthermore Hk is selected at the beginning of episode k, and so it is adapted with respect to Gq.\nHence ψk,h is a martingale difference with |ψk,h| ≤1. Therefore, using similar arguments as in proof of Lem. 4.6, by\nAzuma-Hoeffding’s inequality, we have with probability 1 −2δ\n3\nK\nX\nk=1\nHk−1\nX\nh=1\nψk,h ≤2\nv\nu\nu\nt2(\nK\nX\nk=1\nHk) log(3(PK\nk=1 Hk)2\nδ\n)\n≤2\nr\n2KαK log(3(KαK)2\nδ\n).\nFurther, in the same vein as proof of Lem. 4.6 we have\nK\nX\nk=1\nHk−1\nX\nh=1\nβk(sk,h, ˜πk(sk,h)) ≤2|S|\nr\n8|A|KαK log(2|A|KαK\nδ\n).\nNow, we need to bound PK\nk=1 P(˜λk((sinit)) > Hk −1). Using Thm. 2.5.3 in (Latouche & Ramaswami, 1999), we have\nK\nX\nk=1\nP(˜λk(sinit) > Hk −1) =\nK\nX\nk=1\n1sinit ˜QHk−1\nk\n1,\nwhere 1s denotes the |S| −1-sized one-hot vector at the position of state s ∈S. Finally, from Holder’s inequality, we have\nK\nX\nk=1\nP(˜λk((sinit)) > Hk −1) =\nK\nX\nk=1\n1sinit ˜QHk−1\nk\n1\n≤\nK\nX\nk=1\n∥1sinit∥1∥˜QHk−1\nk\n1∥∞≤\nK\nX\nk=1\n∥˜QHk−1\nk\n1∥∞.\nTherefore, by the choice of Hk = min{n > 1 | ∥˜Qn\nk∥∞≤\n1\n√\nk} we get\nK\nX\nk=1\nP(˜λk((sinit)) > Hk −1) ≤\nK\nX\nk=1\n1\n√\nk\n≤2\n√\nK.\n18",
    "pdf_filename": "Regret-Free_Reinforcement_Learning_for_LTL_Specifications.pdf"
}