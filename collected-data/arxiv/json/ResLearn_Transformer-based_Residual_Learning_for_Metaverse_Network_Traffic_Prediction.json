{
    "title": "ResLearn Transformer-based Residual Learning for Metaverse Network Traffic Prediction",
    "context": "predicting Metaverse network traffic, addressing the growing demand for intelligent resource management in eXtended Reality (XR) services. We first introduce a state-of-the-art testbed captur- ing a real-world dataset of virtual reality (VR), augmented reality (AR), and mixed reality (MR) traffic, made openly available for further research. To enhance prediction accuracy, we then propose a novel view-frame (VF) algorithm that accurately identifies video frames from traffic while ensuring privacy compliance, and we develop a Transformer-based progressive error-learning algorithm, referred to as ResLearn for Metaverse traffic prediction. ResLearn significantly improves time-series predictions by using fully connected neural networks to reduce errors, particularly during peak traffic, outperforming prior work by 99%. Our contributions offer Internet service providers (ISPs) robust tools for real-time network management to satisfy Quality of Service (QoS) and enhance user experience in the Metaverse. Index Terms—Metaverse Network Traffic Prediction, Residual Learning, Extended Reality (XR), virtual reality (VR), augmented reality (AR), and mixed reality (MR). The Metaverse is a comprehensive ecosystem of inter- connected virtual worlds that provide immersive experiences to users. The ecosystem enhances existing and generates new value from economic, environmental, social, and cultural perspectives [1]. Services in the Metaverse ecosystem are designed to be accessed using immersive extended reality (XR) environments. XR is an umbrella term that describes the technologies affecting the user’s immersive experience, such as virtual reality (VR), augmented reality (AR), and mixed reality (MR) [2]. VR allows users to interact with virtually generated environments designed to simulate real-world experiences. AR overlays interactive, virtually generated information onto real- world objects or within real-world spaces. XR technologies lie on a spectrum between AR and VR. In cases where the dis- tinction between the realities is ambiguous, the experience is considered MR. As the Metaverse’s growth continues and XR evolves, the popularity of its services increases. Driven by the rapid growth of the Metaverse, Internet traffic is expected to surpass current forecasts significantly [3]. The entertainment and social media industries have seen the most substantial growth of Metaverse services, as evidenced by popular virtual performance events, one of which attracted an audience of 36 million users [4], [5]. Healthcare, training, and marketing for Metaverse services have also grown recently [6], [7]. Cloud rendering for Metaverse is crucial to offloading computing resources to make the services affordable, a popular technique for VR games [8]. Consequently, the Ericsson 2022 report emphasizes the growing need for more intelligent interactions between XR services and the network to maintain high Quality of Service (QoS) [3]. Therefore, network management is crucial for Internet service providers (ISPs) to accommodate adequate resources and avoid cybersickness among users [9]– [11]. Metaverse traffic consists of video, audio, and control flows [8], among all downlink video frames are resource-demanding in the case of VR, and both uplink/downlink video frames for AR and MR traffic. Therefore, predicting the frame size is vital, and for latency-related issues, it is essential to predict frame inter-arrival time and frame duration [12]. Predicting frame size, inter-arrival time, and frame duration will help ISPs prepare and manage the Metaverse network for holistic and in- telligent traffic management. However, there needs to be more real-world Metaverse data and research in prediction to make progress in the field. Essentially, frame-related information is time series data. The recent advent of different state-of-the-art artificial intelligence (AI) based time series models has shown tremendous progress in time series predictions [13]. The only VR frame size prediction work is available at [14]; therefore, we consider it state-of-the-art (SoA) work for benchmark comparison. The work studies different AI models to predict VR frame size. It establishes stacked LSTM to produce better results based on transfer learning methodologies. Therefore, the solution aims for online prediction, imperative for real- time network management. However, the work is evaluated on a small dataset captured in a controlled environment. Also, the performance can be improved with further reduction in the error. The frame identification methodology used in the work might need to be revised because the frame loss for 120 Mbps is more than 54 Mbps since the dataset is captured in a controlled environment. Our literature review identifies the arXiv:2411.11894v1  [cs.AI]  7 Nov 2024",
    "body": "ResLearn: Transformer-based Residual Learning for\nMetaverse Network Traffic Prediction\nYoga Suhas Kuruba Manjunath∗, Mathew Szymanowski ∗, Austin Wissborn∗,\nMushu Li †, Lian Zhao∗, and Xiao-Ping Zhang ‡\n∗Department of Electrical, Computer & Biomedical Engineering, Toronto Metropolitan University, Toronto, Canada\n†Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, USA\n‡ Shenzhen Key Laboratory of Ubiquitous Data Enabling, Tsinghua Shenzhen International\nGraduate School, Tsinghua University,\nEmail: {yoga.kuruba@torontomu.ca, austin.wissborn@torontomu.ca, mszymanowski@torontomu.ca,\nmul224@lehigh.edu, l5zhao@torontomu.ca, xpzhang@ieee.org}\nAbstract—Our work proposes a comprehensive solution for\npredicting Metaverse network traffic, addressing the growing\ndemand for intelligent resource management in eXtended Reality\n(XR) services. We first introduce a state-of-the-art testbed captur-\ning a real-world dataset of virtual reality (VR), augmented reality\n(AR), and mixed reality (MR) traffic, made openly available\nfor further research. To enhance prediction accuracy, we then\npropose a novel view-frame (VF) algorithm that accurately\nidentifies video frames from traffic while ensuring privacy\ncompliance, and we develop a Transformer-based progressive\nerror-learning algorithm, referred to as ResLearn for Metaverse\ntraffic prediction. ResLearn significantly improves time-series\npredictions by using fully connected neural networks to reduce\nerrors, particularly during peak traffic, outperforming prior\nwork by 99%. Our contributions offer Internet service providers\n(ISPs) robust tools for real-time network management to satisfy\nQuality of Service (QoS) and enhance user experience in the\nMetaverse.\nIndex Terms—Metaverse Network Traffic Prediction, Residual\nLearning, Extended Reality (XR), virtual reality (VR), augmented\nreality (AR), and mixed reality (MR).\nI. INTRODUCTION\nThe Metaverse is a comprehensive ecosystem of inter-\nconnected virtual worlds that provide immersive experiences\nto users. The ecosystem enhances existing and generates\nnew value from economic, environmental, social, and cultural\nperspectives [1]. Services in the Metaverse ecosystem are\ndesigned to be accessed using immersive extended reality\n(XR) environments. XR is an umbrella term that describes the\ntechnologies affecting the user’s immersive experience, such as\nvirtual reality (VR), augmented reality (AR), and mixed reality\n(MR) [2]. VR allows users to interact with virtually generated\nenvironments designed to simulate real-world experiences. AR\noverlays interactive, virtually generated information onto real-\nworld objects or within real-world spaces. XR technologies lie\non a spectrum between AR and VR. In cases where the dis-\ntinction between the realities is ambiguous, the experience is\nconsidered MR. As the Metaverse’s growth continues and XR\nevolves, the popularity of its services increases. Driven by the\nrapid growth of the Metaverse, Internet traffic is expected to\nsurpass current forecasts significantly [3]. The entertainment\nand social media industries have seen the most substantial\ngrowth of Metaverse services, as evidenced by popular virtual\nperformance events, one of which attracted an audience of 36\nmillion users [4], [5]. Healthcare, training, and marketing for\nMetaverse services have also grown recently [6], [7]. Cloud\nrendering for Metaverse is crucial to offloading computing\nresources to make the services affordable, a popular technique\nfor VR games\n[8]. Consequently, the Ericsson 2022 report\nemphasizes the growing need for more intelligent interactions\nbetween XR services and the network to maintain high Quality\nof Service (QoS) [3]. Therefore, network management is\ncrucial for Internet service providers (ISPs) to accommodate\nadequate resources and avoid cybersickness among users [9]–\n[11].\nMetaverse traffic consists of video, audio, and control flows\n[8], among all downlink video frames are resource-demanding\nin the case of VR, and both uplink/downlink video frames for\nAR and MR traffic. Therefore, predicting the frame size is\nvital, and for latency-related issues, it is essential to predict\nframe inter-arrival time and frame duration [12]. Predicting\nframe size, inter-arrival time, and frame duration will help ISPs\nprepare and manage the Metaverse network for holistic and in-\ntelligent traffic management. However, there needs to be more\nreal-world Metaverse data and research in prediction to make\nprogress in the field. Essentially, frame-related information is\ntime series data. The recent advent of different state-of-the-art\nartificial intelligence (AI) based time series models has shown\ntremendous progress in time series predictions [13]. The only\nVR frame size prediction work is available at [14]; therefore,\nwe consider it state-of-the-art (SoA) work for benchmark\ncomparison. The work studies different AI models to predict\nVR frame size. It establishes stacked LSTM to produce better\nresults based on transfer learning methodologies. Therefore,\nthe solution aims for online prediction, imperative for real-\ntime network management. However, the work is evaluated\non a small dataset captured in a controlled environment. Also,\nthe performance can be improved with further reduction in\nthe error. The frame identification methodology used in the\nwork might need to be revised because the frame loss for 120\nMbps is more than 54 Mbps since the dataset is captured in\na controlled environment. Our literature review identifies the\narXiv:2411.11894v1  [cs.AI]  7 Nov 2024\n\nFig. 1: System model of the proposed solution. x in f x[t]\nrepresents one of the features. EDA is the exploratory data\nanalysis.\nfollowing gaps: i) the need for holistic, comprehensive, and\nreal-world Metaverse datasets and ii) accurate frame-related\ndata predicting algorithms.\nWe chart out a state-of-the-art Metaverse testbed to capture\na holistic, comprehensive, and real-world Metaverse dataset\ncomprising VR games, VR videos, VR chat/VoIP, AR, and\nMR traffic. We also make it open for the research fraternity\navailable at [15]. Our work treats the Metaverse traffic in\nsegments to enhance the predictability of the data at higher\nspeed. We propose an accurate view-frame (VF) algorithm that\nhelps to identify the types of video frames using application-\nlevel information to comply with privacy-related policies. Our\nproposed VF algorithm can predict the number of frames in a\nsegment, total frame size, and frame inter-arrival time. Finally,\nwe propose a state-of-the-art residual learning (ResLearn)\nalgorithm that uses transformer [16] to predict time-series data\nand fully connected neural networks (FCNN) to learn errors\nwith a bias to identify the peaks of the frame-related data for\naccurate network management. Our solution outperforms the\nSoA work [14] by 99% in reducing the prediction errors. The\nimplementation of the solution is made open for the research\ncommunity at [17].\nII. SYSTEM MODEL\nThe system model, in Figure 1 illustrates the proposed\nframework for predicting Metaverse network traffic, focus-\ning on frame-level features. The process begins with data\npreprocessing, where the VF algorithm is applied to extract\nrelevant frame-related data. The key features extracted from\nthe incoming traffic include frame count (f c), frame size (f s),\nand frame inter-arrival time (IAT) (f iat). Our VF algorithm\nworks on application-level features: time, packet length, packet\ndirection, and packet inter-arrival time. Frame-related packets\nhave a more considerable length with relatively more minor\ninter-arrival time. We use this property to determine the\nthresholds for packet length and inter-arrival time to identify\nframe-related packets as shown in Figure 2. In Figure 2, lenT H\nis determined as 25% of the maximum length of the observed\npacket length. durth is the frame duration threshold deter-\nmined between the first two peaks. The first peak represents\nthe start of the video frame packet with less inter-arrival time,\nand the second peak represents the end of the video frame\n(a)\n(b)\nFig. 2: (a) PDF of packet lengths, and (b) PDF of inter-arrival\ntime for a sample Metaverse traffic segment.\nwhen audio and control-related flows are transmitted with\nslightly higher inter-arrival time. The lenT H and durth are\ncalculated from the first segment at a session’s start for the VF\nalgorithm. All packets within these thresholds are collected\nto identify the frames. The method is verified on different\nMetaverse rendering platforms: Meta air link [18], and Virtual\nDesktop Streamer (VDS) [19].\nBased on the requirement, one of the features is selected for\nprediction. These features undergo Exploratory Data Analysis\n(EDA) and Data Engineering to ensure data quality and\nintegrity, followed by Time Series Data Preparation with\nsegmentation for model training. The core of the system is\nframe feature prediction (f x[t]), where the previous frame\nfeature values (f x[t −1]) are used as inputs to predict the\ncurrent network traffic behaviour. With this information, we\nprovide the mathematical problem statement to predict key\nfeatures of the Metaverse network traffic features. Let ft be a\nframe vector, identified by VF algorithm for a given segment,\nhaving frame-related information given as ft = [f c\nt , f s\nt , f iat\nt\n],\nwhere t indicates the time index of the given segment. Let\nf x[t−1] represents the Metaverse frame-related network traffic\nat the previous time step, where x indicates one of the three\nframe-related features in f[t−1]. Therefore, frame count, size,\nand inter-arrival time are individually predicted using historical\ndata. The prediction function for the current network traffic,\nf x[t], is\nf x[t] = ψ(f x[t −1]),\n(1)\nwhere ψ(·) is the prediction model.\nIII. RESLEARN: TRANSFORMER-BASED RESIDUAL\nLEARNING FOR METAVERSE NETWORK TRAFFIC\nPREDICTION\nThe residual Learning (ResLearn) algorithm is a two-\nstep prediction approach involving a transformer deep neural\n\nnetwork model [16] designed to enhance Metaverse network\ntraffic forecasting by leveraging residual learning inspired by\nResNet [20]. Transformer is known for learning short and\nlong-term dependencies from time series data. However, the\nprediction error is inevitable due to the randomness introduced\nby network health and users in Metaverse infrastructure.\nHowever, we can learn the nature of error using a neural\nnetwork. ResLearn is a novel approach that uses a Transformer\nin the first step, given as F1(·), the predictive Transformer\nmodel. The residual from the F1(·) is fed to a fully connected\nneural network (FCNN) to learn the nature of the error, given\nas F2(·). The final prediction model from Eq. (1) is given as\nψ(·) = F1(·) + F2(·).\nFigure 3 illustrates the workflow of the ResLearn algorithm.\nThe input data is split into training (ytrain) and validation\n(yval) sets. The first model, including a transformer network,\nprocesses the training data and generates an initial prediction\nresult (ytrain pred). This output, called Output 1, is then com-\npared to the actual training data to calculate the residuals,\nwhich capture the difference between the predicted and true\nvalues. These residuals are passed to the second model, an\nFCNN designed to learn and predict the patterns in the\nresiduals, producing a corrected prediction (respred). Both\npredictions from the transformer and FCNN are combined\nto form the final output (yout) of the ResLearn model. This\ncombined output is then validated with the unseen validation\ndata, improving the final prediction’s overall accuracy. The\nResLearn training algorithm is shown in Algorithm 1. SX\nN\nis a time-series data segment, where N is the segment size\nand X is the number of segments. For each segment, the\ndata is split into training and validation sets. The transformer\nmodel is trained on the training set, and its predictions, TP R,\nare computed. The residuals, representing the error between\nthe actual and predicted values, are calculated and adjusted\nby adding a bias, ResB, to highlight essential peaks. The\ndense model is then trained on these residuals, and the final\nResLearn model, MRL, is created by combining the outputs\nof both models. The combined model is then evaluated on the\nvalidation set using error metrics. The process is repeated for\neach segment, refining the prediction accuracy iteratively until\nthe validation error is stabilized for MRL. The algorithm’s\noutput is the final ResLearn model MRL, which integrates\nthe strengths of both models to deliver improved predictive\nperformance.\nThe time complexity of the ResLearn algorithm, which\ninvolves training a transformer model and a FCNN, can\nbe approximated as follows: the training complexity of the\ntransformer model is typically O(T · N 2), where T is the\nnumber of training epochs and N is the sequence length.\nThe training complexity of the FCNN can be approximated\nas O(T ′ · N · D), where T ′ is the number of epochs, D is the\nnumber of neurons in the hidden layer, and N represents the\nnumber of training samples [21]. Considering X segments of\ndata, the overall time complexity of the algorithm is given by\nO(X · (T · N 2 + T ′ · N · D)).\nFig. 3: ResLearn algorithm.\nAlgorithm 1 Residual Learning (ResLearn) Training\nData: Time series segmented data (SX\nN ∈SN) where N is the\nsize of the segment and X is segment number\nResult: ResLearn model (MRL).\ninitialization\nmodel1 = transformer()\n▷to learn short and long-term\ndependencies\nmodel2 = dense()\n▷to learn residuals\nwhile i != X do\ntrain, val = splitData(SX\nNi, TR)\nmodel1.Train(train)\nTP R = model1.Predict(train)\n▷predicts the data\nRes = train - TP R\nResB = |min(Res)|\n▷bias to identify peaks\nresidualTrain = Res + ResB\nmodel2.Train(residualTrain)\nMRL = model1 + model2\nResP R = MRL.Predict(val)\nerror metrics(val, ResP R)\n▷calculates all error metrics\ni++\nend\nreturn MRL\nIV. EXPERIMENTATION SETUP AND RESULTS\nA. Datasets and predictability\nThe experiments are designed to evaluate the performance\nof various VR, AR, and MR services across three datasets, as\nshown in Table I. The Dataset I [15] (in-house dataset) focuses\non diverse services, including gaming, video streaming, and\ncommunication (chat/VoIP), and is tested with applications\nsuch as Dirt Rally 2.0, Bigscreen, VR Chat, Solar System,\nand Reality Mixer. Figure 4 shows the testbed used in the\ndata capture. In the testbed, a virtual desktop streamer (VDS)\nrendering platform is used for the setup. A cloud computer\nwith a VDS server is a rendering device to which the VDS\nclient on the Oculus Quest 2 is connected. Traffic manager\n\nFig. 4: Experimental platform used for data capture.\nis used to simulate low latency networks to replicate real-\nworld scenarios. Traffic is captured on the cloud computer\nusing Wireshark. More details and packet captures (pcap) are\navailable at [15]. Dataset II [8] examines slow and fast VR\ntraffic using Steam VR Home and Beat Saber to study the im-\npact of different traffic patterns. Dataset III [22] involves two\nexperiments: the first explores fast and slow VR traffic across\nBeat Saber, Medal of Honor, Forklift Simulator, and Cooking\nSimulator, while the second focuses on a subset of applications\n(Forklift Simulator, Cooking Simulator, Beat Saber, and Medal\nof Honor) to assess network performance under varying traffic\nconditions further. We use 50% of data for training, in which\n20% of the data is used for validation. Another 50% of the\ndata is used for testing. The solution is developed in Python\nusing data science libraries such as Scikit-learn (Sklearn),\nNumPy, and Pandas. The implementation is available at [17].\nThe experiments are conducted on a Windows system with an\nNvidia RTX2800S GPU. The Windows environment is set up\nwith Anaconda to support machine learning libraries, including\nTensorFlow.\nThe analysis of data predictability begins with an ex-\nploratory data analysis (EDA) that suggests randomness,\nshown in Figure 5 depicted by blue line, in the network traffic\ndata. Runs test is a statistical procedure which determines\nwhether a sequence of data within a given distribution have\nbeen derived with a random process or not [23]. Runs test of\nthe raw data provides a p-value of 0.15, which indicates no\nsignificant evidence against the null hypothesis of randomness.\nHowever, a deeper examination using decomposition tech-\nniques reveals underlying patterns in the time series, breaking\nit down into trend, seasonal, and residual components. This is\nfurther reinforced by applying a rolling window average (with\na window size of 20), where the rolling mean (depicted by\nthe red line in Figure 5) closely follows the shape of the data,\nrevealing a clear trend. The corresponding Runs test, now with\na p-value close to zero, confirms that the data is predictable\nand not random. Rolling statistics effectively uncovers this\nstructure, making the data suitable for forecasting models.\nB. Performancce Metrics\nThe performance of Metaverse network traffic prediction\nmodels can be evaluated using the following metrics: RMSE\np\nFig. 5: frame size time series data with roll-over averaging\nwindow.\nTABLE I: Datasets and plan of experiments.\nExp.\nservices\nApplications\nDataset I\nexp. 1\nVR Game, VR Video,\nVR chat/VoIP, AR, MR\nDirt Rally 2.0, Bigscreen,\nVR Chat, Solar System,\nReality Mixer\nDataset II\nexp. 1\nSlow VR Traffic, Fast\nVR Traffic\nSteam VR Home, Beat\nSaber\nDataset III\nexp. 1\nFast VR Traffic Game 1,\nFast VR Traffic Game 2,\nSlow VR Traffic Game 1,\nSlow VR Traffic Game 2\nBeatSaber, Medal of\nHonor, Forklift Sim,\nCooking Sim.\nexp. 2\nSlow VR Traffic, Fast\nVR Traffic\nSForklift Sim, Cooking\nSim, BeatSaber, Medal of\nHonor\n(Root Mean Squared Error), MAPE (Mean Absolute Percent-\nage Error), and SMAPE (Symmetric Mean Absolute Percent-\nage Error). These metrics quantify the differences between\npredicted and actual traffic values, aiding in assessing the\nmodel’s accuracy. The RMSE measures the square root of\nthe average squared differences between predicted and actual\nvalues, emphasizing more significant errors, and is given by:\nRMSE =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\n(ˆyi −yi)2 ,\nwhere ˆyi represents the predicted value, yi is the actual\nvalue, and n is the number of predictions. The MAPE is used\nto compute the average percentage error, offering an intuitive\ninterpretation of prediction errors, and is formulated as:\nMAPE = 100%\nn\nn\nX\ni=1\n\f\f\f\f\nyi −ˆyi\nyi\n\f\f\f\f .\nLastly, SMAPE provides a symmetric approach to percent-\nage error by accounting for both over- and under-predictions.\nIt is computed as:\nSMAPE = 1\nn\nn\nX\ni=1\n|ˆyi −yi|\n(|ˆyi| + |yi|) /2.\n\nTABLE II: Performance for frame size from Dataset I\nModel\nMetrics\nRMSE\nMAPE\nSMAPE\nNon-Residual Algorithm\nTransformer\n4872.49\n0.0078\n0.78\n% SMAPE Improvement\nLSTM\n4904.88\n0.0079\n0.79\nGRU\n4915.61\n0.0079\n0.79\nStacked LSTM\n4929.88\n0.0079\n0.79\nResLearn Solution\nTransformer\n2164.95\n0.0024\n0.24\n68.87%\nLSTM\n2725.03\n0.0034\n0.34\n56.78%\nGRU\n2786.97\n0.0031\n0.31\n61.04%\nStacked LSTM\n3666.30\n0.0046\n0.46\n41.78%\nNote: The top table provides results for the Non-residual version\nof the model in which residuals are not learned. The bottom table\nprovides the result of the proposed solution. The % SMAPE improve-\nment is given in the fifth column.\nSMAPE balances the errors across different magnitudes of\nactual and predicted values, making it particularly suitable for\ndynamic environments like Metaverse traffic prediction. These\nmetrics offer a comprehensive view of model performance,\nhelping improve prediction accuracy for fluctuating network\ndemands.\nC. Performance Evaluation\nThe Tables II III, IV, & V present the performance eval-\nuation of various models (Transformer, LSTM, GRU, and\nStacked LSTM) across three datasets, with metrics such as\nRMSE, MAPE, and SMAPE. Table II shows the performance\nfor frame size (Dataset I), Table III for frame count (Dataset\nII), Table IV for frame IAT (Dataset III, exp 1), and Table\nV for frame size (Dataset III, exp2). Each table compares the\nmodels’ non-residual version (where residuals are not learned)\nwith the residual learning approach, the ResLearn algorithm.\nIn each case, SMAPE improvement is calculated, highlight-\ning the percentage improvement in predictive accuracy when\nresidual learning is applied.\nThe results indicate that the ResLearn algorithm signifi-\ncantly improves performance across all models and datasets,\nespecially regarding SMAPE. For example, the transformer\nmodel in Table II achieves a SMAPE reduction from 0.78 to\n0.24 (68.87% improvement), and a similar trend is observed in\nTables III and IV, where the transformer and Stacked LSTM\nmodels show substantial SMAPE improvements of over 70%.\nThe observation is similar to Dataset III exp 2; however, GRU\nis better than the transformer. This demonstrates that residual\nlearning can enhance the accuracy of time series models,\nparticularly for the transformer architecture, making it the most\neffective among the evaluated models across all datasets.\nD. Performance Comparision and Discussion\nThe comparison of SMAPE between the SoA transfer learn-\ning model [14] and the proposed ResLearn solution (Table VI)\ndemonstrates a significant performance improvement in favour\nof ResLearn. In various traffic conditions, as considered in\n[14], such as BeatSaber and Steam VR house at different Mbps\nrates, ResLearn consistently achieves a near-perfect reduction\nTABLE III: Performance for frame count from Dataset II\nModel\nMetrics\nRMSE\nMAPE\nSMAPE\nNon-Residual Algorithm\nTransformer\n0.042\n0.0072\n0.72\n% SMAPE Improvement\nLSTM\n0.042\n0.0073\n0.73\nGRU\n0.043\n0.0076\n0.76\nStacked LSTM\n0.043\n0.0076\n0.77\nResLearn Solution\nTransformer\n0.018\n0.0020\n0.20\n71.85%\nLSTM\n0.017\n0.0023\n0.23\n68.67%\nGRU\n0.018\n0.0025\n0.25\n67.09%\nStacked LSTM\n0.019\n0.0022\n0.22\n71.72%\nTABLE IV: Performance for frame IAT from Dataset III exp.\n1\nModel\nMetrics\nRMSE\nMAPE\nSMAPE\nNon-Residual Algorithm\nTransformer\n0.037\n0.0032\n0.32\n% SMAPE Improvement\nLSTM\n0.034\n0.0029\n0.29\nGRU\n0.034\n0.0030\n0.30\nStacked LSTM\n0.033\n0.0027\n0.29\nResLearn Solution\nTransformer\n0.029\n0.0020\n0.20\n38.01%\nLSTM\n0.030\n0.0021\n0.21\n25.11%\nGRU\n0.035\n0.0025\n0.25\n13.74%\nStacked LSTM\n0.028\n0.0019\n0.19\n36.04%\nin SMAPE, reaching over 99% improvement across all scenar-\nios. Similar reductions are observed in other settings, like the\nSteam VR house 40 Mbps, where SMAPE decreased from\n562.87 to 0.45, showcasing ResLearn’s superior accuracy.\nFigure 6 compares time series prediction of ResLearn and non-\nResLearn solutions. Overall, the ResLearn solution is superior\nfor network management because it accurately predicts the\npeaks at which maximum resource is required.\nTABLE V: Performance for frame size from Dataset III exp.\n2\nModel\nMetrics\nRMSE\nMAPE\nSMAPE\nNon-Residual Algorithm\nTransformer\n78.409\n0.0103\n1.04\n% SMAPE Improvement\nLSTM\n74.140\n0.009\n0.98\nGRU\n78.014\n0.0103\n1.03\nStacked LSTM\n78.677\n0.0104\n1.04\nResLearn Solution\nTransformer\n42.950\n0.0041\n0.41\n60.69%\nLSTM\n42.860\n0.0041\n0.41\n58.08%\nGRU\n40.724\n0.004\n0.40\n61.39%\nStacked LSTM\n48.184\n0.0042\n0.42\n60.1%\nTABLE VI: SMAPE comparison between Transfer Learning\nSoA and ResLearn Solution\nTraffic\nTransfer\nLearning SoA\n[14]\nResLearn\n% SMAPE\nImprove-\nment\nBeatSaber 40 Mbps\n404.05\n0.36\n99.91%\nBeatSaber 54 Mbps\n285.29\n1.01\n99.64%\nBeatSaber 120 Mbps\n371.82\n0.15\n99.95%\nSteam VR house 40 Mbps\n562.87\n0.45\n99.92%\nSteam VR house 54 Mbps\n404.41\n0.69\n99.82%\n\n(a)\n(b)\n(c)\n(d)\nFig. 6: (a) ResLearn prediction of 18th segment for Dataset I,\n(b) 18th segment prediction from non-ResLearn solution, (c)\nResLearn’s Overall prediction of Dataset I, (d) non-ResLearn’s\noverall prediction of Dataset I. All four figures are predicted\nfor frame size. ResLearn is significantly better at predicting the\npeak necessary for network management for higher resource\nconsumption.\nV. CONCLUSION AND FUTURE WORK\nOur work significantly advances Metaverse network traffic\nprediction by introducing a comprehensive, real-world dataset\nand developing novel algorithms including the view-frame and\nResLearn algorithms. These algorithms enable ISPs to manage\nnetwork resources in an effective manner, satisfying the QoS\nand enhancing the user experience for Metaverse applications.\nGiven that our solution substantially reduces prediction errors\nabout 99% than the SoA [14], future work can focus on\nexpanding the dataset to cover a broader range of Metaverse\napplications and environments, integrating advanced AI tech-\nniques to improve prediction accuracy further, and exploring\nreal-time deployment in diverse network architectures. Addi-\ntionally, adaptive algorithms for dynamic resource allocation\nin response to traffic fluctuations will be investigated for en-\nhancing the robustness in provisioning Metaverse ecosystem.\nREFERENCES\n[1] E. Kontogianni and L. Anthopoulos, “Towards a standardized metaverse\ndefinition: Empirical evidence from the itu metaverse focus group,” IEEE\nEngineering Management Review, 2024.\n[2] A. Suh and J. Prophet, “The state of immersive technology research: A\nliterature analysis,” Computers in Human behavior, vol. 86, pp. 77–90,\n2018.\n[3] E. Ekudden, “Future network trends driving universal metaverse\nmobility,” https://www.ericsson.com/4a7138/assets/local/reports-papers/\nericsson-technology-review/docs/2022/technology-trends-2022.pdf,\n2022, (Accessed on 08/14/2024).\n[4] Qualcomm, “The mobile future of extended reality (XR),” https://www.\nqualcomm.com/content/dam/qcomm-martech/dm-assets/documents/\nawe 2017 - the mobile future of extended reality -for pdf 1.pdf,\n11 2020, (Accessed on 08/14/2024).\n[5] Roblox, “Roblox partners with sony music entertainment to bring their\nartists into the metaverse,” https://corp.roblox.com/newsroom/2021/07/\nroblox-partners-sony-music-entertainment-bring-artists-metaverse,\n06\n2021, (Accessed on 08/14/2024).\n[6] A. Musamih, I. Yaqoob, K. Salah, R. Jayaraman, Y. Al-Hammadi,\nM. Omar, and S. Ellahham, “Metaverse in healthcare: Applications, chal-\nlenges, and future directions,” IEEE Consumer Electronics Magazine,\nvol. 12, no. 4, pp. 33–46, 2022.\n[7] H. Wang, H. Ning, Y. Lin, W. Wang, S. Dhelim, F. Farha, J. Ding,\nand M. Daneshmand, “A survey on the metaverse: The state-of-the-\nart, technologies, applications, and challenges,” IEEE Internet of Things\nJournal, vol. 10, no. 16, pp. 14 671–14 688, 2023.\n[8] S. Zhao, H. Abou-zeid, R. Atawia, Y. S. K. Manjunath, A. B. Sediq, and\nX.-P. Zhang, “Virtual reality gaming on the cloud: A reality check,” in\n2021 IEEE Global Communications Conference (GLOBECOM). IEEE,\n2021, pp. 1–6.\n[9] I. T. Feldstein and S. R. Ellis, “A simple video-based technique for\nmeasuring latency in virtual reality or teleoperation,” IEEE Transactions\non Visualization and Computer Graphics, vol. 27, no. 9, pp. 3611–3625,\n2020.\n[10] M. Li, J. Gao, C. Zhou, X. Shen, and W. Zhuang, “User dynamics-aware\nedge caching and computing for mobile virtual reality,” IEEE Journal\nof Selected Topics in Signal Processing, vol. 17, no. 5, pp. 1131–1146,\n2023.\n[11] Y. S. K. Manjunath, S. Zhao, X.-P. Zhang, and L. Zhao, “Time-\ndistributed feature learning for internet of things network traffic clas-\nsification,” IEEE Transactions on Network and Service Management, p.\n”early access”, 2024.\n[12] W. Luo, W. Liu, D. Lian, and S. Gao, “Future frame prediction network\nfor video anomaly detection,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 44, no. 11, pp. 7505–7520, 2022.\n[13] M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb,\nI. King, and S. Pan, “A survey on graph neural networks for time series:\nForecasting, classification, imputation, and anomaly detection,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2024.\n[14] S. Vaidya, H. Abou-Zeid, and D. Krishnamurthy, “Transfer learning for\nonline prediction of virtual reality cloud gaming traffic,” in GLOBECOM\n2023 - 2023 IEEE Global Communications Conference, 2023, pp. 4668–\n4673.\n[15] Y. S. Kuruba Manjunath, L. Zhao, and X.-P. Zhang, “Metaverse network\ntraffic for classification and prediction,” 2024. [Online]. Available:\nhttps://dx.doi.org/10.21227/0qs9-f852\n[16] A. Vaswani, “Attention is all you need,” Advances in Neural Information\nProcessing Systems, 2017.\n[17] Y. S. K. Manjunath, M. Szymanowski, and A. Wissborn, “yoga-suhas-\nkm/reslearn: Reslearn: Transformer-based residual learning for meta-\nverse network traffic prediction,” https://github.com/yoga-suhas-km/\nResLearn, (Accessed on 10/11/2024).\n[18] Meta, “Set up and connect meta quest link and air link — meta store,”\nhttps://www.meta.com/help/quest/articles/headsets-and-accessories/\noculus-link/connect-with-air-link/, (Accessed on 10/08/2024).\n[19] V. Desktop, “Virtual desktop streamer,” https://www.vrdesktop.net/, (Ac-\ncessed on 09/29/2024).\n[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016, pp. 770–778.\n[21] B. Shah and H. Bhavsar, “Time complexity in deep learning models,”\nProcedia Computer Science, vol. 215, pp. 202–210, 2022.\n[22] S. Baldoni, F. Battisti, F. Chiariotti, F. Mistrorigo, A. B. Shofi,\nP. Testolina, A. Traspadini, A. Zanella, and M. Zorzi, “Questset: A VR\ndataset for network and quality of experience studies,” in Proceedings of\nthe 15th ACM Multimedia Systems Conference, ser. MMSys ’24, 2024, p.\n408–414. [Online]. Available: https://doi.org/10.1145/3625468.3652187\n[23] C. Asano, “Runs test for a circular distribution and a table of probabil-\nities,” Annals of the Institute of Statistical Mathematics, vol. 17, no. 1,\npp. 331–346, 1965.",
    "pdf_filename": "ResLearn_Transformer-based_Residual_Learning_for_Metaverse_Network_Traffic_Prediction.pdf"
}