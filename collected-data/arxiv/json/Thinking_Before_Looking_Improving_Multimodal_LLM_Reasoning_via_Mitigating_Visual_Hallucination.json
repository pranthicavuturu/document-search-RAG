{
    "title": "Thinking Before Looking:",
    "abstract": "have rapidly advanced in recent years [1, 2, 20, 21], ex- Multimodal large language models (MLLMs) have ad- tending LLMs’ capabilities into the multimodal realm by vanced the integration of visual and linguistic modali- integrating visual backbones to align visual and language ties,establishingthemselvesasthedominantparadigmfor representations. MLLMs have demonstrated exceptional visual-language tasks. Current approaches like chain of performance in a range of vision-related tasks, including thought(CoT)reasoninghaveaugmentedthecognitiveca- visual question answering [4], object recognition [7, 42], pabilitiesoflargelanguagemodels(LLMs),yettheiradap- andvideocomprehension[22],highlightingtheimpressive tation to MLLMs is hindered by heightened risks of hallu- evolutionofAI-drivenvisual-languageunderstanding. The cination in cross-modality comprehension. In this paper, successofCoTpromptinginunimodalcontextssuggestsa wefindthatthethinkingwhilelookingparadigmincurrent promising extension to multimodal scenarios. Due to the multimodal CoT approaches—where reasoning chains are integration of pretrained vision models [27, 32] and lan- generated alongside visual input—fails to mitigate hallu- guage models [44] in MLLMs, various types of hallucina- cinations caused by misleading images. To address these tionshavebeenobserved[5,16,18],includingnonexistent limitations, we propose the Visual Inference Chain (VIC) object generation [33], visual misinterpretations [17], and framework, a novel approach that constructs reasoning cross-modalitybiases[35]. chainsusingtextualcontextalonebeforeintroducingvisual Offering advantages in tackling complex multimodal input,effectivelyreducingcross-modalbiasesandenhanc- tasks,currentpromptingapproaches[46,47]predominantly ing multimodal reasoning accuracy. Comprehensive eval- adhere to the thinking while looking paradigm, where rea- uations demonstrate that VIC significantly improves zero- soningoccurssimultaneouslywiththeintegrationofvisual shotperformanceacrossvariousvision-relatedtasks,miti- elements. However, this paradigm encounters substantial gatinghallucinationswhilerefiningthereasoningcapabil- challenges due to the prevalence of hallucinations, which ities of MLLMs. Our anonymized code repository can be undermineboththereliabilityofthereasoningprocessand found at https://github.com/Terry-Xu-666/ the accuracy of the responses. As illustrated in Figure 1, visual_inference_chain. the MLLM often falls into stereotypes when processing a question-image pair, where it recalls similar prior contexts and overlooks subtle variations, leading to erroneous re- 1.Introduction sponses. EvenwithCoTprompting,themodeltendstorely Large Language Models (LLMs), such as GPT-4 [1] and onmemory-basedstereotypesratherthanengaginginaccu- Llama[34],havedrivenremarkableadvancementsinworld ratereasoning,whichleadstoincorrectresponses. comprehension[11]andreasoningcapability[12]. Prompt- To overcome hallucinations from visual inputs and har- ing techniques like CoT [9, 28, 38] have been developed ness the reasoning capabilities of LLMs, we propose the toenhanceLLMs’abilitytohandlecomplextasksthrough VisualInferenceChain(VIC),whichintroducesareasoning processthatoccurspriortoengagingwithvisualelements, *Theseauthorscontributedequallytothiswork. †VisitingstudentatLehighUniversity. following the thinking before looking paradigm. This ap- 1 4202 voN 51 ]VC.sc[ 1v19521.1142:viXra",
    "body": "Thinking Before Looking:\nImproving Multimodal LLM Reasoning via Mitigating Visual Hallucination\nHaojieZheng1*† TianyangXu2 *† HanchiSun3 ShuPu4 RuoxiChen3† LichaoSun3\n1UniversityofPennsylvania 2ColumbiaUniversity\n3LehighUniversity 4IndependentResearcher\nhaojiez@seas.upenn.edu, tx2240@columbia.edu, pushuabc@gmail.com\nhas423,lis221@lehigh.edu,chenrx0830@gmail.com\nAbstract human-like step-by-step reasoning. Meanwhile, MLLMs\nhave rapidly advanced in recent years [1, 2, 20, 21], ex-\nMultimodal large language models (MLLMs) have ad- tending LLMs’ capabilities into the multimodal realm by\nvanced the integration of visual and linguistic modali- integrating visual backbones to align visual and language\nties,establishingthemselvesasthedominantparadigmfor representations. MLLMs have demonstrated exceptional\nvisual-language tasks. Current approaches like chain of performance in a range of vision-related tasks, including\nthought(CoT)reasoninghaveaugmentedthecognitiveca- visual question answering [4], object recognition [7, 42],\npabilitiesoflargelanguagemodels(LLMs),yettheiradap- andvideocomprehension[22],highlightingtheimpressive\ntation to MLLMs is hindered by heightened risks of hallu- evolutionofAI-drivenvisual-languageunderstanding. The\ncination in cross-modality comprehension. In this paper, successofCoTpromptinginunimodalcontextssuggestsa\nwefindthatthethinkingwhilelookingparadigmincurrent promising extension to multimodal scenarios. Due to the\nmultimodal CoT approaches—where reasoning chains are integration of pretrained vision models [27, 32] and lan-\ngenerated alongside visual input—fails to mitigate hallu- guage models [44] in MLLMs, various types of hallucina-\ncinations caused by misleading images. To address these tionshavebeenobserved[5,16,18],includingnonexistent\nlimitations, we propose the Visual Inference Chain (VIC) object generation [33], visual misinterpretations [17], and\nframework, a novel approach that constructs reasoning cross-modalitybiases[35].\nchainsusingtextualcontextalonebeforeintroducingvisual\nOffering advantages in tackling complex multimodal\ninput,effectivelyreducingcross-modalbiasesandenhanc-\ntasks,currentpromptingapproaches[46,47]predominantly\ning multimodal reasoning accuracy. Comprehensive eval-\nadhere to the thinking while looking paradigm, where rea-\nuations demonstrate that VIC significantly improves zero-\nsoningoccurssimultaneouslywiththeintegrationofvisual\nshotperformanceacrossvariousvision-relatedtasks,miti-\nelements. However, this paradigm encounters substantial\ngatinghallucinationswhilerefiningthereasoningcapabil-\nchallenges due to the prevalence of hallucinations, which\nities of MLLMs. Our anonymized code repository can be\nundermineboththereliabilityofthereasoningprocessand\nfound at https://github.com/Terry-Xu-666/\nthe accuracy of the responses. As illustrated in Figure 1,\nvisual_inference_chain.\nthe MLLM often falls into stereotypes when processing a\nquestion-image pair, where it recalls similar prior contexts\nand overlooks subtle variations, leading to erroneous re-\n1.Introduction\nsponses. EvenwithCoTprompting,themodeltendstorely\nLarge Language Models (LLMs), such as GPT-4 [1] and onmemory-basedstereotypesratherthanengaginginaccu-\nLlama[34],havedrivenremarkableadvancementsinworld ratereasoning,whichleadstoincorrectresponses.\ncomprehension[11]andreasoningcapability[12]. Prompt-\nTo overcome hallucinations from visual inputs and har-\ning techniques like CoT [9, 28, 38] have been developed\nness the reasoning capabilities of LLMs, we propose the\ntoenhanceLLMs’abilitytohandlecomplextasksthrough\nVisualInferenceChain(VIC),whichintroducesareasoning\nprocessthatoccurspriortoengagingwithvisualelements,\n*Theseauthorscontributedequallytothiswork.\n†VisitingstudentatLehighUniversity. following the thinking before looking paradigm. This ap-\n1\n4202\nvoN\n51\n]VC.sc[\n1v19521.1142:viXra\nFigure1.ThisexamplefromHallusionBenchdemonstratesthedifferencesbetweenzero-shot,zero-shotCoT,andVIC.Thezero-shotCoT\nrepresentsthethinkingwhilelookingapproach,whichtendstoexhibitstereotypicalreasoningpatternswhenprocessingbothvisualand\ntextualinputssimultaneously.Incontrast,ourthinkingbeforelookingparadigm,VIC,enhancesreasoningqualitybydecouplingthevisual\nandtextualinputs.MoreexamplescanbefoundinAppendixE.\nproachmirrorshumancognition[30,31], wherereasoning tialimprovementof31.74%ontheMMVPbenchmark,in\noften precedes perception. For example, when adults hear ameanwhiletheGPT-4ominimodelshowsanincreaseof\na question before seeing the image, they generate a pre- 16.59%. Across all benchmarks, GPT-series models av-\nliminary plan based on their accumulated experience. The eragea8.02%refinementandGemini-seriesmodelsshow\nquestionactivatesrelevantmemoriesandcontextualknowl- anaverageimprovementof7.19%, underscoringboththe\nedge, allowing them to deduce a forward-looking reason- effectivenessandrobustnessofourmethod.\ning strategy ahead of engaging in visual stereotypes. This\nunderscores that the direct impact of visual elements on 2.RelatedWorks\nthe reasoning process is relatively limited. Analogous to\nCoT reasoning with LLMs. Chain-of-thought (CoT)\nhuman cognition, MLLMs can adopt the thinking before\nreasoning [38] has significantly enhanced large language\nlooking paradigm. By leveraging this paradigm, VIC taps\nmodels (LLMs) performance by guiding them to break\ninto the accumulated forward-looking reasoning capabili-\ndowncomplextasksintointermediatereasoningsteps.Sub-\ntiesofpowerfulLLMs,enablingMLLMstoanticipateand\nsequent work introduced self-consistency [37] where mul-\nrecognize patterns more efficiently by dynamically adjust-\ntiple reasoning paths are generated, and the most consis-\ningreasoningsteps. Additionally, theVICframeworkem-\ntentanswerischosen.Recentadvancesfocusonoptimizing\nploys a systematic multi-step detachment strategy to min-\nCoTwithexampleselectionandenhancedreasoningframe-\nimize compounding hallucinations from both the question\nworks [45], exploring selecting diverse examples to guide\nand the image, further enhancing the MLLMs’ reasoning\nCoT reasoning. Step-Aware Verifier framework enhances\ncapabilitiesforcomplextasks.\nCoT by incorporating a verification step at each interme-\nIn this work, we demonstrate that our thinking be- diatereasoningstage,improvingreasoningreliability[15].\nfore looking strategy outperforms the conventional think- Meanwhile, Tree of Thought (ToT) [40] extends CoT by\ningwhilelookingapproachinmultimodalreasoningtasks. exploring multiple reasoning paths in a tree structure, en-\nOur method demonstrates improved performance in em- suringthoroughconsiderationofpotentialsolutions. Other\npirical studies on GPT-series [26] and Gemini-series [6] refinementsallowmodelstoself-reflectonandadjusttheir\nmodels across various visual question benchmarks. VIC reasoningsteps[39].\nachieves notable improvements on hallucination-specific\nbenchmarks such as MMVP [33], HallusionBench [10], MultimodalCoTreasoning. Consideringthenaturalgap\nand POPE [16], as well as on general multimodal bench- betweentextandvisiondata, MultimodalLargeLanguage\nmarks including MME [8], MathVista [23], and SEED- Models (MLLMs) are less capable of utilizing the reason-\nBench[14].Forinstance,theGemini1.5Proseesasubstan- ing ability of CoT. MMCoT [46] adds a rationale genera-\n2\ntion block before answer inference. DDCoT [47] is pro- tionofhowforward-lookingreasoningcanenhancetherea-\nposed to enhance multimodal reasoning by assigning spe- soningprocessmoreeffectivelythanthinkingwhilelooking\ncific tasks to language and visual models. Meanwhile, Vi- paradigm. Additionally, we explore two key components\nsualCoT[29]incorporatesthenotionofsegmentation,and intheframeworkinSection3.4and3.5, respectively. The\nperforms well for object-detection tasks but lacks versatil- detailed implementation of the prompting mechanism dis-\nity. Image-of-Thought [48] further explores the MLLMs cussed in this section is provided in Appendix D, and the\nCoTpromptingbyintroducingspecificactionslikesegmen- overviewofourmethodisillustratedinFigure2.\ntation, zoom-in, and color-space conversions in the CoT\n3.1.MultimodalLLM\nchain,extendingtheapplicablescenarioswiththesacrifice\nofflexibility. CompositionalCoT[24]extractsthecompo- MLLMs are designed to handle data from various modali-\nsitional information of images as a scene graph (SG) and ties, such as text, images, and audio, enabling the integra-\nthen utilize it during the CoT process. Other works like tionandgenerationofmultimodalinformation. Inthetask\nCoCoT [43] and KAM-CoT [25], consider multiple input of Visual Question Answering (VQA), the MLLM is pro-\nimagescenariosandincorporateknowledgegraphs(KG)of vided with an image input I and a textual input Q. These\nmultimodal information. However, many such techniques inputs are then encoded into a shared representation space\nrely on fixed templates for extracting predetermined infor- usingapre-trainedvisualencodingmodel,andafixedtext\nmation,resultinginalackofflexibilityandtheutilizationof tokenizationprocessforthetextualdata. Theencodedrep-\nreasoning capabilities. The straightforward thinking while resentations from both visual and textual data are subse-\nlooking approach struggles to resolve problematic cross- quently processed by large language model. We charac-\nmodalinteractions,oftenresultinginhallucinations. terize the whole MLLM as f′(∗), which has been trained\nonunifiedrepresentationdatafrombothmodalities.During\nHallucination in MLLMs. Hallucination remains a sig- inference,themodelgeneratesaresponseAgivenIandQ,\nnificant challenge in MLLMs, where models generate in- whichcanbeformalizedas:\nformationthatisfactuallyincorrectorirrelevanttothegiven\ninputs[3]. Inmultimodalreasoning,hallucinationscanoc-\nA=f′(I,Q). (1)\ncur when models produce textual outputs not grounded in\nthe visual data, leading to fabricated details in tasks such For the sake of simplicity, we ignore the tokenizer and\nas image captioning or visual question answering (VQA). pre-trained visual encoding model since they primarily\nFor hallucination evaluation, FaithScore [13] extracts fine- serve as preprocessing components. This formulation al-\ngrained atomic facts from the generated answer and then lowsMLLMstofocusonleveragingthecombinedinforma-\nconducts consistency verifications. POPE, a polling-based tionfrommultipledatatypes, enablingthemtoeffectively\nquerymethod[16]isproposedforabetterevaluationofob- process and respond to complex, multimodal queries with\nject hallucination. Recently, HallusionBench [10] further greaterefficiency.\nexplored the hallucination evaluation, by editing the origi-\n3.2.Thinkingwhilelooking\nnalinputimageandformingdifferenttext-imagepairstodi-\nagnosefailuretypesfromlanguagehallucinationandvisual A simple VQA inference process is illustrated in Equa-\nillusion. Severalhallucination-mitigatingmethodsarealso tion 1. Moreover, the thinking while looking paradigm in-\nproposed. To conduct instruction tuning that mitigate hal- volves step-by-step reasoning trajectory while simultane-\nlucination,LRV[19]servesasthefirstlargeanddiversevi- ously processing the visual input. This approach gener-\nsualinstructiontuningdatasetandVIGC[36]aimsfordata ates a reasoning strategy chain {s }k , accompanied by\nn n=1\ngeneration. As for training-free method, Woodpecker [41] corresponding rationales, denoted as {r }k , where the\nn n=1\nuses fixed steps to correct hallucination in MLLMs. Our value of k varies dynamically depending on both the spe-\nmethod, however, generates applicable reasoning trajecto- cific model and the given question-image pair. In parallel\nries withoutvisual elements fora given question, which is with generating these reasoning steps and rationales, the\nmore flexible and accurate than fixed analyzing steps, and modelalsoproducesthefinalanswerA.\ncouldbefurtherutilizedacrossdifferentVQAtasks.\n3.Method ({s ,r }k ,A)=f′(I,Q,P ). (2)\nn n n=1 cot\nIn this section, we begin by introducing the preliminary Inthisformula,{s ,r }k representsthesequenceof\nn n n=1\narchitecture of MLLMs and the process of thinking while reasoningstepsandcorrespondingrationales. P denotes\ncot\nlooking paradigm in Section 3.1 and Section 3.2 respec- the prompt used for CoT prompting, which can be either\ntively. In Section 3.3, we present the foundation of think- a zero-shot prompt such as “Let’s think step by step” or a\ningbeforelookingparadigmalongwithadetailedexplana- few-shotpromptdesignedinanin-contextlearningmanner.\n3\nFigure2. TheoverallframeworkofVIC(VisualInferenceChain). VICdecouplesvisualandtextualinputstoimprovereasoning. It\nfirstgeneratesintermediatereasoningstepsfromthequestionQandpromptP .TheimageIisprocessedthroughanMLLMtoextract\nvic\nrationalesR,which,combinedwiththevisualinferencesteps{s }k,leadtothefinalanswerAwithenhancedaccuracy.\nn 1\nAlthough Equation 2 demonstrates how CoT elucidates sual elements. Thus, LLMs f(∗) can automatically gener-\ntheMLLM’sthinkingprocessandenhancesitscapacityto ate forward-looking reasoning steps as an average over a\nmanagecomplextasks,thetightinterweavingoftextualand broader context of similar situations, triggered by the in-\nvisualinputsintroduceshallucinationissues,asthereason- put(Q,P ), ratherthanfocusingsolelyonaspecificin-\nvic\ningsequence{s ,r }k canbecomebiased. Asshownin putpair(Q,I). Thisprocessdiscretizesthereasoningsteps\nn n n=1\nFigure 1, the model generates a hallucination by identify- from the specific input pair, reducing hallucinations while\ning a non-existent Tichener circle illusion due to the com- maintainingthevalidityofinstructionsforimageanalysis,\nbinedinfluenceofvisualcuesandthequestion,whichacti- as these instructions align with the aggregate of the most\nvatesmemoriesofthemostsimilarcontextfromitstraining relevantcontextualknowledge.Consequently,thevisualin-\nphase. This example highlights the biased reasoning steps ferenceprocesscanbeexpressedasfollows.\nproduced by the vanilla CoT method, leading to improper\nreasoninginthemodel’sresponses.\n{s }k =f(Q,P ) (3)\nn n=1 vic\n3.3.VICgeneration\nByemployingthisspecificprompt,wegenerateavisual\nThe limitations of the thinking while looking paradigm inference chain, denoted as {s }k , which serves as the\nn n=1\ninspire the development of the thinking before looking reasoningprocessderivedexclusivelyfromthegivenques-\nparadigm. This new approach promotes forward-looking tion. This process can be divided into two main compo-\nreasoning, enhancingthereasoningprocessandimproving nents. The initial k−1 steps, {s }k−1, correspond tothe\nn n=1\nthe quality of rationality. Separating visual and textual in- sequenceofinstructionsrelatedtobothrecognitionandrea-\nputsallowsformorestructuredthinkingandclearercogni- soning. Andthefinalstep,s ,ofthevisualinferencechain\nk\ntive steps. This approach decouples the question from the introduces a format instruction based on the specific ques-\nimage, reducing bias in the reasoning steps sequence and tion. This step is critical in further enhancing the frame-\nenhancingoverallreasoningperformance. work’sabilitytofollowcomplexinstructionseffectively.\nThelanguagemodelshaveinternalizedextensivereason- The advantage of the thinking before looking phase is\ning knowledge from large-scale pre-trained data, allowing thatiteliminatestheimmediateneedforvisualinformation.\nthemtogainbeneficialinsightsintoimagereasoninganaly- As a result, the model f(∗) can function as either a large\nsis. Duetotheirhighlydevelopedpatternrecognitionabil- languagemodeloramultimodallanguagemodeloperating\nities, these models can accumulate sufficient background in a blind mode. This concept of thinking before looking\nknowledgeandgeneratereasonablereasoningstepsforvi- allowsustoleveragethesuperiorreasoningcapabilitiesof\n4\nadvancedlargelanguagemodels, offeringsignificantflexi- 4.Experiment\nbilityandgeneralizability.\n4.1.ExperimentSetup\n3.4.VICrationaleextraction Datasets Our framework is evaluated on six benchmark\ndatasets. To evaluate the generality and versatility of the\nHallucinationscommonlyarisefromdeepentanglementbe-\nVIC framework, we tested it across two key benchmark\ntween image and textual inputs. In this step, we decouple\ncategories: (1) Hallucination detection benchmark includ-\ntheoriginalquestionandrequiringtheMLLMtorecognize\ning HallusionBench [10], MMVP [33], and POPE [16],\nand follow the visual inference chain step by step, miti-\nallofwhicharedesignedtoanalyzehallucinationsthatare\ngating the effects of textual bias. Moreover, the visual in-\nprone to occur in different forms. (2) General multimodal\nferencechainprovidesamoreprecisetrajectorycompared\ncapability benchmark, MME [8] and SEED-Bench [14],\ntotheoriginalquestion,necessitatingdetailedextractionof\nwhich cover general and comprehensive forms of visual\nrelevant information and thus reducing the risk of halluci-\nquestion answering. MathVista [23] that focuses on math\nnationsbyvisualinferencechain.\nelementrecognitionandreasoningproblems, evaluatesthe\nUnlike the previous step, which can use any modal-\ncompound capability to solve visual math challenges. We\nity model f, this step specifically employs the MLLM f′.\ndiscussthedetailsandimplementationsforeachbenchmark\nLeveraging the multitask-following capability of closed-\ninAppendixA.\nformmodels,wegeneratetheVICrationalesinasinglestep\ntoproducetheentiresetofrationales{r }k−1,denotedcol-\nn n=1 Baseline In our experiments, we compare the proposed\nlectivelyasR. Wefurtherexplorethedifferencesbetween\nVIC methodology with two primary baselines, as outlined\nsingle-steprationaleextractionandmulti-steprationaleex-\nin Table 1. The first baseline involves applying the model\ntractioninourablationexperiments.\ndirectlytoeachbenchmarkwithoutincorporatinganyspe-\ncializedtechniques,enablingustoevaluatetheaddedben-\nefit of our method to the pretrained model’s performance.\nR=f′(I,P ,{s }k−1) (4)\nextract n n=1 Thesecondbaselineiszero-shotCoTpromptengineering.\nIn this approach, we append the reasoning prompt such as\nInthisformula,Irepresentstheimagecorrespondingto ”Let’sthinkstepbystep”totheinput, followingtheques-\nthe specific question Q. The sequence {s }k−1 refers to tion. Sincethemodel’soutputinthiscaseincludesmultiple\nn n=1\nthe first k − 1 steps of the process, excluding the format reasoningsteps,wethenpassitthroughananswerextractor\ninstructions. P isapromptdesignedtointegratethe to derive the final answer, in a manner similar to the final\nextract\nimagewiththevisualinferencechain,facilitatingeffective stage of the VIC method. This comparison highlights the\ninformationextraction. differences and advantages of our thinking before looking\nstrategy. Additionally, we introduce human performance\n3.5.AnswerInference andrandomchoicesasfurtherbaselines, providingabasic\nperformancereferencepointforeachbenchmark.\nInthefinalstep,weprovidethesameinputstotheMLLM\nf asusedduringtheVICrationalegenerationphase.These\nϕ Implementation We conducted our experiments primar-\ninputs include the original question-image pair Q,I, the\nily on four popular closed-source MLLMs: Gemini 1.5\nVICrationaleresultsR, theformatinstructions , andthe\nk Flash,Gemini1.5Pro,GPT-4o,andGPT-4omini. Therea-\nreflectionpromptP .\nreflect sonforchoosingthesemodelsmainlyinvolvedtwoaspects.\n(i)Ourapproachisgroundedinthebeliefthatthethinking\nbefore looking capability should not be exclusive to large\nA=f′(I,Q,R,P ,s ) (5)\nreflect k models, driving us to experiment on both large and small\nmodels.(ii)Ourmethodalsoleveragesthelonginstructions\nThisprocessincorporatestheVICrationaleresultsasad- following ability, which is a key strength of closed-source\nditional information to support the model’s response. Fur- models. The experiments were conducted in three steps.\nthermore,weintroduceareflectionmechanismatthisstage, Firstly, we utilized the VIC-prompt, where the input ques-\nencouraging the model to reconsider the question, image, tion text was fed into the models to generate Visual Infer-\nandVICrationaleresults, ratherthantreatingtherationale enceChain. Thesegeneratedinstructionswerethenpaired\nasthedefinitiveanswer. Theformatinstructions ensures withtheoriginalimagetoextractVICrationals.Finally,we\nk\nthe response adheres to the desired format by guiding the derivetheanswertothequestionbasedontherationalewe\nmodel’sanalysisoftheuser’squestionorquery,therebyim- extracted. Additionally,wealsoranseveralcomplementary\nprovinginstruction-followingperformance. experimentsonopen-sourcemodels,suchasQwen-VL.For\n5\nMME\nModels Method MMVP HallusionBench POPEadversarial Mathvista SEED-Benchsingle Average\nPerception Cognition\nHuman 0.957 0.986 0.995 0.603 0.967 0.901 - -\nBaselines\nRandomchoice 0.250 0.500 0.500 0.179 0.250 0.336 - -\nOrigin 0.446 0.574 0.786 0.526 0.636 0.590 1097.23 407.14\nGPT-4omini zero-shotCoT 0.443(↓2.85%) 0.611(↑6.46%) 0.773(↓1.65%) 0.520(↓1.14%) 0.660(↑3.77%) 0.601(↑1.86%) 1069.81(↓2.50%) 417.14(↑2.46%)\nVIC 0.520(↑16.59%) 0.639(↑11.37%) 0.793(↑0.89%) 0.536(↑1.90%) 0.696(↑9.43%) 0.637(↑7.96%) 1105.27(↑0.73%) 505.00(↑24.04%)\nOrigin 0.527 0.560 0.769 0.479 0.658 0.599 1077.36 358.92\nGemini1.5Flash zero-shotCoT 0.513(↓2.54%) 0.614(↑9.69%) 0.741(↓3.64%) 0.520(↑8.56%) 0.672(↑2.13%) 0.612(↑2.17%) 1105.9(↑2.65%) 500.71(↑39.50%)\nVIC 0.553(↑5.05%) 0.638(↑13.93%) 0.780(↑1.43%) 0.516(↑7.72%) 0.713(↑8.36%) 0.640(↑6.84%) 1118.54(↑3.82%) 508.21(↑41.59%)\nOrigin 0.673 0.626 0.811 0.597 0.657 0.673 1174.39 522.85\nGPT-4o zero-shotCoT 0.687(↑1.99%) 0.673(↑7.49%) 0.793(↓2.22%) 0.622(↑4.19%) 0.739(↑12.48%) 0.701(↑4.16%) 1166.12(↓0.70%) 537.14(↑2.73%)\nVIC 0.747(↑10.90%) 0.692(↑10.52%) 0.827(↑1.27%) 0.620(↑3.85%) 0.751(↑14.31%) 0.727(↑8.08%) 1238.69(↑5.48%) 557.85(↑6.69%)\nOrigin 0.420 0.617 0.779 0.568 0.678 0.612 1166.82 462.14\nGemini1.5Pro zero-shotCoT 0.500(↑19.05%) 0.611(↓0.97%) 0.793(↑1.67%) 0.563(↓0.88%) 0.691(↑1.92%) 0.632(↑3.20%) 1099.33(↓5.78%) 521.42(↑12.83%)\nVIC 0.553(↑31.74%) 0.664(↑7.62%) 0.803(↑3.08%) 0.558(↓1.76%) 0.713(↑5.16%) 0.658(↑7.55%) 1147.23(↓1.68%) 561.42(↑21.48%)\nTable1.MainResultsAcrossModelsandBenchmarks.ComparisonofmodelperformanceonbenchmarkssuchasMMVP,Hallusion-\nBench,andMathvista,includingmetricsforPerceptionandCognitionunderMME.Improvementsaremarkedingreenanddeclinesin\nred.Boldvaluesdenotethebestresults.\nfurtherdetailsandresultsfromtheseexperiments,pleasere-\nfertoSection4.3.\n4.2.Results\nThe results of the Visual-Inference-Chain on six different\nbenchmarksusingtwoopen-sourcemodelsarelistedinTa-\nble 1. We primarily use average accuracy as the evalua-\ntionmetrictoquantifythemodels’performance,exceptfor\ntheMMEbenchmark,whereweretaintheoriginalcompos-\nite evaluation score that combines accuracy and a refined\naccuracy metric (Accuracy+). This approach allows for a\nFigure3. DetailedevaluationcomparisonswithandwithoutVIC\nconsistentcomparisonwithpreviousrecordsonMME.For\nfortwomodelsonHallusionBench.Features:AA-AllAccuracy,\nthe MMVP benchmark, we use pair accuracy to measure\nHA-HardAccuracy,EA-EasyAccuracy,FA-FigureAccuracy,\nthemodel’sabilitytoovercomethehallucinationcausedby QPA-QuestionPairAccuracy.\nCLIP.\nAcross the comprehensive benchmarks MME and\nSEED-Bench,themodelsshowanaverageperformanceim- tasks that are highly vision-dependent. Regarding the hal-\nprovementof9.13%, demonstratingthatourmethodVIC lucinationbenchmarks,wefirsttestonadversarialsamples\nconsistently enhances performance across all tasks. These in POPE. Our method still effectively handles these chal-\nresultshighlighttheefficacyofVICinboostingmultimodal lenging scenarios, improving the models’ performance by\nreasoning capabilities. For the Mathvista benchmark, al- 1.36%. AsforMMVP,theaverageimprovementachieves\nthoughtheaveragegainisslightlyreducedto3.15%,thisis 15.62%, which marks that VIC significantly overcomes\nlargelyduetothenatureofthisbenchmark,whichlacksde- thehallucinationcausedbyusingCLIPasavisionencoder.\ntailedexplanationsofquestionsandisheavilyreliantonvi- HallusionBench is another critical benchmark we want to\nsualinformation.Forexample,questionslike”IsR0greater focus on, the details of HallusionBench experiments are\nthan0?”requiredirectvisualinterpretation, makingitdif- shown in Figure 3. where we observe notable improve-\nficult to generate an effective visual inference chain with- mentsinbothQuestionPairAccuracyandFigureAccuracy,\nout explicit vision inputs. Despite these challenges, the indicatingthatVIChelpsmitigatebothlanguagehallucina-\nimprovement underscores the versatility of VIC, even in tions and visual illusions. Another key observation is the\n6\nmodels for VIC generation. In our experiments, we em-\nployed a range of models, including pure language mod-\nels such as GPT-4 Turbo and Qwen-Max, alongside mul-\ntimodal models like GPT-4o mini and Gemini 1.5 Flash,\nasvisualinferencechaingenerators. Additionally,wecon-\nducted ablation experiments to investigate the differences\nbetween single-step and multi-step VIC rationale extrac-\ntion, the impact of reflective prompting during the answer\ngenerationphase,andtheperformanceofopen-sourcemod-\nels. Formoredetailedresults,pleaserefertoAppendixC.\nFigure4.PerformancecomparisonofGemini1.5FlashandGem-\nini1.5ProusingZero-shotCoTandVICmethodsacrossvarious DifferentVICgenerator. AsshowninFigure5,different\nevaluationmetricsonHallusionBench. visualinferencechain(VIC)generatorssignificantlyaffect\nVIC performance. Leveraging the ”thinking before look-\ning”paradigm,wecanselecteitherapurelanguagemodel\nenhancement in Hard Accuracy, which measures the mod- oramultimodalmodeloperatinginablindmode. Thus,we\nels’ ability to interpret human-edited images from Hallu- tested GPT-4 Turbo, Qwen-Max, GPT-4o mini, and Gem-\nsionBench. ini1.5FlashasVICgeneratorsontwobenchmarks,Hallu-\nThecategorizationresultsofGemini1.5FlashandGem- sionBench and SEED-Bench. On SEED-Bench, the maxi-\nini 1.5 Pro on HallusionBench are presented in Figure 4. mumperformancevariationis3.95%forGemini1.5Flash\nComparedtoZero-shotCoT,VICdemonstratessignificant and5.98%forGPT-4omini. Akeyfindingistheminimal\nimprovementsinfigureunderstanding,reductionofillusion performance difference between chains generated by pure\nerrors,andenhancedmapidentification,whilemaintaining languagemodelsandthosebymultimodalmodels. Forin-\nstrong performance across other tasks. This result high- stance, Qwen-Max’s visual inference chain performs best\nlightsVIC’sabilitytosignificantlyimprovetherobustness for Gemini 1.5 Flash on HallusionBench, while GPT-4o\nof the models, positioning it as a vital innovation in anti- mini’schainachievesthehighestscoreforGemini1.5Flash\nhallucinationmethods. Moredetailsontheexperimentcan onSEED-Bench.Thissuggeststhatbothpurelanguageand\nbefoundinAppendixB. multimodalmodelsoffercomparablereasoningcapabilities\nforimage-relatedtasks,eventhoughmultimodalmodelsare\nmorespecializedinimagecomprehension. Overall,thereis\nnoclearpatterninwhichchainconsistentlyperformsbest,\naseffectivenessvariesbasedonfactorssuchasthecompat-\nibilityofthevisualinferencechainwiththeMLLMandthe\nVIC generator’s ability to address specific question types.\nForexample, theGPT-4ominivisualchainexcelsongen-\neralquestionsinSEED-Benchbutperformstheweakeston\nHallusionBench.\nOnestepVICrationaleextraction. TheVICgeneration\nprocessusestwomainmethods: asingle-stepapproachand\na multi-step approach. In the single-step method, the en-\ntire inference chain is processed as a unified input, while\nthemulti-stepapproachcompletestasksincrementally,inte-\ngratingintermediateresultsateachstep. Evaluationsonthe\nFigure5.PerformanceofDifferentVICGenerators.Thischart HallusionBench and SEED-Bench benchmarks show dis-\ncompares the performance of various VIC generators on Hallu- tinct advantages for each approach. Overall, the single-\nsionBenchandSEED-Bench.Greybarsrepresenttheoriginalper- step method yields more stable improvements, ensuring\nformanceforreference. greater consistency across tasks. This approach also re-\nduceslatencyandcomputationalcostduetoitssingle-input\nnature. In contrast, the multi-step approach requires re-\n4.3.AblationExperiment\ninputtingimagesandpromptsateachstep,whichincreases\nBy adopting a blind input approach, where the model op- responsetimesandresourcedemands. Althoughthemulti-\neratesindependentlyoftheimage,wecanleveragevarious stepmethodhassomeadvantagesonbenchmarkslikeHal-\n7\nlusionBench, where detailed, stepwise reasoning can im-\nMethod Gemini1.5Flash\nproveaccuracy,thesingle-stepapproachremainsgenerally\nCategory\nsuperiorintermsofefficiency,accuracy,andreliability,es- Origin COTvanilla VICw/oreflectionprompt VIC\npeciallyonmodelslikeGemini1.5FlashandGPT-4omini, Acc 0.658 0.672 0.699 0.701\nachievingnotableperformancegainsonSEED-Benchwith-\nInstanceAttributes 0.677 0.692 0.714 0.711\nouttheriskoferrorpropagation.\nInstanceIdentity 0.763 0.734 0.777 0.806\nInstanceInteraction 0.900 0.700 0.800 0.800\nModel Methods Hallusionbench SEED-bench InstanceLocation 0.500 0.484 0.578 0.594\nInstancesCounting 0.542 0.603 0.648 0.626\nGemini1.5Flash Original 0.560 0.658\nSceneUnderstanding 0.724 0.724 0.710 0.719\nVicM 0.66 0.676\n(↑17.86%) (↑2.74%)\nSpatialRelation 0.419 0.558 0.581 0.605\nVicOneStep 0.638 0.701\n(↑13.93%) (↑6.53%) TextUnderstanding 0.500 0.500 1.000 1.000\nGPT-4oMini Original 0.574 0.636 VisualReasoning 0.821 0.786 0.786 0.750\nVicM 0.673 0.629\n(↑17.21%) (↓1.10%) Table3.PerformancemetricsfordifferentmethodsofGemini1.5\nVicOneStep 0.648 0.676 FlashonSeed-Bench\n(↑12.92%) (↑6.29%)\nTable2. Performancecomparisonacrossmodelsandmethodson\nstrongeroverallperformanceacrossawiderrangeofcapa-\nHallusionbenchandSEED-bench.\nbilitiestoimplementourframeworkeffectively.\nReflection prompt. In the final stage of our approach, a\n5.Discussion\ncriticaltechniqueistotreattheVICrationalenotasdefini-\ntiveinformationbutasaninputthatinvitesreflectionwithin Although our thinking before looking framework demon-\ntheMLLMtogeneratethefinalanswer. Thisreflectiveap- strates remarkable performance across diverse vision-text\nproachencouragesthemodeltore-evaluatethecorrectness tasks, we view it as a crucial enhancement rather than the\nof the VIC rationale, thus enhancing the robustness of the ultimatesolutionforMLLMsreasoning. Itprovidesafresh\noverallframework. TheablationexperimentonGemini1.5 perspective in this domain and holds potential for integra-\nFlash in SEED-Bench shows the impact of this reflection tionwiththethinkingwhilelookingparadigmtoachievesu-\nprompt: removingitleadstoaslightperformancedecrease perioroutcomes.Incomprehensiveevaluationsofthesetwo\nof 0.3%, yet the model still achieves a notable 6.23% im- approaches,wefoundthattheperformanceofthinkingbe-\nprovement over the baseline without the VIC framework. forelookinglaggedbehindthinkingwhilelookingincertain\nAsshowninTable3,theVICframeworksignificantlysur- categories.Wesuggestthatfutureadvancementsshouldpri-\npasses the baseline across various tasks, particularly in ar- oritizemergingtheseparadigmsthroughaselectionmecha-\neas like accuracy, spatial reasoning, and instance identity. nismoraprocessofmutualreflection. Suchacombination\nFor example, accuracy rises from 0.658 with the Origin wouldallowtheparadigmstocomplementeachothereffec-\nmodel to 0.699 with VIC (without reflection) and further tively,addressingtheirindividuallimitationsandpavingthe\nto 0.701 with full reflection. The VIC framework also en- wayformoresophisticatedreasoningstrategies.\nhancesspatialreasoning,withscoresimprovingfrom0.419\ninthebaselineto0.605inthefullsetup.Theinclusionofthe 6.Conclusion\nreflectionpromptsolidifiesthemodel’srobustness,asitpro-\nIn this paper, we introduce the Visual Inference Chain\nmotescareful reassessmentofinitial answers, contributing\n(VIC)framework,anovelmethodthatmitigatesreasoning\ntomoreaccurateanddependablepredictionsacrosstasks.\nbiases in MLLMs by decoupling visual and textual inputs,\nOpen source model. We tested the VIC framework on advancing the thinking before looking paradigm. By sys-\nan open-source model, Qwen-VL Plus, an MLLM with a tematicallyseparatingvisualelements,VICframeworken-\nmediumparametersize. Forsomebenchmarks,theperfor- hances reasoning robustness, significantly reduces halluci-\nmancewiththeVICframeworkdeclined,whileothersstill nations, and improves performance across diverse vision-\nshowedsignificantimprovement.Webelievethisisbecause language tasks. Our experiments demonstrate that VIC\nthe VIC framework requires a high capacity in areas such consistentlyboostszero-shotperformanceandprovidesde-\nasmulti-stepinstructionfollowingandtheabilitytohandle tailed insights into the impact of different VIC genera-\nlongcontextswithvisualinput. Thelackofthesecapabil- tors,underscoringtheeffectivenessofreflectiveprompting.\nities can likely lead to the failure of the VIC framework. Overall,VICframeworkenhancesaccuracyandstrengthens\nTherefore, we prioritized using closed-source models with thereliabilityofmultimodalreasoningeffectively.\n8\nReferences [13] LiqiangJing, RuosenLi, YunmoChen, MengzhaoJia, and\nXinya Du. Faithscore: Evaluating hallucinations in large\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nvision-languagemodels. arXivpreprintarXiv:2311.01477,\nmad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,\n2023. 3\nJankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.\n[14] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\nGpt-4 technical report. arXiv preprint arXiv:2303.08774,\nWang,RuimaoZhang,andYingShan. Seed-bench: Bench-\n2023. 1\nmarkingmultimodallargelanguagemodels. InProceedings\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan oftheIEEE/CVFConferenceonComputerVisionandPat-\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren ternRecognition,pages13299–13308,2024. 2,5\nZhou.Qwen-vl:Afrontierlargevision-languagemodelwith [15] YifeiLi,ZeqiLin,ShizhuoZhang,QiangFu,BeiChen,Jian-\nversatileabilities.arXivpreprintarXiv:2308.12966,2023.1\nGuangLou,andWeizhuChen.Makinglargelanguagemod-\n[3] ZechenBai,PichaoWang,TianjunXiao,TongHe,Zongbo elsbetterreasonerswithstep-awareverifier. arXivpreprint\nHan,ZhengZhang,andMikeZhengShou. Hallucinationof arXiv:2206.02336,2022. 2\nmultimodallargelanguagemodels:Asurvey.arXivpreprint [16] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\narXiv:2404.18930,2024. 3 Zhao, and Ji-Rong Wen. Evaluating object hallucina-\n[4] AnkanBansal,YutingZhang,andRamaChellappa. Visual tion in large vision-language models. arXiv preprint\nquestion answering on image sets. In Computer Vision– arXiv:2305.10355,2023. 1,2,3,5\nECCV2020: 16thEuropeanConference,Glasgow,UK,Au- [17] FuxiaoLiu,TianruiGuan,ZongxiaLi,LichangChen,Yaser\ngust23–28, 2020, Proceedings, PartXXI16, pages51–67. Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-\nSpringer,2020. 1 bench: Youseewhatyouthink? oryouthinkwhatyousee?\n[5] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi animage-contextreasoningbenchmarkchallengingforgpt-\nParikh, et al. Rubi: Reducing unimodal biases for visual 4v(ision),llava-1.5,andothermulti-modalitymodels.arXiv\nquestion answering. Advances in neural information pro- preprintarXiv:2310.14566,2023. 1\ncessingsystems,32,2019. 1 [18] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser\n[6] GoogleDeepMind. Gemini1.5: Unlockingmultimodalun- Yacoob, and Lijuan Wang. Aligning large multi-modal\nderstandingacrossmillionsoftokensofcontext. https: model with robust instruction tuning. arXiv preprint\n//deepmind.google/technologies/gemini/, arXiv:2306.14565,2023. 1\n2024. Accessed:2024-11-14. 2 [19] FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYa-\ncoob, and Lijuan Wang. Mitigating hallucination in large\n[7] YuDu,FangyunWei,ZiheZhang,MiaojingShi,YueGao,\nmulti-modal models via robust instruction tuning. In The\nandGuoqiLi. Learningtopromptforopen-vocabularyob-\nTwelfth International Conference on Learning Representa-\njectdetectionwithvision-languagemodel.InProceedingsof\ntions,2023. 3\ntheIEEE/CVFConferenceonComputerVisionandPattern\n[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nRecognition,pages14084–14093,2022. 1\nImproved baselines with visual instruction tuning. In Pro-\n[8] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nceedingsoftheIEEE/CVFConferenceonComputerVision\nMengdanZhang,XuLin,JinruiYang,XiawuZheng,KeLi,\nandPatternRecognition,pages26296–26306,2024. 1\nXingSun,YunshengWu,andRongrongJi.Mme:Acompre-\n[21] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\nhensiveevaluationbenchmarkformultimodallargelanguage\nVisual instruction tuning. Advances in neural information\nmodels,2024. 2,5\nprocessingsystems,36,2024. 1\n[9] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\n[22] YuanxinLiu,ShichengLi,YiLiu,YuxiangWang,Shuhuai\nTushar Khot. Complexity-based prompting for multi-step\nRen,LeiLi,SishuoChen,XuSun,andLuHou. Tempcom-\nreasoning. In The Eleventh International Conference on\npass:Dovideollmsreallyunderstandvideos? arXivpreprint\nLearningRepresentations,2022. 1\narXiv:2403.00476,2024. 1\n[10] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,Zongxia [23] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,\nLi,XiaoyuLiu,XijunWang,LichangChen,FurongHuang, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nYaserYacoob,etal. Hallusionbench: anadvanceddiagnos- Galley,andJianfengGao. Mathvista:Evaluatingmathemat-\nticsuiteforentangledlanguagehallucinationandvisualil- icalreasoningoffoundationmodelsinvisualcontexts.arXiv\nlusion in large vision-language models. In Proceedings of preprintarXiv:2310.02255,2023. 2,5\ntheIEEE/CVFConferenceonComputerVisionandPattern\n[24] ChancharikMitra,BrandonHuang,TrevorDarrell,andRoei\nRecognition,pages14375–14385,2024. 2,3,5\nHerzig.Compositionalchain-of-thoughtpromptingforlarge\n[11] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen multimodalmodels. InProceedingsoftheIEEE/CVFCon-\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with ferenceonComputerVisionandPatternRecognition,pages\nlanguagemodelisplanningwithworldmodel,2023. 1 14420–14431,2024. 3\n[12] JieHuangandKevinChen-ChuanChang. Towardsreason- [25] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj\ning in large language models: A survey. In Findings of Singh,andGodawariSudhakarRao. Kam-cot: Knowledge\nthe Association for Computational Linguistics: ACL 2023, augmentedmultimodalchain-of-thoughtsreasoning.InPro-\npages 1049–1065, Toronto, Canada, 2023. Association for ceedings of the AAAI Conference on Artificial Intelligence,\nComputationalLinguistics. 1 pages18798–18806,2024. 3\n9\n[26] OpenAI. Gpt-4osystemcard. https://openai.com/ [40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom\nindex/gpt-4o-system-card/, 2024. Accessed: Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of\n2024-11-14. 2 thoughts: Deliberate problem solving with large language\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya models. Advances in Neural Information Processing Sys-\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, tems,36,2024. 2\nAmandaAskell,PamelaMishkin,JackClark,etal.Learning [41] ShukangYin,ChaoyouFu,SiruiZhao,TongXu,HaoWang,\ntransferable visual models from natural language supervi- Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong\nsion.InInternationalconferenceonmachinelearning,pages Chen.Woodpecker:Hallucinationcorrectionformultimodal\n8748–8763.PMLR,2021. 1 large language models. arXiv preprint arXiv:2310.16045,\n[28] OhadRubin,JonathanHerzig,andJonathanBerant. Learn- 2023. 3\ningtoretrievepromptsforin-contextlearning.arXivpreprint [42] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\narXiv:2112.08633,2021. 1 ChenChangeLoy. Contextualobjectdetectionwithmulti-\n[29] HaoShao, ShengjuQian, HanXiao, GuangluSong, Zhuo- modallargelanguagemodels.InternationalJournalofCom-\nfanZong,LetianWang,YuLiu,andHongshengLi. Visual puterVision,pages1–19,2024. 1\ncot: Unleashingchain-of-thoughtreasoninginmulti-modal [43] Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin,\nlanguagemodels.arXivpreprintarXiv:2403.16999,2024.3 Yuan Yao, Mingkai Chen, and Jiebo Luo. Cocot: Con-\n[30] RobertLSolso,MKimberlyMacLin,andOttoHMacLin. trastive chain-of-thought prompting for large multimodal\nCognitive psychology. Pearson Education New Zealand, models with multiple image inputs. arXiv preprint\n2005. 2 arXiv:2401.02582,2024. 3\n[31] Christopher Summerfield and Tobias Egner. Expectation [44] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-\n(andattention)invisualcognition. Trendsincognitivesci- jun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nences,13(9):403–409,2009. 2 Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of\n[32] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue language models with zero-init attention. arXiv preprint\nCao.Eva-clip:Improvedtrainingtechniquesforclipatscale. arXiv:2303.16199,2023. 1\narXivpreprintarXiv:2303.15389,2023. 1 [45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\n[33] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,Yann Automatic chain of thought prompting in large language\nLeCun, and Saining Xie. Eyes wide shut? exploring the models. arXivpreprintarXiv:2210.03493,2022. 2\nvisualshortcomingsofmultimodalllms. InProceedingsof [46] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\ntheIEEE/CVFConferenceonComputerVisionandPattern George Karypis, and Alex Smola. Multimodal chain-of-\nRecognition,pages9568–9578,2024. 1,2,5 thought reasoning in language models. arXiv preprint\n[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier arXiv:2302.00923,2023. 1,2\nMartinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste [47] GeZheng,BinYang,JiajinTang,Hong-YuZhou,andSibei\nRozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Yang. Ddcot: Duty-distinctchain-of-thoughtpromptingfor\nLlama: Open and efficient foundation language models. multimodalreasoninginlanguagemodels.AdvancesinNeu-\narXivpreprintarXiv:2302.13971,2023. 1 ralInformationProcessingSystems,36:5168–5191,2023.1,\n[35] AliVosoughi,ShijianDeng,SongyangZhang,YapengTian, 3\nChenliangXu,andJieboLuo. Crossmodalitybiasinvisual [48] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang\nquestionanswering:Acausalviewwithpossibleworldsvqa. Gao,andYueZhang.Image-of-thoughtpromptingforvisual\nIEEETransactionsonMultimedia,2024. 1 reasoningrefinementinmultimodallargelanguagemodels.\n[36] BinWang,FanWu,XiaoHan,JiahuiPeng,HuapingZhong, arXivpreprintarXiv:2405.13872,2024. 3\nPan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,\net al. Vigc: Visual instruction generation and correction. A.Datasetdetails\nIn Proceedings of the AAAI Conference on Artificial Intel-\nligence,pages5309–5317,2024. 3 MMVP. MMVP is designed to evaluate MLLMs’ visual\n[37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed capability, the benchmark is organized into nine different\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny visualpatternsandthereare15pairsofzero-shotquestions.\nZhou. Self-consistency improves chain of thought reason- It focuses on visual errors commonly arising from models\ning inlanguage models. arXiv preprintarXiv:2203.11171, using CLIP-based vision encoders. MMVP tests the mod-\n2022. 2 els’ capability to correctly identify and interpret subtle vi-\n[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nsualfeaturessuchasobjectrelationships,orientations,and\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nfine-grained details. By highlighting these shortcomings,\nChain-of-thought prompting elicits reasoning in large lan-\nthebenchmarkaimstoprovideamorerigorousevaluation\nguage models. Advances in neural information processing\nframework,helpingresearchersidentifyareaswherevisual\nsystems,35:24824–24837,2022. 1,2\nintegrationinMLLMscanbeimproved.Weused150ques-\n[39] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao,\nMin-YenKan,JunxianHe,andMichaelXie.Self-evaluation tion pairs (300 questions in total) in the MMVP test set to\nguidedbeamsearchforreasoning. AdvancesinNeuralIn- evaluatewhetherourpromptingmethodcanmitigatehallu-\nformationProcessingSystems,36,2024. 2 cinationsarisingfromthevisionencoder.\n10\nHallusionBench. HallusionBench is a diagnostic suite benchmark evaluations, as models like GPT-3.5 and GPT-\ndesigned to analyze the dual issues of language hallucina- 4ominicanachievenearly100%accuracyinsuchanswer-\ntionandvisualillusioninMLLMs. Itincludes1129hand- parsing tasks, as demonstrated in several previous works.\ncrafted VQA pairs featuring 165 original images and 181 Therefore, we employ this evaluation framework for the\nimages expertly modified by human professionals. Since VICframework,andthecodeforourevaluationisavailable\nour framework is designed for single image and question inourrepository.\npairs, we selected a subset from HallucinationBench that\ncontains only single-image questions, excluding text-only B.DetailedResultsandAnalysis\nquestions and two-image questions. There are 674 single\nB.1.Hallusionbench\nimagequestionsinourselectedsubsetintotal.\nTable 4 presents detailed experimental results of vari-\nMME. MME is a comprehensive benchmark that mea- ousmodelsonHallucinationBench,evaluatingperformance\nsuresbothperceptionandcognitionacross14subtasks. All across subtasks such as chart, figure, illusion, map, and\ntasksinvolve”Yes-or-No”questions, andthescoringcom- others. Overall, the VIC method consistently outperforms\nbines accuracy and a refined accuracy metric (accuracy+). both the original and zero-shot CoT methods in terms of\nThetotalscoresforperceptionandcognitionare2000and accuracy, highlighting its effectiveness across the models.\n800, respectively, reflecting a broad assessment of MLLM Notably, for the table and figure subtasks, zero-shot CoT\nperformance in both areas. We used all the questions in slightlysurpassesVIC.Thiscanbeattributedtothefactthat\nMMEtotestourframework. these subtasks rely heavily on visual information, which\ndiminishes the advantage of VIC’s Thinking before look-\ning paradigm. However, VIC demonstrates superior per-\nMathvista. Mathvista is introduced to systematically\nformance in most other tasks, with its average results sur-\nevaluate mathematical reasoning ability in visual contexts.\npassing zero-shot CoT. Particularly in the illusion subtask,\nItisderivedfrom31differentdatasetsandcompletingthese\nVIC’sdominanceunderscoresitsstrengthinmitigatinghal-\ntasks requires fine-grained, deep visual understanding and\nlucinations, making it highly effective in handling anti-\ncompositional reasoning. We used the testMini set from\nhallucinationtasks.\nMathvista,whichcontains1,000visualmathquestions.\nB.2.POPEadversarial\nPOPE. POPE focuses on assessing object hallucinations Table 5 shows that VIC demonstrates its robustness in\ninMLLMs. Thedatasetincludesthreesamplingstrategies: POPE adversarial samples. While zero-shot CoT exhibits\nrandom, popular, and adversarial. Since the random and higher precision, VIC maintains a better balance between\npopularstrategiesarelesschallenging,ourexperimentcon- precision and recall, leading to more reliable overall per-\ncentrateson1000VQApairsgeneratedfromtheadversarial formance. Forexample,Gemini1.5Proshowsaprecision\nstrategy,whichpresentsmoredifficulttestcases. of0.888withVICcomparedto0.905withzero-shotCoT,\nbut VIC achieves a better recall (0.713 vs. 0.701). The\nSEED-Bench. SEED-Bench serves as a comprehensive Yes rate remains relatively consistent across all methods,\nbenchmark to assess the generative capabilities of multi- but VIC continues to provide more balanced results. This\nmodallargelanguagemodels.Itfocusesonevaluatingmod- pattern of improvement across different models highlights\nelsthroughtasksrequiringdeepcomprehensionacrossboth VIC’s general effectiveness in delivering robust, superior,\nvisualandtextualinputs.SEED-Benchprovidesachalleng- andwell-roundedmodelperformanceonPOPE.\ningenvironmentbyintroducingnovelgenerativetasks,like\nB.3.MME\nfree-formanswergeneration,imageunderstanding,andrea-\nsoning. To evaluate our framework on SEED-Bench, we Table6presentsdetailedexperimentalresultsontheMME\nrandomly sampled 1,000 single-image instances from the benchmark, highlighting that the VIC method consistently\nSEED-Benchdatasetasourtestset. outperformszero-shotCoTacrossarangeofmodelsinboth\nTomaintainconsistencyandaccuracyintheevaluation, cognition and perception tasks. VIC demonstrates signif-\nwe developed a unified evaluation framework by recon- icant advantages in most subtasks, such as existence, po-\nstructing the previous six benchmark evaluation metrics. sition, landmark recognition, artwork analysis, numerical\nThisframeworkisdesignedtohandlemultiple-choiceques- calculation,texttranslation,andcodereasoning. However,\ntions (MCQ), yes/no (YORN) questions, and mixed-form both VIC and zero-shot CoT perform poorly on tasks like\nquestions. Ourevaluationframeworkusesalargelanguage OCR,colorrecognition,andposteridentification,whichdo\nmodeltoextractanswers,parseoptions,yes/nosignals,and notrequireadvancedreasoning. Overall,VICenhancesthe\nmore. This approach has been widely adopted in various model’sreasoningabilities,leadingtosuperiorperformance\n11\nmodel method Acc chart figure illusion map math ocr table video precision Yes rate\nGPT-4omini origin 0.574 0.585 0.585 0.625 0.516 0.426 0.620 0.741 0.406 0.543 0.481\nzero-shotCoT 0.611 0.638 0.610 0.611 0.578 0.519 0.630 0.848 0.366 0.595 0.454\nVIC 0.639 0.723 0.512 0.694 0.547 0.648 0.660 0.777 0.426 0.576 0.438\nGemini1.5Flash origin 0.560 0.623 0.732 0.597 0.516 0.648 0.450 0.661 0.366 0.559 0.350\nzero-shotCoT 0.614 0.715 0.732 0.569 0.578 0.593 0.590 0.759 0.366 0.564 0.429\nVIC 0.625 0.669 0.732 0.625 0.625 0.630 0.570 0.714 0.475 0.612 0.352\nGPT-4o origin 0.626 0.738 0.780 0.403 0.594 0.537 0.610 0.804 0.465 0.579 0.433\nzero-shotCoT 0.673 0.777 0.854 0.542 0.578 0.574 0.670 0.848 0.495 0.634 0.405\nVIC 0.692 0.754 0.756 0.667 0.625 0.519 0.800 0.750 0.455 0.625 0.408\nGemini1.5Pro origin 0.617 0.677 0.683 0.389 0.609 0.704 0.610 0.705 0.545 0.612 0.352\nzero-shotCoT 0.611 0.746 0.659 0.389 0.563 0.519 0.610 0.714 0.545 0.588 0.386\nVIC 0.664 0.785 0.829 0.639 0.578 0.537 0.670 0.714 0.525 0.646 0.340\nTable4.PerformancecomparisonofdifferentmodelsandmethodsevaluatedonHallusionBench.\nwhich undermines VIC’s ability to leverage its strength in\nmodel method Acc F1 precision recall yesrate\nreasoning throughcomplex problems. The lack of contex-\nGPT-4omini origin 0.786 0.797 0.847 0.752 0.445\ntual detail hinders the method’s potential to fully apply its\nzero-shotCoT 0.773 0.755 0.91 0.645 0.355 inference capabilities. We believe that by expanding the\nVIC 0.793 0.757 0.92 0.643 0.35 question text and providing more detailed background de-\nGemini1.5Flash origin 0.769 0.724 0.92 0.597 0.325 scriptions, VIC could regain its superiority given by the\nThinkingbeforelookingparadigminsolvingcomplextasks\nzero-shotCoT 0.741 0.685 0.978 0.527 0.27\nwithinthisbenchmark.\nVIC 0.78 0.751 0.89 0.649 0.365\nGPT-4o origin 0.811 0.824 0.869 0.782 0.451\nzero-shotCoT 0.793 0.804 0.897 0.729 0.407\nVIC 0.827 0.822 0.869 0.78 0.45\nB.5.SEED-Bench\nGemni1.5Pro origin 0.779 0.77 0.849 0.705 0.416\nzero-shotCoT 0.792 0.79 0.905 0.701 0.388\nVIC 0.803 0.79 0.888 0.713 0.402 Table 8 illustrates that our method, VIC, consistently out-\nperforms both the original and zero-shot CoT methods\nTable5. Performancecomparisonofdifferentmodelsandmeth- across all models (Gemini 1.5 Flash, GPT-4o mini, GPT-\nods. 4o, and Gemini 1.5 Pro) in terms of overall accuracy and\nmostsubtasksinSEED-Bench. VICdemonstratesparticu-\nlarlystrongperformanceintasksrequiringcomplexreason-\nonmorecomplexandsophisticatedproblems.\ning, suchasinstancelocationandspatialrelations, achiev-\ningnotablyhigherscoresthantheothermethods. Although\nB.4.Mathvista\nzero-shotCoTshowsslightlybetterperformanceininstance\nTable7presentsthedetailedevaluationresultsfortheMath- identity for certain models, VIC generally leads in most\nvistabenchmark, wheretheVICmethoddoesnotperform othertasks.Furthermore,allthreemethodsperformequally\nas well compared to other benchmarks. The primary rea- well on simpler tasks that do not rely on reasoning, such\nson for this underperformance is attributed to the lack of as text understanding. Overall, VIC proves to be more ef-\ndetailed textual information in the benchmark’s questions. fective across a wider range of reasoning-intensive tasks,\nManyofthequestions,suchasIsR0greaterthanRorFind demonstratingitsrobustnessandabilitytohandlecomplex\nx,lacksufficientexplanationsandbackgroundinformation, multimodalchallenges.\n12\nModel Method Per Cog Ex Cnt Pos Col Post Cel Scn Lmk Art OCR CSR NumC TxtT CodR\nGPT-4omini Origin 1097.23 407.14 136.67 115 78.33 135 134.35 45.88 122.5 91.5 88 150 117.14 87.5 110 92.5\nzero-shotCoT 1069.82 417.14 126.67 111.67 73.33 135 135.03 49.12 119.5 77.75 91.75 150 112.14 105 100 100\nVIC 1105.27 505.00 140 120 95 131.67 132.65 49.71 111 116.5 101.25 107.5 125 140 100 140\nGemini1.5Flash Origin 1077.37 358.93 130 93.33 58.33 123.33 118.71 124.41 117.75 95.5 101 115 101.43 90 90 77.5\nzero-shotCoT 1105.9 500.71 123.33 128.33 68.33 126.67 129.93 97.06 111.5 87.5 98.25 135 125.71 150 85 140\nVIC 1118.54 508.21 136.67 130 90 133.33 126.53 111.76 99 96.25 102.5 92.5 115.71 150 102.5 140\nGPT-4o Origin 1174.39 522.86 143.33 128.33 101.67 143.33 140.14 10.59 115.75 136 105.25 150 132.86 120 140 130\nzero-shotCoT 1166.11 537.14 135 130 106.67 140 140.82 15.88 112.5 129 111.25 145 127.14 130 135 145\nVIC 1238.69 557.85 146.67 121.67 118.33 141.67 135.03 73.82 112.75 137.5 116.25 135 127.86 145 140 145\nGemini1.5Pro Origin 1166.82 462.14 133.33 120 76.67 135 131.63 127.94 113 103.5 100.75 125 117.14 115 100 130\nzero-shotCoT 1099.33 521.42 133.33 123.33 86.67 126.67 115.99 97.35 113.5 80.5 102 120 121.43 145 120 135\nVIC 1147.23 561.42 140 118.33 91.67 135 133.67 122.06 104.75 93 98.75 110 126.43 150 140 145\nTable6. Performancecomparisonofdifferentmodelsandmethods. Per: Perception,Cog: Cognition,Ex: Existence,Cnt: Count,Pos:\nPosition, Col: Color, Post: Posters, Cel: Celebrity, Scn: Scene, Lmk: Landmark, Art: Artwork, OCR:OpticalCharacterRecognition,\nCSR:CommonsenseReasoning,NumC:NumericalCalculation,TxtT:TextTranslation,CodR:CodeReasoning.\nmodel method Acc LR AR GR SR ALR SCR NC\nGPT-4omini origin 0.526 0.236 0.581 0.611 0.637 0.189 0.467 0.582\nzero-shotCoT 0.520 0.243 0.467 0.586 0.588 0.601 0.574 0.271\nVIC 0.536 0.582 0.405 0.544 0.515 0.499 0.571 0.292\nGemini1.5Flash origin 0.479 0.523 0.519 0.324 0.566 0.419 0.518 0.340\nzero-shotCoT 0.520 0.537 0.615 0.641 0.324 0.445 0.519 0.340\nVIC 0.516 0.639 0.292 0.635 0.456 0.378 0.427 0.456\nGPT-4o origin 0.597 0.216 0.535 0.389 0.704 0.664 0.630 0.607\nzero-shotCoT 0.622 0.297 0.661 0.721 0.569 0.389 0.676 0.689\nVIC 0.620 0.734 0.351 0.656 0.633 0.598 0.623 0.361\nGemini1.5Pro origin 0.568 0.687 0.615 0.665 0.456 0.299 0.648 0.270\nzero-shotCoT 0.563 0.619 0.577 0.664 0.162 0.711 0.250 0.470\nVIC 0.558 0.326 0.691 0.507 0.541 0.502 0.297 0.639\nTable 7. Performance comparison of different models and methods across various reasoning categories. Acc: Accuracy, LR: Logical\nReasoning,AR:ArithmeticReasoning,GR:GeometryReasoning,SR:StatisticalReasoning,ALR:AlgebraicReasoning,SCR:Scientific\nReasoning,NC:NumericCommonsense.\nC.AblationExperiments acrossbothbenchmarksandmodels,achievinghighpreci-\nsion(0.645)andF1-score(0.596)withGemini1.5Flashon\nC.1.DifferentVICgenerator\nHallusionBench. It also excels in tasks such as Instance\nThe detailed analysis of the two benchmarks, Hallusion- Interaction (0.8) and Visual Reasoning (0.75) on SEED-\nBench and SEED-Bench, in table 9 and table 10, offers Bench.\nvaluable insights into the performance differences across However, the data further illustrates that certain VIC\nmodelsandVICgenerators. Akeyfindingisthatnosingle generators are more suited to particular models and tasks.\nVIC generator demonstrates consistent superiority across Forexample,Gemini1.5Flashdemonstratesstrongperfor-\nallmetrics,reinforcingtheoriginalablationresults. Gener- mancewhenpairedwithitsnativegeneratoronHallusion-\natorQwenMaxexhibitsnotableversatility,performingwell Bench, achieving top scores in chart-related tasks (0.746)\n13\nmodel method Acc IA II IntI ILoc ICount SU SR TU VR\nGemini1.5Flash origin 0.658 0.677 0.763 0.9 0.5 0.542 0.724 0.419 0.5 0.821\nzero-shotCoT 0.672 0.692 0.734 0.7 0.484 0.603 0.724 0.558 0.5 0.786\nVIC 0.713 0.726 0.820 0.7 0.531 0.665 0.719 0.698 0.5 0.75\nGPT-4omini origin 0.636 0.649 0.763 0.8 0.391 0.497 0.714 0.512 0.5 0.857\nzero-shotCoT 0.660 0.665 0.770 0.8 0.422 0.553 0.729 0.581 0.5 0.857\nVIC 0.696 0.714 0.755 0.6 0.531 0.603 0.767 0.581 0.5 0.857\nGPT-4o origin 0.657 0.720 0.691 0.8 0.484 0.486 0.733 0.558 1 0.75\nzero-shotCoT 0.739 0.800 0.791 0.6 0.578 0.637 0.767 0.605 1 0.821\nVIC 0.751 0.791 0.784 0.6 0.609 0.676 0.757 0.814 1 0.821\nGemini1.5Pro origin 0.678 0.680 0.784 0.8 0.5 0.598 0.733 0.558 0.5 0.786\nzero-shotCoT 0.691 0.692 0.712 0.6 0.5 0.665 0.776 0.535 1 0.786\nVIC 0.713 0.726 0.777 0.8 0.516 0.687 0.733 0.698 0.5 0.714\nTable 8. Performance comparison of different models and methods. Columns: Acc - Accuracy, IA - Instance Attributes, II - Instance\nIdentity, IntI - Instance Interaction, ILoc - Instance Location, ICount - Instances Counting, SU - Scene Understanding, SR - Spatial\nRelation,TU-TextUnderstanding,VR-VisualReasoning.\nand illusion tasks (0.708). Yet, this advantage is not uni-\nformly observed across all tasks, indicating that the syn-\nergy between the model and generator plays a crucial role\nin task-specific performance. Similarly, GPT4-turbo dis-\nplays notable strengths in text-heavy and reasoning tasks,\nachieving a perfect score in Text Understanding (1.0) and\nahighscoreinVisualReasoning(0.857)onSEED-Bench.\nNonetheless, it exhibits weaknesses in Scene Understand-\ning (0.592), highlighting that no single model-generator\npairingachievesoptimalperformanceacrossallevaluation\ncategories.\nIn summary, this fine-grained analysis underscores that\nFigure6.ReflectionpromptusedinVICframework\nbothpurelanguagemodelsandmultimodalmodelsperform\neffectively across tasks, with no clear evidence of consis-\ntent superiority between the two. Performance outcomes\nare highly contingent upon task complexity and the com-\npatibility between the model and the VIC generator. This\nemphasizestheneedforanadaptiveapproachtoVICgen-\neration,wheredifferentgeneratorsareselectedbasedonthe\nspecific requirements of the task. For instance, QwenMax\nmay be particularly effective for spatial and interaction-\nfocusedtasks,whileGPT4-turbocouldbebettersuitedfor\ntext-intensive tasks. Such a strategy would enable mod-\nFigure7.ExtractionpromptusedinVICframework\nelstoleveragethecomplementarystrengthsofvariousVIC\ngenerators, thereby optimizing performance across diverse\nbenchmarks.\n14\nModel VICgenerator Acc chart figure illusion map math ocr table video precision recall f1 yesrate\nGemini1.5Flash GPT-4omini 0.625 0.669 0.732 0.625 0.625 0.63 0.57 0.714 0.475 0.612 0.531 0.569 0.352\nGemini1.5Flash 0.638 0.746 0.756 0.708 0.656 0.574 0.66 0.661 0.376 0.61 0.568 0.588 0.377\nGPT4-turbo 0.631 0.723 0.732 0.667 0.484 0.593 0.61 0.732 0.465 0.614 0.564 0.588 0.372\nQwen-Max 0.639 0.7 0.707 0.653 0.578 0.611 0.64 0.732 0.475 0.645 0.553 0.596 0.347\nGPT-4omini GPT-4omini 0.639 0.723 0.512 0.694 0.547 0.648 0.66 0.777 0.426 0.576 0.623 0.599 0.438\nGemini1.5Flash 0.648 0.777 0.634 0.736 0.594 0.556 0.67 0.768 0.356 0.588 0.597 0.593 0.411\nGPT4-turbo 0.638 0.646 0.585 0.778 0.469 0.63 0.67 0.795 0.455 0.573 0.634 0.602 0.448\nQwen-Max 0.635 0.708 0.634 0.667 0.594 0.63 0.69 0.741 0.376 0.582 0.612 0.596 0.426\nTable9.ComparisonofdifferentmodelsandVICgeneratorsbasedonvariousmetricsonHallusionbench.\nModel VICGen. Acc IA II Int. IL IC SU SR TU VR\nGemini1.5Flash Qwen-Max 0.701 0.710 0.805 0.8 0.593 0.625 0.719 0.604 1 0.75\nGemini1.5Flash 0.696 0.726 0.805 0.5 0.421 0.636 0.704 0.697 0.5 0.821\nGPT4-turbo 0.687 0.735 0.762 0.7 0.484 0.597 0.738 0.534 0.5 0.642\nGPT-4omini 0.713 0.726 0.820 0.7 0.531 0.664 0.719 0.697 0.5 0.75\nGPT-4omini Qwen-Max 0.676 0.710 0.784 0.8 0.5 0.569 0.709 0.534 0.5 0.75\nGemini1.5Flash 0.668 0.661 0.712 0.5 0.421 0.536 0.676 0.558 0.5 0.714\nGPT4-turbo 0.706 0.726 0.805 0.7 0.5 0.592 0.766 0.627 0.5 0.857\nGPT-4omini 0.696 0.713 0.755 0.6 0.531 0.603 0.766 0.581 0.5 0.857\nTable 10. Performance comparison of different models and VIC generators on various evaluation metrics on SEED-Bench. Column\nabbreviations: IA(InstanceAttributes),II(InstanceIdentity),Int. (InstanceInteraction),IL(InstanceLocation),IC(InstancesCounting),\nSU(SceneUnderstanding),SR(SpatialRelation),TU(TextUnderstanding),VR(VisualReasoning).\nwork leverages zero-shot capabilities, meaning it expects\nMethod Qwen-VLPlus\nthemodeltogeneralizeeffectivelyacrosstaskswithoutspe-\nBenchmark\nOriginal VIC PercentageChange(%) cific fine-tuning. However, the framework’s full potential\ncan only be realized if the underlying model can manage\nHallucinationBench 0.31 0.40 29.03%\ncomplexinteractionsbetweenmultipleinputs,maintainco-\nMMVP 0.36 0.21 -40.75%\nherence over extended contexts, and switch efficiently be-\nMME 1479.69 1375.89 -7.02%\ntweendiversetasks.\nmathvista 0.254 0.317 24.80%\nAkeychallengeliesinmultimodallong-contextcompre-\nPOPEadversal 0.827 0.787 -4.84%\nhension. TheVICframeworkrequiresthemodeltoextract\nSEED-Bench 0.678 0.536 -20.94% and retain information across multiple modalities over an\nextended sequence of inputs. This means that the model\nTable11.PerformanceComparisonofQwen-VLPlusinDifferent mustnotonlyprocesseachmodalityindependentlybutalso\nBenchmarks\nintegratethemseamlesslyacrossalongtemporalorcontex-\ntual span. Open-source models often struggle to maintain\nsuch cross-modal coherence, especially when the context\nC.2.Opensourcemodels\ninvolves several dependent interactions across modalities.\nIncompleteorinconsistentcontextualunderstandingcanre-\nOpen-source models, such as Qwen-VL Plus, may strug-\nducetheeffectivenessofVICinthesescenarios.\ngle with the VIC framework primarily due to their limita-\ntionsinhandlingmulti-modallong-contextprocessingand Anothercrucialfactorismulti-taskexecution. TheVIC\nmulti-task execution (shown in table 11). The VIC frame- framework expects the model to perform a wide range of\n15\ntasks simultaneously or switch seamlessly between them.\nThisinvolvesahighdegreeofflexibilityandtaskcoordina-\ntion,whichrequiresrobustunderlyingmechanismsfortask\nmanagement and cross-task transfer. Open-source models\nmaylackthecapacityneededtoefficientlyhandlemultiple\ntasks together, leading to degraded performance when the\nframeworkisdeployedinmulti-taskscenarios.\nIn summary, the VIC framework demands strong ca-\npabilitiesinmanagingmulti-modallong-contextreasoning\nand multi-task execution. As a result, while these models\nmay exhibit promising zero-shot abilities in simpler tasks,\ntheystruggletounlockthefullpotentialoftheVICframe-\nwork.\n16\nD.VisualInferenceChainPrompt\n17\nE.ExperimentExamples\n18\n19\n20",
    "pdf_filename": "Thinking_Before_Looking_Improving_Multimodal_LLM_Reasoning_via_Mitigating_Visual_Hallucination.pdf"
}