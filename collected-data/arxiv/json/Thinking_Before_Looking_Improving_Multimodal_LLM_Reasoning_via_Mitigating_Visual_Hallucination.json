{
    "title": "Thinking Before Looking Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination",
    "context": "Multimodal large language models (MLLMs) have ad- vanced the integration of visual and linguistic modali- ties, establishing themselves as the dominant paradigm for visual-language tasks. Current approaches like chain of thought (CoT) reasoning have augmented the cognitive ca- pabilities of large language models (LLMs), yet their adap- tation to MLLMs is hindered by heightened risks of hallu- cination in cross-modality comprehension. In this paper, we find that the thinking while looking paradigm in current multimodal CoT approaches—where reasoning chains are generated alongside visual input—fails to mitigate hallu- cinations caused by misleading images. To address these limitations, we propose the Visual Inference Chain (VIC) framework, a novel approach that constructs reasoning chains using textual context alone before introducing visual input, effectively reducing cross-modal biases and enhanc- ing multimodal reasoning accuracy. Comprehensive eval- uations demonstrate that VIC significantly improves zero- shot performance across various vision-related tasks, miti- gating hallucinations while refining the reasoning capabil- ities of MLLMs. Our anonymized code repository can be found at https://github.com/Terry-Xu-666/ visual_inference_chain. Large Language Models (LLMs), such as GPT-4 [1] and Llama [34], have driven remarkable advancements in world comprehension [11] and reasoning capability [12]. Prompt- ing techniques like CoT [9, 28, 38] have been developed to enhance LLMs’ ability to handle complex tasks through *These authors contributed equally to this work. †Visiting student at Lehigh University. human-like step-by-step reasoning. Meanwhile, MLLMs have rapidly advanced in recent years [1, 2, 20, 21], ex- tending LLMs’ capabilities into the multimodal realm by integrating visual backbones to align visual and language representations. MLLMs have demonstrated exceptional performance in a range of vision-related tasks, including visual question answering [4], object recognition [7, 42], and video comprehension [22], highlighting the impressive evolution of AI-driven visual-language understanding. The success of CoT prompting in unimodal contexts suggests a promising extension to multimodal scenarios. Due to the integration of pretrained vision models [27, 32] and lan- guage models [44] in MLLMs, various types of hallucina- tions have been observed [5, 16, 18], including nonexistent object generation [33], visual misinterpretations [17], and cross-modality biases [35]. Offering advantages in tackling complex multimodal tasks, current prompting approaches [46, 47] predominantly adhere to the thinking while looking paradigm, where rea- soning occurs simultaneously with the integration of visual elements. However, this paradigm encounters substantial challenges due to the prevalence of hallucinations, which undermine both the reliability of the reasoning process and the accuracy of the responses. As illustrated in Figure 1, the MLLM often falls into stereotypes when processing a question-image pair, where it recalls similar prior contexts and overlooks subtle variations, leading to erroneous re- sponses. Even with CoT prompting, the model tends to rely on memory-based stereotypes rather than engaging in accu- rate reasoning, which leads to incorrect responses. To overcome hallucinations from visual inputs and har- ness the reasoning capabilities of LLMs, we propose the Visual Inference Chain (VIC), which introduces a reasoning process that occurs prior to engaging with visual elements, following the thinking before looking paradigm. This ap- 1 arXiv:2411.12591v1  [cs.CV]  15 Nov 2024",
    "body": "Thinking Before Looking:\nImproving Multimodal LLM Reasoning via Mitigating Visual Hallucination\nHaojie Zheng1*† Tianyang Xu2 *†\nHanchi Sun3\nShu Pu4\nRuoxi Chen3† Lichao Sun3\n1University of Pennsylvania\n2Columbia University\n3Lehigh University\n4Independent Researcher\nhaojiez@seas.upenn.edu, tx2240@columbia.edu, pushuabc@gmail.com\nhas423,lis221@lehigh.edu,chenrx0830@gmail.com\nAbstract\nMultimodal large language models (MLLMs) have ad-\nvanced the integration of visual and linguistic modali-\nties, establishing themselves as the dominant paradigm for\nvisual-language tasks.\nCurrent approaches like chain of\nthought (CoT) reasoning have augmented the cognitive ca-\npabilities of large language models (LLMs), yet their adap-\ntation to MLLMs is hindered by heightened risks of hallu-\ncination in cross-modality comprehension. In this paper,\nwe find that the thinking while looking paradigm in current\nmultimodal CoT approaches—where reasoning chains are\ngenerated alongside visual input—fails to mitigate hallu-\ncinations caused by misleading images. To address these\nlimitations, we propose the Visual Inference Chain (VIC)\nframework, a novel approach that constructs reasoning\nchains using textual context alone before introducing visual\ninput, effectively reducing cross-modal biases and enhanc-\ning multimodal reasoning accuracy. Comprehensive eval-\nuations demonstrate that VIC significantly improves zero-\nshot performance across various vision-related tasks, miti-\ngating hallucinations while refining the reasoning capabil-\nities of MLLMs. Our anonymized code repository can be\nfound at https://github.com/Terry-Xu-666/\nvisual_inference_chain.\n1. Introduction\nLarge Language Models (LLMs), such as GPT-4 [1] and\nLlama [34], have driven remarkable advancements in world\ncomprehension [11] and reasoning capability [12]. Prompt-\ning techniques like CoT [9, 28, 38] have been developed\nto enhance LLMs’ ability to handle complex tasks through\n*These authors contributed equally to this work.\n†Visiting student at Lehigh University.\nhuman-like step-by-step reasoning. Meanwhile, MLLMs\nhave rapidly advanced in recent years [1, 2, 20, 21], ex-\ntending LLMs’ capabilities into the multimodal realm by\nintegrating visual backbones to align visual and language\nrepresentations.\nMLLMs have demonstrated exceptional\nperformance in a range of vision-related tasks, including\nvisual question answering [4], object recognition [7, 42],\nand video comprehension [22], highlighting the impressive\nevolution of AI-driven visual-language understanding. The\nsuccess of CoT prompting in unimodal contexts suggests a\npromising extension to multimodal scenarios. Due to the\nintegration of pretrained vision models [27, 32] and lan-\nguage models [44] in MLLMs, various types of hallucina-\ntions have been observed [5, 16, 18], including nonexistent\nobject generation [33], visual misinterpretations [17], and\ncross-modality biases [35].\nOffering advantages in tackling complex multimodal\ntasks, current prompting approaches [46, 47] predominantly\nadhere to the thinking while looking paradigm, where rea-\nsoning occurs simultaneously with the integration of visual\nelements. However, this paradigm encounters substantial\nchallenges due to the prevalence of hallucinations, which\nundermine both the reliability of the reasoning process and\nthe accuracy of the responses. As illustrated in Figure 1,\nthe MLLM often falls into stereotypes when processing a\nquestion-image pair, where it recalls similar prior contexts\nand overlooks subtle variations, leading to erroneous re-\nsponses. Even with CoT prompting, the model tends to rely\non memory-based stereotypes rather than engaging in accu-\nrate reasoning, which leads to incorrect responses.\nTo overcome hallucinations from visual inputs and har-\nness the reasoning capabilities of LLMs, we propose the\nVisual Inference Chain (VIC), which introduces a reasoning\nprocess that occurs prior to engaging with visual elements,\nfollowing the thinking before looking paradigm. This ap-\n1\narXiv:2411.12591v1  [cs.CV]  15 Nov 2024\n\nFigure 1. This example from HallusionBench demonstrates the differences between zero-shot, zero-shot CoT, and VIC. The zero-shot CoT\nrepresents the thinking while looking approach, which tends to exhibit stereotypical reasoning patterns when processing both visual and\ntextual inputs simultaneously. In contrast, our thinking before looking paradigm, VIC, enhances reasoning quality by decoupling the visual\nand textual inputs. More examples can be found in Appendix E.\nproach mirrors human cognition [30, 31], where reasoning\noften precedes perception. For example, when adults hear\na question before seeing the image, they generate a pre-\nliminary plan based on their accumulated experience. The\nquestion activates relevant memories and contextual knowl-\nedge, allowing them to deduce a forward-looking reason-\ning strategy ahead of engaging in visual stereotypes. This\nunderscores that the direct impact of visual elements on\nthe reasoning process is relatively limited. Analogous to\nhuman cognition, MLLMs can adopt the thinking before\nlooking paradigm. By leveraging this paradigm, VIC taps\ninto the accumulated forward-looking reasoning capabili-\nties of powerful LLMs, enabling MLLMs to anticipate and\nrecognize patterns more efficiently by dynamically adjust-\ning reasoning steps. Additionally, the VIC framework em-\nploys a systematic multi-step detachment strategy to min-\nimize compounding hallucinations from both the question\nand the image, further enhancing the MLLMs’ reasoning\ncapabilities for complex tasks.\nIn this work, we demonstrate that our thinking be-\nfore looking strategy outperforms the conventional think-\ning while looking approach in multimodal reasoning tasks.\nOur method demonstrates improved performance in em-\npirical studies on GPT-series [26] and Gemini-series [6]\nmodels across various visual question benchmarks. VIC\nachieves notable improvements on hallucination-specific\nbenchmarks such as MMVP [33], HallusionBench [10],\nand POPE [16], as well as on general multimodal bench-\nmarks including MME [8], MathVista [23], and SEED-\nBench [14]. For instance, the Gemini 1.5 Pro sees a substan-\ntial improvement of 31.74% on the MMVP benchmark, in\na meanwhile the GPT-4o mini model shows an increase of\n16.59%. Across all benchmarks, GPT-series models av-\nerage a 8.02% refinement and Gemini-series models show\nan average improvement of 7.19%, underscoring both the\neffectiveness and robustness of our method.\n2. Related Works\nCoT reasoning with LLMs.\nChain-of-thought (CoT)\nreasoning [38] has significantly enhanced large language\nmodels (LLMs) performance by guiding them to break\ndown complex tasks into intermediate reasoning steps. Sub-\nsequent work introduced self-consistency [37] where mul-\ntiple reasoning paths are generated, and the most consis-\ntent answer is chosen. Recent advances focus on optimizing\nCoT with example selection and enhanced reasoning frame-\nworks [45], exploring selecting diverse examples to guide\nCoT reasoning. Step-Aware Verifier framework enhances\nCoT by incorporating a verification step at each interme-\ndiate reasoning stage, improving reasoning reliability [15].\nMeanwhile, Tree of Thought (ToT) [40] extends CoT by\nexploring multiple reasoning paths in a tree structure, en-\nsuring thorough consideration of potential solutions. Other\nrefinements allow models to self-reflect on and adjust their\nreasoning steps [39].\nMultimodal CoT reasoning.\nConsidering the natural gap\nbetween text and vision data, Multimodal Large Language\nModels (MLLMs) are less capable of utilizing the reason-\ning ability of CoT. MMCoT [46] adds a rationale genera-\n2\n\ntion block before answer inference. DDCoT [47] is pro-\nposed to enhance multimodal reasoning by assigning spe-\ncific tasks to language and visual models. Meanwhile, Vi-\nsual CoT [29] incorporates the notion of segmentation, and\nperforms well for object-detection tasks but lacks versatil-\nity. Image-of-Thought [48] further explores the MLLMs\nCoT prompting by introducing specific actions like segmen-\ntation, zoom-in, and color-space conversions in the CoT\nchain, extending the applicable scenarios with the sacrifice\nof flexibility. Compositional CoT [24] extracts the compo-\nsitional information of images as a scene graph (SG) and\nthen utilize it during the CoT process. Other works like\nCoCoT [43] and KAM-CoT [25], consider multiple input\nimage scenarios and incorporate knowledge graphs (KG) of\nmultimodal information. However, many such techniques\nrely on fixed templates for extracting predetermined infor-\nmation, resulting in a lack of flexibility and the utilization of\nreasoning capabilities. The straightforward thinking while\nlooking approach struggles to resolve problematic cross-\nmodal interactions, often resulting in hallucinations.\nHallucination in MLLMs.\nHallucination remains a sig-\nnificant challenge in MLLMs, where models generate in-\nformation that is factually incorrect or irrelevant to the given\ninputs [3]. In multimodal reasoning, hallucinations can oc-\ncur when models produce textual outputs not grounded in\nthe visual data, leading to fabricated details in tasks such\nas image captioning or visual question answering (VQA).\nFor hallucination evaluation, FaithScore [13] extracts fine-\ngrained atomic facts from the generated answer and then\nconducts consistency verifications. POPE, a polling-based\nquery method [16] is proposed for a better evaluation of ob-\nject hallucination. Recently, HallusionBench [10] further\nexplored the hallucination evaluation, by editing the origi-\nnal input image and forming different text-image pairs to di-\nagnose failure types from language hallucination and visual\nillusion. Several hallucination-mitigating methods are also\nproposed. To conduct instruction tuning that mitigate hal-\nlucination, LRV [19] serves as the first large and diverse vi-\nsual instruction tuning dataset and VIGC [36] aims for data\ngeneration. As for training-free method, Woodpecker [41]\nuses fixed steps to correct hallucination in MLLMs. Our\nmethod, however, generates applicable reasoning trajecto-\nries without visual elements for a given question, which is\nmore flexible and accurate than fixed analyzing steps, and\ncould be further utilized across different VQA tasks.\n3. Method\nIn this section, we begin by introducing the preliminary\narchitecture of MLLMs and the process of thinking while\nlooking paradigm in Section 3.1 and Section 3.2 respec-\ntively. In Section 3.3, we present the foundation of think-\ning before looking paradigm along with a detailed explana-\ntion of how forward-looking reasoning can enhance the rea-\nsoning process more effectively than thinking while looking\nparadigm. Additionally, we explore two key components\nin the framework in Section 3.4 and 3.5, respectively. The\ndetailed implementation of the prompting mechanism dis-\ncussed in this section is provided in Appendix D, and the\noverview of our method is illustrated in Figure 2.\n3.1. Multimodal LLM\nMLLMs are designed to handle data from various modali-\nties, such as text, images, and audio, enabling the integra-\ntion and generation of multimodal information. In the task\nof Visual Question Answering (VQA), the MLLM is pro-\nvided with an image input I and a textual input Q. These\ninputs are then encoded into a shared representation space\nusing a pre-trained visual encoding model, and a fixed text\ntokenization process for the textual data. The encoded rep-\nresentations from both visual and textual data are subse-\nquently processed by large language model. We charac-\nterize the whole MLLM as f ′(∗), which has been trained\non unified representation data from both modalities. During\ninference, the model generates a response A given I and Q,\nwhich can be formalized as:\nA = f ′(I, Q).\n(1)\nFor the sake of simplicity, we ignore the tokenizer and\npre-trained visual encoding model since they primarily\nserve as preprocessing components. This formulation al-\nlows MLLMs to focus on leveraging the combined informa-\ntion from multiple data types, enabling them to effectively\nprocess and respond to complex, multimodal queries with\ngreater efficiency.\n3.2. Thinking while looking\nA simple VQA inference process is illustrated in Equa-\ntion 1. Moreover, the thinking while looking paradigm in-\nvolves step-by-step reasoning trajectory while simultane-\nously processing the visual input.\nThis approach gener-\nates a reasoning strategy chain {sn}k\nn=1, accompanied by\ncorresponding rationales, denoted as {rn}k\nn=1, where the\nvalue of k varies dynamically depending on both the spe-\ncific model and the given question-image pair. In parallel\nwith generating these reasoning steps and rationales, the\nmodel also produces the final answer A.\n({sn, rn}k\nn=1, A) = f ′(I, Q, Pcot).\n(2)\nIn this formula, {sn, rn}k\nn=1 represents the sequence of\nreasoning steps and corresponding rationales. Pcot denotes\nthe prompt used for CoT prompting, which can be either\na zero-shot prompt such as “Let’s think step by step” or a\nfew-shot prompt designed in an in-context learning manner.\n3\n\nFigure 2. The overall framework of VIC (Visual Inference Chain). VIC decouples visual and textual inputs to improve reasoning. It\nfirst generates intermediate reasoning steps from the question Q and prompt Pvic. The image I is processed through an MLLM to extract\nrationales R, which, combined with the visual inference steps {sn}k\n1, lead to the final answer A with enhanced accuracy.\nAlthough Equation 2 demonstrates how CoT elucidates\nthe MLLM’s thinking process and enhances its capacity to\nmanage complex tasks, the tight interweaving of textual and\nvisual inputs introduces hallucination issues, as the reason-\ning sequence {sn, rn}k\nn=1 can become biased. As shown in\nFigure 1, the model generates a hallucination by identify-\ning a non-existent Tichener circle illusion due to the com-\nbined influence of visual cues and the question, which acti-\nvates memories of the most similar context from its training\nphase. This example highlights the biased reasoning steps\nproduced by the vanilla CoT method, leading to improper\nreasoning in the model’s responses.\n3.3. VIC generation\nThe limitations of the thinking while looking paradigm\ninspire the development of the thinking before looking\nparadigm. This new approach promotes forward-looking\nreasoning, enhancing the reasoning process and improving\nthe quality of rationality. Separating visual and textual in-\nputs allows for more structured thinking and clearer cogni-\ntive steps. This approach decouples the question from the\nimage, reducing bias in the reasoning steps sequence and\nenhancing overall reasoning performance.\nThe language models have internalized extensive reason-\ning knowledge from large-scale pre-trained data, allowing\nthem to gain beneficial insights into image reasoning analy-\nsis. Due to their highly developed pattern recognition abil-\nities, these models can accumulate sufficient background\nknowledge and generate reasonable reasoning steps for vi-\nsual elements. Thus, LLMs f(∗) can automatically gener-\nate forward-looking reasoning steps as an average over a\nbroader context of similar situations, triggered by the in-\nput (Q, Pvic), rather than focusing solely on a specific in-\nput pair (Q, I). This process discretizes the reasoning steps\nfrom the specific input pair, reducing hallucinations while\nmaintaining the validity of instructions for image analysis,\nas these instructions align with the aggregate of the most\nrelevant contextual knowledge. Consequently, the visual in-\nference process can be expressed as follows.\n{sn}k\nn=1 = f(Q, Pvic)\n(3)\nBy employing this specific prompt, we generate a visual\ninference chain, denoted as {sn}k\nn=1, which serves as the\nreasoning process derived exclusively from the given ques-\ntion. This process can be divided into two main compo-\nnents. The initial k −1 steps, {sn}k−1\nn=1, correspond to the\nsequence of instructions related to both recognition and rea-\nsoning. And the final step, sk, of the visual inference chain\nintroduces a format instruction based on the specific ques-\ntion. This step is critical in further enhancing the frame-\nwork’s ability to follow complex instructions effectively.\nThe advantage of the thinking before looking phase is\nthat it eliminates the immediate need for visual information.\nAs a result, the model f(∗) can function as either a large\nlanguage model or a multimodal language model operating\nin a blind mode. This concept of thinking before looking\nallows us to leverage the superior reasoning capabilities of\n4\n\nadvanced large language models, offering significant flexi-\nbility and generalizability.\n3.4. VIC rationale extraction\nHallucinations commonly arise from deep entanglement be-\ntween image and textual inputs. In this step, we decouple\nthe original question and requiring the MLLM to recognize\nand follow the visual inference chain step by step, miti-\ngating the effects of textual bias. Moreover, the visual in-\nference chain provides a more precise trajectory compared\nto the original question, necessitating detailed extraction of\nrelevant information and thus reducing the risk of halluci-\nnations by visual inference chain.\nUnlike the previous step, which can use any modal-\nity model f, this step specifically employs the MLLM f ′.\nLeveraging the multitask-following capability of closed-\nform models, we generate the VIC rationales in a single step\nto produce the entire set of rationales {rn}k−1\nn=1, denoted col-\nlectively as R. We further explore the differences between\nsingle-step rationale extraction and multi-step rationale ex-\ntraction in our ablation experiments.\nR = f ′(I, Pextract, {sn}k−1\nn=1)\n(4)\nIn this formula, I represents the image corresponding to\nthe specific question Q. The sequence {sn}k−1\nn=1 refers to\nthe first k −1 steps of the process, excluding the format\ninstructions. Pextract is a prompt designed to integrate the\nimage with the visual inference chain, facilitating effective\ninformation extraction.\n3.5. Answer Inference\nIn the final step, we provide the same inputs to the MLLM\nfϕ as used during the VIC rationale generation phase. These\ninputs include the original question-image pair Q, I, the\nVIC rationale results R, the format instruction sk, and the\nreflection prompt Preflect.\nA = f ′(I, Q, R, Preflect, sk)\n(5)\nThis process incorporates the VIC rationale results as ad-\nditional information to support the model’s response. Fur-\nthermore, we introduce a reflection mechanism at this stage,\nencouraging the model to reconsider the question, image,\nand VIC rationale results, rather than treating the rationale\nas the definitive answer. The format instruction sk ensures\nthe response adheres to the desired format by guiding the\nmodel’s analysis of the user’s question or query, thereby im-\nproving instruction-following performance.\n4. Experiment\n4.1. Experiment Setup\nDatasets\nOur framework is evaluated on six benchmark\ndatasets. To evaluate the generality and versatility of the\nVIC framework, we tested it across two key benchmark\ncategories: (1) Hallucination detection benchmark includ-\ning HallusionBench [10], MMVP [33], and POPE [16],\nall of which are designed to analyze hallucinations that are\nprone to occur in different forms. (2) General multimodal\ncapability benchmark, MME [8] and SEED-Bench [14],\nwhich cover general and comprehensive forms of visual\nquestion answering. MathVista [23] that focuses on math\nelement recognition and reasoning problems, evaluates the\ncompound capability to solve visual math challenges. We\ndiscuss the details and implementations for each benchmark\nin Appendix A.\nBaseline\nIn our experiments, we compare the proposed\nVIC methodology with two primary baselines, as outlined\nin Table 1. The first baseline involves applying the model\ndirectly to each benchmark without incorporating any spe-\ncialized techniques, enabling us to evaluate the added ben-\nefit of our method to the pretrained model’s performance.\nThe second baseline is zero-shot CoT prompt engineering.\nIn this approach, we append the reasoning prompt such as\n”Let’s think step by step” to the input, following the ques-\ntion. Since the model’s output in this case includes multiple\nreasoning steps, we then pass it through an answer extractor\nto derive the final answer, in a manner similar to the final\nstage of the VIC method. This comparison highlights the\ndifferences and advantages of our thinking before looking\nstrategy. Additionally, we introduce human performance\nand random choices as further baselines, providing a basic\nperformance reference point for each benchmark.\nImplementation\nWe conducted our experiments primar-\nily on four popular closed-source MLLMs: Gemini 1.5\nFlash, Gemini 1.5 Pro, GPT-4o, and GPT-4o mini. The rea-\nson for choosing these models mainly involved two aspects.\n(i) Our approach is grounded in the belief that the thinking\nbefore looking capability should not be exclusive to large\nmodels, driving us to experiment on both large and small\nmodels. (ii) Our method also leverages the long instructions\nfollowing ability, which is a key strength of closed-source\nmodels. The experiments were conducted in three steps.\nFirstly, we utilized the VIC-prompt, where the input ques-\ntion text was fed into the models to generate Visual Infer-\nence Chain. These generated instructions were then paired\nwith the original image to extract VIC rationals. Finally, we\nderive the answer to the question based on the rationale we\nextracted. Additionally, we also ran several complementary\nexperiments on open-source models, such as Qwen-VL. For\n5\n\nModels\nMethod\nMMVP\nHallusionBench\nPOPEadversarial\nMathvista\nSEED-Benchsingle\nAverage\nMME\nPerception\nCognition\nBaselines\nHuman\n0.957\n0.986\n0.995\n0.603\n0.967\n0.901\n-\n-\nRandom choice\n0.250\n0.500\n0.500\n0.179\n0.250\n0.336\n-\n-\nGPT-4o mini\nOrigin\n0.446\n0.574\n0.786\n0.526\n0.636\n0.590\n1097.23\n407.14\nzero-shot CoT\n0.443 (↓2.85%)\n0.611 (↑6.46%)\n0.773 (↓1.65%)\n0.520 (↓1.14%)\n0.660 (↑3.77%)\n0.601 (↑1.86%)\n1069.81 (↓2.50%)\n417.14 (↑2.46%)\nVIC\n0.520 (↑16.59%)\n0.639 (↑11.37%)\n0.793 (↑0.89%)\n0.536 (↑1.90%)\n0.696 (↑9.43%)\n0.637 (↑7.96%)\n1105.27 (↑0.73%)\n505.00 (↑24.04%)\nGemini 1.5 Flash\nOrigin\n0.527\n0.560\n0.769\n0.479\n0.658\n0.599\n1077.36\n358.92\nzero-shot CoT\n0.513 (↓2.54%)\n0.614 (↑9.69%)\n0.741 (↓3.64%)\n0.520 (↑8.56%)\n0.672 (↑2.13%)\n0.612 (↑2.17%)\n1105.9 (↑2.65%)\n500.71 (↑39.50%)\nVIC\n0.553 (↑5.05%)\n0.638 (↑13.93%)\n0.780 (↑1.43%)\n0.516 (↑7.72%)\n0.713 (↑8.36%)\n0.640 (↑6.84%)\n1118.54 (↑3.82%)\n508.21 (↑41.59%)\nGPT-4o\nOrigin\n0.673\n0.626\n0.811\n0.597\n0.657\n0.673\n1174.39\n522.85\nzero-shot CoT\n0.687 (↑1.99%)\n0.673 (↑7.49%)\n0.793 (↓2.22%)\n0.622 (↑4.19%)\n0.739 (↑12.48%)\n0.701 (↑4.16%)\n1166.12 (↓0.70%)\n537.14 (↑2.73%)\nVIC\n0.747 (↑10.90%)\n0.692 (↑10.52%)\n0.827 (↑1.27%)\n0.620 (↑3.85%)\n0.751 (↑14.31%)\n0.727 (↑8.08%)\n1238.69 (↑5.48%)\n557.85 (↑6.69%)\nGemini 1.5 Pro\nOrigin\n0.420\n0.617\n0.779\n0.568\n0.678\n0.612\n1166.82\n462.14\nzero-shot CoT\n0.500 (↑19.05%)\n0.611 (↓0.97%)\n0.793 (↑1.67%)\n0.563 (↓0.88%)\n0.691 (↑1.92%)\n0.632 (↑3.20%)\n1099.33 (↓5.78%)\n521.42 (↑12.83%)\nVIC\n0.553 (↑31.74%)\n0.664 (↑7.62%)\n0.803 (↑3.08%)\n0.558 (↓1.76%)\n0.713 (↑5.16%)\n0.658 (↑7.55%)\n1147.23 (↓1.68%)\n561.42 (↑21.48%)\nTable 1. Main Results Across Models and Benchmarks. Comparison of model performance on benchmarks such as MMVP, Hallusion-\nBench, and Mathvista, including metrics for Perception and Cognition under MME. Improvements are marked in green and declines in\nred. Bold values denote the best results.\nfurther details and results from these experiments, please re-\nfer to Section 4.3.\n4.2. Results\nThe results of the Visual-Inference-Chain on six different\nbenchmarks using two open-source models are listed in Ta-\nble 1. We primarily use average accuracy as the evalua-\ntion metric to quantify the models’ performance, except for\nthe MME benchmark, where we retain the original compos-\nite evaluation score that combines accuracy and a refined\naccuracy metric (Accuracy+). This approach allows for a\nconsistent comparison with previous records on MME. For\nthe MMVP benchmark, we use pair accuracy to measure\nthe model’s ability to overcome the hallucination caused by\nCLIP.\nAcross the comprehensive benchmarks MME and\nSEED-Bench, the models show an average performance im-\nprovement of 9.13%, demonstrating that our method VIC\nconsistently enhances performance across all tasks. These\nresults highlight the efficacy of VIC in boosting multimodal\nreasoning capabilities. For the Mathvista benchmark, al-\nthough the average gain is slightly reduced to 3.15%, this is\nlargely due to the nature of this benchmark, which lacks de-\ntailed explanations of questions and is heavily reliant on vi-\nsual information. For example, questions like ”Is R0 greater\nthan 0?” require direct visual interpretation, making it dif-\nficult to generate an effective visual inference chain with-\nout explicit vision inputs.\nDespite these challenges, the\nimprovement underscores the versatility of VIC, even in\nFigure 3. Detailed evaluation comparisons with and without VIC\nfor two models on HallusionBench. Features: AA - All Accuracy,\nHA - Hard Accuracy, EA - Easy Accuracy, FA - Figure Accuracy,\nQPA - Question Pair Accuracy.\ntasks that are highly vision-dependent. Regarding the hal-\nlucination benchmarks, we first test on adversarial samples\nin POPE. Our method still effectively handles these chal-\nlenging scenarios, improving the models’ performance by\n1.36%. As for MMVP, the average improvement achieves\n15.62%, which marks that VIC significantly overcomes\nthe hallucination caused by using CLIP as a vision encoder.\nHallusionBench is another critical benchmark we want to\nfocus on, the details of HallusionBench experiments are\nshown in Figure 3.\nwhere we observe notable improve-\nments in both Question Pair Accuracy and Figure Accuracy,\nindicating that VIC helps mitigate both language hallucina-\ntions and visual illusions. Another key observation is the\n6\n\nFigure 4. Performance comparison of Gemini 1.5 Flash and Gem-\nini 1.5 Pro using Zero-shot CoT and VIC methods across various\nevaluation metrics on HallusionBench.\nenhancement in Hard Accuracy, which measures the mod-\nels’ ability to interpret human-edited images from Hallu-\nsionBench.\nThe categorization results of Gemini 1.5 Flash and Gem-\nini 1.5 Pro on HallusionBench are presented in Figure 4.\nCompared to Zero-shot CoT, VIC demonstrates significant\nimprovements in figure understanding, reduction of illusion\nerrors, and enhanced map identification, while maintaining\nstrong performance across other tasks. This result high-\nlights VIC’s ability to significantly improve the robustness\nof the models, positioning it as a vital innovation in anti-\nhallucination methods. More details on the experiment can\nbe found in Appendix B.\nFigure 5. Performance of Different VIC Generators. This chart\ncompares the performance of various VIC generators on Hallu-\nsionBench and SEED-Bench. Grey bars represent the original per-\nformance for reference.\n4.3. Ablation Experiment\nBy adopting a blind input approach, where the model op-\nerates independently of the image, we can leverage various\nmodels for VIC generation. In our experiments, we em-\nployed a range of models, including pure language mod-\nels such as GPT-4 Turbo and Qwen-Max, alongside mul-\ntimodal models like GPT-4o mini and Gemini 1.5 Flash,\nas visual inference chain generators. Additionally, we con-\nducted ablation experiments to investigate the differences\nbetween single-step and multi-step VIC rationale extrac-\ntion, the impact of reflective prompting during the answer\ngeneration phase, and the performance of open-source mod-\nels. For more detailed results, please refer to Appendix C.\nDifferent VIC generator.\nAs shown in Figure 5, different\nvisual inference chain (VIC) generators significantly affect\nVIC performance. Leveraging the ”thinking before look-\ning” paradigm, we can select either a pure language model\nor a multimodal model operating in a blind mode. Thus, we\ntested GPT-4 Turbo, Qwen-Max, GPT-4o mini, and Gem-\nini 1.5 Flash as VIC generators on two benchmarks, Hallu-\nsionBench and SEED-Bench. On SEED-Bench, the maxi-\nmum performance variation is 3.95% for Gemini 1.5 Flash\nand 5.98% for GPT-4o mini. A key finding is the minimal\nperformance difference between chains generated by pure\nlanguage models and those by multimodal models. For in-\nstance, Qwen-Max’s visual inference chain performs best\nfor Gemini 1.5 Flash on HallusionBench, while GPT-4o\nmini’s chain achieves the highest score for Gemini 1.5 Flash\non SEED-Bench. This suggests that both pure language and\nmultimodal models offer comparable reasoning capabilities\nfor image-related tasks, even though multimodal models are\nmore specialized in image comprehension. Overall, there is\nno clear pattern in which chain consistently performs best,\nas effectiveness varies based on factors such as the compat-\nibility of the visual inference chain with the MLLM and the\nVIC generator’s ability to address specific question types.\nFor example, the GPT-4o mini visual chain excels on gen-\neral questions in SEED-Bench but performs the weakest on\nHallusionBench.\nOne step VIC rationale extraction.\nThe VIC generation\nprocess uses two main methods: a single-step approach and\na multi-step approach. In the single-step method, the en-\ntire inference chain is processed as a unified input, while\nthe multi-step approach completes tasks incrementally, inte-\ngrating intermediate results at each step. Evaluations on the\nHallusionBench and SEED-Bench benchmarks show dis-\ntinct advantages for each approach.\nOverall, the single-\nstep method yields more stable improvements, ensuring\ngreater consistency across tasks.\nThis approach also re-\nduces latency and computational cost due to its single-input\nnature.\nIn contrast, the multi-step approach requires re-\ninputting images and prompts at each step, which increases\nresponse times and resource demands. Although the multi-\nstep method has some advantages on benchmarks like Hal-\n7\n\nlusionBench, where detailed, stepwise reasoning can im-\nprove accuracy, the single-step approach remains generally\nsuperior in terms of efficiency, accuracy, and reliability, es-\npecially on models like Gemini 1.5 Flash and GPT-4o mini,\nachieving notable performance gains on SEED-Bench with-\nout the risk of error propagation.\nModel\nMethods\nHallusionbench\nSEED-bench\nGemini 1.5 Flash\nOriginal\n0.560\n0.658\nVic M\n0.66 (↑17.86%)\n0.676 (↑2.74%)\nVic One Step\n0.638 (↑13.93%)\n0.701 (↑6.53%)\nGPT-4o Mini\nOriginal\n0.574\n0.636\nVic M\n0.673 (↑17.21%)\n0.629(↓1.10%)\nVic One Step\n0.648 (↑12.92%)\n0.676 (↑6.29%)\nTable 2. Performance comparison across models and methods on\nHallusionbench and SEED-bench.\nReflection prompt.\nIn the final stage of our approach, a\ncritical technique is to treat the VIC rationale not as defini-\ntive information but as an input that invites reflection within\nthe MLLM to generate the final answer. This reflective ap-\nproach encourages the model to re-evaluate the correctness\nof the VIC rationale, thus enhancing the robustness of the\noverall framework. The ablation experiment on Gemini 1.5\nFlash in SEED-Bench shows the impact of this reflection\nprompt: removing it leads to a slight performance decrease\nof 0.3%, yet the model still achieves a notable 6.23% im-\nprovement over the baseline without the VIC framework.\nAs shown in Table 3, the VIC framework significantly sur-\npasses the baseline across various tasks, particularly in ar-\neas like accuracy, spatial reasoning, and instance identity.\nFor example, accuracy rises from 0.658 with the Origin\nmodel to 0.699 with VIC (without reflection) and further\nto 0.701 with full reflection. The VIC framework also en-\nhances spatial reasoning, with scores improving from 0.419\nin the baseline to 0.605 in the full setup. The inclusion of the\nreflection prompt solidifies the model’s robustness, as it pro-\nmotes careful reassessment of initial answers, contributing\nto more accurate and dependable predictions across tasks.\nOpen source model.\nWe tested the VIC framework on\nan open-source model, Qwen-VL Plus, an MLLM with a\nmedium parameter size. For some benchmarks, the perfor-\nmance with the VIC framework declined, while others still\nshowed significant improvement. We believe this is because\nthe VIC framework requires a high capacity in areas such\nas multi-step instruction following and the ability to handle\nlong contexts with visual input. The lack of these capabil-\nities can likely lead to the failure of the VIC framework.\nTherefore, we prioritized using closed-source models with\nCategory\nMethod\nGemini 1.5 Flash\nOrigin\nCOTvanilla\nVIC w/o reflection prompt\nVIC\nAcc\n0.658\n0.672\n0.699\n0.701\nInstance Attributes\n0.677\n0.692\n0.714\n0.711\nInstance Identity\n0.763\n0.734\n0.777\n0.806\nInstance Interaction\n0.900\n0.700\n0.800\n0.800\nInstance Location\n0.500\n0.484\n0.578\n0.594\nInstances Counting\n0.542\n0.603\n0.648\n0.626\nScene Understanding\n0.724\n0.724\n0.710\n0.719\nSpatial Relation\n0.419\n0.558\n0.581\n0.605\nText Understanding\n0.500\n0.500\n1.000\n1.000\nVisual Reasoning\n0.821\n0.786\n0.786\n0.750\nTable 3. Performance metrics for different methods of Gemini 1.5\nFlash on Seed-Bench\nstronger overall performance across a wider range of capa-\nbilities to implement our framework effectively.\n5. Discussion\nAlthough our thinking before looking framework demon-\nstrates remarkable performance across diverse vision-text\ntasks, we view it as a crucial enhancement rather than the\nultimate solution for MLLMs reasoning. It provides a fresh\nperspective in this domain and holds potential for integra-\ntion with the thinking while looking paradigm to achieve su-\nperior outcomes. In comprehensive evaluations of these two\napproaches, we found that the performance of thinking be-\nfore looking lagged behind thinking while looking in certain\ncategories. We suggest that future advancements should pri-\noritize merging these paradigms through a selection mecha-\nnism or a process of mutual reflection. Such a combination\nwould allow the paradigms to complement each other effec-\ntively, addressing their individual limitations and paving the\nway for more sophisticated reasoning strategies.\n6. Conclusion\nIn this paper, we introduce the Visual Inference Chain\n(VIC) framework, a novel method that mitigates reasoning\nbiases in MLLMs by decoupling visual and textual inputs,\nadvancing the thinking before looking paradigm. By sys-\ntematically separating visual elements, VIC framework en-\nhances reasoning robustness, significantly reduces halluci-\nnations, and improves performance across diverse vision-\nlanguage tasks.\nOur experiments demonstrate that VIC\nconsistently boosts zero-shot performance and provides de-\ntailed insights into the impact of different VIC genera-\ntors, underscoring the effectiveness of reflective prompting.\nOverall, VIC framework enhances accuracy and strengthens\nthe reliability of multimodal reasoning effectively.\n8\n\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 1\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1\n[3] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo\nHan, Zheng Zhang, and Mike Zheng Shou. Hallucination of\nmultimodal large language models: A survey. arXiv preprint\narXiv:2404.18930, 2024. 3\n[4] Ankan Bansal, Yuting Zhang, and Rama Chellappa. Visual\nquestion answering on image sets.\nIn Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XXI 16, pages 51–67.\nSpringer, 2020. 1\n[5] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi\nParikh, et al.\nRubi: Reducing unimodal biases for visual\nquestion answering.\nAdvances in neural information pro-\ncessing systems, 32, 2019. 1\n[6] Google DeepMind. Gemini 1.5: Unlocking multimodal un-\nderstanding across millions of tokens of context. https:\n//deepmind.google/technologies/gemini/,\n2024. Accessed: 2024-11-14. 2\n[7] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao,\nand Guoqi Li. Learning to prompt for open-vocabulary ob-\nject detection with vision-language model. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14084–14093, 2022. 1\n[8] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li,\nXing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre-\nhensive evaluation benchmark for multimodal large language\nmodels, 2024. 2, 5\n[9] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot.\nComplexity-based prompting for multi-step\nreasoning.\nIn The Eleventh International Conference on\nLearning Representations, 2022. 1\n[10] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia\nLi, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,\nYaser Yacoob, et al. Hallusionbench: an advanced diagnos-\ntic suite for entangled language hallucination and visual il-\nlusion in large vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14375–14385, 2024. 2, 3, 5\n[11] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen\nWang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with\nlanguage model is planning with world model, 2023. 1\n[12] Jie Huang and Kevin Chen-Chuan Chang. Towards reason-\ning in large language models: A survey.\nIn Findings of\nthe Association for Computational Linguistics: ACL 2023,\npages 1049–1065, Toronto, Canada, 2023. Association for\nComputational Linguistics. 1\n[13] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and\nXinya Du.\nFaithscore: Evaluating hallucinations in large\nvision-language models. arXiv preprint arXiv:2311.01477,\n2023. 3\n[14] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\nWang, Ruimao Zhang, and Ying Shan. Seed-bench: Bench-\nmarking multimodal large language models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 13299–13308, 2024. 2, 5\n[15] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-\nGuang Lou, and Weizhu Chen. Making large language mod-\nels better reasoners with step-aware verifier. arXiv preprint\narXiv:2206.02336, 2022. 2\n[16] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 1, 2, 3, 5\n[17] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser\nYacoob, Dinesh Manocha, and Tianyi Zhou.\nHallusion-\nbench: You see what you think? or you think what you see?\nan image-context reasoning benchmark challenging for gpt-\n4v (ision), llava-1.5, and other multi-modality models. arXiv\npreprint arXiv:2310.14566, 2023. 1\n[18] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser\nYacoob, and Lijuan Wang.\nAligning large multi-modal\nmodel with robust instruction tuning.\narXiv preprint\narXiv:2306.14565, 2023. 1\n[19] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-\ncoob, and Lijuan Wang. Mitigating hallucination in large\nmulti-modal models via robust instruction tuning.\nIn The\nTwelfth International Conference on Learning Representa-\ntions, 2023. 3\n[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 26296–26306, 2024. 1\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024. 1\n[22] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai\nRen, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcom-\npass: Do video llms really understand videos? arXiv preprint\narXiv:2403.00476, 2024. 1\n[23] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts. arXiv\npreprint arXiv:2310.02255, 2023. 2, 5\n[24] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei\nHerzig. Compositional chain-of-thought prompting for large\nmultimodal models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n14420–14431, 2024. 3\n[25] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj\nSingh, and Godawari Sudhakar Rao. Kam-cot: Knowledge\naugmented multimodal chain-of-thoughts reasoning. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 18798–18806, 2024. 3\n9\n\n[26] OpenAI. Gpt-4o system card. https://openai.com/\nindex/gpt-4o-system-card/, 2024.\nAccessed:\n2024-11-14. 2\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 1\n[28] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learn-\ning to retrieve prompts for in-context learning. arXiv preprint\narXiv:2112.08633, 2021. 1\n[29] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuo-\nfan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual\ncot: Unleashing chain-of-thought reasoning in multi-modal\nlanguage models. arXiv preprint arXiv:2403.16999, 2024. 3\n[30] Robert L Solso, M Kimberly MacLin, and Otto H MacLin.\nCognitive psychology.\nPearson Education New Zealand,\n2005. 2\n[31] Christopher Summerfield and Tobias Egner.\nExpectation\n(and attention) in visual cognition. Trends in cognitive sci-\nences, 13(9):403–409, 2009. 2\n[32] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue\nCao. Eva-clip: Improved training techniques for clip at scale.\narXiv preprint arXiv:2303.15389, 2023. 1\n[33] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann\nLeCun, and Saining Xie.\nEyes wide shut? exploring the\nvisual shortcomings of multimodal llms. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9568–9578, 2024. 1, 2, 5\n[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1\n[35] Ali Vosoughi, Shijian Deng, Songyang Zhang, Yapeng Tian,\nChenliang Xu, and Jiebo Luo. Cross modality bias in visual\nquestion answering: A causal view with possible worlds vqa.\nIEEE Transactions on Multimedia, 2024. 1\n[36] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,\nPan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,\net al.\nVigc: Visual instruction generation and correction.\nIn Proceedings of the AAAI Conference on Artificial Intel-\nligence, pages 5309–5317, 2024. 3\n[37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed\nChi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint arXiv:2203.11171,\n2022. 2\n[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in neural information processing\nsystems, 35:24824–24837, 2022. 1, 2\n[39] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao,\nMin-Yen Kan, Junxian He, and Michael Xie. Self-evaluation\nguided beam search for reasoning. Advances in Neural In-\nformation Processing Systems, 36, 2024. 2\n[40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom\nGriffiths, Yuan Cao, and Karthik Narasimhan.\nTree of\nthoughts: Deliberate problem solving with large language\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 36, 2024. 2\n[41] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang,\nDianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong\nChen. Woodpecker: Hallucination correction for multimodal\nlarge language models.\narXiv preprint arXiv:2310.16045,\n2023. 3\n[42] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\nChen Change Loy. Contextual object detection with multi-\nmodal large language models. International Journal of Com-\nputer Vision, pages 1–19, 2024. 1\n[43] Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin,\nYuan Yao, Mingkai Chen, and Jiebo Luo.\nCocot: Con-\ntrastive chain-of-thought prompting for large multimodal\nmodels with multiple image inputs.\narXiv preprint\narXiv:2401.02582, 2024. 3\n[44] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-\njun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nLi, and Yu Qiao.\nLlama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention.\narXiv preprint\narXiv:2303.16199, 2023. 1\n[45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\nAutomatic chain of thought prompting in large language\nmodels. arXiv preprint arXiv:2210.03493, 2022. 2\n[46] Zhuosheng Zhang,\nAston Zhang,\nMu Li,\nHai Zhao,\nGeorge Karypis, and Alex Smola.\nMultimodal chain-of-\nthought reasoning in language models.\narXiv preprint\narXiv:2302.00923, 2023. 1, 2\n[47] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei\nYang. Ddcot: Duty-distinct chain-of-thought prompting for\nmultimodal reasoning in language models. Advances in Neu-\nral Information Processing Systems, 36:5168–5191, 2023. 1,\n3\n[48] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang\nGao, and Yue Zhang. Image-of-thought prompting for visual\nreasoning refinement in multimodal large language models.\narXiv preprint arXiv:2405.13872, 2024. 3\nA. Dataset details\nMMVP.\nMMVP is designed to evaluate MLLMs’ visual\ncapability, the benchmark is organized into nine different\nvisual patterns and there are 15 pairs of zero-shot questions.\nIt focuses on visual errors commonly arising from models\nusing CLIP-based vision encoders. MMVP tests the mod-\nels’ capability to correctly identify and interpret subtle vi-\nsual features such as object relationships, orientations, and\nfine-grained details. By highlighting these shortcomings,\nthe benchmark aims to provide a more rigorous evaluation\nframework, helping researchers identify areas where visual\nintegration in MLLMs can be improved. We used 150 ques-\ntion pairs (300 questions in total) in the MMVP test set to\nevaluate whether our prompting method can mitigate hallu-\ncinations arising from the vision encoder.\n10\n\nHallusionBench.\nHallusionBench is a diagnostic suite\ndesigned to analyze the dual issues of language hallucina-\ntion and visual illusion in MLLMs. It includes 1129 hand-\ncrafted VQA pairs featuring 165 original images and 181\nimages expertly modified by human professionals. Since\nour framework is designed for single image and question\npairs, we selected a subset from HallucinationBench that\ncontains only single-image questions, excluding text-only\nquestions and two-image questions. There are 674 single\nimage questions in our selected subset in total.\nMME.\nMME is a comprehensive benchmark that mea-\nsures both perception and cognition across 14 subtasks. All\ntasks involve ”Yes-or-No” questions, and the scoring com-\nbines accuracy and a refined accuracy metric (accuracy+).\nThe total scores for perception and cognition are 2000 and\n800, respectively, reflecting a broad assessment of MLLM\nperformance in both areas. We used all the questions in\nMME to test our framework.\nMathvista.\nMathvista is introduced to systematically\nevaluate mathematical reasoning ability in visual contexts.\nIt is derived from 31 different datasets and completing these\ntasks requires fine-grained, deep visual understanding and\ncompositional reasoning. We used the testMini set from\nMathvista, which contains 1,000 visual math questions.\nPOPE.\nPOPE focuses on assessing object hallucinations\nin MLLMs. The dataset includes three sampling strategies:\nrandom, popular, and adversarial. Since the random and\npopular strategies are less challenging, our experiment con-\ncentrates on 1000 VQA pairs generated from the adversarial\nstrategy, which presents more difficult test cases.\nSEED-Bench.\nSEED-Bench serves as a comprehensive\nbenchmark to assess the generative capabilities of multi-\nmodal large language models. It focuses on evaluating mod-\nels through tasks requiring deep comprehension across both\nvisual and textual inputs. SEED-Bench provides a challeng-\ning environment by introducing novel generative tasks, like\nfree-form answer generation, image understanding, and rea-\nsoning. To evaluate our framework on SEED-Bench, we\nrandomly sampled 1,000 single-image instances from the\nSEED-Bench dataset as our test set.\nTo maintain consistency and accuracy in the evaluation,\nwe developed a unified evaluation framework by recon-\nstructing the previous six benchmark evaluation metrics.\nThis framework is designed to handle multiple-choice ques-\ntions (MCQ), yes/no (YORN) questions, and mixed-form\nquestions. Our evaluation framework uses a large language\nmodel to extract answers, parse options, yes/no signals, and\nmore. This approach has been widely adopted in various\nbenchmark evaluations, as models like GPT-3.5 and GPT-\n4o mini can achieve nearly 100% accuracy in such answer-\nparsing tasks, as demonstrated in several previous works.\nTherefore, we employ this evaluation framework for the\nVIC framework, and the code for our evaluation is available\nin our repository.\nB. Detailed Results and Analysis\nB.1. Hallusionbench\nTable 4 presents detailed experimental results of vari-\nous models on HallucinationBench, evaluating performance\nacross subtasks such as chart, figure, illusion, map, and\nothers. Overall, the VIC method consistently outperforms\nboth the original and zero-shot CoT methods in terms of\naccuracy, highlighting its effectiveness across the models.\nNotably, for the table and figure subtasks, zero-shot CoT\nslightly surpasses VIC. This can be attributed to the fact that\nthese subtasks rely heavily on visual information, which\ndiminishes the advantage of VIC’s Thinking before look-\ning paradigm. However, VIC demonstrates superior per-\nformance in most other tasks, with its average results sur-\npassing zero-shot CoT. Particularly in the illusion subtask,\nVIC’s dominance underscores its strength in mitigating hal-\nlucinations, making it highly effective in handling anti-\nhallucination tasks.\nB.2. POPE adversarial\nTable 5 shows that VIC demonstrates its robustness in\nPOPE adversarial samples. While zero-shot CoT exhibits\nhigher precision, VIC maintains a better balance between\nprecision and recall, leading to more reliable overall per-\nformance. For example, Gemini 1.5 Pro shows a precision\nof 0.888 with VIC compared to 0.905 with zero-shot CoT,\nbut VIC achieves a better recall (0.713 vs. 0.701). The\nYes rate remains relatively consistent across all methods,\nbut VIC continues to provide more balanced results. This\npattern of improvement across different models highlights\nVIC’s general effectiveness in delivering robust, superior,\nand well-rounded model performance on POPE.\nB.3. MME\nTable 6 presents detailed experimental results on the MME\nbenchmark, highlighting that the VIC method consistently\noutperforms zero-shot CoT across a range of models in both\ncognition and perception tasks. VIC demonstrates signif-\nicant advantages in most subtasks, such as existence, po-\nsition, landmark recognition, artwork analysis, numerical\ncalculation, text translation, and code reasoning. However,\nboth VIC and zero-shot CoT perform poorly on tasks like\nOCR, color recognition, and poster identification, which do\nnot require advanced reasoning. Overall, VIC enhances the\nmodel’s reasoning abilities, leading to superior performance\n11\n\nmodel\nmethod\nAcc\nchart\nfigure\nillusion\nmap\nmath\nocr\ntable\nvideo\nprecision\nYes rate\nGPT-4o mini\norigin\n0.574\n0.585\n0.585\n0.625\n0.516\n0.426\n0.620\n0.741\n0.406\n0.543\n0.481\nzero-shot CoT\n0.611\n0.638\n0.610\n0.611\n0.578\n0.519\n0.630\n0.848\n0.366\n0.595\n0.454\nVIC\n0.639\n0.723\n0.512\n0.694\n0.547\n0.648\n0.660\n0.777\n0.426\n0.576\n0.438\nGemini 1.5 Flash\norigin\n0.560\n0.623\n0.732\n0.597\n0.516\n0.648\n0.450\n0.661\n0.366\n0.559\n0.350\nzero-shot CoT\n0.614\n0.715\n0.732\n0.569\n0.578\n0.593\n0.590\n0.759\n0.366\n0.564\n0.429\nVIC\n0.625\n0.669\n0.732\n0.625\n0.625\n0.630\n0.570\n0.714\n0.475\n0.612\n0.352\nGPT-4o\norigin\n0.626\n0.738\n0.780\n0.403\n0.594\n0.537\n0.610\n0.804\n0.465\n0.579\n0.433\nzero-shot CoT\n0.673\n0.777\n0.854\n0.542\n0.578\n0.574\n0.670\n0.848\n0.495\n0.634\n0.405\nVIC\n0.692\n0.754\n0.756\n0.667\n0.625\n0.519\n0.800\n0.750\n0.455\n0.625\n0.408\nGemini 1.5 Pro\norigin\n0.617\n0.677\n0.683\n0.389\n0.609\n0.704\n0.610\n0.705\n0.545\n0.612\n0.352\nzero-shot CoT\n0.611\n0.746\n0.659\n0.389\n0.563\n0.519\n0.610\n0.714\n0.545\n0.588\n0.386\nVIC\n0.664\n0.785\n0.829\n0.639\n0.578\n0.537\n0.670\n0.714\n0.525\n0.646\n0.340\nTable 4. Performance comparison of different models and methods evaluated on HallusionBench.\nmodel\nmethod\nAcc\nF1\nprecision\nrecall\nyes rate\nGPT-4o mini\norigin\n0.786\n0.797\n0.847\n0.752\n0.445\nzero-shot CoT\n0.773\n0.755\n0.91\n0.645\n0.355\nVIC\n0.793\n0.757\n0.92\n0.643\n0.35\nGemini 1.5 Flash\norigin\n0.769\n0.724\n0.92\n0.597\n0.325\nzero-shot CoT\n0.741\n0.685\n0.978\n0.527\n0.27\nVIC\n0.78\n0.751\n0.89\n0.649\n0.365\nGPT-4o\norigin\n0.811\n0.824\n0.869\n0.782\n0.451\nzero-shot CoT\n0.793\n0.804\n0.897\n0.729\n0.407\nVIC\n0.827\n0.822\n0.869\n0.78\n0.45\nGemni 1.5 Pro\norigin\n0.779\n0.77\n0.849\n0.705\n0.416\nzero-shot CoT\n0.792\n0.79\n0.905\n0.701\n0.388\nVIC\n0.803\n0.79\n0.888\n0.713\n0.402\nTable 5. Performance comparison of different models and meth-\nods.\non more complex and sophisticated problems.\nB.4. Mathvista\nTable 7 presents the detailed evaluation results for the Math-\nvista benchmark, where the VIC method does not perform\nas well compared to other benchmarks. The primary rea-\nson for this underperformance is attributed to the lack of\ndetailed textual information in the benchmark’s questions.\nMany of the questions, such as Is R0 greater than R or Find\nx, lack sufficient explanations and background information,\nwhich undermines VIC’s ability to leverage its strength in\nreasoning through complex problems. The lack of contex-\ntual detail hinders the method’s potential to fully apply its\ninference capabilities. We believe that by expanding the\nquestion text and providing more detailed background de-\nscriptions, VIC could regain its superiority given by the\nThinking before looking paradigm in solving complex tasks\nwithin this benchmark.\nB.5. SEED-Bench\nTable 8 illustrates that our method, VIC, consistently out-\nperforms both the original and zero-shot CoT methods\nacross all models (Gemini 1.5 Flash, GPT-4o mini, GPT-\n4o, and Gemini 1.5 Pro) in terms of overall accuracy and\nmost subtasks in SEED-Bench. VIC demonstrates particu-\nlarly strong performance in tasks requiring complex reason-\ning, such as instance location and spatial relations, achiev-\ning notably higher scores than the other methods. Although\nzero-shot CoT shows slightly better performance in instance\nidentity for certain models, VIC generally leads in most\nother tasks. Furthermore, all three methods perform equally\nwell on simpler tasks that do not rely on reasoning, such\nas text understanding. Overall, VIC proves to be more ef-\nfective across a wider range of reasoning-intensive tasks,\ndemonstrating its robustness and ability to handle complex\nmultimodal challenges.\n12\n\nModel\nMethod\nPer\nCog\nEx\nCnt\nPos\nCol\nPost\nCel\nScn\nLmk\nArt\nOCR\nCSR\nNumC\nTxtT\nCodR\nGPT-4o mini\nOrigin\n1097.23\n407.14\n136.67\n115\n78.33\n135\n134.35\n45.88\n122.5\n91.5\n88\n150\n117.14\n87.5\n110\n92.5\nzero-shot CoT\n1069.82\n417.14\n126.67\n111.67\n73.33\n135\n135.03\n49.12\n119.5\n77.75\n91.75\n150\n112.14\n105\n100\n100\nVIC\n1105.27\n505.00\n140\n120\n95\n131.67\n132.65\n49.71\n111\n116.5\n101.25\n107.5\n125\n140\n100\n140\nGemini 1.5 Flash\nOrigin\n1077.37\n358.93\n130\n93.33\n58.33\n123.33\n118.71\n124.41\n117.75\n95.5\n101\n115\n101.43\n90\n90\n77.5\nzero-shot CoT\n1105.9\n500.71\n123.33\n128.33\n68.33\n126.67\n129.93\n97.06\n111.5\n87.5\n98.25\n135\n125.71\n150\n85\n140\nVIC\n1118.54\n508.21\n136.67\n130\n90\n133.33\n126.53\n111.76\n99\n96.25\n102.5\n92.5\n115.71\n150\n102.5\n140\nGPT-4o\nOrigin\n1174.39\n522.86\n143.33\n128.33\n101.67\n143.33\n140.14\n10.59\n115.75\n136\n105.25\n150\n132.86\n120\n140\n130\nzero-shot CoT\n1166.11\n537.14\n135\n130\n106.67\n140\n140.82\n15.88\n112.5\n129\n111.25\n145\n127.14\n130\n135\n145\nVIC\n1238.69\n557.85\n146.67\n121.67\n118.33\n141.67\n135.03\n73.82\n112.75\n137.5\n116.25\n135\n127.86\n145\n140\n145\nGemini 1.5 Pro\nOrigin\n1166.82\n462.14\n133.33\n120\n76.67\n135\n131.63\n127.94\n113\n103.5\n100.75\n125\n117.14\n115\n100\n130\nzero-shot CoT\n1099.33\n521.42\n133.33\n123.33\n86.67\n126.67\n115.99\n97.35\n113.5\n80.5\n102\n120\n121.43\n145\n120\n135\nVIC\n1147.23\n561.42\n140\n118.33\n91.67\n135\n133.67\n122.06\n104.75\n93\n98.75\n110\n126.43\n150\n140\n145\nTable 6. Performance comparison of different models and methods. Per: Perception, Cog: Cognition, Ex: Existence, Cnt: Count, Pos:\nPosition, Col: Color, Post: Posters, Cel: Celebrity, Scn: Scene, Lmk: Landmark, Art: Artwork, OCR: Optical Character Recognition,\nCSR: Commonsense Reasoning, NumC: Numerical Calculation, TxtT: Text Translation, CodR: Code Reasoning.\nmodel\nmethod\nAcc\nLR\nAR\nGR\nSR\nALR\nSCR\nNC\nGPT-4o mini\norigin\n0.526\n0.236\n0.581\n0.611\n0.637\n0.189\n0.467\n0.582\nzero-shot CoT\n0.520\n0.243\n0.467\n0.586\n0.588\n0.601\n0.574\n0.271\nVIC\n0.536\n0.582\n0.405\n0.544\n0.515\n0.499\n0.571\n0.292\nGemini 1.5 Flash\norigin\n0.479\n0.523\n0.519\n0.324\n0.566\n0.419\n0.518\n0.340\nzero-shot CoT\n0.520\n0.537\n0.615\n0.641\n0.324\n0.445\n0.519\n0.340\nVIC\n0.516\n0.639\n0.292\n0.635\n0.456\n0.378\n0.427\n0.456\nGPT-4o\norigin\n0.597\n0.216\n0.535\n0.389\n0.704\n0.664\n0.630\n0.607\nzero-shot CoT\n0.622\n0.297\n0.661\n0.721\n0.569\n0.389\n0.676\n0.689\nVIC\n0.620\n0.734\n0.351\n0.656\n0.633\n0.598\n0.623\n0.361\nGemini 1.5 Pro\norigin\n0.568\n0.687\n0.615\n0.665\n0.456\n0.299\n0.648\n0.270\nzero-shot CoT\n0.563\n0.619\n0.577\n0.664\n0.162\n0.711\n0.250\n0.470\nVIC\n0.558\n0.326\n0.691\n0.507\n0.541\n0.502\n0.297\n0.639\nTable 7. Performance comparison of different models and methods across various reasoning categories. Acc: Accuracy, LR: Logical\nReasoning, AR: Arithmetic Reasoning, GR: Geometry Reasoning, SR: Statistical Reasoning, ALR: Algebraic Reasoning, SCR: Scientific\nReasoning, NC: Numeric Commonsense.\nC. Ablation Experiments\nC.1. Different VIC generator\nThe detailed analysis of the two benchmarks, Hallusion-\nBench and SEED-Bench, in table 9 and table 10, offers\nvaluable insights into the performance differences across\nmodels and VIC generators. A key finding is that no single\nVIC generator demonstrates consistent superiority across\nall metrics, reinforcing the original ablation results. Gener-\nator QwenMax exhibits notable versatility, performing well\nacross both benchmarks and models, achieving high preci-\nsion (0.645) and F1-score (0.596) with Gemini 1.5 Flash on\nHallusionBench. It also excels in tasks such as Instance\nInteraction (0.8) and Visual Reasoning (0.75) on SEED-\nBench.\nHowever, the data further illustrates that certain VIC\ngenerators are more suited to particular models and tasks.\nFor example, Gemini 1.5 Flash demonstrates strong perfor-\nmance when paired with its native generator on Hallusion-\nBench, achieving top scores in chart-related tasks (0.746)\n13\n\nmodel\nmethod\nAcc\nIA\nII\nIntI\nILoc\nICount\nSU\nSR\nTU\nVR\nGemini 1.5 Flash\norigin\n0.658\n0.677\n0.763\n0.9\n0.5\n0.542\n0.724\n0.419\n0.5\n0.821\nzero-shot CoT\n0.672\n0.692\n0.734\n0.7\n0.484\n0.603\n0.724\n0.558\n0.5\n0.786\nVIC\n0.713\n0.726\n0.820\n0.7\n0.531\n0.665\n0.719\n0.698\n0.5\n0.75\nGPT-4o mini\norigin\n0.636\n0.649\n0.763\n0.8\n0.391\n0.497\n0.714\n0.512\n0.5\n0.857\nzero-shot CoT\n0.660\n0.665\n0.770\n0.8\n0.422\n0.553\n0.729\n0.581\n0.5\n0.857\nVIC\n0.696\n0.714\n0.755\n0.6\n0.531\n0.603\n0.767\n0.581\n0.5\n0.857\nGPT-4o\norigin\n0.657\n0.720\n0.691\n0.8\n0.484\n0.486\n0.733\n0.558\n1\n0.75\nzero-shot CoT\n0.739\n0.800\n0.791\n0.6\n0.578\n0.637\n0.767\n0.605\n1\n0.821\nVIC\n0.751\n0.791\n0.784\n0.6\n0.609\n0.676\n0.757\n0.814\n1\n0.821\nGemini 1.5 Pro\norigin\n0.678\n0.680\n0.784\n0.8\n0.5\n0.598\n0.733\n0.558\n0.5\n0.786\nzero-shot CoT\n0.691\n0.692\n0.712\n0.6\n0.5\n0.665\n0.776\n0.535\n1\n0.786\nVIC\n0.713\n0.726\n0.777\n0.8\n0.516\n0.687\n0.733\n0.698\n0.5\n0.714\nTable 8. Performance comparison of different models and methods. Columns: Acc - Accuracy, IA - Instance Attributes, II - Instance\nIdentity, IntI - Instance Interaction, ILoc - Instance Location, ICount - Instances Counting, SU - Scene Understanding, SR - Spatial\nRelation, TU - Text Understanding, VR - Visual Reasoning.\nand illusion tasks (0.708). Yet, this advantage is not uni-\nformly observed across all tasks, indicating that the syn-\nergy between the model and generator plays a crucial role\nin task-specific performance. Similarly, GPT4-turbo dis-\nplays notable strengths in text-heavy and reasoning tasks,\nachieving a perfect score in Text Understanding (1.0) and\na high score in Visual Reasoning (0.857) on SEED-Bench.\nNonetheless, it exhibits weaknesses in Scene Understand-\ning (0.592), highlighting that no single model-generator\npairing achieves optimal performance across all evaluation\ncategories.\nIn summary, this fine-grained analysis underscores that\nboth pure language models and multimodal models perform\neffectively across tasks, with no clear evidence of consis-\ntent superiority between the two. Performance outcomes\nare highly contingent upon task complexity and the com-\npatibility between the model and the VIC generator. This\nemphasizes the need for an adaptive approach to VIC gen-\neration, where different generators are selected based on the\nspecific requirements of the task. For instance, QwenMax\nmay be particularly effective for spatial and interaction-\nfocused tasks, while GPT4-turbo could be better suited for\ntext-intensive tasks. Such a strategy would enable mod-\nels to leverage the complementary strengths of various VIC\ngenerators, thereby optimizing performance across diverse\nbenchmarks.\nFigure 6. Reflection prompt used in VIC framework\nFigure 7. Extraction prompt used in VIC framework\n14\n\nModel\nVIC generator\nAcc\nchart\nfigure\nillusion\nmap\nmath\nocr\ntable\nvideo\nprecision\nrecall\nf1\nyes rate\nGemini 1.5 Flash\nGPT-4o mini\n0.625\n0.669\n0.732\n0.625\n0.625\n0.63\n0.57\n0.714\n0.475\n0.612\n0.531\n0.569\n0.352\nGemini 1.5 Flash\n0.638\n0.746\n0.756\n0.708\n0.656\n0.574\n0.66\n0.661\n0.376\n0.61\n0.568\n0.588\n0.377\nGPT4-turbo\n0.631\n0.723\n0.732\n0.667\n0.484\n0.593\n0.61\n0.732\n0.465\n0.614\n0.564\n0.588\n0.372\nQwen-Max\n0.639\n0.7\n0.707\n0.653\n0.578\n0.611\n0.64\n0.732\n0.475\n0.645\n0.553\n0.596\n0.347\nGPT-4o mini\nGPT-4o mini\n0.639\n0.723\n0.512\n0.694\n0.547\n0.648\n0.66\n0.777\n0.426\n0.576\n0.623\n0.599\n0.438\nGemini 1.5 Flash\n0.648\n0.777\n0.634\n0.736\n0.594\n0.556\n0.67\n0.768\n0.356\n0.588\n0.597\n0.593\n0.411\nGPT4-turbo\n0.638\n0.646\n0.585\n0.778\n0.469\n0.63\n0.67\n0.795\n0.455\n0.573\n0.634\n0.602\n0.448\nQwen-Max\n0.635\n0.708\n0.634\n0.667\n0.594\n0.63\n0.69\n0.741\n0.376\n0.582\n0.612\n0.596\n0.426\nTable 9. Comparison of different models and VIC generators based on various metrics on Hallusionbench.\nModel\nVIC Gen.\nAcc\nIA\nII\nInt.\nIL\nIC\nSU\nSR\nTU\nVR\nGemini 1.5 Flash\nQwen-Max\n0.701\n0.710\n0.805\n0.8\n0.593\n0.625\n0.719\n0.604\n1\n0.75\nGemini 1.5 Flash\n0.696\n0.726\n0.805\n0.5\n0.421\n0.636\n0.704\n0.697\n0.5\n0.821\nGPT4-turbo\n0.687\n0.735\n0.762\n0.7\n0.484\n0.597\n0.738\n0.534\n0.5\n0.642\nGPT-4o mini\n0.713\n0.726\n0.820\n0.7\n0.531\n0.664\n0.719\n0.697\n0.5\n0.75\nGPT-4o mini\nQwen-Max\n0.676\n0.710\n0.784\n0.8\n0.5\n0.569\n0.709\n0.534\n0.5\n0.75\nGemini 1.5 Flash\n0.668\n0.661\n0.712\n0.5\n0.421\n0.536\n0.676\n0.558\n0.5\n0.714\nGPT4-turbo\n0.706\n0.726\n0.805\n0.7\n0.5\n0.592\n0.766\n0.627\n0.5\n0.857\nGPT-4o mini\n0.696\n0.713\n0.755\n0.6\n0.531\n0.603\n0.766\n0.581\n0.5\n0.857\nTable 10. Performance comparison of different models and VIC generators on various evaluation metrics on SEED-Bench. Column\nabbreviations: IA (Instance Attributes), II (Instance Identity), Int. (Instance Interaction), IL (Instance Location), IC (Instances Counting),\nSU (Scene Understanding), SR (Spatial Relation), TU (Text Understanding), VR (Visual Reasoning).\nBenchmark\nMethod\nQwen-VL Plus\nOriginal\nVIC\nPercentage Change (%)\nHallucinationBench\n0.31\n0.40\n29.03%\nMMVP\n0.36\n0.21\n-40.75%\nMME\n1479.69\n1375.89\n-7.02%\nmathvista\n0.254\n0.317\n24.80%\nPOPE adversal\n0.827\n0.787\n-4.84%\nSEED-Bench\n0.678\n0.536\n-20.94%\nTable 11. Performance Comparison of Qwen-VL Plus in Different\nBenchmarks\nC.2. Open source models\nOpen-source models, such as Qwen-VL Plus, may strug-\ngle with the VIC framework primarily due to their limita-\ntions in handling multi-modal long-context processing and\nmulti-task execution (shown in table 11). The VIC frame-\nwork leverages zero-shot capabilities, meaning it expects\nthe model to generalize effectively across tasks without spe-\ncific fine-tuning. However, the framework’s full potential\ncan only be realized if the underlying model can manage\ncomplex interactions between multiple inputs, maintain co-\nherence over extended contexts, and switch efficiently be-\ntween diverse tasks.\nA key challenge lies in multimodal long-context compre-\nhension. The VIC framework requires the model to extract\nand retain information across multiple modalities over an\nextended sequence of inputs. This means that the model\nmust not only process each modality independently but also\nintegrate them seamlessly across a long temporal or contex-\ntual span. Open-source models often struggle to maintain\nsuch cross-modal coherence, especially when the context\ninvolves several dependent interactions across modalities.\nIncomplete or inconsistent contextual understanding can re-\nduce the effectiveness of VIC in these scenarios.\nAnother crucial factor is multi-task execution. The VIC\nframework expects the model to perform a wide range of\n15\n\ntasks simultaneously or switch seamlessly between them.\nThis involves a high degree of flexibility and task coordina-\ntion, which requires robust underlying mechanisms for task\nmanagement and cross-task transfer. Open-source models\nmay lack the capacity needed to efficiently handle multiple\ntasks together, leading to degraded performance when the\nframework is deployed in multi-task scenarios.\nIn summary, the VIC framework demands strong ca-\npabilities in managing multi-modal long-context reasoning\nand multi-task execution. As a result, while these models\nmay exhibit promising zero-shot abilities in simpler tasks,\nthey struggle to unlock the full potential of the VIC frame-\nwork.\n16\n\nD. Visual Inference Chain Prompt\n17\n\nE. Experiment Examples\n18\n\n19\n\n20",
    "pdf_filename": "Thinking_Before_Looking_Improving_Multimodal_LLM_Reasoning_via_Mitigating_Visual_Hallucination.pdf"
}