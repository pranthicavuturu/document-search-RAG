{
    "title": "Error-Feedback Model for Output Correction in Bilateral Control-Based Imitation Learning",
    "context": "networks has enabled robots to perform flexible tasks. However, since neural networks operate in a feedforward structure, they do not possess a mechanism to compensate for output errors. To address this limitation, we developed a feedback mechanism to correct these errors. By employing a hierarchical structure for neural networks comprising lower and upper layers, the lower layer was controlled to follow the upper layer. Additionally, using a multi-layer perceptron in the lower layer, which lacks an internal state, enhanced the error feedback. In the character- writing task, this model demonstrated improved accuracy in writing previously untrained characters. In the character-writing task, this model demonstrated improved accuracy in writing previously untrained characters. Through autonomous control with error feedback, we confirmed that the lower layer could effectively track the output of the upper layer. This study represents a promising step toward integrating neural networks with control theories. Index Terms—imitation learning, deep learning, feedback con- trol In recent years, imitation learning has gained significant attention for enabling robots to perform complex actions [1] [2] [3]. Imitation learning is a type of supervised learning in which neural networks (NNs) learn from human demon- strations. Furthermore, research on imitation learning using position and force information has advanced. Specifically, bi- lateral control-based imitation learning has proven effective in reproducing human force application [4] [5] [6] [7]. Bilateral control is a teleoperation technology that uses two robots: one interacts with the environment, while the other is operated by a human applying force. By collecting data with this technology, both position and force response and command values can This work was supported by JSPS KAKENHI Grant Number 24K00905, JST, PRESTO Grant Number JPMJPR24T3 Japan and JST ALCA-Next Japan, Grant Number JPMJAN24F1. This study was based on the results obtained from the JPNP20004 project subsidized by the New Energy and Industrial Technology Development Organization (NEDO). be obtained, allowing robots to replicate human operational sensations. The use of force-based imitation learning shows promise for replacing human tasks with robots. However, in conventional bilateral control-based imitation learning, NN has a feedforward structure and does not control output errors, as shown in Fig. 1. Hence, errors during the autonomous operation of the NN are not compensated. This issue is observed not only in bilateral control-based imitation learning but also in many NN-based imitation learning approaches. Traditionally, NNs have required an internal state to retain memory for handling time-series data. However, this NN struggles to integrate with controllers due to the significant influence of the internal state. This suggests that the system’s non-Markovian nature complicates NN control. Generally, systems are more likely to exhibit Markovian properties when the sampling period is shortened. Therefore, to realize effec- tive NN control, it is essential to establish a structure with independent components for the short-sampling-period NN, which exhibits Markovian properties and is easier to control, and the long-sampling-period NN, which has non-Markovian properties and enables complex time-series inference. In this study, we developed a control system for a hierar- chical model with different sampling periods. The hierarchical model comprises an upper layer that makes long-term predic- tions and a lower layer that makes short-term predictions. The upper layer is a strong non-Markovian system that predicts action plans based on past information. Conversely, the lower- layer is a strong Markovian system that predicts command values and states with a short sampling period. This type of model has been proposed in previous research [8], demonstrat- ing its effectiveness for long-term tasks. However, prior studies employed Long Short-Term Memories (LSTMs) with internal states for the Markovian lower-layer. Therefore, we employed a multilayer perceptron (MLP), which lacks an internal state, to construct a control system for the output. During the control process, the error in the robot’s state predicted by the upper arXiv:2411.12255v1  [cs.RO]  19 Nov 2024",
    "body": "Error-Feedback Model for Output Correction in\nBilateral Control-Based Imitation Learning\n1st Hiroshi Sato\nIntelligent and Mechanical Interaction Systems\nUniversity of Tsukuba\nTsukuba, Japan\nsato.hiroshi.tkb cu@u.tsukuba.ac.jp\n2th Masashi Konosu\nIntelligent and Mechanical Interaction Systems\nUniversity of Tsukuba\nTsukuba, Japan\nkonosu.masashi.qa@alumni.tsukuba.ac.jp\n3nd Sho Sakaino\nSystems and Information Engineering\nUniversity of Tsukuba\nTsukuba, Japan\nsakaino@iit.tsukuba.ac.jp\n4rd Toshiaki Tsuji\nScience and Engineering\nSaitama University\nSaitama, Japan\ntsuji@ees.saitama-u.ac.jp\nAbstract—In recent years, imitation learning using neural\nnetworks has enabled robots to perform flexible tasks. However,\nsince neural networks operate in a feedforward structure, they\ndo not possess a mechanism to compensate for output errors.\nTo address this limitation, we developed a feedback mechanism\nto correct these errors. By employing a hierarchical structure\nfor neural networks comprising lower and upper layers, the\nlower layer was controlled to follow the upper layer. Additionally,\nusing a multi-layer perceptron in the lower layer, which lacks an\ninternal state, enhanced the error feedback. In the character-\nwriting task, this model demonstrated improved accuracy in\nwriting previously untrained characters. In the character-writing\ntask, this model demonstrated improved accuracy in writing\npreviously untrained characters. Through autonomous control\nwith error feedback, we confirmed that the lower layer could\neffectively track the output of the upper layer. This study\nrepresents a promising step toward integrating neural networks\nwith control theories.\nIndex Terms—imitation learning, deep learning, feedback con-\ntrol\nI. INTRODUCTION\nIn recent years, imitation learning has gained significant\nattention for enabling robots to perform complex actions [1]\n[2] [3]. Imitation learning is a type of supervised learning\nin which neural networks (NNs) learn from human demon-\nstrations. Furthermore, research on imitation learning using\nposition and force information has advanced. Specifically, bi-\nlateral control-based imitation learning has proven effective in\nreproducing human force application [4] [5] [6] [7]. Bilateral\ncontrol is a teleoperation technology that uses two robots: one\ninteracts with the environment, while the other is operated by a\nhuman applying force. By collecting data with this technology,\nboth position and force response and command values can\nThis work was supported by JSPS KAKENHI Grant Number 24K00905,\nJST, PRESTO Grant Number JPMJPR24T3 Japan and JST ALCA-Next Japan,\nGrant Number JPMJAN24F1. This study was based on the results obtained\nfrom the JPNP20004 project subsidized by the New Energy and Industrial\nTechnology Development Organization (NEDO).\nbe obtained, allowing robots to replicate human operational\nsensations. The use of force-based imitation learning shows\npromise for replacing human tasks with robots. However, in\nconventional bilateral control-based imitation learning, NN has\na feedforward structure and does not control output errors,\nas shown in Fig. 1. Hence, errors during the autonomous\noperation of the NN are not compensated. This issue is\nobserved not only in bilateral control-based imitation learning\nbut also in many NN-based imitation learning approaches.\nTraditionally, NNs have required an internal state to retain\nmemory for handling time-series data. However, this NN\nstruggles to integrate with controllers due to the significant\ninfluence of the internal state. This suggests that the system’s\nnon-Markovian nature complicates NN control. Generally,\nsystems are more likely to exhibit Markovian properties when\nthe sampling period is shortened. Therefore, to realize effec-\ntive NN control, it is essential to establish a structure with\nindependent components for the short-sampling-period NN,\nwhich exhibits Markovian properties and is easier to control,\nand the long-sampling-period NN, which has non-Markovian\nproperties and enables complex time-series inference.\nIn this study, we developed a control system for a hierar-\nchical model with different sampling periods. The hierarchical\nmodel comprises an upper layer that makes long-term predic-\ntions and a lower layer that makes short-term predictions. The\nupper layer is a strong non-Markovian system that predicts\naction plans based on past information. Conversely, the lower-\nlayer is a strong Markovian system that predicts command\nvalues and states with a short sampling period. This type of\nmodel has been proposed in previous research [8], demonstrat-\ning its effectiveness for long-term tasks. However, prior studies\nemployed Long Short-Term Memories (LSTMs) with internal\nstates for the Markovian lower-layer. Therefore, we employed\na multilayer perceptron (MLP), which lacks an internal state,\nto construct a control system for the output. During the control\nprocess, the error in the robot’s state predicted by the upper\narXiv:2411.12255v1  [cs.RO]  19 Nov 2024\n\nFollower’s\nresponse\nNeural\nNetwork\nFollower’s\nController\nLeader’s\nresponse\nFig. 1. Overview of the autonomous motion using NN\nand lower layers was fed back into the input of the lower layer.\nThis allows the lower layer to adjust its inference to minimize\nthe error relative to the upper layer’s predictions. We refer to\nthis model as the error feedback model.\nThe effectiveness of the proposed method was validated\nthrough a character-writing task involving both learned and\nunlearned characters. Evaluation was based on the accuracy\nof the drawn characters and the trajectory of angles. For\ncomparison, the lower layer was implemented using both\nLSTM and MLP. It is important to note that this study explores\nthe potential of the error-feedback model for the lower layer,\nassuming that the state predicted by the upper layer is already\nknown. This approach is expected to lead to the development\nof a new model that combines NNs with control systems.\nII. RELATED WORKS\nA. World Model with Control System\nIntegrating control with NNs has been extensively studied\nusing world models [9] [10] [11]. A world model is an NN that\nlearns the structure of the environment from observation data,\nrepresenting it in a latent space. By incorporating a mechanism\nto control errors in this latent space, it becomes possible to\ncombine NNs with control. However, these methods assume\nconstant dynamics, which poses challenges for tasks involving\ncontact or multiple actions, as dynamic changes over time\ncomplicate mapping to the latent space.\nB. Bilateral Control-Based Imitation Learning\nBilateral control-based imitation learning uses bilateral con-\ntrol during the data collection phase. Bilateral control is a\nteleoperation technique that synchronizes the positions and\nforces of two robots: a leader and a follower. The leader\nreceives forces from a human operator, while the follower\ninteracts with the environment. Using this control method to\nperform tasks, the response values of both the leader and\nfollower are collected. The leader’s response value serves as\nthe command for the follower. As a result, the response and\ncommand values of the follower can be collected separately.\nAn NN is then trained to predict the next command value of\nthe follower based on its current response value. Once trained,\nthe NN enables autonomous movements that replicate bilateral\ncontrol, allowing the execution of tasks requiring force control.\nLower\nLayer\nNeural Network\nUpper Layer\nFig. 2. Hierarchical model proposed by Hayashi et al. [8]\nC. Hierarchical Model\nA hierarchical model processes information at different\nlevels of abstraction in each-layer, breaking down complex\ntasks into manageable sub-tasks. [12] [8]. Hayashi et al.\nproposed a hierarchical model for bilateral control-based imi-\ntation learning [8]. The proposed hierarchical model is shown\nin Fig. 2. Here, f and l represent the follower and leader,\nrespectively, and the subscript t indicates the operational step\nof the NN. The upper-layer infers the state 10 steps ahead\nf upper\nk+10 and provides it to the lower-layer. Conversely, the\nlower-layer considers the follower’s current state fk and state\nf upper\nk+10 provided by the upper-layer as inputs. It then predicts\nthe follower’s next state ˆ\nfk+1 and leader’s next state ˆlk+1.\nThe hierarchical model has been shown to be effective\nfor long-term tasks. Additionally, it has been confirmed that\nthe model can accomplish tasks even when the lower layer\nreceives unlearned information from the upper layer. However,\nwithout a control mechanism for the lower layer to follow\nthe upper layer, there is concern about a decrease in task\nperformance accuracy.\nIII. PROPOSED METHOD\nA. Separation of the Markovian and non-Markovian proper-\nties of tasks\nIn this study, a hierarchical model was employed to separate\nthe Markovian and non-Markovian aspects of the system. The\nhierarchical model is the same as those used in previous\nresearch [8]. The upper-layer infers task plans over a long\nperiod, handling the non-Markovian properties of the system.\nIn contrast, the lower-layer performs short-period inference of\ncommands and states based on the current state and a few\nsteps ahead of the follower’s state. Given that it performs\nshort-period inference and involves predictions that interpolate\nbetween different steps, the lower-layer system is considered\nto exhibit high Markovian properties. Based on this reasoning,\ncontrol was constructed for the Markovian aspects of this\nhierarchical model.\nB. hierarchical model with error-feedback mechanism\nIn this study, we propose an error feedback model, as shown\nin Fig. 3. This model is designed to control the system to\n\nNeural Network\nLower\nLayer\nUpper Layer\nCalculate\nError\nFig. 3. Proposed error-feedback model\nreduce the error between the outputs. In the proposed method,\nthe upper-layer generates the state one step ahead f upper\nk+1 and\nstate ten steps ahead f upper\nk+10. Additionally, the error between the\noutput of the lower-layer ˆfk+1 and upper-layer output f upper\nk+1\nwas calculated. Here, the error calculation is defined simply\nas the difference f upper\nk+1 −ˆfk+1. Extending this to various\ncontrol mechanisms is a future work, but the simple difference\nworked well in this study. Subsequently, the information from\nthe upper-layer given to the lower-layer, f upper, is defined as\nfollows:\nf upper = f upper\nk+10 +\n\u0010\nf upper\nk+1 −ˆfk+1\n\u0011\n(1)\nBy adding the error to the upper-layer output, it is expected\nthat the lower-layer will generate an output that has been\ncorrected for errors.\nIt should be noted that f upper\nk+10 is updated every ten steps,\nwhile f upper\nk+1\nis updated at each time step. This was stan-\ndardized to enable comparison with previous research [8].\nAdditionally, this mechanism was applied during the robot’s\nautonomous operation. Therefore, the learning process does\nnot include an error feedback mechanism, similar to conven-\ntional hierarchical models.\nC. NN Design of the lower-layer\nTo address the Markovian properties of the system, the\nlower-layer is designed as a simple multi-layer perceptron\n(MLP) without internal states, as illustrated in Fig. 5. The\nnetwork comprises four layers of fully connected layers with\n200 dimensions using the Tanh function as the activation\nfunction for all layers except for the final one. Additionally,\nfor comparison, the lower layer using the conventional long\nshort-term memory (LSTM) architecture is also presented in\nFig. 4. The LSTM network is comprised of three layers of 200-\ndimensional LSTM units and a fully connected layer, resulting\nin a total of four layers.\nIn this study, the states predicted by the upper-layer are\nassumed to be known. Specifically, time-series states of the\nfollower are stored in advance through bilateral control, and\nthis data is utilized. This allows for the comparison of different\nlower-layers using the same predictions from the upper-layer.\nLSTM\nInput\nFC\nOutput\nLSTM\nLSTM\nNeural Network\nhidden layer = 200 neurons\nFig. 4. LSTM model\nFC\nInput\nFC\nOutput\nFC\nFC\nNeural Network\nhidden layer = 200 neurons\nFig. 5. MLP model\nIV. EXPERIMENT METHOD\nA. Manipulator\nIn this study, CRANE-X7, manufactured by RT Corpora-\ntion, was employed. The manipulator has seven degrees of\nfreedom, and the gripper has one degree of freedom. The\ngripper was replaced by a cross-structured hand [6]. Given\nthat controlling a manipulator with seven degrees of freedom\ncan be challenging for humans, joint 2 was fixed using position\ncontrol, effectively reducing the system to a six degrees\nof freedom manipulator. Each axis of the manipulator was\ncontrolled by a position and force hybrid controller, with a\ncontrol period of 500 Hz [4]. The joint angles θ were measured\nby rotary encoders at each joint, and the angular velocities ˙θ\nwere calculated by its pseudodifferential. The torques τ were\nestimated using a reaction force estimation observer [13].\nB. Verification of autonomous operation\nIn the experiment, a writing task was conducted using the\nrobot. The robot wrote the characters while holding the pen\nfrom the beginning, as shown in Fig. 6. The characters drawn\nby the robot were captured by an Intel RealSense D435i\nmounted on the top of the whiteboard.\n1) Preliminary: As a preliminary experiment, we investi-\ngated the amount of information required as the upper-layer\noutput. Specifically, in Fig. 3, the upper layer outputs one of\nthe following three: f upper\nk+10 = [θ], f upper\nk+10 = [θ, ˙θ], f upper\nk+10 =\n[θ, ˙θ, τ]. The lower-layers were trained to write the character\n’A’ using two model types: LSTM and MLP. With the learned\nNNs, the robot performed the operation of writing ’A.’ In the\nautonomous operation, the upper-layer outputs, which were\ncollected in advance by the bilateral control, were used. The\noptimal amount of information for the upper-layer outputs\nwere selected by the evaluation described below.\n\nFig. 6. Task environment\nFig. 7.\nCharacters drawn by\ntraining data\n2) Evaluation of error-feedback model: A comparison was\nconducted between the conventional hierarchical model and\nproposed hierarchical model. The lower-layer models, which\nwere highly evaluated in the preliminary experiments, were\nutilized. An experiment was conducted to write three different\ntypes of characters: character ’A,’ ’4,’ and ’B.’ Specifically,\n’A’ is a learned character, while ’4’ and ’B’ are characters\nthat had not been learned.\nCharacter ’4’ was selected because it has a shape that\nis similar to ’A’ but somewhat different, while character\n’B’ was chosen for its distinct shape compared to ’A.’ To\nenable the writing of these characters, the upper-layer outputs\nwere modified to correspond to each respective character. The\nupper-layer outputs was determined based on the follower’s\nstate, which was previously collected through bilateral control.\nMeanwhile, the lower-layer NN remained unchanged from\nthe preliminary experiments. In conventional imitation learn-\ning, the NN must be retrained for each new character. When\nthe upper-layer outputs for an unlearned character are used,\nerrors are expected in the NN’s output. Therefore, we aimed\nto verify whether the proposed method could suppress these\nerrors and generate command values that align with the upper-\nlayer outputs.\nC. Training NN\nWe collected training data and validation data by using\nbilateral control to write character ’A.’ The training data were\ncollected seven times, five of which were used as training data\nand two as validation data. Fig. 7 is a diagram that is drawn\nwhen the data is collected. This is the image of the characters\non the whiteboard captured by a camera and superimposed\nafter binarization.\nWhen training NN using time-series data, the learning\nefficiency can be improved by reducing the sampling fre-\nquency. For this purpose, the joint information acquired at\n2-ms intervals was sampled every 10 steps by shifting the\nstarting point, creating a data set with 20-ms intervals [14].\nThis process increased the amount of data by a factor of 10.\nAdditionally, a 20 rad/s low-pass filter was applied to\nthe teacher data to remove high-frequency components. The\ninput data were augmented with normally distributed noise\nwith a variance of 0.01. The length of each sequence was\nunified by padding, which copies the last value. The data were\nnormalized to mean 0 and standard deviation 1 during training.\nMean Squared Error (MSE) was used as the loss function and\nAdam was used for optimization. The learning rate was set to\n0.0001, and the batch size to 16, and the number of epochs to\n1000.\nD. Evaluation Method\nEvaluation of autonomous movements was performed in\ntwo ways: assessing the diagrams drawn by the robot and\nevaluating the joint angles during autonomous movements.\nAutonomous movements were performed five times, and the\nmeans and standard deviations were calculated for both eval-\nuation methods. The outputs from the upper-layer in this\nstudy were collected using bilateral control. Therefore, the\nupper-layer outputs of the drawn characters and joint angle\ninformation were collected in advance.\n1) IoU: Intersection over Union (IoU) was used to evaluate\nthe accuracy of the diagrams. The diagrams drawn by the robot\nwere captured by a camera and binarized in black and white.\nThe IoU is calculated as follows:\nIoU = Bupper ∩Boutput\nBupper ∪Boutput\n(2)\nwhere Bupper represents the character area drawn in the upper-\nlayer output, and Boutput denotes the character area drawn by\nthe autonomous motion of NN. As this value approaches 1, it\nindicates a higher degree of match between the two characters.\nBy comparing the IoU values, we evaluated whether the\nautonomous control followed the upper-layer output.\n2) MSE of Angles: The joint angles of autonomous move-\nment were evaluated. The purpose of this study is to control\nthe lower-layer to approach the upper-layer outputs. In other\nwords, by calculating the error between the joint angles\nobtained by autonomous movements and joint angles of the\nupper-layer output, it is possible to evaluate the follow-up to\nthe upper-layer outputs. We termed this as Angular Error and\ncalculated it as follows:\nAngularError =\nn\nX\nk=0\n(θupper\nk\n−θres\nk )\n(3)\nwhere k, k = 0, k = n, θupper, and θres denote specific\ntime, task start time, task end time, the angle of the upper-\nlayer output, and the angle response value of the robot\nduring autonomous operation. By comparing these values, we\nevaluated the tracking performance of the robot relative to the\nupper-layer output.\nV. EXPERIMENT\nA. Preliminary\nThe results of the preliminary experiments are shown in\nFig. 8. The black line in the figure represents the drawing made\nby autonomous actions, while the light red color indicates the\nupper-layer output. Comparing the drawn characters with the\nIoU, LSTM and MLP achieved the highest values when using\n\nMLP\nLSTM\nFigure\nUpper-layer output\n= [ ]\n0.104 ±  0.016\n0\nIoU\n10−3(2.01 ±  0.20)\n10−3(11.76 ±  2.66)\nA.E.\nFigure\nUpper-layer output\n= [\n]\n0.173 ±  0.017\n0.190 ±  0.068\nIoU\n10−3(1.48 ±  0.24)\n10−3(1.37 ±  0.19)\nA.E.\nFigure\nUpper-layer output\n= [\n]\n𝟎. 𝟑𝟐𝟓±  𝟎. 𝟎𝟖𝟖\n𝟎. 𝟒𝟏𝟏±  𝟎. 𝟎𝟗𝟔\nIoU\n𝟏𝟎−𝟑(𝟏. 𝟑𝟓± 𝟎. 𝟏𝟒)\n𝟏𝟎−𝟑(𝟏. 𝟎𝟕±  𝟎. 𝟏𝟑)\nA.E.\nA.E = Angular Error\nFig. 8.\nPerformance comparison of autonomous robot operation based on\ndifferences in upper-layer information\nf upper\nk+10\n= [θ, ˙θ, τ] as the upper-layer outputs. Additionally,\nthe angle error was lowest under the same conditions.\nIt is considered that this task required future force because\nwriting characters involves applying force to a board. Based\non these results, f upper\nk+10\n= [θ, ˙θ, τ] was used as the upper-\nlayer output to verify the effectiveness of the error-feedback\nmodel.\nB. Evaluation of error-feedback model\nTo verify the effectiveness of the proposed method, tasks\nwere conducted to write several characters: ’A,’ ’4’ and ’B.’\nThe results with the error-feedback model are described as ’w/\nfeedback.’\n1) character A: The upper part of Fig. 9 presents the results\nfor the task of writing character ’A.’ Using the error-feedback\nmodel, no increase in IoU was observed for either LSTM or\nMLP, though a decrease in Angle Error was confirmed. In\nthe error-feedback model, there was no significant difference\nbetween LSTM and MLP, but the LSTM model showed\nslightly higher IoU results. Overall, it can be concluded that\nthe conventional LSTM provides sufficient performance in\nwriting character ’A,’ as the lower-layer have learned these\ncharacters. The observed decrease in Angle Error, while not\naffecting the IoU, suggests successful approximation of the\nupper-layer output.\n2) character 4: The middle part of the Fig. 9 presents\nthe results for the task of writing character ’4.’ In the error-\nfeedback model, MLP showed an increase in IoU, while\nLSTM showed a slight increase. Similarly, MLP reduced the\nAngle Error, and LSTM showed a slight reduction. When\ncomparing the performance of LSTM and MLP, MLP achieved\na larger IoU and a smaller Angle Error than LSTM. These\nresults indicate that using MLP with the error-feedback model\nimproved task accuracy. Given that MLP does not have an\ninternal state, it made appropriate predictions based on error\nfeedback without being influenced by past memories.\n3) character B: The bottom part of the Fig. 9 presents\nthe results for the task of writing character ’B.’ In the error-\nfeedback model, no increase in IoU was observed for either\nLSTM or MLP. However, a decrease in Angle Error was noted.\nWhen comparing LSTM and MLP, MLP exhibited a smaller\nAngle Error than LSTM.\nThese results suggest that NN model faced difficulties in\nperforming the task of writing the character ’B.’ However, with\nthe error-feedback model using MLP, there was a significant\nreduction in Angle Error. This indicates that the followability\nto the upper-layer outputs improved. The low IoU values are\nthought to be due to the pen not making contact with the\nboard, despite following the upper-layer outputs.\nThe difficulty in performing the task of writing the character\n’B’ may be attributed to the fact that the motion for ’B’\ninvolved extrapolation beyond the learned data. In particular,\nthe second stroke falls outside the range of the motion used for\ndrawing ’A.’ As a result, the lower-layer may not have learned\nthe skills necessary for writing ’B,’ making it challenging to\nfollow the upper-layer output.\nVI. CONCLUSION\nIn this study, we proposed an error-feedback model for a\nhierarchical NN that addresses output errors through feedback.\nThe method calculates output errors between upper and lower\nlayers and incorporates them into a lower-layer input. In\nthe writing task experiments, the model accurately followed\nthe upper-layer output. Additionally, using an MLP in the\nlower-layer enhanced tracking performance and improved the\naccuracy of character generation. It is believed that these\nresults represent a significant first step toward integrating NNs\nwith control theory.\nVII. FUTURE WORKS\nThe next step is to develop an lower-layer that can more\naccurately follow the upper-layer output. In this study, the\nlower-layer learned to write only character ’A.’ To enable the\nlower-layer to more flexibly follow the upper-layer output, it is\nnecessary for lower layer to learn a wider range of behaviors.\nThus, it is essential to verify whether teaching a variety of\nbehaviors to the lower-layer improves their ability to follow\nthe upper-layer output.\nAdditionally, the next challenge is the extension to the\nlower-layer, where only the trajectory is provided from the\nupper-layer. In this study, the outputs of the upper-layer were\nangle, angular velocity, and torque. However, in many cases,\nit is difficult to obtain all three of these physical quantities.\nIf the lower-layer only require the angle information from\n\nMLP w/ feedback\nLSTM w/ feedback\nMLP\nLSTM\nFigure\nCharacter of A\n0.355 ±  0.008\n0.366 ±  0.062\n0.325 ±  0.088\n0.411 ±  0.096\nIoU\n10 −3 ( 0.49 ±  0.070 )\n10 −3 ( 0.49 ±  0.04 )\n10 −3 ( 1.35 ±  0.14 )\n10 −3 ( 1.07 ±  0.13 )\nA.E.\nFigure\nCharacter of 4\n𝟎. 𝟐𝟎𝟒 ±  𝟎. 𝟎𝟔𝟓\n0.045 ±  0.010\n0.127 ±  0.034\n0.034 ±  0.011\nIoU\n𝟏𝟎 −𝟑 ( 𝟏. 𝟔𝟑 ±  𝟎. 𝟏𝟎 )\n10 −3 ( 2.84 ±  0.06 )\n10 −3 ( 2.78 ±  0.18 )\n10 −3 ( 3.63 ±  0.18 )\nA.E.\nFigure\nCharacter of B\n0.025 ±  0.002\n0.022 ±  0.015\n0.035 ±  0.005\n0.013 ±  0.004\nIoU\n𝟏𝟎 −𝟑 ( 𝟑. 𝟑𝟏 ±  𝟎. 𝟑𝟗 )\n10 −3 ( 6.64 ±  1.32 )\n10 −3 ( 6.27 ±  0.28 )\n10 −3 ( 7.90 ±  0.39 )\nA.E.\nA.E. = Angular Error\nFig. 9. Comparison of Autonomous Performance: error-feedback model in Writing ’A,’ ’4,’ and ’B’\nthe upper-layer, it becomes possible to combine this approach\nwith methods such as direct teaching. To realize this, a lower-\nlayer capable of generating effective outputs from limited\ninformation is required. For example, by using generative\nmodels, such as Conditional Variational Autoencoders (CVAE)\n[15], it is expected that other physical quantities can be\ngenerated from angle information. These development can lead\nto further advancements in the field of robotics.\nREFERENCES\n[1] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Imitation learning for\nvision-based manipulation with object proposal priors,” in Proceedings\nof The 6th Conference on Robot Learning, ser. Proceedings of Machine\nLearning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol.\n205.\nPMLR, 14–18 Dec 2023, pp. 1199–1210. [Online]. Available:\nhttps://proceedings.mlr.press/v205/zhu23a.html\n[2] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile aloha: Learning bimanual\nmobile manipulation with low-cost whole-body teleoperation,” arXiv\npreprint arXiv:2401.02117, 2024.\n[3] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained\nbimanual manipulation with low-cost hardware,” 2023. [Online].\nAvailable: https://arxiv.org/abs/2304.13705\n[4] Y. Saigusa, S. Sakaino, and T. Tsuji, “Imitation learning for nonprehen-\nsile manipulation through self-supervised learning considering motion\nspeed,” IEEE Access, vol. 10, pp. 68 291–68 306, 2022.\n[5] T. Akagawa and S. Sakaino, “Autoregressive model considering low\nfrequency errors in command for bilateral control-based imitation learn-\ning,” IEEJ Journal of Industry Applications, vol. 12, no. 1, pp. 26–32,\n2023.\n[6] K. Yamane, Y. Saigusa, S. Sakaino, and T. Tsuji, “Soft and rigid object\ngrasping with cross-structure hand using bilateral control-based imitation\nlearning,” IEEE Robotics and Automation Letters, vol. 9, no. 2, pp.\n1198–1205, 2024.\n[7] T. Buamanee, M. Kobayashi, Y. Uranishi, and H. Takemura, “Bi-act:\nBilateral control-based imitation learning via action chunking with\ntransformer,” 2024. [Online]. Available: https://arxiv.org/abs/2401.17698\n[8] K. Hayashi, S. Sakaino, and T. Tsuji, “An independently learnable hierar-\nchical model for bilateral control-based imitation learning applications,”\nIEEE Access, vol. 10, pp. 32 766–32 781, 2022.\n[9] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, “Embed\nto control: A locally linear latent dynamics model for control from\nraw images,” in Advances in Neural Information Processing Systems,\nC. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds.,\nvol. 28.\nCurran Associates, Inc., 2015.\n[10] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy\nevolution,” in Advances in Neural Information Processing Systems 31.\nCurran Associates, Inc., 2018, pp. 2451–2463.\n[11] M. Jaques, M. Burke, and T. Hospedales, “Newtonianvae: Proportional\ncontrol and goal identification from pixels via physical latent spaces,”\nin 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2021, pp. 4452–4461.\n[12] H. Ichiwara, H. Ito, K. Yamamoto, H. Mori, and T. Ogata, “Modality\nattention for prediction-based robot motion generation: Improving inter-\npretability and robustness of using multi-modality,” IEEE Robotics and\nAutomation Letters, vol. 8, no. 12, pp. 8271–8278, 2023.\n[13] T. Murakami, F. Yu, and K. Ohnishi, “Torque sensorless control in\nmultidegree-of-freedom manipulator,” IEEE Transactions on Industrial\nElectronics, vol. 40, no. 2, pp. 259–265, 1993.\n[14] R. Rahmatizadeh, P. Abolghasemi, A. Behal, and L. B¨ol¨oni, “From\nvirtual demonstration to real-world manipulation using lstm and mdn,”\nin Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32,\nno. 1, 2018.\n[15] D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling,\n“Semi-supervised learning with deep generative models,” in Advances\nin Neural Information Processing Systems, Z. Ghahramani, M. Welling,\nC. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27.\nCurran\nAssociates, Inc., 2014.",
    "pdf_filename": "Error-Feedback_Model_for_Output_Correction_in_Bilateral_Control-Based_Imitation_Learning.pdf"
}