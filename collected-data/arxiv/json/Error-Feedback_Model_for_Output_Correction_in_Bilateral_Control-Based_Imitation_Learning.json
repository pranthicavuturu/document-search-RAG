{
    "title": "Error-Feedback Model for Output Correction in",
    "abstract": "networks has enabled robots to perform flexible tasks. However, sensations. The use of force-based imitation learning shows since neural networks operate in a feedforward structure, they promise for replacing human tasks with robots. However, in do not possess a mechanism to compensate for output errors. conventionalbilateralcontrol-basedimitationlearning,NNhas To address this limitation, we developed a feedback mechanism to correct these errors. By employing a hierarchical structure a feedforward structure and does not control output errors, for neural networks comprising lower and upper layers, the as shown in Fig. 1. Hence, errors during the autonomous lowerlayerwascontrolledtofollowtheupperlayer.Additionally, operation of the NN are not compensated. This issue is usingamulti-layerperceptroninthelowerlayer,whichlacksan observed not only in bilateral control-based imitation learning internal state, enhanced the error feedback. In the character- but also in many NN-based imitation learning approaches. writing task, this model demonstrated improved accuracy in writingpreviouslyuntrainedcharacters.Inthecharacter-writing Traditionally, NNs have required an internal state to retain task, this model demonstrated improved accuracy in writing memory for handling time-series data. However, this NN previously untrained characters. Through autonomous control struggles to integrate with controllers due to the significant with error feedback, we confirmed that the lower layer could influence of the internal state. This suggests that the system’s effectively track the output of the upper layer. This study non-Markovian nature complicates NN control. Generally, represents a promising step toward integrating neural networks with control theories. systems are more likely to exhibit Markovian properties when IndexTerms—imitationlearning,deeplearning,feedbackcon- the sampling period is shortened. Therefore, to realize effec- trol tive NN control, it is essential to establish a structure with independent components for the short-sampling-period NN, I. INTRODUCTION which exhibits Markovian properties and is easier to control, and the long-sampling-period NN, which has non-Markovian In recent years, imitation learning has gained significant properties and enables complex time-series inference. attention for enabling robots to perform complex actions [1] In this study, we developed a control system for a hierar- [2] [3]. Imitation learning is a type of supervised learning chicalmodelwithdifferentsamplingperiods.Thehierarchical in which neural networks (NNs) learn from human demon- model comprises an upper layer that makes long-term predic- strations. Furthermore, research on imitation learning using tionsandalowerlayerthatmakesshort-termpredictions.The position and force information has advanced. Specifically, bi- upper layer is a strong non-Markovian system that predicts lateralcontrol-basedimitationlearninghasproveneffectivein actionplansbasedonpastinformation.Conversely,thelower- reproducing human force application [4] [5] [6] [7]. Bilateral layer is a strong Markovian system that predicts command controlisateleoperationtechnologythatusestworobots:one values and states with a short sampling period. This type of interactswiththeenvironment,whiletheotherisoperatedbya modelhasbeenproposedinpreviousresearch[8],demonstrat- humanapplyingforce.Bycollectingdatawiththistechnology, ingitseffectivenessforlong-termtasks.However,priorstudies both position and force response and command values can employed Long Short-Term Memories (LSTMs) with internal This work was supported by JSPS KAKENHI Grant Number 24K00905, states for the Markovian lower-layer. Therefore, we employed JST,PRESTOGrantNumberJPMJPR24T3JapanandJSTALCA-NextJapan, a multilayer perceptron (MLP), which lacks an internal state, Grant Number JPMJAN24F1. This study was based on the results obtained toconstructacontrolsystemfortheoutput.Duringthecontrol from the JPNP20004 project subsidized by the New Energy and Industrial TechnologyDevelopmentOrganization(NEDO). process, the error in the robot’s state predicted by the upper 4202 voN 91 ]OR.sc[ 1v55221.1142:viXra",
    "body": "Error-Feedback Model for Output Correction in\nBilateral Control-Based Imitation Learning\n1st Hiroshi Sato 2th Masashi Konosu\nIntelligent and Mechanical Interaction Systems Intelligent and Mechanical Interaction Systems\nUniversity of Tsukuba University of Tsukuba\nTsukuba, Japan Tsukuba, Japan\nsato.hiroshi.tkb cu@u.tsukuba.ac.jp konosu.masashi.qa@alumni.tsukuba.ac.jp\n3nd Sho Sakaino 4rd Toshiaki Tsuji\nSystems and Information Engineering Science and Engineering\nUniversity of Tsukuba Saitama University\nTsukuba, Japan Saitama, Japan\nsakaino@iit.tsukuba.ac.jp tsuji@ees.saitama-u.ac.jp\nAbstract—In recent years, imitation learning using neural be obtained, allowing robots to replicate human operational\nnetworks has enabled robots to perform flexible tasks. However, sensations. The use of force-based imitation learning shows\nsince neural networks operate in a feedforward structure, they\npromise for replacing human tasks with robots. However, in\ndo not possess a mechanism to compensate for output errors.\nconventionalbilateralcontrol-basedimitationlearning,NNhas\nTo address this limitation, we developed a feedback mechanism\nto correct these errors. By employing a hierarchical structure a feedforward structure and does not control output errors,\nfor neural networks comprising lower and upper layers, the as shown in Fig. 1. Hence, errors during the autonomous\nlowerlayerwascontrolledtofollowtheupperlayer.Additionally, operation of the NN are not compensated. This issue is\nusingamulti-layerperceptroninthelowerlayer,whichlacksan\nobserved not only in bilateral control-based imitation learning\ninternal state, enhanced the error feedback. In the character-\nbut also in many NN-based imitation learning approaches.\nwriting task, this model demonstrated improved accuracy in\nwritingpreviouslyuntrainedcharacters.Inthecharacter-writing Traditionally, NNs have required an internal state to retain\ntask, this model demonstrated improved accuracy in writing memory for handling time-series data. However, this NN\npreviously untrained characters. Through autonomous control struggles to integrate with controllers due to the significant\nwith error feedback, we confirmed that the lower layer could\ninfluence of the internal state. This suggests that the system’s\neffectively track the output of the upper layer. This study\nnon-Markovian nature complicates NN control. Generally,\nrepresents a promising step toward integrating neural networks\nwith control theories. systems are more likely to exhibit Markovian properties when\nIndexTerms—imitationlearning,deeplearning,feedbackcon- the sampling period is shortened. Therefore, to realize effec-\ntrol tive NN control, it is essential to establish a structure with\nindependent components for the short-sampling-period NN,\nI. INTRODUCTION which exhibits Markovian properties and is easier to control,\nand the long-sampling-period NN, which has non-Markovian\nIn recent years, imitation learning has gained significant\nproperties and enables complex time-series inference.\nattention for enabling robots to perform complex actions [1]\nIn this study, we developed a control system for a hierar-\n[2] [3]. Imitation learning is a type of supervised learning\nchicalmodelwithdifferentsamplingperiods.Thehierarchical\nin which neural networks (NNs) learn from human demon-\nmodel comprises an upper layer that makes long-term predic-\nstrations. Furthermore, research on imitation learning using\ntionsandalowerlayerthatmakesshort-termpredictions.The\nposition and force information has advanced. Specifically, bi-\nupper layer is a strong non-Markovian system that predicts\nlateralcontrol-basedimitationlearninghasproveneffectivein\nactionplansbasedonpastinformation.Conversely,thelower-\nreproducing human force application [4] [5] [6] [7]. Bilateral\nlayer is a strong Markovian system that predicts command\ncontrolisateleoperationtechnologythatusestworobots:one\nvalues and states with a short sampling period. This type of\ninteractswiththeenvironment,whiletheotherisoperatedbya\nmodelhasbeenproposedinpreviousresearch[8],demonstrat-\nhumanapplyingforce.Bycollectingdatawiththistechnology,\ningitseffectivenessforlong-termtasks.However,priorstudies\nboth position and force response and command values can\nemployed Long Short-Term Memories (LSTMs) with internal\nThis work was supported by JSPS KAKENHI Grant Number 24K00905, states for the Markovian lower-layer. Therefore, we employed\nJST,PRESTOGrantNumberJPMJPR24T3JapanandJSTALCA-NextJapan, a multilayer perceptron (MLP), which lacks an internal state,\nGrant Number JPMJAN24F1. This study was based on the results obtained\ntoconstructacontrolsystemfortheoutput.Duringthecontrol\nfrom the JPNP20004 project subsidized by the New Energy and Industrial\nTechnologyDevelopmentOrganization(NEDO). process, the error in the robot’s state predicted by the upper\n4202\nvoN\n91\n]OR.sc[\n1v55221.1142:viXra\nLeader’s Follower’s Upper Layer\nresponse Neural response\nNetwork\nLower\nFollower’s Layer\nController Neural Network\nFig.1. OverviewoftheautonomousmotionusingNN\nFig.2. HierarchicalmodelproposedbyHayashietal.[8]\nandlowerlayerswasfedbackintotheinputofthelowerlayer.\nThisallowsthelowerlayertoadjustitsinferencetominimize C. Hierarchical Model\nthe error relative to the upper layer’s predictions. We refer to\nA hierarchical model processes information at different\nthis model as the error feedback model.\nlevels of abstraction in each-layer, breaking down complex\nThe effectiveness of the proposed method was validated\ntasks into manageable sub-tasks. [12] [8]. Hayashi et al.\nthrough a character-writing task involving both learned and\nproposed a hierarchical model for bilateral control-based imi-\nunlearned characters. Evaluation was based on the accuracy\ntation learning [8]. The proposed hierarchical model is shown\nof the drawn characters and the trajectory of angles. For\nin Fig. 2. Here, f and l represent the follower and leader,\ncomparison, the lower layer was implemented using both\nrespectively, and the subscript t indicates the operational step\nLSTMandMLP.Itisimportanttonotethatthisstudyexplores\nof the NN. The upper-layer infers the state 10 steps ahead\nthe potential of the error-feedback model for the lower layer, fupper and provides it to the lower-layer. Conversely, the\nassuming that the state predicted by the upper layer is already k+10\nlower-layer considers the follower’s current state f and state\nk\nknown. This approach is expected to lead to the development fupper provided by the upper-layer as inputs. It then predicts\nof a new model that combines NNs with control systems. thk e+ f1 o0 llower’s next state fˆ and leader’s next state ˆl .\nk+1 k+1\nII. RELATEDWORKS The hierarchical model has been shown to be effective\nfor long-term tasks. Additionally, it has been confirmed that\nA. World Model with Control System\nthe model can accomplish tasks even when the lower layer\nIntegrating control with NNs has been extensively studied receivesunlearnedinformationfromtheupperlayer.However,\nusingworldmodels[9][10][11].AworldmodelisanNNthat without a control mechanism for the lower layer to follow\nlearns the structure of the environment from observation data, the upper layer, there is concern about a decrease in task\nrepresentingitinalatentspace.Byincorporatingamechanism performance accuracy.\nto control errors in this latent space, it becomes possible to\ncombine NNs with control. However, these methods assume\nIII. PROPOSEDMETHOD\nconstantdynamics,whichposeschallengesfortasksinvolving A. Separation of the Markovian and non-Markovian proper-\ncontact or multiple actions, as dynamic changes over time ties of tasks\ncomplicate mapping to the latent space. Inthisstudy,ahierarchicalmodelwasemployedtoseparate\nthe Markovian and non-Markovian aspects of the system. The\nB. Bilateral Control-Based Imitation Learning\nhierarchical model is the same as those used in previous\nBilateralcontrol-basedimitationlearningusesbilateralcon- research [8]. The upper-layer infers task plans over a long\ntrol during the data collection phase. Bilateral control is a period, handling the non-Markovian properties of the system.\nteleoperation technique that synchronizes the positions and In contrast, the lower-layer performs short-period inference of\nforces of two robots: a leader and a follower. The leader commands and states based on the current state and a few\nreceives forces from a human operator, while the follower steps ahead of the follower’s state. Given that it performs\ninteracts with the environment. Using this control method to short-periodinferenceandinvolvespredictionsthatinterpolate\nperform tasks, the response values of both the leader and between different steps, the lower-layer system is considered\nfollower are collected. The leader’s response value serves as toexhibithighMarkovianproperties.Basedonthisreasoning,\nthe command for the follower. As a result, the response and control was constructed for the Markovian aspects of this\ncommand values of the follower can be collected separately. hierarchical model.\nAn NN is then trained to predict the next command value of\nB. hierarchical model with error-feedback mechanism\nthefollowerbasedonitscurrentresponsevalue.Oncetrained,\ntheNNenablesautonomousmovementsthatreplicatebilateral Inthisstudy,weproposeanerrorfeedbackmodel,asshown\ncontrol,allowingtheexecutionoftasksrequiringforcecontrol. in Fig. 3. This model is designed to control the system to\nNeural Network\nUpper Layer\nInput Output\nCalculate\nError\nLower hidden layer = 200 neurons\nLayer\nNeural Network Fig.4. LSTMmodel\nNeural Network\nFig.3. Proposederror-feedbackmodel Input Output\nreducetheerrorbetweentheoutputs.Intheproposedmethod,\nthe upper-layer generates the state one step ahead fupper and\nstatetenstepsaheadfupper.Additionally,theerrorbetk w+ e1\nenthe\nhidden layer = 200 neurons\nk+10\noutput of the lower-layer fˆ and upper-layer output fupper\nk+1 k+1 Fig.5. MLPmodel\nwas calculated. Here, the error calculation is defined simply\nas the difference fupper − fˆ . Extending this to various\nk+1 k+1\ncontrolmechanismsisafuturework,butthesimpledifference IV. EXPERIMENTMETHOD\nworked well in this study. Subsequently, the information from\nthe upper-layer given to the lower-layer, fupper, is defined as A. Manipulator\nfollows: In this study, CRANE-X7, manufactured by RT Corpora-\n(cid:16) (cid:17)\nfupper =fupper + fupper−fˆ (1) tion, was employed. The manipulator has seven degrees of\nk+10 k+1 k+1\nfreedom, and the gripper has one degree of freedom. The\nBy adding the error to the upper-layer output, it is expected gripper was replaced by a cross-structured hand [6]. Given\nthat the lower-layer will generate an output that has been that controlling a manipulator with seven degrees of freedom\ncorrected for errors. canbechallengingforhumans,joint2wasfixedusingposition\nIt should be noted that fupper is updated every ten steps, control, effectively reducing the system to a six degrees\nk+10\nwhile fupper is updated at each time step. This was stan- of freedom manipulator. Each axis of the manipulator was\nk+1\ndardized to enable comparison with previous research [8]. controlled by a position and force hybrid controller, with a\nAdditionally, this mechanism was applied during the robot’s controlperiodof500Hz[4].Thejointanglesθweremeasured\nautonomous operation. Therefore, the learning process does by rotary encoders at each joint, and the angular velocities θ˙\nnot include an error feedback mechanism, similar to conven- were calculated by its pseudodifferential. The torques τ were\ntional hierarchical models. estimated using a reaction force estimation observer [13].\nC. NN Design of the lower-layer B. Verification of autonomous operation\nTo address the Markovian properties of the system, the In the experiment, a writing task was conducted using the\nlower-layer is designed as a simple multi-layer perceptron robot. The robot wrote the characters while holding the pen\n(MLP) without internal states, as illustrated in Fig. 5. The from the beginning, as shown in Fig. 6. The characters drawn\nnetwork comprises four layers of fully connected layers with by the robot were captured by an Intel RealSense D435i\n200 dimensions using the Tanh function as the activation mounted on the top of the whiteboard.\nfunction for all layers except for the final one. Additionally, 1) Preliminary: As a preliminary experiment, we investi-\nfor comparison, the lower layer using the conventional long gated the amount of information required as the upper-layer\nshort-term memory (LSTM) architecture is also presented in output. Specifically, in Fig. 3, the upper layer outputs one of\nFig.4.TheLSTMnetworkiscomprisedofthreelayersof200- thefollowingthree:fupper =[θ], fupper =[θ,θ˙], fupper =\nk+10 k+10 k+10\ndimensionalLSTMunitsandafullyconnectedlayer,resulting [θ,θ˙,τ]. The lower-layers were trained to write the character\nin a total of four layers. ’A’usingtwomodeltypes:LSTMandMLP.Withthelearned\nIn this study, the states predicted by the upper-layer are NNs, the robot performed the operation of writing ’A.’ In the\nassumed to be known. Specifically, time-series states of the autonomous operation, the upper-layer outputs, which were\nfollower are stored in advance through bilateral control, and collected in advance by the bilateral control, were used. The\nthisdataisutilized.Thisallowsforthecomparisonofdifferent optimal amount of information for the upper-layer outputs\nlower-layers using the same predictions from the upper-layer. were selected by the evaluation described below.\nLSTM\nFC\nLSTM\nFC\nLSTM\nFC\nFC\nFC\nunifiedbypadding,whichcopiesthelastvalue.Thedatawere\nnormalizedtomean0andstandarddeviation1duringtraining.\nMean Squared Error (MSE) was used as the loss function and\nAdam was used for optimization. The learning rate was set to\n0.0001, and the batch size to 16, and the number of epochs to\n1000.\nD. Evaluation Method\nEvaluation of autonomous movements was performed in\ntwo ways: assessing the diagrams drawn by the robot and\nevaluating the joint angles during autonomous movements.\nFig.6. Taskenvironment Fig. 7. Characters drawn by Autonomous movements were performed five times, and the\ntrainingdata\nmeans and standard deviations were calculated for both eval-\nuation methods. The outputs from the upper-layer in this\n2) Evaluation of error-feedback model: A comparison was study were collected using bilateral control. Therefore, the\nconducted between the conventional hierarchical model and upper-layer outputs of the drawn characters and joint angle\nproposed hierarchical model. The lower-layer models, which information were collected in advance.\nwere highly evaluated in the preliminary experiments, were 1) IoU: IntersectionoverUnion(IoU)wasusedtoevaluate\nutilized.Anexperimentwasconductedtowritethreedifferent theaccuracyofthediagrams.Thediagramsdrawnbytherobot\ntypes of characters: character ’A,’ ’4,’ and ’B.’ Specifically, were captured by a camera and binarized in black and white.\n’A’ is a learned character, while ’4’ and ’B’ are characters The IoU is calculated as follows:\nthat had not been learned. Bupper∩Boutput\nCharacter ’4’ was selected because it has a shape that IoU = (2)\nBupper∪Boutput\nis similar to ’A’ but somewhat different, while character\nwhereBupper representsthecharacterareadrawnintheupper-\n’B’ was chosen for its distinct shape compared to ’A.’ To\nlayeroutput,andBoutput denotesthecharacterareadrawnby\nenable the writing of these characters, the upper-layer outputs\nthe autonomous motion of NN. As this value approaches 1, it\nwere modified to correspond to each respective character. The\nindicatesahigherdegreeofmatchbetweenthetwocharacters.\nupper-layer outputs was determined based on the follower’s\nBy comparing the IoU values, we evaluated whether the\nstate,whichwaspreviouslycollectedthroughbilateralcontrol.\nautonomous control followed the upper-layer output.\nMeanwhile, the lower-layer NN remained unchanged from\n2) MSE of Angles: The joint angles of autonomous move-\nthe preliminary experiments. In conventional imitation learn-\nment were evaluated. The purpose of this study is to control\ning, the NN must be retrained for each new character. When\nthe lower-layer to approach the upper-layer outputs. In other\nthe upper-layer outputs for an unlearned character are used,\nwords, by calculating the error between the joint angles\nerrors are expected in the NN’s output. Therefore, we aimed\nobtained by autonomous movements and joint angles of the\nto verify whether the proposed method could suppress these\nupper-layer output, it is possible to evaluate the follow-up to\nerrorsandgeneratecommandvaluesthatalignwiththeupper-\nthe upper-layer outputs. We termed this as Angular Error and\nlayer outputs.\ncalculated it as follows:\nC. Training NN\nn\nWe collected training data and validation data by using AngularError =(cid:88) (θupper−θres) (3)\nk k\nbilateral control to write character ’A.’ The training data were\nk=0\ncollectedseventimes,fiveofwhichwereusedastrainingdata\nwhere k, k = 0, k = n, θupper, and θres denote specific\nand two as validation data. Fig. 7 is a diagram that is drawn\ntime, task start time, task end time, the angle of the upper-\nwhen the data is collected. This is the image of the characters\nlayer output, and the angle response value of the robot\non the whiteboard captured by a camera and superimposed\nduring autonomous operation. By comparing these values, we\nafter binarization.\nevaluated the tracking performance of the robot relative to the\nWhen training NN using time-series data, the learning\nupper-layer output.\nefficiency can be improved by reducing the sampling fre-\nquency. For this purpose, the joint information acquired at\nV. EXPERIMENT\n2-ms intervals was sampled every 10 steps by shifting the\nA. Preliminary\nstarting point, creating a data set with 20-ms intervals [14].\nThis process increased the amount of data by a factor of 10. The results of the preliminary experiments are shown in\nAdditionally, a 20 rad/s low-pass filter was applied to Fig.8.Theblacklineinthefigurerepresentsthedrawingmade\nthe teacher data to remove high-frequency components. The by autonomous actions, while the light red color indicates the\ninput data were augmented with normally distributed noise upper-layer output. Comparing the drawn characters with the\nwith a variance of 0.01. The length of each sequence was IoU, LSTM and MLP achieved the highest values when using\nLSTM showed a slight increase. Similarly, MLP reduced the\nLSTM MLP\nAngle Error, and LSTM showed a slight reduction. When\ncomparingtheperformanceofLSTMandMLP,MLPachieved\na larger IoU and a smaller Angle Error than LSTM. These\nresultsindicatethatusingMLPwiththeerror-feedbackmodel\nimproved task accuracy. Given that MLP does not have an\nIoU 0 0.104± 0.016 internal state, it made appropriate predictions based on error\nfeedback without being influenced by past memories.\nA.E. 10−3(11.76± 2.66) 10−3(2.01± 0.20)\n3) character B: The bottom part of the Fig. 9 presents\nthe results for the task of writing character ’B.’ In the error-\nfeedback model, no increase in IoU was observed for either\nLSTMorMLP.However,adecreaseinAngleErrorwasnoted.\nWhen comparing LSTM and MLP, MLP exhibited a smaller\nIoU 0.190± 0.068 0.173± 0.017 Angle Error than LSTM.\nThese results suggest that NN model faced difficulties in\nA.E. 10−3(1.37± 0.19) 10−3(1.48± 0.24)\nperformingthetaskofwritingthecharacter’B.’However,with\nthe error-feedback model using MLP, there was a significant\nreduction in Angle Error. This indicates that the followability\nto the upper-layer outputs improved. The low IoU values are\nthought to be due to the pen not making contact with the\nIoU 𝟎.𝟒𝟏𝟏± 𝟎.𝟎𝟗𝟔 𝟎.𝟑𝟐𝟓± 𝟎.𝟎𝟖𝟖 board, despite following the upper-layer outputs.\nThedifficultyinperformingthetaskofwritingthecharacter\nA.E. 𝟏𝟎−𝟑(𝟏.𝟎𝟕± 𝟎.𝟏𝟑) 𝟏𝟎−𝟑(𝟏.𝟑𝟓±𝟎.𝟏𝟒)\n’B’ may be attributed to the fact that the motion for ’B’\nA.E = Angular Error involved extrapolation beyond the learned data. In particular,\nthesecondstrokefallsoutsidetherangeofthemotionusedfor\nFig. 8. Performance comparison of autonomous robot operation based on\ndrawing’A.’Asaresult,thelower-layermaynothavelearned\ndifferencesinupper-layerinformation\nthe skills necessary for writing ’B,’ making it challenging to\nfollow the upper-layer output.\nfupper = [θ,θ˙,τ] as the upper-layer outputs. Additionally,\nthk e+ a1 n0\ngle error was lowest under the same conditions.\nVI. CONCLUSION\nIt is considered that this task required future force because In this study, we proposed an error-feedback model for a\nwriting characters involves applying force to a board. Based hierarchicalNNthataddressesoutputerrorsthroughfeedback.\non these results, fupper = [θ,θ˙,τ] was used as the upper- The method calculates output errors between upper and lower\nk+10\nlayer output to verify the effectiveness of the error-feedback layers and incorporates them into a lower-layer input. In\nmodel. the writing task experiments, the model accurately followed\nthe upper-layer output. Additionally, using an MLP in the\nB. Evaluation of error-feedback model\nlower-layer enhanced tracking performance and improved the\nTo verify the effectiveness of the proposed method, tasks accuracy of character generation. It is believed that these\nwere conducted to write several characters: ’A,’ ’4’ and ’B.’ resultsrepresentasignificantfirststeptowardintegratingNNs\nTheresultswiththeerror-feedbackmodelaredescribedas’w/ with control theory.\nfeedback.’\n1) characterA: TheupperpartofFig.9presentstheresults\nVII. FUTUREWORKS\nfor the task of writing character ’A.’ Using the error-feedback The next step is to develop an lower-layer that can more\nmodel, no increase in IoU was observed for either LSTM or accurately follow the upper-layer output. In this study, the\nMLP, though a decrease in Angle Error was confirmed. In lower-layer learned to write only character ’A.’ To enable the\nthe error-feedback model, there was no significant difference lower-layertomoreflexiblyfollowtheupper-layeroutput,itis\nbetween LSTM and MLP, but the LSTM model showed necessary for lower layer to learn a wider range of behaviors.\nslightly higher IoU results. Overall, it can be concluded that Thus, it is essential to verify whether teaching a variety of\nthe conventional LSTM provides sufficient performance in behaviors to the lower-layer improves their ability to follow\nwriting character ’A,’ as the lower-layer have learned these the upper-layer output.\ncharacters. The observed decrease in Angle Error, while not Additionally, the next challenge is the extension to the\naffecting the IoU, suggests successful approximation of the lower-layer, where only the trajectory is provided from the\nupper-layer output. upper-layer. In this study, the outputs of the upper-layer were\n2) character 4: The middle part of the Fig. 9 presents angle, angular velocity, and torque. However, in many cases,\nthe results for the task of writing character ’4.’ In the error- it is difficult to obtain all three of these physical quantities.\nfeedback model, MLP showed an increase in IoU, while If the lower-layer only require the angle information from\n=\n[\n]\n=\n[\n]\n=\n[\n]\nUpper-layer\noutput\nUpper-layer\noutput\nUpper-layer\noutput\nFigure\nFigure\nFigure\nLSTM MLP LSTM w/ feedback MLP w/ feedback\nIoU 0.411 ± 0.096 0.325 ± 0.088 0.366 ± 0.062 0.355 ± 0.008\nA.E. 10 −3 ( 1.07 ± 0.13 ) 10 −3 ( 1.35 ± 0.14 ) 10 −3 ( 0.49 ± 0.04 ) 10 −3 ( 0.49 ± 0.070 )\nIoU 0.034 ± 0.011 0.127 ± 0.034 0.045 ± 0.010 𝟎.𝟐𝟎𝟒 ± 𝟎.𝟎𝟔𝟓\nA.E. 10 −3 ( 3.63 ± 0.18 ) 10 −3 ( 2.78 ± 0.18 ) 10 −3 ( 2.84 ± 0.06 ) 𝟏𝟎 −𝟑 ( 𝟏.𝟔𝟑 ± 𝟎.𝟏𝟎 )\nIoU 0.013 ± 0.004 0.035 ± 0.005 0.022 ± 0.015 0.025 ± 0.002\nA.E. 10 −3 ( 7.90 ± 0.39 ) 10 −3 ( 6.27 ± 0.28 ) 10 −3 ( 6.64 ± 1.32 ) 𝟏𝟎 −𝟑 ( 𝟑.𝟑𝟏 ± 𝟎.𝟑𝟗 )\nA.E. = Angular Error\nFig.9. ComparisonofAutonomousPerformance:error-feedbackmodelinWriting’A,’’4,’and’B’\nthe upper-layer, it becomes possible to combine this approach [7] T. Buamanee, M. Kobayashi, Y. Uranishi, and H. Takemura, “Bi-act:\nwith methods such as direct teaching. To realize this, a lower- Bilateral control-based imitation learning via action chunking with\ntransformer,”2024.[Online].Available:https://arxiv.org/abs/2401.17698\nlayer capable of generating effective outputs from limited\n[8] K.Hayashi,S.Sakaino,andT.Tsuji,“Anindependentlylearnablehierar-\ninformation is required. For example, by using generative chicalmodelforbilateralcontrol-basedimitationlearningapplications,”\nmodels,suchasConditionalVariationalAutoencoders(CVAE) IEEEAccess,vol.10,pp.32766–32781,2022.\n[9] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, “Embed\n[15], it is expected that other physical quantities can be\nto control: A locally linear latent dynamics model for control from\ngeneratedfromangleinformation.Thesedevelopmentcanlead raw images,” in Advances in Neural Information Processing Systems,\nto further advancements in the field of robotics. C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds.,\nvol.28. CurranAssociates,Inc.,2015.\nREFERENCES [10] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy\nevolution,” in Advances in Neural Information Processing Systems 31.\n[1] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Imitation learning for CurranAssociates,Inc.,2018,pp.2451–2463.\nvision-basedmanipulationwithobjectproposalpriors,”inProceedings [11] M.Jaques,M.Burke,andT.Hospedales,“Newtonianvae:Proportional\nofThe6thConferenceonRobotLearning,ser.ProceedingsofMachine control and goal identification from pixels via physical latent spaces,”\nLearning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. in2021IEEE/CVFConferenceonComputerVisionandPatternRecog-\n205. PMLR, 14–18 Dec 2023, pp. 1199–1210. [Online]. Available: nition(CVPR),2021,pp.4452–4461.\nhttps://proceedings.mlr.press/v205/zhu23a.html [12] H. Ichiwara, H. Ito, K. Yamamoto, H. Mori, and T. Ogata, “Modality\n[2] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile aloha: Learning bimanual attentionforprediction-basedrobotmotiongeneration:Improvinginter-\nmobile manipulation with low-cost whole-body teleoperation,” arXiv pretabilityandrobustnessofusingmulti-modality,”IEEERoboticsand\npreprintarXiv:2401.02117,2024. AutomationLetters,vol.8,no.12,pp.8271–8278,2023.\n[3] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained [13] T. Murakami, F. Yu, and K. Ohnishi, “Torque sensorless control in\nbimanual manipulation with low-cost hardware,” 2023. [Online]. multidegree-of-freedom manipulator,” IEEE Transactions on Industrial\nAvailable:https://arxiv.org/abs/2304.13705 Electronics,vol.40,no.2,pp.259–265,1993.\n[4] Y.Saigusa,S.Sakaino,andT.Tsuji,“Imitationlearningfornonprehen- [14] R. Rahmatizadeh, P. Abolghasemi, A. Behal, and L. Bo¨lo¨ni, “From\nsile manipulation through self-supervised learning considering motion virtualdemonstrationtoreal-worldmanipulationusinglstmandmdn,”\nspeed,”IEEEAccess,vol.10,pp.68291–68306,2022. inProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.32,\n[5] T. Akagawa and S. Sakaino, “Autoregressive model considering low no.1,2018.\nfrequencyerrorsincommandforbilateralcontrol-basedimitationlearn- [15] D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling,\ning,”IEEJJournalofIndustryApplications,vol.12,no.1,pp.26–32, “Semi-supervised learning with deep generative models,” in Advances\n2023. inNeuralInformationProcessingSystems,Z.Ghahramani,M.Welling,\n[6] K.Yamane,Y.Saigusa,S.Sakaino,andT.Tsuji,“Softandrigidobject C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27. Curran\ngraspingwithcross-structurehandusingbilateralcontrol-basedimitation Associates,Inc.,2014.\nlearning,” IEEE Robotics and Automation Letters, vol. 9, no. 2, pp.\n1198–1205,2024.\nCharacter\nof\nA\nCharacter\nof\n4\nCharacter\nof\nB\nFigure\nFigure\nFigure",
    "pdf_filename": "Error-Feedback_Model_for_Output_Correction_in_Bilateral_Control-Based_Imitation_Learning.pdf"
}