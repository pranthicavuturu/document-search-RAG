{
    "title": "SSEditor Controllable Mask-to-Scene Generation with Diffusion Model",
    "abstract": "Recent advancements in 3D diffusion-based semantic scene generation have gained attention. However, existing meth- ods rely on unconditional generation and require multi- ple resampling steps when editing scenes, which signifi- cantly limits their controllability and flexibility. To this end, we propose SSEditor, a controllable Semantic Scene Ed- itor that can generate specified target categories without multiple-step resampling. SSEditor employs a two-stage diffusion-based framework: (1) a 3D scene autoencoder is trained to obtain latent triplane features, and (2) a mask- conditional diffusion model is trained for customizable 3D semantic scene generation. In the second stage, we in- troduce a geometric-semantic fusion module that enhance the model’s ability to learn geometric and semantic infor- mation. This ensures that objects are generated with cor- rect positions, sizes, and categories. Extensive experiments on SemanticKITTI and CarlaSC demonstrate that SSEditor outperforms previous approaches in terms of controllability and flexibility in target generation, as well as the quality of semantic scene generation and reconstruction. More impor- tantly, experiments on the unseen Occ-3D Waymo dataset show that SSEditor is capable of generating novel urban scenes, enabling the rapid construction of 3D scenes.",
    "body": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model\nHaowen Zheng, YanyanLiang\nMacau University of Science and Technology\nzhengnayin@gmail.com, yyliang@must.edu.mo\nCreate Background\nAdd Objects\nGround Truth\n(a) Generate a scene from scratch\n(b) Eliminate trailing artifacts\nAfter Editing\nBackground Masks\n…\nObject Masks\n…\n(c) Widen a road\nRoad \nMasks\nEditing\nCar \nMasks\nEditing\n(d) Controllable scene  \noutpainting\nScene Mask 1\n…\n…\nScene Mask n\n…\n…\nConcatenate\n…\nFigure 1. Controllable 3D semantic scene generation by SSEditor. The proposed SSEditor enables users to customize the generation or\nediting of 3D scenes using pre-built mask assets: (a) create a background scene and generate objects on it; (b) eliminate trailing artifacts\nof dynamic objects in SemanticKITTI [2]; (c) modify roads, such as expanding a two-lane road to a four-lane road; (d) concatenate masks\nfrom various scenes to produce a larger-scale 3D scene.\nAbstract\nRecent advancements in 3D diffusion-based semantic scene\ngeneration have gained attention. However, existing meth-\nods rely on unconditional generation and require multi-\nple resampling steps when editing scenes, which signifi-\ncantly limits their controllability and flexibility. To this end,\nwe propose SSEditor, a controllable Semantic Scene Ed-\nitor that can generate specified target categories without\nmultiple-step resampling.\nSSEditor employs a two-stage\ndiffusion-based framework: (1) a 3D scene autoencoder is\ntrained to obtain latent triplane features, and (2) a mask-\nconditional diffusion model is trained for customizable 3D\nsemantic scene generation.\nIn the second stage, we in-\ntroduce a geometric-semantic fusion module that enhance\nthe model’s ability to learn geometric and semantic infor-\nmation. This ensures that objects are generated with cor-\nrect positions, sizes, and categories. Extensive experiments\non SemanticKITTI and CarlaSC demonstrate that SSEditor\noutperforms previous approaches in terms of controllability\nand flexibility in target generation, as well as the quality of\nsemantic scene generation and reconstruction. More impor-\ntantly, experiments on the unseen Occ-3D Waymo dataset\nshow that SSEditor is capable of generating novel urban\nscenes, enabling the rapid construction of 3D scenes.\n1. Introduction\nIn recent years, 3D diffusion models have made notable\nachievements in generating both indoor [13, 32, 40] and\noutdoor [15, 16, 19, 26, 35] environments, as well as a sin-\ngle object [14, 31, 43]. Compared to indoor scenes and\nindividual objects, outdoor scenes present more challenges\ndue to their sparser and more complex representations. For\n1\narXiv:2411.12290v1  [cs.CV]  19 Nov 2024\n\ninstance, voxel-based representations of outdoor environ-\nments often contain a significant number of empty vox-\nels. Moreover, outdoor environments contain smaller tar-\ngets, such as pedestrians and cyclists, further complicating\nthe generation process. While voxel-based representations\n[15, 19, 26, 35] provide a straightforward approach to mod-\neling 3D semantic scenes, they suffer from redundancy in\nempty regions and high computational cost. To mitigate\nthese issues, the triplane representation [5] is utilized to re-\nduce unnecessary information in 3D outdoor scenes [16].\nAlthough these methods have shown promising results, they\nstill face several limitations.\nThe primary limitation lies in their weak controllabil-\nity. Unconditional generation restricts the ability to guide\nthe creation of 3D scenes, while conditioning on the en-\ntire scene (e.g., scene refinement based on ground truth) is\noverly rigid. This lack of flexible control leads to another\ndrawback: editing specific local regions, such as adding\nor removing objects, necessitates masking non-target areas\nand employing a multi-step resampling process for repaint-\ning [20]. It significantly increases generation time. Despite\nthe use of this resampling strategy, repainting remains un-\ncontrollable and often fails to produce the desired results.\nTo address the aforementioned challenges, we propose\nSSEditor, a flexible and controllable two-stage framework\nfor semantic scene generation based on the latent diffusion\nmodel (LDM) [28]. In the first stage, we train a 3D scene\nautoencoder to learn triplane features via semantic scene\nreconstruction. In the second stage, we train a mask con-\nditional diffusion model on the triplane features. Specifi-\ncally, to enable the customizable generation of 3D semantic\nscenes, we present a Geometric-Semantic Fusion Module\n(GSFM), which consists of a geometric branch and a se-\nmantic branch. The geometric branch encodes 3D masks\nthat represent an object’s position, size, and orientation,\nwhile the semantic branch processes semantic labels and\ntokens for providing coarse and fine-grained semantic in-\nformation.\nThe semantic tokens are generated from the\nfeatures of a specific category.\nThese features are then\naggregated and integrated into the cross-attention module\nof the diffusion model, enhancing its perception of both\ngeometric and semantic information. Benefiting from the\nabove design, SSEditor effectively accomplishes the mask-\nto-semantic scene generation task.\nIn addition, we create a 3D mask asset library encom-\npassing various categories to facilitate custom scene gen-\neration during inference. The 3D masks in the library are\nstored in the form of trimasks, which are composed of three\northogonal 2D planes derived from the decomposition of the\n3D mask. As shown in Fig. 1, users can choose from a range\nof assets, such as cross-shaped roads, vehicles, pedestrians,\nand cyclists, to generate their desired 3D semantic scenes.\nThe assets can also be edited to simulate more urban sce-\nnarios, such as expanding a two-lane road to four or more\nlanes.\nOur contributions can be summarized into three points:\n• We propose SSEditor, a controllable mask-to-scene gen-\neration framework that enables users to easily customize\nand generate 3D semantic scenes using various assets.\n• We propose GSFM to integrate geometric and semantic\ninformation. In GSFM, the geometric branch encodes 3D\nmasks as embeddings to accurately control the position,\nsize, and orientation of objects, while the semantic branch\nprocesses semantic labels and tokens for improved class\ncontrol of the generated targets.\n• Experiments on outdoor datasets demonstrate that our\nproposed method achieves superior generation quality\nand reconstruction performance.\nFurthermore, qualita-\ntive results indicate that SSEditor can controllably per-\nform various downstream tasks, such as scene inpainting,\nresource expansion, novel urban scene generation, and re-\nmoval of trailing artifacts.\n2. Related Work\nControllable Diffusion Models. Denoising diffusion prob-\nabilistic models (DDPM) [11] inspires a series of diffusion-\nbased controllable generation approaches. Text-guided im-\nage generation shows strong capabilities in image editing\ntasks, such as inpainting [1, 24, 25] and outpainting [29]. In\naddition, several studies incorporate more control signals,\nsuch as layouts [42], semantic maps [8, 28, 36, 41], to fa-\ncilitate image generation. Building on these advancements,\ncontrollable diffusion models have been further extended to\nthe 3D domain. These models can leverage images [6, 39],\ntext [17, 21], partial point clouds [23] or multi-modal con-\nditions (e.g., text-image or text-voxels) [22, 34] to guide the\ngeneration of a single 3D object. However, the aforemen-\ntioned controllable generative models can only be applied to\n2D images or individual 3D objects, making it challenging\nfor them to handle complex large-scale 3D scenes.\n3D Semantic Scene Generation. 3D semantic scene gen-\neration can be categorized into indoor and outdoor scene\ngeneration. CommonScenes [40] generates indoor scenes\nbased on scene graphs. DiffuScene [32] performs indoor\nscene generation and completion based on a text prompt or\nincomplete 3D targets. InstructScene [18] incorporates user\ninstructions into semantic graph priors and decodes them\ninto 3D indoor scenes. Build-A-Scene [7] enables users\nto flexibly create indoor scenes by adjusting layouts. In\ncontrast, outdoor scene generation is more complex, which\nfeatures diverse objects, more occlusions, and varying dis-\ntances. [15] generates 3D multi-object scenes in simulated\noutdoor environments, while PDD [19] employs a coarse-\nto-fine strategy to further improve generation quality. For\nmore complex real-world outdoor scenes, SemCity [16]\nuses triplane diffusion to achieve unconditional generation\n2\n\nSemantic Encoder\n…\nSemantic \nLabel\n…\nSemantic \nToken\n…\nSemantic Embeddings\nSemantic Branch\nAvg. \nPooling\nQuery based on \ncoordinate\nEncoder\n…\nquery points 𝑝\n…\nDecoder\nScene 𝒚\n𝒛\nTriplane 𝓣\nPE(𝒑)\n…\nPredicted points 𝒑ෝ\nReconstructed Scene 𝒚ෝ\n(a) Learning Triplane Representation via 3D Scene Autoencoder\nAssets\n…\nRoad \nTrimask\nVegetation \nTrimask\nCar \nTrimask\nPerson \nTrimask\n…\n…\nMask Embeddings\nGeometric Encoder\nGeometric Branch\nSemantic\nBranch\nGeometric\nBranch\nSelf Attention\nAdd & Norm\nDenoising UNet\nTrimask 𝓜\nCA\nCA\nCA\nCA\n𝓣𝒕\n𝓣𝒕ି𝟏\nTrimask\nGeometric Semantic\nFusion Module\n𝓣𝒕\n𝓣𝟎\nDiffusion\nProcess\n(b) Controllable Mask-to-Semantic\nScene Generation\nConcat\nSkip\nConnection\nCross\nAttention\nCA\nCross Attention\nAdd & Norm\nFigure 2. Illustration of our SSEditor framework. It comprises two main processes: (a) a 3D autoencoder learns the triplane representation\nvia scene reconstruction, and (b) controllable semantic scene generation is achieved through masks, semantic labels, and tokens. The\nGeometric-Semantic Fusion Module is essential for the diffusion model to effectively learn both geometric and semantic information.\nor conditional 3D occupancy refinement.\nDue to the significant differences between indoor and\noutdoor environments, these controllable indoor scene gen-\neration methods [7, 18, 32] are difficult to apply to outdoor\nscenes. For outdoor environments, [16, 19] can only re-\nfine scenes by conditionally inputting the entire 3D layout.\nMoreover, when conducting scene inpainting, SemCity [16]\nrequires multiple-step resampling [20] and lacks one-step\nsampling capability.\nAdditionally, it can not control the\ncategories of the generated regions. This lack of flexible\ncontrol prevents users from generating their desired scenes.\nIn this paper, our proposed SSEditor overcomes these lim-\nitations and enables users to generate large-scale outdoor\nscenes from masks with traditional DDPM sampling [11].\n3. Method\nIn this paper, we propose our SSEditor, as illustrated in Fig.\n2. The primary objective of SSEditor is to enable users to\ngenerate 3D outdoor semantic scenes with flexibility and\ncontrollability. To achieve this goal, we first leverage a 3D\nscene autoencoder to learn the triplane representation (Sec.\n3.1) and then create an asset library for storing 3D masks\n(Sec. 3.2). To enhance the accuracy for generating the po-\nsitions, sizes, and categories of target objects, we imple-\nment a geometry-semantic fusion module that improves the\nmodel’s understanding of geometric and semantic informa-\ntion, facilitating our controllable mask-to-scene generation.\n(Sec. 3.3). During inference, users can flexibly select or\ncreate assets to customize 3D scene construction, such as\ncontrollable inpainting, novel urban scene generation and\ntrailing artifacts removal (Sec. 3.4).\n3.1. 3D Scene Autoencoder with Triplane\nFig. 2(a) illustrates that the 3D scene autoencoder learns the\ntriplane representation through scene reconstruction. We\nemploy an encoder composed of 3D convolutions to en-\ncode a given scene y ∈RX×Y ×Z into z ∈RCz× X\nd × Y\nd × Z\ndz ,\nwhere Cz, X, Y and Z denote the number of channel and\nthe resolution of 3D voxel space, while d and dz indi-\ncate the down-sampling factors. Axis-wise average pool-\ning is then applied across the three dimensions of z to de-\nrive the triplane representation T = [T xy, T xz, T yz]. In\naddition, we sample query points p from the scene vox-\nels and aggregate the corresponding triplane features based\non their coordinates, which can be represented as T (p) =\nT xy(pxy)+T xz(pxz)+T yz(pyz). The aggregated triplane\nfeatures, combined with positional embedding, are decoded\nto obtain the predicted points ˆp. The predicted points re-\n3\n\nCategorized\nCategory \nDecomposition\nScene\nAssets\n…\nRoad\nVegetation\nPerson\n…\nCar\nTrimask Assets\n…\nℳ௫௬\nℳ௫௭\nℳ௬௭\nRoad Assets\nVegetation \nAssets\nCar Assets\n…\nTrimask Basic Assets\n…\nℳ௫௬\nℳ௫௭\nℳ௬௭\nCategorized\nRoad Basic \nAssets\nVegetation \nBasic Assets\nCar Basic \nAssets\n…\n(Optional)\nFigure 3. Pipeline of building 3D mask assets. The 3D mask is\nstored in the corresponding category in the form of a trimask.\nconstruct the scene ˆy based on the original coordinate infor-\nmation. The autoencoder is trained with a joint loss LAE,\nincluding the cross-entropy loss LCE [27] on the points,\nand the Lov´asz-softmax loss LLov [3] on the reconstructed\nscene:\nLAE = LCE(ˆp, p) + αLLov(ˆy, y)\n(1)\nwhere α is a loss weight.\n3.2. 3D Mask Assets\nTo achieve a customizable generation of 3D scenes, con-\ntrolling conditions need to be user-friendly inputs that can\naccurately reflect information such as target position and\nsize. A 3D mask effectively serves this purpose. By uti-\nlizing the triplane representation, as illustrated in Fig. 3,\nwe compress the 3D voxel mask into three 2D orthogonal\nplanes, forming a trimask. The trimask can be represented\nas M = [Mxy, Mxz, Myz]. All categories in the scene\nare decomposed into trimasks and stored in corresponding\nasset libraries. In addition to these scene-level assets, we\nalso provide a basic version of the assets, which contains\nindividual or segmented assets. This allows users to more\nconveniently utilize the basic assets to customize and con-\nstruct scene-level assets. More importantly, users can also\ndraw masks directly within the basic assets or scene-level\nassets. For example, the assets collected in the dataset only\ninclude small roads (2-lane and 4-lane). Users can edit the\nbasic road assets (e.g., by copying, translating, or rotating)\nto create wider lanes, such as 6-lane or 8-lane roads, to sup-\nport the generation of more complex 3D scenes.\n3.3. Controllable Mask-to-Scene Generation\nThe trimasks in the established assets offer valuable ge-\nometric information, including position, orientation, and\nscale. However, this is not enough for effective mask-to-\nsemantic scene generation.\nWe also need to extract de-\ntailed semantic information to ensure accurate object cat-\negory generation. To tackle this, we propose a Geometric-\nSemantic Fusion Module (GSFM), as shown in Fig. 2(b),\nwhich consists of two branches: a geometric branch and a\nsemantic branch.\nGeometric Branch. The geometric branch encodes the tri-\nmask into mask embedding using an multi-layer perception\n(MLP), consisting of two linear layers and one activation\nlayer. For simplicity, we first concatenate the trimask into a\n2D feature maps M′ ∈RN×(Xm+Zm)×(Ym+Zm), where N\nis the number of semantic classes, Xm = X\nd , Ym = Y\nd and\nZm =\nZ\ndz . The mask embedding Em ∈RN×Cemb can be\nobtained by\nMLP(x) = Linear(GeLU(Linear(x)\n(2)\nEm = MLP(M′)\n(3)\nThe extracted mask embeddings currently operate inde-\npendently and lack geometric information from other cate-\ngories. To resolve this, we employ self-attention to capture\nthe geometric relationships between masks of different cat-\negories through Eq. 4. This allows the model to detect tar-\ngets of varying scales within the same category and identify\noverlapping regions between different category masks.\nE′\nm = Em + LayerNorm(SelfAttn(Em)).\n(4)\nSemantic Branch. In the semantic branch, we begin with\nan embedding layer to convert semantic labels into label\nembeddings Elabel ∈RN×Cemb. However, the label em-\nbeddings offer only coarse-grained semantic information,\nwhich is inadequate for large-scale scene generation. The\nvoxels generated within the mask regions may still contain\na number of incorrect categories. To address this, we in-\ntroduce a finer-grained semantic token Tsem ∈RN×Cemb,\nwhich is defined as:\nTi\nsem = Spatial Pooling(Mi · T )\n(5)\nwhere i indicates the i-th semantic class and spatial pooling\nrepresents average pooling along the spatial dimension. As\na result, the semantic embeddings Esem ∈RN×Cemb can\nbe formulated as\nEsem = MLP(Elabel + Tsem)\n(6)\n4\n\nModel\nFID ↓\nKID ↓\nIS ↑\nPrec. ↑\nRec. ↑\nSemanticKITTI [2]\nSSD [15]\n117.46\n0.12\n2.15 ± 0.13\n0.01\n0.08\nSemCity [16]\n61.20\n0.04\n2.43 ± 0.11\n0.19\n0.12\nSSEditor (ours)\n47.93\n0.03\n2.55 ± 0.14\n0.31\n0.51\nCarlaSC [37]\nSSD [15]\n148.14\n0.18\n2.23 ± 0.10\n0.15\n0.01\nSemCity [16]\n137.94\n0.13\n3.03 ± 0.17\n0.20\n0.02\nSSEditor (ours)\n50.98\n0.03\n2.28 ± 0.08\n0.37\n0.18\nTable 1. Quantitative results on SemanticKITTI and CarlaSC. The\nmetrics are measured between the rendered image of the generated\nscene and the real scene. Prec. and Rec. indicates precision and\nrecall, respectively.\nGeometric-Semantic Fusion Module. The GSFM inte-\ngrates mask embeddings Em and semantic embeddings\nEsem through cross-attention. We use the geometric em-\nbeddings as the query Q ∈RN×Cemb and concatenate the\ngeometric and semantic embeddings to form the key K and\nvalue V ∈R2N×Cemb. The fused embeddings Efused can\nthen be represented as:\nEfused = E′\nm + LayerNorm(CrossAttn(Q, K, V ))\n(7)\nMask Conditional Diffusion Model.\nFollowing LDM\n[28], we conduct diffusion and denoising process on the\ntriplane features T to learn our mask conditional diffu-\nsion model Dθ.\nWe add t steps of Gaussian noise to\na clean triplane features T0 and obtain a noised triplane\nTt ∼q(Tt|T0) = N(√αtT , (1 −αt)I), where N is the\nGaussian distribution, αt = Qt\ni=1 αi and αt = 1 −βt with\na variance schedule βt. Then the diffusion model Dθ can be\ntrained with the mean-squared error loss:\nL = Et∼[1,T ]∥T0 −Dθ(Concat(Tt, M), t)∥2\n(8)\nTo support mask conditional generation, we inject the\nfused embedding Efused obtained from Eq. 7 into the cross\nattention of the diffusion model. In addition, we concate-\nnate the trimask with Tt to further enhance the guidance of\nthe mask. Following classifier-free guidance [10], we ran-\ndomly set the trimask to zero during training to simulate the\neffect of not using the trimask.\n3.4. Downstream Applications\nUnlike unconditional scene generation [16], our SSEditor\ncan flexibly handle various downstream tasks based on the\ncreated assets, such as controllable scene inpainting and\ncontrollable scene outpainting. Note that our method does\nnot require a resampling strategy [20].\nControllable Scene Inpainting can facilitate basic scene\nediting, such as adding or removing objects. Based on this,\nModel\nInput\nIoU ↑\nmIoU ↑\nSymphonies [12]\nRGB\n41.92\n14.89\nSCPNet [38]\nPoint Cloud\n50.24\n37.55\nSSEditor (ours)\n3D Mask\n57.85\n43.09\nTable 2. Quantitative results on SemanticKITTI validation set.\nIoU and mIoU indicate how effectively our method handles geo-\nmetric information and comprehends semantic information during\ngeneration, respectively.\nSSEditor can simulate corner cases in autonomous driving\nscenarios, such as vehicle congestion at intersections, bi-\ncycles haphazardly parked on the roadside, and pedestrians\ncrossing the street. Furthermore, the accumulation of mul-\ntiple LiDAR frames causes trailing artifacts in dynamic ob-\njects within the SemanticKITTI dataset [2]. Our SSEditor\neffectively resolves this issue. In addition, by editing back-\nground assets such as roads and sidewalks, SSEditor can\nalso widen roads to simulate scenarios with greater traffic.\nControllable Scene Outpainting can assist in scene exten-\nsion. By selecting appropriate background assets and com-\nbining them, such as stitching together continuous roads,\nwe can controllably extend the scene.\nNovel Urban Scene Generation enables the rapid con-\nstruction of 3D occupancy datasets. Imagine that we want\nto build a 3D semantic scene for a new city: we can cre-\nate different assets based on LiDAR point clouds, and then\ngenerate a novel urban scene based on these assets.\nRemoving trailing artifacts. SemanticKITTI [2] aggre-\ngates multiple LiDAR frames to create dense 3D occupancy\nscenes, but this introduces trailing artifacts for moving ob-\njects in the ground truth, as shown in Fig. 1(b). Our method\ncan effectively remove these artifacts and utilizes existing\nobject assets to generate new objects.\n4. Experiments\n4.1. Datasets\nWe conduct our experiments on the SemanticKITTI [2] and\nCarlaSC [37] datasets. SemanticKITTI dataset is a large-\nscale real-world benchmark for semantic scene understand-\ning in autonomous driving. It contains 20 semantic classes.\nEach scene is represented by a 256×256×32 voxel grid\nwith a voxel resolution of 0.2m. CarlaSC dataset is a syn-\nthetic dataset with labels for 11 semantic classes, gener-\nated using the CARLA simulator. Each scene has a res-\nolution of 128×128×8, covering an area of 25.6 meters\naround the vehicle, with a height of 3 meters. Addition-\nally, we validated the cross-dataset transferability of SSEd-\nitor on Occ3D-Waymo [33]. We only included the occu-\npancy labels from Occ3D-Waymo [33] as trimasks in our\nasset library and then simulated the generation of unknown\n5\n\n…\nTrimask Assets\nRoad\nSidewalk\nCar\nOther-vehicle\n1.Remove Objects: Car, Other-vehicle, …\n2.Editing background: Road, Sidewalk, …\nCar\nOther-vehicle\nGround Truth\n…\nBicycle\n…\nRoad\nSidewalk\nVegetation\n3.Add Objects: Car, Other-vehicle, …\n…\nCar\nOther-vehicle\nBicycle\nFigure 4. The details of editing 3D scenes with SSEditor: 1. When\nthe mask of an object is set to 0, the corresponding object can be\ncompletely removed. 2. The background can be edited, such as\nwidening roads to simulate heavier traffic. 3. Objects can be added\nto the edited scene.\nurban scenes. Note that we disregard the Occ3D-Waymo\ncategories not present in SemanticKITTI.\n4.2. Implementation Details\nAll experiments are conducted on a single NVIDIA RTX\n3090-24G GPU. For the 3D scene autoencoder, the batch\nsize is set to 4, while for the controllable mask-to-scene gen-\neration, the batch size is set to 1. The downsampling factors\nare configured as d = 2 and dz = 1. The loss weight α in\nthe Eq. 1 is set to 1, the latent channel of triplane features\nT equals 16 and the embedding channel Cemb = 64. The\nlearning rate for the autoencoder is 1e-3, while the learning\nrate for the diffusion model is 1e-4. Following the settings\nof [15, 16], the sampling time steps is set to 100 during\nboth training and testing of the diffusion model. We utilize\nDDPM sampling strategy [11] for downstream tasks, omit-\nting the need for the resampling strategy in RePaint [20].\n4.3. Evaluation Metrics\nWe adopt evaluation metrics from prior works [16, 32, 40]\nrendering 3D scenes into 2D images and use traditional 2D\nevaluation metrics to assess the quality and diversity of gen-\nerated scenes:\nFr´echet Inception Distance (FID) [9] measures the simi-\nlarity between the real and generated data distributions by\ncomparing their feature statistics in the latent space of the\nImageNet-pretrained Inception network.\nInception Score (IS) [30] evaluates both the quality and\ndiversity of generated samples by computing a statistical\nscore from the Inception network.\nKernel Inception Distance (KID) [4] computes the\nsquared Maximum Mean Discrepancy (MMD) between the\nreal and generated data distributions using features ex-\ntracted from the Inception network.\nPrecision measures the proportion of generated samples\nthat fall within the support of the real data distribution,\nwhile Recall measures the proportion of the real data dis-\ntribution covered by the generated samples.\nIn addition, we use the intersection over union (IoU) and\nmean IOU (mIoU) metrics to evaluate the overall scene re-\nconstruction quality and the reconstruction quality for each\nclass, respectively.\n4.4. Quantitative Results\nGeneration.\nTable 1 provides quantitative results on\nSmeanticKITTI and CarlaSC comparing with SSD [15] and\nSemCity [16]. In overall generation quality and diversity,\nour SSEditor outperforms the previous methods [15, 16] on\nSemanticKITTI [2], particularly in FID and recall, where\nwe achieve improvements of 21.68% and 39%, respectively,\ncompared to SemCity. On CarlaSC [37], SSEditor leads in\nall metrics except for IS, with FID improving by 63.04%\nover SemCity. Note that SemCity do not disclose which\nimage sets are used for evaluation, making the results non-\nreproducible. To ensure a fair comparison, we train on the\ntraining set and generate scenes on the validation set to ob-\ntain the evaluation results.\nSemantic Scene Completion.\nWe assess the controlla-\nbility and scene reconstruction capabilities of our method\nthrough semantic scene completion. Table 2 demonstrates\nthat SSEditor performs well on the SemanticKITTI valida-\ntion set. We only reference two state-of-the-art methods\nfrom different modalities, as other unconditional diffusion\nmodels [15, 16] lack the ability to reconstruct 3D semantic\nscenes. The IoU metric indicates that our method provides\nstrong control over the position and size of objects during\nscene generation, while the mIoU score reflects a robust un-\nderstanding of the semantics of the generated objects.\n4.5. Qualitative Results\nGeneration.\nFig.\n5 showcases the qualitative results\nof the proposed SSEditor and SemCity [16] on the Se-\nmanticKITTI [2] and CarlaSC [37] datasets. While Sem-\nCity [16] effectively generates a variety of scenes using\ntriplane representations, it lacks sufficient control, making\nscene customization challenging. In contrast, SSEditor al-\nlows for precise generation of 3D scenes guided by masks,\noffering enhanced controllability. In Fig. 5, we create tri-\nmasks based on ground truth to verify our method’s con-\ntrollability. The results demonstrate that SSEditor excels in\n6\n\nSSEditor (Ours)\nGround Truth\nSemCity\nCar\nRoad\n…\nSidewalk\nVegetation\nCar\nOther Vehicle\n…\nRoad\nPatking\nCar\n…\nPerson\nRoad\nTraffic Sign\nSemanticKITTI\nCar\nGround\n…\nRoad\nSidewalk\nCarlaSC\nTrimasks\nCar\nOther\n…\nPole\nRoad\nCar\nBicycle\nMotorcycle\nTruck\nOther-vehicle\nBicyclist\nPerson\nMotorcyclist\nRoad\nPatking\nSidewalk\nOther-ground\nBuilding\nFence\nVegetation\nTrunk\nTerrain\nPole\nTraffic-sign\nVehicle\nOther\nGround\nRoad\nSidewalk\nBuilding\nFences\nVegetation\nPole\nPedestrian\nFigure 5. Visualization of semantic scene generation comparing with SemCity [16] on SemanticKITTI [2] and CalarSC [37]. Under the\nguidance of the trimask as a condition, SSEditor demonstrates its strong controllability.\ncontrolling both the overall background (e.g., road, vegeta-\ntion) and specific objects (e.g., vehicles, pedestrians).\nScene Editing. Fig. 4 highlights the details of scene editing\nwith SSEditor. By setting the trimask of a target object or\nbackground to zero, we can effectively remove it from the\nscene. We can also edit background assets for more realistic\nscenarios, like creating four-lane or eight-lane assets. Once\nthe background is adjusted, we can add objects, like increas-\ning the number of cars to simulate higher traffic volumes, to\ncreate more dynamic scenarios.\nNovel Scene Generation. To further validate the control-\nlability of SSEditor in generating new scenes, we apply the\ntrained model to the Occ-3D Waymo dataset [33]. We ad-\njust the trimasks from Occ-3D Waymo through interpola-\ntion to align with the standard size of trimasks in our asset\nlibrary, due to the different resolutions of the datasets. Note\nthat we only create trimasks for categories that appear in\nSemanticKITTI [2]. The generated results in Fig. 6 demon-\n7\n\nCreate a new urban scene\nOcc-3D Waymo\nGround Truth\nScene Mask 1\n…\n…\nScene Mask 2\n…\n…\nFigure 6. Create a novel urban scene from masks. The novel scene\ngeneration is tested on the unseen Occ-3D Waymo dataset [33].\nstrate that SSEditor can effectively adapt to new scene gen-\neration, enabling the rapid creation of urban environments.\n4.6. Ablation Studies\nWe conduct ablation experiments on the SemanticKITTI [2]\nvalidation set to assess the contribution of each component\nof SSEditor, as shown in Table 3.\nFirst, we evaluate the effectiveness of the geometric\nbranch by retaining the semantic branch and concatenating\nthe trimask with the noised triplane Tt as input. Next, we\nremove the semantic branch, followed by the semantic to-\nkens within the branch, to examine their individual impact.\nFinally, we input only the noised triplane Tt to assess the\nrole of concatenating the trimask. In all ablation experi-\nments, removing any component results in a performance\ndrop, highlighting the necessity of each component for op-\ntimal performance.\nAdditionally, as shown in Table 4, we compared two\nsampling strategies: DDPM [11] and the resampling tech-\nnique from RePaint [20]. While resampling improves ob-\nject integration with the environment during generation, it\ngreatly increases inference time for 3D scene generation.\nIn contrast, our method employs traditional DDPM sam-\npling, which maintains high quality and controllability in\nboth scene inpainting and outpainting, while reducing in-\nference time.\n5. Limitations\nAlthough SSEditor demonstrates strong capabilities for\ncontrollable scene generation, it still faces challenges with\ngenerating small objects, such as bicyclists and pedestri-\nans.\nThe generated areas sometimes contain incorrectly\nclassified voxels, and the model’s performance is highly\nMethod\nFID ↓\nKID ↓\nIS ↑\nPrec. ↑\nRec. ↑\nw/o geometric branch\n60.32\n0.05\n2.45 ± 0.15\n0.24\n0.28\nw/o semantic branch\n54.96\n0.05\n2.49 ± 0.13\n0.27\n0.37\nw/o semantic tokens\n53.67\n0.04\n2.49 ± 0.12\n0.27\n0.38\nw/o mask concat\n54.08\n0.04\n2.43 ± 0.17\n0.23\n0.19\nSSEditor (ours)\n47.93\n0.03\n2.55 ± 0.14\n0.31\n0.51\nTable 3. Ablation studies on scene generation. We validated the\neffectiveness of the geometric branch, semantic branch, and the\nconcatenated input of the trimask on SemanticKITTI [2].\nMethod\nSteps\nSampling\nInference Time\nSSEditor\n100\nResampling [20]\n56.44s\nDDPM [11]\n13.40s\n20\nResampling [20]\n13.89s\nDDPM [11]\n3.66s\n10\nResampling [20]\n6.91s\nDDPM [11]\n2.08s\nTable 4. Ablation studies on sampling strategy. The inference time\nis reported based on 100 sample runs.\nsensitive to surrounding objects, which can lead to inac-\ncuracies.These issues negatively affect the performance of\ndownstream tasks that rely on high-quality scene genera-\ntion. Predicting small objects in semantic scene completion\nis inherently challenging due to their low visibility and the\ncomplex interactions they have with the environment, re-\nsulting in lower mIoU performance. Future work could fo-\ncus on addressing the long-tail distribution of data by incor-\nporating more robust methods for representing and detect-\ning small objects, as well as developing more fine-grained\nrepresentation techniques that can improve the handling of\nthese challenging cases.\n6. Conclusion\nIn this paper, we propose SSEditor, a two-stage control-\nlable scene generation framework based on the diffusion\nmodel. In the first stage, we leverage a 3D scene autoen-\ncoder to learn triplane representations. We then create a\ntrimask asset library as a preparatory step for the second\nphase of training. In the second stage, we train a mask-\nconditional diffusion model for mask-to-scene generation,\nincorporating a geometric-semantic fusion module to en-\nhance the extraction of geometric and semantic informa-\ntion.\nExperimental results on SemanticKITTI, CarlaSC,\nand Occ-3D Waymo demonstrate that our method outper-\nforms existing unconditional diffusion approaches, offering\nsuperior controllability and high-quality scene generation.\nMoreover, SSEditor supports a wide range of applications,\nincluding the generation of novel 3D urban scenes (such as\ncross-dataset generation and road widening), controllable\ngeneration of dynamic objects, and scene outpainting.\n8\n\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 18208–18218, 2022. 2\n[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-\nzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall.\nSe-\nmantickitti: A dataset for semantic scene understanding of\nlidar sequences.\nIn Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9297–9307,\n2019. 1, 5, 6, 7, 8\n[3] Maxim Berman, Amal Rannen Triki, and Matthew B\nBlaschko. The lov´asz-softmax loss: A tractable surrogate for\nthe optimization of the intersection-over-union measure in\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4413–4421,\n2018. 4\n[4] Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and\nArthur Gretton. Demystifying mmd gans. arXiv preprint\narXiv:1801.01401, 2018. 6\n[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 16123–16133, 2022. 2\n[6] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese.\n3d-r2n2: A unified approach\nfor single and multi-view 3d object reconstruction. In Com-\nputer Vision–ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part VIII 14, pages 628–644. Springer, 2016. 2\n[7] Abdelrahman Eldesokey and Peter Wonka. Build-a-scene:\nInteractive 3d layout control for diffusion-based image gen-\neration. arXiv preprint arXiv:2408.14819, 2024. 2, 3\n[8] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In Eu-\nropean Conference on Computer Vision, pages 89–106.\nSpringer, 2022. 2\n[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[10] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 2, 3, 6, 8\n[12] Haoyi Jiang, Tianheng Cheng, Naiyu Gao, Haoyang Zhang,\nTianwei Lin, Wenyu Liu, and Xinggang Wang.\nSym-\nphonize 3d semantic scene completion with contextual in-\nstance queries. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 20258–\n20267, 2024. 5\n[13] Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang,\nYu Qiao, and Hongsheng Li. Diffindscene: Diffusion-based\nhigh-quality 3d indoor scene generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4526–4535, 2024. 1\n[14] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n18423–18433, 2023. 1\n[15] Jumin Lee, Woobin Im, Sebin Lee, and Sung-Eui Yoon. Dif-\nfusion probabilistic models for scene-scale 3d categorical\ndata. arXiv preprint arXiv:2301.00527, 2023. 1, 2, 5, 6\n[16] Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong\nSeon, and Sung-Eui Yoon. Semcity: Semantic scene genera-\ntion with triplane diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 28337–28347, 2024. 1, 2, 3, 5, 6, 7\n[17] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-\nsdf: Text-to-shape via voxelized diffusion. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12642–12651, 2023. 2\n[18] Chenguo Lin and Yadong Mu. Instructscene: Instruction-\ndriven 3d indoor scene synthesis with semantic graph prior.\narXiv preprint arXiv:2402.04717, 2024. 2, 3\n[19] Yuheng Liu, Xinke Li, Xueting Li, Lu Qi, Chongshou Li,\nand Ming-Hsuan Yang. Pyramid diffusion for fine 3d large\nscene generation. arXiv preprint arXiv:2311.12085, 2023. 1,\n2, 3\n[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 11461–11471, 2022. 2, 3, 5, 6, 8\n[21] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492–\n13502, 2022. 2\n[22] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-\nham Tulsiani.\nAutosdf:\nShape priors for 3d comple-\ntion, reconstruction and generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 306–315, 2022. 2\n[23] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui\nHuang, Shi-Min Hu, Ke Li, and Leonidas Guibas. Difffacto:\nControllable part-based 3d point cloud generation with cross\ndiffusion.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 14257–14267, 2023.\n2\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[25] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\n9\n\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[26] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth,\nSanja Fidler, and Francis Williams. Xcube: Large-scale 3d\ngenerative modeling using sparse voxel hierarchies. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4209–4219, 2024. 1, 2\n[27] Luis Roldao, Raoul de Charette, and Anne Verroust-Blondet.\nLmscnet: Lightweight multiscale 3d semantic completion.\nIn 2020 International Conference on 3D Vision (3DV), pages\n111–119. IEEE, 2020. 4\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 2, 5\n[29] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 conference proceedings, pages 1–10,\n2022. 2\n[30] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 6\n[31] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20875–20886, 2023. 1\n[32] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus\nThies, and Matthias Nießner. Diffuscene: Denoising diffu-\nsion models for generative indoor scene synthesis. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 20507–20518, 2024. 1, 2, 3,\n6\n[33] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao,\nHuitong Yang, Yue Wang, Yilun Wang, and Hang Zhao.\nOcc3d: A large-scale 3d occupancy prediction benchmark\nfor autonomous driving.\nAdvances in Neural Information\nProcessing Systems, 36, 2024. 5, 7, 8\n[34] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany,\nSanja Fidler, Karsten Kreis, et al. Lion: Latent point dif-\nfusion models for 3d shape generation. Advances in Neural\nInformation Processing Systems, 35:10021–10039, 2022. 2\n[35] Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang,\nZhiyong Cui, Haiyang Yu, and Jiwen Lu.\nOccsora: 4d\noccupancy generation models as world simulators for au-\ntonomous driving. arXiv preprint arXiv:2405.20337, 2024.\n1, 2\n[36] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is all\nyou need for image-to-image translation.\narXiv preprint\narXiv:2205.12952, 2022. 2\n[37] Joey Wilson, Jingyu Song, Yuewei Fu, Arthur Zhang, An-\ndrew Capodieci, Paramsothy Jayakumar, Kira Barton, and\nMaani Ghaffari. Motionsc: Data set and network for real-\ntime semantic mapping in dynamic environments.\nIEEE\nRobotics and Automation Letters, 7(3):8439–8446, 2022. 5,\n6, 7\n[38] Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin\nMa, Yikang Li, Yuenan Hou, and Yu Qiao.\nScpnet: Se-\nmantic scene completion on point cloud. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 17642–17651, 2023. 5\n[39] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nMech, and Ulrich Neumann. Disn: Deep implicit surface\nnetwork for high-quality single-view 3d reconstruction. Ad-\nvances in neural information processing systems, 32, 2019.\n2\n[40] Guangyao Zhai, Evin Pınar ¨Ornek, Shun-Cheng Wu, Yan\nDi, Federico Tombari, Nassir Navab, and Benjamin Busam.\nCommonscenes: Generating commonsense 3d indoor scenes\nwith scene graphs. Advances in Neural Information Process-\ning Systems, 36, 2024. 1, 2, 6\n[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836–3847, 2023. 2\n[42] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi,\nYing Shan, and Xi Li. Layoutdiffusion: Controllable diffu-\nsion model for layout-to-image generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 22490–22499, 2023. 2\n[43] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 5826–5835, 2021. 1\n10",
    "pdf_filename": "SSEditor_Controllable_Mask-to-Scene_Generation_with_Diffusion_Model.pdf"
}