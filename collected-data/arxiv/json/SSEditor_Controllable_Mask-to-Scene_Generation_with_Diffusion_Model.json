{
    "title": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model",
    "abstract": "onSemanticKITTIandCarlaSCdemonstratethatSSEditor outperformspreviousapproachesintermsofcontrollability Recentadvancementsin3Ddiffusion-basedsemanticscene andflexibilityintargetgeneration,aswellasthequalityof generationhavegainedattention. However,existingmeth- semanticscenegenerationandreconstruction.Moreimpor- ods rely on unconditional generation and require multi- tantly, experiments on the unseen Occ-3D Waymo dataset ple resampling steps when editing scenes, which signifi- show that SSEditor is capable of generating novel urban cantlylimitstheircontrollabilityandflexibility. Tothisend, scenes,enablingtherapidconstructionof3Dscenes. we propose SSEditor, a controllable Semantic Scene Ed- itor that can generate specified target categories without multiple-step resampling. SSEditor employs a two-stage 1.Introduction diffusion-basedframework: (1)a3Dsceneautoencoderis trained to obtain latent triplane features, and (2) a mask- In recent years, 3D diffusion models have made notable conditionaldiffusionmodelistrainedforcustomizable3D achievements in generating both indoor [13, 32, 40] and semantic scene generation. In the second stage, we in- outdoor[15,16,19,26,35]environments,aswellasasin- troduce a geometric-semantic fusion module that enhance gle object [14, 31, 43]. Compared to indoor scenes and the model‚Äôs ability to learn geometric and semantic infor- individualobjects,outdoorscenespresentmorechallenges mation. This ensures that objects are generated with cor- duetotheirsparserandmorecomplexrepresentations. For 1 4202 voN 91 ]VC.sc[ 1v09221.1142:viXra",
    "body": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model\nHaowenZheng,YanyanLiang\nMacauUniversityofScienceandTechnology\nzhengnayin@gmail.com, yyliang@must.edu.mo\nCreate Background Add Objects Ground Truth Car After Editing\nMasks\nBackground Masks Object Masks ‚Ä¶ ‚Ä¶ Editing\n(b) Eliminate trailing artifacts\nScene Mask 1 Scene Mask n\n‚Ä¶ ‚Ä¶ ‚Ä¶\n(a) Generate a scene from scratch\n‚Ä¶ ‚Ä¶\nRoad Concatenate\nMasks\nEditing (d) Controllable scene\n(c) Widen a road outpainting\nFigure1. Controllable3DsemanticscenegenerationbySSEditor. TheproposedSSEditorenablesuserstocustomizethegenerationor\neditingof3Dscenesusingpre-builtmaskassets: (a)createabackgroundsceneandgenerateobjectsonit;(b)eliminatetrailingartifacts\nofdynamicobjectsinSemanticKITTI[2];(c)modifyroads,suchasexpandingatwo-laneroadtoafour-laneroad;(d)concatenatemasks\nfromvariousscenestoproducealarger-scale3Dscene.\nAbstract rectpositions,sizes,andcategories. Extensiveexperiments\nonSemanticKITTIandCarlaSCdemonstratethatSSEditor\noutperformspreviousapproachesintermsofcontrollability\nRecentadvancementsin3Ddiffusion-basedsemanticscene\nandflexibilityintargetgeneration,aswellasthequalityof\ngenerationhavegainedattention. However,existingmeth-\nsemanticscenegenerationandreconstruction.Moreimpor-\nods rely on unconditional generation and require multi-\ntantly, experiments on the unseen Occ-3D Waymo dataset\nple resampling steps when editing scenes, which signifi-\nshow that SSEditor is capable of generating novel urban\ncantlylimitstheircontrollabilityandflexibility. Tothisend,\nscenes,enablingtherapidconstructionof3Dscenes.\nwe propose SSEditor, a controllable Semantic Scene Ed-\nitor that can generate specified target categories without\nmultiple-step resampling. SSEditor employs a two-stage\n1.Introduction\ndiffusion-basedframework: (1)a3Dsceneautoencoderis\ntrained to obtain latent triplane features, and (2) a mask- In recent years, 3D diffusion models have made notable\nconditionaldiffusionmodelistrainedforcustomizable3D achievements in generating both indoor [13, 32, 40] and\nsemantic scene generation. In the second stage, we in- outdoor[15,16,19,26,35]environments,aswellasasin-\ntroduce a geometric-semantic fusion module that enhance gle object [14, 31, 43]. Compared to indoor scenes and\nthe model‚Äôs ability to learn geometric and semantic infor- individualobjects,outdoorscenespresentmorechallenges\nmation. This ensures that objects are generated with cor- duetotheirsparserandmorecomplexrepresentations. For\n1\n4202\nvoN\n91\n]VC.sc[\n1v09221.1142:viXra\ninstance, voxel-based representations of outdoor environ- narios, such as expanding a two-lane road to four or more\nments often contain a significant number of empty vox- lanes.\nels. Moreover, outdoor environments contain smaller tar- Ourcontributionscanbesummarizedintothreepoints:\ngets, suchaspedestriansandcyclists, furthercomplicating ‚Ä¢ WeproposeSSEditor, acontrollablemask-to-scenegen-\nthe generation process. While voxel-based representations erationframeworkthatenablesuserstoeasilycustomize\n[15,19,26,35]provideastraightforwardapproachtomod- andgenerate3Dsemanticscenesusingvariousassets.\neling 3D semantic scenes, they suffer from redundancy in ‚Ä¢ We propose GSFM to integrate geometric and semantic\nempty regions and high computational cost. To mitigate information. InGSFM,thegeometricbranchencodes3D\ntheseissues,thetriplanerepresentation[5]isutilizedtore- masks as embeddings to accurately control the position,\nduce unnecessary information in 3D outdoor scenes [16]. size,andorientationofobjects,whilethesemanticbranch\nAlthoughthesemethodshaveshownpromisingresults,they processes semantic labels and tokens for improved class\nstillfaceseverallimitations. controlofthegeneratedtargets.\nThe primary limitation lies in their weak controllabil- ‚Ä¢ Experiments on outdoor datasets demonstrate that our\nity. Unconditional generation restricts the ability to guide proposed method achieves superior generation quality\nthe creation of 3D scenes, while conditioning on the en- and reconstruction performance. Furthermore, qualita-\ntirescene(e.g., scenerefinementbasedongroundtruth)is tive results indicate that SSEditor can controllably per-\noverly rigid. This lack of flexible control leads to another formvariousdownstreamtasks,suchassceneinpainting,\ndrawback: editing specific local regions, such as adding resourceexpansion,novelurbanscenegeneration,andre-\norremovingobjects,necessitatesmaskingnon-targetareas movaloftrailingartifacts.\nandemployingamulti-stepresamplingprocessforrepaint-\n2.RelatedWork\ning[20]. Itsignificantlyincreasesgenerationtime. Despite\nthe use of this resampling strategy, repainting remains un-\nControllableDiffusionModels.Denoisingdiffusionprob-\ncontrollableandoftenfailstoproducethedesiredresults.\nabilisticmodels(DDPM)[11]inspiresaseriesofdiffusion-\nTo address the aforementioned challenges, we propose basedcontrollablegenerationapproaches. Text-guidedim-\nSSEditor, a flexible and controllable two-stage framework age generation shows strong capabilities in image editing\nforsemanticscenegenerationbasedonthelatentdiffusion tasks,suchasinpainting[1,24,25]andoutpainting[29]. In\nmodel (LDM) [28]. In the first stage, we train a 3D scene addition, several studies incorporate more control signals,\nautoencoder to learn triplane features via semantic scene such as layouts [42], semantic maps [8, 28, 36, 41], to fa-\nreconstruction. In the second stage, we train a mask con- cilitateimagegeneration. Buildingontheseadvancements,\nditional diffusion model on the triplane features. Specifi- controllablediffusionmodelshavebeenfurtherextendedto\ncally,toenablethecustomizablegenerationof3Dsemantic the3Ddomain. Thesemodelscanleverageimages[6,39],\nscenes, we present a Geometric-Semantic Fusion Module text[17,21],partialpointclouds[23]ormulti-modalcon-\n(GSFM), which consists of a geometric branch and a se- ditions(e.g.,text-imageortext-voxels)[22,34]toguidethe\nmantic branch. The geometric branch encodes 3D masks generation of a single 3D object. However, the aforemen-\nthat represent an object‚Äôs position, size, and orientation, tionedcontrollablegenerativemodelscanonlybeappliedto\nwhile the semantic branch processes semantic labels and 2Dimagesorindividual3Dobjects,makingitchallenging\ntokens for providing coarse and fine-grained semantic in- forthemtohandlecomplexlarge-scale3Dscenes.\nformation. The semantic tokens are generated from the 3DSemanticSceneGeneration. 3Dsemanticscenegen-\nfeatures of a specific category. These features are then eration can be categorized into indoor and outdoor scene\naggregated and integrated into the cross-attention module generation. CommonScenes [40] generates indoor scenes\nof the diffusion model, enhancing its perception of both based on scene graphs. DiffuScene [32] performs indoor\ngeometric and semantic information. Benefiting from the scenegenerationandcompletionbasedonatextpromptor\nabovedesign,SSEditoreffectivelyaccomplishesthemask- incomplete3Dtargets. InstructScene[18]incorporatesuser\nto-semanticscenegenerationtask. instructions into semantic graph priors and decodes them\nIn addition, we create a 3D mask asset library encom- into 3D indoor scenes. Build-A-Scene [7] enables users\npassing various categories to facilitate custom scene gen- to flexibly create indoor scenes by adjusting layouts. In\neration during inference. The 3D masks in the library are contrast,outdoorscenegenerationismorecomplex,which\nstoredintheformoftrimasks,whicharecomposedofthree featuresdiverseobjects, moreocclusions, andvaryingdis-\northogonal2Dplanesderivedfromthedecompositionofthe tances. [15]generates3Dmulti-objectscenesinsimulated\n3Dmask.AsshowninFig.1,userscanchoosefromarange outdoor environments, while PDD [19] employs a coarse-\nofassets,suchascross-shapedroads,vehicles,pedestrians, to-fine strategy to further improve generation quality. For\nand cyclists, to generate their desired 3D semantic scenes. more complex real-world outdoor scenes, SemCity [16]\nThe assets can also be edited to simulate more urban sce- uses triplane diffusion to achieve unconditional generation\n2\n(a) Learning Triplane Representation via 3D Scene Autoencoder\nAvg.\nEncoder Pooling Decoder\nùíõ Triplane ùì£ PE(ùíë)\nSceneùíö query po ‚Ä¶ints ùëù Query based on ‚Ä¶ Predicted ‚Ä¶points ùíë(cid:3549)\ncoordinate Reconstructed Sceneùíö(cid:3549)\nMask Embeddings Semantic Embeddings\n‚Ä¶ ‚Ä¶\nAdd & Norm\nGeometric Encoder Semantic Encoder Cross Attention ùì£ ùì£\nùíï ùíï(cid:2879)ùüè\nTrimask ùìú\nTrimask Denoising UNet\nAdd & Norm\nCar Person Semantic Semantic\nTrimask ‚Ä¶ Trimask Label Token\nSelf Attention Diffusion\nRoad Vegetation Process\nTrimask Trimask ‚Ä¶ ‚Ä¶ ùì£ ùíï ùì£ ùüé\nGeometric Semantic\nAssets Branch Branch Concat Skip CA Cross\n‚Ä¶ Connection Attention\nGeometric Semantic\nGeometric Branch Semantic Branch (b) Controllable Mask-to-Semantic\nFusion Module\nScene Generation\nFigure2.IllustrationofourSSEditorframework.Itcomprisestwomainprocesses:(a)a3Dautoencoderlearnsthetriplanerepresentation\nvia scene reconstruction, and (b) controllable semantic scene generation is achieved through masks, semantic labels, and tokens. The\nGeometric-SemanticFusionModuleisessentialforthediffusionmodeltoeffectivelylearnbothgeometricandsemanticinformation.\norconditional3Doccupancyrefinement. mentageometry-semanticfusionmodulethatimprovesthe\nDue to the significant differences between indoor and model‚Äôsunderstandingofgeometricandsemanticinforma-\noutdoorenvironments,thesecontrollableindoorscenegen- tion,facilitatingourcontrollablemask-to-scenegeneration.\nerationmethods[7,18,32]aredifficulttoapplytooutdoor (Sec. 3.3). During inference, users can flexibly select or\nscenes. For outdoor environments, [16, 19] can only re- create assets to customize 3D scene construction, such as\nfinescenesbyconditionallyinputtingtheentire3Dlayout. controllable inpainting, novel urban scene generation and\nMoreover,whenconductingsceneinpainting,SemCity[16] trailingartifactsremoval(Sec. 3.4).\nrequires multiple-step resampling [20] and lacks one-step\nsampling capability. Additionally, it can not control the 3.1.3DSceneAutoencoderwithTriplane\ncategories of the generated regions. This lack of flexible\nFig.2(a)illustratesthatthe3Dsceneautoencoderlearnsthe\ncontrolpreventsusersfromgeneratingtheirdesiredscenes.\ntriplane representation through scene reconstruction. We\nInthispaper, ourproposedSSEditorovercomestheselim-\nemploy an encoder composed of 3D convolutions to en-\nitations and enables users to generate large-scale outdoor codeagivensceney‚ààRX√óY√óZ intoz‚ààRCz√óX d√óY d√ó dZ z,\nscenesfrommaskswithtraditionalDDPMsampling[11].\nwhere C , X, Y and Z denote the number of channel and\nz\nthe resolution of 3D voxel space, while d and d indi-\n3.Method z\ncate the down-sampling factors. Axis-wise average pool-\nInthispaper,weproposeourSSEditor,asillustratedinFig. ing is then applied across the three dimensions of z to de-\n2. The primary objective of SSEditor is to enable users to rive the triplane representation T = [Txy,Txz,Tyz]. In\ngenerate 3D outdoor semantic scenes with flexibility and addition, we sample query points p from the scene vox-\ncontrollability. Toachievethisgoal,wefirstleveragea3D elsandaggregatethecorrespondingtriplanefeaturesbased\nsceneautoencodertolearnthetriplanerepresentation(Sec. on their coordinates, which can be represented as T(p) =\n3.1) and then create an asset library for storing 3D masks Txy(pxy)+Txz(pxz)+Tyz(pyz).Theaggregatedtriplane\n(Sec. 3.2). Toenhancetheaccuracyforgeneratingthepo- features,combinedwithpositionalembedding,aredecoded\nsitions, sizes, and categories of target objects, we imple- to obtain the predicted points pÀÜ. The predicted points re-\n3\nAC AC AC AC\nAssets tocreatewiderlanes,suchas6-laneor8-laneroads,tosup-\n‚Ä¶ portthegenerationofmorecomplex3Dscenes.\nCategory\nRoad Vegetation\nDecomposition 3.3.ControllableMask-to-SceneGeneration\n‚Ä¶\nThe trimasks in the established assets offer valuable ge-\nPerson Car\nScene ometric information, including position, orientation, and\nscale. However, this is not enough for effective mask-to-\nRoad Assets ‚Ñ≥(cid:3051)(cid:3052) TrimaskAssets semantic scene generation. We also need to extract de-\nVegetation ‚Ñ≥(cid:3051)(cid:3053) tailed semantic information to ensure accurate object cat-\nAssets Categorized ‚Ä¶\negorygeneration. Totacklethis, weproposeaGeometric-\nSemantic Fusion Module (GSFM), as shown in Fig. 2(b),\nCar Assets ‚Ñ≥(cid:3052)(cid:3053) which consists of two branches: a geometric branch and a\nsemanticbranch.\n(Optional)\nGeometricBranch. Thegeometricbranchencodesthetri-\nRoad Basic ‚Ñ≥(cid:3051)(cid:3052) TrimaskBasic Assets\nAssets maskintomaskembeddingusinganmulti-layerperception\nBV aseg icet Aat ssio etn s Categorized ‚Ñ≥(cid:3051) ‚Ä¶(cid:3053) (MLP), consisting of two linear layers and one activation\nlayer. Forsimplicity,wefirstconcatenatethetrimaskintoa\n2DfeaturemapsM‚Ä≤ ‚àà RN√ó(Xm+Zm)√ó(Ym+Zm),whereN\nCar Basic\nAssets ‚Ñ≥(cid:3052)(cid:3053) isthenumberofsemanticclasses,X\nm\n= X d,Y\nm\n= Y\nd\nand\nZ\nm\n= dZ z. The mask embedding E\nm\n‚àà RN√óCemb can be\nobtainedby\nFigure3. Pipelineofbuilding3Dmaskassets. The3Dmaskis\nstoredinthecorrespondingcategoryintheformofatrimask.\nMLP(x)=Linear(GeLU(Linear(x) (2)\nconstructthesceneyÀÜbasedontheoriginalcoordinateinfor- E m =MLP(M‚Ä≤) (3)\nmation. The autoencoder is trained with a joint loss L ,\nAE\nincluding the cross-entropy loss L [27] on the points, Theextractedmaskembeddingscurrentlyoperateinde-\nCE\nandtheLova¬¥sz-softmaxlossL [3]onthereconstructed pendentlyandlackgeometricinformationfromothercate-\nLov\nscene: gories. Toresolvethis,weemployself-attentiontocapture\nthegeometricrelationshipsbetweenmasksofdifferentcat-\negoriesthroughEq. 4. Thisallowsthemodeltodetecttar-\nL =L (pÀÜ,p)+Œ±L (yÀÜ,y) (1)\nAE CE Lov\ngetsofvaryingscaleswithinthesamecategoryandidentify\nwhereŒ±isalossweight. overlappingregionsbetweendifferentcategorymasks.\n3.2.3DMaskAssets E‚Ä≤ =E +LayerNorm(SelfAttn(E )). (4)\nm m m\nTo achieve a customizable generation of 3D scenes, con-\nSemantic Branch. In the semantic branch, we begin with\ntrolling conditions need to be user-friendly inputs that can\nan embedding layer to convert semantic labels into label\naccurately reflect information such as target position and\nsize. A 3D mask effectively serves this purpose. By uti-\nembeddings E\nlabel\n‚àà RN√óCemb. However, the label em-\nbeddings offer only coarse-grained semantic information,\nlizing the triplane representation, as illustrated in Fig. 3,\nwhich is inadequate for large-scale scene generation. The\nwe compress the 3D voxel mask into three 2D orthogonal\nvoxelsgeneratedwithinthemaskregionsmaystillcontain\nplanes, formingatrimask. Thetrimaskcanberepresented\na number of incorrect categories. To address this, we in-\nas M = [Mxy,Mxz,Myz]. All categories in the scene\nare decomposed into trimasks and stored in corresponding\ntroduce a finer-grained semantic token T\nsem\n‚àà RN√óCemb,\nwhichisdefinedas:\nasset libraries. In addition to these scene-level assets, we\nalso provide a basic version of the assets, which contains\nTi =SpatialPooling(M ¬∑T) (5)\nindividual or segmented assets. This allows users to more sem i\nconveniently utilize the basic assets to customize and con- whereiindicatesthei-thsemanticclassandspatialpooling\nstruct scene-level assets. More importantly, users can also representsaveragepoolingalongthespatialdimension. As\ndraw masks directly within the basic assets or scene-level a result, the semantic embeddings E\nsem\n‚àà RN√óCemb can\nassets. Forexample,theassetscollectedinthedatasetonly beformulatedas\nincludesmallroads(2-laneand4-lane). Userscaneditthe\nbasicroadassets(e.g.,bycopying,translating,orrotating) E =MLP(E +T ) (6)\nsem label sem\n4\n‚Ä¶\n‚Ä¶\nModel FID‚Üì KID‚Üì IS‚Üë Prec.‚Üë Rec.‚Üë Model Input IoU‚Üë mIoU‚Üë\nSemanticKITTI[2]\nSymphonies[12] RGB 41.92 14.89\nSSD[15] 117.46 0.12 2.15¬±0.13 0.01 0.08\nSCPNet[38] PointCloud 50.24 37.55\nSemCity[16] 61.20 0.04 2.43¬±0.11 0.19 0.12\nSSEditor(ours) 47.93 0.03 2.55¬±0.14 0.31 0.51 SSEditor(ours) 3DMask 57.85 43.09\nCarlaSC[37]\nTable 2. Quantitative results on SemanticKITTI validation set.\nSSD[15] 148.14 0.18 2.23¬±0.10 0.15 0.01\nSemCity[16] 137.94 0.13 3.03¬±0.17 0.20 0.02 IoUandmIoUindicatehoweffectivelyourmethodhandlesgeo-\nSSEditor(ours) 50.98 0.03 2.28¬±0.08 0.37 0.18 metricinformationandcomprehendssemanticinformationduring\ngeneration,respectively.\nTable1.QuantitativeresultsonSemanticKITTIandCarlaSC.The\nmetricsaremeasuredbetweentherenderedimageofthegenerated\nsceneandtherealscene. Prec. andRec. indicatesprecisionand SSEditor can simulate corner cases in autonomous driving\nrecall,respectively. scenarios, such as vehicle congestion at intersections, bi-\ncycleshaphazardlyparkedontheroadside,andpedestrians\ncrossingthestreet. Furthermore,theaccumulationofmul-\nGeometric-Semantic Fusion Module. The GSFM inte-\ntipleLiDARframescausestrailingartifactsindynamicob-\ngrates mask embeddings E and semantic embeddings\nm jects within the SemanticKITTI dataset [2]. Our SSEditor\nE through cross-attention. We use the geometric em-\nsem effectivelyresolvesthisissue. Inaddition,byeditingback-\nbeddings as the query Q ‚àà RN√óCemb and concatenate the\nground assets such as roads and sidewalks, SSEditor can\ngeometricandsemanticembeddingstoformthekeyK and\nalsowidenroadstosimulatescenarioswithgreatertraffic.\nvalueV ‚àà R2N√óCemb. ThefusedembeddingsE\nfused\ncan\nControllableSceneOutpaintingcanassistinsceneexten-\nthenberepresentedas:\nsion. Byselectingappropriatebackgroundassetsandcom-\nbining them, such as stitching together continuous roads,\nE =E‚Ä≤ +LayerNorm(CrossAttn(Q,K,V)) (7) wecancontrollablyextendthescene.\nfused m\nNovel Urban Scene Generation enables the rapid con-\nMask Conditional Diffusion Model. Following LDM struction of 3D occupancy datasets. Imagine that we want\n[28], we conduct diffusion and denoising process on the to build a 3D semantic scene for a new city: we can cre-\ntriplane features T to learn our mask conditional diffu- atedifferentassetsbasedonLiDARpointclouds,andthen\nsion model D Œ∏. We add t steps of Gaussian noise to generateanovelurbanscenebasedontheseassets.\na clean triplane featur ‚àöes T 0 and obtain a noised triplane Removing trailing artifacts. SemanticKITTI [2] aggre-\nT t ‚àº q(T t|T 0) = N( Œ± tT,(1 ‚àí Œ± t)I), where N is the gatesmultipleLiDARframestocreatedense3Doccupancy\nGaussiandistribution,Œ± t\n=(cid:81)t\ni=1Œ± iandŒ± t =1‚àíŒ≤ twith scenes, butthisintroducestrailingartifactsformovingob-\navariancescheduleŒ≤ t.ThenthediffusionmodelD Œ∏canbe jectsinthegroundtruth,asshowninFig. 1(b). Ourmethod\ntrainedwiththemean-squarederrorloss: can effectively remove these artifacts and utilizes existing\nobjectassetstogeneratenewobjects.\nL=E ‚à•T ‚àíD (Concat(T ,M),t)‚à• (8)\nt‚àº[1,T] 0 Œ∏ t 2 4.Experiments\nTo support mask conditional generation, we inject the\n4.1.Datasets\nfusedembeddingE obtainedfromEq. 7intothecross\nfused\nattention of the diffusion model. In addition, we concate- WeconductourexperimentsontheSemanticKITTI[2]and\nnatethetrimaskwithT tofurtherenhancetheguidanceof CarlaSC [37] datasets. SemanticKITTI dataset is a large-\nt\nthe mask. Following classifier-free guidance [10], we ran- scalereal-worldbenchmarkforsemanticsceneunderstand-\ndomlysetthetrimasktozeroduringtrainingtosimulatethe inginautonomousdriving. Itcontains20semanticclasses.\neffectofnotusingthetrimask. Each scene is represented by a 256√ó256√ó32 voxel grid\nwith a voxel resolution of 0.2m. CarlaSC dataset is a syn-\n3.4.DownstreamApplications\nthetic dataset with labels for 11 semantic classes, gener-\nUnlike unconditional scene generation [16], our SSEditor ated using the CARLA simulator. Each scene has a res-\ncanflexiblyhandlevariousdownstreamtasksbasedonthe olution of 128√ó128√ó8, covering an area of 25.6 meters\ncreated assets, such as controllable scene inpainting and around the vehicle, with a height of 3 meters. Addition-\ncontrollable scene outpainting. Note that our method does ally,wevalidatedthecross-datasettransferabilityofSSEd-\nnotrequirearesamplingstrategy[20]. itor on Occ3D-Waymo [33]. We only included the occu-\nControllable Scene Inpainting can facilitate basic scene pancy labels from Occ3D-Waymo [33] as trimasks in our\nediting,suchasaddingorremovingobjects. Basedonthis, assetlibraryandthensimulatedthegenerationofunknown\n5\nTrimask Assets Ground Truth Inception Score (IS) [30] evaluates both the quality and\ndiversity of generated samples by computing a statistical\n‚Ä¶\nscorefromtheInceptionnetwork.\nKernel Inception Distance (KID) [4] computes the\nRoad Sidewalk Car Other-vehicle\nsquaredMaximumMeanDiscrepancy(MMD)betweenthe\n1.Remove Objects: Car, Other-vehicle, ‚Ä¶ real and generated data distributions using features ex-\ntractedfromtheInceptionnetwork.\nCar Other-vehicle Bicycle\n‚Ä¶ Precision measures the proportion of generated samples\nthat fall within the support of the real data distribution,\nwhile Recall measures the proportion of the real data dis-\n2.Editing background: Road, Sidewalk, ‚Ä¶\ntributioncoveredbythegeneratedsamples.\n‚Ä¶ Inaddition,weusetheintersectionoverunion(IoU)and\nmeanIOU(mIoU)metricstoevaluatetheoverallscenere-\nconstructionqualityandthereconstructionqualityforeach\nRoad Sidewalk Vegetation\nclass,respectively.\n3.Add Objects: Car, Other-vehicle, ‚Ä¶\nCar Other-vehicle Bicycle 4.4.QuantitativeResults\n‚Ä¶\nGeneration. Table 1 provides quantitative results on\nSmeanticKITTIandCarlaSCcomparingwithSSD[15]and\nSemCity [16]. In overall generation quality and diversity,\nFigure4.Thedetailsofediting3DsceneswithSSEditor:1.When\nourSSEditoroutperformsthepreviousmethods[15,16]on\nthemaskofanobjectissetto0,thecorrespondingobjectcanbe\nSemanticKITTI [2], particularly in FID and recall, where\ncompletelyremoved. 2. Thebackgroundcanbeedited, suchas\nweachieveimprovementsof21.68%and39%,respectively,\nwideningroadstosimulateheaviertraffic.3.Objectscanbeadded\ntotheeditedscene. comparedtoSemCity. OnCarlaSC[37],SSEditorleadsin\nall metrics except for IS, with FID improving by 63.04%\nover SemCity. Note that SemCity do not disclose which\nurban scenes. Note that we disregard the Occ3D-Waymo imagesetsareusedforevaluation,makingtheresultsnon-\ncategoriesnotpresentinSemanticKITTI. reproducible. Toensureafaircomparison, wetrainonthe\ntrainingsetandgeneratescenesonthevalidationsettoob-\n4.2.ImplementationDetails\ntaintheevaluationresults.\nAll experiments are conducted on a single NVIDIA RTX Semantic Scene Completion. We assess the controlla-\n3090-24G GPU. For the 3D scene autoencoder, the batch bility and scene reconstruction capabilities of our method\nsizeissetto4,whileforthecontrollablemask-to-scenegen- through semantic scene completion. Table 2 demonstrates\neration,thebatchsizeissetto1.Thedownsamplingfactors thatSSEditorperformswellontheSemanticKITTIvalida-\nareconfiguredasd = 2andd = 1. ThelossweightŒ±in tion set. We only reference two state-of-the-art methods\nz\ntheEq. 1issetto1, thelatentchanneloftriplanefeatures from different modalities, as other unconditional diffusion\nT equals 16 and the embedding channel C = 64. The models[15,16]lacktheabilitytoreconstruct3Dsemantic\nemb\nlearningratefortheautoencoderis1e-3,whilethelearning scenes. TheIoUmetricindicatesthatourmethodprovides\nrateforthediffusionmodelis1e-4. Followingthesettings strong control over the position and size of objects during\nof [15, 16], the sampling time steps is set to 100 during scenegeneration,whilethemIoUscorereflectsarobustun-\nbothtrainingandtestingofthediffusionmodel. Weutilize derstandingofthesemanticsofthegeneratedobjects.\nDDPMsamplingstrategy[11]fordownstreamtasks,omit-\n4.5.QualitativeResults\ntingtheneedfortheresamplingstrategyinRePaint[20].\nGeneration. Fig. 5 showcases the qualitative results\n4.3.EvaluationMetrics\nof the proposed SSEditor and SemCity [16] on the Se-\nWeadoptevaluationmetricsfrompriorworks[16,32,40] manticKITTI [2] and CarlaSC [37] datasets. While Sem-\nrendering3Dscenesinto2Dimagesandusetraditional2D City [16] effectively generates a variety of scenes using\nevaluationmetricstoassessthequalityanddiversityofgen- triplane representations, it lacks sufficient control, making\neratedscenes: scene customization challenging. In contrast, SSEditor al-\nFre¬¥chet Inception Distance (FID) [9] measures the simi- lowsforprecisegenerationof3Dscenesguidedbymasks,\nlarity between the real and generated data distributions by offering enhanced controllability. In Fig. 5, we create tri-\ncomparing their feature statistics in the latent space of the masks based on ground truth to verify our method‚Äôs con-\nImageNet-pretrainedInceptionnetwork. trollability. TheresultsdemonstratethatSSEditorexcelsin\n6\nSemCity SSEditor(Ours) Ground Truth\nTrimasks\n‚Ä¶\n‚Ä¶\n‚Ä¶\nCar Bicycle Motorcycle Truck Other-vehicle Person Bicyclist Motorcyclist Patking Sidewalk\nSemanticKITTI\nPole Fence Building Road Vegetation Trunk Terrain Traffic-sign Other-ground\n‚Ä¶\n‚Ä¶\nVehicle Pole Fences Building Road Other Vegetation Ground Sidewalk Pedestrian CarlaSC\nFigure5. VisualizationofsemanticscenegenerationcomparingwithSemCity[16]onSemanticKITTI[2]andCalarSC[37]. Underthe\nguidanceofthetrimaskasacondition,SSEditordemonstratesitsstrongcontrollability.\ncontrollingboththeoverallbackground(e.g.,road,vegeta- createmoredynamicscenarios.\ntion)andspecificobjects(e.g.,vehicles,pedestrians). Novel Scene Generation. To further validate the control-\nSceneEditing.Fig.4highlightsthedetailsofsceneediting labilityofSSEditoringeneratingnewscenes,weapplythe\nwith SSEditor. By setting the trimask of a target object or trained model to the Occ-3D Waymo dataset [33]. We ad-\nbackground to zero, we can effectively remove it from the just the trimasks from Occ-3D Waymo through interpola-\nscene.Wecanalsoeditbackgroundassetsformorerealistic tiontoalignwiththestandardsizeoftrimasksinourasset\nscenarios,likecreatingfour-laneoreight-laneassets. Once library,duetothedifferentresolutionsofthedatasets. Note\nthebackgroundisadjusted,wecanaddobjects,likeincreas- that we only create trimasks for categories that appear in\ningthenumberofcarstosimulatehighertrafficvolumes,to SemanticKITTI[2]. ThegeneratedresultsinFig. 6demon-\n7\nraC\nklawediS\nraC\ngniktaP\nraC\nngiS\nciffarT\nraC\nklawediS\nraC\neloP\nRoad\nVegetation\nOther\nVehicle\nRoad\nPerson\nRoad\nGround\nRoad\nOther\nRoad\nOcc-3D Waymo\nCreate a new urban scene Method FID‚Üì KID‚Üì IS‚Üë Prec.‚Üë Rec.‚Üë\nGround Truth\nw/ogeometricbranch 60.32 0.05 2.45¬±0.15 0.24 0.28\nw/osemanticbranch 54.96 0.05 2.49¬±0.13 0.27 0.37\nw/osemantictokens 53.67 0.04 2.49¬±0.12 0.27 0.38\nw/omaskconcat 54.08 0.04 2.43¬±0.17 0.23 0.19\nSSEditor(ours) 47.93 0.03 2.55¬±0.14 0.31 0.51\nScene Mask 1\n‚Ä¶\nTable3. Ablationstudiesonscenegeneration. Wevalidatedthe\n‚Ä¶\neffectiveness of the geometric branch, semantic branch, and the\nconcatenatedinputofthetrimaskonSemanticKITTI[2].\nMethod Steps Sampling InferenceTime\nResampling[20] 56.44s\n100\nScene Ma ‚Ä¶sk 2 DDPM[11] 13.40s\n‚Ä¶ Resampling[20] 13.89s\nSSEditor 20\nDDPM[11] 3.66s\nResampling[20] 6.91s\n10\nFigure6.Createanovelurbanscenefrommasks.Thenovelscene DDPM[11] 2.08s\ngenerationistestedontheunseenOcc-3DWaymodataset[33].\nTable4.Ablationstudiesonsamplingstrategy.Theinferencetime\nisreportedbasedon100sampleruns.\nstratethatSSEditorcaneffectivelyadapttonewscenegen-\neration,enablingtherapidcreationofurbanenvironments.\nsensitive to surrounding objects, which can lead to inac-\n4.6.AblationStudies\ncuracies.These issues negatively affect the performance of\nWeconductablationexperimentsontheSemanticKITTI[2] downstream tasks that rely on high-quality scene genera-\nvalidationsettoassessthecontributionofeachcomponent tion. Predictingsmallobjectsinsemanticscenecompletion\nofSSEditor,asshowninTable3. isinherentlychallengingduetotheirlowvisibilityandthe\nFirst, we evaluate the effectiveness of the geometric complex interactions they have with the environment, re-\nbranchbyretainingthesemanticbranchandconcatenating sultinginlowermIoUperformance. Futureworkcouldfo-\nthe trimask with the noised triplane T as input. Next, we cusonaddressingthelong-taildistributionofdatabyincor-\nt\nremove the semantic branch, followed by the semantic to- porating more robust methods for representing and detect-\nkenswithinthebranch,toexaminetheirindividualimpact. ing small objects, as well as developing more fine-grained\nFinally, we input only the noised triplane T to assess the representationtechniquesthatcanimprovethehandlingof\nt\nrole of concatenating the trimask. In all ablation experi- thesechallengingcases.\nments, removing any component results in a performance\ndrop,highlightingthenecessityofeachcomponentforop- 6.Conclusion\ntimalperformance.\nIn this paper, we propose SSEditor, a two-stage control-\nAdditionally, as shown in Table 4, we compared two\nlable scene generation framework based on the diffusion\nsampling strategies: DDPM [11] and the resampling tech-\nmodel. In the first stage, we leverage a 3D scene autoen-\nnique from RePaint [20]. While resampling improves ob-\ncoder to learn triplane representations. We then create a\nject integration with the environment during generation, it\ntrimask asset library as a preparatory step for the second\ngreatly increases inference time for 3D scene generation.\nphase of training. In the second stage, we train a mask-\nIn contrast, our method employs traditional DDPM sam-\nconditional diffusion model for mask-to-scene generation,\npling, which maintains high quality and controllability in\nincorporating a geometric-semantic fusion module to en-\nboth scene inpainting and outpainting, while reducing in-\nhance the extraction of geometric and semantic informa-\nferencetime.\ntion. Experimental results on SemanticKITTI, CarlaSC,\nand Occ-3D Waymo demonstrate that our method outper-\n5.Limitations\nformsexistingunconditionaldiffusionapproaches,offering\nAlthough SSEditor demonstrates strong capabilities for superior controllability and high-quality scene generation.\ncontrollable scene generation, it still faces challenges with Moreover, SSEditorsupportsawiderangeofapplications,\ngenerating small objects, such as bicyclists and pedestri- includingthegenerationofnovel3Durbanscenes(suchas\nans. The generated areas sometimes contain incorrectly cross-dataset generation and road widening), controllable\nclassified voxels, and the model‚Äôs performance is highly generationofdynamicobjects,andsceneoutpainting.\n8\nReferences [13] Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang,\nYuQiao,andHongshengLi. Diffindscene: Diffusion-based\n[1] OmriAvrahami,DaniLischinski,andOhadFried. Blended\nhigh-quality3dindoorscenegeneration. InProceedingsof\ndiffusionfortext-driveneditingofnaturalimages. InPro-\ntheIEEE/CVFConferenceonComputerVisionandPattern\nceedings of the IEEE/CVF conference on computer vision\nRecognition,pages4526‚Äì4535,2024. 1\nandpatternrecognition,pages18208‚Äì18218,2022. 2\n[14] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\n[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-\nNiloyJMitra. Holodiffusion:Traininga3ddiffusionmodel\nzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-\nusing 2d images. In Proceedings of the IEEE/CVF con-\nmantickitti: A dataset for semantic scene understanding of\nference on computer vision and pattern recognition, pages\nlidar sequences. In Proceedings of the IEEE/CVF inter-\n18423‚Äì18433,2023. 1\nnationalconferenceoncomputervision, pages9297‚Äì9307,\n[15] JuminLee,WoobinIm,SebinLee,andSung-EuiYoon.Dif-\n2019. 1,5,6,7,8\nfusion probabilistic models for scene-scale 3d categorical\n[3] Maxim Berman, Amal Rannen Triki, and Matthew B data. arXivpreprintarXiv:2301.00527,2023. 1,2,5,6\nBlaschko.Thelova¬¥sz-softmaxloss:Atractablesurrogatefor\n[16] Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong\nthe optimization of the intersection-over-union measure in\nSeon,andSung-EuiYoon.Semcity:Semanticscenegenera-\nneuralnetworks. InProceedingsoftheIEEEconferenceon\ntionwithtriplanediffusion.InProceedingsoftheIEEE/CVF\ncomputervisionandpatternrecognition,pages4413‚Äì4421,\nConference on Computer Vision and Pattern Recognition,\n2018. 4\npages28337‚Äì28347,2024. 1,2,3,5,6,7\n[4] Miko≈ÇajBin¬¥kowski,DanicaJSutherland,MichaelArbel,and\n[17] MuhengLi,YueqiDuan,JieZhou,andJiwenLu.Diffusion-\nArthur Gretton. Demystifying mmd gans. arXiv preprint\nsdf: Text-to-shape via voxelized diffusion. In Proceedings\narXiv:1801.01401,2018. 6\noftheIEEE/CVFconferenceoncomputervisionandpattern\n[5] EricRChan,ConnorZLin,MatthewAChan,KokiNagano, recognition,pages12642‚Äì12651,2023. 2\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\n[18] Chenguo Lin and Yadong Mu. Instructscene: Instruction-\nGuibas,JonathanTremblay,SamehKhamis,etal. Efficient\ndriven3dindoorscenesynthesiswithsemanticgraphprior.\ngeometry-aware3dgenerativeadversarialnetworks. InPro-\narXivpreprintarXiv:2402.04717,2024. 2,3\nceedings of the IEEE/CVF conference on computer vision\n[19] Yuheng Liu, Xinke Li, Xueting Li, Lu Qi, Chongshou Li,\nandpatternrecognition,pages16123‚Äì16133,2022. 2\nandMing-HsuanYang. Pyramiddiffusionforfine3dlarge\n[6] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin scenegeneration.arXivpreprintarXiv:2311.12085,2023.1,\nChen, and Silvio Savarese. 3d-r2n2: A unified approach 2,3\nforsingleandmulti-view3dobjectreconstruction. InCom-\n[20] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher\nputerVision‚ÄìECCV2016: 14thEuropeanConference,Am-\nYu,RaduTimofte,andLucVanGool. Repaint: Inpainting\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nusingdenoisingdiffusionprobabilisticmodels. InProceed-\nings,PartVIII14,pages628‚Äì644.Springer,2016. 2\nings of the IEEE/CVF conference on computer vision and\n[7] Abdelrahman Eldesokey and Peter Wonka. Build-a-scene: patternrecognition,pages11461‚Äì11471,2022. 2,3,5,6,8\nInteractive3dlayoutcontrolfordiffusion-basedimagegen-\n[21] OscarMichel,RoiBar-On,RichardLiu,SagieBenaim,and\neration. arXivpreprintarXiv:2408.14819,2024. 2,3\nRana Hanocka. Text2mesh: Text-driven neural stylization\n[8] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, for meshes. In Proceedings of the IEEE/CVF Conference\nDevi Parikh, and Yaniv Taigman. Make-a-scene: Scene- onComputerVisionandPatternRecognition,pages13492‚Äì\nbased text-to-image generation with human priors. In Eu- 13502,2022. 2\nropean Conference on Computer Vision, pages 89‚Äì106.\n[22] ParitoshMittal,Yen-ChiCheng,ManeeshSingh,andShub-\nSpringer,2022. 2\nham Tulsiani. Autosdf: Shape priors for 3d comple-\n[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, tion, reconstruction and generation. In Proceedings of\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a theIEEE/CVFConferenceonComputerVisionandPattern\ntwotime-scaleupdateruleconvergetoalocalnashequilib- Recognition,pages306‚Äì315,2022. 2\nrium. Advances in neural information processing systems, [23] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui\n30,2017. 6 Huang,Shi-MinHu,KeLi,andLeonidasGuibas. Difffacto:\n[10] Jonathan Ho and Tim Salimans. Classifier-free diffusion Controllablepart-based3dpointcloudgenerationwithcross\nguidance. arXivpreprintarXiv:2207.12598,2022. 5 diffusion. In Proceedings of the IEEE/CVF International\n[11] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif- ConferenceonComputerVision,pages14257‚Äì14267,2023.\nfusionprobabilisticmodels. Advancesinneuralinformation 2\nprocessingsystems,33:6840‚Äì6851,2020. 2,3,6,8 [24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\n[12] HaoyiJiang,TianhengCheng,NaiyuGao,HaoyangZhang, Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and\nTianwei Lin, Wenyu Liu, and Xinggang Wang. Sym- MarkChen. Glide:Towardsphotorealisticimagegeneration\nphonize 3d semantic scene completion with contextual in- andeditingwithtext-guideddiffusionmodels.arXivpreprint\nstancequeries. InProceedingsoftheIEEE/CVFConference arXiv:2112.10741,2021. 2\nonComputerVisionandPatternRecognition,pages20258‚Äì [25] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,\n20267,2024. 5 andMarkChen. Hierarchicaltext-conditionalimagegener-\n9\nationwithcliplatents. arXivpreprintarXiv:2204.06125,1 RoboticsandAutomationLetters,7(3):8439‚Äì8446,2022. 5,\n(2):3,2022. 2 6,7\n[26] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, [38] Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin\nSanjaFidler,andFrancisWilliams. Xcube: Large-scale3d Ma, Yikang Li, Yuenan Hou, and Yu Qiao. Scpnet: Se-\ngenerativemodelingusingsparsevoxelhierarchies. InPro- manticscenecompletiononpointcloud. InProceedingsof\nceedingsoftheIEEE/CVFConferenceonComputerVision the IEEE/CVF conference on computer vision and pattern\nandPatternRecognition,pages4209‚Äì4219,2024. 1,2 recognition,pages17642‚Äì17651,2023. 5\n[27] LuisRoldao,RaouldeCharette,andAnneVerroust-Blondet. [39] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir\nLmscnet: Lightweight multiscale 3d semantic completion. Mech, and Ulrich Neumann. Disn: Deep implicit surface\nIn2020InternationalConferenceon3DVision(3DV),pages networkforhigh-qualitysingle-view3dreconstruction. Ad-\n111‚Äì119.IEEE,2020. 4 vancesinneuralinformationprocessingsystems,32,2019.\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, 2\nPatrick Esser, and Bjo¬®rn Ommer. High-resolution image [40] Guangyao Zhai, Evin Pƒ±nar O¬®rnek, Shun-Cheng Wu, Yan\nsynthesis with latent diffusion models. In Proceedings of Di,FedericoTombari,NassirNavab,andBenjaminBusam.\nthe IEEE/CVF conference on computer vision and pattern Commonscenes:Generatingcommonsense3dindoorscenes\nrecognition,pages10684‚Äì10695,2022. 2,5 withscenegraphs.AdvancesinNeuralInformationProcess-\n[29] ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee, ingSystems,36,2024. 1,2,6\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad [41] LvminZhang, AnyiRao, andManeeshAgrawala. Adding\nNorouzi. Palette: Image-to-image diffusion models. In conditional control to text-to-image diffusion models. In\nACMSIGGRAPH2022conferenceproceedings,pages1‚Äì10, ProceedingsoftheIEEE/CVFInternationalConferenceon\n2022. 2 ComputerVision,pages3836‚Äì3847,2023. 2\n[30] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\n[42] GuangcongZheng,XianpanZhou,XueweiLi,ZhongangQi,\nCheung,AlecRadford,andXiChen. Improvedtechniques\nYingShan,andXiLi. Layoutdiffusion: Controllablediffu-\nfortraininggans.Advancesinneuralinformationprocessing\nsionmodelforlayout-to-imagegeneration. InProceedings\nsystems,29,2016. 6 oftheIEEE/CVFConferenceonComputerVisionandPat-\n[31] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, ternRecognition,pages22490‚Äì22499,2023. 2\nJiajunWu,andGordonWetzstein.3dneuralfieldgeneration\n[43] LinqiZhou,YilunDu,andJiajunWu. 3dshapegeneration\nusing triplane diffusion. In Proceedings of the IEEE/CVF\nandcompletionthroughpoint-voxeldiffusion. InProceed-\nConference on Computer Vision and Pattern Recognition,\ningsoftheIEEE/CVFinternationalconferenceoncomputer\npages20875‚Äì20886,2023. 1\nvision,pages5826‚Äì5835,2021. 1\n[32] JiapengTang,YinyuNie,LevMarkhasin,AngelaDai,Justus\nThies,andMatthiasNie√üner. Diffuscene: Denoisingdiffu-\nsionmodelsforgenerativeindoorscenesynthesis. InPro-\nceedings of the IEEE/CVF conference on computer vision\nandpatternrecognition,pages20507‚Äì20518,2024. 1,2,3,\n6\n[33] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao,\nHuitong Yang, Yue Wang, Yilun Wang, and Hang Zhao.\nOcc3d: A large-scale 3d occupancy prediction benchmark\nfor autonomous driving. Advances in Neural Information\nProcessingSystems,36,2024. 5,7,8\n[34] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany,\nSanja Fidler, Karsten Kreis, et al. Lion: Latent point dif-\nfusionmodelsfor3dshapegeneration. AdvancesinNeural\nInformationProcessingSystems,35:10021‚Äì10039,2022. 2\n[35] Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang,\nZhiyong Cui, Haiyang Yu, and Jiwen Lu. Occsora: 4d\noccupancy generation models as world simulators for au-\ntonomousdriving. arXivpreprintarXiv:2405.20337,2024.\n1,2\n[36] TengfeiWang,TingZhang,BoZhang,HaoOuyang,Dong\nChen, Qifeng Chen, and Fang Wen. Pretraining is all\nyou need for image-to-image translation. arXiv preprint\narXiv:2205.12952,2022. 2\n[37] Joey Wilson, Jingyu Song, Yuewei Fu, Arthur Zhang, An-\ndrew Capodieci, Paramsothy Jayakumar, Kira Barton, and\nMaani Ghaffari. Motionsc: Data set and network for real-\ntime semantic mapping in dynamic environments. IEEE\n10",
    "pdf_filename": "SSEditor_Controllable_Mask-to-Scene_Generation_with_Diffusion_Model.pdf"
}