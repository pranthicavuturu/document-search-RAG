{
    "title": "UnderReview-ProceedingsTrack1–15,2024 SymmetryandGeometryinNeuralRepresentations",
    "abstract": "In machine learning, a loss function measures the difference between model predictions and ground-truth (or target) values. For neural network models, visualizing how this loss changes as model parameters are varied can provide insights into the local structure of the so-called loss landscape (e.g., smoothness) as well as global properties of the underlying model (e.g., generalization performance). While various methods for visualizing the loss landscape have been proposed, many approaches limit sampling to just one or two direc- tions, ignoring potentially relevant information in this extremely high-dimensional space. Thispaperintroducesanewrepresentationbasedontopologicaldataanalysisthatenables the visualization of higher-dimensional loss landscapes. After describing this new topolog- ical landscape profile representation, we show how the shape of loss landscapes can reveal newdetailsaboutmodelperformanceandlearningdynamics,highlightingseveralusecases, including image segmentation (e.g., UNet) and scientific machine learning (e.g., physics- informedneuralnetworks). Throughtheseexamples,weprovidenewinsightsintohowloss landscapes vary across distinct hyperparameter spaces: we find that the topology of the loss landscape is simpler for better-performing models; and we observe greater variation in the shape of loss landscapes near transitions from low to high model performance.",
    "body": "UnderReview-ProceedingsTrack1–15,2024 SymmetryandGeometryinNeuralRepresentations\nVisualizing Loss Functions as Topological Landscape Profiles\nCaleb Geniesse∗ cgeniesse@lbl.gov\nLawrence Berkeley National Laboratory\nJiaqing Chen∗ jchen501@asu.edu\nArizona State University\nTiankai Xie∗ txie21@asu.edu\nArizona State University\nGe Shi geshi@ucdavis.edu\nUniversity of California, Davis\nYaoqing Yang yaoqing.yang@dartmouth.edu\nDartmouth College\nDmitriy Morozov dmorozov@lbl.gov\nLawrence Berkeley National Laboratory\nTalita Perciano tperciano@lbl.gov\nLawrence Berkeley National Laboratory\nMichael W. Mahoney mmahoney@stat.berkeley.edu\nICSI, LBNL, and UC Berkeley\nRoss Maciejewski rmacieje@asu.edu\nArizona State University\nGunther H. Weber ghweber@lbl.gov\nLawrence Berkeley National Laboratory\nEditors: List of editors’ names\nAbstract\nIn machine learning, a loss function measures the difference between model predictions\nand ground-truth (or target) values. For neural network models, visualizing how this loss\nchanges as model parameters are varied can provide insights into the local structure of the\nso-called loss landscape (e.g., smoothness) as well as global properties of the underlying\nmodel (e.g., generalization performance). While various methods for visualizing the loss\nlandscape have been proposed, many approaches limit sampling to just one or two direc-\ntions, ignoring potentially relevant information in this extremely high-dimensional space.\nThispaperintroducesanewrepresentationbasedontopologicaldataanalysisthatenables\nthe visualization of higher-dimensional loss landscapes. After describing this new topolog-\nical landscape profile representation, we show how the shape of loss landscapes can reveal\nnewdetailsaboutmodelperformanceandlearningdynamics,highlightingseveralusecases,\nincluding image segmentation (e.g., UNet) and scientific machine learning (e.g., physics-\ninformedneuralnetworks). Throughtheseexamples,weprovidenewinsightsintohowloss\nlandscapes vary across distinct hyperparameter spaces: we find that the topology of the\nloss landscape is simpler for better-performing models; and we observe greater variation in\nthe shape of loss landscapes near transitions from low to high model performance.\nKeywords: Topological data analysis, loss landscapes, model diagnosis\n∗ Equal contribution.\n©2024C.Geniesseet al.\n4202\nvoN\n91\n]GL.sc[\n1v63121.1142:viXra\nGeniesse et al.\n1. Introduction\nAcentralaimofmachinelearning(SimonyanandZisserman,2014;Heetal.,2016;Krizhevsky\net al., 2017; Vaswani et al., 2017; Devlin et al., 2018; Liu et al., 2019) is to learn the un-\nderlying structure of data. This learning process is governed by a loss function, denoted as\nL(θ), where θ is the set of parameters (or weights) defining, e.g., a neural network. The loss\nfunction measures the difference between the outputs of a neural network and ground-truth\nvalues. In this way, the loss reflects how good (or bad) the current weights are at making\ncorrect predictions and how to adjust these weights during training. Given the important\nrole that the loss function plays during learning, examining it with respect to a neural net-\nwork’s weights—by visualizing the so-called loss landscape—can provide valuable insights\ninto both network architecture and learning dynamics (Goodfellow et al., 2014; Im et al.,\n2016; Li et al., 2018; Yao et al., 2020; Martin and Mahoney, 2021; Martin et al., 2021; Yang\net al., 2022b, 2021; Zhou et al., 2023; Sakarvadia et al., 2024; Khan et al., 2024). Indeed,\nthe loss landscape has been essential for understanding other aspects of deep learning, in-\ncluding generalizability (Cha et al., 2021; Yang et al., 2021) and robustness (Kurakin et al.,\n2016; Djolonga et al., 2021; Yang et al., 2022a). In addition, the loss landscape has been\ncharacterized in the context of scientific machine learning, e.g., to understand why different\nphysics-informed architectures and loss functions are often brittle, exhibiting failure modes,\nand are hard to optimize (Krishnapriyan et al., 2021; Rathore et al., 2024; Xie et al., 2024).\nDespite its promise and appeal, loss landscape visualization is a complex and often\nbespoke process. Indeed, exploring and extracting insights from a loss landscape—which\nis inherently high-dimensional, with as many dimensions as the number of parameters in\nthe model—is challenging to do, especially when trying to visualize directly on a two-\ndimensional screen. Most efforts to date have focused on projecting the loss function down\nto one or two dimensions. Goodfellow et al. (2014) proposed a random-direction-based\napproach, where model parameters are interpolated along a one-dimensional path to see\nhow the loss changes. Im et al. (2016) later introduced an extension of this method which\ninvolves projecting the loss landscape onto a two-dimensional space using barycentric inter-\npolation between triplets of points and bilinear interpolation between quartets of points. Li\net al. (2018) continued improving the resolution of loss landscapes by introducing filter-wise\nnormalization to remove the scaling effects incurred by previous approaches. A more so-\nphisticated approach to visualizing the loss landscape leverages the Hessian to define more\nrelevant directions along which the model can be interpolated. More recently, Yao et al.\n(2020) used the top two Hessian eigenvectors as directions, thereby capturing more impor-\ntant changes in the underlying loss landscape. While various methods have been proposed,\nmost applications have limited sampling to just one or two directions. Importantly, by\nrestricting the sampling of loss landscapes to two dimensions, whether it be using random\nor Hessian-based directions, we ignore potentially informative information captured by ad-\nditional dimensions (e.g., the eigenvectors associated with the dominant eigenvalues of the\nHessian matrix).\nTowardscharacterizinghigher-dimensionallosslandscapes,herewetakeinspirationfrom\ntopological data analysis (TDA). Specifically, we use a merge tree to encode the critical\npoints of an n-dimensional neural network loss landscape, and we represent the merge\ntree as a topological landscape profile. The merge tree allows us to capture important\n2\nVisualizing Loss Functions as Topological Landscape Profiles\nfeatures in an arbitrary-dimensional loss landscape; and by using the topological landscape\nprofile, we are able to re-represent this information in two dimensions. Note, we first\nexplored applications of TDA in our previous work (Xie et al., 2024). There, we focused on\nquantifying loss landscapes and developing new topology-based metrics, but the sampling\nwas limited to two dimensions. Here, we focus on developing a new visual representation\nof the loss landscape which allows us to visualize higher-dimensional loss landscapes. We\ndemonstrate the utility of our new topological landscape profile representation by exploring\nhigher-dimensional loss landscapes, i.e., sampling along more directions and representing\nthese higher-dimensional subspaces as topological landscape profiles. This approach allows\nus to extract more information from the additional dimensions we consider. While our\napproach technically can work with arbitrary dimensional loss landscapes, in practice we\nare limited by sampling. As such, here we limit ourselves to three and four-dimensional loss\nlandscapes.\nWe demonstrate the versatility of our new topological profile representations of loss\nlandscapes and our complementary visualization tool through several use case scenarios.\nThrough these examples, we show the many different ways our tool can be used to extract\ninsights about neural network models based on our topological landscape profiles and by\ncomparing loss landscapes across different hyperparameters. In doing so, we also provide\nnew insights into how loss landscapes vary across distinct hyperparameter spaces, finding\nthat (1) the topology of loss landscapes is simpler for better-performing models, and (2)\nthis topology often exhibits greater variability near transitions from low to high model\nperformance. For example, for the scientific machine learning models we study here, we\nobserve a sharp transition from low to high error as one of the physical parameters is\nincreased. We find that models with lower error have smoother and more funnel-like loss\nlandscapes, whereas models with higher error have flatter (but rougher) and more bowl-like\nloss landscapes. Along this transition from low to high error, models have more variably\nshaped loss landscapes (i.e., different shapes are observed across different random seeds).\n2. Background\n2.1. Topological Data Analysis\nTopological data analysis (TDA) aims to reveal the global underlying structure of data.\nTDA is particularly useful for studying high-dimensional data or functions, where direct\nvisualization (in two or three dimensions) is inherently not possible. We leverage ideas\nand algorithms from TDA to study the global structure of the loss function—that is, the\nshape of the so-called loss landscape. Much of TDA is based on the more general idea\nof “connectedness.” In the context of a loss function, we are interested in the number of\nminima (i.e., unique sets of parameters for which the loss is locally minimized) and how\n“prominent” they are (i.e., measuring how many other sets of neighboring parameters have\nahigherlossthantheparametersetthatminimizesthelossfunction). Suchinformationcan\nbe obtained from a persistence diagram (i.e., captured by the zero-dimensional persistent\nhomology) and the so-called merge tree.\nA merge tree (Carr et al., 2003; Heine et al., 2016) tracks connected components of\nsub-level sets L−(v) = {x ∈ D;x ≤ v} as a threshold, v, is increased. The merge tree\nencodes changes in the loss landscape as nodes in a tree-like structure. The local minima\n3\nGeniesse et al.\nare represented by degree-one nodes, which are connected to other local minima through\na single saddle point. The saddle points connecting different minima are represented by\ndegree-three nodes (each connecting two local minima and one other saddle point). Loss\nfunctionsoftendisplaymanyshallowlocalminimawithlowbarriers(i.e.,thevaluedifference\nbetweentheminimaandtheconnectingsaddlepointissmall)correspondingto“short-lived”\nconnected components that merge quickly with other connected components.\nIn our work, we use the merge tree to extract the underlying structure of a loss land-\nscape. Wethenusethisextractedinformationtoconstructourtopologicallandscapeprofile\nrepresentations. Since the merge tree can be computed for an arbitrary dimensional loss\nlandscape, we can use it to construct our representation for higher-dimensional loss land-\nscapes, which would otherwise be difficult to visualize.\n2.2. Topological Landscape Profiles\nTo enable the visualization of higher-dimensional loss landscapes, we introduce a new topo-\nlogicallandscapeprofilerepresentationthatcapturestheminimaandsaddlepointsencoded\nbymergetrees. ThisworkbuildsuponOesterlingetal.(2013),whofirstintroducedtheidea\nof representing high-dimensional data clusters (and their nesting) as hills in a landscape,\nwhere the height, width, and shape of each hill encodes the coherence, size, and stability\nof each cluster. To construct the landscape profile, they first use a merge tree to encode\nthe distribution (or density) of the data points. They then use this merge tree to construct\nthe landscape profile, by representing maxima in the merge tree as hills in the landscape,\nwhere the size and shape of each hill are determined by characteristics like persistence and\nthe number of points along the corresponding branch. In the context of loss functions, we\nare more interested in minima than maxima, so here we introduce a new version of this\ntopological landscape profile, using the metaphor of valleys (or basins) rather than hills.\n3. Methods\nTo construct our new topological landscape profile representations, we build on traditional\nloss landscape sampling approaches and leverage tools from TDA to capture the underlying\nshape (or topology) of the sampled loss landscapes. First, we select n vectors (n ≤ m) to\ndefine an n-dimensional subspace (Figure 1.1), where m is the number of parameters in the\nmodel. We then sample a set of points from this subspace, where each point corresponds to\na distinct set of parameters. We evaluate the loss for each set of parameters and represent\nthe set of points (and their associated loss values) as an unstructured grid (Figure 1.2).\nWe then compute a merge tree to capture the topology of the n-dimensional loss landscape\n(Figure 1.3), and we construct our final topological landscape profile based on this merge\ntree (Figure 1.4). In this section, we go into more detail about each of these steps.\n3.1. Loss Landscape Construction and Representation\nIn this work, we limit our analysis to Hessian-based loss landscapes. We calculate the top\nn Hessian eigenvectors using PyHessian (Yao et al., 2020) (Figure 1.1) and then sample\nalong the subspace spanned by these directions (Figure 1.2). The idea is that by using\nthe eigenvectors associated with the top n largest eigenvalues, we can visualize the most\n4\nVisualizing Loss Functions as Topological Landscape Profiles\nPrevious Approaches\nTop Hessian Eigenvalues\n012345678910111213141516171819\nNeural Network Subspace of Top-2 Hessian Vectors 2-D Loss Landscape\nTop Hessian Eigenvalues\nTop Hessian Eigenvalues\n012345678910111213141516171819\nSubspace of Top-3 Hessian Vectors 3-D Loss Landscape\nTop Hessian Eigenvalues\n012345678910111213141516171819\nPartial Hessian Spectrum\n(Top-20 Hessian Vectors)\n012345678910111213141516171819\n1 Subspace of Top-4 Hessian Vectors 2 4-D Loss Landscape 3 Merge Tree 4 Topological Landscape\nFigure 1: Our topological landscape profiles enable the visualization of higher-dimensional\nloss landscapes by capturing their underlying shape (or topology). Here we show\nloss landscapes based on the top n Hessian eigenvectors. See Section 3 for details.\nsignificant local loss fluctuations for a given model. Given the n orthogonal directions, we\ngeneralize the approach taken by Li et al. (2018) by expanding the subspace beyond two\ndimensions. Formally, we perturb trained model parameters along the n directions and\nevaluate the loss L as follows:\nf(α ...α ) = L(θ+Σn α δ ), (1)\n1 n i=1 i i\nwhere α ...α are the coordinates in the n-dimensional subspace, δ is the i-th direction\n1 n i\nin that subspace, and θ is the original model. As such, each coordinate corresponds to\na point associated with a computed loss value, and the collection of loss values forms an\nn-dimensional loss landscape. In this work, we use an equally spaced grid by taking each α\ni\nto be the set of equally spaced integers between 0 and r, where r is the resolution of each\ndimension in the grid. Here we use r = 41, such that the center of the grid corresponds to\nthe original model, i.e., Σn α δ = 0.\ni=1 i i\nGiven an n-dimensional loss landscape, we can represent the sampled points as an\nunstructured grid, where each vertex in the grid is associated with n coordinates and a\nscalar loss value. Before we can characterize how the loss changes throughout the landscape\n(i.e., asparametersareperturbedfromonevertextothenext), weneedtodefinethespatial\nproximity(orconnectivity)ofverticesinthegridbasedonthesimilarityoftheircoordinates.\nHereweuseascalable,approximatenearestneighboralgorithmtoconstructaneighborhood\ngraph representation of the loss landscape (Dong et al., 2011). The neighborhood graph,\nproposed by Jaromczyk and Toussaint (1992), of a dataset D is a graph G = (D,E) where\ntwo points u and v are connected by an edge (u,v) ∈ E if they are similar. Here we focus\nonthek-nearestneighborgraph, whereeachpointisconnectedtothek mostsimilarpoints.\n5\nGeniesse et al.\nSize Minimum Saddle Point\nE0\nE0\nE0 Loss\nLoss E0 Mean of E0 E1\nE1\nLoss Mean of E1 E2\nE1 E2 E2\nLoss Mean of E2\nE1 E2\nEdge\n(A) (B) Basin Color (C)\nFigure 2: Representing the merge tree as a topological landscape profile. In (A) we show\na single basin corresponding to a merge tree with a single branch, and in (B) we\nshow multiple basins corresponding to multiple branches. In (C) we color the\nbasins based on their average loss.\nWe also use a symmetric version of this graph, where points are only considered neighbors\nif each point is a neighbor of the other. In this case, an edge (u,v) is pruned from the\ngraph if u is not one of the k nearest points to v, or vice versa. We note that this approach\ninvolves selecting an appropriate value for the k parameter. Here we use k = 4×n, such\nthattheconnectivityissimilartothespatialproximityofpixelsinanimage(i.e., eachpixel\nhaving k = 8 neighbors, corresponding to the left, right, top, bottom, and all four corners).\n3.2. Topological Structures and Landscape Profiles\nAfter defining the subspace and computing the loss landscape, we perform topological data\nanalysis to extract and summarize the most important features. In this work, we use a\nmerge tree to extract key information from the loss landscape, which we then use to define\nour topological landscape profile. We compute the merge tree for each loss landscape using\nthe Topology ToolKit (TTK), developed by Bin Masood et al. (2021).\nGivenamergetree,wethenconstructthetopologicallandscapeprofileusingthemethod\nproposed by Oesterling et al. (2013). In this representation, each branch (in the merge tree)\nending in a local minimum is represented by a basin (in the landscape profile), and each\nsub-branch ending in a saddle point is represented as a sub-basin, below which other basins\nare placed. In either case, each basin (or sub-basin) is represented by a set of rectangles\nencoding the cumulative size of the branch (or sub-branch), from bottom to top, such that\nthetopofthebasinisaswideasthenumberofpointsfoundalongthecorrespondingbranch\nin the merge tree.\nWe introduce this topological landscape profile representation of loss functions to ef-\nfectively capture more information from higher-dimensional loss landscapes, in such a way\nthat can still be visualized. While this topological representation and the merge tree used\nto create it both capture important features of the high-dimensional space, it also discards\nsome important information by design. Here, we reincorporate some of this discarded in-\nformation back into our representation, for example, by using the loss values to color the\ndifferent basins. As shown in Figure 2.C, we compute the average loss across the points in\neach basin, and we use darker blues to represent lower average loss values. Thus, deeper\nbasins are represented by a darker blue color, evoking the idea of deeper ocean depths. In\n6\nVisualizing Loss Functions as Topological Landscape Profiles\naddition to coloring the topological landscape profile, we also annotate the basins with the\ncritical points, including saddle points (orange dots) and minima (red dots). Interestingly,\nthe distribution (or density) of saddle points and minima reflects local characteristics of the\nloss landscape, such as locally sharp or locally flat.\n4. Empirical Evaluation\n4.1. Visualizing Different Physical Constraints\nIn our first evaluation, we look at a set of physics-informed neural network (PINN) models\ntrained to solve simple convection problems (Krishnapriyan et al., 2021). Here we aim to\ninvestigate the PINN’s soft regularization and how it helps (or fails to help) the optimizer\nfind an optimal solution to a seemingly simple convection problem. We show how the\nshape and complexity of our topological landscape profiles change as a physical “wave\nspeed” parameter is increased and the PINN fails to solve this seemingly simple physical\nproblem. Specifically, we consider the one-dimensional convection problem, a hyperbolic\npartial differential equation that is commonly used to model transport phenomena:\n∂u ∂u\n+β = 0, x ∈ Ω, t ∈ [0,T] (2)\n∂t ∂x\nu(x,0) = h(x), x ∈ Ω (3)\nwhere β is the convection coefficient and h(x) is the initial condition. The general loss\nfunction for this problem is\n1 (cid:88)Nu 1 (cid:88)N f ∂uˆ ∂uˆ\nL(θ) = (uˆ−ui)2+ λ ( +β )2+L (4)\nN 0 N i ∂t ∂x B\nu f\ni=1 i=1\nwhere uˆ = NN(θ,x,t) is the output of the NN, and L is the boundary loss. While increas-\nB\ningthephysicalwavespeedparameter,β,shouldnotnecessarilymakethisaharderproblem\ntosolve,itcanmakePINNmodelshardertotrain. Interestingly,Krishnapriyanetal.(2021)\nrelated these failure modes to changes in the corresponding loss landscape, showing that\nit becomes increasingly complicated, such that optimizing the model becomes increasingly\ndifficult. Here we explore these failure modes in more detail using three-dimensional and\nfour-dimensional Hessian-based loss landscapes, finding more variability in the shape of loss\nlandscapes near the transition between high and low-performing models.\nIn Figure 3, we show a heat map corresponding to the average relative error across\ndifferent values of the physical wave speed parameter and across different learning rates.\nInterestingly, we observe that the error increases with this physical parameter, but more\nslowly for higher learning rates. The smallest learning rate displays higher error rates even\nforsmallervaluesofthephysicalparameter. Whenlookingatthelosslandscapes,weobserve\nconsistently more funnel-like loss landscapes for the smaller values of β, corresponding\nto lower error (Figure 3.1). In contrast, we observe a consistently more bowl-like loss\nlandscape for the larger values of β, corresponding to higher error (Figure 3.3). The funnel-\nlike landscapes correspond to when the PINN models find a physically reasonable solution,\nalbeit constrained to a smaller space of solutions by the physical wave speed parameter.\n7\nGeniesse et al.\nFigure 3: Analyzing the loss function of a physics-informed neural network (PINN) trained\nto solve simple physical convection problems. See Section 4.1 for details.\nIn other words, since the solution is constrained by the physical parameter, perturbing\nthe model results in a faster increase in the loss, given that the physical problem is no\nlonger satisfied. In contrast, the more bowl-like landscapes correspond to the failure to find\na reasonable solution, such that perturbing the model does not immediately change the\nalready high loss. Note, the landscapes corresponding to these failure modes also include\nmore saddle points and are otherwise more complex.\nTo verify that these representations are stable across different model initializations, we\nshow five different landscapes for each hyperparameter configuration, corresponding to the\nsame model trained using different random seeds. We see the landscapes look similar across\ndifferent random seeds for the low and high values of the physical wave speed parameter.\nMoreover, we observe more variation in the loss landscapes near the transition from low\nto high error (Figure 3.2). This suggests that while the error starts to increase near the\ntransition, only some of the models are failing whereas other may be finding physically\nreasonable solutions, as indicated by the funnel-like loss landscapes.\nIn Figure A.5 we compare the topological landscape profiles based on three- and four-\ndimensional loss landscapes. An important insight here is that, in higher dimensions, we\nobserve many more critical points and that the basins in the much spikier landscape can\nbe mapped back to the wider basins in the topological landscape profiles based on the\nthree-dimensional loss landscapes. Overall the global shape of the topological landscape\nprofile looks similar when comparing the same random seed. Moreover, this highlights an\nimportant feature of our new representations—the ability to visualize higher-dimensional\nloss landscapes, i.e., sampling along more than just one or two dimensions.\n8\nVisualizing Loss Functions as Topological Landscape Profiles\n4.2. Visualizing Loss Landscapes Over Training\nIn our second evaluation, we explore how loss landscapes change throughout training and\nacrossdifferentlearningrates. Todothis,westudyUNetmodelswithalearnableCRF-RNN\nlayer (Avaylon et al., 2022) trained on the Oxford-IIIT Pet dataset (Parkhi et al., 2012).\nWetrainedthemodelsusingfivedifferentrandomseedsacrosssevendifferentlearningrates\nfor 30 epochs. For each checkpoint, we computed two-dimensional loss landscapes based on\nthe top two Hessian eigenvectors. The model was perturbed using a distance of 0.01 and\nlayerwise normalization was adopted (Li et al., 2018).\nIn Figure 4 and Figure B.6, we show the same heat map corresponding to average\ntest accuracy over training and across different learning rates. We observe that the test\naccuracy improves over training, with some variation across the different learning rates. In\nFigure 4, we consider how the loss landscape changes over training. When looking at the\nloss landscapes for three different random seeds, after zooming in, we observe an initially\nshallow loss landscape but with the global minimum at a much higher loss compared to the\nend of training. As training proceeds, we see that the global minimum becomes lower, but\nthe basin itself becomes deeper with the edges remaining at much higher loss values. As the\nglobal minimum continues to drop, we also observe additional flattening of the basin, such\nthat all points have a much lower loss compared to the beginning of training. Interestingly,\nthe flat basin at a much higher loss corresponds to a phase of learning where perturbing the\nmodel in any one direction doesn’t really increase the already high loss. After five epochs,\nthe much deeper basin reflects a less stable model, where perturbing the model results in\nrelatively higher loss. As training proceeds, we observe a flattening of the basin, which\nLearning Rate mean_crf_test_acc\n0.01 85\n0.005\n0.002 80\n0.001 75\n0.0005\n70\n0.0002\n0.0001 65\nU-Net (with CRF)\nEpochs\nLoss 1.4\nSeed A\n0\nInitially, the loss minimum drops but loss at the edges remains high,\nwhich mea1n.s4 the model is more sensitive to changes in the weights\nSeed B\n0\n1.4 As training proceeds, loss at the edges decreases, which means\nthe model is becoming less sensitive to changes in the weights\nSeed C\n0\nFigure 4: Loss landscapes over training for UNet models with a CRF layer trained on the\nOxford-IIIT Pet dataset. See Section 4.2 for details.\n9\n0 1 2 3 4 5 6 7 8 9 01 11 21 31 41 51 61 71 81 91 02 12 22 32 42 52 62 72 82 92\nGeniesse et al.\nmeans the model becomes more stable, as perturbations result in smaller changes in loss.\nIn Figure B.6, we consider how the loss landscape changes across different learning rates.\nWhen looking at the loss landscapes for three different random seeds, after zooming in, we\nobserve consistent variation in the depth and shape of the loss landscape as the learning\nrate is varied. This variation is also reflected in the test accuracy scores shown in the heat\nmap. Interestingly, we observe deeper basins when the learning rate is too small or too big,\nindicating that the trained models are less stable compared to those with shallower basins.\n5. Conclusion and Future Work\nIn this paper, we introduced a new topological landscape profile representation of neural\nnetwork loss landscapes. To demonstrate the many different ways this new representa-\ntion of loss landscapes can be used, we explored several different machine learning exam-\nples, including image segmentation (e.g., UNet-CRF) and scientific machine learning (e.g.,\nPINNs). Along the way, we provided new insights into how loss landscapes vary across dis-\ntinct hyperparameter spaces, finding that the topology of the loss landscape is simpler for\nbetter-performing models and that this topology is more variable near transitions from low\nto high model performance. Moreover, by using a merge tree to extract the most important\nfeatures from a computed loss landscape, we are able to construct a new representation\nencoding these features. By separating this new representation from the original space\nin which the loss landscape was sampled, our approach opens up the door to visualizing\nhigher-dimensional loss landscapes.\nWhile we only explore up to four dimensions here, our approach can be extended to any\nnumber of dimensions. The limiting factor is sampling, which requires exponentially many\nmore resources as the number of dimensions increases. However, future advances towards\nmore efficient sampling could be combined with our current approach to reveal the higher-\ndimensional structure of loss functions. Complementary advances in sampling more global\nloss landscapes (combining multiple independently trained models) could also benefit from\nour new representations. In that case, we would expect to see more distinct basins in our\ntopological landscape profiles.\n6. Acknowledgments\nThisworkwassupportedbytheU.S.DepartmentofEnergy,OfficeofScience,AdvancedSci-\nentificComputingResearch(ASCR)programunderContractNumberDE-AC02-05CH11231\nto Lawrence Berkeley National Laboratory and Award Number DE-SC0023328 to Arizona\nStateUniversity(“VisualizingHigh-dimensionalFunctionsinScientificMachineLearning”).\nThis research used resources at the National Energy Research Scientific Computing Center\n(NERSC), a U.S. Department of Energy, Office of Science, User Facility under NERSC\nAward Number ASCR-ERCAP0026937.\nReferences\nMatthewAvaylon,RobbieSadre,ZheBai,andTalitaPerciano.Adaptabledeeplearningand\nprobabilistic graphical model system for semantic segmentation. Advances in Artificial\nIntelligence and Machine Learning, 02:288–302, 2022.\n10\nVisualizing Loss Functions as Topological Landscape Profiles\nTalha Bin Masood, Joseph Budin, Martin Falk, Guillaume Favelier, Christoph Garth,\nCharlesGueunet,PierreGuillou,LutzHofmann,PetarHristov,AdhityaKamakshidasan,\nChristopher Kappe, Pavol Klacansky, Patrick Laurin, Joshua Levine, Jonas Lukasczyk,\nDaisuke Sakurai, Maxime Soler, Peter Steneteg, Julien Tierny, Will Usher, Jules Vidal,\nand Michal Wozniak. An overview of the Topology ToolKit. In Topological Methods in\nData Analysis and Visualization VI, pages 327–342. Springer International Publishing,\n2021.\nHamish Carr, Jack Snoeyink, and Ulrike Axen. Computing contour trees in all dimensions.\nComputational Geometry,24(2):75–94,2003. ISSN0925-7721. SpecialIssueontheFourth\nCGC Workshop on Computational Geometry.\nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung\nLee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances\nin Neural Information Processing Systems, 34:22405–22418, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nJosipDjolonga,JessicaYung,MichaelTschannen,RobRomijnders,LucasBeyer,Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan,\nGelly Sylvain, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and trans-\nferability of convolutional neural networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16458–16468, 2021.\nWei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for\ngeneric similarity measures. In Proceedings of the 20th international conference on World\nwide web, pages 577–586, 2011.\nIan J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural\nnetwork optimization problems. arXiv preprint arXiv:1412.6544, 2014.\nKaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage\nrecognition. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\nChristianHeine,HeikeLeitte,MarioHlawitschka,FedericoIuricich,LeilaDeFloriani,Gerik\nScheuermann, Hans Hagen, and Christoph Garth. A Survey of Topology-based Methods\nin Visualization. Computer Graphics Forum, 35(3):643–667, 2016.\nDaniel Jiwoong Im, Michael Tao, and Kristin Branson. An empirical analysis of the opti-\nmization of deep network loss surfaces. arXiv preprint arXiv:1612.04010, 2016.\nJerzy W Jaromczyk and Godfried T Toussaint. Relative neighborhood graphs and their\nrelatives. Proceedings of the IEEE, 80(9):1502–1517, 1992.\nArham Khan, Todd Nief, Nathaniel Hudson, Mansi Sakarvadia, Daniel Grzenda, Aswathy\nAjith, JordanPettyjohn, KyleChard, andIanFoster. Sok: Onfindingcommongroundin\n11\nGeniesse et al.\nloss landscapes using deep model merging techniques. arXiv preprint arXiv:2410.12927,\n2024.\nAditiKrishnapriyan,AmirGholami,ShandianZhe,RobertKirby,andMichaelWMahoney.\nCharacterizing possible failure modes in physics-informed neural networks. Advances in\nNeural Information Processing Systems, 34:26548–26560, 2021.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep\nconvolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale.\narXiv preprint arXiv:1611.01236, 2016.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the\nloss landscape of neural nets. Advances in neural information processing systems, 2018.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized\nBERT Pretraining Approach. arXiv preprint arXiv:1907.11692, 2019.\nCharles H Martin and Michael W Mahoney. Implicit Self-Regularization in Deep Neural\nNetworks: Evidence from Random Matrix Theory and Implications for Learning. The\nJournal of Machine Learning Research, 22(1):7479–7551, 2021.\nCharles H Martin, Tongsu Peng, and Michael W Mahoney. Predicting trends in the quality\nof state-of-the-art neural networks without access to training or testing data. Nature\nCommunications, 12(1):4122, 2021.\nPatrick Oesterling, Christian Heine, Gunther H. Weber, and Gerik Scheuermann. Visualiz-\ning nd point clouds as topological landscape profiles to guide local data analysis. IEEE\nTransactions on Visualization and Computer Graphics, 19(3):514–526, 2013.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In\n2012 IEEE conference on Computer Vision and Pattern Recognition, pages 3498–3505.\nIEEE, 2012.\nPratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, and Madeleine Udell. Challenges in\ntraining pinns: A loss landscape perspective. arXiv preprint arXiv:2402.01868, 2024.\nMansi Sakarvadia, Aswathy Ajith, Arham Khan, Nathaniel Hudson, Caleb Geniesse, Kyle\nChard, Yaoqing Yang, Ian Foster, and Michael W. Mahoney. Mitigating memorization\nin language models. arXiv preprint arXiv:2410.02159, 2024.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, L(cid:32) ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in\nneural information processing systems, 2017.\n12\nVisualizing Loss Functions as Topological Landscape Profiles\nTiankai Xie, Caleb Geniesse, Jiaqing Chen, Yaoqing Yang, Dmitriy Morozov, Michael W\nMahoney, Ross Maciejewski, and Gunther H Weber. Evaluating loss landscapes from a\ntopology perspective. arXiv preprint arXiv:2411.09807, 2024.\nJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized Out-of-Distribution\nDetection: A Survey, 2022a.\nYaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E Gonzalez, Kannan\nRamchandran, and Michael W Mahoney. Taxonomizing local versus global structure in\nneural network loss landscapes. Advances in Neural Information Processing Systems,\npages 18722–18733, 2021.\nYaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E Gonzalez, Kannan Ramchan-\ndran, Charles H Martin, and Michael W Mahoney. Evaluating natural language process-\ning models with generalization metrics that do not need access to any training or testing\ndata. arXiv preprint arXiv:2202.02842, 2022b.\nZhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. PyHessian: Neural\nnetworks through the lens of the hessian. In 2020 IEEE international conference on big\ndata (Big data), pages 581–590. IEEE, 2020.\nYefanZhou, YaoqingYang, ArinChang, andMichaelWMahoney. Athree-regimemodelof\nnetwork pruning. In International Conference on Machine Learning, pages 42790–42809.\nPMLR, 2023.\n13\nGeniesse et al.\nAppendix A. Visualizing Different Physical Constraints\nInFigureA.5,weshowtopologicallandscapeprofilesfortwodifferentrandominitializations\n(from left to right) of a physics-informed neural network (PINN). Note, that the landscapes\nlook different for the different random initializations because we are looking at a model\ncorresponding to the transition from low to high error. Overall the global shape of the\ntopological landscape profile looks similar when comparing the same random seed across\nthree- and four-dimensional loss landscapes. In four dimensions, we observe many more\ncritical points and that the basins in the much spikier landscape can be mapped back to the\nwider basins in the topological landscape profiles based on the three-dimensional loss land-\nscapes. Thesevisualizationsalsohighlightanimportantfeatureofourtopologicallandscape\nprofile representations—the ability to visualize higher-dimensional loss landscapes.\n(A) Topological Profile of a 3D Loss Landscape\n(B) Topological Profile of a 4D Loss Landscape\nFigure 5: Comparing topological landscape profiles based on (A) three-dimensional and\n(B) four-dimensional loss landscapes. See Section 4.1 for details.\n14\nVisualizing Loss Functions as Topological Landscape Profiles\nAppendix B. Visualizing Loss Landscapes Over Training\nIn Figure B.6, we show how the loss landscape changes across different learning rates.\nWhen looking at the loss landscapes for three different random seeds, after zooming in, we\nobserve consistent variation in the depth and shape of the loss landscape as the learning\nrate is varied. See Section 4.2 for details.\nLearning Rate mean_crf_test_acc\n0.01 85\n0.005\n0.002 80\n0.001 75\n0.0005\n70\n0.0002\n0.0001 65\nU-Net (with CRF)\nEpochs\nLoss\n1.4\nSeed A\n0\n1.4\nSeed B\n0\n1.4\nSeed C\n0\nFigure 6: Loss landscapes across learning rates for UNet models with a CRF layer trained\non the Oxford-IIIT Pet dataset. See Section 4.2 for details.\n15\n0 1 2 3 4 5 6 7 8 9 01 11 21 31 41 51 61 71 81 91 02 12 22 32 42 52 62 72 82 92",
    "pdf_filename": "Visualizing_Loss_Functions_as_Topological_Landscape_Profiles.pdf"
}