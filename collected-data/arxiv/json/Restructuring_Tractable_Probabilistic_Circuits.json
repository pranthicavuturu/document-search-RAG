{
    "title": "Restructuring Tractable Probabilistic Circuits",
    "abstract": "ence queries like marginalization. The tractability of PCshasnowprovencrucialinarangeofapplications, Probabilisticcircuits(PCs)isaunifyingrep- such as causal inference (Zeˇcevi´c et al., 2021; Wang resentationforprobabilisticmodelsthatsup- and Kwiatkowska, 2023; Busch et al., 2024), knowl- port tractable inference. Numerous applica- edge graph learning (Loconte et al., 2023) and ensur- tions of PCs like controllable text generation ing fairness in decision making (Choi et al., 2021). depend on the ability to efficiently multiply two circuits. Existing multiplication algo- Probabilistic circuits represent distributions as com- rithms require that the circuits respect the putation graphs of sums and products. A crucial as- same structure, i.e. variable scopes decom- pect to the design of PCs is the structure of the com- poses according to the same vtree. In this putation graph, that is, how distributions are fac- work, we propose and study the task of re- torized into (conditionally) independent components. structuring structured(-decomposable) PCs, The structure of PCs affects their tractability, model- that is, transforming a structured PC such ing performance and computational efficiency. In this that it conforms to a target vtree. We pro- work, we consider the problem of restructuring PCs: pose a generic approach for this problem constructing a new PC that follows a particular (tar- and show that it leads to novel polynomial- get) structure while representing the same distribu- time algorithms for multiplying circuits re- tion. Wepresentageneralalgorithmforrestructuring specting different vtrees, as well as a practi- structured-decomposable circuits by considering their cal depth-reduction algorithm that preserves graphicalmodelrepresentations. Specifically,welever- structured decomposibility. Our work opens age the graphical models to reason about conditional up new avenues for tractable PC inference, independencies and recursively construct a new PC suggestingthepossibilityoftrainingwithless conforming to the desired structure. restrictive PC structures while enabling effi- We then investigate two key applications of PC re- cient inference by changing their structures structuring: circuit multiplication and depth reduc- at inference time. tion. Circuit multiplication is a fundamental opera- tionusedforansweringvariousinferencequeries(Ver- 1 INTRODUCTION gari et al., 2021), such as conditioning on logical con- straints (Choi et al., 2015; Ahmed et al., 2022; Liu et al., 2024b; Zhang et al., 2023, 2024), computing A key challenge in deep generative modeling is the in- expected predictions of classifiers (Khosravi et al., tractabilityofprobabilisticreasoning(Roth,1996;Geh 2019) and causal backdoor adjustment (Wang and et al., 2024). To address this challenge, probabilistic Kwiatkowska, 2023), as well as in improving the ex- circuits (PCs) (Darwiche, 2003; Poon and Domingos, pressive power of circuits through squaring (Loconte 2011; Choi et al., 2020) has emerged as a unifying et al., 2024c,b; Wang and Van den Broeck, 2024). representation of tractable generative models, which Though the problem of multiplying circuits of differ- ent structures is in general #P-hard (Vergari et al., Preprint. 2021), we identify a new class of PCs, which we call CorrespondencetoHonghuaZhang[hzhang19@cs.ucla.edu] contiguous circuits, where it is possible to multiply and Benjie Wang [benjiewang@ucla.edu]. circuitsofdifferentstructuresinpolynomial(orquasi- 4202 voN 91 ]IA.sc[ 1v65221.1142:viXra",
    "body": "Restructuring Tractable Probabilistic Circuits\nHonghua Zhang Benjie Wang\nUniversity of California, Los Angeles University of California, Los Angeles\nMarcelo Arenas Guy Van den Broeck\nPontificia Universidad Cat´olica de Chile University of California, Los Angeles\nAbstract support efficient and exact evaluation of various infer-\nence queries like marginalization. The tractability of\nPCshasnowprovencrucialinarangeofapplications,\nProbabilisticcircuits(PCs)isaunifyingrep-\nsuch as causal inference (Zeˇcevi´c et al., 2021; Wang\nresentationforprobabilisticmodelsthatsup-\nand Kwiatkowska, 2023; Busch et al., 2024), knowl-\nport tractable inference. Numerous applica-\nedge graph learning (Loconte et al., 2023) and ensur-\ntions of PCs like controllable text generation\ning fairness in decision making (Choi et al., 2021).\ndepend on the ability to efficiently multiply\ntwo circuits. Existing multiplication algo- Probabilistic circuits represent distributions as com-\nrithms require that the circuits respect the putation graphs of sums and products. A crucial as-\nsame structure, i.e. variable scopes decom- pect to the design of PCs is the structure of the com-\nposes according to the same vtree. In this putation graph, that is, how distributions are fac-\nwork, we propose and study the task of re- torized into (conditionally) independent components.\nstructuring structured(-decomposable) PCs, The structure of PCs affects their tractability, model-\nthat is, transforming a structured PC such ing performance and computational efficiency. In this\nthat it conforms to a target vtree. We pro- work, we consider the problem of restructuring PCs:\npose a generic approach for this problem constructing a new PC that follows a particular (tar-\nand show that it leads to novel polynomial- get) structure while representing the same distribu-\ntime algorithms for multiplying circuits re- tion. Wepresentageneralalgorithmforrestructuring\nspecting different vtrees, as well as a practi- structured-decomposable circuits by considering their\ncal depth-reduction algorithm that preserves graphicalmodelrepresentations. Specifically,welever-\nstructured decomposibility. Our work opens age the graphical models to reason about conditional\nup new avenues for tractable PC inference, independencies and recursively construct a new PC\nsuggestingthepossibilityoftrainingwithless conforming to the desired structure.\nrestrictive PC structures while enabling effi-\nWe then investigate two key applications of PC re-\ncient inference by changing their structures\nstructuring: circuit multiplication and depth reduc-\nat inference time.\ntion. Circuit multiplication is a fundamental opera-\ntionusedforansweringvariousinferencequeries(Ver-\n1 INTRODUCTION gari et al., 2021), such as conditioning on logical con-\nstraints (Choi et al., 2015; Ahmed et al., 2022; Liu\net al., 2024b; Zhang et al., 2023, 2024), computing\nA key challenge in deep generative modeling is the in-\nexpected predictions of classifiers (Khosravi et al.,\ntractabilityofprobabilisticreasoning(Roth,1996;Geh\n2019) and causal backdoor adjustment (Wang and\net al., 2024). To address this challenge, probabilistic\nKwiatkowska, 2023), as well as in improving the ex-\ncircuits (PCs) (Darwiche, 2003; Poon and Domingos,\npressive power of circuits through squaring (Loconte\n2011; Choi et al., 2020) has emerged as a unifying\net al., 2024c,b; Wang and Van den Broeck, 2024).\nrepresentation of tractable generative models, which\nThough the problem of multiplying circuits of differ-\nent structures is in general #P-hard (Vergari et al.,\nPreprint. 2021), we identify a new class of PCs, which we call\nCorrespondencetoHonghuaZhang[hzhang19@cs.ucla.edu] contiguous circuits, where it is possible to multiply\nand Benjie Wang [benjiewang@ucla.edu].\ncircuitsofdifferentstructuresinpolynomial(orquasi-\n4202\nvoN\n91\n]IA.sc[\n1v65221.1142:viXra\nRestructuring Tractable Probabilistic Circuits\npolynomial) time using our algorithm. represent exactly and in polynomial time. Two com-\nmonly assumed properties known as smoothness and\nWe also consider depth reduction, a well-established\ndecomposability ensure efficient marginalization:\ntheoretical tool for reducing the depth of a cir-\ncuit (Valiant et al., 1983; Raz and Yehudayoff, 2008). Definition 2.2 (Smoothness and Decomposability).\nRecentPCimplementationshavefocusedonlayer-wise A sum node is smooth if all of its children have the\nparallelization of PC inference via modern GPUs, and same scope. A product node is decomposable if its\ndepth reduction enables greater parallelization (Pe- children have disjoint scope. A PC is smooth (resp.\nharz et al., 2020; Dang et al., 2021; Liu et al., 2024a; decomposable) if all of its sum (resp. product) nodes\nLoconte et al., 2024a). In this work, we show that our are smooth (resp. decomposable).\nrestructuring algorithm can be used to transform a\nIntuitively, decomposability requires that a product\nstructured-decomposable circuit to an equivalent log-\nnode partitions its scope among its children. For\ndepth circuit, with much tighter upper bounds than\nmany other important queries, it is useful to enforce a\ngivenbypriorwork. Thisopensupnewpossibilitiesof\nstrongerformofdecomposability,knownasstructured-\npractically implementing depth reduction techniques\ndecomposability,thatrequiresthatproductnodeswith\nto speed up PC inference.\nthe same scope decompose in the same way.\nDefinition2.3(Vtree). AvtreeV overvariablesXis\n2 PROBABILISTIC CIRCUITS\na rooted binary tree, where each X ∈X is associated\nwithauniqueleafnodev (wewriteX forthevariable\nv\nNotation Wewilluseuppercasetodenotevariables\nassociated with node v). Each inner node v covers a\n(e.g. X) and lowercase to denote values of those vari-\nset of variables X , satisfying X = X ∪X where\nv v l r\nables (e.g. x). We use boldface to denote sets of vari-\nl,r are the children of v. We write V to denote the\nv\nables/values (e.g. X,x).\nsubtree rooted at v.\nDefinition 2.1 (Probabilistic Circuit). A probabilis-\nDefinition 2.4 (Structured Decomposability). A PC\ntic circuit (PC) A = (G,w) represents a joint prob- A is structured-decomposable (w.r.t a vtree V) if ev-\nability distribution over random variables X through ery product node t∈A decomposes its scope accord-\narooteddirectedacyclic(computation)graph(DAG),\ning to some inner vtree node v ∈V.\nconsisting of sum (⊕), product (⊗), and leaf nodes\n(L), parameterized by w. Each node t represents a The main advantage of structured decomposability is\nprobability distribution p t(X), defined recursively by: that it enables tractable circuit multiplication of two\ncircuitsrespectingthesamevtree,whichisacoresub-\n\nf (x) if t is a leaf node routine for many applications. However, structured\n t\np (x)= (cid:81) p (x) if t is a product node decomposable circuits can be less expressive efficient\nt c∈ch(t) c\n(cid:80)\nw p (x) if t is a sum node\nin general (de Colnet and Mengel, 2021).\nc∈ch(t) t,c c\nwheref (x)isaunivariateinputdistributionfunction 3 PC RESTRUCTURING\nt\n(e.g. Gaussian, Categorical), we use ch(t) to denote\nthe set of children of a node t, and w is the non- In this section, we describe a generic approach that\nt,c\nnegative weight associated with the edge (t,c) in the restructuresanystructured-decomposablePCrespect-\nDAG,whichsatisfytheconstraintthat(cid:80) w = ing a target vtree. The approach consists of three\nc∈ch(t) t,c\n1foreverysumnodet. Wedefinethescope ofanodet steps: (1) construct a Bayesian network representa-\nto be the variables it depends on. The function repre- tion of the PC; (2) find sets of latent variables in the\nsented by a PC, denoted pA(x), is the function repre- Bayesian network that induce conditional independe-\nsented by its root node; and the size of a PC, denoted cies required by the target vtree; (3) construct a new\n|A|, is the number of edges in its graph. structured PC recursively leveraging the conditional\nindependence derived in (2).\nIntuitively,productnodesrepresentafactorizedprod-\nuct of its child distributions, while sum nodes repre- 3.1 Structured PCs as Bayesian Networks\nsent a weighted mixture of its child distributions. For\nsimplicity, in the rest of this paper we assume that It is known that one can efficiently compile a tree-\nsum/leaf and product nodes alternate (i.e. child of a shaped Bayesian network to an equivalent probabilis-\nsum is a product, and child of a product is a leaf or ticcircuit(Darwiche,2003;PoonandDomingos,2011;\nsum), andthateachproducthasexactlytwochildren. Dang et al., 2020; Liu and Van den Broeck, 2021). In\nThe key feature of PCs is their tractability, i.e., the this subsection, we describe how to go in the oppo-\nability to answer queries about the distributions they site direction, i.e. converting an arbitrary structured-\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\ndecomposable PC to a tree-shaped Bayesian network 11:6:16:6 Z 1:6ZZ 11::66 {} {}{}\nwith linearly many variables.\nLetA beastructuredPCovervariablesXrespecting 11::31 3:3 4:46:4 6:6 Z 1:Z 3Z 11 :3:3 Z 4:6ZZ 44 :: 66 {Z 2:3,{ZZ {4:2Z 6:}3 2, :3Z ,4Z:6 4} :6} {Z 1:3,Z{ 2Z :3{1}:Z3, 1Z :32,:Z3} 2:3}\nvtree V. Given a vtree node v ∈ V, we write prod(v)\nWto ed den efiot ne et th he es he it do df enall stp ar to ed su izc et n ho od fes thw eit ch irs cc uo itpe toX bv e. X X1 1X 12 2: :32 3:3 4 4:5 :4 5:5X 6XX 66 X 1XX 11Z 2:3ZZ 22 :3:3 Z 4:5ZZ 44 :X : 556 XX 66 {Z 2:3,{Z {Z 4:Z26:} 23 :, 3Z{ ,4Z Z:46 4:} :5 6} }{Z {4Z:5 4} :5X }1 X 1XX 12 X 2X 2\nX X X X X X X X\nmax v∈V |prod(v)|. Writing n for the number of vari- X X X XX XX X X XX XX XX X 3 X3 3 6 6X 6 4 4X 45 5X 5\nables, the size of the circuit is then O(nh2).1 X2 2 X3 3X4 4 5X 5 2X 2 3 X43X45 X5\n2 3 4 5 2 3 4 5\n(a) A (contiguous) vtree V (b)BayesiannetworkV\nv(cid:55)→Zv\nWe begin by providing a latent variable interpreta-\ntion of structured PCs. Specifically, we define an aug- a{ a}a {}{}\nmentedPCwhichexplicitlyassociateslatentvariables\nwith product nodes for each variable scope. Given b{Zb2:{3,ZZ 24:3:6,}Z 4:6} c{Zc 1:3{,ZZ 12::33,}Z 2:3}\nsome vtree node v, let us associate each t ∈ prod(v) b{Z 2:3,Z 4:6} c{Z 1:3,Z 2:3}\nwithauniqueindexidx(t)∈{0,...,|prod(v)|−1},also d {dZ 2{:3Z,Z 2:4 3:,6}Z 4:6} e{Ze4:5{}Z 4:X 5}1{Z X1 1:3{}ZX 1:2 3}{ZX22:3{}Z 2:3}\nwritingt v,i torefertotheproductnodewithindexiin d {Z 2:3,Z 4:6} e{Z 4:5} X 1{Z 1:3}X 2{Z 2:3}\nprod(v). Then we can introduce a categorical latent X 3{XZ 2{:3Z}X 6}{XZ 4:{6}ZX 4}{Z X4:{5}ZX 5}{Z X4:5{}Z\n}\nvariable Z whose value corresponds to a particular 3 2:3 6 4:6 4 4:5 5 4:5\nv X {Z }X {Z }X {Z }X {Z }\nproduct node in prod(v): 3 2:3 6 4:6 4 4:5 5 4:5\n(c) A labelling of vtree W.\nDefinition3.1(AugmentedPC). Givenastructured-\ndecomposable and smooth PC A over variables X re- Figure1: Fig.1ashowsavtreeV forsomecontiguous\nspecting vtree V, we define the augmented PC A PC A; Fig. 1b shows a Bayesian network representa-\naug\nto be a copy of A where for each vtree node v ∈ V, tionGA forA; Fig.1cshowsavalidlabellingofvtree\nwe add an additional child t\naug\nto each product node W with respect to GA.\nt∈prod(v) that is a leaf node with scope Z and leaf\nv\nfunction f (Z )=1 .\ntaug v Zv=idx(t)\normaynothaveaweightededgetot (whoseweight\nv,i\nIt is not hard to see that the augmented PC A we denote by w if it exists). We thus define:\naug ij\nis a PC over variables X,Z and retains structured\n(cid:40)\ndecomposability and smoothness. Further, the stan- w ∃ path from t to t\np∗(Z =i|Z =j)= ij p,j v,i\ndard marginalization algorithm for PCs ensures that v p 0 otherwise\nthe augmented PC has the correct distribution:\n(cid:80)\nProposition 3.2. pA(X)= zpA aug(X,z) It remains to show that this distribution faithfully\nrepresents the distribution of the augmented PC, i.e.\nLet V v→Zv be the rooted DAG obtained by replacing pA =p∗. The intuitive idea is that each value of Z\nall inner nodes v in vtree V with variable Z v (cf. Fig. cora ru eg sponds to a subtree of A , whose value is pre-\naug\n1). Now, we claim that the augmented PC can be\ncisely given by the product of weights and leaf func-\ninterpreted as a Bayesian network with graph struc-\ntionsspecifiedbytheBayesiannetwork;wereferread-\nture V . To do this, we construct a distribution\nv→Zv ers to the Appendix for the complete proof. We thus\np∗(X,Z), based on the augmented PC, that factor-\nhave the following mapping from structured PCs to\nizes as required by the Bayesian network structure.\ntree-shaped Bayesian networks:\nThere are three cases to consider: (i) the root node\nTheorem 3.3. Let A be a structured-decomposable\np∗(Zroot(V)), (ii) the leaf nodes p∗(X v|Z p), and (iii)\nand smooth PC over variables X respecting vtree V.\nother nodes p∗(Z |Z ) (where we write p for the par-\nv p\nent of v in V). In case (i), we set p∗(Z = i) := w\nThen there exists a Bayesian network GA over vari-\nv i\nables X and Z = {Z |v ∈ V} with graph V such\nwhere w\ni\nis the weight of the edge from the root sum\n(cid:80)\nv v(cid:55)→Zv\nnode to the product node t . In case (ii), we set\nthat zp G(X,z)=pA(X).\nv,i\np∗(X |Z =j)=p (X ),wheretistheleafnodechild\nv p t v Since we have shown that pA and p\nG\nrepresents the\n(with scope X ) of the product node t . Finally, in\nv p,j same distribution over the observed variables X, we\ncase (iii) we note that due to alternating sums and\nwill drop the subscripts when there is no ambiguity.\nproducts, t must have a sum node child, which may\np,j\n1The number of active sum nodes per vtree node is at 3.2 Recursive PC Restructuring\nmost h, as each such node must have a different product\nnodeparentcorrespondingtotheparentvtreenodescope. Suppose we have a PC A with its Bayesian network\nThis leads to O(h2) edges per vtree node. representation GA and vtree V, and let W be some\nRestructuring Tractable Probabilistic Circuits\np(Xw|Cw) by a bottom-up recursion on W. For the base case,\nif w is a leaf node representing some random variable\np(Cl,Cr|Cw)\nX j, p(X\nj\n|C Xj)=p(X\nj\n|parent of X\nj\nin GA), which\np(Xw|Cl,Cr) isdirectlygivenbytheconditionalprobabilitytableof\nGA. For the induction step, when w is a inner node\nwith children l and r, we have the recurrence relation:\np(X |C )\np(Xl|Cl) p(Xr|Cr) w (cid:88)w\n= p(X ,X |C ,C )·p(C ,C |C )\nl r l r l r w\nFigure 2: Recursive construction of vectors of sum\n(Cl∪Cr)\\Cw\nnodes representing p(X w|C w) (cid:88)\n= p(X |C )·p(X |C )·p(C ,C |C )\nl l r r l r w\n(Cl∪Cr)\\Cw\nothervtree. WenowshowhowtoconstructanewPC\nHere the first step follows from Property 2 and 3, and\nrespectingW thatencodesthesamedistributionasA.\nthe second step follows from all properties in Defin-\nTheroughideaistolabeleachvtreenodew∈W with\ntion 3.6. The circuit materialization of the recurrence\na subset of latent variables C w⊆GA such that X\nw\nis\nrelation is shown in Figure 2. Note that if w is the\nconditionally independent from X\\X given C . To\nw w root, then p(X |C ) becomes p(X), which is a sin-\ncharacterize such properties, we introduce covers: w w\ngle sum node representing the distribution of A. The\nDefinition 3.4 (Cover). Given a tree-shaped complete recursion is given by Algorithm 1.\nBayesian network GA as constructed in Sec. 3.1, we\nsay that C ⊆ Z covers S ⊆ X if C blocks2 all paths Algorithm 1 Construct PC with respect to W\nbetween S and X\\S in GA.\nprocedure ConstructCircuit(w)\nif w is a leaf node X then\nOur definition of cover is a special case of d- i\nreturn p(X |C )\nseparation (Geiger et al., 1990), which characterizes i Xi\nend if\nconditional independence for Bayesian networks:\nl,r ←Children(w)\nProposition 3.5 (Geiger et al. (1990)). A,B⊆GA (cid:76) ←ConstructCircuit(l)\nare conditionally independent given C ⊆ GA if and (cid:76)Xl,Cl ←ConstructCircuit(r)\nonly if C blocks all paths between A and B. In partic- (cid:76)Xr,Cr (cid:80) (cid:76) (cid:76)\n← · ·p(C ,C |C )\nular, if C covers S then S and X\\S are conditionally Xw,Cw Cl Cr l r w\nindependent given C. return (cid:76)\n(Cl∪Cr)\\Cw\nCw\nend procedure\nOur goal is to recursively construct vectors of sum\nnodes ⊕ representing the probability distributions\ni\np(X |C =i). Lettinglandrbethechildrenofw,we Theorem3.7. Lethbethenumberofhiddenstatesof\nw w\nwillestablisharecurrencerelationbetweenp(X |C ), the original PC A and n the number of random vari-\nw w\np(X |C )andp(X |C ). Thisrequiresthevtreelabels ables. The number of hidden states of the restructured\nl l r r\nto satisfy the following properties: PC is given by O(hM) where M=max w∈W|C l ∪C r|\nand the size of the restructured PC is bounded by\nDefinition 3.6 (Valid Vtree Labelling). Given the\nO(nhM′) where M′=max |C ∪C ∪C |≤2M.\nBayesian network GA and target vtree W, a valid la-\nWe refer to M′ as the\ncardw i∈ nW alityl\nof\nthr\ne\nlabew\nlling C .\nbelling of W with respect to GA associates each node w\nw∈W with a subset of latent variables C ⊆G s.t.\nw V Proof. Let A′ be the restructured circuit respecting\nW. As described in Algorithm 1, for each inner node\n1. C\nw\ncovers X\nw\nin GA.\nw ∈ W, we construct two layers of nodes as shown\n2. C blocks all paths between X and C ∪C .\nl l r w in Figure 2. By construction, the product layer con-\n3. C blocks all paths between X and C ∪C .\nr r l w tains all product nodes respecting the vtree node w\nand its cardinality is given by O(h|Cl∪Cr|); we set\nFurthermore, w.l.o.g., we set C := ∅ and\nrootofW M :=max |C ∪C | and it follows that the hid-\nw∈W l r\nC Xj:=parent of X j in GA for the leaf nodes X j∈W. den states size of B is given by O(hM). Similarly,\nSee Figure 1c for an example.\nthe number of edges in the sum layer is given by\nO(h|Cl∪Cr∪Cw|) and the number of product edges is\nAssuming that we have computed a valid labelling for\ngivenbyO(h|Cl∪Cr|);sincethereareO(n)vtreenodes\nW, we can then proceed to construct the desired PC\nin total, the total number of edges in B is given by\n2a path P is blocked by a set S if P ∩S̸=∅. O(nhM′), with M′ =max |C ∪C ∪C |.\nw∈W l r w\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nRemark 3.8. ByTheorem3.7,therestructuredPCA′ C . We refer readers to the Appendix for details.\nw\nhas hidden state size O(hM), which gives a circuit of\nProposition 3.9. Algorithm 2 computes a valid la-\nsizeΘ(nh2M)onlyifA′ isdenselyconnected. Infact,\nbelling with respect to GA.\nwe will show in Section 4 and 5 that the restructured\nPCs are often sparsely connected, resulting in sizes\nProof. We prove by a top-down induction on W that\nmuch smaller than O(nh2M). Thus, while the graphi-\nthe labelling C computed by Algorithm 2 is valid.\nw\ncal model representation is useful for reasoning about\nAssume that C\nw\ncovers X\nw\nin GA, we want to show\nconditional independencies, the circuit representation\nthat C and C satisfy the properties from Defini-\nl r\nallows us to visualize and exploit the sparsity for effi-\ntion 3.6. To prove that C covers X , we consider a\nl l\ncient inference (Dang et al., 2022a; Liu et al., 2024a).\npathfromX ∈X toX ∈X\\X . (1)IfX andX are\na l b l a b\nin the same G , then the path is blocked by C . (2) If\ni i\n3.3 Computing Vtree Labelling X andX areindifferentG s,thenthepathcontains\na b i\nsome node Z ∈C , and we can choose from the path\nThe next question that immediately arises is how to w\nthe first Z∈C . Then Z∈C by construction, imply-\ncompute a valid labelling for W with respect to GA. ingthatthepaw thisblockedbl\nyC . Henceweconclude\nOne naive solution is to set C to be Z, the set of all l\nw that C is a cover for X , satisfying Property 1. To\nlatent variables in GA. However, this is not desirable\nprove\ntl\nhat C satisfies\nPrl\noperty 2, we argue that be-\nas M′=max |C ∪C ∪C |=|Z|=n−1, result- l\nw∈W l r w cause C and C are both subsets of D , all paths\ning in the restructured circuit having exponential size r w w\nfrom X to C ∪C will be blocked by C by the way\nO(nhn−1). Hence we present a greedy approach that l r w l\nthat C is constructed. We can show that C satisfies\ncomputes a labelling while trying to minimize M′. l r\nProperty1andProperty3bythesameargument.\nThealgorithmproceedstop-downonW. Forthebase\ncase where w is the root, we set C := ∅. For the Though Algorithm 2 computes a valid labelling while\nw\ninductive step, let l and r be the children of w and greedily minimizing |C ∪C ∪C |, we do not know\nl r w\nassume that we have computed C as a cover for X whetherM′ =max |C ∪C ∪C |isgloballymin-\nw w w∈W l r w\nin GA: we (1) split GA into connected components imized or not. In addition, we hypothesize that if we\n{G } via C ; then (2) within each connected com- canfindaminimumvtreelabelling,thenthesizeofthe\ni w\nponent G , we compute a minimum d-separator C PCconstructedbyAlgorithm1isoptimal. Weleaveit\ni i\nthat blocks all paths between X ∩G and X ∩G by as an open problem to design an algorithm that com-\nl i r i\ncalling the sub-routine MinimumSeparator. We set putes minimum labellings and prove the optimality of\n(cid:83)\nD :=( C )∪C and observe that D covers both Algorithm 1 given a minimum labelling.\nw i i w w\nX landX rinGA. TocomputeC l,similarlyforC r,we\nNonetheless we show that Algorithm 1 yields novel\nconsider all paths starting from X and stopping im-\nl polynomial-time algorithms for the tasks of PC mul-\nmediatelywhenreachingsomeZ ∈D ,andweletC\nj w l tiplication and depth-reduction. Specifically, we show\ntobethesetcontainingallsuchZ s. Thepseudocode\nj thatforimportantsubclassesofPCs, wecan compute\nisshowninAlgorithm2. NotethattheMinimumSep-\nvtreelabellingsofconstantorO(logn)cardinality. We\nrefer readers to Section 4 and Section 5 for details.\nAlgorithm 2 Computing C for w∈W\nw\nprocedure ComputeLabel(w,C )\nw 3.4 Corollaries\n{G i}←ConnectedComponents(GA,C w)\nC i ←MinimumSeparator(G i,X l∩G i,X r∩G i) Withourrestructuringalgorithminhand, wenowex-\n(cid:83)\nD w ←( iC i)∪C w amine the restructuring of two other types of circuits:\nC l ←{Z j∈D w :Paths(X l,Z j)∩D w={Z j}} namely, deterministic PCs, and logical circuits.\nC ←{Z ∈D :Paths(X ,Z )∩D ={Z }}\nr j w r j w j Definition 3.10 (Determinism). A sum node is de-\nComputeLabel(l,C )\nl terministic iffor every value x of X, at most onechild\nComputeLabel(r,C )\nr c returns a non-zero value (i.e. p (x) > 0). A PC is\nend procedure c\ndeterminstic if all of its sum nodes are deterministic.\narator procedure called in Algorithm 2 computes a Determinismiscrucialfortractabilityofvariousinfer-\nminimum d-separator that blocks all paths between ence queries such as computing the most likely state\nX andX inthesubgraphG . Eventhoughpolytime (MAP) (Peharz et al., 2016; Conaty et al., 2017) or\nl r i\nalgorithms for computing minimum d-separators exist computing the entropy of the PC’s distribution (Shih\ninliterature(Tianetal.,1998),wederivealinear-time and Ermon, 2020; Vergari et al., 2021). It is thus of\nalgorithm that is easy to implement for our use case, interest to ask whether applying our restructuring al-\nwhere G is a rooted tree with leaves in X , X and gorithm maintains determinism.\ni l r\nRestructuring Tractable Probabilistic Circuits\nClaim 3.11. Algorithm 1 preserves determinism. Z Z Z\n1 2 n\nProof. Iftheoriginalcircuitisdeterministic,theneach X X X\n1 2 n\nassignment to the observed variables fully determines\nthe values of all latent variables (and thus the latents Figure 3: GA for A with a linear vtree\nbeingconditionedonfortherestructuring). Hencethe\nconstructed sum nodes are deterministic.\nNote that a contiguous circuit is not necessarily\nstructured-decomposableand0.5⊗p(X )⊗p(X ,X )⊕\nAlthough we have focused on probabilistic circuits up 1 2 3\n0.5⊗p(X ,X )⊗p(X )issuchanexample. Intuitively,\nto this point, our restructuring algorithm also ap- 1 2 3\nrandom variables forming contiguous scopes can often\nplies to logical circuits - in particular, structured-\nbe covered by vtree labellings of small cardinalities.\ndecomposable negation normal form (SDNNF) cir-\ncuits3 (Pipatsrisawat and Darwiche, 2008). To see Case 1. For the multiplication of contiguous PCs A\nthis, we use a simple trick: (1) convert the logical cir- and B, we start by considering the case when A is a\ncuit into a probabilistic circuit by replacing ∨ with contiguous structured PC respecting the linear vtree\n⊕ and ∧ with ⊗, and assigning positive weights to ⊕ V and B is a contiguous structured PC respecting\nedges;(2)restructurethePC;(3)convertthePCback an arbitrary vtree W. It follows from Section 3.1 that\nto a logical circuit by replacing ⊕ with ∨ and ⊗ with theBayesiannetworkrepresentationforA isahidden\n∧, andremovingtheweights. Itisimmediatethatthe Markov model (Rabiner, 1989), as shown in Figure 3.\nlogical circuits and the corresponding PCs have the By the definition of contiguity, each node w∈W has\nsame support throughout the process. a scope of the form X := {X ,...,X } and we can\na:b a b\nlabelitwithC :={Z ,Z };inparticular,wedrop\nIt is also not hard to see that this procedure for log- a:b a b+1\nZ if a=1 and drop Z if b=n.\nical circuits retains determinism, so, e.g, an ordered a b+1\nbinarydecisiondiagram(OBDD)canbeefficientlyre- Claim 4.2. C is a valid vtree labelling of W re-\na:b\nstructured into a deterministic SDNNF with the re- specting GA with cardinality M′ =3.\nverse order while maintaining the ability to perform\nmodel counting (Darwiche and Marquis, 2002). Then it follows from Theorem 3.7 that the size of A′,\ni.e., the PC obtained by restructuring A respecting\nW,isboundedbyO(nh3),withO(|A|2)beingalooser\n4 PC MULTIPLICATION\nbound. EventuallywecancomputetheproductofA′\nof B tractably by the existing algorithm for multiply-\nOneimportantapplicationofrestructuringPCsiscir-\ningtwocircuitsrespectingthesamevtree(Shenetal.,\ncuit multiplication: given two PCs A and B, the\n2016; Vergari et al., 2021).\ngoal is to construct a tractable PC C such that\npC(x) ∝ pA(x)·pB(x). PC multiplication was pre- Theorem 4.3. LetA andB becontiguousstructured\nviously only addressed for structured PCs respecting PCs. If A has a linear vtree, then A and B can\nthesame vtree(Shenetal.,2016;Vergarietal.,2021). be multiplied in polynomial time and the size of the\nCircuit restructuring immediately gives us a means of product PC is bounded by O(|A|2|B|).\nmultiplying two structured circuits respecting differ-\nCase 2.Thenweconsiderthemoregeneralcasewhere\nent vtrees, as we can simply restructure one of them\nA isacontiguousstructuredPCofdepthdrespecting\nto be compatible with the other. Though the restruc-\nvtree V and B is a contiguous structured PC with an\ntured PC will in general have exponential size, in this\narbitrary vtree W. Similarly to the previous case, our\nsection, we consider practical cases where circuit mul-\ngoal is to come up with a small labelling of W with\ntiplicationswithrespecttodifferent vtreesistractable.\nrespect to GA. Since A is contiguous, its vtree V\nWe start by introducing a new structural property of canbeviewedasasegment tree (Cormenetal.,2022).\ntractable PCs called contiguity. Algorithm 3, which is adapted from the segment tree\nDefinition 4.1 (Contiguity). Giventhecanonicalor- querying algorithm, computes a cover C a:b ⊆ GA for\ndering of variables X 1,X 2,...,X n, a PC node is con- eachcontiguoussegmentX a:b. Foreachw∈W, X w =\ntiguous ifitsscopeisoftheformX a,X a+1,...,X b for X a:b for some 1≤a≤b≤n and we set C w =C a:b =\nsome 1≤a≤b≤n. A smooth and decomposable PC is SegmentCover(V,X a:b).\ncontiguous if all of its nodes are contiguous. Proposition 4.4. C is a valid vtree labelling with\nw\n3Manyotherrepresentations,suchastheorderedbinary\nrespect to GA.\ndecisiondiagram(OBDD)anddeterministicfiniteautoma-\nton (DFA), can be converted efficiently to (deterministic) In addition to the fact that C w is a valid labelling,\nSDNNFs (Amarilli et al., 2024). by the runtime analysis of the original segment tree\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nAlgorithm 3 Compute Cover for Segment X 2024), in which linear PCs (HMMs) are multiplied\na:b\nprocedure SegmentCover(v, X ) with deterministic finite automata (DFAs) represent-\na:b\nif X =∅ then ing the logical constraint. Our results imply that one\na:b\nreturn ∅ canalsomultiplyaHMMwithacontiguouslogicalcir-\ncuitsuchasasententialdecisiondiagram(SDD)(Dar-\nend if\nwiche, 2011), which have been shown to be exponen-\nif X =X then\na:b v\ntially more expressive efficient (Bova, 2016).\nreturn {Z }\nv\nend if\nl,r ←Children(v) 5 PC DEPTH REDUCTION\nL←SegmentCover(l,X ∩X )\nl a:b\nR←SegmentCover(r,X ∩X )\nr a:b Algorithm 4 Depth Reduction Vtree\nreturn L∪R\n1: procedure BalancedVtree(V,S =∅,S =∅)\nend procedure l r\n2: if |V|=1 then\n3: return leaf(V; S ∪S )\nl r\nqueryingalgorithm,weknowthatthenumberofnodes 4: end if\nvisitedateachlevelofV isatmost4anditfollowsthat 5: v ←root(V)\n|C |≤4d; hence the cardinality of C is bounded by 6: l,r ←Children(v)\nw w\n12d. Then following a similar analysis, we have the 7: while |V r|> 32|V| do\nfollowing results for multiplying two contiguous PCs. 8: v ←r\nTheorem 4.5. LetA andB becontiguousstructured 9: l,r ←Children(v) ▷ assume |V l|≤|V r|\nPCs. Let d be the depth of the vtree for A, then A 10: end while\nand B can be multiplied in time O(|A|12d|B|). 11: V l′ ←BalancedVtree(V [v(cid:55)→l],S l,{Z v})\n12: V′ ←BalancedVtree(V ,{Z },S )\nr r v r\nCorollary 4.6. If A is of depth O(logn) then A and 13: return join(V′,V′;S ∪S )\nl r l r\nB can be multiplied in quasi-polynomial time. 14: end procedure\nRemark 4.7. In this work, we assumed product nodes\nalways have two children and binary vtrees. Hence Depth reduction of a probabilistic circuit refers to\nthe depths of PCs are lower-bounded by Ω(logn) un- the construction of an equivalent circuit with reduced\nder such assumptions. However, if we allow product depth, e.g. to a depth logarithmic in the number of\nnodes to have arbitrarily many children, we can have variables. Adepthreductionalgorithmforgeneralcir-\nPCsofsmallerorevenconstantdepths(RazandYehu- cuitsisknown(Valiantetal.,1983;RazandYehuday-\ndayoff, 2009) and we hypothesize that Theorem 3.7 off, 2008; Yin and Zhao, 2024) but does not take ad-\ncanbeadaptedtosuchgeneralizedcasesthusgivinga vantage of structuredness. We show how to reduce a\npolynomial-time algorithm for multiplying contiguous structured-decomposable circuit to an equivalent log-\nstructured circuits of constant depths. depthcircuitbyrestructuring. Thekeystepistoiden-\nRemark 4.8. Thus far, we have assumed that both A tify a log-depth vtree such that restructuring to that\nand B are structured PCs. We claim that we can fur- vtree using Algorithm 1 (and some valid choice of la-\nther generalize our results by removing the constraint bels) results in at most a polynomial increase in size.\nthatBhastobestructured,andTheorems4.3and4.5\nAlgorithm 4 constructs a log-depth vtree labelling of\nwouldstillhold. Weillustratetheideabyshowinghow\ntomultiplyacontiguousstructuredPCA respectinga constantcardinality. Intuitively,eachstepofthealgo-\nlinear vtree and an arbitrary contiguous PC B. Since rithm breaks a vtree down into two connected compo-\nB is not structured decomposable, we cannot restruc- nents, which are then depth-reduced recursively. One\nture A to the vtree of B. However, we can use the selects a single vtree node by traversing the vtree top-\nsame idea as Algorithm 1 to restructure A “on-the- down, until the split would be balanced in the sense\nfly” as we multiply it with B in a bottom-up way. that the two connected components have size between\n1 and 2 oftheinputvtree(Lines7-10). Thealgorithm\nSpecifically, for each possible scope X a:b that appears 3 3\nin B, we recursively construct circuit representations simulataneously constructs a valid label for the vtree\nnode. The join routine then returns a labelled vtree\nfor the functions p q(X a:b)·pA(X a:b|Z a=i,Z b=j) for\ni,j andq∈BwithscopeX . Therecurrencerelation that consists of a single root node with the aforemen-\na:b\ntionedlabel,connectedtothedepth-reducedvtreesfor\nis similar to that of Algorithm 1 and we refer readers\nthecomponents. Notethatthealgorithmproducesex-\nto the Appendix for details.\nactlyonevtreenodeforeachvtreenodeintheoriginal\nAs an explicit application of circuit multiplication, let vtree; we can thus write v(w) for the node in V cor-\nus consider constrained text generation (Zhang et al., responding to w. Then we have the following result:\nRestructuring Tractable Probabilistic Circuits\nbilistic generating circuits (Zhang et al., 2021; Harvi-\nainen et al., 2023; Agarwal and Bl¨aser, 2024; Broad-\nTheorem 5.1 (Depth Reduction Vtree). Given any\nricketal.,2024). Significantefforthasbeendevotedto\nvtree V, Algorithm 4 returns a vtree W of depth\nlearning PC structures to fit data (Liang et al., 2017;\nO(log|V|) with a valid labelling of cardinality 3.\nDang et al., 2020; Yang et al., 2023), but the impli-\ncations for the structure-dependent queries have been\nProof. The depth reduction to O(log|V|) is achieved\nlessstudied. Webridgethisgapbyprovidingageneral\nas the algorithm increases the depth by one in each\nrestructuring algorithm with specific cases of (quasi-\nrecursive call, but reduces the vtree size by a multi-\n)polynomial complexity.\nplicative factor. The validity condition holds due to\nthe separation into connected components (the labels Astractablerepresentationsofdistributions,PCshave\ncan also be obtained from Algorithm 2). The value of been employed extensively as a compilation target\nM′ follows by noting that S and S are either empty for inference in graphical models (Darwiche, 2003;\nl r\nor singleton sets, and that the algorithm produces Chavira and Darwiche, 2008; Rooshenas and Lowd,\nC =S ∪S ,C =S ∪{Z },andC ={Z }∪S 2014). Hiddentree-structuredBayesiannetworkshave\nw l r l l v(w) r v(w) l\nwhere Z for each inner node w ∈W. also been used as a starting point for the learning\nv(w)\nof a probabilistic circuit (Dang et al., 2020; Liu and\nRemark 5.2. Firstly, the depth-reduced PC retains\nVandenBroeck,2021;Dangetal.,2022a). Aparticu-\nstructuredness, which is not guaranteed by the exist-\nlarly useful analysis technique for learning probabilis-\ning depth-reduction algorithms. Secondly, exploiting\ntic circuits has been to interpret them as latent vari-\nstructuredness and tracking the hidden state size en-\nable models (Peharz et al., 2016). Decomposable and\nables a more fine-grained analysis of the size of the\nsmooth PCs can be interpreted as Bayesian networks\ndepth-reduced circuit. Since the size of the original\nby introducing a latent variable for each sum node in\ncircuitisO(nh2),usingtheknowncubicboundonthe\nthePC(Zhaoetal.,2015). Ourconversionfromstruc-\nsizeofthedepth-reducedcircuit(RazandYehudayoff,\ntured PC to Bayesian network is most closely related\n2008)givesO(n3h6). However,byTheorem5.1,wesee\nto the decompilation methods of Butz et al. (2020);\nthat M′=max |C ,C ,C |≤3 and so by Theo-\nw∈W l r W Papantonis and Belle (2023), but we do not assume\nrem 3.7 we immediately obtain a much tighter bound\nthe PC has been compiled from a Bayesian network.\nof O(nh3) for the size of the resulting circuit.\nThe seminal work of Valiant et al. (1983) showed that\nCorollary 5.3. Any structured PC over n variables\nany poly-size arithmetic circuit can be transformed\nand with hidden state size h can be restructured to a\ninto an equivalent circuit of polylogarithmic depth.\nstructured PC of depth O(logn) and size O(nh3) that\nRaz and Yehudayoff (2008) show that this procedure\nrepresents the same distribution.\nmaintains syntactic multilinearity (decomposability).\nRecently, Yin and Zhao (2024) showed a quasipoly-\nWhile this result is of independent theoretical inter-\nnomial upper bound on converting decomposable and\nest,thesub-quadraticcomplexityofO(nh3)alsoopens\nsmoothPCstotree-shapedPCsviaadepth-reduction\nup practical applications of depth-reduction. Al-\nprocedure. Ourapplicationofrestructuringfocuseson\nmost all PC inference and learning algorithms involve\nstructured-decomposable circuits and shows a tighter\nforward/backward passes through the computation\nbound based on a graphical model interpretation.\ngraph, where computation is only parallelized across\nnodes of the same depth such that O(depth of PC)\nsequential computations are required. This is prob- 7 CONCLUSION\nlematic when the number of variables n is large, as is\noften the case in applications such as computational\nWe introduce the problem of restructuring probabilis-\nbiology (Dang et al., 2022b). In such cases, depth re-\ntic circuits, and develop a general algorithm for re-\nductioncanbeapracticalstrategywheretheimproved\nstructuring a structured-decomposable circuit to any\nparallelism outweighs the increased circuit size.\ntarget vtree structure. Our method exploits an inter-\npretationofstructured-decomposablecircuitsaslatent\n6 RELATED WORK tree Bayesian networks, which enables recursive con-\nstruction of a circuit respecting the target vtree using\nProbabilisticcircuitshaveemergedasaunifyingrepre- probabilistic semantics of the Bayesian network. As\nsentationoftractableprobabilisticmodels(Choietal., concreteapplicationsofrestructuring,weshowhowto\n2020; Sidheekh and Natarajan, 2024), such as sum- tractably multiply two circuits which do not necessar-\nproduct networks (Poon and Domingos, 2011), cutset ily share the same structure but satisfy a contiguity\nnetworks (Rahman et al., 2014), probabilistic senten- property,andshowhowtorestructureacircuittolog-\ntial decision diagrams (Kisa et al., 2014) and proba- depth with a sub-quadratic increase in size.\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nAcknowledgements Conaty,D.,Maua,D.D.,anddeCampos,C.P.(2017).\nApproximation complexity of maximum a posteri-\nThis work was funded in part by the DARPA ANSR ori inference in sum-product networks. In The 33rd\nprogram under award FA8750-23-2-0004, the DARPA Conference on Uncertainty in Artificial Intelligence\nPTGProgramunderawardHR00112220005,andNSF (UAI). AUAI.\ngrant#IIS-1943641. Thisworkwasdoneinpartwhile\nCormen, T. H., Leiserson, C. E., Rivest, R. L., and\nBW,MAandGVdBwerevisitingtheSimonsInstitute\nStein, C. (2022). Introduction to algorithms. MIT\nfor the Theory of Computing.\npress.\nReferences Dang, M., Khosravi, P., Liang, Y., Vergari, A., and\nVan den Broeck, G. (2021). Juice: A julia package\nAgarwal, S. and Bl¨aser, M. (2024). Probabilis-\nforlogicandprobabilisticcircuits. InProceedings of\ntic generating circuits–demystified. arXiv preprint\nthe 35th AAAI Conference on Artificial Intelligence\narXiv:2404.02912.\n(Demo Track).\nAhmed, K., Teso, S., Chang, K.-W., Van den Broeck,\nG., and Vergari, A. (2022). Semantic probabilis- Dang, M., Liu, A., and Van den Broeck, G. (2022a).\ntic layers for neuro-symbolic learning. Advances in Sparse probabilistic circuits via pruning and grow-\nNeural Information Processing Systems, 35:29944– ing. In Advances in Neural Information Processing\n29959. Systems 35 (NeurIPS).\nAmarilli,A.,Arenas,M.,Choi,Y.,Monet,M.,Broeck, Dang, M., Liu, A., Wei, X., Sankararaman, S., and\nG. V. d., and Wang, B. (2024). A circus of circuits: Van den Broeck, G. (2022b). Tractable and ex-\nConnections between decision diagrams, circuits, pressivegenerativemodelsofgeneticvariationdata.\nand automata. arXiv preprint arXiv:2404.09674. In Proceedings of the International Conference on\nResearch in Computational Molecular Biology (RE-\nBova, S. (2016). Sdds are exponentially more succinct\nCOMB).\nthanobdds. InProceedings of the AAAI Conference\non Artificial Intelligence, volume 30. Dang,M.,Vergari,A.,andBroeck,G.(2020). Strudel:\nBroadrick, O., Zhang, H., and Van den Broeck, G. Learning structured-decomposable probabilistic cir-\n(2024). Polynomial semantics of tractable proba- cuits. In International Conference on Probabilistic\nbilistic circuits. In Proceedings of the 40th Confer- Graphical Models, pages 137–148. PMLR.\nenceonUncertaintyinArtificialIntelligence(UAI).\nDarwiche, A. (2003). A differential approach to in-\nBusch, F. P., Willig, M., Seng, J., Kersting, K., and ference in bayesian networks. Journal of the ACM\nDhami, D. S. (2024). Ψnet: Efficient causal model- (JACM), 50(3):280–305.\ning at scale. In International Conference on Proba-\nDarwiche, A. (2011). Sdd: A new canonical represen-\nbilistic Graphical Models, pages 452–469. PMLR.\ntationofpropositionalknowledgebases. InTwenty-\nButz, C., Oliveira, J. S., and Peharz, R. (2020). Sum-\nSecond International Joint Conference on Artificial\nproduct network decompilation. In International\nIntelligence.\nConferenceonProbabilisticGraphicalModels,pages\nDarwiche, A. and Marquis, P. (2002). A knowledge\n53–64. PMLR.\ncompilation map. Journal of Artificial Intelligence\nChavira, M. and Darwiche, A. (2008). On probabilis-\nResearch, 17:229–264.\ntic inference by weighted model counting. Artificial\nIntelligence, 172(6-7):772–799. deColnet,A.andMengel,S.(2021). Acompilationof\nsuccinctness results for arithmetic circuits. In Pro-\nChoi, A., Van den Broeck, G., and Darwiche, A.\nceedings of the International Conference on Prin-\n(2015). Tractablelearningforstructuredprobability\nciples of Knowledge Representation and Reasoning,\nspaces: A case study in learning preference distri-\nvolume 18, pages 205–215.\nbutions. In Proceedings of 24th International Joint\nConference on Artificial Intelligence (IJCAI). Geh, R. L., Zhang, H., Ahmed, K., Wang, B., and\nChoi, Y., Dang, M., and Van den Broeck, G. (2021). Van den Broeck, G. (2024). Where is the signal in\nGroup fairness by probabilistic modeling with la- tokenizationspace? InProceedingsofthe2024Con-\ntentfairdecisions. InProceedingsoftheAAAICon- ference on Empirical Methods in Natural Language\nference on Artificial Intelligence, volume 35, pages Processing (EMNLP).\n12051–12059.\nGeiger, D., Verma, T., and Pearl, J. (1990). d-\nChoi, Y., Vergari, A., and Van den Broeck, G. (2020). separation: From theorems to algorithms. In\nProbabilistic circuits: A unifying framework for Machine intelligence and pattern recognition, vol-\ntractable probabilistic models. ume 10, pages 139–148. Elsevier.\nRestructuring Tractable Probabilistic Circuits\nHarviainen, J., Ramaswamy, V. P., and Koivisto, M. sum-product networks. IEEE transactions on pat-\n(2023). On inference and learning with probabilis- ternanalysisandmachineintelligence,39(10):2030–\ntic generating circuits. In Uncertainty in Artificial 2044.\nIntelligence, pages 829–838. PMLR.\nPeharz,R.,Lang,S.,Vergari,A.,Stelzner,K.,Molina,\nKhosravi, P., Choi, Y., Liang, Y., Vergari, A., and A., Trapp, M., Van den Broeck, G., Kersting, K.,\nVan den Broeck, G. (2019). On tractable computa- andGhahramani,Z.(2020). Einsumnetworks: Fast\ntion of expected predictions. In Advances in Neural and scalable learning of tractable probabilistic cir-\nInformation Processing Systems 32 (NeurIPS). cuits. In Proceedings of the 37th International Con-\nference on Machine Learning (ICML).\nKisa, D., Van den Broeck, G., Choi, A., and Dar-\nwiche, A. (2014). Probabilistic sentential decision Pipatsrisawat, K.andDarwiche, A.(2008). Newcom-\ndiagrams. In Fourteenth International Conference pilation languages based on structured decompos-\non the Principles of Knowledge Representation and ability. In AAAI, volume 8, pages 517–522.\nReasoning. Poon, H. and Domingos, P. (2011). Sum-product net-\nLiang, Y., Bekker, J., and Van den Broeck, G. (2017). works: A new deep architecture. In 2011 IEEE In-\nLearningthestructureofprobabilisticsententialde- ternational Conference on Computer Vision Work-\ncision diagrams. In Proceedings of the 33rd Confer- shops (ICCV Workshops), pages 689–690. IEEE.\nenceonUncertaintyinArtificialIntelligence(UAI). Rabiner, L. R. (1989). A tutorial on hidden markov\nLiu, A., Ahmed, K., and Van den Broeck, G. (2024a). models and selected applications in speech recogni-\nScaling tractable probabilistic circuits: A systems tion. Proceedings of the IEEE, 77(2):257–286.\nperspective. InProceedingsofthe41thInternational Rahman, T., Kothalkar, P., and Gogate, V. (2014).\nConference on Machine Learning (ICML). Cutset networks: A simple, tractable, and scalable\nLiu,A.,Niepert,M.,andVandenBroeck,G.(2024b). approach for improving the accuracy of chow-liu\nImage inpainting via tractable steering of diffusion trees. In Machine Learning and Knowledge Dis-\nmodels. In Proceedings of the Twelfth International covery in Databases: European Conference, ECML\nConference on Learning Representations (ICLR). PKDD 2014, Nancy, France, September 15-19,\n2014. Proceedings, Part II 14, pages 630–645.\nLiu, A. and Van den Broeck, G. (2021). Tractable\nSpringer.\nregularization of probabilistic circuits. In Ad-\nvancesinNeuralInformationProcessingSystems34 Raz, R. and Yehudayoff, A. (2008). Balancing syn-\n(NeurIPS). tactically multilinear arithmetic circuits. Computa-\ntional Complexity, 17:515–535.\nLoconte,L.,DiMauro,N.,Peharz,R.,andVergari,A.\n(2023). How to turn your knowledge graph embed- Raz,R.andYehudayoff,A.(2009). Lowerboundsand\ndings into generative models. Advances in Neural separations for constant depth multilinear circuits.\nInformation Processing Systems, 36. Computational Complexity, 18:171–207.\nLoconte, L., Mari, A., Gala, G., Peharz, R., de Cam- Rooshenas, A. and Lowd, D. (2014). Learning sum-\npos, C., Quaeghebeur, E., Vessio, G., and Vergari, product networks with direct and indirect variable\nA.(2024a). Whatistherelationshipbetweentensor interactions. In International Conference on Ma-\nfactorizations and circuits (and how can we exploit chine Learning, pages 710–718. PMLR.\nit)? arXiv preprint arXiv:2409.07953. Roth, D.(1996). Onthehardnessofapproximaterea-\nLoconte, L., Mengel, S., and Vergari, A. (2024b). soning. Artificial Intelligence, 82(1-2):273–302.\nSum of squares circuits. arXiv preprint Shen,Y.,Choi,A.,andDarwiche,A.(2016).Tractable\narXiv:2408.11778. operations for arithmetic circuits of probabilistic\nLoconte, L., Sladek, A., Mengel, S., Trapp, M., Solin, models. Advances in Neural Information Process-\nA., Gillis, N., and Vergari, A. (2024c). Subtractive ing Systems, 29.\nmixture models via squaring: Representation and Shih, A. and Ermon, S. (2020). Probabilistic circuits\nlearning. In International Conference on Learning for variational inference in discrete graphical mod-\nRepresentations (ICLR). els. Advances in neural information processing sys-\ntems, 33:4635–4646.\nPapantonis, I. and Belle, V. (2023). Transparency in\nsum-product network decompilation. In European Sidheekh, S. and Natarajan, S. (2024). Building ex-\nConference on Artificial Intelligence, pages 1827– pressiveandtractableprobabilisticgenerativemod-\n1834. IOS Press. els: A review. arXiv preprint arXiv:2402.00759.\nPeharz, R., Gens, R., Pernkopf, F., and Domingos, Tian, J., Paz, A., and Pearl, J. (1998). Finding mini-\nP. (2016). On the latent variable interpretation in mal d-separators. Citeseer.\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nValiant, L., Skyum, S., Berkowitz, S., and Rackoff, C.\n(1983). Fast parallel computation of polynomials\nusing few processors. SIAM Journal on Computing,\n12(4):641–644.\nVergari, A., Choi, Y., Liu, A., Teso, S., and Van den\nBroeck, G. (2021). A compositional atlas of\ntractable circuit operations for probabilistic infer-\nence. InAdvances in Neural Information Processing\nSystems 34 (NeurIPS).\nWang,B.andKwiatkowska,M.(2023).Compositional\nprobabilistic and causal inference using tractable\ncircuit models. In International Conference on Ar-\ntificial Intelligence and Statistics, pages 9488–9498.\nPMLR.\nWang, B. and Van den Broeck, G. (2024). On the\nrelationship between monotone and squared proba-\nbilisticcircuits. InProceedingsoftheUAIWorkshop\non Tractable Probabilistic Modeling (TPM).\nYang, Y., Gala, G., and Peharz, R. (2023). Bayesian\nstructure scores for probabilistic circuits. In Inter-\nnational Conference on Artificial Intelligence and\nStatistics, pages 563–575. PMLR.\nYin, L. and Zhao, H. (2024). On the expressive\npower of tree-structured probabilistic circuits. In\nAdvancesinNeuralInformationProcessingSystems\n37 (NeurIPS).\nZeˇcevi´c, M., Dhami, D., Karanam, A., Natarajan,\nS., and Kersting, K. (2021). Interventional sum-\nproduct networks: Causal inference with tractable\nprobabilistic models. Advances in neural informa-\ntion processing systems, 34:15019–15031.\nZhang, H., Dang, M., Peng, N., and Van den Broeck,\nG. (2023). Tractable control for autoregressive lan-\nguage generation. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning (ICML).\nZhang, H., Juba, B., and Van den Broeck, G. (2021).\nProbabilistic generating circuits. In Proceedings\nof the 38th International Conference on Machine\nLearning (ICML).\nZhang, H., Kung, P.-N., Yoshida, M., Broeck, G.\nV. d., and Peng, N. (2024). Adaptable logical\ncontrol for large language models. arXiv preprint\narXiv:2406.13892.\nZhao, H., Melibari, M., and Poupart, P. (2015). On\ntherelationshipbetweensum-productnetworksand\nbayesian networks. In International Conference on\nMachine Learning, pages 116–124. PMLR.\nZhao, H., Poupart, P., and Gordon, G. J. (2016). A\nunifiedapproachforlearningtheparametersofsum-\nproduct networks. Advances in neural information\nprocessing systems, 29.\nRestructuring Tractable Probabilistic Circuits\nSupplementary Materials\nA ADDITIONAL PROOFS\n(cid:80)\nProposition 3.2. pA(X)= zpA aug(X,z)\nProof. Suppose that A is a structured decomposable and smooth PC respecting vtree V. Write prod(v) and\nsum(v)forthesetofproductandsumnodeswithscopeX . TheaugmentedPCA isdecomposableasifleaves\nv aug\nwith scope {Z } were contained in (the subcircuits rooted at) two different children t ,t of a product node,\nv 1 2\nthentheirparents(nodesinprod(v))wouldbecontainedint ,t , whichisacontradictionofdecomposabilityof\n1 2\nA. It is also smooth as for any sum node, if one sum node contains some leaf with scope {Z }, then it contains\nv\nsome node in sum(v), hence by smoothness of A all sum nodes contain some node in sum(v) and thus some leaf\nwith scope {Z }.\nv\nConsiderthestandardmarginalizationalgorithmforPCs(Darwiche,2003;Choietal.,2020),whereonereplaces\neachleafwhosescopeiscontainedwithinthevariablesbeingmarginalizedoutwiththeconstant1. Thiscorrectly\nmarginalizes the function represented by the PC if the PC is decomposable and smooth. If we marginalize over\nall newly introduced latents Z, it is immediate that the resulting PC represents the same function as A.\nTheorem 3.3. Let A be a structured-decomposable and smooth PC over variables X respecting vtree V. Then\n(cid:80)there exists a Bayesian network GA over variables X and Z = {Z v|v ∈ V} with graph V\nv(cid:55)→Zv\nsuch that\nzp G(X,z)=pA(X).\nProof. In Section 3.1 we described a Bayesian network p = p∗ with the required graph. It remains to show\nG\nthat this network represents the same distribution as A. We will do this by showing that the Bayesian network\nhas the same distribution as the augmented PC, i.e. p G(X,Z)=pA aug(X,Z).\nThe key observation is to consider the induced trees of the augmented PC (Zhao et al., 2016):\nDefinition A.1 (Induced Trees). Given a decomposable and smooth circuit A, let T be a subgraph of A. We\nsay that T is an induced tree of A if (1) root(A) ∈ T; (2) If t ∈ T is a sum node, then exactly one child\nof t (and the corresponding edge) is in T; and (3) If t ∈ T is a product node, then all children of t (and the\ncorresponding edges) are in T.\nIt is easy to see that an induced tree T is indeed a tree, as otherwise decomposability would be violated. Let T\nbe the set of all induced trees of A . Each induced tree defines a function:\naug\n(cid:89) (cid:89) (cid:89)\npA aug,T(X,Z):= w\nti,tj\nf t(X sc(t)) f t(Z sc(t)) (1)\n(ti,tj)∈sumedges(T) t∈leaves X(T) t∈leaves Z(T)\nwhere sumedges(T) denotes the set of outgoing edges from sum nodes in T, and leaves (T),leaves (T)\nX Z\ndenote the set of leaf nodes in T with scope corresponding to a variable in X,Z respectively. The distribution\nof the augmented PC is then in fact given by the sum of these functions over all induced trees:\n(cid:80)\nProposition A.2 (Zhao et al. (2016)). pA aug(X,Z)=\nT∈T\npA aug,T(X,Z).\nNow, let path(v,i,j) be a predicate indicating whether there is a path between t and t (where we use p\np,j v,i\nto denote the parent vtree node of v, and as before e.g. t indicates the product node with scope X and\nv,i v\ncorresponding to Z = i). We will consider two cases depending on the value of the latents. Specifically, we\nv\nwill say that an assignment z is consistent if path(v,z ,z ) holds for all non-root inner nodes in the vtree, and\nv p\ninconsistent otherwise.\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nIfanassignmentz isinconsistent,thenforanyassignmentxoftheobservedvariables,wehavethatp (x,z)=0\nG\nby definition of the Bayesian network distribution. Now consider any induced subtree T ∈ T. Each T must\ncontain one product node for every variable scope. In particular, T must contain some product node t\nv,i\nsuch that z ̸= i (otherwise, since z is inconsistent, a (connected) tree would be impossible). We then have\nv\np (cid:80)A aug,T(x,z) = 0 for all x, as Equation 1 then contains a leaf function f t(z v) = 1 zv=i = 0. Thus pA aug(x,z) =\nT∈T\npA aug,T(x,z)=0 for any x.\nIf an assignment z is consistent, note that, by our assumption of alternating sums and products, there can be\nexactly one path from t to t , as t has a unique sum node child with scope containing X , and this sum\np,j v,j p,j v\nnode must immediately have t as a child. Thus there is exactly one induced tree T containing t for all\nv,j v,zv\n(non-root)innervtreenodesv. Further,examiningthedefinitionoftheBayesiannetworkdistributionp (X,z),\nG\nthis exactly matches the definition of pA aug,T(X,z): each sum node edge weight in the tree corresponds to a\nsum node edge weight along a path from some t to t and thus the CPT of Z given Z (the root sum\np,zp v,zv v p\nnode edge weight corresponds to the CPT for Zroot(V)), and each leaf node distribution for observed variables\ncorresponds to the CPT for that variable given its parent.\nThus we have shown that pA aug(X,Z)=p G(X,Z), as required.\nProposition 4.4. C\nw\nis a valid vtree labelling with respect to GA.\nProof. We first show that C satisfies the following properties:\nw\n1. X =(cid:83) Leaves(Z ) is a disjoint union.\n2. Ifa a:b ≤c≤Z di ≤∈C ba ,:b then for Z∈i C c:d, there exists Z′ ∈C\na:b\nsuch that Z′ is an ancestor of Z in GA.\nProperty1followsfromtheproofofcorrectnessofthesegmenttreequeryingalgorithm. Property2followsfrom\nProperty1togetherwiththekeyobservationthatwecancomputeC via(cid:83) SegmentCover(Z ,X ∩\nLeaves(Z )). Let w = a:b be a node in W with children l = a:c ac n:d d r = cZi +∈C 1a ::b b; it follows from Proi perc t: yd 1\ni\nthat C covers X and C blocks all paths from X to C ; it follows from Property 2 that C blocks\na:b a:b a:c a:c c+1:b a:c\nall paths from X to X . Hence we conclude that C is a valid labelling. A minor catch is that C may\na:c a:b w w\ncontain variables in X, but we can replace them by their parent in GA without affecting the validity of C w.\nCorollary 5.3. Any structured PC over n variables and with hidden state size h can be restructured to a\nstructured PC of depth O(logn) and size O(nh3) that represents the same distribution.\nProof. By Theorem 5.1, given a structured PC X over n variables with hidden state size h and respecting vtree\nV, we can generate a vtree W of depth O(logn) and with labelling cardinality M′ = 3. Thus, by Theorem 3.7\nwe can construct a PC representing the same function and respecting vtree W of size O(nh3). The depth of the\nPC is then also O(logn) as we have assumed alternating sum and product nodes, so the depth of the circuit is\nat most double that of the vtree.\nB Computing Minimum D-separators\nLet G be a tree-shaped Bayesian network rooted at Z; in particular, assume that the leaves of G ⊆ X∪Z and\nthe internal nodes of G ⊆ Z (e.g. Figure 1b). Then, we want to prove that Algorithm 5 computes a minimum\nd-separator C⊆G for A,B⊆X.\nAsshowninAlgorithm5,giventree-shapedBayesiannetworkGrootedatZ,theprocedureMinimumSepartor\ncomputes three sets of latent variables C , C and C. Specifically, we shall prove the following properties:\nA B\n1. C is a minimum d-separator between A and B that also blocks all paths from A to Z.\nA\n2. C is a minimum d-separator between A and B that also blocks all paths from B to Z.\nB\n3. Either C or C is a minimum d-separator between A and B in G (rooted at Z); hence C is a minimum\nA B\nd-separator between A and B in G.\nProof of Property 3. It suffices to show that P := {d-separators between A and B} and Q := {d-separators\nbetween A and B that blocks all paths from A to Z} ∪ {d-separators between A and B that blocks all paths\nRestructuring Tractable Probabilistic Circuits\nAlgorithm 5 Computing minimum d-separators for tree-shaped Bayesian network G rooted at Z\nprocedure MinimumSeparator(Z, A, B)\nif A=∅ and B=∅ then\nreturn ∅,∅,∅\nend if\nif B=∅ then\nreturn {Z},∅,∅\nend if\nif A=∅ then\nreturn ∅,{Z},∅\nend if\nfor Z ∈Children(Z) do\ni\nC ,C ,C ←MinimumSeparator(\ni,A i,B i\nZ ,A∩Leaves(Z ),B∩Leaves(Z ))\ni i i\nend for\nC\n←Min((cid:83)\nC\n∪{Z},(cid:83)\nC )\nA i i i i,A\nC\n←Min((cid:83)\nC\n∪{Z},(cid:83)\nC )\nB i i i i,B\nC←Min(C ,C )\nA B\nreturn C ,C ,C\nA B\nend procedure\nfrom B to Z} are the same set. It is obvious that Q ⊆ P and let’s prove that P ⊆ Q. Let C be a d-separator\nbetween A and B in G, then C either blocks all path from A to Z or blocks all path from B to Z; suppose not,\nthen there is a path connecting A and B through Z; contradiction.\nProof of Property 1 and Property 2. We prove Property 1 (and Property 2) by a bottom-up induction on G.\nFirst of all it is easy to verify that the three base cases, i.e. A=∅ and B=∅, A=∅ and B=∅, are correct.\nWe now prove Property 1 (the proof for Property 2 is symmetric) by induction; first of all it is clear that both\n(cid:83) (cid:83)\nC ∪{Z} and C form d-separators between A and B, and it remains to show that the minimum of\ni i i i,A\nthese two is a minimum d-separator between A and B that blocks all paths from A to Z. Suppose, towards a\ncontradiction, let C ′ be such a d-separator of size <Min(|(cid:83) C ∪{Z}|,|(cid:83) C |). There are two cases:\nA i i i i,A\n• If C ′ contains Z: let G be the subtree rooted at Z and set C ′ = C ′ ∩G . It is immediate that\nA i i i A i\nC ′ is a d-separator between A and B in G and by assumption, there exists at least one one i such that\ni i\n|C ′|<|C |; contradicting the induction hypothesis.\ni i\n• If C ′ does not contain Z: let G be the subtree rooted at Z and set C ′ = C ′∩G . Similarly, it\nA i i i,A A i\nis not hard to see that C ′ is a d-separator between A and B in G that blocks all paths from A∩G\ni,A i i\nto Z . By assumption, there exists at least one i such that |C ′| < |C |; contradicting the induction\ni i,A i,A\nhypothesis.\nC MULTIPLICATION WITH NON-STRUCTURED CIRCUITS\nGiven a contiguous structured PC A respecting a linear vtree and an arbitrary contiguous PC B, which is not\nnecessarily structured, we show a recursive algorithm that multiplies A and B in polynomial time. Specifically,\nfor each possible scope X that appears in B, we recursively construct circuit representations for the functions\na:b\np q(X a:b)·pA(X a:b|Z a=i,Z b=j) for ⊕ nodes q∈B with scope X\na:b\nand i,j hidden states of A. In particular,\nfrom pA(X a:b|Z a=i,Z b=j) we drop Z\na\n=i if a=1 and drop Z\nb+1\n=j if b=n, thus pA(X a:b|Z a=i,Z b=j)\ncorresponds to pA(X a:b|C a:b) as defined in Case 1. of Section 4.\nThe recurrence relation is similar to that of Algorithm 1. In the following derivation, we use the notations: (1)\ndenote the children of ⊕ node q by c∈Ch(q); (2) denote the children of ⊗ node c by c and c ; (3) denote the\n1 2\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nweight of the edge connecting q and c by w ; (4) for each ⊗ node c∈Ch(q), c splits X ={X ,...,X } into two\nqc q a b\ncontiguoussegments,anddenotethembyX ={X ,...,X }andX ={X ,...,X }forsomea≤m ≤b.\nc1 a mc c2 mc+1 b c\np q(X a:b)·pA(X a:b+1|Z a=i,Z b+1=j)\n(cid:88)\n= p c(X a:b)·w qc·pA(X a:b+1|Z a=i,Z b+1=j)\nc∈Ch(q)\n(cid:88)\n= p c1(X a:mc)·p c2(X mc+1:b+1)·w qc·pA(X a:b+1|Z a=i,Z b+1=j)\nc∈Ch(q)\n(cid:88) (cid:88)\n= p c1(X a:mc)·p c2(X mc+1:b+1)·w qc· pA(X a:b+1,Z mc=k|Z a=i,Z b+1=j)\nc∈Ch(q) k\n(cid:88) (cid:88)\n= w qc·pA(Z mc+1=k|Z a=i,Z b+1=j)\nc∈Ch(q) k\n·p c1(X a:mc)·pA(X\na:mc\n|Z mc+1=k,Z a=i,Z b+1=j)·p c2(X mc+1:b)·pA(X a:b|Z mc+1=k,Z a=i,Z b+1=j)\n(cid:88) (cid:88)\n= w qc·pA(Z mc+1=k|Z a=i,Z b+1=j)\nc∈Ch(q) k\n· p c1(X a:mc)·pA(X\na:mc\n|Z a=i,Z mc+1=k) · p c2(X mc+1:b)·pA(X a:b|Z mc+1=k,Z b+1=j)\nNow let’s analyze the complexity of the constructed circuit, which we denote by C. C has O(mkh2) ⊕ nodes\nin total, where m is the number of scopes in B, k := max SascopeinB|{⊕ ∈ B with scope S}|, and h is the\nhidden states size of A. Suppose that each ⊕ node in B has at most r children, then each ⊕ node in C has\nat most O(rh) children. Hence the size of C is bounded by O(mkh2 ·rh) = O(mkr·h3). Note that O(mkr)\ncorresponds to O(|B|) and O(h3) is upper-bounded by O(h4), which is O(|A|2); hence the size of C is bounded\nby O(|A|2|B|), which is the same complexity as stated in Theorem 4.3. Hence, we can remove the assumption\nthat B has to be structured from Theorem 4.3.\nTheorem C.1. LetA andB becontiguousPCswithB notnecessarilystructured. IfA isstructuredrespecting\nthe linear vtree, then A and B can be multiplied in polynomial time and the size of the product PC is bounded\nby O(|A|2|B|).\nBy a similar recursive construction, we can also remove the assumption that B has to be structured from\nTheorem 4.5:\nTheorem C.2. Let A and B be contiguous PCs. If A is structured of depth d, then then we can construct a\nproduct circuit of A and B of size bounded by O(|A|12d|B|).",
    "pdf_filename": "Restructuring_Tractable_Probabilistic_Circuits.pdf"
}