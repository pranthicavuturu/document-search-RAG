{
    "title": "Restructuring Tractable Probabilistic Circuits",
    "context": "Probabilistic circuits (PCs) is a unifying rep- resentation for probabilistic models that sup- port tractable inference. Numerous applica- tions of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algo- rithms require that the circuits respect the same structure, i.e. variable scopes decom- poses according to the same vtree. In this work, we propose and study the task of re- structuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We pro- pose a generic approach for this problem and show that it leads to novel polynomial- time algorithms for multiplying circuits re- specting different vtrees, as well as a practi- cal depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling effi- cient inference by changing their structures at inference time. 1 A key challenge in deep generative modeling is the in- tractability of probabilistic reasoning (Roth, 1996; Geh et al., 2024). To address this challenge, probabilistic circuits (PCs) (Darwiche, 2003; Poon and Domingos, 2011; Choi et al., 2020) has emerged as a unifying representation of tractable generative models, which Preprint. Correspondence to Honghua Zhang [hzhang19@cs.ucla.edu] and Benjie Wang [benjiewang@ucla.edu]. support efficient and exact evaluation of various infer- ence queries like marginalization. The tractability of PCs has now proven crucial in a range of applications, such as causal inference (Zeˇcevi´c et al., 2021; Wang and Kwiatkowska, 2023; Busch et al., 2024), knowl- edge graph learning (Loconte et al., 2023) and ensur- ing fairness in decision making (Choi et al., 2021). Probabilistic circuits represent distributions as com- putation graphs of sums and products. A crucial as- pect to the design of PCs is the structure of the com- putation graph, that is, how distributions are fac- torized into (conditionally) independent components. The structure of PCs affects their tractability, model- ing performance and computational efficiency. In this work, we consider the problem of restructuring PCs: constructing a new PC that follows a particular (tar- get) structure while representing the same distribu- tion. We present a general algorithm for restructuring structured-decomposable circuits by considering their graphical model representations. Specifically, we lever- age the graphical models to reason about conditional independencies and recursively construct a new PC conforming to the desired structure. We then investigate two key applications of PC re- structuring: circuit multiplication and depth reduc- tion. Circuit multiplication is a fundamental opera- tion used for answering various inference queries (Ver- gari et al., 2021), such as conditioning on logical con- straints (Choi et al., 2015; Ahmed et al., 2022; Liu et al., 2024b; Zhang et al., 2023, 2024), computing expected predictions of classifiers (Khosravi et al., 2019) and causal backdoor adjustment (Wang and Kwiatkowska, 2023), as well as in improving the ex- pressive power of circuits through squaring (Loconte et al., 2024c,b; Wang and Van den Broeck, 2024). Though the problem of multiplying circuits of differ- ent structures is in general #P-hard (Vergari et al., 2021), we identify a new class of PCs, which we call contiguous circuits, where it is possible to multiply circuits of different structures in polynomial (or quasi- arXiv:2411.12256v1  [cs.AI]  19 Nov 2024",
    "body": "Restructuring Tractable Probabilistic Circuits\nHonghua Zhang\nBenjie Wang\nUniversity of California, Los Angeles\nUniversity of California, Los Angeles\nMarcelo Arenas\nGuy Van den Broeck\nPontificia Universidad Cat´olica de Chile\nUniversity of California, Los Angeles\nAbstract\nProbabilistic circuits (PCs) is a unifying rep-\nresentation for probabilistic models that sup-\nport tractable inference. Numerous applica-\ntions of PCs like controllable text generation\ndepend on the ability to efficiently multiply\ntwo circuits.\nExisting multiplication algo-\nrithms require that the circuits respect the\nsame structure, i.e. variable scopes decom-\nposes according to the same vtree.\nIn this\nwork, we propose and study the task of re-\nstructuring structured(-decomposable) PCs,\nthat is, transforming a structured PC such\nthat it conforms to a target vtree. We pro-\npose a generic approach for this problem\nand show that it leads to novel polynomial-\ntime algorithms for multiplying circuits re-\nspecting different vtrees, as well as a practi-\ncal depth-reduction algorithm that preserves\nstructured decomposibility. Our work opens\nup new avenues for tractable PC inference,\nsuggesting the possibility of training with less\nrestrictive PC structures while enabling effi-\ncient inference by changing their structures\nat inference time.\n1\nINTRODUCTION\nA key challenge in deep generative modeling is the in-\ntractability of probabilistic reasoning (Roth, 1996; Geh\net al., 2024). To address this challenge, probabilistic\ncircuits (PCs) (Darwiche, 2003; Poon and Domingos,\n2011; Choi et al., 2020) has emerged as a unifying\nrepresentation of tractable generative models, which\nPreprint.\nCorrespondence to Honghua Zhang [hzhang19@cs.ucla.edu]\nand Benjie Wang [benjiewang@ucla.edu].\nsupport efficient and exact evaluation of various infer-\nence queries like marginalization. The tractability of\nPCs has now proven crucial in a range of applications,\nsuch as causal inference (Zeˇcevi´c et al., 2021; Wang\nand Kwiatkowska, 2023; Busch et al., 2024), knowl-\nedge graph learning (Loconte et al., 2023) and ensur-\ning fairness in decision making (Choi et al., 2021).\nProbabilistic circuits represent distributions as com-\nputation graphs of sums and products. A crucial as-\npect to the design of PCs is the structure of the com-\nputation graph, that is, how distributions are fac-\ntorized into (conditionally) independent components.\nThe structure of PCs affects their tractability, model-\ning performance and computational efficiency. In this\nwork, we consider the problem of restructuring PCs:\nconstructing a new PC that follows a particular (tar-\nget) structure while representing the same distribu-\ntion. We present a general algorithm for restructuring\nstructured-decomposable circuits by considering their\ngraphical model representations. Specifically, we lever-\nage the graphical models to reason about conditional\nindependencies and recursively construct a new PC\nconforming to the desired structure.\nWe then investigate two key applications of PC re-\nstructuring: circuit multiplication and depth reduc-\ntion. Circuit multiplication is a fundamental opera-\ntion used for answering various inference queries (Ver-\ngari et al., 2021), such as conditioning on logical con-\nstraints (Choi et al., 2015; Ahmed et al., 2022; Liu\net al., 2024b; Zhang et al., 2023, 2024), computing\nexpected predictions of classifiers (Khosravi et al.,\n2019) and causal backdoor adjustment (Wang and\nKwiatkowska, 2023), as well as in improving the ex-\npressive power of circuits through squaring (Loconte\net al., 2024c,b; Wang and Van den Broeck, 2024).\nThough the problem of multiplying circuits of differ-\nent structures is in general #P-hard (Vergari et al.,\n2021), we identify a new class of PCs, which we call\ncontiguous circuits, where it is possible to multiply\ncircuits of different structures in polynomial (or quasi-\narXiv:2411.12256v1  [cs.AI]  19 Nov 2024\n\nRestructuring Tractable Probabilistic Circuits\npolynomial) time using our algorithm.\nWe also consider depth reduction, a well-established\ntheoretical tool for reducing the depth of a cir-\ncuit (Valiant et al., 1983; Raz and Yehudayoff, 2008).\nRecent PC implementations have focused on layer-wise\nparallelization of PC inference via modern GPUs, and\ndepth reduction enables greater parallelization (Pe-\nharz et al., 2020; Dang et al., 2021; Liu et al., 2024a;\nLoconte et al., 2024a). In this work, we show that our\nrestructuring algorithm can be used to transform a\nstructured-decomposable circuit to an equivalent log-\ndepth circuit, with much tighter upper bounds than\ngiven by prior work. This opens up new possibilities of\npractically implementing depth reduction techniques\nto speed up PC inference.\n2\nPROBABILISTIC CIRCUITS\nNotation\nWe will use uppercase to denote variables\n(e.g. X) and lowercase to denote values of those vari-\nables (e.g. x). We use boldface to denote sets of vari-\nables/values (e.g. X, x).\nDefinition 2.1 (Probabilistic Circuit). A probabilis-\ntic circuit (PC) A = (G, w) represents a joint prob-\nability distribution over random variables X through\na rooted directed acyclic (computation) graph (DAG),\nconsisting of sum (⊕), product (⊗), and leaf nodes\n(L), parameterized by w. Each node t represents a\nprobability distribution pt(X), defined recursively by:\npt(x) =\n\n\n\n\n\nft(x)\nif t is a leaf node\nQ\nc∈ch(t) pc(x)\nif t is a product node\nP\nc∈ch(t) wt,cpc(x)\nif t is a sum node\nwhere ft(x) is a univariate input distribution function\n(e.g. Gaussian, Categorical), we use ch(t) to denote\nthe set of children of a node t, and wt,c is the non-\nnegative weight associated with the edge (t, c) in the\nDAG, which satisfy the constraint that P\nc∈ch(t) wt,c =\n1 for every sum node t. We define the scope of a node t\nto be the variables it depends on. The function repre-\nsented by a PC, denoted pA (x), is the function repre-\nsented by its root node; and the size of a PC, denoted\n|A |, is the number of edges in its graph.\nIntuitively, product nodes represent a factorized prod-\nuct of its child distributions, while sum nodes repre-\nsent a weighted mixture of its child distributions. For\nsimplicity, in the rest of this paper we assume that\nsum/leaf and product nodes alternate (i.e. child of a\nsum is a product, and child of a product is a leaf or\nsum), and that each product has exactly two children.\nThe key feature of PCs is their tractability, i.e., the\nability to answer queries about the distributions they\nrepresent exactly and in polynomial time. Two com-\nmonly assumed properties known as smoothness and\ndecomposability ensure efficient marginalization:\nDefinition 2.2 (Smoothness and Decomposability).\nA sum node is smooth if all of its children have the\nsame scope.\nA product node is decomposable if its\nchildren have disjoint scope. A PC is smooth (resp.\ndecomposable) if all of its sum (resp. product) nodes\nare smooth (resp. decomposable).\nIntuitively, decomposability requires that a product\nnode partitions its scope among its children.\nFor\nmany other important queries, it is useful to enforce a\nstronger form of decomposability, known as structured-\ndecomposability, that requires that product nodes with\nthe same scope decompose in the same way.\nDefinition 2.3 (Vtree). A vtree V over variables X is\na rooted binary tree, where each X ∈X is associated\nwith a unique leaf node v (we write Xv for the variable\nassociated with node v). Each inner node v covers a\nset of variables Xv, satisfying Xv = Xl ∪Xr where\nl, r are the children of v. We write Vv to denote the\nsubtree rooted at v.\nDefinition 2.4 (Structured Decomposability). A PC\nA is structured-decomposable (w.r.t a vtree V ) if ev-\nery product node t ∈A decomposes its scope accord-\ning to some inner vtree node v ∈V .\nThe main advantage of structured decomposability is\nthat it enables tractable circuit multiplication of two\ncircuits respecting the same vtree, which is a core sub-\nroutine for many applications.\nHowever, structured\ndecomposable circuits can be less expressive efficient\nin general (de Colnet and Mengel, 2021).\n3\nPC RESTRUCTURING\nIn this section, we describe a generic approach that\nrestructures any structured-decomposable PC respect-\ning a target vtree.\nThe approach consists of three\nsteps: (1) construct a Bayesian network representa-\ntion of the PC; (2) find sets of latent variables in the\nBayesian network that induce conditional independe-\ncies required by the target vtree; (3) construct a new\nstructured PC recursively leveraging the conditional\nindependence derived in (2).\n3.1\nStructured PCs as Bayesian Networks\nIt is known that one can efficiently compile a tree-\nshaped Bayesian network to an equivalent probabilis-\ntic circuit (Darwiche, 2003; Poon and Domingos, 2011;\nDang et al., 2020; Liu and Van den Broeck, 2021). In\nthis subsection, we describe how to go in the oppo-\nsite direction, i.e. converting an arbitrary structured-\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\ndecomposable PC to a tree-shaped Bayesian network\nwith linearly many variables.\nLet A be a structured PC over variables X respecting\nvtree V . Given a vtree node v ∈V , we write prod(v)\nto denote the set of all product nodes with scope Xv.\nWe define the hidden state size h of the circuit to be\nmaxv∈V |prod(v)|. Writing n for the number of vari-\nables, the size of the circuit is then O(nh2).1\nWe begin by providing a latent variable interpreta-\ntion of structured PCs. Specifically, we define an aug-\nmented PC which explicitly associates latent variables\nwith product nodes for each variable scope.\nGiven\nsome vtree node v, let us associate each t ∈prod(v)\nwith a unique index idx(t) ∈{0, ..., |prod(v)|−1}, also\nwriting tv,i to refer to the product node with index i in\nprod(v). Then we can introduce a categorical latent\nvariable Zv whose value corresponds to a particular\nproduct node in prod(v):\nDefinition 3.1 (Augmented PC). Given a structured-\ndecomposable and smooth PC A over variables X re-\nspecting vtree V , we define the augmented PC Aaug\nto be a copy of A where for each vtree node v ∈V ,\nwe add an additional child taug to each product node\nt ∈prod(v) that is a leaf node with scope Zv and leaf\nfunction ftaug(Zv) = 1Zv=idx(t).\nIt is not hard to see that the augmented PC Aaug\nis a PC over variables X, Z and retains structured\ndecomposability and smoothness. Further, the stan-\ndard marginalization algorithm for PCs ensures that\nthe augmented PC has the correct distribution:\nProposition 3.2. pA (X) = P\nz pAaug(X, z)\nLet Vv→Zv be the rooted DAG obtained by replacing\nall inner nodes v in vtree V with variable Zv (cf. Fig.\n1).\nNow, we claim that the augmented PC can be\ninterpreted as a Bayesian network with graph struc-\nture Vv→Zv. To do this, we construct a distribution\np∗(X, Z), based on the augmented PC, that factor-\nizes as required by the Bayesian network structure.\nThere are three cases to consider: (i) the root node\np∗(Zroot(V )), (ii) the leaf nodes p∗(Xv|Zp), and (iii)\nother nodes p∗(Zv|Zp) (where we write p for the par-\nent of v in V ). In case (i), we set p∗(Zv = i) := wi\nwhere wi is the weight of the edge from the root sum\nnode to the product node tv,i.\nIn case (ii), we set\np∗(Xv|Zp = j) = pt(Xv), where t is the leaf node child\n(with scope Xv) of the product node tp,j. Finally, in\ncase (iii) we note that due to alternating sums and\nproducts, tp,j must have a sum node child, which may\n1The number of active sum nodes per vtree node is at\nmost h, as each such node must have a different product\nnode parent corresponding to the parent vtree node scope.\nThis leads to O(h2) edges per vtree node.\nX3\nX6\nX4\nX5\nX1\nX2\nX2\nX3 X4\nX5\nX1\nX6\n1:6\n1:3\n2:3\n4:5\n4:6\n{Z4:5}\n{Z1:3, Z2:3}\n{Z2:3, Z4:6}\n{}\nX2\nX3 X4\nX5\nX1\nX6\nZ1:6\nZ1:3\nZ2:3\nZ4:5\nZ4:6\nX3\n{Z2:3, Z4\n{Z2\n{Z2:3, Z4:6}\na\nb\nc\nd\ne\n{Z2:3}\n{Z4:6}\n{Z4:5}\n{Z4:5}\n{Z1:3}\n{Z2:3}\n(a) A (contiguous) vtree V\nX3\nX6\nX4\nX5\nX1\nX2\nX2\nX3 X4\nX5\nX1\nX6\n1:6\n1:3\n2:3\n4:5\n4:6\n{Z4:5}\n{Z1:3, Z2:3}\n{Z2:3, Z4:6}\n{}\nX2\nX3 X4\nX5\nX1\nX6\nZ1:6\nZ1:3\nZ2:3\nZ4:5\nZ4:6\n{Z\n{Z2:3, Z4:6}\na\nb\nc\nd\ne\n{Z2:3}\n{Z4:6}\n{Z4:5}\n{Z4:5}\n{Z1:3}\n{Z2:3}\n(b) Bayesian network Vv7→Zv\nX3\nX6\nX4\nX5\nX1\nX2\nX2\nX3 X4\nX5\nX1\nX6\n1:6\n1:3\n2:3\n4:5\n4:6\n{Z4:5}\n{Z1:3, Z2:3}\n{Z2:3, Z4:6}\n{}\nX2\nX3 X4\nX5\nX1\nX6\nZ1:6\nZ1:3\nZ2:3\nZ4:5\nZ4:6\n{\n{Z2:3, Z4:6}\na\nb\nc\nd\ne\n{Z2:3}\n{Z4:6}\n{Z4:5}\n{Z4:5}\n{Z1:3}\n{Z2:3}\n(c) A labelling of vtree W.\nFigure 1: Fig. 1a shows a vtree V for some contiguous\nPC A ; Fig. 1b shows a Bayesian network representa-\ntion GA for A ; Fig. 1c shows a valid labelling of vtree\nW with respect to GA .\nor may not have a weighted edge to tv,i (whose weight\nwe denote by wij if it exists). We thus define:\np∗(Zv = i|Zp = j) =\n(\nwij\n∃path from tp,j to tv,i\n0\notherwise\nIt remains to show that this distribution faithfully\nrepresents the distribution of the augmented PC, i.e.\npAaug = p∗. The intuitive idea is that each value of Z\ncorresponds to a subtree of Aaug, whose value is pre-\ncisely given by the product of weights and leaf func-\ntions specified by the Bayesian network; we refer read-\ners to the Appendix for the complete proof. We thus\nhave the following mapping from structured PCs to\ntree-shaped Bayesian networks:\nTheorem 3.3. Let A be a structured-decomposable\nand smooth PC over variables X respecting vtree V .\nThen there exists a Bayesian network GA over vari-\nables X and Z = {Zv|v ∈V } with graph Vv7→Zv such\nthat P\nz pG(X, z) = pA (X).\nSince we have shown that pA and pG represents the\nsame distribution over the observed variables X, we\nwill drop the subscripts when there is no ambiguity.\n3.2\nRecursive PC Restructuring\nSuppose we have a PC A with its Bayesian network\nrepresentation GA and vtree V , and let W be some\n\nRestructuring Tractable Probabilistic Circuits\np(Xw|Cw)\np(Xl|Cl)\np(Xr|Cr)\np(Xw|Cl, Cr)\np(Cl, Cr|Cw)\nFigure 2: Recursive construction of vectors of sum\nnodes representing p(Xw | Cw)\nother vtree. We now show how to construct a new PC\nrespecting W that encodes the same distribution as A .\nThe rough idea is to label each vtree node w∈W with\na subset of latent variables Cw ⊆GA such that Xw is\nconditionally independent from X \\ Xw given Cw. To\ncharacterize such properties, we introduce covers:\nDefinition\n3.4\n(Cover).\nGiven\na\ntree-shaped\nBayesian network GA as constructed in Sec. 3.1, we\nsay that C ⊆Z covers S ⊆X if C blocks2 all paths\nbetween S and X\\S in GA .\nOur definition of cover is a special case of d-\nseparation (Geiger et al., 1990), which characterizes\nconditional independence for Bayesian networks:\nProposition 3.5 (Geiger et al. (1990)). A, B ⊆GA\nare conditionally independent given C ⊆GA if and\nonly if C blocks all paths between A and B. In partic-\nular, if C covers S then S and X\\S are conditionally\nindependent given C.\nOur goal is to recursively construct vectors of sum\nnodes ⊕i representing the probability distributions\np(Xw|Cw =i). Letting l and r be the children of w, we\nwill establish a recurrence relation between p(Xw|Cw),\np(Xl|Cl) and p(Xr|Cr). This requires the vtree labels\nto satisfy the following properties:\nDefinition 3.6 (Valid Vtree Labelling). Given the\nBayesian network GA and target vtree W, a valid la-\nbelling of W with respect to GA associates each node\nw∈W with a subset of latent variables Cw ⊆GV s.t.\n1. Cw covers Xw in GA .\n2. Cl blocks all paths between Xl and Cr ∪Cw.\n3. Cr blocks all paths between Xr and Cl ∪Cw.\nFurthermore, w.l.o.g., we set Croot of W := ∅and\nCXj :=parent of Xj in GA for the leaf nodes Xj ∈W.\nSee Figure 1c for an example.\nAssuming that we have computed a valid labelling for\nW, we can then proceed to construct the desired PC\n2a path P is blocked by a set S if P ∩S ̸= ∅.\nby a bottom-up recursion on W. For the base case,\nif w is a leaf node representing some random variable\nXj, p(Xj | CXj) = p(Xj | parent of Xj in GA ), which\nis directly given by the conditional probability table of\nGA . For the induction step, when w is a inner node\nwith children l and r, we have the recurrence relation:\np(Xw | Cw)\n=\nX\n(Cl∪Cr)\\Cw\np(Xl, Xr | Cl, Cr) · p(Cl, Cr | Cw)\n=\nX\n(Cl∪Cr)\\Cw\np(Xl | Cl) · p(Xr | Cr) · p(Cl, Cr | Cw)\nHere the first step follows from Property 2 and 3, and\nthe second step follows from all properties in Defin-\ntion 3.6. The circuit materialization of the recurrence\nrelation is shown in Figure 2. Note that if w is the\nroot, then p(Xw | Cw) becomes p(X), which is a sin-\ngle sum node representing the distribution of A . The\ncomplete recursion is given by Algorithm 1.\nAlgorithm 1 Construct PC with respect to W\nprocedure ConstructCircuit(w)\nif w is a leaf node Xi then\nreturn p(Xi | CXi)\nend if\nl, r ←Children(w)\nL\nXl,Cl ←ConstructCircuit(l)\nL\nXr,Cr ←ConstructCircuit(r)\nL\nXw,Cw ←\nP\n(Cl∪Cr)\\Cw\nL\nCl · L\nCr ·p(Cl, Cr|Cw)\nreturn L\nCw\nend procedure\nTheorem 3.7. Let h be the number of hidden states of\nthe original PC A and n the number of random vari-\nables. The number of hidden states of the restructured\nPC is given by O(hM) where M = maxw∈W |Cl ∪Cr|\nand the size of the restructured PC is bounded by\nO(nhM ′) where M ′ =maxw∈W |Cl ∪Cr ∪Cw| ≤2M.\nWe refer to M ′ as the cardinality of the labelling Cw.\nProof. Let A ′ be the restructured circuit respecting\nW. As described in Algorithm 1, for each inner node\nw ∈W, we construct two layers of nodes as shown\nin Figure 2. By construction, the product layer con-\ntains all product nodes respecting the vtree node w\nand its cardinality is given by O(h|Cl∪Cr|); we set\nM := maxw∈W |Cl ∪Cr| and it follows that the hid-\nden states size of B is given by O(hM).\nSimilarly,\nthe number of edges in the sum layer is given by\nO(h|Cl∪Cr∪Cw|) and the number of product edges is\ngiven by O(h|Cl∪Cr|); since there are O(n) vtree nodes\nin total, the total number of edges in B is given by\nO(nhM ′), with M ′ = maxw∈W |Cl ∪Cr ∪Cw|.\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nRemark 3.8. By Theorem 3.7, the restructured PC A ′\nhas hidden state size O(hM), which gives a circuit of\nsize Θ(nh2M) only if A ′ is densely connected. In fact,\nwe will show in Section 4 and 5 that the restructured\nPCs are often sparsely connected, resulting in sizes\nmuch smaller than O(nh2M). Thus, while the graphi-\ncal model representation is useful for reasoning about\nconditional independencies, the circuit representation\nallows us to visualize and exploit the sparsity for effi-\ncient inference (Dang et al., 2022a; Liu et al., 2024a).\n3.3\nComputing Vtree Labelling\nThe next question that immediately arises is how to\ncompute a valid labelling for W with respect to GA .\nOne naive solution is to set Cw to be Z, the set of all\nlatent variables in GA . However, this is not desirable\nas M ′ = maxw∈W |Cl ∪Cr ∪Cw| = |Z| = n−1, result-\ning in the restructured circuit having exponential size\nO(nhn−1). Hence we present a greedy approach that\ncomputes a labelling while trying to minimize M ′.\nThe algorithm proceeds top-down on W. For the base\ncase where w is the root, we set Cw := ∅. For the\ninductive step, let l and r be the children of w and\nassume that we have computed Cw as a cover for Xw\nin GA : we (1) split GA into connected components\n{Gi} via Cw; then (2) within each connected com-\nponent Gi, we compute a minimum d-separator Ci\nthat blocks all paths between Xl ∩Gi and Xr ∩Gi by\ncalling the sub-routine MinimumSeparator. We set\nDw := (S\niCi) ∪Cw and observe that Dw covers both\nXl and Xr in GA . To compute Cl, similarly for Cr, we\nconsider all paths starting from Xl and stopping im-\nmediately when reaching some Zj ∈Dw, and we let Cl\nto be the set containing all such Zjs. The pseudo code\nis shown in Algorithm 2. Note that the MinimumSep-\nAlgorithm 2 Computing Cw for w∈W\nprocedure ComputeLabel(w, Cw)\n{Gi} ←ConnectedComponents(GA , Cw)\nCi ←MinimumSeparator(Gi, Xl∩Gi, Xr∩Gi)\nDw ←(S\niCi) ∪Cw\nCl ←{Zj ∈Dw : Paths(Xl, Zj) ∩Dw ={Zj}}\nCr ←{Zj ∈Dw : Paths(Xr, Zj) ∩Dw ={Zj}}\nComputeLabel(l, Cl)\nComputeLabel(r, Cr)\nend procedure\narator procedure called in Algorithm 2 computes a\nminimum d-separator that blocks all paths between\nXl and Xr in the subgraph Gi. Even though polytime\nalgorithms for computing minimum d-separators exist\nin literature (Tian et al., 1998), we derive a linear-time\nalgorithm that is easy to implement for our use case,\nwhere Gi is a rooted tree with leaves in Xl, Xr and\nCw. We refer readers to the Appendix for details.\nProposition 3.9. Algorithm 2 computes a valid la-\nbelling with respect to GA .\nProof. We prove by a top-down induction on W that\nthe labelling Cw computed by Algorithm 2 is valid.\nAssume that Cw covers Xw in GA , we want to show\nthat Cl and Cr satisfy the properties from Defini-\ntion 3.6. To prove that Cl covers Xl, we consider a\npath from Xa ∈Xl to Xb ∈X\\Xl. (1) If Xa and Xb are\nin the same Gi, then the path is blocked by Ci. (2) If\nXa and Xb are in different Gis, then the path contains\nsome node Z ∈Cw, and we can choose from the path\nthe first Z ∈Cw. Then Z ∈Cl by construction, imply-\ning that the path is blocked by Cl. Hence we conclude\nthat Cl is a cover for Xl, satisfying Property 1. To\nprove that Cl satisfies Property 2, we argue that be-\ncause Cr and Cw are both subsets of Dw, all paths\nfrom Xl to Cr ∪Cw will be blocked by Cl by the way\nthat Cl is constructed. We can show that Cr satisfies\nProperty 1 and Property 3 by the same argument.\nThough Algorithm 2 computes a valid labelling while\ngreedily minimizing |Cl ∪Cr ∪Cw|, we do not know\nwhether M ′ = maxw∈W |Cl∪Cr∪Cw| is globally min-\nimized or not. In addition, we hypothesize that if we\ncan find a minimum vtree labelling, then the size of the\nPC constructed by Algorithm 1 is optimal. We leave it\nas an open problem to design an algorithm that com-\nputes minimum labellings and prove the optimality of\nAlgorithm 1 given a minimum labelling.\nNonetheless we show that Algorithm 1 yields novel\npolynomial-time algorithms for the tasks of PC mul-\ntiplication and depth-reduction. Specifically, we show\nthat for important subclasses of PCs, we can compute\nvtree labellings of constant or O(log n) cardinality. We\nrefer readers to Section 4 and Section 5 for details.\n3.4\nCorollaries\nWith our restructuring algorithm in hand, we now ex-\namine the restructuring of two other types of circuits:\nnamely, deterministic PCs, and logical circuits.\nDefinition 3.10 (Determinism). A sum node is de-\nterministic if for every value x of X, at most one child\nc returns a non-zero value (i.e. pc(x) > 0). A PC is\ndeterminstic if all of its sum nodes are deterministic.\nDeterminism is crucial for tractability of various infer-\nence queries such as computing the most likely state\n(MAP) (Peharz et al., 2016; Conaty et al., 2017) or\ncomputing the entropy of the PC’s distribution (Shih\nand Ermon, 2020; Vergari et al., 2021). It is thus of\ninterest to ask whether applying our restructuring al-\ngorithm maintains determinism.\n\nRestructuring Tractable Probabilistic Circuits\nClaim 3.11. Algorithm 1 preserves determinism.\nProof. If the original circuit is deterministic, then each\nassignment to the observed variables fully determines\nthe values of all latent variables (and thus the latents\nbeing conditioned on for the restructuring). Hence the\nconstructed sum nodes are deterministic.\nAlthough we have focused on probabilistic circuits up\nto this point, our restructuring algorithm also ap-\nplies to logical circuits - in particular, structured-\ndecomposable negation normal form (SDNNF) cir-\ncuits3 (Pipatsrisawat and Darwiche, 2008).\nTo see\nthis, we use a simple trick: (1) convert the logical cir-\ncuit into a probabilistic circuit by replacing ∨with\n⊕and ∧with ⊗, and assigning positive weights to ⊕\nedges; (2) restructure the PC; (3) convert the PC back\nto a logical circuit by replacing ⊕with ∨and ⊗with\n∧, and removing the weights. It is immediate that the\nlogical circuits and the corresponding PCs have the\nsame support throughout the process.\nIt is also not hard to see that this procedure for log-\nical circuits retains determinism, so, e.g, an ordered\nbinary decision diagram (OBDD) can be efficiently re-\nstructured into a deterministic SDNNF with the re-\nverse order while maintaining the ability to perform\nmodel counting (Darwiche and Marquis, 2002).\n4\nPC MULTIPLICATION\nOne important application of restructuring PCs is cir-\ncuit multiplication:\ngiven two PCs A and B, the\ngoal is to construct a tractable PC C\nsuch that\npC (x) ∝pA (x) · pB(x). PC multiplication was pre-\nviously only addressed for structured PCs respecting\nthe same vtree (Shen et al., 2016; Vergari et al., 2021).\nCircuit restructuring immediately gives us a means of\nmultiplying two structured circuits respecting differ-\nent vtrees, as we can simply restructure one of them\nto be compatible with the other. Though the restruc-\ntured PC will in general have exponential size, in this\nsection, we consider practical cases where circuit mul-\ntiplications with respect to different vtrees is tractable.\nWe start by introducing a new structural property of\ntractable PCs called contiguity.\nDefinition 4.1 (Contiguity). Given the canonical or-\ndering of variables X1, X2, . . . , Xn, a PC node is con-\ntiguous if its scope is of the form Xa, Xa+1, . . . , Xb for\nsome 1≤a≤b≤n. A smooth and decomposable PC is\ncontiguous if all of its nodes are contiguous.\n3Many other representations, such as the ordered binary\ndecision diagram (OBDD) and deterministic finite automa-\nton (DFA), can be converted efficiently to (deterministic)\nSDNNFs (Amarilli et al., 2024).\nZ1\nX1\nZ2\nX2\nZn\nXn\nFigure 3: GA for A with a linear vtree\nNote that a contiguous circuit is not necessarily\nstructured-decomposable and 0.5⊗p(X1)⊗p(X2, X3)⊕\n0.5⊗p(X1, X2)⊗p(X3) is such an example. Intuitively,\nrandom variables forming contiguous scopes can often\nbe covered by vtree labellings of small cardinalities.\nCase 1. For the multiplication of contiguous PCs A\nand B, we start by considering the case when A is a\ncontiguous structured PC respecting the linear vtree\nV and B is a contiguous structured PC respecting\nan arbitrary vtree W. It follows from Section 3.1 that\nthe Bayesian network representation for A is a hidden\nMarkov model (Rabiner, 1989), as shown in Figure 3.\nBy the definition of contiguity, each node w ∈W has\na scope of the form Xa:b := {Xa, . . . , Xb} and we can\nlabel it with Ca:b := {Za, Zb+1}; in particular, we drop\nZa if a = 1 and drop Zb+1 if b = n.\nClaim 4.2. Ca:b is a valid vtree labelling of W re-\nspecting GA with cardinality M ′ = 3.\nThen it follows from Theorem 3.7 that the size of A ′,\ni.e., the PC obtained by restructuring A respecting\nW, is bounded by O(nh3), with O(|A |2) being a looser\nbound. Eventually we can compute the product of A ′\nof B tractably by the existing algorithm for multiply-\ning two circuits respecting the same vtree (Shen et al.,\n2016; Vergari et al., 2021).\nTheorem 4.3. Let A and B be contiguous structured\nPCs.\nIf A has a linear vtree, then A and B can\nbe multiplied in polynomial time and the size of the\nproduct PC is bounded by O(|A |2|B|).\nCase 2. Then we consider the more general case where\nA is a contiguous structured PC of depth d respecting\nvtree V and B is a contiguous structured PC with an\narbitrary vtree W. Similarly to the previous case, our\ngoal is to come up with a small labelling of W with\nrespect to GA .\nSince A is contiguous, its vtree V\ncan be viewed as a segment tree (Cormen et al., 2022).\nAlgorithm 3, which is adapted from the segment tree\nquerying algorithm, computes a cover Ca:b ⊆GA for\neach contiguous segment Xa:b. For each w∈W, Xw =\nXa:b for some 1 ≤a ≤b ≤n and we set Cw = Ca:b =\nSegmentCover(V, Xa:b).\nProposition 4.4. Cw is a valid vtree labelling with\nrespect to GA .\nIn addition to the fact that Cw is a valid labelling,\nby the runtime analysis of the original segment tree\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nAlgorithm 3 Compute Cover for Segment Xa:b\nprocedure SegmentCover(v, Xa:b)\nif Xa:b = ∅then\nreturn ∅\nend if\nif Xa:b = Xv then\nreturn {Zv}\nend if\nl, r ←Children(v)\nL ←SegmentCover(l, Xl ∩Xa:b)\nR ←SegmentCover(r, Xr ∩Xa:b)\nreturn L ∪R\nend procedure\nquerying algorithm, we know that the number of nodes\nvisited at each level of V is at most 4 and it follows that\n|Cw| ≤4d; hence the cardinality of Cw is bounded by\n12d. Then following a similar analysis, we have the\nfollowing results for multiplying two contiguous PCs.\nTheorem 4.5. Let A and B be contiguous structured\nPCs. Let d be the depth of the vtree for A , then A\nand B can be multiplied in time O(|A |12d|B|).\nCorollary 4.6. If A is of depth O(log n) then A and\nB can be multiplied in quasi-polynomial time.\nRemark 4.7. In this work, we assumed product nodes\nalways have two children and binary vtrees.\nHence\nthe depths of PCs are lower-bounded by Ω(log n) un-\nder such assumptions. However, if we allow product\nnodes to have arbitrarily many children, we can have\nPCs of smaller or even constant depths (Raz and Yehu-\ndayoff, 2009) and we hypothesize that Theorem 3.7\ncan be adapted to such generalized cases thus giving a\npolynomial-time algorithm for multiplying contiguous\nstructured circuits of constant depths.\nRemark 4.8. Thus far, we have assumed that both A\nand B are structured PCs. We claim that we can fur-\nther generalize our results by removing the constraint\nthat B has to be structured, and Theorems 4.3 and 4.5\nwould still hold. We illustrate the idea by showing how\nto multiply a contiguous structured PC A respecting a\nlinear vtree and an arbitrary contiguous PC B. Since\nB is not structured decomposable, we cannot restruc-\nture A to the vtree of B. However, we can use the\nsame idea as Algorithm 1 to restructure A “on-the-\nfly” as we multiply it with B in a bottom-up way.\nSpecifically, for each possible scope Xa:b that appears\nin B, we recursively construct circuit representations\nfor the functions pq(Xa:b) · pA (Xa:b | Za =i, Zb =j) for\ni, j and q∈B with scope Xa:b. The recurrence relation\nis similar to that of Algorithm 1 and we refer readers\nto the Appendix for details.\nAs an explicit application of circuit multiplication, let\nus consider constrained text generation (Zhang et al.,\n2024), in which linear PCs (HMMs) are multiplied\nwith deterministic finite automata (DFAs) represent-\ning the logical constraint. Our results imply that one\ncan also multiply a HMM with a contiguous logical cir-\ncuit such as a sentential decision diagram (SDD) (Dar-\nwiche, 2011), which have been shown to be exponen-\ntially more expressive efficient (Bova, 2016).\n5\nPC DEPTH REDUCTION\nAlgorithm 4 Depth Reduction Vtree\n1: procedure BalancedVtree(V, Sl = ∅, Sr = ∅)\n2:\nif |V | = 1 then\n3:\nreturn leaf(V ; Sl ∪Sr)\n4:\nend if\n5:\nv ←root(V )\n6:\nl, r ←Children(v)\n7:\nwhile |Vr| > 2\n3|V | do\n8:\nv ←r\n9:\nl, r ←Children(v)\n▷assume |Vl|≤|Vr|\n10:\nend while\n11:\nV ′\nl ←BalancedVtree(V[v7→l], Sl, {Zv})\n12:\nV ′\nr ←BalancedVtree(Vr, {Zv}, Sr)\n13:\nreturn join(V ′\nl , V ′\nr; Sl ∪Sr)\n14: end procedure\nDepth reduction of a probabilistic circuit refers to\nthe construction of an equivalent circuit with reduced\ndepth, e.g. to a depth logarithmic in the number of\nvariables. A depth reduction algorithm for general cir-\ncuits is known (Valiant et al., 1983; Raz and Yehuday-\noff, 2008; Yin and Zhao, 2024) but does not take ad-\nvantage of structuredness. We show how to reduce a\nstructured-decomposable circuit to an equivalent log-\ndepth circuit by restructuring. The key step is to iden-\ntify a log-depth vtree such that restructuring to that\nvtree using Algorithm 1 (and some valid choice of la-\nbels) results in at most a polynomial increase in size.\nAlgorithm 4 constructs a log-depth vtree labelling of\nconstant cardinality. Intuitively, each step of the algo-\nrithm breaks a vtree down into two connected compo-\nnents, which are then depth-reduced recursively. One\nselects a single vtree node by traversing the vtree top-\ndown, until the split would be balanced in the sense\nthat the two connected components have size between\n1\n3 and 2\n3 of the input vtree (Lines 7-10). The algorithm\nsimulataneously constructs a valid label for the vtree\nnode. The join routine then returns a labelled vtree\nthat consists of a single root node with the aforemen-\ntioned label, connected to the depth-reduced vtrees for\nthe components. Note that the algorithm produces ex-\nactly one vtree node for each vtree node in the original\nvtree; we can thus write v(w) for the node in V cor-\nresponding to w. Then we have the following result:\n\nRestructuring Tractable Probabilistic Circuits\nTheorem 5.1 (Depth Reduction Vtree). Given any\nvtree V , Algorithm 4 returns a vtree W of depth\nO(log |V |) with a valid labelling of cardinality 3.\nProof. The depth reduction to O(log |V |) is achieved\nas the algorithm increases the depth by one in each\nrecursive call, but reduces the vtree size by a multi-\nplicative factor. The validity condition holds due to\nthe separation into connected components (the labels\ncan also be obtained from Algorithm 2). The value of\nM ′ follows by noting that Sl and Sr are either empty\nor singleton sets, and that the algorithm produces\nCw = Sl∪Sr, Cl = Sl∪{Zv(w)}, and Cr = {Zv(w)}∪Sl\nwhere Zv(w) for each inner node w ∈W.\nRemark 5.2. Firstly, the depth-reduced PC retains\nstructuredness, which is not guaranteed by the exist-\ning depth-reduction algorithms. Secondly, exploiting\nstructuredness and tracking the hidden state size en-\nables a more fine-grained analysis of the size of the\ndepth-reduced circuit. Since the size of the original\ncircuit is O(nh2), using the known cubic bound on the\nsize of the depth-reduced circuit (Raz and Yehudayoff,\n2008) gives O(n3h6). However, by Theorem 5.1, we see\nthat M ′ = maxw∈W |Cl, Cr, CW | ≤3 and so by Theo-\nrem 3.7 we immediately obtain a much tighter bound\nof O(nh3) for the size of the resulting circuit.\nCorollary 5.3. Any structured PC over n variables\nand with hidden state size h can be restructured to a\nstructured PC of depth O(log n) and size O(nh3) that\nrepresents the same distribution.\nWhile this result is of independent theoretical inter-\nest, the sub-quadratic complexity of O(nh3) also opens\nup practical applications of depth-reduction.\nAl-\nmost all PC inference and learning algorithms involve\nforward/backward passes through the computation\ngraph, where computation is only parallelized across\nnodes of the same depth such that O(depth of PC)\nsequential computations are required.\nThis is prob-\nlematic when the number of variables n is large, as is\noften the case in applications such as computational\nbiology (Dang et al., 2022b). In such cases, depth re-\nduction can be a practical strategy where the improved\nparallelism outweighs the increased circuit size.\n6\nRELATED WORK\nProbabilistic circuits have emerged as a unifying repre-\nsentation of tractable probabilistic models (Choi et al.,\n2020; Sidheekh and Natarajan, 2024), such as sum-\nproduct networks (Poon and Domingos, 2011), cutset\nnetworks (Rahman et al., 2014), probabilistic senten-\ntial decision diagrams (Kisa et al., 2014) and proba-\nbilistic generating circuits (Zhang et al., 2021; Harvi-\nainen et al., 2023; Agarwal and Bl¨aser, 2024; Broad-\nrick et al., 2024). Significant effort has been devoted to\nlearning PC structures to fit data (Liang et al., 2017;\nDang et al., 2020; Yang et al., 2023), but the impli-\ncations for the structure-dependent queries have been\nless studied. We bridge this gap by providing a general\nrestructuring algorithm with specific cases of (quasi-\n)polynomial complexity.\nAs tractable representations of distributions, PCs have\nbeen employed extensively as a compilation target\nfor inference in graphical models (Darwiche, 2003;\nChavira and Darwiche, 2008; Rooshenas and Lowd,\n2014). Hidden tree-structured Bayesian networks have\nalso been used as a starting point for the learning\nof a probabilistic circuit (Dang et al., 2020; Liu and\nVan den Broeck, 2021; Dang et al., 2022a). A particu-\nlarly useful analysis technique for learning probabilis-\ntic circuits has been to interpret them as latent vari-\nable models (Peharz et al., 2016). Decomposable and\nsmooth PCs can be interpreted as Bayesian networks\nby introducing a latent variable for each sum node in\nthe PC (Zhao et al., 2015). Our conversion from struc-\ntured PC to Bayesian network is most closely related\nto the decompilation methods of Butz et al. (2020);\nPapantonis and Belle (2023), but we do not assume\nthe PC has been compiled from a Bayesian network.\nThe seminal work of Valiant et al. (1983) showed that\nany poly-size arithmetic circuit can be transformed\ninto an equivalent circuit of polylogarithmic depth.\nRaz and Yehudayoff (2008) show that this procedure\nmaintains syntactic multilinearity (decomposability).\nRecently, Yin and Zhao (2024) showed a quasipoly-\nnomial upper bound on converting decomposable and\nsmooth PCs to tree-shaped PCs via a depth-reduction\nprocedure. Our application of restructuring focuses on\nstructured-decomposable circuits and shows a tighter\nbound based on a graphical model interpretation.\n7\nCONCLUSION\nWe introduce the problem of restructuring probabilis-\ntic circuits, and develop a general algorithm for re-\nstructuring a structured-decomposable circuit to any\ntarget vtree structure. Our method exploits an inter-\npretation of structured-decomposable circuits as latent\ntree Bayesian networks, which enables recursive con-\nstruction of a circuit respecting the target vtree using\nprobabilistic semantics of the Bayesian network. As\nconcrete applications of restructuring, we show how to\ntractably multiply two circuits which do not necessar-\nily share the same structure but satisfy a contiguity\nproperty, and show how to restructure a circuit to log-\ndepth with a sub-quadratic increase in size.\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nAcknowledgements\nThis work was funded in part by the DARPA ANSR\nprogram under award FA8750-23-2-0004, the DARPA\nPTG Program under award HR00112220005, and NSF\ngrant #IIS-1943641. This work was done in part while\nBW, MA and GVdB were visiting the Simons Institute\nfor the Theory of Computing.\nReferences\nAgarwal, S. and Bl¨aser, M. (2024).\nProbabilis-\ntic generating circuits–demystified. arXiv preprint\narXiv:2404.02912.\nAhmed, K., Teso, S., Chang, K.-W., Van den Broeck,\nG., and Vergari, A. (2022).\nSemantic probabilis-\ntic layers for neuro-symbolic learning. Advances in\nNeural Information Processing Systems, 35:29944–\n29959.\nAmarilli, A., Arenas, M., Choi, Y., Monet, M., Broeck,\nG. V. d., and Wang, B. (2024). A circus of circuits:\nConnections between decision diagrams, circuits,\nand automata. arXiv preprint arXiv:2404.09674.\nBova, S. (2016). Sdds are exponentially more succinct\nthan obdds. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 30.\nBroadrick, O., Zhang, H., and Van den Broeck, G.\n(2024).\nPolynomial semantics of tractable proba-\nbilistic circuits. In Proceedings of the 40th Confer-\nence on Uncertainty in Artificial Intelligence (UAI).\nBusch, F. P., Willig, M., Seng, J., Kersting, K., and\nDhami, D. S. (2024). Ψnet: Efficient causal model-\ning at scale. In International Conference on Proba-\nbilistic Graphical Models, pages 452–469. PMLR.\nButz, C., Oliveira, J. S., and Peharz, R. (2020). Sum-\nproduct network decompilation.\nIn International\nConference on Probabilistic Graphical Models, pages\n53–64. PMLR.\nChavira, M. and Darwiche, A. (2008). On probabilis-\ntic inference by weighted model counting. Artificial\nIntelligence, 172(6-7):772–799.\nChoi, A., Van den Broeck, G., and Darwiche, A.\n(2015). Tractable learning for structured probability\nspaces: A case study in learning preference distri-\nbutions. In Proceedings of 24th International Joint\nConference on Artificial Intelligence (IJCAI).\nChoi, Y., Dang, M., and Van den Broeck, G. (2021).\nGroup fairness by probabilistic modeling with la-\ntent fair decisions. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 35, pages\n12051–12059.\nChoi, Y., Vergari, A., and Van den Broeck, G. (2020).\nProbabilistic circuits:\nA unifying framework for\ntractable probabilistic models.\nConaty, D., Maua, D. D., and de Campos, C. P. (2017).\nApproximation complexity of maximum a posteri-\nori inference in sum-product networks. In The 33rd\nConference on Uncertainty in Artificial Intelligence\n(UAI). AUAI.\nCormen, T. H., Leiserson, C. E., Rivest, R. L., and\nStein, C. (2022). Introduction to algorithms. MIT\npress.\nDang, M., Khosravi, P., Liang, Y., Vergari, A., and\nVan den Broeck, G. (2021). Juice: A julia package\nfor logic and probabilistic circuits. In Proceedings of\nthe 35th AAAI Conference on Artificial Intelligence\n(Demo Track).\nDang, M., Liu, A., and Van den Broeck, G. (2022a).\nSparse probabilistic circuits via pruning and grow-\ning. In Advances in Neural Information Processing\nSystems 35 (NeurIPS).\nDang, M., Liu, A., Wei, X., Sankararaman, S., and\nVan den Broeck, G. (2022b).\nTractable and ex-\npressive generative models of genetic variation data.\nIn Proceedings of the International Conference on\nResearch in Computational Molecular Biology (RE-\nCOMB).\nDang, M., Vergari, A., and Broeck, G. (2020). Strudel:\nLearning structured-decomposable probabilistic cir-\ncuits. In International Conference on Probabilistic\nGraphical Models, pages 137–148. PMLR.\nDarwiche, A. (2003).\nA differential approach to in-\nference in bayesian networks. Journal of the ACM\n(JACM), 50(3):280–305.\nDarwiche, A. (2011). Sdd: A new canonical represen-\ntation of propositional knowledge bases. In Twenty-\nSecond International Joint Conference on Artificial\nIntelligence.\nDarwiche, A. and Marquis, P. (2002). A knowledge\ncompilation map. Journal of Artificial Intelligence\nResearch, 17:229–264.\nde Colnet, A. and Mengel, S. (2021). A compilation of\nsuccinctness results for arithmetic circuits. In Pro-\nceedings of the International Conference on Prin-\nciples of Knowledge Representation and Reasoning,\nvolume 18, pages 205–215.\nGeh, R. L., Zhang, H., Ahmed, K., Wang, B., and\nVan den Broeck, G. (2024). Where is the signal in\ntokenization space? In Proceedings of the 2024 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nGeiger, D., Verma, T., and Pearl, J. (1990).\nd-\nseparation:\nFrom theorems to algorithms.\nIn\nMachine intelligence and pattern recognition, vol-\nume 10, pages 139–148. Elsevier.\n\nRestructuring Tractable Probabilistic Circuits\nHarviainen, J., Ramaswamy, V. P., and Koivisto, M.\n(2023). On inference and learning with probabilis-\ntic generating circuits. In Uncertainty in Artificial\nIntelligence, pages 829–838. PMLR.\nKhosravi, P., Choi, Y., Liang, Y., Vergari, A., and\nVan den Broeck, G. (2019). On tractable computa-\ntion of expected predictions. In Advances in Neural\nInformation Processing Systems 32 (NeurIPS).\nKisa, D., Van den Broeck, G., Choi, A., and Dar-\nwiche, A. (2014). Probabilistic sentential decision\ndiagrams. In Fourteenth International Conference\non the Principles of Knowledge Representation and\nReasoning.\nLiang, Y., Bekker, J., and Van den Broeck, G. (2017).\nLearning the structure of probabilistic sentential de-\ncision diagrams. In Proceedings of the 33rd Confer-\nence on Uncertainty in Artificial Intelligence (UAI).\nLiu, A., Ahmed, K., and Van den Broeck, G. (2024a).\nScaling tractable probabilistic circuits: A systems\nperspective. In Proceedings of the 41th International\nConference on Machine Learning (ICML).\nLiu, A., Niepert, M., and Van den Broeck, G. (2024b).\nImage inpainting via tractable steering of diffusion\nmodels. In Proceedings of the Twelfth International\nConference on Learning Representations (ICLR).\nLiu, A. and Van den Broeck, G. (2021).\nTractable\nregularization of probabilistic circuits.\nIn Ad-\nvances in Neural Information Processing Systems 34\n(NeurIPS).\nLoconte, L., Di Mauro, N., Peharz, R., and Vergari, A.\n(2023). How to turn your knowledge graph embed-\ndings into generative models. Advances in Neural\nInformation Processing Systems, 36.\nLoconte, L., Mari, A., Gala, G., Peharz, R., de Cam-\npos, C., Quaeghebeur, E., Vessio, G., and Vergari,\nA. (2024a). What is the relationship between tensor\nfactorizations and circuits (and how can we exploit\nit)? arXiv preprint arXiv:2409.07953.\nLoconte, L., Mengel, S., and Vergari, A. (2024b).\nSum\nof\nsquares\ncircuits.\narXiv\npreprint\narXiv:2408.11778.\nLoconte, L., Sladek, A., Mengel, S., Trapp, M., Solin,\nA., Gillis, N., and Vergari, A. (2024c). Subtractive\nmixture models via squaring: Representation and\nlearning. In International Conference on Learning\nRepresentations (ICLR).\nPapantonis, I. and Belle, V. (2023). Transparency in\nsum-product network decompilation. In European\nConference on Artificial Intelligence, pages 1827–\n1834. IOS Press.\nPeharz, R., Gens, R., Pernkopf, F., and Domingos,\nP. (2016). On the latent variable interpretation in\nsum-product networks. IEEE transactions on pat-\ntern analysis and machine intelligence, 39(10):2030–\n2044.\nPeharz, R., Lang, S., Vergari, A., Stelzner, K., Molina,\nA., Trapp, M., Van den Broeck, G., Kersting, K.,\nand Ghahramani, Z. (2020). Einsum networks: Fast\nand scalable learning of tractable probabilistic cir-\ncuits. In Proceedings of the 37th International Con-\nference on Machine Learning (ICML).\nPipatsrisawat, K. and Darwiche, A. (2008). New com-\npilation languages based on structured decompos-\nability. In AAAI, volume 8, pages 517–522.\nPoon, H. and Domingos, P. (2011). Sum-product net-\nworks: A new deep architecture. In 2011 IEEE In-\nternational Conference on Computer Vision Work-\nshops (ICCV Workshops), pages 689–690. IEEE.\nRabiner, L. R. (1989). A tutorial on hidden markov\nmodels and selected applications in speech recogni-\ntion. Proceedings of the IEEE, 77(2):257–286.\nRahman, T., Kothalkar, P., and Gogate, V. (2014).\nCutset networks: A simple, tractable, and scalable\napproach for improving the accuracy of chow-liu\ntrees.\nIn Machine Learning and Knowledge Dis-\ncovery in Databases: European Conference, ECML\nPKDD 2014, Nancy, France, September 15-19,\n2014. Proceedings,\nPart II 14,\npages 630–645.\nSpringer.\nRaz, R. and Yehudayoff, A. (2008).\nBalancing syn-\ntactically multilinear arithmetic circuits. Computa-\ntional Complexity, 17:515–535.\nRaz, R. and Yehudayoff, A. (2009). Lower bounds and\nseparations for constant depth multilinear circuits.\nComputational Complexity, 18:171–207.\nRooshenas, A. and Lowd, D. (2014). Learning sum-\nproduct networks with direct and indirect variable\ninteractions.\nIn International Conference on Ma-\nchine Learning, pages 710–718. PMLR.\nRoth, D. (1996). On the hardness of approximate rea-\nsoning. Artificial Intelligence, 82(1-2):273–302.\nShen, Y., Choi, A., and Darwiche, A. (2016). Tractable\noperations for arithmetic circuits of probabilistic\nmodels. Advances in Neural Information Process-\ning Systems, 29.\nShih, A. and Ermon, S. (2020). Probabilistic circuits\nfor variational inference in discrete graphical mod-\nels. Advances in neural information processing sys-\ntems, 33:4635–4646.\nSidheekh, S. and Natarajan, S. (2024). Building ex-\npressive and tractable probabilistic generative mod-\nels: A review. arXiv preprint arXiv:2402.00759.\nTian, J., Paz, A., and Pearl, J. (1998). Finding mini-\nmal d-separators. Citeseer.\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nValiant, L., Skyum, S., Berkowitz, S., and Rackoff, C.\n(1983).\nFast parallel computation of polynomials\nusing few processors. SIAM Journal on Computing,\n12(4):641–644.\nVergari, A., Choi, Y., Liu, A., Teso, S., and Van den\nBroeck,\nG. (2021).\nA compositional atlas of\ntractable circuit operations for probabilistic infer-\nence. In Advances in Neural Information Processing\nSystems 34 (NeurIPS).\nWang, B. and Kwiatkowska, M. (2023). Compositional\nprobabilistic and causal inference using tractable\ncircuit models. In International Conference on Ar-\ntificial Intelligence and Statistics, pages 9488–9498.\nPMLR.\nWang, B. and Van den Broeck, G. (2024).\nOn the\nrelationship between monotone and squared proba-\nbilistic circuits. In Proceedings of the UAI Workshop\non Tractable Probabilistic Modeling (TPM).\nYang, Y., Gala, G., and Peharz, R. (2023). Bayesian\nstructure scores for probabilistic circuits. In Inter-\nnational Conference on Artificial Intelligence and\nStatistics, pages 563–575. PMLR.\nYin, L. and Zhao, H. (2024).\nOn the expressive\npower of tree-structured probabilistic circuits.\nIn\nAdvances in Neural Information Processing Systems\n37 (NeurIPS).\nZeˇcevi´c, M., Dhami, D., Karanam, A., Natarajan,\nS., and Kersting, K. (2021).\nInterventional sum-\nproduct networks: Causal inference with tractable\nprobabilistic models. Advances in neural informa-\ntion processing systems, 34:15019–15031.\nZhang, H., Dang, M., Peng, N., and Van den Broeck,\nG. (2023). Tractable control for autoregressive lan-\nguage generation. In Proceedings of the 40th Inter-\nnational Conference on Machine Learning (ICML).\nZhang, H., Juba, B., and Van den Broeck, G. (2021).\nProbabilistic generating circuits.\nIn Proceedings\nof the 38th International Conference on Machine\nLearning (ICML).\nZhang, H., Kung, P.-N., Yoshida, M., Broeck, G.\nV. d., and Peng, N. (2024).\nAdaptable logical\ncontrol for large language models.\narXiv preprint\narXiv:2406.13892.\nZhao, H., Melibari, M., and Poupart, P. (2015). On\nthe relationship between sum-product networks and\nbayesian networks. In International Conference on\nMachine Learning, pages 116–124. PMLR.\nZhao, H., Poupart, P., and Gordon, G. J. (2016). A\nunified approach for learning the parameters of sum-\nproduct networks. Advances in neural information\nprocessing systems, 29.\n\nRestructuring Tractable Probabilistic Circuits\nSupplementary Materials\nA\nADDITIONAL PROOFS\nProposition 3.2. pA (X) = P\nz pAaug(X, z)\nProof. Suppose that A is a structured decomposable and smooth PC respecting vtree V . Write prod(v) and\nsum(v) for the set of product and sum nodes with scope Xv. The augmented PC Aaug is decomposable as if leaves\nwith scope {Zv} were contained in (the subcircuits rooted at) two different children t1, t2 of a product node,\nthen their parents (nodes in prod(v)) would be contained in t1, t2, which is a contradiction of decomposability of\nA . It is also smooth as for any sum node, if one sum node contains some leaf with scope {Zv}, then it contains\nsome node in sum(v), hence by smoothness of A all sum nodes contain some node in sum(v) and thus some leaf\nwith scope {Zv}.\nConsider the standard marginalization algorithm for PCs (Darwiche, 2003; Choi et al., 2020), where one replaces\neach leaf whose scope is contained within the variables being marginalized out with the constant 1. This correctly\nmarginalizes the function represented by the PC if the PC is decomposable and smooth. If we marginalize over\nall newly introduced latents Z, it is immediate that the resulting PC represents the same function as A .\nTheorem 3.3. Let A be a structured-decomposable and smooth PC over variables X respecting vtree V . Then\nthere exists a Bayesian network GA over variables X and Z = {Zv|v ∈V } with graph Vv7→Zv such that\nP\nz pG(X, z) = pA (X).\nProof. In Section 3.1 we described a Bayesian network pG = p∗with the required graph. It remains to show\nthat this network represents the same distribution as A . We will do this by showing that the Bayesian network\nhas the same distribution as the augmented PC, i.e. pG(X, Z) = pAaug(X, Z).\nThe key observation is to consider the induced trees of the augmented PC (Zhao et al., 2016):\nDefinition A.1 (Induced Trees). Given a decomposable and smooth circuit A , let T be a subgraph of A . We\nsay that T is an induced tree of A if (1) root(A ) ∈T; (2) If t ∈T is a sum node, then exactly one child\nof t (and the corresponding edge) is in T; and (3) If t ∈T is a product node, then all children of t (and the\ncorresponding edges) are in T.\nIt is easy to see that an induced tree T is indeed a tree, as otherwise decomposability would be violated. Let T\nbe the set of all induced trees of Aaug. Each induced tree defines a function:\npAaug,T (X, Z) :=\nY\n(ti,tj)∈sumedges(T )\nwti,tj\nY\nt∈leavesX(T )\nft(Xsc(t))\nY\nt∈leavesZ(T )\nft(Zsc(t))\n(1)\nwhere sumedges(T) denotes the set of outgoing edges from sum nodes in T, and leavesX(T), leavesZ(T)\ndenote the set of leaf nodes in T with scope corresponding to a variable in X, Z respectively. The distribution\nof the augmented PC is then in fact given by the sum of these functions over all induced trees:\nProposition A.2 (Zhao et al. (2016)). pAaug(X, Z) = P\nT ∈T pAaug,T (X, Z).\nNow, let path(v, i, j) be a predicate indicating whether there is a path between tp,j and tv,i (where we use p\nto denote the parent vtree node of v, and as before e.g. tv,i indicates the product node with scope Xv and\ncorresponding to Zv = i). We will consider two cases depending on the value of the latents. Specifically, we\nwill say that an assignment z is consistent if path(v, zv, zp) holds for all non-root inner nodes in the vtree, and\ninconsistent otherwise.\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nIf an assignment z is inconsistent, then for any assignment x of the observed variables, we have that pG(x, z) = 0\nby definition of the Bayesian network distribution. Now consider any induced subtree T ∈T . Each T must\ncontain one product node for every variable scope.\nIn particular, T must contain some product node tv,i\nsuch that zv ̸= i (otherwise, since z is inconsistent, a (connected) tree would be impossible). We then have\npAaug,T (x, z) = 0 for all x, as Equation 1 then contains a leaf function ft(zv) = 1zv=i = 0. Thus pAaug(x, z) =\nP\nT ∈T pAaug,T (x, z) = 0 for any x.\nIf an assignment z is consistent, note that, by our assumption of alternating sums and products, there can be\nexactly one path from tp,j to tv,j, as tp,j has a unique sum node child with scope containing Xv, and this sum\nnode must immediately have tv,j as a child. Thus there is exactly one induced tree T containing tv,zv for all\n(non-root) inner vtree nodes v. Further, examining the definition of the Bayesian network distribution pG(X, z),\nthis exactly matches the definition of pAaug,T (X, z): each sum node edge weight in the tree corresponds to a\nsum node edge weight along a path from some tp,zp to tv,zv and thus the CPT of Zv given Zp (the root sum\nnode edge weight corresponds to the CPT for Zroot(V )), and each leaf node distribution for observed variables\ncorresponds to the CPT for that variable given its parent.\nThus we have shown that pAaug(X, Z) = pG(X, Z), as required.\nProposition 4.4. Cw is a valid vtree labelling with respect to GA .\nProof. We first show that Cw satisfies the following properties:\n1. Xa:b = S\nZi∈Ca:b Leaves(Zi) is a disjoint union.\n2. If a≤c≤d≤b, then for Z ∈Cc:d, there exists Z′ ∈Ca:b such that Z′ is an ancestor of Z in GA .\nProperty 1 follows from the proof of correctness of the segment tree querying algorithm. Property 2 follows from\nProperty 1 together with the key observation that we can compute Cc:d via S\nZi∈Ca:b SegmentCover(Zi, Xc:d ∩\nLeaves(Zi)). Let w = a : b be a node in W with children l = a : c and r = c + 1 : b; it follows from Property 1\nthat Ca:b covers Xa:b and Ca:c blocks all paths from Xa:c to Cc+1:b; it follows from Property 2 that Ca:c blocks\nall paths from Xa:c to Xa:b. Hence we conclude that Cw is a valid labelling. A minor catch is that Cw may\ncontain variables in X, but we can replace them by their parent in GA without affecting the validity of Cw.\nCorollary 5.3. Any structured PC over n variables and with hidden state size h can be restructured to a\nstructured PC of depth O(log n) and size O(nh3) that represents the same distribution.\nProof. By Theorem 5.1, given a structured PC X over n variables with hidden state size h and respecting vtree\nV , we can generate a vtree W of depth O(log n) and with labelling cardinality M ′ = 3. Thus, by Theorem 3.7\nwe can construct a PC representing the same function and respecting vtree W of size O(nh3). The depth of the\nPC is then also O(log n) as we have assumed alternating sum and product nodes, so the depth of the circuit is\nat most double that of the vtree.\nB\nComputing Minimum D-separators\nLet G be a tree-shaped Bayesian network rooted at Z; in particular, assume that the leaves of G ⊆X∪Z and\nthe internal nodes of G ⊆Z (e.g. Figure 1b). Then, we want to prove that Algorithm 5 computes a minimum\nd-separator C ⊆G for A, B ⊆X.\nAs shown in Algorithm 5, given tree-shaped Bayesian network G rooted at Z, the procedure MinimumSepartor\ncomputes three sets of latent variables CA, CB and C. Specifically, we shall prove the following properties:\n1. CA is a minimum d-separator between A and B that also blocks all paths from A to Z.\n2. CB is a minimum d-separator between A and B that also blocks all paths from B to Z.\n3. Either CA or CB is a minimum d-separator between A and B in G (rooted at Z); hence C is a minimum\nd-separator between A and B in G.\nProof of Property 3. It suffices to show that P := {d-separators between A and B} and Q := {d-separators\nbetween A and B that blocks all paths from A to Z} ∪{d-separators between A and B that blocks all paths\n\nRestructuring Tractable Probabilistic Circuits\nAlgorithm 5 Computing minimum d-separators for tree-shaped Bayesian network G rooted at Z\nprocedure MinimumSeparator(Z, A, B)\nif A=∅and B=∅then\nreturn ∅, ∅, ∅\nend if\nif B=∅then\nreturn {Z}, ∅, ∅\nend if\nif A=∅then\nreturn ∅, {Z}, ∅\nend if\nfor Zi ∈Children(Z) do\nCi,A, Ci,B, Ci ←MinimumSeparator(\nZi, A∩Leaves(Zi), B∩Leaves(Zi))\nend for\nCA ←Min(S\niCi ∪{Z}, S\niCi,A)\nCB ←Min(S\niCi ∪{Z}, S\niCi,B)\nC ←Min(CA, CB)\nreturn CA, CB, C\nend procedure\nfrom B to Z} are the same set. It is obvious that Q ⊆P and let’s prove that P ⊆Q. Let C be a d-separator\nbetween A and B in G, then C either blocks all path from A to Z or blocks all path from B to Z; suppose not,\nthen there is a path connecting A and B through Z; contradiction.\nProof of Property 1 and Property 2. We prove Property 1 (and Property 2) by a bottom-up induction on G.\nFirst of all it is easy to verify that the three base cases, i.e. A = ∅and B = ∅, A = ∅and B = ∅, are correct.\nWe now prove Property 1 (the proof for Property 2 is symmetric) by induction; first of all it is clear that both\nS\niCi ∪{Z} and S\niCi,A form d-separators between A and B, and it remains to show that the minimum of\nthese two is a minimum d-separator between A and B that blocks all paths from A to Z. Suppose, towards a\ncontradiction, let CA\n′ be such a d-separator of size < Min(|S\niCi ∪{Z}|, |S\niCi,A|). There are two cases:\n• If CA\n′ contains Z: let Gi be the subtree rooted at Zi and set Ci\n′ = CA\n′ ∩Gi. It is immediate that\nCi\n′ is a d-separator between A and B in Gi and by assumption, there exists at least one one i such that\n|Ci\n′| < |Ci|; contradicting the induction hypothesis.\n• If CA\n′ does not contain Z: let Gi be the subtree rooted at Zi and set Ci,A\n′ = CA\n′ ∩Gi. Similarly, it\nis not hard to see that Ci,A\n′ is a d-separator between A and B in Gi that blocks all paths from A ∩Gi\nto Zi. By assumption, there exists at least one i such that |Ci,A\n′| < |Ci,A|; contradicting the induction\nhypothesis.\nC\nMULTIPLICATION WITH NON-STRUCTURED CIRCUITS\nGiven a contiguous structured PC A respecting a linear vtree and an arbitrary contiguous PC B, which is not\nnecessarily structured, we show a recursive algorithm that multiplies A and B in polynomial time. Specifically,\nfor each possible scope Xa:b that appears in B, we recursively construct circuit representations for the functions\npq(Xa:b) · pA (Xa:b | Za =i, Zb =j) for ⊕nodes q ∈B with scope Xa:b and i, j hidden states of A . In particular,\nfrom pA (Xa:b | Za =i, Zb =j) we drop Za = i if a = 1 and drop Zb+1 = j if b = n, thus pA (Xa:b | Za =i, Zb =j)\ncorresponds to pA (Xa:b | Ca:b) as defined in Case 1. of Section 4.\nThe recurrence relation is similar to that of Algorithm 1. In the following derivation, we use the notations: (1)\ndenote the children of ⊕node q by c∈Ch(q); (2) denote the children of ⊗node c by c1 and c2; (3) denote the\n\nHonghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck\nweight of the edge connecting q and c by wqc; (4) for each ⊗node c∈Ch(q), c splits Xq ={Xa, . . . , Xb} into two\ncontiguous segments, and denote them by Xc1 ={Xa, . . . , Xmc} and Xc2 = {Xmc+1, . . . , Xb} for some a≤mc ≤b.\npq(Xa:b) · pA (Xa:b+1 | Za =i, Zb+1 =j)\n=\nX\nc∈Ch(q)\npc(Xa:b) · wqc · pA (Xa:b+1 | Za =i, Zb+1 =j)\n=\nX\nc∈Ch(q)\npc1(Xa:mc) · pc2(Xmc+1:b+1) · wqc · pA (Xa:b+1 | Za =i, Zb+1 =j)\n=\nX\nc∈Ch(q)\npc1(Xa:mc) · pc2(Xmc+1:b+1) · wqc ·\nX\nk\npA (Xa:b+1, Zmc =k | Za =i, Zb+1 =j)\n=\nX\nc∈Ch(q)\nX\nk\nwqc · pA (Zmc+1 =k | Za =i, Zb+1 =j)\n· pc1(Xa:mc) · pA (Xa:mc | Zmc+1 =k, Za =i, Zb+1 =j) · pc2(Xmc+1:b) · pA (Xa:b | Zmc+1 =k, Za =i, Zb+1 =j)\n=\nX\nc∈Ch(q)\nX\nk\nwqc · pA (Zmc+1 =k | Za =i, Zb+1 =j)\n· pc1(Xa:mc) · pA (Xa:mc | Za =i, Zmc+1 =k) · pc2(Xmc+1:b) · pA (Xa:b | Zmc+1 =k, Zb+1 =j)\nNow let’s analyze the complexity of the constructed circuit, which we denote by C . C has O(mkh2) ⊕nodes\nin total, where m is the number of scopes in B, k := maxS a scope in B |{⊕∈B with scope S}|, and h is the\nhidden states size of A . Suppose that each ⊕node in B has at most r children, then each ⊕node in C has\nat most O(rh) children. Hence the size of C is bounded by O(mkh2 · rh) = O(mkr · h3). Note that O(mkr)\ncorresponds to O(|B|) and O(h3) is upper-bounded by O(h4), which is O(|A |2); hence the size of C is bounded\nby O(|A |2|B|), which is the same complexity as stated in Theorem 4.3. Hence, we can remove the assumption\nthat B has to be structured from Theorem 4.3.\nTheorem C.1. Let A and B be contiguous PCs with B not necessarily structured. If A is structured respecting\nthe linear vtree, then A and B can be multiplied in polynomial time and the size of the product PC is bounded\nby O(|A |2|B|).\nBy a similar recursive construction, we can also remove the assumption that B has to be structured from\nTheorem 4.5:\nTheorem C.2. Let A and B be contiguous PCs. If A is structured of depth d, then then we can construct a\nproduct circuit of A and B of size bounded by O(|A |12d|B|).",
    "pdf_filename": "Restructuring_Tractable_Probabilistic_Circuits.pdf"
}