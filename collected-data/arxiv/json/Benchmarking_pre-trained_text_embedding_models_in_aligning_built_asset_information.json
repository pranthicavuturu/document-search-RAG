{
    "title": "Benchmarking pre-trained text embedding models in aligning built asset information",
    "context": "Accurate mapping of the built asset information to established data classification systems and taxonomies is crucial for effective asset management, whether for compliance at project handover or ad-hoc data integration scenarios. Due to the complex nature of built asset data, which predominantly comprises technical text elements, this process remains largely manual and reliant on domain expert input. Recent breakthroughs in contextual text representation learning (text embedding), particularly through pre-trained large language models, offer promising approaches that can facilitate the automation of cross-mapping of the built asset data. However, no comprehensive evaluation has yet been conducted to assess these models’ ability to effectively represent the complex semantics specific to built asset technical terminology. This study presents a comparative benchmark of state-of-the-art text embedding models to evaluate their effectiveness in aligning built asset informa- tion with domain-specific technical concepts. Our proposed datasets are derived from two renowned built asset data classification dictionaries. The results of our benchmarking across six proposed datasets, covering three tasks of clustering, re- trieval, and reranking, highlight the need for future research on domain adaptation techniques. The benchmarking resources are published as an open-source library, which will be maintained and extended to support future evaluations in this field. 1 Asset management plays a pivotal role in ensuring optimal performance and extended life span of the built environment through a systematic process of monitoring and maintaining various facilities and equipment. The rapid advancement of digital technologies has led asset owners to increasingly demand enriched digital twins at project handover to support real-time operations and maintenance of the built assets [Love and Matthews, 2019]. Simultaneously, the growing awareness of the benefits of digitized asset management highlights the essential need for federated access to built asset data [Moretti et al., 2023]. This requires aligning extensive data sources and their underlying schema with established data models, classification systems, or taxonomies to facilitate data accessibility for diverse stakeholders and improve interoperability across various software environments. However, aligning built asset data with pre-defined classification systems poses significant challenges in practice. A key challenge stems from the multi-source and multi-disciplinary nature of built asset data, which leads to the use of diverse formats and terminologies across different projects and stakeholders. For example, the terminology that architects utilize to describe the specifications for a particular building component or system can vastly differ from those used by structural engineers or subcontractors. Moreover, the structures of domain-specific classifications used in different disciplines often vary ∗For correspondence, please contact: mehrzad.shahinmoghadam.1@ens.etsmtl.ca Preprint. Under review. arXiv:2411.12056v1  [cs.CL]  18 Nov 2024",
    "body": "Benchmarking pre-trained text embedding models in\naligning built asset information\nMehrzad Shahinmoghadam∗\nDepartment of construction engineering\nÉcole de technologie supérieure\nMontreal, H3C 1K3, Canada\nmehrzad.shahinmoghadam.1@ens.etsmtl.ca\nAli Motamedi\nDepartment of construction engineering\nÉcole de technologie supérieure\nMontreal, H3C 1K3, Canada\nali.motamedi@etsmtl.ca\nAbstract\nAccurate mapping of the built asset information to established data classification\nsystems and taxonomies is crucial for effective asset management, whether for\ncompliance at project handover or ad-hoc data integration scenarios. Due to the\ncomplex nature of built asset data, which predominantly comprises technical text\nelements, this process remains largely manual and reliant on domain expert input.\nRecent breakthroughs in contextual text representation learning (text embedding),\nparticularly through pre-trained large language models, offer promising approaches\nthat can facilitate the automation of cross-mapping of the built asset data. However,\nno comprehensive evaluation has yet been conducted to assess these models’ ability\nto effectively represent the complex semantics specific to built asset technical\nterminology. This study presents a comparative benchmark of state-of-the-art text\nembedding models to evaluate their effectiveness in aligning built asset informa-\ntion with domain-specific technical concepts. Our proposed datasets are derived\nfrom two renowned built asset data classification dictionaries. The results of our\nbenchmarking across six proposed datasets, covering three tasks of clustering, re-\ntrieval, and reranking, highlight the need for future research on domain adaptation\ntechniques. The benchmarking resources are published as an open-source library,\nwhich will be maintained and extended to support future evaluations in this field.\n1\nIntroduction\nAsset management plays a pivotal role in ensuring optimal performance and extended life span of\nthe built environment through a systematic process of monitoring and maintaining various facilities\nand equipment. The rapid advancement of digital technologies has led asset owners to increasingly\ndemand enriched digital twins at project handover to support real-time operations and maintenance\nof the built assets [Love and Matthews, 2019]. Simultaneously, the growing awareness of the benefits\nof digitized asset management highlights the essential need for federated access to built asset data\n[Moretti et al., 2023]. This requires aligning extensive data sources and their underlying schema\nwith established data models, classification systems, or taxonomies to facilitate data accessibility for\ndiverse stakeholders and improve interoperability across various software environments. However,\naligning built asset data with pre-defined classification systems poses significant challenges in practice.\nA key challenge stems from the multi-source and multi-disciplinary nature of built asset data, which\nleads to the use of diverse formats and terminologies across different projects and stakeholders. For\nexample, the terminology that architects utilize to describe the specifications for a particular building\ncomponent or system can vastly differ from those used by structural engineers or subcontractors.\nMoreover, the structures of domain-specific classifications used in different disciplines often vary\n∗For correspondence, please contact: mehrzad.shahinmoghadam.1@ens.etsmtl.ca\nPreprint. Under review.\narXiv:2411.12056v1  [cs.CL]  18 Nov 2024\n\nin granularity. For instance, the detailed engineering descriptions of an HVAC system provided by\nmechanical engineers may be far more comprehensive than those required and used by operations and\nmaintenance teams. Finally, variations in local regulations and standards can further complicate the\nalignment process, particularly for large-scale or international projects. These issues, combined with\nthe dynamic and evolving nature of built asset data throughout an asset’s lifecycle, lead to potential\ninconsistencies when integrating this data into a unified digital asset management environment.\nIn response, there have been several initiatives aimed at facilitating the digital delivery of built asset\ninformation while ensuring its conformity with predefined or standardized descriptions (data models,\ntaxonomies, etc.). One major initiative is buildingSMART Data Dictionary (bSDD)[buildingSmart\nInternational, 2024a], an international and ongoing effort whose main objective is to create shared def-\ninitions for describing the built environment. This is achieved through a collection of interconnected\ndata dictionaries that are both human-readable and machine-readable[buildingSmart International,\n2024a]. Although making various data dictionaries programmatically accessible will facilitate access\nto agreed and consistent terms, the complexity and dynamic diversity of the built asset terminology\nnecessitate robust data mapping strategies to accommodate various data descriptions and updates\n[Forth et al., 2024]. As a result, the asset information alignment process remains predominantly\nmanual, relying heavily on the expertise of domain specialists to accurately map complex technical\ndata [Roberts et al., 2018]. The significant challenges associated with the manual alignment process,\nincluding high costs, time consumption, and potential for human error, highlight the need for more\nautomated and reliable data mapping solutions.\nThe central thesis of our research builds upon the argument that recent advancements in natural\nlanguage processing/understanding research can significantly enhance automated data mapping\nprocesses. In particular, the rich and contextualized representation of textual inputs as numeric\nvectors, commonly known as text embedding [Pennington et al., 2014, Lee et al., 2024b], provides\nadvanced capabilities for machines to understand the semantics of the intricate terminologies. Earlier\nmethods such as word2vec [Mikolov et al., 2013] and GloVe [Pennington et al., 2014] relied on\nstatic embeddings, i.e., generating fixed representations of numerical vectors for each word based on\ntheir co-occurrence in large corpora. However, recent neural language models, dominantly built on\ntop of the transformer architecture [Vaswani et al., 2017], can generate dynamic, context-sensitive\nembeddings. The capability of recent embedding models in adapting the representation of words\n(or sub-word tokens) based on their surrounding context has motivated researchers and practitioners\nacross diverse fields to leverage the power of contextual text embeddings to drive advancements\nin their respective domains. From traditional databases integration [Cappuzzo et al., 2020], to\ninformation management in biomedicine [Zhang et al., 2019], or public figure perceptions in social\nscience studies [Cao and Kosinski, 2024], the increasing volume of encouraging reports on leveraging\ntext embedding models to deliver a more nuanced text understanding in various specialized domains\n[Rasmy et al., 2021, Ostendorff et al., 2021, Rouhizadeh et al., 2024, Wilkho et al., 2024, Cao and\nKosinski, 2024] reinforces the relevance of these models in automating data alignment in the domain\nof built asset information management.\nBased on the observation that built asset data predominantly exists in textual form [Wu et al., 2022],\nwe argue that state-of-the-art text embedding models present promising opportunities to refine the\nautomated alignment of built asset information. However, the extensive and increasing availability of\npre-trained language models has led to the proliferation of potential text embedding models, creating\nconfusion regarding model selection for different use cases [Muennighoff et al., 2022]. Moreover,\nrecent research indicates that general-purpose text embedding models often struggle to maintain\nconsistent performance across diverse tasks and domains [Lee et al., 2024b]. This is while most\nprevious studies utilizing pre-trained or fine-tuned language models in built environment research\nhave been significantly limited in scope, primarily focusing on ad-hoc downstream tasks with small\nevaluation datasets [Shahinmoghadam et al., 2024, Jung et al., 2024, Wang et al., 2024, Forth et al.,\n2024, Jeon et al., 2024]. Such limitations can result in a potentially skewed perspective on the overall\ndomain-specific text understanding of these models [Shahinmoghadam et al., 2024]. Additionally,\nscarce public access to the datasets used in previous works poses another important challenge to the\ntransparency and reproducibility of the reported results. This motivates us to examine the extent to\nwhich existing language models can be directly leveraged to deliver contextually accurate mappings\nof domain-specific terminology within the context of built asset information management. In this\nwork, we present a comprehensive benchmark of state-of-the-art text embedding models to evaluate\ntheir effectiveness in capturing and representing the semantics of textual descriptions related to\n2\n\nbuilt assets. Through this evaluation, we aim to identify the strengths and limitations of existing\nlanguage models in enhancing data alignment practices within the built asset domain. Our proposed\nbenchmark is aligned with the Massive Text Embedding Benchmark (MTEB) [Muennighoff et al.,\n2022], a benchmark recognized extensively in both academic and practical contexts for its robustness\nand utility. We benchmark 24 text embedding models on our developed datasets that amount to\na total of more than ten thousand data entries across six tasks, making our evaluations the most\ncomprehensive ones in this specialized field to date. By making our datasets and benchmark software\npublicly available, we encourage future research to build upon our work, contributing to continuous\nimprovements in this domain.\n2\nMethods\n2.1\nData sources\nGiven the built environment’s multidisciplinary nature, the datasets included in the benchmark\nmust encompass an expansive spectrum of sub-domain subjects, including architectural, structural,\nmechanical, and electrical systems. To ensure a diverse coverage of built products in our benchmark,\nwe carefully examined the selection of data sources used for creating task-specific datasets. A detailed\ndescription of the corpus development and data extraction processes is provided below.\nThe initial step in creating the benchmark’s task-specific datasets is the development of a consistent\ncorpus of built products. Based on the requirements of the tasks within our benchmark, the core\ncorpus needed to include the following key information for each product: name or title, description,\nand corresponding labels (group categories). The two primary sources used to develop the built\nproduct corpora are as follows:\nIndustry Foundation Classes (IFC). Published and maintained by buildingSMART International[bsi,\n2024], IFC is an open international data model offering comprehensive digital descriptions of various\naspects of building and infrastructure projects. Originally designed to facilitate interoperability\nand information exchange among different software applications and stakeholders, IFC provides a\ncomprehensive representation of various aspects of built asset entities. We utilize IFC version 4.3.2.0\n[buildingSmart International, 2024b], recently approved as an ISO standard (ISO 16739-1:2024).\nUniclass. Developed and maintained by the National Building Specification (NBS)[NBS, 2024a],\nUniclass is a unified classification system for the built environment. We utilize version 1.33 of the\nUniclass Pr Product Table[NBS, 2024b]. Uniclass has extensive coverage, encompassing over 8,000\nproduct types, making it one of the most recognized and widely adopted classification systems in the\nbuilt asset industry.\n2.2\nData extraction\nTo create a corpus of products with corresponding names, descriptions, and labels, we undertook the\nfollowing steps: For Uniclass, we utilize the publicly-available CSV format of the products table,\nwhich comprises over 8,000 products categorized into three hierarchical levels. Product names were\ndirectly extracted from the table, while product categories were inferred from the numeric codes\nassociated with hierarchical categories (see Figure 1). To automatically extract the corresponding\ntextual labels for each product, we developed a script to scrape the table programmatically. As the\noriginal table does not include product descriptions, we propose a method (detailed in the subsequent\nsubsection) to synthesize a description for each product. We retained only those products that have\nlabels for all three classification levels. After applying this filtering process, the Uniclass corpus\ncomprises 4,234 instances, which remains sufficiently large for our benchmarking purposes.\nRegarding the IFC schema, we parse the official schema content by utilizing resources from an\nopen-source Python library[ifc, 2024] that enables programmatic access to IFC entities. Initially,\nwe extracted entities of interest from a JSON-formatted file containing the complete list of IFC\nentities, their type enumeration, and their definition (derived from IFC’s official documentation).\nAn analysis of the \"IfcProduct\" class within the IFC schema indicated that a significant majority of\nproduct entities are classified under the \"IfcElement\" class. Therefore, we focused exclusively on the\n\"IfcElement\" subclasses. After removing IFC entities with missing descriptions (less than 1% of total\n\"IfcElement\" entities), we developed a script to extract each entity’s top three parent classes to serve\nas the product category labels. In addition to entity superclass groups, we use the domain-specific\n3\n\nFigure 1: Overview of the main steps in developing the built product corpus: (a) Example of extracting\ncategories and synthesizing entity descriptions from raw Uniclass entries; (b) Example of hierarchical\nrelation extraction for main entities and their enumerated types from the IFC schema; (c) Sample\nrecords from the developed corpus, containing product titles, descriptions, and categories with three\nlevels of granularity.\nschemas (e.g., structural, HVAC, building control) from IFC’s official documentations[buildingSmart\nInternational, 2024b] as an additional source for entity label assignment. The resulting IFC corpus\ncomprises 977 entities (total of parent entities and type enumerations).\n2.3\nData augmentation and curation\nThe process of generating textual descriptions for Uniclass entities is depicted in Figure 1(a). Initial\nentity descriptions are synthesized by sequentially concatenating the entity’s category titles, pro-\ngressing from the most specific to the most general. An example of the synthesized descriptions is\nprovided in Figure 1(a). These concatenated descriptions are then paraphrased using a generative\nlanguage model to create more nuanced and natural descriptions, relaxing the text from the rigid\ntemplate initially employed. We generated paraphrased descriptions using the most advanced version\nof the GPT-4 model available at the time of conducting the experiments (gpt-4-turbo-2024-04-09).\nAlthough the prompts used for generating paraphrased descriptions were designed to prevent the\nalteration or addition of facts, it was essential to manually review all generated descriptions due to\nknown potential inaccuracies of generative language models. The review is carried out by two domain\nexperts, each with over ten years of experience in the field. Each expert cross-checked the issues\nidentified by the other, and the final decisions were made based on mutual agreement. The following\ncuration steps are undertaken to ensure the accuracy and consistency of the extracted product names\nand descriptions. We preprocess native IFC entity names and convert them into a readable form\n(e.g., \"IfcHeatExchanger\" to Heat Exchanger; see examples in Figure 1(b) and (c)). For IFC class\nenumeration types, where the enumeration name alone might be ambiguous, we append the parent\nclass type in parentheses. For example, the enumeration WATER, a subclass of \"IfcBoilerTypeEnum\",\nis represented as \"WATER (Boiler Type)\" (see examples in Figure 1(c)). Following the same logic,\n4\n\nwe enrich the product descriptions by concatenating the product’s name at the beginning of the\ndescription for both Unicalss and IFC entities. This step reinforces contextual clarity, as the natural\nentity names carry significant semantic information. Finally, we manually review and modify the\nentity descriptions that contain inconsistent information, such as notes related to the schema version\nhistory or future depreciation notes.\n2.4\nSampling\nTo ensure a robust entity selection when creating task-specific datasets, we implemented the following\nsampling strategies: For positive sampling, we adopt a semantic diversity approach. Given a targetted\nsubset of built products, we generate text embeddings for all corresponding text inputs, i.e., product\nnames and descriptions. Embeddings are generated using a state-of-the-art text embedding model\n(\"mxbai-embed-large-v1\"[Li and Li, 2023]). From this set of embeddings, we randomly choose an\ninitial sample as a starting point. Subsequently, we iteratively select additional samples by identifying\nthose that exhibit the slightest similarity to the most recently selected sample, as determined by cosine\nsimilarity scores, i.e., the cosine of the angle between two embedding vectors. This process repeats\nuntil the desired number of samples is achieved. This method ensures that the samples selected\nfor a particular subset (e.g., products of the same category) yield diverse representations within the\nembedding space by selecting inputs that are semantically dissimilar to the ones already chosen. For\nnegative sampling, we prioritize the selection of product samples that yield closer semantic similarity\nto a given query (a product name or description) but belong to a different class. We compute the\ncosine similarities between the query and negative samples using the same embedding model used\nin the semantic diversity sampling and select samples with higher similarities. By selecting more\nsimilar candidates as negative samples, the dataset can better benchmark the model’s capability to\ncapture the subtle differences between closely related classes. This method, commonly known as hard\nnegative sampling, is particularly effective for evaluations involving fine-grained classifications, such\nas differentiating between closely related categories in IFC and Uniclass classification hierarchies. In\nall sampling methods, including plain random sampling, once a sample is selected, it is only reused\nin another subset once all samples included within the pool have been exhausted. This way, we\nmaximize the utilization of available samples and maintain diversity within the datasets.\n3\nBenchmark\n3.1\nTasks overview\nEvaluating text embeddings across different tasks is crucial for assessing the transferability of their\ncapabilities to various downstream applications. Hence, our proposed benchmark covers three main\ntasks: clustering, retrieval, and reranking. In addition to domain coverage and cross-task adaptability,\nevaluating text embedding models requires careful consideration of input text length. To ensure the\ncoverage for varying input lengths, the text entities included in our datasets fall into two categories:\n(a) sentences, which are derived from product titles/names, and (b) paragraphs, which are derived\nfrom product descriptions/definitions. Accordingly, each task-specific dataset in our benchmark is\ngrouped into one of the following categories:\n• Sentence to Sentence (S2S): Utilizing product titles as input text.\n• Paragraph to Paragraph (P2P): Utilizing product descriptions (which can be concatenated\nwith the product name) as input text.\n• Sentence to Paragraph (S2P): Comparing product titles against product descriptions.\nOur proposed benchmark follows MTEB [Muennighoff et al., 2022] for reporting text embedding\nperformance scores. Hence, various metrics are implemented within our benchmark, which can be\ncomputed with flexible parameter configurations. The primary metrics, which serve as default scores\nfor task-specific as well as overall comparisons reported in this study, are outlined in each task’s\ndescription.\n3.1.1\nClustering\nClustering tasks involve grouping similar built products into meaningful clusters based on their\nsimilarities in textual representation. Our proposed tasks include S2S and P2P categories, where\n5\n\nproduct names and descriptions act as input text for each dataset type, respectively. Each clustering\ntask dataset is comprised of various subsets, covering diverse subdomain subjects and different levels\nof granularity. To create the subsets within each clustering dataset, we first select a subset of product\nlabels from one of the three levels of product hierarchy, either from one specific corpus or across both\ncorpora. We then apply the previously described diversity-based sampling method to sample product\nnames (S2S datasets) or descriptions (P2P datasets) for selected labels.\nTo ensure the quality of the subsets, we evaluate the baseline scores using two embedding models, one\nfor the upper threshold (\"mxbai-embed-large-v1\"[Li and Li, 2023]) and one for the lower threshold\n(\"paraphrase-multilingual-MiniLM-L12-v2\"[Reimers and Gurevych, 2019]). A subset is included in\nthe dataset only if its score with the upper threshold model is below 0.8 and greater than 1/N with\nthe baseline model, where N is the number of unique labels. The upper and lower thresholds are set\nto maintain task difficulty and ensure the task performs better than random guessing, respectively.\nSubsets meeting these criteria are shuffled to eliminate order bias before being added to the dataset.\nWe compute V-measure scores [Rosenberg and Hirschberg, 2007] by training a mini-batch k-means\nmodel using vector embeddings, with k set to the number of unique labels in each clustering subset.\nThe V-measure, ranging from 0 to 1 (higher is better), represents the harmonic mean of two distinct\nmetrics: homogeneity and completeness. Here, homogeneity measures the extent to which clusters\ncontain only products from a single category, while completeness indicates how well all products\nfrom a given category are grouped into the same cluster. More details regarding the calculation of\nV-measure can be found in [Rosenberg and Hirschberg, 2007].\n3.1.2\nRetrieval\nRetrieval tasks aim to identify relevant documents, i.e., product textual descriptions, in response to\na given query. Our proposed retrieval datasets are framed as S2P and P2P tasks, where built asset\ndescriptions serve as the corpus (the documents to be retrieved), and product titles and descriptions\nact as queries for the S2P and P2P tasks, respectively. The query-document relevancy ground truth\nis derived from existing mappings that identify the alignment between IFC and Unicalss product\nentities. These mappings, validated and published by NBS[NBS, 2024a], can be found in the official\nUnicalss table release[NBS, 2024b].\nFirst, we encode all queries and product descriptions into corresponding embedding vectors. These\nembeddings are then used to calculate the pairwise similarity between a given query and all product\ndescriptions using cosine similarity. Subsequently, product descriptions included in each retrieval\ndataset are ranked according to descending cosine similarity scores. Finally, we compute nDCG@10\n(Normalized Discounted Cumulative Gain [Järvelin and Kekäläinen, 2002] at rank 10) as the primary\nmetric. This score, which can range between 0 and 1 (higher is better), reflects the relevancy of the\nranked products based on their positions within the top 10 ranks by applying a logarithmic discount\nfactor to penalize results that appear lower.\n3.1.3\nReranking\nIn our reranking tasks, the aim is to rank a set of product descriptions with reference to their relevance\nto a product query. Similar to retrieval tasks, reranking tasks are framed as S2P and P2P types, and\npairwise similarity between query and product description embeddings is computed based on cosine\nsimilarity. The primary distinction between retrieval and reranking tasks lies in their scope and focus.\nWhile our retrieval tasks involve ranking the entire product corpus, reranking narrows the focus\nto a smaller set of positive and negative subsets, which are selected using the methods outlined in\nthe previous section to ensure diversity and difficulty (avoiding very high scores from overfitting)\nwithin the dataset. Positive and negative samples are selected using the methods described in the\nprevious section, thereby maintaining the diversity and difficulty of the dataset. By concentrating on\na smaller and more challenging group of product descriptions, our reranking tasks aim to provide a\nmore fine-grained evaluation of the model’s ability to rank relevant items accurately.\nSimilar to retrieval tasks, we use cosine similarity to compute pairwise similarity between a given\nquery and product descriptions included in corresponding positive and negative sets. Subsequent to\nranking the descriptions based on the cosine similarity scores, we compute MAP (Mean Average\nPrecision) as our primary metric. MAP provides an averaged measure of precision across all relevant\nproducts, ranging between 0 and 1, with higher values indicating better performance. It is worth\n6\n\nClustering tasks\nNo. of\nUnique/total\nAvg. sample\nTotal No. of\nAvg. unique label\nsubsets\nsamples\nlength\nunique labels\nper subset\nClustering-s2s\n18\n2545/3815\n28.04\n31\n5\nClustering-p2p\n20\n3067/4577\n207.91\n35\n5\nRetrieval tasks\nNo. of\nAvg. query\nNo. of\nAvg. document\nNo. of document\nqueries\nlength\ndocuments\nlength\nper query (Avg.)\nRetrieval-s2p\n977\n30.35\n2761\n312.75\n8\nRetrieval-p2p\n977\n128.5\n2761\n312.75\n8\nReranking tasks\nNo. of\nAvg. query\nNo. of positives\nNo. of negatives\nAvg. samples\nqueries\nlength\n(unique/total)\n(unique/total)\nlength\nReranking-s2p\n179\n27.89\n1253/1253\n2281/3759\n310.15\nReranking-p2p\n179\n140.44\n1253/1253\n2241/3759\n309.66\nTable 1: Summary of dataset statistics per benchmark task.\nnoting that retrieval metrics reflect overall ranking quality while reranking metrics focus on how early\nrelevant products appear in the list.\n4\nResults\nTable 1 provides a comprehensive summary of the dataset statistics across the three main tasks\nin our benchmark. The unique number of sample entries in our clustering datasets shows that\nmore than half of the samples available from the combined product corpora could pass the quality\nthresholds explained in the methods section. In the retrieval and reranking task, the same retrieval\nand reranking document corpus is shared between the subtasks of each task category. This design\nenables a comparative analysis of model performance on different query types, with S2P focusing\non shorter product names and P2P targeting longer product descriptions. We applied a 1:3 positive-\nto-negative sampling ratio to create a balanced yet challenging evaluation set, ensuring that models\nmust distinguish effectively between relevant and irrelevant documents.\nTo outline the distinctions between our newly constructed datasets and existing ones, we conducted\na thematic semantic similarity comparison between our clustering datasets and those from MTEB\nbenchmark. Using the \"stella-en-400M-v5\" model, which is the most performant small-sized model\nin our evaluations (see Table 2), we generated embeddings for 200 randomly selected samples and\naveraged them within each dataset. Figure 2 depicts the cosine similarity matrix as a heatmap, where\ndarker shades indicate higher content similarity. The high similarity scores between our proposed\nsubtasks confirm strong internal consistency within our benchmark. Moreover, moderate to high\nsimilarities with StackExchange, Reddit, and Arxiv datasets reflect thematic overlaps with broader\ndomain content. A discussion of the observed similarities is provided in the next section.\nIn our benchmarking experiments, we evaluated models across a broad range of sizes, from relatively\nsmall models with 33 million parameters to significantly larger models exceeding seven billion\nparameters. However, due to computational constraints, the majority of models tested have less than\none billion parameters. The selected models span various positions on the most recent record of\nMTEB leaderboard (as of September 21, 2024), ranging from first place (i.e., \"NV-Embed-v2\"[Lee\net al., 2024a]) to 136th place (i.e., \"paraphrase-multilingual-MiniLM-L12-v2\"). For models that\nare pre-trained with instruction-based data, we used built-in or recommended prompts as provided\nin the model card’s official web page or associated research papers, when available. For example,\n\"mxbai-embed-large-v1\" requires custom prompts only for retrieval and reranking tasks, while \"NV-\nEmbed-v2\" needs specific task-based prompts for clustering tasks as well. For models without built-in\ntask instructions, we applied a general set of prompts to ensure consistency across tasks (prompts are\navailable at the project’s public GitHub repository[Mehrzad, 2024]).\nThe top-ranked model in our benchmark, \"NV-Embed-v2\", also holds first place on the latest MTEB\nleaderboard. However, it does not consistently outperform all other models across all tasks. In fact,\na closer examination reveals variability in model size and performance relationship. For example,\n\"gte-small\", the smallest model in our evaluation with 33 million parameters, delivers competitive\nscores, nearly matching the average scores of models ten times its size and even outperforming larger\nmodels in specific tasks. Despite the previously reported strong correlation between model size and\n7\n\nclustering-s2s\nclustering-p2p\narxiv-clustering-p2p\narxiv-clustering-s2s\nbiorxiv-clustering-p2p\nbiorxiv-clustering-s2s\nmedrxiv-clustering-p2p\nmedrxiv-clustering-s2s\nreddit-clustering\nreddit-clustering-p2p\nstackexchange-clustering\nstackexchange-clustering-p2p\ntwentynewsgroups-clustering\nclustering-s2s\nclustering-p2p\narxiv-clustering-p2p\narxiv-clustering-s2s\nbiorxiv-clustering-p2p\nbiorxiv-clustering-s2s\nmedrxiv-clustering-p2p\nmedrxiv-clustering-s2s\nreddit-clustering\nreddit-clustering-p2p\nstackexchange-clustering\nstackexchange-clustering-p2p\ntwentynewsgroups-clustering\n0.92\n0.77\n0.71\n0.81\n0.71\n0.92\n0.69\n0.66\n0.84\n0.77\n0.77\n0.70\n0.81\n0.87\n0.94\n0.66\n0.64\n0.74\n0.68\n0.86\n0.81\n0.74\n0.70\n0.75\n0.79\n0.85\n0.90\n0.95\n0.87\n0.78\n0.76\n0.83\n0.71\n0.80\n0.68\n0.78\n0.80\n0.74\n0.74\n0.77\n0.69\n0.75\n0.66\n0.72\n0.92\n0.91\n0.81\n0.81\n0.87\n0.74\n0.83\n0.70\n0.80\n0.94\n0.88\n0.75\n0.69\n0.77\n0.73\n0.68\n0.67\n0.58\n0.60\n0.74\n0.76\n0.81\n0.89\n0.79\n0.77\n0.85\n0.72\n0.81\n0.68\n0.78\n0.96\n0.90\n0.96\n0.77\nCosine Similarity Heatmap of Clustering Dataset Embeddings\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nFigure 2: Thematic similarity heatmap between our proposed clustering tasks and those from MTEB.\nAverage embeddings are derived from 200 random samples per dataset, encoded using the \"mxbai-\nembed-large-v1\" model[Li and Li, 2023]. Datasets from our proposed benchmark are highlighted in\nred.\nperformance[Muennighoff et al., 2022], our experiments show that superior performance associated\nwith larger models is only evident at the extreme upper end of the parameter scale. This observation\nsupports the growing emphasis on developing and deploying smaller, more efficient models for both\nresearch and real-world applications in this specialized field.\nMotivated by the hypothesis that existing datasets with similar thematic content would yield com-\nparable performance evaluations, we examined the consistency of relative model performances as\nfollows: Given the observed thematic similarity between our clustering datasets and specific MTEB\ndatasets, particularly \"StackExchange\" and \"Reddit\" (see Figure 2), we compared the rankings of\nmodel performance across both our datasets and the selected MTEB datasets. As it can be seen from\nTable 3, the comparative evaluation of the relative rankings indicates a notable variation in model per-\nformances, notably in the case of \"multilingual-e5-large-instruct\", \"gte-small\", \"stella_en_1.5B_v5\",\nand \"text-embedding-3-small\". These observed variabilities further highlight the limitations of relying\non general-purpose benchmark datasets, even when relatively high thematic similarities are present,\nunderscoring the importance of domain-specific evaluations.\nWhile our benchmarking experiments primarily focused on open-source models, we also included the\nproprietary text embedding models from OpenAI, both the small and large versions. The inclusion of\nthe proprietary models is motivated by a recent study where closed-source models tend to achieve\nrelatively higher performance when embedding text in underrepresented languages [Enevoldsen et al.,\n2024]. We hypothesize that built asset text, as an underexplored domain, might be similarly better\nrepresented by proprietary models. Notably, text-embedding-3-large ranks second in our benchmark,\nperforming nearly on par with the top-ranked model. In contrast, the smaller model performed\nmore moderately, ranking in the middle of our benchmark. While the former observation aligns\nwith the findings of [Enevoldsen et al., 2024], the latter is in line with the latest MTEB leaderboard\nresults where closed-source commercial embedding APIs generally underperform compared to their\n8\n\nTasks (→)\nClustering\nRetrieval\nReranking\nAvg.\nParam.\nMTEB\nModels (↓)\ns2s\np2p\ns2p\np2p\ns2p\np2p\n-\n(mil)\nRank\nPre-trained without task instructions\ngte-base-en-v1.5\n48.38\n51.83\n79.98\n59.42\n66.54\n66.73\n62.15\n137\n39\ngte-large-en-v1.5\n43.42\n51.05\n83.32\n63.27\n72.76\n70.15\n64.00\n434\n24\nbge-base-en-v1.5\n43.00\n51.78\n82.56\n61.65\n67.01\n63.38\n61.56\n109\n43\nbge-large-en-v1.5\n46.69\n52.41\n82.60\n64.86\n68.44\n65.47\n63.41\n335\n35\nUAE-Large-V1\n45.45\n49.53\n83.32\n66.42\n70.04\n68.53\n63.88\n335\n29\nGIST-Embedding-v0\n46.43\n49.96\n82.82\n62.78\n68.81\n65.75\n62.76\n109\n41\nGIST-large-Embedding-v0\n47.97\n47.91\n84.01\n67.06\n69.53\n68.03\n64.08\n335\n34\ne5-base-v2\n42.59\n50.24\n80.83\n61.46\n69.11\n62.91\n61.19\n109\n64\ne5-large-v2\n42.11\n49.45\n81.95\n64.63\n68.61\n64.58\n61.89\n335\n55\nmultilingual-e5-large-instruct\n48.01\n52.82\n80.35\n64.55\n67.85\n65.90\n63.25\n560\n42\nmultilingual-e5-small\n42.98\n48.16\n76.38\n55.03\n64.78\n62.34\n58.28\n118\n112\nall-MiniLM-L12-v2\n42.00\n46.52\n79.97\n58.81\n66.20\n63.97\n59.58\n33\n117\nparaphrase-multilingual-MiniLM-L12-v2\n37.60\n45.70\n69.01\n49.90\n61.23\n59.15\n53.77\n118\n136\ngte-base\n45.96\n51.55\n82.91\n62.95\n68.97\n66.26\n63.10\n109\n51\ngte-large\n48.54\n55.24\n84.32\n66.08\n70.94\n69.25\n65.73\n335\n47\ngte-small\n44.31\n55.55\n82.37\n60.55\n68.82\n65.23\n62.80\n33\n70\nPre-trained with task instructions\ngte-Qwen2-7B-instruct\n50.19\n62.39\n86.28\n73.20\n69.47\n67.51\n68.17\n7069\n6\nmxbai-embed-large-v1\n47.49\n52.45\n83.51\n66.60\n70.10\n69.66\n64.97\n335\n28\nmultilingual-e5-large-instruct\n48.10\n59.43\n82.91\n64.42\n70.53\n69.23\n65.77\n560\n42\nNV-Embed-v2\n58.61\n67.34\n85.23\n77.02\n66.67\n70.34\n70.87\n7851\n1\nstella-en-1.5B-v5\n53.60\n54.57\n84.18\n71.21\n71.57\n71.77\n67.82\n1545\n3\nstella-en-400M-v5\n53.39\n55.78\n84.60\n70.00\n69.58\n69.36\n67.12\n435\n7\nProprietary embedding APIs\ntext-embedding-3-small\n49.72\n49.72\n79.97\n65.68\n65.33\n66.99\n62.90\n-\n58\ntext-embedding-3-large\n49.75\n55.48\n84.99\n75.38\n71.93\n72.46\n68.33\n-\n30\nTable 2: Average scores of benchmarked models per task, based on the task-specific metrics mentioned\nin the task descriptions. The first and second highest scores for each task are highlighted in bold and\nunderlined, respectively. MTEB ranks are sourced from records as of September 21, 2024.\nopen-source counterparts. These observations raise questions about the underlying factors. However,\nthe lack of knowledge about the key characteristics of proprietary models, such as their size and\ndiversity in training data, prevents us from offering a detailed, conclusive account of their relative\nperformance.\nOur benchmarking results reveal a notable difference in performance between shorter and longer text\ninputs in different tasks. In particular, across the board, models consistently show lower performance\nin the S2S clustering task compared to the P2P one. This observation can be attributed to the limited\npresence of contextual clues given the significantly short length of the input text in the S2S clustering\ntask (see Table 1). On the other hand, in reranking and retrieval tasks, the majority of the models yield\nmoderately higher scores in S2P tasks. The likely explanation for the latter observation is that the\nshorter length of the sentences (product names) in S2P tasks can lead to a lower amount of irrelevant\ninformation (noise) in the input query. Since product names tend only to encapsulate the critical\ninformation about the target product, they can yield more precise and discriminative text (query)\nrepresentations for similarity matching.\n5\nDiscussion\nOur benchmarking results offer critical insights into the effectiveness of state-of-the-art pre-trained\ntext embedding models in aligning built asset information. One of the key findings of our study is\nthe variability in performance across tasks, even among top-performing models. Our results suggest\nthat model effectiveness is not strongly correlated across model sizes, emphasizing that size alone is\nnot a reliable predictor of model performance in the specialized domain of built asset information\nmanagement. The interpretation of the relationship between model size and embedding effectiveness\nis further complicated by the performance gap observed when comparing models pre-trained with\nand without instruction tuning. Instruction-tuned models showed higher performance in the majority\nof our benchmark tasks. Considering the larger size of the instruction-tuned models included in our\nexperiments, the latter observation raises an important question for future research: To what extent\n9\n\nNV-Embed-v2\ngte-Qwen2-7B-instruct\nmultilingual-e5-large-instruct\nstella_en_400M_v5\ngte-small\ntext-embedding-3-large\ngte-large\nstella_en_1.5B_v5\nmxbai-embed-large-v1\nbge-large-en-v1.5\ngte-base-en-v1.5\nbge-base-en-v1.5\ngte-base\ngte-large-en-v1.5\ne5-base-v2\nGIST-Embedding-v0\ntext-embedding-3-small\nUAE-Large-V1\ne5-large-v2\nmultilingual-e5-small\nGIST-large-Embedding-v0\nall-MiniLM-L12-v2\nparaphrase-multilingual-MiniLM-L12-v2\nclustering-p2p (ours)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nstackexchange-clustering\n1\n3\n10\n4\n19\n7\n9\n2\n16\n14\n8\n20\n11\n6\n18\n15\n5\n12\n17\n21\n13\n22\n23\nreddit-clustering\n4\n1\n16\n3\n18\n10\n6\n2\n9\n15\n14\n21\n12\n11\n17\n13\n5\n8\n19\n23\n7\n20\n22\nTable 3: Comparison of model rankings across datasets with high thematic similarity (see Figure 2)\ncan instruction-tuning help smaller models adapt to the specialized domain of the built environment?\nThis opens a promising line of investigation into how task-specific training with instruction-based data\ncan better align a model’s understanding with the intricate semantics of built asset data, particularly\nfor models with smaller sizes. Finally, in addition to the variability in model performance across\ndifferent tasks and text input lengths, the results of our comparative examinations highlight the limited\ntransferability of evaluations based on general benchmarks. Our experiments indicate that, even with\nrelatively high thematic similarity, general-purpose benchmarks remain inadequate in capturing the\nunique semantic complexity and contextual dependencies present in the textual descriptions of the\nbuilt asset.\nThe above-mentioned points highlight the critical need for tailored benchmarking datasets to examine\nthe effectiveness of various domain adaptation strategies in this field of research. Our work contributes\nto the body of research by laying a robust foundation for future evaluations and providing a benchmark\nthat is carefully constructed to reflect the complexities of built asset data. Our proposed datasets\ncover diverse subdomains and exhibit varying levels of granularity, mirroring real-world scenarios\nwhere built products are required to be mapped across various data dictionaries. The datasets can\nbe used not only for evaluating new or fine-tuned text embedding models for cross-mapping built\nasset data but also as a contextually rich text corpus to support the training of task-specific language\nmodels for other downstream tasks, such as information extraction. Finally, this work contributes to\nthe broader discourse on the transferability of the general-purpose language models’ capabilities by\nfocusing on built asset data as a representative example of niche and underexplored domains.\nOne key limitation of our study is that the text sources used in our work are exclusively in English,\nlimiting the generalizability of our findings to other languages. Another significant challenge was\nidentifying data sources that were both of high quality and could be redistributed as public datasets. In\nthis light, although the developed datasets proved sufficient for our current analysis, future work could\nbenefit from larger-scale datasets and introduce training and validation splits to support new tasks. It\nis recommended to prioritize exploring more extensive and diverse text sources to include multiple\nlanguages and new tasks where the availability of large training splits plays a crucial role, such as text\nclassification or reranking based on cross-encoder architectures. Finally, through the public release of\nour benchmarking resources in alignment with the MTEB’s open-source software, we aim to ensure\nthe reproducibility and extendability of our work through community-driven enhancements.\nData availability\nThe datasets and codes developed in this study are openly accessible at the following GitHub\nrepository: https://github.com/mehrzadshm/built-bench-paper. All materials are licensed\nunder the Creative Commons Attribution-NoDerivatives 4.0 International License (CC BY-ND 4.0).\nAny future updates, including references to additional data and relevant resources, will be incorporated\ninto this repository.\nReferences\nbuildingsmart international. https://www.buildingsmart.org/, 2024. Accessed: 2024-06-24.\n10\n\nIfcopenshell. https://github.com/IfcOpenShell/IfcOpenShell/, 2024. Accessed: 2024-06-24.\nbuildingSmart International. buildingsmart data dictionary (bsdd). https://www.buildingsmart.org/\nusers/services/buildingsmart-data-dictionary/, 2024a. Accessed: 2024-06-24.\nbuildingSmart International.\nIfc 4.3 documentation.\nhttps://standards.buildingsmart.org/IFC/\nRELEASE/IFC4_3/, 2024b. Accessed: 2024-06-24.\nX. Cao and M. Kosinski. Large language models know how the personality of public figures is perceived by the\ngeneral public. Scientific Reports, 14(1):6735, 2024.\nR. Cappuzzo, P. Papotti, and S. Thirumuruganathan. Creating embeddings of heterogeneous relational datasets\nfor data integration tasks. In Proceedings of the 2020 ACM SIGMOD international conference on management\nof data, pages 1335–1349, 2020.\nK. Enevoldsen, M. Kardos, N. Muennighoff, and K. L. Nielbo. The scandinavian embedding benchmarks: Com-\nprehensive assessment of multilingual and monolingual text embedding. arXiv preprint arXiv:2406.02396,\n2024.\nK. Forth, P. Berggold, and A. Borrmann. Domain-specific fine-tuning of llm for material matching of bim\nelements and material passports. In Proc. of 2024 ASCE International Conference on Computing in Civil\nEngineering, 2024.\nK. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4):422–446, 2002.\nK. Jeon, G. Lee, S. Yang, Y. Kim, and S. Suh. Dynamic building defect categorization through enhanced\nunsupervised text classification with domain-specific corpus embedding methods. Automation in Construction,\n157:105182, 2024.\nY. Jung, J. Hockenmaier, and M. Golparvar-Fard. Transformer language model for mapping construction\nschedule activities to uniformat categories. Automation in Construction, 157:105183, 2024.\nC. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. Nv-embed: Improved techniques for\ntraining llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024a.\nJ. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, et al. Gecko:\nVersatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024b.\nX. Li and J. Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023.\nP. E. Love and J. Matthews. The ‘how’of benefits management for digital technology: From engineering to asset\nmanagement. Automation in Construction, 107:102930, 2019.\nS.\nMehrzad.\nbuilt-bench-paper\n(GitHub\nrepository).\nhttps://github.com/mehrzadshm/\nbuilt-bench-paper, 2024. Accessed: 2024-10-20.\nT. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases\nand their compositionality. Advances in neural information processing systems, 26, 2013.\nN. Moretti, X. Xie, J. Merino Garcia, J. Chang, and A. Kumar Parlikad. Federated data modeling for built\nenvironment digital twins. Journal of Computing in Civil Engineering, 37(4):04023013, 2023.\nN. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark. arXiv preprint\narXiv:2210.07316, 2022.\nNBS. National building specification. https://www.thenbs.com/, 2024a. Accessed: 2024-06-24.\nNBS. Uniclass. https://uniclass.thenbs.com/, 2024b. Accessed: 2024-06-24.\nM. Ostendorff, E. Ash, T. Ruas, B. Gipp, J. Moreno-Schneider, and G. Rehm. Evaluating document represen-\ntations for content-based legal literature recommendations. In Proceedings of the eighteenth international\nconference on artificial intelligence and law, pages 109–118, 2021.\nJ. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Proceedings\nof the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543,\n2014.\nL. Rasmy, Y. Xiang, Z. Xie, C. Tao, and D. Zhi. Med-bert: pretrained contextualized embeddings on large-scale\nstructured electronic health records for disease prediction. NPJ digital medicine, 4(1):86, 2021.\n11\n\nN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, 11 2019. URL http://arxiv.org/abs/1908.10084.\nC. J. Roberts, E. A. Pärn, D. J. Edwards, and C. Aigbavboa. Digitalising asset management: concomitant benefits\nand persistent challenges. International Journal of Building Pathology and Adaptation, 36(2):152–173, 2018.\nA. Rosenberg and J. Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure.\nIn Proceedings of the 2007 joint conference on empirical methods in natural language processing and\ncomputational natural language learning (EMNLP-CoNLL), pages 410–420, 2007.\nH. Rouhizadeh, I. Nikishina, A. Yazdani, A. Bornet, B. Zhang, J. Ehrsam, C. Gaudet-Blavignac, N. Naderi,\nand D. Teodoro. A dataset for evaluating contextualized representation of biomedical concepts in language\nmodels. Scientific Data, 11(1):455, 2024.\nM. Shahinmoghadam, S. E. Kahou, and A. Motamedi. Neural semantic tagging for natural language-based\nsearch in building information models: Implications for practice. Computers in Industry, 155:104063, 2024.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30, 2017.\nZ. Wang, M. Bergés, and B. Akinci. Pre-trained language model based method for building information model\nto building energy model transformation at metamodel level. In ISARC. Proceedings of the International\nSymposium on Automation and Robotics in Construction, volume 41, pages 17–25. IAARC Publications,\n2024.\nR. S. Wilkho, S. Chang, and N. G. Gharaibeh. Ff-bert: A bert-based ensemble for automated classification of\nweb-based text on flash flood events. Advanced Engineering Informatics, 59:102293, 2024.\nC. Wu, X. Li, Y. Guo, J. Wang, Z. Ren, M. Wang, and Z. Yang. Natural language processing for smart\nconstruction: Current status and future directions. Automation in Construction, 134:104059, 2022.\nY. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu. Biowordvec, improving biomedical word embeddings with\nsubword information and mesh. Scientific data, 6(1):52, 2019.\n12",
    "pdf_filename": "Benchmarking_pre-trained_text_embedding_models_in_aligning_built_asset_information.pdf"
}