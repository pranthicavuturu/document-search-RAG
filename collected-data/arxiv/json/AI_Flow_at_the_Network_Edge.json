{
    "title": "1",
    "abstract": "(LLMs) and their multimodal variants have led to remarkable As a result, on-device model inference struggles to support progress across various domains, demonstrating impressive ca- real-timeresponsessincethelargefoundationmodelsdemand pabilities and unprecedented potential. In the era of ubiquitous intensive computation [10]. Besides, edge devices are often connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered wirelessly connected. The limited uplink bandwidth and the services accessible at the network edge. However, pushing large highlydynamicnatureofwirelesschannelshindertransmitting models from the cloud to resource-constrained environments large volumes of data collected by edge devices to cloud faces critical challenges. Model inference on low-end devices servers in real-time. leadstoexcessivelatencyandperformancebottlenecks,whileraw Aiming at efficient delivery of diversified intelligent ser- data transmission over limited bandwidth networks causes high communicationoverhead.ThisarticlepresentsAIFlow,aframe- vices, we propose AI Flow, a framework that seamlessly workthatstreamlinestheinferenceprocessbyjointlyleveraging integrates intelligence capabilities directly at the network theheterogeneousresourcesavailableacrossdevices,edgenodes, edge. Specifically, AI Flow aims to streamline the inference and cloud servers, making intelligence flow across networks. To processbyjointlyleveragingtheheterogeneouscomputational facilitate cooperation among multiple computational nodes, the resources available across devices, edge nodes, and cloud proposed framework explores a paradigm shift in the design of communication network systems from transmitting information servers. This framework distributes the inference workload flow to intelligence flow, where the goal of communications is acrossdifferentnetworklayersandadaptstodynamicnetwork task-orientedandfoldedintotheinferenceprocess.Experimental conditions. To facilitate cooperation among multiple compu- results demonstrate the effectiveness of the proposed framework tational nodes, AI Flow provides a paradigm shift in the through an image captioning use case, showcasing the ability to design of communication network systems from transmitting reduceresponselatencywhilemaintaininghigh-qualitycaptions. This article serves as a position paper for identifying the informationflowtointelligenceflow.Thegoalofcommunica- motivation, challenges, and principles of AI Flow. tions needs to be folded into the inference process. Instead of sending raw data from edge devices to servers for processing, the intelligence flow features a task-oriented property, where I. INTRODUCTION edgedevicesextractonlycriticalfeaturesfromtherawsensory We are witnessing a transformative era in the field of dataanddiscardredundantinformationtoreducecommunica- artificial intelligence (AI) with groundbreaking technologies tion overhead. To summarize, the advantages of the AI Flow [1]–[3]. LLMs, like ChatGPT and PaLM, have demonstrated framework are manifold. remarkable capabilities in natural language understanding, generation, and reasoning. Going beyond language, multi- • Ubiquity: The widespread deployment of AI capabilities attheedgemakesintelligentresponsespossiblewherever modal foundation models, like CLIP and Llama, have show- thereisnetworkaccess.Thisensuresthatvariousdevices cased exceptional performance in cross-modal perception, benefit from AI-driven insights without the need for understanding, and interaction tasks. These advancements led constant connectivity to central servers. to their adaptations in a broad spectrum of application do- mains, including embodied robotics, augmented reality, and • Adaptivity: The framework distributes inference tasks according to available computational resources, dynamic autonomousdriving[4]–[7].Mostrecently,edgedevices(mo- network conditions, and task requirements. Adaptively bile phones, smart wearables, and IoT sensors) are becoming scheduling task execution maintains performance in fluc- increasingly widespread, and sensory data are easy to access. tuating environments. This has sparked a surge of interest in pushing intelligence applications from the central cloud to the network edge [8], • Low-latency: By prioritizing data processing close to the source, the delay in transmitting information to powerful [9]. As visualized in Fig. 1, the future envisions a scenario servers is minimized. This provides a quicker end-to-end where AI technologies are seamlessly integrated into various response and achieves a smoother user experience. aspects of daily life. Whileofferingexcitingopportunities,deployinglargemod- The rest of the article is organized as follows. Section II els at the network edge faces new challenges. In contrast to introduces typical applications at the network edge, and III cloud intelligence, performing AI tasks at the network edge providesasystemoverviewofAIFlow.Twotypesofenabling differs significantly in many cases. First, the fundamental techniques,namely,cooperativeinferenceandmodelinference difference is the available resources. Edge devices are usually speedup, have been elaborated in Section IV and Section V, equipped with limited computation resources, whereas cloud respectively. Section VI presents a case study to evaluate the effectiveness of the proposed AI Flow framework. Finally, we TheauthorsarewiththeInstituteofArtificialIntelligence(TeleAI),China concludethisarticleandpointoutfutureresearchopportunities Telecom, China (E-mail: shaojw2@chinatelecom.cn, xuelong_li@ieee.org). ThecorrespondingauthorisXuelongLi. in Section VII. 4202 voN 91 ]PS.ssee[ 1v96421.1142:viXra",
    "body": "1\nAI Flow at the Network Edge\nJiawei Shao and Xuelong Li, Fellow, IEEE\nAbstract—Recent advancements in large language models servers possess a large number of powerful processing units.\n(LLMs) and their multimodal variants have led to remarkable As a result, on-device model inference struggles to support\nprogress across various domains, demonstrating impressive ca-\nreal-timeresponsessincethelargefoundationmodelsdemand\npabilities and unprecedented potential. In the era of ubiquitous\nintensive computation [10]. Besides, edge devices are often\nconnectivity, leveraging communication networks to distribute\nintelligence is a transformative concept, envisioning AI-powered wirelessly connected. The limited uplink bandwidth and the\nservices accessible at the network edge. However, pushing large highlydynamicnatureofwirelesschannelshindertransmitting\nmodels from the cloud to resource-constrained environments large volumes of data collected by edge devices to cloud\nfaces critical challenges. Model inference on low-end devices\nservers in real-time.\nleadstoexcessivelatencyandperformancebottlenecks,whileraw\nAiming at efficient delivery of diversified intelligent ser-\ndata transmission over limited bandwidth networks causes high\ncommunicationoverhead.ThisarticlepresentsAIFlow,aframe- vices, we propose AI Flow, a framework that seamlessly\nworkthatstreamlinestheinferenceprocessbyjointlyleveraging integrates intelligence capabilities directly at the network\ntheheterogeneousresourcesavailableacrossdevices,edgenodes, edge. Specifically, AI Flow aims to streamline the inference\nand cloud servers, making intelligence flow across networks. To\nprocessbyjointlyleveragingtheheterogeneouscomputational\nfacilitate cooperation among multiple computational nodes, the\nresources available across devices, edge nodes, and cloud\nproposed framework explores a paradigm shift in the design of\ncommunication network systems from transmitting information servers. This framework distributes the inference workload\nflow to intelligence flow, where the goal of communications is acrossdifferentnetworklayersandadaptstodynamicnetwork\ntask-orientedandfoldedintotheinferenceprocess.Experimental conditions. To facilitate cooperation among multiple compu-\nresults demonstrate the effectiveness of the proposed framework\ntational nodes, AI Flow provides a paradigm shift in the\nthrough an image captioning use case, showcasing the ability to\ndesign of communication network systems from transmitting\nreduceresponselatencywhilemaintaininghigh-qualitycaptions.\nThis article serves as a position paper for identifying the informationflowtointelligenceflow.Thegoalofcommunica-\nmotivation, challenges, and principles of AI Flow. tions needs to be folded into the inference process. Instead of\nsending raw data from edge devices to servers for processing,\nthe intelligence flow features a task-oriented property, where\nI. INTRODUCTION\nedgedevicesextractonlycriticalfeaturesfromtherawsensory\nWe are witnessing a transformative era in the field of dataanddiscardredundantinformationtoreducecommunica-\nartificial intelligence (AI) with groundbreaking technologies tion overhead. To summarize, the advantages of the AI Flow\n[1]–[3]. LLMs, like ChatGPT and PaLM, have demonstrated framework are manifold.\nremarkable capabilities in natural language understanding,\ngeneration, and reasoning. Going beyond language, multi- • Ubiquity: The widespread deployment of AI capabilities\nattheedgemakesintelligentresponsespossiblewherever\nmodal foundation models, like CLIP and Llama, have show-\nthereisnetworkaccess.Thisensuresthatvariousdevices\ncased exceptional performance in cross-modal perception,\nbenefit from AI-driven insights without the need for\nunderstanding, and interaction tasks. These advancements led\nconstant connectivity to central servers.\nto their adaptations in a broad spectrum of application do-\nmains, including embodied robotics, augmented reality, and • Adaptivity: The framework distributes inference tasks\naccording to available computational resources, dynamic\nautonomousdriving[4]–[7].Mostrecently,edgedevices(mo-\nnetwork conditions, and task requirements. Adaptively\nbile phones, smart wearables, and IoT sensors) are becoming\nscheduling task execution maintains performance in fluc-\nincreasingly widespread, and sensory data are easy to access.\ntuating environments.\nThis has sparked a surge of interest in pushing intelligence\napplications from the central cloud to the network edge [8], • Low-latency: By prioritizing data processing close to the\nsource, the delay in transmitting information to powerful\n[9]. As visualized in Fig. 1, the future envisions a scenario\nservers is minimized. This provides a quicker end-to-end\nwhere AI technologies are seamlessly integrated into various\nresponse and achieves a smoother user experience.\naspects of daily life.\nWhileofferingexcitingopportunities,deployinglargemod- The rest of the article is organized as follows. Section II\nels at the network edge faces new challenges. In contrast to introduces typical applications at the network edge, and III\ncloud intelligence, performing AI tasks at the network edge providesasystemoverviewofAIFlow.Twotypesofenabling\ndiffers significantly in many cases. First, the fundamental techniques,namely,cooperativeinferenceandmodelinference\ndifference is the available resources. Edge devices are usually speedup, have been elaborated in Section IV and Section V,\nequipped with limited computation resources, whereas cloud respectively. Section VI presents a case study to evaluate the\neffectiveness of the proposed AI Flow framework. Finally, we\nTheauthorsarewiththeInstituteofArtificialIntelligence(TeleAI),China\nconcludethisarticleandpointoutfutureresearchopportunities\nTelecom, China (E-mail: shaojw2@chinatelecom.cn, xuelong_li@ieee.org).\nThecorrespondingauthorisXuelongLi. in Section VII.\n4202\nvoN\n91\n]PS.ssee[\n1v96421.1142:viXra\n2\nRoboticControl SmartHome\nAutonomousDriving AugmentedReality\nFig.1. Typicalintelligenceapplicationsatthenetworkedge.\nII. TYPICALINTELLIGENCEAPPLICATIONSATTHE insufficient for real-time inference due to the limitations of\nNETWORKEDGE network bandwidth and the difficulty in synchronization.\nLarge models can be directly applied or fine-tuned to a Augmented reality [6] enhances user interactions with their\nbroad range of tasks. As illustrated in Fig. 1, this part will surroundings through glasses, headsets, and other smart wear-\nfocus on four promising use cases: robotic control, smart ables. These devices overlay digital information, guided by\nhome, augmented reality, and autonomous driving. generativemodels,directlyintothefieldofvision.Forvisually\nRobotic control [4] is crucial for optimizing the interaction impairedindividuals,AIglassescanhighlightimportantvisual\nbetween robots and the environment. Vision-language models features such as object edges, provide navigational cues, or\n(VLMs) enhance the perception capabilities of robots. For display relevant contextual information about the environ-\ninstance, by combining three-dimensional point clouds with ment,greatlyenhancingspatialawareness.Additionally,LLM-\nvisual-language features from a VLM, a three-dimensional powered devices can streamline human communication by\nmap of the physical world can be created. Integrating audio translating spoken language into clear, concise text displayed\nsignals and haptic features with visual data enables embodied in real-time. However, the need to sense the environment and\nroboticstonavigateusingmultimodalobjectives.Withremark- process large volumes of multimodal data makes it imprac-\nablegeneralizationcapabilities,LLMsallowrobotstocompre- tical for resource-constrained headsets. This necessitates the\nhendhumaninstructionorcomplicatedenvironments,enabling development of more efficient processing techniques and the\nthemtoperformembodiedreasoningandbreakdowncomplex optimization of data transmission at the network edge.\ntasks into actionable steps. Nevertheless, centralized robotic Autonomous driving [7] has great promise to revolutionize\ncontrolinvolvesmassivestreamingvideoupload,whichcould transportation. VLMs enable a combination of scene descrip-\noverwhelm the wireless networks. tionandhierarchicalplanning.Typicalmethodsleveragevideo\nSmarthomes[5]benefitfromtheintegrationofmultimodal fromafront-viewcameratomakedrivingdecisionsregarding\nlarge language models (MLLMs), which utilize diverse sen- speed and angle. With LiDAR sensor data, more accurate\nsory data from the home environment. For instance, MLLMs perception can be achieved than with visual data alone, as\ncan intelligently adjust air conditioner settings by integrating cameraimagerycanbeimpactedbyweatherconditions.More-\ntemperaturedataandvoicecommands.Furthermore,thechan- over, LLMs can use textual data, such as traffic and weather\nnel state information (CSI) obtained from WiFi sensing pro- reports, to optimize route planning. Additionally, LLMs in-\nvides extra information for localization. This allows MLLMs teracting with passengers help them to understand the motion\nto make context-aware decisions, such as turning lights on controlprocessbyexplainingeachdrivingdecision.However,\nor off based on the predicted trajectory of humans inferred autonomous driving has stringent latency requirements. High\nfrom the CSI. However, the large models demand inten- latency between vehicles and the data center poses a risk\nsive computational resources. While heterogeneous devices in to safety. Addressing this issue is crucial for the successful\nsmarthomesprovidedistributedcomputingpower,theyremain implementation of autonomous driving systems.\n3\nAIFlow\nEdge devices Edgeservers Cloudservers\nFig.2. AsystemoverviewoftheAIFlowframework.\nIII. SYSTEMOVERVIEW B. Inference Paradigms\nThissectionoutlinesthemodules,inferenceparadigms,and There are three types of cooperative schemes in AI Flow,\nkey design considerations in AI Flow. including, on-device inference, device-edge cooperative infer-\nence, and device-edge-cloud cooperative inference.\n1) On-device inference: In this scheme, the devices com-\nA. Modules\nplete some simple inference tasks locally and independently\nThe architecture, as illustrated in Fig. 2, comprises the basedonthebuilt-inAIchip.Duetolimitationsinstorageand\nfollowing three modules. computationalresources,modellight-weightingtechniquesare\n1) Edge devices: The edge devices denote the end-user widely adopted to reduce storage and computational costs.\nequipment, such as smartphones, wearables, and IoT sensors, In addition, hardware accelerators, such as neural processing\nthat collect data and interact with users directly. They are units and field programmable gate arrays, are utilized to\nessential for initial data capture, preliminary processing, and enhance processing capabilities and energy efficiency. These\ninformation transmission. As these devices are often the first enable the scalable deployment of AI models, making on-\ntouchpoint in data processing, they play a critical role in device inference a widely adopted solution.\ndetermining the responsiveness and effectiveness of the AI 2) Device-edge cooperative inference: While on-device in-\nFlow framework. However, constrained by limited computing ference is suitable for basic tasks, it struggles to handle com-\nand energy resources, these devices cannot perform intensive plex tasks that require larger AI models, leading to excessive\ncomputation or storage-heavy tasks. This limitation necessi- latency. Edge computing offers supplementary computational\ntatesacooperativeprocessingarrangementwithedgeorcloud resourcesasapromisingsolution.Thissetupallowsdevicesto\nservers. either send raw data directly to edge servers for inference or\n2) Edge servers: The edge servers provide processing, toperformpreliminarycomputationslocallyandthensendthe\nstorage, and other resources at an edge location. Typical edge intermediate results to the edge servers for further processing.\nservers are deployed at base stations and cell towers that are Instead of uploading data to a central server far away, the\ndirectly connected to edge users and devices. By processing proximity to data sources reduces latency, saves bandwidth,\ndataclosertothesource,edgeserverscansignificantlyreduce and improves system responsiveness.\nthe round-trip time without sending data to the remote cloud 3) Device-edge-cloud cooperative inference: For tasks that\nserver.Thisenablesafasterresponsetimeforlatency-sensitive require extensive computational resources, massive data stor-\napplications, such as real-time video streaming, gaming, and age, or external knowledge, the integration of the device-\nautonomous vehicles. Besides, edge computing allows for edge setup and the cloud infrastructure creates a hierarchical\nthe distribution of processing and storage resources across architecture. Initial data processing begins at the network\nmultiple edge devices and servers, fully leveraging the het- edge. Then the cloud provides further computational power\nerogeneous resources at the network edge. and access to knowledge bases to refine the results. For\n3) Cloud servers: The cloud servers provide powerful example, retrieval-augmented generation requires abundant\ncomputing resources and vast storage capacities essential for extra contextual data to alleviate the hallucination issue. Re-\nhandlingcomplexandresource-intensivetasks.Theyalsohave identification tasks need large-scale databases to perform\naccess to a broad range of AI services and tools that can be intensive search operations. With sufficient computational\nallocatedbasedondemand.Inaddition,thecloudcenterplays power, the beam search maintains multiple hypotheses to find\na critical role in network management, capable of scheduling the most probable sequence of tokens. Additionally, the cloud\ndataflowandintegratinginformationfromvariousedgenodes can also aggregate information from multiple edge nodes to\nformoresophisticatedanalytics.However,duetothephysical gain a more comprehensive view. This cooperative inference\ndistance between the cloud and end devices, relying solely on scheme leverages the respective advantages of devices, edge,\ncloudserversmayintroducehigherlatencyandconsumemore andcloudtoenableAIservicesthatarebeyondthecapabilities\nbandwidth compared to edge computing. of any single tier.\n4\nC. Key Design Considerations\nIndesigningtheAIFlowframework,severalcriticalconsid-\nerations must be addressed to ensure performance, efficiency, √\nand user experience. Two questions that need to be answered Small Large √\nare as follows: model model\n×\n1) How to enable efficient cooperation? Given the con-\n×\nstrained computing capabilities of edge devices, extra com-\nputational support is essential to complete inference tasks.\nA significant challenge in this process is the substantial\ncommunication latency incurred during the transmission of\nsensorydataorintermediateresults.Additionally,variationsin\nwireless connectivity between edge devices and edge servers,\nalong with changes in routing from the edge to the cloud, can\nEdge device Edge server\nfurther delay response times.\n2) How to speed up model inference? The most notable\nFig.3. Cooperationbetweensmallandlargemodelsforedgeinferencebased\ncharacteristic of large models is the massive number of\non speculative decoding. A small model and a large model are deployed at\nparameters, bringing significant computational demands. The an edge device and an edge server, respectively. In this example, the small\ncomputational complexity of transformer-decoder models in- modelgeneratesfourdrafttokens(t1,t2,t3,t4)andsendsthemtothelarge\nmodelforverification.Thefirsttwotokenspasstheverificationwhilethelast\ncreases quadratically with the token length. While methods\ntwofail.\nlike key-value (KV) cache mitigate this challenge by avoiding\nrecomputation, the substantial memory consumption bottle-\nlies in adopting a small model for compact feature extraction\nnecks model inference.\nat the edge device.\nTo address the above questions, two types of enabling\ntechniques are introduced in the following Section IV and\nB. Enabling Techniques\nSection V.\nMany techniques are available to support such cooperative\nIV. COOPERATIVEINFERENCE inference architecture, where a small part of the computation\ntask is completed at the edge side, which can be seen as\nProvidingintelligenceservicesattheedgedevicestypically\na feature extraction process, and more computation-intensive\nrequires transmitting raw data to the remote servers for pro-\noperations are supported by the cloud.\ncessing. When the raw data collected by the edge sensors are\nSplit inference is such a method, which splits a model\nlarge in size, this framework leads to high bandwidth usage\ninto two parts and deploys one part at the device and the\nand delay. Cooperative inference is a promising solution that\nother part at the server. Instead of sending the raw data, split\noffloads the inference task to multiple computational nodes.\ninference allows sending the intermediate activations at the\nEdge devices can first perform data pre-processing to discard\nsplit point to the server. A variant of split inference is the\nredundancy and determine what information is important for\nearly exiting method. By selecting multiple exit points in the\ntransmission. A well-designed cooperative strategy can strike\nbackboneofthemodelandaugmentingadditionalsidebranch\na balance between the communication overhead and the on-\nnetworks, this architecture allows input samples to exit at dif-\ndevice computation to reduce the end-to-end system latency.\nferentpoints.Thepowerfulserverleveragesthecorresponding\nbranch network for further processing. For transformer-based\nA. Theoretical Insights\nmodels, a similar idea is to deploy the near-input embedding\nThe critical problem in cooperative inference is identify- layer at the device and offload the computational-intensive\ning task-relevant information from raw data and discarding transformer blocks at the server. Such deployment extracts\nredundancy to reduce communication overhead. Based on the initialcontextualembeddingsthataregeneralizabletomultiple\ninformation bottleneck principle, cooperative inference can be downstream tasks.\ncharacterized by its ability to filter and compress input data. Thesynergyoflargeandsmallmodelscanalsosupportco-\nOnly essential features should be sent to centralized systems operativeinference.AsillustratedinFig.3,onesuchapproach\nforfurtheranalysis,wheretheamountofinformationisupper is speculative decoding [13]. Typical LLMs and VLMs need\nboundedbythemutualinformationI(X;Y)betweentheinput to generate tokens in an autoregressive manner that outputs\ndataX andthetargetoutputY.Thischaracteristicimpliesthat tokens iteratively. Speculative decoding introduces a novel\nsignificant data compression is achievable without degrading inferenceparadigmwhereasmallmodeldeployedattheedge\nthe inference performance. Ideally, denoting the random vari- device drafts multiple tokens once. Then a large model at the\nableZ astheminimalinformationthatmaintainstheinference server verifies and corrects these tokens in parallel. When the\nperformance, such Z satisfies that the mutual information acceptance rate of draft tokens by the large model is high,\nterms I(X;Z), I(Y;Z), and I(X;Y) are equal [11], [12]. significant latency reduction can be achieved compared to the\nAssuming the mapping from X to Z is deterministic, the traditional server-only inference. This is due to the low cost\nminimal communication overhead is the entropy of Z. To of generating draft tokens by a small model, coupled with the\nreducetheend-to-endlatencyincooperativeinference,thekey low overhead of transmitting these tokens.\n5\nNestedmodels Foundationmodel\nSmall Modelsize Large\nFig.4. Anillustrationofthenestedneuralnetwork.Alargefoundationmodelcontainssub-modelsofdifferentsizes.Thesesub-modelsshareparametersby\nbeingnestedwithinthelargerones.\nV. MODELINFERENCESPEEDUP balance performance with computational efficiency. Besides,\nPowerful AI models have achieved great success in various model quantization decreases the informativeness of activa-\nfields. Meanwhile, they are experiencing rapid growth in tions by transforming high-precision weights into lower-bit\nmodel size. The substantial computational load of large foun- float or integer values. Alternatively, knowledge distillation\ndation models presents a significant bottleneck, especially for transfers knowledge from a large and complex model to a\nmodel inference on resource-constrained devices. It is widely compact one to mimic the functionality of the teacher model,\nrecognized that many popular deep neural network architec- yet with fewer parameters and often faster inference times.\ntures are over-parameterized. This indicates that maintaining Besides permanently removing some parts of the model,\nhigh performance does not necessarily require large models. dynamic neural networks are capable of adjusting their struc-\nTherefore, there is an opportunity for model compression and ture or parameters on the fly, depending on the input they\ninference speedup without sacrificing effectiveness. receive. In particular, Mixture-of-Experts (MoE) characterizes\ndifferent parts of a model as experts specializing in different\nA. Theoretical Insights tasks. Only pertinent experts are engaged for a given input,\nstrikingabalancebetweencomputationalefficiencyandmodel\nThe typical model training involves finding weights such\ncapability. Similarly, Mixture-of-Depths (MoD) chooses to\nthat the maximum information pertaining to the target output\neither apply computation to a token or pass it through a\npropagates from an input to the network output. Model com-\nresidual connection. Such routing emphasizes how individual\npression can be characterized as minimizing the information\ntokens pass through different numbers of layers, or blocks,\nredundancy between adjacent layers while maintaining the\nthrough the depth of the transformer. Furthermore, nested\nrelevance between the activations and the target output [14].\nneural networks provide scalable and adaptive performance\nThe information bottleneck principle provides a convenient\ncapabilities. As shown in Fig. 4, a large foundation model\nmechanism for penalizing information redundancy in data\nconsists of a series of sub-models, which share parameters\nprocessing. Denote the hidden layer activations at layer i−1\nthrough a nesting structure. These sub-models are arranged\nand layer i as Z and Z , respectively. For each layer i, the\ni−1 i hierarchically, such that smaller sub-models form the core of\ninformationbottleneckobjectivetargetsminimizingthemutual\nlarger ones. Each sub-model varies in size and can solve the\ninformation between Z and Z , while maximizing the\ni−1 i same task as the foundation model. This hierarchical design\nmutualinformationbetweenZ andthetargetoutputY.From\ni enables efficient storage and flexible model deployment to\ntheperspectiveofinformationcapacity(IC),theefficiencyofa\nmeet diverse budget constraints.\nmodelischaracterizedbytheamountofpreservedinformation\nFurthermore, there are complementary techniques that can\ndivided by the number of parameters. To reduce the model\nenhance the inference efficiency of large models. During the\nsize and speed up inference, the key lies in identifying and\nautoregressive generation, KV pairs from previous tokens\nremoving redundant information propagation across layers.\nare stored in a cache to assist in generating future tokens.\nHowever,givenalonginputsequence,thecachesizeincreases\nB. Enabling Techniques\nrapidly,leadingtolonginferencelatency.Popularoptimization\nThere are many empirical successes in speeding up model\ntechniques include quantizing the KVs and evicting the less\ninference through compression techniques. Model parameter\ninformative KVs. These approaches allow for faster token\npruningisdevotedtoremovingunimportantcomponents[15].\ngeneration by strategically compressing the cache.\nByaggregatingusefulinformationintoasubsetofactivations,\nother neurons or channels containing task-irrelevant informa-\ntion can be pruned without performance loss. Similarly, low-\nVI. CASESTUDY:SPECULATIVEDECODINGFOREDGE\nrank factorization decomposes parameter matrices into lower-\nINFERENCE\ndimensional components, eliminating redundant information This section provides performance evaluations to demon-\nand reducing the model size. Neural architecture search lever- strate the performance of AI Flow, which leverages the\nagesreinforcementlearningtodesignmodelarchitecturesthat speculative decoding approach to enhance the efficiency of\n6\n140\nSpeculative decoding\n130\nServer-only inference\n120\n110\n100\n90\n80\n70\n60\n1 5 10 15 20 24\nEdge Draft token length\nPlease detect what is in the picture.\nDevice\nFig.6. Timeperoutputtoken(TPOT)asafunctionofthedrafttokenlength.\nThe image captures a nostalgic street scene in black\nand white, highlighting an old-fashioned ice cream\ncart parked on a quiet, tree-lined street. The vendor a\nproducesmoreerrorswithlongertokensequences,resultingin\nman with a striped shirt and dark pants, stands\na lower rate of token acceptance. Specifically, once the token\nEdge behind the cart, seemingly serving customers. The\nServer cart, labeled SINCE 1914, suggests a long-standing lengthexceeds22,theoveralllatencyishigherthanthatofthe\ntradition, likely a family-owned business ...... baseline.Therefore,carefullyselectingtheoptimaldrafttoken\nlength has the potential to enhance the performance gains of\nFig.5. Anillustrationoftheimagecaptioningtask.Anedgedevicecaptures speculative decoding in cooperative inference systems.\nreal-time images of certain places and then asks for Visual-LLM service to\ndetectobjectsandidentifyongoingevents.\nVII. CONCLUSIONSANDFUTUREWORK\nThis article introduced AI Flow, a novel framework de-\ncooperative inference. Consider a device-edge cooperative signed to optimize the inference process by effectively uti-\ninferencesystem.TheedgedeviceisequippedwithaGeForce lizing heterogeneous resources across devices, edge nodes,\nRTX4090,andtheedgenodeisapowerfulGPUcluster.They and cloud servers. To facilitate cooperation among multiple\nare connected by wireless channels with data rates ranging computational nodes, we advocated a paradigm shift in the\nfrom 500 KB/s to 2 MB/s. We evaluate the performance of design of communication network systems from reliably re-\nspeculative decoding on an image captioning task, as shown constructingtherawdatatotransmittingthemosttask-relevant\nin Fig. 5. Due to the limitation of local resources, a small information. This allows edge devices to extract only critical\nInternVL2-2B model is deployed on the device to generate features from the raw sensory data and discard redundant\ndrafttokens.Meanwhile,alargeInternVL2-26Bmodel,which information to reduce communication overhead. A proof-\nserves as the target model, is deployed on the edge server. of-concept case study demonstrated the effectiveness of the\nFor the baseline method, known as server-only inference, proposed framework through an image captioning use case,\nthe edge device transmits raw images directly to the edge showcasingthe abilityto reduceresponselatency whilemain-\nserver. The server then uses the target model for inference. taining high-quality captions. Several areas could be explored\nThe experiments are conducted on the Vehicles-OpenImage for future advancements.\ndataset, which is a slice of the OpenImage dataset containing Security and privacy considerations: Our framework\n627 images of various vehicle classes. We focus on the prioritizes inference efficiency, but security and privacy are\nresponse latency in the experiment and select the time per alsoimportantinedgesystems.Integratingtechniquessuchas\noutput token (TPOP) as the metric. For a fair comparison, we homomorphic encryption and differential privacy mechanisms\nmaintain that the outputs from speculative decoding have the can reduce attack surfaces and protect sensitive information\nsame distributions as those from the server-only inference. against unauthorized access or data breaches.\nAs shown in Fig. 6, speculative decoding significantly Software-hardware co-design: While we center on soft-\nreduces the TPOP compared with the baseline method. In ware algorithms, bringing hardware and software closer could\nparticular, the optimal draft token length in this experiment lead to further optimizations. High-throughput generation en-\nis 4, where the inference speed is approximately double that gines can be flexibly configured by aggregating memory and\nof the baseline. This optimal length balances the acceptance computation from heterogeneous resources. Besides, creating\nrateofdrafttokenswiththeirquantity,maximizingthenumber specializedprocessorstailoredtoAIoperatorslikematrixmul-\nof tokens that pass verification at the edge server. When the tiplication and convolutions also speeds up model execution.\ndraft token length is set below 4, there is a slight increase in Scalability and stability: Concurrent requests from a large\nresponse latency. This occurs because generating fewer draft number of edge devices may strain the system. Strategies for\ntokens does not fully leverage the predictive capabilities of load balancing, workload scheduling, and efficient resource\nthe smaller model on the edge device. In contrast, setting the allocationneedfurtherinvestigation.Redundancyandfailover\ndraft token length above 4 leads to an increase in response mechanisms should also be incorporated to ensure system\nlatency. This is because the smaller model on the edge device resilience and maintain high availability.\n)sm(\nTOPT\n7\nREFERENCES [10] J. Shao and J. Zhang, “Communication-computation trade-off in\nresource-constrained edge inference,” IEEE Commun. Mag., vol. 58,\n[1] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,\npp.20–26,Dec.2020.\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language\n[11] A.A.Alemi,I.Fischer,J.V.Dillon,andK.Murphy,“Deepvariational\nmodels are few-shot learners,” in Proc. Adv. Neural Inf. Process. Syst.\ninformation bottleneck,” in Proc. Int. Conf. Learn. Representations\n(NeurIPS),pp.1877–1901,Dec.2020.\n(ICLR),2022.\n[2] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG.Sastry,A.Askell,P.Mishkin,J.Clark,etal.,“Learningtransferable [12] J.Shao,Y.Mao,andJ.Zhang,“Learningtask-orientedcommunication\nvisual models from natural language supervision,” in Proc. Int. Conf. for edge inference: An information bottleneck approach,” IEEE J. Sel.\nMach.Learn.(ICML),pp.8748–8763,PMLR,2021. AreaCommun.,vol.40,pp.197–211,Jan.2022.\n[3] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”inProc. [13] Y.Leviathan,M.Kalman,andY.Matias,“Fastinferencefromtransform-\nAdv.NeuralInf.Process.syst.(NeurIPS),vol.36,2024. ersviaspeculativedecoding,”inProc.Int.Conf.Mach.Learn.(ICML),\n[4] J.Duan,S.Yu,H.L.Tan,H.Zhu,andC.Tan,“Asurveyofembodied pp.19274–19286,PMLR,2023.\nAI: From simulators to research tasks,” IEEE Trans. Emerg. Topics [14] B. Dai, C. Zhu, B. Guo, and D. Wipf, “Compressing neural networks\nComput.Intell.,vol.6,no.2,pp.230–244,2022. usingthevariationalinformationbottleneck,”inProc.Int.Conf.Mach.\n[5] E. King, H. Yu, S. Lee, and C. Julien, “Sasha: creative goal-oriented Learn.(ICML),pp.1135–1144,2018.\nreasoning in smart homes with large language models,” in Proc. ACM\n[15] X.Ma,G.Fang,andX.Wang,“Llm-pruner:Onthestructuralpruningof\nInt.Mob.WearableUbiquitousTechnol.(IMWUT),vol.8,no.1,pp.1–\nlargelanguagemodels,”Proc.Adv.NeuralInf.Process.Syst.(NeurIPS),\n38,2024.\nvol.36,pp.21702–21720,2023.\n[6] D.Zhang,Y.Li,Z.He,andX.Li,“Empoweringsmartglasseswithlarge\nlanguage models: Towards ubiquitous AGI,” in Proc. ACM Int. Joint\nJiawei Shao [S’20-M’24] (shaojw2@chinatelecom.cn) is a\nConf.PervasiveUbiquitousComput.(UbiComp),pp.631–633,2024.\n[7] Z.Xu,Y.Zhang,E.Xie,Z.Zhao,Y.Guo,K.-Y.K.Wong,Z.Li,and Research Scientist at the Institute of Artificial Intelligence\nH. Zhao, “Drivegpt4: Interpretable end-to-end autonomous driving via (TeleAI), China Telecom. He received his Ph.D. at the Hong\nlargelanguagemodel,”IEEERobot.Automat.Lett.,2024.\nKong University of Science and Technology.\n[8] Y.Shen,J.Shao,X.Zhang,Z.Lin,H.Pan,D.Li,J.Zhang,andK.B.\nLetaief, “Large language models empowered autonomous edge AI for Xuelong Li [M’02-SM’07-F’12] (xuelong_li@ieee.org) is the\nconnectedintelligence,”IEEECommun.Mag.,2024. CTOandChiefScientistofChinaTelecom,wherehefounded\n[9] K.B.Letaiefetal.,“Edgeartificialintelligencefor6G:Vision,enabling\nthe Institute of Artificial Intelligence (TeleAI) of China Tele-\ntechnologies, and applications,” IEEE J. Sel. Areas Commun., vol. 40,\npp.5–36,Jan.2022. com.",
    "pdf_filename": "AI_Flow_at_the_Network_Edge.pdf"
}