{
    "title": "AI Flow at the Network Edge",
    "context": "(LLMs) and their multimodal variants have led to remarkable progress across various domains, demonstrating impressive ca- pabilities and unprecedented potential. In the era of ubiquitous connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered services accessible at the network edge. However, pushing large models from the cloud to resource-constrained environments faces critical challenges. Model inference on low-end devices leads to excessive latency and performance bottlenecks, while raw data transmission over limited bandwidth networks causes high communication overhead. This article presents AI Flow, a frame- work that streamlines the inference process by jointly leveraging the heterogeneous resources available across devices, edge nodes, and cloud servers, making intelligence flow across networks. To facilitate cooperation among multiple computational nodes, the proposed framework explores a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow, where the goal of communications is task-oriented and folded into the inference process. Experimental results demonstrate the effectiveness of the proposed framework through an image captioning use case, showcasing the ability to reduce response latency while maintaining high-quality captions. This article serves as a position paper for identifying the motivation, challenges, and principles of AI Flow. We are witnessing a transformative era in the field of artificial intelligence (AI) with groundbreaking technologies [1]–[3]. LLMs, like ChatGPT and PaLM, have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. Going beyond language, multi- modal foundation models, like CLIP and Llama, have show- cased exceptional performance in cross-modal perception, understanding, and interaction tasks. These advancements led to their adaptations in a broad spectrum of application do- mains, including embodied robotics, augmented reality, and autonomous driving [4]–[7]. Most recently, edge devices (mo- bile phones, smart wearables, and IoT sensors) are becoming increasingly widespread, and sensory data are easy to access. This has sparked a surge of interest in pushing intelligence applications from the central cloud to the network edge [8], [9]. As visualized in Fig. 1, the future envisions a scenario where AI technologies are seamlessly integrated into various aspects of daily life. While offering exciting opportunities, deploying large mod- els at the network edge faces new challenges. In contrast to cloud intelligence, performing AI tasks at the network edge differs significantly in many cases. First, the fundamental difference is the available resources. Edge devices are usually equipped with limited computation resources, whereas cloud The authors are with the Institute of Artificial Intelligence (TeleAI), China Telecom, China (E-mail: shaojw2@chinatelecom.cn, xuelong_li@ieee.org). The corresponding author is Xuelong Li. servers possess a large number of powerful processing units. As a result, on-device model inference struggles to support real-time responses since the large foundation models demand intensive computation [10]. Besides, edge devices are often wirelessly connected. The limited uplink bandwidth and the highly dynamic nature of wireless channels hinder transmitting large volumes of data collected by edge devices to cloud servers in real-time. Aiming at efficient delivery of diversified intelligent ser- vices, we propose AI Flow, a framework that seamlessly integrates intelligence capabilities directly at the network edge. Specifically, AI Flow aims to streamline the inference process by jointly leveraging the heterogeneous computational resources available across devices, edge nodes, and cloud servers. This framework distributes the inference workload across different network layers and adapts to dynamic network conditions. To facilitate cooperation among multiple compu- tational nodes, AI Flow provides a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow. The goal of communica- tions needs to be folded into the inference process. Instead of sending raw data from edge devices to servers for processing, the intelligence flow features a task-oriented property, where edge devices extract only critical features from the raw sensory data and discard redundant information to reduce communica- tion overhead. To summarize, the advantages of the AI Flow framework are manifold. • Ubiquity: The widespread deployment of AI capabilities at the edge makes intelligent responses possible wherever there is network access. This ensures that various devices benefit from AI-driven insights without the need for constant connectivity to central servers. • Adaptivity: The framework distributes inference tasks according to available computational resources, dynamic network conditions, and task requirements. Adaptively scheduling task execution maintains performance in fluc- tuating environments. • Low-latency: By prioritizing data processing close to the source, the delay in transmitting information to powerful servers is minimized. This provides a quicker end-to-end response and achieves a smoother user experience. The rest of the article is organized as follows. Section II introduces typical applications at the network edge, and III provides a system overview of AI Flow. Two types of enabling techniques, namely, cooperative inference and model inference speedup, have been elaborated in Section IV and Section V, respectively. Section VI presents a case study to evaluate the effectiveness of the proposed AI Flow framework. Finally, we conclude this article and point out future research opportunities in Section VII. arXiv:2411.12469v1  [eess.SP]  19 Nov 2024",
    "body": "1\nAI Flow at the Network Edge\nJiawei Shao and Xuelong Li, Fellow, IEEE\nAbstract—Recent advancements in large language models\n(LLMs) and their multimodal variants have led to remarkable\nprogress across various domains, demonstrating impressive ca-\npabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute\nintelligence is a transformative concept, envisioning AI-powered\nservices accessible at the network edge. However, pushing large\nmodels from the cloud to resource-constrained environments\nfaces critical challenges. Model inference on low-end devices\nleads to excessive latency and performance bottlenecks, while raw\ndata transmission over limited bandwidth networks causes high\ncommunication overhead. This article presents AI Flow, a frame-\nwork that streamlines the inference process by jointly leveraging\nthe heterogeneous resources available across devices, edge nodes,\nand cloud servers, making intelligence flow across networks. To\nfacilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of\ncommunication network systems from transmitting information\nflow to intelligence flow, where the goal of communications is\ntask-oriented and folded into the inference process. Experimental\nresults demonstrate the effectiveness of the proposed framework\nthrough an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions.\nThis article serves as a position paper for identifying the\nmotivation, challenges, and principles of AI Flow.\nI. INTRODUCTION\nWe are witnessing a transformative era in the field of\nartificial intelligence (AI) with groundbreaking technologies\n[1]–[3]. LLMs, like ChatGPT and PaLM, have demonstrated\nremarkable capabilities in natural language understanding,\ngeneration, and reasoning. Going beyond language, multi-\nmodal foundation models, like CLIP and Llama, have show-\ncased exceptional performance in cross-modal perception,\nunderstanding, and interaction tasks. These advancements led\nto their adaptations in a broad spectrum of application do-\nmains, including embodied robotics, augmented reality, and\nautonomous driving [4]–[7]. Most recently, edge devices (mo-\nbile phones, smart wearables, and IoT sensors) are becoming\nincreasingly widespread, and sensory data are easy to access.\nThis has sparked a surge of interest in pushing intelligence\napplications from the central cloud to the network edge [8],\n[9]. As visualized in Fig. 1, the future envisions a scenario\nwhere AI technologies are seamlessly integrated into various\naspects of daily life.\nWhile offering exciting opportunities, deploying large mod-\nels at the network edge faces new challenges. In contrast to\ncloud intelligence, performing AI tasks at the network edge\ndiffers significantly in many cases. First, the fundamental\ndifference is the available resources. Edge devices are usually\nequipped with limited computation resources, whereas cloud\nThe authors are with the Institute of Artificial Intelligence (TeleAI), China\nTelecom, China (E-mail: shaojw2@chinatelecom.cn, xuelong_li@ieee.org).\nThe corresponding author is Xuelong Li.\nservers possess a large number of powerful processing units.\nAs a result, on-device model inference struggles to support\nreal-time responses since the large foundation models demand\nintensive computation [10]. Besides, edge devices are often\nwirelessly connected. The limited uplink bandwidth and the\nhighly dynamic nature of wireless channels hinder transmitting\nlarge volumes of data collected by edge devices to cloud\nservers in real-time.\nAiming at efficient delivery of diversified intelligent ser-\nvices, we propose AI Flow, a framework that seamlessly\nintegrates intelligence capabilities directly at the network\nedge. Specifically, AI Flow aims to streamline the inference\nprocess by jointly leveraging the heterogeneous computational\nresources available across devices, edge nodes, and cloud\nservers. This framework distributes the inference workload\nacross different network layers and adapts to dynamic network\nconditions. To facilitate cooperation among multiple compu-\ntational nodes, AI Flow provides a paradigm shift in the\ndesign of communication network systems from transmitting\ninformation flow to intelligence flow. The goal of communica-\ntions needs to be folded into the inference process. Instead of\nsending raw data from edge devices to servers for processing,\nthe intelligence flow features a task-oriented property, where\nedge devices extract only critical features from the raw sensory\ndata and discard redundant information to reduce communica-\ntion overhead. To summarize, the advantages of the AI Flow\nframework are manifold.\n• Ubiquity: The widespread deployment of AI capabilities\nat the edge makes intelligent responses possible wherever\nthere is network access. This ensures that various devices\nbenefit from AI-driven insights without the need for\nconstant connectivity to central servers.\n• Adaptivity: The framework distributes inference tasks\naccording to available computational resources, dynamic\nnetwork conditions, and task requirements. Adaptively\nscheduling task execution maintains performance in fluc-\ntuating environments.\n• Low-latency: By prioritizing data processing close to the\nsource, the delay in transmitting information to powerful\nservers is minimized. This provides a quicker end-to-end\nresponse and achieves a smoother user experience.\nThe rest of the article is organized as follows. Section II\nintroduces typical applications at the network edge, and III\nprovides a system overview of AI Flow. Two types of enabling\ntechniques, namely, cooperative inference and model inference\nspeedup, have been elaborated in Section IV and Section V,\nrespectively. Section VI presents a case study to evaluate the\neffectiveness of the proposed AI Flow framework. Finally, we\nconclude this article and point out future research opportunities\nin Section VII.\narXiv:2411.12469v1  [eess.SP]  19 Nov 2024\n\n2\nAugmented Reality\nSmart Home\nRobotic Control\nAutonomous Driving\nFig. 1. Typical intelligence applications at the network edge.\nII. TYPICAL INTELLIGENCE APPLICATIONS AT THE\nNETWORK EDGE\nLarge models can be directly applied or fine-tuned to a\nbroad range of tasks. As illustrated in Fig. 1, this part will\nfocus on four promising use cases: robotic control, smart\nhome, augmented reality, and autonomous driving.\nRobotic control [4] is crucial for optimizing the interaction\nbetween robots and the environment. Vision-language models\n(VLMs) enhance the perception capabilities of robots. For\ninstance, by combining three-dimensional point clouds with\nvisual-language features from a VLM, a three-dimensional\nmap of the physical world can be created. Integrating audio\nsignals and haptic features with visual data enables embodied\nrobotics to navigate using multimodal objectives. With remark-\nable generalization capabilities, LLMs allow robots to compre-\nhend human instruction or complicated environments, enabling\nthem to perform embodied reasoning and break down complex\ntasks into actionable steps. Nevertheless, centralized robotic\ncontrol involves massive streaming video upload, which could\noverwhelm the wireless networks.\nSmart homes [5] benefit from the integration of multimodal\nlarge language models (MLLMs), which utilize diverse sen-\nsory data from the home environment. For instance, MLLMs\ncan intelligently adjust air conditioner settings by integrating\ntemperature data and voice commands. Furthermore, the chan-\nnel state information (CSI) obtained from WiFi sensing pro-\nvides extra information for localization. This allows MLLMs\nto make context-aware decisions, such as turning lights on\nor off based on the predicted trajectory of humans inferred\nfrom the CSI. However, the large models demand inten-\nsive computational resources. While heterogeneous devices in\nsmart homes provide distributed computing power, they remain\ninsufficient for real-time inference due to the limitations of\nnetwork bandwidth and the difficulty in synchronization.\nAugmented reality [6] enhances user interactions with their\nsurroundings through glasses, headsets, and other smart wear-\nables. These devices overlay digital information, guided by\ngenerative models, directly into the field of vision. For visually\nimpaired individuals, AI glasses can highlight important visual\nfeatures such as object edges, provide navigational cues, or\ndisplay relevant contextual information about the environ-\nment, greatly enhancing spatial awareness. Additionally, LLM-\npowered devices can streamline human communication by\ntranslating spoken language into clear, concise text displayed\nin real-time. However, the need to sense the environment and\nprocess large volumes of multimodal data makes it imprac-\ntical for resource-constrained headsets. This necessitates the\ndevelopment of more efficient processing techniques and the\noptimization of data transmission at the network edge.\nAutonomous driving [7] has great promise to revolutionize\ntransportation. VLMs enable a combination of scene descrip-\ntion and hierarchical planning. Typical methods leverage video\nfrom a front-view camera to make driving decisions regarding\nspeed and angle. With LiDAR sensor data, more accurate\nperception can be achieved than with visual data alone, as\ncamera imagery can be impacted by weather conditions. More-\nover, LLMs can use textual data, such as traffic and weather\nreports, to optimize route planning. Additionally, LLMs in-\nteracting with passengers help them to understand the motion\ncontrol process by explaining each driving decision. However,\nautonomous driving has stringent latency requirements. High\nlatency between vehicles and the data center poses a risk\nto safety. Addressing this issue is crucial for the successful\nimplementation of autonomous driving systems.\n\n3\nEdge servers\nEdge devices\nCloud servers\nAI Flow\nFig. 2. A system overview of the AI Flow framework.\nIII. SYSTEM OVERVIEW\nThis section outlines the modules, inference paradigms, and\nkey design considerations in AI Flow.\nA. Modules\nThe architecture, as illustrated in Fig. 2, comprises the\nfollowing three modules.\n1) Edge devices: The edge devices denote the end-user\nequipment, such as smartphones, wearables, and IoT sensors,\nthat collect data and interact with users directly. They are\nessential for initial data capture, preliminary processing, and\ninformation transmission. As these devices are often the first\ntouchpoint in data processing, they play a critical role in\ndetermining the responsiveness and effectiveness of the AI\nFlow framework. However, constrained by limited computing\nand energy resources, these devices cannot perform intensive\ncomputation or storage-heavy tasks. This limitation necessi-\ntates a cooperative processing arrangement with edge or cloud\nservers.\n2) Edge servers: The edge servers provide processing,\nstorage, and other resources at an edge location. Typical edge\nservers are deployed at base stations and cell towers that are\ndirectly connected to edge users and devices. By processing\ndata closer to the source, edge servers can significantly reduce\nthe round-trip time without sending data to the remote cloud\nserver. This enables a faster response time for latency-sensitive\napplications, such as real-time video streaming, gaming, and\nautonomous vehicles. Besides, edge computing allows for\nthe distribution of processing and storage resources across\nmultiple edge devices and servers, fully leveraging the het-\nerogeneous resources at the network edge.\n3) Cloud servers:\nThe cloud servers provide powerful\ncomputing resources and vast storage capacities essential for\nhandling complex and resource-intensive tasks. They also have\naccess to a broad range of AI services and tools that can be\nallocated based on demand. In addition, the cloud center plays\na critical role in network management, capable of scheduling\ndata flow and integrating information from various edge nodes\nfor more sophisticated analytics. However, due to the physical\ndistance between the cloud and end devices, relying solely on\ncloud servers may introduce higher latency and consume more\nbandwidth compared to edge computing.\nB. Inference Paradigms\nThere are three types of cooperative schemes in AI Flow,\nincluding, on-device inference, device-edge cooperative infer-\nence, and device-edge-cloud cooperative inference.\n1) On-device inference: In this scheme, the devices com-\nplete some simple inference tasks locally and independently\nbased on the built-in AI chip. Due to limitations in storage and\ncomputational resources, model light-weighting techniques are\nwidely adopted to reduce storage and computational costs.\nIn addition, hardware accelerators, such as neural processing\nunits and field programmable gate arrays, are utilized to\nenhance processing capabilities and energy efficiency. These\nenable the scalable deployment of AI models, making on-\ndevice inference a widely adopted solution.\n2) Device-edge cooperative inference: While on-device in-\nference is suitable for basic tasks, it struggles to handle com-\nplex tasks that require larger AI models, leading to excessive\nlatency. Edge computing offers supplementary computational\nresources as a promising solution. This setup allows devices to\neither send raw data directly to edge servers for inference or\nto perform preliminary computations locally and then send the\nintermediate results to the edge servers for further processing.\nInstead of uploading data to a central server far away, the\nproximity to data sources reduces latency, saves bandwidth,\nand improves system responsiveness.\n3) Device-edge-cloud cooperative inference: For tasks that\nrequire extensive computational resources, massive data stor-\nage, or external knowledge, the integration of the device-\nedge setup and the cloud infrastructure creates a hierarchical\narchitecture. Initial data processing begins at the network\nedge. Then the cloud provides further computational power\nand access to knowledge bases to refine the results. For\nexample, retrieval-augmented generation requires abundant\nextra contextual data to alleviate the hallucination issue. Re-\nidentification tasks need large-scale databases to perform\nintensive search operations. With sufficient computational\npower, the beam search maintains multiple hypotheses to find\nthe most probable sequence of tokens. Additionally, the cloud\ncan also aggregate information from multiple edge nodes to\ngain a more comprehensive view. This cooperative inference\nscheme leverages the respective advantages of devices, edge,\nand cloud to enable AI services that are beyond the capabilities\nof any single tier.\n\n4\nC. Key Design Considerations\nIn designing the AI Flow framework, several critical consid-\nerations must be addressed to ensure performance, efficiency,\nand user experience. Two questions that need to be answered\nare as follows:\n1) How to enable efficient cooperation? Given the con-\nstrained computing capabilities of edge devices, extra com-\nputational support is essential to complete inference tasks.\nA significant challenge in this process is the substantial\ncommunication latency incurred during the transmission of\nsensory data or intermediate results. Additionally, variations in\nwireless connectivity between edge devices and edge servers,\nalong with changes in routing from the edge to the cloud, can\nfurther delay response times.\n2) How to speed up model inference? The most notable\ncharacteristic of large models is the massive number of\nparameters, bringing significant computational demands. The\ncomputational complexity of transformer-decoder models in-\ncreases quadratically with the token length. While methods\nlike key-value (KV) cache mitigate this challenge by avoiding\nrecomputation, the substantial memory consumption bottle-\nnecks model inference.\nTo address the above questions, two types of enabling\ntechniques are introduced in the following Section IV and\nSection V.\nIV. COOPERATIVE INFERENCE\nProviding intelligence services at the edge devices typically\nrequires transmitting raw data to the remote servers for pro-\ncessing. When the raw data collected by the edge sensors are\nlarge in size, this framework leads to high bandwidth usage\nand delay. Cooperative inference is a promising solution that\noffloads the inference task to multiple computational nodes.\nEdge devices can first perform data pre-processing to discard\nredundancy and determine what information is important for\ntransmission. A well-designed cooperative strategy can strike\na balance between the communication overhead and the on-\ndevice computation to reduce the end-to-end system latency.\nA. Theoretical Insights\nThe critical problem in cooperative inference is identify-\ning task-relevant information from raw data and discarding\nredundancy to reduce communication overhead. Based on the\ninformation bottleneck principle, cooperative inference can be\ncharacterized by its ability to filter and compress input data.\nOnly essential features should be sent to centralized systems\nfor further analysis, where the amount of information is upper\nbounded by the mutual information I(X; Y ) between the input\ndata X and the target output Y . This characteristic implies that\nsignificant data compression is achievable without degrading\nthe inference performance. Ideally, denoting the random vari-\nable Z as the minimal information that maintains the inference\nperformance, such Z satisfies that the mutual information\nterms I(X; Z), I(Y ; Z), and I(X; Y ) are equal [11], [12].\nAssuming the mapping from X to Z is deterministic, the\nminimal communication overhead is the entropy of Z. To\nreduce the end-to-end latency in cooperative inference, the key\n√\n√\n×\n×\nLarge \nmodel\nSmall \nmodel\nEdge device\nEdge server\nFig. 3. Cooperation between small and large models for edge inference based\non speculative decoding. A small model and a large model are deployed at\nan edge device and an edge server, respectively. In this example, the small\nmodel generates four draft tokens (t1, t2, t3, t4) and sends them to the large\nmodel for verification. The first two tokens pass the verification while the last\ntwo fail.\nlies in adopting a small model for compact feature extraction\nat the edge device.\nB. Enabling Techniques\nMany techniques are available to support such cooperative\ninference architecture, where a small part of the computation\ntask is completed at the edge side, which can be seen as\na feature extraction process, and more computation-intensive\noperations are supported by the cloud.\nSplit inference is such a method, which splits a model\ninto two parts and deploys one part at the device and the\nother part at the server. Instead of sending the raw data, split\ninference allows sending the intermediate activations at the\nsplit point to the server. A variant of split inference is the\nearly exiting method. By selecting multiple exit points in the\nbackbone of the model and augmenting additional side branch\nnetworks, this architecture allows input samples to exit at dif-\nferent points. The powerful server leverages the corresponding\nbranch network for further processing. For transformer-based\nmodels, a similar idea is to deploy the near-input embedding\nlayer at the device and offload the computational-intensive\ntransformer blocks at the server. Such deployment extracts\ninitial contextual embeddings that are generalizable to multiple\ndownstream tasks.\nThe synergy of large and small models can also support co-\noperative inference. As illustrated in Fig. 3, one such approach\nis speculative decoding [13]. Typical LLMs and VLMs need\nto generate tokens in an autoregressive manner that outputs\ntokens iteratively. Speculative decoding introduces a novel\ninference paradigm where a small model deployed at the edge\ndevice drafts multiple tokens once. Then a large model at the\nserver verifies and corrects these tokens in parallel. When the\nacceptance rate of draft tokens by the large model is high,\nsignificant latency reduction can be achieved compared to the\ntraditional server-only inference. This is due to the low cost\nof generating draft tokens by a small model, coupled with the\nlow overhead of transmitting these tokens.\n\n5\nFoundation model\nModel size\nLarge\nSmall\nNested models\nFig. 4. An illustration of the nested neural network. A large foundation model contains sub-models of different sizes. These sub-models share parameters by\nbeing nested within the larger ones.\nV. MODEL INFERENCE SPEEDUP\nPowerful AI models have achieved great success in various\nfields. Meanwhile, they are experiencing rapid growth in\nmodel size. The substantial computational load of large foun-\ndation models presents a significant bottleneck, especially for\nmodel inference on resource-constrained devices. It is widely\nrecognized that many popular deep neural network architec-\ntures are over-parameterized. This indicates that maintaining\nhigh performance does not necessarily require large models.\nTherefore, there is an opportunity for model compression and\ninference speedup without sacrificing effectiveness.\nA. Theoretical Insights\nThe typical model training involves finding weights such\nthat the maximum information pertaining to the target output\npropagates from an input to the network output. Model com-\npression can be characterized as minimizing the information\nredundancy between adjacent layers while maintaining the\nrelevance between the activations and the target output [14].\nThe information bottleneck principle provides a convenient\nmechanism for penalizing information redundancy in data\nprocessing. Denote the hidden layer activations at layer i −1\nand layer i as Zi−1 and Zi, respectively. For each layer i, the\ninformation bottleneck objective targets minimizing the mutual\ninformation between Zi−1 and Zi, while maximizing the\nmutual information between Zi and the target output Y . From\nthe perspective of information capacity (IC), the efficiency of a\nmodel is characterized by the amount of preserved information\ndivided by the number of parameters. To reduce the model\nsize and speed up inference, the key lies in identifying and\nremoving redundant information propagation across layers.\nB. Enabling Techniques\nThere are many empirical successes in speeding up model\ninference through compression techniques. Model parameter\npruning is devoted to removing unimportant components [15].\nBy aggregating useful information into a subset of activations,\nother neurons or channels containing task-irrelevant informa-\ntion can be pruned without performance loss. Similarly, low-\nrank factorization decomposes parameter matrices into lower-\ndimensional components, eliminating redundant information\nand reducing the model size. Neural architecture search lever-\nages reinforcement learning to design model architectures that\nbalance performance with computational efficiency. Besides,\nmodel quantization decreases the informativeness of activa-\ntions by transforming high-precision weights into lower-bit\nfloat or integer values. Alternatively, knowledge distillation\ntransfers knowledge from a large and complex model to a\ncompact one to mimic the functionality of the teacher model,\nyet with fewer parameters and often faster inference times.\nBesides permanently removing some parts of the model,\ndynamic neural networks are capable of adjusting their struc-\nture or parameters on the fly, depending on the input they\nreceive. In particular, Mixture-of-Experts (MoE) characterizes\ndifferent parts of a model as experts specializing in different\ntasks. Only pertinent experts are engaged for a given input,\nstriking a balance between computational efficiency and model\ncapability. Similarly, Mixture-of-Depths (MoD) chooses to\neither apply computation to a token or pass it through a\nresidual connection. Such routing emphasizes how individual\ntokens pass through different numbers of layers, or blocks,\nthrough the depth of the transformer. Furthermore, nested\nneural networks provide scalable and adaptive performance\ncapabilities. As shown in Fig. 4, a large foundation model\nconsists of a series of sub-models, which share parameters\nthrough a nesting structure. These sub-models are arranged\nhierarchically, such that smaller sub-models form the core of\nlarger ones. Each sub-model varies in size and can solve the\nsame task as the foundation model. This hierarchical design\nenables efficient storage and flexible model deployment to\nmeet diverse budget constraints.\nFurthermore, there are complementary techniques that can\nenhance the inference efficiency of large models. During the\nautoregressive generation, KV pairs from previous tokens\nare stored in a cache to assist in generating future tokens.\nHowever, given a long input sequence, the cache size increases\nrapidly, leading to long inference latency. Popular optimization\ntechniques include quantizing the KVs and evicting the less\ninformative KVs. These approaches allow for faster token\ngeneration by strategically compressing the cache.\nVI. CASE STUDY: SPECULATIVE DECODING FOR EDGE\nINFERENCE\nThis section provides performance evaluations to demon-\nstrate the performance of AI Flow, which leverages the\nspeculative decoding approach to enhance the efficiency of\n\n6\nEdge\nServer\nEdge\nDevice\nThe image captures a nostalgic street scene in black\nand white, highlighting  an old-fashioned ice cream\ncart parked on a quiet, tree-lined street. The vendor a\nman  with a striped shirt and dark pants, stands\nbehind the cart, seemingly serving customers. The\ncart, labeled SINCE 1914, suggests a long-standing\ntradition, likely a family-owned business ......\nPlease detect what is in the picture.\nFig. 5. An illustration of the image captioning task. An edge device captures\nreal-time images of certain places and then asks for Visual-LLM service to\ndetect objects and identify ongoing events.\ncooperative inference. Consider a device-edge cooperative\ninference system. The edge device is equipped with a GeForce\nRTX 4090, and the edge node is a powerful GPU cluster. They\nare connected by wireless channels with data rates ranging\nfrom 500 KB/s to 2 MB/s. We evaluate the performance of\nspeculative decoding on an image captioning task, as shown\nin Fig. 5. Due to the limitation of local resources, a small\nInternVL2-2B model is deployed on the device to generate\ndraft tokens. Meanwhile, a large InternVL2-26B model, which\nserves as the target model, is deployed on the edge server.\nFor the baseline method, known as server-only inference,\nthe edge device transmits raw images directly to the edge\nserver. The server then uses the target model for inference.\nThe experiments are conducted on the Vehicles-OpenImage\ndataset, which is a slice of the OpenImage dataset containing\n627 images of various vehicle classes. We focus on the\nresponse latency in the experiment and select the time per\noutput token (TPOP) as the metric. For a fair comparison, we\nmaintain that the outputs from speculative decoding have the\nsame distributions as those from the server-only inference.\nAs shown in Fig. 6, speculative decoding significantly\nreduces the TPOP compared with the baseline method. In\nparticular, the optimal draft token length in this experiment\nis 4, where the inference speed is approximately double that\nof the baseline. This optimal length balances the acceptance\nrate of draft tokens with their quantity, maximizing the number\nof tokens that pass verification at the edge server. When the\ndraft token length is set below 4, there is a slight increase in\nresponse latency. This occurs because generating fewer draft\ntokens does not fully leverage the predictive capabilities of\nthe smaller model on the edge device. In contrast, setting the\ndraft token length above 4 leads to an increase in response\nlatency. This is because the smaller model on the edge device\n1\n5\n10\n15\n20\n24\nDraft token length\n60\n70\n80\n90\n100\n110\n120\n130\n140\nTPOT (ms)\nServer-only inference\nSpeculative decoding\nFig. 6. Time per output token (TPOT) as a function of the draft token length.\nproduces more errors with longer token sequences, resulting in\na lower rate of token acceptance. Specifically, once the token\nlength exceeds 22, the overall latency is higher than that of the\nbaseline. Therefore, carefully selecting the optimal draft token\nlength has the potential to enhance the performance gains of\nspeculative decoding in cooperative inference systems.\nVII. CONCLUSIONS AND FUTURE WORK\nThis article introduced AI Flow, a novel framework de-\nsigned to optimize the inference process by effectively uti-\nlizing heterogeneous resources across devices, edge nodes,\nand cloud servers. To facilitate cooperation among multiple\ncomputational nodes, we advocated a paradigm shift in the\ndesign of communication network systems from reliably re-\nconstructing the raw data to transmitting the most task-relevant\ninformation. This allows edge devices to extract only critical\nfeatures from the raw sensory data and discard redundant\ninformation to reduce communication overhead. A proof-\nof-concept case study demonstrated the effectiveness of the\nproposed framework through an image captioning use case,\nshowcasing the ability to reduce response latency while main-\ntaining high-quality captions. Several areas could be explored\nfor future advancements.\nSecurity and privacy considerations: Our framework\nprioritizes inference efficiency, but security and privacy are\nalso important in edge systems. Integrating techniques such as\nhomomorphic encryption and differential privacy mechanisms\ncan reduce attack surfaces and protect sensitive information\nagainst unauthorized access or data breaches.\nSoftware-hardware co-design: While we center on soft-\nware algorithms, bringing hardware and software closer could\nlead to further optimizations. High-throughput generation en-\ngines can be flexibly configured by aggregating memory and\ncomputation from heterogeneous resources. Besides, creating\nspecialized processors tailored to AI operators like matrix mul-\ntiplication and convolutions also speeds up model execution.\nScalability and stability: Concurrent requests from a large\nnumber of edge devices may strain the system. Strategies for\nload balancing, workload scheduling, and efficient resource\nallocation need further investigation. Redundancy and failover\nmechanisms should also be incorporated to ensure system\nresilience and maintain high availability.\n\n7\nREFERENCES\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language\nmodels are few-shot learners,” in Proc. Adv. Neural Inf. Process. Syst.\n(NeurIPS), pp. 1877–1901, Dec. 2020.\n[2] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable\nvisual models from natural language supervision,” in Proc. Int. Conf.\nMach. Learn. (ICML), pp. 8748–8763, PMLR, 2021.\n[3] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” in Proc.\nAdv. Neural Inf. Process. syst. (NeurIPS), vol. 36, 2024.\n[4] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, “A survey of embodied\nAI: From simulators to research tasks,” IEEE Trans. Emerg. Topics\nComput. Intell., vol. 6, no. 2, pp. 230–244, 2022.\n[5] E. King, H. Yu, S. Lee, and C. Julien, “Sasha: creative goal-oriented\nreasoning in smart homes with large language models,” in Proc. ACM\nInt. Mob. Wearable Ubiquitous Technol. (IMWUT), vol. 8, no. 1, pp. 1–\n38, 2024.\n[6] D. Zhang, Y. Li, Z. He, and X. Li, “Empowering smart glasses with large\nlanguage models: Towards ubiquitous AGI,” in Proc. ACM Int. Joint\nConf. Pervasive Ubiquitous Comput. (UbiComp), pp. 631–633, 2024.\n[7] Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K.-Y. K. Wong, Z. Li, and\nH. Zhao, “Drivegpt4: Interpretable end-to-end autonomous driving via\nlarge language model,” IEEE Robot. Automat. Lett., 2024.\n[8] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.\nLetaief, “Large language models empowered autonomous edge AI for\nconnected intelligence,” IEEE Commun. Mag., 2024.\n[9] K. B. Letaief et al., “Edge artificial intelligence for 6G: Vision, enabling\ntechnologies, and applications,” IEEE J. Sel. Areas Commun., vol. 40,\npp. 5–36, Jan. 2022.\n[10] J. Shao and J. Zhang, “Communication-computation trade-off in\nresource-constrained edge inference,” IEEE Commun. Mag., vol. 58,\npp. 20–26, Dec. 2020.\n[11] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep variational\ninformation bottleneck,” in Proc. Int. Conf. Learn. Representations\n(ICLR), 2022.\n[12] J. Shao, Y. Mao, and J. Zhang, “Learning task-oriented communication\nfor edge inference: An information bottleneck approach,” IEEE J. Sel.\nArea Commun., vol. 40, pp. 197–211, Jan. 2022.\n[13] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transform-\ners via speculative decoding,” in Proc. Int. Conf. Mach. Learn. (ICML),\npp. 19274–19286, PMLR, 2023.\n[14] B. Dai, C. Zhu, B. Guo, and D. Wipf, “Compressing neural networks\nusing the variational information bottleneck,” in Proc. Int. Conf. Mach.\nLearn. (ICML), pp. 1135–1144, 2018.\n[15] X. Ma, G. Fang, and X. Wang, “Llm-pruner: On the structural pruning of\nlarge language models,” Proc. Adv. Neural Inf. Process. Syst. (NeurIPS),\nvol. 36, pp. 21702–21720, 2023.\nJiawei Shao [S’20-M’24] (shaojw2@chinatelecom.cn) is a\nResearch Scientist at the Institute of Artificial Intelligence\n(TeleAI), China Telecom. He received his Ph.D. at the Hong\nKong University of Science and Technology.\nXuelong Li [M’02-SM’07-F’12] (xuelong_li@ieee.org) is the\nCTO and Chief Scientist of China Telecom, where he founded\nthe Institute of Artificial Intelligence (TeleAI) of China Tele-\ncom.",
    "pdf_filename": "AI_Flow_at_the_Network_Edge.pdf"
}